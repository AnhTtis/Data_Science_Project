%\documentclass[twoside]{article}
\documentclass[12pt]{colt2023}
%\usepackage{aistats2022}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%
%\usepackage[accepted]{aistats2022}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%square,sort,comma,numbers
%\usepackage[round]{natbib}

%\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:

%\bibliographystyle{apalike} % nt permitted

%MY stuff
%\usepackage{amsthm} %clash with jmlr

\usepackage{amsmath,amssymb}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{xcolor}

%\usepackage{subfig} %clash with jmlr
\usepackage{verbatim,hyperref}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{thmtools,thm-restate}


\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{dirtytalk}

\usepackage{stmaryrd}

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{marginnote} % for better margin notes
%\usepackage{easyReview}

%algorithms
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\usepackage[margin=1in]{geometry}
%%%%%%%%%%%%%

%tables
\usepackage{booktabs}
\usepackage{siunitx}
%%%%%%%%%%%%%%
\usepackage{bbm}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
%\newtheorem{proposition}{Proposition}
\newtheorem{fact}{Fact}
%\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{note}{Note}
\newtheorem{lie}{Lie theory derivation}

%\newtheorem{definition}{Definition}
%\newtheorem{example}{Example}

\newcommand{\e}{\epsilon}

\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\poly}{\text{poly}}
\newcommand{\supp}{\text{supp}}

% our macros
\newcommand{\samp}{{\mathcal S}}
\newcommand{\dist}{{\mathcal D}}
\newcommand{\family}{{\mathcal H}}
\newcommand{\loss}{{\ell}}
\newcommand{\expectation}{{\mathbb E}}
\newcommand{\x}{\pmb{x}} % generic vector of functions when we talk about Lagrangian 
\newcommand{\xdot}{\dot{\pmb{x}}} % time derivative of \x
\newcommand{\xddot}{\ddot{\pmb{x}}} %second time derivative of \x
\newcommand{\dx}{\pmb{\xi}} % time derivative of \x
\newcommand{\dxdot}{\dot{\pmb{\xi}}} %second time derivative of \x

\newcommand{\w}{\pmb{w}} % generic vector of
\newcommand{\ths}{\pmb{t}} % generic vector of
\newcommand{\slopes}{\pmb{v}} % generic vector of
\newcommand{\bias}{\pmb{b}} % generic vector of 
\newcommand{\wdot}{\dot{\pmb{w}}} % time derivative of \x
\newcommand{\wddot}{\ddot{\pmb{w}}} %second time derivative of \x
\newcommand{\gen}{\pmb{g}} %generator of symmetries
\newcommand{\y}{\pmb{y}} 
\newcommand{\uu}{\pmb{u}} 
\newcommand{\alphap}{\pmb{\alpha}} 
\newcommand{\variation}{V}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\codim}{\mathrm{codim}}

\newcommand{\bet}{\pmb{\beta}} %vector of parameters for Swish
\newcommand{\betdot}{\pmb{\beta}} %time derivative of vector of parameters for Swish
\newcommand{\betddot}{\pmb{\beta}} %second time deriv. of vector of parameters for Swish

\newcommand{\z}{\pmb{z}} % generic vector of values
\newcommand{\Lag}{{\mathcal L}} % Lagrangian
\newcommand{\pot}{U} % Lagrangian
\newcommand{\Act}{J} % the action associated to a Lagrangian
\newcommand{\vd}{\nabla \Act} % variational derivative


\newcommand{\trace}{{\text{tr}}} % trace
\newcommand{\pred}{{f}}

\newcommand{\cnstdd}{{\kappa_2}}
\newcommand{\cnstd}{{\kappa_1}}

\newcommand{\noise}{{\eta}}

%%%%%%% from other paper
\newcommand{\code}{{\mathcal C}}
\newcommand{\coset}{{\mathcal K}}
\newcommand{\group}{{\mathcal G}}
\newcommand{\PB}{P_{\text{B}}^{\text{MAP}}}
%\newcommand{\set}{{\mathcal S}}
\newcommand{\pset}{{\mathcal P}}
\newcommand{\Ldens}{{}}
\newcommand{\shell}{{\mathcal S}}
\newcommand{\eset}{{\mathcal E}}
\newcommand{\vset}{{\mathcal V}}
\newcommand{\prob}{{\mathbb P}}
\newcommand{\reals}{{\mathbb R}}
\newcommand{\field}{{\mathbb F}_2}
\newcommand{\naturals}{{\mathbb N}}
\newcommand{\cw}{c}
%\newcommand{\argmax}{\text{argmax}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\xv}{\mathbf x}
\newcommand{\vv}{\mathbf v}
\newcommand{\yv}{\mathbf y}
\newcommand{\Xv}{\mathbf X}
\newcommand{\muv}{\mathbf \mu}
\newcommand{\wv}{\mathbf w}
\newcommand{\Wv}{\mathbf W}
\newcommand{\Dv}{\mathbf D}
\newcommand{\Rv}{\mathbf R}
\newcommand{\Zv}{\mathbf Z}
\newcommand{\Uv}{\mathbf U}
\newcommand{\Iv}{\mathbf I}
\newcommand{\Sigmav}{\mathbf \Sigma}
%\newcommand{\muv}{\mathbf \mu}
\newcommand{\Kv}{\mathbf K}
\newcommand{\model}{\mathcal H}
\newcommand{\indim}{m}
\newcommand{\famdim}{k}
\newcommand{\outdim}{n}
\newcommand{\inspace}{{\mathcal X}}
\newcommand{\outspace}{{\mathcal Y}}
\newcommand{\sampspace}{{\mathcal Z}}
\newcommand{\btheta}{\boldsymbol{\theta}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
%%%%%%%% Check if needed
\usepackage{comment}

%\usepackage{todonotes}
%%% USE TO REMOVE COMMENTS
%\usepackage[disable]{todonotes}

\newcounter{mycomment}
\newcommand{\comm}[2]{%initials of the author (optional) + note in the margin
\refstepcounter{mycomment}
{%
    %\setstretch{0.7}% spacing
    \todo[author = \textbf{#1~\#~\themycomment}, color={red!100!green!35}, fancyline, size = \footnotesize]{%
        #2}%
    }
}

\newenvironment{proofsketch}{\paragraph{Proof Sketch:}}{\hfill$\square$}

\newcommand{\com}[1]{\textcolor{magenta}{gg: #1}}

%equations in tables

%\usepackage{tabularx} %Clash with jmlr

\usepackage{array,collcell}
\newcommand\AddLabel[1]{%
  \refstepcounter{equation}% increment equation counter
  (\theequation)% print equation number
  \label{#1}% give the equation a \label
}
\newcolumntype{M}{>{\hfil$\displaystyle}X<{$\hfil}} % mathematics column
\newcolumntype{L}{>{\collectcell\AddLabel}r<{\endcollectcell}}

\title[Bayes Complexity of Learners vs Overfitting]{Bayes Complexity of Learners vs Overfitting}

\coltauthor{\Name{Grzegorz Głuch} \Email{grzegorz.gluch@epfl.ch}\\
  \Name{Ruediger Urbanke} \Email{ruediger.urbanke@epfl.ch}\\
  \addr EPFL, Lausanne, Switzerland}

\begin{document}

\maketitle

\begin{abstract}
%Why do neural network generalize so well? In Saverese et al. it was shown that for two-layer neural networks with scalar inputs, minimizing the loss plus the square of the $\ell_2$ norm of the parameters is equivalent to finding a hypothesis that minimizes the loss plus the ``complexity'' of the hypothesis. Here, the complexity is defined as the integral of the second derivative of the function, a natural measure of the variability. In words, in this setting ``simple'' functions are preferred over complicated ones, assuming that they have a roughly equal loss. This partially explains the good generalization behavior of these neural nets. Unfortunately it is challenging to extend this approach to more general networks (more layers, non-scalar inputs). We propose a new notion of complexity. We call it the {\em Bayes} complexity. For the case considered by Saverese et al. these two complexity notions roughly agree with each other. But the Bayes measure naturally applies also to more general and more complicated settings. Our notion has close connections to the PAC-Bayes bound. Moreover, using this notion, we show a marked difference between linear schemes and neural networks. This difference better explains why those two hypothesis classes show a marked difference in their generalization behavior.

We introduce a new notion of complexity
of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks
and linear schemes. While there is a large set of papers which describes bounds that 
have each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers. 

Even though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. 
An upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and 4-layer neural networks for periodic functions. 

%We give such a lower bound for 4-layer neural networks. Assuming it's a good approximation to the true complexity we show that for periodic functions the number of samples needed for a good generalization bound for the case of 4-layer neural networks is quadratically smaller than for 2-layer ones.

\end{abstract}

%\tableofcontents

\section{Introduction}\label{sec:intro}
There is a large body of literature devoted to the question of generalization, both from a practical point of view as well as concerning our theoretical understanding, e.g., \citet{pmlr-v80-arora18b,DBLP:journals/corr/NeyshaburBMS17,DBLP:journals/corr/NeyshaburBMS17aa,NIPS2017_b22b257a,DBLP:journals/corr/NeyshaburTS15, NEURIPS2019_05e97c20}, to mention just a few. We add to this discussion. In particular, we ask what role is played by the hypothesis class assuming a Bayesian point of view. Our main observation is that there is a striking theoretical difference between linear schemes and neural networks. In a nutshell, neural networks, when trained with appropriate gradient methods using a modest amount of training data, strongly prefer hypothesis that are ``easy'' to represent in the sense that there is a large parameter space that approximately represents this hypothesis. For linear schemes no such preference exists. This leads us to a notion of a complexity of a function with respect to a given hypothesis class and prior. We then show that (i) this complexity is the main component in a standard PAC-Bayes bound, and (ii) that the ordering implied by this complexity corresponds well to ``natural'' notions of complexity of functions that have previously been discussed in the literature. In words, neural networks learn ``simple'' functions and hence do not tend to overfit.

For $n \in \N$ we define $[n] = \{1,\dots,n\}$. Let $\inspace$ be the input space, $\outspace$ be the output space and $\sampspace := \inspace \times \outspace$ be the sample space. Let $\mathcal{H}_{\theta}$ be the hypothesis class, parameterized by $\theta \in \mathbb{R}^m $. We define the loss as a function $\ell: \mathcal {H} \times  \sampspace \rightarrow \reals_+$. We focus on the clipped to $C$ version of the quadratic loss but our results can be generalized to other loss functions. We denote by $\dist_x$ a distribution on the input space $\inspace$, and by $\dist$ a distribution on the sample space $\sampspace$. Finally, we let $\samp =\{z_1, \cdots z_N\}$ be the given sample set, where we assume that the individual samples are chosen iid according to the distribution ${\mathcal D}$.

\subsection{The PAC Bayes Bound}
Our starting point is a version of the well-known PAC-Bayes bound, see \citet{pacbayes}.
\begin{lemma}[PAC-Bayes Bound]
Let the loss function $\ell$ be bounded, i.e., $\ell: {\mathcal H} \times {\mathcal Z} \rightarrow [0, C]$. 
Let $P$ be a prior on ${\mathcal H}$ and $Q$ be any other distribution on ${\mathcal H}$ (possibly dependent on $\samp$). Then 
\begin{align} \label{equ:pacbound}
\expectation_{\samp}\left[L_{{\mathcal D}}(Q)\right]  \leq \expectation_{\samp} \left[ L_{\samp}(Q) + C\sqrt{\frac{D(Q \| P)}{2N}} \right],
\end{align}
where
\begin{align} \label{equ:truepacloss}
L_{{\mathcal D}}(Q) & = \expectation_{z \sim {\mathcal D}; h \sim Q}[\ell(h, z)],
\end{align}
\begin{align} \label{equ:empiricalpacloss}
L_{\samp}(Q) & = \expectation_{ h \sim Q}\left[\frac1N\sum_{n=1}^{N} \ell(h, z_i)\right],
\end{align}
and the divergence $D(Q \| P)$ is defined as
\begin{align*}
    D(Q \|P) = \int Q \log \frac{Q}{P}.
\end{align*}
\end{lemma}
There is a large body of literature that discusses use cases, interpretations, and extensions of this bound. Let us just mention a few closely related works.

A related prior notion is that of flat minima. These are minimizers in the parameter space that are surrounded by many functions with similarly small empirical error. The reason for this connection is straightforward. In order for $Q$ to give a good bound two properties have to be fullfilled: (i) $Q$ must be fairly broad so that $D(Q \| P)$ is not too large (afterall, $P$ must be broad since we do not know the function a priori), and (ii) $Q$ must give rise to a low expected empirical error. These properties are exactly the characteristics one expects from a flat minimum. The importance of such minima was recognized early on, see e.g., \citet{hintonflatminima} and \citet{schmidhuberflatminima}. More recently \citet{zecchina1} and \citet{zecchina2} derive from this insight an algorithm for training discrete neural networks that explicitly drives the local search towards non-isolated solution. Using a Bayesian approach they argue that these minima have good generalization. Building on these ideas \citet{dziugaite} give an algorithm with the aim to directly optimize \eqref{equ:pacbound}. They demonstrate empirically that the distributions $Q$'s they find give non-vacuous generalization bounds. 

To summarize, the bound \eqref{equ:pacbound} can be used in various ways. In the simplest case, given a prior $P$ and an algorithm that produces a ``posterior'' $Q$, (\ref{equ:pacbound}) gives a probabilistic upper bound on the average true risk if we sample the hypothesis according to $Q$. But (\ref{equ:pacbound}) can also be taken as the starting point of an optimization problem. Given a prior distribution $P$ one can in principle look for the posterior $Q$ that gives the best such bound. Further, one can split the available data and use one part to find a suitable prior $P$ and the remaining part to define a that posterior $Q$ distribution that minimizes this bound.  

We take the PAC-Bayes bound as our starting point. We impose a Gaussian distribution on the weights of the model. This defines our prior $P$. In principle other priors can be used for our approach but a Gaussian is the most natural choice and it illustrates the main point of the paper in the cleanest fashion. Further, we postulate that the samples $z_n=(x_n, y_n)$ are iid and, assuming that the true parameter is $\theta$, come from the stochastic model

\begin{align} \label{equ:datamodel}
x_n \mapsto y_n = f_{\theta}(x_n) + \eta_n, \eta_n \sim \mathcal{N}(0,\sigma_e^2).
\end{align}
In words, we assume that the actual underlying function is {\em realizable}, that we receive noisy samples, and that the noise is Gaussian and independent from sample to sample.  

This gives rise to the posterior distribution,
\begin{align} \label{equ:posterior}
Q(\theta) = \frac{P(\theta) e^{- \frac{1}{2 \sigma_y^2} \sum_{n=1}^{N} (y_n - f_{\theta}(x_n))^2}}{\int P(\theta') e^{- \frac{1}{2 \sigma_y^2} \sum_{n=1}^{N} (y_n - f_{\theta'}(x_n))^2} d \theta'}.
\end{align}
One valid criticism of this approach is that it is model dependent. But there is a significant payoff. First recall that this posterior can at least in principle be sampled by running the SG algorithm with Langevin dynamics. For the convenience of the reader we include in Section~\ref{sec:langevin} a short review. Most importantly, taking this point of view a fairly clear picture arises why neural networks tend not to overfit. In a nutshell, if we sample from this posterior distribution then we are more likely to sample ``simple'' functions. The same framework also shows that this is not the case for linear schemes.
 
\subsection{Stochastic Gradient Langevin Dynamics} \label{sec:langevin}
We follow \citet{marceaucaron2017natural}. Assume that we are given the data set
\begin{align*}
\samp = \{z_1, \cdots, z_N\} = \{(x_1, y_1), \cdots, (x_N, y_N)\},
\end{align*}
where the samples $z_n=(x_n, y_n)$, $n=1, \cdots, N$, are chosen iid according to an unknown distribution $\dist$. We model the relationship between $x$ and $y$ probabilistically in the parametrized form
\begin{align*}
y \sim p(y \mid x, \theta).
\end{align*}
We use the log-loss 
\begin{align*}
\loss_{\theta}(x, y) = - \ln p(y \mid x, \theta).
\end{align*}
Assume further that we use the {\em stochastic gradient Langevin descent} (SGLD) algorithm:
\begin{align*}
  \theta^{(t)} &= \theta^{(t-1)} - \eta \expectation_{Z \sim \dist} \left[\nabla_{\theta} \loss_{\theta}(X, Y) - \frac1N \ln P(\theta) \right] \\
  &+ \sqrt{\frac{2 \eta}{N}} {\mathcal N}(0, I),
\end{align*}
where $t = 1, 2, \cdots$;  $\eta>0$ is the learning rate, $P(\theta)$ is the density of the prior, and ${\mathcal N}(0, I)$ denotes a zero-mean Gaussian vector of dimension $\text{dim}(\theta)$ with iid components and variance $1$ in each component.

%To this corresponds a stochastic gradient Langevin flow (the continuous time version of the descent algorithm) which has the form,
%\begin{align*}
%    xxxxx
%\end{align*}
Note that due to the injected noise, the distribution of $\theta$ at time $\tau$, call it $\pi_{\tau}(\theta)$, converges to the posterior distribution of $\theta$ given the data, i.e., it converges to 
\begin{align*}
&p(\theta \mid \{z_1, \cdots, z_N\}) 
= \frac{p(\theta, \{z_1, \cdots, z_N\})}{p(\{z_1, \cdots, z_N\})} \\
&=
\frac{P(\theta) p(\{z_1, \cdots, z_N\} \mid \theta)}{p(\{z_1, \cdots, z_N\})} \\
 & =\frac{P(\theta) \prod_{n=1}^{N} p(y_n \mid x_n, \theta)}{\prod_{n=1}^{N} p(y_n \mid x_n)} \propto P(\theta) \prod_{n=1}^{N} p(y_n \mid x_n, \theta).
\end{align*}
This is shown in \citet{teh2015consistency,chen2016convergence}. In the sequel we use the more common notation $p_{\theta}(y_n \mid x_n)$ instead of $p(y_n \mid x_n, \theta)$. This makes a clear distinction between the parameters of the model and the samples we received.

A few remarks are in order. An obvious choice from a theoretical point of view is to use an iid Gaussian prior. In practice it is best not to use iid Gaussian prior in order to speed up the convergence. Indeed, the main point of \citet{marceaucaron2017natural} is to discuss suitable schemes. But for our current conceptual purpose we will ignore this (important) practical consideration. 

\section{The PAC Bayes Bound and Bayes Complexity}
Let us now get back to the main point of this paper. We start by defining two notions of complexity. Both of them are ``Bayes'' complexities in the sense that both relate to the size of the parameter space (as measured by a prior) that approximately represents a given function. We will then see how this complexity enters the PAC-Bayes bound.

\paragraph{Contribution.} Our main contribution is an introduction of a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions, and (iii) it explains the generalization gap between neural networks and linear schemes in some regime. While there is a large set of papers which describes each such criterion, and even some that fulfill both (e.g., \citet{srebronormbound}), as far as we know, this is a first notion that satisfies all three of them. 

\begin{definition}[Sharp complexity]\label{def:sharpcomplexity}
For every $\e > 0$ we define the sharp complexity of a function $g$ with respect to the hypothesis class $\mathcal{H}_\theta$ as
\begin{align*}
\chi^\#(\mathcal{H}_\theta, g, \dist_x, \e^2) &:= -\log \left[ \prob_\theta\{ \theta : \expectation_{x \sim \dist_x} [(g(x) - f_{\theta}(x))^2] \leq \e^2 \} \right],   
\end{align*}
where the probability $\prob_\theta$ is taken wrt to the prior $P$.
\end{definition}

In words, we compute the probability, under prior $P$, of all these functions $f_\theta$ that are close to $g$ under the quadratic loss and distribution $\dist_x$.

In general, it is difficult to compute $\chi^\#$ for a given
$\epsilon$.  However, for realizable functions it is often possible to compute
the limiting value of the sharp complexity, properly normalized, when $\epsilon$
tends to $0$.
\begin{definition}[Limiting Complexity]\label{def:limitingcomp}
We define the sharp complexity of a function $g$ with respect to the hypothesis class
\begin{equation} \label{comp-def}
\chi^\#(\mathcal{H}_\theta, g, \dist_x)     := \lim_{\epsilon \rightarrow 0}  \frac{\log \left[\prob_\theta \left\{\theta: \expectation_{x \sim \dist_x} \left[ (g(x) - f_\theta(x))^2 \right] \leq \e^2 \right\}\right]}{\log(\e)}.
\end{equation}
\end{definition}

The above definitions of complexity implicitly depend on the hypothesis class $\mathcal{H}_\theta$. If the hypothesis class (and/or $\dist_x$) is clear from context we will omit it from notation, e.g. $\chi^\#(g, \e^2) = \chi^\#(g, \dist_x, \e^2) = \chi^\#(\mathcal{H}_\theta, g, \dist_x, \e^2)$. 

We now state the main theorem. It is a generalization bound, which crucially depends on the sharp complexity from Definition~\ref{def:sharpcomplexity}. The proof is deferred to Appendix~\ref{apx:generalization}.

\begin{theorem}\label{thm:generalizationbound}
If $L_\dist(P) \geq 2\sigma_e^2$ and $g \in \text{supp}(P)$ then for every $ \beta \in (0,1]$ there exists $\sigma_{\text{alg}}^2$ such that if we set $\sigma_y^2 = \sigma_{\text{alg}}^2$ then $\expectation_{\samp \sim \dist^N}[L_S(Q(\sigma_y^2))] = (1+\beta)\sigma_e^2$ and
\begin{align}
\expectation_{\samp \sim \dist^N}[L_\dist(Q(\sigma_y^2))]
&\leq
\sigma_e^2 + \left[ \beta \sigma_e^2 + \frac{C}{\sqrt{2}}\sqrt{\frac{\chi^\#(g, \dist_x, \beta \sigma_e^2)}{N}} \right]. \label{eq:mainthm}
\end{align}
\end{theorem}

\paragraph{Discussion of Assumptions.}
Requiring that $g \in \text{supp}(P)$ is only natural as it indicates that $g$ is realizable with prior $P$. It is also natural to assume that $L_\dist(P) \geq 2\sigma_e^2$ as the lowest possible error is attained by $g$ and is equal $\sigma_e^2$. Thus we require that the expected loss over the prior is twice as big as the minimal one. As $P$ should cover a general class of functions it is only natural that $L_\dist(P) \geq 2\sigma_e^2$.

For a fixed $\beta$, $\sigma_{\text{alg}}^2$ from Theorem~\ref{thm:generalizationbound} is, in general, not known. However, as proven in Appendix~\ref{apx:generalization}, we have
$$
\lim_{\sigma_y^2 \rightarrow 0} \expectation_{\samp \sim \dist^N}[L_\samp(Q(\sigma_y^2))]= \sigma_e^2, \ \ \lim_{\sigma_y^2 \rightarrow \infty} \expectation_{\samp \sim \dist^N}[L_\samp(Q(\sigma_y^2))]= 2\sigma_e^2.
$$
Moreover, $\expectation_{\samp \sim \dist^N}[L_\samp(Q(\sigma_y^2))]$ is continuous in $\sigma_y^2$, which implies that $\sigma_{\text{alg}}^2$ can be found by a binary search-like procedure by holding out some part of $\samp$ for estimating $\expectation_{\samp \sim \dist^N}[L_\samp(Q(\sigma_y^2))]$ for different $\sigma_y^2$ values.

\paragraph{Bound \eqref{eq:mainthm} in terms of limiting complexity.}  Notice that \eqref{eq:mainthm} is governed by 
$\chi^\#(g,\dist_x,\beta \sigma_e^2)$. Aaccording to \eqref{comp-def}, for small enough $\beta\sigma_e^2$, we have
\begin{equation}\label{eq:relationbetweencomp}
\chi^\#(g,\dist_x,\beta \sigma_e^2) \approx -\chi^\#(g,\dist_x)\log(\beta \sigma_e^2).
\end{equation}
This means that for small enough noise level, where the exact regime for which the approximation holds depends on a specific problem, we have
\begin{equation}\label{eq:generalizationlimit}
\expectation_{\samp \sim \dist^N}[L_\dist(Q(\sigma_y^2))]
\lessapprox
(1 + \beta) \sigma_e^2 + \frac{C}{\sqrt{2}}\sqrt{\frac{-\chi^\#(g, \dist_x)\log(\beta \sigma_e^2)}{N} }.    
\end{equation}
We see that the generalization bound depends crucially on the limiting complexity. 
%But it is important to note that the bound holds only if $\beta \sigma_e^2$ is sufficiently small.

\paragraph{Main message.} 
Note that the smallest we can hope to get on the right hand side is $\sigma_e^2$ since this is the variance of the noise and this is achievable if we pick $Q$ that puts all its mass on $g$.
This means that $\beta \sigma_e^2$ plus the square root term from \eqref{eq:mainthm} represents the expected excess generalization error. 

This brings us to the punch line of this paper. In the subsequent sections we will see that
(i) natural notions of complexity that have previously been discussed in the literature align with our new notion when we consider neural networks, whereas 
(ii) for linear schemes our notion of complexity is essentially independent of the function (as long as it is realizable) and as a consequence is as high as for the most complex (in the natural sense) function in our hypothesis class.

To the degree that we assume that reality prefers simple functions this explains why neural nets generalize significantly better than linear schemes.

In Section~\ref{sec:complexityGaussianCase} we show that for neural networks and a piece-wise linear function $g$ the limiting complexity is equal to the number of slope changes $g$. In light of \eqref{eq:generalizationlimit}, this means that neural networks require the fewer samples (for a good generalization bound) the fewer slope changes $g$ has.

There is a further connection to a natural notion of complexity. In Section~\ref{sec:epscompcase} we show that sharp complexity is related to the variation of $g$, i.e. the integral of the second derivative of $g$. Thus, in the light of \eqref{eq:mainthm}, fewer samples are needed (for a good generalization) for $g$'s with smaller variation.

As we discussed above, sharp and limiting complexity are related via \eqref{eq:relationbetweencomp} when $\beta \sigma_e^2$ is small. We can thus think of sharp complexity as a refinement of limiting complexity. This is reflected in the two cases discussed above -- the number of slope changes can be seen as an \say{approximation} of the variation of a function.

In Section~\ref{sec:linear}, on the other hand, we show that for linear schemes the limiting complexity is virtually independent of the function and \say{equal} to the number of basis functions. This means that in this case the number of samples needed for a good generalization bound is the same for simple and complicated functions.


%The interplay of all the parameters might be mysterious at this point. We give an intuition of how Theorem~\ref{thm:generalizationbound} will be used later on. We will set $\e^2 = (1 + 1/r(N)) \sigma_e^2$, where $r$ will often be a polynomial. With that choice Theorem~\ref{thm:generalizationbound} becomes
%\begin{equation}\label{eq:important}
%\sigma_e^2 + \sqrt{\frac{\chi^\#(g, \dist_x, \sigma_e^2 / r(N))}{N}  + \frac{\sigma_e^2}{2\sigma^2_y \cdot r(N)} + \frac{\ln(N)}{\delta N}}   
%\end{equation}
%If $\frac{\sigma_e^2}{\sigma_y^2} = O\left(\frac{r(N)}{N}\right)$ then the bound will be dominated by the term involving $\chi^\#(g, \dist_x, \sigma_e^2 / r(N))$.

%This can be interpreted as follows. In expectation over the draw of $S$ the gap between the empirical error $L_\samp(Q)$ and the true error $L_\dist(Q)$ is bounded by the square root term from \eqref{eq:generalizationbound}. Note that $\chi^\#(g,\dist_x,\e)$ is decreasing with $\e$. 
%This implies that to get the best bound we should choose $\e = \Omega \left(\frac{\sigma_y^4}{N} \right)$ as otherwise $\frac{\sigma_y^2}{N}$ term dominates $\frac{\e}{2\sigma_y^2}$ and the $\chi^\#$ term only increases. 

\section{Models}

Although the basic idea applies to any parametric family, we will consider restricted types of families and demonstrate our concepts with two concrete examples, namely linear schemes and NNs. We will be interested in parametric families of functions from $\inspace$ to $\outspace$. More precisely families of the form $\mathcal{H}_{\theta} := \left\{\pred_{\theta} : \inspace \xrightarrow{} \outspace, \theta \in \R^m \right\}$,
where $\theta$ is the vector of parameters. for a function $g : \inspace \rightarrow \outspace$ and a distribution $\dist_x$ we define the set of exact representations as $A_{g,\mathcal{H},\dist_x} := \{\theta \in \R^m : f_\theta \equiv_{\supp(\dist_x)} g \}$. If $\mathcal{H}$ and $\dist_x$ are clear from context we will often write $A_g$. The $0$ function will play an important role, thus we also define $A_0 := \{\theta \in \R^m : f_\theta \equiv_{\supp(\dist_x)} 0 \}$

\subsection{Linear Schemes} \label{sec:linearmodel}
Consider the linear family $\model_\theta^{(\text{L, o})}=\{f_{\theta}(x): f_{\theta}(x) = \sum_{i=0}^{d-1} \w_i b_i(x), x \in \inspace = [-1, 1]\}$,
i.e., the vector of parameters $\theta$ is equal to the vector of weights $\w$. We assume that the functions $\{b_i(x)\}$ form an orthonormal basis.
Although the exact basis that is used is not of importance one might think of $b_i(x)$ as a polynomial of degree $i$ or the first few Legendre polynomials. In this way the basis functions are naturally ordered by complexity. 


\subsection{Neural Networks} \label{sec:nn}
Consider the family $\model^{\text{NN}}$ represented by NNs with layers numbered from $0$ (input) to $K$ (output), containing $d = d_0, d_1, \dots$, and $d_K = d_y$ neurons respectively. We will limit our attention to $d_y = 1$. The activation functions for the layers $1$ to $K$ are presumed to be $\sigma_1, \dots, \sigma_K : \R \xrightarrow{} \R$. The weight matrices will be denoted by $W^{(1)}, W^{(2)}, \dots, W^{(K)}$, respectively, where matrix  $W^{(k)}$ connects layer $k-1$ to layer $k$. We define 
\begin{equation}\label{eq:defoff}
\pred_{\theta}(x) := \sigma_K (\bias^{(K)} + W^{(K)} \sigma_{K-1}( \dots \sigma_{1}(\bias^{(1)} + W^{(1)} x ))) .
\end{equation}
%We are interested in $\pred_{\theta}$ for $\x \in \inspace$, where $\inspace$ might e.g. be a spherical region.
%The dimension of $\w$ is $m = \sum_{i=0}^{K-1} d_i (d_{i+1} + 1)$. 


\section{Why Neural Nets Generalize Well}\label{sec:nngeneralizewell}
We now get to the main point of this paper, namely why neural nets generalize much better than other schemes, in particular linear schemes.

The basic idea is simple. We have seen in the previous sections that (i) a suitable version of SGD gives us a posterior of the form (\ref{equ:posterior}), and (ii) this posterior gives rise to a an upper bound on the generalization error that depends mainly on the ``complexity'' of the underlying true hypothesis.

This notion of complexity of a function depends on the underlying hypothesis class.
To close the circle we will now discuss how this complexity behaves for interesting hypothesis classes. In particular, as we will see that there is a striking difference between linear schemes and neural networks. For linear schemes, every realizable function has essentially the same complexity. This in particular means that we do not expect to learn a ``simple'' function (e.g., think of a constant function) with fewer samples than a ``complex'' one (think of a highly variable one). For neural nets the complexity behaves entirely differently and there is a large dynamic range. As we will see, in a suitable limit the complexity is to first order determined by the number of degrees of freedom that have to be fixed in order to realize a function. Therefore, for neural nets, simple functions have a much lower complexity than complicated ones. 

\subsection{Neural Networks with a Single Hidden Layer}

We start with analyzing our notion of complexity for the case of NN with a single hidden layer and $1$-dimensional input. More precisely let $x \in \R$ denote the input and $y \in \R$ denote the output.  There are $k$ nodes in the hidden layer. More precisely, the network represents the function
 \begin{align*}
f_{\theta}(x) 
& = \sum_{i=1}^k \w_i^{(2)} \sigma \left( \w_i^{(1)} x + \bias_i^{(1)}\right)  + b^{(2)} \\
& = \sum_{i=1}^k \w_i^{(2)} \left[ \w_i^{(1)} x + \bias_i^{(1)} \right]_+ + b^{(2)},
\end{align*}
i.e., we use ReLU activation functions. 
%Here, $W^{(1)}$ is a $k \times d$ matrix, where the $i$-th row, denoted by $\w_i^{(1)}$, connects the $d$ inputs to the $i$-th node. 
The $\bias_i^{(1)}$ denotes the bias of the $i$-th node, the $\w_i^{(2)}$ represents the weight of the $i$-th output signal, and $b^{(2)}$ is the global bias term of the output. We let $\theta = (\theta_w, \theta_b) = ((\w^{(1)}, \w^{(2)}), (\bias^{(1)}, b^{(2))})$ denote the set of all parameters, where $\theta_w$ denotes the set of weights and $\theta_b$ denotes the set of bias terms.

\paragraph{Parametrization and prior.} We will use the following non-standard parametrization of the network
\begin{align*}
f_\theta(x) 
&= \sum_{i=1}^k \w_i^{(2)} \left[\w_i^{(1)}\left(x - \bias_i^{(1)}\right) \right]_+ + b^{(2)} \\
&= \sum_{i=1}^k \w_i^{(2)} \cdot \left|\w_i^{(1)}\right| \cdot \left[\text{sgn}(\w_i^{(1)})( x - \bias_i^{(1)})\right]_+ + b^{(2)},
\end{align*}
where in the last equality we used the fact that the ReLU activation function is 1-homogenous. Note that there are two kinds of ReLU functions (depending on the sign of $w_i^{(1)}$) they are either of the form $[x -b]_+$ or $0$ at $[-(x-b)]_+$. If we restrict our attention to how $f_\theta$ behaves on a compact interval then considering just one of the kinds gives us the same expressive power as having both. This is why for the rest of this section we restrict our attention only to the case of $[x-b]_+$ as it simplifies the proofs considerably. Thus the final parametrization we consider is
$$
f_\theta(x) = \sum_{i=1}^k \w_i^{(2)} \cdot \w_i^{(1)}\left[x - \bias_i^{(1)}\right]_+ + b^{(2)}.
$$

We define the prior on $\theta$ as follows: each component of $\theta_w$ comes i.i.d. from $\mathcal{N}(0,\sigma_y^2)$, each component of $\bias^{(1)}$ comes i.i.d. from $U([0,M])$, where $M$ will be fixed later and $b^{(2)}$ comes i.i.d. from $\mathcal{N}(0,\sigma_b^2)$\footnote{The different parametrization and the uniform prior on the bias terms are non-standard choices that we make to simplify the proofs. These choices would not affect the spirit of our results but as always the details need to be verified.}.

We will argue that our notion of complexity ($\chi^\#(\dist_x,g,\e^2)$ 
and $\chi^\#(\dist_x,g)$) corresponds, in a case of NN, to natural notions of complexity of functions. %We will present our results for the case of the quadratic loss and the hypothesis class is fixed to the one described above thus we will omit $(\model_{\theta}, \dist_\xv, \ell)$ \comm{GG}{What about the distribution. it's not really omitted at this point}.

%Then we know that we have to more or less "fix" $c$ components and the other ones are more or less free. Hence, if we look at the limit of the complexity as $\sigma^2$ tends to $0$ the complexity scales as
%$c \ln \left(\frac1{\sqrt{2 \pi \sigma^2})}\right)$.

\paragraph{Target function.} We will be interested in target functions $g$ that are representable with a single hidden layer networks. Let $g : [0, 1] \xrightarrow{} \R$ be continuous and piece-wise linear. I.e., there is a sequence of points $0=t_1 < t_2 < \cdots < t_{l+1}=1$ so that for $x \in [t_i, t_{i+1}]$, $1 \leq i < l+1$,
\begin{align} \label{equ:polygone}
g(x) = c_i + \alpha_i (x-t_i),
\end{align}
for some constants $c_i$ and $\alpha_i$, where $c_{i+1} = c_i + \alpha_i (x_{i+1}-x_i)$. Then $f$ can be written as a sum of ReLU functions, 
\begin{align} \label{equ:firstrepresentation}
g(x) = b + \sum_{i=1}^{l} v_i [x-t_i]_+,
\end{align}
where $v_1=\alpha_1$ and $v_{i}=\alpha_{i}-\alpha_{i-1}$, $i=2, \cdots, l$. The terms in \eqref{equ:firstrepresentation} for which $v_i = 0$ can be dropped without changing the function. We call the number of nonzero $v_i$'s in \eqref{equ:firstrepresentation} to be the number of changes of slope of $g$. 

\begin{comment}
\begin{enumerate}
\item Start with the fundamental questions of generalization and review various papers.
\item Say what the overall idea is, namely NNs with regularizer minimizes the sum of the  complexity of the function plus loss.
    \item Start with Srebo paper that says that for a one-hidden NN minimizing the loss plus the square of the norm is equal to finding a hypothesis so that the sum of the loss plus complexity are minimized.
    
    \item We show that, suitably generalized, a similar picture emerges for the "general" case.
    \begin{enumerate}
        \item We consider a general network.
        \item We consider the SGLD.
        \item We impose a Gaussian prior on the weights.
    \end{enumerate}
    \item We then show that the general measure of complexity is given by the "Bayesian" complexity of a function (need a better word). I.e., in general, the samples we get from the SGLD are such that they minimize the sum of two exponents, one coming from the approximation error and one from the complexity measure.
    \item The multiplicity complexity measure is naturally connected to several other perhaps more intuitive complexity measures. E.g., the initial scheme is one example but it would be nice to find at least one other example (perhaps the square functions)
    \item We show that if we apply the same framework to linear schemes the complexity measure does not behave in the same way, giving a strong indication why overfitting does not happen to the same degree for NNs.
    \item We show what happens if we add layers to a network.
    \item We explore the role of dropout (not so sure if we can do this; what does this mean for the dynamics?)
\end{enumerate}
\end{comment}

\subsubsection{Complexity in the Asymptotic Case} \label{sec:complexityGaussianCase}
In this section we explore what is the limiting value of the sharp complexity for the case of NN.

\begin{comment}
\begin{lemma}[Asymptotic Scaling]\label{lem:scaling}
Assume that we are given a Gaussian vector $\Wv$ of length $\famdim$,
with mean $\muv$, and with covariance matrix $\Kv$ that has full rank.  Let $\wv \in
\reals^\famdim$.  Let $\Wv_{1, c}$ and $\wv_{1, c}$ denote the
restrictions of $\Wv$ and $\wv$ to the first $c$ components and
let $\Wv_{c+1, \famdim}$ denote the restriction of $\Wv$ to the
last $\famdim-c$ components. Finally, let $R \subseteq \reals^{\famdim-c}$
be a set of strictly positive Lebesgue measure.  Then
\begin{align*}
\lim_{\epsilon \rightarrow 0} \frac{\log[\{\prob\{\Wv: \Wv_{c+1, \famdim} \in R \wedge \|\Wv_{1, c}-\wv_{1, c}\|_2 \leq \epsilon\}]}{\log(\epsilon)} = c.
\end{align*}
\end{lemma}
Before we proceed to the proof let us quickly discuss how we will apply this observation. Assume that we can represent a given function $g(x)$ exactly within a model $\model_\theta$ by fixing $c$ of the components to a definite value and that the remaining $\famdim-c$ components
can be chosen within a range that does not depend on $\epsilon$. This is e.g. the case for neural networks. Due to the non-linearity some parameters can range freely without changing the function. Assume further, that the model has a finite derivative with respect to each of the $c$ fixed values.  For Gaussian prior we have by Lemma~\ref{lem:scaling} that the complexity of this function is $c$. In the above discussion we implicitly assumed that the function has a unique representation. But, as we will discuss in Section~\ref{sec:epscompcase} and in in the appendix, in general, realizable functions do have many representations. Besides the discrete symmetries inherent in many models there are also continuous symmetries that often arise. E.g., the output of a single ReLU can be exactly replicated by the sum of several ReLU functions. Nevertheless, even though the actual probability for a fixed $\epsilon$ can be significantly larger due to this multiplicity, the asymptotic limit remains the same \com{Is it clear?}.
\begin{proof}[Proof of Lemma~\ref{lem:scaling}]
Let us start by assuming that the Gaussian distribution has iid components. In this case the probability factors into the probability that the last $k-c$ components are contained in the region $R$, which by assumption is a strictly positive number, independent of $\epsilon$ and the probability that the first $c$ components are contained in a ball of radius $\epsilon$ around a fixed point. Note that this second probability behaves like $\kappa \epsilon^c$, where $\kappa$ is strictly positive and does not depend on $\epsilon$. The result follows by taking the log, dividing by $\log(\epsilon)$ and letting $\epsilon$ tend to $0$. 

The general case is similar. Write 
\begin{align*}
& \prob\{\Wv: \Wv_{c+1, \famdim} \in R \wedge \|\Wv_{1, c}-\wv_{1, c}\|_2 \leq \epsilon\} \\
&= \int_{\wv_{c+1, k} \in R} \int_{\|\wv_{1, c}-\wv_{1, c}^*\| \leq \epsilon} f(\wv_{1, c}, \wv_{c+1, k}) \\
& = \int_{\wv_{c+1, k} \in R} f(\wv_{c+1, k}) \int_{\|\wv_{1, c}-\wv_{1, c}^*\| \leq \epsilon}  f(\wv_{1, c} \mid \wv_{c+1, k}).
\end{align*}
Now note that each value of $\wv_{c+1, k}$ the inner integral scales like $\epsilon^c$, and hence this is also true once we integrate over all values of $\wv_{c+1, k}$.
\end{proof}
\end{comment}

\begin{comment}
\begin{example}[Function with $c$ changes of slope]\label{exp:chchangesofslope}
Imagine that $d=1$, that is $g : \R \rightarrow \R$ and assume that $g$ is a piece-wise linear function with $c$ changes of slope. We can represent this function by fixing $c$ degrees freedom to definite values. For instance we can choose $c$ nodes in the hidden layer and represent one change of slope with each of these neurons. If $\supp(\dist_x)$ contains all $x$'s for which the changes of slope occur then Lemma~\ref{lem:scaling} guarantees that $\chi^\#(\mathcal{D}_x,g) = c$ \com{It's not super clear to me. What about for instance the fact that we can distribute the change of slope as $\sqrt{a}, \sqrt{a}$ and $a^{1/3}, a^{2/3}$?}. Plugging this result in \eqref{eq:generalizationbound} we get that for small $\e$ the true versus empirical loss gap behaves as
\begin{align*}
&\approx \sqrt{\frac{c \log(1/\e)}{N} + \frac{\sigma_y^2}{N} + \frac{\e}{2\sigma^2_y} + \frac{\ln(N)}{\delta N}}.
\end{align*}
We see that in this case the generalization bound strongly depends on the complexity of $g$, which in this case is the number of changes of slope.
\end{example}
\end{comment}

It will turn out that the key object useful for computing $\chi^\#(g)$ is a particular notion of dimension of $A_g$. 
\begin{definition}[Minkowski–Bouligand co-dimension]
For $A, S \subseteq \R^m$ we define the Minkowski-Bouligand co-dimension of $A$ w.r.t. $S$ as
$$\codim_S(A) := \lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log(\vol( (A + B_\e) \cap B_R \cap S))}{\log(\e)} , $$
where $\vol$ is the Lebesgue measure and $+$ denotes the Minkowski sum. 
\end{definition}

\begin{note}
Our definition is a variation of the standard Minkowski-Bouligand dimension. The first difference is that we measure the co-dimension instead of the dimension. Secondly,  we compute $\lim_{R \rightarrow \infty}$. We do this because the sets we will be interested in are unbounded. We also define the co-dimension wrt to an auxilary set $S$, i.e., all volumes are computed only inside of $S$. One can view it as restricting the attention to a particular region. In our use cases this region will be equal to the support of the prior. We will sometimes use $\codim_P(A)$ to denote $\codim_{\supp(P)}(A)$, when $P$ is a distribution.

Technically the notion is not well defined for all sets. Formally, one defines a lower and an upper co-dimension, corresponding to taking $\liminf$ and $\limsup$. Sets $A$ and $S$ need also be measurable wrt to the Lebesgue measure. We will however assume that for all of our applications the limits are equal, sets are measurable and thus the co-dimension is well defined. This is the case because all sets we will be interested in are defined by polynomial equations.
\end{note}

The first lemma relates sharp complexity and co-dimension.

\begin{lemma}\label{lem:complexitytodimension}
Let $g(x) = b + \sum_{i=1}^c v_i [x - t_i]_+,$
where $0 < t_1 < \dots < t_c < 1$, $v_1,\dots,v_c \neq 0$ and $ c \leq k$. Then
$$
\frac1{5} \codim_{P}(A_g) \leq \chi^\#(g, U([0,1])) \leq \codim_{P}(A_g).
$$
Recall that $A_g = \{ \theta : f_\theta \equiv_{[0,1]} g\}$.
\end{lemma}

The next lemma computes the co-dimension of a function with $c$ changes of slope.

\begin{lemma}[Function with $c$ changes of slope - co-dimension]\label{lem:cchangesco-dimension}
Let $g(x) = b + \sum_{i=1}^c v_i [x - t_i]_+,$
where $0 < t_1 < \dots < t_c < 1$, $v_1,\dots,v_c \neq 0$ and $ c \leq k$. Then
$$
\codim_P(A_g) = 2c+1.
$$
\end{lemma}

\begin{comment}
\begin{lemma}
There exists a universal constant $C$ such that for all \com{In the general case there's also $b^{(2)}$ but I guess it'll work.} $
f_{\theta_0}(x) = \sum_{i=1}^k \w_i\left[x - \bias_i^{(1)}\right]_+,
$ such that $\| f_{\theta_0}\|_2^2 = \e^2$, there exists $\theta_1$ such that $f_{\theta_1} \equiv_{[0,1]} 0$ and $\|\theta_0 - \theta_1\|_2^2 \leq O(\e^C)$.
\end{lemma}

\begin{proof}
Let $L(\theta) := \|f_{\theta}\|^2$. Consider the following differential equation
\begin{equation}\label{eq:differentialfortheta}
\dot{\theta} = - \frac{\nabla L(\theta)}{\|\nabla L(\theta)||_2},
\end{equation}
which can be understood as a normalized gradient flow. By definition
\begin{equation}\label{eq:differentialforL}
\dot{L} = (\nabla L(\theta))^T \dot{\theta} \stackrel{(\ref{eq:differentialfortheta})}{=} - \|\nabla L(\theta)\|_2.
\end{equation}
We will later show that 
\begin{equation}\label{eq:goal}
\|\nabla L(\theta)\|_2 \geq L^{0.8}. 
\end{equation}
Note that the solution to $\dot{L} = - L^{0.8}$ is of the form $L(t) = c (C - t)^{0.8}$. More precisely, with the initial condition $L(0) = \e^2$ we get that $C = \left(\frac{\e^2}{c} \right)^\frac{1}{4/5}$. What follows is that $L \left(\left(\frac{\e^2}{c} \right)^{5/4} \right) = 0$. Using \eqref{eq:goal} we get that there exists $t^* < \left(\frac{\e^2}{c} \right)^{5/4}$ such that $L(t^*) = 0$. Because the change of $\theta$ is normalized (see \eqref{eq:differentialfortheta}) we get that $\|\theta(0) - \theta(t^*)\|_2^2 \leq \left(\frac{\e^2}{c} \right)^{4/5} = O \left(\e^{\frac{4}{5/4}} \right)$. What is left is to show \eqref{eq:goal}.

We start by computing derivatives of $L$ wrt to $\theta$. For every $i \in [1,k]$
\begin{equation}\label{eq:dfdb}
\frac{\partial L}{\partial \bias_i^{(1)}} = \w_i \int_{\bias_i^{(1)}}^1 f_\theta(x) \ dx.
\end{equation}
\begin{equation}\label{eq:dfdw}
\frac{\partial L}{\partial \w_i} = \int_{\bias_i^{(1)}}^1 f_\theta(x)(x - \bias_i^{(1)}) \ dx.
\end{equation}
We will show that there exists $i \in [1,k]$ such that $\max\{\left|\frac{\partial L}{\partial \bias_i^{(1)}}\right|,\left|\frac{\partial L}{\partial \w_i}\right|\}$ is large. 

For a function $f : [0,1] \rightarrow \R, f(0) = 0, f'(0) = 0$ (that one should understand as an abstraction of $f_\theta$) consider the following expression (related to \eqref{eq:dfdb})
\begin{equation}\label{eq:secondderivative}
f''(y) \int_a^1 f(y) \ dx.
\end{equation}
The following computation will be helpful
\begin{align}
\alpha(a,b) &:= \int_a^b f''(y) \int_y^1 f(x) \ dx \ dy \nonumber \\
&= \left[f'(y) \int_y^1 f(x) \ dx \right]_a^b - \int_a^b f'(y) \cdot (- f(y)) \ dy && \text{by parts} \nonumber \\
&= f'(b)\int_b^1 f(x) \ dx - f'(a) \int_a^1 f(x) \ dx + \left[\frac12 f^2(x) \right]_a^b \nonumber  \\
&= \frac{f^2(b)}{2} + f'(b)\int_b^1 f(x) \ dx - \frac{f^2(a)}{2} - f'(a) \int_a^1 f(x). \label{eq:alphaaverage}
\end{align}
Now note that
\begin{align}
\alpha(0,b) 
&= \frac{f^2(b)}{2} + f'(b)\int_b^1 f(x) \ dx - \frac{f^2(0)}{2} - f'(0) \int_0^1 f(x) \nonumber \\    
&= \frac{f^2(b)}{2} + f'(b)\int_b^1 f(x) \ dx && \text{As } f'(0) = f(0) = 0. \label{eq:alphaaverage2} 
\end{align}
Let $M := \max_{x \in [0,1]} |f(x)|$ and $x^* \in f^{-1}({M})$. We claim that 
\begin{equation}\label{eq:maxaverage}
\alpha(0,x^*) = \frac{M^2}{2}.
\end{equation}
To see that use \eqref{eq:alphaaverage2} and note that either $x^* \in [0,1]$ and then $f'(x^*) = 0$ because it is an extremal point, or $x^* = 0$ and then $f'(0)=$ by definition, or $x^* = 1$ and then $\int_1^1 f(x) \ dx = 0$. Using \eqref{eq:maxaverage} and the definition of $\alpha$ we get that there exists $x_0 \in [0,x^*]$ such that
\begin{equation}\label{eq:highvaluesecond}
\left|f''(x_0) \int_{x_0}^1 f(x) \ dx \right| \geq \frac{M^2}{2 x^*} \geq \frac{M^2}{2}.
\end{equation}
Now note that $f_\theta$ satisfies $f_\theta(0) = 0$. It might not be true that $f'_\theta(0) = 0$ but if we increase all the bias terms by a negligible amount then $f'_\theta(0) = 0$ and the quantity of interest \eqref{eq:secondderivative} changes only negligibly \com{I guess it's true}. Moreover observe that for every $i \in [1,k]$ $f''_\theta(\bias_i^{(1)}) = \sum_{j : \bias_j^{(1)} = \bias_i^{(1)}} \w_j$ and for all $x \in [0,1] \setminus \{\bias_1^{(1)}, \dots, \bias_k^{(1)}\}$ we have $f''_\theta(x) = 0$. As the number of nodes is $k$ we get from \eqref{eq:highvaluesecond} that there exists $i \in [1,k]$ such that
$$
\left|\w_i \int_{\bias_i^{(1)}}^1 f(x) \ dx \right| \geq \frac{M^2}{2 k^2}. 
$$
If $M \geq \e^{0.9}$ then 
$$\left|\frac{\partial L}{\partial \bias_i^{(1)}} \right| \geq \frac{\e^{1.8}}{2 k^2} \geq \frac{1}{2k^2} \left(\e^2 \right)^{0.9} \geq \frac{1}{2k^2} L(\theta)^{0.9},$$
which implies \eqref{eq:goal} and ends the proof in this case. Thus we can assume for the rest of the proof that $M < \e^{0.9}$.

By Holder's inequality we have 
\begin{equation}\label{eq:l1lwrbnd}
\|f_\theta\|_1 \geq \|f_\theta\|_2^2 / \|f_\theta\|_\infty \geq \e^2 / \e^{0.9} = \e^{1.1}.
\end{equation}
Let $0 = a_1 \leq a_2 \leq \dots \leq a_{k+2} = 1$ be the ordering of $\{b_1^{(1)}, \dots, b_k^{(1)}\} \cup \{0,1\}$. Consider a generalization of \eqref{eq:dfdw}
$$
\int_a^1 f(x) (x - a) \ dx.
$$
Let $I(a) := \int_a^1 f_\theta(x) \ dx$. Note that
\begin{equation}\label{eq:derivativeexpression}
\frac{d}{d a} \int_a^1 f(x) (x - a) \ dx = \int_a^1 f(x) = I(a). 
\end{equation}
Let $i \in [1,k]$ be such that it satisfies
\begin{enumerate}
    \item $\int_{a_i}^{a_{i+2}} |f_\theta(x)| \ind_{\{ \text{sgn}(f_\theta(x)) = \text{sgn}(f_\theta(a_{i+1})) \}} \ dx \geq \e^{1.1}/k$, \label{eq:firstproperty}
    \item $\int_{a_i}^{a_{i+2}} |f_\theta(x)| \ind_{\{ \text{sgn}(f_\theta(x)) = \text{sgn}(f_\theta(a_{i+1})) \}} \ dx \geq \int_{a_i}^{a_{i+2}} |f_\theta(x)| \ind_{\{ \text{sgn}(f_\theta(x)) \not= \text{sgn}(f_\theta(a_{i+1})) \}} \ dx $. \label{eq:secondproperty}
\end{enumerate} 
Such an $i$ exists because of $\eqref{eq:l1lwrbnd}$ and the fact that $f_\theta$ crosses $0$ at most $k$ times \com{Is it enough of a proof?}. Assume without loss of generality that $f_\theta(a_{i+1}) > 0$. By definition $f_\theta$ is two-piece linear on $[a_i, a_{i+2}]$, because of that and the assumption that $f_\theta(a_{i+1}) > 0$ we know that $\int_a^1 f_\theta(x)$ first increases, then decreases and finally increases (the first and the third phase might not happen). By \eqref{eq:secondproperty} we know that $I(a_i) \geq I(a_{i+2})$. Let $a_{max} := \argmax_a I(a), a_{min} := \argmin_a I(a)$. By \eqref{eq:firstproperty} we know that $ I(a_{max}) - I(a_{min}) > \e^{1.1}/k$. Consider two cases:
\paragraph{Case 1: $I(a_{max}) \geq \frac{I(a_{max}) - I(a_{min})}{2}$.}

\paragraph{Case 2: $I(a_{max}) < \frac{I(a_{max}) - I(a_{min})}{2}$.}


\com{We need a bound on weights!!! Or do wee}
\end{proof}
\end{comment}

This brings us to the main result of this subsection

\begin{example}[Function with $c$ changes of slope - Bayes Complexity]\label{lem:cchangeslimit}
Let $g : [0,1] \rightarrow \R$ and assume that $g$ is a piece-wise linear function with $c \leq k$ changes of slope. Then
$$
\frac{2c+1}{5} \leq \chi^\#(g, U([0,1])) \leq 2c+1.
$$
\end{example}

We see that the limiting complexity is $\approx c$, for $ c \leq k$. This means that the complexity depends strongly on the function and simpler - in a sense of fewer changes of slopes - functions have smaller complexity. In Section~\ref{sec:linearmodel} we will compute the limiting complexity for linear models. It will turn out, see Example~\ref{ex:linearlimit}, that in this case the complexity doesn't depend on the function and is equal to the number of basis functions used in the linear model.

\subsubsection{The $\e$-Complexity Case}\label{sec:epscompcase}

We saw in the previous section that for the case of neural networks our notion of complexity corresponds (in the limit and up to constant factors) to the number of degrees of freedom that need to be fixed to represent a given function.
When we evaluate the complexity at more refined scales it can be shown that it is closely related to another natural complexity measure. 

\begin{comment}
\begin{example}[Function with $\int |g''(x)| dx = a$] Let
\[
C(g) = \max \left(\int |g''(x)| dx, |g'(-\infty) + g'(+\infty)| \right).
\]
In \citet{srebronormbound} it was shown that, for the case of a single hidden layer NN with 1-D input, for every $g : \R \rightarrow \R$ if we let the width of the network go to infinity \com{Is $\theta_w$ defined?} then
\[
\min_{\theta : f_\theta = g} \|\theta_w\|^2 = C(g).
\]
This means that if we use an $\ell_2$ regularizer for training a neural network
\[
\theta^* = \argmin_\theta \left( L_S(f_\theta) + \lambda\|\theta_w\|^2 \right),
\]
then $C(f_{\theta^*}) = \|\theta^*_w\|^2$. In words, the function that is found via this scheme balances the empirical error and $C(g)$.

In the appendix we show that in some regimes $C(g) \approx \chi^\#(\dist_x, g, \e)$. Plugging it in \eqref{eq:generalizationbound} we get that the expected true versus empirical loss gap is
\begin{align*}
&\approx \sqrt{O_{\sigma_w^2,\e}\left(\frac{C(g)}{N}\right) + \frac{\sigma_y^2}{N} + \frac{\e}{2\sigma^2_y} + \frac{\ln(N)}{\delta N}},
\end{align*}
where $O_{\sigma_w^2,\e}$ drops terms dependent on $\sigma_w^2,\e$. See the appendix for details. We see that the gap crucially relies on $C(g)$. This result can be seen as a quantitative version of Example~\ref{exp:chchangesofslope} as $\int |g''(x)| dx$ can be seen as a more refined version of the number of changes of slope.

\end{example}
\end{comment}

\input{ecomplexity}

\subsection{Neural Networks with Several Hidden Layers} \label{sec:severalhiddenlayer}
Consider now exactly the same set-up as before, except that now we have $K=4$, i.e., we have three hidden layers and still $d = 1$. We can still represent piece-wise linear functions (e.g., by using the first layer to represent the function and just a single node in the second layer to sum the output of the previous layers). But the asymptotic complexity of some functions is now different! 

\begin{example}[Periodic function]
Imagine that we want to represent a function $g : [0,l] \rightarrow \R$ that is periodic with period $1$. That is $g(x - 1) = g(x)$ for all $x \in [1,l]$. What we can do is to (i) represent $g|_{[0,1]}$ in the output of a single neuron $v$ in layer $2$ (ii) represent shifted versions of $g|_{[0,1]}$ (which are equal to $g|_{[1,2]}, g|_{[2,3]}, \dots$ due to periodicity) in the outputs of neurons in layer $3$ (iii) sum outputs of neurons from layer $3$ in the single output neuron in layer $4$. Assume moreover that $g|_{[0,1]}$ has $m$ changes of slope. Then observe that we implemented $g$ fixing $O(l+m)$ degrees of freedom. But $g$ itself has $m \cdot l$ changes of slope over the whole domain. 

This representation gives an upper-bound for limiting complexity as there might be other ways to represent the function. 
%It is however tempting to conjecture that this representation gives rise to the true complexity. 
But because of Example~\ref{exm:onechange} it is enough to arrive at a separation. Indeed if $l \approx m$ then the asymptotic complexity of $g$ for NN with $4$ layers is smaller than for $2$ layers, which is in $\Omega(m l)$. In words, we obtain a quadratic gain in terms of the number of samples needed to get the same generalization bound. 



We leave it for future work to explore this direction in more depth (no pun intended).
\end{example}
 

\section{Why Linear Schemes Generalize Poorly}\label{sec:linear}

In Section~\ref{sec:nngeneralizewell} we've seen that for NNs our notion of complexity aligns well with natural notions of complexity. This, in the light of the connections to the PAC-Bayes bound, partly explains their good generalization. In this section we will show that for the case of linear schemes the complexity is basically independent of a function.  

We investigate  \label{sec:linearmodel}
the linear model $\model_\theta^{(\text{L, o})}=\{f_{\theta}(x): f_{\theta}(x) = \sum_{i=0}^{d-1} \w_i b_i(x), x \in \inspace = [-1, 1]\}$. Further let $\dist_x$ be the uniform distribution on $[-1, 1]$. We assume a prior on $\w_i$'s to be iid Gaussians of mean $0$ and variance $\sigma_w^2$.

We will see that in this setting all realizable functions have the same complexity. This in the light of \eqref{eq:generalizationbound} tells us that even if reality prefers simple functions the number of samples needed to get a non vacuous bound is as big as the one needed for the highest complexity function in the class.  In short: it is equally ``easy'' to represent a ``complicated'' function as it is to represent a ``simple'' one. Therefore, given some samples, there is no reason to expect that linear models will fit a simple function to the data. Indeed, to the contrary. If the data is noisy, then linear models will tend to overfit this noise.

\subsection{Orthonormal Basis} \label{sec:Lo}

For simplicity assume that the basis functions are the Legendre polynomials. I.e., we start with the polynomials $\{1, x, x^2, ...\}$ and then create from this an orthonormal set on $[-1, 1]$ via the Gram-Schmidt process.

\begin{example}[Constant Function]
Let $g(x)=\frac{1}{\sqrt{2}}$. This function is realizable. Indeed,
it is equal to the basis function $b_0(x)$.  Let us compute $\chi^\#(\model^{(\text{L, o})},g,
\epsilon^2)$.  If we pick all weights in $f_{\w}(x) = \sum_{i=0}^{d-1}
\w_i b_i(x)$ equal to $0$ except $\w_0$ equal to $1$ then we get
$g(x)$.  Hence, taking advantage of the fact that the basis functions
are orthonormal, we have
\begin{align*}
&\expectation_{x \sim \dist_x}[(f_{\w} - g(x))^2]  =
\frac12 \int_{-1}^{1} (f_{\w}(x)-g(x))^2 dx  \\  
= & \frac12 \sum_{i=0}^{d-1} (\w_i-\ind_{\{i=0\}})^2 \int_{-1}^{1} b_i(x)^2 dx  
= \frac12 \sum_{i=0}^{d-1} (\w_i-\ind_{\{i=0\}})^2.
\end{align*}
So we need to compute the probability 
\begin{align*}
\prob\left[\w:  \frac12 \sum_{i=0}^{d-1} (\w_i-\ind_{\{i=0\}})^2 \leq \e^2\right].
\end{align*}
Recall that our weights are iid Gaussians of mean $0$ and variance $\sigma_w^2$. Hence
\begin{align*}
\sum_{i=1}^{d-1} \w_i^2  \sim \Gamma \left(\frac{d-1}{2}, 2 \sigma_w^2 \right),
\end{align*}
where $\Gamma(k, \theta)$ denotes the Gamma distribution with shape
$k$ and scale $\theta$.  It follows that the probability we are
interested in can be expressed as $\prob\left[\w:  \frac12 \sum_{i=0}^{d-1} (\w_i-\ind_{\{i=0\}})^2 \leq \e^2\right] = q(\kappa=1, \sigma_w, \epsilon)$, where
$$q(\kappa, \sigma_w, \epsilon) =
\frac{1}{\sqrt{2 \pi \sigma_w^2}}\int_{0}^{\epsilon} F(\epsilon^2-x^2; \frac{d-1}{2}, 2 \sigma_w^2) \left[e^{-\frac{(\kappa+x)^2}{2 \sigma_w^2}}+e^{-\frac{(\kappa-x)^2}{2 \sigma_w^2}} \right]dx.
$$
Here, $F(x; k, \theta)$ denotes the cdf of the Gamma distribution
with shape $k$ and scale $\theta$. From the above expression we can compute $\chi^\#(\model^{(\text{L, o})}, g(x) =
\frac{1}{\sqrt{2}},\epsilon^2)$, although there does not seem to be an elementary expression.

\begin{lemma}[$q(\kappa, \sigma_w, \epsilon)$] \label{lem:qproperties}
For non-negative $\kappa$, $\sigma_w$, and $\epsilon \in (0, 1]$ the function
$q(\alpha, \sigma_w, \epsilon)$ has the following properties:
\begin{enumerate}[(i)]
\item Scaling: $q(\kappa, \sigma_w, \epsilon) = \kappa q(1, \sigma_w/\kappa, \epsilon/\kappa)$
\item Monotonicity in $\kappa$: $q(\kappa, \sigma_w, \epsilon)$ is non-increasing in $\kappa$
\item Monotonicity in $\sigma_w$: $q(\kappa, \sigma_w, \epsilon)$ is non-increasing in $\sigma_w$
\item Monotonicity in $\epsilon$: $q(\kappa, \sigma_w, \epsilon)$ is non-decreasing in $\epsilon$
\item Limit: $\lim_{\epsilon \rightarrow 0}\log(q(\kappa, \sigma_w, \epsilon))/\log(\epsilon)=d$
\end{enumerate}
\end{lemma}

If we are just interested in $\chi^\#(\model^{(\text{L, o})}, g(x) = \frac{1}{\sqrt{2}})$,
we can start from $\chi^\#(\model^{(\text{L, o})}, g(x) = \frac{1}{\sqrt{2}}, \epsilon^2)$
or we can make use of (v) of Lemma~\ref{lem:qproperties} to get 
\begin{equation}
\chi^\#(\model^{(\text{L, o})}, g(x) = \frac{1}{\sqrt{2}})=d.    
\end{equation}
To see this result intuitively note that all weights
have to be fixed to a definite value in order to realize $g(x)$.
\end{example}

\begin{example}[Basis Function]
Although we assumed in the above derivation that $g(x)=b_0(x)$ the
calculation is identical for any $g(x)=b_i(x)$, $i=0, \cdots, d-1$.
We conclude that $\chi^\#(\model^{(\text{L, o})}, b_i(x))$ does not depend
on $i$.  \end{example}

\begin{example}[Realizable Function of Norm $1$]
Assume that $g(x)= \sum_{i=0}^{d-1} \widetilde{\w}_i b_i(x)$ with
$\sum_{i=0}^{d-1} \widetilde{\w}_i^2=1$. In other words, the function
is realizable and has squared norm equal to $1$.

If we ``rotate'' (orthonormal transform) our basis $\{b_i(x)\}_{i=0}^d$
into the new basis $\{\tilde{b}_i(x)\}_{i=0}^d$ so that
$g(x)=\tilde{b}_0(x)$ then due to the rotational symmetry of our
prior we are back to our first example.

We conclude that for any realizable function $g(x)$ of norm $1$,
$$\chi^\#(\model^{(\text{L, o})}, g(x), \epsilon^2) = \chi^\#(\model^{(\text{L, o})}, b_0(x), \epsilon^2).$$
\end{example}

\begin{example}[Realizable Function]
Assume that $g(x)= \sum_{i=0}^{d-1} \widetilde{\w}_i b_i(x)$ with
$\sum_{i=0}^{d-1} \widetilde{\w}_i^2=\kappa^2$. In other words, the
function is realizable and has norm equal to $\kappa$.

Using the scaling property of Lemma~\ref{lem:qproperties} we can write 
\begin{align*}
&\chi^\#\left(\model_{\w(\sigma_w)}^{(\text{L, o})}, g(x), \epsilon^2 \right) \\
& = -\log(q(\kappa, \sigma_w, \epsilon)) \\
& = -\log(\kappa q(1, \sigma_w/\kappa, \epsilon/\kappa)) \\
& = -\log(\kappa) + \chi^\#(\model_{\w(\sigma_w/\kappa)}^{(\text{L, o})}, b_0(x), \epsilon^2/\kappa^2),
\end{align*}
where we wrote $\model_{\w(\sigma_w)}^{(\text{L, o})}$ to indicate that in the model
each parameter's prior is a Gaussian with variance $\sigma_w^2$.

This means that the complexity of a function changes depending on the norm of the vector of weights $\w$ that represent it. However if we are interested in the asymptotic complexity all functions (apart from the $0$ function) have the same complexities as $\lim_{\e \rightarrow 0} \frac{\log(\kappa) }{\log(\e)} = 0$, which leads to the next example.
\end{example}

\begin{example}[Limiting Sharp Complexity]\label{ex:linearlimit}
Assume that $g(x)= \sum_{i=0}^{d-1} \widetilde{\w}_i b_i(x)$. Then
$$
\chi^\#(\model^{(\text{L, o})}, g(x))=d.
$$
\end{example}

Recall that we showed (Example~\ref{lem:cchangeslimit}) that for the case of 2-layer neural networks the limiting complexity depends strongly on the function and simpler functions - in a sense of number of changes of slope - have lower complexity. Here we see that for linear models basically all functions have the same complexity, which is equal to the number of basis functions in the model.

\begin{example}[Unrealizable Function]
Given any function $g(x)$, we can represent it as
$g(x)=g_{\perp}(x)+g_{\|}(x)$, where the two components are orthogonal
and where $g_{\|}(x)$ represents the realizable part. We then have
that $\chi^\#(\model_{\w(\sigma_w)}^{(\text{L, o})}, g(x), \epsilon^2)$ is equal to
\begin{align*}
\begin{cases}
\infty, & \|g_{\perp}(x)\|_2^2 > \epsilon^2, \\
-\log \left(q \left(1, \sigma_w, \sqrt{\epsilon^2-\|g_{\perp}(x)\|_2^2} \right) \right), & 
\|g_{\perp}(x)\|_2^2 < \epsilon^2.
\end{cases}
\end{align*}
%\com{It used to be this:
%$$
%\frac{\log(\|g_{\|}(x)\|_2)}{\log(\epsilon)}+ \frac{\log \left(q \left(1, \frac{\sigma_w}{\|g_{\|}(x)\|_2}, \frac{\sqrt{\epsilon^2-\|g_{\perp}(x)\|_2^2}}{\|g_{\|%}(x)\|_2} \right) \right)}{\log(\epsilon)}
%$$
%}
\end{example}

\subsection{Non-Orthonormal Basis}
\begin{example}[Non-Orthogonal Basis]
If the functions do not form an orthonormal basis but are independent, then we
can transform them into such base. After the transform the probability distribution is
still a Gaussian but no longer with independent components. Now the
"equal simplicity" lines are ellipsoids.

And if we have dependent components then we also still have Gaussians
but we are in a lower dimensional space.
\end{example}

\subsection{Summary}
We have seen that for the linear model the complexity of a function
$g(x)$ only depends on the norm of the signal. This complexity measure is therefore only
weakly correlated with other natural complexity measures. E.g., if
the basis consists of polynomials of increasing degrees and the reality is modeled by a function of low degree then the bound from \eqref{eq:generalizationbound} is the same as when the reality is modeled by a high degree polynomial. It means that the number of samples needed for a good generalization bound is independent of $g$.

%\section{Conclusion and Future Work}

%\begin{itemize}
%    \item Many layers,
%    \item different activation functions,
%    \item ...
%\end{itemize}

\bibliography{references.bib}

\clearpage
\clearpage

\appendix

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.

%\aistatstitle{Supplementary Material}

\section{Generalization bound}\label{apx:generalization}

To derive the bound from Theorem~\ref{thm:generalizationbound} in terms of ``sharp complexity'' we first define a series of related notions that are helpful during the derivation.

% NOTE: 
% 1. notation is ugly
% 2. there is no normalization in there wrt to the noise
We define the {\em empirical} complexity of a function $g$ as 
%\begin{align*}
%&\chi^E(g, \samp_x, \sigma^2, \alpha) \\
%&:= -\log \left[ \left( \int_{\theta} P(\theta) e^{-\frac{1}{2\sigma_y^2} \sum_{n=1}^{N} (g(x_n)  - f_{\theta}(x_n))^2} d \theta\right) \right].  
%\end{align*}
\begin{align*}
&\chi^E(g, \samp_x, \samp_{\epsilon}, \sigma_y^2) \\ 
&:= -\log \left[ \left( \int_{\theta} P(\theta) e^{-\frac{1}{2\sigma_y^2N} \sum_{n=1}^{N} (g(x_n) + \eta_n - f_{\theta}(x_n))^2} d \theta\right) \right],
\end{align*}
where we denoted by $\samp_x$ the $x$'s part of $\samp$ and by $\samp_\e$ the particular realization of noise used for generating $\samp$, i.e. $\eta$'s.

In order to compute it, we integrate over the parameter space and weigh the prior $P(\theta)$ by an exponential factor which is the smaller the further the function $f_{\theta}$ is from $g$ on the given sample $\samp_x$ plus noise $\samp_\e$. Recall that noise samples $\samp_{\epsilon}$ come from an iid Gaussian zero-mean sequence of variance $\sigma_e^2$. We then take the negative logarithm of this integral.

The {\em true complexity with noise} is defined as
\begin{align*}
&\chi^N(g, \dist_x, \sigma_y^2, \sigma_\e^2) := \\
&-\log \left[ \left( \int_{\theta} P(\theta) e^{-\frac{1}{2\sigma_y^2} \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) + \noise - f_{\theta}(x))^2]} d \theta\right) \right],   
\end{align*}
where the sum has been replaced by an expectation using the underlying distribution of the input.

The {\em exponential complexity} is
\begin{align*}
&\chi(g, \dist_x, \sigma_y^2) := \\
&-\log \left[ \left( \int_{\theta} P(\theta) e^{-\frac{1}{2\sigma_y^2} \expectation_{x \sim \dist_x} [(g(x) - f_{\theta}(x))^2]} d \theta\right) \right].   
\end{align*}
Note that
\begin{align*}
\chi(g, \dist_x, \sigma_y^2) + \frac{ \sigma_e^2}{2 \sigma_y^2} = \chi^N(g, \dist_x, \sigma_y^2, \sigma_e^2).
\end{align*}

Finally, the {\em sharp complexity with noise} is defined as
\begin{align*}
&\chi^{\#N}(g, \dist_x, \sigma_e^2, \e^2) \\
&:= -\log \left[ \prob_\theta[ \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) + \noise - f_{\theta}(x))^2] \leq \e^2] \right].
\end{align*}

The following two lemmas establish some relationships between these notions of complexity.
\begin{lemma}[Sharp complexity: with and without noise]\label{lem:sharpwithnoisenonoise}
For every $\dist_x$, every $g : \inspace \rightarrow \outspace$, and $\e^2 > 0$ we have:
\begin{align*}
&\chi^{\#N}(g, \dist_x, \sigma_e^2, \e^2) = \chi^\#(g, \dist_x, \e^2 - \sigma_e^2).
\end{align*}
\end{lemma}

\begin{proof}
\begin{align*}
&\chi^{\#N}(g, \dist_x, \sigma_e^2, \e^2) \\
& = -\log \left[ \prob_\theta \left[ \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) + \noise - f_{\theta}(x))^2] \leq \e^2 \right] \right] \\
&= -\log \left[ \prob_\theta \left[ \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) - f_{\theta}(x))^2] \leq \e^2 - \sigma_e^2 \right] \right] \\
&= \chi^\#(g, \dist_x, \e^2 - \sigma_e^2),
\end{align*}
where in the second equality we write $(g(x) +\noise - f_\theta(x))^2$ as the sum of $(g(x)-f_\theta(x))^2$, $2\noise(g(x) - f_\theta(x))$ and $\noise^2$ and use the fact that $\expectation[\noise] = 0$ and $\expectation[\noise^2] = \sigma_e^2$.
\end{proof}

\begin{lemma}[True versus sharp complexity] \label{lem:trueversussharp}
For every $\dist_x$, every $g : \inspace \rightarrow \outspace$, and $\sigma_y^2, \sigma_e^2, \e^2 > 0$ we have:
\[
\chi^N(g, \dist_x, \sigma_y^2, \sigma_e^2) \leq \chi^{\#N}(g, \dist_x, \sigma_e^2, \e^2) + \frac{\e^2}{2\sigma_y^2}.
\]
\end{lemma}

\begin{proof}
\begin{align*}
&\chi^{\#N}(g, \sigma_e^2,\e^2) \\
&=
-\log \left( \int_{\theta} P(\theta) \mathds{1} \left\{ \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) + \noise - f_{\theta}(x))^2] \leq \e^2 \right\} d \theta \right) \\
&\stackrel{\alpha>0}{=} -\log \left( \int_{\theta} P(\theta) \mathds{1} \left\{ \frac{\alpha}{2\sigma_y^2} \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) + \noise - f_{\theta}(x))^2] \leq \frac{\alpha \e^2}{2\sigma_y^2} \right\} d \theta \right) \\
&\stackrel{e^{ x} \geq \mathds{1}\{x \geq 0 \}}{\geq} 
\!\!\!\!-\log \left( \int_{\theta} P(\theta) e^{\frac{\alpha \e^2}{2\sigma_y^2} -\frac{\alpha}{2\sigma_y^2} \expectation_{x \sim \dist_x, \noise \sim \mathcal{N}(0,\sigma_e^2)} [(g(x) + \noise - f_{\theta}(x))^2]} d \theta \right) \\
&= \chi^N(g, \sigma_y^2/\alpha, \sigma_e^2) - \frac{\alpha \e^2}{2\sigma_y^2}.
\end{align*}
\end{proof}

The sharp complexity is very convenient to work with. Hence we will formulate our final bound in terms of the sharp complexity. The reason we call it {\em sharp} complexity is that the region of $\theta$ we integrate over is defined by an indicator function whereas for the true complexity the ``boundary'' of integration is defined by a smooth function.

Let us now look more closely at the divergence where we assume the data model (\ref{equ:datamodel}) and that the true hypothesis is $g$. We have
\begin{align}
&D(Q \| P) \nonumber \\
&= \int \frac{P(\theta) e^{- \frac{1}{2 \sigma_y^2} \sum_{n=1}^{N} (y_n - f_{\theta}(x_n))^2}}{\int P(\theta') e^{- \frac{1}{2 \sigma_y^2} \sum_{n=1}^{N} (y_n - f_{\theta'}(x_n))^2} d \theta'} \cdot \nonumber \\
&\cdot \log \left(
\frac{ e^{- \frac{1}{2 \sigma_y^2} \sum_{n=1}^{N} (y_n - f_{\theta}(x_n))^2}}{\int P(\theta') e^{- \frac{1}{2 \sigma_y^2} \sum_{n=1}^{N} (y_n - f_{\theta'}(x_n))^2} d \theta'} 
\right) d \theta \nonumber \\
 &\leq \chi^E(g,\samp_x, \samp_{\epsilon},\sigma_y^2/N)- \frac{N}{2\sigma_y^2} L_\samp(Q), %\nonumber \\
 %&\leq \chi^E(g,\samp_x, \samp_{\epsilon},\sigma_y^2, N) 
 \label{eq:divergenceupperbound}
 \end{align}
where in the last inequality we used the fact that we use a clipped version of a square loss.
Therefore the expectation over $S \sim \dist^N$ of the square root term of the right-hand side of the PAC-Bayes bound \eqref{equ:pacbound} can be upper-bounded as
\begin{align}
&\expectation_{\samp \sim \dist^{N}} \left[C\sqrt{\frac{D(Q \| P)}{2 N}} \right] \nonumber \\
&\stackrel{\text{By } \eqref{eq:divergenceupperbound}}{\leq} \expectation_{\samp \sim \dist^{N}} \left[C \sqrt{\frac{\chi^E(g, \samp_x,\samp_\e, \sigma^2_y/N) - \frac{N}{2\sigma_y^2} L_\samp(Q) }{2 N}} \right] \nonumber \\
&\stackrel{\sqrt{\cdot} \text{ concave}}{\leq} %\nonumber\\
%&\leq
\frac{C}{\sqrt{2}}\sqrt{\frac{\expectation_{\samp \sim \dist^{N}} \left[\chi^E(g, \samp_x,\samp_\e, \sigma^2_y/N) \right]}{N} -\frac{\widehat{L}}{2\sigma_y^2}} \label{eq:generalboundempcomp},
\end{align}
where we denoted $\expectation_{\samp \sim \dist^N}[L_\samp(Q)]$ by $\hat{L}$. Before we proceed we state a helpful lemma.

\begin{lemma} \label{lem:megaineq}
Let $X$ and $Y$ be independent random variables and $f(X, Y)$ be a non-negative function. Then
\begin{align*}
\expectation_X \left[ \ln \left( \expectation_Y \left[ e^{-f(X, Y)}\right] \right)  \right]
\geq \ln \left( \expectation_Y\left[e^{-\expectation_X[f(X, Y)]}\right]\right).
\end{align*}
\end{lemma}
\begin{proof}
We limit our proof to the simple case where the distributions are discrete and have a finite support, lets say from $\{1, \cdots, I\}$. We claim that for $1 \leq j <I$,
\begin{align*}
& (\sum_{i=1}^{j} p(X=x_i))
\ln \left( \expectation_Y\left[e^{-\frac{\sum_{i=1}^{j}p(X=x_i) f(x_i, Y)}{\sum_{i=1}^{j} p(X=x_i) }}\right]\right) + 
\sum_{i=j+1}^{I} p(X=x_i) \left[ \ln \left( \expectation_Y \left[ e^{-f(x_{i}, Y)}\right] \right)  \right] \\
\geq &
 (\sum_{i=1}^{j+1} p(X=x_i))
\ln \left( \expectation_Y\left[e^{-\frac{\sum_{i=1}^{j+1} p(X=x_i) f(x_i, Y)}{\sum_{i=1}^{j+1} p(X=x_i) }}\right]\right) +
\sum_{i=j+2}^{I} p(X=x_i) \left[ \ln \left( \expectation_Y \left[ e^{-f(x_{i}, Y)}\right] \right)  \right].
\end{align*}
This gives us a chain of inequalities. Note that the very first term in this chain is equal to the left-hand side of the desired inequality and the very last term is equal to the right-hand side of the inequality. 

Consider the $j$-th such inequality. Cancelling common terms, it requires us to prove
\begin{align*}
& (\sum_{i=1}^{j} p(X=x_i))
\ln \left( \expectation_Y\left[e^{-(\sum_{i=1}^{j} \frac{p(X=x_i) f(x_i, Y)}{\sum_{i=1}^{j} p(X=x_i) })}\right]\right) + 
 p(X=x_{j+1}) \left[ \ln \left( \expectation_Y \left[ e^{-f(x_{j+1}, Y)}\right] \right)  \right] \\
\geq &
 (\sum_{i=1}^{j+1} p(X=x_i))
\ln \left( \expectation_Y\left[e^{-(\sum_{i=1}^{j+1} \frac{p(X=x_i) f(x_i, Y)}{\sum_{i=1}^{j+1} p(X=x_i) })}\right]\right).
\end{align*}
Taking the prefactors into the logs, combining the two log terms on the left-hand side, and finally cancelling the logs, the claimed inequality is true iff
\begin{align*}
\expectation_Y\!\!\! \left[e^{-\frac{\sum_{i=1}^{j} p(X=x_i) f(x_i, Y)}{\sum_{i=1}^{j} p(X=x_i) }}\right]^{\frac{\sum_{i=1}^{j} p(X=x_i)}{\sum_{i=1}^{j+1} p(X=x_i)}} 
   \!\!\!\!\!\! \expectation_Y \!\!\! \left[ e^{-f(x_{j+1}, Y)}\right] ^{\frac{p(X=x_{j+1})}{\sum_{i=1}^{j+1} p(X=x_i)}}
\geq &
\expectation_Y \!\!\! \left[e^{-\frac{\sum_{i=1}^{j+1} p(X=x_i) f(x_i, Y)}{\sum_{i=1}^{j+1} p(X=x_i) }}\right].
\end{align*}
But this statement is just an instance of the Hoelder inequality with $1/p+1/q=1$, where $1/p=\frac{\sum_{i=1}^{j} p(X=x_i)}{\sum_{i=1}^{j+1} p(X=x_i)}$ and $1/q=\frac{p(X=x_{j+1})}{\sum_{i=1}^{j+1} p(X=x_i)}$.
\end{proof}


Now we bound the complexity term from \eqref{eq:generalboundempcomp} further. We have for every $\e^2 > 0$
\begin{align}
&\expectation_{S \sim \dist^{N}}[\chi^E(g, \samp_x,\samp_\e, \sigma_y^2/ N)] \nonumber \\
&= -\expectation_{S \sim \dist^{N}} \left[ \log  \left( \int_{\theta} P(\theta) e^{-\frac{1}{2\sigma^2_y} \sum_{n=1}^{N} (g(x_n) + \e_n  - f_{\theta}(x_n))^2} d \theta\right)   \right] \nonumber \\
&\stackrel{\text{Lem~\ref{lem:megaineq}}}{\leq} -\log \left( \int_{\theta} P(\theta) e^{-\frac{N}{2\sigma^2_y} \expectation_{\stackrel{x \sim \dist_x}{ \noise \sim \mathcal{N}(0,\sigma_e^2)}} [(g(x) + \noise - f_\theta(x))^2]} d \theta\right) \nonumber \\
&= \chi^N(g, \dist_x, \sigma^2_y/N, \sigma_e^2)
\stackrel{\text{Lem~\ref{lem:trueversussharp}}}{\leq} \chi^{\#N}(g, \dist_x, \sigma_e^2, \e^2) + \frac{\e^2 N }{2 \sigma^2_y} \nonumber \\
&\stackrel{\text{Lem}~\ref{lem:sharpwithnoisenonoise}}{=} \chi^\#(g, \dist_x, \e^2 - \sigma_e^2) + \frac{\e^2 N }{2 \sigma^2_y}. \label{eq:generalboundtruecomp} 
\end{align}
Hence by combining \eqref{eq:generalboundempcomp} and \eqref{eq:generalboundtruecomp} we get that for every $\e^2 > 0$ the expectation over $S \sim \dist^N$ of the PAC-Bayes bound can be bounded as
\begin{align}
&\expectation_{\samp \sim \dist^{N}}\left[L_{\samp}(Q) + C\sqrt{\frac{D(Q \| P) }{2 N}}\right] \nonumber \\
&\leq \widehat{L} + \frac{C}{\sqrt{2}}\sqrt{\frac{\chi^\#(g, \dist_x, \e^2 - \sigma_e^2)}{N}  + \frac{1}{2\sigma^2_y}\left(\e^2 - \widehat{L}\right)} \label{eq:generalizationbound}.
\end{align}
%Next we analyze the $\frac{1}{2\sigma_y^2}\left(\e^2 - \widehat{L}\right)$ term. 
Let $\beta \in (0,1]$. Recall that parameter $\sigma_y^2$ is chosen freely by the learning algorithm. By the assumption of the theorem we have 
%on the data generating model we have that for all $Q$ 
\begin{equation}\label{eq:lwrbndonLd}
L_\dist(P) \geq 2\sigma_e^2. 
\end{equation}
Because $g \in \text{supp}(P)$, which in words means that $g$ is realizable with prior $P$, then
\begin{align}
\lim_{\sigma_y^2 \rightarrow 0} \widehat{L}
&= \lim_{\sigma_y^2 \rightarrow 0} \expectation_{\samp \sim \dist^N} [ L_\samp(Q) ]  \nonumber\\
&= \expectation_{\samp \sim \dist^N} \left[ \lim_{\sigma_y^2 \rightarrow 0} L_\samp(Q) \right]  \nonumber\\
&\leq \expectation_{\samp \sim \dist^N} L_\samp(g) \nonumber\\
&= \sigma_e^2 . \label{eq:Lhatzero}
\end{align}

%Note that if there exists $\theta \in \text{supp}(P)$, such that $\sum_n (y_n - f_\theta(x_n))^2 = 0$, which in words means that the dataset is realizable with prior $P$ \com{before we only had an assumption on $g$ being in the support of $P$. But I guess it's fine too}, then 
%\begin{equation}\label{eq:Lhatzero}
%\lim_{\sigma_y^2 \rightarrow 0} \widehat{L} = 0.
%\end{equation}
where in the second equality we used Lebesgue dominated convergence theorem and in the inequality we used the fact that the smaller $\sigma_y^2$ gets the bigger the penalty on $\sum_n (y_n - f_\theta(x_n))^2$ in $Q$, which means that, in the limit, $L_\samp(Q)$ is smaller than $L_\samp(h)$ for every fixed $h \in \text{supp}(P)$ and in particular for $g$. 

On the other hand, by an analogous argument, we have
\begin{align}
\lim_{\sigma_y^2 \rightarrow \infty} \widehat{L} 
&= \expectation_{\samp \sim \dist^N} \left[ L_\samp(P) \right] \nonumber \\
&= \expectation_{\samp \sim \dist^N} \left[  \expectation_{\theta \sim P} \left[\frac{1}{N} \sum_{i=1}^N \ell(f_\theta, y_n) \right] \right] \nonumber \\
&=  \expectation_{\theta \sim P} \left[ \expectation_{\samp \sim \dist^N} \left[\frac{1}{N} \sum_{i=1}^N \ell(f_\theta, y_n)  \right] \right] \nonumber \\
&= L_\dist(P)  \nonumber \\
&\geq 2 \sigma_e^2, \label{eq:Lhatinfty}
\end{align}
where we used the independence of $P$ and $\samp$ in the third equality and \eqref{eq:lwrbndonLd} in the inequality.

Equations \eqref{eq:Lhatzero} and \eqref{eq:Lhatinfty} and the fact that $\widehat{L}$ is a continuous function of $\sigma_y^2$ give us that there exists $\sigma_{\text{alg}}^2 > 0$ such that
$$ 
\expectation_{\samp \sim \dist^N} \left[L_\samp(Q(\sigma_{\text{alg}}^2)) \right] = (1 + \beta) \sigma_e^2,
$$
where we wrote $Q(\sigma_{\text{alg}}^2)$ to explicitly express the dependence of $Q$ on $\sigma_y^2$. With this choice for $\sigma_y^2$ and setting $\e^2 = (1+\beta)\sigma_e^2$ applied to \eqref{eq:generalizationbound} we arrive at the statement of Theorem~\ref{thm:generalizationbound}. Note that with this choice of parameters term $\frac1{2\sigma_y^2}(\e^2 - \widehat{L})$ from \eqref{eq:generalizationbound} is equal to $0$.


\section{Omitted proofs}\label{app:proofs}

\begin{lemma}\label{lem:prefsumstonumbers}
Let $\{x_i\}_{i=1}^{k}$ be a set of real numbers. For $i=1, \cdots, k$, define the partial sums $X_i=\sum_{j=1}^{i} x_j$. Then
\begin{align*}
\sum_{i=1}^{k} X_i^2 \geq \frac18 \sum_{i=1}^{k} x_i^2.
\end{align*}
\end{lemma}
\begin{proof}
Define $X_0=0$. Note that for $i=1, \cdots, k$, $X_i = X_{i-1}+x_i$. Hence if $|X_{i-1}| \leq \frac12 |x_i|$ then $|X_{i}|\geq\frac12 |x_i|$ so that $X_{i}^2\geq\frac14 x_i^2$. And if $|X_{i-1}| \geq \frac12 |x_i|$ then $X_{i-1}^2\geq \frac14 x_i^2$. Therefore, $X_{i-1}^2+X_{i}^2 \geq \frac14 x_i^2$. Summing the last inequality over $i=1, \cdots, k$, and adding $X_k^2$ to the left hand side we get $2 \sum_{i=1}^{k} X_i^2 \geq  \frac14 \sum_{i=1}^{k} x_i^2$.
\end{proof}

\begin{lemma}\label{lem:l2lwrbnd}
Let $f(x)= \sum_{i=1}^{k} w_i [x-b_i]_+$, where $0 \leq b_1 \leq \cdots \leq b_k \leq 1 = b_{k+1}$. For $i=1, \cdots, k$, define the partial sums $W_i=\sum_{j=1}^{i} w_j$. Then 
\begin{align*}
\|f\|^2 \geq \frac{1}{12} \sum_{i=1}^{k} W_i^2 (b_{i+1} - b_i)^3.
\end{align*}
\end{lemma}
\begin{proof}
Note that there are $k$ non-overlapping intervals, namely [$b_1, b_2], \cdots, [b_k, 1]$, where the function is potentially non-zero. On the $i$-th interval the function is linear (or more precisely, affine) with a slope of $W_i$ and, by assumption, the interval has length $b_{i+1}-b_i$. On this interval the integral of $f(x)^2$ must have a value of at least $\frac1{12} W_i^2 (b_{i+1}-b_i)^3$. The last statement follows by minimizing the integral of the square of an affine function with slope $W_i$ over the choice of the parameters.
\end{proof}

\begin{lemma}\label{lem:hardone}
Let $f_\theta(x)= \sum_{i=1}^{k} w_i [x-b_i]_+$, where $0 \leq b_1 \leq \cdots \leq b_k < +\infty$.
%, $f_\theta(0) = 0$ and if $f_\theta'(0) \neq 0$ then $0 \in \{b_1, \dots, b_k\}$. 
If $\|f_\theta\|^2 < \frac{1}{12(k+1)^5}$ then there exists $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} 0$ and
\begin{align*}
\|\theta - \theta^*\|^2 \leq O \left( k^{13/5} \|f_\theta\|^{4/5} \right).
\end{align*}
\end{lemma}

\begin{proof}
Starting with the parameter $\theta$ that defines the function $f_\theta(x)$, we define a process of changing it until the resulting function is equal to the zero function on $[0, 1]$. Most importantly, this process does not change $\theta$ too much compared to the norm of $f_\theta(x)$. 

Note that there are two ways of setting the function to $0$ on a particular interval. Either, we can make the length of the interval to be $0$. This requires to change one of the bias terms by the length of the interval. Or we set the slope of this interval to be $0$ (assuming that the function is already $0$ at the start of the interval. Our approach uses both of those mechanisms. Let $\theta^0 \leftarrow \theta$. The process has two phases. In the first phase we change the bias terms and in the second phase we change the weights. For $x \in [0,1]$, define the partial sums $W(x)=\sum_{j: b_j \leq x} w_j$. 

\paragraph{First phase.} Let %$i_{\text{min}} = \argmin_{i} \{b_i \geq 0\}$ and 
$S := \{[b_1,b_2], \dots, [b_{k-1},b_k],[b_k,1]\}$ and  $S_b := \{[l,r] \in S : r - l < |W(l)| \}$. Let $\{[l_0,r_0],[l_1,r_1], \dots, [l_i,r_i]\} \subseteq S_b$ be a maximal continuous subset of intervals in $S_b$. That is, for all $j \in [i]$, $r_j = l_{j+1}$ and the intervals ending at $l_0$ and starting at $r_i$ are not in $S_b$. Perform the following: for all $b_j \in [l_0,r_i]$ set $b_j \leftarrow r_i$. We do this operation for all maximal, continuous subsets of $S_b$. This finishes the first phase. Call the resulting vector of parameters $\theta^1$. We bound
\begin{align}
\|\theta^0 - \theta^1\|^2 
&\leq k \left(\sum_{[l,r] \in S_b} (r-l) \right)^2 \nonumber \\
&\leq k^{13/5} \left(\sum_{[l,r] \in S_b} (r-l)^5 \right)^{2/5} && \text{By the Power Mean Inequality} \nonumber \\
&\leq k^{13/5} \left(\sum_{[l,r] \in S_b} (r-l)^3 W(l)^2 \right)^{2/5} && \text{By definition of } S_b \nonumber \\
&\leq k^{13/5} (12 \|f_\theta\|^2)^{2/5} && \text{By Lemma~\ref{lem:l2lwrbnd}} \label{eq:first stagebound}
\end{align}

\paragraph{Second phase.} Observe that $f_{\theta^1}$ has the following properties. For every $x \in [0,1] \setminus \bigcup_{[l,r] \in S_b} [l,r)$ we have $W^1(x) = W^0(x)$. It is enough to make $W(l) = 0$ for all $[l,r]$ such that $[l,r] \in S \setminus S_b$.  Let $i_1 < i_2 < \dots < i_p$ be all $i_j$'s such that $[b_{i_j}, b_{i_j+1}] \in S \setminus S_b$. Applying Lemma~\ref{lem:prefsumstonumbers} to $\{W_{i_1}, W_{i_2} - W_{i_1}, \dots, W_{i_p} - W_{i_{p-1}}\}$ we get that
\begin{equation}\label{eq:makeallslopeszero}
8\sum_{j=1}^p W_{i_j}^2 \geq W_{i_1}^2 + (W_{i_2} - W_{i_1})^2 + \dots (W_{i_p} - W_{i_{p-1}})^2     
\end{equation}
The RHS of \eqref{eq:makeallslopeszero} gives an upper-bound on the $\|\cdot\|^2$ norm distance needed to change $w_i$'s in $\theta^1$ so that all $W_{i_j} = 0$. It is because we can change $w_1, \dots, w_{i_1}$ by at most $W_{i_1}^2$ to make $W_{i_1} = 0$ and so on for $i_2, \dots, i_p$. Call the resulting vector of parameters $\theta^2$. We bound the change in the second phase
\begin{align}
\|\theta^1 - \theta^2\|^2
&\leq 8 \sum_{j=1}^p W_{i_j}^2 && \text{\eqref{eq:makeallslopeszero}} \nonumber \\
&\leq 8k \left(\frac{1}{k} \sum_{j=1}^p |W_{i_j}^5| \right)^{2/5} && \text{Power Mean Inequality} \nonumber \\
&= 8k^{3/5} \left( \sum_{i : [b_{i}, b_{i+1}] \in S \setminus S_b } |W_i^5| \right)^{2/5} && \text{By definition} \nonumber \\
&\leq 8k^{3/5} \left( \sum_{i : [b_{i}, b_{i+1}] \in S \setminus S_b } (b_{i+1} - b_i)^3 |W_i^2| \right)^{2/5} && \text{By definition of } S_b \nonumber \\
&\leq 8 k^{3/5} \left(12 \|f_\theta\|^2 \right)^{2/5} && \text{By Lemma~\ref{lem:l2lwrbnd}} \label{eq:secondstagebound}.
\end{align}
We conclude by
\begin{align*}
\|\theta^0 - \theta^2\|^2 
&\leq 4 \max\left(\|\theta^0 - \theta^1\|^2, \|\theta^1 - \theta^2\|^2 \right) && \text{Triangle inequality} \\
&\leq 96 k^{13/5} \left(\|f_\theta\|^2 \right)^{2/5} && \text{\eqref{eq:first stagebound} and \eqref{eq:secondstagebound}}
\end{align*} 

\begin{comment}
Let $S^0 = \{[b_1, b_2], \dots, [b_k,1]\}$ be the set of {\em active} intervals at time $t=0$. I.e., initially all intervals are active. For $t \geq 0$ 
\begin{equation}\label{eq:condition}
\text{if there exists an $i$ such that } [b_i^t,b_{i+1}^t] \in S^t \text{ and } b_{i+1}^t - b_i^t < |W^t(b_i^t)| 
\end{equation}
then perform 
\begin{align*}
&\theta^{t+1} \leftarrow \theta^t, \\
&\alpha \leftarrow b_{i}^t, \ \beta \leftarrow b_{i+1}^t,\\
&\text{for every \com{only rightendpoint} } j \text{ such that } b_j^t = \beta \text{ set } b^{t+1}_j \leftarrow \alpha, \\ 
&S^{t+1} \leftarrow S^t \setminus \{[b^t_{i}, b^t_{i+1}] \}.
\end{align*}
In each step of the process one interval is removed from $S$, hence the process terminates in at most $t_{\max} \leq k$ steps.
The following properties hold for every $t < t_{\max}$:
\begin{enumerate}
    \item $\|\theta^{t+1} - \theta^{t}\|^2\leq 2k \cdot (b^t_{i+1} - b^t_i)^2 < 2k (W^t(b_i^t))^2$, as at most $2k$ bias terms were changed, \label{prop1}
    \item for every $x \in [0,1] \setminus [b_i^t, b_{i+1}^t)$ we have $W^{t+1}(x) = W^t(x)$, i.e. in the $t$-th step the slope changes only at $[b_i^t, b_{i+1}^t)$, \label{prop2}
    \item for every $x \in [b_i^t, b_{i+1}^t)$ we have $W^{t+1}(x) = W^{t}(b_{i+1}^t)$. \label{prop3}
\end{enumerate}
Note that, by construction, for every $[b_i^{t_{\max}}, b_{i+1}^{t_{\max}}] \in S^{t_{\max}}$ we have 
\begin{equation}\label{eq:weightischeaper}
b_{i+1}^{t_{\max}} - b_{i}^{t_{\max}} \geq |W^{t_{\max}}(b_{i}^{t_{\max}})|.
\end{equation}
We bound
\begin{align}
&12 \int_0^1 f(x)^2 \ dx  \nonumber \\
&\geq \sum_{i=1}^{k} W_i^2 (b_{i+1} - b_i)^3 && \text{ Lemma~\ref{lem:l2lwrbnd}} \nonumber \\
&\geq \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \in S^{t_{\max}} } W_i^2 (b_{i+1} - b_i)^3 + \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \not\in S^{t_{\max}} } W_i^2 (b_{i+1} - b_i)^3 \nonumber \\
&= \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \in S^{t_{\max}} } W^{t_{\max}}(b_i^{t_{\max}})^2 (b_{i+1}^{t_{\max}} - b_i^{t_{\max}})^3 \nonumber \\
&\ \ \ + \sum_{t=1}^{t_{\max}-1} \sum_{i : [b_i^{t-1}, b_{i+1}^{t-1}] \in S^{t-1} \setminus S^t}  W^{t-1}(b^{t-1}_i)^2(b^{t-1}_{i+1} - b^{t-1}_i)^3 && \text{By Property \eqref{prop2}} \nonumber \\
&\geq \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \in S^{t_{\max}} } |W^{t_{\max}}(b_i^{t_{\max}})^5| + \sum_{t=1}^{t_{\max}-1} \sum_{i : [b_i^{t-1}, b_{i+1}^{t-1}] \in S^{t-1} \setminus S^{t}} (b^{t-1}_{i+1} - b^{t-1}_i)^5 && \text{By \eqref{eq:weightischeaper} and \eqref{eq:condition}} \nonumber \\
&= \sum_{i : [b_{i}, b_{i+1}] \in S^{t_{\max}} } |W_i^5| + \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \not\in S^{t_{\max}} } (b_{i+1} - b_i)^5 && \text{By Property \eqref{prop2}}\label{eq:secondphasebound}
\end{align}
We bound the change in the first phase
\begin{align}
\|\theta - \theta^{t_{\max}}\|^2 
&\leq \left(\sum_{t=1}^{t_{\max}}\|\theta^{t-1} - \theta^t\| \right)^2 &&\text{Triangle inequality}  \nonumber \\
&\leq 2\left( \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \not\in S^{t_{\max}} } k^{1/2}(b_{i+1} - b_i) \right)^2 && \text{By Property } \eqref{prop1} \nonumber \\
&\leq 2k^3 \left(\frac1k \sum_{i : [b_{i}^{t_{\max}}, b_{i+1}^{t_{\max}}] \not\in S^{t_{\max}} } (b_{i+1} - b_i)^5 \right)^{2/5} && \text{Power Mean Inequality} \nonumber \\
&\leq 6k^{13/5} \left(\int_0^1 f_\theta(x)^2 \ dx \right)^{2/5} &&  \eqref{eq:secondphasebound} \label{eq:firststagebound}
\end{align}
\comm{RU}{now sure what we use above; what does 1 refer to? it seems that we have several 1s and 2s references around}

Now we show how to change the $w_i$'s in $\theta^{t_{\max}}$ to make the function the $0$ function - this is the second phase. By Properties \eqref{prop2} and \eqref{prop3} it is enough to make $W_i = 0$ for all $i$ such that $[b_{i}, b_{i+1}] \in S^{t_{\max}}$. Let $i_1 < i_2 < \dots < i_p$ be all $i_j$'s such that $[b_{i_j}, b_{i_j+1}] \in S^{t_{\max}}$. Applying Lemma~\ref{lem:prefsumstonumbers} to $\{W_{i_1}, W_{i_2} - W_{i_1}, \dots, W_{i_p} - W_{i_{p-1}}\}$ we get that
\begin{equation}\label{eq:makeallslopeszero}
8\sum_{j=1}^p W_{i_j}^2 \geq W_{i_1}^2 + (W_{i_2} - W_{i_1})^2 + \dots (W_{i_p} - W_{i_{p-1}})^2     
\end{equation}
The RHS of \eqref{eq:makeallslopeszero} gives an upper-bound on the $\|\cdot\|^2$ norm distance needed to change $w_i$'s in $\theta^{t_{\max}}$ so that all $W_{i_j} = 0$. It is because we can change $w_1, \dots, w_{i_1}$ by at most $W_{i_1}^2$ to make $W_{i_1} = 0$ and so on for $i_2, \dots, i_p$. Call the resulting vector of parameters $\theta^*$. We bound the change in the second phase
\begin{align}
\|\theta^{t_{\max}} - \theta^*\|^2
&\leq 8 \sum_{j=1}^p W_{i_j}^2 && \text{\eqref{eq:makeallslopeszero}} \nonumber \\
&\leq 8k \left(\frac{1}{k} \sum_{j=1}^p |W_{i_j}^5| \right)^{2/5} && \text{Power Mean Inequality} \nonumber \\
&= 8k^{3/5} \left( \sum_{i : [b_{i}, b_{i+1}] \in S^{t_{\max}} } |W_i^5| \right)^{2/5} && \text{By definition} \nonumber \\
&\leq 24 k^{3/5} \left(\int_0^1 f_\theta(x)^2 \ dx \right)^{2/5} &&\text{\eqref{eq:secondphasebound}} \label{eq:secondstagebound}.
\end{align}
We conclude by
\begin{align*}
\|\theta - \theta^*\|^2 
&\leq 4 \max\left(\|\theta - \theta^{t_{\max}}\|^2, \|\theta^{t_{\max}} - \theta^*\|^2 \right) && \text{Triangle inequality} \\
&\leq 96 k^{13/5} \left(\int_0^1 f_\theta(x)^2 \ dx \right)^{2/5} && \text{\eqref{eq:firststagebound} and \eqref{eq:secondstagebound}}
\end{align*}

\end{comment}

\end{proof}


\begin{lemma}\label{lem:hardwithbias}[\textbf{With} $b^{(2)}$]
Let $R \in \R_+, \theta \in B_R \cap \supp(P)$ be such that $f_\theta(x)= b^{(2)} + \sum_{i=1}^{k} w_i [x-b_i]_+$, where $0 \leq b_1 \leq \cdots \leq b_k < +\infty$. If $\|f_\theta\|^2$ is small enough, where the bound depends only on $R$ and $k$, then there exists $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} 0$ and
\begin{align*}
\|\theta - \theta^*\|^2 \leq O \left( k^{5} R^{4/5} \|f_\theta\|^{2/5} \right) .
\end{align*}
\end{lemma}

\begin{proof}
Let $\e^2 = \|f_\theta\|^2$. For $x \in \R$, define the partial sums $W(x)=\sum_{j: b_j \leq x} w_j$. 
%Let $i_+ := \argmin_i b_i \geq 0$. Note that
%\begin{equation}\label{eq:boundinfirst}
%\e^2 \geq \frac1{12} W(0)^2 b_{i_+}^3.
%\end{equation}
Consider the following cases:
\paragraph{Case $|b^{(2)}| \leq \e^{1/2}$. } We perform $\theta' \leftarrow \theta, b^{(2)'} \leftarrow 0$. By triangle inequality we can bound
$\|f_{\theta'}||^2 \leq \left( \e + |b^{(2)}| \right)^2 \leq 4 \e.$ We apply Lemma~\ref{lem:hardone} to $\theta'$ to obtain $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} 0 $ and $\|\theta' - \theta^*\|^2 \leq O\left(k^{13/5} \|f_{\theta'}||^{4/5}\right) \leq O\left(k^{13/5} \e^{2/5}\right)$. We conclude by noticing
\begin{align*}
\|\theta - \theta^*\|^2 
&\leq \left(\|\theta - \theta'\| + \|\theta' - \theta^*\| \right)^2 && \text{Triangle inequality} \\
&\leq \left(\e^{1/2} + O\left(k^{13/10} \e^{1/5}\right) \right)^2 \\
&\leq O\left(k^{13/5} \|f_\theta\|^{2/5}\right) && \text{As } \e^2 \leq 1.
\end{align*}

\paragraph{Case $|b^{(2)}| > \e^{1/2}$. } Without loss of generality assume that $b^{(2)}>0$. There exists $x_0 \in (0,\e/4),$ such that $f_\theta(x_0) = \frac{b^{(2)}}{2}$, as otherwise 
$$\e^2 \geq \int_0^{\e/4} f_\theta(x)^2 \ dx \geq \int_0^{\e/4} (b^{(2)})^2 / 4 \ dx > \e^2.$$ By the mean value theorem there exists $x_1 \in (0,x_0) \setminus \{b_1, \dots, b_k\}$ such that 
\begin{equation}\label{eq:x1bounds}
f_\theta(x_1) \in [b^{(2)}/2, b^{(2)}] \text{ and } W(x_1) \leq \frac{f_\theta(x_0) - f_\theta(0)}{x_0 - 0} \leq -\frac{4b^{(2)}}{2\e} \leq -2\e^{-1/2}.
\end{equation}
We perform the following transformation
\begin{align*}
&\theta' \leftarrow \theta, \\
&\text{for every } i \text{ such that } b_i < x_1 \text{ do } b'_i \leftarrow b_i - x_1 + \frac{f_\theta(x_1)}{W(x_1)}, \\
&i_0 \leftarrow \argmin_i b_i > x_1, \\
&b'_{i_0} \leftarrow 0.
\end{align*}
Observe that we shifted all $b_i$'s exactly so that $f_{\theta'}(0) = 0$. Note also that $b_{i_0} \leq 4\e$ as otherwise by Lemma~\ref{lem:l2lwrbnd}
$$
\e^2 \geq \int_{x_1}^{b_{i_0}} f_\theta(x)^2 \ dx \geq \frac1{12} W(x_1)^2 (b_{i_0} - x_1)^3 > \frac1{12} 4\e^{-1} (3\e)^3 \geq \e^2.
$$
By \eqref{eq:x1bounds} we can bound 
\begin{equation}\label{eq:shiftbiasesthetabound}
\|\theta - \theta'\|^2 
\leq k \left(-x_1 + \frac{f_\theta(x_1)}{W(x_1)}\right)^2 + 16\e^2 \leq O(k\e^2). 
\end{equation}
$f_\theta$ is $R$-Lipshitz wrt to $b_i$'s in $B_R$ thus the triangle inequality and \eqref{eq:shiftbiasesthetabound} gives 
\begin{equation}\label{eq:aftershiftingnorm}
\|f_{\theta'}\|^2
\leq  (\|f_\theta\| + O(R k^{3/2}\e))^2 
\leq O(R^2 k^5 \e^2). 
\end{equation}
We apply Lemma~\ref{lem:hardone} to $f_{\theta'}$, after we removed all $b'_i < 0$ and set $w'_{i_0} \leftarrow \sum_{j \leq i_0} w_j$. Lemma~\ref{lem:hardone} might require to change $w'_{i_0}$, which we can realize with the same cost by changing $\{w_j : j \leq i_0\}$. Thus Lemma~\ref{lem:hardone} and \eqref{eq:aftershiftingnorm} gives us that there exists $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} 0$ and $\|\theta' - \theta^*\|^2 \leq O(k^{13/5} k^2 R^{4/5} \e^{4/5})$. We conclude by using the triangle inequality and \eqref{eq:shiftbiasesthetabound} to get
$
\|\theta - \theta^*\|^2 \leq O\left(k^{23/5} R^{4/5} \|f_\theta\|^{4/5}\right).
$
\end{proof}

\begin{lemma}\label{lem:mostimportant}
Let $R \in \R_+, \theta \in B_R \cap \supp(P)$ be such that $f_\theta(x)= b^{(2)} + \sum_{i=1}^{k} w_i [x-b_i]_+$ and $g(x) = b + \sum_{i=1}^c v_i [x - t_i]_+$, where $c \leq k$, $0 \leq b_1 \leq \cdots \leq b_k < +\infty, 0 < t_1 < \dots < t_c < 1$ and $v_1,\dots,v_c \neq 0$. If $\|g - f_\theta\|^2$ is small enough,  where the bound depends only on $g,R$ and $k$, then there exists $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} g$ and
\begin{align*}
\|\theta - \theta^*\|^2 \leq O \left( k^{7} R^{4/5} \|g - f_\theta\|^{2/5} \right) .
\end{align*}
\end{lemma}

\begin{proof}
Consider a model on $c+k \leq 2k$ neurons represented as
\begin{equation}
h_{\boldsymbol{\theta}} := \left(b^{(2)} - b\right) + \sum_{i=1}^{k} w_i [x-b_i]_+ - \sum_{i=1}^c v_i [x - t_i]_+, 
\end{equation}
where, to distinguish it from $\theta$, we denoted by $\btheta$ the set of parameters of $h$. Observe that $\|h\|^2 = \|g - f_\theta\|^2$. By Lemma~\ref{lem:hardwithbias} there exists $\btheta^*$ such that $h_{\btheta^*} \equiv_{[0,1]} 0$ and $\|\btheta - \btheta^*\|^2 \leq O \left( k^{5} R^{4/5} \|g - f_\theta\|^{2/5} \right)$. If $\e$ is small enough then the parameters in $\btheta^*$ corresponding to $v_i$'s are all still all non-zero and the bias terms corresponding to $t_i$'s are still all different. As $h_{\btheta^*} \equiv_{[0,1]} 0$ it implies that for every $i \in [c]$ there is a set of bias terms corresponding to $b_j$'s that are exactly at where $t_i$ was moved. Let $\pi : [c] \rightarrow 2^{[k]}$ be the mapping from $t_i$'s to subsets of $b_i$'s certifying that. 

We define $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} g$ as the result of two steps. First, changing $\theta$ as its corresponding parameters were changed in the transition $\btheta \rightarrow \btheta^*$. Second, changing the parameters as $v_i$'s and $t_i$'s are changed in $\btheta^* \rightarrow \btheta$ under the map $\pi$. Observe that $\|\theta - \theta^*\|^2 \leq k^2 \|\btheta - \btheta^*\|^2$. It is because in the second step we move at most $k$ bias terms for every parameter corresponding to $t_i$.     
\end{proof}

Proof of Lemma~\ref{lem:complexitytodimension}

\begin{proof}
%We will use these interchangeably $\|g - f_\theta\|^2 = \int_0^1 (g(x) - f_\theta(x))^2 dx = \expectation_{x \sim U([0,1])} (g(x) - f_\theta(x))^2$.
%Minkowski-Bouligand dimension is finitely additive, that is for a finite collection $\{A_1, A_2, \dots, A_n\}$ we have $\dim(A_1 \cup \dots A_n) = \max(\dim(A_1), \dots \dim(A_n))$. Thus for every $R > 0$ we have $\dim(A_g) = \max(\dim(A_g \cap B_R), \dim(A_g \cap \overline{B_R}))$. \com{Why does it exist?} Let $R > 0$ be such that $\dim(A_g \cap B_R) = \dim(A_g)$.
%there exist two non-decreasing, continuous functions $\delta_1, \delta_2 : [0,\infty) \rightarrow [0,\infty), \delta_1(0), \delta_2(0) = 0$ such 
Let $R \in \R_+$. Notice that $f_\theta$ is $R^2$-Lipschitz with respect to each of its parameters, when restricted to a ball $B_{R}$. This implies that for all $\e > 0$ 
\begin{equation}\label{eq:inclusion1}
(A_g + B_\e) \cap B_R \subseteq \left\{\theta : \|g - f_\theta\|^2 \leq R^4 \e^2 \right\}.
\end{equation}
On the other hand by Lemma~\ref{lem:mostimportant} we have that for small enough $\e$
\begin{equation}\label{eq:inclusion2}
\left\{\theta : \|g - f_\theta\|^2 \leq \e^2 \right\} \cap B_R \cap \supp(P) \subseteq A_g + B_{O \left( k^{7/2} R^{2/5} \e^{1/5} \right) } \subseteq A_g + B_{C(k,R)\e^{1/5} },    
\end{equation}
for some function $C$.
\begin{comment}
Next we prove \eqref{eq:inclusion2}. Let $\theta \in B_R$ be such that $\|g - f_\theta\|^2 \leq \e^2$. Let $\eta(\Delta, W)$ denotes the minimum $\ell_2$ difference on $[-\Delta, \Delta]$ between a linear function and a two-piece linear function that has a change of slope of $W$ at $0$, i.e.
$\eta(\Delta,W) = \min_{a,b} \int_{-\Delta}^0 (ax + b)^2 dx + \int_0^\Delta (ax+b - W x)^2 \ dx.$ Solving the minimization problem we get
\begin{equation}\label{eq:etavalue}
\eta(\Delta, W) = \frac{\Delta^3 W^2}{24}.
\end{equation}
 
We proceed by changing $\theta$ in phases to arrive at an exact representation of $g$ while incurring only a small change to $\theta$ in the $\|\cdot\|^2$ norm. In phase 1 we make sure that $f''$ roughly agrees with $g''$ at $t_1, \dots, t_c$, then, in phase 2, we make sure that the agreement is exact and finally, in phase 3, we enforce agreement of $f$ and $g$ on whole $[0,1]$.

\paragraph{Phase 1.} %Let $\beta_\e$ be such that $\eta(\beta_\e,\beta_\e) = 2\e^2$. By definition $\lim_{\e^2 \rightarrow 0} \beta_\e = 0$. We require $\beta_\e < \kappa$. %Let $\gamma_0$ be such that $\eta(\gamma_0, |v_i|) = 2\e^2$. Note that $\gamma_0$ is a function of only $|v_i|$ and $\epsilon^2$. By definition $\lim_{\e^2 \rightarrow 0} \gamma_0 = 0$. We require $\e^2$ to be such that $\gamma_0 < \kappa$.
%Next, observe that there exists $\bias_i^{(1)}$ such that $|\bias_i^{(1)} - t_i| \leq \gamma_0$ as otherwise the cost incurred to $\|g - f_\theta\|^2$ on $[t_i -\gamma_0, t_i + \gamma_0]$ is at least $2\e^2$. 
We perform the following transformation
\begin{align*}
&\theta' \leftarrow \theta, \\
&\text{for every } i \in [1,c] \text{ such that } |v_i| \geq \e^{1/2} \text{ do } \\
& \ \ \ \ \ \ \ \text{for every } j \in [1,k] \text{ such that } |\bias_j^{(1)} - t_i| \leq 4\e^{1/3} \text{ do } \\
& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \bias_j^{(1)'} \leftarrow t_i,
\end{align*}
First note that every bias term is changed at most once because the intervals $[t_i - 4\e^{1/3},t_i + 4\e^{1/3}]$ don't intersect by assumption that $\e = o(\kappa^3)$. After this transformation the following holds. For every $i \in [1,c]$ we have $|f''_{\theta'} - v_i| \leq \e^{1/2}$
Observe that there exists $\bias_j^{(1)}$ such that $|\bias_j^{(1)} - t_i| \leq 4\e^{1/3}$ as otherwise the cost incurred to $\|g - f_\theta\|^2$ on $[t_i - 4\e^{1/3}, t_i + 4\e^{1/3}]$ is at least $\frac{64}{24}\e^2$. Note that we implicitly assumed that $\e < \frac14 \kappa^3$.

If we perform $\theta' \leftarrow \theta$, $\bias_i^{(1)'} \leftarrow t_i$ then $\|\theta - \theta'\|^2 \leq 16\e^{2/3}$ and 
\begin{align}
\|g - f_{\theta'}\|^2 
&\leq (\|g - f_\theta\| + \|f_\theta - f_{\theta'}\|)^2 \nonumber \\
&\leq (\e + 4 R^2 \e^{1/3})^2 && \text{$f$ is $R^2$-Lipschitz in $B_R$ with respect to $\bias_i^{(1)}$} \nonumber \\
&\leq \e^2 + 8\e^{4/3} R^2 + 16 R^4 \e^{2/3} \nonumber \\
&\leq 32 R^4 \e^{2/3}. \label{eq:onechangebound}
\end{align}

\com{I think we need to be careful here. All operations should be done at the same time}
We can view the transformation $\theta \rightarrow \theta'$ as an operation after which we have a new target function $g' = g - \w_i^{(2)} \w_i^{(1)} [x - t_i]_+$ and a new model for $f$, where we drop the $i$-th node. We apply the operation for as long as possible. This process terminates because in each step we remove one node. After the process is finished, if we denote the resulting set of parameters by $\theta''$, we have that for every $i \in [1,c]$ 
$$
|g''(t_i) - f_{\theta''}''(t_i)| < \e^{1/2}.
$$
Moreover by an analogous argument to \eqref{eq:onechangebound} we have that $\|g - f_{\theta''}\|^2 \leq O( k R^4 \e^{2/3} )$. We also have $\|\theta - \theta''\|^2 \leq O( k \e^{2/3})$.

\paragraph{Phase 2.} In this phase we change $\theta''$ further to obtain $\theta'''$ so that for every $j \in [1,c]$ $g''(t_j) = f_{\theta'''}''(t_j)$. Let $j \in [1,c]$ and let $S_j := \{i \in [1,c] : \bias_i^{(1)''} = t_j\}$. Let $i \in S_j$. We can change each of $w_i^{(2)''}, w_i^{(1)''}$ by at most $\e^{1/2}$ in the $\| \cdot \|^2$ norm so that $\sum_{i \in S_j} w_i^{(2)''}  w_i^{(1)''} = f_{\theta''}''(t_j) = g''(t_j)$. We apply such a transformation for every $j \in [1,c]$ and call the result $\theta'''$.  The result satisfies $\|\theta - \theta'''\|^2 \leq O(k \e^{2/3}) + 2k \e^{1/2} \leq O(k \e^{1/2})$, \begin{align*}
\|g - f_{\theta'''}\|^2 
&\leq O( k R^4 \e^{2/3} ) + k(R + \e^{1/4})^4 \e^{1/2} \\
&\leq O( k R^4 \e^{1/2} ) && \text{As } \e^{1/4} < R,
\end{align*}
where in the first inequality we used the fact that $f_\theta$ is $R$-Lipshitz with respect to $w_i^{(2)}$ in $B_R$. 

\paragraph{Phase 3.} Let $S := \{i \in [1,k] : \bias_i^{(1)'''} \in \{t_1, \dots, t_c \} \}$. Let $\theta^0$ represent a model where the weights are equal to $\theta'''$ but all nodes in $S$ are removed. We will change $\theta^0$ so that it represents the $0$ function. By definition
\begin{equation}\label{eq:fnorm}
\|f_{\theta^0}\|^2 \leq O( k R^4 \e^{1/2} ).
\end{equation}
We would like to now use Lemma~\ref{lem:hardwithbias}. But note that in this lemma we assumed that the model is $b^{(2)} +\sum_{i=1}^{k} w_i [x-b_i]_+$ not $ \sum_{i=1}^k \w_i^{(2)} \cdot \w_i^{(1)}\left[x - \bias_i^{(1)}\right]_+ + b^{(2)}$. Let $i \in [1,k]$. If $w_i$ was changed by $\delta^2$ in the $\|\cdot \|^2$ norm then we can realize the same effective change in $\w_i^{(2)} \cdot \w_i^{(1)}$ by changing the weight with the smaller absolute value by at most $\delta +\delta^2$ in the $\|\cdot\|^2$ norm. Thus Lemma~\ref{lem:hardwithbias} and \eqref{eq:fnorm} give us that there exists $\theta^*$ such that $f_{\theta^*} \equiv_{[0,1]} 0$ and
$$
\|\theta - \theta^*\|^2 \leq O \left(k^{5} R^{4/5} k^{1/5} R^{4/5} \e^{1/10} \right) \leq O \left(k^6 R^{8/5} \e^{1/10} \right).
$$

%Let $\eta' : [0,\infty)^2 \rightarrow [0,\infty)$ be a function that is defined as 
%\begin{equation}\label{eq:etaprimdef}
%\eta'(\gamma,\alpha) = \min_{b} \int_{0}^\gamma (\alpha x + b)^2 dx.
%\end{equation}
%Intuitively $\eta'(\gamma, \alpha)$ denotes the minimum $\ell_2$ difference on $[0, \gamma]$ between a linear function of slope $\alpha$ and the $0$ function. Solving \eqref{eq:etaprimdef} we get
%\begin{equation}\label{eq:etaprimvalue}
%\eta'(\gamma, \alpha) = \frac{\gamma^3 \alpha^2}{12}.
%\end{equation}

%We will show inductively on $k$ that we can change $\theta^*$ slightly to modify $f_{\theta^*}$ to represent the $0$ function on $[0,1]$.
%Let 
%$$j_{\text{1st}} := \argmax_{i \in [1,k]} \bias_i^{(1)^*},j_{\text{2nd}} := \argmax_{i \in [1,k] \setminus \{j_{\text{1st}}\}} \bias_i^{(1)^*}.$$
%Consider cases. 
%\paragraph{Case 1.} If $\bias_{j_{\text{2nd}}}^{(1)^*} - \bias_{j_{\text{1st}}}^{(1)^*} < \e^{1/8}$ then change $\bias_{j_{\text{1st}}}^{(1)^*} \mapsto \bias_{j_{\text{2nd}}}^{(1)^*}$. By doing that $\theta^*$ changed by at most $\e^{1/4}$ in the $\|\cdot \|^2$-norm, $\|f_{\theta^*}\|^2$ increased by at most $O(R^4\e^{1/4})$ because of the $R^2$-Lipshitz constant wrt to the bias terms and $f$ is constantly $0$ on $[0,\bias_{j_{\text{2nd}}}^{(1)^*}]$. After that operation we can think of the model as being represented with one fewer neuron and apply the inductive hypothesis. 
%\paragraph{Case 2.} If $\bias_{j_{\text{2nd}}}^{(1)^*} - \bias_{j_{\text{1st}}}^{(1)^*} \geq \e^{1/8}$ then the cost incurred to $\|f_{\theta^*}\|^2$ on $[\bias_{j_{\text{1st}}}^{(1)^*}, \bias_{j_{\text{2nd}}}^{(1)^*}]$ is equal to 
%\begin{align*}
%\frac12 \left(w_{j_{\text{1st}}}^{(2)^*} w_{j_{\text{1st}}}^{(1)^*}\right)^2 \left(\bias_{j_{\text{2nd}}}^{(1)^*} - \bias_{j_{\text{1st}}}^{(1)^*}\right)^2  \geq \frac{\e^{1/4}}{2} \left(w_{j_{\text{1st}}}^{(2)^*} w_{j_{\text{1st}}}^{(1)^*}\right)^2. 
%\end{align*}
%By \eqref{eq:fnorm} we get that $\left(w_{j_{\text{1st}}}^{(2)^*} w_{j_{\text{1st}}}^{(1)^*}\right)^2  = O(k R^4 \e^{1/4})$. This means that we can change each of $w_{j_{\text{1st}}}^{(2)^*}, w_{j_{\text{1st}}}^{(1)^*}$ by at most $O(k R^4 \e^{1/8})$ in the $\|\cdot \|^2$-norm so that $w_{j_{\text{1st}}}^{(2)^*} w_{j_{\text{1st}}}^{(1)^*} = 0$. The change incurred to $\|f_{\theta^*}\|^2$ is at most  

%Let $T := \e^{1/2} \N \cap [0,1]$ be the set of thresholds spaced by $\e^{1/2}$. For every node change its bias term to the closest one in $T$, call the result $\theta^{*'}$. We have that $\|\theta^{*} - \theta^{*'}\|^2 \leq O( k \e)$ and, using the fact that the model is $R^2$-Lipshitz wrt to the bias parameters in $B_R$,  $\|f_{\theta^{*'}}\|^2 \leq O( k R^4 \e^{1/2} ) + O( k R^2 \e^{1/2}) \leq O(kR^4\e^{1/2})$. \com{The second term should have had $\e$}. 

%\paragraph{Phase 4.}
%The final step is to set all $w_i^{(1)}$'s in $\theta^{*'}$ to $0$, call the result $\theta^{*''}$. By definition $f_{\theta^{*''}} \equiv_{[0,1]} 0$, moreover by Lemma~\ref{lem:l2lwrbnd} we get
%$$
%\|\theta^{*''} -\theta^{*'}\|^2 \leq \frac{\|f_{\theta^{*''}}\|^2}{96 \Delta^3}\leq O(k R^4 \e^{1/2} \e^{-3/2}).
%$$


%By \eqref{eq:etavalue} we know that there exists $\zeta_\e = O()$ such that $\eta(\e^{1/2}, \zeta_\e) > \|f_{\theta^{*'}}\|^2$. Then, by definition, we have that for very $t \in T$, $|f_{\theta^{*'}}''(t)| < \zeta_\e$ as otherwise the contribution to $\|f_{\theta^{*'}}\|^2$ at $[t - \beta_\e, t + \beta_\e]$ would be too big. We perform one final operation to create $\theta^{*''}$, where for every $t \in T$ we change the weights of nodes with bias terms equal to $t$ so that $f_{\theta^{*''}}''(t) = 0$. This operations satisfies $\|\theta^{*''} - \theta^*\|^2 \leq 100k \beta_\e^2 + k \zeta_\e$ This finishes our construction. We analyze the contributions of all the changes:
%\begin{equation}\label{eq:finalchange}
%\|\theta - \theta^{*''}\|^2 \leq 3k \beta_\e + 100 k \beta_\e^2 + k \zeta_\e.
%\end{equation}
%To show \eqref{eq:inclusion2} it is enough to notice that the RHS of \eqref{eq:finalchange} goes to $0$ when $\e \rightarrow 0$. To see that recall that $\lim_{\e \rightarrow 0} \beta_\e = 0$. By definition of $\zeta_\e$ we also have $\lim_{\beta_\e \rightarrow 0} \zeta_\e = 0$.
\end{comment}
To finish the proof we bound
\begin{align}
&\chi^\#(g, U([0,1])) \nonumber \\
&= \lim_{\epsilon \rightarrow 0}  \frac{\log[\prob_\theta\{\theta: \|g - f_\theta\|^2 \leq \e^2 \}]}{\log(\e)} \nonumber \\
&\stackrel{(1)}{=} \lim_{R \rightarrow \infty} \lim_{\epsilon \rightarrow 0}  \frac{\log[\prob_\theta\{\theta: \|g - f_\theta\|^2 \leq \e^2, \|\theta\|_2 \leq R \}]}{\log(\e)} \nonumber \\
&\stackrel{(2)}{\geq} \lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log \left(\vol( (A + B_{C(k,R) \e^{1/5}}) \cap B_R \cap \supp(P) ) \max_{\theta \in B_R} P(\theta) \right)}{\log(\e)}   \nonumber \\
&\stackrel{(3)}{=} \lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log \left(\vol( (A + B_{C(k,R) \e^{1/5}}) \cap B_R \cap \supp(P)) \right)}{\log(C(k,R) \e^{1/5})} \cdot \frac{\log(C(k,R) \e^{1/5})}{\log(\e)} \nonumber \\
%&= \lim_{R \rightarrow 
%\infty} \lim_{\e \rightarrow 0} \frac{\log \left(\vol( (A + B_{C(k,R) \e^{1/5}}) \cap B_R \cap \supp(P)) \right)}{\log(C(k,R) \e^{1/5})} \cdot \left(\frac{\log(C(k,R))}{\log(\e)} + \frac1{5} \right) \nonumber \\
&\stackrel{(4)}{=} \frac1{5}  \lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log \left(\vol( (A + B_{C(k,R) \e^{1/5}}) \cap B_R \cap \supp(P)) \right)}{\log(C(k,R) \e^{1/5})} \nonumber \\
&= \frac1{5} \codim_{P}(A_g), \nonumber
\end{align}
where in (1) we assumed that the two quantities are equal, in (2) we used \eqref{eq:inclusion2}, in (3) we used  $\lim_{\e \rightarrow 0} \frac{\max_{\theta \in B_R} P(\theta)}{\log(\e)} = 0$ and in (4) we used $\lim_{\e \rightarrow 0} \frac{\log(C(k,R)\e^{1/5})}{\log(\e)} = \frac15$. The second bound reads
\begin{align}
&\chi^\#(g, U([0,1])) \nonumber \\
&= \lim_{R \rightarrow \infty} \lim_{\epsilon \rightarrow 0}  \frac{\log[\prob_\theta\{\theta: \|g - f_\theta\|^2 \leq \e^2, \|\theta\|_2 \leq R \}]}{\log(\e)} \nonumber \\
&\stackrel{(1)}{\leq} \lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log \Big(\vol( (A + B_{R^2 \e}) \cap B_R \cap \supp(P)) \cdot \min_{\theta \in B_R \cap \supp(P)} P(\theta) \Big)}{\log(\e)}   \nonumber \\
&\stackrel{(2)}{=} \lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log(\vol( (A + B_{R^2 \e}) \cap B_R \cap \supp(P)))}{\log( R^2 \e)} \cdot \frac{\log(R^2 \e)}{\log(\e)} \nonumber \\
&= \codim_{P}(A_g), \nonumber
\end{align}
where in (1) we used \eqref{eq:inclusion1} and in (2) we used $\min_{\theta \in B_R \cap \supp(P)} P(\theta) > 0$, which is true because $B_R$ is compact.

%Using this argument inductively we can argue that there exist $f_{\theta''}$ such that $\|g - f_{\theta''}\|^2 \leq 2 k \e R^2\gamma_0 + k R^4\gamma_0^2$, $\|\theta - \theta''\|^2 \leq k \gamma_0^2$ and the set of thresholds of $f_{\theta''}$ is equal to the set of thresholds of $g$. 

%The next step is to make sure that $f$ agrees with $g$ at the threshold points. Let $i \in [1,c]$ the cost incurred to $\|g - f_{\theta''}\|^2$ is lower bounded by a function of $|g(t_i) - f_{\theta''}(t_i)\|^2$ that is independent of $\e$. Thus we can change $w_i$'s by 


%Now consider a node for which $b_i^{(1)''} = t_c$ and $w_i^{(1)''} > 0$. This node exists unless then we can change the bigger of $w_i^{(1)''}, w_i^{(2)''}$ by at most $\frac{2\e + 2k R^4 \gamma_0^3}{(1- t_c)^2}$ such that $g$ agrees with $f$ on $[t_c,1]$ perfectly. 
\end{proof}

Proof of Lemma~\ref{lem:cchangesco-dimension}

\begin{proof}
Let $\ths$ and $\slopes$ denote the vectors of $t_i$'s, and $v_i$'s respectively. Note that if for $i \in [1,c]$ we define  $b_i^{(1)} := t_i$, $w_i^{(2)} := \frac{v_i}{w_i^{(1)}}$ and $b^{(2)} := b$ then for every $x \in [0,1]$
$$
g(x) = \sum_{i=1}^c w_i^{(2)} \cdot w_i^{(1)} \left[x - b_i^{(1)} \right]_+ + b^{(2)}.
$$
Moreover if the neurons $i \in [c+1,k]$ are inactive on $[0,1]$, that is if $b_i^{(1)} > 1$ for all $i > c$, then $g \equiv_{[0,1]} f_\theta$, i.e. functions $g$ an $f_\theta$ agree on $[0,1]$. If we denote by $\bias_{[p,q]}$ the restrictions of $\bias$ to coordinates $p,\dots,q$, then for $\e < \max(t_1, t_2 -t_1, \dots, t_c, 1 - t_c)$ we can write
\begin{align}
&(A_g + B_\e) \cap B_R \cap \supp(P) \nonumber \\
&\supseteq \left\{ \theta : \|\bias_{[1,c]} - \ths\|^2 \leq \frac{\e^2}{3}, \bias_{[c+1,k]} \in [1,M]^{k-c}, \|\w^{(2)} \w^{(1)} - \slopes\|^2 \leq \frac{\e^2}{3}, (b^{(2)} - b)^2 \leq \frac{\e^2}{3} \right\} \cap B_R. \label{eq:inclusionlwrbnd}
\end{align}
Now we will estimate $\vol(\{\w : \|\w^{(2)} \w^{(1)} - \slopes\|^2 \leq \e^2 \} \cap B_R)$.
%\begin{align}
%&\vol\left(\left\{w^{(1)},w^{(2)} \in \R : (w^{(2)}w^{(2)} - v)^2 \leq \e^2 \right\} \cap B_R \right) \nonumber \\ 
%&\leq 2\int_{|v|/R}^{R} \frac{2\e}{w^{(1)}} \ dw^{(1)} \nonumber \\
%&= 4\e(\log(R) - \log(|v|/R)) = 4\e(2\log(R) - \log(|v|))
%\end{align}
If $k=1$ and $R^2 > 5|v_1|$:
\begin{align}
&\vol\left(\left\{w^{(1)},w^{(2)} \in \R : (w^{(2)}w^{(2)} - v)^2 \leq \e^2 \right\} \cap B_R \right) \nonumber \\ 
&\geq 2\int_{|v|^{1/2}}^{2|v|^{1/2}} \frac{2\e}{w^{(1)}} \ dw^{(1)} \nonumber \\
&= 4\e(\log(2|v|^{1/2}) - \log(|v|^{1/2})) = 4\log(2) \e. \label{eq:onedimensioncase}
\end{align}
Bound from \eqref{eq:onedimensioncase} generalizes to higher dimensions. If $R^2 > 5\|\slopes\|^2$ then
\begin{equation}\label{eq:boundonslopes}
\vol(\{\w : \|\w^{(2)} \w^{(1)} - \slopes\|^2 \leq \e^2 \} \cap B_R) \geq \kappa \e^c,
\end{equation}
where $\kappa$ is independent of $\e$, $\kappa$ depends only on the volume of balls in $\R^c$ and the constants $4\log(2)$ from \eqref{eq:onedimensioncase}. Now we can lower-bound the co-dimension
\begin{align*}
\codim_P(A_g) &=
\lim_{R \rightarrow 
\infty} \lim_{\e \rightarrow 0} \frac{\log(\vol( (A_g + B_\e) \cap B_R \cap \supp(P)))}{\log(\e)} \\
&\leq \lim_{\e \rightarrow 0} \frac{\log \left(\kappa' (\frac{\e}{\sqrt{3}})^c \cdot (M-1)^{k-c} \cdot \kappa (\frac{\e}{\sqrt{3}})^c \cdot \frac{2\e}{\sqrt{3}} \right)}{\log(\e)} && \text{By \eqref{eq:boundonslopes} and \eqref{eq:inclusionlwrbnd}} \\
&= 2c+1,
\end{align*}
where similarly as before $\kappa'$ is a constant independent of $\e$.

Now we will show an inequality in the other direction. Assume towards contradiction that $\codim(A_g) < 2c+1$. This means that there exists $\theta \in \text{int}(\supp(P)), f_\theta = g$ and $u_1, \dots, u_{3k+1-2c} \in \R^{3k+1}$ linearly independent such that $\theta + \text{ConvHull}(u_1, \dots, u_{3k+1-2c}) \subseteq A_g$. Fix one such $\theta$.

Next observe that 
\begin{equation}\label{eq:globalbiasagrees}
b^{(2)} = b.
\end{equation}
Moreover
\begin{equation}\label{eq:allthresholds}
\{t_1, \dots, t_c\} \subseteq \{\bias_1^{(1)},\dots, \bias_k^{(1)} \},
\end{equation}
because if there was $t_i \not\in \{\bias_1^{(1)},\dots, \bias_k^{(1)} \}$ then $f''_\theta(t_i) = 0$ but $g''(t_i) = v_i \neq 0$. For every $i \in [1,k]$ define $S_i := \{j \in [1,k] : \bias_j^{(1)} = \bias_i^{(1)} \}$. Note that for every $i \in [1,k]$ such that $\bias_i^{(1)} = t_j$ for some $j \in [1,c]$ we have:
\begin{equation}\label{eq:slopesthesame}
%\sum_{p \in S_i} \w_p^{(2)} \cdot \w_p^{(1)} = v_j.
\sum_{p \in S_i} \w_p^{(2)} \cdot \w_p^{(1)} = \begin{cases} 
            v_j \mbox{,} & \mbox{if } \bias_i^{(1)} = t_j \\ 
            0 \mbox{,} & \mbox{if } \bias_i^{(1)} \in [0,1] \setminus \{t_1, \dots, t_c\}
        \end{cases} 
\end{equation}
If not then let $i_0$ be such that $\bias_{i_0}^{(1)}$ is the minimal one such that \eqref{eq:slopesthesame} doesn't hold. Note that then $g \not\equiv_{\left[\bias_{i_0}^{(1)}, \bias_{i_0}^{(1)} + \delta \right]} f_\theta$, where $\delta > 0$ is small enough so that $\left\{\bias_1^{(1)},\dots, \bias_k^{(1)} \right\} \cap (\bias_{i_0}^{(1)} , \bias_{i_0}^{(1)} + \delta) = \emptyset$. 
Now observe that \eqref{eq:globalbiasagrees}, \eqref{eq:allthresholds} and \eqref{eq:slopesthesame} give us locally at least $2c+1$ linearly independent equations around $\theta$ which contradicts with $\theta + \text{ConvHull}(u_1, \dots, u_{3k+1-2c}) \subseteq A_g$. Thus $\codim(A_g) \geq 2c+1$. 
\end{proof}

Next we give a helpful fact.

\begin{fact}\label{fact:productdensity}
Let $X,Y$ be two independent random variables distributed according to $\mathcal{N}(0,\sigma_w^2)$. Then for every $a_0 \in \R$ we have that the density of $XY$ at $a_0$ is equal to
\begin{equation}\label{eq:smallfact}
f_{XY}(a_0) = \frac{1}{2\pi \sigma_w^2} \int_{-\infty}^{+\infty} e^{-\frac{1}{2\sigma_w^2}\left(w^2 + \frac{a_0^2}{w^2}\right)} dw = \frac{1}{\sqrt{2\pi \sigma_w^2}}e^{-\frac{|a_0|}{\sigma_w^2}}.
\end{equation}
\end{fact}

Proof of Lemma~\ref{lem:complexityforonechange}

\begin{proof}
To prove the lemma we estimate the probability of $f_\theta$'s close to $g_1$. Without loss of generality assume that $a > 0$.

\paragraph{Upper bound.} We can represent $g_1$ with a single node $i$ by assigning $\sqrt{a}$ to the outgoing weight ($\w^{(2)}_i$), $\sqrt{a}$ to the incoming weight ($\w^{(1)}_i$) of this node, the bias term ($\bias^{(1)}_i$) to $t$ and $b^{(2)}$ to $b$. The bias terms of all other nodes lie in $(1,M]$, i.e. they are inactive in the interval $[0,1]$.

These are exact representations of the function but to compute a lower bound on the probability we should also consider functions that are close to $g_1$. We can change $\w_i^{(1)}, \w_i^{(2)}, \bias_i^{(1)}$ by a little bit and still have a function that satisfies $\|g_1 - f_{\theta}\|^2 \leq \e^2$. We claim that the target probability is lower bounded by
\begin{equation}\label{eq:probabilitylowerbound}
 \left( \frac{\e}{2} \frac{1}{\sqrt{2\pi\sigma_w^2}} e^{-\frac{10 a}{9 \sigma_w^2}} \right) \cdot \left(\frac{9\e} {20 M a} \right) \cdot \left( \frac{\e}{40} \frac{1}{\sqrt{2\pi\sigma_b^2}} e^{-\frac{(|b| + \frac{\e}{40})^2}{2\sigma_b^2}} \right) \cdot \left( \frac{M-1}{M} \right)^{k-1}.
\end{equation}
We arrive at this expression by noting the following facts. By \eqref{eq:smallfact} and the assumption that $a \geq 20\e$ the probability that $\w_i^{(2)} \w_i^{(1)} = a \pm \frac{\e}{2}$ is lower bounded by $\frac{\e}{2} \frac{1}{\sqrt{2\pi\sigma_w^2}} e^{-\frac{10 a}{9 \sigma_w^2}}$. The probability that $\bias_i^{(1)} = t \pm \frac{9\e}{20a}$ is equal $\frac{9\e} {20M a}$. The probability that $b^{(2)} = b \pm \frac{\e}{40}$ is lower bounded by $\frac{\e}{40} \frac{1}{\sqrt{2\pi\sigma_b^2}} e^{-\frac{(|b| + \frac{\e}{40})^2}{2\sigma_b^2}}$. The last term is the probability that all other nodes have bias terms in $[1,M]$. Their weights can realm over the whole space and these nodes don't affect the function on $[0,1]$. We claim that all functions of this form satisfy $\|g_1 - f_{\theta}\|^2 \leq \e^2$. We bound the pointwise difference of $g_1$ and $f_\theta$ in $[0,1]$, i.e. for every $x \in [0,1]$
\begin{align*}
&f_\theta(x) = b + \left(\w_i^{(2)}\w_i^{(1)} \pm \frac{\e}{2}\right)\left[x - \left(\bias_i^{(1)} \pm \frac{9\e}{20a}\right)\right]_+ \pm \frac{\e}{40} \\
&= b + \w_i^{(2)}\w_i^{(1)}\left[x - \left(\bias_i^{(1)} \pm \frac{9\e}{20a}\right)\right]_+ \pm \frac{\e}{2}\left[x - \left(\bias_i^{(1)} \pm \frac{9\e}{20a}\right)\right]_+ \pm \frac{\e}{40} \\
&= b + \w_i^{(2)}\w_i^{(1)}\left[x - \bias_i^{(1)}\right]_+ \pm \w_i^{(2)}\w_i^{(1)} \frac{9\e}{20a} \pm \frac{\e}{2}\left(1 + \frac{9\e}{20a}\right) \pm \frac{\e}{40} \\
&= b + \w_i^{(2)}\w_i^{(1)}\left[x - \bias_i^{(1)}\right]_+ \pm \frac{9\e}{20} \pm \frac{\e}{2}\left(\frac{21}{20} + \frac{9\e}{20a}\right) && \text{As } \w_i^{(2)}\w_i^{(1)} = a \\
&= b + \w_i^{(2)}\w_i^{(1)}\left[x - \bias_i^{(1)}\right]_+ \pm \e && \text{As } a \geq 20\e, \\
\end{align*}
which implies that for such representations $\|g_1 - f_{\theta}\|^2 \leq \e^2$. From \eqref{eq:probabilitylowerbound} we get an upper bound on the sharp complexity 
\begin{align}
&\chi^\#( g_1, \e^2) \nonumber \\
&\leq -\log\left[ \left( \frac{\e}{2} \frac{1}{\sqrt{2\pi\sigma_w^2}} e^{-\frac{10 a}{9 \sigma_w^2}} \right) \cdot \left(\frac{9\e} {20 M a} \right) \cdot \left( \frac{\e}{40} \frac{1}{\sqrt{2\pi\sigma_b^2}} e^{-\frac{(|b| + \frac{\e}{40})^2}{2\sigma_b^2}} \right) \cdot \left( \frac{M-1}{M} \right)^{k-1}\right] \nonumber \\
&\leq \frac{10}{9} \left(\frac{a}{\sigma_w^2} + \frac{|b|}{\sigma_b^2} \right) + \log\left(M a \right) - (k-1) \log(1 - 1/M) + \log(2\pi \sigma_w \sigma_b) + 7 - 3 \log \left(\e \right)\nonumber  \\
&\leq \frac{10}{9} \left(\frac{a}{\sigma_w^2} + \frac{|b|}{\sigma_b^2} \right) + \log\left(M a \right) - (k-1) \log(1 - 1/M) + 10 - 3 \log \left(\e \right). &&\text{As } \sigma_b^2 \leq \frac{1}{\sigma_w^2} \nonumber \\
&\leq 2\left(\frac{a}{\sigma_w^2} + \frac{|b|}{\sigma_b^2} \right) + 11 - 3\log(\e),\label{eq:uprbndonechange} 
\end{align}
where in the last inequality we used that $\log(x) < x/2, \ \log(1+x) < x$ for $ x> 0$ and the assumption $k \leq M \leq \frac{1}{\sigma_w^2} $.
%Substituting $M = k, \sigma_w^2 = 1/k$ we get
%\begin{align}
%&\chi^\#(\dist_x, g_1, \e^2) \nonumber \\
%&\leq \frac{10}{9} ak + \log\left(a k \right) + 7 + (k-1) \log \left( \frac{k}{k-1}\right) - 3\log \left(\e \right) \nonumber \\
%&\leq \frac{10}{9} ak + \log\left(a k \right) + 4 - 3\log \left(\e \right) && \log(1+x) < x \text{ for } x>0 \nonumber \\
%&\leq 2 ak + 4 - 3\log \left(\e \right) && \log(x) < x/2 \text{ for } x>0 \label{eq:uprbndonechange} 
%\end{align}
%We can finish the proof by further upper bounding the complexity $
%\chi^\#(\dist_x, g_1, \e) \leq a k + 3\log\left(k \sqrt{a} \right) - 2  -3 \log \left(\e \right)
%$ using the assumption $a \geq \frac{\log(k)}{k}$. 
%- this assumption can be understood as considering functions $g_1$ sufficiently different from constant $0$ function. With that we arrive at 
%\begin{equation}\label{eq:finalonechange}
%\chi^\#(\dist_x, g_1, \e) \leq 2a k  - 3 \log \left(\e \right).
%\end{equation}

\begin{note}
Observe that according to Corollary~\ref{lem:cchangeslimit} we have that $\chi^\#(g_1, \dist_x) \leq 3$. Recall that $\chi^\#(g_1, \dist_x) = \lim_{\e \rightarrow 0} -\chi^\#(g_1, \dist_x, \e^2)/\log(\e)$. This means that, at least approximately, if we took the bound from \eqref{eq:uprbndonechange}, divided it by $-\log(\e)$ we would get an upper bound on  $\chi^\#(g_1, \dist_x)$. This would yield for us $\chi^\#(g_1, \dist_x) \leq 3$, as all other terms go to $0$ when $\e \rightarrow 0$. 
%We might wonder why this discrepancy for the limiting complexity exists. The reason is that we fixed the weights for the $i$-th node to be $\sqrt{a}$ and $\sqrt{a}$ but as we've seen in the proof of Lemma~\ref{lem:complexitytodimension} the only restriction on the weights is that their product equals $a$. If we accounted for that we would arrive at $\chi^\#(\dist_x, g_1) \leq 2$. However our aim now is to focus on the dependence of $\chi^\#$ on $a$, so this discrepancy becomes inconsequential for us as it starts playing a role for very small $\e$.
\end{note}


\paragraph{Lower bound.} There are other $\theta$'s that represent the function approximately. For example we could represent $g_1$ with more than $1$ node, by \say{spreading} the change of slope $a$ over many nodes. Another possibility is that a number of nodes with the same bias terms $t \neq b \in [0,1]$ effectively cancel out. These $\theta$'s contribute to the probability and decrease the complexity. 

%We want to claim however that the probability is dominated by the functions that we already took into account in \eqref{eq:finalonechange}.


%First note that there has to exist a node $i$ with a $b^{(1)}_i$ that is $\approx \e/a$ close to $t$. Otherwise the distance between $f_\theta$ and $g_1$ is to large. 

Let $\theta$ be such that $\|g_1 - f_\theta\|^2 \leq \e^2$ and let $S := \{i \in \{1, \dots, k\} : \bias_i^{(1)} \in [t - 9\e^{1/2}, t + 9\e^{1/2}] \}$. 
%Note that 
%$$
%f'_\theta \left(t+\frac{\e}{a}\right) = f'_\theta \left(t - \frac{\e}{a} \right) + \sum_{i \in S} \w_i^{(1)} \w_i^{(2)}.
%$$
Assume towards contradiction that $\sum_{i \in S} |\w_i^{(1)} \w_i^{(2)}| < a -\e^{1/4}$. This implies that either
\begin{equation}\label{eq:case1}
\sum_{i : \bias_i^{(1)} \in [t - 9\e^{1/2},t]} |\w_i^{(1)}\w_i^{(2)}| < f'_\theta(t) - \e^{1/4}/2
\end{equation}
or
\begin{equation}\label{eq:case2}
\sum_{i : \bias_i^{(1)} \in [t, t + 9\e^{1/2}]} |\w_i^{(1)}\w_i^{(2)}| < a - f'_\theta(t) -  \e^{1/4}/2.
\end{equation}
Assume that \eqref{eq:case2} holds. A similar argument covers \eqref{eq:case1}. Now consider two cases.

\paragraph{Case 1.} For all $x \in [t, t+ 3\e^{1/2}]$ we have $f_\theta(x) > a(x-t) + \e^{3/4}$. Then $\|g_1 - f_{\theta}\|^2 \geq 3\e^{1/2} \cdot \e^{3/2} > \e^2$ $\lightning$. 
\paragraph{Case 2.} There exists $x_0 \in [t,t+3\e^{1/2}]$ such that \begin{equation}\label{eq:x0property}
f_\theta(x_0) < a(x_0-t) + \e^{3/4}.
\end{equation}
By \eqref{eq:case2} we know that for all $x \in [t,t+9\e^{1/2}]$ we have $f'_\theta(x) < a - \e^{1/4}/2$. This means that $f_\theta(x)$ is below a linear function of slope $a-\e^{1/4}/2$ passing through $(x_0,f_\theta(x_0))$. Now we lower bound the error using the fact that $f_\theta$ is below this line. 
\begin{align}
&\|g_1 - f_{\theta}\|^2 \nonumber \\
&\geq \int_{x_0}^{t+9\e^{1/2}} \left[a(x-t) - \left(f(x_0) + \left(a - \e^{1/4}/2\right)(x-x_0)\right) \right]^2 \mathbbm{1}_{\{a(x-t) > f(x_0) + \left(a - \e^{1/4}/2\right)(x-x_0)\}} dx \label{eq:errolwrbnd}
%&\geq \int_{t+5\e^{1/2}}^{t+9\e^{1/2}} \left[ax - \left(f(x_0) + \left(a - \e^{1/4}/2\right)(x-x_0)\right) \right]^2 dx \\
%&\geq 
\end{align}
Note that the function $\delta(x) := a(x-t) - \left(f(x_0) + \left(a - \e^{1/4}/2\right)(x-x_0)\right)$ is increasing in $x$ and moreover 
\begin{align}
\delta(7\e^{1/2} + t) 
&=  a(x_0-t) - f(x_0) + \frac{\e^{1/4}}{2}(7\e^{1/2} + t - x_0) \nonumber \\
&\geq -\e^{3/4} + 2\e^{3/4} && \text{By \eqref{eq:x0property} and } x_0 < t + 3\e^{1/2} \nonumber \\
&\geq \e^{3/4}.\label{eq:fnctvalue}
\end{align}
Combining \eqref{eq:errolwrbnd} and \eqref{eq:fnctvalue} we get that
$$
\|g_1 - f_{\theta}\|^2 \geq 2\e^{1/2} \cdot \e^{6/4} > \e^2,
$$
which is a contradiction $\lightning$.

We arrived at a contradiction in both cases thus $\sum_{i \in S} |\w_i^{(1)} \w_i^{(2)}| \geq a -\e^{1/4}$. We claim that for every such $S$ the probability of $\sum_{i \in S} |\w_i^{(1)} \w_i^{(2)}| \geq a -\e^{1/4}$ is at most
\begin{equation}\label{eq:forafixedset}
\left( \frac{18\e^{1/2}}{M} \right)^{|S|} \int_{a - \e^{1/4}}^\infty x^{(|S| - 1)} \frac{2^{|S|}}{|S|!} \cdot \frac{2}{\sqrt{2\pi \sigma_w^2}}e^{-\frac{x}{\sigma_w^2}} \ dx.
\end{equation}

%\begin{equation}\label{eq:forafixedset}
%\int_{a - \e^{1/4}}^\infty x^{(|S| - 1)/2} \frac{2 \pi^{|S|/2}}{\Gamma(|S|/2)} \cdot \left( \frac{18\e^{1/2}}{M} \right)^{|S|} \cdot \sqrt{2\pi}e^{-\frac{x}{\sigma_w^2}} dx.
%\end{equation}
We arrive at this expression by noting that $x^{(|S| - 1)} \frac{2^{|S|}}{|S|!}$ is the area of an $\ell_1$ sphere of radius $x$ in $|S|$ dimensions; the density for $\w_i$'s satisfying $\sum_{i \in S} |\w_i^{(1)} \w_i^{(2)}| = x$ is, by Fact~\ref{fact:productdensity}, $\frac{2}{\sqrt{2\pi \sigma_w^2}}e^{-\frac{x}{\sigma_w^2}}$; the probability that a single bias term is equal to $t \pm 9\e^{1/2}$ is $ \frac{18\e^{1/2}}{M}$.

\begin{comment}
There has to exist $S \subseteq \{1, \dots, k\}$ such that $\sum_{i \in S} \w_i^{(1)} \w_i^{(2)} = a \ \pm \e$ and for all $i \in S$ we have $\bias_i^{(1)} = t \ \pm \e/\sqrt{a}$, i.e. there exists a subset of nodes whose slopes add up to approximately $a$ and their bias terms are around $t$. For every such $S$ the probability \com{What about $+1/-1$?} that $\sum_{i \in S} \w_i^{(1)} \w_i^{(2)} = a \ \pm \e$ and $\bias_i^{(1)} = t \ \pm \e/\sqrt{a}$ is approximately
\begin{equation}\label{eq:forafixedset}
a^{(|S| - 1)/2} \frac{2 \pi^{|S|/2}}{\Gamma(|S|/2)} \cdot \e \cdot \left( \frac{\e}{\sqrt{a}M} \right)^{|S|} \cdot \sqrt{2\pi}e^{-\frac{a}{\sigma_w^2}}.
\end{equation}
We arrive at this expression by noting that $a^{(|S| - 1)/2} \frac{2 \pi^{|S|/2}}{\Gamma(|S|/2)}$ is the area of a sphere of radius $\sqrt{a}$ in $|S|$ dimensions, multiplying it by thickness $\e$; the density for $\w_i$'s satisfying $\sum_{i \in S} \w_i^{(1)} \w_i^{(2)} = a$ is by \eqref{eq:smallfact} $\sqrt{2\pi}e^{-\frac{a}{\sigma_w^2}}$; the probability that a single bias term is equal to $t \pm \e/\sqrt{a}$ is $ \frac{\e}{\sqrt{a}M}$.
\end{comment}

Now we upper bound the probability of all these functions by taking a union bound over sets $S$. We get 
\begin{align}
&\prob_\theta \left[\|g_1 - f_{\theta}\|^2 \leq \e^2 \right] \nonumber \\
&\leq \sum_{S \subseteq \{1,\dots,n\}} \int_{a - \e^{1/4}}^\infty x^{(|S| - 1)} \frac{2^{|S|}}{|S|!} \cdot \left( \frac{18\e^{1/2}}{k} \right)^{|S|} \cdot \sqrt{\frac{2}{\pi \sigma_w^2}}e^{-\frac{x}{\sigma_w^2}} \ dx && \text{By \eqref{eq:forafixedset} and $k \leq M$} \nonumber \\
&\leq \sqrt{\frac{2}{\pi \sigma_w^2}} \sum_{i=1}^k \int_{a - \e^{1/4}}^\infty \binom{k}{i} \frac{2^{i}}{i!} \left( \frac{1}{2k} \right)^{i} \cdot x^{i - 1} e^{-\frac{x}{\sigma_w^2}} \ dx  && \text{As } 18\e^{1/2} \leq \frac12 \nonumber \\
&\leq \sqrt{\frac{2}{\pi \sigma_w^2}} \sum_{i=1}^k \int_{a/2}^\infty \frac{x^{i-1}}{2^{i-1}} e^{- \frac{x}{\sigma_w^2}} \ dx && \text{As } \binom{k}{i} \leq k^i, i! \geq 2^{i-1}, a \geq 2\e^{1/4} \label{eq:probuprbnd}
\end{align}
For every $i \in [1,k]$ we can upper bound 
\begin{align}
&\int_{a/2}^\infty \frac{x^{i-1}}{2^{i-1}} e^{-\frac{x}{\sigma_w^2}} \ dx \nonumber \\
&\leq \int_{a/2}^2 e^{-\frac{x}{\sigma_w^2}} \ dx + \left[-\frac{x^i}{2^{i-1}} e^{-\frac{x}{\sigma_w^2}} \right]_2^\infty &&\text{As } \left(-x^i e^{-\frac{x}{\sigma_w^2}}\right)' \geq x^{i-1} e^{-\frac{x}{\sigma_w^2}} \text{ for } x \geq 2 \nonumber \\
&\leq \left[-\sigma_w^2 e^{-\frac{x}{\sigma_w^2}} \right]^2_{a/2}  + 2e^{-\frac{2}{\sigma_w^2}} \nonumber \\
&\leq \sigma_w^2 e^{-\frac{a}{2\sigma_w^2}} + 2e^{-\frac{2}{\sigma_w^2}} \nonumber \\
&\leq 3e^{-\frac{a}{2\sigma_w^2}} && \text{As } \sigma_w^2 \leq 1, a \leq 2 \label{eq:integralbnd}
\end{align}
Plugging \eqref{eq:integralbnd} back to \eqref{eq:probuprbnd} we get 
\begin{equation}\label{eq:probupr}
\prob_\theta \left[\|g_1 - f_{\theta}\|^2 \leq \e^2 \right] \leq \sqrt{\frac{18}{\pi}}\frac{k}{\sigma_w} e^{-\frac{a}{2\sigma_w^2}}.  
\end{equation}
With \eqref{eq:probupr} we can bound the sharp complexity
\begin{align*}
\chi^\#(\dist_x, g_1, \e^2) 
&\geq \frac{a}{2\sigma_w^2} - \log(k/\sigma_w) + \log(\sqrt{2\pi}) \\
&\geq \frac{a}{3\sigma_w^2} && \text{As } \Omega(\sigma_w^2\log(k/\sigma_w)) \leq |a|.
\end{align*}

\begin{comment}
\begin{align*}
&\prob_\theta \left[\|g_1 - f_{\theta}\|^2 \leq \e^2 \right] \\
&\leq \sqrt{2\pi} \e \sum_{S \subseteq \{1,\dots,n\}} e^{-ak} a^{(|S| - 1)/2} \frac{2 \pi^{|S|/2}}{\Gamma(|S|/2)} \cdot \left( \frac{\e}{\sqrt{a}k} \right)^{|S|} && \text{By \eqref{eq:forafixedset}, $k = M = 1/\sigma_w^2$} \\
&\leq \sqrt{2\pi} \e \sum_{i=1}^k e^{-ak} \binom{k}{i} a^{(|S| - 1)/2} \frac{2 \pi^{|S|/2}}{\Gamma(|S|/2)} \cdot \left( \frac{\e}{\sqrt{a}k} \right)^{|S|} \\
&\leq \sqrt{2\pi} \e \sum_{i=1}^k e^{-ak + i\log(\frac{k e}{i}) + \frac{i-1}{2} \log(a) - \frac{i}{2}\log(\frac{i}{2\pi e}) + i \log(\frac{\e}{\sqrt{a}k}) } && \text{As } \binom{k}{i} \leq \left(\frac{k e}{i}\right)^i \text{ and } \Gamma(x+1) \approx \sqrt{2\pi x}(x/e)^x \\
&\leq \sqrt{2\pi} \e \sum_{i=1}^k e^{-ak + i\log \left(\frac{\e  e \sqrt{2\pi e}}{\sqrt{a} i^{3/2}}\right) + \frac{i-1}{2} \log(a)} \\
&\leq \sqrt{2\pi} \e \sum_{i=1}^k e^{-\frac{ak}{2} + i\log \left(\frac{\e  e \sqrt{2\pi e}}{\sqrt{a} i^{3/2}}\right)} && \text{Because } \log(a) \leq a \text{ for } a > 0.
\end{align*}
Using the assumption  $\frac{\e e \sqrt{2 \pi e}}{\sqrt{a}} < 1$ we can upper bound it further
\begin{align*}
&\sqrt{2\pi}e^{-\frac{ak}{2}} \e \cdot \sum_{i=1}^k \left(\frac{\e  e \sqrt{2\pi e}}{\sqrt{a} i^{3/2}}\right)^i \\
&\leq \sqrt{2\pi}e^{-\frac{ak}{2}} \e \cdot 2 && \text{As } \sum_{i=1}^k \left(\frac{1}{i^{3/2}}\right)^i \leq \sum_{i=1}^k 2^{-i} \leq 2.
\end{align*}
Finally we get a lower bound for the complexity
\begin{align*}
\chi^\#(\dist_x, g_1, \e) 
&\geq \frac{ak}{2} -\frac12 \log(2\pi) - \log(\e) - \log(2) \\
&\geq \frac{ak}{2} - \log(\e) - 3.
\end{align*}
\end{comment}

\end{proof}


\end{document}
