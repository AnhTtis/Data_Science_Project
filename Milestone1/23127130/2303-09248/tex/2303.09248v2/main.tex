\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{eso-pic}

% Include other packages here, before hyperref.
\usepackage{booktabs}
% fred's own
% \usepackage[small,compact]{titlesec}
\newcommand{\PAR}[1]{\vskip4pt \noindent{\bf #1~}}

% choose either one
\usepackage[inline, shortlabels]{enumitem}
\usepackage{soul}
\usepackage{array,calc}
\usepackage{placeins}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{colortbl}
\usepackage[dvipsnames]{xcolor}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{makecell}
\usepackage{arydshln}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks]{hyperref}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\iccvfinalcopy % *** Uncomment this line for the final submission
\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Cross-Dimensional Refined Learning for Real-Time 3D Visual Perception from Monocular Video}

\author{Ziyang Hong \qquad C. Patrick Yue \\
Hong Kong University of Science and Technology \\
% Hong Kong \\
{\tt\small frederick.hong@connect.ust.hk \qquad eepatrick@ust.hk}}

\vspace{-3.2cm}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\includegraphics[width=1\linewidth]{figs/teaser.pdf}
    \captionof{figure}{
        \textbf{Comparison between the proposed approach and baselines.} 
        Our model is more accurate and coherent in real time, compared to two baseline methods with input from monocular video, Atlas~\cite{murez2020atlas} and NeuralRecon~\cite{sun2021neuralrecon} + Semantic-Heads. Real-time 3D perception efficiency $\eta_{3D}$ the higher the better. Color denotes different semantic segmentation labeling.
        \vspace{15pt}
    }\label{fig:teaser}
}]
\thispagestyle{empty}

% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
We present a novel real-time capable learning method that jointly perceives a 3D sceneâ€™s geometry structure and semantic labels.
Recent approaches to real-time 3D scene reconstruction mostly adopt a  volumetric scheme, where a Truncated Signed Distance Function (TSDF) is directly regressed.
However, these volumetric approaches tend to focus on the global coherence of their reconstructions, which leads to a lack of local geometric detail.
To overcome this issue, we propose to leverage the latent geometric prior knowledge in 2D image features 
by explicit depth prediction and anchored feature generation, to refine the occupancy learning in TSDF volume.
Besides, we find that this cross-dimensional feature refinement methodology can also be adopted for the semantic segmentation task by utilizing semantic priors.
Hence, we proposed an end-to-end cross-dimensional refinement neural network (CDRNet) to extract both 3D mesh and 3D semantic labeling in real time.
The experiment results show that this method achieves a state-of-the-art 3D perception efficiency on multiple datasets, which indicates the great potential of our method for industrial applications.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Recovering 3D geometry and semantics of objects or environment scenes prevails these days with the advent of ubiquitous digitization. 
The digitization of the world where people live can not only help them better understand their environment scenes, but also enable robots to comprehend what they need to know about the world and therewith conducting assigned tasks.
Generally, with surrounding environment measurements as input, 3D reconstruction and 3D semantic segmentation are two key 3D perception techniques~\cite{dahnert2021panoptic, sun2020scalability, han2020live} in the computer vision society, which enable a wide range of applications, including digital twins~\cite{jiang2022ditto, bozic2021neural}, virtual/augmented reality (VR/AR)~\cite{newcombe2011kinectfusion, sun2021neuralrecon}, building information modeling~\cite{mahmud2020boundary, vanegas2010building}, and autonomous driving~\cite{cao2022monoscene, li2022reconstruct}.

Tremendous research efforts have been made for 3D perception techniques. Based on the sensor types, researches on 3D perception can be divided into two main streams, namely active range sensors that capture surface geometry information and RGB cameras that capture texture with perspective projection. 
Originated from KinectFusion~\cite{newcombe2011kinectfusion}, the commodity RGB-D range sensor is used to measure depth data first and then fuse it into Truncated Signed Distance Function (TSDF) volume for 3D reconstruction. Although the follow-up depth-based TSDF fusion methods~\cite{weder2020routedfusion, weder2021neuralfusion, azinovic2022neural, xu2022hrbf, sommer2022gradient} achieve detailed dense reconstruction result, they suffer from global incoherence due to the lack of sequential correlation, the tendency of noise disturbance due to redundant overlapped calculations, and the incapability of semantic deduction due to the lack of texture features.

On the other hand, as camera-equipped smartphones become readily available with built-in inertial measurement units, recent advances have emerged to explore 3D perception with RGB cameras on mobile devices. The problem of reconstructing 3D geometry with posed RGB images input only is referred to as multi-view stereo (MVS).
Existing methods for MVS that are based on deep learning, tend to adopt a volumetric scheme by directly regressing the TSDF volume~\cite{murez2020atlas, stier2021vortx, choe2021volumefusion, sun2021neuralrecon} either as a whole or in fragments. 
However, these volumetric learning methods extract 3D geometric feature representation simply from the back projection of 2D image features, resulting in the mismatch to the 2D information priors for the predicted 3D reconstruction. Moreover, the intrinsic end-to-end learning manner and the lack of local details on the reconstructed mesh of these volumetric schemes result in an inferior semantic deduction based on its 3D reconstruction prediction.

What's worse, these learning-based methods tend to store their entire computational graphs in memory for aggregation and require prohibitive 3D convolution operations~\cite{murez2020atlas, rich20213dvnet, stier2021vortx}, which keeps them from being deployed on robots due to the real-time and low-latency requirements in SLAM. 
% It is necessary to maintain low computation overhead for 3D perception to achieve real-time capability.
These limitations motivate our key idea to utilize 2D explicit predictions to further impose a light-weight feature refinement on the 3D features input in a sparse manner, while keeping the global coherence within the fragments. Unlike these preceding learning-based volumetric works, we conjecture that the utilization of 2D prior knowledge coming out of explicit predictions as a latent feature refinement plays a significant role in learning the feature representation 
in 3D perception. In addition, the feature refinement brought by 2D explicit prediction can be operated within the fragment input for keeping the computation redundancy and thus overhead low, while having the global coherence by correlating different fragments to extract the target 3D semantic mesh.

In this paper, we propose a novel framework, \textit{CDRNet}, to accomplish both 3D meshing and 3D semantic labeling tasks in real-time.
Our key contributions are as follows.
\begin{itemize}[itemsep=0pt,topsep=3pt,leftmargin=*]
    \item We propose a novel, end-to-end trainable network architecture, which cross-dimensionally refines the 3D features with the prior knowledge extracted from the explicit estimations of depths and 2D semantics.
    \item The proposed cross-dimensional refinements yield more accurate and robust 3D reconstruction and semantic segmentation results. We highlight that the explicit estimations of both depths and 2D semantics serve as efficient yet effective prior knowledge for 3D perception learning.
    \item To achieve real-time 3D perception capability, our approach performs both geometric and semantic localized updates to the global map.
    We present a progressive 3D perception system that is capable of real-time interaction with input data streaming from cellphones with a monocular camera.
\end{itemize}

%------------------------------------------------------------------------
\section{Related Work}
\label{sec:related_work}
\input{sections/2_related_work}

%------------------------------------------------------------------------
\section{Methods}
\label{sec:methods}
\input{sections/3_methods.tex}

%------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
\input{sections/4_experiments}

%------------------------------------------------------------------------
\vspace{-0.15cm}
\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed a lightweight volumetric method, \textit{CDRNet}, that leverages the 2D latent information about depths and semantics as the feature refinement to handle 3D reconstruction and semantic segmentation tasks effectively. We demonstrated that our method has real-time 3D perception capabilities, and justified the significance of utilizing 2D prior knowledge when solving 3D perception tasks.
% Extensive experiments on various datasets
Experiments on multiple datasets justify the 3D perception performance improvement of our method compared to prior arts. From the application point of view, the scalability of \textit{CDRNet} supports the notion that 2D priors should not be disregarded in 3D perception tasks and opens up new avenues for achieving real-time 3D perception using input data from readily accessible portable devices such as smartphones and tablets.

\PAR{Acknowledgements:}
This work is in part supported by Bright Dream Robotics (BDR) and the HKUST-BDR Joint Research Institute Funding Scheme under Project HBJRI-FTP-005 (OKT22EG06).

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}