Given a posed image sequence $\mathbf{I}$, our goal is to extract a 3D mesh model that can represent both \textbf{3D geometry} and \textbf{3D semantic labeling}, i.e., 3D meshing with vertices $\mathcal{K}\in\mathbb{R}^3$, surfaces $\mathcal{G}\in\mathbb{N}^3$, and its corresponding 3D semantic labeling $\mathcal{S}\in \mathbb{N}$.
% Each voxel contains a  and a  of our interest.
We achieve this goal by jointly predicting TSDF value $T\in [-1, 1]$ and semantic label $S\in\mathbb{N}$ for each voxel, and then extracting the mesh with the marching cubes \cite{lorensen1987marching}.
Meanwhile, our proposed method aims at establishing a real-time capable deep learning model for these two 3D perception tasks. To quantitatively evaluate the efficiency of conducting these two tasks simultaneously, we define a 3D perception efficiency metric $\eta_{3D}$ by involving frames per second (FPS) in runtime, as shown in Sec. \ref{subsec:datasets_metrics}.

The proposed network architecture is illustrated in Fig. \ref{fig:arch}.
In Sec. \ref{subsec:fragment}, we introduce the joint fragment learning on depth, 2D semantic category, intermediate TSDF, and occupancy using key frames input, for the following cross-dimensional refinements of TSDF and 3D semantics. For each fragment, the geometric features are progressively extracted in a coarse-to-fine hierarchy using binomial inputs GRU to build the learned representations of 3D.
Sec. \ref{subsec:cdr} describes the cross-dimensional refinements for 3D features that refines 3D features with anchored features and semantic pixel-to-vertex correspondences enabled by the depth and 2D semantic predictions, which helps the learning of not only the TSDF value,
% from the latent geometric information in the extracted features
but also the 3D semantic labeling in a sparsified manner. We also present the implementation details including loss design in Sec. \ref{subsec:implementations}. 
Specifications of the network are elaborated in the supplement.

\subsection{Sparse Joint Fragment Learning in a Coarse-to-Fine Manner}\label{subsec:fragment}
Given the inherent nature of great sparsity in the ordinary real-world 3D scene, we utilize sparse 3D convolutions to efficiently extract the 3D feature from each input scene.
However, the memory overhead of processing a 3D scene is still prohibitive, thus we fragment the whole 3D scene and progressively handle each of them,  to further release the memory burden of holding up the huge 3D volume data.
Inspired by~\cite{murez2020atlas, gu2020cascade, sun2021neuralrecon, stier2021vortx, rich20213dvnet}, we adopt a coarse-to-fine learning paradigm for the sparse 3D convolutions to effectively exploit the representation of 3D features in multiple scales.
In each stage of the hierarchy, the raw features in the fragment bounding volume (FBV) is extracted from a GRU by correlating local features and global feature volume.

\PAR{FBV Construction by Image Features.}
Following~\cite{wang2018mvdepthnet, sun2021neuralrecon}, we select a set of key frames as the input sequences out of a  monocular RGB video by querying on each frame's pose, namely the relative translation and optical center rotation with empirical thresholds, $\theta_{key}$ and $t_{key}$. Key frames $\mathbf{I}$, camera intrinsics $\mathbf{K}$, and transform matrices $\mathbf{T}\in SE(3)$ which is an inversion of the camera pose, are all wrapped into a fragment $\mathbf{F}_i=\{\mathbf{I}_{i,j},\mathbf{K}_{i,j},\mathbf{T}_{i,j} \}_{j=1}^{N_k}$ as the input to the network, where $i$, $j$, and $N_k$ denote the fragment index, the key frame index, and the number of key frames in each fragment, respectively.

Once the fragment $\mathbf{F}_i$ is constructed, it is processed by a 2D feature extractor pyramid to extract image features. In the decoder part of the extractor pyramid, three different resolutions of feature maps are extracted sequentially as $\mathcal{P}_s\in\{P_2, P_3, P_4\}$, where the suffix notation of $P$ denotes the scaling ratio level in $\log_2$ similar to~\cite{lin2017feature}. The extracted feature $\mathcal{P}_s$ is then back-projected into a local 3D volume, according to the projection matrix of each frame in $\mathbf{F}_i$. We hereby define FBV as the current local volume $\mathcal{F}_{s,i}=\{T_{s,i}^{x\times y\times z}, S_{s,i}^{x\times y\times z}\}$ that is conditioned on the pyramid layers $\mathcal{P}_s$, where all the 3D voxels that are casted in the view frustums of current $\mathbf{F}_i$ are included.
% Each voxel contains a TSDF value $T\in [-1, 1]$ and a semantic label $S\in\mathbb{N}$ of our interest.

\PAR{Initial Depth and 2D Semantics Learning.}
With the fine feature $P_2$ as input, we build up differentiable homography fronto-parallel planes for the coarse-level depth prediction $\hat{D}_4$. Likewise, 2D semantics prediction $\hat{S}_4^{2D}$ is extracted with a pointwise convolutional decoder as the 2D semantic head using $P_2$. The resolution gap between the input and output feature map provides generalizability. 
The initial depth estimation is retrieved from the features using a light-weight multi-view stereo network via plane sweep~\cite{yao2018mvsnet}. For each source feature map $x$ in $P_2$, we conduct the planar transformation $\mathbf{x}_j \sim \mathbf{H}_j(d) \cdot x$, where ``$\sim$" denotes the projective equality and $\mathbf{H}_j(d)$ is the homography of the $j^{\text{th}}$ key frame at depth $d$. The $j^{\text{th}}$ homography\footnote{For brevity's sake, the transformation from homogeneous coordinates to Euclidean coordinates in the camera projection is omitted here.} in a given fragment input $\mathbf{F}_i$ is defined as:
\begin{equation}
	\mathbf{H}_j(d)
	= d \cdot \mathbf{K}_j \cdot (\mathbf{T}_j \cdot \mathbf{T}_1^{-1}) \cdot \mathbf{K}_1^T
    \enspace.
\end{equation}
To measure the similarity after conducting homography warping, we calculate the variance cost of $\mathbf{x}_j$ and further process it with an encoder-decoder-based cost regularization network. The output logit from the regularization network is treated as the depth probability on each plane and the \textit{soft argmin}~\cite{yao2018mvsnet} is conducted to have initial depth predictions.

\PAR{Geometric and Semantic GRU Fusion.}
Meanwhile, as the 2D features are extracted in different resolutions, they are back-projected from each of the pyramid level in $\mathcal{P}_s$ into raw geometric 3D features $\mathcal{V}_s\in\{V_2, V_3, V_4\}$, which are further sparsified by sparse 3D convolutions.
To improve the global coherence and temporal consistency of the reconstructed 3D mesh, following~\cite{sun2021neuralrecon}, we first correlate the sparse geometric feature $\mathcal{V}_s$ in the current $\mathcal{F}_{s,i}$ using GRU, with the local FBV hidden states $H_{s,i-1}$ whose information coming from all of the previous fragments $\mathcal{F}_{s,i^\prime}, i^\prime<i$ and coordinates are masked to be the same as $\mathcal{V}_s$. Such correlation outputs a temporal-coherent local feature $L_{s,i}$ for each stage $s$, which is used to generate dense occupancy intermediate $o_{s,i}$, and passed to the 2D-to-3D cross-dimensional refinements. The global feature volume for the entire scene $G_{s,i}$ is fused by $G_{s,i-1}$ and $L_{s,i}$ given the coordinates of $\mathcal{V}_s$ as masks, and update $H_{s,i}$.
Unlike~\cite{sun2021neuralrecon}, we reuse the same parameters in GRU to process the back-projected and upsampled 3D semantic features to generalize better for the semantic prediction $\hat{S}$ in the current FBV.
This is because inputting TSDF and semantic features sequentially into GRU enables its selective fusion across modalities, thus the feature extracted from the hidden state incorporates more semantic information, as pointed out in~\cite{roldao20223d}. 

For the sake of learning 3D features consistently between scales, we update $\mathcal{V}_s$ at each stage by fusing with the upsampled $L_{s+1,i}$. Inspired by the \textit{meta data} mechanism proposed in~\cite{sayed2022simplerecon}, we further concatenate sparse features, with sparse TSDF, occupancy and semantics after masking with $o_{s,i}$, as the meta feature $L_{s+1,i}$ to be upsampled.
We found the inclusion of semantic information in the hidden state of GRU helps build up a good starting point for the upcoming feature refinements, which is verified in the ablation.

\subsection{2D-to-3D Cross-Dimensional Refinements}\label{subsec:cdr}
The raw coherent features from GRUs lack detailed geometric descriptions, leading to unsatisfactory meshing and semantic labeling results. 
% To this aim, 
To overcome these issues, we propose to leverage the 2D feature that is latent after incorporating the learning of depth and semantic frame for the refinement purposes. We notice that with
the learning of depth and 2D semantics, the 2D features now reside in the latent space which can generalize to more accurate
3D geometry and semantics via cross-dimensional refinements.

\PAR{2D-to-3D Prior Knowledge.}
Consider a probabilistic prior in the latent space of the output coherent feature coming from GRU, which accounts for the prior knowledge that the pixel information in both depth predictions and 2D semantic predictions should produce high confidence matching with regard to their own 3D representations. The prior conditioned 3D feature for both perception tasks is defined as: 
\begin{equation}
% \nonumber\\[-37pt]
\label{eq:x-prior}
    X_{prior} = f(L_{s,i})= f\bigl(H_{s,i} (\mathcal{V}_s,H_{s,i-1}\mid\mathcal{F}_{s,i})\bigr)
	 \enspace,
\end{equation}
where $f(\cdot)$ is the 2D-to-3D feature refinement process for either 3D meshing or 3D semantic labeling, whose input is $L_{s,i}$ extracted from $\mathcal{V}_s$ and $H_{s,i-1}$ given $\mathcal{F}_{s,i}$. We borrow the notation of $H_{s,i}$ to be a constructor function $H_{s,i}(\cdot)$ indicating GRU. For each voxel in $\mathcal{F}_{s,i}$, both TSDF and semantic labeling predictions can be formulated as:
\begin{equation}
% \nonumber\\[-37pt]
\label{eq:prior-both}
\hat{I}_{s,i} = \epsilon h\bigl(H_{s,i}(\mathcal{V}_s, H_{s,i-1}\mid\mathcal{F}_{s,i})\bigr) + (1-\epsilon)X_{prior} 
% \\\mathcal{L} = & \zseta_2 H_i + (1-\zeta_2)X_{prior-semseg}
	 \enspace,
\end{equation}  % is it \times symbol here? Relate with the supplement derivation, and highlight the derivation is in the supplement, here.
where $\hat{I}_{s,i}\in\mathcal{F}_{s,i}$ is the refined prediction; $\epsilon$ is a random variable for the respective prior, which is jointly learned by the feature refinement modules representing the
2D-to-3D priors and the GRU network trained with maximum likelihood estimation losses; $h(\cdot)$ is the prediction head. The proof of Eq. (\ref{eq:prior-both}) can be found in the supplement.

The key insight is that the voxels back-projected from either depth prediction or semantic label prediction of the input images has strong evidence on its 3D counterparts.
We hereby define anchored voxels $\alpha_i$, as those voxels in $\mathcal{F}_{s,i}$ that are incorporating all the back-projected depth points, given the fact that the 3D reconstruction task is essentially an inverse problem. 
We propose two progressive feature refinement modules to learn the high confidence of the refined features in latent space such that a more accurate $\hat{I}_{s,i}$ can be extracted with the help of 2D-to-3D prior knowledge.

\PAR{Depth-Anchored Occupancy Refinement.}
Unlike the volumetric methods~\cite{murez2020atlas, sun2021neuralrecon} that directly regress on the TSDF volume, we propose a novel module in each stage $s$ that can explicitly refine the initial depth, predict depths in resolutions, and further create the 3D anchored features with the depth prediction, as shown in Fig. \ref{fig:depth_anchor_refmnt}. The anchored feature is generated by 3D sparse convolutions with an anchored voxel 
% in the original FBV
on the occupancy intermediate $o_i$
\footnote{The universal stage suffix $s$ is hereinafter omitted for brevity.}.
\input{figs/overall_depth_refmnt_tex}

Intuitively, the anchored voxel has higher confidence of achieving a valid $o_i$ and $T_{s,i}$ close to zero. We imposed the anchored feature on the occupancy feature to reinforce the occupancy information brought by the depth prior.

Inspired by~\cite{chen2019point, rich20213dvnet}, we conduct PointFlow algorithm for each stage in the coarse-to-fine structure $\mathcal{V}_s$ to determine the depth displacement on the initial depth prediction such that finer depth prediction can be achieved. 
% We define $h$ hypothesis points for each depth pixel. 
Different from the PointFlow algorithm used in~\cite{rich20213dvnet}, we utilize the back-projected depth points from all $N_k$ views in the fragment to query an anchored voxel, which can be further aggregated with $o_i$. Fig. \ref{fig:pointflow} illustrates how these hypothesis points are selected and turned into depth displacement prediction, such that the anchored voxel can be generated.
\input{figs/pointflow_tex}
The anchored voxel index in the 3D volume is sparsified as a mask to update the occupancy prediction as $\hat{o}_i$ in the following:
\begin{equation}
    % \hat{o}^i = o^i \wedge \alpha_i
    \hat{o}_i = o_i \cap \alpha_i
    \enspace.
\end{equation}
The enhanced occupancy prediction $\hat{o}_i$ is used to condition the TSDF volume at the current stage to generate the refined $\hat{T}_i$, which is further sparsified with a light-weight pointwise convolution and upsampled to concatenate with $L_{s,i}$.

\PAR{Pixel-to-Vertex Matching Semantic Refinement.}
In addition to the depth anchor refinement, we propose a semantic cross-dimensional refinement which utilizes the semantic prior that lies in the 2D semantic prediction to have a refined 3D voxel semantic prediction, implemented as follows.
First, the 2D feature backbone learns the 2D semantic prior information that is useful for 3D voxel semantic labeling learning by incorporating the learning of 2D frame semantic labeling.
Second, the sparse 3D feature $L_{s,i}$ is passed to pointwise 3D convolution layers and comes up with the initial 3D voxel semantic labeling predictions in respective scales.
Third, to conduct the semantic feature refinement, we observed that there is a sole 3D voxel counterpart in $\mathcal{F}_{s,i}$ for each pixel on a 2D semantic prediction of $\mathbf{I}_{i,j}$, since 
the surface edges are encoded as vertices.
We define these vertices as the one-on-one matching correspondences to their camera-projected pixels, which are recorded in a matching matrix for masking the 2D features $\mathcal{P}_s$. 

The upper part of Fig. \ref{fig:cdr_matching} illustrates the design of the matching matrix that is used to correlate the pixel-vertex pairs for each frame $\mathbf{I}_{i,j}$ across all vertices in $\mathcal{F}_{s,i}$. We construct the matching matrix $\mathbf{M}=\{\overrightarrow{m}_{idx}\}^N_{idx=1}$ for each semantic labeling frame, where $N$ is the number of the vertices in the volume $\mathcal{F}_{s,i}$. Each column of the matching matrix $\mathbf{M}$ is defined as:
\begin{equation}
    \mathbf{M}(idx) = \overrightarrow{m}_{idx} = 
    \begin{bmatrix}
        u_{idx} \\
        v_{idx} \\
        \text{mask}
    \end{bmatrix}.
\end{equation}
For each column, each pixel-vertex pair recorded in the matching matrix, i.e., the $idx^{\text{th}}$ vertex in the 3D volume on the right-hand side of the upper part and its correspondence pixel on the left-hand side is recorded. The last entry of the pixel-vertex pair represents a mask which is recorded as valid when the 2D correspondence for $\mathbf{M}$ is in the current view frustum of the frame.

After the matching matrix $\mathbf{M}$ is constructed, it will be used for masking each of the feature map $\mathcal{P}_s$ with the $\log_2$ scale of $s$ to create a refined feature, whose voxel number is the same as the number of sparse 3D features, 
as shown in the lower part of Fig. \ref{fig:cdr_matching}.
Meanwhile, the coordinates of the sparse 3D features $L_{s,i}$ are mapped as the coordinate of the refined feature. 
By doing so, the underlying semantic information from the $\mathcal{P}_s$ can be incorporated by $L_{s,i}$, such that better 3D semantic prediction can be achieved.
Then we use the sparse pointwise convolution to extract its underlined feature from 2D semantics and concatenate it with $L_{s,i}$ to create $L_{s-1,i}$ with semantic information for the refinement in the next finer stage, so as to ensure the 2D semantic priors to have reliable refinement on the sparse coherent features.

\subsection{Implementation Details}\label{subsec:implementations}
Our model is implemented in PyTorch, trained and tested on an NVIDIA RTX3090 graphics card. We empirically set the optimizer as Adam without weight decay~\cite{loshchilov2018decoupled}, with an initial learning rate of 0.001, which goes through 3 halves throughout the training. The first momentum and second momentum are set to 0.9 and 0.999, respectively.
For key frame selection, following~\cite{wang2018mvdepthnet, sun2021neuralrecon}, we set thresholds $\theta_{key}$, $t_{key}$ and fragment input number $N_k$ as 15 degrees, 0.1 meters, and 9, respectively.
A fraction of FPN~\cite{lin2017feature} is adopted as the 2D backbone with its classifier as MNasNet~\cite{tan2019mnasnet}.
MinkowskiEngine~\cite{choy20194d} is utilized as the sparse 3D tensor library. More details are introduced in the supplement.
\input{figs/cdr_semseg_link_tex}
\PAR{Loss Design.}
Our model is trained in an end-to-end fashion except for the pre-trained 2D backbone. Since our target is to learn the 3D geometry and semantic segmentation of the surrounding scene given the posed images input, we regress the TSDF value with the mean absolute error (MAE) loss, classify the occupancy value with the binary cross-entropy (BCE) loss and the semantic labeling with cross-entropy (CE) loss as:
% \vspace{-0.2cm}
\begin{align}
% \nonumber\\[-37pt]
\label{eq:loss-3D}
\mathcal{L}_{3D} = &\sum^{4}_{s=2}
	\alpha_s \mathcal{L}_\text{MAE}(T_s, \hat{T}_s) + 
	\lambda \alpha_s \mathcal{L}_\text{BCE}(O_s, \hat{O}_s)\nonumber \\ & + 
	\beta_s \mathcal{L}_\text{CE}(S_s, \hat{S}_s)
	\enspace,
\end{align}
% \vspace{-0.1cm}
where $T$, $S$, and $O$ denote TSDF value, semantic labeling, and occupancy predictions. $\alpha_s$, $\beta_s$, and $\lambda$ are the weighting coefficients in different stages for TSDF volume, semantic volume, and positive weight for BCE loss, respectively. By doing so, the learning process stays most sensitive and relevant to the supervisory signals in the coarse stage, and less fluctuating as the 3D features become finer with the upsampling, after log-transforming the predicted and ground-truth TSDF value following~\cite{murez2020atlas}.

To conduct cross-dimensional refinements, we regress the depth estimation with MAE loss and classify the 2D semantic segmentation with CE loss:
\begin{align}
\label{eq:loss-2D-total}
    \mathcal{L}_{2D} = & \mathcal{L}_\text{MAE}(d_{init}, \hat{D}_{init}) + 
    \mathcal{L}_\text{CE}(S^{2D}_2, \hat{S}^{2D}_2)\nonumber \\ & + 
    \sum^{4}_{s=2} \gamma_s \mathcal{L}_\text{MAE}(D_s, \hat{D}_s)
	 \enspace,
 \end{align}
where $D$ and $\gamma_s$ denote depth and the weighting coefficient for depth estimation in different stages. We further wrap the losses into an overall loss $\mathcal{L}= \mathcal{L}_{3D} + \mu \mathcal{L}_{2D}$, where $\mu$ is the coefficient to balance the joint learning of 2D and 3D.
\input{figs/eval_3d_mesh}