\subsection{Datasets and Metrics}\label{subsec:datasets_metrics}
\input{tables/scannet_3dmesh}
We conduct the experiments on two indoor scene datasets, ScanNet (v2)~\cite{dai2017scannet} and SceneNN~\cite{hua2016scenenn}. 
The model is trained on the ScanNet train set, tested and reported on the ScanNet test set and further verified on SceneNN data set.
To quantify the 3D reconstruction and 3D semantic segmentation capability of our method, we use the standard metrics following~\cite{murez2020atlas, sun2021neuralrecon}. Completeness Distance (Comp.), Accuracy Distance (Acc.), Precision, Recall, and F-score, are used for 3D reconstruction, while mean Intersection over Union (mIoU) is used for 3D semantic segmentation.

To evaluate how much robustness a model can achieve while targeting 3D perception tasks in real time, we define the 3D perception efficiency metric $\eta_{3D}=\text{FPS}\times\text{mIoU}\times\text{F-score}$,
since F-score is regarded as the most suitable 3D metric for evaluating 3D reconstruction quality by considering Precision and Recall at the same time~\cite{murez2020atlas,sun2021neuralrecon,sayed2022simplerecon}. It is noteworthy that for fairness across methods, FPS for processing speed is measured in the inference across all captured frames in a given video sequence rather than key frames only, since the input is the same for different methods regardless of their key frame selection scheme.
\input{tables/scannet_3dsemseg}
\input{figs/eval_3d_semantics}
\subsection{Evaluation Results and Discussion}\label{subsec:eval_results}
\PAR{3D Perception.}
To evaluate the 3D perception capability, we mainly compare our methods against state-of-the-art works in two categories: volumetric 3D reconstruction and voxelized 3D semantic segmentation methods. 

For 3D reconstruction capability, we compare our proposed method with the canonical volumetric methods~\cite{murez2020atlas, sun2021neuralrecon} and several state-of-the-art 3D reconstruction methods with posed images input~\cite{rich20213dvnet, sayed2022simplerecon}. Fig. \ref{fig:exp-3dmesh-normal} demonstrates the superiority of our method in terms of 3D reconstruction by showing the 3D meshing results in normal mapping. Table \ref{tab:scannet-3d-mesh} shows that our method outperforms two main baseline methods in terms of 3D meshing accuracy.
We further compare both state-of-the-art depth estimation methods and volumetric methods in depth metrics in the supplement to justify from the depth extraction perspective.

For 3D semantic segmentation quality, we compare Atlas, NeuralRecon with semantic heads, and VoRTX with semantic heads with our methods in Table \ref{tab:scannet-2d-3d-semantics}. We augment three stages of MLP heads on top of the flattened 3D features to predict the semantic segmentation for both baselines. Due to its lack of 3D feature extraction, SimpleRecon, as one of the SOTA baselines, is intrinsically incapable of following this modification for semantics as well as being combined with our proposed cross-dimensional refinement techniques.
Table \ref{tab:scannet-2d-3d-semantics} shows that our method outperforms these two baselines. Besides mIoU for semantic segmentation, we include FPS and $\eta_{3D}$ for 3D perception efficiency in the comparison.
We also include two state-of-the-art 3D semantic segmentation methods, 3DMV~\cite{dai20183dmv} and BP-Net~\cite{hu2021bidirectional}. It shows that our method can achieve mIoU results nearly comparable to 3DMV, but with only RGB images as input.
Overall, our method achieves the best 3D semantic segmentation performance and highest 3D perception efficiency among all the volumetric methods.
Fig. \ref{fig:exp-3dsemantics} and Fig. \ref{fig:exp-3dscenenn} illustrate the 3D semantic labeling results. 
We found that the semantic information generation on VoRTX is unsatisfying, 
mostly caused by its bias on geometric features brought by the projective occupancy mentioned in \cite{stier2021vortx}.

\PAR{Efficiency.}
Since our main goal is to achieve real-time processing performance while solving 3D perception tasks, we compare the computational efficiency of our model against other RGB-input-only volumetric methods in Table \ref{tab:scannet-2d-3d-semantics}. The 3D perception efficiency metric $\eta_{3D}$ for several 3D semantic segmentation works are shown there.
We employ FPS, which is commonly used to measure efficiency for
2D-input 3D perception methods~\cite{murez2020atlas,sun2021neuralrecon,stier2021vortx}, as a metric to bring out and emphasize the nature of real-time system.
We also include the floating-point operations per frame (FLOPF) to
compare the learnable parameters' operations across different methods.
The superiority in $\eta_{3D}$ of our method manifests that it has better deployment potential for real-life 3D perception applications. From the human user's and robotic SLAM's points of view, our method greatly surpasses the threshold of being real-time, 90.17 FPS, as elaborated in the supplement. 
It shows that our method is more suitable for real-time industrial scenarios with input data from low-cost portable devices compared to baseline methods.

\input{tables/scenenn_3dsemseg}

\subsection{Ablation Study}\label{subsec:ablation_study}
To analyze the effectiveness of cross-dimensional refinement,
we present 3D perception efficiency $\eta_{3D}$ and its components of with different modifications in Table \ref{tab:ablations}. In other experiments above, we adopt (e) as our method.

\noindent\textbf{Binomial GRU Fusion.}
In (a), we remove the back-projected semantics input to GRU in the pipeline. Compared with (e), both F-score and mIoU of the removal degrade since no hidden semantic information from last FBV is fused with GRU anymore. Although FPS increases due to fewer computations, the efficiency $\eta_{3D}$ is worse.

\noindent\textbf{Depth Refinement.}
In (c), we remove the depth anchor refinement in the pipeline. The loss in F-score and mIoU manifests that the geometric feature without depth anchor refinement becomes inferior, which means depth anchor refinement can improve 3D reconstruction performance.
\input{tables/ablation}

\noindent\textbf{Semantic Refinement.}
We validate the semantic refinement in the pipeline by removing this module and, as shown in (d). The mIoU drops due to the insufficient learning information from semantic heads only. This result demonstrates the effectiveness of our semantic refinement scheme based on pixel-to-vertex matching for improving 3D semantic segmentation performance. We also experiment with no refinements but depth and 2D semantics learning setup in (b), which gives the highest FPS but not satisfying 3D perception performance.