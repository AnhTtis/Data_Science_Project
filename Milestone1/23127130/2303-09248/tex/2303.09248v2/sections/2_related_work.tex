\input{figs/cdrnet_arch_tex}
\PAR{Real-Time 3D Perception.} 
The prosperity of deep learning hardwares enables both inference and training at the edge~\cite{lecun20191, hong2022efficient}, thus it consolidates the foundation to deploy more and more learning-based 3D perception techniques in real time. KinectFusion~\cite{newcombe2011kinectfusion} first brought in the concept of handling 3D reconstruction tasks in real time with commodity RGB-D sensors. 
Han et al.~\cite{han2020live} presented a real-time 3D meshing and semantic labeling system similar to our work, however, depth measurements from RGB-D sensors are required as input in their work.
Pham et al.~\cite{pham2019real} built up 3D meshes with voxel hashing, and then fuse the initial semantic labeling with super-voxel clustering and a high-order conditional random field (CRF) to improve labeling coherence.
Menini et al.~\cite{menini2021real} extended RoutedFusion~\cite{weder2020routedfusion} by merging semantic estimation in its TSDF extraction scheme for each incoming depth-semantics pair.
NeuralRecon~\cite{sun2021neuralrecon} adopted sparse 3D convolutions and the gated recurrent unit (GRU) to achieve a real-time 3D reconstruction on cellphones, without the capability of semantic deduction. For depth estimation and semantic segmentation, there are also works achieving real-time processing capability~\cite{wang2018mvdepthnet, narita2019panopticfusion, pham2019real}.

\PAR{Voxelized 3D Semantic Segmentation.} 
The learning of semantic segmentation on the voxelized map started from~\cite{cavallari2016semanticfusion}, which extends TSDF fusion pipeline~\cite{newcombe2011kinectfusion} with per-pixel labels. 3DMV~\cite{dai20183dmv} and MVPNet~\cite{jaritz2019multi} further combined both depth and RGB modalities to train an end-to-end network with 3D semantics for voxels and point clouds, respectively. PanopticFusion~\cite{narita2019panopticfusion} performed map regularization based on adopting a CRF on the predicted panoptic labels. Atlas~\cite{murez2020atlas} utilized its extracted 3D features and passed them to a set of semantic heads for voxel labeling, the pyramid features are proven to have strong semantics at all scales than the gradient pyramid in nature, as proven in~\cite{lin2017feature}. BPNet~\cite{hu2021bidirectional} proposed to have a joint-2D-3D reasoning in an end-to-end learning manner. 
Two derivative works~\cite{menini2021real, huang2021real} of RoutedFusion incorporated semantic priors into their depth fusion scheme 
and removed their routing module for less overhead. 
However, none of these works utilize the prior knowledge within the estimated 2D semantics as a 3D feature refinement.

\PAR{Volumetric 3D Surface Reconstruction.} 
Volumetric TSDF fusion became prevalent for 3D surface reconstruction starting from the seminal work KinectFusion~\cite{newcombe2011kinectfusion} due to its high accuracy and low latency. A follow-up work, PSDF-Fusion~\cite{park2019deepsdf} augmented TSDF with a random variable to improve its robustness to sensor noise. Starting from DeepSDF~\cite{park2019deepsdf}, the learned representations of TSDF using depth input dominates the current fad. These learning-based substitutes~\cite{weder2020routedfusion, weder2021neuralfusion, bozic2021neural, azinovic2022neural, xu2022hrbf, sommer2022gradient, yu2022monosdf} to TSDF fusion achieve impressive 3D reconstruction quality compared to the baseline method with the availability of RGB-D range sensors.

Given the fact that range sensors have relatively higher cost and energy consumption than RGB cameras, MonoFusion~\cite{pradeep2013monofusion} is one of the first works to learn TSDF volume from RGB images by fusing the estimated depth into an implicit model. Atlas~\cite{murez2020atlas} started the trend of learning-based methods by a direct regression on TSDF volume. NeuralRecon~\cite{sun2021neuralrecon} achieved a real-time 3D reconstruction learning capability by utilizing sparse 3D convolutions and recurrent networks with key frames as input. TransformerFusion~\cite{bozic2021transformerfusion} and VoRTX~\cite{stier2021vortx} introduced transformers~\cite{vaswani2017attention} to improve the performance by more relevant inter-frame correlation. 
These learning-based methods prevail thanks to the availability of these general 2D feature extractors, such as FPN~\cite{lin2017feature} and U-Net~\cite{ronneberger2015u}. 2D information in RGB images can be effectively extracted and further utilized for constructing their 3D perception counterparts.

However, the learning of the explicit representations of 2D latent geometric features, such as depths and semantics, is \textbf{typically ignored} by all the prior arts. They only treat the 2D feature as an intermediate in the network and then conduct ray back-projection upon it, without considering the explicit representations for their 3D embodiment, which we found are significant prior knowledge for 3D perception.
To extract depth as the explicit 2D representation, VolumeFusion~\cite{choe2021volumefusion} and SimpleRecon~\cite{sayed2022simplerecon} performed local MVS and further fused it into TSDF volume with its customized network, while 3DVNet~\cite{rich20213dvnet} performed sparse 3D convolutions on the feature-back-projected point cloud. Different from above, our method extracts the 2D representations from light-weight network modules, including a portion of MVSNet~\cite{yao2018mvsnet} for depth and a simple 2D MLP head for 2D semantics, 
to conduct the 3D feature refinements. 
% Similar to~\cite{lengyel2021zero, lin2022deep}, 
The refinement incorporates the geometric and semantic prior information to improve the generalizability of our network by correlating the 2D representations in their 3D counterparts.

To the best of our knowledge, we present the very first learning-based method which uses posed RGB images input only to conduct 3D perception tasks in real time, including 3D meshing and semantic labeling.