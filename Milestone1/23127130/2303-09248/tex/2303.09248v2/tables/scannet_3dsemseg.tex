\begin{table}[t]
    % \centering
    \vspace{-0.2cm}
    \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{ccccccc} 
      \Xhline{3\arrayrulewidth}
      Method        & FPS $\uparrow$   & KFPS $\uparrow$  & FLOPF $\downarrow$ & mIoU $\uparrow$   & $\eta_{3D}$ $\uparrow$       \\ 
      \hline
      3DMV \cite{dai20183dmv}                 & 7.04    & N/A & 65.06G              & 44.2 & N/A                           \\
      BPNet \cite{hu2021bidirectional}        & 4.46    & N/A & 141.06G             & 74.9 & N/A                           \\
      \hline
      Atlas \cite{murez2020atlas}             & 66.3  & N/A                   & 267.04G    & 34.0 & 11.25                              \\
      NeuralRecon \cite{sun2021neuralrecon} + Semantics-Heads  & \textbf{228}                  & \textbf{30.9} & \textbf{42.38G}  & 27.9 & 32.82           \\
      VoRTX \cite{stier2021vortx} + Semantic-Heads              & 119   & 13.5                  & 150.23G    & 13.2  & 9.47                                \\
      Ours                                    & 158   & 21.4                  & 90.62G     & \textbf{39.1}  & \textbf{37.81}   \\ 
      \Xhline{3\arrayrulewidth}
      \end{tabular}
        }
    \caption{\textbf{Quantitative 3D voxel semantic segmentation and overall 3D perception results on ScanNet.} \textit{Upper:} Two representative state-of-the-art methods for semantic segmentation whose input requires either depth or 3D mesh, respectively. No key-frame selection and F-score are involved due to their input modality; \textit{Lower:} RGB-input-only volumetric methods.
    Key-frame FPS (KFPS) is measured with the same selection scheme across all methods. FLOPF is measured with PyTorch operation counter across operations of neural network's learnable modules.
}
    \label{tab:scannet-2d-3d-semantics}
    \vspace{-0.2cm}
\end{table}