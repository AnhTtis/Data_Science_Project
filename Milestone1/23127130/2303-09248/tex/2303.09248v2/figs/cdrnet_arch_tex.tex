\begin{figure*}[ht]
    \vspace{-1.2cm}
    \centering
       \includegraphics[width=\linewidth]{figs/cdrnet_arch.pdf}
       \caption{
           \textbf{Overview of CDRNet.} Posed RGB images from monocular videos are wrapped as fragment input for 2D feature extraction, which is used for both depth and 2D semantic predictions for cross-dimensional refinement purposes. 
           To learn the foundational 3D geometry before conducting refinements, the extracted 2D features are back-projected into raw 3D features $\mathcal{V}_s$ in different resolutions without any 2D priors involved.
           At each resolution, after being processed by the GRU, the output feature $L_s$ in the local volume is further fed into {\color{YellowOrange}Depth} and {\color{ProcessBlue}Semantics} refinement modules sequentially to have a 2D-prior-refined feature with better representations.
        %   At the fine stage, the output $\mathbf{S}_t^l$ is used to replace corresponding voxels in the global TSDF volume $\mathbf{S}_t^{g}$, yielding the final reconstruction at time $t$.
           }\label{fig:arch}
      \vspace{-0.2cm}
\end{figure*}