\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{eso-pic}

% Include other packages here, before hyperref.
\usepackage{booktabs}
% fred's own
% \usepackage[small,compact]{titlesec}
\newcommand{\PAR}[1]{\vskip4pt \noindent{\bf #1~}}

% choose either one
\usepackage[inline, shortlabels]{enumitem}
% \usepackage{enumitem}
% \usepackage{paralist}
\usepackage{soul}
\usepackage{verbatim}
\usepackage{array,calc}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{colortbl}
\usepackage[dvipsnames]{xcolor}
\usepackage{inconsolata}
\usepackage{floatrow}
\usepackage{gensymb}
\usepackage{makecell}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1430} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Cross-Dimensional Refined Learning for Real-Time 3D Visual Perception from Monocular Video}

\author{Ziyang Hong \qquad C. Patrick Yue \\
Hong Kong University of Science and Technology \\
% Hong Kong \\
{\tt\small frederick.hong@connect.ust.hk \qquad eepatrick@ust.hk}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% C. Patrick Yue\\
% Institution2\\
% Hong Kong University of Science and Technology \\
% {\tt\small secondauthor@i2.org}
}
\vspace{-2.2cm}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\includegraphics[width=1\linewidth]{figs/teaser.pdf}
    \captionof{figure}{
        \textbf{Comparison between the proposed approach and baselines.} 
        Our model is more accurate and coherent in real time, comparing to two baseline methods with input from monocular video, Atlas~\cite{murez2020atlas} and NeuralRecon~\cite{sun2021neuralrecon} + Semantic-Heads. Real-time 3D perception efficiency $\eta_{3D}$ the higher the better. Color denotes different semantic segmentation labeling.
        \vspace{15pt}
    }\label{fig:teaser}
}]

% \maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
We present a novel real-time capable learning method that jointly perceives a 3D sceneâ€™s geometry structure and semantic labels.
Recent approaches to real-time 3D scene reconstruction mostly adopt a  volumetric scheme, where a truncated signed distance function (TSDF) is directly regressed.
However, these volumetric approaches tend to focus on the global coherence of their reconstructions, which leads to a lack of local geometrical detail.
To overcome this issue, we propose to leverage the latent geometrical prior knowledge in 2D image features 
by explicit depth prediction and anchored feature generation, to refine the occupancy learning in TSDF volume.
Besides, we find that this cross-dimensional feature refinement methodology can also be adopted for the semantic segmentation task.
Hence, we proposed an end-to-end cross-dimensional refinement neural network (CDRNet) to extract both 3D mesh and 3D semantic labeling in real time.
The experiment results show that the proposed method achieves state-of-the-art 3D perception efficiency on multiple datasets, which indicates the great potential of our method for industrial applications.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Recovering 3D geometry of objects or environment scenes prevails these days with the advent of the ubiquitous digitization. 
The digitization of the world where people live can not only help them better understand their environment scenes, but also enable robots to comprehend what they need to know about the world and therewith conducting assigned tasks.
Generally, with surrounding environment measurements as input, 3D reconstruction and 3D semantic segmentation are two key 3D perception techniques~\cite{dahnert2021panoptic, sun2020scalability, han2020live} in the computer vision society, which enable a wide range of applications, including digital twins~\cite{jiang2022ditto, bozic2021neural}, virtual/augmented reality (VR/AR)~\cite{sun2021neuralrecon, xie2022planarrecon}, building information modeling~\cite{mahmud2020boundary, vanegas2010building}, and autonomous driving~\cite{cao2022monoscene, li2022reconstruct}.

Tremendous research efforts have been made for 3D perception techniques. Based on the sensor types, researches on 3D perception can be divided into two main streams, namely active range sensors that capture surface geometry information and RGB cameras that capture texture with perspective projection. 
Originated from KinectFusion~\cite{newcombe2011kinectfusion}, the commodity RGB-D range sensor is used to measure depth data first and then fuse it into TSDF volume for 3D reconstruction. Although the follow-up depth-based TSDF fusion methods~\cite{weder2020routedfusion, weder2021neuralfusion, azinovic2022neural, xu2022hrbf, sommer2022gradient} achieve detailed dense reconstruction result, they suffer from the global incoherence due to the lack of sequential correlation, the tendency of noise disturbance due to redundant overlapped calculations, and the incapability of semantic deduction due to the lack of texture features.

On the other hand, as the camera-equipped smartphones become readily available with the built-in inertial measurement units, 
% researchers
recent advances have emerged to explore 3D perception with RGB camera on mobile devices. The problem of reconstructing 3D geometry with posed RGB images input only is referred as multi-view stereo (MVS).
Existing methods for MVS methods based on deep learning tend to adopt a volumetric scheme by directly regressing the TSDF volume~\cite{murez2020atlas, stier2021vortx, choe2021volumefusion, sun2021neuralrecon} either as a whole or in fragments. 
However, these volumetric schemes lack local details on the reconstructed mesh and therefore result in inferior semantic deduction based on its 3D reconstruction prediction, and besides, these volumetric learning methods extract 3D geometrical feature representation simply from the back projection of 2D image features, resulting in the mismatch to the 2D information priors for the predicted 3D reconstruction. Moreover, due to the low-latency requirement in SLAM of robotics, it is also necessary to maintain low computation overhead for 3D perception to achieve real-time capability.

These limitations motivate our key idea to utilize 2D explicit predictions to further impose feature refinement on the 3D features input, while keeping the global coherence within the fragments. Unlike these preceding learning-based volumetric works, we argue that the utilization of 2D prior knowledge coming out of explicit predictions as a latent feature refinement plays a significant role in learning the feature representation 
in 3D perception. In addition, the feature refinement brought by 2D explicit prediction can be operated within the fragment input for keeping the computation redundancy and thus overhead low, while having the global coherence by correlating different fragments to extract the target 3D mesh.

In this paper, we propose a novel framework, \textit{CDRNet}, to accomplish both 3D meshing and 3D semantic labeling tasks in real-time, with the help of cross-dimensional feature refinement.
% \newpage
\noindent
Our key contributions are as follows.
\begin{itemize}[itemsep=0pt,topsep=3pt,leftmargin=*]
    \item We propose a novel, end-to-end trainable network architecture, which cross-dimensionally refines the 3D features with the prior knowledge extracted from the explicit estimations of depths and 2D semantics.
    \item The proposed cross-dimensional refinement yields more accurate and robust 3D reconstruction and semantic segmentation results. We highlight that the explicit estimations of both depths and 2D semantics serve as efficient yet effective prior knowledge for 3D perception learning.
    \item To achieve the real-time 3D perception capability, our approach performs both geometric and semantic localized updates to the global map.
    We present a progressive 3D perception system that is capable of the real-time interaction with the monocular camera on cellphones.
\end{itemize}

%------------------------------------------------------------------------
\section{Related Work}
\label{sec:related_work}
\input{sections/2_related_work}

%------------------------------------------------------------------------
\section{Methods}
\label{sec:methods}
\input{sections/3_methods.tex}

%------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
\input{sections/4_experiments}

%------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed a light-weight volumetric method, \textit{CDRNet}, that leverages the 2D latent information about depths and semantics as the feature refinement to handle 3D reconstruction and semantic segmentation tasks effectively. We demonstrated that our method has the capability to solve 3D perception tasks in real time, and justified the significance of utilizing 2D prior knowledge when solving 3D perception tasks.
% Extensive experiments on various datasets
Experiments on ScanNet justify the 3D perception performance improvement of our method comparing to prior arts. From the application point of view, the great scalability shown by \textit{CDRNet} proves that 2D priors should not be ignored in 3D perception tasks and further enables a new paradigm to achieve real-time 3D perception on personal commodity devices, such as cellphones and tablets.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}