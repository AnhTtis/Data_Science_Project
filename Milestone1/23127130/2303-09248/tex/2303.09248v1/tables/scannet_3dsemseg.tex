\begin{table}[t]
    % \centering
    \resizebox{1.0\textwidth}{!}{
        \begin{tabular}{ccccccc}
            \Xhline{3\arrayrulewidth}
            Method                                                  & mIoU $\uparrow$   & FPS $\uparrow$    & $\eta_{3D}\uparrow$   \\
            \hline  
            3DMV \cite{dai20183dmv}                                 & 44.2              & -                 & -               \\
            BPNet \cite{hu2021bidirectional}                        & 74.9              & -                 & -               \\
            % PointNet++ \cite{chen2019point}                         & 33.9              & 0.180               & 0.115               \\
            \hline  
            Atlas \cite{murez2020atlas}                             & 34.0              & 66.3              & 11.25                 \\
            NeuralRecon \cite{sun2021neuralrecon} + Semantic-Heads  & 27.9              & \textbf{228}      & 32.82             \\
            VoRTX \cite{stier2021vortx} + Semantic-Heads            & 13.2              & 119               & 9.79             \\
            Ours                                                    & \textbf{39.1}     & 158               & \textbf{37.81}        \\
            \Xhline{3\arrayrulewidth}
        \end{tabular} }
    \caption{\textbf{Quantitative 3D voxel semantic segmentation and overall 3D perception results on ScanNet.} Upper: two representative state-of-the-art methods for semantic segmentation whose input requires either depth or 3D mesh, respectively. Lower: RGB-input-only volumetric methods. Besides mIoU for semantic segmentation, we include FPS and $\eta_{3D}$ for 3D perception efficiency in the comparison. Our method achieves the best 3D semantic segmentation performance and highest 3D perception efficiency among all the volumetric methods.}
    \label{tab:scannet-2d-3d-semantics}
    \vspace{-0.2cm}
\end{table}