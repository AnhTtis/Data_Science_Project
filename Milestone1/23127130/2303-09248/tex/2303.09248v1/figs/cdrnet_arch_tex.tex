\begin{figure*}[ht]
    \vspace{-1.2cm}
    \centering
       \includegraphics[width=\linewidth]{figs/cdrnet_arch.pdf}
       \caption{
           \textbf{Overview of CDRNet.} Posed RGB images from monocular videos are wrapped as fragment input for 2D feature extraction, which are used for both depth and 2D semantic predictions for cross-dimensional refinement purpose. 
           To learn the foundational 3D geometry before conducting refinement, the extracted 2D features are back projected into raw 3D features, $F_s$ in different resolutions.
           In the each stage of 3D feature learning with occupancy considered, the fragment bounding volume (FBV) is extracted using the concatenation of upsampled feature from previous stage and back-projected feature at current stage as the input to GRU, and the reuse of upsampled 3D semantics. Raw extracted FBV is further fed into {\color{ProcessBlue}Semantics} and {\color{YellowOrange}Depth} refinement modules, such that the ultimate FBV results with refinements condition on the raw one.
        %   At the fine stage, the output $\mathbf{S}_t^l$ is used to replace corresponding voxels in the global TSDF volume $\mathbf{S}_t^{g}$, yielding the final reconstruction at time $t$.
           }\label{fig:arch}
      \vspace{-0.2cm}
\end{figure*}