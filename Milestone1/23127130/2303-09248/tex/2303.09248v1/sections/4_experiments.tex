\subsection{Datasets and Metrics}
\input{tables/scannet_3dmesh}
We conduct the experiments on two indoor scene datasets, ScanNet (v2)~\cite{dai2017scannet} and SceneNN~\cite{hua2016scenenn}. 
The model is trained on the ScanNet train set, tested and reported on the ScanNet test set and further verified on SceneNN data set.
To quantify the 3D reconstruction and 3D semantic segmentation capability of our method, we use the standard metrics following~\cite{murez2020atlas, sun2021neuralrecon}. Completeness Distance (Comp.), Accuracy Distance (Acc.), Precision, Recall, and F-score, are used for 3D reconstruction, while mean Intersection over Union (mIoU) is used for 3D semantic segmentation.

To justify the correctness of the reconstructed 3D mesh, we also measure standard 2D depth estimation quality to validate the 3D reconstruction result. To evaluate how much robustness a model can achieve while targeting solving 3D perception tasks in real time, we define the 3D perception efficiency metric $\eta_{3D}=\text{FPS}\times\text{mIoU}\times\text{F-score}$, % fps denition explicitly
since F-score is regarded as the most suitable 3D metric for evaluating 3D reconstruction quality by considering Precision and Recall at the same time. It is noteworthy that for fairness across methods, FPS for processing speed is measured in the inference across all captured frames in a given video sequence rather than key frames only, since the input sequence is the same for all different methods regardless of their key frame selection scheme.
% Further details about the metrics are introduced in the supplement.
\input{tables/scannet_3dsemseg}
\input{figs/eval_3d_semantics}
\subsection{Evaluation Results and Discussion}\label{subsec:eval_results}
\PAR{3D Perception.}
To evaluate the 3D perception capability, we mainly compare our methods against state-of-the-art works in two categories: a) volumetric 3D reconstruction methods, and b) voxelized 3D semantic segmentation methods. 
% Additionally, we compare with some real-time perception methods about the perception efficiency.
It is noteworthy that most of the baseline works are not real-time due to their focus on fine-grained performance.
% We finetuned all baseline methods and run them on the validation set of ScanNet to compare with ours.

For 3D reconstruction capability, we compare our proposed method with the canonical volumetric methods~\cite{murez2020atlas, sun2021neuralrecon} and several state-of-the-art 3D reconstruction methods with posed images input~\cite{rich20213dvnet, sayed2022simplerecon,  yu2022monosdf}. Fig. \ref{fig:exp-3dmesh-normal} demonstrates the superiority of our method in terms of 3D reconstruction by showing the 3D meshing results in normal mapping. Tab. \ref{tab:scannet-3d-mesh} shows that our method outperforms two main baseline methods in terms of 3D meshing accuracy.
We further compare both state-of-the-art depth estimation methods and volumetric methods in depth metrics in the supplement to justify from the depth extraction perspective.
Our method outperforms the two main volumetric baseline methods, and is comparable to the SOTA volumetric method, SimpleRecon~\cite{sayed2022simplerecon}, in terms of the extracted depth estimation quality.

For 3D semantic segmentation quality, we compare Atlas, NeuralRecon + Semantic-Heads, and VoRTX + Semantic-Heads with our methods in Tab. \ref{tab:scannet-2d-3d-semantics}. We augment three stages of MLP heads on top of the flattened 3D features to predict the semantic segmentation for both baselines. One of the SOTA baselines, SimpleRecon is intrinsically unable to follow this modification for semantics due to the lack of 3D feature extraction.
Tab. \ref{tab:scannet-2d-3d-semantics} shows that our method outperforms these two baselines. We also include two state-of-the-art 3D semantic segmentation methods, 3DMV~\cite{dai20183dmv} and BP-Net~\cite{hu2021bidirectional}. It shows that our method can achieve mIoU results nearly comparable to 3DMV, but with only RGB images as input. Since these two methods are too time-consuming to fit in the real-time requirement thus their FPS and $\eta_{3D}$ are not reported in Tab. \ref{tab:scannet-2d-3d-semantics}.
Fig. \ref{fig:exp-3dsemantics} illustrates the 3D semantic labeling results. 
We found that the semantic information generation on VoRTX is unsatisfying, 
mostly caused by its bias on geometrical features brought by the projective occupancy mentioned in \cite{stier2021vortx}.

\PAR{Efficiency.}
Since our main goal is to achieve real-time processing performance while solving 3D perception tasks, we compare the computational efficiency of our model against other RGB-input-only volumetric methods in Tab. \ref{tab:scannet-2d-3d-semantics}. The 3D perception efficiency metric $\eta_{3D}$ for several 3D semantic segmentation works are shown there. The superiority in $\eta_{3D}$ of our method manifests that it has better implementation potential for real-life 3D perception applications.
By achieving the highest 3D perception efficiency among the RGB-input-only volumetric methods, our method is proven to be more suitable for real-time industrial scenarios with low-cost portable devices, such as cellphones.

\input{tables/scenenn_3dsemseg}

\subsection{Ablation Study}\label{subsec:ablation_study}
To analyze the effectiveness of cross-dimensional refinement,
we present 3D perception efficiency $\eta_{3D}$ and its components of with different modifications in Tab. \ref{tab:ablations}. In other experiments, we adopt (e) as our method.

\PAR{Binomial GRU Fusion.}
In (a), we remove the back-projected semantics input to GRU in the pipeline. Compared with (e), both F-score and mIoU of the removal degrade since no hidden information from last FBV is fused with GRU anymore. Although FPS increases due to less computations, the efficiency $\eta_{3D}$ is worse.

\PAR{Depth Refinement.}
In (c), we remove the depth anchored refinement in the pipeline. The loss in F-score and mIoU manifests that the geometrical feature without depth anchored refinement becomes inferior, which means depth anchored refinement can improve 3D reconstruction performance.
\input{tables/ablation}

\PAR{Semantic Refinement.}
We validate the semantic refinement in the pipeline by removing this module and, as shown in (d). The mIoU drops due to the insufficient learning information from semantics heads only. This result demonstrates the effectiveness of our semantic refinement scheme based on pixel-to-vertex matching for improving 3D semantic segmentation performance. We also experiment with no refinements setup in (b), which gives the highest FPS but not satisfying perception performance.