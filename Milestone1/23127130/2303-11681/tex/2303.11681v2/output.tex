


\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{floatrow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{mathrsfs, eucal}
\newcommand{\conditioner}{\tau_\theta}
\newcommand{\R}{\mathbb{R}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\input CSMacrosV2.tex


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\newcommand{\diffmask}{\textup{DiffuMask}\xspace}
\newcommand{\Ours}{\textup{DiffuMask}\xspace}

% \newcommand{\cisdq}{\textsc{\textbf{C}i\textbf{SDQ}}\xspace}

% \usepackage[ruled]{algorithm2e}             
\usepackage{array}
\newcolumntype{I}{!{\vrule width 3pt}}
\newlength\savedwidth
\newcommand\whline{\noalign{\global\savedwidth\arrayrulewidth
                           \global\arrayrulewidth 2pt}%
                  \hline
                  \noalign{\global\arrayrulewidth\savedwidth}}
\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
                           \global\arrayrulewidth 0.5pt}%
                  \hline
                  \noalign{\global\arrayrulewidth\savewidth}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\hypersetup{
colorlinks=true,
linkcolor=black
}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
% \newcommand{\blue}[1]{\textcolor[RGB]{0,0,255}{#1}}



\usepackage{tabulary,multirow,overpic,xcolor,subfloat}

\definecolor{linkcolor}{HTML}{ED1C24}

\newcommand{\bd}[1]{\textbf{#1}}
\newcommand{\app}{\raise.17ex\hbox{$\scriptstyle\sim$}}
\newcommand{\ncdot}{{\mkern 0mu\cdot\mkern 0mu}}
\def\x{\times}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcommand{\dt}[1]{\fontsize{5pt}{0.1em}\selectfont (#1)}

\definecolor{Gray}{gray}{0.5}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother


\newcommand{\lmatch}[1]{{\cal L}_{\rm match}(#1)}

\def\eg{\emph{e.g.}}
\def\Eg{\emph{E.g.,}}
\def\ie{\emph{i.e.}}
\def\etal{\emph{et al.}}

\newcommand \footnoteONLYtext[1]
{
	\let \mybackup \thefootnote
	\let \thefootnote \relax
	\footnotetext{#1}
	\let \thefootnote \mybackup
	\let \mybackup \imareallyundefinedcommand
}


\iccvfinalcopy % *** Uncomment this line for the final submission


%\vspace{-4mm}
\def\iccvPaperID{4328} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{\Ours: 
Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models}

\author{Weijia Wu$^{1,3}$,
~~
Yuzhong Zhao$^2$,
~~
Mike Zheng Shou$^3$\footnote{$^*$ Corresponding author},
~~
Hong Zhou$^1$$^*$,
~~
Chunhua Shen$^{1,4}$\\[0.205cm]
\normalsize 
$^1$ Zhejiang University~~
$^2$ University of Chinese Academy of Sciences~~
$^3$ National University of Singapore ~~
$^4$ Ant Group
%
}

% \maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

\input{figures/fig1}

\begin{abstract}
\footnoteONLYtext{$^*$ Corresponding author}
Collecting and annotating images with pixel-wise labels is time-consuming and laborious.
%
% CS: By contrast, synthetic data is largely available from the generative model~(\eg, DALL-E, Stable Diffusion) freely and automatically.
In contrast, synthetic data can be freely available using a generative model~(\eg, DALL-E, Stable Diffusion).
%
% 
%
% However, there are existing two huge challenges for using synthetic data, \ie{} 1) How to obtain the precise mask; 2) Severe domain gap between synthetic and real data.  
%
In this paper, 
%we present \Ours, an automatic procedure to generate massive image and pixel-level semantic annotation without human effort.
we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the %pre-trained
Off-the-shelf 
Stable Diffusion model, which uses only text-image pairs during training. 
%
% relies on the power of the recent diffusion model, and
Our approach, 
%called
termed 
\Ours, exploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation.
%
\Ours uses text-guided cross-attention information to localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask.
%
The methods help to 
significantly 
reduce data collection and annotation costs.
%obviously.
%
% Our core contributions mainly include two aspects for two challenges:
%
% 1) \textbf{Precise Mask:} an effective strategy is designed to combine the pixel semantic affinity learning (global prototype) and Dense CRF (local relationship % from 
% defined by 
% color %,
% and distance of pixels) to obtain the precise mask. 
% %
% Besides, noise learning is used to filter noisy labels.
% %
% % We also employed noise learning to filter out noisy data with imprecise masks. 
%     %
% 2) \textbf{Domain Gap:} retrieval-based prompt and data augmentations, as two effective solutions, are designed to reduce the domain gap by enhancing the diversity of data.
% %
Experiments demonstrate that the existing segmentation methods trained on synthetic data of \Ours can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes).
%
%
For some classes (\eg{}, bird), \Ours presents %a
promising performance, close to the %SOTA
state-of-the-art 
result of real data (%less than
within \textbf{3\%} mIoU gap).
%
Moreover, in the open-vocabulary segmentation (zero-shot) setting, \Ours achieves new %SOTA
state-of-the-art 
results on the \texttt{Unseen} classes of VOC 2012.
%
The project website can be found at \href{https://weijiawu.github.io/DiffusionMask/}{\color{blue}{$\tt DiffuMask$}}.
\end{abstract}




%%%%%%%%% BODY TEXT
\section{Introduction}
Semantic segmentation is a fundamental task in vision, and existing data-hungry semantic segmentation models usually require a large amount of data with pixel-level annotations to achieve significant progress. 
%
Unfortunately, pixel-wise mask annotation is a labor-intensive and expensive process.
%
For example, labeling a single semantic urban image in Cityscapes~\cite{cordts2016cityscapes} can take up to 60 minutes, underscoring the level of difficulty involved in this task
%
% Cityscapes~\cite{cordts2016cityscapes} presents labeling a single semantic urban image can take around 60 minutes, highlighting the level of difficulty.
%
Additionally, in some cases, it may be challenging or even impossible to collect images due to existing privacy and copyright.
%
To reduce the cost of annotation, weakly-supervised learning has become a popular approach in recent years. This approach involves training strong segmentation models using weak or cheap labels, such as image-level labels~\cite{ahn2018learning,lee2021anti,wu2021embedded,xu2021leveraging,ru2021learning,ru2022learning}, points~\cite{akiva2021towards}, scribbles~\cite{lin2016scribblesup,zhang2021affinity}, and bounding boxes~\cite{lee2021bbam}.
%
Although these methods are free of pixel-level annotations, still suffer from several disadvantages, including low-performance accuracy, complex training strategy, indispensable extra annotation cost (\eg{}, edge), and image collection cost.


With the great development of computer graphics~(\eg{}, generative model), an alternative way is to utilize synthetic data, which is largely available from the virtual world, and the pixel-level ground truth can be freely and automatically generated.
%
DatasetGAN~\cite{zhang2021datasetgan} firstly exploits the feature space of a trained GAN and trains a shallow decoder to produce pixel-level labeling.
%
BigDatasetGAN~\cite{li2022bigdatasetgan} extends DatasetGAN to handle the large class diversity of ImageNet.
%
However, both methods suffer from certain drawbacks, the need for a small number of \textbf{pixel-level} labeled examples to generalize to the rest of the latent space and suboptimal performance due to imprecise generative masks.
%

Recently, large-scale language-image generation (LLIG) models, such as DALL-E~\cite{ramesh2022hierarchical}, and Stable Diffusion~\cite{rombach2022high}, have shown phenomenal generative semantic and compositional power, as shown in Fig.~\ref{fig:teaser}. 
%
Given one language description, the text-conditioned image generation model can create corresponding semantic things and stuff,
%
where visual and textual embedding are fused using spatial cross-attention.  
%
We dive deep into the cross-attention layers and explore how they affect the generative semantic object and structure of the image.
%
We find that cross-attention maps are the core, which binds visual pixels and text tokens of the prompt text.
%
%And 
Also, the cross-attention maps contain rich class~(text token) discriminative spatial localization information, which critically affects the generated image.



\begin{figure}
% 	\centering
	\begin{minipage}{0.98\linewidth}
% 		\centering
		\includegraphics[width=0.99\linewidth]{figures/fig2_attention1.pdf}
        \vspace{-0.2cm}
		\subcaption{Cross attention maps of different text tokens.}
		\label{fig:1a}	
	\end{minipage}
	\qquad
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth]{figures/fig2_attention2.pdf}
        \vspace{-0.2cm}
		\subcaption{Cross attention maps of different resolutions.}
		 \label{fig:1b}	
	\end{minipage}
        \qquad
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth]{figures/threshold2.pdf}
        \vspace{-0.2cm}
	\subcaption{Binarization 
 Mask with different thresholds $\gamma$ in Equ.~\eqref{eq:binarization}.}
		 \label{fig:1c}	
	\end{minipage}
        \vspace{-0.2cm}
	\caption{\textbf{Cross-attention maps of a text-conditioned diffusion model~(\ie{}, Stable Diffusion~\cite{rombach2022high}).} Prompt language: `\texttt{a horse on the grass}'.
}
\label{fig2_attention}
\end{figure}

\textbf{Can the attention map be used as mask annotation?}
%
Consider semantic segmentation~\cite{(voc)everingham2010pascal,cordts2016cityscapes}---a ‘good’ pixel-level semantic mask annotation should satisfy two conditions: (a) class-discriminative (\ie{}, localize and distinguish the categories in the image); (b) high-resolution, precise mask (\ie{}, capture fine-grained detail).
%
Fig.~\ref{fig:1b} presents a visualization of cross attention map between text token and vision.
%
$8\times8$, $16\times16$, $32\times32$, and $64\times64$, as four different resolutions, are extracted from different layers of the U-Net of Stable Diffusion \cite{rombach2022high}.
%
$8\times8$ feature map is the lowest resolution, including obvious class-discriminative location.
%
$32\times32$ and $64\times64$ feature maps include high-resolution and highlight fine-grained details.
%
The average map shows the possibility for us to use for semantic segmentation, where it is class-discriminative and fine-grained.
%
To further validate the potential of the attention map of the generative task, we convert the probability map to a binary map with fixed thresholds $\gamma$, and refine them with Dense CRF~\cite{krahenbuhl2011efficient}, as shown in Fig.~\ref{fig:1c}.
%
With the $0.35$ threshold, the mask presents a wonderful precision on fine-grained details~(\eg{}, foot, ear of the `\textit{horse}').



% \textbf{Can the cross attention map from text token be used as mask annotation?}
%
% In this paper, inspired by attention-related works~\cite{hertz2022prompt} and CAM-based segmentation task~\cite{selvaraju2017grad},
%
Based on the above observation, we present \Ours, an automatic procedure to generate a massive high-quality image with a pixel-level semantic mask.
% using the pre-trained Stable Diffusion~\cite{rombach2022high}.
%via exploiting the potential of attention map in diffusion model~(\eg{},  Stable Diffusion~\cite{rombach2022high}), namely \diffmask. 
%
Unlike DatasetGAN~\cite{zhang2021datasetgan} and BigDatasetGAN~\cite{li2022bigdatasetgan}, \diffmask does not require any pixel-level annotations. This approach takes full advantage of powerful zero-shot text-to-image generative models such as Stable Diffusion~\cite{rombach2022high}, which are trained on web-scale image-text pairs.
%
% Key to our approach is to exploit the feature space of a trained diffusion model and utilize cross attention map between a text vector and image to generate a high-quality pixel-level semantic mask.
%
% 1) \textbf{Precise Mask:} an effective strategy is designed to combine the pixel semantic affinity learning (global prototype) and Dense CRF (local relationship % from 
% defined by 
% color %,
% and distance of pixels) to obtain the precise mask. 
% %
% Besides, noise learning is used to filter noisy labels.
% %
% % We also employed noise learning to filter out noisy data with imprecise masks. 
%     %
% 2) \textbf{Domain Gap:} retrieval-based prompt and data augmentations, as two effective solutions, are designed to reduce the domain gap by enhancing the diversity of data.
\Ours mainly includes two advantages for two challenges: 1) \textit{Precise Mask.} An adaptive threshold of binarization is proposed to convert the probability map~(attention map) to a binary map, as the mask annotation.
%
Besides, noise learning~\cite{northcutt2021confident,song2022learning} is used to filter noisy labels.
% defined by 
% color %,
% and distance of pixels) to obtain the precise mask. 
% %
% Besides, noise learning is used to filter noisy labels.
2) \textit{Domain Gap:} retrieval-based prompt~(various and verisimilar prompt guidance) and data augmentations~(\eg{}, Splicing~\cite{bochkovskiy2020yolov4}), as two effective solutions, are designed to reduce the domain gap via enhancing the diversity of data. 
%
With the above advantages, \diffmask can generate infinite images with pixel-level annotation for any class without human effort.
%
These synthetic data can then be used for training any semantic segmentation architecture~(\eg{}, mask2former~\cite{cheng2022masked}), replacing real data.  
%just as real data are.

% How to obtain \textbf{precise masks}~\cite{ahn2018learning,ru2022learning}. As shown in Fig.~\ref{fig:teaser} and Fig.~\ref{fig2_attention}, cross attention maps
% %is
% are in general 
% too coarse as
% supervised segmentation masks. 
% 2) The \textbf{Domain gap}~\cite{tremblay2018training,wang2019learning} between synthetic and real images. 
% %
% \textit{Is the model trained on synthetic data from generative models can perform well in the real world?}
% %



% To break the dilemma, \textbf{for the former one}, we propose to combine the pixel semantic affinity learning~\cite{ahn2018learning,ru2022learning} (representing whole class prototype) and Dense CRF~\cite{krahenbuhl2011efficient} (representing detailed information from color, the distance of pixels relationship) for obtaining 
% %precise
% accurate 
% semantic masks.
%
% Our key insight is that existing affinity-based weakly supervised segmentation methods %can not 
% fail to 
% generate precise details~(\eg{},  feet of chair and tail of bird in Fig.~\ref{fig:teaser}), and Dense CRF~\cite{krahenbuhl2011efficient} focuses on the low-level local relationship of pixel (details from color and distance) without considering whole class prototype.
% %
% It is natural to explore combining the two methods.
% %
% Certainly, there are still existing some noisy/imprecise mask annotations, as they are predicted by the algorithm without human effort.
% %
% Motivated by noise learning~\cite{northcutt2021confident,song2022learning}, we design a simple, yet effective noise learning to filter the noisy synthetic data.
% %
% \textbf{For the later}, retrieval-based prompt~(various and verisimilar prompt guidance) and data augmentations~(\eg{} Paste~\cite{dwibedi2017cut}, Splicing~\cite{bochkovskiy2020yolov4}, Gaussian Blur~\cite{lopes2019improving}), as two effective solutions, are designed to reduce the domain gap via enhancing the diversity of data.
%


% We evaluate \diffmask using two popular datasets, \eg{} Pascal-VOC 2012~\cite{(voc)everingham2010pascal} (20 classes) and Cityscapes~\cite{(cityscape)cordts2016cityscapes} (only for human and vehicle).
%
To %sum it up
summarize, our contributions are three-folds:
\begin{itemize}
    \itemsep-0.1cm 
    \item We show a novel insight that it is possible to automatically obtain the synthetic image and mask annotation from a text-supervised pre-trained diffusion model. 
    
    % \diffmask, an automatic procedure to generate massive image and pixel-level semantic annotation \textit{without} human effort and any manual mask annotation,
    %
    % which exploits the potential of the cross-attention map between text and image.
    
    \item We present \diffmask, an automatic procedure to generate massive image and pixel-level semantic annotation \textit{without} human effort and any manual mask annotation,
    %
    which exploits the potential of the cross-attention map between text and image.
    % For two huge challenges, \ie{}, Precise mask generation and domain gap, \diffmask designs corresponding effective strategies, \eg{}, the combination of affinity learning and Dense CRF, noise learning, retrieval-based prompt, and data augmentation.
    
    \item Experiments demonstrate that  segmentation methods trained on \Ours perform competitively on real data, \eg{}, VOC 2012.
    %
    For some classes, \eg{}, \texttt{dog}, the performance is close to that of training with real data 
    (within \textbf{3\%} gap).
    %
    Moreover, in the open-vocabulary segmentation (zero-shot) setting, \Ours achieves  new SOTA results on the \texttt{Unseen}  classes of VOC 2012.



    
    
\end{itemize}


\section{Related Work}

\textbf{Reducing Annotation Cost.} Various ways can be explored to reduce the segmentation data cost, including interactive human-in-the-loop annotation~\cite{acuna2018efficient,ling2019fast}, nearest-neighbor mask transfer~\cite{guillaumin2014imagenet}, or weak/cheap mask annotation supervision in different levels, such as image-level labels~\cite{ahn2018learning,lee2021anti,wu2021embedded,xu2021leveraging,ru2021learning,ru2022learning}, points~\cite{akiva2021towards}, scribbles~\cite{lin2016scribblesup,zhang2021affinity},  and bounding boxes~\cite{lee2021bbam,chen2014beat,kulharia2020box2seg}.
%
Among the above-related works, image-level label supervised learning~\cite{ru2021learning,ru2022learning} presents the lowest cost, and its performance is unacceptable.
%
Bounding boxes~\cite{chen2014beat,kulharia2020box2seg} annotation usually shows a competitive performance than pixel-wise supervised methods, but its annotation cost is the most expensive.
%
By comparison, synthetic data presents many advantages, including lower data cost without image collection, and infinite availability for enhancing the diversity of data.



\textbf{Image Generation.}
%
Image generation is a basic and challenging  task in computer vision.
%
There are several mainstream methods for the task, including Generative Adversarial Networks
(GAN)~\cite{goodfellow2020generative}, Variational autoencoders (VAE)~\cite{kingma2013auto}, flow-based models~\cite{dinh2014nice}, and Diffusion Probabilistic Models (DM)~\cite{sohl2015deep,rombach2022high,gu2023mix}.
%
Recently, the diffusion model has drawn lots of attention due to its wonderful performance.
%
GLIDE~\cite{nichol2021glide} used pre-trained  language model~(CLIP~\cite{radford2021learning}) and the  cascaded diffusion structure for text-to-image  generation.
%
Similarly, DALL-E 2~\cite{ramesh2022hierarchical} of OpenAI Imagen~\cite{saharia2022photorealistic} obtain the corresponding text embedding with CLIP and adopted a similar hieratical  structure to generate images.
%
To increase accessibility and reduce  significant resource consumption, Stable Diffusion~\cite{rombach2022high} of Stability AI introduced a novel direction in which the model diffuses on VAE latent spaces instead of pixel spaces. 

\textbf{Synthetic Dataset Generation.}
%
Prior works~\cite{kar2019meta,devaranjan2021unsupervised} for dataset synthesis mainly utilize 3D scene graphs to render images and their labels.
%
2D methods, \ie{}, Generative Adversarial Networks (GAN)~\cite{goodfellow2020generative} mainly is used to solve domain adaptation task~\cite{choi2019self,choi2019self}, which leverages 
image-to-image translation to reduce the domain gap.
%
Recently, inspired by the success of generative model~(\eg{}, DALL-E 2, Stable Diffusion), some works further try to explore the potential of synthetic data to replace real data as the training data in many downstream tasks, including image classification~\cite{he2022synthetic,besnier2020dataset}, object detection~\cite{wu2022synthetic,ni2022imaginarynet,ge2022dall,ge2022neural,zhao2023generative,zhao2023flowtext}, image segmentation~\cite{li2022bigdatasetgan,zhang2021datasetgan,li2023guiding}, 
3D Rendering~\cite{zhang2020image,poole2022dreamfusion}.
%
DatasetGAN~\cite{zhang2021datasetgan} utilized a few labeled real images to train a segmentation mask decoder, leading to an infinite synthetic image and mask generator.
%
Based on DatasetGAN, BigDatasetGAN~\cite{poole2022dreamfusion} scale the class diversity to ImageNet size, which generates 1k classes with manually annotated 5 images per class.
%
With Stable diffusion and Mask R-CNN pre-trained on COCO dataset, Li \textit{et al.}~\cite{li2023guiding} design and train a grounding module to generate images and segmentation masks.
%
Different from the above methods, we go one step further and synthesize accurate semantic labels by exploiting the potential of cross attention map between text and image. 
%
% The biggest advantage of the proposed \diffmask is that it does not need any manual localization~(\ie{}, box and mask) annotation, and only requires \textit{text supervision}.
%
One significant advantage of the \diffmask is that it does not require any manual localization annotations~(\ie{}, box and mask) and only rely on \textit{text supervision}.
%2) Open vocabulary. Infinite classes extension with only text prompt.


% \begin{figure}[!t]
% % 	\centering
% 	\includegraphics[width=0.99\linewidth]{figures/fig2_attention.pdf}
% 	% \vspace{-0.2cm}
% 	\caption{\textbf{Average cross attention maps and gradient between text and image.} Prompt language: \textit{a horse on the grass}.}
% 	\vspace{-0.2cm}
% \label{fig2_attention}
% \end{figure}




\section{Methodology}
In this paper, we explore simultaneously generating images and the semantic mask described in the text prompt with the existing pre-trained diffusion model.
%
Using the synthetic data to train the existing segmentation methods, and apply them to the real images.

The core is to exploit the potential of the \textit{cross-attention map} in the generative model and \textit{domain gap} between synthetic and real data, providing corresponding new insights, solutions, and analysis.
%
We introduce the preliminary of cross attention in Sec.~\ref{Problem}, 
%
Mask generation and refinement with cross-attention map in text-conditioned diffusion models in Sec.~\ref{Mask}, data diversity enhancement with prompt engineering in Sec.~\ref{Prompt}, data augmentation in Sec.~\ref{Data}. 



\subsection{Cross-Attention of Text-Image}
\label{Problem}
% \textbf{Cross-Attention of Text-Image}
Text-guided generative models~(\eg{}, Imagen~\cite{saharia2022photorealistic}, Stable Diffusion~\cite{rombach2022high}) use a text prompt $\mathcal{P}$ to guide the content-related image $\mathcal{I}$ generation from a random gaussian image noise $z$, where visual and textual embedding are fused using the spatial cross-attention.  
%
Specifically, Stable Diffusion~\cite{rombach2022high} consists of a text encoder, a variational autoencoder~(VAE), and a U-shaped network~\cite{ronneberger2015u}.
%
The interaction between the text and vision occurs in the U-Net for the latent vectors at each time step, where cross-attention layers are used to fuse the embeddings of the visual and textual features and produce spatial attention maps for each textual token.
 %
Formally, for step $t$, the visual features of the noisy image $\varphi(z_t) \in \R^{H \times W \times C}$ are flatted and linearly projected into a \texttt{Query} vector $Q = \ell_Q(\varphi(z_t))$.
%
The text prompt $\mathcal{P}$ is projected into the textual embedding $\conditioner(\mathcal{P}) \in \R^{N\times d }$ ($N$ refers to the sequence length of text tokens and $d$ is the latent projection dimension) with the text encoder~$\conditioner$,
%
then is mapped into a \texttt{Key} matrix $K = \ell_K(\conditioner(\mathcal{P}))$ and a \texttt{Value} matrix $V = \ell_V(\conditioner(\mathcal{P}))$, via learned projections $\ell_Q, \ell_K, \ell_V$.
%
The \textit{cross attention maps} can be calculated by: 
\begin{equation}
\mathcal{A}=\text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right),
\label{equ1}
\end{equation}
where $\mathcal{A} \in \R^{H\times W \times N}$ (re-shape). 
For $j$-th text token, \eg{}, \textit{horse} on Fig.~\ref{fig:1a}, the corresponding weight $\mathcal{A} _{j} \in \R^{H\times W}$ on the visual map $\varphi(z_t)$ can be obtained. 
%
Finally, the output of cross-attention can be obtained with $\widehat{\varphi}\left(z_t\right)=\mathcal{A} V$, which is then used to update the spatial features $\varphi(z_t)$.
%






% The text prompt $\mathcal{P}$ is projected into the textual embedding $\conditioner(\mathcal{P}) \in \R^{M\times d}$ with the text encoder~$\conditioner$

% use  to guide the content-related image $\mathcal{I}$ generation from a random gaussian image noise $z$, where visual and textual embedding are fused using the spatial cross-attention.  

\subsection{Mask Generation and Refinement}
Based on Equ.~\ref{equ1}, we can obtain the corresponding cross attention map $\mathcal{A} _{j}^{s,t}$.
$s$ denotes the attention map from $s$-th layer of U-Net, and corresponding to four different resolutions, \ie{}, $8\times8$, $16\times16$, $32\times32$, and $64\times64$, as shown in Fig.~\ref{fig:1b}.
%
$t$ denotes $t$-th diffusion step~(time).
%
Then the average cross-attention map can be calculated by aggregating the multi-layer and multi-time attention maps as follows:
\begin{align}
    \mathcal{\hat{A}}_j = \frac{1}{S\cdot T}\sum_{s\in S,t\in T}\frac{\mathcal{A} _{j}^{s,t}}{\text{max}(\mathcal{A} _{j}^{s,t})},\label{eq:agg}
\end{align}
where $S$ and $T$ refer to the total steps and the number of layers (\ie{}, four for U-Net). 
%
Normalization is necessary due the value of the attention map from the output of \texttt{Softmax} is not a probability between 0 and 1.


\label{Mask}
\subsubsection{Standard Binarization}
Given an average attention map~(a  probability map) $M \in \R^{H \times W}$ for $j$-th text token produced by the cross attention in Equ.~\eqref{equ1},
%
it is essential to convert it to a binary map, where pixels with $1$ as the foreground region~(\eg{}, `\texttt{horse}').
%
Usually, as shown in Fig.~\ref{fig:1c}, the simplest solution for the binarization process is using a fixed threshold value $\gamma$, and refining with DenseCRF~\cite{krahenbuhl2011efficient} (local relationship % from 
defined by 
color %,
and distance of pixels) as follows:
\begin{equation}
    B=
    \text{DenseCRF}(\left [ \gamma;\mathcal{\hat{A}}_j  \right ]_{\texttt{argmax}} )\;.
    \label{eq:binarization}
\end{equation}
The above method is not practical and effective, while the \textit{optimal threshold} of each image and each category are not exactly the same.
%
To explore the relationship between threshold and binary mask quality, we set a simple analysis experiment.
%
Stable Diffusion~\cite{rombach2022high} is used to generate 1k images and corresponding attention maps for each class.
%
The prediction of Mask2former~\cite{cheng2022masked} pre-trained on Pascal-VOC 2012 as the ground truth is adopted to calculate the quality of mask quality~(mIoU), as shown in Fig.~\ref{iou}. 
%
The optimal threshold of different classes usually are different, \eg{}, around $0.48$ for `\texttt{Bottle}' class, different from that~(\ie{},  around $0.39$) of `\texttt{Dog}' class.
%
To achieve the best quality of the mask, the \textit{adaptive threshold} is a feasible solution for the various binarization for each image and class.




\begin{figure}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{figures/threshold.pdf}
	% \vspace{-0.2cm}
	\caption{\textbf{Relationship between mask quality~(IoU) and threshold for 
 %the different
 various 
 categories.} 
 $1k$ generative images are used for each class from Stable Diffusion~\cite{rombach2022high}. Mask2former~\cite{cheng2022masked} pre-trained on Pascal-VOC 2012~\cite{(voc)everingham2010pascal} is used to generate the ground truth. The optimal threshold of different classes usually is different.}
 \vspace{-0.2cm}
\label{iou}
\end{figure}

\begin{figure*}[t]
	\includegraphics[width=0.99\linewidth]{figures/pipeline.pdf}
	\vspace{-0.05cm}
	\caption{\textbf{Pipeline for \diffmask with a prompt: `\texttt{Photo of a [sub-class] car in the street}'}. \diffmask mainly includes three steps: 1) Prompt engineering is used to enhance the diversity and reality of prompt language (Sec.~\ref{Prompt}). 2) Image and mask generation and refinement with adaptive threshold from AffinityNet~(Sec.~\ref{Mask}). 3) Noise learning is designed to further improve the quality of data via filtering the noisy label~(Sec.~\ref{CL}). 
    }
    \vspace{-0.25cm}
\label{pipeline}
\end{figure*}
\vspace{-0.25cm}
\subsubsection{Adaptive Threshold for Binarization}
% It is quite difficult to judge which threshold is suitable.
It is challenging to determine the optimal threshold for binarizing the probability maps because of the variation in shape and region for each object class.
%
The image generation relies on \textbf{text-supervision}, which does not provide a precise definition of the shape and region of object classes. 
%
% Image generation is a \textbf{text-supervision} task, and does not give an accurate definition for the shape and region of the object class.
%
For example, the masks with $0.45 %\,
\gamma$ and that with $0.35 %\,
\gamma$ in Fig.~\ref{fig:1c}, the model can not judge which one is better, while no location information as supervision and reference is provided by human effort.
%


Looking deeper at the challenge, pixels with a middle confidence score cause uncertainty, while that with a high and low score usually represent the true foreground and the background.
%
To address the challenge, semantic affinity learning (\ie{},  AffinityNet~\cite{ahn2018learning}) is used to give an estimation for those pixels with a middle confidence score.
%
Thus we can obtain the definition for global prototype, \ie{}, \textit{which semantic masks with different threshold $\gamma$ is suitable to represent the whole prototype}. 
%
AffinityNet aims to predict semantic affinity between a pair of adjacent coordinates.
%
During the training phase, those pixels in the middle score range are considered as \textit{neutral}.
%
If one of the adjacent coordinates is \textit{neutral}, the network simply ignores the pair during training.
%
Without \textit{neutral} pixels, the affinity label of two coordinates is set to $1$~(positive pair) if their classes are the same, and 0~(negative pair)  otherwise.
%
During the inference phase, a coarse affinity map $\hat{B} \in \R^{H \times W}$ can be predicted by AffinityNet for each class of each image.
%
$\hat{B}$ is used to search for a suitable threshold $\hat{\gamma}$ during a search space $\Omega = \{\gamma_i\}_{i=1}^{L}$ as follows:
\begin{equation}
\label{eq:matching}
    \hat{\gamma} = \argmax_{\gamma\in\Omega } \sum_{}^{} \lmatch{\hat{B}, B_{\gamma}},
\end{equation}
% \vspace{-2mm}
where $\lmatch{\hat{B}, B_{\gamma}}$ is a pair-wise \emph{matching cost} of IoU between affinity map $\hat{B}$ and a binary map from attention map with threshold $\gamma$.
%
As a result, an adaptive threshold $\hat{\gamma}$ can be obtained for each image of each class.
%
The red points in Fig.~\ref{iou} represent the corresponding threshold from matching with the affinity map.
%
They are usually close to the optimal threshold.
%

\begin{figure}[t]
% 	\centering
	\begin{minipage}{0.47\linewidth}
% 		\centering
		\includegraphics[width=0.99\linewidth]{figures/horse_iou.pdf}
        \vspace{-0.6cm}
	\subcaption{Distribution of `\texttt{Horse}'.}
		\label{fig:3a}	
	\end{minipage}
    \quad
	\begin{minipage}{0.47\linewidth}
		\centering
		\includegraphics[width=0.99\linewidth]{figures/bird_iou.pdf}
        \vspace{-0.6cm}
	\subcaption{Distribution of `\texttt{Bird}'.}
		 \label{fig:3b}	
	\end{minipage}
        \vspace{-0.2cm}
	\caption{\textbf{Effect of Noise Learning~(NL).} 30k generative images are used for each class. NL prunes $70\%$ images on the basis of the rank of IoU.
    Mask2former~\cite{cheng2022masked} pre-trained on VOC 2012~\cite{(voc)everingham2010pascal} is used to generate the ground truth. NL brings obvious improvement in mask quality by pruning data.}
\label{iou_density}
\end{figure}
% \vspace{-0.5cm}

\subsection{Noise Learning}
\label{CL}
Although refined mask $B_{\hat{\gamma}}$ presents a competitive result, there are still existing noisy labels with low precision.
%
Fig.~\ref{iou_density} provides the probability density distribution of IoU for the `\texttt{Horse}' and `\texttt{Bird}' classes.
%
The masks with IoU under $80\%$ account for a non-negligible proportion and may cause a significant performance drop.
%
% The mask under $80\%$ IoU accounts for a non-negligible proportion, which is not unacceptable and will cause a serious performance drop.
%
Inspired by noise learning~\cite{northcutt2021confident,song2022learning,chen2019understanding} for the classification task, we design a simple, yet effective noise learning~(NL) strategy to prune the noise labels for the segmentation task.

NL improves the data quality by identifying and filtering noisy labels.
%
The main procedure~(see Fig.~\ref{pipeline}) comprises two steps:  (1) \textbf{Count}: estimating the distribution of label noise ${Q_{B_{\hat{\gamma}}, B^*}}$ to characterize pixel-level label noise, $B^*$ refers to the prediction of model. (2) \textbf{Rank}, and \textbf{Prune}: filter out noisy examples and train with errors removed data.
%
Formally, given massive generative images and annotations $\{(\mathcal{I},B_{\hat{\gamma}})\}$, a segmentation model $\bm{\theta}$~(\eg{}, Mask2former~\cite{cheng2022masked}, Mask-RCNN~\cite{he2017mask}) is used to predict out-of-sample probabilities of segmentation result $\bm{\theta}: \mathcal{I} \rightarrow \bm{M}_c(B_{\hat{\gamma}}; \mathcal{I}, \bm{\theta})$ by cross-validation.
%
Then we can estimate the joint distribution of noisy labels $B_{\hat{\gamma}}$ and true labels, ${Q^c_{B_{\hat{\gamma}}, B^*}}=\Phi_{\text{IoU}}(B_{\hat{\gamma}}, B^*)$, where $c$ denotes $c$-th class.
%
With ${Q^c_{B_{\hat{\gamma}}, B^*}}$, some interpretable and explainable ranking methods, such as loss reweighting~\cite{DBLP:conf/iclr/GoldbergerB17_smodel,NIPS2013_5073} can be used for CL to find label errors using.
%
In this paper, we adopt a simple and effective modularized rank and prune method, \ie{}, \textit{Prune by Class}, which decouples the model and data cleaning procedure.
%
For each class, select and prune $\alpha \%$ examples with the lowest self-confidence ${Q^c_{B_{\hat{\gamma}}, B^*}}$ as the noisy data, and train model $\bm{\theta}$ with the remaining clean data.
%
While $\alpha \%$ is set to $50\%$, the probability density distribution of IoU from the remaining clean data is presented in Fig.~\ref{iou_density} (yellow).
%
CL can bring an obvious gain for the mask precision, which further taps the potential of attention map as mask annotation.



\begin{figure}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{figures/fig2_bird.pdf}
	\vspace{-0.2cm}
	\caption{\textbf{Prompt for diversity in sub-class for the \textit{bird} class.} $100$ sub-classes for \textit{bird} class in total for our experiment. The same prompt strategy is used for other classes, \eg, cat, car.}
	\vspace{-0.1cm}
\label{VIS12}
\end{figure}

\subsection{Prompt Engineering}
\label{Prompt}
Previous works~\cite{ni2022imaginarynet,witteveen2022investigating} have shown the effectiveness of prompt engineering on diversity enhancement of generative data.
%
These studies utilize a variety of prompt modifiers to influence the generated images, \eg{}, GPT3 used by ImaginaryNet~\cite{ni2022imaginarynet}.
%
Unlike generation-based or modification-based prompts, we design two practical, reality-based prompt strategies.
%

\textbf{Prompt with Sub-Classes.} Simple text prompts, such as `\texttt{Photo of a bird}', often results in monotony for generative images, as depicted in Fig.~\ref{VIS12}~(upper), they fail to capture the diverse range of objects and scenes found in the real world. 
%which can not cover the diversity in the real world, as shown in Fig.~\ref{VIS12}~(upper). 
%
To address this challenge, we incorporate `\texttt{sub-classes}' for each category to improve diversity.
%
To achieve this, we select $K$ sub-classes for each category from Wiki\footnote{https://en.wikipedia.org/wiki/Main\_Page} and integrate this information into the prompt templates.
%
Fig.~\ref{VIS12}~(down) presents an example for `\texttt{bird}' category.
%
Given $K$ sub-classes, \ie{}, Golden Bullul, Crane, this allows us to obtain $K$ corresponding text prompts `\texttt{Photo of a [sub-class] bird}', denoted by $\{\mathcal{\hat{P}}_1,\mathcal{\hat{P}}_2,...,\mathcal{\hat{P}}_{K} \}$.
%
% With the sub-class prompt, the diversity and reality of generative images will be improved obviously.
%

\textbf{Retrieval-based Prompt.}
The prompt $\mathcal{\hat{P}}$ still is a handcrafted sentence template, we expect to develop it into a real language prompt in the human community.
%
One feasible solution for that is through prompt retrieval~\cite{beaumont-2022-clip-retrieval,radford2021learning}.
%
As shown in Fig.~\ref{pipeline}, given a prompt $\mathcal{\hat{P}}$, \ie{}, `\texttt{Photo of a [sub-class] car in the street}', Clipretrieval~\cite{beaumont-2022-clip-retrieval} pre-trained on Laion5B~\cite{schuhmann2022laion} is used to retrieve top $N$ real images and captions, where the captions as the final prompt sets.
%
Using this approach, we can collect a total of $K\times N$ text prompts, denoted by $\sum_{i=1}^{K\times N} \mathcal{\hat{P}}_i$, for our synthetic data. During inference, we randomly sample a prompt from this set to generate each image.

\begin{figure}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{figures/fig_augmentation.pdf}
	\vspace{-0.2cm}
	\caption{\textbf{Data Augmentation.} Four data augmentations are used to reduce the domain gap.}
	\vspace{-0.1cm}
\label{VIS12_data}
\end{figure}





\begin{table*}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/VOC_semantic.tex}
    \vspace{-0.2cm}
    \caption{\textbf{Result of Semantic Segmentation on the VOC 2012 \texttt{val}.} \textit{mIoU} is for $20$ classes. `S' and `R' refer to `Synthetic' and `Real'.}
    % \vspace{+2mm}
    \label{VOC_semantic}
    % \vspace{-2mm}
\end{table*}


% \begin{table*}[t]
%     \centering
%     \small 
%     % \renewcommand\arraystretch{1.0}
%     \setlength{\tabcolsep}{1mm}
%     \input{tables/COCO2017.tex}
%     \vspace{-0.1cm}
%     \caption{\textbf{The mIoU (\%) of Semantic Segmentation on the COCO 2017 \texttt{val}.}}
%     % \vspace{+2mm}
%     % \vspace{+2mm
%     \label{COCO_semantic}
%     % \vspace{-2mm}
% \end{table*}



% \begin{table*}[t]
%     \centering
%     \small 
%     % \renewcommand\arraystretch{1.0}
%     \setlength{\tabcolsep}{1mm}
%     \input{tables/COCO_all_class.tex}
%     \vspace{-0.1cm}
%     \caption{\textbf{The mIoU (\%) of Semantic Segmentation for all classes on the COCO 2017 \texttt{val}.} mIoU of the experiment is $61.7\%$. R50 is used as backbone.}
%     % \vspace{+2mm}
    
%     % \vspace{+2mm
%     \label{COCO_semantic_all}
%     % \vspace{-2mm}
% \end{table*}



% \begin{table*}[t]
%     \centering
%     \small 
%     % \renewcommand\arraystretch{1.0}
%     \setlength{\tabcolsep}{1mm}
%     \input{tables/cityscapes.tex}
%     \vspace{-0.1cm}
%     \caption{\textbf{The mIoU (\%) of Semantic Segmentation on the Cityscapes \texttt{val}.}}
%     % \vspace{+2mm}
    
%     % \vspace{+2mm
%     \label{cityscapes}
%     % \vspace{-2mm}
% \end{table*}


\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/cityscapes_human_car.tex}
    \vspace{-0.2cm}
    \caption{\textbf{The mIoU (\%) of Semantic Segmentation on Cityscapes \texttt{val}.} `Human' includes two sub-classes \texttt{person} and \texttt{rider}. `Vehicle' includes four sub-classes, \ie{}, \texttt{car}, \texttt{bus}, \texttt{truck} and \texttt{train}. Mask2former~\cite{cheng2022masked} with ResNet50 is used.}
    %
    % `Ride' includes two sub-classes \textit{motorcycle} and \textit{bicycle}.
    % \textbf{CS:
    % the results of 
    % Fine-tuning do not convey much info.
    % You need a result of ``Synthetic + much fewer real data''
    % }
    
    % \vspace{+2mm}
    
    % \vspace{+2mm
    \label{cityscapes_human}
    % \vspace{-2mm}
\end{table}





\subsection{Data Augmentation}
\label{Data}
% , synthetic images usually present that an object~(foreground) occupies the main body in a simple background, while real image is complex with multi-scale size, occlusion.
%
To further reduce the domain gap between the generated images and the real-world images in terms of size, blur, and occlusion,
data augmentations~$\Phi(\cdot )$~(\eg, Splicing~\cite{bochkovskiy2020yolov4}), as the effective strategies are used, as shown in Fig.~\ref{VIS12_data}.
%
\textbf{Splicing.} Synthetic image usually present normal size for the foreground~(object), \ie{}, objects typically occupy the majority of image.
%
However, real-world images often contain objects of varying resolutions, including small objects in datasets such as Cityscapes~\cite{(cityscape)cordts2016cityscapes}.
%
To address this issue, we use Splicing augmentation.
%
Fig.~\ref{VIS12_data} (a) presents one example for the image splicing~($2\times2$).
%
In the experiment, six scales of image splicing are used, \ie{}, $1\times2$, $2\times1$, $2\times2$, $3\times3$, $5\times5$, and $8\times8$, and the images are sampled from train set randomly. 
%
\textbf{Gaussian Blur.} Synthetic images typically exhibit a uniform level of blur, whereas real images exhibit varying degrees of blur due to motion, focus, and artifact issues.
%
Gaussian Blur~\cite{lopes2019improving} is used to increase the diversity of blur, where the length of Gaussian Kernel is randomly sampled from a range of $6$ to $22$.
%
\textbf{Occlusion}. Similar to CutMix~\cite{yun2019cutmix},  to make the model focus on discriminative parts of objects, patches of another image are cut and pasted among training images where the corresponding labels are also mixed
proportionally to the area of the patches.
%
\textbf{Perspective Transform.} Similar to the above augmentations, perspective transform is used to improve the diversity of the generated images by simulating different viewpoints.

\section{Experiments}
\subsection{Experimental Setups}
\textbf{Datasets and Task.} \textit{Datasets.} Following the previous works~\cite{cheng2022masked,li2023guiding} for semantic segmentation, Pascal-VOC 2012~\cite{(voc)everingham2010pascal},  ADE20k~\cite{(ade)zhou2017scene} and Cityscapes~\cite{(cityscape)cordts2016cityscapes} are used to evaluate \Ours. 
%
\textit{Tasks.} Three tasks are adopted in our experiment, \ie{}, semantic segmentation, open-vocabulary segmentation, and domain generalization.





\textbf{Implementation Details}
The pre-trained Stable Diffusion~\cite{rombach2022high}, the text encoder of CLIP~\cite{radford2021learning}, AffinityNet~\cite{ahn2018learning} are adopted as the base components.
%
We do not finetune the Stable Diffusion and only train AffinityNet for each category.
%
The corresponding parameter optimization and setting~(\eg{}, initialization, data augmentation, batch size, learning rate) all are similar to that of the original paper.
%
\textit{Synthetic data for training.} 
For each category on Pascal-VOC 2012~\cite{(voc)everingham2010pascal}, we generate $10k$ images and set $\alpha$ of noise learning to $0.7$ to filter $7k$ images.
%
As a result, we collect $60k$ synthetic data for $20$ classes as the final training set, and the spatial resolution is $512\times 512$.
%
For Cityscapes~\cite{cordts2016cityscapes}, we only evaluate $2$ important classes, \ie{}, `Human' and `Vehicle', including six sub-classes, \texttt{person}, \texttt{rider}, \texttt{car}, \texttt{bus}, \texttt{truck}, \texttt{train}, and generate $30k$ images for each sub-category, where $10k$ images are selected as the final training data by noise learning.
%
Considering the relationship between \texttt{rider} and \texttt{motorbike}/\texttt{bicycle}, we set the two classes to be ignored, while evaluating the `Human' class on Tab.~\ref{cityscapes_human} and Tab.~\ref{dg}.
%
In our experiment, only a single object for an image is considered.
%
Multi-categories generation~\cite{li2023guiding} usually causes the unstable quality of the images, limited by the generation ability of Stable Diffusion. 
%
Mask2Former~\cite{cheng2022masked} is used as the baseline to evaluate the dataset.
% , and all settings are similar to the official code and paper.
%
8 Tesla V100 GPUs are used for all experiments.


\textbf{Evaluation Metrics.} \textit{Mean intersection-over-union (mIoU)}~\cite{(voc)everingham2010pascal,cheng2022masked}, as the common metric of semantic segmentation, is used to evaluate the performance.
%
For open-vocabulary segmentation, following the prior~\cite{ding2022decoupling,cheng2021sign}, the mIoU averaged on seen classes, unseen classes, and their \textit{harmonic mean} are used.
%


\textbf{Mask Smoothness.} The mask $B_{\hat{\gamma}}$ generated by the Dense CRF often contains jagged edges and numerous small regions that do not correspond to distinct objects in the image.
%
To address these issues, we trained a segmentation model $\bm{\theta}$~(\ie{} Mask2Former), using the mask $B_{\hat{\gamma}}$ generated by the Dense CRF as input. 
%
We then used this model to predict the pseudo labels for the training set of synthetic data, resulting in a final semantic mask annotation

\textbf{Cross Validation for Noise Learning.}
In the experiment, we performed the three-fold cross-validation for each class.
%
The five-fold cross-validation (CV) is a process in which all data is randomly split into $k$ folds, in our case $k$ $=$ $3$, and then the model is trained on the $k - 1$ folds, while one fold is left to test the quality.
%


\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/zs3.tex}
    \vspace{-0.2cm}
    \caption{\textbf{Performance for Zero-Shot Semantic Segmentation Task on PASCAL VOC.} `Seen', `Unseen', and `Harmonic' denote mIoU of seen, unseen categories, and their harmonic mean. Priors are trained with real data and masks.}
    % \vspace{+2mm
    \label{zs3}
\end{table}


\begin{table*}[t]
  \input{tables/ablation_study.tex}
  \vspace{-0.2cm}
  \caption{\textbf{\diffmask ablations.} We perform ablations on VOC 2012 \texttt{val}. $\gamma$ and `AT' denotes the `Threshold' and `Adaptive Threshold', respectively.  $\alpha$ refers to the proportion of data pruning. $\Phi_1$, $\Phi_2$, $\Phi_3$ and $\Phi_4$ refer to `Splicing', `Gaussian Blur', `Occlusion', and `Perspective Transform', respectively. `Retri.' and `Sub-C' denotes `retrieval-based' and `Sub-Class', respectively. Mask2former with Swin-B is adopted as the baseline.
  }
  \label{ablation_diffmask}
  \vspace{-1mm}
\end{table*}
% \vspace{-5mm}
\subsection{Protocol-I: Semantic Segmentation}
\textbf{VOC 2012.} Tab.~\ref{VOC_semantic} presents the results of semantic segmentation on the VOC 2012.
%
% We generate $60.0$k generative images for training, where $3.0$k per category and the spatial resolution are $512\times 512$.
%
The existing segmentation methods trained on synthetic data~(\diffmask) can achieve a competitive performance, \ie{}, $70.6\%$ $v.s.$ $84.3\%$ for mIoU with Swin-B backbone.
%
A point worth emphasizing is that our synthetic data does not need any manual localization and mask annotation, while real data need humans to perform a pixel-wise mask annotation. 
%
For some categories, \ie{}, \texttt{bird}, \texttt{cat}, \texttt{cow}, \texttt{horse}, \texttt{sheep}, \diffmask presents a powerful performance, which is quite close to that of training on real~(within $5\%$ gap).
%
Besides, finetune on few real data, the results can be improved further, and exceed that of training on full real data, \eg{}, $84.9\%$ mIoU finetune on $5.0$k real data $v.s$ $83.4\%$ mIoU training on full real data~($11.5$k).

\textbf{Cityscapes.} Tab.~\ref{cityscapes_human} presents the results on Cityscapes.
%
Urban street scenes of Cityscapes are more challenging, including a mass of small objects and complex backgrounds.
%
We only evaluate two classes, \ie{}, \texttt{Vehicle} and \texttt{Human}, which are the two most important categories in the driving scene.
%
Compared with training on real images, \diffmask presents a competitive result, \ie{}, $79.6\%$ $vs.$\  $90.8\%$ mIoU.

\textbf{ADE20K}
%
ADE20K, as one more challenging dataset, is also used to evaluate the \Ours.
%
Tab.~\ref{ADE20k_semantic} presents the results of three categories~(\texttt{bus}, \texttt{car}, \texttt{person}) on ADE20K.
%
With fewer synthetic images~($6$k), we achieve a competitive performance than that of a mass of real images~($20.2$k).
%
Compared with the other two categories, Class \texttt{car} achieves the best performance, with $73.4\%$ mIoU. 

% \begin{table}[t]
%     \centering
%     \small 
%     % \renewcommand\arraystretch{1.0}
%     \setlength{\tabcolsep}{1mm}
%     \input{tables/augmentation}
%     \vspace{-0.1cm}
%     \caption{\textbf{Ablation study for data augmentation.} .}
%     % \vspace{+2mm
%     \label{ablation_data}
% \end{table}


\subsection{Protocol-II: Open-vocabulary Segmentation}
As shown in Fig.~\ref{fig:teaser}, it is natural and seamless to extend the text-driven synthetic data~(our \diffmask) to the open-vocabulary~(zero-shot) task.
%
% Tab.~\ref{zs3} provides a comparison with the existing zero-shot semantic segmentation approaches.
%
As shown in Tab.~\ref{zs3}, compared with priors training on real images with manually annotated mask, \diffmask can achieve a SOTA result on \texttt{Unseen} classes.
%
It is worth mentioning that \diffmask is pure synthetic/fake data and supervised by text, while priors all must need the real image and corresponding manual mask annotation.
%
Li \textit{et al.}, as one contemporaneous work, use the segmentation model pre-trained on COCO~\cite{lin2014microsoft} to predict the pseudo label of the synthetic image, which is high-cost.


\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/ADE_semantic.tex}
    \vspace{-0.1cm}
    \caption{\textbf{The mIoU (\%) of Semantic Segmentation on the ADE20K \texttt{val}.}}
    % \vspace{+2mm}
    % \vspace{+2mm
    \label{ADE20k_semantic}
    % \vspace{-2mm}
\end{table}


\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/domain_generalization.tex}
    \vspace{-0.2cm}
    \caption{\textbf{Performance for Domain Generalization between different datasets.} Mask2former~\cite{cheng2022masked} with ResNet50 is used as the baseline. \texttt{Person} and \texttt{Rider} classes of Cityscapes~\cite{cordts2016cityscapes} are consider as the same class, \ie{}, \texttt{Person} in the experiment. }
    % \vspace{+2mm}
    % \vspace{+2mm
    \label{dg}
    % \vspace{-2mm}
\end{table}

% \begin{figure*}[t]
% % 	\centering
% \includegraphics[width=0.99\linewidth]{figures/fig_open_world.pdf}
% 	\vspace{-0.1cm}
% 	\caption{\textbf{Visualization of \diffmask for open-vocabulary mask generation.} 
%     }
%     \vspace{-0.2cm}
% \label{vis_voc_open}
% \end{figure*}




\subsection{Protocol-III: Domain Generalization}
% The generalization of \diffmask is also another vital point for us to discuss.
%
Tab.~\ref{dg} presents the results for cross-dataset validation, which can evaluate the generalization of data.
%
Compared with real data, \diffmask show powerful effectiveness on domain generalization, \eg{}, $69.5\%$ with \diffmask $v.s$ $68.0$ with ADE20K~\cite{(ade)zhou2017scene} on VOC 2012 \texttt{val}.
%
The domain gap~\cite{toldo2020unsupervised} between real datasets sometimes is bigger than that among synthetic and real data.
%
For \texttt{Motorbike} class, model training with Cityscapes only achieves $28.9\%$ mIoU, but that of \diffmask is $63.2\%$ mIoU.
%
We argue that the main reason is domain shift in foreground and background domains, \ie{}, Cityscapes contains images of city roads, with the majority of \texttt{Motorbike} objects being small in size.
%
But VOC 2012 is an open-set scenario, where \texttt{Motorbike} objects vary greatly in size and include close-up shots.



\subsection{Ablation Study}
\textbf{Compared with Attention Map.}
Tab.~\ref{ablation_DQ1} presents the comparison with the attention map and the impact of binarization threshold $\gamma$.
%
It is %obvious
clear 
that the optimal threshold for different categories is different, even various for different images of the same category.
%
% And 
Sometimes it is sensitive for some categories, such as \texttt{Dog}.
%
The mIoU of $0.4$ $\gamma$ is better than that of $0.6$ $\gamma$ around $40\%$ mIoU, which can not be neglectful.
%
By contrast, our adaptive threshold is robust. %And 
Fig.~\ref{iou} also shows it is close to the optimal threshold.

% \textbf{Adaptive Threshold.} 

\textbf{Prompt Engineering.} Tab.~\ref{tab:ablation:maskformer:b} provides the related ablation study for prompt strategies.
%
Retrieval-based and sub-classes prompt all can bring an obvious gain.
%
For \texttt{dog}, $10$ sub-classes prompt brings a $7.7\%$ mIoU improvement, which is quite significant.  
%
It is reasonable, the fine-grained prompts can directly enhance the diversity of generative images, as shown in Fig.~\ref{VIS12}. 


\textbf{Noise Learning.} Tab.~\ref{ablation_NL} presents the impact of prune threshold $\alpha$.
%
$10k$ synthetic images for each class are used in this experiment.
%
% With bigger $\alpha$, the model can obtain a better result.
%
% And
% Moreover, 
The gain is %obvious,
considerable 
while $\alpha$ changes from $0.3$ to $0.5$.
%
In other experiments, we set the $\alpha$ to $0.7$ for each category.


\textbf{Data Augmentation.} The ablation study for the four augmentations is shown in Tab.~\ref{ablation_param}.
%
Compared with the other three augmentations, the gain of image splicing is the biggest.
%
One main reason is that the synthetic images are all $512\times512$ resolution and the size of the object usually is normal, image splicing can enhance the diversity of scale.



\textbf{What causes the performance gap between synthetic and real data.}
Domain gap and mask precision are the main reasons for the performance gap between synthetic and real data.
%
Tab.~\ref{gap} is set To further explore the problem.
%
Li \textit{et al.}~\cite{li2023guiding} shows that the pseudo mask of the synthetic image from Mask2former~\cite{cheng2022masked} pre-trained on VOC 2012 is quite accurate, and can as the ground truth.
%
Thus, we also use the pseudo label from the pre-trained Mask2former to train the model.
% , where we argue that the pseudo label is accurate.
%
As shown in Tab.~\ref{gap}, mask precision cause $6.4\%$ mIoU gap, and the domain gap of images causes  $4.5\%$ mIoU gap.
%
Notably, for the \texttt{bird} class, the use of synthetic data with a pseudo label resulted in better results than the corresponding real images. 
%
This observation suggests that there may be no domain gap for the \texttt{bird} class in the VOC 2012 dataset.

\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/Ablation_Backbone}
    \vspace{-0.2cm}
    \caption{\textbf{Impact of Backbone on VOC 2012 \texttt{val}.} Mask2former~\cite{cheng2022masked} is used as the baseline. }
    \label{backbone}
    % \vspace{-2mm}
\end{table}




\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/Ablation_GaP}
    \vspace{-0.2cm}
    \caption{\textbf{Impact of Mask Precision and Domain Gap on VOC 2012 \texttt{val}.} Mask2former~\cite{cheng2022masked} with Swin-B is used as the baseline. `Pseudo' denotes pseudo mask annotation from Mask2former~\cite{cheng2022masked} pre-trained on VOC 2012.}
    % \vspace{+2mm}
    \label{gap}
    % \vspace{-2mm}
\end{table}


\begin{figure}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{figures/backbone.pdf}
	\vspace{-0.2cm}
	\caption{\textbf{Impact of Backbone.} Stronger backbone is robust for classification, False Negative, and mask precision.}
	\vspace{-0.1cm}
\label{backbone_vis}
\end{figure}



\textbf{Backbone}
Tab.~\ref{backbone} presents the ablation study for the backbone.
%
For some classes, \eg{} \texttt{sheep}, the stronger backbone can bring obvious gains, \ie{} Swin-B achieves $27.5\%$ mIoU improvement than that of ResNet 50.
%
And the mIoU of all classes with Swin-B achieves $19.2\%$ mIoU improvements.
%
It is an interesting and novel insight that a stronger backbone can reduce the domain gap between synthetic and real data. 
%
To give a further analysis for that, we present some results comparison of visualizations, as shown in Fig.~\ref{backbone_vis}.
%
Swin-B brings an obvious improvement in classification, False Negatives, and mask precision.
%
% Compared with the gain between different backbones, different versions (size) of the same backbone seems can not obtain an effective gain, \eg{} ResNet101 only obtain $0.5\%$ mIoU improvements than that of ResNet50.

% Acknowledgements: 
\section{Conclusion}
A new insight is presented in this paper, demonstrating that the accurate semantic mask of generative images can be automatically obtained through the use of a text-driven diffusion model.
% The paper provides a novel insight that it is possible to automatically obtain the accurate semantic mask of the generative images from a text-driven diffusion model.
%
To achieve this goal, we present \Ours, an automatic procedure to generate image and pixel-level semantic annotation.
%
The existing segmentation methods training on synthetic data of \Ours can achieve a competitive performance over the counterpart of real data.
%
Besides, \Ours shows the powerful performance for open-vocabulary segmentation, which can achieve a promising result on \texttt{Unseen} category.
%
We hope \diffmask can bring new insights and inspiration for bridging generative data and real-world data in the community.





\section*{Acknowledgements} 
% This work was supported by %the 
% National Key R\&D Program of China (\# 2022ZD0118700), %the
% National Key Research and Development Program of China (\# 2022YFC3602601), and 
% %the
% Key Research and Development Program of Zhejiang Province of China (\# 2021C02037).

%Weijia Wu, Chunhua Shen 
W. Wu, C. Shen's participation was 
 supported by the National Key R\&D Program of China (No.\  2022ZD0118700). 
%
W. Wu, H. Zhou's participation was  supported by the National Key Research and Development Program of China (No.\ 2022YFC3602601), and the Key Research and Development Program of Zhejiang Province of China (No.\ 2021C02037).
%
M. Shou's participation was  supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and his Start-Up Grant from National University of Singapore.





% \appendix
% \section{More Details}
% \textbf{Evaluation Metrics.} \textit{Mean intersection-over-union (mIoU)}~\cite{(voc)everingham2010pascal,cheng2022masked}, as the common metric of semantic segmentation, is used to evaluate the performance.
% %
% For open-vocabulary segmentation, following the prior~\cite{ding2022decoupling,cheng2021sign}, the mIoU averaged on seen classes, unseen classes, and their \textit{harmonic mean} are used.
% %



% \textbf{Mask Smoothness.} The mask $B_{\hat{\gamma}}$ generated by the Dense CRF often contains jagged edges and numerous small regions that do not correspond to distinct objects in the image.
% %
% To address these issues, we trained a segmentation model $\bm{\theta}$~(\ie{} Mask2Former), using the mask $B_{\hat{\gamma}}$ generated by the Dense CRF as input. 
% %
% We then used this model to predict the pseudo labels for the training set of synthetic data, resulting in a final semantic mask annotation
% %
% % In our experiment, we use the segmentation model $\bm{\theta}$~(\ie{} Mask2Former) to smooth the mask, \ie{} we train the model with $B_{\hat{\gamma}}$ and predict the pseudo label of the training set for synthetic data, which are the final semantic mask annotation.


% \textbf{Cross Validation for Noise Learning.}
% In the experiment, we performed the three-fold cross-validation for each class.
% %
% The five-fold cross-validation (CV) is a process in which all data is randomly split into $k$ folds, in our case $k$ $=$ $3$, and then the model is trained on the $k - 1$ folds, while one fold is left to test the quality.
% %


% % \begin{figure*}[t]
% % % 	\centering
% % 	\includegraphics[width=0.99\linewidth]{figures/fig1_voc.pdf}
% % 	\vspace{-0.1cm}
% % 	\caption{\textbf{Visualization of \diffmask for $20$ classes on Pascal-VOC 2012~\cite{(voc)everingham2010pascal}.} 
% %     }
% %     \vspace{-0.2cm}
% % \label{vis_voc}
% % \end{figure*}









% \begin{table}[t]
%     \centering
%     \small 
%     % \renewcommand\arraystretch{1.0}
%     \setlength{\tabcolsep}{1mm}
%     \input{tables/feature}
%     \vspace{-0.2cm}
%     \caption{\textbf{Impact of different attention maps from different layers.} Mask2former~\cite{cheng2022masked} with Swin-B is used as the baseline.}
%     % \vspace{+2mm}
%     \label{fature}
%     % \vspace{-2mm}
% \end{table}



% \begin{figure}[!t]
% % 	\centering
% 	\includegraphics[width=0.99\linewidth]{figures/fig2_attention_graident.pdf}
% 	\vspace{-0.2cm}
% 	\caption{\textbf{Gradient from Text Tokens for Stable Diffusion.} Prompt language: `\texttt{a horse on the grass}'.}
% 	\vspace{-0.1cm}
% \label{Gradient}
% \end{figure}




% \section{More Ablation Study}

% \textbf{What causes the performance gap between synthetic and real data.}
% Domain gap and mask precision are the main reasons for the performance gap between synthetic and real data.
% %
% Tab.~\ref{gap} is set To further explore the problem.
% %
% Li \textit{et al.}~\cite{li2023guiding} shows that the pseudo mask of the synthetic image from Mask2former~\cite{cheng2022masked} pre-trained on VOC 2012 is quite accurate, and can as the ground truth.
% %
% Thus, we also use the pseudo label from the pre-trained Mask2former to train the model, where we argue that the pseudo label is accurate.
% %
% As shown in Tab.~\ref{gap}, mask precision cause $6.4\%$ mIoU gap, and the domain gap of images causes  $4.5\%$ mIoU gap.
% %
% Notably, for the \texttt{bird} class, the use of synthetic data with a pseudo label resulted in better results than the corresponding real images. 
% %
% This observation suggests that there may be no domain gap for the \texttt{bird} class in the VOC 2012 dataset.

% \begin{figure*}[!t]
% % 	\centering
% 	\includegraphics[width=0.99\linewidth]{figures/fig2_cityscapes.pdf}
% 	% \vspace{-0.2cm}
% 	\caption{\textbf{Visualization for the model trained with \textit{only} \diffmask on Cityscapes.} \diffmask presents a competitive performance on challenging driving scenario.}
% 	% \vspace{-0.2cm}
% \label{VIS122}
% \end{figure*}
% \textbf{Backbone}
% Tab.~\ref{backbone} presents the ablation study for the backbone.
% %
% For some classes, \eg{} \texttt{sheep}, the stronger backbone can bring obvious gains, \ie{} Swin-B achieves $27.5\%$ mIoU improvement than that of ResNet 50.
% %
% And the mIoU of all classes with Swin-B achieves $19.2\%$ mIoU improvements.
% %
% It is an interesting and novel insight that a stronger backbone can reduce the domain gap between synthetic and real data. 
% %
% To give a further analysis for that, we present some results comparison of visualizations, as shown in Fig.~\ref{backbone_vis}.
% %
% Swin-B brings an obvious improvement in classification, False Negatives, and mask precision.
% %
% Compared with the gain between different backbones, different versions (size) of the same backbone seems can not obtain an effective gain, \eg{} ResNet101 only obtain $0.5\%$ mIoU improvements than that of ResNet50.


% \textbf{Attention Maps of different resolutions.}
% Table \ref{fature} shows the results of an ablation study conducted on cross attention maps with varying resolutions from different layers. The performance of both high resolution ($64\times64$) and low resolution ($8\times8$) maps was found to be unsatisfactory. This can be attributed to the lack of detail in low-resolution maps and the presence of noise in high-resolution maps. On the other hand, integrating (by averaging) all attention maps produced the best performance.
% %






% \section{Visual explanation with gradients} The gradient is another way to provide an excellent visual explanation of the generative model, Fig.~\ref{Gradient} presents the corresponding gradient visualization from different text tokens.
% %
% Given a text prompt $\mathcal{P}$, \ie{}, `\texttt{a horse on the grass}' and a random Gaussian image noise $z$, the text-guided generative model is in principle capable of modeling conditional distributions of the form $\mathcal{I}:= p(z \vert \conditioner(\mathcal{P}))$, where $\conditioner(\mathcal{P}) \in \R^{N\times d }$ and $\conditioner$ refers to the text encoder~\cite{radford2021learning}.
% %
% For the $k$-th word $t_k$~(\eg{}, `\textit{horse}' from Fig.~\ref{Gradient}) from $\mathcal{P}$, we can compute the corresponding gradient as following: $\alpha^{}_{k} = \vphantom{\sum_{i}\sum_{j}} \frac{\partial \mathcal{I}}{\partial t_k}$
% % \begin{equation} 
% % \label{eq:alpha1}
% %     \alpha{}_{i} = \vphantom{\sum_{i}\sum_{j}} \frac{\partial A_{ij}}{\partial t_i}
% % \end{equation}
% %
% % , where $A_{ij}$ refers to the pixel of generative image $\mathcal{I}$. 
% %
% where $\alpha{}_{k}$ is the gradient weight from the $k$-th word $t_k$.
% %
% The corresponding gradient weight can be computed by adding a small variate (that is, Numbers close to zero) to the $t_k$.
% %
% For convenience, we add a small variate $\bigtriangleup \beta \in \R^{d} $~($\bigtriangleup \beta = \mu \vec{\textbf{1}}_{d}$, where $\vec{\textbf{1}}_{d}$ and $\mu$ refer to the unit matrix and weight) to the text feature map $\conditioner(t_k)$ and obtain the corresponding gradient visualization, as shown in Fig.~\ref{Gradient}.
% %
% The gradient visualization is highly class-discriminative (\ie{},  the `\texttt{horse}' explanation exclusively highlights the `\textit{horse}' regions).



% \section{Limitation} \diffmask mainly includes two limitations: 1) The inference speed of the text-to-image diffusion model is relatively slow.
% %
% With 8 Tesla V100 GPUs, generating $10k$ images usually need to spend around 8 hours.
% %
% Therefore, scaling up the synthetic dataset to a million level is difficult for some institutions.
% %
% %And
% It is the main reason why we do not provide more experiments for other datasets with rich categories, \eg{} ADE20K or COCO.
% %
% Similarly, we can not scale up the synthetic data to the million level due to the limitation of time and computational cost.
% %
% But we argue the cost can be reduced by adopting advanced faster Sampling for Diffusion Models~\cite{song2020score,karras2022elucidating}.
% %
% 2) There are still existing obvious result gaps for some classes, \eg{} \texttt{person} on VOC 2012.
% %
% The main reason is the obvious domain gap for these classes.
% %
% The synthetic image usually presents a simple foreground and background, while %the 
% real images are %more examples 
% the ones 
% with multi-views, multi-scales, blur, and occlusion.
% %
% Even so, our \diffmask, as the first work to synthesize image and mask annotation using an image-text pre-trained diffusion model, provides %a
% promising performance and many new insights.
% %
% We verify the feasibility of training with text-driven synthetic data and applications in the real world, where worth mentioning the diffusion model is trained with only language-image pairs.


%\textbf{Acknowledgements}: This work was supported by National Key R\&D Program of China (No.\ 2022ZD0118700).

{\small
\bibliographystyle{ieee_fullname}
\bibliography{output}
}
\end{document}