\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{floatrow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{mathrsfs, eucal}
\newcommand{\conditioner}{\tau_\theta}
\newcommand{\R}{\mathbb{R}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\input CSMacrosV2.tex


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\newcommand{\diffmask}{\textup{DiffuMask}\xspace}
\newcommand{\Ours}{\textup{DiffuMask}\xspace}

% \newcommand{\cisdq}{\textsc{\textbf{C}i\textbf{SDQ}}\xspace}

% \usepackage[ruled]{algorithm2e}             
\usepackage{array}
\newcolumntype{I}{!{\vrule width 3pt}}
\newlength\savedwidth
\newcommand\whline{\noalign{\global\savedwidth\arrayrulewidth
                           \global\arrayrulewidth 2pt}%
                  \hline
                  \noalign{\global\arrayrulewidth\savedwidth}}
\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
                           \global\arrayrulewidth 0.5pt}%
                  \hline
                  \noalign{\global\arrayrulewidth\savewidth}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
% \newcommand{\blue}[1]{\textcolor[RGB]{0,0,255}{#1}}



\usepackage{tabulary,multirow,overpic,xcolor,subfloat}

\definecolor{linkcolor}{HTML}{ED1C24}

\newcommand{\bd}[1]{\textbf{#1}}
\newcommand{\app}{\raise.17ex\hbox{$\scriptstyle\sim$}}
\newcommand{\ncdot}{{\mkern 0mu\cdot\mkern 0mu}}
\def\x{\times}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcommand{\dt}[1]{\fontsize{5pt}{0.1em}\selectfont (#1)}

\definecolor{Gray}{gray}{0.5}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother


\newcommand{\lmatch}[1]{{\cal L}_{\rm match}(#1)}

\def\eg{\emph{e.g.}}
\def\Eg{\emph{E.g.,}}
\def\ie{\emph{i.e.}}
\def\etal{\emph{et al.}}



% \iccvfinalcopy % *** Uncomment this line for the final submission
\vspace{-4mm}
\def\iccvPaperID{4328} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Supplementary Material for `\Ours'}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


\section{More Details}
\textbf{Evaluation Metrics.} \textit{Mean intersection-over-union (mIoU)}~\cite{(voc)everingham2010pascal,cheng2022masked}, as the common metric of semantic segmentation, is used to evaluate the performance.
%
For open-vocabulary segmentation, following the prior~\cite{ding2022decoupling,cheng2021sign}, the mIoU averaged on seen classes, unseen classes, and their \textit{harmonic mean} are used.
%



\textbf{Mask Smoothness.} The mask $B_{\hat{\gamma}}$ generated by the Dense CRF often contains jagged edges and numerous small regions that do not correspond to distinct objects in the image.
%
To address these issues, we trained a segmentation model $\bm{\theta}$~(\ie{} Mask2Former), using the mask $B_{\hat{\gamma}}$ generated by the Dense CRF as input. 
%
We then used this model to predict the pseudo labels for the training set of synthetic data, resulting in a final semantic mask annotation
%
% In our experiment, we use the segmentation model $\bm{\theta}$~(\ie{} Mask2Former) to smooth the mask, \ie{} we train the model with $B_{\hat{\gamma}}$ and predict the pseudo label of the training set for synthetic data, which are the final semantic mask annotation.


\textbf{Cross Validation for Noise Learning.}
In the experiment, we performed the three-fold cross-validation for each class.
%
The five-fold cross-validation (CV) is a process in which all data is randomly split into $k$ folds, in our case $k$ $=$ $3$, and then the model is trained on the $k - 1$ folds, while one fold is left to test the quality.
%


\begin{figure*}[t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{iccv2023AuthorKit/figures/fig1_voc.pdf}
	\vspace{-0.1cm}
	\caption{\textbf{Visualization of \diffmask for $20$ classes on Pascal-VOC 2012~\cite{(voc)everingham2010pascal}.} 
    }
    \vspace{-0.2cm}
\label{vis_voc}
\end{figure*}




\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{tables/ADE_semantic.tex}
    \vspace{-0.1cm}
    \caption{\textbf{The mIoU (\%) of Semantic Segmentation on the ADE20K \texttt{val}.}}
    % \vspace{+2mm}
    % \vspace{+2mm
    \label{ADE20k_semantic}
    % \vspace{-2mm}
\end{table}

\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{iccv2023AuthorKit/tables/Ablation_GaP}
    \vspace{-0.2cm}
    \caption{\textbf{Impact of Mask Precision and Domain Gap on VOC 2012 \texttt{val}.} Mask2former~\cite{cheng2022masked} with Swin-B is used as the baseline. `Pseudo' denotes pseudo mask annotation from Mask2former~\cite{cheng2022masked} pre-trained on VOC 2012.}
    % \vspace{+2mm}
    \label{gap}
    % \vspace{-2mm}
\end{table}


\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{iccv2023AuthorKit/tables/feature}
    \vspace{-0.2cm}
    \caption{\textbf{Impact of different attention maps from different layers.} Mask2former~\cite{cheng2022masked} with Swin-B is used as the baseline.}
    % \vspace{+2mm}
    \label{fature}
    % \vspace{-2mm}
\end{table}

\begin{table}[t]
    \centering
    \small 
    % \renewcommand\arraystretch{1.0}
    \setlength{\tabcolsep}{1mm}
    \input{iccv2023AuthorKit/tables/Ablation_Backbone}
    \vspace{-0.2cm}
    \caption{\textbf{Impact of Backbone on VOC 2012 \texttt{val}.} Mask2former~\cite{cheng2022masked} is used as the baseline. }
    % \vspace{+2mm}
    % \vspace{+2mm
    \label{backbone}
    % \vspace{-2mm}
\end{table}

\begin{figure}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{iccv2023AuthorKit/figures/fig2_attention_graident.pdf}
	\vspace{-0.2cm}
	\caption{\textbf{Gradient from Text Tokens for Stable Diffusion.} Prompt language: `\texttt{a horse on the grass}'.}
	\vspace{-0.1cm}
\label{Gradient}
\end{figure}


\begin{figure}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{iccv2023AuthorKit/figures/backbone.pdf}
	\vspace{-0.2cm}
	\caption{\textbf{Impact of Backbone.} Stronger backbone is robust for classification, False Negative, and mask precision.}
	\vspace{-0.1cm}
\label{backbone_vis}
\end{figure}


\section{More Ablation Study}

\textbf{What causes the performance gap between synthetic and real data.}
Domain gap and mask precision are the main reasons for the performance gap between synthetic and real data.
%
Tab.~\ref{gap} is set To further explore the problem.
%
Li \textit{et al.}~\cite{li2023guiding} shows that the pseudo mask of the synthetic image from Mask2former~\cite{cheng2022masked} pre-trained on VOC 2012 is quite accurate, and can as the ground truth.
%
Thus, we also use the pseudo label from the pre-trained Mask2former to train the model, where we argue that the pseudo label is accurate.
%
As shown in Tab.~\ref{gap}, mask precision cause $6.4\%$ mIoU gap, and the domain gap of images causes  $4.5\%$ mIoU gap.
%
Notably, for the \texttt{bird} class, the use of synthetic data with a pseudo label resulted in better results than the corresponding real images. 
%
This observation suggests that there may be no domain gap for the \texttt{bird} class in the VOC 2012 dataset.

\begin{figure*}[!t]
% 	\centering
	\includegraphics[width=0.99\linewidth]{iccv2023AuthorKit/figures/fig2_cityscapes.pdf}
	% \vspace{-0.2cm}
	\caption{\textbf{Visualization for the model trained with \textit{only} \diffmask on Cityscapes.} \diffmask presents a competitive performance on challenging driving scenario.}
	% \vspace{-0.2cm}
\label{VIS122}
\end{figure*}
\textbf{Backbone}
Tab.~\ref{backbone} presents the ablation study for the backbone.
%
For some classes, \eg{} \texttt{sheep}, the stronger backbone can bring obvious gains, \ie{} Swin-B achieves $27.5\%$ mIoU improvement than that of ResNet 50.
%
And the mIoU of all classes with Swin-B achieves $19.2\%$ mIoU improvements.
%
It is an interesting and novel insight that a stronger backbone can reduce the domain gap between synthetic and real data. 
%
To give a further analysis for that, we present some results comparison of visualizations, as shown in Fig.~\ref{backbone_vis}.
%
Swin-B brings an obvious improvement in classification, False Negatives, and mask precision.
%
Compared with the gain between different backbones, different versions (size) of the same backbone seems can not obtain an effective gain, \eg{} ResNet101 only obtain $0.5\%$ mIoU improvements than that of ResNet50.


\textbf{Attention Maps of different resolutions.}
Table \ref{fature} shows the results of an ablation study conducted on cross attention maps with varying resolutions from different layers. The performance of both high resolution ($64\times64$) and low resolution ($8\times8$) maps was found to be unsatisfactory. This can be attributed to the lack of detail in low-resolution maps and the presence of noise in high-resolution maps. On the other hand, integrating (by averaging) all attention maps produced the best performance.
%




\section{Experiment on ADE20K}
%
ADE20K, as one more challenging dataset, is also used to evaluate the \Ours.
%
Tab.~\ref{ADE20k_semantic} presents the results of three categories~(\texttt{bus}, \texttt{car}, \texttt{person}) on ADE20K.
%
With fewer synthetic images~($6$k), we achieve a competitive performance than that of a mass of real images~($20.2$k).
%
Compared with the other two categories, Class \texttt{car} achieves the best performance, with $73.4\%$ mIoU. 

\section{Visual explanation with gradients.} The gradient is another way to provide an excellent visual explanation of the generative model, Fig.~\ref{Gradient} presents the corresponding gradient visualization from different text tokens.
%
Given a text prompt $\mathcal{P}$, \ie{}, `\texttt{a horse on the grass}' and a random Gaussian image noise $z$, the text-guided generative model is in principle capable of modeling conditional distributions of the form $\mathcal{I}:= p(z \vert \conditioner(\mathcal{P}))$, where $\conditioner(\mathcal{P}) \in \R^{N\times d }$ and $\conditioner$ refers to the text encoder~\cite{radford2021learning}.
%
For the $k$-th word $t_k$~(\eg{}, `\textit{horse}' from Fig.~\ref{Gradient}) from $\mathcal{P}$, we can compute the corresponding gradient as following: $\alpha^{}_{k} = \vphantom{\sum_{i}\sum_{j}} \frac{\partial \mathcal{I}}{\partial t_k}$
% \begin{equation} 
% \label{eq:alpha1}
%     \alpha{}_{i} = \vphantom{\sum_{i}\sum_{j}} \frac{\partial A_{ij}}{\partial t_i}
% \end{equation}
%
% , where $A_{ij}$ refers to the pixel of generative image $\mathcal{I}$. 
%
where $\alpha{}_{k}$ is the gradient weight from the $k$-th word $t_k$.
%
The corresponding gradient weight can be computed by adding a small variate (that is, Numbers close to zero) to the $t_k$.
%
For convenience, we add a small variate $\bigtriangleup \beta \in \R^{d} $~($\bigtriangleup \beta = \mu \vec{\textbf{1}}_{d}$, where $\vec{\textbf{1}}_{d}$ and $\mu$ refer to the unit matrix and weight) to the text feature map $\conditioner(t_k)$ and obtain the corresponding gradient visualization, as shown in Fig.~\ref{Gradient}.
%
The gradient visualization is highly class-discriminative (\ie{} the `\texttt{horse}' explanation exclusively highlights the `\textit{horse}' regions).



\section{Limitation.} \diffmask mainly includes two limitations: 1) The inference speed of the text-to-image diffusion model is relatively slow.
%
With 8 Tesla V100 GPUs, generating $10k$ images usually need to spend around 8 hours.
%
Therefore, scaling up the synthetic dataset to a million level is difficult for some institutions.
%
And it is the main reason why we do not provide more experiments for other datasets with rich categories, \eg{} ADE20K or COCO.
%
Similarly, we can not scale up the synthetic data to the million level due to the limitation of time and computational cost.
%
But we argue the cost can be reduced by adopting advanced faster Sampling for Diffusion Models~\cite{song2020score,karras2022elucidating}.
%
2) There are still existing obvious result gaps for some classes, \eg{} \texttt{person} on VOC 2012.
%
The main reason is the obvious domain gap for these classes.
%
The synthetic image usually presents a simple foreground and background, while the real image is more examples with multi-views, multi-scales, blur, and occlusion.
%
Even so, our \diffmask, as the first work to synthesize image and mask annotation using an image-text pre-trained diffusion model, provide a promising performance and many new insights.
%
We verify the feasibility of training with text-driven synthetic data and applications in the real world, where worth mentioning the diffusion model is trained with only language-image pairs.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}