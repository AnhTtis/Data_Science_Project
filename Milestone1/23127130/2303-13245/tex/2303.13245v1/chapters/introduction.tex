\section{Introduction}
\label{sec:introduction}
Self-supervised learning (SSL) has gone a long and successful way since its beginning using carefully hand-crafted proxy tasks such as colorization \cite{larsson2017colorization}, jigsaw puzzle solving \cite{noroozi2016unsupervised}, or image rotations prediction \cite{gidaris2018unsupervised}. In recent years, a consensus seems to have been reached, and \textit{cross-view consistency} is used in almost all state-of-the-art (SOTA) visual SSL methods  \cite{chen2020simple,he2020momentum,grill2020bootstrap,caron2020unsupervised,caron2021emerging}. In that context, the whole training objective revolves around the consistency of representation in the presence of information-preserving transformations \cite{chen2020simple}, e.g., \textit{blurring}, \textit{cropping}, \textit{solarization}, etc. Although this approach is well grounded in learning image-level representations in the unrealistic scenario of \textit{object-centric} datasets, e.g., ImageNet \cite{5206848}, it cannot be trivially extended to accommodate \textit{scene-centric} datasets and even less to learn dense representations. Indeed, in the presence of complex scene images, the random \textit{cropping} operation used as image transformation loses its semantic-preserving property, as a single image can yield two crops bearing antipodean semantic content \cite{mo2021object,van2021revisiting,purushwalkam2020demystifying,selvaraju2021casting}. Along the same line, it's not clear how to relate sub-regions of the image from one crop to the other, which is necessary to derive a localized supervisory signal.
\par
To address the above issue, some methods \cite{mo2021object,selvaraju2021casting} constrain the location of the crops based on some heuristics and using a pre-processing step. This step is either not learnable or requires the use of a pre-trained model.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/joint_vs_intersection.png}
    \caption{
    \textbf{Schematic for different categories of self-supervised learning methods for dense downstream tasks.} a) Prior to the training, a pre-trained model or color-based heuristic is used to produce the clustering/matching of the whole dataset. c) The matching/clustering is identified online but restrains the domain of application of the loss to the intersection of the two views. b) Our method takes the best of both worlds, leverages online clustering, and enforces constraints on the whole spatiality of the views.
    }
    \label{fig:joint_vs_intersect}
\end{figure}
%Various approaches have been proposed to tackle the aforementioned limitations. The issue that arises from the randomness of the cropping site is typically treated by supervising the locations of the crops. This pre-processing steps, is either not learnable or requires the usage of a pre-trained model \cite{mo2021object,selvaraju2021casting}, which might simply not exists for some modalities, e.g., medical images. 
Alternatively, the location of the crops (\textit{geometric pooling} \cite{ziegler2022self,xiao2021region}) and/or an attention mechanism (\textit{attentive pooling} \cite{ziegler2022self,xie2021propagate,o2020unsupervised,wen2022self,wang2022exploring}) can be used to infer the region of overlap in each view and only apply the consistency objective to that region (Fig.~\ref{fig:joint_vs_intersect}\textcolor{red}{.c}). A consequence of these pooling mechanisms is that only a sub-region of each view is exploited, which mislays a significant amount of the image and further questions the usage of \textit{cropping}. There are two strategies to tackle the issue of locating and linking the objects from the two views: the first is a feature-level approach that extends the global consistency criterion to the spatial features after inferring pairs of positives through similarity bootstrapping or positional cues \cite{liu2020self,xie2021propagate,bardes2022vicregl,wang2021dense,li2021efficient,ziegler2022self}. It is unclear how much semantics a single spatial feature embeds, and this strategy can become computationally intensive. These issues motivate the emergence of the second line of work which operates at the object-level \cite{wen2022self,henaff2021efficient,henaff2022object,xie2021unsupervised,van2021unsupervised,wang2022freesolo,wei2021aligning}. In that second scenario, the main difficulty lies in generating the object segmentation masks and matching objects from one view to the other. The straightforward approach is to leverage unsupervised heuristics \cite{henaff2021efficient} or pre-trained models \cite{xie2021unsupervised} to generate pseudo labels prior to the training phase (Fig.~\ref{fig:joint_vs_intersect}\textcolor{red}{.a}), which is not an entirely data-driven approach and cannot be trivially extended to any modalities. Alternatively, \cite{henaff2022object} proposed to use K-Means and an additional global image (encompassing the two main views) to generate online pseudo labels, but this approach is computationally intensive. \par
To address these limitations, we propose $\mname$, whose underpinning mechanism is an efficient \textbf{Cr}oss-view \textbf{O}nline \textbf{C}lustering that conjointly generates segmentation masks for the union of both views (Fig.~\ref{fig:joint_vs_intersect}\textcolor{red}{.b}).

% Understanding object-level relationships are crucial for learning visual representation from unlabeled \textit{scene-centric} data. Nonetheless, enforcing correspondence across image views at the semantic level requires acquiring exhaustive pixel-level semantic labels, typically unavailable or expensive to obtain. In response to this limitation, current works still heavily rely on heuristic strategies based on hand-crafted objectness priors, such as using segmentation heuristics to localize objects \cite{henaff2021efficient} or unsupervised clustering \cite{henaff2022object}.\Behzad{(Highlighting only \cite{henaff2021efficient} might be tricky because in the subsequent work, Odin, the authors already mentioned the limitation of DetCon, and tried to address it. Or mention limitations of both DetCon and Odin.)}. This issue may impede their generalizability of the representation and levels of domain-specific prior, e.g., manual intervention, required to obtain objectness priors.}
% \par
% \Thomas{
% The current trend in SSL mostly relies on image-level pretext tasks. Although well grounded as an instance discrimination \cite{chen2020simple,he2020momentum}, this trend cannot be directly applied to derive localized supervisory signal, e.g., object level representation, as it disregards the localization of the objects in the real-world image data with the complex layout. More precisely, it fails to answer the following questions: \textit{where are the objects in the image}? and \textit{how to link semantically related objects}? 
% We address these questions in Sec.~\ref{ssec:where_objects?} and Sec.~\ref{ssec:link_objects?}, respectively.
% We introduce a new object-aware self-supervised learning method \dots

Our main contributions are: 1) we propose a novel object-level self-supervised learning framework that leverages an online clustering algorithm yielding segmentation masks for the union of two image views. 2) The introduced method is inherently compatible with scene-centric datasets and does not require a pre-trained model. 3) We empirically and thoroughly demonstrate that our approach rivals or out-competes existing SOTA self-supervised methods even when pre-trained in an unfavorable setting (smaller and more complex dataset). 
% \begin{itemize}
%     \item Novel cluster-level self-supervised signal (object-level or part-object-level).
%     \item New evaluation procedure for self-supervised models.
%     \item (Object discovery).
% \end{itemize}
% n summary, our main contributions in this paper are: 1) We show that the decomposition of natural scenes (semantic grouping) can be done in a learnable fashion and jointly optimized with the representations from scratch. 2) We demonstrate that semantic grouping can bring object-centric representation learning to large-scale real-world scenarios. 3) Combining semantic grouping and representation learning, we unleash the potential of scene-centric pre-training, largely close its gap with object-centric pre-training and achieve state-of-the-art results in various downstream tasks 
%}