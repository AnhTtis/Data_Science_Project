\section{Method}
\label{sec:method}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/samno_pipeline_small.png}
    \caption{
    \textbf{Overview of $\mname$.} The augmented views, $\aug_1$ and $\aug_2$, are processed independently by a ViT encoder $\backbone$. The \textit{joint} representation, $\dense_{\text{cat}}$, of the two image views, is obtained by concatenation along the token axis and serves as input to the clustering algorithm, $\mathcal{C}$, to \textit{locate} the objects. The joint clustering assignments, $\Q^{*}$, are \textit{split} view-wise and used to compute the corresponding centroids. A self-distillation loss enforces consistency between pairs of related centroids via a projection head $h$.}
    \label{fig:pipeline}
\end{figure*}
% \Behzad{I suggest starting with a subsection 3.1 (Method Overview) with a tighter connection with Figure 1. Please move Figure 1 to page 3. Basically, this subsection should cover the high-level description of full method, Figure 1 and the rationale behind the proposed approach. Then, we bring details in the following subsections. I assume you add the description of Figure 1 here, then we continue with the rationale behind below ...}
\subsection{Overview}
\label{ssec:method_overview}
This paper tackles the problem of learning dense visual representations from unlabeled scene-centric data. Recent efforts using a self-supervised multi-view consistency paradigm to address this problem rely on a two steps procedure: \textit{i)} \textit{locate} the objects in each image view and \textit{ii)} \textit{link} the related objects from one image view to the other. We now discuss how $\mname$ elegantly palliates the limitations evoked in sections~\ref{sec:introduction} and~\ref{sec:related_work}.
% In sections~\ref{sec:introduction} and \ref{sec:related_work}, we evoke the fact that existing methods either restrain the region on which consistency is enforced to the intersection of the two crops or rely on offline heuristics and/or pre-trained models to pseudo-label the whole dataset. 




%\Thomas{
%The \textit{de facto} method to learn dense representations in a multi-crops self-supervised manner, relies on a two steps procedure \textit{i)} \textit{locate} the objects in each crop/view and \textit{ii)} \textit{link} the related objects from one view two the other. In Sections~\ref{sec:introduction} \& \ref{sec:related_work}, we evoke the potential pitfalls of this strategy. We now discuss how $\mname$ elegantly tackles these limitations.
%} 
\par
We observe that most of the difficulties arise because the \textit{locate-link} strategy treats the two image views independently. In contrast, both views stem from the same image, and their representations lie in the same space. The former observation offers the possibility to benefit from the coordinates of the cropped image regions as a cue for the \textit{locate} step, while the latter indicates that some operations could be performed conjointly. Consequently, we propose to depart from the typical strategy and introduce a novel paradigm dubbed \textit{join-locate-split}, described below:

%\Thomas{
% The former observation offers the possibility to benefit from the coordinates of the cropping site as a cue for the \textit{locate} step, while the latter indicates that some operations could be performed conjointly. As a consequence we propose to depart from the common strategy and introduce a novel one: \textit{join-locate-split}.
%}
\par \noindent
\textbf{Join.}
The two augmented image views, $\aug_1$ and $\aug_2$, are processed by a ViT \cite{dosovitskiy2020image} encoder $\backbone$ yielding the dense visual representations $\dense_{\{1,2 \}} \in \mathbb{R}^{N \times d}$, where $N$ and $d$ denote the number of spatial tokens and feature dimension, respectively. The dense visual representations are then concatenated along the token axis to obtain the joint representation, $\dense_{\text{cat}} \in \mathbb{R}^{2N \times d}$.
 \par\noindent
\textbf{Locate.}
The objective is to find semantically coherent clusters of tokens in the joint representation space. As the quality of the input representation improves, we expect the found clusters to represent the different objects or object parts illustrated in the image. The joint representation is fed to the clustering algorithm $\mathcal{C}$, which outputs the joint clustering assignments, $\Q^{*} \in \mathbb{R}^{2N \times K}$. The soft assignments matrix $\Q^{*}$ models the probability of each of the $2N$ tokens to belong to one of the $K$ clusters found in the joint space.  
% \Behzad{The clusters could represent meaningful semantics, e.g., objects.}
 \par \noindent
\textbf{Split.}
By splitting $\Q^{*}$ in two along the first dimension, the assignment matrix of each view, namely $\Q^{*}_{\{1,2 \}} \in \mathbb{R}^{N \times K}$ are obtained. One can observe that the \textit{link} operation is provided for free and that it is trivial to discard any cluster that does not span across the two views.
\par
Given the view-wise assignments $\Q^{*}_{\{1,2 \}}$, and the corresponding dense representations $\dense_{\{1,2 \}}$, $K$ object/cluster-level representations can be obtained for each view:
\begin{align}
    \label{eq:compute_cetntroids}
    \begin{split}
    \centroids^{\top}_{1} = \dense^{\top}_{1} \Q^{*}_{1}
    \end{split}
\end{align}
% by a sum of the rows of $\dense_{i}$ weighted by the columns of $\Q^{*}_{i}$
%
$\centroids$ denotes the centroids. Analogously to the image-level consistency objective, one can enforce similarity constraints between pairs of centroids.

% It follows that a consistency loss between pairs of centroids from each view can be enforced in a reciprocal manner to what would be done at the image-level.
% We leverage ViTâ€™s class (\texttt{[CLS]}) token for the image-level representation, which also results from a weighted sum of the other tokens' representations. 


%Indeed, in the case of a ViT, the image-level representation is the class (\texttt{[CLS]}) token, which also results from a weighted sum of the other token representations. 

% The introduced self-supervised learning method leverages a teacher-student distillation approach to provide dense supervisory signal at the semantic level. 
% We propose to mitigate these issues by presenting our novel framework (dubbed SAMNO), a new \textit{in-image self-labeling} approach to efficiently produce online segmentation masks. These online masks can then be used to enforce consistency between semantically connected objects or object parts. \Thomas{We now detail the nuts and bolts of $\mname$.}
%Enforcing consistency at the semantic-level requires to first acquire exhaustive pixel-level labels, which are typically not available and/or expensive to obtain. As a response to this limitation, pseudo-labels such as color-based segmentation masks have been successfully incorporated to guide the local representations \cite{henaff2021efficient}.As the generation of the pseudo-labels is color-based and not learnable, it begs the question whether or not this approach can generalize to different models, e.g. medical imaging, and how much manual intervention is required to obtain decent masks in such situations. 
 %\Thomas{We propose to mitigate these issues by using an in-image self-labelling approach to efficiently produce online segmentation masks, which can then be used to enforce consistency between semantically connected objects or object-parts. 
%We now detail the nuts and bolts of $\mname$}
%\Thomas{
%The current trend in image-level SSL is to rely on a contrastive pretext task to encourage the similarity of semantically related images (positive pairs) and conversely for unrelated images (negative pairs). In the absence of labels, the \textit{de facto} approach \cite{chen2020simple,he2020momentum} is to employ information preserving transformations i.e., data augmentation, to construct positive pairs and to consider any other two images as a negative pair. Although well grounded as an image-level supervision, this method cannot be directly applied to derive localized supervisory signal, as it disregards the localization of the objects in the image. More precisely, it fails to answer the following questions: \textit{where are the objects in the image?} and \textit{how to link semantically related objects?} We address these questions in Sec.~\ref{ssec:where_objects?} and Sec.~\ref{ssec:link_objects?}, respectively.
% Towards that end, we employ identical teacher and student models, respectively $g_{t}$ and $g_{s}$, both of which are composed of a backbone, $f$, and a projection head, $h$, hence we have $g_{t} = h_{t} \circ f_{t}$ and similarly for the student model. 
%}


%Enforcing object-level consistency requires identifying and localizing the objects represented in a given image to attribute a semantic label to each pixel. 


%\Thomas{
 %In practice, since the consistency objective is only applied at the feature-level, it is sufficient to obtain labels for the spatial features $\dense \in \mathbb{R}^{N \times d}$, whose resolution is typically much coarser than that of the original image $\im \in \mathbb{R}^{C \times H \times W}$. 
 %As a consequence, it offers the possibility to bootstrap the features similarity to compute the masks, which eventually boils down to clustering the features space. 
 
%There exists a plethora of valid algorithms to perform the clustering task. For that reason, we first assume that the exists an oracle able to return a meaningful assignment over $K$ clusters when fed with $N$ spatial features. The produced assignments takes the shape of a matrix, $\Q \in \mathbb{R}^{N \times K}$, where the entry $\Q_{nk}$ represents the contribution of the $n^{th}$ token to the $k^{th}$ centroid. Under that assumption we directly dig into the details of the self-distillation mechanism proposed to enforce localized consistency and then come back to the clustering oracle in section \ref{ssec:where_objects?}.
%}

% \subsection{How to link Semantically Related Objects? }
% \label{ssec:link_objects?}
% Under the self-distillation paradigm, obtaining pairs of semantically related images, i.e., \textit{positives}, is straightforward as it suffices to rely on different augmented versions, $\aug_{1}$ and $\aug_{2}$, of a given image $x$. On the other hand, it is less evident how to link the constituent objects and sub-regions of $\aug_{1}$ and $\aug_2$. In particular, the knowledge of the two views clustering assignments, $\Q_{1}$ and $\Q_{2}$, is of little help as the clustering objective is permutation invariant, i.e., the \textit{cat} cluster of one view might be mapped to a different column in the corresponding assignment matrix than in that of the second view.

% Although combinational optimization algorithms, e.g., Hungarian matching, could solve this issue, it is only valid under the hypothesis that the two views share the same semantic content, which often fails to hold for scene-centric data. Alternatively, a common approach is to rely on variations of RoI-Pooling \cite{jiang2018acquisition} or RoI-Align \cite{he2017mask} to align image areas across two views. As a consequence, consistency is only enforced at the intersection of the two views, which mislays the benefits of spatial transformations.

% As opposed to that, our method operates on \textit{the union space of the two image views} by leveraging a simple yet effective trick (\Behzad{do we have any formulation or figure here?}). Instead of trying to match the two views clusters, we perform the clustering operation on the joint view (obtained by concatenating the two views' dense representations) and then split the clusters view-wise. Any cluster which does not span across the two views is simply discarded, such that we elegantly avoid the pitfall of encouraging similarity between different objects

%\Thomas{
%Alternatively, a common approach \cite{ziegler2022self,xiao2021region} is to rely on variations of RoI Pooling \cite{jiang2018acquisition} or RoI Align \cite{he2017mask} to find the region common to the two views. 
%As a consequence, the consistency is only enforced at the intersection of the two views, which mislays the benefits of spatial transformations. 
%As opposed to that, our method operates on the union space of the two views by leveraging a simple yet effective trick (\Behzad{do we have any formulation here?}): instead of trying to match the two views clusters, we perform the clustering operation on the joint view (obtained by the concatenation of the two views dense representations) and then split the clusters view-wise. Any cluster which does not span across the two views is simply discarded, such that we elegantly avoid the pitfall of encouraging similarity between different objects. 
% In the above paragraph, the proposed method to conjointly segment and extract representations of the different things and stuffs represented in a single image is detailed. Nonetheless, it remains unclear how it can be used in SSL setting and in particular, it begs the question: \textit{How can we construct positive pairs of object representations?} Indeed, the standard multi-crop strategy is ill-suited for this setting as the inter-crops matching of the centroids is ambiguous, especially when the semantic content differs from one crop to the other. Surprisingly, the solution is quite simple: it suffices to perform the object discovery on the joint representations.
%}

\subsection{Dense self-distillation}
\label{ssec:dense_self_distillation}
This section details the integration of the \textit{join-locate-split} strategy (Sec.~\ref{ssec:method_overview}) in a self-distillation scheme\footnote{Our implementations build upon DINO \cite{caron2021emerging}, but it's not limited to it.}.
% Our dense self-distillation pipeline builds upon siamese networks using a ViT \cite{dosovitskiy2020image} as the vision encoder, $f$. Given an image $\im \in \mathbb{R}^{C \times H \times W}$, the object-level consistency can be formulated at the feature level to obtain semantic labels for the spatial features $\dense \in \mathbb{R}^{N \times D}$, whose resolution is typically much coarser than the original image, where $N$ denotes the number of spatial tokens. Consequently, it offers the possibility to bootstrap the features' similarity to compute the segmentation masks, which eventually boils down to clustering the features space.
% We assume that there exists a \Thomas{clustering oracle, $\mathcal{C}$,} that returns a meaningful assignment over $K$ \Thomas{non-overlapping} clusters when fed with $N$ spatial tokens (features), \Thomas{ i.e.:
% \begin{equation}
    % \label{eq:oracle}
    % \Q = \mathcal{C}(\dense )
% \end{equation}
% The produced assignments take the shape of a matrix, $\Q \in \mathbb{R}^{N \times K}$, where the entry $\Q_{nk}$ represents the contribution of the $n^{th}$ token to the $k^{th}$ centroid. Under that assumption, we directly dig into the details of the proposed self-distillation mechanism to enforce localized consistency (\Behzad{Sec.~\ref{ssec:link_objects?}?}) and then detail the clustering \Thomas{oracle, $\mathcal{C}$,} in Sec.~\ref{ssec:where_objects?}.
% In the above two sections, we described a mechanism to match semantically related pairs of regions between two crops of a given image and further detailed a method to obtain such semantically coherent regions. It, therefore, remains to explain how the aforementioned elements can be combined to create a stand-alone SSL pre-training scheme for dense downstream tasks.
Our self-distillation approach relies on a teacher-student pair of Siamese networks, $\model_t$ and $\model_s$, each composed of an encoder $\backbone_{\{t,s \}}$ and a projection head $\head_{\{t,s\}}$. Given the input image $\im \in \mathbb{R}^{C \times H \times W}$, two augmented views $\aug_{1}$ and $\aug_{2}$ are obtained using random augmentations. Both augmented views are independently passed through the teacher and student encoders, yielding the spatial representations $\dense_{t,\{1,2\}}$ and $\dense_{s,\{1,2\}}$, respectively. The teacher model's representations are concatenated (\textit{join}) and fed to the clustering algorithm (Sec.~\ref{ssec:where_objects?}) to obtain the assignment matrix $\Q^{*}$ (\textit{locate}), which is assumed to be already filtered of any column corresponding to an object/cluster represented in only one of the two views (cf. Sec.~\ref{sssec:pruning}). The assignment matrix is \textit{split} view-wise to get $\Q^{*}_{\{1,2 \}}$ and to compute the teacher and student centroids of each view:
\begin{align}
\begin{split}
        % \centroids^{\top}_{t,\{1,2 \}} &= \dense^{\top}_{t,\{1,2\}} \left( \mask_{\{1,2 \}} \odot \Q^{*}_{\{1,2\}} \right) \\
        % \centroids^{\top}_{s,\{1,2 \}} &= \dense^{\top}_{s,\{1,2\}} \left( \mask_{\{1,2 \}} \odot \Q^{*}_{\{1,2\}} \right)
        \centroids^{\top}_{\{t,s\},\{1,2 \}} &= \dense^{\top}_{\{t,s \},\{1,2\}} \Q^{*}_{\{1,2\}} \\
        % \centroids^{\top}_{s,\{1,2 \}} &= \dense^{\top}_{s,\{1,2\}} \Q^{*}_{\{1,2\}}
\end{split}
\end{align}
The final step is to feed the teacher and student centroids, $\centroids_{t}$ and $\centroids_{s}$, to the corresponding projection heads, $\head_{t}$ and $\head_{s}$, which output probability distributions over $L$ dimensions denoted by $\PP_{t}$ and $\PP_{s}$, respectively. The probabilities of the teacher and student models are obtained by normalizing their projection heads' outputs with a \texttt{softmax} scaled by  temperatures $\tau_{t}$ and $\tau_{s}$:
%The final step is to feed the centroids to the projection head, $\head$, which models the probability of a given centroid to belong to the $L$ learnable concepts stored by the head:
\begin{align}
    \label{eq:projection}
    \begin{split}
    \PP_{t, \{1,2 \}} &= \underset{L}{\texttt{softmax}}\left(\head_{t}(\centroids_{t,\{1,2\}}) / \tau_{t} \right)  \\
    \PP_{s, \{1,2 \}} &= \underset{L}{\texttt{softmax}}\left(\head_{s}(\centroids_{s,\{1,2\}}) / \tau_{s} \right)  \\
    % \PP_{s, \{1,2 \}} &= \head_{s}(\centroids_{s,\{1,2\}}) 
    \end{split}
\end{align}
The dense self-distillation objective $\mathcal{L}_{\text{dense}}$ enforces cross-view consistency of the teacher and student model projections using the cross-entropy loss:
\begin{equation}
    \label{eq:dense_loss}
\mathcal{L}_{\text{dense}} = \frac{1}{2}\left( H(\PP_{t, 1}, \PP_{s,2}) +  H(\PP_{t, 2}, \PP_{s,1}) \right)
\end{equation}
where $H(\mathbf{A}, \mathbf{B}) = -\frac{1}{K} \sum_{k=1}^{K} \sum_{l=1}^{L} \mathbf{A}_{kl} \log (\mathbf{B}_{kl})$ computed by averaging over all clusters.\par
% \Behzad{averaging over all spatial locations and clusters.}

For the dense self-distillation loss to be meaningful, the clustering assignments of spatial tokens corresponding to similar objects must be semantically coherent, which requires good-quality representations. To address this issue, we additionally apply a global representation loss by feeding the image-level representations to a dedicated projection head, $\overbar{\head}$, to obtain the $\overbar{L}$-dimensional distributions:

%However, we may face a chicken-egg problem. In order for the loss to be meaningful, the produced clustering assignments must be semantically coherent, which prerequisites qualitative dense representations, i.e., that a meaningful loss was previously enforced. To address this issue, we apply a loss to the image-level representations, $\glob$. Similarly to the above case, the global representations are fed to a dedicated image-level projection head, $\overbar{\head}$, to obtain the $\overbar{L}$-dimensional distributions:
% However, we may face a chicken-egg problem: two mutually interconnected sub-problems of obtaining coherent clusters for the loss to be meaningful. At the same time, the clustering quality is dependent on that of the dense representations, $\dense$. To address this issue, we propose applying a loss to the image-level representations, $\glob$. The global representations are first fed to a dedicated image-level projection head, $\overbar{\head}$, to obtain the $\overbar{D}$-dimensional distributions:}
%One can observe that, as it is, we might be facing a chicken-egg problem. Indeed, we need to obtain coherent clusters for the loss to be meaningful, and at the same time the clustering quality is dependant on that of the dense representations, $\dense$. 
%For that reason, a loss is also applied on the image-level representations, $\glob$. The global representations are first fed to a dedicated image-level projection head, $\overbar{\head}$, to obtain the $\overbar{D}$-dimensional distributions:

\begin{align}
    \label{eq:image_proj}
    \begin{split}
    \p_{t, \{1,2 \}} &= \underset{\overbar{L}}{\texttt{softmax}} \left(\overbar{\head}_{t}(\glob_{t,\{1,2\}} / \overbar{\tau}_{t})\right) \\
    \p_{s, \{1,2 \}} &= \underset{\overbar{L}}{\texttt{softmax}} \left(\overbar{\head}_{s}(\glob_{s,\{1,2\}} / \overbar{\tau}_{s})\right)
    % \p_{s, \{1,2 \}} &= \overbar{\head}_{s}(\glob_{s,\{1,2\}})     
    \end{split}
\end{align}
%
The sharpness of the output distribution for teacher and student models is controlled by the temperature parameters $\overbar{\tau}_{t}$ and $\overbar{\tau}_{s}$, respectively, and $\glob_{\{t, s\}}$ denotes the image-level representations of the teacher and student models. Hence the global representation loss $\mathcal{L}_{\text{glob}}$ is computed as follows:
\begin{equation}
    \label{eq:glob_loss}
    \mathcal{L}_{\text{glob}} =  \frac{1}{2}\left( H(\p_{t, 1}, \p_{s,2}) +  H(\p_{t, 2}, \p_{s,1}) \right)
\end{equation}
where $H(\mathbf{a}, \mathbf{b}) = -\sum_{l=1}^{\overbar{L}} \mathbf{a}_{l} \log (\mathbf{b}_{l})$. Therefore, the overall loss function used for the training of $\mname$ is:
\begin{align}
    \label{eq:total_loss}
    \mathcal{L} = \alpha \mathcal{L}_{\text{dense}} + \mathcal{L}_{\text{glob}}
\end{align}
where $\alpha$ denotes a hyperparameter to balance the loss terms. We set $\alpha=1.0$ for all experiments without the need for hyperparameter tuning.


\subsection{Where are the objects in the image?} 
\label{ssec:where_objects?}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/segmentation_masks.png}
    \caption{
    \textbf{Representation of the iterative clustering algorithm in the joint space.} The algorithm is initialized with a fixed number of centroids that are iteratively merged until only two remain. The ideal number of centroids is determined \textit{a posteriori}. The procedure's last seven steps (columns) are represented for three different heads of a ViT-S/16 pre-trained with \mname. 
    }
    \label{fig:segmentation_masks}
\end{figure*}
So far, we assumed that there existed an algorithm able to assign a set of input data points to an undetermined number of clusters. This section covers the details of this algorithm.\par
The online optimization objective for computing the clusters and corresponding assignments relies on an optimal transport formulation and the Sinkhorn-Knopp algorithm \cite{cuturi2013sinkhorn}. This choice is motivated by \textit{i)} its efficiency, \textit{ii)} the ease of incorporating  external knowledge (Sec.~\ref{sssec:position_cues}), and \textit{iii)} it returns a measure of the clustering quality, which can be used to infer the optimal number of clusters $K$ for a given image. The last point is of utmost importance as it allows us to devise a \textit{ad-hoc} selection criterion for $K$. Indeed, the iterative procedure progressively merges the centroids until only two remain, i.e., background/foreground (see Fig.~\ref{fig:segmentation_masks}). The number of centroids $K$ is selected \textit{a posteriori} and independently for each image in the batch.
%\Thomas{
%As already mentioned, there are no specific constraints with regards to the clustering algorithm. Our implementation relies on an optimal transport formulation and the Sinkhorn-Knopp algorithm \cite{cuturi2013sinkhorn} to compute the clusters and the corresponding assignments. This choice is motivated by \textit{i)} its efficiency, \textit{ii)} the ease to incorporate external knowledge and \textit{iii)} it returns a measure of the clustering quality which can be used to infer the optimal number of clusters, $K$, for a given image. The last point is of utmost importance as it allows us to devise an iterative procedure, which progressively merges the centroids until there only remain two, i.e., background/foreground, at which point $K$ is selected \textit{a posteriori} and independently for each image in the batch.} 
\par
More formally, let's consider a ViT encoder $\backbone$ fed with a positive pair of augmented views, $\aug_{1}$ and $\aug_{2}$, and yielding the corresponding representations, $\dense_{1}$ and $\dense_{2}$. The clustering is performed on the joint representation, $\dense_{\text{cat}} \in \mathbb{R}^{2N \times d}$ obtained from the concatenation of $\dense_{1}$ and $\dense_{2}$ along the token axis. The procedure starts by sampling $K_{\text{start}}$ of the $2N$ tokens, which serve as initialization for the centroids, $\centroids \in \mathbb{R}^{K_{\text{start}} \times d}$:
\begin{equation}
    \label{eq:cent_init}
    \centroids = \Y^{\top} \dense_{\text{cat}}
\end{equation}
%
where $\Y \in \{0,1\}^{2N \times K_{\text{start}}}$ is a matrix of column one-hot vectors indicating the position of the $K_{\text{start}}$ tokens used to initialize the centroids.
% The sampling can either be uniform, in which case the probability of an object being initially represented is proportional to the area it occupies in the image, or based on the vision transformer's \textit{self-attention map}, which highlights the patches proportionally to their contribution to the image-level representation. The cost of assigning a token to a given centroid should reflect their similarity, hence:
The sampling is based on the \textit{attention map} of the \texttt{[CLS]} token, which highlights the patches proportionally to their contribution to the image-level representation. The cost of assigning a token to a given centroid should reflect their similarity, hence:
 \begin{equation}
     \label{eq:cost}
     \costsem = - \dense_{\text{cat}} \centroids^{\top}
 \end{equation}
%
where $\costsem \in \mathbb{R}^{2N \times K}$ denotes the cost matrix of the assignments. A handy property of the selected clustering algorithm is that it offers the possibility to scale the importance of the tokens and centroids based on external knowledge injected using a token distribution $\tmarg$ and a centroid distributions $\cmarg$. Here, the \textit{attention map} of the \texttt{[CLS]} token is used as the token distribution due to its ability to highlight the sensible semantic regions of the image \cite{caron2021emerging}. Along the same line, the centroids distribution is defined as:
% Not all tokens and centroids are equally important, which is taken into consideration by introducing a token distribution, $\tmarg$, and centroid distribution, $\cmarg$, that capture their relative importance. The token distribution, $\tmarg$, is set in accordance with the distribution used for sampling the initial centroids. Similarly, the centroids distribution is obtained as follows:
 \begin{equation}
     \label{eq:cmarg}
     \cmarg =  \texttt{softmax}(\Y^{\top} \tmarg)
 \end{equation}
%
% where $\Y \in \{0,1\}^{2N \times K_{\text{start}}}$ is a matrix of column one-hot vectors indicating the position of the $K_{\text{start}}$ tokens used  to initialize the centroids.
Given the cost matrix, $\costsem$, and the two marginals, $\tmarg$ and $\cmarg$, the Sinkhorn-Knopp clustering produces the assignment matrix $\Q^{*}$:

\begin{equation}
    \label{eq:assignments}
    \Q ^{*} = \underset{\Q \in \mathcal{U}(\tmarg, \cmarg)}{\arg\min} <\Q, \costsem> - \frac{1}{\lambda} H(\Q)
\end{equation}
where $<\cdot, \cdot >$ denotes the entry-wise product followed by a sum reduction. The second term is a regularization of the entropy of the assignments, i.e., it controls the sharpness of the clustering. $\mathcal{U}(\tmarg, \cmarg)$ is the transportation polytope, i.e., the set of valid assignments defined as:
\begin{equation}
    \label{eq:transport_polytope}
    \mathcal{U}(\tmarg, \cmarg) = \{ \Q \in \mathbb{R}^{2N \times K}_{+} \:|\: \Q \mathbf{1}_{K} = \tmarg, \Q^{\top} \mathbf{1}_{2N} = \cmarg \}
\end{equation}
%
Additionally, the transportation cost $d_{\text{c}}$ measures the cost of assigning the tokens to the different centroids and can therefore be interpreted as the quality of the clustering, i.e., the ability to find a representative centroid for each token.
% 
\begin{equation}
\label{eq:transportationcost}
    d_{\text{c}} = \: <\Q^{*}, \costsem>
\end{equation}
% 
% In principle, we could stop at this stage, but we observed that better results could be obtained by iteratively merging the clusters, hence after obtention $\Q^{*}$, the centroids are first recomputed as:
%with $<\cdot, \cdot >$ denoting the entry-wise product followed by a sum reduction. 
The centroids are updated after each step ($\centroids^{\top} = \dense^{\top} \Q^{*}$),
% 
% \begin{equation}
    % \label{eq:object_update}
    % \centroids^{\top} = \dense^{\top} \Q^{*}
% \end{equation}
% 
and the two centroids, $(i^{*}, j^{*})$, having the highest cosine similarity, are merged:
% 
\begin{equation}
\label{eq:merge}
    \centroids, \Y \leftarrow \texttt{merge}(\centroids, \Y, i^{*}, j^{*})
\end{equation}
%
where \texttt{merge} denotes the merging operator; the merging procedure averages the selected columns of $\Y$ and the corresponding rows of $\centroids$; in both cases, obsolete columns/rows are simply removed from the matrices. Before reiterating through the clustering algorithm, the matrix cost, $\costsem$, and centroid distribution, $\cmarg$, are updated using Eq.~\ref{eq:cost} and Eq.~\ref{eq:cmarg}, respectively. The whole procedure is repeated until only two centroids remain. By comparing the transportation cost $d_{\text{c}}$ incurred at each step (from $K_{\text{start}}$ to 2), one can select \textit{a posteriori} the optimal number of centroids and the corresponding assignment $\Q^{*}$ for each image independently based on the $\Q^{*}$ that minimizes $d_{\text{c}}$. The procedure's final step consists of the row-wise normalization of the assignments and the pruning of clusters (cf. Sec.~\ref{sssec:pruning}).
% 
\subsubsection{Cluster pruning}
\label{sssec:pruning}
% We discard any empty cluster in either of the two views to anticipate cases where some objects are only represented in one image view. More formally, we introduce a binary version, $\mask \in \mathbb{R}^{2N \times K}$, of the assignments $\Q^{*}$, which is considered as a ``hard'' version of the assignments, i.e.:}

An important property of $\mname$ is that it allows to easily discard clusters corresponding to content that is not shared across the two views (e.g., purple cluster corresponding to the helmet in Fig.~\ref{fig:pipeline}). To that end, we first compute the hard version of the assignments (each token is assigned to precisely one centroid):

\begin{equation}
\label{eq:binary_mask}
    \mask_{n,k} = \mathds{1}_{ k = \underset{j}{\text{argmax}}\left\{ \Q^{*}_{nj}\right\}}
\end{equation}
%
The hard assignments are split view-wise to obtain $\mask_{1}$ and $\mask_{2}$, and we introduce the sets $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$, which store indices of the zero columns of $\mask_{1}$, and  $\mask_{2}$, respectively. Therefore, any column of $\Q^{*}_{\{1,2\}}$ and $\mask_{\{1,2 \}}$, whose index is in $\mathcal{S} = \mathcal{S}_{1} \cup \mathcal{S}_{2}$, is filtered out:
%\Thomas{
% Encouraged by the strong results of recent self-distillation approaches \cite{grill2020bootstrap,caron2021emerging},
 % 
\begin{equation}
\label{eq:filter}
    \Q_{\{1,2\}}^{*}, \mask_{\{1,2 \}} \leftarrow \texttt{filter}(\Q_{\{1, 2\}}^{*}, \mask_{\{1,2 \}}, \mathcal{S})
\end{equation}
%
where \texttt{filter} denotes the filtering operator, which drops the indexed columns of the input matrices. 
% In practice, we observed that using masked assignments is crucial for the performance of the method hence instead of using $\Q^{*}_{\{1,2 \}}$, we use the masked version:
% \begin{equation}
    % \Q^{*}_{\{1,2\}} \leftarrow \mask_{\{1,2 \}} \odot \Q^{*}_{\{1,2\}} \\
% \end{equation}

%\Thomas{
 %At this point we have all the required elements for the Sinkhorn-Knopp algorithm to run smoothly. When fed with the matrix cost, $\cost$, and the two marginals, $\tmarg$ and $\cmarg$, it produces the assignment matrix $\Q^{*}$:
%, where the second term is a regularization term on the entropy of the assignments, i.e. it controls the smoothness of the clustering, 
%and $\mathcal{U}(\tmarg, \cmarg)$ is the transportation polytope, i.e., the set of valid assignments:
%Additionally, this algorithm returns the transportation cost:
%it measures the cost assigning the tokens to the different centroids and can therefore be interpreted as the quality of the clustering. 
%In principle, we could stop at this stage, but we observed that better results can be obtained by iteratively merging the clusters, hence after obtention $\Q^{*}$, the centroids are first recomputed:
%}
% It results from that step, an assignment matrix, $\Q_{\text{cat}} \in \mathbb{R}^{2N \times K}$, which is split view-wise to yield $\Q_{1} \in \mathbb{R}^{N \times K}$ and $\Q_{2} \in \mathbb{R}^{N \times K}$. Before computing the final centroids of each view, it must be verified that none of them is only represented in one of the two views. To that end, it is sufficient to discard any centroid corresponding to a zero column of $\mask_{1}$ or $\mask_{2}$ (see Eq.~\ref{eq:binary_mask}). At this point, Eq.~\ref{eq:final_update} is used to compute each view's objects representations and similarity can be enforced between positive pairs of objects. 
% .For the sake of clarity, we initially hypothesize that the number of objects, $K$, is known in advance and that their representation in the latent space is available and given by $\objects \in \mathbb{R}^{K \times d}$. Under these assumptions, the model's predictions, $\PP$, can be computed by exploiting the similarity between the features and the object representations:
% \begin{equation}
%     \label{eq:predictions}
%     \PP = \dense \objects^{\top}
% \end{equation}
% When the ground-truths labels exist as a matrix of one-hot row vectors, $\Y \in \mathbb{R}^{N \times K}$, they can be used to guide the model's learning process via the cross-entropy loss:
% \begin{equation}
%     \label{eq:ce_labels}
%     \min_{\PP} <\frac{1}{N}\Y, - \log(\PP)>
% \end{equation}
% In practice the labels are typically not available and must be inferred. In these circumstances, it is tempting to simply replace the ground-truths labels by a learnable joint distribution over the features and labels, i.e., an assignment matrix, $\Q$, of the same shape and to jointly minimize Eq.~\ref{eq:ce_labels} w.r.t. both $\Q$ and $\PP$. As it is, this approach does not work as it would lead to a degenerate solution, namely assigning the same label to all features. This pitfall can be easily avoided by restraining the set of admissible joint distributions, $\Q$, to those whose marginals, $\tmarg$ and $\cmarg$, are desired distributions:
% \begin{equation}
%     \label{eq:polytope}
%     \mathcal{U}(\tmarg, \cmarg) = \{ \Q \in \mathbb{R}^{N \times K}_{+} \:|\: \Q \mathbf{1}_{K} = \tmarg, \Q^{\top} \mathbf{1}_{N} = \cmarg \}
% \end{equation}
% In the absence of external/prior knowledge, the uniform distribution over $N$ and $K$ is a suitable choice for $\tmarg$ and $\cmarg$, respectively (see Sec.~\ref{ssec:tricks} for other distributions). The labels are 
% the features labels, $y_{1}, ..., y_{N} \in \{1, ..., K\}$, can be conveniently stored as a matrix $\Y \in \mathbb{R}^{N \times K}$ and used to enforce local consistency via a cross In the absence of ground truth targets, the labels must 
% More formally, provided an input image, $\im \in \mathbb{R}^{C \times H \times W}$, the model, $f$, produces the spatial representation, $\dense \in \mathbb{R}^{N \times d}$: we tackle the challenging task of conjointly discovering the number of represented objects, $K$, and assigning to each feature or spatial token a label $y_{1}, ..., y_{N} \in \{1, ..., K\}$. For the sake of clarity, we initially hypothesize that the number of objects is known in advance and that their representation in the latent space is given by $\objects \in \mathbb{R}^{K \times d}$. Similarly, let's assume that the distributions of the semantic/relevance of the tokens and objects exists and are conveniently stored as probability vectors, $\tmarg$ and $\cmarg$, respectively. Under these assumptions, the task of matching tokens to objects based on their similarity can be elegantly formulated as an optimal transport problem. In that setting, the objective is to find the token-objects assignment, $\Q^{*} \in \mathbb{R}^{N \times K}$, that minimizes the transportation cost: 
% \begin{equation}
%     \label{eq:assignments}
%     \Q ^{*} = \underset{\Q \in \mathcal{U}(\tmarg, \cmarg)}{\arg\max} <\Q, \cost> %- \frac{1}{\lambda} h(Q)
% \end{equation}
% , where $\cost = \dense \objects ^{\top}$ expresses the cost of matching the $N$ tokens to the $K$ objects and $\mathcal{U}(\tmarg, \cmarg)$ is the transportation polytope, i.e., the set of admissible assignments.
% \begin{equation}
%     \label{eq:transport_polytope}
%     \mathcal{U}(\tmarg, \cmarg) = \{ \Q \in \mathbb{R}^{N \times K}_{+} \:|\: \Q \mathbf{1}_{K} = \tmarg, \Q^{\top} \mathbf{1}_{N} = \cmarg \}
% \end{equation}
% So far, we have assumed that both the number of objects and their representations were known in advance, which in principle is not the case. We tackle both difficulties by adopting an iterative approach. More precisely, a fix number, $K_{\text{start}}$, of objects/centroids are initialized with the representations of $K_{\text{start}}$ carefully selected tokens from $\dense$. After computation of the optimal assignments, $\Q^{*}$ (see Eq.~\ref{eq:assignments}), the object representations are updated with:
% \begin{equation}
%     \label{eq:object_update}
%     \objects^{\top} = \dense^{\top} \Q^{*}
% \end{equation}
% The two most similar centroids are then merged and the optimal assignments recomputed. This procedure is repeated until there only remain two centroids (background/foreground) at which point we select \textit{a posteriori} the ideal number of object $K$ as the one which minimizes:
% \begin{equation}
%     c = ...
% \end{equation}
% % cost = (((1 - hard_assignments) * assignments).sum(dim=1) / (hard_assignments * assignments).sum(dim=1)).mean(dim=-1) 
% The initialization of the centroids is a critical part of the pipeline, as it ultimately determines the quality of the clustering. Ideally, the initial set of centroids should reflect the semantic contribution of the different instances that compose the image. For that purpose, we sample $K_{\text{start}}$ tokens from, $\tmarg$, namely the $\cls$ token attention map which has been shown to reflect the distribution of the semantic content in images \cite{caron2021emerging}. Similarly, the object/centroid -level semantic distribution, $\cmarg$, is obtained via:
% \begin{equation}
%     \label{eq:cmarg}
%     \cmarg^{\top}  = \tmarg^{\top} \Q
% \end{equation}
% For reason that'll become clear in the following section, we compute the final object/centroids with masked assignments:
% \begin{equation}
%     \label{eq:final_update}
%     \objects^{\top} = \dense^{\top} \left( \mask \odot \Q^{*} \right)
% \end{equation}
% , where $\odot$ denotes the entry-wise dot product and $\mask$ is a binary mask of hard token assignments, i.e.:
% \begin{equation}
% \label{eq:binary_mask}
%     \mask_{n,k} = \mathds{1}_{\left\{ k = \text{argmax}\left\{q_{n}^{\top}\right\}\right\}}
% \end{equation}
\subsubsection{Positional cues}
\label{sssec:position_cues}
In Sec.~\ref{ssec:dense_self_distillation}, we mention the need for an image-level self-distillation loss to break the interdependence between the features' quality and the correctness of the enforced dense loss.  Along the same line, positional cues can be leveraged to guide the clustering operation, such that spatially coherent clusters can be obtained even when the features do not fully capture the semantics of the underlying data. Indeed, it appears natural to bias the clustering in favor of matching together tokens resulting from the same region \textbf{in the original image}. To that end, a positional constraint is added to the matrix transportation cost $\costsem$, which is modified to incorporate this desired property.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/posenc.png}
    \caption{
    The positional cues use the top-left corner of the \textbf{original image} as a reference point, such that the position coordinates of each view lie in the same space and can be used to guide the clustering algorithm.
    }
    \label{fig:positional_cues}
\end{figure}
%To that end, a positional constraint is added to the matrix transportation cost, $\costsem$, is modified in the sort of incorporating this desired property.
\par

We start by observing that the augmented views, $\aug_{\{1,2\}}$, result from the composition and use of a set of geometric and photometric transformations on the original image $\im$. We propose to extract the coordinates of the patches in each view with respect to the original image referential (cf. Fig.~\ref{fig:positional_cues}). More precisely, we generate the tensors, $\pos_{\{1,2\}} \in \mathbb{R}^{N\times 2}$, which store the 2D coordinates of each patch in the two views. 
% The positional transportation cost, $\costpos$, is computed reciprocally to the semantic cost, $\costsem$, but using the position coordinates, $\pos$, instead of the dense representations $\dense$. 
The coordinates are first concatenated along the patch/token axis to obtain $\pos_{\text{cat}} \in \mathbb{R}^{2N \times 2}$, and the positions of the centroids, $\pos_{\text{cen}} \in \mathbb{R}^{K_{\text{start}} \times 2}$, are computed as in Eq. \ref{eq:cent_init} ($\pos_{\text{cen}} = \Y^{\top} \pos_{\text{cat}}$). The entries of the positional transportation cost $\costpos \in \mathbb{R}^{2N \times K_{\text{start}}}$ are computed as follows:
\begin{equation}
    \costpos_{ij} = \frac{1}{S}|| \mathbf{e}^{\text{(cat)}}_{i} -  \mathbf{e}^{\text{(cen)}}_{j}||_{2}
\end{equation}
%
where $S$ is a normalization constant that ensures that the entries of the positional transportation cost are in $[0, 1]$. After incorporation of the positional bias, the total matrix transportation cost is defined as follows:
\begin{equation}
\costtot = \costsem + \lambda_{\text{pos}} \costpos
\end{equation}
The scalar weight  $\lambda_{\text{pos}}$ regulates the importance of the positional cues. As detailed in Sec.~\ref{ssec:where_objects?}, the clustering algorithm relies on the iterative merging of the centroids; hence their respective position must also be merged reciprocally, i.e., by averaging (cf. Eq.~\ref{eq:merge}).




\subsubsection{Multiple clustering assignments using MSA}
\label{sssec:multiple_heads}
In this section, we detail a mechanism to obtain multiple complementary clustering assignments $\Q^{*}$ per image. This mechanism relies on the multi-head self-attention (MSA) module inherent to the transformer architecture. \par
Arguably, the main ingredient behind the transformer architecture's success is the self-attention module. Indeed, \textit{i)} it allows capturing of long-range inter-dependencies between the patches that constitute the image, and \textit{ii)} it endows the local representations with global or contextual information. Formally, the multi-head attention operation of the $l^{th}$ transformer block is expressed as:
\begin{align}
\label{eq:multi_head}
\begin{split}
\texttt{multi}&\texttt{-head} \left(\dense^{(l-1)} \right) \\
 &= \texttt{concat}\left(\text{head}_{1}, ...,\text{head}_{n_{h}}\right)\mathbf{W}^{o}
\end{split}
\end{align}
where $\mathbf{W}^{o} \in \mathbb{R}^{d \times d}$ is a learnable projection weight, and $\text{head}_{i}$, for $i=1,\cdots ,n_{h}$, denotes a single attention head:
\begin{align}
\label{eq:single_head} 
\text{head}_{i} &= \texttt{attention}\left(\dense^{(l-1)}, \mathbf{W}^{\{q,k,v\}}_{i} \right) \\
 &= \texttt{softmax}\left( \frac{\dense^{(l-1)} \mathbf{W}_{i}^{q} \left(\dense^{(l-1)} \mathbf{W}_{i}^{k}\right)^{\top}}{\sqrt{D}} \right) \dense^{(l-1)} \mathbf{W}_{i}^{v} \nonumber
\end{align}
%
where $\mathbf{W}_{i}^{\{q,k,v \}} \in \mathbb{R}^{d \times d/n_{h}}$ denotes head-specific learnable projection weights\footnote{The layer index, which starts from 0, is omitted.}. Following the same reasoning that motivates the use of multiple heads, i.e., the inter-patches relationship is not unique, we use as many clustering assignments as there are heads in the MSA module. In practice, it turns out to be as simple as independently feeding each attention head's dense representation to the clustering algorithm:
\begin{equation}
    \Q^{i} = \mathcal{C}\left(\dense^{(B-2)} \mathbf{W}_{i}^{k} \right)
\end{equation}
where $B$ is the number of transformer blocks in the model. Note that only one of the \texttt{keys/queries/values} representation is used (here exemplified with the \texttt{keys}). Consequently, the final assignment matrix $\Q^{*}$ results from the concatenation of the head-wise assignments $\Q^{i}$
% centroids concatenation of each head's centroids:
% \begin{align}
% \label{eq:multi_clusterint}
% \begin{split}
%     \Q^{*} &= \underset{i}{\texttt{concat}}\left( \Q^{i} \right) \\
%     % \centroids^{\top} &= \underset{i}{\texttt{concat}}\left( \left(\dense_{k}^{i}\right)^{\top} \Q^{i} \right) \\
% \end{split}
% \end{align}
%
% where \texttt{concat} denotes the concatenation operator, which is performed 
along the centroid dimension. Up to pruning (Sec.~\ref{sssec:pruning}), the effective number of centroids is $n_{h}$ times higher. Even though the clusters overlap, we do not enforce contradictory objectives as \textit{i)} the consistency is enforced pair-wise (from one centroid in the first view to the corresponding one in the second view) and \textit{ii)} in the framework of self-distillation there are no negative pairs.
% This multi-clustering approach is well aligned with the choice of a self-distillation method, which doesn't rely on negatives and hence never enforces contradictory objectives.
%ok}