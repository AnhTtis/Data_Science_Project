\section{Conclusion}
\label{sec:conclusion}
%\Thomas{
We introduced $\mname$; a novel SSL pre-training method for dense downstream tasks. $\mname$ does not resort to using hand-crafted priors \underline{and} the online clustering algorithm generates pseudo labels for both views in a single and united step. As such, the generated segmentation masks are more coherent and avoid encouraging similarity between objects not univocally represented in both views. $\mname$ is thoroughly evaluated on various  downstream tasks and datasets. In spite of being pre-trained on a medium size scene-centric dataset, the proposed learning paradigm is competitive or outperforms existing methods using ImageNet.
%}

\noindent
{\bf Limitation.}
\label{par:limitations}
%\Thomas{
As is the case with most dense SSL methods, $\mname$ is only implemented and tested with a single model.
%}