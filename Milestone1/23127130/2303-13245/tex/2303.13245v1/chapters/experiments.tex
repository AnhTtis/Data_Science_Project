\section{Experiments}
\label{sec:experiments}

\subsection{Implementations details}
\label{ssec:implementation_details}
\noindent
{\bf Pre-training datasets.}
\label{par:pre_training}
Our models are pre-trained on two uncurated and scene-centric datasets, namely COCO (\texttt{train2017}, $\sim$118k images) and COCO+ (\texttt{unlabeled2017} + \texttt{train2017}, $\sim$241k images). We further explore the possibility of using $\mname$ in an object-centric scenario and therefore adopt ImageNet \cite{5206848} as a pre-training dataset ($\sim10\times$ more images and $\sim4\times$ fewer objects/image).
% This choice of datasets contrasts the \textit{de facto} object-centric SSL pre-training dataset, ImageNet \cite{5206848}, which contains $\sim10\times$ more images and $\sim4\times$ fewer objects/image, hence $\mname$ is placed in a particularly challenging setup. 

\noindent
{\bf Network architecture.}
\label{par:network}
We use a ViT-small (ViT-S/16) as the backbone $f$. This choice is in line with its adoption in concurrent methods and for its comparability \cite{caron2021emerging,ziegler2022self,yun2022patch} with the $\resnet$50, which is the backbone of the remaining baselines. The architecture of the projection heads is identical to that of \cite{caron2021emerging}. Notably, the image-level and centroids-level heads, $\overbar{\head}$ and $\head$, share their weights except for the last layer, which has output dimensions, $\overbar{L}=65,536$ and $L=8,192$, respectively.

\noindent
{\bf Optimization.}
\label{par:optimization}
 $\mname$ is trained for 300 epochs on COCO and COCO+ under an identical optimization scheme. A batch size of 256, distributed over 2 Tesla V100 GPUs is used. The pre-training on ImageNet uses a batch size of 1024, distributed over 4 AMD MI250X GPUs. The remaining optimization setting is identical to that of $\dino$ \cite{caron2021emerging}. 

\noindent
{\bf Hyperparameters.}
% The hyperparameters are common to all experiments independently of the pre-training dataset (COCO/COCO+).
The same weight is given to the dense and global loss, i.e., $\alpha=1$. We use $\lambda=20$ for the regularization term of the transportation objective. The dense and global projection heads use the same temperature parameters, namely $\overbar{\tau}_{s} = \tau_{s} = 0.1$ and $\overbar{\tau}_{t} = \tau_{t} = 0.07$ (see Eqs.~\ref{eq:projection} \&~\ref{eq:image_proj}). Generally, any hyper-parameter common to $\dino$ uses its recommended value. The results of section~\ref{ssec:segmentation_results} which use COCO or COCO+ as pre-training datasets are obtained with $\lambda_{\text{pos}}=4$, $K_{\text{start}}=12$ and the $\texttt{values}$ tokens as parameters of the clustering algorithm. For ImageNet, we only report results with $\lambda_{\text{pos}}=3$, $K_{\text{start}}=12$ and the $\texttt{keys}$ tokens. These values correspond to the default setting of the grid search performed on COCO (see~\cref{ssec:ablation}).

\subsection{Evaluation protocols}
\label{ssec:evaluation_protocols}
We opt for dense evaluation downstream tasks, which require as little manual intervention as possible, such that the reported results truly reflect the quality of the features. Details of the implementations and datasets are available in~\Cref{sec:app_evaluation_protocols}.
%\Thomas{
%We opt for dense evaluation downstream tasks which requires as little manual intervention as possible, such that the reported results truly reflect the quality of the features.
% As a consequence, we do not perform any fine-tuning or training of large prediction heads such as Mask-RCNN \cite{he2017mask}, which can yield misleading results. Indeed, \cite{li2021benchmarking} reports better results when training Mask-RCNN on top of a randomly initialized ViT, than on top of one pre-trained on ImageNet with MoCo V3 \cite{chen2021empirical} pre-trained one. 
%Details of the implementations and datasets are available in \supp.
%}

\noindent
{\bf Transfer learning via linear segmentation.}
\label{par:linear_segmentation}
The linear separability of the learned spatial features is evaluated by training a linear layer on top of the frozen features of the pre-trained encoder. The linear layer implements a mapping from the embedding space to the label space and is trained to minimize the cross-entropy loss. We report the mean Intersection over Union (mIoU) of the resulting segmentation maps on four different datasets, namely, PVOC12 \cite{pascal-voc-2012}, COCO-Things, COCO-Stuff \cite{lin2014microsoft} and ADE20K \cite{zhou2017scene}.

% \Thomas{
% ``The first way we evaluate the quality of the distilled segmentation features is
% through transfer learning effectiveness. As in \cite{van2021unsupervised,cho2021picie} we train a linear projection from segmentation features to class labels using the cross-entropy loss.'' Sentence from STEGO \cite{hamilton2022unsupervised}.
%  ``We evaluate STEGO on the 27 mid-level classes of the CocoStuff class hierarchy and on the 27 classes of Cityscapes. Like prior art, we first resize images to 320 pixels along the minor axis followed by a (320 × 320) center crops of each validation image. We use mean intersection over union (mIoU) and Accuracy for evaluation metric'' sentence from \cite{hamilton2022unsupervised} (STEGO). ``Original setting is from \cite{ji2019invariant} (IIC).
%  ``For linear classifier (LC), we fine-tune a 1x1 convolutional layer on top of the frozen spatial token'' sentence from \cite{ziegler2022self} (Leopart). Report results on VOC and COCO(-Stuff/-Things)
%  ``For overclustering, we run K-Means on all spatial tokens of a given dataset. We then group cluster to ground-truth classes by greedily matching by pixel-wise precision and run Hungarian matching on the merged cluster maps to make our evaluation metric permutation-invarian'' sentence from \cite{ziegler2022self}.
% }
\noindent
{\bf Transfer learning via unsupervised segmentation.}
\label{par:kmeans_segmentation}
We evaluate the ability of the methods to produce spatial features that can be grouped into coherent clusters. We perform  K-Means clustering on the spatial features of every image in a given dataset with as many centroids as there are classes in the dataset. Subsequently, a label is assigned to each cluster via Hungarian matching \cite{kuhn1955hungarian}. We report the mean Intersection over Union (mIoU) of the resulting segmentation maps on three different datasets, namely PVOC12 \cite{pascal-voc-2012}, COCO-Things, and COCO-Stuff \cite{lin2014microsoft}.
% \Thomas{
% ``Unlike the linear probe, the clustering step does not have access to ground truth supervised labels. As in prior art, we use a Hungarian matching algorithm to align our unlabeled clusters and the ground truth labels for evaluation and visualization purposes. This measures how consistent the predicted semantic segments are with the ground truth labels and is invariant to permutations of the predicted class labels.'' Sentence from STEGO \cite{hamilton2022unsupervised}
% }

% \noindent
% {\bf Unsupervised object discovery.}
% \label{par:object_discovery}
% A desirable property of a pre-trained model is its ability to locate the image's important region(s), i.e., the object(s) it contains. This property is evaluated by using two SOTA unsupervised object discovery methods, namely, LOST \cite{simeoni2021localizing} and TokenCut \cite{wang2022self}, which leverage the features' similarity to identify the location (returned as a bounding box) of the object in a given image. The Correct Localization (CorLoc) metric, i.e., the percentage of predicted bounding boxes having an Intersection over Union (IoU) superior to 50\%, is reported. Three standard datasets (PVOC07 \cite{pascal-voc-2007}, PVOC12 \cite{pascal-voc-2012} and COCO20k \cite{lin2014microsoft}) are used for this tasks \Thomas{(results in~\Cref{sec:app_od_results})}.


% \Thomas{
% The spatial tokens of the pre-trained and frozen model are fed to object discovery methods, i.e. LOST \cite{simeoni2021localizing}, TokenCut \cite{wang2022self} and DSM \cite{melas2022deep}. The commonly adopted CorLoc metric is reported on standard single object discovery datasets: VOC07, VOC12 and COCO20k. ``The Correct Localization (CorLoc) metric, i.e., the percentage of correct boxes, where a predicted box is considered correct if it has an intersection over union (IoU) score superior to 0.5 with one of the labeled object bounding boxes.'' Sentence from LOST \cite{simeoni2021localizing}.
% }
\noindent
{\bf Semi-supervised video object segmentation.}
\label{par:video_segmentation}
We assess our method's generalizability for semi-supervised video object segmentation on the DAVIS'17 benchmark. The purpose of this experiment is to evaluate the spatiotemporal consistency of the learned features. First, the features of each frame in a given video are independently obtained; secondly, a nearest-neighbor approach is used to propagate (from one frame to the next) the ground-truth labels of the first frame  (see results in~\cref{sec:app_video_segmentation}).
% \Thomas{
% ``As a further test of scene understanding, we assess whether learned representations can continue to recognize parts of an object as they evolve over time. Video object segmentation, specifically in its semi-supervised setting, captures this ability, which we evaluate on the DAVIS’17 benchmark \cite{perazzi2016benchmark}. Having evaluated a learned representation on a video independently across frames, we segment these features with nearest neighbor matching from frame to frame, given a segmentation of the first frame. In this way, the segmentation is propagated according to the similarity of the representation across space and time.'' Sentence from Odin \cite{henaff2022object}.
% }

\subsection{Segmentation results}
\label{ssec:segmentation_results}
In~\Cref{table:linear_segmentation,table:linear_segmentation_ade20k}, we report mIoU results on the linear segmentation task. When pre-trained on COCO, $\mname$ exceeds concurrent methods using COCO(+) as pre-training datasets, even though $\orl$ and $\byol$ use a longer training protocol (800 epochs). With a pre-training on COCO+, $\mname$ outperforms all other methods, except $\cpsq$ \cite{wang2022cp2}, on every evaluation dataset, despite their usage of ImageNet and their finetuning on one of the target datasets (PVOC12). Noteworthy that $\cpsq$ is initialized with a pre-trained model and cannot be trained from scratch. Pre-training on a larger and object-centric dataset appears to be highly beneficial in that setting.
 \par
\input{tables/linear_segmentation.tex}
In Table~\ref{table:kmeans_segmentation}, the results for the unsupervised segmentation task are reported. As for linear segmentation experiments, $\mname$ is already competitive with only a pre-training on the COCO dataset and surpasses all competing methods except $\densecl$ \cite{wang2021dense}. 
\input{tables/linear_segmentation_ade20k}
%\Thomas{
The largest improvements are observed on the COCO-Stuff dataset; this is unsurprising as this dataset contains semantic labels such as $\texttt{water}$, $\texttt{ground}$, or $\texttt{sky}$, which correspond to regions that are typically overlooked by other methods, but on which $\mname$ puts a significant emphasis. The model pre-trained on ImageNet appears to perform poorly on that task, which is surprising considering the excellent results depicted in~\Cref{table:linear_segmentation} on the exact same datasets. This might hint that using evaluations that are not adjustable to each baseline is sub-optimal. Overall we observe that producing features that can be clustered class-wise without labels remains an open challenge.
% Leopart \cite{ziegler2022self} show that better results can be obtained by using a number of clusters higher than that of classes ($K=500$). Nonetheless, we do not follow that direction as it does not 
%} 
\input{tables/kmeans_segmentation.tex}

% \input{tables/object_discovery.tex}

\subsection{Ablation study}
\label{ssec:ablation}
We scrutinize the roles played by different components of $\mname$. Unless otherwise stated, $\lambda_{\text{pos}}=3$, $K_{\text{start}}=12$ and the $\texttt{keys}$ tokens are used for the ablations. Rows corresponding to the chosen setting are \hl{highlighted}.

\vspace{1ex} 
\noindent
\textbf{Weight of the positional cues $\lambda_{\text{pos}}$.} The first element that is ablated is the contribution of the positional bias to the overall performance. In Table~\ref{table:lambdapos_segmentation}, we observe that an increased positional bias leads to improved performance on the unsupervised segmentation task, but a slightly worsened one on the linear segmentation task.
%\paragraph{Weight of the Positional Cues $\lambda_{\text{pos}}$.}
%\label{par:ablation_position}
\input{tables/lambdapos_kmeans_segmentation.tex}
\input{tables/kstart_kmeans_segmentation.tex}

% \vspace{1ex} 
\noindent
\textbf{Number of initial centroids $K_{\text{start}}$.} As can be seen in Table~\ref{table:kstart_segmentation}, the linear segmentation scores monotonically increase with the number of initial centroids, whereas for unsupervised segmentation, there seems to be a middle ground.

\vspace{1ex} 
\noindent
\textbf{Type of clustering tokens.}
\label{par:ablation_tokens} 
Table~\ref{table:keys_segmentation} shows that the choice of spatial tokens plays a determinant role in the downstream results and that the multi-clustering approach (Sec.~\ref{sssec:multiple_heads}) can yield a significant boost in performance compared to the case when the clustering uses last spatial tokens $\dense^{(B-1)}$ (\texttt{last}).
\input{tables/keys_kmeans_segmentation.tex}


% \subsection{Baselines}
% \label{ssec:baselines}
% \Thomas{
% For a fair comparison of $\mname$, we report the results of concurrent SSL methods relying on identical backbones, namely $\vit$s. $\dino$ apart, we only compare with approaches which guide the representation learning process at a localized level and which report their performances on dense downstream tasks.
% \begin{itemize}
%     \item $\dino$ \cite{caron2021emerging}: vanilla version of our work.
%     \item $\mae$ \cite{he2021masked}: dense loss + dense downstream tasks reported.
%     \item $\ibot$ \cite{zhou2021ibot}: dense loss + dense downstream tasks reported.
%     \item ($\beit$ \cite{bao2021beit}: dense loss + dense downstream tasks reported). Not exactly the same backbone?
% \end{itemize}
% We should also compare with methods intended for dense downstream tasks, e.g. DetCon \cite{henaff2021efficient}, MaskContrast \cite{van2021unsupervised}, but with $\vit$s. We do not compare with Leopart \cite{ziegler2022self}, which cannot be trained from scract, nor with Odin \cite{henaff2022object} whose implementation is not publicly available.}

% \begin{itemize}
%     \item LOST and TokenCut on  VOC12, VOC07, (COCO20k).
%     \item Multi-class linear eval on COCO.
%     \item IF POSSIBLE: Mask RCNN finetuning on COCO
%     \begin{itemize}
%         \item read this paper: https://arxiv.org/pdf/2204.02964.pdf
%         \item code here: https://github.com/hustvl/MIMDet
%     \end{itemize} 
% \end{itemize}



% \input{tables/gs_studenttemp_pretrained}