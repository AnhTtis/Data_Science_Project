\appendix
% Table and figure numbering for figures
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
%\section{Overview}
%\label{sec:app_overview}
%\Thomas{
%In this supplementary draft, we provide additional details and materials. The remaining of this supplementary draft is organized as follows. 
%}
\section*{Appendix}
This Appendix provides additional details and qualitative results organized as follows. In~\Cref{sec:app_datasets}, we thoroughly describe the datasets used in our paper. Additional details about concurrent methods and comparison are discussed in~\Cref{sec:app_implementation_details}. A detailed description of the evaluation protocols is presented in~\Cref{sec:app_evaluation_protocols}. Additional results on semi-supervised video segmentation are reported in~\Cref{sec:app_video_segmentation}. In~\Cref{sec:app_overhead}, we briefly discuss the efficiency and computational overhead of $\mname$. Finally, we provide examples of the clusters found on the combined views of complex scene images with the proposed online clustering algorithm in~\Cref{sec:app_clustering_results}.


\noindent
\section{Datasets}
\label{sec:app_datasets}

% \noindent
% {\bf ImageNet}
% \label{par:imagenet}
% \Thomas{
% An object-centric dataset, spanning 1000 object classes \cite{5206848}. The train/validation/test splits encompass $\sim$1.3M/50k/100k labelled images, respectively.
% % In average, the training images contain $1.7$ objects \cite{wen2022self}.
% }

\noindent
{\bf COCO.}
\label{par:coco}
The COCO (Microsoft Common Objects in Context) dataset \cite{lin2014microsoft} consists of scene-centric images spanning 91 stuff categories and 80 objects/things categories. The \texttt{train2017}, \texttt{val2017} and \texttt{test2017} splits incorporate approximately 118k, 5k and 41k images, respectively. Additionally, a set of $\sim$123k unlabeled images, \texttt{unlabeled2017}, can be used in conjunction with the \texttt{train2017} split to obtain the so-called COCO+ training set.


% \noindent
% {\bf COCO20k.}
% \label{par:coco20k}
% In the absence of a large-scale dataset tailored for object discovery, \cite{vo2020toward} sampled a random subset of 20k images from COCO \texttt{train2017} and filtered out all \textit{crowd} bounding boxes. Similarly, images that only encompass \textit{crowd} bounding boxes are discarded from the dataset, yielding a total of 19817 images and 143951 objects.
%It results from this pre-processing step 

\noindent
{\bf COCO-Things.}
\label{par:coco_things}
The COCO-Things dataset follows the implementation of \cite{ziegler2022self}. This dataset is based on COCO images and the panoptic labels of \cite{kirillov2019panoptic}. More precisely, the instance-level labels are merged, and so are the 80 ``things'' categories, yielding the following 12 super-categories: \texttt{electronic}, \texttt{kitchen}, \texttt{appliance}, \texttt{sports}, \texttt{vehicle}, \texttt{animal}, \texttt{food}, \texttt{furniture}, \texttt{person}, \texttt{accessory}, \texttt{indoor}, and \texttt{outdoor}. As the underlying images are the same as in the COCO dataset, so are the training/validation/test splits.

 
\noindent
{\bf COCO-Stuff.}
\label{par:coco_stuff}
The COCO-Stuff dataset follows the implementation of \cite{ziegler2022self}. The stuff annotations are those of \cite{caesar2018coco}. As for COCO-Things, the 91 ``stuff'' categories are merged into 15 super-categories: \texttt{water}, \texttt{structural}, \texttt{ceiling}, \texttt{sky}, \texttt{building}, \texttt{furniture-stuff}, \texttt{solid}, \texttt{wall}, \texttt{raw-material}, \texttt{plant}, \texttt{textile}, \texttt{floor}, \texttt{food-stuff}, \texttt{ground} and \texttt{window}. This dataset follows the same training/validation/test splits as in the COCO dataset.

% \noindent
% {\bf PVOC07.}
% \label{par:pvoc07}
% The PASCAL VOC07 (PVOC07) dataset \cite{pascal-voc-2007} is a scene-centric dataset. 
% It is composed of 5011 images spanning 20 object classes (+1 background class): \texttt{person}, \texttt{bird}, \texttt{cat}, \texttt{cow}, \texttt{dog}, \texttt{horse}, \texttt{sheep}, \texttt{aeroplane}, \texttt{bicycle}, \texttt{boat}, \texttt{bus}, \texttt{car}, \texttt{motorbike}, \texttt{train}, \texttt{bottle}, \texttt{chair}, \texttt{dining table}, \texttt{potted plant}, \texttt{sofa}, \texttt{tv/monitor} and \texttt{background}.


\noindent
{\bf PVOC12.}
\label{par:pvoc12}
The PASCAL VOC12 (PVOC12) dataset \cite{pascal-voc-2012} is a scene-centric dataset. The \texttt{trainaug} split relies on the extra annotations of \cite{hariharan2011semantic} such that 10582 images with pixel-level labels can be used for the training phase as opposed to the 1464 segmentation masks initially available. The validation set encompasses 1449 finely annotated images. The dataset spans 20 object classes (+1 background class): \texttt{person}, \texttt{bird}, \texttt{cat}, \texttt{cow}, \texttt{dog}, \texttt{horse}, \texttt{sheep}, \texttt{aeroplane}, \texttt{bicycle}, \texttt{boat}, \texttt{bus}, \texttt{car}, \texttt{motorbike}, \texttt{train}, \texttt{bottle}, \texttt{chair}, \texttt{dining table}, \texttt{potted plant}, \texttt{sofa}, \texttt{tv/monitor} and \texttt{background}.


\noindent
{\bf ADE20K.}
\label{par:ade20k}
The ADE20K dataset \cite{zhou2017scene} is a scene-centric dataset encompassing more than 20K scene-centric images and pixel-level annotations. The labels span 150 semantic categories, including ``stuff'' categories, \eg \texttt{sky}, \texttt{road}, or \texttt{grass}, and ``thing'' categories, \eg \texttt{person}, \texttt{car}, \etc.


\section{Implementation details}
\label{sec:app_implementation_details}
\noindent
\subsection{Comparison with competing methods}
\label{par:app_baselelines}
To compare $\mname$ on an equal footing with concurrent methods, we evaluate all baselines using our evaluation pipeline, except for the evaluation of $\resnet$50 on the semi-supervised video segmentation task which are taken as is from \cite{yun2022patch}. With our implementation, the results were worse than the ones reported in \cite{yun2022patch} or \cite{henaff2022object}; hence we report their results. Furthermore, for $\byol$ \cite{grill2020bootstrap}\footnote{The checkpoint for $\byol$ is provided and trained by the authors of $\orl$ \cite{xie2021unsupervised}.}, $\orl$ \cite{xie2021unsupervised}, $\densecl$ \cite{wang2021dense}, $\soco$ \cite{wei2021aligning}, $\resim$ \cite{xiao2021region}, $\pixpro$ \cite{xie2021propagate}, $\vicregl$ \cite{bardes2022vicregl} and $\cpsq$ \cite{wang2022cp2}, we use publicly available model checkpoints. The only two exceptions are $\mae$ \cite{he2021masked} and $\dino$ \cite{caron2021emerging} methods. Indeed, no public model checkpoint exists for $\vit$-S/16 pre-trained with $\mae$. Since our implementation builds upon $\dino$, it is important to have $\mname$ and $\dino$ models trained in a similar setting for comparison purposes. 

\noindent
{\bf MAE.}
\label{par:app_mae}
The $\vit$-S/16 is pre-trained under $\mae$ framework on the COCO dataset with the following parameters:
\begin{itemize}
\setlength\itemsep{-0.5em}
    \item \texttt{mask\_ratio}: 0.75
    \item \texttt{weight\_decay}: 0.05
    \item \texttt{base\_lr}: 0.00015
    \item \texttt{min\_lr}: 0.0
    \item \texttt{warmup\_epochs}: 40
    \item \texttt{batch\_size}: 256
    \item \texttt{epochs}: 300
\end{itemize}
We use the following decoder architecture: 
\begin{itemize}
\setlength\itemsep{-0.5em}
    \item \texttt{decoder\_embed\_dim}: 512
    \item \texttt{decoder\_depth}: 8
    \item \texttt{decoder\_num\_heads}: 16
\end{itemize}


\noindent
{\bf DINO.}
\label{par:app_dino}
The $\vit$-S/16 is pre-trained under $\dino$ framework on the COCO dataset with the following parameters\footnote{$\mname$ uses the same setting.}:
\begin{itemize}
\setlength\itemsep{-0.5em}
    \item \texttt{out\_dim}: 65536
    \item \texttt{norm\_last\_layer}: false
    \item \texttt{warmup\_teacher\_temp}: 0.04
    \item \texttt{teacher\_temp}: 0.07
    \item \texttt{warmup\_teacher\_temp\_epochs}: 30
    \item \texttt{use\_fp16}: true
    \item \texttt{weight\_decay}: 0.04
    \item \texttt{weight\_decay\_end}: 0.4
    \item \texttt{clip\_grad}: 0
    \item \texttt{batch\_size}: 256
    \item \texttt{epochs}: 300
    \item \texttt{freeze\_last\_layer}: 1
    \item \texttt{lr}: 0.0005
    \item \texttt{warmup\_epochs}: 10
    \item \texttt{min\_lr}: 1e-05
    \item \texttt{global\_crops\_scale}: [0.25, 1.0]
    \item \texttt{local\_crops\_number}: 0
    \item \texttt{optimizer}: adamw
    \item \texttt{momentum\_teacher}: 0.996
    \item \texttt{use\_bn\_in\_head}: false
    \item \texttt{drop\_path\_rate}: 0.1
\end{itemize}

\section{Evaluation protocols}
\label{sec:app_evaluation_protocols}
For all evaluation protocols and models, the evaluation operates on the frozen features of the backbone. The projection heads, if any, are simply discarded. The output features from \texttt{layer4} of $\resnet$50 are used in all downstream tasks. The resulting features have dimension $d=2048$, whereas the spatial tokens of a $\vit$-S/16 have dimension $d=384$ only.  We concatenate the spatial tokens from the last $n_{b}$ transformer blocks, similar to \cite{caron2021emerging}, to compensate for that difference. 


\noindent
{\bf Transfer learning via linear segmentation.}
\label{par:app_linear_segmentation}
Our implementation is based on that of \cite{van2021unsupervised,ziegler2022self}. The input images are re-scaled to $448\times448$ pixels and fed to the frozen model. Following existing works \cite{ziegler2022self}, in the case of $\resnet$50, dilated convolutions are used in the last bottleneck layer such that the resolution of the features is identical for all models. Prior to their processing by the linear layer, the features are up-sampled with bilinear interpolation such that the predictions and the ground-truths masks have the same resolution. Unlike previous works \cite{ziegler2022self,bardes2022vicregl,van2021unsupervised}, we use Adam \cite{kingma2014adam} as an optimizer instead of SGD. Indeed, we observe that this led to significant improvements for \underline{all} baselines, indicating that the reported results were obtained in a sub-optimal regime and hence did not fully reflect the quality of the learned features. We report results on the PVOC12 validation set after training the linear layer on the \texttt{trainaug} split for 45 epochs. For the COCO-Things and COCO-Stuff, the linear layer is first trained for 10 epochs on the training set and subsequently evaluated on the validation set. Regardless of the evaluation dataset and model, we find that a learning rate $\texttt{lr=1e-3}$ works well and that the selected number of epochs is sufficient to reach convergence. Note that contrary to \cite{ziegler2022self}, which randomly samples 10\% of the COCO-Things/-Stuff training images, we use the full set of available images to avoid introducing additional randomness in the results. \par

For the evaluation with ADE20K, we rely on MMSegmentation \cite{mmseg2020} and the \textit{40k iterations schedule}. We set the batch size to 16, and we report for each method the best result after trying learning rates in $\{\texttt{1e-03}, \texttt{8e-04}, \texttt{3e-04}, \texttt{1e-04}, \texttt{8e-05} \}$.

\noindent
{\bf Transfer learning via unsupervised segmentation.}
\label{par:app_kmeans_segmentation}
Our implementation is based on that of \cite{van2021unsupervised,ziegler2022self}. The input images are re-scaled to $448\times448$ pixels and fed to the frozen model. Following existing works \cite{ziegler2022self}, in the case of $\resnet$50, dilated convolutions are used in the last bottleneck layer such that the resolution of the features is identical for all models. Similarly to \cite{ziegler2022self}, the ground-truth segmentation masks and features are down-/up-sampled to have the same resolution ($100\times100$). Consequently, we ran K-Means on the spatial features of all images with as many clusters as there are classes in the dataset. A label is greedily assigned to each cluster with Hungarian matching \cite{kuhn1955hungarian}. We report the mean Intersection over Union (mIoU) score averaged over five seeds. Importantly, \cite{ziegler2022self} observed that better results could be obtained by using a larger number of clusters $K$ than the number of classes in the dataset and hereby having clusters of object-parts instead of objects. Indeed, if this approach provides information on the consistency of the features within object-part clusters, it does not tell anything about the inter-object-parts relationship. For instance, the mIoU scores will reflect the ability of features corresponding to ``car wheels'' to be clustered together and similarly for ``car body'' features, but it won't be impacted by the distance of the two clusters from one another, which is undesirable. We report results on the PVOC12, COCO-Things, and COCO-Stuff validation sets.


% \noindent
% {\bf Unsupervised object discovery.}
% \label{par:app_object_discovery}
% After pre-training, the frozen spatial features of the model are fed to the object discovery evaluation pipeline. As this task is biased in favor of $\vit$s, we do not evaluate $\resnet$50 models. We use the recommended settings, i.e., for the LOST method \cite{simeoni2021localizing}:
% \begin{itemize}
% \setlength\itemsep{-0.5em}
%     \item \texttt{no\_hard}: false
%     \item \texttt{which\_features}: k
%     \item \texttt{k\_patches}: 100
% \end{itemize}
% Similarly, we follow the setting of the TokenCut \cite{wang2022self} as follows:

% \begin{itemize}
% \setlength\itemsep{-0.5em}
%     \item \texttt{no\_hard}: false
%     \item \texttt{which\_features}: k
%     \item \texttt{k\_patches}: 100
%     \item \texttt{tau}: 0.2
%     \item \texttt{eps} 1e-5
%     \item \texttt{no\_binary\_graph} false
% \end{itemize}
% The Correct Localization (CorLoc) metric, i.e., the percentage of predicted bounding boxes having an Intersection over Union (IoU) superior to 50\%, is reported on the COCO20k \cite{vo2020toward} and the \texttt{trainval} sets of PVOC(07/12) \cite{pascal-voc-2007,pascal-voc-2012}.



% \Thomas{
% The spatial tokens of the pre-trained and frozen model are fed to object discovery methods, i.e. LOST \cite{simeoni2021localizing}, TokenCut \cite{wang2022self} and DSM \cite{melas2022deep}. The commonly adopted CorLoc metric is reported on standard single object discovery datasets: VOC07, VOC12 and COCO20k. ``The Correct Localization (CorLoc) metric, i.e., the percentage of correct boxes, where a predicted box is considered correct if it has an intersection over union (IoU) score superior to 0.5 with one of the labeled object bounding boxes.'' Sentence from LOST \cite{simeoni2021localizing}.
% }

\noindent
{\bf Semi-supervised video object segmentation.}
The semi-supervised video object segmentation evaluation follows the implementation of \cite{caron2021emerging,yun2022patch}. We report the mean contour-based accuracy $\mathcal{F}_{m}$, mean region similarity $\mathcal{J}_{m}$ and their average $(\mathcal{J} \& \mathcal{F})_{m}$ on the 30 videos from the validation set of the DAVIS'17 \cite{pont20172017}. The following parameters are used:
\begin{itemize}
\setlength\itemsep{-0.5em}
    \item \texttt{n\_last\_frames}: 7
    \item \texttt{size\_mask\_neighborhood}: 12
    \item \texttt{topk}: 5
\end{itemize}

\section{Semi-supervised video segmentation results}
\label{sec:app_video_segmentation}
\input{tables/video_segmentation.tex}
Good results are obtained on the semi-supervised video segmentation (Table~\ref{table:video_segmentation}), indicating the ability of $\mname$ to produce features consistent through time and space.



% \section{Object discovery results}
% \label{sec:app_od_results}
% \input{tables/object_discovery}
% The results of the unsupervised object discovery task point to the important property of $\mname$ to decipher the region of interest from the background in complex scene images. \Thomas{Pre-training on COCO(+) yields} consistent improvements over existing methods, as can be observed in~Table~\ref{table:object_discovery}. \Thomas{On the contrary, the ImageNet pre-trained model obtains very low scores, which is concerning especially considering the strong performance of the model on linear segmentation and the same images. More effort has to be put in to decipher if it's an issue with the evaluation protocol or the pre-trained weights.}

\section{Computational overhead}
\label{sec:app_overhead}
An important property of $\mname$ is that it generates pseudo-labels/cluster assignments online. Consequently, this step must be efficient. In Table~\ref{table:time_oh}, we verify that the operations inherent to the clustering step amount to less than $10\%$ of the total time of the $\mname$ pipeline.
\input{tables/time_oh.tex}

% \newpage

\section{Qualitative results}
\label{sec:app_clustering_results}

% \noindent
% {\bf Scrutinizing the assignments}
\label{par:app_clusters_viz}

The cluster assignments found by $\mname$'s dedicated online clustering algorithm $\mathcal{C}$ over the combined views are depicted in Fig.~\ref{fig:clusters}. The model used to generate the illustrated assignments is pre-trained on the COCO+ for 300 epochs with $\mname$ and the following meta-parameters: $\lambda_{\text{pos}}=4$, $K_{\text{start}}=12$ and $\texttt{values}$ tokens. During training, we use the same augmentations as in DINO \cite{caron2021emerging}; consequently, we visualize the generated masks based on augmented views in the same manner, such that the results depicted in Fig.~\ref{fig:clusters} truly reflect the consistency enforced by $\mname$.
% apply the same augmentation to generate the self-labeling step robust to those transformations, e.g., photometric transform and obtain the pseudo segmentation labels},
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/clusters.jpg}
    \caption{
    \textbf{Illustration of the clusters found online in the space of the combined views}. Rows correspond to combined views and columns to heads of the $\vit$. Bicubic interpolation is used to up-sample the assignments $\Q^{*}$ to the same resolution as the images.}
    \label{fig:clusters}
\end{figure*}

