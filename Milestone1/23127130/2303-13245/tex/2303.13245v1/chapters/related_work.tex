\section{Related work}
\label{sec:related_work}
\noindent
{\bf Global features.}
\label{par:global_features}
The collateral effect of \cite{chen2020simple}, is that it effectively uniformized the choice of the proxy task for SSL to the extent that \textit{cross-view consistency} is almost exclusively used. The remaining degree of freedom lies in the technique used to avoid the collapse to trivial solutions. The use of negative samples \cite{chen2020simple,hjelm2018learning} effectively and intuitively treats this degeneracy at the cost of using large batches, which can be mitigated by a momentum encoder \cite{he2020momentum}. At the other end of the spectrum, clustering-based approaches \cite{asano2019self,caron2018deep,caron2020unsupervised,caron2021emerging} have shown that enforcing equipartition of the samples over a set of clusters was sufficient to palliate the collapsing issue.


\noindent
{\bf Local features.}
\label{par:local_features}
Local methods aim at completing the image-level objective by encouraging cross-view consistency at a localized level such that the resulting features are well aligned with dense downstream tasks. Broadly speaking, these methods can be categorized by the granularity at which the similarity is enforced. The first category encompasses approaches \cite{wang2021dense,liu2020self,o2020unsupervised,lebailly2022global}, where similarity is encouraged directly at the feature level, i.e., from one feature to the other. The difficulty lies in obtaining valid pairs or groups of features. To that end, various methods \cite{wang2021dense,liu2020self} rely solely on the similarity of the features, whereas the matching criterion of \cite{xie2021propagate,o2020unsupervised} is driven by their distances/positions. 
\cite{lebailly2022global} studies both approaches and \cite{bardes2022vicregl} incorporates both in a single objective.\par

The second category of methods \cite{cho2021picie,wen2022self,henaff2021efficient,henaff2022object,xie2021unsupervised} enforce consistency at a higher level, which first requires finding semantically coherent groups of features. For that purpose, \cite{xie2021unsupervised}, resort to using a pre-trained model and an offline ``correspondences discovery'' stage to find pairs of the region of interest. Along the same line, \cite{henaff2021efficient} proposes to use various heuristics prior to the training phase to generate pseudo-segmentation labels. An online version of this latest algorithm has been introduced, but it requires forwarding an additional global view. 
\par

Alternatively, dense fine-tuning approaches \cite{hamilton2022unsupervised,ziegler2022self,yun2022patch,wang2022cp2} have been proposed. These methods aim to endow models pre-trained under an image-level objective \cite{caron2021emerging} with local consistency properties, but cannot be trained from scratch.
\par
Finally, MAE \cite{he2021masked} relies on a masked autoencoder pipeline and a reconstruction objective to learn dense representations. As MAE does not rely on a cross-view consistency objective, this approach is well-suited for scene-centric datasets and of particular interest to us.

% \Tim{
% Representation learning methods exploit the invariances in the data to apply a self-supervised coherency loss between (subsets of) the inputs. This can be done at different levels: \textit{image-level}, \textit{local-level} or \textit{region-level}.
% }
% \Behzad{
% In the literature, in addition to Image-Level, they categorize into Object-Level representation learning e.g., DetCon and Odin, and Object-centric representation learning which should be relevant to region-level here e.g., DenseCL. I am unsure if local-level is very relevant, and what methods will be categorized there?
% }
% \begin{itemize}
%     \item Picie \cite{cho2021picie} (starts from a pre-trained model on ImageNet).
%     \item STEGO \cite{hamilton2022unsupervised} (cannot be trained from scratch).
%     \item $\cpsq$ \cite{wang2022cp2} (cannot be trained from scratch).
%     \item IIC \cite{ji2019invariant}.
%     \item Leopart \cite{ziegler2022self} (cannot be trained from scratch).
%     \item FreeSOLO \cite{wang2022freesolo} (cannot be trained from scratch).
%     \item $\densecl$ \cite{wang2021dense}.
%     \item $\orl$ \cite{xie2021unsupervised} (starts from a pre-trained model + complicated 3-stages procedures).
%     \item $\mae$ \cite{he2021masked}
%     \item DetCo \cite{xie2021detco}
%     \item SlotCon \cite{wen2022self}
% \end{itemize}
