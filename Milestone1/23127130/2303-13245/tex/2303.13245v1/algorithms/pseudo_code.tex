\begin{python} 
    # x_t: teacher tokens [(2 b), n, d]
    # x_s: student tokens [(2 b), n, d]
    # tau_s: student temperature
    
    # Normalize the tokens representations
    x_t = F.normalize(x_t[:, 1:], dim=-1)
    x_s = F.normalize(x_s[:, 1:], dim=-1)

    # The loss operates in the joint space
    x_t = rearrange(x_t, '(m b) n d -> b (m n) d', m=2)
    x_s = rearrange(x_s, '(m b) n d -> b (m n) d', m=2)

    # Get the centroids' indices
    indices = self.compute_random_indicators(x_t)

    # Compute the teacher's and student's centroids 
    c_t = torch.einsum('b n d, b k n -> b k d', x_t, indices)
    c_s = torch.einsum('b n d, b k n -> b k d', x_s, indices)

    # Compute the teacher's tokens-centroids similarity 
    s_t = torch.einsum('b n d, b k d -> b n k', x_t, c_t)
    s_t = rearrange(s_t, 'b n k -> b n k')

    # The transportation cost is proportional to the inverse of the s
    M = 1. - s_t

    # Compute the marginals
    b, n, k = s_t.shape
    r = (torch.ones([b, n, 1]) / n, device=x_t.device)
    c = (torch.ones([b, 1, k]) / k, device=x_t.device)

    # Compute the pseudo-labels
    q_t, _ = sinkhorn( M=M, r=r, c=c)

    # Normalize the assignments
    normalization = q_t.sum(dim=1, keepdim=True)
    q_t /= normalization

    # Compute the student's predictions
    s_s = torch.einsum('b n d, b k d -> b n k', x_s, c_s)
    s_s = rearrange(s_s, 'b n k -> b n k')
    p_s = F.log_softmax(s_s / tau_s, dim=1)

    # Compute the loss
    loss = - torch.mean(torch.sum(q_t * p_s, dim=1))
\end{python}