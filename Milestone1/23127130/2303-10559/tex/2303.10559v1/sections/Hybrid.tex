\section{Cross-Sensor Model}
\label{sec:hybrid}

Multi-sensor calibration estimates intrinsic and extrinsic parameters of multiple sensors like cameras, LiDARs, and IMUs. This ensures that data from different sensors are synchronized and registered in a common coordinate system, allowing them to be fused together for a more accurate representation of the environment. Accurate multi-sensor calibration is crucial for applications like autonomous driving and robotics, where reliable sensor fusion is necessary for safe and efficient operation.

In this part, we mainly review the literature on learning-based camera-LiDAR calibration, \textit{i.e.}, predicting the 6-DoF rigid body transformation between a camera and a 3D LiDAR, without requiring any presence of specific features or landmarks in the implementation. Like calibration works on other types of cameras/systems, this research field can also be classified into regression-based solutions and flow/reconstruction-based solutions. But we are prone to follow the special \textit{matching} principle in camera-LiDAR calibration and divide the existing learning-based literature into three categories: pixel-level solution, semantics-level solution, and object/keypoint-level solution.
\vspace{-0.3cm}

\subsection{Pixel-level Solution}
The first deep learning technique in camera-LiDAR calibration, RegNet~\cite{schneider2017regnet}, used CNNs to combine feature extraction, feature matching, and global regression to infer the 6-DoF extrinsic parameters. It processed the RGB and LiDAR depth map separately and branched two parallel data network streams. Then, a specific correlation layer was proposed to convolve the stacked LiDAR and RGB features as a joint representation. After this feature matching, the global information fusion and parameter regression were achieved by two fully connected layers with a Euclidean loss function. Motivated by this work, the subsequent works made a further step into more accurate camera-LiDAR calibration in terms of the geometric constraint~\cite{iyer2018calibnet,shi2020calibrcnn}, temporal correlation~\cite{shi2020calibrcnn}, loss design~\cite{yuan2020rggnet}, feature extraction~\cite{wang2022fusionnet}, feature matching~\cite{lv2021lccnet, wu2021netcalib}, feature fusion~\cite{wang2022fusionnet}, and calibration representation~\cite{lv2021cfnet, jing2022dxq}.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/CalibNet.png}
  %\vspace{-20pt}
  \caption{Network architecture of CalibNet. The figure is from ~\cite{iyer2018calibnet}.}
  \label{fig:CalibNet}
  \vspace{-0.3cm}
\end{figure}

For example, as shown in Figure~\ref{fig:CalibNet}, CalibNet~\cite{iyer2018calibnet} designed a network to predict calibration parameters that maximize the geometric and photometric consistency of images and point clouds, solving the underlying physical problem by 3D Spatial Transformers~\cite{handa2016gvnn}. To refine the calibration model, CalibRCNN~\cite{shi2020calibrcnn} presented a synthetic view and an epipolar geometry constraint to measure the photometric and geometric inaccuracies between consecutive frames, of which the temporal information learned by the LSTM network has been investigated in the learning-based camera-LiDAR calibration for the first time. Since the output space of the LiDAR-camera calibration is on the 3D Special Euclidean Group ($SE(3)$) rather than the normal Euclidean space, RGGNet~\cite{yuan2020rggnet} considered Riemannian geometry constraints in the loss function, namely, used a $SE(3)$ geodesic distance equipped with left-invariant Riemannian metrics to optimize the calibration network. LCCNet~\cite{lv2021lccnet} exploited the cost volume layer to learn the correlation between the image and the depth transformed by the point cloud. Because the depth map ignores the 3D geometric structure of the point cloud, FusionNet~\cite{wang2022fusionnet} leveraged PointNet++~\cite{qi2017pointnet++} to directly learn the features from the 3D point cloud. Subsequently, a feature fusion with Ball Query~\cite{qi2017pointnet++} and attention strategy was proposed to effectively fuse the features of images and point clouds.

CFNet~\cite{lv2021cfnet} first proposed the calibration flow for camera-LiDAR calibration, which represents the deviation between the positions of initial projected 2D points and ground truth. Compared to directly predicting extrinsic parameters, learning the calibration flow helped the network to understand the underlying geometric constraint. To build precise 2D-3D correspondences, CFNet~\cite{lv2021cfnet} corrected the originally projected points using the estimated calibration flow. Then the efficient Perspective-n-Point (EPnP) algorithm was applied to calculate the final extrinsic parameters by RANSAC. Because RANSAC is nondifferentiable, DXQ-Net~\cite{jing2022dxq} further presented a probabilistic model for LiDAR-camera calibration flow, which estimates the uncertainty to measure the quality of LiDAR-camera data association. Then, the differentiable pose estimation module was designed for solving extrinsic parameters, back-propagating the extrinsic error to the flow prediction network. 
\vspace{-0.3cm}

\subsection{Semantics-level Solution}
Semantic features can be well learned and represented by deep neural networks. A perfect calibration enables to accurately align the same instance in different sensors. To this end, some works~\cite{wang2020soic, zhu2020online, liu2021semalign, SST-Calib} explored to guide the camera-LiDAR calibration with the semantic information. SOIC~\cite{wang2020soic} calibrated and transforms the initialization issue into the semantic centroids' PnP problem using semantic information. Since the 3D semantic centroids of the point cloud and the 2D semantic centroids of the picture cannot match precisely, a matching constraint cost function based on the semantic components was presented. SSI-Calib~\cite{zhu2020online} reformulated the calibration as an optimization problem with a novel calibration quality metric based on semantic features. Then, a non-monotonic subgradient ascent algorithm was proposed to calculate the calibration parameters. Other works utilized the off-the-shelf segmentation networks for point cloud and image, and optimized the calibration parameters by minimizing semantic alignment loss in single-direction~\cite{liu2021semalign} and bi-direction~\cite{SST-Calib}.
\vspace{-0.2cm}
\subsection{Object/Keypoint-level Solution}
ATOP~\cite{ATOP} designed an attention-based object-level matching network, \textit{i.e.}, Cross-Modal Matching Network to explore the overlapped FoV between camera and LiDAR, which facilitated generating the 2D-3D object-level correspondences. 2D and 3D object proposals were
detected by YOLOv4~\cite{bochkovskiy2020yolov4} and PointPillar~\cite{lang2019pointpillars}. Then, two cascaded PSO-based algorithms~\cite{poli2007particle} were devised to estimate the calibration extrinsic parameters in the optimization stage. Using the deep
declarative network (DDN)~\cite{gould2021deep}, RKGCNet~\cite{RKGCNet} combined the standard neural layer and a PnP solver in the same network, formulating the 2D–3D data association and pose estimation as a bilevel optimization problem. Therefore, both the feature extraction capability of the convolutional layer and the conventional geometric solver can be employed. Microsoft’s human keypoint extraction network~\cite{xiao2018simple} was applied to detect the 2D–3D matching keypoints. Additionally, RKGCNet~\cite{RKGCNet} presented a learnable weight layer that determines the keypoints involved in the solver, enabling the whole pipeline to be trained end-to-end.
\vspace{-0.2cm}
\subsection{Discussion}
\subsubsection{Technique Summary}
The current method can be briefly classified based on the principle of building 2D and 3D matching, namely, the calibration target. In summary, most pixel-level solutions utilized the end-to-end framework to address this task. While these solutions delivered satisfactory performances on specific datasets, their generalization abilities are limited.
Semantics-level and object/keypoint-level methods derived from traditional calibration offered both acceptable performances and generalization abilities. However, they heavily relied on the quality of fore-end feature extraction.
\vspace{-0.2cm}
\subsubsection{Research Trend}

(1) Network architecture is becoming more complex with the use of different structures for feature extraction, matching, and fusion. Current methods employ strategies like multi-scale feature extraction, cross-modal interaction, cost-volume establishment, and confidence-guided fusion.

(2) Directly regressing 6-DoF parameters yields weak generalization ability. To overcome this, intermediate representations like calibration flow have been introduced. Additionally, calibration flow can handle non-rigid transformations that are common in real-world applications.

(3) Traditional methods require specific environments but have well-designed strategies. To balance accuracy and generalization, a combination of geometric solving algorithms and learning methods has been investigated.
\vspace{-0.1cm}
\subsubsection{Future Effort}

(1) Camera-LiDAR calibration methods typically rely on datasets like KITTI, which provide only initial extrinsic parameters. To create a decalibration dataset, researchers add noise transformations to the initial extrinsics, but this approach assumes a fixed position camera-LiDAR system with miscalibration. In real-world applications, the camera-LiDAR relative pose varies, making it challenging to collect large-scale real data with ground truth extrinsics. To address this challenge, generating synthetic camera-LiDAR data using simulation systems could be a valuable solution.

(2) To optimize the combination of networks and traditional solutions, a more compact approach is needed. Current methods mainly use networks as feature extractors, resulting in non-end-to-end pipelines with inadequate feature extraction adjustments for calibration. A deep declarative network (DDN) is a promising framework for making the entire pipeline differentiable. The aggregation of learning and traditional methods can be optimized using DDN.

(3) The most important aspect of camera-LiDAR calibration is 2D-3D matching. To achieve this, the point cloud is commonly transformed into a depth image. However, large deviations in extrinsic simulation can result in detail loss. With the great development of Transformer and cross-modal techniques, we believe leveraging Transformer to directly learn the features of image and point cloud in the same pipeline could facilitate better 2D-3D matching. 