\section{Preliminaries}
\label{sec2}
Deep learning has brought new inspirations to camera calibration, enabling a fully automatic calibration procedure without manual intervention. Here, we first summarize two prevalent paradigms in learning-based camera calibration: regression-based calibration and reconstruction-based calibration. Then, the widely-used learning strategies are reviewed in this research field. The detailed definitions for classical camera models and their corresponding calibration objectives are exhibited in the supplementary material.

\vspace{-0.3cm}

\subsection{Learning Paradigm}
Driven by different architectures of the neural network, the researchers have developed two main paradigms for learning-based camera calibration and its applications.

\noindent \textbf{Regression-based Calibration}
Given an uncalibrated input, the regression-based calibration first extracts the high-level semantic features using stacked convolutional layers. Then, the fully connected layers aggregate the semantic features and form a vector of the estimated calibration objective. The regressed parameters are used to conduct subsequent tasks such as distortion rectification, image warping, camera localization, etc. This paradigm is the earliest and has a dominant role in learning-based camera calibration and its applications. All the first works in various objectives, \textit{e.g.}, intrinsics: Deepfocal \cite{DeepFocal}, extrinsic: PoseNet \cite{PoseNet}, radial distortion: Rong et al. \cite{Rong}, rolling shutter distortion: URS-CNN \cite{URS-CNN}, homography matrix: DHN \cite{DHN}, hybrid parameters: Hold-Geoffroy et al. \cite{Hold-Geoffroy}, camera-LiDAR parameters: RegNet \cite{schneider2017regnet} have been achieved with this paradigm.

\noindent \textbf{Reconstruction-based Calibration}
On the other hand, the reconstruction-based calibration paradigm discards the parameter regression and directly learns the pixel-level mapping function between the uncalibrated input and target, inspired by the conditional image-to-image translation \cite{pix2pix} and dense visual perception\cite{long2015fully, eigen2014depth}. The reconstructed results are then calculated for the pixel-wise loss with the ground truth. In this regard, most reconstruction-based calibration methods \cite{DR-GAN, DDM, DaRecNet, BlindCor} design their network architecture based on the fully convolutional network such as U-Net\cite{ronneberger2015u}. Specifically, an encoder-decoder network, with skip connections between the encoder and decoder features at the same spatial resolution, progressively extracts the features from low-level to high-level and effectively integrates multi-scale features. At the last convolutional layer, the learned features are aggregated into the target channel, reconstructing the calibrated result at the pixel level.

In contrast to the regression-based paradigm, the reconstruction-based paradigm does not require the label of diverse camera parameters. Besides, the imbalance loss problem can be eliminated since it only optimizes the photometric loss of calibrated results. Therefore, the reconstruction-based paradigm enables a blind camera calibration without a strong camera model assumption.

\vspace{-0.3cm}

\subsection{Learning Strategies}
In the following, we review the learning-based camera calibration literature regarding different learning strategies.

\noindent \textbf{Supervised Learning}
Most learning-based camera calibration methods train their networks with the supervised learning strategy, from the classical methods \cite{DeepFocal, PoseNet, DHN, DeepVP, Rong, DeepCalib} to the state-of-the-art methods \cite{DVPD, EvUnroll, FishFormer, DAMG-Homo, SST-Calib}. In terms of the learning paradigm, this strategy supervises the network with the ground truth of the camera parameters (regression-based paradigm) or paired data (reconstruction-based paradigm). In general, they synthesize the training dataset from other large-scale datasets, under the random parameter/transformation sampling and camera model simulation. Some recent works \cite{Zhao, Tan, SPEC, DeepUnrollNet} establish their training dataset using a real-world setup and label the captured images with manual annotations, thereby fostering advancements in this research domain.

\noindent \textbf{Semi-Supervised Learning}
Training the network using an annotated dataset under diverse scenarios is an effective learning strategy. However, human annotation can be prone to errors, leading to inconsistent annotation quality or the inclusion of contaminated data. Consequently, increasing the training dataset to improve performance can be challenging due to the complexity and cost of constructing the dataset. To address this challenge, SS-WPC\cite{SS-WPC} proposes a semi-supervised method for correcting portraits captured by a wide-angle camera. It employs a surrogate task (segmentation) and a semi-supervised method that utilizes direction and range consistency and regression consistency to leverage both labeled and unlabeled data.

\noindent \textbf{Weakly-Supervised Learning}
Although significant progress has been made, data labeling for camera calibration is a notorious costly process, and obtaining perfect ground-truth labels is challenging. As a result, it is often preferable to use weak supervision with machine learning methods. Weakly supervised learning refers to the process of building prediction models through learning with inadequate supervision. Zhu et al. \cite{Zhu} present a weakly supervised camera calibration method for single-view metrology in unconstrained environments, where there is only one accessible image of a scene composed of objects of uncertain sizes. This work leverages 2D object annotations from large-scale datasets, where people and buildings are frequently present and serve as useful ``reference objects'' for determining 3D size.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{figures/taxonomy1.pdf}
  %\vspace{-20pt}
  \caption{The structural and hierarchical taxonomy of camera calibration with deep learning. Some classical methods are listed under each category.}
  \label{fig:taxonomy}
  \vspace{-0.2cm}
\end{figure*}

\noindent \textbf{Unsupervised Learning}
Unsupervised learning, commonly referred to as unsupervised machine learning, analyzes and groups unlabeled datasets using machine learning algorithms. UDHN \cite{UDHN} is the first work for a cross-view camera model using unsupervised learning, which estimates the homography matrix of a paired image without the projection labels. By reducing a pixel-wise intensity error that does not require ground truth data, UDHN \cite{UDHN} outperforms previous supervised learning techniques. While preserving superior accuracy and robustness to fluctuation in light, the proposed unsupervised algorithm can also achieve faster inference time. Inspired by this work, increasing more methods leverage the unsupervised learning strategy to estimate the homography such as CA-UDHN \cite{CA-UDHN}, BaseHomo \cite{BasesHomo}, HomoGAN\cite{HomoGAN}, and Liu et al. \cite{Liu}. Besides, UnFishCor \cite{UnFishCor} frees the demands for distortion parameters and designs an unsupervised framework for the wide-angle camera.

\noindent \textbf{Self-supervised Learning}
Robotics is where the phrase ``self-supervised learning'' first appears, as training data is automatically categorized by utilizing relationships between various input sensor signals. Compared to supervised learning, self-supervised learning leverages input data itself as the supervision. Many self-supervised techniques are presented to learn visual characteristics from massive amounts of unlabeled photos or videos without the need for time-consuming and expensive human annotations. SSR-Net \cite{SSR-Net} presents a self-supervised deep homography estimation network, which relaxes the need for ground truth annotations and leverages the invertibility constraints of homography. To be specific, SSR-Net \cite{SSR-Net} utilizes the homography matrix representation in place of other approaches' typically-used 4-point parameterization, to apply the invertibility constraints. SIR \cite{SIR} devises a brand-new self-supervised camera calibration pipeline for wide-angle image rectification, based on the principle that the corrected results of distorted images of the same scene taken with various lenses need to be the same. With self-supervised depth and pose learning as a proxy aim, Fang et al. \cite{Fang} present to self-calibrate a range of generic camera models from raw video, offering for the first time a calibration evaluation of camera model parameters learned solely via self-supervision.

\noindent \textbf{Reinforcement Learning}
Instead of aiming to minimize at each stage, reinforcement learning can maximize the cumulative benefits of a learning process as a whole. To date, DQN-RecNet~\cite{DQN-RecNet} is the first and only work in camera calibration using reinforcement learning. It applies a deep reinforcement learning technique to tackle the fisheye image rectification by a single Markov Decision Process, which is a multi-step gradual calibration scheme. In this situation, the current fisheye image represents the state of the environment. The agent, Deep Q-Network \cite{mnih2015human}, generates an action that should be executed to correct the distorted image.

In the following, we will review the specific methods and literature for learning-based camera calibration. The structural and hierarchical taxonomy is shown in Figure~\ref{fig:taxonomy}. 
	
	\begin{table*}
		\rowcolors{1}{gray!20}{white}
		\centering
		\caption{
			{Details of the learning-based camera calibration and its extended applications from 2015 to 2022, including the method abbreviation, publication, calibration objective, network architecture, loss function, dataset, evaluation metrics, learning strategy, platform, and simulation or not (training data). For the learning strategies, SL, USL, WSL, Semi-SL, SSL, and RL denote supervised learning, unsupervised learning, weakly-supervised learning, semi-supervised learning, self-supervised learning, and reinforcement learning, respectively. }
		}
		\vspace{-6pt}
		\label{table:methods}
		\begin{threeparttable}
			\resizebox{1\textwidth}{!}{
				\setlength\tabcolsep{2pt}
				\renewcommand\arraystretch{0.98}
				% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}
				\begin{tabular}{c|r||c|c|c|c|c|c|c|c|c}
					\hline
					%\thickhline
					% &\#&
					&\textbf{Method}~~~~~~~~~&\textbf{Publication} &\textbf{Objective} &\textbf{Network}
					&\textbf{Loss Function} & \textbf{Dataset} &\textbf{Evaluation} & \textbf{Learning} &\textbf{Platform} &\textbf{Simulation}\\
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2015}}}
					% &1 &
					&DeepFocal~\cite{DeepFocal} &ICIP &Standard &AlexNet
					&$\mathcal{L}_2$ loss &1DSfM\cite{1DSfM} & Accuracy & SL &Caffe &\\
					&PoseNet~\cite{PoseNet} &ICCV &Standard 
					&GoogLeNet
					&$\mathcal{L}_2$ loss &Cambridge Landmarks\cite{Cambridge_Landmarks} &Accuracy &SL &Caffe& \\
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2016}}}
					% &1&
					&DeepHorizon~\cite{DeepHorizon} &BMVC &Standard &GoogLeNet	&Huber loss &HLW\cite{HLW} & Accuracy & SL &Caffe &\\
					
					&DeepVP~\cite{DeepVP} &CVPR &Standard 
					&AlexNet
					&Logistic loss &YUD\cite{YUD}, ECD\cite{ECD}, HLW\cite{HLW} &Accuracy &SL &Caffe& \\	
					
					&Rong et al.~\cite{Rong} &ACCV &Distortion &AlexNet
					&Softmax loss &ImageNet\cite{ImageNet} &Line length &SL &Caffe&\checkmark\\
					
					&DHN\cite{DHN} &RSSW &Cross-View &VGG
					&$\mathcal{L}_2$ loss &MS-COCO\cite{MS-COCO} &MSE &SL &Caffe&\checkmark\\		\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2017}}}
					% &1&
					&CLKN~\cite{CLKN} &CVPR &Cross-View  &CNNs	&Hinge loss &MS-COCO\cite{MS-COCO} & MSE & SL &Torch &\checkmark\\
					
		            &HierarchicalNet~\cite{HierarchicalNet} &ICCVW &Cross-View 
					&VGG
					&$\mathcal{L}_2$ loss &MS-COCO\cite{MS-COCO} &MSE &SL &TensorFlow&\checkmark \\
					
					&URS-CNN~\cite{URS-CNN} &CVPR &Distortion 
					&CNNs
					&$\mathcal{L}_2$ loss &Sun\cite{xiao2010sun}, Oxford\cite{philbin2007object}, Zubud\cite{shao2003zubud}, LFW\cite{huang2008labeled} &PSNR, RMSE &SL &Torch&\checkmark\\
					
					&RegNet~\cite{schneider2017regnet} &IV &Cross-Sensor 
					&CNNs
					&$\mathcal{L}_2$ loss &KITTI\cite{KITTI} &MAE &SL &Caffe&\checkmark\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2018}}}
					% &1&
					&Hold-Geoffroy et al.~\cite{Hold-Geoffroy} &CVPR &Standard &DenseNet	&Entropy loss &SUN360\cite{SUN360} & Human sensitivity & SL &- &\\
					
					&DeepCalib~\cite{DeepCalib} &CVMP &Distortion 
					&Inception-V3
					&Logcosh loss &SUN360\cite{SUN360} &Mean error &SL &TensorFlow&\checkmark \\	
					&FishEyeRecNet~\cite{FishEyeRecNet} &ECCV &Distortion &VGG
					&$\mathcal{L}_2$ loss &ADE20K\cite{ADE20K} &PSNR, SSIM &SL &Caffe&\checkmark\\
					
					&Shi et al.\cite{Shi} &ICPR &Distortion &ResNet
					&$\mathcal{L}_2$ loss &ImageNet\cite{ImageNet} &MSE &SL &PyTorch&\checkmark\\
					
					&DeepFM\cite{DeepFM} &ECCV &Cross-View &ResNet
					&$\mathcal{L}_2$ loss &T\&T\cite{TT}, KITTI\cite{KITTI}, 1DSfM\cite{1DSfM} &F-score, Mean &SL &PyTorch&\checkmark\\
					
					&Poursaeed et al.\cite{Poursaeed} &ECCVW &Cross-View &CNNs
					&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &EPI-ABS, EPI-SQR &SL &-& \\
					
					&UDHN\cite{UDHN} &RAL &Cross-View &VGG
					&$\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO} &RMSE &USL &TensorFlow&\checkmark\\
					
					&PFNet\cite{PFNet} &ACCV &Cross-View &FCN
					&Smooth $\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO} &MAE &SL &TensorFlow&\checkmark\\
					
					&CalibNet\cite{iyer2018calibnet} &IROS &Cross-Sensor &ResNet
					&Point cloud distance, $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &Geodesic distance, MAE &SL &TensorFlow&\checkmark\\

                        &Chang et al.\cite{chang2018deepvp} &ICRA &Standard &AlexNet
					&Cross-entropy loss &DeepVP-1M~\cite{chang2018deepvp} &MSE, Accuracy &SL &Matconvnet&\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2019}}}
					% &1&
					&Lopez et al.~\cite{Lopez} &CVPR &Distortion &DenseNet	&Bearing loss &SUN360\cite{SUN360} &MSE & SL &PyTorch &\\
					
					&UprightNet~\cite{UprightNet} &ICCV &Standard &U-Net	&Geometry loss &InteriorNet\cite{InteriorNet}, ScanNet\cite{ScanNet}, SUN360\cite{SUN360} &Mean error & SL &PyTorch &\\
					
					&Zhuang et al.~\cite{Zhuang} &IROS &Distortion &ResNet	&$\mathcal{L}_1$ loss & KITTI\cite{KITTI} &Mean error, RMSE & SL &PyTorch &\checkmark\\
					
					&SSR-Net~\cite{SSR-Net} &PRL &Cross-View &ResNet	&$\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO} &MAE & SSL &PyTorch &\checkmark\\
					
					&Abbas et al.~\cite{Abbas} &ICCVW &Cross-View &CNNs	&Softmax loss & CARLA\cite{CARLA} &AUC\cite{AUC}, Mean error & SL &TensorFlow &\checkmark\\
					
					&DR-GAN~\cite{DR-GAN} &TCSVT &Distortion &GANs	&Perceptual loss & MS-COCO\cite{MS-COCO} &PSNR, SSIM & SL &TensorFlow &\checkmark\\
					
					&STD~\cite{STD} &TCSVT &Distortion &GANs+CNNs	&Perceptual loss & MS-COCO\cite{MS-COCO} &PSNR, SSIM & SL &TensorFlow &\checkmark\\
					
					&Deep360Up~\cite{Deep360Up} &VR &Standard &DenseNet	&Log-cosh loss\cite{Log-cosh} & SUN360\cite{SUN360} &Mean error & SL &- &\checkmark\\
					
					&UnFishCor~\cite{UnFishCor} &JVCIR &Distortion &VGG	&$\mathcal{L}_1$ loss & Places2\cite{Places2} &PSNR, SSIM & USL &TensorFlow &\checkmark\\
					
					&BlindCor~\cite{BlindCor} &CVPR &Distortion &U-Net	&$\mathcal{L}_2$ loss & Places2\cite{Places2} &MSE & SL &PyTorch &\checkmark\\
					
					&RSC-Net~\cite{RSC-Net} &CVPR &Distortion &ResNet	&$\mathcal{L}_1$ loss & KITTI\cite{KITTI} &Mean error & SL &PyTorch &\checkmark\\
					
					&Xue et al.~\cite{Xue} &CVPR &Distortion &ResNet	&$\mathcal{L}_2$ loss & Wireframes\cite{Wireframes}, SUNCG\cite{SUNCG} &PSNR, SSIM, RPE & SL &PyTorch &\checkmark\\
					
					&Zhao et al.~\cite{Zhao} &ICCV &Distortion &VGG+U-Net	&$\mathcal{L}_1$ loss & Self-constructed+BU-4DFE\cite{BU-4DFE} &Mean error &SL &- &\checkmark\\

					&NeurVPS~\cite{zhou2019neurvps} &NeurIPS &Standard &CNNs	&Binary cross entropy, chamfer-$\mathcal{L}_2$ loss &ScanNet~\cite{ScanNet}, SU3~\cite{SU3} &Angle accuracy &SL &PyTorch &\\

     
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2020}}}
					% &1&
					&Sha et al.~\cite{Sha} &CVPR &Cross-View &U-Net	& Cross-entropy loss &World Cup 2014\cite{homayounfar2017sports} &IoU & SL &TensorFlow &\\
				    
				    &Lee et al.~\cite{Lee} &ECCV &Standard &PointNet + CNNs	& Cross-entropy loss &Google Street View\cite{googleStreet}, HLW\cite{HLW} &Mean error, AUC\cite{AUC} & SL &- &\\
				    
				    &MisCaliDet~\cite{MisCaliDet} &ICRA &Distortion &CNNs	& $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &MSE & SL &TensorFlow &\checkmark\\
				    
				    &DeepPTZ~\cite{DeepPTZ} &WACV &Distortion &Inception-V3	& $\mathcal{L}_1$ loss &SUN360\cite{SUN360} &Mean error & SL &PyTorch &\checkmark\\
				    
				    &MHN~\cite{MHN} &CVPR &Cross-View &VGG	&Cross-entropy loss &MS-COCO\cite{MS-COCO}, Self-constructed &MAE & SL &TensorFlow &\checkmark\\
				    
				    &Davidson et al.~\cite{Davidson} &ECCV &Standard &FCN	&Dice loss &SUN360\cite{SUN360} &Accuracy &SL &- &\checkmark\\
				    
				    &CA-UDHN~\cite{CA-UDHN} &ECCV &Cross-View &FCN + ResNet	&Triplet loss &Self-constructed &MSE &USL &PyTorch &\\
				    
				    &DeepFEPE~\cite{DeepFEPE} &IROS &Standard &VGG + PointNet	&$\mathcal{L}_2$ loss &KITTI\cite{KITTI}, ApolloScape\cite{Apolloscape} &Mean error &SL &PyTorch &\\
				    
				    &DDM~\cite{DDM} &TIP &Distortion &GANs	&$\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO} &PSNR, SSIM &SL &TensorFlow &\checkmark\\
				    
				    &Li et al.~\cite{Li} &TIP &Distortion &CNNs	&Cross-entropy, $\mathcal{L}_1$ loss &CelebA\cite{CelebA} &Cosine distance &SL &- &\checkmark\\
				    
				    &PSE-GAN~\cite{PSE-GAN} &ICPR &Distortion &GANs	&$\mathcal{L}_1$, WGAN loss &Place2\cite{Places2} &MSE &SL &- &\checkmark\\
				    
				    &RDC-Net~\cite{RDC-Net} &ICIP &Distortion &ResNet	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &ImageNet\cite{ImageNet} &PSNR, SSIM &SL &PyTorch &\checkmark\\
				    
				    &FE-GAN~\cite{FE-GAN} &ICASSP &Distortion &GANs	&$\mathcal{L}_1$, GAN loss &Wireframe\cite{Wireframes}, LSUN\cite{LSUN} &PSNR, SSIM, RMSE &SSL &PyTorch &\checkmark\\
				    
				    &RDCFace~\cite{RDCFace} &CVPR &Distortion &ResNet	&Cross-entropy, $\mathcal{L}_2$ loss &IMDB-Face\cite{IMDB-Face} &Accuracy &SL &- &\checkmark\\
				    
				    &LaRecNet~\cite{LaRecNet} &arXiv &Distortion &ResNet	&$\mathcal{L}_2$ loss &Wireframes\cite{Wireframes}, SUNCG\cite{SUNCG} &PSNR, SSIM, RPE &SL &PyTorch &\checkmark\\
				    
				    &Baradad et al.~\cite{Baradad} &CVPR &Standard &CNNs	&$\mathcal{L}_2$ loss &ScanNet\cite{ScanNet}, NYU\cite{NYU}, SUN360\cite{SUN360} &Mean error, RMS &SL &PyTorch &\\
				    
				    &Zheng et al.~\cite{Zheng} &CVPR &Standard &CNNs	&$\mathcal{L}_1$ loss &FocaLens\cite{FocaLens} &Mean error, PSNR, SSIM &SL &- &\checkmark\\
				    
				    &Zhu et al.~\cite{Zhu} &ECCV &Standard &CNNs + PointNet	&$\mathcal{L}_1$ loss &SUN360\cite{SUN360}, MS-COCO\cite{MS-COCO} &Mean error, Accuracy &WSL &PyTorch &\checkmark\\
				    
				    &DeepUnrollNet~\cite{DeepUnrollNet} &CVPR &Distortion &FCN	&$\mathcal{L}_1$, perceptual, total variation loss &Carla-RS\cite{DeepUnrollNet}, Fastec-RS\cite{DeepUnrollNet}  &PSNR, SSIM &SL &PyTorch &\checkmark\\
				    
				    &RGGNet~\cite{yuan2020rggnet} &RAL &Cross-Sensor &ResNet	&Geodesic distance loss &KITTI\cite{KITTI}  &MSE, MSEE, MRR &SL &TensorFlow &\checkmark\\
				    
				    &CalibRCNN~\cite{shi2020calibrcnn} &IROS &Cross-Sensor &RNNs	&$\mathcal{L}_2$, Epipolar geometry loss &KITTI~\cite{KITTI}  &MAE &SL &TensorFlow &\checkmark\\

				    &SSI-Calib~\cite{zhu2020online} &ICRA &Cross-Sensor &CNNs	&$\mathcal{L}_2$ loss &Pascal VOC 2012~\cite{pascal-voc-2012}  &Mean/standard deviation &SL &TensorFlow &\checkmark\\

				    &SOIC~\cite{wang2020soic} &arXiv &Cross-Sensor &ResNet + PointRCNN	& Cost function &KITTI~\cite{KITTI}  &Mean error &SL &- &\\        

				    &NetCalib~\cite{wu2021netcalib} &ICPR &Cross-Sensor &CNNs	&$\mathcal{L}_1$ loss &KITTI~\cite{KITTI}  &MAE &SL &PyTorch &\checkmark\\

				    &SRHEN~\cite{SRHEN} &ACM-MM &Cross-View &CNNs	&$\mathcal{L}_2$ loss &MS-COCO~\cite{MS-COCO}, SUN397~\cite{SUN360}  &MACE &SL &- &\checkmark\\
                        
				   
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2021}}}
					% &1&
					&StereoCaliNet~\cite{StereoCaliNet} &TCI &Standard &U-Net	&$\mathcal{L}_1$ loss &TAUAgent\cite{TAUAgent}, KITTI\cite{KITTI} &Mean error & SL &PyTorch &\checkmark\\
					
					&CTRL-C~\cite{CTRL-C} &ICCV &Standard &Transformer	&Cross-entropy, $\mathcal{L}_1$ loss &Google Street View\cite{googleStreet}, SUN360\cite{SUN360} &Mean error, AUC\cite{AUC} & SL &PyTorch &\checkmark\\
					
				   &Wakai et al.~\cite{Wakai} &ICCVW &Distortion &DenseNet	&Smooth $\mathcal{L}_1$ loss &StreetLearn\cite{StreetLearn} &Mean error, PSNR, SSIM & SL &- &\checkmark\\
				   &OrdianlDistortion~\cite{OrdianlDistortion} &TIP &Distortion &CNNs	&Smooth $\mathcal{L}_1$ loss & MS-COCO\cite{MS-COCO} &PSNR, SSIM, MDLD & SL &TensorFlow &\checkmark\\
				   
				   &PolarRecNet~\cite{PolarRecNet} &TCSVT &Distortion &VGG + U-Net	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO}, LMS\cite{LMS} &PSNR, SSIM, MSE & SL &PyTorch &\checkmark\\
				   
				   &DQN-RecNet~\cite{DQN-RecNet} &PRL &Distortion &VGG	&$\mathcal{L}_2$ loss & Wireframes\cite{Wireframes} &PSNR, SSIM, MSE & RL &PyTorch &\checkmark\\
				   
				   &Tan et al.~\cite{Tan} &CVPR &Distortion &U-Net &$\mathcal{L}_2$ loss & Self-constructed &Accuracy & SL &PyTorch & \\
				   
				   &PCN~\cite{PCN} &CVPR &Distortion &U-Net &$\mathcal{L}_1$, $\mathcal{L}_2$, GAN loss & Place2\cite{Places2} &PSNR, SSIM, FID, CW-SSIM & SL &PyTorch &\checkmark \\
				   
				   &DaRecNet~\cite{DaRecNet} &ICCV &Distortion &U-Net &Smooth $\mathcal{L}_1$, $\mathcal{L}_2$ loss & ADE20K\cite{ADE20K} &PSNR, SSIM & SL &PyTorch &\checkmark \\
				   
				   &DLKFM~\cite{DLKFM} &CVPR &Cross-View &Siamese-Net &$\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO}, Google Earth, Google Map &MSE & SL &TensorFlow &\checkmark \\
				   
				   &LocalTrans~\cite{LocalTrans} &ICCV &Cross-View &Transformer &$\mathcal{L}_1$ loss & MS-COCO\cite{MS-COCO} &MSE, PSNR, SSIM & SL &PyTorch &\checkmark \\
				   
				   &BasesHomo~\cite{BasesHomo} &ICCV &Cross-View &ResNet &Triplet loss & CA-UDHN\cite{CA-UDHN} &MSE & USL &PyTorch & \\
				   &ShuffleHomoNet~\cite{ShuffleHomoNet} &ICIP &Cross-View &ShuffleNet &$\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO} &RMSE & SL &TensorFlow &\checkmark \\
				   
				   &DAMG-Homo~\cite{DAMG-Homo} &TCSVT &Cross-View &CNNs &$\mathcal{L}_1$ loss & MS-COCO\cite{MS-COCO}, UDIS\cite{UDIS} &RMSE, PSNR, SSIM & SL &TensorFlow &\checkmark \\
				   
				   &SA-MobileNet~\cite{SA-MobileNet} &BMVC &Standard &MobileNet &Cross-entropy loss& SUN360\cite{SUN360}, ADE20K\cite{ADE20K}, NYU\cite{NYU} &MAE, Accuracy & SL &TensorFlow &\checkmark \\
				   
				   &SPEC~\cite{SPEC} &ICCV &Standard &ResNet &Softargmax-$\mathcal{L}_2$ loss&Self-constructed &W-MPJPE, PA-MPJPE & SL &PyTorch &\checkmark \\
				   
				   &DirectionNet~\cite{DirectionNet} &CVPR &Standard &U-Net &Cosine similarity loss &InteriorNet\cite{InteriorNet}, Matterport3D\cite{Matterport3D}&Mean and median error  & SL &TensorFlow &\checkmark \\
				   
				   &JCD~\cite{JCD} &CVPR &Distortion &FCN &Charbonnier\cite{Charbonnier}, perceptual loss &BS-RSCD \cite{JCD}, Fastec-RS
                   \cite{DeepUnrollNet}&PSNR, SSIM, LPIPS  & SL &PyTorch & \\
                   
                   &LCCNet~\cite{lv2021lccnet} &CVPRW &Cross-Sensor &CNNs &Smooth $\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &MSE  & SL &PyTorch &\checkmark \\
                   
                   &CFNet~\cite{lv2021cfnet} &Sensors &Cross-Sensor &FCN &$\mathcal{L}_1$, Charbonnier\cite{Charbonnier} loss &KITTI\cite{KITTI}, KITTI-360\cite{liao2022kitti} &MAE, MSEE, MRR  & SL &PyTorch &\checkmark \\

                   &Fan\etal~\cite{fan2021inverting} &ICCV &Distortion &U-Net &$\mathcal{L}_1$, perceptual loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS  & SL &PyTorch & \\

                   &SUNet~\cite{SUNet} &ICCV &Distortion &DenseNet + ResNet &$\mathcal{L}_1$, perceptual loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM  & SL &PyTorch & \\

                   &SemAlign~\cite{liu2021semalign} &IROS &Cross-Sensor &CNNs & Semantic alignment loss &KITTI~\cite{KITTI} &Mean/median rotation errors & SL &PyTorch &\checkmark\\
       
				   \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2022}}}
					% &1&
				   &DVPD~\cite{DVPD} &CVPR &Standard &CNNs	&Cross-entropy loss &SU3\cite{SU3}, ScanNet\cite{ScanNet}, YUD\cite{YUD}, NYU\cite{NYU} &Accuracy, AUC\cite{AUC} & SL &PyTorch &\checkmark\\
				   
				   &Fang et al.~\cite{Fang} &ICRA &Standard &CNNs	&$\mathcal{L}_2$ loss &KITTI\cite{KITTI}, EuRoC\cite{EuRoC}, OmniCam\cite{OmniCam} &MRE, RMSE & SSL &PyTorch &\\
				   
				   &CPL~\cite{CPL} &ICASSP &Standard &Inception-V3	&$\mathcal{L}_1$ loss &CARLA\cite{CARLA}, CyclistDetection\cite{CyclistDetection} &MAE & SL &TensorFlow &\checkmark\\
				   
				   &IHN~\cite{IHN} &CVPR &Cross-View  &Siamese-Net	&$\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO}, Google Earth, Google Map &MACE & SL &PyTorch &\checkmark\\
				   
				   &HomoGAN~\cite{HomoGAN} &CVPR &Cross-View  &GANs	&Cross-entropy, WGAN loss &CA-UDHN\cite{CA-UDHN} &Mean error & USL &PyTorch &\checkmark\\
				   
				   &SS-WPC~\cite{SS-WPC} &CVPR &Distortion  &Transformer	&Cross-entropy, $\mathcal{L}_1$ loss &Tan et al.\cite{Tan} &Accuracy & Semi-SL &PyTorch &\\
				   
				   &AW-RSC~\cite{AW-RSC} &CVPR &Distortion  &CNNs	&Charbonnier\cite{Charbonnier}, perceptual loss &Self-constructed, FastecRS\cite{DeepUnrollNet} &PSNR, SSIM &SL &PyTorch &\\
				   
				   &EvUnroll~\cite{EvUnroll} &CVPR &Distortion  &U-Net	&Charbonnier, perceptual, TV loss &Self-constructed, FastecRS\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS &SL &PyTorch &\\
				   
				   &Do et al.~\cite{Do} &CVPR &Standard  &ResNet&$\mathcal{L}_2$, Robust angular \cite{RobustAngular} loss &Self-constructed, 7-SCENES\cite{7-SCENES} &Median error, Recall &SL &PyTorch &\\
				   
				   &DiffPoseNet~\cite{DiffPoseNet} &CVPR &Standard  &CNNs + LSTM&$\mathcal{L}_2$ loss &TartanAir\cite{TartanAir}, KITTI\cite{KITTI}, TUM-RGBD\cite{TUM-RGBD} &PEE, AEE\cite{AEE} &SSL &PyTorch &\\
				   
				   &SceneSqueezer~\cite{SceneSqueezer} &CVPR &Standard  &Transformer&$\mathcal{L}_1$ loss &RobotCar Seasons\cite{RobotCar}, Cambridge Landmarks\cite{Cambridge_Landmarks}  &Mean error, Recall\cite{AEE} &SL &PyTorch &\\
				   
				   &FocalPose~\cite{FocalPose} &CVPR &Standard  &CNNs&$\mathcal{L}_1$, Huber loss &Pix3D\cite{Pix3D}, CompCars\cite{StanfordCars}, StanfordCars\cite{StanfordCars}  &Median error, Accuracy &SL &PyTorch &\\
				   
				   &DXQ-Net~\cite{jing2022dxq} &arXiv &Cross-Sensor  &CNNs + RNNs&$\mathcal{L}_1$, geodesic loss &KITTI\cite{KITTI}, KITTI-360\cite{liao2022kitti}  &MSE &SL &PyTorch &\checkmark\\
				   
				   &SST-Calib~\cite{SST-Calib} &ITSC &Cross-Sensor  &CNNs &$\mathcal{L}_2$ loss &KITTI\cite{KITTI}  &QAD, AEAD &SL &PyTorch &\checkmark\\
				   &CCS-Net~\cite{zhang2022learning} &IROS &Distortion  &U-Net&$\mathcal{L}_1$ loss &TUM-RGBD\cite{TUM-RGBD} &MAE, RPE &SL &PyTorch &\checkmark\\
				   
				   &FishFormer~\cite{FishFormer} &arXiv &Distortion  &Transformer&$\mathcal{L}_2$ loss &Place2\cite{Places2}, CelebA\cite{CelebA}  &PSNR, SSIM, FID &SL &PyTorch &\checkmark\\
				   
                  &SIR~\cite{SIR} &TIP &Distortion &ResNet &$\mathcal{L}_1$ loss & ADE20K\cite{ADE20K}, WireFrames\cite{Wireframes}, MS-COCO\cite{MS-COCO} &PSNR, SSIM & SSL &PyTorch &\checkmark \\

				   &ATOP~\cite{ATOP} &TIV &Cross-Sensor  &CNNs &Cross entropy loss &Self-constructed + KITTI\cite{KITTI}  &RRE, RTE &SL &- &\\

				   &FusionNet~\cite{wang2022fusionnet} &ICRA &Cross-Sensor  &CNNs+PointNet &$\mathcal{L}_2$ loss &KITTI\cite{KITTI}  &MAE &SL &PyTorch &\checkmark\\

				   &RKGCNet~\cite{RKGCNet} &TIM &Cross-Sensor  &CNNs+PointNet &$\mathcal{L}_1$ loss &KITTI\cite{KITTI}  &MSE &SL &PyTorch &\checkmark\\

                    &GenCaliNet~\cite{GenCaliNet} &ECCV &Distortion &DenseNet	&$\mathcal{L}_2$ loss &StreetLearn\cite{StreetLearn}, SP360\cite{SP360} &MAE, PSNR, SSIM & SL &- &\checkmark\\
       
				   &Liu et al.~\cite{Liu} &TPAMI &Cross-View &ResNet&Triplet loss &Self-constructed  &MSE, Accuracy &USL &PyTorch &\\
				   
				
				\hline
				\end{tabular}
			}
		\end{threeparttable}
	\end{table*}
	
	
	
	
	
	
	
	
	
	
	
	
	
