\section{Standard Model}
\label{sec:pure}
Generally, for learning-based calibration works, the objectives of the intrinsics calibration contain focal length and optical center, and the objectives of the extrinsic calibration contain the rotation matrix and translation vector.
\vspace{-0.2cm}
\subsection{Intrinsics Calibration}
Deepfocal \cite{DeepFocal} is a pioneer work in learning-based camera calibration, it aims to estimate the focal length of any image ``in the wild''. In detail, Deepfocal considered a simple pinhole camera model and regressed the horizontal field of view using a deep convolutional neural network. Given the width $w$ of an image, the relationship between the horizontal field of view $H_\theta$ and focal length $f$ can be described by:
\begin{equation}
H_\theta = 2\arctan(\frac{w}{2f}).
\label{eq-focal-length}
\end{equation}

Due to component wear, temperature fluctuations, or outside disturbances like collisions, the calibrated parameters of a camera are susceptible to change over time. To this end, MisCaliDet \cite{MisCaliDet} proposed to identify if a camera needs to be recalibrated intrinsically. Compared to the conventional intrinsic parameters such as the focal length and image center, MisCaliDet presented a new scalar metric, \textit{i.e.}, the average pixel position difference (APPD) to measure the degree of camera miscalibration, which describes the mean value of the pixel position differences over the entire image.

\subsection{Extrinsics Calibration}
In contrast to intrinsic calibration, extrinsic calibration infers the spatial correspondence of the camera and its located 3D scene. PoseNet\cite{PoseNet} first proposed deep convolutional neural networks to regress 6-DoF camera pose in real-time. A pose vector $\textbf{p}$ was predicted by PoseNet, given by the 3D position $\textbf{x}$ and orientation represented by quaternion $\textbf{q}$ of a camera, namely, $\textbf{p} = [\textbf{x}, \textbf{q}]$. For constructing the training dataset, the labels are automatically calculated from a video of the scenario using a structure from motion method \cite{wu2013towards}.

Inspired by PoseNet\cite{PoseNet}, the following works improved the extrinsic calibration in terms of the intermediate representation, interpretability, data format, learning objective, etc. For example, to optimize the geometric pose objective, DeepFEPE \cite{DeepFEPE} designed an end-to-end keypoint-based framework with learnable modules for detection, feature extraction, matching, and outlier rejection. Such a pipeline imitated the traditional baseline, in which the final performance can be analyzed and improved by the intermediate differentiable module. To bridge the domain gap between the extrinsic objective and image features, recent works proposed to first learn an intermediate representation from the input, such as surface geometry \cite{UprightNet}, depth map \cite{StereoCaliNet}, directional probability distribution \cite{DirectionNet}, and normal flow \cite{DiffPoseNet}, etc. Then, the extrinsic are reasoned by geometric constraints and learned representation. Therefore, the neural networks are gradually guided to perceive the geometry-related features, which are crucial for extrinsic estimation. Considering the privacy concerns and limited storage problem, some recent works compressed the scene and exploited the point-like feature to estimate the extrinsic. For example, Do et al. \cite{Do} trained a network to recognize sparse but significant 3D points, dubbed scene landmarks, by encoding their appearance as implicit features. And the camera pose can be calculated using a robust minimal solver followed by a Levenberg-Marquardt-based nonlinear refinement. SceneSqueezer \cite{SceneSqueezer} compressed the scene information from three levels: the database frames are clustered using pairwise co-visibility information, a point selection module prunes each cluster based on estimation performance, and learned quantization further compresses the selected points.

\subsection{Joint Intrinsic and Extrinsic Calibration}

\subsubsection{Geometric Representations}

\noindent \textbf{Vanishing Points}
The intersection of projections of a set of parallel lines in the world leads to a vanishing point. The detection of vanishing points is a fundamental and crucial challenge in 3D vision. In general, vanishing points reveal the direction of 3D lines, allowing the agent to deduce 3D scene information from a single 2D image.

DeepVP \cite{DeepVP} is the first learning-based work for detecting the vanishing points given a single image. It reversed the conventional process by scoring the horizon line candidates according to the vanishing points they contain. Chang et al. \cite{chang2018deepvp} redesigned this task as a CNN classification problem using an output layer with 225 discrete possible vanishing point locations. For constructing the dataset, the camera view is panned and tilted with step 5° from -35° to 35° in the panorama scene (total 225 images) from a single GPS location. To directly leverage the geometric properties of vanishing points, NeurVPS \cite{zhou2019neurvps} proposed a canonical conic space and a conic convolution operator that can be implemented as regular convolutions in this space, where the learning model is capable of calculating the global geometric information of vanishing points locally. To overcome the need for a large amount of training data in previous methods, DVPD \cite{DVPD} incorporated the neural network with two geometric priors: Hough transformation and Gaussian sphere. First, the convolutional features are transformed into a Hough domain, mapping lines to distinct bins. The projection of the Hough bins is then extended to the Gaussian sphere, where lines are transformed into great circles and vanishing points are located at the intersection of these circles. Geometric priors are data-efficient because they eliminate the necessity for learning this information from data, which enables an interpretable learning framework and generalizes better to domains with slightly different data distributions.

\noindent \textbf{Horizon Lines}
The horizon line is a crucial contextual attribute for various computer vision tasks especially image metrology, computational photography, and 3D scene understanding. The projection of the line at infinity onto any plane that is perpendicular to the local gravity vector determines the location of the horizon line.

Given the FoV, pitch, and roll of a camera, it is straightforward to locate the horizon line in its captured image space. DeepHorizon \cite{DeepHorizon} proposed the first learning-based solution for estimating the horizon line from an image, without requiring any explicit geometric constraints or other cues. To train the network, a new benchmark dataset, Horizon Lines in the Wild (HLW), was constructed, which consists of real-world images with labeled horizon lines. SA-MobileNet \cite{SA-MobileNet} proposed an image tilt detection and correction with self-attention MobileNet \cite{howard2019searching} for smartphones. A spatial self-attention module was devised to learn long-range dependencies and global context within the input images. To address the difficulty of the regression task, they trained the network to estimate multiple angles within a narrow interval of the ground truth tilt, penalizing only those values that locate outside this narrow range.
\vspace{-0.2cm}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/CTRL-C.pdf}
  %\vspace{-20pt}
  \caption{Overview of CTRL-C. The figure is from ~\cite{CTRL-C}.}
  \label{fig:CTRL-C}
  \vspace{-0.3cm}
\end{figure}

\subsubsection{Composite Parameters}
Calibrating the composite parameters aims to estimate the intrinsic parameters and extrinsic parameters simultaneously.
By jointly estimating composite parameters and training using data from a large-scale panorama dataset \cite{SUN360}, Hold-Geoffroy\etal \cite{Hold-Geoffroy} largely outperformed previous independent calibration tasks. Moreover, Hold-Geoffroy\etal \cite{Hold-Geoffroy} performed human perception research in which the participants were asked to evaluate the realism of 3D objects composited with and without accurate calibration. This data was further designed to a new perceptual measure for the calibration errors. In terms of the feature category, Lee\etal \cite{Lee} and CTRL-C \cite{CTRL-C} considered both semantic features and geometric cues for camera calibration. They showed how taking use of geometric features, is capable of facilitating the network to comprehend the underlying perspective structure of an image. The pipeline of CTRL-C is illustrated in Figure~\ref{fig:CTRL-C}. In recent literature, more applications are jointly studied with camera calibration, for example, single view metrology \cite{Zhu}, 3D human pose and shape estimation \cite{SPEC}, depth estimation \cite{Baradad, Fang}, object pose estimation \cite{FocalPose}, and image reflection removal \cite{Zheng}, etc.

Considering the heterogeneousness and visual implicitness of different camera parameters, CPL \cite{CPL} estimated the parameters using a novel camera projection loss, exploiting the camera model neural network to reconstruct the 3D point cloud. The proposed loss addressed the training imbalance problem by representing different errors of camera parameters in terms of a unified metric.

\subsection{Discussion}

\subsubsection{Technique Summary}
The above methods target automatic calibration without manual intervention and scene assumption. Early literature~\cite{DeepFocal, PoseNet} separately studied the intrinsic calibration or extrinsic calibration. Driven by large-scale datasets and powerful networks, subsequent works~\cite{DeepVP, DeepHorizon, Hold-Geoffroy, CTRL-C} considered a comprehensive camera calibration, inferring various parameters and geometric representations. To relieve the difficulty of learning the camera parameters, some works~\cite{UprightNet, StereoCaliNet, DirectionNet, DiffPoseNet} proposed to learn an intermediate representation. In recent literature, more applications are jointly studied with camera calibration~\cite{Zhu, SPEC, Baradad, Fang, Zheng}. This suggests solving the downstream vision tasks, especially in 3D tasks may require prior knowledge of the image formation model. Moreover, some geometric priors~\cite{DVPD} can alleviate the data-starved requirement of deep learning, showing the potential to bridge the gap between the calibration target and semantic features.  

It is interesting to find that increasing more extrinsic calibration methods~\cite{DeepFEPE, Do, SceneSqueezer} revisited and restored the traditional feature point-based solutions. The standard extrinsics that describe the camera motion contain limited degrees of freedom, and thus some local features can well represent the spatial correspondence. Besides, the network designed for point learning significantly improves the efficiency of calibration models, such as PointNet~\cite{qi2017pointnet} and PointCNN~\cite{li2018pointcnn}. Such a pipeline also enables clear interpretability of learning-based camera calibration, which promotes understanding of how the network calibrates and magnifies the influences of intermediate modules.

\subsubsection{Future Effort}

(1) Explore more vision/geometric priors. Due to the scarce real-world dataset in the learning-based camera calibration field, digging more priors that ease the demand of learning from data is promising. For example, the prior of the image formation model could allow us to associate the relationship between 3D camera parameters and 2D image layout. 

(2) Decouple different stages in an end-to-end calibration learning model. Most learning-based camera calibration methods include a feature extraction stage and an objective estimation stage. However, how the networks learn the features related to calibration is ambiguous. Therefore, decoupling the learning process by different traditional calibration stages can guide the way of feature extraction. It would be meaningful to extend the idea in extrinsic calibration~\cite{DeepFEPE, Do, SceneSqueezer} to more general calibration problems.

(3) Transfer the measurement space from the parameter error to the geometric difference. When it comes to jointly calibrating various camera parameters, the training process will suffer from an imbalance loss optimization problem. The main reason is different camera parameters correspond to different sample distributions. The simple normalization strategy cannot unify their error spaces. Therefore, we can formulate a straightforward measurement space in terms of the geometric property of different camera parameters.
