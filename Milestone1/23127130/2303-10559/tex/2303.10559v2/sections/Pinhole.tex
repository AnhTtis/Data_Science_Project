\section{Standard Model}
\label{sec:pure}
Generally, for learning-based calibration works, the objectives of the intrinsics calibration contain focal length and optical center, and the objectives of the extrinsic calibration contain the rotation matrix and translation vector.
\vspace{-0.2cm}
\subsection{Intrinsics Calibration}
Deepfocal \cite{DeepFocal} is a pioneer work in learning-based camera calibration, it aims to estimate the focal length of any image ``in the wild''. In detail, Deepfocal considered a simple pinhole camera model and regressed the horizontal field of view using a deep convolutional neural network. Given the width $w$ of an image, the relationship between the horizontal field of view $H_\theta$ and focal length $f$ can be described by:
\begin{equation}
H_\theta = 2\arctan(\frac{w}{2f}).
\label{eq-focal-length}
\end{equation}

Due to component wear, temperature fluctuations, or outside disturbances like collisions, the calibrated parameters of a camera are susceptible to change over time. To this end, MisCaliDet \cite{MisCaliDet} proposed to identify if a camera needs to be recalibrated intrinsically. Compared to the conventional intrinsic parameters such as the focal length and image center, MisCaliDet presented a new scalar metric, \textit{i.e.}, the average pixel position difference (APPD) to measure the degree of camera miscalibration, which describes the mean value of the pixel position differences over the entire image.

\subsection{Extrinsics Calibration}
In contrast to intrinsic calibration, extrinsic calibration infers the spatial correspondence of the camera and its located 3D scene. PoseNet\cite{PoseNet} first proposed deep convolutional neural networks to regress 6-DoF camera pose in real-time. A pose vector $\textbf{p}$ was predicted by PoseNet, given by the 3D position $\textbf{x}$ and orientation represented by quaternion $\textbf{q}$ of a camera, namely, $\textbf{p} = [\textbf{x}, \textbf{q}]$. For constructing the training dataset, the labels are automatically calculated from a video of the scenario using a structure from motion method \cite{wu2013towards}.

Inspired by PoseNet\cite{PoseNet}, the following works improved the extrinsic calibration in terms of the intermediate representation, interpretability, data format, learning objective, etc. For example, to optimize the geometric pose objective, DeepFEPE \cite{DeepFEPE} designed an end-to-end keypoint-based framework with learnable modules for detection, feature extraction, matching, and outlier rejection. Such a pipeline imitated the traditional baseline, in which the final performance can be analyzed and improved by the intermediate differentiable module. To bridge the domain gap between the extrinsic objective and image features, recent works proposed to first learn an intermediate representation from the input, such as surface geometry \cite{UprightNet}, depth map \cite{StereoCaliNet}, directional probability distribution \cite{DirectionNet}, and normal flow \cite{DiffPoseNet}, etc. Then, the extrinsic are reasoned by geometric constraints and learned representation. Therefore, the neural networks are gradually guided to perceive the geometry-related features, which are crucial for extrinsic estimation. Considering the privacy concerns and limited storage problem, some recent works compressed the scene and exploited the point-like feature to estimate the extrinsic. For example, Do et al. \cite{Do} trained a network to recognize sparse but significant 3D points, dubbed scene landmarks, by encoding their appearance as implicit features. And the camera pose can be calculated using a robust minimal solver followed by a Levenberg-Marquardt-based nonlinear refinement. SceneSqueezer \cite{SceneSqueezer} compressed the scene information from three levels: the database frames are clustered using pairwise co-visibility information, a point selection module prunes each cluster based on estimation performance, and learned quantization further compresses the selected points.

\subsection{Joint Intrinsic and Extrinsic Calibration}

\subsubsection{Geometric Representations}

\noindent \textbf{Vanishing Points}
The intersection of projections of a set of parallel lines in the world leads to a vanishing point. The detection of vanishing points is a fundamental and crucial challenge in 3D vision. In general, vanishing points reveal the direction of 3D lines, allowing the agent to deduce 3D scene information from a single 2D image.

DeepVP \cite{DeepVP} is the first learning-based work for detecting the vanishing points given a single image. It reversed the conventional process by scoring the horizon line candidates according to the vanishing points they contain. Chang et al. \cite{chang2018deepvp} redesigned this task as a CNN classification problem using an output layer with 225 discrete possible vanishing point locations. For constructing the dataset, the camera view is panned and tilted with step 5° from -35° to 35° in the panorama scene (total 225 images) from a single GPS location. To directly leverage the geometric properties of vanishing points, NeurVPS \cite{zhou2019neurvps} proposed a canonical conic space and a conic convolution operator that can be implemented as regular convolutions in this space, where the learning model is capable of calculating the global geometric information of vanishing points locally. To overcome the need for a large amount of training data in previous methods, DVPD \cite{DVPD} incorporated the neural network with two geometric priors: Hough transformation and Gaussian sphere. First, the convolutional features are transformed into a Hough domain, mapping lines to distinct bins. The projection of the Hough bins is then extended to the Gaussian sphere, where lines are transformed into great circles and vanishing points are located at the intersection of these circles. Geometric priors are data-efficient because they eliminate the necessity for learning this information from data, which enables an interpretable learning framework and generalizes better to domains with slightly different data distributions.

\noindent \textbf{Horizon Lines}
The horizon line is a crucial contextual attribute for various computer vision tasks especially image metrology, computational photography, and 3D scene understanding. The projection of the line at infinity onto any plane that is perpendicular to the local gravity vector determines the location of the horizon line.

Given the FoV, pitch, and roll of a camera, it is straightforward to locate the horizon line in its captured image space. DeepHorizon \cite{DeepHorizon} proposed the first learning-based solution for estimating the horizon line from an image, without requiring any explicit geometric constraints or other cues. To train the network, a new benchmark dataset, Horizon Lines in the Wild (HLW), was constructed, which consists of real-world images with labeled horizon lines. SA-MobileNet \cite{SA-MobileNet} proposed an image tilt detection and correction with self-attention MobileNet \cite{howard2019searching} for smartphones. A spatial self-attention module was devised to learn long-range dependencies and global context within the input images. To address the difficulty of the regression task, they trained the network to estimate multiple angles within a narrow interval of the ground truth tilt, penalizing only those values that locate outside this narrow range.
\vspace{-0.2cm}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/CTRL-C.pdf}
  %\vspace{-20pt}
  \caption{Overview of CTRL-C. The figure is from ~\cite{CTRL-C}. It estimates parameters including the zenith VP, FoV, and horizon line for camera calibration from an input image and a set of line segments. Moreover, two auxiliary outputs (vertical and horizontal convergence line scores) guide the network in learning scene geometry for calibration.}
  \label{fig:CTRL-C}
  \vspace{-0.3cm}
\end{figure}

\subsubsection{Composite Parameters}
Calibrating the composite parameters aims to estimate the intrinsic parameters and extrinsic parameters simultaneously.
By jointly estimating composite parameters and training using data from a large-scale panorama dataset \cite{SUN360}, Hold-Geoffroy\etal \cite{Hold-Geoffroy} largely outperformed previous independent calibration tasks. Moreover, Hold-Geoffroy\etal \cite{Hold-Geoffroy} performed human perception research in which the participants were asked to evaluate the realism of 3D objects composited with and without accurate calibration. This data was further designed to a new perceptual measure for the calibration errors. In terms of the feature category, Lee\etal \cite{Lee} and CTRL-C \cite{CTRL-C} considered both semantic features and geometric cues for camera calibration. They showed how taking use of geometric features, is capable of facilitating the network to comprehend the underlying perspective structure of an image. The pipeline of CTRL-C is illustrated in Figure~\ref{fig:CTRL-C}. In recent literature, more applications are jointly studied with camera calibration, for example, single view metrology \cite{Zhu}, 3D human pose and shape estimation \cite{SPEC}, depth estimation \cite{Baradad, Fang}, object pose estimation \cite{FocalPose}, and image reflection removal \cite{Zheng}, etc.

Considering the heterogeneousness and visual implicitness of different camera parameters, CPL \cite{CPL} estimated the parameters using a novel camera projection loss, exploiting the camera model neural network to reconstruct the 3D point cloud. The proposed loss addressed the training imbalance problem by representing different errors of camera parameters in terms of a unified metric.

\subsection{Discussion}

\subsubsection{Technique Summary}
The above methods target automatic calibration without manual intervention and scene assumption. Early literature~\cite{DeepFocal, PoseNet} separately studied the intrinsic calibration or extrinsic calibration. Driven by large-scale datasets and powerful networks, subsequent works~\cite{DeepVP, DeepHorizon, Hold-Geoffroy, CTRL-C} considered a comprehensive camera calibration, inferring various parameters and geometric representations. To relieve the difficulty of learning the camera parameters, some works~\cite{UprightNet, StereoCaliNet, DirectionNet, DiffPoseNet} proposed to learn an intermediate representation. In recent literature, more applications are jointly studied with camera calibration~\cite{Zhu, SPEC, Baradad, Fang, Zheng}. This suggests solving the downstream vision tasks, especially in 3D tasks may require prior knowledge of the image formation model. Moreover, some geometric priors~\cite{DVPD} can alleviate the data-starved requirement of deep learning, showing the potential to bridge the gap between the calibration target and semantic features.  

It is interesting to find that increasing more extrinsic calibration methods~\cite{DeepFEPE, Do, SceneSqueezer} revisited and restored the traditional feature point-based solutions. The standard extrinsics that describe the camera motion contain limited degrees of freedom, and thus some local features can well represent the spatial correspondence. Besides, the network designed for point learning significantly improves the efficiency of calibration models, such as PointNet~\cite{qi2017pointnet} and PointCNN~\cite{li2018pointcnn}. Such a pipeline also enables clear interpretability of learning-based camera calibration, which promotes understanding of how the network calibrates and magnifies the influences of intermediate modules.

NeRF, recognized for its groundbreaking capability in synthesizing novel views from 2D images, has seen various advancements recently. Progress in this arena includes the incorporation of additional trainable components, prior constraints, enhanced network designs, and novel training strategies. Specifically, NeRF$--$~\cite{wang2021nerf} marked a pivotal moment, demonstrating the simultaneous optimization of camera parameters and poses during training. It introduced a trainable pinhole camera model, followed by SCNeRF~\cite{jeong2021self} which proposed a more complex camera model, comprising of a pinhole design, radial distortion, and a pixel-specific noise model. While increased trainable parameters offer enhanced representational capabilities, they also complicate training. To counteract this, researchers have utilized prior geometric knowledge as constraints to stabilize optimization, integrating depth estimation~\cite{truong2023sparf, wang2023altnerf}, multi-view correspondence~\cite{jeong2021self,bian2023nope, wang2023altnerf}, and GAN-based constraints~\cite{meng2021gnerf}. Additionally, refinements to NeRF's architecture have emerged, leveraging Gaussian~\cite{chng2022gaussian} or sinusoidal activations~\cite{xia2022sinerf}. Concurrently, training strategies such as coarse-to-fine pipelines~\cite{lin2021barf,truong2023sparf} and advanced sampling techniques~\cite{lin2021barf,yen2020inerf, xia2022sinerf} have been proposed, enhancing both reconstruction quality and parameter precision.

Popular frameworks like InstantNGP~\cite{muller2022instant} and NeRFStudio~\cite{tancik2023nerfstudio} offer features to fine-tune camera parameters. Typically, NeRF methods use outputs from tools like COLMAP or Polycam, with rendering quality tied closely to initial data quality. Minor errors in intrinsics or poses can lead to noticeable rendering artifacts. Current approaches~\cite{muller2022instant, tancik2023nerfstudio} integrate intrinsic parameters, distortion coefficients, and camera poses into NeRF's training optimization, enhancing both geometric structure learning and parameter optimization. This boosts 3D geometry robustness and refines camera models. However, fine-tuning parameters may complicate optimization and can destabilize training. It may also not work well if there are not enough views to reliably adjust the intrinsics without overfitting.

\subsubsection{Future Effort}

(1) Explore more model priors. Most learning-based calibration methods study the parametric camera models but their generalization abilities are limited. In contrast, non-parametric models directly model the relationship between the 3D imaging ray and its resulting pixel in the image, encoding valuable priors in the learned semantic features to reason the camera parameters. Recent works~\cite{xian2023neural, zhu2023tame, jin2023perspective} also incorporate the perspective of modeling pixel-wise information for camera calibration, making minimal assumptions on the camera model and showing more interpretable and in line with how humans perceive.

(2) Decouple different stages in an end-to-end calibration learning model. Most learning-based camera calibration methods include a feature extraction stage and an objective estimation stage. However, how the networks learn the features related to calibration is ambiguous. Therefore, decoupling the learning process by different traditional calibration stages can guide the way of feature extraction. It would be meaningful to extend the idea in extrinsic calibration~\cite{DeepFEPE, Do, SceneSqueezer} to more general calibration problems.

(3) Transfer the measurement space from the parameter error to the geometric difference. When it comes to jointly calibrating various camera parameters, the training process will suffer from an imbalance loss optimization problem. The main reason is different camera parameters correspond to different sample distributions. The simple normalization strategy cannot unify their error spaces. Therefore, we can formulate a straightforward measurement space in terms of the geometric properties of different camera parameters.

(4) Despite the strides achieved by recent NeRF methods, training NeRF without precise camera parameters remains a challenge, especially in scenarios with sparse views, pronounced movements, low-texture regions, and suboptimal initial values. Contemporary NeRF-based methodologies do optimize camera parameters, yielding notable results. However, they exhibit significant computational demands and lack the generalization seen in current deep-learning calibration techniques. We contend that in the present NeRF-based methods, camera parameters often play a secondary role. Thus, crafting effective calibration algorithms that capitalize on NeRF remains an arduous but promising endeavor.