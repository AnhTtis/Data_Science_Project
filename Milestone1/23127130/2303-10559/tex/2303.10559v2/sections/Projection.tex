\section{Cross-View Model}
\label{sec:projection}

 The existing deep calibration methods can estimate the specific camera parameters from a single camera. In fact, there can be more complicated parameter representations in multi-camera circumstances. For example, in the multi-view model, the fundamental matrix and essential matrix describe the epipolar geometry and they are intricately tangled with intrinsics and extrinsics. The homography depicts the pixel-level correspondences between different views. In addition to intrinsics and extrinsics, it is also intertwined with depth. Among these complex parameter representations, homography is the most widely leveraged in practical applications and its related learning-based methods are the most investigated. To this end, we mainly focus on the review of deep homography estimation solutions for the cross-view model and they can be divided into three categories: direct, cascaded, and iterative solution.
\vspace{-0.3cm}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/UDHN.png}
  %\vspace{-20pt}
  \caption{Architectures of DHN~\cite{DHN} and UDHN~\cite{UDHN}. The figure is from ~\cite{UDHN}. The supervised approach~\cite{DHN} learns to regress a 4 point parameterization of homography $\mathbf{\tilde{H}}_{4pt}$ using $\mathcal{L}_2$ loss. The unsupervised approach~\cite{UDHN}  outputs $\mathbf{\tilde{H}}_{4pt}$ that minimizes the $\mathcal{L}_1$ pixel-wise photometric loss of paired inputs (DLT: direct linear transform; PSGG: parameterized sampling grid generator; DS: differentiable sampling).}
  \label{fig:UDHN}
  \vspace{-0.3cm}
\end{figure}

\subsection{Direct Solution}
\label{subsec:Direct}
We review the direct deep homography solutions from the perspective of different parameterizations, including the classical 4-pt parameterization and other parameterizations.
\subsubsection{4-pt Parameterization}
\label{subsec:4pt_parameterization}

%by randomly perturbing four corners of an image patch
Deep homography estimation is first proposed in DHN\cite{DHN}, where a VGG-style network is adopted to predict the 4-pt parameterization $H_{4pt}$. To train and evaluate the network, a synthetic dataset named Warped MS-COCO is created to provide ground truth 4-pt parameterization $\hat{H}_{4pt}$. The pipeline is illustrated in Fig. \ref{fig:UDHN}(a), and the objective function is formulated as $L_{H}$:
\begin{equation}
    L_{H} = \frac{1}{2}\parallel H_{4pt}-\hat{H}_{4pt}\parallel_2^2.
    \label{super_homo}
 \end{equation}
Then the 4-pt parameterization can be solved as a $3\times 3$ homography matrix using normalized DLT\cite{hartley2003multiple}. However, DHN is limited to synthetic datasets where the ground truth can be generated for free or requires costly labeling of real-world datasets. Subsequently, the first unsupervised solution named UDHN\cite{UDHN} is proposed to address this problem. As shown in Fig. \ref{fig:UDHN}(c), it used the same network architecture as DHN and defined an unsupervised loss function by minimizing the average photometric error motivated by traditional methods\cite{lucas1981iterative}:

 \begin{equation}
    L_{PW} = \parallel\mathcal{P}(I_A(x))-\mathcal{P}(I_B(\mathcal{W}(x;p)))\parallel_1,
    \label{unsuper_homo}
 \end{equation}
where $\mathcal{W}(\cdot;\cdot)$ and $\mathcal{P}(\cdot)$ denote the operations of warping via homography parameters $p$ and extracting an image patch, respectively. $I_A$ and $I_B$ are the original images with overlapping regions.
The input of UDHN is a pair of image patches, but it warps the original images when calculating the loss. In this manner, it avoids the adverse effects of invalid pixels after warping and lifts the magnitude of pixel supervision. To gain accuracy and speed with a tiny model, Chen et al. proposed ShuffleHomoNet~\cite{ShuffleHomoNet}, which integrates ShuffleNet compressed units\cite{ma2018shufflenet} and location-aware pooling\cite{Poursaeed} into a lightweight model. To further handle large displacement, a multi-scale weight-sharing version is exploited by extracting multi-scale feature representations and adaptively fusing multi-scale predictions. However, the homography cannot perfectly align images with parallax caused by non-planar structures with non-overlapping camera centers. To deal with parallax, CA-UDHN\cite{CA-UDHN} designs learnable attention masks to overlook the parallax regions, contributing to better background plane alignment. Besides, the 4-pt homography can be extended to meshflow\cite{Liu} to realize non-planar accurate alignment. 
\vspace{-0.1cm}
\subsubsection{Other Parameterizations}
\label{subsec:other_parameterization}
In addition to 4-pt parameterization, the homography can be parameterized as other formulations. To better utilize homography invertibility, Wang et al. proposed SSR-Net~\cite{SSR-Net}. They established the invertibility constraint through a conventional matrix representation in a cyclic manner.
Zeng et al. \cite{PFNet} argued that the 4-point parameterization regressed by a fully-connected layer can harm the spatial order of the corners and be susceptible to perturbations, since four points are the minimum requirement to solve the homography. To address these issues, they formulated the parameterization as a perspective field (PF) that models pixel-to-pixel bijection and designed a PFNet. This extends the displacements of the four vertices to as many dense pixel points as possible. The homography can then be solved using RANSAC \cite{fischler1981random} with outlier filtering, enabling robust estimation by utilizing dense correspondences. Nevertheless, dense correspondences lead to a significant increase in the computational complexity of RANSAC. Furthermore, Ye et al.\cite{BasesHomo} proposed an 8-DOF flow representation without extra post-processing, which has a size of $H\times W \times 2$ in an 8D subspace constrained by the homography. To represent arbitrary homography flows in this subspace, 8 flow bases are defined, and the proposed BasesHomo is to predict the coefficients for the flow bases. To obtain desirable bases, BasesHomo first generates 8 homography flows by modifying every single entry of an identity homography matrix except for the last entry. Then, these flows are normalized by their largest flow magnitude followed by a QR decomposition, enforcing all the bases normalized and orthogonal.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/HomoGAN.png}
  %\vspace{-20pt}
  \caption{Architecture of HomoGAN. The figure is from ~\cite{HomoGAN}. In particular, the homography estimation transformer with cascaded encoder-decoder blocks takes a feature pyramid of each image as inputs, and predicts the homography from coarse to fine. Coplanarity-aware GAN imposes coplanarity constraints on the model by predicting soft masks of the dominant plane.}
  \label{fig:HomoGAN}
  \vspace{-0.3cm}
\end{figure}
\vspace{-0.2cm}
\subsection{Cascaded Solution}
\label{subsec:Cascaded}
Direct solutions explore various homography parameterizations with simple network structures, while the cascaded ones focus on complex designs of network architectures.

In HierarchicalNet\cite{HierarchicalNet}, Nowruzi et al. hold that the warped images can be regarded as the input of another network. Therefore they stacked the networks sequentially to reduce the error bounds of the estimate. Based on HierarchicalNet, SRHEN~\cite{SRHEN} introduced the cost volume\cite{Sun_2018_CVPR} to the cascaded network, measuring the feature correlation by cosine distance and formulating it as a volume. The stacked networks and cost volume increase the performance, but they cannot handle the dynamic scenes. MHN~\cite{MHN} developed a multi-scale neural network and proposed to learn homography estimation and dynamic content detection simultaneously. Moreover, to tackle the cross-resolution problem, LocalTrans \cite{LocalTrans} formulated it as a multimodal problem and proposed a local transformer network embedded within a multiscale structure to explicitly learn correspondences between the multimodal inputs. These inputs include images with different resolutions, and LocalTrans achieved superior performance on cross-resolution cases with a resolution gap of up to 10x. All the solutions mentioned above leverage image pyramids to progressively enhance the ability to address large displacements. However, every image pair at each level requires a unique feature extraction network, resulting in the redundancy of feature maps. To alleviate this problem, some researchers\cite{nie2022learning, UDIS, DAMG-Homo, HomoGAN} replaced image pyramids with feature pyramids. Specifically, they warped the feature maps directly instead of images to avoid excessive feature extraction networks. To address the low-overlap homography estimation problem in real-world images\cite{UDIS}, Nie et al.\cite{UDIS} modified the unsupervised constraint (Eq. \ref{unsuper_homo}) to adapt to low-overlap scenes:
\begin{equation}
    L'_{PW} = \parallel I_A(x)\cdot\mathbbm{1}(\mathcal{W}(x;p))-I_B(\mathcal{W}(x;p))\parallel_1,
    %\label{unsuper_homo}
 \end{equation}
where $\mathbbm{1}$ is an all-one matrix with the same size as $I_A$ or $I_B$. It solved the low-overlap problem by taking the original images as network input and ablating the corresponding pixels of $I_A$ to the invalid pixels of warped $I_B$. 
To solve the non-planar homography estimation problem, DAMG-Homo\cite{DAMG-Homo} proposed backward multi-gird deformation with contextual correlation to align parallax images. Compared with traditional cost volume, the proposed contextual correlation helped to reach better accuracy with lower computational complexity. Another way to address the non-planar problem is to focus on the dominant plane. In HomoGAN \cite{HomoGAN}, an unsupervised GAN is proposed to impose a coplanarity constraint on the predicted homography, as shown in Figure \ref{fig:HomoGAN}. To implement this approach, a generator is used to predict masks of aligned regions, while a discriminator is used to determine whether two masked feature maps were produced by a single homography.
\vspace{-0.2cm}
\subsection{Iterative Solution}
\label{subsec:Iterative}
Compared with cascaded methods, iterative solutions achieve higher accuracy by iteratively optimizing the last estimation. Lucas-Kanade (LK) algorithm\cite{lucas1981iterative} is usually used in image registration to estimate the parameterized warps iteratively, such as affine transformation, optical flow, etc. It aims at the incremental update of warp parameters $\varDelta p$ every iteration by minimizing the sum of squared error between a template image $T$ and an input image $I$:
\begin{equation}
    E(\varDelta p) = \parallel T(x)-I(\mathcal{W}(x;p+\varDelta p))\parallel_2^2.
    \label{lk}
 \end{equation}
However, when optimizing Eq. \ref{lk} using first-order Taylor expansion, $\partial I(\mathcal{W}(x;p))/\partial p$ should be recomputed every iteration because $I(\mathcal{W}(x;p))$ varies with $p$. To avoid this problem, the inverse compositional (IC) LK algorithm\cite{baker2004lucas}, an equivalence to LK algorithm, can be used to reformulate the optimization goal as follows:
\begin{equation}
    E'(\varDelta p) = \parallel T(\mathcal{W}(x;\varDelta p))-I(\mathcal{W}(x;p))\parallel_2^2.
    \label{ic-lk}
 \end{equation}
After linearizing Eq. \ref{ic-lk} with first-order Taylor expansion, we compute $\partial T(\mathcal{W}(x;0))/\partial p$ instead of $\partial I(\mathcal{W}(x;p))/\partial p$, which would not vary every iteration.

To combine the advantages of deep learning with IC-LK iterator, CLKN~\cite{CLKN} conducted LK iterative optimization on semantic feature maps extracted by CNNs as follows:
 \begin{equation}
    E^{f}(\varDelta p) = \parallel F_T(\mathcal{W}(x;\varDelta p))-F_I(\mathcal{W}(x;p))\parallel_2^2,
    \label{feature-ic-lk}
 \end{equation}
where $F_T$ and $F_I$ are the feature maps of the template and input images. Then, they enforced the network to run a single iteration with a hinge loss, while the network runs multiple iterations until the stopping condition is met in the testing stage. Besides, CLKN stacked three similar LK networks to further boost the performance by treating the output of the last LK network as the initial warp parameters of the next LK network. From Eq. \ref{feature-ic-lk}, the IC-LK algorithm heavily relied on feature maps, which tend to fail in multimodal images. Instead, DLKFM \cite{DLKFM} constructed a single-channel feature map by using the eigenvalues of the local covariance matrix on the output tensor. To learn DLKFM, it designed two special constraint terms to align multimodal feature maps and contribute to convergence.

However, LK-based algorithms can fail if the Jacobian matrix is rank-deficient \cite{nocedal1999numerical}. Additionally, the IC-LK iterator is untrainable, which means this drawback is theoretically unavoidable. To address this issue, a completely trainable iterative homography network (IHN) \cite{IHN} was proposed. Inspired by RAFT \cite{teed2020raft}, IHN updates the cost volume to refine the estimated homography using the same estimator repeatedly every iteration. Furthermore, IHN can handle dynamic scenes by producing an inlier mask in the estimator without requiring extra supervision.
\vspace{-0.2cm}

\subsection{Discussion}
\label{subsec:discussion}
\subsubsection{Technique Summary}

The above works are devoted to exploring different homography parameterizations such as 4-pt parameterization\cite{DHN}, perspective field\cite{PFNet}, and motion bases representation\cite{BasesHomo}, which contributes to better convergence and performance.
Other works tend to design various network architectures. In particular, cascaded and iterative solutions are proposed to refine the performance progressively, which can be further combined together to reach higher accuracy. To make the methods more practical, various challenging problems are preliminarily addressed, such as cross resolutions\cite{LocalTrans}, multiple modalities\cite{DLKFM, IHN}, dynamic objects\cite{MHN, IHN}, and non-planar scenes\cite{CA-UDHN, HomoGAN, DAMG-Homo}, etc.

\subsubsection{Challenge and Future Effort}
We summarize the existing challenges as follows:

(1) Many homography estimation solutions are designed for fixed resolutions, while real-world applications often involve much more flexible resolutions. When pre-trained models are applied to images with different resolutions, performance can dramatically drop due to the need for input resizing to satisfy the regulated resolution.

(2) Unlike optical flow estimation, which assumes small motions between images, homography estimation often deals with images that have significantly low-overlap rates. In such cases, existing methods may exhibit inferior performance due to limited receptive fields.

(3) Existing methods address the parallax or dynamic objects by learning to reject outliers in the feature extractor\cite{CA-UDHN}, cost volume\cite{li2022ssorn}, or estimator\cite{IHN}. However, it is still unclear which stage is more appropriate for outlier rejection.

Based on the challenges we have discussed, some potential research directions for future efforts can be identified:

(1) To overcome the first challenge, we can design various strategies to enhance resolution robustness, such as resolution-related data augmentation, and continual learning on multiple datasets with different resolutions. Besides, we can also formulate a resolution-free parameterization form. The perspective field \cite{PFNet} is a typical case, which represents the homography as dense correspondences with the same resolution as input images. But it requires RANSAC as the post-processing approach, introducing extra computational complexity, especially in the case of extensive correspondences. Therefore, a resolution-free and efficient parameterization form should be explored.

(2) To enhance the performance in low-overlap rate, the main insight is to increase the receptive fields of a network. To this end, the cross-attention module of the transformer explicitly leverages the long-range correlation to eliminate short-range inductive bias\cite{vaswani2017attention}. On the other hand, we can exploit beneficial varieties of cost volume to integrate feature correlation \cite{DAMG-Homo, IHN}.

(3) As there is no interaction between different image features in the feature extractor, it is reasonable to assume that outlier rejection should occur after feature extraction. It is not possible to identify outliers within a single image as the depth alone cannot be used as an outlier cue. For example, images captured by purely rotated cameras do not contain parallax outliers. Additionally, it seems intuitive to learn the capability of outlier rejection by combining global and local correlation, similar to the insight of RANSAC.
