\section{Distortion Model}
\label{sec:distortion}
In the learning-based camera calibration, calibrating the radial distortion and roll shutter distortion gains increasing attention due to their widely used applications for the wide-angle lens and CMOS sensor. In this part, we mainly review the calibration/rectification of these two distortions.
\vspace{-0.4cm}

\subsection{Radial Distortion}
The literature on learning-based radial distortion calibration can be classified into two main categories: regression-based solutions and reconstruction-based solutions.
\vspace{-0.2cm}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.45\textwidth]{figures/DeepCalib.pdf}
  %\vspace{-20pt}
  \caption{Three common learning solutions of the regression-based wide-angle camera calibration: (a) SingleNet, (b) DualNet, (c) SeqNet, where $\mathbf{I}$ is the distortion image and $f$ and $\xi$ denote the focal length and distortion parameters, respectively. The figure is from ~\cite{DeepCalib}.}
  \label{fig:DeepCalib}
  \vspace{-0.3cm}
\end{figure}

\subsubsection{Regression-based Solution}
Rong \etal~\cite{Rong} and DeepCalib~\cite{DeepCalib} are pioneer works for the learning-based wide-angle camera calibration. They treated the camera calibration as a supervised classification~\cite{Rong} or regression~\cite{DeepCalib} problem, and then the networks with the convolutional layers and fully connected layers were used to learn the distortion features of inputs and predict the camera parameters. In particular, DeepCalib~\cite{DeepCalib} explored three learning solutions for wide-angle camera calibration as illustrated in Figure~\ref{fig:DeepCalib}. Their experiments showed the simplest architecture SingleNet achieves the best performance on both accuracy and efficiency. To enhance the distortion perception of networks, the following works investigated introducing more diverse features such as the semantic features~\cite{FishEyeRecNet} and geometry features~\cite{Xue, LaRecNet, RDCFace}. Additionally, some works improved the generalization by designing learning strategies such as unsupervised learning~\cite{UnFishCor}, self-supervised learning~\cite{SIR}, and reinforcement learning~\cite{Zhao}. By randomly chosen coefficients throughout each mini-batch of the training process, RDC-Net~\cite{RDC-Net} was able to dynamically generate distortion images on-the-fly. It enhanced the rectification performance and prevents the learning model from overfitting. Instead of contributing to the techniques of deep learning, other works leaned to explore the vision prior to interpretable calibration. For example, having observed the radial distortion image owns the center symmetry characteristics, in which the texture far from the image center has stronger distortion, Shi\etal~\cite{Shi} and PSE-GAN~\cite{PSE-GAN} developed a position-aware weight layer (fixed~\cite{Shi} and learnable~\cite{PSE-GAN}) of this property and enabled the network to explicitly perceive the distortion. Lopez\etal~\cite{Lopez} proposed a novel parameterization for radial distortion that is better suited for networks than directly learning the distortion parameters. Furthermore, OrdinalDistortion~\cite{OrdianlDistortion} presented a learning-friendly representation, \textit{i.e.}, ordinal distortion. Compared to the implicit and heterogeneous camera parameters, such a representation can facilitate the distortion perception of the neural network due to its clear relation to the image features.

\subsubsection{Reconstruction-based Solution}
Inspired by the conditional image-to-image translation and dense visual perception, the reconstruction-based solution starts to evolve from the conventional regression-based paradigm. DR-GAN~\cite{DR-GAN} is the first reconstruction-based solution for calibrating the radial distortion, which directly models the pixel-wise mapping between the distorted image and rectified image. It achieved the camera parameter-free training and one-stage rectification. Thanks to the liberation of the assumption of camera models, the reconstruction-based solution showed the potential to calibrate various types of cameras in one learning network. For example, DDM~\cite{DDM} unified different camera models into a domain by presenting the distortion distribution map, which explicitly describes the distortion level of each pixel in a distorted image. Then, the network learned to reconstruct the rectified image using this geometric prior map. To make the mapping function interpretable, the subsequent works \cite{STD, BlindCor, Zhao, FE-GAN, PCN, Tan, SS-WPC, PolarRecNet} developed the displacement filed between the distorted image and rectified image. Such a manner is able to eliminate the generated artifacts in the pixel-level reconstruction. In particular, FE-GAN~\cite{FE-GAN} integrated the geometry prior like Shi\etal~\cite{Shi} and PSE-GAN~\cite{PSE-GAN} into their reconstruction-based solution and presented a self-supervised strategy to learn the distortion flow for wide-angle camera calibration in Figure~\ref{fig:FE-GAN}. Most reconstruction-based solutions exploit a U-Net-like architecture to learn pixel-level mapping. However, the distortion feature can be transferred from encoder to decoder by the skip-connection operation, leading to a blurring appearance and incomplete correction in reconstruction results. To address this issue, Li\etal~\cite{Li} abandoned the skip-connection in their rectification network. To keep the feature fusion and restrain the geometric difference simultaneously, PCN~\cite{PCN} designed a correction layer in skip-connection and applied the appearance flows to revise the convolved features in different encoder layers. Having noticed that the previous sampling strategy of the convolution kernel neglected the radial symmetry of distortion, PolarRecNet~\cite{PolarRecNet} transformed the distorted image from the Cartesian coordinates domain into the polar coordinates domain. 
\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/FE-GAN.pdf}
  %\vspace{-20pt}
  \caption{Architecture of FE-GAN. The figure is from ~\cite{FE-GAN}. It consists of two components: a generator $G = (U, W)$ that rectifies the distortion image $x$, and a discriminator $D = (D_{adv}, D_{cls})$. The module $U$ in $G$ predicts the distortion flow $f = U(x)$, while $W$ rectifies the distortion image using $f$.}
  \label{fig:FE-GAN}
  \vspace{-0.3cm}
\end{figure}

\subsection{Roll Shutter Distortion}
The existing deep learning calibration works on roll shutter (RS) distortion can be classified into two categories: single-frame-based~\cite{URS-CNN, RSC-Net, EvUnroll} and multi-frame-based~\cite{DeepUnrollNet, JCD, SUNet, fan2021inverting, AW-RSC}. The single-frame-based solution studies the case of a single roll shutter image as input and directly learns to correct the distortion using neural networks. The ideal corrected result can be regarded as the global shutter (GS) image. It is an ill-posed problem and requires some additional prior assumptions to be defined. On the contrary, the multi-frame-based solution considers the consecutive frames (two or more) of a video taken by a roll shutter camera, in which the strong temporal correlation can be investigated for more reasonable correction. 
\vspace{-0.2cm}

\subsubsection{Single-frame-based Solution}
URS-CNN~\cite{URS-CNN} is the first learning work for calibrating the rolling shutter camera. In this work, a neural network with long kernel characteristics was used to understand how the scene structure and row-wise camera motion interact. To specifically address the nature of the RS effect produced by the row-wise exposure, the row-kernel and column-kernel convolutions were leveraged to extract attributes along horizontal and vertical axes. RSC-Net~\cite{RSC-Net} improved URS-CNN~\cite{URS-CNN} from 2 degrees of freedom (DoF) to 6-DoF and presents a structure-and-motion-aware RS correction model, where the camera scanline velocity and depth were estimated. Compared to URS-CNN~\cite{URS-CNN}, RSC-Net~\cite{RSC-Net} further reasoned about the concealed motion between the scanlines as well as the scene structure as shown in Figure~\ref{fig:RSC-Net}. To bridge the spatiotemporal connection between RS and GS, EvUnroll~\cite{EvUnroll} exploited the neuromorphic events to correct the RS effect. Event cameras can overcome a number of drawbacks of conventional frame-based activities for dynamic situations with quick motion due to their high temporal resolution property with microsecond-level sensitivity.
\vspace{-0.2cm}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/RSC-Net.pdf}
  %\vspace{-20pt}
  \caption{Architecture of RSC-Net. The figure is from ~\cite{RSC-Net}. It consists of two sub-networks, namely DepthNet and Velocity-Net, for learning an RS depth map and RS camera motion from an input image, respectively. Among them, a 6-DOF camera velocity is regressed, including a 3D translational velocity vector $v$ and 3D angular velocity vector $w$.}
  \label{fig:RSC-Net}
  \vspace{-0.06cm}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/AW-RSC.pdf}
  %\vspace{-20pt}
  \caption{Architecture of AW-RSC. The figure is from ~\cite{AW-RSC}. To address current imprecise motion estimation, it attempts to predict multiple displacement fields instead of only one. Additionally, AW-RSC suggests an adaptive warping module that uses the bundle of fields to guide the adaptive warping of the RS features into the GS one.}
  \label{fig:AW-RSC}
  \vspace{-0.3cm}
\end{figure}

\subsubsection{Multi-frame-based Solution}
Most multi-frame-based solutions are based on the reconstruction paradigm, they mainly devote to contributing how to represent the dense displacement field between RS and global GS images and accurately warp the RS domain to the GS domain. For the first time, DeepUnrollNet~\cite{DeepUnrollNet} proposed an end-to-end network for two consecutive rolling shutter images using a differentiable forward warping module. In this method, a motion estimation network is used to estimate the dense displacement field from a rolling shutter image to its matching global shutter image. The second contribution of DeepUnrollNet~\cite{DeepUnrollNet} is to construct two novel datasets: the Fastec-RS dataset and the Carla-RS dataset. Furthermore, JCD~\cite{JCD} jointly considered the rolling shutter correction and deblurring (RSCD) techniques, which largely exist in the medium and long exposure cases of rolling shutter cameras. It applied bi-directional warping streams to compensate for the displacement while keeping the non-warped deblurring stream to restore details. The authors also contributed a real-world dataset using a well-designed beam-splitter acquisition system, BS-RSCD, which includes both ego-motion and object motion in dynamic scenes. SUNet~\cite{SUNet} extended DeepUnrollNet~\cite{DeepUnrollNet} from the middle time of the second frame ($\frac{3\tau}{2}$) into the intermediate time of two frames ($\tau$). By using PWC-Net~\cite{Sun_2018_CVPR}, SUNet~\cite{SUNet} estimated the symmetric undistortion fields and reconstructed the potential GS frames by a time-centered GS image decoder network. To effectively reduce the misalignment between the contexts warped from two consecutive RS images, the context-aware undistortion flow estimator and the symmetric consistency enforcement were designed. To achieve a higher frame rate, Fan\etal~\cite{fan2021inverting} generated a GS video from two consecutive RS images based on the scanline-dependent nature of the RS camera. In particular, they first analyzed the inherent connection between bidirectional RS undistortion flow and optical flow, demonstrating the RS undistortion flow map has a more pronounced scanline dependency than the isotropically smooth optical flow map. Then, they developed the bidirectional undistortion flows to describe the pixel-wise RS-aware displacement, and further devised a computation technique for the mutual conversion between different RS undistortion flows corresponding to various scanlines. To eliminate the inaccurate displacement field estimation and error-prone warping problems in previous methods, AW-RSC ~\cite{AW-RSC} proposed to predict multiple fields and adaptively warped the learned RS features into global shutter counterparts. Using a coarse-to-fine approach, these warped features were combined and generated to precise global shutter frames as shown in Figure~\ref{fig:AW-RSC}. Compared to previous works~\cite{DeepUnrollNet, JCD, SUNet, fan2021inverting}, the warping operation consisting of adaptive multi-head attention and a convolutional block in AW-RSC ~\cite{AW-RSC} is learnable and effective. In addition, AW-RSC ~\cite{AW-RSC} contributed a real-world rolling shutter correction dataset: BS-RSC, where the RS videos with corresponding GS ground truth are captured simultaneously with a beam-splitter-based acquisition system.
\vspace{-0.2cm}
\subsection{Discussion}

\subsubsection{Technique Summary}
The deep learning works on wide-angle camera and roll shutter calibration share a similar technique pipeline. Along this research trend, most early literature begins with the regression-based solution~\cite{Rong, DeepCalib, URS-CNN}. The subsequent works innovated the traditional calibration with a reconstruction perspective~\cite{DR-GAN, DDM, FE-GAN, DeepUnrollNet}, which directly learns the displacement field to rectify the uncalibrated input. For higher accuracy of calibration, a more intuitive displacement field, and more effective warping strategy have been developed~\cite{PCN, AW-RSC, JCD, fan2021inverting}. To fit the distribution of different distortions, some works designed different shapes of the convolutional kernel~\cite{URS-CNN} or transformed the convolved coordinates~\cite{PolarRecNet}.

Existing works devoted themselves to designing more powerful networks and introducing more diverse features to facilitate calibration performance. Increasingly more methods focused on the geometry priors of the distortion~\cite{FE-GAN, PSE-GAN, Shi}. These priors can be directly weighted into the convolutional layers or used to supervise network training, promoting the learning model to converge faster.
\vspace{-0.2cm}

\subsubsection{Future Effort}

(1) The development of wide-angle camera calibration and roll shutter camera calibration can promote each other. For instance, the well-studied multi-frame-based solution in roll shutter calibration is able to inspire wide-angle calibration. The same object located at different sequences could provide useful priors regarding to radial distortion. Additionally, the elaborate studies of the displacement field and warping layer~\cite{AW-RSC, JCD, fan2021inverting} have the potential to motivate the development of wide-angle camera calibration and other fields. Furthermore, the investigation of geometric priors in wide-angle calibration could also improve the interpretability of the network in roll shutter calibration.

(2) Most methods synthesize their training dataset based on random samples from all camera parameters. However, for the images captured by real lenses, the distribution of camera parameters probably locates at a potential manifold \cite{Lopez}. Learning on a label-redundant calibration dataset makes the training process inefficient. Thus, exploring a practical sampling strategy for the synthesized dataset could be a meaningful task in the future direction.

(3) To overcome the ill-posed problem of single-frame calibration, introducing other high-precision sensors can compensate for the current calibration performance, such as event cameras~\cite {EvUnroll}. With the rapid development of vision sensors, joint calibration using multiple sensors is valuable. Consequently, more cross-modal and multi-modal fusion techniques will be investigated along this research way. 
\vspace{-0.3cm}

