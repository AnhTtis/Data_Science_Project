\section{Benchmark}
\label{sec:evaluation}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{figures/benchmark.pdf}
  %\vspace{-20pt}
  \caption{Overview of our collected benchmark, which covers all models reviewed in this paper. In this dataset, the image and video derive from diverse cameras under different environments. The accurate ground truth and label are provided for each sample.}
  \label{fig:benchmark}
  \vspace{-0.3cm}
\end{figure*}

As there is no public and unified benchmark in learning-based camera calibration, we contribute a dataset that can serve as a platform for generalization evaluations. In this dataset, the images and videos are captured by different cameras under diverse scenes, including simulation environments and real-world settings. Additionally, we provide the calibration ground truth, parameter label, and visual clues in this dataset based on different conditions. Figure~\ref{fig:benchmark} shows some samples of our collected dataset. Please refer to the evaluation of representative calibration methods on this benchmark in supplementary material.

\textbf{Standard Model}. We collected 300 high-resolution images on the Internet, captured by popular digital cameras such as Canon, Fujifilm, Nikon, Olympus, Sigma, Sony, etc. For each image, we provide the specific focal length of its lens. We have included a diverse range of subjects, including landscapes, portraits, wildlife, architecture, etc. The range of focal length is from 4.5mm to 600mm.
  
\textbf{Distortion Model}. We created a comprehensive dataset for the distortion camera model, with a focus on wide-angle cameras. The dataset is comprised of three subcategories. The first is a synthetic dataset, which was generated using the widely-used 4$^{th}$ order polynomial model. It contains both circular and rectangular structures, with 1,000 distortion-rectification image pairs. The second subcategory consists of data captured under real-world settings, derived from the raw calibration data for around 40 types of wide-angle cameras. For each calibration data, the intrinsics, extrinsics, and distortion coefficients are available. Finally, we exploit a car equipped with different cameras to capture video sequences. The scenes cover both indoor and outdoor environments, including daytime and nighttime footage.
  
\textbf{Cross-View Model}. We selected 500 testing samples at random from each of four representative datasets (MS-COCO~\cite{DHN}, GoogleEarch~\cite{DLKFM}, GoogleMap~\cite{DLKFM}, CAHomo~\cite{CA-UDHN}) to create a dataset for the cross-view model. It covers a range of scenarios: MS-COCO provides natural synthetic data, GoogleEarch contains aerial synthetic data, and GoogleMap offers multi-modal synthetic data. Parallax is not a factor in these three datasets, while CAHomo provides real-world data with non-planar scenes. To standardize the dataset, we converted all images to a unified format and recorded the matched points between two views. In MS-COCO, GoogleEarch, and GoogleMap, we used four vertices of the images as the matched points. In CAHomo, we identified six matched key points within the same plane.

\textbf{Cross-Sensor Model}. We collected RGB and point cloud data from Apollo~\cite{huang2019apolloscape}, DAIR-V2X~\cite{yu2022dair}, KITTI~\cite{KITTI}, KUCL~\cite{kang-2020-jfr}, NuScenes~\cite{caesar2020nuscenes}, and ONCE~\cite{mao2021one}. Around 300 data pairs with calibration parameters are included in each category. The datasets are captured in different countries to provide enough variety. Each dataset has a different sensor setup, obtaining camera-LiDAR data with varying image resolution, LiDAR scan pattern, and camera-LiDAR relative location. The image resolution ranges from 2448$\times$2048 to 1242$\times$375, while the LiDAR sensors are from Velodyne and Hesai, with 16, 32, 40, 64, and 128 beams. They include not only normal surrounding multi-view images but also small baseline multi-view data. Additionally, we also added random disturbance of around 20 degrees rotation and 1.5 meters translation based on classical settings~\cite{schneider2017regnet} to simulate vibration and collision.




