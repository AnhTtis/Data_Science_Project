\section{Preliminaries}
\label{sec2}
Deep learning has brought new inspirations to camera calibration, enabling a fully automatic calibration procedure without manual intervention. Here, we first summarize two prevalent paradigms in learning-based camera calibration: regression-based calibration and reconstruction-based calibration. Then, the widely-used learning strategies are reviewed in this research field. The detailed definitions for classical camera models and their corresponding calibration objectives are exhibited in the supplementary material.

\subsection{Learning Paradigm}
Driven by different architectures of the neural network, the researchers have developed two main paradigms for learning-based camera calibration and its applications.

\noindent \textbf{Regression-based Calibration}
Given an uncalibrated input, the regression-based calibration extracts the high-level semantic features using stacked convolutional layers. Then, the fully connected layers aggregate the semantic features and form a vector of the estimated calibration objective. The regressed parameters are used to conduct subsequent tasks such as distortion rectification, image warping, camera localization, etc. This paradigm is the earliest and has a dominant role in learning-based camera calibration. All the first works in various objectives, \textit{e.g.}, intrinsics: Deepfocal \cite{DeepFocal}, extrinsic: PoseNet \cite{PoseNet}, radial distortion: Rong et al. \cite{Rong}, rolling shutter distortion: URS-CNN \cite{URS-CNN}, homography: DHN \cite{DHN}, hybrid parameters: Hold-Geoffroy et al. \cite{Hold-Geoffroy}, camera-LiDAR parameters: RegNet \cite{schneider2017regnet} have been achieved with this paradigm.

\noindent \textbf{Reconstruction-based Calibration}
On the other hand, the reconstruction-based calibration paradigm discards the parameter regression and directly learns the pixel-level mapping function between the uncalibrated input and target, inspired by the conditional image-to-image translation \cite{pix2pix} and dense visual perception\cite{long2015fully, eigen2014depth}. The reconstructed results are then calculated for the pixel-wise loss with the ground truth. In this regard, most reconstruction-based calibration methods \cite{DR-GAN, DDM, DaRecNet, BlindCor} design their network architecture based on the fully convolutional network such as U-Net\cite{ronneberger2015u}. Specifically, an encoder-decoder network, with skip connections between the encoder and decoder features at the same spatial resolution, progressively extracts the features from low-level to high-level and effectively integrates multi-scale features. At the last convolutional layer, the learned features are aggregated into the target channel, forming the calibrated result or calibration representation at the pixel level. Recent works also explore harnessing the powerful generation ability of the diffusion model to help reconstruct the calibration targets~\cite{CAR, DiffCalib, DM-Calib, RS-Diffusion}.

In contrast to the regression-based paradigm, the reconstruction-based paradigm does not require the label of diverse camera parameters during training. Besides, the imbalance loss problem can be eliminated since it only optimizes the photometric loss of calibrated results or calibration representations. Therefore, the reconstruction-based paradigm enables a blind camera calibration without a strong camera model assumption~\cite{camposeco2015non, schops2020having, pan2022camera}.

\subsection{Learning Strategies}
In the following, we review the learning-based camera calibration literature regarding different learning strategies.

\noindent \textbf{Supervised Learning}
Most learning-based camera calibration methods train their networks with the supervised learning strategy, from the classical methods \cite{DeepFocal, PoseNet, DHN, DeepVP, Rong, DeepCalib} to the state-of-the-art methods \cite{DVPD, EvUnroll, FishFormer, DAMG-Homo, SST-Calib, GeoCalib}. In terms of the learning paradigm, this strategy supervises the network with the ground truth of the camera parameters (regression-based paradigm) or paired data (reconstruction-based paradigm). In general, they synthesize the training dataset from other large-scale datasets, under the random parameter/transformation sampling and camera model simulation. Some recent works \cite{Zhao, Tan, SPEC, DeepUnrollNet} establish their training dataset using a real-world setup and label the captured images with manual annotations, thereby fostering advancements in this research domain.

\noindent \textbf{Semi-Supervised Learning}
Training the network using an annotated dataset under diverse scenarios is an effective learning strategy. However, human annotation can be prone to errors, leading to inconsistent annotation quality or the inclusion of contaminated data. Consequently, increasing the dataset to improve performance can be challenging due to the complexity and construction cost. To address this challenge, SS-WPC\cite{SS-WPC} proposes a semi-supervised method for correcting portraits captured by a wide-angle camera. It employs a surrogate segmentation task and a semi-supervised method that utilizes direction and range consistency and regression consistency to leverage both labeled and unlabeled data.

\noindent \textbf{Weakly-Supervised Learning}
Although significant progress has been made, data labeling for camera calibration is a notorious costly process, and obtaining perfect ground-truth labels is challenging. As a result, it is often preferable to use weak supervision with machine learning methods. Weakly supervised learning refers to the process of building prediction models through learning with inadequate supervision. Zhu et al. \cite{Zhu} present a weakly supervised camera calibration method for single-view metrology in unconstrained environments, where there is only one accessible image of a scene composed of objects of uncertain sizes. This work leverages 2D object annotations from large-scale datasets, where people and buildings are frequently present and serve as useful ``reference objects'' for determining 3D size.

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{figures/taxonomy_new.pdf}
  \caption{The structural and hierarchical taxonomy of camera calibration with deep learning. Some classical methods are listed under each category.}
  \label{fig:taxonomy}
  \vspace{-0.2cm}
\end{figure*}

\noindent \textbf{Unsupervised Learning}
Unsupervised learning analyzes and groups unlabeled datasets using machine learning algorithms. UDHN \cite{UDHN} is the first work for the cross-view camera model using unsupervised learning, which estimates the homography matrix of a paired image without labels. By reducing a pixel-wise error that does not require ground truth data, UDHN \cite{UDHN} outperforms previous supervised learning techniques. While preserving superior accuracy and robustness to light fluctuations, it can also achieve faster inference time. Inspired by this work, increasing more methods leverage the unsupervised learning strategy to estimate the cross-view mapping such as CA-UDHN \cite{CA-UDHN}, BaseHomo \cite{BasesHomo}, HomoGAN\cite{HomoGAN}, and Liu et al. \cite{Liu}. Besides, UnFishCor \cite{UnFishCor} frees the demands for distortion parameters and designs an unsupervised framework for the wide-angle camera.

\noindent \textbf{Self-supervised Learning}
Robotics is where the phrase ``self-supervised learning'' first appears, as training data is automatically categorized by utilizing relationships between various input sensor signals~\cite{rani2023self}. Compared to supervised learning, self-supervised learning leverages input data itself as the supervision. Many self-supervised techniques are presented to learn visual characteristics from massive amounts of unlabeled photos or videos without the need for time-consuming and expensive human annotations. SSR-Net \cite{SSR-Net} presents a self-supervised deep homography estimation network, which relaxes the need for ground truth annotations and leverages the invertibility constraints of homography. To be specific, SSR-Net \cite{SSR-Net} utilizes the homography matrix representation in place of other approaches' typically-used 4-point parameterization, to apply the invertibility constraints. SIR \cite{SIR} devises a brand-new self-supervised pipeline for wide-angle image rectification, based on the principle that the corrected results of distorted images of the same scene captured by various lenses, should obtain identical rectified results. Using self-supervised depth and pose learning as a proxy task, Fang et al. \cite{Fang} propose a method for self-calibrating a range of generic camera models from raw video, providing the first calibration evaluation of camera model parameters learned entirely through self-supervision.

\noindent \textbf{Reinforcement Learning}
Instead of aiming to minimize at each stage, reinforcement learning can maximize the cumulative benefits of a learning process as a whole. To date, DQN-RecNet~\cite{DQN-RecNet} is the first work in camera calibration using reinforcement learning. It applies a deep reinforcement learning technique to tackle the fisheye image rectification by a single Markov Decision Process, which is a multi-step gradual calibration scheme. In this situation, the current fisheye image represents the state of the environment. The agent, Deep Q-Network \cite{mnih2015human}, generates an action that should be executed to correct the distorted image.

In the following, we will review the specific methods and literature for learning-based camera calibration. The structural and hierarchical taxonomy is shown in Figure~\ref{fig:taxonomy}. 
	
	\begin{table*}
		\rowcolors{1}{gray!20}{white}
		\centering
		\caption{
			{Details of the learning-based camera calibration for \textbf{the standard and distortion camera models} and the extended applications, including the method abbreviation, publication, calibration objective, network architecture, loss function, dataset, evaluation metrics, learning strategy, platform, and simulation or not (training data). For the learning strategies, SL, USL, WSL, Semi-SL, SSL, and RL denote supervised learning, unsupervised learning, weakly-supervised learning, semi-supervised learning, self-supervised learning, and reinforcement learning.}%,  
		}
		\vspace{-6pt}
		\label{table:methods:standard:distortion}
		\begin{threeparttable}
			\resizebox{1.0\textwidth}{!}{
				\setlength\tabcolsep{2pt}
				\renewcommand\arraystretch{1}
				% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}
				\begin{tabular}{c|r||c|c|c|c|c|c|c|c|c}
					\hline
                        %\large
					%\thickhline
					% &\#&
					&\textbf{Method}~~~~~~~~~&\textbf{Publication} &\textbf{Objective} &\textbf{Network}
					&\textbf{Loss Function} & \textbf{Dataset} &\textbf{Evaluation} & \textbf{Learning} &\textbf{Platform} &\textbf{Simulation}\\
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2015}}}
					% &1 &
					&DeepFocal~\cite{DeepFocal} &ICIP &Standard &AlexNet
					&$\mathcal{L}_2$ loss &1DSfM\cite{1DSfM} & Accuracy & SL &Caffe &\\
					&PoseNet~\cite{PoseNet} &ICCV &Standard 
					&GoogLeNet
					&$\mathcal{L}_2$ loss &Cambridge Landmarks\cite{Cambridge_Landmarks} &Accuracy &SL &Caffe& \\
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2016}}}
					% &1&
					&DeepHorizon~\cite{DeepHorizon} &BMVC &Standard &GoogLeNet	&Huber loss &HLW\cite{HLW} & Accuracy & SL &Caffe &\\
					
					&DeepVP~\cite{DeepVP} &CVPR &Standard 
					&AlexNet
					&Logistic loss &YUD\cite{YUD}, ECD\cite{ECD}, HLW\cite{HLW} &Accuracy &SL &Caffe& \\	
					
					&Rong et al.~\cite{Rong} &ACCV &Distortion &AlexNet
					&Softmax loss &ImageNet\cite{ImageNet} &Line length &SL &Caffe&\checkmark\\	
                        \hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2017}}}
					% &1&
					
					&URS-CNN~\cite{URS-CNN} &CVPR &Distortion 
					&CNNs
					&$\mathcal{L}_2$ loss &Sun\cite{xiao2010sun}, Oxford\cite{philbin2007object}, Zubud\cite{shao2003zubud}, LFW\cite{huang2008labeled} &PSNR, RMSE &SL &Torch&\checkmark\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2018}}}
					% &1&
					&Hold-Geoffroy et al.~\cite{Hold-Geoffroy} &CVPR &Standard &DenseNet	&Entropy loss &SUN360\cite{SUN360} & Human sensitivity & SL &- &\\

                        &Chang et al.\cite{chang2018deepvp} &ICRA &Standard &AlexNet
					&Cross-entropy loss &DeepVP-1M~\cite{chang2018deepvp} &MSE, Accuracy &SL &Matconvnet&\\
					
					&DeepCalib~\cite{DeepCalib} &CVMP &Distortion 
					&Inception-V3
					&Logcosh loss &SUN360\cite{SUN360} &Mean error &SL &TensorFlow&\checkmark \\	
					
                        &FishEyeRecNet~\cite{FishEyeRecNet} &ECCV &Distortion &VGG
					&$\mathcal{L}_2$ loss &ADE20K\cite{ADE20K} &PSNR, SSIM &SL &Caffe&\checkmark\\
					
					&Shi et al.\cite{Shi} &ICPR &Distortion &ResNet
					&$\mathcal{L}_2$ loss &ImageNet\cite{ImageNet} &MSE &SL &PyTorch&\checkmark\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2019}}}
					% &1&
					
					&UprightNet~\cite{UprightNet} &ICCV &Standard &U-Net	&Geometry loss &InteriorNet\cite{InteriorNet}, ScanNet\cite{ScanNet}, SUN360\cite{SUN360} &Mean error & SL &PyTorch &\\

					&NeurVPS~\cite{zhou2019neurvps} &NeurIPS &Standard &CNNs	&Binary cross entropy, chamfer-$\mathcal{L}_2$ loss &ScanNet~\cite{ScanNet}, SU3~\cite{SU3} &Angle accuracy &SL &PyTorch &\\

					&Deep360Up~\cite{Deep360Up} &VR &Standard &DenseNet	&Log-cosh loss\cite{Log-cosh} & SUN360\cite{SUN360} &Mean error & SL &- &\checkmark\\

					&Lopez et al.~\cite{Lopez} &CVPR &Distortion &DenseNet	&Bearing loss &SUN360\cite{SUN360} &MSE & SL &PyTorch &\\
					
					&Zhuang et al.~\cite{Zhuang} &IROS &Distortion &ResNet	&$\mathcal{L}_1$ loss & KITTI\cite{KITTI} &Mean error, RMSE & SL &PyTorch &\checkmark\\
					
					&DR-GAN~\cite{DR-GAN} &TCSVT &Distortion &GANs	&Perceptual loss & MS-COCO\cite{MS-COCO} &PSNR, SSIM & SL &TensorFlow &\checkmark\\
					
					&STD~\cite{STD} &TCSVT &Distortion &GANs+CNNs	&Perceptual loss & MS-COCO\cite{MS-COCO} &PSNR, SSIM & SL &TensorFlow &\checkmark\\
					
					&UnFishCor~\cite{UnFishCor} &JVCIR &Distortion &VGG	&$\mathcal{L}_1$ loss & Places2\cite{Places2} &PSNR, SSIM & USL &TensorFlow &\checkmark\\
					
					&BlindCor~\cite{BlindCor} &CVPR &Distortion &U-Net	&$\mathcal{L}_2$ loss & Places2\cite{Places2} &MSE & SL &PyTorch &\checkmark\\
					
					&RSC-Net~\cite{RSC-Net} &CVPR &Distortion &ResNet	&$\mathcal{L}_1$ loss & KITTI\cite{KITTI} &Mean error & SL &PyTorch &\checkmark\\
					
					&Xue et al.~\cite{Xue} &CVPR &Distortion &ResNet	&$\mathcal{L}_2$ loss & Wireframes\cite{Wireframes}, SUNCG\cite{SUNCG} &PSNR, SSIM, RPE & SL &PyTorch &\checkmark\\
					
					&Zhao et al.~\cite{Zhao} &ICCV &Distortion &VGG+U-Net	&$\mathcal{L}_1$ loss & Self-constructed+BU-4DFE\cite{BU-4DFE} &Mean error &SL &- &\checkmark\\
     
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2020}}}
					% &1&
				    
				    &Lee et al.~\cite{Lee} &ECCV &Standard &PointNet+CNNs	& Cross-entropy loss &Google Street View\cite{googleStreet}, HLW\cite{HLW} &Mean error, AUC\cite{AUC} & SL &- &\\

				    &Baradad et al.~\cite{Baradad} &CVPR &Standard &CNNs	&$\mathcal{L}_2$ loss &ScanNet\cite{ScanNet}, NYU\cite{NYU}, SUN360\cite{SUN360} &Mean error, RMS &SL &PyTorch &\\
				    
				    &Zheng et al.~\cite{Zheng} &CVPR &Standard &CNNs	&$\mathcal{L}_1$ loss &FocaLens\cite{FocaLens} &Mean error, PSNR, SSIM &SL &- &\checkmark\\
				    
				    &Zhu et al.~\cite{Zhu} &ECCV &Standard &CNNs+PointNet	&$\mathcal{L}_1$ loss &SUN360\cite{SUN360}, MS-COCO\cite{MS-COCO} &Mean error, Accuracy &WSL &PyTorch &\checkmark\\

				    &Davidson et al.~\cite{Davidson} &ECCV &Standard &FCN	&Dice loss &SUN360\cite{SUN360} &Accuracy &SL &- &\checkmark\\
				    
				    &DeepFEPE~\cite{DeepFEPE} &IROS &Standard &VGG+PointNet	&$\mathcal{L}_2$ loss &KITTI\cite{KITTI}, ApolloScape\cite{Apolloscape} &Mean error &SL &PyTorch &\\
				    
				    &MisCaliDet~\cite{MisCaliDet} &ICRA &Distortion &CNNs	& $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &MSE & SL &TensorFlow &\checkmark\\
				    
				    &DeepPTZ~\cite{DeepPTZ} &WACV &Distortion &Inception-V3	& $\mathcal{L}_1$ loss &SUN360\cite{SUN360} &Mean error & SL &PyTorch &\checkmark\\
				    
				    &DDM~\cite{DDM} &TIP &Distortion &GANs	&$\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO} &PSNR, SSIM &SL &TensorFlow &\checkmark\\
				    
				    &Li et al.~\cite{Li} &TIP &Distortion &CNNs	&Cross-entropy, $\mathcal{L}_1$ loss &CelebA\cite{CelebA} &Cosine distance &SL &- &\checkmark\\
				    
				    &PSE-GAN~\cite{PSE-GAN} &ICPR &Distortion &GANs	&$\mathcal{L}_1$, WGAN loss &Place2\cite{Places2} &MSE &SL &- &\checkmark\\
				    
				    &RDC-Net~\cite{RDC-Net} &ICIP &Distortion &ResNet	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &ImageNet\cite{ImageNet} &PSNR, SSIM &SL &PyTorch &\checkmark\\
				    
				    &FE-GAN~\cite{FE-GAN} &ICASSP &Distortion &GANs	&$\mathcal{L}_1$, GAN loss &Wireframe\cite{Wireframes}, LSUN\cite{LSUN} &PSNR, SSIM, RMSE &SSL &PyTorch &\checkmark\\
				    
				    &RDCFace~\cite{RDCFace} &CVPR &Distortion &ResNet	&Cross-entropy, $\mathcal{L}_2$ loss &IMDB-Face\cite{IMDB-Face} &Accuracy &SL &- &\checkmark\\
				    
				    &LaRecNet~\cite{LaRecNet} &arXiv &Distortion &ResNet	&$\mathcal{L}_2$ loss &Wireframes\cite{Wireframes}, SUNCG\cite{SUNCG} &PSNR, SSIM, RPE &SL &PyTorch &\checkmark\\
				    
				    &DeepUnrollNet~\cite{DeepUnrollNet} &CVPR &Distortion &FCN	&$\mathcal{L}_1$, perceptual, total variation loss &Carla-RS\cite{DeepUnrollNet}, Fastec-RS\cite{DeepUnrollNet}  &PSNR, SSIM &SL &PyTorch &\checkmark\\
                        
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2021}}}
					% &1&
					&StereoCaliNet~\cite{StereoCaliNet} &TCI &Standard &U-Net	&$\mathcal{L}_1$ loss &TAUAgent\cite{TAUAgent}, KITTI\cite{KITTI} &Mean error & SL &PyTorch &\checkmark\\
					
					&CTRL-C~\cite{CTRL-C} &ICCV &Standard &Transformer	&Cross-entropy, $\mathcal{L}_1$ loss &Google Street View\cite{googleStreet}, SUN360\cite{SUN360} &Mean error, AUC\cite{AUC} & SL &PyTorch &\checkmark\\

				   &SA-MobileNet~\cite{SA-MobileNet} &BMVC &Standard &MobileNet &Cross-entropy loss& SUN360\cite{SUN360}, ADE20K\cite{ADE20K}, NYU\cite{NYU} &MAE, Accuracy & SL &TensorFlow &\checkmark \\
				   
				   &SPEC~\cite{SPEC} &ICCV &Standard &ResNet &Softargmax-$\mathcal{L}_2$ loss&Self-constructed &W-MPJPE, PA-MPJPE & SL &PyTorch &\checkmark \\
				   
				   &DirectionNet~\cite{DirectionNet} &CVPR &Standard &U-Net &Cosine similarity loss &InteriorNet\cite{InteriorNet}, Matterport3D\cite{Matterport3D}&Mean and median error  & SL &TensorFlow &\checkmark \\
					
				   &Wakai et al.~\cite{Wakai} &ICCVW &Distortion &DenseNet	&Smooth $\mathcal{L}_1$ loss &StreetLearn\cite{StreetLearn} &Mean error, PSNR, SSIM & SL &- &\checkmark\\
				   &OrdianlDistortion~\cite{OrdianlDistortion} &TIP &Distortion &CNNs	&Smooth $\mathcal{L}_1$ loss & MS-COCO\cite{MS-COCO} &PSNR, SSIM, MDLD & SL &TensorFlow &\checkmark\\
				   
				   &PolarRecNet~\cite{PolarRecNet} &TCSVT &Distortion &VGG+U-Net	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO}, LMS\cite{LMS} &PSNR, SSIM, MSE & SL &PyTorch &\checkmark\\
				   
				   &DQN-RecNet~\cite{DQN-RecNet} &PRL &Distortion &VGG	&$\mathcal{L}_2$ loss & Wireframes\cite{Wireframes} &PSNR, SSIM, MSE & RL &PyTorch &\checkmark\\
				   
				   &Tan et al.~\cite{Tan} &CVPR &Distortion &U-Net &$\mathcal{L}_2$ loss & Self-constructed &Accuracy & SL &PyTorch & \\
				   
				   &PCN~\cite{PCN} &CVPR &Distortion &U-Net &$\mathcal{L}_1$, $\mathcal{L}_2$, GAN loss & Place2\cite{Places2} &PSNR, SSIM, FID, CW-SSIM & SL &PyTorch &\checkmark \\
				   
				   &DaRecNet~\cite{DaRecNet} &ICCV &Distortion &U-Net &Smooth $\mathcal{L}_1$, $\mathcal{L}_2$ loss & ADE20K\cite{ADE20K} &PSNR, SSIM & SL &PyTorch &\checkmark \\
				   
				   &JCD~\cite{JCD} &CVPR &Distortion &FCN &Charbonnier\cite{Charbonnier}, perceptual loss &BS-RSCD \cite{JCD}, Fastec-RS
                   \cite{DeepUnrollNet}&PSNR, SSIM, LPIPS  & SL &PyTorch & \\

                   &Fan\etal~\cite{fan2021inverting} &ICCV &Distortion &U-Net &$\mathcal{L}_1$, perceptual loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS  & SL &PyTorch & \\

                   &SUNet~\cite{SUNet} &ICCV &Distortion &DenseNet+ResNet &$\mathcal{L}_1$, perceptual loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM  & SL &PyTorch & \\
       
				   \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2022}}}
					% &1&
				   &DVPD~\cite{DVPD} &CVPR &Standard &CNNs	&Cross-entropy loss &SU3\cite{SU3}, ScanNet\cite{ScanNet}, YUD\cite{YUD}, NYU\cite{NYU} &Accuracy, AUC\cite{AUC} & SL &PyTorch &\checkmark\\
				   
				   &Fang et al.~\cite{Fang} &ICRA &Standard &CNNs	&$\mathcal{L}_2$ loss &KITTI\cite{KITTI}, EuRoC\cite{EuRoC}, OmniCam\cite{OmniCam} &MRE, RMSE & SSL &PyTorch &\\
				   
				   &CPL~\cite{CPL} &ICASSP &Standard &Inception-V3	&$\mathcal{L}_1$ loss &CARLA\cite{CARLA}, CyclistDetection\cite{CyclistDetection} &MAE & SL &TensorFlow &\checkmark\\
				   
				   &Do et al.~\cite{Do} &CVPR &Standard  &ResNet&$\mathcal{L}_2$, Robust angular \cite{RobustAngular} loss &Self-constructed, 7-SCENES\cite{7-SCENES} &Median error, Recall &SL &PyTorch &\\
				   
				   &DiffPoseNet~\cite{DiffPoseNet} &CVPR &Standard  &CNNs+LSTMs&$\mathcal{L}_2$ loss &TartanAir\cite{TartanAir}, KITTI\cite{KITTI}, TUM RGB-D\cite{TUM_RGB-D} &PEE, AEE\cite{AEE} &SSL &PyTorch &\\
				   
				   &SceneSqueezer~\cite{SceneSqueezer} &CVPR &Standard  &Transformer&$\mathcal{L}_1$ loss &RobotCar Seasons\cite{RobotCar}, Cambridge Landmarks\cite{Cambridge_Landmarks}  &Mean error, Recall\cite{AEE} &SL &PyTorch &\\
				   
				   &FocalPose~\cite{FocalPose} &CVPR &Standard  &CNNs&$\mathcal{L}_1$, Huber loss &Pix3D\cite{Pix3D}, CompCars\cite{StanfordCars}, StanfordCars\cite{StanfordCars}  &Median error, Accuracy &SL &PyTorch &\\

   				   &SS-WPC~\cite{SS-WPC} &CVPR &Distortion  &Transformer	&Cross-entropy, $\mathcal{L}_1$ loss &Tan et al.\cite{Tan} &Accuracy & Semi-SL &PyTorch &\\
				   
				   &AW-RSC~\cite{AW-RSC} &CVPR &Distortion  &CNNs	&Charbonnier\cite{Charbonnier}, perceptual loss &Self-constructed, FastecRS\cite{DeepUnrollNet} &PSNR, SSIM &SL &PyTorch &\\
				   
				   &EvUnroll~\cite{EvUnroll} &CVPR &Distortion  &U-Net	&Charbonnier, perceptual, TV loss &Self-constructed, FastecRS\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS &SL &PyTorch &\\
				   
				   &CCS-Net~\cite{zhang2022learning} &RAL &Distortion  &U-Net&$\mathcal{L}_1$ loss &TUM RGB-D\cite{TUM_RGB-D} &MAE, RPE &SL &PyTorch &\checkmark\\
				   
				   &FishFormer~\cite{FishFormer} &arXiv &Distortion  &Transformer&$\mathcal{L}_2$ loss &Place2\cite{Places2}, CelebA\cite{CelebA}  &PSNR, SSIM, FID &SL &PyTorch &\checkmark\\
				   
                  &SIR~\cite{SIR} &TIP &Distortion &ResNet &$\mathcal{L}_1$ loss & ADE20K\cite{ADE20K}, WireFrames\cite{Wireframes}, MS-COCO\cite{MS-COCO} &PSNR, SSIM & SSL &PyTorch &\checkmark \\

                    &GenCaliNet~\cite{GenCaliNet} &ECCV &Distortion &DenseNet	&$\mathcal{L}_2$ loss &StreetLearn\cite{StreetLearn}, SP360\cite{SP360} &MAE, PSNR, SSIM & SL &- &\checkmark\\

				   &IFED~\cite{IFED} &ECCV &Distortion  &CNNs	&Charbonnier, perceptual, TV loss &RS-GOPRO~\cite{IFED}, FastecRS\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS &SL &PyTorch &\checkmark\\


                      \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2023}}}
					% &1&

					&PM-Calib~\cite{PM-Calib} &TPAMI &Standard &CNNs	&Kullback-Leibler loss &360Cities~\cite{360Cities} &Mean error, Human sensitivity & SL &- &\checkmark\\

					&PerspectiveField~\cite{PerspectiveField} &CVPR &Standard &CNNs+Transformers	&Cross-entropy loss &360Cities~\cite{360Cities}, Stanford2D3D~\cite{Stanford2D3D}, TartanAir~\cite{TartanAir} &Mean error & SL &PyTorch &\checkmark\\

					&Orienternet~\cite{Orienternet} &CVPR &Standard &CNNs	&Log-likelihood loss &MGL~\cite{Orienternet} &Positions and rotation errors & SL &PyTorch &\\

					&Neumap~\cite{Neumap} &CVPR &Standard &CNNs+Transformers	&$\mathcal{L}_2$, cross-entropy loss &Cambridge Landmarks~\cite{Cambridge_Landmarks}, ScanNet~\cite{ScanNet}, 7scenes~\cite{7-SCENES} &Median error & SL &PyTorch &\\

					&SESC~\cite{SESC} &IROS &Standard &CNNs	&$\mathcal{L}_1$ loss &KITTI~\cite{KITTI}, DDAD~\cite{DDAD} &Median error & SSL &PyTorch &\\

					&CROSSFIRE~\cite{CROSSFIRE} &ICCV &Standard &CNNs	&$\mathcal{L}_2$, TV loss &Cambridge Landmarks~\cite{Cambridge_Landmarks}, 7scenes~\cite{7-SCENES} &Median error & SSL &PyTorch &\\

					&WildCamera~\cite{WildCamera} &NeurIPS &Standard &CNNs	&Cosine similarity loss &ScanNet~\cite{ScanNet}, MegaDepth~\cite{MegaDepth}, KITTI~\cite{KITTI} &Median error & SL &PyTorch &\checkmark\\

					&DroidCalib~\cite{DroidCalib} &ICCV &Standard &CNNs	&$\mathcal{L}_1$ loss &TartanAir~\cite{TartanAir}, EuRoC~\cite{EuRoC}, TUM RGB-D~\cite{TUM_RGB-D} &Median error & SL &PyTorch &\checkmark\\


					&NeuralLens~\cite{NeuralLens} &CVPR &Distortion &CNNs	&$\mathcal{L}_2$ loss &SynLens~\cite{NeuralLens} &RMS & SL &PyTorch &\checkmark\\
                    
				   &DDA~\cite{DDA} &ICCV &Distortion &Diffusion	&Diffusion loss &Places2~\cite{Places2}, Woodscape~\cite{yogamani2019woodscape} &PSNR, SSIM, MS-SSIM, FID, LPIPS & USL &PyTorch &\checkmark\\

				   &CACM-Net~\cite{CACM-Net} &CVPR &Distortion &CNNs	&$\mathcal{L}_2$ loss &Wireframes\cite{Wireframes} &StraightAcc, ShapeAcc, ConformalAcc & SL &- &\checkmark\\

				   &RDTR~\cite{RDTR} &TIP &Distortion &CNNs+Transformers	&Cross-entropy, $\mathcal{L}_1$ loss &Places2~\cite{Places2} &EPE, PSNR, SSIM & SL &PyTorch &\checkmark\\

				   &DaFIR~\cite{DaFIR} &TCSVT &Distortion &Transformers	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &Places2~\cite{Places2} &PSNR, SSIM & SL &PyTorch &\checkmark\\
				   
				   &SimFIR~\cite{SimFIR} &ICCV &Distortion &Transformers	&Cross-entropy, $\mathcal{L}_1$ loss &Places2~\cite{Places2} &PSNR, SSIM, FID & SL &PyTorch &\checkmark\\

				   &RecRecNet~\cite{Recrecnet} &ICCV &Distortion &CNNs	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &MS-COCO\cite{MS-COCO} &PSNR, SSIM, FID, LPIPS & SL &PyTorch &\checkmark\\

				   &SDP-Net~\cite{SDP-Net} &AAAI &Distortion &CNNs	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &DAVIS~\cite{perazzi2016benchmark}, Youtube-VOS~\cite{xu2018youtube} &PSNR, SSIM, FID, EPE & SL &PyTorch &\checkmark\\

				   &Darswin~\cite{Darswin} &ICCV &Distortion &Transformers	&Cross-entropy loss &ImageNet~\cite{ImageNet} &Accuracy & SL &PyTorch &\checkmark\\

				   &REG-Net~\cite{REG-Net} &ACM MM &Distortion &CNNs	&$\mathcal{L}_2$ loss &REG-HDR~\cite{REG-Net} &PSNR, SSIM, LPIPS & SL &PyTorch &\\

				   &SelfDRSC~\cite{SelfDRSC} &ICCV &Distortion &CNNs	&$\mathcal{L}_2$ loss &RS-GOPRO~\cite{IFED} &PSNR, SSIM, LPIPS, NIQE, NRQM, PI & SSL &PyTorch &\checkmark\\

				   &SSL-RSC~\cite{SSL-RSC} &arXiv &Distortion &CNNs	&$\mathcal{L}_1$ loss &Gev-RS~\cite{EvUnroll}, Fastec-RS~\cite{DeepUnrollNet}, ERS-VFI~\cite{SSL-RSC} &PSNR, SSIM, LPIPS & SSL &- &\checkmark\\

				   &EvShutter~\cite{EvShutter} &CVPR &Distortion &CNNs	&$\mathcal{L}_1$ loss &RS-ERGB~\cite{EvShutter}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS & SL &- &\\

				   &SelfUnroll~\cite{SelfUnroll} &arXiv &Distortion &CNNs	&$\mathcal{L}_1$, TV loss &RS-ERGB~\cite{EvShutter}, Fastec-RS~\cite{DeepUnrollNet}, Gev-RS~\cite{EvUnroll}, DRE~\cite{SelfUnroll} &PSNR, SSIM, LPIPS & SSL &PyTorch &\checkmark\\

				   &PatchNet~\cite{PatchNet} &WACV &Distortion &CNNs	&$\mathcal{L}_1$, TV loss &BS-RSCD~\cite{JCD}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS & SL &PyTorch &\checkmark\\

				   &JAMNet~\cite{JAMNet} &CVPR &Distortion &CNNs+Transformers	&$\mathcal{L}_1$, TV loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet}, BS-RSCD~\cite{JCD} &PSNR, SSIM, LPIPS & SL &PyTorch &\checkmark\\

				   &QRSC~\cite{QRSC} &ICCV &Distortion &Transformers	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet}, BS-RSCD~\cite{JCD} &PSNR, SSIM, LPIPS & SL &PyTorch &\checkmark\\

				   &Deep\_HM~\cite{Deep_HM} &ICCV &Distortion &CNNs	&$\mathcal{L}_2$ loss &Carla-RS~\cite{DeepUnrollNet}, RS-Homo~\cite{Deep_HM} &PSNR, SSIM & SL &PyTorch &\checkmark\\

                      \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2024}}}
					% &1&

					&DM-Calib~\cite{DM-Calib} &arXiv &Standard &Diffusion	&Diffusion loss & NuScenes~\cite{caesar2020nuscenes}, KITTI~\cite{KITTI}, SUN3D~\cite{SUN3D}, CityScapes~\cite{CityScapes}, etc &Relative error& SL &PyTorch &\checkmark\\

					&GeoCalib~\cite{GeoCalib} &ECCV &Standard &CNNs	&$\mathcal{L}_1$ loss & OpenPano~\cite{GeoCalib} &Median error, AUC& SL &PyTorch &\checkmark\\

					&ExtremeRotation~\cite{ExtremeRotation} &arXiv &Standard &Transformers	&Cross-entropy loss & ExtremeLandmarkPairs~\cite{ExtremeRotation} &Geodesic error& SL &PyTorch &\\

					&GAT-Calib~\cite{GAT-Calib} &WACV &Standard &GNNs	&Cross-entropy, $\mathcal{L}_2$ loss & World Cup 2014~\cite{homayounfar2017sports} &IoU& SL &- &\checkmark\\

					&SOFI~\cite{SOFI} &BMVC &Standard &CNNs+Transformers	&Focal loss & Google Street View~\cite{googleStreet}, SUN360~\cite{SUN360}, HLW\cite{HLW} &Mean error, AUC& SL &PyTorch &\\

					&PWT-Calib~\cite{PWT-Calib} &WACV &Standard &CNNs	&$\mathcal{L}_2$ loss & CUTC~\cite{PWT-Calib} &Mean error, RMSE& SL &PyTorch &\\

					&NeFeS~\cite{NeFeS} &CVPR &Standard &CNNs+MLPs	&$\mathcal{L}_1$, cosine similarity loss & Cambridge Landmarks~\cite{Cambridge_Landmarks}, 7-Scenes~\cite{7-SCENES} &Median error& SL &PyTorch &\\

					&U-ARE-ME~\cite{U-ARE-ME} &arXiv &Standard &CNNs	&$\mathcal{L}_2$ loss & ICL-NUIM~\cite{ICL-NUIM}, TUM RGB-D~\cite{TUM_RGB-D}, ScanNet~\cite{ScanNet} &ARE& SL &PyTorch &\checkmark\\

					&FlowMap~\cite{FlowMap} &arXiv &Standard &CNNs	&$\mathcal{L}_1$ loss & MipNeRF-360~\cite{Barron_2021_ICCV}, Tanks \& Temples~\cite{TAT}, LLFF~\cite{LLFF}, CO3D~\cite{CO3D} &ATE& SL &PyTorch &\\

					&MSCC~\cite{MSCC} &WACV &Standard &CNNs+Transformers	&Cross-entropy, angular distance loss & Google Street View~\cite{googleStreet}, SYN-Citypark~\cite{SPEC}, Flickr~\cite{SPEC}, HLW~\cite{HLW} &Mean, median errors& SL &PyTorch &\checkmark\\

                  &HC-Net~\cite{HC-Net} &NeurIPS &Standard &CNNs	&infoNCE loss  &VIGOR~\cite{VIGOR}, KITTI~\cite{KITTI} &Mean, median errors & SL &Pytorch &\checkmark\\

					&DiffCalib~\cite{DiffCalib} &arXiv &Standard &Diffusion	&Diffusion loss & Hypersim~\cite{Hypersim}, NuScenes~\cite{caesar2020nuscenes}, KITTI~\cite{KITTI}, CitySpace~\cite{CityScapes}, NYUv2~\cite{NYU} &Relative error& SL &PyTorch &\checkmark\\

					&CAR~\cite{CAR} &ICLR &Standard &Diffusion	&Diffusion loss & CO3Dv2~\cite{CO3D} &Relative error& SL &PyTorch &\\
                    
				   &ADPs~\cite{ADPs} &CVPR &Distortion &CNNs	&$\mathcal{L}_2$ loss &StreetLearn\cite{StreetLearn}, SP360\cite{SP360} &Mean absolute error, REPE & SL &PyTorch &\checkmark\\
                   
				   &CDM~\cite{CDM} &TCSVT &Distortion &CNNs	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &MS-COCO~\cite{MS-COCO}, ADE20K~\cite{ADE20K}, Wireframes~\cite{Wireframes} &PSNR, SSIM & SL &PyTorch &\checkmark\\

				   &QueryCDR~\cite{QueryCDR} &ECCV &Distortion &CNNs+Transformers	&$\mathcal{L}_1$ loss &MS-COCO~\cite{MS-COCO}, Places2~\cite{Places2} &PSNR, SSIM & SL &PyTorch &\checkmark\\

				   &VACR~\cite{VACR} &ECCV &Distortion &CNNs	&$\mathcal{L}_2$ loss &KITTI-360~\cite{liao2022kitti}, StreetLearn\cite{StreetLearn}, Woodscape~\cite{yogamani2019woodscape}   &PSNR, SSIM, FID & SL &PyTorch &\checkmark\\

				   &DualPriorsCorrection~\cite{DualPriorsCorrection} &ECCV &Distortion &CNNs+GANs	&$\mathcal{L}_2$ loss &Tan~\cite{Tan}   &LineAcc, ShapeAcc & SL &PyTorch &\checkmark\\

				   &Disco~\cite{Disco} &IJCV &Distortion &GANs	&$\mathcal{L}_2$ loss &CMDP~\cite{fried2016perspective}, USCPP~\cite{Zhao}  &LMK-E, PSNR, SSIM, LIPIPS, ID & SL &- &\\

				   &MOWA~\cite{MOWA} &arXiv &Distortion &Transformers	&Cross-entroy, $\mathcal{L}_1$, $\mathcal{L}_2$ loss & StitchRect~\cite{StitchRect}, Place2~\cite{Places2}, MS-COCO~\cite{MS-COCO}, RotationCorr~\cite{RotationCorr}, Tan~\cite{Tan} &PSNR, SSIM, ShapeAcc & SL &PyTorch &\checkmark\\

				   &LBCNet~\cite{LBCNet} &TPAMI &Distortion &CNNs	&$\mathcal{L}_1$ loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM & SL &PyTorch &\checkmark\\

				   &TACA-Net~\cite{TACA-Net} &ACM MM &Distortion &Transformers	&$\mathcal{L}_1$ loss &Gev-RS~\cite{EvUnroll}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM, LPIPS & SL &- &\checkmark\\

				   &UniINR~\cite{UniINR} &ECCV &Distortion &CNNs	&$\mathcal{L}_1$ loss &Gev-RS~\cite{EvUnroll}, Fastec-RS~\cite{DeepUnrollNet} &PSNR, SSIM & SL &PyTorch &\checkmark\\

				   &DFRSC~\cite{DFRSC} &CVPR &Distortion &CNNs	&$\mathcal{L}_1$ loss &Carla-RS~\cite{DeepUnrollNet}, Fastec-RS~\cite{DeepUnrollNet}, BS-RSCD~\cite{JCD} &PSNR, SSIM, LPIPS & SL &PyTorch &\checkmark\\

				   &RS-Diffusion~\cite{RS-Diffusion} &arXiv &Distortion &Diffusion	&Diffusion loss &RS-Homo~\cite{Deep_HM}, RS-Real~\cite{RS-Diffusion} &PSNR, SSIM, EPE & SL &PyTorch &\checkmark\\
                   
				\hline
				\end{tabular}
			}
		\end{threeparttable}
	\end{table*}






	\begin{table*}
		\rowcolors{1}{gray!20}{white}
		
		\caption{
			{Details of the learning-based camera calibration for \textbf{the cross-view and cross-sensor camera models} and the extended applications, including the method abbreviation, publication, calibration objective, network architecture, loss function, dataset, evaluation metrics, learning strategy, platform, and simulation or not (training data). For the learning strategies, SL, USL, WSL, Semi-SL, SSL, and RL denote supervised learning, unsupervised learning, weakly-supervised learning, semi-supervised learning, self-supervised learning, and reinforcement learning. }%, 
		}
            \centering
		\vspace{-6pt}
		\label{table:methods:cross-view:cross-sensor}
		\begin{threeparttable}
			\resizebox{1\textwidth}{!}{
				\setlength\tabcolsep{2pt}
				\renewcommand\arraystretch{0.98}
				% \begin{tabular}{|c|c|r||c|c|c|c|c|c|c|c|c|}  % {lccc}
				\begin{tabular}{c|r||c|c|c|c|c|c|c|c|c}
					\hline
					%\thickhline
					% &\#&
					&\textbf{Method}~~~~~~~~~&\textbf{Publication} &\textbf{Objective} &\textbf{Network}
					&\textbf{Loss Function} & \textbf{Dataset} &\textbf{Evaluation} & \textbf{Learning} &\textbf{Platform} &\textbf{Simulation}\\
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2016}}}
					% &1&
					&DHN\cite{DHN} &RSSW &Cross-View &VGG
					&$\mathcal{L}_2$ loss &MS-COCO\cite{MS-COCO} &MSE &SL &Caffe&\checkmark\\		
                        \hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2017}}}
					% &1&
					&CLKN~\cite{CLKN} &CVPR &Cross-View  &CNNs	&Hinge loss &MS-COCO\cite{MS-COCO} & MSE & SL &Torch &\checkmark\\
					
		            &HierarchicalNet~\cite{HierarchicalNet} &ICCVW &Cross-View 
					&VGG
					&$\mathcal{L}_2$ loss &MS-COCO\cite{MS-COCO} &MSE &SL &TensorFlow&\checkmark \\
					
					&RegNet~\cite{schneider2017regnet} &IV &Cross-Sensor 
					&CNNs
					&$\mathcal{L}_2$ loss &KITTI\cite{KITTI} &MAE &SL &Caffe&\checkmark\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2018}}}
					% &1&
					&DeepFM\cite{DeepFM} &ECCV &Cross-View &ResNet
					&$\mathcal{L}_2$ loss &T\&T\cite{TT}, KITTI\cite{KITTI}, 1DSfM\cite{1DSfM} &F-score, Mean &SL &PyTorch&\checkmark\\
					
					&Poursaeed et al.\cite{Poursaeed} &ECCVW &Cross-View &CNNs
					&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &EPI-ABS, EPI-SQR &SL &-& \\
					
					&UDHN\cite{UDHN} &RAL &Cross-View &VGG
					&$\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO} &RMSE &USL &TensorFlow&\checkmark\\
					
					&PFNet\cite{PFNet} &ACCV &Cross-View &FCN
					&Smooth $\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO} &MAE &SL &TensorFlow&\checkmark\\
					
					&CalibNet\cite{iyer2018calibnet} &IROS &Cross-Sensor &ResNet
					&Point cloud distance, $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &Geodesic distance, MAE &SL &TensorFlow&\checkmark\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2019}}}
					% &1&
					&SSR-Net~\cite{SSR-Net} &PRL &Cross-View &ResNet	&$\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO} &MAE & SSL &PyTorch &\checkmark\\
					
					&Abbas et al.~\cite{Abbas} &ICCVW &Cross-View &CNNs	&Softmax loss & CARLA\cite{CARLA} &AUC\cite{AUC}, Mean error & SL &TensorFlow &\checkmark\\
					
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2020}}}
					% &1&
					&Sha et al.~\cite{Sha} &CVPR &Cross-View &U-Net	& Cross-entropy loss &World Cup 2014\cite{homayounfar2017sports} &IoU & SL &TensorFlow &\\
				    
				    &MHN~\cite{MHN} &CVPR &Cross-View &VGG	&Cross-entropy loss &MS-COCO\cite{MS-COCO}, Self-constructed &MAE & SL &TensorFlow &\checkmark\\
				    
				    &CA-UDHN~\cite{CA-UDHN} &ECCV &Cross-View &FCN+ResNet	&Triplet loss &Self-constructed &MSE &USL &PyTorch &\\

				    &SRHEN~\cite{SRHEN} &ACM MM &Cross-View &CNNs	&$\mathcal{L}_2$ loss &MS-COCO~\cite{MS-COCO}, SUN397~\cite{SUN360}  &MACE &SL &- &\checkmark\\
				    
				    &RGGNet~\cite{yuan2020rggnet} &RAL &Cross-Sensor &ResNet	&Geodesic distance loss &KITTI\cite{KITTI}  &MSE, MSEE, MRR &SL &TensorFlow &\checkmark\\
				    
				    &CalibRCNN~\cite{shi2020calibrcnn} &IROS &Cross-Sensor &RNNs	&$\mathcal{L}_2$, Epipolar geometry loss &KITTI~\cite{KITTI}  &MAE &SL &TensorFlow &\checkmark\\

				    &SSI-Calib~\cite{zhu2020online} &ICRA &Cross-Sensor &CNNs	&$\mathcal{L}_2$ loss &Pascal VOC 2012~\cite{pascal-voc-2012}  &Mean/standard deviation &SL &TensorFlow &\checkmark\\

				    &SOIC~\cite{wang2020soic} &arXiv &Cross-Sensor &ResNet+PointRCNN	& Cost function &KITTI~\cite{KITTI}  &Mean error &SL &- &\\        

				    &NetCalib~\cite{wu2021netcalib} &ICPR &Cross-Sensor &CNNs	&$\mathcal{L}_1$ loss &KITTI~\cite{KITTI}  &MAE &SL &PyTorch &\checkmark\\
                        				   
					\hline
					\hline
					\multirow{1}{*}{\rotatebox{0}{\textbf{2021}}}
					% &1&
				   
				   &DLKFM~\cite{DLKFM} &CVPR &Cross-View &Siamese-Net &$\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO}, Google Earth, Google Map &MSE & SL &TensorFlow &\checkmark \\
				   
				   &LocalTrans~\cite{LocalTrans} &ICCV &Cross-View &Transformer &$\mathcal{L}_1$ loss & MS-COCO\cite{MS-COCO} &MSE, PSNR, SSIM & SL &PyTorch &\checkmark \\
				   
				   &BasesHomo~\cite{BasesHomo} &ICCV &Cross-View &ResNet &Triplet loss & CA-UDHN\cite{CA-UDHN} &MSE & USL &PyTorch & \\
				   &ShuffleHomoNet~\cite{ShuffleHomoNet} &ICIP &Cross-View &ShuffleNet &$\mathcal{L}_2$ loss & MS-COCO\cite{MS-COCO} &RMSE & SL &TensorFlow &\checkmark \\
				   
				   &DAMG-Homo~\cite{DAMG-Homo} &TCSVT &Cross-View &CNNs &$\mathcal{L}_1$ loss & MS-COCO\cite{MS-COCO}, UDIS\cite{UDIS} &RMSE, PSNR, SSIM & SL &TensorFlow &\checkmark \\
                   
                   &LCCNet~\cite{lv2021lccnet} &CVPRW &Cross-Sensor &CNNs &Smooth $\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI\cite{KITTI} &MSE  & SL &PyTorch &\checkmark \\
                   
                   &CFNet~\cite{lv2021cfnet} &Sensors &Cross-Sensor &FCN &$\mathcal{L}_1$, Charbonnier\cite{Charbonnier} loss &KITTI\cite{KITTI}, KITTI-360\cite{liao2022kitti} &MAE, MSEE, MRR  & SL &PyTorch &\checkmark \\

                   &SemAlign~\cite{liu2021semalign} &IROS &Cross-Sensor &CNNs & Semantic alignment loss &KITTI~\cite{KITTI} &Mean/median rotation errors & SL &PyTorch &\checkmark\\
       
				   \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2022}}}
					% &1&
				   &IHN~\cite{IHN} &CVPR &Cross-View  &Siamese-Net	&$\mathcal{L}_1$ loss &MS-COCO\cite{MS-COCO}, Google Earth, Google Map &MACE & SL &PyTorch &\checkmark\\
				   
				   &HomoGAN~\cite{HomoGAN} &CVPR &Cross-View  &GANs	&Cross-entropy, WGAN loss &CA-UDHN\cite{CA-UDHN} &Mean error & USL &PyTorch &\checkmark\\

				   &Liu et al.~\cite{Liu} &TPAMI &Cross-View &ResNet&Triplet loss &Self-constructed  &MSE, Accuracy &USL &PyTorch &\\
				   
				   &DXQ-Net~\cite{jing2022dxq} &IROS &Cross-Sensor  &CNNs+RNNs&$\mathcal{L}_1$, geodesic loss &KITTI\cite{KITTI}, KITTI-360\cite{liao2022kitti}  &MSE &SL &PyTorch &\checkmark\\
				   
				   &SST-Calib~\cite{SST-Calib} &ITSC &Cross-Sensor  &CNNs &$\mathcal{L}_2$ loss &KITTI\cite{KITTI}  &QAD, AEAD &SL &PyTorch &\checkmark\\

				   &ATOP~\cite{ATOP} &TIV &Cross-Sensor  &CNNs &Cross entropy loss &Self-constructed + KITTI\cite{KITTI}  &RRE, RTE &SL &- &\\

				   &FusionNet~\cite{wang2022fusionnet} &ICRA &Cross-Sensor  &CNNs+PointNet &$\mathcal{L}_2$ loss &KITTI\cite{KITTI}  &MAE &SL &PyTorch &\checkmark\\

				   &RGKCNet~\cite{RGKCNet} &TIM &Cross-Sensor  &CNNs+PointNet &$\mathcal{L}_1$ loss &KITTI\cite{KITTI}  &MSE &SL &PyTorch &\checkmark\\


                      \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2023}}}
					% &1&

					&BinoStereo~\cite{BinoStereo} &TIV &Cross-View &CNNs	&Quaternion distance loss &KITTI~\cite{KITTI} &Mean error, SSIM & SL &- &\\

					&DPO-Net~\cite{DPO-Net} &ICCV &Cross-View &GNNs	&Negative log-likelihood, cosine similarity loss &ScanNet~\cite{ScanNet}, MegaDepth~\cite{MegaDepth} &AUC & SL &PyTorch &\\

					&RHWF~\cite{RHWF} &CVPR &Cross-View &CNNs+Transformers	&$\mathcal{L}_1$ loss &Google Earth, Google Map, MS-COCO~\cite{MS-COCO} &MACE &SL &PyTorch &\checkmark\\                    
					&EAC-Homo~\cite{EAC-Homo} &TCSVT &Cross-View &CNNs	&$\mathcal{L}_1$ loss &UDIS-D~\cite{UDIS}, MS-COCO~\cite{MS-COCO} &MACE, PSNR, SSIM & USL &PyTorch &\\

					&PLS-Homo~\cite{PLS-Homo} &TCSVT &Cross-View &CNNs	&Triple, $\mathcal{L}_1$ loss &CA-UDHN~\cite{CA-UDHN} &Point matching error & USL &PyTorch &\\

					&LBHomo~\cite{LBHomo} &AAAI &Cross-View &CNNs	&$\mathcal{L}_1$ loss &Self-constructed &Point matching error & Semi-SL &PyTorch &\\

					&RealSH~\cite{RealSH} &ICCV &Cross-View &CNNs	&$\mathcal{L}_1$ loss &MS-COCO~\cite{MS-COCO}, CA-UDHN~\cite{CA-UDHN}, GHOF~\cite{Gyroflow+} &Point matching error & SL &PyTorch &\\

					&SE-Calib~\cite{SE-Calib} &TGRS &Cross-Sensor &CNNs	&SSRCM score &KITTI~\cite{KITTI} &Reprojection error, mean error & SL &PyTorch &\checkmark\\

					&CM-GNN~\cite{CM-GNN} &TIM &Cross-Sensor &CNNs+GNNs	&Focal, smooth $\mathcal{L}_1$ loss &KITTI~\cite{KITTI} &Geodesic distance & SL &PyTorch &\checkmark\\

					&Calibdepth~\cite{Calibdepth} &ICRA &Cross-Sensor &CNNs+LSTMs	&Chamfer distance, smooth $\mathcal{L}_1$, berHu, geodesic distance loss &KITTI~\cite{KITTI} &Absolute error & SL &PyTorch &\checkmark\\

					&DEdgeNet~\cite{DEdgeNet} &ICRA &Cross-Sensor &CNNs	&$\mathcal{L}_2$ loss, quaternion distance loss &KITTI~\cite{KITTI} &Mean error & SL &- &\checkmark\\

					&MOISST~\cite{MOISST} &IROS &Cross-Sensor &MLPs	&$\mathcal{L}_2$ loss &KITTI-360~\cite{liao2022kitti} &Mean error & SL &- &\checkmark\\

					&ELR-Calib~\cite{ELR-Calib} &ICASSP &Cross-Sensor &CNNs	&Contrastive loss &RELLIS-3D~\cite{RELLIS-3D} &Averaged translation error and rotation error & SL &- &\checkmark\\

					&P2O-Calib~\cite{P2O-Calib} &IROS &Cross-Sensor &CNNs	&Cross-entropy loss &KITTI~\cite{KITTI} &Averaged translation error and rotation error & SL &Pytorch &\checkmark\\

					&SCNet~\cite{SCNet} &RAL &Cross-Sensor &CNNs+Transformers	&Smooth $\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI~\cite{KITTI}, nuScenes~\cite{caesar2020nuscenes} &Averaged translation error and rotation error & SL &Pytorch &\checkmark\\

					&RobustCalib~\cite{RobustCalib} &arXiv &Cross-Sensor &CNNs	&$\mathcal{L}_1$, cross-entropy loss &KITTI~\cite{KITTI}, nuScenes~\cite{caesar2020nuscenes} &Averaged translation error and rotation error & SL &- &\checkmark\\

					&PseudoCal~\cite{PseudoCal} &BMVC &Cross-Sensor &CNNs+Transformers	&$\mathcal{L}_2$ loss &KITTI~\cite{KITTI} &Averaged translation error and rotation error & SL &- &\checkmark\\

					&BatchCalib~\cite{BatchCalib} &CoRL &Cross-Sensor &CNNs	&Reprojection error &KITTI~\cite{KITTI} &Mean, median error  & SL &- &\checkmark\\
                    
                      \hline
				   \hline
				   \multirow{1}{*}{\rotatebox{0}{\textbf{2024}}}
					% &1&
					&NeuralRecalibration~\cite{NeuralRecalibration} &arXiv &Cross-View &Transformers+PointNet	&Geodesic, $\mathcal{L}_2$ loss & Self-constructed &RMSE& SL &PyTorch &\checkmark\\
				  
                  &ArcGeo~\cite{ArcGeo} &WACV &Cross-View &Transformers	&Cross-entropy loss & CVUSA~\cite{CVUSA}, CVACT~\cite{CVACT} &Top-K recall, mAR& SL &- &\\

                  &CalibRBEV~\cite{CalibRBEV} &ACM MM &Cross-View &CNNs+Transformers	&Focal, $\mathcal{L}_1$ loss & nuScenes~\cite{caesar2020nuscenes}, Waymo~\cite{Waymo} &mAP, NDS, mATE, mASE, etc& SL &- &\\

                  &FG-Rect~\cite{FG-Rect} &CVPR &Cross-View &CNNs+Transformers	&$\mathcal{L}_1$ loss & Semi-Truck Highway~\cite{FG-Rect}, Carla~\cite{FG-Rect} &MAE& SL &Pytorch &\\

                  &Mask-Homo~\cite{Mask-Homo} &AAAI &Cross-View &Transformers	&Triplet loss &CA-UDHN~\cite{CA-UDHN} &MSE, PSNR& USL &Pytorch &\\

                  &DSM-DHN~\cite{DSM-DHN} &RAL &Cross-View &CNNs	&$\mathcal{L}_1$ loss &UDIS-D~\cite{UDIS}, MS-COCO~\cite{MS-COCO} &RMSE, PSNR, SSIM& USL &- &\\

                  &DMHomo~\cite{DMHomo} &ToG &Cross-View &Diffusion	&Diffusion loss &CA-UDHN~\cite{CA-UDHN} &PME& USL &Pytorch &\\

                  &DPP-Homo~\cite{DPP-Homo} &ICASSP &Cross-View &CNNs	&$\mathcal{L}_1$ loss &UDIS-D~\cite{UDIS} &PSNR, SSIM& USL &Pytorch &\\

                  &DHE-VPR~\cite{DHE-VPR} &AAAI &Cross-View &Transformer	&Re-projection, triplet loss &Pitts30k~\cite{Pitts30k} &Recall& USL &Pytorch &\\

                  &SRMatcher~\cite{SRMatcher} &ACM MM &Cross-View &CNNs+Transformer	&Focal binary cross-entropy Loss &Oxford5K~\cite{Oxford5K}, Paris6K~\cite{Paris6K} &AUC& SSL &Pytorch &\\

                  &AbHE~\cite{AbHE} &TIM &Cross-View &CNNs+Transformer	&$\mathcal{L}_1$ loss &UDIS-D~\cite{UDIS}, MS-COCO~\cite{MS-COCO} &PSNR, SSIM& USL &TensorFlow &\\

                  &AGNet~\cite{AGNet} &TCSVT &Cross-View &CNNs+Transformer	&$\mathcal{L}_1$ loss &Google Earth, Google Map, MS-COCO~\cite{MS-COCO} &MACE& SL &Pytorch &\checkmark\\

                  &CrossHomo~\cite{CrossHomo} &TPAMI &Cross-View &CNNs	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss &MS-COCO~\cite{MS-COCO}, DPDN~\cite{DPDN} &RMSE, PSNR, SSIM& SL &Pytorch &\checkmark\\

                  &AltO~\cite{AltO} &NeurIPS &Cross-View &CNNs	&Barlow Twins loss &Google Earth, Google Map, DeepNIR~\cite{DeepNIR} &MACE & USL &Pytorch &\checkmark\\

                  &Gyroflow+~\cite{Gyroflow+} &IJCV &Cross-View &CNNs	&Triplet loss  &GOF~\cite{Gyroflow} &MACE & USL &Pytorch &\\

                  &HEN~\cite{HEN} &WACV &Cross-View &CNNs	&$\mathcal{L}_1$ loss  &MS-COCO~\cite{MS-COCO} &MAE & SL &TensorFlow &\checkmark\\

                  &JEDL-Homo~\cite{JEDL-Homo} &ICME &Cross-View &CNNs	&$\mathcal{L}_1$ loss  &MS-COCO~\cite{MS-COCO} &MACE & SL &Pytorch &\checkmark\\

                  &InterNet~\cite{InterNet} &arXiv &Cross-View &CNNs	&$\mathcal{L}_1$, $\mathcal{L}_2$ loss  &Google Map, DPDN~\cite{DPDN}, RGB/NIR~\cite{RGB/NIR} &MACE & SL &Pytorch &\checkmark\\

                  &STHN~\cite{STHN} &RAL &Cross-View &CNNs	&$\mathcal{L}_1$ loss  &Boson-nighttime~\cite{Boson-nighttime} &MACE, CE & SL &Pytorch &\\

                  &SCPNet~\cite{SCPNet} &ECCV &Cross-View &CNNs	&$\mathcal{L}_1$ loss &Google Map, RGB/NIR~\cite{RGB/NIR} &MACE &USL &PyTorch &\checkmark\\ 

                  &MCNet~\cite{MCNet} &CVPR &Cross-View &CNNs	&$\mathcal{L}_1$, FGO loss &Google Earth, Google Map, MS-COCO~\cite{MS-COCO} &MACE &SL &PyTorch &\checkmark\\ 

                  &CodingHomo~\cite{CodingHomo} &TCSVT &Cross-View &CNNs	&Triplet, binary cross-entropy, negative log likelihood loss &CA-UDHN~\cite{CA-UDHN}, GOF~\cite{Gyroflow} &Point matching error &USL &PyTorch &\\ 

                &SOAC~\cite{SOAC} &CVPR &Cross-Sensor &MLPs	& $\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI-360~\cite{liao2022kitti}, nuScenes~\cite{caesar2020nuscenes}, PandaSet~\cite{PandaSet} &Mean error & SL &- &\checkmark\\

                &UniCal~\cite{UniCal} &ECCV &Cross-Sensor &MLPs	& $\mathcal{L}_1$, $\mathcal{L}_2$, surface alignment loss &MS-Cal~\cite{UniCal}, PandaSet~\cite{PandaSet} &Re-projection error, Point-to-Plane distance, PSNR, SSIM, LPIPS & SL &- &\\

                &L2C-Calib~\cite{L2C-Calib} &TIM &Cross-Sensor &Transformers	& $\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI~\cite{KITTI} &Mean error, median error & SL &- &\checkmark\\

                &CalibFormer~\cite{CalibFormer} &ICRA &Cross-Sensor &Transformers	& Smooth $\mathcal{L}_1$, angular distance loss &KITTI~\cite{KITTI} &Mean error & SL &- &\checkmark\\

                &LCCRAFT~\cite{LCCRAFT} &ICRA &Cross-Sensor &CNNs	&Smooth $\mathcal{L}_1$, $\mathcal{L}_2$ loss &KITTI~\cite{KITTI} &Mean error & SL &PyTorch &\checkmark\\

                &SGCalib~\cite{SGCalib} &ICRA &Cross-Sensor &CNNs	&$\mathcal{L}_2$ loss &KITTI~\cite{KITTI} &Mean error & SL &- &\checkmark\\

                &LCANet~\cite{LCANet} &TITS &Cross-Sensor &CNNs+Transformers	&Smooth$\mathcal{L}_1$ loss &KITTI~\cite{KITTI} &Mean error & SL &PyTorch &\checkmark\\

                &SAM-Calib~\cite{SAM-Calib} &ICRA &Cross-Sensor &Transformers	&Cost function &KITTI~\cite{KITTI}, nuScenes~\cite{caesar2020nuscenes} &Mean error & SL &PyTorch &\checkmark\\

                &HIFMNet~\cite{HIFMNet} &ICRA &Cross-Sensor &CNNs	&Quaternion distance, smooth $\mathcal{L}_1$ loss  &KITTI~\cite{KITTI} &Mean error & SL &- &\checkmark\\

                &SensorX2Vehicle~\cite{SensorX2Vehicle} &RAL &Cross-Sensor &CNNs+Transformers	&Cross-entropy, cosine similarity, $\mathcal{L}_1$ loss  &KITTI~\cite{KITTI}, nuScenes~\cite{caesar2020nuscenes} &Mean error & SL &PyTorch &\checkmark\\

                &Edgecalib~\cite{Edgecalib} &RAL &Cross-Sensor &Transformers	&Projection function  &KITTI~\cite{KITTI} &Mean error & SL &- &\checkmark\\
                   
				\hline
				\end{tabular}
			}
		\end{threeparttable}
	\end{table*}
	
	
	
	
	
	
	
	
	
	
	
	
	
