\section{Development Recap}
\subsection{Milestone}
A concrete milestone from 2015 to 2024 of deep learning-based camera calibration is shown in Figure \ref{fig:milestones}, spanning the main deep learning era. We classify all literature based on the uncalibrated camera model and its extended applications: standard model, distortion model, cross-view model, and cross-sensor model.

\begin{figure*}[htbp]
	\centering  \centerline{\includegraphics[width=1\linewidth]{figures/time_line_new.pdf}}
	\caption{A concise milestone of deep learning-based camera calibration methods. We classify all methods based on the uncalibrated camera model and its extended applications: standard model, distortion model, cross-view model, and cross-sensor model.}
	\label{fig:milestones}
\end{figure*}

\subsection{Statistic Analysis}
As we can observe in Figure~\ref{fig:publication_number}, the number of learning-based camera calibrations has grown since 2015 and boomed since 2019. And the learning targets are extended from the simple and pure parameters to complicated and hybrid parameters, driven by larger datasets, more reasonable learning strategies, more explicit learning representations, and more solid network architectures, etc.
\begin{figure*}[t]
  \centering
  \includegraphics[width=.95\textwidth]{figures/publication_number_new.pdf}
  %\vspace{-20pt}
  \caption{A statistic analysis of deep learning-based camera calibration methods. Specifically, we summarize all literature based on the number of publications per year, calibration objectives, simulation of the dataset, and learning strategy.}
  \label{fig:publication_number}
  %\vspace{-0.3cm}
\end{figure*}

The data analysis of different learning strategies used in learning-based camera calibration is also shown in Figure~\ref{fig:publication_number}. From the statistics, six strategies have been investigated, in which supervised learning accounts for the largest majority (more than 90\%). Considering the expensive labeling works, some recent research explores liberating the training demand for camera parameters using semi-supervised learning, weakly-supervised learning, unsupervised learning, and self-supervised learning. Reinforcement learning also has been exploited to dynamically address the camera calibration problem. 

\section{Camera Model}
Researchers utilize mathematical formulas to establish camera models that describe the imaging process from a point in 3D world coordinates to its projection on a 2D image plane. Different cameras and systems correspond to different types of parametric models. In this section, we first provide a detailed formulation of the basic pinhole camera model. Then, we review more complex and useful camera models, as well as extended models studied in recent literature, to meet the advanced development of cameras and academic/industrial demands.

\subsection{Pinhole Camera Model}
The most popular and commonly applied camera model in computer vision is the pinhole camera model. It can be regarded as a geometrically accurate first-order approximation of the traditional camera. A pinhole camera has one single effective perspective because the pinhole aperture is thought to be an infinitesimal point through which all projection lines must pass.

Using a mathematical formulation, the camera model depicts the imaging process from a point in the 3D world coordinate to its projection on the 2D image plane. Assuming the homogeneous coordinates $\mathbf{P}_w = [X, Y, Z, 1]^\mathsf{T}  \in {\mathbb{R}}^{4\times1}$ and $\mathbf{P}_i =  [u, v, 1]^\mathsf{T} \in {\mathbb{R}}^{3\times1}$ denote a point in the 3D world coordinate and its corresponding point on a 2D image plane, respectively. Then, a camera model can be described by a projection mapping $M \in {\mathbb{R}}^{3\times4}$ between $\mathbf{P}_w$ and $\mathbf{P}_i$:
\begin{equation}\label{eq-general-camera-model}
\mathbf{P}_i =  M\mathbf{P}_w,
\end{equation}
where the projection can be further formed by:
\begin{equation}\label{eq-projection-ex}
\mathbf{P}_c = [\mathbf{R}\,|\,\mathbf{t}]\mathbf{P}_w,
\end{equation}
where $\mathbf{P}_c = [x_c, y_c, z_c]^\mathsf{T} \in {\mathbb{R}}^{3\times1}$ denotes a transformed point in the camera coordinate using a $3\times3$ rotation $\mathbf{R}$ and a $3$-dimension translation $\mathbf{t}$. The $3 \times 4$ matrix $[\mathbf{R}\,|\,\mathbf{t}]$ is generally named as the extrinsic camera matrix, in which the camera rotation can be further parameterized by three angles: yaw $\varphi$, pitch $\theta$, and roll $\psi$. Subsequently, the point $\mathbf{P}_c$ is projected onto a surface. This surface is represented by the pinhole camera model as a plane $z = 1$, and the normalized coordinate of the point in camera coordinate is expressed by $[x_n, y_n]^\mathsf{T} = [\frac{x_c}{z_c}, \frac{y_c}{z_c}]^\mathsf{T}$. 

Finally, the point on the normalized plane is projected onto the image plane, obtaining a pixel $\mathbf{P}_i$ by:
\begin{equation}
    \mathbf{P}_i = K [x_n, y_n, 1]^\mathsf{T},
\label{eq-projection-in}
\end{equation}
where $K\in {\mathbb{R}}^{3\times3}$ is an intrinsic camera matrix, which consists of various camera intrinsic parameters such as the focal length, skew coefficient, and image center:
\begin{equation}
K = \begin{bmatrix} f_x m_u & s & c_u \\ 
                                      0 & f_y m_v & c_v \\
                                      0 & 0 & 1\end{bmatrix},
\label{eq-intrincs}
\end{equation}
where $f_x$ and $f_y$ are the focal lengths at X-axis and Y-axis of the camera, respectively. Generally, for most cameras, $f_x = f_y$, and they are unified to $f$. $m_u$ and $m_v$ are the number of pixels per unit distance, in which $m_u = m_v$, if the image has square pixels. $s$ is the skew coefficient. A CCD sensor's pixels might not be precisely square, which would cause a slight distortion in the X or Y axes. The number of pixels on the CCD sensor per unit length in each direction is known as the skew coefficient. It would become $0$ when X-axis and Y-axis are perpendicular to each other. $[c_u, c_v]^\mathsf{T}$ is the coordinate of the image center. According to previous works and factory design, the intrinsic parameters can be refined by $s=0, m_u=m_v$ and focal length in the pixel unit, then Eq.~\eqref{eq-projection-in} can be reformulated as:    
\begin{equation}
    \mathbf{P}_i = \begin{bmatrix} f_x & 0 & c_u \\ 
                                      0 & f_y & c_v \\
                                      0 & 0 & 1\end{bmatrix} [x_n, y_n, 1]^\mathsf{T}.
\label{eq-projection-in_refined}
\end{equation}

In addition to numerical camera parameters, some geometric representations can provide useful clues for camera calibration, such as vanishing points and horizon lines. These representations establish clear relationships between image features and calibration objectives, which can alleviate the difficulty of learning conventional and implicit camera parameters.

Lines and points are both represented as three-dimensional vectors in homogeneous coordinates. The definitions for computing the line $\mathbf{l}$ that connects two points and the point $\mathbf{p}$ at the intersection of two lines can be given by:
\begin{equation}
  \mathbf{l} = \frac{\mathbf{p}_1 \times \mathbf{p}_2}{||\mathbf{p}_1 \times \mathbf{p}_2||}   \ \ \ \ \ \ \ \   \mathbf{p} = \frac{\mathbf{l}_1 \times \mathbf{l}_2}{||\mathbf{l}_1 \times \mathbf{l}_2||}
\end{equation}

There are two parameterizations of the horizon line: slope-offset ($\theta, \rho$) and left-right ($l, r$). Assuming that the viewing orientation is down the negative $z$-axis, with the positive $x$-direction to the right, and the positive $y$-direction to the up. As a result, the world viewing direction of the camera can be described by $R_c^\mathsf{T}[0,0,-1]^\mathsf{T}$. For the world vector $[0,1,0]^\mathsf{T}$ points in the zenith direction, a set of points $p$ can represent the horizon line:
\begin{equation}
  p^\mathsf{T}K^{-T}R[0,1,0]^\mathsf{T} = 0.
\end{equation}

As mentioned in Barnard \cite{barnard1983interpreting}, the normalized line direction vector $\mathbf{d}$ can be formulated for the Gaussian sphere representation of a vanishing point $\mathbf{v}$. In particular, supposed a 3D ray is described by $\mathbf{o} + \lambda \mathbf{d}$, where $\mathbf{o}$ and $\mathbf{d}$ are its origin and unit direction vector, respectively. Then, the vanishing point can be represented by $\lambda \to \infty$, in which the image coordinate is formed by $\mathbf{v} = [v_x, v_y]^T := \lim_{\lambda \to \infty} [p_x, p_y]^T\in \mathbb{R}^2$. Thus, the 3D direction of a line based on its vanishing point can be calculated by:
\begin{equation}
  \mathbf{d} = \begin{bmatrix} v_x-c_x & v_y-c_y & f \end{bmatrix}^T \in \mathbb{R}^3.  %
\end{equation}
By using $\mathbf{d}$ rather than $\mathbf{v}$, the degraded situations where $\mathbf{d}$ is parallel to the image plane are eliminated. Additionally, it provides a natural measurement for determining the separation between two vanishing points.

\subsection{Wide-angle Camera Model}
The perspective projection model, given a typical pinhole camera with focal length $f$, can be expressed as:
\begin{equation}
r = f \tan \theta, 
\label{eq-projection-ray}
\end{equation}
where $r$ denotes the projection distance between the principal point and the points in the image. $\theta$ denotes the angle between the incident ray and the optical axis of the camera. It is straightforward to determine that $\theta$ should be less than $90^{\circ}$. Without a projection point on the image plane, the incoming ray will not cross with the image plane and the pinhole camera will not be able to view anything behind. Because of their restricted field of view (FoV), most cameras cannot see all of the points in the 3D environment at the same time.

Due to the wide FoV, wide-angle cameras are increasingly widely used in computer vision and robotics tasks such as navigation, localization, and tracking. Specifically, an extra wide-angle lens called a fisheye camera is used to create a broad, hemispherical, or panoramic image. Fisheye lenses employ a specific mapping to produce convex and non-rectilinear images as opposed to images with straight lines of perspective (rectilinear images). However, the wide-angle camera violates the pinhole camera assumption and the captured image suffers from geometric distortions.

Geometric distortion induced by wide-angle cameras can generally be classified into radial distortion and tangential distortion (de-centering distortion). Radial distortion is the primary distortion in central single-view camera systems, exhibiting circular symmetry with respect to the distortion center. This distortion results in points on the image plane being moved away from their ideal location under the perspective camera model along the radial axis from the distortion center. Radial distortion models can be formulated as nonlinear functions of the radial distance \cite{fan2022wide}. On the other hand, tangential distortion occurs when the lens and image plane are not parallel. Tangential distortion, also known as de-centering distortion, is primarily caused by the lens assembly not being centered over and parallel to the image plane. Unlike radial distortion, tangential distortion has a geometric impact that is not solely along the radial axis, and can also cause rotation and skewing of the image plane with respect to the distance from the image center. The camera model with radial distortion and tangential distortion can be parameterized by:
\begin{equation}
 \left\{\begin{matrix}
    x_r &= x_d + \Bar{x}(k_1 r_d^2 + k_2 r_d^4 + k_3 r_d^6 + \cdots) \\ 
    &+ (p_1 (r_d^2 + 2\Bar{x}^2) + 2p_2\Bar{x}\Bar{y})(1+p_3r_d^2 + \cdots)
    \\ 
    y_r &= y_d + \Bar{y}(k_1 r_d^2 + k_2 r_d^4 + k_3 r_d^6 + \cdots) \\ 
    &+ (p_2 (r_d^2 + 2\Bar{y}^2) + 2p_1\Bar{x}\Bar{y})(1+p_3r_d^2 + \cdots)
    \end{matrix}\right.,   
    \label{eq-wide-angle}
\end{equation}
where $\Bar{x} = x_d - c_x$ and $\Bar{y} = y_d - c_y$. $K = (k_1, k_2, k_3, \dots)$ and $P = (p_1, p_2, p_3, \dots)$ are the radial distortion parameters and decentering distortion parameters, respectively. $r_d$ describes the radial distance from an image point to the distortion center $(c_x, c_y)$. Such an equation represents the mapping from a point $[x_d, y_d]^\mathsf{T}$ in the image captured by the wide-angle camera to that in the rectified image without the geometric distortion $[x_r, y_r]^\mathsf{T}$.

Previous works demonstrate that tangential distortion is basically insignificant and can be neglected. Moreover, as we surveyed, all learning-based camera calibration methods only consider the radial distortion for calibrating the wide-angle camera. To this end, Eq.\ref{eq-wide-angle} can be simplified by a Taylor expansion:
\begin{equation}
 \left\{\begin{matrix}
    x_r = x_d(k_1 r_d^2 + k_2 r_d^4 + k_3 r_d^6 + \cdots)
    \\ 
    y_r = y_d(k_1 r_d^2 + k_2 r_d^4 + k_3 r_d^6 + \cdots)
    \end{matrix}\right..   
    \label{eq-wide-angle-polynomial}
\end{equation}
This equation is known as the even-order polynomial model, which can also be expressed as an odd-order polynomial model by shifting the power. However, according to Wang et al. \cite{wang2009simple}, while the polynomial model is suitable for small distortions, it requires an unreasonably high number of non-zero distortion parameters for severe distortions. As an alternative, Fitzgibbon et al. \cite{fitzgibbon2001simultaneous} proposed a division model that more accurately approximates the genuine undistortion function of a common camera. For significant distortion, the division model is preferred over the polynomial model because it requires fewer terms:
\begin{equation}
 \left\{\begin{matrix}
    x_r = \frac{x_d}{k_1 r_d^2 + k_2 r_d^4 + k_3 r_d^6 + \cdots}
    \\ 
    y_r = \frac{y_d}{k_1 r_d^2 + k_2 r_d^4 + k_3 r_d^6 + \cdots}
    \end{matrix}\right..   
    \label{eq-wide-angle-division}
\end{equation}

Some classical works demonstrate the single-parameter division model (only with distortion parameter $k_1$ in Eq.\ref{eq-wide-angle-division}) seems to be sufficient for most wide-angle cameras, which has been widely applied in learning-based wide-angle camera calibration \cite{Rong, BlindCor, DeepCalib, DQN-RecNet}.

\begin{figure}[t]
  \centering
  \includegraphics[width=.4\textwidth]{figures/global-rolling-shutter.pdf}
  %\vspace{-20pt}
  \caption{Comparison of the mechanism of global shutter camera and rolling shutter camera.}
  \label{fig:global-rolling-shutter}
  %\vspace{-0.3cm}
\end{figure}

\subsection{Rolling Shutter Camera Model}
Due to the compact design, low price, and high frame rate, numerous consumer cameras, including webcams and mobile phones, employ CMOS (complementary metal–oxide–semiconductor) sensors. However, they are restricted to using rolling shutter (RS) devices. With a consistent time delay between each row, RS exposes the sensor array row by row from top to bottom, as opposed to global shutter (GS) based on CCD sensors, which simultaneously read out all rows of the sensor array. If the RS camera is moving while capturing the image, various distortions, such as skew, smear, or wobble, will break the reality of the original scene, which deviates from the pinhole camera paradigm. The unknown camera movements during the capturing process induce the so-called RS effects (also known as the jelly effect). In other words, an RS image is a row-by-row combination of GS images taken by a virtual moving GS camera throughout the camera readout time. The comparison of the RS camera and GS camera is shown in Figure~\ref{fig:global-rolling-shutter}.

The RS camera can be regarded as a high-frequency sensor that produces sparse spatial information with rich temporal coverage conveyed by distortions \cite{ait2006simultaneous}. Modeling the RS camera faces a common challenge of estimating the transformation between RS and GS images. Assume a 3D latent space-time volume captures the desired scene across the desired time period $[0, t_0]$ and creates a virtual GS image $\mathbf{I}^{\rm{GS}}$. We suppose the readout direction is from top to bottom, and then the row-by-row readout RS imaging $\mathbf{I}^{\rm{RS}}$ can be expressed by:
\begin{equation}
\mathbf{I}^{\rm{RS}} = \sum_{y=1}^H M(\mathbf{I}^{\rm{GS}}_{t_r}, y),
\label{eq-RS}
\end{equation}
where $H$ is the height of the RS image (total number of rows) and $y$ indicates the vertical coordinate. $M(\cdot, \cdot)$ masks a specific row in the GS image, in which $t_r$ represents the readout (offset) time for each row of RS.

On the other hand, by warping the RS features backward with an estimated displacement field, the GS image can be formulated by:
\begin{equation}
    \mathbf{I}^{\rm{GS}}(\mathbf{x}) = \mathbf{I}^{\rm{RS}}(\mathbf{x}+\mathbf{F}_{{GS}\rightarrow {RS}}(\mathbf{x})),
\label{eq-GS}
\end{equation}
where $\mathbf{F}_{{GS}\rightarrow {RS}} \in {\mathbb{R}}^2$ denotes the displacement field of the pixel $\mathbf{x}$ from the GS image to the RS image.

The above formulations describe the rolling shutter camera model under a short exposure scenario. When the exposure time of the camera increases, the motion blur effects occur in the captured image, jointly with the RS distortion:
\begin{equation}\label{eq:rscd}
	\mathbf{I}^{\rm{RS'}}_{(t)}[i] = \frac{1}{T}\int_{t-t_h+it_r-T/2}^{t-t_h+it_r+T/2}\mathbf{I}^{\rm{GS}}_{(t-t_h+it_r)}[i] dt,
\end{equation}
where $\mathbf{I}^{\rm{RS'}}_{(t)}[i]$ denotes the $i^{th}$ row of the RS distortion image $\mathbf{I}^{\rm{RS}}$ with the middle moment of exposure at time $t$. $T$ indicates the exposure time of camera and $t_h = (H/2)t_r$.

\subsection{Cross-View Camera Model}
The cross-view camera model is a type of multi-view camera system used in computer vision. It involves placing two or more cameras at opposite sides of a scene to capture multiple views of the same scene. This setup enables the creation of 3D reconstructions of the scene by triangulating corresponding points from multiple camera views. The cross-view camera model is commonly used in surveillance, robotics, and augmented reality applications, and provides a more accurate and complete representation of the scene than what can be achieved with a single camera. Alternatively, a camera with stable movement can also be regarded as a cross-view camera model.

In a cross-view camera model, the captured images can be used to calculate the fundamental matrix and homography matrix, which are essential tools for 3D reconstruction, image rectification, and camera calibration.

\noindent \textbf{Fundamental Matrix}
Geometric relationships between the 3D points and their projections onto the 2D plane impose constraints on the image points when two cameras capture the same 3D scene from different perspectives. This intrinsic projective geometry can be embodied by a fundamental matrix $\textbf{F}$.
\begin{equation}\label{eq-fundamental}
    \mathbf{F} = \mathbf{K_2}^{-T} [\mathbf{t}]_{\times} \mathbf{R}  \mathbf{K_1}^{-1}.
\end{equation}
Such an equation describes the epipolar geometry, where $\mathbf{K_1}$ and $\mathbf{K_2}$ indicate the intrinsic parameters of two cameras, and $\mathbf{R}$ and $[\mathbf{t}]_{\times}$ are the relative camera rotation and translation, respectively.

The fundamental matrix can be calculated from the correspondences of projected scene points by $q^T \mathbf{F} p = 0$, in which $q$ and $p$ are the matching points derived from two views. Specifically, the eight-point algorithm \cite{longuet1981computer} uses 8 point correspondences and enforces the rank-2 constraint using Singular Value
Decomposition (SVD), computing a matrix with the minimum Frobenius distance.

\noindent \textbf{Homography Matrix}
Estimating a 2D homography matrix (or projection transformation) is an elemental geometric task for a pair of images that are captured from the same planar surface in a 3D scene with different perspectives. An invertible mapping from one image plane to another with eight degrees of freedom: two each for translation, rotation, scale, and lines at infinity, is known as a homography. Supposed that the homogeneous coordinates $\mathbf{x} = [u, v, 1]^\mathsf{T}  \in {\mathbb{R}}^{3\times1}$ and $\mathbf{x}' = [u', v', 1]^\mathsf{T}  \in {\mathbb{R}}^{3\times1}$ are points from two images but indicating the same point in the 3D scene, a non-singular $3\times3$ matrix can represent a linear transformation that maps $\mathbf{x} \Leftrightarrow \mathbf{x}'$ as a planar projective transformation or homography $\mathbf{H}$:
\begin{align}
			\begin{bmatrix}
            u' \\ 
            v'   \\ 
            1  
			\end{bmatrix} 
          & \sim \begin{bmatrix}
            h_{11} & h_{12} & h_{13} \\ 
            h_{21} & h_{22} & h_{23} \\ 
            h_{31} & h_{32} & h_{33} 
			\end{bmatrix} 
            \begin{bmatrix}
            u \\ 
            v  \\ 
            1  
			\end{bmatrix}, 
\label{eq-homography}
\end{align}
where the transformation can be simplified as $\mathbf{x}' \sim \mathbf{H} \mathbf{x}$. This transformation can be rewritten by two following equations:
\begin{align}
 u' = \frac{h_{11}u + h_{12}v + h_{13}}{h_{31}u + h_{32}v + h_{33}} ;  
 v' = \frac{h_{21}u + h_{22}v + h_{23}}{h_{31}u + h_{32}v + h_{33}}.
 \label{eq-homography1}
\end{align}

Previous methods \cite{DHN, UDHN} point out that the above conventional $3\times3$ parameterization $\mathbf{H}$ is not desirable for training neural networks. Concretely, it is challenging to guarantee the non-singularity of $\mathbf{H}$ due to the significant variance in the size of the members of the $3\times3$ homography matrix. Moreover, the rotation, translation, scale, and shear components of the homography transformation are mixed in $\mathbf{H}$. For instance, the submatrix $[h_{11}\ \ h_{12}; h_{21}\ \ h_{22}]$ describes the homography's rotational term and the vector $[h_{13}, h_{23}]^T$ denotes the translation transformation. Considering the rotation and shear components typically have smaller magnitudes than the translation component, it will have a negligible impact on the loss function of the component elements, leading to an imbalance training problem with a neural network. Instead, a 4-point parameterization \cite{baker2006parameterizing} has been demonstrated to be more learning-friendly for learning-based homography estimation than the $3\times3$ parameterization. 
Supposed that the offsets of the image's vertex are $\Delta u_i = u_i' - u_i$ and $\Delta v_i = v_i' - v_i$, then the 4-point parameterization $\mathbf{\widetilde H}$ can describe a homography by:

\begin{equation}
\mathbf{\widetilde H} = \begin{pmatrix} \Delta u_{1} & \Delta v_{1} \\  \Delta u_{2} & \Delta v_{2}   \\  \Delta u_{3} & \Delta v_{3} \\  \Delta u_{4} & \Delta v_{4}  \end{pmatrix}. 
\end{equation}
The 4-point parameterization owns eight variables, which are equivalent to the matrix formulation of the homography. It is straightforward to transform from $\mathbf{\widetilde H}$ to $\mathbf{H}$ using the normalized Direct Linear Transform (DLT)~\cite{horn1987direct} if the four corners' displacement is known.

\subsection{Cross-Sensor Model}
Modern robots are often equipped with various sensors to provide a comprehensive understanding of the environment. These sensors capture scenes using different types of representations. For autonomous cars and robotics, cameras and Light Detection and Ranging sensors (LiDAR) are commonly used for vision tasks. The 3D LiDAR records long-range spatial data as sparse point clouds, while the camera captures texturally dense 2D color RGB images. Combining these sensors can facilitate 3D reconstruction and provide precise and robust perception for the robots, overcoming the limitations of individual sensors.

However, collision and vibration problems can occur when using different sensors in a robot or system. Additionally, the 3D point clouds cannot be effectively projected onto a 2D image without accurate extrinsic parameters, making it difficult to reliably correlate pixels in an image with depth information. Therefore, it is crucial to precisely calibrate the 2D-3D matching correspondences between pairs of temporally synchronized camera and LiDAR data.

The appropriate extrinsic calibration of the transformation (\textit{i.e.}, rotation and translation) between the camera and LiDAR in 6-DoF is a key condition for data fusion. To be more specific, 3D LiDAR point cloud $PC = [X, Y, Z] \in \mathbb{R}^3$ can be projected onto the image plane by transforming it into the camera coordinate using the extrinsic matrix $T$ between the camera and LiDAR as well as camera intrinsic $K$. The inverse depth and the projected 2D coordinates can be represented as $d = 1 / Z$ and $p = [u, v] \in \mathbb{R}^2$, respectively. Then, the camera-LiDAR model can be described by:
\begin{equation}
\begin{bmatrix}
    u\\ 
    v\\ 
    d
    \end{bmatrix}
    = 
    \begin{bmatrix}
        f_x(\hat{X}/\hat{Z}) + c_x \\
        f_y (\hat{Y}/\hat{Z}) + c_y \\
        1/\hat{Z}
    \end{bmatrix},
\label{eq-camera-LiDAR}
\end{equation}
where $(f_x, f_y)$ and $(c_x, c_y)$ indicate the focal lengths and the image center as listed in Eq. \ref{eq-intrincs}. $[\hat{X}, \hat{Y}, \hat{Z}]$ is the transformed point cloud $\hat{PC}$ using the estimated extrinsic matrix:
\begin{equation}
    [\hat{X}, \hat{Y}, \hat{Z}, 1]^\mathsf{T}= T [X, Y, Z, 1]^\mathsf{T}.
\end{equation}

Most deep learning works exploit the Lie algebra to parameterize the calibration camera-LiDAR extrinsic parameters. In particular, the output of the calibration network is a 1 $\times$ 6 vector $\xi = (v, \omega) \in se(3)$ in which $v$ is the translation vector, and $\omega$ is the rotation vector. To recover the original objectives, the rotation vector in $so(3)$ should be transformed to its corresponding rotation matrix. Supposed that $\omega = (\omega_1, \omega_2, \omega_3)^T$, an element $\omega \in so(3)$ can be transformed to $SO(3)$ using the exponential map by:
\begin{equation}
exp: so(3) \rightarrow SO(3); \; \hat{\omega} \mapsto e^{\hat{\omega}},
\end{equation}
where $\hat{\omega}$ and $e^{\hat{\omega}}$ denote the skew-symmetric matrix from $\omega$ and Taylor series expansion for the matrix exponential function, respectively. Then, the rotation matrix can be formed in $SO(3)$, and its Rodrigues formula is derived from the above equation by:
\begin{equation}
R = e^{\hat{\omega}} = I + \frac{\hat{\omega}}{\Vert{\omega}\Vert}\sin{\Vert{\omega}\Vert} + \frac{\hat{\omega}^2}{\Vert{\omega}\Vert^2}(1 - \cos(\Vert\omega\Vert)).
\end{equation}
Thus, the 3D rigid body transformation $T \in SE(3)$ between camera and LiDAR can be represented by:
\begin{equation}
   T = \left( \begin{array}{cc} R & t \\ 0 & 1 \end{array}\right) \text{where } R \in SO(3), t\triangleq v \in \mathbb{R}^3.
\end{equation}


\begin{table*}[t!]
\renewcommand{\arraystretch}{0.9}
\caption{Quantitative evaluation on the representative camera calibration methods for the standard model.
}
\label{tab:standard_eva}
{\footnotesize
\begin{tabularx}{\textwidth}{>{\centering}m{0.3cm}|>{\centering}m{0.6cm}|>{\centering}m{0.6cm}|>{\centering}m{0.6cm}|CC|CC|CC|CC|C}
\toprule
\multicolumn{4}{l|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{Up Direction ($^\circ$) $\downarrow$}  & \multicolumn{2}{c|}{Pitch ($^\circ$) $\downarrow$} & \multicolumn{2}{c|}{Roll ($^\circ$) $\downarrow$} & \multicolumn{2}{c|}{FoV ($^\circ$) $\downarrow$}  & \multirow{2}{*}{\makecell{AUC\\($\%$) $\uparrow$}} \\
\cline{5-12}
\multicolumn{4}{l|}{} & Mean & Med. & Mean & Med. & Mean & Med. & Mean & Med. \\ 
\midrule
\multicolumn{4}{l|}{DeepHorizon~\cite{DeepHorizon}}         &  3.58 &  3.01 &  2.76 &  2.12 &  1.78 &  1.67 &   -   &   -    & 80.29 \\
\multicolumn{4}{l|}{Hold-Geoffroy et al.~\cite{Hold-Geoffroy}}    &  2.73 &  2.13 &  2.39 &  1.78 &  0.96 &  0.66 & 4.61 &  3.89  & 80.40 \\
\multicolumn{4}{l|}{UprightNet~\cite{UprightNet}}             & 28.20 & 26.10 & 26.56 & 24.56 &  6.22 &  4.33 &   -   &   -   &   -   \\
\multicolumn{4}{l|}{Lee et al.~\cite{Lee}}              & 2.12 & 1.61 & 1.92 & 1.38 & 0.75 & {0.47} &  6.01 & 3.72 & 83.12 \\
\multicolumn{4}{l|}{{CTRL-C}~\cite{CTRL-C}}                    & {1.80} & {1.52} & {1.58} & {1.31} & {0.66} & 0.53 & {3.59} & {2.72} & {87.29} \\
\multicolumn{4}{l|}{{PerspectiveField}~\cite{PerspectiveField}}                    & {-} & {-} & {1.36} & {1.18} & {0.66} & 0.52 & {3.07} & {2.33} & {-} \\
\bottomrule
\end{tabularx}
}
\end{table*} 




\begin{table*}[htbp]
  \centering
  \caption{Quantitative evaluation on the representative camera calibration methods for distortion model.}
  %\vspace{-0.2cm}
    \begin{tabular}{lcccccccccc}
    \toprule
    \multicolumn{3}{c}{Comparison on Test1} &       & \multicolumn{7}{c}{Metrics} \\
\cmidrule{1-3}\cmidrule{5-11}    Methods &       & \multicolumn{1}{c}{Type} &       & \multicolumn{1}{c}{PSNR $\uparrow$} & \multicolumn{1}{c}{SSIM $\uparrow$ } & \multicolumn{1}{c}{MS-SSIM $\uparrow$} &       & \multicolumn{1}{c}{FID $\downarrow$} & \multicolumn{1}{c}{LPIPS-Alex $\downarrow$} & \multicolumn{1}{c}{LPIPS-Vgg $\downarrow$} \\
\cmidrule{1-1}\cmidrule{3-3}\cmidrule{5-7}\cmidrule{9-11}    

DCCNN \cite{Rong} &       &   Regression    &       &  9.18     &   0.1583    &   0.1617    &       &  360.90     &   0.4081    & 0.5258 \\

DeepCalib \cite{DeepCalib} &       &  Regression     &       &  10.71     &   0.2289    &   0.2534    &       &   161.31    &   0.3330    & 0.4482 \\

FishFormer \cite{FishFormer} &       &  Regression     &       &  -     &   -    &   -    &       &   -    &   -    & - \\

\midrule
BlindCor \cite{BlindCor} &       &   Reconstruction    &       &  8.02     &   0.1356    &   0.1392    &       &  400.77     &   0.4956    & 0.5536 \\

DR-GAN \cite{DR-GAN}&       &   Reconstruction    &       &  16.81     &  0.5058     &   0.6794    &       & 148.50     &   0.1782    & 0.3268 \\

DDM \cite{DDM}&       &    Reconstruction   &       &  17.43     &   0.5659    &  0.7191     &       &  125.34     &    0.1455   &  0.2333\\
PCN \cite{PCN} &      &   Reconstruction    &       &  24.59     &  0.8726     &   0.9594    &       &  42.78     &   0.0388    & 0.0671 \\

\midrule
\midrule
 \multicolumn{3}{c}{Comparison on Test2} &       & \multicolumn{7}{c}{Metrics} \\
\cmidrule{1-3}\cmidrule{5-11}    Methods &       & \multicolumn{1}{c}{Type} &       & \multicolumn{1}{c}{PSNR $\uparrow$} & \multicolumn{1}{c}{SSIM $\uparrow$ } & \multicolumn{1}{c}{MS-SSIM $\uparrow$} &       & \multicolumn{1}{c}{FID $\downarrow$} & \multicolumn{1}{c}{LPIPS-Alex $\downarrow$} & \multicolumn{1}{c}{LPIPS-Vgg $\downarrow$} \\
\cmidrule{1-1}\cmidrule{3-3}\cmidrule{5-7}\cmidrule{9-11}    

DCCNN \cite{Rong} &       &   Regression    &       &  15.23     &   0.4205    &   0.3609    &       &  125.99     &   0.1586    & 0.2110 \\

DeepCalib \cite{DeepCalib} &       &  Regression     &       &  11.47     &   0.2715    &   0.3655    &       &   139.26    &   0.2853    & 0.3886 \\

FishFormer \cite{FishFormer} &       &  Regression     &       &  21.52     &   0.7540    &   0.8971    &       &   66.37    &   0.1133    & 0.1107 \\

\midrule
BlindCor \cite{BlindCor}&       &   Reconstruction    &       &  12.01     & 0.3075     &   0.2531    &       & 133.16     &   0.3595    & 0.3907 \\

DR-GAN \cite{DR-GAN}&       &   Reconstruction    &       &  17.31     &  0.5133     &   0.6901    &       & 116.78     &   0.1218    & 0.2056 \\

DDM \cite{DDM}&       &    Reconstruction   &       &  18.84     &   0.6247    &  0.7614     &       &  78.38     &    0.1055   &  0.1894\\
PCN \cite{PCN} &      &   Reconstruction    &       &   21.28     &  0.7027     &   0.8595    &       &  54.64     &   0.0812    & 0.1088 \\

\bottomrule
    \end{tabular}%
  \label{tab:distortion_eva}%
\end{table*}%


\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{figures/crop_distortion_eva.pdf}
  \caption{Quantitative evaluation on the representative camera calibration methods for distortion model.}
  \label{fig:distortion_eva}
  %\vspace{-0.3cm}
\end{figure*}


\begin{table*}
\begin{center}
\caption{Quantitative evaluation on the representative methods for cross-view camera model.}
\label{tab:view_eva}
\begin{tabular}{c|cccc}
\hline
Method & MSCOCO~\cite{MS-COCO} & GoogleEarth~\cite{DLKFM} & GoogleMap~\cite{DLKFM} & CA-Homo~\cite{CA-UDHN}\\
\hline
\hline
MHN~\cite{MHN} & 1.1512 & 10.3078 & 13.1610 &-\\
MHN+DLKFM~\cite{DLKFM} & 0.7687 & 3.9629 & 5.2664 &-\\
IHN-scale1~\cite{IHN} & 0.2652 & 1.5135 & 0.9610 &-\\
IHN-scale2~\cite{IHN} & 0.1234 & 1.2110 & 0.6751 &- \\
CA-UDHN~\cite{CA-UDHN} & - & - & - & 0.8605\\
BasesHomo~\cite{BasesHomo} & - & - & - & 0.6808\\
HomoGAN~\cite{HomoGAN} & - & - & - & 0.3651\\
\hline
\end{tabular}
\end{center}
\end{table*}

\begin{table}[!t]
		\begin{center}
  		\caption{The point matching errors (PME) of representative methods for cross-view camera model.}
		\label{tab:view_eva1}
			\resizebox{0.98\linewidth}{!}{
				\begin{tabular}{rl|ccccccc}
					\toprule
					 & Methods & AVG & RE & FOG & LL & RAIN & SNOW \\
					\midrule
					 & $\mathcal{I}_{3\times3}$ & 6.33  & 4.94  & 7.24  & 8.09  & 5.48  & 5.89  \\
					\midrule
					 & CA-UDHN~\cite{CA-UDHN} & 3.87  & 4.10  & 3.84 & 6.99  & 1.27  & 3.17  \\
					 & BasesHomo~\cite{BasesHomo} & 2.28  & 2.02  & 1.43  & 4.90  & 0.78  & 2.29  \\
					 & HomoGAN~\cite{HomoGAN} & {1.95} & 1.73 & {0.60} & {3.95} & {0.47} & 3.02 \\
					\midrule
					 & DHN~\cite{DHN} & 6.61  & 6.04  & 6.02  & 7.68  & 6.99  & 6.32  \\
					 & LocalTrans~\cite{LocalTrans} & 5.72 & 4.06 & 6.49 & 5.95 & 5.78 & 6.34 \\
					 & IHN~\cite{IHN} & 8.17 & 7.10 & 8.71 & 9.34 & 6.57 & 9.13 \\
					\midrule
					 & $\textrm{DHN}^*$~\cite{DHN} & 3.01 & 1.92 & 3.94 & 4.54 & 1.98 & 2.66 \\
					 & $\textrm{LocalTrans}^*$~\cite{LocalTrans} & 2.89 & 1.78 & 4.27 & 4.59 & 1.37 & 2.43 \\
					 & $\textrm{IHN}^*$~\cite{IHN} & 2.59 & 2.21 & 3.05 & 4.70 & 0.98 & {2.03} \\ 
					 & RealSH~\cite{RealSH} & {1.72} & {1.60} & {0.88} & {4.42} & {0.43} & {1.28} \\
					\bottomrule
			\end{tabular}}
		\end{center}
	\end{table}%

\begin{table*}[htbp]
\caption{Quantitative evaluation on the representative calibration methods for Cross-Sensor model.}
\label{tab:cross-sensor}
\begin{center}
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Methods}}                 & \multicolumn{4}{c}{Translation (cm)} & \multicolumn{4}{c}{Rotation (${}^\circ   $)} \\
\multicolumn{2}{c}{} & ${{E}_{t}}$ & X      & Y     & Z     & ${{E}_{R}}$    & Roll    & Pitch   & Yaw     \\
\midrule
\multirow{3}{*}{CMRNet\cite{cattaneo2019cmrnet} $10{}^\circ   /2.0\operatorname{m}$} 
                        & KITTI~\cite{KITTI}                                    & 126.2820        & 72.8394         & 55.3118         & 77.1015        & 12.9863         & 5.7665        & 5.3867          & 3.9606          \\
                        & Apollo~\cite{huang2019apolloscape, L3NET_2019_CVPR}   & 136.7067        & 44.6239         & 109.4236        & 82.5008        & 12.2843         & 8.2973        & 6.6979          & 5.7054          \\
                        & NuScenes~\cite{caesar2020nuscenes}                    & 124.7204        & 43.6849         & 69.5890         & 64.1339        & 11.4339         & 9.9125        & 9.3100          & 5.3005           \\
                        & ONCE~\cite{mao2021one}                                &145.6419         & 83.0297         & 26.1744         & 70.9436        & 11.3011         & 7.2812        & 7.4463          & 3.2014          \\
                        & DAIR-V2X~\cite{yu2022dair}                            &  158.3873       & 51.8842         & 85.4628         & 79.1833        & 10.3119         & 3.1635        & 3.7430          & 5.5995          \\
                        \hline
\multirow{3}{*}{CalibNet\cite{iyer2018calibnet} $10{}^\circ   /0.2\operatorname{m}$}   
                        & KITTI~\cite{KITTI}                                    & 72.3976        & 37.4153         & 46.3160         & 30.4167        & 10.5577         & 4.8652        & 7.3470          & 5.4789          \\
                        & Apollo~\cite{huang2019apolloscape, L3NET_2019_CVPR}   & 71.6006        & 18.7960         & 50.2996         & 16.5990        & 11.0372         & 6.0806        & 5.4223          & 9.4484           \\
                        & NuScenes~\cite{caesar2020nuscenes}                    & 73.4821        & 52.6383         & 45.7675         & 27.4490        & 11.5573         & 8.0735        & 8.4622          & 5.6564          \\
                        & ONCE~\cite{mao2021one}                                & 74.0186        & 46.3395         & 49.3340         & 31.7829        & 12.4912         & 5.3240        & 5.3686          & 6.7609          \\
                        & DAIR-V2X~\cite{yu2022dair}                            & 77.3034        & 41.9963         & 54.7920         & 51.8221        & 11.6145         & 7.4061        & 1.7497          & 6.4258          \\
                        \hline
\multirow{3}{*}{LCCNet\cite{lv2021lccnet} $20{}^\circ   /1.5\operatorname{m}$} 
                        & KITTI~\cite{KITTI}                                    & 45.0945         & 9.2671          & 7.2877          & 11.6077        & 1.8611          & 0.6823        & 0.5427          & 0.2704          \\
                        & Apollo~\cite{huang2019apolloscape, L3NET_2019_CVPR}   & 159.9080        & 45.5809         & 67.7147         & 106.1134       & 15.1098         & 2.5412        & 3.8089          & 2.8912          \\
                        & NuScenes~\cite{caesar2020nuscenes}                    & 232.1684        & 73.4198         & 120.1943        & 98.3628        & 12.3146         & 2.5942        & 3.7928          & 9.0684          \\
                        & ONCE~\cite{mao2021one}                                & 185.0779        & 74.6479         & 72.9388         & 74.6713        & 11.5796         & 8.6251        & 6.2554          & 4.7137          \\
                        & DAIR-V2X~\cite{yu2022dair}                            & 228.1306        & 113.5558        & 86.6683         & 98.3726        & 7.3732          & 2.0581        & 3.7936          & 4.7334          \\
                        \hline
\multirow{3}{*}{RGGNet\cite{yuan2020rggnet} $20{}^\circ   /0.3\operatorname{m}$} 
                        & KITTI~\cite{KITTI}                                    & 77.7839        & 12.5584         & 14.4128         & 68.0927        & 9.8177         & 2.1106        & 2.6808          & 6.7095       \\
                        & Apollo~\cite{huang2019apolloscape, L3NET_2019_CVPR}   & 542.7074       & 215.6324        & 254.2824        & 218.8926       & 35.4403        & 17.8385       & 17.7508         & 18.2130       \\
                        & NuScenes~\cite{caesar2020nuscenes}                    & 92.3324        & 44.9838         & 51.1874         & 51.5143        & 35.5249        & 22.6364       & 25.5031         & 18.2290        \\
                        & ONCE~\cite{mao2021one}                                & 73.8569        & 55.6384         & 49.2977         & 33.9010        & 26.0675        & 6.7498        & 6.7093          & 20.2362          \\
                        & DAIR-V2X~\cite{yu2022dair}                            & 397.6241       & 260.4813        & 208.1069        & 187.8416       & 34.8505        & 22.0118       & 23.7622         & 18.6647         \\
\bottomrule                                                     
\end{tabular}
\end{center}
\end{table*}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{figures/cross_sensor_eva1.pdf}
  \caption{Qualitative evaluation on the representative calibration methods for the cross-sensor model.}
  \label{fig:cross_sensor_eva1}
  %\vspace{-0.3cm}
\end{figure*}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=1\textwidth]{figures/cross_sensor_eva2.pdf}
  \caption{Qualitative evaluation on the representative calibration methods for the cross-sensor model.}
  \label{fig:cross_sensor_eva2}
  %\vspace{-0.3cm}
\end{figure*}


\section{Benchmark Evaluation}
To intuitively demonstrate the effectiveness of the existing learning-based camera calibration methods, we provide quantitative and qualitative evaluations of representative methods using our benchmark. Considering the application gap and comparison integrity, we also collect and summarize the reported metrics from previous works of certain camera models.

As listed in Tab. ~\ref{tab:standard_eva}, the quantitative evaluations on standard model calibration methods are provided. They are summarized from previous methods~\cite{CTRL-C}~\cite{PerspectiveField}~\cite{Lee} and evaluated with the Google Street View dataset. The angular errors of predicted camera Up direction, pitch, roll, and FoV are calculated in terms of the mean error (\textit{Mean}) and median error (\textit{Med}). AUC denotes the area under the curve of the cumulative distribution of the horizon line error.

Tab.~\ref{tab:distortion_eva} and Fig.~\ref{fig:distortion_eva} show the quantitative evaluation and qualitative evaluation of representative works for calibrating the distortion camera model, especially in radial distortion. In particular, we split the benchmark into two test sets (Test1 and Test2) based on the square and circle of the wide-angle image. Two types of metrics, namely, image-level metrics (PSNR, SSIM, MS-SSIM) and perception-level metrics (FID, LPIPS-Alex, LPIPS-Vgg) are used to calculate the errors between the corrected wide-angle image and the ground truth. Since FishFormer~\cite{FishFormer} is customized for the fisheye image with a circular shape, we omit its quantitative metric on Test1 and use the original input as its placeholder in Fig.~\ref{fig:distortion_eva}.

We evaluate the performance of representative methods for the cross-view camera model in Tab.~\ref{tab:view_eva}. The mean average corner error (MACE) is provided for each method. For IHN~\cite{IHN}, we test it from two scales following the same setting reported in the paper. Moreover, the point matching errors (PME) of representative methods are listed in Tab.~\ref{tab:view_eva1}, which are collected and summarized from previous works~\cite{RealSH, Gyroflow+} and evaluated on the GHOF~\cite{Gyroflow+} test set. Five categories are split to exhibit the performance of different scenes, including regular (RE), foggy (FOG), low light (LL), rainy (RAIN), and snowy (SNOW) scenes.

We also evaluate the performance of several open source methods on cross-sensor camera model in Tab.~\ref{tab:cross-sensor}. Experimental results are reported according to the metrics of rotation and translation. For the rotation part, we calculate quaternion angle distance and absolute angle error of Euler angles including Roll, Pitch, and Yaw. For the translation part, we compute the Euclidean distance between the predicted translation vector and ground truth with absolute error in $X$, $Y$, $Z$ directions. Since there has not been a common benchmark for camera-LiDAR calibration yet, all selected methods were trained on the KITTI\cite{KITTI} dataset with different specific settings. In the evaluation, we devote ourselves to aligning the proposed benchmark to each method's original setting. Due to the difference in image size, image content, LiDAR beam number, and procedure of generating input depth map, the input color image and depth map to different trained models are inconsistent. From Tab.~\ref{tab:cross-sensor}, Fig.~\ref{fig:cross_sensor_eva1}, and Fig.~\ref{fig:cross_sensor_eva2}, we can observe there are noticeable performance degradations of comparison methods under the cross-domain evaluations.


\section{More Future Directions}

\subsection{Dataset}

One of the main challenges of learning-based camera calibrations is the difficulty in constructing datasets with high accuracy. This requires laborious manual intervention to obtain real-world data with labels. As we summarized, approximately 70\% of the works rely on synthesized datasets. However, the significant differences between synthesized and real-world datasets cannot be ignored, leading to domain gaps in the learned models. Therefore, the construction of a standardized, large-scale calibration dataset would significantly benefit this community. Recent works have demonstrated that well-designed learning strategies, such as semi-supervised learning~\cite{SS-WPC}, self-supervised learning~\cite{Fang, SSR-Net}, and unsupervised learning~\cite{UDHN, CA-UDHN}, can help address the demand for annotations in learning-based camera calibrations. These strategies also have the potential to discover additional calibration priors within the data itself.

\subsection{Transfer Learning} 
The advancements in deep learning have led to the development of transfer learning techniques, which could facilitate the transfer of knowledge learned from one camera to another. This approach can significantly speed up and streamline the calibration process, making it more efficient and cost-effective. Transfer learning can be especially useful in applications that involve multiple cameras or mobile devices. For example, in a multi-camera system, transfer learning can be used to calibrate all the cameras using the data collected from a single camera, reducing the time and effort required for calibration. Similarly, in mobile devices, transfer learning can enable faster and more accurate calibration of the camera, resulting in improved image quality and performance.

\subsection{Robustness to Noise and Outliers}
Another promising application of deep learning in camera calibration is improving the robustness of calibration to noise and outliers in the data. This approach can help ensure accurate calibration even in challenging environments, with low-quality data or noisy sensor readings. Conventionally, camera calibration algorithms are sensitive to noise and outliers in the data, which can lead to significant errors in the estimated camera parameters. However, with the application of deep learning, it is possible to learn more robust and accurate models that can better handle noise and outliers in the data. For instance, regularization techniques can be used to impose constraints on the learned parameters, preventing overfitting and enhancing the generalization ability of the model. Moreover, outlier detection techniques can be used to identify and exclude data points that are likely to be outliers, reducing their impact on the calibration process. This can be achieved using various statistical and machine-learning methods, such as clustering, classification, and regression.

\subsection{Online Calibration}
With the rapid development of deep learning, online camera calibration is becoming more efficient and practical. This technique involves updating the calibration parameters in real time, allowing for better performance as the camera moves or as the environment changes. This can be achieved using deep learning algorithms that can learn the complex relationships between the camera parameters and the image data. Learning-based camera calibration has the potential to revolutionize various industries, such as robotics and augmented reality. In robotics, online calibration can improve the accuracy of robot vision, which is crucial for tasks such as object detection and manipulation. Similarly, in augmented reality, online calibration can enhance the user experience by ensuring that virtual objects are correctly aligned with the real world. This can help create more realistic and immersive AR applications, which have numerous practical applications in fields such as entertainment, education, and training.

\subsection{Multimodal Calibration}
The potential of deep learning techniques in camera calibration goes beyond traditional photography and computer vision applications. It could also be applied to calibrate cameras with other sensors, such as remote sensing, infrared sensors, or radar. This advancement could lead to more precise and robust perception in various applications, including but not limited to autonomous driving, where multiple sensors are used. Incorporating deep learning-based calibration methods with multiple sensors could enhance the accuracy of the fusion of data from different sources. It could facilitate more accurate perception in challenging environments such as low-light conditions, occlusions, and adverse weather conditions. Furthermore, the ability to calibrate multiple sensors with deep learning methods could provide more reliable and consistent results compared to traditional calibration techniques.

These are a few potential directions for future research in camera calibration with deep learning. As the field continues to evolve, there may be many other exciting avenues for exploration and innovation. In addition, it is also thrilling to see how this technology will continue to impact various industries in the future.