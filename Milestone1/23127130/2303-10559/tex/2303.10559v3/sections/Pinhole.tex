\section{Standard Model}
\label{sec:pure}
Generally, for learning-based calibration works, the objectives of the intrinsics calibration contain focal length and optical center, and the objectives of the extrinsic calibration contain the rotation matrix and translation vector.

\subsection{Intrinsics Calibration}
Deepfocal \cite{DeepFocal} is a pioneer work in learning-based camera calibration, it aims to estimate the focal length of any image ``in the wild''. In detail, Deepfocal considered a simple pinhole camera model and regressed the horizontal field of view using a deep convolutional neural network. Given the width $w$ of an image, the relationship between the horizontal field of view $H_\theta$ and focal length $f$ can be described by:
\begin{equation}
H_\theta = 2\arctan(\frac{w}{2f}).
\label{eq-focal-length}
\end{equation}

Due to component wear, temperature fluctuations, or outside disturbances like collisions, the calibrated parameters of a camera are susceptible to change over time. To this end, MisCaliDet \cite{MisCaliDet} proposed to identify if a camera needs to be recalibrated intrinsically. Compared to the conventional intrinsic parameters such as the focal length and image center, MisCaliDet presented a new scalar metric, \textit{i.e.}, the average pixel position difference (APPD) to measure the degree of camera miscalibration, which describes the mean value of the pixel position differences over the entire image.

\subsection{Extrinsics Calibration}
In contrast to intrinsic calibration, extrinsic calibration infers the spatial correspondence of the camera and its located 3D scene. PoseNet\cite{PoseNet} first proposed deep convolutional neural networks to regress 6-DoF camera pose in real-time. A pose vector $\textbf{p}$ was predicted by PoseNet, given by the 3D position $\textbf{x}$ and orientation represented by quaternion $\textbf{q}$ of a camera, namely, $\textbf{p} = [\textbf{x}, \textbf{q}]$. For constructing the training dataset, the labels are automatically calculated from a video of the scenario using a structure from motion method \cite{wu2013towards}.

Inspired by PoseNet\cite{PoseNet}, the following works improved the extrinsic calibration in terms of the intermediate representation, interpretability, data format, learning objective, etc. For example, to optimize the geometric pose objective, DeepFEPE \cite{DeepFEPE} designed an end-to-end keypoint-based framework with learnable modules for detection, feature extraction, matching, and outlier rejection. Such a pipeline imitated the traditional baseline, in which the final performance can be analyzed and improved by the intermediate differentiable module. To bridge the domain gap between the extrinsic objective and image features, recent works proposed to first learn an intermediate representation from the input, such as surface geometry \cite{UprightNet}, depth map \cite{StereoCaliNet}, directional probability distribution \cite{DirectionNet}, camera rays~\cite{CAR}, normal flow \cite{DiffPoseNet}, and optical flow~\cite{DroidCalib}, etc. Then, the extrinsic are reasoned by geometric constraints and learned representation. Therefore, the neural networks are gradually guided to perceive the geometry-related features, which are crucial for extrinsic estimation. Considering the privacy concerns and limited storage problem, some recent works~\cite{Do, SceneSqueezer, Neumap} compressed the scene and exploited the point-like feature to estimate the extrinsic. For example, Do et al. \cite{Do} trained a network to recognize sparse but significant 3D points, dubbed scene landmarks, by encoding their appearance as implicit features. The camera pose can be calculated using a robust minimal solver followed by a Levenberg-Marquardt-based nonlinear refinement. SceneSqueezer \cite{SceneSqueezer} compressed the scene information from three levels: the database frames are clustered using pairwise co-visibility information, a point selection module prunes each cluster based on estimation performance, and learned quantization further compresses the selected points.

Learning-based SLAM (Simultaneous Localization and Mapping) and SfM (Structure from Motion) are also closely related to extrinsic calibration tasks. Both techniques involve pose estimation and tracking to localize cameras in 3D scenes. Direct attempts focused on modifying classical modules, such as feature extraction\cite{detone2018superpoint,sun2021loftr}, feature matching\cite{sarlin2020superglue,lindenberger2023lightglue} and pose estimation\cite{brachmann2017dsac,sarlin2021back}. These modifications have been proven to be efficient and robust but still need to be integrated into conventional pipelines. There is also a series of methods that solve problems in an end-to-end manner which aim to simplify the traditional pipeline by integrating neural networks and differentiable operations. DeepVO\cite{wang2017deepvo} is one of the pioneering supervised solutions that utilized CNNs and RNNs to estimate camera poses. UnDeepVO\cite{li2018undeepvo} further introduced an unsupervised training strategy that combines depth map prediction with pose estimation. Subsequent end-to-end solutions generally adopted similar formulations but primarily focused on incorporating more constraints from depth \cite{bian2019unsupervised,yang2020d3vo,zhu2024revisit}, optical flow\cite{min2020voldor,teed2021droid,wang2021tartanvo}, and semantics \cite{yu2018ds,wu2020eao,chen2022accurate}.

With the advancement of neural rendering, NeRF-based methods have gained significant attention for their promising results. iMAP\cite{sucar2021imap} pioneered the first online SLAM framework using NeRF, enabling joint optimization of camera poses and scene representations. Subsequent studies expanded scene representations to hierarchical voxel grids\cite{zhu2022nice,rosinol2022nerf}, signed distance fields\cite{ortiz2022isdf,johari2023eslam}, point clouds\cite{sandstrom2023point,hu2024cp}, and 3D Gaussians\cite{yan2024gs}. While most NeRF-based approaches depend on traditional odometry for pose initialization, NeRF remains a potential solution for training pose estimation models with limited accurate labels and enhancing pose accuracy. To reduce reliance on precise pose assumptions, researchers have incorporated geometric priors such as depth estimation~\cite{truong2023sparf,wang2023altnerf}, multi-view correspondence~\cite{jeong2021self,bian2023nope,wang2023altnerf}, and GAN-based constraints~\cite{meng2021gnerf} to stabilize optimization.
 
\subsection{Joint Intrinsic and Extrinsic Calibration}

\subsubsection{Geometric Representations}

\noindent \textbf{Vanishing Points}
The intersection of projections of a set of parallel lines in the world leads to a vanishing point. The detection of vanishing points is a fundamental and crucial challenge in 3D vision. In general, vanishing points reveal the direction of 3D lines, allowing the agent to deduce 3D scene information from a single 2D image.

DeepVP \cite{DeepVP} is the first learning-based work for detecting the vanishing points given a single image. It reversed the conventional process by scoring the horizon line candidates according to the vanishing points they contain. Chang et al. \cite{chang2018deepvp} redesigned this task as a classification problem using an output layer with possible vanishing point locations. To directly leverage the geometric properties of vanishing points, NeurVPS \cite{zhou2019neurvps} proposed a canonical conic space and a conic convolution operator that can be implemented as regular convolutions in this space, where the learning model is capable of calculating the global geometric information of vanishing points locally. To overcome the need for a large amount of training data, DVPD \cite{DVPD} incorporated the neural network with two geometric priors: Hough transformation and Gaussian sphere. First, the convolutional features are transformed into a Hough domain, mapping lines to distinct bins. The projection of the Hough bins is then extended to the Gaussian sphere, where lines are transformed into great circles and vanishing points are located at the intersection of these circles. Geometric priors are data-efficient because they eliminate the necessity for learning this information from data, which enables an interpretable learning framework and generalizes better to domains with slightly different data distributions.

\noindent \textbf{Horizon Lines}
The horizon line is a crucial contextual attribute for various computer vision tasks, especially image metrology, computational photography, and 3D scene understanding. The projection of the line at infinity onto any plane that is perpendicular to the local gravity vector determines the location of the horizon line. Given the FoV, pitch, and roll of a camera, it is straightforward to locate the horizon line in its captured image space. DeepHorizon \cite{DeepHorizon} proposed the first learning-based solution for estimating the horizon line from an image. To train the network, a new benchmark dataset, Horizon Lines in the Wild (HLW), was constructed, which consists of real-world images with labeled horizon lines. SA-MobileNet \cite{SA-MobileNet} proposed an image tilt detection and correction with self-attention MobileNet \cite{howard2019searching} for smartphones. A spatial self-attention module was devised to learn long-range dependencies and global context within the input images. To address the regression difficulty, the network is trained to estimate multiple angles within a narrow interval of the ground truth tilt.

\begin{figure}[!t]
  \centering
  \includegraphics[width=.47\textwidth]{figures/CTRL-C.pdf}
  \caption{Overview of CTRL-C. The figure is from ~\cite{CTRL-C}. It estimates parameters including the zenith VP, FoV, and horizon line for camera calibration from an input image and a set of line segments. Moreover, two auxiliary outputs (vertical and horizontal convergence line scores) guide the network in learning scene geometry for calibration.}
  \label{fig:CTRL-C}
  \vspace{-0.3cm}
\end{figure}

\noindent \textbf{Geometry Fields}
Recent calibration works tend to design a novel \textit{geometry field} to replace the traditional geometric representations as the new learning target, which is inspired by the prior of camera models or the perspective properties of captured images, such as the distortion distribution map~\cite{DDM, OrdianlDistortion}, incidence field~\cite{WildCamera}, incident map~\cite{DiffCalib}, perspective field~\cite{PerspectiveField, GeoCalib}, camera rays~\cite{CAR}, and camera image~\cite{DM-Calib}, etc. These fields represent a pixel-wise or patch-wise parametrization of the intrinsic and/or extrinsic invariants. They show an explicit relationship to the image details and are easy to learn for neural networks. After predicting the geometry field, the calibrated camera parameters can be easily converted and computed via RANSAC, camera reprojection, or Levenberg-Marquardt optimization, etc.

\subsubsection{Composite Parameters}
Calibrating the composite parameters aims to estimate the intrinsic parameters and extrinsic parameters simultaneously. By jointly estimating composite parameters and training using data from a large-scale panorama dataset \cite{SUN360}, Hold-Geoffroy\etal \cite{Hold-Geoffroy} outperformed previous independent calibration tasks. Moreover, Hold-Geoffroy\etal \cite{Hold-Geoffroy, PM-Calib} performed human perception research in which the participants were asked to evaluate the realism of 3D objects composited with and without accurate calibration. This data was further designed to a new perceptual measure for the calibration errors. In terms of the feature category, some methods~\cite{Lee, CTRL-C, SOFI, MSCC} considered both semantic features and geometric cues for camera calibration. They showed how making use of geometric features, is capable of facilitating the network to comprehend the underlying perspective structure. The pipeline of CTRL-C is illustrated in Figure~\ref{fig:CTRL-C}. In recent literature, more applications are jointly studied with camera calibration, for example, single view metrology \cite{Zhu}, 3D human pose and shape estimation \cite{SPEC}, depth estimation \cite{Baradad, Fang, FlowMap}, object pose estimation \cite{FocalPose}, and image reflection removal \cite{Zheng}, etc. Considering the heterogeneousness and visual implicitness of different camera parameters, CPL \cite{CPL} estimated the parameters using a novel camera projection loss, exploiting the neural network to reconstruct the 3D point cloud. The proposed loss addressed the training imbalance problem by representing different errors of camera parameters using a unified metric.

\subsubsection{Calibration with Reconstruction}

Dense reconstruction tasks often involve complex constraints, enabling joint calibration alongside reconstruction. While earlier methods \cite{sucar2021imap,zhu2022nice,rosinol2022nerf,johari2023eslam,sandstrom2023point} primarily focused on pose estimation and geometry reconstruction under the assumption of known and accurate calibration parameters, recent advances in NeRF have facilitated simultaneous calibration. These approaches jointly optimize intrinsic and extrinsic parameters during NeRF training. Notably, NeRF--~\cite{wang2021nerf} pioneered the simultaneous optimization of camera parameters by introducing a trainable pinhole camera model. This pipeline was further advanced by SCNeRF~\cite{jeong2021self}, which proposed a comprehensive camera model incorporating pinhole design, radial distortion, and pixel-specific noise. Similarly, SiNeRF\cite{xia2022sinerf} introduced a sinusoidal activation function and a Mixed Region Sampling strategy to alleviate systematic sub-optimality in joint optimization. Additionally, CAMP\cite{park2023camp} analyzed the selection of camera parameterization and proposed a preconditioned camera optimization technique.

Popular frameworks like InstantNGP~\cite{muller2022instant} and NeRFStudio~\cite{tancik2023nerfstudio} offer features to fine-tune camera parameters. Typically, NeRF methods use outputs from tools like COLMAP or Polycam, with rendering quality tied closely to initial data quality. Minor errors in intrinsics or poses can lead to noticeable rendering artifacts. Current approaches~\cite{muller2022instant, tancik2023nerfstudio} integrate intrinsic parameters, distortion coefficients, and camera poses into NeRF's training optimization, enhancing both geometric structure learning and parameter optimization. This boosts 3D geometry robustness and refines camera models.

\subsection{Discussion}

\subsubsection{Technique Summary}
The above methods target automatic calibration without manual intervention and scene assumption. Early literature~\cite{DeepFocal, PoseNet} separately studied the intrinsic calibration or extrinsic calibration. Driven by large-scale datasets and powerful networks, subsequent works~\cite{DeepVP, DeepHorizon, Hold-Geoffroy, CTRL-C} considered a comprehensive camera calibration, inferring various parameters and geometric representations. To relieve the difficulty of learning the camera parameters, some works~\cite{UprightNet, StereoCaliNet, DirectionNet, DiffPoseNet, PerspectiveField} proposed to learn an intermediate representation. In recent literature, more applications are jointly studied with camera calibration~\cite{Zhu, SPEC, Baradad, Fang, Zheng}. This suggests solving the downstream vision tasks, especially in 3D tasks may require prior knowledge of the image formation model. Moreover, some geometric priors~\cite{DVPD} can alleviate the data-starved requirement of deep learning, showing the potential to bridge the gap between the calibration target and semantic features.  

It is interesting to find that increasing more extrinsic calibration methods~\cite{DeepFEPE, Do, SceneSqueezer} revisited the traditional feature point-based solutions. The extrinsics that describe the camera motion contain limited degrees of freedom, and thus some local features can well represent the spatial correspondence. Besides, the network designed for point learning improves the efficiency of calibration models, such as PointNet~\cite{qi2017pointnet} and PointCNN~\cite{li2018pointcnn}. Such a pipeline also enables clear interpretability of learning-based camera calibration, which promotes understanding of how the network calibrates and magnifies the influences of intermediate modules.

\subsubsection{Future Effort}

(1) Explore more model priors. Most learning-based methods study the parametric camera models but their generalization abilities are limited. In contrast, non-parametric models directly model the relationship between the 3D imaging ray and its resulting pixel in the image, encoding valuable priors in the learned semantic features to reason the camera parameters. A non-parametric model based on implicit neural representations, as explored in \cite{lin2023learning}, has shown remarkable advantages in accurately capturing the camera within a higher-dimensional space. This approach holds potential for future extensions, such as modeling physical factors like aberration. Recent works~\cite{NeuralLens, WildCamera, PerspectiveField} incorporate the perspective of modeling pixel-wise information for camera calibration, making minimal assumptions on the camera model and showing more interpretable and in line with how humans perceive.

(2) Decouple different stages in an end-to-end calibration learning model. Most learning-based methods include a feature extraction stage and an objective estimation stage. However, how the networks learn the features related to calibration is ambiguous. Therefore, decoupling the learning process by different traditional calibration stages can guide the way of feature extraction. It would be meaningful to extend the idea from the extrinsic calibration~\cite{DeepFEPE, Do, SceneSqueezer} to more general calibration problems.

(3) Transfer the measurement space from the parameter error to the geometric difference. Jointly calibrating multiple camera parameters poses an optimization imbalance due to their varying sample distributions. Simple normalization fails to unify their error spaces. A potential solution is to establish a direct measurement space based on the geometric properties of different camera parameters.

(4) Training NeRF without precise camera parameters remains challenging, particularly in scenarios with sparse views, significant motion, low-texture regions, and suboptimal initial values. While modern NeRF-based methods optimize camera parameters with notable results, they are computationally demanding and lack the generalization of deep-learning calibration techniques. We argue that in current NeRF-based approaches, camera parameters often play a secondary role. Therefore, developing effective calibration algorithms that leverage NeRF remains a difficult yet promising pursuit.