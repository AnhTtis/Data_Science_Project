@article{adamsProspectiveMultisiteStudy2022,
  title = {Prospective, Multi-Site Study of Patient Outcomes after Implementation of the {{TREWS}} Machine Learning-Based Early Warning System for Sepsis},
  author = {Adams, Roy and Henry, Katharine E. and Sridharan, Anirudh and Soleimani, Hossein and Zhan, Andong and Rawat, Nishi and Johnson, Lauren and Hager, David N. and Cosgrove, Sara E. and Markowski, Andrew and Klein, Eili Y. and Chen, Edward S. and Saheed, Mustapha O. and Henley, Maureen and Miranda, Sheila and Houston, Katrina and Linton, Robert C. and Ahluwalia, Anushree R. and Wu, Albert W. and Saria, Suchi},
  year = {2022},
  month = jul,
  journal = {Nature Medicine},
  volume = {28},
  number = {7},
  pages = {1455--1460},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-022-01894-0},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/R9A8UEVH/Adams et al. - 2022 - Prospective, multi-site study of patient outcomes .pdf}
}

@misc{baiEmpiricalEvaluationGeneric2018,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1803.01271},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{bennetRicuInterfaceIntensive2021,
  title = {Ricu: {{R}}'s Interface to Intensive Care Data},
  author = {Bennet, Nicolas and Plecko, Drago and Ukor, Ida-Fong and Meinshausen, Nicolai and B{\"u}hlmann, Peter},
  year = {2021},
  month = aug,
  number = {2108.00796},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.00796}
}

@article{cawleyOverfittingModelSelection2010,
  title = {On Over-Fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation},
  author = {Cawley, Gavin C. and Talbot, Nicola L.C.},
  year = {2010},
  month = aug,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  pages = {2079--2107},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.},
  issue_date = {3/1/2010}
}

@misc{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1409.1259},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (stat.ML)}
}

@article{debrayNewFrameworkEnhance2015,
  title = {A New Framework to Enhance the Interpretation of External Validation Studies of Clinical Prediction Models},
  author = {Debray, Thomas P.A. and Vergouwe, Yvonne and Koffijberg, Hendrik and Nieboer, Daan and Steyerberg, Ewout W. and Moons, Karel G.M.},
  year = {2015},
  month = mar,
  journal = {Journal of Clinical Epidemiology},
  volume = {68},
  number = {3},
  pages = {279--289},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2014.06.018},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/4VXXBIYG/Debray et al. - 2015 - A new framework to enhance the interpretation of e.pdf}
}

@article{futomaGeneralizationClinicalPrediction2021,
  title = {Generalization in {{Clinical Prediction Models}}: {{The Blessing}} and {{Curse}} of {{Measurement Indicator Variables}}},
  shorttitle = {Generalization in {{Clinical Prediction Models}}},
  author = {Futoma, Joseph and Simons, Morgan and {Doshi-Velez}, Finale and Kamaleswaran, Rishikesan},
  year = {2021},
  month = jun,
  journal = {Critical Care Explorations},
  volume = {3},
  number = {7},
  pages = {e0453},
  issn = {2639-8028},
  doi = {10.1097/CCE.0000000000000453},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/P4KEKAGB/Futoma et al. - 2021 - Generalization in Clinical Prediction Models The .pdf}
}

@article{goldbergerPhysioBankPhysioToolkitPhysioNet2000,
  title = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}: {{Components}} of a {{New Research Resource}} for {{Complex Physiologic Signals}}},
  shorttitle = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}},
  author = {Goldberger, Ary L. and Amaral, Luis A. N. and Glass, Leon and Hausdorff, Jeffrey M. and Ivanov, Plamen Ch. and Mark, Roger G. and Mietus, Joseph E. and Moody, George B. and Peng, Chung-Kang and Stanley, H. Eugene},
  year = {2000},
  month = jun,
  journal = {Circulation},
  volume = {101},
  number = {23},
  issn = {0009-7322, 1524-4539},
  doi = {10.1161/01.CIR.101.23.e215},
  abstract = {Abstract               \textemdash The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet.org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/NACFET3T/Goldberger et al. - 2000 - PhysioBank, PhysioToolkit, and PhysioNet Componen.pdf}
}

@misc{gulrajaniSearchLostDomain2020,
  title = {In {{Search}} of {{Lost Domain Generalization}}},
  author = {Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2007.01434},
  abstract = {The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions -- datasets, architectures, and model selection criteria -- render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DomainBed, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DomainBed and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DomainBed, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{hylandEarlyPredictionCirculatory2020,
  title = {Early Prediction of Circulatory Failure in the Intensive Care Unit Using Machine Learning},
  author = {Hyland, Stephanie L. and Faltys, Martin and H{\"u}ser, Matthias and Lyu, Xinrui and Gumbsch, Thomas and Esteban, Crist{\'o}bal and Bock, Christian and Horn, Max and Moor, Michael and Rieck, Bastian and Zimmermann, Marc and Bodenham, Dean and Borgwardt, Karsten and R{\"a}tsch, Gunnar and Merz, Tobias M.},
  year = {2020},
  month = mar,
  journal = {Nature Medicine},
  volume = {26},
  number = {3},
  pages = {364--373},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-020-0789-4},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/LHBTJFXJ/Hyland et al. - 2020 - Early prediction of circulatory failure in the int.pdf}
}

@article{johnsonGeneralizabilityPredictiveModels2018,
  title = {Generalizability of Predictive Models for Intensive Care Unit Patients},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Naumann, Tristan},
  year = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1812.02275},
  abstract = {A large volume of research has considered the creation of predictive models for clinical data; however, much existing literature reports results using only a single source of data. In this work, we evaluate the performance of models trained on the publicly-available eICU Collaborative Research Database. We show that cross-validation using many distinct centers provides a reasonable estimate of model performance in new centers. We further show that a single model trained across centers transfers well to distinct hospitals, even compared to a model retrained using hospital-specific data. Our results motivate the use of multi-center datasets for model development and highlight the need for data sharing among hospitals to maximize model performance.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@misc{johnsonMIMICIV2022,
  title = {{{MIMIC-IV}}},
  author = {Johnson, Alistair and Bulgarelli, Lucas and Pollard, Tom and Horng, S and Celi, LA and Mark, R},
  year = {2022},
  month = jun,
  howpublished = {PhysioNet}
}

@article{kdigoKidneyDiseaseImproving2012,
  title = {Kidney {{Disease}}: {{Improving Global Outcomes}} ({{KDIGO}}) {{Acute Kidney Injury Work Group}}: {{KDIGO}} Clinical Practice Guideline for Acute Kidney Injury.},
  author = {{KDIGO}},
  year = {2012},
  month = mar,
  journal = {Kidney International Supplements},
  volume = {2},
  number = {1},
  issn = {21571716},
  doi = {10.1038/kisup.2012.7},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/UCUYPH24/2012 - Work Group Membership.pdf}
}

@article{kdigoKidneyDiseaseImproving2013,
  title = {Kidney {{Disease}}: {{Improving Global Outcomes}} ({{KDIGO}}): {{KDIGO Clinical Practice Guideline}} for the {{Evaluation}} and {{Management}} of {{Chronic Kidney Disease}}},
  author = {{KDIGO}},
  year = {2013},
  month = jan,
  journal = {Kidney International Supplements},
  volume = {3},
  number = {1},
  issn = {21571716},
  langid = {english}
}

@article{kellyKeyChallengesDelivering2019,
  title = {Key Challenges for Delivering Clinical Impact with Artificial Intelligence},
  author = {Kelly, Christopher J. and Karthikesalingam, Alan and Suleyman, Mustafa and Corrado, Greg and King, Dominic},
  year = {2019},
  month = dec,
  journal = {BMC Medicine},
  volume = {17},
  number = {1},
  pages = {195},
  issn = {1741-7015},
  doi = {10.1186/s12916-019-1426-2},
  abstract = {Abstract                            Background               Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice.                                         Main body               Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes.                                         Conclusion               The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/B3ZS8UUP/Kelly et al. - 2019 - Key challenges for delivering clinical impact with.pdf}
}

@article{koynerDevelopmentMachineLearning2018a,
  title = {The {{Development}} of a {{Machine Learning Inpatient Acute Kidney Injury Prediction Model}}*:},
  shorttitle = {The {{Development}} of a {{Machine Learning Inpatient Acute Kidney Injury Prediction Model}}*},
  author = {Koyner, Jay L. and Carey, Kyle A. and Edelson, Dana P. and Churpek, Matthew M.},
  year = {2018},
  month = jul,
  journal = {Critical Care Medicine},
  volume = {46},
  number = {7},
  pages = {1070--1077},
  issn = {0090-3493},
  doi = {10.1097/CCM.0000000000003123},
  langid = {english}
}

@misc{kruegerOutofDistributionGeneralizationRisk2020,
  title = {Out-of-{{Distribution Generalization}} via {{Risk Extrapolation}} ({{REx}})},
  author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Priol, Remi Le and Courville, Aaron},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2003.00688},
  abstract = {Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution ("covariate shift"). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Neural and Evolutionary Computing (cs.NE)}
}

@misc{liLearningGeneralizeMetaLearning2017,
  title = {Learning to {{Generalize}}: {{Meta-Learning}} for {{Domain Generalization}}},
  shorttitle = {Learning to {{Generalize}}},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1710.03463},
  abstract = {Domain shift refers to the well known problem that a model trained in one source domain performs poorly when applied to a target domain with different statistics. \{Domain Generalization\} (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel \{meta-learning\} method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{meyerMachineLearningRealtime2018,
  title = {Machine Learning for Real-Time Prediction of Complications in Critical Care: A Retrospective Study.},
  author = {Meyer, Alexander and Zverinski, Dina and Pfahringer, Boris and Kempfert, Jorg and Kuehne, Titus and Sundermann, Simon H and Stamm, Christof and Hofmann, Thomas and Falk, Volkmar and Eickhoff, Carsten},
  year = {2018},
  journal = {The Lancet. Respiratory medicine},
  series = {Comment in: {{Lancet Respir Med}}. 2018 {{Dec}};6(12):886-888 {{PMID}}: 30416082 [{{https://www.ncbi.nlm.nih.gov/pubmed/30416082]}}},
  volume = {6},
  number = {12},
  pages = {905--914},
  address = {{England}},
  issn = {2213-2619},
  doi = {10.1016/S2213-2600(18)30300-X},
  abstract = {BACKGROUND: The large amount of clinical signals in intensive care units can easily overwhelm health-care personnel and can lead to treatment delays, suboptimal care, or clinical errors. The aim of this study was to apply deep machine learning methods to predict severe complications during critical care in real time after cardiothoracic surgery., METHODS: We used deep learning methods (recurrent neural networks) to predict several severe complications (mortality, renal failure with a need for renal replacement therapy, and postoperative bleeding leading to operative revision) in post cardiosurgical care in real time. Adult patients who underwent major open heart surgery from Jan 1, 2000, to Dec 31, 2016, in a German tertiary care centre for cardiovascular diseases formed the main derivation dataset. We measured the accuracy and timeliness of the deep learning model's forecasts and compared predictive quality to that of established standard-of-care clinical reference tools (clinical rule for postoperative bleeding, Simplified Acute Physiology Score II for mortality, and the Kidney Disease: Improving Global Outcomes staging criteria for acute renal failure) using positive predictive value (PPV), negative predictive value, sensitivity, specificity, area under the curve (AUC), and the F1 measure (which computes a harmonic mean of sensitivity and PPV). Results were externally retrospectively validated with 5898 cases from the published MIMIC-III dataset., FINDINGS: Of 47 559 intensive care admissions (corresponding to 42 007 patients), we included 11 492 (corresponding to 9269 patients). The deep learning models yielded accurate predictions with the following PPV and sensitivity scores: PPV 0.90 and sensitivity 0.85 for mortality, 0.87 and 0.94 for renal failure, and 0.84 and 0.74 for bleeding. The predictions significantly outperformed the standard clinical reference tools, improving the absolute complication prediction AUC by 0.29 (95\% CI 0.23-0.35) for bleeding, by 0.24 (0.19-0.29) for mortality, and by 0.24 (0.13-0.35) for renal failure (p{$<$}0.0001 for all three analyses). The deep learning methods showed accurate predictions immediately after patient admission to the intensive care unit. We also observed an increase in performance in our validation cohort when the machine learning approach was tested against clinical reference tools, with absolute improvements in AUC of 0.09 (95\% CI 0.03-0.15; p=0.0026) for bleeding, of 0.18 (0.07-0.29; p=0.0013) for mortality, and of 0.25 (0.18-0.32; p{$<$}0.0001) for renal failure., INTERPRETATION: The observed improvements in prediction for all three investigated clinical outcomes have the potential to improve critical care. These findings are noteworthy in that they use routinely collected clinical data exclusively, without the need for any manual processing. The deep machine learning method showed AUC scores that significantly surpass those of clinical reference tools, especially soon after admission. Taken together, these properties are encouraging for prospective deployment in critical care settings to direct the staff's attention towards patients who are most at risk., FUNDING: No specific funding. Copyright \textcopyright{} 2018 Elsevier Ltd. All rights reserved.},
  keywords = {*Cardiac Surgical Procedures/ae [Adverse Effects],*Deep Learning,*Postoperative Hemorrhage/di [Diagnosis],*Renal Insufficiency/di [Diagnosis],Aged,BIG DATA,Cardiac Surgical Procedures/mo [Mortality],Female,FUTURE,Humans,Intensive Care Units/sn [Statistics \& Numerical Data],INTENSIVE-CARE,Male,MEDICINE,Middle Aged,Outcome Assessment; Health Care,Postoperative Hemorrhage/ep [Epidemiology],Predictive Value of Tests,Renal Insufficiency/ep [Epidemiology],Retrospective Studies,ROC Curve},
  annotation = {Meyer, Alexander. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany; DZHK (German Centre for Cardiovascular Research), Partner Site Berlin, Berlin, Germany; Berlin Institute of Health, Berlin, Germany. Electronic address: meyera@dhzb.de. Zverinski, Dina. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany; Department of Computer Science, ETH Zurich, Zurich, Switzerland. Pfahringer, Boris. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany; Berlin Institute of Health, Berlin, Germany. Kempfert, Jorg. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany. Kuehne, Titus. Institute of Imaging Science and Computational Modelling, Charite - Universitatsmedizin Berlin, Berlin, Germany. Sundermann, Simon H. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany; Department of Cardiovascular Surgery, Charite - Universitatsmedizin Berlin, Berlin, Germany. Stamm, Christof. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany; Berlin Center for Regenerative Therapies, Charite - Universitatsmedizin Berlin, Berlin, Germany; DZHK (German Centre for Cardiovascular Research), Partner Site Berlin, Berlin, Germany. Hofmann, Thomas. Department of Computer Science, ETH Zurich, Zurich, Switzerland. Falk, Volkmar. Department of Cardiothoracic and Vascular Surgery, Deutsches Herzzentrum Berlin, Berlin, Germany; Department of Cardiovascular Surgery, Charite - Universitatsmedizin Berlin, Berlin, Germany; DZHK (German Centre for Cardiovascular Research), Partner Site Berlin, Berlin, Germany. Eickhoff, Carsten. Department of Computer Science, ETH Zurich, Zurich, Switzerland; Center for Biomedical Informatics, Brown University, Providence, RI, USA.},
  file = {/Users/patrick/Library/CloudStorage/GoogleDrive-rockenschaub.patrick@gmail.com/My Drive/Postdoc/projects/systematic review/06_full_text/papers/10.1016+S2213-2600(18)30300-X.pdf}
}

@misc{moorPredictingSepsisMultisite2021,
  title = {Predicting Sepsis in Multi-Site, Multi-National Intensive Care Cohorts Using Deep Learning},
  author = {Moor, Michael and Bennet, Nicolas and Plecko, Drago and Horn, Max and Rieck, Bastian and Meinshausen, Nicolai and B{\"u}hlmann, Peter and Borgwardt, Karsten},
  year = {2021},
  number = {2107.05230},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2107.05230},
  abstract = {Despite decades of clinical research, sepsis remains a global public health crisis with high mortality, and morbidity. Currently, when sepsis is detected and the underlying pathogen is identified, organ damage may have already progressed to irreversible stages. Effective sepsis management is therefore highly time-sensitive. By systematically analysing trends in the plethora of clinical data available in the intensive care unit (ICU), an early prediction of sepsis could lead to earlier pathogen identification, resistance testing, and effective antibiotic and supportive treatment, and thereby become a life-saving measure. Here, we developed and validated a machine learning (ML) system for the prediction of sepsis in the ICU. Our analysis represents the largest multi-national, multi-centre in-ICU study for sepsis prediction using ML to date. Our dataset contains \$156,309\$ unique ICU admissions, which represent a refined and harmonised subset of five large ICU databases originating from three countries. Using the international consensus definition Sepsis-3, we derived hourly-resolved sepsis label annotations, amounting to \$26,734\$ (\$17.1\textbackslash\%\$) septic stays. We compared our approach, a deep self-attention model, to several clinical baselines as well as ML baselines and performed an extensive internal and external validation within and across databases. On average, our model was able to predict sepsis with an AUROC of \$0.847 \textbackslash pm 0.050\$ (internal out-of sample validation) and \$0.761 \textbackslash pm 0.052\$ (external validation). For a harmonised prevalence of \$17\textbackslash\%\$, at \$80\textbackslash\%\$ recall our model detects septic patients with \$39\textbackslash\%\$ precision 3.7 hours in advance.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{paulwgelbersAmsterdamUMCdb,
  title = {{{AmsterdamUMCdb}}},
  author = {{Paul WG Elbers} and {Patrick J Thoral}},
  howpublished = {DANS}
}

@article{pirracchioMortalityPredictionIntensive2015a,
  title = {Mortality Prediction in Intensive Care Units with the {{Super ICU Learner Algorithm}} ({{SICULA}}): A Population-Based Study},
  shorttitle = {Mortality Prediction in Intensive Care Units with the {{Super ICU Learner Algorithm}} ({{SICULA}})},
  author = {Pirracchio, Romain and Petersen, Maya L and Carone, Marco and Rigon, Matthieu Resche and Chevret, Sylvie and {van der Laan}, Mark J},
  year = {2015},
  month = jan,
  journal = {The Lancet Respiratory Medicine},
  volume = {3},
  number = {1},
  pages = {42--52},
  issn = {22132600},
  doi = {10.1016/S2213-2600(14)70239-5},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/479NIEQ8/Pirracchio et al. - 2015 - Mortality prediction in intensive care units with .pdf}
}

@article{pollardEICUCollaborativeResearch2018,
  title = {The {{eICU Collaborative Research Database}}, a Freely Available Multi-Center Database for Critical Care Research},
  author = {Pollard, Tom J. and Johnson, Alistair E. W. and Raffa, Jesse D. and Celi, Leo A. and Mark, Roger G. and Badawi, Omar},
  year = {2018},
  month = dec,
  journal = {Scientific Data},
  volume = {5},
  number = {1},
  pages = {180178},
  issn = {2052-4463},
  doi = {10.1038/sdata.2018.178},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/U6AGFX5H/Pollard et al. - 2018 - The eICU Collaborative Research Database, a freely.pdf}
}

@misc{pollardEICUCollaborativeResearch2019,
  title = {{{eICU Collaborative Research Database}}},
  author = {Pollard, Tom and Johnson, Alistair and Raffa, J and Celi, LA and Badawi, O and Mark, R},
  year = {2019},
  month = apr,
  howpublished = {PhysioNet}
}

@misc{rameFishrInvariantGradient2021,
  title = {Fishr: {{Invariant Gradient Variances}} for {{Out-of-Distribution Generalization}}},
  shorttitle = {Fishr},
  author = {Rame, Alexandre and Dancette, Corentin and Cord, Matthieu},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2109.02934},
  abstract = {Learning robust models that generalize well under changes in the data distribution is critical for real-world applications. To this end, there has been a growing surge of interest to learn simultaneously from multiple training domains - while enforcing different types of invariance across those domains. Yet, all existing approaches fail to show systematic benefits under controlled evaluation protocols. In this paper, we introduce a new regularization - named Fishr - that enforces domain invariance in the space of the gradients of the loss: specifically, the domain-level variances of gradients are matched across training domains. Our approach is based on the close relations between the gradient covariance, the Fisher Information and the Hessian of the loss: in particular, we show that Fishr eventually aligns the domain-level loss landscapes locally around the final weights. Extensive experiments demonstrate the effectiveness of Fishr for out-of-distribution generalization. Notably, Fishr improves the state of the art on the DomainBed benchmark and performs consistently better than Empirical Risk Minimization. Our code is available at https://github.com/alexrame/fishr.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{reynaEarlyPredictionSepsis2020,
  title = {Early {{Prediction}} of {{Sepsis From Clinical Data}}: {{The PhysioNet}}/{{Computing}} in {{Cardiology Challenge}} 2019.},
  author = {Reyna, Matthew A and Josef, Christopher S and Jeter, Russell and Shashikumar, Supreeth P and Westover, M Brandon and Nemati, Shamim and Clifford, Gari D and Sharma, Ashish},
  year = {2020},
  journal = {Critical care medicine},
  volume = {48},
  number = {2},
  pages = {210--217},
  address = {{United States}},
  issn = {1530-0293},
  doi = {10.1097/CCM.0000000000004145},
  abstract = {OBJECTIVES: Sepsis is a major public health concern with significant morbidity, mortality, and healthcare expenses. Early detection and antibiotic treatment of sepsis improve outcomes. However, although professional critical care societies have proposed new clinical criteria that aid sepsis recognition, the fundamental need for early detection and treatment remains unmet. In response, researchers have proposed algorithms for early sepsis detection, but directly comparing such methods has not been possible because of different patient cohorts, clinical variables and sepsis criteria, prediction tasks, evaluation metrics, and other differences. To address these issues, the PhysioNet/Computing in Cardiology Challenge 2019 facilitated the development of automated, open-source algorithms for the early detection of sepsis from clinical data., DESIGN: Participants submitted containerized algorithms to a cloud-based testing environment, where we graded entries for their binary classification performance using a novel clinical utility-based evaluation metric. We designed this scoring function specifically for the Challenge to reward algorithms for early predictions and penalize them for late or missed predictions and for false alarms., SETTING: ICUs in three separate hospital systems. We shared data from two systems publicly and sequestered data from all three systems for scoring., PATIENTS: We sourced over 60,000 ICU patients with up to 40 clinical variables for each hour of a patient's ICU stay. We applied Sepsis-3 clinical criteria for sepsis onset., INTERVENTIONS: None., MEASUREMENTS AND MAIN RESULTS: A total of 104 groups from academia and industry participated, contributing 853 submissions. Furthermore, 90 abstracts based on Challenge entries were accepted for presentation at Computing in Cardiology., CONCLUSIONS: Diverse computational approaches predict the onset of sepsis several hours before clinical recognition, but generalizability to different hospital systems remains a challenge.},
  keywords = {*Algorithms,*Early Diagnosis,*Intensive Care Units,*Sepsis/di [Diagnosis],Electronic Health Records,Female,Humans,Male,Sepsis/pp [Physiopathology],Severity of Illness Index,Time Factors,United States},
  annotation = {Reyna, Matthew A. Department of Biomedical Informatics, Emory University, Atlanta, GA. Josef, Christopher S. Department of Biomedical Informatics, Emory University, Atlanta, GA. Jeter, Russell. Department of Biomedical Informatics, Emory University, Atlanta, GA. Shashikumar, Supreeth P. Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA. Shashikumar, Supreeth P. Department of Biomedical Informatics, University of California San Diego, San Diego, CA. Westover, M Brandon. Department of Neurology, Massachusetts General Hospital, Boston, MA. Nemati, Shamim. Department of Biomedical Informatics, Emory University, Atlanta, GA. Nemati, Shamim. Department of Biomedical Informatics, University of California San Diego, San Diego, CA. Clifford, Gari D. Department of Biomedical Informatics, Emory University, Atlanta, GA. Clifford, Gari D. Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta, GA. Sharma, Ashish. Department of Biomedical Informatics, Emory University, Atlanta, GA.},
  file = {/Users/patrick/Library/CloudStorage/GoogleDrive-rockenschaub.patrick@gmail.com/My Drive/Postdoc/projects/systematic review/06_full_text/papers/10.1097+CCM.0000000000004145.pdf}
}

@article{riekeFutureDigitalHealth2020,
  title = {The Future of Digital Health with Federated Learning},
  author = {Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletar{\`i}, Fausto and Roth, Holger R. and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N. and Landman, Bennett A. and {Maier-Hein}, Klaus and Ourselin, S{\'e}bastien and Sheller, Micah and Summers, Ronald M. and Trask, Andrew and Xu, Daguang and Baust, Maximilian and Cardoso, M. Jorge},
  year = {2020},
  month = dec,
  journal = {npj Digital Medicine},
  volume = {3},
  number = {1},
  pages = {119},
  issn = {2398-6352},
  doi = {10.1038/s41746-020-00323-1},
  abstract = {Abstract             Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how federated learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/NJS3ANXZ/Rieke et al. - 2020 - The future of digital health with federated learni.pdf}
}

@misc{sagawaDistributionallyRobustNeural2019,
  title = {Distributionally {{Robust Neural Networks}} for {{Group Shifts}}: {{On}} the {{Importance}} of {{Regularization}} for {{Worst-Case Generalization}}},
  shorttitle = {Distributionally {{Robust Neural Networks}} for {{Group Shifts}}},
  author = {Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1911.08731},
  abstract = {Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{sarmaFederatedLearningImproves2021,
  title = {Federated Learning Improves Site Performance in Multicenter Deep Learning without Data Sharing},
  author = {Sarma, Karthik V and Harmon, Stephanie and Sanford, Thomas and Roth, Holger R and Xu, Ziyue and Tetreault, Jesse and Xu, Daguang and Flores, Mona G and Raman, Alex G and Kulkarni, Rushikesh and Wood, Bradford J and Choyke, Peter L and Priester, Alan M and Marks, Leonard S and Raman, Steven S and Enzmann, Dieter and Turkbey, Baris and Speier, William and Arnold, Corey W},
  year = {2021},
  month = jun,
  journal = {Journal of the American Medical Informatics Association},
  volume = {28},
  number = {6},
  pages = {1259--1264},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocaa341},
  abstract = {Abstract                            Objective               To demonstrate enabling multi-institutional training without centralizing or sharing the underlying physical data via federated learning (FL).                                         Materials and Methods               Deep learning models were trained at each participating institution using local clinical data, and an additional model was trained using FL across all of the institutions.                                         Results               We found that the FL model exhibited superior performance and generalizability to the models trained at single institutions, with an overall performance level that was significantly better than that of any of the institutional models alone when evaluated on held-out test sets from each institution and an outside challenge dataset.                                         Discussion               The power of FL was successfully demonstrated across 3 academic institutions while avoiding the privacy risk associated with the transfer and pooling of patient data.                                         Conclusion               Federated learning is an effective methodology that merits further study to enable accelerated development of models across institutions, enabling greater generalizability in clinical use.},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/PVI5ALFY/Sarma et al. - 2021 - Federated learning improves site performance in mu.pdf}
}

@article{sauerSystematicReviewComparison2022a,
  title = {Systematic {{Review}} and {{Comparison}} of {{Publicly Available ICU Data Sets}}\textemdash{{A Decision Guide}} for {{Clinicians}} and {{Data Scientists}}},
  author = {Sauer, Christopher M. and Dam, Tariq A. and Celi, Leo A. and Faltys, Martin and {de la Hoz}, Miguel A. A. and Adhikari, Lasith and Ziesemer, Kirsten A. and Girbes, Armand and Thoral, Patrick J. and Elbers, Paul},
  year = {2022},
  month = jun,
  journal = {Critical Care Medicine},
  volume = {50},
  number = {6},
  pages = {e581-e588},
  issn = {0090-3493},
  doi = {10.1097/CCM.0000000000005517},
  langid = {english}
}

@article{seymourAssessmentClinicalCriteria2016,
  title = {Assessment of {{Clinical Criteria}} for {{Sepsis}}: {{For}} the {{Third International Consensus Definitions}} for {{Sepsis}} and {{Septic Shock}} ({{Sepsis-3}})},
  shorttitle = {Assessment of {{Clinical Criteria}} for {{Sepsis}}},
  author = {Seymour, Christopher W. and Liu, Vincent X. and Iwashyna, Theodore J. and Brunkhorst, Frank M. and Rea, Thomas D. and Scherag, Andr{\'e} and Rubenfeld, Gordon and Kahn, Jeremy M. and {Shankar-Hari}, Manu and Singer, Mervyn and Deutschman, Clifford S. and Escobar, Gabriel J. and Angus, Derek C.},
  year = {2016},
  month = feb,
  journal = {JAMA},
  volume = {315},
  number = {8},
  pages = {762},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.0288},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/AWY4LBHY/Seymour et al. - 2016 - Assessment of Clinical Criteria for Sepsis For th.pdf}
}

@article{shillanUseMachineLearning2019a,
  title = {Use of Machine Learning to Analyse Routinely Collected Intensive Care Unit Data: A Systematic Review},
  shorttitle = {Use of Machine Learning to Analyse Routinely Collected Intensive Care Unit Data},
  author = {Shillan, Duncan and Sterne, Jonathan A. C. and Champneys, Alan and Gibbison, Ben},
  year = {2019},
  month = dec,
  journal = {Critical Care},
  volume = {23},
  number = {1},
  pages = {284},
  issn = {1364-8535},
  doi = {10.1186/s13054-019-2564-9},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/SGFMEJP4/Shillan et al. - 2019 - Use of machine learning to analyse routinely colle.pdf}
}

@article{silvaPredictingInHospitalMortality2012,
  title = {Predicting {{In-Hospital Mortality}} of {{ICU Patients}}: {{The PhysioNet}}/{{Computing}} in {{Cardiology Challenge}} 2012},
  shorttitle = {Predicting {{In-Hospital Mortality}} of {{ICU Patients}}},
  author = {Silva, Ikaro and Moody, George and Scott, Daniel J. and Celi, Leo A. and Mark, Roger G.},
  year = {2012},
  journal = {Computing in Cardiology},
  volume = {39},
  pages = {245--248},
  issn = {2325-8861},
  abstract = {Acuity scores, such as APACHE, SAPS, MPM, and SOFA, are widely used to account for population differences in studies aiming to compare how medications, care guidelines, surgery, and other interventions impact mortality in Intensive Care Unit (ICU) patients. By contrast, the focus of the PhysioNet/CinC Challenge 2012 is to develop methods for patient-specific prediction of in-hospital mortality. The data used for the challenge consisted of 5 general descriptors and 36 time series (measurements of vital signs and laboratory results) from the first 48 hours of the first available ICU stay of 12,000 adult patients from the MIMIC II database. The challenge was organized as two events: event 1 measured performance of a binary classifier, and event 2 measured performance of a risk estimator. The score of event 1 was the lower of sensitivity and positive predictive value. The score for event 2 was a range-normalized Hosmer-Lemeshow statistic. A baseline algorithm (using SAPS-1) obtained event 1 and 2 scores of 0.3125 and 68.58 respectively. Most participants submitted entries that outperformed the baseline algorithm. The top final scores for events 1 and 2 were 0.5353 and 17.88 respectively.},
  langid = {english},
  pmcid = {PMC3965265},
  pmid = {24678516}
}

@article{singerThirdInternationalConsensus2016,
  title = {The {{Third International Consensus Definitions}} for {{Sepsis}} and {{Septic Shock}} ({{Sepsis-3}})},
  author = {Singer, Mervyn and Deutschman, Clifford S. and Seymour, Christopher Warren and {Shankar-Hari}, Manu and Annane, Djillali and Bauer, Michael and Bellomo, Rinaldo and Bernard, Gordon R. and Chiche, Jean-Daniel and Coopersmith, Craig M. and Hotchkiss, Richard S. and Levy, Mitchell M. and Marshall, John C. and Martin, Greg S. and Opal, Steven M. and Rubenfeld, Gordon D. and {van der Poll}, Tom and Vincent, Jean-Louis and Angus, Derek C.},
  year = {2016},
  month = feb,
  journal = {JAMA},
  volume = {315},
  number = {8},
  pages = {801},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.0287},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/RR8C6XUD/Singer et al. - 2016 - The Third International Consensus Definitions for .pdf}
}

@article{spathisLookingOutofDistributionEnvironments2022,
  title = {Looking for {{Out-of-Distribution Environments}} in {{Multi-center Critical Care Data}}},
  author = {Spathis, Dimitris and Hyland, Stephanie L.},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2205.13398},
  abstract = {Clinical machine learning models show a significant performance drop when tested in settings not seen during training. Domain generalisation models promise to alleviate this problem, however, there is still scepticism about whether they improve over traditional training. In this work, we take a principled approach to identifying Out of Distribution (OoD) environments, motivated by the problem of cross-hospital generalization in critical care. We propose model-based and heuristic approaches to identify OoD environments and systematically compare models with different levels of held-out information. We find that access to OoD data does not translate to increased performance, pointing to inherent limitations in defining potential OoD environments potentially due to data harmonisation and sampling. Echoing similar results with other popular clinical benchmarks in the literature, new approaches are required to evaluate robust models on health records.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@incollection{sunDeepCORALCorrelation2016a,
  title = {Deep {{CORAL}}: {{Correlation Alignment}} for {{Deep Domain Adaptation}}},
  shorttitle = {Deep {{CORAL}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016 {{Workshops}}},
  author = {Sun, Baochen and Saenko, Kate},
  editor = {Hua, Gang and J{\'e}gou, Herv{\'e}},
  year = {2016},
  volume = {9915},
  pages = {443--450},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-49409-8_35},
  isbn = {978-3-319-49408-1 978-3-319-49409-8},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/Q2779GMX/Sun and Saenko - 2016 - Deep CORAL Correlation Alignment for Deep Domain .pdf}
}

@article{thoralSharingICUPatient2021,
  title = {Sharing {{ICU Patient Data Responsibly Under}} the {{Society}} of {{Critical Care Medicine}}/{{European Society}} of {{Intensive Care Medicine Joint Data Science Collaboration}}: {{The Amsterdam University Medical Centers Database}} ({{AmsterdamUMCdb}}) {{Example}}*},
  shorttitle = {Sharing {{ICU Patient Data Responsibly Under}} the {{Society}} of {{Critical Care Medicine}}/{{European Society}} of {{Intensive Care Medicine Joint Data Science Collaboration}}},
  author = {Thoral, Patrick J. and Peppink, Jan M. and Driessen, Ronald H. and Sijbrands, Eric J. G. and Kompanje, Erwin J. O. and Kaplan, Lewis and Bailey, Heatherlee and Kesecioglu, Jozef and Cecconi, Maurizio and Churpek, Matthew and Clermont, Gilles and {van der Schaar}, Mihaela and Ercole, Ari and Girbes, Armand R. J. and Elbers, Paul W. G.},
  year = {2021},
  month = jun,
  journal = {Critical Care Medicine},
  volume = {49},
  number = {6},
  pages = {e563-e577},
  issn = {0090-3493},
  doi = {10.1097/CCM.0000000000004916},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/8TWI5H94/Thoral et al. - 2021 - Sharing ICU Patient Data Responsibly Under the Soc.pdf}
}

@article{vapnikOverviewStatisticalLearning1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  year = {Sept./1999},
  journal = {IEEE Transactions on Neural Networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  issn = {10459227},
  doi = {10.1109/72.788640}
}

@inproceedings{vaswaniAttentionAllYou2017a,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@article{wynantsUntappedPotentialMulticenter2019,
  title = {Untapped Potential of Multicenter Studies: A Review of Cardiovascular Risk Prediction Models Revealed Inappropriate Analyses and Wide Variation in Reporting},
  shorttitle = {Untapped Potential of Multicenter Studies},
  author = {Wynants, L. and Kent, D. M. and Timmerman, D. and Lundquist, C. M. and Van Calster, B.},
  year = {2019},
  month = dec,
  journal = {Diagnostic and Prognostic Research},
  volume = {3},
  number = {1},
  pages = {6},
  issn = {2397-7523},
  doi = {10.1186/s41512-019-0046-9},
  abstract = {Abstract                            Background               Clinical prediction models are often constructed using multicenter databases. Such a data structure poses additional challenges for statistical analysis (clustered data) but offers opportunities for model generalizability to a broad range of centers. The purpose of this study was to describe properties, analysis, and reporting of multicenter studies in the Tufts PACE Clinical Prediction Model Registry and to illustrate consequences of common design and analyses choices.                                         Methods               Fifty randomly selected studies that are included in the Tufts registry as multicenter and published after 2000 underwent full-text screening. Simulated examples illustrate some key concepts relevant to multicenter prediction research.                                         Results               Multicenter studies differed widely in the number of participating centers (range 2 to 5473). Thirty-nine of 50 studies ignored the multicenter nature of data in the statistical analysis. In the others, clustering was resolved by developing the model on only one center, using mixed effects or stratified regression, or by using center-level characteristics as predictors. Twenty-three of 50 studies did not describe the clinical settings or type of centers from which data was obtained. Four of 50 studies discussed neither generalizability nor external validity of the developed model.                                         Conclusions               Regression methods and validation strategies tailored to multicenter studies are underutilized. Reporting on generalizability and potential external validity of the model lacks transparency. Hence, multicenter prediction research has untapped potential.                                         Registration               This review was not registered.},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/W7VP8GYE/Wynants et al. - 2019 - Untapped potential of multicenter studies a revie.pdf}
}

@article{yecheHiRIDICUBenchmarkComprehensiveMachine2021,
  title = {{{HiRID-ICU-Benchmark}} -- {{A Comprehensive Machine Learning Benchmark}} on {{High-resolution ICU Data}}},
  author = {Y{\`e}che, Hugo and Kuznetsova, Rita and Zimmermann, Marc and H{\"u}ser, Matthias and Lyu, Xinrui and Faltys, Martin and R{\"a}tsch, Gunnar},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2111.08536},
  abstract = {The recent success of machine learning methods applied to time series collected from Intensive Care Units (ICU) exposes the lack of standardized machine learning benchmarks for developing and comparing such methods. While raw datasets, such as MIMIC-IV or eICU, can be freely accessed on Physionet, the choice of tasks and pre-processing is often chosen ad-hoc for each publication, limiting comparability across publications. In this work, we aim to improve this situation by providing a benchmark covering a large spectrum of ICU-related tasks. Using the HiRID dataset, we define multiple clinically relevant tasks in collaboration with clinicians. In addition, we provide a reproducible end-to-end pipeline to construct both data and labels. Finally, we provide an in-depth analysis of current state-of-the-art sequence modeling methods, highlighting some limitations of deep learning approaches for this type of data. With this benchmark, we hope to give the research community the possibility of a fair comparison of their work.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@inproceedings{zadroznyTransformingClassifierScores2002,
  title = {Transforming Classifier Scores into Accurate Multiclass Probability Estimates},
  booktitle = {Proceedings of the Eighth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining  - {{KDD}} '02},
  author = {Zadrozny, Bianca and Elkan, Charles},
  editor = {{Usama Fayyad} and {Sunita Sarawagi} and {Paul Bradley}},
  year = {2002},
  month = jul,
  pages = {694},
  publisher = {{ACM Press}},
  address = {{Edmonton, Alberta, Canada}},
  doi = {10.1145/775047.775151},
  isbn = {978-1-58113-567-1},
  langid = {english}
}

@inproceedings{zhangEmpiricalFrameworkDomain2021,
  title = {An Empirical Framework for Domain Generalization in Clinical Settings},
  booktitle = {Proceedings of the {{Conference}} on {{Health}}, {{Inference}}, and {{Learning}}},
  author = {Zhang, Haoran and Dullerud, Natalie and {Seyyed-Kalantari}, Laleh and Morris, Quaid and Joshi, Shalmali and Ghassemi, Marzyeh},
  year = {2021},
  month = apr,
  pages = {279--290},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3450439.3451878},
  isbn = {978-1-4503-8359-2},
  langid = {english},
  file = {/Users/patrick/Zotero/storage/6KAD3PQK/Zhang et al. - 2021 - An empirical framework for domain generalization i.pdf}
}
