\section{Evaluation}

We perform three types of analysis to better understand the quantitative and qualitative characteristics of the DyVeRG model.
In the first quantitative benchmark, we task the model with distinguishing genuine temporal dynamics from realistic imposter data created by other generative models.
The second quantitative analysis asks all of the models, including DyVeRG, to generate a graph corresponding to a slice of time from the data; the generated graphs are then compared to the ground truth.
We conclude with a short qualitative analysis and interpretation of the temporal transitions DyVeRG induces between grammar rules.

\subsection{Datasets}


\begin{table}[t]
    \centering
    \caption{Summary of the datasets used in the evaluation.}\label{tab:datasets}
    \scalebox{0.6}{
    \begin{tabular}{l|ccccc}
        \hline
         &~node count~&~edge count~&~\# timestamps~&~\# interactions~&~\# snapshots~\\
        \hline
        % \texttt{email-enron} & \numprint{184} & \numprint{2216} & \numprint{22633} & \numprint{38172} & 31\\
        DNC~Emails & \numprint{1891} & \numprint{4465} & \numprint{19389} & \numprint{32878} & 11\\
        EU~Emails & \numprint{986} & \numprint{16064} & \numprint{207880} & \numprint{327333} & 19\\
        DBLP & \numprint{95391} & \numprint{164479} & \numprint{21} & \numprint{200792} & 21\\
        Facebook & \numprint{61096} & \numprint{614797} & \numprint{736674} & \numprint{788135} & 29\\
        \hline
    \end{tabular}
    }
\end{table}



In this evaluation, we consider four dynamic datasets, listed in \autoref{tab:datasets}.
DNC~Emails and EU~Emails are email networks where user email accounts are nodes and an email from one user to another at a given time is represented by an undirected edge labeled with a UNIX time.
Both of these datasets are aggregated by month; DNC~Emails contains a number of self-edge loops, while EU~Emails contains none.
The DBLP dataset is an undirected academic coauthorship graph where nodes correspond to researchers and an edge is drawn between two researchers during a particular year \emph{iff} they coauthor a paper during that year.
Finally, the Facebook dataset is an undirected graph tracking friendships on a monthly basis, with two users sharing an edge if they were friends during that month.

\begin{figure}[t]
    \centering
    \include{plots/data}
    \vspace{-5ex}
    \caption{Number of nodes and edges in the datasets over time.}
    \label{fig:data}
\end{figure}

We take snapshots $0$ through $10$ for each dataset.
Because these datasets are dynamic, we summarize their orders and sizes in \autoref{fig:data},
noting they tend to grow over time.

\subsection{Baselines}

We compare the DyVeRG model against 5 baselines.
The Erd\H{o}s-Renyi model generates random graphs of a fixed size $n$ with probability $p$ of an edge between any two nodes~\cite{erdos1959random}; for evaluation we set $n$ and $p = \sfrac{2m}{n(n - 1)}$ to the ground-truth values within each timestep.
The configuration model of Chung and Lu generates a random graph approximating a given degree distribution~\cite{chunglu2006book,hagberg2008networkx}; for this baseline, we use the degree distribution from the dataset.

The Erd\H{o}s-Renyi and Configuration models learn very rudimentary features from an input graph.
The following three graph models are different in that they take a whole graph as input and use their own inductive biases to learn features.
The Stochastic Block Model (SBM) uses matrix reductions to represent graphs with structured communities~\cite{holland1982sbm,peixoto2014graphtool}.
Likewise, the more advanced graph recurrent neural network (GraphRNN)~\cite{you2018graphrnn} is able to learn a generative model from an input collection of graphs by adapting walks over nodes as sequential data.
We also provide a static implementation of DyVeRG based on CNRG~\cite{sikdar2019cnrg}, which we call VeRG, as a final point of comparison.

An important note should be made here that some of the data for GraphRNN is missing from the figures.
This is because, when training and testing the model on the two NVIDIA GeForce RTX 2080 Ti cards available to us, with 10~GB of RAM each, we regularly ran out of memory on the larger datasets.

\subsection{Inference}

\begin{figure}[t]
    \centering
    \include{plots/dyvergence}
    \vspace{-5ex}
    \caption{Dyvergence scores and model rankings. The top subplots show, for each model, the deviations over time from the mean dyvergence score.
    The relative rankings of the models are then shown in the corresponding bottom subplots.
    Note that lower is better, so the best-performing model is listed at the bottom.}
    \label{fig:dyvergence}
\end{figure}

The goal of this task, given a temporal sequence of graphs $\langle G_t \rangle_{t = 0}^{10}$, is to distinguish the graph that genuinely comes next from an assortment of impostors.

% \textbf{JUSTUS MODIFIED VERSION STARTS HERE}
For each timestep $t \in \{0, \dots 9\}$, we extract a VRG from $G_t$ and update it using $G_{t + 1}$, yielding DyVeRG grammars $\langle \mathcal{G}_t \rangle_{t = 1}^{10}$.
These grammars are used to compute dyvergence scores (\textit{cf.} \autoref{sec:dyvergence}) for the ground truth. 
This is performed $10$ times independently for each $(G_t, G_{t + 1})$ pair, and we let $D_t$ denote the mean.% of the 10 dyvergences for this pair.

We use the average ground-truth dyvergences $\{D_0, \dots D_{t - 1}\}$ to compute an estimate $\hat{D}_t$ for the expected dyvergence of the next graph pair $(G_t, G_{t + 1})$---\textit{i.e.}, an estimate for $D_t$.
Specifically,
we let $A_t = \sfrac{(\sum_{i = 0}^{t} D_i)}{(t + 1)}$ and compute
\begin{equation}
    \hat{D}_t = A_{t - 1} + (D_{t - 1} - A_{t - 2}).
\end{equation}
% Given this estimate, we can now calculate a realism score for $G_{t + 1}$ and the graphs generated by the imposter models.

% We now explain how to compute the \emph{realism} of a proposed graph.
Separately, each impostor model $\mathcal{M}$ is trained on $G_{t + 1}$ and $10$ graphs $\langle M_{t + 1, i} \rangle_{i = 1}^{10}$ are sampled from its distribution.
Dyvergence scores are calculated for these graphs by extracting a VRG from $G_t$ and then updating it with each of the $M_{t + 1, i}$; aggregate edits are then computed as in \autoref{eq:dyvergence}.
Average dyvergences $D_{\mathcal{M},t}$ are then found for the $(G_t, M_{t + 1, i})$.
We define the \emph{dyvergence} of $\mathcal{M}_t$ by
\begin{equation}
\text{dyvergence}(G_t, \mathcal{M}_t = |\hat{D}_t - D_{\mathcal{M}, t}|)
\end{equation}
Dyvergence for the ground truth is similarly defined by $\text{dyvergence}(G_t, G_{t + 1}) = |\hat{D}_t - D_t|$.
The lower this score is, the higher our confidence would be that the scored graph comes from the same generating distribution as the data.

We illustrate our results in \autoref{fig:dyvergence}.
Here, we determine success by assigning the ground truth a lower dyvergence score than the impostor graphs.
We outperform the competing baselines on the EU~Emails and Facebook datasets.

Our model is also largely successful on the DNC email graph, ranking the ground truth as least-dyvergent the majority of the time, shown clearly by the ranking subfigures in \autoref{fig:dyvergence}.
The model performs poorly only on the DBLP graph.
We conjecture that the amount of dyvergance in DBLP from one time step to another fluctuates more drastically due to the longer timescale for data aggregation in this dataset; whereas the other three datasets were grouped into monthly snapshots, DBLP snapshots are taken annually.
This might lead to inaccuracies in $\hat{D}_t$, negatively impacting the dyvergence scores for the real graph while boosting performance on imposters that are not as temporally turbulent.

% \textbf{AND ENDS HERE}

\iffalse

%For each timestep $t \in \{0, \dots 9\}$, we extract a VRG from $G_t$ and update it using $G_{t + 1}$, yielding DyVeRG grammars $\langle \mathcal{G}_t \rangle_{t = 1}^{10}$, which we use to compute a dyvergence score (\textit{cf.} \autoref{sec:dyvergence}) for the ground truth.
%This is performed $10$ times independently for each $(G_t, G_{t + 1})$ in the given dataset.
%Each impostor model $\mathcal{M}$ is then trained on each $G_{t + 1}$ and $10$ graphs $\langle M_{t + 1, i} \rangle_{i = 1}^{10}$ are sampled from its distribution.
%Dyvergence scores are calculated for these graphs by extracting a VRG from $G_t$ and then updating it with each of the $M_{t + 1, i}$ and computing the aggregate edits as in \autoref{eq:dyvergence}.

%The results of the inference task with dyvergence scores are illustrated in \autoref{fig:dyvergence}.
%Here, we can gauge success in one of two ways: ideally, the ground truth should be assigned the lowest dyvergence score among all graphs, but we can alternatively settle for being able to consistently distinguish between graphs across time by finding stable temporal rankings.
%The DNC~Emails dataset sees particularly poor performance, as none of the graphs are readily distinguishable, primarily due to its extremely small size.
%However, we outperform baselines on the EU~Emails dataset, consistently picking out the ground truth by a sizable margin against the impostors.

%The remaining two datasets are noticeably larger than the prior two, and performance degrades predictably as time increases.
%On DBLP, our model is able to marginally discern the ground truth from the others, but after around the fifth timestep this is no longer the case.
%Despite this, the graph rankings remain relatively stable over time.
%Finally, our model is completely confused on the Facebook dataset, believing that the ground truth is actually least likely to be the genuine graph.
%However, interestingly, the rankings across time are very stable for this largest dataset, indicating that our model is still finding some salient differences between the graphs.
%Each data point presented in \autoref{fig:dyvergence} is the mean across 10 independent trials.

\fi

\begin{figure}[t]
    \centering
    \include{plots/portrait-divergence}
    \vspace{-5ex}
    \caption{Portrait Divergence comparing a generated graph from each model and timestep against a corresponding ground truth graph. Lower is better.}
    \label{fig:portraitdivergence}
\end{figure}

\subsection{Generation}
A natural way to interrogate a generative graph model---like a graph grammar---is to generate graphs with it.
Generative graph models are widely used in modern AI systems for contrastive and adversarial learning.
Here, we use these models in the more traditional way they might be used for a task like hypothesis-testing; we fit the models, generate a graph at a particular time, and then compare the generated graph with the ground truth.
% For each baseline model, we train on the ground-truth at time $t$ and then generate a graph corresponding to the same timestep.
For each baseline model, we train on the ground truth at time $t$ and then generate at this same time.
If the two graphs are similar according to some empirical measure of graph similarity, then we would say that the model performed well.
% For DyVeRG, we train on time $t - 1$, update with time $t$, and then generate targeting time $t$.
For DyVeRG, we train on time $t - 1$, update with time $t$, and then generate at time $t$.
% Although it is possible for the DyVeRG model to train on all timesteps prior to $t$

Comparing two (or more) graphs is a nontrivial task since the distributions from which graphs can be sampled can behave erratically and are often very high-dimensional.
The most natural way to determine similarity between two graphs is by an isomorphism test; however, in addition to being computationally intractable, this provides a far-too-narrow view of graph similarity.
We instead take two alternative views to graph similarity.
Graph portrait divergence~\cite{bagrow2019portrait} provides a holistic view of a graph based on a matrix of random-walk counts sorted by length; these results will be averaged across 10 independent trials.
Maximum mean discrepancy (MMD)~\cite{gretton2012kernel} is a kernel-based sampling test---which will thus not require any averaging---with desirable stability and computational efficiency characteristics.
For both of these, lower is better.

\begin{figure}[t]
    \centering
    \include{plots/spectrum}
    \vspace{-5ex}
    \caption{The MMD of the eigenvalues (Spectrum) of a generated graph from each model and timestep compared a corresponding ground truth graph. Lower is better.}
    \label{fig:mmdspectrum}
\end{figure}

We begin with the Portrait Divergence results, shown in \autoref{fig:portraitdivergence}.
% As expected, the performance of the model degrades as time moves onward.
In general, we can see that the DyVeRG-generated graphs tend to have lower portrait divergence compared to the other models, thus outperforming them.

Next, we analyse the MMD of the eigenvalue spectra of the graphs' Laplacian matrices.
MMD values are bounded between 0 and 2, with a value of 0 indicating belief that the spectrum of the ground truth and the sample spectra of the generated graphs were certainly sampled from the same underlying distribution.
These results are shown in \autoref{fig:mmdspectrum}.
Here, we find that DyVeRG performs no worse than VeRG, its static counterpart, on three of the datasets.
However, on the DBLP dataset, DyVeRG performs worse than almost all of the other models, despite is static analogue VeRG outperforming every model.

%
%\begin{figure}
%    \centering
%    \include{plots/degree-distribution}
%    \caption{Lower is better.}
%    \label{fig:mmddegree}
%\end{figure}
%Likewise, the results of the MMD on the degree distribution are illustrated in %Fig.~\ref{fig:mmddegree}. Again we find that VeRG and DyVeRG tend to perform quite well. 
There are, of course, many additional metrics by which to compare these models, but the main power of DyVeRG comes from its ability to express graph dynamics in a human-interpretable way. 

\iffalse
\begin{figure}
    \centering
    \include{plots/clustering}
    \caption{Lower is better.}
    \label{fig:mmdclustering}
\end{figure}

\begin{figure}
    \centering
    \include{plots/transitivity}
    \caption{Lower is better.}
    \label{fig:mmdtransitivity}
\end{figure}

\begin{figure}
    \centering
    \include{plots/triangle-count}
    \caption{Lower is better.}
    \label{fig:mmdtriangles}
\end{figure}
\fi

\subsection{Interpretability}
To illustrate how the DyVeRG model can help a practitioner understand a complex temporal dataset, we illustrate some specific examples of frequent \emph{rule transitions}---analogous to subgraph-to-subgraph transitions---learned by the model. 

%VRGs 
%Recall that a grammar not only encodes low-level rules consisting of terminal nodes and edges, and high-level, macro-scale rules consisting of mostly nonterminal symbols, but also meso-scale rules representing various levels of refinement. So, although they are similar in nature to the subgraph-to-subgraph transitions of related work, the rule transitions in the DyVeRG model describe changes at all levels of granularity within the graph. Furthermore, DyVeRG's heirarchical filtration scheme automatically focuses the model on which changes are the most relevant to understanding the graph.

\begin{figure}[t]
    \centering
    \scalebox{0.775}{\input{figures/mostfrequentrules}}
    \caption{A sample of the top rule transitions from the EU~Emails dataset. $\times 61$ denotes that the first rule transition was repeated 61 times. These rule transitions describe various changes in graph structure over time.}
    \label{fig:interpret}
\end{figure}

We focus our analysis here on the first $10$ timesteps of the EU~Emails dataset.
For each $t \in \{0, \dots 9\}$, we extract a grammar on $G_t$ and then update it according to the procedure described in \hyperref[sec:dyverg]{Section~3},
giving us a list of DyVeRG grammars $\langle \mathcal{G}_t \rangle_{t = 1}^{10}$.
Then, given two rules $\dot{P}$ and $\ddot{P}$, each of which could be a rule from any of the grammars $\mathcal{G}_t$ for $t \in \{1, \dots 10\}$, we say that a transition of \emph{type} $\dot{P} \implies \ddot{P}$ has occurred \emph{iff} there is a grammar $\mathcal{G}_i$ such that, during the temporal updating procedure, a rule isomorphic to $\dot{P}$ was modified into a rule isomorphic to $\ddot{P}$.
We then go through our list of grammars and tally up the frequency with which every possible rule transition occurs with the idea in mind that the most frequent rule transitions might provide some salient insight into the dynamics of the dataset.
In \autoref{fig:interpret}, we have a sample of four of the most frequent rule transitions learned from EU~Emails, which we will refer to as $\dot{P}_1 \implies \ddot{P}_1$, $\dots$ $\dot{P}_4 \implies \ddot{P}_4$ respectively.

Both $\dot{P}_1 \implies \ddot{P}_1$ and $\dot{P}_2 \implies \ddot{P}_2$ illustrate that a new structure emerged at time $t + 1$ among nodes that did not already exist in $G_t$.
In the first case, we have the introduction of a new user participating in a email exchange with two other people, and this occurs $61$ times throughout the whole dataset.
In the second, we can see a pair of new users emailing each other, one of whom has sent two emails elsewhere in the network and the other of whom has sent out one additional email,
a structure that occurs $5$ times in the data.
In either case, because there was no rule at time $t$ that $\ddot{P}_1$ and $\ddot{P}_2$ are updated versions of, we can be certain they were additions that participated in a larger connected component that was introduced wholly at time $t + 1$.
This reveals a temporal property of the EU~Email network: it is much more frequent for users to send out emails following periods of inactivity when other previously-inactive users are also sending out emails to new people, than it would be for them to suddenly begin sending emails to active users.

The next transition, $\dot{P}_3 \implies \ddot{P}_3$, shows us that three times throughout the data, a heterophilous dyad%
---a communicating pair of users where one is involved in many emails and the other is not---%
will see a reduction in the number of emails in which the less popular user participates.
By contrast, the final transition $\dot{P}_4 \implies \ddot{P}_4$ exemplifies a more extreme version of the \emph{opposite} phenomenon:
twice in the data, when a heterophilous wedge consisting of two unpopular users is bridged by a high-volume email-sender, the bridging user will experience a reduction in email output while the unpopular users become more popular.

The insights obtained by this analysis are over-specific due largely to the precise nature of rule isomorphism.
However, if a more relaxed view of rule isomorphism is adopted, and the definition of rule transition is broadened, then our model could describe even more general temporal trends.
Even so, our model has shown its ability to provide significant insight into network dynamics.
