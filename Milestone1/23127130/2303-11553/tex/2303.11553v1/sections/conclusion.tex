\section{Conclusion}
We introduced the \textbf{Dy}namic \textbf{Ve}rtex \textbf{R}eplacement \textbf{G}rammar (DyVeRG) formalism, which is a graph grammar model that learns rule transitions from a dynamic graph.
Unlike typical graph grammars, these rule transitions encode the dynamics of a graph's evolution over time.
Further, unlike subgraph-to-subgraph transition models, which learn transitions between small configurations of nodes, DyVeRG encodes rule transitions across multiple levels of granularity.

We show through our quantitative analysis across two tasks and three metrics that the fidelity of the DyVeRG model is comparable or better than many existing graph models, even a highly-parameterized, uninterpretable graph neural network.
Finally, we presented a short case study demonstrating how the induced rule transitions can provide insight into a temporal dataset.