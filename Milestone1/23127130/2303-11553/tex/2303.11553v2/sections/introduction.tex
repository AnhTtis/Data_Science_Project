\section{Introduction}
Like the string grammars upon which they are based, graph grammars usually deal with static data.
Although it might be attractive to think of LHS $\rightarrow$ RHS replacement schemes as indicative of change, growth, or evolution over time, this is rarely the case in grammar-based formalisms.
Instead, grammars are typically used to represent hierarchical refinements of a static structure.
The replacements that occur by applying production rules rarely have anything to do with time.

However, modeling time-varying data for real-life processes is fundamentally important for many scholars and scientists.
Because graphs are capable of expressing immensely-complicated discrete topological relationships, they are widely used to model real world phenomena.
In particular, temporal graph models have come to prominence to account for the time-varying nature of many real phenomena.
For example, the Temporal Exponential Random Graph Model (TERGM)~\cite{hanneke2010tergm}, Dynamic Stochastic Block Model (ARSBM)~\cite{matias2017statistical}, and certain versions of newer Graph Neural Network models (GraphRNN, GRAN)~\cite{you2018graphrnn,liao2019efficient} are able to fit sequential graph data and make predictions about future relationships, but these models are difficult to inspect and tend to break down.

Graph grammars have seen a recent increase in popularity, with applications in molecular synthesis~\cite{kajino2019molecular,guo2022molecular,xu2020reinforcement},
software engineering~\cite{lemetayer1996software,leblebici2017triple}, and robotics~\cite{zhao2020robogrammar}. 
Related models focusing on subgraph-to-subgraph transitions are readily interpretable, but need to be hand-tuned to model subgraphs of a predetermined (usually very small) size, usually for computational complexity reasons ~\cite{hibshman2021sst,benson2018scholp}.
These transition models tend to set out a schema for the set of permitted transitions and perform modeling by simply counting transition frequencies.
Despite their simplicity, these transition models are effective tools for understanding changes in dynamic systems.
However, these models struggle with larger changes outside of 3-or-4-node (or similarly small) subgraph sizes~\cite{pennycuff2018synchronous}.

More recently, researchers have found data-driven ways to learn representative hyperedge replacement grammars (HRG)~\cite{aguinaga2019hrg,wang2018hrg} and vertex replacement grammars (VRG)~\cite{sikdar2019cnrg,sikdar2022attributed}.
These models permit the extraction of production rules from a graph and the resulting grammar can be used to reconstruct the graph or generate similar graphs.
However, as discussed earlier, these models are still limited by the inherent static nature of the formalism.
The lack of a dynamic, interpretable, learnable model presents a clear challenge to modeling real-world relational data. 

\begin{figure}[t]
    \centering
    % \scalebox{0.865}{\input{figures/teaser}}
    \scalebox{0.865}{\includegraphics{figures/teaser.pdf}}
    \caption{An example of a rule transition comprised of two production rules: the left-rule extracted from a graph at time $t$ and the right-rule updated at time $t + 1$.
    Here we see evidence of triadic closure, the disappearance of a node with its incident edge,
    and resulting changes to the LHS symbol.}
    \label{fig:headline}
\end{figure}

In the present work, we tackle this challenge by introducing the \textbf{Dy}namic \textbf{Ve}rtex \textbf{R}eplacement \textbf{G}rammars (DyVeRG).
As the name implies, this model extends the VRG framework, which typically begins with a hierarchical clustering of the graph and then extracts graph rules in a bottom-up fashion from the resulting dendrogram.
In order to adapt VRGs to the dynamic setting, the DyVeRG model finds stable mappings between filtrations of the nodes in the dynamic graph across time.
The filtration mappings provide a transparent a way to inspect the changes in the graph without significant performance degradation.

This dynamic graph grammar takes the form of a sequence of production rules we call \textit{rule transitions} that are interpretable and inspectable.
An example of such a rule transition is illustrated in \autoref{fig:headline}: the rule on the left is extracted from a graph $G_t$ at time $t$; the rule on the right covers the same nodes, but corresponds to time $t + 1$ and incorporates changes from $G_{t + 1}$.
In this example, the nonterminal node on the LHS of the left production rule signifies that the RHS has two boundary edges (used to connect elsewhere in the graph).
The RHS also has four terminal nodes and three terminal edges.
However, as the graph changes between times $t$ and $t + 1$, the topology of the rule on the right of \autoref{fig:headline} changes correspondingly.
The blue dotted edges illustrate the addition of one terminal edge and one new boundary edge, which is why the nonterminal label on the LHS increased from 2 to 3.
The red nodes and wavy dotted lines represent the deletion of a node and edge respectively across this temporal chasm.

The paper is organized as follows.
We first introduce some basic concepts and terminology.
Then, we describe the DyVeRG model with the help of illustrations and examples.
We then introduce the dyvergence score, a byproduct of DyVeRG, and explain how rudimentary forecasting can be done as well as the more-traditional graph generation.
Finally, we provide a quantitative and qualitative analysis of the model on real-world dynamic graphs and compare its predictive performance against other generative models.
