\section{Dynamic Vertex-Replacement Grammars} \label{sec:dyverg}
Incredible advances in theory and application have enabled researchers
to parse real-world networks by learning an appropriate graph grammar---%
including vertex replacement schemes~\cite{sikdar2019cnrg,sikdar2022attributed,guo2022molecular},
hyperedge replacement schemes~\cite{wang2018hrg,aguinaga2019hrg},
and Kemp-Tennenbaum~\cite{hibshman2019bugge} grammars.
An important limitation of these established formalisms
is their semantic inability to express temporality in terms of change to their underlying data.
This presents a serious problem for practitioners:
time does not stand still,
and new data may lead to changes in prior beliefs.
By associating the grammar extracted from a dataset with a filtration on the data,
and describing how filtrations on graphs produce compatible grammars,
we construct temporal transitions between grammars
by defining transitions between their associated filtrations,
which are driven by changes in the data.
In the proceeding sections, we detail the
\textbf{Dy}namic \textbf{Ve}rtex-\textbf{R}eplacement \textbf{G}rammar (DyVeRG)
% \uline{Dy}namic \uline{Ve}rtex-\uline{R}eplacement \uline{G}rammar (DyVeRG)
framework for generalizing VRGs in the time domain.
% \underline{dy}namic \underline{ve}rtex-\underline{r}eplacement \underline{g}rammar (DyVeRG),
% which is a temporal generalization of a VRG.


\subsection{Extracting Rules.} \label{sec:extract}

We describe in this section how to parse a graph with a VRG and how its associated filtration is computed.
For simplicity, we focus on just
two temporally-sequential graphs $G_t = (V_t, E_t)$ and $G_{t + 1} = (V_{t + 1}, E_{t + 1})$ at a time,
though the idea generalizes to an arbitrarily-long finite sequence of graphs.
First, an initial filtration $\mathcal{F}_t^{\text{clust}}$ on $G_t$ is produced
using the Leiden hierarchical clustering algorithm~\cite{traag2019leiden}.
We choose to use Leiden for our analyses as opposed to the myriad alternatives---%
    the Louvain algorithm~\cite{blondel2008louvain},
    smart local moving~\cite{waltman2013smart},
    hierarchical Markov clustering~\cite{wang2021markov},
    recursive spectral bi-partitioning~\cite{hagen1992spectral}%
---because its iterative modularity-maximization approach is intuitively appealing,
and it realizes better performance~\cite{traag2019leiden} than Louvain and smart local mover
(on which Leiden is based) while remaining efficient on larger graphs.

Taking inspiration from clustering-based node replacement grammars~\cite{sikdar2019cnrg},
we use the filtration $\mathcal{F}_t^{\text{clust}}$ derived from the hierarchical clustering
to recursively extract rules for the grammar.
Starting from the bottom,
we consider subfiltrations covering at most $\mu$ nodes (terminal or nonterminal) total,
where $\mu \in \mathbb{N}_+$ is set \textit{a priori} to limit the maximum size of any rule's RHS.
Among all subfiltrations of size at-most $\mu$,
we then select a subfiltration $\mathcal{F}^*$
that minimizes the overall description length of the grammar.
$\mathcal{F}^*$ then determines a rule $P^*$ that gets added to our grammar and
$\mathcal{F}_t^{\text{clust}}$ is compressed until every node is in the same cover,
as described by Sikdar~\emph{et al.}~\cite{sikdar2019cnrg}.



\begin{figure}[t]
    \centering
    \vspace{0.725ex}
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        % \scalebox{0.5}{\input{figures/exgraph1}}
        \scalebox{0.5}{\includegraphics{figures/exgraph1.pdf}}
        \caption{Graph $G_t$.}\label{fig:ex1a}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\linewidth}
        \centering
        % \scalebox{0.5}{\input{figures/exfiltrationclust}}
        \scalebox{0.5}{\includegraphics{figures/exfiltrationclust.pdf}}
        \caption{Clustering $\mathcal{F}_t^{\text{clust}}$.}\label{fig:ex1b}
        ~~\\
        % \scalebox{0.5}{\input{figures/exfiltrationgram}}
        \scalebox{0.5}{\includegraphics{figures/exfiltrationgram.pdf}}
        \caption{Grammar $\mathcal{F}_t^{\text{gram}}$.}\label{fig:ex1c}
    \end{subfigure}%
    \begin{subfigure}[b]{0.35\linewidth}
        \centering
        % \scalebox{0.5}{\input{figures/exdecomposition1}}
        \scalebox{0.5}{\includegraphics{figures/exdecomposition1.pdf}}
        \caption{Rule~decomposition.}\label{fig:ex1d}
    \end{subfigure}
    \caption{Overview of the extraction process for a VRG on a static graph $G_t$ pictured in (a).
    A filtration induced by a hierarchical clustering is shown in (b),
    from which the rules in (d) are extracted bottom-up.
    At the same time, the filtration in (c) is derived from the node coverings the extracted rules produce.}\label{fig:ex1}
\end{figure}

Concurrently, we also construct a filtration $\mathcal{F}_t^{\text{gram}}$
whose node covers are determined by the right-hand sides of the rules in the grammar as they are extracted.
The filtration $\mathcal{F}_t^{\text{gram}}$ acts as a rule-based hierarchical decomposition of $G_t$,
keeping track of the parallel hierarchical structure shared by the rules in the grammar and the nodes in the graph.
This naturally produces a one-to-one correspondence between the rules $R_t = \left\{P_{t, 1}, \dots P_{t, r}\right\}$ in our (unweighted) grammar and their corresponding covers in $\mathcal{F}_t^{\text{gram}}$,
allowing us to construct a surjective association $f_t: V_t \surjection R_t$ between each node $v \in V_t$ and the unique rule $f_t(v) \in R_t$ that was extracted with $v$ as a terminal node.
Further, we keep track of the particular terminal symbol node on the right-hand side of $f_t(v)$ corresponding to $v$ when $f_t(v)$ was extracted.
We call this $\alpha_t(v)$, the \emph{alias} of $v$, where $\alpha_t: V_t \injection T$.
An illustration of this process on a small example network is shown in \autoref{fig:ex1}.
The two filtrations are highlighted along with a hierarchical decomposition of the graph induced by the grammar's rules.

The \emph{root} of a grammar is defined to be the rule whose left-hand side is the distinguished starting symbol $S$.
Clearly, this rule covers every node in $G_t$.
Given two rules $\dot{P}_t, \ddot{P}_t \in R_t$, we say $\dot{P}_t$ is an \emph{ancestor} of $\ddot{P}_t$ \emph{iff} every node covered by $\ddot{P}_t$ in the filtration is also covered by $\ddot{P}_t$.
If additionally $\dot{P}_t \neq \ddot{P}_t$, then it is a \emph{proper ancestor}.
% Further, we say that $\dddot{P}_t \in R_t$ is a \emph{common ancestor} of $\dot{P}_t$ and $\ddot{P}_t$ \emph{iff} $\dddot{P}_t$ covers every node covered by $\dot{P}_t$ and $\ddot{P}_t$.
We define one rule to be a \emph{descendant} of another conversely to how ancestors are defined.
Finally, a \emph{common ancestor} of a set of rules $\tilde{R}_t \subseteq R_t$ is rule
$\tilde{P}_t$ that is an ancestor of rule in $\tilde{R}_t$,
and the \emph{least common ancestor} is the one having no common ancestor of $\tilde{R}$ as a proper descendant.
Note that the least common ancestor of a nonempty subset of $R_t$ always exists
since the root rule is an ancestor of every rule in $\mathcal{G}_t$.


\subsection{Updating the Filtration} \label{sec:update}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.25\linewidth}
        \centering
        % \scalebox{0.5}{\input{figures/exgraph2}}
        \scalebox{0.5}{\includegraphics{figures/exgraph2.pdf}}
        \caption{Graph $G_{t + 1}$.}
    \end{subfigure}%
    \begin{subfigure}[b]{0.4\linewidth}
        \centering
        % \scalebox{0.5}{\input{figures/exfiltrationupdate}}
        \scalebox{0.5}{\includegraphics{figures/exfiltrationupdate.pdf}}
        \caption{Updated filtration $\mathcal{F}_{t + 1}^{\text{gram}}$.}
    \end{subfigure}%
    \begin{subfigure}[b]{0.35\linewidth}
        \centering
        % \scalebox{0.5}{\input{figures/exdecomposition2}}
        \scalebox{0.5}{\includegraphics{figures/exdecomposition2.pdf}}
        \caption{Updated grammar.}
    \end{subfigure}
    \caption{The graph from \autoref{fig:ex1a}, the filtration from \autoref{fig:ex1c}, and the grammar from \autoref{fig:ex1d} are shown here with changes from time $t + 1$.
    The changes to $G_{t + 1}$ in (a) stimulate changes to the filtration in (b),
    modifying the corresponding rules in the grammar (c).
    Following \autoref{fig:headline}, the node and edge addition is shown in blue,
    while node and edge removal is indicated in red.
    Nonterminal nodes' borders are colored blue or red if they increased or decreased in value respectively.
    }\label{fig:ex2}
\end{figure}

% We are now ready to talk about how temporal changes to a graph influence the rules and hierarchy of a VRG;
We will refer interchangeably to the covers in the filtration $\mathcal{F}_t^{\text{gram}}$ and the rules $P_{t, i} \in R_t$ in the grammar $\mathcal{G}_t$.
For a visual summary of the proceeding description, please refer to \autoref{fig:ex2}.
% As a result, we will speak about $\mathcal{G}_t$ as an \emph{unweighted} grammar and distinguish between isomorphic rules based on their location in $\mathcal{F}_t^{\text{gram}}$.
First, we categorize the edges $(u, v) \in E_t \union E_{t + 1}$:
\begin{enumerate}
    \item[\textsc{i.}]
        \emph{persistent}:
        % where $u, v \in V_t$ and $(u, v) \in E_t \setminus E_{t + 1}$
        $u \in V_t$,
        $v \in V_t$,
        $u \in V_{t + 1}$,
        $v \in V_{t + 1}$,\\
        and $(u, v) \in E_t \intersect E_{t + 1}$
    \item[\textsc{ii.}]
        \emph{internal additions}:
        % where $u, v \in V_t \intersect V_{t + 1}$
        % and $(u, v) \in E_{t + 1} \setminus E_t$
        $u \in V_t$,
        $v \in V_t$,
        $u \in V_{t + 1}$,\\
        $v \in V_{t + 1}$,
        and $(u, v) \in E_{t + 1} \setminus E_t$
    \item[\textsc{iii.}]
        \emph{frontier additions}:
        % where $u \in V_t \intersect V_{t + 1}$,
        % $v \in V_{t + 1} \setminus V_t$,
        % and $(u, v) \in E_{t + 1}$
        $u \in V_t$,
        $v \not \in V_t$,
        $u \in V_{t + 1}$,\\
        $v \in V_{t + 1}$,
        and $(u, v) \in E_{t + 1} \setminus E_t$
    \item[\textsc{iv.}]
        \emph{external additions}:
        % where $u, v \in V_{t + 1} \setminus V_t$ and $(u, v) \in E_{t + 1} \setminus E_t$
        $u \not \in V_t$,
        $v \not \in V_t$,
        $u \in V_{t + 1}$,\\
        $v \in V_{t + 1}$,
        and $(u, v) \in E_{t + 1} \setminus E_t$
    \item[\textsc{v.}]
        \emph{edge deletions}:
        % where $u, v \in V_t$ and $(u, v) \in E_t \setminus E_{t + 1}$
        $u \in V_t$,
        $v \in V_t$,
        and $(u, v) \in E_t \setminus E_{t + 1}$
\end{enumerate}
Examples of edges from each category are demonstrated in \autoref{fig:edgeclasses}.
These classes determine how each edge induces a change in the filtration:
class \textsc{i.} edges do not influence the filtration,
class \textsc{ii.} edges add new connections between already-existing nodes (thus altering the induced subgraph covers of the filtration)
class \textsc{iii.} edges introduce a new neighbor for an already-existing node,
class \textsc{iv.} edges produce two entirely-new neighboring nodes,
and class \textsc{v.} edges account for the removal of connections from the graph,
which may or may not be associated with nodes' exodus from the network.
Of these, only classes \textsc{iii.}, \textsc{iv.}, and \textsc{v.} are capable of causing structural changes to the hierarchy of the filtration,
but all except for class \textsc{i.} edges will affect the grammar's rules.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        % \scalebox{0.7}{\input{figures/communities}}
        \scalebox{0.7}{\includegraphics{figures/communities.pdf}}
        \caption{A graph with temporal updates.}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        % \scalebox{0.7}{\input{figures/filtration}}
        \scalebox{0.7}{\includegraphics{figures/filtration.pdf}}
        \caption{A filtration mirroring the updates.}
    \end{subfigure}
    \caption{A dynamic graph with three well-defined communities (a) and an associated filtration (b) are shown with temporal updates.
    Class \textsc{i.} edges are shown in solid black.
    The blue dotted edges incident to $v$ in (a) are class \textsc{ii.} edges;
    the blue dotted edges between $u$ and the black nodes are class \textsc{iii.} edges;
    all other blue dotted edges are class \textsc{iv.} edges.
    The wavy edges in red belong to class \textsc{v.}
    }\label{fig:edgeclasses}
\end{figure}

\subsubsection{Internal Additions.}
If an edge $(u, v)$ corresponds to an \emph{internal addition},
we first find the covering rules $f_t(u) = P_u$ and $f_t(v) = P_v$ of the nodes incident on that edge
and let $G_u$ and $G_v$ be their respective RHS graphs.
If $G_u = G_v$, then we simply add an edge between $\alpha_t(u)$ and $\alpha_t(v)$.
However, if $G_u \neq G_v$, then we find their least common ancestor
and add an edge between the appropriate nodes on its right-hand side.
Note that if a nonterminal symbol is incident on the edge added,
then this will necessarily change the symbol---recall that the nonterminal symbols
are defined to be the sum of their degree with their boundary degree.
This change in the symbol must be propagated \emph{down} through the hierarchy
by commensurately increasing LHS's of rules and adding boundary degrees.


\subsubsection{Frontier \& External Additions.}
We handle frontier and external edge additions jointly by cases.
Given an edge $(u, v)$ of class \textsc{iii.} or \textsc{iv.},
let $H_{(u, v)} \subgraph G_{t + 1}$ be
the maximally connected induced subgraph of $G_{t + 1}$ containing $(u, v)$.

In the first case, suppose none of the vertices of $H_{(u, v)}$ coincide with $G_t$.
We begin by independently extracting a grammar $\mathcal{H}_{(u, v)}$ on $H_{(u, v)}$ with its own induced filtration.
We then \emph{merge} this grammar with $\mathcal{G}_t$ by combining the two filtrations under one larger cover.
Specifically, this takes the form of a new root rule whose LHS is $S$ and RHS consists of two disconnected nonterminal symbols%
---one for $G_t$ and one for $H_{(u, v)}$---%
incorporating the rules of $\mathcal{G}_t$ and $\mathcal{H}_{(u, v)}$ as descendants.
To disambiguate, the LHS's for the root rules $\mathcal{G}_t$ and $\mathcal{H}_{(u, v)}$ are updated accordingly.

In the second case, there is at least one node in common between $H_{(u, v)}$ and $G_t$.
% Without loss of generality, let $u \in V_t$ be this node, and let $v \not \in V_t$,
% so that $(u, v)$ is a class \textsc{iii.} edge.
Define the \emph{frontier} $F(G_t, H_{(u, v)})$ between $G_t$ and $H_{(u, v)}$
to be the collection of all such class \textsc{iii.} edges.
Then, for each edge $(u_G, v_H) \in F(G_t, H_{(u, v)})$,
we find the rules $f_t(u_G) = P_{u_G}$ and $P_{v_H}$ from $\mathcal{G}_t$ and $\mathcal{H}_{(u, v)}$
that cover $u_G$ and $v_H$ respectively, and increase their boundary degrees by $1$
to indicate that this node should expect to receive a new edge.
This increase in boundary degree necessitates a change to the LHS symbols of the two rules,
which in turn induces more changes to their ancestor rules;
these changes propagate \emph{up} to the roots of their respective hierarchies.
Finally, once these changes have been made for each frontier edge,
a new rule is created (\textit{cf.} the prior case) with two nonterminal symbols
connected by as many edges as there are in $F(G_t, H_{(u, v)})$, concluding the subgrammar-merging process.
This accounts for all of the class \textsc{iii.} and \textsc{iv.} edges that participated in the connected component $H_{(u, v)}$.


\subsubsection{Deletions.}
An edge deletion $(u, v)$ must have both of its incident nodes existing in $G_t$,
but they need not exist in $G_{t + 1}$.
As a result, we handle class \textsc{v.} edges by first finding the covering rules $f_t(u)$ and $f_t(v)$ and removing the edge between the nodes corresponding to $\alpha_t(u)$ and $\alpha_t(v)$ from their common ancestor.
Note that if this edge was incident on a nonterminal symbol,
its removal will cause a cascade of changes that must be propagated \emph{down} the hierarchy.
Then, if $u$ is not present in $G_{t + 1}$, we also remove the node $\alpha_t(u)$ from the RHS of $f_t(u)$;
similarly with $v$ and the removal of $\alpha_t(v)$ from $f_t(v)$.

% \textit{insert figure here showing $G_t$, $G_{t + 1}$, and the intermediate subgraphs defined above.}


\subsection{Measuring Deviation} \label{sec:dyvergence}
Now that we know how to take a grammar $\mathcal{G}_t$ and temporally modify it into $\mathcal{G}_{t + 1}$,
we can analyze what the specific changes were between the two grammars.
From the process delineated in \autoref{sec:extract},
we obtain a natural correspondence
$\pi_t: R_t \to R_{t + 1}$
between every rule $P_t \in R_t$ from $\mathcal{G}_t$ and its updated version $\pi_t(P_t) \in R_{t + 1}$ in $\mathcal{G}_{t + 1}$.
We can use this mapping to quantify the difference between the two grammars in terms of the number of changes introduced by the temporal update process.
Given a rule $P_{t + 1} \in R_{t + 1}$, we define the change introduced by this rule by computing the graph edit distance (GED)~\cite{zeina2015edit} between the RHS's of $P_{t + 1}$ and $\pi_t^{-1}(\{P_{t + 1}\})$
(with a small penalty to any modifications that need to be made to make the LHS's the same)\footnote{The notation $\pi_t^{-1}(\{P_{t + 1}\})$ denotes the \emph{preimage} of $P_{t + 1}$ under $\pi_t$.}.
If $P_{t + 1}$ is a rule introduced as part of the subgrammar-merging process for class \textsc{iii.} and \textsc{iv.} edges, then we let $\pi_t^{-1}(\{P_{t + 1}\})$ be the empty graph by definition.
By aggregating these edit distances across the rules of $\mathcal{G}_{t + 1}$,
we get an indication of how much $\mathcal{G}_t$ had to be perturbed to accommodate the data seen in $G_{t + 1}$.
Specifically, we compute
\begin{equation}\label{eq:dyvergence}
    \Delta
    = -\ln\frac{1}{1 + ~~\displaystyle\smashoperator{\sum_{P_{t + 1} \in R_{t + 1}}}~~\text{GED}\Big(\pi^{\scriptscriptstyle -1}_t\left(\big\{P_{t + 1}\big\}\right), P_{t + 1}\Big)}.
\end{equation}
%which represents normalized distance between the two grammars.

\subsection{Generating Graphs}
Finally, we can use the rules from the updated grammar $\mathcal{G}_{t + 1}$ to generate graphs.
First, we post-process the rules of the unweighted grammar $\mathcal{G}_{t + 1}$
into the rules of a weighted grammar by combining all isomorphic rules and weighting them by how frequently (up to isomorphism) each rule occurred in $\mathcal{G}_{t + 1}$.
The idea now is identical to the approach traditionally taken by weighted VRGs.
We start with the root rule, which has LHS symbol $S$,
and use the structure on its RHS as our initial graph $\hat{G}_{t + 1}$.
We then iteratively grow the graph by randomly selecting a nonterminal symbol in $\hat{G}_{t + 1}$
and randomly sampling a compatible rule to apply at that symbol,
with the sampling probability for the rules determined by the frequencies of the possible candidate rules for that nonterminal symbol.
Once no nonterminal symbols remain in $\hat{G}_{t + 1}$, we stop and obtain our resulting graph.

% future work: some of the different ways generation could also be done,
% such as fully-dynamic, (constrained-)bag-of-rules, greedy, and beam-search-optimization
