% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{EMNLP2023}
\usepackage[]{EMNLP2023}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% % cui
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float}
\usepackage{subfigure}
\usepackage{tcolorbox}


\newcommand{\thickhline}{\noalign{\hrule height 1pt}}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}
% % end cui

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\newcommand{\zekun}[1]{\textbf{\small {\color{cyan}[#1 -Zekun]}}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
% \setlength\titlebox{6cm}
\title{\textsc{ChatEdit}: Towards Multi-turn Interactive Facial Image Editing via Dialogue}


\author{
    Xing Cui$^1$\thanks{\ \ Equal contribution.} \quad 
    Zekun Li$^2$\samethanks{} \quad 
    Peipei Li$^1$\thanks{\ \ Corresponding author.} \quad 
    Yibo Hu$^3$ \quad \\
    \textbf{Hailin Shi$^3$ \quad 
    Chunshui Cao$^4$ \quad 
    Zhaofeng He$^1$} \\
    % \textbf{Wenquan Wu$^4$ \quad Zheng-Yu Niu$^4$ \quad Hua Wu$^4$ \quad Minlie Huang$^{1,3}$\thanks{\ \ Corresponding author.}} \\ 
    \small $^1$Beijing University of Posts and Telecommunications \quad 
    \small $^2$University of California, Santa Barbara \\
    \small $^3$NIO \quad 
    \small $^4$WATRIX.AI \\ 
    \small \tt \{cuixing, lipeipei, zhaofenghe\}@bupt.edu.cn\quad zekunli@cs.ucsb.edu \\
    \small \tt \{huyibo871079699, hailinshi.work\}@gmail.com \quad chunshui.cao@watrix.ai \\
}

\begin{document}
% \maketitle

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle

\vspace{-10pt}
\begin{center}
    \includegraphics[width=\linewidth]{main_fig/dataset_overview-caption.pdf}%
    \captionof{figure}{\textbf{Illustration of the multi-turn interactive facial image editing task.} 
    The system is required to track the user requests on the facial attributes (in the gray box), edit the image, and generate the response.}
    \label{fig:dataset}
    \vspace*{4.5mm}
\end{center}%
}]

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{~Equal contribution.}
\footnotetext[2]{~Corresponding Author.}


\begin{abstract}
This paper explores interactive facial image editing via dialogue and introduces the \textsc{ChatEdit} benchmark dataset for evaluating image editing and conversation abilities in this context.
\textsc{ChatEdit} is constructed from the CelebA-HQ dataset, incorporating annotated multi-turn dialogues corresponding to user edit requests on the images. The dataset is challenging, as it requires the system to dynamically track user requests, edit images, and generate appropriate responses. Accordingly, we propose three benchmark tasks: (\textit{i}) user edit request tracking, (\textit{i}i) image editing, and (\textit{iii}) response generation. We present a novel baseline framework that integrates a dialogue module for both tracking user requests and generating responses and an image editing module for image editing. Unlike previous approaches, our framework directly tracks user edit requests from the entire dialogue history up to the current turn and modifies the original image rather than adjusting the previous turn's output, thereby reducing error accumulation and preventing attribute forgetfulness. Extensive experiments on the \textsc{ChatEdit} dataset underline our framework's superior performance against prior models, while also highlighting potential room for further research. We will release the code and data publicly to facilitate advancements in complex interactive facial image editing
\footnote[3]{~Our data and codes are available at \url{https://github.com/cuixing100876/ChatEdit}}.
\end{abstract}


\begin{figure}

\begin{center}
\includegraphics[width=0.95\linewidth]{main_fig/step_vs_combine_v2_final.pdf}
\end{center}
\vspace{-5pt}
   \caption{\textbf{Comparison of previous repeated single-turn editing approaches and our proposed multi-turn editing approach.} 
   The cascaded errors in the single-turn approach lead to unintended changes in gender and eye makeup.   }\label{fig:step_vs_combine}
\vspace{-6mm}
\end{figure}


\begin{table*}[t!]
\centering
\caption{\textbf{Comparison of our \textsc{ChatEdit} with existing works on image editing.} }
\vspace{-5pt}
\label{tab:methods_comparision}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lcccccc}
\thickhline
            \textbf{Method} & \textbf{Scene}  & \textbf{Multi-turn Interaction} & \textbf{System Feedback} &\textbf{Dataset} &  \textbf{Text Data} & \textbf{Attributes}    \\
            \hline
            TransEditor~\cite{xu2022transeditor} & Facial image   &  \xmark &  \xmark & CelebA-HQ & \xmark &4\\
            HairCLIP~\cite{wei2022hairclip} & Facial image   &  \xmark &  \xmark & CelebA-HQ & \xmark &$2^{\dagger}$\\
            StyleCLIP~\cite{patashnik2021styleclip} & Facial image   &  \xmark &  \xmark & CelebA-HQ & \xmark &8\\
            ChatPainter~\cite{sharma2018chatpainter} & Realistic image   &  \xmark &  \xmark &  MS COCO & dialogue &-\\
            TiGAN~\cite{zhou2022tigan} & Facial image   &  \cmark &  \xmark & CelebA-HQ & \xmark &8\\
            Talk-to-Edit~\cite{jiang2021talk} & Facial image  &  \cmark  &  \cmark & CelebA-dialog & user utterance &5\\
            \hline
            \textsc{ChatEdit} (Ours) & Facial image   &  \cmark &  \cmark &  \textsc{ChatEdit} & dialogue &21 \\ 
\thickhline
\multicolumn{7}{l}{$\dagger$: HairCLIP only considers hairstyle and hair color. 44 text descriptions are collected for hairstyle and 12 text descriptions for hair color.}
\end{tabular}
}
\vspace{-3mm}
\end{table*}

\section{Introduction}
With the rise of deep generative models such as GANs~\cite{goodfellow2020generative,karras2019style,li2023progressive} and DDPMs~\cite{ho2020denoising,sohl2015deep,zhang2023towards}, significant progress has been achieved in instruction-based facial image editing~\cite{xu2022transeditor,patashnik2021styleclip,li2023pluralistic}. However, an emerging scenario is multi-turn interactive editing, allowing users to iteratively refine their editing instructions through system interaction~\cite{sharma2018chatpainter,cheng2020sequential,kim2019codraw,jiang2021talk}.  Existing approaches~\cite{zhou2022tigan,el2019tell,kim2019codraw} typically treat multi-turn editing as a sequence of successive single-turn edits, leading to issues such as attribute forgetting and error accumulation, as depicted in Fig.~\ref{fig:step_vs_combine} (first line). Moreover, these techniques don't fully harness interactivity and user experience. For example, \cite{sharma2018chatpainter,cheng2020sequential,kim2019codraw} solely process user inputs without offering natural language feedback, while others~\cite{jiang2021talk} rely on rigid, hand-crafted response templates, limiting flexibility and naturalness.

To facilitate the research on multi-turn interactive facial image editing, we introduce a novel benchmark dataset named \textsc{ChatEdit}. Sourced from the CelebA-HQ dataset~\cite{karras2018progressive}, \textsc{ChatEdit} enhances a selected set of 12k images with annotated multi-turn dialogues that align with user edit requests for facial images. The annotations include user utterances, system responses, and the user's ``belief state'', which represents the user edit requests from the beginning of the dialogue to the current turn.
Fig.~\ref{fig:dataset} illustrates that success on \textsc{ChatEdit} necessitates the system to track the user edit requests, edit images based on tracked requests, and provide natural language responses to engage with users. To evaluate the performance of multi-turn interactive editing, we define three tasks: (\textit{i}) user edit request tracking, (\textit{ii}) image editing, and (\textit{iii}) response generation. Correspondingly, we introduce a comprehensive set of metrics that evaluate both response and editing quality.

Based on our benchmark dataset, we propose a baseline framework for multi-turn interactive image editing. Our framework seamlessly integrates a language dialogue module and a visual image editing module. Specifically, we employ an end-to-end task-oriented dialogue (TOD) model to extract the user's image edit requests and generate appropriate responses based on the current conversation context. These tracked user requests are transformed into a text prompt to guide the text-based image editing model, StyleCLIP~\cite{patashnik2021styleclip}, in performing the desired manipulations on the input raw image.
As illustrated in Fig.~\ref{fig:step_vs_combine}’s second line, our benchmark dataset and the proposed framework enable direct editing on the input raw image instead of cascaded modifications to images from previous turns, thereby reducing error accumulation and attribute forgetting issues.
We perform extensive experiments with diverse settings to investigate the effectiveness of our proposed framework on the \textsc{ChatEdit} dataset. The results suggest that our proposed framework is superior to the previously prevalent cascaded single-turn editing methods regarding both image editing quality and response diversity. 

To sum up, our contributions are three-fold:
\textbf{(1)} We introduce the \textsc{ChatEdit} benchmark dataset, which could serve as a valuable resource for advancing research in multi-turn interactive facial image editing.
\textbf{(2)} We propose a novel framework that seamlessly combines a task-oriented dialogue module and an image editing module. This framework effectively tracks user requests, performs image editing, and generates system responses. Importantly, it addresses the issues of attribute forgetting and error accumulation prevalent in previous methods.
\textbf{(3)} Through qualitative and quantitative evaluations on the \textsc{ChatEdit} dataset, we demonstrate the superiority of our proposed multi-turn editing framework over previous cascaded single-turn editing methods. We believe these results not only highlight the strengths of our proposed approach but also spur further exploration in this field.



\section{Related Work}
\paragraph{Facial Image Editing} Traditional image editing techniques have focused on modifying specific given attributes such as age~\cite{li2020hierarchical,li2019global}, hairstyle~\cite{wei2022hairclip}, or other predefined attributes~\cite{zhou2022tigan,li2020dual,zhang2022petsgan}. In recent years, with the development of pre-trained vision-language models~\cite{radford2021learning,li2022blip,wang2023learning}, there has been a growing interest in human-computer interaction scenarios~\cite{kottur2022tell,jiang2021talk}. This leads researchers to explore interactive image editing, where users can dynamically adjust their editing requests through interaction with the system~\cite{kim2019codraw,lachmy2022draw,zhou2022tigan,jiang2021talk}. For instance, TiGAN~\cite{zhou2022tigan} generates images iteratively based on successive editing steps during a conversation.
% utilizing the pre-trained cross-modal model CLIP~\cite{radford2021learning} and the generative model StyleGAN~\cite{karras2020analyzing}.

However, these methods merely ``listen'' to user requests without generating system responses, constraining their interaction capability. A recent work called Talk-to-Edit~\cite{jiang2021talk} attempts to address this by introducing a rule-based method to generate system response. However, this approach lacks flexibility and struggles with unforeseen scenarios that are not predefined. Furthermore, these methods treat multi-turn editing as a sequence of individual single-turn edits, resulting in issues such as error accumulation and attribute forgetting as the number of interactions increases.

\begin{table}
\small
\caption{\textbf{Illustration of editable attributes in \textsc{ChatEdit},} which are categorized into four groups (slots). 
}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.2}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular} {@{}lp{6cm}@{}}
\toprule
\textbf{Slot}  &\textbf{Attribute}\\
\cline{1-2}
Expression\textbf &\textit{smiling, no smiling, angry, sad}\\
\cline{1-2}
Hair color &\textit{brown hair, blond hair, black hair, gray hair}\\
\cline{1-2}
Hair &\textit{receding hairline, sideburns, bangs, no bangs, mustache, goatee, no beard}\\
\cline{1-2}
Makeup &\textit{no makeup, heavy makeup, lipstick, bushy eyebrows, rosy cheeks, pale skin}\\
\bottomrule
\end{tabular}
}
\label{tab:attributes}
\vspace{-3mm}
\end{table}

\paragraph{Task-oriented Dialogue} There are two primary approaches to TOD. Traditional systems adopt a pipelined approach comprising four modules. Firstly, the natural language understanding (NLU) module~\cite{abro2022natural} converts user requests into semantic slots, domain information, and user intention. Secondly, the dialogue state tracking (DST) module~\cite{wu2019transferable,le2019non,lin2021zero,heck2023chatgpt} extracts the dialogue state, which records user requests in the form of slot-value pairs. The dialogue policy learning (POL) module~\cite{chen2017agent,geishauser2022dynamic} determines the next action of the dialogue agent based on the dialogue state. Finally, the natural language generation (NLG) module~\cite{elder2020make} generates the system response.
In more recent times, there has been a shift towards end-to-end task-oriented dialogue systems~\cite{hosseini2020simple,wang2022task,le2020uniconv,zeng2023futuretod}. For example, PPTOD~\cite{su2022multi} employs multi-task training that simultaneously processes all sub-tasks. Currently, there have been approaches that utilize Large Language Models (LLMs) like ChatGPT for task-oriented dialogues~\cite{li2023guiding}. However, existing TOD systems primarily focus on scenarios such as booking and consulting, neglecting the interactive image editing scenario.

\section{\textsc{ChatEdit} Benchmark Dataset}
\textsc{ChatEdit} is designed to simulate a scenario where a user interacts with a system to manipulate a facial image. Each sample in the \textsc{ChatEdit} dataset comprises a facial image and a dialogue between the user and the system regarding the editing of the image. Notably, each turn of the dialogue is annotated with the user belief state, representing the user requests that guide both response generation and image editing. In this section, we will first introduce how to construct the dataset and its statistics. Then, we introduce benchmark tasks of \textsc{ChatEdit} and corresponding evaluation metrics.

\begin{table}[t]
\small
\scriptsize
    \begin{center}
    \caption{\textbf{Statistics of dialogues in \textsc{ChatEdit} dataset.}}
    \label{tab:statics_of_dialogue}
    \vspace{-5pt}
    \resizebox{0.9\linewidth}{!}{%
        \begin{tabular}{lccc}
        \toprule[\heavyrulewidth]
            Total \# dialogues &12,000\\
            Total \# utterances &96,174\\
            Avg \# turns per dialogue &4.0\\
            Avg \# utterances per dialogue &8.0\\
            Avg \# words per user turns &11.6\\
            Avg \# words per system turns &11.3\\
            Avg \# attributes mentioned per dialogue &6.3\\
        \bottomrule[\heavyrulewidth]
        \end{tabular}
        }
    \end{center}
\vspace{-3mm}
\end{table}


\subsection{\textsc{ChatEdit} Dataset Construction}
\paragraph{Facial Image Data}
We construct the \textsc{ChatEdit} dataset on top of the CelebA-HQ~\cite{karras2018progressive}, which is a high-resolution facial image dataset with 30k images. It provides binary annotations of 40 facial attributes.
We select 17 attributes from CelebA-HQ and add another four frequently-used attributes ``\textit{sad}'', ``\textit{angry}'', ``\textit{no smiling}'', and ``\textit{no bangs}'', resulting in a total of 21 editable attributes in the \textsc{ChatEdit} dataset. As shown in Table~\ref{tab:attributes}, these editable attributes are categorized into four groups. 
We select 12k images from the CelebA-HQ and utilize the annotation in CelebA-HQ to build a caption for the image. An example is presented in Fig.~\ref{fig:supdataset} in Appendix.

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{main_fig/dataset_analysisv2-zk1.pdf}
\end{center}
\vspace{-10pt}
\caption{\textbf{Analysis of \textsc{ChatEdit} dataset.} 
Left: Distribution of different attributes. Right: Distribution of  occurrence frequency of various attribute combinations.}\label{fig:dataset_analysis}
\vspace*{-2mm}
\end{figure*}

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{main_fig/dialogue_flow.pdf}
\end{center}
\vspace{-5pt}
   \caption{\textbf{Illustration of the dialogue flow for the first 3 turns in the \textsc{ChatEdit} dataset.} User-$n$ and System-$n$ represent the $n$-th user and system turn, respectively.}\label{fig:dialog_flow}
\vspace{-5mm}
\end{figure}


\paragraph{Dialogue Annotation} 
During each turn of the dialogue, the user expresses their editing requests using natural language. It is essential for the system to detect these requests as belief states and map them to appropriate responses. Consequently, we annotate three types of data for each turn in the dialogue: (1) the user utterance, (2) the user belief state (user requests), and (3) the system response.

We here adopt terminology commonly used in task-oriented dialogue research to introduce the annotations. Each group in Table~\ref{tab:attributes} is considered a slot, and each editable attribute represents a possible slot value for its corresponding slot. As a result, the user request in each turn of the dialogue is represented as slot-value pairs. For example, a user request could be ``\textit{expression: smiling, hairstyle: bangs, hair color: black hair.}''

To reduce human efforts in annotating the dialogues, we leverage a two-phase pipeline~\cite{karras2018progressive} consisting of a simulation phase~\cite{li2022controllable} and a paraphrase phase. 
During the simulation phase, we begin by preparing a collection of varied human-written utterances that describe user requests for each editable attribute and system responses for each system action. The supported system actions include \textit{Next}: general queries on what to edit in the next turn, \textit{Request}: request whether to edit an attribute, \textit{Suggest}: suggestions to edit towards a specific attribute value. We then utilize ChatGPT~\cite{ouyang2022training} to generate more diverse utterances by combining task instruction and a few human-written example utterances as the prompt as shown below~\citep{li2021making}.
\begin{tcolorbox}
\scriptsize
Write diverse sentences to express the given image editing requirements.
\\
\textbf{Requirements}: \textcolor{red}{smiling}\\
\textbf{Sentence 1}: Can you edit the image to show him with a smile?\\
\textbf{Sentence 2}: It would be great if you could add a smile to his face.\\
...\\
\textbf{Sentence N}: I'd love to see a version of the image where he has a big smile on his face.\\
\textbf{Sentence N+1}:
\end{tcolorbox}


Next, we construct the dialogue by following a series of steps. Firstly, we determine the user request of the current turn by randomly selecting editable attributes (excluding the original attributes of the raw image) and expressing them as slot-value pairs. Secondly, we select an utterance that effectively expresses this particular request. Then, based on a predefined policy, we determine the appropriate system action and choose a candidate system response. 
Specifically, the predefined policy is a set of rules that govern the dialogue flow to ensure it aligns logically with the user's instructions and system functions. For instance, if a user requests a change in hair color to blond, the system's predefined policy would prevent it from generating a redundant or illogical suggestion like ``Do you want to dye your hair blond?''. 
Finally, the multi-turn dialogue can be constructed by repeating the above steps.


% \begin{figure}
% \begin{center}
% \includegraphics[width=\linewidth]{main_fig/gpt-zk1.pdf}
% \end{center}
% \vspace{-5pt}
%    \caption{\textbf{Illustration of the utterances generation.} }\label{fig:dialog_gpt}
% \vspace{-5mm}
% \end{figure}

Following the simulation of multi-turn dialogues, a manual review is conducted to ensure their quality. Human annotators are tasked with reviewing the dialogues to ensure they are following the pre-defined dialogue flow logic. Additionally, they are asked to refine the expressions to enhance diversity and naturalness if necessary. Fortunately, this process of checking and revising the dialogues is less labor-intensive compared to annotating the dialogues from scratch, resulting in significantly reduced annotation efforts.



\subsection{\textsc{ChatEdit} Dataset Statistics}
The constructed \textsc{ChatEdit} dataset comprises 12k examples, with each example consisting of a facial image equipped with a corresponding caption and an annotated multi-turn dialogue. We divide the \textsc{ChatEdit} dataset into training, validation, and testing sets with 10k, 1k, and 1k examples, respectively. 
To offer deeper insights into the dataset, we present comprehensive statistics in this section.

\paragraph{Analyzing Dialogues}
Our \textsc{ChatEdit} dataset comprises a total of 12k dialogues, consisting of approximately 96k utterances. The statistical details of the dataset are presented in Table~\ref{tab:statics_of_dialogue}. Moreover, we provide a visual representation of the dialogue flow in Fig.~\ref{fig:dialog_flow}. Each block in the figure corresponds to a specific turn labeled as Start, User-$n$, or System-$n$, where $n$ represents the corresponding turn index.
For user turns, there are four different kinds of blocks, each representing an attribute group slot. Additionally, there are three types of system turns, each representing a system action. The width of each block in the visualization indicates its occurrence frequency, while the connectivity between blocks signifies their co-occurrences in the dialog flow. Notably, the dataset exhibits a balanced distribution of different dialog flows, indicating a high level of diversity.


\paragraph{Analyzing Editing Attributes} 
Regarding the annotated user requests, we provide insights into the frequency of each attribute (left) and the occurrence frequency of different attribute combinations (right) in Fig.~\ref{fig:dataset_analysis}. The figure illustrates that the majority of attributes are mentioned over 500 times, while more than 78\% of the attribute combinations occur less than 5 times. These findings highlight the diversity present in our dataset.




\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=\linewidth]{main_fig/main3-caption.pdf}
\end{center}
% \vspace{-10pt}
\caption{\textbf{Illustration of the proposed framework.} The dialogue understanding module is utilized to track user requests and generate system responses. Then, the tracked user requests are fed to the image editing module to guide the image manipulation.}\label{fig:main}
\vspace{-3mm}
\end{figure*}


\subsection{\textsc{ChatEdit} Evaluation}
To evaluate the system's ability to detect user requests, generate the appropriate system response, and edit the image according to user requests, we propose three benchmark tasks.

\paragraph{User Request Tracking}
Similar to the dialogue belief state tracking task in TOD, we introduce the user request tracking task to evaluate the system's ability in detecting the user requests.
The user requests are represented as slot-value pairs, where each slot denotes a category (group) of attributes and the value indicates a specific attribute value.
The input for this task is the dialogue history from the beginning to the current turn.
The performance is evaluated with \textbf{Joint Accuracy}, which is the ratio of dialogue turns whose slots are predicted completely accurately, i.e., all the slot and slot values are predicted correctly.


\paragraph{Response Generation}
Another objective of the dialogue module is to generate fluent, reasonable, and diverse responses to interact with the users. We use BLEU~\cite{papineni2002bleu} to evaluate the fluency of generated responses with respect to reference responses. 
To evaluate the diversity of generated responses, we employ Distinct-1, 2~\cite{li2015diversity}, where Distinct-$n$ represents the ratio of distinct n-grams in responses.

\paragraph{Image Editing}
Editing an image according to the user request is a primary objective of the \textsc{ChatEdit} system. Given user requests that consist of multiple attributes, a robust system should be capable of simultaneously manipulating these attributes while maintaining high quality. We evaluate the image editing performance from two perspectives: (1) \textit{relevance}: whether the requested attributes are accurately edited; and (2) \textit{quality}: whether the edited image is realistic and natural.

We measure the editing relevance of each attribute via the cosine similarity of the attribute and the edited image. To evaluate the relevance of multiple attributes, we report two metrics: 
\textit{Average Relevance (AvgRel)} reflects the average editing relevance on all requested attributes, where the relevance is measured with the cosine similarity between the edited image and the requested attribute:
\begin{equation}
    AvgRel=avg\left \{ Cos_{\mathrm{CLIP}}\left ( I,t \right )  \right \}_{t\in T}.
\end{equation}
$Cos_{\mathrm{CLIP}}$ represents the cosine similarity function. $T$ represents the set of all editing attributes. 

We also report \textit{Minimum Relevance (MinRel)}, which reflects the worst relevance among the requested attribute:
\begin{equation}
MinRel=min \left \{ Cos_{\mathrm{CLIP}}\left ( I,t \right )  \right \}_{t\in T}.
\end{equation}
As for the measure of image quality, we utilize FID~\cite{heusel2017gans} and LPIPS~\cite{zhang2018unreasonable} metrics, which calculate the statistical similarity between the edited images and the originals.



\section{Method}
In this section, we introduce our proposed framework for \textsc{ChatEdit}.
As illustrated in Fig.~\ref{fig:main}, the system consists of a task-oriented dialogue module and an image editing module. 
We adopt a pre-trained language model-based TOD model as the unified dialogue module for both User Request Tracking and response generation. Specifically, it takes the dialogue context prepended with different prompts as input and outputs the tracked user requests and corresponding system response.
As for the image editing module, a text-based image editing model StyleCLIP~\cite{patashnik2021styleclip} is utilized, which receives the user requests tracked by the dialogue model as input and edits the input image accordingly.



\subsection{Dialogue Module}
The primary goal of the dialogue module is to track user requests and generate natural language feedback as responses. Following~\cite{su2022multi}, we formulate the two tasks as text generation and adopt a unified pre-trained language model T5~\cite{raffel2020exploring} for both two tasks.
Specifically, we prepend a task-specific prompt to the dialogue context to serve as the dialogue language model's input, and the model is trained to output the task-specific output. We use ``\textit{translate dialogue to dialogue state}'' and ``\textit{translate dialogue to dialogue response}'' as prompts for the User Request Tracking task and the response generation task, respectively. As the facial attribute values of the initial image can provide valuable context to enhance the generation of reasonable conversations, we incorporate the image caption of the initial raw image into the input for the response generation task. Finally, the dialogue model is trained to generate user requests and system responses in a multitask learning approach.

We use the T5~\cite{raffel2020exploring} model and initialize it with the PPTOD~\cite{su2022multi} checkpoint that has been pre-trained on a diverse set of dialogue corpus and thus equip with primary TOD task completion skills. 
Given task-specific prompt $z_{t}$, dialogue history $x$, and target output $y$. The dialogue model is trained with a maximum likelihood objective and the loss function is defined as:
\begin{equation}
    \mathcal{L}_{\Theta }=-\sum_{i=1}^{\left | y \right |}logP_{\Theta }\left ( y_{i}|y_{< i};z_{t},x \right ),
\end{equation}
where $\Theta$ is the model parameters.

\subsection{Image Editing Module}
In this paper, we employ a text-driven image editing method inspired by StyleCLIP~\cite{patashnik2021styleclip}. This approach combines the generative capabilities of StyleGAN~\cite{karras2019style} with the joint vision-language representation learned by CLIP~\cite{radford2021learning}.
To generate the manipulated image based on user requests, we utilize a CLIP-based loss, which optimizes the latent code of the input image to align with the directions inferred by the descriptive text of the user requests. 

As the tracked user requests are stored in the slot-value pair format, we first construct them into the descriptive text prompt $t$ with templates. Then, we  manipulate the image by optimizing the latent code.
Specifically, the images are projected to the latent codes and manipulated in the $\mathcal{W}+$ space, which is extended from the $\mathcal{W}$ space proposed in StyleGAN~\cite{karras2019style}.
% for better generation. 
For a StyleGAN with 18 layers, $\mathcal{W}+$ space is defined by the cascade of 18 different vectors $\left [ w_{1},...,w_{18} \right ],w_{i}\in \mathcal{W}$.

As shown in Fig.~\ref{fig:main}, the input image is first inverted by a fixed StyleGAN inversion model e4e~\cite{tov2021designing} to obtain the latent code of the input image $w_{s}\in \mathcal{W}+$. Then, the latent code $w\in \mathcal{W}+$ is initialized as $w_{s}$. $w$ is learnable and will be optimized towards the latent code of the edited image. Denote the optimization result of $w$ as $w^*$, $w^*$ can be viewed as the approximation of the latent code of the edited image. Then, taking $w^*$ as input, we can generate the desired edited image via pretrained StyleGAN. In order to optimize $w$, $w$ is first fed to the pretrained StyleGAN to obtain its corresponding image. Thereby, the supervisory information will be delivered via the image.
Specifically, to force $w$ to be consistent with the user's descriptive text, the CLIP loss~\cite{radford2021learning} is utilized:
\begin{equation}
\mathcal{L}_{\mathrm{CLIP}}= D_{\mathrm{CLIP}}\left ( G\left ( w \right ), t \right ),
\end{equation}
where $G$ is a pretrained StyleGAN generator that maps latent code into an image. $D_{\mathrm{CLIP}}$ measures the cosine distance between the CLIP embedding of the image and the text. Then, $L_{2}$ loss is utilized for preserving the similarity between the input image and the edited image in the $\mathcal{W}+$ space:
\begin{equation}
L_{2}= \left\|w-w_{s}\right\|_2.
\end{equation}
Moreover, $\mathcal{L}_{\mathrm{ID}}$ is used to preserve the identity:
\begin{equation}
    \mathcal{L}_{\mathrm{ID}}=1-\left\langle R\left(G\left(w_s\right)\right), R(G(w))\right\rangle,
\end{equation}
where $R$ is a pretrained face recognition network~\cite{deng2019arcface}, $\left\langle \cdot ,\cdot  \right\rangle$ calculates the cosine similarity.
The overall optimization objective is:
\begin{equation}
w^*=\underset{w \in \mathcal{W}+}{\arg \min } \mathcal{L}_{\mathrm{CLIP}}+\lambda_{\mathrm{L} 2}L_{2}+\lambda_{\mathrm{ID}}\mathcal{L}_{\mathrm{ID}}.
\end{equation}




\section{Experiment}



\begin{table}
% \small
    
    \caption{\textbf{Quantitative results of the dialogue module on user request tracking and response generation. }}
    \vspace{-5pt}
    \centering  % 表居中
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}}&\textbf{User Request Tracking}&\multicolumn{3}{c}{\textbf{Response Generation}}\\
        \cmidrule(lr){2-2}
        \cmidrule(lr){3-5}
        &Joint Acc$\uparrow$&BLEU$\uparrow$&Distinct-1$\uparrow$&Distinct-2$\uparrow$\\
        \hline
        T5$_{\text{small}}$ &88.502$\pm$0.212	&14.806$\pm$0.203	&0.393$\pm$0.025	&0.867$\pm$0.008\\
        % \hline
        T5$_{\text{base}}$ &\textbf{89.065}$\pm$0.194	&14.917$\pm$0.499	&0.408$\pm$0.005	&0.872$\pm$0.002\\
        % \hline
        T5$_{\text{large}}$ &88.953$\pm$0.159	&\textbf{16.064}$\pm$0.594	&\textbf{0.413}$\pm$0.010	&\textbf{0.886}$\pm$0.004\\
        \bottomrule
    \end{tabular}}

    \label{tab:dialog_result}
\vspace{-3mm}
\end{table}

\subsection{Quantitative Evaluation}
\paragraph{Dialogue Module}
We initialize our dialogue model with different sizes of pretrained PPTOD checkpoints PPTOD$_{\text{small}}$, PPTOD$_{\text{base}}$ and PPTOD$_{\text{large}}$, respectively.
We further fine-tune the model leveraging the Adam optimizer with a learning rate of 5e-5 and a batch size of 64. We utilize a multi-task training strategy in which the User Request Tracking task and the Response Generation task are trained simultaneously.

As shown in Table~\ref{tab:dialog_result}, we evaluate the dialogue module on the user request tracking and response generation task. All three models achieve a joint accuracy of over 88\%, indicating their effectiveness. 
% The model with the largest size underperforms the smaller ones, which is also observed in PPTOD~\cite{su2022multi} and might be caused by overfitting. 
\textbf{We also compare our model with ChatGPT}. Specifically, we used the prompt proposed by~\cite{heck2023chatgpt} to leverage ChatGPT for User Request Tacking, which is up-to-date and has achieved impressive results on the MultiWOZ dataset. Our method achieves a Joint Accuracy of 88.86\%, outperforming ChatGPT's 76.26\%

As for the performance regarding the response generation, an improvement can be seen in both the BLEU score and the Distinct-1,2 score are observed as the model size increases, demonstrating that the larger model can generate responses with better quality and diversity. 




\begin{table}[tb]
    % \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
         \caption{\textbf{Quantitative results of the image editing performance.} Input represents the input of the image editing module. USR, Dial, and USR-T stand for oracle user requests, oracle dialogue, and the user requests tracked by the dialogue module respectively. All experiments utilize StyleCLIP to manipulate images. }
        \vspace{-5pt}
        \resizebox{1.0\linewidth}{!}{%
	\begin{tabular}{cccccc}
        \toprule
		\multirow{2}{*}{\textbf{Editing Mode}}&\multirow{2}{*}        {\textbf{Input}}&\multicolumn{4}{c}{\textbf{Image Editing}}\\
		\cmidrule(lr){3-6}
        && FID $\downarrow$ & LPIPS $\downarrow$ & MinRel $\uparrow$ & AvgRel $\uparrow$ \\
        \midrule

        Single-turn &USR &41.451$\pm$0.176 &0.482$\pm$0.002 &0.730$\pm$0.010 &0.752$\pm$0.010\\
        Multi-turn &USR &\textbf{40.115$\pm$0.165} &\textbf{0.449$\pm$0.001} &\textbf{0.754$\pm$0.009} &\textbf{0.773$\pm$0.009}\\
        Multi-turn &Dial  &42.813$\pm$0.161 &0.477$\pm$0.001 &0.741$\pm$0.007 &0.761$\pm$0.006\\ 
        Multi-turn (Ours) &USR-T &\underline{40.536$\pm$0.105} &\underline{0.449$\pm$0.001} &\underline{0.753$\pm$0.010} &\underline{0.773$\pm$0.010}\\
        \bottomrule
	\end{tabular}}
	\label{tab:image_editing}
\vspace{-3mm}
\end{table}


\begin{figure*}

    \begin{center}
        \includegraphics[width=\linewidth]{main_fig/comparisonv5.pdf}
    \end{center}
    \vspace{-5pt}
    \caption{\textbf{Visualization of the comparison between our method with Talk-to-Edit.} Left shows the attribute forgetting problem of Talk-to-Edit, where the \textit{smiling} attribute is gradually diluted. Right shows the error accumulation problem. Besides, our method can generate instructive responses by giving suggestions (highlighted in yellow).}\label{fig:comparison}
    \vspace{-3mm}
\end{figure*}



\paragraph{Image Editing Module} 
The weight of $\lambda_{\mathrm{L} 2}$ and $\lambda_{\mathrm{ID}}$ are 0.008 and 0.005, respectively. The editing step is 300, and the Adam optimizer is used with a learning rate of 0.1. 
For fairness, all experiments utilize StyleCLIP to manipulate images.

To evaluate the effectiveness of our proposed multiple-turn editing approach, we first compare it with single-turn image editing. To eliminate the impact of misidentification of user edit requirements, the oracle user requests are utilized as the input of the image editing module. As shown in the first two lines in Table~\ref{tab:image_editing},
significant improvements can be observed in all four metrics, indicating that our method successfully mitigates the error accumulation problem and achieves higher image quality.
Moreover, we find that the MinRel score of the single-turn method is considerably lower than that of the multi-turn method. This is expected since the single-turn editing approach tends to forget some edited attributes from earlier turns, resulting in suboptimal performance in the poorest attribute.

The last line in Table~\ref{tab:image_editing} presents the results of image editing using our proposed pipeline, where the image editing module edits the image based on the user requests tracked by our dialogue module. Notably, the performance achieved in this setting is comparable to the results when oracle user requests are taken as input. This finding highlights the effectiveness of our pipeline, particularly in real-world applications where obtaining perfect oracle user requests may not be feasible. 


\begin{table}
    % \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
        \caption{\textbf{Quantitative comparison of the image editing performance between ours and Talk-to-Edit.}}
	% \scalebox{0.85}{
        \resizebox{\linewidth}{!}{%
        \setlength{\tabcolsep}{0.4cm}{
	\begin{tabular}{ccccc}
        \toprule
        \textbf{Model} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{MinRel $\uparrow$} & \textbf{AvgRel $\uparrow$} \\
        \midrule
        Talk-to-Edit	&132.555	&0.644	&0.731	&0.739 \\
        Ours	&98.760	&0.439	&0.770	&0.776\\
    
        \bottomrule
	\end{tabular}
        }}
	\label{tab:sup_qua_comparison}
    \vspace{-3mm}
\end{table}

\paragraph{Comparison with Talk-to-Edit}
We conduct a quantitative comparison between our method and the representative single-turn approach Talk-to-Edit on 100 randomly selected samples. As shown in Table~\ref{tab:sup_qua_comparison}, our framework achieves better image quality with lower FID and LPIPS scores, highlighting reduced error accumulation. Moreover, our method excels in MinRel and AvgRel metrics, underscoring its better alignment with user requests and effective mitigation of attribute forgetting.



\paragraph{Tracked user request vs Dialogue}
To better understand the essentials of our introduced dialogue module, we experimented with using the raw dialogue context instead of the extracted user requests as input for the image editing module. Table~\ref{tab:image_editing} reveals that while the MinRel and AvgRel scores from this ablation study remain competitive with our approach, the FID and LPIPS scores drop notably. This discrepancy can be attributed to the presence of noisy information in the dialogue context. These results underscore the importance of accurately extracting user requests from dialogues through our proposed dialogue module. Further visual illustrations can be found in Fig.~\ref{fig:sup_ablation} in the Appendix.



\subsection{Qualitative Evaluation}
In Fig.~\ref{fig:step_vs_combine}, we compare our multi-turn approach with the single-turn approach, illustrating the attribute forgetting and error accumulation problems of the single-turn approach. Additional visualization results are shown in Fig.~\ref{fig:sup_sig_multi} in the Appendix.
We also present examples of manipulation results comparing our method with Talk-to-Edit in Fig.~\ref{fig:comparison}, showcasing the high image editing quality and response generation diversity achieved by our method. More visualization results can be found in Fig.~\ref{fig:sup_t2e_forgetting}, ~\ref{fig:sup_t2e_error}, and~\ref{fig:sup_t2e_fail_dialog} in the Appendix.

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=\linewidth]{main_fig/user_study.pdf}
    \end{center}
    \vspace{-5pt}
    \caption{\textbf{Human evaluation of ours v.s. Talk-to-Edit}
    on one axis on image editing: \textit{Manipulation} and three axes on response generation: \textit{Humanness}, \textit{Interestingness}, and \textit{Engagingness}. 
    }\label{fig:comparison2}
    \vspace{-3mm}
\end{figure}

\subsection{Human Evaluation}
To more comprehensively assess our method's response generation and image editing capabilities compared with Talk-to-Edit~\cite{jiang2021talk},  we conduct human evaluations.We choose 20 random images and initiate multi-turn interactive facial image editing dialogues using both our method and Talk-to-Edit. Five English-fluent graduate students participate in this assessment, conducting pairwise comparisons between the two methods. The evaluations focus on one aspect of the manipulated image: \textit{manipulation}, and three aspects of the generated response: \textit{humanness, interestingness,} and \textit{engagingness}. The final answer for each question is determined by majority voting.

Fig.~\ref{fig:comparison2} visualizes the evaluation results, underscoring the superiority of our method. Specifically, for image editing, our method effectively generates images of high quality, taking advantage of the extracted concise user requests to guide the image editing process. For response generation, our method generates more human-like, interesting and engaging responses, improving the interactivity of the system. By contrast, Talk-to-Edit relies on a rule-based approach to generate template responses and employs a cascaded single-turn image editing approach, limiting its performance.

% \begin{table}
% \small
% \caption{\textbf{Illustration of out-of-distribution attributes.} }
% \vspace{-5pt}
% \centering
% \renewcommand\arraystretch{1.2}
% \resizebox{\linewidth}{!}{%
% \begin{tabular} {@{}lp{6cm}@{}}
% \toprule
% \textbf{Slot}  &\textbf{Attribute}\\
% \cline{1-2}
% Expression &\textit{disgust, surprise, fear}\\
% \cline{1-2}
% Hair color &\textit{pink hair, purple hair, red hair}\\
% \cline{1-2}
% Makeup &\textit{big eyes}\\
% \bottomrule
% \end{tabular}
% }
% \label{tab:sup_attributes}
% \end{table}

% \subsection{Out-of-distribution Generalization}
% A distinct advantage of our proposed framework is its adaptability in utilizing various training strategies for the T5-based dialogue module. Drawing inspiration from~\cite{lin2021zero}, we cast the slot value tracking as a question-answering task, which promotes enhanced generalization on unseen data. For example, the model is prompted with queries like ``what is the hair color?''

% For evaluation, we use an out-of-distribution (OOD) test set comprising 1,000 samples based on previously unseen values, as detailed in Table~\ref{tab:sup_attributes}. Experimental results suggest our model's robustness against OOD scenarios during testing. The dialogue module achieves an accuracy of 58.90\% on the OOD test set. As for image editing, the results on the OOD test set align closely with those of the in-domain test set.

% \begin{table}
%     % \small
% 	\centering  % 表居中
% 	\renewcommand{\arraystretch}{1.2}
% 	\setlength{\tabcolsep}{6pt}
%         \caption{\textbf{Image editing performance on in-domain and out-of-domain test sets.}}
%         \vspace{-5pt}
%         \resizebox{\linewidth}{!}{%
%         \setlength{\tabcolsep}{0.4cm}{
% 	\begin{tabular}{cccccc}
%         \toprule
%         \textbf{Test set} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{MinRel $\uparrow$} & \textbf{AvgRel $\uparrow$} \\
%         \midrule
%         In-domain	&40.115	&0.449	&0.754	&0.773\\
%         Out-of-domain	&40.264	&0.458	&0.739	&0.759\\
%         \bottomrule
% 	\end{tabular}
%         }}
% 	\label{tab:sup_ood}
% \vspace{-4mm}
% \end{table}

\section{Conclusion}
This paper introduces the \textsc{ChatEdit} benchmark dataset, which we believe could facilitate the research on multi-turn interactive facial image editing. The dataset poses significant challenges as it requires systems to accurately track user requests from dialogues, perform image editing based on these requests, and generate appropriate responses. We propose a baseline framework that seamlessly combines a task-oriented dialogue module and an image editing module. The introduction of the task-oriented dialogue not only enables interaction with users but also extracts concise user requests from the dialogue context to direct the image editing, avoiding the attribute forgetting and error accumulation issues in previous single-turn methods. 
The empirical results highlight the efficiency of our approach and the potential for further advancements in this exciting research area.

\section*{Limitations}
Our work is the first benchmark dataset to explore multi-turn interactive facial image editing via dialogue and establishes baseline performance for a variety of scenarios. However, there is room for improvement in the following aspects: 1) In the dataset construction, we consider 21 attributes as editable attributes. However, there are out-of-domain attributes users might want to manipulate. In this case, the dialogue understanding may neglect the user requests or generate extra hallucinations. 2) The proposed baseline model has two stages, which leverages the powerful capabilities of existing models via lightweight fine-tuning. However, both the dialogue understanding module and the image editing module limit the quality of the manipulated image. This issue might be alleviated by training the whole model end-to-end, which will be included in our future research. In addition, other issues, such as how to construct a more generalized and robust facial image editing model, also require further exploration.

\section*{Ethics Statement}
It is important to clarify that the facial images used in the \textsc{ChatEdit} dataset are selected from CelebA-HQ~\cite{karras2018progressive}, which is a dataset derived from CelebA~\cite{liu2015faceattributes}. CelebA consists of images collected from the internet and is publicly available for research purposes only. The images in CelebA-HQ have undergone additional post-processing.
The dialogues in the \textsc{ChatEdit} dataset do not contain sensitive or private information. The dataset has been carefully curated to ensure the privacy and confidentiality of individuals.
Furthermore, participants involved in the manual paraphrase and human evaluation processes were compensated with reasonable wages.

\section*{Acknowledgement}
This work is supported by Major Technology Innovation Program of Hangzhou, China (Grant 2022AIZD0154), National Natural Science Foundation of China (Grant No. 62306041, No. U21B2045), Beijing Nova Program (Grant No. Z211100002121106, 20230484488), and National Key \text{R\&D} Program of China (Grant No. 2022YFF1202400).

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}


\newpage
\appendix
{
    % \centering
    \Large
    \noindent\textbf{Appendix}\\
    \vspace{-10pt}
}

%%%%%%%%% BODY TEXT
\section{\textsc{ChatEdit} Benchmark Dataset}\label{sec:sec1}
Each sample in the \textsc{ChatEdit} dataset contains an input image to be edited with the corresponding caption, the associated dialogue, and the user requests. Notably, similar to most image editing datasets, the dataset does not include ground truth manipulated images for each turn.
Fig.~\ref{fig:supdataset} presents an example in \textsc{ChatEdit} that contains a four-turn interaction. Fig.~\ref{fig:web} presents the web page for human paraphrasing, where the annotators are required to check and revise the dialogue to make the conversation fluent, natural, and consistent with the user requests.


\begin{figure*}

\begin{center}
\includegraphics[width=0.7\linewidth]{sup_fig/sup_dataset-caption.pdf}
\vspace{-10pt}
\end{center}
   \caption{\textbf{An example sample from the \textsc{ChatEdit} dataset.}}\label{fig:supdataset} 
\vspace{-5pt}
\end{figure*}


\begin{figure*}

\begin{center}
\includegraphics[width=0.7\linewidth]{sup_fig/web.pdf}
\vspace{-10pt}
\end{center}
   \caption{\textbf{Interface for the manual paraphrase.}}\label{fig:web} 
\vspace{-10pt}
\end{figure*}

\section{Network Architecture}\label{sec:sec2}
\subsection{Architecture of the Dialogue Module}
The model used in the dialogue module is based on the pre-trained language model T5~\cite{raffel2020exploring}, which is a Transformer~\cite{vaswani2017attention} encoder-decoder framework. Each Transformer layer comprises an attention mechanism and a feed-forward network. Specifically, the attention mechanism is self-attention in the encoder layer and encoder-decoder attention in the decoder layer. The feed-forward network consists of a dense layer with an output dimensionality of $d_\text{ff}$ followed by a ReLU nonlinearity and another dense layer.

For the base model, both the encoder and decoder consist of 12 layers, where the “key” and “value” matrices of all attention
mechanisms have an inner dimensionality of $d_\text{kv}=64$ and all attention mechanisms have 12 heads. The output dimensionality of the first feed-forward network in each block is $d_\text{ff}=3,072$ and the dimensionality of all other sub-layers and embeddings is $d_\text{model}=768$. 
The small model scales the base model by using 6 layers for the encoder and decoder. For each layer, it utilizes 8-headed attention, $d_\text{ff}=2,048$, and $d_\text{model}=512$.
The large model has 24 layers for the encoder and decoder. It scales the base model up by using 16-headed attention, $d_\text{ff}=4,096$, and $d_\text{model}=1,024$. 
Table~\ref{tab:supmodel} summarizes the statistics of three models with different sizes.

\subsection{Architecture of the Image Editing Module} 
To supplement the description of the image editing module, the architecture of StyleGAN2~\cite{karras2020analyzing} generator
is described in detail in this section. Specifically, it generates images gradually from low resolution to high resolution.
Every major layer (every resolution) of the StyleGAN2 generator consists of two types of convolutional blocks: feature space convolutions (Conv), which are leveraged for feature map synthesis, and toRGB convolutions (ToRGB), which utilize convolutions to convert the feature map into an RGB image. 
Each of these convolution blocks is modulated by a vector
of style parameters $w$.
% which is obtained from the $\mathcal{W}$ space. 
In our experiment, we utilized the $\mathcal{W}+$ space where each $\mathcal{W}+$ layer has its own style parameters $w_{i} \in \mathcal{W}$.
The details of StyleGAN2~\cite{karras2020analyzing} generator are listed in Table~\ref{tab:supstylegan}.

\begin{table}[tb]
    \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
        \caption{\textbf{Variants of Dialogue Module.}}
	% \scalebox{0.85}{
        \resizebox{0.9\linewidth}{!}{%
        \setlength{\tabcolsep}{0.4cm}{
	\begin{tabular}{cccccc}
        \toprule
        \textbf{Model} &\textbf{\#Layers} &\textbf{\#Heads} &\textbf{$d_\text{kv}$} &\textbf{$d_\text{ff}$} &\textbf{$d_\text{model}$} \\
        \midrule
        Small &6 &8 &64 &2048 &512\\
        Base &12 &12 &64 &3072 &768\\
        Large &24 &16 &64 &4096 &1024\\
        \bottomrule
	\end{tabular}
        }}
	\label{tab:supmodel}
\vspace{-10pt}
\end{table}

\begin{table}[t]
\small
\caption{The breakdown of the StyleGAN2~\cite{karras2020analyzing}. 
} \label{tab:supstylegan} 
\vspace{-10pt}
\begin{center}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{cclc}
\toprule
\textbf{\makecell[c]{$\mathcal{W+}$\\ layer index} } & \textbf{Resolution} & \textbf{Layer name} & \textbf{\# Channels} \\
\midrule

0 & 4$\times$4 & Conv  & 512 \\
1  & 4$\times$4 & ToRGB  & 512 \\

% \rowcolor[HTML]{D9D9D9} 
2  & 8$\times$8 & Conv0\_up  & 512 \\
% \rowcolor[HTML]{D9D9D9} 
3  & 8$\times$8 & Conv1  & 512 \\
% \rowcolor[HTML]{D9D9D9} 
3  & 8$\times$8 & ToRGB  & 512 \\


4  & 16$\times$16 & Conv0\_up  & 512 \\ 
5  & 16$\times$16 & Conv1 & 512 \\
5  & 16$\times$16 & ToRGB & 512 \\

% \rowcolor[HTML]{D9D9D9} 
6  & 32$\times$32 & Conv0\_up & 512 \\
% \rowcolor[HTML]{D9D9D9} 
7  & 32$\times$32 & Conv1  & 512 \\
% \rowcolor[HTML]{D9D9D9}
7  & 32$\times$32 & ToRGB  & 512 \\


8  & 64$\times$64 & Conv0\_up & 512 \\
9  & 64$\times$64 & Conv1 & 512 \\
9  & 64$\times$64 & ToRGB & 512 \\

% \rowcolor[HTML]{D9D9D9} 
10  & 128$\times$128 & Conv0\_up & 512 \\
% \rowcolor[HTML]{D9D9D9} 
11  & 128$\times$128 & Conv1 & 256 \\
% \rowcolor[HTML]{D9D9D9} 
11  & 128$\times$128 & ToRGB & 256 \\


12  & 256$\times$256 & Conv0\_up & 256 \\
13  & 256$\times$256 & Conv1 & 128 \\
13  & 256$\times$256 & ToRGB & 128 \\

% \rowcolor[HTML]{D9D9D9} 
14  & 512$\times$512 & Conv0\_up & 128 \\
% \rowcolor[HTML]{D9D9D9} 
15  & 512$\times$512 & Conv1 & 64 \\
% \rowcolor[HTML]{D9D9D9} 
15  & 512$\times$512 & ToRGB & 64 \\


16  & 1024$\times$1024 & Conv0\_up & 64 \\
17  & 1024$\times$1024 & Conv1 & 32 \\
17  & 1024$\times$1024 & ToRGB & 32\\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-10pt}
\end{table}

\section{Details of the Human Evaluation}\label{sec:sec3}
\paragraph{Questions of the Human Evaluation.}
In the human evaluations, we compare our framework with Talk-to-Edit over one aspect of the manipulated image: manipulation,
and three aspects on the generated response of each turn
in the dialogues: humanness, interestingness, and engagingness.
The instructions for these four aspects provided to participants are shown as follows:
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}

    \item Manipulation: \textit{Which one manipulates the image better with facial identity unchanged?}
    \item Humanness: \textit{Which one sounds more natural and personable?}
    \item Interestingness: \textit{Which one arouses your curiosity or tells you something new or useful?}
    \item Engagingness: \textit{Which one is more likely to capture your attention and make you want to further interact with it?}
\end{itemize}
We conduct the blind evaluation where participants will not be informed about the source of the manipulated images and generated responses (our framework or Talk-to-edit) to ensure fairness.


\paragraph{Inter-rater agreement.} To evaluate the agreement among the answers of all participants, we calculate the inter-rater agreement score. The average inter-rater agreement score is 0.26 in terms of Fleiss’ kappa~\cite{fleiss1973equivalence}, which demonstrates a fair agreement.

\section{More Qualitative Results}\label{sec:sec4}
\paragraph{Comparison of Single-turn Editing and Our Proposed Multi-turn Editing.}
In Fig.~\ref{fig:sup_sig_multi}, ~\ref{fig:sup_sig_multi5-1}, ~\ref{fig:sup_sig_multi5-2}, we present more visualization results to illustrate the attribute forgetting problem and error accumulation problem of previously single-turn methods, which can be avoided by our multi-turn approach. Specifically, we utilize StyleCLIP as the image editing method for both the sing-turn method and our multi-turn method for fair comparison. Notably, as shown in In Fig.~\ref{fig:sup_sig_multi5-1}, ~\ref{fig:sup_sig_multi5-2}, our method still performs better in the setting that the single-turn method takes oracle user requests as input while ours take the dialogue as input and uses the dialogue module to obtain the tracked user requests.


\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{sup_fig/sup_step_vs_combine.pdf} 
% \vspace{-9mm}
\end{center}
   \caption{\textbf{Comparison of single-turn editing and our proposed multi-turn editing.} Both methods take oracle user requests as input to make a fair comparison. (a) shows the attribute forgetting problem of the single-turn editing method. (b) illustrates the error accumulation problem. By contrast, our proposed multi-turn editing approach avoids these issues.}
   \label{fig:sup_sig_multi} 
   \vspace{-2mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[width=0.9\linewidth]{sup_fig/sup_step_vs_combine-multi5.pdf} 
% \vspace{-9mm}
\end{center}
   \caption{\textbf{Comparison of single-turn editing and our proposed framework (a).} Our method takes dialogue history as input and uses the dialogue module to track user requests while the oracle user requests are used for single-turn methods.}
   \label{fig:sup_sig_multi5-1} 
   \vspace{-2mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[width=0.9\linewidth]{sup_fig/sup_step_vs_combine-multi5_3.pdf} 
% \vspace{-9mm}
\end{center}
   \caption{\textbf{Comparison of single-turn editing and our proposed framework (b).} Our method takes dialogue history as input and uses the dialogue module to track user requests while the oracle user requests are used for single-turn methods.}
   \label{fig:sup_sig_multi5-2} 
   \vspace{-2mm}
\end{figure*}


\paragraph{Visualization Results of Ablation study.}
In the ablation study of the main paper, we demonstrate the effectiveness of the introduced dialogue module that extracts user requests to guide the image editing module quantitatively. We present qualitative results in this section. We experiment with the multi-turn method that isn't equipped with a dialogue module to extract the user requests and thus directly takes dialogue as input. As shown in Fig~\ref{fig:sup_ablation}, it fails to understand the user request accurately and ignores some of the user requests, suggesting the significance of our introduced dialogue module in the multi-turn interactive editing.

\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{sup_fig/sup_ablation_new1.pdf} 
% \vspace{-9mm}
\end{center}
\caption{\textbf{Visualization results of ablation study.} The cases show the influence of the introduced dialogue module that extracts user requests from the dialogue on the manipulated images. The multi-turn approach doesn't equip with the dialogue module and thus takes the whole dialogue as input of the image editing module.}
\label{fig:sup_ablation} 
\vspace{-5mm}
\end{figure*}



\paragraph{Comparison Between Our Method with Talk-to-Edit.}
More qualitative comparisons between our method with Talk-to-Edit~\cite{jiang2021talk} are presented in Fig.~\ref{fig:sup_t2e_forgetting}, ~\ref{fig:sup_t2e_error}, ~\ref{fig:sup_t2e_fail_dialog}.
Fig.~\ref{fig:sup_t2e_forgetting} shows that there exists the attribute forgetting problem in Talk-to-Edit.
Fig.~\ref{fig:sup_t2e_error} illustrates that Talk-to-Edit has an error accumulation problem.
Besides, as shown in these results, our method can generate better responses with proper suggestions, which improves interactivity.
Moreover, error judgment in the rule-based method will lead to the unexpected break-off of the interaction in Talk-to-Edit, which is represented in Fig~\ref{fig:sup_t2e_fail_dialog}.

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{sup_fig/sup_comparison1_1.pdf}
% \vspace{-8mm}
\end{center}
   \caption{\textbf{Comparison of our method and Talk-to-Edit.} These results demonstrate the attribute forgetting problem in Talk-to-Edit, where the goatee attribute is lost in the above cases.}\label{fig:sup_t2e_forgetting} 
\vspace{1mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{sup_fig/sup_comparison1_2.pdf}
% \vspace{-8mm}
\end{center}
   \caption{\textbf{Comparison of our method and Talk-to-Edit.} These results demonstrate the error accumulation problem in Talk-to-Edit, where artifacts occur and propagate to the final edited image.
   }\label{fig:sup_t2e_error} 
\vspace{1mm}
\end{figure*}


\begin{figure*}
\begin{center}

\includegraphics[width=\linewidth]{sup_fig/sup_comparison1_3.pdf}
% \vspace{-8mm}
\end{center}
   \caption{\textbf{Comparison of our method and Talk-to-Edit.} The left present case where the interaction is broken off unexpectedly in Talk-to-Edit due to its rule-based method. \textit{END} represents that the system terminates the interaction. The right present case where Talk-to-Edit fails to edit.
   }\label{fig:sup_t2e_fail_dialog} 
\end{figure*}


\section{More Quantitative Results}
% \subsection{Quantitative Comparison Between Our Dialogue Module with ChatGPT.}
% We conduct a quantitative comparison between our T5-based dialogue model and ChatGPT on 100 randomly selected dialogues.  Specifically, we used the prompt settings proposed by Heck et al. ~\cite{heck2023chatgpt} to leverage ChatGPT for User Request Tacking, which is up-to-date and has achieved impressive results on the MultiWOZ dataset.
% Our method achieves a Joint Accuracy of 88.86\%, outperforming ChatGPT's 76.26\%, which shows the necessity and superiority of our proposed dialogue module.

\begin{table}
\small
\caption{\textbf{Illustration of out-of-distribution attributes.} }
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.2}
\resizebox{\linewidth}{!}{%
\begin{tabular} {@{}lp{6cm}@{}}
\toprule
\textbf{Slot}  &\textbf{Attribute}\\
\cline{1-2}
Expression &\textit{disgust, surprise, fear}\\
\cline{1-2}
Hair color &\textit{pink hair, purple hair, red hair}\\
\cline{1-2}
Makeup &\textit{big eyes}\\
\bottomrule
\end{tabular}
}
\label{tab:sup_attributes}
\end{table}

\subsection{Out-of-distribution Generalization}
A distinct advantage of our proposed framework is its adaptability in utilizing various training strategies for the T5-based dialogue module. Drawing inspiration from~\cite{lin2021zero}, we cast the slot value tracking as a question-answering task, which promotes enhanced generalization on unseen data. For example, the model is prompted with queries like ``what is the hair color?''

For evaluation, we use an out-of-distribution (OOD) test set comprising 1,000 samples based on previously unseen values, as detailed in Table~\ref{tab:sup_attributes}. Experimental results suggest our model's robustness against OOD scenarios during testing. The dialogue module achieves an accuracy of 58.90\% on the OOD test set. As for image editing, the results on the OOD test set align closely with those of the in-domain test set.

\begin{table}
    % \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
        \caption{\textbf{Image editing performance on in-domain and out-of-domain test sets.}}
        \vspace{-5pt}
        \resizebox{\linewidth}{!}{%
        \setlength{\tabcolsep}{0.4cm}{
	\begin{tabular}{cccccc}
        \toprule
        \textbf{Test set} & \textbf{FID $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{MinRel $\uparrow$} & \textbf{AvgRel $\uparrow$} \\
        \midrule
        In-domain	&40.115	&0.449	&0.754	&0.773\\
        Out-of-domain	&40.264	&0.458	&0.739	&0.759\\
        \bottomrule
	\end{tabular}
        }}
	\label{tab:sup_ood}
\vspace{-4mm}
\end{table}

\subsection{Ablation Study of Image Editing Module.}
We performed ablation studies for both identity loss ($L_{id}$) and $L_2$ loss within the image editing module. The results are presented in Table~\ref{tab:sup_ablation}. It is observed that both loss types influence the image-editing performance, with $L_2$ loss having a more significant impact. This is likely because $L_2$ loss constrains the degree of feature vector transformations in hidden space, and significant changes in StyleGAN's hidden space can introduce severe artifacts.

\begin{table}[tb]
    \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
         \caption{\textbf{Ablation study of image editing module.} 
         }
         % \vspace{-5pt}
        \resizebox{1.0\linewidth}{!}{%
	\begin{tabular}{cccccc}
        \toprule
		\multirow{2}{*}{\textbf{Method}}&\multirow{2}{*}{\textbf{Input}}&\multicolumn{4}{c}{\textbf{Image Editing}}\\
		\cmidrule(lr){3-6}
        && FID $\downarrow$ & LPIPS $\downarrow$ & MinRel $\uparrow$ & AvgRel $\uparrow$ \\
        \midrule

        Multi-turn &USR &\textbf{40.115} &\textbf{0.449} &\textbf{0.754} &\textbf{0.773}\\
        w/o $L_{id}$ &USR  &40.475	&0.452	&0.742	&0.763\\ 
        w/o $L_2$ &USR &74.639	&0.794	&0.715	&0.752\\
        \bottomrule
	\end{tabular}}
	\label{tab:sup_ablation}
\end{table}



\end{document}
