\clearpage

\appendix
{
    \centering
    \Large
    \textbf{Supplementary Material}\\
    \vspace{1.0em}
}

%%%%%%%%% BODY TEXT
In this supplementary file, we present a sample of I$^\mathbf{2}$Edit Benchmark Dataset in Section~\ref{sec:sec1}. Then, we introduce the network architectures of our proposed framework in Section~\ref{sec:sec2}. Furthermore, details of the human evaluation are shown in Section~\ref{sec:sec3}. Finally, we provide more qualitative results in Section~\ref{sec:sec4} from diverse perspectives.

%%%%%%%%% BODY TEXT
\section{I$^\mathbf{2}$Edit Benchmark Dataset}\label{sec:sec1}
Each sample in the \textbf{I$^\mathbf{2}$Edit} dataset contains an input image to be edited, the associated user edit requirements, and the corresponding dialogue. Notably, the dataset does not include ground truth manipulated images for each turn. The manipulated images are evaluated in relation to the input image, as discussed in Section 3.3 in the main paper. Fig.~\ref{fig:supdataset} presents an example that contains a four-turn interaction. 

\begin{figure*}

\begin{center}
\includegraphics[width=1.0\linewidth]{sup_fig/sup_dataset.pdf}
% \vspace{-10pt}
\end{center}
   \caption{\textbf{An example sample from the I$^\mathbf{2}$Edit dataset.}}\label{fig:supdataset} 
\end{figure*}

\section{Network Architecture}\label{sec:sec2}
\subsection{Architecture of the Dialogue Module}
The model used in the dialogue module is based on the pre-trained language model T5~\cite{raffel2020exploring}, which is a Transformer~\cite{vaswani2017attention} encoder-decoder framework. Each Transformer layer comprises an attention mechanism and a feed-forward network. Specifically, the attention mechanism is self-attention in the encoder layer and encoder-decoder attention in the decoder layer. The feed-forward network consists of a dense layer with an output dimensionality of $d_\text{ff}$ followed by a ReLU nonlinearity and another dense layer.

For the base model, both the encoder and decoder consist of 12 layers, where the “key” and “value” matrices of all attention
mechanisms have an inner dimensionality of $d_\text{kv}=64$ and all attention mechanisms have 12 heads. The output dimensionality of the first feed-forward network in each block is $d_\text{ff}=3,072$ and the dimensionality of all other sub-layers and embeddings is $d_\text{model}=768$. 
The small model scales the base model by using 6 layers for the encoder and decoder. For each layer, it utilizes 8-headed attention, $d_\text{ff}=2048$, and $d_\text{model}=512$.
The large model has 24 layers for the encoder and decoder. It scales the base model up by using 16-headed attention, $d_\text{ff}=4,096$, and $d_\text{model}=1,024$. 
Table~\ref{tab:supmodel} summarizes the statistics of three models with different sizes.

\subsection{Architecture of the Image Editing Module} 
To supplement the description of the image editing module, the architecture of StyleGAN2~\cite{karras2020analyzing} generator
is described in detail in this section. Specifically, it generates images gradually from low resolution to high resolution.
Every major layer (every resolution) of the StyleGAN2 generator consists of two types of convolutional blocks: feature space convolutions (Conv), which are leveraged for feature map synthesis, and toRGB convolutions (ToRGB), which utilize convolutions to convert the feature map into an RGB image. 
Each of these convolution blocks is modulated by a vector
of style parameters $w$.
% which is obtained from the $\mathcal{W}$ space. 
In our experiment, we utilized the $\mathcal{W}+$ space where each $\mathcal{W}+$ layer has its own style parameters $w_{i} \in \mathcal{W}$.
The details of StyleGAN2~\cite{karras2020analyzing} generator are listed in Table~\ref{tab:supstylegan}.

\begin{table}[tb]
    \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
        \caption{\textbf{Variants of Dialogue Module.}}
	% \scalebox{0.85}{
        \resizebox{\linewidth}{!}{%
        \setlength{\tabcolsep}{0.4cm}{
	\begin{tabular}{cccccc}
        \toprule
        \textbf{Model} &\textbf{\#Layers} &\textbf{\#Heads} &\textbf{$d_\text{kv}$} &\textbf{$d_\text{ff}$} &\textbf{$d_\text{model}$} \\
        \midrule
        Small &6 &8 &64 &2048 &512\\
        Base &12 &12 &64 &3072 &768\\
        Large &24 &16 &64 &4096 &1024\\
        \bottomrule
	\end{tabular}
        }}
	\label{tab:supmodel}
\vspace{-10pt}
\end{table}

\begin{table}[t]
\small
\caption{The breakdown of the StyleGAN2~\cite{karras2020analyzing}. 
} \label{tab:supstylegan} 
\vspace{-10pt}
\begin{center}
\resizebox{\linewidth}{!}{%
\begin{tabular}{cclc}
\toprule
\textbf{\makecell[c]{$\mathcal{W+}$\\ layer index} } & \textbf{Resolution} & \textbf{Layer name} & \textbf{\# Channels} \\
\midrule

0 & 4$\times$4 & Conv  & 512 \\
1  & 4$\times$4 & ToRGB  & 512 \\

\rowcolor[HTML]{D9D9D9} 
2  & 8$\times$8 & Conv0\_up  & 512 \\
\rowcolor[HTML]{D9D9D9} 
3  & 8$\times$8 & Conv1  & 512 \\
\rowcolor[HTML]{D9D9D9} 
3  & 8$\times$8 & ToRGB  & 512 \\


4  & 16$\times$16 & Conv0\_up  & 512 \\ 
5  & 16$\times$16 & Conv1 & 512 \\
5  & 16$\times$16 & ToRGB & 512 \\

\rowcolor[HTML]{D9D9D9} 
6  & 32$\times$32 & Conv0\_up & 512 \\
\rowcolor[HTML]{D9D9D9} 
7  & 32$\times$32 & Conv1  & 512 \\
\rowcolor[HTML]{D9D9D9}
7  & 32$\times$32 & ToRGB  & 512 \\


8  & 64$\times$64 & Conv0\_up & 512 \\
9  & 64$\times$64 & Conv1 & 512 \\
9  & 64$\times$64 & ToRGB & 512 \\

\rowcolor[HTML]{D9D9D9} 
10  & 128$\times$128 & Conv0\_up & 512 \\
\rowcolor[HTML]{D9D9D9} 
11  & 128$\times$128 & Conv1 & 256 \\
\rowcolor[HTML]{D9D9D9} 
11  & 128$\times$128 & ToRGB & 256 \\


12  & 256$\times$256 & Conv0\_up & 256 \\
13  & 256$\times$256 & Conv1 & 128 \\
13  & 256$\times$256 & ToRGB & 128 \\

\rowcolor[HTML]{D9D9D9} 
14  & 512$\times$512 & Conv0\_up & 128 \\
\rowcolor[HTML]{D9D9D9} 
15  & 512$\times$512 & Conv1 & 64 \\
\rowcolor[HTML]{D9D9D9} 
15  & 512$\times$512 & ToRGB & 64 \\


16  & 1024$\times$1024 & Conv0\_up & 64 \\
17  & 1024$\times$1024 & Conv1 & 32 \\
17  & 1024$\times$1024 & ToRGB & 32\\
\bottomrule
\end{tabular}}
\end{center}

\end{table}

\section{Details of the Human Evaluation}\label{sec:sec3}
\noindent\textbf{Questions of the Human Evaluation.}
In the human evaluations, we compare our framework with Talk-to-Edit over one aspect of the manipulated image: manipulation,
and three aspects on the generated response of each turn
in the dialogues: humanness, interestingness, and engagingness.
The instructions for these four aspects provided to participants are shown as follows:
\begin{itemize}
    \item Manipulation: \textit{Which one manipulates the image better with facial identity unchanged?}
    \item Humanness: \textit{Which one sounds more natural and personable?}
    \item Interestingness: \textit{Which one arouses your curiosity or tells you something new or useful?}
    \item Engagingness: \textit{Which one is more likely to capture your attention and make you want to further interact with it?}
\end{itemize}
We conduct the blind evaluation where participants will not be informed about the source of the manipulated images and generated responses (our framework or Talk-to-edit) to ensure fairness and objectivity.


\noindent\textbf{Inter-rater agreement.} To evaluate the agreement among the answers of all participants, we calculate the inter-rater agreement score. The average inter-rater agreement score is 0.26 in terms of Fleiss’ kappa~\cite{fleiss1973equivalence}, which demonstrates a fair agreement.

\section{More Qualitative Results}\label{sec:sec4}
\noindent\textbf{Comparison of Single-turn Editing and Our Proposed Multi-turn Editing.}
We present more visualization results in Fig.~\ref{fig:sup_sig_multi} to illustrate the attribute forgetting problem and error accumulation problem of previously single-turn editing methods, which can be avoided by our proposed multi-turn approach.
To make a fair comparison, both methods take oracle user edit requirements as input and leverage StyleCLIP~\cite{patashnik2021styleclip} model for image manipulation.

\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{sup_fig/sup_step_vs_combine1.pdf} 
\vspace{-9mm}
\end{center}
   \caption{\textbf{Comparison of single-turn editing and our proposed multi-turn editing.} (a) shows the attribute forgetting problem of the single-turn editing method. In the presented two cases, the blond hair attribute and the lipstick attribute are lost in the final edited image, respectively. (b) illustrates the error accumulation problem. In the left one, facial identity and background are changed unexpectedly in the single-turn method and these errors propagate to the final edited image. While in the right one, the hair color is changed in the single-turn method. By contrast, our proposed multi-turn editing approach avoids these issues.}
   \label{fig:sup_sig_multi} 
   \vspace{-2mm}
\end{figure*}

 

\noindent\textbf{Visualization Results of Ablation study.}
In the ablation study of the main paper, we demonstrate the effectiveness of the introduced dialogue module that extracts user requirements to guide the image editing module quantitatively. We present qualitative results in this section. We experiment with the single-turn method that doesn't equip with a dialogue module to  extract the user edit requirements and thus directly takes dialogue as input. As shown in Fig~\ref{fig:sup_ablation}, it fails to understand the user request accurately and ignores some of the user edit requirements, suggesting the significance of our introduced dialogue module in the multi-turn interactive editing scenario.

\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{sup_fig/sup_ablation_new1.pdf} 
\vspace{-9mm}
\end{center}
\caption{\textbf{Visualization results of ablation study} on the influence of the introduced dialogue module that extracts user edit requirements from the dialogue on the manipulated images. The single-turn approach doesn't equip with the dialogue module and thus takes the whole dialogue as input of the image editing module.}
\label{fig:sup_ablation} 
\vspace{-5mm}
\end{figure*}



\noindent\textbf{Comparison Between Our Method with Talk-to-Edit.}
More qualitative comparisons between our method with Talk-to-Edit~\cite{jiang2021talk} are presented in Fig.~\ref{fig:sup_t2e_forgetting}, Fig.~\ref{fig:sup_t2e_error}, Fig.~\ref{fig:sup_t2e_fail_edit}, and Fig.~\ref{fig:sup_t2e_fail_dialog}.
Fig.~\ref{fig:sup_t2e_forgetting} shows that there exists the attribute forgetting problem in Talk-to-Edit.
Fig.~\ref{fig:sup_t2e_error} illustrates that Talk-to-Edit has an error accumulation problem.
More comparisons are shown in Fig~\ref{fig:sup_t2e_fail_edit}. Moreover, error judgment in the rule-based method will lead to the unexpected break-off of the interaction in Talk-to-Edit, which is represented in Fig~\ref{fig:sup_t2e_fail_dialog}.


\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{sup_fig/sup_comparison1.pdf}
\vspace{-8mm}
\end{center}
   \caption{\textbf{Comparison of our method and Talk-to-Edit.} These results demonstrate the attribute forgetting problem in Talk-to-Edit, where the goatee attribute is lost in the left case, while the smiling attribute is lost in the right case.
      }\label{fig:sup_t2e_forgetting} 
\vspace{1mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{sup_fig/sup_comparison1_1.pdf}
\vspace{-8mm}
\end{center}
   \caption{\textbf{Comparison of our method and Talk-to-Edit.} These results demonstrate the error accumulation problem in Talk-to-Edit, where artifacts occur and propagate to the final edited image.
   }\label{fig:sup_t2e_error} 
\vspace{1mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{sup_fig/sup_comparison1_2.pdf}
\vspace{-8mm}
\end{center}
   \caption{\textbf{More visualization of comparison of our method and Talk-to-Edit.}
      }\label{fig:sup_t2e_fail_edit} 
\vspace{-5mm}
\end{figure*}

\begin{figure*}
\begin{center}

\includegraphics[width=\linewidth]{sup_fig/sup_comparison1_3.pdf}
\vspace{-8mm}
\end{center}
   \caption{\textbf{Comparison of our method and Talk-to-Edit.} These results present cases where the interaction is broken off unexpectedly in Talk-to-Edit due to its rule-based method. \textit{END} represents that the system terminates the interaction. 
   }\label{fig:sup_t2e_fail_dialog} 
\end{figure*}



