\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float}
\usepackage{color}
\usepackage{subfigure}

\usepackage[dvipsnames,table,xcdraw]{xcolor}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission
   
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE\LaTeX\ Author Guidelines for ICCV Proceedings
\title{I$^\mathbf{2}$Edit: Towards Multi-turn Interactive Image Editing via Dialogue}
\author{
Xing Cui\textsuperscript{1}\thanks{\ Equal contribution.} \qquad
Zekun Li\textsuperscript{2}\footnotemark[1] \qquad
Peipei Li\textsuperscript{1}\thanks{Corresponding Author} \qquad
Yibo Hu\qquad %
Hailin Shi \qquad %
Zhaofeng He\textsuperscript{1} \\
\textsuperscript{1}Beijing University of Posts and Telecommunications  \\ \textsuperscript{2}University of California, Santa Barbara \\
{\tt\small \{cuixing, lipeipei, zhaofenghe\}@bupt.edu.cn} \\
{\tt\small zekunli@cs.ucsb.edu}
{\tt\small \{huyibo871079699, hailinshi.work\}@gmail.com} 
}

\ificcvfinal\thispagestyle{empty}\fi


\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle

\vspace{-20pt}
\begin{center}
    \includegraphics[width=\linewidth]{main_fig/dataset_overview.pdf}%
    \captionof{figure}{\textbf{Illustration of the multi-turn interactive image editing task.} 
    The system is required to track the user's edit requirements on the facial attributes (in the gray box), edit the image, and generate the response.}
    \label{fig:dataset}
    \vspace*{1.5mm}
\end{center}%
}]

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{~Equal contribution.}
\footnotetext[2]{~Corresponding Author.}


%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace*{-2mm}

Although there have been considerable research efforts on controllable facial image editing, the desirable interactive setting where the users can interact with the system to adjust their requirements dynamically hasn't been well explored. This paper focuses on facial image editing via dialogue and introduces a new benchmark dataset, Multi-turn \textbf{I}nteractive \textbf{I}mage \textbf{Edit}ing (\textbf{I$^\mathbf{2}$Edit}), for evaluating image editing quality and interaction ability in real-world interactive facial editing scenarios. 
The dataset is constructed upon the CelebA-HQ dataset with images annotated with a multi-turn dialogue that corresponds to the user editing requirements.
\textbf{I$^\mathbf{2}$Edit} is challenging, as it needs to 1) track the dynamically updated user requirements and edit the images accordingly, as well as 2) generate the appropriate natural language response to communicate with the user. To address these challenges, we propose a framework consisting of a dialogue module and an image editing module. 
The former is for user edit requirements tracking and generating the corresponding indicative responses, while the latter edits the images conditioned on the tracked user edit requirements.
In contrast to previous works that simply treat multi-turn interaction as a sequence of single-turn interactions, we extract the user edit requirements from the whole dialogue history instead of the current single turn.
The extracted global user edit requirements enable us to directly edit the input raw image to avoid error accumulation and attribute forgetting issues. 
Extensive quantitative and qualitative experiments on the \textbf{I$^\mathbf{2}$Edit} dataset demonstrate the advantage of our proposed framework over the previous single-turn methods. We believe our new dataset could serve as a valuable resource to push forward the exploration of real-world, complex interactive image editing. Code and data will be made public.
\vspace*{-5mm}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
With the advent of deep generative models like GANs~\cite{goodfellow2020generative,karras2019style} and DDPMs~\cite{ho2020denoising,sohl2015deep}, great strides have been made in facial image editing recently. Prior works mainly focus on the setting where fixed control patterns~\cite{xu2022transeditor} or text~\cite{patashnik2021styleclip} are used as input to perform a one-time manipulation on the images. However, in real-world scenes, the multi-turn interactive editing setting where the user can interact with the system, dynamically adjust his edit requirements and see the corresponding manipulated results are more desirable. While some works~\cite{zhou2022tigan,el2019tell,kim2019codraw} have explored multi-turn image editing, they simply treat it as a sequence of successive single-turn editing, i.e., repeatedly editing the manipulated image from the previous turn according to the current turn's requirement. What's worse, they can only ``listen" to the users but not return the response to interact with the users, making them not interactive and user-friendly enough. Although a recent work, Talk-to-Edit~\cite{jiang2021talk}, makes the first attempt to generate natural language responses, the proposed ruled-based method can only generate fixed-pattern responses based on hand-crafted templates, which lacks flexibility and naturalness. 
Overall, multi-turn interactive image editing is still not well explored and is hindered by the lack of standard benchmark datasets for developing and evaluating the corresponding methods. 

To facilitate the research on multi-turn interactive image editing, we construct a new benchmark dataset, \textbf{I$^\mathbf{2}$Edit} (short for \underline{I}nteractive \underline{I}mage \underline{Edit}ing).
We construct the dataset on top of the CelebA-HQ~\cite{karras2018progressive} dataset. 
We annotate a multi-turn dialogue for each facial image from the 12k images selected from the CelebA-HQ dataset. Specifically, for each turn of the dialogue, we provide the user utterance, system response, and also annotation of the user's ``belief state'', which represents the user's global edit requirements from the beginning of the dialogue to the current turn.
The tracked user edit requirements will be used to direct the image editing.
As shown in Fig.~\ref{fig:dataset}, success on \textbf{I$^\mathbf{2}$Edit} requires the system to edit images based on requirements and also provide natural language responses to interact with the users. To evaluate the performance of the multi-turn interactive editing, we propose a series of metrics from multiple perspectives, including response quality and editing quality.

In this paper, we propose a framework to deal with the mixed challenge of dialogue response generation and multi-turn image editing. Our framework seamlessly combines a dialogue module and an image editing module. Specifically, we utilize an end-to-end task-oriented dialogue (TOD) model to extract the user belief state, i.e., user edit requirements and generate the appropriate response, given the current conversation as input. The tracked user edit requirements will be transformed into text prompt to guide a text-based image editing model StyleCLIP~\cite{patashnik2021styleclip} to perform the desired manipulation on the input raw image. 
The models are trained on our constructed \textbf{I$^\mathbf{2}$Edit} dataset.
Since the dialogue module excels at extracting user edit requirements from the beginning of the conversation to the current turn, our framework enables direct editing on the input raw image instead of the edited image from the last turn, avoiding error accumulation and attribute forgetting problems. As shown in Fig.~\ref{fig:step_vs_combine}, repeatedly editing the images from the last turn will dilute some edit requirements in the early stage of the conversation (attribute forgetting) and suffer from error accumulation problems. By contrast, our framework can avoid these problems via one-time editing on the input image based on global edit requirements extracted by our dialogue module. 

We perform extensive experiments with diverse settings to investigate the performance of our proposed framework on the \textbf{I$^\mathbf{2}$Edit} dataset. The experimental results suggest that our proposed framework is superior to the previously prevalent repeated single-turn editing methods in terms of image editing quality and response generation. 

To sum up, our main contributions are three-fold: \textbf{(1)} We propose a new benchmark dataset \textbf{I$^\mathbf{2}$Edit} to facilitate the research on multi-turn interactive image editing. 
\textbf{(2)} We propose a framework seamlessly combing the task-oriented dialogue module and an image editing module for user edit requirement tracking, image editing, and system response generation. It can avoid attribute forgetting and error accumulation issues in the previous methods.
\textbf{(3)} Qualitative and quantitative evaluation results on the \textbf{I$^\mathbf{2}$Edit} dataset suggest the advantages of our proposed multi-turn editing framework and also attract further research in this area.




\begin{figure}

\begin{center}
\includegraphics[width=1.0\linewidth]{main_fig/step_vs_combine_v3.pdf}
\end{center}
\vspace{-15pt}
   \caption{\textbf{Comparison of previous repeated single-turn editing and our proposed multi-turn editing that directly manipulates the input image.} 
   }\label{fig:step_vs_combine}
\vspace{-4mm}
\end{figure}



\section{Related Work}
\noindent\textbf{Interactive Image Editing.} Traditional image editing techniques focus on editing specific attributes such as age~\cite{li2020hierarchical}, hairstyle~\cite{olszewski2020intuitive}, or other attributes~\cite{richardson2021encoding}.
In recent years, human-computer interaction scenes~\cite{kottur2022tell,jiang2021talk} have received much attention, attracting researchers to explore interactive image editing where the users can interact with the system to adjust the edit requirements dynamically~\cite{kim2019codraw,lachmy2022draw,zhou2022tigan,jiang2021talk}.
For example, TiGAN~\cite{zhou2022tigan} generates images through successive editing with the help of the pre-trained cross-modal large model CLIP~\cite{radford2021learning} and generative model StyleGAN~\cite{karras2020analyzing}. 
GeNeVA-GAN~\cite{el2019tell} considers both current and historical information during editing.
However, these methods only ``listen'' to the user requests and return edited images but do not generate system response, making them unable to actually ``interact'' with the users.
To this end, a recent work Talk-to-Edit~\cite{jiang2021talk} introduces a rule-based system response module to generate the response. However, it is not flexible enough and can not cope with scenes that are not predefined. 
Additionally, these works view multi-turn editing as a series of successive single-turn editing, suffering from the error accumulation problem and the attribute forgetting problem as the number of interactions increases.
 
% \vspace{-0.5mm}
\noindent\textbf{Task-oriented Dialogue.} Task-oriented dialogue (TOD) aims at accomplishing the user's goal in a certain domain via interactive dialogue. 
There are two mainstream solutions for TOD. Traditional systems adopt a pipelined approach, which consists of four modules.
Firstly, the natural language understanding (NLU) module~\cite{abro2022natural} converts the current user request into semantic slots, domain information, and user intention. 
Then, the dialogue state tracking (DST) module~\cite{wu2019transferable} extracts
the dialogue state which records user requests in slot-value pairs.
Subsequently, the dialogue policy learning (POL) module~\cite{chen2017agent,geishauser2022dynamic} determines the next action of the dialogue agent based on the dialogue state.
Finally, the system response is generated by the natural language generation (NLG) module~\cite{elder2020make}. 
Recently, most works have resorted to building an end-to-end task-oriented dialogue system with pretrained language models (PLMs). For example, SimpleTOD~\cite{hosseini2020simple} treats all subtasks as a single sequence generation problem and unifies them in a simple framework. GPT-ACN~\cite{wang2022task} fixes the parameters of the PLMs and designs an adapter-copynet network for transfer learning.
PPTOD~\cite{su2022multi} utilizes a multi-task pretraining strategy in order to leverage a large amount of existing data that is partially annotated.
Existing task-oriented dialogue systems mainly focus on interactive scenarios like movie ticket/hotel booking, table reservation, etc. To the best of our knowledge, our \textbf{I$^\mathbf{2}$Edit} is the first benchmark dataset in the interactive image editing scenario.


\begin{table}
\small
\caption{\textbf{Illustration of editable attributes in I$^\mathbf{2}$Edit.} The attributes are categorized into five groups (slots). 
}
\vspace{-5pt}
\centering
\renewcommand\arraystretch{1.1}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular} {@{}lp{6cm}@{}}
\toprule
\textbf{Slot}  &\textbf{Attribute}\\
\cline{1-2}
Expression\textbf &\textit{smiling, angry, sad, no smiling}\\
\cline{1-2}
Hair color &\textit{brown hair, blond hair, black hair, gray hair}\\
\cline{1-2}
Hairstyle &\textit{straight hair, wavy hair, receding hairline, sideburns, bangs}\\
\cline{1-2}
Beard &\textit{mustache, goatee}\\
\cline{1-2}
Makeup &\textit{no makeup, heavy makeup, lipstick, earrings, bushy eyebrows, big lips, rosy cheeks, pale skin}\\
\bottomrule
\end{tabular}
}
\label{tab:attributes}
\vspace{-6mm}
\end{table}


\section{I$^\mathbf{2}$Edit Benchmark Dataset}
\textbf{I$^\mathbf{2}$Edit} simulates the scenario where a user is interacting with the system to manipulate a facial image. Each sample in the \textbf{I$^\mathbf{2}$Edit} dataset consists of a facial image and a dialogue between a user and the system on editing this image. Each turn of the dialogue is annotated with the user belief state, i.e., user edit requirements, which could direct the response generation and image editing. In this section, we will first introduce how to construct the dataset and its statistics. Then, we introduce benchmark tasks of \textbf{I$^\mathbf{2}$Edit} and corresponding evaluation metrics.

\subsection{I$^\mathbf{2}$Edit Dataset Construction}
\noindent\textbf{Facial Image Data.}
We construct the \textbf{I$^\mathbf{2}$Edit} dataset on top of the CelebA-HQ~\cite{karras2018progressive}, which is a high-resolution facial image dataset with 30k images. It provides image annotations, including identity information, 5 landmark locations, and 40 binary attributes.
We select 20 attributes from CelebA-HQ and add another three frequently-used attributes ``\textit{sad}'', ``\textit{angry}'', and ``\textit{no smiling}'', resulting in a total of 23 editable attributes in the \textbf{I$^\mathbf{2}$Edit} dataset. As shown in Table~\ref{tab:attributes}, these editable attributes are categorized into five groups. 
We select 12k images from the CelebA-HQ and annotate a dialogue on editing each image. 


\noindent\textbf{Dialogue Annotation.} 
In each turn of the dialogue, the user expresses his edit requirements on the editable attributes in natural language. The system is required to detect these requirements as belief states and map them to the response according to a pre-defined dialogue policy. Therefore, we annotate three types of data for each turn of a dialogue: (1) the user utterance, (2) user belief state (edit requirements), and (3) the system response.

To achieve that, we first determine the user edit requirements of each turn. The requirements should be limited in the editable attributes as shown in Table~\ref{tab:attributes}. We follow the terminology used in the task-oriented dialogue research and refer to each group in Table~\ref{tab:attributes} as a slot, and each editable attribute thus serves as a possible slot value for the corresponding slot. Therefore, the user edit requirement of each turn in the dialogue is represented as slot-value pairs, such as ``\textit{expression: smiling, hairstyle: bangs, hair color: black hair}''. For each given facial image, we assume that the users will modify current attributes or add more requirements. The user edit requirements are thus obtained by randomly selecting from possible editable attributes excluding the current attributes and are represented as slot-value pairs as in TOD. 


\begin{figure*}
\begin{center}
\includegraphics[width=0.95\linewidth]{main_fig/dataset_analysisv2.pdf}
\end{center}
\vspace{-20pt}
\caption{\textbf{Analysis of I$^\mathbf{2}$Edit dataset.} 
Left: Distribution of different attributes. Right: Distribution of  occurrence frequency of various attribute combinations.}\label{fig:dataset_analysis}
\vspace*{-5mm}
\end{figure*}


\begin{table}[t]
% \small
\scriptsize
    \begin{center}
        % \scalebox{0.92}{
    \caption{\textbf{Statistics of dialogues in I$^\mathbf{2}$Edit dataset.}}
    \label{tab:statics_of_dialogue}
    \vspace{-5pt}
    \resizebox{0.9\linewidth}{!}{%
        \begin{tabular}{lccc}
        \toprule[\heavyrulewidth]
            Total \# dialogues &12,000\\
            Total \# utterances &96,269\\
            Avg \# turns per dialogue &4.0\\
            Avg \# utterances per dialogue &8.0\\
            Avg \# words per user turns &11.5\\
            Avg \# words per system turns &9.1\\
            Avg \# attributes mentioned per dialogue &5.5\\
            Avg \# attributes edited per dialogue &4.0\\
        \bottomrule[\heavyrulewidth]
        \end{tabular}
        }
    \end{center}
\vspace{-4mm}
\end{table}


Given the user edit requirement of a turn, we could construct the user utterance that expresses this requirement and use a pre-defined dialogue flow policy to generate the corresponding system response. 
To reduce human efforts in annotating the dialogue, we construct the conversations with a two-phase pipeline~\cite{karras2018progressive} which contains a simulation phase~\cite{li2022controllable} and a paraphrase phase. During the simulation phase, we first prepare dialogue sketches with the help of human annotations and the large-scale pre-trained language model ChatGPT~\cite{ouyang2022training}. In the paraphrase stage, we manually check and rewrite the dialogues with human annotators to make them more natural and diverse.
Specifically, we first prepare diverse utterances describing user requests and system responses for each editable attribute. We use human-written utterances as well as utterances generated by ChatGPT to improve diversity. ChatGPT has shown expressive power in generating text with similar patterns to the examples provided in the given prompt. To generate more diverse utterances via ChatGPT, we combine instruction and a few human-written example utterances as the prompt. 
As for the system response, we prepared a set of possible dialog acts, including \textit{Next}: general queries on what to edit in the next turn, \textit{Request}: request whether to edit an attribute, \textit{Suggest}: suggestions to edit towards a specific attribute value. For each dialog act, likewise, we prepare a set of candidate system responses. When building the dataset, we first determine the proper dialog act and then select a system response from the candidates.

\noindent\textbf{Manual Paraphrase.} Once the user utterance and system response are collected, we manually check and paraphrase them if they are not satisfying. The human annotators are requested to review if the conversation follows pre-defined dialog flow logic and refine the expression to improve diversity and naturalness. 
As checking and revising the dialogue is less labor-intensive than annotating the dialogue from scratch by human annotators, the annotation efforts are significantly reduced with our two-phase pipeline.

\begin{figure}
\begin{center}
\includegraphics[width=1.0\linewidth]{main_fig/dialogue_flow.pdf}
\end{center}
\vspace{-10pt}
   \caption{\textbf{Illustration of the dialogue flow in the I$^\mathbf{2}$Edit dataset.} User-$n$ and System-$n$ represent the $n$-th user and system turn, respectively.}\label{fig:dialog_flow}
\vspace{-5mm}
\end{figure}



\subsection{I$^\mathbf{2}$Edit Dataset Statistics}
We construct the \textbf{I$^\mathbf{2}$Edit} dataset with a total of 12k examples, with each example consisting of a facial image and an annotated dialogue. We divide the \textbf{I$^\mathbf{2}$Edit} dataset into training, validation, and testing sets with 10k, 1k, and 1k examples, respectively. 
To offer deeper insights into the dataset, we present comprehensive statistics in this section.

\noindent\textbf{Analyzing Dialogues.}
Our \textbf{I$^\mathbf{2}$Edit} dataset has a total of 12k dialogues with 96k utterances, whose statistics are shown in Table~\ref{tab:statics_of_dialogue}.
We also visualize the dialogue flow of the constructed dataset in Fig.~\ref{fig:dialog_flow}. There are 5 kinds of blocks for the user turn where each representing an attribute slot, and 3 kinds of system turns each representing a type of dialog act. The width of each block indicates its occurrence frequency and the connectivity between blocks indicates their co-occurrences in a dialog flow. It can be seen that different dialog flows are relatively balanced in our dataset, indicating its high diversity.


\noindent\textbf{Analyzing Editing Attributes.} 
As for the annotated user edit requirement, we show the requested frequency of each attribute (left) and the occurrence frequency of different attribute combinations (right) in Fig.~\ref{fig:dataset_analysis}. It can be seen that most attributes are mentioned more than 500 times and more than 85\% of the attribute combinations occurred less than 4 times, indicating the diversity of our dataset.




\begin{figure*}[!ht]
\begin{center}
\includegraphics[width=1.0\linewidth]{main_fig/main3.pdf}
\end{center}
\vspace{-10pt}
\caption{\textbf{Illustration of the proposed framework.} The dialogue understanding module is utilized to track the user edit requirements and generate system responses. Then, the tracked user edit requirements are fed to the image editing module to guide the image manipulation.}\label{fig:main}
\vspace{-5mm}
\end{figure*}


\subsection{I$^\mathbf{2}$Edit Evaluation}
To evaluate the system's ability to detect the user edit requirements, generate the appropriate system response, and edit the image according to user requirements, we propose three benchmark tasks along with evaluation metrics.

\noindent\textbf{Edit Requirement Tracking.}
Similar to the dialogue belief state tracking task in TOD, we introduce the user edit requirements tracking task to evaluate the system's ability in detecting the user edit requirements.
The user edit requirements are represented as slot-value pairs, where each slot denotes a category (group) of attributes and the value indicates a specific attribute value.
The input for this task is the dialogue history from the beginning to the current turn.
The performance is evaluated with \textbf{Joint Accuracy}, which is the ratio of dialogue turns whose slots are predicted completely accurately, i.e., all the slot and slot values are predicted correctly.


\noindent\textbf{Response Generation.}
Another objective of the dialogue module is to generate fluent, reasonable, and diverse responses to interact with the users. We use BLEU~\cite{papineni2002bleu} to evaluate the fluency of generated responses with respect to reference responses. 
To evaluate the diversity of generated responses, we employ Distinct-1, 2~\cite{li2015diversity}, where Distinct-n represents the ratio of distinct n-grams in responses.

\noindent\textbf{Image Editing.}
Editing an image based on the user edit requirement is one of the main objectives of \textbf{I$^\mathbf{2}$Edit}. Given the user edit requirement containing multiple requested attributes, a powerful system should be able to manipulate them simultaneously in high quality. We evaluate the attribute editing performance from two perspectives: (1) \textit{relevance}: whether the requested attributes are edited; and (2) \textit{quality}: how well the image is edited on the attribute.

We measure the editing relevance of each attribute via the cosine similarity of the attribute and the edited image. To evaluate the relevance of multiple attributes, we report two metrics.  
\textit{Average Relevance (AvgRel)} reflects the average editing relevance on all requested attributes, where the relevance is measured with the cosine similarity of the edited image with the requested attribute:
\begin{equation}
    AvgRel=avg\left \{ Cos_{\mathrm{CLIP}}\left ( I,t \right )  \right \}_{t\in T},
\end{equation}
where $Cos_{\mathrm{CLIP}}$ represents the cosine similarity between the CLIP embeddings of the image $I$ and text $t$. $T$ represents the set of all editing attributes of the image. We also report \textit{Minimum Relevance (MinRel)}, which reflects the worst relevance among the requested attribute:
\begin{equation}
MinRel=min \left \{ Cos_{\mathrm{CLIP}}\left ( I,t \right )  \right \}_{t\in T}.
\end{equation}
As for the measure of editing quality, we utilize FID~\cite{heusel2017gans} and LPIPS~\cite{zhang2018unreasonable} metrics, which calculate the statistical similarity between the edited images and the originals.



\section{Method}
In this section, we introduce our proposed framework for \textbf{I$^\mathbf{2}$Edit}.
As illustrated in Fig.~\ref{fig:main}, the system consists of a task-oriented dialogue module and an image editing module. 
We adopt a pre-trained language model-based TOD model as the unified dialogue module for both user edit requirement tracking and response generation. Specifically, it takes the dialogue context prepended with different prompts as input and outputs the tracked user requirements and corresponding system response, respectively.
As for the image editing module, a text-based image editing model StyleCLIP~\cite{patashnik2021styleclip} is utilized, which receives the user edit requirements tracked by the dialogue model as input and edits the input image accordingly.



\subsection{Dialogue Module}
The primary goal of the dialogue module is to track user edit requirements and generate natural language feedback as responses. Following~\cite{su2022multi}, we formulate the two tasks as text generation and adopt a unified pre-trained language model T5~\cite{raffel2020exploring} for both two tasks.
Specifically, we prepend a task-specific prompt to the dialogue context to serve as the dialogue language model's input, and the model is trained to output the task-specific output. We use ``\textit{translate dialogue to dialogue response}'' and ``\textit{translate dialogue to dialogue state}'' as prompts for the user edit requirements tracking task and the response generation task, and the model is trained to generate the user edit requirements and system response, respectively


We use the T5~\cite{raffel2020exploring} model and initialize it with the PPTOD~\cite{su2022multi} checkpoint that has been pre-trained on a diverse set of dialogue corpus and thus equipped with primary TOD task completion skills. 
Given task-specific prompt $z_{t}$, dialogue history $x$, and target output $y$. The dialogue model is trained with a maximum likelihood objective and the loss function is defined as:
\begin{equation}
    \mathcal{L}_{\Theta }=-\sum_{i=1}^{\left | y \right |}logP_{\Theta }\left ( y_{i}|y_{< i};z_{t},x \right ),
\end{equation}
where $\Theta$ is the model parameters.

\subsection{Image Editing Module}
In this paper, we adopt a text-driven image editing method as in StyleCLIP~\cite{patashnik2021styleclip}, which combines the generative power of StyleGAN~\cite{karras2019style} and the joint vision-language representation learned by CLIP~\cite{radford2021learning}. For the user edit requirement text prompt, a CLIP-based loss is used to optimize the latent code of the input image toward the direction inferred by the text prompt and thus generate the manipulated image.


As the tracked user edit requirements are stored in the slot-value pair format, we first construct them into the descriptive text prompt $t$ with templates. Then, we utilize the latent optimization approach proposed in StyleCLIP to optimize the latent code and manipulate the image.
Specifically, the images are manipulated in the $\mathcal{W}+$ space, which is extended from the $\mathcal{W}$ space proposed in StyleGAN~\cite{karras2019style}.
% for better generation. 
For a StyleGAN with 18 layers, $\mathcal{W}+$ space is defined by the cascade of 18 different vectors $\left [ w_{1},...,w_{18} \right ],w_{i}\in \mathcal{W}$.
To manipulate the image base on the text, a loss based on the language-image pretraining model CLIP~\cite{radford2021learning} is introduced to optimize the latent code in the $\mathcal{W}$ space by mapping it into a shared embedding space with the texts.

As shown in Fig.~\ref{fig:main}, the input image is first inverted by a fixed StyleGAN inversion model e4e~\cite{tov2021designing} to obtain the latent code of the input image $w_{s}\in \mathcal{W}+$. Then, the learnable latent code of the edited image $w\in \mathcal{W}+$ is initialized with $w_{s}$, which is further fed to the pretrained StyleGAN to obtain its corresponding image. Finally, $w$ will be optimized toward the direction inferred by the user-provided text without changing the identity.
Specifically, the CLIP loss is calculated as the distance between the embeddings of the image and the user descriptive text in the shared embedding space learned by CLIP.
By minimizing the distance, the image is forced to be consistent with the user's descriptive text:
\begin{equation}
\mathcal{L}_{\mathrm{CLIP}}= D_{\mathrm{CLIP}}\left ( G\left ( w \right ), t \right ),
\end{equation}
where $G$ is a pretrained StyleGAN generator that maps latent code into an image. $D_{\mathrm{CLIP}}$ measures the cosine distance between the CLIP embedding of the image and the text. Then, $L_{2}$ loss is utilized for preserving the similarity between the input image and the edited image in the $\mathcal{W}+$ space:
\begin{equation}
L_{2}= \left\|w-w_{s}\right\|_2.
\end{equation}
Moreover, facial identity loss is used to preserve the facial identity:
\begin{equation}
    \mathcal{L}_{\mathrm{ID}}=1-\left\langle R\left(G\left(w_s\right)\right), R(G(w))\right\rangle,
\end{equation}
where $R$ is a pretrained face recognition network~\cite{deng2019arcface}.
Finally, the overall loss function is defined as:
\begin{equation}
\mathcal{L}=\underset{w \in \mathcal{W}+}{\arg \min } \mathcal{L}_{\mathrm{CLIP}}+\lambda_{\mathrm{L} 2}L_{2}+\lambda_{\mathrm{ID}}\mathcal{L}_{\mathrm{ID}}.
\end{equation}
\section{Experiment}


\subsection{Quantitative Evaluation}
\noindent\textbf{Dialogue Module.}
We initialize our dialogue model with different sizes of pretrained PPTOD checkpoints PPTOD$_{\text{small}}$, PPTOD$_{\text{base}}$ and PPTOD$_{\text{large}}$, respectively.
We further fine-tune the model leveraging the Adam optimizer with a learning rate of 5e-5 and a batch size of 64. We utilize a multi-task training strategy in which the user edit requirements tracking task and the natural language response generation task are trained simultaneously.

As shown in Table~\ref{tab:dialog_result}, we first evaluate the dialogue module on the user edit requirements tracking task. All three models achieve a joint accuracy of over 75\%, which indicates the accuracy of tracked edit requirements. The results show that the model with the largest size underperforms the small ones, which is also observed in PPTOD~\cite{su2022multi} and might be caused by overfitting. 
Then, we evaluate the dialogue understanding for the natural language response generation task. As the model size increases, an improvement can be seen in both the BLEU score and the Distinct-1,2 score, demonstrating that the larger model can generate responses with better quality and diversity. 


\begin{table}
\small
    
    \caption{\textbf{Quantitative Evaluation of the dialogue module.} 
    % The user edit requirement tracking is abbreviated to Requirements Tracking. $\uparrow$: higher is better.
    }
    \vspace{-5pt}
    \centering  % 表居中
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \resizebox{1.0\linewidth}{!}{%
    \begin{tabular}{ccccc}
        %\hline
        % \hlinewd{0.75pt}
        \toprule
        \multirow{2}{*}{\textbf{Model}}&\textbf{Requirements Tracking}&\multicolumn{3}{c}{\textbf{Response Generation}}\\
        \cmidrule(lr){2-2}
        \cmidrule(lr){3-5}
        &Joint Acc$\uparrow$&BLEU$\uparrow$&Distinct-1$\uparrow$&Distinct-2$\uparrow$\\
        \hline
        Small &77.482	&29.508	&0.504	&0.884\\
        % \hline
        Base &\textbf{79.206}	&29.512	&0.516	&0.886\\
        % \hline
        Large &75.177	&\textbf{30.030}	&\textbf{0.523}	&\textbf{0.890}\\
        \bottomrule
    \end{tabular}}

    \label{tab:dialog_result}
\vspace{-2mm}
\end{table}


\begin{table}[tb]
    \small
	\centering  % 表居中
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{6pt}
         \caption{\textbf{Quantitative results.} UER and Dial stand for oracle user edit requirements and dialogue, respectively. 
         % $\uparrow$: higher is better and $\downarrow$: lower is better.
         }
         \vspace{-5pt}
	% \scalebox{0.85}{
        \resizebox{1.0\linewidth}{!}{%
	\begin{tabular}{cccccc}
	    %\hline
        \toprule
		% \hlinewd{0.75pt}
		\multirow{2}{*}{\textbf{Editing Mode}}&\multirow{2}{*}{\textbf{Input}}&\multicolumn{4}{c}{\textbf{Image Editing}}\\
		\cmidrule(lr){3-6}
		% \cmidrule(lr){8-9}
        % \toprule
        && FID $\downarrow$ & LPIPS $\downarrow$ & MinRel $\uparrow$ & AvgRel $\uparrow$ \\
        \midrule
        Single-turn &UER &41.765 &0.484 &0.731 &0.752\\
        Multi-turn &UER &\textbf{39.215} &\textbf{0.443} &\textbf{0.752} &\textbf{0.770}\\
        % Single-turn (Dial)  &43.072 &0.475 &0.746 &0.764\\
        \midrule
        Single-turn &Dial  &43.193 &0.475 &0.745 &0.764\\
        Multi-turn &Dial &\textbf{39.524} &\textbf{0.442} &\textbf{0.749} &\textbf{0.769}\\
        \bottomrule
	\end{tabular}}
	\label{tab:image_editing}
\vspace{-6mm}
\end{table}

\noindent\textbf{Image Editing Module.} 
The weight of $\lambda_{\mathrm{L} 2}$ and $\lambda_{\mathrm{ID}}$ are set as 0.008 and 0.005, respectively. The editing step is set as 300, and the Adam optimizer is leveraged with a learning rate of 0.1. 
The quantitative experiment results are shown in Table~\ref{tab:image_editing}. 
To evaluate the effectiveness of our proposed multiple-turn editing approach, we first compare it with its single-turn image editing counterpart, which edits images based on the results of the previous round.  
To eliminate the impact of misidentification of user edit requirements, we utilize the oracle user edit requirements to guide image editing. 
Compared with the single-turn image editing approach, our method has significant improvements in the FID and LPIPS metrics, which indicates improved image quality of our method. We also observe that the MinRel score of the single-turn method is much lower than the multi-turn method. This is reasonable as the single-turn editing approach tends to forget some edited attribute of early turns (the attribute forgetting problem), leading to poor lowest performance.

\noindent\textbf{Ablation Study.}
We also conducted an ablation study to evaluate the effectiveness of the introduced dialogue module that extracts user requirements to guide the image editing module. 
Specifically, we experiment with the raw dialogue context instead of the extracted user edit requirements as input. 
We use oracle dialogues rather than dialogue generated by the dialogue model to eliminate the influence of the accuracy of the dialogue model. 
The results are shown in Table~\ref{tab:image_editing}. Although the MinRel and AvgRel of this approach directly using dialogue as input are comparable to ours, the FID and LPIPS scores are much lower than our approach with a dialogue module, which can be attributed to the noisy and redundant information in the dialogue.
The results highlight the significance of tracking user edit requirements from dialogues with our introduced dialogue module.


\subsection{Qualitative Evaluation}
We conduct human evaluations to compare the response generation and image editing ability between our method and Talk-to-Edit~\cite{jiang2021talk}.
We randomly select 20 images, based on which we conduct multi-turn interactive image editing conversations with our method and Talk-to-Edit. We enlist 5 graduate students who are fluent in English to make pairwise comparisons between our method and Talk-to-Edit over one aspect on the manipulated image: \textit{manipulation}, and three aspects on the generated response of each turn in the dialogues: \textit{humanness, interestingness, and engagingness.} The final answer for each question is chosen by majority voting.

The results are shown in Fig.~\ref{fig:comparison2}.
As can be seen, our method shows superiority over Talk-to-Edit in terms of both image editing and response generation. This is reasonable as Talk-to-Edit employs a rule-based to generate responses from templates and edits the image with a single-turn approach, i.e., editing the manipulated images of the previous turn. By contrast, we use a pretrained language model-based dialogue module and further finetuned it on our \textbf{I$^\mathbf{2}$Edit} to generate the response. We also use the extracted concise global user edit requirements to guide the image editing, which is proved to be more effective than the single-turn approach in our experiments. We present examples of manipulation results of our method and Talk-to-Edit in Fig.~\ref{fig:comparison}, which showcases the attribute forgetting and error accumulation issues of Talk-to-Edit. More visualization results of our method can be found in Fig.~\ref{fig:final}.

\begin{figure}[!t]
    \begin{center}
        \includegraphics[width=1.0\linewidth]{main_fig/user_study.pdf}
    \end{center}
    \vspace{-10pt}
    \caption{\textbf{Human evaluation of our method v.s. Talk-to-Edit} on one axis on image editing: \textit{Manipulation} and three axes on response generation: \textit{Humanness}, \textit{Interestingness}, and \textit{Engagingness}. }\label{fig:comparison2}
    \vspace{-5mm}
\end{figure}



\begin{figure*}

    \begin{center}
        \includegraphics[width=1.0\linewidth]{main_fig/comparison_v5.pdf}
    \end{center}
    \vspace{-7pt}
    \caption{\textbf{Visualization of the comparison between our method with Talk-to-Edit.} Left shows the attribute forgetting problem of Talk-to-Edit, where the \textit{smiling} attribute is gradually diluted. Right shows the error accumulation problem. }\label{fig:comparison}
\end{figure*}


\begin{figure*}
\begin{center}
\includegraphics[width=1.0\linewidth]{main_fig/resultsv2.pdf}
\end{center}
\vspace{-15pt}
   \caption{\textbf{Visualization of multi-turn interactive image editing results with our proposed method.}}\label{fig:final}
\end{figure*}


\section{Conclusion}
This paper introduces the \textbf{I$^\mathbf{2}$Edit} benchmark dataset, which we believe could facilitate the research on multi-turn interactive image editing. The \textbf{I$^\mathbf{2}$Edit} benchmark is challenging as it requires the system to track user edit requirements from the dialogue, edit the images, and generate the response accordingly. We propose a baseline framework that seamlessly combines a task-oriented dialogue module and an image editing module. The introduction of the task-oriented dialogue not only enables interaction with users but also extracts concise user requirements from the dialogue context to direct the image editing, avoiding the attribute forgetting and error accumulation issues in previous single-turn methods. 
We conduct comprehensive experiments and analysis, which shows the effectiveness of our approach and the potential improvement room, encouraging further work. 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\input{supplementary/egpaper_for_arxiv_supplementary}
\end{document}
