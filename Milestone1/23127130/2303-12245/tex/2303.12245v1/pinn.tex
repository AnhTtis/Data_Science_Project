\section{Physics Informed Neural Networks (PINN) for Approximating PDEs}\label{PINN}

\subsection{Generic PDE of Second Order in Time}\label{PINN0}

Consider a compact domain $D\subset \mathbb{R}^d$ ($d>0$ being an integer), and let $\mathcal{D}$ and $\mathcal{B}$ denote the differential and boundary operators. We consider the following general form of an initial boundary value problem with a generic PDE of second order in time. For any $\bm{x} \in D$, $\bm{y} \in \partial D$ and $t\in[0,T]$,
\begin{subequations}\label{general}
	\begin{align}\label{general_eq1}
		&\frac{\partial^2u}{\partial t^2}(\bm{x},t)+
  \mathcal{D}[u](\bm{x},t)=0,\\
		\label{general_eq2}
		&\mathcal{B}u(\bm{y},t)=u_{d}(\bm{y},t),\\
		\label{general_eq3}
		&u(\bm{x},0)=u_{in}(\bm{x}),
  \quad \frac{\partial u}{\partial t}(\bm{x},0) = v_{in}(\bm{x}).
	\end{align}
\end{subequations}
%Here, $u: D\times [0,T] \rightarrow \mathbb{R}^m$ is the solution of the PDE, $u_{d}: \partial D\times [0,T] \rightarrow \mathbb{R}^m$ specifies the (spatial) boundary condition and $u_{in}: D\rightarrow \mathbb{R}^m$ is the initial condition.
Here, $u(\bm x,t)$ is the unknown field solution, $u_{d}$ denotes the boundary data, and $u_{in}$ and $v_{in}$ are the initial distributions for $u$ and %$u_t=\frac{\partial u}{\partial t}$
$\frac{\partial u}{\partial t}$.
We assume that in $\mathcal{D}$ the highest derivative with respect to the time variable $t$, if any,
is of first order.

\subsection{Neural Network Representation of a Function}\label{PINN1}


%In this subsection, we introduce the definition of a neural network and the related terminology. 
Let $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ denote an activation function that is at least twice continuously differentiable. %activation function, like tanh or sigmoid. 
For any $n\in \mathbb{N}$ and $z\in \mathbb{R}^n$, we define $\sigma(z): = (\sigma(z_1),\cdots,\sigma(z_n))$, where $z_i$ ($1\leq i\leq n$) are the components of $z$. We adopt the following formal definition for a feedforward neural network as given in~\cite{2023_IMA_Mishra_NS}.

\begin{Def}[\cite{2023_IMA_Mishra_NS}]\label{pre_lem1}
Let $R\in (0,\infty]$, $L, W \in \mathbb{N}$ and $l_0, \cdots,l_L \in \mathbb{N}$. Let $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ be a twice differentiable function and define
	\begin{equation}\label{pre_eq1}
		\Theta = \Theta_{L,W,R}:= \bigcup_{L' \in \mathbb{N},L' \leq L} \, \bigcup_{l_0, \cdots,l_L \in \{1,\cdots,W\}} \, {\rlap{$ \diagdown $}\diagup}_{k=1}^{L'}([-R,R]^{l_k\times l_{k-1}}\times[-R,R]^{l_k}).
	\end{equation}
\end{Def}

For $\theta\in \Theta$, we define $\theta_k:=(W_k,b_k)$ and $\mathcal{A}_k^{\theta}: \mathbb{R}^{l_{k-1}}\rightarrow \mathbb{R}^{l_k}$ by $ z\mapsto W_kz+b_k$ for $1\leq k\leq L$, and we define $f_k^{\theta}: \mathbb{R}^{l_{k-1}}\rightarrow \mathbb{R}^{l_k}$ by
\begin{align}\label{pre_eq2}
	f_k^{\theta}=\left\{\begin {array}{lll}
	\mathcal{A}_L^{\theta}(z) & k=L,\\
	(\sigma\circ\mathcal{A}_k^{\theta})(z)& 1\leq k <L.
\end{array}\right.
\end{align}
Denote $u_{\theta}: \mathbb{R}^{l_0}\rightarrow \mathbb{R}^{l_L}$ the function that satisfies for all $z\in \mathbb{R}^{l_0}$ that
\begin{equation}\label{pre_eq3}
u_{\theta}(z)=(f_{L}^{\theta}\circ f_{L-1}^{\theta}\circ\cdots\circ f_{1}^{\theta})(z)\qquad z\in \mathbb{R}^{l_0}.
\end{equation}
We set $z=(\bm{x},t)$ and $l_0=d+1$ for approximating  the PDE problem~\eqref{general}. 

$u_{\theta}$ as defined above is the neural-network representation of a parameterized function  associated with the parameter $\theta$. This neural network contains $(L+1)$ layers ($L\geq 2$), with widths $(l_0,l_1, \cdots,l_L)$ for each layer. The input layer has a width $l_0$, and the output layer has a width $l_L$. The $(L-1)$ layers between the input/output layers are the hidden layers, with widths $l_k$ ($1\leq k\leq L-1$). $W_k$ and $b_k$ are the weight/bias coefficients corresponding to layer $k$ for $1\leq k\leq L$. 
%The width of $u_{\theta}$ is defined as $\max(l_0, \cdots,l_L)$. 
From layer to layer the network logic represents an affine transform, followed by a function composition with the activation function $\sigma$.
Note that no activation function is applied to the output layer.
We refer to $u_{\theta}$ with $L=2$ (i.e.~single hidden layer) as a shallow neural network, and  $u_{\theta}$ with $L\geq 3$ (i.e.~multiple hidden layers) as a deeper or deep neural network.

\subsection{Physics Informed Neural Network for Initial/Boundary Value Problem}\label{PINN2}

Let $\Omega = D\times [0,T]$ and $\Omega_* = \partial D\times [0,T]$ be the spatial-temporal domain%\blueSout{space-time} \myColorBrown{spatial-temporal} 
domain. We approximate the solution $u$ to the problem \eqref{general} by a neural network $u_{\theta}: \Omega \rightarrow \mathbb{R}^n$. 
%(see \eqref{pre_eq3} for definition). 
%parameterized by $\theta\in \Theta$, constituting the weights and biases, that 
With PINN we consider the residual function of the initial/boundary value problem~\eqref{general}, defined for any sufficiently smooth function $u: \Omega \rightarrow \mathbb{R}^n$ as,
for any $\bm{x} \in D$, $\bm{y} \in \partial D$ and $t\in[0,T]$,
\begin{subequations}\label{general_pinn}
\begin{align}
	\label{general_pinn_eq1}
	&\mathcal{R}_{int}[u](\bm{x},t)=
	\frac{\partial^2u}{\partial t^2}(\bm{x},t)+
	\mathcal{D}[u](\bm{x},t),\\
	\label{general_pinn_eq2}
	&\mathcal{R}_{sb}[u](\bm{y},t)=\mathcal{B}u(\bm{y},t)-u_{d}(\bm{y},t),\\
	\label{general_pinn_eq3}
	&\mathcal{R}_{tb1}[u](\bm{x},0)=u(\bm{x},0)-u_{in}(\bm{x}),\\
	\label{general_pinn_eq4}
	& \mathcal{R}_{tb2}[u](\bm{x},0)=\frac{\partial u}{\partial t}(\bm{x},0)-v_{in}(\bm{x}).
\end{align}
\end{subequations}
These residuals chacracterize how well a given function $u$ satisfies the initial/boundary value problem~\eqref{general}. If $u$ is the exact solution, $\mathcal{R}_{int}[u]=\mathcal{R}_{sb}[u]=\mathcal{R}_{tb1}[u]
=\mathcal{R}_{tb2}[u]=0$.

To facilitate the subsequent analyses,
%In fact, according to the nature of hyperbolic equations, by 
we introduce an auxiliary function $v=\frac{\partial u}{\partial t}$ and rewrite  $\mathcal{R}_{tb2}$ as 
\begin{equation}\label{general_pinn_eq4.1}
	\mathcal{R}_{tb2}[v](\bm{x},0)=v(\bm{x},0)-v_{in}(\bm{x}).
\end{equation}
We reformulate~\eqref{general_eq1} into two equations, thus separating the interior residual into the following two components:
\begin{align}
		\label{general_pinn_eq1.1}
		&\mathcal{R}_{int1}[u,v](\bm{x},t)=
		\frac{\partial u}{\partial t}(\bm{x},t)-v(\bm{x},t),\\
		\label{general_pinn_eq1.2}
		&\mathcal{R}_{int2}[u,v](\bm{x},t)=
		\frac{\partial v}{\partial t}(\bm{x},t)+
		\mathcal{D}[u](\bm{x},t).
\end{align}
With PINN, we seek a neural network $(u_{\theta},v_{\theta})$ to minimize the following quantity,
%{\color{red} (square root in the last term of the equation below, may not need it at this point)}
\begin{equation}\label{general_0G}
	\begin{split}
		\mathcal{E}_G(\theta)^2=
		&\int_{\Omega}|R_{int1}[u_{\theta}, v_{\theta}](\bm{x},t)|^2\dx
		+\int_{\Omega}|R_{int2}[u_{\theta}, v_{\theta}](\bm{x},t)|^2\dx
		%+\int_{\Omega}|\nabla R_{int1}[u_{\theta}, v_{\theta}](\bm{x},t)|^2\dx\\
		+\int_{D}|R_{tb1}[u_{\theta}](\bm{x})|^2\dx \\
		&+ \int_{D}|R_{tb2}[v_{\theta}](\bm{x})|^2\dx 
		%+\int_{D}|\nabla R_{tb1}[u_{\theta}](\bm{x})|^2\dx\\
		+ \int_{\Omega_*}|R_{sb}[u_{\theta}](\bm{x},t)|^2\ds\dt.
	\end{split}
\end{equation}
The different terms of \eqref{general_0G} may be rescaled by  different weights (penalty coefficients). For simplicity, we set all these weights to one in the analysis. $\mathcal{E}_G$ as defined above is often referred to as the generalization error. 
%[29] of the neural network $u_{\theta}$ 
Because of the integrals involved therein, $\mathcal{E}_G$ can be hard to minimize. In practice, one will approximate  \eqref{general_0G} by an appropriate numerical quadrature rule, as follows
\begin{align}\label{general_0T}
\mathcal{E}_T(\theta,\mathcal{S})^2=&\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2
	+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2
	%+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2
	+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 
	+ \mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2
	%+ \mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2
	+ \mathcal{E}_T^{sb}(\theta,\mathcal{S}_{sb})^2,
\end{align}
where
\begin{subequations}\label{general_TT}
	\begin{align}
		\label{general_T1}
		\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|R_{int1}[u_{\theta}, v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{general_T1.1}
		\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|R_{int2}[u_{\theta}, v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		%\label{general_T1.2}
		%\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\nabla R_{int1}[u_{\theta}, v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\	
		\label{general_T2}
		\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{general_T2.1}
		\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|R_{tb2}[v_{\theta}](\bm{x}_{tb}^n)|^2,\\
		%\label{general_T2.2}
		%\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\nabla R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{general_T3}
		\mathcal{E}_T^{sb}(\theta,\mathcal{S}_{sb})^2 &= \sum_{n=1}^{N_{sb}}\omega_{sb}^n|R_{sb}[u_{\theta}](\bm{x}_{sb}^n,t_{sb}^n)|^2.
	\end{align}
\end{subequations}
%\begin{equation}\label{general_T}	\mathcal{E}_T(\theta,\mathcal{S})^2=\mathcal{E}_T^{int}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{tb}(\theta,\mathcal{S}_{tb})^2 + \mathcal{E}_T^{sb}(\theta,\mathcal{S}_{sb}),
%\end{equation}
%or
The quadrature points in the spatial-temporal domain and on the spatial and temporal boundaries,  $\mathcal{S}_{int}=\{(\bm{x}_{int}^n,t_{int}^n)\}_{n=1}^{N_{int}}$, $\mathcal{S}_{sb}=\{(\bm{x}_{sb}^n,t_{sb}^n)\}_{n=1}^{N_{sb}}$ and $\mathcal{S}_{tb}=\{(\bm{x}_{tb}^n,t_{tb}^n=0)\}_{n=1}^{N_{tb}}$, constitute the input data sets to the neural network. 
In the above equations $\mathcal{E}_T(\theta,\mathcal{S})^2$ is referred to as the training error (or training loss), and $\omega_{\star}^n$ are suitable quadrature weights for $\star=int$, $sb$ and $tb$. Therefore, PINN attempts to minimize the training error $\mathcal{E}_T(\theta,\mathcal{S})^2$ over the network parameters $\theta$, and upon  convergence of optimization the trained $u_{\theta}$ contains the  approximation of the solution $u$ to the problem \eqref{general}. 
%Note that the training error in \eqref{general_T} or \eqref{general_0T} depends on the boundary conditions. 

\begin{Remark}
The generalization error \eqref{general_0G} (with the corresponding training error \eqref{general_0T}) is the standard (canonical) PINN form if one introduces $v=\frac{\partial u}{\partial t}$ and reformulates~\eqref{general_eq1} into two equations. 
%However, we aim at analysing the PDE of second order in time in this paper. In order to better establish the analysis process, we will write it into two equations of the first order in time, which will add some items compared with \eqref{general_0G} and \eqref{general_G}.
%
We would like to emphasize that our analyses below suggest  alternative forms for the generalization error, e.g.
\begin{equation}\label{general_G}
	\begin{split}
		\mathcal{E}_G(\theta)^2=
		&\int_{\Omega}|R_{int1}[u_{\theta}, v_{\theta}](\bm{x},t)|^2\dx
		+\int_{\Omega}|R_{int2}[u_{\theta}, v_{\theta}](\bm{x},t)|^2\dx
		+\int_{\Omega}|\nabla R_{int1}[u_{\theta}, v_{\theta}](\bm{x},t)|^2\dx\\
		&+\int_{D}|R_{tb1}[u_{\theta}](\bm{x})|^2\dx
		+ \int_{D}|R_{tb2}[v_{\theta}](\bm{x})|^2\dx
		+\int_{D}|\nabla R_{tb1}[u_{\theta}](\bm{x})|^2\dx\\
		&+ \left(
		\int_{\Omega_*}|R_{sb}[u_{\theta}](\bm{x},t)|^2\ds\dt
		\right)^{\frac{1}{2}},
	\end{split}
\end{equation}
which differs from~\eqref{general_0G} in the terms $\nabla R_{int1}$,  $\nabla R_{tb1}$ and the last term. The corresponding training error is,
\begin{align}\label{general_T}
\mathcal{E}_T(\theta,\mathcal{S})^2=&\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2
	+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2
	+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2
	+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 
	\nonumber\\
	&+ \mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2
	+ \mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2
	+ \mathcal{E}_T^{sb}(\theta,\mathcal{S}_{sb}),
\end{align}
where
\begin{equation}\left\{
\begin{split}
&\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2 = \sum_{n=1}^{N_{int}}\omega_{int}^n|\nabla R_{int1}[u_{\theta}, v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2, \\
&
\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 = \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\nabla R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2.
\end{split}
\right.
\end{equation}
The error analyses also suggest additional terms in the generalization error for different equations.
%Both the standard form~\eqref{general_0G} and the alternative forms will be considered in the numerical simulations.


\end{Remark}


\subsection{Numerical Quadrature Rules}\label{PINN3}

As discussed above, we need to approximate the integrals of functions. The analysis in the subsequent sections requires well-known results on numerical quadrature rules as reviewed below. 

Given $\Lambda \subset \mathbb{R}^d$ and a function $f\in L^1(\Lambda)$, we would like to approximate $\int_{\Lambda}f(z)\diff{z}$. %with $dy$ denoting the $d$-dimensional Lebesgue measure. 
A  quadrature rule provides an approximation by
\begin{equation}\label{int}
\int_{\Lambda}f(z)\diff{z}\approx
\frac{1}{M}\sum_{n=1}^M \omega_n f(z_n),
\end{equation}
where $z_n\in \Lambda$ ($1\leq n\leq M$) are the quadrature points and $\omega_n$ ($1\leq n\leq M$) denote the appropriate quadrature weights. 
The approximation accuracy is influenced by the type of quadrature rule, the number of quadrature points ($M$), and the regularity of $f$. For the mid-point rule, which is assumed in the analysis in the current work, the approximation accuracy is given by
\begin{equation}\label{int1}
%\mathcal{Q}_M^{\Lambda}[f]:=\frac{1}{M}\sum_{m=1}^M\omega_mf(y_m), \qquad 
\left|\int_{\Lambda}f(z)\diff{z}-
\frac{1}{M}\sum_{n=1}^M \omega_n f(z_n)
%\mathcal{Q}_M^{\Lambda}[f]
\right|\leq C_fM^{-2/d},
\end{equation}
where $C_f\lesssim \|f\|_{C^2(D)}$ ($a\lesssim b$ denotes $a\leq Cb$) and $D$ has been partitioned into $M\sim N^d$ cubes 
%with an edge length $\frac{1}{N}$ 
and $z_n$ ($1\leq n\leq M$) denote the midpoints of these cubes~\cite{DavisR2007}.
%
In this paper, we use $C$ to denote a universal constant, which may depend on $k, d, T, u$ and $v$ but not on $N$. And we use the subscript to emphasize its dependence when necessary, e.g.~$C_{d}$ is a constant depending only on $d$.

We focus on PDE problems in relatively low dimensions ($d\leq 3$) in this paper and employ the standard quadrature rules. %for the approximation of integrals.
We note that in higher dimensions the standard quadrature rules may not be favorable. In this case the random training points or low-discrepancy training points 
\cite{Mishra2021Enhancing} may be preferred. 
%{\color{red}the relatively low-dimensional setting of the Navier-Stokes equations i.e., $d\geq 4$, allows the use of standard deterministic numerical quadrature points. In order to obtain explicit rates, we will focus on the midpoint rule, but our analysis will also hold for general deterministic numerical quadrature rules. (is this from Mishra's paper ??)}

%We briefly recall the midpoint rule. For $N\in \mathbb{N}$, we partition $\Lambda$ into $M\sim N^d$ cubes of edge length $1/N$ and we denote by $\{y_m\}_{m=1}^M$ the midpoints of these cubes. The formula and accuracy of the midpoint rule $\mathcal{Q}_M^{\Lambda}$ are then given by,

\vspace{0.2in} 
%{\color{red} need to add a vspace '\textbackslash vspace\{0.2in\}' here?}
In subsequent sections we focus on three representative dynamic equations of second order in time (the wave equation, Sine-Gordon equation, and the linear elastodynamic equation), and provide the error estimate for approximating these equations by PINN. We note that these  analyses suggest alternative forms for the training loss function that are somewhat different from the standard PINN forms~\cite{Raissi2019pinn}. The PINN numerical results based on the standard  form for the loss function, and based on the alternative forms as suggested by
the error estimate, will be provided after the presentation of the theoretical analysis. In what follows,  for brevity we adopt the notation of $\mathcal{F}_\Xi=\frac{\partial \mathcal{F}}{\partial \Xi}$, $\mathcal{F}_{\Xi\Upsilon}=\frac{\partial^2 \mathcal{F}}{\partial\Xi \partial \Upsilon}$ ($\Xi,\Upsilon\in\{t, x\}$),
for any sufficiently smooth function $\mathcal{F}: \Omega\rightarrow \mathbb R^n$.
%in some places as a means of achieving symbolic concision.




