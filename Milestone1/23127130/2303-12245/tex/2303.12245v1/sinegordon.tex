\section{Physics Informed Neural Networks for Approximating the Sine-Gordon Equation}\label{Sine-Gordon}
\subsection{Sine-Gordon Equation}
%\setcounter{equation}{0}
Let $D\subset \mathbb{R}^d$ be  an open connected bounded set with a boundary $\partial D$. We consider the following Sine-Gordon equation:
\begin{subequations}\label{SG}
             \begin{align}
                          \label{SG_eq0}
                          &u_{t} - v = 0  \ \qquad\qquad\qquad\qquad\qquad\qquad\quad\ \ \, \text{in}\ D\times[0,T],\\
                          \label{SG_eq1}
                          &\varepsilon^2v_{t} = a^2\Delta u - \varepsilon_1^2u-g(u)+f   \  \ \quad\qquad\qquad \text{in}\ D\times[0,T],\\
                          \label{SG_eq2}
                          &u(\bm{x},0)=\psi_{1}(\bm{x})\qquad\qquad\qquad\qquad\qquad\qquad \text{in}\ D,\\
                          \label{SG_eq3}
                          &v(\bm{x},0)=\psi_{2}(\bm{x}) \qquad\qquad\qquad\qquad\qquad\qquad \, \text{in}\ D,\\
                          \label{SG_eq4}
                          &u(\bm{x},t)|_{\partial D}=u_{d}(t)  \qquad\qquad\qquad\qquad\qquad\ \ \ \, \text{in}\ \partial D\times[0,T],
             \end{align}
\end{subequations}
where $u$ and $v$ are the field functions to be solved for, $f$ is a source term, and $u_d$, $\psi_1$ and $\psi_2$ denote the boundary/initial conditions. $\varepsilon>0$, $a>0$ and $\varepsilon_1\geq 0$ are  constants.
% ! 2022 (IMA) Estimates on the generalization error of physics-informed neural networks for approximating PDEs.pdf P_{14}(3.2)
$g(u)$ is a nonlinear term. We assume that the nonlinearity is globally Lipschitz, i.e., there exists a constant $L$ (independent of $v$ and $w$) such that
\begin{equation}\label{non2}
             |g(v) - g(w)|\leq L|v-w|, \qquad \forall v, \, w \in \mathbb{R}.
\end{equation}

\begin{Remark}\label{sec6_Remark1} 
%In recent decades, 
The existence and regularity of the solution to the Sine-Gordon equation with different nonlinear terms have been the subject of several studies in the literature; see~\cite{Baoxiang1997Classical,Kubota2001Global,Shatah1982Global,Shatah1985Normal,Temam1997Infinite}.
             
             The book \cite{Temam1997Infinite} provides the existence and regularity result of the following Sine-Gordon equation, % P_{204} Th3.1;  P_{189} Th2.1
             \[
             u_{tt} + \alpha u_t - \Delta u + g(u)=f.
             \]
             Let $\alpha \in \mathbb{R}$, $g(u)$ be a $C^2$ function from $\mathbb{R}$ to $\mathbb{R}$ and satisfy certain assumptions. If $f\in C([0,T]; L^{2}(D))$, $\psi_{1}\in H^{1}(D)$ and $\psi_{2}\in L^{2}(D)$, then there exists a unique solution $u$ to this Sine-Gordon equation such that $u \in C([0,T];H^{1}(D))$ and $u_t \in C([0,T];L^{2}(D))$. Furthermore, 
             $f'\in C([0,T]; L^{2}(D))$, $\psi_{1}\in H^{2}(D)$ and $\psi_{2}\in H^{1}(D)$, it holds $u \in C([0,T];H^{2}(D))$ and $u_t \in C([0,T];H^{1}(D))$. 
             
             Let $g$ be a smooth function of degree 2. The following equation is studied in \cite{Shatah1985Normal}, 
             \[
             u_{tt} - \Delta u  + u+ g(u,u_t,u_{tt})=0,
             \]
             where it is reformulated as
             \[
             \bm{u}_{t} = A\bm{u} + G(\bm{u}),
             \]
             in which $\bm{u}=\begin{pmatrix}
             	u\\	
             	u_t
             \end{pmatrix}$, $A =\begin{pmatrix}
             0 & 1 \\
             \Delta-1  & 0
         \end{pmatrix}$ 
     and $G=\begin{pmatrix}
         0, \\
         -g(u,u_t,u_{tt})
     \end{pmatrix}$. Set $X=H^k(\mathbb{R}^n) \bigoplus H^{k-1}(\mathbb{R}^n)$, $k>n+2+2a$ with $a>1$. Given $\bm{u}_0 =\begin{pmatrix}
     \psi_1\\	
     \psi_2
 \end{pmatrix} \in X$ and $\|\bm{u}_0\|_{X} = \sigma$, there exists a $T_0 = T_0(\sigma)$ depending on the size of the initial data $\sigma$ and a unique solution $\bm{u} \in C([0,T_0],X)$.

The reference~\cite{Baoxiang1997Classical} provides the following result.
Under certain conditions for the nonlinear term $g(u)$, with $f = 0$, $d\leq 5$, $k \geq \frac{d}{2} + 1$, $\psi_{1}\in H^{k}(D)$ and $\psi_{2}\in H^{k-1}(D)$, there exists a unique solution $u \in C((0,\infty);H^{k}(D))$ of nonlinear Klein–Gordon equation.  
%\cite{Baoxiang1997Classical}
%(Assume $\varphi=0$, Klein–Gordon–Schrodinger equation becomes the Cauchy problem for the non-linear Klein—Gordon equation).

The following result is due to~\cite{Kubota2001Global}.
Under certain conditions for the nonlinear term $g(u)$, with $f = 0$, $\psi_{1}\in H^{k}(D)$ and $\psi_{2}\in H^{k-1}(D)$ with a positive constant $k\geq 4$, there exists a positive constant $T_k$ and a unique solution $u \in C([0,T_k];H^{k}(D))\cap C^1([0,T_k];H^{k-1}(D))\cap C^2([0,T_k];H^{k-2}(D))$ to the nonlinear wave equations with different speeds of propagation.
%\cite{Kubota2001Global}. 
%For large integer $k$, \cite{Shatah1982Global} has the similar results for nonlinear evolution equations.
\end{Remark}

A survey of literature indicates that,
while several works have touched on the regularity of the solution to the Sine-Gordon equations, none of them is comprehensive. 
%For the smooth development of later research, 
To facilitate the subsequent analyses, we make the following  assumption in light of Remark \ref{sec6_Remark1}. Let $k\geq1$, $g(u)$ and $f$ be sufficiently smooth and bounded. Given $\psi_{1}\in H^{r}(D)$ and $\psi_{2}\in H^{r-1}(D)$ with $r \geq \frac{d}{2} + k$,  we assume that there exists $T>0$ and a classical solution $u$ and $v$ to the Sine-Gordon equations \eqref{SG} such that $u \in C([0,T];H^{k}(D))$ and $v \in C([0,T];H^{k-1}(D))$. Then, it follows that $u \in C^k(D\times[0,T])$ and $v \in C^{k-1}(D\times[0,T])$ based on the Sobolev embedding theorem.

\subsection{Physics Informed Neural Networks}

Let $\Omega = D\times [0,T]$ %$D_T = D \times(0,T)$
and $\Omega_* = \partial D\times [0,T]$ be the space-time domain. We define the following residuals for the PINN approximation, $u_{\theta}: \Omega \rightarrow \mathbb{R}$ and $v_{\theta}: \Omega \rightarrow \mathbb{R}$, for the Sine-Gordon equations \eqref{SG}:
\begin{subequations}\label{SG_pinn}
	\begin{align}
		\label{SG_pinn_eq1}
		&R_{int1}[u_{\theta},v_{\theta}](\bm{x},t) =u_{\theta t}-v_{\theta},\\
		\label{SG_pinn_eq2}
		&R_{int2}[u_{\theta},v_{\theta}](\bm{x},t) =\varepsilon^2v_{\theta t}-a^2\Delta u_{\theta} +\varepsilon_1^2u_{\theta}+g(u_{\theta})-f,\\
		\label{SG_pinn_eq3}
		&R_{tb1}[u_{\theta}](\bm{x}) =u_{\theta}(\bm{x},0)-\psi_{1}(\bm{x}),\\
		\label{SG_pinn_eq4}
		&R_{tb2}[v_{\theta}](\bm{x}) =v_{\theta}(\bm{x},0)-\psi_{2}(\bm{x}),\\
		\label{SG_pinn_eq5}
		&R_{sb}[v_{\theta}](\bm{x},t) =v_{\theta}(\bm{x},t)|_{\partial D}-u_{dt}(t),
	\end{align}
\end{subequations}
where $u_{dt}=\frac{\partial u_d}{\partial t}$.
Note that for the exact solution $(u,v)$, $R_{int1}[u,v]=R_{int2}[u,v]=R_{tb1}[u]=R_{tb2}[v]=R_{sb}[v]=0$. 
%Hence, within the PINN algorithm, one seeks to find a neural network $(u_{\theta},v_{\theta})$, for which all residuals are simultaneously minimized, e.g. by 
With PINN we minimize the following generalization error,
\begin{align}\label{SG_G}
	\mathcal{E}_G(\theta)^2&=\int_{\Omega}|R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|R_{int2}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|\nabla R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt
	\nonumber\\
	&+\int_{D}|R_{tb1}[u_{\theta}](\bm{x})|^2\dx+\int_{D}|R_{tb2}[v_{\theta}](\bm{x})|^2\dx
	+\int_{D}|\nabla R_{tb1}[u_{\theta}](\bm{x})|^2\dx
	\nonumber\\
	&
	+\left(\int_{\Omega_*}|R_{sb}[v_{\theta}](\bm{x},t)|^2\ds\dt\right)^{\frac{1}{2}}.
\end{align}

Let 
\begin{equation*}
\hat{u} = u_{\theta}-u, \quad \hat{v} = v_{\theta}-v,
\end{equation*}
where $(u,v)$ denotes the exact solution.
We define the total error of the PINN approximation of the Sine-Gordon equations \eqref{SG} as, 
\begin{equation}\label{SG_total}
	\mathcal{E}(\theta)^2=\int_{\Omega}(|\hat{u}(\bm{x},t)|^2+a^2|\nabla \hat{u}(\bm{x},t)|^2+\varepsilon^2|\hat{v}(\bm{x},t)|^2)\dx\dt.
\end{equation}
Then we choose the training set $\mathcal{S} \subset \overline{D}\times [0,T]$ with $\mathcal{S} = \mathcal{S}_{int} \cup \mathcal{S}_{sb} \cup \mathcal{S}_{tb}$, based on suitable quadrature points:
%$\bullet$
\begin{itemize}
    \item Interior training points $\mathcal{S}_{int}=\{{z}_n\}$ for $1\leq n \leq N_{int}$, with each ${z}_n= (\bm{x},t)_n \in D \times(0,T)$.
    
    \item Spatial boundary training points $\mathcal{S}_{sb}=\{{z}_n\}$ for $1\leq n \leq N_{sb}$, with each ${z}_n= (\bm{x},t)_n \in \partial D\times (0,T)$.
 
    \item Temporal boundary training points $\mathcal{S}_{tb}=\{\bm{x}_n\}$ for $1\leq n \leq N_{tb}$ with  each $\bm{x}_n \in D$.
	%\vspace{-0.2em}
\end{itemize}
The integrals in \eqref{SG_G} are approximated by a  numerical quadrature rule, resulting in the training loss,
\begin{align}\label{SG_T}
	\mathcal{E}_T(\theta,\mathcal{S})^2&
	=\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2
	\nonumber\\
	&
	+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 +\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 +\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 
	+\mathcal{E}_T^{sb}(\theta,\mathcal{S}_{sb}),
\end{align}
where
\begin{subequations}
	\begin{align}
		\label{SG_T1}
		\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|R_{int1}[u_{\theta},v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{SG_T01}
		\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|R_{int2}[u_{\theta},v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{SG_T001}
		\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\nabla R_{int1}[u_{\theta},v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{SG_T2}
		\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{SG_T02}
		\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|R_{tb2}[v_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{SG_T002}
		\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\nabla R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{SG_T3}
		\mathcal{E}_T^{sb}(\theta,\mathcal{S}_{sb})^2&= \sum_{n=1}^{N_{sb}}\omega_{sb}^n|R_{sb}[v_{\theta}](\bm{x}_{sb}^n,t_{sb}^n)|^2.
	\end{align}
\end{subequations}
Here the quadrature points in space-time constitute the data sets $\mathcal{S}_{int} = \{(\bm{x}_{int}^n,t_{int}^n)\}_{n=1}^{N_{int}}$, $\mathcal{S}_{tb} = \{\bm{x}_{tb}^n)\}_{n=1}^{N_{tb}}$ and $\mathcal{S}_{sb} = \{(\bm{x}_{sb}^n,t_{sb}^n)\}_{n=1}^{N_{sb}}$, and $\omega_{\star}^n$ are the quadrature weights with $\star$ being $int$, $tb$ or $sb$.


\subsection{Error Analysis}  %estimate on the generalization error

%In this subsection,  the results on the PINN approximations of the solutions of the Sine-Gordon equations are summarized by the following theorems.

By substracting the Sine-Gordon equations \eqref{SG} from the residual equations~\eqref{SG_pinn}, we get,
\begin{subequations}\label{SG_error}
	\begin{align}
		\label{SG_error_eq1}
		&R_{int1}=\hat{u}_t-\hat{v},\\
		\label{SG_error_eq2}
		&R_{int2}=\varepsilon^2\hat{v}_{t}-a^2\Delta \hat{u} +\varepsilon_1^2\hat{u}+g(u_{\theta})-g(u),\\
		\label{SG_error_eq3}
		&R_{tb1}=\hat{u}(\bm{x},0),\\
		\label{SG_error_eq4}
		&R_{tb2}=\hat{v}(\bm{x},0),\\
		\label{SG_error_eq5}
		&R_{sb}=\hat{v}(\bm{x},t)|_{\partial D}.
	\end{align}
\end{subequations}
The results on the PINN approximations to the Sine-Gordon equations are summarized in the following theorems.

\begin{Theorem}\label{sec6_Theorem1} 
	Let $d$, $r$, $k \in \mathbb{N}$ with $k\geq 3$. Assume that $g(u)$ is Lipschitz continuous, $u \in C^k(D\times[0,T])$ and $v \in C^{k-1}(D\times[0,T])$. Then for every integer $N>5$, there exist $\tanh$ neural networks $u_{\theta}$ and $v_{\theta}$, each with two hidden layers, of widths at most $3\lceil\frac{k}{2}\rceil|P_{k-1,d+2}| + \lceil NT\rceil+ d(N-1)$ and $3\lceil\frac{d+3}{2}\rceil|P_{d+2,d+2}| \lceil NT\rceil N^d$, such that
	\begin{subequations}
		\begin{align}
			\label{lem6.1}
			&\|R_{int1}\|_{L^2(\Omega)},\|R_{tb1}\|_{L^2(D)}\lesssim {\rm ln}NN^{-k+1},\\
			\label{lem6.2}
			&\|R_{int2}\|_{L^2(\Omega)},\|\nabla R_{int1}\|_{L^2(\Omega)}, \|\nabla R_{tb1}\|_{L^2(D)}\lesssim {\rm ln}^2NN^{-k+2},\\
			\label{lem6.3}
			&\|R_{tb2}\|_{L^2(D)},\|R_{sb}\|_{L^2(\partial D\times [0,t])}\lesssim {\rm ln}NN^{-k+2}.
		\end{align}
	\end{subequations}
	%where $a\lesssim b$ denotes $a\leq Cb$, and the constant $C$ may depend on $k, d, T, u$ and $v$ but not on $N$.
\end{Theorem}  
\noindent The proof of this theorem is provided in the Appendix~\ref{Proof_sinegordon}.

Theorem \ref{sec6_Theorem1} implies that the PINN residuals in \eqref{SG_pinn} can be made arbitrarily small by choosing a  sufficiently large $N$. Therefore, the generalization error $\mathcal{E}_G(\theta)^2$ can be made arbitrarily small. 

We next show that the PINN total approximation error $\mathcal{E}(\theta)^2$ can be controlled by the generalization error $\mathcal{E}_G(\theta)^2$ (Theorem \ref{sec6_Theorem2} below), and by the training error~$\mathcal{E}_T(\theta,\mathcal{S})^2$ (Theorem \ref{sec6_Theorem3} below). The proofs for Theorem \ref{sec6_Theorem2} and Theorem \ref{sec6_Theorem3} are provided in the Appendix \ref{Proof_sinegordon}.

\begin{Theorem}\label{sec6_Theorem2} Let $d\in \mathbb{N}$, $u\in C^1(\Omega)$ and $v\in C^0(\Omega)$ be the classical solution of
	the Sine-Gordon equation \eqref{SG}. Let $(u_{\theta},v_{\theta})$ denote the  PINN approximation with parameter $\theta$. Then the following relation holds, 
	\begin{equation}\label{lem6.09}
		\mathcal{E}(\theta)^2 = \int_0^{T}\int_{D}(|\hat{u}(\bm{x},t)|^2+a^2|\nabla \hat{u}(\bm{x},t)|^2+\varepsilon^2|\hat{v}(\bm{x},t)|^2)\dx\dt
		\leq C_GT\exp\left((2+\varepsilon_1^2+L+a^2)T\right),
	\end{equation}
	where
	\begin{align*}
	&C_G=\int_{D}(|R_{tb1}|^2+a^2|\nabla R_{tb1}|^2+\varepsilon^2|R_{tb2}|^2)\dx + \int_{0}^{T}\int_{D}(|R_{int1}|^2+|R_{int2}|^2+a^2|\nabla R_{int1}|^2)\dx\dt\\
	&\qquad +  2C_{\partial D}|T|^{\frac{1}{2}}\left(\int_{0}^{T}\int_{\partial D}|R_{sb}|^2\ds\dt\right)^{\frac{1}{2}},
	\end{align*}
and $C_{\partial D}=a^2|\partial D|^{\frac{1}{2}}(\|u\|_{C^1(\partial D\times[0,t])}+||u_{\theta}||_{C^1(\partial D\times[0,t])})$.
\end{Theorem}

\begin{Theorem}\label{sec6_Theorem3} Let $d\in \mathbb{N}$ and $T>0$, and let $u\in C^4(\Omega)$ and $v\in C^3(\Omega)$ be the classical solution to the Sine-Gordon equation \eqref{SG}. Let $(u_{\theta},v_{\theta})$ denote the PINN approximation with parameter $\theta \in \Theta$. Then the following relation holds, 
             \begin{align}\label{lem6.9}
                          &\int_0^{T}\int_{D}(|\hat{u}(\bm{x},t)|^2+a^2|\nabla \hat{u}(\bm{x},t)|^2+\varepsilon^2|\hat{v}(\bm{x},t)|^2)\dx\dt\leq C_TT\exp\left((2+\varepsilon_1^2+L+a^2)T\right) 
                          	\nonumber\\
                          &\qquad=\mathcal{O}(\mathcal{E}_T(\theta,\mathcal{S})^2 + M_{int}^{-\frac{2}{d+1}} +M_{tb}^{-\frac{2}{d}}+M_{sb}^{-\frac{1}{d}}),                       
             \end{align}
             where the constant $C_T$ is defined by 
             \begin{align*}
             	C_T=&C_{({R_{tb1}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb1}^2)+\varepsilon^2\left(C_{({R_{tb2}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb2}^2) \right)\\
             	&+a^2\left( C_{(|\nabla R_{tb1}|^2)}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(|\nabla R_{tb1}|^2) \right)+C_{({R_{int1}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int1}^2)\\
             	&+C_{({R_{int2}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int2}^2)
             	+a^2\left(C_{(|\nabla R_{int1}|^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla R_{int1}|^2)\right),\\
             	&+2C_{\partial D}|T|^{\frac{1}{2}}\left(C_{({R_{sb}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb}^2)\right)^{\frac{1}{2}}.
             \end{align*}
\end{Theorem}
%The proofs of Theorem \ref{sec6_Theorem2} and Theorem \ref{sec6_Theorem2} are provided in the Appendix \ref{Proof_sinegordon}.

It follows from Theorem \ref{sec6_Theorem3} that the PINN approximation error $\mathcal{E}(\theta)^2$ can be arbitrarily small, provided that the training error $\mathcal{E}_T(\theta,\mathcal{S})^2$ is sufficiently small  and the sample set is sufficiently large.
