\section{Physics Informed Neural Networks for Approximating Wave Equation}\label{Wave}
%\setcounter{equation}{0}

\subsection{Wave Equation}

Consider the following wave equations on the torus $D=[0,1)^d \subset \mathbb{R}^d$ with periodic boundary conditions:
\begin{subequations}\label{wave}
	\begin{align}
		\label{wave_eq0}
		&u_{t} - v = 0  \ \, \qquad\qquad\qquad \text{in}\ D\times[0,T],\\
		\label{wave_eq1}
		&v_{t} -\Delta u = f   \  \ \quad\qquad\qquad \text{in}\ D\times[0,T],\\
		\label{wave_eq2}
		&u(\bm{x},0)=\psi_{1}(\bm{x})\qquad\qquad \ \text{in}\ D,\\
		\label{wave_eq3}
		&v(\bm{x},0)=\psi_{2}(\bm{x}) \,\qquad\qquad \ \text{in}\ D,\\
		\label{wave_eq4}
		&u(\bm{x},t)=u(\bm{x}+1,t)  \qquad\ \ \text{in}\ \partial D\times[0,T],\\
		\label{wave_eq5}
		&\nabla u(\bm{x},t)=\nabla u(\bm{x}+1,t)  \quad \text{in}\ \partial D\times[0,T].
	\end{align}
\end{subequations}

The regularity results for linear evolution equations of the second order in time have been studied in the Book \cite{Temam1997Infinite}. %\citep[Sect II, Theorem 4.2]{Temam1997Infinite}. 
When the self-adjoint operator $\mathcal{A}$ takes $\Delta$, the linear evolution equations of second order in time become the classical wave equations, and then we can also obtain the following regularity results.

\begin{Lemma}\label{sec5_Lemma1} Let $r\geq1$, $\psi_{1}\in H^{r}(D)$, $\psi_{2}\in H^{r-1}(D)$ and $f\in L^{2}([0,T];H^{r-1}(D))$, then there exists a unique solution $u$ to the classical wave equations such that $u \in C([0,T];H^{r}(D))$ and $u_t \in C([0,T];H^{r-1}(D))$.
\end{Lemma}

\begin{Lemma}\label{sec5_Lemma2} Let $k\in\mathbb N$, $\psi_{1}\in H^{r}(D)$, $\psi_{2}\in H^{r-1}(D)$ and $f\in C^{k-1}([0,T];H^{r-1}(D))$ with $r>\frac{d}{2}+k$, then there exists $T>0$ and a classical solution $u$ to the wave equations such that $u(t = 0) = \psi_1$, $u_t(t = 0) = \psi_2$, $u \in C^k(D\times[0,T])$ and $v \in C^{k-1}(D\times[0,T])$.
\end{Lemma}
\begin{proof}  By Lemma \ref{sec5_Lemma1}, there exists $T > 0$ and the solution $(u, v)$ to the wave equations such that $u(t = 0) = \psi_1$, $v(t = 0) = \psi_2$, $u \in C([0,T];H^{r}(D))$ and $v \in C([0,T];H^{r-1}(D))$. As $r>\frac{d}{2}+k$, $H^{r-k}(D)$ is a Banach algebra.
	
	For $k = 1$, since $u \in C([0,T];H^{r}(D))$, $v \in C([0,T];H^{r-1}(D))$ and $f\in C([0,T];H^{r-1}(D))$, we have $u_t = v\in C([0,T];H^{r-1}(D))$ and $v_t = \Delta u +f\in C([0,T];H^{r-2}(D))$. Then, it implies that $u\in C^1([0,T];H^{r-1}(D))$ and $v\in C^1([0,T];H^{r-2}(D))$.
	
	For $k = 2$, by $f\in C^{1}([0,T];H^{r-1}(D))$, we have $u_{tt} = v_t \in C([0,T];H^{r-2}(D))$ and $v_{tt} = \Delta u_t +f_t \in C([0,T];H^{r-3}(D))$. Then, it implies that $u\in C^2([0,T];H^{r-2}(D))$ and $v\in C^2([0,T];H^{r-3}(D))$.
	
	% sobolev 空间和偏微分方程--王术.pdf p_{70} Sobolev embedding
	Repeating the same argument, we have $u \in \cap_{l=0}^k C^l([0,T];H^{r-l}(D))$ and $v \subset \cap_{l=0}^k C^l([0,T];H^{r-l-1}(D))$. Then, applying the Sobolev embedding theorem and $r>\frac{d}{2}+k$, it holds $H^{r-l}(D) \subset C^{r-l}(D)$ and $H^{r-l-1}(D) \subset C^{r-l-1}(D)$ for $0\leq l\leq k$. Therefore, $u \in C^k(D\times[0,T])$ and $v \in C^{k-1}(D\times[0,T])$.
\end{proof} 

\subsection{Physics Informed Neural Networks}

We would like to approximate the solutions to  the problem~\eqref{wave} with PINN.
%In this subsection, we will describe physics-informed neural networks (PINN) for approximating solutions of the wave problem \eqref{wave} in the following steps.
%
We seek deep neural networks $u_{\theta} : D\times [0,T] \rightarrow \mathbb{R}$ and $v_{\theta} : D\times [0,T] \rightarrow \mathbb{R}$, parameterized by $\theta \in \Theta$, 
%constituting the weights and biases, 
that approximate the solution $u$ and $v$ of \eqref{wave}. Define residuals,
%. To this end, the key idea behind PINN is to consider pointwise residuals, defined in the setting of the wave equations \eqref{wave} for any sufficiently smooth $u_{\theta} : \Omega \rightarrow \mathbb{R}$ as, 
\begin{subequations}\label{wave_pinn}
	\begin{align}
		\label{wave_pinn_eq1}
		&R_{int1}[u_{\theta},v_{\theta}](\bm{x},t) =u_{\theta t}-v_{\theta},\\
		\label{wave_pinn_eq2}
		&R_{int2}[u_{\theta},v_{\theta}](\bm{x},t) =v_{\theta t}-\Delta u_{\theta} - f,\\
		\label{wave_pinn_eq3}
		&R_{tb1}[u_{\theta}](\bm{x}) =u_{\theta}(\bm{x},0)-\psi_{1}(\bm{x}),\\
		\label{wave_pinn_eq4}
		&R_{tb2}[v_{\theta}](\bm{x}) =v_{\theta}(\bm{x},0)-\psi_{2}(\bm{x}),\\
		\label{wave_pinn_eq5}
		&R_{sb1}[v_{\theta}](\bm{x},t) =v_{\theta}(\bm{x},t)-v_{\theta}(\bm{x}+1,t),\\
		\label{wave_pinn_eq6}
		&R_{sb2}[u_{\theta}](\bm{x},t) =\nabla u_{\theta}(\bm{x},t)-\nabla u_{\theta}(\bm{x}+1,t).
	\end{align}
\end{subequations}
Note that for the exact solution $R_{int1}[u,v]=R_{int2}[u,v]=R_{tb1}[u]=R_{tb2}[v]=R_{sb1}[v]=R_{sb2}[u]=0$. 
Let $\Omega = D\times [0,T]$ and $\Omega_* = \partial D\times [0,T]$ be the space-time domain. 
With PINN, we minimize the the following generalization error,
%Hence, within the PINN algorithm, one seeks to find neural networks $u_{\theta}$ and $v_{\theta}$, for which all residuals are simultaneously minimized, e.g. by minimizing the quantity,
\begin{align}\label{wave_G}
\mathcal{E}_G(\theta)^2&=\int_{\Omega}|R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx+\int_{\Omega}|R_{int2}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx+\int_{\Omega}|\nabla R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx
	\nonumber\\
	&+\int_{D}|R_{tb1}[u_{\theta}](\bm{x})|^2\dx+\int_{D}|R_{tb2}[v_{\theta}](\bm{x})|^2\dx
	+\int_{D}|\nabla R_{tb1}[u_{\theta}](\bm{x})|^2\dx
	\nonumber\\
	&
	+\int_{\Omega_*}|R_{sb1}[v_{\theta}](\bm{x},t)|^2\ds\dt +\int_{\Omega_*}|R_{sb2}[u_{\theta}](\bm{x},t)|^2\ds\dt.
\end{align}
The form of different terms in this expression will become clearer below.

%\subsubsection{Training set} 
To complete the PINN formulation, we will choose the training set $\mathcal{S} \subset \overline{D}\times [0,T]$ based on suitable quadrature points. We divide the full training set $\mathcal{S} = \mathcal{S}_{int} \cup \mathcal{S}_{sb} \cup \mathcal{S}_{tb}$ into the following three components: 
%$\bullet$
\begin{itemize}
    \item Interior training points $\mathcal{S}_{int}=\{{z}_n\}$ for $1\leq n \leq N_{int}$, with each ${z}_n= (\bm{x},t)_n \in D \times(0,T)$.
    
    \item Spatial boundary training points $\mathcal{S}_{sb}=\{{z}_n\}$ for $1\leq n \leq N_{sb}$, with each ${z}_n= (\bm{x},t)_n \in \partial D\times (0,T)$.
    
    \item Temporal boundary training points $\mathcal{S}_{tb}=\{\bm{x}_n\}$ for $1\leq n \leq N_{tb}$ with  each $\bm{x}_n \in D$.
	%\vspace{-0.2em}
\end{itemize}
%We write $\partial D$ for the $(d-1)$-dimensional Lebesgue measure of $\partial D$,  and $|D|$ for the $d$-dimensional Lebesgue measure of $D$.
%
We define the PINN training loss, $\theta \mapsto \mathcal{E}_T(\theta,\mathcal{S})^2$, as follows,
\begin{align}
	\label{wave_T}
	\mathcal{E}_T(\theta,\mathcal{S})^2&
	=\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2	+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2\nonumber\\
	& +\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 +\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 
	+\mathcal{E}_T^{sb1}(\theta,\mathcal{S}_{sb})^2+\mathcal{E}_T^{sb2}(\theta,\mathcal{S}_{sb})^2,
\end{align}
where
\begin{subequations}\label{wave_G_add1}
	\begin{align}
		\label{wave_T1}
		\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|R_{int1}[u_{\theta},v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{wave_T01}
		\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|R_{int2}[u_{\theta},v_{\theta}]](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{wave_T001}
		\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\nabla R_{int1}[u_{\theta},v_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{wave_T2}
		\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{wave_T02}
		\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|R_{tb2}[v_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{wave_T002}
		\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\nabla R_{tb1}[u_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{wave_T3}
		\mathcal{E}_T^{sb1}(\theta,\mathcal{S}_{sb})^2 &= \sum_{n=1}^{N_{sb}}\omega_{sb}^n|R_{sb1}[v_{\theta}](\bm{x}_{sb}^n,t_{sb}^n)|^2,\\
		\label{wave_T03}
		\mathcal{E}_T^{sb2}(\theta,\mathcal{S}_{sb})^2 &= \sum_{n=1}^{N_{sb}}\omega_{sb}^n|R_{sb2}[u_{\theta}](\bm{x}_{sb}^n,t_{sb}^n)|^2.
	\end{align}
\end{subequations}
Here the quadrature points in space-time constitute the data sets $\mathcal{S}_{int} = \{(\bm{x}_{int}^n,t_{int}^n)\}_{n=1}^{N_{int}}$, $\mathcal{S}_{tb} = \{\bm{x}_{tb}^n)\}_{n=1}^{N_{tb}}$ and $\mathcal{S}_{sb} = \{(\bm{x}_{sb}^n,t_{sb}^n)\}_{n=1}^{N_{sb}}$, and $\omega_{\star}^n$ are suitable quadrature weights with $\star$ denoting $int$, $tb$ or $sb$. 

Let 
\[
\hat{u} = u_{\theta}-u, \qquad \hat{v} = v_{\theta}-v,	
\]
denote the difference between the solution to the wave equations and the PINN approximation of the solution.
%with parameter $\theta$. 
We define the total error of the PINN approximation by
\begin{equation}\label{wave_total}
	\mathcal{E}(\theta)^2=\int_0^{T}\int_{D}(|\hat{u}(\bm{x},t)|^2+|\nabla\hat{u}(\bm{x},t)|^2+|\hat{v}(\bm{x},t)|^2)\dx\dt.
\end{equation}


\subsection{Error Analysis}  
%In this subsection, we will give rigorous analysis on the PINN approximations of the solutions of the wave equations.

In light of the wave equations \eqref{wave} and the definitions for  different residuals \eqref{wave_pinn}, we have
%after a straightforward calculation, it holds
\begin{subequations}\label{wave_error}
	\begin{align}
		\label{wave_error_eq1}
		&R_{int1}=\hat{u}_t-\hat{v},\\
		\label{wave_error_eq2}
		&R_{int2}=\hat{v}_t-\Delta \hat{u}\\
		\label{wave_error_eq3}
		&R_{tb1}=\hat{u}(\bm{x},0),\\
		\label{wave_error_eq4}
		&R_{tb2}=\hat{v}(\bm{x},0),\\
		\label{wave_error_eq5}
		&R_{sb1}=\hat{v}(\bm{x},t)-\hat{v}(\bm{x}+1,t),\\
		\label{wave_error_eq6}
		&R_{sb2}=\nabla\hat{u}(\bm{x},t)-\nabla\hat{u}(\bm{x}+1,t).
	\end{align}
\end{subequations}

\subsubsection{Bound on the Residuals}  
\begin{Theorem}\label{sec5_Theorem1} 
	Let $d$, $r$, $k \in \mathbb{N}$ with $k\geq 3$. Let $\psi_1 \in H^{r}(D)$, $\psi_2 \in H^{r-1}(D)$ and $f\in C^{k-1}([0,T];H^{r-1}(D))$ with $r>\frac{d}{2}+k$. For every integer $N>5$, there exist $\tanh$ neural networks $u_{\theta}$ and $v_{\theta}$, each with two hidden layers, of widths at most $3\lceil\frac{k}{2}\rceil|P_{k-1,d+2}| + \lceil NT\rceil+ d(N-1)$ and $3\lceil\frac{d+3}{2}\rceil|P_{d+2,d+2}| \lceil NT\rceil N^d$, such that
	\begin{subequations}
		\begin{align}
			\label{lem5.1}
			&\|R_{int1}\|_{L^2(\Omega)},\|R_{tb1}\|_{L^2(D)}\lesssim {\rm ln}NN^{-k+1},\\
			\label{lem5.2}
			&\|R_{int2}\|_{L^2(\Omega)},\|\nabla R_{int1}\|_{L^2(\Omega)}, \|\nabla R_{tb1}\|_{L^2(D)}, \|R_{sb2}\|_{L^2(\partial D\times [0,t])}\lesssim {\rm ln}^2NN^{-k+2},\\
			\label{lem5.3}
			&\|R_{tb2}\|_{L^2(D)},\|R_{sb1}\|_{L^2(\partial D\times [0,t])}\lesssim {\rm ln}NN^{-k+2}.
		\end{align}
	\end{subequations}
	%Here, $a\lesssim b$ denotes $a\leq Cb$, and the constant $C$ may depend on $k, d, T, u$ and $v$, but not on $N$.
\end{Theorem}
\begin{proof} Based on Lemma \ref{sec5_Lemma2}, it holds that $u\in H^k(\Omega)$ and $v\in H^{k-1}(\Omega)$. In light of Lemma \ref{Ar_4}, there  exists neural networks $u_{\theta}$ and $v_{\theta}$, with the same two hidden layers and widths $3\lceil\frac{k}{2}\rceil|P_{k-1,d+2}| + \lceil NT\rceil+ d(N-1)$ and $3\lceil\frac{d+3}{2}\rceil|P_{d+2,d+2}| \lceil NT\rceil N^d$, such that for every $0 \leq l \leq 2$ and $0 \leq s \leq 2$,
\begin{align}
\label{lem5.1_eq1}
&\|u_{\theta}-u\|_{H^l(\Omega)}\leq C_{l,k,d+1,u}\lambda_{l,u}(N)N^{-k+l},\\
\label{lem5.1_eq2}
&\|v_{\theta}-v\|_{H^s(\Omega)}\leq C_{s,k-1,d+1,v}\lambda_{s,v}(N)N^{-k+1+s},
\end{align}
where $\lambda_{l,u} = 2^l3^{d+1}(1+\sigma){\rm ln}^l\left(\beta_{l,\sigma,d+1,u}N^{d+k+3}\right)$, $\sigma = \frac{1}{100}$, $\lambda_{s,v}=2^s3^{d+1}(1+\sigma){\rm ln}^s\left(\beta_{s,\sigma,d+1,v}N^{d+k+2}\right)$, and the definition for the other constants can be found in Lemma \ref{Ar_4}. %The weights can be bounded by $\mathcal{O}(N^{(d+k)^2})$.

In light of Lemma \ref{Ar_2}, we can bound the PINN residual terms,
	\begin{align*}
&\|\hat{u}_t\|_{L^2(\Omega)}\leq\|\hat{u}\|_{H^1(\Omega)},\qquad \|\hat{v}_t\|_{L^2(\Omega)}\leq\|\hat{v}\|_{H^1(\Omega)},\\
		&\|\Delta\hat{u}\|_{L^2(\Omega)}\leq\|\hat{u}\|_{H^2(\Omega)},\qquad \|\nabla\hat{u}_t\|_{L^2(\Omega)}\leq\|\hat{u}\|_{H^2(\Omega)},\\
		&\|\nabla\hat{v}\|_{L^2(\Omega)}\leq\|\hat{v}\|_{H^1(\Omega)},\\
		&\|\hat{u}\|_{L^2(D)}\leq \|\hat{u}\|_{L^2(\partial\Omega)}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{u}\|_{H^1(\Omega)},\\
		&\|\hat{v}\|_{L^2(D)}\leq \|\hat{v}\|_{L^2(\partial\Omega)}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{v}\|_{H^1(\Omega)},\\
		&\|\nabla\hat{u}\|_{L^2(D)}\leq \|\nabla\hat{u}\|_{L^2(\partial\Omega)}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{u}\|_{H^2(\Omega)},\\
		&\|\hat{v}\|_{L^2(\partial D\times [0,t])}\leq \|\hat{v}\|_{L^2(\partial\Omega)}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{v}\|_{H^1(\Omega)},\\
		&\|\nabla\hat{u}\|_{L^2(\partial D\times [0,t])}\leq \|\nabla\hat{u}\|_{L^2(\partial\Omega)}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{u}\|_{H^2(\Omega)}.
	\end{align*}
	By combining these relations  with \eqref{lem5.1_eq1} and \eqref{lem5.1_eq2}, we can obtain
	% Based on the definition of $\beta$, it should be >1.
	\begin{align*}
&\|R_{int1}\|_{L^2(\Omega)}=\|\hat{u}_t-\hat{v}\|_{L^2(\Omega)}\leq
		\|\hat{u}\|_{H^1(\Omega)}+\|\hat{v}\|_{L^2(\Omega)}\\
		&\qquad\leq C_{1,k,d+1,u}\lambda_{1,u}(N)N^{-k+1} + C_{0,k-1,d+1,v}\lambda_{0,v}(N)N^{-k+1}\lesssim {\rm ln}NN^{-k+1},\\
		&\|R_{int2}\|_{L^2(\Omega)}=\|\hat{v}_t-\Delta\hat{u}\|_{L^2(\Omega)}\leq
		\|\hat{v}\|_{H^1(\Omega)}+\|\hat{u}\|_{H^2(\Omega)}\\
		&\qquad\leq C_{2,k,d+1,u}\lambda_{2,u}(N)N^{-k+2} + C_{1,k-1,d+1,v}\lambda_{1,v}(N)N^{-k+2}\lesssim {\rm ln}^2NN^{-k+2},\\		
		&\|\nabla R_{int1}\|_{L^2(\Omega)}=\|\nabla(\hat{u}_t-\hat{v})\|_{L^2(\Omega)}\leq
		\|\hat{u}\|_{H^2(\Omega)}+\|\hat{v}\|_{H^1(\Omega)}\\
		&\qquad\leq C_{2,k,d+1,u}\lambda_{2,u}(N)N^{-k+2} + C_{1,k-1,d+1,v}\lambda_{1,v}(N)N^{-k+2}\lesssim {\rm ln}^2NN^{-k+2},\\
		&\|R_{tb1}\|_{L^2(D)}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{u}\|_{H^1(\Omega)}\lesssim {\rm ln}NN^{-k+1},\\	
		&\|R_{tb2}\|_{L^2(D)}, \|R_{sb1}\|_{L^2(\partial D\times [0,t])}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{v}\|_{H^1(\Omega)}\lesssim {\rm ln}NN^{-k+2},\\	
		&\|\nabla R_{tb1}\|_{L^2(D)}, \|R_{sb2}\|_{L^2(\partial D\times [0,t])}\leq C_{h_{\Omega},d+1,\rho_{\Omega}}\|\hat{u}\|_{H^2(\Omega)}\lesssim {\rm ln}^2NN^{-k+2}.
	\end{align*}
   %Similarly, we can get other terms and finish the proof.
\end{proof}

Theorem \ref{sec5_Theorem1} implies that one can make the PINN residuals~\eqref{wave_pinn} arbitrarily small by choosing $N$ to be sufficiently large. It follows that the generalization error $\mathcal{E}_G(\theta)^2$ in~\eqref{wave_G} can be made arbitrarily small.  

\subsubsection{Bounds on the Total Approximation Error} 

We next show that the total error $\mathcal{E}(\theta)^2$ is also small when the generalization error $\mathcal{E}_G(\theta)^2$ is small with the PINN approximation $(u_{\theta},v_{\theta})$. 
%According to the difference between the generalization error and the training error can be made arbitrarily small, 
Then we prove that the total error $\mathcal{E}(\theta)^2$ can be arbitrarily small, provided that the training error $\mathcal{E}_T(\theta,\mathcal{S})^2$ is sufficiently small  and the sample set is sufficiently large. 

\begin{Theorem}\label{sec5_Theorem2} Let $d\in \mathbb{N}$, $u\in C^1(\Omega)$ and $v\in C^0(\Omega)$ be the classical solution to the wave equations \eqref{wave}. Let $u_{\theta}$ and $v_{\theta}$ denote the PINN approximation with parameter $\theta$. Then the following relation holds,
%the total error is bounded as follows,
	\begin{equation}\label{lem5.4}
			\mathcal{E}(\theta)^2=\int_0^{T}\int_{D}(|\hat{u}(\bm{x},t)|^2+|\nabla\hat{u}(\bm{x},\tau)|^2+|\hat{v}(\bm{x},t)|^2)\dx\dt\leq C_GT\exp(2T),
	\end{equation}
	where 
	\begin{align*}
		C_G&=\int_{D}(|R_{tb1}|^2+|R_{tb2}|^2+|\nabla R_{tb1}|^2)\dx +  \int_{0}^{T}\int_{D}(|R_{int1}|^2+|R_{int2}|^2+|\nabla R_{int1}|^2)\dx\dt
		\\
		&\quad+
		\int_{0}^{T}\int_{\partial D}(|R_{sb1}|^2+|R_{sb2}|^2)\ds\dt.
	\end{align*}
%and $C_{\partial D}=|\partial D|^{\frac{1}{2}}(\|u\|_{C^1(\partial D\times[0,t])}+||u_{\theta}||_{C^1(\partial D\times[0,t])})$.
\end{Theorem}
\begin{proof} By taking the inner product of \eqref{wave_error_eq1} and \eqref{wave_error_eq2} with $\hat{u}$ and $\hat{v}$ and integrating over $D$, respectively, we have
	\begin{align}
		\label{sec5_eq0}
		\frac{d}{2dt}\int_{D} |\hat{u}|^2\dx &= \int_{D}\hat{u}\hat{v}\dx+\int_{D} R_{int1}\hat{u}\dx\leq \int_{D} |\hat{u}|^2\dx+\frac{1}{2}\int_{D} |R_{int1}|^2\dx+\frac{1}{2}\int_{D} |\hat{v}|^2\dx,\\
		\label{sec5_eq1}
		\frac{d}{2dt}\int_{D} |\hat{v}|^2\dx &=- \int_{D}\nabla\hat{u}\cdot\nabla\hat{v}\dx+\int_{\partial D} \hat{v}\nabla\hat{u}\cdot\bm{n}\ds+\int_{D} R_{int2}\hat{v}\dx
		\nonumber\\
		&=- \int_{D}\nabla\hat{u}\cdot\nabla\hat{u}_t\dx
		+\int_{D}\nabla\hat{u}\cdot\nabla R_{int1}\dx
		+\int_{\partial D} \hat{v}\nabla\hat{u}\cdot\bm{n}\ds
		+\int_{D} R_{int2}\hat{v}\dx
		\nonumber\\
		&=-\frac{d}{2dt}\int_{D} |\nabla\hat{u}|^2\dx 
		+\int_{D}\nabla\hat{u}\cdot\nabla R_{int1}\dx
		+\int_{\partial D} R_{sb1}R_{sb2}\cdot\bm{n}\ds
		+\int_{D} R_{int2}\hat{v}\dx
		\nonumber\\
		&\leq-\frac{d}{2dt}\int_{D} |\nabla\hat{u}|^2\dx +\frac{1}{2}\int_{D} |\nabla\hat{u}|^2\dx+\frac{1}{2}\int_{D} |\nabla R_{int1}|^2\dx
		\nonumber\\
		&\qquad+\frac{1}{2}\int_{\partial D}(|R_{sb1}|^2+|R_{sb2}|^2)\ds+\frac{1}{2}\int_{D} |\hat{v}|^2\dx+\frac{1}{2}\int_{D} |R_{int2}|^2\dx.
	\end{align}
	Here, we have used $\hat{v}=\hat{u}_t-R_{int1}$. 
 
	By adding \eqref{sec5_eq0} to \eqref{sec5_eq1}, we have
	\begin{align}\label{sec5_eq2}
		&\frac{d}{2dt}\int_{D} |\hat{u}|^2\dx+\frac{d}{2dt}\int_{D} |\nabla\hat{u}|^2\dx +\frac{d}{2dt}\int_{D} |\hat{v}|^2\dx
		\nonumber\\
		&\qquad \leq \int_{D} |\hat{u}|^2\dx+\frac{1}{2}\int_{D} |\nabla\hat{u}|^2\dx+\int_{D} |\hat{v}|^2\dx+\frac{1}{2}\int_{D} |R_{int1}|^2\dx
		\nonumber\\
		&\qquad+\frac{1}{2}\int_{D} |R_{int2}|^2\dx+\frac{1}{2}\int_{D} |\nabla R_{int1}|^2\dx+\frac{1}{2}\int_{\partial D}(|R_{sb1}|^2+|R_{sb2}|^2)\ds.
	\end{align}
 %{\color{red}[Yanxia, in the above inequality, on the right hand side of $\leq$, are the coefficients $\frac12$ or $\frac32$ for the terms $\nabla\hat{u}$ and $\hat{v}$?]}\\
	Integrating \eqref{sec5_eq2} over $[0,
	\tau]$ for any $\tau \leq T$ and applying the Cauchy–Schwarz inequality, we obtain
	\begin{align*}
		&\int_{D} |\hat{u}(\bm{x},\tau)|^2\dx 
		+\int_{D} |\nabla\hat{u}(\bm{x},\tau)|^2\dx
		+\int_{D} |\hat{v}(\bm{x},\tau)|^2\dx\\
		&\qquad\leq\int_{D}|R_{tb1}|^2\dx +\int_{D}|R_{tb2}|^2\dx+\int_{D}|\nabla R_{tb1}|^2\dx
		+2\int_{0}^{\tau}\int_{D} \left(|\hat{u}|^2+ |\nabla\hat{u}|^2+ |\hat{v}|^2 \right)\dx\dt
		\\
		&\qquad+ \int_{0}^{T}\int_{D}\left(|R_{int1}|^2+|R_{int2}|^2+|\nabla R_{int1}|^2\right)\dx\dt+\int_{0}^{T}\int_{\partial D}(|R_{sb1}|^2+|R_{sb2}|^2)\ds\dt.
	\end{align*}
%{\color{red}[Yanxia, the last line of the above equation, is the intregral from 0 to $\tau$ or to $T$?]}

 We apply the integral form of the Gr${\rm\ddot{o}}$nwall inequality to the above inequality to get
	\[
	\int_{D} \left(|\hat{u}(\bm{x},\tau)|^2+|\nabla\hat{u}(\bm{x},\tau)|^2+|\hat{v}(\bm{x},\tau)|^2\right)\dx\leq C_G\exp(2T),
	\]
	where
	\begin{align*}
		C_G&=\int_{D}(|R_{tb1}|^2+|R_{tb2}|^2+|\nabla R_{tb1}|^2)\dx +  \int_{0}^{T}\int_{D}(|R_{int1}|^2+|R_{int2}|^2+|\nabla R_{int1}|^2)\dx\dt
		\\
		&\qquad+\int_{0}^{T}\int_{\partial D}(|R_{sb1}|^2+|R_{sb2}|^2)\ds\dt.
	\end{align*}
	Then, we integrate the above inequality over $[0,T]$ to yield \eqref{lem5.4}.
\end{proof} 

\begin{Remark}\label{sec5_Remark1} 
%In fact, 
For the wave equations~\eqref{wave} with periodic boundary, we would like to mention below two other forms for the generalization error (and the related training loss). Compared with \eqref{wave_G}, they  differ only on the spatial boundary $\Omega_{*}$, i.e.,
	\begin{align}\label{wave_G1}
		\mathcal{E}_G(\theta)^2&=\int_{\Omega}|R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|R_{int2}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|\nabla R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt
		\nonumber\\
		&+\int_{D}|R_{tb1}[u_{\theta}](\bm{x})|^2\dx+\int_{D}|R_{tb2}[v_{\theta}](\bm{x})|^2\dx
		+\int_{D}|\nabla R_{tb1}[u_{\theta}](\bm{x})|^2\dx
		\nonumber\\
		&
		+\left(\int_{\Omega_*}|R_{sb1}[v_{\theta}](\bm{x},t)|^2\ds\dt\right)^{\frac{1}{2}},
	\end{align}
	and
	\begin{align}\label{wave_G2}
		\mathcal{E}_G(\theta)^2&=\int_{\Omega}|R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|R_{int2}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|\nabla R_{int1}[u_{\theta},v_{\theta}](\bm{x},t)|^2\dx\dt
		\nonumber\\
		&+\int_{D}|R_{tb1}[u_{\theta}](\bm{x})|^2\dx+\int_{D}|R_{tb2}[v_{\theta}](\bm{x})|^2\dx
		+\int_{D}|\nabla R_{tb1}[u_{\theta}](\bm{x})|^2\dx
		\nonumber\\
		&
		+\left(\int_{\Omega_*}|R_{sb2}[u_{\theta}](\bm{x},t)|^2\ds\dt\right)^{\frac{1}{2}}.
	\end{align}
	The related training loss functions are given by
	\begin{align}\label{wave_TT1}
		\mathcal{E}_T(\theta,\mathcal{S})^2&
		=\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2	+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2
		\nonumber\\
		& +\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 +\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 
		+\mathcal{E}_T^{sb1}(\theta,\mathcal{S}_{sb}),
	\end{align}
	or
	\begin{align}\label{wave_TT2}
		\mathcal{E}_T(\theta,\mathcal{S})^2&
		=\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2	+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2
		\nonumber\\
		& +\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 +\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2+\mathcal{E}_T^{sb2}(\theta,\mathcal{S}_{sb}).
	\end{align}
	
 These three forms for the generalization error result from different treatments of the boundary term $\int_{\partial D}\hat{v}\nabla\hat{u}\cdot\bm{n}$ in the proof of Theorem~\ref{sec5_Theorem2}:
	\begin{align*}
		&\int_{\partial D} \hat{v}\nabla\hat{u}\cdot\bm{n}\ds=\int_{\partial D} R_{sb1}\nabla\hat{u}\cdot\bm{n}\ds\leq |\partial D|^{\frac{1}{2}}(\|u\|_{C^1(\partial D\times[0,t])}+||u_{\theta}||_{C^1(\partial D\times[0,t])})\left(\int_{\partial D}|R_{sb1}|^2\ds\right)^{\frac{1}{2}},\\
		&\int_{\partial D} \hat{v}\nabla\hat{u}\cdot\bm{n}\ds=\int_{\partial D} \hat{v}R_{sb2}\cdot\bm{n}\ds\leq |\partial D|^{\frac{1}{2}}(\|v\|_{C^0(\partial D\times[0,t])}+||v_{\theta}||_{C^0(\partial D\times[0,t])})\left(\int_{\partial D}|R_{sb2}|^2\ds\right)^{\frac{1}{2}},\\
		&\int_{\partial D} \hat{v}\nabla\hat{u}\cdot\bm{n}\ds=\int_{\partial D} R_{sb1}R_{sb2}\cdot\bm{n}\ds\leq \frac{1}{2}\left(\int_{\partial D} |R_{sb1}|^2\ds+\int_{\partial D}|R_{sb2}|^2\ds\right).
	\end{align*}
	Our numerical experiments indicate that 
 %However, we can get the poor result in the numerical tests when 
 adopting the training loss \eqref{wave_TT1} or \eqref{wave_TT2} seems to lead to poorer simulation results. 
 %According to the nature of the periodic boundary, 
 For the periodic boundary, both terms $R_{sb1}$ and $R_{sb2}$ may be needed for the periodicity information.
 %can ensure the boundary information, 
 We suspect that this may be why only a single boundary term ($R_{sb1}$ or $R_{sb2}$), as given by~\eqref{wave_TT1} and~\eqref{wave_TT2}, leads to poorer numerical results.
\end{Remark}

\begin{Theorem}\label{sec5_Theorem3} Let $d\in \mathbb{N}$ and $T>0$. Let $u\in C^4(\Omega)$ and $v\in C^3(\Omega)$ be the classical solution of
	the wave equations \eqref{wave},  and let $(u_{\theta},v_{\theta})$ denote the PINN approximation with parameter $\theta \in \Theta$. Then the total error satisfies
	\begin{align}\label{lem5.5}
		&\int_0^{T}\int_{D}(|\hat{u}(\bm{x},t)|^2+|\nabla \hat{u}(\bm{x},t)|^2+|\hat{v}(\bm{x},t)|^2)\dx\dt\leq C_TT\exp(2T)
		\nonumber\\
		&\qquad=\mathcal{O}(\mathcal{E}_T(\theta,\mathcal{S})^2 + M_{int}^{-\frac{2}{d+1}} +M_{tb}^{-\frac{2}{d}}+M_{sb}^{-\frac{2}{d}}).
	\end{align}
	The constant $C_T$ is defined as
	\begin{align*}
		C_T =&C_{({R_{tb1}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb1}^2)+C_{({R_{tb2}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb2}^2)+C_{(|\nabla R_{tb1}|^2)}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(|\nabla R_{tb1}|^2)\\
		&+C_{({R_{int1}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int1}^2)+C_{(R_{int2}^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int2}^2)+C_{(|\nabla R_{int1}|^2)}M_{int}^{-\frac{2}{d+1}}\\
		&+\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla R_{int1}|^2)+C_{({R_{sb1}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb1}^2)+C_{({R_{sb2}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb2}^2),
	\end{align*}
where
\begin{align*}
	&C_{({R_{tb1}^2})}\lesssim\|\hat{u}\|_{C^2}^2, \quad C_{({R_{tb2}^2})}\lesssim \|\hat{v}\|_{C^2}^2, \quad C_{(|\nabla R_{tb1}|^2)}\lesssim \|\hat{u}\|_{C^3}^2, \quad C_{({R_{int1}^2})}\lesssim \|\hat{u}\|_{C^3}^2+\|\hat{u}\|_{C^2}^2,\\
	&\qquad C_{(R_{int2}^2)}, C_{(|\nabla R_{int1}|^2)}\lesssim \|\hat{u}\|_{C^4}^2+\|\hat{v}\|_{C^3}^2, \quad C_{({R_{sb1}^2})}\lesssim \|\hat{v}\|_{C^3}^2,\quad C_{({R_{sb2}^2})}\lesssim\|\hat{u}\|_{C^4}^2,
\end{align*}
and the bounds $\|u_{\theta}\|_{C^n}$ and $\|v_{\theta}\|_{C^n}$ ($n \in \mathbb{N}$) are given by Lemma \ref{Ar_3}. 
\end{Theorem}
\begin{proof} By combining Theorem \ref{sec5_Theorem2} with the quadrature error formula \eqref{int1}, we have
%can get the main error estimate of the theorem. 
	\begin{align*}
		\int_{D}|R_{tb1}|^2\dx&=\int_{D}|R_{tb1}|^2\dx-\mathcal{Q}_{M_{tb}}^{D}(R_{tb1}^2)+\mathcal{Q}_{M_{tb}}^{D}(R_{tb1}^2)\\
		&\leq C_{({R_{tb1}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb1}^2),\\
		\int_{D}|R_{tb2}|^2\dx&=\int_{D}|R_{tb2}|^2\dx-\mathcal{Q}_{M_{tb}}^{D}(R_{tb2}^2)+\mathcal{Q}_{M_{tb}}^{D}(R_{tb2}^2)\\
		&\leq C_{({R_{tb2}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb2}^2),\\
		\int_{D}|\nabla R_{tb1}|^2\dx&=\int_{D}|\nabla R_{tb1}|^2\dx-\mathcal{Q}_{M_{tb}}^{D}(|\nabla R_{tb1}|^2)+\mathcal{Q}_{M_{tb}}^{D}(|\nabla R_{tb1}|^2)\\
		&\leq C_{(|\nabla R_{tb1}|^2)}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(|\nabla R_{tb1}|^2),\\
		\int_{\Omega}|R_{int1}|^2\dx\dt&=\int_{\Omega}|R_{int1}|^2\dx\dt-\mathcal{Q}_{M_{int}}^{\Omega}(R_{int1}^2)+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int1}^2)\\
		&\leq C_{({R_{int1}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int1}^2),\\
		\int_{\Omega}|R_{int2}|^2\dx\dt&=\int_{\Omega}|R_{int2}|^2\dx\dt-\mathcal{Q}_{M_{int}}^{\Omega}(R_{int2}^2)+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int2}^2)\\
		&\leq C_{(R_{int2}^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int2}^2),\\
		\int_{\Omega}|\nabla R_{int1}|^2\dx\dt&=\int_{\Omega}|\nabla R_{int1}|^2\dx\dt-\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla R_{int1}|^2)+\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla R_{int1}|^2)\\
		&\leq C_{(|\nabla R_{int1}|^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla R_{int1}|^2),\\
		\int_{\Omega_*}|R_{sb1}|^2\ds\dt&=\int_{\Omega_*}|R_{sb1}|^2\ds\dt-\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb1}^2)+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb1}^2)\\
		&\leq C_{({R_{sb1}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb1}^2),\\
		\int_{\Omega_*}|R_{sb2}|^2\ds\dt&=\int_{\Omega_*}|R_{sb2}|^2\ds\dt-\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb2}^2)+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb2}^2)\\
		&\leq C_{({R_{sb2}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb2}^2).
	\end{align*}
	By the above inequalities and \eqref{lem5.4}, it holds that
	\begin{equation*}
		\int_0^{T}\int_{D}(|\hat{u}(\bm{x},t)|^2+|\nabla \hat{u}(\bm{x},t)|^2+|\hat{v}(\bm{x},t)|^2)\dx\dt\leq  C_TT\exp(2T),
	\end{equation*}
	where 
\begin{align*}
	C_T =&C_{({R_{tb1}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb1}^2)+C_{({R_{tb2}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(R_{tb2}^2)+C_{(|\nabla R_{tb1}|^2)}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(|\nabla R_{tb1}|^2)\\
	&+C_{({R_{int1}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int1}^2)+C_{(R_{int2}^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(R_{int2}^2)+C_{(|\nabla R_{int1}|^2)}M_{int}^{-\frac{2}{d+1}}\\
	&+\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla R_{int1}|^2)+C_{({R_{sb1}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb1}^2)+C_{({R_{sb2}^2})}M_{sb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb}}^{\Omega_*}(R_{sb2}^2).
\end{align*}
The complexities of the constants $C_{({R_{q}^2})}$ are given by Lemma \ref{Ar_3}, and we observe that for every residual $R_q$, it holds that $\|R_q^2\|_{C^n}\leq 2^n\|R_q\|_{C^n}^2$ ($n \in \mathbb{N}$) for $R_q=R_{tb1}$, $R_{tb2}$, $\nabla R_{tb1}$, $R_{int1}$, $R_{int2}$, $\nabla R_{int1}$ and $R_{sb2}$. 
\end{proof} 
