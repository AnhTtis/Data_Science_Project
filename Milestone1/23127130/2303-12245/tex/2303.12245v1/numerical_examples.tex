\section{Numerical Examples}\label{numerical examples}

The theoretical analyses from Sections~\ref{Wave} to \ref{Elasto-dynamics} suggest several forms for the PINN loss function with the wave, Sine-Gordon and the linear elastodynamic equations. These forms contain certain non-standard terms, such as the square root of the residuals or the gradient terms on some boundaries, which would generally be  absent from the canonical PINN formulation of the loss function. The presence of such non-standard terms is crucial to bounding the PINN approximation errors, as shown in the error analyses.

These non-standard forms of the loss function lead to a variant PINN algorithm. In this section we illustrate the performance of the variant PINN algorithm as suggested by the theoretical analysis, as well as the more standard PINN algorithm, using several numerical examples in one spatial dimension (1D) plus time for the wave equation and the Sine-Gordon equation, and in two spatial dimensions (2D) plus time for the linear elastodynamic equation.
%{\color{red} \#[Color blue is the original, Color brown is the updated.]}

%some numerical examples to test our constructed numerical schemes. 
%We first give some settings for training neural networks. 
The following settings are common to all the numerical simulations in this section. Let $(\bm x, t )\in D\times[0,T] $ denote the spatial and temporal coordinates in the spatial-temporal domain, where $ \bm x = x $ and $ \bm x = (x, y) $ for one and two spatial dimensions, respectively. For the wave equation and the Sine-Gordon equation, the neural networks contain two nodes in the input layer (representing $x$ and $t$), two hidden layers with the number of nodes to be specified later, and two nodes in the output layer (representing the solution $u$ and its time detivative $v=\frac{\partial u}{\partial t}$). For the linear elastodynamic equaton, three input nodes and four output nodes are employed in the neural network, as will be explained in more detail later.
We employ the $\tanh$ (hyperbolic tangent) activation function for all the hidden nodes, and no activation function is applied to the output nodes (i.e.~linear).
%The network architecture is employed with two hidden layers with $ 80 $ neurons in each layer and the hyperbolic tangent activation function. 
For training the neural networks, we employ $ N $ collocation points within the spatial-temporal domain drawn from a uniform random distribution, and also $N$ uniform random points on  each spatial boundary and on the initial boundary. In the simulations the value of $ N $ is varied systematically among $ 1000 $, $ 1500 $, $ 2000 $, $ 2500 $ and $ 3000 $. After the neural networks are trained, for the wave equation and the Sine-Gordon equation, we compare the PINN solution and the exact solution on a set of $N_{ev} =3000 \times 3000 $ uniform spatial-temporal grid points (evaluation points) $(x, t)_n\in D\times[0,T] $ ($n=1,\cdots, N_{ev} $) that covers the problem domain and the boundaries. For the elastodynamic equation, we compare the PINN solution and the exact solution at different time instants, and at each time instant the corresponding solutions are evaluated at a uniform set of $ N_{ev} =1500 \times 1500 $  grid points in the spatial domain, $ \bm x_n = (x, y)_n\in D $ ($n=1,\cdots, N_{ev} $).

The PINN errors reported below are computed as follows. Let $ z_n = (\bm x, t)_n $ ($(\bm x, t)_n\in D\times[0,T], n=1,\cdots, N_{ev} $) denote the set of uniform grid  points, where $ N_{ev} $ denote the number of evaluation points. The errors of PINN are defined by,
\begin{subequations}\label{PINN_num_eq1}
	\begin{align}\label{PINN_num_eq1_1}
		&l_2\text{-error} = \frac{\sqrt{\sum_{n=1}^{N_{ev}} |u(z_n) - u_{\theta}(z_n)|^2}}{\sqrt{\sum_{n=1}^{N_{ev}} u(z_n)^2}} =  \frac{\sqrt{\left(\sum_{n=1}^{N_{ev}} |u(z_n) - u_{\theta}(z_n)|^2\right)/N_{ev}}}{\sqrt{\left(\sum_{n=1}^{N_{ev}} u(z_n)^2\right)/N_{ev}}}, \\ 
		\label{PINN_num_eq1_2}
		&l_\infty\text{-error}= \frac{\max\{|u(z_n) - u_{\theta}(z_n)|\}_{n=1}^{N_{ev}} }{\sqrt{\left(\sum_{n=1}^{N_{ev}} u(z_n)^2\right)/N_{ev}}},
	\end{align}
\end{subequations}
where $ u_\theta $ denotes the PINN solution and $u$ denotes the exact solution. 

Our implementation of the PINN algorithm is based on the PyTorch library (\href{https://pytorch.org/}{pytorch.org}). In all the following  numerical examples, we combine the Adam \cite{kingma2014adam} optimizer and the L-BFGS \cite{2006_NumericalOptimization} optimizer (in batch mode) to train the neural network. We first employ the Adam optimizer to train the network for 100 epochs/iterations, and then employ the L-BFGS optimizer to continue the network training for another 30000 iterations. 
%full batch with Adam optimizer for the first 100 number of iterations, followed by L-BFGS optimizer for another 30000 iterations. 
We employ the default parameter values in Adam, with the learning rate $0.001$,  $\beta_1=0.9$ and $\beta_2=0.99$. The initial learning rate $1.0$ is adopted in the L-BFGS optimizer.

\subsection{Wave Equation}

\begin{figure}[tb]
	%\centering
	%\begin{minipage}[t]{0.3\linewidth}
	\centerline{
	\subfloat[True solution for $ u $]{\includegraphics[width=0.25\linewidth]{./figures/Wave_2000_u.pdf}}\hspace{0.2em}
	\subfloat[PINN solution $ u_\theta $]{\includegraphics[width=0.25\linewidth]{./figures/Wave_2000_uh.pdf}}\hspace{0.2em}
	\subfloat[{PINN} {absolute} error for $u$]{\includegraphics[width=0.25\linewidth]{./figures/Wave_2000_uerr.pdf}}
 }
	\centerline{
	\subfloat[True solution for $ v $]{\includegraphics[width=0.25\linewidth]{./figures/Wave_2000_v.pdf}}\hspace{0.2em}
	\subfloat[PINN solution $ v_\theta $]{\includegraphics[width=0.25\linewidth]{./figures/Wave_2000_vh.pdf}}\hspace{0.2em}
	\subfloat[{PINN} {absolute} error for $v$]{\includegraphics[width=0.25\linewidth]{./figures/Wave_2000_verr.pdf}}
 }
	\caption{ Wave equation: Distributions of the True solutions, the PINN solutions and the PINN point-wise absolute errors for $u$ and $v$ in the spatial-temporal domain. $N=2000$ training points within the domain and on each of the domain boundaries.
 %(2000 training points)
 }
	\label{PINN_partpaper_Wave_fig1_1}
	%\end{minipage}
\end{figure}


We next test the PINN algorithm for solving the wave equation \eqref{wave} in one spatial dimension (plus time), under a configuration in accordance with that of~\cite{2021_JCP_Dong_modifiedbatch}. Consider the spatial-temporal domain, $ (x,t)\in D\times[0, T] = [0, 5] \times [0, 2] $, and the initial-boundary value problem with the wave equation on this domain,
\begin{subequations}\label{part_paper_wave_eq1}
	\begin{align}
		&\frac{\partial^2 u}{\partial t^2} - c^2 \frac{\partial^2 u}{\partial x^2} = 0, \\
		& u(0, t) = u(5, t), \qquad
		\frac{\partial u}{\partial x} (0, t) = \frac{\partial u}{\partial x} (5,t), \\
		& u(x,0) = 2\, {\rm sech}^3 \left(\frac{3}{\delta_0}(x-x_0)\right),\qquad
		\frac{\partial u}{\partial t}(x,0) = 0,
	\end{align}
\end{subequations}
where $u(x,t)$ is the wave field  to be solved for, $ c $ is the wave speed, $ x_0 $ is the initial peak location of the wave, $ \delta_0 $ is a constant that controls the width of the wave profile, and the periodic boundary conditions are imposed on $ x=0 $ and $ 5 $. In the simulations, we employ $ c=2 $, $ \delta_0=2 $, and $ x_0 = 3 $. Then the above problem has the  solution,
\begin{equation*}\left\{
\begin{split}
	&u(x, t) = {\rm sech}^3\left(\frac{3}{\delta_0}\left(-2.5 +\xi\right)\right) + {\rm sech}^3\left(\frac{3}{\delta_0}\left(-2.5 +\eta\right)\right),\\
	& \xi = {\rm mod}\left(x-x_0 + ct + 2.5, 5\right),\quad
	\eta = {\rm mod}\left(x-x_0 - ct + 2.5, 5\right),
 \end{split}
 \right.
\end{equation*}
where mod refers to the modulo operation. The two terms in $ u(x,t) $ represent the leftward- and rightward-traveling waves, respectively. 

We reformulate the problem~\eqref{part_paper_wave_eq1} into the following system,
\begin{subequations}\label{eq_62}
\begin{align}
&
u_t - v=0,\qquad
v_t -c^2u_{xx}=0, \label{eq_62a}
\\
& u(0,t) = u(5,t), \qquad
u_x(0,t) = u_x(5,t), \\
& u(x,0) = 2\, {\rm sech}^3 \left(\frac{3}{\delta_0}(x-x_0)\right),\qquad
		v(x, 0) = 0,
\end{align}
\end{subequations}
where $v(x,t)$ is an auxiliary field given by the first equation in~\eqref{eq_62a}.


%%%%%%%%%%%%%%%%%%%%%%

To solve the system \eqref{eq_62} with PINN, we employ $ 90 $ and $ 60 $ neurons in the first and the second hidden layers of neural networks, respectively. 
%in each layer and the hyperbolic tangent activation function. 
We employ the following loss function in PINN in light of~\eqref{wave_T},
\begin{align}\label{eq_63}
	\text{Loss}
 %\mathcal{E}_T(\theta,\mathcal{S})^2
	=& \frac{W_1}{N} \sum_{n=1}^{N}\left[u_{\theta t}(x_{int}^n, t_{int}^n) - v_{\theta}(x_{int}^n, t_{int}^n)\right]^2 \notag \\
 &+ \frac{W_2}{N} \sum_{n=1}^{N} \left[v_{\theta t}(x_{int}^n, t_{int}^n) - u_{\theta xx}(x_{int}^n, t_{int}^n)\right]^2 \nonumber \\
	&+ \frac{W_3}{N} \sum_{n=1}^{N}\left[u_{\theta tx}(x_{int}^n, t_{int}^n) - v_{\theta x}(x_{int}^n, t_{int}^n)\right]^2 \notag \\
 &+ \frac{W_4}{N} \sum_{n=1}^{N}\left[ u_{\theta}(x_{tb}^n, 0) - 2\, {\rm sech}^3 \left(\frac{3}{\delta}_0(x_{tb}^n-x_0)\right)\right]^2 \nonumber \\
	&+ \frac{W_5}{N} \sum_{n=1}^{N} \left[v_{\theta}(x_{tb}^n, 0)\right]^2 +  \frac{W_6}{N} \sum_{n=1}^{N}\left[u_{\theta x}(x_{tb}^n, 0) + \frac{18 \sinh((3x_{tb}^n - 3x_0)/\delta_0)}{\delta_0\cosh^4((3x_{tb}^n- 3x_0)/\delta_0)}\right]^2 \nonumber \\ 
	& + \frac{W_7}{N} \sum_{n=1}^{N} \left[v_{\theta}(0, t_{sb}^n) - v_{\theta}(5, t_{sb}^n)\right]^2 
 + \frac{W_8}{N} \sum_{n=1}^{N} \left[u_{\theta x}(0, t_{sb}^n) - u_{\theta x}(5, t_{sb}^n)\right]^2.
\end{align}
Note that in the simulations we have employed the same number of collocation points ($N$) within the domain and on each of the domain boundaries. The above loss function differs slightly from the one in the error analysis~\eqref{wave_T}, in several aspects.
First, we have added a set of penalty coefficients $W_n>0$ ($1\leq n\leq 8$) for different loss terms in numerical simulations.
Second, the collocation points used in simulations (e.g.~$x_{int}^n$, $t_{int}^n$, $x_{sb}^n$, $t_{sb}^n$, $x_{tb}^n$) are generated randomly within the domain or on the domain boundaries from a uniform distribution. In addition, the averaging used here do not exactly correspond 
to the numerical quadrature rule (mid-point rule) used in the
theoretical analysis.



We have also considered another form (given below) for the loss function, as suggested by an alternate analysis as discussed in Remark~\ref{sec5_Remark1} (see equation~\eqref{wave_TT1}),
\begin{align}\label{eq_64}
	\text{Loss}
 %\mathcal{E}_T(\theta,\mathcal{S})^2
	=& \frac{W_1}{N} \sum_{n=1}^{N}\left[u_{\theta t}(x_{int}^n, t_{int}^n) - v_{\theta}(x_{int}^n, t_{int}^n)\right]^2 \notag \\
 &+ \frac{W_2}{N} \sum_{n=1}^{N} \left[v_{\theta t}(x_{int}^n, t_{int}^n) - u_{\theta xx}(x_{int}^n, t_{int}^n)\right]^2 \nonumber \\
	&+ \frac{W_3}{N} \sum_{n=1}^{N}\left[u_{\theta tx}(x_{int}^n, t_{int}^n) - v_{\theta x}(x_{int}^n, t_{int}^n)\right]^2 \notag \\
 &+ \frac{W_4}{N} \sum_{n=1}^{N}\left[u_{\theta}(x_{tb}^n, 0) - 2\, {\rm sech}^3 \left(\frac{3}{\delta}_0(x_{tb}^n-x_0)\right)\right]^2 \nonumber \\
	&+ \frac{W_5}{N} \sum_{n=1}^{N} \left[v_{\theta}(x_{tb}^n, 0)\right]^2 +  \frac{W_6}{N} \sum_{n=1}^{N}\left[u_{\theta x}(x_{tb}^n, 0) + \frac{18 \sinh((3x_{tb}^n - 3x_0)/\delta_0)}{\delta_0\cosh^4((3x_{tb}^n- 3x_0)/\delta_0)}\right]^2 \nonumber \\ 
	& + \frac{W_7}{N} \sum_{n=1}^{N} \left|v_{\theta}(0, t_{sb}^n) - v_{\theta}(5, t_{sb}^n)\right| 
 + \frac{W_8}{N} \sum_{n=1}^{N} \left|u_{\theta x}(0, t_{sb}^n) - u_{\theta x}(5, t_{sb}^n)\right|.
\end{align}
The difference between this form and the form~\eqref{eq_63} lies in the last two terms, with the terms here not squared.

The loss function~\eqref{eq_63} will be referred to as the loss form \#1 in subsequent discussions, and~\eqref{eq_64} will be referred to as the loss form \#2.
The PINN schemes that employ these two different loss forms will be referred to as
PINN-F1 and PINN-F2, respectively.



%Taking $ N=2000 $ in the spatial-temporal plane, the 
Figure \ref{PINN_partpaper_Wave_fig1_1} shows distributions of the exact solutions, the PINN solutions, and the PINN point-wise {absolute} errors for $u$ and $v$ in the spatial-temporal domain. 
Here the PINN solution is computed by PINN-F1, %based on the loss function~\eqref{eq_63}, 
in which penalty coefficients are given by
$\bm{W}=(W_1,\dots,W_8)= (0.8,0.8,0.8,0.5,0.5,0.5,0.9,0.9) $.
One can observe that the  method has captured the wave fields for $u$ and $\frac{\partial u}{\partial t}$ reasonably well, with the error for $u$ notably smaller than that of $\frac{\partial u}{\partial t}$.


\begin{figure}[tb]
	\centering
	\subfloat[$ t=0.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/Wave_2000_u_t_slice_0.pdf}\\
			\includegraphics[scale=0.25]{./figures/Wave_2000_u_t_slice_error_0.pdf}
		\end{minipage}
  	}\qquad
	\subfloat[$ t=1 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/Wave_2000_u_t_slice_1.pdf}\\
			\includegraphics[scale=0.25]{./figures/Wave_2000_u_t_slice_error_1.pdf}
		\end{minipage}
	}\qquad
	\subfloat[$ t=1.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/Wave_2000_u_t_slice_2.pdf}\\
			\includegraphics[scale=0.25]{./figures/Wave_2000_u_t_slice_error_2.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\caption{Wave equation: Comparison of profiles of $u$ (top row) and its absolute error (bottom row) between the PINN solutions (loss forms \#1 and \#2) and the exact solution at time instants (a) $t=0.5$, (b) $t=1.0$, and (c) $t=1.5$. 
 %The time slice $ u $,  $ u_\theta $ and the corresponding errors(2000 training points)
 $N=2000$ training data points within the domain and on each of the domain boundaries ($x=0$ and $5$, and $t=0$).
 }
	\label{fg_2}
\end{figure}

\begin{figure}[tb]
	\centering
	\subfloat[$ t=0.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/Wave_2000_v_t_slice_0.pdf}\\
			\includegraphics[scale=0.25]{./figures/Wave_2000_v_t_slice_error_0.pdf}
		\end{minipage}
	}\qquad
	\subfloat[$ t=1 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/Wave_2000_v_t_slice_1.pdf}\\
			\includegraphics[scale=0.25]{./figures/Wave_2000_v_t_slice_error_1.pdf}
		\end{minipage}
	}\qquad
	\subfloat[$ t=1.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/Wave_2000_v_t_slice_2.pdf}\\
			\includegraphics[scale=0.25]{./figures/Wave_2000_v_t_slice_error_2.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\caption{Wave equation: Comparison of the profiles of $v=\frac{\partial u}{\partial t}$ (top row) and its absolute error (bottom row) between the PINN solutions (loss forms \#1 and \#2) and the exact solution at time instants (a) $t=0.5$, (b) $t=1.0$, and (c) $t=1.5$. 
 $N=2000$ training data points within the domain and on each of the domain boundaries ($x=0$ and $5$, and $t=0$).
 %The time slice $ v $,  $ v_\theta $ and the corresponding errors(2000 training points)
 }
	\label{fg_3}
\end{figure}



\begin{figure}[tb]
	%\centering
	%\begin{minipage}[t]{0.5\textwidth}
		\centering
  		\subfloat[PINN-F1]{\includegraphics[width=0.4\textwidth]{./figures/Wave_epoch-loss1_nosqrt_1.pdf}}
    \qquad
		\subfloat[PINN-F2]{\includegraphics[width=0.4\textwidth]{./figures/Wave_epoch-loss1_sqrt_1.pdf}}
	%\end{minipage}
	\caption{Wave equation: Histories of the loss function
 versus the training iteration with PINN-F1 and PINN-F2, corresponding to different number of
 training data points ($N$). 
 }
	\label{fg_4}
\end{figure}


% The time slices fig
Figures~\ref{fg_2} and~\ref{fg_3} provide a comparison of the  solutions obtained 
using the two forms of loss functions.
Figure~\ref{fg_2} compares profiles of the PINN-F1 and PINN-F2 solutions, and the exact solution, for $u$ (top row) at three time instants ($t=0.5$, $1.0$, and $1.5$), as well as the error profiles (bottom row).
Figure~\ref{fg_3} shows the corresponding results for the field variable $v=\frac{\partial u}{\partial t}$.
These results are obtained by using $N=2000$ training data points in the domain and on each of the domain boundaries.
It is observed that both PINN schemes, with the loss functions given by~\eqref{eq_63} and~\eqref{eq_64} respectively, have captured the solution reasonably well. We further observe that the PINN-F1 scheme (with the loss form~\eqref{eq_63}) produces notably more accurate results than the PINN-F2 (with loss form~\eqref{eq_64}),
especially for the field $\frac{\partial u}{\partial t}$.



We have varied the number of training data points $N$ systematically and studied its effect on the PINN results.
Figure~\ref{fg_4} shows the loss histories of PINN-F1 and PINN-F2 corresponding to different number of training data points ($N$) in
the simulations, with a total of $30,000$ training iterations.
We can make two observations. First, the history curves with the loss function form \#1 is generally smoother, indicating that the loss function decreases almost monotonically as the training progresses.
On the other hand, significant fluctuations in
the loss history can be observed with the form \#2. Second, the eventual loss values produced by the loss form \#1 are significantly smaller,
by over an order of magnitude, than those produced by the loss form \#2.



Table \ref{tab_1} is a further comparison between the PINN-F1 and PINN-F2. Here the $l_2$ and $l_{\infty}$ errors of $u$ and $v$ computed by PINN-F1 and PINN-F2 corresponding to different training data points ($N$) have been listed.
There appears to be a general trend that 
the errors tend to decrease with increasing number of training points, but the decrease is not monotonic. 
It can be observed that the $u$ errors are notably smaller than those for $v=\frac{\partial u}{\partial t}$, as signified earlier in e.g.~Figure~\ref{PINN_partpaper_Wave_fig1_1}.
One can again observe that PINN-F1 results are notably more accurate than those of PINN-F2 for the wave equation.




%---------------------------%
% The loss fig
%---------------------------%

\begin{table}[tb]
	\caption{Wave equation: The $u$ and $v$ errors versus the number of training data points $N$.
 }
	\label{tab_1}
	\centering
	\begin{tabular}[b]{ c | c|  c   c  | c   c   }
		\hline
  \multirow{2}{*}{method} &
		\multirow{2}{*}{$ N $}&\multicolumn{2}{c}{$ l_2 $-error} & \multicolumn{2}{c}{$ l_\infty $-error} \\ 
		\cline{3-6}
		& &$ u_\theta $  &$ v_\theta $ &$ u_\theta $  &$ v_\theta $ \\
		\hline
  & 1000&  5.7013e-03&  1.3531e-02&   1.8821e-02&  4.6631e-02\\
		\cline{3-6}
	&	1500&  2.1689e-03&   4.1035e-03&  6.7631e-03&   1.5109e-02\\
		\cline{3-6}
 PINN-F1	&	2000&  4.6896e-03&   9.6417e-03&  1.3828e-02&   3.3063e-02\\
		\cline{3-6}
	&	2500&  3.7879e-03&   9.8574e-03&  1.2868e-02&   3.3622e-02\\
		\cline{3-6}
	&	3000&  2.6588e-03&   6.0746e-03&  8.1457e-03&   1.9860e-02\\
  \hline
		& 1000&  4.7281e-02&  9.2431e-02&  1.4367e-01&  3.2764e-01\\
		\cline{3-6}
		& 1500&  4.9087e-02&  1.2438e-01&  2.1525e-01&  5.0601e-01\\
		\cline{3-6}
	PINN-F2	& 2000&  1.8554e-02&  4.9224e-02&  6.0780e-02&  1.6358e-01\\
		\cline{3-6}
		& 2500&  2.3526e-02&  5.4266e-02&  9.8690e-02&  1.9467e-01\\
		\cline{3-6}
		& 3000&  1.4164e-02&  3.7796e-02&  5.3045e-02&  1.4179e-01\\
		\hline
	\end{tabular}
\end{table}


% the error rate
\begin{figure}
	\centering
	\subfloat[PINN-F1]{\includegraphics[width=0.4\linewidth]{./figures/Wave_error_ratio_nosqrt_1.pdf}}
 \qquad
	\subfloat[PINN-F2]{\includegraphics[width=0.4\linewidth]{./figures/Wave_error_ratio_sqrt_1.pdf}}
	\caption{Wave equation: The $l^2$ errors of $u$, $\frac{\partial u}{\partial t}$, and $\frac{\partial u}{\partial x}$ as a function of the training loss value.
$N=2000$ training data points.
 }
	\label{fg_5}
\end{figure}

Theorem \ref{sec5_Theorem3} suggests the solution errors for $u$, $v=\frac{\partial u}{\partial t}$, and $\nabla u$ approximately scale as the square root of
the training loss function. Figure~\ref{fg_5} provides some numerical evidence for this point. Here we plot the $l^2$ errors for $u$, $\frac{\partial u}{\partial t}$ and $\frac{\partial u}{\partial x}$ from our simulations as a function of the training loss value for PINN-F1 and PINN-F2 in logarithmic scales. It is evident that for PINN-F1 the scaling essentially follows the square root relation. For PINN-F2 the relation between the error and the training loss appears to scale with a power somewhat larger than $\frac12$.



\subsection{Sine-Gordon Equation}

\begin{figure}[tb]
	%\centering
	%\begin{minipage}[t]{0.3\linewidth}
	\centering
	\subfloat[True solution for $u$]{\includegraphics[width=0.2\linewidth]{./figures/KG_2000_u.pdf}}\hspace{0.2em}
	\subfloat[PINN solution for $u$]{\includegraphics[width=0.2\linewidth]{./figures/KG_2000_uh.pdf}}\hspace{0.2em}
	\subfloat[Solution error for $u$]{\includegraphics[width=0.2\linewidth]{./figures/KG_2000_uerr.pdf}} \\
 	\subfloat[True solution for $v$]{\includegraphics[width=0.2\linewidth]{./figures/KG_2000_v.pdf}}\hspace{0.2em}
	\subfloat[PINN solution for $v$]{\includegraphics[width=0.2\linewidth]{./figures/KG_2000_vh.pdf}}\hspace{0.2em}
	\subfloat[Solution error for $v$]{\includegraphics[width=0.2\linewidth]{./figures/KG_2000_verr.pdf}}
	\caption{ Sine-Gordon equation:
 Distributions of the exact solution (left column), the PINN solution (middle column) and the PINN absolute error (right column) for $u$ (top row) and for $v=\frac{\partial u}{\partial t}$ (bottom row).
 $N=2000$ collocation points within the domain and on the domain boundaries.
 %{\color{red}[Yongchao, which loss function produces these plots?]}
 %NN: [2, 80, 80, 2]. $\tanh$ activation function.
 %loss: squared loss.
 }
 \label{fg_6}
 \end{figure}

We test the PINN algorithm suggested by the theoretical analysis for the Sine-Gordon equation~\eqref{SG} in this subsection. Consider the spatial-temporal domain
$(x,t)\in\Omega= D\times [0, T] = [0, 1] \times [0, 2]$, and the following initial/boundary value problem on this domain,
\begin{subequations}\label{eq_65}
\begin{align}
	& \frac{\partial^2u}{\partial t^2} - \frac{\partial^2 u}{\partial x^2} + u + \sin(u) = f(x,t), \\
 &u({0},t)=\phi_1(t),  \qquad u({1},t)=\phi_2(t), \\
	&u({x},0)=\psi_{1}({x}),\qquad \frac{\partial u}{\partial t}({x},0)=\psi_{2}({x}).
\end{align}
\end{subequations}
In these equations, $ u(x,t) $ is the field function to be solved for, $ f(x,t) $ is a source term, $ \psi_{1} $ and $ \psi_2 $ are the initial conditions, and $ \phi_1 $ and $ \phi_2 $ are the boundary conditions.
The source term, initial and boundary conditions appropriately are chosen by the following exact solution,  
\begin{align}\label{PINN_SG_eq1}
	u(x, t) = \left[2\cos\left(\pi x + \frac{\pi}{5}\right) + \frac{9}{5}\cos\left(2\pi x + \frac{7\pi}{20}\right)\right]\left[ 2\cos\left(\pi t + \frac{\pi}{5}\right) + \frac{9}{5}\cos\left(2\pi t + \frac{7\pi}{20}\right) \right].
\end{align}


To simulate this problem with PINN, we reformulate the problem as follows,% in accordance with Section~\ref{Sine-Gordon},
\begin{subequations}\label{eq_67}
\begin{align}
	&u_{t} - v = 0, \label{eq_67a} \\ 
 &  v_{t} - u_{xx} + u + \sin(u) = f(x,t), \\
 &u({0},t)=\phi_1(t),  \qquad u({1},t)=\phi_2(t),\\
	&u({x},0)=\psi_{1}({x}),\qquad v({x},0)=\psi_{2}({x}),
\end{align}
\end{subequations}
where $v$ is a variable defined by equation~\eqref{eq_67a}.

In light of~\eqref{SG_T}, we employ the following loss function in PINN,
\begin{align}\label{eq_68}
\text{Loss}=
     &\frac{W_1}{N}\sum_{n=1}^{N}\left[ u_{\theta t}(x_{int}^n, t_{int}^n) - v_{\theta}(x_{int}^n, t_{int}^n) \right]^2 \nonumber \\
 	& + \frac{W_2}{N}\sum_{n=1}^{N}\left[ v_{\theta t}(x_{int}^n, t_{int}^n) - u_{\theta xx}(x_{int}^n, t_{int}^n) + u_{\theta}(x_{int}^n, t_{int}^n) + \sin(u_{\theta}(x_{int}^n, t_{int}^n)) - f(x_{int}^n, t_{int}^n) \right]^2 \nonumber \\
 	& +\frac{W_3}{N}\sum_{n=1}^{N}\left[ u_{\theta t x}(x_{int}^n, t_{int}^n) - v_{\theta x}(x_{int}^n, t_{int}^n) \right]^2 + \frac{W_4}{N} \sum_{n=1}^{N}\left[ u_{\theta}(x_{tb}^n,0) - \psi_{1}(x_{tb}^n)\right]^2 \nonumber \\
 	& + \frac{W_5}{N} \sum_{n=1}^{N}\left[ v_{\theta}(x_{tb}^n,0) - \psi_{2}(x_{tb}^n)\right]^2+ \frac{W_6}{N}\sum_{n=1}^{N} \left[u_{\theta x}(x_{tb}^n,0) - \psi_{1 x}(x_{tb}^n)\right]^2 \nonumber \\
 	& + \frac{W_7}{N} \sum_{n=1}^{N}\left[ | v_{\theta}(0, t_{sb}^n) - \phi_{1t}({t_{sb}^n})| + | v_{\theta}(1, t_{sb}^n) - \phi_{2t}({t_{sb}^n}) |\right],
\end{align}
where $W_n>0$ ($1\leq n\leq 7$)
are the penalty coefficients for different loss terms added in the PINN implementation.
It should be noted that the loss terms with the coefficients $W_3$ and $W_6$ will be absent from the conventional PINN formulation (see~\cite{Raissi2019pinn}). These terms in the training loss are necessary based on the error analysis in Section~\ref{Sine-Gordon}. It should also be noted that the $W_7$ loss terms are not squared, as dictated by the theoretical analysis
of Section~\ref{Sine-Gordon}.

We have also implemented a PINN scheme with a variant form for the loss function,
\begin{align}\label{eq_69}
\text{Loss}=
     &\frac{W_1}{N}\sum_{n=1}^{N}\left[ u_{\theta t}(x_{int}^n, t_{int}^n) - v_{\theta}(x_{int}^n, t_{int}^n) \right]^2 \nonumber \\
 	& + \frac{W_2}{N}\sum_{n=1}^{N}\left[ v_{\theta t}(x_{int}^n, t_{int}^n) - u_{\theta xx}(x_{int}^n, t_{int}^n) + u_{\theta}(x_{int}^n, t_{int}^n) + \sin(u_{\theta}(x_{int}^n, t_{int}^n)) - f(x_{int}^n, t_{int}^n) \right]^2 \nonumber \\
 	& +\frac{W_3}{N}\sum_{n=1}^{N}\left[ u_{\theta t x}(x_{int}^n, t_{int}^n) - v_{\theta x}(x_{int}^n, t_{int}^n) \right]^2 + \frac{W_4}{N} \sum_{n=1}^{N}\left[ u_{\theta}(x_{tb}^n,0) - \psi_{1}(x_{tb}^n)\right]^2 \nonumber \\
 	& + \frac{W_5}{N} \sum_{n=1}^{N}\left[ v_{\theta}(x_{tb}^n,0) - \psi_{2}(x_{tb}^n)\right]^2+ \frac{W_6}{N}\sum_{n=1}^{N} \left[ u_{\theta x}(x_{tb}^n,0) - \psi_{1 x}(x_{tb}^n)\right]^2 \nonumber \\
 	& + \frac{W_7}{N} \sum_{n=1}^{N}\left[ ( v_{\theta}(0, t_{sb}^n) - \phi_{1t}({t_{sb}^n}))^2 + ( v_{\theta}(1, t_{sb}^n) - \phi_{2t}({t_{sb}^n}) )^2\right].
\end{align}
The difference between~\eqref{eq_69} and~\eqref{eq_68} lies in the $W_7$ terms. These $W_7$ terms in~\eqref{eq_69} are squared, and they are not in~\eqref{eq_68}.
We refer to the PINN scheme employing the loss function~\eqref{eq_68} as PINN-G1 and the scheme employing the loss function~\eqref{eq_69} as PINN-G2.

In the simulations we employ a feed-forward neural network with two input nodes (representing $x$ and $t$), two output nodes (representing $u$ and $v$), and two hidden layers, each having a width of $80$ nodes. The $\tanh$ activation function has been used for all the hidden nodes. We employ $N$ collocation points generated from a uniform random distribution within the domain, on 
 each of the domain boundary, and also on the initial boundary, where $N$ is varied systematically in the simulations. The penalty coefficients in the loss functions are taken to be
 $ \bm{W} = (W_1,\dots,W_7)=(0.5, 0.4, 0.5, 0.6, 0.6, 0.6, 0.8)$.


\begin{figure}[tb]
	\centering
	\subfloat[$ t=0.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/KG_2000_u_t_slice_0.pdf}\\
			\includegraphics[scale=0.25]{./figures/KG_2000_u_t_slice_error_0.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/KG_2000_u_t_slice_1.pdf}\\
			\includegraphics[scale=0.25]{./figures/KG_2000_u_t_slice_error_1.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/KG_2000_u_t_slice_2.pdf}\\
			\includegraphics[scale=0.25]{./figures/KG_2000_u_t_slice_error_2.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\caption{Sine-Gordon equation: Top row, comparison of profiles between the exact solution and PINN-G1/PINN-G2 solutions for $u$ at several time instants. Bottom row, profiles of the absolute error of the PINN-G1 and PINN-G2 solutions for $u$. $N=2000$ 
 training collocation points.
 %The time slice $ u $,  $ u_\theta $ and the corresponding errors(2000 training points)
 }
	\label{fg_7}
\end{figure}

\begin{figure}[tb]
	\centering
	\subfloat[$ t=0.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/KG_2000_v_t_slice_0.pdf}\\
			\includegraphics[scale=0.25]{./figures/KG_2000_v_t_slice_error_0.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/KG_2000_v_t_slice_1.pdf}\\
			\includegraphics[scale=0.25]{./figures/KG_2000_v_t_slice_error_1.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.25]{./figures/KG_2000_v_t_slice_2.pdf}\\
			\includegraphics[scale=0.25]{./figures/KG_2000_v_t_slice_error_2.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\caption{Sine-Gordon equation: Top row, comparison of profiles between the exact solution and PINN-G1/PINN-G2 solutions for $v=\frac{\partial u}{\partial t}$ at several time instants. Bottom row, profiles of the absolute error of the PINN-G1 and PINN-G2 solutions for $v$. $N=2000$ 
 training collocation points.
 %The time slice $ v $,  $ v_\theta $ and the corresponding errors(2000 training points)
 }
	\label{fg_8}
\end{figure}


Figure~\ref{fg_6} shows distributions of of $u(x,t)$ and $v=\frac{\partial u}{\partial t}$ from the exact solution (left column) and 
the PINN solution (middle column),
as well as the point-wise absolute errors of the PINN solution for these fields (right column).
These results are obtained by PINN-G2 with $N=2000$ random collocation points within the domain and on each of the domain boundaries. The PINN solution is in good agreement with the true solution.



Figures~\ref{fg_7} and~\ref{fg_8} compare the profiles of $u$ and $v$ between the exact solution, and the solutions obtained by PINN-G1 and PINN-G2, at several time instants ($t=0.5$, $1$ and $1.5$). Profiles of the absolute errors of the PINN-G1/PINN-G2 solutions are also shown in these figures.
We observe that both PINN-G1 and PINN-G2 have captured the solution for $u$ quite accurately, and to a lesser extent, also for $v$.
Comparison of the error profiles between PINN-G1 and PINN-G2 suggests that the PINN-G2 error in general appears to be somewhat smaller than that of PINN-G1. But this seems not to be  true consistently in the entire domain.



\begin{figure}[tb]
	%\centering
	%\begin{minipage}[t]{1\linewidth}
	\centering
	\subfloat[PINN-G1]{\includegraphics[width=0.4\linewidth]{./figures/KG_epoch-loss1_sqrt_1.pdf}}\qquad
	\subfloat[PINN-G2]{\includegraphics[width=0.4\linewidth]{./figures/KG_epoch-loss1_nosqrt_1.pdf}}\hspace{0.1em}
	%\end{minipage}
	\caption{Sine-Gordon equation: Loss histories of (a) PINN-G1 and (b) PINN-G2 corresponding to various numbers of training collocation points. 
 %The Log-Log for iterations and loss function errors
 }
	\label{PINN_partpaper_SG_fig3}
\end{figure}


\begin{table}[tb]
    \caption{Sine-Gordon equation: The $l_2$ and $l_{\infty}$ errors for $u$ and $v$ versus the number of training collocation points $N$ corresponding to PINN-G1 and PINN-G2. }
    \label{tab_2}
	\centering
	\begin{tabular}[b]{ c | c@{\ \ }|  c  @{\ \ \ }  c  @{\ \ } | c  @{\ \ \ }  c  @{\ \ } }
		\hline
    \multirow{2}{*}{method}	&	
  \multirow{2}{*}{$ N $}&\multicolumn{2}{c}{$ l_2 $-error} & \multicolumn{2}{c}{$ l_\infty $-error} \\ 
		\cline{3-6}
		& &$ u_\theta $  &$ v_\theta $ &$ u_\theta $  &$ v_\theta $ \\
		\hline
		& 1000&  3.0818e-03&  4.3500e-03&  9.6044e-03&  1.8894e-02\\
		\cline{3-6}
		& 1500&  3.4335e-03&  4.8035e-03&  1.0566e-02&  1.7050e-02\\
		\cline{3-6}
	PINN-G1	& 2000&  2.1914e-03&  3.0055e-03&  7.5882e-03&  1.1099e-02\\
		\cline{3-6}
		& 2500&  3.0172e-03&  3.5698e-03&  9.2515e-03&  1.4645e-02\\
		\cline{3-6}
		& 3000&  2.5281e-03&  4.4858e-03&  7.2785e-03&  1.6213e-02\\
		\hline
		& 1000&  3.0674e-03&  2.0581e-03&  7.3413e-03&  1.1323e-02\\
		\cline{3-6}
		& 1500&  1.0605e-03&  1.4729e-03&  2.2914e-03&  6.2831e-03\\
		\cline{3-6}
	PINN-G2	& 2000&  2.2469e-03&  1.6072e-03&  4.8842e-03&  8.8320e-03\\
		\cline{3-6}
		& 2500&  6.6072e-04&  6.0509e-04&  1.4099e-03&  4.3423e-03\\
		\cline{3-6}
		& 3000&  6.6214e-04&  1.0830e-03&  1.9697e-03&  7.8866e-03\\
		\hline
	\end{tabular}
\end{table}


% collocation point effects

The effect of the collocation points on the PINN results has been studied by varying the number of training collocation points systematically between $N=1000$ and $N=3000$
within the domain and on each of the domain boundaries. The results are provided in Figure~\ref{PINN_partpaper_SG_fig3} and Table~\ref{tab_2}.
Figure~\ref{PINN_partpaper_SG_fig3} shows  histories of the loss function corresponding to different number of collocation points for PINN-G1 and PINN-G2.
Table~\ref{tab_2} provides the $l_2$ and $l_{\infty}$ errors of $u$ and $v$ versus the number of collocation points computed by PINN-G1 and PINN-G2.
The PINN errors in general tend to decrease with increasing number of collocation points, but this trend is not monotonic.
It can be observed that both PINN-G1 and PINN-G2 have captured the solutions quite accurately, with those errors from PINN-G2 in general slightly better.


% error rate
\begin{figure}
	\centering
	\subfloat[PINN-G1]{\includegraphics[width=0.4\linewidth]{./figures/KG_error_ratio_sqrt_1.pdf}}\qquad
	\subfloat[PINN-G2]{\includegraphics[width=0.4\linewidth]{./figures/KG_error_ratio_nosqrt_1.pdf}}\hspace{0.1em}
	\caption{Sine-Gordon equation: The $l^2$ errors of $u$, $\frac{\partial u}{\partial t}$, and $\frac{\partial u}{\partial x}$ as a function of the training loss value.
 %The Log-Log for scaling of total error vs. training error
 }
	\label{PINN_partpaper_SG_errorrate}
\end{figure}

% total error vs. training loss


Figure \ref{PINN_partpaper_SG_errorrate} provides some numerical evidence for the relation between the total error and the training loss as suggested by Theorem \ref{sec6_Theorem3}.
Here we plot the $l_2$ errors for $u$, $v$ and $\frac{\partial u}{\partial x}$ as a function of the training loss value obtained by PINN-G1 and PINN-G2.  
The results indicate that the total error scales approximately as the square root of the training loss, which in some sense corroborates the error-loss relation as expressed in Theorem \ref{sec6_Theorem3}.


\subsection{Linear Elastodynamic Equation}

In this subsection we look into the linear elastodynamic equation (in two spatial dimensions plus time) and test the PINN algorithm as suggested by the theoretical analysis in Section~\ref{Elasto-dynamics} using this equation.
Consider the spatial-temporal domain $(x,y,t)\in \Omega = D \times [0, T]= [0, 1]\times[0,1]\times [0, 2] $, and the following initial/boundary value problem with the linear elastodynamics equation on $ \Omega $:
\begin{subequations}\label{num_ela_eq00}
\begin{align}\label{num_ela_eq1}
	&\rho\frac{\partial^2 \bm{u}}{\partial t^2} - 2\mu\nabla\cdot(\underline{\bm{\varepsilon}}(\bm{u})) -\lambda\nabla(\nabla\cdot\bm{u})= \bm{f}(\bm{x}, t),\\
	&\bm{u}|_{\Gamma_d}=\bm{\phi}_{d},\qquad \Big(2\mu\underline{\bm{\varepsilon}}(\bm{u}) +\lambda(\nabla\cdot\bm{u})\Big)|_{\Gamma_n}\bm{n}=\bm{\phi}_n,\\
	&\bm{u}(\bm{x}, 0)=\bm{\psi}_{1}, \qquad \frac{\partial\bm u}{\partial t}(\bm{x}, 0)=\bm{\psi}_{2},
\end{align}
\end{subequations}
where $ \bm u = (u_1(\bm x, t), u_2(\bm x, t))^T $ ($ \bm x=(x,y)\in D $, $ t\in[0, T] $) is the displacement field to be solved for,  $ \bm{f}(\bm{x}, t) $ is a  source term, and $\rho$, $\mu$ and $\lambda$ are material constants. $ \Gamma_d $ is the Dirichlet boundary and $ \Gamma_n $ is the Neumann boundary, with $ \partial D=\Gamma_d \cup \Gamma_n $ and $ \Gamma_d \cap \Gamma_n = \emptyset $, where $ \bm{n} $ is the outward-pointing unit normal vector. In our simulations we choose the left boundary ($x=0$) as the Dirichlet boundary, and the rest are Neumann boundaries. $ \bm{\phi}_d $ and $ \bm{\phi}_n $ are Dirichlet and Neumann boundary conditions, respectively. $ \bm{\psi}_1 $ and $ \bm{\psi}_2 $ are the initial conditions for the displacement and the velocity. We employ the material parameter values $ \mu = \lambda = \rho = 1 $, and the following manufactured solution (\cite{2018_CMAME_DGelastodynamics}) to this problem,
\begin{align}\label{num_ela_eq2}
	\bm u(\bm x, t) = \sin(\sqrt{2}\pi t) \begin{bmatrix}
		-\sin(\pi x)^2\sin(2\pi y)\\
		\sin(2\pi x)\sin(\pi y)^2
	\end{bmatrix}.
\end{align}
The source term $ \bm{f}(\bm{x}, t) $, the boundary/initial distributions $ \bm{\phi}_d $, $ \bm{\phi}_n $, $ \bm{\psi}_{1} $ and $ \bm{\psi}_{2} $ are chosen by the expression \eqref{num_ela_eq2}.

To simulate this problem using the PINN algorithm suggested by
the theoretical analysis from Section \ref{Elasto-dynamics}, we reformulate~\eqref{num_ela_eq00} into the following system
\begin{subequations}\label{num_ela_eq3}
 	\begin{align}
 		\label{num_ela_eq3_1}
 		&\bm{u}_{t} - \bm{v} = \bm{0},\qquad \bm{v}_{t} - 2\nabla\cdot(\underline{\bm{\varepsilon}}(\bm{u})) -\nabla(\nabla\cdot\bm{u})= \bm{f}(\bm{x}, t),\\
 		&\bm{u}|_{\Gamma_d}=\bm{\phi}_{d},\qquad \Big(2\underline{\bm{\varepsilon}}(\bm{u}) +(\nabla\cdot\bm{u})\Big)|_{\Gamma_n}\bm{n}=\bm{\phi}_n,\\
 		&\bm{u}(\bm{x}, 0)=\bm{\psi}_{1}, \qquad \bm{v}(\bm{x}, 0)=\bm{\psi}_{2},
 	\end{align}
\end{subequations}
where $ \bm{v}(\bm{x}, t) $ is an intermediate variable (representing the velocity) as given by \eqref{num_ela_eq3_1}.

In light of~\eqref{elast_T},
%In this numerical test, 
we employ the following loss function for PINN, 
\iffalse
\begin{align*}
 	\mathcal{E}_T(\theta,\mathcal{S})^2&
 	=W_1\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2+W_2\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2+W_3\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2+W_4\mathcal{E}_T^{int4}(\theta,\mathcal{S}_{int})^2+W_5\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2
 	\nonumber\\
 	&\quad
 	+W_6\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 +W_7\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2
 	+W_8\mathcal{E}_T^{tb4}(\theta,\mathcal{S}_{tb})^2
 	+W_9\mathcal{E}_T^{sb1}(\theta,\mathcal{S}_{sb1}) 
 	+W_{10}\mathcal{E}_T^{sb2}(\theta,\mathcal{S}_{sb2})\nonumber\\
    &= \frac{W_1}{N_{int}} \sum_{n=1}^{N_{int}} \big( \bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)\big)^2 \nonumber\\
 	& +  \frac{W_2}{N_{int}} \sum_{n=1}^{N_{int}} \big( \bm{v}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - 2\nabla\cdot (\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n))) - \nabla(\nabla\cdot \bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) -\bm{f}(\bm{x}_{int}^n, t_{int}^n)) \big)^2 \nonumber\\
 	& + \frac{W_3}{N_{int}} \sum_{n=1}^{N_{int}}\big( \underline{\bm{\varepsilon}}(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n))\big)^2 + \frac{W_4}{N_{int}} \sum_{n=1}^{N_{int}} \big( \nabla\cdot(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) \big)^2 \nonumber\\
 	&+ \frac{W_5}{N_{tb}} \sum_{n=1}^{N_{tb}} \big( \bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n) \big)^2 + \frac{W_6}{N_{tb}} \sum_{n=1}^{N_{tb}} \big( \bm{v}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_2(\bm{x}_{tb}^n) \big)^2 \nonumber \\
 	&+ \frac{W_7}{N_{tb}} \sum_{n=1}^{N_{tb}} \big( \underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \big)^2 + \frac{W_8}{N_{tb}} \sum_{n=1}^{N_{tb}} \big( \nabla\cdot(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \big)^2  \nonumber \\
 	&+\frac{W_9}{N_{sb1}} \sum_{n=1}^{N_{sb1}} |\bm{v}_{\theta}(\bm{x}_{sb1}^n, t_{sb1}^n) - \bm{\phi}_d(\bm{x}_{sb1}^n, t_{sb1}^n)| \nonumber\\
 	&+ \frac{W_{10}}{N_{sb2}} \sum_{n=1}^{N_{sb2}} | 2\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} +(\nabla\cdot\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} - \bm{\phi}_n(\bm{x}_{sb2}^n, t_{sb2}^n) |.
\end{align*}
\fi 
\begin{align}\label{num_ela_eq4}
 	\text{Loss}
    &= \frac{W_1}{N} \sum_{n=1}^{N} \left[ \bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)\right]^2 \nonumber\\
 	& +  \frac{W_2}{N} \sum_{n=1}^{N} \left[ \bm{v}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - 2\nabla\cdot (\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n))) - \nabla(\nabla\cdot \bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) -\bm{f}(\bm{x}_{int}^n, t_{int}^n)) \right]^2 \nonumber\\
 	& + \frac{W_3}{N} \sum_{n=1}^{N}\left[ \underline{\bm{\varepsilon}}(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n))\right]^2 + \frac{W_4}{N} \sum_{n=1}^{N} \left[ \nabla\cdot(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) \right]^2 \nonumber\\
 	&+ \frac{W_5}{N} \sum_{n=1}^{N} \left[ \bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n) \right]^2 + \frac{W_6}{N} \sum_{n=1}^{N} \left[ \bm{v}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_2(\bm{x}_{tb}^n) \right]^2 \nonumber \\
 	&+ \frac{W_7}{N} \sum_{n=1}^{N} \left[ \underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \right]^2 + \frac{W_8}{N} \sum_{n=1}^{N} \left[\nabla\cdot(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \right]^2  \nonumber \\
 	&+\frac{W_9}{N} \sum_{n=1}^{N} |\bm{v}_{\theta}(\bm{x}_{sb1}^n, t_{sb1}^n) - \bm{\phi}_{dt}(\bm{x}_{sb1}^n, t_{sb1}^n)| \nonumber\\
 	&+ \frac{W_{10}}{N} \sum_{n=1}^{N} | 2\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} +(\nabla\cdot\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} - \bm{\phi}_n(\bm{x}_{sb2}^n, t_{sb2}^n) |,
\end{align}
where we have added the penalty coefficients, $W_n>0$ ($ 1\leq n\leq 10 $), for different loss terms in the implementation, and $N$ denotes the number of collocation points within the domain and on the domain boundaries. 
In the numerical tests we have also implemented another form for the loss function as follows,
\iffalse
\begin{align*}
	\mathcal{E}_T(\theta,\mathcal{S})^2&
	=\frac{W_1}{N_{int}} \sum_{n=1}^{N_{int}} \left[ \bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)\right]^2 \nonumber\\
	& +  \frac{W_2}{N_{int}} \sum_{n=1}^{N_{int}} \left[ \bm{v}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - 2\nabla\cdot (\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n))) - \nabla(\nabla\cdot \bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) -\bm{f}(\bm{x}_{int}^n, t_{int}^n)) \right]^2 \nonumber\\
	& + \frac{W_3}{N_{int}} \sum_{n=1}^{N_{int}} \left[ \underline{\bm{\varepsilon}}(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n))\right]^2 + \frac{W_4}{N_{int}} \sum_{n=1}^{N_{int}} \left[\nabla\cdot(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) \right]^2 \nonumber\\
	&+ \frac{W_5}{N_{tb}} \sum_{n=1}^{N_{tb}} \left[ \bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n) \right]^2 + \frac{W_6}{N_{tb}} \sum_{n=1}^{N_{tb}} \left[ \bm{v}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_2(\bm{x}_{tb}^n) \right]^2 \nonumber \\
	&+ \frac{W_7}{N_{tb}} \sum_{n=1}^{N_{tb}} \left[ \underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \right]^2 + \frac{W_8}{N_{tb}} \sum_{n=1}^{N_{tb}} \left[ \nabla\cdot(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \right]^2  \nonumber \\
	&+\frac{W_9}{N_{sb1}} \sum_{n=1}^{N_{sb1}} \left[\bm{v}_{\theta}(\bm{x}_{sb1}^n, t_{sb1}^n) - \bm{\phi}_d(\bm{x}_{sb1}^n, t_{sb1}^n)\right]^2 \nonumber\\
	&+ \frac{W_{10}}{N_{sb2}} \sum_{n=1}^{N_{sb2}} \left[2\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} +(\nabla\cdot\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} - \bm{\phi}_n(\bm{x}_{sb2}^n, t_{sb2}^n) \right]^2.
\end{align*}
\fi 
\begin{align}\label{num_ela_eq5}
	\text{Loss}&
	=\frac{W_1}{N} \sum_{n=1}^{N} \left[ \bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)\right]^2 \nonumber\\
	& +  \frac{W_2}{N} \sum_{n=1}^{N} \left[ \bm{v}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - 2\nabla\cdot (\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n))) - \nabla(\nabla\cdot \bm{u}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) -\bm{f}(\bm{x}_{int}^n, t_{int}^n)) \right]^2 \nonumber\\
	& + \frac{W_3}{N} \sum_{n=1}^{N}\left[ \underline{\bm{\varepsilon}}(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n))\right]^2 + \frac{W_4}{N} \sum_{n=1}^{N} \left[ \nabla\cdot(\bm{u}_{\theta t}(\bm{x}_{int}^n, t_{int}^n) - \bm{v}_{\theta}(\bm{x}_{int}^n, t_{int}^n)) \right]^2 \nonumber\\
	&+ \frac{W_5}{N} \sum_{n=1}^{N} \left[ \bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n) \right]^2 + \frac{W_6}{N} \sum_{n=1}^{N} \left[ \bm{v}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_2(\bm{x}_{tb}^n) \right]^2 \nonumber \\
	&+ \frac{W_7}{N} \sum_{n=1}^{N} \left[ \underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \right]^2 + \frac{W_8}{N} \sum_{n=1}^{N} \left[ \nabla\cdot(\bm{u}_{\theta}(\bm{x}_{tb}^n, 0) -\bm{\psi}_1(\bm{x}_{tb}^n))  \right]^2  \nonumber \\
	&+\frac{W_9}{N} \sum_{n=1}^{N} \left[\bm{v}_{\theta}(\bm{x}_{sb1}^n, t_{sb1}^n) - \bm{\phi}_{dt}(\bm{x}_{sb1}^n, t_{sb1}^n)\right]^2 \nonumber\\
	&+ \frac{W_{10}}{N} \sum_{n=1}^{N} \left[2\underline{\bm{\varepsilon}}(\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} +(\nabla\cdot\bm{u}_{\theta}(\bm{x}_{sb2}^n, t_{sb2}^n))\bm{n} - \bm{\phi}_n(\bm{x}_{sb2}^n, t_{sb2}^n) \right]^2.
\end{align}
The difference between these two forms for the loss function lies in the $W_9$ and $W_{10}$ terms.
It should be noted that 
the $W_9$ and $ W_{10} $ terms in \eqref{num_ela_eq4} are not squared, in light of the error terms \eqref{elast_T1}$ - $\eqref{elast_T8} from the theoretical analysis. In contrast, these terms are squared in \eqref{num_ela_eq5}. The PINN scheme utilizing the loss function \eqref{num_ela_eq4} is henceforth referred to as PINN-H1, and the scheme that employs the loss function \eqref{num_ela_eq5} shall be referred to as PINN-H2.

In the  simulations, we employ a feed-forward neural network with three input nodes, which represent $ \bm{x}=(x,y) $ and the time variable t, and four output nodes, which represent $ \bm{u}=(u_1, u_2) $ and $ \bm{v}=(v_1, v_2) $. The neural network has two hidden layers, with widths of 90 and 60 nodes, respectively, and the $\tanh$ activation function for all the hidden nodes. 
%To ensure the accuracy and robustness of our simulations, 
For the network training, $ N $ collocation points are generated from a uniform random distribution within the domain, on each of the domain boundary, as well as on the initial boundary. $ N $ is systematically varied in the simulations. We employ the penalty coefficients $ \bm{W} = (W_1, ..., W_{10}) = (0.9, 0.9, 0.9, 0.9, 0.5, 0.5, 0.5, 0.5, 0.9, 0.9) $ in the simulations.
%, thereby ensuring the validity and relevancy of our findings.

\begin{figure}[htpb]
	\centering
	\subfloat[$ t=0.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t1_u.pdf}\\
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t1_uh_sqrt.pdf}\\
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t1_uh_nosqrt.pdf}
		%	\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t1_uherr_sqrt}\\
		%	\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t1_uherr_nosqrt}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t2_u.pdf}\\
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t2_uh_sqrt.pdf}\\
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t2_uh_nosqrt.pdf}
		%	\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t2_uherr_sqrt}\\
		%	\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t2_uherr_nosqrt}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t3_u.pdf}\\
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t3_uh_sqrt.pdf}\\
			\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t3_uh_nosqrt.pdf}
		%	\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t3_uherr_sqrt}\\
		%	\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t3_uherr_nosqrt}
		\end{minipage}
	}%\hspace{-10em}
\caption{Linear elastodynamic equation: 
Visualization of the deformed configuration at time instants (a) $t=0.5$, (b) $t=1.0$, and (c) $t=1.5$ from the exact solution (top row), the PINN-H1 solution (middle row) and the PINN-H2 solution (bottom row). 
Plotted here are the deformed field, $\bm x+\bm u(\bm x,t)$, for a set of grid points $\bm x\in D=[0,1]\times [0,1]$.
$N=2000$ training collocation points within domain and on the domain boundaries.
}
	\label{fg_11}
\end{figure}

\begin{figure}[htpb]
	\centering
	\subfloat[$ t=0.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t1_u}\\
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t1_uh_sqrt}\\
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t1_uh_nosqrt}
			\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t1_uherr_sqrt.pdf}\\
			\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t1_uherr_nosqrt.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1 $]{
		\begin{minipage}[b]{0.25\textwidth}
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t2_u}\\
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t2_uh_sqrt}\\
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t2_uh_nosqrt}
			\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t2_uherr_sqrt.pdf}\\
			\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t2_uherr_nosqrt.pdf}
		\end{minipage}
	}%\hspace{-10em}
	\subfloat[$ t=1.5 $]{
		\begin{minipage}[b]{0.25\textwidth}
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t3_u}\\
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t3_uh_sqrt}\\
		%	\includegraphics[scale=0.24]{./figures/LinearElasticity_2000_t3_uh_nosqrt}
			\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t3_uherr_sqrt.pdf}\\
			\includegraphics[scale=0.21]{./figures/LinearElasticity_2000_t3_uherr_nosqrt.pdf}
		\end{minipage}
	}%\hspace{-10em}
\caption{Linear elastodynamic equation: 
Distributions of the point-wise absolute error,
$\|\bm u_{\theta}-\bm u \|$, of the PINN-H1 solution (top row) and the PINN-H2 solution (bottom row) at three time instants (a) $t=0.5$,
(b) $t=1.0$, and (c) $t=1.5$.
%The time slice $ u $,  $ u_\theta $ and the corresponding errors
$N=2000$ training collocation points within domain and on the domain boundaries.
}
	\label{fg_a11}
\end{figure}


%%----- up updated
In Figures~\ref{fg_11} and~\ref{fg_a11} we compare the PINN-H1/PINN-H2 solutions with the exact solution and provide an overview of their errors.
Figure~\ref{fg_11} is a visualization of the deformed configuration of the domain.
Here we have plotted the deformed field, $\bm x+\bm u(\bm x,t)$, for a set of grid points
$\bm x\in D$ at three time instants from the exact solution, the PINN-H1 and PINN-H2 solutions.
Figure~\ref{fg_a11} shows distributions of 
the point-wise absolute error of the PINN-H1/PINN-H2 solutions,
$\|\bm u_{\theta}-\bm u \|=\sqrt{(u_{\theta 1}(\bm x,t) - u_1(\bm x,t))^2+(u_{\theta 2}(\bm x,t) - u_2(\bm x,t))^2}$, at the same three time instants.
Here $ \bm u_\theta = (u_{\theta 1}, u_{\theta 2}) $ denotes the PINN solution. 
While both  PINN schemes capture the solution fairly well at $ t=0.5$ and $1$, at $ t=1.5 $ both schemes show larger deviations from the true solution. In general, the PINN-H1 scheme appears to produce a better approximation to the solution than PINN-H2. 


\begin{table}[tb]\small
    \caption{Linear elastodynamic equation: The $l_2$ and $l_\infty$ errors for $\bm{u}=(u_1, u_2)$ and $\bm{v}=(v_1,v_2)$ versus the number of training data points $N$ from the PINN-H1 and PINN-H2 solutions.}
	\label{PINN_partpaper_Ela_tab_1}
    \centering
	\begin{tabular}[b]{ c@{\ \ }| c  @{\ \ \ }  c  @{\ \ }  c  @{\ \ \ }  c  | c  @{\ \ \ }  c  @{\ \ } c  @{\ \ \ }  c }
		\hline
		\multirow{2}{*}{$ N $}&\multicolumn{4}{c}{$ l_2 $-error} & \multicolumn{4}{c}{$ l_\infty $-error} \\ 
		\cline{2-9}
		{} &$ u_{\theta 1} $  &$ u_{\theta 2} $  &$ v_{\theta 1} $  &$ v_{\theta 2} $ &$ u_{\theta 1} $  &$ u_{\theta 2} $  &$ v_{\theta 1} $  &$ v_{\theta 2} $ \\
		\hline
		{}&\multicolumn{8}{c}{PINN-H1}\\
		\cline{2-9}
		1000&  4.8837e-02&  6.0673e-02&   4.7460e-02&  5.1640e-02&  1.7189e-01&  2.1201e-01&  6.9024e-01&  6.1540e-01\\
		\cline{2-9}
		1500&  2.8131e-02&  3.1485e-02&   4.1104e-02&  4.1613e-02&  1.9848e-01&  2.4670e-01&  3.4716e-01&  4.0582e-01\\
		\cline{2-9}
		2000&  2.7796e-02&  4.0410e-02&   3.5891e-02&  4.6334e-02&  1.4704e-01&  1.7687e-01&  4.0678e-01&  5.0022e-01\\
		\cline{2-9}
		2500&  3.0909e-02&  4.0215e-02&   3.3966e-02&  4.4024e-02&  1.7589e-01&  2.4211e-01&  4.1403e-01&  3.9570e-01\\
		\cline{2-9}
		3000&  2.6411e-02&  3.5600e-02&   4.3209e-02&  5.2802e-02&  1.4289e-01&  1.3625e-01&  5.1167e-01&  5.3298e-01\\
		\hline 
		{}&\multicolumn{8}{c}{PINN-H2}\\
		\cline{2-9}
		1000&  4.9869e-02&  1.3451e-01&   5.6327e-02&  5.4796e-02&  3.2314e-01&  3.4978e-01&  6.7624e-01&  5.7277e-01\\
		\cline{2-9}
		1500&  5.4708e-02&  1.3987e-01&   4.5871e-02&  5.1622e-02&  2.8609e-01&  5.2598e-01&  4.9343e-01&  2.3518e-01\\
		\cline{2-9}
		2000&  6.2114e-02&  1.0190e-01&   6.4477e-02&  5.0011e-02&  2.5745e-01&  3.1642e-01&  5.9057e-01&  5.8411e-01\\
		\cline{2-9}
		2500&  3.7887e-02&  6.0630e-02&   5.4363e-02&  5.0659e-02&  2.2212e-01&  2.4774e-01&  5.3681e-01&  3.5427e-01\\
		\cline{2-9}
		3000&  5.4862e-02&  6.3407e-02&   5.5208e-02&  6.0082e-02&  3.4102e-01&  2.1308e-01&  5.1894e-01&  4.4995e-01\\
		\hline
	\end{tabular}
\end{table}

The effect of the number of collocation points ($N$) on the PINN results has been studied in Figure~\ref{PINN_partpaper_Ela_fig2} and Table~\ref{PINN_partpaper_Ela_tab_1}, where $N$ is systematically varied in the range $N=1000$ to $N=3000$. Figure~\ref{PINN_partpaper_Ela_fig2} shows the histories of the loss function for training PINN-H1 and PINN-H2 under different collocation points. Table~\ref{PINN_partpaper_Ela_tab_1} lists the corresponding $l_2$ and $l_{\infty}$ errors of $\bm u$ and $\bm v$ obtained from PINN-H1 and PINN-H2. 
One can observe that the PINN errors in general tend to improve with increasing number of collocation points. It can also be observed that the PINN-H1 errors in general appear better than those of PINN-H2 for this problem.


Figure~\ref{PINN_partpaper_LinearElasticity_errorrate} shows the errors of $\bm u$, $\bm u_t$, $\underline{\bm\varepsilon}(\bm u)$ and
$\nabla\cdot\bm u$ as a function of the loss function value in the network training of PINN-H1 and PINN-H2. The data indicates that these errors approximately scale as the square root of the training loss, which is consistent with the relation as given by Theorem~\ref{sec9_Theorem3}. This in a sense provides numerical evidence for the theoretical analysis in Section~\ref{Elasto-dynamics}.




\begin{figure}[tb]
	%\centering
	%\begin{minipage}[t]{1\linewidth}
	\centering
	\subfloat[PINN-H1]{\includegraphics[width=0.4\linewidth]{./figures/LinearElasticity_epoch-loss1_sqrt_1.pdf}}\qquad
	\subfloat[PINN-H2]{\includegraphics[width=0.4\linewidth]{./figures/LinearElasticity_epoch-loss1_nosqrt_1.pdf}}
	\caption{Linear elastodynamic equation: Training loss histories of PINN-H1 and PINN-H2 corresponding to different numbers of collocation points ($N$) in the simulation.
 %The Log-Log for iterations and loss function errors. 
 }
	\label{PINN_partpaper_Ela_fig2}
	%\end{minipage}
\end{figure}






\begin{figure}[tb]
	\centering
	\subfloat[PINN-H1]{\includegraphics[width=0.4\linewidth]{./figures/LinearElasticity2D_error_ratio_sqrt_1.pdf}}\qquad
	\subfloat[PINN-H2]{\includegraphics[width=0.4\linewidth]{./figures/LinearElasticity2D_error_ratio_nosqrt_1.pdf}}\hspace{0.1em}
	\caption{Linear elastodynamic equation: The errors for $\bm u$, $\bm u_t$, $\underline{\bm\varepsilon}(\bm u)$ and $\nabla\cdot\bm u$ versus the training loss value obtained by PINN-H1 and PINN-H2.
 %The Log-Log for scaling of total error vs. training error
 }
\label{PINN_partpaper_LinearElasticity_errorrate}
\end{figure}
