\section{Introduction}\label{sec:intro}

%2021 JCM: A different strategy might be relevant for such problems, namely the so-called Physics informed neural networks (PINNs) which collocate the PDE residual on training points of the approximating deep neural network, thus obviating the need for generating training data. 

Deep neural networks (DNN) have achieved a great success in a number of fields in science and engineering \cite{LeCun2015DP} such as natural language processing, robotics, computer vision, speech and image recognition, to name but a few. 
This has inspired a great deal of research efforts in the past few years to adapt such techniques to scientific computing.
DNN-based techniques seem particularly promising for problems in higher dimensions, e.g.~high-dimensional partial differential equations (PDE), since
%The need for these studies is inspired by the fact that when using 
traditional numerical methods for high-dimensional problems can quickly 
%in a high-dimensional partial differential equations (PDEs), the methods sometimes 
become infeasible due to the exponential increase in the computational effort (so-called  curse of dimensionality). 
%
Under these circumstances deep-learning algorithms can be helpful. In particular, the neural networks approach for PDE problems provide implicit regularization and can alleviate and perhaps overcome the curse of high dimensions  \cite{Beck2019Machine,Berner2020Analysis}. This approach also provides a natural framework for estimating the unknown parameters \cite{Fang2020NN,Raissi2019pinn,Raissi2018Hidden,Thuerey2020Deep,Wang2017pinn}.

As deep neural networks are universal function approximators, it is natural to employ them as ansatz spaces for solutions of (ordinary or partial) differential equations. This paves the way for their  use in physical modeling and scientific computing and gives rise to the field of scientific machine learning \cite{Karniadakisetal2021,SirignanoS2018,Raissi2019pinn,EY2018,Lu2021DeepXDE}. 
The physics-informed neural network (PINN) approach was introduced in \cite{Raissi2019pinn}. 
%2021  JCM
%which collocate the PDE residual on training points of the approximating deep neural network, thus obviating the need for generating training data. 
It has been successfully applied to a variety of forward and inverse PDE problems and has become one of the most commonly-used  methods in scientific machine learning (see e.g.~\cite{Raissi2019pinn,HeX2019,CyrGPPT2020,JagtapKK2020,WangL2020,JagtapK2020,CaiCLL2020,Tartakovskyetal2020,DongN2021,TangWL2021,DongL2021,CalabroFS2021,WanW2022,FabianiCRS2021,KrishnapriyanGZKM2021,DongY2022,DongY2022rm,WangYP2022,Pateletal2022,DongW2022,Siegeletal2022,HuLWX2022,Penwardenetal2023}, among others). 
The references \cite{Karniadakisetal2021,Cuomo2022Scientific} provide a comprehensive review of the literature on PINN and about the  benefits and drawbacks of this approach.

%2020 (CCP) On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs.pdf
The mathematical foundation for PINN aiming at the approximation of PDE solution is currently an active area of research. It is important to account for different components of the neural-network error: optimization error, approximation error, and estimation error~\cite{Niyogi1999Generalization,Shin2020On}. 
%Optimization error arises from the minimization problem. 
%arxiv (2209.02977) Error Estimates and Physics Informed Augmentation of Neural Networks for Thermally Coupled Incompressible Navier Stokes Equations.pdf
Approximation error refers to the discrepancy between the exact functional map and the neural network mapping function on a given network architecture~\cite{Calin2020Deep,Elbrachter2021deep}. 
%mathematical foundations results are generally discussed in papers deeply focused on this topic work . 
Estimation error arises when the network is trained on a finite data set to get a mapping on the target domain. The generalization error is the combination of approximation and estimation errors and defines the accuracy of the neural-network predicted solution trained on the given set of data. 

% arXiv (2203.09346) Error estimates for physics informed neural networks approximating the Navier-Stokes equations +
%2022 JSC
Theoretical understanding of PINN has been advanced by a number of recent works. 
In~\cite{Shin2020On} Shin et al. rigorously justify why PINN works and shows its consistency for linear elliptic and parabolic PDEs under certain assumptions. %, and provide convergence estimate of residual minimization for linear second-order elliptic and parabolic type PDEs.
These results are extended in \cite{Shin2010.08019} to a general abstract framework for analyzing PINN for linear problems with the loss function formulated in terms of the strong or weak forms of the equations. In \cite{Mishra2022Estimates} Mishra and Molinaro provide an abstract framework on PINN for forward PDE problems, and estimate the generalization error by means of the training error and the number of training data points. This framework is extended in~\cite{Mishra2022inverse} to study several inverse PDE problems, including the Poisson, heat, wave and Stokes equations. Bai and Koley \cite{Bai2021PINN} investigate the PINN approximation of nonlinear dispersive PDEs such as the KdV-Kawahara, Camassa-Holm and Benjamin-Ono equations. In~\cite{Biswas2022Error} Biswa et al.~provide explicit error estimates (in suitable norms) and stability analyses for the incompressible Navier–Stokes equations. Zerbinati~\cite{Zerbinati2022pinns} presents PINN as an under-determined point matching collocation method, reveals its connection with Galerkin Least Squares (GALS) method, and establishes an a priori error estimate for elliptic problems. 

An important theoretical result on the approximation errors from the recent work~\cite{DeRyck2021On} establishes  that a feed-forward neural network $\hat{u}_{\theta }$ with a $\tanh$ activation function and two hidden layers may approximate a function $u$ with a bound in a Sobolev space,
\[
\|\hat{u}_{\theta N}-u\|_{w^{k,\infty}}\leq C{\rm ln}(cN)^k/N^{s-k}. 
\]
Here $u \in w^{s,\infty}([0,1]^d)$, $d$ is the dimension of the problem, $N$ is the number of training points, and $c, C > 0$ are explicitly known constants independent of $N$. 
%{\color{red}[Yanxia, what is the $s$ here? $u \in w^{s,\infty}$]}
%Here, we note that the NN has a width of $N^d$, and $\theta$ depends on both $N$ and $d$. 
%On the basis of \cite{DeRyck2021On}, 
Based on this result,
De Ryck et al.~\cite{2023_IMA_Mishra_NS} have studied the PINN for the Navier–Stokes equations and shown that a small training error implies a small generalization error.
%bounds the generalization and training errors, and then establish the convergence of PINN to the exact solution of Navier–Stokes equation:
%\[
%R[\hat{u}_{\theta}]=\|\hat{u}_{\theta}-u\|_{L^2}\leq \sqrt{CR[\hat{u}] + \mathcal{O}(N^{-1/d})}. wrong 
%\]
In particular, Hu et al.~\cite{Ruimeng2209.11929} provide the higher-order (spatial Sobolev norm) error estimates for the primitive equations, which improve the existing results in the PINN literature that only involve $L^2$ errors.
%
In~\cite{DeRyck2022Estimates} it has been shown that,
with a sufficient number of  randomly chosen training points,
the total $L^2$ error can be bounded by the generalization error for Kolmogorov-type PDEs, which in turn is bounded by the training error. It is proved that the size of the PINN and the number of training samples only increase polynomially with the problem dimension, thus enabling PINN to overcome the curse of dimensionality in this case. 
In~\cite{Mishra2021pinn} the authors
%For high-dimensionality problems, Mishra and Molinaro \cite{Mishra2021pinn} 
investigate the high-dimensional radiative transfer equation and prove that the generalization error is bounded by the training error and the number of training points, where the upper bound depends on the dimension only through 
%a the dimensional dependence of the upper bound is only 
a logarithmic factor. 
%\[
%R[\hat{u}_{\theta}]\leq \sqrt{CR[\hat{u}]^2 + c({\rm ln}N)^{2d}/N}.
%\]
Hence PINN does not suffer from the curse of dimensionality, provided that the training errors do not depend on the underlying dimension. 

Although PINN has been widely used for approximating PDEs, theoretical investigations on its convergence and errors are still quite limited and are largely confined to elliptic and parabolic PDEs. 
%As far as we know, 
There seems to be less (or little) theoretical analysis on the convergence of PINN for hyperbolic type PDEs. 
%Due to the nature of hyperbolic equations, they are more difficult to study than the elliptic and parabolic equations. 
In this paper, we consider a class of 
dynamic PDEs of second order in time, which are hyperbolic in nature, and provide an analysis of the convergence and errors of the PINN algorithm applied to such problems. 
We have focused on the wave equation, the Sine-Gordon equation and the linear elastodynamic equation in our analyses. 
Building upon the result of~\cite{DeRyck2021On,2023_IMA_Mishra_NS} on $\tanh$ neural networks with two hidden layers, we have shown that for these three kinds of PDEs: 
\begin{itemize}
\item
The underlying PDE residuals in PINN can be made arbitrarily small with $\tanh$ neural networks having two hidden layers. 
\item
The total error of the PINN approximation is bounded by the generalization error of PINN.
\item
The total error of PINN approximations for the solution field, its time derivative and its gradient is bounded by the training error (training loss) of PINN and the number of quadrature points (training data points).
%using a Gauss quadrature rule. 
\end{itemize}
% 2022 NM

% new numerical algorithms for PINN 
% and numerical simulations

Furthermore, our theoretical analyses have suggested PINN training loss functions for these PDEs that are somewhat different in form than from the canonical PINN formulation. 
These lie in two aspects: (i) Our analyses require certain residual terms (such as the gradient of the initial condition, the time derivative of the boundary condition, or in the case of linear elastodynamic equation the strain and divergence of the initial condition) in the training loss, which would be absent from the canonical PINN formulation of the loss function. (ii) Our analyses may require, depending on the type of boundary conditions, a norm other than the $L^2$ norm for certain boundary residuals in the training loss, which is different from the commonly-used $L^2$ norm in the canonical PINN formulation of the loss function.

These new forms for the training loss function suggested by the theoretical analyses lead to a variant PINN algorithm. We have implemented the PINN algorithm based on these new forms of the training loss function for the wave equation, the Sine-Gordon equation and the linear elastodynamic equation. Ample numerical experiments based on this algorithm have been presented. The simulation results indicate that the method has captured the solution field reasonably well for these PDEs. The numerical results also to some extent corroborate the theoretical relation between the approximation error and the PINN training loss obtained from the error analysis.

The rest of this paper is organized as follows. In Section \ref{PINN} we present an overview of PINN for dynamic PDEs of second order in time. In Sections~\ref{Wave}, \ref{Sine-Gordon} and \ref{Elasto-dynamics}, we present an error analysis of the PINN algorithm for approximating the wave equation, Sine-Gordon equation, and the linear elastodynamic equation, respectively. 
Section~\ref{numerical examples} summarizes a set of
numerical experiments with these three PDEs to supplement and support our theoretical analyses. Section~\ref{Conclusion} concludes the presentation with some closing remarks.
Finally, the appendix (Section~\ref{Appendix}) recalls some auxiliary results for our analysis and provides the proofs of the main theorems in Sections~\ref{Sine-Gordon} and~\ref{Elasto-dynamics}.
