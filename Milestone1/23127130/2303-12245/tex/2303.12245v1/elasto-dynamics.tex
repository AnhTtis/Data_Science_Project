\section{Physics Informed Neural Networks for Approximating  Linear Elastodynamic Equation}\label{Elasto-dynamics}
%\setcounter{equation}{0}
\subsection{Linear Elastodynamic Equation}

Consider an elastic body occupying an open, bounded convex polyhedral domain $D\subset \mathbb{R}^d$. The boundary $\partial D = \Gamma_D\cup \Gamma_N$, with the outward  unit normal vector $\bm{n}$, is assumed to be composed of two disjoint portions $\Gamma_D\neq \emptyset$ and $\Gamma_N$, with $\Gamma_D\cap \Gamma_N=\emptyset$. Given a suitable external load $\bm{f} \in L^2((0,T];\bm{L}^2(D))$, and suitable initial/boundary data $\bm{g} \in C^1((0,T];\bm{H}^{\frac{1}{2}}(\Gamma_N))$, $\bm{\psi}_{1}\in \bm{H}_{0,\Gamma_D}^{\frac{1}{2}}(D)$ and $\bm{\psi}_{2}\in \bm{L}^2(D)$, we consider the linear elastodynamic equations, 
\begin{subequations}\label{elast}
	\begin{align}
		\label{elast_eq0}
		&\bm{u}_{t} - \bm{v} = 0  \ \quad\quad\qquad\qquad\qquad\qquad\qquad\quad\ \ \, \text{in}\ D\times [0,T],\\
		\label{elast_eq1}
		&\rho\bm{v}_{t} - 2\mu\nabla\cdot(\underline{\bm{\varepsilon}}(\bm{u})) -\lambda\nabla(\nabla\cdot\bm{u})= \bm{f}  \, \quad\qquad\text{in}\ D\times [0,T],\\
		\label{elast_eq2}
		&\bm{u}=\bm{u}_{d} \ \ \, \quad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \text{in}\ \Gamma_D\times [0,T],\\
		\label{elast_eq3}
		&2\mu\underline{\bm{\varepsilon}}(\bm{u})\bm{n} +\lambda(\nabla\cdot\bm{u})\bm{n}=\bm{g}\ \ \quad\qquad\qquad\qquad \text{in}\ \Gamma_N\times [0,T],\\
		\label{elast_eq4}
		&\bm{u}=\bm{\psi}_{1} \ \  \quad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \, \text{in}\ D\times\{0\},\\
		\label{elast_eq5}
		&\bm{v}=\bm{\psi}_{2} \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \text{in}\ D\times\{0\}.
	\end{align}
\end{subequations}
In the above system, $\bm{u}=(u_1,u_2,\cdots,u_d)$ and $\bm{v}=(v_1,v_2,\cdots,v_d)$ denote the displacement and the velocity, respectively, and $[0,T]$ (with $T>0$) denotes the time domain. $\underline{\bm{\varepsilon}}(\bm{u})$ is the strain tensor, $\underline{\bm{\varepsilon}}(\bm{u})=\frac{1}{2}(\nabla\bm{u}+\nabla\bm{u}^T)$. The constants $\lambda$ and $\mu$ are the first and the second Lam${\rm \acute{e}}$ parameters, respectively.  

Combining \eqref{elast_eq0} and \eqref{elast_eq1}, we can recover the classical linear elastodynamics equation:
\begin{equation}\label{elast1}
	\rho\bm{u}_{tt} - 2\mu\nabla\cdot(\underline{\bm{\varepsilon}}(\bm{u})) -\lambda\nabla(\nabla\cdot\bm{u})= \bm{f} \qquad\text{in}\ D\times [0,T].
\end{equation}
The well-posedness of this equation is established in~\cite{Hughes1978Classical}.

\begin{Lemma}[\cite{Hughes1978Classical,Yosida1980Functional}]\label{sec9_Lemma1} 
Let $\bm{\psi}_{1}\in H^{r}(D)$, $\bm{\psi}_{2}\in H^{r-1}(D)$ and $\bm{f}\in H^{r-1}(D\times [0,T])$ with $r\geq1$. Then there exists a unique solution $\bm{u}$ to the classical linear elastodynamic equation \eqref{elast1} such that $\bm{u}(t = 0) =\bm{\psi}_{1}$, $\bm{u}_t(t = 0) = \bm{\psi}_{2}$ and $\bm{u} \in C^l([0,T];H^{r-l}(D))$ with $0\leq l \leq r$.
\end{Lemma}

\begin{Lemma}\label{sec9_Lemma2} Let $k\in \mathbb{N}$, $\bm{\psi}_{1}\in H^{r}(D)$, $\bm{\psi}_{2}\in H^{r-1}(D)$ and $\bm{f}\in H^{r-1}(D\times [0,T])$ with $r>\frac{d}{2}+k$, then there exists $T>0$ and a classical solution $(\bm{u},\bm{v})$ to the elastodynamic equations \eqref{elast}  such that $\bm{u}(t = 0) =\bm{\psi}_{1}$, $\bm{u}_t(t = 0) = \bm{\psi}_{2}$, $\bm{u} \in C^k(D\times [0,T])$ and $\bm{v} \in C^{k-1}(D\times [0,T])$.
\end{Lemma}
\begin{proof}  As $r>\frac{d}{2}+k$, $H^{r-k}(D)$ is a Banach algebra. By Lemma \ref{sec9_Lemma1}, there exists $T > 0$ and the solution $(\bm{u}, \bm{v})$ to the linear elastodynamics equations such that $\bm{u}(t = 0) =\bm{\psi}_{1}$, 
	$\bm{v}(t = 0) = \bm{\psi}_{2}$, $\bm{u} \in C^l([0,T];H^{r-l}(D))$ with $0\leq l \leq r$ and $\bm{v} \in C^l([0,T];H^{r-1-l}(D))$ with $0\leq l \leq r-1$. 
	
	Since $\bm{u} \in \cap_{l=0}^k C^l([0,T];H^{r-l}(D))$ and $\bm{v} \subset \cap_{l=0}^{k-1} C^l([0,T];H^{r-l-1}(D))$. By applying the Sobolev embedding theorem and $r>\frac{d}{2}+k$, we obtain $H^{r-l}(D) \subset C^{r-l}(D)$ and $H^{r-l-1}(D) \subset C^{r-l-1}(D)$ for $0\leq l\leq k$. Therefore, $\bm{u} \in C^k(D\times [0,T])$ and $\bm{v} \in C^{k-1}(D\times [0,T])$.
\end{proof} 

\subsection{Physics Informed Neural Networks}

%In this subsection, physics-informed neural networks for approximating solutions of the linear elastodynamics equation \eqref{elast} and training set will be given. 
We now consider the PINN approximation of the linear elastodynamic equations \eqref{elast}.
Let $\Omega = D\times [0,T]$, %$D_T = D \times(0,T)$, 
$\Omega_{D} = \Gamma_D\times [0,T]$ and $\Omega_{N} = \Gamma_N\times [0,T]$ denote the space-time domain. 
%Seek deep neural networks $\bm{u}_{\theta} : \Omega \rightarrow \mathbb{R}$, parameterized by $\theta \in \Theta$, constituting the weights and biases, that approximate the solution $u$ of \eqref{elast}. 
Define the following residuals for the PINN approximation $\bm{u}_{\theta}:\Omega \rightarrow \mathbb{R}$ and $\bm{v}_{\theta}: \Omega \rightarrow \mathbb{R}$ for the elastodynamic equations \eqref{elast}:
\begin{subequations}\label{elast_pinn}
	\begin{align}
		\label{elast_pinn_eq1}
		&\bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x},t) =\bm{u}_{\theta t}-\bm{v}_{\theta},\\
		\label{elast_pinn_eq2}
		&\bm{R}_{int2}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x},t) =\rho \bm{v}_{\theta t}-2\mu\nabla\cdot(\underline{\bm{\varepsilon}}(\bm{u}_{\theta})) -\lambda\nabla(\nabla\cdot\bm{u}_{\theta})-\bm{f},\\
		\label{elast_pinn_eq3}
		&\bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x}) =\bm{u}_{\theta}(\bm{x}, 0)-\bm{\psi}_{1}(\bm{x}),\\
		\label{elast_pinn_eq4}
		&\bm{R}_{tb2}[\bm{v}_{\theta}](\bm{x}) =\bm{v}_{\theta}(\bm{x}, 0)-\bm{\psi}_{2}(\bm{x}),\\
		\label{elast_pinn_eq5}
		&\bm{R}_{sb1}[\bm{v}_{\theta}](\bm{x},t) =\bm{v}_{\theta}|_{\Gamma_D}-\bm{u}_{dt},\\
		\label{elast_pinn_eq6}
		&\bm{R}_{sb2}[\bm{u}_{\theta}](\bm{x},t) =(2\mu\underline{\bm{\varepsilon}}(\bm{u}_{\theta})\bm{n} +\lambda(\nabla\cdot\bm{u}_{\theta})\bm{n})|_{\Gamma_N}-\bm{g}.
	\end{align}
\end{subequations}
Note that for the exact solution $(\bm{u},\bm{v})$,  we have $\bm{R}_{int1}[\bm{u},\bm{v}]=\bm{R}_{int2}[\bm{u},\bm{v}]=\bm{R}_{tb1}[\bm{u}]=\bm{R}_{tb2}[\bm{v}]=\bm{R}_{sb1}[\bm{v}]=\bm{R}_{sb2}[\bm{u}]=0$. 
With PINN we minimize the the following generalization error,
\begin{align}\label{elast_G}
	\mathcal{E}_G(\theta)^2&=\int_{\Omega}|\bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|\bm{R}_{int2}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x},t)|^2\dx\dt+\int_{\Omega}|\underline{\bm{\varepsilon}}(\bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x},t))|^2\dx\dt
	\nonumber\\
	&+\int_{\Omega}|\nabla\cdot(\bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x},t))|^2\dx\dt+\int_{D}|\bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x})|^2\dx+\int_{D}|\bm{R}_{tb2}[\bm{v}_{\theta}](\bm{x})|^2\dx
	\nonumber\\
	&
	+\int_{D}|\underline{\bm{\varepsilon}}(\bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x}))|^2\dx+\int_{D}|\nabla\cdot \bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x})|^2\dx
	\nonumber\\
	&+\left(\int_{\Omega_D}|\bm{R}_{sb1}[\bm{v}_{\theta}](\bm{x},t)|^2\ds\dt\right)^{\frac{1}{2}}
	+\left(\int_{\Omega_N}|\bm{R}_{sb2}[\bm{u}_{\theta}](\bm{x},t)|^2\ds\dt\right)^{\frac{1}{2}}.
\end{align}

Let 
\begin{equation*}
\hat{\bm{u}} = \bm{u}_{\theta}-\bm{u}, \quad \hat{\bm{v}} = \bm{v}_{\theta}-\bm{v}
\end{equation*}
denote the difference between the solution to the elastodynamic equations~\eqref{elast} and the PINN approximation with parameter $\theta$. We define the total error of the PINN approximation as,
\begin{equation}\label{elast_total}
	\mathcal{E}(\theta)^2=\int_{\Omega}( |\hat{\bm{u}}(\bm{x},t)|^2+2\mu|\underline{\bm{\varepsilon}}(\hat{\bm{u}}(\bm{x},t))|^2
	+\lambda|\nabla\cdot\hat{\bm{u}}(\bm{x},t)|^2+\rho|\hat{\bm{v}}(\bm{x},t)|^2)\dx\dt.
\end{equation}

We choose the training set $\mathcal{S} \subset \overline{D}\times [0,T]$ based on suitable quadrature points. The full training set is defined by $\mathcal{S} = \mathcal{S}_{int} \cup \mathcal{S}_{sb} \cup \mathcal{S}_{tb}$, and $\mathcal{S}_{sb}=\mathcal{S}_{sb1}\cup \mathcal{S}_{sb2}$:
%$\bullet$
\begin{itemize}
    \item Interior training points $\mathcal{S}_{int}=\{{z}_n\}$ for $1\leq n \leq N_{int}$, with each ${z}_n= (\bm{x},t)_n \in D \times(0,T)$. 
    
    \item Spatial boundary training points $\mathcal{S}_{sb1}=\{{z}_n\}$ for $1\leq n \leq N_{sb1}$, with each ${z}_n= (\bm{x},t)_n \in \Gamma_D \times (0,T)$, and $\mathcal{S}_{sb2}=\{{z}_n\}$ for $1\leq n \leq N_{sb2}$, with each ${z}_n= (\bm{x},t)_n \in \Gamma_N\times (0,T)$.
 
	\item Temporal boundary training points $\mathcal{S}_{tb}=\{\bm{x}_n\}$ for $1\leq n \leq N_{tb}$ with  each $\bm{x}_n \in D$.%, chosen either as grid points, low-discrepancy sequences or randomly chosen in $D$.
	%\vspace{-0.2em}
\end{itemize}

Then, the integrals in \eqref{elast_G} can be approximated by a suitable numerical quadrature, resulting in the following training loss,
\begin{align}\label{elast_T}
	\mathcal{E}_T(\theta,\mathcal{S})^2&
	=\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{int4}(\theta,\mathcal{S}_{int})^2+\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2
	\nonumber\\
	&\quad
	+\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 +\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2
	+\mathcal{E}_T^{tb4}(\theta,\mathcal{S}_{tb})^2
	+\mathcal{E}_T^{sb1}(\theta,\mathcal{S}_{sb1}) 
	+\mathcal{E}_T^{sb2}(\theta,\mathcal{S}_{sb2}),
\end{align}
where, 
\begin{subequations}\label{elast_TT}
	\begin{align}
		\label{elast_T1}
		\mathcal{E}_T^{int1}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{elast_T2}
		\mathcal{E}_T^{int2}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\bm{R}_{int2}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{elast_T3}
		\mathcal{E}_T^{int3}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\underline{\bm{\varepsilon}}(\bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x}_{int}^n,t_{int}^n))|^2,\\
		\label{elast_T33}
		\mathcal{E}_T^{int4}(\theta,\mathcal{S}_{int})^2 &= \sum_{n=1}^{N_{int}}\omega_{int}^n|\nabla\cdot \bm{R}_{int1}[\bm{u}_{\theta},\bm{v}_{\theta}](\bm{x}_{int}^n,t_{int}^n)|^2,\\
		\label{elast_T4}
		\mathcal{E}_T^{tb1}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{elast_T5}
		\mathcal{E}_T^{tb2}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\bm{R}_{tb2}[\bm{v}_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{elast_T6}
		\mathcal{E}_T^{tb3}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\underline{\bm{\varepsilon}}(\bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x}_{tb}^n))|^2,\\
		\label{elast_T66}
		\mathcal{E}_T^{tb4}(\theta,\mathcal{S}_{tb})^2 &= \sum_{n=1}^{N_{tb}}\omega_{tb}^n|\nabla\cdot \bm{R}_{tb1}[\bm{u}_{\theta}](\bm{x}_{tb}^n)|^2,\\
		\label{elast_T7}
		\mathcal{E}_T^{sb1}(\theta,\mathcal{S}_{sb1})^2&= \sum_{n=1}^{N_{sb1}}\omega_{sb1}^n|\bm{R}_{sb1}[\bm{v}_{\theta}](\bm{x}_{sb}^n,t_{sb}^n)|^2,\\
		\label{elast_T8}
		\mathcal{E}_T^{sb2}(\theta,\mathcal{S}_{sb2})^2&= \sum_{n=1}^{N_{sb2}}\omega_{sb2}^n|\bm{R}_{sb2}[\bm{u}_{\theta}](\bm{x}_{sb}^n,t_{sb}^n)|^2.
	\end{align}
\end{subequations}
Here the quadrature points in space-time constitute the data sets $\mathcal{S}_{int} = \{(\bm{x}_{int}^n,t_{int}^n)\}_{n=1}^{N_{int}}$, $\mathcal{S}_{tb} = \{\bm{x}_{tb}^n)\}_{n=1}^{N_{tb}}$, $\mathcal{S}_{sb1} = \{(\bm{x}_{sb1}^n,t_{sb1}^n)\}_{n=1}^{N_{sb1}}$ and $\mathcal{S}_{sb2} = \{(\bm{x}_{sb2}^n,t_{sb2}^n)\}_{n=1}^{N_{sb2}}$. $\omega_{\star}^n$ denote the suitable quadrature weights with $\star$ being $int$, $tb$, $sb1$ and $sb2$.

\subsection{Error Analysis}  %estimate on the generalization error


Subtracting the elastodynamic equations \eqref{elast} from the residual equations~\eqref{elast_pinn}, we obtain
\begin{subequations}\label{elast_error}
	\begin{align}
		\label{elast_error_eq1}
		&\bm{R}_{int1}=\hat{\bm{u}}_t-\hat{\bm{v}},\\
		\label{elast_error_eq2}
		&\bm{R}_{int2}=\rho\hat{\bm{v}}_t-2\mu\nabla\cdot(\underline{\bm{\varepsilon}}(\hat{\bm{u}})) -\lambda\nabla(\nabla\cdot\hat{\bm{u}}),\\
		\label{elast_error_eq3}
		&\bm{R}_{tb1}=\hat{\bm{u}}|_{t=0},\\
		\label{elast_error_eq4}
		&\bm{R}_{tb2}=\hat{\bm{v}}|_{t=0},\\
		\label{elast_error_eq5}
		&\bm{R}_{sb1}=\hat{\bm{v}}|_{\Gamma_D},\\
		\label{elast_error_eq6}
		&\bm{R}_{sb2}=(2\mu\underline{\bm{\varepsilon}}(\hat{\bm{u}})\bm{n} +\lambda(\nabla\cdot\hat{\bm{u}})\bm{n})|_{\Gamma_N}.
	\end{align}
\end{subequations}	
%
%\iffalse 
The PINN approximation results are summarized in the following three theorems. The proofs of these theorems are provided in the Appendix~\ref{Proof_elasto}.

\begin{Theorem}\label{sec9_Theorem1} 
		Let $d$, $r$, $k \in \mathbb{N}$ with $k\geq 3$. Let $\bm{\psi}_{1}\in H^{r}(D)$, $\bm{\psi}_{2}\in H^{r-1}(D)$ and $\bm{f}\in H^{r-1}(D\times [0,T])$ with $r>\frac{d}{2}+k$. 
		For every integer $N>5$, there exist $\tanh$ neural networks $(\bm{u}_j)_{\theta}$ and $(\bm{v}_j)_{\theta}$, with $j=1,2,\cdots,d$, each with two hidden layers, of widths at most $3\lceil\frac{k}{2}\rceil|P_{k-1,d+2}| + \lceil NT\rceil+ d(N-1)$ and $3\lceil\frac{d+3}{2}\rceil|P_{d+2,d+2}| \lceil NT\rceil N^d$, such that
	\begin{subequations}
		\begin{align}
			\label{lem9.1}
			&\|\bm{R}_{int1}\|_{L^2(\Omega)},\|\bm{R}_{tb1}\|_{L^2(\Omega)}\lesssim {\rm ln}NN^{-k+1},\\
			\label{lem9.2}
			&\|\bm{R}_{int2}\|_{L^2(\Omega)},\|\underline{\bm{\varepsilon}}(\bm{R}_{int1})\|_{L^2(\Omega)},\|\nabla\cdot\bm{R}_{int1}\|_{L^2(\Omega)}\lesssim {\rm ln}^2NN^{-k+2},\\
			\label{lem9.3}
			&\|\underline{\bm{\varepsilon}}(\bm{R}_{tb1})\|_{L^2(D)},\|\nabla\cdot\bm{R}_{tb1}\|_{L^2(D)},\|\bm{R}_{sb2}\|_{L^2(\Gamma_N\times [0,t])}\lesssim {\rm ln}^2NN^{-k+2},\\
			\label{lem9.4}
			&\|\bm{R}_{tb2}\|_{L^2(D)}, \|\bm{R}_{sb1}\|_{L^2(\Gamma_D\times [0,t])}\lesssim {\rm ln}NN^{-k+2}.
		\end{align}
	\end{subequations}
\end{Theorem}

It follows from Theorem \ref{sec9_Theorem1} that,
%However, 
by choosing a sufficiently large $N$, %Theorem \ref{sec9_Theorem1} displays that 
one can make the PINN residuals in~\eqref{elast_pinn}, and thus  the generalization error $\mathcal{E}_G(\theta)^2$ in~\eqref{elast_G}, arbitrarily small. 

\begin{Theorem}\label{sec9_Theorem2} Let $d\in \mathbb{N}$, $\bm{u} \in C^1(\Omega)$ and $\bm{v}\in C(\Omega)$ be the classical solution to the linear elastodynamic equation~\eqref{elast}. Let $(\bm{u}_{\theta},\bm{v}_{\theta})$ denote the PINN approximation with the parameter $\theta$. Then the following relation holds,
%total error is bounded as follows,
	\begin{equation*}
		\int_0^{T}\int_{D}( |\hat{\bm{u}}(\bm{x},t)|^2+2\mu|\underline{\bm{\varepsilon}}(\hat{\bm{u}}(\bm{x},t))|^2
		+\lambda|\nabla\cdot\hat{\bm{u}}(\bm{x},t)|^2+\rho|\hat{\bm{v}}(\bm{x},t)|^2)\dx\dt
		\leq C_GT\exp\left((2+2\mu+\lambda)T\right),
	\end{equation*}
	where
	\begin{align*}
		&C_G=\int_{D}|\bm{R}_{tb1}|^2\dx+\int_{D}2\mu|\underline{\bm{\varepsilon}}(\bm{R}_{tb1})|^2\dx+\int_{D}\lambda|\nabla\cdot \bm{R}_{tb1}|^2\dx+\rho\int_{D}|\bm{R}_{tb2}|^2\dx\\
		&\qquad+\int_{0}^{T}\int_{D}\left(|\bm{R}_{int1}|^2+2\mu|\underline{\bm{\varepsilon}}(\bm{R}_{int1})|^2+\lambda|\nabla\cdot \bm{R}_{int1}|^2+|\bm{R}_{int2}|^2\right)\dx\dt\\
		&\qquad +2|T|^{\frac{1}{2}}C_{\Gamma_D}\left(\int_{0}^{T}\int_{\Gamma_D}|\bm{R}_{sb1}|^2\ds\dt\right)^{\frac{1}{2}}+2|T|^{\frac{1}{2}}C_{\Gamma_N}\left(\int_{0}^{T}\int_{\Gamma_N}|\bm{R}_{sb2}|^2\ds\dt\right)^{\frac{1}{2}},
	\end{align*}
	with $C_{\Gamma_D}=(2\mu+\lambda)|\Gamma_D|^{\frac{1}{2}}\|\bm{u}\|_{C^1(\Gamma_D\times [0,T])}+(2\mu+\lambda)|\Gamma_D|^{\frac{1}{2}}||\bm{u}_{\theta}||_{C^1(\Gamma_D\times [0,T])}$ and $C_{\Gamma_N}=|\Gamma_N|^{\frac{1}{2}}(\|\bm{v}\|_{C(\Gamma_N\times [0,T])}+||\bm{v}_{\theta}||_{C(\Gamma_N\times [0,T])})$.
\end{Theorem}
%The proof of this theorem is provided in Appendix \ref{Proof_elasto}. 
 Theorem \ref{sec9_Theorem2} shows that the total error of the PINN approximation $\mathcal{E}(\theta)^2$ can be controlled by the generalization error $\mathcal{E}_G(\theta)^2$.

\begin{Theorem}\label{sec9_Theorem3} Let $d\in \mathbb{N}$, $\bm{u}\in C^4(\Omega)$ and $\bm{v}\in C^3(\Omega)$ be the classical solution to
	the linear elastodynamic equation~\eqref{elast}. Let $(\bm{u}_{\theta},\bm{v}_{\theta})$ denote the PINN approximation with the parameter $\theta$. Then the following relation holds,
 %total error is bounded as follows,
  \begin{align}\label{lem9.9}
	&\int_0^{T}\int_{D}( |\hat{\bm{u}}(\bm{x},t)|^2+2\mu|\underline{\bm{\varepsilon}}(\hat{\bm{u}}(\bm{x},t))|^2
	+\lambda|\nabla\cdot\hat{\bm{u}}(\bm{x},t)|^2+\rho|\hat{\bm{v}}(\bm{x},t)|^2)\dx\dt
	\leq C_TT\exp\left((2+2\mu+\lambda)T\right)
		\nonumber\\
	&\qquad=\mathcal{O}(\mathcal{E}_T(\theta)^2 + M_{int}^{-\frac{2}{d+1}} +M_{tb}^{-\frac{2}{d}}+M_{sb}^{-\frac{1}{d}}),        
  \end{align}
	where 
	\begin{align*}
		C_T=&C_{({\bm{R}_{tb1}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(\bm{R}_{tb1}^2)
		+\rho\left(C_{({\bm{R}_{tb2}^2})}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(\bm{R}_{tb2}^2)\right)
		+ 2\mu\left(C_{(|\underline{\bm{\varepsilon}}(\bm{R}_{tb1})|^2)}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(|\underline{\bm{\varepsilon}}(\bm{R}_{tb1})|^2)\right)\\
		&+\lambda\left(C_{(|\nabla\cdot \bm{R}_{tb1}|^2)}M_{tb}^{-\frac{2}{d}}+\mathcal{Q}_{M_{tb}}^{D}(|\nabla\cdot \bm{R}_{tb1}|^2)\right)+C_{({\bm{R}_{int1}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(\bm{R}_{int1}^2)\\
		&+ C_{({\bm{R}_{int2}^2})}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(\bm{R}_{int2}^2)
		+2\mu\left(C_{(|\underline{\bm{\varepsilon}}(\bm{R}_{int1})|^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(|\underline{\bm{\varepsilon}}(\bm{R}_{int1})|^2)\right)\\
		&+\lambda\left(C_{(|\nabla\cdot\bm{R}_{int1}|^2)}M_{int}^{-\frac{2}{d+1}}+\mathcal{Q}_{M_{int}}^{\Omega}(|\nabla\cdot\bm{R}_{int1}|^2)\right)
		+ 2|T|^{\frac{1}{2}}C_{\Gamma_D}\left(C_{({\bm{R}_{sb1}^2})}M_{sb1}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb1}}^{\Omega_D}(\bm{R}_{sb1}^2)\right)^{\frac{1}{2}}\\
		&+2|T|^{\frac{1}{2}}C_{\Gamma_N}\left(C_{({\bm{R}_{sb2}^2})}M_{sb2}^{-\frac{2}{d}}+\mathcal{Q}_{M_{sb2}}^{\Omega_N}(\bm{R}_{sb2}^2)\right)^{\frac{1}{2}}.
	\end{align*}
\end{Theorem}
Theorem \ref{sec9_Theorem3} shows that the PINN approximation error $\mathcal{E}(\theta)^2$ can be controlled by the training error $\mathcal{E}_T(\theta,\mathcal{S})^2$ with a large enough sample set $\mathcal{S}$.
