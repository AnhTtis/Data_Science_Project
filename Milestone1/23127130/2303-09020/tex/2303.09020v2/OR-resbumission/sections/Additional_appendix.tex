% \dkreplace{\section{Additional Details of ABM Experiments}}{
\subsection{Additional Details of ABM Experiments}

In this section, we provide additional details about our agent-based model experiments under the categorical model. This includes how we simplify the threshold acceptance policy in the categorical model and how we learn the model parameters from real data.

%\subsection{Simplified Threshold Acceptance Policy}
\subsubsection{Simplified Threshold Acceptance Policy}

\label{app:simplified_threshold}

In the categorical model, a threshold $\tau$ may not uniquely determine a policy, namely, when there is a combination of reviewer signals which occurs with positive probability and induces posterior expected quality exactly $\tau$. 
For convenience of notation and visualization, instead of specifying the additional parameter $r$, we use the following convention to associate a unique policy $\ACCMAP[\tau]$ with each $\tau \in \R$.
Let $\min(\QualSet) \leq U(\RevSigV[1]) < \cdots <  U(\RevSigV[M]) \leq \max(\QualSet)$ be the expected posterior qualities of all possible combinations of review signals that occur with positive probability.
Note that the assumption that all inequalities between $U(\RevSigV[1])$ and $U(\RevSigV[M])$ are strict is basically without loss of generality. If there are multiple review vectors with the same expected posterior quality, our proofs can be adjusted by replacing one vector \RevSigV[i] with the set of all vectors giving rise to the same $U(\RevSigV[i])$. This change is merely syntactic.
\begin{enumerate}
\item If $\tau <  U(\RevSigV[1])$, accept everything.
\item If $\tau \geq \max(\QualSet)$, reject everything.
\item If $ U(\RevSigV[M]) \leq \tau < \max(\QualSet)$, accept papers of posterior expected quality $U(\RevSigV[M])$ with probability $\frac{\max(\QualSet) - \tau}{\max(\QualSet) -  U(\RevSigV[M])}$ and reject all other papers.
\item For the intermediate cases $\tau \in [ U(\RevSigV[1]),  U(\RevSigV[M]))$,
let $i$ be such that $\tau \in [ U(\RevSigV[i]),  U(\RevSigV[i+1]))$. Then, we interpret a threshold of $\tau$ as the policy which accepts all papers of expected posterior quality at least $U(\RevSigV[i+1])$, rejects all papers of expected posterior quality strictly less than $U(\RevSigV[i])$, and accepts papers of expected posterior quality exactly $U(\RevSigV[i])$ with probability $\frac{U(\RevSigV[i+1]) - \tau}{ U(\RevSigV[i+1]) -  U(\RevSigV[i])}$.
\end{enumerate}


%\subsection{Learning Parameters of the Categorical Model from Data}
\subsubsection{Learning Parameters of the Categorical Model from Data}
\label{sec:learning-parameters}

We set the parameters of our model based on the OpenReview datasets of submissions and reviews for ICLR 2020 \citep{iclr2020review} and ICLR 2021 \citep{iclr2021review}. The datasets contain about $1500$ and $2500$ submissions, respectively; typically, each submission is reviewed thrice, with scores in $\SigSet = \SET{0,1,\dots,9}$.
We apply the same learning algorithm to each of the two datasets separately, yielding two plausible parameter settings for evaluation.

\emph{The Number of Paper Quality Scores and Signals.}
While the set $\SigSet = \SET{0,1,\dots,9}$ of available review scores is known, the number (or set) of different paper qualities is not. Thus, our goal is to simultaneously learn the number of paper qualities, the paper quality distribution, and the distribution of reviewer signals conditioned on the paper's quality.

To do so, we exhaustively try all numbers $\QualSetSize$ of paper quality scores in $\SET{2, \ldots, 10}$ for each dataset; for each, we apply a variant of the EM algorithm described below. Once the EM algorithm has converged, we evaluate the likelihood of the learned model for the held-out test data, and retain the model(s) with the highest likelihood scores.

Given a choice of $\QualSetSize$, to learn the paper quality distribution $\QualDist$ and conditional review distribution $\RevSigDist$, we apply the EM algorithm (adding some noise in each iteration for smoothness) and cross-validation to avoid overfitting [\cite{dawid1979maximum}].
% \fang{original version commented out below}
% Specifically, suppose the size of quality space is $|\QualSet|=\QualSetSize$. After randomly dividing the dataset into a training set and a test set with the $80/20$ rule, we evaluate the likelihood of the test data using the model learned with the training set, for which the process is repeated five times and averaged. The regularization is applied in the way that after each iteration, the new confusion matrix is linearly combined with a matrix of size $\QualSetSize\times 10$ and each row is a uniform distribution with a step size of $0.001$. 
% Finally, we choose the $\QualSetSize$ that maximizes the likelihood after $100$ iterations and use the corresponding model to set our DS parameters.
% The results illustrate that $\QualSetSize=4$ or $5$ tend to fit the data well in both of the datasets. For robustness, we conduct our experiments for both of the models. The resulting parameters are shown in \ref{app:DS_parameters} \yichi{put in appending}. 

Specifically, by cross-validation, we first randomly divide the dataset into five subsets of approximately equal size. We choose one of the subsets as the test set while the remaining $80\%$ of data form the training set. This step is repeated five times: each time, a different one of the five subsets is used as the test set.
Given the training and test dataset, for each $\QualSetSize \in \SET{2,3,\ldots,10}$, we run the EM algorithm for $100$ iterations on the training set to estimate the quality of each paper and the confusion matrix for reviewers (i.e., the matrix of review score probabilities conditional on ground truth quality); the EM algorithm alternates between updating the quality distribution with fixed confusion matrix, and updating the confusion matrix with fixed quality distribution.
To avoid overfitting, we repeat the following steps in every iteration: given an estimated confusion matrix $\RevSigDist^{(k)}$ after the $k$-th iteration, we perturb $\RevSigDist^{(k)}$ with a small amount of noise so that each row $i$ becomes the convex combination $0.99 \cdot \RevSigDist[i]^{(k)}+0.01 \cdot \frac{1}{|\SigSet|}\mathbf{1}$;
here, $\frac{1}{|\SigSet|}\mathbf{1}\in \R^{|\SigSet|}$ is the uniform distribution on signals. After each iteration, we evaluate the likelihood of the test data given the trained model, i.e., $\QualDist$ and $\RevSigDist$. For every $\QualSetSize$, this gives us a sequence of models for each iteration. Finally, the model that corresponds to the greatest likelihood on the test data is selected for use.

To choose the value of $\QualSetSize$, we judge the learned model based on the likelihood averaged over five times of cross-validation. 
The paper quality space size $\QualSetSize$ that has the maximum averaged likelihood is selected. Finally, we output the paper quality distribution $\QualDist$ as well as the confusion matrix $\RevSigDist$ as the average of the learned parameters for each of the five runs with different test sets. 
We find that $\QualSetSize \in \SET{4,5,6}$ tends to fit the data well. In our experiments, we choose $\QualSetSize = 6$ to enable a richer set of submissions and thus smoother curves.

The resulting parameters are shown in \cref{tab:learned_para}.
For experiments in which the authors receive \emph{noisy} signals (instead of the ground truth quality), we set the confusion matrix for the authors $\AuthSigDist$ to be the same as the one for reviewers, $\RevSigDist$; this is because unfortunately, no data are available that show how authors evaluate their own papers.

We note that although only the results for ICLR 2020 and $\QualSetSize = 6$ are presented in the paper, all of our qualitative results hold for all of the learned models.

% In our work, we tested four models: $\QualSetSize=4, 5$ for each of the two ICLR datasets (see \cref{tab:learned_para}). In the paper, only the results for the model with $\QualSetSize=4$ and learned from the ICLR 2020 review data are presented. 


In some of our experiments, we want to explicitly evaluate the impact of increasing the noise in reviews. To do so, we consider reviewer signal matrices which are a convex combination of the learned signal matrix $\RevSigDist$ with the uniform signal distribution $\frac{1}{|\SigSet|}\mathbf{1}$; this corresponds to a reviewer who assigns a uniformly random score with probability $1-\lambda_R$. The weight $\lambda_R \in [0,1]$ placed on the learned distribution $\RevSigDist$ then captures the quality of the signal. Similarly, $\lambda_A$ controls the weight of the confusion matrix of the authors' signal.

\emph{The Numerical Values of Paper Quality.} 
Next, we infer the values of the paper qualities, given the estimated parameters $\QualDistTilde, \RevSigDistTilde$ and the review scores $\boldsymbol{s}$. That is, given $\QualSetSize$, we want to learn a vector of values 
$(\qual_1,\ldots,\qual_\QualSetSize)$ of paper qualities. 

First, for each paper $i$ in the dataset, we take the average over the review scores, denoted by $\bar{s}_i$. Then, we set the value of the quality of paper $i$ as $\psi(\bar{s}_i)$, where $\psi: [0,9] \to \mathbb{R}$ is an increasing function that maps the average score to the quality of a paper. In our experiments, we set $\psi$ as a reversed (and shifted and scaled) sigmoid function, i.e., $\psi(x) = 3\cdot \log{\frac{x+0.01}{9.01-x}}$ for $x\in [0,9]$. We choose the reversed sigmoid function because it can assign ``convex'' weights on both very positive reviews and very negative reviews. Furthermore, it is parameterized by a small number of interpretable parameters so that it is not too complex to empirically set the parameters.

Next, given the ``artificial'' quality assigned to each paper in the dataset, we want to compute the average paper quality of each category. This requires us to learn a distribution of the category label for each paper. With the learned model $\QualDistTilde, \RevSigDistTilde$ and paper $i$'s review signals $\boldsymbol{s}_i$, we can infer this distribution based on Bayes' rule. Let $l_{i,k}=\ProbC{\text{paper } i\text{ belongs to the $k$-th category}}{\boldsymbol{s}_i, \QualDistTilde, \RevSigDistTilde}$ for $k\in \{1, 2, \ldots, \QualSetSize\}$.
We can then set the paper quality as a weighted average
\begin{equation*} %\label{eq:paper_quality_average}
    \qual_k = \frac{\sum_i^n l_{i,k}\cdot \psi\left(\bar{s}_i\right)}{\sum_i^n l_{i,k}}.
\end{equation*}

\subsubsection{The Learned Parameters}

\cref{tab:learned_para} summarizes our parameters for ICLR 2020 and $L=6$. The rows of the confusion matrix are ranked based on the expected scores (from low to high). That is, the $k$th row of the confusion matrix $\RevSigDist$ has a lower average score than the ($k+1$)-st row. Given this ranking, we observe that the value of paper qualities learned from our method is monotone increasing in $k$.


\begin{table}[htb]
\centering
\scriptsize
\caption{Learned Prior ($\QualDist$), Confusion Matrix ($\RevSigDist$), and Quality Levels ($\QualSet$) from the ICLR Dataset.}
\label{tab:learned_para}
\begin{tabular}{|c|>{\centering\arraybackslash}p{1.8cm}|>{\centering\arraybackslash}p{10.2cm}|>{\centering\arraybackslash}p{1.8cm}|}
\hline
 & $\QualDist$ & $\RevSigDist$ (Confusion Matrix) & $\QualSet$ \\
\hline
ICLR (Updated) $\QualSetSize = 6$ 
& $\begin{bmatrix}
0.111\\ 0.1736\\ 0.1958\\ 0.2081\\ 0.1856\\ 0.1259
\end{bmatrix}$ 
& $\begin{bmatrix}
0.0059 & 0.0867 & 0.2653 & 0.3958 & 0.1817 & 0.0541 & 0.0087 & 0.0006 & 0.0009 & 0.0003\\
0.0004 & 0.0109 & 0.1204 & 0.3564 & 0.2940 & 0.1441 & 0.0557 & 0.0152 & 0.0027 & 0.0003\\
0.0006 & 0.0074 & 0.0891 & 0.2546 & 0.3131 & 0.2206 & 0.0901 & 0.0168 & 0.0073 & 0.0004\\
0.0016 & 0.0084 & 0.0691 & 0.1935 & 0.2803 & 0.2557 & 0.1451 & 0.0356 & 0.0092 & 0.0015\\
0.0007 & 0.0069 & 0.0387 & 0.1485 & 0.2612 & 0.2811 & 0.1949 & 0.0614 & 0.0060 & 0.0007\\
0.0009 & 0.0007 & 0.0176 & 0.0930 & 0.1674 & 0.2593 & 0.3125 & 0.1038 & 0.0413 & 0.0036
\end{bmatrix}$ 
& $\begin{bmatrix}
-2.9101\\ -0.8668\\ -0.0772\\ 0.6178\\ 1.2811\\ 2.7132
\end{bmatrix}$ 
\\
\hline
\end{tabular}
\end{table}

