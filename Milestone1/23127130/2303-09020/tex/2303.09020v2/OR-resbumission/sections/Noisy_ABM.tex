Thus far, we have examined how parameters and acceptance policies shape the conference’s QB tradeoff under the assumptions that authors perfectly observe their papers’ quality and may resubmit indefinitely until accepted. Under these conditions, we show that authors adopt a straightforward threshold best response, which enables a rigorous theoretical analysis of the problem.

When authors receive noisy signals themselves, they \emph{will} update their beliefs about their papers' quality based on the reviews they receive. For example, an author who initially received a signal indicating that her paper was of high quality may revise this estimate downward after receiving several negative reviews.
As a result, authors may not make the same decision in each iteration; while it may initially be utility-maximizing to submit the paper, after several negative reviews, the author may instead take the side option.

In this section, we focus on a more realistic setting where authors have noisy signals and the model is a real-data estimated categorical model.
After all, understanding the behavior of more realistic authors is also an important robustness check on our results.
Unfortunately, computing the author's posterior belief of the paper quality, conditioned on the reviews in each round, is analytically intractable. 
Therefore, we use agent-based simulations (agent-based modeling (ABM)) to evaluate the impact of conference policies on outcomes.

\subsection{Agent-Based Model Setup}
\label{subsec:noisy_setup}

\subsubsection{Categorical Models Studied}

The \emph{$(\lambda_A, \lambda_R, \NumReviews, \TD)-\text{ICLR}^{y, L}$ model} is a categorical model learned from data.
Specifically, the prior of paper quality $\QualDist^*$ and the review signal distribution $\RevSigDist^*$ are learned from the ICLR OpenReview datasets of year $y$ [\cite{iclr2020review,iclr2021review}]; for each dataset, a model is learned with paper quality sets of sizes $L=6$. 
We use the expectation-maximization (EM) algorithm to estimate and update the model parameters. 
Details of the method used for learning are described in \cref{sec:learning-parameters}, where the learned parameters are shown in \cref{tab:learned_para}. 
% \dkcomment{Is it a good idea to not put them here? Is this just a remnant of the earlier structure where this section was in the main body, or do we think that having basically an appendix to an appendix is better? I think that a subsection at the end of the present section would be better --- readers can still skip it if they want.}
% \yzcomment{I'll mark this, but may address this with a lower priority.}
% \dkcomment{I moved it into Subsection 3.5. Does this work?}

When varying the signal quality in models based on ICLR data, we use accuracy parameters $\lambda_A \in [0, 1]$ for the author's signal and $\lambda_R \in [0, 1]$ for the reviewer's signal; the parameters control the probability with which the signal is drawn from the learned parameters $\RevSigDist^*$ vs.~a uniform distribution. $\lambda_A=1$ ($\lambda_R=1$) implies that the signal is drawn from $\RevSigDist^*$ (the same for both authors and reviewers) while $\lambda_A=0$ ($\lambda_R=0$) implies that the signal is uniformly random.
% When we model noiseless authors, we simply remove the entry $\lambda_A$ from the tuple and call this the $(\lambda_R, \NumReviews, \ConfValue, \TD)-\text{ICLR}^{y, L}$ noiseless-author model.

We remark that the learned distributions $\RevSigDist^*$ of reviewer signals do not strictly satisfy the monotone likelihood ratio property as defined in \cref{def:informative}. However, as we will see, the properties that we are interested in still (approximately) follow.

\subsubsection{Myopic Authors and Agent-Based Simulations}

% \fangcomment{I try to use our existing macros to avoid inconsistency.}
In our ABM experiments, we simulate the submission-review process for $T = 15$ rounds with $\NumNewPapers = 1000$ new papers per round. We give a brief recap of our model from \cref{sec:model}; recall that the dynamics has two major phases: submission and reviewing.

In the first phase, each author updates her belief about her paper based on her private signal that is generated from the distribution $\AuthSigDist[q]$, and the reviews from the previous rounds (if any). At each round, given the belief, the author makes the decision of either submitting to the conference or taking the outside option, depending on the expected utility of each option.
We assume that authors are myopic, meaning that they compute the expected utility of submitting to the conference while assuming that they will take the outside option in the next round immediately if the paper is rejected this round. That is, a myopic author in round $t$ does not foresee the future after round $t+1$. The myopic strategy asks: is it better to submit one more round before giving up or to give up now? While it can happen that submitting two more rounds before giving up is better than giving up now which in turn is better than giving up after one round of submission, such cases are quite rare when the same threshold acceptance policy is used in each round.

The author's utility depends on the conference value $\ConfValue$ which in turn depends on the authors' response. In our ABM, we initialize the conference value $\ConfValue_1 = 2$ in the first round and iteratively update $\ConfValue_t = (1-\lambda_V) \cdot \ConfValue_{t-1} + \lambda_V \cdot\left(1 + \ExpectC{Q_i}{\text{paper } i \text{ was accepted in round } t}\right)$. We set the updating rate $\lambda_V = 0.5$ and compute the expected quality directly using the average quality of the papers accepted in the previous round.


In the second phase, the conference obtains $\NumReviews$ reviews for each submission. These reviews are drawn i.i.d.~from the review signal distribution $\RevSigDist[\dkedit{q}]$, conditioned on the ground truth quality \dkedit{$q$} of the submission. 
Then, for each submission, the conference makes a decision of acceptance or rejection based on a threshold policy with threshold $\tau$. Given the $\NumReviews$ i.i.d.~reviews, denoted as $\RevSigV$, the conference can infer the expected quality of the submission $U(\RevSigV) = \ExpectC[\Qual]{\Qual}{\RevSigV, \QualDist, \RevSigDist}$ (conditioned only on the reviews from the current round) and accept or reject the paper based on the simplified threshold policy for the categorical model described in \cref{app:simplified_threshold}.

% These two phases are repeated for $N = 15$ rounds, giving time for the conference value to be updated based on the average quality of accepted papers. After $N$ rounds, we compute the conference quality by summing all the papers that are accepted in round $N$ and taking the average over $n$; we compute the review burden by summing the total number of reviews assigned to each paper that is submitted or resubmitted in round $N$ and averaging over $n$; we compute author social welfare by summing the utility of all authors who have submitted a paper in round $N$ and averaging over $n$. All metrics exclude any papers accepted (or withdrawn via the outside option) prior to round $T$.

We repeat these two phases for $N = 15$ rounds, allowing the conference value and the author submission strategy to dynamically update based on each other. At the end of round $N$, we compute:
\begin{itemize}
    \item \textbf{Conference quality}, as the sum of the quality of papers accepted in round $N$, normalized by $\NumNewPapers$;
    \item \textbf{Review burden}, as the total number of reviews assigned to papers submitted or resubmitted in round $N$, normalized by $\NumNewPapers$;
    \item \textbf{Author welfare}, as the sum of author utilities who submitted in round $N$, normalized by $\NumNewPapers$. Note that this counts the utilities of authors whose papers are accepted by the prestigious conference, excluding the utilities of authors who decide to take the outside option.
\end{itemize}
In all metrics, we exclude papers accepted or withdrawn (via the outside option) before round $T$.

\subsection{QB-tradeoffs With Noisy Authors}

Fixing the model parameters, we vary the acceptance threshold $\tau\in [-2,2]$ with a step size of $0.1$. Each acceptance threshold leads to a corresponding conference quality and review burden, allowing us to trace the QB-tradeoff curve.

As shown in \cref{fig:QB_tradeoff_noisy}(b), when $\tau$ is close to $-2$, the conference accepts every paper with almost a single round of review, leading to a review burden of approximately $\NumReviews$ (slightly higher than $\NumReviews$ because $\tau = -2$ is still larger than the minimum quality, meaning that clearly bad papers still have a positive probability to be rejected).
These thresholds correspond to the dots close to $(\NumReviews, 0.22)$, where $0.22$ is the conference quality when all papers are submitted and accepted.
Similar to \cref{fig:Pareto_frontier_continuous}, as we raise the acceptance threshold, both the conference quality and the review burden increase until the conference quality is maximized. 
Increasing $\tau$ even further will reduce the conference quality and increase the review burden, resulting in a dominated point on the QB-tradeoff curve.
Interestingly, this pattern is similar to panel (b) of \cref{fig:Pareto_frontier_continuous} with a large review noise, where the Pareto optimal policies are achieved by adopting low acceptance thresholds.

Note that\fangcomment{The reader cannot notice this because we do not display $\tau$.  It would be good if we can also show the location of $\tau = -2, -1, 0, 1, 2$} the quality-maximizing $\tau$ does not necessarily induce a de facto threshold of 0 when authors are noisy. This is because the authors' strategy is more complicated when they observe noisy signals, and thus, the original definition of de facto threshold is not suitable for the new setting.

\begin{figure}
     \FIGURE
     {\begin{subfigure}[b]{0.46\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Plots/QB_tradeoff_noisy_L6_vary_beta.pdf}
         \captionsetup{size=}
         \caption{QB-tradeoff varying $\lambda_R$.}
     \end{subfigure}
     % \begin{subfigure}[b]{0.32\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{Plots/QB_tradeoff_noisy_L6_vary_alpha.pdf}
     %     \captionsetup{size=}
     %     \caption{QB-tradeoff varying $\lambda_A$.}
     % \end{subfigure}
     \begin{subfigure}[b]{0.46\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Plots/QB_tradeoff_noisy_L6_vary_m.pdf}
         \captionsetup{size=}
         \caption{QB-tradeoff varying $m$.}
     \end{subfigure}
     }
     {QB-tradeoff Curves under Reviewer Accuracy Parameter $\lambda_R$ and Number of Reviews Per Paper $m$. \label{fig:QB_tradeoff_noisy}}
     {The figures show the QB-tradeoff curves under different review qualities and numbers of reviews per paper, resulting from different acceptance thresholds in (a) the \emph{${(\lambda_A = 1, \lambda_R, \NumReviews=3, \TD=0.7)}$-ICLR$^{2020, 6}$ model} and (b) the \emph{$(\lambda_A = 1, \lambda_R = 1, \NumReviews, \TD=0.7)$-ICLR$^{2020, 6}$ model}.}
\end{figure}

% \dkcomment{Given that higher $\lambda$ means better reviews, calling it ``noise'' is very misleading. I am trying to change to ``accuracy parameter'' everywhere, but if you think of a better name, please fix. Also, if I forgot any. Most importantly, fix it in the captions (in the PDFs, which I cannot fix)}

We next examine how review noise affects the QB–tradeoff curves.
\cref{fig:QB_tradeoff_noisy} (a) plots these curves for a fixed author‐signal accuracy $\lambda_A = 1$ varying review-signal accuracy levels $\lambda_B\in \{0.7,0.8,0.9,1\}$.
Consistent with our observations in \cref{sec:QB-trade-noise}, improving review quality yields a strictly dominating tradeoff curve. Furthermore, unlike \cref{fig:Pareto_frontier_continuous} --- where the maximum conference quality remains unchanged across noise levels --- we find that when authors themselves are noisy, better reviews actually raise the attainable maximum conference quality. Intuitively, noisy authors tend not to persist in resubmitting after some negative reviews; under high review‐noise, there are more high–quality papers that receive unlucky low scores, which prompts their authors to take the outside option rather than resubmit, and thereby reducing overall conference quality.

% In \cref{fig:QB_tradeoff_noisy} (b), we present how the QB-tradeoff curves change with the noise of authors' signals. We observe that when we raise the acceptance threshold, starting from a very low $\tau$, the difference between different curves is very small at the beginning. This is because our estimated review signal distribution is very likely to assign a low review score to papers with negative quality. Therefore, the authors with negative quality papers roughly decide to quit upon receiving a few negative review no matter how noisy their review signals are. This results in a similar QB-tradeoff curves for various $\lambda_A$. However, when $\tau$ is large, only positive-quality papers are submitted. Authors who are more confident in their papers' quality (when $\lambda_A$ is large) are more persistent in resubmitting their papers even when they observe a few bad reviews. Consequently, there are more rounds of resubmissions, resulting in a larger review burden.

We further present how the number of reviews per paper affects the QB-tradeoff curves in \cref{fig:QB_tradeoff_noisy} (b).
When we fix the review quality at $\lambda_R = 1$ and increase $m$, the QB-tradeoff curve shifts upward and to the right: for $\NumReviews \in \{1,2,3,4\}$, the maximum conference quality is $\CONFUTIL = 0.56, 0.6, 0.61, 0.63$, which is reached at $\PaperReviews = 3.4, 5.8, 8.5, 11$, respectively. This means that a larger $\NumReviews$ greatly increases the review burden but has the benefit of reaching a larger maximum conference quality. However, the marginal benefit of having an additional review is minimal relative to its extra review burden when $\NumReviews \ge 3$. This implies that a small number of high-quality reviews paired with a carefully chosen acceptance threshold can reach a desirable QB-tradeoff. 

\subsection{QA-tradeoffs With Noisy Authors}

We next investigate author welfare. As shown in \cref{fig:QA_tradeoff_noisy}, the QA-tradeoff curves in the real-data estimated categorical model look similar to those in the continuous model, where both the conference quality and author welfare first increase and then decrease as the acceptance threshold increases from $-2$ to $2$; eventually, both the quality and welfare reach 0. Intuitively, a high acceptance threshold rejects too many (both high-quality and low-quality) papers, while a low acceptance threshold accepts too many negative-quality papers and results in a low conference value; both lead to a decrease in the author welfare.
% \fangcomment{we need to update the figure because $\AUTHUTIL$ is always greater or equal to 1}
% \yzcomment{If only the very top papers are accepted, the sum of author utilities is very small. We consider the sum, not the average.}\fangcomment{Does the author get utility by submitting to the side option? Though not always greater than 1, it should not be zero.}\yzcomment{our definition of welfare only counts papers accepted by the prestigious conference.}
\begin{figure}
     \FIGURE
     {\begin{subfigure}[b]{0.46\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Plots/QA_tradeoff_noisy_L6_vary_beta.pdf}
         \captionsetup{size=}
         \caption{QB-tradeoff varying $\lambda_R$.}\label{fig:QA_tradeoff_noisy_a}
     \end{subfigure}
     \begin{subfigure}[b]{0.46\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Plots/QA_tradeoff_noisy_L6_vary_m.pdf}
         \captionsetup{size=}
         \caption{QA-tradeoff varying $m$.}\label{fig:QA_tradeoff_noisy_b}
     \end{subfigure}
     }
     {QA-tradeoff Curves under Reviewer Accuracy Parameter $\lambda_R$ and Number of Reviews Per Paper $m$. \label{fig:QA_tradeoff_noisy}}
     {The figures show the QA-tradeoff curves under different review qualities and numbers of reviews per paper, resulting from different acceptance thresholds in (a) the \emph{${(\lambda_A = 1, \lambda_R, \NumReviews=3, \TD=0.7)}$-ICLR$^{2020, 6}$ model} and (b) the \emph{$(\lambda_A = 1, \lambda_R = 1, \NumReviews, \TD=0.7)$-ICLR$^{2020, 6}$ model}.}
\end{figure}

In \cref{fig:QA_tradeoff_noisy}(a), we confirm our theoretical results in \cref{sec:QB-trade-noise} that a better review quality leads to a dominating QA-tradeoff. With a better review quality, the conference can not only achieve the same quality with a larger author welfare, but can also achieve a larger maximum conference quality.

We next investigate the effect of the number of reviews per paper on the QA-tradeoffs.
Although increasing $\NumReviews$ typically increases the review burden conditioned on the same conference quality, it improves \author welfare. 
This can be observed in \cref{fig:QA_tradeoff_noisy}(b), where the QA-tradeoff curves corresponding to larger $\NumReviews$ dominate those corresponding to smaller $\NumReviews$.
Intuitively, this is because a larger number of i.i.d.~peer reviews work similarly to a decrease in review noise. Therefore, the desired batch of papers can be accepted with fewer rounds of resubmissions, resulting in larger author welfare.

\subsection{Acceptance Rate With Noisy Authors}

In \cref{sec:acc_rate}, we show that whether the acceptance rate is increasing or decreasing in the acceptance threshold can be roughly determined by a quantity resembling the hazard rate of the paper quality prior. The acceptance rate tends to be monotone decreasing in $\tau$ if the hazard rate of the paper quality distribution is monotone. Our experiments on real data echo this observation. The quality prior we estimated from the real ICLR review data has a monotone increasing hazard rate. As expected, in \cref{fig:Acc_rate_noisy}, we observe that the acceptance rates are monotone decreasing in $\tau$.

We also observe that, compared to a weaker review system (i.e., one with a smaller $\lambda_R$ or fewer reviews per paper $\NumReviews$), a stronger review system tends to have a lower acceptance rate when the threshold $\tau$ is small, and a higher acceptance rate when $\tau$ is large. This pattern arises because a small $\tau$ admits many low-quality submissions. A stronger system, being more accurate, is better at filtering out these low-quality papers, leading to fewer acceptances. Conversely, when $\tau$ is large and most submissions are high-quality, a better review system can more efficiently identify good papers\textemdash often with fewer rounds of resubmission\textemdash resulting in a higher acceptance rate.

In practice, top conferences usually face a large number of low-quality submissions and thus have acceptance rates lower than 35\%. In this case, our results suggest that the acceptance rate is typically increasing with review quality and decreases with the acceptance threshold.

\begin{figure}
     \FIGURE
     {\begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Plots/Acceptance_rate_noisy_L6_vary_beta.pdf}
         \captionsetup{size=}
         \caption{Acceptance rate varying $\lambda_R$.}
     \end{subfigure}
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Plots/Acceptance_rate_noisy_L6_vary_m.pdf}
         \captionsetup{size=}
         \caption{Acceptance rate varying $m$.}
     \end{subfigure}}
     {Acceptance Rates under Reviewer Accuracy Parameter $\lambda_R$ and number of reviews per paper $m$. \label{fig:Acc_rate_noisy}}
     {The figures show the relationship between the acceptance rate and the acceptance threshold under different review qualities and numbers of reviews per paper, resulting from different acceptance thresholds in (a) the \emph{${(\lambda_A = 1, \lambda_R, \NumReviews=3, \TD=0.7)}$-ICLR$^{2020, 6}$ model} and (b) the \emph{$(\lambda_A = 1, \lambda_R = 1, \NumReviews, \TD=0.7)$-ICLR$^{2020, 6}$ model}.}
\end{figure}

\begin{remark}[Discussions and limitations of ABM experiments and results]
First, we note that in our noiseless model, the quality distribution of accepted papers equals the prior paper quality distribution conditional on being greater than the de facto threshold. However, our agent-based model presented in this section was instead learned from the \emph{submitted} papers, so it may overcount borderline papers while under-counting low-quality papers. 
% \dkcomment{Given that we don't discuss anything about ICLR data in the main paper (except saying that we do have experiments on it in the appendix), this sentence may lack context for a reader.}\yichicomment{addressed}\gscomment{it has context, but still seems out of place.  Could we move it to the appendix somewhere?  Ideally the main body reads well without the appendix, but this is just something that does not apply.  Okay, I see this is particially contradicted below.  However, because is something that is still a relevant take-away.  this takeway is completely not interesting because it discribes a weakenss of results that we never discuss.  Right?}

Second, we have seen that in our experiments, a small number of reviews per paper can achieve appealing tradeoffs. However, in practice, a larger number of reviews may nonetheless be desirable due to several other considerations:
% \dkcomment{Given that we now analyze author utility explicitly, it might not just be ``real-world'' any more.}\yichicomment{addressed}\gscomment{they are not unmodeled either.  We model them and the results are in the appendix.}
first, more reviews may decrease the average number of times a paper needs to be resubmitted, helping authors; second, our reviewer error model does not capture the situation where there may be a ``fatal flaw'' that only an astute reviewer observes; and finally, more reviewers can provide more feedback.
% Relatedly, in \Cref{fig:tradeoff_noisy_m}, we observe that soliciting only one review per paper may slow the progress of authors learning about the quality of their papers based on historical reviews;
% this results in low conference quality. 
Moreover, our model assumes that reviews are i.i.d., which is not true in reality when the reviewers can communicate (after the rebuttal). The integrated review signals after communication may become much more informative than aggregating each of them as an i.i.d.~review; this may significantly benefit the strategy of soliciting a large number of reviews.
\end{remark}