Conferences play an important role in the publication and {scientific dissemination process in computer science}. They aim to provide attendees with access to high-quality and recent research results (in addition to networking opportunities), and authors view the ability to publish and share their recent results at high-quality venues as conferring scientific credibility and thus status. In order to do so, conferences rely on significant volunteer work from the community, most notably in reviewing large numbers of submitted papers to evaluate their scientific merit.
% \gscomment{Not sure this is the best place for it, but I feel like we should comment on the fact that all three of these "parties" are typically the same researches just in different roles.  Thus, what is good for the "conference" is also good for a researcher as it will raise the prestige of his field to have a high quality conference.}\yichicomment{Added a footnote}

While the conference publication process seems to have served the community fairly well overall --- in particular enabling a high speed of dissemination of scientific results --- different members of the community often see significant room for improvement: authors often feel that their submitted papers are not evaluated sufficiently competently, while conference attendees sometimes find the program diluted with less interesting work.
% \footnote{\yichiedit{or there are too many papers to find the really interesting ones}}\dkcomment{Not sure about this footnote. That's the meaning of diluted, that the stuff you're interested in gets drowned out.}
Attempts have been made by conferences to mitigate such concerns, one of which is to increase the number of reviews assigned to each submission.
For example, nowadays, it is no longer rare for top computer science conferences to assign five or more reviewers to one paper.
However, this approach increases the review burden; indeed, many members of the scientific community now feel overloaded with conference reviewing requests. The approach has the potential to lead to a vicious cycle: the substantial increase in peer review workload, which is further exacerbated by the growth of the research community, may lead to lower-quality reviews, in turn leading to more resubmissions of the same papers and thus a higher review burden.

Our high-level goal is to understand how the parameters of the system, together with the conference's review policy decisions, affect the tradeoff between the conference quality, the review burden on the community, and the welfare of the authors.
In particular, the tradeoffs between conference quality and each of the other two dimensions are particularly interesting.%
\footnote{We note that the three parties in the peer review system are usually the same researchers serving in different roles. Consequently, the tradeoffs among their utilities can be viewed as tradeoffs among different dimensions of the research field.}
Given a fixed number and quality distribution of submissions, the conference's quality is increased by accepting more good and fewer bad papers.
Distinguishing between good and bad papers more accurately requires more reviews, along with enticing self-selection by authors.

We model the conference review process as a Stackelberg game between a top conference and the (homogeneous and strategic) authors (described in detail in \cref{sec:model}).
The authors' papers have qualities (positive or negative) drawn from a commonly known distribution over a (finite or infinite) set; however, both the conference's reviewers and (in some of our analysis) the authors themselves only obtain imperfect signals about the quality of a paper.
The conference's quality is the sum of qualities of the accepted papers,  normalized by the total number of papers.
The conference commits to a review and acceptance policy, which prescribes how many independent reviewers are assigned to each paper, and what the criteria for acceptance are.
In \cref{sec:memory_policy}, we also study policies with ``institutional memory,'' including policies limiting the number of times a paper can be resubmitted, and policies requiring past reviews to be included with resubmissions.

In response to the conference's policies, the authors face a binary decision in each round: submit to the prestigious conference or opt for a safer outside option. Submission to the top conference offers the potential for high utility upon acceptance (proportional to the average quality of accepted papers), but yields zero utility if the submission is rejected.
In contrast, the outside option --- such as a second-tier conference or arXiv --- provides guaranteed but smaller positive utility. 
The utility of acceptance to either venue is exponentially time-discounted in the number of resubmission rounds, modeling that authors prefer timely acceptance of their result. 
Authors best-respond, i.e., choose a utility-maximizing action, based on their private (potentially noisy) signal about their paper's quality as well as the historical reviews collected when the paper was rejected in previous rounds. 
We primarily focus on the following high-level questions: 

\begin{itemize}
    \item The fact that rejected papers can be resubmitted means that weaker papers may be eventually accepted if authors are patient enough. What is the impact of resubmissions on the gap between a conference's intended acceptance threshold and the de facto distribution of paper qualities at the conference?
    \item What is the range of Pareto optimal review and acceptance policies with regard to the tradeoff between conference quality and review burden?  How is it impacted by authors' patience, the number of reviews per paper, and/or review quality?
    \item How does the conference's acceptance policy relate to its acceptance rate? This is non-obvious, as a strict policy may lead to a lot of self-selection on the part of the authors, and thus to a \emph{higher} acceptance rate.
    \item How does the number of reviews per round affect the overall review burden? What is the tradeoff between review quality and quantity, i.e., how many noisy reviews approximate one high-quality review?
    \item How helpful is institutional memory? Can the conference significantly lower the review burden or increase quality by limiting the number of resubmissions, treating resubmitted papers differently, or requiring past reviews to be included?
\end{itemize}

To ensure robustness of our results, we investigate these questions under different models regarding the quality distributions of papers and distributions of review noise, and the information authors have about the quality of their own work.
In the main body of the paper, we focus on settings with noiseless authors who possess perfect information about their own paper quality. 
We consider both a continuous model where the paper quality follows a continuous distribution and reviews are continuous with additive noise, and a categorical model with several discrete paper qualities and a discrete scale of review scores.
Authors do not learn new information from reviews, so each author's decision is either to submit her paper until it is accepted, or to immediately take the outside option. The simplicity of this model, due to the authors' binary best response, allows us to prove theoretical results and draw intuitive conclusions.

In the appendix, we consider agent-based modeling under a categorical model, where authors only observe noisy signals about their papers' qualities. This model allows for more realistic modeling of real-world conferences, and we estimate the parameters from publicly available review data for the ICLR 2020 conference \citep{iclr2020review}. Our agent-based simulations, built on both this detailed categorical model and a simplified binary variant, serve two purposes. First, they provide robustness checks for the theoretical insights derived from our continuous model. Second, they create a flexible platform for investigating sophisticated acceptance policies that incorporate institutional memory and other complex features difficult to analyze theoretically.

% For (1), we consider three models: (a) a model with continuous paper qualities (e.g., drawn from a Gaussian distribution), where reviews are also continuous with additive noise, (b) a class of categorical models with several discrete paper qualities and a discrete scale of review scores, and (c) a model with binary paper qualities (good/bad) and reviews (accept/reject recommendations).  The advantage of (a) and (c) is that they are characterized by a small number of meaningful parameters, making it possible to systematically explore the dependence of outcomes on these parameters.
% On the other hand, (b) allows for more realistic modeling of real-world conferences, and we estimate the parameters from publicly available review data for the ICLR 2020 conference \citep{iclr2020review}.


% For (2), we consider authors with perfect information about their paper's intrinsic quality, and authors who themselves only receive noisy signals. The advantage of the former is that authors do not learn new information from reviews, so each author's decision is either to submit her paper until it is accepted, or to immediately take the side option. In turn, this simplicity of best responses allows us to prove theoretical results about the outcomes.
% On the other hand, this behavior is also somewhat unrealistic, motivating the study of a model in which authors themselves have noisy signals, and update their beliefs about the paper's quality based on the reviews. The resulting Bayesian reasoning makes the model too complex for a theoretical analysis, so we investigate it using agent-based modeling (ABM) and simulations.


% \gs{added things from rebuttal here.  integrated with a previous paragraph that was commented out.}

% \gsdelete{We implemented three models for clarity and robustness. \dkcomment{I am not sure what the preceding sentence says. What does ``implement'' mean when we actually proved things? How does implementing three models result in clarity? Etc. Maybe the preceding paragraph is enough to clarify the three models, so we can delete the first sentence?}} 
% We emphasize results in the model which portrays them most clearly.  We hope that this helps the reader quickly grasp the underlying intuition.  However, we also typically test the robustness of the results in the remaining models.  In particular, many of our results are shown theoretically in simple models, but are shown to continue to hold (at least qualitatively) based on simulations for the more complex models. 

While the models are necessarily a simplification, and one should therefore be cautious of directly basing concrete decisions on the results, we believe that the fundamental insights derived from our analysis (summarized below) remain robust across a reasonable range of variants of our model. We thus envision that our theoretical results (and simulations) can steer the discussion, uncover parameters to focus on, and inform decision makers in practice.

\subsection{Summary of Results}

% \gsreplace{Due to space limitation, we leave a substantial fraction of our results in the appendix. In the main body, we thus focus on the setting of noiseless authors where our theoretical results can provide useful intuitions on how to interpret our contributions in more general settings.}
% \gscomment{do we really focus on the continuous model?  I would propose removing this.}
In the main body, we focus on the setting with noiseless authors, where we can obtain strong theoretical results.  The appendix contains proofs, some further details, and additional results, largely using simulations on models estimated by real data, that both illustrate the robustness of our theoretical results and explore more complex settings.

\vspace{2mm}
\emph{Noiseless Authors: Theoretical Results.}
We first assume that authors know the quality of their manuscripts, but reviewers only obtain noisy signals of the quality. 

We first explain a counter-intuitive phenomenon which we call the \emph{resubmission paradox}: real-world conferences typically have an acceptance rate of around 25\%; yet, many rejected papers are accepted later at other (or even the same) prestigious venues \citep{cortes2021inconsistency}.
Therefore, it is sometimes argued that conferences should accept every paper that is ``above the bar'', by lowering the acceptance threshold. Our model can help explain this counterintuitive phenomenon. Because every submitted paper will be resubmitted until accepted (since the authors learn no new information from the reviews), the conference's \emph{acceptance threshold} induces a \emph{de facto threshold}: a manuscript quality above which every paper will be eventually accepted and below which no manuscript will be submitted. The acceptance threshold and de facto threshold are typically different, sometimes significantly so (in particular when the authors are very patient, the noise of reviews is large, or the conference is very selective, resulting in a large utility upon acceptance) --- we call the difference the \emph{resubmission gap}. 
In most situations, if a conference were to naively set an acceptance threshold identical to their ideal de facto threshold, it would ultimately accept substandard papers, in rounds in which enough reviewers had noisy reviews that ended up too high. We call this phenomenon the \emph{resubmission paradox.} Instead, the conference should select an acceptance threshold higher than the desired threshold of conference acceptances.  However, the resulting ``optimal'' is then counter-intuitive: every paper that is submitted is eventually accepted, yet each round, many of the papers are rejected.
% We exactly characterize the resubmission gap in several settings.

Second, we consider the relationship between the conference quality, the review burden (the total number of reviews per paper throughout its resubmission process), and the authors' welfare. 
Both review burden and author welfare favor inclusive, lenient policies with high acceptance rates, since resubmissions increase review load and reduce authors' utility through discounting. In contrast, maximizing conference quality requires setting an appropriate threshold to ensure that only positive-quality papers are ultimately accepted. This tension gives rise to a \emph{quality-burden tradeoff (QB-tradeoff)}. We show that whether the Pareto frontier of this tradeoff is achieved through lenient or stringent acceptance thresholds depends on the distribution of paper quality and the level of review noise. Stringent policies tend to dominate when low-quality papers are prevalent and reviews are less noisy. % \gscomment{Perhaps we could rewrite this in light of the new organization.  AS is, I think this is one of our least crisp points and in this summary we should shorten it as much as possible.  Currently, the reader is going to get lost and not see the other amazing stuff we ahve done.}

% At a high level, the review burden closely tracks the number of papers just above the de facto threshold, because such papers typically require numerous resubmissions before acceptance.   
% In particular, a conference may be able to significantly reduce the long-term review burden by choosing the threshold such that there are fewer borderline papers near the resulting de facto threshold.

% \dkreplace{Third, we show that if the prestige of the conference increases, the patience of authors increases, or the noise of the reviews increases (in a certain technical sense), then the tradeoff between the  conference's quality and the review burden becomes strictly worse. Thus, one cause of more reviews may be the high prestige placed on certain venues.  This also warns that policies which effect these items may have unintended consequences.}
Third, we explore how the system parameters affect the utilities of the three stakeholders in the review system. We first show that a better review quality --- in the sense of Blackwell-dominance --- can benefit all parties:  the conference can attain higher quality with a lower review burden and increased author welfare. This result holds broadly when the system with better review quality is allowed to adopt any acceptance policy, including non-monotone ones that do not necessarily favor papers with more positive reviews. If we restrict attention to threshold acceptance policies --- a more natural and interpretable class --- the result continues to hold under the additional assumption that review signals satisfy a monotone likelihood ratio. %\gscomment{the previous sentences seem too in the weeds.  I think we shoud rephrase more succinctly.}
We also analyze the impact of author patience. We show that, conditioned on the same conference quality, systems with more patient authors require a higher review burden but do not necessarily yield higher author welfare. This highlights a potential risk: policies that reduce the cost of resubmissions may unintentionally reduce the utility of at least one stakeholder, including the authors themselves.

% show that if the prestige of the conference increases or the patience of authors increases, then the tradeoff between the  conference's quality and the review burden becomes strictly worse. Thus, one cause of more reviews may be the high prestige placed on certain venues.  This also warns that policies which affect these items may have unintended consequences.
% We obtain similar results in terms of the review quality. If the signals of reviewers  in one setting Blackwell-dominate those in another setting, the conference can obtain a weakly better tradeoff between quality and review burden. 
% However, doing so may necessitate using non-monotone acceptance policies, which do not treat papers with strictly more positive reviews better than those with strictly less positive reviews.  We show that a better tradeoff for better signals can be achieved under a restriction to threshold policies as well, but only with the additional assumption that the review signal is \emph{informative} in the sense of monotone likelihood ratios.
 
Fourth, we examine factors influencing the conference acceptance rate. When the review noise is small, the relationship between the acceptance threshold and acceptance rate depends on two key quantities: the acceptance probability of borderline papers and a measure resembling the \emph{hazard rate} of the prior distribution over paper qualities. We show that raising the acceptance threshold lowers the acceptance probability of borderline papers, and thereby reduces the acceptance rate. 
\dkcomment{In the following three sentences, we write ``hazard rate''. Do we mean ``hazard rate'' or our quantity?}
If the quality prior has an increasing hazard rate --- typical for thin-tailed distributions such as the Gaussian --- then a higher threshold increases the hazard rate, which further reduces the acceptance rate. However, for priors with non-monotone hazard rate, the relationship may be non-monotonic, and a stricter threshold can sometimes \emph{increase} the acceptance rate, such as when many borderline papers (which might otherwise be rejected multiple times) cease to be submitted in the first place. Intuitively, for quality distributions with non-monotone hazard rates, raising the acceptance threshold may sharply reduce the proportion of borderline papers, which are major contributors to the review burden through many rounds of resubmissions. We confirm the robustness of these patterns beyond the small-noise setting through simulations.
% \gscomment{I don't understand why we are saying it this way.  Why not, ``in cases where this quantity decreases''?}\yichicomment{Most distributions do not have a monotone decreasing hazard rate.}\gscomment{I see.  I guess I also see this is completely bloated for the into.  I think that the takeaway is that: is "most distributions we try, there higher acceptance ration decrease the acceptance rate, but this need not be the case."  We want to keep the intro crisp and this just seems to run on and it is not clear what we are even saying.}  
 
% Finally, we quantify how the number of reviews is relates to the noise of reviews specifically in the Gaussian case. 

% \vspace{2mm}
% \emph{Noiseless Authors and Real-World Parameters}  
% We further investigate the above phenomena in a more realistic setting based on categorical data.
% One common attempt at improving a conference's quality is to solicit more reviews per submission which, as believed, can help distinguish the good papers from the bad ones. Thus, we next (in \cref{subsec:opt_m} and \ref{subsec:qual-quant}) study how the number of solicited reviews for each submission impacts the conference's quality and the average total number of reviews for each paper. (Recall that each paper may be submitted multiple times.) 
% We find that in our models, increasing the number of solicited reviews per paper beyond three rarely leads to better outcomes.  This is because a small number of solicited reviews can be optimal with a carefully chosen acceptance threshold.
% In contrast, we find that increasing the \emph{quality} of reviews often substantially improves the Pareto frontier.

\vspace{2mm}
\emph{Real-Data-Estimated Categorical Model with Noisy Authors}  
We further examine the robustness of our earlier findings using a model estimated from real data, in a setting where authors are uncertain about their paper's quality and can learn about it through reviewer feedback (\cref{sec:noisy_abm}).
Our results broadly align with the theoretical insights but also reveal new dynamics introduced by the noisy author assumption. In particular, because authors’ self-selection is no longer perfect, the maximum achievable conference quality varies across different settings. We find that increasing the number of reviews per paper can improve the maximum conference quality but at the cost of a higher review burden. In contrast, enhancing review quality raises the maximum achievable quality while requiring a smaller review burden. This result further emphasizes the importance of review quality over quantity.
% In this context, the Pareto frontier for conference quality vs.~review burden is significantly worse than with perfectly appraised authors\textemdash the reason is that the latter can be compelled to self-select with carefully chosen acceptance thresholds.  However, it is not clear if such gains are realizable in practice. 
% Perhaps, new discipline norms of learning the quality of one's paper prior to submission (for example, by sharing early manuscripts with colleagues for feedback) could provide authors with accurate quality signals. 
% Or perhaps, in practice, overcoming authors' unfounded admiration of their own work is not possible.

\vspace{2mm}
\emph{Memory in the System.}
Another popular type of proposal is to give the system more memory, either by limiting the number of resubmissions of the same manuscript or by reusing reviews. 
% \yichicomment{Removed a footnote here.}
% \dkcomment{This seems like a pretty important footnote. I would move it to the main text, in parentheses. Otherwise, reviewers may call this out\textemdash as for instance did the AE at MS.}
(Another important reason for requiring the inclusion of prior reviews --- not modeled here --- is that it lets the conference ascertain that specific concerns from earlier versions have been addressed.)
Our first result shows that limiting the number of resubmissions reduces the overall review burden but also lowers the maximum achievable conference quality. Empirically, we further find that retaining historical reviews across resubmissions allows the conference to maintain (or slightly improve) quality while reducing the review burden. However, this effect can be marginal. Thus, the primary benefit of reusing reviews may be to track how papers are revised --- an objective outside the scope of this study. It should be noted, though, that our analysis here is preliminary and only carried out for the binary model due to computational constraints. 
% \dkcomment{We say the same thing twice in a row.}
% Our results here (in \cref{sec:memory_policy}) show that the main effect of having memory within the review process is that the conference can reduce the review burden while preserving the same (or slightly better) conference quality. However, such an effect can be marginal in some cases; thus, it is not clear whether it is worthwhile to implement these policies broadly. It should be noted, though, that our analysis here is rather preliminary, and only carried out for the binary model.

Our models necessarily abstract away several other aspects of the conference submission ecosystem, which would also be worth investigating.  
% \dkcomment{This is not really true any more, right?} 
These limitations are discussed in more depth in \cref{sec:limitations}.


\subsection{Related Work}
\label{sec:related-work}

\gscomment{Here is another paper that Misha found, we should probably acknowledge him actually, it also has the insite the adding friction often helps things.  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0246675}

Not surprisingly, given the importance of peer review in science, several attempts have been made by different research fields to simulate, understand, and improve the process.
When considering the systems level, agent-based models (ABMs) have been one of the techniques of choice.
The review article by \citet{feliciani2019scoping} gives a fairly comprehensive summary of this line of work. It suggests several general themes to focus on in models: editorial strategies, matching submissions with reviewers, decision making, biases and calibration, and comparisons of alternative peer review systems. 

Among the more prominent works using ABMs are those by \citet{kovanis2016complex,kovanis2017evaluating}. 
\citet{kovanis2016complex} propose a model for a holistic study of the scientific publication ecosystem\textemdash this model includes the acquisition of resources (such as status) by authors, which can be leveraged into future papers.  Their subsequent work \citep{kovanis2017evaluating} builds on these models and implementations to evaluate several alternative systems for peer review.
These models and results differ from ours in several key dimensions: the authors are not strategic, they do not focus on fine-grained policies by journals (or in our case, conferences), and due to the holistic nature and complexity of the model, the model is only amenable to simulation, but not analytically tractable.

Two papers by \citet{bianchi2018peer} and \citet{squazzoni2012saint} also use agent-based modeling approaches. They particularly focus on the fact that researchers must decide how to divide their time between writing and reviewing papers, and investigate (experimentally) the impact of various policies on the efficiency of peer review.
Similarly, \citet{thurner2011peer} and \citet{d2017can} use agent-based models to investigate a specific aspect of peer review, namely, selfish behavior on the part of referees, who may not have incentives to see other strong work published.

\citet{allesina2012modeling} also uses agent-based modeling, in this case, to understand the impact of different high-level approaches (editorial desk rejects, bidding on papers, etc.) on the overall reviewing load.
\citet{Roebber2011PeerReview} use agent-based modeling to evaluate strategies for program officers of funding agencies. One of their findings is similar to ours: that requiring unanimous support for accepting a proposal (i.e., setting a high threshold) can discourage authors from submitting many proposals, thus lowering the review burden.

A more analytical approach is taken by \citet{smith2021accept}. Here, the authors are also interested in the impact of self selection on the acceptance rate of a journal (or university). They study a system with multiple journals or universities announcing different thresholds in the presence of noisy reviews. Due to their motivation, their model does not appear to account for resubmissions, thus differing from our work in a key aspect.

A model for the journal peer review process was proposed and analyzed by \citet{azar2015model}. 
In this model, the authors also know their paper’s quality, referees observe noisy signals, and editors set an acceptance threshold in equilibrium. Similar to one of our insights in \cref{sec:dominating-value-discount}, a key takeaway from this work is that higher submission costs, while discouraging authors, can improve journal quality by deterring low-quality submissions. A key distinction in our model is that authors may resubmit the same paper multiple times to the same venue, reflecting the difference between the typical scenarios for conferences and journals. This feature gives rise to a broader range of design dimensions, such as the QB-tradeoff.

Several other works do more basic theoretical analysis of the impact of conference policies. In particular, they focus on the false positives (accepting bad papers) and false negatives (rejecting good papers) arising as a function of the number of reviewers and their individual qualities.
Based on such calculations, \citet{herron2012expert} suggests that obtaining a large number of low-quality reviews may be better than a small number of expert reviews.  
\citet{neff2006peer} focus in particular on the role of desk rejects by an expert editor.

In response to concerns by authors about the evaluation of their work when submitted to conferences or journals, the scientific community in general, and CS community in particular, has engaged in significant self-evaluation efforts. Many of these have focused on the quality and consistency of reviews provided to conferences \citep{shah2018design,cicchetti1991reliability,ebel1951estimation}.
\citet{cole1981chance} and \citet{tran2020open} study the reproducibility and randomness in review scores and acceptance decisions.
Furthermore, the community has experimented with (or at least suggested) different formats, including increasing the number of reviews per paper, multi-level or multi-stage evaluation processes, having reviews from past submissions follow a paper upon resubmission, and many others \citep{jecmen2020mitigating,noothigattu2018choosing,shah2018design,rogers2020can}.
Many of these approaches appear primarily driven by concern for authors and their desire for accurate evaluation of their submission, though some of them are also part of our evaluation.

Other attempts that try to mitigate the overwhelming demand for reviewers consider solutions based on mechanism design. 
\citet{srinivasan2021auctions} combine a bidding system and peer prediction to simultaneously incentivize high-quality reviews and high-quality submissions. \citet{su2021best} designs a mechanism that elicits ranking information truthfully from the authors, which is proven to empirically benefit the conference's quality.

In our idealized model, we assume that all reviews are i.i.d., that is, reviewers are subject to the same noise distribution. Naturally, this is a simplification. In reality, one of the difficulties faced by conferences and journals is how to aggregate the scores from reviewers with possibly very different scales or expectations.
Indeed, such aggregation is a well-known fundamental problem in statistics \citep{ammar2012efficient,cook2007creating,wang2018your}.
%\dkcomment{Should we say more about these papers?}

Peer review can also be viewed through the lens of a principal-agent problem: the principal decides on the review process and the acceptance rule, and the agents respond.
Besides peer review, related applications include admitting college students \citep{kannan2021best} and recruiting faculty \citep{zhang2021classification}, or endorsing a product \citep{gill2012optimal,lerner2006model}.
In particular, \citet{gill2012optimal,lerner2006model} study models in which the agent (such as an author) can choose among venues one that maximizes the expected utility, as determined by the chance of success and the prestige.
