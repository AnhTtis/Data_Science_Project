
\subsection{Implications}\label{sec:implications}
We suggest some possible interpretations and lessons that might be learned from our analysis.

\emph{Resubmission Gap.}
Perhaps the cleanest result concerns the resubmission gap. If a conference operates like our model with noiseless authors, then we would observe the following:  \emph{Every paper ever submitted to the prestigious conference is eventually accepted; however, at any given conference, many papers will be rejected, and thus need to be submitted multiple times.}  With many parameters, the vast majority of papers are rejected. 
On the surface, this sounds like a dystopian bureaucracy. 

The frequently proposed and obvious reaction is to accept all acceptable papers the first time, without making them resubmit multiple times.  This would allegedly decrease the reviewing load and increase author welfare (because each paper would only be reviewed once), without affecting the quality of the conference because all of the papers were going to be accepted anyway. Superficially, this seems very reasonable.

However, our model warns that this is an unlikely outcome.  Instead, by lowering its acceptance threshold, the conference would also lower its de facto threshold.  While the papers currently being submitted could be overwhelmingly accepted in one round, it would invite more, lower-quality submissions.
These lower-quality submissions would themselves be repeatedly submitted, and so the review burden would not necessarily decrease, and could even increase.
Furthermore, as these lower-quality papers dilute the conference value, author welfare may decline, as illustrated in \cref{fig:QA_Pareto_frontier_continuous}.

Of course, this is not a perfect reflection of reality. In particular, because authors are not all aware of their papers' qualities, some submissions are of low quality and are very unlikely to ever be accepted.  However, the experience of this article's authors is that for some prestigious conferences, the situation is similar to a large extent: the pool of submitted papers has been self-selected to those that will eventually appear in a good venue. 

Additional relevant modeling ``blind spots'' are that the review noise for some papers may be different from others, agents may have different levels of patience and different utilities for their outside options, and the distribution of paper qualities may react to the acceptance policy.  However, it is not clear that any of these model limitations fundamentally challenge the insight that there is a gap between the quality of papers implied by the threshold acceptance policy and the types of papers submitted and eventually accepted.

\emph{Threshold Policy and tradeoffs.}
We investigated how the conference's acceptance policy influences the tradeoffs faced by stakeholders in the review system, with particular emphasis on how conference quality trades off against review burden and author welfare.
In the preceding discussion of the resubmission gap, it was not clear what happens to the review burden as the acceptance policy becomes more strict or lenient.  This is because it depends on the prior distribution of paper qualities and the review quality.  This effect was studied in \cref{subsec:QB-tradeoff}, where we observe the following. First, extremely stringent policies are always Pareto optimal, since the review burden is minimized when the acceptance threshold is set to infinity. Second, lenient policies tend to be Pareto optimal when reviews are highly noisy and the prior expected quality is high. In such cases, the conference benefits from accepting more papers sooner, which corresponds to a low acceptance threshold.
% \dkcomment{Did we explicitly discuss/name/describe them this way?}\yzcomment{No, how confusing is this?}\dkcomment{I found it confusing.}\gscomment{I think the "3"types is from the previous write up.  We don't have the three types in the figure anymore, so this does not make any sense.  Currently, it is a bit of a mess.  But we identify two dicotomies.  First, if the expected quality is positive, then if we accept all papers, people still submit.  Second, is whether, on the margins, increases or decreasing the acceptance give better review burden for a similar marginal decrease in conference quality.}
% (1) accept only a few top papers; (2) accept all worthy papers; (3) accept nearly all papers, only attempting to weed out the worst. 
% \yichiedit{only accept positive-quality papers by setting a relatively high acceptance threshold, or accept some or even all negative-quality papers by setting a low acceptance threshold.}
All of these policies can be easily identified in practice.


% The advantage of policies (1) and (3) is that they tend to require less review.  For policy (1), this is because few papers are submitted; for policy (3), it is because few papers are submitted more than once. The advantage of policy (2) is that it maximizes the conference quality, though often at the expense of a high review burden.
% In our analysis, however, policy (3) is Pareto optimal only if the prior of paper qualities contains mostly positive papers.  If, instead, the paper quality prior is a unimodal distribution centered at 0, then such a policy with a negative de facto threshold will never be Pareto optimal.  
% We note that in the data learned from ICLR (displayed in \Cref{fig:tradeoff_categorical}), all the acceptance policies which accept negative-utility papers are Pareto-dominated. (However, this is not always the case for the models learned from ICLR data for different years or parameter settings). 

\emph{Discount Factor.}
As shown in \cref{sec:dominating-value-discount}, in the noiseless author setting, increasing the discount factor creates a strictly worse QB-tradeoff curve. This implies that an intervention that decreases the discount factor and burdens the authors may improve the QB-tradeoff.  Examples include long review times, rebuttal periods, or onerous formatting requirements. A smaller discount factor for resubmission will intuitively allow the conference to decrease its acceptance threshold while keeping the same de facto threshold, and thus may decrease the review load without impacting conference quality. Essentially, such interventions artificially internalize the negative externality of imposing reviews upon others.
In fact, such impositions on the authors may even increase the authors' utilities, though this outcome is far from universal.


Conversely, our model predicts that well-meaning efforts to reduce the resubmission burdens may very well worsen the QB-tradeoff, and may eventually harm the authors' utility in some cases.  Proposed and executed reforms include: decreasing the required time to review papers for a given conference, ``fast-track'' resubmissions of papers recently rejected with sufficiently high scores, and a ``desk reject'' phase, where papers that appear subpar are quickly returned to authors without review.  These reforms artificially increase the time discount $\TD$ by decreasing the time between submissions.  This forces the conference to increase its acceptance policy threshold if it would like to maintain the same de facto threshold (and thus maintain conference quality). Consequently, the overall review burden is increased and, in some instances, author welfare is harmed as well.


% \gsedit{Conversely, conferences may put in additional hurdles for authors with little or no direct benefit to the conference.  While our model predicts that these may reduce the number of reviews while keeping a constant conference quality, they will sometimes, though not always, hurt author welfare.}
% Of course, it should be emphasized that these observations are not to be taken as recommendations: in particular, as we will discuss in \cref{sec:limitations}, our analysis essentially ignores the authors' utilities, and a longer time until acceptance or more burden to submit leads to a decrease of this utility. The tradeoff should naturally include all three concerns, and our observations should be interpreted as emphasizing one aspect that may not have been considered enough in the past.

\emph{Acceptance Rate.}
In \cref{sec:acc_rate}, we showed that whether the acceptance rate increases, decreases, or remains steady as the acceptance threshold increases depends on the paper quality prior distribution, in particular, a quantity resembling the hazard rate of the quality prior.
This warns against using the acceptance rate as a signal of quality. 
At issue is that for certain priors of paper quality, as the selectivity increases, the fraction of papers near the boundary may increase, leading to a larger acceptance rate.  

Additional factors may complicate this picture. For example, higher-quality papers may have different distributions of review noise than lower-quality papers.

\emph{Quality vs.~Quantity of Reviews.}
Our empirical results here discourage the strategy of soliciting a large number of reviews per paper. As shown in \cref{sec:noisy_abm}, any number of solicited reviews larger than $3$ greatly burdens the review system but is unlikely to bring enough benefits to the conference quality. 
% \dkcomment{Again, this result is only presented in the appendix, so a reader may lack context here. For these kind of results, we might explicitly point to the appendix, \`{a} la ``As we show in Appendix ..., any number of solicited ...''.}
Instead, our model predicts that a small number of solicited reviews, even with one review per paper, can be optimal if the conference is able to find the optimal acceptance threshold. The intuition is that when authors know the quality of their papers well enough, any number of solicited reviews, as long as it is combined with the optimal acceptance threshold, can take advantage of authors' self-selection such that only the desired papers are submitted, and eventually accepted. 

\emph{Institutional Memory.}
Our results in \cref{sec:memory_policy} indicate that having historical reviews follow submissions, or allowing the conference to limit the number of times a paper can be submitted, can help improve the QB-tradeoff (\cref{fig:memory_policies}). However, given that the improvement in the maximum conference quality is rather marginal, its main effect is to reduce the review burden. The intuition is that with such a policy, the conference can be strict in the first few rounds and relax the acceptance threshold for repeated resubmissions so that only the good papers will be submitted and accepted more quickly.

In summary, our results provide the following insight: the design of the acceptance threshold should consider the de facto threshold it may induce. Being aware of the authors' best response, the conference can optimally set its acceptance threshold to achieve a near-optimal quality with a small review burden, even with a small number of solicited reviews per paper and a simple memoryless acceptance policy.
% For any de facto threshold a conference would like to implement, a first-order concern is what acceptance policy to employ. 

\subsection{Limitations and Future Directions} \label{sec:limitations}

% \vspace{2mm}
% \emph{Author's Utility.}
% A major limitation of our work is that we do not explicitly account for author utility.  Although our model of author utility makes sense from the point of view of modeling the authors' local \emph{decisions}, it does not capture the authors' actual utility in a holistic sense. For example, if the conference accepted everything, then every author's utility would be $\ConfValue$ according to the model (and indeed, authors would prefer having their papers accepted); however, the prestige of a conference is also derived from its selectivity, so the authors of high-quality papers would not be happy with this outcome.  
% In reality, $\ConfValue$ is a long-term function of the quality of papers appearing in that venue, a fact that we do not include in the model.

% Indeed, the discussion above about adding needless annoyances to submissions to discourage low-quality submissions is necessarily incomplete without author utility. 
% Even if such policies greatly reduced the review burden, this improvement would have to be weighed against the additional burden to authors.

% We do note that, typically in our analysis, for a fixed de facto threshold,  the average number of submissions for a paper is closely related to the review burden and is a reasonable proxy for the author utility.  Thus, the authors' interests are, to a limited extent, present.  However, this relationship breaks down on occasion, for example, when discussing the number of reviews the conference should allocate per submission.  Here, the cost to the conference is measured in reviews, while the cost to the author is measured in rounds of submission, and because $\NumReviews$ is not fixed, these are not the same. 

% Future work could make author utility a first-order concern, in part by modeling the conference value $\ConfValue$ as dependent on the quality of the papers at the conference.

\emph{A Single Prestigious Conference.}
One limitation of this work is that we assume a single prestigious conference.  As mentioned in \cref{sec:model}, this can model several prestigious conferences that are more or less cooperating to uphold community standards.

Of course, in reality, there is an ecosystem of conferences, and not all of them are either top-tier or a side option. In such a setting, our analysis could model the decision to submit to a top-tier or second-tier conference. The utility for submitting to the second tier could be normalized to 1. The issue with this, however, is that the second-tier conference still needs to review its submissions. Furthermore, having multiple outside options can increase heterogeneity in authors’ utilities of outside options --- authors with higher-quality papers may have more attractive alternatives. This heterogeneity could lead to more complex, non-threshold author best responses.
There are other analyses (see related work in \cref{sec:related-work}) that have considered venues of different values. 

Furthermore, our model omits competition between conferences.  For example, conferences may compete to attract more papers by attempting to increase their quality, making their acceptance policy more predictable, creating a faster turnaround time, etc. 
Future work could extend our model to these settings. 

\emph{Heterogeneity.}
We also did not model various sources of heterogeneity in the process. For example, the qualities of reviews are not uniform, and different authors have different levels of patience (e.g., a Postdoc who will be on the job market vs.~a first-year student or a tenured faculty member). We are also not modeling the effects of biases that may impact different researchers disproportionately.  The uneven impact on different author populations would be made more complex by co-authorship.  As with the previously mentioned endogenous review quality, a main difficulty would be that this model would have more parameters, and as such require learning/setting them, which could lead to arbitrary choices.

\emph{Additional Feedback Loops.}
There are several feedback loops that we disregard.  We model paper quality as exogenous; however, in reality, it is largely determined by authors who decide how much effort or time to expend improving their papers.  Authors often write a paper to target particular venues. As the venues change their policies, the underlying distribution of paper qualities is likely to change.  Moreover, authors may improve their papers in response to reviewer feedback. Additionally, in our model, the review burden does not impact the quality of the reviews or the amount of time authors (who are typically also the reviewers) spend writing papers. 

As mentioned in the discussion of related work, several past papers do try to model these complexities and provide insights with agent-based simulations \citep{kovanis2016complex, bianchi2018peer, squazzoni2012saint}.

Here, we focus on a simpler and cleaner model than can be afforded when including these complexities.  Apart from the analysis being more difficult, it is often difficult to know precisely how these feedback loops function, which may lead to even greater uncertainty regarding the accuracy of a model, and any insights derived from it.  

\emph{Modeling Paper Quality.}
Finally, it should be noted that papers do not have an ``objective'' quality that can be projected to a single dimension or even multiple dimensions.  One approach, which adds minimal complexity along these lines, is to distinguish between different types of poor-quality papers.  Some papers may be deemed low quality because they are methodologically flawed;  others because their contributions may be incremental.  The first of these might be easier for reviews to detect and agree on than the second.

% \subsection{Broader Impacts}

% \dkcomment{I don't understand this paragraph, or rather, why it's there. I assume that MS has some requirement of discussing ``Broader Impact''? Typically, in NSF speak, ``Broader Impacts'' means ``What is the potential impact of your paper besides a handful of your friends citing it in follow-up papers? We have a very good concrete case for broader impact: our paper might genuinely impact the conversation about conference acceptance/review policies, and thus have an impact in practice. I would emphasize that here, rather than speculating about related areas.}
% \yichiedit{
% Although we primarily focus on understanding the problem of conference peer review, the implications of this work can get beyond the peer review setting. Broadly speaking, we study a problem where a principal wishes to select only high-quality submissions. Agents whose submissions are selected by the principal get some rewards which are larger than the side option. In this principal-agent admission setting, even though the agents know their item’s true qualities, due to misaligned incentives, the principal has to make the decision based only on her own noisy signals. A key aspect of the model is agents' reentries: agents are primarily interested in one (or a group of similar) venue(s), and can submit multiple times to the these entities. This framework can be applied to many evaluation-admission process, including grant reviewing, school admissions (where students prefer to reapply next year instead of accepting the offers of the second-choice schools), and company recruitment (again, where agents aim to work only at the top companies). The insights gained from our paper can thus be generalized to these applications. For example, to preserve its reputation, a top university tend to set a relatively high selection bar which may end up with rejecting a large number of good students.}

% In spite of the flaws in our model, we believe that most of our insights presented in this paper exhibit robust applicability across broader scenarios, including real-life situations. In particular, we explain, from the game theory aspect, why prestigious conferences maintain selection threshold that surpasses the mere ``de facto'' standard. Furthermore, in addition to advocating for the maximization of the conference quality, we propose the concept of quality-burden trade-off, where the dimension of review burden is usually under-addressed in real-life. Our discussions on how to achieve the Pareto optimal trade-off and how to improve it can serve as pragmatic guidelines for conference committee to design better conferences. We envisage our findings not merely as an academic contribution, but as the  change the conversation concerning conference acceptance and review policies. In a broader context, our insights can even be useful for a lot more applications including grant reviewing, school admissions and company recruitment.


\subsection{Broader Impact and Conclusion} 

Despite the fact that there may be no agreed-upon objective metric, still, the rigor of peer review is important to a healthy future of academic research.
We avoid concrete recommendations, as many quantities and observed trends depend on model parameters and on unmodeled real-life effects. We hope that our theory (and simulations) can steer the discussion, uncover parameters to focus on, and inform decision makers. We believe that the focus on the resubmission gap, and the importance of reviewing quality over quantity, are important points to start a discussion in the community which may not have been as easily identified without studying a model like ours. 
We thus envisage our findings not merely as an academic contribution, but as catalysts for changing the conversation concerning conferences' acceptance and review policies.
Even more broadly, the impact of our work can reach beyond the context of conference peer review, finding relevance across diverse applications, including grant reviewing,  school admissions, and company recruitment.
