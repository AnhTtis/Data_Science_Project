\documentclass[11pt]{article}
\usepackage[letterpaper, margin=1in]{geometry}

\usepackage{amsmath,amsfonts,amssymb,bbm,amsthm, bm}
\usepackage{graphicx}


\usepackage[numbers]{natbib}



\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{subcaption}
\usepackage{fontawesome}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}

\newenvironment{rtheorem}[3][]{

\bigskip

\noindent \ifthenelse{\equal{#1}{}}{\bf #2 #3}{\bf #2 #3 (#1)}
\begin{it}
}{\end{it}}

\def\QED{{\phantom{x}} \hfill \ensuremath{\Box}}

\newcommand{\extraproof}[1]{\rm \trivlist \item[\hskip \labelsep{\em Proof of #1. }]}
\def\endextraproof{\QED \endtrivlist}



\usepackage{xcolor}
\newcommand{\defn}[1]{{\textbf{\textit{#1}}}}

%%% URL format and footnote without number
\renewcommand\UrlFont{\rmfamily\itshape}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}



\usepackage{xspace,ifthen} % added by David

\newcommand{\gs}[1]{\gscomment{#1}}
\newcommand{\yichi}[1]{\yichicomment{#1}}
\newcommand{\fang}[1]{\fangcomment{#1}}
  
  
\providecommand{\half}{\ensuremath{\frac{1}{2}}\xspace}
\providecommand{\third}{\ensuremath{\frac{1}{3}}\xspace}
\providecommand{\quarter}{\ensuremath{\frac{1}{4}}\xspace}

\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareMathSymbol{\N}{\mathord}{AMSb}{"4E}
\DeclareMathSymbol{\R}{\mathord}{AMSb}{"52}
\DeclareMathSymbol{\Z}{\mathord}{AMSb}{"5A}

\providecommand{\SetCard}[1]{\ensuremath{| #1 |}\xspace}
\providecommand{\SET}[1]{\ensuremath{\{ #1 \}}\xspace}
\providecommand{\Set}[2]{\ensuremath{\SET{#1 \mid #2}}\xspace}

\providecommand{\PROB}{\ensuremath{{\rm Prob}}\xspace}
\providecommand{\Prob}[2][]{\ensuremath{%
\ifthenelse{\equal{#1}{}}{\PROB[#2]}{\PROB_{#1}[#2]}}\xspace}
\providecommand{\ProbC}[3][]{\Prob[#1]{#2\;|\;#3}}
\providecommand{\ProbD}[4][]{\Prob[#1]{#2\;|\;#3}} 
\providecommand{\Expect}[2][]{\ensuremath{%
\ifthenelse{\equal{#1}{}}{\mathbb{E}}{\mathbb{E}_{#1}}%
\left[#2\right]}\xspace}
\providecommand{\ExpectC}[3][]{\Expect[#1]{#2\;|\;#3}}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}


\input{localdefs}
\date{}




\title{A System-Level Analysis of Conference Peer Review\footnote{The data sets and code for experiments in this paper are available at  \url{https://github.com/yichiz97/Conference-Peer-Review}.}}
\author{
  Yichi Zhang\thanks{University of Michigan, \texttt{yichiz@umich.edu}, supported in part by NSF CCF \#2007256}\and Fang-Yi Yu\thanks{George Mason University, \texttt{fangyiyu@gmu.edu}, supported in part by NSF IIS \#2007887}\and Grant Schoenebeck\thanks{University of Michigan, \texttt{schoeneb@umich.edu}, supported in part by NSF CCF \#2007256}\and David Kempe\thanks{University of Southern California, \texttt{david.m.kempe@gmail.com}, supported in part by ARO MURI W911NF1810208}
}
\begin{document}
\maketitle
\begin{abstract}
We undertake a system-level analysis of the conference peer review process. The process involves three constituencies with different objectives: authors want their papers accepted at prestigious venues (and quickly), conferences want to present a program with many high-quality and few low-quality papers, and reviewers want to avoid being overburdened by reviews.
These objectives are far from aligned; the key obstacle is that the evaluation of the merits of a submission (both by the authors and the reviewers) is inherently noisy.
Over the years, conferences have experimented with numerous policies and innovations to navigate the tradeoffs. These experiments include setting various bars for acceptance, varying the number of reviews per submission, requiring prior reviews to be included with resubmissions, and others. The purpose of the present work is to investigate, both analytically and using agent-based simulations, how well various policies work, and more importantly, why they do or do not work.

We model the conference-author interactions as a Stackelberg game in which a prestigious conference commits to a (threshold) acceptance policy which will be applied to the (noisy) reviews of each submitted paper; the authors best-respond by submitting or not submitting to the conference, the alternative being a ``sure accept'' (such as arXiv or a lightly refereed venue).  Our findings include:
\begin{itemize}
\item observing that the conference should typically set a higher acceptance threshold than the actual desired quality, which we call the \emph{resubmission gap} and quantify in terms of various parameters. 
\item observing that the reviewing load is heavily driven by resubmissions of borderline papers --- therefore, a judicious choice of acceptance threshold may lead to fewer reviews while incurring an acceptable loss in conference quality.
\item showing that conference prestige, reviewer inaccuracy, and author patience increase the resubmission gap, and thus increase the review load for a fixed level of conference quality.
\item observing that depending on the paper quality distribution, stricter reviewing may lead to higher or lower acceptance rates --- the former is the result of self selection by the authors.
\item finding that a relatively small increase in review \emph{quality} or in self assessment by the authors is much more effective for conference quality control (without a large increase in review burden) than increases in the \emph{quantity} of reviews per paper. 
\item showing that keeping track of past reviews of papers can help reduce the review burden without a decrease in conference quality.
\end{itemize}
For robustness, we further consider different models of paper quality and compare our theoretical results to simulations based on plausible parameters estimated from real data.
\end{abstract}



\section{Introduction}
Conferences play an important role in the publication and scientific dissemination process in computer science. They aim to provide attendees with access to high-quality and recent research results\footnote{in addition to networking opportunities}, and authors view the ability to publish and share their recent results at high-quality venues as conferring scientific credibility and thus status. In order to do so, conferences rely on significant volunteer work from the community, most notably in reviewing large numbers of submitted papers to evaluate their scientific merit.

While the conference publication process seems to have served the community fairly well overall --- in particular enabling a high speed of dissemination of scientific results --- different members of the community often also see significant room for improvement: authors often feel that their submitted papers are not evaluated sufficiently competently\footnote{and of course always with reviewers erring on the negative side}, while conference attendees sometimes find the program diluted with less interesting work.
Attempts have been made by conferences to mitigate such concerns, one of which is to increase the number of reviews assigned to each submission.\footnote{Assigning five reviewers to one paper is no longer rare for top computer science conferences.} 
However, this approach increases the review burden; indeed, many members of the scientific community now feel overloaded with conference reviewing requests. The approach has the potential to lead to a vicious cycle: the substantial increase in peer review workload\footnote{also exacerbated by the growth of the research community} may lead to lower-quality reviews, in turn leading to more resubmissions of the same papers and thus a higher review burden.

Our high-level goal is to understand how the parameters of the system, together with the conference's review policy decisions, affect the tradeoff between the conference quality and the review burden on the community.
Given a fixed number and quality distribution of submissions, the conference's quality is increased by accepting more good and fewer bad papers.
Distinguishing between good and bad papers more accurately requires more reviews, along with enticing self selection by authors.

We model the conference review process as a Stackelberg game between a top conference and the (homogeneous and strategic) authors (described in detail in \cref{sec:model}).
The authors' papers have qualities (positive or negative) drawn from a commonly known distribution over a (finite or infinite) set; however, both the conference's reviewers and (in some of our analysis) the authors themselves only obtain imperfect signals about the quality of a paper.
The conference's quality is the sum of qualities of the accepted papers,  normalized by the total number of papers.
The conference commits to a review and acceptance policy, which prescribes how many independent reviewers are assigned to each paper, and what the criteria for acceptance are.
In \cref{sec:memory_policy}, we also study policies with ``institutional memory,'' including policies limiting the number of times a paper can be resubmitted, and policies requiring past reviews to be included with resubmissions.

In response to the conference's policies, the authors face a binary decision in each round: whether to submit to the top conference (which leads to some positive utility if the paper is accepted, but zero utility if the paper is rejected), or submit to a safe choice (such as a second-tier conference or arXiv) and get a guaranteed smaller positive utility. 
The utility of acceptance to either venue is exponentially time-discounted in the number of resubmission rounds, modeling that authors prefer timely acceptance of their result. 
Authors best-respond, i.e., choose a utility-maximizing action, based on their private (potentially noisy) signal about their paper's quality as well as the historical reviews collected when the paper was rejected in previous rounds. 
We primarily focus on the following high-level questions: 

\begin{itemize}
    \item The fact that rejected papers can be resubmitted means that weaker papers may be eventually accepted if authors are patient enough. What is the impact of resubmissions on the gap between a conference's acceptance threshold and the de facto distribution of paper qualities at the conference?
    \item What is the range of Pareto optimal review and acceptance policies with regard to the tradeoff between conference quality and review burden?  How is it impacted by conference prestige, authors' patience, and/or review quality?
    \item How does the conference's acceptance policy affect its acceptance rate? Note that this is non-obvious, as a strict policy may lead to a lot of self selection on the part of the authors, and thus to a \emph{higher} acceptance rate.
    \item How does the number of reviews per round affect the overall review burden? What is the tradeoff between review quality and quantity, i.e., how many noisy reviews approximate one high-quality review?
    \item How helpful is institutional memory? Can the conference significantly lower the review burden or increase quality by limiting the number of resubmissions, treating resubmitted papers differently, or requiring past reviews to be included?
\end{itemize}



To ensure robustness of our results, we investigate these questions under different models regarding (1) the quality distributions of papers and distributions of review noise, and (2) the information authors have about the quality of their own work.

For (1), we consider three models: (a) a model with continuous paper qualities (e.g., drawn from a Gaussian distribution), where reviews are also continuous with additive noise, (b) a class of categorical models with several discrete paper qualities and a discrete scale of review scores, and (c) a model with binary paper qualities (good/bad) and reviews (accept/reject recommendations).  The advantage of (a) and (c) is that they are characterized by a small number of meaningful parameters, making it possible to systematically explore the dependence of outcomes on these parameters.
On the other hand, (b) allows for more realistic modeling of real-world conferences, and we estimate the parameters from publicly available review data for the ICLR 2020 and 2021 conferences \cite{iclr2020review,iclr2021review}.


For (2), we consider authors with perfect information about their paper's intrinsic quality, and authors who themselves only receive noisy signals. The advantage of the former is that authors do not learn new information from reviews, so each author's decision is either to submit her paper until it is accepted, or to immediately submit to the sure bet option. In turn, this simplicity of best responses allows us to prove theoretical results about the outcomes.
On the other hand, this behavior is also somewhat unrealistic, motivating the study of a model in which authors themselves have noisy signals, and update their beliefs about the paper's quality based on the reviews. The resulting Bayesian reasoning makes the model too complex for a theoretical analysis, so we investigate it using agent-based modeling (ABM) and simulations.

We emphasize results in the model which portrays them most clearly.  We hope that this helps the reader quickly grasp the underlying intuition.  However, we also typically test the robustness of the results in the remaining models.  In particular, many of our results are shown theoretically in simple models, but are shown to continue to hold (at least qualitatively) based on simulations for the more complex models. 

While the models are necessarily a simplification, and one should therefore be very cautious of directly basing concrete decisions on the results, we hope that our theoretical results (and simulations) can steer the discussion, uncover parameters to focus on, and inform decision makers.

\subsection{Summary of Results}

\paragraph{Noiseless Authors: Theoretical Results.}


We first assume that authors know the quality of their manuscripts, but reviewers only obtain noisy signals  of the quality. 

It is sometimes argued that conferences should accept every paper that is ``above the bar''. Because every submitted paper will be resubmitted until accepted (since the authors learn no new information from the reviews), the conference's \emph{acceptance threshold} induces a \emph{de facto threshold}: a manuscript quality above which every paper will be eventually accepted and below which no manuscript will be submitted. The acceptance threshold and de facto threshold are typically different, sometimes significantly so (in particular when the conference is very attractive to authors, the authors are very patient, or the noise of reviews is large) --- we call the difference the \emph{resubmission gap}. 
In most situations, if a conference naively sets an acceptance threshold identical to their ideal de facto threshold, they will ultimately accept substandard papers, in rounds in which enough reviewers had noisy reviews that ended up too high. We call this phenomenon the \emph{resubmission paradox.} Instead, the conference should select an acceptance threshold higher than the desired threshold of conference acceptances.  However, the resulting ``optimal'' is then counter-intuitive: every paper that is submitted is eventually accepted, yet each round many of the papers are rejected.
We exactly characterize the resubmission gap in several settings.

Second, we consider the tradeoff between the conference quality and the review burden (the total number of reviews per paper throughout its resubmission process), focusing on the Pareto frontier.  At a high level, the review burden closely tracks the number of papers just above the de facto threshold, because such papers typically require numerous resubmissions before acceptance.   
In particular, a conference may be able to significantly reduce the long-term review burden by choosing the threshold such that there are fewer borderline papers near the resulting de facto threshold.

Third, we show that if the prestige of the conference increases or the patience of authors increases, then the tradeoff between the  conference's quality and the review burden becomes strictly worse. Thus, one cause of more reviews may be the high prestige placed on certain venues.  This also warns that policies which affect these items may have unintended consequences.
We obtain similar results in terms of the review quality. If the signals of reviewers  in one setting Blackwell-dominate those in another setting, the conference can obtain a weakly better tradeoff between quality and review burden. 
However, doing so may necessitate using non-monotone acceptance policies, which do not treat papers with strictly more positive reviews better than those with strictly less positive reviews.  We show that a better tradeoff for better signals can be achieved under a restriction to threshold policies as well, but only with the additional assumption that the review signal is \emph{informative} in the sense of monotone likelihood ratios.
 
Fourth, we study what impacts the acceptance rate of the conference.  We show that as the noise of the reviewers' signals shrinks, the relation between the acceptance threshold and the acceptance rate is related to a quantity resembling the \emph{hazard rate} of the prior distribution of papers.
If the quantity increases, corresponding to thin tails of the prior, then as the acceptance threshold increases, the acceptance rate decreases. However, for priors with thick tails, a stricter acceptance threshold often leads to a larger acceptance rate. We illustrate the robustness of these observations beyond the setting where the review noise is sufficiently small using simulations.
 


\paragraph{Noiseless Authors and Real-World Parameters}  
We further investigate the above phenomena in a more realistic setting based on categorical data.
One common attempt at improving a conference's quality is to solicit more reviews per submission which, as believed, can help distinguish the good papers from the bad ones. Thus, we next (in \cref{subsec:opt_m,subsec:qual-quant}) study how the number of solicited reviews for each submission impacts the conference's quality and the average total number of reviews for each paper. (Recall that each paper may be submitted multiple times.) 
We find that in our models, increasing the number of solicited reviews per paper beyond three rarely leads to better outcomes.  This is because a small number of solicited reviews can be optimal with a carefully chosen acceptance threshold.
In contrast, we find that increasing the \emph{quality} of reviews often substantially improves the Pareto frontier.


\paragraph{Authors with Noisy Signals}  
We continue to investigate the robustness of the above results in a setting where authors do not exactly know the quality of their paper and can learn about it from reviews (\cref{sec:noisy}).  In this context, the Pareto frontier for conference quality vs.~review burden is significantly worse than with perfectly appraised authors --- the reason is that the latter can be  compelled to self-select with carefully chosen acceptance thresholds.  However, it is not clear if such gains are realizable in practice. 
Perhaps, new discipline norms of learning the quality of one's paper prior to submission (for example, by sharing early manuscripts with colleagues for feedback) could provide authors with accurate quality signals. 
Or perhaps, in practice, overcoming authors' unfounded admiration of their own work is not possible.


\paragraph{Memory in the System.}
Another popular type of proposal is to give the system more memory, either by limiting the number of resubmissions of the same manuscript or by reusing reviews.\footnote{Another reason for requiring the inclusion of prior reviews --- not modeled here --- is that it lets the conference ascertain that specific concerns from earlier versions have been addressed.}  
Our results here (in \cref{sec:memory_policy}) show that  
the main effect of having memory within the review process is that the conference can reduce the review burden while preserving the same (or slightly better) conference quality. However, such an effect can be marginal in some cases; thus, it is not clear whether it is worthwhile to implement these policies broadly. It should be noted, though, that our analysis here is rather preliminary, and only carried out for the binary model.


Our models are primarily designed to investigate the tradeoff between conference quality and the reviewing burden for the community. They necessarily abstract away several other aspects of the conference submission ecosystem which would also be worth investigating, most importantly the authors' utility.  These limitations are discussed in more depth in \cref{sec:limitations}.


\subsection{Related Work}
\label{sec:related-work}
Not surprisingly, given the importance of peer review in science, several attempts have been made by different research fields to simulate, understand, and improve the process.
When considering the systems level, agent-based models (ABMs) have been one of the techniques of choice.
The review article by \citet{feliciani2019scoping} gives a fairly comprehensive summary of this line of work. It suggests several general themes to focus on in models: editorial strategies, matching submissions with reviewers, decision making, biases and calibration, and comparisons of alternative peer review systems. 

Among the more prominent works using ABMs are those by \citet{kovanis2016complex,kovanis2017evaluating}. 
\citet{kovanis2016complex} propose a model for a holistic study of the scientific publication ecosystem --- this model includes the acquisition of resources (such as status) by authors, which can be leveraged into future papers.  Their subsequent work \cite{kovanis2017evaluating} builds on these models and implementations to evaluate several alternative systems for peer review.
These models and results differ from ours in several key dimensions: the authors are not strategic, they do not focus on fine-grained policies by journals (or in our case, conferences), and due to the holistic nature and complexity of the model, the model is only amenable to simulation, but not analytically tractable.

Two papers by \citet{bianchi2018peer} and \citet{squazzoni2012saint} also use agent-based modeling approaches. They particularly focus on the fact that researchers must decide how to divide their time between writing and reviewing papers, and investigate (experimentally) the impact of various policies on the efficiency of peer review.
Similarly, \citet{thurner2011peer} and \citet{d2017can} use agent-based models to investigate a specific aspect of peer review, namely, selfish behavior on the part of referees, who may not have incentives to see other strong work published.

\citet{allesina2012modeling} also uses agent-based modeling, in this case to understand the impact of different high-level approaches (editorial desk rejects, bidding on papers, etc.) on the overall reviewing load.
\citet{Roebber2011PeerReview} use agent-based modeling to evaluate strategies for program officers of funding agencies. One of their findings is similar to ours: that requiring unanimous support for accepting a proposal (i.e., setting a high threshold) can discourage authors from submitting many proposals, thus lowering the review burden.

A more analytical approach is taken by \citet{smith2021accept}. Here, the authors are also interested in the impact of self selection on the acceptance rate of a journal (or university). They study a system with multiple journals or universities announcing different thresholds in the presence of noisy reviews. Due to their motivation, their model does not appear to account for resubmissions, thus differing from our work in a key aspect.

Several other works do more basic theoretical analysis of the impact of conference policies. In particular, they focus on the false positives (accepting bad papers) and false negatives (rejecting good papers) arising as a function of the number of reviewers and their individual qualities.
Based on such calculations, \citet{herron2012expert} suggests that obtaining a large number of low-quality reviews may be better than a small number of expert reviews.  
\citet{neff2006peer} focus in particular on the role of desk rejects by an expert editor.

In response to concerns by authors about the evaluation of their work when submitted to conferences or journals, the scientific community in general, and CS community in particular, has engaged in significant self evaluation efforts. Many of these have focused on the quality and consistency of reviews provided to conferences \cite{shah2018design,cicchetti1991reliability,ebel1951estimation}.
\citet{cole1981chance} and \citet{tran2020open} study the reproducibility and randomness in review scores and acceptance decisions.
Furthermore, the community has experimented with (or at least suggested) different formats, including increasing the number of reviews per paper, multi-level or multi-stage evaluation processes, having reviews from past submissions follow a paper upon resubmission, and many others \cite{jecmen2020mitigating,noothigattu2018choosing,shah2018design,rogers2020can}.
Many of these approaches appear primarily driven by concern for authors and their desire for accurate evaluation of their submission, though some of them are also part of our evaluation.

Other attempts that try to mitigate the overwhelming demand for reviewers consider solutions based on mechanism design. 
\citet{srinivasan2021auctions} combine a bidding system and peer prediction to simultaneously incentivize high-quality reviews and high-quality submissions. \citet{su2021best} designs a mechanism that elicits ranking information truthfully from the authors, which is proven to empirically benefit the conference's quality.

In our idealized model, we assume that all reviews are i.i.d., that is, reviewers are subject to the same noise distribution. Naturally, this is a simpification. In reality, one of the difficulties faced by conferences and journals is how to aggregate the scores from reviewers with possibly very different scales or expectations.
Indeed, such aggregation is a well-known fundamental problem in statistics~\cite{ammar2012efficient,cook2007creating,wang2018your}.
%\dkcomment{Should we say more about these papers?}

Peer review can also be viewed through the lens of a principal-agent problem: the principal decides on the review process and the acceptance rule, and the agents respond.
Besides peer review, related applications include admitting college students~\cite{kannan2021best} and recruiting faculty~\cite{zhang2021classification}, or endorsing a product~\cite{gill2012optimal,lerner2006model}.
In particular, \cite{gill2012optimal,lerner2006model} study models in which the agent (such as an author) can choose among venues one that maximizes the expected utility, as determined by the chance of success and the prestige.


\section{Model}\label{sec:model}
We consider a process of an \emph{author} (or group of authors) submitting a paper%
\footnote{The analysis focuses on the process for one paper. By considering the (independent) processes for multiple papers, we obtain a systems-level view. This is discussed in more detail in the section on the review burden, below.}
to a prestigious \emph{conference}\footnote{This single conference can refer to multiple ``equivalent'' conferences, e.g., STOC/FOCS.\label{ft:multi_conferences}}.
We model the submission-reviewing process as a multi-round game: in each round, the author decides to submit the paper either to the prestigious conference or an undiscriminating (``sure bet'') conference%
\footnote{Equivalently, we can think of the undiscriminating conference as a preprint site like arXiv. Another viewpoint of this simplification is that the papers under consideration are of sufficient quality that the sure bet will always accept them.}
that will always accept.
Upon submission, the prestigious conference will send the paper out for review and, based upon the reviews, decide to accept or reject.
If the paper is rejected from the prestigious conference, the author sees the reviews, and faces the same decision problem in the next round. 

In the game, two main agents actually make decisions: the author of the paper and the prestigious conference.
(The ``sure bet'' conference simply accepts all papers, and the reviewers simply provide reviews.) 
Whenever we refer to ``the conference'' as a decision maker, we therefore always mean the prestigious conference.

Each paper has a quality $\Qual$ drawn i.i.d.~from a commonly known prior paper quality distribution $\QualDist$ over the set of possible qualities. The support of $\QualDist$, i.e., the set of all possible paper qualities, is denoted by $\QualSet \subseteq \mathbb{R}$, and larger qualities correspond to better papers.
We note that the assumption that $\QualDist$ is commonly known is not essential --- it only matters that the conference (e.g., PC chair) know the distribution.
Without loss of generality, there exist both negative and positive values in $\QualSet$; otherwise, the conference would simply accept/reject all papers without review. 
When the set of qualities is discrete, we write $\QualProb{q} = \Prob{\Qual = q}$; when it is continuous, we use $\QualDens{q}$ to denote the density of \QualDist at $q$.
Because all papers' qualities are drawn from the same distribution, we will not need to reference a specific paper in our notation.

The conference cannot observe the true quality of the paper, but will solicit some number, $\NumReviews$, of reviews for each submission.  
Each review $S_j$, for $j = 1, \ldots, \NumReviews$, is a random variable drawn i.i.d.~from a distribution $\RevSigDist[q]$ where $q$ is the paper's quality. The outcome of the $j$th review is $\RevSig{j} \in \SigSet \subseteq \R$ where $\SigSet$ is the set of possible review scores, and a higher score denotes a more positive review. We assume that $\RevSigDist[q]$ has full support on $\SigSet$ for every $q$.
Thus, the reviews are independent conditioned on the paper's quality. We write $\RevSigV$ for the vector of the $\NumReviews$ reviews. We let $U(\RevSigV) = \ExpectC[\Qual]{\Qual}{\RevSigV, \QualDist, \RevSigDist}$ denote the expected quality of a paper conditioned on the reviews \RevSigV. We assume that the reviews are \emph{informative} about the paper's quality, in the sense of monotone likelihood ratio: 

\begin{definition}[Informative Reviewer Signal \cite{karlin1956theory}] \label{def:informative}
The reviewer's family of signal distributions $\RevSigDist$ is \emph{informative} if it satisfies the monotone likelihood ratio (MLR) property; that is, if for any $q'>q$, whenever $\REVSIGP > \REVSIG$, $\frac{\RevSigProb[q']{\REVSIGP}}{\RevSigProb[q]{\REVSIGP}} > \frac{\RevSigProb[q']{\REVSIG}}{\RevSigProb[q]{\REVSIG}}$.
\end{definition}

For our theoretical results, unless otherwise specified, we assume that review signals are informative.
It is well known that a wide class of commonly used distributions --- including the Gaussian, Beta, and Exponential distributions --- have monotone likelihood ratios \cite{karlin:rubin,karlin1956theory}.

For some of our results, we assume that the authors perfectly observe $\Qual$, the paper's quality. This model has the advantage of being analytically tractable, because the authors do not learn new information from the reviews. We call such authors \emph{noiseless}.
For other results, we consider \emph{noisy} authors, who themselves only assess their papers' qualities approximately.
In this case, we assume that authors have noisy signals $\AuthSig$, which, similar to the conference's signals, are drawn according to some informative distribution $\AuthSigDist[q]$ for a paper of quality $q$. 
The author's signal is independent of the conference's signals conditioned on $\Qual$.
Noisy authors will update their beliefs about their papers' qualities based on the reviews in a Bayesian way. 

We study categorical and continuous models in this paper, as defined next. When a result does not specify one of these models explicitly, it holds for both models.

\begin{description}
\item{\textbf{Categorical Model:}}
In a categorical model, both $\QualSet \subset \R$ and $\SigSet \subset \R$ are finite (and ordered by their natural order on $\R$).
Such a model bears some similarity to the Dawid-Skene model \cite{dawid1979maximum}, although the latter typically does not assume an ordering on labels and thus also no informativeness requirement akin to monotone likelihood ratio.

\item{\textbf{Continuous Model:}} 
In the continuous model, we assume that both $\QualSet \subseteq \R$ and $\SigSet \subseteq \R$ are convex sets.
Furthermore, the reviewers' signals are obtained by adding to the true quality $\Qual$ a noise term drawn from a distribution \REVNOISEDIST which is \emph{independent} of $\Qual$ and has zero mean.\footnote{The zero-mean assumption is without loss of generality so long as the noise distribution is independent of the quality $\Qual$, as any (known) bias could be subtracted out from the reviews.}
More precisely, let $\Sigma_X\subseteq \R$ be the domain of the zero-mean noise (an open and convex set), and $\REVNOISEDIST: \Sigma_X \to (0,1)$ be a monotone increasing bijection.
Then, the cdf of the distribution of reviewer signals conditioned on a quality $\Qual=q$ is $\RevNoiseDist{x-q}$.
\end{description}

\subsection{Conference Acceptance Policy and Quality}
\label{sec:model-acc-policy}

The conference's main lever of control is its acceptance policy.
We primarily focus on \emph{memoryless} acceptance policies.
Under a memoryless acceptance policy, (1) the author can submit a paper an unlimited number of times; 
(2) the same number of reviews $\NumReviews$ and decision policy are used in every round; and (3) in each round $t$ in which the author (re-)submits the paper, the conference's decision depends only on the reviews obtained in round $t$. In other words, each submitted paper is treated as a fresh paper. For that reason, we typically omit the round $t$ from the notation when discussing memoryless policies. 
We discuss alternatives to memoryless policies in \cref{sec:memory_policy}, in particular, limiting the number of times a paper can be submitted, and having old reviews follow a resubmission. 
Except for these sections, unless stated otherwise, all acceptance policies are memoryless.  
We call an acceptance policy \emph{non-trivial} if it neither accepts nor rejects all submissions.

A memoryless acceptance policy is characterized by a function $\ACCMAP: \SigSet^m \to [0,1]$ which determines the probability with which each combination of review signals leads to a paper's acceptance.

We primarily focus on \emph{monotone} acceptance policies. 
A policy is monotone if for any two vectors $\RevSigV, \RevSigVP$ of reviews, 
$U(\RevSigV) \geq U(\RevSigVP)$ implies that $\AccMap{\RevSigV} \geq \AccMap{\RevSigVP}$.

A particularly natural class of monotone acceptance policies prescribe a conditional expected quality threshold.  
We largely restrict our attention to threshold policies because 1) they comprise a natural set of policies, and other policies seem unlikely to arise in practice; 2) it is both conceptually and computationally easy to search over such policies; and 3) we will see that such policies are sufficiently rich to induce any author strategy that can be induced using any monotone acceptance policy.

\begin{definition} \label{def:threshold-policy}
A \emph{threshold acceptance policy} $\ACCMAP[\tau, r]$ is characterized by a threshold $\tau \in \mathbb{R} \cup \{-\infty, +\infty\}$ and a probability $r\in [0,1]$. It accepts a paper with reviews \RevSigV when 
$U(\RevSigV) > \tau$, rejects the paper when 
$U(\RevSigV) < \tau$, and accepts the paper with probability $r$ when $U(\RevSigV) = \tau$.
\end{definition}

When the distribution $\Prob[\RevSigV]{U(\RevSigV)  = \tau}$ has no point mass for any $\tau$, the third (knife-edge) case is an event of probability 0; we then omit $r$ from the notation, and simply use $\ACCMAP[\tau]$ to denote the threshold acceptance policy with threshold $\tau$; in particular, this is true in the continuous model and some generalizations.

Once an acceptance policy \ACCMAP is fixed, the probability of a submitted paper being accepted in a particular round is only a function of its underlying quality $q$. We denote this probability by $\AccP{\ACCMAP}{q}$.
The following proposition shows that when \ACCMAP is monotone, $\AccP{\ACCMAP}{q}$ is monotone in $q$. 

\begin{proposition} \label{prop:monotone-prob}
Assume that the reviewers' signals are informative.
If \ACCMAP is a monotone acceptance policy, then $\AccP{\ACCMAP}{q}$ is non-decreasing in $q$. Moreover, if $\ACCMAP[\tau,r]$ is a non-trivial threshold policy, then $\AccP{\ACCMAP[\tau,r]}{q}$ is strictly increasing in $q$, for any prior $\QualDist$.
\end{proposition}  

In order to prove Proposition~\ref{prop:monotone-prob}, we first note the well-known fact (\cite{whitt1979note,milgrom1981good,shaked2007stochastic}) that the MLR property implies first-order stochastic dominance (FOSD) of the distribution of the signal conditioned on a higher parameter (as well as for the posterior distribution of the parameter conditioned on a higher signal).

\begin{lemma} \label{lem:FOSD}
Assume that the family of review signal distributions is informative.
Then, whenever $q' > q$, the signal distribution for $q'$ first-order stochastically dominates the distribution for $q$; that is, the distributions satisfy that $\Prob[\RevSig{} \sim {\RevSigDist[q']}]{\REVSIG \geq x} > \Prob[\REVSIG \sim {\RevSigDist[q]}]{\REVSIG \geq x}$ for all $x \in (\inf \SigSet, \sup \SigSet)$. 
\end{lemma}

We mentioned above that a higher signal also implies FOSD of the posterior quality distributions. The following lemma captures the stronger property that under the MLR property, this holds even for vectors of signals.

\begin{lemma} \label{lem:monotone_expected_quality}
Suppose $\RevSigV'$ and $\RevSigV$ are two vectors of informative signals that satisfy $\RevSigV'\ge \RevSigV$ component-wise, and the inequality is strict for at least one of the components. 
Then, $U(\RevSigV')> U(\RevSigV)$ holds for any prior $\QualDist$.
\end{lemma}

For continuous distributions, this lemma is proved in \cite{torres2005multivariate}.
We give a self-contained proof for the categorical case, which is largely analogous, in \cref{sec:FOSD-proof}.
We are now ready to prove Proposition~\ref{prop:monotone-prob}.

\begin{extraproof}{Proposition~\ref{prop:monotone-prob}}
We give the proof in the categorical model; it can be straightforwardly generalized to the continuous model.

Let $q' > q$. 
For each reviewer $i$, couple the draws of $\RevSig{i}$ from $\RevSigDist[q]$ and $\RevSigP{i}$ from $\RevSigDist[q']$ by drawing a (common) uniformly random quantile $x$ and letting $\RevSig{i}, \RevSigP{i}$ be the respective signals at quantile $x$ of the corresponding CDFs. 
Because, by \cref{lem:FOSD}, $\RevSigDist[q']$ (strictly) first-order stochastically dominates $\RevSigDist[q]$, this coupling ensures that $\RevSigP{i} \ge \RevSig{i}$;
furthermore, the inequality is strict with positive probability unless $\RevSig{i}=\max_s\in\SigSet$.
By applying this coupling to each individual review (which, recall, is drawn independently of other reviews), we obtain a coupling of vectors of reviews such that $\RevSigVP \geq \RevSigV$ always holds component-wise, and the inequality is strict for at least one of the components with positive probability. By Lemma~\ref{lem:monotone_expected_quality}, this coupling has the property that $U(\RevSigVP) \geq U(\RevSigV)$ always holds, and, conditional on \RevSigV, the inequality is strict with positive probability unless every component of $\RevSigV$ is the maximum signal (if the maximum exists).
Therefore, if \ACCMAP is a monotone acceptance policy, $\AccP{\ACCMAP}{q}$ can never decrease in $q$.

It remains to show that $\AccP{\ACCMAP}{q}$ is \emph{strictly} increasing in $q$ for non-trivial threshold policies $\ACCMAP[\tau,r]$.
First, we may assume w.l.o.g.~that $0 < r < 1$.
For if $r=0$, the policy is equivalent to the policy $\ACCMAP[\tau',\half]$ with any $\tau' \in (\max \Set{U(\RevSigV)}{U(\RevSigV) < \tau}, \tau)$, 
and if $r=1$, it is equivalent to the policy $\ACCMAP[\tau',\half]$ with any $\tau' \in (\tau, \min \Set{U(\RevSigV)}{U(\RevSigV) > \tau})$. Here, the minimum and maximum will be finite because the policy is assumed to be non-trivial.

By Lemma~\ref{lem:monotone_expected_quality}, there must exist \RevSigV, \RevSigVH with $U(\RevSigVH) \geq \tau \geq U(\RevSigV)$ such that at least one of the two inequalities is strict. Let \RevSigV be a vector of reviews maximizing $U(\RevSigV)$ subject to $U(\RevSigV) \leq \tau$.
Because $U(\RevSigVH) > U(\RevSigV)$, the vector \RevSigV cannot be maximal in all components.
Therefore, by the preceding coupling argument, when $\RevSigV$ is drawn with quality $q$, the corresponding vector $\RevSigVP$ drawn with quality $q'$ satisfies $U(\RevSigVP) > U(\RevSigV)$. 
By definition of \RevSigV, the review vector \RevSigVP gives rise to strictly higher acceptance probability than \RevSigV. For if $U(\RevSigV) < \tau$, then \RevSigV always leads to rejection, whereas (by maximality of $U(\RevSigV)$) \RevSigVP leads to acceptance with probability at least $r > 0$.
And if $U(\RevSigV) = \tau$, then \RevSigV leads to acceptance with probability $r < 1$, whereas \RevSigVP leads to acceptance with probability 1.

Therefore, for non-trivial threshold policies, the coupling ensures that a paper of quality $q'$ is accepted at least whenever a paper of quality $q$ is accepted, and with strictly positive probability, only the paper with quality $q'$ is accepted.
This completes the proof.
\end{extraproof}

In round $t$ of (re-)submission, we suppose that the conference faces a (the same) number $\NumNewPapers$ of new papers as well as the previously rejected papers from the past $t-1$ rounds. 
We define the \emph{quality} $\CONFUTIL$ of the conference as the expected value of the sum of all the accepted papers' qualities in round $t$, normalized by $\NumNewPapers$ and taken in the limit of $t$.  As is typical in Stackelberg games, we assume that the authors best-respond; if there are multiple best responses, for convenience of analysis, we assume that we can prescribe a particular tie breaking.
Note that when $t \to \infty$, the expected number of submissions in round $t$ that have previously been submitted $\ell$ times converges for any $\ell \in \mathbb{N}$.

\subsection{Author Utility and Decisions}
In terms of timing, first, the conference announces its review and acceptance policy; subsequently, in each round $t$, the author decides whether to submit the paper to the conference or the ``sure bet'' option.
The game ends when the paper is accepted at the conference or at the ``sure bet'' option.

Authors are characterized by two parameters: their time discount factor $\TD$, capturing how patient they are, and the prestige $\ConfValue > 1$ they ascribe to the conference. (We normalize the value of the sure bet option to 1.)
When the paper is accepted at the conference in round $t$, the author's utility is therefore $\AUTHUTIL = \TD^{t-1} \cdot \ConfValue$; when the paper is accepted at the sure bet option in round $t$, the author's utility is $\TD^{t-1}$.
The $\TD^{t-1}$ term encodes exponential time discounting and models that authors would like their work to be published in a timely manner.
Besides the utility loss due to time discounting, rejection does not cause additional cost for the author.

The author's decisions depend on all available information, i.e., her own (possibly perfect) private signal $\AuthSig$ as well as all the reviews she has received for previous submissions.
We assume that the author is rational and Bayesian, so her decisions are based on posterior quality distributions taking into account all available information. 
She will submit to the conference in round $t$ if and only if her expected utility from doing so (over all future time steps $t' \geq t$) exceeds her expected utility from the sure bet (which is exactly $\TD^{t-1}$ at the point she is making the decision).
Notice that unless the author obtains a perfect signal, the reviews she obtains (in addition to her own signal $\AuthSig$) change her posterior conditional probability over the paper's quality, which in turn changes her belief of the probability distribution of future reviews. 

\begin{definition}
  The model in which authors have perfect information about their papers' qualities, and papers may be resubmitted an unlimited number of times, is called the model of \emph{noiseless authors with unlimited resubmissions}.
\end{definition}

Under the model of noiseless authors with unlimited resubmissions, a theoretical analysis becomes more tractable. This is because authors' beliefs of their papers' qualities will not be updated based on the reviews. As a result, the papers that are submitted in the first round will be repeatedly resubmitted until acceptance. Consequently, the quality of the conference depends entirely on the authors' self-selection. 

\subsection{Review Burden and Tradeoffs}  \label{subsec:number-reviews}
As we discussed in the introduction, we are primarily interested in the tradeoff between the conference's quality and the review burden, which is captured by the number of requested reviews imposed on the community.\footnote{This downplays the utility of the authors, a fact which is discussed in \cref{sec:limitations}.} 
We call this the QB-tradeoff. 
We denote the expected number of reviews of a paper by $\PaperReviews$, which will be called the \emph{review burden}.
To be precise, the conference's policy, along with the author's best response, determines a probability distribution $\NumSubmitDist$,
where $\NumSubmitProb{t} = \Prob{\text{The paper is submitted at least $t$ times}}$.
Then, $\PaperReviews = \NumReviews \cdot \sum_{t = 1}^{\infty}  \NumSubmitProb{t}$.
In the limit as the number of rounds becomes large, the reviewing load is spread out evenly over rounds. Therefore, with $\NumNewPapers$ new submissions in each round, the review burden on the community is $\NumNewPapers \PaperReviews$.

We say that a policy $\ACCMAP$ with $\CONFUTIL$ and $\PaperReviews$ \emph{weakly dominates} another  policy $\hat{\ACCMAP}$ with $\CONFUTILH$ and $\PaperReviewsH$ if (1) it has a higher (or equal) expected utility, $\CONFUTIL \geq \CONFUTILH$, and (2) the number of reviews is smaller (or equal), $\PaperReviews \leq \PaperReviewsH$.  We say that a policy $\ACCMAP$ with $\CONFUTIL$ and $\PaperReviews$ \emph{dominates} another policy $\hat{\ACCMAP}$ if it weakly dominates, and at least one of the inequalities is strict. 
Given a set of policies, a policy is \emph{Pareto optimal} if it is not strictly dominated by any other policy in the set.
If policy $\ACCMAP$ and policy $\hat{\ACCMAP}$ do not induce unique conference qualities and review burdens (because they allow different best responses by the author), we say that $\ACCMAP$ (weakly) dominates $\hat{\ACCMAP}$ if every pair $(\CONFUTILH,\PaperReviewsH)$ induced by $\hat{\ACCMAP}$ is (weakly) dominated by a pair $(\CONFUTIL,\PaperReviews)$ induced by $\ACCMAP$.

We say that the QB-tradeoff in one setting \emph{weakly dominates} another if for any point (corresponding to a policy $\hat{\ACCMAP}$) of the second QB-tradeoff, there exists a point (corresponding to a policy $\ACCMAP$) in the first setting that weakly dominates it.

Typically, instead of optimizing over all memoryless acceptance policies, we will restrict our attention to memoryless threshold acceptance policies.  In such a case, we can look at the \emph{QB-tradeoff curve}, which maps out each point of the QB-tradeoff as the acceptance policy threshold $\tau$ increases from $-\infty$ to $+\infty$, and for $\tau$ where $\Prob[\RevSigV]{U(\RevSigV)  = \tau} > 0$, $r$ increases from 0 to 1.  

We say that one QB-tradeoff curve $\mathcal{C}$ \emph{(weakly) dominates} another QB-tradeoff curve $\hat{\mathcal{C}}$ if for any point on $\hat{\mathcal{C}}$ that does not correspond to accepting all papers or rejecting all papers, there exists a point on $\mathcal{C}$ which (weakly) dominates it.
Note that this implies that no point on the Pareto frontier of the QB-tradeoff curve $\mathcal{C}$ is dominated by any of the points on the QB-tradeoff curve $\hat{\mathcal{C}}$.  


\section{Noiseless Authors: Thresholds and Resubmission Gaps} \label{sec:thresholds-gaps}
We first focus on the case of noiseless authors. Recall that in this setting, because the author will not update her belief about the paper's quality, she will either submit to the conference until the paper is accepted (or, in the case of limited submissions, until the paper may not be resubmitted any more), or immediately submit to the sure bet option. In turn, this allows us to obtain analytical expressions for the conference's quality and optimal strategy.

In this section, we focus on the \emph{resubmission gap}: the difference between the threshold the conference sets for acceptance and the actual threshold of accepted papers, taking into account the resubmission of previously rejected papers. An analysis of the resubmission gap is of interest in its own right (since it may crucially inform conference acceptance policies), and also serves as the foundation of our further investigation of tradeoffs in \cref{sec:continuous-tradeoffs}.


\subsection{The Author's Best Response: De Facto Thresholds}
\label{sec:submission-threshold}

When the conference's acceptance policy $\ACCMAP$ is fixed, from the author's perspective, it induces an acceptance probability $\AccP{\ACCMAP}{q}$ for each paper quality $q$.
By Proposition~\ref{prop:monotone-prob}, this probability is weakly increasing in $q$ if the acceptance policy is monotone, and strictly increasing under a non-trivial threshold policy. 
We will show that as a result, if the acceptance policy is monotone, 
the author has a best response\footnote{There may be other best responses which do not fit this pattern.} which can be characterized by a submission threshold $\theta$.
To state this result, we first define suitable nomenclature and notation.

\begin{definition}
In a \emph{$\theta$-threshold strategy} for the author, an author submits (and resubmits until they are accepted)  all papers of quality $\Qual = q > \theta$, and no paper of quality $q < \theta$. The author may handle papers of quality $q = \theta$ arbitrarily.  
An author strategy is called a \emph{threshold strategy} if it is a $\theta$-threshold strategy for some $\theta$.
\end{definition}

When $q = \theta$, the author is typically indifferent between submitting and not submitting. In this case, we assume that we can prescribe the authors tie breaking behavior for ease of analysis. 

We now formally define a central concept of our paper:

\begin{definition}[De Facto Threshold]
Consider a conference with memoryless acceptance policy \ACCMAP and noiseless authors.
A value $\theta$ such that under every best response the author submits a paper of quality $\Qual = q$ if $q > \theta$ and does not submit any paper with $q < \theta$ is called a \emph{de facto threshold}.\footnote{All such papers will be submitted until accepted, so de facto, the conference will accept all such papers.}
\end{definition}

Based on the definition, the de facto threshold exists for an acceptance policy if and only if every best response of the author is a threshold strategy. In particular, if there are qualities $q \neq q'$ such that the author is indifferent between submitting or not submitting papers of qualities $q$ and $q'$, then the de facto threshold does not exist.

The following proposition characterizes the best responses of an author to monotone and threshold acceptance policies.
For its statement and proof, and most others later, we capture the appeal of submitting to a conference by an \emph{attractiveness factor} $\rho$ (where larger values correspond to conferences that are more attractive to submit to), based on its value \ConfValue and the discount factor \TD:
\begin{align}
\rho & := \frac{\ConfValue - \TD}{1-\TD}.
\label{eqn:rho-definition}
\end{align}


\begin{proposition} \label{prop:de_facto}
Consider a memoryless conference with a non-trivial monotone acceptance policy \ACCMAP. Let the authors be noiseless, with value for acceptance $\ConfValue>1$ and discount factor $\TD \in (0,1)$. 

\begin{enumerate}
    \item An author with a paper of quality $\Qual=q$ is indifferent between submitting or not submitting, if and only if $\AccP{\ACCMAP}{q} = 1/\rho$.
   \item There exists\footnote{As will be evident in the proof, for general monotone policies, there may be other types of best responses, or multiple different thresholds.} a threshold strategy best response for the author.
  \item If the conference acceptance polity \ACCMAP is additionally a threshold policy, then there exists a de facto threshold. This implies that every best response for the author must be a threshold strategy.
  \item If the conference applies a threshold acceptance policy and the model is continuous, then the de facto threshold $\theta$ is unique in the following sense: no best response is a $\theta'$-threshold strategy for $\theta' \neq \theta$.  
  Moreover, $\AccP{\ACCMAP[\tau]}{\theta} = 1/\rho$. 
\end{enumerate}

\end{proposition}

Note that our results (here and later) depend on \TD and \ConfValue only through $\rho$. Therefore, in a sense, they are ``interchangeable,'' albeit not linearly. That is, an increase in author patience (\TD) is tantamount to a (different) increase in conference prestige, as far as author behavior is concerned.

\begin{proof}
Let \ACCMAP be the acceptance policy. 
The probability of acceptance for a paper of quality $\Qual=q$ is $\AccP{\ACCMAP}{q}$. By Proposition~\ref{prop:monotone-prob}, this probability is non-decreasing for monotone acceptance policies, and strictly increasing for non-trivial threshold policies.
The author submits the paper if the expected utility of submitting is greater than 1 (the value of the ``sure bet'' option), does not submit the paper if the expected utility is less than 1, and is indifferent between submitting or not if the expected utility is equal to 1. Since the author will face the same tradeoff in future rounds\footnote{Crucially, a noiseless author does not learn any new information from rejection in previous rounds.}, she will make the same decision, so she will submit until acceptance. The expected utility can be obtained as the time-discounted sum of the utility from acceptance:

\begin{align}
\AUTHUTIL(\ACCMAP,q)
& = 
\sum_{t\ge 1} \ConfValue \cdot \TD^{t-1} \AccP{\ACCMAP}{q} \cdot (1-\AccP{\ACCMAP}{q})^{t-1} 
\; = \; \frac{\ConfValue \cdot \AccP{\ACCMAP}{q}}{1-\TD \cdot (1-\AccP{\ACCMAP}{q})}.
\label{eqn:submission-inequality}
\end{align}

Solving the inequalities $\AUTHUTIL(\ACCMAP,q) > 1$, $\AUTHUTIL(\ACCMAP,q) < 1$, and $\AUTHUTIL(\ACCMAP,q) = 1$ for $q$, the author submits the paper if $\AccP{\ACCMAP}{q} > 1/\rho$, does not submit if $\AccP{\ACCMAP}{q} < 1/\rho$, and is indifferent between submitting or not if $\AccP{\ACCMAP}{q} = 1/\rho$, respectively. 
This completes the proof of the first part of the proposition.

We next prove the second part.
Let $\ubar{\theta} = \sup \Set{q}{\AccP{\ACCMAP}{q} < 1/\rho}$ and $\bar{\theta} = \inf \Set{q}{\AccP{\ACCMAP}{q} > 1/\rho}$. (If the policy accepts/rejects all papers, it is possible that $\ubar{\theta} = -\infty$ or $\bar{\theta} = \infty$.) 
By Proposition~\ref{prop:monotone-prob}, $\ubar{\theta} \leq \bar{\theta}$. 
Furthermore, the author will submit the paper (and resubmit until accepted) if $q > \bar{\theta}$, and not submit if $q < \ubar{\theta}$.
If $\ubar{\theta} = \bar{\theta}$, then $\theta = \ubar{\theta} = \bar{\theta}$ satisfies the claim.
Otherwise, $\AccP{\ACCMAP}{q} = 1/\rho$ for all $q \in  (\ubar{\theta}, \bar{\theta})$, so the   author is indifferent between submitting and not submitting for all such $q$. Thus, any $\theta \in (\ubar{\theta}, \bar{\theta})$ satisfies the claim.  
This completes the proof of the second part.
Note that the threshold $\theta$ may not be unique. Indeed, the author may make arbitrary decisions for $q \in  (\ubar{\theta}, \bar{\theta})$, showing that non-threshold strategies may be best responses for the author as well. 

We next prove the third part, so we assume that the conference's policy is a non-trivial threshold acceptance policy.
By \cref{prop:monotone-prob}, $\AccP{\ACCMAP}{q}$ is strictly monotone in $q$. 
Therefore, in the categorical model, there exists at most one $\hat{q} \in \QualSet$ such that $\AccP{\ACCMAP}{\hat{q}}=1/\rho$.
If such a $\hat{q}$ exists, it is a de facto threshold. If not, then because the threshold acceptance policy is non-trivial, there exist qualities $\ubar{q} < \bar{q}$ such that $\AccP{\ACCMAP}{\ubar{q}} < 1/\rho$, $\AccP{\ACCMAP}{\bar{q}} > 1/\rho$, and there are no qualities in $\QualSet$ between $\ubar{q}$ and $\bar{q}$.  
Then, any $\theta \in [\ubar{q}, \bar{q}]$ is a de facto threshold. To complete the third part of the proof, notice that if $\theta$ is a de facto threshold, then every best response by the author is a $\theta$-threshold strategy.


Finally, we prove the fourth part. 
By \cref{prop:monotone-prob}, $\AccP{\ACCMAP}{q}$ is strictly increasing in $q$. Moreover, in the continuous model, the distribution of the review noise is assumed to be a bijection, which implies continuity of $\AccP{\ACCMAP}{q}$ as a function of $q$. Therefore, the first case $\ubar{\theta} = \bar{\theta}$ must occur in our proof of the second part of the proposition, and $\AccP{\ACCMAP}{\bar{\theta}} = 1/\rho$, proving the uniqueness of the author's best response.
\end{proof}

\cref{prop:de_facto} assumes that the agents are allowed to resubmit their papers arbitrarily many times. This assumption is not essential: an essentially identical calculation (in \cref{sec:time_limited_policy}) shows that the de facto threshold does not change when the conference restricts the number of times a paper can be resubmitted.

\subsection{Threshold Acceptance Policies and the Resubmission Gap}
\label{sec:threshold-resubmission}  

Threshold acceptance policies (see \cref{def:threshold-policy}) comprise a very natural class of policies for a conference to apply. Recall that they accept all papers whose posterior (based on the reviews) expected quality strictly clears some threshold, and reject all papers whose posterior expected quality falls short of the threshold. 
We show that every possible de facto threshold can be induced by a threshold acceptance policy.

\begin{proposition} 
\label{prop:threshold-policy}
Let $\theta \in \mathbb{R}$ be any de facto threshold. Then, there exists a threshold acceptance policy with threshold $\hat{\tau}$ and probability $\hat{r}$ such that a rational noiseless author will submit to the prestigious conference which uses $\ACCMAP[\hat{\tau},\hat{r}]$ if $\Qual > \theta$ and submit to the sure bet if $\Qual < \theta$ (and the author is indifferent between submitting or not submitting if $\Qual=\theta\in \QualSet$).
Moreover, if the model is continuous and authors are neither submitting all papers nor submitting no papers, the threshold $\hat{\tau}$ is unique.\footnote{Recall that in the continuous model, the parameter $r$ can be omitted.}
\end{proposition}

In order to prove the proposition, we want to relate the acceptance threshold to how ``strict'' the policy is. We begin by defining a comparison between the strictness of two policies:

\begin{definition} \label{def:stricter}
An acceptance policy \ACCMAP['] is \emph{(weakly) stricter} than another policy \ACCMAP if it accepts every paper with a (weakly) smaller probability, i.e., $\AccP{\ACCMAP[']}{q} < \AccP{\ACCMAP}{q}$ for all $q$ (resp., $\AccP{\ACCMAP[']}{q} \leq \AccP{\ACCMAP}{q}$ for all $q$ for the weak version).
\end{definition}

Being stricter appears to be a very demanding requirement, in that it requires an inequality for all paper qualities. 
We next show that for threshold policies, it in fact follows from a strictly smaller acceptance probability for just one paper. 


\begin{lemma}\label{lem:stricter_policy}
Let $\ACCMAP$ and $\ACCMAP[']$ be two threshold acceptance policies. If there exists a $q\in \QualSet$ such that $\AccP{\ACCMAP[']}{q} < \AccP{\ACCMAP}{q}$, then $\ACCMAP[']$ is stricter than $\ACCMAP$.
\end{lemma}

\begin{proof}
  We want to show that if a threshold policy accepts one type of paper with strictly smaller probability, it accepts \emph{every} paper with strictly smaller probability. 

  First, because the review signals are informative, for every paper quality, the conditional signal distribution has full support on the signal space. Because multiple reviews are i.i.d., conditioned on any paper quality $q$, the review signal distribution has full support over all vectors of review signals.

  Because \ACCMAP, \ACCMAP['] are \emph{threshold} policies, any review vector that leads to acceptance under \ACCMAP['] must lead to acceptance under \ACCMAP with at least the same probability. And because a paper of quality $q$ is accepted with strictly higher probability by \ACCMAP, there must exist at least one review vector \RevSigV which is accepted with strictly higher probability under \ACCMAP than under \ACCMAP['].
  
  Because \RevSigV occurs with positive probability for every paper quality $q'$, every paper is accepted with strictly higher probability by \ACCMAP than by \ACCMAP['], completing the proof.
\end{proof}

The following lemma relates the strictness of a threshold acceptance policy with its acceptance threshold.


\begin{lemma} \label{prop:monotone-prob-threshold}
In the categorical model, let $\ACCMAP[\tau,r]$ and $\ACCMAP[\tau',r']$ be two threshold acceptance policies with $r, r'\in (0,1]$.
Then, if either $\tau'>\tau$ or $\tau'=\tau$ and $r'< r$, $\ACCMAP[\tau',r']$ is weakly stricter than $\ACCMAP[\tau,r]$. 

 In the continuous model, let $\tau$ and $\tau'$ be the thresholds for two non-trivial threshold policies. Then, $\ACCMAP[\tau']$ is stricter than $\ACCMAP[\tau]$ if and only if $\tau' > \tau$.
\end{lemma}

\begin{proof}
We begin by proving the first part of the lemma, regarding the categorical model. By the definition of $\ACCMAP[\tau,r]$ and $\ACCMAP[\tau',r']$, whenever a paper with some review vector $\RevSigV$ is accepted by $\ACCMAP[\tau',r']$ with positive probability, it will be accepted by $\ACCMAP[\tau,r]$ with at least the same probability. This means that $\ACCMAP[\tau,r]$ accepts every paper with at least the same probability as  $\ACCMAP[\tau',r']$.

Next, we prove the second statement, regarding the continuous model. For the ``if'' direction, there always exists a review vector \RevSigV with $U(\RevSigV) \in (\tau, \tau')$. Thus, for every paper, there is a positive probability that it is rejected by $\ACCMAP[\tau']$ (namely, when the review vector is \RevSigV) but accepted by $\ACCMAP[\tau]$. For the ``only if'' direction, we know that a stricter threshold policy accepts every paper with strictly smaller probability. In the continuous model, $\ACCMAP[\tau']$ must reject some review vectors that are accepted under $\ACCMAP[\tau]$. This implies $\tau'>\tau$.
\end{proof}

\begin{extraproof}{\cref{prop:threshold-policy}}   
Let $\hat{\theta}$ be a value in $\QualSet$ closest to $\theta$, i.e.~$\hat{\theta} \in \arg\min_{q\in \QualSet} (q-\theta)^2$ (breaking ties arbitrarily).
Let $f(\tau) := \AccP{\ACCMAP[\tau,0]}{\hat{\theta}}$ be the probability that a paper of quality $\hat{\theta}$ is accepted under the policy $\ACCMAP[\tau, 0]$, which accepts the paper if and only if its expected posterior quality is greater than $\tau$.
  
Then, $\lim_{\tau \to -\infty} f(\tau) = 1 > 1/\rho > 0 = \lim_{\tau \to \infty} f(\tau)$. Furthermore, by Lemma \ref{prop:monotone-prob-threshold}, $f(\tau)$ is a non-increasing function of $\tau$. Therefore, there must exist a $\hat{\tau}$ such that $\lim_{\tau \to \hat{\tau} \uparrow} f(\tau) \geq 1/\rho \geq \lim_{\tau \to \hat{\tau} \downarrow} f(\tau)$. Fix this $\hat{\tau}$.
  
If $f$ is continuous at $\hat{\tau}$, then the threshold policy $\ACCMAP[\hat{\tau},0]$ has $\AccP{\ACCMAP[\hat{\tau},0]}{\hat{\theta}} = 1/\rho$ by definition.
Otherwise, let $z = \lim_{\tau \to \hat{\tau} \uparrow} f(\tau) - \lim_{\tau \to \hat{\tau} \downarrow} f(\tau) > 0$. We can infer that there must be a discrete probability of $z$ for the event that $\ExpectC{\Qual}{\RevSigV} = \hat{\tau}$, i.e., that $\Prob[{\RevSigV \sim \RevSigDist[{\hat{\theta}}]}]{\ExpectC{\Qual}{\RevSigV} = \hat{\tau}} = z$.
We then consider the threshold policy $\ACCMAP[\hat{\tau},\hat{r}]$ with threshold $\hat{\tau}$ which conditioned on $\ExpectC{\Qual}{\RevSigV} = \tau$ accepts a paper with probability $\hat{r} := \frac{1/\rho-\lim_{\tau \to \hat{\tau} \downarrow} f(\tau)}{z}$. The overall acceptance probability of $\ACCMAP[\hat{\tau},\hat{r}]$ for a paper with quality $\hat{\theta}$ is therefore

\begin{align*} 
 & \Prob[{\RevSigV \sim \RevSigDist[{\hat{\theta}}]}]{\ExpectC{\Qual}{\RevSigV} > \hat{\tau}} + \Prob[{\RevSigV \sim \RevSigDist[{\hat{\theta}}]}]{\ExpectC{\Qual}{\RevSigV} = \hat{\tau}} \cdot \frac{1/\rho-\lim_{\tau \to \hat{\tau} \downarrow} f(\tau)}{z}
\\ & = \lim_{\tau \to \hat{\tau} \downarrow} f(\tau)
+ z \cdot \frac{1/\rho-\lim_{\tau \to \hat{\tau} \downarrow} f(\tau)}{z}
\; = \; 1/\rho.
\end{align*}

Thus, under the threshold acceptance policy \ACCMAP[\hat{\tau},\hat{r}], papers with quality $\hat{\theta}$ are indifferent between submitting and not submitting. 
Note that if $\theta\in\QualSet$, then $\hat{\theta}=\theta$. The result for the special case in the proposition statement straightforwardly follows.
For the general case, assume w.l.o.g.~(the other case is symmetric) that $\hat{\theta} \geq \theta$.
Because the author is indifferent between submitting and not submitting papers of quality $\Qual = \hat{\theta}$, we may assume that such papers are submitted, and hence all papers of quality at least $\hat{\theta}$. 
By definition of $\hat{\theta}$, no papers have quality $\Qual \in [\theta, \hat{\theta})$, so all papers of quality at least $\theta$ are submitted. Thus, $\theta$ is also a de facto threshold for the author.

Finally, uniqueness of the conference's threshold policy in the continuous model when the author's submission decision is non-trivial follows directly because by Lemma~\ref{prop:monotone-prob-threshold}, the solution for $\hat{\tau}$ of $\AccP{\ACCMAP[\hat{\tau},0]}{\hat{\theta}} = 1/\rho$ is unique in the continuous model when agents are neither submitting all papers nor submitting no papers.
\end{extraproof}

Proposition~\ref{prop:threshold-policy} in part justifies our focus on threshold policies.  For any monotone policy, Proposition~\ref{prop:de_facto} implies the existence of a threshold best response for the author, and thus, Proposition~\ref{prop:threshold-policy} implies the existence of a threshold policy for which the author best-responds in the same way. Thus, assuming that authors break ties
in favor of using threshold strategies, any conference quality that can be achieved with a monotone policy can be achieved with a threshold acceptance policy.  Interestingly, it does not follow that the QB-tradeoff for threshold policies weakly dominates that of all monotone acceptance policies, as we will show in \cref{sec:testing_tradeoff}.

While Proposition~\ref{prop:threshold-policy} implies the existence of an acceptance threshold $\tau$ inducing the desired submission threshold $\theta$, these two thresholds will typically be different. The threshold $\tau$ can be interpreted as the ``declared'' quality goal of the conference, attained in isolation. $\theta$ is the actual quality of the conference, taking into consideration resubmissions and reviewing noise. The difference is a key concept we study, and term the ``resubmission gap''.

\begin{definition}[Resubmission Gap] 
Consider a memoryless conference with a non-trivial threshold acceptance policy $\ACCMAP[\tau,r]$, and a noiseless author.
Let $\theta$ be the smallest de facto threshold in response to $\ACCMAP[\tau,r]$.

The difference between the acceptance threshold $\tau$ and $\theta$ is called the \emph{resubmission gap}.\footnote{The name reflects that there is a gap between the ``stated'' and ``de facto'' quality of accepted papers, caused by the fact that authors are free to resubmit papers.}
\end{definition}

In general, the de facto threshold is not unique in the categorical model; we define the resubmission gap based on the \emph{smallest} de facto threshold. This issue disappears in the continuous model, where the resubmission gap is unique for every non-trivial threshold acceptance policy. 
The fact that the actual threshold of papers at the conference differs from the declared acceptance threshold for each year in isolation (namely, by the resubmission gap) may appear somewhat unexpected at first, and is frequently missing from conversations about policy changes. 
The resubmission gap leads to the following oddity, which we call the \emph{resubmission paradox}:  all submitted papers will eventually be accepted, but in any one round, many (and often most) are rejected.  While accepting all papers initially seems like a solution, in our model, it is not.  The only way to initially accept all papers is to lower the acceptance threshold.  However, this will lower the de facto threshold (unless the de facto threshold already accepts all papers), which in turn means that additional papers will be submitted.  Moreover, these additional papers will often be rejected multiple times, but will be resubmitted until they are accepted.  Thus, this apparent solution actually reproduces the problem, just with a lower de factor threshold.

Next, we observe that using the de facto threshold, one can cleanly characterize optimal policies.

\begin{proposition}\label{prop:max-general}
The conference's optimal policy induces a de facto threshold of $\theta = 0$. 
 \end{proposition}

\begin{proof}
Given a de facto threshold $\theta$, the conference's quality is $\int_{\theta}^\infty \qual \,d\QualDens{\qual}$.\footnote{For discrete quality distributions, the corresponding conference quality is obtained by replacing the integral by the sum and the density function by the probability function.}
This expression is maximized when $\theta = 0$, proving the optimality of this de facto threshold.
(Notice that while $\QualDist$ may have point mass at $\theta$ --- giving different values depending on whether the author submits papers of quality exactly $\theta$ --- the conference quality of all such policies is dominated by that for $\theta = 0$, where a point mass does not affect the value of the integral.)
\end{proof}

Combining Proposition~\ref{prop:max-general} with Proposition~\ref{prop:de_facto} allows us some insights into the right acceptance threshold for a conference aiming for maximum quality: under the chosen threshold acceptance policy, $\AccP{\ACCMAP[\hat{\tau},r]}{0} = 1/\rho$. This means that the more attractive the conference is (i.e., the larger $\rho$), the more likely borderline papers must be rejected. Because borderline papers are resubmitted until accepted, this means that for the same optimal quality, an attractive conference (or patient authors) leads to higher reviewing load. We will investigate this phenomenon in significantly more depth in \cref{sec:QB-trade-noise}.

\subsection{Continuous Model with Additive Noise}
\label{sec:additive-noise}

To derive some further insights, we now focus on the continuous model with additive noise and $\NumReviews = 1$ review only. Importantly, recall that the noise distribution is independent of the true underlying quality, and has zero mean.
That is, for any quality $\Qual=q$, the reviewer observes a signal of $q+X$ where the distribution of the review noise, \RevNoiseDist{X}, is independent of $q$. 
Under this model, the resubmission gap of a threshold acceptance policy does not depend on its threshold $\tau$.

\begin{proposition} \label{prop:gap-invariant}
  Given an acceptance threshold $\tau$, in the continuous model with a single review and additive noise drawn from \REVNOISEDIST, the de facto threshold is the unique solution to the equation $1/\rho = 1-\RevNoiseDist{\tau-\theta}$.
  In particular, the resubmission gap $\tau - \theta$ is independent of $\tau$ and the prior distribution $\QualDist$ over paper qualities.
\end{proposition}

\begin{proof}
  Consider an acceptance threshold $\tau$, and corresponding policy $\ACCMAP[\tau]$. Recall that under a continuous model, the probability that the conditional expected quality of a paper is exactly $\tau$ is 0, so $\tau$ uniquely defines the threshold acceptance policy.
  By Proposition~\ref{prop:de_facto}, the de facto threshold $\theta$ satisfies $\AccP{\ACCMAP[\tau]}{\theta} = 1/\rho$.

In the continuous model with a single review, the conference will accept a paper with quality $q$ if and only if the review $s$ satisfies $s=q+x>\tau$. This happens with probability $\AccP{\ACCMAP[\tau]}{q} = 1-\RevNoiseDist{\tau-q}$.
Thus, $\theta$ solves $1/\rho = 1-\RevNoiseDist{\tau-\theta}$. 

A solution for $\theta$ exists because $1/\rho = \frac{1-\TD}{\ConfValue-\TD} \in (0, 1)$ (because $\TD \in (0, 1)$ and $\ConfValue > 1$) and $\RevNoiseDist{\cdot}$ is a distribution. 
The uniqueness of $\theta$ follows because we assumed $\RevNoiseDist{\cdot}$ to be a bijection.
\end{proof}

Substituting $\theta = 0$ from Proposition~\ref{prop:max-general} into Proposition~\ref{prop:gap-invariant}, we immediately obtain the following corollary.

\begin{proposition} \label{prop:max}
  For the continuous model with a single review and additive noise drawn from \REVNOISEDIST (independently of the true paper quality), the threshold that maximizes the conference quality is
  $\tau^* = \left(\REVNOISEDIST\right)^{-1}\left(\frac{\ConfValue-1}{\ConfValue-\TD}\right)$.

  Then, the author submits (and resubmits until accepted) the paper if and only if\footnote{In this case, the event $\Qual = 0$ has probability 0, so there is a unique optimal strategy for the author.} the quality is non-negative: $\Qual \geq 0$.
  The resulting (maximum) quality for the conference is 
  $\CONFUTIL = \int_{0}^\infty \qual \,d\QualDens{\qual}$.
\end{proposition}

First, note that \cref{prop:max} also implies that the quality-maximizing acceptance threshold does not depend on the distribution $\QualDist$ of the papers' qualities.   
Second, notice that a large conference value $\ConfValue$ (very high prestige) or a large discount factor $\TD$ (very patient authors) encourages authors to consistently resubmit bad papers. This leads to a large resubmission gap: the conference has to set a significantly higher bar to sufficiently discourage such resubmissions, and will reject many good papers repeatedly before they are finally accepted. 
In contrast, when the conference is not attractive enough or authors are not patient enough, the conference has to lower the acceptance threshold even below the de facto threshold, to provide strong enough assurance to good papers that they will be immediately accepted. 
However, this can only occur for $\ConfValue < 2$; otherwise, the resubmission gap is non-negative for authors with any level of patience.



\section{Noiseless Authors: Tradeoffs and Acceptance Rate} \label{sec:continuous-tradeoffs}
In this section, we build on the fundamental concepts of threshold policies, de facto thresholds, and resubmission gap, to undertake a more in-depth investigation of the tradeoffs a conference may face. In particular, we consider the tradeoff between conference quality and review burden on the community, and between acceptance thresholds and acceptance rate. 

We primarily study the tradeoff between the conference's quality and the review burden based on threshold acceptance policies. In \cref{subsec:QB-tradeoff}, we first use numerical examples to visualize the QB-tradeoffs and the Pareto frontiers in the continuous model. 
Then, the rest of this section investigates when threshold policies are optimal, and identifies factors that affect QB-tradeoffs. Finally, we examine the relationship between the acceptance threshold and the acceptance rate.

\subsection{Continuous Models Studied}\label{sec:conti_model}
In this section (and the following sections that contain discussions on the continuous model), we frequently use the following special cases of our general continuous model as examples for our analysis and plots.

The \emph{$(\sigma, \QualDist, \NumReviews, \ConfValue, \TD)$-Gaussian model} is a continuous model with noiseless authors; the noise for each review is drawn from a Gaussian distribution $\REVNOISEDIST = \Gaussian{0}{\sigma}$. The parameters \QualDist, \NumReviews, \ConfValue, and \TD are, as before, the prior, the number of solicited reviews, the value of the prestigious conference, and the discount factor, respectively. 
The \emph{$(\sigma, \mu_{\QualDist}, \sigma_{\QualDist}, \NumReviews, 
\ConfValue, \TD)$-Double Gaussian model} is a Gaussian model with the prior $\QualDist = \Gaussian{\mu_{\QualDist}}{\sigma_{\QualDist}}$.

\subsection{Tradeoff between Conference Quality and Review Burden: QB-tradeoff} 
\label{subsec:QB-tradeoff}

We first consider the tradeoff between the conference quality and the review burden. We focus on the average number of reviews per \emph{paper} (including papers that were never submitted, and thus incurred $0$ reviews) as the relevant measure of the review burden.

Intuitively, if there are many borderline papers, whose quality is near the acceptance policy threshold $\tau$, they may go through many rounds of resubmission and increase the review burden.  Raising or lowering the threshold slightly might lead to a different borderline regime with a smaller fraction of papers on the border.  We already saw that the conference quality is maximized by a de facto threshold of 0.  Thus, changing the threshold will come at a cost to the conference quality, either by losing out on some good papers, or by accepting some bad papers.  In the extremes, rejecting everything will lead to a review burden of 0, and accepting everything will lead to a review burden of 1.  The \emph{QB-Tradeoff} traces how the review burden and conference quality jointly vary across all possible  acceptance policy thresholds.   

We would like to understand the Pareto frontier of the QB-tradeoff over threshold acceptance policies. That is, fixing all the parameters except the acceptance threshold, we want to understand which thresholds are Pareto optimal. 

Notice that deviations from the optimal de facto threshold of 0 in \emph{either} direction could be Pareto optimal. First, the conference can decrease the threshold to accept some negative-quality papers, in order to accept the positive-quality papers in fewer rounds; alternatively, the conference can increase the threshold to give up on some borderline papers with positive quality which might otherwise take a large number of rounds until acceptance. Clearly, which intervals of strategies are Pareto optimal depends on the distribution of paper quality. For example, if there is a substantially larger number of borderline papers with negative quality than positive quality, marginally lowering the threshold will both degrade conference quality and increase review burden, but marginally increasing the threshold will decrease the review burden though still degrade conference quality.  The latter will be Pareto optimal while the former will not.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Pareto_optimal_vs_noise_continuous.png}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Pareto_optimal_vs_noise_continuous_case2.png}
         \caption{}
     \end{subfigure}
     \hfill
     \caption{(a) shows the QB-tradeoff of a \emph{$(\sigma, \mu_{\QualDist} = 0, \sigma_{\QualDist} = 1, \NumReviews = 1, \ConfValue = 5, \TD = .7)$-Double Gaussian model}.  (b) shows the QB-tradeoff of a \emph{$(\sigma, \mu_{\QualDist} = .8, \sigma_{\QualDist} = 1, \NumReviews = 1, \ConfValue = 5, \TD = .7)$-Double Gaussian model}.  In each case, the review quality $\sigma$ is varied over five discrete options.  For each $\sigma$, the curve shows the possible QB-tradeoffs as the acceptance threshold is varied continuously. The Pareto frontier is shown with solid lines, while dominated points are shown with dashed lines.     \label{fig:Pareto_frontier_continuous}}
\end{figure}



\cref{fig:Pareto_frontier_continuous} maps the QB-tradeoff for various settings. 
In each setting, there is a point at (0, 0) that corresponds to rejecting all submissions. As the threshold is decreased, high-quality papers start being submitted, increasing both the conference quality and review burden.  When the de facto threshold is 0, conference quality is maximized. Subsequently, a further decrease in the threshold leads to more low-quality papers being accepted, lowering the conference quality.  We see here that the effect on the review burden is mixed: sometimes it increases while other times it decreases.  
As the threshold decreases further, in each case, we see a point with review burden 1 where all the papers are accepted.  At that point, the conference quality is 0 in panel (a), but .8 in panel (b) due to the different priors on paper quality.     

In Panel (a), the Pareto optimal de facto thresholds are always greater than 0.  This is because, for any $\theta > 0$, the de facto thresholds of $\theta$ and $-\theta$ have the same conference quality, but $\theta$ will have a smaller review burden because the number of submitted papers overall is smaller, and the number of submitted papers just above the boundary will also be smaller.    

Panel (b) contains some interesting observations. First, for $\sigma=0.2, 2$ and $5$, marginally increasing the de facto threshold from 0 remains Pareto optimal, while marginally decreasing it is Pareto dominated.  However, the opposite is true for $\sigma=0.5$ and 1. This is because, in all cases, when the threshold decreases, the number of submitted papers increases and the number of papers just above the borderline (that require more rounds of submission in expectation) decreases.  However, the rate at which the latter decreases depends non-monotonically on the review quality.  Second, notice that in the four largest settings of $\sigma$, some Pareto optimal thresholds lie both above and below the de facto threshold of 0.  

Thus, for all curves in panel (a) and $\sigma=0.2$ in panel (b), we have that the Pareto optimal QB-tradeoffs either optimize the conference quality, or increase the threshold to trade off conference quality for a reduced review burden.  

However, for $\sigma=0.5$ and 1 in panel (b), the Pareto optimal QB-tradeoffs correspond to a set of de-facto thresholds that is not convex. In particular, there exists a $\theta_0 > 0$, such that the Pareto optimal QB-tradeoffs either (1) only accept papers with quality at least $\theta_0$ or (2) keep out the bad papers (with quality less than some threshold $\theta \leq 0$). Here, having a de facto threshold of $0 < \theta < \theta_0$ is Pareto dominated by some threshold of $\theta' < 0$ which yields the same conference quality, but at a lower review burden.


Finally, for $\sigma=2$ and 5 in panel (b), there are three ranges of Pareto optimal QB-tradeoffs! That is, there exists $\theta_2 < 0 < \theta_1 < \theta_0$ such that the Pareto optimal policies either (1) only accept papers above some threshold $\theta \geq \theta_0$;  (2) reject the really bad papers by setting the threshold to $\theta \leq \theta_2$; or (3) maximize conference quality despite a relatively high review burden with  $0 \leq \theta \leq \theta_1$.


\subsection{When Are Threshold Acceptance Policies Optimal?} \label{sec:testing_tradeoff}

Our analysis of QB-tradeoffs is largely based on threshold acceptance policies. To contextualize this focus, we first investigate when using such policies is optimal in terms of achieving Pareto frontiers of the QB-tradeoff.
While analyzing monotone acceptance policies, we showed that they always induce a threshold strategy for authors as one of the best responses (see \cref{prop:threshold-policy}). Furthermore, under threshold acceptance policies, non-threshold best responses do not exist; and even for monotone policies, they exist only in knife-edge cases, where authors are indifferent between submitting or not submitting for multiple paper qualities. 
We therefore restrict our attention to threshold best responses for the authors in the following discussions. 

In \cref{prop:threshold-policy}, we showed that every de facto threshold can be implemented with a threshold acceptance policy. In particular, this applies to the optimal de facto threshold of $\theta=0$ --- see \cref{prop:max-general}. Therefore, every conference quality that is achievable by a monotone policy can be achieved by a threshold policy. 
It remains to understand when a threshold acceptance policy minimizes the corresponding review burden (among all monotone policies) for every conference quality. Our results suggest that when only $\NumReviews = 1$ review is obtained, there always is an optimal threshold acceptance policy. 
However, even for $\NumReviews = 2$, there are in general instances for which monotone non-threshold policies strictly outperform threshold policies in terms of the review burden, while achieving the same (optimal) conference quality.


\begin{proposition}\label{prop.threshold_opt}
When $\NumReviews = 1$ and authors always respond with threshold strategies if possible, the QB-tradeoff for threshold policies weakly dominates that of all monotone acceptance policies.
\end{proposition}

We defer the proof to the appendix.
The optimality of threshold policies ceases to hold when $\NumReviews > 1$, as we illustrate next with a counter-example.

\begin{table}[ht]
    \centering
     \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
 $\RevSigV$ & $(L,L)$ &     
 \begin{tabular}{@{}c@{}}$(L,M)$ or\\ $(M,L)$ \end{tabular}
 & \begin{tabular}{@{}c@{}}$(L,H)$ or\\ $(H,L)$ \end{tabular} 
 & $(M,M)$ 
 &  \begin{tabular}{@{}c@{}}$(M,H)$ or\\ $(H,M)$ \end{tabular} 
 & $(H,H)$ \\
         \hline
        $\ProbC{\RevSigV}{q = -2}$ & $\frac{4}{9}$ & $\frac{2}{9}$ & $\frac{2}{9}$ & $\frac{1}{36}$ & $\frac{1}{18}$ & $\frac{1}{36}$\\
        $\ProbC{\RevSigV}{q = 1}$ & $\frac{1}{9}$ & $\frac{1}{9}$ & $\frac{1}{3}$ & $\frac{1}{36}$ & $\frac{1}{6}$ & $\frac{1}{4}$\\
        $\ProbC{\RevSigV}{q = 5}$ & $\frac{1}{36}$ & $\frac{1}{18}$ & $\frac{2}{9}$ & $\frac{1}{36}$ & $\frac{2}{9}$ & $\frac{4}{9}$\\
        \hline
    \end{tabular}
    \caption{Likelihood function in \cref{ex:threshold-counterexample}. Columns are ordered based on expected conference quality (from low to high) conditioned on the review vector.
    \label{tab:threshold-counterexample}}
\end{table}

\begin{example} \label{ex:threshold-counterexample}
The set of paper qualities is $\{-2,1,5\}$, with uniform prior $\QualDist = (1/3,1/3,1/3)$.  The review signal set is $\SigSet = \{L,M,H\}$, and the number of reviews is $\NumReviews = 2$.  The conditional distributions of review signals are $\RevSigDist[-2] = (2/3,1/6,1/6), \RevSigDist[1] = (1/3,1/6,1/2)$, and $\RevSigDist[5] = (1/6,1/6,2/3)$; it can be verified that this information structure is informative. 
The conference's attractiveness factor is $\rho = 24/5$.
\Cref{tab:threshold-counterexample} shows the likelihood of each vector of review signals, with $\NumReviews = 2$.

Consider the following policy $\ACCMAP[']$. 
(1) When the review vectors are $(H,H), (M,H)$ or $(H,M)$,
accept the paper with probability 1;
(2) when the review vectors are $(M,M), (L,H)$ or $(H,L)$,
accept the paper with probability $1/2$; and
(3) otherwise, reject the paper with probability 1.
Notice that this policy is monotone (because the acceptance probability is non-decreasing in the conditional expected quality), but it is not a threshold policy. 
This is because the conditional expected paper qualities of the signal vectors $(M,M)$ and $(L,H)$ (or $(H,L)$) are
$U(M,M) = \frac{4}{3} > \frac{9}{7} = U(H,L) = U(L,H)$,
yet $(H,L)$ and $(L,H)$ lead to acceptance with positive probability, while $(M,M)$ does not lead to acceptance with probability 1.
Next, we compute the acceptance probabilities of papers with different qualities:

\begin{align*}
\AccP{\ACCMAP[']}{5} & = 19/24 > 1/\rho
& \AccP{\ACCMAP[']}{1} & = 43/72 > 1/\rho
& \AccP{\ACCMAP[']}{-2} & = 5/24 = 1/\rho.
\end{align*}

Thus, all papers of positive quality are submitted, while the papers of negative quality are indifferent between submitting and not submitting; we can assume that they are not submitted. As a result, $\ACCMAP[']$ is a monotone policy with de facto threshold 0, maximizing the conference quality.  

Next, consider any threshold policy \ACCMAP implementing the de facto threshold of $\theta=0$. It cannot accept papers with review vectors $(L,H), (H,L)$ with probability exceeding $7/16$. 
Otherwise, by virtue of being a threshold policy, \ACCMAP would have to accept all papers with higher review vectors with probability 1; as a result, the acceptance probability of a paper with quality $-2$ would exceed $7/16 \cdot 2/9 + 1 \cdot (1/36 + 1/18 + 1/36) = 5/24$.

Again by virtue of being a threshold policy, \ACCMAP must reject all papers with review vectors $(L,L), (L,M), (M,L)$. 
Thus, a paper of quality $5$ is accepted with probability at most $1 \cdot (4/9 + 2/9 + 1/36) + 7/16 \cdot 2/9 = 19/24$, while a paper of quality $1$ is accepted with probability at most $1 \cdot (1/4+1/6+1/36) + 7/16 \cdot 1/3 = 85/144$.
Thus, papers of quality $5$ are accepted with the same probability as under the policy \ACCMAP['], while papers of quality $1$ are accepted with strictly smaller probability. As a result, a strictly higher review load is required.
\end{example}

We further note that \cref{ex:threshold-counterexample} also shows that the combination of independent informative review signals is not necessarily informative. To see this, in \cref{tab:threshold-counterexample}, $(M, M)$ is a better review signal than $(L, H)$ or $(H, L)$ in the sense that the expected paper quality is higher conditioned on the former. However, it is not hard to observe that these two signals violate the definition of informativeness. In particular, $\frac{\RevSigDist[1]((M,M))}{\RevSigDist[-2]((M,M))}= 1 < 1.5 = \frac{\RevSigDist[1]((L,H)\text{ or }(H,L))}{\RevSigDist[-2]((L,H)\text{ or }(H,L))}$.


\subsection{Dominating QB-tradeoffs: Review Noise}
\label{sec:QB-trade-noise}

In comparing the different QB-tradeoff curves of \cref{fig:Pareto_frontier_continuous}, we observe that any curves corresponding to higher-quality (i.e., lower-variance) reviews dominate similar curves corresponding to lower-quality reviews.
We show that this is not a coincidence and holds not just for Gaussian noise in the reviews, but for Blackwell dominating review quality (defined in \cref{def:blackwell}).
Note that the former is a special case of the latter.

The full story is a bit more subtle. Whether Blackwell-dominating reviews imply better QB-tradeoffs depends on what space of acceptance policies the conference can optimize over. We show that if the conference has all memoryless acceptance policies available, then better reviews can always be used to  simulate the worse reviews, and the conference can thus obtain at least the same QB-tradeoff. Therefore, better reviews weakly dominate worse reviews even when the signals do not satisfy the informativeness definition. 
However, if the conference is restricted to threshold policies and the reviews are not necessarily informative (\cref{def:informative}), carefully chosen ``worse'' reviews may actually permit the use of a better threshold policy, achieving a better QB-tradeoff. 
However, such behavior is indeed the result of signals violating the informativeness definition: if the review signals are informative, Blackwell dominance again implies a weakly better QB-tradeoff under threshold policies.

\begin{definition}[Blackwell Dominance \cite{bohnenblust1949reconnaissance,blackwell1953equivalent}]
\label{def:blackwell}
Let $\RevSigDist: \QualSet \times \SigSet \to [0,1]$ and $\RevSigDistP: \QualSet \times \SigSet' \to [0,1]$ be two review signal distributions.
$\RevSigDist$ \emph{Blackwell dominates} $\RevSigDistP$ if there exists a garbling $\gamma: \SigSet \times \SigSet' \to [0,1]$ from $\SigSet$ to $\SigSet'$, where for all $\REVSIG \in \SigSet$,
% \dkcomment{Replacing signal \emph{vector} with signals everywhere here. Please keep consistent with future edits.} 
$(\gamma(\REVSIG, \REVSIGP))_{\REVSIGP \in \SigSet'}$ is a distribution on $\SigSet'$, such that for all $\REVSIGP \in \SigSet'$ and all $q \in \QualSet$:

\begin{align*}
\RevSigProbP[q]{\REVSIGP} 
& = \sum_{\REVSIG \in \SigSet} \RevSigProb[q]{\REVSIG} \cdot \gamma(\REVSIG, \REVSIGP).
\end{align*}
\end{definition}

\subsubsection{General memoryless acceptance policies.}
We state the following proposition in the categorical model with $\NumReviews = 1$ review. We discuss the (straightforward) extension to the continuous model and multiple reviews below.

\begin{proposition}
\label{prop:blackwell}
Consider two settings with $\NumReviews = 1$ in the categorical model that are identical except for the review signal distributions (which need not be informative): 
the distribution $\RevSigDist$ of the first setting Blackwell-dominates the distribution $\RevSigDistP$ of the second setting.
Then, over memoryless acceptance policies, the QB-tradeoff in the first setting weakly dominates the QB-tradeoff in the second. 
\end{proposition}

\begin{proof}
We will show that any memoryless acceptance policy $\ACCMAP[']: \SigSet' \to [0,1]$ in the second setting can be simulated in the first setting with the same expected conference quality and review burden. It follows that the QB-tradeoff in the first setting weakly dominates the QB-tradeoff in the second setting.

Because $\RevSigDist$ Blackwell dominates $\RevSigDistP$, there exists a garbling $\gamma$ from $\SigSet$ to $\SigSet'$.  
We define a memoryless acceptance policy $\ACCMAP$ in the first setting: for any review signal $\REVSIG$, we set

\begin{align*}
\AccMap{\REVSIG} & = \sum_{\REVSIGP \in \SigSet'} 
\AccMap[']{\REVSIGP} \cdot \gamma (\REVSIG, \REVSIGP). 
\end{align*}

First, because $(\gamma(\REVSIG, \REVSIGP))_{\REVSIGP \in \SigSet'}$ is a distribution on $\SigSet'$ and $\AccMap[']{\REVSIGP} \in [0,1]$ for all $\REVSIGP$, the output of $\ACCMAP$ is in $[0,1]$. 
Moreover, for any paper quality $\qual \in \QualSet$,
\begin{align*}
\AccP{\ACCMAP}{\qual} 
& = \sum_{\RevSigV \in \SigSet} 
\RevSigProb[\qual]{\REVSIG}
\cdot \AccMap{\REVSIG} \\
& = \sum_{\RevSigV \in \SigSet} 
 \RevSigProb[\qual]{\REVSIG}
\cdot \sum_{\REVSIGP \in \SigSet'} \AccMap[']{\REVSIGP} \cdot
\gamma(\REVSIG, \REVSIGP) \tag{Definition of $\ACCMAP$}\\
& = \sum_{\REVSIGP \in \SigSet'} \left( \sum_{\REVSIG \in \SigSet}
\RevSigProb[\qual]{\REVSIG} \cdot \gamma(\REVSIG, \REVSIGP) \right)
\cdot \AccMap[']{\REVSIGP} \tag{Changing order of summation} \\
& = \sum_{\REVSIGP \in \SigSet'}  \RevSigProbP[\qual]{\REVSIGP} \cdot \AccMap[']{\REVSIGP} \tag{$\gamma$ is a garbling} \\
& = \AccP{\ACCMAP[']}{\qual}.
\end{align*}

Thus, the acceptance policies $\ACCMAP$ and $\ACCMAP[']$ have identical acceptance probabilities for each paper quality; this makes them indistinguishable to authors. In particular, both acceptance policies have the same expected conference quality and review burden. 
\end{proof}

For the continuous setting, the proof can be modified by replacing summation with integration.
When the two settings have $\NumReviews > 1$ reviews drawn independently from $\RevSigDist$ and $\RevSigDistP$, respectively, where $\RevSigDist$ Blackwell-dominates $\RevSigDistP$, we can use the fact that applying the same garbling independently in each dimension gives a garbling on the $\NumReviews$-dimensional signal vectors. Therefore, viewing the entire vector as just one signal, the distribution in the first setting Blackwell-dominates that in the second setting, and Proposition~\ref{prop:blackwell} applies directly. 
Performing this reduction relies on the fact that Proposition~\ref{prop:blackwell} did not require informativeness of the signals. After all, our notion of informativeness (and monotone likelihood ratio in general) is defined only for scalar-valued signals.
Similarly to the case of \emph{better} reviews, when \emph{more} reviews are obtained in the first setting, and the reviews in both settings are drawn from the same distribution, the signal of the combined reviews in the first setting Blackwell-dominates the signal in the second setting: this is because discarding the additional signals is easily seen to be a garbling.

\subsubsection{Threshold acceptance policies.}
\label{subsec:threshold-BW-domi}
Proposition~\ref{prop:blackwell} shows that a better review quality (in the Blackwell sense) implies a better QB-tradeoff if we allow the conference to apply any memoryless acceptance policy. This result even holds for review signals that are not necessarily informative (\cref{def:informative}). In the following proposition, we further show that even if the conference is restricted to apply threshold policies, the same result holds if the review signals are informative, i.e., review signals satisfy the monotone likelihood ratio. 

\begin{proposition} \label{prop:blackwell-threshold}
  Consider two settings with $\NumReviews = 1$ informative review in the categorical model that are identical except for the review signal distributions: 
  the distribution $\RevSigDist$ of the first setting Blackwell-dominates the distribution $\RevSigDistP$ of the second setting.
  Then, over threshold acceptance policies, the QB-tradeoff curve in the first setting weakly dominates the QB-tradeoff curve in the second setting.
\end{proposition}

The following lemma is central to the proof of \cref{prop:blackwell-threshold}, and proved below.

\begin{lemma}\label{claim:blackwell-RB-better}
    Consider two threshold acceptance policies $\ACCMAP$ and $\ACCMAP[']$ which accept papers of quality $\bar{q}$ with probability $1/\rho\in (0, 1)$ in the first and the second setting, respectively.  
    Then, under the author's $\bar{q}$-threshold strategy, the review burden of $\ACCMAP$ in the first setting is no larger than the review burden of $\ACCMAP'$ in the second setting.  
\end{lemma}

This lemma says that if the policies $\ACCMAP$ and $\ACCMAP[']$ induce indifference at the same quality threshold in the authors, then the first setting has a lower review burden than the second.
The proof of Proposition~\ref{prop:blackwell-threshold}, which we turn to now, then follows by creating an appropriate $\ACCMAP$ from $\ACCMAP[']$.
%to compare. 
Given a $\ACCMAP[']$ for the second setting, if there is a paper quality in the second setting that agents are indifferent toward submitting, this is pretty straightforward.  If there is no such paper quality, we make $\ACCMAP[']$ slightly stricter so that such a quality exists. 
In this case, the modified $\ACCMAP[']$ is ``better'' than the old  $\ACCMAP[']$, and, again using Proposition~\ref{prop:blackwell-threshold}, we can show that there is a $\ACCMAP$ inducing the same threshold in the first setting which is ``better'' than the modified $\ACCMAP[']$.

\begin{extraproof}{\cref{prop:blackwell-threshold}}
For any threshold acceptance policy $\ACCMAP[']$ in the second setting (the one that has weaker review signals), let $\vartheta$ be any of the author best response strategies to $\ACCMAP[']$.
(As discussed in \cref{prop:de_facto}, there might be more than one best response.)
We will show the existence of a threshold acceptance policy $\ACCMAP$ in the first setting such that: 1) $\vartheta$ is also a best response to $\ACCMAP$; 2) the conference quality is the same in both settings; and 3) the review burden induced by the policy-response pair $(\ACCMAP, \vartheta)$  in the first setting is at most the review burden induced by the policy-response pair  $(\ACCMAP', \vartheta)$ in the second setting. If the above is true, then every point on the QB-tradeoff curve of the second setting is weakly dominated by a point on the QB-tradeoff curve of the first setting, which completes the proof.


First note that if $\ACCMAP[']$ always accepts or always rejects, then the proposition trivially holds.  
Thus, we can assume that $\ACCMAP[']$ is non-trivial. 
By \cref{prop:de_facto}, the author's best response to a non-trivial threshold acceptance policy is a threshold strategy with some $\theta$: the author will always submit a paper with quality $\Qual > \theta$, not submit if $\Qual < \theta$, and can decide arbitrarily if $\Qual = \theta$.
Additionally, if $\vartheta$ is the strategy under which the author always submits everything, then letting $\ACCMAP$ be the policy that accepts everything minimizes the review burden without changing the conference quality. Therefore, without loss of generality, we can assume that not every paper is submitted under $\vartheta$.

Now, we consider two cases based on whether there are authors who are indifferent between submitting or not submitting.

\begin{enumerate}
\item The first case is that there exists a quality $\bar{q}$ such that an author with threshold strategy $\vartheta$ sometimes, but not always, submits papers of quality $\bar{q}$.  Because $\vartheta$ is a threshold strategy, there can be at most one such quality.  By \cref{prop:de_facto}, the acceptance probability at $\bar{q}$ must be exactly $1/\rho$.  

Now, consider a threshold acceptance policy $\ACCMAP$ in the first setting that also induces $\vartheta$ as the author's best response. By construction, $\bar{q}$ is a de facto threshold. Therefore, the existence of $\ACCMAP$ is guaranteed by \cref{prop:threshold-policy}.
Note that the conference quality remains unchanged: papers with qualities strictly greater than $\bar{q}$ are all accepted, no paper with quality strictly less than $\bar{q}$ is accepted; and the same batch of papers with quality $\bar{q}$ are submitted and accepted i.i.d.~with probability $1/\rho$ in each setting.  
The claim now follows by \cref{claim:blackwell-RB-better}.

\item In the second case, for any possible paper quality $q$, an author with threshold strategy $\vartheta$, either always or never submits papers of quality $q$.  
Let $\bar{q}$ be the highest paper quality which is never submitted. Such a $\vartheta$ exists because we are assuming the categorical model, and we assumed that not every paper is submitted. 

Let $\hat{\phi}'$ denote the threshold policy which accepts papers of quality $\bar{q}$ with probability $1/\rho$ in the second setting. 
Note that both $\ACCMAP'$ and $\hat{\phi}'$ induce $\vartheta$ as the author's best response, because we can assume that authors with papers of quality $\bar{q}$ never submit under $\hat{\phi}'$, given that $\AccP{\hat{\phi}'}{\bar{q}} = 1/\rho$, i.e.~authors are indifferent between submitting papers of quality $\bar{q}$ or not. 
This implies that $\hat{\phi}'$ induces the same conference quality as $\ACCMAP'$. Fixing the author's best response $\vartheta$, the review burden of $\hat{\phi}'$ is no larger than that of $\ACCMAP'$. This is because by definition, $\ACCMAP'$ accepts papers of quality $\bar{q}$ with probability at most $1/\rho$, which is the acceptance probability of papers of quality $\bar{q}$ under $\hat{\phi}'$. Then, by \cref{lem:stricter_policy}, the acceptance probability of papers of any quality is weakly larger under $\hat{\phi}'$ than $\ACCMAP'$, resulting in a weakly smaller review burden under $\hat{\phi}'$.

Finally, in the first setting, let $\ACCMAP$ be a threshold policy that induces a threshold best response with a threshold $\bar{q}$ for the author. Because $\ACCMAP$ is a non-trivial threshold policy, by \cref{prop:de_facto}, $\bar{q}$ is a de facto threshold. Therefore, the existence of $\ACCMAP$ is guaranteed by \cref{prop:threshold-policy}. Furthermore, by \cref{prop:de_facto}, papers of quality $\bar{q}$ are accepted with probability $1/\rho$. By this construction, first note that $\vartheta$ is also a best response to $\ACCMAP$.
Second, the conference quality under $\ACCMAP$ is identical to the conference quality under $\ACCMAP'$ because the same batch of papers are submitted, and all are eventually accepted. Finally, by \cref{claim:blackwell-RB-better}, the review burden of $\ACCMAP$ is no larger than the review burden of $\hat{\phi}'$ which is no larger than the review burden of $\ACCMAP'$. This completes the proof.
\end{enumerate}
\end{extraproof}

\begin{extraproof}{\cref{claim:blackwell-RB-better}}
    Let $(\tau, r)$ and $(\tau', r')$ be the parameters corresponding to the threshold policies $\ACCMAP$ and $\ACCMAP'$, respectively.
    Given that the acceptance probability of $\bar{q}$ is the same under $\ACCMAP[\tau,r]$ and $\ACCMAP[\tau', r']$, we want to show that for every $q > \bar{q}$, the acceptance probability is weakly higher under $\ACCMAP[\tau,r]$ than under $\ACCMAP[\tau', r']$.
    By decomposing the acceptance probability into the individual signals, we need to show that
    
    \begin{equation} \label{eq:blackwell_ineq}
        r \cdot \RevSigProb[q]{\tau} + \sum_{s>\tau} \RevSigProb[q]{\REVSIG} 
        - \left(r'\cdot \RevSigProbP[q]{\tau'} + \sum_{\REVSIGP>\tau'} \RevSigProbP[q]{\REVSIGP} \right) 
        \ge 0,
    \end{equation}
    for any $q > \bar{q}$.

    Let $\gamma$ be the garbling from $\RevSigDist$ to $\RevSigDistP$ (see \cref{def:blackwell}), so that $\RevSigProbP[q']{\REVSIGP} = \sum_{\REVSIG} \RevSigProb[q']{\REVSIG} \cdot \gamma(\REVSIG, \REVSIGP)$ for any $q'$ and $\REVSIGP$. 
    Substituting the garbling-based characterization and changing the order of summation, the left-hand side of \cref{eq:blackwell_ineq} can be rewritten as
    \begin{align*}
        \lefteqn{r \cdot \RevSigProb[q]{\tau} + \sum_{s>\tau} \RevSigProb[q]{\REVSIG}
        - \left( 
        r' \cdot \sum_{\REVSIG} \RevSigProb[q]{\REVSIG} \cdot \gamma(\REVSIG, \tau')
        + \sum_{\REVSIGP>\tau'} \sum_{\REVSIG} \RevSigProb[q]{\REVSIG} \cdot \gamma(\REVSIG, \REVSIGP)  \right)}
        \\ & =
        r \cdot \RevSigProb[q]{\tau} + \sum_{s>\tau} \RevSigProb[q]{\REVSIG} 
        - \sum_{\REVSIG} \left( r'\cdot\gamma(\REVSIG, \tau') + \sum_{\REVSIGP>\tau'} \gamma(\REVSIG, \REVSIGP) \right) \cdot \RevSigProb[q]{\REVSIG}.
    \end{align*}
    
    For notational simplicity, let $h(\REVSIG)=  r'\cdot\gamma(\REVSIG, \tau') + \sum_{\REVSIGP>\tau'} \gamma(\REVSIG, \REVSIGP)$. 
    Because $h(s)$ is a probability (the probability of observing an instantiation of the Blackwell dominated signal that is accepted by $\ACCMAP[\tau', r']$ when the instance of the Blackwell dominating signal is $s$), $0\le h(\REVSIG)\le 1$. 
    We now break the summation over $s$ in the above equation into three summations, namely, the summation over $s<\tau$, $s=\tau$, and $s>\tau$. Combining the summations over the same subset of signals allows us to rewrite the preceding equation as
    \begin{align}
        \lefteqn{r \cdot \RevSigProb[q]{\tau} + \sum_{s>\tau}\RevSigProb[q]{\REVSIG} 
        - \sum_{\REVSIG} h(\REVSIG) \cdot \RevSigProb[q]{\REVSIG}} \nonumber 
       \\ & =
          (r-h(\tau)) \cdot \RevSigProb[q]{\tau} 
        + \sum_{s>\tau} (1-h(\REVSIG)) \cdot \RevSigProb[q]{\REVSIG} 
        - \sum_{s<\tau} h(\REVSIG) \cdot \RevSigProb[q]{\REVSIG}.\label{eq:blackwell_acc_prob_diff}
    \end{align}

    Next, we use the monotone likelihood ratio property.
    Let $\eta_s := \frac{\RevSigProb[q]{s}}{\RevSigProb[\bar{q}]{s}}$ denote the likelihood ratio for signal $s$. 
    Informativeness of signals, being defined as monotone likelihood ratio, then implies that $\eta_{s'} > \eta_s$ for any $s'>s$.
    Using this monotonicity, we can bound \eqref{eq:blackwell_acc_prob_diff} as
    
    \begin{align*}
        \eqref{eq:blackwell_acc_prob_diff} 
        & = (r-h(\tau)) \cdot \eta_\tau \RevSigProb[\bar{q}]{\tau} 
        + \sum_{s>\tau} (1-h(\REVSIG)) \cdot \eta_s \RevSigProb[\bar{q}]{\REVSIG} 
        - \sum_{s<\tau} h(\REVSIG) \cdot \eta_s \RevSigProb[\bar{q}]{\REVSIG}
        \\ & \ge
        (r-h(\tau)) \cdot \eta_\tau \RevSigProb[\bar{q}]{\tau} 
        + \sum_{s>\tau} (1-h(\REVSIG)) \cdot \eta_\tau \RevSigProb[\bar{q}]{\REVSIG} 
        - \sum_{s<\tau} h(\REVSIG) \cdot \eta_\tau \RevSigProb[\bar{q}]{\REVSIG} 
        \\ & =
        \eta_\tau \cdot \left(
        r\cdot \RevSigProb[\bar{q}]{\tau} 
        + \sum_{s>\tau} \RevSigProb[\bar{q}]{\REVSIG} 
        - \sum_{\REVSIG}h(\REVSIG)\RevSigProb[\bar{q}]{\REVSIG}
        \right)
        \\ & =
        \eta_\tau \cdot 
        \left( \AccP{\ACCMAP[\tau,r]}{\bar{q}} - \AccP{\ACCMAP[\tau',r']}{\bar{q}} \right)
        \\ & = 0.
    \end{align*}
    Here, the inequality uses the monotone likelihood property separately for $s > \tau$ and $s < \tau$ (observing the signs of the multipliers of $\eta_s$), and the final step follows from the assumption that the acceptance probabilities of papers of quality $\bar{q}$ in both settings are equal $1/\rho$.
\end{extraproof}


While the result again generalizes from the (presented) categorical model to continuous signals, it does not generalize to $\NumReviews > 1$ signals. The reason is that it relies on informativeness of the signals, a property that is not preserved when combining signals (shown in \cref{ex:threshold-counterexample}). 
In particular, we have numerically found counterexamples which suggest that even though the review signal distribution of one setting Blackwell-dominates the distribution of another setting (and both review signals are informative), it is possible that after combining two independent signals in each setting, the QB-tradeoff achieved by threshold policies in the first setting does not (weakly) dominate the QB-tradeoff in the second setting.\footnote{Our counterexamples use three types of paper qualities, and the signal set contains three signals. Unfortunately, these counterexamples are complicated and not very intuitive, so we choose not to include them in this paper. The counter-examples were found by exhaustive computational search, and the code for this search is available at \url{https://github.com/yichiz97/Conference-Peer-Review}.}
 
These examples also suggest that if the reviews do not satify informativeness, Blackwell dominance does not imply better QB-tradeoffs under threshold acceptance policies.

\subsection{Dominating QB-tradeoffs: Conference Value and Discount Factor}
\label{sec:dominating-value-discount}

We just saw that higher review quality leads to a better QB-tradeoff for the conference. Next, we elaborate on an observation made in \cref{sec:additive-noise}, namely, that a more attractive conference or more patient authors leads to a worse QB-tradeoff, because authors will be more persistent in resubmitting borderline papers.

\begin{proposition} \label{lemma:query burden value}
Consider two settings that are identical except that they have attractiveness factors $\rho$ and $\rho'<\rho$, respectively.
Consider their corresponding QB-tradeoff curves as the acceptance threshold is varied from $-\infty$ to $\infty$.
Then, the QB-tradeoff curve of the first setting dominates the QB-tradeoff curve of the second.
\end{proposition}


\begin{proof}
To verify the definition of the dominance of QB-tradeoff curves, we will show that for any point on $\mathcal{C'}$ that neither corresponds to accepting all papers nor to rejecting all papers, there is a point on $\mathcal{C}$ that has the same conference quality but strictly smaller review burden.

Fix any non-trivial de facto threshold $\theta$; the existence of $\theta$ is guaranteed by \cref{prop:de_facto}.
We first focus on the continuous case.
By \cref{prop:threshold-policy}, both settings have a unique corresponding non-trivial threshold acceptance policy with thresholds $\tau$ and $\tau'$. By \cref{prop:de_facto}, a paper with quality $\theta$ is accepted with probabilities $1/\rho$ and $1/\rho'$, respectively, in the two settings.  
This means that papers with quality $\theta$ are accepted with strictly smaller probability by $\ACCMAP[\tau]$ than $\ACCMAP[\tau']$. 
By \cref{lem:stricter_policy}, $\ACCMAP[\tau]$ is stricter than $\ACCMAP[\tau']$.
Thus, for papers of every quality $q$, the expected number of rounds that a paper of quality $q$ has to be resubmitted is strictly larger in the second setting, which implies a strictly larger review burden.

The argument for categorical models is essentially the same, but slight care needs to be taken to account for the non-uniqueness of threshold policies. Note that fixing a de facto threshold does not fix the conference quality in the categorical model when $\theta\in\QualSet$. However, we can always additionally fix the author's strategy (for both settings considered) if there exists a paper quality for which the authors are indifferent between submitting or not; this fixes the conference quality. Therefore, fixing a de facto threshold $\theta$ is still sufficient to complete the proof.
Let $q < \theta$ be the largest quality in the quality set \QualSet that is strictly smaller than $\theta$. (We define $q$ this way even if $\theta$ itself is in the quality set.)
Such a $q$ must exist because $\theta$ was a non-trivial threshold.
Let $\ACCMAP$ be any threshold acceptance policy which induces $\theta$ as the de facto threshold in the setting with conference attractiveness $\rho$.
Because papers of quality $q$ are not submitted, we must have $\AccP{\ACCMAP}{q} \leq 1/\rho$.
Now, let $\ACCMAP[']$ be a threshold acceptance policy in the setting with attractiveness $\rho'$ which accepts papers of quality $q$ with probability $1/\rho'$. Such a policy $\ACCMAP[']$ exists by Proposition~\ref{prop:threshold-policy}. Because $1/\rho' > 1/\rho$, by Lemma~\ref{lem:stricter_policy}, \ACCMAP is stricter than \ACCMAP['].
Therefore, all papers are accepted with at least the same probability under \ACCMAP['] as under \ACCMAP. 
Therefore, the review burden under \ACCMAP['] is strictly smaller than under \ACCMAP, and since this holds for all non-trivial $\theta$, we have shown domination of the QB-tradeoff.
\end{proof}

\cref{lemma:query burden value} directly implies that either increasing the conference value $\ConfValue$ or increasing the authors' discount factor will harm the QB-tradeoff, in the sense that it will be dominated by the original setting. This result holds across all the models that we consider; it is also confirmed by our analysis based on real data in \cref{sec:noiseless-ICLR}.


\subsection{Acceptance Rate}\label{sec:acc_rate}

One may suspect that the higher the threshold, the more selective the conference, so the lower the acceptance rate.
But this is not always true. The reason is self selection by authors of weaker papers, who may not submit in the first place.  As a result, those papers will not be rejected. 

We first develop some mathematical tools to help us reason about the interaction between the selectivity of the conference (the de facto threshold) and the acceptance rate.

Consider the continuous model.
Let $\tau$ be a  non-trivial acceptance threshold, and $\theta$ the corresponding de facto threshold. As before, let $\AccP{\tau}{q}$ be the probability that a paper of quality $\Qual=q$ is accepted at the conference.  
In round $t$, the total resubmission ``density''
of papers with quality equal to $q\ge \theta$ is equal to 
$\QualDens{q}\cdot \sum_{j=0}^{t} (1-\AccP{\tau}{q})^{t-j}.$
As $t$ gets larger,\footnote{Taking $t$ large enables us to take into account the full history of previously rejected papers that are resubmitted.} this converges to $\QualDens{q}/\AccP{\tau}{q}$. 
Of these papers, a $\AccP{\tau}{q}$ fraction will be accepted in each round.
Hence, the acceptance rate converges to

\begin{equation}\label{eq:accept-rate}
    \AccRate = \frac{\text{Number of papers accepted this round}}{\text{Number of papers submitted this round}}  = \frac{\int_{\theta}^{\infty}\QualDens{q} dq}{%
        \int_{\theta}^{\infty} \QualDens{q}/\AccP{\tau}{q} dq}. 
\end{equation}

We now use \cref{eq:accept-rate} to intuitively reason about how the prior distribution $\QualDist$ affects the acceptance rate $\AccRate$.  Notice that it is the papers with low acceptance probabilities which disproportionally decrease the acceptance rate.  This is because they add only their mass to the numerator, but add their mass scaled by $1/\AccP{\tau}{q}$ to the denominator.  Thus, intuitively, for papers with quality at least $\theta$, if a $z$ fraction of them are borderline with quality  ``near'' $\theta$, and the other $1-z$ fraction has a very high acceptance probability,  the acceptance rate can be approximated by $\frac{1}{z/\AccP{\tau}{\theta} + (1 - z)}$.  Notice that this is decreasing with $z$: the larger $z$, the smaller the acceptance rate.   

The measurement of $z$ intuitively resembles the \emph{hazard rate} of a distribution, which is defined as $\frac{f(x)}{1-F(x)}$, where $f$ is the probability density function, and $F$ is the cumulative distribution function.  Similar to $z$, the hazard rate measures the probability of a paper on a boundary at $x$, $f(x)$, relative to the mass of papers larger than $x$, $1 - F(x)$.   The hazard rate is known to be monotone for thin-tailed distributions, like the Gaussian distribution.  Conversely,  the hazard rate is known to be non-monotone for heavy-tailed distributions, like the Cauchy distribution.  Finally, the hazard rate is known to be (eventually) constant for the Laplace distribution.  

Using the above intuition, we might expect the acceptance rates to decrease for the Gaussian prior.  This is because Gaussian distributions have a monotone (increasing) hazard rate; thus, $z$ is likely to increase and drive down the acceptance rate.  Additionally, we might expect the acceptance rates to increase for the Cauchy distribution. This is because Cauchy distributions have a non-monotone hazard rate; thus, $z$ is likely to decrease at some point and drive up the acceptance rate.  Finally, we might expect that the acceptance rate is relatively constant after some point for the Laplace prior.   
\cref{fig:Acc_rate_continuous} gives evidence to support this intuition.  
We find it remarkable that despite  the heuristic reasoning ``by analogy'' to the hazard rate, we exactly observe these outcomes for the paradigmatic distributions in their respective classes. 

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Acc_rate_vs_threshold_Gaussian.png}
         \caption{Gaussian prior.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Acc_rate_vs_threshold_Laplace.png}
         \caption{Laplace prior.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Acc_rate_vs_threshold_Cauchy.png}
         \caption{Cauchy prior.}
     \end{subfigure}
     \hfill
     \caption{
     The acceptance rate vs.~acceptance threshold under different prior distributions. Here, the noise distribution \REVNOISEDIST is fixed as a normal distribution with zero mean and standard deviation of $1$. Three types of prior distributions of the paper quality, all with zero mean and standard deviation (scaling factor) $\sigma$, are considered: (a) the Normal distribution; (b) the Laplace distribution; and (c) the Cauchy distribution. \label{fig:Acc_rate_continuous}}
\end{figure}

Another interesting observation is that the quality distribution of submitted papers is not a good reflection of the prior quality distribution of papers, even conditional on being above the de facto threshold.  The reason is that papers nearer the de facto threshold need to be submitted more times (on average) before being accepted than higher-quality papers.  Therefore they are over-represented among the submitted papers.\footnote{This aligns well with many reviewers' observation in the real world that many of their assigned papers seem to be borderline.}  However, by \cref{eq:accept-rate}, the quality distribution of accepted papers is an accurate reflection of the prior quality distribution of papers conditional on being above the de facto threshold. 


\section{Noiseless Authors and Real-World Parameters}\label{sec:noiseless-ICLR}
In \cref{sec:thresholds-gaps}, we defined the key notions of de facto thresholds and resubmission gaps, and characterized them generically in terms of model parameters. In \cref{sec:continuous-tradeoffs}, we built on this foundation to obtain theoretical insights into Pareto-optimal QB-tradeoffs and the relationship between the acceptance threshold and acceptance rate. In this section, our goal is to investigate these basic phenomena on more realistic categorical data. We will study two different categorical models: a more theoretical model with binary qualities and signals (which allows for very clean interpretation of model parameters), and a more realistic model with parameters learned from publicly available ICLR data.

We emphasize that the results in this section were obtained analytically rather than with agent-based simulations; however, we use figures to visualize them, as the analytical formulations would be extremely cumbersome.

\subsection{Categorical Models Studied} 
\label{sec:discrete models}

We define the following two special cases of our general model. Both have discrete sets of qualities \QualSet and signals \SigSet. Both models are defined more generally, allowing for noisy authors, for use in later sections.

The \emph{$(\AUTHSIGPROB, \REVSIGPROB, \NumReviews, \ConfValue, \TD)$-binary model} is a categorical model in which there are two paper qualities $\{-1, +1\}$ and two review signals.  One paper quality is referred to as negative (``bad papers''), and the other as positive (``good papers''). The prior is such that each paper is equally likely to be bad or good.  Authors receive the correct signal about their papers with probability \AUTHSIGPROB and otherwise receive the opposite signal.  Similarly, each reviewer receives the correct signal about his assigned paper with probability \REVSIGPROB and otherwise receives the opposite signal.  The parameters \NumReviews, \ConfValue, and \TD are, as in general, the number of solicited reviews, the value of the prestigious conference, and the discount factor, respectively.

The \emph{$(\lambda_A, \lambda_R, \NumReviews, \ConfValue, \TD)-\text{ICLR}^{y, L}$ model} is a categorical model learned from data.
Specifically, the prior of paper quality $\QualDist^*$ and the review signal distribution $\RevSigDist^*$ are learned from the ICLR 2020 and 2021 open review datasets \cite{iclr2020review,iclr2021review}; for each dataset, a model is learned with paper quality sets of sizes $L=|\QualSet|\in \SET{4, 5}$. 
The year of the dataset is denoted by $y$. 
Details of the method used for learning are described in \cref{sec:learning-parameters}, where the learned parameters are shown in \cref{tab:learned_para}. 


When varying the signal quality in models based on ICLR data, we use $\lambda_A \in [0, 1]$ for the author's signal and $\lambda_R \in [0, 1]$ for the reviewer's signal; the parameters control the probability with which the signal is drawn from the learned parameters $\RevSigDist^*$ vs.~a uniform distribution. $\lambda_A=1$ ($\lambda_R=1$) implies that the signal is drawn from $\RevSigDist^*$ (the same for both authors and reviewers) while $\lambda_A=0$ ($\lambda_R=0$) implies that the signal is uniformly random.
The parameters \NumReviews, \ConfValue, and \TD are again the number of solicited reviews, the value of the prestigious conference, and the discount factor, respectively. 
When we model noiseless authors, we simply remove the entry $\lambda_A$ from the tuple and call this the $(\lambda_R, \NumReviews, \ConfValue, \TD)-\text{ICLR}^{y, L}$ noiseless-author model.

We remark that the learned distributions $\RevSigDist^*$ of reviewer signals 
do not strictly satisfy informativeness as defined in \cref{def:informative}. However, as we will see, the properties that we are interested in still (approximately) follow.


\subsection{The Resubmission Gap in Categorical Models}
\label{sec:generalization_categorical}

We begin by juxtaposing the acceptance threshold and de facto threshold in a model whose parameters are learned from ICLR data, and in which each paper is reviewed $\NumReviews=3$ times.
We study this juxtaposition to understand the extent of the resubmission gap for a situation with realistic parameters for a real-world conference.

By Proposition~\ref{prop:threshold-policy}, each de facto threshold can be achieved by a threshold policy with some acceptance threshold; we therefore focus on threshold acceptance policies.\footnote{Threshold acceptance policies may not achieve the optimal QB-tradeoff, but are natural and likely more readily accepted in practice.}
However, given that the models we consider here are categorical, a threshold $\tau$ may not uniquely determine a policy, namely, when there is a combination of reviewer signals which occurs with positive probability and induces posterior expected quality exactly $\tau$. 
For convenience of notation and visualization, instead of specifying the additional parameter $r$, we use the following convention to associate a unique policy $\ACCMAP[\tau]$ with each $\tau \in \R$.
Let $\min(\QualSet) \leq U(\RevSigV[1]) < \cdots <  U(\RevSigV[M]) \leq \max(\QualSet)$ be the expected posterior qualities of all possible combinations of review signals that occur with positive probability.\footnote{The assumption that all inequalities between $U(\RevSigV[1])$ and $U(\RevSigV[M])$ are strict is basically without loss of generality. If there are multiple review vectors with the same expected posterior quality, our proofs can be adjusted by replacing one vector \RevSigV[i] with the set of all vectors giving rise to the same $U(\RevSigV[i])$. This change is merely syntactic.} 
\begin{enumerate}
\item If $\tau <  U(\RevSigV[1])$, accept everything.
\item If $\tau \geq \max(\QualSet)$, reject everything.
\item If $ U(\RevSigV[M]) \leq \tau < \max(\QualSet)$, accept papers of posterior expected quality $U(\RevSigV[M])$ with probability $\frac{\max(\QualSet) - \tau}{\max(\QualSet) -  U(\RevSigV[M])}$ and reject all other papers.
\item For the intermediate cases $\tau \in [ U(\RevSigV[1]),  U(\RevSigV[M]))$,
let $i$ be such that $\tau \in [ U(\RevSigV[i]),  U(\RevSigV[i+1]))$. Then, we interpret a threshold of $\tau$ as the policy which accepts all papers of expected posterior quality at least $U(\RevSigV[i+1])$, rejects all papers of expected posterior quality strictly less than $U(\RevSigV[i])$, and accepts papers of expected posterior quality exactly $U(\RevSigV[i])$ with probability $\frac{U(\RevSigV[i+1]) - \tau}{ U(\RevSigV[i+1]) -  U(\RevSigV[i])}$.
\end{enumerate}

Recall from Proposition~\ref{prop:max-general} that optimal policies for the conference are exactly those that induce a de facto threshold of $\theta=0$ in the authors; that is, they induce authors to submit all papers of positive quality but no papers of negative quality.
If there are multiple threshold acceptance policies all achieving the same de facto threshold of 0, we have the conference choose the most lenient of these, i.e., maximizing the acceptance probability. This is because the same set of papers is submitted and accepted eventually, but each is reviewed the smallest number of times.

By Proposition~\ref{prop:de_facto}, a paper with quality $\Qual = q$ is submitted if $\AccP{\tau}{q}  > 1/\rho = \frac{1-\TD}{\ConfValue - \TD}$ and not submitted if $\AccP{\tau}{q} < 1/\rho$.
\cref{fig:Resubmission_gap_categorical} plots $\AccP{\tau}{q}$ as a function of the acceptance threshold $\tau$, for all $q \in \QualSet$ in the categorical model. 
In this figure, there are four discrete paper qualities, two of which are negative. 
The optimal de facto threshold is just above the largest negative quality, which is $\hat{\theta} = -0.41$.
The optimal acceptance thresholds are determined by the intersection between the horizontal line $y=1/\rho$ and the curves $\AccP{\tau}{\hat{\theta}}$ (the orange curves), which implies that the optimum thresholds $\tau^*$ are the lowest thresholds to guarantee the maximum quality. 
By definition, the resubmission gaps are then $\tau^*-\hat{\theta}$.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.55\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Resubmission_gap_categorical_noiseless.png}
     \end{subfigure}
     \hfill
     \caption{The probability of acceptance $\AccP{\tau}{q}$ as a function of the acceptance threshold $\tau$ in the \emph{$(\lambda_R, \NumReviews=3, \ConfValue, \TD)-\text{ICLR}^{2020, 4}$ noiseless-author model}.
     In this example, $1/\rho = 0.3$, and the optimal de facto threshold is $\theta = -0.41$.
     We consider two different levels of review quality, characterized by $\lambda_R = 1$ (higher quality) and $\lambda_R = 0.5$; the corresponding curves are shown in solid and dashed lines, respectively.
     The corresponding optimal acceptance thresholds are $\tau^*_1 = -0.24$ and $\tau^*_2 = -0.11$.
     \label{fig:Resubmission_gap_categorical}}
\end{figure}

In \cref{fig:Resubmission_gap_categorical}, the optimum thresholds $\tau^*$ are the intersections between the horizontal line $\AccP{\cdot}{\cdot}=\frac{1}{\rho}$ and the orange curves, which correspond to $q=-0.41$. From this figure, we observe that decreasing $\rho$ or increasing $\lambda$ leads to a decrease in the resubmission gap when the conference uses the optimal de facto threshold. Recall that $\rho = \frac{\ConfValue - \TD}{1-\TD}$. 
Hence, the resubmission gap is increasing in $\ConfValue$ and $\TD$, and decreasing in the review quality. 
Both observations are in line with our theoretical results in \cref{sec:thresholds-gaps}. 
Note that the curves in \cref{fig:Resubmission_gap_categorical} are serrated 
because the probability of acceptance of papers with quality $q$ is approximately equal to $\sum_{i: U(\RevSigV[i]) > \tau} \Pr(\RevSigV[i])$ while ignoring the knife-edge cases. The values of $U(\RevSigV[i])$ are typically distributed unevenly in the range $[U(\RevSigV[1]),  U(\RevSigV[M])]$. Thus, for some $\tau_0$, an increase in the acceptance threshold from $\tau_0$ to $\tau_0+\epsilon$ does not rule out any $\RevSigV[i]$ (resulting in a flat curve at $\tau_0$), while for other $\tau'_0$, the same increase in the acceptance threshold will rule out multiple $\RevSigV[i]$ (resulting in a steep drop in $\ACCP$ at $\tau'_0$).

\subsection{The Quality-Burden Tradeoff and Acceptance Threshold}
\label{subsubsec:QB-tradeoff_categorical}

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.42\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Quality_review_tradeoff_categorical_v2.png}
         \caption{Quality-burden tradeoff}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Acceptance_rate_categorical_v2.png}
         \caption{Acceptance rate.}
     \end{subfigure}
     \hfill
     \caption{In the \emph{$(\lambda_R, \NumReviews=3, \ConfValue=5, \TD=0.7)-\text{ICLR}^{2020, 4}$ noiseless-author model}, (a) shows the different quality-burden tradeoffs; (b) shows the acceptance rate as a function of the acceptance threshold. For (a), the Pareto optimal points are marked with triangles for different review qualities. Note that the de facto threshold which accepts $Q_2$, $Q_3$ and $Q_4$  has expected conference quality $-0.002$, and thus is dominated by ``accepting nothing.''  Similarly, ``accept all'' is dominated by ``accept nothing.''}
     \label{fig:tradeoff_categorical}
\end{figure}

We next want to understand the tradeoff between the conference's quality and the review burden on the community, as well as the relationship between the conference's acceptance rate, the review quality, and the acceptance threshold in the categorical model.

\cref{fig:tradeoff_categorical}(a) shows the QB-tradeoff. Similar to the continuous case, the conference can reduce the number of reviews while giving up some quality, by increasing the threshold and only accepting the papers of highest quality. 
Note that the Pareto frontier of a threshold policy under the categorical model is not a curve, but a set of discrete points. This is because the quality space is discrete, and we assume papers whose authors are indifferent between submitting or not always make the same decision, for convenience of visualization. 
In line with the intuition behind \cref{prop:blackwell,prop:blackwell-threshold}, the Pareto optimal points improve with the review quality (as captured by $\lambda$), i.e., higher review quality allows for a lower review burden while achieving the same conference quality.

The acceptance rate, as shown in \cref{fig:tradeoff_categorical}(b), has a sawtooth like shape as a function of the acceptance threshold. It jumps up whenever the acceptance threshold reaches a level where the lowest remaining tier of papers will not be submitted any more. Subsequently, as the acceptance threshold increases further, the acceptance rate decreases, as the same papers are submitted, but more papers are rejected due to the higher threshold.
Furthermore, keeping the acceptance threshold constant, higher review quality leads to \emph{lower} acceptance rate when the acceptance threshold is relatively low, but to \emph{higher} acceptance rate when the threshold is high. 
The intuition is that fixing the review quality, there is a minimum posterior quality $U(\RevSigV)$ which the conference can discern (denoted by $\tau_1$ in \cref{sec:generalization_categorical}). Once the acceptance threshold is below $U(\RevSigV)$, all papers are immediately accepted and thus also submitted.  
The high-quality reviews have a lower minimum $U(\RevSigV)$; thus, for low thresholds, they will reject papers with poor reviews; nonetheless, all papers are submitted, resulting in lower acceptance rate. When the acceptance threshold is high, high-quality reviews lead to more careful self-selection than low-quality reviews, and thus lead to a higher acceptance rate.

\subsection{The Optimal Number of Solicited Reviews}
\label{subsec:opt_m}

We next study the optimal number of reviews the conference should solicit in each round. Notice that an increase in the number of reviews per round may be counter-balanced by a decrease in the number of rounds until a paper is accepted. More specifically, while even with one review, a correct acceptance threshold can induce the author to perfectly self-select and only submit papers of positive quality, this threshold may be very high. As a result, it may take many rounds of resubmission to accept the desirable papers. 
More reviews per round can be interpreted as a more accurate signal, allowing earlier acceptance for desirable papers.
We empirically study this tradeoff and find the optimal $\NumReviews$.   


\begin{figure}[htb]
     \begin{subfigure}[b]{0.41\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Burden_beta_m_binary.png}
         \caption{Binary.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Burden_beta_m_categorical_v2.png}
         \caption{Categorical.}
     \end{subfigure}
     \hfill
     \caption{The review burden vs.~review quality under different numbers of solicited reviews in 
     (a) the \emph{$(\AUTHSIGPROB=1, \REVSIGPROB, \NumReviews, \ConfValue=5, \TD=0.7)$-binary model} and  (b) the \emph{$(\lambda_R, \NumReviews, \ConfValue=5, \TD=0.7)-\text{ICLR}^{2020, 4}$ noiseless-author model}.  \label{fig:R_beta_varying_m}}
\end{figure}

In \cref{fig:R_beta_varying_m}, we show examples of how $\NumReviews$ and the review quality affect the review burden. 
We do not trace out all the points of the QB-tradeoff. Instead, 
we set the de facto threshold as permissibly as possible while maximizing conference quality.
Note that in \cref{fig:R_beta_varying_m}, the optimal number of reviews per round is either 1 or 2.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{opt_m_binary.png}
         \caption{Optimal $m$, binary.}
     \end{subfigure}
     \hfill
     \centering
     \begin{subfigure}[b]{0.42\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Heat_map_burden_varying_V_binary.png}
         \caption{Review burden, binary.}
     \end{subfigure}
     \hfill
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Heat_map_opt_m_varying_V_categorical_2020_L4_v2.png}
         \caption{Optimal $m$, categorical.}
     \end{subfigure}
     \hfill
     \centering
     \begin{subfigure}[b]{0.42\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Heat_map_burden_varying_V_categorical_2020_L4_v2.png}
         \caption{Review burden, categorical.}
     \end{subfigure}
     \hfill
     \caption{
     The $\NumReviews$ that leads to the minimum review burden while guaranteeing the maximum conference quality in (a) the \emph{$(\AUTHSIGPROB=1, \REVSIGPROB, \NumReviews, \ConfValue, \TD=0.7)$-binary model} and (c) the \emph{$(\lambda_R, \NumReviews, \ConfValue, \TD=0.7)$-ICLR$^{2020, 4}$ noiseless-author model}. (b) and (d) show the minimum review burden that can be achieved while guaranteeing the maximum conference quality in the same two models, respectively. \label{fig:opt_m_burden_binary} }
\end{figure}

Next, in \cref{fig:opt_m_burden_binary}, we investigate how the optimal number of solicited reviews and the review burden depend  on the combination of the conference's value $\ConfValue$ and the review quality, in both the binary model and the categorical models.
We observe that, in general, a relatively small number of solicited reviews ($\NumReviews \leq 3$) is optimal in most cases, especially when the conference is not much more valuable than the sure bet.

Inspecting the results for the binary model in \cref{fig:opt_m_burden_binary}(a), we observe that a single review is optimal both when the review quality is low ($\REVSIGPROB$ is close to $0.5$) and almost perfect ($\REVSIGPROB$ is close to $1$). 
The intuition is that when the review quality is poor, slightly more reviews do not help with distinguishing the papers significantly better; on the other hand, when the reviews are almost perfect, the conference does not need more reviews to distinguish the papers. Additional reviews are most beneficial in the intermediate range of \REVSIGPROB. 

\subsection{The Tradeoff between Review Quality and Quantity} 
\label{subsec:qual-quant}

We next consider how a change in the quality of reviews affects the review burden.  More specifically, we investigate how many fewer reviews are sufficient to retain the same conference quality when each review has slightly higher quality.  Naturally, producing or procuring higher-quality reviews takes extra effort; our results can be interpreted as guidance on whether the extra effort is worth the improvement within the context.

Revisiting \cref{fig:R_beta_varying_m}, our results show that in general, improving the review quality lowers the review burden.  This is true both if we fix $m$ or if we optimize over the best $m$ (which traces out the lower envelope of the plots).
For the more controlled binary model (in which review noise can be unambiguously quantified), monotonicity indeed holds throughout.
In contrast, in the more complicated categorical model, the review burden $\PaperReviews$ is not always monotone decreasing in the review quality $\lambda_R$. 
One possible reason is that, as shown in \cref{subsec:threshold-BW-domi}, Blackwell dominance in the review signal distribution does not necessarily imply dominance in the QB-tradeoff curves if the conference applies a threshold acceptance policy. Counterexamples like this may occur occasionally in the more complicated categorical model that is estimated from real data.

We next investigate the dependency more quantitatively in the binary model in \cref{fig:R_beta_varying_m}(a) for a fixed value of $m$.
For example, observe that for $m = 3$, the review burden for $\beta = 0.7$ is $R=3.4$ while for $\beta = 0.6$, it is just shy of $6.7$, almost twice as high.  Thus, the review burden can be nearly halved by obtaining slightly more accurate reviews. Note that with $m = 3$, the smallest possible review burden is $1.5$.  This is because when the reviews are perfect, the bad half of the papers will not be submitted, while the good half will be accepted after one round (entailing 3 reviews).  

We observe that generally, the effect of improving review quality on the review burden is more significant when the review quality is low. To see this, we look at the lower envelope of \cref{fig:R_beta_varying_m}(a) when $m$ is optimized.  Here we see that the impact on review burden for improving $\beta$ is larger when $\beta < 0.75$ than when $\beta > 0.75$. 
Similar patterns can be observed in the categorical model in \cref{fig:R_beta_varying_m}(b).

We also consider the joint dependency on the conference's value and the review quality. \cref{fig:opt_m_burden_binary}(b) and \cref{fig:opt_m_burden_binary}(d) show the review burden for different conference values \ConfValue, optimized over $\NumReviews$ and the policy threshold.  We see that, fixing a review quality, the review burden increases with $\ConfValue$; the increase is steeper when the reviews have low quality.
(The distance between contours is smaller when the review quality is low.)  
Seen from another perspective, to maintain a desired review burden (fixing a contour in the figure), if the conference value is larger, the conference requires a higher review quality.

\subsection{The Author's Utility}

So far, our discussion and analysis have focused exclusively on the conference's optimization tradeoff between quality and review burden, without consideration for the author's utility. 
We next investigate how this tradeoff changes when taking the author's utility into account.
Specifically, we consider the author's utility as a constraint imposing that each submitted paper be accepted within 3 rounds of review in expectation, over the randomness in both paper quality and reviews. Recall that each submitted paper is accepted \emph{eventually}, so we are now considering a constraint that this eventual acceptance be fast enough.

In order to satisfy this additional constraint, if the review quality is fixed, the conference can (or has to) request more reviews in each round. The conference will still ensure that the de facto threshold is 0, but due to the more accurate signal obtained from more reviews, a smaller number of rounds of review is required.

\cref{fig:author_utility_opt_m} is analogous to \cref{fig:R_beta_varying_m} in the binary model, but with the additional constraint that the number of rounds be at most $3$.\footnote{Obtaining the same results for the categorical model would be very time consuming, as the number of solicited reviews is more than $5$; therefore, the corresponding figures are not presented.} 
Not surprisingly, the required minimum number of solicited reviews is increasing as the conference value increases and the review quality decreases. However, when the review quality is sufficiently high ($\beta > 0.7$), soliciting no more than three reviews is still the optimal choice.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.42\textwidth}
         \centering
         \includegraphics[width=\textwidth]{opt_m_author_utility.png}
         \caption{Optimal $m$.}
     \end{subfigure}
     \hfill
     \centering
     \begin{subfigure}[b]{0.42\textwidth}
         \centering
         \includegraphics[width=\textwidth]{review_burden_author_utility.png}
         \caption{Review burden.}
     \end{subfigure}
     \hfill
     \caption{
     The conference's tradeoff when ensuring that the expected number of rounds of resubmission is at most $3$ and the de facto threshold is $0$. (a) shows the minimum number of solicited reviews that is required, and (b) is the corresponding minimum expected review burden. Both figures are in the \emph{$(\AUTHSIGPROB=1, \REVSIGPROB, \NumReviews, \ConfValue, \TD=0.7)$-binary model}. \label{fig:author_utility_opt_m} }
\end{figure}


\section{Authors with Noisy Signals: ABM Experiments}\label{sec:noisy}
So far, we have considered the impact of parameters and policies on the conference's tradeoffs in a simple, clean, and analytically tractable continuous model (in \cref{sec:continuous-tradeoffs}) and in two discrete models, one of which is learned from ICLR data (in \cref{sec:noiseless-ICLR}); there, no clean closed form was available, but analytical results still lent themselves to easy plots. The primary reason for why these models were tractable was that authors had perfect signals about their own papers' qualities, and thus did not update their beliefs in response to reviews.

When authors receive noisy signals themselves, they \emph{will} update their beliefs about their papers' quality based on the reviews they receive. For example, an author who initially received a signal indicating that her paper was of high quality may revise this estimate downward after receiving several negative reviews.
As a result, authors may not make the same decision in each iteration; while it may initially be utility-maximizing to submit the paper, after several negative reviews, the author may instead submit to the sure bet.

In this section, we focus on authors with noisy signals. 
After all, understanding the behavior of more realistic authors (with imperfect information) is also an important robustness check on our results.
Unfortunately, computing the author's posterior belief of the paper quality, conditioned on the reviews in each round, is  analytically quite intractable. 
Therefore, we use agent-based simulations (agent-based modeling (ABM)) to evaluate the impact of conference policies on outcomes.

\subsection{Experimental Setup}
\label{sec:setup_noisy}

In our ABM experiments, we simulate the submission-review process for $n$ papers. As a brief recap of our model from \cref{sec:model}, we conceptualize the submission-review process in three phases: submission--reviewing--decision. 

In the first phase, each author updates her belief about her paper based on her private signal that is generated based on $\AUTHSIGPROB$, and the reviews from the previous rounds (if any). Given the belief, the author reasons about the expected utility of submitting to the conference or to the ``sure bet'' option. We consider two strategies for the authors' decision making. The \emph{optimal strategy} assumes that authors are best-responding to the conference's policy. That is, given that the game lasts for $T$ rounds\footnote{Due to running time concerns, we set $T=10$ in our experiments.}, the author uses backward induction with dynamic programming to optimize the decision. Specifically, she first reasons about the expected utility in round $T$ given all possible histories of reviews; she then similarly updates the reviews-utility mapping in rounds $T-1$, $T-2$ and so on. Eventually, she can infer the optimal action in the current round.

We also consider a simpler \emph{myopic strategy}. Here, the author reasons about the expected utility of submitting to the conference by assuming that after a rejection, she will submit to the sure bet option. That is, a myopic author in round $t$ does not foresee the future after round $t+1$.  Empirically, the myopic strategy performs almost as well as the optimal strategy.  The myopic strategy asks: is it better to submit one more round before giving up or to give up now?  While it can happen that submitting two more rounds before giving up is better than giving up now which in turn is better than giving up after one round of submission, such cases are quite rare when the same threshold acceptance policy is used in each round.


In the second phase, the conference obtains $\NumReviews$ reviews for each submission. These reviews are drawn i.i.d.~from the review signal distribution $\RevSigDist$, conditioned on the ground truth quality of the submission. 

In the third phase, for each submission, the conference makes a decision of acceptance or rejection based on a threshold policy with threshold $\tau$. Given the $\NumReviews$ i.i.d.~reviews sampled in the second phase, the conference can infer the conditional expected quality of the submission  (conditioned only on the reviews from the current round) and accept or reject the paper based on the simplified threshold policy for the categorical model (described in \cref{sec:generalization_categorical}).

The three phases are repeated for $T$ rounds. After $T$ rounds, all papers that have not been accepted (by either the top conference or the sure bet) are submitted to and accepted by the sure bet. 

\paragraph{Choosing model parameters.} We consider the same two types of models as in \cref{sec:noiseless-ICLR}: a simple binary quality model (which has a concise representation), and the more fine-grained categorical models learned from ICLR data.

For the binary model, we model the authors as using the optimal strategy. When studying the categorical model, we model authors as using the myopic strategy; this is done due to running-time concerns, because finding the optimal strategy would involve a search over too many possible options.

For both models, we set $\NumReviews=3$, $\ConfValue = 3$ and $\TD = 0.7$.
There are $n=10000$ submissions, and we consider $T=10$ rounds. We note that {in most cases, $T=10$ is enough for the conference to accept all submitted papers}. 

\subsection{QB-Tradeoff and Acceptance Rate}
\label{subsubsec:QB_tradeoff_noisy}

In \cref{fig:tradeoff_noisy}, we show the QB-tradeoff and its dependence on the signal qualities of the authors and reviewers.
In \cref{fig:tradeoff_noisy}(b), the signal quality is captured by the parameters $\lambda_A$ and $\lambda_R$ with which the author and the reviewer, respectively, obtain meaningful (as opposed to uniformly random) signals;
see the definition in \cref{sec:discrete models,sec:learning-parameters}.

\subsubsection{The QB-tradeoff}
We first note that the Pareto frontier of the QB-tradeoff consists of a larger number of points when authors are noisy.  For example, in the binary case, the acceptance threshold can be set so that agents with signals indicating high-quality papers only submit once (or alternatively twice) before giving up, while agents with signals indicating low-quality papers never submit.  In the case where agents submit only once before giving up, the review burden will be less, but so will the conference quality, as some high-quality submissions will give up before being accepted. Cases like this can never occur when agents have perfect signals about their papers.

An immediate observation in \cref{fig:tradeoff_noisy} is that improving the signal quality for either the author or the reviewer is generally beneficial to the QB-tradeoff. 

In \cref{fig:tradeoff_noisy}(b), we hypothesize that the Pareto frontier of the current review system is located in the grey area between the brown triangles and the red triangles. The former represents the case of noiseless authors, and the latter represents the case of authors whose signals are as noisy as the reviewers'. In both cases, the reviewers' signals are learned from the ICLR 2020 dateset. 
Comparing the red triangles (which comprise the lower bound of the Pareto frontier of the current system) with the brown triangle at $(2.7, 0.16)$ (which is the Pareto optimal point maximizing the conference quality when authors are noisy), we obtain a sense of how well the current system is performing. Our results suggest that with a properly designed threshold policy, the current review system could achieve at least $94\%$ of the maximum conference quality with at most $180\%$ of the review burden achieved if the authors were noiseless; at another Pareto optimal point, the current system could achieve at least $82\%$ of the maximum conference quality with the same review burden as in the case of noiseless authors.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{quality_review_tradeoff_noisy_binary.png}
         \caption{Binary.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{quality_review_tradeoff_noisy_2020_m4.png}
         \caption{Categorical.}
     \end{subfigure}
     \hfill
     \caption{Pareto optimal points of the quality-review tradeoff under different review qualities and author signal qualities, resulting from different acceptance thresholds in (a) the \emph{$(\AUTHSIGPROB, \REVSIGPROB, \NumReviews=3, \ConfValue=5, \TD=0.7)$-binary model} and (b) the \emph{$(\lambda_A, \lambda_R, \NumReviews=3, \ConfValue=5, \TD=0.7)$-ICLR$^{2020, 4}$ model}. In (a), authors are best-responding; and in (b), authors use the myopic strategy if noisy and are best-responding if noiseless. The grey area is defined by the Pareto frontier of the red triangles and the brown triangles; we hypothesize that this is the Pareto frontier of the real review system. \label{fig:tradeoff_noisy}}
\end{figure}


\subsubsection{The Acceptance Rate}

As in \cref{fig:tradeoff_categorical}(b), the acceptance rate as a function of the acceptance threshold --- shown in \cref{fig:acc_rate_noisy} --- roughly follows a sawtooth shape: as the threshold increases, the rate decreases, except at discrete points.
At these points, the lowest remaining quality level of papers is not submitted any more, leading to a sudden jump in acceptance rate. 
We can also observe that this effect is more pronounced when authors have better signals about their papers' qualities. 
Another interesting observation from \cref{fig:acc_rate_noisy}(a) is that there exist multiple jumps in the acceptance rate as the acceptance threshold increases (e.g., for the brown dashed curve, one at $\tau=-0.25$ and another at $\tau=0.3$). This seems counterintuitive since there are only two categories in the binary model, and thus there should be at most one jump where the bad papers stop submitting. However, note that when authors are noisy, bad papers may stop submitting in different ways. For example, in \cref{fig:acc_rate_noisy}(a) with $\alpha = \beta = 0.9$, when $\tau=-0.25$, the best response of an author with a negative signal is to submit in the first round and stop submitting if she receives more than two negative reviews in the first round of submission; when $\tau=0.3$, her best response is to never submit a paper with a negative signal.

We further look at the effects of the author/review signals on the acceptance rate. On one hand, \cref{fig:acc_rate_noisy} shows that better author self-selection quality leads to a higher acceptance rate. The effect of better self-selection emerges after at least the bottom quality tier of papers ceases to be submitted. 
On the other hand, the effect of the review quality can be discussed in two cases. First, when the acceptance threshold is low, e.g., lower than $0$ in \cref{fig:acc_rate_noisy}(b), high review quality (shown in brown) decreases the acceptance rate, since more bad papers are rejected. Second, when the acceptance threshold is higher than $0$, higher review quality generally increases the acceptance rate since it helps the authors' self-selection. 
This is similar to the behavior observed with perfectly informed authors in
\cref{fig:tradeoff_categorical}(b),
discussed in \cref{subsubsec:QB-tradeoff_categorical}.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Acceptance_rate_binary.png}
         \caption{Binary.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Acc_rate_noisy_2020_m4.png}
         \caption{Categorical.}
     \end{subfigure}
     \hfill
     \caption{Acceptance rate as a function of the acceptance threshold in (a) the \emph{$(\AUTHSIGPROB, \REVSIGPROB, \NumReviews=3, \ConfValue=5, \TD=0.7)$-binary model} and (b) the \emph{$(\lambda_A, \lambda_R, \NumReviews=3, \ConfValue=5, \TD=0.7)-\text{ICLR}^{2020, 4}$ model}. In (a), authors are best-responding; and in (b), authors are using the myopic strategy. \label{fig:acc_rate_noisy}}
\end{figure}


\subsection{Review Quality vs.~Quantity}
\label{subsec:qual_quan_noisy}

We next investigate the tradeoff between the number and quality of reviews, in the presence of noisy authors.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{QB_tradeoff_noisy_binary.png}
         \caption{Binary.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{QB_tradeoff_noisy_categorical.png}
         \caption{Categorical.}
     \end{subfigure}
     \hfill
     \caption{Pareto optimal points of the quality-review tradeoff under different review qualities and $\NumReviews$, resulting from different acceptance thresholds in (a) the \emph{$(\AUTHSIGPROB=0.8, \REVSIGPROB, \NumReviews, \ConfValue=5, \TD=0.7)$-binary model} and (b) the \emph{$(\lambda_A=1, \lambda_R, \NumReviews, \ConfValue=5, \TD=0.7)-\text{ICLR}^{2020, 4}$ model}. In (a), authors are best-responding; and in (b), authors are using the myopic strategy (\cref{sec:setup_noisy}) if noisy and are best-responding if noiseless. \label{fig:tradeoff_noisy_m}}
\end{figure}

\cref{fig:tradeoff_noisy_m} is akin to \cref{fig:tradeoff_noisy}; instead of varying the author's noise, we varied the number $\NumReviews$ of reviews. 
In general, a larger number of solicited reviews $\NumReviews$ leads to a heavier review burden, but can achieve a slightly higher maximum conference quality. The intuition is that more reviews per round helps the authors update their belief about their papers faster. Thus, better self-selection can be induced in fewer rounds which results in fewer mistakes made by the conference.
Again, in line with \cref{fig:R_beta_varying_m}, our results show that the Pareto frontiers when the number of solicited reviews is larger than $3$ are largely dominated by the frontiers when $\NumReviews \le 3$. However, we observe that the disadvantage of using $\NumReviews=1$ is enlarged when authors are noisy: authors cannot learn the true quality of their papers fast enough and because we run the resubmission process for $T=10$ rounds (due to computation issues), this results in significantly lower maximal conference quality. 
For example, in \cref{fig:tradeoff_noisy_m}(b), compared with the light orange circles ($m=2$), the yellow circles ($m=1$) only achieve less than $40\%$ of the conference quality.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Heat_map_noisy_binary.png}
         \caption{Binary.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Heat_map_noisy_categorical_2020_L4.png}
         \caption{Categorical.}
     \end{subfigure}
     \hfill
     \caption{The maximum conference quality that can be achieved with review quality $\REVSIGPROB$ (or $\lambda_R$) and review burden $\PaperReviews$ when authors are noisy in (a) the \emph{$(\AUTHSIGPROB=0.8, \REVSIGPROB, \NumReviews, \ConfValue=5, \TD=0.7)$-binary model} and (b) the \emph{$(\lambda_A = 1, \lambda_R, \NumReviews, \ConfValue=5, \TD=0.7)-\text{ICLR}^{2020, 4}$ model}. For each \REVSIGPROB and $\lambda_R$, we search over all numbers of solicited reviews $\NumReviews\in\{1,\ldots,5\}$, and for each $\NumReviews$, we search over acceptance thresholds $\tau$ from $[\min(\QualSet), \max(\QualSet)]$ with step size $0.01$. Then, the Pareto optimal points are plotted as green dots, and the heat map is drawn to represent the maximum conference quality that can be achieved with the particular review quality and review burden. (The maximum conference quality that can possibly be reached is $0.5$ in (a) and $0.161$ in (b).)\label{fig:heat_map_noisy}}
\end{figure}

\cref{fig:heat_map_noisy} provides a different visualization of the interaction of the conference quality and review burden, which now also incorporates the review quality. For each review quality, we compute the maximum conference quality attainable without going over some review burden.
We see that higher review quality yields more favorable QB-tradeoffs. Although increasing the number of solicited reviews per paper can indeed yield higher conference quality, it usually results in a substantial increase in the review burden. Instead, improving review quality can yield the same conference quality, while reducing the review burden. Furthermore, when the review quality is reasonably high, soliciting a small number of reviews per paper often leads to a fairly good QB-tradeoff. 
For example, in \cref{fig:heat_map_noisy}(a), $95\%$ of the maximum conference quality can be achieved with $\NumReviews=2$ when the review quality is $\REVSIGPROB=0.8$.



\section{Institutional Memory}
\label{sec:memory_policy}
Most conferences' acceptance policies are memoryless, in the sense that resubmissions are treated the same as new submissions.
However, in part to deal with the large number of papers that are repeatedly resubmitted, several conferences have experimented with models that have ``institutional memory.''  We consider the following types of policies which are not memoryless and contain some institutional memory.

\begin{description}
\item[Time Limited, Fixed Threshold:] The simplest way to incorporate memory into the submission process is to limit the number of times the same paper can be submitted.
We call such a policy a \emph{$T$-round fixed-threshold policy.} 

\item[Time Limited, Variable Threshold:] A generalization of $T$-round fixed threshold policies is to allow different acceptance thresholds for different rounds. This allows a conference to set higher/lower standards for resubmissions.
However, we require the conference to solicit the same number of reviews for each round.\footnote{We do so for two reasons. First, this reduces the policy space --- this is significant in terms of computation when optimizing over policies with memory. Second, it excludes highly unrealistic policies with very specific dependency on model parameters. For example, when authors are noiseless, applying a policy with memory can achieve maximum conference quality with minimum review burden: the conference rejects all submissions $T-1$ times without review. In round $T$, one review is solicited, and the paper is accepted if and only if the expected quality conditioned on the review is larger than $\tau$; finally, in round $T+1$, the submission is accepted without review. 
A careful choice of $T$ and $\tau$, taking advantage of authors' patience (or lack thereof) and knowledge of their own paper's quality, ensures that no negative-quality papers are submitted, yet all positive-quality papers are submitted and eventually accepted.}
Formally, a \emph{round-dependent threshold policy} is defined by a threshold vector $\bm{\tau}=\left(\tau^{(1)},\tau^{(2)}, \ldots, \tau^{(T)}\right)$; in round $t \leq T$,  a paper with reviews $\RevSigV$ is accepted if and only if its expected quality conditioned on the most recent review vector $\RevSigV$ (not including reviews from earlier rounds) is at least $\tau^{(t)}$.


\item[Review Following:]
Under a \emph{$T$-round review-following threshold memory policy}, not only does the conference track the number of resubmissions; it also considers all past reviews as equal (additional) reviews of resubmissions. That is, reviews are treated identically regardless of which round they were provided in.\footnote{As such, they do not serve the purpose of verifying whether authors addressed concerns about previous versions of their paper.}
Again, we have the conference obtain the same number of reviews in each round of resubmissions, i.e., $\NumReviews[t]=\NumReviews$ for all $t$.
The conference commits to a number $T$ of rounds and rejects any paper that has been submitted $T$ times. The conference also commits to a sequence of thresholds $\bm{\tau}=\left( \tau^{(1)}, \tau^{(2)}, \ldots, \tau^{(T)}\right)$, such that in round $t$, a paper with historical reviews $\left(\RevSigV[1], \ldots, \RevSigV[t]\right)$ is accepted if and only if its expected quality conditioned on $\left(\RevSigV[1], \ldots, \RevSigV[t]\right)$ is at least $\tau^{(t)}$.
\end{description}

All three policy types are time-limited, in that the number of times any particular paper can be submitted is capped.  This is similar to  certain National Science Foundation programs (e.g., CAREER), where the number of times a proposal (or sometimes author) can submit is limited. 
Time-limited policies with fixed threshold treat all submissions, whether initial or resubmitted, equally in each round, until they have reached their resubmission limit. In this section, we analytically investigate such review policies with a focus on how the limit on the number of resubmissions affects the QB-tradeoff. 
In contrast, the other two types of policies allow different thresholds in different rounds. In particular, the review-following model captures the increasingly popular policy of requiring resubmissions to be accompanied by previous reviews (e.g., at IJCAI).
These two generalizations are more complicated to analyze, and we use ABMs to investigate their QB-tradeoff.

Perhaps subtly, review-following policies do not fully subsume the other two policies, because past reviews cannot be treated differently from new reviews. For example, review-following policies cannot simulate a time-limited fixed threshold policy in which every round, a paper obtains two reviews and is accepted iff both reviews are positive. The reason is that a review-following policy cannot distinguish the case of having one positive review in each round (which should lead to rejection) from the case of having zero positive reviews in the first round and two positive reviews in the second round (which should lead to acceptance).

\subsection{Time-limited Policy With Fixed Threshold}
\label{sec:time_limited_policy}

We start with investigating fixed-threshold policies. We first show that when a conference merely restricts the number of times a paper can be submitted (to some value $T$), noiseless authors will respond exactly as if the conference allowed unlimited resubmissions. Next, based on this property, we show how $T$ affects the QB-tradeoff.

\begin{proposition}\label{prop:best-response-finite}
Consider a conference which allows a paper to be submitted at most $T$ times, and for each of these submissions independently decides whether to accept the paper, according to the same monotone policy \ACCMAP.

Then, the author's best response is to submit the paper (in each round) if $\AccP{\ACCMAP}{\Qual} > 1/\rho$, and to submit to the sure bet option when $\AccP{\ACCMAP}{\Qual} < 1/\rho$.
\end{proposition}

\begin{proof}
  The proof is very similar to that of \cref{prop:de_facto}.
  The author will submit a paper with quality $\Qual \ = q$ if her expected utility is greater than 1, and not submit it if her expected utility is less than 1.
  We compute the expected utility for an author who submits the paper exactly $T$ times, akin to the proof of \cref{prop:de_facto}:

\begin{align*}
    \AUTHUTIL(\ACCMAP,q) 
    &= \ConfValue \cdot \sum_{t=1}^T \left( \AccP{\ACCMAP}{q} \cdot  
    (\TD \cdot (1-\AccP{\ACCMAP}{q}))^{t-1} \right) 
    + (\TD \cdot (1-\AccP{\ACCMAP}{q}))^T \\
    &= \ConfValue \cdot 
    \frac{\AccP{\ACCMAP}{q} \cdot \left( 1 - (\TD \cdot (1-\AccP{\ACCMAP}{q}))^T \right)}{1 - \TD \cdot (1-\AccP{\ACCMAP}{q})} 
    + (\TD \cdot (1-\AccP{\ACCMAP}{q}))^T \\
    &= \left(\frac{\ConfValue \cdot \AccP{\ACCMAP}{q}}{1 - \TD \cdot (1-\AccP{\ACCMAP}{q})} - 1 \right)
    \cdot \left( 1 - (\TD \cdot (1-\AccP{\ACCMAP}{q}))^T \right) + 1.
\end{align*}

To determine when this utility is strictly larger resp.~strictly smaller than 1, we need to determine when the product of the first two terms is positive resp.~negative. The second factor is always positive, and the first has the same sign as $\AccP{\ACCMAP}{q} - 1/\rho$, regardless of $T$.
This completes the proof.
\end{proof}

Notice that while the author's expected \emph{utility} depends on $T$, her best \emph{response} does not. Hence, as with an unlimited number of submissions, an author will either submit as often as she is allowed to or not even submit once, and the author's threshold for doing so is the same as with an unlimited number of resubmissions. 

The property we proved in \cref{prop:best-response-finite} allows us to characterize the expected value of the conference quality and the review burden.
For a given acceptance threshold $\tau$, let $\mathcal{S}_{\tau} \subseteq \QualSet$ be the set of paper qualities which an author will submit at this threshold.
When $\QualSet$ is discrete\footnote{For continuous qualities, the corresponding quantities are obtained by replacing sums by integrals and probabilities by densities.}, the expected conference quality and review burden are as follows:

\begin{align*}
    \CONFUTIL(\tau) 
    & = \sum_{q\in \mathcal{S}_{\tau}} \QualDens{q} \cdot q \cdot 
    {\sum_{t=0}^{T-1} (1-\AccP{\tau}{q})^{t}\AccP{\tau}{q}}
%\\ & = 
\; = \; \sum_{q \in \mathcal{S}_{\tau}} 
    \QualDens{q} \cdot q \cdot \left( 1-(1-\AccP{\tau}{q})^T \right).
    \end{align*}
Note that $(1-\AccP{\tau}{q})^T$ is the probability of a paper with quality $q$ being rejected for all $T$ rounds; thus, the expected conference quality is the expected value of papers in $\mathcal{S}_\tau$, weighted by the acceptance probabilities.  
To compute the review burden, note that the expectation of a non-negative random variable $Z$ is $\Expect{Z} = \int_{0}^\infty \Prob{Z > z}\,dz$. For a paper of quality $q$, the number of submissions exceeds $z \geq 0$ with probability $(1-\AccP{\tau}{q})^{\lfloor z \rfloor}$ for $z < T$, and 0 otherwise. Therefore, the expected review load is

\begin{align*}
    \PaperReviews(\tau) & = \NumReviews \cdot \sum_{q\in \mathcal{S}_{\tau}} \QualDens{q} \cdot 
    {\sum_{t=0}^{T-1} (1-\AccP{\tau}{q})^{t}}
    \; = \; \NumReviews \cdot \sum_{q\in \mathcal{S}_{\tau}} \QualDens{q} \cdot \frac{1}{\AccP{\tau}{q}} 
    \cdot \left( 1-(1-\AccP{\tau}{q})^T \right). 
\end{align*}

\cref{fig:tradeoff_continuous} shows the QB-tradeoff for time-limited fixed-threshold policies in the continuous model. Not surprisingly,  the conference can reduce the review burden at the expense of quality by lowering $T$. Doing so reduces the maximum conference quality that can be reached by the review policy but can also reduce the review burden if the desired conference quality is reachable. 

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Finite_round_memoryless_continuous.png}
     \end{subfigure}
     \hfill
     \caption{The Pareto optimal curves of the QB-tradeoff under $T$-round fixed threshold policies in the \emph{$(\sigma=0.5, \mu_{\QualDist} = 0, \sigma_{\QualDist} = 1, \NumReviews = 1, \ConfValue = 3, \TD = .7)$-Double Gaussian noiseless-author model}. Pareto dominated points are shown as dashed lines, while undominated points are shown as solid lines.  \label{fig:tradeoff_continuous}}
\end{figure}

\cref{fig:tradeoff_finite} shows results analogous to \cref{fig:tradeoff_continuous}, but in the categorical model, and further varying the number of solicited reviews and the acceptance threshold (to accept different categories of paper qualities).
The same pattern can also be observed here: by comparing different colors of dots while fixing an $m$ (fixing a line), we observe that the conference can reduce the review burden at the expense of quality by lowering $T$.

Additionally, when $T$ is small, a large number of reviews per submission contributes more to the conference quality. This can be seen by fixing a color and a shape of marker and looking at the dots on different types of the lines: solid lines ({one review}) result in the lowest conference quality when $T$ is small. The intuition is that when $\NumReviews$ is small, the conference has to apply a very strict acceptance threshold to discourage low-quality papers from being submitted. As a result, such a policy will need a large number of rounds to accept the good papers. 
We conclude that if the conference severely limits the number of times a paper can be submitted, soliciting more reviews per paper will contribute more to a higher conference quality at the expense of additional reviews.

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Finite_round_memoryless.png}
     \end{subfigure}
     \hfill
     \caption{The Pareto optimal points of the quality-burden tradeoff under $T$-round fixed threshold policies in the \emph{$(\lambda_R=1, \NumReviews, \ConfValue=5, \TD=0.7)$-ICLR$^{2020, 4}$ noiseless-author model}. 
     The colors of the markers distinguish $T$, and the shapes of the markers denote the accepted papers' qualities with 
     $\Qual_1 < \Qual_2 < 0 < \Qual_3 < \Qual_4$. 
     Furthermore, we vary the number $\NumReviews$ of reviews per paper, shown by the line type. \label{fig:tradeoff_finite}}
\end{figure}


\subsection{More Fine-Grained Institutional Memory, and Noisy Authors}
\label{sec:memory_noisy}

Next, we investigate the extent to which more fine-grained institutional memory --- different acceptance thresholds in different rounds and reuse of past reviews of a paper --- may further improve the QB-tradeoff for the conference.
We do so in the setting of noisy authors, and therefore --- as before --- use 
ABM experiments. 


\subsubsection{Experimental Setup}

Our approach involves searching over policies with $T$ submissions per paper. Unfortunately, the number of such policies grows exponentially in $T$. We therefore restrict our experiments to $T = 5$, and set $\NumReviews = 3$ for only the binary model.
Computing the optimal strategy for authors in categorical or continuous models requires solving a dynamic program with a much larger state space; the resulting computational requirements prevent us from including such experiments.

We generate candidate policies by searching over different thresholds.
For $T$-round fixed-threshold policies, we select the $40$ candidate thresholds  $\tau \in \{-1, -0.95,\ldots, 0.95\}$; in each run, one of these thresholds is used for all rounds. 
For the other two types of policies, to reduce the number of samples, we only enumerate over the thresholds $\tau^{(t)}$ for the first three rounds while fixing $\tau^{(t)}$ for $t = 4, 5$. 
More precisely, we select $40^3$ candidate threshold vectors $\bm{\tau}$, as follows:
for each $t \in \{1, 2, 3\}$, we select $40$ thresholds $\tau^{(t)}$ from $\{-1, -0.95,\ldots, 0.95\}$ which gives us $40^3$ vectors of length $3$. 
For each $t=4, 5$, we fix $\tau^{(t)}$ such that the paper is accepted if and only if two of the three newly sampled reviews in round $t$ are positive. 

\subsubsection{Results}\label{sec:memory_noisy_result}

\Cref{fig:memory_policies} shows the Pareto optimal points of the QB-tradeoff for each of the three review policies. 
We summarize the main takeaways as follows:
\begin{itemize}
    \item Compared with time-limited fixed-threshold policies, round-dependent threshold policies (the variable-threshold policy and the review-following policy) have slightly higher maximum conference quality. Furthermore, in general, for each of the Pareto optimal points of the former, there exists a Pareto optimal point of the latter which dominates it. Our results suggest that having historical reviews follow submissions can help improve the QB-tradeoff. However, given that the improvement in the maximum conference quality is rather marginal, the main advantage of review following policies seems to be a reduction in the review burden instead of improving the conference quality.
    \item We further observe that for both variable-threshold policies and review-following policies, with round-dependent thresholds, the Pareto optimal thresholds that lead to large conference qualities tend to have the following pattern: review papers strictly in the first two rounds and more leniently after that.
\end{itemize}

\begin{figure}[htb]
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Review_following_m_3.png}
     \end{subfigure}
     \hfill
     \caption{The Pareto optimal points of the quality-burden tradeoff for three types of policy when authors are noisy in the \emph{$(\alpha = 0.75, \beta=0.75, m=3, V = 5, \TD=0.5, T=5)$-Binary model. The Pareto optimal points of the memoryless policy are also shown for comparison; the memoryless policy is approximated by setting a large $T=50$.}\label{fig:memory_policies}}
\end{figure}

We provide some intuition for both results. Papers that are still being submitted after multiple rejections tend to be  high-quality papers whose authors received strong positive signals. Variable-threshold policies can help reduce the review burden because the conference can induce a desired self-selection (where only the good papers are submitted) while relaxing the acceptance thresholds for papers that are resubmitted for a large number of rounds. This implies that variable-threshold policies can accept good papers within fewer rounds which reduces the review burden.
Furthermore, in order to induce a desired self-selection, the conference should be strict for the first few rounds; this will cause more authors of low-quality papers to refrain from submitting. This leads to the pattern that the Pareto optimal threshold sequence tends to be strict at the beginning and more lenient later on.


\section{Discussion, Future Work, and Conclusion}
\subsection{Implications}\label{sec:implications}
Here, we suggest some possible interpretations and lessons that might be learned from our analysis.

\paragraph{Resubmission Gap}

Perhaps the cleanest result concerns the resubmission gap. If a conference operates like our idealized model with noiseless authors, then we would observe the following:  \emph{Every paper ever submitted to the prestigious conference is eventually accepted; however, at any given conference, many papers will be rejected, and thus need to be submitted multiple times.}  With many parameters, the vast majority of papers are rejected. 
On the surface, this sounds like a dystopian bureaucracy. 

The frequently proposed and obvious reaction is to accept all acceptable papers the first time, without making them resubmit multiple times.  This would allegedly decrease the reviewing load (because each paper would only be reviewed once), without affecting the quality of the conference because all of the papers were going to be accepted anyway. Superficially, this seems very reasonable.

However, our model warns that this is an unlikely outcome.  Instead, by lowering its acceptance threshold, the conference would also lower its de facto threshold.  While the papers being currently submitted could be overwhelmingly accepted in one round, it would invite more, lower-quality submissions.
These lower-quality submissions would themselves be repeatedly submitted, and so the review burden would not necessarily decrease, and could even increase.

Of course, this is a not a perfect reflection of reality. In particular, because authors are not all aware of their papers' qualities, some submissions are of low quality and are very unlikely to ever be accepted.  However, the experience of this article's authors is that for some prestigious conferences, the situation is similar to a large extent: the pool of submitted papers has been self-selected to those that will eventually appear in a good venue. 

Additional relevant modeling ``blinds spots'' are that the review noise for some papers may be different from others, agents may have different levels of patience and different values for their paper being accepted, and the distribution of paper qualities may react to the acceptance policy.  However, it is not clear that any of these model limitations fundamentally challenge the insight that there is a gap between the quality of papers implied by the threshold acceptance policy and the types of papers submitted and eventually accepted.

\paragraph{Threshold Policy and QB-tradeoff}

In the preceding discussion of the resubmission gap, it was not clear what happens to the review burden as the acceptance policy becomes more strict or lenient.  This is because it depends on the prior distribution of paper qualities.  This effect was studied in \cref{subsec:QB-tradeoff}, where we identified three possible Pareto optimal acceptance policies:  (1) accept only a few top papers; (2) accept all worthy papers; (3) accept nearly all papers, only attempting to weed out the worst.  All of these policies can be easily identified in practice.

The advantage of policies (1) and (3) is that they tend to require less reviewing.  For policy (1), this is because few papers are submitted; for policy (3), it is because few papers are submitted more than once. The advantage of policy (2) is that it maximizes the conference quality, though often at the expense of a high review burden.

In our analysis, however, the  policy (3) is Pareto optimal only if the prior of paper qualities contains mostly positive papers.  If, instead, the paper quality prior is a unimodal distribution centered at 0, then such a policy with a negative de facto threshold will never be Pareto optimal.  
We note that in the data learned from ICLR displayed in \cref{fig:tradeoff_categorical}, all the acceptance policies which accept negative-utility papers are Pareto dominated. (However, this is not always the case for the models learned from ICLR data for different years or parameter settings). 


\paragraph{Conference Value and Discount Factor}
We showed, in the noiseless author setting, that increasing a conference's prestige and thus value, or increasing the discount factor, creates a strictly worse QB-tradeoff curve. This has a range of practical implications.  

First, having very high prestige conferences creates a higher reviewing burden.  In our model, this happens regardless of the acceptance policy; however, more realistically, 
lowering the acceptance policy causally decreases the value of the conference which our model does not account for.

Second, policies that burden the author may improve the QB-tradeoff.  Examples include long review times, rebuttal periods, or onerous formatting requirements.  This will intuitively allow the conference to decrease their acceptance threshold while keeping the same de facto threshold, and thus may decrease the review load without impacting conference quality. Essentially, such policies artificially internalize the negative externality of imposing reviews upon others.  

Conversely, our model predicts that well-meaning efforts to reduce these burdens may very well worsen the QB-tradeoff.  Proposed and executed reforms include: decreasing the required time to review papers for a given conference and a ``desk reject'' phase, where papers that appear subpar are quickly returned to authors without review.  These policies artificially increase the time discount \TD by decreasing the time between submissions.  This forces the conference to increase its acceptance policy threshold if it would like to maintain the same de facto threshold (and thus maintain conference quality).   

Of course, it should be emphasized that these observations are not to be taken as recommendations: in particular, as we will discuss in \cref{sec:limitations}, our analysis essentially ignores the authors' utilities, and a longer time until acceptance or more burden to submit leads to a decrease of this utility. The tradeoff should naturally include all three concerns, and our observations should be interpreted as emphasizing one aspect that may not have been considered enough in the past.

\paragraph{Acceptance Rate}

In \cref{sec:acc_rate}, we showed that empirically, whether the acceptance rate increases, decreases, or remains steady as the acceptance threshold increases is a function of a quantity resembling the hazard rate of the prior paper quality distribution.
This warns against using the acceptance rate as a signal of quality.  At issue is that for certain priors of paper quality, as the selectivity increases, the fraction of papers near the boundary decreases.  

Additional factors may complicate this picture. For example, higher-quality papers may have different distributions of review noise than lower-quality papers.

We note that in our noiseless model, the quality distribution of accepted papers equals the prior paper quality distribution conditional on being greater than the de facto threshold. However, our ICLR data set was instead learned from the \emph{submitted} papers, so it may overcount borderline papers while under-counting low-quality papers.  

\paragraph{Quality vs.~Quantity of Reviews}
Our results here discourage the strategy of soliciting a large number of reviews per paper. In particular, any number of solicited reviews larger than $3$ greatly burdens the review system but is unlikely to bring enough benefits to the conference quality. Instead, our model predicts that a small number of solicited reviews, even with one review per paper, can be optimal if the conference is able to find the optimal acceptance threshold. The intuition is that when authors know the quality of their papers well enough, any number of solicited reviews, as long as it is combined with the optimal acceptance threshold, can take advantage of authors' self-selection such that only the desired papers are submitted, and eventually accepted. 

A larger number of reviews may nonetheless be desirable due to several real-world considerations: first, more reviews may decrease the average number of times a paper needs to be resubmitted, helping authors; second, our reviewer error model does not capture the situation where there may be a ``fatal flaw'' that only an astute reviewer observes; and finally, more reviewers can provide more feedback.
Relatedly, in \cref{fig:tradeoff_noisy_m}, we observe that soliciting only one review per paper may slow the progress of authors learning about the quality of their papers based on historical reviews;
this results in low conference quality. Moreover, our model assumes that reviews are i.i.d., which is not true in reality when the reviewers can communicate (after the rebuttal). The integrated review signals after communication may become much more informative than aggregating each of them as an i.i.d.~review; this may significantly benefit the strategy of soliciting a large number of reviews.

\paragraph{Institutional Memory}

Our results indicate that having historical reviews follow submissions, or allowing the conference to limit the number of times a paper can be submitted, can help improve the QB-tradeoff. However, given that the improvement in the maximum conference quality is rather marginal, its main effect seems to be reducing the review burden. The intuition is that with such a policy, the conference can be strict in the first few rounds and relax the acceptance threshold for repeated resubmissions so that only the good papers will be submitted and accepted more quickly.

We note that due to computational concerns, our conclusion here is largely based on the ABM experiments in the simplest binary model. Given that there seems to be general resistance to the idea of review-following (perhaps for fear that a bad, but erroneous, review may bring bias to the following reviews and doom a paper's chances for a long time), even if these modest gains generalize, this idea is unlikely to be a game changer.

In summary, our results provide the following insight: with an optimal de facto threshold, the conference can balance its quality and the review burden, even with a small number of solicited reviews per paper and with a memoryless acceptance policy.  
For any de facto threshold a conference would like to implement, a first-order concern is what acceptance policy to employ.

\subsection{Limitations and Future Directions} \label{sec:limitations}

\paragraph{Author's Utility}
A major limitation of our work is that we do not explicitly account for author utility.  Although our model of author utility makes sense from the point of view of modeling the authors' local \emph{decisions}, it does not  capture the authors' actual utility in a holistic sense. For example, if the conference accepted everything, then every author's utility would be \ConfValue according to the model (and indeed, authors would prefer having their papers accepted); however, the prestige of a conference is also derived from its selectivity, so the authors of high-quality papers would not be happy with this outcome.  
In reality, \ConfValue is a long-term function of the quality of papers appearing in that venue, a fact that we do not include in the model.

Indeed, the discussion above about adding needless annoyances to submissions to discourage low-quality submissions is necessarily incomplete without author utility. 
Even if such policies greatly reduced the review burden, this improvement would have to be weighed against the additional burden to authors.

We do note that, typically in our analysis, for a fixed de facto threshold,  the average number of submission for a paper is closely related to review burden and is a reasonable proxy for the author utility.  Thus, the authors' interests are, to a limited extent, present.  However, this relationship breaks down on occasion, for example, when discussing the number of reviews the conference should allocate per submission.  Here, the cost to the conference is measured in reviews, while the cost to the author is measured in rounds of submission, and because \NumReviews is not fixed, these are not the same. 

Future work could make author utility a first-order concern, in part by modeling the conference value $\ConfValue$ as dependent on the quality of the papers at the conference.

\paragraph{A Single Prestigious Conference}

Another limitation of this work is that we assume a single prestigious conference.  As mentioned in \cref{ft:multi_conferences}, this can model several prestigious conferences that are more or less cooperating to uphold community standards.

Of course, in reality, there is an ecosystem of conferences and not all of them are either top-tier or sure bets.  In such a setting, our analysis could model the decision to submit to a top-tier or second-tier conference.  The utility for submitting to the second tier could be normalized to 1.  The issue with this, however, is that the second-tier conference still needs to review its submissions. 
Other analyses (see related work in \cref{sec:related-work}) have considered venues of different values. 

Furthermore, our model omits competition between conferences.  For example, conferences may compete to attract more papers by attempting to increase their quality, making their acceptance policy more predictable, creating a faster turnaround time, etc. 
Future work could extend our model to these settings. 

\paragraph{Heterogeneity}
We also did not model various sources of heterogeneity in the process. For example, the qualities of reviews are not uniform, and different authors have different levels of patience (e.g., a Postdoc who will be on the job market vs.~a first-year student or a tenured faculty member). We are also not modeling the effects of biases that may impact different researchers disproportionately.  The uneven impact on different author populations would be made more complex by co-authorship.  As with the previously mentioned endogenous review quality, a main difficulty would be that this model would have more parameters, and as such require learning/setting them, which could lead to arbitrary choices.

\paragraph{Additional Feedback Loops}

There are several feedback loops that we disregard.  We model paper quality as exogenous; however, in reality, it is largely determined by  authors who decide how much effort or time to expend improving their papers.  Authors often write a paper to target particular venues.  As the venues change their policies, the underlying distribution of paper qualities is likely to change.  Moreover, authors may improve their papers in response to reviewer feedback. Additionally, as mentioned already, in our model, the acceptance policy does not impact the conference value, and the review burden does not impact the quality of the reviews or the amount of time authors (who are typically also the reviewers) spend writing papers. 

As mentioned in the discussion of related work, several past papers do try to model these complexities and provide insights with agent-based simulations \cite{kovanis2016complex, bianchi2018peer, squazzoni2012saint}.

Here, we focus on a simpler and cleaner model than can be afforded when including these complexities.  Apart from the analysis being more difficult, it is often difficult to know precisely how these feedback loops function, which may lead to even greater uncertainty regarding the accuracy of a model, and any insights derived from it.  

\paragraph{Modeling Paper Quality}

Finally, it should be noted that papers do not have an ``objective'' quality that can be projected to a single dimension or even multiple dimensions.  One approach, which adds minimal complexity along these lines, is to distinguish between different types of poor-quality papers.  Some papers may be deemed low quality because they are methodologically flawed;  others because their contributions may be incremental.  The first of these might be easier for reviews to detect and agree on than the second.

\subsection{Conclusion} 

Despite the fact that there may be no agreed-upon objective metric, still the rigor of peer review is important to a healthy future of academic research.
We avoid concrete recommendations, as many quantities and observed trends depend on model parameters and on unmodeled real-life effects. We hope that our theory (and simulations) can steer the discussion, uncover parameters to focus on, and inform decision makers. We believe that the focus on the resubmission gap, and the importance of reviewing quality over quantity, are important points to start a discussion in the community which may not have been as easily identified without studying a model like ours.


\subsubsection*{Acknowledgments}
We would like to thank Nageeb Ali, Shaddin Dughmi, Jonathan Libgober, and Omer Tamuz for useful discussions and pointers.
DK was supported in part by ARO MURI W911NF1810208.  YZ and GS were supported in part by NSF CCF \#2007256.  FYY was supported by NSF IIS-\#2007887.

\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[icl(2020)]{iclr2020review}
{ICLR} 2020 review data, 2020.
\newblock https://github.com/shaohua0116/ICLR2020-OpenReviewData.

\bibitem[icl(2021)]{iclr2021review}
{ICLR} 2021 review data, 2021.
\newblock https://github.com/evanzd/ICLR2021-OpenReviewData.

\bibitem[Allesina(2012)]{allesina2012modeling}
Stefano Allesina.
\newblock Modeling peer review: An agent-based approach.
\newblock \emph{Ideas in Ecology and Evolution}, 5\penalty0 (2), 2012.

\bibitem[Ammar and Shah(2012)]{ammar2012efficient}
Ammar Ammar and Devavrat Shah.
\newblock Efficient rank aggregation using partial data.
\newblock \emph{ACM SIGMETRICS Performance Evaluation Review}, 40\penalty0
  (1):\penalty0 355--366, 2012.

\bibitem[Bianchi et~al.(2018)Bianchi, Grimaldo, Bravo, and
  Squazzoni]{bianchi2018peer}
Federico Bianchi, Francisco Grimaldo, Giangiacomo Bravo, and Flaminio
  Squazzoni.
\newblock The peer review game: an agent-based model of scientists facing
  resource constraints and institutional pressures.
\newblock \emph{Scientometrics}, 116\penalty0 (3):\penalty0 1401--1420, 2018.

\bibitem[Blackwell(1953)]{blackwell1953equivalent}
David Blackwell.
\newblock Equivalent comparisons of experiments.
\newblock \emph{The Annals of Mathematical Statistics}, pages 265--272, 1953.

\bibitem[Bohnenblust et~al.(1949)Bohnenblust, Shapley, and
  Sherman]{bohnenblust1949reconnaissance}
Henri~Fr{\'e}d{\'e}ric Bohnenblust, Lloyd~S. Shapley, and Seymour Sherman.
\newblock \emph{Reconnaissance in game theory}.
\newblock Rand Corporation, 1949.

\bibitem[Cicchetti(1991)]{cicchetti1991reliability}
Domenic~V. Cicchetti.
\newblock The reliability of peer review for manuscript and grant submissions:
  A cross-disciplinary investigation.
\newblock \emph{Behavioral and brain sciences}, 14\penalty0 (1):\penalty0
  119--135, 1991.

\bibitem[Cole et~al.(1981)Cole, Cole, and Simon]{cole1981chance}
Stephen Cole, Jonathan~R. Cole, and Gary~A. Simon.
\newblock Chance and consensus in peer review.
\newblock \emph{Science}, 214\penalty0 (4523):\penalty0 881--886, 1981.

\bibitem[Cook et~al.(2007)Cook, Golany, Penn, and Raviv]{cook2007creating}
Wade~D. Cook, Boaz Golany, Michal Penn, and Tal Raviv.
\newblock Creating a consensus ranking of proposals from reviewers partial
  ordinal rankings.
\newblock \emph{Computers \& Operations Research}, 34\penalty0 (4):\penalty0
  954--965, 2007.

\bibitem[Dawid and Skene(1979)]{dawid1979maximum}
Alexander~Philip Dawid and Allan~M. Skene.
\newblock Maximum likelihood estimation of observer error-rates using the em
  algorithm.
\newblock \emph{Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 28\penalty0 (1):\penalty0 20--28, 1979.

\bibitem[DAndrea and ODwyer(2017)]{d2017can}
Rafael DAndrea and James~P. ODwyer.
\newblock Can editors save peer review from peer reviewers?
\newblock \emph{PloS One}, 12\penalty0 (10):\penalty0 e0186111, 2017.

\bibitem[Ebel(1951)]{ebel1951estimation}
Robert~L. Ebel.
\newblock Estimation of the reliability of ratings.
\newblock \emph{Psychometrika}, 16\penalty0 (4):\penalty0 407--424, 1951.

\bibitem[Feliciani et~al.(2019)Feliciani, Luo, Ma, Lucas, Squazzoni,
  Maru{\v{s}}i{\'c}, and Shankar]{feliciani2019scoping}
Thomas Feliciani, Junwen Luo, Lai Ma, Pablo Lucas, Flaminio Squazzoni, Ana
  Maru{\v{s}}i{\'c}, and Kalpana Shankar.
\newblock A scoping review of simulation models of peer review.
\newblock \emph{Scientometrics}, 121\penalty0 (1):\penalty0 555--594, 2019.

\bibitem[Gill and Sgroi(2012)]{gill2012optimal}
David Gill and Daniel Sgroi.
\newblock The optimal choice of pre-launch reviewer.
\newblock \emph{Journal of Economic Theory}, 147\penalty0 (3):\penalty0
  1247--1260, 2012.

\bibitem[Herron(2012)]{herron2012expert}
Daniel~M. Herron.
\newblock Is expert peer review obsolete? a model suggests that
  post-publication reader review may exceed the accuracy of traditional peer
  review.
\newblock \emph{Surgical Endoscopy}, 26\penalty0 (8):\penalty0 2275--2280,
  2012.

\bibitem[Jecmen et~al.(2020)Jecmen, Zhang, Liu, Shah, Conitzer, and
  Fang]{jecmen2020mitigating}
Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar~B. Shah, Vincent Conitzer, and Fei
  Fang.
\newblock Mitigating manipulation in peer review via randomized reviewer
  assignments.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/93fb39474c51b8a82a68413e2a5ae17a-Abstract.html}.

\bibitem[Kannan et~al.(2021)Kannan, Niu, Roth, and Vohra]{kannan2021best}
Sampath Kannan, Mingzi Niu, Aaron Roth, and Rakesh Vohra.
\newblock Best vs.~all: Equity and accuracy of standardized test score
  reporting.
\newblock arXiv preprint arXiv:2102.07809, 2021.

\bibitem[Karlin and Rubin(1956{\natexlab{a}})]{karlin1956theory}
Samuel Karlin and Herman Rubin.
\newblock The theory of decision procedures for distributions with monotone
  likelihood ratio.
\newblock \emph{The Annals of Mathematical Statistics}, pages 272--299,
  1956{\natexlab{a}}.

\bibitem[Karlin and Rubin(1956{\natexlab{b}})]{karlin:rubin}
Samuel Karlin and Herman Rubin.
\newblock Distributions posessing a monotone likelihood ratio.
\newblock \emph{Journal of the American Statistical Association}, 51\penalty0
  (276):\penalty0 637--643, 1956{\natexlab{b}}.

\bibitem[Keener(2010)]{keener2010theoretical}
Robert~W Keener.
\newblock \emph{Theoretical statistics: Topics for a core course}.
\newblock Springer, 2010.

\bibitem[Kovanis et~al.(2016)Kovanis, Porcher, Ravaud, and
  Trinquart]{kovanis2016complex}
Michail Kovanis, Rapha{\"e}l Porcher, Philippe Ravaud, and Ludovic Trinquart.
\newblock Complex systems approach to scientific publication and peer-review
  system: development of an agent-based model calibrated with empirical journal
  data.
\newblock \emph{Scientometrics}, 106\penalty0 (2):\penalty0 695--715, 2016.

\bibitem[Kovanis et~al.(2017)Kovanis, Trinquart, Ravaud, and
  Porcher]{kovanis2017evaluating}
Michail Kovanis, Ludovic Trinquart, Philippe Ravaud, and Rapha{\"e}l Porcher.
\newblock Evaluating alternative systems of peer review: a large-scale
  agent-based modelling approach to scientific publication.
\newblock \emph{Scientometrics}, 113\penalty0 (1):\penalty0 651--671, 2017.

\bibitem[Lerner and Tirole(2006)]{lerner2006model}
Josh Lerner and Jean Tirole.
\newblock A model of forum shopping.
\newblock \emph{American economic review}, 96\penalty0 (4):\penalty0
  1091--1113, 2006.

\bibitem[Milgrom(1981)]{milgrom1981good}
Paul~R Milgrom.
\newblock Good news and bad news: Representation theorems and applications.
\newblock \emph{The Bell Journal of Economics}, pages 380--391, 1981.

\bibitem[Neff and Olden(2006)]{neff2006peer}
Bryan~D. Neff and Julian~D. Olden.
\newblock Is peer review a game of chance?
\newblock \emph{BioScience}, 56\penalty0 (4):\penalty0 333--340, 2006.

\bibitem[Noothigattu et~al.(2018)Noothigattu, Shah, and
  Procaccia]{noothigattu2018choosing}
Ritesh Noothigattu, Nihar~B. Shah, and Ariel Procaccia.
\newblock Choosing how to choose papers.
\newblock arXiv preprint arxiv:1808.09057, 2018.

\bibitem[Roebber and Schultz(2011)]{Roebber2011PeerReview}
Paul~J. Roebber and David~M. Schultz.
\newblock Peer review, program officers and science funding.
\newblock \emph{PLoS ONE}, 6\penalty0 (4):\penalty0 e18680, 2011.

\bibitem[Rogers and Augenstein(2020)]{rogers2020can}
Anna Rogers and Isabelle Augenstein.
\newblock What can we do to improve peer review in nlp?
\newblock In Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Findings of
  the Association for Computational Linguistics: {EMNLP} 2020, Online Event,
  16-20 November 2020}, volume {EMNLP} 2020 of \emph{Findings of {ACL}}, pages
  1256--1262. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.findings-emnlp.112}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.findings-emnlp.112}.

\bibitem[Shah et~al.(2018)Shah, Tabibian, Muandet, Guyon, and
  Von~Luxburg]{shah2018design}
Nihar~B. Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon, and Ulrike
  Von~Luxburg.
\newblock Design and analysis of the nips 2016 review process.
\newblock \emph{Journal of Machine Learning Research}, 19:\penalty0 1--49,
  2018.

\bibitem[Shaked and Shanthikumar(2007)]{shaked2007stochastic}
M.~Shaked and J.G. Shanthikumar.
\newblock \emph{Stochastic Orders}.
\newblock Springer Series in Statistics. Springer New York, 2007.
\newblock ISBN 9780387346755.
\newblock URL \url{https://books.google.com/books?id=rPiToBK2rwwC}.

\bibitem[Smith and Wilson(2021)]{smith2021accept}
Lones Smith and Andrea Wilson.
\newblock Accept this paper.
\newblock Manuscript in preparation. Slide deck available at
  https://econ.la.psu.edu/events/seminar-documents/accept2021.pdf, 2021.

\bibitem[Squazzoni and Gandelli(2012)]{squazzoni2012saint}
Flaminio Squazzoni and Claudio Gandelli.
\newblock Saint matthew strikes again: An agent-based model of peer review and
  the scientific community structure.
\newblock \emph{Journal of Informetrics}, 6\penalty0 (2):\penalty0 265--275,
  2012.

\bibitem[Srinivasan and Morgenstern(2021)]{srinivasan2021auctions}
Siddarth Srinivasan and Jamie Morgenstern.
\newblock Auctions and prediction markets for scientific peer review.
\newblock arXiv preprint arXiv:2109.00923, 2021.

\bibitem[Su(2021)]{su2021best}
Weijie~J. Su.
\newblock You are the best reviewer of your own papers: An owner-assisted
  scoring mechanism, 2021.

\bibitem[Thurner and Hanel(2011)]{thurner2011peer}
Stefan Thurner and Rudolf Hanel.
\newblock Peer-review in a world with rational scientists: Toward selection of
  the average.
\newblock \emph{The European Physical Journal B}, 84\penalty0 (4):\penalty0
  707--711, 2011.

\bibitem[Torres(2005)]{torres2005multivariate}
Ricard Torres.
\newblock Multivariate monotone bayesian updating.
\newblock \emph{Available at SSRN 2167744}, 2005.

\bibitem[Tran et~al.(2020)Tran, Valtchanov, Ganapathy, Feng, Slud, Goldblum,
  and Goldstein]{tran2020open}
David Tran, Alex Valtchanov, Keshav Ganapathy, Raymond Feng, Eric Slud, Micah
  Goldblum, and Tom Goldstein.
\newblock An open review of openreview: A critical analysis of the machine
  learning conference review process.
\newblock arXiv preprint arXiv:2010.05137, 2020.

\bibitem[Wang and Shah(2019)]{wang2018your}
Jingyan Wang and Nihar~B. Shah.
\newblock Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in
  ratings.
\newblock In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew~E. Taylor,
  editors, \emph{Proceedings of the 18th International Conference on Autonomous
  Agents and MultiAgent Systems, {AAMAS} '19, Montreal, QC, Canada, May 13-17,
  2019}, pages 864--872. International Foundation for Autonomous Agents and
  Multiagent Systems, 2019.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3331778}.

\bibitem[Whitt(1979)]{whitt1979note}
Ward Whitt.
\newblock A note on the influence of the sample on the posterior distribution.
\newblock \emph{Journal of the American Statistical Association}, 74\penalty0
  (366a):\penalty0 424--426, 1979.

\bibitem[Zhang et~al.(2021)Zhang, Cheng, and Conitzer]{zhang2021classification}
Hanrui Zhang, Yu~Cheng, and Vincent Conitzer.
\newblock Classification with few tests through self-selection.
\newblock In \emph{Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
  {AAAI} 2021, Thirty-Third Conference on Innovative Applications of Artificial
  Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances in
  Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9, 2021},
  pages 5805--5812. {AAAI} Press, 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/16727}.

\end{thebibliography}


\newpage

\appendix

\section{Proof of Lemma~\ref{lem:monotone_expected_quality}}
\label{sec:FOSD-proof}
Here, we prove Lemma~\ref{lem:monotone_expected_quality}, restated here for convenience. 

\begin{rtheorem}{Lemma}{\ref{lem:monotone_expected_quality}}
Suppose $\RevSigVP$ and $\RevSigV$ are two vectors of informative signals that satisfy $\RevSigVP\ge \RevSigV$ component-wise, and the inequality is strict for at least one of the components. 
Then, $U(\RevSigVP)> U(\RevSigV)$ holds for any prior $\QualDist$.
\end{rtheorem}

\begin{proof}
Here, we provide a proof for the categorical model. It can be easily modified for the continuous model, and the result also is shown as Theorem~1 in \cite{torres2005multivariate}.
We show the result by induction on the size of the quality set $\QualSet$.

\emph{Base case:} We show that the inequality holds for any binary quality set $\QualSet=\SET{q_1, q_2}$ with $q_2>q_1$.  For $\RevSigV \in \SigSet^{\NumReviews}$, let $\gamma(\RevSigV)=\frac{\RevSigProb[q_2]{\RevSigV}}{\RevSigProb[q_1]{\RevSigV}}$ be the likelihood ratio. We first rewrite the ex-post expected quality:

\begin{align*}
    U(\RevSigV) & = \ExpectC{Q}{\RevSigV}\\
    &= \sum_q q\cdot \ProbC{Q=q}{\RevSigV}\\
    &= \sum_q q \cdot \frac{\Prob{q}\cdot \ProbC{\RevSigV}{Q=q}}{\sum_{q'}\Prob{q'} \cdot \ProbC{\RevSigV}{Q=q'}}\\
    &= q_1 \cdot \frac{\QualProb{q_1}\RevSigProb[q_1]{\RevSigV}}{\QualProb{q_1}\RevSigProb[q_1]{\RevSigV}+\QualProb{q_2}\RevSigProb[q_2]{\RevSigV}} + q_2\cdot \frac{\QualProb{q_2}\RevSigProb[q_2]{\RevSigV}}{\QualProb{q_1}\RevSigProb[q_1]{\RevSigV}+\QualProb{q_2}\RevSigProb[q_2]{\RevSigV}}\\
    &=  \frac{q_1 \cdot \QualProb{q_1}\RevSigProb[q_1]{\RevSigV} + q_2\cdot\QualProb{q_2}\RevSigProb[q_2]{\RevSigV}}{\QualProb{q_1}\RevSigProb[q_1]{\RevSigV}+\QualProb{q_2}\RevSigProb[q_2]{\RevSigV}}\\
    &= \frac{q_1\cdot\QualProb{q_1} + q_2\cdot\QualProb{q_2}\gamma(\RevSigV)}{\QualProb{q_1}+\QualProb{q_2}\gamma(\RevSigV)}.
\end{align*}
    
Now, we consider the difference $U(\RevSigVP) - U(\RevSigV)$, write it using a common denominator, and cancel common terms. Then, 
% \dkdeletecomment{we'd also need to specify the one strict inequality, no?}{because $\RevSigVP \ge \RevSigV$ component-wise,} 
by \cref{def:informative}, we obtain that $\gamma(\RevSigVP)>\gamma(\RevSigV)$, which we substitute:

\begin{align*}
    U(\RevSigVP) - U(\RevSigV)
    &=  \frac{\QualProb{q_1}\QualProb{q_2} \cdot \left(q_1\gamma(\RevSigV) + q_2\gamma(\RevSigVP) - q_1\gamma(\RevSigVP) - q_2\gamma(\RevSigV)\right)}{\left(\QualProb{q_1}+\QualProb{q_2}\gamma(\RevSigVP)\right) \cdot \left(\QualProb{q_1}+\QualProb{q_2}\gamma(\RevSigV)\right)}\\
    &= \frac{\QualProb{q_1}\QualProb{q_2} \cdot (\gamma(\RevSigVP) - \gamma(\RevSigV)) \cdot (q_2-q_1)}{\left(\QualProb{q_1}+\QualProb{q_2}\gamma(\RevSigVP)\right) \cdot \left(\QualProb{q_1}+\QualProb{q_2}\gamma(\RevSigV)\right)}\\
    &>0.
\end{align*}

\emph{Induction step: } We show that if the inequality holds for all categorical models with support size $|\QualSet|=n-1$, it also holds for categorical models with support size $|\QualSet|=n$. 
Let $\QualSet=\SET{q_1, q_2, \ldots, q_n}$ with $q_n>\cdots>q_1$. Using the same derivation as in the base case, 

\begin{align*}
    U(\RevSigVP) - U(\RevSigV)
    &= \frac{\left(\sum_{i=1}^n q_i\QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right) \cdot \left(\sum_{i=1}^n \QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right) - \left(\sum_{i=1}^n q_i\QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right) \cdot \left(\sum_{i=1}^n \QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right)}{\left(\sum_{i=1}^n \QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right) \cdot \left(\sum_{i=1}^n \QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right)}.
\end{align*}

Because the denominator is strictly positive, we now focus on showing that the numerator is as well.
We show that the difference between the actual numerator, and the version where the upper bound of each summation is $n-1$, is positive. Then, because the latter is positive by induction hypothesis, we will have shown the claim.

\begin{align*}
& \left( \left(\sum_{i=1}^{n} q_i\QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right) \cdot \left(\sum_{i=1}^{n} \QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right) - \left(\sum_{i=1}^{n} q_i\QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right)\left(\sum_{i=1}^{n} \QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right) \right)
\\ - & 
 \left( \left(\sum_{i=1}^{n-1} q_i\QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right) \cdot \left(\sum_{i=1}^{n-1} \QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right) - \left(\sum_{i=1}^{n-1} q_i\QualProb{q_i}\RevSigProb[q_i]{\RevSigV}\right)\left(\sum_{i=1}^{n-1} \QualProb{q_i}\RevSigProb[q_i]{\RevSigVP}\right) \right)
\\ = & 
 q_n \QualProb{q_n} \RevSigProb[q_{n}]{\RevSigVP} \cdot \left( \sum_{i=1}^{n-1} \QualProb{q_i}\RevSigProb[q_i]{\RevSigV} \right) 
 + \QualProb{q_n} \RevSigProb[q_n]{\RevSigV} \cdot \left( \sum_{i=1}^{n-1} q_i \QualProb{q_i}\RevSigProb[q_i]{\RevSigVP} \right)
\\ - & q_n \QualProb{q_n} \RevSigProb[q_n]{\RevSigV} \cdot \left( \sum_{i=1}^{n-1} \QualProb{q_i} \RevSigProb[q_i]{\RevSigVP} \right) 
- \QualProb{q_n} \RevSigProb[q_n]{\RevSigVP} \cdot \left(\sum_{i=1}^{n-1} q_i \QualProb{q_i}\RevSigProb[q_i]{\RevSigV} \right)
\\ = & 
\sum_{i=1}^{n-1} \QualProb{q_i} \QualProb{q_n} \cdot \left( \RevSigProb[q_n]{\RevSigVP}\RevSigProb[q_i]{\RevSigV} - \RevSigProb[q_n]{\RevSigV} \RevSigProb[q_i]{\RevSigVP} \right) \cdot \left( q_n - q_i \right).
\end{align*}


By \cref{def:informative}, $\RevSigProb[q_n]{\RevSigVP} \RevSigProb[q_i]{\RevSigV} > \RevSigProb[q_n]{\RevSigV}\RevSigProb[q_i]{\RevSigVP}$, proving that the difference is strictly positive. Thus, by induction hypothesis, the utility difference is strictly positive, completing the proof.
\end{proof}


\section{Proof of Proposition~\ref{prop.threshold_opt}}
Our proof will use the following theorem about uniformly most powerful tests in statistical testing.

\begin{theorem}[KarlinRubin Theorem (Theorem~12.9 of \cite{keener2010theoretical})] \label{thm.ump}  Consider two sets $\QualSet$ and $\SigSet\subseteq \R$ and a family of pdfs or pmfs $\{g(\REVSIG |\qual):\qual\in \QualSet\}$ on $\SigSet$ possessing the monotone likelihood property (\cref{def:informative}).  
For any $\theta\in \QualSet$, $\tau\in \R$, $r\in [0,1]$, and two (randomized) tests $h, h^*: \SigSet\to [0,1]$, if $h^*$ is a threshold function, i.e., of the form
$$h^*(\REVSIG) = \begin{cases} 1\text{ if }\REVSIG>\tau\\
r \text{ if }\REVSIG = \tau\\
0 \text{ if }\REVSIG<\tau\end{cases},$$ and 
$\sup_{\qual\in \Theta_0}\E_{\REVSIG\sim g(\cdot|\qual)}[h^*(\REVSIG)]\ge \sup_{\qual\in \Theta_0}\E_{\REVSIG\sim g(\cdot|\qual)}[h(\REVSIG)]$ with $\Theta_0:=\{\qual\in \QualSet: \qual\le \theta\}$, then 
$$\E_{\REVSIG\sim g(\cdot|\qual)}[h^*(\REVSIG)]\ge \E_{\REVSIG\sim g(\cdot|\qual)}[h(\REVSIG)]\text{, for all } \qual\in \QualSet\setminus \Theta_0.$$

We call $h^*$ \emph{uniformly most powerful} among all tests $h$ with 
$$\sup_{\qual\in \Theta_0}\E_{\REVSIG\sim g(\cdot|\qual)}[h^*(\REVSIG)]\ge \sup_{\qual\in \Theta_0}\E_{\REVSIG\sim g(\cdot|\qual)}[h(\REVSIG)].$$
\end{theorem}

\begin{extraproof}{Proposition~\ref{prop.threshold_opt}}

As discussed after \cref{prop:threshold-policy}, by assuming that authors break ties in favor of using threshold strategies, any conference quality that can be achieved with a monotone policy is $\int^\infty_\theta q\,dp(q)$ for some de facto threshold $\theta\in \R$, and the quality can be achieved with a threshold acceptance policy. 
Thus, it is sufficient to show that for any de facto threshold $\theta$ there exists a threshold policy that minimizes the review burden over all monotone policies.  

Given a de facto threshold $\theta$ and attractiveness $\rho$, by \cref{prop:de_facto}, a monotone policy with de facto threshold $\theta$ can be considered as
a statistic test on $\{\Qual\in \QualSet: \Qual\le \theta\}$ vs.~$\{\Qual\in \QualSet: \Qual> \theta\}$.
To ensure that authors do not submit papers of quality less than $\theta$, the false positive rate (accepting any paper of quality $q < \theta$) must be at most $1/\rho$. Subject to those constraints, the false negative rate (rejecting any paper of quality $q > \theta$) should be minimized.  By \cref{prop:threshold-policy}, there exists a threshold policy $\ACCMAP^*$ with de facto threshold $\theta$.
The KarlinRubin Theorem (as given in \cref{thm.ump}) states that when the observed signal (in our case: the review) has the monotone likelihood ratio property, the above threshold test (policy) $\ACCMAP^*$ is uniformly most powerful so that $\AccP{\ACCMAP^*}{q}\ge \AccP{\ACCMAP}{q}$ for all $q > \theta$.  
Thus, $\ACCMAP^*$ has the desired de facto threshold $\theta$ and minimizes the probability of false negatives (rejections) for all paper qualities that should be eventually accepted.
Thus, it minimizes the review burden.
\end{extraproof}

\section{Learning Parameters of the Categorical Model from Data}\label{sec:learning-parameters}

We set the parameters of our model based on the OpenReview datasets of submissions and reviews for ICLR 2020 \cite{iclr2020review} and ICLR 2021 \cite{iclr2021review}. The datasets contain about $1500$ and $2500$ submissions, respectively; typically, each submission is reviewed thrice, with scores in $\SigSet = \SET{0,1,\dots,9}$.
We apply the same learning algorithm to each of the two datasets separately, yielding two plausible parameter settings for evaluation.

\paragraph{The Number of Paper Quality Scores and Signals.}
While the set $\SigSet = \SET{0,1,\dots,9}$ of available review scores is known, the number (or set) of different paper qualities is not. Thus, our goal is to simultaneously learn the number of paper qualities, the paper quality distribution, and the distribution of reviewer signals conditioned on the paper's quality.

To do so, we exhaustively try all numbers $\QualSetSize$ of paper quality scores in $\SET{2, \ldots, 10}$; for each, we apply a variant of the EM algorithm described below. Once the EM algorithm has converged, we evaluate the likelihood of the learned model for the held-out test data, and retain the model(s) with the highest likelihood scores.

Given a choice of $\QualSetSize$, to learn the paper quality distribution $\QualDist$ and conditional review distribution $\RevSigDist$, we apply the EM algorithm (adding some noise in each iteration) and cross-validation to avoid overfitting \cite{dawid1979maximum}.

Specifically, by cross-validation, we first randomly divide the dataset into five subsets with approximately equal size. We choose one of the subsets as the test set while the remaining $80\%$ of data form the training set. This step is repeated five times: each time, a different one of the five subsets is used as the test set.
Given the training and test dataset, for each $\QualSetSize \in \SET{2,3,\ldots,10}$, we run the EM algorithm for $100$ iterations on the training set to estimate the quality of each paper and the confusion matrix for reviewers (i.e., the matrix of review score probabilities conditional on ground truth quality); the EM algorithm alternates between updating the quality distribution with fixed confusion matrix, and updating the confusion matrix with fixed quality distribution.
To avoid overfitting, we repeat the following steps in every iteration: given an estimated confusion matrix $\RevSigDist^{(k)}$ after the $k$-th iteration, we perturb $\RevSigDist^{(k)}$ with a small amount of noise so that each row $i$ becomes the convex combination $0.99 \cdot \RevSigDist[i]^{(k)}+0.01 \cdot \frac{1}{|\SigSet|}\mathbf{1}$;
 
here, $\frac{1}{|\SigSet|}\mathbf{1}\in \R^{|\SigSet|}$ is the uniform distribution on signals. After each iteration, we evaluate the likelihood of the test data given the trained model, i.e., $\QualDist$ and $\RevSigDist$. For every $\QualSetSize$, this gives us a sequence of models for each iteration. Finally, the model that corresponds to the greatest likelihood on the test data is selected for use.

To choose the value of $\QualSetSize$, we judge the learned model based on the likelihood averaged over the five times cross-validation. 
The paper quality space size $\QualSetSize$ that has the maximum averaged likelihood is selected. Finally, we output the paper quality distribution $\QualDist$ as well as the confusion matrix $\RevSigDist$ as the average of the learned parameters for each of the five runs with different test sets. 
We find that $\QualSetSize \in \SET{4,5}$ tends to fit the data well in both of the datasets. 

The resulting parameters are shown in \cref{tab:learned_para}.
For experiments in which the authors receive \emph{noisy} signals (instead of the ground truth quality), we set the confusion matrix for the authors $\AuthSigDist$ to be the same as the one for reviewers, $\RevSigDist$; this is because unfortunately, no data are available that show how authors evaluate their own papers.

In our work, we tested four models: $\QualSetSize=4, 5$ for each of the two ICLR datasets (see \cref{tab:learned_para}). In the paper, only the results for the model with $\QualSetSize=4$ and learned from the ICLR 2020 review data are presented. We note that all of our qualitative results hold for all of the learned models. The selection of $\QualSetSize=4$ is because a smaller number of categories makes visualization easier; the selection of the 2020 dataset is because the learned paper qualities contain two negative and two positive qualities, which is more balanced than the $\QualSet$ learned from the 2021 dataset (with three negative qualities and only one positive quality). 

In some of our experiments, we want to explicitly evaluate the impact of increasing the noise in reviews. To do so, we consider reviewer signal matrices which are a convex combination of the learned signal matrix $\RevSigDist$ with the uniform signal distribution $\frac{1}{|\SigSet|}\mathbf{1}$; this corresponds to a reviewer who assigns a uniformly random score with probability $1-\lambda_R$. The weight $\lambda_R \in [0,1]$ placed on the learned distribution $\RevSigDist$ then captures the quality of the signal. Similarly, $\lambda_A$ controls the weight of the confusion matrix of the authors' signal.

\paragraph{The Numerical Values of Paper Quality.} 
Next, we infer the values of the paper qualities, given the estimated parameters $\QualDistTilde, \RevSigDistTilde$ and the review scores $\bm{s}$. That is, given $\QualSetSize$, we want to learn a vector of values 
$(\qual_1,\ldots,\qual_\QualSetSize)$ of paper qualities. 

First, for each paper $i$ in the dataset, we take the average over the review scores, denoted by $\bar{s}_i$. Then, we set the value of the quality of paper $i$ as $\psi(\bar{s}_i)$, where $\psi: [0,9] \to \mathbb{R}$ is an increasing function that maps the average score to the quality of a paper. In our experiments, we set $\psi$ as a reversed (and shifted and scaled) sigmoid function, i.e., $\psi(x) = \log{\frac{x+0.01}{9.01-x}}$ for $x\in [0,9]$. We choose the reversed sigmoid function because it can assign ``convex'' weights on both very positive reviews and very negative reviews. Furthermore, it is parameterized by a small number of interpretable parameters so that it is not too complex to empirically set the parameters.

Next, given the ``artificial'' quality assigned to each paper in the dataset, we want to compute the average paper quality of each category. This requires us to learn a distribution of the category label for each paper. With the learned model $\QualDistTilde, \RevSigDistTilde$ and paper $i$'s review signals $\bm{s}_i$, we can infer this distribution based on Bayes' rule. Let $l_{i,k}=\ProbC{\text{paper } i\text{ belongs to the $k$th category}}{\bm{s}_i, \QualDistTilde, \RevSigDistTilde}$ for $k\in \{1, 2, \ldots, \QualSetSize\}$.
We can then set the paper quality as a weighted average
\begin{equation*} %\label{eq:paper_quality_average}
    \qual_k = \frac{\sum_i^n l_{i,k}\cdot \psi\left(\bar{s}_i\right)}{\sum_i^n l_{i,k}}.
\end{equation*}


\subsection{The Learned Parameters}

\cref{tab:learned_para} summarizes our parameters for the two datasets. For each dataset, we infer parameters for two sizes $L=4,5$ of the set of paper qualities.

{\small 
\begin{table}[htb]
\begin{tabular}{|c|c|c|c|}
\hline
   & $\QualDist$ & $\RevSigDist$ & $\QualSet$ \\ \hline

 ICLR 2020, $\QualSetSize = 4$ 
 & $\left[\begin{smallmatrix}\\\\  0.0772\\\\ 0.3987\\\\ 0.2648\\\\ 0.2593\\\\ \end{smallmatrix}\right] $    
 & $ \left[ \begin{smallmatrix}\\\\ 0.0400&  0.1706&  0.4200&  0.2729&  0.0685&  0.0028&  0.0247&  0.0002&  0.0002& 0.0001\\\\ 0.0004&  0.0123&  0.1195&  0.3816&  0.2948&  0.1288&  0.0450&  0.0119&  0.0056& 0.0001\\\\ 0.0001&  0.0043&  0.0226&  0.0959&  0.3235&  0.3626&  0.1648&  0.0188&  0.0073& 0.0001\\\\ 0.0001&  0.0016&  0.0090&  0.0302&  0.0534&  0.2922 & 0.4285 & 0.1427 & 0.0375& 0.0048\\\\ \end{smallmatrix}\right] $   
 & $ \left[ \begin{smallmatrix}\\\\  -1.1145\\\\ -0.4079\\\\  0.0544 \\\\ 0.5606 \\\\ \end{smallmatrix}\right] $        
 \\ \hline

ICLR 2020, $\QualSetSize = 5$ 
& $\left[\begin{smallmatrix}\\\\  0.0923\\\\ 0.282\\\\  0.25\\\\   0.206 \\\\ 0.1696\\\\ \end{smallmatrix}\right] $
& $\left[ \begin{smallmatrix}\\\\ 0.0340&  0.1538&  0.3981&  0.2865&  0.0928&  0.0085&  0.0257&  0.0002&  0.0003& 0.0001\\\\ 0.0003&  0.0077&  0.1214& 0.4345&  0.2495&  0.1266&  0.0415&  0.0153&  0.0031& 0.0001\\\\ 0.0001&  0.0079&  0.0523&  0.1603&  0.4388 & 0.2270 & 0.0918&  0.0184 & 0.0033& 0.0001\\\\ 0.0001&  0.0031&  0.0126 & 0.0668&  0.1180&  0.4503&  0.3010 & 0.0444&  0.0034& 0.0003\\\\ 0.0001&  0.0014&  0.0082&  0.0206&  0.0604 & 0.2216&  0.4446&  0.1823&  0.0535& 0.0073\\\\ \end{smallmatrix}\right]$  
& $\left[ \begin{smallmatrix}\\\\  -1.0552\\\\ -0.4337\\\\ -0.1378\\\\  0.2747\\\\  0.6454 \\\\ \end{smallmatrix}\right] $
\\ \hline

ICLR 2021, $\QualSetSize = 4$ 
& $ \left[\begin{smallmatrix}\\\\ 0.0267\\\\ 0.287\\\\  0.6152\\\\ 0.0711\\\\ \end{smallmatrix}\right] $
& $ \left[ \begin{smallmatrix}\\\\ 0.0228&  0.2288&  0.4211&  0.2415&  0.0639&  0.0201&  0.0003&  0.0010 & 0.0002& 0.0001\\\\ 0.0007&  0.0241 & 0.1568 & 0.3899 & 0.2712 & 0.1134 & 0.0316 & 0.0056 & 0.0063& 0.0003\\\\ 0.0008 & 0.0062 & 0.0583 & 0.1805 & 0.2783 & 0.2617&  0.1619 & 0.0450 & 0.0062& 0.0011\\\\ 0.0021 & 0.0031&  0.0070 & 0.0544 & 0.1173 & 0.2435 & 0.3725 & 0.1256 & 0.0701 & 0.0043\\\\ \end{smallmatrix}\right] $
& $ \left[ \begin{smallmatrix}\\\\  -1.1544\\\\ -0.4886\\\\ -0.0331\\\\  0.4927 \\\\ \end{smallmatrix}\right] $
\\ \hline

ICLR 2021, $\QualSetSize = 5$ 
& $ \left[\begin{smallmatrix}\\\\  0.0228\\\\ 0.2712\\\\ 0.306\\\\  0.3304\\\\ 0.0695 \end{smallmatrix}\right] $
& $ \left[ \begin{smallmatrix}\\\\ 0.0260 & 0.2320 & 0.4249 & 0.2337 & 0.0630 & 0.0183 & 0.0003 & 0.0014 & 0.0002 & 0.0001 \\\\ 0.0007 & 0.0267 & 0.1616&  0.3966 & 0.2672 & 0.1074 & 0.0285&  0.0050 & 0.0060& 0.0004\\\\ 0.0011 & 0.0049 & 0.0600 & 0.1957 & 0.3141& 0.2614 & 0.1253&  0.0301&  0.0063& 0.0011\\\\ 0.0006 & 0.0068 & 0.0578 & 0.1734 & 0.2435 & 0.2578 & 0.1947 & 0.0512 & 0.0128& 0.0015\\\\ 0.0026&  0.0037&  0.0038 & 0.0616 & 0.1390 & 0.2640 & 0.3416 & 0.1075 & 0.0718 &0.0046\\\\ \end{smallmatrix}\right] $ 
& $ \left[ \begin{smallmatrix}\\\\  -1.1795\\\\ -0.5077\\\\ -0.0937 \\\\ 0.0153 \\\\ 0.4641 \\\\ \end{smallmatrix}\right] $
\\ \hline
\end{tabular}
\caption{Parameters learned from the ICLR datasets.
The rows of the confusion matrix are ranked based on the expected scores (from low to high). That is, the $k$th row of the confusion matrix $\RevSigDist$ has lower average score than the ($k+1$)st row. Given this ranking, we observe that the value of paper qualities learned from our method is monotone increasing in $k$. \label{tab:learned_para}}
\end{table}
}


\end{document}