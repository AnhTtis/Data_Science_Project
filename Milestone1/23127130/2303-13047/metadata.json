{
    "arxiv_id": "2303.13047",
    "paper_title": "Towards Better Dynamic Graph Learning: New Architecture and Unified Library",
    "authors": [
        "Le Yu",
        "Leilei Sun",
        "Bowen Du",
        "Weifeng Lv"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-09-26"
    ],
    "latest_version": 2,
    "categories": [
        "cs.LG"
    ],
    "abstract": "We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning. DyGFormer is conceptually simple and only needs to learn from nodes' historical first-hop interactions by: (1) a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their historical sequences; (2) a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing exhaustive experiments on thirteen datasets for dynamic link prediction and dynamic node classification tasks, we find that DyGFormer achieves state-of-the-art performance on most of the datasets, demonstrating its effectiveness in capturing nodes' correlations and long-term temporal dependencies. Moreover, some results of baselines are inconsistent with previous reports, which may be caused by their diverse but less rigorous implementations, showing the importance of DyGLib. All the used resources are publicly available at https://github.com/yule-BUAA/DyGLib.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13047v1",
        "http://arxiv.org/pdf/2303.13047v2"
    ],
    "publication_venue": "Accepted at NeurIPS 2023"
}