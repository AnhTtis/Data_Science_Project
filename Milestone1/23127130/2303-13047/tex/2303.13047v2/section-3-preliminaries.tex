\section{Preliminaries}
\label{section-3}

\begin{definition}
    \textbf{Dynamic Graph}. We represent a dynamic graph as a sequence of non-decreasing chronological interactions $\mathcal{G}=\left\{\left(u_1,v_1,t_1\right), \left(u_2,v_2,t_2\right), \cdots \right\}$ with $0 \leq t_1 \leq t_2 \leq \cdots$, where $u_i, v_i \in \mathcal{N}$ denote the source node and destination node of the $i$-th link at timestamp $t_i$.
    % \footnote{In this paper, we mainly focus on link addition, which is a widely studied interaction type in previous research. We leave the investigations of other interaction types (e.g., node addition/deletion/feature transformations and link deletion/feature transformations) for future work.}. 
    $\mathcal{N}$ is the set of all the nodes. Each node $u \in \mathcal{N}$ can be associated with node feature $\bm{x}_u \in \mathbb{R}^{d_N}$, and each interaction $\left(u,v,t\right)$ has link feature $\bm{e}_{u,v}^t \in \mathbb{R}^{d_E}$. $d_N$ and $d_E$ denote the dimensions of the node feature and link feature. If the graph is non-attributed, we simply set the node feature and link feature to zero vectors, i.e., $\bm{x}_u=\bm{0}$ and $\bm{e}_{u,v}^t=\bm{0}$. 
\end{definition}

\begin{definition}
    \textbf{Problem Formalization}. Given the source node $u$, destination node $v$, timestamp $t$, and historical interactions before $t$, i.e., $\left\{\left(u^\prime,v^\prime,t^\prime\right) | t^\prime < t \right\}$, representation learning on dynamic graph aims to design a model to learn time-aware representations $\bm{h}_u^t \in \mathbb{R}^d$ and $\bm{h}_v^t \in \mathbb{R}^d$ for $u$ and $v$ with $d$ as the dimension. We validate the effectiveness of the learned representations via two common tasks in dynamic graph learning: (\romannumeral1) dynamic link prediction, which predicts whether $u$ and $v$ are connected at $t$; (\romannumeral2) dynamic node classification, which infers the state of $u$ or $v$ at $t$.
\end{definition}
