\section{Experiments}
\label{section-5}
In this section, we report the results of various approaches by using DyGLib. We show the superiority of DyGFormer over existing methods and also give an in-depth analysis of DyGFormer.

\subsection{Experimental Settings}
\textbf{Datasets and Baselines}. We experiment with thirteen datasets (Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote, and Contact), which are collected by \cite{poursafaei2022towards} and cover diverse domains. Details of the datasets are shown in \secref{section-appendix-descriptions_datasets}. We compare DyGFormer with eight popular continuous-time dynamic graph learning baselines that are based on graph convolutions, memory networks, random walks, and sequential models, including JODIE \cite{DBLP:conf/kdd/KumarZL19}, DyRep \cite{DBLP:conf/iclr/TrivediFBZ19}, TGAT \cite{DBLP:conf/iclr/XuRKKA20}, TGN \cite{DBLP:journals/corr/abs-2006-10637}, CAWN \cite{DBLP:conf/iclr/WangCLL021}, EdgeBank \cite{poursafaei2022towards}, TCL \cite{DBLP:journals/corr/abs-2105-07944}, and GraphMixer \cite{cong2023do}. We give the descriptions of baselines in \secref{section-appendix-descriptions_baselines}.

\textbf{Evaluation Tasks and Metrics}.
We follow \cite{DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/iclr/WangCLL021,poursafaei2022towards} to evaluate models for dynamic link prediction, which predicts the probability of a link occurring between two given nodes at a specific time. This task has two settings: the transductive setting aims to predict future links between nodes that are observed during training, and the inductive setting predicts future links between unseen nodes. We use a multi-layer perceptron to take the concatenated representations of two nodes as inputs and return the probability of a link as the output. Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. We adopt random (rnd), historical (hist), and inductive (ind) negative sampling strategies in \cite{poursafaei2022towards} for evaluation, where the latter two strategies are more challenging. Please refer to \cite{poursafaei2022towards} for more details. We also follow \cite{DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637} to conduct dynamic node classification, which estimates the state of a node in a given interaction at a specific time. A multi-layer perceptron is employed to map the node representations to the labels. We use AUC-ROC as the evaluation metric due to the label imbalance. For both tasks, we chronologically split each dataset with the ratio of 70\%/15\%/15\% for training/validation/testing.

\textbf{Model Configurations}. For baselines, in addition to following their official settings, we also perform an exhaustive grid search to find the optimal configurations of some critical hyperparameters for more reliable comparisons. As DyGFormer can access longer histories, we vary each node's input sequence length from 32 to 4096 by a factor of 2. To keep the computational complexity at a constant level that is irrelevant to the input length, we correspondingly increase the patch size from 1 to 128. Please see \secref{section-appendix-configurations} for the detailed configurations of different models.

\textbf{Implementation Details}. For both tasks, we optimize all models (i.e., excluding EdgeBank which has no trainable parameters) by Adam \cite{DBLP:journals/corr/KingmaB14} and use supervised binary cross-entropy loss as the objective function. We train the models for 100 epochs and use the early stopping strategy with a patience of 20. We select the model that achieves the best performance on the validation set for testing. We set the learning rate and batch size to 0.0001 and 200 for all the methods on all the datasets. We run the methods five times with seeds from 0 to 4 and report the average performance to eliminate deviations. Experiments are conducted on an Ubuntu machine equipped with one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz with 16 physical cores. The GPU device is NVIDIA Tesla T4 with 15 GB memory. 

\subsection{Performance Comparisons and Discussions}
\begin{table}[!htbp]
\centering
\caption{AP for transductive dynamic link prediction with random, historical, and inductive negative sampling strategies. NSS is the abbreviation of Negative Sampling Strategies.}
\label{tab:average_precision_transductive_dynamic_link_prediction}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{0.9mm}
{
\begin{tabular}{c|c|ccccccccc}
\hline
NSS                    & Datasets    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & EdgeBank     & TCL          & GraphMixer   & DyGFormer    \\ \hline
\multirow{14}{*}{rnd}  & Wikipedia   & 96.50 $\pm$ 0.14 & 94.86 $\pm$ 0.06 & 96.94 $\pm$ 0.06 & 98.45 $\pm$ 0.06 & \underline{98.76 $\pm$ 0.03} & 90.37 $\pm$ 0.00 & 96.47 $\pm$ 0.16 & 97.25 $\pm$ 0.03 & \textbf{99.03 $\pm$ 0.02} \\
                       & Reddit      & 98.31 $\pm$ 0.14 & 98.22 $\pm$ 0.04 & 98.52 $\pm$ 0.02 & 98.63 $\pm$ 0.06 & \underline{99.11 $\pm$ 0.01} & 94.86 $\pm$ 0.00 & 97.53 $\pm$ 0.02 & 97.31 $\pm$ 0.01 & \textbf{99.22 $\pm$ 0.01} \\
                       & MOOC        & 80.23 $\pm$ 2.44 & 81.97 $\pm$ 0.49 & 85.84 $\pm$ 0.15 & \textbf{89.15 $\pm$ 1.60} & 80.15 $\pm$ 0.25 & 57.97 $\pm$ 0.00 & 82.38 $\pm$ 0.24 & 82.78 $\pm$ 0.15 & \underline{87.52 $\pm$ 0.49} \\
                       & LastFM      & 70.85 $\pm$ 2.13 & 71.92 $\pm$ 2.21 & 73.42 $\pm$ 0.21 & 77.07 $\pm$ 3.97 & \underline{86.99 $\pm$ 0.06} & 79.29 $\pm$ 0.00 & 67.27 $\pm$ 2.16 & 75.61 $\pm$ 0.24 & \textbf{93.00 $\pm$ 0.12} \\
                       & Enron       & 84.77 $\pm$ 0.30 & 82.38 $\pm$ 3.36 & 71.12 $\pm$ 0.97 & 86.53 $\pm$ 1.11 & \underline{89.56 $\pm$ 0.09} & 83.53 $\pm$ 0.00 & 79.70 $\pm$ 0.71 & 82.25 $\pm$ 0.16 & \textbf{92.47 $\pm$ 0.12} \\
                       & Social Evo. & 89.89 $\pm$ 0.55 & 88.87 $\pm$ 0.30 & 93.16 $\pm$ 0.17 & \underline{93.57 $\pm$ 0.17} & 84.96 $\pm$ 0.09 & 74.95 $\pm$ 0.00 & 93.13 $\pm$ 0.16 & 93.37 $\pm$ 0.07 & \textbf{94.73 $\pm$ 0.01} \\
                       & UCI         & 89.43 $\pm$ 1.09 & 65.14 $\pm$ 2.30 & 79.63 $\pm$ 0.70 & 92.34 $\pm$ 1.04 & \underline{95.18 $\pm$ 0.06} & 76.20 $\pm$ 0.00 & 89.57 $\pm$ 1.63 & 93.25 $\pm$ 0.57 & \textbf{95.79 $\pm$ 0.17} \\
                       & Flights     & 95.60 $\pm$ 1.73 & 95.29 $\pm$ 0.72 & 94.03 $\pm$ 0.18 & 97.95 $\pm$ 0.14 & \underline{98.51 $\pm$ 0.01} & 89.35 $\pm$ 0.00 & 91.23 $\pm$ 0.02 & 90.99 $\pm$ 0.05 & \textbf{98.91 $\pm$ 0.01} \\
                       & Can. Parl.  & 69.26 $\pm$ 0.31 & 66.54 $\pm$ 2.76 & 70.73 $\pm$ 0.72 & 70.88 $\pm$ 2.34 & 69.82 $\pm$ 2.34 & 64.55 $\pm$ 0.00 & 68.67 $\pm$ 2.67 & \underline{77.04 $\pm$ 0.46} & \textbf{97.36 $\pm$ 0.45} \\
                       & US Legis.   & 75.05 $\pm$ 1.52 & \underline{75.34 $\pm$ 0.39} & 68.52 $\pm$ 3.16 & \textbf{75.99 $\pm$ 0.58} & 70.58 $\pm$ 0.48 & 58.39 $\pm$ 0.00 & 69.59 $\pm$ 0.48 & 70.74 $\pm$ 1.02 & 71.11 $\pm$ 0.59 \\
                       & UN Trade    & 64.94 $\pm$ 0.31 & 63.21 $\pm$ 0.93 & 61.47 $\pm$ 0.18 & 65.03 $\pm$ 1.37 & \underline{65.39 $\pm$ 0.12} & 60.41 $\pm$ 0.00 & 62.21 $\pm$ 0.03 & 62.61 $\pm$ 0.27 & \textbf{66.46 $\pm$ 1.29} \\
                       & UN Vote     & \underline{63.91 $\pm$ 0.81} & 62.81 $\pm$ 0.80 & 52.21 $\pm$ 0.98 & \textbf{65.72 $\pm$ 2.17} & 52.84 $\pm$ 0.10 & 58.49 $\pm$ 0.00 & 51.90 $\pm$ 0.30 & 52.11 $\pm$ 0.16 & 55.55 $\pm$ 0.42 \\
                       & Contact     & 95.31 $\pm$ 1.33 & 95.98 $\pm$ 0.15 & 96.28 $\pm$ 0.09 & \underline{96.89 $\pm$ 0.56} & 90.26 $\pm$ 0.28 & 92.58 $\pm$ 0.00 & 92.44 $\pm$ 0.12 & 91.92 $\pm$ 0.03 & \textbf{98.29 $\pm$ 0.01} \\ \cline{2-11} 
                       & Avg. Rank   & 5.08         & 5.85         & 5.69         & \underline{2.54}         & 4.31         & 7.54         & 6.92         & 5.46         & \textbf{1.62}         \\ \hline
\multirow{14}{*}{hist} & Wikipedia   & 83.01 $\pm$ 0.66 & 79.93 $\pm$ 0.56 & 87.38 $\pm$ 0.22 & 86.86 $\pm$ 0.33 & 71.21 $\pm$ 1.67 & 73.35 $\pm$ 0.00 & \underline{89.05 $\pm$ 0.39} & \textbf{90.90 $\pm$ 0.10} & 82.23 $\pm$ 2.54 \\
                       & Reddit      & 80.03 $\pm$ 0.36 & 79.83 $\pm$ 0.31 & 79.55 $\pm$ 0.20 & \underline{81.22 $\pm$ 0.61} & 80.82 $\pm$ 0.45 & 73.59 $\pm$ 0.00 & 77.14 $\pm$ 0.16 & 78.44 $\pm$ 0.18 & \textbf{81.57 $\pm$ 0.67} \\
                       & MOOC        & 78.94 $\pm$ 1.25 & 75.60 $\pm$ 1.12 & 82.19 $\pm$ 0.62 & \textbf{87.06 $\pm$ 1.93} & 74.05 $\pm$ 0.95 & 60.71 $\pm$ 0.00 & 77.06 $\pm$ 0.41 & 77.77 $\pm$ 0.92 & \underline{85.85 $\pm$ 0.66} \\
                       & LastFM      & 74.35 $\pm$ 3.81 & 74.92 $\pm$ 2.46 & 71.59 $\pm$ 0.24 & \underline{76.87 $\pm$ 4.64} & 69.86 $\pm$ 0.43 & 73.03 $\pm$ 0.00 & 59.30 $\pm$ 2.31 & 72.47 $\pm$ 0.49 & \textbf{81.57 $\pm$ 0.48} \\
                       & Enron       & 69.85 $\pm$ 2.70 & 71.19 $\pm$ 2.76 & 64.07 $\pm$ 1.05 & 73.91 $\pm$ 1.76 & 64.73 $\pm$ 0.36 & \underline{76.53 $\pm$ 0.00} & 70.66 $\pm$ 0.39 & \textbf{77.98 $\pm$ 0.92} & 75.63 $\pm$ 0.73 \\
                       & Social Evo. & 87.44 $\pm$ 6.78 & 93.29 $\pm$ 0.43 & \underline{95.01 $\pm$ 0.44} & 94.45 $\pm$ 0.56 & 85.53 $\pm$ 0.38 & 80.57 $\pm$ 0.00 & 94.74 $\pm$ 0.31 & 94.93 $\pm$ 0.31 & \textbf{97.38 $\pm$ 0.14} \\
                       & UCI         & 75.24 $\pm$ 5.80 & 55.10 $\pm$ 3.14 & 68.27 $\pm$ 1.37 & 80.43 $\pm$ 2.12 & 65.30 $\pm$ 0.43 & 65.50 $\pm$ 0.00 & 80.25 $\pm$ 2.74 & \textbf{84.11 $\pm$ 1.35} & \underline{82.17 $\pm$ 0.82} \\
                       & Flights     & 66.48 $\pm$ 2.59 & 67.61 $\pm$ 0.99 & \textbf{72.38 $\pm$ 0.18} & 66.70 $\pm$ 1.64 & 64.72 $\pm$ 0.97 & 70.53 $\pm$ 0.00 & 70.68 $\pm$ 0.24 & \underline{71.47 $\pm$ 0.26} & 66.59 $\pm$ 0.49 \\
                       & Can. Parl.  & 51.79 $\pm$ 0.63 & 63.31 $\pm$ 1.23 & 67.13 $\pm$ 0.84 & 68.42 $\pm$ 3.07 & 66.53 $\pm$ 2.77 & 63.84 $\pm$ 0.00 & 65.93 $\pm$ 3.00 & \underline{74.34 $\pm$ 0.87} & \textbf{97.00 $\pm$ 0.31} \\
                       & US Legis.   & 51.71 $\pm$ 5.76 & \textbf{86.88 $\pm$ 2.25} & 62.14 $\pm$ 6.60 & 74.00 $\pm$ 7.57 & 68.82 $\pm$ 8.23 & 63.22 $\pm$ 0.00 & 80.53 $\pm$ 3.95 & 81.65 $\pm$ 1.02 & \underline{85.30 $\pm$ 3.88} \\
                       & UN Trade    & 61.39 $\pm$ 1.83 & 59.19 $\pm$ 1.07 & 55.74 $\pm$ 0.91 & 58.44 $\pm$ 5.51 & 55.71 $\pm$ 0.38 & \textbf{81.32 $\pm$ 0.00} & 55.90 $\pm$ 1.17 & 57.05 $\pm$ 1.22 & \underline{64.41 $\pm$ 1.40} \\
                       & UN Vote     & \underline{70.02 $\pm$ 0.81} & 69.30 $\pm$ 1.12 & 52.96 $\pm$ 2.14 & 69.37 $\pm$ 3.93 & 51.26 $\pm$ 0.04 & \textbf{84.89 $\pm$ 0.00} & 52.30 $\pm$ 2.35 & 51.20 $\pm$ 1.60 & 60.84 $\pm$ 1.58 \\
                       & Contact     & 95.31 $\pm$ 2.13 & \underline{96.39 $\pm$ 0.20} & 96.05 $\pm$ 0.52 & 93.05 $\pm$ 2.35 & 84.16 $\pm$ 0.49 & 88.81 $\pm$ 0.00 & 93.86 $\pm$ 0.21 & 93.36 $\pm$ 0.41 & \textbf{97.57 $\pm$ 0.06} \\ \cline{2-11} 
                       & Avg. Rank   & 5.46         & 5.08         & 5.08         & \underline{3.85}         & 7.54         & 5.92         & 5.46         & 4.00         & \textbf{2.62}         \\ \hline
\multirow{14}{*}{ind}  & Wikipedia   & 75.65 $\pm$ 0.79 & 70.21 $\pm$ 1.58 & \underline{87.00 $\pm$ 0.16} & 85.62 $\pm$ 0.44 & 74.06 $\pm$ 2.62 & 80.63 $\pm$ 0.00 & 86.76 $\pm$ 0.72 & \textbf{88.59 $\pm$ 0.17} & 78.29 $\pm$ 5.38 \\
                       & Reddit      & 86.98 $\pm$ 0.16 & 86.30 $\pm$ 0.26 & 89.59 $\pm$ 0.24 & 88.10 $\pm$ 0.24 & \textbf{91.67 $\pm$ 0.24} & 85.48 $\pm$ 0.00 & 87.45 $\pm$ 0.29 & 85.26 $\pm$ 0.11 & \underline{91.11 $\pm$ 0.40} \\
                       & MOOC        & 65.23 $\pm$ 2.19 & 61.66 $\pm$ 0.95 & 75.95 $\pm$ 0.64 & \underline{77.50 $\pm$ 2.91} & 73.51 $\pm$ 0.94 & 49.43 $\pm$ 0.00 & 74.65 $\pm$ 0.54 & 74.27 $\pm$ 0.92 & \textbf{81.24 $\pm$ 0.69} \\
                       & LastFM      & 62.67 $\pm$ 4.49 & 64.41 $\pm$ 2.70 & 71.13 $\pm$ 0.17 & 65.95 $\pm$ 5.98 & 67.48 $\pm$ 0.77 & \textbf{75.49 $\pm$ 0.00} & 58.21 $\pm$ 0.89 & 68.12 $\pm$ 0.33 & \underline{73.97 $\pm$ 0.50} \\
                       & Enron       & 68.96 $\pm$ 0.98 & 67.79 $\pm$ 1.53 & 63.94 $\pm$ 1.36 & 70.89 $\pm$ 2.72 & \underline{75.15 $\pm$ 0.58} & 73.89 $\pm$ 0.00 & 71.29 $\pm$ 0.32 & 75.01 $\pm$ 0.79 & \textbf{77.41 $\pm$ 0.89} \\
                       & Social Evo. & 89.82 $\pm$ 4.11 & 93.28 $\pm$ 0.48 & 94.84 $\pm$ 0.44 & \underline{95.13 $\pm$ 0.56} & 88.32 $\pm$ 0.27 & 83.69 $\pm$ 0.00 & 94.90 $\pm$ 0.36 & 94.72 $\pm$ 0.33 & \textbf{97.68 $\pm$ 0.10} \\
                       & UCI         & 65.99 $\pm$ 1.40 & 54.79 $\pm$ 1.76 & 68.67 $\pm$ 0.84 & 70.94 $\pm$ 0.71 & 64.61 $\pm$ 0.48 & 57.43 $\pm$ 0.00 & \underline{76.01 $\pm$ 1.11} & \textbf{80.10 $\pm$ 0.51} & 72.25 $\pm$ 1.71 \\
                       & Flights     & 69.07 $\pm$ 4.02 & 70.57 $\pm$ 1.82 & \underline{75.48 $\pm$ 0.26} & 71.09 $\pm$ 2.72 & 69.18 $\pm$ 1.52 & \textbf{81.08 $\pm$ 0.00} & 74.62 $\pm$ 0.18 & 74.87 $\pm$ 0.21 & 70.92 $\pm$ 1.78 \\
                       & Can. Parl.  & 48.42 $\pm$ 0.66 & 58.61 $\pm$ 0.86 & 68.82 $\pm$ 1.21 & 65.34 $\pm$ 2.87 & 67.75 $\pm$ 1.00 & 62.16 $\pm$ 0.00 & 65.85 $\pm$ 1.75 & \underline{69.48 $\pm$ 0.63} & \textbf{95.44 $\pm$ 0.57} \\
                       & US Legis.   & 50.27 $\pm$ 5.13 & \textbf{83.44 $\pm$ 1.16} & 61.91 $\pm$ 5.82 & 67.57 $\pm$ 6.47 & 65.81 $\pm$ 8.52 & 64.74 $\pm$ 0.00 & 78.15 $\pm$ 3.34 & 79.63 $\pm$ 0.84 & \underline{81.25 $\pm$ 3.62} \\
                       & UN Trade    & 60.42 $\pm$ 1.48 & 60.19 $\pm$ 1.24 & 60.61 $\pm$ 1.24 & 61.04 $\pm$ 6.01 & \underline{62.54 $\pm$ 0.67} & \textbf{72.97 $\pm$ 0.00} & 61.06 $\pm$ 1.74 & 60.15 $\pm$ 1.29 & 55.79 $\pm$ 1.02 \\
                       & UN Vote     & \textbf{67.79 $\pm$ 1.46} & 67.53 $\pm$ 1.98 & 52.89 $\pm$ 1.61 & \underline{67.63 $\pm$ 2.67} & 52.19 $\pm$ 0.34 & 66.30 $\pm$ 0.00 & 50.62 $\pm$ 0.82 & 51.60 $\pm$ 0.73 & 51.91 $\pm$ 0.84 \\
                       & Contact     & 93.43 $\pm$ 1.78 & 94.18 $\pm$ 0.10 & \underline{94.35 $\pm$ 0.48} & 90.18 $\pm$ 3.28 & 89.31 $\pm$ 0.27 & 85.20 $\pm$ 0.00 & 91.35 $\pm$ 0.21 & 90.87 $\pm$ 0.35 & \textbf{94.75 $\pm$ 0.28} \\ \cline{2-11} 
                       & Avg. Rank   & 6.62         & 6.38         & \underline{4.15}         & 4.38         & 5.46         & 5.62         & 4.69         & 4.46         & \textbf{3.23}         \\ \hline
\end{tabular}
}
}
\end{table}
We report the performance of different methods on the AP metric for transductive dynamic link prediction with three negative sampling strategies in \tabref{tab:average_precision_transductive_dynamic_link_prediction}. The best and second-best results are emphasized by \textbf{bold} and \underline{underlined} fonts. Note that the results are multiplied by 100 for a better display layout. Please refer to \secref{section-appendix-numerical-auc-roc-performance-transductive-dynamic-link-prediction} and \secref{section-appendix-numerical-performance-inductive-dynamic-link-prediction} for the results of AP for inductive dynamic link prediction as well as AUC-ROC for transductive and inductive dynamic link prediction tasks. Since EdgeBank can be only evaluated for transductive dynamic link prediction, we do not show its performance under the inductive setting. 
From the results, we have two main observations. 

Firstly, DyGFormer outperforms existing methods in most cases and achieves an average rank of 2.49/2.69 on AP/AUC-ROC for transductive dynamic link prediction and 2.69/2.56 on AP/AUC-ROC for inductive dynamic link prediction across three negative sampling strategies. We summarize the superiority of DyGFormer in two aspects. (\romannumeral1) The neighbor co-occurrence encoding scheme enables DyGFormer to exploit the correlations of the source node and destination node, which are often informative for predicting their future links. (\romannumeral2) The patching technique allows DyGFormer to effectively access longer histories and capture long-term temporal dependencies. As shown in \tabref{tab:num_neighbors_configuration} in \secref{section-appendix-configurations}, the input sequence lengths of DyGFormer are much longer than those of the baselines on most of the datasets, indicating that DyGFormer can utilize longer sequences better.

Secondly, some of our findings of baselines differ from previous reports. For instance, the performance of some baselines can be significantly improved by properly setting some hyperparameters. Additionally, some methods would obtain worse results after we fix the problems or make adaptions in their implementations. More explanations can be found in \secref{section-appendix-inconsistent-observations}. These observations highlight the importance of rigorously evaluating different methods by a unified library and verify the necessity of introducing DyGLib to facilitate the development of dynamic graph learning.

We also report the results of dynamic node classification in \tabref{tab:auc_roc_dynamic_node_classification} in \secref{section-appendix-node-clasification}. We observe that DyGFormer obtains better performance than most baselines and achieves an impressive average rank of 2.50 among them, demonstrating the superiority of DyGFormer once again. 

\begin{figure}[!htbp] 
\centering
\begin{minipage}[c]{0.39\linewidth} 
\centering 
\captionof{table}{AP for TCL with NCoE.} 
\label{tab:tcl_generalizability_neighbor_co_occurrence_encoding} 
\setlength{\tabcolsep}{0.7mm}
{
\begin{tabular}{c|ccc}
\hline
Datasets    & TCL    & w/ NCoE & Improv. \\ \hline
Wikipedia   & 96.47 & 99.09      & 2.72\%  \\
Reddit      & 97.53 & 99.04      & 1.55\%  \\
MOOC        & 82.38 & 86.92      & 5.51\%  \\
LastFM      & 67.27 & 84.02      & 24.90\% \\
Enron       & 79.70 & 90.18      & 13.15\% \\
Social Evo. & 93.13 & 94.06      & 1.00\%  \\
UCI         & 89.57 & 94.69      & 5.72\%  \\
Flights     & 91.23 & 97.71      & 7.10\%  \\
Can. Parl.  & 68.67 & 69.34      & 0.98\%  \\
US Legis.   & 69.59 & 69.47      & -0.17\% \\
UN Trade    & 62.21 & 63.46      & 2.01\%  \\
UN Vote     & 51.90 & 51.52      & -0.73\% \\
Contact     & 92.44 & 97.98      & 5.99\%  \\ \hline
\end{tabular}
\vspace{-13pt}
}
\end{minipage} 
\hfill 
\begin{minipage}[c]{0.6\linewidth} 
\centering 
    \vspace{20pt}
    \includegraphics[width=1.0\columnwidth]{figures/methods_varying_input_lengths.jpg}
    \captionof{figure}{Performance of different methods on LastFM and Can. Parl. with varying input lengths.}
    \label{fig:methods_varying_input_lengths}
\end{minipage} 
\end{figure}

\subsection{Generalizability of Neighbor Co-occurrence Encoding Scheme}\label{section-5-node_cooccurrence_generalizability}
Our \textbf{N}eighbor \textbf{Co}-occurrence \textbf{E}ncoding scheme (NCoE) is versatile and can be easily integrated with dynamic graph learning methods based on sequential models. Hence, we incorporate NCoE with TCL and GraphMixer and show their performance in \tabref{tab:tcl_generalizability_neighbor_co_occurrence_encoding} and \tabref{tab:complete_generalizability_neighbor_co_occurrence_encoding} in \secref{section-appendix-node-cooccurrence-generalizability}. We find TCL and GraphMixer usually yield better results with NCoE, achieving an average improvement of 5.36\% and 1.86\% over all datasets. This verifies the effectiveness and versatility of the neighbor co-occurrence encoding, and highlights the importance of capturing correlations between nodes. Also, as TCL and DyGFormer are built upon Transformer, TCL w/ NCoE can achieve similar results with DyGFormer on datasets that enjoy shorter input sequences (in which cases the patching technique in DyGFormer contributes little). However, when datasets exhibit more obvious long-term temporal dependencies (e.g., LastFM, Can. Parl.), the performance gaps become more significant.

\subsection{Advantages of Patching Technique}\label{section-5-investigation_patch_technique}
We validate the advantages of our patching technique in preserving the local temporal proximities and reducing the computational complexity, which helps DyGFormer effectively and efficiently utilize longer histories. We conduct experiments on LastFM and Can. Parl. since they can benefit from longer historical records. For baselines, we sample more neighbors or perform more causal anonymous walks (starting from 32) to make them access longer histories. The results are depicted in \figref{fig:methods_varying_input_lengths}, where the x-axis is represented by a logarithmic scale with base 2. We also plot the performance of baselines with the optimal length by unconnected points based on \tabref{tab:num_neighbors_configuration} in \secref{section-appendix-configurations}. Note that the results of some baselines are incomplete since they raise the out-of-memory error when the lengths are longer. For example, TGAT is only computationally feasible when extending the input length to 32, resulting in two discrete points with lengths 20 (the optimal length) and 32. 

From \figref{fig:methods_varying_input_lengths}, we conclude that: (\romannumeral1) most of the baselines perform worse when the input lengths become longer, indicating they lack the ability to capture long-term temporal dependencies; (\romannumeral2) the baselines usually encounter expensive computational costs when computing on longer histories. Although memory network-based methods (i.e., DyRep and TGN) can handle longer histories with affordable computational costs, they cannot benefit from longer histories due to the potential issues of vanishing or exploding gradients; (\romannumeral3) DyGFormer consistently achieves gains from longer sequences, demonstrating the advantages of the patching technique in leveraging longer histories. 

We also compare the running time and memory usage of DyGFormer with and without the patching technique during the training process. The results are shown in \tabref{tab:comparison_training_time_memory_usage_with_without_patching} in \secref{section-appendix-comparison-training-time-memory-usage}. We could observe that the patching technique efficiently reduces model training costs in both time and space, allowing DyGFormer to access longer histories. As the input sequence length increases, the reductions become more significant. Moreover, with the patching technique, we find that DyGFormer achieves an average improvement of 0.31\% and 0.74\% in performance on LastFM and Can. Parl. than DyGFormer without patching. This observation further demonstrates the advantage of our patching technique in leveraging the local temporal proximities for better results.

\subsection{Verification of the Motivation of Neighbor Co-occurrence Encoding Scheme}\label{section-appendix-verify-motivation-neighbor-co-occurrence-encoding}
To verify the motivation of NCoE (i.e., nodes with more common historical neighbors tend to interact in the future), we compare the performance of DyGFormer and DyGFormer without NCoE. We use TP, TN, FN, and FP to denote True Positive, True Negative, False Positive, and False Negative. Common Neighbor Ratio (CNR) is defined as the ratio of common neighbors in source node $u$’s sequence $S_u$ and destination node $v$’s sequence $S_v$, i.e., $|S_u \cap S_v|/|S_u \cup S_v|$. We focus on links whose predictions of DyGFormer w/o NCoE are changed by DyGFormer (i.e., FN→TP, FP→TN, TP→FN, and TN→FP). We define Changed Link Ratio (CLR) as the ratio of the changed links to their original set, which is respectively computed by $|$FN→TP$|/|$FN$|$, $|$FP→TN$|/|$FP$|$, $|$TP→FN$|/|$TP$|$, and $|$TN→FP$|/|$TN$|$. If NCoE is helpful, DyGFormer will revise more wrong predictions (more FN→TP and FP→TN) and make fewer incorrect changes (fewer TP→FN and TN→FP). We report CLR and average CNR of links in the above sets on five typical datasets in \tabref{tab:CLR_CNR_modification}.

\begin{table}[!htbp]
\centering
\caption{CLR and CNR of changes made by DyGFormer.}
\label{tab:CLR_CNR_modification}
\resizebox{0.97\textwidth}{!}
{
\setlength{\tabcolsep}{0.9mm}
{
\begin{tabular}{c|cccc|cccc}
\hline
\multirow{2}{*}{Datasets} & \multicolumn{4}{c|}{CLR (\%)}                & \multicolumn{4}{c}{CNR (\%)}                 \\ \cline{2-9} 
                          & FN→TP & FP→TN & TP→FN & TN→FP & FN→TP & FP→TN & TP→FN & TN→FP \\ \hline
Wikipedia                 & 68.36     & 72.73     & 1.68      & 1.69      & 18.16     & 0.01      & 0.10      & 2.49      \\
UCI                       & 71.45     & 94.11     & 7.29      & 1.82      & 19.08     & 2.49      & 3.35      & 13.02     \\
Flights                   & 83.66     & 83.83     & 1.73      & 2.11      & 37.09     & 2.28      & 7.06      & 20.28     \\
US Legis.                 & 31.63     & 23.67     & 6.63      & 1.59      & 69.92     & 62.13     & 61.14     & 63.80     \\
UN Vote                   & 44.02     & 36.46     & 28.95     & 30.53     & 78.57     & 81.39     & 80.86     & 77.02     \\ \hline
\end{tabular}
}
}
\end{table}

We find NCoE effectively helps DyGFormer rectify wrong predictions of DyGFormer w/o NCoE on datasets with \textit{significantly higher CNR of positive links than negative ones}, which happens with most datasets. Concretely, for Wikipedia, UCI, and Flights, their CNRs of FN→TP are much higher than FP→TN (e.g., 37.09\% vs. 2.28\% on Flights) and DyGFormer revises most wrong predictions of DyGFormer w/o NCoE (e.g., 83.66\% for positive links in FN and 83.83\% for negative links in FP on Flights). Corrections made by our encoding scheme are less obvious on datasets whose \textit{CNRs between positive and negative links are similar}, which occurs in only 2 of 13 datasets. For US Legis. and UN Vote, their CNRs between FN and FP are analogous (e.g., 69.92\% vs. 62.13\% on US Legis.), weakening the advantage of our neighbor co-occurrence encoding scheme (e.g., only 31.63\%/23.67\% of positive/negative links are corrected in FN/FP on US Legis.). Therefore, we conclude that the neighbor co-occurrence encoding scheme helps DyGFormer capture common historical neighbors in $S_u$ and $S_v$, and bring better results in most cases. 

\subsection{When Will DyGFormer Be a Good Choice?}\label{section-appendix-when-will-dygformer-good}
Note that DyGFormer is superior to baselines by 1) exploring the source and destination nodes’ correlations from their historical sequences by neighbor co-occurrence encoding scheme; 2) using the patching technique to attend longer histories. Thus, DyGFormer tends to perform better on datasets that favor these two designs. 
We define Link Ratio (LR) as the ratio of links in their corresponding positive or negative set, which can be computed by TP$/$(TP+FN), TN$/$(TN+FP), FN$/$(TP+FN), and FP$/$(TN+FP). As a method with more TP and TN (i.e., fewer FN and FP) is better, we report the results of LR and average CNR of links in TP and TN on five typical datasets in \tabref{tab:LR_CNR_TP_TN}.

\begin{table}[!htbp]
\begin{minipage}{0.45\linewidth} 
\centering 
\caption{LR and CNR of TP and TN with random negative sampling strategy.}
\label{tab:LR_CNR_TP_TN}
\setlength{\tabcolsep}{1.0mm}
{
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{Datasets} & \multicolumn{2}{c|}{LR (\%)} & \multicolumn{2}{c}{CNR (\%)} \\ \cline{2-5} 
                          & TP       & TN       & TP       & TN       \\ \hline
Wikipedia                 & 92.74      & 97.19      & 59.09      & 0.01       \\
UCI                       & 82.70      & 96.77      & 28.03      & 1.45       \\
Flights                   & 96.13      & 95.33      & 47.58      & 1.40     \\  
US Legis.                 & 78.95      & 56.83      & 75.18      & 53.98      \\
UN Vote                   & 65.18      & 45.43      & 56.24      & 76.02      \\ \hline
\end{tabular}
}
\end{minipage} 
\begin{minipage}{0.55\linewidth} 
\centering 
\caption{LR and CNR of FP under random, historical, and inductive negative sampling strategy.}
\label{tab:rnd_hist_ind_fp}
\setlength{\tabcolsep}{1.0mm}
{
\begin{tabular}{c|ccc|ccc}
\hline
\multirow{2}{*}{Datasets} & \multicolumn{3}{c|}{LR(\%)} & \multicolumn{3}{c}{CNR(\%)} \\ \cline{2-7} 
                          & rnd      & hist     & ind     & rnd      & hist     & ind     \\ \hline
Wikipedia                 & 2.81     & 89.28    & 94.53   & 0.02     & 14.00    & 11.66   \\
UCI                       & 3.23     & 64.93    & 76.42   & 9.98     & 12.22    & 13.81   \\
Flights                   & 4.67     & 94.52    & 92.94   & 0.01     & 35.62    & 30.29   \\
US Legis.                 & 43.17    & 17.31    & 21.21   & 79.40    & 87.51    & 75.51   \\
UN Vote                   & 54.57    & 39.90    & 52.92   & 79.60    & 75.53    & 79.15   \\ \hline
\end{tabular}
}
\end{minipage} 
\end{table}

We observe when \textit{CNR of TP is significantly higher than CNR of TN in the datasets, DyGFormer often outperforms baselines} (most datasets satisfy this property). For Wikipedia, UCI, and Flights, their CNRs of TP are much higher than those of TN (e.g., 59.09\% vs. 0.01\% on Wikipedia). Such a characteristic matches the motivation of our neighbor co-occurrence encoding scheme, enabling DyGFormer to correctly predict most links (e.g., 92.74\% of positive links and 97.19\% of negative links are properly predicted on Wikipedia). Moreover, as LastFM and Can. Parl. can gain from longer histories (see \figref{fig:methods_varying_input_lengths} and \tabref{tab:num_neighbors_configuration} in \secref{section-appendix-configurations}), DyGFormer is significantly better than baselines on these two datasets. When \textit{CNRs of TP and TN are less distinguishable in the datasets, DyGFormer may perform worse} (only 2 out of 13 datasets show this property). For US Legis., the CNRs between TP and TN are close (i.e., 75.18\% vs. 53.98\%), making DyGFormer worse than memory-based baselines (i.e., JODIE, DyRep, and TGN). For UN Vote, its CNR of TP is even lower than that of TN (i.e., 56.24\% vs. 76.02\%), which is opposite to our motivation, leading to poor results of DyGFormer than a few baselines. Since these two datasets cannot obviously gain from longer sequences either (see \tabref{tab:num_neighbors_configuration} in \secref{section-appendix-configurations}), DyGFormer obtains worse results on them. Thus, we conclude that for datasets with much higher CNR of TP than CNR of TN or datasets that can benefit from longer histories, DyGFormer is a good choice. Otherwise, we may need to try other methods.

\subsection{Why do DyGFormer's Performance Vary across Different Negative Sampling Strategies?}\label{section-appendix-varying-performance-DyGFormer-negative-sampling-strategies}
Compared with the random (rnd) strategy, historical (hist) and inductive (ind) strategies will sample previous links as negative ones. This makes previous positive links negative, which may hurt the performance DyGFormer since the motivation of our neighbor co-occurrence encoding scheme is violated. As positive links are identical among rnd, hist, and ind, we compute LR and the average CNR of links in FP and show results in \tabref{tab:rnd_hist_ind_fp}.

We find \textit{when hist or ind causes several magnitudes higher CNR of FP than rnd in the datasets, DyGFormer drops sharply}. For Wikipedia, UCI, and Flights, the CNRs of FP with hist/ind are much higher than rnd (e.g., 14.00\%/11.66\% vs. 0.02\% on Wikipedia). This misleads DyGFormer to predict negative links as positive and causes drops (e.g., 89.28\%/94.53\% of negative links are incorrectly predicted with hist/ind on Wikipedia, while only 2.81\% are wrong with rnd). We also note the drops in UCI are milder since the changes in CNR caused by hist or ind vs. rnd are less obvious than changes in Wikipedia and Flights. \textit{When changes in CNR of FP caused by hist or ind are not obvious in the datasets, DyGFormer is less affected}. Since hist/ind makes little changes in CNRs of FP on US Legis., we find it ranks second with hist/ind, which may indicate DyGFormer is less influenced by the neighbor co-occurrence encoding scheme and generalizes well to various negative sampling strategies. For UN Vote, although its CNRs of FP are not affected by hist and ind either, DyGFormer still performs worse due to its inferior performance with rnd. Hence, we deduce that our neighbor co-occurrence encoding may be sometimes fragile to various negative sampling strategies if its motivation is violated, leading to the varying performance of DyGFormer.

\subsection{Ablation Study}\label{section-5-ablation_study}
Finally, we validate the effectiveness of the neighbor co-occurrence encoding, time encoding, and mixing of the sequence of source node and destination node in DyGFormer. From \figref{fig:ablation_study} in \secref{section-appendix-ablation-study}, we observe that DyGFormer obtains the best performance when using all the components, and the results would be worse when any component is removed. This illustrates the necessity of each design in DyGFormer. Please refer to \secref{section-appendix-ablation-study} for detailed implementations and discussions.
