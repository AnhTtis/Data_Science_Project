\section{Related Work}
\label{section-2}

\textbf{Dynamic Graph Learning}.
Representation learning on dynamic graphs has been widely studied in recent years \cite{DBLP:journals/jmlr/KazemiGJKSFP20,skarding2021foundations,xue2022dynamic}. 
Discrete-time methods manually divide the dynamic graph into a sequence of snapshots and apply static graph learning methods on each snapshot, which ignore the temporal order of nodes in each snapshot \cite{DBLP:conf/aaai/ParejaDCMSKKSL20,DBLP:journals/kbs/GoyalCC20,DBLP:conf/wsdm/SankarWGZY20,DBLP:journals/corr/abs-2111-10447,DBLP:conf/kdd/YouDL22}. In contrast, continuous-time methods directly learn on the whole dynamic graph with temporal graph neural networks \cite{DBLP:conf/iclr/TrivediFBZ19,DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/cikm/ChangLW0FS020,DBLP:conf/sigmod/WangLLXYWWCYSG21}, memory networks \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/iclr/TrivediFBZ19,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/sigmod/WangLLXYWWCYSG21,luo2022neighborhoodaware}, temporal random walks \cite{DBLP:conf/iclr/WangCLL021,jin2022neural} or sequential models \cite{DBLP:journals/corr/abs-2105-07944,cong2023do}. Although insightful, most existing dynamic graph learning methods neglect the correlations between two nodes in an interaction. They also fail to handle nodes with longer interactions due to unaffordable computational costs of complex modules or issues in optimizing models (e.g., the vanishing/exploding gradients). In this paper, we propose a new Transformer-based architecture to show the necessity of capturing nodes' correlations and long-term temporal dependencies, which is achieved by two designs: a neighbor co-occurrence encoding scheme and a patching technique.

\textbf{Transformer-based Applications in Various Fields}. 
Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} is an innovative model that employs the self-attention mechanism to handle sequential data, which has been successfully applied in a variety of domains, such as natural language processing \cite{DBLP:conf/naacl/DevlinCLT19,DBLP:journals/corr/abs-1907-11692,DBLP:conf/nips/BrownMRSKDNSSAA20}, computer vision \cite{DBLP:conf/eccv/CarionMSUKZ20,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/iccv/LiuL00W0LG21} and time series forecasting \cite{DBLP:conf/nips/LiJXZCWY19,DBLP:conf/aaai/ZhouZPZLXZ21,DBLP:conf/nips/WuXWL21}. The idea of dividing the original data into patches as inputs of the Transformer has been attempted in some studies. ViT \cite{DBLP:conf/iclr/DosovitskiyB0WZ21} splits an image
into multiple patches and feeds the sequence of patches' linear embeddings into a Transformer, which achieves surprisingly good performance on image classification. PatchTST \cite{nie2023a} divides a time series into subseries-level patches and calculates the patches by a channel-independent Transformer for long-term multivariate time series forecasting. In this work, we propose a patching technique to learn on dynamic graphs, which can provide our approach with the ability to handle nodes with longer histories.

\textbf{Graph Learning Library}. Currently, there exist many libraries for static graphs \cite{DBLP:journals/corr/abs-1806-01261,DBLP:journals/corr/abs-1909-01315,DBLP:journals/corr/abs-1903-02428,DBLP:conf/nips/HuFZDRLCL20,DBLP:journals/corr/abs-2103-00959,DBLP:conf/icse/LiXCZL21,DBLP:journals/jmlr/LiuLWXYGYXZLYLF21}, but few for dynamic graph learning \cite{DBLP:journals/corr/abs-1811-10734,DBLP:conf/cikm/RozemberczkiSHP21,zhou2022tgl}. 
DynamicGEM \cite{DBLP:journals/corr/abs-1811-10734} focuses on dynamic graph embedding methods, which just consider the graph topology and cannot leverage node features. PyTorch Geometric Temporal \cite{DBLP:conf/cikm/RozemberczkiSHP21} implements discrete-time algorithms for spatiotemporal signal processing and is mainly applicable for nodes with aligned historical observations. TGL \cite{zhou2022tgl} trains on large-scale dynamic graphs with some engineering tricks. Though TGL has integrated some continuous-time methods, they are somewhat out-of-date, resulting in the lack of state-of-the-art models. Moreover, TGL is implemented by both PyTorch and C++, which needs additional compilation and increases the usage difficulty. In this paper, we present a unified continuous-time dynamic graph learning library with thorough baselines, diverse datasets, extensible implementations, and comprehensive evaluations to facilitate dynamic graph learning research.
