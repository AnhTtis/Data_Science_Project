\section{Introduction}
\label{section-1}

% Background
Dynamic graphs denote entities as nodes and represent their interactions as links with timestamps \cite{DBLP:journals/jmlr/KazemiGJKSFP20}, which can model many real-world scenarios such as social networks \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/wsdm/Song0WCZT19,alvarez2021evolutionary}, user-item interaction systems \cite{DBLP:conf/icdm/LiZWLWY20,DBLP:conf/cikm/FanLZX0Y21,DBLP:conf/www/YuWS0L22,zhang2022dynamic,yu2023continuous},
traffic networks \cite{DBLP:conf/ijcai/YuYZ18,DBLP:conf/ijcai/WuPLJZ19,DBLP:conf/aaai/GuoLFSW19,DBLP:conf/nips/0001YL0020,DBLP:journals/ijon/YuDHSHL21}, and physical systems \cite{DBLP:conf/nips/HuangS020,DBLP:conf/icml/Sanchez-Gonzalez20,DBLP:conf/iclr/PfaffFSB21}. In recent years, representation learning on dynamic graphs has become a trending research topic \cite{DBLP:journals/jmlr/KazemiGJKSFP20,skarding2021foundations,xue2022dynamic}. There are two main categories of existing methods: discrete-time \cite{DBLP:conf/aaai/ParejaDCMSKKSL20,DBLP:journals/kbs/GoyalCC20,DBLP:conf/wsdm/SankarWGZY20,DBLP:journals/corr/abs-2111-10447,DBLP:conf/kdd/YouDL22} and continuous-time \cite{DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/iclr/WangCLL021,DBLP:conf/nips/SouzaMKG22,cong2023do}. In this paper, we focus on the latter approaches because they are more flexible and effective than the formers and are being increasingly investigated.

% Issues of existing methods for dynamic graph learning
Despite the rapid development of dynamic graph learning methods, they still suffer from two limitations. Firstly, most of them independently compute the temporal representations of nodes in an interaction without exploiting nodes' correlations, which are often indicative of future interactions. Moreover, existing methods follow the interaction-level learning paradigm and only work for nodes with fewer interactions. When nodes have longer histories, they require sampling strategies to truncate the interactions for feasible calculations of the computationally expensive modules like graph convolutions \cite{DBLP:conf/iclr/TrivediFBZ19,DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/cikm/ChangLW0FS020,DBLP:conf/sigmod/WangLLXYWWCYSG21}, temporal random walks \cite{DBLP:conf/iclr/WangCLL021,jin2022neural} and sequential models \cite{DBLP:journals/corr/abs-2105-07944,cong2023do}. Though some approaches use memory networks \cite{DBLP:journals/corr/WestonCB14,DBLP:conf/nips/SukhbaatarSWF15} to sequentially process interactions with affordable computational costs \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/iclr/TrivediFBZ19,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/sigmod/WangLLXYWWCYSG21,luo2022neighborhoodaware}, they are faced with the vanishing/exploding gradients due to the usage of recurrent neural networks \cite{DBLP:conf/icml/PascanuMB13,DBLP:journals/corr/abs-2105-07944}. Therefore, we conclude that \textit{previous methods lack the ability to capture either nodes' correlations or long-term temporal dependencies}. 

Secondly, the training pipelines of different methods are inconsistent and often lead to poor reproducibility. Also, existing methods are implemented by diverse frameworks (e.g., Pytorch \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19}, Tensorflow \cite{DBLP:conf/osdi/AbadiBCCDDDGIIK16}, DGL \cite{DBLP:journals/corr/abs-1909-01315}, PyG \cite{DBLP:journals/corr/abs-1903-02428}, C++), making it time-consuming and difficult for researchers to quickly understand the algorithms and further dive into the core of dynamic graph learning. Although there exist some libraries for dynamic graph learning \cite{DBLP:journals/corr/abs-1811-10734,DBLP:conf/cikm/RozemberczkiSHP21,zhou2022tgl}, they mainly focus on dynamic network embedding methods \cite{DBLP:journals/corr/abs-1811-10734}, discrete-time graph learning methods \cite{DBLP:conf/cikm/RozemberczkiSHP21}, or engineering techniques for training on large-scale dynamic graphs \cite{zhou2022tgl} (elaborated in \secref{section-2}). Currently, we find that \textit{there are still no standard tools for continuous-time dynamic graph learning}.

% Present work and contributions
In this paper, we aim to address the above drawbacks with two key technical contributions.

\textbf{We propose a new Transformer-based dynamic graph learning architecture (DyGFormer)}. DyGFormer is conceptually simple by solely learning from the sequences of nodesâ€™ historical first-hop interactions. To be specific, DyGFormer is designed with a neighbor co-occurrence encoding scheme, which encodes the appearing frequencies of each neighbor in the sequences of the source and destination nodes to explicitly explore their correlations. In order to capture long-term temporal dependencies, DyGFormer splits each node's sequence into multiple patches and feeds them to Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17}. This patching technique not only makes the model effectively benefit from longer histories via preserving local temporal proximities, but also efficiently reduces the computational complexity to a constant level that is irrelevant to the input sequence length.

% allows DyGFormer 

\textbf{We present a unified continuous-time dynamic graph learning library (DyGLib)}. DyGLib is an open-source toolkit with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies, aiming to foster standard, scalable, and reproducible dynamic graph learning research. DyGLib has integrated a variety of continuous-time dynamic graph learning methods as well as benchmark datasets from various domains. It trains all the methods via the same pipeline to eliminate the influence of different implementations and adopts a modularized design for developers to conveniently incorporate new datasets and algorithms based on their specific requirements. Moreover, DyGLib supports both dynamic link prediction and dynamic node classification tasks with exhaustive evaluating strategies to provide comprehensive comparisons of existing methods. 

To evaluate the model performance, we conduct extensive experiments based on DyGLib, including dynamic link prediction under transductive and inductive settings with three negative sampling strategies as well as dynamic node classification. From the results, we observe that: (\romannumeral1) DyGFormer outperforms existing methods on most datasets, demonstrating its superiority in capturing nodes' correlations and long-term temporal dependencies; (\romannumeral2) some findings of baselines are not in line with previous reports because of their varied pipelines and problematic implementations, which illustrates the necessity of introducing DyGLib.
We also provide an in-depth analysis of the neighbor co-occurrence encoding and patching technique for a better understanding of DyGFormer.
