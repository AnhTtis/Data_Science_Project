\section{New Architecture and Unified Library}
\label{section-4}

\subsection{DyGFormer: Transformer-based Architecture for Dynamic Graph Learning}
The framework of our DyGFormer is shown in \figref{fig:DyGFormer_framework}, which employs Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} as the backbone. Given an interaction $\left(u,v,t\right)$, we first extract historical first-hop interactions of source node $u$ and destination node $v$ before timestamp $t$ and obtain two interaction sequences $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$. Next, in addition to computing the encodings of neighbors, links, and time intervals for each sequence, we also encode the frequencies of every neighbor's appearances in both $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$ to exploit the correlations between $u$ and $v$, resulting in four encoding sequences for $u$/$v$ in total. Then, we divide each encoding sequence into multiple patches and feed all the patches into a Transformer for capturing long-term temporal dependencies. Finally, the outputs of the Transformer are averaged to derive time-aware representations of $u$ and $v$ at timestamp $t$ (i.e., $\bm{h}_u^t$ and $\bm{h}_v^t$), which can be applied in various downstream tasks like dynamic link prediction and dynamic node classification.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/DyGFormer_framework.jpg}
    \caption{Framework of the proposed model.}
    \label{fig:DyGFormer_framework}
\end{figure}


\textbf{Learning from Historical First-hop Interactions}. Unlike most previous methods that require nodes' historical interactions from multiple hops (e.g., DyRep \cite{DBLP:conf/iclr/TrivediFBZ19}, TGAT \cite{DBLP:conf/iclr/XuRKKA20}, TGN \cite{DBLP:journals/corr/abs-2006-10637}, CAWN \cite{DBLP:conf/iclr/WangCLL021}), we only learn from the sequences of nodes' historical first-hop interactions, turning the dynamic graph learning task into a simpler sequence learning problem. Mathematically, given an interaction $\left(u,v,t\right)$, for source node $u$ and destination node $v$, we obtain the sequences that involve first-hop interactions of $u$ and $v$ before timestamp $t$, which are denoted by $\mathcal{S}_u^t=\left\{\left(u,u^\prime,t^\prime\right) | t^\prime < t \right\} \cup \left\{\left(u^\prime,u,t^\prime\right) | t^\prime < t \right\}$ and $\mathcal{S}_v^t=\left\{\left(v,v^\prime,t^\prime\right) | t^\prime < t \right\} \cup \left\{\left(v^\prime,v,t^\prime\right) | t^\prime < t \right\}$, respectively.

\textbf{Encoding Neighbors, Links, and Time Intervals}. For source node $u$, we retrieve the features of involved neighbors and links in sequence $\mathcal{S}_u^t$ based on the given features to represent their encodings, which are denoted by $\bm{X}_{u,N}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_N}$ and $\bm{X}_{u,E}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_E}$. Following \cite{DBLP:conf/iclr/XuRKKA20}, we learn the periodic temporal patterns by encoding the time interval $\Delta t^\prime = t - t^\prime$ via $\sqrt{\frac{1}{d_T}}\left[\cos\left(w_1 \Delta t^\prime \right), \sin\left(w_1 \Delta t^\prime \right), \cdots, \cos\left(w_{d_T} \Delta t^\prime \right), \sin \left(w_{d_T} \Delta t^\prime \right)\right]$, where $w_1,\cdots,w_{d_T}$ are trainable parameters. $d_T$ is the encoding dimension. The time interval encodings of interactions in $\mathcal{S}_u^t$ is denoted by $\bm{X}_{u,T}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_T}$. We use the same process to get the corresponding encodings for destination node $v$, i.e., $\bm{X}_{v,N}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times d_N}$, $\bm{X}_{v,E}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times d_E}$, and $\bm{X}_{v,T}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times d_T}$.

\textbf{Neighbor Co-occurrence Encoding Scheme}. Existing methods separately compute representations of node $u$ and $v$ without modeling their correlations. We present a neighbor co-occurrence encoding scheme to tackle this issue, which assumes the appearing frequency of a neighbor in a sequence indicates its importance, and the occurrences of a neighbor in sequences of $u$ and $v$ (i.e., co-occurrence) could reflect the correlations between $u$ and $v$. That is to say, if $u$ and $v$ have more common historical neighbors in their sequences, they are more likely to interact in the future. 

Formally, for each neighbor in the interaction sequence $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$, we count its occurrences in both $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$, and derive a two-dimensional vector. By packing the vectors of all the neighbors together, we can get the neighbor co-occurrence features for $u$ and $v$, which are represented by $\bm{C}_{u}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times 2}$ and $\bm{C}_{v}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times 2}$. For example, suppose the historical neighbors of $u$ and $v$ are $\left\{a, b, a\right\}$ and $\left\{b, b, a, c\right\}$. The appearing frequencies of $a$, $b$, and $c$ in $u$/$v$'s historical interactions are 2/1, 1/2, and 0/1, respectively. Therefore, the neighbor co-occurrence features of $u$ and $v$ are denoted by $\bm{C}_u^t=\left[\left[2, 1\right], \left[1, 2\right],\left[2, 1\right]\right]^\top$ and $\bm{C}_v^t=\left[\left[1, 2\right], \left[1, 2\right],\left[2, 1\right],\left[0, 1\right]\right]^\top$. Then, we apply a function $f\left(\cdot\right)$ to encode the neighbor co-occurrence features by 
\begin{equation}
\label{equ:node_co_occurrence_encoding}
\begin{split}
   \bm{X}_{*,C}^t = f\left(\bm{C}_*^t\left[:,0\right]\right) + f\left(\bm{C}_*^t\left[:,1\right]\right) \in \mathbb{R}^{|\mathcal{S}_*^t| \times d_C},
\end{split}
\end{equation}
where $*$ could be $u$ or $v$. The input and output dimensions of $f\left(\cdot\right)$ are 1 and $d_C$. In this paper, we implement $f\left(\cdot\right)$ by a two-layer perceptron with ReLU activation \cite{DBLP:conf/icml/NairH10}.
It is important to note that the neighbor co-occurrence encoding scheme is general and can be easily integrated into some dynamic graph learning methods for better results. We will demonstrate its generalizability in \secref{section-5-node_cooccurrence_generalizability}. 

\textbf{Patching Technique}. Instead of focusing on the interaction level, we divide the encoding sequence into multiple non-overlapping patches to break through the bottleneck of existing methods in capturing long-term temporal dependencies. Let $P$ denote the patch size. Each patch is composed of $P$ temporally adjacent interactions with flattened encodings and can preserve local temporal proximities. Take the patching of $\bm{X}_{u,N}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_N}$ as an example. $\bm{X}_{u,N}^t$ will be divided into $l_u^t =\lceil \frac{|\mathcal{S}_u^t|}{P} \rceil$ patches in total (note that we will pad $\bm{X}_{u,N}^t$ if its length $|\mathcal{S}_u^t|$ cannot be divided by $P$), and the patched encoding is represented by $\bm{M}_{u,N}^t \in \mathbb{R}^{l_u^t \times d_N \cdot P}$. Similarly, we can also get the patched encodings $\bm{M}_{u,E}^t \in \mathbb{R}^{l_u^t  \times d_E \cdot P}$, $\bm{M}_{u,T}^t \in \mathbb{R}^{l_u^t \times d_T \cdot P}$, $\bm{M}_{u,C}^t \in \mathbb{R}^{l_u^t  \times d_C \cdot P}$, 
$\bm{M}_{v,N}^t \in \mathbb{R}^{l_v^t  \times d_N \cdot P}$, $\bm{M}_{v,E}^t \in \mathbb{R}^{l_v^t  \times d_E \cdot P}$, $\bm{M}_{v,T}^t \in \mathbb{R}^{l_v^t  \times d_T \cdot P}$, and $\bm{M}_{v,C}^t \in \mathbb{R}^{l_v^t  \times d_C \cdot P}$. Note that when $|\mathcal{S}_u^t|$ becomes longer, we will correspondingly increase $P$, making the number of patches (i.e., $l_u^t$ and $l_v^t$) at a constant level to reduce the computational cost.

\textbf{Transformer Encoder}. We first align the patched encodings to the same dimension $d$ with trainable weight $\bm{W}_* \in \mathbb{R}^{d_* \cdot P \times d}$ and $\bm{b}_* \in \mathbb{R}^{d}$ to obtain $\bm{Z}_{u,*}^t \in \mathbb{R}^{l_u^t \times d}$ and $\bm{Z}_{v,*}^t \in \mathbb{R}^{l_v^t \times d}$, where $*$ could be $N$, $E$, $T$ or $C$. To be specific, the alignments are realized by
\begin{equation}
    \label{equ:projection_layer}
    \bm{Z}_{u,*}^t = \bm{M}_{u,*}^t \bm{W}_* + \bm{b}_* \in \mathbb{R}^{l_u^t \times d}, \bm{Z}_{v,*}^t = \bm{M}_{v,*}^t \bm{W}_* + \bm{b}_* \in \mathbb{R}^{l_v^t \times d}.
\end{equation}
Then, we concatenate the aligned encodings of $u$ and $v$, and get $\bm{Z}_{u}^t = \bm{Z}_{u,N}^t \| \bm{Z}_{u,E}^t \| \bm{Z}_{u,T}^t \| \bm{Z}_{u,C}^t \in \mathbb{R}^{l_u^t \times 4d}$ and $\bm{Z}_{v}^t = \bm{Z}_{v,N}^t \| \bm{Z}_{v,E}^t \| \bm{Z}_{v,T}^t \| \bm{Z}_{v,C}^t \in \mathbb{R}^{l_v^t \times 4d}$. 

Next, we employ a Transformer encoder to capture the temporal dependencies, which is built by stacking $L$ Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) blocks. The residual connection \cite{DBLP:conf/cvpr/HeZRS16} is employed after every block. We follow \cite{DBLP:conf/iclr/DosovitskiyB0WZ21} by using GELU \cite{hendrycks2016gaussian} instead of ReLU \cite{DBLP:conf/icml/NairH10} between the two-layer perception in each FFN block and applying Layer Normalization (LN) \cite{ba2016layer} before each block rather than after.
% , which are slightly different from \cite{DBLP:conf/nips/VaswaniSPUJGKP17}. 
Instead of individually processing $\bm{Z}_{u}^t$ and $\bm{Z}_{v}^t$, our Transformer encoder takes the stacked $\bm{Z}^t=\left[\bm{Z}_{u}^t; \bm{Z}_{v}^t\right] \in \mathbb{R}^{(l_u^t + l_v^t) \times 4d}$ as inputs, aiming to learn the temporal dependencies within and across the sequences of $u$ and $v$. The calculation process is
% \begin{gather*} 
\begin{gather} 
\label{equ:transformer}
   \text{Attention}\left(\bm{Q}, \bm{K}, \bm{V}\right)= \text{Softmax}\left(\frac{\bm{Q} \bm{K}^\top}{\sqrt{d_k}}\right) \bm{V},\\
   \text{FFN}\left(\bm{O}, \bm{W}_1, \bm{b}_1, \bm{W}_2, \bm{b}_2\right)= \text{GELU}\left(\bm{O}\bm{W}_1+\bm{b}_1\right)\bm{W}_2 + \bm{b}_2,\\
   \bm{O}_i^{t,l} = \text{Attention}\left( \text{LN}(\bm{Z}^{t,l-1}) \bm{W}_{Q,i}^l, \text{LN}(\bm{Z}^{t,l-1}) \bm{W}_{K,i}^l, \text{LN}(\bm{Z}^{t,l-1}) \bm{W}_{V,i}^l \right),\\
   % \bm{O}^{t,l} = \text{CONCAT}\left(\bm{O}_1^{t,l},\cdots,\bm{O}_I^{t,l}\right)\bm{W}_O^l + \bm{Z}^{t,l-1},\\
   % \bm{O}^{t,l} = \left[\bm{O}_1^{t,l};\cdots;\bm{O}_I^{t,l}\right]\bm{W}_O^l + \bm{Z}^{t,l-1},\\
   \bm{O}^{t,l} = \text{MSA}\left(\bm{Z}^{t,l-1}\right) + \bm{Z}^{t,l-1} = \left(\bm{O}_1^{t,l} \| \cdots \| \bm{O}_I^{t,l}\right)\bm{W}_O^l + \bm{Z}^{t,l-1},\\
   % \bm{O}^{t,l} = \|_{i = 1}^I \bm{O}_i^{t,l} \bm{W}_O^l + \bm{Z}^{t,l-1},\\
   \bm{Z}^{t,l} = \text{FFN}\left(\text{LN}\left(\bm{O}^{t,l}\right), \bm{W}_1^l, \bm{b}_1^l, \bm{W}_2^l, \bm{b}_2^l\right) + \bm{O}^{t,l}.
\end{gather}
% \end{gather*}
$\bm{W}_{Q,i}^l \in \mathbb{R}^{4d \times d_k}$, $\bm{W}_{K,i}^l \in \mathbb{R}^{4d \times d_k}$, $\bm{W}_{V,i}^l \in \mathbb{R}^{4d \times d_v}$, $\bm{W}_O^l \in \mathbb{R}^{I \cdot d_v \times 4d}$, $\bm{W}_1^l \in \mathbb{R}^{4d \times 16d}$, $\bm{b}_1^l \in \mathbb{R}^{16d}$, $\bm{W}_2^l \in \mathbb{R}^{16d \times 4d}$ and $\bm{b}_2^l \in \mathbb{R}^{4d}$ are trainable parameters at the $l$-th layer. We set $d_k=d_v=4d/I$ with $I$ as the number of attention heads. The input of the first layer is $\bm{Z}^{t,0} = \bm{Z}^t \in \mathbb{R}^{(l_u^t + l_v^t) \times 4d}$, and the output of the $L$-th layer is denoted by $\bm{H}^t=\bm{Z}^{t,L} \in \mathbb{R}^{(l_u^t + l_v^t) \times 4d}$.

\textbf{Time-aware Node Representation}. The time-aware representations of node $u$ and $v$ at timestamp $t$ are derived by averaging their related representations in $\bm{H}^t$ with an output layer,
\begin{equation}
\label{equ:final_temporal_representation}
\begin{split}
   \bm{h}_u^{t} & = \text{MEAN}\left(\bm{H}^t[:l_u^t,:]\right) \bm{W}_{out} + \bm{b}_{out} \in \mathbb{R}^{d_{out}},\\
   \bm{h}_v^{t} & = \text{MEAN}\left(\bm{H}^t[l_u^t:l_u^t + l_v^t,:]\right) \bm{W}_{out} + \bm{b}_{out} \in \mathbb{R}^{d_{out}},\\
\end{split}
\end{equation}
where $\bm{W}_{out} \in \mathbb{R}^{4d \times d_{out}}$ and $\bm{b}_{out} \in \mathbb{R}^{d_{out}}$ are trainable weights with $d_{out}$ as the output dimension.

\subsection{DyGLib: Unified Library for Continuous-Time Dynamic Graph Learning}
We introduce a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies for reproducible, scalable, and credible continuous-time dynamic graph learning research. The overall procedure of DyGLib is shown in \figref{fig:procedure_DyGLib} in \secref{section-appendix-DyGLib-procedure}.


\textbf{Standard Training Pipelines}. To eliminate the influence of different training pipelines in previous studies, we unify the data format, create a customized data loader, and train all the methods with the same model trainers. Our standard training pipelines guarantee reproducible performance and enable users to quickly identify the key components of different models. Researchers only need to focus on designing the model architecture without considering other irrelevant implementation details.

\textbf{Extensible Coding Interfaces}. We provide extensible coding interfaces for the datasets and algorithms, which are all implemented by PyTorch. These scalable designs enable users to incorporate new datasets and popular models based on their specific requirements, which can significantly reduce the usage difficulty for beginners and allow experts to conveniently validate new ideas. Currently, DyGLib has integrated thirteen datasets from various domains and nine continuous-time dynamic graph learning methods. It is worth noticing that we also found some issues in previous implementations and have fixed them in DyGLib (see details in \secref{section-appendix-issues-existing-methods}).

\textbf{Comprehensive Evaluating Protocols}. DyGLib supports both transductive/inductive dynamic link prediction and dynamic node classification tasks. Most previous works evaluate their methods on the dynamic link prediction task with the random negative sampling strategy but a few models already reach saturation performance under such a strategy, making it hard to distinguish more advanced designs. For more reliable comparisons, we adopt three strategies (i.e., random, historical, and inductive negative sampling strategies) in \cite{poursafaei2022towards} to comprehensively evaluate the model performance.
