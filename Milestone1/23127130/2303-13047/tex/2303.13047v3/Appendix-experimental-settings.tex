\section{Detailed Experimental Settings}
% We illustrate the details of the experiments in this section.
\subsection{Descriptions of Datasets}\label{section-appendix-descriptions_datasets}
We use thirteen datasets collected by \cite{poursafaei2022towards} in the experiments, which are publicly available\footnote{\url{https://zenodo.org/record/7213796\#.Y1cO6y8r30o}}:
\begin{itemize}
    \item  \textbf{Wikipedia} is a bipartite interaction graph that contains the edits on Wikipedia pages over a month. Nodes represent users and pages, and links denote the editing behaviors with timestamps. Each link is associated with a 172-dimensional Linguistic Inquiry and Word Count (LIWC) feature \cite{pennebaker2001linguistic}. This dataset additionally contains dynamic labels that indicate whether users are temporarily banned from editing.

    \item  \textbf{Reddit} is bipartite and records the posts of users under subreddits during one month. Users and subreddits are nodes, and links are the timestamped posting requests. Each link has a 172-dimensional LIWC feature. This dataset also includes dynamic labels representing whether users are banned from posting.

    \item  \textbf{MOOC} is a bipartite interaction network of online sources, where nodes are students and course content units (e.g., videos and problem sets). Each link denotes a student's access behavior to a specific content unit and is assigned with a 4-dimensional feature.

    \item  \textbf{LastFM} is bipartite and consists of the information about which songs were listened to by which users over one month. Users and songs are nodes, and links denote the listening behaviors of users.

    \item  \textbf{Enron} records the email communications between employees of the ENRON energy corporation over three years.

    \item  \textbf{Social Evo.} is a mobile phone proximity network that monitors the daily activities of an entire undergraduate dormitory for a period of eight months, where each link has a 2-dimensional feature.

    \item  \textbf{UCI} is an online communication network, where nodes are university students and links are messages posted by students.
    
    \item  \textbf{Flights} is a dynamic flight network that displays the development of air traffic during the COVID-19 pandemic. Airports are represented by nodes and the tracked flights are denoted as links. Each link is associated with a weight, indicating the number of flights between two airports in a day.

    \item  \textbf{Can. Parl.} is a dynamic political network that records the interactions between Canadian Members of Parliament (MPs) from 2006 to 2019. Each node represents an MP from an electoral district and a link is created when two MPs both vote "yes" on a bill. The weight of each link refers to the number of times that one MP voted “yes” for another MP in a year. 
    
    \item  \textbf{US Legis.} is a senate co-sponsorship network that tracks social interactions between legislators in the US Senate. The weight of each link specifies the number of times two congresspersons have co-sponsored a bill in a given congress.

    \item  \textbf{UN Trade} contains the food and agriculture trade between 181 nations for more than 30 years. The weight of each link indicates the total sum of normalized agriculture import or export values between two particular countries.

    \item  \textbf{UN Vote} records roll-call votes in the United Nations General Assembly. If two nations both voted "yes" to an item, the weight of the link between them is increased by one.

    \item  \textbf{Contact} describes how the physical proximity evolves among about 700 university students over a month. Each student has a unique identifier and links denote that they are within close proximity to each other. Each link is associated with a weight, revealing the physical proximity between students.
\end{itemize}
We show the statistics of the datasets in \tabref{tab:data_statistics}, where \#N\&L Feat stands for the dimensions of node and link features. We notice a slight difference between the statistics of the Contact dataset reported in \cite{poursafaei2022towards} (which has 694 nodes and 2,426,280 links) and our own calculations, although both of them are computed based on the released dataset by \cite{poursafaei2022towards}. We ultimately report our statistics of the Contact dataset in this paper.



\begin{table}[!htbp]
\centering
\caption{Statistics of the datasets.}
\label{tab:data_statistics}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{0.45mm}
{
\begin{tabular}{c|cccccccc}
\hline
Datasets    & Domains     & \#Nodes & \#Links   & \#N\&L Feat & Bipartite & Duration  & Unique Steps & Time Granularity    \\ \hline
Wikipedia   & Social      & 9,227  & 157,474   & -- \& 172                & True      & 1 month    &  152,757      &  Unix timestamps   \\
Reddit      & Social      & 10,984 & 672,447   & -- \& 172                & True      & 1 month    &  669,065      &  Unix timestamps          \\
MOOC        & Interaction & 7,144  & 411,749   & -- \& 4                  & True      & 17 months    &  345,600      & Unix timestamps         \\
LastFM      & Interaction & 1,980  & 1,293,103 & -- \& --                 & True      & 1 month    &   1,283,614     & Unix timestamps           \\
Enron       & Social      & 184    & 125,235   & -- \& --                 & False     & 3 years    &   22,632     &    Unix timestamps        \\
Social Evo. & Proximity   & 74     & 2,099,519 & -- \& 2                  & False     & 8 months    &  565,932      &  Unix timestamps         \\
UCI         & Social      & 1,899  & 59,835    & -- \& --                 & False     & 196 days    &  58,911      &  Unix timestamps         \\
Flights     & Transport   & 13,169 & 1,927,145 & -- \& 1                  & False     & 4 months    &  122      &   days        \\
Can. Parl.  & Politics    & 734    & 74,478    & -- \& 1                  & False     & 14 years    &   14     &   years        \\
US Legis.   & Politics    & 225    & 60,396    & -- \& 1                  & False     & 12 congresses    &   12     &  congresses    \\
UN Trade    & Economics   & 255    & 507,497   & -- \& 1                  & False     & 32 years    &    32    &   years        \\
UN Vote     & Politics    & 201    & 1,035,742 & -- \& 1                  & False     & 72 years    &    72    &     years      \\
Contact     & Proximity   & 692    & 2,426,279 & -- \& 1                  & False     & 1 month    &    8,064    &   5 minutes         \\ \hline
\end{tabular}
}
}
\end{table}


\subsection{Descriptions of Baselines}\label{section-appendix-descriptions_baselines}
We select the following eight baselines:
\begin{itemize}
    \item  \textbf{JODIE} is designed for temporal bipartite networks of user-item interactions. It employs two coupled recurrent neural networks to update the states of users and items. A projection operation is introduced to learn the future representation trajectory of each user/item \cite{DBLP:conf/kdd/KumarZL19}.
    
    \item  \textbf{DyRep} proposes a recurrent architecture to update node states upon each interaction. It also includes a temporal-attentive aggregation module to consider the temporally evolving structural information in dynamic graphs \cite{DBLP:conf/iclr/TrivediFBZ19}.

    \item  \textbf{TGAT} computes the node representation by aggregating features from each node's temporal-topological neighbors based on the self-attention mechanism. It is also equipped with a time encoding function for capturing temporal patterns \cite{DBLP:conf/iclr/XuRKKA20}.
    
    \item  \textbf{TGN} maintains an evolving memory for each node and updates this memory when the node is observed in an interaction, which is achieved by the message function, message aggregator, and memory updater. An embedding module is leveraged to generate the temporal representations of nodes \cite{DBLP:journals/corr/abs-2006-10637}.

    \item  \textbf{CAWN} first extracts multiple causal anonymous walks for each node, which can explore the causality of network dynamics and generate relative node identities. Then, it utilizes recurrent neural networks to encode each walk and aggregates these walks to obtain the final node representation \cite{DBLP:conf/iclr/WangCLL021}. 

    \item  \textbf{EdgeBank} is a pure memory-based approach without trainable parameters for transductive dynamic link prediction. It stores the observed interactions in the memory unit and updates the memory through various strategies. An interaction will be predicted as positive if it was retained in the memory, and negative otherwise \cite{poursafaei2022towards}. The publication presents two updating strategies, but their official implementations include two more\footnote{\url{https://github.com/fpour/DGB/blob/main/EdgeBank/link_pred/edge_bank_baseline.py}}. To be specific, the four variants of EdgeBank are: EdgeBank$_\infty$ with unlimited memory to store all the observed edges; EdgeBank$_\text{tw-ts}$ and EdgeBank$_\text{tw-re}$, which only remember edges within a fixed-size time window from the immediate past. The window size of EdgeBank$_\text{tw-ts}$ is set to the duration of the test split, while EdgeBank$_\text{tw-re}$ makes it related to the time intervals of repeated edges; EdgeBank$_\text{th}$ that retains edges with appearing counts higher than a threshold. We experiment with all four variants and report the best performance among them.
    
    \item  \textbf{TCL} first generates each node’s interaction sequence by performing a breadth-first search algorithm on the temporal dependency interaction sub-graph. Then, it presents a graph transformer that considers both graph topology and temporal information to learn node representations. It also incorporates a cross-attention operation for modeling the inter-dependencies between two interaction nodes \cite{DBLP:journals/corr/abs-2105-07944}.

    \item  \textbf{GraphMixer} shows that a fixed time encoding function performs better than the trainable version. It incorporates the fixed function into a link encoder based on MLP-Mixer \cite{DBLP:conf/nips/TolstikhinHKBZU21} to learn from temporal links. A node encoder with neighbor mean-pooing is employed to summarize node features \cite{cong2023do}.
\end{itemize}


\subsection{Some Problematic Implementations in Baselines}\label{section-appendix-issues-existing-methods}
JODIE, DyRep, and TGN models are based on memory networks, and their implementations have designed the raw messages to avoid information leakage. However, they fail to store the raw messages when saving models because the raw messages are maintained in a dictionary and thus cannot be saved as model parameters\footnote{\url{https://github.com/twitter-research/tgn/blob/2aa295a1f182137a6ad56328b43cb3d8223cae77/train_self_supervised.py\#L302}}. Our DyGLib has addressed this issue by additionally saving the correct raw messages when saving models. In the official implementation of CAWN, the encoding of each causal anonymous walk is represented by the embedding at the last position of the walk\footnote{\url{https://github.com/snap-stanford/CAW/blob/f994ff2b2c29778e6250b6a9928fd9943e0163f7/module.py\#L1069}}. However, some walks are padded to support mini-batch training in practice, making the last position of these padded walks meaningless. To get the correct encoding, it is necessary to use the actual length of each walk. Moreover, there are issues with the nodeedge2idx dictionary computed by the get\_ts2idx function\footnote{\url{https://github.com/snap-stanford/CAW/blob/f994ff2b2c29778e6250b6a9928fd9943e0163f7/graph.py\#L79}} if multiple interactions simultaneously occur at the last timestamp. This would lead to information leakage since the model can potentially access more interactions for a given interaction even if they happen at the same time. Our DyGLib has fixed these problems as well. 

\subsection{Some Inconsistent Observations with Previous Reports}\label{section-appendix-inconsistent-observations}
In the experiments, we find the behaviors of baselines are inconsistent with their previous reports in some cases. We provide detailed illustrations and attribute these phenomena to the following reasons.

\textbf{Suboptimal Settings of Hyperparameters}. Some important hyperparameters in the baselines are not sufficiently fine-tuned, such as the dropout rate, the number of sampled neighbors, the number of random walks, the length of input sequences, and the neighbor sampling strategies. In this paper, we perform the grid search to find the best settings of these hyperparameters 
% and obtain the improved performance of many baselines. 
and observe that the performance of many baselines can be significantly improved by properly setting certain hyperparameters. Take TGAT for transductive dynamic link prediction with the random negative sampling strategy as an example (see \tabref{tab:average_precision_transductive_dynamic_link_prediction}). Compared with the performance in \cite{poursafaei2022towards}, the results of AP are significantly improved on datasets like MOOC (from 0.61 to 0.86), LastFM (from 0.50 to 0.73), Enron (from 0.59 to 0.71), Social Evo. (from 0.76 to 0.93), Flights (from 0.89 to 0.94), and Contact (from 0.58 to 0.96). Similar improvements can also be found on JODIE, DyRep, and TGN on several datasets. 

\textbf{Usage of Problematic Implementations}. Some previous studies utilize problematic implementations in their experiments (illustrated in \secref{section-appendix-issues-existing-methods}), and the reported results may not be rigorous. Therefore, some methods would obtain worse results after we fix the problems in their implementations. Take CAWN for transductive dynamic link prediction with the random negative sampling strategy as an example (see \tabref{tab:average_precision_transductive_dynamic_link_prediction}). Compared with the results in \cite{poursafaei2022towards}, the performance on the AP metric of CAWN drops sharply on datasets like LastFM (from 0.98 to 0.87), Can. Parl. (from 0.94 to 0.70), US Legis. (from 0.97 to 0.71), UN Trade (from 0.97 to 0.65), and UN Vote (from 0.82 to 0.53). This is because \cite{poursafaei2022towards} uses the problematic implementations of CAWN to conduct experiments and the results will sometimes become worse after fixing the issues.

\textbf{Adaptions for Evaluations}. There also exist some differences between GraphMixer's results in the original paper and our work because we modify its implementation to fit our evaluations. The original GraphMixer could only be evaluated for transductive dynamic link prediction. In this work, we remove the one-hot encoding of nodes in GraphMixer to adapt it to the inductive setting, which may lead to decreased performance in certain situations. Take the performance of GraphMixer for transductive dynamic link prediction with the random negative sampling strategy as an example (see \tabref{tab:average_precision_transductive_dynamic_link_prediction}). Compared with the results in \cite{cong2023do}, the performance of GraphMixer slightly drops on datasets like Wikipedia (from 0.9985 to 0.9725) and Reddit (from 0.9993 to 0.9731).

\subsection{Configurations of Different Methods}\label{section-appendix-configurations}
We first present the official settings of baselines as well as the configurations of DyGFormer. These configurations remain unchanged across all the datasets.
\begin{itemize}
    \item  \textbf{JODIE}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Memory updater: vanilla recurrent neural network
    \end{itemize}

    \item  \textbf{DyRep}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: vanilla recurrent neural network
    \end{itemize}
    
    \item  \textbf{TGAT}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 2
    \end{itemize}
    
    \item  \textbf{TGN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: gated recurrent unit \cite{DBLP:conf/ssst/ChoMBB14}
    \end{itemize}
    
    \item  \textbf{CAWN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of position encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads for encoding walks: 8
    \item Length of each walk (including the target node): 2
    \item Time scaling factor $\alpha$: 1e-6
    \end{itemize}
    
    \item  \textbf{TCL}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of depth encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads: 2
    \item Number of Transformer layers: 2
    \end{itemize}

    \item  \textbf{GraphMixer}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of MLP-Mixer layers: 2
    \item Time gap $T$: 2000
    \end{itemize}

    \item  \textbf{DyGFormer}:
    \begin{itemize}
    \item Dimension of time encoding $d_T$: 100
    \item Dimension of neighbor co-occurrence encoding $d_C$: 50
    \item Dimension of aligned encoding $d$: 50
    \item Dimension of output representation $d_{out}$: 172
    \item Number of attention heads $I$: 2
    \item Number of Transformer layers $L$: 2
    \end{itemize}
\end{itemize}

Then, we perform the grid search to find the best settings of some critical hyperparameters, where the searched ranges and related methods are shown in \tabref{tab:searched_ranges_related_methods}. It is worth noticing that DyGFormer can directly handle nodes with sequence lengths shorter than the defined length. When the sequence length exceeds the specified length, we select the most recent interactions up to the defined length. 
\begin{table}[!htbp]
\centering
\caption{Searched ranges of hyperparameters and the related methods.}
\label{tab:searched_ranges_related_methods}
\setlength{\tabcolsep}{2.0mm}
{
\begin{tabular}{c|cc}
\hline
Hyperparameters                                                             & Searched Ranges                                                                                                                        & Related Methods                                                                                      \\ \hline
Dropout Rate \cite{DBLP:journals/jmlr/SrivastavaHKSS14}                                                              & \begin{tabular}[c]{@{}c@{}}[0.0, 0.1, 0.2, 0.3, \\ 0.4, 0.5, 0.6]\end{tabular}                                                         & \begin{tabular}[c]{@{}c@{}}JODIE, DyRep, TGAT, TGN, CAWN, \\ TCL, GraphMixer, DyGFormer\end{tabular} \\
\begin{tabular}[c]{@{}c@{}}Number of \\ Sampled Neighbors\end{tabular}      & [10, 20, 30]                                                                                                                           & \begin{tabular}[c]{@{}c@{}}DyRep, TGAT, TGN, \\ TCL, GraphMixer\end{tabular}                         \\
\begin{tabular}[c]{@{}c@{}}Neighbor Sampling \\ Strategies\end{tabular}     & [uniform,recent]                                                                                                                       & \begin{tabular}[c]{@{}c@{}}DyRep, TGAT, TGN, \\ TCL, GraphMixer\end{tabular}                         \\
\begin{tabular}[c]{@{}c@{}}Number of Causal \\ Anonymous Walks\end{tabular} & [16, 32, 64, 128]                                                                                                                      & CAWN                                                                                                 \\
\begin{tabular}[c]{@{}c@{}}Memory Updating \\ Variants\end{tabular}       & \begin{tabular}[c]{@{}c@{}}[EdgeBank$_\infty$, EdgeBank$_\text{tw-ts}$, \\ EdgeBank$_\text{tw-re}$, EdgeBank$_\text{th}$]\end{tabular} & EdgeBank                                                                                             \\
\begin{tabular}[c]{@{}c@{}}Length of Input \\ Sequences \& \\ Patch Size\end{tabular} & \begin{tabular}[c]{@{}c@{}}[32 \& 1, 64 \& 2, 128 \& 4, \\ 256 \& 8, 512 \& 16, 1024 \& 32, \\ 2048 \& 64, 4096 \& 128]\end{tabular}   & DyGFormer                                                                                            \\ \hline
\end{tabular}
}
\end{table}

Finally, we show the hyperparameter settings of various methods that are determined by the grid search in \tabref{tab:dropout_configuration}, \tabref{tab:num_neighbors_configuration}, \tabref{tab:neighbors_sampling_configuration} and \tabref{tab:edgebank_configuration}.

\begin{table}[!htbp]
\centering
\caption{Configurations of the dropout rate of different methods.}
\label{tab:dropout_configuration}
\setlength{\tabcolsep}{2.0mm}
{
\begin{tabular}{c|cccccccc}
\hline
Datasets    & JODIE & DyRep & TGAT & TGN & CAWN & TCL & GraphMixer & DyGFormer \\ \hline
Wikipedia   & 0.1   & 0.1   & 0.1  & 0.1 & 0.1  & 0.1 & 0.5        & 0.1       \\
Reddit      & 0.1   & 0.1   & 0.1  & 0.1 & 0.1  & 0.1 & 0.5        & 0.2       \\
MOOC        & 0.2   & 0.0   & 0.1  & 0.2 & 0.1  & 0.1 & 0.4        & 0.1       \\
LastFM      & 0.3   & 0.0   & 0.1  & 0.3 & 0.1  & 0.1 & 0.0        & 0.1       \\
Enron       & 0.1   & 0.0   & 0.2  & 0.0 & 0.1  & 0.1 & 0.5        & 0.0       \\
Social Evo. & 0.1   & 0.1   & 0.1  & 0.0 & 0.1  & 0.0 & 0.3        & 0.1       \\
UCI         & 0.4   & 0.0   & 0.1  & 0.1 & 0.1  & 0.0 & 0.4        & 0.1       \\
Flights     & 0.1   & 0.1   & 0.1  & 0.1 & 0.1  & 0.1 & 0.2        & 0.1       \\
Can. Parl.  & 0.0   & 0.0   & 0.2  & 0.3 & 0.0  & 0.2 & 0.2        & 0.1       \\
US Legis.   & 0.2   & 0.0   & 0.1  & 0.1 & 0.1  & 0.3 & 0.4        & 0.0       \\
UN Trade    & 0.4   & 0.1   & 0.1  & 0.2 & 0.1  & 0.0 & 0.1        & 0.0       \\
UN Vote     & 0.1   & 0.1   & 0.2  & 0.1 & 0.1  & 0.0 & 0.0        & 0.2       \\
Contact     & 0.1   & 0.0   & 0.1  & 0.1 & 0.1  & 0.0 & 0.1        & 0.0       \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Configurations of the number of sampled neighbors, the number of causal anonymous walks, and the length of input sequences $\&$ the patch size of different methods.}
\label{tab:num_neighbors_configuration}
% \setlength{\tabcolsep}{1.0mm}
% {
\begin{tabular}{c|ccccccc}
\hline
Datasets    & DyRep & TGAT & TGN & CAWN & TCL & GraphMixer & DyGFormer  \\ \hline
Wikipedia   & 10    & 20   & 10  & 32   & 20  & 30         & 32 \& 1    \\
Reddit      & 10    & 20   & 10  & 32   & 20  & 10         & 64 \& 2    \\
MOOC        & 10    & 20   & 10  & 64   & 20  & 20         & 256 \& 8   \\
LastFM      & 10    & 20   & 10  & 128  & 20  & 10         & 512 \& 16  \\
Enron       & 10    & 20   & 10  & 32   & 20  & 20         & 256 \& 8   \\
Social Evo. & 10    & 20   & 10  & 64   & 20  & 20         & 32 \& 1    \\
UCI         & 10    & 20   & 10  & 64   & 20  & 20         & 32 \& 1    \\
Flights     & 10    & 20   & 10  & 64   & 20  & 20         & 256 \& 8   \\
Can. Parl.  & 10    & 20   & 10  & 128  & 20  & 20         & 2048 \& 64 \\
US Legis.   & 10    & 20   & 10  & 32   & 20  & 20         & 256 \& 8   \\
UN Trade    & 10    & 20   & 10  & 64   & 20  & 20         & 256 \& 8   \\
UN Vote     & 10    & 20   & 10  & 64   & 20  & 20         & 128 \& 4   \\
Contact     & 10    & 20   & 10  & 64   & 20  & 20         & 32 \& 1    \\ \hline
\end{tabular}
% }
\end{table}

\begin{table}[!htbp]
\centering
\caption{Configurations of neighbor sampling strategies of different methods.}
\label{tab:neighbors_sampling_configuration}
% \setlength{\tabcolsep}{1.0mm}
% {
\begin{tabular}{c|ccccc}
\hline
Datasets    & DyRep   & TGAT    & TGN     & TCL     & GraphMixer \\ \hline
Wikipedia   & recent  & recent  & recent  & recent  & recent     \\
Reddit      & recent  & uniform & recent  & uniform & recent     \\
MOOC        & recent  & recent  & recent  & recent  & recent     \\
LastFM      & recent  & recent  & recent  & recent  & recent     \\
Enron       & recent  & recent  & recent  & recent  & recent     \\
Social Evo. & recent  & recent  & recent  & recent  & recent     \\
UCI         & recent  & recent  & recent  & recent  & recent     \\
Flights     & recent  & recent  & recent  & recent  & recent     \\
Can. Parl.  & uniform & uniform & uniform & uniform & uniform    \\
US Legis.   & recent  & recent  & recent  & uniform & recent     \\
UN Trade    & recent  & uniform & recent  & uniform & uniform    \\
UN Vote     & recent  & recent  & uniform & uniform & uniform    \\
Contact     & recent  & recent  & recent  & recent  & recent     \\ \hline
\end{tabular}
% }
\end{table}


\begin{table}[!htbp]
\centering
\caption{Configurations of the variants of EdgeBank with three negative sampling strategies.}
\label{tab:edgebank_configuration}
% \setlength{\tabcolsep}{1.0mm}
% {
\begin{tabular}{c|ccc}
\hline
Datasets    & Random                  & Historical              & Inductive               \\ \hline
Wikipedia   & EdgeBank$_\infty$       & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
Reddit      & EdgeBank$_\infty$       & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
MOOC        & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
LastFM      & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
Enron       & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
Social Evo. & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
UCI         & EdgeBank$_\infty$       & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ \\
Flights     & EdgeBank$_\infty$       & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
Can. Parl.  & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{th}$    \\
US Legis.   & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-ts}$ \\
UN Trade    & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
UN Vote     & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ \\
Contact     & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\ \hline
\end{tabular}
% }
\end{table}
