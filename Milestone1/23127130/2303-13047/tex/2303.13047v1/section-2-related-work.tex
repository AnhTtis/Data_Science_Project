\section{Related Work}
\label{section-2}

\textbf{Dynamic Graph Learning}.
Due to the importance of dynamic graphs, representation learning on dynamic graphs has been extensively investigated in recent years \cite{DBLP:journals/jmlr/KazemiGJKSFP20,skarding2021foundations,xue2022dynamic}. 
One part of the methods treat the dynamic graph as a sequence of snapshots and apply static graph learning methods on each snapshot (i.e., discrete-time methods), where the snapshots are regularly sampled with the same time interval \cite{DBLP:conf/aaai/ParejaDCMSKKSL20,DBLP:journals/kbs/GoyalCC20,DBLP:conf/wsdm/SankarWGZY20,DBLP:journals/corr/abs-2111-10447,DBLP:conf/kdd/YouDL22}. 
% For example, EvolveGCN \cite{DBLP:conf/aaai/ParejaDCMSKKSL20} captures the dynamics in the sequence of snapshots by employing RNNs \cite{DBLP:journals/neco/HochreiterS97,chung2014empirical} to evolve the parameters in Graph Convolutional Networks (GCNs) \cite{DBLP:conf/iclr/KipfW17}. DySAT \cite{DBLP:conf/wsdm/SankarWGZY20} leverages the self-attention mechanism to jointly learn structural and temporal patterns in dynamic graphs. 
However, these methods need to manually determine the time interval and ignore the temporal order of nodes in each snapshot due to the use of static graph learning methods. To solve these issues, more and more researchers have attempted to design continuous-time methods by directly learning on the whole dynamic graph via temporal graph neural networks \cite{DBLP:conf/iclr/TrivediFBZ19,DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/cikm/ChangLW0FS020,DBLP:conf/sigmod/WangLLXYWWCYSG21}, memory networks \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/iclr/TrivediFBZ19,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/sigmod/WangLLXYWWCYSG21,luo2022neighborhoodaware}, temporal random walks \cite{DBLP:conf/iclr/WangCLL021,jin2022neural} and sequential models \cite{DBLP:journals/corr/abs-2105-07944,cong2023do}. Although dynamic graph learning has achieved great success, most of the existing methods still ignore the correlations between two nodes in an interaction. Moreover, they also have difficulty in handling nodes with longer interactions due to unaffordable computational costs of complex modules or the difficulties in optimizing models (e.g., the staleness problem, the vanishing and exploding gradient issues). In this paper, we propose DyGFormer, a new Transformer-based architecture for dynamic graph learning to verify the necessity of capturing nodes' correlations and long-term temporal dependencies. DyGFormer is incorporated with two distinct designs: a neighbor co-occurrence encoding scheme and a patching technique.
% leverages the patching technique to enhance the local temporal proximities and reduce the model complexity for efficiently computing on nodes with longer interaction sequences.

\textbf{Transformer-based Applications in Various Fields}. 
Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} is an innovative model that employs the self-attention mechanism to handle sequential data, which has been successfully applied in a variety of domains, such as natural language processing \cite{DBLP:conf/naacl/DevlinCLT19,DBLP:journals/corr/abs-1907-11692,DBLP:conf/nips/BrownMRSKDNSSAA20}, computer vision \cite{DBLP:conf/eccv/CarionMSUKZ20,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/iccv/LiuL00W0LG21} and time series forecasting \cite{DBLP:conf/nips/LiJXZCWY19,DBLP:conf/aaai/ZhouZPZLXZ21,DBLP:conf/nips/WuXWL21}. The idea of dividing the original data into patches as inputs of the Transformer has been attempted in some studies. ViT \cite{DBLP:conf/iclr/DosovitskiyB0WZ21} splits an image
into multiple patches and feeds the sequence of patches' linear embeddings into a Transformer, which achieves surprisingly good performance on image classification. PatchTST \cite{nie2023a} divides a time series into subseries-level patches and calculates the patches by a channel-independent Transformer for long-term multivariate time series forecasting. In this work, we propose a patching technique for the dynamic graph learning task, which provides Transformer with the potential to handle nodes with longer histories.
% In this work, we apply the patch technique to the dynamic graph learning task for providing Transformer with the potential to handle nodes with longer histories.

\textbf{Graph Learning Library}. To promote the development of deep learning on graphs, several libraries have been proposed \cite{DBLP:journals/corr/abs-1806-01261,DBLP:journals/corr/abs-1909-01315,DBLP:journals/corr/abs-1903-02428,DBLP:conf/nips/HuFZDRLCL20,DBLP:journals/corr/abs-2103-00959,DBLP:conf/icse/LiXCZL21,DBLP:journals/jmlr/LiuLWXYGYXZLYLF21}, but they mainly focus on static graphs. To the best of our knowledge, only a few libraries are specifically designed for dynamic graphs \cite{DBLP:journals/corr/abs-1811-10734,DBLP:conf/cikm/RozemberczkiSHP21,zhou2022tgl}. 
DynamicGEM \cite{DBLP:journals/corr/abs-1811-10734} is a library for dynamic graph embedding methods, which only considers the graph topology and cannot leverage node features. PyTorch Geometric Temporal \cite{DBLP:conf/cikm/RozemberczkiSHP21} implements the discrete-time graph learning algorithms for spatiotemporal signal processing, which require the manual partition of snapshots and are primarily applicable for nodes with aligned historical observations. TGL \cite{zhou2022tgl} aims to realize the training on large-scale dynamic graphs with some engineering tricks. Though TGL has integrated several continuous-time dynamic graph learning methods, they are somewhat out-of-date, resulting in the lack of the current state-of-the-art. Moreover, TGL is implemented by both PyTorch and C++, which needs additional compilation and increases the usage difficulty. In this paper, we present a unified continuous-time dynamic graph learning library with thorough baselines, diverse datasets, extensible implementations, and comprehensive evaluations for facilitating dynamic graph learning research.
