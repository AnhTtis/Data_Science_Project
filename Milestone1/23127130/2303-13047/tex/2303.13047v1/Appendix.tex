\appendix
\label{section-appendix}

\section{Details of the Experiments}
% We illustrate the details of the experiments in this section.
\subsection{Descriptions of Datasets}\label{section-appendix-descriptions_datasets}
We use thirteen datasets that are collected by \cite{poursafaei2022towards} in the experiments:
\begin{itemize}
    \item  \textbf{Wikipedia} is a bipartite interaction graph that contains the edits on Wikipedia pages over a month. Nodes represent users and pages, and links denote the editing behaviors with timestamps. Each link is associated with a 172-dimensional Linguistic Inquiry and Word Count (LIWC) feature \cite{pennebaker2001linguistic}. This dataset additionally contains dynamic labels that indicate whether users are temporarily banned from editing.

    \item  \textbf{Reddit} is bipartite and records the posts of users under subreddits during one month. Users and subreddits are the nodes, and links are the timestamped posting requests. Each link has a 172-dimensional LIWC feature. This dataset also includes dynamic labels representing whether users are banned from posting.

    \item  \textbf{MOOC} is a bipartite interaction network of online sources, where nodes are students and course content units (e.g., videos and problem sets). Each link denotes a student's access behavior to a specific content unit and is assigned with a 4-dimensional feature.

    \item  \textbf{LastFM} is bipartite and consists of the information about which songs were listened to by which users over one month. Users and songs are nodes, and links denote the listening behaviors of users.

    \item  \textbf{Enron} records the email communications between employees of the ENRON energy corporation over three years.

    \item  \textbf{Social Evo.} is a mobile phone proximity network that monitors the daily activities of an entire undergraduate dormitory for a period of eight months, where each link has a 2-dimensional feature.

    \item  \textbf{UCI} is an online communication network, where nodes are university students and links are messages posted by students.
    
    \item  \textbf{Flights} is a dynamic flight network that displays the development of air traffic during the COVID-19 pandemic. Airports are represented by nodes and the tracked flights are denoted as links. Each link is associated with a weight, indicating the number of flights between two airports in a day.

    \item  \textbf{Can. Parl.} is a dynamic political network that records the interactions between Canadian Members of Parliament (MPs) from 2006 to 2019. Each node represents an MP from an electoral district and a link is created when two MPs both vote "yes" on a bill. The weight of each link refers to the number of times that one MP voted “yes” for another MP in a year. 
    
    \item  \textbf{US Legis.} is a senate co-sponsorship network that tracks social interactions between legislators in the US Senate. The weight of each link specifies the number of times two congresspersons have co-sponsored a bill in a given congress.

    \item  \textbf{UN Trade} contains the food and agriculture trade between 181 nations for more than 30 years. The weight of each link indicates the total sum of normalized agriculture import or export values between two particular countries.

    \item  \textbf{UN Vote} records roll-call votes in the United Nations General Assembly. If two nations both voted "yes" to an item, the weight of the link between them is increased by one.

    \item  \textbf{Contact} describes how the physical proximity evolves among about 700 university students over a month. Each student has a unique identifier and links denote that they are within close proximity to each other. Each link is associated with a weight, revealing the physical proximity between students.
\end{itemize}
We show the statistics of the datasets in \tabref{tab:data_statistics}. We notice a slight difference between the statistics of the Contact dataset reported in \cite{poursafaei2022towards} (which has 694 nodes and 2,426,280 links) and our own calculations, although both of them are computed based on the released dataset by \cite{poursafaei2022towards}. We ultimately report our statistics of the Contact dataset in this paper.

\begin{table}[!htbp]
\centering
\caption{Statistics of the datasets.}
\label{tab:data_statistics}
\setlength{\tabcolsep}{1.25mm}
{
\begin{tabular}{c|cccccc}
\hline
Datasets    & Domains     & \#Nodes & \#Links   & \#Node \& Link Features & Bipartite & Duration      \\ \hline
Wikipedia   & Social      & 9,227  & 157,474   & -- \& 172                & True      & 1 month       \\
Reddit      & Social      & 10,984 & 672,447   & -- \& 172                & True      & 1 month       \\
MOOC        & Interaction & 7,144  & 411,749   & -- \& 4                  & True      & 17 months     \\
LastFM      & Interaction & 1,980  & 1,293,103 & -- \& --                 & True      & 1 month       \\
Enron       & Social      & 184    & 125,235   & -- \& --                 & False     & 3 years       \\
Social Evo. & Proximity   & 74     & 2,099,519 & -- \& 2                  & False     & 8 months      \\
UCI         & Social      & 1,899  & 59,835    & -- \& --                 & False     & 196 days      \\
Flights     & Transport   & 13,169 & 1,927,145 & -- \& 1                  & False     & 4 months      \\
Can. Parl.  & Politics    & 734    & 74,478    & -- \& 1                  & False     & 14 years      \\
US Legis.   & Politics    & 225    & 60,396    & -- \& 1                  & False     & 12 congresses \\
UN Trade    & Economics   & 255    & 507,497   & -- \& 1                  & False     & 32 years      \\
UN Vote     & Politics    & 201    & 1,035,742 & -- \& 1                  & False     & 72 years      \\
Contact     & Proximity   & 692    & 2,426,279 & -- \& 1                  & False     & 1 month       \\ \hline
\end{tabular}
}
\end{table}

\subsection{Descriptions of Baselines}\label{section-appendix-descriptions_baselines}
We select the following eight baselines:
\begin{itemize}
    \item  \textbf{JODIE} is designed for temporal bipartite networks of user-item interactions. It employs two coupled recurrent neural networks to update the states of users and items. A projection operation is introduced to learn the future representation trajectory of each user/item \cite{DBLP:conf/kdd/KumarZL19}.
    
    \item  \textbf{DyRep} proposes a recurrent architecture to update node states upon each interaction. It also includes a temporal-attentive aggregation module to consider the temporally evolving structural information in dynamic graphs \cite{DBLP:conf/iclr/TrivediFBZ19}.

    \item  \textbf{TGAT} computes the node representation by aggregating features from each node's temporal-topological neighbors based on the self-attention mechanism. It is also equipped with a time encoding function for capturing temporal patterns \cite{DBLP:conf/iclr/XuRKKA20}.
    
    \item  \textbf{TGN} maintains an evolving memory for each node and updates this memory when the node is observed in an interaction, which is achieved by the message function, message aggregator, and memory updater. An embedding module is leveraged to generate the temporal representations of nodes \cite{DBLP:journals/corr/abs-2006-10637}.

    \item  \textbf{CAWN} first extracts multiple causal anonymous walks for each node, which can explore the causality of network dynamics and generate relative node identities. Then, it utilizes recurrent neural networks to encode each walk and aggregates these walks to obtain the final node representation \cite{DBLP:conf/iclr/WangCLL021}. 

    \item  \textbf{EdgeBank} is a pure memory-based approach without trainable parameters for transductive dynamic link prediction. It stores the observed interactions in the memory unit and updates the memory through various strategies. An interaction will be predicted as positive if it was retained in the memory, and negative otherwise \cite{poursafaei2022towards}. The publication presents two updating strategies, but their official implementations include two more\footnote{\url{https://github.com/fpour/DGB/blob/main/EdgeBank/link_pred/edge_bank_baseline.py}}. To be specific, the four variants of EdgeBank are: EdgeBank$_\infty$ with unlimited memory to store all the observed edges; EdgeBank$_\text{tw-ts}$ and EdgeBank$_\text{tw-re}$, which only remember edges within a fixed-size time window from the immediate past. The window size of EdgeBank$_\text{tw-ts}$ is set to the duration of the test split, while EdgeBank$_\text{tw-re}$ makes it related to the time intervals of repeated edges; EdgeBank$_\text{th}$ that retains edges with appearing counts higher than a threshold. We experiment with all four variants and report the best performance among them.
    
    \item  \textbf{TCL} first generates each node’s interaction sequence by performing a breadth-first search algorithm on the temporal dependency interaction sub-graph. Then, it presents a graph transformer that considers both graph topology and temporal information to learn node representations. It also incorporates a cross-attention operation for modeling the inter-dependencies of two interaction nodes \cite{DBLP:journals/corr/abs-2105-07944}.

    \item  \textbf{GraphMixer} shows that a fixed time encoding function performs better than the trainable version. It incorporates the fixed function into a link encoder based on MLP-Mixer \cite{DBLP:conf/nips/TolstikhinHKBZU21} to learn from temporal links. A node encoder with neighbor mean-pooing is employed to summarize node features \cite{cong2023do}.
\end{itemize}


\subsection{Some Problematic Implementations in Baselines}\label{section-appendix-issues-existing-methods}
JODIE, DyRep, and TGN models are based on memory networks, and their implementations have designed the raw messages to avoid information leakage. However, they fail to store the raw messages when saving models because the raw messages are maintained in a dictionary and thus cannot be saved as model parameters\footnote{\url{https://github.com/twitter-research/tgn/blob/2aa295a1f182137a6ad56328b43cb3d8223cae77/train_self_supervised.py\#L302}}. Our DyGLib has addressed this issue by additionally saving the correct raw messages when saving models. In the official implementation of CAWN, the encoding of each causal anonymous walk is represented by the embedding at the last position of the walk\footnote{\url{https://github.com/snap-stanford/CAW/blob/f994ff2b2c29778e6250b6a9928fd9943e0163f7/module.py\#L1069}}. However, some walks are padded to support mini-batch training in practice, making the last position of these padded walks meaningless. To get the correct encoding, it is necessary to use the actual length of each walk. Moreover, there can be problems with the nodeedge2idx dictionary computed by the get\_ts2idx function\footnote{\url{https://github.com/snap-stanford/CAW/blob/f994ff2b2c29778e6250b6a9928fd9943e0163f7/graph.py\#L79}} when multiple interactions simultaneously occur at the last timestamp. This can lead to information leakage since the model may potentially access more interactions for a given interaction even if they happen at the same time. Our DyGLib has fixed these problems. 

\subsection{Some Inconsistent Observations with Previous Reports}\label{section-appendix-inconsistent-observations}
In the experiments, we find that the behaviors of baselines are inconsistent with their previous reports in some cases. We attribute these phenomena to the following reasons.

\textbf{Suboptimal Settings of Hyperparameters}. Some important hyperparameters in the baselines are not sufficiently fine-tuned, such as the dropout rate, the number of sampled neighbors, the number of random walks, the length of input sequences, and the neighbor sampling strategies. We perform the grid search to find the best settings of these hyperparameters and obtain the improved performance of many baselines.

\textbf{Usage of Problematic Implementations}. Some previous studies utilize problematic implementations in their experiments, and the reported results may not be rigorous. For example, we notice that \cite{poursafaei2022towards} shows better performance on CAWN, but after fixing the issues we mentioned above, the results of CAWN become worse in some cases.

\textbf{Adaptions for Evaluations}. There also exist some differences between GraphMixer's results in the original paper and our work because we modify its implementation to fit our evaluations. The original GraphMixer could only be evaluated for transductive dynamic link prediction. In this work, we remove the one-hot encoding of nodes in GraphMixer to adapt it to the inductive setting, which may lead to decreased performance in certain situations.

\subsection{Configurations of Different Methods}\label{section-appendix-configurations}
We first present the official settings of baselines as well as the configurations of DyGFormer. These configurations remain unchanged across all the datasets.
\begin{itemize}
    \item  \textbf{JODIE}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Memory updater: vanilla recurrent neural network
    \end{itemize}

    \item  \textbf{DyRep}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: vanilla recurrent neural network
    \end{itemize}
    
    \item  \textbf{TGAT}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 2
    \end{itemize}
    
    \item  \textbf{TGN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of node memory: 172
    \item Dimension of output representation: 172
    \item Number of graph attention heads: 2
    \item Number of graph convolution layers: 1
    \item Memory updater: gated recurrent unit \cite{DBLP:conf/ssst/ChoMBB14}
    \end{itemize}
    
    \item  \textbf{CAWN}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of position encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads for encoding walks: 8
    \item Length of each walk (including the target node): 2
    \item Time scaling factor $\alpha$: 1e-6
    \end{itemize}
    
    \item  \textbf{TCL}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of depth encoding: 172
    \item Dimension of output representation: 172
    \item Number of attention heads: 2
    \item Number of Transformer layers: 2
    \end{itemize}

    \item  \textbf{GraphMixer}:
    \begin{itemize}
    \item Dimension of time encoding: 100
    \item Dimension of output representation: 172
    \item Number of MLP-Mixer layers: 2
    \item Time gap $T$: 2000
    \end{itemize}

    \item  \textbf{DyGFormer}:
    \begin{itemize}
    \item Dimension of time encoding $d_T$: 100
    \item Dimension of neighbor co-occurrence encoding $d_C$: 50
    \item Dimension of aligned encoding $d$: 50
    \item Dimension of output representation $d_{out}$: 172
    \item Number of attention heads $I$: 2
    \item Number of Transformer layers $L$: 2
    \end{itemize}
\end{itemize}

Then, we perform the grad search to find the best settings of some critical hyperparameters, where the searched ranges and related methods are shown in \tabref{tab:searched_ranges_related_methods}. It is worth noticing that DyGFormer can directly handle nodes with sequence lengths shorter than the defined length. When the sequence length exceeds the specified length, we select the most recent interactions up to the defined length. 
\begin{table}[!htbp]
\centering
\caption{Searched ranges of hyperparameters and the related methods.}
\label{tab:searched_ranges_related_methods}
\setlength{\tabcolsep}{2.0mm}
{
\begin{tabular}{c|cc}
\hline
Hyperparameters                                                             & Searched Ranges                                                                                                                        & Related Methods                                                                                      \\ \hline
Dropout Rate \cite{DBLP:journals/jmlr/SrivastavaHKSS14}                                                              & \begin{tabular}[c]{@{}c@{}}[0.0, 0.1, 0.2, 0.3, \\ 0.4, 0.5, 0.6]\end{tabular}                                                         & \begin{tabular}[c]{@{}c@{}}JODIE, DyRep, TGAT, TGN, CAWN, \\ TCL, GraphMixer, DyGFormer\end{tabular} \\
\begin{tabular}[c]{@{}c@{}}Number of \\ Sampled Neighbors\end{tabular}      & [10, 20, 30]                                                                                                                           & \begin{tabular}[c]{@{}c@{}}DyRep, TGAT, TGN, \\ TCL, GraphMixer\end{tabular}                         \\
\begin{tabular}[c]{@{}c@{}}Neighbor Sampling \\ Strategies\end{tabular}     & [uniform,recent]                                                                                                                       & \begin{tabular}[c]{@{}c@{}}DyRep, TGAT, TGN, \\ TCL, GraphMixer\end{tabular}                         \\
\begin{tabular}[c]{@{}c@{}}Number of Causal \\ Anonymous Walks\end{tabular} & [16, 32, 64, 128]                                                                                                                      & CAWN                                                                                                 \\
\begin{tabular}[c]{@{}c@{}}Memory Updating \\ Variants\end{tabular}       & \begin{tabular}[c]{@{}c@{}}[EdgeBank$_\infty$, EdgeBank$_\text{tw-ts}$, \\ EdgeBank$_\text{tw-re}$, EdgeBank$_\text{th}$]\end{tabular} & EdgeBank                                                                                             \\
\begin{tabular}[c]{@{}c@{}}Length of Input \\ Sequences \& \\ Patch Size\end{tabular} & \begin{tabular}[c]{@{}c@{}}[32 \& 1, 64 \& 2, 128 \& 4, \\ 256 \& 8, 512 \& 16, 1024 \& 32, \\ 2048 \& 64, 4096 \& 128]\end{tabular}   & DyGFormer                                                                                            \\ \hline
\end{tabular}
}
\end{table}

Finally, we show the hyperparameter settings of various methods that are determined by the grad search in \tabref{tab:dropout_configuration}, \tabref{tab:num_neighbors_configuration}, \tabref{tab:neighbors_sampling_configuration} and \tabref{tab:edgebank_configuration}.

\begin{table}[!htbp]
\centering
\caption{Configurations of the dropout rate of different methods.}
\label{tab:dropout_configuration}
\setlength{\tabcolsep}{2.0mm}
{
\begin{tabular}{c|cccccccc}
\hline
Datasets    & JODIE & DyRep & TGAT & TGN & CAWN & TCL & GraphMixer & DyGFormer \\ \hline
Wikipedia   & 0.1   & 0.1   & 0.1  & 0.1 & 0.1  & 0.1 & 0.5        & 0.1       \\
Reddit      & 0.1   & 0.1   & 0.1  & 0.1 & 0.1  & 0.1 & 0.5        & 0.2       \\
MOOC        & 0.2   & 0.0   & 0.1  & 0.2 & 0.1  & 0.1 & 0.4        & 0.1       \\
LastFM      & 0.3   & 0.0   & 0.1  & 0.3 & 0.1  & 0.1 & 0.0        & 0.1       \\
Enron       & 0.1   & 0.0   & 0.2  & 0.0 & 0.1  & 0.1 & 0.5        & 0.0       \\
Social Evo. & 0.1   & 0.1   & 0.1  & 0.0 & 0.1  & 0.0 & 0.3        & 0.1       \\
UCI         & 0.4   & 0.0   & 0.1  & 0.1 & 0.1  & 0.0 & 0.4        & 0.1       \\
Flights     & 0.1   & 0.1   & 0.1  & 0.1 & 0.1  & 0.1 & 0.2        & 0.1       \\
Can. Parl.  & 0.0   & 0.0   & 0.2  & 0.3 & 0.0  & 0.2 & 0.2        & 0.1       \\
US Legis.   & 0.2   & 0.0   & 0.1  & 0.1 & 0.1  & 0.3 & 0.4        & 0.0       \\
UN Trade    & 0.4   & 0.1   & 0.1  & 0.2 & 0.1  & 0.0 & 0.1        & 0.0       \\
UN Vote     & 0.1   & 0.1   & 0.2  & 0.1 & 0.1  & 0.0 & 0.0        & 0.2       \\
Contact     & 0.1   & 0.0   & 0.1  & 0.1 & 0.1  & 0.0 & 0.1        & 0.0       \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Configurations of the number of sampled neighbors, the number of causal anonymous walks, or the length of input sequences $\&$ the patch size of different methods.}
\label{tab:num_neighbors_configuration}
% \setlength{\tabcolsep}{1.0mm}
% {
\begin{tabular}{c|ccccccc}
\hline
Datasets    & DyRep & TGAT & TGN & CAWN & TCL & GraphMixer & DyGFormer  \\ \hline
Wikipedia   & 10    & 20   & 10  & 32   & 20  & 30         & 32 \& 1    \\
Reddit      & 10    & 20   & 10  & 32   & 20  & 10         & 64 \& 2    \\
MOOC        & 10    & 20   & 10  & 64   & 20  & 20         & 256 \& 8   \\
LastFM      & 10    & 20   & 10  & 128  & 20  & 10         & 512 \& 16  \\
Enron       & 10    & 20   & 10  & 32   & 20  & 20         & 256 \& 8   \\
Social Evo. & 10    & 20   & 10  & 64   & 20  & 20         & 32 \& 1    \\
UCI         & 10    & 20   & 10  & 64   & 20  & 20         & 32 \& 1    \\
Flights     & 10    & 20   & 10  & 64   & 20  & 20         & 256 \& 8   \\
Can. Parl.  & 10    & 20   & 10  & 128  & 20  & 20         & 2048 \& 64 \\
US Legis.   & 10    & 20   & 10  & 32   & 20  & 20         & 256 \& 8   \\
UN Trade    & 10    & 20   & 10  & 64   & 20  & 20         & 256 \& 8   \\
UN Vote     & 10    & 20   & 10  & 64   & 20  & 20         & 128 \& 4   \\
Contact     & 10    & 20   & 10  & 64   & 20  & 20         & 32 \& 1    \\ \hline
\end{tabular}
% }
\end{table}

\begin{table}[!htbp]
\centering
\caption{Configurations of neighbor sampling strategies of different methods.}
\label{tab:neighbors_sampling_configuration}
% \setlength{\tabcolsep}{1.0mm}
% {
\begin{tabular}{c|ccccc}
\hline
Datasets    & DyRep   & TGAT    & TGN     & TCL     & GraphMixer \\ \hline
Wikipedia   & recent  & recent  & recent  & recent  & recent     \\
Reddit      & recent  & uniform & recent  & uniform & recent     \\
MOOC        & recent  & recent  & recent  & recent  & recent     \\
LastFM      & recent  & recent  & recent  & recent  & recent     \\
Enron       & recent  & recent  & recent  & recent  & recent     \\
Social Evo. & recent  & recent  & recent  & recent  & recent     \\
UCI         & recent  & recent  & recent  & recent  & recent     \\
Flights     & recent  & recent  & recent  & recent  & recent     \\
Can. Parl.  & uniform & uniform & uniform & uniform & uniform    \\
US Legis.   & recent  & recent  & recent  & uniform & recent     \\
UN Trade    & recent  & uniform & recent  & uniform & uniform    \\
UN Vote     & recent  & recent  & uniform & uniform & uniform    \\
Contact     & recent  & recent  & recent  & recent  & recent     \\ \hline
\end{tabular}
% }
\end{table}


\begin{table}[!htbp]
\centering
\caption{Configurations of the variants of EdgeBank with three negative sampling strategies.}
\label{tab:edgebank_configuration}
% \setlength{\tabcolsep}{1.0mm}
% {
\begin{tabular}{c|ccc}
\hline
Datasets    & Random                  & Historical              & Inductive               \\ \hline
Wikipedia   & EdgeBank$_\infty$       & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
Reddit      & EdgeBank$_\infty$       & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
MOOC        & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
LastFM      & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
Enron       & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
Social Evo. & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
UCI         & EdgeBank$_\infty$       & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-re}$ \\
Flights     & EdgeBank$_\infty$       & EdgeBank$_\text{th}$    & EdgeBank$_\text{th}$    \\
Can. Parl.  & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{th}$    \\
US Legis.   & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-ts}$ & EdgeBank$_\text{tw-ts}$ \\
UN Trade    & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\
UN Vote     & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ \\
Contact     & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{tw-re}$ & EdgeBank$_\text{th}$    \\ \hline
\end{tabular}
% }
\end{table}


\section{Detailed Experimental Results}\label{section-appendix-numerical-performance-dynamic-link-prediction}
\subsection{Additional Results for Transductive Dynamic Link Prediction}
We show the AUC-ROC for transductive dynamic link prediction with three negative sampling strategies in \tabref{tab:auc_roc_transductive_dynamic_link_prediction}.

\begin{table}[!htbp]
\centering
\caption{AUC-ROC for transductive dynamic link prediction with random, history, and inductive negative sampling strategies.}
\label{tab:auc_roc_transductive_dynamic_link_prediction}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{1.05mm}
{
\begin{tabular}{c|c|ccccccccc}
\hline
NSS                    & Datasets    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & EdgeBank     & TCL          & GraphMixer   & DyGFormer    \\ \hline
\multirow{14}{*}{rnd}  & Wikipedia   & 96.33 ± 0.07 & 94.37 ± 0.09 & 96.67 ± 0.07 & 98.37 ± 0.07 & \underline{98.54 ± 0.04} & 90.78 ± 0.00 & 95.84 ± 0.18 & 96.92 ± 0.03 & \textbf{98.91 ± 0.02} \\
                       & Reddit      & 98.31 ± 0.05 & 98.17 ± 0.05 & 98.47 ± 0.02 & 98.60 ± 0.06 & \underline{99.01 ± 0.01} & 95.37 ± 0.00 & 97.42 ± 0.02 & 97.17 ± 0.02 & \textbf{99.15 ± 0.01} \\
                       & MOOC        & 83.81 ± 2.09 & 85.03 ± 0.58 & 87.11 ± 0.19 & \textbf{91.21 ± 1.15} & 80.38 ± 0.26 & 60.86 ± 0.00 & 83.12 ± 0.18 & 84.01 ± 0.17 & \underline{87.91 ± 0.58} \\
                       & LastFM      & 70.49 ± 1.66 & 71.16 ± 1.89 & 71.59 ± 0.18 & 78.47 ± 2.94 & \underline{85.92 ± 0.10} & 83.77 ± 0.00 & 64.06 ± 1.16 & 73.53 ± 0.12 & \textbf{93.05 ± 0.10} \\
                       & Enron       & 87.96 ± 0.52 & 84.89 ± 3.00 & 68.89 ± 1.10 & 88.32 ± 0.99 & \underline{90.45 ± 0.14} & 87.05 ± 0.00 & 75.74 ± 0.72 & 84.38 ± 0.21 & \textbf{93.33 ± 0.13} \\
                       & Social Evo. & 92.05 ± 0.46 & 90.76 ± 0.21 & 94.76 ± 0.16 & \underline{95.39 ± 0.17} & 87.34 ± 0.08 & 81.60 ± 0.00 & 94.84 ± 0.17 & 95.23 ± 0.07 & \textbf{96.30 ± 0.01} \\
                       & UCI         & 90.44 ± 0.49 & 68.77 ± 2.34 & 78.53 ± 0.74 & 92.03 ± 1.13 & \underline{93.87 ± 0.08} & 77.30 ± 0.00 & 87.82 ± 1.36 & 91.81 ± 0.67 & \textbf{94.49 ± 0.26} \\
                       & Flights     & 96.21 ± 1.42 & 95.95 ± 0.62 & 94.13 ± 0.17 & 98.22 ± 0.13 & \underline{98.45 ± 0.01} & 90.23 ± 0.00 & 91.21 ± 0.02 & 91.13 ± 0.01 & \textbf{98.93 ± 0.01} \\
                       & Can. Parl.  & 78.21 ± 0.23 & 73.35 ± 3.67 & 75.69 ± 0.78 & 76.99 ± 1.80 & 75.70 ± 3.27 & 64.14 ± 0.00 & 72.46 ± 3.23 & \underline{83.17 ± 0.53} & \textbf{97.76 ± 0.41} \\
                       & US Legis.   & \underline{82.85 ± 1.07} & 82.28 ± 0.32 & 75.84 ± 1.99 & \textbf{83.34 ± 0.43} & 77.16 ± 0.39 & 62.57 ± 0.00 & 76.27 ± 0.63 & 76.96 ± 0.79 & 77.90 ± 0.58 \\
                       & UN Trade    & \underline{69.62 ± 0.44} & 67.44 ± 0.83 & 64.01 ± 0.12 & 69.10 ± 1.67 & 68.54 ± 0.18 & 66.75 ± 0.00 & 64.72 ± 0.05 & 65.52 ± 0.51 & \textbf{70.20 ± 1.44} \\
                       & UN Vote     & \underline{68.53 ± 0.95} & 67.18 ± 1.04 & 52.83 ± 1.12 & \textbf{69.71 ± 2.65} & 53.09 ± 0.22 & 62.97 ± 0.00 & 51.88 ± 0.36 & 52.46 ± 0.27 & 57.12 ± 0.62 \\
                       & Contact     & 96.66 ± 0.89 & 96.48 ± 0.14 & 96.95 ± 0.08 & \underline{97.54 ± 0.35} & 89.99 ± 0.34 & 94.34 ± 0.00 & 94.15 ± 0.09 & 93.94 ± 0.02 & \textbf{98.53 ± 0.01} \\ \cline{2-11} 
                       & Avg. Rank   & 4.38         & 5.77         & 6.00         & \underline{2.54}         & 4.38         & 7.31         & 7.23         & 5.77         & \textbf{1.62}         \\ \hline
\multirow{14}{*}{hist} & Wikipedia   & 80.77 ± 0.73 & 77.74 ± 0.33 & 82.87 ± 0.22 & 82.74 ± 0.32 & 67.84 ± 0.64 & 77.27 ± 0.00 & \underline{85.76 ± 0.46} & \textbf{87.68 ± 0.17} & 78.80 ± 1.95 \\
                       & Reddit      & 80.52 ± 0.32 & 80.15 ± 0.18 & 79.33 ± 0.16 & \textbf{81.11 ± 0.19} & 80.27 ± 0.30 & 78.58 ± 0.00 & 76.49 ± 0.16 & 77.80 ± 0.12 & \underline{80.54 ± 0.29} \\
                       & MOOC        & 82.75 ± 0.83 & 81.06 ± 0.94 & 80.81 ± 0.67 & \textbf{88.00 ± 1.80} & 71.57 ± 1.07 & 61.90 ± 0.00 & 72.09 ± 0.56 & 76.68 ± 1.40 & \underline{87.04 ± 0.35} \\
                       & LastFM      & 75.22 ± 2.36 & 74.65 ± 1.98 & 64.27 ± 0.26 & 77.97 ± 3.04 & 67.88 ± 0.24 & \underline{78.09 ± 0.00} & 47.24 ± 3.13 & 64.21 ± 0.73 & \textbf{78.78 ± 0.35} \\
                       & Enron       & 75.39 ± 2.37 & 74.69 ± 3.55 & 61.85 ± 1.43 & \underline{77.09 ± 2.22} & 65.10 ± 0.34 & \textbf{79.59 ± 0.00} & 67.95 ± 0.88 & 75.27 ± 1.14 & 76.55 ± 0.52 \\
                       & Social Evo. & 90.06 ± 3.15 & 93.12 ± 0.34 & 93.08 ± 0.59 & \underline{94.71 ± 0.53} & 87.43 ± 0.15 & 85.81 ± 0.00 & 93.44 ± 0.68 & 94.39 ± 0.31 & \textbf{97.28 ± 0.07} \\
                       & UCI         & \textbf{78.64 ± 3.50} & 57.91 ± 3.12 & 58.89 ± 1.57 & 77.25 ± 2.68 & 57.86 ± 0.15 & 69.56 ± 0.00 & 72.25 ± 3.46 & \underline{77.54 ± 2.02} & 76.97 ± 0.24 \\
                       & Flights     & 68.97 ± 1.87 & 69.43 ± 0.90 & \underline{72.20 ± 0.16} & 68.39 ± 0.95 & 66.11 ± 0.71 & \textbf{74.64 ± 0.00} & 70.57 ± 0.18 & 70.37 ± 0.23 & 68.09 ± 0.43 \\
                       & Can. Parl.  & 62.44 ± 1.11 & 70.16 ± 1.70 & 70.86 ± 0.94 & 73.23 ± 3.08 & 72.06 ± 3.94 & 63.04 ± 0.00 & 69.95 ± 3.70 & \underline{79.03 ± 1.01} & \textbf{97.61 ± 0.40} \\
                       & US Legis.   & 67.47 ± 6.40 & \textbf{91.44 ± 1.18} & 73.47 ± 5.25 & 83.53 ± 4.53 & 78.62 ± 7.46 & 67.41 ± 0.00 & 83.97 ± 3.71 & 85.17 ± 0.70 & \underline{90.77 ± 1.96} \\
                       & UN Trade    & 68.92 ± 1.40 & 64.36 ± 1.40 & 60.37 ± 0.68 & 63.93 ± 5.41 & 63.09 ± 0.74 & \textbf{86.61 ± 0.00} & 61.43 ± 1.04 & 63.20 ± 1.54 & \underline{73.86 ± 1.13} \\
                       & UN Vote     & \underline{76.84 ± 1.01} & 74.72 ± 1.43 & 53.95 ± 3.15 & 73.40 ± 5.20 & 51.27 ± 0.33 & \textbf{89.62 ± 0.00} & 52.29 ± 2.39 & 52.61 ± 1.44 & 64.27 ± 1.78 \\
                       & Contact     & \underline{96.35 ± 0.92} & 96.00 ± 0.23 & 95.39 ± 0.43 & 93.76 ± 1.29 & 83.06 ± 0.32 & 92.17 ± 0.00 & 93.34 ± 0.19 & 93.14 ± 0.34 & \textbf{97.17 ± 0.05} \\ \cline{2-11} 
                       & Avg. Rank   & 4.38         & 4.77         & 5.85         & \underline{3.46}         & 7.38         & 5.38         & 6.08         & 4.77         & \textbf{2.92}         \\ \hline
\multirow{14}{*}{ind}  & Wikipedia   & 70.96 ± 0.78 & 67.36 ± 0.96 & 81.93 ± 0.22 & 80.97 ± 0.31 & 70.95 ± 0.95 & 81.73 ± 0.00 & \underline{82.19 ± 0.48} & \textbf{84.28 ± 0.30} & 75.09 ± 3.70 \\
                       & Reddit      & 83.51 ± 0.15 & 82.90 ± 0.31 & \underline{87.13 ± 0.20} & 84.56 ± 0.24 & \textbf{88.04 ± 0.29} & 85.93 ± 0.00 & 84.67 ± 0.29 & 82.21 ± 0.13 & 86.23 ± 0.51 \\
                       & MOOC        & 66.63 ± 2.30 & 63.26 ± 1.01 & 73.18 ± 0.33 & \underline{77.44 ± 2.86} & 70.32 ± 1.43 & 48.18 ± 0.00 & 70.36 ± 0.37 & 72.45 ± 0.72 & \textbf{80.76 ± 0.76} \\
                       & LastFM      & 61.32 ± 3.49 & 62.15 ± 2.12 & 63.99 ± 0.21 & 65.46 ± 4.27 & 67.92 ± 0.44 & \textbf{77.37 ± 0.00} & 46.93 ± 2.59 & 60.22 ± 0.32 & \underline{69.25 ± 0.36} \\
                       & Enron       & 70.92 ± 1.05 & 68.73 ± 1.34 & 60.45 ± 2.12 & 71.34 ± 2.46 & \textbf{75.17 ± 0.50} & \underline{75.00 ± 0.00} & 67.64 ± 0.86 & 71.53 ± 0.85 & 74.07 ± 0.64 \\
                       & Social Evo. & 90.01 ± 3.19 & 93.07 ± 0.38 & 92.94 ± 0.61 & \underline{95.24 ± 0.56} & 89.93 ± 0.15 & 87.88 ± 0.00 & 93.44 ± 0.72 & 94.22 ± 0.32 & \textbf{97.51 ± 0.06} \\
                       & UCI         & 64.14 ± 1.26 & 54.25 ± 2.01 & 60.80 ± 1.01 & 64.11 ± 1.04 & 58.06 ± 0.26 & 58.03 ± 0.00 & \underline{70.05 ± 1.86} & \textbf{74.59 ± 0.74} & 65.96 ± 1.18 \\
                       & Flights     & 69.99 ± 3.10 & 71.13 ± 1.55 & \underline{73.47 ± 0.18} & 71.63 ± 1.72 & 69.70 ± 0.75 & \textbf{81.10 ± 0.00} & 72.54 ± 0.19 & 72.21 ± 0.21 & 69.53 ± 1.17 \\
                       & Can. Parl.  & 52.88 ± 0.80 & 63.53 ± 0.65 & 72.47 ± 1.18 & 69.57 ± 2.81 & \underline{72.93 ± 1.78} & 61.41 ± 0.00 & 69.47 ± 2.12 & 70.52 ± 0.94 & \textbf{96.70 ± 0.59} \\
                       & US Legis.   & 59.05 ± 5.52 & \textbf{89.44 ± 0.71} & 71.62 ± 5.42 & 78.12 ± 4.46 & 76.45 ± 7.02 & 68.66 ± 0.00 & 82.54 ± 3.91 & 84.22 ± 0.91 & \underline{87.96 ± 1.80} \\
                       & UN Trade    & 66.82 ± 1.27 & 65.60 ± 1.28 & 66.13 ± 0.78 & 66.37 ± 5.39 & \underline{71.73 ± 0.74} & \textbf{74.20 ± 0.00} & 67.80 ± 1.21 & 66.53 ± 1.22 & 62.56 ± 1.51 \\
                       & UN Vote     & \textbf{73.73 ± 1.61} & 72.80 ± 2.16 & 53.04 ± 2.58 & 72.69 ± 3.72 & 52.75 ± 0.90 & \underline{72.85 ± 0.00} & 52.02 ± 1.64 & 51.89 ± 0.74 & 53.37 ± 1.26 \\
                       & Contact     & \underline{94.47 ± 1.08} & 94.23 ± 0.18 & 94.10 ± 0.41 & 91.64 ± 1.72 & 87.68 ± 0.24 & 85.87 ± 0.00 & 91.23 ± 0.19 & 90.96 ± 0.27 & \textbf{95.01 ± 0.15} \\ \cline{2-11} 
                       & Avg. Rank   & 5.92         & 6.15         & 4.85         & \underline{4.54}         & 5.15         & 5.08         & 5.00         & 4.77         & \textbf{3.54}         \\ \hline
\end{tabular}
}
}
\end{table}

\subsection{Additional Results for Inductive Dynamic Link Prediction}
We present the AP and AUC-ROC for inductive dynamic link prediction with three negative sampling strategies in \tabref{tab:ap_inductive_dynamic_link_prediction} and \tabref{tab:auc_roc_inductive_dynamic_link_prediction} .

\begin{table}[!htbp]
\centering
\caption{AP for inductive dynamic link prediction with random, history, and inductive negative sampling strategies.}
\label{tab:ap_inductive_dynamic_link_prediction}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{1.05mm}
{
\begin{tabular}{c|c|cccccccc}
\hline
NSS                    & Datasets    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & TCL          & GraphMixer   & DyGFormer    \\ \hline
\multirow{14}{*}{rnd}  & Wikipedia   & 94.82 ± 0.20 & 92.43 ± 0.37 & 96.22 ± 0.07 & 97.83 ± 0.04 & \underline{98.24 ± 0.03} & 96.22 ± 0.17 & 96.65 ± 0.02 & \textbf{98.59 ± 0.03} \\
                       & Reddit      & 96.50 ± 0.13 & 96.09 ± 0.11 & 97.09 ± 0.04 & 97.50 ± 0.07 & \underline{98.62 ± 0.01} & 94.09 ± 0.07 & 95.26 ± 0.02 & \textbf{98.84 ± 0.02} \\
                       & MOOC        & 79.63 ± 1.92 & 81.07 ± 0.44 & 85.50 ± 0.19 & \textbf{89.04 ± 1.17} & 81.42 ± 0.24 & 80.60 ± 0.22 & 81.41 ± 0.21 & \underline{86.96 ± 0.43} \\
                       & LastFM      & 81.61 ± 3.82 & 83.02 ± 1.48 & 78.63 ± 0.31 & 81.45 ± 4.29 & \underline{89.42 ± 0.07} & 73.53 ± 1.66 & 82.11 ± 0.42 & \textbf{94.23 ± 0.09} \\
                       & Enron       & 80.72 ± 1.39 & 74.55 ± 3.95 & 67.05 ± 1.51 & 77.94 ± 1.02 & \underline{86.35 ± 0.51} & 76.14 ± 0.79 & 75.88 ± 0.48 & \textbf{89.76 ± 0.34} \\
                       & Social Evo. & \underline{91.96 ± 0.48} & 90.04 ± 0.47 & 91.41 ± 0.16 & 90.77 ± 0.86 & 79.94 ± 0.18 & 91.55 ± 0.09 & 91.86 ± 0.06 & \textbf{93.14 ± 0.04} \\
                       & UCI         & 79.86 ± 1.48 & 57.48 ± 1.87 & 79.54 ± 0.48 & 88.12 ± 2.05 & \underline{92.73 ± 0.06} & 87.36 ± 2.03 & 91.19 ± 0.42 & \textbf{94.54 ± 0.12} \\
                       & Flights     & 94.74 ± 0.37 & 92.88 ± 0.73 & 88.73 ± 0.33 & 95.03 ± 0.60 & \underline{97.06 ± 0.02} & 83.41 ± 0.07 & 83.03 ± 0.05 & \textbf{97.79 ± 0.02} \\
                       & Can. Parl.  & 53.92 ± 0.94 & 54.02 ± 0.76 & 55.18 ± 0.79 & 54.10 ± 0.93 & 55.80 ± 0.69 & 54.30 ± 0.66 & \underline{55.91 ± 0.82} & \textbf{87.74 ± 0.71} \\
                       & US Legis.   & 54.93 ± 2.29 & \underline{57.28 ± 0.71} & 51.00 ± 3.11 & \textbf{58.63 ± 0.37} & 53.17 ± 1.20 & 52.59 ± 0.97 & 50.71 ± 0.76 & 54.28 ± 2.87 \\
                       & UN Trade    & 59.65 ± 0.77 & 57.02 ± 0.69 & 61.03 ± 0.18 & 58.31 ± 3.15 & \textbf{65.24 ± 0.21} & 62.21 ± 0.12 & 62.17 ± 0.31 & \underline{64.55 ± 0.62} \\
                       & UN Vote     & \underline{56.64 ± 0.96} & 54.62 ± 2.22 & 52.24 ± 1.46 & \textbf{58.85 ± 2.51} & 49.94 ± 0.45 & 51.60 ± 0.97 & 50.68 ± 0.44 & 55.93 ± 0.39 \\
                       & Contact     & 94.34 ± 1.45 & 92.18 ± 0.41 & \underline{95.87 ± 0.11} & 93.82 ± 0.99 & 89.55 ± 0.30 & 91.11 ± 0.12 & 90.59 ± 0.05 & \textbf{98.03 ± 0.02} \\ \cline{2-10} 
                       & Avg. Rank   & 4.69         & 5.77         & 5.23         & \underline{3.77}         & \underline{3.77}         & 5.77         & 5.23         & \textbf{1.54}         \\ \hline
\multirow{14}{*}{hist} & Wikipedia   & 68.69 ± 0.39 & 62.18 ± 1.27 & \underline{84.17 ± 0.22} & 81.76 ± 0.32 & 67.27 ± 1.63 & 82.20 ± 2.18 & \textbf{87.60 ± 0.30} & 71.42 ± 4.43 \\
                       & Reddit      & 62.34 ± 0.54 & 61.60 ± 0.72 & 63.47 ± 0.36 & \underline{64.85 ± 0.85} & 63.67 ± 0.41 & 60.83 ± 0.25 & 64.50 ± 0.26 & \textbf{65.37 ± 0.60} \\
                       & MOOC        & 63.22 ± 1.55 & 62.93 ± 1.24 & 76.73 ± 0.29 & \underline{77.07 ± 3.41} & 74.68 ± 0.68 & 74.27 ± 0.53 & 74.00 ± 0.97 & \textbf{80.82 ± 0.30} \\
                       & LastFM      & 70.39 ± 4.31 & 71.45 ± 1.76 & 76.27 ± 0.25 & 66.65 ± 6.11 & 71.33 ± 0.47 & 65.78 ± 0.65 & \textbf{76.42 ± 0.22} & \underline{76.35 ± 0.52} \\
                       & Enron       & 65.86 ± 3.71 & 62.08 ± 2.27 & 61.40 ± 1.31 & 62.91 ± 1.16 & 60.70 ± 0.36 & \underline{67.11 ± 0.62} & \textbf{72.37 ± 1.37} & 67.07 ± 0.62 \\
                       & Social Evo. & 88.51 ± 0.87 & 88.72 ± 1.10 & 93.97 ± 0.54 & 90.66 ± 1.62 & 79.83 ± 0.38 & \underline{94.10 ± 0.31} & 94.01 ± 0.47 & \textbf{96.82 ± 0.16} \\
                       & UCI         & 63.11 ± 2.27 & 52.47 ± 2.06 & 70.52 ± 0.93 & 70.78 ± 0.78 & 64.54 ± 0.47 & \underline{76.71 ± 1.00} & \textbf{81.66 ± 0.49} & 72.13 ± 1.87 \\
                       & Flights     & 61.01 ± 1.65 & 62.83 ± 1.31 & \underline{64.72 ± 0.36} & 59.31 ± 1.43 & 56.82 ± 0.57 & 64.50 ± 0.25 & \textbf{65.28 ± 0.24} & 57.11 ± 0.21 \\
                       & Can. Parl.  & 52.60 ± 0.88 & 52.28 ± 0.31 & 56.72 ± 0.47 & 54.42 ± 0.77 & \underline{57.14 ± 0.07} & 55.71 ± 0.74 & 55.84 ± 0.73 & \textbf{87.40 ± 0.85} \\
                       & US Legis.   & 52.94 ± 2.11 & \textbf{62.10 ± 1.41} & 51.83 ± 3.95 & \underline{61.18 ± 1.10} & 55.56 ± 1.71 & 53.87 ± 1.41 & 52.03 ± 1.02 & 56.31 ± 3.46 \\
                       & UN Trade    & 55.46 ± 1.19 & \underline{55.49 ± 0.84} & 55.28 ± 0.71 & 52.80 ± 3.19 & 55.00 ± 0.38 & \textbf{55.76 ± 1.03} & 54.94 ± 0.97 & 53.20 ± 1.07 \\
                       & UN Vote     & \underline{61.04 ± 1.30} & 60.22 ± 1.78 & 53.05 ± 3.10 & \textbf{63.74 ± 3.00} & 47.98 ± 0.84 & 54.19 ± 2.17 & 48.09 ± 0.43 & 52.63 ± 1.26 \\
                       & Contact     & 90.42 ± 2.34 & 89.22 ± 0.66 & \textbf{94.15 ± 0.45} & 88.13 ± 1.50 & 74.20 ± 0.80 & 90.44 ± 0.17 & 89.91 ± 0.36 & \underline{93.56 ± 0.52} \\ \cline{2-10} 
                       & Avg. Rank   & 5.38         & 5.46         & 4.00         & 4.54         & 5.92         & 3.92         & \underline{3.54}         & \textbf{3.23}         \\ \hline
\multirow{14}{*}{ind}  & Wikipedia   & 68.70 ± 0.39 & 62.19 ± 1.28 & \underline{84.17 ± 0.22} & 81.77 ± 0.32 & 67.24 ± 1.63 & 82.20 ± 2.18 & \textbf{87.60 ± 0.29} & 71.42 ± 4.43 \\
                       & Reddit      & 62.32 ± 0.54 & 61.58 ± 0.72 & 63.40 ± 0.36 & \underline{64.84 ± 0.84} & 63.65 ± 0.41 & 60.81 ± 0.26 & 64.49 ± 0.25 & \textbf{65.35 ± 0.60} \\
                       & MOOC        & 63.22 ± 1.55 & 62.92 ± 1.24 & 76.72 ± 0.30 & \underline{77.07 ± 3.40} & 74.69 ± 0.68 & 74.28 ± 0.53 & 73.99 ± 0.97 & \textbf{80.82 ± 0.30} \\
                       & LastFM      & 70.39 ± 4.31 & 71.45 ± 1.75 & 76.28 ± 0.25 & 69.46 ± 4.65 & 71.33 ± 0.47 & 65.78 ± 0.65 & \textbf{76.42 ± 0.22} & \underline{76.35 ± 0.52} \\
                       & Enron       & 65.86 ± 3.71 & 62.08 ± 2.27 & 61.40 ± 1.30 & 62.90 ± 1.16 & 60.72 ± 0.36 & \underline{67.11 ± 0.62} & \textbf{72.37 ± 1.38} & 67.07 ± 0.62 \\
                       & Social Evo. & 88.51 ± 0.87 & 88.72 ± 1.10 & 93.97 ± 0.54 & 90.65 ± 1.62 & 79.83 ± 0.39 & \underline{94.10 ± 0.32} & 94.01 ± 0.47 & \textbf{96.82 ± 0.17} \\
                       & UCI         & 63.16 ± 2.27 & 52.47 ± 2.09 & 70.49 ± 0.93 & 70.73 ± 0.79 & 64.54 ± 0.47 & \underline{76.65 ± 0.99} & \textbf{81.64 ± 0.49} & 72.13 ± 1.86 \\
                       & Flights     & 61.01 ± 1.66 & 62.83 ± 1.31 & \underline{64.72 ± 0.37} & 59.32 ± 1.45 & 56.82 ± 0.56 & 64.50 ± 0.25 & \textbf{65.29 ± 0.24} & 57.11 ± 0.20 \\
                       & Can. Parl.  & 52.58 ± 0.86 & 52.24 ± 0.28 & 56.46 ± 0.50 & 54.18 ± 0.73 & \underline{57.06 ± 0.08} & 55.46 ± 0.69 & 55.76 ± 0.65 & \textbf{87.22 ± 0.82} \\
                       & US Legis.   & 52.94 ± 2.11 & \textbf{62.10 ± 1.41} & 51.83 ± 3.95 & \underline{61.18 ± 1.10} & 55.56 ± 1.71 & 53.87 ± 1.41 & 52.03 ± 1.02 & 56.31 ± 3.46 \\
                       & UN Trade    & 55.43 ± 1.20 & 55.42 ± 0.87 & \underline{55.58 ± 0.68} & 52.80 ± 3.24 & 54.97 ± 0.38 & \textbf{55.66 ± 0.98} & 54.88 ± 1.01 & 52.56 ± 1.70 \\
                       & UN Vote     & \underline{61.17 ± 1.33} & 60.29 ± 1.79 & 53.08 ± 3.10 & \textbf{63.71 ± 2.97} & 48.01 ± 0.82 & 54.13 ± 2.16 & 48.10 ± 0.40 & 52.61 ± 1.25 \\
                       & Contact     & 90.43 ± 2.33 & 89.22 ± 0.65 & \textbf{94.14 ± 0.45} & 88.12 ± 1.50 & 74.19 ± 0.81 & 90.43 ± 0.17 & 89.91 ± 0.36 & \underline{93.55 ± 0.52} \\ \cline{2-10} 
                       & Avg. Rank   & 5.31         & 5.54         & 3.85         & 4.38         & 5.85         & 3.92         & \underline{3.46}         & \textbf{3.31}         \\ \hline
\end{tabular}
}
}
\end{table}

\begin{table}[!htbp]
\centering
\caption{AUC-ROC for inductive dynamic link prediction with random, history, and inductive negative sampling strategies.}
\label{tab:auc_roc_inductive_dynamic_link_prediction}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{1.05mm}
{
\begin{tabular}{c|c|cccccccc}
\hline
NSS                    & Datasets    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & TCL          & GraphMixer   & DyGFormer    \\ \hline
\multirow{14}{*}{rnd}  & Wikipedia   & 94.33 ± 0.27 & 91.49 ± 0.45 & 95.90 ± 0.09 & 97.72 ± 0.03 & \underline{98.03 ± 0.04} & 95.57 ± 0.20 & 96.30 ± 0.04 & \textbf{98.48 ± 0.03} \\
                       & Reddit      & 96.52 ± 0.13 & 96.05 ± 0.12 & 96.98 ± 0.04 & 97.39 ± 0.07 & \underline{98.42 ± 0.02} & 93.80 ± 0.07 & 94.97 ± 0.05 & \textbf{98.71 ± 0.01} \\
                       & MOOC        & 83.16 ± 1.30 & 84.03 ± 0.49 & 86.84 ± 0.17 & \textbf{91.24 ± 0.99} & 81.86 ± 0.25 & 81.43 ± 0.19 & 82.77 ± 0.24 & \underline{87.62 ± 0.51} \\
                       & LastFM      & 81.13 ± 3.39 & 82.24 ± 1.51 & 76.99 ± 0.29 & 82.61 ± 3.15 & \underline{87.82 ± 0.12} & 70.84 ± 0.85 & 80.37 ± 0.18 & \textbf{94.08 ± 0.08} \\
                       & Enron       & 81.96 ± 1.34 & 76.34 ± 4.20 & 64.63 ± 1.74 & 78.83 ± 1.11 & \underline{87.02 ± 0.50} & 72.33 ± 0.99 & 76.51 ± 0.71 & \textbf{90.69 ± 0.26} \\
                       & Social Evo. & 93.70 ± 0.29 & 91.18 ± 0.49 & 93.41 ± 0.19 & 93.43 ± 0.59 & 84.73 ± 0.27 & 93.71 ± 0.18 & \underline{94.09 ± 0.07} & \textbf{95.29 ± 0.03} \\
                       & UCI         & 78.80 ± 0.94 & 58.08 ± 1.81 & 77.64 ± 0.38 & 86.68 ± 2.29 & \underline{90.40 ± 0.11} & 84.49 ± 1.82 & 89.30 ± 0.57 & \textbf{92.63 ± 0.13} \\
                       & Flights     & 95.21 ± 0.32 & 93.56 ± 0.70 & 88.64 ± 0.35 & 95.92 ± 0.43 & \underline{96.86 ± 0.02} & 82.48 ± 0.01 & 82.27 ± 0.06 & \textbf{97.80 ± 0.02} \\
                       & Can. Parl.  & 53.81 ± 1.14 & 55.27 ± 0.49 & 56.51 ± 0.75 & 55.86 ± 0.75 & \underline{58.83 ± 1.13} & 55.83 ± 1.07 & 58.32 ± 1.08 & \textbf{89.33 ± 0.48} \\
                       & US Legis.   & 58.12 ± 2.35 & \underline{61.07 ± 0.56} & 48.27 ± 3.50 & \textbf{62.38 ± 0.48} & 51.49 ± 1.13 & 50.43 ± 1.48 & 47.20 ± 0.89 & 53.21 ± 3.04 \\
                       & UN Trade    & 62.28 ± 0.50 & 58.82 ± 0.98 & 62.72 ± 0.12 & 59.99 ± 3.50 & \underline{67.05 ± 0.21} & 63.76 ± 0.07 & 63.48 ± 0.37 & \textbf{67.25 ± 1.05} \\
                       & UN Vote     & \underline{58.13 ± 1.43} & 55.13 ± 3.46 & 51.83 ± 1.35 & \textbf{61.23 ± 2.71} & 48.34 ± 0.76 & 50.51 ± 1.05 & 50.04 ± 0.86 & 56.73 ± 0.69 \\
                       & Contact     & 95.37 ± 0.92 & 91.89 ± 0.38 & \underline{96.53 ± 0.10} & 94.84 ± 0.75 & 89.07 ± 0.34 & 93.05 ± 0.09 & 92.83 ± 0.05 & \textbf{98.30 ± 0.02} \\ \cline{2-10} 
                       & Avg. Rank   & 4.69         & 5.85         & 5.31         & \underline{3.38}         & 4.00         & 6.00         & 5.31         & \textbf{1.46}         \\ \hline
\multirow{14}{*}{hist} & Wikipedia   & 61.86 ± 0.53 & 57.54 ± 1.09 & 78.38 ± 0.20 & 75.75 ± 0.29 & 62.04 ± 0.65 & \underline{79.79 ± 0.96} & \textbf{82.87 ± 0.21} & 68.33 ± 2.82 \\
                       & Reddit      & 61.69 ± 0.39 & 60.45 ± 0.37 & 64.43 ± 0.27 & 64.55 ± 0.50 & \textbf{64.94 ± 0.21} & 61.43 ± 0.26 & 64.27 ± 0.13 & \underline{64.81 ± 0.25} \\
                       & MOOC        & 64.48 ± 1.64 & 64.23 ± 1.29 & 74.08 ± 0.27 & \underline{77.69 ± 3.55} & 71.68 ± 0.94 & 69.82 ± 0.32 & 72.53 ± 0.84 & \textbf{80.77 ± 0.63} \\
                       & LastFM      & 68.44 ± 3.26 & 68.79 ± 1.08 & 69.89 ± 0.28 & 66.99 ± 5.62 & 67.69 ± 0.24 & 55.88 ± 1.85 & \underline{70.07 ± 0.20} & \textbf{70.73 ± 0.37} \\
                       & Enron       & 65.32 ± 3.57 & 61.50 ± 2.50 & 57.84 ± 2.18 & 62.68 ± 1.09 & 62.25 ± 0.40 & 64.06 ± 1.02 & \textbf{68.20 ± 1.62} & \underline{65.78 ± 0.42} \\
                       & Social Evo. & 88.53 ± 0.55 & 87.93 ± 1.05 & 91.87 ± 0.72 & 92.10 ± 1.22 & 83.54 ± 0.24 & 93.28 ± 0.60 & \underline{93.62 ± 0.35} & \textbf{96.91 ± 0.09} \\
                       & UCI         & 60.24 ± 1.94 & 51.25 ± 2.37 & 62.32 ± 1.18 & 62.69 ± 0.90 & 56.39 ± 0.10 & \underline{70.46 ± 1.94} & \textbf{75.98 ± 0.84} & 65.55 ± 1.01 \\
                       & Flights     & 60.72 ± 1.29 & 61.99 ± 1.39 & \underline{63.38 ± 0.26} & 59.66 ± 1.04 & 56.58 ± 0.44 & \textbf{63.48 ± 0.23} & 63.30 ± 0.19 & 56.05 ± 0.21 \\
                       & Can. Parl.  & 51.62 ± 1.00 & 52.38 ± 0.46 & 58.30 ± 0.61 & 55.64 ± 0.54 & \underline{60.11 ± 0.48} & 57.30 ± 1.03 & 56.68 ± 1.20 & \textbf{88.68 ± 0.74} \\
                       & US Legis.   & 58.12 ± 2.94 & \textbf{67.94 ± 0.98} & 49.99 ± 4.88 & \underline{64.87 ± 1.65} & 54.41 ± 1.31 & 52.12 ± 2.13 & 49.28 ± 0.86 & 56.57 ± 3.22 \\
                       & UN Trade    & 58.73 ± 1.19 & 57.90 ± 1.33 & 59.74 ± 0.59 & 55.61 ± 3.54 & \underline{60.95 ± 0.80} & \textbf{61.12 ± 0.97} & 59.88 ± 1.17 & 58.46 ± 1.65 \\
                       & UN Vote     & \underline{65.16 ± 1.28} & 63.98 ± 2.12 & 51.73 ± 4.12 & \textbf{68.59 ± 3.11} & 48.01 ± 1.77 & 54.66 ± 2.11 & 45.49 ± 0.42 & 53.85 ± 2.02 \\
                       & Contact     & 90.80 ± 1.18 & 88.88 ± 0.68 & \underline{93.76 ± 0.41} & 88.84 ± 1.39 & 74.79 ± 0.37 & 90.37 ± 0.16 & 90.04 ± 0.29 & \textbf{94.14 ± 0.26} \\ \cline{2-10} 
                       & Avg. Rank   & 5.08         & 6.00         & 4.23         & 4.54         & 5.38         & 4.00         & \underline{3.69}         & \textbf{3.08}         \\ \hline
\multirow{14}{*}{ind}  & Wikipedia   & 61.87 ± 0.53 & 57.54 ± 1.09 & 78.38 ± 0.20 & 75.76 ± 0.29 & 62.02 ± 0.65 & \underline{79.79 ± 0.96} & \textbf{82.88 ± 0.21} & 68.33 ± 2.82 \\
                       & Reddit      & 61.69 ± 0.39 & 60.44 ± 0.37 & 64.39 ± 0.27 & 64.55 ± 0.50 & \textbf{64.91 ± 0.21} & 61.36 ± 0.26 & 64.27 ± 0.13 & \underline{64.80 ± 0.25} \\
                       & MOOC        & 64.48 ± 1.64 & 64.22 ± 1.29 & 74.07 ± 0.27 & \underline{77.68 ± 3.55} & 71.69 ± 0.94 & 69.83 ± 0.32 & 72.52 ± 0.84 & \textbf{80.77 ± 0.63} \\
                       & LastFM      & 68.44 ± 3.26 & 68.79 ± 1.08 & 69.89 ± 0.28 & 66.99 ± 5.61 & 67.68 ± 0.24 & 55.88 ± 1.85 & \underline{70.07 ± 0.20} & \textbf{70.73 ± 0.37} \\
                       & Enron       & 65.32 ± 3.57 & 61.50 ± 2.50 & 57.83 ± 2.18 & 62.68 ± 1.09 & 62.27 ± 0.40 & 64.05 ± 1.02 & \textbf{68.19 ± 1.63} & \underline{65.79 ± 0.42} \\
                       & Social Evo. & 88.53 ± 0.55 & 87.93 ± 1.05 & 91.88 ± 0.72 & 92.10 ± 1.22 & 83.54 ± 0.24 & 93.28 ± 0.60 & \underline{93.62 ± 0.35} & \textbf{96.91 ± 0.09} \\
                       & UCI         & 60.27 ± 1.94 & 51.26 ± 2.40 & 62.29 ± 1.17 & 62.66 ± 0.91 & 56.39 ± 0.11 & \underline{70.42 ± 1.93} & \textbf{75.97 ± 0.85} & 65.58 ± 1.00 \\
                       & Flights     & 60.72 ± 1.29 & 61.99 ± 1.39 & \underline{63.40 ± 0.26} & 59.66 ± 1.05 & 56.58 ± 0.44 & \textbf{63.49 ± 0.23} & 63.32 ± 0.19 & 56.05 ± 0.22 \\
                       & Can. Parl.  & 51.61 ± 0.98 & 52.35 ± 0.52 & 58.15 ± 0.62 & 55.43 ± 0.42 & \underline{60.01 ± 0.47} & 56.88 ± 0.93 & 56.63 ± 1.09 & \textbf{88.51 ± 0.73} \\
                       & US Legis.   & 58.12 ± 2.94 & \textbf{67.94 ± 0.98} & 49.99 ± 4.88 & \underline{64.87 ± 1.65} & 54.41 ± 1.31 & 52.12 ± 2.13 & 49.28 ± 0.86 & 56.57 ± 3.22 \\
                       & UN Trade    & 58.71 ± 1.20 & 57.87 ± 1.36 & 59.98 ± 0.59 & 55.62 ± 3.59 & \underline{60.88 ± 0.79} & \textbf{61.01 ± 0.93} & 59.71 ± 1.17 & 57.28 ± 3.06 \\
                       & UN Vote     & \underline{65.29 ± 1.30} & 64.10 ± 2.10 & 51.78 ± 4.14 & \textbf{68.58 ± 3.08} & 48.04 ± 1.76 & 54.65 ± 2.20 & 45.57 ± 0.41 & 53.87 ± 2.01 \\
                       & Contact     & 90.80 ± 1.18 & 88.87 ± 0.67 & \underline{93.76 ± 0.40} & 88.85 ± 1.39 & 74.79 ± 0.38 & 90.37 ± 0.16 & 90.04 ± 0.29 & \textbf{94.14 ± 0.26} \\ \cline{2-10} 
                       & Avg. Rank   & 5.08         & 5.92         & 4.15         & 4.54         & 5.38         & 4.00         & \underline{3.77}         & \textbf{3.15}         \\ \hline
\end{tabular}
}
}
\end{table}
