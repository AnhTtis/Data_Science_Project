\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% ready for submission
% \usepackage{neurips_2022}
\usepackage[preprint]{neurips_2022}
% \usepackage[final]{neurips_2022}
% \usepackage[nonatbib]{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{array}
\usepackage{amsthm}
\usepackage{rotating}
\usepackage{pifont}
\usepackage{wrapfig}

% \title{Dynamic Graph Learning with Transformers}
\title{Towards Better Dynamic Graph Learning: \\New Architecture and Unified Library}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  % Anonymous Author(s)
  Le Yu, Leilei Sun, Bowen Du, Weifeng Lv\\
  % \thanks{Use footnote for providing further information
    % about author (webpage, alternative address)---\emph{not} for acknowledging
    % funding agencies.} \\
  School of Computer Science and Engineering\\
  State Key Laboratory of Software Development Environment\\
  Beihang University\\
  \texttt{\{yule,leileisun,dubowen,lwf\}@buaa.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\newtheorem{definition}{Definition}
% \newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
% \newcommand{\citet}[1]{\cite \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\equref}[1]{Equation (\ref{#1})}
\newcommand{\appendixref}[1]{Appendix}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}

\begin{document}


\maketitle


\begin{abstract}
We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: (\romannumeral1) a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; (\romannumeral2) a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: (\romannumeral1) DyGFormer achieves state-of-the-art performance on most of the datasets, demonstrating the effectiveness of capturing nodes' correlations and long-term temporal dependencies; (\romannumeral2) the results of baselines vary across different datasets and some findings are inconsistent with previous reports, which may be caused by their diverse pipelines and problematic implementations. We hope our work can provide new insights and facilitate the development of the dynamic graph learning field. All the resources including datasets, data loaders, algorithms, and executing scripts are publicly available at \url{https://github.com/yule-BUAA/DyGLib}.


\end{abstract}

\input{section-1-introduction}

\input{section-2-related-work}

\input{section-3-preliminaries}

\input{section-4-methodology}

\input{section-5-experiments}

\input{section-6-conclusion}

% \bibliographystyle{plain}
% \bibliographystyle{plainnat}
% \bibliographystyle{unsrtnat}
\bibliographystyle{ACM-Reference-Format}
\bibliography{reference.bib}

\input{Appendix}

\end{document}