\section{Introduction}
\label{section-1}

Dynamic graphs denote entities as nodes and represent their interactions as edges with timestamps \cite{DBLP:journals/jmlr/KazemiGJKSFP20}, which are pervasive in a variety of real-world scenarios such as social networks \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/wsdm/Song0WCZT19,alvarez2021evolutionary}, user-item interaction systems \cite{DBLP:conf/icdm/LiZWLWY20,DBLP:conf/cikm/FanLZX0Y21,DBLP:conf/www/YuWS0L22,zhang2022dynamic,DBLP:journals/corr/abs-2204-05490}, traffic networks \cite{DBLP:conf/ijcai/YuYZ18,DBLP:conf/ijcai/WuPLJZ19,DBLP:conf/aaai/GuoLFSW19,DBLP:conf/nips/0001YL0020,DBLP:journals/ijon/YuDHSHL21}, and physical systems \cite{DBLP:conf/nips/HuangS020,DBLP:conf/icml/Sanchez-Gonzalez20,DBLP:conf/iclr/PfaffFSB21}. In recent years, representation learning on dynamic graphs has attracted the wide attention of many scholars \cite{DBLP:journals/jmlr/KazemiGJKSFP20,skarding2021foundations,xue2022dynamic}. Existing methods can be roughly divided into two categories: discrete-time methods \cite{DBLP:conf/aaai/ParejaDCMSKKSL20,DBLP:journals/kbs/GoyalCC20,DBLP:conf/wsdm/SankarWGZY20,DBLP:journals/corr/abs-2111-10447,DBLP:conf/kdd/YouDL22} and continuous-time methods \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/iclr/TrivediFBZ19,DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/cikm/ChangLW0FS020,DBLP:journals/corr/abs-2105-07944,DBLP:conf/iclr/WangCLL021,DBLP:conf/sigmod/WangLLXYWWCYSG21,jin2022neural,luo2022neighborhoodaware,cong2023do}. In this paper, we focus on the latter approaches because they can offer better flexibility and performance than the formers and are being increasingly investigated.

% Issues of existing methods for dynamic graph learning
Despite the rapid development of dynamic graph learning methods, they still suffer from two limitations. On the one hand, most of the previous methods independently learn the temporal representations of nodes in an interaction without exploiting their correlations, which are often indicative of future interactions. Moreover, existing methods follow the interaction-level learning paradigm and only work for nodes with fewer interactions. When nodes have longer histories, they rely on sampling strategies to truncate the interactions for feasible calculations of the computationally expensive modules such as graph convolutions \cite{DBLP:conf/iclr/TrivediFBZ19,DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/cikm/ChangLW0FS020,DBLP:conf/sigmod/WangLLXYWWCYSG21}, temporal random walks \cite{DBLP:conf/iclr/WangCLL021,jin2022neural} and sequential models \cite{DBLP:journals/corr/abs-2105-07944,cong2023do}. Even if some approaches utilize memory networks \cite{DBLP:journals/corr/WestonCB14,DBLP:conf/nips/SukhbaatarSWF15} to sequentially process interactions with affordable computational costs \cite{DBLP:conf/kdd/KumarZL19,DBLP:conf/iclr/TrivediFBZ19,DBLP:journals/corr/abs-2006-10637,DBLP:conf/sigir/0001GRTY20,DBLP:conf/sigmod/WangLLXYWWCYSG21,luo2022neighborhoodaware}, they are usually troubled with the staleness problem \cite{DBLP:journals/jmlr/KazemiGJKSFP20} or vanishing/exploding gradient issues due to the usage of recurrent neural networks \cite{DBLP:conf/icml/PascanuMB13,DBLP:journals/corr/abs-2105-07944}. Therefore, we conclude that \textit{previous methods lack the ability in capturing either the correlations between nodes or long-term temporal dependencies}. 

On the other hand, the training pipelines of different methods are inconsistent and lead to poor reproducibility.
% Some methods even have problematic implementations (See \secref{section-appendix-issues-existing-methods} for more details.).
Moreover, existing methods are implemented by diverse frameworks (e.g., Pytorch \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19}, Tensorflow \cite{DBLP:conf/osdi/AbadiBCCDDDGIIK16}, DGL \cite{DBLP:journals/corr/abs-1909-01315}, PyG \cite{DBLP:journals/corr/abs-1903-02428}, C++), making it time-consuming and difficult for researchers to quickly understand the algorithms and further dive into the core of dynamic graph learning. Although there exist some libraries for dynamic graph learning \cite{DBLP:journals/corr/abs-1811-10734,DBLP:conf/cikm/RozemberczkiSHP21,zhou2022tgl}, they mainly focus on dynamic network embedding methods \cite{DBLP:journals/corr/abs-1811-10734}, discrete-time graph learning methods \cite{DBLP:conf/cikm/RozemberczkiSHP21}, or engineering techniques for training on large-scale dynamic graphs \cite{zhou2022tgl} (elaborated in \secref{section-2}). Currently, we find that \textit{there are still no standard tools for continuous-time dynamic graph learning}.

% Present work and contributions
In this paper, we aim to address the above drawbacks for better learning on dynamic graphs. We have the following two key technical contributions.
% and our technical contributions are two-fold.
% (\romannumeral1) a new Transformer-based dynamic graph learning architecture to capture correlations between the source node and destination node as well as long-term temporal dependencies; and (\romannumeral2) a unified continuous-time dynamic graph learning library to facilitate reproducible, scalable, and credible dynamic graph learning research.

\textbf{We propose a new Transformer-based dynamic graph learning architecture (DyGFormer)}. DyGFormer is conceptually simple and only needs to learn from the sequences of nodesâ€™ historical first-hop interactions. To be specific, DyGFormer is equipped with a neighbor co-occurrence encoding scheme, which encodes the appearing frequencies of each neighbor in the sequences of the source node and destination node, and can explicitly explore the correlations between two nodes. Instead of learning at the interaction level, DyGFormer splits each source/destination node's sequence into multiple patches and feeds them to Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17}. The patching technique allows DyGFormer to capture long-term temporal dependencies since it can not only make DyGFormer effectively benefit from longer histories by preserving the local temporal proximities, but also efficiently reduce the computational complexity to a constant level that is irrelevant to the input sequence length.

% ability to effectively and efficiently break through the bottleneck of previous methods in capturing long-term temporal dependencies via three advantages: \textcolor{red}{local temporal proximities are preserved in each patch; the computational complexity is reduced to a constant level that is irrelevant to the input sequence length; longer histories can be attended by the model.}
% efficiently the computational complexity is reduced to a constant level that is irrelevant to the input sequence length and effectively make the model benefit from longer histories.

\textbf{We present a unified continuous-time dynamic graph learning library (DyGLib)}. DyGLib is an open-source toolkit with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies, aiming to foster standard, scalable, and reproducible dynamic graph learning research. DyGLib is implemented by PyTorch and has integrated thirteen datasets from various domains as well as nine continuous-time dynamic graph learning methods (including DyGFormer). DyGLib trains all the methods via the same pipeline to eliminate the influence of different implementations of previous methods. It also adopts a modularized design for good extensibility and allows developers to conveniently incorporate new datasets and algorithms based on specific requirements. Moreover, DyGLib supports the commonly used dynamic link prediction and dynamic node classification tasks with exhaustive evaluating strategies to provide comprehensive comparisons of existing methods. 
% DyGLib significantly decreases the usage difficulty and makes it easier for researchers to go deep into the dynamic graph learning field. 

To evaluate the model performance, we conduct extensive experiments based on DyGLib, including dynamic link prediction under both transductive and inductive settings with three negative sampling strategies as well as the dynamic node classification task. From the results, we find that: (\romannumeral1) DyGFormer outperforms existing methods on most datasets, which demonstrates its advantages in capturing nodes' correlations and long-term temporal dependencies; (\romannumeral2) the baselines show unstable results across different datasets and some observations are inconsistent with those in previous works due to their less rigorous evaluations. 
We also provide an in-depth analysis of the neighbor co-occurrence encoding and patching technique for a better understanding of DyGFormer.

% The rest of this paper is organized as follows:
% \secref{section-2} reviews the related research.
% \secref{section-3} presents the background of the studied problem.
% \secref{section-4} presents the framework and introduces each component step by step.
% \secref{section-5} conducts experiments to evaluate the proposed model.
% Finally, \secref{section-6} concludes the entire paper.
