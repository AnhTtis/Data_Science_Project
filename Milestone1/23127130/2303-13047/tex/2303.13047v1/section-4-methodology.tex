\section{Methodology}
\label{section-4}

% We promote the development of dynamic graph learning by presenting: (\romannumeral1) a new Transformer-based architecture; and (\romannumeral2) a unified continuous-time dynamic graph learning library.

\subsection{DyGFormer: Transformer-based Architecture for Dynamic Graph Learning}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/DyGFormer_framework.jpg}
    % \includegraphics[scale=0.4]{figures/DyGFormer_framework.jpg}
    \caption{Framework of the proposed model.}
    \label{fig:DyGFormer_framework}
\end{figure}
The framework of our model is shown in \figref{fig:DyGFormer_framework}, which employs Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} as the backbone. Given an interaction $\left(u,v,t\right)$, we first extract historical first-hop interactions of source node $u$ and destination node $v$ before timestamp $t$ and obtain two interaction sequences $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$. Next, in addition to computing the encodings of neighbors, links, and time intervals for each sequence, we also encode the frequencies of every neighbor's appearances in both $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$ to exploit the correlations between $u$ and $v$, resulting in four encoding sequences for $u$ and $v$ in total. Then, we divide each encoding sequence into multiple patches and feed all the patches into a Transformer for capturing long-term temporal dependencies. Finally, the outputs of the Transformer are averaged to derive time-aware representations of $u$ and $v$ at timestamp $t$ (i.e., $\bm{h}_u^t$ and $\bm{h}_v^t$), which can be applied in various downstream tasks, such as dynamic link prediction and dynamic node classification.

\textbf{Learning from Historical First-hop Interactions}. Unlike most previous methods that need to retrieve nodes' interactions from multiple hops, we only learn from the sequences of first-hop interactions of nodes, which turns the dynamic graph learning task into a simpler sequence learning problem. Mathematically, given an interaction $\left(u,v,t\right)$, for source node $u$ and destination node $v$, we respectively obtain the sequences that involve historical interactions of $u$ and $v$ before timestamp $t$ (including $u$ and $v$), which are denoted by $\mathcal{S}_u^t=\left\{\left(u,u^\prime,t^\prime\right) | t^\prime < t \right\} \cup \left\{\left(u^\prime,u,t^\prime\right) | t^\prime < t \right\}$ and $\mathcal{S}_v^t=\left\{\left(v,v^\prime,t^\prime\right) | t^\prime < t \right\} \cup \left\{\left(v^\prime,v,t^\prime\right) | t^\prime < t \right\}$.

\textbf{Encoding Neighbors, Links, and Time Intervals}. As illustrated in \secref{section-3}, a dynamic graph is often associated with the features of nodes and links. Therefore, for node $u$, we just need to retrieve the features of involved neighbors and links in sequence $\mathcal{S}_u^t$ based on the given features to represent their encodings, which are denoted by $\bm{X}_{u,N}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_N}$ and $\bm{X}_{u,E}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_E}$. Following \cite{DBLP:conf/iclr/XuRKKA20}, we learn the periodic temporal patterns by encoding the time interval $\Delta t^\prime = t - t^\prime$ via $\sqrt{\frac{1}{d_T}}\left[\cos\left(w_1 \Delta t^\prime \right), \sin\left(w_1 \Delta t^\prime \right), \cdots, \cos\left(w_{d_T} \Delta t^\prime \right), \sin \left(w_{d_T} \Delta t^\prime \right)\right]$, where $w_1,\cdots,w_{d_T}$ are trainable parameters. $d_T$ is the dimension of time interval encoding. Based on the above function, we get the time interval encodings of interactions in $\mathcal{S}_u^t$, that is, $\bm{X}_{u,T}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_T}$. Following the same process, we can also obtain the corresponding encodings for node $v$, which are denoted by $\bm{X}_{v,N}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times d_N}$, $\bm{X}_{v,E}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times d_E}$, and $\bm{X}_{v,T}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times d_T}$.

\textbf{Neighbor Co-occurrence Encoding}. Existing methods separately compute the temporal representations of node $u$ and $v$ without considering their correlations, which motivates us to present a neighbor co-occurrence encoding scheme to address this limitation. The main assumption is that the appearing frequency of a neighbor in a sequence indicates its importance, and the occurrences of a neighbor in the sequences of $u$ and $v$ (i.e., co-occurrence) could reflect the correlations between $u$ and $v$. That is to say, if $u$ and $v$ have more common historical neighbors in their sequences, they are more likely to have interactions in the future.

Formally, given the interaction sequences $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$, we count the occurrences of each neighbor in both $\mathcal{S}_u^t$ and $\mathcal{S}_v^t$, and derive a two-dimensional vector. By packing together the vectors of all the neighbors, we can get the neighbor co-occurrence features for $u$ and $v$, which are represented by $\bm{C}_{u}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times 2}$ and $\bm{C}_{v}^t \in \mathbb{R}^{|\mathcal{S}_v^t| \times 2}$. For example, suppose the historical neighbors of $u$ and $v$ are $\left\{a, b, a\right\}$ and $\left\{b, b, a, c\right\}$. The appearing frequencies of $a$, $b$, and $c$ in $u$/$v$'s historical interactions are 2/1, 1/2, and 0/1, respectively. Therefore, the neighbor co-occurrence features of $u$ and $v$ can be denoted by $\bm{C}_u^t=\left[\left[2, 1\right], \left[1, 2\right],\left[2, 1\right]\right]^\top$ and $\bm{C}_v^t=\left[\left[1, 2\right], \left[1, 2\right],\left[2, 1\right],\left[0, 1\right]\right]^\top$. Then, we apply a function $f\left(\cdot\right)$ to encode the neighbor co-occurrence features by 
\begin{equation}
\label{equ:node_co_occurrence_encoding}
\begin{split}
   \bm{X}_{*,C}^t = f\left(\bm{C}_*^t\left[:,0\right]\right) + f\left(\bm{C}_*^t\left[:,1\right]\right) \in \mathbb{R}^{|\mathcal{S}_*^t| \times d_C},
\end{split}
\end{equation}
where $*$ could be $u$ or $v$. The input and output dimensions of $f\left(\cdot\right)$ are 1 and $d_C$. In this paper, we implement $f\left(\cdot\right)$ by a two-layer perceptron with ReLU activation \cite{DBLP:conf/icml/NairH10}.
It is important to note that the neighbor co-occurrence encoding scheme is general and can be easily integrated into some dynamic graph learning methods for better results. We will demonstrate its generalizability in \secref{section-5-node_cooccurrence_generalizability}. 

% $g\left(s,\mathcal{S}\right)=|\left\{\right\}|$
% We then leverage a two-layered Multi-Layer Perceptron with ReLU activation function to encode the neighbor co-occurrence features, which is computed by
% \begin{equation}
% \label{equ:node_co_occurrence_encoding}
% \begin{split}
%    \bm{X}_{u,C}^t = MLP(\bm{C}_u^t\left[:,0\right]) + MLP(\bm{C}_u^t\left[:,1\right]),\\
%     \bm{X}_{v,C}^t = MLP(\bm{C}_v^t\left[:,0\right]) + MLP(\bm{C}_v^t\left[:,1\right]),
% \end{split}
% \end{equation}

\textbf{Patching Technique}. Instead of focusing on the interaction level, we divide the encoding sequence into multiple non-overlapping patches to break through the bottleneck of existing methods in capturing long-term temporal dependencies. Let $P$ denote the patch size, and thus each patch is composed of $P$ temporally adjacent interactions with flattened encodings and can preserve local temporal proximities. Take the patching of $\bm{X}_{u,N}^t \in \mathbb{R}^{|\mathcal{S}_u^t| \times d_N}$ as an example. $\bm{X}_{u,N}^t$ will be divided into $l_u^t =\lceil \frac{|\mathcal{S}_u^t|}{P} \rceil$ patches in total (note that we will pad $\bm{X}_{u,N}^t$ if its length $|\mathcal{S}_u^t|$ cannot be divided by $P$), and the patched encoding is represented by $\bm{M}_{u,N}^t \in \mathbb{R}^{l_u^t \times d_N \cdot P}$. Similarly, we can also get the patched encodings $\bm{M}_{u,E}^t \in \mathbb{R}^{l_u^t  \times d_E \cdot P}$, $\bm{M}_{u,T}^t \in \mathbb{R}^{l_u^t \times d_T \cdot P}$, $\bm{M}_{u,C}^t \in \mathbb{R}^{l_u^t  \times d_C \cdot P}$, 
$\bm{M}_{v,N}^t \in \mathbb{R}^{l_v^t  \times d_N \cdot P}$, $\bm{M}_{v,E}^t \in \mathbb{R}^{l_v^t  \times d_E \cdot P}$, $\bm{M}_{v,T}^t \in \mathbb{R}^{l_v^t  \times d_T \cdot P}$, and $\bm{M}_{v,C}^t \in \mathbb{R}^{l_v^t  \times d_C \cdot P}$. Note that when $|\mathcal{S}_u^t|$ becomes longer, we will correspondingly increase $P$, making the number of patches (i.e., $l_u^t$ and $l_v^t$) at a constant level to reduce the computational cost. 
% The patch technique allows DyGFormer to effectively utilize longer histories via preserving local temporal proximities. Additionally, it reduces computational complexity to a constant level that is independent of the input sequence length.
% preserve the temporal proximities in each patch; decrease the model complexity to a constant level that is agnostic to the input sequence length; and enable the model to access longer histories. These characteristics will be validated in \secref{section-5-investigation_patch_technique}.

\textbf{Transformer Encoder}. We first align the patched encodings to the same dimension $d$ with trainable weight $\bm{W}_* \in \mathbb{R}^{d_* \cdot P \times d}$ and $\bm{b}_* \in \mathbb{R}^{d}$ to obtain $\bm{Z}_{u,*}^t \in \mathbb{R}^{l_u^t \times d}$ and $\bm{Z}_{v,*}^t \in \mathbb{R}^{l_v^t \times d}$, where $*$ could be $N$, $E$, $T$ or $C$. To be specific, the alignments are realized by
\begin{equation}
    \label{equ:projection_layer}
    \bm{Z}_{u,*}^t = \bm{M}_{u,*}^t \bm{W}_* + \bm{b}_* \in \mathbb{R}^{l_u^t \times d}, \bm{Z}_{v,*}^t = \bm{M}_{v,*}^t \bm{W}_* + \bm{b}_* \in \mathbb{R}^{l_v^t \times d}.
\end{equation}
Then, we concatenate the aligned encodings of $u$ and $v$, and get $\bm{Z}_{u}^t = \bm{Z}_{u,N}^t \| \bm{Z}_{u,E}^t \| \bm{Z}_{u,T}^t \| \bm{Z}_{u,C}^t \in \mathbb{R}^{l_u^t \times 4d}$ and $\bm{Z}_{v}^t = \bm{Z}_{v,N}^t \| \bm{Z}_{v,E}^t \| \bm{Z}_{v,T}^t \| \bm{Z}_{v,C}^t \in \mathbb{R}^{l_v^t \times 4d}$. 

Next, we employ a Transformer encoder to capture the temporal dependencies, which is built by stacking $L$ Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) blocks. The residual connection \cite{DBLP:conf/cvpr/HeZRS16} is employed after every block. We follow \cite{DBLP:conf/iclr/DosovitskiyB0WZ21} by using GELU \cite{hendrycks2016gaussian} instead of ReLU \cite{DBLP:conf/icml/NairH10} between the two-layer perception in each FFN block and applying Layer Normalization (LN) \cite{ba2016layer} before each block rather than after.
% , which are slightly different from \cite{DBLP:conf/nips/VaswaniSPUJGKP17}. 
Instead of individually processing $\bm{Z}_{u}^t$ and $\bm{Z}_{v}^t$, our Transformer encoder takes the stacked $\bm{Z}^t=\left[\bm{Z}_{u}^t; \bm{Z}_{v}^t\right] \in \mathbb{R}^{(l_u^t + l_v^t) \times 4d}$ as inputs, aiming to learn the temporal dependencies within and across the sequences of $u$ and $v$. The calculation process is
% \begin{gather*} 
\begin{gather} 
\label{equ:transformer}
   \text{MSA}\left(\bm{Q}, \bm{K}, \bm{V}\right)= \text{Softmax}\left(\frac{\bm{Q} \bm{K}^\top}{\sqrt{d_k}}\right) \bm{V},\\
   \text{FFN}\left(\bm{O}, \bm{W}_1, \bm{b}_1, \bm{W}_2, \bm{b}_2\right)= \text{GELU}\left(\bm{O}\bm{W}_1+\bm{b}_1\right)\bm{W}_2 + \bm{b}_2,\\
   \bm{O}_i^{t,l} = \text{MSA}\left( \text{LN}(\bm{Z}^{t,l-1}) \bm{W}_{Q,i}^l, \text{LN}(\bm{Z}^{t,l-1}) \bm{W}_{K,i}^l, \text{LN}(\bm{Z}^{t,l-1}) \bm{W}_{V,i}^l \right),\\
   % \bm{O}^{t,l} = \text{CONCAT}\left(\bm{O}_1^{t,l},\cdots,\bm{O}_I^{t,l}\right)\bm{W}_O^l + \bm{Z}^{t,l-1},\\
   % \bm{O}^{t,l} = \left[\bm{O}_1^{t,l};\cdots;\bm{O}_I^{t,l}\right]\bm{W}_O^l + \bm{Z}^{t,l-1},\\
   \bm{O}^{t,l} = \left(\bm{O}_1^{t,l} \| \cdots \| \bm{O}_I^{t,l}\right)\bm{W}_O^l + \bm{Z}^{t,l-1},\\
   % \bm{O}^{t,l} = \|_{i = 1}^I \bm{O}_i^{t,l} \bm{W}_O^l + \bm{Z}^{t,l-1},\\
   \bm{Z}^{t,l} = \text{FFN}\left(\text{LN}\left(\bm{O}^{t,l}\right), \bm{W}_1^l, \bm{b}_1^l, \bm{W}_2^l, \bm{b}_2^l\right) + \bm{O}^{t,l}.
\end{gather}
% \end{gather*}
$\bm{W}_{Q,i}^l \in \mathbb{R}^{4d \times d_k}$, $\bm{W}_{K,i}^l \in \mathbb{R}^{4d \times d_k}$, $\bm{W}_{V,i}^l \in \mathbb{R}^{4d \times d_v}$, $\bm{W}_O^l \in \mathbb{R}^{I \cdot d_v \times 4d}$, $\bm{W}_1^l \in \mathbb{R}^{4d \times 16d}$, $\bm{b}_1^l \in \mathbb{R}^{16d}$, $\bm{W}_2^l \in \mathbb{R}^{16d \times 4d}$ and $\bm{b}_2^l \in \mathbb{R}^{4d}$ are trainable parameters at the $l$-th layer. We set $d_k=d_v=4d/I$ with $I$ as the number of attention heads. The input of the first layer is $\bm{Z}^{t,0} = \bm{Z}^t \in \mathbb{R}^{(l_u^t + l_v^t) \times 4d}$, and the output of the $L$-th layer is denoted by $\bm{H}^t=\bm{Z}^{t,L} \in \mathbb{R}^{(l_u^t + l_v^t) \times 4d}$.

\textbf{Time-aware Node Representation}. The time-aware representations of node $u$ and $v$ at timestamp $t$ are derived by averaging their related representations in $\bm{H}^t$ with an output layer,
\begin{equation}
\label{equ:final_temporal_representation}
\begin{split}
   \bm{h}_u^{t} & = \text{MEAN}\left(\bm{H}^t[:l_u^t,:]\right) \bm{W}_{out} + \bm{b}_{out} \in \mathbb{R}^{d_{out}},\\
   \bm{h}_v^{t} & = \text{MEAN}\left(\bm{H}^t[l_u^t:l_u^t + l_v^t,:]\right) \bm{W}_{out} + \bm{b}_{out} \in \mathbb{R}^{d_{out}},\\
\end{split}
\end{equation}
where $\bm{W}_{out} \in \mathbb{R}^{4d \times d_{out}}$ and $\bm{b}_{out} \in \mathbb{R}^{d_{out}}$ are trainable weights with $d_{out}$ as the output dimension.

\subsection{DyGLib: Unified Library for Continuous-Time Dynamic Graph Learning}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.5]{figures/DyGLib_procedure.jpg}
    \caption{DyGLib is equipped with \textcolor[RGB]{0,176,80}{standard training pipelines}, \textcolor[RGB]{255,0,0}{extensible coding interfaces}, and \textcolor[RGB]{46,117,182}{comprehensive evaluating protocols}. DyG denotes the abbreviation of Dynamic Graph.}
    \label{fig:procedure_DyGLib}
\end{figure}
We introduce a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating strategies to facilitate reproducible, scalable, and credible continuous-time dynamic graph learning research. The overall procedure of DyGLib is shown in \figref{fig:procedure_DyGLib}.


\textbf{Standard Training Pipelines}. To eliminate the influence of different training pipelines in previous works, we unify the data format, create a customized data loader and train all the methods with the same model trainers. Our standard training pipelines guarantee reproducible performance and enable users to quickly identify the key components of different models. Researchers only need to focus on designing the model architecture without considering other irrelevant implementation details.

\textbf{Extensible Coding Interfaces}. We provide extensible coding interfaces for the datasets and algorithms, which are all implemented by PyTorch. These scalable designs enable users to incorporate new datasets and popular models based on specific requirements, which can significantly reduce the usage difficulty for beginners and allow experts to conveniently validate new ideas. Currently, DyGLib has integrated thirteen datasets from various domains and nine continuous-time dynamic graph learning methods. Note that we also find some issues in the implementations of previous studies and have fixed them in DyGLib (see details in \secref{section-appendix-issues-existing-methods}).

\textbf{Comprehensive Evaluating Protocols}. DyGLib supports the commonly used downstream tasks for dynamic graph learning, including transductive/inductive dynamic link prediction and dynamic node classification tasks. Previous methods are mainly evaluated on the dynamic link prediction task with the random negative sampling strategy. However, many models can achieve saturation performance under such a strategy, making it hard to distinguish more advanced designs. For more reliable comparisons, we adopt three strategies (i.e., random, historical, and inductive negative sampling strategies) in \cite{poursafaei2022towards} to comprehensively evaluate the model performance on the dynamic link prediction task. The dynamic node classification task is also included for providing additional results.

% $\mathcal{S}_u=\left\{\left(u^\prime, t^\prime\right) | \left(\left(u,u^\prime,t^\prime\right) \land t^\prime < t \right) \lor \left(\left(u^\prime,u,t^\prime\right) \land t^\prime < t \right)  \right\}$ and $\mathcal{S}_v=\left\{\left(v^\prime, t^\prime\right) | \left(\left(v,v^\prime,t^\prime\right) \land t^\prime < t \right) \lor \left(\left(v^\prime,v,t^\prime\right) \land t^\prime < t \right)  \right\}$. 

% $\mathcal{S}_u=\left\{\left(u^\prime, t^\prime\right) | \left(u^\prime,v^\prime,t^\prime\right) \land t^\prime < t \land \left(u^\prime == u or v^\prime == u\right) \right\}$

% $\mathcal{S}_u^t=\left\{\left(u,u^\prime,t^\prime\right) \lor \left(u^\prime,u,t^\prime\right) | t^\prime < t \right\}$ and $\mathcal{S}_v^t=\left\{\left(v,v^\prime,t^\prime\right) \lor \left(v^\prime,v,t^\prime\right) | t^\prime < t \right\}$.

% \begin{equation}
% \label{equ:transformer}
% \begin{split}
%    MSA\left(\bm{Q}, \bm{K}, \bm{V}\right)= Softmax\left(\frac{\bm{Q} \bm{K}^\top}{\sqrt{d_k}}\right) \bm{V},\\
%    FFN\left(\bm{O}\right)= GELU\left(\bm{O}\bm{W}_1+\bm{b}_1\right)\bm{W}_2 + \bm{b}_2,\\
%    \bm{O}_i^{t,l} = MSA\left(LN(\bm{Z}^{t,l-1}) \bm{W}_{Q,i}^l, LN(\bm{Z}^{t,l-1}) \bm{W}_{K,i}^l, LN(\bm{Z}^{t,l-1}) \bm{W}_{V,i}^l \right),\\
%    \bm{O}^{t,l} = Concat\left(\bm{O}_1^{t,l},\cdots,\bm{O}_I^{t,l}\right)\bm{W}_O^l + \bm{Z}^{t,l-1},\\
%    \bm{H}^{t,l} = FFN\left(LN\left(\bm{O}^{t,l}\right)\right) + \bm{O}^{t,l}.
% \end{split}
% \end{equation}

% \begin{equation}
%     MSA\left(\bm{Q}, \bm{K}, \bm{V}\right)= Softmax\left(\frac{\bm{Q} \bm{K}^\top}{\sqrt{d_k}}\right) \bm{V},
% \end{equation}
% \begin{equation}
%     FFN\left(\bm{O}\right)= GELU\left(\bm{O}\bm{W}_1+\bm{b}_1\right)\bm{W}_2 + \bm{b}_2,
% \end{equation}
% \begin{equation}
%     \bm{O}_i^{t,l} = MSA\left(LN(\bm{Z}^{t,l-1}) \bm{W}_{Q,i}^l, LN(\bm{Z}^{t,l-1}) \bm{W}_{K,i}^l, LN(\bm{Z}^{t,l-1}) \bm{W}_{V,i}^l \right),
% \end{equation}
% \begin{equation}
%     \bm{O}^{t,l} = Concat\left(\bm{O}_1^{t,l},\cdots,\bm{O}_I^{t,l}\right)\bm{W}_O^l + \bm{Z}^{t,l-1},
% \end{equation}
% \begin{equation}
%     \bm{H}^{t,l} = FFN\left(LN\left(\bm{O}^{t,l}\right)\right) + \bm{O}^{t,l}.
% \end{equation}
