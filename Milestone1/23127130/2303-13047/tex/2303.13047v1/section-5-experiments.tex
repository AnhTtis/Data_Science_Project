\section{Experiments}
\label{section-5}
In this section, extensive experiments are conducted. We report the results of various models using DyGLib, which can be directly referenced in the follow-up research. We also demonstrate the superiority of DyGFormer over existing methods and give an in-depth analysis of the neighbor co-occurrence encoding scheme and the patching technique.

\subsection{Experimental Settings}
\textbf{Datasets and Baselines}. We experiment with thirteen datasets (Wikipedia, Reddit, MOOC, LastFM, Enron, Social Evo., UCI, Flights, Can. Parl., US Legis., UN Trade, UN Vote, and Contact), which are collected by \cite{poursafaei2022towards} and cover diverse domains. Details of the datasets are shown in \secref{section-appendix-descriptions_datasets}. We compare DyGFormer with eight popular continuous-time dynamic graph learning baselines that are based on graph convolutions, memory networks, random walks, and sequential models, including JODIE \cite{DBLP:conf/kdd/KumarZL19}, DyRep \cite{DBLP:conf/iclr/TrivediFBZ19}, TGAT \cite{DBLP:conf/iclr/XuRKKA20}, TGN \cite{DBLP:journals/corr/abs-2006-10637}, CAWN \cite{DBLP:conf/iclr/WangCLL021}, EdgeBank \cite{poursafaei2022towards}, TCL \cite{DBLP:journals/corr/abs-2105-07944}, and GraphMixer \cite{cong2023do}. We give the descriptions of baselines in \secref{section-appendix-descriptions_baselines}.

\textbf{Evaluation Tasks and Metrics}.
We closely follow \cite{DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637,DBLP:conf/iclr/WangCLL021,poursafaei2022towards} by evaluating the model performance for dynamic link prediction, which predicts the probability of a link occurring between two given nodes at a specific time. This task has two settings: the transductive setting aims to predict future links between nodes that are observed during training, while the inductive setting predicts the future links between unseen nodes. We utilize a multi-layer perceptron to take the concatenated representations of two nodes as inputs and return the probability of a link as the output. Average Precision (AP) and Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are adopted as the evaluation metrics. To provide more comprehensive comparisons, we adopt three negative sampling strategies in \cite{poursafaei2022towards} for evaluating dynamic link prediction, including random (rnd), historical (hist), and inductive (ind) negative sampling strategies, where the latter two strategies are more challenging. Please refer to \cite{poursafaei2022towards} for more details. We also follow \cite{DBLP:conf/iclr/XuRKKA20,DBLP:journals/corr/abs-2006-10637} to conduct the dynamic node classification task, which estimates the state of a node in a given interaction at a specific time. We employ another multi-layer perceptron to map the node representations to the labels. We use AUC-ROC as the evaluation metric due to the label imbalance. For all the tasks, we chronologically split each dataset with the ratio of 70\%, 15\%, and 15\% for training, validation, and testing. 

\textbf{Model Configurations}. For baselines, in addition to following their official settings, we also perform an exhaustive grid search to find the optimal configurations of some critical hyperparameters for more reliable comparisons. As DyGFormer can access longer histories, we vary each node's input sequence length from 32 to 4096 by a factor of 2. To keep the computational complexity at a constant level that is irrelevant to the input length, we correspondingly increase the patch size from 1 to 128. Please see \secref{section-appendix-configurations} for the detailed configurations of different models.

\textbf{Implementation Details}. Adam \cite{DBLP:journals/corr/KingmaB14} is adopted as the optimizer. We train all the models for 100 epochs and use the early stopping strategy with patience of 20. We select the model that achieves the best performance on the validation set for testing. We set the learning rate and batch size to 0.0001 and 200 for all the methods on all the datasets. We run the methods five times with seeds from 0 to 4 and report the average performance to eliminate deviations. The experiments are conducted on an Ubuntu machine equipped with one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz with 16 physical cores. The GPU device is NVIDIA Tesla T4 with 15 GB memory. 

\subsection{Performance Comparisons and Discussions}
Due to space limitations, we report the performance of different methods on the AP metric for transductive dynamic link prediction with three negative sampling strategies in \tabref{tab:average_precision_transductive_dynamic_link_prediction}. The best and second-best results are emphasized by \textbf{bold} and \underline{underlined} fonts. Note that the results are multiplied by 100 for a better display layout. Please refer to \secref{section-appendix-numerical-performance-dynamic-link-prediction} for the results of AP for inductive dynamic link prediction as well as AUC-ROC for transductive and inductive link prediction tasks. Note that EdgeBank can be only evaluated for transductive dynamic link prediction, so its results under the inductive setting are not presented. From \tabref{tab:average_precision_transductive_dynamic_link_prediction} and \secref{section-appendix-numerical-performance-dynamic-link-prediction}, we have two main observations. 

\begin{table}[!htbp]
\centering
\caption{AP for transductive dynamic link prediction with random, history, and inductive negative sampling strategies. NSS is the abbreviation of Negative Sampling Strategies.}
\label{tab:average_precision_transductive_dynamic_link_prediction}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{1.05mm}
{
\begin{tabular}{c|c|ccccccccc}
\hline
NSS                    & Datasets    & JODIE        & DyRep        & TGAT         & TGN          & CAWN         & EdgeBank     & TCL          & GraphMixer   & DyGFormer    \\ \hline
\multirow{14}{*}{rnd}  & Wikipedia   & 96.50 ± 0.14 & 94.86 ± 0.06 & 96.94 ± 0.06 & 98.45 ± 0.06 & \underline{98.76 ± 0.03} & 90.37 ± 0.00 & 96.47 ± 0.16 & 97.25 ± 0.03 & \textbf{99.03 ± 0.02} \\
                       & Reddit      & 98.31 ± 0.14 & 98.22 ± 0.04 & 98.52 ± 0.02 & 98.63 ± 0.06 & \underline{99.11 ± 0.01} & 94.86 ± 0.00 & 97.53 ± 0.02 & 97.31 ± 0.01 & \textbf{99.22 ± 0.01} \\
                       & MOOC        & 80.23 ± 2.44 & 81.97 ± 0.49 & 85.84 ± 0.15 & \textbf{89.15 ± 1.60} & 80.15 ± 0.25 & 57.97 ± 0.00 & 82.38 ± 0.24 & 82.78 ± 0.15 & \underline{87.52 ± 0.49} \\
                       & LastFM      & 70.85 ± 2.13 & 71.92 ± 2.21 & 73.42 ± 0.21 & 77.07 ± 3.97 & \underline{86.99 ± 0.06} & 79.29 ± 0.00 & 67.27 ± 2.16 & 75.61 ± 0.24 & \textbf{93.00 ± 0.12} \\
                       & Enron       & 84.77 ± 0.30 & 82.38 ± 3.36 & 71.12 ± 0.97 & 86.53 ± 1.11 & \underline{89.56 ± 0.09} & 83.53 ± 0.00 & 79.70 ± 0.71 & 82.25 ± 0.16 & \textbf{92.47 ± 0.12} \\
                       & Social Evo. & 89.89 ± 0.55 & 88.87 ± 0.30 & 93.16 ± 0.17 & \underline{93.57 ± 0.17} & 84.96 ± 0.09 & 74.95 ± 0.00 & 93.13 ± 0.16 & 93.37 ± 0.07 & \textbf{94.73 ± 0.01} \\
                       & UCI         & 89.43 ± 1.09 & 65.14 ± 2.30 & 79.63 ± 0.70 & 92.34 ± 1.04 & \underline{95.18 ± 0.06} & 76.20 ± 0.00 & 89.57 ± 1.63 & 93.25 ± 0.57 & \textbf{95.79 ± 0.17} \\
                       & Flights     & 95.60 ± 1.73 & 95.29 ± 0.72 & 94.03 ± 0.18 & 97.95 ± 0.14 & \underline{98.51 ± 0.01} & 89.35 ± 0.00 & 91.23 ± 0.02 & 90.99 ± 0.05 & \textbf{98.91 ± 0.01} \\
                       & Can. Parl.  & 69.26 ± 0.31 & 66.54 ± 2.76 & 70.73 ± 0.72 & 70.88 ± 2.34 & 69.82 ± 2.34 & 64.55 ± 0.00 & 68.67 ± 2.67 & \underline{77.04 ± 0.46} & \textbf{97.36 ± 0.45} \\
                       & US Legis.   & 75.05 ± 1.52 & \underline{75.34 ± 0.39} & 68.52 ± 3.16 & \textbf{75.99 ± 0.58} & 70.58 ± 0.48 & 58.39 ± 0.00 & 69.59 ± 0.48 & 70.74 ± 1.02 & 71.11 ± 0.59 \\
                       & UN Trade    & 64.94 ± 0.31 & 63.21 ± 0.93 & 61.47 ± 0.18 & 65.03 ± 1.37 & \underline{65.39 ± 0.12} & 60.41 ± 0.00 & 62.21 ± 0.03 & 62.61 ± 0.27 & \textbf{66.46 ± 1.29} \\
                       & UN Vote     & \underline{63.91 ± 0.81} & 62.81 ± 0.80 & 52.21 ± 0.98 & \textbf{65.72 ± 2.17} & 52.84 ± 0.10 & 58.49 ± 0.00 & 51.90 ± 0.30 & 52.11 ± 0.16 & 55.55 ± 0.42 \\
                       & Contact     & 95.31 ± 1.33 & 95.98 ± 0.15 & 96.28 ± 0.09 & \underline{96.89 ± 0.56} & 90.26 ± 0.28 & 92.58 ± 0.00 & 92.44 ± 0.12 & 91.92 ± 0.03 & \textbf{98.29 ± 0.01} \\ \cline{2-11} 
                       & Avg. Rank   & 5.08         & 5.85         & 5.69         & \underline{2.54}         & 4.31         & 7.54         & 6.92         & 5.46         & \textbf{1.62}         \\ \hline
\multirow{14}{*}{hist} & Wikipedia   & 83.01 ± 0.66 & 79.93 ± 0.56 & 87.38 ± 0.22 & 86.86 ± 0.33 & 71.21 ± 1.67 & 73.35 ± 0.00 & \underline{89.05 ± 0.39} & \textbf{90.90 ± 0.10} & 82.23 ± 2.54 \\
                       & Reddit      & 80.03 ± 0.36 & 79.83 ± 0.31 & 79.55 ± 0.20 & \underline{81.22 ± 0.61} & 80.82 ± 0.45 & 73.59 ± 0.00 & 77.14 ± 0.16 & 78.44 ± 0.18 & \textbf{81.57 ± 0.67} \\
                       & MOOC        & 78.94 ± 1.25 & 75.60 ± 1.12 & 82.19 ± 0.62 & \textbf{87.06 ± 1.93} & 74.05 ± 0.95 & 60.71 ± 0.00 & 77.06 ± 0.41 & 77.77 ± 0.92 & \underline{85.85 ± 0.66} \\
                       & LastFM      & 74.35 ± 3.81 & 74.92 ± 2.46 & 71.59 ± 0.24 & \underline{76.87 ± 4.64} & 69.86 ± 0.43 & 73.03 ± 0.00 & 59.30 ± 2.31 & 72.47 ± 0.49 & \textbf{81.57 ± 0.48} \\
                       & Enron       & 69.85 ± 2.70 & 71.19 ± 2.76 & 64.07 ± 1.05 & 73.91 ± 1.76 & 64.73 ± 0.36 & \underline{76.53 ± 0.00} & 70.66 ± 0.39 & \textbf{77.98 ± 0.92} & 75.63 ± 0.73 \\
                       & Social Evo. & 87.44 ± 6.78 & 93.29 ± 0.43 & \underline{95.01 ± 0.44} & 94.45 ± 0.56 & 85.53 ± 0.38 & 80.57 ± 0.00 & 94.74 ± 0.31 & 94.93 ± 0.31 & \textbf{97.38 ± 0.14} \\
                       & UCI         & 75.24 ± 5.80 & 55.10 ± 3.14 & 68.27 ± 1.37 & 80.43 ± 2.12 & 65.30 ± 0.43 & 65.50 ± 0.00 & 80.25 ± 2.74 & \textbf{84.11 ± 1.35} & \underline{82.17 ± 0.82} \\
                       & Flights     & 66.48 ± 2.59 & 67.61 ± 0.99 & \textbf{72.38 ± 0.18} & 66.70 ± 1.64 & 64.72 ± 0.97 & 70.53 ± 0.00 & 70.68 ± 0.24 & \underline{71.47 ± 0.26} & 66.59 ± 0.49 \\
                       & Can. Parl.  & 51.79 ± 0.63 & 63.31 ± 1.23 & 67.13 ± 0.84 & 68.42 ± 3.07 & 66.53 ± 2.77 & 63.84 ± 0.00 & 65.93 ± 3.00 & \underline{74.34 ± 0.87} & \textbf{97.00 ± 0.31} \\
                       & US Legis.   & 51.71 ± 5.76 & \textbf{86.88 ± 2.25} & 62.14 ± 6.60 & 74.00 ± 7.57 & 68.82 ± 8.23 & 63.22 ± 0.00 & 80.53 ± 3.95 & 81.65 ± 1.02 & \underline{85.30 ± 3.88} \\
                       & UN Trade    & 61.39 ± 1.83 & 59.19 ± 1.07 & 55.74 ± 0.91 & 58.44 ± 5.51 & 55.71 ± 0.38 & \textbf{81.32 ± 0.00} & 55.90 ± 1.17 & 57.05 ± 1.22 & \underline{64.41 ± 1.40} \\
                       & UN Vote     & \underline{70.02 ± 0.81} & 69.30 ± 1.12 & 52.96 ± 2.14 & 69.37 ± 3.93 & 51.26 ± 0.04 & \textbf{84.89 ± 0.00} & 52.30 ± 2.35 & 51.20 ± 1.60 & 60.84 ± 1.58 \\
                       & Contact     & 95.31 ± 2.13 & \underline{96.39 ± 0.20} & 96.05 ± 0.52 & 93.05 ± 2.35 & 84.16 ± 0.49 & 88.81 ± 0.00 & 93.86 ± 0.21 & 93.36 ± 0.41 & \textbf{97.57 ± 0.06} \\ \cline{2-11} 
                       & Avg. Rank   & 5.46         & 5.08         & 5.08         & \underline{3.85}         & 7.54         & 5.92         & 5.46         & 4.00         & \textbf{2.62}         \\ \hline
\multirow{14}{*}{ind}  & Wikipedia   & 75.65 ± 0.79 & 70.21 ± 1.58 & \underline{87.00 ± 0.16} & 85.62 ± 0.44 & 74.06 ± 2.62 & 80.63 ± 0.00 & 86.76 ± 0.72 & \textbf{88.59 ± 0.17} & 78.29 ± 5.38 \\
                       & Reddit      & 86.98 ± 0.16 & 86.30 ± 0.26 & 89.59 ± 0.24 & 88.10 ± 0.24 & \textbf{91.67 ± 0.24} & 85.48 ± 0.00 & 87.45 ± 0.29 & 85.26 ± 0.11 & \underline{91.11 ± 0.40} \\
                       & MOOC        & 65.23 ± 2.19 & 61.66 ± 0.95 & 75.95 ± 0.64 & \underline{77.50 ± 2.91} & 73.51 ± 0.94 & 49.43 ± 0.00 & 74.65 ± 0.54 & 74.27 ± 0.92 & \textbf{81.24 ± 0.69} \\
                       & LastFM      & 62.67 ± 4.49 & 64.41 ± 2.70 & 71.13 ± 0.17 & 65.95 ± 5.98 & 67.48 ± 0.77 & \textbf{75.49 ± 0.00} & 58.21 ± 0.89 & 68.12 ± 0.33 & \underline{73.97 ± 0.50} \\
                       & Enron       & 68.96 ± 0.98 & 67.79 ± 1.53 & 63.94 ± 1.36 & 70.89 ± 2.72 & \underline{75.15 ± 0.58} & 73.89 ± 0.00 & 71.29 ± 0.32 & 75.01 ± 0.79 & \textbf{77.41 ± 0.89} \\
                       & Social Evo. & 89.82 ± 4.11 & 93.28 ± 0.48 & 94.84 ± 0.44 & \underline{95.13 ± 0.56} & 88.32 ± 0.27 & 83.69 ± 0.00 & 94.90 ± 0.36 & 94.72 ± 0.33 & \textbf{97.68 ± 0.10} \\
                       & UCI         & 65.99 ± 1.40 & 54.79 ± 1.76 & 68.67 ± 0.84 & 70.94 ± 0.71 & 64.61 ± 0.48 & 57.43 ± 0.00 & \underline{76.01 ± 1.11} & \textbf{80.10 ± 0.51} & 72.25 ± 1.71 \\
                       & Flights     & 69.07 ± 4.02 & 70.57 ± 1.82 & \underline{75.48 ± 0.26} & 71.09 ± 2.72 & 69.18 ± 1.52 & \textbf{81.08 ± 0.00} & 74.62 ± 0.18 & 74.87 ± 0.21 & 70.92 ± 1.78 \\
                       & Can. Parl.  & 48.42 ± 0.66 & 58.61 ± 0.86 & 68.82 ± 1.21 & 65.34 ± 2.87 & 67.75 ± 1.00 & 62.16 ± 0.00 & 65.85 ± 1.75 & \underline{69.48 ± 0.63} & \textbf{95.44 ± 0.57} \\
                       & US Legis.   & 50.27 ± 5.13 & \textbf{83.44 ± 1.16} & 61.91 ± 5.82 & 67.57 ± 6.47 & 65.81 ± 8.52 & 64.74 ± 0.00 & 78.15 ± 3.34 & 79.63 ± 0.84 & \underline{81.25 ± 3.62} \\
                       & UN Trade    & 60.42 ± 1.48 & 60.19 ± 1.24 & 60.61 ± 1.24 & 61.04 ± 6.01 & \underline{62.54 ± 0.67} & \textbf{72.97 ± 0.00} & 61.06 ± 1.74 & 60.15 ± 1.29 & 55.79 ± 1.02 \\
                       & UN Vote     & \textbf{67.79 ± 1.46} & 67.53 ± 1.98 & 52.89 ± 1.61 & \underline{67.63 ± 2.67} & 52.19 ± 0.34 & 66.30 ± 0.00 & 50.62 ± 0.82 & 51.60 ± 0.73 & 51.91 ± 0.84 \\
                       & Contact     & 93.43 ± 1.78 & 94.18 ± 0.10 & \underline{94.35 ± 0.48} & 90.18 ± 3.28 & 89.31 ± 0.27 & 85.20 ± 0.00 & 91.35 ± 0.21 & 90.87 ± 0.35 & \textbf{94.75 ± 0.28} \\ \cline{2-11} 
                       & Avg. Rank   & 6.62         & 6.38         & \underline{4.15}         & 4.38         & 5.46         & 5.62         & 4.69         & 4.46         & \textbf{3.23}         \\ \hline
\end{tabular}
}
}
\end{table}

(\romannumeral1) DyGFormer outperforms existing methods in most cases and achieves an average rank of 2.49/2.69 on AP/AUC-ROC for transductive dynamic link prediction and 2.69/2.56 on AP/AUC-ROC for inductive dynamic link prediction across the three negative sampling strategies on thirteen datasets. We summarize the superiority of DyGFormer in two aspects. Firstly, DyGFormer presents the neighbor co-occurrence encoding scheme to exploit the correlations of the source node and destination node, which are often informative in predicting their future linked probability. Secondly, the patching technique allows DyGFormer to effectively leverage longer histories and capture long-term temporal dependencies. As shown in Table \tabref{tab:num_neighbors_configuration}, the input sequence lengths of DyGFormer are 256+ on half of the datasets, which are significantly longer than those of the baselines, demonstrating that DyGFormer can benefit from longer sequences. 
% From \tabref{tab:num_neighbors_configuration}, we observe the input sequence lengths of DyGFormer are 256+ on half of the datasets, which are much longer than those of baselines, demonstrating DyGFormer can benefit from longer sequences. 
% We will give a deeper analysis of the above two designs in \secref{section-5-node_cooccurrence_generalizability} and \secref{section-5-investigation_patch_technique}. 

\begin{wraptable}{R}{8.0cm}
\vspace{-10pt}
\caption{AUC-ROC for dynamic node classification.}
\label{tab:auc_roc_dynamic_node_classification}
\setlength{\tabcolsep}{1.1mm}
{
\begin{tabular}{c|cc|c}
\hline
Methods    & Wikipedia                & Reddit                   & \begin{tabular}[c]{@{}c@{}}Avg.\\ Rank\end{tabular} \\ \hline
JODIE      & \textbf{0.8899 ± 0.0105} & 0.6037 ± 0.0258          & 4.50                                                \\
DyRep      & 0.8639 ± 0.0098          & 0.6372 ± 0.0132          & 5.00                                                \\
TGAT       & 0.8409 ± 0.0127          & \textbf{0.7004 ± 0.0109} & \underline{4.00}                                                \\
TGN        & 0.8638 ± 0.0234          & 0.6327 ± 0.0090          & 6.00                                                \\
CAWN       & 0.8488 ± 0.0133          & 0.6634 ± 0.0178          & 5.00                                                \\
TCL        & 0.7783 ± 0.0213          & {\underline{0.6887 ± 0.0215}}    & 5.00                                                \\
GraphMixer & 0.8680 ± 0.0079          & 0.6422 ± 0.0332          & \underline{4.00}                                                \\
DyGFormer  & {\underline{0.8744 ± 0.0108}}    & 0.6800 ± 0.0174          & \textbf{2.50}                                                \\ \hline
\end{tabular}
}
\vspace{-10pt}
\end{wraptable}

(\romannumeral2) Some of our findings differ from previous reports. For instance, the performance of some baselines can be significantly improved by properly setting certain hyperparameters. Additionally, some methods would obtain worse results after we fix the problems or make adaptions in their implementations. More explanations can be found in \secref{section-appendix-inconsistent-observations}. These observations highlight the importance of rigorously evaluating different methods by a unified library and verify the necessity of introducing DyGLib to facilitate the development of dynamic graph learning.

In addition, we present the results of various methods for dynamic node classification on Wikipedia and Reddit (the only two datasets with dynamic labels) in \tabref{tab:auc_roc_dynamic_node_classification}. We observe that DyGFormer obtains better performance than most baselines and achieves an impressive average rank of 2.50 among them, which shows the superiority of DyGFormer once again. Due to space limitations, we only report the AP metric for transductive dynamic link prediction with the random sampling strategy in the subsequent sections. Similar trends could also be observed in other situations (e.g., AUC-ROC metric, inductive setting, historical/inductive negative sampling strategy).

\subsection{Ablation Study}\label{section-5-ablation_study}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/ablation_study.jpg}
    \caption{Ablation study of the components in DyGFormer.}
    \label{fig:ablation_study}
\end{figure}

We further validate the effectiveness of some designs in DyGFormer via an ablation study, including the usage of \textbf{N}eighbor \textbf{Co}-occurrence \textbf{E}ncoding (NCoE), the usage of \textbf{T}ime \textbf{E}ncoding (TE), and the \textbf{Mix}ing of the sequence of \textbf{S}ource node and \textbf{D}estination node (MixSD). We respectively remove these modules and denote the remaining parts as w/o NCoE, w/o TE, and w/o MixSD. We also \textbf{Sep}arately encode the \textbf{N}eighbor \textbf{O}ccurrence in the source node's or destination node's sequence and denote this variant as w/ SepNO. We report the performance of different variants on MOOC, Social Evo., UCI, and UN Trade datasets from four domains in \figref{fig:ablation_study}. 
We find that DyGFormer usually performs best when using all the components, and the results would be worse when any component is removed. The neighbor co-occurrence encoding scheme has the most significant impact on the performance as it effectively captures correlations between nodes. Separately encoding neighbor occurrences or encoding the temporal information could also improve performance. Mixing the sequences of the source node and destination node causes relatively minor improvements due to the usage of the neighbor co-occurrence encoding scheme because both of them aim to explore the node correlations.

\subsection{Generalizability of Neighbor Co-occurrence Encoding}\label{section-5-node_cooccurrence_generalizability}
Our neighbor co-occurrence encoding scheme is general and can be incorporated into sequential model-based dynamic graph learning methods, such as TCL and GraphMixer. Therefore, we integrate the neighbor co-occurrence encoding with the inputs of TCL and GraphMixer and show the performance in \tabref{tab:generalizability_neighbor_co_occurrence_encoding}. 
% Therefore, for TCL and GraphMixer, we integrate the neighbor co-occurrence encoding with their inputs via combination and addition, and report better performance of these two implementations.
We could find that TCL and GraphMixer usually yield better results when the neighbor co-occurrence encoding is employed, which indicates the effectiveness and versatility of the proposed encoding scheme, and also highlights the importance of capturing correlations between nodes. Additionally, since both TCL and DyGFormer are built upon the Transformer architecture, they achieve similar performance on datasets that enjoy shorter input sequences (e.g., Wikipedia, Reddit, Social Evo., UCI, Contact), where the patching technique in DyGFormer contributes little. However, when datasets exhibit long-term temporal dependencies, the performance gap between TCL and DyGFormer would become more significant.

\begin{table}[!htbp]
\centering
\caption{AP for different methods when equipped with the neighbor co-occurrence encoding.}
\label{tab:generalizability_neighbor_co_occurrence_encoding}
\resizebox{1.01\textwidth}{!}
{
\setlength{\tabcolsep}{1.05mm}
{
\begin{tabular}{c|ccccccc}
\hline
\multirow{2}{*}{Datasets} & \multicolumn{3}{c|}{TCL}                               & \multicolumn{3}{c|}{GraphMixer}                        & \multirow{2}{*}{DyGFormer} \\ \cline{2-7}
                          & Original & w/ NCoE & \multicolumn{1}{c|}{Improvements} & Original & w/ NCoE & \multicolumn{1}{c|}{Improvements} &                            \\ \hline
Wikipedia                 & 0.9647   & \textbf{0.9909}  & \multicolumn{1}{c|}{2.72\%}       & 0.9725   & 0.9790  & \multicolumn{1}{c|}{0.67\%}       & \underline{0.9903}                     \\
Reddit                    & 0.9753   & \underline{0.9904}  & \multicolumn{1}{c|}{1.55\%}       & 0.9731   & 0.9763  & \multicolumn{1}{c|}{0.33\%}       & \textbf{0.9922}                     \\
MOOC                      & 0.8238   & \underline{0.8692}  & \multicolumn{1}{c|}{5.51\%}       & 0.8278   & 0.8358  & \multicolumn{1}{c|}{0.97\%}       & \textbf{0.8752}                     \\
LastFM                    & 0.6727   & \underline{0.8402}  & \multicolumn{1}{c|}{24.90\%}      & 0.7561   & 0.7648  & \multicolumn{1}{c|}{1.15\%}       & \textbf{0.9300}                     \\
Enron                     & 0.7970   & \underline{0.9018}  & \multicolumn{1}{c|}{13.15\%}      & 0.8225   & 0.8883  & \multicolumn{1}{c|}{8.00\%}       & \textbf{0.9247}                     \\
Social Evo.               & 0.9313   & 0.9406  & \multicolumn{1}{c|}{1.00\%}       & 0.9337   & \underline{0.9437}  & \multicolumn{1}{c|}{1.07\%}       & \textbf{0.9473}                     \\
UCI                       & 0.8957   & \underline{0.9469}  & \multicolumn{1}{c|}{5.72\%}       & 0.9325   & 0.9348  & \multicolumn{1}{c|}{0.25\%}       & \textbf{0.9579}                     \\
Flights                   & 0.9123   & \underline{0.9771}  & \multicolumn{1}{c|}{7.10\%}       & 0.9099   & 0.9690  & \multicolumn{1}{c|}{6.50\%}       & \textbf{0.9891}                     \\
Can. Parl.                & 0.6867   & 0.6934  & \multicolumn{1}{c|}{0.98\%}       & \underline{0.7704}   & 0.7638  & \multicolumn{1}{c|}{-0.86\%}      & \textbf{0.9736}                     \\
US Legis.                 & 0.6959   & 0.6947  & \multicolumn{1}{c|}{-0.17\%}      & \underline{0.7074}   & 0.7026  & \multicolumn{1}{c|}{-0.68\%}      & \textbf{0.7111}                     \\
UN Trade                  & 0.6221   & \underline{0.6346}  & \multicolumn{1}{c|}{2.01\%}       & 0.6261   & 0.6277  & \multicolumn{1}{c|}{0.26\%}       & \textbf{0.6646}                    \\
UN Vote                   & 0.5190   & 0.5152  & \multicolumn{1}{c|}{-0.73\%}      & 0.5211   & \underline{0.5213}  & \multicolumn{1}{c|}{0.04\%}       & \textbf{0.5555}                     \\
Contact                   & 0.9244   & \underline{0.9798}  & \multicolumn{1}{c|}{5.99\%}       & 0.9192   & 0.9794  & \multicolumn{1}{c|}{6.55\%}       & \textbf{0.9829}                     \\ \hline
Avg. Rank                 & 4.62     & \underline{2.62}    & ---                               & 3.85     & 2.85    & ---                                & \textbf{1.08}                       \\ \hline
\end{tabular}
}
}
\end{table}
 

\subsection{Advantages of Patching Technique}\label{section-5-investigation_patch_technique}
We aim to validate the advantages of the patching technique in (\romannumeral1) preserving the local temporal proximities to help DyGFormer effectively benefit from longer histories and (\romannumeral2) efficiently reducing the computational complexity to a constant level that is agnostic to the input sequence length. We conduct experiments on LastFM and Can. Parl. since these two datasets tend to achieve improvements from longer historical records. For baselines, we sample more neighbors or perform more causal anonymous walks to make them access longer histories (starts from 32). The results are depicted in \figref{fig:baselines_varying_input_lengths}, where the x-axis is represented by a logarithmic scale with base 2. We also plot the performance of baselines with the optimal length by unconnected points according to \tabref{tab:num_neighbors_configuration}. Note that some results of baselines are not shown as they raise the out-of-memory error when the lengths are longer. For example, TGAT is only computationally feasible when extending the input length to 32, resulting in two discrete points with length 20 (the optimal length) and 32.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/baselines_varying_input_lengths.jpg}
    \caption{Performance of different methods with varying input lengths.}
    \label{fig:baselines_varying_input_lengths}
\end{figure}

From \figref{fig:baselines_varying_input_lengths}, we could conclude that: (\romannumeral1) most baselines (excluding CAWN) perform worse when the input lengths become longer, indicating they lack the ability in capturing long-term temporal dependencies; (\romannumeral2) the baselines usually encounter expensive computational costs when computing on longer histories. Although memory network-based methods (i.e., DyRep and TGN) can handle longer histories with affordable computational costs, they cannot benefit from longer histories due to the staleness or vanishing/exploding gradient issues; (\romannumeral3) DyGFormer consistently achieves gains from longer sequences, demonstrating the advantages of the patching technique in preserving local temporal proximities and enabling effective access to longer histories. 

We also compare the running time and memory usage during the training process of DyGFormer with and without the patching technique when using input sequences of the same length. The results are shown in \tabref{tab:comparison_training_time_memory_usage_with_without_patching}. We could observe that the patching technique efficiently reduces model training costs in both time and space, and allows DyGFormer to attend to longer histories. As the input sequence length increases, the reductions become more significant. We also find that when equipped with the patching technique, DyGFormer achieves an average improvement of 0.31\% and 0.74\% in performance on LastFM and Can. Parl., compared to DyGFormer without patching. This observation further demonstrates the advantage of the patching technique in leveraging the local temporal proximities for better results.

\begin{table}[!htbp]
\centering
\caption{Comparisons of running time and memory usage of DyGFormer with and without the patching technique, where OOM stands for Out-Of-Memory.}
\label{tab:comparison_training_time_memory_usage_with_without_patching}
% \resizebox{1.01\textwidth}{!}
% {
% \setlength{\tabcolsep}{1.05mm}
% {
\begin{tabular}{c|c|c|cc|c}
\hline
Datasets                    & \begin{tabular}[c]{@{}c@{}}Input \\ Lengths\end{tabular} & Metrics      & \begin{tabular}[c]{@{}c@{}}DyGFormer \\ w/ patching\end{tabular} & \begin{tabular}[c]{@{}c@{}}DyGFormer \\ w/o patching\end{tabular} & \begin{tabular}[c]{@{}c@{}}Reduced \\ Ratios\end{tabular} \\ \hline
\multirow{8}{*}{LastFM}     & \multirow{2}{*}{64}                                      & running time & 13min 58s                                                        & 21min 01s                                                         & 1.50                                                      \\
                            &                                                          & memory usage & 3,945 MB                                                         & 7,953 MB                                                          & 2.02                                                      \\ \cline{2-6} 
                            & \multirow{2}{*}{128}                                     & running time & 16min 54s                                                        & 45min 29s                                                         & 2.69                                                      \\
                            &                                                          & memory usage & 4,677 MB                                                         & 7,585 MB                                                          & 1.62                                                      \\ \cline{2-6} 
                            & \multirow{2}{*}{256}                                     & running time & 24min 41s                                                        & 2h 5min 50s                                                       & 5.10                                                      \\
                            &                                                          & memory usage & 4,635 MB                                                         & 14,583 MB                                                         & 3.15                                                      \\ \cline{2-6} 
                            & \multirow{2}{*}{512}                                     & running time & 37min 04s                                                        & ---                                                               & ---                                                       \\
                            &                                                          & memory usage & 7,547 MB                                                         & OOM                                                               & ---                                                       \\ \hline
\multirow{8}{*}{Can. Parl.} & \multirow{2}{*}{64}                                      & running time & 58s                                                              & 1min 14s                                                          & 1.28                                                      \\
                            &                                                          & memory usage & 2,121 MB                                                         & 3,263 MB                                                          & 1.54                                                      \\ \cline{2-6} 
                            & \multirow{2}{*}{128}                                     & running time & 1min 02s                                                         & 2min 39s                                                          & 2.56                                                      \\
                            &                                                          & memory usage & 2,369 MB                                                         & 6,417 MB                                                          & 2.71                                                      \\ \cline{2-6} 
                            & \multirow{2}{*}{256}                                     & running time & 1min 26s                                                         & 6min 37s                                                          & 4.62                                                      \\
                            &                                                          & memory usage & 2,855 MB                                                         & 14,923 MB                                                         & 5.23                                                      \\ \cline{2-6} 
                            & \multirow{2}{*}{512}                                     & running time & 1min 57s                                                         & ---                                                               & ---                                                       \\
                            &                                                          & memory usage & 4,511 MB                                                         & OOM                                                               & ---                                                       \\ \hline
\end{tabular}
% }
% }
\end{table}