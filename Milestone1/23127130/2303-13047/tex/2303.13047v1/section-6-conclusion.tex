\section{Conclusion}
\label{section-6}
In this paper, we proposed a new Transformer-based architecture (DyGFormer) and a unified library (DyGLib) to foster the development of dynamic graph learning. DyGFormer differs from existing methods by: (\romannumeral1) a neighbor co-occurrence encoding scheme to exploit the correlations of nodes in an interaction; and (\romannumeral2) a patching technique to provide the model with the ability to capture long-term temporal dependencies. DyGLib served as a toolkit for reproducible, scalable, and credible continuous-time dynamic graph learning with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols. We hope that our research can provide new perspectives on the design of new frameworks for dynamic graph learning and encourage more researchers to dive into this field. In the future, we will continue to enrich DyGLib by incorporating the recently released datasets and state-of-the-art models.

% Limitations of DyGFormer: no capturing of high-order relationships