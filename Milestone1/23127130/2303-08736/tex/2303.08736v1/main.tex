%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A template for Wiley article submissions developed by 
% Overleaf for the Overleaf-Wiley pilot which ran 
% during 2017 and 2018.
% 
% This template is no longer supported, but is provided
% for historical reference. Last updated January 2019.
%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% Document class options:
% =======================
% blind: Anonymise all author, affiliation, correspondence
%        and funding information.
%
% lineno: Adds line numbers.
%
% serif: Sets the body font to be serif. 
%
% twocolumn: Sets the body text in two-column layout. 
% 
% num-refs: Uses numerical citation and references style 
%           (Vancouver-authoryear).
%
% alpha-refs: Uses author-year citation and references style
%             (rss).
%
% Using other bibliography styles:
% =======================
%
% To specify a different bibiography style
%
% 1) Do not use either num-refs or alpha-refs in documentclass.
% 2) Load natbib package with the options set as needed.
% 3) Use the \bibliographystyle command to specify the style
% 
% Included NJD styles are: 
%   WileyNJD-ACS
%   WileyNJD-AMA
%   WileyNJD-AMS
%   WileyNJD-APA
%   WileyNJD-Harvard
%   WileyNJD-VANCOUVER
%
% or you may upload an alternative .bst file 
% (if requested by the journal).
%
\documentclass[twocolumn,alpha-refs,serif]{wiley-article}
% Examples:
% =======================
%% Example: Using numerical, sort-by-authors citations.
% \documentclass[twocolumn,num-refs,serif]{wiley-article}

%% Example: Using author-year citations and anonymising submission
% \documentclass[blind,alpha-refs]{wiley-article}

%% Example: Using unsrtnat for numerical, in-sequence citations
% \documentclass{wiley-article}
% \usepackage[numbers]{natbib}
% \bibliographystyle{unsrtnat}

%% Example: Using WileyNJD-AMA reference style and superscript
%%          citations, two-column and serif fonts for AIChE
% \documentclass[serif,twocolumn,lineno]{wiley-article}
% \usepackage[super]{natbib}
% \bibliographystyle{WileyNJD-AMA}
% \makeatletter
% \renewcommand{\@biblabel}[1]{#1.}
% \makeatother

% Add additional packages here if required
%\usepackage{orcidlink}
\usepackage{siunitx}
%\usepackage{subcaption}
\usepackage{cleveref}
\newcommand{\todo}[1]{\textcolor{red}{(#1)}}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{graphbox}
\usepackage{multirow}



% Update article type if known
\papertype{Research Article}
% Include section in journal if known, otherwise delete
%\paperfield{Journal Section}

\title{A machine-learning approach to thunderstorm forecasting through post-processing of simulation data}

% List abbreviations here, if any. Please note that it is preferred that abbreviations be defined at the first instance they appear in the text, rather than creating an abbreviations list.
%\abbrevs{ABC, a black cat; DEF, doesn't ever fret; GHI, goes home immediately.}

% Include full author names and degrees, when required by the journal.
% Use the \authfn to add symbols for additional footnotes and present addresses, if any. Usually start with 1 for notes about author contributions; then continuing with 2 etc if any author has a different present address.
%\author[1]{Kianusch Vahid Yousefnia\,\orcidlink{0000-0003-2644-2539}}
%\author[1]{Tobias Bölle\, \orcidlink{0000-0003-3714-6882}}
%\author[1]{Isabella Zöbisch\, \orcidlink{0000-0003-2035-7931}}
%\author[1]{Thomas Gerz\, \orcidlink{0000-0003-2923-3126}}
\author[1]{Kianusch Vahid Yousefnia}
\author[1]{Tobias Bölle}
\author[1]{Isabella Zöbisch}
\author[1]{Thomas Gerz}

%\contrib[\authfn{1}]{Equally contributing authors.}

% Include full affiliation details for all authors
\affil[1]{Deutsches Zentrum für Luft- und Raumfahrt (DLR), Institut für Physik der Atmosphäre, Oberpfaffenhofen, Germany}
%\affil[2]{Faculty of Physics, Ludwig-Maximilians-Universität (LMU), Munich, Germany}


\corraddress{Kianusch Vahid Yousefnia, DLR Oberpfaffenhofen, Institut für Physik der Atmosphäre, Münchner Str. 20, D-82234 Wessling, Germany}
\corremail{kianusch.vahidyousefnia@dlr.de}

%\presentadd[\authfn{2}]{Department, Institution, City, State or Province, Postal Code, Country}

\fundinginfo{Internal DLR project DIAL}

% Include the name of the author that should appear in the running header
\runningauthor{Vahid Yousefnia et al.}

\begin{document}

\begin{frontmatter}
\maketitle

\begin{abstract}
Thunderstorms pose a major hazard to society and economy, which calls for reliable thunderstorm forecasts.
In this work, we introduce SALAMA, a feedforward neural network model for identifying thunderstorm occurrence in numerical weather prediction (NWP) data. The model is trained on convection-resolving ensemble forecasts over Central Europe and lightning observations. Given only a set of pixel-wise input parameters that are extracted from NWP data and related to thunderstorm development, SALAMA infers the probability of thunderstorm occurrence in a reliably calibrated manner.
For lead times up to eleven hours, we find a forecast skill superior to classification based only on convective available potential energy. Varying the spatiotemporal criteria by which we associate lightning observations with NWP data, we show that the time scale for skillful thunderstorm predictions increases linearly with the spatial scale of the forecast.


% Please include a maximum of seven keywords
\keywords{thunderstorms / lightning / atmospheric electricity, severe weather, convection, mesoscale, numerical methods and NWP, forecasting (methods), ensembles}
\end{abstract}
\end{frontmatter}

%%%%%%%INTRODUCTION%%%%%%%%%
\section{Introduction}
% Make a point of thunderstorms being dangerous, end with thunderstorm forecasts being crucial, especially in changing climate
While thunderstorms undoubtedly constitute inspiring natural spectacles that move any human being to a certain extent, their impact in the form of lightning, strong winds and heavy precipitation (including hail) is hazardous to society and economy. Besides the small but real chance of being struck by lightning \citep{Holle2016}, thunderstorms pose a threat to crops  and lifestock \citep{Holle2014} as well, and are known to trigger wild fires \citep{Veraverbeke2017}. In addition, they constitute a major safety concern for aviation \citep{Gerz2012, Borsky2019}. Furthermore, thunderstorms and lightning damage electrical infrastructure such as wind turbines \citep{Yasuda2012}, which jeopardizes the transition to sustainable energy production. Finally, since the number of severe thunderstorms is expected to increase due to climate change \citep{Diffenbaugh2013, Raedler2019}, accurate thunderstorm forecasts become ever more relevant.

% Explain why forecasting of deep moist convection is difficult  -> make point that it is unclear how thunderstorms look like in simulations
Thunderstorm forecasts with lead times of more than one hour usually rely on numerical weather prediction (NWP).
This method consists of simulating the future atmospheric state by numerically solving equations derived from the laws of physics. The accuracy of NWP has improved with the advent of high-performance computing, the increased availability of observational data through satellite imagery, as well as advances in data assimilation \citep{Bauer2015, Yano2018}. In order to use NWP data for thunderstorm predictions, one needs to know how thunderstorms manifest themselves in terms of the NWP output fields. In a post-processing step, this knowledge is then used to identify signs of thunderstorm occurrence in simulation data.

%Over the last decades, two approaches to the prediction of thunderstorms, and deep moist convection in general, have emerged. On the one hand, the nowcasting approach based on the extrapolation of past observations into the future is well-established for predictions up to approximately one hour in advance, while skill drops significantly for higher lead times  \citep{Dixon1993, Leinonen2022}. One reason for this may be that extrapolations do not explicitly account for the physics of convection. A complementary approach, numerical weather prediction (NWP), consists of simulating the future atmospheric state by numerically solving equations derived from the laws of physics. The accuracy of NWP has improved with the advent of high-performance computing, the increased availability of observational data through satellite imagery, as well as advances in data assimilation \citep{Bauer2015, Palmer2017}. However, in order to use NWP data for thunderstorm predictions, one needs to know how thunderstorms manifest themselves in terms of the NWP output fields. In a post-processing step, this knowledge is then used to identify signs of thunderstorm occurrence in simulation data. 
%In addition, one needs to decide which meteorological phenomenon associated with thunderstorms one wishes to predict, typically either lightning or heavy precipitation. \todo{TG: ???}

% -> quick review of thunderstorm nowcasting and what exactly is being forecast, lightning, contours from satellite imagery, rain rate. Also recent advances in machine learning
Various ideas for identifying signs of thunderstorm occurrence have been put forward in recent years. For instance, post-processing of NWP data has been blended with nowcasting methods \citep{Kober2012, Hwang2015}. Empirical knowledge on convective activity has been translated into expert systems using fuzzy logic \citep{Lin2012, Li2021}. The fuzzy logic technique allows the construction of decision rules for thunderstorm occurrence based on domain knowledge.  Lately, machine learning (ML) methods based on artificial neural networks have gained popularity. These methods generalize the fuzzy logic approach in the sense that decision rules are constructed by solving a data-driven optimization problem. Works include neural networks with relatively few neurons \citep{Ukkonen2019, Jardines2021, Kamangir2020}, as well as deep neural networks with convolutional layers and millions of trainable parameters \citep{Geng2021, Zhou2022}. Findings suggest that neural network models are more skillful at predicting thunderstorm occurrence than comparable ML approaches like random forests \citep{Ukkonen2019}.

% Promising results in ML have not been applied to high-resolution NWP data over Central Europe, let alone for ensemble data -> this is why we propose our study. Give overview of sections and key results
The promising results in ML have encouraged us to apply neural network methods to historical simulation data of ICON-D2-EPS, an NWP ensemble model for Central Europe with a horizontal resolution of  ca. \SI{2}{\kilo\meter} \citep{Zaengl2015, Reinert2020}. ICON-D2-EPS is a limited-area model which explicitly resolves convection and is run operationally by the German Meteorological Service (DWD). To the best of our knowledge, neural networks have not yet been employed for the identification of thunderstorm occurrence in ensemble data with a comparable horizontal resolution. In this work, we present the neural network model SALAMA (Signature-based Approach of identifying Lightning Activity using MAchine learning). It has been trained to predict thunderstorm occurrence through the post-processing of  simulation data. 
In \cref{sec:data}, we describe how independent datasets for the training, testing and validation of our model have been compiled from NWP forecasts and lightning data. Details on the ML architecture are provided in \cref{sec:methods}. % Our results are presented in \cref{sec:results} and conclusions drawn in \cref{sec:conclusion}.
While thunderstorm occurrence is identified in a pixel-wise manner, we systematically vary the spatiotemporal criteria by which the lightning observations are associated with the NWP data. This enables us to study the effect of different spatial scales on the model identification skill and allows us to estimate the advection speed of thunderstorms.
Further results are presented in \cref{sec:results} and demonstrate that, for lead times up to at least eleven hours, SALAMA is more skillful than a baseline method based only on convective available potential energy. In addition, we show a linear relationship between the spatial resolution scale of our model and the time scale during which skill decreases with lead time. This is consistent with earlier findings that resolving smaller scales brings faster growing forecast errors about \citep{Lorenz1969,Selz2015}.
% This suggests that classification skill decreases more slowly for large patterns of thunderstorm activity than for small-scale structures.


%%%%%%%DATA%%%%%%%%%%%%%
\section{Data}\label{sec:data}
We  collected simulation data from the ICON-D2-EPS ensemble model, as well as lightning observations from the lightning detection network LINET \citep{Betz2009}.
The simulations were used to extract predictors of thunderstorm occurrence. The lightning observations serve as ground truth.
The model domain of ICON-D2-EPS covers the areas of Germany, Switzerland, Austria, Denmark, Belgium, the Netherlands and parts of the neighboring countries. For our study, we cropped the model domain at its borders by approximately \SI{100}{\kilo\meter}  to reduce boundary computation errors, which yielded the study region given in \cref{fig:study_region}. In a cylindrical projection, it  corresponds to a rectangle with the southwest corner located at \ang{45}N, \ang{1}E, the northeast corner located at \ang{56}N, \ang{16}E and all sides being either parallels or meridians. There are daily model runs  every three hours starting at 00~UTC, with 20 ensemble members per run. We collected simulation data from June to August 2021 over the entire domain in \cref{fig:study_region} in hourly steps, taking always the latest available forecast for each hour. Following this procedure results in forecasts with lead times of \SI{0}{\hour}, \SI{1}{\hour} or \SI{2}{\hour}.
\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{ICON.pdf}
\caption{Map of the study region.}
\label{fig:study_region}
\end{figure}


\subsection{NWP predictors}
%\todo{Kürzen, alles zu feature engineering weglassen, nicht den Eindruck erwecken, dass man es eigentlich ganz anders machen sollte als wir es gemacht haben}
%ICON-D2-EPS computes primitive fields, like temperature, pressure and the three components of wind speed, on 65 vertical levels. If we used these vertical profiles directly, this would entail several hundred model inputs per grid point.

%As it would likely require deep neural networks with sophisticated architectures like convolutional layers to transform the raw data into higher level representations, we use instead quantities that have been derived from vertical profiles and are known to be relevant for convection. This attempt at reducing data dimensionality is commonly referred to as feature engineering. 

The atmospheric fields used as predictors of thunderstorm occurrence in this study are given in \cref{tab:input_features}. They have been selected as follows: We considered as candidate predictors all two-dimensional fields provided in ICON-D2-EPS, as well as two ICON-D2-EPS pressure-level fields associated with deep moist convection in the literature, namely the relative humidity at \SI{700}{\hecto\pascal} and the vertical wind speed in pressure coordinates at \SI{500}{\hecto\pascal} \citep{Li2021}. In addition, we stipulated that the predictors be available on the open-data server of the DWD (https://opendata.dwd.de, last visit: 2023-03-14), such that the trained model can eventually be used in real-time.
For a given candidate input field, we retrieved values on the grid points and time instants on the study domain and period. We also checked for each value whether a thunderstorm occurred (cf. \cref{sec:lightning}).  Next, we compared histograms of the distribution of the given field during and in the absence of thunderstorm occurrence and kept only fields that differed significantly in the two distributions.  

As shown in \cref{tab:input_features}, all predictors can be related to thunderstorm activity through physical mechanisms like instability and moisture. In particular, our selection process has led to predictors that agree with findings in the literature \citep{Ukkonen2019, Jardines2021, Leinonen2022}. Conversely, convective inhibition (CIN), which is sometimes listed as a convective predictor \citep{Kamangir2020}, has not passed the selection process. This is likely due to the fact that we have checked for predictive power in terms of developed thunderstorms. CIN, however, correlates with the hours leading up to a thunderstorm and has been removed once the storm reaches its mature stage.

It is worth stressing that we have excluded certain parameters on purpose, namely the geographical location of a thunderstorm event, the time of the day, and the time of the year. In doing so, we assume the existence of a universal signature shared by all thunderstorms, irrespectively of where and when they occur. In addition, the list of predictors does not include the lead time of the forecast. We check in \cref{sec:results} whether our model, which has been trained on data with lead times between \SI{0}{\hour} and \SI{2}{\hour}, displays skill on data with longer lead times.

\begin{table*}[htbp]
    \centering
    \caption{List of the 21 input parameters used in the study ("DIA": including sub-grid scale). }
    \label{tab:input_features}
    \begin{tabular}{lll}
    \toprule
  physical significance & ICON parameter name  & description \\
   \midrule
        instability & CAPE\textunderscore ML & mixed-layer convective available potential energy\\
                 & CEILING &  ceiling height\\
                 & OMEGA500 & vertical wind speed in pressure coordinates at \SI{500}{\hecto\pascal}\\
                & PS & surface pressure \\
                 & PMSL & surface pressure reduced to mean sea level \\
        cloud cover & CLCH & high level clouds (0-\SI{400}{\hecto\pascal}) \\
            & CLCM & mid-level clouds (400-\SI{800}{\hecto\pascal}) \\
            & CLCL & low-level clouds (\SI{800}{\hecto\pascal} to soil) \\
            & CLCT & total cloud cover \\
        precipitation and & DBZ\textunderscore CMAX & maximal radar reflectivity \\
        moisture& ECHOTOP & echotop pressure \\
        & RELHUM700 & relative humidity at \SI{700}{\hecto\pascal} \\
        & RELHUM\textunderscore 2M & \SI{2}{\meter} relative humidity \\
    column-integrated & TQC, TQC\textunderscore DIA & cloud water \\
       % & TQC\textunderscore DIA & cloud water, including sub-grid scale \\
       water quantities & TQG & graupel \\
        & TQI, TQI\textunderscore DIA & ice \\
        & TQV, TQV\textunderscore DIA & water vapor \\
        & TWATER & total water content \\
        \bottomrule
    \end{tabular}
\end{table*}

%\begin{table*}[htbp]
%    \centering
%    \caption{List of the 21 input parameters used in the study ("DIA: including sub-grid scale). }
%    \label{tab:input_features}
%    \begin{tabular}{lll}
%    \toprule
%  Physical significance & ICON parameter name  & description \\
%   \midrule
%        Instability & CAPE\textunderscore ML & mixed-layer convective available potential energy\\
%                 & CEILING &  Ceiling height\\
%                 & OMEGA500 & vertical wind speed in pressure coordinates at \SI{500}{\hecto\pascal}\\
%                & PS & surface pressure \\
%                 & PMSL & surface pressure reduced to mean sea level \\
%        Cloud cover & CLCH & high level clouds (0-\SI{400}{\hecto\pascal}) \\
%            & CLCM & mid-level clouds (400-\SI{800}{\hecto\pascal}) \\
%            & CLCL & low-level clouds (\SI{800}{\hecto\pascal} to soil) \\
%            & CLCT & total cloud cover \\
%        precipitation and & DBZ\textunderscore CMAX & maximal radar reflectivity \\
%        moisture& ECHOTOP & echotop pressure \\
%        & RELHUM700 & relative humidity at \SI{700}{\hecto\pascal} \\
%        & RELHUM\textunderscore 2M & \SI{2}{\meter} relative humidity \\
%    column-integrated & TQC, TQC\textunderscore DIA & cloud water \\
%       % & TQC\textunderscore DIA & cloud water, including sub-grid scale \\
%       water quantities & TQG & graupel \\
%        & TQI & ice \\
%        & TQI\textunderscore DIA & ice, including sub-grid scale \\
%        & TQV & water vapor \\
%        & TQV\textunderscore DIA & water vapor, including sub-grid scale \\
%        & TWATER & total water content \\
%        \bottomrule
%    \end{tabular}
%\end{table*}



\subsection{Lightning observations}\label{sec:lightning}
In supervised learning, ML models are trained on  data for which the ground truth is known. For this reason, we required knowledge of thunderstorm occurrence for our study domain and period. By reason of their high detection efficiency and spatial accuracy over the entire study region, we employed lightning observations to assess the occurrence of thunderstorms.
Specifically, we resorted to the LINET network \citep{Betz2009}, which exploits the radio spectrum to continuously measure strokes of lightning over Europe. The technology achieves a detection efficiency of more than \SI{95}{\percent} and an average location accuracy of \SI{150}{\meter}. While the technology is able to differentiate between cloud-to-ground and intracloud flashes, we have considered all lightning events as we are only interested in the yes/no occurrence of thunderstorm activity.

Given a set of predictors retrieved from a grid point $x$ on the study domain at time $t$ during the study period, we considered thunderstorm activity to occur at $(x,t)$ if a flash was detected at any $(x_l, t_l)$ with
\begin{equation}
    \left\| x-x_l \right\| < \Delta r \quad \text \quad |t-t_l| < \Delta t,
\end{equation}
where $\left\|\cdot\right\|$ denotes the great-circle distance between $x$ and $x_l$. We trained our model with different values for the spatial and temporal thresholds $\Delta r$ and $\Delta t$ in order to study the relationship between them and classification skill systematically. 


\subsection{Compiling independent data sets}\label{sec:datasets}
The data obtained from NWP and lightning observations can be considered a set of tuples $(\xi, y)$, where $\xi\in \mathbb{R}^n$ denotes the $n=21$ input parameters and $y \in \{0,1\}$ corresponds to a label of the ground truth (1: thunderstorm occurrence, 0: no thunderstorm occurrence). As the input fields were provided on a triangular grid, we first performed an interpolation onto a $\ang{0.125}\times\ang{0.125}$ longitude/latitude grid. The labels were produced on the same grid. For each full hour during the study period, for each ensemble member and for each grid point, we fetched the input parameters and the corresponding label, taking always the latest available forecast.

We compiled three statistically independent data sets: The training set is used only for training the neural network model (a precise definition of training is given in \cref{sec:model_description}), while its skill is measured on a test set with data that the model has not seen during training. A third data set, the validation set, is used to monitor training progress (cf. \cref{sec:model_description}). In an attempt to assure statistical independence between the data sets, we took two measures. First, assuming possible day-to-day correlations in the input parameters (e.g. induced by the synoptic scale) to be negligible for convective events with life spans of the order of a few hours, we used separate days for training, testing and validation. 
 In addition, we took into account that intense thunderstorms that form in the afternoon may well live on after 0~UTC. We therefore defined days to begin at 8~UTC, a time of the day chosen by checking when lightning activity in the collected data is minimal. The latter measure prevents data from one thunderstorm at different times to appear in separate data sets. 
 
 \Cref{fig:training_days} offers an overview of the days contained in each data set. The days were randomly distributed among the three sets. Furthermore, we have randomly subsampled the sets such that the training set consists of \num{4e5} tuples, and the test and validation sets each contain \num{e5} tuples. 
 The subsampling has been performed in a manner that the training set is class-balanced (both labels appear equally frequently), while no correction to imbalance has been done for the test and validation sets. The reason for this correction was to prevent the model from learning to simply predict the majority class at all times. This correction, however, requires additional care, which is discussed in \cref{sec:model_reliability}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{training_dates.pdf}
\caption{Days (from 8~UTC to 8~UTC) during the summer of 2021 which were used for compiling the datasets for training (dark brown), testing (light blue with bold numerals) and validation (light green). The days have been distributed at random among the three sets.}
\label{fig:training_days}
\end{figure}




%%%%%%%METHODS%%%%%%%%%%
\section{Methods}\label{sec:methods}
In this section, we provide details on SALAMA, focusing on how it has been trained and calibrated. In addition, we introduce metrics for the evaluation of model skill and present a baseline model for comparison. 


\subsection{Model description} \label{sec:model_description}
It is worthwhile to introduce some ML terminology. The three data sets used for training, testing and validation (cf. \cref{sec:datasets}) are made up of examples $(\xi, y)$. Each example consists of a pattern $\xi \in \mathbb{R}^n$ of $n$ input features and a label $y\in\{ 0,1\}$.

Given a pattern $\xi$, the problem at hand is to infer the probability of thunderstorm occurrence, which constitutes a task known as binary classification. In the following, we consider both the pattern and its corresponding label to originate from a random experiment. Therefore, let $\Xi$ be an $n$-dimensional  random variable for the pattern and let $Y$ be a random variable of thunderstorm occurrence (1: thunderstorm, 0: no thunderstorm). We are interested in $P(Y=1 | \Xi=\xi)$, namely the conditional probability of thunderstorm occurrence if the pattern is known. A feedforward artificial neural network model is a function $f: \mathbb{R}^n\to (0,1)$ that models the relationship between the input pattern and the corresponding probability of thunderstorm occurrence. We refer to $f$ simply as neural network. Neural networks use compositions of matrix multiplications, as well as non-linear operations referred to as activation functions. The architecture of our neural network is presented in \cref{fig:architecture}: It consists of the input and output layer as well as hidden layers, where each layer is a vector of numbers obtained from the previous layer by one matrix multiplication and by applying an activation function to the result in a component-wise manner. The complexity of $f$ is adjustable through the number of hidden layers and the size of each hidden layer, i.e. the number of nodes. Our model has three hidden layers and 20 nodes per hidden layer. Moreover, we use rectified linear units for the hidden layers and a sigmoid function to map the output layer to a probability between zero and one. 
\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{neural_networks.pdf}
\caption{(Color online) The architecture of SALAMA: Input features are scaled to order 1. We use rectified linear units as activation functions in the hidden layers. A sigmoid function maps the output layer  to the open interval $(0,1)$.}
\label{fig:architecture}
\end{figure}

The entries, also referred to as weights, of the matrices that connect the layers are adjusted according to the data in the training set. We therefore add a subscript $w\in\mathbb{R}^d$ to $f$  to express the dependence on the $d$ weights. If $f_w$ constitutes an accurate representation of the conditional probability of thunderstorm occurrence, i.e. $f_w(\xi)\approx P(Y=1 | \Xi=\xi)$, then the likelihood of observing a label $y$ for a given input feature $\xi$ reads
\begin{equation}
L(w | \xi,y) = 
\begin{cases}
f_w(\xi), & y =1 \\
1-f_w(\xi), & y=0 
\end{cases}
\end{equation}

Denote by $(\xi^{(i)},y^{(i)})_{i=1\dots N}$ the training set with $N$ examples. The most likely configuration of weights, given the training set, is then obtained by minimizing the negative logarithm of the likelihood function,
\begin{equation}
-\log \mathcal{L} (w) = -\sum_{i=1}^{N} \log L\left(w | \xi^{(i)},y^{(i)}\right),
\label{eq:loglikelihoodloss}
\end{equation}
with respect to the weights.
The expression in \cref{eq:loglikelihoodloss} is referred to as binary cross-entropy loss function in ML terminology. The process of determining the weights that minimize loss is called training.  We trained SALAMA using the robust iterative stochastic method Adam \citep{Kingma2014}. However, if one used the configuration of weights which minimizes \cref{eq:loglikelihoodloss} exactly, a neural network would likely suffer from overfitting, i.e. learning parts of the noise in the data as well. To this end, we implemented an early stopping procedure, in which loss was monitored on the validation set during training. Once the validation loss no longer decreased, training was stopped. 

Before training, each input feature has been scaled in a way that its sample standard deviation in the training set is of the order of unity. In addition, we trained not only on the architecture presented in \cref{fig:architecture} but also varied the number of hidden layers,  as well as the number of nodes per layer. We found that once a certain complexity was reached in terms of the size of the network, adding new nodes or layers had no effect on the validation loss at the end of training. The architecture in \cref{fig:architecture} constitutes the smallest network for which this complexity threshold has been exceeded.

\subsection{Model reliability}\label{sec:model_reliability}

As described in \cref{sec:datasets}, we have artificially increased the fraction of positive examples in the data set used for the training of our neural network. It is crucial to understand that if  the trained model were naively applied to a test set with a different fraction of positive examples, it would produce probabilities that are inconsistent with the observed relative frequency of thunderstorm occurrence.

In order to see this, we use Bayes' theorem to expand the conditional probability of thunderstorm occurrence given a pattern $\xi$, which yields:
\begin{equation}
P(Y=1 | \Xi=\xi)  = \frac{P(\Xi=\xi | Y = 1) P(Y=1)}{P(\Xi=\xi)}.
\end{equation}
The denominator can be expressed as
\begin{equation}
\begin{split}
P(\Xi=\xi) &= P(\Xi=\xi | Y = 1)P(Y=1)  \\ 
&+  P(\Xi=\xi | Y = 0)P(Y=0).
\end{split}
\end{equation}
Let $P(Y=1)=1-P(Y=0) = g$, where $g$ denotes the climatological probability of thunderstorm occurrence with no prior knowledge. Then,
\begin{equation}
P(Y=1 | \Xi=\xi)  = \frac{1}{1+(1-g)R(\xi)/g} \label{eq:g-dep},
\end{equation}
where the residual function $R(\xi)=P(\Xi=\xi | Y = 0)/P(\Xi=\xi | Y = 1)$ is not expected to depend on $g$. \Cref{eq:g-dep} shows that the conditional probability of thunderstorm occurrence carries an implicit $g$-dependence. Now remember that the training set contains an increased fraction $\tilde{g}=1/2$ of positive examples, while the corresponding fraction in the test set is (up to fluctuations due to the finite sample size) equal to the climatological value $g$. During training, the neural network, therefore, learns to produce the following model output
\begin{equation}
f_w(\xi,\tilde{g})  = \frac{1}{1+(1-\tilde{g})R(\xi)/\tilde{g}}.
 \label{eq:g-dep2}
\end{equation}
When we want to apply our neural network to a dataset with $g\neq \tilde{g}$, the correct probability output reads
\begin{equation}
f_w(\xi, g) = \frac{f_w(\xi,\tilde{g})}{f_w(\xi,\tilde{g}) + \frac{1-g}{g}\frac{\tilde{g}}{1-\tilde{g}}(1-f_w(\xi,\tilde{g}))},
\label{eq:probability_test}
\end{equation}
which can be derived by formulating \cref{eq:g-dep2} with $g$ and $\tilde{g}$ and substituting one equation into the other one.

If  the model probability output is consistent with the observed relative frequency of thunderstorm occurrence, the model forecasts are referred to as reliable.
In order to check whether our neural network  provides reliable forecasts, we used the test set to produce a reliability diagram. For this purpose, one partitions the interval $(0,1)$ of possible forecast probabilities into bins. For each bin, one considers all examples whose model probability falls into the bin. Then, one computes the relative frequency of thunderstorm occurrence and plots it against the average model probability per bin. An example for one configuration of lightning labels is shown in \cref{fig:reliability_diagram}, for which 10 equidistant bins have been used. The uncertainty on the observed frequency spans the 5th and 95th percentiles of fluctuations and has been estimated through a bootstrap resampling procedure similar to \cite{Broecker2007}: By drawing with replacement, one produces variations of the original test set and considers the sample-to-sample fluctuations of observed relative frequencies. While producing reliable forecasts for probabilities close to 0 and 1, our model underestimates the relative frequency of thunderstorm occurrence for forecast probabilities below \num{0.7}. Further calibration could be done using statistical methods like isotonic regression \citep{Niculescu-Mizil2005}, which is beyond the scope of this work. Instead, we consider our model sufficiently reliable and appreciate that the level of reliability has been attained by means of the analytical correction \eqref{eq:probability_test} alone.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{reliability_diagram_FeedForwardNN_with_20_and_20_and_20_hidden_dims_summer2021.pdf}
\caption{Observed relative frequency of thunderstorm occurrence as a function of model output probability, evaluated for the test set with the label configuration $\Delta r = \SI{15}{\kilo\meter}, \Delta t = \SI{30}{\minute}$ (cf. \cref{sec:lightning}).  Line thickness corresponds to the symmetric \SI{90}{\percent} confidence interval obtained by bootstrapping \num{200} resamples, see text for details. The dashed 1:1 line corresponds to a model with perfect reliability.}
\label{fig:reliability_diagram}
\end{figure}

\subsection{Skill evaluation metrics}\label{sec:skill_scores}
%Metrics for evaluating classification skill using a test set with $N$ examples include the Brier score, 
%\begin{equation}
%\text{BS} = \sum_{i=1}^{N} (p^{(i)} - y^{(i)})^2, \qquad p^{(i)} = f_w(\xi^{(i)},g),
%\end{equation}
%which is popular for being strictly proper \todo{source} and directly acts on the probability outputs $p^{(i)}$ of the model. \todo{use the Brier score somewhere.} 
%For many other scores, it is necessary to convert the probabilities to binary output first. 
While the neural network outputs a continuous probability $p$, many metrics for evaluating classification skill require a conversion from probability into binary output.
This is done by introducing a decision threshold $\tilde{p}$. If $p>\tilde{p}$, thunderstorm occurrence for the corresponding example is deemed "true", otherwise "false". In combination with the two options from the label, there are four possible outcomes for each example. They are presented as a contingency matrix in \cref{tab:contingency_matrix}.
\begin{table}[htbp]
\caption{Contingency matrix for binary classification.}
\label{tab:contingency_matrix}
\begin{tabular}{cccc}
\toprule
&& \multicolumn{2}{c}{ observed thunderstorm} \\
\midrule
&& true & false \\
\multirow{2}{*}{forecast thunderstorm} & true & hit & false alarm \\
 &false & miss & correct reject \\
\bottomrule
\end{tabular}
\end{table}
Given a test set and a fixed decision threshold, the probability of detection (POD) and false-alarm ratio (FAR) are defined by
\begin{align}
\text{POD} &= \frac{\text{hits}}{\text{hits}+\text{misses}}, \\
\text{FAR} &= \frac{\text{false alarms}}{\text{hits}+\text{false alarms}}.
\end{align}
Here,  "hits" refers to the number of examples in the test set that qualify as "hit" according to \cref{tab:contingency_matrix}. 

\subsection{Baseline model}\label{sec:baseline_intro}
Mixed-layer convective available potential energy (CAPE) is a measure of atmospheric instability and a necessary condition for deep moist convection \citep{Markowski2011, Ukkonen2019}. We implemented a simple baseline model which forecasts a thunderstorm if CAPE exceeds a fixed decision threshold.
Although this baseline approach does not explicitly involve thunderstorm probabilities, its skill can be nevertheless quantified by the scores introduced in \cref{sec:skill_scores}. The contingency matrix elements in \cref{tab:contingency_matrix} adapt straightforwardly. For instance, an example in a test set is referred to as a "hit" if a positive lightning label coincides with CAPE exceeding the decision threshold.




%For the purpose of comparison, we prepare a baseline model that uses logistic regression to convert CAPE into probabilities.
%Using the same symbols as in \cref{fig:architecture}, the architecture of our baseline model is presented on the left-hand side of \cref{fig:architecture_baseline}. Just like for SALAMA, we train the baseline model on a class-balanced training set. The resulting correspondence between CAPE values and probabilities for a test set with a fraction $g$ of positive examples consistent with climatology  is shown on the right-hand side of \cref{fig:architecture_baseline}.
%\begin{figure*}[htbp]
%\centering
%\includegraphics[align=c,width=0.59\textwidth]{Code for figures/neural_networks_Platt.pdf}
%\includegraphics[align=c,width=0.4\textwidth]{CAPE-prob.pdf}
%\caption{(Color online) (a) The architecture of the baseline model, with a sigmoid function in the output layer to produce probability-like output. The (two) weights connecting the input and output layer are determined by minimizing \cref{eq:loglikelihoodloss}, just like for SALAMA. (b) Resulting correspondence between CAPE values and calibrated probability output (cf. \cref{eq:probability_test}) for a climatologically consistent test set with the label configuration $\Delta r = \SI{15}{\kilo\meter}, \Delta t = \SI{30}{\minute}$ (cf. \cref{sec:lightning}). }
%\label{fig:architecture_baseline}
%\end{figure*}




%%%%%%RESULTS%%%%%%%%%%%%
\section{Results}\label{sec:results}
Having trained SALAMA, we can straightforwardly post-process ensemble data with it. For example, if we have, for a given location, a \SI{3}{\hour}-forecast of ICON-D2-EPS at hand, it consists of 20 input feature tuples (one tuple for each ensemble member). One can now compute a thunderstorm probability according to \cref{eq:probability_test} for each member. As we will discuss in \cref{sec:lead-dependence}, the ensemble spread of thunderstorm probability is linked to the model uncertainty of the input features.
In the following, we compare SALAMA to the  baseline model based on CAPE (cf. \cref{sec:baseline_intro}) and move on to investigating how the spatiotemporal thresholds of the lightning label configuration (cf. \cref{sec:lightning}) influence the classification skill of SALAMA as a function of lead time.

\subsection{Comparison to baseline model}\label{sec:baseline_comp}
We keep the thresholds of the lightning label configuration (cf. \cref{sec:lightning}) fixed to the particular choice $\Delta r = \SI{15}{\kilo\meter}, \Delta t = \SI{30}{\minute}$ in this section. The climatological fraction of thunderstorm examples in the test set amounts to $g=\num{0.021}$ in this configuration. The results, however, do not change qualitatively if another configuration is used.

In \cref{fig:case_visualization}, we run SALAMA  for three consecutive hours of an evening with thunderstorm occurrence over Southern Germany. More precisely, we plot the ensemble-averaged probability of thunderstorm occurrence for the entire study domain on a map. Pixels with observed thunderstorm occurrence are colored in as well. For a comparison to the baseline model, we add a row of plots which show the spatial distribution of ensemble-averaged mixed-layer CAPE. For both models, we compute POD and FAR based on the map pixels. The decision threshold used for their computation (cf. \cref{sec:skill_scores}) is chosen for each model such that forecasts are unbiased. This means that the average fraction of examples classified as thunderstorms is equal to the observed fraction of thunderstorm examples. The threshold found for CAPE is supported by previous findings \citep{Zoebisch2020}. The color bar is prepared such that probabilities above the decision threshold are marked with a significantly darker color than probabilities below it. 
For this example case, one can see immediately that SALAMA detects thunderstorm occurrence more reliably than the baseline model (which we simply call CAPE in the following), while at the same time providing fewer false alarms. Note that this particular day has not been used for the training of SALAMA.
\begin{figure*}[htbp]
\centering
\includegraphics[height=0.37\textwidth]{likelihood_t030r015_2021062319_mean_FeedForwardNN_with_20_and_20_and_20_hidden_dims_summer2021.pdf}
\includegraphics[height=0.37\textwidth]{likelihood_t030r015_2021062320_mean_FeedForwardNN_with_20_and_20_and_20_hidden_dims_summer2021.pdf}
\includegraphics[height=0.37\textwidth]{likelihood_t030r015_2021062321_mean_FeedForwardNN_with_20_and_20_and_20_hidden_dims_summer2021.pdf}
\includegraphics[height=0.37\textwidth]{likelihood_t030r015_2021062319_mean_CAPE_ML_summer2021.pdf}
\includegraphics[height=0.37\textwidth]{likelihood_t030r015_2021062320_mean_CAPE_ML_summer2021.pdf}
\includegraphics[height=0.37\textwidth]{likelihood_t030r015_2021062321_mean_CAPE_ML_summer2021.pdf}
\caption{(Color online) Upper row: probability of thunderstorm occurrence (red) for SALAMA  on June 23, 2021 from 19~UTC on. The model lead times for the three hours are \SI{1}{\hour}, \SI{2}{\hour}, and \SI{0}{\hour}, respectively.  Displayed is an average over 20 ensemble members. Lightning labels ($\Delta r = \SI{15}{\kilo\meter}, \Delta t = \SI{30}{\minute}$, cf. \cref{sec:lightning}) in blue. Lower row: ensemble-averaged mixed-layer CAPE (red) and lightning labels (blue). 
We have introduced a jump in the color maps to show the decision thresholds. The thresholds (SALAMA: \SI{14.8}{\percent}, CAPE: \SI{763}{\joule\per\kilo\gram}) are chosen such that the models are unbiased.}
\label{fig:case_visualization}
\end{figure*}

In order to demonstrate that the difference in skill is not specific to the particular evening displayed in \cref{fig:case_visualization} but generalizes to the entire study period, we compute skill scores for the test set introduced in \cref{sec:datasets}, which consists of examples of the entire summer of 2021. 
POD and FAR depend on the decision threshold used to convert probabilities (or CAPE values) to binary output (cf. \cref{sec:skill_scores}). Drawing $(\text{POD},1-\text{FAR})$ for different decision thresholds into one diagram, one obtains the curve shown on the left-hand side of \cref{fig:ROC_identification}. It is referred to as precision-recall (PR) diagram and is used to evaluate the classification skill of a model. A random model with no skill corresponds to the dashed horizontal curve $1-\text{FAR}=g$, where $g$ denotes the climatological fraction of positive examples in the test set. Models with skill display PR curves above the horizontal line, with higher areas under the curve (AUC) indicating higher classification skill.
The uncertainties are computed here, as well as for the remaining figures in this section, by the bootstrap resampling method introduced in \cref{sec:model_reliability}. The models which are considered here display higher skill than a random model following climatology would. SALAMA, however, has higher classification skill than CAPE, as can be seen from the higher AUC in the PR curve in \cref{fig:ROC_identification}. This illustrates that a multi-parameter approach to thunderstorm forecasting is superior to employing a single input feature.

\begin{figure*}[htbp]
\centering
%\includegraphics[width=0.49\textwidth]{ROC.pdf}
\includegraphics[width=0.48\textwidth]{PR.pdf}
\includegraphics[width=0.49\textwidth]{PR_AUC_leadtime.pdf}
\caption{(Color online) (a) PR curve for SALAMA (solid) and the CAPE-based model (dashed), evaluated on the test set. The annotations added to the curves correspond to different decision thresholds, cf. \cref{sec:skill_scores} (b) Classification skill (quantified by the area under the PR curve) as a function of lead time. 
In both figures, grey dotted lines denote models with no identification skill. Uncertainties are obtained from 200 bootstrap resamples and show the symmetric \SI{90}{\percent} confidence interval.}
\label{fig:ROC_identification}
\end{figure*}

With this quantitative analysis at hand, a couple of comments are in order. Firstly, the PR curve of CAPE increases for $\text{POD}\lesssim \num{0.39}$, which suggests that the visual overlap between the CAPE output and the labels in \cref{fig:case_visualization} could be slightly increased by choosing a lower decision threshold. This, however, would cause the model to become biased towards predicting too many thunderstorms. Secondly, the decision threshold used for SALAMA in \cref{fig:case_visualization} corresponds to a point with $\text{POD}=1-\text{FAR} \approx \num{0.40}$,  which means that the skill which SALAMA displays in \cref{fig:case_visualization} is above the average skill during the summer of 2021. Thirdly, it is worth appreciating that the PR curves of SALAMA and CAPE can be directly compared to each other even though they are parameterized differently (probabilities vs. CAPE values). The mathematical reason for this is that the shape of a parametrized curve, as well as the area under its curve, is invariant under reparametrizations.  There is a crucial implication to this, which concerns the reliability diagram of SALAMA (cf. \cref{fig:reliability_diagram}): Even if we included a further calibration step to increase reliability, this would actually have no effect on the classification skill measured by the area under the PR curve.




\subsection{Lead time dependence of classification skill}\label{sec:lead-dependence}
In the remainder of this section, we investigate how the classification skill of SALAMA depends on the lead time of the NWP forecast from which the input features originate. For this purpose, we generate test sets in which the examples come from NWP forecasts with fixed lead time. Each set contains \num{e5} examples. We use the same dates as for the test sets introduced in \cref{sec:datasets}. On the right-hand side of \cref{fig:ROC_identification},  we plot the SALAMA classification skill, measured in terms of the area under the PR curve, as a function of lead time and compare it to the dependence obtained for CAPE. The figure shows that, for SALAMA,  classification skill decreases approximately exponentially (note the log-scaling of the $y$-axis) for lead times longer than \SI{1}{\hour}, while the corresponding curve for CAPE remains nearly constant for all lead times considered here. Nevertheless, even for \SI{11}{\hour} forecasts, SALAMA remains more skillful than any forecast using CAPE.  The classification skill of SALAMA at a lead time of \SI{1}{\hour} is actually higher than at \SI{0}{\hour}. This is likely a spin-up effect resulting from the NWP model \citep{Sun2014}.

%The constancy of CAPE skill suggests that the mixed-layer CAPE can be predicted with relatively high accuracy for all considered lead times while sufficiently many other input fields display a faster decrease in accuracy as a function of lead time. 
%This is likely the reason for the decrease in skill observed for SALAMA. \todo{Maybe it is possible to comment on difference in performance with respect to Cb-Fusion, even though different simulation data} 
It is tempting to assume that the decrease in skill with lead time originates from an increasing NWP forecast uncertainty for longer lead times. We can use ensemble data to check this hypothesis. Let $q$ be either one of the 21 input features or the model thunderstorm probability, i.e. a quantity that is given for each ensemble member and for all lead times. Then define the ensemble spread $\sigma'_q$ of $q$ as the ensemble standard deviation of $q$,
\begin{equation}
\sigma'_q (t_\text{lead}) = \sqrt{\langle q(t_\text{lead})^2 \rangle - \langle q(t_\text{lead}) \rangle^2},
\label{eq:ensemble_spread}
\end{equation}
where we make the dependence on the lead time $t_\text{lead}$ explicit. The brackets $\langle \cdot \rangle$ denote the average over all 20 ensemble members. Denote by $\overline{\sigma'_q}(t_\text{lead})$ the expression obtained by performing an average of $\sigma'_q$ over the entire study region and all times associated with the test set.
Lastly, we define the normalized ensemble spread of $q$,
\begin{equation}
\sigma_q (t_\text{lead}) = \frac{\overline{\sigma'_q}(t_\text{lead})}{\overline{\sigma'_q}(\SI{0}{\hour})},
\label{eq:ensemble_spread_avg}
\end{equation}
as a function of lead time. It quantifies ensemble spread in such a way that different input features can be directly compared to each other. In \cref{fig:variance_in-out}, the normalized ensemble spread of each of the 21 input features is shown as thin solid lines and the corresponding curve for the model output of SALAMA is drawn in thick and dashed. One can see that the ensemble spread does indeed increase with lead time for most input features, the increase being approximately linear. The ensemble spread of the SALAMA output increases in line with the majority of the input features and with a similar slope. This suggests that the decrease in classification skill  observed in \cref{fig:ROC_identification} is solely due to the increasing variance in the simulation data.
\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{variability_leadtime.pdf}
\caption{Normalized ensemble spread (cf. \cref{eq:ensemble_spread_avg}) of input features in comparison to spread of model thunderstorm probability as a function of lead time. Each thin solid line refers to one of the 21 input features. The thick dashed green line is associated with SALAMA probability output. Its line thickness represents the symmetric \SI{90}{\percent} confidence interval of uncertainty, estimated with 200 bootstrap resamples.}
\label{fig:variance_in-out}
\end{figure}

\subsection{Effect of the label size}\label{sec:label_role}
So far, the temporal and spatial thresholds,  $\Delta r$ and $\Delta t$, of the label configuration have been fixed. In this section, we study their effect on the classification skill of SALAMA. As a first step, we compute the area under the PR curve for several label configurations, which is shown in \cref{fig:mean_wind}. The color pattern in the figure suggests that the two thresholds are not independent variables of classification skill. Instead, one can find a parameter $c$ with the units of a velocity such that classification skill is nearly constant along lines
\begin{equation}
   s = \Delta r + c \Delta t = \text{const}.
   \label{eq:forecast_scale}
\end{equation}
Indeed, $s$ corresponds to a spatial resolution scale; it determines the minimal spatial accuracy that can be expected from a model trained with a given label configuration. We expect the parameter $c$ to roughly quantify the speed at which regions of thunderstorm occurrence are advected in the atmosphere. A fit to the data provides $c = \SI{5.2(3)}{\meter\per\second}$, which is similar to typical low- to mid-tropospheric wind speeds in Central Europe. Lines of constant spatial scale appear as dashed lines in \cref{fig:mean_wind}. Classification skill increases with $s$. This is consistent with the work of \cite{Roberts2008}, which investigates the spatial variation of precipitation forecast skill.
\begin{figure}[htbp]
\includegraphics[width=\columnwidth]{PR_AUC_labels_annotated.pdf}
\caption{(Color online) Classification skill of SALAMA, expressed in terms of the area under the PR curve, as a function of the label configuration (cf. \cref{sec:lightning}).  The slope of the dashed lines is chosen such that classification skill is approximately constant along the lines. Each line corresponds to a specific spatial scale $s$ (cf. \cref{eq:forecast_scale}).}
\label{fig:mean_wind}
\end{figure}

Next, we investigate how the decrease of classification skill with lead time depends on the spatial scale. Motivated by the observed decay of classification skill with lead time (cf. \cref{sec:lead-dependence}), we fit an exponential function $\exp(-t_\text{lead}/\tau)$ to the lead time dependence of classification skill (measured again by the area under the PR curve). The skill decay time $\tau$ then provides a characteristic time scale for the decrease of classification skill.  For each label configuration in \cref{fig:mean_wind}, we compute the corresponding spatial scale as well as $\tau$. In \cref{fig:skill_decay}, we present a scatter plot of $\tau$ and $s$. The figure shows a tight positive linear correlation between the two quantities, which means that classification skill decreases more slowly for coarser label configurations. This is in agreement with the  anticipation \citep{Lorenz1969} that the ability to resolve smaller scales in NWP models results in forecast errors growing more rapidly. Our finding is complementary to convection studies involving a scale-dependent skill score \citep{Roberts2008}, and high-resolution simulations \citep{Selz2015}.
\begin{figure}[htbp]
\includegraphics[width=\columnwidth]{skill_decays_scatter_PR_AUC.pdf}
\caption{Decay time of classification skill (quantified by the area under the PR curve) as a function of the spatial scale. Each data point corresponds to one label configuration in \cref{fig:mean_wind}. The parameters of a linear fit are also shown, as well as the Pearson coefficient of correlation.}
\label{fig:skill_decay}
\end{figure}


%%%%%%%%DISCUSSION%%%%%%%
\section{Conclusion and perspectives}\label{sec:conclusion}

Addressing the need for accurate thunderstorm forecasting and leveraging advances in high-resolution NWP and ML, we have presented SALAMA, a feedforward neural network model that identifies thunderstorm occurrence in NWP forecasts up to \SI{11}{\hour} in advance in a pixel-wise manner. The inference of the probability of thunderstorm occurrence is based on input parameters that are physically related to thunderstorm activity and do not explicitly feature information on location, time or forecast range. This gives reason to expect that the signature learned by the model generalizes to thunderstorms outside the study region of this work and remains valid in a changing climate. In addition, the availability of all input features in real time makes SALAMA readily available for operational use.

We have addressed the technical challenge caused by the rarity of thunderstorms and the corresponding small fraction of positive examples by increasing this fraction during training and analytically accounting for the increase when testing. This approach has allowed us to ensure reasonable reliability without calibration fits. We have argued that including a further calibration step would not affect the performance results of SALAMA.

Working with ensemble data, we have studied how the NWP forecast uncertainty depends on the lead time of the forecast and  related it to the classification skill decrease of SALAMA. This has suggested that the decrease in skill is the result of an increasing uncertainty in the input feature forecasting. 

During the training process, we have systematically varied the spatiotemporal criteria by which we associate lightning observations with NWP data. This has allowed us to test SALAMA with different spatial scales and to estimate the order of magnitude of the speed at which thunderstorms are advected in the atmosphere. We have shown that classification skill (quantified by the area under the precision-recall curve) increases with the spatial scale of the forecast and is higher than for a baseline model based on CAPE alone. Furthermore, we have found that the decay time of classification skill is proportional to the spatial scale. In combination with the result that the SALAMA classification skill is correlated with the NWP forecast uncertainty, our findings have indicated that resolving thunderstorms at smaller scales reduces the predictability of thunderstorm occurrence.

In a future work, it is useful to check the universality of the thunderstorm signature learned by SALAMA, e.g. by testing it on data outside of Central Europe or for a different time period than the summer of 2021. Moreover, one may explore whether classification skill can be improved by shifting from a pixel-wise consideration of input features to taking their spatiotemporal structure into account as well.





\section{Author contributions}
\textbf{K. Vahid Yousefnia:} Methodology; software; investigation; resources; visualization; writing - original draft; writing - review and editing.
\textbf{T. Bölle:} Conceptualization; writing - review and editing.
\textbf{I. Zöbisch:} Conceptualization; resources; writing - review and editing.
\textbf{T. Gerz:} Writing - review and editing; supervision.

\section{Data availability}
The Python code for SALAMA will be made available upon reasonable request.


\section{Acknowledgements}
We thank George Craig and Tobias Selz for helpful discussions. This work was funded through the internal project DIAL of the German Aerospace Center (DLR). We gratefully acknowledge the computational and data resources provided through the joint high-performance data analytics (HPDA) project "terrabyte" of the DLR and the Leibniz Supercomputing Center (LRZ). The authors declare that there are no conflicts of interest to disclose.


%\section{First Level Heading}
%Please lay out your article using the section headings and example objects below, and remember to delete all help text prior to submitting your article to the journal.
%
%\begin{figure*}[bt]
%\centering
%\includegraphics[width=6cm]{example-image-rectangle}
%\caption{Although we encourage authors to send us the highest-quality figures possible, for peer-review purposes we can accept a wide variety of formats, sizes, and resolutions. Legends should be concise but comprehensive -- the figure and its legend must be understandable without reference to the text. Include definitions of any symbols used and define/explain all abbreviations and units of measurement.}
%\end{figure*}
%
%\subsection{Second Level Heading}
%If data, scripts or other artefacts used to generate the analyses presented in the article are available via a publicly available data repository, please include a reference to the location of the material within the article.
%
%% Equations should be inserted using standard LaTeX equation and eqnarray environments, not as graphics, and should be set in the main text
%This is an equation, numbered
%\begin{equation}
%\int_0^{+\infty}e^{-x^2}dx=\frac{\sqrt{\pi}}{2}
%\end{equation}
%And one that is not numbered
%\begin{equation*}
%e^{i\pi}=-1
%\end{equation*}
%
%\subsection{Document class options}
%
%The \texttt{wiley-article} document class offers the following options:
%
%\begin{description}
%\item[\texttt{blind}] Anonymise all author, affiliation, correspondence
%       and funding information.
%
%\item[\texttt{lineno}] Adds line numbers.
%
%\item[\texttt{serif}] Sets the body font to be serif.
%
%\item[\texttt{twocolumn}] Sets the body text in two-column layout.
%
%\item[\texttt{num-refs}] Uses the numerical Vancouver bibliography style; see section \ref{sec:bibstyles}.
%
%\item[\texttt{alpha-refs}] Uses the author-year RSS bibliography style; see section \ref{sec:bibstyles}.
%\end{description}
%
%\subsection{Adding Citations and a References List}
%\label{sec:bibstyles}
%
%Please use a \verb|.bib| file to store your references. When using Overleaf to prepare your manuscript, you can upload a \verb|.bib| file or import your Mendeley or Zotero library directly as a \verb|.bib| file via the Add Files menu. You can then cite entries from it, like this: \cite{urmson2008autonomous,lees2010theoretical} and \cite{geiger2012we}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|.
%
%You can find a video tutorial here to learn more about BibTeX: \url{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}.
%
%This template provides two \verb|\documentclass| options for the citation and reference list style: 
%\begin{description}
%\item[Numerical style] Use \verb|\documentclass[...,num-refs]{wiley-article}| (Vancouver style)
%\item[Author-year style] Use \verb|\documentclass[...,alpha-refs]{wiley-article}| (RSS style)
%\end{description}
%
%If you are submitting to a journal that uses neither of the above, then instead of specifying one of these options in the document class you should use  \verb|\bibliographystyle| to specify the appropriate \verb|WileyNJD-...| style, and load the \verb|natbib| with the appropriate options as needed. If the journal provides you with an alternative .bst file to use, please upload this first, and then specify it with the \verb|\bibliographystyle| as above.
%
%\subsection{Usage Examples of \texttt{wiley-article} Options}
%
%\begin{enumerate}
%\item Using numerical, sort-by-authors citations:
%\begin{verbatim}
%\documentclass[num-refs]{wiley-article}
%\end{verbatim}
%
%\item Anonymised submission using author-year citations:
%\begin{verbatim}
%\documentclass[blind,alpha-refs]{wiley-article}
%\end{verbatim}
%
%\item Using \texttt{unsrtnat} for numerical, in-sequence citations:
%\begin{verbatim}
%\documentclass{wiley-article}
%\usepackage[numbers]{natbib}
%\bibliographystyle{unsrtnat}
%\end{verbatim}
%
%\item Using the \texttt{WileyNJD-AMA} reference style and superscript citations, two-column and serif fonts, for submission to AIChE:
%
%\begin{verbatim}
%\documentclass[serif,twocolumn]{wiley-article}
%\usepackage[super]{natbib}
%\bibliographystyle{WileyNJD-Harvard}
%\makeatletter
%\renewcommand{\@biblabel}[1]{#1.}
%\makeatother
%\end{verbatim}
%
%\end{enumerate}
%
%\subsubsection{Third Level Heading}
%Supporting information will be included with the published article. For submission any supporting information should be supplied as separate files but referred to in the text.
%
%Appendices will be published after the references. For submission they should be supplied as separate files but referred to in the text.
%
%\paragraph{Fourth Level Heading}
%% Here are examples of quotes and epigraphs.
%\begin{quote}
%The significant problems we have cannot be solved at the same level of thinking with which we created them.\endnote{Albert Einstein said this.}
%\end{quote}
%
%\begin{epigraph}{Albert Einstein}
%Anyone who has never made a mistake has never tried anything new.
%\end{epigraph}
%
%\subparagraph{Fifth level heading}
%Measurements should be given in SI or SI-derived units.
%Chemical substances should be referred to by the generic name only. Trade names should not be used. Drugs should be referred to by their generic names. If proprietary drugs have been used in the study, refer to these by their generic name, mentioning the proprietary name, and the name and location of the manufacturer, in parentheses.
%
%\begin{table*}[bt]
%\caption{This is a table. Tables should be self-contained and complement, but not duplicate, information contained in the text. They should be not be provided as images. Legends should be concise but comprehensive – the table, legend and footnotes must be understandable without reference to the text. All abbreviations must be defined in footnotes.}
%\begin{threeparttable}
%\begin{tabular}{lccrr}
%\headrow
%\thead{Variables} & \thead{JKL ($\boldsymbol{n=30}$)} & \thead{Control ($\boldsymbol{n=40}$)} & \thead{MN} & \thead{$\boldsymbol t$ (68)}\\
%Age at testing & 38 & 58 & 504.48 & 58 ms\\
%Age at testing & 38 & 58 & 504.48 & 58 ms\\
%Age at testing & 38 & 58 & 504.48 & 58 ms\\
%Age at testing & 38 & 58 & 504.48 & 58 ms\\
%\hiderowcolors
%stop alternating row colors from here onwards\\
%Age at testing & 38 & 58 & 504.48 & 58 ms\\
%Age at testing & 38 & 58 & 504.48 & 58 ms\\
%\hline  % Please only put a hline at the end of the table
%\end{tabular}
%
%\begin{tablenotes}
%\item JKL, just keep laughing; MN, merry noise.
%\end{tablenotes}
%\end{threeparttable}
%\end{table*}
%
%\section*{acknowledgements}
%Acknowledgements should include contributions from anyone who does not meet the criteria for authorship (for example, to recognize contributions from people who provided technical help, collation of data, writing assistance, acquisition of funding, or a department chairperson who provided general support), as well as any funding or other support information.
%
%\section*{conflict of interest}
%You may be asked to provide a conflict of interest statement during the submission process. Please check the journal's author guidelines for details on what to include in this section. Please ensure you liaise with all co-authors to confirm agreement with the final statement.
%
%\section*{Supporting Information}
%
%Supporting information is information that is not essential to the article, but provides greater depth and background. It is hosted online and appears without editing or typesetting. It may include tables, figures, videos, datasets, etc. More information can be found in the journal's author guidelines or at \url{http://www.wileyauthors.com/suppinfoFAQs}. Note: if data, scripts, or other artefacts used to generate the analyses presented in the paper are available via a publicly available data repository, authors should include a reference to the location of the material within their paper.

\printendnotes

% Submissions are not required to reflect the precise reference formatting of the journal (use of italics, bold etc.), however it is important that all key elements of each reference are included.
\bibliography{main.bbl}

%\begin{biography}[example-image-1x1]{A.~One}
%Please check with the journal's author guidelines whether author biographies are required. They are usually only included for review-type articles, and typically require photos and brief biographies (up to 75 words) for each author.
%\bigskip
%\bigskip
%\end{biography}

%\graphicalabstract{graphical_abstract.pdf}{SALAMA is a neural network model for predicting the probability of thunderstorm occurrence by post-processing convection-resolving ensemble forecasts. Shown is a case visualization for June 23, 2023, at 20~UTC, with pixels in which thunderstorm probability exceeds a decision threshold (red), and pixels in which lightning occurs (blue).}

\end{document}