\section{Analysis}
%In this section, we provide the experiment results starting with sensitivity analysis of EWC and LwF methods on the CIFAR-10. We conducted an analysis experiment to see the sensitivity of Elastic weight consolidation and learning without forgetting methods to their regularization hyperparameter. We run experiments by setting the lambda value manually to different numbers and provided the results in Figure \ref{fig:sensitivity}. After that we run experiments on the CIFAR-100 and mini-Imagenet if adaptive hyperparameter selection will yield better results than the vanilla approach which utilizes a fixed hyperparameter through all tasks.

%\begin{figure}[h]
 % \centering
 % \includegraphics[width=1\textwidth]{ewc_lambda_sensitivity.png}
 % \caption{Regularization Hyperparameter Sensitivity  of (a) EWC and (b) LwF methods on CIFAR-10}
 % \label{fig:sensitivity}
%\end{figure}

%As a first finding, we have observed that EWC and LwF methods are sensitive to the lambda hyperparameter since changing the lambda significantly fluctuates the incremental accuracy. The reason for that is if the lambda is set to a low value the model will aggressively update its parameters and learn the new task well but will forget the previous tasks drastically. On the other hand, if it is set to a high value it will remember the old task by making the least changes to model parameters but stop learning the new tasks at all. It is also not reasonable to find the best hyperparameter with a trial-and-error approach for every task since it is inefficient in terms of time and computation. Therefore, the strength of the regularization penalty should not be kept fixed rather it should be adjusted adaptively. Based on this motivation, we proposed an adaptive regularization approach and compare it with vanilla baseline on CIFAR-100 and mini-Imagenet datasets.

%\begin{figure}[h]
%  \centering
%  \includegraphics[width=1\textwidth]{cifar100_adaptive_figure.png}
%  \caption{Performance of Adaptive Regularization Compared to Fixed Regularization on CIFAR-100}
%  \label{fig:cifar100res}
%\end{figure}

%\begin{table}[h]
    %\centering
    %\small
    %\begin{tabular}{ccccccc}
    %\hline
        %~ & ~ & EWC & ~ & ~ & LWF &   \\ \hline
        %Method & Inc Acc & Inc Acc  & BWT \uparrow & Inc Acc  & Inc Acc & BWT \uparrow \\
        %& (last) \uparrow & (avg) \uparrow &  & (last) \uparrow & (avg) \uparrow & \\ %\hline
        %vanilla & 9.11$\pm$ 0.08 & 27.18 $\pm$ 0.23 & -86.16 $\pm$ 1.52 & 18.91 $\pm$ %0.13 & 39.03 $\pm$ 0.01 & -56.58 $\pm$ 10.14 \\
        %adaptive & 23.32 $\pm$ 0.87 & 40.76 $\pm$ 0.66 & -8.03 $\pm$ 4.76 & 27.29 $\pm$ %1.64 & 47.19 $\pm$ 0.22 & -25.24 $\pm$ 3.63 \\ \hline
    %\end{tabular}
    %\caption{Comparing adaptive regularization and fixed regularization  on Split CIFAR100}
%    \label{Table1}
%\end{table}

%On CIFAR-100 dataset, our approach CARBON consistently outperforms the vanilla baseline (Figure 3a, 3b) and it is nice to see that it variates the lambda value to balance between plasticity and stability while learning new tasks (Figure \ref{fig:cifar100res}c, \ref{fig:cifar100res}d). In Table \ref{Table1}, our adaptive approach shows a clear superiority to the vanilla baseline in terms of incremental accuracy (last), incremental accuracy (avg), and backward transfer score. We also observed that LwF is more robust regularization method in both vanilla and adaptive strategies in terms of incremental accuracy and backward transfer scores. We believe the reason behind this LwF is more responsive to its regularization hyperparamater than EWC which can be seen in Figure \ref{fig:sensitivity}. While EWC method requires high changes in the lambda hyperparameter to be effectively regularize the model parameters, the LwF method influences the learning performance more even with minimal changes in the hyperparameter. 


%%%%%%%%%%%%%%%%%%% Mert's Version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we address the following three research questions: 
\begin{itemize}
    \item \textbf{RQ1}: Does adaptive regularization improve backward transfer? 
    \item \textbf{RQ2}: To what extent adaptive regularization improve incremental learning accuracy per-task? 
    \item \textbf{RQ3:} How does the predicted $\lambda$ values differ across different incremental learning steps? 
\end{itemize}

%In what follows, we share our results and insights. 

%\begin{figure}[h]
%  \centering
%  \begin{subfigure}{0.47\textwidth}
%    \includegraphics[width=\textwidth]{figures/ceren_ewc_comparison_cifar100.pdf}
%    \caption{EWC results.}
 % \end{subfigure}
 % \begin{subfigure}{0.47\textwidth}
 %   \includegraphics[width=\textwidth]{figures/ceren_lwf_comparison_cifar100.pdf}
 %   \caption{LwF results.}
 % \end{subfigure}
 % \caption{Comparing fixed \textit{vs.} adaptive regularization on CIFAR100. Regularizing adaptively outperforms fixed regularization by a large margin, across all incremental learning steps.}
 %   \label{fig:mainresults1}
%\end{figure}

\subsection{RQ1: Does Adaptive Regularization Improve Backward Transfer?} 

First, we investigate the backward transfer performance of fixed \textit{vs.} adaptive regularization. The results are presented in Table~\ref{tab:main}. 

%Secondly, we study the influence of adaptivity on backward transfer. Adaptive regularization not only boosts the incremental accuracy overall but also improves the backward transfer as can be seen from Table~\ref{tab:main}.

\begin{table}[H]
\centering
\small
\begin{tabular}{ccccc}
\hline
\multicolumn{5}{c}{\textbf{CIFAR-100}}                                                                                  \\ \hline
                & \multicolumn{2}{c}{EWC}                      & \multicolumn{2}{c}{LwF}                       \\ \cline{2-5} 
Method          & ACC                  & BWT                   & ACC                  & BWT                    \\ \hline
Fixed           & $9.11$ $\pm$ $0.08$  & $-86.16$ $\pm$ $1.52$ & $18.91$ $\pm$ $0.13$ & $-56.58$ $\pm$ $10.14$ \\ \rowcolor{lightcyan} 
Adaptive (ours) & $23.32$ $\pm$ $0.87$ & $-8.03$ $\pm$ $4.76$  & $27.29$ $\pm$ $1.64$ & $-25.24$ $\pm$ $3.63$  \\ 
$\Delta$           & $14.21$              & $78.13$               & $8.38$               & $31.34$              \\ \hline
\multicolumn{5}{c}{\textbf{MiniImageNet}}                                                                               \\ \hline
                & \multicolumn{2}{c}{EWC}                      & \multicolumn{2}{c}{LwF}                       \\ \cline{2-5} 
Method          & ACC                  & BWT                   & ACC                  & BWT                    \\ \hline 
Fixed           & $8.84$ $\pm$ $0.22$  & $-84.87$ $\pm$ $1.25$ & $15.14$ $\pm$ $0.48$ & $-64.20$ $\pm$ $8.11$  \\ \rowcolor{lightcyan} 
Adaptive (ours) & $21.01$ $\pm$ $0.78$ & $-6.94$ $\pm$ $0.79$  & $28.23$ $\pm$ $1.72$ & $-27.09$ $\pm$ $4.79$  \\ 
$\Delta$           & $12.17$           & $77.93$               & $13.09$              & $37.11$ \\ \hline              
\end{tabular}
\caption{Final task accuracy and backward transfer scores of adaptive \textit{vs.} fixed regularization on CIFAR100 and MiniImageNet.  Regularizing adaptively outperforms fixed regularization by a large margin, in both ACC and BWT.}
\label{tab:main}
\end{table}

As can be seen from Table~\ref{tab:main}, adaptive regularization outperforms on both CIFAR100 and MiniImageNet. In CIFAR100 experiments, adaptivity has a more significant improvement on the accuracy of EWC. In MiniImageNet experiments, on the other hand, LwF improved slightly better than EWC in terms of accuracy score. 

We also observed that BWT performance significantly improved on EWC compared to LwF which signals EWC is more prone to forgetting, thus requires better adaptation to the current learning task. 

\subsection{RQ2: To What Extent Adaptive Regularization Improve Incremental Learning Per-Task?} 

In our second analysis, we compare the fixed- and adaptive- versions of EWC and LwF. Results are presented in Figure~\ref{fig:mainresults2}. 

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.49\textwidth}
    \caption{EWC results on CIFAR-100.}
    \includegraphics[width=\textwidth]{figures/ceren_ewc_comparison_cifar100.pdf}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \caption{LwF results on CIFAR-100.}
    \includegraphics[width=\textwidth]{figures/ceren_lwf_comparison_cifar100.pdf}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \caption{EWC results on MiniImageNet.}
    \includegraphics[width=\textwidth]{figures/ceren_ewc_comparison_minimgnt.pdf}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \caption{LwF results on MiniImageNet.}
    \includegraphics[width=\textwidth]{figures/ceren_lwf_comparison_minimgnt.pdf}
  \end{subfigure}
  \caption{Comparing fixed \textit{vs.} adaptive regularization on CIFAR-100 and MiniImageNet. Regularizing adaptively outperforms fixed regularization by a large margin, across all incremental learning steps.}
    \label{fig:mainresults2}
\end{figure}

As can be seen, regardless of  the method, adaptivity leads to a significant increase in top-1 accuracy, sometimes leading up to $20\%$ for EWC and $12\%$ for LwF on CIFAR100, while up to $14\%$ for EWC and $15\%$ for LwF on MiniImageNet. This confirms our hypothesis that regularization-based class-incremental learners can benefit a lot from learning to adapt to the current task.


%\partitle{EWC \textit{vs.} LwF.} An interesting observation from Figure~\ref{fig:mainresults1}, \ref{fig:mainresults2} is that EWC benefits more from adaptive regularization in comparison to LwF. This indicates prior-based regularization demands adaptation more than distillation-based methods, and we leave exploring different prior- and distillation- based works as future work. 


\subsection{RQ3: How Does the Predicted $\lambda$ Values Differ Across Incremental Learning Steps?} 

In our last analysis, we present per-incremental learning task $\lambda$ values in Figure~\ref{fig:lambda}. 

\begin{figure}[H]
  \centering
  \begin{subfigure}{0.47\textwidth}
    \includegraphics[width=\textwidth]{figures/ceren_ewc_comparison_lambda.pdf}
    \caption{EWC $\lambda$.}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth} % bu 45 kalsin cunku diger grafik bundan buyuk nedense
    \includegraphics[width=\textwidth]{figures/ceren_lwf_comparison_lambda.pdf}
    \caption{LwF $\lambda$.}
  \end{subfigure}
  \caption{$\lambda$ values with respect to incremental learning tasks. The model learns to adjust the magnitude of regularization per-learning task with high variability.}
    \label{fig:lambda}
\end{figure}

As is visible, our method is able to adjust the regularization magnitude according to the learning task with high variance, reaffirming that fixed regularization scheme is unnatural, and explains the superior performance reported in previous sections. 





