\section{Experimental Setup}
\label{sec:setup}

%In this section, we provide the setup that we designed for our experiments. We follow the class incremental learning setup of the PYCIL framework \citep{zhou2021pycil} which only extends the classifier layer by the number of classes per task. PYCIL offers various continual learning baselines and we implement the hyperparameter optimization on two regularization methods which are EWC and LwF. We use the RayTune framework \citep{liaw2018tune} for the hyperparameter optimization and we did not use a pre-trained architecture and train all tasks from scratch. All our experiments run on a single-GPU setup of Nvidia A100.

%\subsection{Datasets and Architecture}
%We use the commonly used Split CIFAR100 dataset as in most of the CIL research and to be sure that results are more convincing, we also use mini-Imagenet dataset to further investigate our adaptive approach. We applied the same transformations defined in the PYCIL framework except we set resize to 64 for mini-Imagenet dataset to fasten the training process. For Split CIFAR-100 experiments we run all experiments with 3 different seeds to see the effect of task ordering and we observed that the task order does not have a significant effect in terms of performance. Therefore, we did not run mini-Imagenet experiments with different seeds. 

%\noindent
%\textbf{1) Split CIFAR-10:} It includes 10 object classes \citep{krizhevsky2009learning}. We used Split CIFAR-10 dataset to make a sensitivity analysis to observe how sensitive the EWC and LwF methods to their regularization hyperparameter.

%\noindent
%\textbf{2) Split CIFAR-100:} It includes 100 classes of visual object instances of CIFAR-100 \citep{krizhevsky2009learning}. In our setup, we randomly divide 100 classes into 10 tasks with 10 classes per task.

%\noindent
%\textbf{3) mini-Imagenet:} It is a variation of Imagenet dataset containing 100 classes of visual objects \citep{deng2009imagenet}. In our setup, we randomly divide 100 classes into 10 tasks with 10 classes per task.

%We used a specific version of the Resnets called Resnet32 \citep{he2016deep} for all our experiments to interpret the results more clearly.

%\subsection{Hyperparameters}

%EWC and LwF methods introduce an important lambda hyperparameter that controls the trade-off between stability and plasticity. In our experiments, we selected the baselines as vanilla EWC and vanilla LwF. In the vanilla settings, we set lambda to 1 which gives equal emphasis to current and previous tasks, and kept it fixed during the whole learning. On the other hand, in our adaptive approach, CARBON searches pre-defined search space and chooses the best lambda value based on validation accuracy. The search space for lambda is set to [1, $10^5$] for EWC and [1, 50] for the LwF based on our sensitivity analysis.

%\subsection{Performance Metrics}

%To measure the performance of each configuration we evaluated the model based on the validation data which is simply a portion of the training data. We derived two different accuracy metrics from ACC given in Eq(\ref{eqn:acc}) and called them incremental accuracy (last) and incremental accuracy (avg). Incremental accuracy (last) denotes the top-1 accuracy after the last task and it is a proper metric to measure the overall accuracy among all learned classes. The incremental accuracy often decreases with more tasks learned since CIL is continually adapted. However, only comparing incremental accuracy (last) ignores the performance evaluation along the learning trajectory. Therefore, we denoted incremental accuracy (avg) which considers the performance after every incremental stage. The higher value indicates a stronger performance along the incremental stages \citep{zhou2023deep}. We  also measure the level of forgetting with a backward transfer metric BWT Eq(\ref{eqn:bwt}) where $A_{T, i}$ is the test accuracy for task $i$ after training on task $T$. Higher BWT indicates a lower forget ratio \citep{kang2022forget}.

%\begin{equation}
%\label{eqn:acc}
%ACC = {\frac{1}{T}} \sum_{i=1}^{T} A_{T, i} 
%\end{equation}


%\begin{equation}
%\label{eqn:bwt}
%BWT = {\frac{1}{T-1}} \sum_{i=1}^{T-1} A_{T, i} - A_{i, i}
%\end{equation}


%%%%%%%%%%%%%%%%%%% Mert's Version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \partitle{Datasets.} In this paper, we experiment with \textbf{Split-CIFAR100}~\citep{krizhevsky2009learning} and \textbf{Split-MiniImageNet}~\citep{deng2009imagenet}. Each dataset exhibits objects from $100$ different categories, such as bird, snake and spider. We train all the models with $10$ tasks, with $10$ classes within each learning task on both CIFAR100 and MiniImageNet. Both datasets have 5000 training, and 1000 testing color images per learning task, each with $32\times32$ and $64\times64$ resolution for CIFAR100 and MiniImageNet respectively. 
%%%%% V1 %%%%%%%%%%%%%%%%%%
 
 %\partitle{Metrics.}  We evaluate the performance of our model using two standard metrics, top-1 accuracy and backward transfer for  
%$T$ tasks~\citep{diaz2018don}. Specifically, accuracy measures the test performance for all the observed tasks until $T$, whereas backward transfer measures how well the inclusion of task at time step $t-1$ influences the performance on task at previous time steps. Formally: 
%\begin{eqnarray}
%\mbox{\small {\bf Average Accuracy: } \normalsize ACC} & = & \frac{1}{T}
%\sum_{i=1}^T A_{T,i} \label{eq:acc} \\
%\mbox{\small {\bf Backward Transfer: } \normalsize BWT} & = & \frac{1}{T-1}
%\sum_{i=1}^{T-1} A_{T,i} - A_{i,i}   \label{eq:bwt} \\
%\end{eqnarray}
%\noindent where $A_{T,i}$ is the test accuracy of the model on task $i$ at time step $T$. For each metrics, the higher is better. 


%%%%%%%%%%%%%% V2 %%%%%%%%%%%%%%%%%%%%%

\partitle{Metrics.}  We resort to the standard metrics for evaluation, accuracy (ACC) which measures the final accuracy averaged over all tasks,  and backward transfer (BWT) which measures the average accuracy change of each task after learning new tasks. Formally for accuracy:
{\small
\vspace{-0.1cm}
\begin{align}
    ACC=\frac{1}{T}\sum\nolimits_{i=1}^T A_{T,i},
    \vspace{-0.1cm}
\end{align}}%

\noindent and for backward transferability: 
{\small
\begin{align}
    BWT=\frac{1}{T-1}\sum\nolimits_{i=1}^{T-1} (A_{T,i}-A_{i,i})
    \vspace{-0.1cm}
\end{align}}

\noindent where $A_{T,i}$ represents the testing accuracy of task $T$ after learning task $i$. In both cases, higher values indicate better performance. 