\section{Empirical Method}
\label{sec:empirical method}

%Regularization techniques aim to minimize the parameter updates of a model during the process of learning new tasks, with the purpose of preventing the phenomenon of catastrophic forgetting. LWF and EWC are among the most widely recognized and commonly utilized methods in research. However, the issue with these methods is that they are very sensitive to their hyperparameters \citep{hua2022hyperparameter}. This hyperparameter is assumed to be the same for all tasks and fixed at the beginning of the learning process in the current literature. Instead, the regularization hyperparameter determined in the previous task may be different while learning the next tasks since each task might differ from the other. In order to achieve this, it is crucial to utilize an adaptive method for hyperparameter selection. That is why, in this study, we have adopted an AutoML tool Bayesian Optimization Tree Parzen Estimator to adaptively select regularization hyperparameter since AutoML demonstrated their effectiveness in this area.

%\paragraph{Class Incremental Learning}
%CIL aims to learn from an incoming new stream of tasks that each task includes unique classes. Assume there is a sequence of $T$ number of tasks  {$D_1, D_2, …, D_T$} without overlapping classes where $D_t = {(x_i^t , y_i^t)_i_=_1}^n$ is the $t$-th incremental step with $n$ training samples and each instance has a class of $y_i \in Y_t$. $Y_t$ is the label space of task t where ${Y_t \cap Y_{t}^’} = \varnothing$  for  $t \neq t^’$. In the CIL setup the classification layer is expanded with newly learned classes and the trained model is evaluated over all learned classes so far \citep{zhou2023deep}. 

%\paragraph{Elastic Weight Consolidation}
%EWC is the first work to address parameter regularization. This method aims to determine the important network parameters during the learning process and to keep the important ones constant to preserve prior knowledge. In addition to cross-entropy loss Eq(\ref{eqn:crossentropy}) where $N$ is the total number of training examples, $C$ is the number of classes. $y_{ij}$ is the ground truth label for the $i$-th training example and $j$-th class, and $p_{ij}$ is the predicted probability of the $i$-th training example belonging to the $j$-th class, EWC builds a regularization term and adds it to the cross entropy loss to remember the old ones as stated in Eq(\ref{eqn:ewcloss}) where $n$ denotes the number of tasks and $d$ is the number of parameters in a model \citep{kirkpatrick2017overcoming}. $\theta_{ij}$ is the $i$-th parameter in the $j$-th layer. Thus, $\Delta\theta_{ij}$ represents the parameter change from old task to new task.  

%The $\lambda$ term is a hyperparameter of the EWC method that controls the amount of change in the model parameters \citep{kirkpatrick2017overcoming}. When $\lambda$ is set to 1, the model assigns equal importance to both old and new tasks. However, when the $\lambda$ is set to higher values it indicates that the model should give more attention to old task parameters to prevent forgetting.

%\begin{equation}
%\label{eqn:crossentropy}
%    L_{CE} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C} y_{ij}\log(p_{ij})
%\end{equation}

%\begin{equation}
%\label{eqn:ewcloss}
%    L_{EWC} = L_{CE} + \sum_{i=1}^{n}\frac{\lambda}{2}\left[\sum_{j}^{d}(\Delta\theta_{ij})^2\frac{\partial^2L_{CE}}{\partial\theta_{ij}^2}\right]
%\end{equation}

%\paragraph{Learning without Forgetting}
%We can define an “old model” that is trained with previous tasks $D_{t-1}$ and “new model” that is currently updated with the new task $D_t$. One approach to transfer knowledge from an old model to a new model is by using knowledge distillation (KD). LwF adopts KD with using the old model as a teacher and a new model as a student with the goal of teaching a student model not to forget previous knowledge by forcing to create similar logits \citep{li2017learning}. LWF is the first method that adopts the KD in CIL \citep{zhou2023deep}. The $\lambda$  hyperparameter is responsible for regulating the amount of regularization stated in Eq(\ref{eqn:lwfloss}) and addressing the forgetting problem, just like EWC algorithm.

%Let $D_t=\{(x_i,y_i)\}_{i=1}^{n_t}$ be the source dataset and $D_{t+1}=\{(x_i)\}_{i=1}^{n_{t+1}}$ be the target dataset. The objective of LwF is to learn a classifier $f_\theta$ that can classify both source and target data, without forgetting the knowledge learned from the source dataset. To achieve this, LwF minimizes the following loss function:
%\begin{equation}
%\label{eqn:lwfloss}
%L(\theta) = L_{CE} + \lambda L_{t+1}(\theta,\theta_s),
%\end{equation}

%where $L_{t+1}(\theta,\theta_s)$ is the distillation loss between the current model $f_\theta$ and the previously learned model, and $\lambda$ is a hyperparameter that controls the importance of the distillation loss.

%\paragraph{Bayesian Optimization Tree Parzen Estimator}
%Hyperparameters are the settings or configurations of a machine learning algorithm that are set before training, such as the learning rate, the number of hidden layers, or the regularization parameter. These settings can have a significant impact on the performance of the algorithm, and finding the optimal hyperparameters can be a difficult and time-consuming task. Therefore, to find the optimal hyperparameters, several optimization methods are utilized. Bayesian optimization is a widely accepted method for finding optimal hyperparameters of an algorithm. It works by constructing a probabilistic model of the objective function (e.g. the validation loss or accuracy) and uses it to guide the optimization search. During the search, a set of hyperparameters are selected iteratively and evaluated with objective function and the model is updated based on the new information. The goal is to find the best hyperparameters that maximize/minimize the objective function \citep{frazier2018tutorial}. 

%Tree-structured Parzen Estimators (TPE) is a specific implementation of Bayesian optimization that uses a tree-structured model to represent the probability distribution over the hyperparameters. The TPE algorithm constructs two separate models which are for the probability of hyperparameters that are “good” and “bad”. Good refers to resulting in better results on the objective function and bad is vice versa. Then, the algorithm uses the ratio of these probabilities to guide the search for optimal hyperparameters \citep{bergstra2011algorithms}.

%\paragraph{Asynchronous Successive Halving}
%AutoML algorithms can be time-consuming, especially while working with complex datasets and big models. A scheduler is a beneficial tool to optimize the use of time by running different algorithms in parallel or sequentially. Successive halving is a machine learning algorithm that optimizes models by training multiple models for a set number of iterations, and removing the lowest-performing models until only one model remains. Asynchronous Successive Halving is a modified version of the algorithm that trains models asynchronously on different computing nodes and eliminates the worst models as soon as they are identified. This approach enables faster convergence and more efficient utilization of resources in distributed computing environments \citep{li2018massively}. In this study, we benefit from the Asynchronous Successive Halving algorithm as a scheduler.

%\paragraph{CARBON: Continual Adaptive Regularization with Bayesian Optimization}
%We introduce an adaptive regularization that selects the best regularization strength for EWC and LwF while considering both previously learned and new tasks by employing Bayesian Optimization in class incremental scenarios. To this end, we extend our validation set  by adding 20\% of training samples after learning each task. Therefore, the model will be able to consider the current task as well as previous tasks while selecting the best regularization strength. To be more clear, we train the model with the current task only as usual but benefit from older tasks while validating which creates no computational overhead costs during training. 



%%%%%%%%%%%%%%%%%%% Mert's Version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\partitle{Overview.} In class-incremental learning, the goal is to update a neural network with an incoming stream of new data. Formally, the learner receives a sequence of learning tasks $\mathcal{T}_{1:t} = (\mathcal{T}_{1}, \mathcal{T}_{2}, ...,\mathcal{T}_{t})$, with a corresponding dataset $\mathcal{D}_{\mathcal{T}} = \{ (x_{i,t}, y_{i,t})_{i=1}^{n_{t}} \}$ with $n_{t}$ instances per-task. Here, an input pair $\{x_{i,t}, y_{i,t}\} \in \mathcal{X}_{t} \times \mathcal{Y}_{t}$ is sampled from an unknown distribution. Learning tasks are mutually exclusive: $\mathcal{Y}_{t-1} \cap \mathcal{Y}_{t} = \emptyset$. When the learning task arrives, a deep convolutional network is optimized to embed the input instance into the classifier space $f_{\Theta}: \mathcal{X}_{t}\rightarrow \mathcal{Y}_{t}$, where $\Theta$ is the parameters of the learner. We drop the subscript for clarity from now on. 


\partitle{Overview.} Class-incremental learning involves updating a neural network with new data as it comes in. Specifically, the learner receives a sequence of learning tasks $\mathcal{T}_{1:t} = (\mathcal{T}_{1}, \mathcal{T}_{2}, ...,\mathcal{T}_{t})$, each with a corresponding dataset $\mathcal{D}_{\mathcal{T}} = { (x_{i,t}, y_{i,t})^{n_{t}} }$ consisting of $n_{t}$ instances per task. Each input pair ${x_{i,t}, y_{i,t}} \in \mathcal{X}{t} \times \mathcal{Y}{t}$ is sampled from an unknown distribution. It's important to note that the learning tasks are mutually exclusive, i.e., $\mathcal{Y}_{t-1} \cap \mathcal{Y}_{t} = \emptyset$.

When a new learning task arrives, a deep convolutional network is optimized to embed the input instance into the classifier space $f_{\Theta}: \mathcal{X}_{t}\rightarrow \mathcal{Y}_{t}$, where $\Theta$ represents the parameters of the learner. To improve clarity, we will drop the subscript moving forward.


%When the learning task arrives, a deep convolutional network is optimized to embed the input instance into the classifier space $f_{\Theta}: \mathcal{X}_{t}\rightarrow \mathcal{R}^{D}$. Also, a linear classifier further projects this input to the class space of the current task $h_{\phi}: \mathcal{R}^{D}\rightarrow\mathcal{Y}_{t}$ jointly parameterized by $\mathcal{W}_{t}=\{\Theta_{t}, \phi_{t}\}$ for the current task $t$. 

%Also, a linear classifier further projects this input to the class space of the current task $h_{\phi}: \mathcal{R}^{D}\rightarrow\mathcal{Y}_{t}$ jointly parameterized by $\mathcal{W}_{t}=\{\Theta_{t}, \phi_{t}\}$ for the current task $t$. 

%The incremental learner optimizes for two goals: To effectively learn the current task (\textit{plasticity}), while retaining the performance on all the previous tasks (\textit{stability}). This is achieved via optimizing the following function: 

%\begin{align}
%    \label{equ:main}
%    \mathcal{L} = CE(f(\mathcal{X}_{t}), \mathcal{Y}_{t})  + \lambda \cdot Reg(\Theta)
%\end{align}

%\noindent where $CE(\cdot)$ is the standard Cross-Entropy used in classification, and $Reg(\cdot)$ is a regularization term penalizing abrupt changes of the neural network weights~\citep{li2017learning,kirkpatrick2017overcoming}. 

The incremental learner has two goals: to effectively learn the current task (\textit{plasticity}) while retaining performance on all previous tasks (\textit{stability}). This is accomplished by optimizing the following function:

\begin{align}
\label{equ:main}
\mathcal{L} = CE(f(x_{i,t}), y_{i, t}) + \lambda \cdot Reg(\Theta)
\end{align}

\noindent Here, $CE(\cdot)$ represents the standard Cross-Entropy used in classification, and $Reg(\cdot)$ is a regularization term that penalizes abrupt changes in the neural network weights~\citep{li2017learning,kirkpatrick2017overcoming}.

%\partitle{Regularization Constancy Assumption.} Here, the scalar parameter $\lambda$ balances the contribution of the classification and regularization loss functions. A big $\lambda$ would ensure that minimal weight updates are performed, while sacrificing the learning on the current task. On the contrary, a small $\lambda$ would yield good performance on the current task, while sacrificing the performance on the previous tasks, exacerbating catastrophic forgetting. The de-facto approach is to set $\lambda$ to a fixed scalar value throughout all incremental learning sessions $\lambda_{t-1} = \lambda_{t}$, $ \forall t \in \mathcal{T}_{1:t}$.  

\partitle{Regularization Constancy Assumption.} The scalar parameter $\lambda$ balances the contribution of the classification and regularization loss functions. A large value of $\lambda$ ensures minimal weight updates, which can sacrifice learning on the current task. Conversely, a small $\lambda$ yields good performance on the current task but may sacrifice performance on previous tasks, exacerbating catastrophic forgetting. As a common practice, $\lambda$ is set to a fixed scalar value throughout all incremental learning sessions, such that $\lambda_{t-1} = \lambda_{t}$ for all $t \in \mathcal{T}_{1:t}$.

%Although simple, in this paper, we hypothesize that such assumption of \textit{regularization constancy} is unrealistic to build accurate lifelong learning machines. Our reasoning is two-fold: 
%\begin{itemize}
%    \item \textbf{Low Plasticity and High Stability:} The incremental learner may observe a novel object highly familiar with the previous learning tasks. For example, it may observe the category \textit{dog} after observing many other animal categories, such as \textit{\{cat, cow, bird\}}. This implies that the incremental learner does not have to be too plastic, as it can quickly transfer knowledge from previously learned tasks. Hence, no drastic updates to the learned filters are necessary. 
%    \item \textbf{High Plasticity and Low Stability:} The incremental learner may observe a novel object highly unfamiliar with the previous learning tasks. For example, it may observe the category \textit{car} after observing many other animal categories, such as \textit{\{cat, cow, bird\}}. This implies that the incremental learner requires high plasticity to learn about the novel object with never-before-seen parts, such as wheels. 
%\end{itemize}


We hypothesize that the assumption of \textit{regularization constancy} is unrealistic for building accurate lifelong learning machines. Our reasoning is twofold:

\begin{itemize}
\item \textbf{Low Plasticity and High Stability:} The incremental learner may encounter a novel object that is highly familiar with the previously learned tasks. For example, it may encounter the category \textit{dog} after observing many other animal categories, such as \textit{{cat, cow, bird}}. In this case, the learner does not need to be too plastic, as it can quickly transfer knowledge from the previous tasks. Hence, no drastic updates to the learned filters are necessary.

\item \textbf{High Plasticity and Low Stability:} The incremental learner may encounter a novel object that is highly unfamiliar with the previous tasks. For example, it may encounter the category \textit{car} after observing many other animal categories, such as \textit{{cat, cow, bird}}. In this case, the learner requires high plasticity to learn about the novel object with never-before-seen parts, such as wheels.
\end{itemize}


%\partitle{Adaptive Regularization.} Motivated by this, in this paper, we advocate that the regularization magnitude should be a function of time, conditioned on the current learning task and all previous tasks. Formally: 

%\begin{align}
%\lambda(t) = {\lambda_1, \lambda_2, \ldots, \lambda_{t-1}, \lambda_t}
%\end{align}

%\noindent where $\lambda_{t}$ is predicted by minimizing the following optimization %problem: 

%\begin{aligned}
%\label{equ:optim}
%\lambda^* &= \arg\min_{\lambda} \mathcal{L}(\Theta;\lambda, V_{t})\
%&= \arg\min_{\lambda} \sum_{i=1}^{|V_{t}|} \left[CE(f(x_{i,t};\Theta), y_{i,t}) + %\lambda \cdot Reg(\Theta)\right]
%\end{aligned}

\partitle{Adaptive Regularization.} This paper argues that the assumption of regularization constancy is unrealistic and proposes an alternative approach: adaptive regularization. In this approach, the regularization magnitude is a function of time, conditioned on the current learning task and all previous tasks. Formally, we define $\lambda(t) = {\lambda_1, \lambda_2, \ldots, \lambda_{t-1}, \lambda_t}$, where $\lambda_{t}$ is predicted by minimizing the following optimization problem:

\begin{align}
\label{equ:optim}
\lambda^* &= \arg\min_{\lambda} \mathcal{L}(\Theta;\lambda, V_{t})
= \arg\min_{\lambda} \sum_{i=1}^{|V_{t}|} \left[CE(f(x_{i,t};\Theta), y_{i,t}) + \lambda \cdot Reg(\Theta)\right]
\end{align}

\noindent Here, $V_{t}$ is the subset of the validation set for the current task and previous tasks, and $\mathcal{L}(\Theta;\lambda, V_{t})$ is the loss function with the regularization coefficient $\lambda$ determined by solving the optimization problem. This approach allows us to automatically adjust the regularization strength according to the specific learning task, avoiding the unrealistic assumption of a fixed regularization strength throughout the learning process.

%\noindent where $V_{t}$ is the validation set that stores a subset of the current task and previous task data, and $\lambda^*$ is the optimal regularization strength at the time step $t$. This simple modification will resolve the aforementioned issues of regularization constancy. If the learner encounters a novel, unfamiliar object, this will elevate the classification loss for the current learning task. Hence, the expression will yield a smaller $\lambda$ value (low regularization) to allow the model to fit this new task. Similarly, if the model encounters a familiar object, this time the expression will yield a higher $\lambda$ to keep the classification loss for the previously observed categories minimal. 

In what follows, we describe the regularization as well as the optimization objectives adopted in our work. 

\subsection{Regularization}


To regularize the weights of the backbone, we experimented with two popular, well-established techniques: LwF~\citep{li2017learning} and EWC~\citep{kirkpatrick2017overcoming}.

\partitle{LwF.} Learning-without-Forgetting~\citep{li2017learning} is a knowledge-distillation approach where the teacher branch is the model from the previous task, and the student branch is the current model. The aim is to match the activations of the teacher and student branches, either at the feature or logit layer. We found that logit-based distillation yielded better performance. Formally, LwF minimizes the following objective:

\begin{align}
Reg(\Theta) = KL(f(x_{i,t}) , f^{\prime}(x_{i,t}))
\end{align}

\noindent where $f^{\prime}$ is the model from the previous learning step, and $KL(p_1, p_2)$ is the KL-divergence between two probability distributions $p_{1}$ and $p_{2}$.

\partitle{EWC.} Elastic Weight Consolidation~\citep{kirkpatrick2017overcoming} can be viewed as a weighted regularization approach. The authors argue that not all weights contribute equally to learning a new task. Thus, they estimate the importance of each weight in minimizing the classification loss for the current task: $Reg(\Theta) = || \mathcal{F}(\Theta - \Theta^{\prime}) ||$, where $\Theta^{\prime}$ is the model weights from the previous learning step, $\mathcal{F}$ is the Fisher matrix of the same size as the weight matrices $\Theta$, re-weighting the contributions of each weight to stabilize the important neurons per task.


%To regularize the weights of the underlying backbone, we experiment with two prominent techniques, namely Learning-without-Forgetting (LwF)~\citep{li2017learning} and Elastic-Weight-Consolidation (EWC)~\citep{kirkpatrick2017overcoming}. 

%\partitle{LwF.} Learning-without-Forgetting~\citep{li2017learning} is a teacher-student knowledge-distillation approach. The teacher branch is the model from the previous learning task, whereas the student branch is the model from the current learning task. The goal is to match the activations of the teacher-student, either at the feature layer (\ie previous layers) or the logit layer (\ie classification layer). We experimented with both, and found out that logit-based distillation yields better performance. Formally, LwF minimizes the following objective: 

%\begin{align}
%    Reg(\Theta) = KL(f(x_{i,t}) , f^{\prime}(x_{i,t}))
%\end{align}

%\noindent where $f^{\prime}$ is the model from the previous learning step, $KL(p_1, p_2)$ is KL-divergence between two probability distributions $p_{1}$ and $p{2}$. 

%\partitle{EWC.} Elastic Weight Consolidation~\citep{kirkpatrick2017overcoming} can be seen as a weighted regularization approach. The authors claim that to learn a novel task, not all weights contribute equally. To that end, the authors estimate the importance of each weight in minimizing the classification loss for the current task: $Reg(\Theta) = || \mathcal{F}(\Theta - \Theta^{\prime}) ||$, where $\Theta^{\prime}$ is the model weights from the previous learning step, $\mathcal{F}$ is the fisher matrix of the same size as the weight matrices $\Theta$, re-weighting the contributions of each weight to stabilize the important neurons per-task. 

\subsection{Bayesian Optimization via Parzen Estimator}

We optimize the equation~\ref{equ:optim} using multivariate tree-structured parzen estimator (TPE)~\citep{bergstra2011algorithms}, thanks to its high-performance and accuracy. TPE builds a conditional probability tree that maps hyperparameters to their respective model performances. Then it can be used to guide a search algorithm to find the optimal set of hyperparameters for the given model. Specifically, we build upon the implementation from the RayTune library~\citep{liaw2018tune}.

The estimator is trained on the validation set accumulated across incremental learning sessions. We store  a small subset of $20\%$ of the validation data from each incremental learning step in the memory. The search space for $\lambda$ is set to [1, $10^5$] for EWC and we uniformly sampled $20$ configurations. Similarly, the search space for $\lambda$ is set to [1, 50] for LwF and uniformly sampled $15$ configurations.

 \partitle{Implementation Details.} We implement all the methods in PyTorch~\citep{NEURIPS2019_9015}. We use ResNet-$32$ as the backbone~\citep{he2016deep}. We rely on the PyCIL library~\citep{zhou2021pycil} for the regularization objectives EWC~\citep{kirkpatrick2017overcoming} and LwF~\citep{li2017learning}, and use all the hyperparameters as is without further tuning. We set number of epochs to $100$ for each configuration but used the Asynchronous Successive Halving~\citep{li2018massively} scheduler for a more efficient search. We set an initial learning rate of $0.1$ which is decayed by factor $0.1$ at epoch $60$ for the first task and $70$ for the rest of the tasks. We use SGD optimizer with momentum parameter set to 0.9 and weight decay set to $5e^4$ for the first task and $2e^4$ for the rest of the tasks. The batch size is set to $128$. We run experiments on three different seeds ($2, 1993, 2022$) and report their average. 


%\partitle{Regularization.}

%\partitle{Hyper-Param Optimization.}

