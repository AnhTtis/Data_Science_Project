
\section{Motivation}
%In recent years deep neural networks have proven their superiority in various applications. The standard way of training deep models is to collect the whole data beforehand and train the model with all the available data \citep{silver2018general, russakovsky2015imagenet}. However, in our dynamic world, this is not a realistic scenario since the training data comes in a stream format and it is not possible or efficient to hold all these streams for long due to storage and privacy constraints \citep{krempl2014open}. Thus, this requires models to adapt themselves incrementally when a new task arrives. Continual learning studies the challenge of learning from an infinite stream of data with the intention of gradually extending learned knowledge \citep{chen2018lifelong}. A successful model should find a good balance between retaining the patterns of previously learned knowledge while representing the new one. This balance is referred to as the "stability-plasticity dilemma" in neural systems. Stability refers to the capacity to maintain previous knowledge, while plasticity refers to the ability to adjust to new patterns \citep{grossberg2012studies}. 

%In incremental learning, there are 3 fine-grained settings which are known as Domain-Incremental Learning (DIL) where the new tasks contain the same classes as previous ones but with distribution change, Class-Incremental Learning (CIL) where always new classes observed within the new task, finally Task-Incremental Learning (TIL) which is exactly same as CIL but the difference lies in the inference stage where TIL only requires classifying the instance among corresponding task space and does not require to perform cross-task discrimination. Therefore, TIL is known to be the easiest setup while CIL is the hardest scenario among these 3 fine-grained settings \citep{zhou2023deep}. In this study, our evaluations are done based on the CIL setup.

%To overcome catastrophic forgetting while continuously learning there are three main strategies: Rehearsal-based methods, Parameter-isolation/Architecture-based methods, and Regularization-based methods. Rehearsal-based methods store real or synthesized exemplars from each task and revisit them during the training process of the new task, this way it aims to prevent catastrophic forgetting \citep{rebuffi2017icarl, bang2021rainbow, aljundi2019gradient}. Parameter-isolation/Architecture-based methods propose to minimize the inter-task interference via designing subnetworks \citep{mallya2018piggyback, li2019learn} or growing the architecture with respect to the coming task \citep{rusu2016progressive, {wang2022foster}}. Regularization-based methods introduce an extra regularization term in the loss function when learning the new task to consolidate previous knowledge \citep{kirkpatrick2017overcoming, zenke2017continual, li2017learning}. This regularization term introduces an extra hyperparameter that needs to be carefully tuned to balance the stability-plasticity trade-off in continual learning \citep{hua2022hyperparameter}. Currently, this hyperparameter is usually selected based on an expert opinion or via grid search and kept fixed during the whole incremental stages. However, fixing the regularization hyperparameter is not always true since the hyperparameters found to work well for the previous task may not work well at further learning process. To address this issue, in this paper, we propose an adaptive approach CARBON (Figure \ref{fig:CARBON}) which allows to select the regularization hyperparameter automatically at the beginning of each learning phase. We conduct a set of experiments on CIFAR-100 and mini-Imagenet to prove that adapting the learners to the shifts is crucial for mitigating forgetting and a promising way to build more robust regularization-based continual learners.

%\begin{figure}[t]
%  \centering
%  \includegraphics[width=1\textwidth]{carbon.png}
 % \caption{Adaptive regularization selects the best regularization hyperparameter while considering both previous and new tasks by employing Bayesian Optimization in class incremental scenarios. While vanilla approach keeps the regularization hyperparameter fixed through all incremental steps, our method CARBON is able to adapt itself.}
 % \label{fig:CARBON}
%\end{figure}


%%%%%%%%%%%%%%%%%%% Mert's Version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\partitle{Motivation for class-incremental learning.}

%\partitle{Approaches to CIL: Regularization and Memory.}

%\partitle{Limitation of regularization approaches: Constancy assumption}

%\partitle{This work.}

%\partitle{Contributions.}

%This paper strives for Class-Incremental Learning (CIL) of deep neural network representations. CIL is an alternative method for training neural networks~\citep{xxx}, which does not assume access to the data from all categories at once, as is done in standard batch learning. Hence, it allows us to update a deep pre-trained classifier with novel categories \emph{without} revisiting the old data. For example, one can expand a pre-trained classifier of $\{dogs, cats\}$ with the novel category of \emph{cars}, with no re-training on the previously observed classes, see Figure~\ref{fig:teaser}. 

%This paper tackles Class-Incremental Learning (CIL) of deep neural network representations~~\citep{masana2020class,de2021continual}. Unlike standard batch learning, which requires access to data from all categories at once, CIL does not make this assumption. This enables updating a pre-trained deep classifier with novel categories without revisiting old data. 

This paper focuses on Class-Incremental Learning of deep neural network representations~\citep{masana2020class,de2021continual}. Unlike standard batch learning, which requires access to data from all categories simultaneously, Class-Incremental Learning can update a pre-trained deep classifier with new categories without revisiting old data. This allows for more efficient learning and avoids the need to store large amounts of old data.

%For instance, one can add a new category like "cars" to a pre-trained classifier of "dogs" and "cats" without retraining on the old classes, see Figure~\ref{fig:teaser}.

%The ability of CIL to dynamically expand a classifier with no retraining comes with the important cost of \emph{catastrophic forgetting}~\citep{xxx} of previously seen classes. The deep learner sacrifices accuracy on old categories to accommodate learning of the novel categories. To mitigate forgetting, two major directions explore regularization~\citep{xxx} and replay~\citep{xxx}. Replay stores a few exemplars per-class within the memory to replay during new learning increments, whereas regularization prevents abrupt shifts of the neural network weights. In this paper, we follow regularization-based approach due to their simplicity and effectivity. 

%CIL's ability to expand a classifier without retraining comes with a significant cost known as \emph{catastrophic forgetting}, where the deep learner sacrifices accuracy on previously seen classes to learn the novel ones. To address this issue, two major approaches have been explored: regularization~\citep{kirkpatrick2017overcoming,li2017learning} and replay~\citep{lopez2017gradient,chaudhry2018efficient}. Replay stores a few exemplars per-class in memory and replays them during new learning increments, while regularization prevents abrupt shifts in the neural network weights. In this paper, we adopt the regularization-based approach because of its effectiveness and simplicity.

While Class-Incremental Learning enables expanding a classifier without retraining, it often results in a significant cost known as \emph{catastrophic forgetting}. This occurs when the deep learner sacrifices accuracy on previously seen classes to learn new ones. Two major approaches have been explored to address this issue: regularization~\citep{kirkpatrick2017overcoming,li2017learning} and replay~\citep{lopez2017gradient}. Regularization prevents abrupt shifts in the neural network weights, while replay stores a few exemplars per-class in memory and replays them during new learning increments. This paper focuses on regularization, as it is simple yet effective. 

%There are two main approaches for regularization, namely prior-based and distillation-based approaches. Prior-based methods predict the importance of each weight, and ensure they remain stable across learning sessions~\citep{xxx}, often with the help of fisher matrix~\citep{xxx}. Notable examples include EwC~\citep{xxx}, SI~\citep{xxx}, RWalk~\citep{xxx} and IMM~\citep{xxx}. Distillation-based approaches operate on a teacher-student setting, and ensures the output of the teacher from the previous learning session aligns with the student model in the current session. Notable examples include LwF~\citep{xxx}, iCaRL~\citep{xxx}, BiC~\citep{xxx} and COIL~\citep{xxx}. Despite the increasing performance regardless of methodology, these methods fix the regularization magnitude throughout learning, and solely focus on \emph{how} to regularize.

%There are two main regularization approaches: prior-based and distillation-based. Prior-based methods predict the importance of each weight and ensure they remain stable across learning sessions using techniques such as the Fisher matrix~\citep{le2012asymptotic}. Examples of prior-based methods include EwC~\citep{kirkpatrick2017overcoming}, SI~\citep{zenke2017continual}, RWalk~\citep{chaudhry2018riemannian}, and IMM~\citep{lee2017overcoming}. Distillation-based approaches work in a teacher-student setting and ensure the output of the teacher from the previous learning session aligns with the student model in the current session. Examples of distillation-based methods include LwF~\citep{li2017learning}, iCaRL~\citep{rebuffi2017icarl}, BiC~\citep{wu2019large}, and COIL~\citep{zhou2021co}. Although these methods show improved performance, they fix the regularization magnitude throughout learning and solely focus on \emph{how} to regularize.


Regularization-based approaches to address catastrophic forgetting can be categorized into two main types: prior-based and distillation-based. Prior-based methods predict the importance of each weight and ensure their stability across learning sessions using techniques such as the Fisher matrix~\citep{le2012asymptotic}. Examples of prior-based methods include EwC~\citep{kirkpatrick2017overcoming}, SI~\citep{zenke2017continual}, RWalk~\citep{chaudhry2018riemannian}, and IMM~\citep{lee2017overcoming}. In contrast, distillation-based approaches work in a teacher-student setting and align the output of the previous teacher model with the current student model. Examples of distillation-based methods include LwF~\citep{li2017learning}, iCaRL~\citep{rebuffi2017icarl}, BiC~\citep{wu2019large}, and COIL~\citep{zhou2021co}. While these methods improve performance, they use a fixed regularization magnitude throughout learning and focus solely on \emph{how} to regularize.

%In this paper, we raise the equally important aspect of \emph{how much} to regularize and ask ourselves: \textit{Is Adaptation Necessary in Class-Incremental Learning?} We make the first attempt to treat the regularization magnitude as a latent variable that should be adjusted based on the current state of the learner, as well as the familiarity of the learning task, see Figure~\ref{fig:teaser}. We rely on Bayesian Optimization to predict the best regularization strength per-task, and show that even in this simplistic scenario, adapting to the current task yields drastic improvement, as will be shown via large-scale experiments on CIFAR-100~\citep{xxx}. 

%In this paper, for the first time, we address the crucial issue of \emph{how much} to regularize, and we ask: \textit{Is Adaptation Necessary in Class-Incremental Learning?} We treat the regularization magnitude as a latent variable that should be adjusted based on the current state of the learner and the complexity of the learning task (see Figure~\ref{fig:teaser}). To achieve this, we use Bayesian Optimization to predict the best regularization strength per task. Our experiments on CIFAR-100 demonstrate that even in this simple scenario, adapting to the current task results in significant improvement.

This paper addresses the issue of \emph{how much} to regularize in Class-Incremental Learning for the first time. We explore whether adaptation is necessary for optimal performance, treating the regularization magnitude as a latent variable that should be adjusted based on the current state of the learner and the complexity of the task (see Figure~\ref{fig:teaser}). We use Bayesian Optimization to predict the best regularization strength per task. Our experiments on CIFAR-100 and MiniImageNet demonstrate that adapting to the current task results in significant improvement, even in these simple scenarios.