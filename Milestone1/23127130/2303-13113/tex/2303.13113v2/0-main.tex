\documentclass[11pt]{article}

% This file will be kept up-to-date at the following GitHub repository:
%
% https://github.com/automl-conf/LatexTemplate
%
% Please file any issues/bug reports, etc. you may have at:
%
% https://github.com/automl-conf/LatexTemplate/issues

\usepackage{microtype} % microtypography
\usepackage{booktabs}  % tables
\usepackage{url}  % urls
\usepackage{graphicx}
\usepackage{float}

% AMS math
\usepackage{amsmath}
\usepackage{amsthm}

% With no package options, the submission will be anonymized, the supplemental
% material will be suppressed, and line numbers will be added to the manuscript.
%
% To hide the supplementary material (e.g., for the first submission deadline),
% use the [hidesupplement] option:
%
% \usepackage[hidesupplement]{automl}
%
% To compile a non-anonymized camera-ready version, add the [final] option (for
% the main track), or the [finalworkshop] option (for the workshop track), e.g.,
%
%\usepackage[final]{automl}
% \usepackage[finalworkshop]{automl}
%
% or
%
\usepackage[final, hidesupplement]{automl}
% \usepackage[finalworkshop, hidesupplement]{automl}

%\usepackage[hidesupplement]{automl}
\input{style}

% You may use any reference style as long as you are consistent throughout the
% document. As a default we suggest author--year citations; for bibtex and
% natbib you may use:

\usepackage{natbib}


%\bibliographystyle{apalike}

% and for biber and biblatex you may use:

% \usepackage[%
%   backend=biber,
%   style=authoryear-comp,
%   sortcites=true,
%   natbib=true,
%   giveninits=true,
%   maxcitenames=2,
%   doi=false,
%   url=true,
%   isbn=false,
%   dashed=false
% ]{biblatex}
% \addbibresource{...}

%\title{CARBON: Continual Adaptive Regularization with Bayesian Optimization}

%\title{Is Adaptation Necessary in Class-Incremental Learning?}
\title{Adaptive Regularization for Class-Incremental Learning}

% The syntax for adding an author is
%
% \author[i]{\nameemail{author name}{author email}}
%
% where i is an affiliation counter. Authors may have
% multiple affiliations; e.g.:
%
% \author[1,2]{\nameemail{Anonymous}{anonymous@example.com}}

\author[1]{\nameemail{Elif Ceren Gok Yildirim}{e.c.gok@tue.nl}}
\author[1]{\nameemail{Murat Onur Yildirim}{m.o.yildirim@tue.nl}}
\author[1]{\nameemail{Mert Kilickaya}{kilickayamert@gmail.com}}
\author[1]{\nameemail{Joaquin Vanschoren}{j.vanschoren@tue.nl}}

% the list might continue:
% \author[2,3]{\nameemail{Author 2}{email2@example.com}}
% \author[3]{\nameemail{Author 3}{email3@example.com}}
% \author[4]{\nameemail{Author 4}{email4@example.com}}

% if you need to force a linebreak in the author list, prepend an \author entry
% with \\:

% \author[3]{\\\nameemail{Author 5}{email5@example.com}}

% Specify corresponding affiliations after authors, referring to counter used in
% \author:

\affil[1]{Automated Machine Learning Group, Eindhoven University of Technology}

% the list might continue:
% \affil[2]{Institution 2}
% \affil[3]{Institution 3}
% \affil[4]{Institution 4}

% define PDF metadata, please fill in to aid in accessibility of the resulting PDF
\hypersetup{%
  pdfauthor={}, % will be reset to "Anonymous" unless the "final" package option is given
  pdftitle={},
  pdfsubject={},
  pdfkeywords={}
}

\begin{document}

\maketitle

\begin{abstract}
 %Continual learning is the idea of learning a model from data with changing distributions throughout time without forgetting what has already been learned. Regularization is one of the ways to prevent catastrophic forgetting by ensuring minimal changes in the weight updates during the learning process. However, current studies in this field assume that all tasks require the same amount of regularization. This assumption may not be necessarily true as different tasks may require different levels of regularization due to the shifts in distributions in continual learning scenarios. To address this issue, in this paper, we are proposing a Continual Adaptive Regularization with Bayesian Optimization (CARBON) for selecting the regularization hyperparameter in class incremental learning. We conduct large-scale experiments on two benchmark datasets, CIFAR-100 and mini-ImageNet, and demonstrate the effectiveness of our approach by outperforming vanilla regularization-based continual learners  through adapting the learner to distributional shifts. We showed that adapting the learners to the shifts is crucial for regularization-based continual learning.

%Class-Incremental Learning (CIL) aims at updating a learner using incoming stream of tasks from varying distributions. The goal is to expand a deep classifier with novel categories, while maintaining the performance on the previously seen classes. To maintain the performance on previous categories, hence minimize forgetting, a prominent method is to regularize the weights of the neural network and prevent abrupt shifts across learning tasks. While working well, existing regularizers make a strong assumption: The regularization magnitude should remain constant through all learning sessions. Such assumption is unrealistic, since how much to regularize should be a function of the current state of the learner and the learning task. To that end, in this paper, we take an alternative approach and ask ourselves: \emph{Is Adaptive Regularization Necessary in Class-Incremental Learning}? We propose a method to automatically determine the magnitude of regularization based on Bayesian Optimization. Our experiments on CIFAR-100 using two prominent regularizers reveal that indeed, adaptive regularization is crucial to build more accurate, less forgetful visual incremental learners. 

%Class-Incremental Learning (CIL) involves updating a deep classifier with new categories while maintaining previous class performance. Regularizing the weights of the neural network is a prominent method to minimize forgetfulness of previously seen categories while learning novel classes. However, existing regularizers assume a constant regularization magnitude through all learning sessions, which is unrealistic. How much to regularize should be a function of the familiarity of the incremental learner with the current learning task: Learning a highly unfamiliar set of categories should be treated differently than learning familiar categories. To that end, in this paper, we ask ourselves: \emph{Is Adaptive Regularization Necessary in Class-Incremental Learning}? We propose an alternative approach to automatically determine the regularization magnitude using Bayesian Optimization per-learning task. Our experiments on CIFAR-100 using two regularizers demonstrate the importance of adaptive regularization for more accurate and less forgetful visual incremental learning.

%Class-Incremental Learning (CIL) updates deep classifier with new categories while maintaining previous class performance. Neural network weight regularization is a prominent method to minimize forgetfulness of previously seen categories while learning novel classes. However, existing regularizers regularize constantly with the same magnitude throughout the learning which is not realistic. How much to regularize should be a function of the familiarity of the incremental learner with the current learning task: Learning a highly unfamiliar set of categories should be treated differently than learning familiar categories. To that end, in this paper, we ask ourselves: \emph{Is Adaptive Regularization Necessary in Class-Incremental Learning}? We propose an alternative approach to automatically determine the regularization magnitude using Bayesian Optimization per-learning task. Our experiments on CIFAR-100 using two regularizers demonstrate the importance of adaptive regularization for more accurate and less forgetful visual incremental learning.

%Class-Incremental Learning (CIL) is the process of updating a deep classifier with new categories while maintaining the performance of previously learned classes. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which is not realistic. The regularization amount should be based on the familiarity of the incremental learner with the current task, hence adaptive. This study aims to determine whether adaptive regularization is necessary in CIL. We propose a method to automatically determine the regularization magnitude using Bayesian Optimization for each learning task. Our experiments on CIFAR-100 dataset using two regularizers show the importance of adaptive regularization for accurate and less forgetful visual incremental learning.

Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.

\end{abstract}
\input{1-intro}
\input{2-relwork}
\input{3-method}
\input{4-setup}
\input{5-experiments}
\input{6-discussion}

% The 9 pages allocated for the main paper must include a broader impact
% statement regarding the approach, datasets and applications proposed/used in
% your paper. It should reflect on the environmental, ethical and societal
% implications of your work. The statement should require at most one page and
% must be included both at submission and camera-ready time.
%
% If authors have reflected on their work and determined that there are no
% likely negative broader impacts, they may use the following statement:
%
% After careful reflection, the authors have determined that this work presents
% no notable negative impacts to society or the environment.
%
% This section is included in the template as a default, but you can also place these
% discussions anywhere else in the main paper, e.g., in the introduction/future work.
%
% The Centre for the Governance of AI has written an excellent guide for writing
% good broader impact statements (for the NeurIPS conference) that may be a
% useful resource for AutoML-Conf authors:
%
% https://medium.com/@GovAI/a-guide-to-writing-the-neurips-impact-statement-4293b723f832

{\small
\bibliographystyle{apalike}
\bibliography{egbib_v2}
}

\section{Submission Checklist}
% The submission checklist is a combination of the NeurIPS '21 checklist:
%
%   https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist
%
% and the NAS checklist:
%
%   https://www.automl.org/wp-content/uploads/NAS/NAS_checklist.pdf
%
% For each question, change the default \answerTODO{} to either:
%
%     \answerYes{[justification]},
%     \answerNo{[justification]}, or
%     \answerNA{[justification]}.
%
% *You must include a brief justification to your answer,* either by
% referencing the appropriate section of your paper or providing a brief inline
% description.  For example:
%
% - Did you include the license of the code and datasets?
%   \answerYes{See Section~\ref{sec:code}.}
%
% - Did you include all the code for running experiments?
%   \answerNo{We include the code we wrote, but it depends on proprietary
%   libraries for executing on a compute cluster and as such will not be
%   runnable without modifications. We also include a runnable sequential
%   version of the code that we also report experiments in the paper with.}
%
% - Did you include the license of the datasets?
%   \answerNA{Our experiments were conducted on publicly available datasets and
%   we did not introduce new datasets.}
%
% Please note that if you answer a question with \answerNo{}, we expect that you
% compensate for it (e.g., if you cannot provide the full evaluation code, you
% should at least provide code for a minimal reproduction of the main insights
% of your paper).
%
% Please do not modify the questions and only use the provided macros for your
% answers. Note that this section does not count towards the page limit.

\begin{enumerate}
\item For all authors\dots
  %
  \begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately
    reflect the paper's contributions and scope?
    %
    \answerYes{}
    %
  \item Did you describe the limitations of your work?
    %
    \answerYes{See Section~\ref{sec:limit}}
    %
  \item Did you discuss any potential negative societal impacts of your work?
    %
    \answerNA{We do not have any potential negative societal impacts}
    %
  \item Have you read the ethics author's and review guidelines and ensured that
    your paper conforms to them? \url{https://automl.cc/ethics-accessibility/}
    %
    \answerYes{}
    %
  \end{enumerate}
  %
\item If you are including theoretical results\dots
  %
  \begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    %
    \answerNA{We are not presenting theoretical results}
    %
  \item Did you include complete proofs of all theoretical results?
    %
    \answerNA{We are not presenting theoretical results}
    %
  \end{enumerate}
  %
\item If you ran experiments\dots
  %
  \begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the
    main experimental results, including all requirements (e.g.,
    \texttt{requirements.txt} with explicit version), an instructive
    \texttt{README} with installation, and execution commands (either in the
    supplemental material or as a \textsc{url})?
    %
    \answerYes{We provided the README file on how to reproduce experiment results and we also included the requirements.txt file in Github repo.}
    %
  \item Did you include the raw results of running the given instructions on the
    given code and data?
    %
    \answerYes{}
    %
  \item Did you include scripts and commands that can be used to generate the
    figures and tables in your paper based on the raw results of the code, data,
    and instructions given?
    %
    \answerYes{We provided the README file on how to reproduce experiment result.}
    %
  \item Did you ensure sufficient code quality such that your code can be safely
    executed and the code is properly documented?
    %
    \answerYes{}
    %
  \item Did you specify all the training details (e.g., data splits,
    pre-processing, search spaces, fixed hyperparameter settings, and how they
    were chosen)?
    %
    \answerYes{}
    %
  \item Did you ensure that you compared different methods (including your own)
    exactly on the same benchmarks, including the same datasets, search space,
    code for training and hyperparameters for that code?
    %
    \answerYes{We made a comparison with exactly same datasets and hyperparameters.}
    %
  \item Did you run ablation studies to assess the impact of different
    components of your approach?
    %
    \answerNA{We did not run ablation study.}
    %
  \item Did you use the same evaluation protocol for the methods being compared?
    %
    \answerYes{Yes we use the same evaluation protocol while making performance comparison.}
    %
  \item Did you compare performance over time?
    %
    \answerYes{See Section~\ref{sec:empirical method}}
    %
  \item Did you perform multiple runs of your experiments and report random seeds?
    %
    \answerYes{See Section~\ref{sec:empirical method}}
    %
  \item Did you report error bars (e.g., with respect to the random seed after
    running experiments multiple times)?
    %
    \answerYes{See Section~\ref{sec:empirical method}}
    %
  \item Did you use tabular or surrogate benchmarks for in-depth evaluations?
    %
    \answerYes{We used surrogate benchmarks which are CIFAR100 and MiniImageNet for image classification. See Section~\ref{sec:setup} }
    %
  \item Did you include the total amount of compute and the type of resources
    used (e.g., type of \textsc{gpu}s, internal cluster, or cloud provider)?
    %
    \answerYes{See Section~\ref{sec:technical_details}}
    %
  \item Did you report how you tuned hyperparameters, and what time and
    resources this required (if they were not automatically tuned by your AutoML
    method, e.g. in a \textsc{nas} approach; and also hyperparameters of your
    own method)?
    %
    \answerYes{See Section~\ref{sec:empirical method} and Section~\ref{sec:technical_details}}
    %
  \end{enumerate}
  %
\item If you are using existing assets (e.g., code, data, models) or
  curating/releasing new assets\dots
  %
  \begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    %
    \answerYes{See Section~\ref{sec:empirical method}}
    %
  \item Did you mention the license of the assets?
    %
    \answerYes{See Section~\ref{sec:technical_details}. We have used open-source datasets and frameworks that are publicly available and can be used without any licensing issues or legal restrictions.}
    %
  \item Did you include any new assets either in the supplemental material or as
    a \textsc{url}?
    %
    \answerYes{We provide all our code anonymously via \url{https://anon-github.automl.cc/r/Adaptive_Regularization-71B7}.}
    %
  \item Did you discuss whether and how consent was obtained from people whose
    data you're using/curating?
    %
    \answerNA{}
    %
  \item Did you discuss whether the data you are using/curating contains
    personally identifiable information or offensive content?
    %
    \answerNA{}
    %
  \end{enumerate}
  %
\item If you used crowdsourcing or conducted research with human subjects\dots
  %
  \begin{enumerate}
  \item Did you include the full text of instructions given to participants and
    screenshots, if applicable?
    %
    \answerNA{}
    %
  \item Did you describe any potential participant risks, with links to
    Institutional Review Board (\textsc{irb}) approvals, if applicable?
    %
    \answerNA{}
    %
  \item Did you include the estimated hourly wage paid to participants and the
    total amount spent on participant compensation?
    %
    \answerNA{}
    %
  \end{enumerate}
\end{enumerate}

% content will be automatically hidden during submission
%\begin{acknowledgements}

%\end{acknowledgements}

% print bibliography -- for bibtex / natbib, use:

% \bibliography{...}

% and for biber / biblatex, use:

% \printbibliography

% supplemental material -- everything hereafter will be suppressed during
% submission time if the hidesupplement option is provided!
%\appendix

\section{Appendix}
\label{sec:technical_details}
We have built our experiments on PyCIL \citep{zhou2021pycil} (MIT License) and RayTune \citep{liaw2018tune} (Apache License 2.0) frameworks. Our experiments were run on NVIDIA A100  taking around 148 GPU hours. Total emissions are estimated to be around 15 kg CO2.

\end{document}
