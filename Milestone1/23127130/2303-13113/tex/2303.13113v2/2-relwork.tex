
\begin{figure}[t]
 
  \centering
  \includegraphics[width=0.8\textwidth]{figures/teaser.pdf}

 % \caption{A comparison of fixed \textit{vs} adaptive regularization (this work) in class-incremental learning. In class-incremental learning, the learner receives a sequence of tasks, such as dogs, cats or cars. When the new task arrives, the data that belongs to previous tasks are discarded, leading to forgetting. To prevent forgetfulness, researchers regularize neural network weights and prevent abrupt changes across tasks. While working well, they assume regularization should be of equal magnitude throughout learning sessions. In this work, we explore an alternative, and instead learn to predict (tune) the regularization magnitude by adapting to the current task.}
 \caption{A comparison of fixed \textit{vs.} adaptive regularization (ours). In this work, we explore the potential of tuning regularization per-learning task, allowing to learn adaptively.}
  \label{fig:teaser}
\end{figure}


\section{Related Work}

%In the realm of deep learning, traditional models are trained on all available data at once, whereas continual learning involves a stream of data with an evolving distribution. One particularly difficult scenario is the class incremental scenario, in which the model must learn to distinguish between an expanding number of objects or classes over time \citep{van2022three}. The main challenge in continual learning lies in striking a balance between stability and plasticity, that is, retaining previously learned knowledge while still being able to learn new tasks. This issue remains a significant obstacle, and many new techniques have been proposed to mitigate this problem. Despite ongoing efforts to address this challenge, there is still much to be done to overcome it fully.

%The proposed methods can be investigated further under three categories namely; replay-based, architecture-based, and regularization-based methods. Replay-based methods store real or generated samples of a learned task to revisit them during the learning process of the new task to alleviate forgetting \citep{rebuffi2017icarl, lopez2017gradient, bang2021rainbow}. Although replay-based methods are very strong methods to remember old tasks, storing the raw data and using it, especially for training rises a concern in terms of privacy and compute efficiency. Parameter-isolation/Architecture-based methods continually add new subnetworks which are responsible for each task specifically. These methods have started gaining more attention in recent years. However, the problem with these methods is extending the network by adding new subnetworks after each task is not feasible, efficient, and scalable \citep{mallya2018piggyback, li2019learn}. The regularization-based methods propose an extra regularization term to the loss function to consolidate past knowledge when learning a new task \citep{kirkpatrick2017overcoming, li2017learning, zenke2017continual}. While EWC and SI follow very similar working principles which simply regularize the network parameters, LwF distills the knowledge from the old model that is trained on the previous task to a new model in a way that prevents the new model to forget the old task. The drawback of these methods is introducing an extra hyperparameter to balance the stability-plasticity trade-off. These hyperparameters are mainly defined at the very beginning of the learning stage and kept fixed during the whole learning journey which highly affects the incremental performance of the model \citep{de2021continual}.


%%%%%%%%%%%%%%%%%%% Mert's Version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-4mm}
\partitle{Class-Incremental Learning.} Class-Incremental Learning updates a deep classifier with sequentially arriving data, usually with mutually exclusive categories~\citep{masana2020class,de2021continual,wang2023comp,zhou2023class,kilickaya2023towards}. However, when novel data arrives, previous training data becomes unavailable, leading to catastrophic forgetting. To mitigate this, researchers have developed two main approaches: replay methods, which store a subset of training data to rehearse during learning~\citep{lopez2017gradient,chaudhry2018efficient,aljundi2019gradient,ostapenko2019learning,xiang2019incremental}, and regularization-based approaches, which stabilize important parameters or distill previous knowledge into the model~\citep{kirkpatrick2017overcoming,chaudhry2018riemannian,zenke2017continual,lee2017overcoming,li2017learning,rebuffi2017icarl,wu2019large,zhou2021co}. 

However, assuming a constant amount of regularization throughout learning sessions is unnatural, since learning unfamiliar objects requires more plasticity than learning familiar ones. To address this issue, we propose a regularization method in which the regularization magnitude is a function of time and is automatically tuned using Bayesian Optimization~\citep{turner2021bayesian}. We evaluate the generality of our approach by choosing EWC~\citep{kirkpatrick2017overcoming} as the prior-based regularizer and LwF~\citep{li2017learning} as the distillation-based regularizer.


%Class-Incremental Learning involves updating a deep classifier with sequentially arriving data, typically with mutually exclusive categories~\citep{masana2020class,de2021continual,wang2023comp,zhou2023class,kilickaya2023towards}. However, when novel data arrives, previous training data becomes unavailable and cannot be used for further optimization, resulting in catastrophic forgetting. Researchers have developed two main approaches to mitigate this problem: replay and regularization methods. Replay methods store a subset of training data to rehearse during the learning of novel classes~\citep{lopez2017gradient,chaudhry2018efficient,aljundi2019gradient,ostapenko2019learning,xiang2019incremental}. In contrast, regularization-based approaches use prior-based or distillation-based regularization to stabilize important parameters or distill previous knowledge into the current model~\citep{kirkpatrick2017overcoming,chaudhry2018riemannian,zenke2017continual,lee2017overcoming,li2017learning,rebuffi2017icarl,wu2019large,zhou2021co}. Regardless of the technique, the amount of regularization is constant throughout learning sessions. However, a constant amount of regularization throughout the learning sessions is an unnatural assumption, since learning an unfamiliar object requires more plasticity than learning a familiar one. To address this issue, we propose a regularization method in which the regularization magnitude is a function of time and is automatically tuned using Bayesian Optimization~\citep{turner2021bayesian}. We choose EWC~\citep{kirkpatrick2017overcoming} as prior-based baseline LwF~\citep{li2017learning} as the distillation-based baseline to show the generality of our approach. 


%Class-Incremental Learning deals with updating a deep classifier with sequentially arriving data, often with the mutually exclusive categories~\citep{masana2020class,de2021continual,wang2023comp,zhou2023class,kilickaya2023towards}. Once the novel data arrives, previous training data is no longer available, therefore can't be used for further optimization, leading to catastrophic forgetting. To mitigate this, researchers either replay or regularize. Replay methods store a subset of training data to rehearse during learning of novel classes~\citep{lopez2017gradient,chaudhry2018efficient,aljundi2019gradient,ostapenko2019learning,xiang2019incremental}. In this work, we follow regularization-based approach. Prior-based regularization selects important parameters to stabilize their weights during novel task optimization~\citep{kirkpatrick2017overcoming,chaudhry2018riemannian,zenke2017continual,lee2017overcoming}. Distillation-based regularization instead distills previous knowledge into the current model in a teacher-student knowledge distillation scheme~\citep{li2017learning,rebuffi2017icarl,wu2019large,zhou2021co}. Regardless of the technique, the amount of regularization is constant throughout learning sessions. Such assumption is unnatural: Learning a rather unfamiliar object requires more plasticity in comparison to learning a familiar object. To that end, in this paper, we turn regularization magnitude a function of the time, and automatically tune it via Bayesian Optimization~\citep{turner2021bayesian}. 

\vspace{-3mm}

\partitle{Hyper-Parameter Optimization.} Hyper-Parameter Optimization (HPO) aims to optimize the hyper-parameters of a given deep learning model, including the learning rate, layer size, or balance of different loss functions. In this paper, our focus is on balancing the contribution of standard classification and the regularization loss. To tackle the HPO problem, complex techniques such as bi-level optimization~\citep{franceschi2018bilevel} or gradient-based optimization~\citep{baydin2018automatic} have been proposed. Bi-level optimizers alternate between optimizing neural network weights and tuning the hyper-parameters, while gradient-based methods treat the entire network weights as a hyper-parameter to be updated. 

However, in this work, we propose to use Bayesian Optimization~\citep{turner2021bayesian} due to its simplicity and effectiveness.
In summary, this paper makes the following contributions: 
%Hyper-Parameter Optimization (HPO) optimizes the set of hyper-parameters of the underlying learner. In case of deep learning, most important parameters include learning rate, layer size, or balancing different loss functions. In this paper, our goal is to balance the contribution of standard classification and the regularization loss. To tackle HPO, complex techniques include bi-level optimization~\citep{franceschi2018bilevel} or gradient-based optimization~\citep{baydin2018automatic}. Bi-level optimizers alternate between optimizing neural network weights and then tune the hyper-parameters. Gradient-based methods treat the whole network weights as a hyper-parameter to be updated. In this paper, we resort to Bayesian Optimization~\citep{turner2021bayesian}, thanks to its simplicity and effectivity. 


%To tackle HPO, advanced techniques include bi-level  ~\citep{xxx} where the learner alternates between training the learner weights and then tuning the hyper-parameters. Other methods include gradient-based optimization~\citep{xxx}


\begin{enumerate}[label=\Roman*.]

\item In this paper, for the first time, we raise the important issue of adaptive regularization in class-incremental learning. 
\item We propose to predict the regularization magnitude conditioned on the state of the deep learner and the current learning task via Bayesian Optimization. 
\item Through large-scale experiments on well-established benchmarks, we show that learning adaptively yields significant performance improvements, in terms of increasing accuracy while reducing forgetting. 
%\item Through experiments on CIFAR-100 and MiniImageNet, we show that adaptivity is consistently superior to fixed regularization, leading to $13\%$ top-1accuracy improvement with EWC~\citep{kirkpatrick2017overcoming} and $10\%$ improvement with LWF~\citep{li2017learning}.

\end{enumerate}
