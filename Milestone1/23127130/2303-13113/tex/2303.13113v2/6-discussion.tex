
%Continual learning involves training a model on multiple tasks over time. Each task may have different data distributions and require different levels of regularization. An adaptive approach allows the model to adjust the regularization hyperparameter for each task, rather than using a fixed value that may not be optimal for all tasks.
%When a machine learning model is trained on multiple tasks over time, some parameters may become more important for certain tasks than others. To prevent forgetting, previously learned information while adapting to new tasks, methods like Elastic Weight Consolidation (EWC) and Learning without Forgetting (LWF) use a regularization term to penalize changes to important parameters learned from earlier tasks. However, the strength of this penalty should be adjusted based on the importance of the parameters for each new task, which can be done automatically as the model continues to learn.

%In this paper we proposed an adaptive approach CARBON that can help to prevent catastrophic forgetting by adjusting the regularization hyperparameter adaptively. Hence, the model can balance the need to learn new information with the need to maintain performance on previously learned tasks. We showed that CARBON outperforms the vanilla regularization-based strategies. We define three drawbacks of our approach which are keeping a small part of previous task samples to optimize our objective function, sacrificing learning the new task while keeping the old information due to the nature of regularization-based methods, and optimizing based on the same search space for all tasks regardless of their similarity.

%Overall, an adaptive approach to regularization in continual learning is essential for achieving good performance on multiple tasks over time, preventing catastrophic forgetting, and avoiding the need for manual tuning of the hyperparameter. We believe that we proved that adaptive regularization strength is a need in continual learning and as a future research direction, more clever strategies can be developed based on task similarity to adjust the defined search space for a faster and more efficient optimization process.  



%%%%%%%%%%%%%%%%%%% Mert's Version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\partitle{Summary of approach and findings.}

%%%%%%%% V1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\partitle{Summary.} In this work, we tackled class-incremental learning, where the learning tasks arrive in sequence as a set of mutually exclusive categories. We augmented two prominent regularization approach, LwF~\citep{li2017learning} and EWC~\citep{kirkpatrick2017overcoming} with the ability to adjust regularization weight per-learning task. We auto-tune the regularization weight with the help of off-the-shelf, black-box Bayesian Optimization technique of multivariate tree parzen estimator. 

%Through experiments on CIFAR-100, we showed that with this very simple approach, regardless of the technique, tuning regularization per-learning task is crucial, as it yields significant improvements in accuracy and forgetfulness. This indicates the constant regularization assumption is a core limitation of the regularization-based approaches. 

%\partitle{Limitations.} One important limitation of our work is the assumption to access a subset of the validation set from previous classes. This resembles replay-based techniques, where the learner stores a subset of the training data from the previous categories~\citep{lopez2017gradient,chaudhry2018efficient,aljundi2019gradient,ostapenko2019learning,xiang2019incremental}. However, we highlight that our goal here is not to designate a novel, State-of-the-Art incremental learning technique, but rather to show the viability of learning to regularize for regularization-based incremental learning. Also, stored exemplars are not used for any other purposes than adaptive regularization. 

%Another important limitation is the use of only two algorithms to showcase adaptive regularization. In the future work, we will explore SI~\citep{zenke2017continual}, RWalk~\citep{chaudhry2018riemannian}, and IMM~\citep{lee2017overcoming} to see to what extent our empirical evaluation extends beyond EWC and LwF. 

%\partitle{Statement on Our Submission.} Finally, we state that our paper is shorter than a typical AutoML submission. We have a very simple proposal, and our point can be made in less than 7 pages. Additionally, we mainly target the Workshop track, and we submit herein to collect constructive feedback from valuable AutoML reviewers. 

%\partitle{Future Work.}

%%%%%%%% V2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\section{Limitations}

%We identify three limitations with our work: 

%\begin{itemize}
%    \item \textbf{Storage: } Firstly, our work requires a subset of the validation set from previous classes. This is similar to replay-based techniques, but our objective is not to propose a new incremental learning technique. Instead, we aim to demonstrate the feasibility of learning to regularize for regularization-based incremental learning. Stored exemplars are only used for tuning $\lambda$.
    
 %   \item \textbf{Efficiency: } Secondly, our method needs to perform auto-tuning per-incremental learning step to identify the best regularization magnitude. This procedure scales linearly with the number of tasks, thus adding a small but non-negligible cost for the training efficiency. \mert{For example, a typical experiment with vanilla EWC takes XXX GPU hours whereas our approach takes XXX GPU hours}.  
    
  %  \item \textbf{Baselines:} In this paper, we experiment with two complimentary baselines, EWC and LwF to test the generality of the approach for prior-based and distillation-based techniques. A natural extension would be to experiment with more techniques to see how adapting to the current task affects the performance. Luckily, our proposal is very simple and method agnostic, thus can be easily applied to other baselines with no algorithmic modification. 
%\end{itemize}

%%%%%%%% V3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations and Broader Impact}
\label{sec:limit}
Our work has three limitations that we want to acknowledge:

\begin{itemize}
\item \textbf{Storage: } Our approach requires a subset of the validation set from previous classes. Although this is similar to replay-based techniques, our objective is not to introduce a new incremental learning technique. Instead, we demonstrate the feasibility of learning to regularize for regularization-based incremental learning. Stored exemplars are only used for tuning the regularization coefficient $\lambda$.

\item \textbf{Efficiency: } Our method requires auto-tuning at each incremental learning step to identify the optimal regularization magnitude. This process scales linearly with the number of tasks, which adds a minor but not insignificant training cost. In a broader context, as we move towards longer, larger-scale incremental learning scenarios, our method is expected to improve the learning efficiency by automating the selection of $\lambda$, as typically researchers perform multifold cross validation to find optimal parameters per-task via grid search, eventually reducing the required carbon footprint. 

\item \textbf{Baselines:} In this paper, we experimented with two complementary baselines, EWC and LwF, to assess the generality of our approach for prior-based and distillation-based techniques. Extending this work would involve experimenting with additional techniques to see how adapting to the current task affects performance. Fortunately, our proposal is straightforward and method-agnostic, so it can be easily applied to other baselines with no algorithmic modification.

\end{itemize}



%%%%%%%% End of V3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

This work addresses class-incremental learning, where learning tasks arrive as a sequence of mutually exclusive categories. We propose a proof-of-concept study to test whether adapting to the current task is necessary in class-incremental learning. We enhance two popular regularization approaches, LwF~\citep{li2017learning} and EWC~\citep{kirkpatrick2017overcoming}, by enabling the adjustment of regularization weight per-learning task. We auto-tuned the regularization weight using the black-box Bayesian Optimization technique of multivariate tree parzen estimator. 

Through experiments on CIFAR-100 and MiniImageNet, we demonstrate that tuning regularization per-learning task is critical and leads to significant improvements in accuracy and transfer, regardless of the technique used. This suggests that the constant regularization assumption is a fundamental limitation of regularization-based approaches. We hope our work will inspire future research for further auto-tuning the otherwise hand-crafted parameters in incremental learning. 

%We hope our work will inspire future research into making incremental learners to automatically adapt to their learning tasks. 

%Finally, we only use two algorithms to showcase adaptive regularization. In future work, we plan to explore other approaches, such as SI~\citep{zenke2017continual}, RWalk~\citep{chaudhry2018riemannian}, and IMM~\citep{lee2017overcoming}, to further evaluate our empirical findings beyond EWC and LwF.



%%%%%%%% V3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\begin{tcolorbox}[width=\textwidth]
%\textbf{Statement on Our Submission.} Our paper is shorter than a typical AutoML submission, as we have a straightforward proposal that can be communicated under seven pages. We primarily target the Workshop track and seek feedback from esteemed AutoML reviewers.
%\end{tcolorbox}