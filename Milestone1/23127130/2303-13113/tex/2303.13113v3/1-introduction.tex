\section{Introduction}
This paper focuses on Class-Incremental Learning of deep neural network representations~\citep{masana2020class,defying}. Unlike standard batch learning, which requires access to data from all categories simultaneously, Class-Incremental Learning can update a pre-trained deep classifier with new categories by expanding the classifier layer with new output nodes for new classes. This leads to more efficient learning and avoids the need to store task identities which often are not available in real-world scenarios.
\vspace{1pt}

While Class-Incremental Learning enables expanding a classifier without requiring task identities, it often results in \emph{catastrophic forgetting}. This occurs when the deep learner sacrifices accuracy on previously seen classes to learn new ones. Three major approaches have been explored to address this issue: regularization, replay and architecture adaptation. Regularization prevents abrupt shifts in the neural network weights while learning new classes~\citep{kirkpatrick2017overcoming,li2017learning}. Replay stores a few exemplars per class in memory and replays them during new learning increments~\citep{lopez2017gradient}. Architecture-based approaches build network structures by either expanding the existing network \citep{pnn, der} or by partially isolating network parameters to retain past class information \citep{aanets, wsn, cps}. Although these methods improve the performance, they always use a fixed learning rate, regularization magnitude, and pre-defined memory size throughout the learning process, which is likely suboptimal.




 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/teaser_pic.pdf}
  \caption{Comparison of fixed \textit{vs.} adaptive continual learning (AdaCL). In this work, we hypothesize that different tasks may require different settings and explore the potential of tuning learning rate ($\eta$), regularization strength ($\lambda$) and memory size per task ($m$), allowing to learn adaptively.}
  \label{fig:teaser}
\vspace{-15pt}
\end{figure}


This paper addresses the issue of dynamically adjusting \emph{how much} to regularize or store in memory for each new task. We explore whether adaptation is necessary for optimal performance, treating the learning rate, regularization magnitude, and memory size as latent variables that should be adjusted based on the current state of the learner and the complexity of the task (see Figure~\ref{fig:teaser}). We use Bayesian Optimization to efficiently discover the best hyperparameters per task. Our experiments on CIFAR-100 and MiniImageNet demonstrate that adapting these parameters to the tasks results in significant improvements and give us new insight into how to adapt to various new tasks. In summary, this paper makes the following contributions: 

%\vspace{-5pt}
\begin{enumerate}[label=\Roman*.]
\item In this paper, for the first time, we raise the important issue of adaptive hyperparameter selection in class-incremental learning.\vspace{5pt}
\item We propose to predict the learning rate, regularization magnitude, and memory size conditioned on the state of the deep learner and the current learning task via Bayesian Optimization.\vspace{5pt}
\item Through large-scale experiments on well-established benchmarks, we show that learning adaptively yields significant performance and efficiency improvements, both increasing accuracy and reducing forgetting. 
\end{enumerate} 
%Son cumleyi degistirelim experimentler henuz runlanmamis oldugu icin

