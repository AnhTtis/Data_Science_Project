\section{Experimental Protocol}
\label{sec:setup}
%\vspace{-5pt}

In this section, we describe our experimental setup, present our findings and results, and provide an ablation study.
\textbf{Datasets.} In this paper, we experiment with \textbf{CIFAR100}~\citep{krizhevsky2009learning} and \textbf{MiniImageNet}~\citep{vinyals}. Each dataset exhibits objects from $100$ different categories. We train all the models with $10$ tasks, with $10$ classes within each learning task on both CIFAR100 and MiniImageNet. Both datasets have 5000 training, and 1000 testing color images per learning task, each with $32\times32$ and $64\times64$ resolution for CIFAR100 and MiniImageNet respectively.

\paragraph{Metrics.}  We resort to the standard metrics for evaluation, accuracy (ACC) which measures the final accuracy averaged over all tasks,  and backward transfer (BWT) which measures the average accuracy change of each task after learning new tasks:

\vspace{-18pt}
{\footnotesize
\begin{align}
ACC=\frac{1}{T}\sum\nolimits_{i=1}^T A_{T,i}
\end{align}}

\vspace{-15pt}
{\footnotesize
\begin{align}
\label{equ:main}
BWT=\frac{1}{T-1}\sum\nolimits_{i=1}^{T-1} (A_{T,i}-A_{i,i})
\end{align}}

\noindent where $A_{T,i}$ represents the testing accuracy of task $T$ after learning task $i$.

\paragraph{Baselines.} EWC, LwF, iCARL, and WA are our direct baselines since we use them as base models in AdaCL. We also compare our common baseline results with OMDP \citep{omdp}. Finally, we select one recent memory-free approach FeTrIL~\citep{fetril}, and one recent memory-based method PODNet~\citep{podnet} to provide more comprehensive insights.

\paragraph{Implementation Details.} 
We employ adaptive hyperparameter optimization on the methods discussed in section 3.2, and compare them with their fixed (original) versions. For the fixed versions, we use the default {$\eta$,} $\lambda$ and $m$ as defined in PYCIL \citep{zhou2021pycil}.
% The optimizer, validation size, and search space were determined based on the ablation experiments on CIFAR 100, and results are given in the Appendix in Figures ~\ref{fig:validationfig}, \ref{fig:searchspacefig}, \ref{fig:optimizerfig} and Tables~\ref{table:validationsize}, \ref{table:searchspace}, \ref{table:optimizer}.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.44\textwidth]{figures/all_cifar100.pdf}
  \caption{Accuracy after each task on \textbf{CIFAR100}. AdaCL significantly boosts the performance on regularization-based methods and improves the efficiency by storing fewer exemplars on memory-based methods while yielding on par performance.}
  \label{fig:all_cifar100}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.44\textwidth]{figures/all_mini.pdf}
  \caption{Accuracy after each task on \textbf{MiniImageNet}. The results align with the observations on CIFAR100.}
  \label{fig:all_mini}
\end{figure}


We use ResNet-$32$ as the backbone~\citep{he2016deep}. We set the number of epochs to $100$ but use the Successive Halving~\citep{li2018massively} scheduler for a more efficient search. We use SGD optimizer with momentum parameter set to $0.9$ and weight decay $5e^{-4}$ for the first task and $2e^{-4}$ for the rest of the tasks. The batch size is set to $128$. We run experiments on three different seeds and report their average. We store a small subset of the validation data from each incremental learning step to evaluate the search algorithm. 
The search space for the learning rate and the maximum memory size per class within a task is set to [0.05, 0.1] and 50 respectively. The search space for $\lambda$ is determined based on the ablation experiments and details are given in Appendix \ref{searchspace_ablate}. 


\section{Experimental Results}
\paragraph{The Effect of Adaptivity.}
We investigate the efficacy of our adaptive method compared to traditional fixed hyperparameter approaches across the CIFAR100 and MiniImageNet datasets. Our results highlight significant advancements of our adaptive approach AdaCL, particularly notable in regularization-centric techniques like EWC and LwF as illustrated in Figure~\ref{fig:all_cifar100} and Figure~\ref{fig:all_mini}. 


\begin{table*}[h]
\caption{Performance comparison of various methods on the CIFAR100 and MiniImageNet datasets in terms of ACC, BWT, and memory size. Baseline methods such as EWC and LwF do not utilize memory. Our proposed methods, denoted with (ours), demonstrate better or competitive performance across both datasets while using less memory.}
\label{tab:results}
\fontsize{5}{6.5}\selectfont
\resizebox{\textwidth}{!}{%
\begin{tabular}{lrrcrrc}
\hline
        & \multicolumn{3}{c}{\textbf{CIFAR100}}         & \multicolumn{3}{c}{\textbf{MiniImageNet}}     \\ \hline
Method & \multicolumn{1}{c}{ACC (\%)} & \multicolumn{1}{c}{BWT (\%)} & Memory Size & \multicolumn{1}{c}{ACC (\%)} & \multicolumn{1}{c}{BWT (\%)} & Memory Size \\ \hline
PODNet    & 39.47 ± 1.39 & -24.14 ± 5.49  & 4500 & 43.49 ± 0.31  & -10.84 ± 10.83 & 4500 \\
OMDP      & 46.94 ± 2.11   &  -28.34 ± 1.28             & 4500 &    46.15 ± 0.55           &  -25.54 ± 4.18              & 4500 \\
FeTrIL    & 27.56 ± 1.50 & -18.92 ± 5.70  & -    & 24.46 ± 1.60  & -15.84 ± 0.73  & -    \\ \hline
EWC       & 15.26 ± 1.37 & -63.96 ± 3.21  & -    & 13.30 ± 0.38  & -60.64 ± 3.11  & -    \\
\textbf{Ada-EWC} \textbf{(ours)}   & 21.06 ± 1.37 & -13.28 ± 3.09  & -    & 20.31 ± 0.39  & -11.86 ± 2.32  & -    \\
LwF       & 21.74 ± 0.73 & -48.88 ± 12.43 & -    & 20.97 ± 0.19  & -50.06 ± 9.59  & -    \\
\textbf{Ada-LwF} \textbf{(ours)}  & 29.41 ± 0.65  & -22.34 ± 4.18  & -    & 30.33 ± 1.65  & -28.71 ± 6.44  & -    \\ \hline
iCaRL     & 46.13 ± 1.35 & -28.84 ± 5.06  & 4500 & 45.83  ± 1.43 & -27.33 ± 5.54  & 4500 \\ 
\textbf{Ada-iCaRL} \textbf{(ours)} & 46.44 ± 2.50 & -28.62 ± 2.85  & 4125 & 46.10 ± 2.26  & -28.77 ± 4.26  & 3950 \\
WA        & 50.84 ± 2.37 & -17.23 ± 1.51  & 4500 & 51.96 ± 0.74  & -22.46 ± 1.67  & 4500 \\
\textbf{Ada-WA} \textbf{(ours)}    & 50.87 ± 3.19 & -20.49 ± 2.88  & 4085 & 51.85 ± 1.12  & -24.22 ± 4.41  & 4050 \\ \hline
\end{tabular}%
}
\vskip -0.1in
\end{table*}

\begin{figure*}[h]
  \centering
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/wa_reg.pdf}
    \caption{WA regularization strength}
    \label{fig:wa_reg}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/wa_lr.pdf}
    \caption{WA learning rate}
    \label{fig:wa_lr}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/wa_mem.pdf}
    \caption{WA memory size}
    \label{fig:wa_mem}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/icarl_reg.pdf}
    \caption{iCaRL regularization strength}
    \label{fig:icarl_reg}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/icarl_lr.pdf}
    \caption{iCaRL learning rate}
    \label{fig:icarl_reg}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/icarl_mem.pdf}
    \caption{iCaRL memory size}
    \label{fig:icarl_lr}
  \end{subfigure}
  \caption{Adaptive modifications in regularization strength, learning rate, and memory allocation. The selected hyperparameters diversely change across task sequences, datasets, and methods and indicate the necessity of adaptivity in CL.}
  \label{fig:wa_hp}
\vskip -0.1in
\end{figure*}

For example, we find $8\%$ and $10\%$ increase in accuracy while $26\%$ and  $21\%$ improvement in backward transfer on CIFAR100 and MiniImagenet respectively with LwF by adjusting the regularization strength and learning rate. 

In memory-centric methods, we find that they show greater resilience to the changes in hyperparameters. Storing sufficient exemplars aids the model in capturing the distribution of different tasks simultaneously, thereby reducing the reliance on hyperparameter optimization. Despite minor differences, we consistently observe similar accuracy and backward transfer, as seen in Table \ref{tab:results}. 

\paragraph{Comparison with Recent Baselines.} We include results from recent baselines PODNet and FeTrIL to provide comprehensive insights. An intriguing finding is that the initial performance of fixed LwF is outperformed by the recent FeTrIL method, but when we tune LwF, it outperforms FeTrIL by $2\%$ and $6\%$ on CIFAR100 and MiniImageNet, respectively. 
Furthermore, we compare AdaCL with another HPO-based method OMDP, on iCaRL, our only common baseline. Our performance closely aligns with OMDP but a key advantage of our adaptive approach is that it does so while using less memory. 

\vspace{-1pt}
\paragraph{Memory Allocation.}
We investigate the memory allocation of iCaRL and WA by specifically tuning the memory size for these methods. We observe that in our adaptive approach we were able to attain similar results while utilizing less memory compared to those obtained from fixed versions as given in Table \ref{tab:results}.
This is due to AdaCL capability to choose exemplars from both decision boundaries and the center (Figure \ref{fig:tsne}), highlighting how the adaptive approach can achieve comparable results.

\begin{figure*}[h]  
\centering
  \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task1.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task2.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task3.pdf}
  \end{subfigure}
   \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task4.pdf}
  \end{subfigure}
   \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task5.pdf}
  \end{subfigure}
   \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task6.pdf}
  \end{subfigure}
   \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task7.pdf}
  \end{subfigure}
   \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task8.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{0.27\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar_tsne_task9.pdf}
  \end{subfigure}
  \caption{t-SNE plots of selected exemplars. Ada-WA selects exemplars from boundaries and center. This way, it is able to achieve on-par performance with less memory. The final task is omitted from the visualization, since memory selection is not necessary for it.}
  \label{fig:tsne}
  \vskip -0.1cm
\end{figure*}

\paragraph{Exploring Hyperparameter Dynamics.}
We observe the selected hyperparameters throughout the process of continual learning and reveal intriguing dynamics in the adjustment of regularization strength, learning rate, and memory size across tasks. This observation highlights the crucial role of adaptability in continual learners, allowing them to adapt dynamically to the changing demands of each task. 

For example, Figure \ref{fig:wa_hp} presents the chosen hyperparameters for WA and iCaRL, illustrating the subtle adjustments made throughout the learning. Please refer to Appendix \ref{hpo_dynamics} for other methods.

\vspace{-3pt}
\subsection{Ablation Study}
In Table 3, we provide a comprehensive analysis of how different hyperparameters interact with model performance. Our findings reveal that the performance of the model is intricately linked to the interplay of various hyperparameters. While optimizing solely the learning rate and regularization strength yields the highest accuracy, we also incorporate with memory size to enhance memory allocation efficiency.

Our comprehensive hyperparameter optimization strategy showcases an enhancement in memory efficiency by minimizing the amount of stored exemplars while maintaining on-par accuracy. This insight underscores the importance of not only optimizing individual hyperparameters but also understanding their collective impact on model performance, particularly in scenarios where resource constraints necessitate efficient memory allocation in practical applications.

\vspace{3pt}
\begin{table}[h]
\caption{The findings of our ablation study on the WA method, where different hyperparameters were tuned simultaneously and individually on CIFAR100 dataset. Tuning all hyperparameters simultaneously results in a negligible decrease in ACC but yields improvements in memory.}
\fontsize{12}{18}\selectfont
\label{tab:wa_ablate}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc}
\hline
\textbf{Regularization} &\textbf{ Learning} & \textbf{Memory} & \textbf{ACC}  & \textbf{Stored Memory} \\
\textbf{Strength}       & \textbf{Rate}     & \textbf{Size}   & \textbf{(\%)}       & \textbf{Size}          \\ \hline
    -           &      -    &    -    & 53.48    & 4500          \\
\checkmark              &    -      &     -   & 54.00    & 4500          \\
     -          & \checkmark         &    -    & 53.65    & 4500          \\
      -         &      -    & \checkmark       & 51.02    & 3350          \\
\checkmark               & \checkmark         &     -   & 54.24    & 4500          \\
\checkmark               &    -      & \checkmark       & 51.73    & 3350          \\
     -          & \checkmark         & \checkmark       & 51.40    & 3750          \\
\checkmark               & \checkmark         & \checkmark       & 53.16    & 4250          \\ \hline
\end{tabular}%
}
\end{table}



Finally, in Table \ref{tab:random_memory}, we investigate the impact of employing random memory selection as an alternative to herding under the WA method. Although there is a slight tendency to decrease in both incremental accuracy and forgetting, we found that the adoption of random memory selection leads to an insignificant change in accuracy and forgetting.


\begin{table}[h]
\caption{An investigation into the WA method when using random and herding as the memory selection strategies on CIFAR100 dataset. Findings indicate that random memory selection produces similar results to herding.}
\fontsize{5}{10}\selectfont
\label{tab:random_memory}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llcc}
\hline
\textbf{Method}        & \textbf{selector} &\textbf{ ACC (\%) }    & \textbf{BWT (\%)}      \\ \hline
WA            & herding & 50.84 ± 2.37 & -17.23 ± 1.51 \\
Ada-WA        & herding & 50.87 ± 3.19 & -20.49 ± 2.88 \\
WA            & random  & 49.95 ± 2.72 & -17.59 ± 2.22 \\
Ada-WA        & random  & 49.96 ± 2.55 & -21.51 ± 1.28 \\ \hline
\end{tabular}%
}
\end{table}


%We implement all the methods in PyTorch~\cite{NEURIPS2019_9015}. We use ResNet-$32$ as the backbone~\cite{he2016deep}. We rely on the PyCIL library~\cite{zhou2021pycil} for the regularization objectives EWC~\cite{kirkpatrick2017overcoming} and LwF~\cite{li2017learning}, and use all the hyperparameters as is without further tuning. We set the number of epochs to $100$ for each configuration but used the Asynchronous Successive Halving~\cite{li2018massively} scheduler for a more efficient search. We set an initial learning rate of $0.1$ which is decayed by factor $0.1$ at epoch $60$ for the first and all the remaining tasks. We use SGD optimizer with momentum parameter set to 0.9 and weight decay set to $5e^4$ for the first task and $2e^4$ for the rest of the tasks. The batch size is set to $128$. We run experiments on three different seeds and report their average. 

%\textcolor{blue}{\textbf{Comparison within memory-free methods.} 
%We chose two complementary and fundamental memory-free baselines namely EWC and LwF. We make these methods adaptive by tuning the important regularization term. We compare the adaptive EWC and LwF with the recent memory-free work FeTrIL~\citep{fetril} to provide detailed insight.}

%\textcolor{blue}{\textbf{Comparison within memory-based methods.} 
%We selected two pivotal memory-based methods, namely iCARL and WA. Our approach involves making these methods adaptive by optimizing the important regularization term and memory size. To offer comprehensive insights, we conduct a thorough comparison between the adaptive iCARL and WA methods and the recent memory-based techniques PODNet~\citep{podnet} and RMM~\citep{rmm}.}


