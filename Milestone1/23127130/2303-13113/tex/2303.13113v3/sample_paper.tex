\documentclass[twoside]{article}


\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{arydshln}
\usepackage{amssymb}
\usepackage{url}
% If your paper is accepted, change the options for the package
% claiunconf2023 as follows:
%
\usepackage[accepted]{claiunconf2023}

%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
\bibliographystyle{apalike}


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\CLAIUnconftitle{AdaCL: Adaptive Continual Learning}

\CLAIUnconfauthor{ Elif Ceren Gok Yildirim \And Murat Onur Yildirim \And  Mert Kilickaya \And Joaquin Vanschoren }

\CLAIUnconfaddress{ Automated Machine Learning Group, Eindhoven University of Technology } ]

\begin{abstract}
Class-Incremental Learning aims to update a deep classifier to learn new categories while maintaining or improving its accuracy on previously observed classes. Common methods to prevent forgetting previously learned classes include regularizing the neural network updates and storing exemplars in memory, which come with hyperparameters such as the learning rate, regularization strength, or the number of exemplars. However, these hyperparameters are usually only tuned at the start and then kept fixed throughout the learning sessions, ignoring the fact that newly encountered tasks may have varying levels of novelty or difficulty. This study investigates the necessity of hyperparameter `adaptivity' in Class-Incremental  Learning: the ability to dynamically adjust hyperparameters such as the learning rate, regularization strength, and memory size according to the properties of the new task at hand. We propose AdaCL, a Bayesian Optimization-based approach to automatically and efficiently determine the optimal values for those parameters with each learning task. We show that 
 adapting hyperpararmeters on each new task leads to improvement in accuracy, forgetting and memory. Code is available at \url{https://github.com/ElifCerenGokYildirim/AdaCL}.

\end{abstract}

\input{1-introduction}
\input{2-relatedwork}
\input{3-method}
\input{4-setup}
\input{5-conclusion}


%\bibliographystyle{apalike}
\bibliography{biblo}
%\section{MISSING PROOFS}

%The supplementary materials may contain detailed proofs of the results that are missing in the main paper.

%\subsection{Proof of Lemma 3}

%\textit{In this section, we present the detailed proof of Lemma 3 and then [ ... ]}

%\section{ADDITIONAL EXPERIMENTS}

%If you have additional experimental results, you may include them in the supplementary materials.

%\subsection{The Effect of Regularization Parameter}

%\textit{Our algorithm depends on the regularization parameter $\lambda$. Figure 1 below illustrates the effect of this parameter on the performance of our algorithm. As we can see, [ ... ]}

%\section{NON-TEXTUAL SUPPLEMENTARY MATERIAL}

%The (optional) non-textual supplementary material (e.g. code for the paper) should be submitted as a separate ZIP file.

%\begin{itemize}
%\item Your supplementary material file should be named 642-supp.zip (with 642
%  replaced with your paper ID on CMT).
%\item If you wish to include any code/datasets as part of the supplementary
%  material, you can: (i) include it in the ZIP file of your supplementary
%  material, and/or (ii) provide the public URL where the code can be found in
%  the appropriate field of CMT submission form.  If you promised to release
%  code/datasets (either in the original submission PDF or during the rebuttal),
%  the code/dataset must be released. In this case, the ContinualAI Unconference Chairs will
%  check the availability of the code/dataset and, if it isnâ€™t available by the
%  camera-ready deadline, your submission may be excluded from the conference
%  proceedings.
%\item If you wish to include any **videos** as part of the supplementary material,
%  please do not include them in the ZIP file. Instead, simply provide the
%  public URL where the video is available in the appropriate field of the CMT
%  submission form. NOTE: This does not refer to the recorded presentation
% explaining your paper, which will be submitted/recorded separately, but to any other videos %containing, e.g., experimental results.
%\item Do not include any textual supplementary material in the ZIP file. It must be
%  included in the same PDF as the main paper.
%\end{itemize}


\vfill
\input{6-appendix}
\end{document}
