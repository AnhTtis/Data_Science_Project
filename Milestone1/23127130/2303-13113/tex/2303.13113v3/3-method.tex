\section{Method}
\label{sec:empirical method}
\textbf{Overview.} Class-incremental learning involves updating a neural network with new classes as it comes in. Specifically, the learner receives a sequence of learning tasks $\mathcal{T}_{1:t} = (\mathcal{T}_{1}, \mathcal{T}_{2}, ...,\mathcal{T}_{t})$, each with a corresponding dataset $\mathcal{D}_{\mathcal{T}} = { (x_{i,t}, y_{i,t})^{n_{t}} }$ consisting of $n_{t}$ instances per task. Each input pair ${x_{i,t}, y_{i,t}} \in \mathcal{X}_{t} \times \mathcal{Y}_{t}$ is sampled from an unknown distribution where $x_{i,t}$ is the sample and $y_{i,t}$ is the corresponding label. It's important to note that the learning tasks are mutually exclusive, i.e., $\mathcal{Y}_{t-1} \cap \mathcal{Y}_{t} = \emptyset$. When a new learning task arrives, the deep convolutional network is optimized to embed the input instance into the classifier space $f_{\Theta}: \mathcal{X}_{t}\rightarrow \mathcal{Y}_{t}$, where $\Theta$ represents the parameters of the learner. %To improve clarity, we will drop the subscript moving forward. 

The incremental learner has two goals: to effectively learn the current task (\textit{plasticity}) while retaining performance on all previous tasks (\textit{stability}). This can be accomplished by optimizing the following function where $CE(\cdot)$ represents the Cross-Entropy used in classification, and $Reg(\cdot)$ is a regularization term that penalizes abrupt changes in the neural network weights~\citep{li2017learning,kirkpatrick2017overcoming,rebuffi2017icarl,zhao2020maintaining}:
% burada EWC de abrupt changes olmasini engelliyoruz ama KD yontemlerinde previous tasklari unutmamak icin knowledge distill ediyoruz bunu da ayrica belirtmemiz gerekir mi 

\vspace{-8pt}
{\small
\begin{align}
\label{equ:train}
\mathcal{L} = CE(f(x_{i,t}), y_{i, t}) + \lambda \cdot Reg(\Theta)
\end{align}}

\subsection{Base Models for AdaCL}
AdaCL can be combined with many base incremental learners. We experimented with four popular, well-established techniques: EWC~\citep{kirkpatrick2017overcoming}, LwF~\citep{li2017learning} iCaRL~\citep{rebuffi2017icarl} and WA~\citep{zhao2020maintaining}. We select baselines that complement each other and serve as strong baselines within the field of incremental learning (Table \ref{tab:baselines}).  

\begin{table}[h]
\caption{Selected models to evaluate the impact of adaptivity in Class-Incremental Learning.}
\label{tab:baselines}
\centering
\small
\begin{tabularx}{\columnwidth}{>{\centering\arraybackslash}m{1 cm}*{4}{>{\centering\arraybackslash}X}}
\hline
method & prior-based & distillation-based & exemplar collection & classifier correction \\ \hline
EWC    &                \ding{51}            &                                   &                     &                       \\
LwF    &                            &           \ding{51}                        &                     &                       \\
iCaRL  &                            &                \ding{51}                   &          \ding{51}           &                       \\
WA     &                            &              \ding{51}                     &         \ding{51}            &      \ding{51}                 \\ \hline
\end{tabularx}
\end{table}

\textbf{EWC.} Elastic Weight Consolidation~\citep{kirkpatrick2017overcoming} is a weighted regularization approach. The authors argue that not all weights contribute equally to learning a new task and estimate the importance of each weight in minimizing the classification loss for the current task: $Reg(\Theta) = || \mathcal{F}(\Theta - \Theta^{\prime}) ||$, where $\Theta^{\prime}$ is the model weights from the previous learning step, $\mathcal{F}$ is the Fisher matrix of the same size as the weight matrices $\Theta$, re-weighting the contributions of each weight to stabilize the important neurons per task.

\textbf{LwF.} Learning-without-Forgetting~\citep{li2017learning} is a knowledge-distillation approach where the teacher branch is the model from the previous task, and the student branch is the current model. The aim is to match the activations of the teacher and student branches, either at the feature or logit level. Formally, LwF minimizes the following objective where $f^{\prime}$ is the model from the previous learning step, and $KL(p_1, p_2)$ is the KL-divergence between two probability distributions $p_{1}$ and $p_{2}$:

\vspace{-10pt}
\begin{align}
\label{KD}
Reg(\Theta) = KL(f(x_{i,t}) , f^{\prime}(x_{i,t}))
\end{align}
\vspace{-10pt}

\textbf{iCaRL.} The Incremental Classifier and Representation Learning~\citep{rebuffi2017icarl} leverages a hybrid approach that involves two main components: exemplar-based memory which is carefully selected to maintain representation and a regularization. The exemplar-based memory module retains a subset of exemplar samples from previous tasks, representing important instances that encapsulate the learned knowledge. By utilizing exemplars, iCaRL ensures the model's ability to recognize and classify past instances while discriminating between learned and new classes. The distillation loss as in Eq. \ref{KD} used for regularization, enables knowledge distillation from previous models to guide the learning process for new tasks. This distillation process allows the model to align logits of new classes with already learned classes to mitigate catastrophic forgetting.

\textbf{WA.} Maintaining Discrimination and Fairness in Class Incremental Learning~\citep{zhao2020maintaining}
is a method that consists of two phases: maintaining discrimination and maintaining fairness. The first phase is similar to the previously established method~\citep{rebuffi2017icarl}. Their study demonstrates that knowledge distillation is not sufficient by itself to prevent the model to treat old classes and new classes fairly since there is a high tendency towards new classes in the classifier layer to minimize the Eq \ref{KD}. Therefore, the second stage named Weight Aligning (WA) focuses on maintaining fairness to correct this classifier bias towards new classes. WA showed that it treats all classes fairly, and significantly improves the overall performance.

\vspace{-9pt}
\subsection{Constancy Assumption in Class Incremental Learning} 
The scalar parameter $\lambda$ balances the contribution of the classification and regularization loss functions. A large value of $\lambda$ ensures minimal weight updates, which can sacrifice learning on the current task. Conversely, a small $\lambda$ yields good performance on the current task but may sacrifice performance on previous tasks, exacerbating catastrophic forgetting. Similarly, requirement for a fixed or predetermined memory size per task may not always be optimal, as it depends on the new task and its relationship to previous tasks. Specifically,where the new task is highly similar to previous tasks, it is possible to retain past knowledge by storing only a small number of representative samples. Conversely, when the new task is significantly distinct, it is reasonable to store a larger number of examples in memory to prevent catastrophic forgetting while learning new tasks.
However, as a common practice, important hyperparameters such as learning rate ($\eta$), regularization strength ($\lambda$), and memory size ($m$) are set to a fixed or pre-defined scalar value throughout all incremental learning sessions with $t \in \mathcal{T}_{1:t}$; such that $\eta_{t} = \eta_{t-1}$, $\lambda_{t} = \lambda_{t-1}$ or $\lambda_{t} = \frac{t*c}{(t*c) + c}$ where c is the number of classes per task. Similarly, $m_{t} = m_{t-1}$ or $m_{t} = \frac{M}{t}$ where M is the pre-defined total memory size.

We hypothesize that the assumption of \textit{constant or pre-defined learning rate, regularization strength, and exemplar size per task} is suboptimal for building accurate lifelong learning machines. Our reasoning is two-fold:

\textbf{Low Plasticity and High Stability.} The incremental learner may encounter a novel object that is highly familiar with the previously learned tasks. For example, it may encounter the category \textit{dog} after observing many other animal categories, such as \textit{{cat, cow, bird}}. In this case, the learner does not need to store many exemplars from previous tasks or to be too plastic, as it can quickly transfer knowledge from the previous tasks where it is similar to the human learning process and referred to \textit{low road transfer} \citep{perkins1992transfer}. Hence, no drastic updates to the learned filters are necessary.

\textbf{High Plasticity and Low Stability.} Conversely, the learner may encounter a novel object that is highly unfamiliar with the previous tasks. For example, it may encounter the category \textit{car} after observing many other animal categories, such as \textit{{cat, cow, bird}}. In this case, the learner would require replaying more exemplars from previous tasks to preserve old knowledge and high plasticity to learn about the novel object with never-before-seen parts, such as wheels.

\vspace{-4pt}
\subsection{AdaCL: Adaptive Continual Learning} 
AdaCL aims to optimize the regularization magnitude $\lambda$ and memory size $m$ as a function of model performance over a set of incremental tasks, conditioned on the current learning task and all previous tasks. We define $\eta(t)={\eta_1, \eta_2, \ldots, \eta_{t-1}, \eta_t}$, and $\lambda(t)~=~{\lambda_1, \lambda_2, \ldots, \lambda_{t-1}, \lambda_t}$, and $m(t)~=~{m_1, m_2, \dots, m_{t-1}, m_t}$ where $\eta_{t}$, $\lambda_{t}$ and $m_{t}$ are predicted by minimizing the following optimization problem:

\vspace{-15pt}
{\small
\begin{align}
\label{equ:optim}
\arg\min_{\eta, \lambda, m} \mathcal{L}(\Theta; V_{t})
= \arg\min_{\eta, \lambda, m} \sum_{i=1}^{|V_{t}|} [CE(f(x_{i,t};\Theta), y_{i,t}) 
\end{align}}

\begin{algorithm}
    \caption{AdaCL: Adaptive Continual Learning}
    \label{algo}
    \small
    \begin{algorithmic}[1]
    \Require
    \Statex{$\theta_{t-1}$} \Comment{model from previous task}
    \Statex{$X_t$} \Comment{dataset from new task}
    \Statex{$M_{t-1} = m_1,\dots,m_{t-2}, m_{t-1}$} \Comment{memory from old tasks}
    \Statex{$V_{t-1} = v_1,\dots,v_{t-2}, v_{t-1}$} \Comment{val. set from tasks seen so far}
    \Statex{$\eta_{space}$}  \Comment{search space for learning rate}
    \Statex{$\lambda_{space}$}  \Comment{search space for regularization}
    \Statex{$m_{space}$}  \Comment{search space for memory}
    \Statex{$configs, epochs$} \Comment{\# of configurations and epochs}
    \State $V_{t} = V_{t-1} \cup v_t \gets X_t$
    \For{$c = 1,\dots,configs$}
    \State $\eta_t \gets \eta_{space}$ \Comment{$\eta$ for new task}
    \State $\lambda_t \gets \lambda_{space}$ \Comment{$\lambda$ for new task}
    \State {$M_{t}: m_{t} \gets m_{space}$} \Comment{memory with a size of $m_t$}
    \State{$D = X_t \cup M_{t-1} \cup M_t$} \Comment{concat new data and memory}
    \For{$e = 1,\dots,epochs$}
    \State{Train  Eq. \ref{equ:train} with $\theta_{t-1}$ and $D$}
    \State{Evaluate Eq. \ref{equ:optim} with $V_t$}
    \EndFor
    \EndFor
    \State \textbf{return} $\theta_{t}, V_t, \eta_t^*, \lambda_t^*,  M_t^*$ \Comment{new model with optimal learning rate, regularization strength and memory size}
    \end{algorithmic}
\end{algorithm}

\noindent Here, $V_{t}$ is a randomly selected class-balanced subset of the current task and previous tasks that guide the model's adaptation with careful consideration of both new and previous tasks' characteristics and prevents bias over certain classes. $\mathcal{L}(\Theta; V_{t})$ is the loss function where the learning rate $\eta$, the regularization coefficient $\lambda$, and memory size per task $m$ is determined by solving the optimization problem. Our adaptive approach, AdaCL (Algorithm \ref{algo}), starts after the first task since it is just a standard batch learning. 

In the following tasks, it retains the model $\theta_{t-1}$ trained on the previous task, receives current task data $X_{t}$, and creates a validation set $V_{t}$. Then, training data $D$ is constructed and trained with Eq. \ref{equ:train} after the configuration for {$\eta_t$,} $\lambda_t$ and $m_{t}$ is selected by Bayesian Optimization (see Section 3.4). After each epoch, the selected configuration is evaluated on the validation set $V_{t}$ with Eq. \ref{equ:optim}. Subsequently, this process is repeated until reaching the total number of configurations. The optimal learning rate $\eta_t^*$, lambda $\lambda_t^*$, and memory size per task $m_t^*$ are determined based on the validation performance.

This approach allows us to automatically adjust the {learning rate, regularization strength, and memory size per task according to the specific learning task based on the given loss function which lets the model find the degree of difficulty itself, avoiding the unrealistic assumption of a fixed learning rate, regularization strength, and memory size throughout the learning process.

\vspace{-5pt}
\subsection{Bayesian Optimization via Parzen Estimator}

We optimize the objective function using multivariate tree-structured parzen estimators (TPE)~\citep{bergstra2011algorithms}. TPE builds a conditional probability tree that maps hyperparameters to their respective model performances. Then it can be used to guide a search algorithm to find the optimal set of hyperparameters for the given model. In this study, TPE is utilized as a search algorithm where it searches within the provided range for learning rate, regularization strength, and memory size per task and then searches for the best value by evaluating across accumulated validation set which consists of previous and new tasks throughout incremental learning sessions. We use Optuna \citep{optuna} for TPE implementation.



