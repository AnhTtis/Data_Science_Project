\newpage
\section{Conclusion}
This study introduces the idea of adaptive hyperparameter tuning for Class-Incremental Learning. These hyperparameters are treated as tunable variables that can be adjusted for an each new task according to the learner's current condition and the complexity of the task. Leveraging the sample-efficiency of Bayesian Optimization, the paper presents a methodology to predict the optimal values for these hyperparameters in each learning task. By conducting experiments on well-established benchmarks, the study showcases the remarkable enhancements in performance achieved through adaptive learning, resulting in improved accuracy, diminished forgetting, and less memory. Potential avenues for future investigation could involve the reduction of hyperparameter tuning costs (e.g. via warm-starting) and the exploration of alternative methods for constructing or optimizing the validation set.

To sum up, our study leads the way in introducing the concept of adaptive hyperparameter optimization in Class-Incremental Learning, with a mindful consideration of the limitations we've recognized. As the field further advances, we anticipate that these insights will shape the evolution of advanced continual learning approaches, empowering deep neural networks to adapt to streams of real-world tasks.

\section*{Acknowledgements}
This work is supported by; TAILOR, a project funded by the EU Horizon 2020 research and innovation programme under GA No. 952215, and the Dutch national e-infrastructure with the support of SURF, Cooperative using grant no. EINF-4569, and the Turkish MoNE scholarship.