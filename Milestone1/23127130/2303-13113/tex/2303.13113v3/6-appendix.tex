\clearpage
\appendix
\section{Appendix}


\subsection{Search Space Range}
\label{searchspace_ablate}
In this section, we present the findings of our search space ablation study conducted to determine the optimal range of regularization strength. Our analysis revealed that regularization strengths within the interval of [1, 100] yielded optimal performance for LwF, thus it is used in our main experiments. Similarly, for EWC  the optimal regularization strength falls within the range of [1, 50000] and is employed in our main experiments.

\begin{table}[h]
\caption{Ablation of the search space for regularization hyperparameter on EWC and LwF methods. The search space intervals were determined based on achieving optimal accuracy results. Best results are highlighted in bold.}
\label{tab:reg_searchspace}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
\multicolumn{1}{c}{Search Space} & {[}1,1000{]} & {[}1,25000{]} & {[}1,50000{]} & {[}1,10000{]} \\ \hline
EWC & 9.17  & 22.02 & \textbf{22.39} & 21.09         \\ \hline
\multicolumn{1}{c}{Search Space} & {[}1,10{]}   & {[}1,25{]}    & {[}1,50{]}    & {[}1,100{]}   \\ \hline
LwF & 23.99 & 25.49 & 26.73          & \textbf{28.9} \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Hyperparameter Dynamics}
\label{hpo_dynamics}
In this section, we present an overview of the selected hyperparameters for the EWC and LWF methods. Our analysis highlights that our adaptive approach allows all models to flexibly adjust their hyperparameters across different tasks as illustrated in Figure \ref{fig:ewc_hp} and \ref{fig:lwf_hp}. This flexibility plays a role in improving the performance of the models over time by adaptively adjusting hyperparameters.
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/ewc_reg.pdf}
    \caption{Selected regularization strength}
    \label{fig:ewc_reg}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/ewc_lr.pdf}
    \caption{Selected learning rate}
    \label{fig:ewc_lr}
  \end{subfigure}
  \caption{Adaptive adjustments in (a) regularization strength and (b) learning rate for EWC across various task sequences and datasets.}
  \label{fig:ewc_hp}
\end{figure}


\begin{figure}[t]
\vskip -12.5cm
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/lwf_reg.pdf}
    \caption{Selected regularization strength}
  \end{subfigure}
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/lwf_lr.pdf}
    \caption{Selected learning rate}
  \end{subfigure}
  \caption{LwF's (a) regularization strength and (b) learning rate change dynamically across different task sequences and datasets.}
  \label{fig:lwf_hp}
\end{figure}


%In our investigation of LwF, we noticed a progressive decline in learning rates over successive epochs, alongside fluctuating regularization strengths, suggesting a strategic balance between retaining prior knowledge and accommodating new information for both datasets.  Conversely, in the case of EWC, although learning rates remain consistent, the spectrum of regularization strengths displays significant variability. These nuanced hyperparameter dynamics shed light on the adaptability required to address the challenges posed by diverse datasets.