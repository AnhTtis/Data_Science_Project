\documentclass[12pt]{article} % add titlepage param for separate title page

\input{preamble}

\title{\bf\Large How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on 67 Replicated Studies%
   \thanks{Apoorva Lal, PhD Candidate, Stanford University; Email: \url{apoorval@stanford.edu}. Mac Lockhart, PhD Candidate, University of California, San Diego; Email: \url{mwlockha@ucsd.edu}. Yiqing Xu (corresponding author), Assistant Professor, Stanford University; Email: \url{yiqingxu@stanford.edu} Ziwen Zu, PhD Student, University of California, San Diego; Email: \url{zzu@ucsd.edu}.
   We thank Te Bao, Daniel Chen, Gary Cox, Charles Crabtree, Ted Enamorado, Hanming Fang, Avi Feller, Don Green, Justin Grimmer, Anna Grzymala-Busse, Jens Hainmueller, David Laitin, Adeline Lo, Justin McCrary, Jacob Montgomery, Doug Rivers, Henrik Sigstad, Brandon Stewart, Arthur Yu, Xiang Zhou, and seminar participants at Stanford University, Washington University in St. Louis, APSA 2021, and Polmeth 2021 for extremely valuable comments.
   }
}

\author{Apoorva Lal (Stanford) \and Mac Lockhart (UCSD)
\and Yiqing Xu (Stanford) \and Ziwen Zu (UCSD)}

\date{\bigskip\normalsize
      First Version: July 10, 2021\\
      This Version: \today
   \\
   %Word Count: 10,627 \\
   \vspace{-2em}
}

\begin{document}

% \input{note}

\thispagestyle{empty}
\clearpage

\maketitle

\thispagestyle{empty}
\setcounter{page}{0}
\thispagestyle{empty}

\begin{abstract}\vspace{-0.5em}\noindent
Instrumental variable (IV) strategies are widely used in political science to establish causal relationships, but the identifying assumptions required by an IV design are demanding, and assessing their validity remains challenging. In this paper, we replicate 67 papers published in three top political science journals from 2010-2022 and identify several concerning patterns. First, researchers often overestimate the strength of their instruments due to non-i.i.d. error structures such as clustering. Second, the commonly used $t$-test for two-stage-least-squares (2SLS) estimates frequently underestimates uncertainty. Using more robust inferential methods, we find that about 19-30\% of the 2SLS estimates in our sample are underpowered. Third, in most replicated studies, 2SLS estimates are significantly larger than ordinary-least-squares estimates, with their ratio negatively correlated with instrument strength in studies with non-experimentally generated instruments, suggesting potential violations of unconfoundedness or exclusion restriction. We provide a checklist and software to help researchers avoid these pitfalls and improve their practice.\\

\noindent\textbf{Keywords:} instrumental variables, two-stage-least-squared, replications, weak instrument, exclusion restriction, 

\end{abstract}

\setcounter{page}{0}
\thispagestyle{empty}

% \clearpage
% \tableofcontents
% \vspace{5mm}
% \listoftodos

% #### ##    ## ######## ########   #######
%  ##  ###   ##    ##    ##     ## ##     ##
%  ##  ####  ##    ##    ##     ## ##     ##
%  ##  ## ## ##    ##    ########  ##     ##
%  ##  ##  ####    ##    ##   ##   ##     ##
%  ##  ##   ###    ##    ##    ##  ##     ##
% #### ##    ##    ##    ##     ##  #######


% \linenumbers
% \modulolinenumbers[5]

\setcounter{page}{0}
\thispagestyle{empty}

\clearpage
\doublespacing



\section{Introduction} \label{intro}



The instrumental variable (IV) approach is a commonly used empirical method in the social sciences, including political science, for establishing causal relationships. It is often used when selection on observables is implausible, experimentation is infeasible or unethical, and rule-based assignments that allow for sharp regression-discontinuity (RD) designs are not available. In recent years, there has been a growing number of papers published in top political science journals, such as the \emph{American Political Science Review} (APSR),  \emph{American Journal of Political Science} (AJPS), and \emph{Journal of Politics} (JOP), that use IV as a primary causal identification strategy. This trend can be traced back to the publication of the textbook \emph{Mostly Harmless Econometrics} \citep{angrist2008mostly}, which popularized the modern interpretation of IV designs, and \citet{sovey2011instrumental}, which clarifies the assumptions required by an IV approach and provides a useful checklist for political scientists.


\begin{figure}[!h]
\caption{IV Papers Published in the \emph{APSR}, \emph{AJPS}, and \emph{JOP}}\label{fig:pubs.year}\centering
\begin{minipage}{0.9\linewidth}{
\begin{center}
\hspace{2em}\includegraphics[width=1\textwidth]{graphs/summary.pdf}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Our criteria rule out IV models appearing in the online appendix only, in dynamic panel settings, with multiple endogenous variables, and with nonlinear link functions. Non-replicability is primarily due to a lack of data and/or coding errors.}}
\end{minipage}
\end{figure}

Despite its popularity, some researchers have questioned the validity of the IV approach, noting that  two-stage least-squares (2SLS) estimates are often much larger in magnitude than ``naïve'' ordinary-least-squares (OLS) estimates, even when the main concern with the latter is upward omitted-variables bias.\footnote{For example, in the 2016 National Bureau of Economic Research--Political Economy Meeting, following a presentation of a study using an IV approach, the late political economist Alberto Alesina asked the audience: ``How come 2SLS estimates are always five times bigger than OLS estimates in political economy?''} Others have raised concerns about the validity of inferential methods used for 2SLS estimation \citep[e.g.][]{Lee2020-mi,Young2022}.

These observations motivate our systematic examination of the use of IVs in the empirical political science literature. We set out to replicate all papers published in the APSR, AJPS, and JOP during the past thirteen years (2010-2022) that use an IV design with a single endogenous variable as one of the main identification strategies.\footnote{Focusing on design with a single endogenous variable allows us to calculate the correlation coefficient between the treatment and the predicted treatment and apply powerful tools such as the Anderson-Rubin (AR) test and the $tF$ test (when there is only a single instrument). Moreover, we find it difficult to justify the exclusion restriction in a multiple-treatment-multiple-instrument setting in the first place.}  Out of 114 papers meeting this criterion, 71 have complete replication materials online, which is itself a troubling pattern. We successfully replicate at least one of the main IV results for 67 of the 71 remaining papers, with three papers having two separate IV designs producing separate 2SLS results.


Using data from these 70 IV designs, we conduct a programmatic replication exercise and find three troubling patterns. First, a significant number of IV designs in political science either do not report the first-stage partial $F$ statistic or overestimate it by failing to adjust standard errors (SEs) for factors such as heteroskedasticity, serial correlation, or clustering structure. Using the effective $F$-statistic \citep{Olea2013-pa}, we find that at least 11\% of the published IV studies rely on what econometricians call ``weak instruments,'' the consequences of which have been well-documented in the literature (see \citet{andrews2019weak} for a comprehensive review).

A second related issue is statistical inference. We find that a considerable number of IV designs are underpowered, with almost all studies relying on $t$-tests based on analytic standard errors and traditional critical values (such as 1.96 for statistical significance at the 5\% level) to make inferences about the 2SLS coefficients. When we use bootstrapping procedures, the AR test, or the $tF$ procedure, an $F$-statistic-dependent $t$-test \citep{Lee2020-mi}, to perform hypothesis testing, we find that, depending on the method employed, 19-30\% of designs cannot reject the null hypothesis of no effect at the 5\% level, whereas the number based on the SEs or $p$-values reported in the original papers is only 10\%. This suggests that inferences based on traditional $t$-tests may not accurately reflect the uncertainties in 2SLS estimates in a significant portion of cases.

Finally, our replications corroborate evidence from economics and finance that the 2SLS estimates are often much bigger in magnitude than the OLS estimates obtained from regressing the outcome on the potentially endogenous treatment variables and covariates
\citep{jiang2017have}. In 68 out of the 70 designs (97\%), the 2SLS estimates are bigger than the OLS estimates in magnitude; among them, 24 (34\%) are at least five times bigger. This is alarming because, in an IV design with observational data, researchers often say that they are most concerned about the upward bias of the treatment effect estimates produced by naïve OLS. Even after we exclude 15 papers that explicitly claim to expect downward biases in OLS estimates, the percentages remain high (96\% and 35\%, respectively).

The first two patterns may be due to researchers' unfamiliarity with recent development in the IV literature, such as the effective $F$ statistic and the  $tF$ test,  or under-utilization of inferential procedures robust to weak instruments, such as the AR test. Therefore, researchers can avoid these problems by adopting better practices. The third finding, however, is the most concerning. We cannot explain it with weak instruments alone because at least in the case of i.i.d. errors, when instruments are exogenous, weak instruments bias 2SLS estimates toward OLS estimates in finite samples  \citep{bound1995problems}. But what we observe is the opposite: The ratio between the magnitudes of the 2SLS and OLS estimates is strongly negatively correlated with the strength of the first stage among studies that use non-experimental instruments, and the relationship is almost nonexistent among studies with experimental instruments. We suspect that this is primarily driven by a combination of weak instruments and failure of exogeneity, although other mechanisms such as publication bias, heterogeneous treatment effects (HTE), and measurement error may also contribute. 

% We suspect several possibilities are primarily responsible for this pattern. First, it can be caused by a combination of weak first stages and the failure of the (conditional) exogeneity assumption, either because the instruments are confounded or because they violate the exclusion restriction. Intuitively, because the 2SLS estimator is a
% ratio, when the magnitude of the numerator is inflated due to endogenous IVs or the failure of the exclusion restriction while the magnitude of the denominator is a small number because of weak instruments, the 2SLS estimate explodes. Another possibility is the publication bias: a weak first stage leads to highly variant 2SLS estimates and those with large magnitudes are more likely to be reported and published \citep{gelman2014beyond}, hence the negative correlation.  Other explanations, such as heterogeneous treatment effects (HTE) and measurement errors in treatment variables, may also contribute to this phenomenon, though we doubt they are the main driving forces given the different patterns between observational and experimental studies.


What do these findings mean for empirical IV studies in political science? First, traditional $t$~tests for the 2SLS estimates (especially those based on classic analytic SEs) mask the fact that most IV results are highly uncertain, which likely leads to selective reporting and publication bias. Second, and more importantly, many of the 2SLS estimates likely suffer from large biases due to failures of unconfoundedness or exclusion restriction and hence are not credible.  Although we cannot definitively say which estimates are problematic, the underlying issue seems to prevail in the IV literature. However, the goal of this paper is not to discredit existing IV studies or dissuade researchers from ever using the IV method. On the contrary, we want to caution researchers against ad-hoc justifications for IVs in observational studies and provide practical advice to improve future practices. This includes accurately quantifying instrument strength and 2SLS estimate uncertainties, as well as conducting additional analysis, such as placebo tests, to corroborate the identifying assumptions.

Our work builds on a growing literature evaluating IV strategies in social sciences and offering methods to improve empirical practice. Notable studies include \citet{Young2022}, which finds IV estimates to be more sensitive to outliers and conventional $t$-tests to understate uncertainties; \citet{jiang2017have}, which observes larger IV estimates in finance journals and attributed this to exclusion restriction violations and weak instruments; \citet{mellon2020rain}, which emphasizes the vulnerability of weather instruments; \citet{dieterle2016simple}, which develops a quadratic over-identification test and discovered significant non-linearities in the first stage regression; \citet{FeltonStewart2022}, which finds unstated assumptions and a lack of weak-instrument robust tests in top sociology journals; and \citet{cinelli2022omitted}, which proposes a sensitivity analysis for IV designs in an omitted variable bias framework. Our study is the first large-scale replication effort focusing on IV designs and the first to link the discrepancy between 2SLS and OLS estimates to weak instruments using extensive replication data across social sciences.

% We contribute to a growing literature that evaluates the use of IV
% strategies in empirical work across the social sciences and provides
% methods to improve empirical practice. \cite{Young2022} replicates IV
% designs from 32 articles published in economics journals and finds that
% IV estimates are more sensitive to outliers than their OLS counterparts
% and that conventional $t$ tests systematically understate uncertainties
% and therefore have high false discovery rates. \cite{jiang2017have}
% surveys over 250 articles published in finance journals and finds that
% the vast majority of them report IV estimates that are larger than the
% corresponding OLS estimates regardless of the sign of the potential
% omitted variables bias, and postulates that this bias can be attributed
% to exclusion restriction violations exacerbated by weak instruments, or
% local effects that are far from representative of the population
% treatment effect; however, the author does not conduct replications.
% \citet{mellon2020rain} proposes to use sensitivity analysis to quantify
% the vulnerability of using weather as instruments to exclusion
% restriction violations.  \cite{dieterle2016simple} develop a quadratic
% over-identification test for the first stage 2SLS, apply it to 15
% published papers that use linear first stage, and find significant
% non-linearities in 10 of them, and suggest that evaluating the implied
% patterns of heterogeneity from their test relative to theoretical
% predictions can be used to gauge the validity of an instrument.
% \citet{FeltonStewart2022} survey 15 IV papers published in top sociology
% journals and find that the assumptions required by an IV design are
% often unstated and none of the studies use weak-instrument robust tests
% or bootstrapped CIs. To the best of our knowledge, we are the first to
% link the discrepancy between 2SLS and OLS estimates with the problem of
% weak IVs using large-scale replication data from the social sciences.

% The organization of the paper is as follows. In Section~\ref{theory}, we
% briefly review the IV method under the traditional parametric framework,
% including the required identifying assumptions, estimation strategies,
% potential pitfalls, and newly developed methods in the literature.
% Section~\ref{desc} details our case selection criteria and the resulting
% replication sample. Section~\ref{sec:repl} describes our replication
% procedure and presents the main results of our replication exercise. The
% last section concludes the paper with a set of practical advice for
% researchers.

% section~\ref{sec:placebo} suggests diagnostic tools for potential
% exclusion restriction violations and illustrates them using a case
% study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ##     ##    ###    ######## ##     ##  ######
% ###   ###   ## ##      ##    ##     ## ##    ##
% #### ####  ##   ##     ##    ##     ## ##
% ## ### ## ##     ##    ##    #########  ######
% ##     ## #########    ##    ##     ##       ##
% ##     ## ##     ##    ##    ##     ## ##    ##
% ##     ## ##     ##    ##    ##     ##  ######



\section{Theoretical Refresher} \label{theory}

In this section, we offer a brief overview of the IV approach, including
the setup, the identifying assumptions, as well as the 2SLS estimators.
We then discuss potential pitfalls and survey several inferential
methods. To cover the vast majority of IV studies in political science,
we adopt a traditional constant treatment effect approach to IV designs,
which imposes a set of parametric assumptions. For example, 51 (73\%)
designs in our replicated sample employ continuous treatment variables
and make no reference to treatment effect heterogeneity, hence, they are
ill-suited for the local average treatment effect (LATE) approach
\citep{angrist1996identification}.

%\footnote{Marginal treatment effect (MTE) based methods \citep{heckman2007econometric, Mogstad2018-me, Mogstad2018-ta} extend the LATE approach to apply to continuous treatments, but these methods have not yet been widely adopted in political science.}

For simplicity, we do not include additional exogenous controls in the
discussion without loss of generality. This is because, by the
Frisch-Waugh-Lovell theorem, we can remove them by regressing the
outcome, treatment, and IVs on the controls and using the residuals for
all subsequent analyses.

\begin{figure}[!ht]
\caption{A Directed Acyclic Graph of an IV Design}
\centering\label{fig:dag}
\includegraphics[width=0.35\textwidth]{graphs/dag.png}\vspace{-1.5em}
\end{figure}

Apart from the ``canonical'' use of IVs in addressing non-compliance in
experimental encouragement designs, we observe that in the majority of
the papers we review, researchers use IVs in observational settings to
establish causality between a single treatment variable $d$ and an
outcome variable $y$. The basic idea of this approach is to use an
instrument $z$ to isolate "exogenous" variation in $d$ (i.e., the
variation in $d$ that is not related to potential confounders) and
estimate its causal effect on $y$. Figure 1 illustrates the directed
acyclic graph for an IV design, where $\varepsilon$ denotes the error
term that captures all unexplained variations in $y$. The figure depicts
that because $d$ and $\varepsilon$ are correlated, an observed
correlation between $d$ and $y$ does not identify the causal effect of
$d$ on $y$. It also shows that an IV approach relies on three crucial
assumptions: (1) the \emph{relevance} of the instrument, which is
directly testable, meaning that $z$ is correlated with $d$; (2) the
\emph{unconfoundedness} assumption, which states that $z$ is
quasi-randomly assigned, and (3) the \emph{exclusion restriction}, which
posits that $z$ does not have a direct effect on $y$ beyond the channel
through $d$.


\subsection{Estimation Strategies}

Imposing a set of parametric assumptions, we define a system of
simultaneous equations:
\vspace{-1em}\begin{align}
  \text{Structural equation:}\quad y =&\ \alpha + \tau d  + \varepsilon\label{eq1} \\
  \text{First-stage equation:}\quad   d =&\ \pi_0 + \pi_1 z  + \nu\label{eq2}
\end{align}
in which $y$ is the outcome variable, $d$ is a scalar treatment
variable; $z$ is a vector of instruments for $d$; $\tau$ captures the (constant)
treatment effect and is the key quantity of interest.
Equations~(\ref{eq1}) and (\ref{eq2}) are referred to as the structural
equation and the first-stage equation, respectively. The error terms $\varepsilon$ and
$\nu$ in the two equations may be correlated.

The endogeneity problem for $\tau$ in Equation~(\ref{eq1}) arises when $d$
and $\varepsilon$ are correlated, which renders $\hat\tau_{OLS}$ from
a naïve OLS regression of $y$ on $d$ inconsistent. The endogeneity
problem may be due to one of the following reasons: (1) unmeasured
omitted variables that are correlated with both $y$ and $d$; (2)
measurement error in $d$, or (3) simultaneity or reverse causality,
which means $y$ may also affect $d$. The IV approach addresses this
problem by taking advantage of the exogenous variation in $d$ brought
by $z$. Substituting $d$ in Equation~(\ref{eq1}) using
Equation~(\ref{eq2}), we have the reduced form equation:

\vspace{-1em}\begin{equation}
\text{Reduced form:}\quad y =
\underbrace{(\alpha + \tau \pi_0)}_{\gamma_0} +
\underbrace{(\tau \pi_1)}_{\gamma_1} z + (\tau\nu + \varepsilon).
\end{equation}

Substitution establishes that $\gamma_1 = \tau \pi_1$, rearranging
yields $\tau = \frac{\gamma_1}{\pi_1}$ (assuming that we only use one instrument, but the intuition carries over to cases with multiple instruments). The IV
estimate, therefore, is the ratio of the reduced-form and first-stage
coefficients. To identify $\tau$, we make the following assumptions
\citep[Chapter 12]{greene2003econometric}.

\begin{assum}[Relevance] $\pi_1 \neq 0$. \label{assumption:rel}
This assumption requires that the IVs can predict the treatment variable, and is therefore equivalently stated
as $d \not \indep z$.
\end{assum}


\begin{assum}[Exogeneity] $\text{Cov}(z, \varepsilon) = 0$ and $\E[\varepsilon] =0$. \label{assumption:exog} Assumption~\ref{assumption:exog} is satisfied when unconfoundedness and the exclusion restriction are satisfied. However, without additional structural assumptions. failures of unconfoundedness and the exclusion restriction are observationally equivalent, therefore, we do not distinguish them in the analysis and diagnostics.
\end{assum}
% there are no omitted variables in the first stage and the IVs are not reversely affected by the treatment; moreover, conditional on covariates, the IVs are uncorrelated with unobservables in the structural equation.
% \footnote{In the LATE framework, because the treatment effects are allowed to be heterogeneous, an additional ``monotonicity'' assumption is added to make sure that the instruments change the probability of getting treatment or treatment intensity monotonically.}

Under Assumptions~\ref{assumption:rel} and \ref{assumption:exog}, the 2SLS estimator is shown to be consistent for the structural parameter $\tau$. Consider a sample of $N$ observations. We can write $\Mat{d} = (d_1, d_2, \cdots, d_N)'$ and $\Mat{y}= (y_1, y_2, \cdots, y_N)'$ as $(N\times 1)$ vectors of the treatment and outcome data, and $\Mat{z}= (z_1, z_2, \cdots, z_N)'$ as $(N\times p_z)$ matrix of instruments in which $p_z$ is the number of instruments. To simplify mathematics, we residualize original $\Mat{d}$, $\Mat{y}$, and each column of $\Mat{z}$ against the exogenous covariates, obtaining $\Mat{y}$, $\Mat{d}$, and $\Mat{z}$, respectively. The 2SLS estimator is written as follows: 
\vspace{-0.5em}
\begin{equation} \hat\tau_{\text{2SLS}} = \inv{\Mat{d}'
\Mat{P}_{z} \Mat{d}}  \Mat{d}' \Mat{P}_{z} \ve{y}
\vspace{-0.5em}\end{equation}
in which $\Mat{P}_{z} = \Mat{z}\inv{\Mat{z}' \Mat{z}} \Mat{z}'$ is the hat-maker matrix from the first stage which projects the endogenous treatment variable $\Mat{d}$ into the column space of $\Mat{z}$, thereby preserving only the exogenous variation in $\Mat{d}$ that is uncorrelated with $\varepsilon$. This formula permits the use of more than one instrument, in which case the model is said to be ``overidentified.'' The 2SLS estimator belongs to a
class of generalized method of moments (GMM) estimators taking advantage of the moment condition $\E[z\varepsilon] =0$, including the two-step GMM \citep{hansen1982large} and limited information maximum likelihood (LIML) estimators  \citep{anderson1982evaluation}. We use the 2SLS estimator throughout the replication exercise because of its simplicity and because every single paper in our replication sample uses it in at least one specification.

When the model is exactly identified, i.e., the number of treatment
variables equals the number of instruments, the 2SLS estimator can be
simplified as the IV estimator: $\hat{\tau}_{\text{2SLS}} = \hat{\tau}_{\text{IV}} =
\inv{\Mat{z}'\Mat{d}} \Mat{z}'\Mat{y}$. In the case of one instrument and one treatment, the 2SLS estimator can also be written as
a ratio of two sample covariances: $\hat{\tau}_{2SLS} = \hat{\tau}_{IV}
= \frac{\hat\gamma_1}{\hat\pi_1} = \frac{\widehat{\rm Cov}(\Mat{y},
\Mat{z})}{\widehat{\rm Cov}(\Mat{d}, \Mat{z})}$, which illustrates
that the 2SLS estimator is a ratio between reduced-form and
first-stage coefficients in this special case. This further simplifies
to a ratio of the difference in means when $z$ is binary, which is
called a Wald estimator.

% The 2SLS estimator is consistent and asymptotically normal, but has
% bad finite-sample properties and is biased toward the OLS result in
% small samples when identifying assumptions are invalid
% \citep{bound1995problems}. The finite sample bias is seen clearly from
% the expectation of the simple IV estimator: \vspace{-0.5em}$$
% \Exp{\hat{\tau}_{IV}} = \Exp{\inv{\sumin z_i z_i'} \sumin z_i y_i} =
% \tau + \Exp{\inv{\sumin z_i x_i'} \sumin z_i \varepsilon_i}.
% \vspace{-0.5em}$$ The second term may not go to zero even when $\E[z_i
% \varepsilon_i]=0$. This is because the denominator cannot pass through
% the conditional expectation as in the OLS case, since we are not
% conditioning on $\Mat{d}$, only on $\Mat{z}$. This bias is decreasing
% in sample size, and is worsened by weak instruments and ``too many
% instruments'' \citep{Roodman2009-kj}. \citet{hirano2015location} prove
% that unbiased estimation is infeasible for linear IV models with an
% unrestricted parameters space for the first-stage coefficients.

% \paragraph*{Inference.} We use the IV estimator to illustrate why
% inference is more challenging with the 2SLS estimator than with the
% OLS estimator. A commonly used variance estimator for $\hat{\tau}_{IV}$
% can be written as: \begin{equation}\label{eqn:ivvar}
% \hat{\mathbb{V}}(\hat{\tau}_{IV}) \approx \frac{\hat{\Sigma}^2}{\sumin
% (x_i - \Ol{x})^2} \frac{1}{R_{xz}^2} =
% \hat{\mathbb{V}}(\hat{\tau}_{OLS}) \frac{1}{R_{xz}^2} \end{equation} in
% which $\hat{\Sigma}^2$ is a variance estimator for the error term and
% $R^2_{xz}$ is the R-squared from the first-stage. The estimated
% variance is mechanically larger than the estimated variance of the OLS
% estimator as long as $R^2_{xz} < 1$. It is decreasing in $R^2_{xz}$,
% i.e. stronger instruments produce more precise IV estimates. Robust
% SEs can be computed using the IV analogue of the Huber-White sandwich
% formula \citep{bekker1994alternative}. We can also obtain uncertainty
% estimates for the 2SLS estimates using nonparametric bootstraps or
% cluster-bootstraps.



\subsection{Potential Pitfalls in Implementing an IV Strategy} % (fold)
\label{sub:problems_in_iv_estimation}

The challenges with 2SLS estimation and inference are often due to the violation of the two identifying assumptions. These difficulties can result in (1) significant uncertainty around 2SLS estimates and size distortion for $t$ tests due to weak instruments even when Assumption~\ref{assumption:exog} is valid; and (2) potentially larger biases in 2SLS estimates compared to OLS estimates when both assumptions are violated.

% https://microeconomics.ca/archive/vadim_marmer/weak_iv.pdf
% https://www.nber.org/econometrics_minicourse_2018/

\paragraph*{Inferential problem due to weak instruments.} Since the IV
coefficient is a ratio, the weak instrument problem is a
``divide-by-zero'' problem, which arises when $\Covar{z, x} \approx 0$
(i.e., when Assumption~\ref{assumption:rel} is violated). The instability of ratio estimators like $\wh\tau_{\text{2SLS}}$ when the denominator is approximately
zero has been extensively studied going back to
\citet{fieller1954some}. The conventional wisdom in the past two
decades has been that the first-stage partial $F$ statistic needs to
be bigger than 10, and it should be clearly reported
\citep{Staiger1997-lo}. As a rule of thumb, the original cutoff is chosen based on simulation results to meet two criteria under i.i.d. errors: (1) in the worst case, the bias of the 2SLS estimator does not exceed 10\% of the bias of the OLS estimator, and (2) a $t$-test based on the 2SLS estimator with a size of 5\% does not lead to size over 15\%.


The literature has discussed at least three issues caused by weak
instruments when Assumption ~\ref{assumption:exog} is valid. First, under i.i.d. errors, a weak first stage exacerbates the finite-sample bias of the 2SLS estimator toward the inconsistent OLS estimator, thereby reproducing the endogeneity problem that an IV design was meant to solve \citep{Staiger1997-lo}\footnote{The 2SLS estimator may not have a mean when the first stage is weak, its median is centered around the OLS coefficient \citep{hirano2015location}}. Second, the 2SLS estimates become very imprecise.%
\footnote{To illustrate, a commonly used variance estimator for
$\hat{\tau}_{IV}$ can be written as:
$\hat{\mathbb{V}}(\hat{\tau}_{IV})
\approx \frac{\hat{\Sigma}^2}{\sumin (x_i - \Ol{x})^2}
\frac{1}{R_{xz}^2} = \hat{\mathbb{V}}(\hat{\tau}_{OLS}) \frac{1}{R_{xz}^2}$
in which $\hat{\sigma}^2$ is a variance estimator for the error term and
$R^2_{xz}$ is the R-squared from the first stage. The estimated variance
is mechanically larger than the estimated variance of the OLS estimator
as long as $R^2_{xz} < 1$. It is decreasing in $R^2_{xz}$, i.e. stronger
instruments produce more precise IV estimates.} A third and related
issue is that the tests are of the wrong size and the $t$-statistics
don't follow a $t$-distribution \citep{nelson1990some}. Issues relating to imprecision and test-statistic size arise from the fact that the distribution of $\hat{\tau}$ is derived from its linear approximation of $\hat{\tau}$ in ($\hat{\gamma}, \hat{\pi}$), wherein normality of the two OLS coefficients implies the normality of their ratio. However, this normal approximation
breaks down when $\hat{\pi} \approx 0$. Moreover, this approximation
failure cannot generally be rectified by bootstrapping
\citep{andrews2009validity}, although \citet{Young2022} argues that it
nevertheless allows for improved inference when outliers are present. Overall, valid IV inference relies crucially on correctly identifying strong IVs.

In general, there are two approaches to conducting inference in an IV design: pretesting and direct testing. The pretesting approach involves using an $F$ statistic to test the first stage strength, and if it exceeds a certain threshold (e.g., $F > 10$), proceeding to test the null hypothesis about the treatment effect (e.g., $\tau = 0$). In contrast, the direct testing approach does not rely on passing a pretest. Nearly all reviewed studies employ the pretesting approach. We examine four methods for statistical inference in IV designs, with the first three related to pretesting and the last one being a direct test.


First, \citet{Olea2013-pa} propose the effective $F$ statistic for both just-identified and over-identified settings and accommodates robust or cluster-robust SEs. The effective $F$ is a scaled version of the first-stage $F$ statistic and is computed as $F_{\text{Eff}} = \hat{\pi}'\hat{Q}_{\text{ZZ}} \hat{\pi} / \text{tr}(\hat{\Sigma}_{\pi \pi} \hat{Q}_{\text{ZZ}})$, where $\hat{\Sigma}_{\pi \pi}$ is the variance-covariance matrix of the first stage regression, and $\hat{Q}_{\text{ZZ}} = \ooN \sumin z_i z_i'$. In just-identified cases, $F_{\text{Eff}}$ is the same as $F$ statistics based on robust or cluster-robust SEs. The authors derive the critical values for $F_{\text{Eff}}$ and note that the statistic and corresponding critical values are identical to the better-known robust $F$ statistic $\hat{\pi} \hat{\Sigma}_{\pi \pi}^{-1} \hat{\pi}$ and corresponding \citet{stock2005asymptotic} critical values.  $F_{\text{Eff}}>10$ is shown to be a reasonable rule of thumb under heteroskedasticity in simulations \citep{Olea2013-pa, andrews2019weak}.

Second, \citet{Young2022} recommends researchers report two types of bootstrap confidence intervals (CIs), \emph{bootstrap-c} and \emph{bootstrap-t}, for $\hat{\tau}_{2SLS}$ under non-i.i.d. errors with outliers, which is common in social science settings. This involves $B$ replications of the following procedure: (1) sample $n$ triplets $(y_i^*, d_i^*, \Mat{z}_i^*)$ independently and with replacement from the original sample (with appropriate modifications for clustered dependence) and (2) compute the
$\hat{\tau}_{\text{2SLS}}$ coefficient and SE, as well as the corresponding test statistic $t^* = \hat{\tau}^*_{\text{2SLS}} / \hat{\text{SE}} (\hat{\tau}^*_{\text{2SLS}})$ on each replication. The \emph{bootstrap-c} method calculates the CIs by taking the $\alpha/2$ and $(1-\alpha/2)$ percentiles of the bootstrapped 2SLS coefficient $\hat{\tau}_{\text{2SLS}}^*$, while the \emph{bootstrap-t} method calculates the percentile-$t$ refined CIs by plugging in the $\alpha/2$ and $(1-\alpha/2)$ percentile of bootstrapped $t$ statistics $t^*_{\alpha/2}$ and $t^*_{1-\alpha/2}$ into the expression $\hat{\tau}_{\text{2SLS}} \pm t^*_{\alpha \mid 1 - \alpha}
\hat{\text{SE}}(\hat{\tau}^*_{\text{2SLS}})$. \citet{hall1996bootstrap} show that \emph{bootstrap-t} achieves an asymptotic refinement over \emph{bootstrap-c}.%
\footnote{We use the percentile method instead of bootstrapped SEs because the $t$-test based on the latter may be overly conservative \citep{Hahn2021-lq}.}

Third, in just-identified single treatment settings, \citet{Lee2020-mi} propose the $tF$ procedure that smoothly adjusts the $t-$ratio inference based on the first-stage $F$ statistic, which improves upon the ad-hoc screening rule of $F > 10$. The adjustment factor applied to 2SLS SEs is based on the first stage $t-$ratio $\hat{f} \defeq \hat{\pi}/\sqrt{\hat{\mathbb{V}}(\hat{\pi})}$, with the first stage
$\hat{F} = \hat{f}^2$, and relies on the fact that the distortion from
employing the standard 2SLS $t$-ratio $\hat{t} \defeq \hat{\tau}/\sqrt{\hat{\mathbb{V}}(\hat{\tau})}$ can be quantified in terms of $\hat{F}$ and $AR-$statistic, which gives rise to a set of critical values for a given pair of $\hat{t}$ and $\hat{F}$. The authors also show that, if no adjustment is made to the $t$-test's critical value (e.g., using 1.96 as the threshold for 5\% statistical significance), a first stage $\hat{F}$ of 104.7 is required to guarantee a correct size of $5\%$ for a two-sided $t$-test for the 2SLS coefficient.

Finally, where there is one endogenous treatment variable, the AR procedure, which is essentially an $F$ test on the reduced form, is a direct inferential method robust to weak instruments \citep{anderson1949estimation, chernozhukov2008reduced}. Without loss of generality, assume that we are interested in testing the null
hypothesis that $\tau = 0$, which then implies that the reduced form
coefficient from regressing $y$ on $\Mat{z}$, $\gamma_1 = 0$. This
motivates the following procedure: given a set $\mathcal{T}$ of
potential values for $\wt{\tau}$, for each value $\wt{\tau}$,
construct $\wt{y} = y - d \wt{\tau}$, and regress $\wt{y}$ on
$\Mat{z}$ to obtain a point estimate $\wt{\gamma}$ and (robust, or
cluster robust) covariance matrix $\wt{\mathbb{V}}(\wt{\gamma})$, and
construct a Wald statistic $\wt{W}_s(\wt{\gamma}) \defeq \wt{\gamma}'
\wt{\mathbb{V}}(\wt{\gamma})^{-1} \wt{\gamma}$. Then, the AR
CI is the set of $\wt{\gamma}$ such that
$\wt{W}_s(\wt{\gamma}) \leq c(1-p)$ where $c(1-p)$ is the
$(1-p)^{\text{th}}$ percentile of the $\chi^2_1$ distribution. The AR test not only requires no pretesting but is also shown to be the uniformly most powerful unbiased test in the just-identified case \citep{moreira2009tests}. However, it is not as commonly used as procedures that involve pretesting, possibly because researchers are more accustomed to using $t$-tests than $F$/Wald tests and reporting SEs rather than CIs.


\paragraph*{Bias amplification and the failure of Assumption~\ref{assumption:exog}.} When the number of instruments is bigger than the number of endogenous treatments,
researchers can use an over-identification test to gauge the
plausibility of Assumption~\ref{assumption:exog} \citep{arellano2002sargan}. However, such a
test is often underpowered and has bad finite sample properties
\citep{davidson2015bootstrap}. In just-identified cases, Assumption~\ref{assumption:exog} is
not directly testable. When combined with weak instruments, even small violations of Assumption~\ref{assumption:exog} can produce inconsistency. This is because $\plim\hat{\tau}_{IV} = \tau
+ \frac{\Covar{z, \varepsilon}}{\Covar{z, d}}$. When $\Covar{z, d}
\approx 0$, even small violations of exogeneity, i.e., $\Covar{z, \varepsilon}
\neq 0$, will enlarge the second term, resulting in large biases. Thus,
the two identifying assumption failures exacerbate each other: having
weak instruments compounds problems from confounding or exclusion
restriction violations, and vice versa. With invalid instruments, it is
possible that the asymptotic bias of the 2SLS estimator is greater than
that of the OLS estimator, i.e., $\left|\frac{\Covar{z, \varepsilon}}{\Covar{z, d}}\right| \gg  \left|\frac{\Covar{d, \varepsilon}}{\Var{ d}}\right|$ in the single instrument case.


While the inference problem can be alleviated by employing alternative inferential methods as described above, addressing the failure of Assumption~\ref{assumption:exog} is more challenging since it is fundamentally a research design issue that should be tackled at the design stage. Researchers often devote significant effort to arguing for unconfoundedness and exclusion restrictions in their settings. In Section A3 of the SM, we provide an exposition of the ``zero-first-stage'' (ZFS) test \citep{bound2000compulsory}, which is essentially a placebo test on a subsample where the instrument is expected to be uncorrelated with the treatment, to help researchers gauge the validity of their instruments. These estimates can then be used to debias the 2SLS estimate using the methods proposed in \citet{Conley2012-mu}. 



\FloatBarrier

% subsection exclusion_restriction_violations (end)

% subsubsection weak_instruments (end)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ########  ########  ######   ######
% ##     ## ##       ##    ## ##    ##
% ##     ## ##       ##       ##
% ##     ## ######    ######  ##
% ##     ## ##             ## ##
% ##     ## ##       ##    ## ##    ##
% ########  ########  ######   ######

\section{Data and Types of Instruments} \label{desc}

In this section, we first discuss our case selection criteria and the
replication sample, which is the focus of our subsequent analysis. We
then describe the types of instruments in the replicable studies.

\paragraph*{Data.} We examine all empirical papers published in the
APSR, AJPS, and JOP from 2010 to 2022 and
identify studies that use an IV strategy as one of the main
identification strategies, including papers that use binary or
continuous treatments and that use a single or multiple instruments.
We use the following criteria: (1) the discussion of the
IV result needs to appear in the main text and support a main argument
in the paper; (2) we consider linear models only; in other words, papers that use discrete outcome models are excluded from our sample; (3) we exclude papers that include multiple endogenous variables in a single specification (multiple endogenous variables in separate specifications are included); (4) we exclude papers that use IV or GMM estimators in a dynamic panel setting because they are subject to a separate set of empirical issues and their poor performance has been thoroughly
discussed in the literature \citep{bun2010weak}. These criteria result
in 30 papers in the APSR, 33 papers in the AJPS, and 51 papers in the JOP. We then strive to find replication materials for these papers from public data-sharing platforms, such as the Harvard Dataverse, and the
authors' websites. We are able to locate complete replication materials for 76 (62\%) papers. However, code completeness and quality of documentation vary a great deal. Data availability has significantly improved since 2016-2017 following new editorial policies requiring authors to make replication materials publicly available, though none of the journals requires full
replicability administrated by a third party as a condition for
publication \citep{key2016we}, which would constitute a major
improvement in our view.

\begin{table}[htbp]
  \centering\small
  \caption{Data availability and replicability of IV papers.}
    \label{tb:sample}%
    \begin{tabular}{lccccc}\hline\hline
          \multicolumn{1}{p{5em}}{} & \multicolumn{1}{p{5em}}{} &
          \multicolumn{1}{p{5em}}{\centering Incomplete} & \multicolumn{1}{p{5em}}{\centering Incomplete} & \multicolumn{1}{p{5em}}{\centering Replication} & \multicolumn{1}{p{5em}}{}  \\
            & \#All Papers  & Data & Code & Error  & Replicable  \\ \hline
    APSR  & 30  & 16  & 0 & 3 & 14 (42\%) \\
    AJPS  & 33  & 3  & 1 & 1 & 25 (83\%) \\
    JOP   & 51  & 19 & 3  & 1 & 28 (55\%)\\ \hline
    Total & 114 & 38 & 4 & 5 & 67 (59\%) \\\hline
    \end{tabular}%
\end{table}%

Using data and code from the replication materials, we set out to
replicate the main IV results in the 67 papers. Our replicability
criterion is simple: As long as we can exactly replicate \emph{one} 2SLS point estimate that appears in the paper, we deem the paper replicable. We do not aim at exactly replicating SEs, $z$-scores, or level of statistical significance for the 2SLS estimates because they involve the choice of the inferential method, which we will discuss in the next section.

After much effort and hundreds of hours of work, we are able to
replicate the main results of 61 papers.\footnote{For three papers, we
are able to produce the 2SLS estimates with perfectly executable code;
however, our replicated estimates are inconsistent with what was
reported in the original studies. We suspect the inconsistencies are
caused by data rescaling or misreporting; hence, we keep them in the
sample.} The low replication rate is consistent with what is reported in \citet{Hainmueller2019-wx}. The main reasons for failures of replication are incomplete data (38 papers), incomplete code or poor documentation (4 papers), and replication errors (5 papers). Table~\ref{tb:sample} presents summary statistics on data availability and replicability of IV papers for each of the three journals. The rest of this paper focuses on results based on these 67 replicable papers (and 70 IV designs).


\paragraph*{Types of instruments.} Inspired by \citet{sovey2011instrumental}, in Table~\ref{tb:iv.type}, we summarize the types of IVs in the replicable designs, although our categories differ from theirs to reflect changes in the types of instruments used in the discipline. As in \citet{sovey2011instrumental}, the biggest category is ``Theory,'' in which the authors justify Assumption~\ref{assumption:exog}, including IVs' quasi-randomness and the exclusion restriction, using social science theories or substantive knowledge. We further divide theory-based IVs into four subcategories: geography/climate/weather, history, treatment diffusion,
and others.

Many studies in the theory category justify the choices of their instruments
based on geography, climate, or weather conditions. For example,
\citet{zhu2017} uses weighted geographic closeness as an instrument for the
activities of multinational corporations; \citet{hager2019} use
mean elevation and distance to rivers to instrument equitable
inheritance customs; and \citet{grossman_etal2017} use the number of
distinct landmasses as an instrument for government fragmentation.
\citet{henderson2016mediating} use rainfall around Election Day as an IV
for democratic vote margins. The popularity of weather instruments for a
whole host of outcomes necessarily implies that the exclusion
restriction is especially tenuous in such cases \citep{mellon2020rain}.

\begin{table}[htbp]
  \centering
  \caption{Types of Instruments} \label{tb:iv.type}%
    \begin{tabular}{lcc}\hline\hline
    Type & \multicolumn{1}{c}{\#Papers} & \multicolumn{1}{c}{Percentage \%} \\\hline
    \textbf{Theory}    & 42    & 60.0 \\
    $\qquad$ Geography/climate/weather & $\qquad$ 13     & $\qquad$ 18.6 \\
    $\qquad$ History & $\qquad$ 11    & $\qquad$ 15.7 \\
    $\qquad$ Treatment diffusion & $\qquad$ 2    & $\qquad$ 2.9 \\
    $\qquad$ Others & $\qquad$ 16 & $\qquad$ 22.9 \\
    \textbf{Experiment} & 12     & 17.1 \\
    \textbf{Econometrics} & 9     & 12.9 \\
    $\qquad$ Interactions/``Bartik'' & $\qquad$ 7     & $\qquad$ 10.0 \\
    $\qquad$ Lagged treatment & $\qquad$ 1     & $\qquad$ 1.4 \\
    $\qquad$ Empirical test & $\qquad$ 1     & $\qquad$ 1.4 \\
    \textbf{Rules \&  policy changes}  & 7     & 10.0 \\
    $\qquad$ Change in exposure & $\qquad$ 3     & $\qquad$ 4.3 \\
    $\qquad$ Fuzzy RD & $\qquad$ 4     & $\qquad$ 5.7 \\ \hline
    \textbf{Total} & 70    & 100.0 \\\hline
    \end{tabular}%
\end{table}%

Historical instruments are based on historical differences between units that
cannot be explained by current levels of the treatment. For example,
\citet{vernby2013} uses historical immigration levels as an instrument for the
current number of non-citizen residents. Similarly,
\citet{spenkuch_tillmann2018} use historical decisions by rulers in
Europe over the religion of their region to instrument for the current
religion of survey respondents. These studies use historical variation
as instruments for current or modern variables.

Several studies base their choices on regional diffusion of treatment.
For example, \citet{dube2015} use US military aid to countries outside
Latin America as an instrument for US military aid to Colombia.
\citet{grossman2017} use over-time variation in the number of regional
governments to instrument government fragmentation in sub-Saharan
Africa. \citet{dorsch_maarek2019} use the regional share of democracies as an instrument for democratization in a country-year panel.

Finally, several papers rely on a unique instrument based on theories that we could not place in a category. For example, \citet{carnegie_marinov2017}
use the rotating presidency of the Council of the European Union as an
instrument for official development aid. They argue that countries that were
colonized by the country that holds the presidency receive exogenously
more aid than other countries. \citet{dower_etal2018} use religious polarization as an instrument for the frequency of unrest and argue that religious polarization could only impact collective action through its impact on representation in local institutions. 

The second-biggest category is randomized experiments. Articles in this
category employ randomization, designed and conducted by researchers or
a third party, to make causal inference and use 2SLS estimation to address
non-compliance issues in an encouragement design---the IV normally is
being encouraged to take the treatment. With random assignment, we have
more confidence in Assumption~\ref{assumption:exog} because $z\indep v$ by design, and the direct effect of encouragement on the outcome is easier to rule out than without random assignment.


Another category of instruments are based on explicit rules, which generate
quasi-random variation in the treatment. \citet{sovey2011instrumental}
refer to this category as ``Natural Experiment.'' We avoid this
terminology because it is widely misused. We limit this category to two
circumstances: fuzzy regression discontinuity (RD) designs and variation
in exposure to policies due to time of birth or
eligibility.\footnote{The difference between the two is subtle: For the
latter, the gap in the forcing variable, such as birth cohort, is fixed
and cannot be arbitrarily small.} For example, \citet{kim2019} leverages
a reform in Sweden that requires municipalities above a population
threshold to adopt direct democratic institutions. \citet{dinas2014}
uses eligibility to vote based on age at the time of an election as an
instrument for whether respondents did vote. 
% Respondents who were 18 as of election day could vote while those who were 17 could not, making this a problem of one-sided non-compliance, as some respondents who were over 18 did not vote.

The last category of instruments are based on econometric assumptions. This
category includes what \citet{sovey2011instrumental} call ``Lags.'' These are econometric transformations of variables argued to constitute instruments. For example, \citet{lorentzen_etal2014} use a measure of the
independent variable from 8 years earlier to mitigate endogeneity
concerns. Another example is instruments relying on variable transformations to satisfy assumptions, such as Shift-share ``Bartik" instruments based on interactions between multiple variables. For example, \citet{baccini2021} use the share of jobs in a specific industry within a county, interacted with national-level changes in employment in that industry, to study the effect of manufacturing layoffs on voting.

Compared to IV papers published before 2010, there is a significant increase in the proportion of papers using experiment-generated IVs (from 2.9\% to 17.1\%) due to the growing popularity of survey and field experiments. In contrast, the number of papers relying on econometric techniques or flawed empirical tests (such as regressing $y$ on $d$ and $z$ and checking if the coefficient of $z$ is significant) has decreased, thanks to improving empirical practices in the discipline. The percentage of papers using theory-justified instruments remains nearly the same at around 60\%.


\section{Replication Procedure and Results} \label{sec:repl}

In this section, we describe our replication procedure and report the main findings.

\paragraph*{Procedure.} For each paper, we select the main IV
specification that plays a central role in supporting a main claim in
the paper; it is either referred to as the baseline specification or
appears in one of the main tables or figures.  Focusing on this
specification, our replication procedure involves the following steps.
First, we compute the first-stage partial $F$ statistics based on (1)
classic analytic SEs, (2) Huber White heteroskedastic-robust SEs, (3) cluster-robust SEs (if applicable and based on the original specifications), and (4) bootstrapped SEs.%
\footnote{They are calculated by $F_{boot} =
\hat\tau_{2SLS}'{\hat{\rm Var}_{boot}(\hat\tau_{2SLS})}^{-1}\hat\tau_{2SLS}/p_{z}$, where $p_{z}$
is the number of IVs and $\hat{\rm Var}_{boot}(\hat\tau_{2SLS})$ is the
estimated variance-covariance matrix based on a nonparametric bootstrap
procedure, in which we repeatedly sample the rows of the data matrix
with replacement. If the data have a clustered structure, we use cluster-bootstrapping instead by sampling with replacement each cluster of data \citep{Colin_Cameron2015-wp,Esarey2019-qt}. We include $F_{boot}$ as a reference to the classic $F$ and effective $F$. In Section A.2 of the SM, we compare the five types of $F$ statistics and show that the effective $F$ and $F$ based on bootstrapping are usually more conservative than other $F$ statistics.} %
We also calculate $F_{\texttt{Eff}}$. 

We then replicate the original IV result using the 2SLS estimator and apply four different inferential procedures. First, we make inferences based on analytic SEs, including robust SEs or cluster-robust SEs (if applicable). Additionally, we use two nonparametric bootstrap procedures, as described in Section 2, \emph{bootstrap-c} and \emph{bootstrap-t}. For specifications with only a single instrument, we also employ the $tF$ procedure proposed by \citet{Lee2020-mi}, using 2SLS $t$-statistics and first-stage $F$-statistics based on analytic SEs accounting for the originally specified clustering structure. Finally, we conduct an AR procedure and record the $p$-values and CIs.

We record the point estimates, SEs (if applicable), 95\% CIs, and $p$-values for each procedure (the point estimates fully replicate the reported estimates in the original papers and are the same across all procedures). In addition, we estimate a naïve OLS model by regressing the outcome variable on the treatment and control variables, leaving out the instrument. We calculate the ratio between the magnitudes of the 2SLS and OLS estimates. We also record other useful information, such as the number of observations, the number of clusters, the types of instruments, the methods used to calculate SEs or CIs, and the rationale for each paper's IV strategy. Our replication yields the following three main findings.


\paragraph*{Finding 1. First-stage partial $F$ statistics.} Our first finding regards the strengths of the instruments. To our surprise, among the 70 IV designs, 12 (17\%) do not report this crucial statistic despite its key role in justifying the validity of an IV design. Among the remaining 58 studies that report $F$ statistics, 9 (16\%) use classic analytic SEs, thus not adjusting for potential heteroskedasticity or clustering structure. In Figure~\ref{fig:fstat}, we plot the replicated first-stage partial $F$ statistics based on the authors' original model specifications and choices of variance estimators on the x-axis against effective $F$ statistics (a) or bootstrapped $F$ statistics (b) on the y-axis. Both axes are on a logarithmic scale.\footnote{We use the replicated $F$ statistics instead of the reported ones because some authors either do not report or misreport their $F$ statistics (see SM for a comparison between the reported and replicated $F$ statistics).}


\begin{figure}[!h]
\caption{Original vs. Effective and Bootstrapped $F$} \label{fig:fstat}
\centering
\begin{minipage}{1\linewidth}{
\begin{center}
\hspace{-2em}
\subfigure[Original $F$ vs. Effective $F$]{\includegraphics[width=0.42\textwidth]{graphs/f_rep_effective.pdf}}\hspace{1em}
\subfigure[Original $F$ vs. Bootstrapped $F$]{\includegraphics[width=0.42\textwidth]{graphs/f_rep_boot.pdf}}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Circles and triangles represent applications with and without a clustering structure, respectively. Studies that do not report $F$ statistics are painted in red. The original $F$ statistics are replicated based on the authors' original model specifications and choices of variance estimators in the 2SLS regressions. They may differ from those reported in the papers because of misreporting.}}
\end{minipage}\vspace{-0.5em}
\end{figure}

In the original studies, the authors used various SE estimators, such as classic SEs, robust SEs, or cluster-robust SEs. As a result, the effective $F$ may be larger or smaller than the original ones. However, a notable feature of Figure~\ref{fig:fstat} is that when a clustering structure exists, the original $F$ statistics tend to be larger than the effective $F$ or bootstrapped $F$. When using the effective $F$ as the benchmark, 8 studies (11\%) have $F_{\texttt{Eff}}<10$. This number increases to 12 (17\%) when the bootstrapped $F$ statistics are used. The median first-stage $F_{\texttt{Eff}}$ statistic is higher in experimental studies compared to non-experimental ones (67.7 versus 53.5). It is well known that failing to cluster the SEs at appropriate levels or using the analytic cluster-robust SE with too few clusters can lead to a severe overstatement of statistical significance \citep{cameron2008bootstrap}. However, this problem has received less attention when evaluating IV strength using $F$ statistics.

\paragraph*{Finding 2. Inference.} Next, we compare the reported and replicated $p$-values for the null hypothesis of no effect. For studies that do not report a $p$-value, we calculate it based on a standard normal distribution using the reported point estimates and SEs. The replicated $p$-values are based on (1) \emph{bootstrap-c}, (2) \emph{bootstrap-t}, and (3) the AR procedure. Since we can exactly replicate the point estimates for the papers in the replication sample, the differences in $p$-values are the result of the inferential methods used. Figure~\ref{fig:inference}(a)-(c) plot reported and replicated $p$-values, from which we observed two patterns. First, most of the reported $p$-values are smaller than 0.05 or 0.10, the conventional thresholds for statistical significance. Second, consistent with \citet{Young2022}'s finding, our replicated $p$-values based on the AR procedure or bootstrap methods are usually bigger than the reported $p$-value (exceptions are mostly caused by rounding errors), which are primarily based on $t$ statistics calculated using analytic SEs. Using the AR test, we cannot reject the null hypothesis of no effect at the 5\% level in 13 studies (19\%), compared with 7 (10\%) in the original studies. The number increases to 15 (21\%) and 20 (29\%) when we use $p$-values from the \emph{bootstrap-t} and \emph{-c} methods. Note that very few papers we review utilize inferential procedures specifically designed for weak instruments, such as the AR test (2 papers), the conditional likelihood-ratio test \citep{Moreira2003-oj} (1 paper), and confident sets \citep{Mikusheva2006-lk} (none).

We also apply the $tF$ procedure to 54 studies that use single IVs using $F_{\texttt{Eff}}$ statistics and $t$ statistics based on robust or cluster-robust SEs. Figures~\ref{fig:inference}(d) shows that 16 studies (30\%) are not statistically significant at the 5\% level, and 5 studies deemed statistically significant when using the conventional fixed critical values for the $t$-test become statistically insignificant using the $tF$ procedure, indicating that overly optimistic critical values due to weak instruments also contribute to overestimation of statistical power, but not as the primary factor. These results suggest that both weak instruments and non-i.i.d. errors have contributed to severe overstatements of power in IV studies in political science.


\begin{figure}[!ht]
\caption{Alternative inferential methods.}\label{fig:inference}
\begin{center}\vspace{-1.5em}
\subfigure[Bootstrap-c method]{\hspace{-1em}\includegraphics[width=0.4\textwidth]{graphs/p_bootc.pdf}}\hspace{0em}
\subfigure[Bootstrap-t method]{\hspace{0em}\includegraphics[width=0.4\textwidth]{graphs/p_boott.pdf}}\\
\subfigure[Anderson-Rubin]{\hspace{0em}\includegraphics[width=0.4\textwidth]{graphs/p_AR.pdf}}
\subfigure[$tF$ procedure]{\hspace{0em}\includegraphics[width=0.4\textwidth]{graphs/tf_cF.pdf}}
\end{center}\vspace{-0.5em}
{\footnotesize\textbf{\textit{Note:}} In subfigures (a)-(c), we compare original $p$-values to those from alternative inferential methods, testing against the null that $\tau = 0$. Both axes use a square-root scale. Original $p$-values are adapted from original papers or calculated using standard-normal approximations of $z$-scores. Solid circles represent \citet{arias2019large}, where authors argue for a null effect using IV strategy. \emph{Bootstrap-c} and \emph{-t} represent percentile methods based on 2SLS estimates and $t$-statistics, respectively, using original model specifications. Hollow triangles in subfigure (c) indicate unbounded 95\% CIs from the AR test using the inversion method. Subfigure (d) presents $tF$ procedure results from 54 single instrument designs. Green and red dots represent studies remaining statistically significant at the 5\% level using the $tF$ procedure and those that don't, respectively. Subfigures (a)-(c) are inspired by Figure 3 in \citet{Young2022}, and subfigure (d) by Figure 3 in \citet{Lee2020-mi}.}    
\end{figure}

\paragraph*{Finding 3. 2SLS-OLS discrepancy.} Finally, we investigate the relationship between the 2SLS estimates and naïve OLS estimates. In Figure~\ref{fig:ratio}(a), we plot the 2SLS coefficients against the OLS coefficients, both normalized using reported OLS SEs. The shaded area indicates the range beyond which the OLS estimates are statistically significant at the 5\% level. It shows that for most studies in our sample, the 2SLS estimates and OLS estimates share the same direction and that the magnitudes of the former are often much larger than those of the latter. Figure~\ref{fig:ratio}(b) plots the distribution of the ratio between the 2SLS and OLS estimates (in absolute terms). The mean and median of the absolute ratios are 12.4 and 3.4, respectively. In fact, in all but two designs (97\%), the 2SLS estimates are bigger than the OLS estimates, consistent with \citet{jiang2017have}'s finding based on finance research. While it is theoretically possible for most OLS estimates in our sample to be biased towards zero, only 21\% of the studies have researchers expressing their belief in downward biases of the OLS estimates. Meanwhile, 40\% of the studies consider the OLS results to be their main findings. The fact that researchers use IV designs as robustness checks for OLS estimates due to concerns of upward biases is apparently at odds with the significantly larger magnitudes of the 2SLS estimates.

\begin{figure}[!ht]
\caption{Relationship between OLS and 2SLS Estimates}\label{fig:ratio}
\begin{center}\vspace{-1.5em}
  \subfigure[OLS vs 2SLS Estimates]
  {\includegraphics[width=0.42\textwidth]{graphs/coefs_iv_ols.pdf}}\hspace{2em}
  \subfigure[Distribution of Ratio]
  {\includegraphics[width=0.42\textwidth]{graphs/hist_ratio.pdf}}\\
  \subfigure[\scriptsize First Stage and the Ratio: Full Sample]{\includegraphics[width=0.42\textwidth]{graphs/ratio_exp.pdf}}\hspace{1em}
   \subfigure[\scriptsize First Stage and Ratio: Subsample]{\includegraphics[width=0.42\textwidth]{graphs/ratio_ols_signif.pdf}}
\end{center}
{\footnotesize\textbf{\textit{Note:}} Subfigures (a) and (b) use reported 2SLS and OLS coefficient estimates. In subfigure (a), both axes are normalized by reported OLS SE estimates with the gray band representing the $[-1.96, 1.96]$ interval. Subfigures (c) and (d) feature the relationship between the correlational coefficient between $d$ and $\hat{d}$ and the ratio of 2SLS and OLS estimates. Gray and red circles represent observational and experimental studies, respectively. Subfigure (d) highlights studies with statistically significant OLS results at the 5\% level, claimed as part of the main findings.}
\end{figure}

In Figure~\ref{fig:ratio}(c), we further explore whether the 2SLS-OLS discrepancy is related to IV strength, measured by $\hat\rho(d, \hat{d})$, the estimated correlation coefficient between the treatment and predicted treatment. We find a strong negative correlation between
$|\hat{\tau}_{2SLS} / \hat{\tau}_{OLS}|$ and $|\hat\rho
(d,\hat{d})|$ among studies using non-experimental instruments (grey dots). The adjusted $R^2$ is $0.268$, with $p = 0.000$. However, the relationship is much weaker among studies using experiment-generated instruments (red dots). The adjusted $R^2$ is $-0.014$ with $p = 0.378$. In Figure~\ref{fig:ratio}(d), we limit our focus to the subsample in which the OLS estimates are statistically significant at the 5\% level and researchers accept them as (part of) the main findings, and the strong negative correlation remains.
At first glance, this result may seem mechanical: as the correlation between $d$ and $\hat{d}$ increases, the 2SLS estimates naturally converge to the OLS estimates. However, the properties of the 2SLS estimator under the identifying
assumptions do not predict the negative relationship (we confirm it in simulations in the SM), and such a relationship is not found in experimental studies.

We believe that several factors contribute to this pattern, including (1) the failure of Assumption~\ref{assumption:exog}, (2) publication bias, (3) HTE, and (4) measurement error in $d$. We suspect the first two factors are the main driving forces. As previously mentioned, when Assumption~\ref{assumption:exog} is violated, weak instruments amplify the biases from endogenous IVs or exclusion restriction failures, i.e., $\frac{\text{Bias}_{IV}}{\text{Bias}_{OLS}} = \frac{\Covar{z, \varepsilon}\Var{d}}{\Covar{z, d} \Covar{d, \varepsilon}} \gg 1$. Publication bias may also play a role. When the first stage is weak, IV estimates have a larger variance and can be very large or very small in magnitude compared to OLS estimates. If researchers selectively report statistically significant results or journals tend to publish papers with statistically significant findings, we may observe a negative relationship as in Figure~\ref{fig:ratio}. This phenomenon is also referred to as Type-M bias in the psychology and sociology literature \citep{gelman2014beyond, FeltonStewart2022}.

Moreover, 30\% of the replicated studies in our sample mention HTE as a possible explanation for this discrepancy. OLS and 2SLS place different weights on covariate strata in the sample, and therefore if compliers, those whose treatment status is affected by the instrument, are more responsive to the treatment than the rest of the units in the sample, we might see diverging OLS and 2SLS estimates. Under the assumption that the exclusion restriction holds, this gap can be decomposed into covariate weight difference, treatment-level weight difference, and endogeneity bias components using the procedure developed in \citep{Ishimaru2021-ik}. In the SM, we investigate this possibility and find that it is highly unlikely that HTE \emph{alone} can explain the difference in magnitudes between 2SLS and OLS estimates we observe in the replication data, i.e.,  the variance in treatment effects needed for this gap is implausibly large.

Finally, an IV design can correct for the downward bias of the
measurement error in $d$, resulting in $|\hat{\tau}_{2SLS} / \hat{\tau}_{OLS}| > 1$. If the measurement error is large, this can weaken the relationship between $d$ and $\hat{d}$, producing a negative correlation. However, it is worth noting that only 4 papers in our sample (6\%) attribute the IV strategy to measurement error; the negative correlation remains even when the OLS estimates are the main findings (indicating measurement error may not be as concerning for researchers).

We summarize the main findings from our replication exercise in Table~\ref{tb:des}. The three issues we have identified are observed in all three journals included in the study. Based on these results, we believe that a significant portion of the IV results either lack credibility or do not provide new information beyond what is already provided by OLS regressions.


\begin{table}[!ht]
  \centering\small
  \caption{Summary of Replication Results}
  \label{tb:des}%
    \begin{tabular}{p{15em}cccc}\hline\hline
    (\%) & APSR (15) & AJPS (25) & JOP (30) & All (70) \\ \hline
    \emph{Panel A: First-Stage $F$ Statistic}  & \\
    $\quad$Unreported  &  0.0     &  20.0  & 23.3 &  17.1\\
    $\quad$Reported $F >$ 1.3 effective $F$  &  20.0  & 25.0 &  30.4  & 25.9 \\
    $\quad$Effective $F < 10$  &  13.3 &  12.0 & 10.0  &  11.4 \\
    $\quad$Bootstrapped $F < 10$  &  13.3 &  20.0 & 16.7  &  17.1 \\ \\
    \emph{Panel B: Inference for IV Designs}  & \\
    $\quad$Original $p > 0.05$ &  20.0  & 8.0 &  6.7   & 10.0 \\
    $\quad$AR $p > 0.05$  &  13.3   & 24.0    &   16.7  & 18.6 \\
    $\quad$Bootstrap-c $p > 0.05$  &  20.0   & 32.0    &   30.0  & 28.6 \\
    $\quad$Bootstrap-t $p > 0.05$ &    26.7   & 24.0   &   16.7    & 21.4 \\
    $\quad$$tF$ procedure $p > 0.05$  &  38.5  & 23.6  & 29.2  & 29.6  \\ \\
    \emph{Panel C: 2SLS-OLS Relationship} & \\
    $\quad$ $sign(\hat\tau_{2SLS}) = sign(\hat\tau_{OLS})$  &  93.3 &  100.0 &  86.7  & 92.9 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 1$  &  93.3     &  100.0  &  96.7  & 97.1 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 3$  &  53.3   &  44.0  &  60.0  & 52.9 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 5$  &  40.0  &    32.0  & 33.3  & 34.3 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 10$  &  13.3   & 16.0  &  20.0 & 17.1 \\
    \hline
    \end{tabular}%
\end{table}




%\vspace{-1em}%

%\FloatBarrier



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recommendations} \label{conc}

% In this paper, we replicate 70 IV designs in 67 studies published in three top journals in political science that employ IVs as one of the main identification strategies. We make two main contributions to the
% literature. First, we show that many political science studies overestimate the strength of the IVs and underestimate the uncertainty of the 2SLS estimates. Specifically, we find that close to 19-30\% of designs are underpowered according to current best practice, and underestimation of uncertainties seem to be related to non-i.i.d. error processes in data, such as heteroscedasticity and clustering structures. Second, we observe significant disparities between 2SLS and OLS estimates, sometimes reaching magnitudes of tens or hundreds. We believe that this is primarily driven by a combination of weak instruments and failure of exogeneity, as well as publication bias, although other mechanisms such as HTE and measurement error may also contribute. Our findings suggest that 2SLS estimates in many existing political science studies, especially observational ones, likely have larger biases than naïve OLS estimates.
% Finally, we provide researchers with practical recommendations, including a ZFS test and a LTZ procedure, to avoid these pitfalls as much as possible.


% We find that researchers often overestimate the strength of their IVs and underestimate the uncertainties around the 2SLS estimates. When using a bootstrap procedure to obtain the uncertainties, we find many 2SLS estimates become uninformative---they are often statistically indistinguishable from the naïve OLS estimates and often times 0. Moreover, we show that the 2SLS estimates are often much larger in magnitude than the OLS estimates, and their difference is negatively correlated with the strength of the IVs. We argue that this is because weak instruments amplify the biases from exclusion restriction failures.

IV designs in experimental and observational studies differ fundamentally. In randomized experiments, the instruments' unconfoundedness is ensured by design, and researchers can address potential exclusion restriction failures at the design stage, e.g., by testing potential design effects through randomization \citep[pp. 140-141]{Gerber2012-fr}. Practices like power analysis, placebo tests, and preregistration in experimental studies also help reduce improper use of IV designs. In contrast, analyzing observational IV design based on ``natural experiments'' requires detailed knowledge of the assignment mechanism, making them more complex and prone to potential issues \citep{sekhon2012natural}.

% How do we avoid these pitfalls when using an IV approach?
Our findings suggest that using an IV strategy in an observational setting is much more challenging. Since unconfoundedness is not guaranteed by design, researchers have a greater burden of proof for the validity of IVs. On the one hand, truly random (and strong) instruments are rare; on the other hand, it is difficult to conduct placebo tests, such as the ZFS test, for the exclusion restriction after data collection. Additionally, researchers often cannot easily increase the sample size to obtain sufficient statistical power. To prevent misusing IVs in observational studies, we provide a checklist for researchers to consider when applying or considering applying an IV strategy with observational data (in the case of one endogenous treatment variable):
\begin{itemize}[leftmargin=*]\itemsep0em
    \item[] \textbf{Design}
    \item Prior to using an IV strategy, consider how selection bias may be affecting treatment effect estimates obtained through OLS. If the main concern is underestimating an already statistically significant treatment effect, an IV strategy may not be necessary. 
    \item During the research design phase, consider whether the chosen instrument can realistically create random or quasi-random variations in treatment assignment while remaining excluded from the outcome equation.
    \item[] \textbf{Characterizing the first-stage}
    \item Calculate and report $F_{\texttt{Eff}}$ for the first stage, taking into account heteroscedasticity and clustering structure as needed. However, do not discard a design simply because $F_{\texttt{Eff}}<0$.
    \item If $d$ and $z$ are continuous, plot $d$ against its predicted values $\hat{d}$ (with covariates and fixed effects already partialled out from both) and visually verify whether their relationship aligns with theoretical expectations.
    \item[] \textbf{Hypothesis testing and inference}
    \item \textit{Option 1. $t$-test with $F_{\texttt{Eff}}$ pretesting.} If $F_{\texttt{Eff}} < 10$, choose Options 2 or 3. Utilize conservative methods like \emph{bootstrap-t} and \emph{bootstrap-c} if outliers or group structures are present.
    \item \textit{Option 2. $tF$ procedure.} For single treatment and instrument cases, adjust $t$-test critical values based on $F_{\texttt{Eff}}$.
    \item \textit{Option 3. Direct testing.} Apply weak-instrument-robust procedures, such as the AR test.
    \item[] \textbf{Communicating your findings}
    \item Present OLS and IV estimates alongside CIs from various inferential methods in a graphical format, like in Figure~\ref{fig:estimates}. These CIs may not concur on statistical significance, but they collectively convey the findings' robustness to different inferential approaches. 
    \item Remember to report first-stage and reduced-form estimation results, including 95\% CIs for coefficients, as they offer insight into both instrument strength and statistical power.
    \item[] \textbf{Additional diagnostics}
    \item If you expect the OLS results to be upward biased, be concerned if the 2SLS estimator yields much larger estimates.
    \item If there is good reason to believe that treatment effects on compliers are significantly larger in magnitude than those on non-compliers, explain this through profiling of these principal strata \citep{Abadie2003-sl,Marbach2020-ts}.
    \item If it is possible to identify an observational analogue of ``never takers'' or a subset of them, conduct a placebo test by estimating the effect of the instrument on the outcome of interest in this ZFS sample. Using results from the ZFS test, obtain local-to-zero IV estimates and CIs and compare them to the original estimates and CIs.  See the SM for a detailed example.
\end{itemize}\medskip

\begin{figure}[!ht]
    \centering
    \caption{Replicated OLS and 2SLS estimates with 95\% CIs\\ \citep[][Table 2 column 1]{mcclendon2014}}\label{fig:estimates}
    \vspace{-0.5em}
\begin{minipage}{1\linewidth}{
        \begin{center}
        \includegraphics[width=0.7\textwidth]{graphs/McClendon2014.pdf}\hspace{0em}
        \end{center}\vspace{-1.5em}
{\footnotesize\textbf{\textit{Note:}} The treatment is reading an email with a promise of social esteem. The instrument is being encouraged to take the treatment. The outcome is attending LGBTQ events.  The AR test does not rely on the first-stage $F$. Similar figures for each of the 70 IV designs are shown in the SM. This plot is made by \href{https://github.com/apoorvalal/ivDiag}{ivDiag}.}}
\end{minipage}\vspace{-0.5em}
\end{figure}

We provide an accompanying \texttt{R} package, \href{https://github.com/apoorvalal/ivDiag}{ivDiag}, to implement our recommended procedures. Our aim is to address concerns regarding IVs in social science research and improve the quality of estimation and inference, especially for non-experimental IV designs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \pagebreak
\vspace{3em}
\clearpage

\setstretch{1.5}
\bibliographystyle{apsr}
\bibliography{main}

%    ###    ########  ########  ########  ##     ##
%   ## ##   ##     ## ##     ## ##     ##  ##   ##
%  ##   ##  ##     ## ##     ## ##     ##   ## ##
% ##     ## ########  ########  ##     ##    ###
% ######### ##        ##        ##     ##   ## ##
% ##     ## ##        ##        ##     ##  ##   ##
% ##     ## ##        ##        ########  ##     ##


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
