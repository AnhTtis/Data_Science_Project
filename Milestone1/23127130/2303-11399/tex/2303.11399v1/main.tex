\documentclass[12pt]{article} % add titlepage param for separate title page

\input{preamble}

\title{\bf\Large How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies%
   \thanks{Apoorva Lal, PhD Candidate, Stanford University; Email: \url{apoorval@stanford.edu}. Mac Lockhart, PhD Candidate, University of California, San Diego; Email: \url{mwlockha@ucsd.edu}. Yiqing Xu (corresponding author), Assistant Professor, Stanford University; Email: \url{yiqingxu@stanford.edu} Ziwen Zu, PhD Student, University of California, San Diego; Email: \url{zzu@ucsd.edu}.
   We thank Te Bao, Daniel Chen, Gary Cox, Charles Crabtree, Ted Enamorado, Hanming Fang, Avi Feller, Don Green, Justin Grimmer, Anna Grzymala-Busse, Jens Hainmueller, David Laitin, Justin McCrary, Jacob Montgomery, Doug Rivers, Henrik Sigstad, Brandon Stewart, and seminar participants at Stanford University, Washington University in St. Louis, APSA 2021, and Polmeth 2021 for extremely valuable comments.
   }
}

\author{Apoorva Lal (Stanford) \and Mac Lockhart (UCSD)
\and Yiqing Xu (Stanford) \and Ziwen Zu (UCSD)}

\date{\bigskip\normalsize
      First Version: July 10, 2021\\
      This Version: \today
   \\
   %Word Count: 10,627 \\
   \vspace{-2em}
}

\begin{document}

% \input{note}

\thispagestyle{empty}
\clearpage

\maketitle

\thispagestyle{empty}
\setcounter{page}{0}
\thispagestyle{empty}

\begin{abstract}\vspace{-0.5em}\noindent
Instrumental variable (IV) strategies are widely used in political science to establish causal relationships. However, the identifying assumptions required by an IV design are demanding, and it remains challenging for researchers to assess their validity. In this paper, we replicate 67 papers published in three top journals in political science during 2010-2022 and identify several troubling patterns. First, researchers often overestimate the strength of their IVs due to non-i.i.d. errors, such as a clustering structure. Second, the most commonly used $t$-test for the two-stage-least-squares (2SLS) estimates often severely underestimates uncertainty. Using more robust inferential methods, we find that around 19-30\% of the 2SLS estimates in our sample are underpowered. Third, in the majority of the replicated studies, the 2SLS estimates are much larger than the ordinary-least-squares estimates, and their ratio is negatively correlated with the strength of the IVs in studies where the IVs are not experimentally generated, suggesting potential violations of unconfoundedness or the exclusion restriction. To help researchers avoid these pitfalls, we provide a checklist for better practice.\\

\noindent\textbf{Keywords:} instrumental variables,
two-stage-least-squared, replications, weak instrument, exclusion
restriction, replication

\end{abstract}

\setcounter{page}{0}
\thispagestyle{empty}

% \clearpage
% \tableofcontents
% \vspace{5mm}
% \listoftodos

% #### ##    ## ######## ########   #######
%  ##  ###   ##    ##    ##     ## ##     ##
%  ##  ####  ##    ##    ##     ## ##     ##
%  ##  ## ## ##    ##    ########  ##     ##
%  ##  ##  ####    ##    ##   ##   ##     ##
%  ##  ##   ###    ##    ##    ##  ##     ##
% #### ##    ##    ##    ##     ##  #######


% \linenumbers
% \modulolinenumbers[5]

\setcounter{page}{0}
\thispagestyle{empty}

\clearpage
\doublespacing



\section{Introduction} \label{intro}



The instrumental variable (IV) approach is a commonly used empirical
method in the social sciences, including political science, for
establishing causal relationships. It is often used when selection on
observables is implausible, experimentation is infeasible or
unethical, and rule-based assignments that allow for sharp
regression-discontinuity (RD) designs are not available. In recent years,
there has been a growing number of papers published in top political
science journals, such as the \emph{American Political Science Review}
(APSR),  \emph{American Journal of Political Science} (AJPS), and
\emph{Journal of Politics} (JOP), that use IV as a primary causal
identification strategy. This trend can be traced back to the
publication of the textbook \emph{Mostly Harmless Econometrics}
\citep{angrist2008mostly}, which popularized the modern interpretation
of IV designs, and \cite{sovey2011instrumental}, which clarifies the
assumptions required by an IV approach and provides a useful checklist
for political scientists.

\begin{figure}[!h]
\caption{IV Papers Published in the \emph{APSR}, \emph{AJPS}, and \emph{JOP}}\label{fig:pubs.year}\centering
\begin{minipage}{0.9\linewidth}{
\begin{center}
\hspace{2em}\includegraphics[width=1\textwidth]{graphs/summary.pdf}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Our criteria rule out IV models appearing in the online appendix only, in dynamic panel settings, with multiple endogenous variables, and with nonlinear link functions. Non-replicability is primarily due to a lack of data and/or coding errors.}}
\end{minipage}
\end{figure}

Despite its popularity, some researchers have questioned the validity of
the IV approach based on the observation that  two-stage least-squares
(2SLS) estimates are often much larger in magnitude than ``naïve''
ordinary-least-squares (OLS) estimates, even when the main concern with
the latter is upward omitted-variables bias.\footnote{For example, in
the 2016 National Bureau of Economic Research--Political Economy
Meeting, following a presentation of a study using an IV approach, the
late political economist Alberto Alesina asked the audience: ``How come
2SLS estimates are always five times bigger than OLS estimates in
political economy?''} Others have raised concerns about the validity of
inferential methods used for 2SLS estimation \citep[e.g.][]{Lee2020-mi,
Young2022}.


% , which points towards potential violations of the identifying assumptions \citep{hahn2005estimation}

% In this paper, we document patterns in the findings of IV designs in political science in the last decade, evaluate the plausibility of the assumptions that underpin their validity, and provide practical advice for researchers seeking to use IV designs.

These observations motivate our systematic examination of the use of IVs
in the empirical political science literature. We set out to replicate
all papers published in the APSR, AJPS, and JOP during the past thirteen
years (2010-2022) that use an IV design with a single endogenous
variable as one of the main identification strategies.\footnote{Focusing
on design with a single endogenous variable allows us to calculate the
correlation coefficient between the treatment and the predicted
treatment and apply powerful tools such as the Anderson-Rubin (AR) test
and the $tF$-$cF$ test (when there is only one IV). Moreover, we find it
difficult to justify the exclusion restriction in a
multiple-treatment-multiple-instrument setting in the first place.} We
start with the universe of 114 papers that meet this criterion, of which
76 have complete replication materials online, a finding that is a
troubling pattern in its own right. Among the 71 papers, we successfully
replicate at least one of the main IV results for 67 papers. Among
those, three papers each have two separate IV designs, producing
separate 2SLS results.

Using data from these 70 IV designs, we conduct a programmatic
replication exercise and find three troubling patterns. First, we find
that a significant number of IV designs in political science either do
not report the first-stage partial $F$ statistic or underestimate it by
failing to adjust standard errors (SEs) for factors such as
heteroskedasticity, serial correlation, or clustering structure. Using
the effective $F$-statistic \citep{Olea2013-pa}, we find that at least
11\% of the published IV studies rely on what econometricians call
``weak instruments,'' the consequences of which have been
well-documented in the literature under the assumption that the
instrument is valid.\footnote{See \citet{andrews2019weak} for a
comprehensive review.}

A second related issue we examine is statistical inference. We find that
a considerable number of the IV designs we replicate are underpowered.
Most studies rely on $t$-tests based on analytic standard errors and
traditional critical values (such as 1.96 for statistical significance
at the 5\% level) to make inferences about the 2SLS coefficients.
However, recent research  \citep{Young2022} demonstrates that in the
presence of outliers, such tests severely underestimate the
uncertainties of the 2SLS estimates and may lead to false discoveries in
economic research. Our findings are consistent with this pattern. When
we use bootstrapping procedures, the AR test, or the $tF$-$cF$
procedure, an $F$-statistic-dependent $t$-test \citep{Lee2020-mi}, to
perform hypothesis testing, we find that, depending on the method
employed, 19-30\% of designs are statistically underpowered at the 5\%
level, whereas the number based on the SEs or $p$-values reported in the
original papers is only 10\%. This suggests that inferences based on
traditional $t$-tests may not accurately reflect the uncertainties in
2SLS estimates in a significant portion of cases.

Last but not least, our replications corroborate evidence from economics
and finance that the 2SLS estimates are often much bigger in magnitude
than the OLS estimates obtained from regressing the outcome on the
potentially endogenous treatment variables and covariates
\citep{jiang2017have}. In 68 out of the 70 designs (97\%), the 2SLS
estimates are bigger than the OLS estimates in magnitude; among them, 24
(34\%) are at least five times bigger. This is alarming because, in an
IV design with observational data, researchers often say that they are
most concerned about the upward bias of the treatment effect estimates
produced by naïve OLS. Even after we exclude 15 papers that explicitly
claim to expect downward biases in OLS estimates, the percentages remain
high (96\% and 35\%, respectively).

The first two patterns may be due to researchers' unfamiliarity with
recent development in the IV literature, such as the effective $F$
statistic and the  $tF$-$cF$ test,  or under-utilization of inferential
procedures robust to weak instruments, such as the AR test. Therefore,
researchers can avoid these problems by adopting better practices. The
third finding, however, is the most concerning. We cannot explain it
with weak instruments alone because, at least in the case of i.i.d.
errors, when the other identifying assumptions are satisfied. weak
instruments bias 2SLS estimates toward OLS estimates in finite samples
\citep{bound1995problems}, but what we observe is the opposite. The
ratio between the magnitudes of the 2SLS and OLS estimates is strongly
negatively correlated with the replicated partial correlation
coefficient between the instrument and the treatment among studies that
use non-experimental instruments; the relationship is much weaker among
studies with experimental instruments.

We suspect several possibilities are primarily responsible for this
pattern. First, it can be caused by a combination of weak first stages
and the failure of the (conditional) exogeneity assumption, either
because the instruments are confounded or because they violate the
exclusion restriction. Intuitively, because the 2SLS estimator is a
ratio, when the magnitude of the numerator is inflated due to endogenous
IVs or the failure of the exclusion restriction while the magnitude of
the denominator is a small number because of weak instruments, the 2SLS
estimate explodes. Another possibility is the publication bias: a weak
first stage leads to highly variant 2SLS estimates and those with large
magnitudes are more likely to be reported and published
\citep{gelman2014beyond}, hence the negative correlation.  Other
explanations, such as heterogeneous treatment effects (HTE) and
measurement errors in treatment variables, may also contribute to this
phenomenon, though we doubt they are the main driving forces given the
different patterns between observational and experimental studies.

What do these findings mean for the empirical IV literature in political
science? First, traditional $t$ tests for the 2SLS estimates (especially
those based on classic analytic SEs) mask the fact that most IV results
are highly uncertain, which creates additional opportunities for
selective reporting, leading to publication bias. Second, and more
importantly, many of the 2SLS estimates likely suffer from large biases
due to failures of the identifying assumptions and hence are not
credible. As \citet{sekhon2012natural} point out, many ``natural
experiments'' in observational studies on which many instruments are
generated are fundamentally different from real experiments and seldom
straightforward to analyze without detailed knowledge of the assignment
mechanism. Although we cannot definitively say which estimates are
problematic, the underlying issue seems to prevail in the IV literature.
% def

The goal of this paper is not to discredit existing IV studies or
dissuade researchers from ever using the IV approach. On the contrary,
our objectives are twofold: We want to caution researchers against the
danger of justifying their instruments in an ad-hoc fashion, especially
in observational studies; and we want to provide researchers with
practical advice that we hope can improve future practice, including
correctly quantifying the strength of instruments and uncertainties of
2SLS estimates, as well as conducting additional analysis such as a
placebo test to corroborate the identifying assumptions.

We contribute to a growing literature that evaluates the use of IV
strategies in empirical work across the social sciences and provides
methods to improve empirical practice. \cite{Young2022} replicates IV
designs from 32 articles published in economics journals and finds that
IV estimates are more sensitive to outliers than their OLS counterparts
and that conventional $t$ tests systematically understate uncertainties
and therefore have high false discovery rates. \cite{jiang2017have}
surveys over 250 articles published in finance journals and finds that
the vast majority of them report IV estimates that are larger than the
corresponding OLS estimates regardless of the sign of the potential
omitted variables bias, and postulates that this bias can be attributed
to exclusion restriction violations exacerbated by weak instruments, or
local effects that are far from representative of the population
treatment effect; however, the author does not conduct replications.
\citet{mellon2020rain} proposes to use sensitivity analysis to quantify
the vulnerability of using weather as instruments to exclusion
restriction violations.  \cite{dieterle2016simple} develop a quadratic
over-identification test for the first stage 2SLS, apply it to 15
published papers that use linear first stage, and find significant
non-linearities in 10 of them, and suggest that evaluating the implied
patterns of heterogeneity from their test relative to theoretical
predictions can be used to gauge the validity of an instrument.
\citet{FeltonStewart2022} survey 15 IV papers published in top sociology
journals and find that the assumptions required by an IV design are
often unstated and none of the studies use weak-instrument robust tests
or bootstrapped CIs. To the best of our knowledge, we are the first to
link the discrepancy between 2SLS and OLS estimates with the problem of
weak IVs using large-scale replication data from the social sciences.

The organization of the paper is as follows. In Section~\ref{theory}, we
briefly review the IV method under the traditional parametric framework,
including the required identifying assumptions, estimation strategies,
potential pitfalls, and newly developed methods in the literature.
Section~\ref{desc} details our case selection criteria and the resulting
replication sample. Section~\ref{sec:repl} describes our replication
procedure and presents the main results of our replication exercise. The
last section concludes the paper with a set of practical advice for
researchers.

% section~\ref{sec:placebo} suggests diagnostic tools for potential
% exclusion restriction violations and illustrates them using a case
% study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ##     ##    ###    ######## ##     ##  ######
% ###   ###   ## ##      ##    ##     ## ##    ##
% #### ####  ##   ##     ##    ##     ## ##
% ## ### ## ##     ##    ##    #########  ######
% ##     ## #########    ##    ##     ##       ##
% ##     ## ##     ##    ##    ##     ## ##    ##
% ##     ## ##     ##    ##    ##     ##  ######



\section{Theoretical Refresher} \label{theory}

In this section, we offer a brief overview of the IV approach, including
the setup, the identifying assumptions, as well as the 2SLS estimators.
We then discuss potential pitfalls and survey several inferential
methods. To cover the vast majority of IV studies in political science,
we adopt a traditional constant treatment effect approach to IV designs,
which imposes a set of parametric assumptions. For example, 51 (73\%)
designs in our replicated sample employ continuous treatment variables
and make no reference to treatment effect heterogeneity, hence, they are
ill-suited for the local average treatment effect (LATE) approach
\citep{angrist1996identification}.

%\footnote{Marginal treatment effect (MTE) based methods \citep{heckman2007econometric, Mogstad2018-me, Mogstad2018-ta} extend the LATE approach to apply to continuous treatments, but these methods have not yet been widely adopted in political science.}

For simplicity, we do not include additional exogenous controls in the
discussion without loss of generality. This is because, by the
Frisch-Waugh-Lovell theorem, we can remove them by regressing the
outcome, treatment, and IVs on the controls and using the residuals for
all subsequent analyses.

\begin{figure}[!ht]
\caption{A Directed Acyclic Graph of an IV Design}
\centering\label{fig:dag}
\includegraphics[width=0.35\textwidth]{graphs/dag.png}\vspace{-1.5em}
\end{figure}

Apart from the ``canonical'' use of IVs in addressing non-compliance in
experimental encouragement designs, we observe that in the majority of
the papers we review, researchers use IVs in observational settings to
establish causality between a single treatment variable $d$ and an
outcome variable $y$. The basic idea of this approach is to use an
instrument $z$ to isolate "exogenous" variation in $d$ (i.e., the
variation in $d$ that is not related to potential confounders) and
estimate its causal effect on $y$. Figure 1 illustrates the directed
acyclic graph for an IV design, where $\varepsilon$ denotes the error
term that captures all unexplained variations in $y$. The figure depicts
that because $d$ and $\varepsilon$ are correlated, an observed
correlation between $d$ and $y$ does not identify the causal effect of
$d$ on $y$. It also shows that an IV approach relies on three crucial
assumptions: (1) the \emph{relevance} of the instrument, which is
directly testable, meaning that $z$ is correlated with $d$; (2) the
\emph{unconfoundedness} assumption, which states that $z$ is
quasi-randomly assigned, and (3) the \emph{exclusion restriction}, which
posits that $z$ does not have a direct effect on $y$ beyond the channel
through $d$.




% The intuitiveness of the method masks the strong and untestable
% assumptions that underlie its applications in most observational
% empirical settings. \footnote{We show a more general directional
% acyclical graph in the Online Appendix when additional exogenous
% controls are present.}

% (2) $z$ and $\varepsilon$ are (conditionally) \emph{exogenous}, which
% is satisfied if two conditions are met: (i) $z$ is quasi-randomly
% assigned, or \emph{unconfounded}, and (ii) it does not have a direct
% effect on $y$ beyond the channel through $d$, referred to as the
% \emph{exclusion restriction}. Because failures of (i) and (ii) are
% observationally equivalent, we do not distinguish them in the analysis
% and diagnostics.


\subsection{Estimation Strategies}

Imposing a set of parametric assumptions, we define a system of
simultaneous equations:
\vspace{-1em}\begin{align}
  \text{Structural equation:}\quad y =&\ \alpha + \tau d  + \varepsilon\label{eq1} \\
  \text{First-stage equation:}\quad   d =&\ \pi_0 + \pi_1 z  + \nu\label{eq2}
\end{align}
in which $y$ is the outcome variable, $d$ is a scalar treatment
variable; $z$ is a vector of instruments for $d$; $\tau$ captures the (constant)
treatment effect and is the key quantity of interest.
Equations~(\ref{eq1}) and (\ref{eq2}) are referred to as the structural
equation and the first-stage equation, respectively. The error terms $\varepsilon$ and
$\nu$ in the two equations may be correlated.

The endogeneity problem for $\tau$ in Equation~(\ref{eq1}) arises when $d$
and $\varepsilon$ are correlated, which renders $\hat\tau_{OLS}$ from
a naïve OLS regression of $y$ on $d$ inconsistent. The endogeneity
problem may be due to one of the following reasons: (1) unmeasured
omitted variables that are correlated with both $y$ and $d$; (2)
measurement error in $d$, or (3) simultaneity or reverse causality,
which means $y$ may also affect $d$. The IV approach addresses this
problem by taking advantage of the exogenous variation in $d$ brought
by $z$. Substituting $d$ in Equation~(\ref{eq1}) using
Equation~(\ref{eq2}), we have the reduced form equation:

\vspace{-1em}\begin{equation}
\text{Reduced form:}\quad y =
\underbrace{(\alpha + \tau \pi_0)}_{\gamma_0} +
\underbrace{(\tau \pi_1)}_{\gamma_1} z + (\tau\nu + \varepsilon).
\end{equation}

Substitution establishes that $\gamma_1 = \tau \pi_1$, rearranging
yields $\tau = \frac{\gamma_1}{\pi_1}$ (assuming that we only use one instrument, but the intuition carries over to cases with multiple instruments). The IV
estimate, therefore, is the ratio of the reduced-form and first-stage
coefficients. To identify $\tau$, we make the following assumptions
\citep[Chapter 12]{greene2003econometric}.

\begin{assum}[Relevance] $\pi_1 \neq 0$. \label{assumption:rel}
This assumption requires that the IVs can predict the treatment variable, and is therefore equivalently stated
as $d \not \indep z$.
\end{assum}


\begin{assum}[Exogeneity] $\text{Cov}(z, \varepsilon) = 0$ and $\E[\varepsilon] =0$. \label{assumption:exog} Assumption~\ref{assumption:exog} is satisfied when unconfoundedness and the exclusion restriction are satisfied. However, without additional structural assumptions. failures of unconfoundedness and the exclusion restriction are observationally equivalent, therefore, we do not distinguish them in the analysis and diagnostics.
\end{assum}
% there are no omitted variables in the first stage and the IVs are not reversely affected by the treatment; moreover, conditional on covariates, the IVs are uncorrelated with unobservables in the structural equation.
% \footnote{In the LATE framework, because the treatment effects are allowed to be heterogeneous, an additional ``monotonicity'' assumption is added to make sure that the instruments change the probability of getting treatment or treatment intensity monotonically.}
Under Assumptions~\ref{assumption:rel} and \ref{assumption:exog},
the 2SLS estimator is shown to be consistent for the structural
parameter $\tau$.

\paragraph*{The 2SLS estimator.} Consider a sample of $N$ observations.
We can write $\Mat{d} = (d_1, d_2, \cdots, d_N)'$ and $\Mat{y}= (y_1,
y_2, \cdots, y_N)'$ as $(N\times 1)$ vectors of the treatment and
outcome data, and $\Mat{z}= (z_1, z_2, \cdots, z_N)'$ as $(N\times p_z)$
matrix of instruments in which $p_z$ is the number of instruments. To simplify
mathematics, we residualize original $\Mat{d}$, $\Mat{y}$, and each column of $\Mat{z}$ against the exogenous covariates, obtaining $\Mat{y}$,
$\Mat{d}$, and $\Mat{z}$, respectively. The 2SLS estimator is written as
follows: \vspace{-0.5em}
\begin{equation} \hat\tau_{2SLS} = \inv{\Mat{d}'
\Mat{P}_{z} \Mat{d}}  \Mat{d}' \Mat{P}_{z} \ve{y}
\vspace{-0.5em}\end{equation}
in which $\Mat{P}_{z} = \Mat{z}\inv{\Mat{z}' \Mat{z}} \Mat{z}'$ is the hat-maker matrix from the first stage which projects the endogenous treatment variable $\Mat{d}$ into the column space of $\Mat{z}$, thereby preserving only the exogenous
variation in $\Mat{d}$ that is uncorrelated with $\varepsilon$. This
formula permits the use of more than one instrument, in which case the
model is said to be ``overidentified.'' The 2SLS estimator belongs to a
class of generalized method of moments (GMM) estimators taking advantage
of the moment condition $\E[z\varepsilon] =0$, including the two-step
GMM \citep{hansen1982large} and limited information maximum likelihood
(LIML) estimators  \citep{anderson1982evaluation}. We use the 2SLS
estimator throughout the replication exercise because of its simplicity
and because every single paper in our replication sample uses it in at
least one specification.

When the model is exactly identified, i.e., the number of treatment
variables equals the number of instruments, the 2SLS estimator can be
simplified as the IV estimator: $\hat{\tau}_{2SLS} = \hat{\tau}_{IV} =
\inv{\Mat{z}'\Mat{d}} \Mat{z}'\Mat{y}$. In the case of one instrument and one treatment, the 2SLS estimator can also be written as
a ratio of two sample covariances: $\hat{\tau}_{2SLS} = \hat{\tau}_{IV}
= \frac{\hat\gamma_1}{\hat\pi_1} = \frac{\widehat{\rm Cov}(\Mat{y},
\Mat{z})}{\widehat{\rm Cov}(\Mat{d}, \Mat{z})}$, which illustrates
that the 2SLS estimator is a ratio between reduced-form and
first-stage coefficients in this special case. This further simplifies
to a ratio of the difference in means when $z$ is binary, which is
called a Wald estimator.

% The 2SLS estimator is consistent and asymptotically normal, but has
% bad finite-sample properties and is biased toward the OLS result in
% small samples when identifying assumptions are invalid
% \citep{bound1995problems}. The finite sample bias is seen clearly from
% the expectation of the simple IV estimator: \vspace{-0.5em}$$
% \Exp{\hat{\tau}_{IV}} = \Exp{\inv{\sumin z_i z_i'} \sumin z_i y_i} =
% \tau + \Exp{\inv{\sumin z_i x_i'} \sumin z_i \varepsilon_i}.
% \vspace{-0.5em}$$ The second term may not go to zero even when $\E[z_i
% \varepsilon_i]=0$. This is because the denominator cannot pass through
% the conditional expectation as in the OLS case, since we are not
% conditioning on $\Mat{d}$, only on $\Mat{z}$. This bias is decreasing
% in sample size, and is worsened by weak instruments and ``too many
% instruments'' \citep{Roodman2009-kj}. \citet{hirano2015location} prove
% that unbiased estimation is infeasible for linear IV models with an
% unrestricted parameters space for the first-stage coefficients.

% \paragraph*{Inference.} We use the IV estimator to illustrate why
% inference is more challenging with the 2SLS estimator than with the
% OLS estimator. A commonly used variance estimator for $\hat{\tau}_{IV}$
% can be written as: \begin{equation}\label{eqn:ivvar}
% \hat{\mathbb{V}}(\hat{\tau}_{IV}) \approx \frac{\hat{\Sigma}^2}{\sumin
% (x_i - \Ol{x})^2} \frac{1}{R_{xz}^2} =
% \hat{\mathbb{V}}(\hat{\tau}_{OLS}) \frac{1}{R_{xz}^2} \end{equation} in
% which $\hat{\Sigma}^2$ is a variance estimator for the error term and
% $R^2_{xz}$ is the R-squared from the first-stage. The estimated
% variance is mechanically larger than the estimated variance of the OLS
% estimator as long as $R^2_{xz} < 1$. It is decreasing in $R^2_{xz}$,
% i.e. stronger instruments produce more precise IV estimates. Robust
% SEs can be computed using the IV analogue of the Huber-White sandwich
% formula \citep{bekker1994alternative}. We can also obtain uncertainty
% estimates for the 2SLS estimates using nonparametric bootstraps or
% cluster-bootstraps.



\subsection{Potential Pitfalls in Implementing an IV Strategy} % (fold)
\label{sub:problems_in_iv_estimation}

The challenges with 2SLS estimation and inference are often due to the
violation of the two identifying assumptions. These difficulties can
result in (1) significant uncertainty around 2SLS estimates and size
distortion for $t$ tests due to weak instruments even when Assumption~\ref{assumption:exog} is valid; and (2) potentially larger biases in 2SLS estimates compared to OLS estimates when both assumptions are violated.

% https://microeconomics.ca/archive/vadim_marmer/weak_iv.pdf
% https://www.nber.org/econometrics_minicourse_2018/

\paragraph*{Inferential problem due to weak instruments.} Since the IV
coefficient is a ratio, the weak instrument problem is a
``divide-by-zero'' problem, which arises when $\Covar{z, x} \approx 0$
(i.e., when Assumption~\ref{assumption:rel} is violated). The instability of ratio estimators like $\wh\tau_{2SLS}$ when the denominator is approximately
zero has been extensively studied going back to
\citet{fieller1954some}. The conventional wisdom in the past two
decades has been that the first-stage partial $F$ statistic needs to
be bigger than 10, and it should be clearly reported
\citep{Staiger1997-lo}. As a rule of thumb, the original cutoff is chosen based on simulation results to meet two criteria under i.i.d. errors: (1) in the worst case, the bias of the 2SLS estimator does not exceed 10\% of the bias of the OLS estimator, and (2) a $t$-test based on the 2SLS estimator with a size of 5\% does not lead to size over 15\%.



% Hence, it is clear that the rule of thumb of $F > 10$ is no more than practical expediency. Recently,
% \citet{Lee2020-mi} show that $F$ needs to be as big as 104.7 for a
% conventional $t$-test to have the correct size and propose to adjust
% the critical values of the $t$ test based on the $F$ statistic.

The literature has discussed at least three issues caused by weak
instruments when Assumption~\ref{assumption:exog} is valid. First, under i.i.d. errors, a weak first stage exacerbates the finite bias of the 2SLS estimator toward the inconsistent OLS estimator, thereby reproducing the endogeneity problem that an IV design was meant to solve \citep{Staiger1997-lo}. Second, the 2SLS estimates become very imprecise.%
\footnote{To illustrate, a commonly used variance estimator for
$\hat{\tau}_{IV}$ can be written as:
\begin{equation*}\label{eqn:ivvar}
\hat{\mathbb{V}}(\hat{\tau}_{IV})
\approx \frac{\hat{\Sigma}^2}{\sumin (x_i - \Ol{x})^2}
\frac{1}{R_{xz}^2} = \hat{\mathbb{V}}(\hat{\tau}_{OLS}) \frac{1}{R_{xz}^2}
\end{equation*}
in which $\hat{\sigma}^2$ is a variance estimator for the error term and
$R^2_{xz}$ is the R-squared from the first stage. The estimated variance
is mechanically larger than the estimated variance of the OLS estimator
as long as $R^2_{xz} < 1$. It is decreasing in $R^2_{xz}$, i.e. stronger
instruments produce more precise IV estimates.} A third and related
issue is that the tests are of the wrong size and the $t$-statistics
don't follow a $t$-distribution \citep{nelson1990some}.
% this can be seen by examining the expectation of the IV estimator: $\Exp{\hat{\tau}_{2SLS} - \tau} \approx \frac{\sigma_{\nu \varepsilon}}{\sigma_\varepsilon^2} \frac{1}{F + 1}$, in which $\frac{\sigma_{\nu \varepsilon}}{\sigma_\varepsilon^2}$ is the bias from OLS and $F$ is the first-stage $F$ statistic \citep[pp. 206-208]{angrist2008mostly}. As a result, as $F\to 0$, the bias of the IV tends to the bias of the OLS
Issues relating to imprecision and test-statistic size arise from the
fact that the distribution of $\hat{\tau}$ is derived from its linear
approximation of $\hat{\tau}$ in ($\hat{\gamma}, \hat{\pi}$), wherein
normality of the two OLS coefficients implies the normality of their
ratio. However, this normal approximation
breaks down when $\hat{\pi} \approx 0$. Moreover, this approximation
failure cannot generally be rectified by bootstrapping
\citep{andrews2009validity}, although \citet{Young2022} argues that it
nevertheless allows for improved inference when outliers are present. Overall, valid IV inference relies crucially on correctly identifying strong IVs.

In general, there are two approaches for conducting inference in an IV design: the pretesting approach and the direct testing approach. The pretesting approach involves testing the strength of the first stage using some form of $F$ statistic, and if the $F$ statistic exceeds a certain threshold (e.g. $F > 10$), proceeding with a test on the null hypothesis about the treatment effect, such as $\tau = 0$, while the direct testing approach does not rely on passing a pretest. Almost all of the studies we review employ the pretesting approach. In this paper, we review four methods for statistical inference for IV designs. The first three methods are related to pretesting, while the last one is a direct test.

% Below we review four potential strategies to address inferential problems in % IV; first two are based on alternate $F$ statistics, which therefore
% makes them ``pre-testing'' approaches, while the subsequent two involve
% the use of the weak-identification robust Anderson-Rubin (AR)
% construction, and the bootstrap.

First, \citet{Olea2013-pa} propose the effective $F$ statistic for both just-identified and over-identified settings and accommodates robust or cluster-robust SEs. The effective $F$ is a scaled version of the first-stage $F$ statistic and is computed as $F_{\text{Eff}} = \hat{\pi}'\hat{Q}_{ZZ} \hat{\pi} / \text{tr}(\hat{\Sigma}_{\pi \pi} \hat{Q}_{ZZ})$, where $\hat{\Sigma}_{\pi \pi}$ is the variance-covariance matrix of the first stage regression, and $\hat{Q}_{ZZ} = \ooN \sumin z_i z_i'$. In just-identified cases, $F_{\text{Eff}}$ is the same as $F$ statistics based on robust or cluster-robust SEs. \citet{Olea2013-pa} derive the critical values for $F_{\text{Eff}}$ and notes that the statistic and corresponding critical values are identical to the better-known robust $F$ statistic $\hat{\pi} \hat{\Sigma}_{\pi \pi}^{-1} \hat{\pi}$ and corresponding \citet{stock2005asymptotic} critical values. Using Monte Carlo exercises,  researchers have shown that $F_{\text{Eff}}>10$ is a reasonable rule of thumb under heteroskedasticity or group structure \citep{Olea2013-pa, andrews2019weak}.

Second, \citet{Young2022} recommend researchers report two types of bootstrap confidence intervals (CIs), \emph{bootstrap-c} and \emph{bootstrap-t}, for $\hat{\tau}_{2SLS}$ under non-i.i.d. errors with outliers, which is common in social science settings. This involves $B$ replications of the following procedure: (1) sample $n$ triplets $(y_i^*, d_i^*, \Mat{z}_i^*)$ independently and without replacement from the original sample (with appropriate modifications for clustered dependence) and (2) compute the
$\hat{\tau}_{\text{2SLS}}$ coefficient and SE, as well as the corresponding test statistic $t^* = \hat{\tau}^*_{\text{2SLS}} / \hat{\text{SE}} (\hat{\tau}^*_{\text{2SLS}})$ on each replication. The \emph{bootstrap-c} method calculates the CIs by taking the $\alpha/2$ and $(1-\alpha/2)$ percentiles of the bootstrapped 2SLS coefficient $\hat{\tau}_{\text{2SLS}}^*$, while the \emph{bootstrap-t} method calculates the percentile-$t$ refined CIs by plugging in the $\alpha/2$ and $(1-\alpha/2)$ percentile of bootstrapped $t$ statistics $t^*_{\alpha/2}$ and $t^*_{1-\alpha/2}$ into the expression $\hat{\tau}_{\text{2SLS}} \pm t^*_{\alpha \mid 1 - \alpha}
\hat{\text{SE}}(\hat{\tau}^*_{\text{2SLS}})$. \citet{hall1996bootstrap} show that \emph{bootstrap-t} achieves an asymptotic refinement over \emph{bootstrap-c}.%
\footnote{We use the percentile method instead of bootstrapped SEs because the $t$-test for the 2SLS estimates based on the latter may be overly conservative \citep{Hahn2021-lq}.}

Third, in just-identified single treatment settings, \citet{Lee2020-mi} propose the $tF$-$cF$ procedure that smoothly adjusts the $t-$ratio inference based on the first-stage $F$ statistic, which improves upon the ad-hoc screening rule of $F > 10$. The adjustment factor applied to 2SLS SEs is based on the first stage $t-$ratio $\hat{f} \defeq \hat{\pi}/\sqrt{\hat{\mathbb{V}}(\hat{\pi})}$, with the first stage
$\hat{F} = \hat{f}^2$, and relies on the fact that the distortion from
employing the standard 2SLS $t$-ratio $\hat{t} \defeq \hat{\tau}/\sqrt{\hat{\mathbb{V}}(\hat{\tau})}$ can be quantified in terms of $\hat{F}$ and $AR-$statistic, which gives rise to a set of critical values for a given pair of $\hat{t}$ and $\hat{F}$. The authors also show that, if no adjustment is made to the $t$-test's critical value (e.g., using 1.96 as the threshold for 5\% statistical significance), a first stage $\hat{F}$ of 104.7 is required to guarantee a correct size of $5\%$ for a two-sided $t$-test for the 2SLS coefficient.

Finally, in the single treatment case, the AR procedure, which is essentially an $F$ test on the reduced form, is a direct inferential method robust to weak instruments \citep{anderson1949estimation, chernozhukov2008reduced}. Without loss of generality, assume that we are interested in testing the null
hypothesis that $\tau = 0$, which then implies that the reduced form
coefficient from regressing $y$ on $\Mat{z}$, $\gamma_1 = 0$. This
motivates the following procedure: given a set $\mathcal{T}$ of
potential values for $\wt{\tau}$, for each value $\wt{\tau}$,
construct $\wt{y} = y - d \wt{\tau}$, and regress $\wt{y}$ on
$\Mat{z}$ to obtain a point estimate $\wt{\gamma}$ and (robust, or
cluster robust) covariance matrix $\wt{\mathbb{V}}(\wt{\gamma})$, and
construct a Wald statistic $\wt{W}_s(\wt{\gamma}) \defeq \wt{\gamma}'
\wt{\mathbb{V}}(\wt{\gamma})^{-1} \wt{\gamma}$. Then, the AR
confidence region (CR) is the set of $\wt{\gamma}$ such that
$\wt{W}_s(\wt{\gamma}) \leq c(1-p)$ where $c(1-p)$ is the
$(1-p)^{\text{th}}$ percentile of the $\chi^2_1$ distribution. The AR test not only requires no pretesting but is also shown to be the most efficient in the just-identified case. However, it is not as commonly used as procedures that involve pretesting, possibly because researchers are more accustomed to using $t$-tests than $F$/Wald tests and reporting SEs rather than CRs.




\paragraph*{Bias amplification and the failure of Assumption~\ref{assumption:exog}.} When the number of instruments is bigger than the number of endogenous treatments,
researchers can use an over-identification test to gauge the
plausibility of Assumption~\ref{assumption:exog} \citep{arellano2002sargan}. However, such a
test is often underpowered and has bad finite sample properties
\citep{davidson2015bootstrap}. In just-identified cases, Assumption~\ref{assumption:exog} is
not directly testable.
% Because the consistency of the 2SLS estimator
% depends on it, researchers usually spend a great amount of effort
% verbally arguing for its plausibility based on theories and contextual
% information.
When combined with weak instruments, even small violations of Assumption~\ref{assumption:exog} can produce inconsistency. This is because $\plim\hat{\tau}_{IV} = \tau
+ \frac{\Covar{z, \varepsilon}}{\Covar{z, d}}$. When $\Covar{z, d}
\approx 0$, even small violations of exogeneity, i.e., $\Covar{z, \varepsilon}
\neq 0$, will enlarge the second term, resulting in large biases. Thus,
the two identifying assumption failures exacerbate each other: having
weak instruments compounds problems from confounding or exclusion
restriction violations, and vice versa. With invalid instruments, it is
possible that the asymptotic bias of the 2SLS estimator is greater than
that of the OLS estimator, i.e., $\left|\frac{\Covar{z, \varepsilon}}{\Covar{z, d}}\right| \gg |\Covar{d, \varepsilon}|$ in the single instrument case.

% To see this, we write the ratio of biases of
% two estimators as follows (assuming $\hat\tau_{OLS}$ is inconsistent):
% \vspace{-1em}\begin{equation}\label{eq:ratio}
% \frac{\text{Bias}_{IV}}{\text{Bias}_{OLS}} = \frac{\plim
% \hat{\tau}_{IV} - \tau}{\plim \hat{\tau}_{OLS} - \tau} =
% \frac{\Covar{z, \varepsilon}}{\Covar{z, d} \Covar{d, \varepsilon}}
% \vspace{-1em}\end{equation}
% in which $\rho(z, \varepsilon)$, $\rho(z,
% x)$ and $\rho(d, \varepsilon)$ are the correlation coefficients between
% $z$ and $\varepsilon$, $z$ and $d$, and $d$ and $\varepsilon$,
% respectively. When the IV is weak, i.e., $\Covar{z, d}$ is small, the
% magnitude of the ratio is likely to be large.

While the inference problem can be alleviated by employing alternative inferential methods as described above, the failure of Assumption~\ref{assumption:exog} is much harder to diagnose and address. This is because it is inherently a research design issue and should ultimately be tackled at the research design stage. Indeed, researchers typically spend considerable effort arguing for both unconfoundedness and the exclusion restrictions in their particular setting.
In the SI, we provide an explanation of the ``zero-first-stage'' (ZFS) test \citep{bound2000compulsory}, which is essentially a placebo test on a subsample where the instrument is expected to be uncorrelated with the treatment, to help researchers gauge the validity of their instruments. These estimates can then be used to debias the 2SLS estimate using the methods proposed in \citet{Conley2012-mu}. We discuss these methods in the Supporting Materials (SM). 


\FloatBarrier

% subsection exclusion_restriction_violations (end)

% subsubsection weak_instruments (end)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ########  ########  ######   ######
% ##     ## ##       ##    ## ##    ##
% ##     ## ##       ##       ##
% ##     ## ######    ######  ##
% ##     ## ##             ## ##
% ##     ## ##       ##    ## ##    ##
% ########  ########  ######   ######

\section{Data and Types of Instruments} \label{desc}

In this section, we first discuss our case selection criteria and the
replication sample, which is the focus of our subsequent analysis. We
then describe the types of instruments in the replicable studies.

\paragraph*{Data.} We examine all empirical papers published in the
APSR, AJPS, and JOP (printed versions) from 2010 to 2022 and
identify studies that use an IV strategy as one of the main
identification strategies, including papers that use binary or
continuous treatments and that use a single or multiple instruments.
Specifically, we use the following criteria: (1) the discussion of the
IV result needs to appear in the main text and support a main argument
in the paper; (2) we consider linear models only; in other words, papers that use discrete outcome models are excluded from our sample; (3) we exclude papers that include multiple endogenous variables in a single specification because estimated treatment effects are correlated with one another (multiple endogenous variables in separate specifications are included); (4) we exclude papers that use IV or GMM estimators in a dynamic panel setting because they are subject to a separate set of empirical issues and their poor performance has been thoroughly
discussed in the literature \citep{bun2010weak}. These criteria result
in 30 papers in the APSR, 33 papers in the AJPS, and 51 papers in the JOP. We then strive to find replication materials for these papers from public data-sharing platforms, such as the Harvard Dataverse, and the
authors' websites. We are able to locate replication materials for 76 (62\%) papers.\footnote{Among them, two papers have incomplete data,
which makes it impossible for us to replicate the results.} However,
code completeness and quality of documentation vary a great deal. Data
availability has significantly improved since 2016-2017 following new
editorial policies requiring authors to make replication materials
publicly available, though none of the journals requires full
replicability administrated by a third party as a condition for
publication \citep{key2016we}, which would constitute a major
improvement in our view.

\begin{table}[htbp]
  \centering\small
  \caption{Data Availability and Replicability of Papers Using IVs}
    \label{tb:sample}%
    \begin{tabular}{lccccc}\hline\hline
          \multicolumn{1}{p{5em}}{} & \multicolumn{1}{p{5em}}{} &
          \multicolumn{1}{p{5em}}{\centering Incomplete} & \multicolumn{1}{p{5em}}{\centering Incomplete} & \multicolumn{1}{p{5em}}{\centering Replication} & \multicolumn{1}{p{5em}}{}  \\
            & \#All Papers  & Data & Code & Error  & Replicable  \\ \hline
    APSR  & 30  & 16  & 0 & 3 & 14 (42\%) \\
    AJPS  & 33  & 3  & 1 & 1 & 25 (83\%) \\
    JOP   & 51  & 19 & 3  & 1 & 28 (55\%)\\ \hline
    Total & 114 & 38 & 4 & 5 & 67 (59\%) \\\hline
    \end{tabular}%
\end{table}%

Using data and code from the replication materials, we set out to
replicate the main IV results in the 67 papers. Our replicability
criterion is simple: As long as we can exactly replicate \emph{one} 2SLS point estimate that appears in the paper, we deem the paper replicable. We do not aim at exactly replicating SEs, $z$-scores, or level of statistical significance for the 2SLS estimates because they involve the choice of the inferential method, which we will discuss in the next section.

After much effort and hundreds of hours of work, we are able to
replicate the main results of 61 papers.\footnote{For three papers, we
are able to produce the 2SLS estimates with perfectly executable code;
however, our replicated estimates are inconsistent with what was
reported in the original studies. We suspect the inconsistencies are
caused by data rescaling or misreporting; hence, we keep them in the
sample.} The low replication rate is consistent with what is reported in \citet{Hainmueller2019-wx}. The main reasons for failures of replication are incomplete data (38 papers), incomplete code or poor documentation (4 papers), and replication errors (5 papers).
% Researchers can improve replicability by explicitly listing of software dependencies and their versions, %\footnote{For example, by using software solutions such as \texttt{renv} or \texttt{packrat} in R or timestamped \texttt{ado} files in Stata.}
% using relative file-paths,
%\footnote{e.g. \texttt{"read.csv("C:/Users/JaneDoe/Documents/project/file.csv"}, which is certain to fail on everyone but Jane Doe's computer. While we manually corrected all such instances in order to replicate results, this severely hampers replicability in automated settings and should be avoided. Researchers may use automated solutions such as \texttt{here} or simply define a \texttt{root} directory and perform all read/write relative to this path using \texttt{file.path / paste}, so that a single change suffices.},
% and supplying replication packages with informatively named files and ideally a ``caller'' script that runs data-prep and analysis programs in order.
Table~\ref{tb:sample} presents summary statistics on data availability and replicability of IV papers for each of the three journals. The rest of this paper focuses on results based on these 61 replicable papers
(and 64 IV designs).


\paragraph*{Types of IVs.} Inspired by \cite{sovey2011instrumental}, in
Table~\ref{tb:iv.type}, we summarize the types of IVs in the replicable
designs, although our categories differ from theirs to reflect changes
in the types of instruments used in the discipline. As in
\cite{sovey2011instrumental}, the biggest category is ``Theory,'' in
which the authors justify Assumption~\ref{assumption:exog}, including IVs' quasi-randomness
and the exclusion restriction, using social science theories or
substantive knowledge. We further divide theory-based IVs into four
subcategories: geography/climate/weather, history, treatment diffusion,
and others.

Many studies in the theory category justify the choices of their instruments
based on geography, climate, or weather conditions. For example,
\citet{zhu2017} uses weighted geographic closeness as an instrument for the
activities of multinational corporations; \citet{hager2019ethnic} use
mean elevation and distance to rivers to instrument equitable
inheritance customs; and \citet{grossman_etal2017} use the number of
distinct landmasses as an instrument for government fragmentation.
\citet{henderson2016mediating} use rainfall around Election Day as an IV
for democratic vote margins. The popularity of weather instruments for a
whole host of outcomes necessarily implies that the exclusion
restriction is especially tenuous in such cases \citep{mellon2020rain}.

\begin{table}[htbp]
  \centering
  \caption{Types of Instruments} \label{tb:iv.type}%
    \begin{tabular}{lcc}\hline\hline
    Type & \multicolumn{1}{c}{\#Papers} & \multicolumn{1}{c}{Percentage \%} \\\hline
    \textbf{Theory}    & 42    & 60.0 \\
    $\qquad$ Geography/climate/weather & $\qquad$ 13     & $\qquad$ 18.6 \\
    $\qquad$ History & $\qquad$ 11    & $\qquad$ 15.7 \\
    $\qquad$ Treatment diffusion & $\qquad$ 2    & $\qquad$ 2.9 \\
    $\qquad$ Others & $\qquad$ 16 & $\qquad$ 22.9 \\
    \textbf{Experiment} & 12     & 17.1 \\
    \textbf{Econometrics} & 9     & 12.9 \\
    $\qquad$ Interactions/``Bartik'' & $\qquad$ 7     & $\qquad$ 10.0 \\
    $\qquad$ Lagged treatment & $\qquad$ 1     & $\qquad$ 1.4 \\
    $\qquad$ Empirical test & $\qquad$ 1     & $\qquad$ 1.4 \\
    \textbf{Rules \&  policy changes}  & 7     & 10.0 \\
    $\qquad$ Change in exposure & $\qquad$ 3     & $\qquad$ 4.3 \\
    $\qquad$ Fuzzy RD & $\qquad$ 4     & $\qquad$ 5.7 \\ \hline
    \textbf{Total} & 70    & 100.0 \\\hline
    % \multicolumn{3}{p{13cm}}{\footnotesize\textbf{\textit{Note:}} One paper uses both geography-based IVs and an instrument based on treatment diffusion from neighbors. We count 0.5 for each category.}
    \end{tabular}%
\end{table}%

Historical IVs are based on historical differences between units that
cannot be explained by current levels of the treatment. For example,
\citet{vernby2013} uses historical immigration levels as an instrument for the
current number of non-citizen residents. Similarly,
\citet{spenkuch_tillmann2018} use historical decisions by rulers in
Europe over the religion of their region to instrument for the current
religion of survey respondents. These studies use historical variation
as instruments for current or modern variables.

Several studies base their choices on regional diffusion of treatment.
For example, \citet{dube2015} use US military aid to countries outside
Latin America as an instrument for US military aid to Colombia.
\citet{grossman2017} use over-time variation in the number of regional
governments to instrument government fragmentation in sub-Saharan
Africa. \citet{dorsch2019} use regional share of democracies as an IV
for democratization in a country-year panel.

Finally, several papers rely on a unique instrument based on theories that we
could not place in a category. For example, \citet{carnegie_marinov2017}
use the rotating presidency of the Council of the European Union as an
instrument for official development aid. They argue that countries that were
colonized by the country that holds the presidency receive exogenously
more aid than other countries. In another example,
\citet{dower_etal2018} use religious polarization as an instrument for the
frequency of unrest and argue that religious polarization could only
impact collective action through its impact on representation in local
institutions. These papers employed IVs that were usually unique to the
paper.

The second-biggest category is randomized experiments. Articles in this
category employ randomization, designed and conducted by researchers or
a third party, to make causal inference and use 2SLS estimation to address
non-compliance issues in an encouragement design---the IV normally is
being encouraged to take the treatment. With random assignment, we have
more confidence in Assumption~\ref{assumption:exog} because $z\indep v$ by design, and the
direct effect of encouragement on the outcome is easier to rule out than
without random assignment.
%\footnote{We also include \citet{healy2013} in this category, in which the instrument is a biological ``lottery'' of a newborn's gender.}

Another category of IVs are based on explicit rules, which generate
quasi-random variation in the treatment. \citet{sovey2011instrumental}
refer to this category as ``Natural Experiment.'' We avoid this
terminology because it is widely misused. We limit this category to two
circumstances: fuzzy regression discontinuity (RD) designs and variation
in exposure to policies due to time of birth or
eligibility.\footnote{The difference between the two is subtle: For the
latter, the gap in the forcing variable, such as birth cohort, is fixed
and cannot be arbitrarily small.} For example, \citet{kim2019} leverages
a reform in Sweden that requires municipalities above a population
threshold to adopt direct democratic institutions. \citet{dinas2014}
uses eligibility to vote based on age at the time of an election as an
instrument for whether respondents did vote. Respondents who were 18 as of
election day could vote while those who were 17 could not, making this a
problem of one-sided non-compliance, as some respondents who were over
18 did not vote.
% \citet{lerman_mccabe2017} use eligibility for Medicare based on age as an instrument for accessing publicly funded health insurance in the United states.
% \citet{cirone2018} use quasi-random assignment of budget incumbents to bureaux as an instrument for budget committee service for deputies in the French Third Republic.
% \citet{lerman_mccabe2017} use being born between 1946 and 1947 as an instrument for enrollment in public health insurance in Country. The rationale behind these IVs is that policy change that impact a subset of the society defined by birth years can be seen as quasi-random for people close to the cohort cutoff because birth years are pre-determined and sorting based on birth year is unlikely to happen.

The last category of IVs are based on econometric assumptions. This
category includes what \cite{sovey2011instrumental} call lags. These are
econometric transformations of variables the author argues constitute an
instrument. For example, \citet{lorentzen_etal2014} use a measure of the
independent variable from 8 years earlier to mitigate endogeneity
concerns. Another example of this are instruments that rely on a
transformation of variables to satisfy the assumptions. For example,
\citet{dorsch_maarek2019} use the sum of neighboring countries with
similar institutions that are democracies as an instrument for democratization
within a country. Shift-share ``Bartik'' instruments that are based on
interactions between multiple variables are also included in this
category \citep{goldsmith2020bartik}. For example, \cite{baccini2021} use the share of jobs in a given industry within a county interacted with national-level changes in employment in that industry to study the effect of manufacturing layoffs on voting.

Compared with IV papers published before 2010, there is a significant
increase in the proportion of papers using experiment-generated IVs
(from 2.9\% to 18.8\%) thanks to an increased popularity of survey and
field experiments. In stark contrast, the number of papers
relying on econometric techniques or flawed empirical tests (such as
regression $y$ on $d$ and $z$ in one regression and check whether the
coefficient of $z$ is significant) to justify potential IVs has
decreased thanks to improving empirical practice in the discipline. The
percentage of papers using theory-justified IVs remains almost the same
at around  60\%.


\section{Replication Procedure and Results} \label{sec:repl}

In this section, we describe our replication procedure and report the main findings.

\paragraph*{Procedure.} For each paper, we select the main IV
specification that plays a central role in supporting a main claim in
the paper; it is either referred to as the baseline specification or
appears in one of the main tables or figures.  Focusing on this
specification, our replication procedure involves the following steps.
First, we compute the first-stage partial $F$ statistics based on (1)
classic analytic SEs, (2) Huber White heteroskedastic-robust SEs, (3) cluster-robust SEs, if there is a clustering structure according to the authors, and (4) bootstrapped SEs.%
\footnote{They are calculated by $F_{boot} =
\hat\tau_{2SLS}'{\hat{\rm Var}_{boot}(\hat\tau_{2SLS})}^{-1}\hat\tau_{2SLS}/p_{z}$, where $p_{z}$
is the number of IVs and $\hat{\rm Var}_{boot}(\hat\tau_{2SLS})$ is the
estimated variance-covariance matrix based on a nonparametric bootstrap
procedure, in which we repeatedly sample the rows of the data matrix
with replacement. If the data have a clustered structure, we use cluster-bootstrapping instead by sampling with replacement each cluster of data \citep{Colin_Cameron2015-wp,Esarey2019-qt}. We include $F_{boot}$ as a reference to the classic $F$ and effective $F$. In Section A.2 of the SM, we compare the five types of $F$ statistics and show that the effective $F$ and $F$ based on bootstrapping are usually more conservative than other $F$ statistics.} %
We also calculate the effective $F$.


Second, we replicate the original IV result using the 2SLS estimator and apply four different procedures for inference. First, we make inferences about the 2SLS results based on analytic SEs, including robust SEs or cluster-robust SEs (if applicable).
Additionally, we use two nonparametric bootstrap procedures, as described in Section 2, \emph{bootstrap-c} and \emph{bootstrap-t}. For specifications with only a single
instrument, we also employ the $tF$ procedure proposed by \citet{Lee2020-mi}, using 2SLS $t$-statistics and first-stage $F$-statistics based on analytic SEs accounting for the
originally specified clustering structure. Finally, we conduct an AR procedure and record the $p$-values and CRs.

We record the point estimates, SEs (if applicable), 95\% CIs or CRs, and $p$-values for each procedure (the point estimates fully replicate the reported estimates in the original papers and are the same across all procedures). In addition, we estimate a naïve OLS model by regressing the outcome variable on the treatment and control variables, leaving out the instrument. We calculate the ratio between the magnitudes of the 2SLS and OLS estimates. We also record other useful information, such as the
number of observations, the number of clusters, the types of instruments, the methods used to calculate SEs or CIs, and the rationale for each paper's IV strategy. Our replication yields the following three main findings.

\paragraph*{Finding 1. First-stage partial $F$ statistics.} Our first
finding regards the strengths of the instruments. To our surprise, among the 70 IV designs, 12 (17\%) do not report this crucial statistic
despite its key role in justifying the validity of an IV design. Among the remaining 58 studies that report $F$ statistics, 9 (16\%) use classic analytic SEs, thus not adjusting for potential heteroskedasticity or clustering structure. In Figure~\ref{fig:fstat},
we plot the replicated first-stage partial $F$ statistics based on the authors' original model specifications and choices of variance estimators on the x-axis against effective $F$ statistics (a) or bootstrapped $F$ statistics (b) on the y-axis. Both axes are on a logarithmic scale.\footnote{We use the replicated $F$ statistics instead of the reported ones because some authors either report do not or misreport their $F$ statistics (see SM for a comparison between the reported and replicated $F$ statistics).}
\begin{figure}[!h]
\caption{Original vs. Effective and Bootstrapped $F$} \label{fig:fstat}
\centering
\begin{minipage}{1\linewidth}{
\begin{center}
\hspace{-2em}
\subfigure[Original $F$ vs. Effective $F$]{\includegraphics[width=0.45\textwidth]{graphs/f_rep_effective.pdf}}\hspace{1em}
\subfigure[Original $F$ vs. Bootstrapped $F$]{\includegraphics[width=0.45\textwidth]{graphs/f_rep_boot.pdf}}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Circles and triangles represent applications with and without a clustering structure, respectively. Studies that do not report $F$ statistics are painted in red. The original $F$ statistics are the replicated $F$ statistics based on the authors' original model specifications and choices of variance estimators. They may differ from those reported in the papers because of misreporting.}}
\end{minipage}\vspace{-0.5em}
\end{figure}

In the original studies, the authors used various SE estimators, such as classic SEs, robust SEs, or cluster-robust SEs. As a result, $F_{\texttt{Eff}}$ may be larger or smaller than the replicated ones. However, a notable feature of Figure~\ref{fig:fstat} is that when a clustering structure exists, the replicated $F$ tend to be larger than the effective $F$ (15 studies, 21\%) or bootstrapped $F$ (53 studies, 75\%). When using the effective $F$ as the benchmark, 8 studies (11\%) had $F_{\texttt{Eff}}<10$. This number increases to 12 (17\%) when the bootstrapped $F$ statistics are used as the benchmark.  The median first-stage $F_{\texttt{Eff}}$ statistic is higher in experimental studies compared to non-experimental ones (67.7 versus 53.5). It is well known that failing to cluster the SEs at the appropriate level or using the analytic cluster-robust SE with too few clusters can lead to a severe overstatement of statistical significance \citep{cameron2008bootstrap}. However, this problem has received less attention when evaluating the strength of instruments using $F$ statistics.

\paragraph*{Finding 2. Inference.} Next, we compare the reported and
replicated $p$-values for the null hypothesis of no effect. For studies
that do not report a $p$-value, we calculate it based on a standard normal distribution. The replicated $p$-values are based on (1) the \emph{bootstrap-c} method, (2) the \emph{bootstrap-t} method, and (3) the AR procedure. Since we can exactly replicate the point estimates for the papers in the replication sample, the differences in $p$-values are the result of the
inferential methods used. Figure~\ref{fig:pvalue} plots reported and
replicated $p$-values, from which we observed two patterns. First, most of the reported $p$-values are smaller than 0.05 or 0.10, the conventional thresholds for statistical significance. Second, consistent with \citet{Young2022}'s finding, our replicated
$p$-values based on the AR procedure or bootstrap methods are usually bigger than the reported $p$-value, which are primarily based on $t$ statistics calculated using analytic SEs.
\begin{figure}[!ht]
\caption{$P$-Values for 2SLS Estimates: Original vs. Replicated}\label{fig:pvalue}
\vspace{-1.5em}
\begin{center}
\subfigure[Bootstrap-c method]{\hspace{-1em}\includegraphics[width=0.33\textwidth]{graphs/p_bootc.pdf}}\hspace{0em}
\subfigure[Bootstrap-t method]{\hspace{0em}\includegraphics[width=0.33\textwidth]{graphs/p_boott.pdf}}
\subfigure[Anderson-Rubin]{\hspace{0em}\includegraphics[width=0.33\textwidth]{graphs/p_AR.pdf}}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} All tests are against the null that $\tau = 0$. Both axes are on a squared-root scale. Original $p$-values are either adapted from the original papers when they are reported or calculated based on a standard-normal approximation of the sampling distribution of the $z$-scores when they are not. The solid circles represent \citet{arias2019large}, in which the authors use the IV strategy to argue for a null effect. \emph{Bootstrap-c} and \emph{-t} represent percentile methods based on the distributions of 2SLS estimates and $t$-statistics, respectively, using the authors' original model specifications. The hollow triangles in panel (c) indicate unbounded 95\% confidence intervals from the Anderson-Rubin test using the inversion method.
 The look of these plots is inspired by Figure 3 in \citet{Young2022}.}
\end{figure}
% Replicated $p$-values are those obtained from replication analyses using the authors' original specifications and variance estimators.
Using the AR test, we cannot reject the null hypothesis of no effect at the 5\% level in 13 studies (19\%), compared with 7 (10\%) in the original studies. The number increases to 20 (29\%) and 15 (21\%) when we use $p$-values from the \emph{bootstrap-c} and \emph{-t} methods. Note that very few papers we review utilize inferential procedures specifically designed for weak instruments, such as the AR test (2
papers), the conditional likelihood-ratio test \citep{Moreira2003-oj} (1 paper), and confident sets \citep{Mikusheva2006-lk} (none).
%

We also apply the $tF$-$cF$ procedure to 54 studies that use single IVs using $F_{\texttt{Eff}}$ statistics and $t$ statistics based on robust or cluster-robust SEs. Figures~\ref{fig:tF} shows that 16 studies (30\%) are not statistically significant at the 5\% level, and 5 studies deemed statistically significant when using the conventional fixed critical values for the $t$-test become statistically insignificant using the $tF$-$cF$ procedure, indicating that overly optimistic critical values due to weak instruments also contribute to overestimation of statistical power, but not as the primary factor. These results suggest that both weak instruments and non-i.i.d. errors have contributed to severe overstatements of power in IV studies in political science.
\begin{figure}[!th]
\caption{Statistical Significance for 54 Single Instrument Studies: $c(F)$ Threshold}\label{fig:tF}
\begin{center}\vspace{-1.5em}
\hspace{-2em}\includegraphics[width=0.6\textwidth]{graphs/tf_cF.pdf}
\vspace{-1em}
\end{center}
{\footnotesize\textbf{\textit{Note:}} The green and red dots represent the studies that remain statistically significant at the 5\% level using the $tF$ procedure and those that do not, respectively. We use the effective $F$ statistics and $t$ statistics based on robust or cluster-robust SEs in the procedure. This plot is inspired by Figure 3 in \citet{Lee2020-mi}.}
\end{figure}

\paragraph*{Finding 3. 2SLS-OLS discrepancy.} Finally, we investigate
the relationship between the 2SLS estimates and naïve OLS estimates. In
Figure~\ref{fig:ratio.raw}(a), we plot the 2SLS coefficients against the OLS coefficients, both normalized using reported OLS SEs. The shaded area indicates the range beyond which the OLS estimates are statistically significant at the 5\% level. It shows that for
most studies in our sample, the 2SLS estimates and OLS estimates
share the same direction and that the magnitudes of the former are often much larger than those of the latter. Figure~\ref{fig:ratio.raw}(b) plots the distribution of the ratio between the 2SLS and OLS estimates (in absolute terms). The mean and median of the absolute ratios are 12.4 and 3.4, respectively. In fact, in all but two designs (97\%), the 2SLS estimates are bigger than the OLS estimates, consistent with \citet{jiang2017have}'s finding based on finance research. While it is theoretically possible for most OLS estimates in our sample to be biased towards zero, only 21\% of the studies have researchers expressing their belief in downward biases of the OLS estimates. Meanwhile, 40\% of the studies consider the OLS results to be their main findings. The fact that researchers use IV designs as robustness checks for OLS estimates due to concerns of upward biases seems at odds with the significantly larger magnitudes of the 2SLS estimates.
\begin{figure}
\caption{Relationship between OLS and 2SLS Estimates}\label{fig:ratio.raw}
\begin{center}\vspace{-1.5em}
  \subfigure[Scatterplot]
  {\includegraphics[width=0.46\textwidth]{graphs/coefs_iv_ols.pdf}}\hspace{2em}
  \subfigure[Ratio Histogram]
  {\includegraphics[width=0.46\textwidth]{graphs/hist_ratio.pdf}}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Reported 2SLS and OLS coefficient estimates are used in both plots. The left panel uses reported OLS SE estimates for normalization; the gray band indicates the $[-1.96, 1.96]$ interval.}
\end{figure}

We further explore whether the 2SLS-OLS discrepancy is related to IV
strength, measured by $\hat\rho(d, \hat{d})$, the estimated correlation
coefficient between the treatment and predicted treatment. We find a strong negative correlation between
$\left|\frac{\hat{\tau}_{2SLS}}{\hat{\tau}_{OLS}}\right|$ and $|\hat\rho
(d,\hat{d})|$ among studies using non-experimental instruments (grey dots in Figure~\ref{fig:ratio.rho}(a). The adjusted $R^2$ is $0.268$, with $p m= 0.000$. However, the relationship is much weaker among studies using experiment-generated instruments (red dots). The adjusted $R^2$ is $-0.014$ with $p = 0.378$. In Figure~\ref{fig:ratio.rho}(b), we limit our focus to the subsample in which the OLS estimates are statistically significant at the 5\% level and researchers accept them as (part of) the main findings, and the strong negative correlation remains.
\begin{figure}[!ht]
\caption{IV Strength and the 2SLS-OLS Discrepancy}\label{fig:ratio.rho}
\vspace{-1.5em}
\begin{center}
\subfigure[\scriptsize Full Sample]{\includegraphics[width=0.46\textwidth]{graphs/ratio_exp.pdf}}\hspace{1em}
\subfigure[\scriptsize Subsample with Statistically Significant OLS Results]{\includegraphics[width=0.46\textwidth]{graphs/ratio_ols_signif.pdf}}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} In the left panel, gray and red circles represent observational and experimental studies, respectively. In the right panel, we highlight studies in which the OLS results are claimed to be (part of) the main findings.}
\end{figure}
This indicates that even when researchers do not show explicit concerns about their OLS results (and often use IV strategies as robustness checks), the 2SLS-OLS discrepancy is still related to IV strength. At first glance, this result may seem mechanical: as the correlation between $d$ and $\hat{d}$ increases, the 2SLS estimates naturally converge to the OLS estimates.
However, the properties of the 2SLS estimator under the identifying
assumptions do not predict the negative relationship (we confirm it in simulations in the SM), and such a relationship is not found in experimental studies.

We believe a multitude of factors contribute to this pattern, including (1) the failure of the exogeneity assumption (Assumption~\ref{assumption:exog}), (2) publication bias (3) treatment effect heterogeneity (HTE), and (4) measurement error in $d$. We suspect the first two factors are the main driving forces. As previously mentioned, when Assumption~\ref{assumption:exog} is violated, weak instruments amplify the biases from endogenous IVs or exclusion restriction failures, i.e.,
$\frac{\text{Bias}_{IV}}{\text{Bias}_{OLS}} = \frac{\Covar{z,
\varepsilon}}{\Covar{z, d} \Covar{d, \varepsilon}} \gg 1$. Second, publication bias may also play a role. When the first stage is weak, IV estimates have a larger variance and can be very large or very small in
magnitude compared to OLS estimates. If researchers selectively report
statistically significant results or journals tend to publish papers
with statistically significant findings, we may observe a negative
relationship as in Figure~\ref{fig:ratio.rho}. This phenomenon is also
referred to as Type-M bias in the psychology and sociology literature
\citep{gelman2014beyond, FeltonStewart2022}.

% Another two factors may also play a role, though we do not think they are the primary reasons for the discrepancy.
Third, 30\% of the replicated studies in our sample mention HTE as a possible explanation for this discrepancy. OLS and 2SLS place different weights on covariate strata in the sample, and therefore if compliers, those whose treatment status is affected by the instrument, are more responsive to the treatment than the rest of the units in the sample, we might see diverging OLS and 2SLS estimates. Under the assumption that the exclusion restriction holds, this gap can be decomposed into covariate weight difference, treatment-level weight difference, and endogeneity bias components using the procedure developed in \citep{Ishimaru2021-ik}. In the SM, we investigate this possibility and find that it is highly unlikely that HTE \emph{alone} can explain the difference in magnitudes between 2SLS and OLS estimates we observe in the replication data, i.e.,  the variance in treatment effects needed for this gap is implausibly large.

Finally, an IV design can correct for the downward bias of the
measurement error in $d$, resulting in $|\hat{\tau}_{2SLS}/\hat{\tau}_{OLS}| > 1$. If the measurement error is
large, this can weaken the relationship between $z$ and $d$, producing a negative correlation. However, it is worth noting that almost no papers in our sample attribute the IV strategy to measurement error; the negative correlation remains even when the OLS estimates are the main findings (indicating measurement error may not be as concerning for researchers).


\begin{table}[!ht]
  \centering\small
  \caption{Summary of Replication Results}
  \label{tb:des}%
    \begin{tabular}{p{15em}cccc}\hline\hline
    (\%) & APSR (15) & AJPS (25) & JOP (30) & All (70) \\ \hline
    \emph{Panel A: First-Stage $F$ Statistic}  & \\
    $\quad$Unreported  &  0.0     &  20.0  & 23.3 &  17.1\\
    $\quad$Large discrepancy  &  20.0  & 25.0 &  30.4  & 25.9 \\
    $\quad$Effective $F < 10$  &  13.3 &  12.0 & 10.0  &  11.4 \\
    $\quad$Bootstrapped $F < 10$  &  13.3 &  20.0 & 16.7  &  17.1 \\ \\
    \emph{Panel B: Inference for IV Designs}  & \\
    $\quad$Original $p > 0.05$ &  20.0  & 8.0 &  6.7   & 10.0 \\
    $\quad$AR $p > 0.05$  &  13.3   & 24.0    &   16.7  & 18.6 \\
    $\quad$Bootstrap-c $p > 0.05$  &  20.0   & 32.0    &   30.0  & 28.6 \\
    $\quad$Bootstrap-t $p > 0.05$ &    26.7   & 24.0   &   16.7    & 21.4 \\
    $\quad$$tF$-$cF$ procedure $p > 0.05$  &  38.5  & 23.6  & 29.2  & 29.6  \\ \\
    \emph{Panel C: 2SLS-OLS Relationship} & \\
    $\quad$ $sign(\hat\tau_{2SLS}) = sign(\hat\tau_{OLS})$  &  93.3 &  100.0 &  86.7  & 92.9 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 1$  &  93.3     &  100.0  &  96.7  & 97.1 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 3$  &  53.3   &  44.0  &  60.0  & 52.9 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 5$  &  40.0  &    32.0  & 33.3  & 34.3 \\
    $\quad\ |\hat\tau_{2SLS}/\hat\tau_{OLS}|> 10$  &  13.3   & 16.0  &  20.0 & 17.1 \\
    \hline
    \multicolumn{5}{p{37em}}{\footnotesize\textbf{\textit{Note:}} ``Large Discrepancy'' in Panel A is defined as $F_{\texttt{Eff}}$ being at least 30\% larger than the reported first-stage $F$ statistics.}
    % $tF$ procedure based on bootstrapped SEs may be overly conservative.
    \end{tabular}%
\end{table}

We summarize the main findings from our replication exercise in Table~\ref{tb:des}. The three issues we have identified are observed in all three journals included in the study. Based on these results, we believe that a significant portion of the IV results either lack credibility or do not provide new information beyond what is already provided by OLS regressions.


%\vspace{-1em}%

%\FloatBarrier



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recommendations} \label{conc}

In this paper, we replicate 70 IV designs in 67 studies published in three top journals in political science that employ IVs as one of the main identification strategies. We make two main contributions to the
literature. First, we show that many political science studies overestimate the strength of the IVs and underestimate the uncertainty of the 2SLS estimates. Specifically, we find that close to 19-30\% of designs are underpowered according to current best practice, and underestimation of uncertainties seem to be related to non-i.i.d. error processes in data, such as heteroscedasticity and clustering structures. Second, we observe significant disparities between 2SLS and OLS estimates, sometimes reaching magnitudes of tens or hundreds. We believe that this is primarily driven by a combination of weak instruments and failure of exogeneity, as well as publication bias, although other mechanisms such as HTE and measurement error may also contribute. Our findings suggest that 2SLS estimates in many existing political science studies, especially observational ones, likely have larger biases than naïve OLS estimates.
% Finally, we provide researchers with practical recommendations, including a ZFS test and a LTZ procedure, to avoid these pitfalls as much as possible.


% We find that researchers often overestimate the strength of their IVs and underestimate the uncertainties around the 2SLS estimates. When using a bootstrap procedure to obtain the uncertainties, we find many 2SLS estimates become uninformative---they are often statistically indistinguishable from the naïve OLS estimates and often times 0. Moreover, we show that the 2SLS estimates are often much larger in magnitude than the OLS estimates, and their difference is negatively correlated with the strength of the IVs. We argue that this is because weak instruments amplify the biases from exclusion restriction failures.

It is important to emphasize that IVs in experimental and observational
studies are fundamentally different. When researchers use IVs to account for non-compliance in randomized experiments, their unconfoundedness is guaranteed by design. While the exclusion restriction may fail, researchers can guard against this at the design stage, for example, by testing potential design effects through randomization \citep[pp. 140-141]{Gerber2012-fr}. Moreover, as transparency-enhancing practices, such as power analysis, placebo tests, and preregistration, become more common
in experimental studies (see, for example,
\citealt{mcdermott2014research}), they will likely reduce the risk of
improper use of IV designs.

% How do we avoid these pitfalls when using an IV approach?
Our findings suggest that using an IV strategy in an observational setting is much more challenging. Since unconfoundedness is not guaranteed by design, researchers have a greater burden of proof for the validity of IVs. On the one hand, truly random (and strong) instruments are rare; on the other hand, it is difficult to conduct placebo tests, such as the ZFS test, for the exclusion restriction after data collection. Additionally, researchers often cannot easily increase the sample size to obtain sufficient statistical power. To prevent misusing IVs in observational studies, we provide a checklist for researchers to consider when applying or considering applying an IV strategy with observational data (in the case of one endogenous treatment variable):
\begin{itemize}[leftmargin=*]\itemsep0em
    \item[] \textbf{Design}
    \item Prior to using an IV strategy, consider how selection bias may be affecting treatment effect estimates obtained through OLS. If the main concern is underestimating an already statistically significant treatment effect, an IV strategy may not be necessary. Very few studies currently do so.
    \item During the research design phase, consider whether the chosen instrument can realistically create random or quasi-random variations in treatment assignment while remaining excluded from the outcome equation.
    \item[] \textbf{Characterizing the first-stage}
    \item Calculate and report the effective $F$ statistic for the first stage, taking into account heteroscedasticity and clustering structure as needed.
    \item If $d$ and $z$ are continuous, plot $d$ against its predicted values $\hat{d}$ (with covariates and fixed effects already partialled out from both) and visually verify whether their relationship aligns with theoretical expectations.
    \item[] \textbf{Hypothesis testing and inference}
    \item \textit{Option 1. $t$-test after pretesting based on $F$.} Use the effective $F$ for pretesting; consider more conservative inferential methods such as \emph{bootstrap-t} and \emph{bootstrap-c} procedures.
    \item \textit{Option 2. $tF$-$cF$.} For cases with one treatment and one instrument, consider adjusting the critical values for the $t$-test based on the instrument's strength.
    \item \textit{Option 3. direct testing.} Use direct, weak-instrument-robust procedures, such as the AR test.
    \item Make sure to report estimation results from the first stage and the reduced form, including 95\% CIs of the coefficients, as they are informative of both the strength of the instruments and statistical power.
    \item[] \textbf{Additional diagnostics}
    \item If you expect the OLS results to be upward biased, be concerned if the 2SLS estimator yields much larger estimates.
    \item If there is good reason to believe that treatment effects on compliers are significantly larger in magnitude than those on non-compliers, explain this through profiling of these principal strata \citep{Abadie2003-sl,Marbach2020-ts}.
    \item If it is possible to identify an observational analogue of ``never takers'' or a subset of them, conduct a placebo test by estimating the effect of the instrument on the outcome of interest in this ZFS sample. Using results from the ZFS test, obtain LTZ IV estimates and CIs and compare them to the original estimates and CIs.  See the SM for a detailed example.
\end{itemize}

We develop an \texttt{R} package,
\href{https://github.com/apoorvalal/ivDiag}{ivDiag}, to systematically
implement the recommended procedures. We hope that our
recommendations, as well as the software routine, will address
concerns about using IVs in social science research and improve the
quality of estimation and inference, particularly when the IVs are not
generated through experiments.

% \subsubsection{\citet{acharya2016political}} % (fold)

% ABS(2016) study the political legacy of American slavery as observed
% from attitudinal measures from the ANES, using proportion of slaves in
% 1860 as the (likely endogenous) regressor of interest, and
% cotton-suitability as an instrument. We replicate their primary
% county-level results and report them in table ~\ref{table:abs2016_t4}.
% ABS also report the `ZFS' test in the appendix to their paper, arguing
% that their instrument (cotton suitability) is likely only predictive
% of the prevalence of slavery in the South \footnote{the states of
% Alabama, Arkansas, Georgia, Florida, Kentucky, Louisiana, Mississippi,
% Missouri, North and South Carolina, Tennessee, Texas, Virginia, and
% West Virginia} given historical crop cultivation patterns. Therefore,
% they argue that the first-stage effect can be expected to be zero (as
% shown in column (1) in table \ref{table:abs_zfs}), and therefore
% instrument validity would be suggested by a zero reduced-form effect
% in Northern states (columns 3, 5, 7).

% This makes the implementation of the LTZ method quite straightforward.
% We illustrate the diagnostics using a single outcome variable:
% proportion democrat in each county. First, we bootstrap the regression
% reported in column (2) of table~\ref{table:abs2016_t4} 10000 times and
% use it to compute bootstrapped standard errors, which we report in
% blue in figure~\ref{fig:abs_bootfig} alongside the conventional
% analytic sampling distribution in red. We use the coefficient for FAO
% Cotton Suitability in column (3) as $\mu_{\gamma}$ and estimate
% $\hat{\tau}$ and corresponding standard error and confidence intervals
% implied by eqn~\ref{eqn:gausdist}, and outline them in olive green in
% figure~\ref{fig:abs_bootfig}. We also report them alongside the
% authors' IV estimates in table~\ref{table:abs_LTZ}. Since the
% violation of the exclusion restriction is practically nonexistent, the
% two methods yield very similar estimates, which suggests that the IV
% estimate in ABS is fairly robust.

% \input{tables/abs_zfs}
% \input{tables/abs_LTZ}

% \begin{figure}[]
%   \centering
%   \includegraphics[width=.8\columnwidth, keepaspectratio]{graphs/ABS_facet_fig.pdf}
%   \caption{Distribution of IV coefficient for percent-democrat outcome, ABS(2016)}
%   \label{fig:abs_bootfig}
% \end{figure}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsubsection{\citet{goldstein2017cities}} % (fold)
% \label{ssub:goldstein2017cities}

% Goldstein and You (2017) study inter-governmental lobbying between
% cities and the federal government driven by preference-incongruence
% between a city and its parent state government. They instrument for
% cities' lobbying expenditure using a dummy variable for a direct
% flight from the city to Washington, DC, with the intuition that direct
% flights facilitate lobbying by allowing local government
% representatives to travel to DC and meet house members and senators.
% We report a replication of their primary specification for $\log$
% Earmark amount \citep[column 2, table 4]{goldstein2017cities} in
% table~\ref{table:gy2017_t4} \footnote{We get slightly larger estimates
% than GY(2017) because we use the only instrument they describe in
% their paper, while their replication code \texttt{table 4.do} uses
% both \texttt{direct\_flight\_dc diverge2\_e}, the latter of which is
% not discussed in the paper as an instrument. We suspect this might be
% a programming error.}. First, to evaluate the standard error, we
% bootstrap the regression reported in column (3) of
% table~\ref{table:gy2017_t4} 10000 times and use it to compute
% bootstrapped standard errors, which we report in blue in
% figure~\ref{fig:gy_bootfig} alongside the conventional analytic point
% estimate and confidence interval in red. We find that the bootstrap
% confidence interval is larger than the analytic interval.

% Stipulating the authors' mechanism for how the instrument increases
% lobbying, we identify a subset of states that are (loosely) within
% driving distance from DC, and treat it as the candidate
% `zero-first-stage' sample wherein the instrument likely has little
% effect in treatment assignment\footnote{we use the following states:
% DE, MD, PA, NJ, VA, WV. We also used a more expansive definition using
% the Northeast `Acela' Corridor, which has easy access to DC by rail,
% and get similar results.}. We then estimate the first-stage and
% reduced-form regressions within this sub-sample and report them in
% table~\ref{table:gy_zfs} columns 1 and 2. We find a large, significant effect of the
% instrument on the treatment (log lobbying spending) even within this
% subsample (in column 1), which casts doubt on the authors' posited
% mechanism by which the instrument influences treatment. Further, we
% find that the reduced-form effect of the instrument is quite large in this subsample,
% which further weakens the case for the exclusion restriction.

% To evaluate the consequences of this likely violation, we use the
% coefficient in column 2 as $\mu_{\gamma}$ and implement the LTZ
% method. We report the LTZ point estimate and confidence interval in
% figure~\ref{fig:gy_bootfig} in green and alongside the IV estimates in
% table~\ref{table:gy_LTZ}. The LTZ mean is large and negative, in stark
% contrast to the analytic point estimates, and the two sets of
% confidence intervals don't even overlap. The nonzero first stage in
% the candidate first-stage sample, combined with the fact that the IV
% and LTZ coefficients don't even agree on sign suggests that the
% exclusion restriction is very likely violated in this context. The
% exclusion restriction violations in this case are appear to be so
% severe as to overturn the interpretability of the primary
% specification.

% \footnotesize
% \input{tables/gy_zfs}
% \normalsize

% \input{tables/gy_LTZ}

% \begin{figure}[]
%   \centering
%   \includegraphics[width=.8\columnwidth, keepaspectratio]{graphs/GY_facet_fig.pdf}
%   \caption{Distribution of IV coefficient for ln-Earmark outcome, GY(2017)}
%   \label{fig:gy_bootfig}
% \end{figure}


% subsubsection goldstein2017cities (end)

% subsubsection  (end)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \pagebreak
\vspace{3em}
%\clearpage

\setstretch{1.5}
\bibliographystyle{apsr}
\bibliography{ivbib}

%    ###    ########  ########  ########  ##     ##
%   ## ##   ##     ## ##     ## ##     ##  ##   ##
%  ##   ##  ##     ## ##     ## ##     ##   ## ##
% ##     ## ########  ########  ##     ##    ###
% ######### ##        ##        ##     ##   ## ##
% ##     ## ##        ##        ##     ##  ##   ##
% ##     ## ##        ##        ########  ##     ##


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{supp}

\end{document}
