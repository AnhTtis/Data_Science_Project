% \documentclass[12pt]{article} % add titlepage param for separate

% \input{preamble}

% \begin{document}

\clearpage
\appendix
\onehalfspacing
\setcounter{page}{1}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{equation}{0}
\setcounter{footnote}{0}
\renewcommand\thetable{A\arabic{table}}
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand{\thepage}{A-\arabic{page}}
\renewcommand{\theequation}{A\arabic{equation}}
\renewcommand{\thefootnote}{A\arabic{footnote}}
%\setcounter{proposition}{0}

\section{Supplementary Materials -- Appendix A}
\bigskip
% \begin{center}
% {\large\bf How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice based on Over 60 Replicated Studies}
% \end{center}

% move to appendix





\noindent\hspace{0em}{\large\bf \underline{Table of Contents}}

{\bf
\begin{enumerate}\itemsep0ex
    \item[A.1.] Additional Replication Results
    \begin{enumerate}
    \item[A.1.1.] Replication Sample
    \item[A.1.2.] Comparison of Multiple $F$ Statistics
    \end{enumerate}\medskip
    \item[A.2.] Monte Carlo Evidence
    \begin{enumerate}
    \item[A.2.1.] Comparing $F$ Tests for Detecting Weak Instruments
    \item[A.2.2.] Explaining the 2SLS-OLS Discrepancy
    \end{enumerate}\medskip
    \item[A.3.] Evaluating IV Validity
    \begin{enumerate}
    \item[A.3.1] The ZFS Test and Modified Inference
    \item[A.3.2.] Simulation Evidence
    \item[A.3.3.] A Case Study
    \end{enumerate}\medskip
    \item[A.4.] Summary of Replicated Papers
\end{enumerate}
}

% \bigskip\noindent\textbf{\textit{Note:}} A 86-page \emph{Appendix-B} that details the replication results for each of the 64 IV design is available upon request (we did not upload it due to space limitations for supporting materials).

\thispagestyle{empty}
\setcounter{page}{0}
\thispagestyle{empty}

\clearpage

% \subsection{\large Popularity of IV Designs}

% In Figure~\ref{fig:pubs.year}, we show the popularity of IV designs over time. We find that IV strategies are well represented in papers published between 2010 and 2020 with an average of around 12 papers published each year in one of \emph{APSR}, \emph{AJPS}, or \emph{JOP}. The drop in 2020 is caused by the truncation of the sample when we stop searching for new studies in June 2020. Data availability has improved significantly over time, especially after 2016.

% \begin{figure}[!h]
% \centering
% \caption{Papers using instrumental variables published in the \emph{APSR}, \emph{AJPS}, and \emph{JOP}, by year. }\label{fig:pubs.year}
% \includegraphics[width=.9\columnwidth, keepaspectratio]{graphs/summary.pdf}
% \end{figure}

\clearpage

\subsection{\large Additional Information on the Replication Sample}

\subsubsection{Replication Sample}

Figure~\ref{fig:f.hist} plots the histograms of effective $F$ statistics using experiment-generated IVs (dark gray) and non-experimental IVs (light gray). The median effective $F$ for experimental and observational designs are 67.7 and 53.5, respectively.
\begin{figure}[!h]
\caption{Histogram of Effective $F$ Statistic}\label{fig:f.hist}
\centering\vspace{0em}
\includegraphics[width=0.8\textwidth]{graphs/F_hist.pdf}
\end{figure}

Figure~\ref{fig:overtime} shows the overtime change in the percentage of experimental studies (a), the median effective $F$ statistics (b), and the median ratio between 2SLS and OLS coefficients (c) in the replication sample.

\clearpage



\begin{figure}[!h]
\caption{Additional Information on the Sample}\label{fig:overtime}
\centering\vspace{0em}
\hspace{-1em}
\subfigure[Percentage of experimental studies]{\hspace{0em}\includegraphics[width=0.6\textwidth]{graphs/overtime_exper.pdf}}\\
\subfigure[Effective $F$ statistics (median)]{\hspace{0em}\includegraphics[width=0.6\textwidth]{graphs/overtime_F.pdf}
}\\
\subfigure[Ratio between 2SLS and OLS coefficients]{\hspace{0em}\includegraphics[width=0.6\textwidth]{graphs/overtime_ratio.pdf}}
\end{figure}


\clearpage

\subsubsection{Comparison of Multiple $F$ Statistics}

Figure~\ref{fig:f.report} compares the reported and replicated first-stage partial $F$ statistics (for studies that have reported the $F$ statistics). The replicated $F$ statistics are based on the authors' chosen model specifications and variance estimators in 2SLS estimation. The discrepancy arises from the fact that some authors report the first-stage $F$ statistic based on a different variance estimator than the one used in the 2SLS estimation. In the paper, we use the replicated ones to maintain consistency.
\begin{figure}[!h]
\caption{Reported vs. Replicated $F$ Statistics}\label{fig:f.report}
\begin{center}
\vspace{0em}
\includegraphics[width=0.65\textwidth]{graphs/F_report_replicate.pdf}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Circles and triangles represent applications with and without a clustering structure, respectively. Studies that do not report $F$ statistics are not shown.}
\end{figure}
\clearpage

In Figure~\ref{fig:f.compare}, we compare the traditional $F$ statistics (based on classic analytic SEs), the Huber White robust $F$ statistics, the effective $F$ statistics (robust or cluster-bootstrap SEs) and (cluster-)bootstrapped $F$ statistics. It shows that (cluster-)bootstrapped $F$ statistics are usually the most conservative (smallest). Circles and triangles represent applications with and without a clustering structure, respectively.

\begin{figure}[!ht]
\caption{Comparison of Different $F$ Statistics}\label{fig:f.compare}
\begin{center}\vspace{0em}
\hspace{-1em}\subfigure[\scriptsize Traditional $F$ vs. H.W. Robust $F$]{\hspace{0em}\includegraphics[width=0.44\textwidth]{graphs/F_compare1.pdf}}\hspace{1em}
\subfigure[\scriptsize H.W. Robust $F$ vs. Effective $F$]{\hspace{0em}\includegraphics[width=0.44\textwidth]{graphs/F_compare2.pdf}
}\\
\subfigure[\scriptsize Effective $F$ vs. Bootstrapped $F$]{\hspace{0em}\includegraphics[width=0.44\textwidth]{graphs/F_compare3.pdf}}\hspace{1em}
\subfigure[\scriptsize Traditional $F$ vs. Bootstrapped $F$]{\hspace{0em}\includegraphics[width=0.44\textwidth]{graphs/F_compare4.pdf}}
\end{center}\vspace{-1em}
{\footnotesize\textbf{\textit{Note:}} Circles and triangles represent applications with and without a clustering structure, respectively.}
\end{figure}


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\large Monte Carlo Evidence}


\subsubsection{Comparing $F$ Tests for Detecting Weak Instruments}

We conduct a simulation study with a clustered DGP in order to evaluate the relative performance of analytic and bootstrap $F$ tests to detect weak instruments. We simulate data from the following DGP
\begin{align*}
 \text{clustered instrument and error components } \; \nu_j, \eta_j & \sim \Normal{0, 0.5} \\
 \text{instrument }                                \;         z_i   & \sim \Normal{0, 1} + \nu_j \\
 \text{error }                                     \;     \epsi_i   & \sim \Normal{0, 1} + \eta_j \\
 \text{endogenous variable } \;                                 x_i & =    \pi z_i + \epsi_i
\end{align*}
with errors and instrument components drawn from $J$ clusters. This
DGP ensures that the data has dependent structure within each cluster $j$. We then evaluate the strength of the instrument analytically by
computing the t-statistic for $H_0: \pi = 0$, or by using the
corresponding bootstrap statistic $\frac{\pi^2}{\wh{\sigma}^2}$ where
$\wh{\sigma}^2$ is the bootstrap estimate of the variance of $\pi$. We
evaluate the analytic and bootstrap $F$ statistics for various values of
$\pi$ and $J$ for 100 replications of the above DGP in Figure~(\ref{fig:F_sim}).

\begin{figure}[!h]
\caption{Comparisons of $F$ Statistics}\label{fig:F_sim}
\centering\vspace{0em}
\subfigure[Cluster-bootstrap $F$ statistic vs. Huber-White (non-clustered) $F$ statistic]{\includegraphics[width=0.8\textwidth]{graphs/Fstats_scatter_noclust.pdf}}
\subfigure[Cluster-bootstrap $F$ statistic vs. cluster-robust analytic $F$ statistic ($F_{\texttt{Eff}}$)]{\includegraphics[width=0.8\textwidth]{graphs/Fstats_scatter_clust.pdf}}
\end{figure}

As seen in panel A, when robust analytic standard errors ignore the
clustered structure, they vastly over-estimate the strength of the
instrument relative to the block-bootstrap, with both ``few'' (10) and
``many'' (50) clusters and with ``strong'' ($\pi = 0.5$) and ``weak'' ($\pi
= 0.001$) instruments. With appropriate clustered analytic SEs,
however, the $F$ statistic is typically comparable to the bootstrap
based equivalent (panel B), although the bootstrap F is marginally
more conservative with a small number of clusters and weak instrument.

In summary, we find that cluster-bootstrap $F$ statistic and the
cluster-robust F statistic, which is equivalent to the ``effective'' $F$
\citep{Olea2013-pa} in just-identified settings such as this one, are comparable in detecting weak instruments, and recommend reporting these statistics in applied settings. We also recommend reporting Anderson-Rubin confidence intervals for the IV coefficient, as it is robust to arbitrarily weak instruments \citep{andrews2019weak,kang2020ivmodel}.


\newpage

\subsubsection{Explaining the 2SLS-OLS Discrepancy}

In this section, we conduct Monte Carlo exercises to explore potential
causes of the discrepancy between 2SLS and OLS estimates observed in
the replication data. We consider three causes: (1) violations of the
exclusion restriction (A2), (2) publication bias, and (3)
heterogeneous treatment effects (HTE). Below is our data-generating
process (DGP):

\vspace{-1em}\begin{align*}
y_{i} & = 5 + \beta_{i} x_{i} + \mu z_{i} + u_{i} + b_{i}\\
x_{i}^{*} & = \delta_{i} z_{i} + (1 - \delta_{i}) a_{i} + 0.2 v_{i}\quad\text{and}\quad  \delta_{i} = \max(\min(\kappa_{i}\pi_{i}, 1), 0)\\
x_{i} & = x_{i}^{*},\ z_{i}\overset{i.i.d.}{\sim} N(0,2)\qquad\text{(continuous-continuous case)}\\
\text{or}\qquad x_{i} &= 1\{x_{i}^{*}>0\},\
z_{i}\overset{i.i.d.}{\sim} \text{Bern}(0.5)\qquad\text{(binary-binary case)}
\end{align*}

in which $z$ is the instrument, $x$ is the treatment, and $y$ is the outcome. We consider two scenarios: (1) both $x$ and $z$ are continuous, and (2) both are binary. Correlated errors $\begin{bmatrix}u_{i}\\v_{i}\end{bmatrix}\overset{i.i.d.}{\sim} N\left(\begin{bmatrix} 0 \\ 0\end{bmatrix}, \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1\end{bmatrix}\right)$; $a_{i}\overset{i.i.d.}{\sim} N(0,1)$, $b_{i}\overset{i.i.d.}{\sim} N(0,1)$ are i.i.d. errors. We use $\kappa$ to control the strength of the instrument. HTE can be generated by $\begin{bmatrix}\beta_{i}\\\pi_{i}\end{bmatrix}\overset{i.i.d.}{\sim} N\left(\begin{bmatrix} 2 \\ 1\end{bmatrix}, \sigma_{h}^{2}\begin{bmatrix} 1 & \lambda \\ \lambda & 0.5\end{bmatrix}\right)$, in which $\sigma_{h}$ controls the amount of  heterogeneity in $\beta_{i}$ and $\pi_{i}$ while $\lambda$ controls the correlation between the first stage and reduced form coefficients. $\delta_{i}$ is limited to be in $[0, 1]$. When $\lambda > 0$, it means that a unit's treatment effect is positively correlated with its responsiveness to the IV.%
\footnote{For example, under selection-on-gains type settings, which are typically considered in generalized Roy models underlying MTE approaches to IV.}  The sample size is fixed at 200.

Under constant treatment effect ($\sigma_{h} = 0$) and with a valid instrument ($\mu = 0$), the expected value of $\hat\beta_{2SLS} / \hat\beta_{OLS}$ is $0.74$ for the continuous-continuous case and $0.57$ for the binary-binary case. We consider four scenarios sequentially:
\begin{enumerate}
    \item Violations of Assumption 2 are captured by $\mu \neq 0$ (failure of the exclusion restriction).
    \item Publication bias can be simulated by dropping the cases in which the 2SLS estimates are statistically insignificant at the 5\% using a conventional $t$ test.
    \item HTE is generated by setting $\sigma_{h} = 0.05$ and $\lambda = 0.7$, i.e., $\beta_{i}$ and $\pi_{i}$ are highly correlated.
    \item The combination of HTE and publication bias.
\end{enumerate}

\begin{figure}[!h]
\caption{Consequences of Exclusion Restriction Failure\\under Constant Effect}\label{fig:sim.bias}
\centering\vspace{0em}
\subfigure[\scriptsize Continuous-Continuous Case: w/o and w/ exclusion restriction failure]{\hspace{0em}\includegraphics[width=0.45\textwidth]{graphs/sim_cont.pdf}\hspace{1em}
\includegraphics[width=0.45\textwidth]{graphs/sim_cont_biased.pdf}
}
\subfigure[\scriptsize Binary-Binary Case: w/o and w/ exclusion restriction failure]{\hspace{0em}\includegraphics[width=0.45\textwidth]{graphs/sim_binary.pdf}\hspace{1em}
\includegraphics[width=0.45\textwidth]{graphs/sim_binary_biased.pdf}
}\vspace{-1em}
\end{figure}

\paragraph*{Violating Assumption 2.} The results for Scenario ~1 are
shown in Figure~\ref{fig:sim.bias}. Each dot represents one simulated
sample. Figure~\ref{fig:sim.bias} shows that, in both
continuous-continuous and binary-binary setups, when the treatment
effect is constant ($\beta_{i} = \beta, \pi_{i} = \pi$), in
expectation, there is no mechanical negative relationship between the
correlation coefficient between $d$ and $\hat{d}$ and the 2SLS-OLS
discrepancy (left panels in both subfigures). However, when the
exclusion restriction fails, e.g., $\mu=1$(right panels in both
subfigures), a strong negative correlation appears. These results
support our argument in the paper that a weak first stage amplifies
the bias from the failure of Assumption 2.

\begin{figure}[!h]
\caption{Consequences of Publication Bias\\under Constant Treatment Effect}\label{fig:sim.pubbias}
\centering\vspace{0em}
\subfigure[\scriptsize Continuous-Continuous Case: w/o and w/ publication bias]{\hspace{0em}\includegraphics[width=0.45\textwidth]{graphs/sim_cont.pdf}\hspace{1em}
\includegraphics[width=0.45\textwidth]{graphs/sim_cont_sig.pdf}
}
\subfigure[\scriptsize Binary-Binary Case: w/o and w/ publication bias]{\hspace{0em}\includegraphics[width=0.45\textwidth]{graphs/sim_binary.pdf}\hspace{1em}
\includegraphics[width=0.45\textwidth]{graphs/sim_binary_sig.pdf}}\vspace{-1em}
\end{figure}

\paragraph*{Publication bias.} Figure~\ref{fig:sim.pubbias} illustrates the consequences of publication bias (Scenario 2), where statistically insignificant results are omitted (right panels). The left panels are identical to the left panels in Figure~\ref{fig:sim.bias}. In the binary-binary case, we observe a moderate negative correlation; however, this correlation is much weaker than those caused by exclusion restriction failures.


\paragraph*{HTE and publication bias.} Finally, we investigate the consequences of HTE (Scenario 3) and its interaction with publication bias (Scenario 4). Figures~\ref{fig:sim.hte} shows results under HTE, i.e., $\sigma_{h} > 0$ and $\lambda = 0.7$ ($\beta_{i}$ and $\pi_{i}$ are highly positively correlated). On the logarithmic scale, the correlation is almost nonexistent (left panels in Figure~\ref{fig:sim.hte}).
\begin{figure}[!ht]
\caption{Consequences of Publication Bias under HTE}\label{fig:sim.hte}
\centering\vspace{0em}
\subfigure[\scriptsize Continuous-Continuous Case: w/o and w/ publication bias]{\hspace{0em}\includegraphics[width=0.45\textwidth]{graphs/sim_cont_hte.pdf}\hspace{1em}
\includegraphics[width=0.45\textwidth]{graphs/sim_cont_hte_sig.pdf}
}
\subfigure[\scriptsize Binary-Binary Case: w/o and w/ publication bias]{\hspace{0em}\includegraphics[width=0.45\textwidth]{graphs/sim_binary_hte.pdf}\hspace{1em}
\includegraphics[width=0.45\textwidth]{graphs/sim_binary_hte_sig.pdf}
}\vspace{-1em}
\end{figure}
When we revert to the original scale, we do observe a small to moderate negative correlation in both continuous-continuous and binary-binary cases (figures not shown). When we further introduce publication bias, we begin to see weak negative correlations between the first stage $\rho$ and the 2SLS-OLS discrepancy on the logarithmic scale, especially in the binary-binary case. However, their magnitudes are much smaller than what we observed in Figure~\ref{fig:sim.bias} under the exclusion restriction failure. This suggests that the observed strong negative relationship in the paper is unlikely to be solely explained by HTE and different levels of responsiveness to the IV.

In summary, the Monte Carlo exercises demonstrated that the strong negative correlations between the first stage $\rho$ and the 2SLS-OLS discrepancy are most likely caused by violations of Assumption 2. Other factors, such as publication bias and HTE, may also play a role.



\clearpage


\subsection{\large Evaluating the Exogeneity Assumption}

Assumption 2 is a strong and generally untestable assumption that underlies the validity of the instrument; indeed, researchers typically spend considerable effort arguing for both unconfoundedness and the exclusion restrictions in their particular setting. However, some placebo tests have recently become popular as a way to argue for the validity of identification assumptions in causal designs \citep{Eggers2021-qk}, especially in observational settings where the choice of IV is guided by detailed domain knowledge. \citet{bound2000compulsory} suggest first using an auxiliary regression on a subsample where the IV is not expected to influence treatment assignment, known  as ``zero-first-stage'' (ZFS) tests. The primary intuition is that in a subsample that one has a strong prior that the first stage is zero---hence, they are ``never takers,'' to use the language of the LATE framework---the reduced form effect should also be zero if Assumption 2 is satisfied. In other words, motivated by a substantive prior that the first-stage effect of the IV is likely zero for a subsample of the population (henceforth, the ``ZFS subsample''), the researcher then proceeds to show that the reduced-form coefficient for the IV (by regression $Y$ on $Z$) is approximately zero \emph{in the ZFS subsample}, which is suggestive evidence in favor of IV validity. Most observational instruments ought to yield some ZFS subsample based on substantive knowledge of the assignment mechanism.

% This is because if one has strong reason to believe that the
% first-stage effect of $Z$ on $X$ is zero in the ZFS subsample, and
% it is shown that reduced-form effect of $Z$ on $Y$ is also zero, it is
% reasonable to conclude that $Z$ has no direct effect on $Y$
% independent of treatment assignment $X$, i.e. the exclusion
% restriction is satisfied.

This style of placebo is particularly popular in studies of historical
political economy, where particular historical or geographic features
are argued to be valid instruments for treatment assignment, and thus
they are unlikely to be driving treatment assignment outside a
specific context. For example, \citet{nunn2008long} studies the
effects of the slave trade on modern-day development in Africa using
sailing distance from each country to the nearest locations of demand
for slave labor as an IV for the normalized number of slaves taken.
The author then argues that distance to demand locations in the New
World are likely to be a valid IV by using a placebo test that the
first-stage effect (the IV regressed on the outcome, modern-day GDP)
is approximately zero for countries outside Africa, where the posited
mechanism (that places close to demand locations exported more slaves
only in the transatlantic slave trade) has no traction, thereby
providing a candidate ZFS sample. In a related paper,
\citet{nunn2011slave} use the same strategy to show that distance to
slave-trade ports does not predict modern-day trust attitudes in the
Asiabarometer, while they do in the Afrobarometer (which is the
primary study population). \citet{acharya2016political} perform a
similar exercise where they believe that their instrument (cotton
suitability) predicts the treatment (slaves per capita) in the
Southern States but not the Northern states, and therefore find that
the reduced form effect of cotton suitability on modern day racial
attitudes is approximately zero in the Northern states.

\subsubsection{The ZFS Test and Modified Inference}

While this is a useful heuristic check that we advise most
observational IV papers adopt, it is an informal test and provides no
debiasing procedure to correct potentially biased IV estimates.
\citet{VanKippersluis2018-kn} suggest that the ZFS test can be
fruitfully combined with the ``plausibly exogenous'' method suggested
by \citet{Conley2012-mu} (henceforth, CHR \citeyear{Conley2012-mu}).
To illustrate the method, we first rewrite the IV simultaneous
equations in CHR (\citeyear{Conley2012-mu})'s notation:

\begin{equation}
Y  = X \beta + Z \gamma + \varepsilon;\quad X  = Z \Pi + \nu,\label{eqn:conley_eqn1}
\end{equation}

where $Z$ also enters the structural equation, and the exclusion
restriction amounts to a dogmatic prior that $\gamma = 0$. CHR
(\citeyear{Conley2012-mu}) suggest that this assumption can be
relaxed, and replaced with a user-specified assumption on a plausible
value, range, or distribution for $\gamma$ depending on the
researcher's beliefs regarding the degree of exclusion restriction
violation. They propose three different approaches for inference that
involve specifying the range of values for $\gamma$, a prior
distributional assumption for $\gamma$, and a fully Bayesian analysis
that requires priors over all model parameters and corresponding
parametric distributions. We focus on the second method, which CHR
(\citeyear{Conley2012-mu}) call the ``local to zero'' (LTZ)
approximation because of its simplicity and transparency. The LTZ
approximation considers ``local'' violations of the exclusion
restriction\footnote{LTZ asymptotics consider a sequence of constants
$\gamma = C/\sqrt{N}$ for some constant $C$ and sample size $N$}   and
requires a prior over $\gamma$ alone. CHR (\citeyear{Conley2012-mu})
show that replacing the standard assumption that $\gamma = 0$ with the
weaker assumption that $\gamma
\sim \mathbb{F}$, a prior distribution, implies distribution for $\wh{\beta}$ in Equation~(\ref{eqn:asymdist}).
\begin{align}
\wh{\ve{\beta}} &\sim^a \Normal{\ve{\beta}, \mathbb{V}_{2SLS}} +
\Mat{A} \gamma \; \; \text{where} \; \Mat{A} \equiv (\Mat{X}' \Mat{Z}
(\Mat{Z} '\Mat{Z})^{-1} \Mat{Z}' \Mat{X})^{-1} \Mat{X}'\Mat{Z}
\label{eqn:asymdist}\\
\wh{\ve{\beta}} &\sim^a \Normal{\ve{\beta} + \Mat{A} \mu_\gamma ,
  \mathbb{V}_{2SLS} + \Mat{A} \Mat{\Omega} \Mat{A}'} \label{eqn:gausdist}
\end{align}
where the original 2SLS asymptotic distribution is inflated by the additional term. While a simulation-based approach can be used to implement Equation~(\ref{eqn:asymdist}) for an arbitrary distribution for $\gamma$, the distribution takes its most convenient form when one uses a Gaussian prior over $\gamma \sim \Normal{\mu_\gamma, \Omega_\gamma}$, which simplifies Equation~(\ref{eqn:asymdist}) to Equation~(\ref{eqn:gausdist}), with a posterior being a Gaussian centered at $\ve{\beta} + \Mat{A} \mu_\gamma$.

CHR (\citeyear{Conley2012-mu}) suggest that researchers use domain
knowledge to choose $\mu_{\gamma}, \Omega_{\gamma}$, since they often
hold strong priors about instruments anyway (which presumably
motivates the choice of the instrument). \citet{VanKippersluis2018-kn}
suggest that a principled method to choose $\mu_{\gamma}$ is to
estimate Equation~(\ref{eqn:conley_eqn1}) on the ZFS population
(wherein $\Pi$ is assumed to be zero), and use this estimate
$\wh{\gamma}_{ZFS}$ as $\mu_{\gamma}$. This approach combines the
informal ZFS test with the plausibly exogenous method in a
straightforward manner, and software to implement it is available in
both \texttt{R} (accompanying this paper) and \texttt{STATA}
\citep{RePEc:boc:bocode:s457832}. We begin with a simulation-based
illustration and illustrate the application of this method to a
published empirical paper next.

\subsubsection{Simulation Evidence}

In this subsection, we demonstrate the LTZ method when the exclusion restriction is not satisfied. Consider the following DGP,
\begin{align*}
Y_i  =& \beta_i D_i + \gamma Z_i +
\varepsilon_i\\
D_i =& \mathbf{1}\{D_i^* > 0\}\\
D_i^* =& \alpha_i + \pi_i Z_i + \varepsilon_i
\end{align*}
in which $Z_i \sim \text{Bernoulli}(0.5)$ is a binary instrument,
$\pi_i \sim \Unif{1.5, 2.5}$, $\alpha_i \sim \Normal{-1, 1}$,
$\varepsilon_i  \sim \Normal{0, 1}$, $\beta_i  \sim \Normal{1, 0.25}$.
We generate $Y_i$ with $Z_i$ directly entering the structural
equation, which allows us to vary the magnitude of the exclusion
restriction violation. We then estimate $\wh{\beta}_{2SLS}$ using
conventional two-staged-least-squares on this data. As we vary
$\gamma$, $\hat{\beta}_{2SLS}$ is inconsistent for all values except
when $\gamma = 0$. We set $\pi = 0$ for the last 20\% observations of
the simulated data (the ZFS subsample). We then estimate the
reduced-form regression on this (known) subsample and use the
coefficient as a prior for $\mu_{\gamma}$, and compute the LTZ IV
estimate.

\begin{figure}[!htb]
  \centering
\caption{IV and LTZ Estimates for Varying $\gamma$}
\includegraphics[width=.7\columnwidth, keepaspectratio]{graphs/sims_zfs.pdf}
  \label{fig:LTZ_sims}
\end{figure}

Figure~\ref{fig:LTZ_sims} shows, unlike the 2SLS estimator (blue), the
LTZ estimator (orange) uncovers the true value of $\beta = 2$ even for
large degrees of exclusion restriction violations (large $|\gamma|$).


\subsubsection{A Case Study}

We illustrate the diagnostics described above by applying it to the
IV analysis in \citet{guiso2016long} (henceforth
GSZ \citeyear{guiso2016long}), who revisit
\citet{leonardi2001making}'s conjecture that Italian cities that
achieved self-government in the Middle Ages have higher modern-day
levels of social capital. More specifically, they study the effects of
free city-state status on social capital as measured by the number of
non-profits and organ donations per capita, and a measure of whether
students cheat in mathematics.


\begin{figure}[!ht]
\begin{center}
\caption{Table 6 in \citet{guiso2016long}}\label{fig:GSZ.t6}
\includegraphics[width=0.8\textwidth, keepaspectratio]{graphs/GSZ_t6.png}
\end{center}\vspace{-1em}
{\footnotesize\textit{\textbf{Note:}} ``Ease of coordination'' is the IV ``Bishop in city.'' We replicated columns (I), (II), (IV), and (V).}
\end{figure}

\begin{table}[!htbp]
\centering\small
\caption{\label{tab:gsz_tab6} Replication of GSZ (\citeyear{guiso2016long}) Table 6\\Reduced Form Regressions}
%\resizebox{1\textwidth}{!}{
\begin{tabular}{lccccc}
\hline\hline
& \multicolumn{2}{c}{North} & & \multicolumn{2}{c}{South (ZFS)}\\ \cline{2-3} \cline{5-6}
\emph{Outcome Variables} & Nonprofit & Organ Donation & &  Nonprofit & Organ Donation \\
&(1) & (2) &  & (3) &  (4) \\
\hline   \\
Bishop (IV) &1.612 & 0.472  &&  0.178 & 0.189 \\
  &(0.219) & (0.047)  &&  (0.137) & (0.065)\\
\\ \hline
Observations & 5,357 & 5,535  && 2,175 & 2,178\\
\hline\hline\multicolumn{6}{l}{\footnotesize\textbf{\textit{Note:}} Bootstrapped SEs are in the parentheses. See Figure A4 in the SM for the original table.}
\end{tabular}
%}
\vspace{-1em}
\end{table}


GSZ (\citeyear{guiso2016long}) use a dummy for whether the city was the seat of a bishop in the Middle Ages, based on historical accounts of coordination preceding commune formation in the Middle Ages as an IV for the ``free-city experience'' (Section 5). They argue that conditional on a host of geographic covariates, this IV, a bishop seat, influences contemporary social capital solely through its increasing the likelihood of commune formation. As suggestive evidence for the validity of their instrument, they estimate the reduced-form effect of medieval bishop presence of contemporary social capital measures separately in the north (where the IV is conjectured to have an effect) and the south (where it is conjectured to be irrelevant). They fail to reject the null of no effects in the south, conclude that the IV appears to have face validity, and proceed to use bishop presence as an IV for their IV estimates.
\begin{figure}[!ht]
  \centering
  \caption{IV Coefficients for Non-profits and Organ Donation}
  \includegraphics[width=.7\columnwidth, keepaspectratio]{graphs/GSZ_facet_fig_wide.pdf}
  \label{fig:gsz_bootfig}
\end{figure}

We begin by calculating the first-stage partial $F$ statistic based on bootstrapped SEs for the north sample, which is 67.3. Because there were no ``free cities'' in the south, the $F$ statistic for the south is zero by definition. We then replicate their reduced-form estimates in Table~\ref{tab:gsz_tab6}. The separate north and south reduced-form estimates in GSZ (\citeyear{guiso2016long}) can be readily used for the LTZ test described above. The authors substantively believe that the south is a ZFS sample where bishop presence is irrelevant for treatment assignment,\footnote{The authors claim this indirectly by reporting the reduced form effects separately for the north and south subsamples in Table 6, and state that since the reduced form is attenuated in the south, this justifies the use of bishop presence as an IV (p. 1427).} we can use the reduced-form estimates of 0.178 and 0.189 in the south for non-profits per capita and organ donation (columns 3-4 in Table~\ref{tab:gsz_tab6}) as the prior $\mu_\gamma$ for the direct effect of the IV on the outcome. Finally, we report the analytic, bootstrap, and LTZ IV results in Figure~\ref{fig:gsz_bootfig}.  We find that conventional robust SEs understate the uncertainty of the estimates relative to the bootstrap and that accounting for direct effect using LTZ attenuates GSZ (\citeyear{guiso2016long})'s estimates somewhat and substantially increases the SE of the estimate for the non-profit outcome. For organ donation, however, where we suspect a violation of Assumption 2 because the reduced form effect is statistically distinguishable from zero, the use of the LTZ method to account for this exclusion restriction violation yields a smaller and substantially more uncertain estimate whose CI contains 0. This example shows how researchers may take advantage of the ZFS test and the LTZ technique to gauge the robustness of their findings based on an IV strategy.

\FloatBarrier


%\pagestyle{empty}

\begin{flushleft}
    \input{appendixtable}
\end{flushleft}
%\pagestyle{plain}


% \setstretch{1.5}
% \bibliographystyle{apsr}
% \bibliography{ivbib}


% \end{document}
