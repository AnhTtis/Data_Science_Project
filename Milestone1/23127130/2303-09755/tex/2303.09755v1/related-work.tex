\section{Related work}

\subsection{Flaky test detection}
Several techniques have been proposed to detect flaky tests, most of which determine flakiness by observing transitions of test outcome across multiple runs \cite{bell2018deflaker,dutta2020detecting}. There are two main approaches used for detecting flaky tests: static techniques that rely only on analyzing the test code without actually executing tests or dynamic techniques that involve the execution of tests \cite{rasheed2022test}.
Most of the existing tools focus on specific causes of test flakiness, such as concurrency (e.g., Shaker \cite{silva2020shake}) and test order-dependency (e.g., iFixFlakies \cite{shi2019ifixflakies}). There have been recent attempts to build lightweight static approaches for flaky test prediction that aim to avoid or minimize test reruns \cite{alshammari2021flakeflagger,fatima2022flakify}.

\subsection{Instrumentation and flakiness}
Wing et al. \cite{wing2019rootcause} used instrumentation to record runtime properties for root-causing flaky tests. They reported that their instrumentation could change runtime behaviour and thus decrease or increase test flakiness. In their study, runtime overhead from instrumentation was observed to affect the reproducibility of flaky tests (by executing a random sample from 59 flaky tests, two tests were flaky only with instrumentation and three were flaky without instrumentation). Ivankovi\'{c} et al. \cite{ivankoviFSE2019} reported that flakiness due to coverage instrumentation is a common reason for failed coverage computation, which is manifested by performance failure or increased flakiness of non-deterministic tests. Tengeri et al. \cite{tengeriSANER16} reported cases where coverage instrumentation changes the behaviour of tests and their results. Finally, Dietrich et al. \cite{dietrichflaky} used instrumentation to intercept network errors to control flakiness caused by dependency on network connectivity.
