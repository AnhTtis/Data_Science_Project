\section{Results and Discussion}

\subsection{Experimental Results}
Detailed results for the experiments are shown below. Table~\ref{tab:results} shows the results for the experiments indicating flakiness counts. This includes the count of test outcomes (success, failure/error and flaky) and test runtimes (rt.) for each configuration (i.e., instrumentation). The numbers are averages for 20 runs.  Table~\ref{tab:scores} shows the flakiness scores discussed in Section \ref{sec:flaky-score} (configurations with zero values are omitted for brevity).

\begin{table}[]
\caption{Test results for all configurations, the numbers reported are the tests resulting in success, failure,  error or  skipped. Tests are reported across \textbf{20 test runs}. Numbers in brackets means variation across runs and report the mean. The number of  flaky tests across those runs are reported as well.  Runtimes are in seconds (rt(s)).}
\label{tab:results}
\centering
 \resizebox{\columnwidth}{!}{
	\begin{tabular}{|l|r|r|r|r|r|r|}
		\hline
		\textbf{program} & \textbf{config} & \textbf{succ.} & \textbf{fail/err} & \textbf{skip} & \textbf{flak.} & \textbf{rt.} \\
		\hline
\multirow{6}{*}{\textit{ChronicleQueue}}&baseline & 664 & 1 & 28 & 0 & (179.2) \\
 & opentele & (663.9) & (1.1) & 28 & 1 & (178.8) \\
 & jacoco & 664 & 1 & 28 & 0 & (251.9) \\
 & intelli & 664 & 1 & 28 & 0 & (215.2) \\
 & jfr & 664 & 1 & 28 & 0 & (178.8) \\
 & elastic & 663 & 2 & 28 & 9 & (190.0) \\
		\hline

\multirow{6}{*}{\textit{CorfuDB}}&baseline & (771.4) & (24.6) & 5 & 15 & (2784.2) \\
 & opentele & (768.7) & (27.2) & 5 & 38 & (3018.5) \\
 & jacoco & (773.0) & (23.0) & 5 & 10 & (2697.5) \\
 & intelli & (773.1) & (22.8) & 5 & 12 & (2664.9) \\
 & jfr & (769.2) & (26.8) & 5 & 74 & (2632.1) \\
 & elastic & (765.2) & (30.8) & 5 & 75 & (2685.6) \\
		\hline

\multirow{6}{*}{\textit{azure-iot-sdk-java}}&baseline & (4302.2) & (0.8) & 1 & 17 & (102.3) \\
 & opentele & (4302.8) & (1.5) & 1 & 16 & (102.6) \\
 & jacoco & (4303.2) & 0 & 1 & 0 & (102.8) \\
 & intelli & 4,305 & 0 & 1 & 0 & (102.5) \\
 & jfr & (4304.9) & (0.0) & 1 & 1 & (103.1) \\
 & elastic & (4303.2) & (0.7) & 1 & 13 & (103.8) \\
		\hline

\multirow{6}{*}{\textit{exhibitor}}&baseline & 106 & 0 & 0 & 0 & (84.4) \\
 & opentele & 106 & 0 & 0 & 0 & (88.4) \\
 & jacoco & 106 & 0 & 0 & 0 & (86.1) \\
 & intelli & 106 & 0 & 0 & 0 & (85.1) \\
 & jfr & (105.9) & (0.1) & (0.1) & 1 & (84.4) \\
 & elastic & 106 & 0 & 0 & 0 & (84.9) \\
		\hline

\multirow{6}{*}{\textit{flow}}&baseline & (4390.5) & (0.5) & 9 & 1 & (250.2) \\
 & opentele & (4390.5) & (0.5) & 9 & 1 & (265.3) \\
 & jacoco & (4389.5) & (1.5) & 9 & 1 & (231.8) \\
 & intelli & (4389.4) & (1.5) & 9 & 2 & (239.3) \\
 & jfr & (4390.5) & (0.5) & 9 & 1 & (245.9) \\
 & elastic & (4389.5) & (1.5) & 9 & 1 & (250.1) \\
		\hline

\multirow{6}{*}{\textit{karate}}&baseline & 604 & 1 & 0 & 0 & (57.5) \\
 & opentele & 604 & 1 & 0 & 0 & (57.3) \\
 & jacoco & (603.9) & (1.1) & 0 & 1 & (53.3) \\
 & intelli & 604 & 1 & 0 & 0 & (54.1) \\
 & jfr & 604 & 1 & 0 & 0 & (53.8) \\
 & elastic & 604 & 1 & 0 & 0 & (53.9) \\
		\hline

\multirow{6}{*}{\textit{killbill}}&baseline & (1017.9) & (0.5) & (24.6) & 65 & (133.6) \\
 & opentele & (1017.0) & (0.5) & (25.4) & 65 & (124.2) \\
 & jacoco & (1029.8) & (0.4) & (12.9) & 60 & (100.4) \\
 & intelli & (1019.6) & (0.5) & (22.9) & 65 & (124.4) \\
 & jfr & (1010.2) & (0.5) & (32.4) & 65 & (140.1) \\
 & elastic & (1017.4) & (0.5) & (25.1) & 65 & (121.6) \\
		\hline

\multirow{6}{*}{\textit{mockserver}}&baseline & 2,870 & 0 & 0 & 0 & (92.2) \\
 & opentele & (2867.9) & (1.1) & 0 & 1 & (92.7) \\
 & jacoco & (2869.8) & (0.1) & 0 & 3 & (92.7) \\
 & intelli & 2,870 & 0 & 0 & 0 & (91.9) \\
 & jfr & 2,870 & 0 & 0 & 0 & (92.7) \\
 & elastic & (2869.9) & (0.1) & 0 & 1 & (92.8) \\
		\hline

\multirow{6}{*}{\textit{ripme}}&baseline & (87.2) & (72.8) & 67 & 6 & (382.6) \\
 & opentele & (86.8) & (73.2) & 67 & 10 & (378.0) \\
 & jacoco & (87.7) & (72.3) & 67 & 4 & (361.6) \\
 & intelli & (88) & (72) & 67 & 2 & (354.4) \\
 & jfr & (87.6) & (72.4) & 67 & 6 & (367.5) \\
 & elastic & (87.0) & (73.0) & 67 & 6 & (377.7) \\
		\hline

	\end{tabular}}
\end{table}
\begin{table}[]
\caption{Flakiness scores (mean) over 20 runs for each program/configuration (0 values omitted for brevity).}
\label{tab:scores}
\center
	\begin{tabular}{|l|l|r|}
		\hline
			 \textbf{program} & \textbf{config} & \textbf{flakiness-score} \\ \hline
\textit{ChronicleQueue}&opentele & (0.200)\\
 & elastic & (0.339)\\
 \hline
\textit{CorfuDB}&baseline & (0.475)\\
 & opentele & (0.227)\\
 & jacoco & (0.556)\\
 & intelli & (0.484)\\
 & jfr & (0.119)\\
 & elastic & (0.352)\\
 \hline
\textit{azure-iot-sdk-java}&baseline & (0.082)\\
 & opentele & (0.210)\\
 & jfr & (0.200)\\
 & elastic & (0.029)\\
 \hline
\textit{exhibitor}&jfr & (0.200)\\
 \hline
\textit{flow}&baseline & (0.533)\\
 & opentele & (0.467)\\
 & jacoco & (0.467)\\
 & intelli & (0.370)\\
 & jfr & (0.556)\\
 & elastic & (0.533)\\
% \hline
%\textit{karate}&jacoco & 0\\
 \hline
\textit{killbill}&baseline & (0.450)\\
 & opentele & (0.502)\\
 & jacoco & (0.199)\\
 & intelli & (0.435)\\
 & jfr & (0.623)\\
 & elastic & (0.570)\\
 \hline
\textit{mockserver}&jacoco & (0.200)\\
 & elastic & (0.200)\\
 \hline
\textit{ripme}&baseline & (0.518)\\
 & opentele & (0.402)\\
 & jacoco & (0.486)\\
 & intelli & (0.556)\\
 & jfr & (0.474)\\
 & elastic & (0.330)\\
 \hline
	\end{tabular}
\end{table}


   \begin{figure}[]
        \centering
        \includegraphics[width=\columnwidth]{corfudb.pdf}
        \caption{Variation of flakiness score for \textit{CorfuDB}}
\label{fig:boxplot}
    \end{figure}

%       \begin{figure}[]
%        \centering
%        \includegraphics[width=\columnwidth]{killbill.pdf}
%        \caption{Variation of flakiness score for \textit{killbill}}
%\label{fig:boxplot2}
%    \end{figure}


In answering our research question if instrumentation changes the test outcome, we observe that, in general, there are no significant changes in test outcomes with or without the use of different instrumentation tools. However, there were cases in some of the programs we examined that showed variation in test outcomes. The two tests that are flaky across configurations, which are in the programs \textit{flow} and \textit{ripme} are due to external flakiness. The test,  \texttt{FrontendToolsLocatorTest::toolLocated}, in \textit{ripme} fails due to an error in executing in an external program, and \texttt{BaraagRipperTest::testRip} fails to access network resources, which is incidental and not caused by instrumentation. The only confirmed case of flakiness introduced by instrumentation (OpenTelemetry) is the failure of tests in \texttt{MockServerExtensionConstructorInjection\-WithSettingsMultiplePortTest} in \textit{mockserver} and \texttt{FrontendToolsLocatorTest::toolLocated} in \textit{flow}.

On the related question if instrumentation increases test flakiness over multiple runs; again, instrumentation does not appear to have a discernible effect on the stability of flaky tests. However, this is difficult to measure for most programs in the dataset as the count of flaky tests, seen in Table~\ref{tab:results}, is sparse for most configurations, with one flaky test per configuration being common. One of the programs where it shows noticeable results are for \textit{CorfuDB}. \textit{CorfuDB}, as listed in Table~\ref{tab:scores} has relatively more flaky tests when compared to the other programs. This varies from 10 to 75 flaky tests. As depicted in Figure~\ref{fig:boxplot} for \textit{CorfuDB}, flakiness across runs varies for the different instrumentations.

\subsection{Discussion of Findings}
Our experiment shows that while instrumentation may cause flakiness, this effect is rare. There are only a few cases, which show an impact of instrumentation on the presence of flaky tests. There is no unified pattern that we could identify with regard to specific instrumentation tools or configurations that would introduce/increase flakiness across the programs we study.

A related question is whether instrumentation can interfere with existing flakiness detection and prevention techniques. Given a large number of such techniques \cite{parry2021survey,rasheed2022test}, this discussion is incomplete.%, and a lot of questions still need to be answered.
We aim to investigate this question further in our future research.

For \textit{detection techniques based on static analysis} such as \cite{fatima2022flakify}, interference is possible (but may still be unlikely) simply because the instrumentation is not part of the analyses, and this may lead to additional false positives and/or false negatives. This effect can  be difficult to mitigate. %In theory, the agents instrumenting the program could be made part of the analyses by applying instrumentation early (using source code or static bytecode instead of load-time/runtime weaving \cite{kiczales2001overview}), but this approach is not well-supported by tools, and may require the agent source code to be available.

Dynamic prevention and detection techniques such as \textit{RootFinder} \cite{wing2019rootcause} and \textit{saflate} \cite{dietrichflaky} (both use instrumentation)
can be prone to instrumentation order dependencies. While it is possible to craft examples showing this, it is unlikely to occur in practice. % \todo[]{why} check this.
%\todo[inline] {remove FlakeFlagger if it does not use instrumentation}

On the other hand, dynamic prevention and detection techniques that use a particular platform (such as \textit{NonDex} \cite{gyori2016nondex}  using a modified standard library or VMVM \cite{bell2014unit}) may also be sensible to instrumentation, as the instrumentation code itself is affected by those changes.


\section{Threats to Validity}
Even though the results of our experiments indicate the effects of instrumentation on test flakiness, there are threats to generalising them. First, the set of programs may not be representative, which we address using a set of programs from a previous flakiness study. As flaky tests can be non-deterministic and caused by environmental factors,  results may vary if the experiment is repeated. To account for such variance, we have executed the experiment 20 times and fixed the factors we control, such as the hardware, OS and JVM. Nonetheless, the results could change with more reruns of the experiment.
