\section{Study design}

This section lists the set of programs used in our study, the types of instrumentation used in the experiments, and the evaluation metrics we use. Our experiment requires rerunning each program's tests 20 times with the baseline (without instrumentation) and then with the use of each of the five instrumentations listed in Table~\ref{tab:instr}  (total of six). 

\subsection{Dataset}
We have used the set of programs used in Cordeiro et. al's study on manifesting flakiness by adding noise to the environment \cite{cord2021ASE}. In addition, they are 11 GitHub projects written in Java and use Maven for build automation. For two of the programs (\textit{ozone} and \textit{hbase}), this was not possible due to their size, as they take  considerably longer than the other programs ($>$30min per run). Thus, we have included only nine projects in our analysis (Table \ref{tab:programs}).%\todo[]{wouldn't the part of the excluded program make more sense in the following section (dataset)?}. done this to fix previous issue as well.

\begin{table}[]
\centering
\caption{Programs with GitHub repository name and commit ID/tag}
\label{tab:programs}
\begin{tabular}{l|l}
\hline

\textbf{repository} & \textbf{commit ID/tag}\\
\hline
OpenHFT/Chronicle-Queue & bec195b \\
CorfuDB/CorfuDB & b99ecff \\
Azure/azure-iot-sdk-java & a9226a5 \\
soabase/exhibitor & d345d2d \\
vaadin/flow & 6.0.6 \\
intuit/karate & 09bc49e \\
% killbill/killbill & killbill-0.22.21 \\
mock-server/mockserver & b1093ef \\
RipMeApp/ripme & 19ea20d \\
killbill/killbill & killbill-0.22.21\\
\hline

\end{tabular}
\end{table}


\subsection{Instrumentation}
While it is possible to craft an instrumentation that interferes with a particular test execution, we were interested in studying real-world instrumentation scenarios. To achieve this, we have identified several popular agents used for different purposes -- coverage capture, monitoring and profiling.

The instrumentation tools used in the experiment are listed in Table~\ref{tab:instr}. This includes Elastic APM, an application performance monitoring system, OpenTelemetry, JaCoCo for Java code coverage, IntelliJ's code coverage, and Java Flight Recorder, which collects profiling data for Java applications.


\begin{table}[]
\centering
\caption{Types of instrumentation}
\label{tab:instr}
\resizebox{\columnwidth}{!}{

\begin{tabular}{l|l|l|l}
\hline
\textbf{inst.} & \textbf{type} & \textbf{version}  &\textbf{rep}\\
\hline
elastic & APM & 1.34.1 & \url{/elastic/apm}\\
opentelemetry & APM & 1.12.0 & \url{/open-telemetry/opentelemetry-java}\\
jacoco & coverage & 0.8.8 & \url{/jacoco/jacoco/} \\
intellij & coverage & 1.0.684  & \url{/JetBrains/intellij-coverage} \\
jfr & profiling & N/A & \url{https://docs.oracle.com/en/java/java-components}\\
\hline
\end{tabular}}
\end{table}

\subsection{Evaluation Metrics}

\subsubsection{Flaky test count}

Flaky test count measures the number of tests over $N$ runs that result in different states across runs for a given instrumentation configuration. If the set of outcomes (as in JUnit) are $\{success, failure, error, skip\}$, a test, $t$, is flaky across runs (or configurations) if for any two runs, $r_1$ and $r_2$, there is a transition across test states, i.e. $r_1(t) \neq r_2(t) $ if we consider test runs as a mapping from tests to states.


\subsubsection{Flakiness score}
\label{sec:flaky-score}
The flakiness score measures the variability between test runs. For instance, we may already observe flakiness if a test fails only in 1/20 runs. It is still interesting to see whether we can see this changing to a higher (or lower) value with instrumentation being used, such as 5/20.

For each configuration (baseline or one particular instrumentation) for a program in the dataset, we compute this as follows:

\begin{itemize}
\item $FT$ is the set of tests that are flaky in all configurations. Note that this may not include tests that are flaky across configurations. Including those tests in the baseline would be problematic as adding another configuration may affect the flakiness scores for existing configurations.
\item For each run $i$ compute a set, $r_i$ consisting of pairs $(t, state) \in FT \times \{pass,fail,error,skip\}$
\item For each pair of runs, $r_i$ and $r_j$, compute the Jaccard distance, $d (r_i, r_j) = 1-\dfrac{|r_i \cap r_j |}{|r_i \cup r_j|}$, for $N$ runs.
 This yields $N(N-1)/2$ values (20 runs resulting in 190 values)
\item The distances across runs can then be $aggregated$ (e.g. mean or median) to obtain a flakiness score.

\end{itemize}


If the flakiness score is 0, there is no flakiness.
The flakiness score is greater than 0 if there is flakiness. For example, the baseline for a program's test results has some flakiness (one test fails once), but this one test fails a few more times with instrumentation. In this case, the flakiness score would go up even though the flaky count would remain the same.
%More tests become flaky with instrumentation: flakiness would generally go up, even though in theory, there could be cases where it does not â€“ this would be the justification to report both metrics; flakiness-score does not replace flaky-count

\subsection{Experimental setup and process}

We run each program's tests 20 times in six different configurations (120 runs for each program),  i.e., baseline and the listed five types of instrumentation. Experiments were run on a computer with a 3.2 GHz 6-Core Intel Core i7 CPU with Oracle's Java SE Development Kit 8u301. We made the data from the experiments available online \url{http://www.bitbucket.org/unshorn/inst-study}.
