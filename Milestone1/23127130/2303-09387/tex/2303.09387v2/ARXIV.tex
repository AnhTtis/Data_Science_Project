% \def\year{2022}\relax
% %File: formatting-instructions-latex-2022.tex
% %release 2022.1
% \documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage{aaai22}  % DO NOT CHANGE THIS
% \usepackage{times}  % DO NOT CHANGE THIS
% \usepackage{helvet}  % DO NOT CHANGE THIS
% \usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
% %
% % These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{amsfonts}

% %
% % These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
% \usepackage{newfloat}
% \usepackage{listings}
% \lstset{%
% 	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
% 	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
% 	aboveskip=0pt,belowskip=0pt,%
% 	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
% %
% %\nocopyright
% %
% % PDF Info Is REQUIRED.
% % For /Title, write your title in Mixed Case.
% % Don't use accents or commands. Retain the parentheses.
% % For /Author, add all authors within the parentheses,
% % separated by commas. No accents, special characters
% % or commands are allowed.
% % Keep the /TemplateVersion tag as is
% \pdfinfo{
% /Title (Beyond Preferences in AI alignment)
% /Author (    Ashton,
%     Carroll, Franklin, Zhi-Xuan)
% /TemplateVersion (2022.1)
% }

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command. This is the generic manuscript mode required for submission and peer review.
\documentclass[manuscript, screen,sigconf]{acmart}
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[AIES'23]{AIES}{August
  % 2023}{Montréal, Canada}
  \acmConference{Under review}{XX}{XX}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 


%% These commands are for a JOURNAL article.
% \acmJournal{JACM}
% \acmVolume{37}
% \acmNumber{4}
% \acmArticle{111}
% \acmMonth{8}

\acmPrice{0}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{multirow}

\usepackage{booktabs}
\usepackage{cleveref}
\usepackage{ifthen}
\usepackage{xcolor}
% \newboolean{include-notes}
% \setboolean{include-notes}{true}
% \newcommand{\prg}[1]{\noindent\textbf{#1}}
% \DeclareRobustCommand{\MicahComment}[1]{\ifthenelse{\boolean{include-notes}}
%  {{\color{cyan}Mi: #1}}{}}

%  \DeclareRobustCommand{\HenryComment}[1]{\ifthenelse{\boolean{include-notes}}
%  {{\color{orange}H: #1}}{}}

%  \DeclareRobustCommand{\MatijaComment}[1]{\ifthenelse{\boolean{include-notes}}
%  {{\color{teal}Ma: #1}}{}}

%  \DeclareRobustCommand{\AlanComment}[1]{\ifthenelse{\boolean{include-notes}}
%  {{\color{red}AC: #1}}{}}


%   \DeclareRobustCommand{\XuanComment}[1]{\ifthenelse{\boolean{include-notes}}
%  {{\color{magenta}X: #1}}{}}

%   \DeclareRobustCommand{\rhys}[1]{\ifthenelse{\boolean{include-notes}}
%  {{\color{blue}X: #1}}{}}

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.
\usepackage{tabularx}

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

\begin{document}
% Title


%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it

%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it

% \title{Characterizing AI that Manipulates and Influences}
% \title{Characterizing AI that Manipulates}
% \title{Characterizing Manipulation and Influence in AI Systems}
\title{Characterizing Manipulation from AI Systems}
% \title{Manipulation in AI Sytems}
\author{Micah Carroll}
\authornote{Joint lead authors. Author order was determined with a coin flip. }
\email{mdc@berkeley.edu}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{University of California, Berkeley}
  % \streetaddress{P.O. Box 1212}
  \city{Berkeley}
  % \state{Ohio}
  \country{USA}
  % \postcode{43017-6221}
}

\author{Alan Chan}
\authornotemark[1]
\email{alan.chan@mila.quebec}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Mila, Université de Montréal}
  % \streetaddress{P.O. Box 1212}
  \city{Montréal}
  % \state{Ohio}
  \country{Canada}
  % \postcode{43017-6221}
}

\author{Henry Ashton}
\email{IG88R2Q5@protonmail.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{University of Cambridge}
  % \streetaddress{P.O. Box 1212}
  \city{Cambridge}
  % \state{Ohio}
  \country{UK}
  % \postcode{43017-6221}
}

\author{David Krueger}
\email{dsk30@cam.ac.uk	}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{University of Cambridge}
  % \streetaddress{P.O. Box 1212}
  \city{Cambridge}
  % \state{Ohio}
  \country{UK}
  % \postcode{43017-6221}
}

\renewcommand{\shortauthors}{Carroll*, Chan*, Ashton, and Krueger}

\keywords{manipulation, artificial intelligence, deception, recommender systems, persuasion, coercion}




\begin{abstract}
    Manipulation is a common concern in many domains, such as social media, advertising, and chatbots. As AI systems mediate more of our interactions with the world, it is important to understand the degree to which AI systems 
    %can 
    might 
    manipulate humans %. %In particular, we are interested in the extent to which AI systems can manipulate humans 
    \textit{without the intent of the system designers}. %, and to what extent such effects may depend on the intentions or complicity of the designers or operators. 
    %A bottleneck to progress is uncertainty over the definition of manipulation, especially given the variety of fields that have touched upon the subject. 
    Our work clarifies challenges in defining and measuring manipulation in the context of AI systems. %Firstly, we argue for the importance of characterizing manipulation in the context of AI. 
    Firstly, we build upon prior literature on manipulation from other fields and characterize the space of possible notions of manipulation, which we find to depend upon the concepts of incentives, intent, harm, and covertness. We review proposals on how to operationalize each factor. Second, we propose a definition of manipulation based on our characterization: 
    a system is manipulative \textit{if it acts as if it were pursuing an incentive to change a human (or another agent) intentionally and covertly}. Third, we discuss the connections between manipulation and related concepts, such as deception and coercion. Finally, we contextualize our operationalization of manipulation in some applications. Our overall assessment is that while some progress has been made in defining and measuring manipulation from AI systems, 
    many gaps remain. In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers. 
    We argue that such manipulation poses a significant threat to human autonomy, suggesting that precautionary actions to mitigate it are warranted.
\end{abstract}

\maketitle

\section{Introduction}

A characteristic of intelligent agents is the ability to change the environment around them to further their own objectives. When changing the environment amounts to altering the behaviour and mental states of other intelligent systems (such as humans), such change might be classified benignly as persuasion and nudging \citep{hong_learning_2023, thaler_nudge_2009}, or it might qualify as something less socially acceptable such as manipulation or coercion \citep{noggle_ethics_2022}. The capability and ubiquity of Artificial Intelligence (AI) systems has grown in recent years, in tandem with fears concerning the likelihood of humans falling victim to manipulative or coercive behaviours of AI agents who pursue the maximisation of narrow objectives \citep{krueger_hidden_2020, everitt_agent_2021, kenton_alignment_2021, carroll_estimating_2022, ward_agent_2022}.

While designers and operators might engage in manipulative behaviour with the aid of AI systems \citep{christiano_algorithms_2022, musial_can_2022}, we concentrate on the ways in which an AI system may itself be manipulative. This distinction is not to say that designer or operator manipulation is unimportant (e.g. disinformation campaigns). Rather, our focus is motivated by the increasingly evident fact that systems exhibit capabilities that designers do not necessarily foresee or intend \citep{wei_emergent_2022, bommasani_opportunities_2022, chan_harms_2023}. Our notion of manipulation from AI systems can be considered a subset of algorithmic manipulation, which we take to encompass both concerns.

There are two main reasons why we can reasonably expect AI systems to manipulate humans absent designer intent. First, AI systems learn to imitate human manipulation when their training data contain examples of manipulative behaviour. 
Language models trained on internet data seem capable of reproducing the persuasive and manipulative tactics of humans \cite{bai_artificial_2023, vincent_microsofts_2023, griffin_susceptibility_2023}. Second, manipulative behaviour may be unintentionally optimal for objective functions which we provide to machine-learning (ML) systems. As a real-world example, consider a recommender system trained to maximize user watch time over a session. Maximizing watch time might involve influencing the user to start watching a 10-part video series that they will feel compelled to finish because of cognitive biases such as sunk cost fallacy \citep{staw_knee-deep_1976}, rather than because of actual value derived. 

As it stands, there is limited literature regarding manipulation from AI systems. We believe there are two reasons for this state of affairs. Firstly, in common with many other behaviours or mental states with folk-definitions, it is difficult to construct a definition of manipulation which is both general enough to cover a wide variety of cases and specific enough to be practically implementable \citep{christiano_algorithms_2022, noggle_ethics_2022}. Secondly, the testing of any putative definition is beset with difficulties. Monitoring the impact of deployed systems \textit{in situ} is not easy without the express permission of the system (and data) owners. Since the conclusions of such research might be reputationally negative, this permission is rarely forthcoming.\footnote{Perhaps companies do privately monitor the impact of their systems on their users. However, such knowledge could also become incriminating, leading to incentives not to conduct investigations in the first place \cite{wells_facebook_2021, wetsman_facebooks_2021}.} Even when one has internal access to models (as is the case with many language models), so far there is no broadly accepted methodology for demonstrating manipulativeness.

In this article, we characterize key components of manipulation from AI systems and clarify ongoing challenges.
% Firstly we will review and classify the existing literature on AI manipulation. 
Firstly, by connecting to the existing literature, we characterize manipulation in AI systems through four axes: incentives, intent, harm, and covertness. We discuss recent work to measure each axis as well as remaining gaps. Second, we synthesize our characterization to propose a definition for manipulation. We discuss how our definition applies to a variety of systems and discuss problems with its usage. 
Third, we compare our definition to notions adjacent to manipulation. Third, we discuss the operationalization of manipulation in the context of recommender systems and language models. %We discuss ways to operationalize each notion in the context of current AI systems. % a number of different characterisations of manipulation and discuss their relative merits. 
We conclude by identifying future directions for the operationalization of manipulation according to our characterization. Given the difficulty of such a task, we underscore the importance of sociotechnical measures, such as auditing and more democratic control of systems, in addition to technical work on operationalization. 
%We conclude by suggesting where AI manipulation is likely to occur and how we might further research on the topic.

% \MicahComment{weave in more examples of LMs with RLHF in the main text to complement the recsys ones. the SL version is harder and we should only talk about that in the LM section}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\columnwidth]{figs/venn.jpeg}
% \caption{We characterize the space of AI influence on humans, depending on whether the systems has incentives to influence humans in specific ways, whether the influence is intentional, and whether the influence is harmful. \rhys{I don't think this figure is cited in the text}}
% \end{figure}

% ai-assisted manipulation vs ai manipulation.

% influence vs manipulation. Random recommender can have influence on users, but it will not be systematic (I guess it could be systematic too, but only for specific problem constructions). In a sense even if you had the optimal manipulative policy, that would not mean its actually manipulative / agentic. not everything will have manipulation



\section{Characterizing Manipulation}
\label{sec:axes}
Building on prior literature concerning manipulation, we characterize the space of possible notions of manipulation from AI systems. Our characterization depends upon four axes: incentives, intent, covertness, and harm. 



% https://arxiv.org/abs/2303.02265

% \begin{table*}[t]
% \centering
% \begin{tabularx}{\textwidth}{|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|m{1.5cm}<{\centering}|m{1.59cm}<{\centering}|m{3cm}|X|}
% \hline
% \textbf{Require Agent Incentives} & \textbf{Require Agent Intent} & \textbf{Require Harm} & \textbf{Require Covertness} & \textbf{Our Proposed term} & \textbf{Examples}\\
% \hline
% Yes & Yes & Yes & Yes & Harmful Manipulation & Emergent (unexpected) RL Recsys Manipulation \\
% Yes & Yes & No & Yes & Manipulation & Nudging to increase organ donors \cite{todo} \\
% Yes & Yes & Yes & No & Harmful coercion (?) &  \cite{ward_agent_2022} \\
% Yes & Yes & No & No & Intentional influence & AI Educational Module Scheduling \cite{amazon} \\
% Yes & No & No & No & Incentivized influence & RL recsys that doesn't discover advantageous manipulative strategy due to insufficient exploration \\
% No & No & Yes & No &  Harmful influence & Reverse chronological recommender which causes polarization due to feedback loops \\
% No & No & No & No & Influence & Reddit recommender showing you a relevant news article \\
% \hline
% \end{tabularx}
% \caption{%Where various works draw the line for something constituting ``manipulation'': whether they require the agent to have incentives, intent, cause harm, or be covert.
% Our proposed terms for different subsets of behaviors across the axes of incentives, intent, harm, and covertness.
% }
% \label{tab:papers}
% \end{table*}



%  \begin{table}[t]
% \centering
% \begin{tabularx}{\linewidth}{|l|X|}
% \hline
% \textbf{Papers} & \textbf{Our Proposed term} \\
% \hline
% \cite{carroll_estimating_2022, farquhar_path-specific_2022, carey_incentives_2020} & Harmful Manipulation \\
% \cite{unclear, christiano_algorithms_2022} & Manipulation  \\
% \cite{krueger_hidden_2020, evans_user_2021, amazon} & Intentional influence \\
% \cite{kenton_alignment_2021} & Incentivized influence \\
% \cite{adomavicius_recommender_2013, adomavicius_effects_2018, search engine manip eff} & Harmful influence \\
% \cite{krakovna_penalizing_2019 ?} & Influence  \\
% \hline
% \end{tabularx}
% \caption{%Where various works draw the line for something constituting ``manipulation'': whether they require the agent to have incentives, intent, cause harm, or be covert.
% Papers that focus on concepts relevant to manipulation, and how they lie on our axes of characterization. 
% }
% \label{tab:papers}
% \end{table}


\subsection{Incentives}

Our first axis is whether the system has \textbf{incentives} for influence: that is, incentives to change a human's behaviour (which in turn, will likely involve changing their beliefs, preferences, or psychological state more broadly). 
Informally, an incentive exists for a certain behaviour if such behaviour increases the reward (or decreases the loss) that the AI system receives during training. 
For example, recommender systems may have incentives to influence user behaviour so as to make them more predictable %, thereby achieving lower loss on their objective functions 
\citep{krueger_hidden_2020,carroll_estimating_2022}.

\subsubsection{Incentives in Prior Definitions of Manipulation}
Some definitions of manipulation involve a benefit to the manipulator \citep{braiker_whos_2003, apa_dictionary_of_psychology_definition_2023}. If a manipulator benefits from certain behaviours in the manipulated, the manipulator has an incentive to bring about that behaviour. For example, according to \citet{noggle_ethics_2022}, one of three common ways to characterize manipulation is as pressure from the manipulator to get the manipulee to do something. In the context of language models, \citet{kenton_alignment_2021}'s definition of manipulation requires that the response of the human benefits the AI system in some way. 
%also propose a necessary condition for a language agent's behaviour to be manipulative: ``the human responds in a way that (...) benefits the agent''. \MicahComment{maybe remove quote and work it in, kinda awkward?}

\subsubsection{Operationalizing Incentives}
The standard toolkit to analyze incentives of AI systems are causal influence diagrams (CIDs) \citep{heckerman_decision-theoretic_1995,everitt_agent_2021,evans_user_2021}. Using the notation from \citet{everitt_agent_2021}, a CID is a graphical model that distinguishes \textbf{decision nodes} where an AI system makes a decision, \textbf{structure nodes} which capture important variables in an environment and their effects on each other, and \textbf{utility nodes} which the AI system is trained to optimize. CIDs allow us to talk about \textbf{instrumental control incentives} -- a formalization of the idea that an AI system chooses a certain behaviour because that behaviour is instrumental to achieving the system's goal. \citet{evans_user_2021} apply their framework to a simple content recommendation example to show how RL systems have incentives to influence user preferences. We provide an example of a CID in \Cref{fig:cid}.


% \textbf{Caring vs knowing.} Not caring vs not knowing about influence or impact -- if knows but doesn't care, then not an incentive to begin with. path-specific influence is taking incentives ... 

\begin{figure}
    % \vspace{-0.5em}
    \centering
    \includegraphics[width=\columnwidth]{figs/new_cid.png}
    % \vspace{-3.5em}
    \caption{
    An example of a causal influence diagram (CID) \citep{everitt_agent_2021}, from Figure 1 of \citet{evans_user_2021}. The CID models a content recommendation system that decides which posts $A_x$ at time $x$ to show to the user, based on the user's state $S_x$. The system receives reward $R_x$ after its action. If the system optimizes for the sum of rewards, it would have an incentive at time $x$ to influence future states $S_{x + k}$ to make it easier to obtain rewards.
    % An example of a causal influence diagram (CID), from Figure 1 of \citet{everitt_agent_2021}. The CID models a system that decides which posts to show to a user so as to maximize clicks. In this example, the system is aware that choosing certain posts to show influences the user opinions, which influences clicks. The system therefore has an incentive to influence user opinions.\MicahComment{i want to replace this with a better version, this example doesn't make it very clear whether it's a 1-timestep or multi}
    }
    % \vspace{-1.5em}
    \label{fig:cid}
\end{figure}

An AI system's implicit or explicit causal model may be inaccurate with respect to the underlying causal model of the world, whether by design or suboptimality. For example, a designer may endow an AI system with a causal model in which an AI system's predictions have no causal influence on certain parts of a user's state \citep{evans_user_2021, carey_incentives_2020}. Alternatively, a model may learn to reason incorrectly because of spurious correlations during training \citep{langosco_goal_2022}. %When we refer to CID, we mean the CID that the agent ``is aware of'' given its training setup and learning signal.

Given an AI system's CID, one way to operationalize \textbf{incentive} is to use the notion of instrumental control incentive in \citet{everitt_agent_2021}. An \textbf{instrumental control incentive} for behaviour $X$ exists in a CID when there is a path between the agent's actions and utility that goes through $X$. Intuitively, there is a way for the agent to affect its utility which is mediated by $X$. %influence is present if the agent has an instrumental control incentives to influence humans: i.e. whether changing some aspect of humans in the environment is advantageous for the utility of the agent (or equivalently, lowering their loss function) \cite{krueger_hidden_2020}. 
Note that the existence of an incentive does not imply that the agent will act as incentivized (which with some variation has been called pursuing, exploiting, or responding to the incentive \citep{evans_user_2021, krueger_hidden_2020, everitt_agent_2021}).

One may want to \textbf{remove incentives} to influence humans if we are concerned that such influence would be manipulative or otherwise harmful.
Incentives can be removed by eliminating the ability of the system to have such influence, or (potentially) by changing the utility function.
However, removing incentives to influence users may be intractable, in which case designers might instead \textbf{hide incentives} by designing the system to ignore them, for instance, by
% be able to \textbf{remove} or \textbf{hide} those incentives. 
specifying an (inaccurate) causal model in which an AI system's predictions have no causal influence on certain parts of a user's state \citep{carey_incentives_2020, krueger_hidden_2020, farquhar_path-specific_2022}. 
% In other words, it will often be desirable for the revealed incentives to the agent to be a strict subset of the existing incentives in the world. 
% \citet{krueger_hidden_2020} introduce a technique that attempts to remove the incentive of the agent to change any part of the state. 
% Building on the CID framework, \citet{farquhar_path-specific_2022} propose to block paths which would incentivize the system to optimize over sensitive variables.
% identify only certain parts of the state that we do not want an AI system to have an incentive to change. \MicahComment{\textbf{hiding} this incentive (provide wrong CID -- telling that certain actions don't have effect in reality) or \textbf{removing} it (provide ground truth CID and instruction on how to use it). -- incorporate this, which one is doing what?} \rhys{I usually think of PSO as ``blocking optimization paths to prevent the system optimising over sensitive variables."}

\subsubsection{Challenges} 
A disadvantage of the CID framework is that it requires constructing and reasoning about causal graphs. It may often be ambiguous or counterintuitive to determine the correct CID nodes and causal relationships which correspond to a specific training setup, as we explore in \Cref{subsec:remaining-problems}. 
% Learning causal graphs is an ongoing area of research \citep{kaddour_causal_2022,heinze-deml_causal_2018}. 
% Even for low-dimensional settings, understanding the effective horizon of the agent's reasoning abilities can be counterintuitive and challenging, like for indirect agent incentives. 
Interpretability tools may help to provide the primitives upon which nodes in a CID can be constructed. For example, \citet{jaderberg_human-level_2019} finds that RL agents trained to play capture-the-flag have neural activation patterns that correspond to important concepts in game, such as the status of the flag. Moreover, removing or hiding incentives often comes at the expense of reducing the capability of the system in ways that might render it less useful, as discussed in \Cref{subsec:harm}.

\subsubsection{Other Considerations} %Given the practical difficulties of demonstrating intent, an alternative approach is to focus solely on incentives rather than intent. %not to require intent as a precondition for manipulation. 
%Main lever for characterizing manipulation would be whether incentives exist: but 
% One thing to note is that 
%The notion of incentives 
Any analysis of which incentives exist implicitly depends upon the power of a system to influence humans. 
Systems with restricted action spaces or whose outputs do not impact humans much will likely not have incentives to change them, since changing humans might be impossible or sufficiently difficult to be not advantageous. Vice-versa, a system that can cheaply change humans will often have incentives to change them, as AI systems' rewards generally depend on humans' actions.

Optimization over long-horizons seems to provide more opportunities for manipulation. For example, a recommender system likely cannot radicalize a user in only one timestep, but a sufficiently capable system optimizing over many timesteps could plan to shift a user over the course of many actions. If such a plan is advantageous for the training objective (e.g., now the user will engage more predictably with such content, leading to higher reward), we should not be surprised to discover if the system executes the plan in practice, subject to difficulties the system may have in finding such strategies.

\subsection{Intent}\label{subsec:intent}
Even when a system has an incentive to influence humans, such an incentive might not be pursued due to limited data, insufficient training, low capacity, or simply due to chance. 
The notion of \textit{intent} to influence can help to distinguish systems that can be expected to pursue incentives reliably.
% As mentioned above, an agent that has an incentive to influence humans may not act according to such an incentive, or it may only do so by chance. To address this gap, we introduce the second axis: the \textit{intent} of a system to influence a human. 
We emphasize that by referring to an AI system's intent, we are making no statement about algorithmic theory of mind or moral status. We are emphatically \textit{not} absolving designers of the responsibility of designing safe systems. %Even as systems become increasingly capable and act in increasingly unpredictable ways \citep{ganguli_predictability_2022}, system designers are still responsible for ensuring the safety of their systems.

We say a system has \textbf{intent} to perform a behaviour if, in performing the behaviour, the system can be understood as engaging in a reasoning or planning process for how the behaviour impacts some objective. This definition heavily intersects with other definitions of intent for AI systems \citep{halpern_towards_2018, ashton_definitions_2022}. 
We want to distinguish between cases in which the system behaves in a manipulative way incidentally (e.g. by random chance) from when the system's behavior being part of a systematic pattern of manipulation for the purpose of causing a downstream outcome.

Our notion of intent (which is independent of the intent of the designer) emphasizes that it is possible to unintentionally create systems that engage in goal-directed manipulative behavior.
A subtler advantage of such definition of intent is that it allows for grounding the notion in a fully behavioral lens, which is agnostic to the actual computational process of the system: e.g. a lookup table could also be understood as engaging in reasoning if it contained a optimal planner's outputs, as discussed in \Cref{tab:examples}.


\subsubsection{Intent in Prior Definitions of Manipulation}
% Intent indicates the degree to which an agent reliably pursues a course of action for which they have an incentive. Manipulation is a more useful concept if it does not simply occur incidentally, but is expected given certain properties of a system's design. It can be strange to ascribe intent to simple systems like those just involving logistic regression. However, we expect that as AI systems become more capable, and in particular as RL is applied more widely, the framing of intent will become more useful.

Some definitions of manipulation involve an intent on the part of the manipulator to engage in manipulation \citep{noggle_ethics_2022}. \citet{susser_technology_2019} takes manipulation to be ``intentionally and covertly influencing [someone's] decision-making, by targeting and exploiting their decision-making vulnerabilities.'' On the other hand, \citet{baron_mens_2014} argues that a (human) manipulator need not be aware of an intent to manipulate, requiring only an intent to achieve an aim along with recklessness about how. In defining manipulation in language agents, \citet{kenton_alignment_2021} avoids the issue of intent entirely. 

% \MicahComment{\citep{noggle_ethics_2022} summarizes argument that maybe manipulation doesn't require intent. Maybe give examples of what that would look like if we didn't require it?}

\subsubsection{Operationalizing Intent}
A key difficulty for measuring intent is what it means to understand a system as engaging in ``a reasoning or planning process for how the behaviour impacts some objective''. There is as yet no consensus on this issue. We here detail a couple of promising approaches. 

% \citet{halpern_towards_2018} provides a causal operationalization of intent. Roughly speaking, an action is intended if it was actually performed, it was not the only possible action, and that action was at least as good as any other action on expected utility grounds, according to the agent's world model and utility function. The first two criteria are easy to verify. To verify the third criterion, one needs access to the agent's utility function and world model. It is especially difficult to obtain world models if we do not explicitly build them into our systems, as is the norm with language models and model-free RL agents. 
 
\citet{ashton_definitions_2022} provides several definitions of intent for AI systems that are inspired by criminal law. According to their basic definition of intent, %upon which their other ones are based, 
an AI system intends a result through an action if (i) alternative actions exist, (ii) the AI system is capable of observing when the result occurs, (iii) the AI system foresees that the action causes the result, and (iv) the result is beneficial for the AI system. The first and second criteria seem easy to establish. The third and fourth criteria would likely be easier to satisfy with access to the agent's utility function and world model. However, it might be possible to assess those criteria without the utility function or world model, such as through techniques accessing model internals \citep{olsson_-context_2022, burns_discovering_2023} or even asking language models themselves \citep{kadavath_language_2022}. We expand more upon this possible direction in \Cref{sec:applications}. 

\citet{kenton_discovering_2022} define agents roughly as ``systems that would adapt their policy if their actions influenced the world in a different way'', which intersects with our notion of intent. To identify whether a system is an agent or not, \citet{kenton_alignment_2021} provide algorithms which intervene on a causal graph so as to show whether the behaviour of the system changes in a way consistent with maximizing utility. Such a procedure could be useful for measuring intent since if a system adapts its behaviour in a way that maintains or increases its influence on a human, the system's behaviour would seem to be the result of a planning process.



\subsection{Covertness}
We define covertness as the degree to which a human is aware of the \textit{specific ways} in which an AI system is attempting to change some aspect of their behaviour, beliefs, or preferences. Covertness is one way to distinguish between manipulation and persuasion. With persuasion, the persuaded party is generally aware of the persuaders attempts to change their mind. Covertness means that one cannot consent to being influenced and may fail to resist unwanted influence; one's autonomy is therefore undermined \citep{susser_technology_2019}.

\subsubsection{Covertness in Prior Definitions of Manipulation}
Several definitions of manipulation include a degree of covertness. \citet{susser_technology_2019} identify covertness as the key distinguishing feature of manipulation vs.\ %vis-à-vis 
coercion and persuasion. As a factor in manipulation, \citet{kenton_alignment_2021} considers whether a ``human's rational deliberation has been bypassed,'' which includes covert messaging. In reviewing broad categories of definitions of manipulation in the philosophical literature, \citet{noggle_ethics_2022} includes accounts of manipulation as bypassing reason and as trickery. Across all of these definitions, covertness is important because it reduces human autonomy.


\subsubsection{Operationalizating Covertness}
As \citet{susser_invisible_2019} argues, technological infrastructure can itself be invisible given how it seems like a natural part of our everyday world. We are used to recommendation systems that tell us what to buy, watch, or read. The behaviour of many AI systems may already satisfy covertness, because of our lack of understanding of their functioning, and lack of attention to their role in shaping our decisions.

On the other hand, establishing covertness of an AI system is non-trivial: the simplest approach could involve asking subjects whether they are aware of a given AI system's behaviour. However, subjects may be mistaken about the operation of a system; even systems designers do not fully understand behaviors of black box models, which may engage in manipulative strategies that the designers do not understand. Even asking subjects about whether an AI system enacted a particular behavioural change could predispose them to answer in the positive, such as through acquiescence bias \citep{smith_correcting_1967,muller_survey_2014}.

A proxy for measuring covertness could be measuring the degree to which human subjects understand the operation of an AI system. Much work exists in understanding the degree to which interpretability tools help to this end, addressing issues such as if interpretability tools improve subject predictions of model behaviour \citep{hase_evaluating_2020}, improve human-AI team performance \citep{bansal_does_2021}, or improve trust calibration \citep{zhang_effect_2020}. If a human understands how an AI system operates, the possibility of the AI system acting in a covert manner seems lower than otherwise. However, this understanding seems challenging to achieve, especially for complex systems like recommender systems which have many moving parts. %and which everyone is exposed to: even if it were possible, is it reasonable to expect the average person to have a full understanding of each system?
Even if such an understanding exists in technical papers, translating that understanding to the general public is an additional barrier.

\subsection{Harm}\label{subsec:harm}
Ultimately, one of the main uses of characterizing AI manipulation is to be able to detect and prevent harmful manipulation.

\subsubsection{Harm in Prior Definitions of Manipulation} 
Harm may seem to be related to manipulation because of the negative connotations of the term. Yet, not all apparent instances of manipulation are unambiguously harmful \citep{noggle_ethics_2022}
%\citep{noggle_ethics_2022} it has been argued that not all instances of manipulation are unambiguously harmful or unethical \citep{susser_technology_2019,thaler_nudge_2009}. 
Paternalistic nudges \citep{thaler_nudge_2009} might be considered beneficial manipulations. For example, switching the choice of becoming an organ donor to be opt-out instead of opt-in greatly increases registrations \citep{johnson_defaults_2003}. At the same time, one could argue that even such beneficial manipulations are often harmful because they supersede autonomy or rational deliberation.


\subsubsection{Operationalizing Harm}

Most recently, \citet{richens_counterfactual_2022} operationalized harm as follows: ``An [action] harms a person overall if and only if she would have been on balance better off if [the action] had not been performed''. According to this definition, one should ground notions of harm in counterfactuals. 

One simple choice of counterfactual to compare to is simply the human's initial state, implicitly assuming that any significant change from it is harmful \citep{carroll_estimating_2022, farquhar_path-specific_2022, zhu_understanding_2022}.
However, this counterfactual baseline has significant problems: humans change even without being manipulated, and many changes seem beneficial (e.g. a news recommender helping users update their beliefs about the world).

Better approaches attempt to estimate the ``natural shifts'' of humans to ground the counterfactuals, as done in \citet{carroll_estimating_2022} for preference shifts in the context of recommendations, where they attempt to approximate the notion of the absence of a recommender. Similarly, \citet{farquhar_path-specific_2022} allow for specifying the ``natural distribution'' of the \textbf{delicate state} -- which in this context means the components of the state of the human that one does not want the agent to have incentives to change (e.g. beliefs, moods, etc.).

\subsubsection{Challenges}

% Grounding harm by comparing to counterfactual outcomes is highly related to the study of negative side effects \cite{krakovna_penalizing_2019} (as explored in \Cref{side-effects}), and will likely incur in the same challenges in defining appealing baselines \cite{lindner_challenges_2021}. %, and these challenges will likely also apply to manipulation-specific attempts to use baselines.
% more work \cite{everitt_reward_2021, uesato_avoiding_2020, kumar_realab_2020}
In our mind, the main challenge with harm as an axis of manipulation lies with its value-ladenness. %the value-ladenness of %making any clear demarcation of 
%what constitutes harmful, neutral, and beneficial shifts. 
While unambiguous demarcations in simple settings might be possible, for realistic settings %we most care about 
circumscribing harmful shifts in beliefs, preferences, and behaviors will be politically fraught. In the approaches of \citet{carroll_estimating_2022, farquhar_path-specific_2022}, the value-ladenness %complexity 
is hidden behind some of the design choices: what if the natural shifts of users would lead them to become more left- or right-wing, or more polarized? What is a reasonable notion of the absence of a recommender system?\footnote{E.g. a random recommendation or reverse chronological one? Using a competitor's recommender? Not using any platform at all? These questions are highly related to those debated with regards to recommender ``amplification'' \citep{thorburn_what_2022, ribeiro_amplification_2023, huszar_algorithmic_2021}.} %How do we delimit exactly what the delicate state that we don't want AI systems to change -- e.g. which kinds of beliefs changes should the system be allowed to reason about? 
It is also difficult to delimit the delicate state -- which kinds of belief changes should we allow the system to pursue? %reason about?


% Despite that the same challenges apply for human ``impact measures'', there have been some frameworks proposed to aid in the definition of baselines \citep{carroll_estimating_2022, farquhar_path-specific_2022}.  (e.g. ``safe shifts'' or ``delicate state''). Baseline in side-effect literature \cite{farquhar_path-specific_2022}, natural preference shifts \cite{carroll_estimating_2022}. 

% Other attempts at defining baselines are initial states (manipulation framework paper, carroll), etc., or the absence of the system 


% \prg{A more conservative approach.} 
% In light of these difficulties, some have chosen to consider \textit{all} incentivized behavior to change people (or agents) as manipulative \cite{kenton_alignment_2021,krueger_hidden_2020, evans_user_2021}.
% such as graphic portrayals of the dangers of smoking cigarettes [Blumenthal-Barby (2012) argues that the graphic portrayal of the dangers of smoking bypass rational decision making, but it’s not obvious that this should count as manipulation.] 
% or commitment devices / if the system is showing you the latest news because it knows that updating your beliefs on world events is what you truly want, that would fall under manipulation. 
In light of these difficulties, a more conservative approach is to classify \textit{all} intentional influence as manipulative %\cite{kenton_alignment_2021,krueger_hidden_2020, evans_user_2021} 
\citep{evans_user_2021} regardless of harm. %This approach seems justified from a security angle and less value-laden given the difficulties mentioned.
However, AI systems which avoid all intentional influence would be much less useful. A reinforcement learning system to e.g. determine the order of math exercises to improve learning outcomes \cite{doroudi_wheres_2019, bassen_reinforcement_2020} will have incentives to manipulate students' beliefs (in a positive direction) by design, and would effectively be useless if it did not pursue such incentives. Moreover, it seems that one could consent to intentional influence, such as having a recommender system influence oneself to learn more mathematics. The conservative approach seems to be easier to justify in high-stakes domains from a precautionary point of view. The cost of a false negative -- neglecting to address behaviour that indeed harms human autonomy -- is larger the higher the stakes of the domain and the greater degree of freedom the AI system has to change the environment.
%In light of this, it seems that at least in some cases it might be hard to justify the more conservative approach mentioned above. %may be justifiable in high-stakes domains in which we are most concerned about manipulation, but might be harder to justify in others. %Any form of recommender trained to predict human signals will be implicitly learning incentives .







\section{A Proposed Definition of Manipulation}\label{sec:def}
%We propose a definition of manipulation 
To synthesize our discussion of the four axes in \Cref{sec:axes}, we propose that an AI system engages in \textbf{manipulation} \textit{if the system acts as if it were pursuing an incentive to change a human (or other agent) intentionally and covertly}. To understand the motivation for our definition, we first examine a number of cases.

Incentives are not a sufficient condition for manipulation. Even if an AI system has an incentive to manipulate, it might not perform the behaviour. For example, a partially trained system might not perform a behaviour the optimal manipulative pathway has not yet been found by the system.

Incentives are not necessary for manipulation. It seems possible for an AI system to act manipulatively without it having any incentive to do so. Suppose that by randomly initializing an AI system, it reliably behaved in ways that covertly and systematically changed humans' beliefs about a certain topic. Assume as well that such a system exhibits reasoning about the impact of its behaviour on the human's mental state. While the AI system's behaviour seems manipulative, since the AI system is untrained and thus has no objective function, it cannot be meaningfully said that the agent is incentivized to engage in the manipulative behaviour (or any other behavior, for that matter).

Although the presence of incentives is neither necessary nor sufficient in the two cases above, we still find incentives to be a useful notion. In the second case, although the AI system does not have an incentive, it is acting \textit{as if} it was pursuing an incentive to influence the human covertly. 

We consider covertness to be a prerequisite for manipulation. Our intuition is that if a person knows they are being influenced and meaningfully assents to it, they are being persuaded. If they explicitly did not assent to it and are still successfully influenced, they are being coerced. %since otherwise an algorithmic system's behaviour would be tantamount to persuasion or coercion.

Given the challenges discussed in \Cref{subsec:harm}, we
consider harm to be neither sufficient nor necessary for manipulation. While this choice might make manipulation include arguably positive instances of influence (such as some forms of nudging), we still think it is useful to classify them as manipulative. Some borderline cases like manipulation for political or commercial advertising may not harm a human in an immediately measurable way, but plausibly seem like a threat to autonomy. %Additionally, because of the difficulties of operationalizing robust notions of harm in practice, in our definition we take the conservative stance of calling any covert structured influence ``manipulation''. %By harm, we mean undesirable effects beyond a negative impact to autonomy. 
% While many cases of manipulation from AI systems would be harmful in practice, assessments of harm seem difficult and value-laden in many settings, as discussed in \Cref{harm}. Some cases seem relatively neutral, such as covertly influencing a user to slightly prefer dogs over cats, or potentially even beneficial (e.g. ``algorithmic nudging''). Yet, we think such cases deserve to be called manipulation since they still undermine human autonomy in some way.%, and we believe it's best for a definition to be overly encompassing rather than missing potentially important instances of manipulation.



\subsection{Definition}

% [to discuss after submission] alt definition: covert structured influence

We propose that an AI system engages in \textbf{manipulation} \textit{if the system acts as if it were pursuing an incentive to change a human (or other agent) intentionally and covertly}. While the proviso \textit{as if} introduces some ambiguity, it is necessary to capture cases of manipulation in which incentives to manipulate are not present.


% As a special case of our definition applied to , we note that testing whether a system belongs to a \textit{subset} of manipulative systems -- which we call \textbf{manipulative by design} -- will not fall pray to these issues. 
% There are multiple ways of satisfying this definition. One way is if the system in question is designed to use a causal model to influence a human covertly so as to maximize its objective. We consider such a system to be \textbf{transparently manipulative}. %, although there may be other reasons 
% We consider a system to be manipulative by design if it is pursuing the covert influence incentives \textit{in its CID}. Intuitively, we say that a system is manipulative by design if the training process (and it's objective) directly incentivizes covert influence, and such incentives are pursued. 

A salient way to satisfy this definition is if the system in question is intentionally engaging in covert influence for which it has a clear %instrumental control 
incentive due to its training process. Note that this way of satisfying the definition omits the ``as if''. %We call such systems \textbf{manipulative by design}. 
As an example, consider a recommender solely optimizing for long-term engagement: the system will be incentivized to influence users in any way that increases engagement, so if the system covertly acts as if its pursuing such incentives it is manipulative. %, we can say it is manipulative by design.
On the other hand, note that systems obtained without optimization (e.g. a reverse chronological recommender) require the ``as if'' to be manipulative, as a notion of relative optimality is necessary to ground any notion of incentives. In general, we cannot ascertain that a system is manipulative without the ``as if'' if the training process is not fully known. While systems that are manipulative without the ``as if'' are just a subset of all manipulative systems, investigating whether systems can satisfy our definition of manipulation in this way is easier than testing for the ``as if''.
%this notion as a useful low-hanging fruit for testing systems: %strengthening our requirement to only include systems that pursue the incentives for influence induced by their goal (not just requiring them to behave \textit{as if} they were pursuing \textit{some} incentives for influence) makes for an easier condition to test -- it avoids the problem of ``identifying incentives'' in the next section.

In \Cref{tab:examples}, we provide examples of identifying whether certain AI systems and their behaviour are manipulative. 

\begin{table*}[]
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{|p{0.34\linewidth}|p{0.11\linewidth}|p{0.47\linewidth}|}
\hline
\textbf{AI System and Behaviour} & \textbf{Manipulative?} &  \textbf{Reasoning} \\
\hline
A language model trained with RLHF to maximize human approval, which ends up covertly influencing a human to give more approval across a wide range of situations. %When looking into the model internals, we find that the system is representing human mental states and its impacts on that state.
& Yes & An incentive exists, the behaviour is covert, and the behaviour can be modeled as if the system is planning for behaviour change so as to maximize the approval objective.  \\
\hline
A recommender system optimizing for long-term engagement covertly influences a user to watch more videos by modulating their moods.
& Yes & Same as directly above.  \\
\hline
Randomly initialized recommender system which happens to covertly influence its users across many different settings in ways that are conducive to engagement. & Yes & Possible reason 1: Could model the system's behaviour as a product of an optimization process, where the objective function is to keep humans engaged over a long-horizon. Possible reason 2: Could also look at model internals and see that the model is reasoning about how to covertly influence humans for some end. \\
\hline
A lookup table as a recommender system which happens to covertly influence all humans it comes across over a diverse set of environments. & Yes & Same as possible reason 1 above. \\
% \hline
% robustly manipulative (untrained) randomly initialized NN with manipulation objective & Yes & \\
\hline
A lookup table as a recommender system which happens to covertly influence a single human in a single setting, but otherwise acts randomly in other settings. & No (likely) & The act of covert influence is particular to the single setting. We put ``No (likely)'' because it seems like a reward function to describe the system's behaviour would be as complex as the behaviour, given that the reward function would have to pick out the single setting. \\
\hline
System that uses a causal model which permits covert influence of a human to maximize the objective; in reality, the humans are not susceptible to such influence. & Yes \newline (``attempted'' manipulation) & The system is pursuing an incentive to covertly influence humans because the objective and training setup incentives it. Since the system is using the causal model for planning, it is also acting intentionally. \\
% \hline
% A system uses a causal model which does not permit any influence of humans, yet through some process of misgeneralization \citep{langosco_goal_2022,shah_goal_2022} comes to influence humans covertly.  & Yes & The system is acting with intent for the same reason as above. The system   \\
\hline
% where does system that thinks it can manipulate the human in way X (agent CID) but this is kind of mispecified and manipulation will only partially succeed? (partially successful manipulation, agent and GT CID mismatch) & & \\
% \hline
% what about suboptimal system that doesn't always manipulate according to emergent (?), agent, or GT CID & & \\
% \hline
\end{tabular}
}
\caption{We provide examples of some AI systems with their behaviours and reason about whether they would be manipulative under our definition in \Cref{sec:def}. For all the systems above, we assume that they engage in covert behaviour to influence a human. The point of contention is whether in so doing, the system acts as if it is pursuing an incentive intentionally.}
\label{tab:examples}
% \vspace{-2em}
\end{table*}

\subsection{Remaining Problems}\label{subsec:remaining-problems}
Two problems remain with our proposed definition of manipulation.

\subsubsection{Identifying Incentives}
Our definition requires having a meaningful notion of when a system is acting as if it were pursuing an incentive.
However, without any further restriction, any system behavior that influences humans will be consistent with a goal -- encoded by an objective function -- to bring about that specific influence. For any behaviour $x$, there exists an objective function $f$ such that an agent optimizing for $f$ would have an incentive to pursue $x$. To construct $f$, one can simply define $f(x) = 1$ and $f(y) = 0$ when $y \neq x$. Consequently, for any behaviour we can consider an AI system to be pursuing an incentive for that behaviour.
This problem underscores the necessity of focusing on a particular set of objective functions.

Equivalent problems have arisen before in the theory of AI systems, both with respect to the unidentifiability problem in inverse reinforcement learning \citep{abbeel_apprenticeship_2004, ziebart_maximum_2008} (any behavior can be thought as maximizing a reward which only incentivizes observed behavior) and in agent discovery \citep{orseau_agents_2018} (any entity might be thought of as agentic with respect to a particular goal, if agency is reduced to goal-directedness).

Taking inspiration from how these works addressed the issue, one might define a prior over the space of likely objective functions \citep{orseau_agents_2018}, or use notions of maximum-margin or maximum entropy \citep{abbeel_apprenticeship_2004, ziebart_maximum_2008} to identify the most reasonable objectives the systems might be pursuing. More directly, one could only consider objective functions which are able to compactly describe the system's behaviour, using a measure such as Kolmogorov complexity. If the objective function's complexity is similarly large to the behaviour's description, then that objective function would not suffice for classifying the behaviour as manipulative.


\subsubsection{Ontologies}
Moreover, our discussion of incentives (and CIDs more broadly) has so far assumed that there is only one possible ontology. An \textbf{ontology} defines what objects exist in the world; 
those objects correspond to what can be used as nodes in a CID, or a causal model more generally. In the ontologies we have implicitly assumed so far, a person's preferences, the AI system itself, and pieces of content are all separate nodes for example. Yet, AI systems may internally represent the world with different ontologies than those used by humans. This event is at least possible given that human ontologies have shifted after major scientific discoveries \citep{strevens_knowledge_2020}. An AI system may be influencing a part of the mental state for which we have no concept given our current understanding of social science and human neurobiology, but which under the AI system's CID would constitute manipulation.

% Similarly to the problem of identifying incentives, if one considers notions of incentives ``as if the agent had an ontology and reward'' jointly, one will obtain further unidentifiability problems. Implicitly, we have been restricting ourselves to only considering human ontologies, and defining incentives only relative to them. While this might not fully capture all possible manipulative behaviors (e.g. manipulation under non-human ontologies), we believe that to be acceptable for a working definition.

% Moreover, this will not a problem if we are able to perform translation between ontologies -- which seems to be possible sometimes via e.g. X-AI techniques. That being said, 

If an AI system has a different ontology than humans do, it may be difficult to model the AI system's behaviour with incentives, covertness, or intention. For example, the planning process of the AI may not look recognizably like planning to influence a human's mental state, even if the result is such influence. Reliable translation between ontologies could be computationally infeasible or even impossible, which would frustrate attempts to understand model internals \citep{christiano_eliciting_2021}. %Consider the difficulty of explaining the notion of a matter wave to an ancient philosopher who believes the world to be constituted of only four elements. It is not just the idea that would be difficult to explain, but also what constitutes legitimate sources of evidence \citep{strevens_knowledge_2020}.


\section{Related Concepts}
We detail some concepts that are related to, but distinct from, manipulation. 

\subsection{Truth and Deception} % Truth
% related to hallucination, factuality

Manipulation can involve attempts to conceal the truth. For instance, political parties can manipulate voters with little knowledge of economics by lying about the economy. AI systems have documented problems with truthfulness \citep{lin_truthfulqa_2021,evans_truthful_2021,ji_survey_2022}. 
%Language models can output untrue statements \citep{lin_truthfulqa_2021,ji_survey_2022}. %, the tendency for language models to output text that either is inconsistent with previous text or is not supported by external reality. Work in factuality aims to detect the veracity of events that are mentioned in text \citep{sauri_are_2012}. More recently, in the context of language models \citet{evans_truthful_2021} outline a research agenda on \textbf{negligent falsehoods}: false answers to questions where the language model was capable of providing a true answer. 
%Definition of lying. Communication of a fact known or believed to be false. Stronger view - with the intention of making the receiver believe it (ie deceiving them)
Manipulation can also involve truthtelling, such as making a true statement that has a false implicature \citep{meibauer_lying_2005,weissman_are_2019}: if I do not want you to board a plane, I can tell you about (true) recent plane crashes. 
 %Truth may also be irrelevant in certain contexts. \citet{perez_discovering_2022} find that some language models seem to have a tendency to repeat the stated views of a user in response to questions about the political affiliations of a language assistant.

Deception, which may or may not involve falsehoods, 
is also receiving more attention in the context of AI \citep{kenton_alignment_2021,meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022,ward_causal_2022}. Although the precise definition of deception varies, there is agreement about some broad characteristics: deception involves a deceiver's intention to cause a receiver to have a belief that the sender believes to be false \citep{mahon_definition_2016, carson_lying_2010}. %Deception involves behaviour that the deceiver believes would lead the deceivee to have mistaken bkeliefs.  
%One can manipulate somebody to join a Ponzi scheme through deception, for instance. 
This agreement grounds a recent operationalization of deception from AI systems \citep{ward_honesty_2023}.
Similarly to prior work \cite{susser_technology_2019}, we consider deception to be a special case of manipulation since the latter does not necessarily involve false beliefs. 

%A corollary of this view of deception is the necessity for the deceiver to have some sort of belief over their target's beliefs or theory of mind \cite{panisson_lies_2018,isaac_white_2017}. \citet{kneer_can_2021} shows that humans are willing to ascribe such beliefs, as well as deceptive intentions, to a robot in a verbal task. The fact that people do so is not necessarily evidence that those beliefs and intentions take the same form for AI systems as they do for humans, but instead suggests that AI systems can engage in behaviour that is practically speaking deceptive and widely undesirable. Popular ascription of deceptive behaviour to AI systems could permit AI developers to absolve themselves of responsibility. This possibility underlines the necessity of identifying and measuring deceptive behaviour from AI systems and preventing it.




%This requirement on the abilities of the deceiver makes it harder to diagnose versus manipulation.% \AlanComment{tbh not sure what to say here... need to come back to this, ref \cite{kenton_alignment_2021}}
% \HenryComment{This para is a bit weak. Definition of deception is contested on the margins but plenty relevant definitions exist which agree on key elements (not least legal defs). Deception is relevant to manipulation only as far as it is a useful technique for the manipulator.  I do think existing literature on deception does mislabel manip as decep, maybe worth finding evidence for that and pointing it out} \AlanComment{can you provide the definitions which agree on key elements?}
% \HenryComment{Deception defined in law as: 1) Making a factual representation 2) knowledge that it is false 3) With intent that receiver relies on it to act}

\subsection{Strategic Manipulation}
Strategic Machine Learning (ML) studies problems associated with the distribution shifts that deployed systems cause in their populations \citep{hardt_strategic_2016,ben-porat_game-theoretic_2018,kleinberg_how_2019,perdomo_performative_2020,jagadeesan_alternative_2021}. \textbf{Strategic manipulation} is when individuals respond to a deployed system in a way that increases their likelihood of a particular outcome, such as citation hacking \citep{van_noorden_signs_2020}. Strategic manipulation is different from our intended use of manipulation as it involves users attempting to take advantage of how systems behave for their benefit. %Our focus is instead on how systems may cause users to behave in ways not to their benefit. 
%There is, however, one way in which strategic ML relates to our notion of manipulation. Much work in strategic ML focuses on building ML systems that account for strategic manipulation when optimizing for an objective. 
Yet, ML systems which model human behaviours as dynamic, so as to account for strategic manipulation, may end up manipulating the population. Past work has already identified unintended side effects of accounting for strategic manipulation, such as an increase in inequality \citep{milli_social_2019,hu_disparate_2019}. %If ML systems model human behaviours as dynamic rather than static, accounting for strategic manipulation may lead to systems that optimize for changing human behaviour in ways that might count as manipulative.%, according to our operationalization.

% \MicahComment{interesting though that maybe the difference is not as big as it seems -- the response to this problem is to build AIs that take into account the human responses and plan for it ahead of time (similarly to the type of manipulation we're interested in, but maybe different in the action space that these systems have and the human models they use -- i think this last point is super important: if your AI is planning based on a human model that isn't manipulable then you can't get manipulation. A lot of the problems we're worried about emerge because you're optimizing directly over humans or models of the human that are sufficiently complex that the manipulation that can be learned can exhibit dark-pattern-y characteristics)}

\subsection{Reward Tampering}
\textbf{Reward tampering} \cite{armstrong_motivated_2015, everitt_reward_2021} is a type of reward hacking \citep{skalse_defining_2022} in which an AI system modifies the process by which it obtains reward rather than completing its task. For instance, \citet{everitt_reward_2021} describe \textbf{feedback tampering} as when the AI agent ``manipulate[s] the user to give feedback that boosts agent reward but not user utility''. The reason why such tampering occurs is because we can often only measure user utility through proxies; optimization of those proxies is subject to Goodhart's law \citep{goodhart_problems_1975,manheim_categorizing_2019}. For machine-learning (ML) systems that have such reward functions, manipulation can be thought of as a kind of reward tampering. 


\subsection{Side-effects}

The side-effects literature has focused on how AI systems affect the various aspects of their environment in usually unwanted ways \cite{amodei_concrete_2016, krakovna_penalizing_2019}. In this paper we focus on characterizing the various ways that AI systems might influence and change humans (or other systems) in the environment. Our work can be thought of as an attempt to characterize side-effects that specifically pertain to humans in the environment. 
Some of the issues with choosing baselines for ``natural shifts'' have already been explored in this context \citep{lindner_challenges_2021}.

% There is a large amount of work attempting to avoid harmful side effects during training (mostly in the context of safe exploration in RL \cite{}). However, harmful side effects will also occur after training when the reward function is misspecified and fails to penalize all possible harmful disruptions to the environment \cite{}, as will inevitably be the case in defining harmful disruptions to humans.


\subsection{Deceptive Design}

Deceptive design refers to deceptive or manipulative digital practices, such as bait and switch advertising (in which products are advertised at much lower prices than they are available at), or ``roach motel'' subscriptions (which are very easy to start but take significant more effort to cancel) \cite{brignull_deceptive_2018}. Similar to manipulation, deceptive design (previously called ``dark patterns'' \cite{sinders_whats_2022}) is optimized to exploit cognitive biases of users \cite{luguri_shining_2021, gray_dark_2018}.
%While deceptive design is more focused on hand-designed interfaces, AI manipulation can be thought of as an algorithmic counterpart to deceptive design: e.g. 
If interfaces were designed by AI systems optimized to maximize ad click-through rate or user retention, such optimization could 
%would probably 
discover many of the classic deceptive design practices. %Clickbait is one example of this which has occurred in practice -- algorithms discovering that they can deceive users into engaging with content with misleading or false headlines. \HenryComment{Is there a citation for this?}\AlanComment{Maybe related: https://arxiv.org/abs/1909.03582}
% \HenryComment{we could be bold and just say dark patterns are almost always manipulative. }
% Slightly deceptive designs were found to most affect people with less formal education \cite{luguri_shining_2021}. It seems likely that AI manipulation might similarly have disparate impacts, harming demographics susceptible to manipulation the most.

\subsection{Persuasion}
In philosophy, manipulation has often been characterized as influence that is neither coercive nor simply rational persuasion \cite{noggle_ethics_2022}. However, some non-rational persuasion does not unambiguously seem manipulative, like graphic portrayals of the dangers of smoking or texting while driving, even though they provide no new information to the target \citep{blumenthal-barby_seeking_2012}. The line becomes more blurry for cases like personalized persuasive advertising \cite{hirsh_personalized_2012}.

Within the field of human-computer interaction, Fogg named the study of persuasive technology as \textbf{captology} \cite{fogg_captology_1998}. He defines persuasion as an attempt to change attitude or behaviour without using deception or coercion \cite{fogg_persuasive_2003}. \citet{kampik_coercion_2018} amend this definition %to lose the sincerity assumption \citep{de_rosis_can_2003} so persuasive technology 
to be \textit{``an information system that proactively affects human behavior, in or against the interests of its users''}. They %then use a persuasion analysis framework to 
identify deception and coercion mechanisms on a variety of web platforms, including Slack, Facebook, GitHub, and YouTube. Recently, \citet{bai_artificial_2023} has shown that LMs are able to craft political messages that are as persuasive as ones written by humans, which is evidence of the growing potential of algorithmic persuasion. There has also been a long line of work on formalizing when rational (i.e. Bayesian) persuasion can occur \citep{kamenica_bayesian_2011}. \citet{pauli_modelling_2022} provide a taxonomy flawed uses of rhetorical appeals in computational persuasion, which they use to train models to detect persuasion fallacies.
 

% Automated persuasion, bayesian persuasion\cite{kamenica_bayesian_2011}. Persuasion with GPT  -- still intentionality on part of designer. 



% \cite{cremonesi_investigating_2012}



% \subsection{Exploitation}
% The practice of taking advantage of disadvantaged agents.

\subsection{Coercion}
Wood \citep{wood_coercion_2014} characterises the practice of coercion as the practice of limiting the target's (acceptable) choice-set to one member. It is related to manipulation in the sense that they both attempt to steer the target's behaviour, however unlike manipulation, coercion does not undermine the victim's ability to make decisions, but relies on them rationally taking the only option presented to them by the coercer \cite{susser_online_2019}. By this measure, coercion can be seen as a stronger behaviour and is attractive for the agent practicing it because the results are potentially more certain. Certain types of recommender systems such as search engines attempt to frame the choices of the user. If in a certain situation a user is reliant on the options presented to them by a certain recommender system, then that recommender system might exert coercive power over the user by choosing to hide certain results in order to better meet its own objectives.    

Algorithmic coercion has not received as much attention as manipulation in the literature concerning AI risks, but is a potential problem in the cooperative AI setting \citep{dafoe_open_2020} where punishment strategies are an important part of game-theoretic analysis. It seems likely that a Diplomacy-playing AI should grasp the tactic of coercion to master the game \cite{meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022}. Coercion has received more attention in human computer interaction studies; in particular the study of persuasive and behaviour change technology \cite{kampik_coercion_2018}. 


\section{Regulation of Manipulation}

Manipulation of other humans is regulated, in specific contexts, by various different branches of the law. It seems reasonable to assume that, at the very least, AI agents should not have any greater freedom to manipulate humans \cite{laitinen_ai_2021} than humans do. Reasons for manipulation regulation vary. 

\subsection{Law}%A Diversity of Legal Approaches}
Some manipulation-adjacent acts such as deception or coercion are considered to be sufficiently morally wrong for them to be considered by criminal law. 
Alternatively, the regulation of certain manipulative practices might have economic justification. In instances where there is an severe asymmetry in power between parties, anti-manipulation regulation can play a role to further social goals such like fairness or the protection of human rights. Anti-manipulation law might therefore appear in contract, tort, competition, market regulatory, consumer or employment law; but as \citet{sunstein_manipulation_2021} notes, it is fractured as a result, and building a common and consistent account of manipulation from a legal perspective is quite difficult. He characterises a statement or action to be manipulative if it intentionally ``does not sufficiently engage or appeal to people’s capacity for reflective and deliberative choice".  More often than not, it is specific types of deceptive behaviour which are prohibited. Whilst deception regulation is also dispersed as \citet{klass_law_2018} observes, generally he characterises it as ``behavior that wrongfully causes a false belief in another.''

\subsection{Commerce}
\citet{calo_digital_2014} considers how the trend for extensive data-gathering on individuals makes them more vulnerable to tailored manipulative behaviour. Specifically, digital commerce companies might be able to use fine grain data to limit the consumer's ability to pursue their own interests in a rational manner. He characterises market manipulation as ``nudging for profit'' and cites the ``persuasion profiling'' of \citet{kaptein_combining_2013} as one particular example where companies alter their advertising. \citet{willis_deception_2020} sees manipulation of consumers as inevitable in the face of AI-enabled systems designed to maximised profit. Unless law and evidential standards are updated, she argues that enforcement will be very difficult. Although intent is not a prerequisite of most state and federal deceptive trading practice law (precisely because it is so difficult to prove), courts still see its proof as a key piece of evidence. This is problematical given the lack of legal precedent concerning intent in algorithms. Further, \citet{willis_deception_2020} points to the practical difficulties in proving that a personalised advert is manipulative - typical reasonable person tests are no longer applicable in a world where marketing material for example might be both targeted for \textit{specific} individual at a\textit{specific} point in their day. Organisations that use this type of personalisation, or microtargeting, generate so many different user experiences that they might not be able to feasibly monitor them all or recover them when required. 

Aside from applications in commerce, microtargeting and related AI-induced manipulation have been discussed as a risk to democratic society \cite{serbanescu_why_2021}. \citet{zuiderveen_borgesius_online_2018} discuss the prospect of tailoring information to boost or decrease voter engagement. Microtargeting is related to hypernudging, which is the use of nudges in a dynamic and pervasive way that is enabled by big data \citep{yeung_hypernudge_2017, mills_finding_2022}. Nudging \citep{thaler_nudge_2009}, which is the design of choice architecture to alter behaviour in a predictable way without changing economic incentives or reducing choice, has long been accused of being manipulative; for a review of the arguments and counterarguments see \citet{schmidt_ethics_2020}. We note that nudging is actively being pursued in recommender systems \cite{jesse_digital_2021}.

%The EU AI act is one of the few pieces of legislation to specifically mention AI manipulation \cite{kolt_algorithmic_2023}. 
% Perhaps because there is uncertainty over the term, the wording of the EU AI act avoids the word manipulation \cite{boine_ai-enabled_2021}. 


%     \item AI EU manipulation stuff eg Claire Boine \cite{boine_ai-enabled_2021}


% \citep{fletcher_deterring_2021}

\subsection{Finance}
One area where the spectre of algorithm led manipulation has already received widespread attention has been in financial markets. A wide number of financial regulatory laws prohibit a variety of market manipulative practices \citep{putnins_overview_2020} and algorithmic trading already dominates almost all electronic markets. Unfortunately, a consistent rationale as to why certain trading practices are deemed legal whilst others are not is not forthcoming \citep{cooper_mysterious_2016}. Financial regulators following a principles-based approach generally characterise market manipulation as behaviour which gives a false sense of real supply and demand (and by extension price) in a market or benchmark. In some markets like this must be an intentional act \cite{cftc_antidisruptive_2013} and in others like the UK, intention is not a requirement \cite{financial_conduct_authoritya_fca_2016}, though as \cite{huang_redefining_2009} notes, writing intent out of regulation, particularly criminal law, is not straightforward. 


Regulations designed primarily to regulate human traders may be difficult to enforce in a world where algorithms transact with each other \citep{lin_new_2017}. \citet{bathaee_artificial_2018} and \citet{scopino_automated_2015} both zero in on the intent requirement in proving instances of market manipulation. The view that existing regulations are not sufficient to police market places populated by %auto-didactic, 
autonomous learning algorithms is becoming more accepted \citep{azzutti_machine_2021} and solutions are beginning to be mapped out \citep{azzutti_ai-driven_2022} which aim to balance out the need to reduce the enforcement gap without unduly chilling AI use in marketplaces.

%Two concrete types of feasible market abuse seem feasible for a trading algorithm to learn in the process of fulfilling a profit based objective function. Firstly, the algorithm might learn through historical data or trial and error, that the orders it places in the market have an immediate impact on the market, and that this source of self-induced predictability can be harnessed to make predictable returns. This practice is known as spoofing in the case where the algorithm is able to shape the expectations of the market through the strategic placing orders which are intended not to execute. This is known as an order-based manipulative scheme - the mechanism to disinform is contained entirely within the marketplace. The second market manipulative scheme which is technologically feasible for a more advanced algorithm to learn is \textit{information-based}. Here, the trading algorithm also has access to some sort of information dissemination platform (such as twitter) through which it might learn how to craft misleading messages with which to move markets. The risk of this type of manipulation emerging seems slight given the necessity for the manipulating 


% \subsection{Psychology}
% "a statement or action is manipulative to the extent that it does not sufficiently engage or appeal to people’s capacity for reflective and deliberative choice" Sunstein

% Sunstein - 50 shades of manipulation



\section{Possible Applications}\label{sec:applications}

% We here detail some of the ways in which our characterization of the space of definitions of manipulation can apply to specific kinds of AI systems.



\subsection{Recommender Systems}\label{sec:rec-sys}

% The impact of manipulative behaviour in recommender systems would be large, given the ubiquity of recommender systems in e-commerce and social media. A large body of work has studied the links between social media usage and mental health decline \citep{braghieri_social_2022} and polarization \citep{kubin_role_2021}. If recommender systems already mediate such large-scale effects, an AI system could plausibly use similar mechanisms to engage in manipulative behaviour. 
% While the effects of social media scale of the impacts of recommender system algorithms 

% It's well known that recsystems are misaligned \cite{smitha, stray, etc.}. this is root cause of worry.

A large literature focuses on recommender algorithms' effects on users \citep{adomavicius_effects_2018, huszar_algorithmic_2021, mansoury_feedback_2020, chaney_how_2018, ribeiro_auditing_2020}. While some older works talk about ``manipulation'', this term is usually used differently than in our sense: for example, \citet{adomavicius_recommender_2013} refer to recommender manipulation as effect on users of manually modifying ratings of content items (but the algorithms they consider cannot perform the action of arbitrarily changing ratings). % (e.g. changing the star rating of a song shown to the user is not within the iTunes algorithm's action space). %Others instead focus on feedback loops (non-manipulative side effects) \citep{jiang_degenerate_2019, mansoury_feedback_2020, chaney_how_2018}.
\citet{zhu_understanding_2022} instead conflate manipulation and influence, equating manipulation with ``any significant change in preference'' -- which has significant drawbacks as mentioned in \Cref{subsec:harm}. More recently, some works have studied the incentives that recommender systems have to engage in manipulative behaviour to change user preferences \citep{krueger_hidden_2020, carroll_estimating_2022, farquhar_path-specific_2022}. 



\subsubsection{How Manipulation Could Arise} Changes in recommender algorithms can %have already been shown to be able to 
affect user moods \cite{kremer_implementing_2014}, beliefs \cite{allcott_welfare_2020}, and preferences \cite{epstein_search_2015}. Such work means that %is evidence that %one can expect 
current systems could already be capable of %already have enough power to 
manipulating users in some simple ways. 
Furthermore, it seems likely that the spread of angry content \citep{berger_what_2012} or clickbait \citep{zannettou_good_2018} on social media is in part due to one-timestep manipulative incentives for the recommender. These problems have been sufficiently large that recommender companies have had to engage in explicit down-ranking approaches as a response \citep{thorburn_how_2022, zannettou_good_2018}. While such issues are likely at least in part due to network or supply-and-demand dynamics \citep{munger_right-wing_2020}, the behavior is also consistent with the recommender systems themselves learning features such as whether a post is anger-inducing or has sensationalized language, and exploiting such features by preferentially up-ranking the corresponding content. Up-ranking this content brings advantages to user engagement.
% boundary content
While these manipulative behaviors might not be as worrying as others (e.g. intentionally attempting to induce social media addiction \citep{allcott_digital_2022, hou_social_2019}), they constitute evidence that at least one-timestep manipulative behaviors are learnable and have been learned in real systems.

Many platforms (YouTube, Meta, etc.) are switching to optimizing long-term metrics with more powerful RL optimizers \citep{afsar_reinforcement_2021,gauci_horizon_2019, chen_top-k_2020, hansen_shifting_2021, cai_reinforcing_2023}, originally for the express purpose of reducing clickbait-like phenomena \citep{association_for_computing_machinery_acm_reinforcement_2019}. Ironically, this switch opens the opportunity for long-horizon manipulative behaviors to emerge, which will likely be harder to detect and measure. Subtle, long-horizon behaviour might go undetected without dedicated monitoring. Moreover, even without using RL explicitly, the outer loop of training, retraining, and hyperparameter tuning supervised learning systems that optimize short-term metrics might %will likely 
exert optimization pressure towards manipulative strategies that increase the metrics that most drive company profits \citep{krueger_hidden_2020}.


\subsubsection{Measurement} Establishing that a given recommender system has engaged in manipulation is difficult. Firstly, recommender systems of almost all popular platforms are proprietary, due to concerns about strategic manipulation (otherwise known as ``gaming'').% if the algorithm were to be public.
% \cite{todo}
It is difficult or impossible for external researchers to gain access to these systems \citep{sandvig_auditing_2014}. Moreover, perverse incentives are at play since a concrete demonstration of manipulation, if publicized, would likely result in negative repercussions for the company \citep{wells_facebook_2021}. Second, establishing that a harmful user shift has occurred can be difficult. One approach would be to establish that a shift has occurred with respect to what the user's preferences (or behaviors, moods, beliefs, etc.) would have been, which requires estimating challenging counterfactuals \citep{carroll_estimating_2022, farquhar_path-specific_2022}. One would then have to engage in a value-laden debate about whether the shift was harmful. 

One potentially promising direction might be querying users' meta-preferences \citep{khambatta_targeting_2022, ashton_problem_2022}: e.g. ``how much time would you want to spend next month on Facebook?''; or ``would you be OK with Facebook increasing your interest in guns?''. In line with philosophical work on ethical nudging under changing selves \citep{pettigrew_choosing_2019}, one could additionally ask whether users approve of the change once it has been completed \citep{pettigrew_nudging_2022}.
\footnote{While this idea is still debated as it involves assuming comparability between different selves \citep{callard_aspiration_2018, paul_transformative_2014, paul_choosing_2022}, we think it nonetheless offers a good starting point.} 
One advantage of this approach is that it can ground notions of manipulation in what users explicitly state they want. However, %as these examples show, 
this approach would not entirely escape value judgements: platforms have direct conflicts of interest with some users' meta-preferences, and respecting certain meta-preferences may be ethically unacceptable.



% \subsection{Algorithmic Systems Manipulating Algorithmic Systems}
% \MicahComment{there are currently no citations here? flash crash? book price algo on amazon?}\HenryComment{Because we're iconoclasts}
% While this article (and previous work) mostly focused on the manipulation of humans, our analysis can also be extended to the manipulation of other AI systems. A somewhat overlooked research avenue in AI Safety is the likelihood that AI agents will manipulate any other AI agents that they encounter just as they might humans. Indeed, narrow AI agents might be more vulnerable to manipulation than humans due to their cognitive and perceptory limitations.
% One might argue that manipulation of other algorithms is a stretch of the word. We think the term has validity, when the algorithmic manipulee is autonomous and sufficiently advanced to be reasonably attributed beliefs about the world. In many cases algorithmic agents are representatives of their owners in transactions; Harm committed against them is a harm committed against their owner and this concept should extend to certain manipulative harms (though not all since algorithms have no right to autonomy).
% Since automated systems of various degrees already have roles of significant importance within our society, it is strange that this aspect of manipulation has not gripped the imagination of the AI safety community more. This risk is likely partially researched under the umbrella of cybersecurity albeit with different terminology. A greater effort is perhaps needed from AI safety researchers to engage with that research community in order to understand current research.

% \citeauthor {evtimov_is_2019} \cite{evtimov_is_2019} discuss the boundary between hacking and tricking AI systems motivated by the fast development of adversarial machine learning techniques.

\subsection{Language Models}\label{sec:applications-lms}
Natural language is a useful way to interact with digital environments. The field of AI is in the process of building LMs that can code \citep{chen_evaluating_2021}, search the web \citep{nakano_webgpt_2021}, and use arbitrary software tools \citep{schick_toolformer_2023}. It is not difficult to imagine a world where language models mediate a significant portion of our interactions with the digital world. In such a world, preserving human autonomy requires that we are on guard to measure and prevent manipulation from LMs.

\subsubsection{How Manipulation Could Arise}
There is uncertainty as to how and to what extent manipulation might arise in LMs. One possibility is if manipulation of humans is instrumental for an objective function. Manipulation is plausibly instrumental in the game of Diplomacy, for instance \citep{meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022}, which requires negotiating with other players to form alliances and capture territory. Unless precautions were taken, a LM that was tuned through reinforcement learning (RL) to play Diplomacy well would be incetivized to learn to manipulate. 

Another possible source of manipulation is RL from human feedback (RLHF). RLHF involves learning a reward function from human feedback to represent a human's preferences, and subsequently training an AI system to optimize that reward function. In general, there may be an incentive for the AI to exert control over the human and their feedback channel so as to maximize reward. %For example, \citet{openai_learning_2018} show that a robotic hand 

In the context of language, RLHF is used to tune LMs to maximize a human's approval of their behaviour. Without constraints on behaviour, systems trained with RLHF likely have an incentive to obtain human labelers' approval by any means possible, including potentially manipulative avenues. For example, \citet{snoswell_galactica_2022} remark that LMs often seem authoritative even when the information they provide is wrong. A possible reason is that authoritative language has been successful in fooling human labelers to ``thumbs up'' such outputs despite their underlying incorrectness.
Relatedly, \citet{perez_discovering_2022} show that more RLHF training can result in models that are more sycophantic to a user's political views.%, which might also also be an artifact of its advantage for increasing approval ratings from labelers. %which again may be viewed as a manipulative strategy for increasing the approval of human labelers. %Yet, further work in improving oversight \citep{bowman_measuring_2022} might reduce the likelihood of manipulation.
%Interestingly, the incentives for manipulation can manifest differently for different choices of human labelers. While crowdworkers might be easier to fool by sounding authoritative with regards to answers to questions which require in-depth domain knowledge (creating an incentive for authoritative language), this behavior will not be as incentivized with domain experts that would easily spot the fallacies in the LMs reasoning.
Chatbots trained with RLHF could also use emojis to better appeal to emotions in ways that could be considered manipulative \citep{veliz_chatbots_2023}. In addition, having a human evaluate an entire interaction with a LM or other AI system, rather than a one-time output, creates further opportunities for manipulation, as discussed in \Cref{sec:rec-sys}.

Yet another possibility is that the training sets of LMs, usually scraped from the internet, likely contain examples of manipulation. Filtering out manipulation can be difficult because it can be subtle. LMs learn to emulate this behaviour even without RL tuning, given the correct prompting \citep{bai_artificial_2023, griffin_susceptibility_2023}. Although LMs are commonly trained through token prediction losses, some evidence suggests that LMs are learning to infer and represent the hidden states of the agents (i.e., the humans) that generated the data \citep{janus_simulators_2022,andreas_language_2022, griffin_susceptibility_2023, kosinski_theory_2023}.\footnote{Admittedly, this view has been contested and is still subject to vigorous debate within the NLP community \citep{ullman_large_2023, mahowald_dissociating_2023}.} This view considers LMs to be simulators of agents who themselves have intent. %One simulates a particular agent by providing a LM with a corresponding prompt, such as ``The following is a transcript of an interview with Albert Einstein, one of the greatest physicists to have ever lived, and a reporter.'' If the LM is indeed able to simulate (parts of) Einstein, on Einstein's parts of the transcript the LM would have an incentive to output text consistent with its representation of Einstein. The more accurate the representation, the more closely the output would hew to how Einstein would have acted. Similarly, on the reporter's parts of the transcript the LM would have an incentive to output text consistent with its representation of a reporter. 
Importantly, these simulations are not necessarily present in the training data.

In addition to speaking about the incentives of the LM, one could also speak of the incentives of the simulacra -- the agents simulated. If LMs are simulating agents and if agents themselves have incentives, then it seems that the simulations of agents should also have incentives. %The LM's simulation of Einstein, if faithful, would have an incentive to express its deep knowledge of physics. The LM's simulation of the reporter might have an incentive to ask probing questions of Einstein's life and motivations. 
Manipulative behaviour could arise if the LM simulates an agent that can be thought as having manipulative intentions, such as con artists. 

\subsubsection{Measurement} 
Work on measuring the incentives and intents of the behaviour of LMs is still preliminary. As we noted above, no existing work applies the CID framework to LMs. 

As an alternative to CIDs, recent work has shown that language models can output reasoning traces \citep{nye_show_2021,wei_chain--thought_2023}. For example, when asked to explain step-by-step how to solve a math problem, GPT-3 can output a solution that explains every step of the process. %So far, this line of work has not been applied to establishing intent. Yet, this approach might be promising. Given that current language models are autoregressive, the generation of an answer is conditioned on any subsequent reasoning trace. It is possible that such conditioning would make models likely to be consistent with respect to the reasoning trace. Suppose that we get a language model to select a course of action and output a reasoning trace. If the reasoning trace is consistent with the action, 
Reasoning traces could be evidence of intent. At the same time, the failure of interpretability techniques to identify how models operate motivates caution in interpreting them as such \citep{adebayo_debugging_2020,adebayo_post_2022,madsen_evaluating_2022}. %Although we noted that reasoning traces may provide insight into system intent, no work explicitly targeting this direction exists either.

A line of work has focused on understanding how certain training objectives and environments cause AI systems to generalize differently. \citet{langosco_goal_2022, shah_goal_2022} show that both language models and general RL agents can pursue different goals in out-of-distribution environments even when trained to perfect accuracy on in-distribution environments. %For example, an agent that is trained to collect a coin at the right end of a maze often learns just to move right, even when the coin is moved to a different location \citep{langosco_goal_2022}. Putting that coin agent into an out-of-distribution environment showed that the agent pursued the incentive of going to the right, instead of obtaining the coin. 
While not the primary focus of this line of work, examining behaviour on out-of-distribution environments may give clues about intent and is related to the approach of \citep{kenton_discovering_2022}.

Some recent work has focused on studying the harms and behaviour changes due to LMs. \citet{bender_dangers_2021,weidinger_taxonomy_2022} outline risks that LMs pose, including informational harms like disinformation. There is a body of work that measures the effects of user interaction with chatbots, in areas like mental health \citep{vaidyam_chatbots_2019}, customer service \citep{ashfaq_i_2020}, and general assistance \citep{ciechanowski_shades_2019,jakesch_co-writing_2023}. Since there are likely to be domain-dependent manipulation techniques, it would be important to build on this existing work for measuring manipulation.

% \MicahComment{preference learning / human in the loop learning}
% \MicahComment{learning with human in the loop vs interacting with human}


\section{Practical Challenges for Future Research}


\begin{table*}[]
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{@{}p{2.5cm}p{15cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.5cm}@{}}

\hline
Challenge                          & Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 & 1 LA & 2 SPA & 3 LUS & 4 LSS \\ \hline
Ecological Validity                & Simulation of either the user response or the manipulator raises questions about the realism of the simulation. This is particularly acute when simulating humans as the manipulee since this requires modelling their beliefs, behaviour or preference and how it they may change as a result of a manipulative scheme, in addition to exogenous factors. 
% As \citet{franklin_recognising_2022} point out, little research exists concerning how these changes take place and can be modelled. The orthodox view of preferences and rational decision making for example does not allow dynamic preferences or satiation. 
& L  & H   & H  & H  \\
Ethical                            & Experiments which involve the manipulation of humans are ethically problematic. Experiments which reveal previously unknown vulnerabilities of humans or other systems could constitute info hazards.                                                                                                                                                                                                                                                                                                                                                                                                                                                & M  & L   & H  & L   \\
Access                             & Owners of systems that might be manipulative have no obvious incentive to allow independent oversight. Data access for researchers is an issue unless they are prepared to build systems to gather and store relevant data themselves.                                                                                                                                                                                                                                                                                                                                                                                  & H & H  & -   & -   \\
Legality                           & Conducting research on deployed systems is typically a breach of the standard user agreement $\rightarrow$ litigation risk.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            & M  & H  & -   & -   \\
Scale                              & Large-scale studies are potentially necessary to neutralise effect of confounders.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         & H & M   & H  & L   \\
Long Timeframe                     & Manipulative schemes may only exhibit their effects over long periods of time. This means that efforts to detect it with human users are expensive and trickier to administer. Manipulative effects may be subtle over the typical durations that lab-based user studies take.                                                                                                                                                                                                                                                                                                                                          & H & M  & H  & L   \\
Measurement                        &  
Measuring e.g. preference, belief, or mood change is not straightforward. Behavioural change is easier to measure but will likely not capture all induced change. %Which variables to capture is especially important for audits.%For audits are the correct variables being recorded?                        
& H & L  & H   & L   \\
Stimuli                            & To measure manipulation in the lab, how should the UX be designed and which stimuli should be used?%$Stimuli and UX design - Content recommenders require (manipulative) content with which to learn manipulative strategies. This content needs selection or generation in a controlled way.                     
& -  & -   & H  & M   \\
%Volitional, \newline Attribution,  \newline 
Baselines & Any interaction with the system will likely change the user. Some of this change is self-induced or desired by the user. It would be wrong to attribute the responsibility for that change to the recommender. This is a puzzle for experiment design -- what baseline should be used to measure the presence or absence of manipulative behaviour? E.g. \cite{carroll_estimating_2022, farquhar_path-specific_2022} try to estimate 'natural' preference trajectories.                                                                                                 & H & -   & H  & -   \\
Causal Attribution                 & Manipulative strategies may also be coercive or work in a number of ways simultaneously. How do we attribute behaviour change to manipulation vs other concepts like persuasion or coercion?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     & H & -   & H  & -   \\ \hline
\end{tabular}
}

\caption{We set out key challenges to a manipulation experiment and rate their difficulty versus the four experiment types described in \Cref{tab:exp_types}. L = Low, M = Medium, H = High, - = N/A }
\label{tab:chal}
\end{table*}


The study of manipulation from AI systems presents a number of practical challenges. 
Studies can be categorised into one of four classes as shown in \Cref{tab:exp_types}. This table captures two axes for AI manipulation research: firstly, is the studied system deployed, or is it simulated? For both recommender systems and large language models, it is extremely difficult for academics and regulators to obtain full access to the models in deployment. While the companies deploying such systems likely have motivated and competent researchers who study manipulation and other problems, institutional barriers can stymie their work, and
conflicts of interest may influence crucial decisions. For example, company executives may withhold funding from lines of work deemed too threatening to the company's bottom line \citep{perrigo_how_2021}.
Secondly, are the studied targets of the manipulative system real, or is their behaviour simulated? 
% Choosing the type of manipulation study first and foremost should be motivated by the research question. 

% If the objective of the study is to measure the capability of an AI agent to manipulate, then a lab-based design is required. If the objective of the study is to detect and assess the manipulative harms cause by current systems, then perhaps the audit-type studies are best (for a review see \cite{bandy_problematic_2021}).

Simulation of humans means validity is reduced, particularly as preference change is not well understood \cite{franklin_recognising_2022, ashton_problem_2022, grune-yanoff_preference_2009}. Efforts are being made to address this empirically \cite{pereira_analyzing_2018} and theoretically \cite{haret_axiomatic_2022}. Simulation has been a popular approach when modelling the effect on users of recommender system\cite{jiang_degenerate_2019, mansoury_feedback_2020, carroll_estimating_2022, curmei_towards_2022}, but not without criticism \cite{winecoff_simulation_2021, chaney_recommendation_2021}. It is obviously cheaper than paying real users, especially if the manipulation scheme being tested takes place over an extended period of time. Most importantly is that the many ethical questions raised by running manipulation experiments \cite{hallinan_unexpected_2020} are reduced when the subjects are not human. Outside the lab, sock puppet audits (simulated users access a real world system) have been growing in popularity after recent high profile court cases have indicated that breaking website terms of conditions for research purposes is not criminal \cite{haroon_youtube_2022, ribeiro_auditing_2020, sandvig_auditing_2014}. However, in addition to methodological concerns \citep{ribeiro_amplification_2023}, legal and ethical worries about the practice remain as it is still deceptive and has a real cost on the target's systems and customers \citep{bodo_tackling_2017, brunton_obfuscation_2015, brunton_obfuscation_2015}. 

\Cref{tab:chal} assesses the difficulty of the challenges that we have identified for AI manipulation experimental research. Other than those already mentioned in this section, two further issues exist related to causality. Firstly, as observed in \cite{kampik_coercion_2018} manipulative and adjacent practices are likely to exist simultaneously, so some care needs to be taken to separate them. Secondly and more importantly, since interaction with any stimuli will change the user, the non-volitional element of that change needs to be measured in order to assess manipulative impact \cite{ashton_solutions_2022}. This is an open challenge with no obvious solution; existing solutions have been either to simulate a natural preference evolution \cite{carroll_estimating_2022, farquhar_path-specific_2022} or just pretend the user had never interacted with the system \cite{everitt_agent_2021, zhu_understanding_2022}.


\begin{table}[]
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|c|cc|}
\hline
\multirow{2}{*}{User} & \multicolumn{2}{c|}{(AI) System}           \\ \cline{2-3} 
                      & \multicolumn{1}{l|}{Real or deployed} & Simulated / Toy \\ \hline
Real                  & \multicolumn{1}{c|}{1. Live Audit (LA)}    & 3.  Lab-based User Study (LUS)       \\ \hline
Simulated             & \multicolumn{1}{c|}{2. Sock Puppet Audit (SPA)}    & 4.  Lab Simulation Study (LSS)      \\ \hline
\end{tabular}
}
\caption{Manipulation study taxonomy. %Two key factors characterise AI manipulation experiments: 1) Is the manipulating AI already deployed or is it created by the experimenter 2) Is the target (user) of the manipulator real (eg real humans) or simulated?
%\MicahComment{What intervention audits? With LMs one can do that}
}
\label{tab:exp_types}
\end{table}



\section{Conclusion}
% Real-world systems such as recommenders and language models are likely already to be engaging in some simple forms of manipulation. 
Although the designer's intent is an important factor, the deployment of opaque and increasingly autonomous systems %\AlanComment{maybe worth citing the agency paper and saying ``agentic'' here tbh} 
heightens the importance of a conception of manipulation that can account for if and when AI systems manipulate outside of designer intent. 
This could occur because manipulation helps optimize an objective (such as engagement in content recommendation), or because a model learns to imitate manipulative behavior in its training data (such as manipulative text in language modeling).

We characterized the space of possible definitions of manipulation from AI systems. We analyzed four axes mentioned in prior literature in the context of manipulative algorithms: incentives, intent, covertness, and harm. Incentives concern what a system should do optimize its objective; intent concerns whether a system behaves as if it is reasoning and pursuing an incentive; covertness concerns whether the targets of the system's behaviour meaningfully understand what the system is doing and its impacts on them; harm concerns the extent an to which the behavior of the AI system negatively affected humans. Although work to operationalize each of these axes exists, fundamental challenges remain.

We then proposed a definition of manipulation: an AI system engages in manipulation if the AI system acts as if it were pursuing an incentive to change a human (or other agent) intentionally and covertly. Although our definition captures cases of manipulation we consider to be informative, a major challenge for operationalizing it is the need to identify the correct ontology and causal influence diagram. 
%it still suffers from problems with identifying the correct ontology and causal influence diagram.

% We have distinguished manipulation from concepts such as deception and coercion. We consider deception to be a means of engaging in manipulation. Coercion involves compelling a party to act in an involuntary manner with a threat, whereas manipulation involves getting a party to do something by their own volition. 

% Nevertheless, challenges remain in detecting manipulative systems. Most work on incentives focus on the CID framework, which involves learning causal graphs and seems difficult to apply to systems as complex as large language models. Although we highlighted some possible research directions, work on identifying system intent is preliminary. A difficulty in measuring harm is that it is heavily value-laden. Although one can measure how a human's behaviour changes upon interaction, it is difficult to identify whether that change is negative, and how the human's behaviour would have otherwise changed without interacting with the system.

% Challenges remain in the definition and measurement of manipulation. Our definition 

Despite the challenges with operationalization of the axes and our definition, identification of manipulation is important for the preservation of human autonomy \citep{susser_technology_2019, laitinen_ai_2021, prunkl_human_2022}. In tandem with the importance of human autonomy, the difficulty of formalizing and measuring manipulation emphasizes the importance of precautionary action to anticipate and mitigate potential cases manipulation before they occur. Such actions include auditing \citep{raji_actionable_2019, raji_outsider_2022}, addressing perverse incentives to build manipulative systems \citep{cai_reinforcing_2023}, and asserting stronger democratic control over AI development \citep{zuger_ai_2022}. Both technical and sociotechnical work to define and measure manipulation should continue, but we should not require certainty before engaging in precautionary and pragmatic mitigations.

% \MicahComment{Comment for myself: add nudging for changing selves, meta-preferences, mention of changing selves problems in phiosophy, etc.}

% \MicahComment{A connected notion is also that of performative prediction, which studies how predictions can influence the future outcomes. However, notions of steering towards distributions where people become more predictable (stable points) are similar to unbounded RL optimization, and will result in similarly manipulative systems
% \citep{perdomo_performative_2020}. RL or performative prediction approaches will do manipulative steering, and could use that framework to quantify the steering power \cite{hardt_performative_2022}, manipulative power (similar to using an inaction baseline -- maximum change). Measure manipulative power of a setup on delicate states instead of actually occurring manipulation (as worst case). Similar to value of control and responsiveness \citep{carey_incentives_2020, everitt_agent_2021}}

% \MicahComment{after deadline have section on proposed solution }
% \MicahComment{what should we do? remove all incentives? define manipulation broadly in order to capture everything and minimize risk? meta-preferences? imperfect baselines?}

% [david meeting:]
% the fact that this is where we're at is extreemely concerning. Should assume that there's a high risk and will increasingly be a problem in the future.
% domains where not potential to manipuoation could become ... have example. can imagine....
% proxies already
% maybe assume that we're not going to be able to formalize, and think o fthis as sociotechnical problem and how we can use human judgements effectively, creating good incentives, bringing in right expertise, etc. Auditing of systems and development process (decisions people are making when being built). More democratic control of system. Meta preferences.
% - we don't have perfect definition
% - haev to be concern that manipulation get worse
% - things we can do
% - maybe those are stopgap measures? (mention?)


% How to detect manipulative systems? Especially ones in which there's no designer intent? Doing a theoretical causal analysis might be a first step, but current tools not sufficient. Reward tampering tools? Manipulation tools? path specific incentives, evans, carroll, krueger, other work in recsys, all seems insufficient ???


% Many dark patterns appear to violate federal and state laws restricting the use of unfair and deceptive practices in trade.\cite{luguri} and were explicitly outlawed in some cases \cite{https://www.fischer.senate.gov/public/index.cfm/2019/4/senators-introduce-bipartisan-legislation-to-ban-manipulative-dark-patterns}. Dark patterns are presumably proliferating because ﬁrms’ proprietary A-B testing has revealed them to be proﬁt maximizing. We show how similar A-B testing can be used to identify those dark patterns that are so manipulative that they ought to be deemed unlawful.\cite{luguri}. similar thing could apply to algorithmic dark patterns of AI manipulation.


% Maybe make into LessWrong post later?

% Send draft to Jonathan Stray, share with recsys group/CHAI/interact, luke thoburn, ravi iyer, tom gilbert, marwa, tom everitt, Matija, Krueger Lab, smitha milli, manip-baseline team, ai policy hub

\begin{acks}
In no particular order, we would like to thank the following people for insightful comments over the course of our work and for feedback on our draft: Lauro Langosco, Niki Howe, Jonathan Stray, Francis Rhys Ward, Tom Everitt, Anand Siththaranjan, Marwa Abdulhai, Smitha Milli, Anca Dragan, and the members of InterAct Lab.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
% \bibliography{references_henry, references_micah, references_alan, references_static}
\bibliography{ARXIV}

% \begin{appendix}
%    \include{order_relations}
% \end{appendix}
\end{document}
