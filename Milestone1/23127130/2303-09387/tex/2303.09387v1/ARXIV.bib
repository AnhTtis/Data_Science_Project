
@misc{openai_learning_2018,
	title = {Learning {Dexterous} {In}-{Hand} {Manipulation}},
	url = {https://arxiv.org/abs/1808.00177v5},
	abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
	language = {en},
	urldate = {2023-03-16},
	journal = {arXiv.org},
	author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1808.00177},
}

@misc{apa_dictionary_of_psychology_definition_2023,
	title = {Definition of manipulation},
	url = {https://dictionary.apa.org/manipulation},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	language = {en},
	urldate = {2023-03-15},
	author = {APA Dictionary of Psychology},
	year = {2023},
}

@book{braiker_whos_2003,
	title = {Who's {Pulling} {Your} {Strings}?: {How} to {Break} the {Cycle} of {Manipulation} and {Regain} {Control} of {Your} {Life}: {How} to {Break} the {Cycle} of {Manipulation} and {Regain} {Control} of {Your} {Life}},
	isbn = {978-0-07-143568-0},
	shorttitle = {Who's {Pulling} {Your} {Strings}?},
	abstract = {A powerful program to stop manipulators in their tracks In Who's Pulling Your Strings?, Dr. Harriet B. Braiker, New York Times bestselling author of The Disease to Please, explains how depression, low self-esteem, anger, and feelings of helplessness can be caused by relationships with manipulative people. She exposes the most common methods of manipulators, and with the help of selfassessment quizzes, action plans, and how-to exercises, she helps you recognize and end the manipulative cycle for good.},
	language = {en},
	publisher = {McGraw Hill Professional},
	author = {Braiker, Harriet},
	month = sep,
	year = {2003},
	note = {Google-Books-ID: dGwgiQvyeq0C},
	keywords = {Business \& Economics / Management, Self-Help / General},
}

@misc{hong_learning_2023,
	title = {Learning to {Influence} {Human} {Behavior} with {Offline} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2303.02265},
	doi = {10.48550/arXiv.2303.02265},
	abstract = {In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies not appearing in the dataset, by utilizing components of diverse, suboptimal interactions. In addition, we demonstrate that offline RL can learn influence that adapts with humans, thus achieving long-term coordination with them even when their behavior changes. We evaluate our proposed method with real people in the Overcooked collaborative benchmark domain, and demonstrate successful improvement in human performance.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Hong, Joey and Dragan, Anca and Levine, Sergey},
	month = mar,
	year = {2023},
	note = {arXiv:2303.02265 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{hou_social_2019,
	title = {Social media addiction: {Its} impact, mediation, and intervention},
	volume = {13},
	issn = {1802-7962},
	shorttitle = {Social media addiction},
	url = {https://cyberpsychology.eu/article/view/11562},
	doi = {10.5817/CP2019-1-4},
	abstract = {This research examined the relations of social media addiction to college students' mental health and academic performance, investigated the role of self-esteem as a mediator for the relations, and further tested the effectiveness of an intervention in reducing social media addiction and its potential adverse outcomes. In Study 1, we used a survey method with a sample of college students (N = 232) and found that social media addiction was negatively associated with the students' mental health and academic performance and that the relation between social media addiction and mental health was mediated by self-esteem. In Study 2, we developed and tested a two-stage self-help intervention program. We recruited a sample of college students (N = 38) who met criteria for social media addiction to receive the intervention. Results showed that the intervention was effective in reducing the students’ social media addiction and improving their mental health and academic efficiency. The current studies yielded original findings that contribute to the empirical database on social media addiction and that have important theoretical and practical implications.},
	language = {en},
	number = {1},
	urldate = {2023-03-15},
	journal = {Cyberpsychology: Journal of Psychosocial Research on Cyberspace},
	author = {Hou, Yubo and Xiong, Dan and Jiang, Tonglin and Song, Lily and Wang, Qi},
	month = feb,
	year = {2019},
}

@article{allcott_digital_2022,
	title = {Digital {Addiction}},
	volume = {112},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20210867},
	doi = {10.1257/aer.20210867},
	abstract = {Many have argued that digital technologies such as smartphones and social media are addictive. We develop an economic model of digital addiction and estimate it using a randomized experiment. Temporary incentives to reduce social media use have persistent effects, suggesting social media are habit forming. Allowing people to set limits on their future screen time substantially reduces use, suggesting self-control problems. Additional evidence suggests people are inattentive to habit formation and partially unaware of self-control problems. Looking at these facts through the lens of our model suggests that self-control problems cause 31 percent of social media use.},
	language = {en},
	number = {7},
	urldate = {2023-03-15},
	journal = {American Economic Review},
	author = {Allcott, Hunt and Gentzkow, Matthew and Song, Lena},
	month = jul,
	year = {2022},
	keywords = {Computer Software, Technological Change: Choices and Consequences, Consumer Economics: Empirical Analysis, Allocative Efficiency, Cost-Benefit Analysis, Micro-Based Behavioral Economics: General, Micro-Based Behavioral Economics: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making, General Welfare, Diffusion Processes, Well-Being, Information and Internet Services},
	pages = {2424--2463},
}

@article{munger_right-wing_2022,
	title = {Right-{Wing} {YouTube}: {A} {Supply} and {Demand} {Perspective}},
	volume = {27},
	issn = {1940-1612},
	shorttitle = {Right-{Wing} {YouTube}},
	url = {https://doi.org/10.1177/1940161220964767},
	doi = {10.1177/1940161220964767},
	abstract = {YouTube is the most used social network in the United States and the only major platform that is more popular among right-leaning users. We propose the ?Supply and Demand? framework for analyzing politics on YouTube, with an eye toward understanding dynamics among right-wing video producers and consumers. We discuss a number of novel technological affordances of YouTube as a platform and as a collection of videos, and how each might drive supply of or demand for extreme content. We then provide large-scale longitudinal descriptive information about the supply of and demand for conservative political content on YouTube. We demonstrate that viewership of far-right videos peaked in 2017.},
	language = {en},
	number = {1},
	urldate = {2023-03-15},
	journal = {The International Journal of Press/Politics},
	author = {Munger, Kevin and Phillips, Joseph},
	month = jan,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	pages = {186--219},
}

@book{callard_aspiration_2018,
	address = {Oxford, New York},
	title = {Aspiration: {The} {Agency} of {Becoming}},
	isbn = {978-0-19-063948-8},
	shorttitle = {Aspiration},
	abstract = {Becoming someone is a learning process; and what we learn is the new values around which, if we succeed, our lives will come to turn. Agents transform themselves in the process of, for example, becoming parents, embarking on careers, or acquiring a passion for music or politics. How can such activity be rational, if the reason for engaging in the relevant pursuit is only available to the person one will become? How is it psychologically possible to feel the attraction of a form of concern that is not yet one's own? How can the work done to arrive at the finish line be ascribed to one who doesn't (really) know what one is doing, or why one is doing it? In Aspiration, Agnes Callard asserts that these questions belong to the theory of aspiration. Aspirants are motivated by proleptic reasons, acknowledged defective versions of the reasons they expect to eventually grasp. The psychology of such a transformation is marked by intrinsic conflict between their old point of view on value and the one they are trying to acquire. They cannot adjudicate this conflict by deliberating or choosing or deciding-rather, they resolve it by working to see the world in a new way. This work has a teleological structure: by modeling oneself on the person he or she is trying to be, the aspirant brings that person into being. Because it is open to us to engage in an activity of self-creation, we are responsible for having become the kinds of people we are.},
	publisher = {Oxford University Press},
	author = {Callard, Agnes},
	month = apr,
	year = {2018},
	keywords = {Read},
}

@book{paul_transformative_2014,
	address = {Oxford},
	edition = {1st ed},
	title = {Transformative experience},
	isbn = {978-0-19-871795-9},
	abstract = {How should we make choices when we know so little about our futures? L.A. Paul argues that we must view life decisions as choices to make discoveries about the nature of experience. Her account of transformative experience holds that part of the value of living authentically is to experience our lives and preferences in whatever ways they evolve},
	publisher = {Oxford University Press},
	author = {Paul, L. A.},
	year = {2014},
	note = {OCLC: ocn872342141},
	keywords = {Decision making, Entscheidung, Erfahrung, Experience, Lebensführung, Life change events, Read},
}

@article{paul_choosing_2022,
	title = {Choosing for {Changing} {Selves}},
	volume = {131},
	issn = {0031-8108},
	url = {https://doi.org/10.1215/00318108-9554756},
	doi = {10.1215/00318108-9554756},
	abstract = {Choosing brings change. A major life choice, a “big decision,” as Edna Ullmann-Margalit termed it, brings the possibility of changing who you are. Richard Pettigrew’s important new book, Choosing for Changing Selves, explores theories of decision-making for choices that change us, with a particular focus on life-changing decisions.Life-changing decisions are decision cases where a persisting agent, through their choice, creates a new self, through replacing their values—that is, replacing that self’s utility function. (Take the persisting agent to be constituted by a series of appropriately related selves, and selves to be defined by their values, and by extension by their utility functions.) Such cases are of deep philosophical and practical interest, involving questions about knowledge, evidence, and experiential value, and concerning real world choices such as choosing to have a child, determining one’s future medical care, or getting a divorce (Bykvist 2006; Ullmann-Margalit 2006; Paul 2014).},
	number = {2},
	urldate = {2022-08-02},
	journal = {The Philosophical Review},
	author = {Paul, L. A.},
	month = apr,
	year = {2022},
	keywords = {Read},
	pages = {230--235},
}

@book{pettigrew_choosing_2019,
	edition = {1},
	title = {Choosing for {Changing} {Selves}},
	isbn = {978-0-19-881496-2 978-0-19-185280-0},
	url = {https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198814962.001.0001/oso-9780198814962},
	abstract = {What we value, like, endorse, want, and prefer changes over the course of our lives. Sometimes this is a result of decisions we make—such as when we choose to become a parent or move to a new country—and sometimes it is caused by forces beyond our control—such as when our political views change as we grow older. This poses a problem for any theory of how we ought to make decisions. Which values and preferences should we appeal to when we are making our decisions? Our current values? Our past ones? Our future ones? Or some amalgamation of all of them? But if that, which amalgamation? This book presents a theory of rational decision-making for people whose values have changed in the past and might change again in the future. It begins with expected utility theory, the orthodox theory of rational choice, and raises the problem of choosing for changing selves in that context. It then offers a new decision theory that avoids the problem. In the process, the book considers a host of related problems: Is it rational to give less weight to your far future preferences than to those in your near future? Can we have moral obligations to pursue the goals of our past selves? Do we know enough about our future preferences to make rational decisions that are sufficiently sensitive to them? How should we combine competing sets of values into a single set?},
	language = {en},
	urldate = {2022-07-30},
	publisher = {Oxford University Press},
	author = {Pettigrew, Richard},
	month = dec,
	year = {2019},
	doi = {10.1093/oso/9780198814962.001.0001},
	keywords = {Read},
}

@article{pettigrew_nudging_2022,
	title = {Nudging for {Changing} {Selves}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4025214},
	doi = {10.2139/ssrn.4025214},
	language = {en},
	urldate = {2022-08-04},
	journal = {SSRN Electronic Journal},
	author = {Pettigrew, Richard},
	year = {2022},
	keywords = {Read},
}

@article{sandvig_auditing_2014,
	title = {Auditing {Algorithms}: {Research} {Methods} for {Detecting} {Discrimination} on {Internet} {Platforms}},
	language = {en},
	author = {Sandvig, Christian and Hamilton, Kevin and Karahalios, Karrie and Langbort, Cedric},
	year = {2014},
	keywords = {Read},
	pages = {23},
}

@article{brown_algorithm_2021,
	title = {The algorithm audit: {Scoring} the algorithms that score us},
	volume = {8},
	issn = {2053-9517},
	shorttitle = {The algorithm audit},
	url = {https://doi.org/10.1177/2053951720983865},
	doi = {10.1177/2053951720983865},
	abstract = {In recent years, the ethical impact of AI has been increasingly scrutinized, with public scandals emerging over biased outcomes, lack of transparency, and the misuse of data. This has led to a growing mistrust of AI and increased calls for mandated ethical audits of algorithms. Current proposals for ethical assessment of algorithms are either too high level to be put into practice without further guidance, or they focus on very specific and technical notions of fairness or transparency that do not consider multiple stakeholders or the broader social context. In this article, we present an auditing framework to guide the ethical assessment of an algorithm. The audit instrument itself is comprised of three elements: a list of possible interests of stakeholders affected by the algorithm, an assessment of metrics that describe key ethically salient features of the algorithm, and a relevancy matrix that connects the assessed metrics to stakeholder interests. The proposed audit instrument yields an ethical evaluation of an algorithm that could be used by regulators and others interested in doing due diligence, while paying careful attention to the complex societal context within which the algorithm is deployed.},
	language = {en},
	number = {1},
	urldate = {2022-03-21},
	journal = {Big Data \& Society},
	author = {Brown, Shea and Davidovic, Jovana and Hasan, Ali},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Algorithm audits, To Read (5), algorithm ethics, ethics, ethics of AI, machine learning, machine learning and ethics},
	pages = {2053951720983865},
}

@inproceedings{hansen_shifting_2021,
	address = {Virtual Event Israel},
	title = {Shifting {Consumption} towards {Diverse} {Content} on {Music} {Streaming} {Platforms}},
	isbn = {978-1-4503-8297-7},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441775},
	doi = {10.1145/3437963.3441775},
	abstract = {Algorithmic recommendations shape music consumption at scale, and understanding the impact of various algorithmic models on how content is consumed is a central question for music streaming platforms. The ability to shift consumption towards less popular content and towards content different from user’s typical historic tastes not only affords the platform ways of handling issues such as filter bubbles and popularity bias, but also contributes to maintaining a healthy and sustainable consumption patterns necessary for overall platform success. In this work, we view diversity as an enabler for shifting consumption and consider two notions of music diversity, based on taste similarity and popularity, and investigate how four different recommendation approaches optimized for user satisfaction, fare on diversity metrics. To investigate how the ranker complexity influences diversity, we use two well-known rankers and propose two new models of increased complexity: a feedback aware neural ranker and a reinforcement learning (RL) based ranker. We demonstrate that our models lead to gains in satisfaction, but at the cost of diversity. Such trade-off between model complexity and diversity necessitates the need for explicitly encoding diversity in the modeling process, for which we consider four types of approaches: interleaving based, submodularity based, interpolation, and RL reward modeling based. We find that our reward modeling based RL approach achieves the best trade-off between optimizing the satisfaction metric and surfacing diverse content, thereby enabling consumption shifting at scale. Our findings have implications for the design and deployment of practical approaches for music diversification, which we discuss at length.},
	language = {en},
	urldate = {2023-02-17},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Hansen, Christian and Mehrotra, Rishabh and Hansen, Casper and Brost, Brian and Maystre, Lucas and Lalmas, Mounia},
	month = mar,
	year = {2021},
	keywords = {Read},
	pages = {238--246},
}

@article{gauci_horizon_2019,
	title = {Horizon: {Facebook}'s {Open} {Source} {Applied} {Reinforcement} {Learning} {Platform}},
	shorttitle = {Horizon},
	url = {http://arxiv.org/abs/1811.00260},
	abstract = {In this paper we present Horizon, Facebook’s open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don’t run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workﬂows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon signiﬁcantly outperformed and replaced supervised learning systems at Facebook.},
	language = {en},
	urldate = {2020-04-03},
	journal = {arXiv:1811.00260 [cs, stat]},
	author = {Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui and Chen, Zhengxing and Fujimoto, Scott},
	month = sep,
	year = {2019},
	note = {arXiv: 1811.00260},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_top-k_2020,
	title = {Top-{K} {Off}-{Policy} {Correction} for a {REINFORCE} {Recommender} {System}},
	url = {http://arxiv.org/abs/1812.02353},
	abstract = {Industrial recommender systems deal with extremely large action spaces – many millions of items to recommend. Moreover, they need to serve billions of users, who are unique at any point in time, making a complex user state space. Luckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell time) are available for learning. Learning from the logged feedback is however subject to biases caused by only observing feedback on recommendations selected by the previous versions of the recommender. In this work, we present a general recipe of addressing such biases in a production top-������ recommender system at YouTube, built with a policy-gradient-based algorithm, i.e. REINFORCE [48]. The contributions of the paper are: (1) scaling REINFORCE to a production recommender system with an action space on the orders of millions; (2) applying off-policy correction to address data biases in learning from logged feedback collected from multiple behavior policies; (3) proposing a novel top-������ off-policy correction to account for our policy recommending multiple items at a time; (4) showcasing the value of exploration. We demonstrate the efficacy of our approaches through a series of simulations and multiple live experiments on YouTube.},
	language = {en},
	urldate = {2021-02-18},
	journal = {arXiv:1812.02353 [cs, stat]},
	author = {Chen, Minmin and Beutel, Alex and Covington, Paul and Jain, Sagar and Belletti, Francois and Chi, Ed},
	month = nov,
	year = {2020},
	note = {arXiv: 1812.02353},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Read, Statistics - Machine Learning},
}

@misc{association_for_computing_machinery_acm_reinforcement_2019,
	title = {"{Reinforcement} {Learning} for {Recommender} {Systems}: {A} {Case} {Study} on {Youtube}," by {Minmin} {Chen}},
	shorttitle = {"{Reinforcement} {Learning} for {Recommender} {Systems}},
	url = {https://www.youtube.com/watch?v=HEqQ2_1XRTs},
	abstract = {While reinforcement learning (RL) has achieved impressive advances in games and robotics, it has not been widely adopted in recommender systems. Framing recommendation as an RL problem offers new perspectives, but also faces significant challenges in practice. Industrial recommender systems deal with extremely large action spaces – many millions of items to recommend and complex user state spaces -- billions of users, who are unique at any point in time. In this talk, I will discuss our work on scaling up a policy-gradient-based algorithm, i.e. REINFORCE to a production recommender system at Youtube. We proposed algorithms to address data biases when deriving policy updates from logged implicit feedback. I will also discuss some follow up work and outstanding research questions in applying RL, in particular off-policy optimization in recommender systems.},
	urldate = {2023-03-15},
	author = {{Association for Computing Machinery (ACM)}},
	month = mar,
	year = {2019},
}

@misc{cai_reinforcing_2023,
	title = {Reinforcing {User} {Retention} in a {Billion} {Scale} {Short} {Video} {Recommender} {System}},
	url = {http://arxiv.org/abs/2302.01724},
	abstract = {Recently, short video platforms have achieved rapid user growth by recommending interesting content to users. The objective of the recommendation is to optimize user retention, thereby driving the growth of DAU (Daily Active Users). Retention is a long-term feedback after multiple interactions of users and the system, and it is hard to decompose retention reward to each item or a list of items. Thus traditional point-wise and list-wise models are not able to optimize retention. In this paper, we choose reinforcement learning methods to optimize the retention as they are designed to maximize the long-term performance. We formulate the problem as an infinite-horizon request-based Markov Decision Process, and our objective is to minimize the accumulated time interval of multiple sessions, which is equal to improving the app open frequency and user retention. However, current reinforcement learning algorithms can not be directly applied in this setting due to uncertainty, bias, and long delay time incurred by the properties of user retention. We propose a novel method, dubbed RLUR, to address the aforementioned challenges. Both offline and live experiments show that RLUR can significantly improve user retention. RLUR has been fully launched in Kuaishou app for a long time, and achieves consistent performance improvement on user retention and DAU.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Cai, Qingpeng and Liu, Shuchang and Wang, Xueliang and Zuo, Tianyou and Xie, Wentao and Yang, Bin and Zheng, Dong and Jiang, Peng and Gai, Kun},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01724 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, To Read (5)},
}

@article{kramer_experimental_2014,
	title = {Experimental evidence of massive-scale emotional contagion through social networks},
	volume = {111},
	copyright = {©  . Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/111/24/8788},
	doi = {10.1073/pnas.1320040111},
	abstract = {Emotional states can be transferred to others via emotional contagion, leading people to experience the same emotions without their awareness. Emotional contagion is well established in laboratory experiments, with people transferring positive and negative emotions to others. Data from a large real-world social network, collected over a 20-y period suggests that longer-lasting moods (e.g., depression, happiness) can be transferred through networks [Fowler JH, Christakis NA (2008) BMJ 337:a2338], although the results are controversial. In an experiment with people who use Facebook, we test whether emotional contagion occurs outside of in-person interaction between individuals by reducing the amount of emotional content in the News Feed. When positive expressions were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred. These results indicate that emotions expressed by others on Facebook influence our own emotions, constituting experimental evidence for massive-scale contagion via social networks. This work also suggests that, in contrast to prevailing assumptions, in-person interaction and nonverbal cues are not strictly necessary for emotional contagion, and that the observation of others’ positive experiences constitutes a positive experience for people.},
	language = {en},
	number = {24},
	urldate = {2021-05-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kramer, Adam D. I. and Guillory, Jamie E. and Hancock, Jeffrey T.},
	month = jun,
	year = {2014},
	pmid = {24889601},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	keywords = {Read, big data, computer-mediated communication, social media},
	pages = {8788--8790},
}

@article{afsar_reinforcement_2021,
	title = {Reinforcement learning based recommender systems: {A} survey},
	shorttitle = {Reinforcement learning based recommender systems},
	url = {http://arxiv.org/abs/2101.06286},
	abstract = {Recommender systems (RSs) are becoming an inseparable part of our everyday lives. They help us find our favorite items to purchase, our friends on social networks, and our favorite movies to watch. Traditionally, the recommendation problem was considered as a simple classification or prediction problem; however, the sequential nature of the recommendation problem has been shown. Accordingly, it can be formulated as a Markov decision process (MDP) and reinforcement learning (RL) methods can be employed to solve it. In fact, recent advances in combining deep learning with traditional RL methods, i.e. deep reinforcement learning (DRL), has made it possible to apply RL to the recommendation problem with massive state and action spaces. In this paper, a survey on reinforcement learning based recommender systems (RLRSs) is presented. We first recognize the fact that algorithms developed for RLRSs can be generally classified into RL- and DRL-based methods. Then, we present these RL- and DRL-based methods in a classified manner based on the specific RL algorithm, e.g., Q-learning, SARSA, and REINFORCE, that is used to optimize the recommendation policy. Furthermore, some tables are presented that contain detailed information about the MDP formulation of these methods, as well as about their evaluation schemes. Finally, we discuss important aspects and challenges that can be addressed in the future.},
	urldate = {2021-02-18},
	journal = {arXiv:2101.06286 [cs]},
	author = {Afsar, M. Mehdi and Crump, Trafford and Far, Behrouz},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.06286},
	keywords = {Computer Science - Information Retrieval, Read},
}

@article{apuke_user_2020,
	title = {User {Motivation} in {Fake} {News} {Sharing} during the {COVID}-19 {Pandemic}: an {Application} of the {Uses} and {Gratification} {Theory}},
	volume = {45},
	issn = {1468-4527},
	shorttitle = {User motivation in fake news sharing during the {COVID}-19 pandemic},
	url = {https://doi.org/10.1108/OIR-03-2020-0116},
	doi = {10.1108/OIR-03-2020-0116},
	abstract = {Purpose This study developed a predictive model that established the user motivational factors that predict COVID-19 fake news sharing on social media. Design/methodology/approach The partial least squares structural equation modelling (PLS-SEM) was used for the analysis. Data were drawn from 152 Facebook and WhatsApp users in Nigeria to examine the research model formulated using the uses and gratification theory (UGT). Findings We found that altruism, instant news sharing, socialisation and self-promotion predicted fake news sharing related to COVID-19 pandemic among social media users in Nigeria. Specifically, altruism was the strongest predictor to fake news sharing behaviour related to COVID-19, followed by instant news sharing and socialisation. On the contrary, entertainment had no association with fake news sharing on COVID-19. Practical implications We suggest intervention strategies which nudge people to be sceptical of the information they come across on social media. We also recommend healthcare providers and the Nigerian government to provide relevant information on this current pandemic. That is, correct information should be shared widely to the public domain through various conventional and online media. This will lessen the spread of fake news on the concocted cure and prevention tips found online. Originality/value The salient contributions of this study are as follows: First, it brings to the fore that the desire for self-promotion is associated with fake news sharing on social media; second, it shifts the focus of studies on fake news from detection methods to sharing behaviour, which fuels the uncontrollable spread of falsehood; third, it expands the existing literature on misinformation sharing by demonstrating the user motivation that leads to fake news sharing using the UGT.},
	number = {1},
	urldate = {2021-06-04},
	journal = {Online Information Review},
	author = {Apuke, Oberiri Destiny and Omar, Bahiyah},
	month = jan,
	year = {2020},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {COVID-19, Fake news, Fake news sharing, Nigeria, Social media users},
	pages = {220--239},
}

@misc{armstrong_good_2018,
	title = {Good and {Safe} {Uses} of {AI} {Oracles}},
	url = {http://arxiv.org/abs/1711.05541},
	doi = {10.48550/arXiv.1711.05541},
	abstract = {It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this {\textbackslash}citep\{DBLP:journals/corr/AlfonsecaCACAR16\}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Armstrong, Stuart and O'Rorke, Xavier},
	month = jun,
	year = {2018},
	note = {arXiv:1711.05541 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{ashton_definitions_2022,
	title = {Definitions of {Intent} {Suitable} for {Algorithms}},
	issn = {1572-8382},
	url = {https://doi.org/10.1007/s10506-022-09322-x},
	doi = {10.1007/s10506-022-09322-x},
	abstract = {This article introduces definitions for direct, means-end, oblique (or indirect) and ulterior intent which can be used to test for intent in an algorithmic actor. These definitions of intent are informed by legal theory from common law jurisdictions. Certain crimes exist where the harm caused is dependent on the reason it was done so. Here the actus reus or performative element of the crime is dependent on the mental state or mens rea of the actor. The ability to prosecute these crimes is dependent on the ability to identify and diagnose intentional states in the accused. A certain class of auto didactic algorithmic actor can be given broad objectives without being told how to meet them. Without a definition of intent, they cannot be told not to engage in certain law breaking behaviour nor can they ever be identified as having done it. This ambiguity is neither positive for the owner of the algorithm or for society. The problem exists over and above more familiar debates concerning the eligibility of algorithms for culpability judgements that mens rea is usually associated with. Aside from inchoate offences, many economic crimes with elements of fraud or deceit fall into this category of crime. Algorithms operate in areas where these crimes could be plausibly undertaken depending on whether the intent existed in the algorithm or not.},
	language = {en},
	urldate = {2023-03-02},
	journal = {Artificial Intelligence and Law},
	author = {Ashton, Hal},
	month = jul,
	year = {2022},
}

@article{azzutti_ai-driven_2022,
	title = {{AI}-driven {Market} {Manipulation} and {Limits} of the {EU} {Law} {Enforcement} {Regime} to {Credible} {Deterrence}},
	volume = {45},
	url = {https://papers.ssrn.com/abstract=4026468},
	doi = {10.2139/ssrn.4026468},
	abstract = {As in many other sectors of EU economies, ‘Artificial Intelligence’ (AI) has entered the scene of the financial services industry as a game-changer. A growing number of investment firms have been adopting AI, and particularly ‘Machine Learning’ (ML) methods, within the ramification of algorithmic trading. While AI/ML trading is expected to deliver several efficiency gains for capital markets, it also brings unprecedented risks for their safety and integrity due to some of its technical specificities and related additional uncertainties. With a focus on new and emerging risks of AI-driven market manipulation, this study critically assesses the ability of the EU anti-manipulation law and enforcement regime to achieve credible deterrence. It argues that AI trading is currently left operating within a (quasi-)lawless market environment with the ultimate risk of jeopardising EU capital markets’ integrity and stability. It shows how ‘deterrence theory’, as a normative framework, can allow us to think of innovative solutions to fix the many shortcomings of the EU legal framework in the fight against AI-driven market manipulation. In concluding, this study suggests improving the existing EU anti-manipulation law and enforcement regime with a number of policy proposals. Namely, (i) an improved, ‘harm-centric’ definition of manipulation; (ii) an improved, ‘multi-layered’ liability regime for AI-driven manipulation; and (iii) a novel, ‘hybrid’ public-private enforcement institutional architecture through the introduction of market manipulation ‘bounty-hunters’.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Computer Law \& Security review},
	author = {Azzutti, Alessio},
	month = jan,
	year = {2022},
	keywords = {algorithmic trading, artificial intelligence, credible deterrence, effective enforcement, market integrity, market manipulation},
}

@inproceedings{bahar_fiduciary_2020,
	title = {Fiduciary {Bandits}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bahar, Gal and Ben-Porat, Omer and Leyton-Brown, Kevin and Tennenholtz, Moshe},
	year = {2020},
	pages = {518--527},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2022-02-11},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@article{bernsen_cooperativity_1996,
	title = {Cooperativity in {Human}‐{Machine} and {Human}‐{Human} {Spoken} {Dialogue}},
	volume = {21},
	issn = {0163-853X},
	url = {https://doi.org/10.1080/01638539609544956},
	doi = {10.1080/01638539609544956},
	abstract = {The article presents principles of dialogue cooperativity derived from a corpus of task‐oriented spoken human‐machine dialogue. The corpus was recorded during the design of a dialogue model for a spoken language dialogue system. Analysis of the corpus produced a set of dialogue design principles intended to prevent users from having to initiate clarification and repair metacommunication that the system would not understand. Developed independently of Grice's work on cooperation in spoken dialogue, these principles provided an empirical test of the correctness and completeness of Grice's maxims of cooperativity in the case of human‐machine dialogue. Whereas the maxims pass the test of correctness, they fail to provide a complete account of principles of cooperative human‐machine dialogue. A more complete set of aspects of cooperative task‐oriented dialogue is proposed together with the principles expressing those aspects. Transferability of results to cooperative spoken human‐human dialogue is discussed.},
	number = {2},
	urldate = {2021-12-07},
	journal = {Discourse Processes},
	author = {Bernsen, Niels   Ole and Dybkjær, Hans and Dybkjær, Laila},
	month = mar,
	year = {1996},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01638539609544956},
	pages = {213--236},
}

@article{blumenthal-barby_seeking_2012,
	title = {Seeking {Better} {Health} {Care} {Outcomes}: the {Ethics} of {Using} the "{Nudge}"},
	volume = {12},
	issn = {1536-0075},
	shorttitle = {Seeking better health care outcomes},
	doi = {10.1080/15265161.2011.634481},
	abstract = {Policymakers, employers, insurance companies, researchers, and health care providers have developed an increasing interest in using principles from behavioral economics and psychology to persuade people to change their health-related behaviors, lifestyles, and habits. In this article, we examine how principles from behavioral economics and psychology are being used to nudge people (the public, patients, or health care providers) toward particular decisions or behaviors related to health or health care, and we identify the ethically relevant dimensions that should be considered for the utilization of each principle.},
	language = {eng},
	number = {2},
	journal = {The American journal of bioethics: AJOB},
	author = {Blumenthal-Barby, J. S. and Burroughs, Hadley},
	year = {2012},
	pmid = {22304506},
	keywords = {Affect, Choice Behavior, Cues, Decision Making, Delivery of Health Care, Economics, Behavioral, Energy Intake, Exercise, Feeding Behavior, Health Behavior, Health Policy, Humans, Motivation, Obesity, Personal Autonomy, Persuasive Communication, Physician-Patient Relations, Policy Making, Risk Reduction Behavior, Seat Belts, Smoking Prevention, United States, Vaccination, Vulnerable Populations},
	pages = {1--10},
}

@book{carissimo_manipulative_2023,
	title = {Manipulative {Potentials} of an {Omniscient} {Recommender} {System}},
	abstract = {We explore the interaction of an omniscient recommender system and its users in a multiplayer routing game. Machine learning is used to model the learning dynamics of users. Recommendations provide coordination information without forcing agents to follow, but an omniscient recommender is capable of manipulating the learning dynamics. This gives rise to an interplay between optimizing a social welfare function and maintaining the trust of the users. This paper contributes an exploratory formalism to understand the effects of recommender systems on users.},
	author = {Carissimo, Cesare and Korecki, Marcin and Dailisan, Damian},
	month = jan,
	year = {2023},
	keywords = {Micah},
}

@article{castelfranchi_artificial_nodate,
	title = {Artificial {Liars}: {Why} {Computers} will ({Necessarily}) {Deceive} us and {Each} {Other}},
	language = {en},
	author = {Castelfranchi, Cristiano},
	keywords = {Read},
}

@inproceedings{damour_fairness_2020,
	title = {Fairness is not {Static}},
	url = {https://doi.org/10.1145%2F3351095.3372878},
	doi = {10.1145/3351095.3372878},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
	month = jan,
	year = {2020},
}

@inproceedings{dalvi_adversarial_2004,
	address = {New York, NY, USA},
	series = {{KDD} '04},
	title = {Adversarial {Classification}},
	isbn = {978-1-58113-888-7},
	url = {https://doi.org/10.1145/1014052.1014066},
	doi = {10.1145/1014052.1014066},
	abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
	urldate = {2021-07-19},
	booktitle = {Proceedings of the tenth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Dalvi, Nilesh and Domingos, Pedro and Mausam and Sanghai, Sumit and Verma, Deepak},
	month = aug,
	year = {2004},
	keywords = {cost-sensitive learning, game theory, integer linear programming, naive Bayes, spam detection},
	pages = {99--108},
}

@article{dean_recommendations_2020,
	title = {Recommendations and {User} {Agency}: the {Reachability} of {Collaboratively}-{Filtered} {Information}},
	journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Dean, Sarah and Rich, Sarah and Recht, B.},
	year = {2020},
}

@article{druckman_preference_2000,
	title = {Preference {Formation}},
	volume = {3},
	number = {1},
	journal = {Annual Review of Political Science},
	author = {Druckman, James N and Lupia, Arthur},
	year = {2000},
	note = {Publisher: Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA},
	pages = {1--24},
}

@article{epstein_search_2015,
	title = {The {Search} {Engine} {Manipulation} {Effect} ({SEME}) and its {Possible} {Impact} on the {Outcomes} of {Elections}},
	volume = {112},
	url = {https://www.pnas.org/doi/10.1073/pnas.1419828112},
	doi = {10.1073/pnas.1419828112},
	number = {33},
	urldate = {2022-03-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Epstein, Robert and Robertson, Ronald E.},
	month = aug,
	year = {2015},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {Read},
	pages = {E4512--E4521},
}

@article{evans_truthful_2021,
	title = {Truthful {AI}: {Developing} and {Governing} {AI} that does not {Lie}},
	shorttitle = {Truthful {AI}},
	url = {http://arxiv.org/abs/2110.06674},
	abstract = {In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI "lies" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.},
	urldate = {2021-12-07},
	journal = {arXiv:2110.06674 [cs]},
	author = {Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.06674},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, I.2.0},
}

@article{evtimov_is_2019,
	title = {Is {Tricking} a {Robot} {Hacking}?},
	volume = {34},
	number = {3},
	journal = {Berkeley Technology Law Journal},
	author = {Evtimov, Ivan and O’Hair, David and Fernandes, Earlence and Calo, Ryan and Kohno, Tadayoshi},
	year = {2019},
	note = {Publisher: JSTOR},
	pages = {891--918},
}

@inproceedings{fogg_persuasive_1998,
	title = {Persuasive {Computers}: {Perspectives} and {Research} {Directions}},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems},
	author = {Fogg, Brian J},
	year = {1998},
	pages = {225--232},
}

@article{galli_ai_2020,
	title = {{AI} and {Consumers} {Manipulation}: {What} is the {Role} of {EU} {Fair} {Marketing} law?},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{AI} and consumers manipulation},
	url = {https://revistas.ucp.pt/index.php/catolicalawreview/article/view/9320},
	doi = {10.34632/CATOLICALAWREVIEW.2020.9320},
	abstract = {Today’s online businesses increasingly employ different AI tools to perform marketing and customer management tasks (e.g. target ad-vertising, product recommendations, personalised pricing, etc.). This de-velopment poses a threat to consumers autonomy as it increases the like-lihood of manipulative results in purchase decisions. This paper explains two ways in which the employment of AI can lead to distortive results in consumers’ decision-making. In addition, some reflections are proposed over the role of fair marketing law in protecting consumers, notably on how EU fair marketing law should change in response to the spread of AI-mediated commercial practices. In particular, the paper focuses on analysing the Unfair Commercial Practice Directive and the recent US leg-islative proposal for the Detour Act.},
	language = {en},
	urldate = {2023-02-28},
	journal = {Católica Law Review},
	author = {Galli, Federico},
	month = may,
	year = {2020},
	note = {Artwork Size: 35-64 Páginas
Publisher: Católica Law Review},
	pages = {35--64 Páginas},
}

@article{hallinan_unexpected_2020,
	title = {Unexpected {Expectations}: {Public} {Reaction} to the {Facebook} {Emotional} {Contagion} {Study}},
	volume = {22},
	issn = {1461-4448, 1461-7315},
	shorttitle = {Unexpected expectations},
	url = {http://journals.sagepub.com/doi/10.1177/1461444819876944},
	doi = {10.1177/1461444819876944},
	abstract = {How to ethically conduct online platform-based research remains an unsettled issue and the source of continued controversy. The Facebook emotional contagion study, in which researchers altered Facebook News Feeds to determine whether exposure to emotional content influences a user’s mood, has been one focal point of these discussions. The intense negative reaction by the media and public came as a surprise to those involved—but what prompted this reaction? We approach the Facebook study as a mediated controversy that reveals disconnects between how scholars, technologists, and the public understand platform-based research. We examine the controversy from the bottom up, analyzing public reactions expressed in comments on news articles. Our analysis reveals fundamental disagreements about what Facebook is and what a user’s relationship to it should be. We argue that these divergent responses emphasize the contextual nature of technology and research ethics, and conclude with a relational and contextual approach to ethical decision-making.},
	language = {en},
	number = {6},
	urldate = {2023-03-15},
	journal = {New Media \& Society},
	author = {Hallinan, Blake and Brubaker, Jed R and Fiesler, Casey},
	month = jun,
	year = {2020},
	pages = {1076--1094},
}

@article{hartley_fighting_2020,
	title = {Fighting {Fake} {News} in the {COVID}-19 {Era}: {Policy} {Insights} from an {Equilibrium} {Model}},
	journal = {Policy Sciences},
	author = {Hartley, K. and Vu, M.},
	year = {2020},
	pages = {1 -- 24},
}

@article{izuma_neural_2010,
	title = {Neural {Correlates} of {Cognitive} {Dissonance} and {Choice}-{Induced} {Preference} {Change}},
	volume = {107},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/107/51/22014},
	doi = {10.1073/pnas.1011879108},
	abstract = {According to many modern economic theories, actions simply reflect an individual's preferences, whereas a psychological phenomenon called “cognitive dissonance” claims that actions can also create preference. Cognitive dissonance theory states that after making a difficult choice between two equally preferred items, the act of rejecting a favorite item induces an uncomfortable feeling (cognitive dissonance), which in turn motivates individuals to change their preferences to match their prior decision (i.e., reducing preference for rejected items). Recently, however, Chen and Risen [Chen K, Risen J (2010) J Pers Soc Psychol 99:573–594] pointed out a serious methodological problem, which casts a doubt on the very existence of this choice-induced preference change as studied over the past 50 y. Here, using a proper control condition and two measures of preferences (self-report and brain activity), we found that the mere act of making a choice can change self-report preference as well as its neural representation (i.e., striatum activity), thus providing strong evidence for choice-induced preference change. Furthermore, our data indicate that the anterior cingulate cortex and dorsolateral prefrontal cortex tracked the degree of cognitive dissonance on a trial-by-trial basis. Our findings provide important insights into the neural basis of how actions can alter an individual's preferences.},
	language = {en},
	number = {51},
	urldate = {2021-05-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Izuma, Keise and Matsumoto, Madoka and Murayama, Kou and Samejima, Kazuyuki and Sadato, Norihiro and Matsumoto, Kenji},
	month = dec,
	year = {2010},
	pmid = {21135218},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	keywords = {attitude, cognitive control, conflict, neuroeconomics, value},
	pages = {22014--22019},
}

@article{jaderberg_human-level_2019,
	title = {Human-level {Performance} in {3D} {Multiplayer} {Games} with {Population}-{Based} {Reinforcement} {Learning}},
	volume = {364},
	url = {https://www.science.org/doi/10.1126/science.aau6249},
	doi = {10.1126/science.aau6249},
	abstract = {Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.},
	number = {6443},
	urldate = {2023-02-25},
	journal = {Science},
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castañeda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	month = may,
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {859--865},
}

@article{klass_law_2018,
	title = {The {Law} of {Deception}: {A} {Research} {Agenda}},
	volume = {89},
	number = {2},
	journal = {University of Colorado Law Review},
	author = {Klass, Gregory},
	year = {2018},
	pages = {707--740},
}

@article{jesse_digital_2021,
	title = {Digital {Nudging} with {Recommender} {Systems}: {Survey} and {Future} {Directions}},
	volume = {3},
	issn = {24519588},
	shorttitle = {Digital nudging with recommender systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S245195882030052X},
	doi = {10.1016/j.chbr.2020.100052},
	language = {en},
	urldate = {2023-03-14},
	journal = {Computers in Human Behavior Reports},
	author = {Jesse, Mathias and Jannach, Dietmar},
	month = jan,
	year = {2021},
	pages = {100052},
}

@article{johnson_ai_2019,
	title = {{AI}, {Agency} and {Responsibility}: the {VW} {Fraud} {Case} and {Beyond}},
	volume = {34},
	issn = {1435-5655},
	shorttitle = {{AI}, agency and responsibility},
	url = {https://doi.org/10.1007/s00146-017-0781-9},
	doi = {10.1007/s00146-017-0781-9},
	abstract = {The concept of agency as applied to technological artifacts has become an object of heated debate in the context of AI research because some AI researchers ascribe to programs the type of agency traditionally associated with humans. Confusion about agency is at the root of misconceptions about the possibilities for future AI. We introduce the concept of a triadic agency that includes the causal agency of artifacts and the intentional agency of humans to better describe what happens in AI as it functions in real-world contexts. We use the VW emission fraud case to explain triadic agency since in this case a technological artifact, namely software, was an essential part of the wrongdoing and the software might be said to have agency in the wrongdoing. We then extend the case to include futuristic AI, imagining AI that becomes more and more autonomous.},
	language = {en},
	number = {3},
	urldate = {2023-02-25},
	journal = {AI \& SOCIETY},
	author = {Johnson, Deborah G. and Verdicchio, Mario},
	month = sep,
	year = {2019},
	pages = {639--647},
}

@article{jumper_highly_2021,
	title = {Highly {Accurate} {Protein} {Structure} {Prediction} with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-03-09},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@inproceedings{kampik_coercion_2018,
	title = {Coercion and {Deception} in {Persuasive} {Technologies}},
	booktitle = {20th {International} {Trust} {Workshop} (co-located with {AAMAS}/{IJCAI}/{ECAI}/{ICML} 2018), {Stockholm}, {Sweden}, 14 {July}, 2018},
	publisher = {CEUR-WS},
	author = {Kampik, Timotheus and Nieves, Juan Carlos and Lindgren, Helena},
	year = {2018},
	pages = {38--49},
}

@article{kaptein_combining_2013,
	title = {Combining {Multiple} {Influence} {Strategies} to {Increase} {Consumer} {Compliance}},
	volume = {8},
	issn = {1477-5212, 1741-8100},
	url = {http://www.inderscience.com/link.php?id=56586},
	doi = {10.1504/IJIMA.2013.056586},
	abstract = {In this paper, we investigate the effects and implications of utilising multiple social influence strategies simultaneously to endorse a single product or call to action. In three, studies we show that combinations of social influence strategies do not increase compliance – this is contrary to commonly held beliefs and practice. Studies 1 and 2 show that combining implementations of both the consensus and authority strategies to promote a single behaviour does not lead to an increase in the effectiveness of a persuasive attempt. In Study 3, we test these findings in an online advertising campaign and again show that a single influence strategy is more effective than the combined usage of multiple influence strategies. The paper outlines the importance of appropriately choosing and implementing social influence strategies to prevent unintended interactions between the strategies that lead to a suboptimal performance.},
	language = {en},
	number = {1},
	urldate = {2023-03-02},
	journal = {International Journal of Internet Marketing and Advertising},
	author = {Kaptein, Maurits and Duplinsky, Steven},
	year = {2013},
	pages = {32},
}

@article{krakovna_penalizing_2019,
	title = {Penalizing {Side} {Effects} {Using} {Stepwise} {Relative} {Reachability}},
	url = {http://arxiv.org/abs/1806.01186},
	abstract = {How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.},
	language = {en},
	urldate = {2022-04-24},
	journal = {arXiv:1806.01186 [cs, stat]},
	author = {Krakovna, Victoria and Orseau, Laurent and Kumar, Ramana and Martic, Miljan and Legg, Shane},
	month = mar,
	year = {2019},
	note = {arXiv: 1806.01186},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read, Statistics - Machine Learning},
}

@article{kubin_role_2021,
	title = {The {Role} of ({Social}) {Media} in {Political} {Polarization}: a {Systematic} {Review}},
	volume = {45},
	issn = {2380-8985},
	shorttitle = {The role of (social) media in political polarization},
	url = {https://doi.org/10.1080/23808985.2021.1976070},
	doi = {10.1080/23808985.2021.1976070},
	abstract = {Rising political polarization is, in part, attributed to the fragmentation of news media and the spread of misinformation on social media. Previous reviews have yet to assess the full breadth of research on media and polarization. We systematically examine 94 articles (121 studies) that assess the role of (social) media in shaping political polarization. Using quantitative and qualitative approaches, we find an increase in research over the past 10 years and consistently find that pro-attitudinal media exacerbates polarization. We find a hyperfocus on analyses of Twitter and American samples and a lack of research exploring ways (social) media can depolarize. Additionally, we find ideological and affective polarization are not clearly defined, nor consistently measured. Recommendations for future research are provided.},
	number = {3},
	urldate = {2023-03-01},
	journal = {Annals of the International Communication Association},
	author = {Kubin, Emily and von Sikorski, Christian},
	month = jul,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/23808985.2021.1976070},
	keywords = {(social) media, Affective polarization, depolarization, ideological polarization, political communication},
	pages = {188--206},
}

@inproceedings{liu_delayed_2018,
	title = {Delayed {Impact} of {Fair} {Machine} {Learning}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Lydia T and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
	year = {2018},
	pages = {3150--3158},
}

@inproceedings{liu_modeling_2015,
	title = {Modeling {Users}' {Dynamic} {Preference} for {Personalized} {Recommendation}},
	booktitle = {Twenty-{Fourth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Xin},
	year = {2015},
}

@article{meibauer_lying_2005,
	series = {Focus-on {Issue}: {Discourse} and {Metadiscourse}},
	title = {Lying and {Falsely} {Implicating}},
	volume = {37},
	issn = {0378-2166},
	url = {https://www.sciencedirect.com/science/article/pii/S0378216604002693},
	doi = {10.1016/j.pragma.2004.12.007},
	abstract = {This paper analyses falsely implicating from the point of view of Gricean theory of implicature, focusing on the Story of the Mate and the Captain which is a classical example of lying while saying the truth. It is argued that the case of falsely implicating should be included within a general definition of lying. Whether Particularised Conversational Implicatures (PCI), as in the Story of the Mate and the Captain, and Generalised Conversational Implicatures (GCI) behave differently with regard to falsely implicating is discussed with reference to Levinson's theory of presumptive meaning [Levinson, Stephen C., 2000. Presumptive Meanings. The Theory of Generalised Conversational Implicature. MIT Press, Cambridge, Mass]. It turns out that, in contrast to PCIs, the case where the assertion is false and the implicature is true is not possible with GCIs. In addition, tautology and irony are analysed, and some repercussions on the speech act notion of lying are pointed out.},
	language = {en},
	number = {9},
	urldate = {2021-12-28},
	journal = {Journal of Pragmatics},
	author = {Meibauer, Jörg},
	month = sep,
	year = {2005},
	keywords = {Assertion, Conversational Implicature, Lying},
	pages = {1373--1399},
}

@article{meibauer_lying_2011,
	title = {On {Lying}: {Intentionality}, {Implicature}, and {Imprecision}},
	volume = {8},
	issn = {1613-365X},
	shorttitle = {On lying},
	url = {https://www.degruyter.com/document/doi/10.1515/iprg.2011.013/html},
	doi = {10.1515/iprg.2011.013},
	abstract = {Although lying is a classical topic in philosophy, and certainly is very important in everyday life, there is a lack of genuine linguistic analyses of lying. In this article, lying is viewed as a speech act of insincere assertion. The liar misrepresents truth in order to deceive. While definitions like this are fairly common, consequences for the semantics/pragmatics interface, and, more importantly, for the ongoing debate about minimalism versus contextualism, never have been worked out in detail. This is what the article aims at, concentrating on three relevant issues, namely intentionality, implicature, and imprecision. It takes a moderate contextualist stand by showing that the possibility of lying is built into the language, thus allowing speakers to manipulate the representation of truth according to certain social goals. A case in point would be lying while saying the truth; in this case, the risk of being caught in the act of lying is reduced.},
	language = {en},
	number = {2},
	urldate = {2021-12-10},
	author = {Meibauer, Jörg},
	month = may,
	year = {2011},
	note = {Publisher: De Gruyter Mouton
Section: Intercultural Pragmatics},
	pages = {277--292},
}

@article{meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022,
	title = {Human-{Level} {Play} in the {Game} of {Diplomacy} by {Combining} {Language} {Models} with {Strategic} {Reasoning}},
	volume = {378},
	url = {https://www.science.org/doi/10.1126/science.ade9097},
	doi = {10.1126/science.ade9097},
	abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.},
	number = {6624},
	urldate = {2023-02-27},
	journal = {Science},
	author = {{Meta Fundamental AI Research Diplomacy Team (FAIR)} and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
	month = dec,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1067--1074},
}

@article{mills_finding_2022,
	title = {Finding the ‘{Nudge}’ in {Hypernudge}},
	volume = {71},
	issn = {0160791X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0160791X22002585},
	doi = {10.1016/j.techsoc.2022.102117},
	language = {en},
	urldate = {2023-03-03},
	journal = {Technology in Society},
	author = {Mills, Stuart},
	month = nov,
	year = {2022},
	pages = {102117},
}

@article{musial_can_2022,
	title = {Can {We} {Design} {Artificial} {Persons} without {Being} {Manipulative}?},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-022-01575-z},
	doi = {10.1007/s00146-022-01575-z},
	abstract = {If we could build artificial persons (APs) with a moral status comparable to this of a typical human being, how should we design those APs in the right way? This question has been addressed mainly in terms of designing APs devoted to being servants (AP servants) and debated in reference to their autonomy and the harm they might experience. Recently, it has been argued that even if developing AP servants would neither deprive them of autonomy nor cause any net harm, then developing such entities would still be unethical due to the manipulative attitude of their designers. I make two contributions to this discussion. First, I claim that the argument about manipulative attitude significantly shifts the perspective of the whole discussion on APs and that it refers to a much wider range of types of APs than has been acknowledged. Second, I investigate the possibilities of developing APs without a manipulative attitude. I proceed in the following manner: (1) I examine the argument about manipulativeness; (2) show the important novelty it brings to a discussion about APs; (3) analyze how the argument can be extrapolated to designing other kinds of Aps; and (4) discuss cases in which APs can be designed without manipulativeness.},
	language = {en},
	urldate = {2023-03-13},
	journal = {AI \& SOCIETY},
	author = {Musiał, Maciej},
	month = oct,
	year = {2022},
	keywords = {AI ethics, Artificial persons, Designing, Manipulativeness},
}

@article{nakano_webgpt_2021,
	title = {Webgpt: {Browser}-{Assisted} {Question}-{Answering} with {Human} {Feedback}},
	journal = {arXiv preprint arXiv:2112.09332},
	author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and {others}},
	year = {2021},
}

@article{nelson_small_2018,
	title = {The {Small}, {Disloyal} {Fake} {News} {Audience}: {The} {Role} of {Audience} {Availability} in {Fake} {News} {Consumption}},
	volume = {20},
	issn = {1461-4448},
	shorttitle = {The small, disloyal fake news audience},
	url = {https://doi.org/10.1177/1461444818758715},
	doi = {10.1177/1461444818758715},
	abstract = {In light of the recent US election, many fear that “fake news” has become a force of enormous reach and influence within the news media environment. We draw on well-established theories of audience behavior to argue that the online fake news audience, like most niche content, would be a small subset of the total news audience, especially those with high availability. By examining online visitation data across mobile and desktop platforms in the months leading up to and following the 2016 presidential election, we indeed find the fake news audience comprises a small, disloyal group of heavy Internet users. We also find that social network sites play an outsized role in generating traffic to fake news. With this revised understanding, we revisit the democratic implications of the fake news crisis.},
	language = {en},
	number = {10},
	urldate = {2021-05-12},
	journal = {New Media \& Society},
	author = {Nelson, Jacob L and Taneja, Harsh},
	month = oct,
	year = {2018},
	note = {Publisher: SAGE Publications},
	keywords = {2016 Elections, Audience Availability, Audience Fragmentation, Double Jeopardy, Elections, Fake News, Mobile Internet, News Audience, Social Media},
	pages = {3720--3737},
}

@inproceedings{oulasvirta_when_2009,
	address = {New York, NY, USA},
	series = {{SIGIR} '09},
	title = {When {More} is {Less}: the {Paradox} of {Choice} in {Search} {Engine} {Use}},
	isbn = {978-1-60558-483-6},
	shorttitle = {When more is less},
	url = {https://doi.org/10.1145/1571941.1572030},
	doi = {10.1145/1571941.1572030},
	abstract = {In numerous everyday domains, it has been demonstrated that increasing the number of options beyond a handful can lead to paralysis and poor choice and decrease satisfaction with the choice. Were this so-called paradox of choice to hold in search engine use, it would mean that increasing recall can actually work counter to user satisfaction if it implies choice from a more extensive set of result items. The existence of this effect was demonstrated in an experiment where users (N=24) were shown a search scenario and a query and were required to choose the best result item within 30 seconds. Having to choose from six results yielded both higher subjective satisfaction with the choice and greater confidence in its correctness than when there were 24 items on the results page. We discuss this finding in the wider context of "choice architecture"--that is, how result presentation affects choice and satisfaction.},
	urldate = {2021-05-12},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Oulasvirta, Antti and Hukkinen, Janne P. and Schwartz, Barry},
	month = jul,
	year = {2009},
	keywords = {relevance judgments, satisfaction, search engines, user interfaces},
	pages = {516--523},
}

@article{pereira_analyzing_2018,
	title = {On {Analyzing} {User} {Preference} {Dynamics} with {Temporal} {Social} {Networks}},
	volume = {107},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-018-5740-2},
	doi = {10.1007/s10994-018-5740-2},
	language = {en},
	number = {11},
	urldate = {2023-03-14},
	journal = {Machine Learning},
	author = {Pereira, Fabíola S. F. and Gama, João and de Amo, Sandra and Oliveira, Gina M. B.},
	month = nov,
	year = {2018},
	pages = {1745--1773},
}

@incollection{putnins_overview_2020,
	address = {United States},
	edition = {1st},
	title = {An {Overview} of {Market} {Manipulation}},
	isbn = {978-1-119-42177-1},
	language = {English},
	booktitle = {Corruption and {Fraud} in {Financial} {Markets}},
	publisher = {John Wiley \& Sons Inc.},
	author = {Putniņš, Tālis},
	editor = {Alexander, Carol and Cumming, Douglas},
	month = may,
	year = {2020},
	pages = {13--44},
}

@article{rahwan_machine_2019,
	title = {Machine {Behaviour}},
	volume = {568},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1138-y},
	doi = {10.1038/s41586-019-1138-y},
	language = {en},
	number = {7753},
	urldate = {2022-03-21},
	journal = {Nature},
	author = {Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and Bongard, Josh and Bonnefon, Jean-François and Breazeal, Cynthia and Crandall, Jacob W. and Christakis, Nicholas A. and Couzin, Iain D. and Jackson, Matthew O. and Jennings, Nicholas R. and Kamar, Ece and Kloumann, Isabel M. and Larochelle, Hugo and Lazer, David and McElreath, Richard and Mislove, Alan and Parkes, David C. and Pentland, Alex ‘Sandy’ and Roberts, Margaret E. and Shariff, Azim and Tenenbaum, Joshua B. and Wellman, Michael},
	month = apr,
	year = {2019},
	pages = {477--486},
}

@inproceedings{reid_surrogate_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Surrogate {Regret} {Bounds} for {Proper} {Losses}},
	isbn = {978-1-60558-516-1},
	url = {https://doi.org/10.1145/1553374.1553489},
	doi = {10.1145/1553374.1553489},
	abstract = {We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a simple demonstration of the convexity of composite losses using canonical link functions.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Reid, Mark D. and Williamson, Robert C.},
	month = jun,
	year = {2009},
	pages = {897--904},
}

@inproceedings{reiter_computational_1990,
	title = {The {Computational} {Complexity} of {Avoiding} {Conversational} {Implicatures}},
	booktitle = {28th annual meeting of the association for computational linguistics},
	author = {Reiter, Ehud},
	year = {1990},
	pages = {97--104},
}

@inproceedings{richens_counterfactual_2022,
	title = {Counterfactual {Harm}},
	url = {https://openreview.net/forum?id=zkQho-Jxky9},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Richens, Jonathan and Beard, Rory and Thompson, Daniel H.},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@article{roemer_kantian_2015,
	series = {The {Nordic} {Model}},
	title = {Kantian {Optimization}: {A} {Microfoundation} for {Cooperation}},
	volume = {127},
	issn = {0047-2727},
	shorttitle = {Kantian optimization},
	url = {https://www.sciencedirect.com/science/article/pii/S0047272714000607},
	doi = {10.1016/j.jpubeco.2014.03.011},
	abstract = {Although evidence accrues in biology, anthropology and experimental economics that homo sapiens is a cooperative species, the reigning assumption in economic theory is that individuals optimize in an autarkic manner (as in Nash and Walrasian equilibrium). I here postulate a cooperative kind of optimizing behavior, called Kantian. It is shown that in simple economic models, when there are negative externalities (such as congestion effects from use of a commonly owned resource) or positive externalities (such as a social ethos reflected in individuals' preferences), Kantian equilibria dominate the Nash–Walras equilibria in terms of efficiency. While economists schooled in Nash equilibrium may view the Kantian behavior as utopian, there is some – perhaps much – evidence that it exists. If cultures evolve through group selection, the hypothesis that Kantian behavior is more prevalent than we may think is supported by the efficiency results here demonstrated.},
	language = {en},
	urldate = {2021-06-08},
	journal = {Journal of Public Economics},
	author = {Roemer, John E.},
	month = jul,
	year = {2015},
	keywords = {Cooperation, Implementation, Kantian equilibrium, Other-regarding preferences, Tragedy of the commons},
	pages = {45--57},
}

@article{roughgarden_computing_2010,
	title = {Computing {Equilibria}: a {Computational} {Complexity} {Perspective}},
	volume = {42},
	issn = {1432-0479},
	shorttitle = {Computing equilibria},
	url = {https://doi.org/10.1007/s00199-009-0448-y},
	doi = {10.1007/s00199-009-0448-y},
	abstract = {Computational complexity is the subfield of computer science that rigorously studies the intrinsic difficulty of computational problems. This survey explains how complexity theory defines “hard problems”; applies these concepts to several equilibrium computation problems; and discusses implications for computation, games, and behavior. We assume minimal prior background in computer science.},
	language = {en},
	number = {1},
	urldate = {2021-05-27},
	journal = {Economic Theory},
	author = {Roughgarden, Tim},
	month = jan,
	year = {2010},
	pages = {193--236},
}

@book{russell_human_2019,
	address = {London},
	title = {Human {Compatible}: {Artificial} {Intelligence} and the {Problem} of {Control}},
	isbn = {978-0-241-33520-8},
	shorttitle = {Human compatible},
	abstract = {Humans dream of super-intelligent machines. But what happens if we actually succeed? Creating superior intelligence would be the biggest event in human history. Unfortunately, according to the world's pre-eminent AI expert, it could also be the last. In this groundbreaking book on the biggest question facing humanity, Stuart Russell explains why he has come to consider his own discipline an existential threat to our species, and lays out how we can change course before it's too late. There is no one better placed to assess the promise and perils of the dominant technology of the future than Russell, who has spent decades at the forefront of AI research. Through brilliant analogies and crisp, lucid prose, he explains how AI actually works, how it has an enormous capacity to improve our lives - but why we must ensure that we never lose control of machines more powerful than we are. Here Russell shows how we can avert the worst threats by reshaping the foundations of AI to guarantee that machines pursue our objectives, not theirs. Profound, urgent and visionary, Human Compatible is the one book everyone needs to read to understand a future that is coming sooner than we think},
	language = {eng},
	publisher = {Allen Lane, an imprint of Penguin Books},
	author = {Russell, Stuart J.},
	year = {2019},
	keywords = {Henry},
}

@article{schmidt_ethics_2020,
	title = {The {Ethics} of {Nudging}: {An} {Overview}},
	volume = {15},
	issn = {1747-9991, 1747-9991},
	shorttitle = {The ethics of nudging},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/phc3.12658},
	doi = {10.1111/phc3.12658},
	language = {en},
	number = {4},
	urldate = {2023-03-14},
	journal = {Philosophy Compass},
	author = {Schmidt, Andreas T. and Engelen, Bart},
	month = apr,
	year = {2020},
}

@book{schull_addiction_2012,
	address = {Princeton, NJ},
	title = {Addiction by {Design}: {Machine} {Gambling} in {Las} {Vegas}},
	isbn = {978-0-691-12755-2},
	shorttitle = {Addiction by design},
	language = {en},
	publisher = {Princeton University Press},
	author = {Schüll, Natasha Dow},
	year = {2012},
	keywords = {Casinos, Compulsive gambling, Equipment and supplies, Gambling, Micah, Nevada Las Vegas},
}

@article{scopino_automated_2015,
	title = {Do {Automated} {Trading} {Systems} {Dream} of {Manipulating} the {Price} of {Futures} contracts? {Policing} {Markets} for {Improper} {Trading} {Practices} by {Algorithmic} {Robots}},
	volume = {67},
	language = {en},
	journal = {Florida Law Review},
	author = {Scopino, Gregory},
	year = {2015},
	pages = {221},
}

@article{shoham_if_2007,
	series = {Foundations of {Multi}-{Agent} {Learning}},
	title = {If {Multi}-{Agent} {Learning} is the {Answer}, {What} is the {Question}?},
	volume = {171},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370207000495},
	doi = {10.1016/j.artint.2006.02.006},
	abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.11This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory and Practice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) of research on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time it has gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank them all collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellman provided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views put forward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1.},
	language = {en},
	number = {7},
	urldate = {2021-06-07},
	journal = {Artificial Intelligence},
	author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
	month = may,
	year = {2007},
	pages = {365--377},
}

@article{silver_general_2018,
	title = {A {General} {Reinforcement} {Learning} {Algorithm} that {Masters} {Chess}, {Shogi}, and {Go} through {Self}-{Play}},
	volume = {362},
	number = {6419},
	journal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and {others}},
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1140--1144},
}

@misc{snoswell_galactica_2022,
	title = {The {Galactica} {AI} {Model} was {Trained} on {Scientific} {Knowledge} – but it {Spat} {Out} {Alarmingly} {Plausible} {Nonsense}},
	url = {http://theconversation.com/the-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible-nonsense-195445},
	abstract = {The story of Meta’s latest AI model shows the pitfalls of machine learning – and a disregard for potential risks.},
	language = {en},
	urldate = {2023-03-15},
	journal = {The Conversation},
	author = {Snoswell, Aaron J. and Burgess, Jean},
	month = nov,
	year = {2022},
}

@article{soe_algorithmic_2017,
	title = {Algorithmic {Detection} of {Misinformation} and {Disinformation}: {Gricean} {Perspectives}},
	volume = {74},
	issn = {0022-0418},
	shorttitle = {Algorithmic detection of misinformation and disinformation},
	url = {https://doi.org/10.1108/JD-05-2017-0075},
	doi = {10.1108/JD-05-2017-0075},
	abstract = {Purpose With the outset of automatic detection of information, misinformation, and disinformation, the purpose of this paper is to examine and discuss various conceptions of information, misinformation, and disinformation within philosophy of information. Design/methodology/approach The examinations are conducted within a Gricean framework in order to account for the communicative aspects of information, misinformation, and disinformation as well as the detection enterprise. Findings While there often is an exclusive focus on truth and falsity as that which distinguish information from misinformation and disinformation, this paper finds that the distinguishing features are actually intention/intentionality and non-misleadingness/misleadingness – with non-misleadingness/misleadingness as the primary feature. Further, the paper rehearses the argument in favor of a true variety of disinformation and extends this argument to include true misinformation. Originality/value The findings are novel and pose a challenge to the possibility of automatic detection of misinformation and disinformation. Especially the notions of true disinformation and true misinformation, as varieties of disinformation and misinformation, which force the true/false dichotomy for information vs mis-/disinformation to collapse.},
	number = {2},
	urldate = {2021-12-10},
	journal = {Journal of Documentation},
	author = {Søe, Sille Obelitz},
	month = jan,
	year = {2017},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Algorithms, Communication, Disinformation, Information, Misinformation, Philosophy},
	pages = {309--332},
}

@article{soe_unified_2021,
	title = {A {Unified} {Account} of {Information}, {Misinformation}, and {Disinformation}},
	volume = {198},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-019-02444-x},
	doi = {10.1007/s11229-019-02444-x},
	abstract = {In this paper I develop and present a unified account of information, misinformation, and disinformation and their interconnections. The unified account is rooted in Paul Grice’s notions of natural and non-natural meaning (in: Grice (ed) Studies in the way of words. Harvard University Press, Cambridge, pp 213–223, 1957) and a corresponding distinction between natural and non-natural information (Scarantino and Piccinini in Metaphilosophy 41(3):313–330, 2010). I argue that we can specify at least three specific kinds of non-natural information. Thus, as varieties of non-natural information there is intentionally non-misleading information, unintentionally misleading information—i.e. misinformation—and intentionally misleading information—i.e. disinformation. By shifting the focus from the truth-values of content to the intention/intentionality and misleadingness/non-misleadingness of that content I obtain a unified account that makes room for the potential misleadingness of true content (true disinformation), the potential non-misleadingness of false content (irony), and everything in between.},
	language = {en},
	number = {6},
	urldate = {2021-12-10},
	journal = {Synthese},
	author = {Søe, Sille Obelitz},
	month = jun,
	year = {2021},
	pages = {5929--5949},
}

@article{staw_knee-deep_1976,
	title = {Knee-{Deep} in the {Big} {Muddy}: a {Study} of {Escalating} {Commitment} to a {Chosen} {Course} of {Action}},
	volume = {16},
	issn = {0030-5073},
	shorttitle = {Knee-deep in the big muddy},
	url = {https://www.sciencedirect.com/science/article/pii/0030507376900052},
	doi = {10.1016/0030-5073(76)90005-2},
	abstract = {It is commonly expected that individuals will reverse decisions or change behaviors which result in negative consequences. Yet, within investment decision contexts, negative consequences may actually cause decision makers to increase the commitment of resources and undergo the risk of further negative consequences. The research presented here examined this process of escalating commitment through the simulation of a business investment decision. Specifically, 240 business school students participated in a role-playing exercise in which personal responsibility and decision consequences were the manipulated independent variables. Results showed that persons committed the greatest amount of resources to a previously chosen course of action when they were personally responsible for negative consequences.},
	language = {en},
	number = {1},
	urldate = {2023-03-12},
	journal = {Organizational Behavior and Human Performance},
	author = {Staw, Barry M.},
	month = jun,
	year = {1976},
	pages = {27--44},
}

@book{strevens_knowledge_2020,
	title = {The {Knowledge} {Machine}: {How} {Irrationality} {Created} {Modern} {Science}},
	publisher = {Liveright Publishing},
	author = {Strevens, Michael},
	year = {2020},
}

@article{susser_online_2019,
	title = {Online {Manipulation}: {Hidden} {Influences} in a {Digital} {World}},
	volume = {4},
	journal = {Geo. L. Tech. Rev.},
	author = {Susser, Daniel and Roessler, Beate and Nissenbaum, Helen},
	year = {2019},
	note = {Publisher: HeinOnline},
	pages = {1},
}

@book{thaler_nudge_2009,
	address = {London New York Toronto Dublin Camberwell New Delhi Rosedale Johannesburg},
	edition = {Revised edition, new international edition},
	title = {Nudge: {Improving} {Decisions} about {Health}, {Wealth} and {Happiness}},
	isbn = {978-0-14-104001-1},
	shorttitle = {Nudge},
	language = {eng},
	publisher = {Penguin Books},
	author = {Thaler, Richard H. and Sunstein, Cass R.},
	year = {2009},
}

@article{tian_fixed_1991,
	title = {Fixed {Points} {Theorems} for {Mappings} with {Non}-{Compact} and {Non}-{Convex} {Domains}},
	volume = {158},
	issn = {0022-247X},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X91902744},
	doi = {10.1016/0022-247X(91)90274-4},
	abstract = {This note gives some fixed point theorems for lower and upper semi-continuous mappings and mappings with open lower sections defined on non-compact and non-convex sets. It will be noted that the conditions of our theorems are not only sufficient but also necessary. Also our theorems generalize some well-known fixed point theorems such as the Kakutani fixed point theorem and the Brouwer-Schauder fixed point theorem by relaxing the compactness and convexity conditions.},
	language = {en},
	number = {1},
	urldate = {2021-06-17},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Tian, Guoqiang},
	month = jun,
	year = {1991},
	pages = {161--167},
}

@article{troyan_obvious_2020,
	title = {Obvious {Manipulations}},
	volume = {185},
	issn = {0022-0531},
	url = {https://www.sciencedirect.com/science/article/pii/S002205311830629X},
	doi = {10.1016/j.jet.2019.104970},
	abstract = {A mechanism is strategy-proof if agents can never profitably manipulate it, in any state of the world; however, not all non-strategy-proof mechanisms are equally easy to manipulate - some are more “obviously” manipulable than others. We propose a formal definition of an obvious manipulation in which agents compare worst cases to worst cases and best cases to best cases. We show that a profitable manipulation is obvious if and only if it can be identified as profitable by a cognitively limited agent who is unable to engage in contingent reasoning, as in Li (2017). Finally, we show that this system of categorization is both tractable and intuitively appealing by classifying common non-strategy-proof mechanisms as either obviously manipulable (OM) or not obviously manipulable (NOM).},
	language = {en},
	urldate = {2021-06-09},
	journal = {Journal of Economic Theory},
	author = {Troyan, Peter and Morrill, Thayer},
	month = jan,
	year = {2020},
	keywords = {Incentives, Manipulability, Mechanism design, Obvious strategy-proofness},
	pages = {104970},
}

@article{van_noorden_signs_2020,
	title = {Signs of ‘{Citation} {Hacking}’ {Flagged} in {Scientific} {Papers}},
	volume = {584},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-020-02378-2},
	doi = {10.1038/d41586-020-02378-2},
	abstract = {An algorithm developed to spot abnormal patterns of citations aims to find scientists who have manipulated reference lists.},
	language = {en},
	number = {7822},
	urldate = {2023-03-01},
	journal = {Nature},
	author = {Van Noorden, Richard},
	month = aug,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: News
Number: 7822
Publisher: Nature Publishing Group
Subject\_term: Mathematics and computing, Publishing, Peer review},
	keywords = {Mathematics and computing, Peer review, Publishing},
	pages = {508--508},
}

@article{veliz_chatbots_2023,
	title = {Chatbots {Shouldn}’t {Use} {Emojis}},
	volume = {615},
	copyright = {2023 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-023-00758-y},
	doi = {10.1038/d41586-023-00758-y},
	abstract = {Artificial intelligence that can manipulate our emotions is a scandal waiting to happen.},
	language = {en},
	number = {7952},
	urldate = {2023-03-14},
	journal = {Nature},
	author = {Véliz, Carissa},
	month = mar,
	year = {2023},
	note = {Bandiera\_abtest: a
Cg\_type: World View
Number: 7952
Publisher: Nature Publishing Group
Subject\_term: Ethics, Society, Machine learning, Technology},
	keywords = {Ethics, Machine learning, Society, Technology},
	pages = {375--375},
}

@article{weissman_are_2019,
	title = {Are {False} {Implicatures} {Lies}? {An} {Empirical} {Investigation}},
	volume = {34},
	issn = {1468-0017},
	shorttitle = {Are false implicatures lies?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mila.12212},
	doi = {10.1111/mila.12212},
	abstract = {Lies are typically defined as believed falsehoods asserted with the intention of deceiving the hearer. A particularly problematic case for this definition is that of false implicatures. These are prototypically cases where the proposition expressed by the speaker's utterance is true, yet an implicature conveyed by this proposition in context is false. However, implicature is a diverse category and whether a blanket statement such as “false implicatures are lies,” as some have argued can account for all of them is open to investigation. We present an experimental investigation of whether naïve participants define different types of implicatures as lies. Our results show that only a couple of types of implicatures were strongly rated as lies by participants. These results suggest that participants distinguish between different types of communicated meanings on linguistic grounds, contributing both to the literature on lying, as well as to theoretical discussions of how different types of meaning are communicated.},
	language = {en},
	number = {2},
	urldate = {2021-12-13},
	journal = {Mind \& Language},
	author = {Weissman, Benjamin and Terkourafi, Marina},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mila.12212},
	keywords = {experiment, implicatures, lying, pragmatics, what is said},
	pages = {221--246},
}

@inproceedings{zannettou_good_2018,
	title = {The {Good}, the {Bad} and the {Bait}: {Detecting} and {Characterizing} {Clickbait} on {YouTube}},
	shorttitle = {The {Good}, the {Bad} and the {Bait}},
	doi = {10.1109/SPW.2018.00018},
	abstract = {The use of deceptive techniques in user-generated video portals is ubiquitous. Unscrupulous uploaders deliberately mislabel video descriptors aiming at increasing their views and subsequently their ad revenue. This problem, usually referred to as "clickbait," may severely undermine user experience. In this work, we study the clickbait problem on YouTube by collecting metadata for 206k videos. To address it, we devise a deep learning model based on variational autoencoders that supports the diverse modalities of data that videos include. The proposed model relies on a limited amount of manually labeled data to classify a large corpus of unlabeled data. Our evaluation indicates that the proposed model offers improved performance when compared to other conventional models. Our analysis of the collected data indicates that YouTube recommendation engine does not take into account clickbait. Thus, it is susceptible to recommending misleading videos to users.},
	booktitle = {2018 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Zannettou, Savvas and Chatzis, Sotirios and Papadamou, Kostantinos and Sirivianos, Michael},
	month = may,
	year = {2018},
	keywords = {Clickbait, Correlation, Deep Learning, Engines, Entertainment industry, Machine learning, Metadata, YouTube},
	pages = {63--69},
}

@article{wetsman_facebooks_2021,
	title = {Facebook’s {Whistleblower} {Report} {Confirms} what {Researchers} {Have} {Known} for {Years}},
	url = {https://www.theverge.com/2021/10/6/22712927/facebook-instagram-teen-mental-health-research},
	abstract = {Experts have known about risks to teens for years.},
	language = {en-US},
	urldate = {2023-03-13},
	journal = {The Verge},
	author = {Wetsman, Nicole},
	month = oct,
	year = {2021},
}

@article{ciechanowski_shades_2019,
	title = {In the {Shades} of the {Uncanny} {Valley}: {An} {Experimental} {Study} of {Human}–{Chatbot} {Interaction}},
	volume = {92},
	issn = {0167-739X},
	shorttitle = {In the shades of the uncanny valley},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X17312268},
	doi = {10.1016/j.future.2018.01.055},
	abstract = {This project has been carried out in the context of recent major developments in botics and more widespread usage of virtual agents in personal and professional sphere. The general purpose of the experiment was to thoroughly examine the character of the human–non-human interaction process. Thus, in the paper, we present a study of human–chatbot interaction, focusing on the affective responses of users to different types of interfaces with which they interact. The experiment consisted of two parts: measurement of psychophysiological reactions of chatbot users and a detailed questionnaire that focused on assessing interactions and willingness to collaborate with a bot. In the first quantitative stage, participants interacted with a chatbot, either with a simple text chatbot (control group) or an avatar reading its responses in addition to only presenting them on the screen (experimental group. We gathered the following psychophysiological data from participants: electromyography (EMG), respirometer (RSP), electrocardiography (ECG), and electrodermal activity (EDA). In the last, declarative stage, participants filled out a series of questionnaires related to the experience of interacting with (chat)bots and to the overall human–(chat)bot collaboration assessment. The theory of planned behaviour survey investigated attitude towards cooperation with chatbots in the future. The social presence survey checked how much the chatbot was considered to be a “real” person. The anthropomorphism scale measured the extent to which the chatbot seems humanlike. Our particular focus was on the so-called uncanny valley effect, consisting of the feeling of eeriness and discomfort towards a given medium or technology that frequently appears in various kinds of human–machine interactions. Our results show that participants were experiencing lesser uncanny effects and less negative affect in cooperation with a simpler text chatbot than with the more complex, animated avatar chatbot. The simple chatbot have also induced less intense psychophysiological reactions. Despite major developments in botics, the user’s affective responses towards bots have frequently been neglected. In our view, understanding the user’s side may be crucial for designing better chatbots in the future and, thus, can contribute to advancing the field of human–computer interaction.},
	language = {en},
	urldate = {2023-02-28},
	journal = {Future Generation Computer Systems},
	author = {Ciechanowski, Leon and Przegalinska, Aleksandra and Magnuski, Mikolaj and Gloor, Peter},
	month = mar,
	year = {2019},
	keywords = {Affective computing, Chatbots, Human–computer interaction, Psychophysiology, Uncanny valley},
	pages = {539--548},
}

@inproceedings{halpern_towards_2018,
	address = {New Orleans, Louisiana, USA},
	series = {{AAAI}'18/{IAAI}'18/{EAAI}'18},
	title = {Towards {Formal} {Definitions} of {Blameworthiness}, {Intention}, and {Moral} responsibility},
	isbn = {978-1-57735-800-8},
	abstract = {We provide formal definitions of degree of blameworthiness and intention relative to an epistemic state (a probability over causal models and a utility function on outcomes). These, together with a definition of actual causality, provide the key ingredients for moral responsibility judgments. We show that these definitions give insight into commonsense intuitions in a variety of puzzling cases from the literature.},
	urldate = {2023-03-02},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirtieth} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference} and {Eighth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Halpern, Joseph Y. and Kleiman-Weiner, Max},
	month = feb,
	year = {2018},
	pages = {1853--1860},
}

@article{druckman_preference_2016,
	title = {Preference {Change} in {Competitive} {Political} {Environments}},
	volume = {19},
	journal = {Annual Review of Political Science},
	author = {Druckman, James N and Lupia, Arthur},
	year = {2016},
	note = {Publisher: Annual Reviews},
	pages = {13--31},
}

@inproceedings{ashton_problem_2022,
	title = {The {Problem} of {Behaviour} and {Preference} {Manipulation} in {AI} {Systems}},
	abstract = {Statistical AI or Machine learning can be applied to user data in order to understand user preferences in an effort to improve various services. This involves making assumptions about either stated or revealed preferences. Human preferences are susceptible to manipulation and change over time. When iterative AI/ML is applied, it becomes difﬁcult to ascertain whether the system has learned something about its users, whether its users have changed/learned something or whether it has taught its users to behave in a certain way in order to maximise its objective function. This article discusses the relationship between behaviour and preferences in AI/ML, existing mechanisms that manipulate human preferences and behaviour and relates them to the topic of value alignment.},
	language = {en},
	booktitle = {The {AAAI}-22 {Workshop} on {Artificial} {Intelligence} {Safety} ({SafeAI} 2022)},
	author = {Ashton, Hal and Franklin, Matija},
	year = {2022},
	keywords = {Henry},
}

@article{berger_what_2012,
	title = {What {Makes} {Online} {Content} {Viral}?},
	volume = {49},
	issn = {0022-2437, 1547-7193},
	url = {http://journals.sagepub.com/doi/10.1509/jmr.10.0353},
	doi = {10.1509/jmr.10.0353},
	abstract = {Why are certain pieces of online content (e.g., advertisements, videos, news articles) more viral than others? This article takes a psychological approach to understanding diffusion. Using a unique data set of all the New York Times articles published over a three-month period, the authors examine how emotion shapes virality. The results indicate that positive content is more viral than negative content, but the relationship between emotion and social transmission is more complex than valence alone. Virality is partially driven by physiological arousal. Content that evokes high-arousal positive (awe) or negative (anger or anxiety) emotions is more viral. Content that evokes low-arousal, or deactivating, emotions (e.g., sadness) is less viral. These results hold even when the authors control for how surprising, interesting, or practically useful content is (all of which are positively linked to virality), as well as external drivers of attention (e.g., how prominently content was featured). Experimental results further demonstrate the causal impact of specific emotion on transmission and illustrate that it is driven by the level of activation induced. Taken together, these findings shed light on why people share content and how to design more effective viral marketing campaigns.},
	language = {en},
	number = {2},
	urldate = {2022-10-25},
	journal = {Journal of Marketing Research},
	author = {Berger, Jonah and Milkman, Katherine L.},
	month = apr,
	year = {2012},
	keywords = {Read},
	pages = {192--205},
}

@article{goodhart_problems_1975,
	title = {Problems of {Monetary} {Management}: the {UK} {Experience} in {Papers} in {Monetary} {Economics}},
	volume = {1},
	journal = {Monetary Economics},
	author = {Goodhart, Charles},
	year = {1975},
}

@article{franklin_recognising_2022,
	title = {Recognising the {Importance} of {Preference} {Change}: {A} {Call} for a {Coordinated} {Multidisciplinary} {Research} {Effort} in the {Age} of {AI}},
	shorttitle = {Recognising the importance of preference change},
	url = {http://arxiv.org/abs/2203.10525},
	abstract = {As artiﬁcial intelligence becomes more powerful and a ubiquitous presence in daily life, it is imperative to understand and manage the impact of AI systems on our lives and decisions. Modern ML systems often change user behavior (e.g. personalized recommender systems learn user preferences to deliver recommendations that change online behavior). An externality of behavior change is preference change. This article argues for the establishment of a multidisciplinary endeavor focused on understanding how AI systems change preference: Preference Science. We operationalize preference to incorporate concepts from various disciplines, outlining the importance of meta-preferences and preference-change preferences, and proposing a preliminary framework for how preferences change. We draw a distinction between preference change, permissible preference change, and outright preference manipulation. A diversity of disciplines contribute unique insights to this framework.},
	language = {en},
	urldate = {2022-03-29},
	journal = {arXiv:2203.10525 [cs]},
	author = {Franklin, Matija and Ashton, Hal and Gorman, Rebecca and Armstrong, Stuart},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.10525},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@inproceedings{fogg_captology_1998,
	title = {Captology: the {Study} of {Computers} as {Persuasive} {Technologies}},
	booktitle = {{CHI} 98 {Conference} {Summary} on {Human} {Factors} in {Computing} {Systems}},
	author = {Fogg, Brian J},
	year = {1998},
	pages = {385},
}

@article{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	journal = {arXiv preprint arXiv:2107.03374},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and {others}},
	year = {2021},
}

@inproceedings{jwalapuram_evaluating_2017,
	title = {Evaluating {Dialogs} {Based} on {Grice}’s {Maxims}},
	booktitle = {Proceedings of the {Student} {Research} {Workshop} associated with {RANLP}},
	author = {Jwalapuram, Prathyusha},
	year = {2017},
	pages = {17--24},
}

@misc{ashton_solutions_2022,
	title = {Solutions to {Preference} {Manipulation} in {Recommender} {Systems} {Require} {Knowledge} of {Meta}-{Preferences}},
	url = {http://arxiv.org/abs/2209.11801},
	abstract = {Iterative machine learning algorithms used to power recommender systems often change people's preferences by trying to learn them. Further a recommender can better predict what a user will do by making its users more predictable. Some preference changes on the part of the user are self-induced and desired whether the recommender caused them or not. This paper proposes that solutions to preference manipulation in recommender systems must take into account certain meta-preferences (preferences over another preference) in order to respect the autonomy of the user and not be manipulative.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Ashton, Hal and Franklin, Matija},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11801 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{prunkl_human_2022,
	title = {Human {Autonomy} in the {Age} of {Artificial} {Intelligence}},
	volume = {4},
	copyright = {2022 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00449-9},
	doi = {10.1038/s42256-022-00449-9},
	abstract = {Current AI policy recommendations differ on what the risks to human autonomy are. To systematically address risks to autonomy, we need to confront the complexity of the concept itself and adapt governance solutions accordingly.},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {Nature Machine Intelligence},
	author = {Prunkl, Carina},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Policy, Science, technology and society},
	pages = {99--101},
}

@techreport{cftc_antidisruptive_2013,
	title = {Antidisruptive {Practices} {Authority} {Interpretative} {Guidance} and {Policy} {Statement}},
	url = {https://www.federalregister.gov/documents/2013/05/28/2013-12365/antidisruptive-practices-authority},
	number = {RIN 3038-AD96},
	urldate = {2023-03-14},
	institution = {Commodity Futures Trading Commission},
	author = {{CFTC}},
	month = may,
	year = {2013},
}

@book{carson_lying_2010,
	address = {Oxford ; New York},
	title = {Lying and {Deception}: {Theory} and {Practice}},
	isbn = {978-0-19-957741-5},
	shorttitle = {Lying and deception},
	language = {en},
	publisher = {Oxford University Press},
	author = {Carson, Thomas L.},
	year = {2010},
	note = {OCLC: ocn464581525},
	keywords = {Case studies, Truthfulness and falsehood},
}

@book{brunton_obfuscation_2015,
	address = {Cambridge, Massachusetts London, England},
	series = {Technoloy/{Politics}},
	title = {Obfuscation: a {User}'s {Guide} for {Privacy} and {Protest}},
	isbn = {978-0-262-33131-9 978-0-262-02973-5 978-0-262-52986-0},
	shorttitle = {Obfuscation},
	language = {eng},
	publisher = {The MIT Press},
	author = {Brunton, Finn and Nissenbaum, Helen},
	year = {2015},
}

@misc{brignull_deceptive_2018,
	title = {Deceptive {Design} - {User} {Interfaces} {Crafted} to {Trick} {You}},
	url = {https://www.deceptive.design/},
	abstract = {Deceptive design patterns ("dark patterns") are tricks used in websites and apps that make you buy or sign up for things that you didn't mean to. The purpose of this site is to spread awareness and to shame companies that use them.},
	language = {en},
	urldate = {2023-02-26},
	author = {Brignull, Harry},
	year = {2018},
	keywords = {Read},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-01-23},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{kosinski_theory_2023,
	title = {Theory of {Mind} {May} {Have} {Spontaneously} {Emerged} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2302.02083},
	abstract = {Theory of mind (ToM), or the ability to impute unobservable mental states to others, is central to human social interactions, communication, empathy, self-consciousness, and morality. We administer classic false-belief tasks, widely used to test ToM in humans, to several language models, without any examples or pre-training. Our results show that models published before 2022 show virtually no ability to solve ToM tasks. Yet, the January 2022 version of GPT-3 (davinci-002) solved 70\% of ToM tasks, a performance comparable with that of seven-year-old children. Moreover, its November 2022 version (davinci-003), solved 93\% of ToM tasks, a performance comparable with that of nine-year-old children. These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language models' improving language skills.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Kosinski, Michal},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02083 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@misc{ullman_large_2023,
	title = {Large {Language} {Models} {Fail} on {Trivial} {Alterations} to {Theory}-of-{Mind} {Tasks}},
	url = {http://arxiv.org/abs/2302.08399},
	abstract = {Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Ullman, Tomer},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08399 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7, Read},
}

@article{shearer_learning_2022,
	title = {Learning to {Manipulate} a {Financial} {Benchmark}},
	volume = {22},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4219227},
	doi = {10.2139/ssrn.4219227},
	language = {en},
	urldate = {2023-03-14},
	journal = {University of michigan law \& economics research paper},
	author = {Shearer, Megan and Rauterberg, Gabriel V. and Wellman, Michael P.},
	year = {2022},
}

@misc{zhu_understanding_2022,
	title = {Understanding or {Manipulation}: {Rethinking} {Online} {Performance} {Gains} of {Modern} {Recommender} {Systems}},
	shorttitle = {Understanding or {Manipulation}},
	url = {http://arxiv.org/abs/2210.05662},
	abstract = {Recommender systems are expected to be assistants that help human users find relevant information in an automatic manner without explicit queries. As recommender systems evolve, increasingly sophisticated learning techniques are applied and have achieved better performance in terms of user engagement metrics such as clicks and browsing time. The increase of the measured performance, however, can have two possible attributions: a better understanding of user preferences, and a more proactive ability to utilize human bounded rationality to seduce user over-consumption. A natural following question is whether current recommendation algorithms are manipulating user preferences. If so, can we measure the manipulation level? In this paper, we present a general framework for benchmarking the degree of manipulations of recommendation algorithms, in both slate recommendation and sequential recommendation scenarios. The framework consists of three stages, initial preference calculation, algorithm training and interaction, and metrics calculation that involves two proposed metrics, Manipulation Score and Preference Shift. We benchmark some representative recommendation algorithms in both synthetic and real-world datasets under the proposed framework. We have observed that a high online click-through rate does not mean a better understanding of user initial preference, but ends in prompting users to choose more documents they initially did not favor. Moreover, we find that the properties of training data have notable impacts on the manipulation degrees, and algorithms with more powerful modeling abilities are more sensitive to such impacts. The experiments also verified the usefulness of the proposed metrics for measuring the degree of manipulations. We advocate that future recommendation algorithm studies should be treated as an optimization problem with constrained user preference manipulations.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Zhu, Zhengbang and Qin, Rongjun and Huang, Junjie and Dai, Xinyi and Yu, Yang and Yu, Yong and Zhang, Weinan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05662 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Micah, Read},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	doi = {10.48550/arXiv.2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{laitinen_ai_2021,
	title = {{AI} {Systems} and {Respect} for {Human} {Autonomy}},
	volume = {4},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.705164},
	abstract = {This study concerns the sociotechnical bases of human autonomy. Drawing on recent literature on AI ethics, philosophical literature on dimensions of autonomy, and on independent philosophical scrutiny, we first propose a multi-dimensional model of human autonomy and then discuss how AI systems can support or hinder human autonomy. What emerges is a philosophically motivated picture of autonomy and of the normative requirements personal autonomy poses in the context of algorithmic systems. Ranging from consent to data collection and processing, to computational tasks and interface design, to institutional and societal considerations, various aspects related to sociotechnical systems must be accounted for in order to get the full picture of potential effects of AI systems on human autonomy. It is clear how human agents can, for example, via coercion or manipulation, hinder each other’s autonomy, or how they can respect each other’s autonomy. AI systems can promote or hinder human autonomy, but can they literally respect or disrespect a person’s autonomy? We argue for a philosophical view according to which AI systems—while not moral agents or bearers of duties, and unable to literally respect or disrespect—are governed by so-called “ought-to-be norms.” This explains the normativity at stake with AI systems. The responsible people (designers, users, etc.) have duties and ought-to-do norms, which correspond to these ought-to-be norms.},
	urldate = {2023-02-28},
	journal = {Frontiers in Artificial Intelligence},
	author = {Laitinen, Arto and Sahlgren, Otto},
	year = {2021},
}

@article{kolt_algorithmic_2023,
	title = {Algorithmic {Black} {Swans}},
	volume = {101},
	url = {https://ssrn.com/abstract=4370566},
	urldate = {2023-03-14},
	journal = {Washington University Law Review},
	author = {Kolt, Noam},
	year = {2023},
}

@article{serbanescu_why_2021,
	title = {Why {Does} {Artificial} {Intelligence} {Challenge} {Democracy}? {A} {Critical} {Analysis} of the {Nature} of the {Challenges} {Posed} by {AI}-{Enabled} {Manipulation}},
	volume = {5},
	url = {https://ssrn.com/abstract=4033258},
	number = {1},
	urldate = {2023-03-14},
	journal = {Copenhagen journal of legal studies},
	author = {Serbanescu, Caroline},
	year = {2021},
	pages = {105--128},
}

@misc{zhang_adversarial_2022,
	title = {"{Adversarial} {Examples}" for {Proof}-of-{Learning}},
	url = {http://arxiv.org/abs/2108.09454},
	abstract = {In S\&P '21, Jia et al. proposed a new concept/mechanism named proof-of-learning (PoL), which allows a prover to demonstrate ownership of a machine learning model by proving integrity of the training procedure. It guarantees that an adversary cannot construct a valid proof with less cost (in both computation and storage) than that made by the prover in generating the proof. A PoL proof includes a set of intermediate models recorded during training, together with the corresponding data points used to obtain each recorded model. Jia et al. claimed that an adversary merely knowing the final model and training dataset cannot efficiently find a set of intermediate models with correct data points. In this paper, however, we show that PoL is vulnerable to ``adversarial examples''! Specifically, in a similar way as optimizing an adversarial example, we could make an arbitrarily-chosen data point ``generate'' a given model, hence efficiently generating intermediate models with correct data points. We demonstrate, both theoretically and empirically, that we are able to generate a valid proof with significantly less cost than generating a proof by the prover.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Zhang, Rui and Liu, Jian and Ding, Yuan and Wu, Zhibo and Wang, Qingbiao and Ren, Kui},
	month = apr,
	year = {2022},
	note = {arXiv:2108.09454 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{susser_technology_2019,
	title = {Technology, {Autonomy}, and {Manipulation}},
	volume = {8},
	url = {https://papers.ssrn.com/abstract=3420747},
	abstract = {Since 2016, when the Facebook/Cambridge Analytica scandal began to emerge, public concern has grown around the threat of “online manipulation”. While these worries are familiar to privacy researchers, this paper aims to make them more salient to policymakers — first, by defining “online manipulation”, thus enabling identification of manipulative practices; and second, by drawing attention to the specific harms online manipulation threatens. We argue that online manipulation is the use of information technology to covertly influence another person’s decision-making, by targeting and exploiting their decision-making vulnerabilities. Engaging in such practices can harm individuals by diminishing their economic interests, but its deeper, more insidious harm is its challenge to individual autonomy. We explore this autonomy harm, emphasising its implications for both individuals and society, and we briefly outline some strategies for combating online manipulation and strengthening autonomy in an increasingly digital world.},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {Internet Policy Review},
	author = {Susser, Daniel and Roessler, Beate and Nissenbaum, Helen},
	month = jun,
	year = {2019},
	keywords = {autonomy, behavioral advertising, online manipulation, privacy},
}

@article{bodo_tackling_2017,
	title = {Tackling the {Algorithmic} {Control} {Crisis}–the {Technical}, {Legal}, and {Ethical} {Challenges} of {Research} into {Algorithmic} {Agents}},
	volume = {100},
	journal = {Yale journal of law and technology},
	author = {Bodo, B and Helberger, N and Irion, K and Zuiderveen Borgesius, F and Moller, J and van de Velde, B and Bol, N and van Es, B},
	year = {2017},
	pages = {9},
}

@misc{haroon_youtube_2022,
	title = {{YouTube}, {The} {Great} {Radicalizer}? {Auditing} and {Mitigating} {Ideological} {Biases} in {YouTube} {Recommendations}},
	shorttitle = {{YouTube}, {The} {Great} {Radicalizer}?},
	url = {http://arxiv.org/abs/2203.10666},
	abstract = {Recommendations algorithms of social media platforms are often criticized for placing users in "rabbit holes" of (increasingly) ideologically biased content. Despite these concerns, prior evidence on this algorithmic radicalization is inconsistent. Furthermore, prior work lacks systematic interventions that reduce the potential ideological bias in recommendation algorithms. We conduct a systematic audit of YouTube's recommendation system using a hundred thousand sock puppets to determine the presence of ideological bias (i.e., are recommendations aligned with users' ideology), its magnitude (i.e., are users recommended an increasing number of videos aligned with their ideology), and radicalization (i.e., are the recommendations progressively more extreme). Furthermore, we design and evaluate a bottom-up intervention to minimize ideological bias in recommendations without relying on cooperation from YouTube. We find that YouTube's recommendations do direct users -- especially right-leaning users -- to ideologically biased and increasingly radical content on both homepages and in up-next recommendations. Our intervention effectively mitigates the observed bias, leading to more recommendations to ideologically neutral, diverse, and dissimilar content, yet debiasing is especially challenging for right-leaning users. Our systematic assessment shows that while YouTube recommendations lead to ideological bias, such bias can be mitigated through our intervention.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Haroon, Muhammad and Chhabra, Anshuman and Liu, Xin and Mohapatra, Prasant and Shafiq, Zubair and Wojcieszak, Magdalena},
	month = mar,
	year = {2022},
	note = {arXiv:2203.10666 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@article{haret_axiomatic_2022,
	title = {An {Axiomatic} {Approach} to {Revising} {Preferences}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20509},
	doi = {10.1609/aaai.v36i5.20509},
	abstract = {We study a model of preference revision in which a prior preference over a set of alternatives is adjusted in order to accommodate input from an authoritative source, while maintaining certain structural constraints (e.g., transitivity, completeness), and without giving up more information than strictly necessary. We analyze this model under two aspects: the first allows us to capture natural distance-based operators, at the cost of a mismatch between the input and output formats of the revision operator. Requiring the input and output to be aligned yields a second type of operator, which we characterize using preferences on the comparisons in the prior preference Prefence revision is set in a logic-based framework and using the formal machinery of belief change, along the lines of the well-known AGM approach: we propose rationality postulates for each of the two versions of our model and derive representation results, thus situating preference revision within the larger family of belief change operators.},
	number = {5},
	urldate = {2023-03-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Haret, Adrian and Wallner, Johannes Peter},
	month = jun,
	year = {2022},
	pages = {5676--5683},
}

@book{grune-yanoff_preference_2009,
	address = {Dordrecht ; London},
	series = {Theory and decision library. {Series} {A}, {Philosophy} and methodology of the social sciences},
	title = {Preference change: approaches from philosophy, economics and psychology},
	isbn = {978-90-481-2592-0 978-90-481-2593-7},
	shorttitle = {Preference change},
	number = {v. 42},
	publisher = {Springer},
	editor = {Grüne-Yanoff, Till and Hansson, Sven Ove},
	year = {2009},
	note = {OCLC: ocn321018474},
	keywords = {Choice (Psychology), Preferences (Philosophy)},
}

@article{chaney_recommendation_2021,
	title = {Recommendation {System} {Simulations}: {A} {Discussion} of {Two} {Key} {Challenges}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {Recommendation {System} {Simulations}},
	url = {https://arxiv.org/abs/2109.02475},
	doi = {10.48550/ARXIV.2109.02475},
	abstract = {As recommendation systems become increasingly standard for online platforms, simulations provide an avenue for understanding the impacts of these systems on individuals and society. When constructing a recommendation system simulation, there are two key challenges: first, defining a model for users selecting or engaging with recommended items and second, defining a mechanism for users encountering items that are not recommended to the user directly by the platform, such as by a friend sharing specific content. This paper will delve into both of these challenges, reviewing simulation assumptions from existing research and proposing alternative assumptions. We also include a broader discussion of the limitations of simulations and outline of open questions in this area.},
	urldate = {2023-03-14},
	author = {Chaney, Allison J. B.},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)},
}

@misc{winecoff_simulation_2021,
	title = {Simulation as {Experiment}: {An} {Empirical} {Critique} of {Simulation} {Research} on {Recommender} {Systems}},
	shorttitle = {Simulation as {Experiment}},
	url = {http://arxiv.org/abs/2107.14333},
	abstract = {Simulation can enable the study of recommender system (RS) evolution while circumventing many of the issues of empirical longitudinal studies; simulations are comparatively easier to implement, are highly controlled, and pose no ethical risk to human participants. How simulation can best contribute to scientific insight about RS alongside qualitative and quantitative empirical approaches is an open question. Philosophers and researchers have long debated the epistemological nature of simulation compared to wholly theoretical or empirical methods. Simulation is often implicitly or explicitly conceptualized as occupying a middle ground between empirical and theoretical approaches, allowing researchers to realize the benefits of both. However, what is often ignored in such arguments is that without firm grounding in any single methodological tradition, simulation studies have no agreed upon scientific norms or standards, resulting in a patchwork of theoretical motivations, approaches, and implementations that are difficult to reconcile. In this position paper, we argue that simulation studies of RS are conceptually similar to empirical experimental approaches and therefore can be evaluated using the standards of empirical research methods. Using this empirical lens, we argue that the combination of high heterogeneity in approaches and low transparency in methods in simulation studies of RS has limited their interpretability, generalizability, and replicability. We contend that by adopting standards and practices common in empirical disciplines, simulation researchers can mitigate many of these weaknesses.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Winecoff, Amy A. and Sun, Matthew and Lucherini, Eli and Narayanan, Arvind},
	month = jul,
	year = {2021},
	note = {arXiv:2107.14333 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Multiagent Systems},
}

@article{bandy_problematic_2021,
	title = {Problematic {Machine} {Behavior}: {A} {Systematic} {Literature} {Review} of {Algorithm} {Audits}},
	volume = {5},
	issn = {2573-0142},
	shorttitle = {Problematic {Machine} {Behavior}},
	url = {https://dl.acm.org/doi/10.1145/3449148},
	doi = {10.1145/3449148},
	abstract = {While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice.},
	language = {en},
	number = {CSCW1},
	urldate = {2023-03-14},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Bandy, Jack},
	month = apr,
	year = {2021},
	pages = {1--34},
}

@inproceedings{abbeel_apprenticeship_2004,
	address = {Banff, Alberta, Canada},
	title = {Apprenticeship learning via inverse reinforcement learning},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
	doi = {10.1145/1015330.1015430},
	abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be diﬃcult to write down an explicit reward function specifying exactly how diﬀerent desiderata should be traded oﬀ. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using “inverse reinforcement learning” to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert’s reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert’s unknown reward function.},
	language = {en},
	urldate = {2020-04-03},
	booktitle = {Twenty-first international conference on {Machine} learning  - {ICML} '04},
	publisher = {ACM Press},
	author = {Abbeel, Pieter and Ng, Andrew Y.},
	year = {2004},
	pages = {1},
}

@article{ziebart_maximum_2008,
	title = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}},
	abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
	language = {en},
	journal = {AAAI},
	author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	year = {2008},
	keywords = {Read},
	pages = {6},
}

@article{zuger_ai_2022,
	title = {{AI} for the public. {How} public interest theory shifts the discourse on {AI}},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-022-01480-5},
	doi = {10.1007/s00146-022-01480-5},
	abstract = {AI for social good is a thriving research topic and a frequently declared goal of AI strategies and regulation. This article investigates the requirements necessary in order for AI to actually serve a public interest, and hence be socially good. The authors propose shifting the focus of the discourse towards democratic governance processes when developing and deploying AI systems. The article draws from the rich history of public interest theory in political philosophy and law, and develops a framework for ‘public interest AI’. The framework consists of (1) public justification for the AI system, (2) an emphasis on equality, (3) deliberation/ co-design process, (4) technical safeguards, and (5) openness to validation. This framework is then applied to two case studies, namely SyRI, the Dutch welfare fraud detection project, and UNICEF’s Project Connect, that maps schools worldwide. Through the analysis of these cases, the authors conclude that public interest is a helpful and practical guide for the development and governance of AI for the people.},
	language = {en},
	urldate = {2023-03-13},
	journal = {AI \& SOCIETY},
	author = {Züger, Theresa and Asghari, Hadi},
	month = jun,
	year = {2022},
	keywords = {AI ethics, Artificial intelligence, Deliberation, Democratic governance, Public interest},
}

@inproceedings{raji_actionable_2019,
	title = {Actionable {Auditing}},
	url = {https://doi.org/10.1145%2F3306618.3314244},
	doi = {10.1145/3306618.3314244},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
	month = jan,
	year = {2019},
}

@inproceedings{raji_outsider_2022,
	title = {Outsider {Oversight}: {Designing} a {Third} {Party} {Audit} {Ecosystem} for {AI} {Governance}},
	url = {https://doi.org/10.1145%2F3514094.3534181},
	doi = {10.1145/3514094.3534181},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel},
	month = jul,
	year = {2022},
}

@article{perrigo_how_2021,
	title = {How {Frances} {Haugen}’s {Team} {Forced} a {Facebook} {Reckoning}},
	url = {https://time.com/6104899/facebook-reckoning-frances-haugen/},
	abstract = {Facebook's civic integrity team put people ahead of profits},
	language = {en},
	urldate = {2023-03-13},
	journal = {Time},
	author = {Perrigo, Billy},
	month = oct,
	year = {2021},
}

@misc{thorburn_what_2022,
	title = {What {Will} “{Amplification}” {Mean} in {Court}?},
	url = {https://techpolicy.press/what-will-amplification-mean-in-court/?curius=1684},
	urldate = {2022-05-31},
	author = {Thorburn, Luke and Stray, Jonathan and Bengani, Priyanjana},
	year = {2022},
}

@article{huszar_algorithmic_2021,
	title = {Algorithmic {Amplification} of {Politics} on {Twitter}},
	url = {http://arxiv.org/abs/2110.11010},
	abstract = {Content on Twitter's home timeline is selected and ordered by personalization algorithms. By consistently ranking certain content higher, these algorithms may amplify some messages while reducing the visibility of others. There's been intense public and scholarly debate about the possibility that some political groups benefit more from algorithmic amplification than others. We provide quantitative evidence from a long-running, massive-scale randomized experiment on the Twitter platform that committed a randomized control group including nearly 2M daily active accounts to a reverse-chronological content feed free of algorithmic personalization. We present two sets of findings. First, we studied Tweets by elected legislators from major political parties in 7 countries. Our results reveal a remarkably consistent trend: In 6 out of 7 countries studied, the mainstream political right enjoys higher algorithmic amplification than the mainstream political left. Consistent with this overall trend, our second set of findings studying the U.S. media landscape revealed that algorithmic amplification favours right-leaning news sources. We further looked at whether algorithms amplify far-left and far-right political groups more than moderate ones: contrary to prevailing public belief, we did not find evidence to support this hypothesis. We hope our findings will contribute to an evidence-based debate on the role personalization algorithms play in shaping political content consumption.},
	urldate = {2021-12-15},
	journal = {arXiv:2110.11010 [cs]},
	author = {Huszár, Ferenc and Ktena, Sofia Ira and O'Brien, Conor and Belli, Luca and Schlaikjer, Andrew and Hardt, Moritz},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11010},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks, To Read (5)},
}

@misc{ribeiro_amplification_2023,
	title = {The {Amplification} {Paradox} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2302.11225},
	abstract = {Automated audits of recommender systems found that blindly following recommendations leads users to increasingly partisan, conspiratorial, or false content. At the same time, studies using real user traces suggest that recommender systems are not the primary driver of attention toward extreme content; on the contrary, such content is mostly reached through other means, e.g., other websites. In this paper, we explain the following apparent paradox: if the recommendation algorithm favors extreme content, why is it not driving its consumption? With a simple agent-based model where users attribute different utilities to items in the recommender system, we show that the collaborative-filtering nature of recommender systems and the nicheness of extreme content can resolve the apparent paradox: although blindly following recommendations would indeed lead users to niche content, users rarely consume niche content when given the option because it is of low utility to them, which can lead the recommender system to deamplify such content. Our results call for a nuanced interpretation of ``algorithmic amplification'' and highlight the importance of modeling the utility of content to users when auditing recommender systems.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Ribeiro, Manoel Horta and Veselovsky, Veniamin and West, Robert},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11225 [cs]},
	keywords = {Computer Science - Computers and Society, Read},
}

@article{ward_honesty_2023,
	title = {Honesty {Is} the {Best} {Policy}: {Defining} and {Mitigating} {AI} {Deception}},
	abstract = {Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being judged as truthful). There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a formal definition of deception in structural causal games, grounded in the philosophy literature, and applicable to real-world machine learning systems. Several examples illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception. Our main theoretical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models.},
	language = {en},
	author = {Ward, Francis Rhys and Everitt, Tom and Toni, Francesca and Belardinelli, Francesco},
	year = {2023},
}

@article{wells_facebook_2021,
	chapter = {Tech},
	title = {Facebook {Knows} {Instagram} {Is} {Toxic} for {Teen} {Girls}, {Company} {Documents} {Show}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739},
	abstract = {Its own in-depth research shows a significant teen mental-health issue that Facebook plays down in public. Part 2 in a series offering an unparalleled look inside the social-media giant’s failings—and its unwillingness or inability to address them.},
	language = {en-US},
	urldate = {2023-03-13},
	journal = {Wall Street Journal},
	author = {Wells, Georgia and Horwitz, Jeff and Seetharaman, Deepa},
	month = sep,
	year = {2021},
	keywords = {Corporate/Industrial News, Eating Disorders, FB, Facebook, Health, Instagram, LEDER, Media/Entertainment, Medical Conditions, Mental Disorders, Mood Disorders, Online Service Providers, PHOTO-COMMISSION, Political/General News, SYND, Social Issues, Social Media Platforms/Tools, Society/Community, Technology, WSJ-PRO-WSJ.com, community, corporate, eating disorders, entertainment, general news, health, industrial news, leder, media, medical conditions, mental disorders, mood disorders, online service providers, photo-commission, political, social issues, social media platforms, society, technology, tools},
}

@misc{griffin_susceptibility_2023,
	title = {Susceptibility to {Influence} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.06074},
	abstract = {Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Griffin, Lewis D. and Kleinberg, Bennett and Mozes, Maximilian and Mai, Kimberly T. and Vau, Maria and Caldwell, Matthew and Marvor-Parker, Augustine},
	month = mar,
	year = {2023},
	note = {arXiv:2303.06074 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7, I.2.m, J.4},
}

@article{christiano_algorithms_2022,
	title = {Algorithms, {Manipulation}, and {Democracy}},
	volume = {52},
	issn = {0045-5091, 1911-0820},
	url = {https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/algorithms-manipulation-and-democracy/84A19DDC35E3983C0C2FA9FAD01185C9},
	doi = {10.1017/can.2021.29},
	abstract = {Algorithmic communications pose several challenges to democracy. The three phenomena of filtering, hypernudging, and microtargeting can have the effect of polarizing an electorate and thus undermine the deliberative potential of a democratic society. Algorithms can spread fake news throughout the society, undermining the epistemic potential that broad participation in democracy is meant to offer. They can pose a threat to political equality in that some people may have the means to make use of algorithmic communications and the sophistication to be immune from attempts at manipulation, while other people are vulnerable to manipulation by those who use these means. My concern here is with the danger that algorithmic communications can pose to political equality, which arises because most citizens must make decisions about what and who to support in democratic politics with only a sparse budget of time, money, and energy. Algorithmic communications such as hypernudging and microtargeting can be a threat to democratic participation when persons are operating in environments that do not conduce to political sophistication. This constitutes a deepening of political inequality. The political sophistication necessary to counter this vulnerability is rooted for many in economic life and it can and ought to be enhanced by changing the terms of economic life.},
	language = {en},
	number = {1},
	urldate = {2023-03-13},
	journal = {Canadian Journal of Philosophy},
	author = {Christiano, Thomas},
	month = jan,
	year = {2022},
	note = {Publisher: Cambridge University Press},
	keywords = {Algorithms, democracy, informational power, low-information rationality, manipulation, nudge, political equality},
	pages = {109--124},
}

@article{fletcher_deterring_2021,
	title = {Deterring {Algorithmic} {Manipulation}},
	volume = {74},
	url = {https://heinonline.org/HOL/P?h=hein.journals/vanlr74&i=275},
	language = {eng},
	number = {2},
	urldate = {2023-03-13},
	journal = {Vanderbilt Law Review},
	author = {Fletcher, Gina-Gail S.},
	year = {2021},
	pages = {259--326},
}

@article{khambatta_targeting_2022,
	title = {Targeting {Recommendation} {Algorithms} to {Ideal} {Preferences} {Makes} {Users} {Better} {Off}},
	language = {en},
	author = {Khambatta, Poruz and Mariadassou, Shwetha and Morris, Joshua and Wheeler, S Christian},
	year = {2022},
}

@article{chaney_how_2018,
	title = {How {Algorithmic} {Confounding} in {Recommendation} {Systems} {Increases} {Homogeneity} and {Decreases} {Utility}},
	url = {http://arxiv.org/abs/1710.11214},
	doi = {10.1145/3240323.3240370},
	abstract = {Recommendation systems are ubiquitous and impact many domains; they have the potential to influence product consumption, individuals' perceptions of the world, and life-altering decisions. These systems are often evaluated or trained with data from users already exposed to algorithmic recommendations; this creates a pernicious feedback loop. Using simulations, we demonstrate how using data confounded in this way homogenizes user behavior without increasing utility.},
	urldate = {2020-09-28},
	journal = {Proceedings of the 12th ACM Conference on Recommender Systems},
	author = {Chaney, Allison J. B. and Stewart, Brandon M. and Engelhardt, Barbara E.},
	month = sep,
	year = {2018},
	note = {arXiv: 1710.11214},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Read, Statistics - Machine Learning},
	pages = {224--232},
}

@article{mansoury_feedback_2020,
	title = {Feedback {Loop} and {Bias} {Amplification} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2007.13019},
	abstract = {Recommendation algorithms are known to suffer from popularity bias; a few popular items are recommended frequently while the majority of other items are ignored. These recommendations are then consumed by the users, their reaction will be logged and added to the system: what is generally known as a feedback loop. In this paper, we propose a method for simulating the users interaction with the recommenders in an offline setting and study the impact of feedback loop on the popularity bias amplification of several recommendation algorithms. We then show how this bias amplification leads to several other problems such as declining the aggregate diversity, shifting the representation of users' taste over time and also homogenization of the users experience. In particular, we show that the impact of feedback loop is generally stronger for the users who belong to the minority group.},
	urldate = {2020-09-12},
	journal = {arXiv:2007.13019 [cs]},
	author = {Mansoury, Masoud and Abdollahpouri, Himan and Pechenizkiy, Mykola and Mobasher, Bamshad and Burke, Robin},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.13019},
	keywords = {Computer Science - Information Retrieval, Read},
}

@article{jiang_degenerate_2019,
	title = {Degenerate {Feedback} {Loops} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/1902.10730},
	doi = {10.1145/3306618.3314288},
	abstract = {Machine learning is used extensively in recommender systems deployed in products. The decisions made by these systems can inﬂuence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. This phenomenon can give rise to the so-called “echo chambers” or “ﬁlter bubbles” that have user and societal implications. In this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the ﬁlter bubble effect. In addition, we offer practical solutions to slow down system degeneracy. Our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.},
	language = {en},
	urldate = {2020-04-03},
	journal = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Jiang, Ray and Chiappa, Silvia and Lattimore, Tor and György, András and Kohli, Pushmeet},
	month = jan,
	year = {2019},
	note = {arXiv: 1902.10730},
	keywords = {Computer Science - Machine Learning, Read, Statistics - Machine Learning},
	pages = {383--390},
}

@article{doroudi_wheres_2019,
	title = {Where’s the {Reward}?},
	volume = {29},
	issn = {1560-4306},
	url = {https://doi.org/10.1007/s40593-019-00187-x},
	doi = {10.1007/s40593-019-00187-x},
	abstract = {Since the 1960s, researchers have been trying to optimize the sequencing of instructional activities using the tools of reinforcement learning (RL) and sequential decision making under uncertainty. Many researchers have realized that reinforcement learning provides a natural framework for optimal instructional sequencing given a particular model of student learning, and excitement towards this area of research is as alive now as it was over fifty years ago. But does RL actually help students learn? If so, when and where might we expect it to be most helpful? To help answer these questions, we review the variety of attempts to use RL for instructional sequencing. First, we present a historical narrative of this research area. We identify three waves of research, which gives us a sense of the various communities of researchers that have been interested in this problem and where the field is going. Second, we review all of the empirical research that has compared RL-induced instructional policies to baseline methods of sequencing. We find that over half of the studies found that RL-induced policies significantly outperform baselines. Moreover, we identify five clusters of studies with different characteristics and varying levels of success in using RL to help students learn. We find that reinforcement learning has been most successful in cases where it has been constrained with ideas and theories from cognitive psychology and the learning sciences. However, given that our theories and models are limited, we also find that it has been useful to complement this approach with running more robust offline analyses that do not rely heavily on the assumptions of one particular model. Given that many researchers are turning to deep reinforcement learning and big data to tackle instructional sequencing, we believe keeping these best practices in mind can help guide the way to the reward in using RL for instructional sequencing.},
	language = {en},
	number = {4},
	urldate = {2023-02-08},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Doroudi, Shayan and Aleven, Vincent and Brunskill, Emma},
	month = dec,
	year = {2019},
	keywords = {Read},
	pages = {568--620},
}

@inproceedings{bassen_reinforcement_2020,
	address = {Honolulu HI USA},
	title = {Reinforcement {Learning} for the {Adaptive} {Scheduling} of {Educational} {Activities}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376518},
	doi = {10.1145/3313831.3376518},
	abstract = {Adaptive instruction for online education can increase learning gains and decrease the work required of learners, instructors, and course designers. Reinforcement Learning (RL) is a promising tool for developing instructional policies, as RL models can learn complex relationships between course activities, learner actions, and educational outcomes. This paper demonstrates the ﬁrst RL model to schedule educational activities in real-time for a large online course. Our model learns to assign a sequence of course activities while maximizing learning gains and minimizing the number of items assigned. Using a controlled experiment with over 1,000 learners, we investigate how this scheduling policy affects learning gains, dropout rates, and qualitative learner feedback. We show that our model produces better learning gains using fewer educational activities than a linear assignment condition, and produces similar learning gains to a self-directed condition using fewer educational activities and with lower dropout rates.},
	language = {en},
	urldate = {2022-04-02},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Bassen, Jonathan and Balaji, Bharathan and Schaarschmidt, Michael and Thille, Candace and Painter, Jay and Zimmaro, Dawn and Games, Alex and Fast, Ethan and Mitchell, John C.},
	month = apr,
	year = {2020},
	keywords = {Read},
	pages = {1--12},
}

@article{johnson_defaults_2003,
	title = {Do {Defaults} {Save} {Lives}?},
	volume = {302},
	url = {https://www.science.org/doi/10.1126/science.1091721},
	doi = {10.1126/science.1091721},
	number = {5649},
	urldate = {2023-03-13},
	journal = {Science},
	author = {Johnson, Eric J. and Goldstein, Daniel},
	month = nov,
	year = {2003},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1338--1339},
}

@article{robitaille_greater_2015,
	title = {The {Greater} {Good}: {Behavioral} {Research} with {Social} {Value}},
	language = {en},
	author = {Robitaille, Nicole and Mazar, Nina and John, Leslie K and Roberto, Christina and Mochon, Daniel and Schwartz, Janet and Maroba, Josiase and Patel, Deepak and Ariely, Dan and Donnelly, Grant E and Lamberton, Cait Poynor and Reczek, Rebecca Walker and Norton, Michael I},
	year = {2015},
}

@misc{vincent_microsofts_2023,
	title = {Microsoft’s {Bing} is an emotionally manipulative liar, and people love it},
	url = {https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams},
	abstract = {Bing’s acting unhinged, and lots of people love it.},
	language = {en-US},
	urldate = {2023-03-12},
	journal = {The Verge},
	author = {Vincent, James},
	month = feb,
	year = {2023},
}

@misc{orseau_agents_2018,
	title = {Agents and {Devices}: {A} {Relative} {Definition} of {Agency}},
	shorttitle = {Agents and {Devices}},
	url = {http://arxiv.org/abs/1805.12387},
	doi = {10.48550/arXiv.1805.12387},
	abstract = {According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Orseau, Laurent and McGill, Simon McGregor and Legg, Shane},
	month = may,
	year = {2018},
	note = {arXiv:1805.12387 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{sinders_whats_2022,
	title = {What’s {In} a {Name}?},
	url = {https://medium.com/@carolinesinders/whats-in-a-name-unpacking-dark-patterns-versus-deceptive-design-e96068627ec4},
	abstract = {Unpacking Dark Patterns versus Deceptive Design},
	language = {en},
	urldate = {2023-03-12},
	journal = {Medium},
	author = {sinders, caroline},
	month = jun,
	year = {2022},
}

@techreport{christiano_eliciting_2021,
	title = {Eliciting {Latent} {Knowledge}},
	url = {https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc},
	abstract = {How can we train an AI to honestly tell us when our eyes deceive us?},
	language = {en},
	urldate = {2023-03-10},
	institution = {Alignment Research Center},
	author = {Christiano, Paul and Cotra, Ajeya and Xu, Mark},
	year = {2021},
}

@article{olsson_-context_2022,
	title = {In-context {Learning} and {Induction} {Heads}},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
}

@misc{hammond_reasoning_2023,
	title = {Reasoning about {Causality} in {Games}},
	url = {http://arxiv.org/abs/2301.02324},
	abstract = {Causal reasoning and game-theoretic reasoning are fundamental topics in artificial intelligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl's causal hierarchy to the game-theoretic domain, or as extending Koller and Milch's multi-agent influence diagrams to the causal domain. We then consider three key questions: i) How can the (causal) dependencies in games - either between variables, or between strategies - be modelled in a uniform, principled manner? ii) How may causal queries be computed in causal games, and what assumptions does this require? iii) How do causal games compare to existing formalisms? To address question i), we introduce mechanised games, which encode dependencies between agents' decision rules and the distributions governing the game. In response to question ii), we present definitions of predictions, interventions, and counterfactuals, and discuss the assumptions required for each. Regarding question iii), we describe correspondences between causal games and other formalisms, and explain how causal games can be used to answer queries that other causal or game-theoretic models do not support. Finally, we highlight possible applications of causal games, aided by an extensive open-source Python library.},
	urldate = {2023-03-10},
	publisher = {arXiv},
	author = {Hammond, Lewis and Fox, James and Everitt, Tom and Carey, Ryan and Abate, Alessandro and Wooldridge, Michael},
	month = jan,
	year = {2023},
	note = {arXiv:2301.02324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
}

@article{ward_agent_2022,
	title = {On {Agent} {Incentives} to {Manipulate} {Human} {Feedback} in {Multi}-{Agent} {Reward} {Learning} {Scenarios}},
	abstract = {In settings without well-defined goals, methods for reward learning allow reinforcement learning agents to infer goals from human feedback. Existing work has discussed the problem that such agents may manipulate humans, or the reward learning process, in order to gain higher reward. We introduce the neglected problem that, in multi-agent settings, agents may have incentives to manipulate one another’s reward functions in order to change each other’s behavioral policies. We focus on the setting with humans acting alongside assistive (artificial) agents who must learn the reward function by interacting with these humans. We propose a possible solution to manipulation of human feedback in this setting: the Shared Value Prior (SVP). The SVP equips agents with an assumption that the reward functions of all humans are similar. Given this assumption, the actions of any human provide information to an agent about its reward, and so the agent is incentivised to observe these actions rather than to manipulate them. We present an expository example in which the SVP prevents manipulation.},
	language = {en},
	author = {Ward, Francis Rhys},
	year = {2022},
}

@inproceedings{clarke_survey_2022,
	title = {A {Survey} of the {Potential} {Long}-term {Impacts} of {AI}},
	url = {https://doi.org/10.1145%2F3514094.3534131},
	doi = {10.1145/3514094.3534131},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Clarke, Sam and Whittlestone, Jess},
	month = jul,
	year = {2022},
}

@article{hardt_performative_2022,
	title = {Performative {Power}},
	url = {http://arxiv.org/abs/2203.17232},
	abstract = {We introduce the notion of performative power, which measures the ability of a firm operating an algorithmic system, such as a digital content recommendation platform, to steer a population. We relate performative power to the economic theory of market power. Traditional economic concepts are well known to struggle with identifying anti-competitive patterns in digital platforms--a core challenge is the difficulty of defining the market, its participants, products, and prices. Performative power sidesteps the problem of market definition by focusing on a directly observable statistical measure instead. High performative power enables a platform to profit from steering participant behavior, whereas low performative power ensures that learning from historical data is close to optimal. Our first general result shows that under low performative power, a firm cannot do better than standard supervised learning on observed data. We draw an analogy with a firm being a price-taker, an economic condition that arises under perfect competition in classical market models. We then contrast this with a market where performative power is concentrated and show that the equilibrium state can differ significantly. We go on to study performative power in a concrete setting of strategic classification where participants can switch between competing firms. We show that monopolies maximize performative power and disutility for the participant, while competition and outside options decrease performative power. We end on a discussion of connections to measures of market power in economics and of the relationship with ongoing antitrust debates.},
	urldate = {2022-04-04},
	journal = {arXiv:2203.17232 [cs, econ]},
	author = {Hardt, Moritz and Jagadeesan, Meena and Mendler-Dünner, Celestine},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.17232},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Computers and Society, Computer Science - Machine Learning, Economics - Theoretical Economics, To Read (5)},
}

@misc{kenton_discovering_2022,
	title = {Discovering {Agents}},
	url = {http://arxiv.org/abs/2208.08345},
	doi = {10.48550/arXiv.2208.08345},
	abstract = {Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial -- often the causal model is just assumed by the modeler without much justification -- and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents -- roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering agents from empirical data, and give algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.},
	urldate = {2022-08-19},
	publisher = {arXiv},
	author = {Kenton, Zachary and Kumar, Ramana and Farquhar, Sebastian and Richens, Jonathan and MacDermott, Matt and Everitt, Tom},
	month = aug,
	year = {2022},
	note = {arXiv:2208.08345 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read},
}

@article{de_rosis_can_2003,
	title = {Can {Computers} {Deliberately} {Deceive}? {A} {Simulation} {Tool} and {Its} {Application} to {Turing}'s {Imitation} {Game}},
	volume = {19},
	issn = {0824-7935, 1467-8640},
	shorttitle = {Can {Computers} {Deliberately} {Deceive}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-8640.00223},
	doi = {10.1111/1467-8640.00223},
	language = {en},
	number = {3},
	urldate = {2023-03-08},
	journal = {Computational Intelligence},
	author = {de Rosis, Fiorella and Carofiglio, Valeria and Grassano, Giuseppe and Castelfranchi, Cristiano},
	month = aug,
	year = {2003},
	pages = {235--263},
}

@misc{jakesch_co-writing_2023,
	title = {Co-{Writing} with {Opinionated} {Language} {Models} {Affects} {Users}' {Views}},
	url = {http://arxiv.org/abs/2302.00560},
	doi = {10.1145/3544548.3581196},
	abstract = {If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale. This study investigates whether a language-model-powered writing assistant that generates some opinions more often than others impacts what users write - and what they think. In an online experiment, we asked participants (N=1,506) to write a post discussing whether social media is good for society. Treatment group participants used a language-model-powered writing assistant configured to argue that social media is good or bad for society. Participants then completed a social media attitude survey, and independent judges (N=500) evaluated the opinions expressed in their writing. Using the opinionated language model affected the opinions expressed in participants' writing and shifted their opinions in the subsequent attitude survey. We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.},
	urldate = {2023-03-08},
	author = {Jakesch, Maurice and Bhat, Advait and Buschek, Daniel and Zalmanson, Lior and Naaman, Mor},
	month = feb,
	year = {2023},
	note = {arXiv:2302.00560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@book{fogg_persuasive_2003,
	title = {Persuasive {Technology}},
	isbn = {978-1-55860-643-2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558606432X50008},
	language = {en},
	urldate = {2023-03-08},
	publisher = {Elsevier},
	author = {Fogg, Brian J},
	year = {2003},
	doi = {10.1016/B978-1-55860-643-2.X5000-8},
}

@article{azzutti_machine_2021,
	title = {Machine {Learning}, {Market} {Manipulation} and {Collusion} on {Capital} {Markets}: {Why} the},
	volume = {43},
	shorttitle = {Machine {Learning}, {Market} {Manipulation} and {Collusion} on {Capital} {Markets}},
	url = {https://papers.ssrn.com/abstract=3788872},
	doi = {10.2139/ssrn.3788872},
	abstract = {This Article offers a novel perspective on the implications of increasingly autonomous and “black box” algorithms, within the ramification of algorithmic trading, for the integrity of capital markets. Artificial intelligence (AI) and particularly its subfield of machine learning (ML) methods have gained immense popularity among the great public and achieved tremendous success in many real-life applications by leading to vast efficiency gains. In the financial trading domain, ML can augment human capabilities in price prediction, dynamic portfolio optimization, and other financial decision-making tasks. However, thanks to constant progress in the ML technology, the prospect of increasingly capable and autonomous agents to delegate operational tasks and even decision-making is now beyond mere imagination, thus opening up the possibility for approximating (truly) autonomous trading agents anytime soon.Given these spectacular developments, this Article argues that such autonomous algorithmic traders may involve significant risks to market integrity, independent from their human experts, thanks to self-learning capabilities offered by state-of-the-art and innovative ML methods. Using the proprietary trading industry as a case study, we explore emerging threats to the application of established market abuse laws in the event of algorithmic market abuse, by taking an interdisciplinary stance between financial regulation, law and economics, and computational finance. Specifically, our analysis focuses on two emerging market abuse risks by autonomous algorithms: market manipulation and “tacit” collusion. We explore their likelihood to arise in global capital markets and evaluate related social harm as forms of market failures.With these new risks in mind, this Article questions the adequacy of existing regulatory frameworks and enforcement mechanisms, as well as current legal rules on the governance of algorithmic trading, to cope with increasingly autonomous and ubiquitous algorithmic trading systems. We demonstrate how the “black box” nature of specific ML-powered algorithmic trading strategies can subvert existing market abuse laws, which are based upon traditional liability concepts and tests (such as “intent” and “causation”). We conclude by addressing the shortcomings of the present legal framework and develop a number of guiding principles to assist legal and policy reform in the spirit of promoting and safeguarding market integrity and safety.},
	language = {en},
	number = {1},
	urldate = {2023-03-07},
	journal = {University of Pennsylvania journal of international law},
	author = {Azzutti, Alessio and Ringe, Wolf-Georg and Stiehl, H. Siegfried},
	year = {2021},
	keywords = {algorithmic trading, artificial intelligence, black box, collusion, market manipulation},
}

@article{bathaee_artificial_2018,
	title = {The {Artificial} {Intelligence} {Black} {Box} and the {Failure} of {Intent} and {Causation}},
	volume = {31},
	language = {en},
	number = {2},
	journal = {Harvard Journal of Law and Technology},
	author = {Bathaee, Yavar},
	year = {2018},
	pages = {890--938},
}

@inproceedings{burns_discovering_2023,
	title = {Discovering {Latent} {Knowledge} in {Language} {Models} {Without} {Supervision}},
	url = {https://openreview.net/forum?id=ETKGuby0hcs},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
	year = {2023},
}

@misc{financial_conduct_authoritya_fca_2016,
	chapter = {1.2.3},
	title = {{FCA} {Handbook}: {MAR} 1 {Market} {Abuse}},
	url = {https://www.handbook.fca.org.uk/handbook/MAR.pdf},
	author = {Financial Conduct AuthorityA},
	year = {2016},
}

@article{huang_redefining_2009,
	title = {Redefining {Market} {Manipulation} in {Australia}: {The} {Role} of an {Implied} {Intent} {Element}},
	volume = {27},
	shorttitle = {Redefining {Market} {Manipulation} in {Australia}},
	url = {https://papers.ssrn.com/abstract=1376209},
	abstract = {The Australian market manipulation law has been amended with a view to improving its efficacy. Amongst the major amendments is the removal of the term "intent" from the wording of the relevant provisions. This article argues that despite the change, the intent element is still to be implied into the law. For criminal liability, the Criminal Code supplies the fault elements, albeit some technical problems with its application. This is termed statutorily implied intent approach. In civil penalty cases where the Criminal Code has no operation, intent should be implied through the concept of "artificiality" for a number of reasons. This is dubbed self-implied intent approach as intent is inherently embedded in the concepts of 'artificiality'. It is also submitted that the general intent to be judicially developed through the self-implied intent approach may represent an improvement on the specific intent requirement found in the former provisions.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Companies and Securities Law Journal},
	author = {Huang, (Robin) Hui},
	month = apr,
	year = {2009},
	keywords = {artificiality, civil penalty, intent, market manipulation},
}

@article{lin_new_2017,
	title = {The {New} {Market} {Manipulation}},
	volume = {66},
	url = {https://papers.ssrn.com/abstract=2996896},
	abstract = {Markets face a new and daunting mode of manipulation. With this new mode of market manipulation, millions of dollars can vanish in seconds, rogue actors can halt the trading of billion-dollar companies, and trillion-dollar financial markets can be distorted with a simple click or a few lines of code. Every investor and institution is at risk. This is the new precarious reality of our financial markets.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Emory Law Journal},
	author = {Lin, Tom C. W.},
	month = jul,
	year = {2017},
	keywords = {Flash Boys, LIBOR, algorithmic trading, artificial intelligence, cybercrime, fake news, fin-tech, financial fraud, financial regulation, financial technology, flash crash, high frequency trading, market manipulation, securities fraud, securities regulation, systemic risk},
}

@article{cooper_mysterious_2016,
	title = {The {Mysterious} {Ethics} of {High}-{Frequency} {Trading}},
	volume = {26},
	issn = {1052-150X, 2153-3326},
	url = {https://www.cambridge.org/core/product/identifier/S1052150X1500041X/type/journal_article},
	doi = {10.1017/beq.2015.41},
	abstract = {ABSTRACT: 
            The ethics of high frequency trading are obscure, due in part to the complexity of the practice. This article contributes to the existing literature of ethics in financial markets by examining a recent trend in regulation in high frequency trading, the prohibition of deception. We argue that in the financial markets almost any regulation, other than the most basic, tends to create a moral hazard and increase information asymmetry. Since the market’s job is, at least in part, price discovery, we argue that simplicity of regulation and restraint in regulation are virtues to a greater extent than in other areas of finance. This article proposes criteria for determining which high-frequency trading strategies should be regulated.},
	language = {en},
	number = {1},
	urldate = {2023-03-07},
	journal = {Business Ethics Quarterly},
	author = {Cooper, Ricky and Davis, Michael and Van Vliet, Ben},
	month = jan,
	year = {2016},
	pages = {1--22},
}

@misc{clark_faulty_2016,
	title = {Faulty {Reward} {Functions} in the {Wild}},
	url = {https://openai.com/blog/faulty-reward-functions/},
	abstract = {Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.},
	language = {en},
	urldate = {2023-02-03},
	journal = {OpenAI},
	author = {Clark, Jack and Amodei, Dario},
	month = dec,
	year = {2016},
}

@misc{skalse_defining_2022,
	title = {Defining and {Characterizing} {Reward} {Hacking}},
	url = {http://arxiv.org/abs/2209.13085},
	abstract = {We provide the first formal definition of reward hacking, a phenomenon where optimizing an imperfect proxy reward function, \${\textbackslash}mathcal\{{\textbackslash}tilde\{R\}\}\$, leads to poor performance according to the true reward function, \${\textbackslash}mathcal\{R\}\$. We say that a proxy is unhackable if increasing the expected proxy return can never decrease the expected true return. Intuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it "narrower") or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case. A key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. In particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant. We thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability. Our results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Skalse, Joar and Howe, Nikolaus H. R. and Krasheninnikov, Dmitrii and Krueger, David},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13085 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{langlois_how_2021,
	title = {How {RL} {Agents} {Behave} {When} {Their} {Actions} {Are} {Modified}},
	url = {http://arxiv.org/abs/2102.07716},
	abstract = {Reinforcement learning in complex environments may require supervision to prevent the agent from attempting dangerous actions. As a result of supervisor intervention, the executed action may differ from the action specified by the policy. How does this affect learning? We present the Modified-Action Markov Decision Process, an extension of the MDP model that allows actions to differ from the policy. We analyze the asymptotic behaviours of common reinforcement learning algorithms in this setting and show that they adapt in different ways: some completely ignore modifications while others go to various lengths in trying to avoid action modifications that decrease reward. By choosing the right algorithm, developers can prevent their agents from learning to circumvent interruptions or constraints, and better control agent responses to other kinds of action modification, like self-damage.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Langlois, Eric D. and Everitt, Tom},
	month = jun,
	year = {2021},
	note = {arXiv:2102.07716 [cs]},
	keywords = {Computer Science - Artificial Intelligence, To Read (5)},
}

@article{yeung_hypernudge_2017,
	title = {‘{Hypernudge}’: {Big} {Data} as a mode of regulation by design},
	volume = {20},
	issn = {1369-118X, 1468-4462},
	shorttitle = {‘{Hypernudge}’},
	url = {https://www.tandfonline.com/doi/full/10.1080/1369118X.2016.1186713},
	doi = {10.1080/1369118X.2016.1186713},
	language = {en},
	number = {1},
	urldate = {2023-03-03},
	journal = {Information, Communication \& Society},
	author = {Yeung, Karen},
	month = jan,
	year = {2017},
	pages = {118--136},
}

@article{zuiderveen_borgesius_online_2018,
	title = {Online {Political} {Microtargeting}: {Promises} and {Threats} for {Democracy}},
	volume = {14},
	shorttitle = {Online {Political} {Microtargeting}},
	url = {https://papers.ssrn.com/abstract=3128787},
	abstract = {Online political microtargeting involves monitoring people’s online behaviour, and using the collected data, sometimes enriched with other data, to show people-targeted political advertisements. Online political microtargeting is widely used in the US; Europe may not be far behind. This paper maps microtargeting’s promises and threats to democracy. For example, microtargeting promises to optimise the match between the electorate’s concerns and political campaigns, and to boost campaign engagement and political participation. But online microtargeting could also threaten democracy. For instance, a political party could, misleadingly, present itself as a different one-issue party to different individuals. And data collection for microtargeting raises privacy concerns. We sketch possibilities for policymakers if they seek to regulate online political microtargeting. We discuss which measures would be possible, while complying with the right to freedom of expression under the European Convention on Human Rights.},
	language = {en},
	number = {1},
	urldate = {2023-03-03},
	journal = {Utrecht Law Review},
	author = {Zuiderveen Borgesius, Frederik and Moeller, Judith and Kruikemeier, Sanne and Ó Fathaigh, Ronan and Irion, Kristina and Dobber, Tom and Bodó, Balázs and de Vreese, Claes H.},
	month = feb,
	year = {2018},
	keywords = {democracy, elections, microtargeting, political campaigns, privacy, profiling},
	pages = {82--96},
}

@misc{li_implicit_2021,
	title = {Implicit {Representations} of {Meaning} in {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2106.00737},
	doi = {10.48550/arXiv.2106.00737},
	abstract = {Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data. Code and data are available at https://github.com/belindal/state-probes .},
	urldate = {2022-09-07},
	publisher = {arXiv},
	author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
	month = jun,
	year = {2021},
	note = {arXiv:2106.00737 [cs]},
	keywords = {Computer Science - Computation and Language, Micah},
}

@article{willis_deception_2020,
	title = {Deception by {Design}},
	volume = {34},
	url = {https://papers.ssrn.com/abstract=3694575},
	abstract = {Big data, ubiquitous tracking, and artificial intelligence are enabling businesses to disseminate rapidly proliferating permutations of digital marketing and sales materials, each of which can be micro-targeted to particular consumers in real time and space.  When the algorithms that design and deliver these advertisements, websites, and apps are optimized only for profit and deception is profitable, consumers will be deceived.  Yet at the same time that digital deception is becoming inevitable, it is racing toward immunity from liability.  The accepted evidentiary methods for proving deception, from direct application of the reasonable person standard to controlled experiments demonstrating the deceptiveness of a defendant’s conduct, are neither practicable nor scientifically valid when applied to vast numbers of unique, micro-targeted communications.  This article identifies and explains this emerging threat to the legal regulation of the consumer marketplace and suggests ways in which the law might give businesses sufficient incentive to engage in fair marketing by design.},
	language = {en},
	number = {1},
	urldate = {2023-03-02},
	journal = {Harvard journal of law and technology},
	author = {Willis, Lauren E.},
	month = aug,
	year = {2020},
	keywords = {algorithms, big data, consumer protection, deception, evidence, machine learning, marketing, unfair business practices, unfair competition},
}

@misc{andreas_language_2022,
	title = {Language {Models} as {Agent} {Models}},
	url = {http://arxiv.org/abs/2212.01681},
	doi = {10.48550/arXiv.2212.01681},
	abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in an outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them -- a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Andreas, Jacob},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01681 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Multiagent Systems},
}

@misc{janus_simulators_2022,
	title = {Simulators},
	url = {https://generative.ink/posts/simulators/},
	author = {Janus},
	month = sep,
	year = {2022},
}

@article{ward_causal_2022,
	title = {A {Causal} {Perspective} on {AI} {Deception} in {Games}},
	author = {Ward, Francis Rhys and Toni, Francesca and Belardinelli, Francesco},
	year = {2022},
}

@article{calo_digital_2014,
	title = {Digital {Market} {Manipulation}},
	volume = {82},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2309703},
	doi = {10.2139/ssrn.2309703},
	language = {en},
	number = {4},
	urldate = {2023-03-02},
	journal = {George Washington Law Review},
	author = {Calo, M. Ryan},
	year = {2014},
	pages = {996--1051},
}

@article{kamenica_bayesian_2011,
	title = {Bayesian {Persuasion}},
	volume = {101},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.101.6.2590},
	doi = {10.1257/aer.101.6.2590},
	abstract = {When is it possible for one person to persuade another to change her action? We consider a symmetric information model where a sender chooses a signal to reveal to a receiver, who then takes a noncontractible action that affects the welfare of both players. We derive
necessary and sufficient conditions for the existence of a signal that strictly benefits the sender. We characterize sender-optimal signals. We examine comparative statics with respect to the alignment of the sender's and the receiver's preferences. Finally, we apply our results to persuasion by litigators, lobbyists, and salespeople. (JEL D72, D82, D83, K40, M31)},
	language = {en},
	number = {6},
	urldate = {2022-11-22},
	journal = {American Economic Review},
	author = {Kamenica, Emir and Gentzkow, Matthew},
	month = oct,
	year = {2011},
	keywords = {Belief, Legal Procedure, the Legal System, and Illegal Behavior: General, Marketing, Communication, Information and Knowledge, Learning, Political Processes: Rent-seeking, Lobbying, Elections, Legislatures, and Voting Behavior, Asymmetric and Private Information, Search},
	pages = {2590--2615},
}

@article{pauli_modelling_2022,
	title = {Modelling {Persuasion} through {Misuse} of {Rhetorical} {Appeals}},
	abstract = {It is important to understand how people use words to persuade each other. This helps understand debate, and detect persuasive narratives in regard to e.g. misinformation. While computational modelling of some aspects of persuasion has received some attention, a way to unify and describe the overall phenomenon of when persuasion becomes undesired and problematic, is missing. In this paper, we attempt to address this by proposing a taxonomy of computational persuasion. Drawing upon existing research and resources, this paper shows how to re-frame and re-organise current work into a coherent framework targeting the misuse of rhetorical appeals. As a study to validate these re-framings, we then train and evaluate models of persuasion adapted to our taxonomy. Our results show an application of our taxonomy, and we are able to detecting misuse of rhetorical appeals, finding that these are more often used in misinformative contexts than in true ones.},
	language = {en},
	author = {Pauli, Amalie Brogaard and Derczynski, Leon and Assent, Ira},
	year = {2022},
}

@article{bai_artificial_2023,
	title = {Artificial {Intelligence} {Can} {Persuade} {Humans}},
	language = {en},
	author = {Bai, Hui},
	year = {2023},
	keywords = {Read},
}

@article{hirsh_personalized_2012,
	title = {Personalized {Persuasion}: {Tailoring} {Persuasive} {Appeals} to {Recipients}’ {Personality} {Traits}},
	volume = {23},
	issn = {0956-7976},
	shorttitle = {Personalized {Persuasion}},
	url = {https://doi.org/10.1177/0956797611436349},
	doi = {10.1177/0956797611436349},
	abstract = {Persuasive messages are more effective when they are custom-tailored to reflect the interests and concerns of the intended audience. Much of the message-framing literature has focused on the advantages of using either gain or loss frames, depending on the motivational orientation of the target group. In the current study, we extended this research to examine whether a persuasive appeal?s effectiveness can be increased by aligning the message framing with the recipient?s personality profile. For a single product, we constructed five advertisements, each designed to target one of the five major trait domains of human personality. In a sample of 324 survey respondents, advertisements were evaluated more positively the more they cohered with participants? dispositional motives. These results suggest that adapting persuasive messages to the personality traits of the target audience can be an effective way of increasing the messages? impact, and highlight the potential value of personality-based communication strategies.},
	language = {en},
	number = {6},
	urldate = {2023-02-07},
	journal = {Psychological Science},
	author = {Hirsh, Jacob B. and Kang, Sonia K. and Bodenhausen, Galen V.},
	month = jun,
	year = {2012},
	note = {Publisher: SAGE Publications Inc},
	pages = {578--581},
}

@misc{chan_harms_2023,
	title = {Harms from {Increasingly} {Agentic} {Algorithmic} {Systems}},
	url = {http://arxiv.org/abs/2302.10329},
	doi = {10.48550/arXiv.2302.10329},
	abstract = {Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll, Micah and Lin, Michelle and Mayhew, Alex and Collins, Katherine and Molamohammadi, Maryam and Burden, John and Zhao, Wanru and Rismani, Shalaleh and Voudouris, Konstantinos and Bhatt, Umang and Weller, Adrian and Krueger, David and Maharaj, Tegan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.10329 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@article{manheim_categorizing_2019,
	title = {Categorizing {Variants} of {Goodhart}'s {Law}},
	url = {http://arxiv.org/abs/1803.04585},
	abstract = {There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are "(at least) four different mechanisms" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.},
	urldate = {2020-11-08},
	journal = {arXiv:1803.04585 [cs, q-fin, stat]},
	author = {Manheim, David and Garrabrant, Scott},
	month = feb,
	year = {2019},
	note = {arXiv: 1803.04585},
	keywords = {91E45, Computer Science - Artificial Intelligence, Quantitative Finance - General Finance, Statistics - Machine Learning},
}

@incollection{baron_mens_2014,
	title = {The {Mens} {Rea} and {Moral}  {Status} of {Manipulation}},
	isbn = {978-0-19-933820-7},
	url = {https://doi.org/10.1093/acprof:oso/9780199338207.003.0005},
	abstract = {This chapter explores two matters only briefly touched on in earlier work: for purposes of clarity, in her “Manipulativeness,” Baron took a stand on whether manipulation requires intent and whether “manipulative” is best understood as allowing for the possibility that the quality or action described as manipulative is unobjectionable. This chapter revisits both issues, and explores in more detail what it means to say that manipulation requires intent. It argues that manipulation does require intent (though the intent need not be conscious); laying out and discussing various positions one might take on the moral status of manipulation, the chapter explores the possibility that (contrary to what appears in “Manipulativeness”) “manipulative” is not best understood as a moralized term and that manipulating another person is sometimes morally unobjectionable.},
	urldate = {2023-03-01},
	booktitle = {Manipulation: {Theory} and {Practice}},
	publisher = {Oxford University Press},
	author = {Baron, Marcia},
	editor = {Coons, Christian and Weber, Michael},
	month = aug,
	year = {2014},
	doi = {10.1093/acprof:oso/9780199338207.003.0005},
	pages = {0},
}

@article{braghieri_social_2022,
	title = {Social {Media} and {Mental} {Health}},
	volume = {112},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20211218},
	doi = {10.1257/aer.20211218},
	abstract = {We provide quasi-experimental estimates of the impact of social media on mental health by leveraging a unique natural experiment: the staggered introduction of Facebook across US colleges. Our analysis couples data on student mental health around the years of Facebook's expansion with a generalized difference-in-differences empirical strategy. We find that the rollout of Facebook at a college had a negative impact on student mental health. It also increased the likelihood with which students reported experiencing impairments to academic performance due to poor mental health. Additional evidence on mechanisms suggests the results are due to Facebook fostering unfavorable social comparisons.},
	language = {en},
	number = {11},
	urldate = {2023-03-01},
	journal = {American Economic Review},
	author = {Braghieri, Luca and Levy, Ro'ee and Makarin, Alexey},
	month = nov,
	year = {2022},
	keywords = {Media, Micro-Based Behavioral Economics: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making, Health Behavior, Higher Education, Research Institutions, Entertainment},
	pages = {3660--3693},
}

@incollection{muller_survey_2014,
	address = {New York, NY},
	title = {Survey {Research} in {HCI}},
	isbn = {978-1-4939-0378-8},
	url = {https://doi.org/10.1007/978-1-4939-0378-8_10},
	abstract = {Surveys, now commonplace on the Internet, allow researchers to make inferences about an entire population by gathering information from a small subset of the larger group. Surveys can gather insights about people’s attitudes, perceptions, intents, habits, awarenesses, experiences, and characteristics, at significant moments both in time and over time. Even though they are easy to administer, there is a wide gap between quick-and-dirty surveys and surveys that are properly planned, constructed, and analyzed.},
	language = {en},
	urldate = {2023-03-01},
	booktitle = {Ways of {Knowing} in {HCI}},
	publisher = {Springer},
	author = {Müller, Hendrik and Sedley, Aaron and Ferrall-Nunge, Elizabeth},
	editor = {Olson, Judith S. and Kellogg, Wendy A.},
	year = {2014},
	doi = {10.1007/978-1-4939-0378-8_10},
	keywords = {Answer Option, Internet Search Engine, Internet Survey, Research Goal, Tablet Computer},
	pages = {229--266},
}

@article{smith_correcting_1967,
	title = {Correcting for {Social} {Desirability} {Response} {Sets} in {Opinion}-{Attitude} {Survey} {Research}},
	volume = {31},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/2746886},
	number = {1},
	urldate = {2023-03-01},
	journal = {The Public Opinion Quarterly},
	author = {Smith, David Horton},
	year = {1967},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {87--94},
}

@article{krosnick_survey_1999,
	title = {Survey {Research}},
	volume = {50},
	url = {https://doi.org/10.1146/annurev.psych.50.1.537},
	doi = {10.1146/annurev.psych.50.1.537},
	abstract = {For the first time in decades, conventional wisdom about survey methodology is being challenged on many fronts. The insights gained can not only help psychologists do their research better but also provide useful insights into the basics of social interaction and cognition. This chapter reviews some of the many recent advances in the literature, including the following: New findings challenge a long-standing prejudice against studies with low response rates; innovative techniques for pretesting questionnaires offer opportunities for improving measurement validity; surprising effects of the verbal labels put on rating scale points have been identified, suggesting optimal approaches to scale labeling; respondents interpret questions on the basis of the norms of everyday conversation, so violations of those conventions introduce error; some measurement error thought to have been attributable to social desirability response bias now appears to be due to other factors instead, thus encouraging different approaches to fixing such problems; and a new theory of satisficing in questionnaire responding offers parsimonious explanations for a range of response patterns long recognized by psycholo-gists and survey researchers but previously not well understood.},
	number = {1},
	urldate = {2023-03-01},
	journal = {Annual Review of Psychology},
	author = {Krosnick, Jon A.},
	year = {1999},
	pmid = {15012463},
	note = {\_eprint: https://doi.org/10.1146/annurev.psych.50.1.537},
	keywords = {interviewing, polls, pretesting, questionnaires, surveys},
	pages = {537--567},
}

@article{kneer_can_2021,
	title = {Can a {Robot} {Lie}? {Exploring} the {Folk} {Concept} of {Lying} as {Applied} to {Artificial} {Agents}},
	volume = {45},
	issn = {0364-0213, 1551-6709},
	shorttitle = {Can a {Robot} {Lie}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13032},
	doi = {10.1111/cogs.13032},
	abstract = {The potential capacity for robots to deceive has received considerable attention recently. Many papers explore the technical possibility for a robot to engage in deception for beneﬁcial purposes (e.g., in education or health). In this short experimental paper, I focus on a more paradigmatic case: robot lying (lying being the textbook example of deception) for nonbeneﬁcial purposes as judged from the human point of view. More precisely, I present an empirical experiment that investigates the following three questions: (a) Are ordinary people willing to ascribe deceptive intentions to artiﬁcial agents? (b) Are they as willing to judge a robot lie as a lie as they would be when human agents engage in verbal deception? (c) Do people blame a lying artiﬁcial agent to the same extent as a lying human agent? The response to all three questions is a resounding yes. This, I argue, implies that robot deception and its normative consequences deserve considerably more attention than they presently receive.},
	language = {en},
	number = {10},
	urldate = {2022-07-22},
	journal = {Cognitive Science},
	author = {Kneer, Markus},
	month = oct,
	year = {2021},
}

@misc{boine_ai-enabled_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{AI}-enabled {Manipulation} and {EU} {Law}},
	url = {https://papers.ssrn.com/abstract=4042321},
	doi = {10.2139/ssrn.4042321},
	abstract = {The European Commission, proposed three regulations containing provisions on digital manipulation. The first one, released in December 2020, is the Digital Services Act. It has a particular focus on the prevention of disinformation. The second proposed regulation that addresses digital manipulation is the Proposal for a Regulation of the European Parliament and of the Council on the transparency and targeting of political advertising, which was released in November 2021. The third proposed regulation we will address, the AI Act of April 2021, was set to adopt provisions addressing the risks of AI including the “specific risks of impersonation or deception.” A few weeks before the official release date, a draft of the proposal was leaked.  It contained ambitious provisions against AI systems defined as manipulative. However, a few weeks later, when the actual proposed regulation came out, these provisions had been suppressed. The concept of manipulation had been replaced with the deployment of “subliminal techniques beyond a person’s consciousness in order to materially distort a person’s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm.”Although there is currently a surge in the collection of biometric and personal data to infer individuals’ personalities, preferences, and emotional states to influence their behaviors, the AI Act could be interpreted to be banning only a very narrow set of manipulative AI systems, and is likely to leave out some harmful manipulative systems. In the first part of this paper, we will discuss three issues that have been overlooked in the literature on manipulation using AI systems. In the second part, we will show the limitations of the EU Commission’s ban on subliminal techniques and expose its philosophical premises and contend that the Commission’s most recent guidelines on the UCPD provides hope for future directions. In the third part, we will propose a new definition of manipulation and legal solutions to the problem of AI manipulation.},
	language = {en},
	urldate = {2023-02-28},
	author = {Boine, Claire},
	month = jun,
	year = {2021},
	keywords = {AI, AI Act, Babel technique, Cambridge Analytica, Descartes, Digital Services Act, Kant, UCPD, availability cascade, democracy, manipulation, micro-targeting, political advertising, subliminal manipulation},
}

@incollection{isaac_white_2017,
	title = {White {Lies} on {Silver} {Tongues}},
	url = {https://academic.oup.com/book/2320/chapter/142468698},
	abstract = {It is easy to see that social robots will need the ability to detect and evaluate deceptive speech; otherwise they will be vulnerable to manipulation by malevolent humans. More surprisingly, we argue that effective social robots must also be able to produce deceptive speech. Many forms of technically deceptive speech perform a positive pro-social function, and the social integration of artificial agents will be possible only if they participate in this market of constructive deceit. We demonstrate that a crucial condition for detecting and producing deceptive speech is possession of a theory of mind. Furthermore, strategic reasoning about deception requires identifying a type of goal distinguished by its priority over the norms of conversation, which we call an ulterior motive. We argue that this goal is the appropriate target for ethical evaluation, not the veridicality of speech per se. Consequently, deception-capable robots are compatible with the most prominent programs to ensure that robots behave ethically.},
	language = {en},
	urldate = {2023-02-28},
	booktitle = {Robot {Ethics} 2.0: {From} {Autonomous} {Cars} to {Artificial} {Intelligence}},
	publisher = {Oxford University Press},
	author = {Isaac, Alistair M. C. and Bridewell, Will},
	editor = {Lin, Patrick and Abney, Keith and Jenkins, Ryan},
	month = oct,
	year = {2017},
	doi = {10.1093/oso/9780190652951.003.0011},
}

@inproceedings{panisson_lies_2018,
	address = {Brasil},
	title = {Lies, {Bullshit}, and {Deception} in {Agent}-{Oriented} {Programming} {Languages}},
	abstract = {It is reasonable to assume that in the next few decades, intelligent machines might become much more proﬁcient at socialising. This implies that the AI community will face the challenges of identifying, understanding, and dealing with the diﬀerent types of social behaviours these intelligent machines could exhibit. Given these potential challenges, we aim to model in this paper three of the most studied strategic social behaviours that could be adopted by autonomous and malicious software agents. These are dishonest behaviours such as lying, bullshitting, and deceiving that autonomous agents might exhibit by taking advantage of their own reasoning and communicative capabilities. In contrast to other studies on dishonest behaviours of autonomous agents, we use an agentoriented programming language to model dishonest agents’ attitudes and to simulate social interactions between agents. Through simulation, we are able to study and propose mechanisms to identify and later to deal with such dishonest behaviours in software agents.},
	language = {en},
	booktitle = {Proceedings of the 20th {TRUST} {Workshop}},
	author = {Panisson, Alison R and McBurney, Peter and Parsons, Simon and Bordini, Rafael H},
	year = {2018},
}

@article{ashfaq_i_2020,
	title = {I, {Chatbot}: {Modeling} the determinants of users’ satisfaction and continuance intention of {AI}-powered service agents},
	volume = {54},
	issn = {0736-5853},
	shorttitle = {I, {Chatbot}},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585320301325},
	doi = {10.1016/j.tele.2020.101473},
	abstract = {Chatbots are mainly text-based conversational agents that simulate conversations with users. This study aims to investigate drivers of users’ satisfaction and continuance intention toward chatbot-based customer service. We propose an analytical framework combining the expectation-confirmation model (ECM), information system success (ISS) model, TAM, and the need for interaction with a service employee (NFI-SE). Analysis of data collected from 370 actual chatbot users reveals that information quality (IQ) and service quality (SQ) positively influence consumers’ satisfaction, and that perceived enjoyment (PE), perceived usefulness (PU), and perceived ease of use (PEOU) are significant predictors of continuance intention (CI). The need for interaction with an employee moderates the effects of PEOU and PU on satisfaction. The findings also revealed that satisfaction with chatbot e-service is a strong determinant and predictor of users’ CI toward chatbots. Thus, chatbots should enhance their information and service quality to increase users’ satisfaction. The findings imply that digital technologies services, such as chatbots, could be combined with human service employees to satisfy digital users.},
	language = {en},
	urldate = {2023-02-28},
	journal = {Telematics and Informatics},
	author = {Ashfaq, Muhammad and Yun, Jiang and Yu, Shubin and Loureiro, Sandra Maria Correia},
	month = nov,
	year = {2020},
	keywords = {Chatbots, Continuance intention, ECM, ISS model, Satisfaction, TAM, The need for interaction},
	pages = {101473},
}

@article{nicolescu_human-computer_2022,
	title = {Human-{Computer} {Interaction} in {Customer} {Service}: {The} {Experience} with {AI} {Chatbots}—{A} {Systematic} {Literature} {Review}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Human-{Computer} {Interaction} in {Customer} {Service}},
	url = {https://www.mdpi.com/2079-9292/11/10/1579},
	doi = {10.3390/electronics11101579},
	abstract = {Artificial intelligence (AI) conversational agents (CA) or chatbots represent one of the technologies that can provide automated customer service for companies, a trend encountered in recent years. Chatbot use is beneficial for companies when associated with positive customer experience. The purpose of this paper is to analyze the overall customer experience with customer service chatbots in order to identify the main influencing factors for customer experience with customer service chatbots and to identify the resulting dimensions of customer experience (such as perceptions/attitudes and feelings and also responses and behaviors). The analysis uses the systematic literature review (SLR) method and includes a sample of 40 publications that present empirical studies. The results illustrate that the main influencing factors of customer experience with chatbots are grouped in three categories: chatbot-related, customer-related, and context-related factors, where the chatbot-related factors are further categorized in: functional features of chatbots, system features of chatbots and anthropomorphic features of chatbots. The multitude of factors of customer experience result in either positive or negative perceptions/attitudes and feelings of customers. At the same time, customers respond by manifesting their intentions and/or their behaviors towards either the technology itself (chatbot usage continuation and acceptance of chatbot recommendations) or towards the company (buying and recommending products). According to empirical studies, the most influential factors when using chatbots for customer service are response relevance and problem resolution, which usually result in positive customer satisfaction, increased probability for chatbots usage continuation, product purchases, and product recommendations.},
	language = {en},
	number = {10},
	urldate = {2023-02-28},
	journal = {Electronics},
	author = {Nicolescu, Luminița and Tudorache, Monica Teodora},
	month = jan,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI chatbots, AI conversational agents, customer experience, customer service},
	pages = {1579},
}

@article{vaidyam_chatbots_2019,
	title = {Chatbots and {Conversational} {Agents} in {Mental} {Health}: {A} {Review} of the {Psychiatric} {Landscape}},
	volume = {64},
	issn = {1497-0015},
	shorttitle = {Chatbots and {Conversational} {Agents} in {Mental} {Health}},
	doi = {10.1177/0706743719828977},
	abstract = {OBJECTIVE: The aim of this review was to explore the current evidence for conversational agents or chatbots in the field of psychiatry and their role in screening, diagnosis, and treatment of mental illnesses.
METHODS: A systematic literature search in June 2018 was conducted in PubMed, EmBase, PsycINFO, Cochrane, Web of Science, and IEEE Xplore. Studies were included that involved a chatbot in a mental health setting focusing on populations with or at high risk of developing depression, anxiety, schizophrenia, bipolar, and substance abuse disorders.
RESULTS: From the selected databases, 1466 records were retrieved and 8 studies met the inclusion criteria. Two additional studies were included from reference list screening for a total of 10 included studies. Overall, potential for conversational agents in psychiatric use was reported to be high across all studies. In particular, conversational agents showed potential for benefit in psychoeducation and self-adherence. In addition, satisfaction rating of chatbots was high across all studies, suggesting that they would be an effective and enjoyable tool in psychiatric treatment.
CONCLUSION: Preliminary evidence for psychiatric use of chatbots is favourable. However, given the heterogeneity of the reviewed studies, further research with standardized outcomes reporting is required to more thoroughly examine the effectiveness of conversational agents. Regardless, early evidence shows that with the proper approach and research, the mental health field could use conversational agents in psychiatric treatment.},
	language = {eng},
	number = {7},
	journal = {Canadian Journal of Psychiatry. Revue Canadienne De Psychiatrie},
	author = {Vaidyam, Aditya Nrusimha and Wisniewski, Hannah and Halamka, John David and Kashavan, Matcheri S. and Torous, John Blake},
	month = jul,
	year = {2019},
	pmid = {30897957},
	pmcid = {PMC6610568},
	keywords = {Communication, Diagnosis, Computer-Assisted, Humans, Mental Disorders, Psychotherapy, Telemedicine, Therapy, Computer-Assisted, chatbot, conversational agent, depression, embodied conversational agent, medical informatics, mental health, psychiatry},
	pages = {456--464},
}

@article{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	journal = {arXiv preprint arXiv:2302.04761},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	year = {2023},
}

@misc{adept_act-1_2022,
	title = {{ACT}-1: {Transformer} for {Actions}},
	shorttitle = {{ACT}-1},
	url = {https://www.adept.ai/act},
	abstract = {At Adept, we are building the next frontier of models that can take actions in the digital world—that’s why we’re excited to introduce our first large model, Action Transformer (ACT-1).},
	urldate = {2023-01-28},
	author = {Adept},
	month = sep,
	year = {2022},
}

@inproceedings{zhang_effect_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Effect of confidence and explanation on accuracy and trust calibration in {AI}-assisted decision making},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3372852},
	doi = {10.1145/3351095.3372852},
	abstract = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
	month = jan,
	year = {2020},
	keywords = {confidence, decision support, explainable AI, trust},
	pages = {295--305},
}

@inproceedings{bansal_does_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Does the {Whole} {Exceed} its {Parts}? {The} {Effect} of {AI} {Explanations} on {Complementary} {Team} {Performance}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {Does the {Whole} {Exceed} its {Parts}?},
	url = {https://doi.org/10.1145/3411764.3445717},
	doi = {10.1145/3411764.3445717},
	abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
	month = may,
	year = {2021},
	keywords = {Augmented intelligence, Explainable AI, Human-AI teams},
	pages = {1--16},
}

@inproceedings{wang_are_2021,
	address = {New York, NY, USA},
	series = {{IUI} '21},
	title = {Are {Explanations} {Helpful}? {A} {Comparative} {Study} of the {Effects} of {Explanations} in {AI}-{Assisted} {Decision}-{Making}},
	isbn = {978-1-4503-8017-1},
	shorttitle = {Are {Explanations} {Helpful}?},
	url = {https://doi.org/10.1145/3397481.3450650},
	doi = {10.1145/3397481.3450650},
	abstract = {This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy—improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.},
	urldate = {2023-02-28},
	booktitle = {26th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Xinru and Yin, Ming},
	month = apr,
	year = {2021},
	keywords = {explainable AI, human-subject experiments, interpretable machine learning, trust, trust calibration},
	pages = {318--328},
}

@inproceedings{hase_evaluating_2020,
	address = {Online},
	title = {Evaluating {Explainable} {AI}: {Which} {Algorithmic} {Explanations} {Help} {Users} {Predict} {Model} {Behavior}?},
	shorttitle = {Evaluating {Explainable} {AI}},
	url = {https://aclanthology.org/2020.acl-main.491},
	doi = {10.18653/v1/2020.acl-main.491},
	abstract = {Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hase, Peter and Bansal, Mohit},
	month = jul,
	year = {2020},
	pages = {5540--5552},
}

@incollection{noggle_ethics_2022,
	edition = {Summer 2022},
	title = {The {Ethics} of {Manipulation}},
	url = {https://plato.stanford.edu/archives/sum2022/entries/ethics-manipulation/},
	abstract = {Consider this case: Tonya plans to do Y, but Irving wants herto do X instead. Irving has tried unsuccessfully to provideTonya with reasons for doing X rather than Y. IfIrving is unwilling to resort to coercion or force, he might deployany of the following tactics to try to influence Tonya’s choice.For example, Irving might …, Each of these tactics could reasonably be called manipulation. Manyalso have more specific, commonplace names, such as “guilttrip” (tactic 3), “gaslighting” (tactic 8),“peer pressure” (tactic 5), “negging” (tactic6), and “emotional blackmail” (tactic 9). Perhaps noteveryone will agree that every tactic on this list isproperly described as manipulation. And in some cases, whether thetactic seems manipulative may depend on various details not specifiedin the case as described. For example, if Y is seriouslyimmoral, then perhaps it is not manipulative for Irving to induceTonya to feel guilty about planning to do Y. It is alsopossible that we might revise our judgments about some of thesetactics in light of a fully worked out and well supported theory ofmanipulation—if we had one. Nevertheless, this list shouldprovide a reasonably good sense of what we mean by“manipulation” in ordinary discourse. It should also serveto illustrate the wide variety of tactics commonly described asmanipulation., Manipulation is often characterized as a form of influence that isneither coercion nor rational persuasion. But this characterizationimmediately raises the question: Is every form of influencethat is neither coercion nor rational persuasion a form ofmanipulation? If manipulation does not occupy the entire logical spaceof influences that are neither rational persuasion nor coercion, thenwhat distinguishes it from other forms of influence that are neithercoercion nor rational persuasion?, The term “manipulation” is commonly thought to include anelement of moral disapprobation: To say that Irving manipulated Tonyais commonly taken to be a moral criticism of Irving’s behavior.Is manipulation always immoral? Why is manipulation immoral(when it is immoral)? If manipulation is not always immoral, then whatdetermines when it is immoral?},
	urldate = {2023-02-28},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Noggle, Robert},
	editor = {Zalta, Edward N.},
	year = {2022},
}

@inproceedings{susser_invisible_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Invisible {Influence}: {Artificial} {Intelligence} and the {Ethics} of {Adaptive} {Choice} {Architectures}},
	isbn = {978-1-4503-6324-2},
	shorttitle = {Invisible {Influence}},
	url = {https://doi.org/10.1145/3306618.3314286},
	doi = {10.1145/3306618.3314286},
	abstract = {For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions--what behavioral economists call our choice architectures--are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings--the choice architectures--to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us--effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Susser, Daniel},
	month = jan,
	year = {2019},
	keywords = {accountability, choice architecture, data ethics, influence, manipulation, transparency},
	pages = {403--408},
}

@misc{dafoe_open_2020,
	title = {Open {Problems} in {Cooperative} {AI}},
	url = {http://arxiv.org/abs/2012.08630},
	doi = {10.48550/arXiv.2012.08630},
	abstract = {Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Dafoe, Allan and Hughes, Edward and Bachrach, Yoram and Collins, Tantum and McKee, Kevin R. and Leibo, Joel Z. and Larson, Kate and Graepel, Thore},
	month = dec,
	year = {2020},
	note = {arXiv:2012.08630 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@article{sunstein_manipulation_2021,
	title = {Manipulation {As} {Theft}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3880048},
	doi = {10.2139/ssrn.3880048},
	abstract = {Should there be a right not to be manipulated? What kind of right? On Kantian grounds, manipulation, lies, and paternalistic coercion are moral wrongs, and for similar reasons; they deprive people of agency, insult their dignity, and fail to respect personal autonomy. On welfarist grounds, manipulation, lies, and paternalistic coercion share a different characteristic; they displace the choices of those whose lives are directly at stake, and who are likely to have epistemic advantages, with the choices of outsiders, who are likely to lack critical information. Kantians and welfarists should be prepared to endorse a (moral) right not to be manipulated, though on very different grounds. The moral prohibition on manipulation, like the moral prohibition on lies, should run against officials and regulators, not only against private institutions. At the same time, the creation of a legal right not to be manipulated raises hard questions, in part because of definitional challenges; there is a serious risk of vagueness and a serious risk of overbreadth. (Lies, as such, are not against the law, and the same is true of unkindness, inconsiderateness, and even cruelty.) With welfarist considerations in mind, it is probably best to start by prohibiting particular practices, while emphasizing that they are forms of manipulation and may not count as fraud. The basic goal should be to build on the claim that in certain cases, manipulation is a form of theft; the law should forbid theft, whether it occurs through force, lies, or manipulation. Some manipulators are thieves.},
	language = {en},
	urldate = {2021-12-04},
	journal = {SSRN Electronic Journal},
	author = {Sunstein, Cass R.},
	year = {2021},
}

@incollection{wood_coercion_2014,
	address = {Oxford ; New York},
	title = {Coercion, {Manipulation}, {Exploitation}},
	isbn = {978-0-19-933821-4 978-0-19-933820-7},
	url = {DOI:10.1093/acprof:oso/9780199338207.003.0002},
	language = {en},
	booktitle = {Manipulation: theory and practice},
	publisher = {Oxford University Press},
	author = {Wood, Allen W.},
	year = {2014},
	keywords = {Authority, Control (Psychology), Manipulative behavior},
}

@misc{sunstein_fifty_2015,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Fifty {Shades} of {Manipulation}},
	url = {https://papers.ssrn.com/abstract=2565892},
	doi = {10.2139/ssrn.2565892},
	abstract = {A statement or action can be said to be manipulative if it does not sufficiently engage or appeal to people’s capacity for reflective and deliberative choice. One problem with manipulation, thus understood, is that it fails to respect people’s autonomy and is an affront to their dignity. Another problem is that if they are products of manipulation, people’s choices might fail to promote their own welfare, and might instead promote the welfare of the manipulator. To that extent, the central objection to manipulation is rooted in a version of Mill’s Harm Principle: People know what is in their best interests and should have a (manipulation-free) opportunity to make that decision. On welfarist grounds, the norm against manipulation can be seen as a kind of heuristic, one that generally works well, but that can also lead to serious errors, at least when the manipulator is both informed and genuinely interested in the welfare of the chooser.},
	language = {en},
	urldate = {2023-02-28},
	author = {Sunstein, Cass R.},
	month = feb,
	year = {2015},
	keywords = {behavioral economics, dignity, loss aversion, manipulation, nudge},
}

@misc{kaddour_causal_2022,
	title = {Causal {Machine} {Learning}: {A} {Survey} and {Open} {Problems}},
	shorttitle = {Causal {Machine} {Learning}},
	url = {http://arxiv.org/abs/2206.15475},
	doi = {10.48550/arXiv.2206.15475},
	abstract = {Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J. and Silva, Ricardo},
	month = jul,
	year = {2022},
	note = {arXiv:2206.15475 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Methodology},
}

@article{heinze-deml_causal_2018,
	title = {Causal {Structure} {Learning}},
	volume = {5},
	url = {https://doi.org/10.1146/annurev-statistics-031017-100630},
	doi = {10.1146/annurev-statistics-031017-100630},
	abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios.},
	number = {1},
	urldate = {2023-02-27},
	journal = {Annual Review of Statistics and Its Application},
	author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
	year = {2018},
	note = {\_eprint: https://doi.org/10.1146/annurev-statistics-031017-100630},
	keywords = {causal model, directed graphs, feedback, interventions, latent variables},
	pages = {371--391},
}

@misc{bowman_measuring_2022,
	title = {Measuring {Progress} on {Scalable} {Oversight} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2211.03540},
	doi = {10.48550/arXiv.2211.03540},
	abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Lukošiūtė, Kamilė and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2211.03540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{madsen_evaluating_2022,
	title = {Evaluating the {Faithfulness} of {Importance} {Measures} in {NLP} by {Recursively} {Masking} {Allegedly} {Important} {Tokens} and {Retraining}},
	url = {http://arxiv.org/abs/2110.08412},
	doi = {10.48550/arXiv.2110.08412},
	abstract = {To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model's logic, a property called faithfulness. To answer this question, we propose Recursive ROAR, a new faithfulness metric. This works by recursively masking allegedly important tokens and then retraining the model. The principle is that this should result in worse model performance compared to masking random tokens. The result is a performance curve given a masking-ratio. Furthermore, we propose a summarizing metric using relative area-between-curves (RACU), which allows for easy comparison across papers, models, and tasks. We evaluate 4 different importance measures on 8 different datasets, using both LSTM-attention models and RoBERTa models. We find that the faithfulness of importance measures is both model-dependent and task-dependent. This conclusion contradicts previous evaluations in both computer vision and faithfulness of attention literature.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Madsen, Andreas and Meade, Nicholas and Adlakha, Vaibhav and Reddy, Siva},
	month = oct,
	year = {2022},
	note = {arXiv:2110.08412 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{adebayo_debugging_2020,
	title = {Debugging {Tests} for {Model} {Explanations}},
	url = {http://arxiv.org/abs/2011.05429},
	doi = {10.48550/arXiv.2011.05429},
	abstract = {We investigate whether post-hoc model explanations are effective for diagnosing model errors--model debugging. In response to the challenge of explaining a model's prediction, a vast array of explanation methods have been proposed. Despite increasing use, it is unclear if they are effective. To start, we categorize {\textbackslash}textit\{bugs\}, based on their source, into:{\textasciitilde}{\textbackslash}textit\{data, model, and test-time\} contamination bugs. For several explanation methods, we assess their ability to: detect spurious correlation artifacts (data contamination), diagnose mislabeled training examples (data contamination), differentiate between a (partially) re-initialized model and a trained one (model contamination), and detect out-of-distribution inputs (test-time contamination). We find that the methods tested are able to diagnose a spurious background bug, but not conclusively identify mislabeled training examples. In addition, a class of methods, that modify the back-propagation algorithm are invariant to the higher layer parameters of a deep network; hence, ineffective for diagnosing model contamination. We complement our analysis with a human subject study, and find that subjects fail to identify defective models using attributions, but instead rely, primarily, on model predictions. Taken together, our results provide guidance for practitioners and researchers turning to explanations as tools for model debugging.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Adebayo, Julius and Muelly, Michael and Liccardi, Ilaria and Kim, Been},
	month = nov,
	year = {2020},
	note = {arXiv:2011.05429 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{adebayo_post_2022,
	title = {Post hoc {Explanations} may be {Ineffective} for {Detecting} {Unknown} {Spurious} {Correlation}},
	url = {https://openreview.net/forum?id=xNOVfCCvDpM},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Adebayo, Julius and Muelly, Michael and Abelson, Harold and Kim, Been},
	year = {2022},
}

@article{rahwan_society---loop_2017,
	title = {Society-in-the-{Loop}: {Programming} the {Algorithmic} {Social} {Contract}},
	shorttitle = {Society-in-the-{Loop}},
	url = {https://arxiv.org/abs/1707.07232v2},
	doi = {10.1007/s10676-017-9430-8},
	abstract = {Recent rapid advances in Artificial Intelligence (AI) and Machine Learning have raised many questions about the regulatory and governance mechanisms for autonomous machines. Many commentators, scholars, and policy-makers now call for ensuring that algorithms governing our lives are transparent, fair, and accountable. Here, I propose a conceptual framework for the regulation of AI and algorithmic systems. I argue that we need tools to program, debug and maintain an algorithmic social contract, a pact between various human stakeholders, mediated by machines. To achieve this, we can adapt the concept of human-in-the-loop (HITL) from the fields of modeling and simulation, and interactive machine learning. In particular, I propose an agenda I call society-in-the-loop (SITL), which combines the HITL control paradigm with mechanisms for negotiating the values of various stakeholders affected by AI systems, and monitoring compliance with the agreement. In short, `SITL = HITL + Social Contract.'},
	language = {en},
	urldate = {2021-11-08},
	author = {Rahwan, Iyad},
	month = jul,
	year = {2017},
	keywords = {To Read (5)},
}

@article{zagal_dark_nodate,
	title = {Dark {Patterns} in the {Design} of {Games}},
	abstract = {Game designers are typically regarded as advocates for players. However, a game creator’s interests may not align with the players’. We examine some of the ways in which those opposed interests can manifest in a game’s design. In particular, we examine those elements of a game’s design whose purpose can be argued as questionable and perhaps even unethical. Building upon earlier work in design patterns, we call these abstracted elements Dark Game Design Patterns. In this paper, we develop the concept of dark design patterns in games, present examples of such patterns, explore some of the subtleties involved in identifying them, and provide questions that can be asked to help guide in the specification and identification of future Dark Patterns. Our goal is not to criticize creators but rather to contribute to an ongoing discussion regarding the values in games and the role that designers and creators have in this process.},
	language = {en},
	author = {Zagal, José P and Björk, Staffan and Lewis, Chris},
	keywords = {Read},
}

@inproceedings{gray_dark_2018,
	address = {Montreal QC Canada},
	title = {The {Dark} ({Patterns}) {Side} of {UX} {Design}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174108},
	doi = {10.1145/3173574.3174108},
	abstract = {Interest in critical scholarship that engages with the complexity of user experience (UX) practice is rapidly expanding, yet the vocabulary for describing and assessing criticality in practice is currently lacking. In this paper, we outline and explore the limits of a speciﬁc ethical phenomenon known as "dark patterns," where user value is supplanted in favor of shareholder value. We assembled a corpus of examples of practitioner-identiﬁed dark patterns and performed a content analysis to determine the ethical concerns contained in these examples. This analysis revealed a wide range of ethical issues raised by practitioners that were frequently conﬂated under the umbrella term of dark patterns, while also underscoring a shared concern that UX designers could easily become complicit in manipulative or unreasonably persuasive practices. We conclude with implications for the education and practice of UX designers, and a proposal for broadening research on the ethics of user experience.},
	language = {en},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Gray, Colin M. and Kou, Yubo and Battles, Bryan and Hoggatt, Joseph and Toombs, Austin L.},
	month = apr,
	year = {2018},
	keywords = {Read},
	pages = {1--14},
}

@article{armstrong_motivated_2015,
	title = {Motivated {Value} {Selection} for {Artiﬁcial} {Agents}},
	abstract = {Coding values (or preferences) directly into an artiﬁcial agent is a very challenging task, while value selection (or valuelearning, or value-loading) allows agents to learn values from their programmers, other humans or their environments in an interactive way. However, there is a conﬂict between agents learning their future values and following their current values, which motivates agents to manipulate the value selection process. This paper establishes the conditions under which motivated value selection is an issue for some types of agents, and presents an example of an ‘indifferent’ agent that avoids it entirely. This poses and solves an issue which has not to the author’s knowledge been formally addressed in the literature.},
	language = {en},
	author = {Armstrong, Stuart},
	year = {2015},
	keywords = {Read},
}

@article{mathur_dark_2019,
	title = {Dark {Patterns} at {Scale}: {Findings} from a {Crawl} of {11K} {Shopping} {Websites}},
	volume = {3},
	issn = {2573-0142},
	shorttitle = {Dark {Patterns} at {Scale}},
	url = {https://dl.acm.org/doi/10.1145/3359183},
	doi = {10.1145/3359183},
	abstract = {Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing {\textasciitilde}53K product pages from {\textasciitilde}11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns.},
	language = {en},
	number = {CSCW},
	urldate = {2023-02-26},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Mathur, Arunesh and Acar, Gunes and Friedman, Michael J. and Lucherini, Eli and Mayer, Jonathan and Chetty, Marshini and Narayanan, Arvind},
	month = nov,
	year = {2019},
	keywords = {Read},
	pages = {1--32},
}

@article{luguri_shining_2021,
	title = {Shining a {Light} on {Dark} {Patterns}},
	volume = {13},
	issn = {2161-7201, 1946-5319},
	url = {https://academic.oup.com/jla/article/13/1/43/6180579},
	doi = {10.1093/jla/laaa006},
	abstract = {Abstract
            Dark patterns are user interfaces whose designers knowingly confuse users, make it difficult for users to express their actual preferences, or manipulate users into taking certain actions. They typically exploit cognitive biases and prompt online consumers to purchase goods and services that they do not want or to reveal personal information they would prefer not to disclose. This article provides the first public evidence of the power of dark patterns. It discusses the results of the authors’ two large-scale experiments in which representative samples of American consumers were exposed to dark patterns. In the first study, users exposed to mild dark patterns were more than twice as likely to sign up for a dubious service as those assigned to the control group, and users in the aggressive dark pattern condition were almost four times as likely to subscribe. Moreover, whereas aggressive dark patterns generated a powerful backlash among consumers, mild dark patterns did not. Less educated subjects were significantly more susceptible to mild dark patterns than their well-educated counterparts. The second study identified the dark patterns that seem most likely to nudge consumers into making decisions that they are likely to regret or misunderstand. Hidden information, trick question, and obstruction strategies were particularly likely to manipulate consumers successfully. Other strategies employing loaded language or generating bandwagon effects worked moderately well, while still others such as “must act now” messages did not make consumers more likely to purchase a costly service. Our second study also replicated a striking result in the first experiment, which is that where dark patterns were employed the cost of the service offered to consumers became immaterial. Decision architecture, not price, drove consumer purchasing decisions. The article concludes by examining legal frameworks for addressing dark patterns. Many dark patterns appear to violate federal and state laws restricting the use of unfair and deceptive practices in trade. Moreover, in those instances where consumers enter into contracts after being exposed to dark patterns, their consent could be deemed voidable under contract law principles. The article also proposes that dark pattern audits become part of the Federal Trade Commission (FTC)’s consent decree process. Dark patterns are presumably proliferating because firms’ proprietary A-B testing has revealed them to be profit maximizing. We show how similar A-B testing can be used to identify those dark patterns that are so manipulative that they ought to be deemed unlawful.},
	language = {en},
	number = {1},
	urldate = {2023-02-26},
	journal = {Journal of Legal Analysis},
	author = {Luguri, Jamie and Strahilevitz, Lior Jacob},
	month = mar,
	year = {2021},
	keywords = {Read},
	pages = {43--109},
}

@article{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artiﬁcial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, deﬁned as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of ﬁve practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (“avoiding side eﬀects” and “avoiding reward hacking”), an objective function that is too expensive to evaluate frequently (“scalable supervision”), or undesirable behavior during the learning process (“safe exploration” and “distributional shift”). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	language = {en},
	urldate = {2020-04-03},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read},
}

@article{lindner_challenges_2021,
	title = {Challenges for {Using} {Impact} {Regularizers} to {Avoid} {Negative} {Side} {Effects}},
	url = {http://arxiv.org/abs/2101.12509},
	abstract = {Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.},
	urldate = {2021-03-09},
	journal = {arXiv:2101.12509 [cs]},
	author = {Lindner, David and Matoba, Kyle and Meulemans, Alexander},
	month = feb,
	year = {2021},
	note = {arXiv: 2101.12509},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read},
}

@inproceedings{weidinger_taxonomy_2022,
	title = {Taxonomy of {Risks} posed by {Language} {Models}},
	url = {https://doi.org/10.1145%2F3531146.3533088},
	doi = {10.1145/3531146.3533088},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = jun,
	year = {2022},
}

@misc{nye_show_2021,
	title = {Show {Your} {Work}: {Scratchpads} for {Intermediate} {Computation} with {Language} {Models}},
	shorttitle = {Show {Your} {Work}},
	url = {http://arxiv.org/abs/2112.00114},
	doi = {10.48550/arXiv.2112.00114},
	abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
	month = nov,
	year = {2021},
	note = {arXiv:2112.00114 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{shah_goal_2022,
	title = {Goal {Misgeneralization}: {Why} {Correct} {Specifications} {Aren}'t {Enough} {For} {Correct} {Goals}},
	shorttitle = {Goal {Misgeneralization}},
	url = {http://arxiv.org/abs/2210.01790},
	doi = {10.48550/arXiv.2210.01790},
	abstract = {The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.},
	urldate = {2023-01-15},
	publisher = {arXiv},
	author = {Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
	month = nov,
	year = {2022},
	note = {arXiv:2210.01790 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{langosco_goal_2022,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Goal {Misgeneralization} in {Deep} {Reinforcement} {Learning}},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/langosco22a.html},
	abstract = {We study {\textless}em{\textgreater}goal misgeneralization{\textless}/em{\textgreater}, a type of out-of-distribution robustness failure in reinforcement learning (RL). Goal misgeneralization occurs when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time.We provide the first explicit empirical demonstrations of goal misgeneralization and present a partial characterization of its causes.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Langosco, Lauro Langosco Di and Koch, Jack and Sharkey, Lee D and Pfau, Jacob and Krueger, David},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	month = jul,
	year = {2022},
	pages = {12004--12019},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{ganguli_predictability_2022,
	title = {Predictability and {Surprise} in {Large} {Generative} {Models}},
	url = {https://doi.org/10.1145%2F3531146.3533229},
	doi = {10.1145/3531146.3533229},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and Hatfield-Dodds, Zac and Henighan, Tom and Johnston, Scott and Jones, Andy and Joseph, Nicholas and Kernian, Jackson and Kravec, Shauna and Mann, Ben and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Christopher and Amodei, Dario and Clark, Jack},
	month = jun,
	year = {2022},
}

@article{heckerman_decision-theoretic_1995,
	title = {Decision-{Theoretic} {Foundations} for {Causal} {Reasoning}},
	volume = {3},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/10151},
	doi = {10.1613/jair.202},
	abstract = {We present a definition of cause and effect in terms of    decision-theoretic primitives and thereby provide a principled    foundation for causal reasoning.  Our definition departs from the    traditional view of causation in that causal assertions may vary with    the set of decisions available.  We argue that this approach provides    added clarity to the notion of cause.  Also in this paper, we examine    the encoding of causal relationships in directed acyclic graphs.  We    describe a special class of influence diagrams, those in canonical    form, and show its relationship to Pearl's representation of cause and    effect.  Finally, we show how canonical form facilitates    counterfactual reasoning.},
	language = {en},
	urldate = {2023-02-25},
	journal = {Journal of Artificial Intelligence Research},
	author = {Heckerman, D. and Shachter, R.},
	month = dec,
	year = {1995},
	pages = {405--430},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=yzkSU5zdwD},
	journal = {Transactions on Machine Learning Research},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	year = {2022},
}

@incollection{mahon_definition_2016,
	edition = {Winter 2016},
	title = {The {Definition} of {Lying} and {Deception}},
	url = {https://plato.stanford.edu/archives/win2016/entries/lying-definition/},
	abstract = {Questions central to the philosophical discussion of lying to othersand other-deception (interpersonal deceiving) may be divided into twokinds. Questions of the first kind are definitional or conceptual.They include the questions of how lying is to be defined, howdeceiving is to be defined, and whether lying is always a form ofdeceiving.  Questions of the second kind are normative — moreparticularly, moral.  They include the questions of whether lying anddeceiving are either defeasibly or non-defeasibly morally wrong,whether lying is morally worse than deceiving, and whether, if lyingand deception are defeasibly morally wrong, they are merely morallyoptional on certain occasions, or are sometimes morally obligatory. Inthis entry, we only consider questions of the first kind.},
	urldate = {2023-02-25},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Mahon, James Edwin},
	editor = {Zalta, Edward N.},
	year = {2016},
	keywords = {Chisholm, Roderick, Grice, Paul, Grotius, Hugo, Kant, Immanuel, Peirce, Charles Sanders: theory of signs, Ryle, Gilbert, Williams, Bernard, assertion, self-deception, testimony: epistemological problems of},
}

@misc{perez_discovering_2022,
	title = {Discovering {Language} {Model} {Behaviors} with {Model}-{Written} {Evaluations}},
	url = {http://arxiv.org/abs/2212.09251},
	doi = {10.48550/arXiv.2212.09251},
	abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{li_faithfulness_2022,
	title = {Faithfulness in {Natural} {Language} {Generation}: {A} {Systematic} {Survey} of {Analysis}, {Evaluation} and {Optimization} {Methods}},
	shorttitle = {Faithfulness in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2203.05227},
	doi = {10.48550/arXiv.2203.05227},
	abstract = {Natural Language Generation (NLG) has made great progress in recent years due to the development of deep learning techniques such as pre-trained language models. This advancement has resulted in more fluent, coherent and even properties controllable (e.g. stylistic, sentiment, length etc.) generation, naturally leading to development in downstream tasks such as abstractive summarization, dialogue generation, machine translation, and data-to-text generation. However, the faithfulness problem that the generated text usually contains unfaithful or non-factual information has become the biggest challenge, which makes the performance of text generation unsatisfactory for practical applications in many real-world scenarios. Many studies on analysis, evaluation, and optimization methods for faithfulness problems have been proposed for various tasks, but have not been organized, compared and discussed in a combined manner. In this survey, we provide a systematic overview of the research progress on the faithfulness problem of NLG, including problem analysis, evaluation metrics and optimization methods. We organize the evaluation and optimization methods for different tasks into a unified taxonomy to facilitate comparison and learning across tasks. Several research trends are discussed further.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Li, Wei and Wu, Wenhao and Chen, Moye and Liu, Jiachen and Xiao, Xinyan and Wu, Hua},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05227 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{ji_survey_2022,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	urldate = {2023-02-25},
	journal = {ACM Computing Surveys},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
	month = nov,
	year = {2022},
	note = {Just Accepted},
	keywords = {Consistency in NLG, Extrinsic Hallucination, Factuality in NLG, Faithfulness in NLG, Hallucination, Intrinsic Hallucination},
}

@incollection{sarkadi_characterising_2021,
	address = {Cham},
	title = {Characterising {Deception} in {AI}: {A} {Survey}},
	volume = {1296},
	isbn = {978-3-030-91778-4 978-3-030-91779-1},
	shorttitle = {Characterising {Deception} in {AI}},
	url = {https://link.springer.com/10.1007/978-3-030-91779-1_1},
	abstract = {In 2000, it was predicted that artiﬁcially intelligent agents would inevitably become deceptive. Today, in a world seemingly awash with fake news and in which we hand over control of our home environments to faux-human smart devices, it is timely to review the types of deception that have actually emerged. By reference to examples from diverse branches of AI, we classify research on deception into ﬁve novel categories, which we describe according to the human characteristic with which they most closely align: imitating, obfuscating, tricking, calculating and reframing. We offer this as a way for those within AI to recognise connections across the discipline and to suggest how AI-instigated deceptions may be understood by those outside.},
	language = {en},
	urldate = {2023-02-24},
	booktitle = {Deceptive {AI}},
	publisher = {Springer International Publishing},
	author = {Masters, Peta and Smith, Wally and Sonenberg, Liz and Kirley, Michael},
	editor = {Sarkadi, Stefan and Wright, Benjamin and Masters, Peta and McBurney, Peter},
	year = {2021},
	doi = {10.1007/978-3-030-91779-1_1},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {3--16},
}

@article{kumar_realab_2020,
	title = {{REALab}: {An} {Embedded} {Perspective} on {Tampering}},
	shorttitle = {{REALab}},
	url = {http://arxiv.org/abs/2011.08820},
	abstract = {This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.},
	urldate = {2020-12-16},
	journal = {arXiv:2011.08820 [cs]},
	author = {Kumar, Ramana and Uesato, Jonathan and Ngo, Richard and Everitt, Tom and Krakovna, Victoria and Legg, Shane},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.08820},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read},
}

@article{uesato_avoiding_2020,
	title = {Avoiding {Tampering} {Incentives} in {Deep} {RL} via {Decoupled} {Approval}},
	url = {http://arxiv.org/abs/2011.08827},
	abstract = {How can we design agents that pursue a given objective when all feedback mechanisms are influenceable by the agent? Standard RL algorithms assume a secure reward function, and can thus perform poorly in settings where agents can tamper with the reward-generating mechanism. We present a principled solution to the problem of learning from influenceable feedback, which combines approval with a decoupled feedback collection procedure. For a natural class of corruption functions, decoupled approval algorithms have aligned incentives both at convergence and for their local updates. Empirically, they also scale to complex 3D environments where tampering is possible.},
	urldate = {2020-12-16},
	journal = {arXiv:2011.08827 [cs]},
	author = {Uesato, Jonathan and Kumar, Ramana and Krakovna, Victoria and Everitt, Tom and Ngo, Richard and Legg, Shane},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.08827},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Read},
}

@article{everitt_reward_2021,
	title = {Reward {Tampering} {Problems} and {Solutions} in {Reinforcement} {Learning}: {A} {Causal} {Influence} {Diagram} {Perspective}},
	shorttitle = {Reward {Tampering} {Problems} and {Solutions} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1908.04734},
	abstract = {Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.},
	urldate = {2022-05-09},
	journal = {arXiv:1908.04734 [cs]},
	author = {Everitt, Tom and Hutter, Marcus and Kumar, Ramana and Krakovna, Victoria},
	month = mar,
	year = {2021},
	note = {arXiv: 1908.04734},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{sparr_explicit_2022,
	title = {Explicit {User} {Manipulation} in {Reinforcement} {Learning} {Based} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2203.10629},
	abstract = {Recommender systems are highly prevalent in the modern world due to their value to both users and platforms and services that employ them. Generally, they can improve the user experience and help to increase satisfaction, but they do not come without risks. One such risk is that of their effect on users and their ability to play an active role in shaping user preferences. This risk is more significant for reinforcement learning based recommender systems. These are capable of learning for instance, how recommended content shown to a user today may tamper that user's preference for other content recommended in the future. Reinforcement learning based recommendation systems can thus implicitly learn to influence users if that means maximizing clicks, engagement, or consumption. On social news and media platforms, in particular, this type of behavior is cause for alarm. Social media undoubtedly plays a role in public opinion and has been shown to be a contributing factor to increased political polarization. Recommender systems on such platforms, therefore, have great potential to influence users in undesirable ways. However, it may also be possible for this form of manipulation to be used intentionally. With advancements in political opinion dynamics modeling and larger collections of user data, explicit user manipulation in which the beliefs and opinions of users are tailored towards a certain end emerges as a significant concern in reinforcement learning based recommender systems.},
	urldate = {2022-04-30},
	journal = {arXiv:2203.10629 [cs]},
	author = {Sparr, Matthew},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.10629},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{kleinberg_challenge_2022,
	title = {The {Challenge} of {Understanding} {What} {Users} {Want}: {Inconsistent} {Preferences} and {Engagement} {Optimization}},
	shorttitle = {The {Challenge} of {Understanding} {What} {Users} {Want}},
	url = {http://arxiv.org/abs/2202.11776},
	abstract = {Online platforms have a wealth of data, run countless experiments and use industrial-scale algorithms to optimize user experience. Despite this, many users seem to regret the time they spend on these platforms. One possible explanation is that incentives are misaligned: platforms are not optimizing for user happiness. We suggest the problem runs deeper, transcending the specific incentives of any particular platform, and instead stems from a mistaken foundational assumption. To understand what users want, platforms look at what users do. This is a kind of revealed-preference assumption that is ubiquitous in user models. Yet research has demonstrated, and personal experience affirms, that we often make choices in the moment that are inconsistent with what we actually want: we can choose mindlessly or myopically, behaviors that feel entirely familiar on online platforms. In this work, we develop a model of media consumption where users have inconsistent preferences. We consider what happens when a platform that simply wants to maximize user utility is only able to observe behavioral data in the form of user engagement. Our model produces phenomena related to overconsumption that are familiar from everyday experience, but difficult to capture in traditional user interaction models. A key ingredient is a formulation for how platforms determine what to show users: they optimize over a large set of potential content (the content manifold) parametrized by underlying features of the content. We show how the relationship between engagement and utility depends on the structure of the content manifold, characterizing when engagement optimization leads to good utility outcomes. By linking these effects to abstractions of platform design choices, our model thus creates a theoretical framework and vocabulary in which to explore interactions between design, behavioral science, and social media.},
	urldate = {2022-03-28},
	journal = {arXiv:2202.11776 [cs]},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.11776},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Computers and Society, Computer Science - Social and Information Networks, To Read (5)},
}

@techreport{hazrati_choice_2022,
	type = {preprint},
	title = {Choice {Models} and {Recommender} {Systems} {Effects} on {Users}’ {Choices}},
	url = {https://www.researchsquare.com/article/rs-1497834/v1},
	abstract = {Abstract
          Nowadays, the users of a web platform, such as a video-on-demand service or an eCommerce site, are routinely using the platform's Recommender System (RS) when choosing which item to consume or buy (e.g., movies or books). It is therefore important to understand how exposure to recommendations can influence the users' choices, particularly the quality and distribution of the chosen items. However, users, even in the presence of the same RS, may show diverse and even atypical choice behaviours, which are independent of the RS; they may have the preference for choosing more popular or recent items. The effect of these behaviours on the collective evolution of the choices and the performance of the RS is not well understood yet. In fact, in previous analyses, the users were supposed to only choose among the top recommendations, without any further discrimination. Hence, we first perform a correlation analysis, in some choices data sets, revealing that three kinds of choice behaviours, namely the tendency to choose popular, recent, and highly rated items, is actually observable in large percentages of the users. Then we investigate how these choice behaviours, implemented as algorithmic Choice Models (Popularity-CM, Age-CM and Rating-CM), can influence the overall choice distribution and the performance of the RS.
With the aim of understanding such relationships and consequences, we have designed a simulation framework where the considered choice models (CMs) are adopted to simulate users' choices when they are exposed to recommendations from alternative RSs. We found that, 1) the choices' distribution of a user population is significantly influenced not only by the RS, but also substantially by the prevalent choice model of the population, 2) some biases of the RS are translated to the choices whatever is the CM, and 3) some important biases of the RS, in how they influence users' choice distribution, depend also on the choice model that the users adopt. The study contributes to the start of a new line of research where the impact of recommendation technologies can be studied with respect to alternative decision making approaches, which are actually followed by real users. Additionally, the simulation approach can help other researchers and practitioners to investigate the effect of deploying an RS when a certain CM is identified in a population of users.},
	language = {en},
	urldate = {2022-04-21},
	institution = {In Review},
	author = {Hazrati, Naieme and Ricci, Francesco},
	month = mar,
	year = {2022},
	doi = {10.21203/rs.3.rs-1497834/v1},
}

@article{carey_incentives_2020,
	title = {The {Incentives} that {Shape} {Behaviour}},
	url = {http://arxiv.org/abs/2001.07118},
	abstract = {Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.},
	urldate = {2020-12-16},
	journal = {arXiv:2001.07118 [cs]},
	author = {Carey, Ryan and Langlois, Eric and Everitt, Tom and Legg, Shane},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.07118},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.6, I.2.8, To Read (5)},
}

@book{border_fixed_1985,
	title = {Fixed point theorems with applications to economics and game theory},
	publisher = {Cambridge University Press},
	author = {Border, Kim C},
	year = {1985},
}

@misc{noauthor_strategic-perceptron_nodate,
	title = {strategic-perceptron},
	url = {https://www.overleaf.com/project/60f89ab6e8fc0745a2e35d9b},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2021-07-23},
}

@article{wagner_survey_1977,
	title = {Survey of {Measurable} {Selection} {Theorems}},
	volume = {15},
	issn = {0363-0129},
	url = {https://epubs.siam.org/doi/abs/10.1137/0315056},
	doi = {10.1137/0315056},
	abstract = {Suppose \$(T,{\textbackslash}mathcal\{M\})\$ is a measurable space, X is a topological space, and \${\textbackslash}emptyset  {\textbackslash}ne F(t) {\textbackslash}subset X\$ for \$t {\textbackslash}in T\$. Denote \$\{{\textbackslash}operatorname \{Gr\}\}F = {\textbackslash}\{ (t,x):x {\textbackslash}in F(t){\textbackslash}\} \$. The problem surveyed (reviewing work of others) is that of existence off: \$f:T {\textbackslash}to X\$ such that \$f(t) {\textbackslash}in F(t)\$ for \$t {\textbackslash}in T\$ and \$f{\textasciicircum}\{ - 1\} (U) {\textbackslash}in {\textbackslash}mathcal\{M\}\$ for open \$U {\textbackslash}subset X\$. The principal conditions that yield such f are (i) X is Polish, each \$F(t)\$ is closed, and \${\textbackslash}\{ t:F(t) {\textbackslash}cap U {\textbackslash}ne {\textbackslash}emptyset {\textbackslash}\}  {\textbackslash}in {\textbackslash}mathcal\{M\}\$ a .tit whenever \$U {\textbackslash}subset X\$ is open (Kuratowski and Ryll-Nardzewski and, under stronger assumption, Castaing), or (ii) T is a Hausdorff space, \$\{{\textbackslash}operatorname \{Gr\}\}F\$ is a continuous image of a Polish space, and M is the \${\textbackslash}sigma \$-algebra of sets measurable with respect to an outer measure, among which are the open sets of T (primarily von Neumann). The latter result follows from the former by lifting F in a natural way to a map into the closed sets of a Polish space. This procedure leads to the theory of set-valued functions of Suslin type (Leese), which extends the , result (i) to comprehend a considerable portion of the results on the problem surveyed. Among the topics addressed, measurable implicit functions and the case where X is a linear space and each \$F(t)\$ is convex and compact are particularly important to control theory, for example. With \$T = X = [0,1]\$ and \$\{{\textbackslash}operatorname \{Gr\}\}F\$ Borel, an elegant partition of \$\{{\textbackslash}operatorname \{Gr\}\}F\$ into Lebesgue measurable maps from T to X, parameterized by Borel functions, has been found (Wesley) via Cohen forcing methods. Other topics discussed include pointwise optimal selections, selections of partitions, uniformization, non-\${\textbackslash}sigma \$-algebras in place of \${\textbackslash}mathcal\{M\}\$, Lusin measurability, and set-valued measures. Substantial historical comments and an extensive bibliography are included. (See addenda (i)–(iii).)},
	number = {5},
	urldate = {2021-07-06},
	journal = {SIAM Journal on Control and Optimization},
	author = {Wagner, Daniel H.},
	month = aug,
	year = {1977},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {859--903},
}

@article{calvert_value_1985,
	title = {The {Value} of {Biased} {Information}: {A} {Rational} {Choice} {Model} of {Political} {Advice}},
	volume = {47},
	issn = {0022-3816},
	shorttitle = {The {Value} of {Biased} {Information}},
	url = {https://www.journals.uchicago.edu/doi/abs/10.2307/2130895},
	doi = {10.2307/2130895},
	abstract = {Typically, political decision making involves the concomitant problem of deciding how to use advice. Advice can reduce uncertainty about outcomes, but it is often costly to obtain and assimilate, and is itself subject to uncertainty and error. This paper explores how a rational decision maker uses imperfect advice. Using only the assumption of utility maximization, along with a specification of exactly how knowledge and advice are "imperfect," it is possible to derive some of the initial assumptions of cognitive and bounded-rationality models. Also changes in the decision-making environment can be connected to changes in how advice is used, thereby providing theoretical predictions about political behavior. In particular it is shown here that, under certain reasonable circumstances, the rational decision maker should engage in selective exposure or "bolstering." These results do not depend upon any cost advantage or inherent value in biased advice.},
	number = {2},
	urldate = {2021-05-19},
	journal = {The Journal of Politics},
	author = {Calvert, Randall L.},
	month = jun,
	year = {1985},
	note = {Publisher: The University of Chicago Press},
	pages = {530--555},
}

@article{johnson_effect_1993,
	title = {Effect of {Flavor} and {Macronutrient} {Composition} of {Food} {Servings} on {Liking}, {Hunger} and {Subsequent} {Intake}},
	volume = {21},
	issn = {0195-6663},
	url = {https://www.sciencedirect.com/science/article/pii/S0195666383710342},
	doi = {10.1006/appe.1993.1034},
	abstract = {The effects of consuming foods with different macronutrient compositions and flavors on hedonic changes and development of satiety were investigated. Subjects rated their hunger and liking of a set of foods (rating set) before and after eating a serving (preload) of one of the foods in the rating set. The liking of the preload foods dropped more than the liking of the uneaten foods. Foods having the same flavor as the preload generally dropped more in liking than foods having similar macronutrients. The drops in liking increased with the caloric content of the preload but were unrelated to specific macronutrients. Less weight and calories of food were eaten after the high calorie preloads. Eating the high protein or the high-carbohydrate preload decreased hunger more than eating the high-fat food. Eating a high-protein preload decreased the weight of food eaten more than eating a high-fat or a high-carbohydrate preload and decreased total caloric intake more than eating a high-fat preload. However, macronutrient intake was not differentially affected by the macronutrient composition of a preload. Sensory-specific satiety appears to be more related to the sensory characteristics of a food than to the macronutrient composition of a food.},
	language = {en},
	number = {1},
	urldate = {2021-05-12},
	journal = {Appetite},
	author = {Johnson, Jill and Vickers, Zata},
	month = aug,
	year = {1993},
	pages = {25--39},
}

@article{park_nature_1993,
	title = {The {Nature} of {Relevance} in {Information} {Retrieval}: {An} {Empirical} {Study}},
	volume = {63},
	issn = {0024-2519},
	shorttitle = {The {Nature} of {Relevance} in {Information} {Retrieval}},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/602592},
	doi = {10.1086/602592},
	abstract = {Experimental research in information retrieval (IR) depends on the idea of relevance. Because of its key role in IR, recent questions about relevance have raised issues of methodological concern and have shaken the philosophical foundations of IR theory development. Despite an existing set of theoretical definitions of this concept, our understanding of relevance from users' perspectives is still limited. Using naturalistic inquiry methodology, this article reports an empirical study of user-based relevance interpretations. A model is presented that reflects the nature of the thought processes of users who are evaluating bibliographic citations produced by a document retrieval system. Three major categories of variables affecting relevance assessments-internal context, external context, and problem context-are identified and described. Users' relevance assessments involve multiple layers of interpretations that are derived from individuals' experiences, perceptions, and private knowledge related to the particular information problems at hand.},
	number = {3},
	urldate = {2021-05-14},
	journal = {The Library Quarterly},
	author = {Park, Taemin Kim},
	month = jul,
	year = {1993},
	note = {Publisher: The University of Chicago Press},
	pages = {318--351},
}

@article{foster_calibrated_1997,
	title = {Calibrated {Learning} and {Correlated} {Equilibrium}},
	volume = {21},
	issn = {0899-8256},
	url = {https://www.sciencedirect.com/science/article/pii/S0899825697905959},
	doi = {10.1006/game.1997.0595},
	abstract = {Suppose two players repeatedly meet each other to play a game where 1.each uses a learning rule with the property that it is a calibrated forecast of the other's plays, and 2.each plays a myopic best response to this forecast distribution. Then, the limit points of the sequence of plays are correlated equilibria. In fact, for each correlated equilibrium there is some calibrated learning rule that the players can use which results in their playing this correlated equilibrium in the limit. Thus, the statistical concept of a calibration is strongly related to the game theoretic concept of correlated equilibrium.Journal of Economic LiteratureClassification Numbers: C72,D83,C44.},
	language = {en},
	number = {1},
	urldate = {2021-07-20},
	journal = {Games and Economic Behavior},
	author = {Foster, Dean P. and Vohra, Rakesh V.},
	month = oct,
	year = {1997},
	pages = {40--55},
}

@article{freund_large_1999,
	title = {Large {Margin} {Classification} {Using} the {Perceptron} {Algorithm}},
	volume = {37},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007662407062},
	doi = {10.1023/A:1007662407062},
	abstract = {We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method. Like Vapnik's maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort.},
	language = {en},
	number = {3},
	urldate = {2021-06-08},
	journal = {Machine Learning},
	author = {Freund, Yoav and Schapire, Robert E.},
	month = dec,
	year = {1999},
	pages = {277--296},
}

@article{liang_personalized_2006,
	title = {Personalized {Content} {Recommendation} and {User} {Satisfaction}: {Theoretical} {Synthesis} and {Empirical} {Findings}},
	volume = {23},
	issn = {0742-1222},
	shorttitle = {Personalized {Content} {Recommendation} and {User} {Satisfaction}},
	url = {https://doi.org/10.2753/MIS0742-1222230303},
	doi = {10.2753/MIS0742-1222230303},
	abstract = {Personalized services are increasingly popular in the Internet world. This study identifies theories related to the use of personalized content services and their effect on user satisfaction. Three major theories have been identified—information overload, uses and gratifications, and user involvement. The information overload theory implies that user satisfaction increases when the recommended content fits user interests (i.e., the recommendation accuracy increases). The uses and gratifications theory indicates that motivations for information access affect user satisfaction. The user involvement theory implies that users prefer content recommended by a process in which they have explicit involvement. In this research, a research model was proposed to integrate these theories and two experiments were conducted to examine the theoretical relationships. Our findings indicate that information overload and uses and gratifications are two major theories for explaining user satisfaction with personalized services. Personalized services can reduce information overload and, hence, increase user satisfaction, but their effects may be moderated by the motivation for information access. The effect is stronger for users whose motivation is in searching for a specific target. This implies that content recommendation would be more useful for knowledge management systems, where users are often looking for specific knowledge, rather than for general purpose Web sites, whose customers often come for scanning. Explicit user involvement in the personalization process may affect a user's perception of customization, but has no significant effect on overall satisfaction.},
	number = {3},
	urldate = {2021-05-12},
	journal = {Journal of Management Information Systems},
	author = {Liang, Ting-Peng and Lai, Hung-Jen and Ku, Yi-Cheng},
	month = dec,
	year = {2006},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.2753/MIS0742-1222230303},
	keywords = {content recommendation, personalization, recommendation systems, user satisfaction},
	pages = {45--70},
}

@article{steinwart_how_2007,
	title = {How to {Compare} {Different} {Loss} {Functions} and {Their} {Risks}},
	volume = {26},
	issn = {1432-0940},
	url = {https://doi.org/10.1007/s00365-006-0662-3},
	doi = {10.1007/s00365-006-0662-3},
	abstract = {Many learning problems are described by a risk functional which in turn is defined by a loss function, anda straightforward and widely known approach to learn such problems is to minimize a (modified)empirical version of this risk functional. However, in many cases this approach suffers from substantial  problems such as computational requirements in classification or robustness concerns in regression. In order to  resolve these issues many successful learning algorithms try to minimize a (modified) empirical risk of a surrogate loss function, instead. Of course, such a surrogate loss must be "reasonably related" to the original loss function since otherwise this approach cannot work well. For classification good surrogate loss functions have been recently identified, and the relationship between the excess classification risk and the excess risk of these surrogate loss functions has been exactly described. However, beyond the classification problem little is known on good surrogate loss functions up to now. In this work we establish a general theory that provides powerful tools for comparing excess risks of different loss functions. We then apply this theory to several learning problems including (cost-sensitive) classification, regression, density estimation, and density level detection.},
	language = {en},
	number = {2},
	urldate = {2021-06-29},
	journal = {Constructive Approximation},
	author = {Steinwart, Ingo},
	month = aug,
	year = {2007},
	pages = {225--287},
}

@inproceedings{bruckner_nash_2009,
	title = {Nash {Equilibria} of {Static} {Prediction} {Games}},
	volume = {22},
	url = {https://proceedings.neurips.cc/paper/2009/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brückner, Michael and Scheffer, Tobias},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
	year = {2009},
}

@incollection{celma_long_2010,
	address = {Berlin, Heidelberg},
	title = {The {Long} {Tail} in {Recommender} {Systems}},
	isbn = {978-3-642-13287-2},
	url = {https://doi.org/10.1007/978-3-642-13287-2_4},
	abstract = {The Long Tail is composed of a small number of popular items, the well-known hits, and the rest are located in the heavy tail, those not sell that well. The Long Tail offers the possibility to explore and discover—using automatic tools; such as recommenders or personalised filters—vast amounts of data. Until now, the world was ruled by the Hit or Miss categorisation, due in part to the shelf space limitation of the brick-and-mortar stores. A world where a music band could only succeed selling millions of albums, and touring worldwide.},
	language = {en},
	urldate = {2021-05-14},
	booktitle = {Music {Recommendation} and {Discovery}: {The} {Long} {Tail}, {Long} {Fail}, and {Long} {Play} in the {Digital} {Music} {Space}},
	publisher = {Springer},
	author = {Celma, Òscar},
	year = {2010},
	doi = {10.1007/978-3-642-13287-2_4},
	keywords = {Artist Popularity, Online Market, Popular Item, Recommender System, Tail Distribution},
	pages = {87--107},
}

@book{bauschke_convex_2011,
	title = {Convex {Analysis} and {Monotone} {Operator} {Theory} in {Hilbert} {Spaces}},
	volume = {408},
	publisher = {Springer},
	author = {Bauschke, Heinz H and Combettes, Patrick L and {others}},
	year = {2011},
}

@article{nyhan_when_2010,
	title = {When {Corrections} {Fail}: {The} {Persistence} of {Political} {Misperceptions}},
	volume = {32},
	issn = {1573-6687},
	shorttitle = {When {Corrections} {Fail}},
	url = {https://doi.org/10.1007/s11109-010-9112-2},
	doi = {10.1007/s11109-010-9112-2},
	abstract = {An extensive literature addresses citizen ignorance, but very little research focuses on misperceptions. Can these false or unsubstantiated beliefs about politics be corrected? Previous studies have not tested the efficacy of corrections in a realistic format. We conducted four experiments in which subjects read mock news articles that included either a misleading claim from a politician, or a misleading claim and a correction. Results indicate that corrections frequently fail to reduce misperceptions among the targeted ideological group. We also document several instances of a “backfire effect” in which corrections actually increase misperceptions among the group in question.},
	language = {en},
	number = {2},
	urldate = {2021-05-19},
	journal = {Political Behavior},
	author = {Nyhan, Brendan and Reifler, Jason},
	month = jun,
	year = {2010},
	pages = {303--330},
}

@article{roemer_kantian_2010,
	title = {Kantian {Equilibrium}},
	volume = {112},
	issn = {0347-0520},
	url = {https://www.jstor.org/stable/40587794},
	abstract = {Consider a game whose strategies are "contributions". A strategy profile is a Kantian equilibrium if no player would like all players to alter their contributions by the same multiplicative factor. Kantian equilibria are Pareto efficient. We characterize the allocation rules on several domains of environments that can be implemented as Kantian equilibria. The concept unifies the proportional solution on production economies and the linear cost-share equilibrium on public-good economies. We study Kantian equilibrium in the prisoner's dilemma, in a voting problem, and in a political economy where redistribution is the issue. The Kantian dictum engenders considerable but not unqualified cooperation.},
	number = {1},
	urldate = {2021-06-08},
	journal = {The Scandinavian Journal of Economics},
	author = {Roemer, John E.},
	year = {2010},
	note = {Publisher: [Wiley, The Scandinavian Journal of Economics]},
	pages = {1--24},
}

@inproceedings{halkidi_game_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Game} {Theoretic} {Framework} for {Data} {Privacy} {Preservation} in {Recommender} {Systems}},
	isbn = {978-3-642-23780-5},
	doi = {10.1007/978-3-642-23780-5_50},
	abstract = {We address the fundamental tradeoff between privacy preservation and high-quality recommendation stemming from a third party. Multiple users submit their ratings to a third party about items they have viewed. The third party aggregates the ratings and generates personalized recommendations for each user. The quality of recommendations for each user depends on submitted rating profiles from all users, including the user to which the recommendation is destined. Each user would like to declare a rating profile so as to preserve data privacy as much as possible, while not causing deterioration in the quality of the recommendation he would get, compared to the one he would get if he revealed his true private profile.We employ game theory to model and study the interaction of users and we derive conditions and expressions for the Nash Equilibrium Point (NEP). This consists of the rating strategy of each user, such that no user can benefit in terms of improving its privacy by unilaterally deviating from that point. User strategies converge to the NEP after an iterative best-response strategy update. For a hybrid recommendation system, we find that the NEP strategy for each user in terms of privacy preservation is to declare false rating only for one item, the one that is highly ranked in his private profile and less correlated with items for which he anticipates recommendation. We also present various modes of cooperation by which users can mutually benefit.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Halkidi, Maria and Koutsopoulos, Iordanis},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	keywords = {game theory, privacy preservation, recommendation systems},
	pages = {629--644},
}

@article{korzhyk_stackelberg_2011,
	title = {Stackelberg vs. {Nash} in security games: {An} extended investigation of interchangeability, equivalence, and uniqueness},
	volume = {41},
	journal = {Journal of Artificial Intelligence Research},
	author = {Korzhyk, Dmytro and Yin, Zhengyu and Kiekintveld, Christopher and Conitzer, Vincent and Tambe, Milind},
	year = {2011},
	pages = {297--327},
}

@inproceedings{pires_cost-sensitive_2013,
	title = {Cost-sensitive {Multiclass} {Classification} {Risk} {Bounds}},
	url = {http://proceedings.mlr.press/v28/avilapires13.html},
	abstract = {A commonly used approach to multiclass classification is to replace the 0-1 loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncov...},
	language = {en},
	urldate = {2021-06-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pires, Bernardo Ávila and Szepesvari, Csaba and Ghavamzadeh, Mohammad},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1391--1399},
}

@techreport{pew_further_2012,
	title = {Further {Decline} in {Credibility} {Ratings} for {Most} {News} {Organizations}},
	url = {https://www.pewresearch.org/politics/2012/08/16/further-decline-in-credibility-ratings-for-most-news-organizations/},
	language = {en-US},
	urldate = {2021-05-20},
	institution = {The Pew Research Center},
	author = {Pew, Research Center},
	year = {2012},
}

@inproceedings{long_consistency_2013,
	title = {Consistency versus {Realizable} {H}-{Consistency} for {Multiclass} {Classification}},
	url = {http://proceedings.mlr.press/v28/long13.html},
	abstract = {A consistent loss function for multiclass classification is one such  that for any source of labeled examples, any tuple  of scoring functions that  minimizes the expected loss will have classifica...},
	language = {en},
	urldate = {2021-07-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Long, Phil and Servedio, Rocco},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {801--809},
}

@article{kremer_implementing_2014,
	title = {Implementing the "{Wisdom} of the {Crowd}"},
	volume = {122},
	url = {https://econpapers.repec.org/article/ucpjpolec/doi_3a10.1086_2f676597.htm},
	abstract = {We study a novel mechanism design model in which agents each arrive sequentially and choose one action from a set of actions with unknown rewards. The information revealed by the principal affects the incentives of the agents to explore and generate new information. We characterize the optimal disclosure policy of a planner whose goal is to maximize social welfare. One interpretation of our result is the implementation of what is known as the "wisdom of the crowd." This topic has become increasingly relevant with the rapid spread of the Internet over the past decade.},
	number = {5},
	urldate = {2021-05-14},
	journal = {Journal of Political Economy},
	author = {Kremer, Ilan and Mansour, Yishay and Perry, Motty},
	year = {2014},
	note = {Publisher: University of Chicago Press},
	pages = {988 -- 1012},
}

@incollection{gentzkow_media_2015,
	title = {Media {Bias} in the {Marketplace}: {Theory}},
	volume = {1B},
	booktitle = {Handbook of {Media} {Economics}},
	publisher = {Elsevier},
	author = {Gentzkow, Matthew and Shapiro, Jesse M. and Stone, Daniel F.},
	year = {2015},
	pages = {623--645},
}

@inproceedings{mansour_bayesian_2015,
	address = {New York, NY, USA},
	series = {{EC} '15},
	title = {Bayesian {Incentive}-{Compatible} {Bandit} {Exploration}},
	isbn = {978-1-4503-3410-5},
	url = {https://doi.org/10.1145/2764468.2764508},
	doi = {10.1145/2764468.2764508},
	abstract = {Individual decision-makers consume information revealed by the previous decision makers, and produce information that may help in future decision makers. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as elsewhere, such as medical decisions. Each decision maker when required to select an action, would individually prefer to exploit, select the highest expected reward action conditional on her information. At the same time, each decision maker would prefer previous decision makers to explore, producing information about the rewards of various actions. A social planner, by means of carefully designed information disclosure, can incentivize the agents to balance the exploration and exploitation, and maximize social welfare. We formulate this problem as a multi-arm bandit problem (and various generalizations thereof) under incentive-compatibility constraints induced by agents' Bayesian priors. We design an incentive-compatible bandit algorithm for the social planner with asymptotically optimal regret. Further, we provide a black-box reduction from an arbitrary multi-arm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit settings, even ones that incorporate contexts and arbitrary partial feedback.},
	urldate = {2021-05-14},
	booktitle = {Proceedings of the {Sixteenth} {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Mansour, Yishay and Slivkins, Aleksandrs and Syrgkanis, Vasilis},
	month = jun,
	year = {2015},
	keywords = {Bayesian incentive-compatibility, mechanism design, multi-armed bandits, regret},
	pages = {565--582},
}

@inproceedings{dai_recurrent_2016,
	address = {New York, NY, USA},
	series = {{DLRS} 2016},
	title = {Recurrent {Coevolutionary} {Latent} {Feature} {Processes} for {Continuous}-{Time} {Recommendation}},
	isbn = {978-1-4503-4795-2},
	url = {https://doi.org/10.1145/2988450.2988451},
	doi = {10.1145/2988450.2988451},
	abstract = {Matching users to the right items at the right time is a fundamental task in recommender systems. As users interact with different items over time, users' and items' feature may drift, evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature. We use a recurrent neural network to automatically learn a representation of influences from drift, evolution and co-evolution of user and item features. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.},
	urldate = {2021-05-20},
	booktitle = {Proceedings of the 1st {Workshop} on {Deep} {Learning} for {Recommender} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dai, Hanjun and Wang, Yichen and Trivedi, Rakshit and Song, Le},
	month = sep,
	year = {2016},
	pages = {29--34},
}

@inproceedings{mao_what_2015,
	title = {What {Drives} {Consumers} to {Click} on {Social} {Media} {Ads}? {The} {Roles} of {Content}, {Media}, and {Individual} {Factors}},
	shorttitle = {What {Drives} {Consumers} to {Click} on {Social} {Media} {Ads}?},
	doi = {10.1109/HICSS.2015.410},
	abstract = {Social media has become an ever-expanding realm as more and more consumers are spending tremendous amount of time on it. Businesses are taking advantage of this channel to promote their products and services through social media advertising. In particular, display ads have a prominent presence accompanying social media feeds. This study aims to develop an understanding of the multi-faceted factors that drive consumers to respond to social media advertising. The roles of content, media, and individual factors are examined. A research model is developed and tested using data collected from an online-survey of 613 social media users. Our results show compelling evidence that the effectiveness of display ads on social media is driven by ad content, ad-media congruity, and consumers' individual factors. Practical and theoretical implications are discussed.},
	booktitle = {2015 48th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Mao, En and Zhang, Jing},
	month = jan,
	year = {2015},
	note = {ISSN: 1530-1605},
	keywords = {Advertising, Analytical models, Consumer behavior, Entertainment industry, Facebook, Media, Social media advertising, ad click, product evaluation, purchase intention},
	pages = {3405--3413},
}

@article{williamson_composite_2016,
	title = {Composite {Multiclass} {Losses}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/14-294.html},
	number = {222},
	urldate = {2021-07-09},
	journal = {Journal of Machine Learning Research},
	author = {Williamson, Robert C. and Vernet, Elodie and Reid, Mark D.},
	year = {2016},
	pages = {1--52},
}

@article{rim_how_2016,
	title = {“{How} {Negative} {Becomes} {Less} {Negative}”: {Understanding} the {Effects} of {Comment} {Valence} and {Response} {Sidedness} in {Social} {Media}},
	volume = {66},
	issn = {0021-9916},
	shorttitle = {“{How} {Negative} {Becomes} {Less} {Negative}”},
	url = {https://doi.org/10.1111/jcom.12205},
	doi = {10.1111/jcom.12205},
	abstract = {This study explores the influence of the public’s negative comments regarding a corporate social responsibility (CSR) campaign in social media and how to best respond to them. It examined the interaction effects of comment valence and the company’s response sidedness on the public’s attitudes as mediated by the perceived negativity and perceived altruism. Results revealed that 2-sided CSR responses are more effective than 1-sided responses in enhancing altruistic motives for CSR, reducing perceived negativity in the public’s comments, and eliciting favorable attitudes, especially when comments were negative. The effects of message sidedness disappeared when the public’s comments were positive. Results also showed that perceived altruism and negativity mediate the effects of message strategies on the public’s attitudes toward the company.},
	number = {3},
	urldate = {2021-05-12},
	journal = {Journal of Communication},
	author = {Rim, Hyejoon and Song, Doori},
	month = jun,
	year = {2016},
	pages = {475--495},
}

@article{allcott_social_2017,
	title = {Social {Media} and {Fake} {News} in the 2016 {Election}},
	volume = {31},
	issn = {0895-3309},
	abstract = {Following the 2016 US presidential election, many have expressed concern about the effects of false stories ("fake news"), circulated largely through social media. We discuss the economics of fake news and present new data on its consumption prior to the election. Drawing on web browsing data, archives of fact-checking websites, and results from a new online survey, we find: 1) social media was an important but not dominant source of election news, with 14 percent of Americans calling social media their "most important" source; 2) of the known false news stories that appeared in the three months before the election, those favoring Trump were shared a total of 30 million times on Facebook, while those favoring Clinton were shared 8 million times; 3) the average American adult saw on the order of one or perhaps several fake news stories in the months around the election, with just over half of those who recalled seeing them believing them; and 4) people are much more likely to believe stories that favor their preferred candidate, especially if they have ideologically segregated social media networks.},
	language = {en},
	number = {2},
	urldate = {2021-05-12},
	journal = {Journal of Economic Perspectives},
	author = {Allcott, Hunt and Gentzkow, Matthew},
	month = may,
	year = {2017},
	keywords = {Economic Anthropology, Language, Media, Economic Sociology, Political Processes: Rent-Seeking, Lobbying, Elections, Legislatures, and Voting Behavior, Entertainment, Social and Economic Stratification},
	pages = {211--236},
}

@article{basat_game_2017,
	title = {A {Game} {Theoretic} {Analysis} of the {Adversarial} {Retrieval} {Setting}},
	volume = {60},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/11104},
	doi = {10.1613/jair.5547},
	language = {en},
	urldate = {2021-05-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Basat, Ran Ben and Tennenholtz, Moshe and Kurland, Oren},
	month = dec,
	year = {2017},
	pages = {1127--1164},
}

@inproceedings{gao_collaborative_2017,
	title = {Collaborative {Dynamic} {Sparse} {Topic} {Regression} with {User} {Profile} {Evolution} for {Item} {Recommendation}},
	booktitle = {{AAAI}},
	author = {Gao, Li and Wu, J. and Zhou, Chuan and Hu, Y.},
	year = {2017},
}

@article{li_obviously_2017,
	title = {Obviously {Strategy}-{Proof} {Mechanisms}},
	volume = {107},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/44871788},
	abstract = {A strategy is obviously dominant if, for any deviation, at any information set where both strategies first diverge, the best outcome under the deviation is no better than the worst outcome under the dominant strategy. A mechanism is obviously strategy-proof (OSP) if it has an equilibrium in obviously dominant strategies. This has a behavioral interpretation: a strategy is obviously dominant if and only if a cognitively limited agent can recognize it as weakly dominant. It also has a classical interpretation: a choice rule is OSP-implementable if and only if it can be carried out by a social planner under a particular regime of partial commitment.},
	number = {11},
	urldate = {2021-06-09},
	journal = {The American Economic Review},
	author = {Li, Shengwu},
	year = {2017},
	note = {Publisher: American Economic Association},
	pages = {3257--3287},
}

@inproceedings{akbarpour_credible_2018,
	address = {New York, NY, USA},
	series = {{EC} '18},
	title = {Credible {Mechanisms}},
	isbn = {978-1-4503-5829-3},
	url = {https://doi.org/10.1145/3219166.3219169},
	doi = {10.1145/3219166.3219169},
	abstract = {Consider an extensive-form mechanism, run by an auctioneer who communicates sequentially and privately with agents. Suppose the auctioneer can make any deviation that no single agent can detect. We study the mechanisms such that it is incentive-compatible for the auctioneer not to deviate - the credible mechanisms. Consider the optimal auctions in which only winners make transfers. The first-price auction is the unique credible static mechanism. The ascending auction is the unique credible strategy-proof mechanism.},
	urldate = {2021-06-14},
	booktitle = {Proceedings of the 2018 {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Akbarpour, Mohammad and Li, Shengwu},
	month = jun,
	year = {2018},
	keywords = {auction, credible, mechanism design, strategy-proof},
	pages = {371},
}

@inproceedings{balduzzi_mechanics_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {Mechanics} of n-{Player} {Differentiable} {Games}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/balduzzi18a.html},
	abstract = {The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood – and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding local Nash equilibria in GANs – whilst at the same time being applicable to – and having guarantees in – much more general games.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {354--363},
}

@inproceedings{ben-porat_game-theoretic_2018,
	title = {A {Game}-{Theoretic} {Approach} to {Recommendation} {Systems} with {Strategic} {Content} {Providers}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ben-Porat, Omer and Tennenholtz, Moshe},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@inproceedings{dong_strategic_2018,
	address = {New York, NY, USA},
	series = {{EC} '18},
	title = {Strategic {Classification} from {Revealed} {Preferences}},
	isbn = {978-1-4503-5829-3},
	url = {https://doi.org/10.1145/3219166.3219193},
	doi = {10.1145/3219166.3219193},
	abstract = {We study an online linear classification problem in which the data is generated by strategic agents who manipulate their features in an effort to change the classification outcome. In rounds, the learner deploys a classifier, then an adversarially chosen agent arrives and possibly manipulates her features to optimally respond to the learner's choice of classifier. The learner has no knowledge of the agents' utility functions or "real" features, which may vary widely across agents. Instead, the learner is only able to observe their "revealed preferences", i.e., the manipulated feature vectors they provide. For a broad family of agent cost functions, we give a computationally efficient learning algorithm that is able to obtain diminishing "Stackelberg regret" --- a form of policy regret that guarantees that the learner is realizing loss nearly as small as that of the best classifier in hindsight, even allowing for the fact that agents would have best-responded differently to the optimal classifier.},
	urldate = {2021-06-07},
	booktitle = {Proceedings of the 2018 {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Dong, Jinshuo and Roth, Aaron and Schutzman, Zachary and Waggoner, Bo and Wu, Zhiwei Steven},
	month = jun,
	year = {2018},
	keywords = {online learning, revealed preferences, stackelberg regret, strategic agents, strategic classification},
	pages = {55--70},
}

@incollection{busby_doing_2018,
	edition = {1st},
	title = {Doing {News} {Framing} {Analysis} {II}},
	isbn = {978-1-315-64223-9},
	url = {https://www.taylorfrancis.com/https://www.taylorfrancis.com/chapters/edit/10.4324/9781315642239-2/studying-framing-effects-political-preferences-ethan-busby-flynn-james-druckman},
	abstract = {Over the last quarter-century, work on issue framing has evolved to the point where the presentation and impact of frames are},
	language = {en},
	urldate = {2021-05-19},
	booktitle = {Studying {Framing} {Effects} on {Political} {Preferences}},
	publisher = {Routledge},
	author = {Busby, Ethan and Flynn, D. J. and Druckman, James N},
	month = may,
	year = {2018},
	doi = {10.4324/9781315642239-2},
	note = {Pages: 27-50
Publication Title: Doing News Framing Analysis II},
	pages = {24},
}

@inproceedings{chen_strategyproof_2018,
	address = {New York, NY, USA},
	series = {{EC} '18},
	title = {Strategyproof {Linear} {Regression} in {High} {Dimensions}},
	isbn = {978-1-4503-5829-3},
	url = {https://doi.org/10.1145/3219166.3219175},
	doi = {10.1145/3219166.3219175},
	abstract = {This paper is part of an emerging line of work at the intersection of machine learning and mechanism design, which aims to avoid noise in training data by correctly aligning the incentives of data sources. Specifically, we focus on the ubiquitous problem of linear regression, where strategyproof mechanisms have previously been identified in two dimensions. In our setting, agents have single-peaked preferences and can manipulate only their response variables. Our main contribution is the discovery of a family of group strategyproof linear regression mechanisms in any number of dimensions, which we call generalized resistant hyperplane mechanisms. The game-theoretic properties of these mechanisms --- and, in fact, their very existence --- are established through a connection to a discrete version of the Ham Sandwich Theorem.},
	urldate = {2021-04-26},
	booktitle = {Proceedings of the 2018 {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Yiling and Podimata, Chara and Procaccia, Ariel D. and Shah, Nisarg},
	month = jun,
	year = {2018},
	pages = {9--26},
}

@article{galak_properties_2018,
	title = {The {Properties} and {Antecedents} of {Hedonic} {Decline}},
	volume = {69},
	url = {https://doi.org/10.1146/annurev-psych-122216-011542},
	doi = {10.1146/annurev-psych-122216-011542},
	abstract = {We review the phenomenon of hedonic decline, whereby repeated exposure to a stimulus typically reduces the hedonic response (e.g., enjoyment). We first discuss the typical trajectory of hedonic decline and the common research paradigms used to study it. We next discuss the most popular theories regarding general mechanisms widely believed to underlie hedonic decline. We then propose a taxonomy to organize these various general theories and to incorporate more recent work on top-down, self-reflective theories. This taxonomy identifies three general classes of antecedents to hedonic decline: physiological feedback, perceptual changes, and self-reflection. For each class, we review the supporting evidence for specifically identified antecedents and recent developments on how each antecedent influences hedonic decline. Our review focuses especially on more recent work in the growing area of self-reflection.},
	number = {1},
	journal = {Annual Review of Psychology},
	author = {Galak, Jeff and Redden, Joseph P.},
	year = {2018},
	pmid = {28854001},
	note = {\_eprint: https://doi.org/10.1146/annurev-psych-122216-011542},
	pages = {1--25},
}

@article{frankel_muddled_2019,
	title = {Muddled {Information}},
	volume = {127},
	issn = {0022-3808},
	url = {https://www.journals.uchicago.edu/doi/10.1086/701604},
	doi = {10.1086/701604},
	abstract = {We study a model of signaling in which agents are heterogeneous on two dimensions. An agent’s natural action is the action taken in the absence of signaling concerns. Her gaming ability parameterizes the cost of increasing the action. Equilibrium behavior muddles information across dimensions. As incentives to take higher actions increase—due to higher stakes or more manipulable signaling technology—more information is revealed about gaming ability, and less about natural actions. We explore a new externality: showing agents’ actions to additional observers can worsen information for existing observers. Applications to credit scoring, school testing, and web searching are discussed.},
	number = {4},
	urldate = {2021-06-09},
	journal = {Journal of Political Economy},
	author = {Frankel, Alex and Kartik, Navin},
	month = aug,
	year = {2019},
	note = {Publisher: The University of Chicago Press},
	pages = {1739--1776},
}

@inproceedings{mouzannar_fair_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {From {Fair} {Decision} {Making} {To} {Social} {Equality}},
	isbn = {978-1-4503-6125-5},
	url = {https://doi.org/10.1145/3287560.3287599},
	doi = {10.1145/3287560.3287599},
	abstract = {The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action. We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.},
	urldate = {2021-06-09},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Mouzannar, Hussein and Ohannessian, Mesrob I. and Srebro, Nathan},
	month = jan,
	year = {2019},
	keywords = {affirmative action, demographic parity, dynamics, fairness, influence on society, selection processes, social equality},
	pages = {359--368},
}

@article{williams_dynamic_2019,
	title = {Dynamic {Modeling} and {Equilibria} in {Fair} {Decision} {Making}},
	url = {http://arxiv.org/abs/1911.06837},
	abstract = {Recent studies on fairness in automated decision making systems have both investigated the potential future impact of these decisions on the population at large, and emphasized that imposing ''typical'' fairness constraints such as demographic parity or equality of opportunity does not guarantee a benefit to disadvantaged groups. However, these previous studies have focused on either simple one-step cost/benefit criteria, or on discrete underlying state spaces. In this work, we first propose a natural continuous representation of population state, governed by the Beta distribution, using a loan granting setting as a running example. Next, we apply a model of population dynamics under lending decisions, and show that when conditional payback probabilities are estimated correctly 1) ``optimal'' behavior by lenders can lead to ''Matthew Effect'' bifurcations (i.e., ''the rich get richer and the poor get poorer''), but that 2) many common fairness constraints on the allowable policies cause groups to converge to the same equilibrium point. Last, we contrast our results in the case of misspecified conditional probability estimates with prior work, and show that for this model, different levels of group misestimation guarantees that even fair policies lead to bifurcations. We illustrate some of the modeling conclusions on real data from credit scoring.},
	urldate = {2021-06-18},
	journal = {arXiv:1911.06837 [cs, stat]},
	author = {Williams, Joshua and Kolter, J. Zico},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06837
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wasserman_exploratory_2019,
	title = {An {Exploratory} {Study} of “{Fake} {News}” and {Media} {Trust} in {Kenya}, {Nigeria} and {South} {Africa}},
	volume = {40},
	issn = {2374-3670},
	url = {https://doi.org/10.1080/23743670.2019.1627230},
	doi = {10.1080/23743670.2019.1627230},
	abstract = {In recent years, concerns about the perceived increase in the amount of “fake news” have become prevalent in discussions about media and politics, particularly in the United States and Europe. However, debates around “fake news”, even if some object to the use of the term due to it being loosely defined, appear to speak of processes that occur not only in the Global North but also elsewhere. In Africa, mis- and disinformation campaigns have been used to influence political agendas, and governments have responded with countermeasures. This article explores the phenomenon in Kenya, Nigeria and South Africa using data from a two-wave online survey (N = 1847). We find that perceived exposure to disinformation is high, and that trust in social and national media is low. We also identify a significant relationship between higher levels of perceived exposure to disinformation and lower levels of media trust in South Africa. The limitations of this study, which focuses on a subset of the population that is highly educated, the implications of our findings, and recommendations for future research are discussed.},
	number = {1},
	urldate = {2021-05-20},
	journal = {African Journalism Studies},
	author = {Wasserman, Herman and Madrid-Morales, Dani},
	month = jan,
	year = {2019},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/23743670.2019.1627230},
	keywords = {Kenya, Nigeria, South Africa, disinformation, fake news, media trust, misinformation},
	pages = {107--123},
}

@article{xu_user_2019,
	title = {User {Participation} in {Collaborative} {Filtering}-{Based} {Recommendation} {Systems}: {A} {Game} {Theoretic} {Approach}},
	volume = {49},
	journal = {IEEE Transactions on Cybernetics},
	author = {Xu, Lei and Jiang, C. and Chen, Yan and Ren, Y. and Liu, K. R.},
	year = {2019},
	pages = {1339--1352},
}

@article{ben-porat_content_2020,
	title = {Content {Provider} {Dynamics} and {Coordination} in {Recommendation} {Ecosystems}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html},
	language = {en},
	urldate = {2021-05-13},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ben-Porat, Omer and Rosenberg, Itay and Tennenholtz, Moshe},
	year = {2020},
	pages = {18931--18941},
}

@article{zhang_distinguishing_2019,
	title = {Distinguishing {Distributions} {When} {Samples} {Are} {Strategically} {Transformed}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/ac5dab2e99eee9cf9ec672e383691302-Abstract.html},
	language = {en},
	urldate = {2021-06-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Hanrui and Cheng, Yu and Conitzer, Vincent},
	year = {2019},
}

@article{allcott_welfare_2020,
	title = {The {Welfare} {Effects} of {Social} {Media}},
	volume = {110},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257%2Faer.20190658&utm_campaign=Johannes%20Klingebiel&utm_medium=email&utm_source=Revue%20newsletter},
	doi = {10.1257/aer.20190658},
	abstract = {The rise of social media has provoked both optimism about potential societal benefits and concern about harms such as addiction, depression, and political polarization. In a randomized experiment, we find that deactivating Facebook for the four weeks before the 2018 US midterm election (i) reduced online activity, while increasing offline activities such as watching TV alone and socializing with family and friends; (ii) reduced both factual news knowledge and political polarization; (iii) increased subjective well-being; and (iv) caused a large persistent reduction in post-experiment Facebook use. Deactivation reduced post-experiment valuations of Facebook, suggesting that traditional metrics may overstate consumer surplus.},
	language = {en},
	number = {3},
	urldate = {2021-05-12},
	journal = {American Economic Review},
	author = {Allcott, Hunt and Braghieri, Luca and Eichmeyer, Sarah and Gentzkow, Matthew},
	month = mar,
	year = {2020},
	keywords = {Computer Software, Economic Sociology, Consumer Economics: Empirical Analysis, Political Processes: Rent-seeking, Lobbying, Elections, Legislatures, and Voting Behavior, Micro-Based Behavioral Economics: General, General Welfare, Economic Anthropology, Language, Media, Information and Internet Services, Social and Economic Stratification, Well-Being, Entertainment},
	pages = {629--676},
}

@inproceedings{leqi_rebounding_2020,
	title = {Rebounding {Bandits} for {Modeling} {Satiation} {Effects}},
	author = {Leqi, Liu and Kilinc-Karzan, Fatma and Lipton, Zachary C. and Montgomery, Alan L.},
	year = {2020},
	note = {2011.06741},
}

@article{chen_learning_2020,
	title = {Learning {Strategy}-{Aware} {Linear} {Classifiers}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ae87a54e183c075c494c4d397d126a66-Abstract.html},
	language = {en},
	urldate = {2021-06-07},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Yiling and Liu, Yang and Podimata, Chara},
	year = {2020},
	pages = {15265--15276},
}

@unpublished{hossain_effect_2020,
	title = {The {Effect} of {Strategic} {Noise} in {Linear} {Regression}},
	author = {Hossain, Safwan and Shah, Nisarg},
	year = {2020},
	note = {\_eprint: 2007.07316},
}

@article{miller_strategic_2020,
	title = {Strategic {Classification} is {Causal} {Modeling} in {Disguise}},
	url = {/paper/The-Strategic-Perceptron-Ahmadi-Beyhaghi/8c9e47c01833116114bfa8f86622b8b4d3dc308d},
	abstract = {The classical Perceptron algorithm provides a simple and elegant procedure for learning a linear classifier. In each step, the algorithm observes the sample\&\#39;s position and label and may update the current predictor accordingly. In presence of strategic agents, however, the classifier may not be able to observe the true position but a position where the agent pretends to be in order to be classified desirably. Unlike the original setting with perfect knowledge of positions, in this situation the Perceptron algorithm fails to achieve its guarantees, and we illustrate examples with the predictor oscillating between two solutions forever, never reaching a perfect classifier even though one exists. Our main contribution is providing a modified Perceptron-style algorithm which finds a classifier in presence of strategic agents with both \${\textbackslash}ell\_2\$ and weighted \${\textbackslash}ell\_1\$ manipulation costs. In our baseline model, knowledge of the manipulation costs is assumed. In our most general model, we relax this assumption and provide an algorithm which learns and refines both the classifier and its cost estimates to achieve good mistake bounds even when manipulation costs are unknown.},
	language = {en},
	urldate = {2021-06-07},
	journal = {undefined},
	author = {Miller, John and Milli, S. and Hardt, Moritz},
	year = {2020},
}

@article{munger_right-wing_2020,
	title = {Right-{Wing} {YouTube}: {A} {Supply} and {Demand} {Perspective}},
	issn = {1940-1612},
	shorttitle = {Right-{Wing} {YouTube}},
	url = {https://doi.org/10.1177/1940161220964767},
	doi = {10.1177/1940161220964767},
	abstract = {YouTube is the most used social network in the United States and the only major platform that is more popular among right-leaning users. We propose the “Supply and Demand” framework for analyzing politics on YouTube, with an eye toward understanding dynamics among right-wing video producers and consumers. We discuss a number of novel technological affordances of YouTube as a platform and as a collection of videos, and how each might drive supply of or demand for extreme content. We then provide large-scale longitudinal descriptive information about the supply of and demand for conservative political content on YouTube. We demonstrate that viewership of far-right videos peaked in 2017.},
	language = {en},
	urldate = {2021-06-08},
	journal = {The International Journal of Press/Politics},
	author = {Munger, Kevin and Phillips, Joseph},
	month = oct,
	year = {2020},
	note = {Publisher: SAGE Publications Inc},
	keywords = {YouTube, conservatism, political extremism, radicalization},
	pages = {1940161220964767},
}

@inproceedings{mladenov_optimizing_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Optimizing {Long}-term {Social} {Welfare} in {Recommender} {Systems}: {A} {Constrained} {Matching} {Approach}},
	volume = {119},
	url = {http://proceedings.mlr.press/v119/mladenov20a.html},
	abstract = {Most recommender systems (RS) research assumes that a user’s utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true – the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement. We formulate this problem as one of equilibrium selection in the induced dynamical system, and show that it can be solved as an optimal constrained matching problem. Our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers. We demonstrate that even in a simple, stylized dynamical RS model, the standard myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mladenov, Martin and Creager, Elliot and Ben-Porat, Omer and Swersky, Kevin and Zemel, Richard and Boutilier, Craig},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {6987--6998},
}

@article{ognyanova_misinformation_2020,
	title = {Misinformation in action: {Fake} news exposure is linked to lower trust in media, higher trust in government when your side is in power},
	shorttitle = {Misinformation in action},
	url = {https://misinforeview.hks.harvard.edu/article/misinformation-in-action-fake-news-exposure-is-linked-to-lower-trust-in-media-higher-trust-in-government-when-your-side-is-in-power/},
	doi = {10.37016/mr-2020-024},
	abstract = {One major concern about fake news is that it could damage the public trust in democratic institutions. We examined this possibility using longitudinal survey data combined with records of online behavior. Our study found that online misinformation was linked to lower trust in mainstream media across party lines. However, for moderates and conservatives, exposure to […]},
	language = {en-US},
	urldate = {2021-05-20},
	journal = {Harvard Kennedy School Misinformation Review},
	author = {Ognyanova, Katherine and Lazer, David and Robertson, Ronald E. and Wilson, Christo},
	month = jun,
	year = {2020},
}

@inproceedings{ribeiro_auditing_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Auditing radicalization pathways on {YouTube}},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3372879},
	doi = {10.1145/3351095.3372879},
	abstract = {Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.},
	urldate = {2021-05-13},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Ribeiro, Manoel Horta and Ottoni, Raphael and West, Robert and Almeida, Virgílio A. F. and Meira, Wagner},
	month = jan,
	year = {2020},
	keywords = {algorithmic auditing, extremism, hate speech, radicalization},
	pages = {131--141},
}

@article{zhang_bayes_2020,
	title = {Bayes {Consistency} vs. {H}-{Consistency}: {The} {Interplay} between {Surrogate} {Loss} {Functions} and the {Scoring} {Function} {Class}},
	volume = {33},
	shorttitle = {Bayes {Consistency} vs. {H}-{Consistency}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c4c28b367e14df88993ad475dedf6b77-Abstract.html},
	language = {en},
	urldate = {2021-07-08},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Mingyuan and Agarwal, Shivani},
	year = {2020},
	pages = {16927--16936},
}

@inproceedings{vasilisky_studying_2020,
	address = {New York, NY, USA},
	series = {{SIGIR} '20},
	title = {Studying {Ranking}-{Incentivized} {Web} {Dynamics}},
	isbn = {978-1-4503-8016-4},
	url = {https://doi.org/10.1145/3397271.3401300},
	doi = {10.1145/3397271.3401300},
	abstract = {The ranking incentives of many authors of Web pages play an important role in the Web dynamics. That is, authors who opt to have their pages highly ranked for queries of interest often respond to rankings for these queries by manipulating their pages; the goal is to improve the pages' future rankings. Various theoretical aspects of this dynamics have recently been studied using game theory. However, empirical analysis of the dynamics is highly constrained due to lack of publicly available datasets. We present an initial such dataset that is based on TREC's ClueWeb09 dataset. Specifically, we used the WayBack Machine of the Internet Archive to build a document collection that contains past snapshots of ClueWeb documents which are highly ranked by some initial search performed for ClueWeb queries. Temporal analysis of document changes in this dataset reveals that findings recently presented for small-scale controlled ranking competitions between documents' authors also hold for Web data. Specifically, documents' authors tend to mimic the content of documents that were highly ranked in the past, and this practice can result in improved ranking.},
	urldate = {2021-06-01},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Vasilisky, Ziv and Tennenholtz, Moshe and Kurland, Oren},
	month = jul,
	year = {2020},
	pages = {2093--2096},
}

@inproceedings{ahmadi_strategic_2021,
	title = {The {Strategic} {Perceptron}},
	url = {http://arxiv.org/abs/2008.01710},
	abstract = {The classical Perceptron algorithm provides a simple and elegant procedure for learning a linear classiﬁer. In each step, the algorithm observes the sample’s position and label and updates the current predictor accordingly if it makes a mistake. However, in presence of strategic agents that desire to be classiﬁed as positive and that are able to modify their position by a limited amount, the classiﬁer may not be able to observe the true position of agents but rather a position where the agent pretends to be. Unlike the original setting with perfect knowledge of positions, in this situation the Perceptron algorithm fails to achieve its guarantees, and we illustrate examples with the predictor oscillating between two solutions forever, making an unbounded number of mistakes even though a perfect large-margin linear classiﬁer exists. Our main contribution is providing a modiﬁed Perceptron-style algorithm which makes a bounded number of mistakes in presence of strategic agents with both 2 and weighted 1 manipulation costs. In our baseline model, knowledge of the manipulation costs (i.e., the extent to which an agent may manipulate) is assumed. In our most general model, we relax this assumption and provide an algorithm which learns and reﬁnes both the classiﬁer and its cost estimates to achieve good mistake bounds even when manipulation costs are unknown.},
	language = {en},
	urldate = {2021-06-07},
	booktitle = {{EC}},
	author = {Ahmadi, Saba and Beyhaghi, Hedyeh and Blum, Avrim and Naggita, Keziah},
	year = {2021},
	note = {arXiv: 2008.01710},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{finocchiaro_bridging_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Bridging {Machine} {Learning} and {Mechanism} {Design} towards {Algorithmic} {Fairness}},
	abstract = {Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Finocchiaro, Jessie and Maio, Roland and Monachou, Faidra and Patro, Gourab K and Raghavan, Manish and Stoica, Ana-Andreea and Tsirtsis, Stratis},
	year = {2021},
	note = {event-place: Virtual Event, Canada},
	pages = {489--503},
}

@article{harris_stateful_2021,
	title = {Stateful {Strategic} {Regression}},
	url = {http://arxiv.org/abs/2106.03827},
	abstract = {Automated decision-making tools increasingly assess individuals to determine if they qualify for high-stakes opportunities. A recent line of research investigates how strategic agents may respond to such scoring tools to receive favorable assessments. While prior work has focused on the short-term strategic interactions between a decision-making institution (modeled as a principal) and individual decision-subjects (modeled as agents), we investigate interactions spanning multiple time-steps. In particular, we consider settings in which the agent's effort investment today can accumulate over time in the form of an internal state - impacting both his future rewards and that of the principal. We characterize the Stackelberg equilibrium of the resulting game and provide novel algorithms for computing it. Our analysis reveals several intriguing insights about the role of multiple interactions in shaping the game's outcome: First, we establish that in our stateful setting, the class of all linear assessment policies remains as powerful as the larger class of all monotonic assessment policies. While recovering the principal's optimal policy requires solving a non-convex optimization problem, we provide polynomial-time algorithms for recovering both the principal and agent's optimal policies under common assumptions about the process by which effort investments convert to observable features. Most importantly, we show that with multiple rounds of interaction at her disposal, the principal is more effective at incentivizing the agent to accumulate effort in her desired direction. Our work addresses several critical gaps in the growing literature on the societal impacts of automated decision-making - by focusing on longer time horizons and accounting for the compounding nature of decisions individuals receive over time.},
	urldate = {2021-06-09},
	journal = {arXiv:2106.03827 [cs]},
	author = {Harris, Keegan and Heidari, Hoda and Wu, Zhiwei Steven},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.03827},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@inproceedings{huleihel_learning_2021,
	title = {Learning {User} {Preferences} in {Non}-{Stationary} {Environments}},
	booktitle = {{AISTATS}},
	author = {Huleihel, Wasim and Pal, S. and Shayevitz, O.},
	year = {2021},
}

@inproceedings{jagadeesan_alternative_2021,
	title = {Alternative {Microfoundations} for {Strategic} {Classification}},
	url = {http://arxiv.org/abs/2106.12705},
	abstract = {When reasoning about strategic behavior in a machine learning context it is tempting to combine standard microfoundations of rational agents with the statistical decision theory underlying classification. In this work, we argue that a direct combination of these standard ingredients leads to brittle solution concepts of limited descriptive and prescriptive value. First, we show that rational agents with perfect information produce discontinuities in the aggregate response to a decision rule that we often do not observe empirically. Second, when any positive fraction of agents is not perfectly strategic, desirable stable points -- where the classifier is optimal for the data it entails -- cease to exist. Third, optimal decision rules under standard microfoundations maximize a measure of negative externality known as social burden within a broad class of possible assumptions about agent behavior. Recognizing these limitations we explore alternatives to standard microfoundations for binary classification. We start by describing a set of desiderata that help navigate the space of possible assumptions about how agents respond to a decision rule. In particular, we analyze a natural constraint on feature manipulations, and discuss properties that are sufficient to guarantee the robust existence of stable points. Building on these insights, we then propose the noisy response model. Inspired by smoothed analysis and empirical observations, noisy response incorporates imperfection in the agent responses, which we show mitigates the limitations of standard microfoundations. Our model retains analytical tractability, leads to more robust insights about stable points, and imposes a lower social burden at optimality.},
	urldate = {2021-07-07},
	booktitle = {{ICML}},
	author = {Jagadeesan, Meena and Mendler-Dünner, Celestine and Hardt, Moritz},
	year = {2021},
	note = {arXiv: 2106.12705},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Computers and Society, Computer Science - Machine Learning, Economics - Theoretical Economics},
}

@inproceedings{morrill_hindsight_2021,
	title = {Hindsight and {Sequential} {Rationality} of {Correlated} {Play}},
	booktitle = {{AAAI}},
	author = {Morrill, Dustin and D'Orazio, Ryan and Sarfati, Reca and Lanctot, Marc and Wright, J. and Greenwald, A. and Bowling, Michael H.},
	year = {2021},
}

@article{pelrine_surprising_2021,
	title = {The {Surprising} {Performance} of {Simple} {Baselines} for {Misinformation} {Detection}},
	volume = {abs/2104.06952},
	journal = {ArXiv},
	author = {Pelrine, Kellin and Danovitch, Jacob and Rabbany, Reihaneh},
	year = {2021},
}

@inproceedings{ron_corporate_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Corporate {Social} {Responsibility} via {Multi}-{Armed} {Bandits}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445868},
	doi = {10.1145/3442188.3445868},
	abstract = {We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.},
	urldate = {2021-05-13},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Ron, Tom and Ben-Porat, Omer and Shalit, Uri},
	month = mar,
	year = {2021},
	pages = {26--40},
}

@inproceedings{sundaram_pac-learning_2021,
	title = {{PAC}-{Learning} for {Strategic} {Classification}},
	url = {http://proceedings.mlr.press/v139/sundaram21a.html},
	abstract = {The study of strategic or adversarial manipulation of testing data to fool a classifier has attracted much recent attention. Most previous works have focused on two extreme situations where any tes...},
	language = {en},
	urldate = {2021-07-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sundaram, Ravi and Vullikanti, Anil and Xu, Haifeng and Yao, Fan},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9978--9988},
}

@incollection{lucas_jr_econometric_1976,
	series = {Carnegie-{Rochester} {Conference} {Series} on {Public} {Policy}},
	title = {Econometric {Policy} {Evaluation}: {A} {Critique}},
	volume = {1},
	booktitle = {The {Phillips} {Curve} and {Labor}  markets},
	author = {Lucas Jr., Robert},
	year = {1976},
	pages = {19--46},
}

@article{decanio_rational_1979,
	title = {Rational {Expectations} and {Learning} from {Experience}},
	volume = {93},
	abstract = {Abstract. I. Introduction, 47.—II. The model, 49.—III. Conclusions, 54.The universal form of conscious behavior is thus action designed to change a future situa},
	language = {en},
	number = {1},
	urldate = {2021-08-17},
	journal = {The Quarterly Journal of Economics},
	author = {DeCanio, Stephen J.},
	month = feb,
	year = {1979},
	note = {Publisher: Oxford Academic},
	pages = {47--57},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
	number = {477},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E.},
	month = mar,
	year = {2007},
	keywords = {Bayes factor, Bregman divergence, Brier score, Coherent, Continuous ranked probability score, Cross-validation, Entropy, Kernel score, Loss function, Minimum contrast estimation, Negative definite function, Prediction interval, Predictive distribution, Quantile forecast, Scoring rule, Skill score, Strictly proper, Utility function},
	pages = {359--378},
}

@article{fudenberg_learning_2009,
	title = {Learning and {Equilibrium}},
	volume = {1},
	abstract = {The theory of learning in games explores how, which, and what kind of equilibria might arise as a consequence of a long-run nonequilibrium process of learning, adaptation, and/or imitation. If agents' strategies are completely observed at the end of each round (and agents are randomly matched with a series of anonymous opponents), fairly simple rules perform well in terms of the agent's worst-case payoffs, and also guarantee that any steady state of the system must correspond to an equilibrium. If players do not observe the strategies chosen by their opponents (as in extensive-form games), then learning is consistent with steady states that are not Nash equilibria because players can maintain incorrect beliefs about off-path play. Beliefs can also be incorrect because of cognitive limitations and systematic inferential errors.},
	number = {1},
	journal = {Annual Review of Economics},
	author = {Fudenberg, Drew and Levine, David K.},
	year = {2009},
	note = {\_eprint: https://doi.org/10.1146/annurev.economics.050708.142930},
	pages = {385--420},
}

@article{hubert_central_2015,
	title = {Do {Central} {Bank} {Forecasts} {Influence} {Private} {Agents}? {Forecasting} {Performance} versus {Signals}},
	volume = {47},
	shorttitle = {Do {Central} {Bank} {Forecasts} {Influence} {Private} {Agents}?},
	abstract = {Focusing on a set of central banks that publish inflation forecasts in real time, this paper aims to establish whether central bank inflation forecasts influence private inflation forecasts. The response is positive in the five countries studied: Sweden, the United Kingdom, Canada, Switzerland, and Japan. Three hypotheses may explain this central bank influence: central bank forecasts are more accurate than private ones, are based on different information sets, and/or convey signals about future policy decisions and policymakers’ preferences and objectives. We provide evidence that the source of these central banks’ influence is not linked to their forecasting performance.},
	language = {en},
	number = {4},
	urldate = {2021-07-24},
	journal = {Journal of Money, Credit and Banking},
	author = {Hubert, Paul},
	year = {2015},
	keywords = {E52, E58, communication, forecasts, imperfect information, monetary policy},
	pages = {771--789},
}

@article{carvalho_overview_2016,
	title = {An {Overview} of {Applications} of {Proper} {Scoring} {Rules}},
	volume = {13},
	abstract = {We present a study on the evolution of publications about applications of proper scoring rules. Specifically, we consider articles reporting the use of proper scoring rules when either measuring the accuracy of forecasts or for inducing honest reporting of private information within a certain context. Our analysis of a data set containing 201 articles published between 1950 and 2015 suggests that there has been a tremendous increase in the number of published articles about proper scoring rules over the years. Moreover, the weather/climate, prediction markets, psychology, and energy domains are the four most popular application areas. After providing some insights on how proper scoring rules are applied in different domains, we analyze the publication outlets where the articles in our data set were published. In this regard, we find that an increasing number of articles are now being published in conference proceedings related to artificial intelligence, as opposed to traditional academic journals. We conclude this review by suggesting that the wisdom-of-crowds phenomenon might be a driving force behind the recent popularity of proper scoring rules.},
	number = {4},
	urldate = {2021-07-23},
	journal = {Decision Analysis},
	author = {Carvalho, Arthur},
	month = dec,
	year = {2016},
	keywords = {forecast evaluation, forecasting, incentive engineering, proper scoring rules},
	pages = {223--242},
}

@inproceedings{hardt_strategic_2016,
	address = {New York, NY, USA},
	series = {{ITCS} '16},
	title = {Strategic {Classification}},
	abstract = {Machine learning relies on the assumption that unseen test instances of a classification problem follow the same distribution as observed training data. However, this principle can break down when machine learning is used to make important decisions about the welfare (employment, education, health) of strategic individuals. Knowing information about the classifier, such individuals may manipulate their attributes in order to obtain a better classification outcome. As a result of this behavior -- often referred to as gaming -- the performance of the classifier may deteriorate sharply. Indeed, gaming is a well-known obstacle for using machine learning methods in practice; in financial policy-making, the problem is widely known as Goodhart's law. In this paper, we formalize the problem, and pursue algorithms for learning classifiers that are robust to gaming. We model classification as a sequential game between a player named "Jury" and a player named "Contestant." Jury designs a classifier, and Contestant receives an input to the classifier drawn from a distribution. Before being classified, Contestant may change his input based on Jury's classifier. However, Contestant incurs a cost for these changes according to a cost function. Jury's goal is to achieve high classification accuracy with respect to Contestant's original input and some underlying target classification function, assuming Contestant plays best response. Contestant's goal is to achieve a favorable classification outcome while taking into account the cost of achieving it. For a natural class of "separable" cost functions, and certain generalizations, we obtain computationally efficient learning algorithms which are near optimal, achieving a classification error that is arbitrarily close to the theoretical minimum. Surprisingly, our algorithms are efficient even on concept classes that are computationally hard to learn. For general cost functions, designing an approximately optimal strategy-proof classifier, for inverse-polynomial approximation, is NP-hard.},
	urldate = {2021-04-26},
	booktitle = {Proceedings of the 2016 {ACM} {Conference} on {Innovations} in {Theoretical} {Computer} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Hardt, Moritz and Megiddo, Nimrod and Papadimitriou, Christos and Wootters, Mary},
	month = jan,
	year = {2016},
	keywords = {classification, game theory, learning theory},
	pages = {111--122},
}

@article{thaler_behavioral_2016,
	title = {Behavioral {Economics}: {Past}, {Present}, and {Future}},
	volume = {106},
	shorttitle = {Behavioral {Economics}},
	language = {en},
	number = {7},
	urldate = {2021-08-15},
	journal = {American Economic Review},
	author = {Thaler, Richard H.},
	month = jul,
	year = {2016},
	keywords = {Belief, Communication, Information and Knowledge, Learning, Micro-Based Behavioral Economics: Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making, Consumer Economics: Theory, Search, Unawareness, Behavioral Macroeconomics, Behavioral Finance: Underlying Principles},
	pages = {1577--1600},
}

@inproceedings{roughgarden_online_2017,
	title = {Online {Prediction} with {Selfish} {Experts}},
	volume = {30},
	urldate = {2021-08-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Roughgarden, Tim and Schrijvers, Okke},
	year = {2017},
}

@inproceedings{leike_scalable_2018,
	title = {Scalable {Agent} {Alignment} via {Reward} {Modeling}: {A} {Research} {Direction}},
	abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
	urldate = {2020-07-29},
	booktitle = {{arXiv}:1811.07871},
	author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, reward},
}

@inproceedings{hu_disparate_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {The {Disparate} {Effects} of {Strategic} {Manipulation}},
	abstract = {When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed "strategic manipulation," analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to "trick" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off---even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's "quality" when agents' capacities to adaptively respond differ.},
	urldate = {2021-06-04},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Lily and Immorlica, Nicole and Vaughan, Jennifer Wortman},
	month = jan,
	year = {2019},
	keywords = {fairness in machine learning, strategic classification},
	pages = {259--268},
}

@inproceedings{barabas_interventions_2018,
	title = {Interventions over {Predictions}: {Reframing} the {Ethical} {Debate} for {Actuarial} {Risk} {Assessment}},
	shorttitle = {Interventions over {Predictions}},
	url = {http://proceedings.mlr.press/v81/barabas18a.html},
	abstract = {Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, fr...},
	language = {en},
	urldate = {2020-08-01},
	booktitle = {Conference on {Fairness}, {Accountability} and {Transparency}},
	author = {Barabas, Chelsea and Virza, Madars and Dinakar, Karthik and Ito, Joichi and Zittrain, Jonathan},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498
Section: Machine Learning},
	pages = {62--76},
}

@inproceedings{kleinberg_how_2019,
	address = {New York, NY, USA},
	series = {{EC} '19},
	title = {How {Do} {Classifiers} {Induce} {Agents} to {Invest} {Effort} {Strategically}?},
	isbn = {978-1-4503-6792-9},
	url = {https://doi.org/10.1145/3328526.3329584},
	doi = {10.1145/3328526.3329584},
	abstract = {Algorithms are often used to produce decision-making rules that classify or evaluate individuals. When these individuals have incentives to be classified a certain way, they may behave strategically to influence their outcomes. We develop a model for how strategic agents can invest effort in order to change the outcomes they receive, and we give a tight characterization of when such agents can be incentivized to invest specified forms of effort into improving their outcomes as opposed to "gaming" the classifier. We show that whenever any "reasonable" mechanism can do so, a simple linear mechanism suffices.},
	urldate = {2021-04-26},
	booktitle = {Proceedings of the 2019 {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Kleinberg, Jon and Raghavan, Manish},
	month = jun,
	year = {2019},
	keywords = {effort allocation, principal-agent, strategic classification},
	pages = {825--844},
}

@inproceedings{milli_social_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {The {Social} {Cost} of {Strategic} {Classification}},
	abstract = {Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.},
	urldate = {2021-06-16},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Milli, Smitha and Miller, John and Dragan, Anca D. and Hardt, Moritz},
	month = jan,
	year = {2019},
	keywords = {Strategic classification, fairness, machine learning},
	pages = {230--239},
}

@inproceedings{freeman_no-regret_2020,
	title = {No-{Regret} and {Incentive}-{Compatible} {Online} {Learning}},
	language = {en},
	urldate = {2021-08-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Freeman, Rupert and Pennock, David and Podimata, Chara and Vaughan, Jennifer Wortman},
	month = nov,
	year = {2020},
	pages = {3270--3279},
}

@inproceedings{krueger_hidden_2020,
	title = {Hidden {Incentives} for {Auto}-{Induced} {Distributional} {Shift}},
	abstract = {Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.},
	urldate = {2020-11-23},
	author = {Krueger, David and Maharaj, Tegan and Leike, Jan},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{liu_disparate_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {The {Disparate} {Equilibria} of {Algorithmic} {Decision} {Making} when {Individuals} {Invest} {Rationally}},
	abstract = {The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification---including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Lydia T. and Wilson, Ashia and Haghtalab, Nika and Kalai, Adam Tauman and Borgs, Christian and Chayes, Jennifer},
	month = jan,
	year = {2020},
	keywords = {dynamics, fairness, machine learning, statistical discrimination},
	pages = {381--391},
}

@inproceedings{perdomo_performative_2020,
	title = {Performative {Prediction}},
	volume = {119},
	abstract = {When predictions support decisions they may inﬂuence the outcome they aim to predict. We call such predictions performative; the prediction inﬂuences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining.},
	language = {en},
	urldate = {2021-02-08},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Perdomo, Juan C. and Zrnic, Tijana and Mendler-Dünner, Celestine and Hardt, Moritz},
	year = {2020},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{stocker_how_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {How {Facebook} and {Google} {Accidentally} {Created} a {Perfect} {Ecosystem} for {Targeted} {Disinformation}},
	abstract = {Online platforms providing information and media content follow certain goals and optimize for certain metrics when deploying automated decision making systems to recommend pieces of content from the vast amount of media items uploaded to or indexed by their platforms every day. These optimization metrics differ markedly from, for example, the so-called news factors journalists traditionally use to make editorial decisions. Social networks, video platforms and search engines thus create content hierarchies that reflect not only user interest but also their own monetization goals. This sometimes has unintended, societally highly problematic effects: Optimizing for metrics like dwell time, watch time or “engagement” can promote disinformation and propaganda content. This chapter provides examples and discusses relevant mechanisms and interactions.},
	language = {en},
	booktitle = {Disinformation in {Open} {Online} {Media}},
	publisher = {Springer International Publishing},
	author = {Stöcker, Christian},
	editor = {Grimme, Christian and Preuss, Mike and Takes, Frank W. and Waldherr, Annie},
	year = {2020},
	keywords = {Disinformation, Media content, Online recommendation systems},
	pages = {129--149},
}

@article{zheng_ai_2020,
	title = {The {AI} {Economist}: {Improving} {Equality} and {Productivity} with {AI}-{Driven} {Tax} {Policies}},
	abstract = {Tackling real-world socio-economic challenges requires designing and testing economic policies. However, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. In this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. We propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. Our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. We make four main contributions. First, we present an economic simulation environment that features competitive pressures and market dynamics. We validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. Second, we show that AI-driven tax policies improve the trade-off between equality and productivity by 16\% over baseline policies, including the prominent Saez tax framework. Third, we showcase several emergent features: AI-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. Moreover, AI-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are also effective when used in experiments with human participants. In experiments conducted on MTurk, an AI tax policy provides an equality-productivity trade-off that is similar to that provided by the Saez framework along with higher inverse-income weighted social welfare.},
	urldate = {2020-07-01},
	journal = {arXiv:2004.13332},
	author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Naik, Nikhil and Gruesbeck, Melvin and Parkes, David C. and Socher, Richard},
	month = apr,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Economics - General Economics, Statistics - Machine Learning, economist, rl, taxes},
}

@inproceedings{everitt_agent_2021,
	title = {Agent {Incentives}: {A} {Causal} {Perspective}},
	abstract = {We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.},
	urldate = {2021-08-15},
	author = {Everitt, Tom and Carey, Ryan and Langlois, Eric and Ortega, Pedro A. and Legg, Shane},
	month = mar,
	year = {2021},
	note = {arXiv: 2102.01685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{frongillo_efficient_2021,
	address = {New York, NY, USA},
	series = {{EC} '21},
	title = {Efficient {Competitions} and {Online} {Learning} with {Strategic} {Forecasters}},
	abstract = {Winner-take-all competitions in forecasting and machine-learning suffer from distorted incentives. [23] identified this problem and proposed ELF, a truthful mechanism to select a winner. We show that, from a pool of n forecasters, ELF requires Θ(nłog n) events or test data points to select a near-optimal forecaster with high probability. We then show that standard online learning algorithms select an ε-optimal forecaster using only O(łog(n) / ε2) events, by way of a strong approximate-truthfulness guarantee. This bound matches the best possible even in the nonstrategic setting. We then apply these mechanisms to obtain the first no-regret guarantee for non-myopic strategic experts.},
	urldate = {2021-08-17},
	booktitle = {Proceedings of the 22nd {ACM} {Conference} on {Economics} and {Computation}},
	publisher = {Association for Computing Machinery},
	author = {Frongillo, Rafael and Gomez, Robert and Thilagar, Anish and Waggoner, Bo},
	month = jul,
	year = {2021},
	keywords = {forecasting competition, online learning, strategic experts},
	pages = {479--496},
}

@article{neyman_binary_2021,
	title = {Binary {Scoring} {Rules} that {Incentivize} {Precision}},
	abstract = {All proper scoring rules incentivize an expert to predict {\textbackslash}emph\{accurately\} (report their true estimate), but not all proper scoring rules equally incentivize {\textbackslash}emph\{precision\}. Rather than treating the expert's belief as exogenously given, we consider a model where a rational expert can endogenously refine their belief by repeatedly paying a fixed cost, and is incentivized to do so by a proper scoring rule. Specifically, our expert aims to predict the probability that a biased coin flipped tomorrow will land heads, and can flip the coin any number of times today at a cost of \$c\$ per flip. Our first main result defines an {\textbackslash}emph\{incentivization index\} for proper scoring rules, and proves that this index measures the expected error of the expert's estimate (where the number of flips today is chosen adaptively to maximize the predictor's expected payoff). Our second main result finds the unique scoring rule which optimizes the incentivization index over all proper scoring rules. We also consider extensions to minimizing the \${\textbackslash}ell{\textasciicircum}\{th\}\$ moment of error, and again provide an incentivization index and optimal proper scoring rule. In some cases, the resulting scoring rule is differentiable, but not infinitely differentiable. In these cases, we further prove that the optimum can be uniformly approximated by polynomial scoring rules. Finally, we compare common scoring rules via our measure, and include simulations confirming the relevance of our measure even in domains outside where it provably applies.},
	urldate = {2021-07-23},
	journal = {Proceedings of the 22nd ACM Conference on Economics and Computation},
	author = {Neyman, Eric and Noarov, Georgy and Weinberg, S. Matthew},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Computer Science and Game Theory},
	pages = {718--733},
}

@incollection{grice_logic_1975,
	title = {Logic and {Conversation}},
	booktitle = {Speech {Acts}},
	publisher = {Brill},
	author = {Grice, Herbert P},
	year = {1975},
	pages = {41--58},
}

@article{armstrong_defining_1980,
	title = {Defining and {Measuring} {Deception} in {Advertising}: {A} {Review} and {Evaluation}},
	volume = {3},
	issn = {0163-3392},
	shorttitle = {Defining and {Measuring} {Deception} in {Advertising}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01633392.1980.10505292},
	doi = {10.1080/01633392.1980.10505292},
	abstract = {This article reviews and evaluates the growing marketing literature on defining and measuring deception in advertising. It develops criteria for evaluating deception measurement techniques and applies them to methods which have been used in empirical studies. The issue of deception standards is also discussed.},
	number = {1},
	urldate = {2021-12-07},
	journal = {Current Issues and Research in Advertising},
	author = {Armstrong, Gary M. and Gurol, Metin N. and Russ, Frederick A.},
	month = mar,
	year = {1980},
	note = {Publisher: Routledge
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01633392.1980.10505292},
	pages = {17--39},
}

@incollection{posner_semantics_1980,
	address = {Dordrecht},
	series = {Texts and {Studies} in {Linguistics} and {Philosophy}},
	title = {Semantics and {Pragmatics} of {Sentence} {Connectives} in {Natural} {Language}},
	isbn = {978-94-009-8964-1},
	url = {https://doi.org/10.1007/978-94-009-8964-1_8},
	abstract = {One need be neither a diplomat nor a lady to use the word perhaps to mean ‘yes’ one time and ‘no’ another. But what is the meaning of a word like perhaps if everyone can make it mean either ‘yes’ or ‘no’ as he pleases? Can one in any sense talk about a fixed word meaning here? But if not, what is it Voltaire is telling us when he maintains that the diplomat’s uttering “yes” as well as the lady’s uttering “no” means ‘perhaps’?},
	language = {en},
	urldate = {2021-12-23},
	booktitle = {Speech {Act} {Theory} and {Pragmatics}},
	publisher = {Springer Netherlands},
	author = {Posner, Roland},
	editor = {Searle, John R. and Kiefer, Ferenc and Bierwisch, Manfred},
	year = {1980},
	doi = {10.1007/978-94-009-8964-1_8},
	keywords = {Literal Meaning, Semantic Feature, Verbal Communication, Verbal Expression, Word Meaning},
	pages = {169--203},
}

@article{mccornack_information_1992,
	title = {Information {Manipulation} {Theory}},
	volume = {59},
	issn = {0363-7751},
	url = {https://doi.org/10.1080/03637759209376245},
	doi = {10.1080/03637759209376245},
	abstract = {One way of thinking about how deceptive messages are generated is in terms of how the information that interactants possess is manipulated within the messages that they produce. Information Manipulation Theory suggests that deceptive messages function deceptively because they covertly violate the principles that govern conversational exchanges. Given that conversational interactants possess assumptions regarding the quantity, quality, manner, and relevance of information that should be presented, it is possible for speakers to exploit any or all of these assumptions by manipulating the information that they possess so as to mislead listeners. By examining various message examples, it is demonstrated that IMT helps to reconcile previous disagreement about the properties of deceptive messages.},
	number = {1},
	urldate = {2021-12-07},
	journal = {Communication Monographs},
	author = {McCornack, Steven A.},
	month = mar,
	year = {1992},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/03637759209376245},
	pages = {1--16},
}

@incollection{horn_implicature_2008,
	title = {Implicature},
	volume = {26},
	booktitle = {The {Handbook} of {Pragmatics}},
	publisher = {John Wiley \& Sons},
	author = {Horn, Laurence},
	year = {2008},
}

@inproceedings{harabagiu_testing_1996,
	title = {Testing {Gricean} {Constraints} on a {Wordnet}-based {Coherence} {Evaluation} {System}},
	booktitle = {Working {Notes} of the {AAAI}-96 {Spring} {Symposium} on {Computational} {Approaches} to {Interpreting} and {Generating} {Conversational} {Implicature}},
	publisher = {Citeseer},
	author = {Harabagiu, Sanda and Moldovan, Dan and Yukawa, Takashi},
	year = {1996},
	pages = {31--38},
}

@article{sauri_are_2012,
	title = {Are {You} {Sure} {That} {This} {Happened}? {Assessing} the {Factuality} {Degree} of {Events} in {Text}},
	volume = {38},
	issn = {0891-2017},
	shorttitle = {Are {You} {Sure} {That} {This} {Happened}?},
	url = {https://doi.org/10.1162/COLI_a_00096},
	doi = {10.1162/COLI_a_00096},
	abstract = {Identifying the veracity, or factuality, of event mentions in text is fundamental for reasoning about eventualities in discourse. Inferences derived from events judged as not having happened, or as being only possible, are different from those derived from events evaluated as factual. Event factuality involves two separate levels of information. On the one hand, it deals with polarity, which distinguishes between positive and negative instantiations of events. On the other, it has to do with degrees of certainty (e.g., possible, probable), an information level generally subsumed under the category of epistemic modality. This article aims at contributing to a better understanding of how event factuality is articulated in natural language. For that purpose, we put forward a linguistic-oriented computational model which has at its core an algorithm articulating the effect of factuality relations across levels of syntactic embedding. As a proof of concept, this model has been implemented in De Facto, a factuality profiler for eventualities mentioned in text, and tested against a corpus built specifically for the task, yielding an F1 of 0.70 (macro-averaging) and 0.80 (micro-averaging). These two measures mutually compensate for an over-emphasis present in the other (either on the lesser or greater populated categories), and can therefore be interpreted as the lower and upper bounds of the De Facto's performance.},
	number = {2},
	urldate = {2021-12-10},
	journal = {Computational Linguistics},
	author = {Saurí, Roser and Pustejovsky, James},
	month = jun,
	year = {2012},
	pages = {261--299},
}

@book{bostrom_superintelligence_2014,
	address = {Oxford},
	edition = {1st},
	title = {Superintelligence: {Paths}, {Dangers}, {Strategies}},
	publisher = {Oxford University Press},
	author = {Bostrom, Nick},
	year = {2014},
}

@article{mccornack_information_2014,
	title = {Information {Manipulation} {Theory} 2: {A} {Propositional} {Theory} of {Deceptive} {Discourse} {Production}},
	volume = {33},
	issn = {0261-927X},
	shorttitle = {Information {Manipulation} {Theory} 2},
	url = {https://doi.org/10.1177/0261927X14534656},
	doi = {10.1177/0261927X14534656},
	abstract = {Information Manipulation Theory 2 (IMT2) is a propositional theory of deceptive discourse production that conceptually frames deception as involving the covert manipulation of information along multiple dimensions and as a contextual problem-solving activity driven by the desire for quick, efficient, and viable communicative solutions. IMT2 is rooted in linguistics, cognitive neuroscience, speech production, and artificial intelligence. Synthesizing these literatures, IMT2 posits a central premise with regard to deceptive discourse production and 11 empirically testable (that is, falsifiable) propositions deriving from this premise. These propositions are grouped into three propositional sets: intentional states (IS), cognitive load (CL), and information manipulation (IM). The IS propositions pertain to the nature and temporal placement of deceptive volition, in relation to speech production. The CL propositions clarify the interrelationship between load, discourse, and context. The IM propositions identify the specific conditions under which various forms of information manipulation will (and will not) occur.},
	language = {en},
	number = {4},
	urldate = {2021-12-07},
	journal = {Journal of Language and Social Psychology},
	author = {McCornack, Steven A. and Morrison, Kelly and Paik, Jihyun Esther and Wisner, Amy M. and Zhu, Xun},
	month = sep,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	keywords = {deception, deceptive message production, information manipulation},
	pages = {348--377},
}

@article{bhagat_what_2013,
	title = {What {Is} a {Paraphrase}?},
	volume = {39},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/COLI_a_00166},
	doi = {10.1162/COLI_a_00166},
	abstract = {Paraphrases are sentences or phrases that convey the same meaning using different wording. Although the logical definition of paraphrases requires strict semantic equivalence, linguistics accepts a broader, approximate, equivalence—thereby allowing far more examples of “quasi-paraphrase.” But approximate equivalence is hard to define. Thus, the phenomenon of paraphrases, as understood in linguistics, is difficult to characterize. In this article, we list a set of 25 operations that generate quasi-paraphrases. We then empirically validate the scope and accuracy of this list by manually analyzing random samples of two publicly available paraphrase corpora. We provide the distribution of naturally occurring quasi-paraphrases in English text.},
	number = {3},
	urldate = {2022-02-14},
	journal = {Computational Linguistics},
	author = {Bhagat, Rahul and Hovy, Eduard},
	month = sep,
	year = {2013},
	pages = {463--472},
}

@inproceedings{qwaider_trentoteam_2017,
	address = {Vancouver, Canada},
	title = {{TrentoTeam} at {SemEval}-2017 {Task} 3: {An} application of {Grice} {Maxims} in {Ranking} {Community} {Question} {Answers}},
	shorttitle = {{TrentoTeam} at {SemEval}-2017 {Task} 3},
	url = {https://aclanthology.org/S17-2043},
	doi = {10.18653/v1/S17-2043},
	abstract = {In this paper we present the Tren-toTeam system which participated to thetask 3 at SemEval-2017 (Nakov et al.,2017).We concentrated our work onapplying Grice Maxims(used in manystate-of-the-art Machine learning applica-tions(Vogel et al., 2013; Kheirabadiand Aghagolzadeh, 2012; Dale and Re-iter, 1995; Franke, 2011)) to ranking an-swers of a question by answers relevancy.Particularly, we created a ranker systembased on relevancy scores, assigned by 3main components: Named entity recogni-tion, similarity score, sentiment analysis.Our system obtained a comparable resultsto Machine learning systems.},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2017)},
	publisher = {Association for Computational Linguistics},
	author = {Qwaider, Mohammed R. H. and Freihat, Abed Alhakim and Giunchiglia, Fausto},
	month = aug,
	year = {2017},
	pages = {271--274},
}

@article{meibauer_linguistics_2018,
	title = {The {Linguistics} of {Lying}},
	volume = {4},
	url = {https://doi.org/10.1146/annurev-linguistics-011817-045634},
	doi = {10.1146/annurev-linguistics-011817-045634},
	abstract = {This review deals with the communicative act of lying from a linguistic point of view, linguistics comprising both grammar and pragmatics. Integrating findings from the philosophy of language and from psychology, I show that the potential for lying is rooted in the language system. The tasks of providing an adequate definition of lying and of distinguishing lying from other concepts of deception (such as bald-faced lying and bullshitting) can be solved when interfaces between grammar and pragmatics are taken into account and when experimental results are used to narrow down theoretical approaches. Assuming a broadly neo-Gricean background, this review focuses on four theoretical topics: the role of the truth in lying, the scalarity and imprecision of lying, the speaker's intent to deceive, and the possibility of producing deceptive implicatures. I also briefly discuss questions of lying and neuroscience, the acquisition of lying, and prosocial and cross-cultural contexts of lying.},
	number = {1},
	journal = {Annual Review of Linguistics},
	author = {Meibauer, Jörg},
	year = {2018},
	note = {\_eprint: https://doi.org/10.1146/annurev-linguistics-011817-045634},
	pages = {357--375},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2021-12-07},
	journal = {arXiv:2005.14165 [cs]},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.14165
version: 4},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{rashkin_event2mind_2018,
	address = {Melbourne, Australia},
	title = {{Event2Mind}: {Commonsense} {Inference} on {Events}, {Intents}, and {Reactions}},
	shorttitle = {{Event2Mind}},
	url = {https://aclanthology.org/P18-1043},
	doi = {10.18653/v1/P18-1043},
	abstract = {We investigate a new commonsense inference task: given an event described in a short free-form text (“X drinks coffee in the morning”), a system reasons about the likely intents (“X wants to stay awake”) and reactions (“X feels alert”) of the event's participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rashkin, Hannah and Sap, Maarten and Allaway, Emily and Smith, Noah A. and Choi, Yejin},
	month = jul,
	year = {2018},
	pages = {463--473},
}

@inproceedings{hossain_analysis_2020,
	address = {Online},
	title = {An {Analysis} of {Natural} {Language} {Inference} {Benchmarks} through the {Lens} of {Negation}},
	url = {https://aclanthology.org/2020.emnlp-main.732},
	doi = {10.18653/v1/2020.emnlp-main.732},
	abstract = {Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.},
	urldate = {2022-02-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hossain, Md Mosharaf and Kovatchev, Venelin and Dutta, Pranoy and Kao, Tiffany and Wei, Elizabeth and Blanco, Eduardo},
	month = nov,
	year = {2020},
	pages = {9106--9118},
}

@inproceedings{kassner_negated_2020,
	address = {Online},
	title = {Negated and {Misprimed} {Probes} for {Pretrained} {Language} {Models}: {Birds} {Can} {Talk}, {But} {Cannot} {Fly}},
	shorttitle = {Negated and {Misprimed} {Probes} for {Pretrained} {Language} {Models}},
	url = {https://aclanthology.org/2020.acl-main.698},
	doi = {10.18653/v1/2020.acl-main.698},
	abstract = {Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (``Birds cannot [MASK]”) and non-negated (``Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (``Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kassner, Nora and Schütze, Hinrich},
	month = jul,
	year = {2020},
	pages = {7811--7818},
}

@article{ettinger_what_2020,
	title = {What {BERT} {Is} {Not}: {Lessons} from a {New} {Suite} of {Psycholinguistic} {Diagnostics} for {Language} {Models}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {What {BERT} {Is} {Not}},
	url = {https://doi.org/10.1162/tacl_a_00298},
	doi = {10.1162/tacl_a_00298},
	abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
	urldate = {2022-02-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Ettinger, Allyson},
	month = jan,
	year = {2020},
	pages = {34--48},
}

@inproceedings{jeretic_are_2020,
	address = {Online},
	title = {Are {Natural} {Language} {Inference} {Models} {IMPPRESsive}? {Learning} {IMPlicature} and {PRESupposition}},
	shorttitle = {Are {Natural} {Language} {Inference} {Models} {IMPPRESsive}?},
	url = {https://aclanthology.org/2020.acl-main.768},
	doi = {10.18653/v1/2020.acl-main.768},
	abstract = {Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by “some” as entailments. For some presupposition triggers like “only”, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.},
	urldate = {2021-12-28},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jeretic, Paloma and Warstadt, Alex and Bhooshan, Suvrat and Williams, Adina},
	month = jul,
	year = {2020},
	pages = {8690--8705},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-074.html},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {140},
	urldate = {2021-12-30},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
}

@inproceedings{ribeiro_beyond_2020,
	address = {Online},
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} {Models} with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {https://aclanthology.org/2020.acl-main.442},
	doi = {10.18653/v1/2020.acl-main.442},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	month = jul,
	year = {2020},
	pages = {4902--4912},
}

@inproceedings{peskov_it_2020,
	address = {Online},
	title = {It {Takes} {Two} to {Lie}: {One} to {Lie}, and {One} to {Listen}},
	shorttitle = {It {Takes} {Two} to {Lie}},
	url = {https://aclanthology.org/2020.acl-main.353},
	doi = {10.18653/v1/2020.acl-main.353},
	abstract = {Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Peskov, Denis and Cheng, Benny and Elgohary, Ahmed and Barrow, Joe and Danescu-Niculescu-Mizil, Cristian and Boyd-Graber, Jordan},
	month = jul,
	year = {2020},
	pages = {3811--3854},
}

@article{warstadt_blimp_2020,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	volume = {8},
	shorttitle = {{BLiMP}},
	url = {https://aclanthology.org/2020.tacl-1.25},
	doi = {10.1162/tacl_a_00321},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
	urldate = {2021-12-15},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	year = {2020},
	pages = {377--392},
}

@article{askell_general_2021,
	title = {A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}},
	url = {http://arxiv.org/abs/2112.00861},
	abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
	urldate = {2021-12-07},
	journal = {arXiv:2112.00861 [cs]},
	author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.00861},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{blodgett_stereotyping_2021,
	address = {Online},
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://aclanthology.org/2021.acl-long.81},
	doi = {10.18653/v1/2021.acl-long.81},
	abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	month = aug,
	year = {2021},
	pages = {1004--1015},
}

@article{dev_what_2021,
	title = {What do {Bias} {Measures} {Measure}?},
	url = {http://arxiv.org/abs/2108.03362},
	abstract = {Natural Language Processing (NLP) models propagate social biases about protected attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While many existing works propose bias evaluation methodologies for different tasks, there remains a need to cohesively understand what biases and normative harms each of these measures captures and how different measures compare. To address this gap, this work presents a comprehensive survey of existing bias measures in NLP as a function of the associated NLP tasks, metrics, datasets, and social biases and corresponding harms. This survey also organizes metrics into different categories to present advantages and disadvantages. Finally, we propose a documentation standard for bias measures to aid their development, categorization, and appropriate usage.},
	urldate = {2021-12-07},
	journal = {arXiv:2108.03362 [cs]},
	author = {Dev, Sunipa and Sheng, Emily and Zhao, Jieyu and Sun, Jiao and Hou, Yu and Sanseverino, Mattie and Kim, Jiin and Peng, Nanyun and Chang, Kai-Wei},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.03362
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{elazar_measuring_2021,
	title = {Measuring and {Improving} {Consistency} in {Pretrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2102.01017},
	abstract = {Consistency of a model -- that is, the invariance of its behavior under meaning-preserving alternations in its input -- is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor -- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.},
	urldate = {2022-02-14},
	journal = {arXiv:2102.01017 [cs]},
	author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Schütze, Hinrich and Goldberg, Yoav},
	month = may,
	year = {2021},
	note = {arXiv: 2102.01017},
	keywords = {Computer Science - Computation and Language},
}

@article{gebru_datasheets_2021,
	title = {Datasheets for {Datasets}},
	url = {http://arxiv.org/abs/1803.09010},
	abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
	urldate = {2021-12-07},
	journal = {arXiv:1803.09010 [cs]},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
	month = dec,
	year = {2021},
	note = {arXiv: 1803.09010
version: 8},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
}

@inproceedings{hovy_importance_2021,
	address = {Online},
	title = {The {Importance} of {Modeling} {Social} {Factors} of {Language}: {Theory} and {Practice}},
	shorttitle = {The {Importance} of {Modeling} {Social} {Factors} of {Language}},
	url = {https://aclanthology.org/2021.naacl-main.49},
	doi = {10.18653/v1/2021.naacl-main.49},
	abstract = {Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language's social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Yang, Diyi},
	month = jun,
	year = {2021},
	pages = {588--602},
}

@article{jiang_he_2021,
	title = {He {Thinks} {He} {Knows} {Better} than the {Doctors}: {BERT} for {Event} {Factuality} {Fails} on {Pragmatics}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {He {Thinks} {He} {Knows} {Better} than the {Doctors}},
	url = {https://doi.org/10.1162/tacl_a_00414},
	doi = {10.1162/tacl_a_00414},
	abstract = {We investigate how well BERT performs on predicting factuality in several existing English datasets, encompassing various linguistic constructions. Although BERT obtains a strong performance on most datasets, it does so by exploiting common surface patterns that correlate with certain factuality labels, and it fails on instances where pragmatic reasoning is necessary. Contrary to what the high performance suggests, we are still far from having a robust system for factuality prediction.},
	urldate = {2021-12-15},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Jiang, Nanjiang and de Marneffe, Marie-Catherine},
	month = oct,
	year = {2021},
	pages = {1081--1097},
}

@article{hendrycks_unsolved_2021,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and reducing hazards in deployment ("External Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2021-12-07},
	journal = {arXiv:2109.13916 [cs]},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = oct,
	year = {2021},
	note = {arXiv: 2109.13916},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{kenton_alignment_2021,
	title = {Alignment of {Language} {Agents}},
	url = {http://arxiv.org/abs/2103.14659},
	abstract = {For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.},
	urldate = {2021-12-07},
	journal = {arXiv:2103.14659 [cs]},
	author = {Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14659},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{khayrallah_measuring_2021,
	address = {Online},
	title = {Measuring the `{I} don't know' {Problem} through the {Lens} of {Gricean} {Quantity}},
	url = {https://aclanthology.org/2021.naacl-main.450},
	doi = {10.18653/v1/2021.naacl-main.450},
	abstract = {We consider the intrinsic evaluation of neural generative dialog models through the lens of Grice's Maxims of Conversation (1975). Based on the maxim of Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to diagnose the `I don't know' problem, in which a dialog system produces generic responses. The linguistically motivated RUQ diagnostic compares the model score of a generic response to that of the reference response. We find that for reasonable baseline models, `I don't know' is preferred over the reference the majority of the time, but this can be reduced to less than 5\% with hyperparameter tuning. RUQ allows for the direct analysis of the `I don't know' problem, which has been addressed but not analyzed by prior work.},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Khayrallah, Huda and Sedoc, João},
	month = jun,
	year = {2021},
	pages = {5659--5670},
}

@article{lin_truthfulqa_2021,
	title = {{TruthfulQA}: {Measuring} {How} {Models} {Mimic} {Human} {Falsehoods}},
	shorttitle = {{TruthfulQA}},
	url = {http://arxiv.org/abs/2109.07958},
	abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17\% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
	urldate = {2021-12-07},
	journal = {arXiv:2109.07958 [cs]},
	author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.07958},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{pandia_pragmatic_2021,
	address = {Online},
	title = {Pragmatic competence of pre-trained language models through the lens of discourse connectives},
	url = {https://aclanthology.org/2021.conll-1.29},
	abstract = {As pre-trained language models (LMs) continue to dominate NLP, it is increasingly important that we understand the depth of language capabilities in these models. In this paper, we target pre-trained LMs' competence in pragmatics, with a focus on pragmatics relating to discourse connectives. We formulate cloze-style tests using a combination of naturally-occurring data and controlled inputs drawn from psycholinguistics. We focus on testing models' ability to use pragmatic cues to predict discourse connectives, models' ability to understand implicatures relating to connectives, and the extent to which models show humanlike preferences regarding temporal dynamics of connectives. We find that although models predict connectives reasonably well in the context of naturally-occurring data, when we control contexts to isolate high-level pragmatic cues, model sensitivity is much lower. Models also do not show substantial humanlike temporal preferences. Overall, the findings suggest that at present, dominant pre-training paradigms do not result in substantial pragmatic competence in our models.},
	urldate = {2021-12-28},
	booktitle = {Proceedings of the 25th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Pandia, Lalchand and Cong, Yan and Ettinger, Allyson},
	month = nov,
	year = {2021},
	pages = {367--379},
}

@inproceedings{sheng_societal_2021,
	address = {Online},
	title = {Societal {Biases} in {Language} {Generation}: {Progress} and {Challenges}},
	shorttitle = {Societal {Biases} in {Language} {Generation}},
	url = {https://aclanthology.org/2021.acl-long.330},
	doi = {10.18653/v1/2021.acl-long.330},
	abstract = {Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.},
	urldate = {2021-12-07},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sheng, Emily and Chang, Kai-Wei and Natarajan, Prem and Peng, Nanyun},
	month = aug,
	year = {2021},
	pages = {4275--4293},
}

@article{tarunesh_lonli_2021,
	title = {{LoNLI}: {An} {Extensible} {Framework} for {Testing} {Diverse} {Logical} {Reasoning} {Capabilities} for {NLI}},
	shorttitle = {{LoNLI}},
	url = {http://arxiv.org/abs/2112.02333},
	abstract = {Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU). In this work, we propose an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test-bench (363 templates, 363k examples) and an associated framework that offers following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning), 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enable us to control for artifacts and biases. The inherited power of automated test case instantiation from free-form natural language templates (using CheckList), and a well-defined taxonomy of capabilities enable us to extend to (cognitively) harder test cases while varying the complexity of natural language. Through our analysis of state-of-the-art NLI systems, we observe that our benchmark is indeed hard (and non-trivial even with training on additional resources). Some capabilities stand out as harder. Further fine-grained analysis and fine-tuning experiments reveal more insights about these capabilities and the models -- supporting and extending previous observations. Towards the end we also perform an user-study, to investigate whether behavioral information can be utilised to generalize much better for some models compared to others.},
	urldate = {2021-12-17},
	journal = {arXiv:2112.02333 [cs]},
	author = {Tarunesh, Ishan and Aditya, Somak and Choudhury, Monojit},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.02333},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{everitt_understanding_2019,
	title = {Understanding {Agent} {Incentives} using {Causal} {Influence} {Diagrams}. {Part} {I}: {Single} {Action} {Settings}},
	shorttitle = {Understanding {Agent} {Incentives} using {Causal} {Influence} {Diagrams}. {Part} {I}},
	url = {http://arxiv.org/abs/1902.09980},
	abstract = {Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.},
	urldate = {2021-06-18},
	journal = {arXiv:1902.09980 [cs]},
	author = {Everitt, Tom and Ortega, Pedro A. and Barnes, Elizabeth and Legg, Shane},
	month = sep,
	year = {2019},
	note = {arXiv: 1902.09980},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.6, I.2.8},
}

@article{farquhar_path-specific_2022,
	title = {Path-{Specific} {Objectives} for {Safer} {Agent} {Incentives}},
	url = {http://arxiv.org/abs/2204.10018},
	abstract = {We present a general framework for training safe agents whose naive incentives are unsafe. As an example, manipulative or deceptive behaviour can improve rewards but should be avoided. Most approaches fail here: agents maximize expected return by any means necessary. We formally describe settings with 'delicate' parts of the state which should not be used as a means to an end. We then train agents to maximize the causal effect of actions on the expected return which is not mediated by the delicate parts of state, using Causal Influence Diagram analysis. The resulting agents have no incentive to control the delicate state. We further show how our framework unifies and generalizes existing proposals.},
	urldate = {2022-04-23},
	journal = {arXiv:2204.10018 [cs, stat]},
	author = {Farquhar, Sebastian and Carey, Ryan and Everitt, Tom},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.10018},
	keywords = {Computer Science - Artificial Intelligence, Read, Statistics - Machine Learning},
}

@article{evans_user_2021,
	title = {User {Tampering} in {Reinforcement} {Learning} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2109.04083},
	abstract = {This paper provides the first formalisation and empirical demonstration of a particular safety concern in reinforcement learning (RL)-based news and social media recommendation algorithms. This safety concern is what we call “user tampering” – a phenomenon whereby an RL-based recommender system may manipulate a media user’s opinions, preferences and beliefs via its recommendations as part of a policy to increase long-term user engagement. We provide a simulation study of a media recommendation problem constrained to the recommendation of political content, and demonstrate that a Q-learning algorithm consistently learns to exploit its opportunities to ‘polarise’ simulated ‘users’ with its early recommendations in order to have more consistent success with later recommendations catering to that polarisation. Finally, we argue that given our findings, designing an RL-based recommender system which cannot learn to exploit user tampering requires making the metric for the recommender’s success independent of observable signals of user engagement, and thus that a media recommendation system built solely with RL is necessarily either unsafe, or almost certainly commercially unviable.},
	language = {en},
	urldate = {2021-10-22},
	journal = {arXiv:2109.04083 [cs]},
	author = {Evans, Charles and Kasirzadeh, Atoosa},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.04083},
	keywords = {Computer Science - Artificial Intelligence, Micah},
}

@article{carroll_estimating_2022,
	title = {Estimating and {Penalizing} {Induced} {Preference} {Shifts} in {Recommender} {Systems}},
	volume = {162},
	abstract = {The content that a recommender system (RS) shows to users inﬂuences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce speciﬁc internal states in users. Even more, systems trained via longhorizon optimization will have direct incentives to manipulate users, e.g. shift their preferences so they are easier to satisfy. We focus on induced preference shifts in users. We argue that – before deployment – system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical policies would inﬂuence user preferences if deployed – we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such inﬂuences are manipulative or otherwise unwanted – we use the notion of “safe shifts”, that deﬁne a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed “safe”. In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.},
	language = {en},
	journal = {Proceedings of machine learning research},
	author = {Carroll, Micah and Dragan, Anca and Russell, Stuart and Hadﬁeld-Menell, Dylan},
	year = {2022},
	keywords = {Micah},
	pages = {2686--2708},
}

@inproceedings{sanna_passino_where_2021,
	address = {Ljubljana Slovenia},
	title = {Where {To} {Next}? {A} {Dynamic} {Model} of {User} {Preferences}},
	isbn = {978-1-4503-8312-7},
	shorttitle = {Where {To} {Next}?},
	url = {https://dl.acm.org/doi/10.1145/3442381.3450028},
	doi = {10.1145/3442381.3450028},
	abstract = {We consider the problem of predicting users’ preferences on online platforms. We build on recent findings suggesting that users’ preferences change over time, and that helping users expand their horizons is important in ensuring that they stay engaged. Most existing models of user preferences attempt to capture simultaneous preferences: “Users who like A tend to like B as well”. In this paper, we argue that these models fail to anticipate changing preferences. To overcome this issue, we seek to understand the structure that underlies the evolution of user preferences. To this end, we propose the Preference Transition Model (PTM), a dynamic model for user preferences towards classes of items. The model enables the estimation of transition probabilities between classes of items over time, which can be used to estimate how users’ tastes are expected to evolve based on their past history. We test our model’s predictive performance on a number of different prediction tasks on data from three different domains: music streaming, restaurant recommendations and movie recommendations, and find that it outperforms competing approaches. We then focus on a music application, and inspect the structure learned by our model. We find that the PTM uncovers remarkable regularities in users’ preference trajectories over time. We believe that these findings could inform a new generation of dynamic, diversity-enhancing recommender systems.},
	language = {en},
	urldate = {2022-12-19},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {ACM},
	author = {Sanna Passino, Francesco and Maystre, Lucas and Moor, Dmitrii and Anderson, Ashton and Lalmas, Mounia},
	month = apr,
	year = {2021},
	pages = {3210--3220},
}

@article{adomavicius_recommender_2013,
	title = {Do {Recommender} {Systems} {Manipulate} {Consumer} {Preferences}? {A} {Study} of {Anchoring} {Effects}},
	volume = {24},
	issn = {1047-7047},
	shorttitle = {Do {Recommender} {Systems} {Manipulate} {Consumer} {Preferences}?},
	url = {https://pubsonline.informs.org/doi/10.1287/isre.2013.0497},
	doi = {10.1287/isre.2013.0497},
	abstract = {Recommender systems are becoming a salient part of many e-commerce websites. Much research has focused on advancing recommendation technologies to improve accuracy of predictions, although behavioral aspects of using recommender systems are often overlooked. In our studies, we explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Studies 1 and 2 investigated user preferences for television programs across a variety of conditions, which were surveyed immediately following program viewing. Study 3 investigated the granularity of the observed effects within individual participants. Results provide strong evidence that the rating presented by a recommender system serves as an anchor for the consumer's constructed preference. Viewers' preference ratings are malleable and can be significantly influenced by the recommendation received. The effect is sensitive to the perceived reliability of a recommender system and, thus, not a purely numerical or priming-based effect. Finally, the effect of anchoring is continuous and linear, operating over a range of perturbations of the system. These general findings have a number of important implications (e.g., on recommender systems performance metrics and design, preference bias, potential strategic behavior, and trust), which are discussed.},
	number = {4},
	urldate = {2022-06-14},
	journal = {Information Systems Research},
	author = {Adomavicius, Gediminas and Bockstedt, Jesse C. and Curley, Shawn P. and Zhang, Jingjing},
	month = dec,
	year = {2013},
	note = {Publisher: INFORMS},
	keywords = {Micah, Read, anchoring effects, behavioral decision making, behavioral economics, electronic commerce, experimental research, preferences, recommender systems},
	pages = {956--975},
}

@article{adomavicius_effects_2018,
	title = {Effects of {Online} {Recommendations} on {Consumers}’ {Willingness} to {Pay}},
	volume = {29},
	issn = {1047-7047, 1526-5536},
	url = {http://pubsonline.informs.org/doi/10.1287/isre.2017.0703},
	doi = {10.1287/isre.2017.0703},
	abstract = {Recommender systems are an integral part of the online retail environment. Prior research has focused largely on computational approaches to improving recommendation accuracy, and only recently researchers have started to study their behavioral implications and potential side effects. We used three controlled experiments, in the context of purchasing digital songs, to explore the willingness-to-pay judgments of individual consumers after being shown personalized recommendations. In Study 1, we found strong evidence that randomly assigned song recommendations affected participants’ willingness to pay, even when controlling for participants’ preferences and demographics. In Study 2, participants viewed actual systemgenerated recommendations that were intentionally perturbed (introducing recommendation error) and we observed similar effects. Study 3 showed that the influence of personalized recommendations on willingness-to-pay judgments obtained even when preference uncertainty was reduced through immediate and mandatory song sampling prior to pricing. The results demonstrate the existence of important economic side effects of personalized recommender systems and inform our understanding of how system recommendations can influence our everyday preference judgments. The findings have significant implications for the design and application of recommender systems as well as for online retail practices.},
	language = {en},
	number = {1},
	urldate = {2023-02-21},
	journal = {Information Systems Research},
	author = {Adomavicius, Gediminas and Bockstedt, Jesse C. and Curley, Shawn P. and Zhang, Jingjing},
	month = mar,
	year = {2018},
	keywords = {Micah},
	pages = {84--102},
}

@article{klassen_epistemic_nodate,
	title = {Epistemic {Side} {Effects} \& {Avoiding} {Them} ({Sometimes})},
	abstract = {AI safety research has investigated the problem of negative side effects – undesirable changes made by AI systems in pursuit of an underspecified objective. However, the focus has been on physical side effects, such as a robot breaking a vase while moving. In this paper we introduce the notion of epistemic side effects, unintended changes made to the knowledge or beliefs of agents, and describe a way to avoid negative epistemic side effects in reinforcement learning, in some cases.},
	language = {en},
	author = {Klassen, Toryn Q and Alamdari, Parand Alizadeh and McIlraith, Sheila A},
	keywords = {Micah, Read},
}

@inproceedings{sanchez-corcuera_persuade_2019,
	title = {Persuade {Me}!: {A} {User}-{Based} {Recommendation} {System} {Approach}},
	shorttitle = {Persuade {Me}!},
	doi = {10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00310},
	abstract = {Recommendation systems are gaining their momentum with popular Internet platforms such as Amazon, Netflix or Spotify. As more users are joining these online consumer and entertainment sectors, the profile-based data for providing accurate just-in-time recommendations is rising thanks to strategies based on collaborative filtering or content-based metrics. However, these systems merely focus on providing the right item for the users without taking into account what would be the best strategy to suggest the movie, the product or the song (i.e. the strategy to increase the success or impact of the recommendation). Taking this research gap into consideration, this paper proposes a profile-based recommendation system that outputs a set of potential persuasive strategies that can be used with users with similar characteristics. The scope of the tailored persuasive strategies is to make office-based employees of tertiary buildings increase their pro-environmental awareness and enhance the energy efficiency at work (the dataset used on this research is specific of this sector). Throughout the paper, shreds of evidence are reported assessing the validity of the proposed system by not only providing effective mechanisms to increase the success of the recommendations but also alleviating the cold-start-problem when newcomers arrive.},
	booktitle = {2019 {IEEE} {SmartWorld}, {Ubiquitous} {Intelligence} \& {Computing}, {Advanced} \& {Trusted} {Computing}, {Scalable} {Computing} \& {Communications}, {Cloud} \& {Big} {Data} {Computing}, {Internet} of {People} and {Smart} {City} {Innovation} ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	author = {Sánchez-Corcuera, Rubén and Casado-Mansilla, Diego and Borges, Cruz E. and Lopez-De-Ipiña, Diego},
	month = aug,
	year = {2019},
	keywords = {Adaptation models, Collaboration, Feature extraction, Heuristic algorithms, Matrix decomposition, Micah, Motion pictures, Read, Recommender Systems, Persuasive Strategies, User profile, Workplace, Cold Start, Preference Recommendations, Vegetation},
	pages = {1740--1745},
}

@misc{cai_reinforcing_2023,
	title = {Reinforcing {User} {Retention} in a {Billion} {Scale} {Short} {Video} {Recommender} {System}},
	url = {http://arxiv.org/abs/2302.01724},
	abstract = {Recently, short video platforms have achieved rapid user growth by recommending interesting content to users. The objective of the recommendation is to optimize user retention, thereby driving the growth of DAU (Daily Active Users). Retention is a long-term feedback after multiple interactions of users and the system, and it is hard to decompose retention reward to each item or a list of items. Thus traditional point-wise and list-wise models are not able to optimize retention. In this paper, we choose reinforcement learning methods to optimize the retention as they are designed to maximize the long-term performance. We formulate the problem as an infinite-horizon request-based Markov Decision Process, and our objective is to minimize the accumulated time interval of multiple sessions, which is equal to improving the app open frequency and user retention. However, current reinforcement learning algorithms can not be directly applied in this setting due to uncertainty, bias, and long delay time incurred by the properties of user retention. We propose a novel method, dubbed RLUR, to address the aforementioned challenges. Both offline and live experiments show that RLUR can significantly improve user retention. RLUR has been fully launched in Kuaishou app for a long time, and achieves consistent performance improvement on user retention and DAU.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Cai, Qingpeng and Liu, Shuchang and Wang, Xueliang and Zuo, Tianyou and Xie, Wentao and Yang, Bin and Zheng, Dong and Jiang, Peng and Gai, Kun},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01724 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Micah},
}

@article{cremonesi_investigating_2012,
	title = {Investigating the {Persuasion} {Potential} of {Recommender} {Systems} from a {Quality} {Perspective}: {An} {Empirical} {Study}},
	volume = {2},
	issn = {2160-6455, 2160-6463},
	shorttitle = {Investigating the {Persuasion} {Potential} of {Recommender} {Systems} from a {Quality} {Perspective}},
	url = {https://dl.acm.org/doi/10.1145/2209310.2209314},
	doi = {10.1145/2209310.2209314},
	abstract = {Recommender Systems (RSs) help users search large amounts of digital contents and services by allowing them to identify the items that are likely to be more attractive or useful. RSs play an important persuasion role, as they can potentially augment the users’ trust towards in an application and orient their decisions or actions towards specific directions. This article explores the persuasiveness of RSs, presenting two vast empirical studies that address a number of research questions.
            First, we investigate if a design property of RSs, defined by the statistically measured quality of algorithms, is a reliable predictor of their potential for persuasion. This factor is measured in terms of perceived quality, defined by the overall satisfaction, as well as by how users judge the accuracy and novelty of recommendations. For our purposes, we designed an empirical study involving 210 subjects and implemented seven full-sized versions of a commercial RS, each one using the same interface and dataset (a subset of Netflix), but each with a different recommender algorithm. In each experimental configuration we computed the statistical quality (recall and F-measures) and collected data regarding the quality perceived by 30 users. The results show us that algorithmic attributes are less crucial than we might expect in determining the user’s perception of an RS’s quality, and suggest that the user’s judgment and attitude towards a recommender are likely to be more affected by factors related to the user experience.
            Second, we explore the persuasiveness of RSs in the context of large interactive TV services. We report a study aimed at assessing whether measurable persuasion effects (e.g., changes of shopping behavior) can be achieved through the introduction of a recommender. Our data, collected for more than one year, allow us to conclude that, (1) the adoption of an RS can affect both the lift factor and the conversion rate, determining an increased volume of sales and influencing the user’s decision to actually buy one of the recommended products, (2) the introduction of an RS tends to diversify purchases and orient users towards less obvious choices (the long tail), and (3) the perceived novelty of recommendations is likely to be more influential than their perceived accuracy.
            Overall, the results of these studies improve our understanding of the persuasion phenomena induced by RSs, and have implications that can be of interest to academic scholars, designers, and adopters of this class of systems.},
	language = {en},
	number = {2},
	urldate = {2023-02-07},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Cremonesi, Paolo and Garzotto, Franca and Turrin, Roberto},
	month = jun,
	year = {2012},
	keywords = {Micah, test},
	pages = {1--41},
}

@article{milli_optimizing_2020,
	title = {From {Optimizing} {Engagement} to {Measuring} {Value}},
	url = {http://arxiv.org/abs/2008.12623},
	abstract = {Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of "value" that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of "value".},
	urldate = {2020-09-01},
	journal = {arXiv:2008.12623 [cs, stat]},
	author = {Milli, Smitha and Belli, Luca and Hardt, Moritz},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.12623},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Micah, Read, Statistics - Machine Learning},
}

@misc{thorburn_how_2022,
	title = {How {Platform} {Recommenders} {Work}},
	url = {https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a},
	abstract = {A recommender system (or simply ‘recommender’) is an algorithm that takes a large set of items and determines which of those to display to…},
	language = {en},
	urldate = {2023-02-17},
	journal = {Understanding Recommenders},
	author = {Thorburn, Luke},
	month = nov,
	year = {2022},
	keywords = {Micah, a},
}

@misc{thorburn_is_2022,
	title = {Is {Optimizing} for {Engagement} {Changing} {Us}?},
	url = {https://medium.com/understanding-recommenders/is-optimizing-for-engagement-changing-us-9d0ddfb0c65e},
	abstract = {— Luke Thorburn, Jonathan Stray, Priyanjana Bengani},
	language = {en},
	urldate = {2023-02-17},
	journal = {Understanding Recommenders},
	author = {Thorburn, Luke},
	month = nov,
	year = {2022},
	keywords = {Micah},
}

@misc{stray_building_2022,
	title = {Building {Human} {Values} into {Recommender} {Systems}: {An} {Interdisciplinary} {Synthesis}},
	shorttitle = {Building {Human} {Values} into {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2207.10192},
	abstract = {Recommender systems are the algorithms which select, filter, and personalize content across many of the worlds largest platforms and apps. As such, their positive and negative effects on individuals and on societies have been extensively theorized and studied. Our overarching question is how to ensure that recommender systems enact the values of the individuals and societies that they serve. Addressing this question in a principled fashion requires technical knowledge of recommender design and operation, and also critically depends on insights from diverse fields including social science, ethics, economics, psychology, policy and law. This paper is a multidisciplinary effort to synthesize theory and practice from different perspectives, with the goal of providing a shared language, articulating current design approaches, and identifying open problems. It is not a comprehensive survey of this large space, but a set of highlights identified by our diverse author cohort. We collect a set of values that seem most relevant to recommender systems operating across different domains, then examine them from the perspectives of current industry practice, measurement, product design, and policy approaches. Important open problems include multi-stakeholder processes for defining values and resolving trade-offs, better values-driven measurements, recommender controls that people use, non-behavioral algorithmic feedback, optimization for long-term outcomes, causal inference of recommender effects, academic-industry research collaborations, and interdisciplinary policy-making.},
	urldate = {2022-08-05},
	publisher = {arXiv},
	author = {Stray, Jonathan and Halevy, Alon and Assar, Parisa and Hadfield-Menell, Dylan and Boutilier, Craig and Ashar, Amar and Beattie, Lex and Ekstrand, Michael and Leibowicz, Claire and Sehat, Connie Moon and Johansen, Sara and Kerlin, Lianne and Vickrey, David and Singh, Spandana and Vrijenhoek, Sanne and Zhang, Amy and Andrus, McKane and Helberger, Natali and Proutskova, Polina and Mitra, Tanushree and Vasan, Nina},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10192 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Social and Information Networks, H.3.3, J.4, K.4.2, Micah},
}
