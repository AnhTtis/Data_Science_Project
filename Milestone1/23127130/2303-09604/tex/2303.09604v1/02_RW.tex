\section{Related work}
\label{sec:related}

\mypara{Learning fonts}
There are numerous techniques to study, design, and stylize fonts. 
%Kautz~\cite{campbell2014learning} utilized GP-LVM algorithm to learn a font manifold from  the polyline representation of glyph outlines.
Campbell and Kautz~\cite{campbell2014learning} utilized an algorithm to learn a font manifold from  the polyline representation of glyph outlines. By exploring this manifold, new fonts can be obtained, or existing fonts can be interpolated to achieve a desired effect. 
Balashova et al.~\cite{balashova2019learning} proposed an approach that uses a stroke-based geometric model for glyphs and a fitting procedure to reparametrize arbitrary fonts to the representation, which is again estimated through a manifold learning technique that estimates a low-dimensional font space.
More recently, Wang et al.~\cite{wang2021deepvecfont} proposed a dual-modality learning scheme to synthesize vector fonts, which are refined with glyph images using differentiable rasterization.

\mypara{Font stylization}
Efforts have also been made to stylize fonts to enhance their artistic and aesthetic appeal.
For instance, Azadi et al.~\cite{azadi2018multi} proposed a conditional-GAN~\cite{mirza2014conditional,isola2017image} to generate glyph images with different font and texture styles that match a given template.
Berio et al. ~\cite{berio2022strokestyles} proposed to segment a fontâ€™s glyphs into a set of overlapping and intersecting strokes to generate artistic stylizations.
To accomplish context-aware text image stylization and synthesis, Yang et al.~\cite{yang2017awesome,yang2018context1,yang2018context2} proposed a style transfer method with the ability to preserve legibility.
Nonetheless, artistic typography surpasses mere manipulation of fonts and often entails imbuing letters with a semantic meaning.

\mypara{Semantic and artistic typography}
Semantic typography involves adding certain elements to a text to emphasize certain aspects, communicate a message, or highlight a property. There have been several endeavors to integrate multiple elements in the creation of a logo, text, or other design elements. One example of this is through the use of collage-based techniques to fill a letter by incorporating semantic elements within it ~\cite{kwan2016pyramid,saputra2019improved,chen2019manufacturable,zhang2022creating}.
The concept of legible calligrams~\cite{zou2016legible} focuses on solving an inverse problem by placing a word or group of letters into a semantic shape. When our approach is applied to an entire word as the input (as shown in Figure~\ref{fig:res_qual_multi_letter}), it may yield results that resemble legible calligrams.
However, our problem statement is entirely distinct, and our approach to solving it is significantly dissimilar because our method is learning-based and not limited to a predetermined template shape that dictates the letter placement and deformation.

The works that are most closely related to ours are those that attempt to alter a letter or a set of letters to achieve a specific semantic meaning, whether through replacement, deformation, or texturization.
For instance, Zhang et al. \cite{zhang2017synthesizing} propose a semi-manual technique that involves manually dividing a letter into sections, fitting semantically related shapes to those sections, and performing post-processing to eliminate artifacts. However, the effectiveness of this method is heavily reliant on the accuracy of manual letter segmentation and the use of predefined shapes, which can impact the quality of the results.
Trick or treat~\cite{tendulkar2019trick} is an attempt to replace a letter with an icon by identifying an icon that closely matches the letter from a joint embedding of letters and icons. The chosen icon is then slightly deformed to better represent the letter. However, this method requires the existence of an icon that closely resembles the letter in order to produce satisfactory results.
Instead, we argue that a more effective solution would involve learning how to \emph{generate} the desired semantic typography by creating letters or words that convey a particular semantic or aesthetic feature in a subtle yet effective manner, as depicted in Figure~\ref{fig:res_qual_single_letter_composed}.

\mypara{Text-based generative design}
Large Language Models such as BERT~\cite{devlin2018bert} significantly advance the understanding of human language, which makes text-based generation tasks much easier.
With the emergence of powerful models that can establish connections between natural language and images, such as CLIP~\cite{radford2021learning}, several downstream tasks have benefited from these models, including mesh and image editing, stylization, and generation~\cite{michel2022text2mesh,jain2022zero,wang2022clip,mildenhall2020nerf}. Of particular relevance to our work, CLIPDraw~\cite{frans2021clipdraw} aims to produce SVG-format drawings by first rasterizing them using DiffVG~\cite{li2020differentiable}, and then utilizing CLIP for evaluation.
Recently, there has been a surge in popularity of diffusion models~\cite{sohl2015diffusion, ho2020ddpm, song2020denoising, song2020improvedsd}, including stable diffusion~\cite{rombach2022high}, which has produced impressive text-to-image results. Our approach utilizes latent diffusion~\cite{rombach2022high} to encode and decode glyph and style images, and employs BERT to condition the denoising process (Figure~\ref{fig:method_pipeline}). To ensure both glyph and style images are respected, we utilize a discriminator to combine adversarial learning and diffusion, making it one of the first of its kind. In contrast to Diffusion-GAN~\cite{wang2022diffusion} which uses a discriminator to distinguish a diffused real image from a diffused fake image at all steps, our discriminator is designed to preserve glyph structures in stylized images as one component of our optimization objectives.

\mypara{Semantic Typography}

In concurrent work, Word-as-Image Semantic Typography (ST) \cite{iluz2023word2img} stylizes a letter through a semantic-aware {\em font deformation\/}; see Figure~\ref{fig:semantic_comp}. 
To guide the deformation, ST uses a pre-trained Stable Diffusion model along with losses to preserve the font structure. 
Our approach differs from ST in multiple ways. We focus on extracting salient features of a style and applying them to a glyph shape and ensure legibility using a discriminator rather than deforming a font. This allows us to incorporate multiple relevant colors to semantic and stylistic attributes in raster form, while the results produced by ST remain single color in vector form. In addition, we can stylize multiple letters together as a single shape (Figure~\ref{fig:res_qual_multi_letter}) while ST deforms each letter individually.



%provide users with multiple outputs for each style + glyph combination, and
%Our goal is to inspire artistic creation and explore the possibilities of adapting text-to-image generators for personalized user experiences.
 % (1) using DiffVG to end2end fine-tune SVG outlines (2) structure and style preserving losses via a low pass filter
%It is an end-to-end network that fine-tunes SVG images using DiffVG \cite{Li:2020:DVG}, a differentiable rasterizer for vector images. 

%They present structure and style preserving losses for a stylized output where font structure is maintained.
%Unlike ST we do not aim to deform a font, rather our aim is to extract underlying salient or representative features of a style and apply them onto a glyph shape. Therefore, we are not bound by the topology of the font. Furthermore, we have the option to provide users multiple outputs for each style + glyph combination, unlike ST which generates one stylistic output per training. We also have the flexibility to incorporate \am{multiple relevant colors to a semantic and} stylistic attributes like Picasso, pixel, sculpture, etc. Most importantly, our work is aimed at artistic creation and inspiring new ideas. We aim to fully explore the possibilities of adapting powerful text-to-image generators for personalized user experience. }


% (1) Unlike ST we are not aiming to deform a specific font. Rather our aim is to abstractly incorporate elements of a style by extracting its underlying salient or representative features onto a glyph. 
% (2) ST is bound by the topology of their single input letter. In one-font mode we are able to follow the shape of the font, but we are not bound by the topology.
% (3) We can 'theoretically' generate infinitely multiple results for each style + glyph combination, while ST works to fine-tune to one output for each combination. In this way, we give end users options to choose from.
% (4) Building on point.4 , we give users the option to include attributes like Picasso, silhouette, pixel, etc. 
% (5) Finally, and most importantly perhaps, our work is aimed at artistic creation, to inspire new ideas. By using the stable diffusion model's powerful generator, we aim to fully explore the possibilities of how it can be adapted to a personalized user experience.





%and BERT~\cite{devlin2018bert}
%Diffusion-GAN~\cite{wang2022diffusion} introduced a discriminator learning to distinguish a diffused real image from a diffused fake image at all steps. 
%The discriminator in our method, however, aims to preserve the glyph structures in stylized images, and it is only one part of our optimization goals.








%\mypara{Combining GANs with Diffusion}
%
%Diffusion-GAN~\cite{wang2022diffusion} introduced a discriminator learning to distinguish a diffused real image from a diffused fake image at all diffusion steps. Different from Diffusion-GAN, the discriminator in our method aims to preserve the glyph structures in stylized images, and it is only one part of our optimization goals.
%building connections between images from two different domains.
%in our method is built on top of the output of Diffusion Models. It 
%The discriminator in Diffusion-GAN is still the same as vanilla GAN, which moves the distributions of fake images to real (target) images.






%introduced a fully automatic method, following the steps of path generation, alignment and deformation, to generate legible compact calligrams.
%To handle this task, Kwan et al.~\cite{kwan2016pyramid} presented pyramid of arclength descriptor (PAD) for partial-shape matching to obtain compact results.
%Saputra et al.~\cite{saputra2019improved} presented RepulsionPak, a deformation-driven packing technique, to handle the situations when elements typically do not fit perfectly with each other or the target container.
%When the target shape is a boundary, Chen et al.~\cite{chen2019manufacturable} simplified the process into heuristic ellipse packing, coarse-level user interaction, and fine-level optimization of the element placement.

%Nagata and Imahori~\cite{nagata2021escherization} handles the Escherization problem which finds the most similar shape to a given goal figure that can tile the plane.
%TReAT~\cite{tendulkar2019trick} aimed to retrieve similar icons with given glyphs by measuring their distance in a learned latent space.
%Text2mesh~\cite{michel2022text2mesh}, Dream Fields~\cite{jain2022zero} and Clip-nerf~\cite{wang2022clip} tried to generate meshes or neural radiance fields~\cite{mildenhall2020nerf} of objects with the guidance of CLIP~\cite{}.