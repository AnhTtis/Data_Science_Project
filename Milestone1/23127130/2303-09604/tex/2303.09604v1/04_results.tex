\section{Results and evaluation}
\label{sec:results}

To evaluate our DS-Fusion for artistic typography generation, we present qualitative 
results with single-letter and whole-word inputs and show the effects of input fonts, style attributes,
and single- vs.~multi-font training.
%We conduct ablation studies to examine the effectiveness of the discriminator and diffusion losses.
Qualitative and quantitative comparisons are made to the closest alternatives that can be 
adapted to produce similar forms of typographies, including 
DALL-E 2~\cite{ramesh2022hierarchical}, Stable Diffusion (SD)~\cite{rombach2022high}, CLIPDraw~\cite{frans2021clipdraw}, and 
a Google search baseline. As artistic creations are often judged subjectively, we conduct
user studies for our comparative studies including those against searchable contents produced by human artists.

\vspace{7pt}

\mypara{Inputs}
%
We test our method on input style prompts and glyphs that are applicable to a variety of object categories such as those from animals, 
plants, professions, etc. The input choices reflect an intention to generate compelling artistic typographies, as well as to facilitate comparisons to 
baseline approaches and artist creations.
%In particular
Particularly, where possible, we attempt to produce results using DS-Fusion on inputs picked
in other works to ensure fairness and stress test our method.
%
On the other hand, the input fonts are randomly selected from Ubuntu built-in font library.

\vspace{7pt}


\mypara{Training details}
%
For all experiments, we use the pre-trained Latent Diffusion Model with BERT~\cite{rombach2022high},
% We keep the U-NET Encoder/Decoder and BERT frozen, while we fine-tune the denoising generator. 
An Adam optimizer with learning rate of 1e-5 is applied for the denoising generator and for the discriminator,
the learning rate is 1e-4. All the hyper parameters associated with the Latent Model are kept to their default values. 
The complete network, including the discriminator, is fine-tuned to one particular style and glyph combination. 
% For example, to generate "Cat" styled "C", we fine-tune the network on style images of "Cat" and rasterized images of glyph C. After training, the network generates multiple results with the given style and glyph combination.
% The rasterized glyph images are either in the desired font, in the case of one-font mode, or a collection of random fonts in the case of multiple-font mode. In either case, the glyphs are rasterized in random colors.  
We train the network for 800 epochs for one-font mode and 1,200 epochs for multi-font mode. On a NVIDIA GeForce RTX 3090, this takes \(\sim\)5 minutes and \(\sim\)8 minutes, respectively,
to obtain the final results.

% \rz{[RZ: Maham to complete. Emphasize fine-tune/overfit network. Training takes input prompt plus glyph. Experimentally determine
% training epochs: 500 for single-letter and 1,200 for whole-word, etc. used throughout all experiments.]}

\vspace{7pt}

\mypara{Parameters}
%
There are two free parameters in our method: the number of style images and discriminator weight $\lambda$. Both were set
experimentally and fixed throughout our experiments. We provide an ablation study in Section~\ref{subsec:ablation}.

\vspace{7pt}

\mypara{Evaluation metrics}
%
%The generated typographies are evaluated in terms of their legibility, fidelity, and artistry or creativity. Objectively, we utilize the CLIP loss 
%for \rz{both style and glyph preservation} and MSE Loss for the content word.
Objectively, we evaluate the generated typographies for their legibility via optical character recognition provided by EasyOCR~\cite{jaided2020easyocr} 
and style incorporation via CLIP. Specifically, we use affinity scores calculated by CLIP between the generated typography results and those produced with 
the prompt ``Picture of $<$style word$>$." Subjective tests to additionally assess the artistry and creativity of the results are conducted via user studies.

\subsection{Qualitative results}

\mypara{Single-letter input}
%
In Figures~\ref{fig:teaser} and \ref{fig:res_qual_single_letter_composed}, we show a sampler of compelling single-letter artistic typography results generated by our method.
Each result is produced by composing generated single-letter results with renderings of the remaining letters in the input style word. Since the generated results may have 
noisy backgrounds, we employ DeepLab~\cite{chen2017deeplab} for object segmentation to remove them.
To render the other letters, we use the same font as the stylized glyph and select a complementing color which is often the dominant color in the stylized glyph.
%
\rz{More results can be found in the supplementary material.}

\input{figs/fig_res_qual_single_letter_composed.tex}

\vspace{7pt}

\mypara{Multi-letter input}
%
Synthesizing artistic typographies from multiple glyphs is more challenging due to the added structural complexities in the inputs, as well as the more global context to account for. With a higher degree of freedom in the generative process, the tradeoff between legibility and artistry becomes harder to control.
% The method then works by reconstructing style images using diffusion loss while balancing discriminator loss to rasterized images of the content input. 
In Figure~\ref{fig:res_qual_multi_letter}, we show a sampler of results from {\em whole-word\/} stylization.

In more ways than one, our DS-Fusion demonstrates its ability to utilize all the letters of a word to convey semantic features, in a creative manner. 
Particularly compelling are the two results for the style word ``OWL''; the owls' bodies are well formed by the stylized glyphs. 
Auxiliary stylizations can also be generated to offer a global context, e.g., the tree and mountain tops, while the ship image shows the word ``SHIP"
replacing the vessel itself, even including a reflection in water. On the other hand, stylization could also be applied to individual letters, e.g., for ``PARIS."

\input{figs/fig_res_qual_multi_letter.tex}

%\subsection{Effect of Input Fonts and Style Images}

\vspace{7pt}

\mypara{Effect of input fonts}
%
Figure~\ref{fig:effect_of_fonts} shows results produced by varying the fonts of the input glyphs, for ``dragon" + $\mathscr G$(A). They highlight the ability of our method
to preserve various shape characteristics of the input, such as stroke thickness, slanting, and even small-scale details such as the accent on top of the letter stylized as dragon heads.

\input{figs/fig_effect_of_fonts.tex}

\vspace{7pt}

\mypara{Effect of style attributes}
%
%The style images are generated from Latent Diffusion model. Our results are generated using prompt ``$<$style\_word$>$ cartoon" or ``$<$style\_word$>$'' illustration". This is because images with these properties tend to have a good transferable style to text and logos.  However other prompts like further adjectives like pixel, realistic, etc can also be used. We show the effect of these prompts 
The use of style attributes in the input prompts can be an effective means to further fine-tune the stylization, as shown in Figure~\ref{fig:effect_of_styles}. 

\input{figs/fig_effect_of_styles.tex}

\vspace{7pt}

\mypara{Single- vs.~multiple-font training}
%
In single-font mode, we pick one random font out of a set of five selected fonts. During training, we change the color of the font randomly. Results of the single-font training show a high degree of  shape correlation to the input font, as shown in Figure~\ref{fig:effect_of_fonts}.

In multiple-font mode, we pick both a random font and a random color in each training step. It takes longer for the model to converge, but it can produce more abstract results, as reflected by the comparisons in Figure~\ref{fig:effect_of_multiple}. Note that we cannot control the font shape in this mode, however, it follows the general shape of the input glyph.



\subsection{Comparisons}

% \input{tables/tab_ablation.tex}

% Comparison using CLIP vs BERT to compute conditions 
\input{figs/fig_res_visual_comp.tex}

\mypara{Qualitative comparisons}
%
We compare DS-Fusion to DALL-E 2~\cite{ramesh2022hierarchical}, vanilla Stable Diffusion (SD)~\cite{rombach2022high}, Google Search, and CLIPDraw~\cite{frans2021clipdraw} for which we modify its input from random strokes to vector outlines of the input glyphs to fit the task at hand.
As we can see from the visual comparisons in Figure~\ref{fig:res_visual_comp}, in most cases, DALL-E 2 cannot blend the semantics into the letters; the objects simply surround the letters. Results from SD usually do not contain the shape of the input letters. Google search could occasionally return good results only because the input prompts have had corresponding artist designs that were retrieved. For less common queries such as ``Snail in letter N," retrieval clearly cannot work. As for CLIPDraw, neither the content legibility nor the semantic embedding is satisfactory.

\input{figs/fig_one_vs_multi_font}

\vspace{7pt}

\mypara{Quantitative comparisons}
%
To evaluate the performance of different methods in terms of legibility and style incorporation, we tested on 21 style words across a wide range of semantic categories (see supplementary material for the list) and stylized all the letters from them. Since both CLIPDraw and our method can receive multiple fonts as input, we also separately report their performances when receiving single and multiple fonts. For each style word, 4 {\em random\/} results were sampled from each method to obtain statistics.

\input{tables/tab_eval_one.tex}

The quantitative results are shown in Table~\ref{tab:eval_one_font}.
On single-font inputs, our method significantly outperforms the others in OCR accuracy both for raw and blurred versions of the generated results. The style score of DS-Fusion is slightly behind the others, which is not unexpected since in one-font mode, the output favors the glyph, and the style is less pronounced; see Figure~\ref{fig:effect_of_multiple}. Since CLIPDraw is over-fitted to the CLIP loss, its style score tends to be high while the corresponding visual results are not satisfactory; see Figure~\ref{fig:res_visual_comp} and the following user study.
When taking multiple fonts as input, DS-Fusion achieves the best score ($24.1$) of style incorporation among all methods. The letters are stylized in a much more abstract fashion, which also makes them more difficult to be recognized. Nevertheless, the OCR accuracy of DS-Fusion is comparable to CLIPDraw. 

% with one-font and multiple fonts with eighteen and fourteen categories  respectively
% Also, CLIP shows our style is also within the range of top methods, especially as CLIPDraw has been finetuned on CLIP. 
%Our results show that not only are the typographies legible, but the style score is also close to other methods like DALL-E-2, CLIPDraw and Stable Diffusion.
%multiple fonts as input on another 14 categories.
%For our results, we also employ our ranking method to automatically pick the top four out of fifty generated samples for evaluation.\mt{[we are not doing this anymore. our samples are also random]}

% \vspace{7pt}

% \mypara{Timing}
%
%\ali{is this supposed to be removed?}

\vspace{7pt}

\mypara{Comparison with Semantic Typography}
We compare DS-Fusion to Word-as-Image Semantic Typography~\cite{iluz2023word2img} in Figure~\ref{fig:semantic_comp}. both methods perform similarly; both results are still readable and semantically relevant (e.g., rhino, violin). However, our method is capable of adding colorful and artistic textures to make the results more semantically relevant and visually appealing, e.g., candle and octopus.

\subsection{User Study}
\label{subsec:user}

A user study serves to evaluate our method via subjective human judgement.
%The study has two parts
We perform two such studies: in the first one, users select between DS-Fusion and other generative methods; in the second, a different group of participants selects between DS-Fusion and artist designs.
%We compare with both results from other generative methods and human artists.
Both studies start by showing participants the definition and examples of artistic typography, which give them an idea of what to expect.
We directly chose {\em artist-created\/} examples from a popular online tutorial on \href{https://amadine.com/useful-articles/create-artistic-typography-designs-with-amadine}{artistic typography design},
 %recommended by Google, 
instead of picking inputs that may favour our method. \rz{The same 10} inputs with distinguishable style words (e.g.,\underline{C}at, \underline{P}arrot, etc.) were used in both studies.

The first study compares our results to those from CLIPDraw, DALL-E 2, and SD.
To prepare the study, we asked a professional designer to select the most representative result for each of the four candidate results generated by each method. The study collected responses from 32 participants with different occupations and varying artistic backgrounds to determine which result was deemed best. Table~\ref{tab:eval_user_study_vs_others} shows that DS-Fusion significantly outperformed other methods. 

While DALL-E 2 also gathered close to 30\% votes, some of its results preferred by users do not contain any stylization of the input letters, e.g., see the last two columns of Figure~\ref{fig:res_visual_comp}. DALL-E 2 simply placed an image of a cat or rooster near an un-stylized letter. Clearly, these are not satisfactory typography results, yet user subjectivity has led to them receiving 25\% and {\bf 50\%} of the votes, respectively.

%In the first \am{study}, we compare with synthetic results from CLIPDraw, DALL-E 2, and Stable Diffusion. \yw{For each question, we  generated \am{four} candidate results from each method. Then, we invited a professional designer to %determine which one is 
%\am{select}
%the most representative result for each method. Afterwards, we passed the user study to
%wide range of 
%\am{31} users from different occupations
%\am{to select the method producing the best result.}
%The users will select which method produces the best result in each question. 
%There are 31 participants in this part.
%\am{Table.~\ref{tab:eval_user_study_vs_others} shows the users' preference, where we outperform all the methods by a large margin.}}
%shows the percentage of user preference for each method, where our results are the most favorite, leading the second DALL-E 2 by a large margin.}\par

\input{figs/fig_semantic_comp}

The second 
%part of our 
study collected responses from 42 participants to choose between our results and professionally crafted examples found in the tutorial. %the users select a better one from two options based on their understanding of artistic typography. 
%There are 40 participants in this part. 
The study results are shown in the second part in Table~\ref{tab:eval_user_study_vs_others}. %Considering that human-designed results rely heavily on the creativity, expertise and efforts, it is impressive that 
In about 42\% of the cases, users found our results better or equivalent to that human-designed examples. This is a satisfactory outcome since human-designed examples heavily rely on the creativity and expertise of designers (more details about our user study in the supplementary material).


\input{tables/tab_user_study_vs_others.tex}
%\input{tables/tab_user_study_vs_human.tex}

\subsection{Ablation studies}
\label{subsec:ablation}

% \input{figs/fig_ablation_nomse}

\mypara{Discriminator weight}
%
%In Figure~\ref{fig:ablation_lambda} we show how changing the  weight of Discriminator loss i.e. the  $\lambda$ value affects the training process.  
The impact of the $\lambda$ value on adjusting the Discriminator loss is shown in Figure~\ref{fig:ablation_lambda}.
% We also show the training results if we remove the diffusion loss altogether. 
A high value of $\lambda$ pushes the generator towards the glyph shape, 
%and vice versa 
%A small value results in the generator either taking a long time to converge to the glyph shape or it is not able to reach it at all.
However, the generator may take a long time to converge to the glyph shape or may not be able to reach it at all when a small value of lambda is used.
Therefore, the value of lambda was selected experimentally. 
%On the other hand, 
%if we eliminate the diffusion loss,
% \am{AlsTro, if the diffusion loss is removed,}
% the generator quickly forgets the semantics of the style images corresponding to our prompt.

\input{figs/fig_ablation_lambda}

% \vspace{7pt}

% \mypara{Conditioning vector}
% %
% In Figure ~\ref{fig:alblation_prompt} we show the effect of using alternative prompts as conditioning for the denoising generator. We show by using a random prompt, which is a five-length random character string, how the method converges over time. However, using a prompt that is a better representative of our style images greatly speeds up and refines the process.


% \input{figs/fig_ablation_prompt}

\vspace{7pt}

\mypara{Number of style images}
%
In Figure ~\ref{fig:ablation_styimg} we show the effect of generating different numbers of style images. When the number is too low, the method
%finds it difficult to 
struggles to
extract common features to apply to the glyph. As the number of images increases, we see not only improvement in generation quality but also  in the diversity of outputs. 
We do not observe a significant improvement in increasing the value excessively, hence we opt for the number 25 as an optimal balance.
%However, we don't see a significant advantage going too high, and therefore choose the number twenty-five as a good balance.

\input{figs/fig_ablation_numimgs}

%\input{tables/tab_eval_multfont.tex}

\vspace{7pt}

\mypara{Text encoder}
%
Our results are generated using BERT. This model has a smaller resolution and is faster to generate compared to CLIP SD.
%makes it faster to develop and generate results. 
However, we show some results using the CLIP text encoder in Figure~\ref{fig:clip_results} to demonstrate the generality of our method.
%verify that our method is applicable to other text encoders.

\input{figs/fig_res_clip}

% 
% The quantitative results are shown in Table~\ref{tab:eval_quan_abc}.
% \input{tables/tab_eval_quan.tex}

