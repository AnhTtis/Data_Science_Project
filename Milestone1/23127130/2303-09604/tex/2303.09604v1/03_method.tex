\section{Method}
\label{sec:method}

%\subsection{Overview}

%Embedding semantics and textures into glyphs is not a straightforward task, thus it is not possible to use pixel-level supervision to obtain the results. A more abstract understanding of the semantics is required to successfully transfer them to a glyph. Latent Diffusion Models (LDMs) .... 

%Our key idea is to employ a discriminator on top of LDMs to discriminate between the latent codes of synthesized and vanilla glyphs. The discriminator aims to guide the generator to produce images falling within the glyph shape. Meanwhile, a diffusion loss ensures that the original style and semantics of input style images are not lost. Through this, the generator associates the text prompt with the glyph shape, and the style of input images. The pipeline of our method is demonstrated in Figure~\ref{fig:method_pipeline}. 

%\vspace{7pt}
%
%\mypara{Input and output}
%
Our method takes as input a {\em style prompt\/}, in the form of texts, and a {\em glyph\/} to be stylized, as a raster image. \rz{As we focus on generating artistic typographies in this work, the input glyph represents a graphical form of a letter or a word, e.g., in a particular font.} The output of our method is a stylized version of the glyph based on the style prompt, which consists of a style {\em word\/}, and optionally a style {\em attribute\/}. The style word is a noun specifying an object \rz{or activity} whose semantics are embedded into the resulting typography, while the style attribute provides a further characterization. 
%
\rz{Compelling typographies can be generated when the input glyph letters are part of the style word, e.g., ``C" in ``cat", 
and the stylized glyph is displayed within the word, resulting in a ``{\em word-as-image\/}'' \cite{berio2022strokestyles,tendulkar2019trick,zhang2017synthesizing,iluz2023word2img}}; see Figure~\ref{fig:teaser}.

We express an input by putting the style prompts in quotations and use the function $\mathscr G(\cdot)$ %\yw{$\mathcal{G(\cdot)}$} 
to denote a mapping from letter contents to glyph images. For example, the running example in Figure~\ref{fig:method_pipeline} has the input, ``cat" + ``cute'' + $\mathscr G$(C). 
%If the whole word is to be stylized, we simply remove the input letters for brevity: ``cat" + ``cute" + $\mathscr G$(CAT).

%If no glyph is provided the method will use the style word as the glyph. 
%Though most of our examples focus on textual input, a glyph can technically be any shape. 
% This glyph is what the style word will be applied to. 

\subsection{Overview}

To generate artistic typography, we first turn the input style word into a visual representation by employing a Latent Diffusion model ~\cite{rombach2022high} to obtain a set of twenty-five style images. The generative task then amounts to embedding the input semantics and the style images into a new, artistically stylized glyph, based on the input font. In the absence of any target images to provide direct supervision, we not only need a suitable feature representation for both the glyph and the style prompts, but also a means to evaluate the stylized results implicitly and effectively.

Utilizing the Latent Diffusion as our architecture backbone, we introduce the key idea of incorporating a {\em discriminator\/}, which guides the Diffusion model to produce images that fully blend styles into the input glyph images. Specifically, the diffusion first constructs the latent space of the given styles and outputs plausible latent codes, then the discriminator aims to distinguish between synthesized results and vanilla glyphs.
%
Figure~\ref{fig:method_pipeline} shows our method pipeline, with details to follow.

%Firstly, the input style word is used to generate a bunch of style images using a latent diffusion model. The style images can also be collected manually.
%
%Alongside this, we generate a set of rasterizations of the given glyph. Depending on the type of experiment we may use one font or multiple fonts. In either case, the glyphs are rasterized with random colors in each iteration. 

%A more abstract understanding of the semantics and glyphs is required to successfully handle this task.

%The given styles are expected to be naturally blended into the glyph shapes so the discriminator is unable to 
%By this means, the LDM associates the text prompt with the glyph shape, and the style of input images. 
%ensures that the original style and semantics of input style images are not lost
%, for which diffusion models are an excellent choice
%In the meantime, we expect the noises in SD models that produce logo and style images to be as close as possible, to maintain the semantics in logo images.

% In this case, a set of 5-10 should suffice, and their content should match the style word.

%When the inputs are ready, the algorithm works to learn the stylized glyphs. The sample images will be used to help fine-tune the denoising generator. 
\subsection{The Style Latent Space}

To generate a diversity of artistic typography images, we need to construct a latent space of given styles.
Following the LDM, the style images are first fed into an encoder and then passed through a diffusion process. The encoder outputs the features maps $z_{s}$. A Gaussian noise $\epsilon \in \mathcal{N}(0, \mathcal{I})$ is applied on $z_{s}$ to obtain $\bar{z}_{s}$, following an iterative noise \rz{injection} process introduced by diffusion models. 

A {\em denoising generator\/}, with a U-Net~\cite{ronneberger2015u} architecture conditioned on \rz{an encoded vector} by BERT, performs the denoising process for $\bar{z}_{s}$. It aims to predict the added noises $\hat{\epsilon}$ from $\bar{z}_{s}$ and then denoise $\bar{z}_{s}$ to $\hat{z}_{s}$. The prompt serves as a condition to the diffusion process, \rz{to enforce the fusion of the styles and glyphs into our desired images.} 
%
\rz{Technically, any text-conditioning vector fed to the denoising generator, even a random one, could be applied since over time, the generator will fine-tune it. However, in our current
implementation, we combine the input style prompt with the designated input letters as a prompt to BERT, and use the BERT-encoded vector for text conditioning, with the intent to ``warm start" the generator for faster convergence.}
% For comparison, we show in the ablation study the effect of using a random word as the prompt. }
Finally, we obtain $\bar{z}_{s}$ for sampling styles and $\hat{z}_{s}$ for generating our desired images. During training and inference, $\hat{z}_{s}$ will be sent into the discriminator and the decoder, respectively.

%LDM works on a latent representation of images which makes it far easier on resources and easier to train. 
%Using the predicted noise $\hat{\epsilon}$ and the number of (de)noising steps, LDM outputs the denoised style latent code $\hat{z}_{s}$. 

%Since our task is a sub-task of image generation, it is impossible to train the model from scratch with only a few images. 
The pre-trained encoder and decoder from Latent Diffusion~\cite{rombach2022high} are used in our network; they are frozen during both the training and testing processes. The encoder and decoder were trained with 400M images and can extract high-quality image features.
%
We employ a diffusion loss which measures the mean square error (MSE) between the predicted noise $\hat{\epsilon}$ and the input noise $\epsilon$: $\mathcal{L}_{diff} = || \hat{\epsilon} - \epsilon ||^2_{2}.$
%\[ \mathcal{L}_{diff} = || \hat{\epsilon} - \epsilon ||^2_{2}. \]

\subsection{The Discriminator}
Since there are no target images available in this task, the generated results can only be supervised implicitly.
It is inappropriate to directly align the generated images with glyph images because the former is highly textured and might have a displacement of glyph outlines. 
To this end, we propose to employ a discriminator in the style of GANs as the examiner. Different from vanilla GANs, the discriminator here takes input as feature maps instead of raw images. The feature maps contain both spatial information and local semantics, which helps the Discriminator to more easily build the correspondence between real and fake inputs. \par
Specifically, the rasterized glyph images are fed into the encoder to obtain the glyph feature maps ${z}_{g}$.
% which are subsequently 
Afterwards, the feature maps of style and glyph images ($\hat{z}_{s}$ and $z_{g}$) are sent into a CNN-based discriminator as fake and real samples, respectively.
The discriminator loss $\mathcal{L}_{dis}$ is Binary Cross-Entropy Loss of real/fake prediction:
\[ \mathcal{L}_{dis} = \log(D({z}_{g})) + \log(1 - D(\hat{z}_{s})), \]
where $D(\cdot)$ denotes the function of the discriminator.
Adversarially training $\mathcal{L}_{dis}$ will fine-tune the denoising generator to adapt the prompt to result incorporating the style of input images but following the shape of glyph. 

\subsection{Overall loss function and result ranking}

The \rz{overall} loss function is composed of the discrimination loss and the diffusion loss.
% \[ \mathcal{L} = \mathcal{L}_{diff} + \lambda * \mathcal{L}_{dis}, \]
We alternately optimize the Discriminator $D$ and the Denoising Generator $G$:

\begin{equation}
\min \limits_{G}\max \limits_{D} (\mathcal{L}_{diff} + \lambda \mathcal{L}_{dis}),
\end{equation}
where $\lambda$ is a hyperparameter that makes a tradeoff between maintaining the shape of letters and incorporating characteristics of style images.
Typically, $\lambda$ works best set less than $1$ and is experimentally set to $0.01$. The effect of different $\lambda$ values on results and training is shown in ablation.
The combination of diffusion loss and discriminator loss results in images that incorporate elements of the style while maintaining a structure similar to the glyph.

\input{figs/fig_ranking.tex}

%\subsection{Ranking the Generated Results}

Since our model outputs several candidate images from random noises, we have designed a strategy to select better candidates automatically. We employ CLIP to judge the quality of results from both stylistic and glyph preservation standards. Two different prompts are employed to judge each individually: the first is the glyph content like ``Letter R" and the second is style word like ``Dragon".
Figure~\ref{fig:ranking} shows a visual example of ranking a set of stylized glyphs, where the most top-right results are preferred.

