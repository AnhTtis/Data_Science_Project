\section{Experiments}
In this section, we conduct experiments on four real-world time-stamped recommendation datasets to show the effectiveness of our method. We mainly focus on the following questions:

\begin{itemize}[leftmargin=*]
% \vspace{-3mm}

\item \textbf{RQ1:} Whether our method can get better recommendation effects than the state-of-art methods on the most common recommendation metrics like \textit{Recall@20} and \textit{NDCG@20}?

\item \textbf{RQ2:} Whether our method is robust to different datasets and GCN-based recommendation models?

\item \textbf{RQ3:} Whether pruning historical convolution parameters to forget the outdated short-term preferences is necessary for improving streaming recommendation?

\item \textbf{RQ4:} Whether temporal user preference modeling as initialization help learn users' preferences in the GCN update phase?

\end{itemize}
\subsection{Experiment Settings}
\subsubsection{Datasets}
\begin{itemize}[leftmargin=*]
   %\item \textbf{Alimama~\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=649}:} filter-threshold:50,50
    \item \textbf{Taobao2014\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=46}:} This dataset contains real users-commodities behavior data collected from Alibaba's M-Commerce platforms, spanning 30 days. The rich background informations, like users' location information and behavior timestamp are also included. We filter out users and items with less than 10 interactions.
    \item \textbf{Taobao2015\footnote{https://tianchi.aliyun.com/dataset/dataDetail?dataId=53}:} This dataset contains user behavior data between July 1st, 2015 and Nov. 30th, 2015 accumulated on Taobao.com and the app Alipay. The online actions and timestamps are both recorded. In this work, we only use the first month data for analysis. Besides, We filter out users and items with less than 20 interactions.
    \item \textbf{Neflix\footnote{https://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a}:} This movie rating dataset contains over 100 million ratings from 480 thousand randomly-chosen Netflix customers over 17 thousand movie titles.  The data were collected between October, 1998 and December, 2005 and reflect the distribution of all ratings received during this period. A similar filtering operation is executed and the thresholds are both set as 30. We use the first 24 months data for analysis.
    \item \textbf{Foursquare%\footnote{https://sites.google.com/site/yangdingqi/home/foursquare-dataset}
    ~\citep{yang2019revisiting, yang2020lbsn2vec++}:} This dataset includes long-term (about 22 months from Apr. 2012 to Jan. 2014) global-scale check-in data collected from Foursquare, a local search-and-discovery mobile APP. The check-in dataset contains 22,809,624 checkins by 114,324 users on 3,820,891 venues. For this dataset, we set the filtering thresholds as 20.
\end{itemize}
The data statistics after filtering of the above datasets are detailed in Appendix~\ref{appendix:dataset}.  Average entity overlapping rate between adjacent segments (AER) is a metric to measure the stability of a data stream. The larger the AER, the more stable the data stream. The data on each segment is split into training, validation, and test sets using a ratio of 8:1:1. We repeat each experiment five times and report the average results to reduce the variance brought by the randomness. 

\subsubsection{Baselines}
\begin{itemize}[leftmargin=*]
    \item \textbf{Finetune}: Finetune first inherits the parameters from the previous segment and then fine-tune the model only with the data of the current segment.  
    %\item \textbf{Full-data Training (Full-data)}: A kind of \textit{experience replay} algorithm which consolidate all the historical data with the present data to train the model from scratch. This kind of method can often achieve good results but is actually not accessible in practice due to the huge memory and time cost, as well as the data regulations.
    \item \textbf{Uniform Sampling (Uniform)}: A kind of naive \textit{experience replay} method which first sample the historical data uniformly and then combine the new data with it. The model is trained with the combined data from scratch at each segment.
    \item \textbf{Inverse Degree Sampling (Inverse) ~\citep{ahrabian2021structure}}: A similar sampling-based \textit{experience replay} method. However, the sampling probability of each interaction is proportional to its user's inverse degree on the interaction graph.
    \item \textbf{ContinualGNN~\citep{wang2020streaming}}: A continual graph learning method which combines the \textit{experience replay} and \textit{knowledge distillation} for existing pattern consolidation.
    \item \textbf{Topology-aware Weight Preserving (TWP)~\citep{liu2020overcoming}}: A \textit{knowledge distillation} method which explores the local structure of the interaction graph and stabilize the parameters playing pivotal roles in the topological aggregation.
    \item \textbf{GraphSAIL~\citep{xu2020graphsail}}: A \textit{knowledge distillation} method which preserves each node's local structure, global structure, and self-information, respectively at the new segment.
    \item \textbf{SGCT~\citep{wang2021graph}}: A \textit{knowledge distillation} method which uses contrastive distillation on each node's embedding where a single user-item graph is used to construct positive samples.
    \item \textbf{MGCT~\citep{wang2021graph}}: A \textit{knowledge distillation} method which uses contrastive distillation on each node's embedding where multiple graphs (user-item, user-user, item-item graphs) is used to construct positive samples.
    \item \textbf{LWC-KD~\citep{wang2021graph}}: Based on MGCT, LWC-KD adds the intermediate layer distillation to inject layer-level supervision.
    %\item \textbf{Single-modal MSCGL~\citep{cai2022multimodal}}: model isolation + model regularization
\end{itemize}
We compare our \textbf{DEGC} model with the above continual graph learning methods. Note that we will show the experiment results of \textbf{DEGC+Finetune} and \textbf{DEGC+LWC-KD} (the combined methods mentioned in Section~\ref{sec:gra}) for a fair comparison. The implementation details are provided in Appendix~\ref{appendix:implementation}. The code is available at \textcolor{blue}{\url{https://github.com/BokwaiHo/DEGC}}.

\subsubsection{Evaluation Metrics}
All the methods are evaluated in terms of \textit{Recall@k} and \textit{NDCG@k}. For each user, our recommendation model will recommend an ordered list of items to her. \textit{Recall@k} (abbreviated as \textit{R@k}) indicates the percentage of her rated items that appear in the top $k$ items of the recommended list. The \textit{NDCG@k} (abbreviated as \textit{N@k}) is the normalized discounted cumulative gain at a ranking position $k$ to measure the ranking quality. Similar to previous related papers~\citep{xu2020graphsail, ahrabian2021structure, wang2021graph}, we set the $k$ as 20.

\begin{table*}[!t]
\caption{The average performance with MGCCF as our base model. $*$ indicates the improvements over baselines are statistically significant ($t$-test, $p$-value $\leq 0.01$).}
\begin{tabular}{|c|cc|cc|cc|cc|}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Taobao2014}            & \multicolumn{2}{c|}{Taobao2015}            & \multicolumn{2}{c|}{Netflix}               & \multicolumn{2}{c|}{Foursquare}                                 \\ \cline{2-9} 
                        & \multicolumn{1}{c|}{Recall@20} & NDCG@20   & \multicolumn{1}{c|}{Recall@20} & NDCG@20   & \multicolumn{1}{c|}{Recall@20} & NDCG@20   & \multicolumn{1}{c|}{Recall@20} & \multicolumn{1}{c|}{NDCG@20}   \\ \hline
Finetune                & \multicolumn{1}{c|}{0.0412}          &   0.0052        & \multicolumn{1}{c|}{0.4256}          &   0.0117        & \multicolumn{1}{c|}{0.3359}          &    0.0580       & \multicolumn{1}{c|}{0.1154}          &         0.0115                       \\ \hline
%Full-data                    & \multicolumn{1}{c|}{}          &           & \multicolumn{1}{c|}{}          &           & \multicolumn{1}{c|}{}          &           & \multicolumn{1}{c|}{}          &                                \\ \hline
Uniform                     & \multicolumn{1}{c|}{0.0308}          &      0.0038     & \multicolumn{1}{c|}{0.4194}          &       0.0113    & \multicolumn{1}{c|}{0.3298}          &       0.0533    & \multicolumn{1}{c|}{0.1024}          &       0.0101                         \\ \hline
Inverse                     & \multicolumn{1}{c|}{0.0323}          &     0.0039      & \multicolumn{1}{c|}{0.4217}          &     0.0114      & \multicolumn{1}{c|}{0.3321}          &    0.0536       & \multicolumn{1}{c|}{0.1063}          &        0.0107                        \\ \hline
ContinualGNN            & \multicolumn{1}{c|}{0.0311} & 0.0035 & \multicolumn{1}{c|}{0.4203} & 0.0111 & \multicolumn{1}{c|}{0.3089} & 0.0491 & \multicolumn{1}{c|}{0.1056} & \multicolumn{1}{c|}{0.0098} \\ \hline
TWP                     & \multicolumn{1}{c|}{0.0398}          &    0.0050       & \multicolumn{1}{c|}{0.4320}          &    0.0122       & \multicolumn{1}{c|}{0.3428}          &    0.0580       & \multicolumn{1}{c|}{0.1048}          &  0.0104                              \\ \hline
GraphSAIL               & \multicolumn{1}{c|}{0.0395}          &   0.0051        & \multicolumn{1}{c|}{0.4371}          &   0.0126        & \multicolumn{1}{c|}{\underline{0.3470}}          &  \underline{0.0583}         & \multicolumn{1}{c|}{0.1086}          &      0.0110                         \\ \hline
SGCT                    & \multicolumn{1}{c|}{0.0423}          &    0.0054       & \multicolumn{1}{c|}{0.4411}          & 0.0129          & \multicolumn{1}{c|}{0.3300}          &   0.0572        & \multicolumn{1}{c|}{0.1255}          &      0.0137                          \\ \hline
MGCT                    & \multicolumn{1}{c|}{0.0421}          &   0.0055        & \multicolumn{1}{c|}{0.4446}          &   0.0131        & \multicolumn{1}{c|}{0.3252}          &    0.0562       & \multicolumn{1}{c|}{0.1192}          &       0.0127                         \\ \hline
LWC-KD                  & \multicolumn{1}{c|}{\underline{0.0440}}          &   \underline{0.0059}        & \multicolumn{1}{c|}{\underline{0.4512}}          &   \underline{0.0139}        & \multicolumn{1}{c|}{0.2616}          &   0.0482        & \multicolumn{1}{c|}{\underline{0.1280}}          &              \underline{0.0144}                 \\ \hline
DEGC+LWC-KD             & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.1082*\\ ($\uparrow$ 146\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0142*\\ ($\uparrow$ 141\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.4892*\\ ($\uparrow$ 8.42\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0167*\\ ($\uparrow$ 20.1\%)\end{tabular}*} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.2949*\\ ($\downarrow$ 15.0\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0520*\\ ($\downarrow$ 10.8\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.1425*\\ ($\uparrow$ 11.3\%)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0178*\\ ($\uparrow$ 23.6\%)\end{tabular}}} \\ \hline
DEGC+Finetune           & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0825*\\ ($\uparrow$ 87.5\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0106*\\ ($\uparrow$ 79.7\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.4563*\\ ($\uparrow$ 1.13\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0142*\\ ($\uparrow$ 2.16\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.3583*\\ ($\uparrow$ 3.26\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0612*\\ ($\uparrow$ 4.97\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.1324*\\ ($\uparrow$ 3.44\%)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0153*\\ ($\uparrow$ 6.25\%)\end{tabular}}} \\ \hline
\end{tabular}
\label{table:mgccf}
\end{table*}

\begin{table}[]
\caption{The average performance with NGCF as our base model. $*$ indicates the improvements over baselines are statistically significant ($t$-test, $p$-value $\leq 0.01$).}
\begin{tabular}{|c|cc|cc|}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Taobao2014}            & \multicolumn{2}{c|}{Netflix}               \\ \cline{2-5} 
                        & \multicolumn{1}{c|}{R@20}      & N@20      & \multicolumn{1}{c|}{R@20}      & N@20      \\ \hline
Finetune                & \multicolumn{1}{c|}{0.0304}          &  0.0040         & \multicolumn{1}{c|}{0.3131}          &     0.0541      \\ \hline
%Full-data                    & \multicolumn{1}{c|}{}          &           & \multicolumn{1}{c|}{}          &           \\ \hline
Uniform                     & \multicolumn{1}{c|}{0.0340}          &  0.0038         & \multicolumn{1}{c|}{\underline{0.3263}}          &     0.0525      \\ \hline
Inverse                     & \multicolumn{1}{c|}{0.0347}          &  0.0039         & \multicolumn{1}{c|}{0.3256}          &     0.0518      \\ \hline
ContinualGNN            & \multicolumn{1}{c|}{0.0338} & 0.0036 & \multicolumn{1}{c|}{0.3047} & 0.0479 \\ \hline
TWP                     & \multicolumn{1}{c|}{0.0358}          &   0.0047        & \multicolumn{1}{c|}{0.3159}          &   0.0531        \\ \hline
GraphSAIL               & \multicolumn{1}{c|}{0.0318}          &     0.0042      & \multicolumn{1}{c|}{0.3245}          &   \underline{0.0554}        \\ \hline
SGCT                    & \multicolumn{1}{c|}{0.0350}          &    0.0046       & \multicolumn{1}{c|}{0.3044}          &     0.0533      \\ \hline
MGCT                    & \multicolumn{1}{c|}{0.0346}          &     0.0045      & \multicolumn{1}{c|}{0.2957}          &       0.0511    \\ \hline
LWC-KD                  & \multicolumn{1}{c|}{\underline{0.0380}}          &    \underline{0.0050}       & \multicolumn{1}{c|}{0.2496}          &    0.0454       \\ \hline
DEGC+LWC-KD             & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0961*\\ ($\uparrow$ 153\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0123*\\ ($\uparrow$ 146\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.2713*\\ ($\downarrow$ 16.9\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0486*\\ ($\downarrow$ 12.3\%)\end{tabular}} \\ \hline
DEGC+Finetune           & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0816*\\ ($\uparrow$ 115\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0107*\\ ($\uparrow$ 114\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.3454*\\ ($\uparrow$ 5.85\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0594*\\ ($\uparrow$ 7.22\%)\end{tabular}} \\ \hline
\end{tabular}
\label{table:ngcf}
\end{table}

\begin{table}[]
\caption{The average performance with LightGCN as our base model. $*$ indicates the improvements over baselines are statistically significant ($t$-test, $p$-value $\leq 0.01$).}
\begin{tabular}{|c|cc|cc|}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Taobao2014}            & \multicolumn{2}{c|}{Netflix}               \\ \cline{2-5} 
                        & \multicolumn{1}{c|}{R@20}      & N@20      & \multicolumn{1}{c|}{R@20}      & N@20      \\ \hline
Finetune                & \multicolumn{1}{c|}{0.0339}          &     0.0040      & \multicolumn{1}{c|}{0.3179}          &     0.0537      \\ \hline
%Full-data                    & \multicolumn{1}{c|}{}          &           & \multicolumn{1}{c|}{}          &           \\ \hline
Uniform                     & \multicolumn{1}{c|}{0.0377}          &   0.0041        & \multicolumn{1}{c|}{\underline{0.3289}}          &    0.0533       \\ \hline
Inverse                     & \multicolumn{1}{c|}{0.0386}          &     0.0042      & \multicolumn{1}{c|}{0.3275}          &     0.0530      \\ \hline
ContinualGNN            & \multicolumn{1}{c|}{0.0382} & 0.0041 & \multicolumn{1}{c|}{0.3035} &  0.0475\\ \hline
TWP                     & \multicolumn{1}{c|}{0.0338}          &     0.0040      & \multicolumn{1}{c|}{0.3204}          &    0.0542       \\ \hline
GraphSAIL               & \multicolumn{1}{c|}{0.0342}          &     0.0042      & \multicolumn{1}{c|}{0.3282}          &    \underline{0.0544}       \\ \hline
SGCT                    & \multicolumn{1}{c|}{0.0342}          &      0.0043     & \multicolumn{1}{c|}{0.3073}          &   0.0519        \\ \hline
MGCT                    & \multicolumn{1}{c|}{0.0357}          &     0.0047      & \multicolumn{1}{c|}{0.2983}          &   0.0516        \\ \hline
LWC-KD                  & \multicolumn{1}{c|}{\underline{0.0402}}          &     \underline{0.0053}      & \multicolumn{1}{c|}{0.2571}          &   0.0461        \\ \hline
DEGC+LWC-KD             & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0975*\\ ($\uparrow$ 143\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0125*\\ ($\uparrow$ 136\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.2776*\\ ($\downarrow$ 15.6\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0491*\\ ($\downarrow$ 9.74\%)\end{tabular}} \\ \hline
DEGC+Finetune           & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.0833*\\ ($\uparrow$ 107\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0109*\\ ($\uparrow$ 106\%)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}0.3483*\\ ($\uparrow$ 5.90\%)\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}0.0596*\\ ($\uparrow$ 9.56\%)\end{tabular}} \\ \hline
\end{tabular}
\label{table:lightgcn}
\end{table}

\subsection{Results and Analysis}
\subsubsection{\textbf{Overall Performance (RQ1)}}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/overall_performance_new_0131.png}
    \caption{The time-varying model performance on the data stream of Taobao2014 dataset and Netflix dataset.
    }
    \Description{The time-varying model performance on the data stream of Taobao2014 dataset and Netflix dataset.}
    \label{fig:overall_performance}
    \vspace{-0.3cm}
\end{figure}
To answer \textbf{RQ1}, we evaluate our model performance from two perspectives: time-varying performance and average performance on the data stream. In Figure~\ref{fig:overall_performance}, we visualize the \textit{Recall@20} and \textit{NDCG@20} curves on the data streams of Taobao2014 dataset and Netflix dataset. Comparing the DEGC+Finetune and DEGC+LWC-KD with Finetune and LWC-KD, respectively, we can observe that our methods achieve significant time-varying performance gain over their corresponding base methods. Note that the zero performance in the first two days of Taobao2014 dataset is due to the limited amount of data and the model overfitting. The main reason that the observed sharp \textit{NDCG@20} decreases in the first five months of Netflix dataset is the rapidly increasing numbers of users and items. In Table~\ref{table:mgccf}, we show the average performance of different methods on four datasets while choosing MGCCF as the base GCN recommendation model. It can be observed that our DEGC+Finetune and DEGC+LWC-KD get better recommendation effects than all state-of-art methods on all four datasets, except the DEGC+LWC-KD on Netflix. We argue that this is because the poor performance of LWC-KD itself on Netflix. Comparing the DEGC+LWC-KD with LWC-KD independently, our method still improves the performance by $12.7\%$ on \textit{Recall@20} and $7.9\%$ on \textit{NDCG@20}, which also demonstrate the effectiveness of our method. As for the poor performance of SGCT, MGCT, LWC-KD on Netflix, this is resulted by the \textit{over-stability} issue. Such type of \textit{knowledge  distillation} methods can hardly accurately capture the user preferences' shifts  when they change rapidly. Besides, it can be noticed that the performance of \textit{experience replay} methods including Uniform, Inverse, and ContinualGNN are even worse than Finetune. Actually, in the streaming recommendation, such methods can replay previous data containing users' outdated short-term preferences, which negatively influences the model learning on new segments. 

\subsubsection{\textbf{Method Robustness Analysis (RQ2)}}
To answer \textbf{RQ2}, we conduct the experiments with NGCF and LightGCN as the base GCN models on both Taobao2014 and Netflix datasets. The corresponding results are shown in Tables~\ref{table:ngcf} and~\ref{table:lightgcn}. For GCN models NGCF and LightGCN, the improvements of our methods on Taobao2014 are both significant. DEGC+Finetune and DEGC+LWC-KD both achieve the state-of-art recommendation performance. An interesting observation is that DEGC+Finetune in Table~\ref{table:lightgcn} even gets better performance than that in Table~\ref{table:mgccf}. This also shows the performance potential of DEGC on different kinds of base GCN models. As for the dataset Netflix, DEGC+Finetune improves the \textit{Recall@20} by $10.3\%$ and \textit{NDCG@20} by $9.8\%$ over Finetune when choosing NGCF as the GCN model. DEGC+LWC-KD improves the \textit{Recall@20} by $8.7\%$ and \textit{NDCG@20} by $7.0\%$ over LWC-KD, meanwhile. Besides, DEGC+Finetune achieves the best recommendation effect over all previous methods. Similar improvements can also be observed when taking LightGCN as the base GCN model. Such observations demonstrate the robustness of our methods to different datasets and GCN-based recommendation models. 


\subsubsection{\textbf{Ablation Study to Historical Convolution Pruning (RQ3)}}
\label{sec:abl hcp}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/Ablation_Study_new.png}
    \caption{Ablation study to historical convolution pruning (HCP)  and temporal preference modeling (TPM) on Taobao2014 dataset.
    }
    \Description{Ablation study to historical convolution pruning (HCP)  and temporal preference modeling (TPM) on Taobao2014 dataset.}
    \label{fig:ablation}
    \vspace{-0.3cm}
\end{figure}
To answer \textbf{RQ3}, we conduct the experiments with DEGC+Finetune (w/o HCP) and DEGC+LWC-KD (w/o HCP) as the continual graph learning methods while taking the MGCCF as base GCN model on Taobao2014 and Netflix datasets. The top two subfigures of Figure ~\ref{fig:ablation} illustrate the time-varying recommendation performance with two metrics: \textit{Recall@20} and \textit{NDCG@20}. Comparing the DEGC+Finetune with DEGC+Finetune (w/o HCP) and DEGC+LWC-KD with DEGC+LWC-KD (w/o HCP), respectively, we can find that historical convolution pruning is of great significance to our methods' effectiveness. These demonstrate that pruning historical convolution parameter to forget the outdated short-term preferences is necessary. It is also validated that conventional continual graph methods that inherit the parameters learned on the last segment indiscriminately and then finetune them with the new data hinders the model learning on the new segment. This can also be regarded as, at least, part of the reasons that lead to the 'over-stability' challenge in continual learning for streaming recommendation. We also quantitatively analyze the performance drop after removing the historical convolution pruning. We find that DEGC+Finetune decreases by $25.3\%$ and DEGC+LWC-KD decreases by $28.4\%$ on average. The more severe decrease effect of DEGC+LWC-KD is due to that LWC-KD preserves more historical knowledge which may contain users' outdated short-term preferences. We can also observe similar results regarding Netflix dataset in Figure~\ref{fig:ablation_netflix} of Appendix~\ref{appendix:sup ablation}.
 

\subsubsection{\textbf{Ablation Study to Temporal Preference Modeling (RQ4)}}
To answer \textbf{RQ4}, we conduct the experiments with DEGC+Finetune (w/o TPM) and DEGC+LWC-KD (w/o TPM) on Taobao2014 and Netflix datasets while taking the MGCCF as the base GCN model. From the bottom two subfigures of Figure~\ref{fig:ablation}, we can observe that the \textit{Recall@20} and \textit{NDCG@20} metrics both decrease obviously after removing the temporal preference modeling. This proves that modeling temporal user preference as initialization does benefit the user preference learning in the GCN update phase. Actually, this also corresponds to the other two challenges except the continuous user preference shifts in streaming recommendation: ever-increasing users and intermittent user activities mentioned in Section~\ref{sec:introduction}. Traditional continual graph learning methods like Finetune and LWC-KD directly use the user embeddings learned in the last segment as the embeddings initialization in the current segment. So they can hardly provide an accurate embedding initialization to users whose active intervals on the online platform are longer than a time segment.  Also, they cannot provide a warm embedding initialization for newly coming users. Our temporal preference modeling as user embedding initialization solves such two challenges to some extent and improves the \textit{Recall@20} by $34.3\%$ and $24.9\%$ on average, over DEGC+Finetune (w/o TPM) and DEGC+LWC-KD (w/o TPM), respectively. The similar trends on Netflix dataset can also be observed in Figure~\ref{fig:ablation_netflix} of Appendix~\ref{appendix:sup ablation}.



% Datasets: Gowalla, Yelp, Amazon
% Recommended datasets: Alimama, Taobao2014, Taobao2015, Netflix, Foursquare
% Metrics: F1, recall@20, NDCG@k
% Base Model: PinSage, NGCF, MGCCF 

%baselines:
% 1. Finetune (Lower Bound) 2. Full-batch (Upper Bound) 3. Topology-aware weight preserving (TWP) 4. ContinualGNN 5. GraphSAIL 6. Local structure preserving（LSP）7. single-modal MSCGL