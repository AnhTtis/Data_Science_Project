\section{Notations}
\label{appendix:notations}
We summarize the main notations used in this paper in Table~\ref{table:notation}.

\begin{table}[h]
\caption{Major notations.}
\begin{tabular}{c l}
\hline
$T$     & \begin{tabular}[c]{@{}l@{}}The total number of data/time segments\end{tabular} \\ 
$K$     & \begin{tabular}[c]{@{}l@{}}The total number of graph convolution layers\end{tabular} \\ 
$\widetilde{\textbf{D}}$    & \begin{tabular}[c]{@{}l@{}}The user-item interaction data stream\end{tabular} \\ 
$D_t$     & \begin{tabular}[c]{@{}l@{}}The data streaming into the system at time segment $t$\end{tabular} \\ 
$\Delta G_t$     & \begin{tabular}[c]{@{}l@{}}The graph structure of interaction data $D_t$\end{tabular} \\ 
$G_t$     & \begin{tabular}[c]{@{}l@{}} The graph structure of the union of $D_1,D_2,...,D_t$\end{tabular} \\ 
$\mathbf{S}_{t}$     & \begin{tabular}[c]{@{}l@{}}The graph convolution structure at segment $t$\end{tabular} \\ 
$\mathbf{W}_{t}$     & \begin{tabular}[c]{@{}l@{}}The graph convolution parameters at segment $t$\end{tabular} \\ 
$\mathbf{W}^K_{t}$     & \begin{tabular}[c]{@{}l@{}}The topmost graph convolution layer parameters at segment $t$\end{tabular} \\ 
$\mathbf{W}^s_{t}$     & \begin{tabular}[c]{@{}l@{}}The short-term preference-related parameters of $\mathbf{W}_{t}$ \end{tabular} \\ 
$\mathbf{W}^l_{t}$ & \begin{tabular}[c]{@{}l@{}} The long-term preference-related parameters of $\mathbf{W}_{t}$\end{tabular} \\ 
$\Delta \mathbf{W}_{t}$     & The expansion part of graph convolution at segment $t$\\ \hline
\end{tabular}
\label{table:notation}
\end{table}

\section{Data statistics of filtered datasets}
The data statistics of fours filter datasets used in this work are summarized in Table~\ref{table:dataset}.
\label{appendix:dataset}
\begin{table}[]
\caption{Data statistics of filtered datasets.}
\begin{tabular}{|c|c|c|c|c|}
\hline
Dataset                                                                                                  & Tb2014 & Tb2015 & Netflix & Foursquare \\ \hline
user \#                                                                                                   &    8K        &     192K       &    301K     &    52K         \\ \hline
item \#                                                                                                   &      39K     &     10K        &     9K    &   37K         \\ \hline
interaction \#                                                                                          &      749K      &      9M      &   49M      &   2M         \\ \hline
time span                                                                                                    &     31 days       &     123 days      &  74 months       &     22 months      \\ \hline
\begin{tabular}[c]{@{}c@{}}AER \end{tabular}     &  35.5\%          &   26.0\%         &     58.4\%    &       60.0\%      \\ \hline
\end{tabular}
\label{table:dataset}
\end{table}

\section{Supplementary Ablation Study}
The ablation study results on Netflix dataset are present in Figure~\ref{fig:ablation_netflix}.
\label{appendix:sup ablation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.50\textwidth]{Figures/ablation_study_netflix_zoomin.png}
    \caption{Ablation study to historical convolution pruning (HCP) and temporal preference modeling (TPM) on Netflix dataset.
    }
    \Description{Ablation study to historical convolution pruning (HCP) and temporal preference modeling (TPM) on Netflix dataset.}
    \label{fig:ablation_netflix}
    \vspace{-0.3cm}
\end{figure}

\section{Implementation Details}
\label{appendix:implementation}
 We report our implementation details here. We use the Adam optimizer with an initial learning rate 
as 0.001. The embedding size and the width of each graph convolution layer are set to 128. The $L1$  regularization coefficient $\lambda_1$ is set to 0.001. The $L2$ regularization coefficient $\lambda_2$ and the GSR regularization coefficient $\lambda_g$ are set to 0.01. We set the batch size as 1,000 when training the GCN models. The number $N$ of the expansion filters at each layer is set as 30. Without specifications, the hyper-parameters are set same as the original papers. We implement our algorithm with Tensorflow and test it on the NVIDIA GeForce RTX 3090 GPU with 24 GB memory.

