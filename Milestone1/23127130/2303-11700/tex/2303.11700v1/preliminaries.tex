\section{Preliminaries}
\label{section:preliminaries}
In this section, we first formalize the continual graph learning for streaming recommendation. Then we briefly introduce three classical graph convolution based recommendation models used in this paper.

\subsection{Definitions and Formulations}
\begin{definition}{Streaming Recommendation.}
Massive user-item interaction data $\widetilde{\textbf{D}}$ streams into industrial recommender system continuously. For convenience~\citep{caccia2021anytime, wang2020streaming, wang2022streaming}, the continuous data stream is split into consecutive data  segments $D_1,...,D_t, ..., D_T$ with the same time span. At each time segment $t$, the model needs to optimize the recommendation performance on $D_t$ with the knowledge inherited from $D_1, D_2,..., D_{t-1}$. The recommendation performance is evaluated along the whole timeline.
\end{definition}

\begin{definition}{Streaming Graph.}
A streaming graph is represented as a sequence of graphs $\mathcal{G}= (G_1, G_2, ..., G_t, ..., G_T)$, where $G_t = G_{t-1} + \Delta G_t$. $G_t = (\mathbf{A}_t, \mathbf{X}_t)$ is an attributed graph at time $t$, where $\mathbf{A}_t$ and $\mathbf{X}_t$ are the adjacency matrix and node features of $G_t$, respectively. $\Delta G_t = (\Delta \mathbf{A}_t, \Delta \mathbf{X}_t)$ is the changes of graph structures and node attributes at $t$. The changes contain newly added nodes and newly built connections between different nodes.
\end{definition}

\begin{definition}{Continual Graph Learning for Streaming Graph.}
\label{def:cgl}
Given a streaming graph $\mathcal{G}= (G_1, G_2, ..., G_t, ..., G_T)$, the goal of \textbf{continual graph learning (CGL)} is to learn $\Delta G_t (D_t)$ sequentially while transferring historical knowledge to new graph segments effectively. Mathematically, the goal of \textbf{CGL} for streaming graph is to find the optimal GNN structure $\mathbf{S}_t$ and parameters $\mathbf{W}_t$ at each segment $t$ such that:
\begin{equation}
    (\mathbf{S}^{*}_t, \mathbf{W}^{*}_t) = \mathop{\arg\min}\limits_{(\mathbf{S}_t, \mathbf{W}_t)}\mathcal{L}_t(\mathbf{S}_t, \mathbf{W}_t, \Delta G_t),
\end{equation}
where $(\mathbf{S}_t, \mathbf{W}_t) \in (\mathcal{S}, \mathcal{W}) $. $\mathcal{L}_t(\mathbf{S}_t, \mathbf{W}_t, \Delta G_t)$ is the loss function of current task defined on $\Delta G_t$. The $\mathcal{S}$ and $\mathcal{W}$ are corresponding search spaces, respectively.
\end{definition}

Since the user-item interaction data is actually a bipartite graph, the continual learning task for streaming recommendation is essentially the continual graph learning for streaming graph. For each segment $t$, the GNN structure $\mathbf{S}_t$ and parameters $\mathbf{W}_t$ need to be adjusted and refined simultaneously to achieve a satisfying recommendation performance. We use the Bayesian Personalized Ranking (BPR)~\citep{bprloss} loss as the loss function in this work, because it is effective and has broad applicability in top-K recommendation tasks. The major notations are summarized in Appendix \ref{appendix:notations}.

%Due to that the parameter $W_{t-1}$ contains the user preference information distilled from historical data $D_1, D_2,...,D_{t-1}$ and represents the user long-term preference, $W_{t-1}$ is notes as \textit{long-term preference parameter} at timestep $t$. Correspondingly, $W_t \backslash W_{t-1} $ is noted as \textit{short-term preference parameter} at timestep $t$ considering it only reveals the user short-term preference at timestep t.

\subsection{GCN-based Recommender Models}
Many graph convolution-based recommender models~\citep{wang2019neural, he2020lightgcn, sun2019multi,ying2018graph} have been developed recently to capture the collaborative signal, which is not encoded by the early matrix factorization and other deep learning based models. A general graph convolution process for such models can be summarized below: 
On the user-item bipartite graph, the layer-$k$ embedding of user $u$ is obtained via the following processing: 
\begin{equation}
\begin{aligned}
\mathbf{h}^{u,k} &= \sigma(\mathbf{W}^{u,k} \cdot [\mathbf{h}^{u,k-1}; \mathbf{h}^{\mathcal{N}(u),k-1}]), \mathbf{h}^{u,0} = \mathbf{e}^u,\\
\mathbf{h}^{\mathcal{N}(u),k-1} &= AGGREGATOR^u({\mathbf{h}^{i,k-1}, i \in \mathcal{N}(u)}), \\
\end{aligned}
\end{equation}
where $\mathbf{e}^u$ is the initial user embeddings, $\sigma(\cdot)$ is the activation function, $\mathbf{h}^{\mathcal{N}(u),k-1}$ is the learned neighborhood embedding, and $\mathbf{W}^{u,k}$ is the layer-$k$ user transformation matrix shared among all users. The $AGGREGATOR^u$ is the designed aggregation function in order to aggregate neighbor information for user nodes. Similarly, the layer-$k$ embedding of item $i$ is obtained via the following processing:
\begin{equation}
\begin{aligned}
\mathbf{h}^{i,k} &= \sigma(\mathbf{W}^{i,k} \cdot [\mathbf{h}^{i,k-1}; \mathbf{h}^{\mathcal{N}(i),k-1}]), \mathbf{h}^{i,0} = \mathbf{e}^i,\\
\mathbf{h}^{\mathcal{N}(i),k-1} &= AGGREGATOR^i({\mathbf{h}^{u,k-1}, u \in \mathcal{N}(i)}). \\
\end{aligned}
\end{equation}
In our setting, considering the timestamps of the above matrices, convolution parameters $\mathbf{W}_t = \left\{\mathbf{W}^{u, 1}_t, \mathbf{W}^{i, 1}_t,..., \mathbf{W}^{u, k}_t, \mathbf{W}^{i, k}_t,...,\mathbf{W}^{u, K}_t, \mathbf{W}^{i, K}_t\right\}$, where $\mathbf{W}^{u, k}_t$ is the $\mathbf{W}^{u,k}$ on the time segment $t$. Each column in such matrices is a graph convolution filter. NGCF~\citep{wang2019neural}, LightGCN~\citep{he2020lightgcn}, and MGCCF~\citep{sun2019multi} are three representative GCN-based recommendation models which will be utilized in our work. Some of their details are provided below.

 
\textbf{NGCF~\citep{wang2019neural}}. One of the most widely used GCN-based recommendation model. NGCF exploits the user-item bipartite graph structure by propagating embeddings on it with graph convolution.  

\textbf{LightGCN~\citep{he2020lightgcn}}. An improved version of NGCF which still convolutes on user-item graph. Compared with NGCF, LightGCN no more needs neighbor node feature transformation and nonlinear activation. Based on the original model setting, we add a dense matrix to each layer to align the dimensions of aggregated embeddings for better adapting to our approach.

\textbf{MGCCF~\citep{sun2019multi}}. A graph convolution-based recommender framework which explicitly incorporates multiple graphs in the embedding learning process. Compared with the above two models, MGCCF adds a Multi-Graph Encoding(MGE) module to capture the inter-user and inter-item proximity information from user-user graph and item-item graph via homogeneous graph convolution, respectively.

%As for the MGE module of MGCCF, on the user-user and item-item homogeneous graphs, the graph convolutions for user $u$ and item $i$ are:
%\begin{equation}
%\begin{aligned}
%\mathbf{z}^u &= \sigma(\mathbf{M}^u \cdot \frac{1}{|\mathcal{N}^{u}(u)|} \underset{j\in\mathcal{N}^{u}(u)}{\sum} \mathbf{e}^j)\\ \mathbf{z}^i &= \sigma(\mathbf{M}^i \cdot \frac{1}{|\mathcal{N}^{i}(i)|} \underset{j\in\mathcal{N}^{i}(i)}{\sum} \mathbf{e}^j)\\
%\end{aligned}
%\end{equation}
%where $\mathcal{N}^{u}(u)$ denotes the neighborhood of user $u$ on the user-user graph and $\mathcal{N}^{i}(i)$ denotes the neighborhood of user $i$ on the item-item graph. The $\mathbf{M}^u$ and $\mathbf{M}^i$ are the corresponding transformation matrices. 
%Unless special cases, $\mathbf{M}^{u}_t$ and $\mathbf{M}^{i}_t$ will not be explained independently.