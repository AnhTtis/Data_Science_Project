\section{Introduction}
\label{sec:introduction}
The recommender system (RS), aiming to provide the personalized contents to different users precisely~\citep{ma2019hierarchical, ma2020probabilistic, chen2021attentive, chen2021hyper, chen2022learning}, has been deployed in many online Internet applications. However, traditional recommender systems trained on the offline static datasets face three challenges in the real online platforms: user preference shift, ever-increasing users and items, and intermittent user activities. In fact, these can cause the unacceptable performance degradation as shown in Figure~\ref{fig:decrease}, and lead to the necessity of dynamical model updating. Thus, how to update the model dynamically to tackle the above challenges has attracted great attention in real-world RS~\citep{cai2022reloop, sima2022ekko}. This ensures the indispensable need of \textit{streaming recommendation}, referring to updating and applying recommendation models dynamically over the data stream. 
% And continual learning is regarded as a common and promising way towards effective streaming recommendation. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Decrease_Grid_1005.png}
    \caption{LightGCN, NCGF, and MGCCF only trained in the first week and then tested in the following three weeks on Taobao2014 dataset.
    }
    \Description{LightGCN, NCGF, and MGCCF only trained in the first week and then tested in the following three weeks on Taobao2014 dataset.}
    \label{fig:decrease}
    \vspace{-0.3cm}
\end{figure}

Due to the strong capability of modeling user-item, user-user, and item-item relationships, graph convolution neural network (GCN) has been widely-used as recommendation models. Some recent works~\citep{xu2020graphsail, wang2021graph, ahrabian2021structure, wang2020streaming, wang2022streaming} develop continual learning methods for GCN-based recommendation methods to achieve the streaming recommendation, also known as continual graph learning for streaming recommendation. 

%To enable continual GCN-based recommendation, most works focus on two realizations: \textit{experience replay} and \textit{knowledge distillation/weight regularization}.
%Although these methods have achieved acceptable results, there are still drawbacks which hinder them from being applied in the real-world systems. First, \textit{experience replay} needs complete and accurate historical data when training the model on the newly coming data. Nevertheless, in the real-world system, the missing data and malicious data attack issues are pretty common. What's worse, more and more strict data governance policies often make the user historical behavior inaccessible. Training with historical data replay also brings the huge memory cost and increasing time consumpation issues. Second, knowledge distillation/model regularization can hardly capture the varying short-term preferences, especially for those users whose preferences change quickly and dramatically. Such issue is also known as \textit{over-stability} in continual learning. Third, \textit{experience replay} and \textit{knowledge distillation/model regularization} both rely heavily on the fine-tuning of multiple hyperparameters. Non-optimal hyperparamters can easily result in poor model performance and even model training failure. Existing few \textit{model isolation}-based methods are designed specially for the typical continual learning paradigm with task boundaries and can hardly handle the growing dataset (data stream) on a single task. Besides, such \textit{model isolation} methods need to search the whole GCN architecture in each task which increases the method complexity greatly. In streaming recommendation setting, \textit{model isolation}-based continual graph learning is less investigated  though some apparent advantages like no longer needing to replay historical data and fewer sensitive hyperparameters. Besides, previous continual graph learning methods for streaming recommendation do not model the user-level temporal preference changes explicitly and only relies on the graph convolution itself to capture the shift, which is greatly limited by the aforementioned two challenges: ever-increasing users and intermittent user activities. This is also far from the fine-grained user modeling for personalized recommendation.

To enable continual GCN-based recommendation, most works focus on two realizations: \textit{experience replay}~\citep{ahrabian2021structure, zhou2021overcoming, wang2022streaming} and \textit{knowledge distillation/weight regularization}~\citep{wang2021graph, liu2020overcoming, xu2020graphsail, wang2020streaming}. Although these methods have achieved acceptable results, there are still drawbacks which hinder them from being applied in the real-world systems. First, \textit{experience replay} needs complete and accurate historical data when training the model on the newly coming data. Nevertheless, in the real-world system, the missing data~\citep{chi2020missing, wang2019doubly} and malicious data attack~\citep{zhang2021data} issues are pretty common. What's worse, more and more strict data governance policies often make the user historical behavior inaccessible. Training with historical data replay also brings the huge memory cost and increasing time consumption. Second, knowledge distillation/model regularization can hardly capture the varying short-term preferences, especially for those users whose preferences change quickly and dramatically. Such issue is also known as \textit{over-stability}~\citep{ostapenko2021continual} in continual learning. Third, previous continual graph learning methods for streaming recommendation do not model the user-level temporal preference changes explicitly and only relies on the graph convolution itself to capture the shift, which is greatly limited by the aforementioned two challenges: ever-increasing users and intermittent user activities. This is also far from the fine-grained user modeling for personalized recommendation. Besides, few existing \textit{model isolation}-based methods~\citep{cai2022multimodal, yuan2021one} (another main approach for continual learning) are designed specially for the typical continual learning paradigm with task boundaries and can hardly handle the growing dataset (data stream) on a single task. Such \textit{model isolation} methods need to search the whole GCN architecture in each task which increases the method complexity greatly. In streaming recommendation setting, \textit{model isolation}-based continual graph learning is less investigated though some apparent advantages like no longer needing to replay historical data and the potential to overcome the \textit{over-stability} issue.


%A summary to our algorithm framework, a model-isolation based continual graph learning method is designed for streaming recommendation which no more needs historical data replay. Based on the motivation of decoupling the user preference to long-term preference and short-term preference~\citep{ma2019hierarchical}, assigning historical parameters to represent long-term preferences and newly expanding filters to extract the short-term preference information, we first derive some structural and attributed signals to describe the user-item interaction bipartite graph pattern change. Then, we propose a LSTM-structured reinforced GNN structure controller to generate newly expanding filters. Next, to distill the robust long-term preference (prevent the disturbance of out-dated short-term preferences) and constrain the model size (prevent the model width explosion), we propose a kind of simple but effective long-term preference parameter pruning mechanism. Finally, a kind of preference-decoupled model training is designed to update two kind of preference-related parameters separately. 

To tackle the above challenges, we propose a model-isolation continual graph learning method, namely \textbf{D}ynamically \textbf{E}xpandable \textbf{G}raph \textbf{C}onvolution (DEGC), to better model the user preference shift without the need of historical data replay. First, we design a graph convolution network-based sparsification training method to disentangle short-time preference-related parameters from long-time preference-related parameters. Then we remove outdated short-term preference-related filters and preserve long-term preference-related filters which are further refined with newly-collected data. Next, the graph convolution network is expanded by additional filters to extract the current short-term preference. The added filters will also be partly pruned to eliminate the redundant ones and prevent the network width explosion catastrophe. Moreover, inspired by the Kalman filter~\citep{welch1995introduction}, a temporal attention model is utilized to explicitly encode the temporal user preference, which works as the user embedding initialization for training on new data.

In summary, the main contributions of this paper are:
 \begin{itemize}[leftmargin=*]
  %	\item We propose a model-isolation based continual graph learning algorithm, DEGC, for streaming recommendation. We utilize the \textit{historical graph convolution pruning and refining} and \textit{graph convolution expanding and pruning} operations to overcome the \textit{over-stability} challenge.
   \item We propose a \textit{model isolation}-based continual graph learning method, DEGC, for streaming recommendation. We design a sequence of graph convolution operations including pruning, refining, and expanding to overcome the \textit{over-stability} challenge.  
   
   \item To address the challenges of ever-increasing users and intermittent user activities, we model the temporal user preference as the user embedding initialization to help learn users' preferences on newly coming data. 
   
   \item Experiments on three representative GCN-based recommendation models and four real-world datasets demonstrate the effectiveness and robustness of our method.
 \end{itemize}

