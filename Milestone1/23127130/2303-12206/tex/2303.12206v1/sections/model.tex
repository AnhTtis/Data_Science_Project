%!TEX root=../Draft1.tex

We first formalize a general problem setting in \cref{s.gmodel}, before describing our policy in \cref{s.decomposedPI}.

\subsection{Model} \label{s.gmodel}
There are $N$ patients and $T$ time steps.
Each patient $i \in [N]$ is associated with a Markov decision process (MDP) represented by $M_i = (\cS, \cA, P_i, R_i)$.
$\cS$ is the state space, and let $S_{it} \in \cS $ denote the state of patient $i$ at time $t$.
The action space is $\cA = \{0, 1\}$, where we will refer to action $A_{it} = 0$ as the \textit{null} action, and action $A_{it}=1$ as the \textit{intervention}.
$P_i(s, s', a) = \Pr(S_{i,t+1} = s' \;|\; S_{it} = s, A_{it} = a)$ is the probability of transition from state $s$ to $s'$ when action $a$ is taken, and $R_i(s, s', a)$ is the immediate reward from this transition.
If $A_{it} = 1$, we say that patient $i$ is \textit{chosen} at time $t$.

Next, we define a \textit{system MDP} by combining the $N$ patient MDPs and adding a budget $B \geq 0$ on the number of interventions at each time step.
That is, $\Aspace = \{(A_1, \dots, A_N) \subseteq \{0, 1\}^N: \sumN A_i \leq B\}$ is the action space for the system MDP.
The state space  is $\cS^N$, and a policy $\pi: \cS^N \times [T] \rightarrow \Aspace$ is a mapping from the patients' states to an action, where this mapping can also depend on the current time step.
At each time $t = 1, \dots, T$, the following sequence of events occur:
\begin{enumerate}
	\item The state $S_{it} \in  \cS$ is observed for each patient $i \in [N]$.
	\item The policy chooses an action, $ \pi((S_{1t}, \dots, S_{Nt}), t) = (A_{1t}, \dots, A_{Nt})$.
	% , corresponding to the set of up to $B$ patients to give the intervention to.
	% \item Each patient transitions independently according to their state $S_{it}$, action $a_{it} = \bI(i \in A_t)$, and transition probabilities $P_i$, to their new state $S_{i,t+1}$.
	\item For each patient $i$, their next state $S_{i,t+1}$ is realized independently according to their transition probabilities $P_i(S_{it}, \cdot, A_{it})$. We gain the reward $\sumN R_i(S_{it}, S_{i,t+1}, A_{it})$.
\end{enumerate}
Let $\mathbf{S}_1 = (S_{11}, \dots, S_{N1})$ be the starting state profile.
An instance of this problem is represented by $(N, T, (M_i)_{i \in [N]}, B, \mathbf{S}_1)$.


\subsection{Decomposed Policy Iteration} \label{s.decomposedPI}

We introduce a natural policy closely related to the ubiquitous \textit{policy iteration} (PI) algorithm \citep{howard1960dynamic}.
As the name suggests, policy iteration is an iterative algorithm that maintains a policy and updates it at each iteration to improve its performance.
Our policy, $\DPI$, can be thought of as performing \textit{one step} of policy iteration.
In addition, $\DPI$ differs from vanilla PI in that the algorithm decomposes to the patient level, which allows us to remove dependence on the exponentially sized state and action space.


\subsubsection{Policy.}
For a policy $\pi: \cS^N \times [T] \rightarrow \Aspace$, let $S^\pi_{it}$ and $A_{it}^\pi$ be the induced random variables corresponding to the state and action, respectively, of patient $i$ at time $t$ under policy $\pi$.
Then, we define 
\begin{align} \label{eq:q}
q_{it}^\pi(s, a) = \bE_{\pi}\left[\sum_{t'=t}^T R(S_{it'}^\pi, S_{i,t'+1}^\pi, A_{it'}^\pi) \;\bigg|\; S_{it}^\pi = s, A_{it}^\pi = a \right],
% q_{it}^\pi(s, a) = \bE_{\pi}\left[\sum_{t'=t}^T R(S_{it'}^\pi, S_{i,t'+1}^\pi, a) \;\bigg|\; S_{it}^\pi = s \right],
\end{align}
which represents the expected future reward from patient $i$ when running policy $\pi$, conditioned that they were in state $s$ and were given action $a$ at time $t$.
Next, we define an \textit{intervention value}, $z^{\pi}_{it}(s)$, to be the difference in the $q^{\pi}_{it}$ values under $a=1$ and $a=0$:
\begin{align*}
z^\pi_{it}(s) = q_{it}^{\pi}(s, 1) - q_{it}^{\pi}(s, 0).
\end{align*}
$z_{it}(s)$ is the difference in expected total future reward from patient $i$ under policy $\pi$ when the patient is given the intervention, compared to when they are not.

% \subsubsection{Decomposed-PI.}
% \subsubsection{Policy.}
Given a policy $\pi$, $\DPI(\pi)$ is the policy that gives the intervention to the patients with the highest intervention values.
Formally, 
\begin{align*}
\DPI(\pi)((S_{1t}, \dots, S_{Nt}), t) \in \argmax_{A \in \cA_N} \sum_{i \in [N]} A_{it} z^\pi_{it}(S_{it}).
\end{align*}

\subsubsection{Implementation.} \label{s.implementation}
The main task needed for $\DPI(\pi)$ is to compute the intervention values through $q^\pi$.
Given data containing trajectories of states and rewards generated from policy $\pi$, estimating $q^\pi$ corresponds to a \textit{prediction} problem.
% \textit{on-policy evaluation}, 
% which is simply a prediction problem.
% one of the simplest tasks in reinforcement learning.
Prediction is one of the simplest tasks in reinforcement learning, and one can use Monte Carlo methods or temporal difference learning for this task \citep{sutton2018reinforcement}.
In \cref{s.casestudy}, we provide a detailed description of a practical implementation of $\DPI$ applied to a behavioral health platform, where we use a linear function approximation to estimate $q^{\pi}$.

\subsubsection{Comparison to policy iteration.}
Given policy $\pi$, one step of PI outputs a new policy $\pi'$ via the following procedure.
For a state profile $\bS = (S_1, \dots, S_N)$ and action profile $\bA = (A_1, \dots, A_N)$, define $Q_{t}^\pi(\bS, \bA)$ to be the expected total future reward when action $\bA$ is played in state $\bS$ at time $t$, and $\pi$ is followed thereafter. Formally, 
\begin{align}  \label{eq:Q}
Q_{t}^\pi(\bS, \bA) = \bE_{\pi}\left[\sum_{t'=t}^T \sumN R(S_{it'}^\pi, S_{i,t'+1}^\pi, A_{it'}^\pi) \;\bigg|\; S_{it}^\pi = S_i, A_{it}^\pi = A_i \; \forall i \in [N] \right].
% \tilde{z}^{\pi}(\bS) = 
\end{align}
Then, define $\pi'$ as the policy that takes the action $\bA$ with the highest $Q$-value at each state.
That is, for all states $\bS$ and time $t$,
\begin{align*}
\pi'(\bS, t) \in \argmax_{\bA \in \cA_N} Q_{t}^\pi(\bS, \bA).
\end{align*}

% Both $\DPI$ and one iteration of PI have the goal of improving on the base policy $\pi$ by changing the action 
The main difference between one iteration of PI and $\DPI$ is that PI uses the function $Q:\cS^N \times \cA_N \rightarrow \bR$ defined in \eqref{eq:Q}, whereas $\DPI$ uses the function $q: \cS \times \{0, 1\} \rightarrow \bR$ defined in \eqref{eq:q}.
$Q$ takes as input the \textit{system} state and action, while $q$ takes as input the state and action of a single patient -- this induces a large disparity in how easy it is to estimate these two functions from data.
Note that the size of the system state and action space is $|\cS|^N$ and $N \choose B$, respectively, 
whereas the size of the state and action space for a single patient is $|\cS|$ and 2, respectively.
% which are both exponentially larger than the ones for a single patient, which is $|\cS|$ and 2 respectively.
Then, if one used a tabular approach to estimating these functions, learning $Q$ requires estimating $|\cS|^N {N \choose B}$ quantities, while learning $q$ requires estimating $2N|\cS|$ quantities (since there is a $q$ function for each patient).
The latter is an exponentially fewer number of estimation tasks than the former.


% \jnote{talk about more iterations of PI? That would require either off-policy evaluation or more interactions with the updated policy.}


\section{Theoretical Guarantees} \label{s.theory}
In this section, we establish a performance guarantee for $\DPI$ under a special case of the model from \cref{s.gmodel}---one that is representative of many behavioral health settings pertaining to patient engagement. We first introduce that special case in \cref{ss.2statemodel}, then describe the form of the $\DPI$ policy for that case in \cref{ss.simplepolicy}, before presenting our performance guarantees (along with a proof outline) in \cref{ss.perfguar}.

\subsection{Two-State MDP Model} \label{ss.2statemodel}
% We now describe a special case of the model in \cref{s.gmodel} that captures the notion of habitual behaviors.
% Our main theoretical result pertains to this model.
The state space is $\cS = \{0, 1\}$, where 0 and 1 correspond to an undesired and desired state, respectively.
Under the null action ($A=0$), $p_i$ and $q_i$ represent the probability of transitioning from state 0 to 1 and 1 to 0, respectively.
The intervention ($A=1$) only changes the probability of transitioning from state 0 to 1, which becomes $p_i + \tau_i$.
We assume that $p_i, q_i, \tau_i \in [0, 1]$ for all $i$.
Lastly, the reward is simply equal to the resulting state; i.e.,\ $R_i(S, S', A) = S'$.
This represents the goal of maximizing the fraction of time that the patient verifies.
The MDP for one patient is specified by the parameters $(p_i, q_i, \tau_i)$, and this MDP is summarized in Figure~\ref{fig:2_state_mc}.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.55\linewidth]{figs/2_stateMDP_loops}
  \caption{MDP for patient $i$.}
  \label{fig:2_state_mc}
\end{center}
\end{figure}

All other aspects of the model in \cref{s.gmodel} stay the same, including the system MDP that is derived by combining the patient MDPs via a budget constraint.
Note that the set of possible policies is finite, since both the state and action spaces are finite. Therefore, for any instance, there exists an optimal policy $\OPT$ which maximizes the objective, $\bE[\sumT \sumN S_{i,t+1}]$. 

This special-case model captures the habitual nature of patient behavior.
 % that we observe in the Keheala data (TODO).
We note that similar Markov models have been used in the literature to model patient behavior.
\cite{mate2020collapsing} and \cite{mate2022field} use the same two-state model and applied it to the setting of TB treatment adherence and maternal, health respectively.
\cite{biswas2021learn} also studies maternal health in which they employ a similar MDP with three states.
The two-state Markov model is originally from the literature on communication systems, where it is referred to as a Gilbert-Elliot channel \citep{gilbert1960capacity}.
The objective function $\bE[\sumT \sumN S_{i,t+1}]$ captures the 
platform's goal of having its users be in the `desired' state, and the same objective has been used in the literature \citep{mate2020collapsing,mate2022field,biswas2021learn}.
% Note that the intervention only affects the probability of transitioning from state 0 to 1. Hence under this model, the intervention value is zero for any patient in state 1, and $\DPI$ will only choose patients that are in state 0.

\subsection{The $\DPI$ Policy for the Two-State MDP Model} \label{ss.simplepolicy}
Let $\NULL$ be the policy that only takes the null action; i.e.\ $\NULL(\bS, t) = (0, \dots, 0)$ for all $\bS, t$.
Our result is with respect to the policy $\DPIs \triangleq \DPI(\NULL)$.
We note that for the infinite horizon version of the simple MDP model introduced above, $\DPIs$ has a very simple form. 
\begin{proposition} \label{prop:z_clean_form}
Suppose $T = \infty$. Then, 
$z^{\NULL}_{it}(0) = \tau_i/(p_i + q_i)$ and $z^{\NULL}_{it}(1) = 0$.
\end{proposition}
That is, the $\DPIs$ policy orders patients in state 0 by the index $\tau_i/(p_i + q_i)$, and intervenes on those with the highest values.
The index increases with $\tau$ and decreases with $p$ and $q$, which is intuitive. 
$\tau$ is the `treatment effect' of an intervention on the probability that the patient switches from state 0 to 1.
The smaller the value of $q$, the longer the patient will stay in state 1 once they are there, increasing the `bang-for-the-buck' of an intervention.
As for $p$, a larger value means that a patient is likely to move to state 1 \textit{without} an intervention; hence lower priority is given to those with a large $p$.


\subsection{Performance Guarantee}\label{ss.perfguar}
% Let $\NULL$ be the policy that only takes the null action; i.e.\ $\NULL(\bS, t) = (0, \dots, 0)$ for all $\bS, t$.
% Our result is with respect to the policy $\DPIs \triangleq \DPI(\NULL)$.
As a slight abuse of notation, we use $\OPT, \NULL$ and $\DPIs$ to also denote the expected reward of those policies.
Before stating the most general version of our theorem, we first state the corollary that states the main implication of the performance guarantee.
\begin{corollary}  \label{thm:vanilla_result}
For any problem instance of the two-state model,
\begin{align*}
\OPT - \NULL \leq 2(\DPIs - \NULL).
\end{align*}
\end{corollary}

\textbf{Discussion of \cref{thm:vanilla_result}.}
\cref{thm:vanilla_result} states a 2-approximation for $\DPIs$ with respect to the \textit{improvement} over the null policy.
That is, using $\NULL$ as a baseline, $\DPIs$ achieves at least half of the improvement in reward as compared to that of the optimal policy.
This is significantly stronger than and implies the \textit{usual} notion of a 2-approximation result, which is the following:
\begin{corollary}[Weaker result]  \label{thm:weaker_result}
For any problem instance of the two-state model,
$\OPT\leq 2\DPIs$.
\end{corollary}
Because a patient can transition from state 0 to 1 \textit{without} an intervention (if $p_i > 0$), the $\NULL$ policy can achieve a significant fraction of $\OPT$.
If it was the case that $\NULL$ is more than half of $\OPT$, then \cref{thm:weaker_result} would be vacuous.
On the other extreme, if an intervention was \textit{necessary} for all patients to transition to state 1
(i.e.\ $p_i = 0$ for all $i$), then it would be that $\NULL = 0$, in which case the result of \cref{thm:vanilla_result} is equivalent to that of \cref{thm:weaker_result}.
% \cref{thm:main_result} is a strict improvement over \cref{thm:weaker_result} on any instance where $\NULL > 0$.
% Therefore, how much \cref{thm:main_result} improves over \cref{thm:weaker_result} depends on the instance parameters. It is a strict improvement whenever $\NULL > 0$.





\subsubsection{Statement of main result.}
% \cref{thm:vanilla_result} is a corollary of the following two results, that actually show a stronger statement.
First, we establish that for any algorithm $\ALG$, the quantity $\ALG - \NULL$ can be written as the sum of the intervention values of the chosen patients.
\begin{proposition} \label{prop:alg_minus_base_main}
For any policy $\ALG$,
% Let $\ALG$ be any policy. Then,
\begin{align} \label{eq.difference_characterization}
\ALG - \NULL = \bE\left[\sumT \sum_{i \in [N]: A_{it}^{\ALG} = 1} z^{\NULL}_{it}(0) \right].
\end{align}
\end{proposition}
\cref{prop:alg_minus_base_main} establishes that the intervention values are indeed the relevant quantity that one needs to maximize in order to maximize $\ALG-\NULL$.
Then, we need to show that when one \textit{greedily} chooses patients with the highest intervention values (as $\DPI$ does), it achieves at least half of the total intervention values of the optimal policy.
For any policy $\ALG$, let $I_t^{\ALG}(0)$ be the set of patients in state 0 at time $t$ under $\ALG$.
Denote by $D_t(I_t^{\ALG}(0)) \subseteq I_t^{\ALG}(0)$ the subset of patients that $\DPI$ \textit{would choose} out of $I_t^{\ALG}(0)$, the $B$ patients with the highest positive intervention values.
The next result shows that the sum of intervention values of the patients in $D_t(I_t^{\ALG}(0))$, will lead to at least half of total sum intervention values for an optimal policy.
\begin{theorem}[Main Result]  \label{thm:main_result}
For any $\ALG$, 
\begin{align} \label{eq.main_result}
\OPT - \NULL \leq 2 \bE\left[ \sumT \sum_{i \in D_t(I_t^{\ALG}(0))} \yit^{\NULL}(0) \right].
\end{align}
\end{theorem}
Note that if $\ALG = \DPI$, then $D_t(I_t^{\ALG}(0))$ is simply the patients that $\DPI$ chooses at time $t$; in that case, the RHS of \eqref{eq.main_result} equals the RHS of \eqref{eq.difference_characterization}.
Then, by \cref{prop:alg_minus_base_main}, the RHS of \eqref{eq.main_result} equals $\DPI - \NULL$, which corresponds exactly to the statement of \cref{thm:vanilla_result}.
However, \cref{thm:main_result} is a stronger statement than \cref{thm:vanilla_result} --- we describe its implications on robustness in the next section.



\subsubsection{Robustness under estimation errors.} \label{s.robustness}
Suppose an algorithm $\ALG$ chooses patients with intervention values that \textit{approximate} the intervention values of the patients with the highest intervention values, $D_t(I_t^{\ALG}(0))$. 
Then, $\ALG$ also admits a performance guarantee.
\begin{corollary}  \label{corr:approx}
Suppose $\ALG$ satisfies, for some $\alpha \in [0, 1]$,
\begin{align} \label{eq.approxvalues}
\bE\left[\sumT \sum_{i \in [N]: A_{it}^{\ALG} = 1} \yit^{\NULL}(0) \right]\geq \alpha \bE\left[\sumT  \sum_{i \in D_t(I_t^{\ALG}(0))} \yit^{\NULL}(0)  \right].
\end{align}
Then,
\begin{align*}
\OPT-\NULL \leq \frac{2}{\alpha} (\ALG-\NULL).
\end{align*}
\end{corollary}
This result follows immediately from \cref{thm:main_result} and \cref{prop:alg_minus_base_main}.

% Then, \cref{thm:main_result} with \cref{prop:alg_minus_base_main} immediately implies that $\ALG$ also admits an approximation guarantee.
The RHS of \eqref{eq.approxvalues} represents the intervention values of the patients that $\DPI$ would have chosen at each time step, when $\ALG$ was run.
Hence for example, if at every time step, $\ALG$ chooses patients whose interventions values are at least half as that of the patients with the highest intervention values, then \eqref{eq.approxvalues} is satisfied with $\alpha = 1/2$.
The condition \eqref{eq.approxvalues} is weaker than this, since the $\alpha$-approximation only has to be satisfied in aggregate over all time steps.

This result implies that one does not have to run $\DPI$ \textit{exactly} in order to achieve good performance. 
In practice, one may estimate the intervention values from data --- even if the estimation is not perfect, \cref{corr:approx} ensures a performance guarantee. 



\subsubsection{Proof Summary} \label{s.proofsketch}
We provide a short summary of the proof of the main result.
The full proof can be found in \cref{s.app.proof}.

First, we introduce a novel sample path coupling mechanism that makes it easier to compare performance between policies.
% This coupling uses shared Bernoulli variables at each time step along with a specific order of state transitions using these variables.
This coupling introduces Bernoulli variables for each patient and time step, $P_{it} \sim \Bern(p_i)$, $Q_{it} \sim \Bern(q_i / (1-p_i))$, $K_{it} \sim \Bern(\tau_i/(1-p_i))$), that are shared between all policies.
Then, the state transitions of the patients are modified in a way that depend on these shared variables.
This specific coupling mechanism is defined in a way to make it easy to characterize $\ALG-\NULL$ for any policy $\ALG$, which we use to show \cref{prop:alg_minus_base_main}.
% \cref{prop:alg_minus_base_main} states that the performance improvement, $\ALG-\NULL$, exactly equals the sum of the intervention values of all patients chosen by $\ALG$.

Then, to show \cref{thm:main_result} given \cref{prop:alg_minus_base_main}, we need to show that when one \textit{greedily} chooses patients with the highest intervention values (as $\DPI$ does), it achieves at least half of the total intervention values of the optimal policy.
To show this, we make a connection from our setting to the literature in revenue management on \textit{reusable resources}.
The mapping from our setting to reusable resources is that each patient can be thought of as a unit of inventory of a resource that is `in stock' if the patient is in state 0, and is `out of stock' if they are in state 1. 
Reward is collected when the patient switches from state 0 to state 1, which corresponds to `selling' the resource.
The resource is `reusable' in the sense that when the patient switches from state 1 to state 0, the inventory returns to stock.
With this connection, we borrow and extend the proof technique from \cite{gong2021online}, where a 1/2 approximation for the greedy policy was shown for online assortment optimization with reusable resources.
Extending their technique to our setting requires a careful analysis with respect to our new coupling mechanism.
In particular, we show that any reward collected by $\OPT$ was either also collected by $\DPI$ in the same time step, 
or there exists a previous time step in which $\DPI$ collected the same reward.
The latter task heavily relies on the precise construction of the sample path coupling. 



