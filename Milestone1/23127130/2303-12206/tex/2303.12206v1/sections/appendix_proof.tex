%!TEX root=../Draft1.tex

We first define a couple of terms and notation.
We say that a patient is `chosen' to mean that the patient received an intervention, and we say that a patient is `available' to mean they are in state 0.
Let $I_t(s) \subseteq [N]$ be the set of patients that are in state $s \in \{0, 1\}$ at time $t$. 
We use $A_t \subseteq [N]$ to refer to the subset of patients who are chosen (instead of using the notation $A_{it} \in \{0, 1\}$ to denote whether patient $i$ is chosen).
Without loss of generality, we assume that $A_t \subseteq I_t(0)$. 
This is because giving an intervention to a patient in state 1 has no impact on their transitions.
We write $z_{it}(0)$ to refer to $z^{\NULL}_{it}(0)$.
% , the intervention value of patient $i$ at time $t$ when they are in state $0$.
We put $\pi$ in the superscript, $I^\pi_t(s)$ and $A^\pi_t$, to refer to the random variables induced by running policy $\pi$.


\subsection{Sample path coupling}
We specify a new set of model dynamics that couples different policies through shared random variables.
We will show that these new dynamics are equivalent to the original dynamics specified in \cref{ss.2statemodel}.

\noindent
\textbf{New dynamics.}
At each time $t=1, \dots, T$, the following occurs:
\begin{enumerate}
	\item States $S_{it}$ are observed for all patients $i$.
	\item A policy selects a subset of patients $A_{t} \subseteq I_t(0)$ for an intervention.
	\item For every patient $i \in [N]$, draw independent, Bernoulli variables
	$Q_{it} \sim \Bern(q_i / (1-p_i))$, $P_{it} \sim \Bern(p_i)$, $K_{it} \sim \Bern(\tau_i/(1-p_i))$.
	\item Patient transitions occur in the following order:
\begin{enumerate}[label=(\roman*)]
	\item \textbf{Return to state 0:} For each patient $i \in I_t(1)$ and $Q_{it} = 1$, the patient returns to state 0.
	Let $I_t'(0) = I_t(0) \cup \{i : Q_{it} = 1, i \in I_t(1)\}$ be the new set of patients in state 0.
	\item \textbf{Passive transitions to state 1:}  For each patient $i \in I_t'(0)$ and $P_{it} = 1$, the patient goes to state 1.
	Let $I_t''(0) = I_t'(0) \setminus \{i : P_{it} = 1, i \in I_t'(0)\}$ be the new set of patients in state 0.
	\item \textbf{Active transitions to state 1:}  For each patient $i \in I_t''(0) \cap A_{t}$ with $K_{it} = 1$, the patient goes to state 1.
	Then, $I_{t+1}(0) = I_t''(0) \setminus \{i : K_{it} = 1, i \in I_t''(0) \cap A_{t}\}$ are the set of patients in state 0 at the next time step, and $I_{t+1}(1) = [N] \setminus I_{t+1}(0)$.
\end{enumerate}
	\item We collect the reward $I_{t+1}(1)$.
\end{enumerate}

\textbf{Equivalence.}
% For each patient $i$, let $q'_i = q_i / (1-p_i)$. We claim that the original model dynamics (from \cref{s.2statemodel}) with patients with parameters $(p, q, \tau)$ is equivalent to the new model dynamics with patients with parameters $(p, q', \tau)$.
We claim that the original model dynamics (from \cref{ss.2statemodel}) is equivalent to these new model dynamics.
To show this, we need to show that the transition probabilities between states are equal under both models.

In the new model, suppose a patient is in state 0.
If they were not chosen, they can only transition to 1 under step (ii), which occurs with probability $p_i$.
If they were chosen, they can transition to 1 under either step (ii) or (iii).
In total, they transition with probability $p_i + (1-p_i) \cdot \tau_i/(1-p_i) = p_i + \tau_i$.
Next, suppose a patient is in state 1.
For the patient to transition to state 0, it must be that $Q_{it} = 1$ and $P_{it} = 0$.
This occurs with probability $q_i / (1-p_i) \cdot (1-p_i) = q_i$.
Therefore, the transition probabilities between the two models are equal for all patients.

\textbf{Coupling sample paths of policies.} 
From now on, we assume that all policies are coupled through the variables $P_{it}, Q_{it}, K_{it}$.
% Now, given multiple policies, we couple the sample paths through the variables 
This coupling immediately gives us the following properties.

\begin{proposition} \label{prop:coupling_property0}
For any policy, if $P_{it} = 1$, then $S_{i, t+1} = 1$.
% Let $\ALG$ be any policy. If $S_{it}^{\BASE} = 1$, then $S_{it}^{\ALG} = 1$.
\end{proposition}
This follows immediately from the dynamics.
The next property says that if a patient is in state 1 under the $\NULL$ policy, they must also be in state 1 under any other policy.

\begin{proposition} \label{prop:coupling_property}
Let $\ALG$ be any policy. If $S_{it}^{\NULL} = 1$, then $S_{it}^{\ALG} = 1$.
This implies that if $S_{it}^{\ALG} = 0$, then $S_{it}^{\BASE} = 0$.
\end{proposition}

\begin{myproof}
Let $i, t$ be such that $S_{it}^{\BASE} = 1$.
Let $t' = \max\{t' < t : P_{it'} = 1\}$ be the most recent time $P$ was 1.
Since $S_{it}^{\BASE} = 1$, $Q_{is} = 0$ for every $s \in \{t'+1, \dots, t-1\}$.
Since $P_{it'} = 1$, \cref{prop:coupling_property0} implies $S^{\ALG}_{i, t'+1} = 1$.
% Therefore, under $\ALG$, since $P_{it'} = 1$, it must be that patient $i$ transitions to state 1 during time $t'$ (if it wasn't already there).
Since $Q_{is} = 0$ for every $s \in \{t'+1, \dots, t-1\}$, $S_{it}^{\ALG} = 1$.
\end{myproof}

% We also have the property that any transitions from $1 \rightarrow 0$ is shared amongst all policies.
% \begin{proposition} \label{prop:coupling_property2}
% Let $\ALG$, $\ALGT$ be any two policies. 
% If $S_{it}^{\ALG} = S_{it}^{\ALG2} = 1$ and $S_{i, t+1}^{\ALG} = 0$, then 
%  $S_{i, t+1}^{\ALGT} = 0$.
% \end{proposition}

% \begin{myproof}
% If $S_{it}^{\ALG} = 1$ and $S_{i, t+1}^{\ALG} = 0$, then it must be that $Q_{it} = 1$ and $P_{it} = 0$.
% This implies $S_{i, t+1}^{\ALGT} = 0$.
% \end{myproof}

% \subsection{Characterizing the Advantage Function}
\subsection{Characterizing $\ALG-\NULL$ via Intervention Values}
Using this coupling, we will show the following result which relates the intervention values $z_{it}(0)$ with the quantity $\ALG - \NULL$.
\begin{proposition} \label{prop:alg_minus_base}
Let $\ALG$ be any policy, and let $A_t$ be the set of patients $\ALG$ chooses at time $t$.
Then,
\begin{align*}
\ALG - \NULL = \bE\left[\sumT \sum_{i \in A^{\ALG}_t} z_{it}(0) \right].
\end{align*}
\end{proposition}

In order to show this, we first we define the notion of a \textit{counterfactual sample path} of a patient being chosen at a time $t$.
% Consider the $\NULL$ policy which never chooses patients.
% Starting the $\NULL$ policy, 
% Let $\tilde{S}_{i r | t}$ be the state of patient $i$ at time $r > t$ if patient $i$ was chosen at time $t$ and never thereafter, and where the $\NULL$ policy was followed up to time $t$.
Define a policy $\ONE(i, t)$ to be the same as the $\NULL$ policy, except that it chooses patient $i$ once at time $t$.
Define $\tilde{S}_{i r | t} = S^{\ONE(i, t)}_{ir}$ to be the state of patient $i$ at time $r$ under this policy, which we call the \textit{counterfactual state}.
% Then, let $Z_{it}$ be the length of time that the counterfactual state diverges from the $\NULL$ policy:
Then, let $Z_{it}$ be the number of times that the counterfactual state is not equal to the state under $\NULL$:
\begin{align} \label{eq:define_Y}
% \tilde{Z}_{it} &= \min\{t' > t; \; : \; S_{it'}^{\NULL} = \tilde{S}_{it' | t} \} - t -1, \\
% Z_{it} &= \min\{\tilde{Z}_{it}, T-t\}. \label{eq:end_of_horizon}
Z_{it} &= |\{t' \in [T] \; : \; S_{it'}^{\NULL} \neq \tilde{S}_{it' | t} \}|.
\end{align}


Note the following properties:
\begin{itemize}
	\item The two states are always equal before time $t$.
	($S_{it'}^{\NULL} =\tilde{S}_{it' | t}$ for any $t' \leq t$.)
	\item Once the two states converge at some time $t' > t$, they will never diverge again since the policies are the same. (If $S_{it'}^{\NULL} =\tilde{S}_{it' | t}$ for some $t' > t$, then the same holds for any $r > t'$.)
	\item The only way that the states can be different is if the state is 0 under $\NULL$ and 1 under $\ONE(i, t)$, due to \cref{prop:coupling_property}.
	% \item If $S_{it}^{\NULL} = 1$, then $Z_{it} = 0$.
\end{itemize}
% Note that the only way that the policies $\NULL$ and $\ONE(i, t)$ can diverge is if the state is 0 under $\NULL$ and 1 under $\ONE(i, t)$, due to \cref{prop:coupling_property}.
Therefore, $Z_{it}$ represents exactly the increase in total reward from patient $i$ caused by the intervention at time $t$, compared to $\NULL$.
More specifically, when $S_{it} = 0$, $Z_{it}$ is the number of time steps that the patient was in state 1 right after time $t$, before it transitioned back to state 0 or the state under the $\NULL$ policy also moved to state 1 (or we reached the last time step).
This is exactly the definition of the intervention value $z_{it}$ when starting from state 0.
That is, $\bE[Z_{it} | S_{it} = 0] = z_{it}(0)$.

\textbf{Characterizing $Z_{it}$.}
More specifically, we can write the distribution of $Z_{it}$ explicitly.
Let $B_{it} = \min\{t' > t : P_{it'} = 1 \} - t$ be the length of time from $t$ until $P_{it} = 1$.
Let $L_{it} = \min\{t' > t : Q_{it'} = 1 \} - t$ be length of time from $t$ until $Q_{it} = 1$.
Then, 
\begin{align} \label{eq:Z_explicit}
Z_{it} = \bI(S_{it}^\NULL = 0, P_{it} = 0, K_{it} = 1) \min\{B_{it}, L_{it}, T-t+1\}.
\end{align}
The indicator represents the fact that $i$ would not transition to state 1 under $\NULL$ ($P_{it} = 0$), but it would transition under $\ONE(i, t)$ ($K_{it} = 1$).
That is, at time $t+1$, the patient is in state 1 under $\ONE$ but in state 0 under $\NULL$.
The $\min\{B_{it}, L_{it}\}$ term counts how long this is the case.
This could end either because patient transitions to state 1 under $\NULL$ (captured by $B_{it}$), or it could be that the patient in $\ONE$ transitions back to state 0 (captured by $L_{it}$).

\textbf{Aside: Proof of \cref{prop:z_clean_form}.}
If we assume $T = \infty$, $z_{it}(0) = \bE[Z_{it} | S_{it} = 0]$ can be written as
\begin{align*}
z_{it}(0)
&= \Pr(K_{it} = 1) \cdot \Pr(P_{it} = 0) \cdot \bE[\min\{B_{it}, L_{it}\}] \\
&= (\tau/(1-p)) \cdot (1-p) \cdot \frac{1}{p + q} \\
&=  \frac{\tau}{ p + q},
\end{align*}
where the last term of the second step comes from the fact that $\min\{B_{it}, L_{it}\}$ is a geometric random variable with parameter $1-(1-\Pr(P_{it} = 1))(1-\Pr(Q_{it} = 1)) = p+q$.
This proves \cref{prop:z_clean_form}.

% \begin{lemma} \label{eq:Z_K}
% If $Z_{it} \geq 1$, it must be that $K_{it} = 1$ and $S_{it}^\NULL = 0$.
% \end{lemma}

We are now ready to prove \cref{prop:alg_minus_base}.
\begin{myproof}[Proof of \cref{prop:alg_minus_base}]
\cref{prop:coupling_property} says that it will never be the case that $S_{it}^{\NULL} = 1$, while $S_{it}^{\ALG} = 0$.
Therefore, 
\begin{align*}
\ALG - \NULL = \bE\left[\sumT \sum_{i \in [N]} \bI(S_{it}^{\ALG} = 1, S_{it}^{\NULL} = 0) \right].
\end{align*}
Due to the sample path coupling, $S_{it}^{\ALG} \neq S_{it}^{\NULL}$ can only occur if
$\ALG$ chose patient $i$ at a prior time step, and the states have been different since then (if the states converged, it will stay the same unless $\ALG$ chose the patient again).

Let $t_1, \dots, t_n$ be the time steps where $\ALG$ chooses patient $i$ when patient $i$ was in state 0.
When $\ALG$ chooses patient $i$ at time $t_k$, the state under $\ALG$ follows the counterfactual state until the next time $i$ is chosen in state 0.
That is, for $r \in [t_k+1, \dots, t_{k+1}]$, $S_{ir}^{\ALG} = \tS_{ir|t}$.

Note that since $\ALG$ can only choose a patient when it is in state 0, it must be that 
$S_{i t_k}^{\ALG} = 0$ for all $k$.
Therefore, $S_{i t_{k+1}}^{\ALG} = \tS_{it_{k+1}|t_k} = 0 = S_{it_{k+1}}^{\NULL}$, where the last equality follows from \cref{prop:coupling_property}.
Hence, $t_{k+1} \geq t + Z_{it_k}$ by definition of $Z_{it_k}$.
In between $t_k$ and $t_{k+1}$, the number of times the event $\{S_{ir}^{\ALG} = 1, S_{ir}^{\NULL} = 0\}$ occurs is exactly $Z_{it}$.


Therefore,
\begin{align}
\ALG - \NULL 
&= \bE\left[\sumT \sum_{i \in A^{\ALG}_t} Z_{it} \right]  \label{eq:propRandom}\\
&= \sumT \sum_{i \in [N]} \bE\left[\bI(i \in  A^{\ALG}_t) Z_{it} \right] \nonumber \\
&= \sumT \sum_{i \in [N]} \bE\left[\bI(i \in  A^{\ALG}_t) \right]\bE\left[ Z_{it} \;|\; i \in  A^{\ALG}_t \right] \nonumber \\
&= \sumT \sum_{i \in [N]} \bE\left[\bI(i \in  A^{\ALG}_t) \right]\bE\left[ Z_{it} \;|\; i \in  A^{\ALG}_t, S_{it} = 0 \right], \nonumber 
\end{align}
where the last equality follows since if $i \in  A^{\ALG}_t$, then it must be that $S_{it} = 0$.
% where the third equality follows because $Z_{it}$ is independent of the algorithm. (But $i \in A^{\ALG}_t$ means the state is in 0?)
Note that $Z_{it}$ is a only function of the state $S_{it}$, and all future Bernoulli variables, $(P_{it'}, Q_{it'}, K_{it'})_{t' \geq t}$, from \eqref{eq:Z_explicit}. 
For any algorithm, its decision at time $t$, $\{i \in  A^{\ALG}_t\}$, is independent of all future Bernoulli variables.
% Note that $Z_{it}$ is defined with respect to the counterfactual sample path, and hence it is independent of any particular algorithm $\ALG$.
Therefore, 
\begin{align*}
\bE\left[ Z_{it} \;|\; i \in  A^{\ALG}_t, S_{it} = 0 \right] 
= \bE\left[ Z_{it} \;|\; S_{it} = 0 \right] = z_{it}(0).
\end{align*}
Combining, we get
\begin{align} \label{eq:propDet}
\ALG - \NULL  
&= \bE\left[\sumT \sum_{i \in A^{\ALG}_t} z_{it}(0) \right] 
\end{align}
as desired.
\end{myproof}





% \subsection{Proof of \cref{thm:2_approx_ys_stronger}}
\subsection{Proof of \cref{thm:main_result}}

Fix any policy $\ALG$.
Let $I^{\ALG}_t$ and $I^*_t$  be the set of patients that are in state 0 under $\ALG$ and $\OPT$ respectively at time $t$.
% Let $A^{\ALG}_t \subseteq I^{\ALG}_t$ and $A^*_t\ \subseteq I^*_t$ be the set of patients that $\ALG$ and $\OPT$ chooses respectively at time $t$.
Recall that $D_t(I_t^{\ALG}) \subseteq I^{\ALG}_t$ are the patients that $\DPI$ would choose, out of patients in $I^{\ALG}_t$.
We first decompose the rewards based on whether or not a patient that was chosen in $\OPT$ was available to be chosen under $\ALG$.
\begin{align}
\OPT - \NULL
&= \bE\left[ \sumT \sum_{i \in A^*_t} Z_{it} \right]\nonumber \\
&= \bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} Z_{it} \right]
+ \bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Z_{it} \right].
% &= \bE_\omega\left[ \sumT \sum_{i \in I^*_t(\omega) \cap I_t(\omega)} \bI(i \in A_t)  Y_{it}  \right] + \bE_\omega\left[ \sumT  \sum_{i \in I^*_t(\omega) \setminus I_t(\omega)} \bI(i \in A_t)  Y_{it}  \right]
\label{eq:opt_minus_base_decomp}
\end{align}

For the first term in \eqref{eq:opt_minus_base_decomp},
by definition of $D_t(I_t^{\ALG})$, we have
$\sum_{i \in A^*_t \cap I^{\ALG}_t} z_{it} \leq \sum_{i \in D_t(I_t^{\ALG})} z_{it}$.
Therefore, 
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} Z_{it} \right]
 = \bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} z_{it}(0) \right]
 \leq 
 \bE\left[ \sumT \sum_{i \in D_t(I_t^{\ALG})}  z_{it}(0) \right],
\end{align*}
where the first equality follows the same reasoning as \eqref{eq:propRandom}-\eqref{eq:propDet}.

% clearly the total reward (wrt $Y_{it}$) at time $t$ is larger than the first term, by definition of greedy.
% Therefore, 
% $\bE_\omega\left[ \sumT \sum_{i \in I^*_t(\omega) \cap I_t(\omega)} Y_{it}  \right] \leq \GT(\ALG) - \BASE$.

\cref{thm:main_result} then follows from the following proposition which bounds the second term in \eqref{eq:opt_minus_base_decomp}. This term represents rewards from patients who are in state 0 under $\OPT$ but not under $\ALG$.
\begin{proposition} \label{prop:second_term}
\begin{align} \label{eq:second_term_claim}
\bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Z_{it} \right]	
\leq 
\bE\left[ \sumT \sum_{i \in A^{\ALG}_t} Z_{it} \right]	
\end{align}
\end{proposition}

\subsubsection{Proof of \cref{prop:second_term}.}
The main idea of this result is that for every $Z_{it}$ term that contributes to the LHS of \eqref{eq:second_term_claim}, since that patient is not available under $\ALG$, we have already collected this reward under $\ALG$.
We will show a one-to-one mapping from every $Z_{it}$ term on the LHS to a $Z_{i \tau_i(t)}$ term on the RHS of \eqref{eq:second_term_claim}.

% Since such a patient is still in state 1 under $\ALG$, 
Let $\cI = \{(i, t) : i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1\}$ be the set of (patient, time) tuples in which patient $i$ is chosen under $\OPT$ but not available under $\ALG$, and moreover, $K_{it} = 1$.
% Fix $i, t$ such that $i \in A^*_t(\omega) \setminus I_t(\omega)$, and $K_{it} = 1$ 
Note that from \eqref{eq:Z_explicit}, $K_{it} = 1$ is a necessary condition for $Z_{it} > 0$. 
Therefore, the LHS of \eqref{eq:second_term_claim} can be written as
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Z_{it} \right]	
= \bE\left[ \sum_{(i, t) \in \cI} Z_{it} \right].
\end{align*}

Fix $(i, t) \in \cI$.
We have that $S_{it}^{\ALG} = 1$ (since $i \notin I^{\ALG}_t$) and $S_{it}^{\OPT} = 0$ (since $i \in A^*_{t}$).
% Since $i \notin I^{\ALG}_t$, $S_{it}^{\ALG} = 1$, and since $i \in A^*_{t}$, $S_{it}^{\OPT} = 0$.
Define $\tau_i(t) < t$ to be the last time that $i$ was in state 0 under $\ALG$:
\begin{align*}
\tau_i(t) = \max\{ t' < t : S_{it'}^{\ALG} = 0\}.
\end{align*}
That is, under $\ALG$, patient $i$ transitioned from state 0 to state 1 between time $\tau_i(t)$ and $\tau_i(t)+1$ and stayed in state 1 since.
% It must have been that $\ALG$ chose the patient at time $\tau_i(t)$; i.e. $A_{it}$
% It must be that $S_{is}^{\BASE} = 0$ for 
% It must be that during this time, under $\OPT$, the patient is in state 0; i.e. for all $s = \tau_i(t)+1, \dots, t$, $S_{is}^{\OPT} = 0$.
We can show that $\tau_i(t)$ satisfies the following property: it must be that $i$ was chosen at time $\tau_i(t)$ under $\ALG$, and moreover, $Z_{i \tau_i(t)}$ is at least $t - \tau_i(t)$.

\begin{claim} \label{claim:properties_of_y}
Let $(i, t) \in \cI$.
Then, $i \in A^{\ALG}_{\tau_i(t)}$, and $Z_{i \tau_i(t)} \geq t - \tau_i(t)$.
\end{claim}	
\begin{myproof}[Proof of \cref{claim:properties_of_y}]
Let $(i, t) \in \cI$.
By definition of $\tau_i(t)$, $S_{i, \tau_i(t)}^{\ALG} = 0$ and $S_{i, \tau_i(t)+1}^{\ALG} = 1$.
To the contrary, suppose $i \notin A_{\tau_i(t)}^{\ALG}$.
Then, $i$ transitioned to state 1 without being chosen, and therefore $P_{i\tau_i(t)} = 1$.
Then, it must be that $i$ transitions to state 1 under $\OPT$ as well; $S_{i, \tau_i(t)+1}^{\OPT} = 1$.
But since $S_{it}^{\OPT} = 0$, $i$ switches back to state 0 before state $t$, which means that this should also happen under $\ALG$ (due to the sample path coupling).
This is a contradiction by the definition of $\tau_i(t)$.
Therefore, $i \in A_{\tau_i(t)}^{\ALG}$.

Moreover, under the same reasoning, it must be that $K_{i\tau_i(t)} = 1$, and that $S_{is}^{\NULL} = 0$ for all $s \in \{\tau_i(t)+1, \dots, t\}$.
Therefore, $Z_{i \tau_i(t)} \geq t - \tau_i(t)$.
\end{myproof}

% Since $S_{it}^{\OPT} = 0$, it must be that under $\ALG$, there was a previous time step $\tau_i(t) < t$ where $\ALG$ chose $i$ and hence it moved to state 1, and has stayed there since then.
% (It could not have been that $i$ moved to state 1 at time $\tau_i(t)$ \textit{without} being chosen by $\ALG$, since if that were the case, it would have also moved to state 1 under $\OPT$.)
% Specifically, $\tau_i(t)$ can be written as 
% \begin{align*}
% \tau_i(t) = \max\{ t' < t : A_{it'}^{\ALG} = 1\}.
% \end{align*}
% Then, for all $(i, t) \in \cI$, it must be that $Y_{i \tau_i(t)} \geq t - \tau_i(t)$.

We next show that the mapping $\tau_i$ is one-to-one.
\begin{claim} \label{claim:1-1}
If $(i, t), (i, s) \in \cI$ for $t \neq s$, then $\tau_i(t) \neq \tau_i(s)$.
\end{claim}
\begin{myproof}[Proof of \cref{claim:1-1}]
Suppose $t < s$ such that $(i, t), (i, s) \in \cI$.
That is, under $\OPT$, patient $i$ was chosen both at time $t$ and $s$, and the patient was already in state 1 under $\ALG$ at both of these times.
Suppose, for contradiction, $\tau_i(t) = \tau_i(s)$.
This means that under $\ALG$, patient $i$ was chosen at time $\tau_i(t)$, and the patient has stayed in state 1 from time $\tau_i(t)+1$ until at least time $s$.
% This implies that for all $t' \in \{\tau_i(t)+1, \dots, s-1\}$

$(i, t), (i, s) \in \cI$ implies $S^{\OPT}_{it} = S^{\OPT}_{is} = 0$.
% that patient $i$ was in state 0 at both of these times.
Since $K_{it} = 1$, the patient transitioned to state 1 at time $t+1$.
Since $S^{\OPT}_{is} = 0$, it must be that there exists a $t' \in \{t+1, \dots, s-1\}$ such that $Q_{it'} = 1, P_{it'} = 0$.
But since $S^{\ALG}_{it'} = 1$, the fact that $Q_{it'} = 1$ and $P_{it'} = 0$ implies that $S^{\ALG}_{i, t'+1} = 0$, which is a contradiction.
\end{myproof}

Claims \ref{claim:properties_of_y} and \ref{claim:1-1} show that every $(i, t) \in \cI$ maps to one term in the RHS of \eqref{eq:second_term_claim}.
Therefore, the right hand side of \eqref{eq:second_term_claim} is at least
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^{\ALG}_t} Z_{it} \right]	
% \bE_\omega\left[ \sumT \sum_{i \in A_t(\omega)} Y_{it} \right]
\geq 
\bE\left[ \sum_{(i, t) \in \cI} Z_{i\tau_i(t)} \right].
\end{align*}
Then, we are done if we can show
\begin{align*} 
\bE\left[ \sum_{(i, t) \in \cI} Z_{it} \right]
\leq
\bE\left[ \sum_{(i, t) \in \cI} Z_{i\tau_i(t)} \right],
\end{align*}
which we can write as
\begin{align}\label{eq:simpler_goal}
\sumT \sumN \bE[Z_{it} \;|\; (i, t) \in \cI\;] \Pr((i, t)\in \cI)
\leq 
\sumT \sumN \bE[Z_{i\tau_i(t)} \;|\; (i, t) \in \cI\;] \Pr((i, t)\in \cI).
\end{align}

The following claim implies \cref{eq:simpler_goal} and finishes the proof of \cref{prop:second_term}.
\begin{claim} \label{claim:compare_ys}
For every $i \in [N]$ and $t \geq 1$,
\begin{align} \label{eq:inner_term}
\bE[Z_{it} \;|\; (i, t)  \in \cI\;] 
\leq 
\bE[Z_{i\tau_i(t)} \;|\; (i, t)  \in \cI\;]
\end{align}
\end{claim}


\begin{myproof}[Proof of \cref{claim:compare_ys}]
Fix $i, t$.
% Let $Y_i$ be a random variable that has the same distribution as $Y_{it}$ for all $t$.
First, we upper bound the LHS of \cref{eq:inner_term}.
\begin{align*}
\bE[Z_{it} \;|\; (i, t)  \in \cI\;]  
&= \bE[Z_{it} \;|\; Z_{it} \geq 1,  (i, t)  \in \cI\;] \Pr(Z_{it} \geq 1 \;|\; (i, t)  \in \cI) \\
&\leq \bE[Z_{it} \;|\; Z_{it} \geq 1,  (i, t)  \in \cI\;].
\end{align*}
Note that conditioned on $Z_{it} \geq 1$, $Z_{it}$ is only a function of $(P_{it'}, Q_{it'})_{t' > t}$, the `future' with respect to $t$, and the event $\{(i, t)  \in \cI\} = \{i \in A^*_t \setminus I^{\ALG}_t,  K_{it} = 1\}$ is independent of these future random variables.
Therefore, conditioned on $Z_{it} \geq 1$, $Z_{it}$ is independent of $(i, t)  \in \cI$, and hence
\begin{align}\label{eq:lhs_ub} 
\bE[Z_{it} \;|\; (i, t)  \in \cI\;]  
\leq
\bE[Z_{it} \;|\; Z_{it} \geq 1].
\end{align}

Next, consider the RHS of \eqref{eq:inner_term}.
% Since $\tau_i(t) < t$, $Z_{i\tau_i(t)}$ is independent of $K_{it}$ ($Z_{i \tau_i(t)}$ depends on $K_{i \tau_i(t)}$, but not $K_{it}$).
From \cref{claim:properties_of_y}, $(i, t) \in \cI$ implies $Z_{i \tau_i(t)} \geq t - \tau_i(t)$.
Therefore, 
\begin{align*}
\bE[Z_{i\tau_i(t)} \;|\; (i, t)  \in \cI\;]
= \bE[Z_{i\tau_i(t)} \;|\; (i, t)  \in \cI, Z_{i \tau_i(t)} \geq t - \tau_i(t)\;].
\end{align*}
Similar to before, conditioned on $Z_{i \tau_i(t)} \geq t - \tau_i(t)$, $Z_{it}$ is only a function of $(P_{it'}, Q_{it'})_{t' > t}$, the `future' with respect to $t$.
Therefore, conditioned on $Z_{i \tau_i(t)} \geq t - \tau_i(t)$, $Z_{it}$ is independent of $(i, t) \in \cI$, and hence
\begin{align*}
\bE[Z_{i\tau_i(t)} \;|\; (i, t)  \in \cI\;]
&= \bE[Z_{i\tau_i(t)} \;|\; Z_{i \tau_i(t)} \geq t - \tau_i(t)\;] \\
&= \sum_{t' < t} \Pr(\tau_i(t) = t' \;|\; Z_{i \tau_i(t)} \geq t - \tau_i(t)) \bE[Z_{it'} \;|\; Z_{i t'} \geq t - t'] \\ 
&\geq \sum_{t' < t} \Pr(\tau_i(t) = t' \;|\; Z_{i \tau_i(t)} \geq t - \tau_i(t)) \bE[Z_{it'} \;|\; Z_{i t'} \geq 1].
\end{align*}


Note that for $s < t$, $\bE[Z_{is} | Z_{is} \geq 1] \geq \bE[Z_{it} | Z_{it} \geq 1]$.
This is because given the equation for $Z_{it}$ in \eqref{eq:Z_explicit}, the only difference between $Z_{it}$ and $Z_{is}$ is that $Z_{it}$ has a smaller maximum value of $T-t$.
% This is because the distribution of $Z_{is}$ and $Z_{it}$ conditioned on them being greater than 1 is almost the same, except that $Z_{it}$ has a smaller maximum value of $T-t$.
Therefore,
\begin{align}
\bE[Z_{i\tau_i(t)} \;|\; (i, t)  \in \cI\;]
&\geq  \bE[ Z_{i t}  | Z_{i t} \geq 1] \sum_{t' < t} \Pr(\tau_i(t) = t' \;|\; Z_{i \tau_i(t)} \geq t - \tau_i(t)) \\
&= \bE[ Z_{it}  | Z_{it} \geq 1]. \label{eq:rhs_lb} 
\end{align}

Combining \eqref{eq:rhs_lb} and \eqref{eq:lhs_ub} proves the result.
\end{myproof}

