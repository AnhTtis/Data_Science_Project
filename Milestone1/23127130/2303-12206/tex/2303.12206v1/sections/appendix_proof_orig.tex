%!TEX root=../Draft1.tex


\section{Proof of \cref{thm:main_result}}

\subsection{Sample path coupling}
First, we will specify the model dynamics in a way that makes it easy to couple sample paths between different algorithms.
% In particular, this sample path coupling will make it easier to characterize the advantage function.
At each time $t$, the following occurs:
\begin{enumerate}
	\item States $S_{it}$ are observed for all patients $i$, and the reward is collected.
	Let $I_t(s) \subseteq [N]$ be the subset of patients in state $s \in \{0, 1\}$ at time $t$.
	\item A policy selects a subset of patients $A_{t} \subseteq I_t(0)$ for an intervention.
	(The policy may choose patients in $I_t(1)$, but these have no effect so only the patients in $I_t(0)$ are the `effective' chosen patients.)
	\item For every patient $i \in [N]$, draw independent, Bernoulli variables
	$Q_{it} \sim \Bern(q_i)$, $P_{it} \sim \Bern(p_i)$, $K_{it} \sim \Bern(\tau_i/(1-p_i))$.
	\item Patient transitions occur in the following order:
\begin{enumerate}[label=(\roman*)]
	\item \textbf{Return to state 0:} For each patient $i \in I_t(1)$ and $Q_{it} = 1$, the patient returns to state 0.
	Let $I_t'(0) = I_t(0) \cup \{i : Q_{it} = 1, i \in I_t(1)\}$ be the new set of patients in state 0.
	\item \textbf{Passive transitions to state 1:}  For each patient $i \in I_t'(0)$ and $P_{it} = 1$, the patient goes to state 1.
	Let $I_t''(0) = I_t'(0) \setminus \{i : P_{it} = 1, i \in I_t'(0)\}$ be the new set of patients in state 0.
	\item \textbf{Active transitions to state 1:}  For each patient $i \in I_t''(0) \cap A_{t}$ with $K_{it} = 1$, the patient goes to state 1.
	Then, $I_{t+1}(0) = I_t''(0) \setminus \{i : K_{it} = 1, i \in I_t''(0) \cap A_{t}\}$ are the set of patients in state 0 at the next time step.
\end{enumerate}
\end{enumerate}

For each patient $i$, let $q'_i = q_i / (1-p_i)$.
We claim that the original model dynamics (from \cref{sec:model}) with patients with parameters $(p, q, \tau)$ is equivalent to the new model dynamics with patients with parameters $(p, q', \tau)$.
To show this, we need to show that the transition probabilities between states are equal under both models.

In the new model, suppose a patient is in state 0.
If they were not chosen, they can only transition to 1 under step (ii), which occurs with probability $p_i$.
If they were chosen, they can transition to 1 under either step (ii) or (iii).
In total, they transition with probability $p_i + (1-p_i) * \tau_i/(1-p_i) = p_i + \tau_i$.
Next, suppose a patient is in state 1.
For the patient to transition to state 0, it must be that $Q_{it} = 1$ and $P_{it} = 0$.
This occurs with probability $q'(1-p) = q$.
Therefore, the transition probabilities between the two models are equal for all patients.

\textbf{Sample Path Coupling.} Now, given multiple policies, we couple the sample paths through the variables $P_{it}, Q_{it}, K_{it}$.
This coupling gives us the following properties:

\begin{proposition} \label{prop:coupling_property0}
For any policy, if $P_{it} = 1$, then $S_{i, t+1} = 1$.
% Let $\ALG$ be any policy. If $S_{it}^{\BASE} = 1$, then $S_{it}^{\ALG} = 1$.
\end{proposition}
This follows immediately from the dynamics.

\begin{proposition} \label{prop:coupling_property}
Let $\ALG$ be any policy. If $S_{it}^{\BASE} = 1$, then $S_{it}^{\ALG} = 1$.
This implies that if $S_{it}^{\ALG} = 0$, then $S_{it}^{\BASE} = 0$.
\end{proposition}

\begin{myproof}
Let $i, t$ be such that $S_{it}^{\BASE} = 1$.
Let $t' = \max\{t' < t : P_{it'} = 1\}$ be the most recent time $P$ was 1.
Since $S_{it}^{\BASE} = 1$, $Q_{is} = 0$ for every $s \in \{t'+1, \dots, t-1\}$.
Since $P_{it'} = 1$, \cref{prop:coupling_property0} implies $S^{\ALG}_{i, t'+1} = 1$.
% Therefore, under $\ALG$, since $P_{it'} = 1$, it must be that patient $i$ transitions to state 1 during time $t'$ (if it wasn't already there).
Since $Q_{is} = 0$ for every $s \in \{t'+1, \dots, t-1\}$, $S_{it}^{\ALG} = 1$.
\end{myproof}

% We also have the property that any transitions from $1 \rightarrow 0$ is shared amongst all policies.
% \begin{proposition} \label{prop:coupling_property2}
% Let $\ALG$, $\ALGT$ be any two policies. 
% If $S_{it}^{\ALG} = S_{it}^{\ALG2} = 1$ and $S_{i, t+1}^{\ALG} = 0$, then 
%  $S_{i, t+1}^{\ALGT} = 0$.
% \end{proposition}

% \begin{myproof}
% If $S_{it}^{\ALG} = 1$ and $S_{i, t+1}^{\ALG} = 0$, then it must be that $Q_{it} = 1$ and $P_{it} = 0$.
% This implies $S_{i, t+1}^{\ALGT} = 0$.
% \end{myproof}

\subsection{Characterizing the Advantage Function}
Using this coupling, we will now characterize the advantage function, and relate it to $\ALG - \BASE$.
% \begin{align*}
% z_{it} = Q_{it}^{\BASE}(0, 1) - Q_{it}^{\BASE}(0, 0).
% \end{align*}
First, we define the notion of a `counterfactual sample path' of a patient being chosen at a time $t$:
let $\tilde{S}_{i r | t}$ be the state of patient $i$ at time $r > t$ if patient $i$ was chosen at time $t$ and never thereafter.
Let $Y_{it}$ be the length of time that the counterfactual path diverges:
\begin{align} \label{eq:define_Y}
% \bI(i \in A_t) 
\tilde{Y}_{it} &= \min\{t' > t; \; : \; S_{it'}^{\NULL} = \tilde{S}_{it' | t} \} - t -1, \\
Y_{it} &= \min\{\tilde{Y}_{it}, T-t\} \label{eq:end_of_horizon}
% Y_{it} = \min_{t' \in \{t+1, \dots, T\}} \{t' - t -1 ; \; : \; S_{it'} = \tilde{S}_{it' | t}\}
\end{align}
% \jnote{The above expression doesn't take $T$ into account --- there should be a $\min\{Y_{it}, T-t\}$ to account for the end of horizon. For now just consider infinite horizon.}

For example, if the state at time $t+1$ is the same regardless of whether or not the patient was chosen at time $t$, then $Y_{it} = 0$.
\eqref{eq:end_of_horizon} is only needed in case the paths diverge until the end of the horizon.
Note that $ S_{it'}^{\NULL} \neq \tilde{S}_{it' | t} $ implies that $\tilde{S}_{it' | t}^{\NULL} = 1$ and $S_{it'} = 0$ by \cref{prop:coupling_property}.
Once the two sample paths converge to the same state, they will never diverge again (since the policy is the same and the randomness is coupled).
Therefore, the advantage function is equal to the expected value of $Y_{it}$; i.e. $y_{it} = \bE[Y_{it}]$.

\begin{proposition} \label{prop:alg_minus_base}
Let $\ALG$ be any policy, and let $A_t$ be the set of patients $\ALG$ chooses at time $t$.
Then,
\begin{align*}
\ALG - \BASE = \bE\left[\sumT \sum_{i \in A_t} Y_{it} \right].
\end{align*}
\end{proposition}


\begin{myproof}[Proof of \cref{prop:alg_minus_base}.]
\cref{prop:coupling_property} says that it will never be the case that $S_{it}^{\BASE} = 1$, while $S_{it}^{\ALG} = 0$.
Therefore, defining $F_{it} = \{S_{it}^{\ALG} = 1, S_{it}^{\BASE} = 0\}$, we have
\begin{align*}
\ALG - \BASE = \bE\left[\sumT \sum_{i \in [N]} \bI(F_{it}) \right].
\end{align*}
Due to the sample path coupling, $S_{it}^{\ALG} \neq S_{it}^{\BASE}$ can only occur if
% the state of a patient can only be different between $\ALG$ and $\BASE$ if 
$\ALG$ chose patient $i$ at a prior time step, and the states have been different since then (if the states converged, it will stay the same unless $\ALG$ chose the patient again).

Let $t_1, \dots, t_n$ be the time steps where $\ALG$ chooses patient $i$ when patient $i$ was in state 0.
When $\ALG$ chooses patient $i$ at time $t_k$, the state under $\ALG$ follows the `counterfactual' state until the next time $i$ is chosen in state 0.
That is, for $r \in [t_k+1, \dots, t_{k+1}]$, $S_{ir}^{\ALG} = \tS_{ir|t}$.

Note that since $\ALG$ can only choose a patient when it is in state 0, it must be that 
$S_{i t_k}^{\ALG} = 0$ for all $k$.
Therefore, $S_{i t_{k+1}}^{\ALG} = \tS_{it_{k+1}|t_k} = 0 = S_{it_{k+1}}^{\BASE}$, where the last equality follows from \cref{prop:coupling_property}.
Hence, $t_{k+1} \geq t + Y_{it_k}$ by definition of $Y_{it_k}$.
In between $t_k$ and $t_{k+1}$, the number of times $F_{ir}$ occurs is exactly $Y_{it}$.
Therefore,
\begin{align}
\ALG - \BASE 
% = \bE\left[\sumT \sum_{i \in [N]} \bI(F_{it}) \right]
= \bE\left[\sumT \sum_{i \in A_t} Y_{it} \right]
\end{align}
as desired.
\end{myproof}



\textbf{Theorem statement.}
Let $\GT$ be the policy that chooses the patients with the highest $y_i$ values, out of all patients in state 0, at each time step.
Let $A_t^{\ALG}$ be the set of patients that $\ALG$ chooses at time $t$.
\begin{theorem}  \label{thm:2_approx_ys}
% For any $\ALG$, 
\begin{align*}
\OPT - \BASE \leq 2 \bE\left[ \sumT \sum_{i \in A_t^{\GT}} Y_{it} \right]
= 2(\GT - \BASE)
\end{align*}
 % By letting $\ALG = \GT$, we get $\OPT - \BASE \leq 2 (\GT - \BASE)$.
\end{theorem}


% Let $\GT$ be the policy that chooses the patients with the highest $y_i$ values, out of all patients in state 0, at each time step.
We actually prove the following, stronger result.
For a subset $I \subseteq [N]$, let $A_t^G(I) \subseteq I$ be the set of $B$ patients with the highest $\yit$ values (i.e.\ these are the patients that $\GT$ would choose).
For any $\ALG$, let $I_t^{\ALG}$ be the patients in state 0 at time $t$.
\begin{theorem}  \label{thm:2_approx_ys_stronger}
For any $\ALG$, 
\begin{align*}
\OPT - \BASE \leq 2 \bE\left[ \sumT \sum_{i \in A_t^G(I_t^{\ALG})} y_{it} \right].
\end{align*}
 % By letting $\ALG = \GT$, we get $\OPT - \BASE \leq 2 (\GT - \BASE)$.
\end{theorem}

This theorem implies that we only need to \textit{approximately} choose the patients with the largest $y_i$ values.
% the $\GT$ policy.
Specifically, suppose, for some $\alpha \in [0, 1]$, a policy $\ALG$ satisfies
\begin{align} \label{eq:approx_result}
% \ALG - \BASE = 
\bE\left[ \sumT \sum_{i \in A_t} y_{it} \right] \geq \alpha 
\bE\left[ \sumT \sum_{i \in A^G(I_t^{\ALG})} y_{it} \right].
\end{align}
The LHS of \eqref{eq:approx_result} corresponds to $\ALG - \BASE$.
Therefore, combining with \cref{thm:2_approx_ys_stronger} results in
\begin{align}
\OPT-\BASE \leq \frac{2}{\alpha} (\ALG-\BASE).
\end{align}
For example, if a policy chooses, at each time step $t$, patients with values of $\yit$ that are half as large as the patients with the largest $\yit$, then it would be that $\alpha = 1/2$, and hence we would get a 4-approximation.



\subsection{Proof of \cref{thm:2_approx_ys_stronger}}
Let $I^{\ALG}_t$ and $I^*_t$  be the set of patients that are in state 0 under $\ALG$ and $\OPT$ respectively at time $t$.
Let $A^{\ALG}_t \subseteq I^{\ALG}_t$ and $A^*_t\ \subseteq I^*_t$ be the set of patients that $\ALG$ and $\OPT$ chooses respectively at time $t$.
Let $A^G(I_t^{\ALG}) \subseteq I^{\ALG}_t$ be patients with the highest $y_i$ values in $I^{\ALG}_t$.
We first decompose the rewards based on whether or not a patient that was chosen in $\OPT$ was available to be chosen under $\ALG$.
\begin{align}
\OPT - \BASE
&= \bE\left[ \sumT \sum_{i \in A^*_t} Y_{it} \right]\nonumber \\
&= \bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} Y_{it} \right]
+ \bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Y_{it} \right]
% &= \bE_\omega\left[ \sumT \sum_{i \in I^*_t(\omega) \cap I_t(\omega)} \bI(i \in A_t)  Y_{it}  \right] + \bE_\omega\left[ \sumT  \sum_{i \in I^*_t(\omega) \setminus I_t(\omega)} \bI(i \in A_t)  Y_{it}  \right]
\label{eq:opt_minus_base_decomp}
\end{align}

% Recall that by definition, $A^G(I_t^{\ALG})$ are the patients in $I_t^{\ALG}$ with the highest $y_i$ values.
% since the greedy policy by definition chooses patients with the highest $y_i$ values for any $t$,
For the first term in \eqref{eq:opt_minus_base_decomp},
by definition of $A^G(I_t^{\ALG})$, we have
$\sum_{i \in A^*_t \cap I^{\ALG}_t} y_{it} \leq \sum_{i \in A^G(I_t^{\ALG})} y_{it}$.
Therefore, 
\begin{align*}
 \bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} y_{it} \right]
 \leq 
 \bE\left[ \sumT \sum_{i \in A^G(I_t^{\ALG})}  y_{it} \right]
\end{align*}

% clearly the total reward (wrt $Y_{it}$) at time $t$ is larger than the first term, by definition of greedy.
% Therefore, 
% $\bE_\omega\left[ \sumT \sum_{i \in I^*_t(\omega) \cap I_t(\omega)} Y_{it}  \right] \leq \GT(\ALG) - \BASE$.

\cref{thm:2_approx_ys} then follows from the following proposition which bounds the second term in \eqref{eq:opt_minus_base_decomp}. This term represents rewards from patients who are in state 0 under $\OPT$ but not under $\ALG$.
\begin{proposition} \label{prop:second_term}
\begin{align} \label{eq:second_term_claim}
\bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Y_{it} \right]	
\leq 
\bE\left[ \sumT \sum_{i \in A^{\ALG}_t} Y_{it} \right]	
\end{align}
\end{proposition}

\subsubsection{Proof of \cref{prop:second_term}}
The main idea of this result is that for every $Y_{it}$ term that contributes to the LHS of \eqref{eq:second_term_claim}, since that patient is not available under $\ALG$, we have already collected this reward under $\ALG$.
We will show a one-to-one mapping from every $Y_{it}$ term on the LHS to a $Y_{i \tau_i(t)}$ term on the RHS of \eqref{eq:second_term_claim}.

% Since such a patient is still in state 1 under $\ALG$, 
Let $\cI = \{(i, t) : i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1\}$ be the set of (patient, time) tuples in which patient $i$ is available in OPT but not ALG, and moreover, $K_{it} = 1$.
% Fix $i, t$ such that $i \in A^*_t(\omega) \setminus I_t(\omega)$, and $K_{it} = 1$ 
% (i.e. it contributes to the second term in \eqref{eq:opt_minus_base_decomp}).
Since $K_{it} = 1$ is a necessary condition for $Y_{it} > 0$, the LHS of \eqref{eq:second_term_claim} can be written as
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Y_{it} \right]	
= \bE\left[ \sum_{(i, t) \in \cI} Y_{it} \right]	
\end{align*}

Fix $(i, t) \in \cI$.
Since $i \notin I^{\ALG}_t$, $S_{it}^{\ALG} = 1$, and since $i \in A^*_{t}$, $S_{it}^{\OPT} = 0$.
Define $\tau_i(t) < t$ to be the last time that $i$ was in state 0 under $\ALG$:
\begin{align*}
\tau_i(t) = \max\{ t' < t : S_{it'}^{\ALG} = 0\}.
\end{align*}
That is, under $\ALG$, patient $i$ transitioned from state 0 to state 1 between time $\tau_i(t)$ and $\tau_i(t)+1$ and stayed in state 1 since.
% It must have been that $\ALG$ chose the patient at time $\tau_i(t)$; i.e. $A_{it}$
% It must be that $S_{is}^{\BASE} = 0$ for 
% It must be that during this time, under $\OPT$, the patient is in state 0; i.e. for all $s = \tau_i(t)+1, \dots, t$, $S_{is}^{\OPT} = 0$.
We can show that $\tau_i(t)$ satisfies the following property: it must be that $i$ was chosen at time $\tau_i(t)$ under $\ALG$, and moreover, $Y_{i \tau_i(t)}$ is at least $t - \tau_i(t)$.

\begin{claim} \label{claim:properties_of_y}
Let $(i, t) \in \cI$.
Then, $i \in A^{\ALG}_{\tau_i(t)}$, and $Y_{i \tau_i(t)} \geq t - \tau_i(t)$.
\end{claim}	
\begin{myproof}[Proof of \cref{claim:properties_of_y}]
Let $(i, t) \in \cI$.
By definition of $\tau_i(t)$, $S_{i, \tau_i(t)}^{\ALG} = 0$ and $S_{i, \tau_i(t)+1}^{\ALG} = 1$.
To the contrary, suppose $i \notin A_{\tau_i(t)}^{\ALG}$.
Then, $i$ transitioned to state 1 without being chosen, and therefore $P_{i\tau_i(t)} = 1$.
Then, it must be that $i$ transitions to state 1 under $\OPT$ as well; $S_{i, \tau_i(t)+1}^{\OPT} = 1$.
But since $S_{it}^{\OPT} = 0$, $i$ switches back to state 0 before state $t$, which means that this should also happen under $\ALG$ (due to the sample path coupling).
This is a contradiction to the definition of $\tau_i(t)$.
Therefore, $i \in A_{\tau_i(t)}^{\ALG}$.

Moreover, under the same reasoning, it must be that $K_{i\tau_i(t)} = 1$, and that $S_{is}^{\BASE} = 0$ for all $s \in \{\tau_i(t)+1, \dots, t\}$.
Therefore, $Y_{i \tau_i(t)} \geq t - \tau_i(t)$.
\end{myproof}

% Since $S_{it}^{\OPT} = 0$, it must be that under $\ALG$, there was a previous time step $\tau_i(t) < t$ where $\ALG$ chose $i$ and hence it moved to state 1, and has stayed there since then.
% (It could not have been that $i$ moved to state 1 at time $\tau_i(t)$ \textit{without} being chosen by $\ALG$, since if that were the case, it would have also moved to state 1 under $\OPT$.)
% Specifically, $\tau_i(t)$ can be written as 
% \begin{align*}
% \tau_i(t) = \max\{ t' < t : A_{it'}^{\ALG} = 1\}.
% \end{align*}
% Then, for all $(i, t) \in \cI$, it must be that $Y_{i \tau_i(t)} \geq t - \tau_i(t)$.

We next show that the mapping $\tau_i$ is one-to-one.
\begin{claim} \label{claim:1-1}
If $(i, t), (i, s) \in \cI$ for $t \neq s$, then $\tau_i(t) \neq \tau_i(s)$.
\end{claim}
\begin{myproof}[Proof of \cref{claim:1-1}]
Suppose $t < s$ such that $(i, t), (i, s) \in \cI$.
That is, under $\OPT$, patient $i$ was chosen both at time $t$ and $s$, and the patient was already in state 1 under $\ALG$ at both of these times.
Suppose, for contradiction, $\tau_i(t) = \tau_i(s)$.
This means that under $\ALG$, patient $i$ was chosen at time $\tau_i(t)$, and the patient has stayed in state 1 from time $\tau_i(t)+1$ until at least time $s$.
% This implies that for all $t' \in \{\tau_i(t)+1, \dots, s-1\}$

$(i, t), (i, s) \in \cI$ implies $S^{\OPT}_{it} = S^{\OPT}_{is} = 0$.
% that patient $i$ was in state 0 at both of these times.
Since $K_{it} = 1$, the patient transitioned to state 1 at time $t+1$.
Since $S^{\OPT}_{is} = 0$, it must be that there exists a $t' \in \{t+1, \dots, s-1\}$ such that $Q_{it'} = 1, P_{it'} = 0$.
But since $S^{\ALG}_{it'} = 1$, the fact that $Q_{it'} = 1$ and $P_{it'} = 0$ implies that $S^{\ALG}_{i, t'+1} = 0$, which is a contradiction.
\end{myproof}

Claims \ref{claim:properties_of_y} and \ref{claim:1-1} show that every $(i, t) \in \cI$ maps to one term in the RHS of \eqref{eq:second_term_claim}.
Therefore, the right hand side of \eqref{eq:second_term_claim} is at least
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^{\ALG}_t} Y_{it} \right]	
% \bE_\omega\left[ \sumT \sum_{i \in A_t(\omega)} Y_{it} \right]
\geq 
\bE\left[ \sum_{(i, t) \in \cI} Y_{i\tau_i(t)} \right].
\end{align*}
Then, our goal is to show
\begin{align} \label{eq:simpler_goal}
\bE\left[ \sum_{(i, t) \in \cI} Y_{it} \right]
\leq
\bE\left[ \sum_{(i, t) \in \cI} Y_{i\tau_i(t)} \right].
\end{align}

First consider the LHS of \cref{eq:simpler_goal}.
Note that if $(i, t) \in \cI$, then $t \geq 2$ (since $S_{i1} = 0$ for all $i$).
Therefore,
\begin{align}
\bE\left[ \sum_{(i, t) \in \cI} Y_{it} \right]
&= \sum_{t=2}^T \sumN \bE[ \bI(i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1) Y_{it}] \\
&= \sum_{t=2}^T \sumN \bE[ Y_{it} | i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1] \Pr (i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1) \\
&= \sum_{t=2}^T \sumN \bE[ Y_{it} | K_{it} = 1] \Pr (i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1),
\end{align}
where the last equality follows since $Y_{it}$ is independent of the history of actions up until (and including) time $t$.
Next, the RHS of \cref{eq:simpler_goal} can be written similarly:
\begin{align}
\bE\left[ \sum_{(i, t) \in \cI} Y_{i\tau_i(t)} \right]
&= \sum_{t=2}^T \sumN \bE[  Y_{i\tau_i(t)}  | i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1] \Pr (i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1) 
\end{align}
The following claim implies \cref{eq:simpler_goal}, and finishes the proof of \cref{prop:second_term}.
\begin{claim} \label{claim:compare_ys}
For every $i \in [N]$ and $t \geq 2$,
\begin{align} \label{eq:inner_term}
 \bE[ Y_{it} | K_{it} = 1] \leq \bE[  Y_{i\tau_i(t)}  | i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1].
\end{align}
\end{claim}


\begin{myproof}[Proof of \cref{claim:compare_ys}.]
Fix $i, t \geq 2$. 
Let $Y_i$ be a random variable that has the same distribution as $Y_{it}$ for all $t$.
First, we upper bound the LHS of \cref{eq:inner_term}.
\begin{align}
 \bE[ Y_{it} | K_{it} = 1]
 &= \Pr(Y_{it} \geq 1 | K_{it} = 1) \bE[Y_{it} |  Y_{it} \geq 1, K_{it} = 1] \nonumber \\
 &\leq \bE[Y_{it} |  Y_{it} \geq 1, K_{it} = 1] \nonumber \\
 &= \bE[Y_{i} |  Y_{i} \geq 1]. \label{eq:lhs_ub}
\end{align}
where the last equality follows since $Y_{it} \geq 1$ implies $K_{it} = 1$.

Next, consider the RHS of \eqref{eq:inner_term}.
 % $\bE[ Y_{i\tau_i(t)}  | i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1]$.
Since $\tau_i(t) < t$, $Y_{i\tau_i(t)}$ is independent of $K_{it}$.
From \cref{claim:properties_of_y}, $Y_{i \tau_i(t)} \geq t - \tau_i(t)$ and $i \in A^{\ALG}_{\tau_i(t)}$.
These two properties imply $i \notin I^{\ALG}_t$.
Therefore, 
\begin{align*}
\bE[ Y_{i\tau_i(t)}  | i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1]
 = \bE[ Y_{i\tau_i(t)}  | i \in A^*_t, i \in A^{\ALG}_{\tau_i(t)}, Y_{i \tau_i(t)} \geq t - \tau_i(t)].
\end{align*}
Moreover, conditioned on $Y_{i \tau_i(t)} \geq t - \tau_i(t)$, both of the events $\{i \in A^*_t\}$ and $\{i \in A^{\ALG}_{\tau_i(t)}\}$ do not change the conditional distribution of $Y_{i\tau_i(t)}$.
% , conditioned on $Y_{i \tau_i(t)} \geq t - \tau_i(t)$.
Therefore,
\begin{align}
\bE[ Y_{i\tau_i(t)}  | i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1]
 &= \bE[ Y_{i\tau_i(t)}  | Y_{i \tau_i(t)} \geq t - \tau_i(t)] \nonumber \\
 &= \sum_{t' < t} \Pr(\tau_i(t) = t') \bE[ Y_{i t'}  | Y_{i t'} \geq t - t']  \nonumber  \\
 &\geq \sum_{t' < t} \Pr(\tau_i(t) = t') \bE[ Y_{i t'}  | Y_{i t'} \geq 1] \nonumber
 % &\geq \bE[ Y_{it}  | Y_{it} \geq 1], \label{eq:rhs_lb}
\end{align}
% where the inequality follows from the fact that $t - t' \geq 1$, and lemma TODO.
Note that for $s < t$, $\bE[Y_{is} | Y_{is} \geq 1] \geq \bE[Y_{it} | Y_{it} \geq 1]$.
This is because the distribution of $Y_{is}$ and $Y_{it}$ conditioned on them being greater than 1 is almost the same, except that $Y_{it}$ has a smaller maximum value of $T-t$.
Therefore,
\begin{align}
 \sum_{t' < t} \Pr(\tau_i(t) = t') \bE[ Y_{i t'}  | Y_{i t'} \geq 1] \geq \bE[ Y_{it}  | Y_{it} \geq 1]. \label{eq:rhs_lb} 
\end{align}

Combining \eqref{eq:rhs_lb} and \eqref{eq:lhs_ub} proves the result.
\end{myproof}

\jnote{Clarify that $Y_{it}$ is defined for all $t$, as it is defined with respect to the $\NULL$ and counterfactual sample paths, both which do not depend on the algorithm. If $S_{it}^{\NULL} =1$, then $Y_{it} = 0$.}

