%!TEX root=../Draft1.tex

%\textbf{P1: Proliferation of technological services aimed at improving behavioral health}\\
For most health conditions, long-term outcomes are determined not only by clinical interventions but also by individual habits and behaviors \citep{bosworth2011medication}. A range of behavioral health interventions, often delivered through digital platforms, have been shown to improve outcomes for various health conditions. These interventions aspire to affect users' habits through motivation, education, nudging, or boosting \citep{Ruggeri20Behavioral}. On the other hand, interventions are associated with a variety of direct and indirect costs. As such, this paper is concerned with optimizing the impact of costly interventions. 

As a concrete example of a digital health intervention, consider our partner organization, \textit{Keheala}. Keheala operates a digital service promoting medication adherence for patients prescribed with tuberculosis (TB) treatment in Kenya, and has already been shown to have a non-trivial impact on adherence and health outcomes \citep{Yoeli19Digital,Boutilier22Improving}. Since a lack of adherence to the prescribed course of treatment for TB can often result in a relapse of the disease, and worse yet, drug resistance, such adherence is incredibly important. Keheala's digital platform comprises a suite of interventions to motivate adherence. Some are on-demand (such as leaderboards and access to information about TB), some are automated (such as daily reminders to motivate treatment adherence), and some are manual (such as outreach messages and phone calls). Keheala employs `support sponsors' to operate the platform and reach out to patients. Another key feature of the service is that patients themselves are required to self-verify treatment adherence using the digital platform every day. The resulting feedback loop allows Keheala to discern the impact of their interventions. As important, it allows the platform to identify patients most in need of an intervention; to that end, support sponsors attempt to call patients who have not verified treatment adherence for two or more consecutive days.

%As a concrete example of a digital health intervention, consider our partner organization, \textit{Keheala}. Keheala operates a digital service promoting medication adherence for patients prescribed with tuberculosis (TB) treatment in Kenya. Since a lack of adherence to the prescribed course of treatment for TB can often result in a relapse of the disease, and worse yet, drug resistance, such adherence is incredibly important. To that end, Keheala's digital platform comprises a suite of automated interventions (e.g., reminder notifications sent to a mobile device) as well as manual interventions wherein `support sponsors' employed by Keheala make phone calls to patients to support treatment adherence. Keheala's intervention has already been shown to have non-trivial impact on adherence and health outcomes \citep{Yoeli19Digital,Boutilier22Improving}. A key feature of the intervention is that patients themselves are required to self-verify treatment adherence using the digital platform every day. The resulting feedback loop allows Keheala to discern the impact of their interventions. As important, it allows the platform to identify patients most in need of an intervention; to that end, support sponsors attempt to call patients who have not verified treatment adherence for two or more consecutive days. 

The Keheala example allows us to identify two salient attributes of intervention programs in the context of digital health:
\begin{itemize}
\item Interventions are costly: In the case of Keheala, calls from support sponsors entail a direct cost. On the other hand, even the automated mobile reminders entail an indirect cost, in that frequent reminders are likely to sensitize patients to the automated messages. 
\item Interventions are rationed: In the case of Keheala, as with most outreach systems, demand for outreach exceeds supply. On an average day in our data, Keheala was supporting 608 patients with 3 support sponsors. The support sponsors had a range of responsibilities but their effective capacity to make phone calls to patients was exceeded by the number of patients eligible for such phone calls by a factor of 8.
%\item Interventions are rationed: In the case of Keheala, on any given day, the number of patients who have not verified treatment adherence on the prior two days (and who, appropriately or not, are deemed eligible for a support sponsor call), is more than 8 times larger than the available support sponsor capacity. 
\end{itemize}
This state of affairs prompts the key question we seek to answer: {\em How does one design a practical intervention policy that maximizes the impact of costly interventions?} 

The notion of `practical' above can mean many things, but at a minimum, we want a policy that is {\em safe}: the policy does not degrade operating performance relative to the status-quo; a policy that can make progress with {\em limited data}: programs like Keheala generate, relative to the ambient complexity of the problem, relatively little data especially in their early stages. And finally, a practical policy must be at least somewhat {\em transparent} in its operation; an entirely blackbox algorithm is unlikely to get traction with operators. 

%% (support sponsors, employed by Keheala, make phone calls to patients to support treatment adherence). 
%(Keheala employs `support sponsors' to make phone calls to patients to support treatment adherence). 
%% A unique feature of their service is that it requires patients to self-verify daily treatment adherence using the digital platform.
%Each day, every patient is required to self-verify treatment adherence using the digital platform; this allows Keheala to potentially use this real-time adherence information to decide who should receive an intervention.

%A salient criticism of those types of interventions is that they lack precision and are not personalized \citep{Mills20Personalized}. 
%
%Implementing personalization in digital behavioral health systems is non-trivial. A case in point is our partner organization, \textit{Keheala}, that operates a digital service promoting medication adherence for patients prescribed with TB treatment in Kenya. 
%Their digital platform comprises a suite of automatic interventions (reminder notifications and access to general TB information) as well as manual interventions 
%% (support sponsors, employed by Keheala, make phone calls to patients to support treatment adherence). 
%(Keheala employs `support sponsors' to make phone calls to patients to support treatment adherence). 
%% A unique feature of their service is that it requires patients to self-verify daily treatment adherence using the digital platform.
%Each day, every patient is required to self-verify treatment adherence using the digital platform; this allows Keheala to potentially use this real-time adherence information to decide who should receive an intervention.
%
%The case of Keheala is representative of many providers of digital health interventions. First, some of the interventions offered are expensive (in this case the outreach by support sponsors) and must be judiciously allocated to the right patients at the right time. Second, 
%% while the the average effectiveness of the service has been established through an RCT \citep{Yoeli19Digital}, 
%the outreach policy used was a one-size-fits-all heuristic by which support sponsors should call patients who had not verified treatment adherence for two consecutive days. Third, their current challenge is therefore to use the field data collected using the heuristic to develop a more targeted and personalized outreach policy, in order to achieve higher efficiency in a future scale-up of the system to a larger population. 

\subsection{What can Reinforcement Learning Offer?}

Reinforcement learning (RL) represents one natural approach to personalizing interventions. A desirable feature of RL algorithms is their {\em model-free} nature and the limited assumptions required of user behavior. As a prototype of the sorts of RL approaches that one might apply to intervention optimization, consider formulating the task at hand as a so-called \textit{contextual bandit}. Specifically, the context of a given patient is their adherence and intervention history so far, and the `reward' corresponds to whether or not they self-verify treatment adherence in the following day. This reward is assumed to be some unknown, noisy function of context and the chosen action (i.e., whether or not the patient received an intervention). If the contexts observed for each patient were drawn independently each day from some fixed distribution, and our goal were to pick a myopically optimal set of users to call, then the problem at hand is {\em precisely} a contextual bandit. Of course, this is hardly the case: for one, a patient's context on a given day impacts their context on the following day by construction\footnote{Despite this gap, one can continue to show roughly that a bandit algorithm effectively learns the myopic policy under certain further conditions.}, and further, a myopically optimal choice may be sub-optimal, but let us ignore these limitations for now.  

Figure \ref{fig:introfig} compares the performance of a common contextual bandit policy, Thompson Sampling, with that of the existing heuristic employed by Keheala using a simulation model fit to data from Keheala's initial pilot\footnote{The model itself was trained by cross-validation with double ML and further validated on a hold out set; see \cref{s.casestudy} for details on this as well as the bandit approach itself.}. In parsing this figure, we note that the x-axis corresponds to the number of calls (and thus, also the amount of data that can be collected about the incremental impact of an intervention) in a single day. 
The operating point for the Keheala pilot corresponds to a budget of 26 interventions per day, as indicated by the yellow star.
Two observations are evident from the figure:
\begin{itemize}
\item Performance can deteriorate: At a low budget, we see that bandit performance actually deteriorates below the baseline. As we further discuss in \cref{ss.results}, this is a data scarcity issue: with a small budget, the bandit simply does not collect enough `exploration' data to learn an improved policy. This is despite the fact that the Thompson sampling algorithm employed had the benefit of a prior learned on approximately half of the data. 
\item Marginal improvement: The bandit captures a similar verification rate as the incumbent Keheala policy with 3 fewer interventions per day (relative to the 26 used by Keheala). While meaningful, this is a somewhat marginal improvement.
\end{itemize}

As it turns out, the myopic policy (which is in essence what the bandit learns) is far from optimal (which explains the somewhat marginal improvement observed). We could certainly turn to a reinforcement learning algorithm that attempts to learn an optimal policy by measuring the long run impact of an action, but then the data requirements of such a policy would increase substantively so that the performance degradation we see for the bandit at low budgets is likely to persist at (much) higher budgets. In summary, this leaves us in a peculiar spot -- whereas in principle RL algorithms could learn an optimal policy, we typically do not have the data budget, or the willingness to risk policy degradation due to exploration, to use such approaches. On the flip side, simpler RL algorithms (such as the contextual bandit) provide a relatively marginal improvement. 


 
% However, traditional RL algorithms also exhibit undesirable properties that render them inappropriate for many behavioral health settings. First, the ability to `experiment' or `explore', which is essential to RL, is often limited in healthcare settings, e.g., due to limited time horizons, limited data availability, as well as ethical considerations. 
%Second, though RL algorithms eventually converge to an optimal policy, its performance is often worse than a status quo algorithm in the short run, and there are no guarantees on how long this deterioration can last.
%This is especially a concern in our setting where platform operators are naturally risk adverse.
%% Second, platform operators are naturally risk averse and worry about the risk that an RL algorithm materially deteriorates performance over the short run, even if it `eventually' learns an optimal policy. 
%Finally, from a statistical perspective, the sample complexity of learning optimal policies can be large in settings where user behavior is inherently high dimensional. 
%
%% Furthermore, it is unclear if standard RL methods work. 
%Given these above concerns, it is unclear if standard RL methods will work in practice.
%As an example, a simple approach for personalizing support sponsor outreach, at Keheala, would be to deploy a multi-armed bandit algorithm that learns the response function of each individual, as measured by whether they self-verify treatment adherence following outreach from a support sponsor. Figure \ref{fig:introfig} compares the performance of a common bandit policy, Thompson Sampling, with the existing heuristic employed by Keheala using an out-of-sample validated simulation model of the setting (for full details on the bandit approach and the simulation model, see \S X). A key observation is that this type of personalization does not improve performance in any meaningful way. The key question then becomes: Can a better method be developed or does outreach personalization simply not add value in this setting?

\begin{figure}[h]
	\begin{center}
		\vspace{-2mm}
		\includegraphics[width=1\linewidth]{figs/results_Feb5_nodpi}
		\vspace{-6mm}
		\caption{Average overall verification rate over 50 runs for each policy and budget. The shaded region indicates a 95\% confidence interval. The star represents the operating point for Keheala.}
		\label{fig:introfig}
		\vspace{-6mm}
	\end{center}
\end{figure}

\subsection{This Paper}

This paper proposes a new RL algorithm for intervention optimization problems we dub \textit{Decomposed Policy Iteration $(\DPI)$}. In a loose sense, $\DPI$ can be thought of as approximating a single step of policy iteration in the (high dimensional) Markov Decision Process corresponding to our intervention optimization problem. $\DPI$ is model free. It takes as input a dataset corresponding to historical interventions and the corresponding outcomes under some incumbent policy and outputs what is effectively a patient-state dependent index. Using $\DPI$ with the capacity to make at most $B$ interventions in a given period, one simply employs the intervention on the patients with the $B$ highest indices. $\DPI$ enjoys a number of salient features which make it a promising practical solution to intervention optimization problems.

\begin{itemize}
\item \textbf{Theoretically Guaranteed Improvement:} In general, a single step of policy iteration does not necessarily guarantee a meaningful improvement. On the other hand, patient dynamics with respect to habitual behavior are not arbitrary. As such, we study the performance of our algorithm when patient behavior is driven by a simple model\footnote{We emphasize that $\DPI$ is a general model-free policy that can be used irrespective of the underlying patient behavior model. The behavioral model is used only for the theoretical analysis.} akin to models considered both in the behavioral health \citep{mate2020collapsing,mate2022field,biswas2021learn} and marketing literature \citep{schmittlein1987counting}. We show that when patient behavior is driven by such models, our algorithm recovers at least $1/2$ of the {\em improvement} between the optimal policy and a null policy that does not allocate interventions.
% the incumbent policy that yielded the historical data. 
Notice that this guarantee is much stronger than a typical uniform performance guarantee, in that it guarantees an improvement, and moreover it guarantees that the improvement is large relative to what is possible. 
Since $\DPI$ must be learned from data we prove that our guarantee is robust, and scales gracefully with respect to estimation errors. 
\item\textbf{ A Practical Policy:} $\DPI$ is practical. 
% First, it is safe -- it is essentially guaranteed to improve from a baseline policy. 
First, it is safe -- the nature of the policy iteration mechanism, in general, yields an improvement from the incumbent policy.
Second, it is data efficient -- the index computed by $\DPI$ is such that the amount of data required under the incumbent policy is itself minimal.
Third, it does not require any online learning or exploration after deployment.
The lack of exploration circumvents ethical concerns that can be an issue in using RL in healthcare applications. 
Finally, $\DPI$ is transparent and explainable by virtue of the fact that the relationship of a patients priority index to features of the patient state can be explained transparently. 
\item \textbf{Empirical Performance:} Returning to the setup identified in Figure~\ref{fig:introfig} the use of $\DPI$ would yield the same verification rate as the incumbent Keheala policy {\em with less than half the capacity}. Furthermore, $\DPI$ outperforms the contextual bandit algorithm (described above) for budgets ranging from 50\% to 150\% of the original capacity. Importantly, the performance gains are highest in low capacity scenarios---the most likely paradigm for a future large-scale roll-out of the system in resource-limited settings. 
\end{itemize}

The remainder of this paper is organized as follows. Following a literature review (\cref{s.litreview}), we introduce a general model of our behavioral health support setting and introduce our policy in \cref{s.model}. We then provide theoretical performance guarantees for our policy in \cref{s.theory}, establish its practical relevance through a numerical study based on a simulation model validated by field data in \cref{s.casestudy}, before concluding in \cref{s.conclusion}.


%possible worry of such a simple policy is whether it attains good performance, especially given that there are no strong guarantees on one iteration of policy iteration.
%% A possible worry of such a simple policy is whether it attains good performance --- this is a completely valid concern as there are no strong guarantees on one iteration of policy iteration. 
%To this end, we prove a $1/2$ approximation guarantee for $\DPI$ with respect to the \textit{performance improvement} from the base policy, for a special class of MDP models.
%Specifically, we consider a class of models where each patient can be in one of two states, a desired state and an undesired state.
%For example, the desired state can represent the patient adhering to medication, and not adhering in the undesired state.
%This two-state MDP captures the habitual nature of patient behaviors, and very similar models have been used in the behavioral health literature \citep{mate2020collapsing,mate2022field,biswas2021learn}.
%We consider the reward being the number of time steps that the patient is in the desired state.
%We show that under this model, $\DPI$ achieves at least {half} of the performance improvement attainable by an optimal policy, compared to a base policy that does not assign any interventions.
%
%Moreover, we show that this performance guarantee is robust under estimation errors in intervention values. 
%That is, if a policy targets interventions to those with \textit{approximately} the highest intervention values, then this policy also admits an approximation guarantee.
%Specifically, the approximation factor of $1/2$ gets multiplied by the approximation factor for the intervention values.
%In the applications we are interested in, since key quantities used in the policy must be estimated from data, this robustness result assures that performance guarantees still hold under estimation errors.
%% Such a robustness result is valuable in data-driven settings, where key quantities used in the policy are estimated from data.
%We note that the two-state model is only used to derive the theory; $\DPI$ is a general policy that does not rely on this special model.


%\end{itemize}


%\textbf{Our Contributions.}
%
%Our main contribution is to propose a simple and practical policy for this problem, by leveraging fundamental ideas from RL, while mitigating its aforementioned limitations.
%We corroborate this policy through establishing a strong theoretical guarantee on its performance, and an empirical evaluation through a rigorous case study on Keheala.
%% In this paper, we seek to leverage the promise of RL for the task of optimizing personalized interventions. 
%% while attempting to mitigate its limitations. 
%% Our main contribution is to propose a simple and practical policy, which we corroborate through a strong theoretical guarantee on its performance and an empirical evaluation through a rigorous case study on Keheala.
%
%We model each patient as a general Markov decision process (MDP) with two actions, 
%where the action represents whether the patient received an intervention.
%The patients are linked through a budget constraint that limit the total number of interventions that can be allocated in each time step.
%Then, the goal is to develop a policy of assigning interventions satisfying this budget constraint that will maximize total reward across all patients.  
%
%We introduce a natural policy called \textit{Decomposed Policy Iteration $(\DPI)$}, which is a variant of the ubiquitous policy iteration (PI) algorithm.
%Given a starting policy, PI uses the value function with respect to this policy to derive a new policy that guarantees a performance improvement.
%$\DPI$ essentially performs one step of PI, while also decomposing the value function down to the patient level in order to remove dependence on the exponentially sized state and action space. 
%This decomposition turns $\DPI$ into a simple index policy. That is, at each time step, we compute an `intervention value' for each patient, defined as a difference of Q-functions between the patient receiving an intervention and not receiving an intervention.
%This intervention value represents the increase in total future reward from this patient induced by an intervention at this time step.
%$\DPI$ allocates interventions to those with the highest intervention values.
%
%A significant appeal of this policy comes from its ease of implementation. 
%To deploy $\DPI$, one simply needs to estimate the Q-functions of the patient MDPs with respect to a base policy.
%As we assume having access to data collected from a base policy, 
%estimating the Q-function corresponds to a \textit{prediction} problem, where one can employ standard supervised learning techniques.
%
%A possible worry of such a simple policy is whether it attains good performance, especially given that there are no strong guarantees on one iteration of policy iteration.
%% A possible worry of such a simple policy is whether it attains good performance --- this is a completely valid concern as there are no strong guarantees on one iteration of policy iteration. 
%To this end, we prove a $1/2$ approximation guarantee for $\DPI$ with respect to the \textit{performance improvement} from the base policy, for a special class of MDP models.
%Specifically, we consider a class of models where each patient can be in one of two states, a desired state and an undesired state.
%For example, the desired state can represent the patient adhering to medication, and not adhering in the undesired state.
%This two-state MDP captures the habitual nature of patient behaviors, and very similar models have been used in the behavioral health literature \citep{mate2020collapsing,mate2022field,biswas2021learn}.
%We consider the reward being the number of time steps that the patient is in the desired state.
%We show that under this model, $\DPI$ achieves at least {half} of the performance improvement attainable by an optimal policy, compared to a base policy that does not assign any interventions.
%
%Moreover, we show that this performance guarantee is robust under estimation errors in intervention values. 
%That is, if a policy targets interventions to those with \textit{approximately} the highest intervention values, then this policy also admits an approximation guarantee.
%Specifically, the approximation factor of $1/2$ gets multiplied by the approximation factor for the intervention values.
%In the applications we are interested in, since key quantities used in the policy must be estimated from data, this robustness result assures that performance guarantees still hold under estimation errors.
%% Such a robustness result is valuable in data-driven settings, where key quantities used in the policy are estimated from data.
%We note that the two-state model is only used to derive the theory; $\DPI$ is a general policy that does not rely on this special model.
%
%To evaluate the practical performance of our proposed policy, we apply it to the setting of Keheala, for improving medication adherence for patients on TB treatment.
%To do this, we first build a simulation model for patient behavior, and we run a careful validation study to ensure that it accurately mimics real patient behaviors.
%We then use this simulation model to evaluate and compare the performance of various policies, and we demonstrate that our approach results in ...{\color{red}[fill in simulation results--include a reference to our policy in Figure \ref{fig:introfig} as well as the logic for attaining the same performance with X\% less capacity.]}.
%
%\iffalse
%From a theoretical perspective, we first formulate a simple Markov Decision Process (MDP) model of patient behavior. This general model captures the main features of many behavioral health situations, namely that patients alternate between being in a desired state (e.g., adhering to medication, following dieting guidelines, or exercising) and relapsing to an undesired state in which they are not following the recommended behavior. A behavioral health intervention can enhance patients' motivation for shifting from the undesired state to the desired one. In accordance with most digital health settings, we observe the state of each patient over time and allow transition rates between states and intervention effects to be patient specific. 
%Using this model, we first define two benchmark policies: $\BASE$ denotes the null-policy of never reaching out to patients while $\OPT$ refers to an optimal policy that maximizes the number of days patients spend in the desired state. We then introduce a \textit{Decomposed Policy Iteration} approach that prioritizes patients with the highest \textit{intervention value}, defined as the difference in the long term reward as a function of whether a patient receives outreach on a given day, relative to the $\BASE$ policy (i.e., the difference in the value of the Q-function). This approach can be thought of as a one-step policy iteration, a main differentiator from standard policy iteration is that it decomposes to the patient level and thus removes the dependence on the exponentially sized action space. 
%We prove that, relative to $\BASE$, the $\DPI$ policy achieves at least half of the improvement attainable by the theoretical upper bound of $\OPT$, in terms of days spent in the desired state. This is a significantly stronger performance guarantee than the standard 2-approximation result (see discussion in \S XX) since it refers to the marginal impact over and above the base policy $\BASE$ of doing nothing, even in settings (such as ours) where patients may stay in the desired state for many periods in the absence of any intervention.
%
%From a more practical perspective, we move beyond the stylized MDP model of patient behavior and develop a modified version of the $\DPI$ policy which relaxes three key assumptions needed for the theoretical performance guarantee which might be violated in practice. 
%First, the patient-level transition rates of the MDP will not generally be known in practice and are difficult to estimate. 
%Second, patient behavior might not be static in practice, as many external factors affect behavior over time. For those two reasons, we extend the state space of each patient by incorporating  observable features that are predictive of adherence, to dynamically estimate the inputs of our policy directly from data in a model-free way. 
%Finally, while our theoretical performance guarantee assumes a one-step policy iteration over the $\BASE$ policy of doing nothing to arrive at our $\DPI$ policy, the field data we have access to is for the existing one-size-fits-all heuristic employed in the original RCT. As a result, we perform the one-step iteration over and above that heuristic, to arrive at an improved practical policy. 
%
%To evaluate the performance of this practical policy, we develop and validate a simulation model for patient behavior, based on the data collected in the RCT. We then conduct counterfactual simulations comparing various policies and demonstrate that our approach results in ...{\color{red}[fill in simulation results--include a reference to our policy in Figure \ref{fig:introfig} as well as the logic for attaining the same performance with X\% less capacity.]}.
%
%Our work contributes to both the academic literature on personalized behavioral health as well as the practical design of digital platforms aiming to support behavior change and improve health outcomes. From an academic perspective, multiple approaches have been suggested for similar problems (for a detailed literature review, see \S \ref{s.litreview}). One approach is to apply bandit algorithms, which aim to maximize short-term patient engagement but fail to incorporate the long-term impact of outreach. Another approach is to posit a simple Markov-Decision-Process (similar to our initial model, presented in \S \ref{s.model}) and try to learn its transition rate, but such approaches are susceptible to modeling error and non-stationarity. A third approach is to apply standard RL methods (e.g., Q learning). Such algorithms are not only hard to implement in a field setting but also suffer from the problem's sample complexity, requiring a long exploration period for each patient, which is not only undesirable in a sensitive healthcare setting but also not guaranteed to result in a good policy within the time horizon of a single patient's treatment regimen. In contrast, our approach uses existing data in a model free way to develop a policy with performance guarantees that can be learned through a one-step on-policy iteration and applied to a new cohort of patients without exploration. 
%From a practical standpoint, we demonstrate the effectiveness of our approach using a validated simulation model based on real data. The approach is general and easily implementable by any organization wishing to improve long-term engagement with their digital services, as long as it has access to an initial dataset (e.g., from a pilot implementation).
%\fi
%

%\textbf{P2: Personalization of those services; how and when to reach out.}\\
%While digital behavioral health platforms differ in the targeted behavior change or underlying clinical condition, they share the fundamental operational challenge of personalizing and targeting costly interventions. Most of them rely on a suite of interventions, from automated ones (e.g., reminders and push notifications) to manual ones (e.g., phone calls or messages from agents). Delivering both types of interventions is costly, from the platform's perspective---manual interventions are operationally costly since they require human resources and automated interventions can result in notification fatigue among users (even if delivering them is operationally cheap). As a result, behavioral health platforms face a need to optimize the timing and content of outreach to each patient, in order to ensure the right patient receives the right intervention at the right time. 

%\textbf{P3: The general setting prior to personalization analysis}\\
%Most digital behavioral health platforms are initially launched with heuristic rules of thumb guiding the timing of outreach to each user.
%During such pilot implementation, the platform then conducts regular data collection from users, aimed at identifying whether they are in a state of compliance with a desired behavior (e.g., medication adherence, exercise routine, correct diet) or not. Such data is usually used to establish an overall average effectiveness of the suite of interventions. Subsequently, the behavioral data collected during a pilot implementation---even if generated using a sub-optimal initial outreach heuristic---opens the door for data-driven optimization for the development of improved outreach policies. 
%In this paper, we address exactly that question: how can observational data---describing a heuristic baseline policy for intervention as well as patient responses---be leveraged to develop improved targeting of interventions? 
