%!TEX root=../Draft1.tex

Our analysis is motivated by our partner organization, Keheala, which operates a digital health platform to support medication adherence among TB patients in highly resource-constraint settings. In this section, we first summarize the state of the global TB epidemic and the Keheala behavioral intervention (\cref{ss.keheala}). We then describe our data sources and the validated simulation model we have developed to test outreach policies (\cref{ss.data} and \cref{ss.sim}). Next, we discuss how our policy, as well as some benchmark algorithms, can be implemented using Keheala's data (\cref{ss.policies}), before presenting our numerical results (\cref{ss.results}).

\subsection{The global TB epidemic and the Keheala intervention}\label{ss.keheala}
TB remains one of the deadliest communicable diseases in the world, causing 1.6 million deaths in 2021. This is remarkable in light of the fact that effective treatment has been available for over 80 years, with the current WHO guidelines recommending a 6 month regimen of antibiotics for drug-susceptible TB and a more intense regiment for drug-resistant strains \citep{World22Global}. A key limiting factor for curbing the epidemic is lack of patient adherence to these treatment regiments, which increases the probability of infection spreading, drug resistance, and poor health outcomes \citep{garfein2019synchronous}. 

Keheala was designed to provide treatment adherence support to TB patients in resource-limited settings. Their platform operates on the Unstructured Supplementary Service Data (USSD) mobile phone protocol, which importantly allows phones without smart capabilities to access the service. Once a patient has enrolled with Keheala, they are meant to self-verify treatment adherence every day, using their mobile phone. In addition, they have access to a range of services. Some are on-demand, for example educational material about TB or leaderboards for verification rates. Others are automatic, such as adherence reminders, which are sent to patients daily (at their pre-determined medication time) in the absence of verification. In addition, the Keheala protocol is to escalate outreach interventions when patients do not self-verify adherence. It states that after one day of non-adherence patients should receive a customized message to encourage resumed adherence and after two days of non-adherence patients should receive a phone call from a support sponsor. While these support sponsors are full-time employees, they are not healthcare professionals. They are members of the local community who have experience with TB treatment and are therefore familiar with the many contributing factors associated with low treatment adherence, such as side-effects, societal stigma against TB patients, and challenges with refilling prescriptions.

The overall effectiveness of Keheala’s combination of services was evaluated in a randomized controlled trial (RCT) in Nairobi, Kenya. The trial demonstrated that Keheala reduced unsuccessful TB treatment outcomes—a composite of loss to follow-up, treatment failure, and death—by roughly two-thirds, as compared to a control group that received the standard of care \citep{Yoeli19Digital}. Given this success, Keheala’s primary practical objective is to ensure that enrolled patients remain engaged with the platform through adherence verification.

In this paper, we focus on the final level in Keheala‘s escalation protocol---support sponsors making phone calls to patients. This part of the outreach was organized through populating a daily list of patients who had not verified treatment adherence for 48 hours. Support sponsors had many responsibilities in operating the platform, but would make phone calls to as many patients on the list as possible on a given day. Since hiring support sponsors is a costly aspect of operating the service, Keheala is interested in implementing a more personalized and targeted approach for prioritizing which patients should receive a phone call on a given day. Being able to maintain a similar performance with fewer support sponsors (or equivalently, serve more patients with the same number of support sponsors) is desirable for any future scale-up of the system.

%Keheala was designed to provide treatment adherence support to TB patients in resource-limited settings. Their platform operates on the Unstructured Supplementary Service Data (USSD) mobile phone protocol, which importantly allows phones without smart capabilities to access the service. Once a patient has enrolled with Keheala, they are meant to self-verify treatment adherence every day, using their mobile phone. In addition, they have access to a range of services. Some are on-demand, for example educational material about TB or leaderboards for verification rates. Others are automatic, such as adherence reminders, which are sent to patients daily (at their pre-determined medication time) in the absence of verification. Most importantly, some services are manual and require human resources. In particular, Keheala employs \textit{support sponsors} who make phone calls to patients if they have not verified treatment adherence for 48 hours. While the support sponsors are full-time employees, they are not healthcare professionals. They are members of the local community who have experience with TB treatment and are therefore familiar with the many contributing factors associated with low treatment adherence, such as side-effects, societal stigma against TB patients, and challenges with refilling prescriptions.
%
%The overall effectiveness of Keheala's combination of services was evaluated in a randomized controlled trial (RCT) in Nairobi, Kenya. The trial demonstrated that Keheala reduced unsuccessful TB treatment outcomes---a composite of loss to follow-up, treatment failure, and death---by roughly two-thirds, as compared to a control group that received the standard of care \citep{Yoeli19Digital}. Given this success, Keheala's primary practical objective is to ensure that enrolled patients remain engaged with the platform through adherence verification.
%
%% Notably, in the aforementioned trial (and a subsequent one---see discussion of our data sources, below), 
%Notably, sponsor outreach was organized through populating a daily list of patients who had not verified treatment adherence for 48 hours. Support sponsors would then attempt to reach as many patients on the list as possible on a given day. Since hiring support sponsors is a costly aspect of operating the service, Keheala is interested in implementing a more personalized and targeted approach for prioritizing which patients should receive outreach on a given day. Being able to maintain a similar performance with fewer support sponsors (or more patients) is desirable for any future scale-up of the system. 

%We apply our method to the setting of Keheala, who have developed a mobile phone service to motivate and increase medication adherence for patients on tuberculosis treatment. A RCT conducted in Nairobi, Kenya demonstrated that Keheala reduced unsuccessful TB treatment outcomes -- a composite of loss to follow-up (LTFU), treatment failure, and death -- by roughly two-thirds, as compared to a control group that received the standard of care. Given this success, Keheala's primary practical objective is to ensure that enrolled patients remain engaged with the platform through adherence verification.

%In this case study, we leverage the data generated from Keheala's RCT to evaluate the performance of our proposed method.
%% We derive the intervention targeting policy, $\DPI$, using historical data from a randomized controlled trial run by Keheala.
%To do this, we first build a \textit{simulation model} of patient behaviors, which allows us to evaluate the performance of different policies.
%We run a careful validation check to ensure that the simulation accurately mimics real patient behaviors.
%Then, we use the simulation model to run $\DPI$ as well as other benchmark policies.
%We find that $\DPI$ substantially improves upon Keheala's baseline policy as well as other benchmark policies, and this improvement is robust over a wide range of values of the intervention budget.


%Individuals could also initiate an asynchronous conversation with the study team via a chat feature to, e.g., request a phone call, ask questions about treatment, raise concerns, or obtain assistance communicating with clinic staff.

%Crucially, a coding error caused the Keheala intervention not to work as intended.  The error prevented individuals who had never verified from appearing on the contact queue, and thus from receiving messages or calls from the study team in the event that they did not verify.  Only after an individual verified for the first time did the Kehala intervention work for them as intended.  
%The error was corrected on September 3, 2019.


%\subsection{Data and Simulation Model}
\subsection{Data sources.}\label{ss.data}
Based on the success of the first RCT, the effectiveness of Keheala was further evaluated in a second RCT\footnote{The trial was approved by the institutional review board of Kenyatta National Hospital and the University of Nairobi. Trial participants or their parents or guardians provided written informed consent. The trial’s protocol and statistical analysis plan were registered in advance with ClinicalTrials.gov (\#NCT04119375).} during 2018-2020. The RCT was conducted in partnership with 902 health clinics distributed across each of Kenya's eight regions, representing a mix of rural and urban clinics. The study included four treatment arms and enrolled over 15,000 patients. We obtained data for 5,433 patients enrolled in the Keheala intervention arm (other arms aimed to independently test specific components of the Keheala intervention). 

As part of the RCT, the study team collected socio-demographic information from all patients. This information includes static covariates such as age, gender, language preferences, location, as well as limited clinical history (see \cref{s.app.list_features} for a full list). In addition, Keheala collected engagement data about each patient during their enrollment in the service. This includes whether a patient verified on a given day, how many reminders they received, and whether they were contacted by a support sponsor. 

After filtering out patients with missing information or not enough data, we conducted all our analysis on 3,671 patients. The average patient was enrolled on the platform for 118 days. On an average day, 608 patients were enrolled and 210 of those were eligible for a support sponsor call according to the protocol (i.e., having not verified treatment adherence for the preceding 48 hours). The support sponsors, employed by Keheala, had a range of responsibilities in operating the platform, including making outreach phone call so to the eligible patients. The average number of support sponsors making calls to patients on a given day was 3 and the average number of calls made was 25.5. Hence, in our analysis, we use a budget of $B$ = 26 as our main point of comparison.
	
%After filtering out patients with missing information or not enough data, we conducted all of our analysis on 3,671 patients. On average, 608 patients were enrolled at any given time and the average patient was enrolled for 118 days. 
%On average, 210 patients were eligible for a support sponsor call, having not verified treatment adherence for the preceding 48 hours. 
%% The average number of outreach calls that the support sponsors made per day was 30.1, hence we use the budget of $B=30$ as our main point of comparison. 
%The average number of outreach calls that the support sponsors made per day was 25.5, hence we use the budget of $B=26$ as our main point of comparison. 

\subsection{Simulation Model.}\label{ss.sim}
We build a simulation model that we use to estimate the counterfactual outcomes of different outreach approaches.
The simulator is effectively represented by a single function, $f(s, a) \in [0, 1]$, which denotes the probability that a patient in state $s$ with action $a$ verifies in the next time step.
This function is used to simulate one step transitions for every patient.
We first describe the state space of the patients, describe the exact simulation procedure, and then discuss how we learn $f$ from data and validate the simulator.

\subsubsection{Patient state space.}\label{ss.simstatespace}
For patient $i$, let $X_i \in \bR^{13}$ be their static covariates. 
Let $V_{it} \in \{0, 1\}$ denote whether patient $i$ verified at time $t$, and let $A_{it}\in \{0, 1\}$ denote whether the patient received the intervention at time $t$.
Let $H_{it} = (V_{i1}, A_{i1}, \dots, V_{i, t-1}, A_{i,t-1}, V_{it}) \in \bR^{2t-1}$ be the history of verifications and interventions up to time $t$.
We define a \textit{condensed} history $\tH_{it} \in \bR^{21}$ by summarizing the history $H_{it}$ into 21 features.
For this condensed history, we summarize the patient's recent and overall behavior.
For statistics such as the number of times the patient verified and the number of interventions they have received, we aggregate them over the past week, as well as in total.
We also include information on their verification and non-verification streaks, as well as how long they have been in the platform.
See \cref{s.app.list_features} for a full list of these features.
Then, we define the state of patient $i$ at time $t$ to be $S_{it} = (X_i, \tH_{it}) \in \bR^{34}$.

\subsubsection{Simulation procedure.} 
Given $f$ and an intervention policy $\pi$, we `mimic' the RCT by simulating patient behavior day by day. 
In total, we simulate $T=700$ time steps, where each $t \in [T]$ corresponds to one day between April 2018 to March 2020. We let $T_{s}(i)$ and $T_{e}(i)$ denote the starting and ending time steps that patient $i$ was enrolled in Keheala. Each patient $i$ is then introduced into the system at time $t=T_s(i)$, and removed at time $t=T_e(i)$. We use the observed data from the RCT for their first 7 days in the system to initialize their state. Then, in each time period $t$, we let $P_t$ be the set of patients that were active in the RCT for more than 7 days. We use some policy $\pi$ for the patients $P_t$ to determine who receives sponsor outreach. If a patient $i$ was in state $S_{it}$ at time $t$ and the policy $\pi$ chose action $A_{it}$, we let $V_{i, t+1}$ be 1 with probability $f(S_{it}, A_{it})$, and 0 otherwise (and randomness is independent across patients and time steps). 
Finally, we use $V_{i, t+1}$ to update their state for the next time step. 

\subsubsection{Estimating the $f$ function.} \label{s.learnf} 
% In estimating the parameters of our simulation functions, we take advantage of the fact that the historical data has \textit{no unobserved confounding}; that is, we have access to all factors that were used to make targeting decisions during the RCT. This allows us to use existing methods in evaluating heterogeneous treatment effects to estimate counterfactuals.
% \edit{JOJ: should we tone this down? It seems reasonable that support sponsors would start recognizing names on the list and choosing not to call (or to call) based on their private information about the patients.}
Using the state space $\cS$ as described above, we construct the function $f: \cS \times \{0, 1\} \to [0, 1]$ using data from the RCT. Specifically, we learn the two functions $f(s, 0)$ and $f(s, 1)$ separately. For $f(s, 0)$, we train a gradient boosting classifier on the dataset $\{(S_{it}, v_{i,t+1})\}_{i \in N, t \in [T] : A_{it} = 0}$, using $v_{it}$ as the outcome variable. For $f(s, 1)$, we write the function as $f(s, 1) = f(s, 0) + \tau(s)$ and we learn $\tau(s)$ using the double machine learning method of estimating heterogeneous treatment effects \citep{chernozhukov2018double}. See \cref{s.app.simulation} for details on implementing this method.

Importantly, we use a different set of patients to estimate the $f$ function (and to run our simulations) from the set of patients we use to train our policies. In particular, we randomly split all patients from the RCT into two groups, which we call \textit{train} and \textit{test}. We use the \textit{test} set to estimate the $f$ function that forms the basis for the simulation model. We keep the \textit{train} set of patients separate and use it to train policies (see \cref{ss.policies}, below). This ensures that the policies we evaluate are not learned off of the same dataset that was used for the simulations. 

% In Keheala's RCT, outreach was only given to patients who have not verified for at least two days in a row. Therefore, we restrict all policies we evaluate to only give an outreach to those who have not verified for two days in a row. Since there is no data on how outreach affects patients in other states,

\subsubsection{Simulation validation.}
We validate the performance of the simulator on a \textit{different} intervention policy than the simulator was trained on, by leveraging the fact that there was variability in the number of interventions given throughout the RCT.
In particular, the average number of interventions given during the first half of the RCT was around double of that of the latter half (45.9 vs. 21.4),
and this variation induces a change in the intervention assignment policy.
Then, dividing the data into halves produces two datasets that are generated using effectively different intervention policies --- that is, the distribution of the observed states of patients are different between the two datasets.

To validate the simulator, we use the method from \cref{s.learnf} to learn $f$ using the first dataset, and then validate its performance on the second dataset. 
Using this procedure, the AUCs on the second dataset for $f(s, 0)$ and $f(s, 1)$ were 0.918 and 0.745, respectively.
We also check the calibration of both of these functions, by grouping the samples into bins based on their predicted probability of verifying the next day, and checking whether the true verification rate was the same.
Figure~\ref{f.calibration} displays these calibration results.

These results demonstrate that the simulator has good performance in mimicking patient behavior. 
As expected, the AUC and the calibration is lower for $f(s, 1)$ compared to $f(s, 0)$; this is caused by a significantly fewer number samples with an intervention in the training data, as well as the increase in variance of doing off-policy estimation.
More specifically, the calibration was slightly off for samples with a high probability of verification (bins 0.7-0.9) for $f(s, 1)$; however, we note that the 0.7-0.9 bins only contain 11.3\% of all samples for $f(s, 1)$.

\begin{figure}[h]%
	\centering
	\subfloat[Calibration for $f(s, 0)$]{{\includegraphics[width=0.48\linewidth]{figs/bar_pred0} }}%
	% \qquad
	\subfloat[Calibration for $f(s, 1)$]{{\includegraphics[width=0.48\linewidth]{figs/bar_pred1} }}%
	\vspace{2mm}
	\caption{Calibration plots for $f(s, 0)$ and $f(s, 1)$ for simulation validation. 
		We group the samples based on the simulated probability of verification
		into bins with a 10\% range, which we label by the lower number. 
		For example, the 0.3 bin on the x-axis represents the samples whose probability of verification according to $f$ is in $[0.3, 0.4)$; hence we should expect the actual number of verifications of those samples to be around 0.35.}
	\label{f.calibration}%
	\vspace{-2mm}
\end{figure}

%We have access to static covariates for each patient, including age, sex, location (county), whether they are HIV positive, etc.

% Let $T\approx 700$ be the total number of time steps that the RCT was run for. For each patient $i$, denote by $\cT_i \subseteq [T]$ the time steps that patient $i$ was in the system.

\subsection{Outreach Policies and Experimental Design}\label{ss.policies}
Using the simulation model described above, we compare the performance of three main policies. For each policy, we vary the budget for outreach interventions per day between 10 and 40. As mentioned before, the average number of sponsor outreaches during a given day of the trial was 26. Importantly, we restrict all policies so that they can only provide an outreach to patients who have not verified for at least two days in a row. This is because that was what was done in the RCT, hence there is no data for how an outreach affects behaviors for those who do not meet this criterion. Therefore, we would not be able to accurately evaluate policies that do not follow this restriction.

We note that attaining improved performance with lower outreach capacity is particularly important for the resource-limited setting at hand as it speaks to the performance achievable during a future scale-up of the system, in which the ratio of patients to support sponsors is likely to be much higher. 

Below, we describe the three main policies implemented, starting with a description of how to implement our $\DPI$ policy, before describing the benchmark approaches. 

\subsubsection{$\DPI$ for Keheala.} 
The first step in operationalizing $\DPI$ is defining the state space for each patient. For this, we use the same condensed state space as described in \cref{ss.simstatespace}, i.e., we define the state of patient $i$ at time $t$ to be $S_{it} = (X_i, \tH_{it}) \in \bR^{34}$ (a full list of these features is included in \ref{s.app.list_features}). Importantly, we note that all of the features of this state space are observable to Keheala at any time $t$, once a patient has been enrolled on the platform for seven days. 

The second step is estimating the $\hz_{it}$ score for each patient at each time period, which is ultimately used to prioritize patients. 
As before, we let $T_{s}(i)$ and $T_{e}(i)$ be the starting and ending time steps that patient $i$ was enrolled in Keheala. Using this notation, we can represent the future verification rate for patient $i$ at time $t$ by $y_{it} = \frac{1}{T_{\text{e}}(i)-t} \sum_{r=t+1}^T v_{ir}$.
Then, the data from the RCT can be written in the form $\{(S_{it}, a_{it}, y_{it})\}_{i \in [N], t \in \{T_{s}(i), \dots, T_{e}(i)\}}$, and we can estimate the function $q_{it}^{\baseline}(s, a)$ using this data.
In our implementation, we use a linear function approximation for the verification rate, assuming the form $q_{it}^{\baseline}(s, a) = \langle \theta_a, s \rangle \cdot (T_{\text{e}}(i)-t)$, for each of the two actions $a \in \{0, 1\}$.
We estimate $\theta_a$ using least squares with an $\ell_2$ regularizer:
\begin{align*}
	\hat{\theta}_a &\in \argmin_{\theta \in \bR^{34}} \bigg( \sum_{i \in N} \sum_{t=T_{\text{s}}(i)}^{T_{\text{e}}(i)}  \bI(a_{it} = a)(y_{it} - \theta^\top S_{it})^2 + ||\theta||^2_2 \bigg).
\end{align*}

Finally, we compute a patient's estimate of their intervention value at time $t$ as
\begin{align*}
	\hz_{it} = \langle \htheta_1 - \htheta_0,S_{it} \rangle \cdot (T_{\text{e}}(i)-t), 
\end{align*}
and the resulting policy is to give the intervention to up to $B$ patients with the highest positive $\hz_{it}$ values.


%\paragraph{State space.}
%% Take a patient $i$ at time $t$. 
%For patient $i$, let $X_i \in \bR^{13}$ be their static covariates. 
%Let $V_{it} \in \{0, 1\}$ denote whether patient $i$ verified at time $t$, and let $A_{it}\in \{0, 1\}$ denote whether the patient received the intervention at time $t$.
%Let $H_{it} = (V_{i1}, A_{i1}, \dots, V_{i, t-1}, A_{i,t-1}, V_{it}) \in \bR^{2t-1}$ be the history of verifications and interventions up to time $t$.
%We define a \textit{condensed} history $\tH_{it} \in \bR^{21}$ by summarizing the history $H_{it}$ into 21 features.
%For this condensed history, we summarize the patient's recent and overall behavior.
%For statistics such as the number of times the patient verified and the number of interventions they have received, we aggregate them over the past week, as well as in total.
%We also include information on their verification and non-verification streaks, as well as how long they have been in the platform.
%See \S \ref{s.app.list_features} for a full list of these features.
%% Refer to the appendix for the full list of the features \jnote{TODO}.
%Then, we define the state of patient $i$ at time $t$ to be $S_{it} = (X_i, \tH_{it}) \in \bR^{34}$.
%% Let $N(S_{it})$ be the number of days that the patient still has left on treatment.

%\paragraph{Learning.}
%For each patient from the RCT, we can construct their state $S_{it}$ for each $t$.
%Let $T_{s}(i)$ and $T_{e}(i)$ be the starting and ending time steps that patient $i$ was enrolled in Keheala.
%Let $y_{it} = \frac{1}{T_{\text{e}}(i)-t} \sum_{r=t+1}^T v_{ir}$ be the future verification rate for patient $i$ at time $t$.
%Then, the data from the RCT can be written in the form $\{(S_{it}, a_{it}, y_{it})\}_{i \in [N], t \in \{T_{s}(i), \dots, T_{e}(i)\}}$, 
%and we estimate the function $q_{it}^{\BASELINE}(s, a)$ using this data.
%We use a linear function approximation for the verification rate, assuming the form $q_{it}^{\BASELINE}(s, a) = \langle \theta_a, s \rangle \cdot (T_{\text{e}}(i)-t)$, for each of the two actions $a \in \{0, 1\}$.
%We estimate $\theta_a$ using least squares with an $\ell_2$ regularizer:
%\begin{align*}
%	\hat{\theta}_a &\in \argmin_{\theta \in \bR^{34}} \bigg( \sum_{i \in N} \sum_{t=T_{\text{s}}(i)}^{T_{\text{e}}(i)}  \bI(a_{it} = a)(y_{it} - \theta^\top S_{it})^2 + ||\theta||^2_2 \bigg).
%\end{align*}
%
%\paragraph{Policy.}
%At time $t$, we compute a patient's estimate of their intervention value as
%\begin{align*}
%	\hz_{it} = \langle \htheta_1 - \htheta_0,S_{it} \rangle \cdot (T_{\text{e}}(i)-t).
%\end{align*}
%The resulting policy is to give the intervention to up to $B$ patients with the highest positive $\hz_{it}$ values.

% For the $\DPI$ policy, our goal is to learn $\ADV_{it}^{\pi}(0, 1) = Q_{it}^{\pi}(0, 1) - V_{it}^{\pi}(0)$, where $\pi$ is the policy run by the RCT.
% We modeled both terms as linear functions. 
% For $V_{it}^{\pi}(0)$, we used all samples in the training data and learned a linear model $\langle \theta_0, x \rangle$, where the outcome was ``total future verifications''.
% Similarly for $Q_{it}^{\pi}(0, 1)$, using all samples in the training data with $a = 1$, we learned a linear model $\langle \theta_1, x \rangle$, where the outcome was ``total future verifications''.

\subsubsection{Bandit.}
The bandit policy aims to choose patients with the highest increase in the probability of next-day verification. In terms of the 2-state model from \cref{ss.2statemodel}, the goal is to choose patients with the highest value of $\tau$.
We essentially use the same state space and linear model as was used for $\DPI$, except that the outcome variable is next-day verification, rather than total future verifications.
We first learn a prior using the offline data, and then we run a Thompson sampling style policy, which continually updates the policy with online data.

Specifically, we assume the linear form $v_{i,t+1} = \langle \beta_a, S_{it} \rangle$ for action $a \in \{0, 1\}$, with unknown parameters $\beta_0, \beta_1 \in \bR^{34}$.
The prior on $(\beta_0, \beta_1)$ is initialized as the output of a least-squares regression using the offline data, the same data that was used to train $\DPI$.
At each time step, $(\tilde{\beta_0}, \tilde{\beta_1})$ is sampled from the posterior. 
Then, the policy chooses the $B$ patients with the highest value of $\langle \tilde{\beta_1}, S_{it} \rangle - \langle \tilde{\beta_0}, S_{it} \rangle$.
After the outcome is observed at each time step, the posterior is updated accordingly.

We note that this policy makes use of strictly more data than $\DPI$, since $\DPI$ only uses the offline data. 
In the results, we confirm that this policy indeed learns myopic rewards correctly.
Therefore, this is a very strong benchmark policy for optimizing myopic rewards.

\subsubsection{Baseline.}
Our baseline policy approximates the heuristic followed by Keheala in the two RCTs that have been implemented. In both cases, the protocol was that patients were added to the support sponsor call queue after not verifying for 48 hours. As a result, the order of patients in the queue is effectively random, determined by a combination of their designated medication time (which prompts automated reminders to take the medicine and verify) and the timing of their self-verification. We approximate the resulting outreach policy by selecting $B$ patients out of all those who have not verified for 48 hours, at random. 

%Out of all patients who have not verified for at least two days in a row, select $B$ of them at random. This is approximately the policy that Keheala used in their RCT.

\subsection{Results}\label{ss.results}

\begin{figure}[h]
\begin{center}
\vspace{-3mm}
  \includegraphics[width=1\linewidth]{figs/results_Feb5}
  \caption{Average overall verification rate over 50 runs for each policy and budget. The shaded region indicates a 95\% confidence interval. The star represents the operating point for Keheala.}
  \label{fig:main_results}
\vspace{-6mm}
\end{center}
\end{figure}

\paragraph{Overall performance.}
The average performance for each budget and policy over 50 runs are shown in Figure~\ref{fig:main_results}, which shows that $\DPI$ clearly outperforms the other policies over a wide range of budget values.
For a practical interpretation of the results, consider \textsf{Baseline} at a budget of 26, the policy and budget that Keheala was operating during the RCT, which results in an overall verification rate of 62.0\%.
By using less than \textit{half} of the budget, $B=12$, $\DPI$ achieves the same verification rate at 62.2\%.
As the costliest aspect of Keheala's system is in hiring staff to provide the interventions, these results imply that they can cut these costs by half to achieve the same outcome.
For the $\bandit$ policy, a budget of 23 results in a similar performance at 61.9\%, hence the cost reduction under this policy is 11.5\%.

Additionally, we observe that the improvement of $\DPI$ compared to the other policies is especially substantial for smaller budgets. 
This implies that when the number of patients that can be targeted is small, $\DPI$ can correctly identify the set of patients to target that result in the largest gains.
This is especially valuable for scaling up the system.
Indeed, if Keheala wanted to expand to include more patients without linearly increasing their staff costs, then the ratio of budget to the number of patients would decrease, resulting in the regime where $\DPI$ offers major improvements.

The fact that the performance of $\bandit$ policy improves over $\baseline$ as the budget increases is caused by the increase in relevant data.
Note that the number of interventions is small ($\sim 26$) relative to the total number of patients in the system ($\sim 600$), implying that the number of data points with $A=1$ is much smaller than that of $A=0$. 
Therefore, the main bottleneck in estimation is learning patient behaviors after receiving an intervention.
As the budget increases, the $\bandit$ has access to more data from patients with an intervention, and hence is able to improve its learning. 


% \paragraph{Patient-level verification rates.}
% The overall number of verifications increases under $\DPI$, but how do these rates get impacted at the patient-level?
% Fixing the budget to be 32, we compute the overall verification rate of each patient, and we examine the distribution of these patient-level rates.
% In \cref{fig:diff_vrates}, we plot the difference in the number of patients that had a particular verification rate under $\DPI$ versus $\BASELINE$.
% We see that under $\DPI$, fewer patients have a low verification rate (less than 50\%), and the number of patients with a moderately high verification rate (50\%-90\%) increases.
% This particular shift in the verification rate distribution is actually desirable, compared to other possible shifts that result in a similar increase in overall verification rate.
% For example, suppose the improvement in overall verification rate of $\DPI$ came from patients who had a 0\% verification under $\BASELINE$ improving to, say, 20\%.
% A 20\% treatment adherence is low enough that the patient is unlikely to be cured of TB.
% Therefore, in terms of treatment outcomes, such a distributional shift would be inconsequential.
% \jnote{TODO: look at data on verification rates to outcomes to see if we can make this argument more precise?}

% \begin{figure}[h]
% \begin{center}
%   \includegraphics[width=0.7\linewidth]{figs/vrates_diff}
%   \caption{TODO. The first bins represent patients whose overall verification rate was between 0\% to 5\%.}
%   \label{fig:diff_vrates}
% \end{center}
% \end{figure}

\paragraph{Description of the targeted patients.}
In \cref{tab.stats}, we fix the budget to be 26 and we show statistics regarding the state of the targeted patients for each of the three policies.
For example, under $\baseline$, on average, the patient that received an intervention had a treatment effect of 8.8\% with respect to the probability that they will verify the next day.
8.8\% is the `true' average treatment effect, in the sense that the numbers that are averaged are taken directly from the simulation model.


\begin{table}[h]
\TableSpaced % INFORMS
\caption{
Average statistics of the state of patients who were given an intervention, across the three policies that were run for $B=26$.
(a) is the average value of $\tau(x)$, the increase in probability that the patient verifies the next day when they are given an intervention. 
(b) is the average $f(x, 0)$, the probability that a patient verifies the next day \textit{without} an intervention. 
% (c) is the average verification rate of the patient when they are given the intervention.
(c) is the average number of remaining days the patient will be on treatment for.
} \label{tab.stats}
\vspace{2mm}
\begin{center}
\begin{tabular}{@{}cccc@{}}
\toprule
                                                 & $\baseline$& $\bandit$ & $\DPI$ \\ \midrule
\multicolumn{1}{l}{(a) Next-day treatment effect}   & 8.8\%    & 13.2\%  & 10.6\%        \\ 
\multicolumn{1}{l}{(b) Next-day base probability}   & 18.2\%   & 35.2\%  & 22.2\%       \\ 
\multicolumn{1}{l}{(c) Days on treatment remaining} & 68.3     & 92.4    & 109.3        \\ \bottomrule
\end{tabular}
\end{center}
\end{table}



Statistic (a) represents exactly what the $\bandit$ policy optimizes for, the increase in probability of the patient verifying the next day.
The fact that $\bandit$ yields the highest value confirms that indeed, the policy correctly learns what it is supposed to learn.
$\DPI$ chooses patients with a higher one-step treatment effect than $\baseline$, but lower than that of $\bandit$.
Then, the fact $\DPI$ outperforms $\bandit$ in terms of overall verification implies that a myopic strategy of looking only one step ahead is not sufficient.
The next two statistics shed light on why this may be.


Statistic (b) represents the probability that the targeted patient would have verified anyway without an intervention, and we see that the $\bandit$ targets patients with a much higher base probability than the other policies. 
We plot the entire distribution of this quantity in Figure~\ref{fig:base_probs}, where we see that $\bandit$ often targets those with a relatively high base probability ($>45\%$), while $\DPI$ targets those with a relatively low base probability ($<15\%$).
This may contribute to the improved performance of $\DPI$, and reasoning for this can be seen through the 2-state model from \cref{ss.2statemodel}, where the base probability corresponds to the parameter $p$.
If two patients have the same values of the parameters $q$ and $\tau$ but differing values for $p$, the intervention value is higher when $p$ is smaller (see \cref{prop:z_clean_form}).
This is because the patient with a high value of $p$ is more likely to switch to state 1 at the current time step as well as all future time steps.
As an extreme example, for a patient with $p=0$, they \textit{need} an intervention to switch to state 1, whereas a patient with $p>0$ may switch to state 1 (either now or in the future), without an intervention.
Therefore, an intervention is more likely to be helpful for those with a smaller base probability, which $\DPI$ targets.


\begin{figure}[h]
\begin{center}
\vspace{-5mm}
  \includegraphics[width=0.54\linewidth]{figs/base_probs_Feb5}
\vspace{-2mm}
  \caption{Histogram of the base probability of targeted patients, the probability that the patients would verify without an intervention.
  This is the entire distribution of the statistic (b) in \cref{tab.stats} for $\bandit$ and $\DPI$.}
  \label{fig:base_probs}
\vspace{-6mm}
\end{center}
\end{figure}

Lastly, statistic (c) is the average number of days a targeted patient has remaining on the platform.
If an intervention positively affects patients for all of their future time steps, then targeting those with longer time left in the system would result in higher benefits. 
The results show that $\DPI$ targets those with the longest days of treatment left.





