%!TEX root=../Draft1.tex

Our work contributes to the growing literature on optimizing individual-level interventions, often in the context of behavioral health support. Here, we summarize prior work on similar problems, and contrast it with our approach. 


\paragraph{Greedy and multi-armed bandits.}
As described in the introduction, a simple greedy approach is to maximize the immediate reward at each time step.
For example, in the treatment adherence setting, one can reach out to patients with the highest increase in their probability of adhering in the next day from the intervention.
A multi-armed bandit is one natural framework that can be used to learn such a policy.
This is the approach used in HeartSteps \citep{lei2017actor,liao2020personalized}, a program to promote physical activity using real-time data collected from wristband sensors.
These papers use a contextual bandit model, where the context represents a user at a particular time step, and they develop a bandit algorithm whose goal is to increase the immediate activity level of the user.
Given the vast literature on contextual bandits, there are a wide variety of algorithms that one can easily plug in.

A fundamental downside of this greedy approach is that it does not capture any potential long-term effects of an action---that is, an action may not only impact a patient's immediate behavior, but their behavior for all future time steps.
One way to address this is to specifically model the type of long-term effect it can have.
For example, \cite{liao2020personalized} introduces a `dosage' variable that models the phenomenon that the treatment effect of an action is often smaller when an action was recently given to that patient in the past.
Similarly, \cite{mintz2020nonstationary} incorporates habituation and recovery dynamics into the bandit framework.
However, these approaches captures only particular types of long-term effects that are explicitly modeled, and there could be other, complex factors that affect the downstream behavior of patients.

Our work does not take this greedy approach, and we aim to learn a policy that maximizes long-term rewards, without specifically modeling the type of long-term effect that an action can have.
We benchmark against a contextual bandit policy, and we observe that incorporating the long-term effects is critical in the behavioral health setting that we study. 

\paragraph{Restless bandits.}
Theoretically, our model falls under the restless multi-armed bandit framework.
Each patient corresponds to an arm, there is a budget on the number of arms that can be `pulled' (given an intervention) at each time step. 
The state of each arm evolves as a Markov chain, where its transition probabilities depend on the action taken.
In general, finding the optimal policy to restless bandits is PSPACE-hard \citep{papadimitriou1994complexity}. 
A commonly used policy is the Whittle's index \citep{whittle1988restless}, which is known to be asymptotically optimal under certain conditions \citep{weber1990index}.
Most of the literature on restless bandits develops algorithms to approximate the optimal policy assuming \textit{perfect knowledge} of the parameters of each arm (e.g., \cite{liu2010indexability,guha2010approximation}), which is not a realistic assumption in practice.

Several recent works on restless bandits remove the assumption of perfect knowledge of the system and study the learning problem.
Some approaches adapt algorithms from the multi-armed bandit literature such as UCB \citep{wang2020restless} or Thompson Sampling \citep{jung2019regret}, and there are also approaches that adapt reinforcement learning methods such as Q-learning (e.g., \cite{fu2019towards,avrachenkov2022whittle}).
All of these methods are \textit{online} learning algorithms --- the algorithm updates over time as it collects more data, and the policy must explore in order to eventually converge to the optimal policy.
% The theoretical results of these online approaches show that they eventually converge to the optimal policy.
% Such methods have been applied to behavioral healthcare settings.
% For example, \cite{mate2022field} studies an information program for maternal health, where they posit an MDP with two states to model the engagement level of a user.
% The authors use data to estimate the transition parameters of this MDP, and employ the Whittle's index using the estimated parameters.
% \cite{biswas2021learn} study a similar setting where they posit an MDP with three states and develop a Q-learning policy that aims to directly learn the Whittle's index of the MDP.

Our approach differs from the aforementioned literature in a couple of ways.
First, we take an \textit{offline} approach, where we leverage existing data to derive a new policy.
This removes the need for exploration, as well as the dependence on a long horizon to guarantee an improvement over a baseline policy.
Second, all of the existing learning approaches for restless bandits estimate quantities for each arm \textit{separately}, while our policy learns across arms by pooling data together.
In settings with many patients, but not much data on any \textit{single} patient, pooling data across patients can be greatly beneficial.
Third, we do not postulate a simple model of patient behavior, and rather, take a model-free approach.
For example, \cite{mate2022field} and \cite{biswas2021learn} posit a simple MDP with two and three states respectively for each user, where a state represents the engagement level of the user. 
\cite{aswani2019behavioral} take a slightly different model-based approach in studying weight loss interventions, where they model user behavior via utility functions.
% Another model-based approach include a line of work that studies weight loss programs that model behavior through user utilities \citep{mintz2017behavioral,zhou2018evaluating,aswani2019behavioral}.
Then, the policy is developed based on these posited models.
These approaches rely heavily on the correctness of the models, and cannot take other \textit{non-modeled} factors into account, such as non-stationarity of patient behaviors.
Moreover, it is unclear how policies such as the Whittle's index behave under model misspecification.
In contrast, we incorporate as much information as available (at the time) into the `state' of a patient, and our policy then operations under the assumption that patients in similar states will behave similarly.
Our work does use a simple, 2-state MDP for the purposes of proving a theoretical guarantee for our policy.
However, the policy itself is not reliant on this 2-state model, and it can be deployed in a model-free way.

\iffalse
\paragraph{Model-based approach.}
One line of approach for this problem is to postulate a simple model of patient behavior, and then apply an optimal or near-optimal policy specialized for that particular model.
For example, \cite{mate2022field} studies an information program for maternal health, where they posit an MDP with two states to model the engagement level of a user. The authors use data to estimate the transition parameters of this MDP, and then they employ a policy that is optimal for this MDP, the Whittle's index, using the estimated parameters.
\cite{biswas2021learn} study a similar setting where they posit an MDP with three states and develop a Q-learning policy that aims to directly learn the Whittle's index of the MDP.
Another line of work that studies weight loss programs take a similar approach based on modeling user utilities \citep{mintz2017behavioral,zhou2018evaluating,aswani2019behavioral}. 
% uses a utility function to model user behavior, estimates the parameters of the utility model based on data, and then optimizes based on this utility model.
A drawback of these approaches is that it heavily relies on the correctness of the postulated model.
For example, though the Whittle's index is known to be a good policy for a given MDP, it is completely unclear how it performs under model misspecification.
Relatedly, these approaches does not take other \textit{non-modeled} factors into account, such as non-stationarity of patient behaviors.

In our work, we propose a general-purpose algorithm that does not rely on a postulated behavioral model.
We do use a simple, 2-state MDP for the purposes of proving a theoretical guarantee for our policy.
However, the policy itself does not rely on this 2-state model, and it can be deployed in a model-free way, which we demonstrate in our case study.

\paragraph{Time frame of rewards.}
Algorithms usually aim to maximize a `reward', but they can differ in what the reward represents.
One simple approach is to be myopic and simply aim to maximize the \textit{immediate} reward from a given action.
% For example, the immediate reward of an outreach to a patient on TB treatment is whether they verified their adherence \textit{the next day}.
This is the approach used in HeartSteps \citep{lei2017actor,liao2020personalized}, a program to promote physical activity using real-time data collected from wristband sensor.
These works use a contextual bandit model, where the context represents a user at a particular time step, and they develop a bandit algorithm whose goal is to increase the immediate activity level of the user.
The obvious downside of this approach is that it does not capture any potential long-term effects of an action.
On the other hand, if an algorithm optimizes for long-term rewards, a drawback is that the feedback cycle is longer, and it is difficult to attribute rewards to a particular action, since other actions were also taken in the future.
We take the latter approach while also benchmarking with the former, and we show that incorporating the long-term effects is critical in the behavioral health setting that we study.

\paragraph{Learning across users.}
Another dimension of an algorithm is whether learning is pooled across users.
That is, to learn about the behavior of a user, how should data collected from other users be used, if at all?
Both pooling and non-pooling approaches have been proposed in the literature; for example, \cite{mate2022field} uses a clustering approach to pool similar users together, while \cite{biswas2021learn}, who study the same setting, does not pool data across users.
Data-pooling can be greatly beneficial in settings where there are many users, but each single user is in the system for a relatively short time frame.

Our work employs a similar approach to the contextual bandit method of \cite{liao2020personalized} in terms learning across users.
We define the notion of a `state' of a patient, which includes both their static features and their activity on the platform.
Our policy then learns and operates based on these states, which assumes that patients who are in similar states act similarly, and hence leverages cross-learning on patients.
% by incorporating their static covariates as well as their history into a `state', and 


\paragraph{Online vs. offline learning.}
Another dimension to take into consideration is whether the learning is done in an online or offline manner.
Multi-armed bandit or RL approaches are inherently online, and a key feature of such algorithms is the need to `explore', or `experiment'.
% Though exploration is beneficial (and often necessary) for the long-run, it deteriorates performance in the short run.
Such algorithms are not only hard to implement in a field setting but also suffer from a long exploration period, which is not only undesirable in a sensitive healthcare context but also not guaranteed to result in a good policy within the time horizon of a single patient's treatment regimen.
In contrast, we take an offline approach by using existing data gathered from a base policy to derive a new policy with performance improvement guarantees, which can be applied to a new cohort of patients without exploration.

% Such algorithms are not only hard to implement in a field setting but also suffer from the problem's sample complexity, requiring a long exploration period for each patient, which is not only undesirable in a sensitive healthcare setting but also not guaranteed to result in a good policy within the time horizon of a single patient's treatment regimen. In contrast, our approach uses existing data in a model free way to develop a policy with performance guarantees that can be learned through a one-step on-policy iteration and applied to a new cohort of patients without exploration. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\citep{mintz2020nonstationary, yao2020power} - bandit work for weight loss?



\cite{Rosenthal20Efficient}
\cite{Das19Learning}


Tanzeem Choudhury
\citep{Rabbi15Mybehavior,Rabbi18Feasibility} - diet and exercising -- JOJ check

Milind's work
\cite{Killian19Learning}
\citep{Yadav18Bridging,yadav2016using,yadav2017influence} - HIV awareness

Susan's work
\cite{nahum2018just}


Multi-armed bandit work
\citep{mintz2020nonstationary, yao2020power,carpenter2020developments,bidargaddi2020designing}.

% \cite{mintz2020nonstationary} explicitly models nonstationarity in a behavioral setting. 
\fi

\paragraph{Healthcare systems.}
Finally, from an application perspective, our work contributes to a growing literature focusing on improving healthcare delivery systems in resource-limited settings. Most related to the paper at hand are recent papers focusing on improving TB outcomes in resource-limited settings. Much of this work has been on the policy level, with \cite{Suen14Disease} evaluating strategic alternatives for disease control of multi-drug resistant TB (MDR TB) in India and finding that with MDR TB transitioning from treatment-generated to transmission-generated, rapid diagnosis of MDR TB becomes increasingly important. Similarly, \cite{Suen18Optimal} optimize the timing of sequential tests for TB drug resistance, a necessary step for transitioning patients to second-line treatment. 

Two papers focus on medication adherence. \cite{Suen22Design} tackle the problem of designing patient-level incentives to motivate medication adherence, in situations where adherence is observable but patients have unobserved and heterogeneous preferences for adherence. They first take a modeling approach to design an optimal incentive scheme and then demonstrate that deploying it would be cost effective in the context of TB control in India. Similar to us, \cite{Boutilier22Improving} focus on a behavioral intervention, demonstrating that data describing patient behavior (e.g., patterns of self-verification of treatment adherence, like in the case of Keheala) can be leveraged to predict short-term behavior as well as long-term outcomes. They use such predictions to assign patients to risk groups and demonstrate empirically that outreach can be effective, even for patients who are classified as at risk. However, they stop short of prescribing an actionable policy for assigning patient outreach, which is the topic of this paper.

%To our knowledge, no prior work has developed personalized and actionable outreach policies for supporting behavioral health in resource-limited settings (e.g., see discussion in \cite{Ruggeri20Behavioral}).
%
%\edit{JOJ: Check Tambe papers, or tone down statement. Use language from intro perhaps. Also need to discuss BMJ Global Health paper}
%
%\edit{Jackie: yea Tambe's papers do this so I think it's too strong.}
%
%\edit{Justin: agree with Jackie: re Milind's papers. I'm not sure we need the BMJ GH paper, but it could be used to support the importance of personalization since we show in that paper that treatment effects are highly heterogenous}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Our work contributes to two, mostly non-overlapping, streams of literature. First, the growing literature that focuses on improving healthcare delivery in resource-limited settings through better operational decisions (\S \ref{ss.globalhealth}). Second, an emerging literature developing dynamic decision making support for mobile health (mHealth) applications or behavioral health support (\S \ref{ss.decisionmaking}). 
%
%\subsection{Global Health Operations Management}\label{ss.globalhealth}
%A growing literature in operations focuses on understanding and improving healthcare delivery systems in resource-limited settings. The general motivation in this literature is that the limited resources result in delivery systems that are structurally different from healthcare systems in less resource-constrained settings, which necessitates tailoring analyses and methods to those differences \citep{Kraiselburd13Supply,Jonasson22Social}.
%
%Some of this literature revisits standard operations management topics: for example the management of inventory and procurement for health products in challenging settings \citep{Gallien17National,Leung16Impact,Natarajan14Inventory,Natarajan17Multi}; supply chain design and optimization for diagnostic samples, medical supplies, and vaccines \citep{Jonasson17Improving,Parvin18Distribution,deBoeck21Vaccine}; how to mitigate agency issues through contract design \citep{Taylor14Subsidizing,Levi17Effectiveness,Zhang20Truthful}; as well as the optimal deployment, routing, or maintenance of vehicle fleets in global health settings \citep{Boutilier20Ambulance,Chen21Vehicle,Gibson20Redesigning, Killian21Unstructured}.
%
%Another stream of work is focused on (usually data-driven) cost-effectiveness analyses for specific health conditions, including chronic ones such as diabetes \citep{Boutilier21Risk}, mental health ones for vulnerable populations \citep{Zhong21Health}, and communicable diseases \citep{barrow2020optimizing,choi2017cost,claypool2019quantifying,claypool2021assessing}.  
%
%Most related to the paper at hand are recent papers focusing on improving TB outcomes in resource-limited settings. Much of this work has been on the policy level, with \cite{Suen14Disease} evaluating strategic alternatives for disease control of multi-drug resistant TB (MDR TB) in India and finding that with MDR TB transitioning from treatment-generated to transmission-generated, rapid diagnosis of MDR TB becomes increasingly important. Similarly \citep{Suen18Optimal} optimize the timing of sequential tests for TB drug resistance, a necessary step for transitioning patients to second-line treatment. 
%
%Two papers focus on medication adherence. \citep{Suen22Design} tackle the problem of designing patient-level incentives to motivate medication adherence, in situations where adherence is observable but patients have unobserved and heterogeneous preferences for adherence. They first take a modeling approach to design an optimal incentive scheme and then demonstrate that deploying it would be cost effective in the context of TB control in India. 
%%Premature cessation of antibiotic therapy (nonadherence) is common in long treatment regimens and can severely compromise health outcomes. In this work, we investigate the problem of designing a schedule of incentive payments to induce socially optimal treatment adherence levels in a setting in which treatment adherence can be observed (e.g., through directly observed therapy for tuberculosis), but patient preferences for treatment adherence are heterogeneous and unobservable to a health provider. The novel elements of this problem stem from its institutional features: there is a single incentive schedule applied to all patients, incentive payments must be increasing in patientsâ€™ adherence, and patients cannot be a priori prohibited from any given levels of adherence. We develop models to design optimal incentives incorporating these features, and they are also applicable in other problem contexts that share the same features. We also conduct a numerical study using representative data in the context of the tuberculosis epidemic in India. Our study shows that our optimally designed incentive schedules are generally cost-effective compared with a linear incentive benchmark.
%Similar to us, \cite{Boutilier22Improving} focus on a behavioral intervention, demonstrating that data describing patient behavior (e.g., patterns of self-verification of treatment adherence, like in the case of Keheala) can be leveraged to predict short-term behavior as well as long-term outcomes. They use such predictions to assign patients to risk groups and demonstrate empirically that outreach can be effective, even for patients who are classified as at risk. However, they fall short of prescribing an actionable policy for assigning patient outreach, which is the topic of this paper.
%
%To our knowledge, no prior work has developed personalized and actionable outreach policies for supporting behavioral health in resource-limited settings (e.g., see discussion in \cite{Ruggeri20Behavioral}).
%
%\subsection{Dynamic Decision Support for Outreach and Engagement}\label{ss.decisionmaking}

%\citep{Mills20Personalized} makes the point that most behavioral interventions (digital?) have been homogeneous, not personalized. 

% Move to introduction of case study
%\paragraph{Material for the introduction of the case study or introduction}
%\citep{Dimatteo02Patient} - importance of adherence for clinical outcomes
%\cite{Nieuwlaat14Interventions}, \cite{Kardas13Determinants}, \cite{Dimatteo12Improving} - reviews for interventions to improve adherence
%\cite{Munro07Patient} and \cite{Kardas13Determinants} - surveys for lack of TB adherence and determinants for adherence