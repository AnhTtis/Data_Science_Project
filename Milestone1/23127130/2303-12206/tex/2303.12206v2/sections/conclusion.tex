

\edit{
This work tackles an important problem of personalizing and optimizing costly interventions in the context of digital systems for behavioral health.  
We develop an approach, $\DPI$, that learns an intervention policy from an existing dataset collected from a pilot study.
 $\DPI$ is model-free, which avoids the need to specify a model of patient behaviors.
Unlike many reinforcement learning methods which often rely on a long horizon to achieve good performance, $\DPI$ leverages offline data to immediately provide an effective policy.
We provide a theoretical guarantee on a special case of the model that represents the practical setting of interest, and it exhibits strong empirical performance on a validated simulation model of a real-world behavioral health setting.





\textbf{Limitations and future directions.}
Lastly, we discuss limitations of the current work that serve as valuable future directions.
One gap between our model and the practical application of Keheala is that the ultimate objective of Keheala is to improve eventual health outcomes (i.e., cure patients of TB).
There are two major hurdles that need to be addressed in order to fully align with this goal, within the existing infrastructure of Keheala (of using daily adherence information).
The first obstacle is the lack of a mapping between treatment adherence patterns to health outcomes.
It has been shown that higher verification rates are associated with better outcomes \citep{Boutilier22Improving}
but it would be valuable to identify more specific and causal relationships (e.g., is it more important for a patient to adhere to their treatment in the earlier phase of their treatment regime compared to later?).
Addressing this issue is very specific to TB but would be tremendously valuable not only for optimizing a platform like Keheala, but also for the broader medical research on TB.
Second, a problem less specific to TB, is to design policies that can optimize for reward functions which are not necessarily additive for each time step (e.g., maximize the number of patients whose overall verification percentage is over 70\%).

Next, there are other interesting extensions to the model and the algorithm that one can consider.
With respect to performance guarantees on the stylized model in \cref{s.theory}, it would be valuable to analyze how the guarantee is impacted by various modeling extensions such as generalizing the MDP (e.g., more states, having interventions impact all states), or extending the class of base policies.
From an algorithmic standpoint, valuable extensions include incorporating online samples, and incorporating other practical considerations such as fairness in how the interventions are distributed across patients.
}


