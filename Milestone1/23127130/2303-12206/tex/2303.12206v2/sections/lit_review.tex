\edit{
Our work relates to the expansive streams of literature on reinforcement learning and (approximate) dynamic programming, as well as the applied operations research literature focusing on improving healthcare delivery in resource-limited settings. Methodologically speaking, most existing solution approaches to problems similar to ours can be classified as either (a) developing policies for a \textit{known} underlying model of behavior or (b) \textit{learning} a policy using data. We summarize these two streams of work in Sections \ref{ss.knownmod} and \ref{ss.unknownmod}, respectively, before discussing prior work on TB treatment as an application area in \cref{ss.litreviewhealth}.

\edit{\subsection{Known Model}\label{ss.knownmod}
Our model (described in detail in \cref{s.gmodel}) assumes that every patient behaves according to a Markov decision process (MDP). Even if all the MDP parameters of these models were known exactly, the size of the system state space would be \textit{exponential} in the number of patients: the size of the state space is $|\cS|^N$, where $\cS$ is the state space for one patient and $N$ is the number of patients. As a result, a direct application of dynamic programming techniques such as backwards induction or policy/value iteration would take exponential time and hence is practically infeasible.

As exact methods are infeasible, one can resort to \textit{approximate} dynamic programming (ADP) techniques developed for weakly-coupled MDPs \citep[e.g.,][]{meuleau1998solving,adelman2008relaxations,d2023optimal}. Specifically, the model we study is a \textit{restless bandit}: each patient corresponds to an arm, there is a budget on the number of arms that can be `pulled' (given an intervention) at each time step. The state of each arm evolves as a Markov chain, where its transition probabilities depend on the action taken. It is known that finding the optimal policy to a restless bandit is PSPACE-hard \citep{papadimitriou1994complexity}. There is a large literature on developing algorithms for this problem \citep[e.g., ][]{whittle1988restless,glazebrook2002index,ansell2003whittle,glazebrook2006some,liu2010indexability,guha2010approximation}.
A commonly used policy is the Whittle's index \citep{whittle1988restless}, which is known to be asymptotically optimal under certain conditions \citep{weber1990index} and has been shown to have good empirical performance 
\citep{ansell2003whittle,glazebrook2002index,glazebrook2006some}.
All of the above methods assume that the model is known, whereas our problem has the further nontrivial complication that the model is unknown.
}


\subsection{Unknown Model}\label{ss.unknownmod}
\edit{Deriving an optimal policy for an MDP with unknown parameters is corresponds to reinforcement learning (RL), a rapidly expanding area of research \citep{sutton2018reinforcement}.
However, a naive application of the RL framework onto our problem results in an exponentially large state space --- this correspondingly results in an exponential blowup in the data requirements to deploy generic RL methods such as Q-learning \citep{jin2018q}.
Therefore, more tailored approaches are required to fit to this regime.}


\paragraph{Greedy and multi-armed bandits.}
One approach, as described in the introduction, is to greedily maximize the immediate reward at each time step.
For example, in the treatment adherence setting, one can reach out to patients with the highest increase in their probability of adhering in the next day from the intervention.
A multi-armed bandit is one natural framework that can be used to learn such a policy.
This is the approach used in HeartSteps \citep{lei2017actor,liao2020personalized}, a program to promote physical activity using real-time data collected from wristband sensors.
These papers use a contextual bandit model, where the context represents a user at a particular time step, and they develop a bandit algorithm whose goal is to increase the immediate activity level of the user.
Given the vast literature on contextual bandits, there are a wide variety of algorithms that one can easily plug in.

A fundamental downside of this greedy approach is that it does not capture any potential long-term effects of an action---that is, an action may not only impact a patient's immediate behavior, but their behavior for all future time steps.
One way to address this is to specifically model the type of long-term effect it can have.
For example, \cite{liao2020personalized} introduce a `dosage' variable that models the phenomenon that the treatment effect of an action is often smaller when an action was recently given to that patient in the past.
Similarly, \cite{mintz2020nonstationary} incorporate habituation and recovery dynamics into the bandit framework.
However, these approaches captures only particular types of long-term effects that are explicitly modeled, and there could be other, complex factors that affect the downstream behavior of patients.

Our work does not take this greedy approach, and we aim to learn a policy that maximizes long-term rewards, without specifically modeling the type of long-term effect that an action can have.
We benchmark against a contextual bandit policy, and we observe that incorporating the long-term effects is critical in the behavioral health setting that we study. 

\paragraph{Learning for restless bandits.}
\edit{A non-greedy approach to this problem corresponds to the restless bandit model in the unknown parameter regime.
There is a nascent literature develops learning algorithms in this setting. One approach is to adapt algorithms from the multi-armed bandit literature such as UCB \citep{wang2020restless} or Thompson Sampling \citep{jung2019regret}. 
Recent approaches similarly adapt reinforcement learning methods such as Q-learning \citep[e.g., ][]{fu2019towards,avrachenkov2022whittle}. Importantly, these methods are \textit{online} learning algorithms that require continuous exploration and assume that the state space is known and finite. This literature focuses on providing theoretical guarantees of the proposed algorithms {eventually} converging to the {optimal} policy.
}

Our work differs from the aforementioned literature in a couple of ways.
First, we take an \textit{offline} approach, where we leverage existing data to derive a new policy.
This removes the need for exploration, as well as the dependence on a long horizon to \edit{obtain} an improvement over a baseline policy.
Second, our work does not focus on deriving an \textit{optimal} policy; we derive a practical policy that can be implemented with limited data, and our theoretical results are approximation guarantees to the optimal policy.
Lastly, we do not postulate a simple model of patient behavior, and rather, take a model-free approach.
For example, \cite{mate2022field} and \cite{biswas2021learn} posit a simple MDP with two and three states respectively for each user, where a state represents the engagement level of the user. 
\cite{aswani2019behavioral} take a slightly different model-based approach in studying weight loss interventions, where they model user behavior via utility functions.
Then, the policy is developed based on these posited models.
These approaches rely heavily on the correctness of the models, and cannot take other \textit{non-modeled} factors into account, such as non-stationarity of patient behaviors.
Moreover, it is unclear how policies such as the Whittle's index behave under model misspecification.
In contrast, our model-free approach allows us to incorporate as much information as available (at the time) into the `state' of a patient, and our policy then operations under the assumption that patients in similar states will behave similarly.
We rely on the prediction algorithm that estimates the state-action values to identify the most relevant features of the state.
In \cref{s.theory}, we use a simple 2-state MDP for the purposes of proving a theoretical guarantee for our policy, but the policy is defined irrespective of the underlying patient behavior model.
}


\subsection{Healthcare systems} \label{ss.litreviewhealth}
Finally, from an application perspective, our work contributes to a growing literature focusing on improving healthcare delivery systems in resource-limited settings. Most related to the paper at hand are recent papers focusing on improving TB outcomes in resource-limited settings. Much of this work has been on the policy level, with \cite{Suen14Disease} evaluating strategic alternatives for disease control of multi-drug resistant TB (MDR TB) in India and finding that with MDR TB transitioning from treatment-generated to transmission-generated, rapid diagnosis of MDR TB becomes increasingly important. Similarly, \cite{Suen18Optimal} optimize the timing of sequential tests for TB drug resistance, a necessary step for transitioning patients to second-line treatment. 

Two papers focus on medication adherence. \cite{Suen22Design} tackle the problem of designing patient-level incentives to motivate medication adherence, in situations where adherence is observable but patients have unobserved and heterogeneous preferences for adherence. They first take a modeling approach to design an optimal incentive scheme and then demonstrate that deploying it would be cost effective in the context of TB control in India. Similar to us, \cite{Boutilier22Improving} focus on a behavioral intervention, demonstrating that data describing patient behavior (e.g., patterns of self-verification of treatment adherence, like in the case of Keheala) can be leveraged to predict short-term behavior as well as long-term outcomes. They use such predictions to assign patients to risk groups and demonstrate empirically that outreach can be effective, even for patients who are classified as at risk. However, they stop short of prescribing an actionable policy for assigning patient outreach, which is the topic of this paper.





