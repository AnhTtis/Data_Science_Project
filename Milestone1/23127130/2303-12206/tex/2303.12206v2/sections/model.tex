\edit{The first objective of this section is to formally describe the intended problem setting in its full complexity (\cref{s.gmodel}). 
We then describe our algorithm (\cref{s.decomposedPI}) and discuss the intended usage and design choices of the algorithm (\cref{s.intended_usage}).
Given the complexity of the setting described here, we do not present any theoretical performance guarantees using the full model. In \cref{s.theory} we will provide theoretical results for a stylized version of the problem while in \cref{s.casestudy} we will revisit the full model and discuss numerical results for a real-life situation. }


\subsection{Model} \label{s.gmodel}
There are $N$ patients and $T$ time steps.
Each patient $i \in [N]$ is associated with a Markov decision process (MDP) represented by $M_i = (\cS, \cA, P_i, R_i)$.
$\cS$ is the state space, and let $S_{it} \in \cS $ denote the state of patient $i$ at time $t$.
The action space is $\cA = \{0, 1\}$, where we will refer to action $A_{it} = 0$ as the \textit{null} action, and action $A_{it}=1$ as the \textit{intervention}.
$P_i(s, s', a) = \Pr(S_{i,t+1} = s' \;|\; S_{it} = s, A_{it} = a)$ is the probability of transition from state $s$ to $s'$ when action $a$ is taken, and $R_i(s, s', a)$ is the immediate reward from this transition.
If $A_{it} = 1$, we say that patient $i$ is \textit{chosen} at time $t$.

Next, we define a \textit{system MDP} by combining the $N$ patient MDPs and adding a budget $B \geq 0$ on the number of interventions at each time step.
That is, $\Aspace = \{(A_1, \dots, A_N) \subseteq \{0, 1\}^N: \sumN A_i \leq B\}$ is the action space for the system MDP.
The state space is $\cS^N$, 
and a policy $\pi = \{\pi_t \;|\; t \in [T]\}$ maps each state to a distribution over the action space $\Aspace$.
At each time $t = 1, \dots, T$, the following sequence of events occur:
\begin{enumerate}
	\item The state $S_{it} \in  \cS$ is observed for each patient $i \in [N]$.
	\item An action $(A_{1t}, \dots, A_{Nt})$ is drawn from $\pi_t(S_{1t}, \dots, S_{Nt})$.
	\item For each patient $i$, their next state $S_{i,t+1}$ is realized independently according to their transition probabilities $P_i(S_{it}, \cdot, A_{it})$. We gain the reward $\sumN R_i(S_{it}, S_{i,t+1}, A_{it})$.
\end{enumerate}
Let $\mathbf{S}_1 = (S_{11}, \dots, S_{N1})$ be the starting state profile.
An instance of this problem is represented by $\cI = (N, T, (M_i)_{i \in [N]}, B, \mathbf{S}_1)$.


\subsection{Decomposed Policy Iteration} \label{s.decomposedPI}

We introduce a natural policy closely related to the ubiquitous \textit{policy iteration} (PI) algorithm \citep{howard1960dynamic}.
As the name suggests, policy iteration is an iterative algorithm that maintains a policy and updates it at each iteration to improve its performance.
Our policy, $\DPI$, can be thought of as performing \textit{one step} of policy iteration.
In addition, $\DPI$ differs from vanilla PI in that the algorithm decomposes to the patient level, which allows us to remove dependence on the exponentially sized state and action space.


\subsubsection{Intervention values.}
For any policy $\pi$, let $S^\pi_{it}$ and $A_{it}^\pi$ be the induced random variables corresponding to the state and action, respectively, of patient $i$ at time $t$ under policy $\pi$.
Then, we define 
\begin{align} \label{eq:q}
q_{it}^\pi(s, a) = \bE_{\pi}\left[\sum_{t'=t}^T R(S_{it'}^\pi, S_{i,t'+1}^\pi, A_{it'}^\pi) \;\bigg|\; S_{it}^\pi = s, A_{it}^\pi = a \right],
\end{align}
which represents the expected future reward from patient $i$ when running policy $\pi$, conditioned that they were in state $s$ and were given action $a$ at time $t$.
In the case where $\Pr(S_{it}^\pi = s, A_{it}^\pi = a) = 0$, we define $q_{it}^\pi(s, a) = 0$.
Next, we define an \textit{intervention value}, $z^{\pi}_{it}(s)$, to be the difference in the $q^{\pi}_{it}$ values under $a=1$ and $a=0$:
\begin{align*}
z^\pi_{it}(s) = q_{it}^{\pi}(s, 1) - q_{it}^{\pi}(s, 0).
\end{align*}
$z^\pi_{it}(s)$ is the difference in expected total future reward from patient $i$ under policy $\pi$ when the patient is given the intervention, compared to when they are not.

\subsubsection{Policy.}
Given a \textit{base policy} $\pi$, $\DPI(\pi)$ is the policy that gives the intervention to the patients with the highest positive intervention values, up to the budget constraint.
Formally, 
\begin{align*}
\DPI(\pi)_t(S_{1t}, \dots, S_{Nt}) \in \argmax_{A \in \cA_N} \sum_{i \in [N]} A_{i} z^\pi_{it}(S_{it}).
\end{align*}

\edit{
\subsection{Intended Usage and Design Choices} \label{s.intended_usage}
Recall that our motivating research question is to leverage a limited dataset collected from a pilot study to learn an effective intervention policy for subsequent deployment. $\DPI$ was designed specifically for this setting. The intended usage of $\DPI$ therefore follows three steps. First, run a policy $\pi$ and let $\mathcal{D}$ be the dataset of trajectories of states, actions, and rewards. Second, use the dataset $\mathcal{D}$ to compute estimates $\hat{q}_{it}^{\pi}(s, a)$ using a prediction algorithm. Third, deploy $\DPI(\pi)$ using the estimated intervention values, $\hat{z}^{\pi}_{it}(s) = \hat{q}_{it}^{\pi}(s, 1) - \hat{q}_{it}^{\pi}(s, 0)$.
Note that the main non-trivial step is the second one, the estimation. $\DPI$ was designed in a way to make this step as easy as possible. In particular, $\DPI$ can essentially be described by two design choices, both of which are motivated by the goal of easing estimation:
\begin{enumerate}
		\item[(a)] It performs \textit{one} round of policy iteration.
		\item[(b)] It uses $q$-values that are \textit{decomposed} at the patient level.
\end{enumerate}	
We explain both of these design choices below.

\subsubsection{Design choice (a): A single iteration.}
Given a dataset $\mathcal{D}$ generated from running $\pi$, estimating $q_{it}^\pi(s, a)$ corresponds to a \textit{prediction} problem (also called on-policy evaluation).
Prediction is one of the simplest tasks in reinforcement learning, 
and there exist a myriad techniques that one can use for this task, such as Monte Carlo methods or temporal difference learning \citep{sutton2018reinforcement,szepesvari2022algorithms}.
Because of this significant existing literature on prediction, our work does not focus on analyzing this step.
In \cref{s.casestudy}, we provide an example of this step using a linear function approximation for the $q$ function.

However, to do \textit{more than one} round of policy iteration, one needs to learn $q^{\pi'}$ for a policy $\pi' \neq \pi$.
Estimating $q^{\pi'}$ from data generated from $\pi$ is an \textit{off-policy evaluation} problem, which is provably hard; \cite{wang2020statistical} show that the sample complexity of off-policy evaluation with a linear function approximation is \textit{exponential} in the time horizon.
Motivated by the hardness of off-policy evaluation, a recent work in the general offline RL literature, \cite{brandfonbrener2021offline}, also proposes the method of doing one-step of policy iteration from offline data and demonstrates strong empirical results of this approach.
Our work is complementary to the empirical findings of \cite{brandfonbrener2021offline}.

\subsubsection{Design choice (b): Decomposition.} \label{sss.designchoiceb}
The main difference between $\DPI$ and one iteration of the usual policy iteration (PI) approach is that $\DPI$ operates on the $q$-function, which is specific to an individual patient.
PI, in contrast, operates on the state space of the entire system MDP.
Specifically, for a state profile $\bS = (S_1, \dots, S_N)$ and action profile $\bA = (A_1, \dots, A_N)$, PI operates on the function $Q_{t}^\pi(\bS, \bA)$ defined as
\begin{align}  \label{eq:Q}
Q_{t}^\pi(\bS, \bA) = \bE_{\pi}\left[\sum_{t'=t}^T \sumN R(S_{it'}^\pi, S_{i,t'+1}^\pi, A_{it'}^\pi) \;\bigg|\; S_{it}^\pi = S_i, A_{it}^\pi = A_i \; \forall i \in [N] \right].
\end{align}


The advantage of the decomposition (using $q$ rather than $Q$) is in its ease of estimation.
Note that the size of the system state and action space is $|\cS|^N$ and $N \choose B$, respectively, whereas the size of the state and action space for a single patient is $|\cS|$ and 2, respectively. Then, if one used a tabular approach to estimating these functions, learning $Q_t$ requires estimating $|\cS|^N {N \choose B}$ quantities, while learning $q_{it}$ for every $i$ requires estimating $2|\cS|N$ quantities. The latter is an \textit{exponentially} fewer number of estimation tasks than the former.

The disadvantage of the decomposition is that since the individual $q$ functions do not take as input the entire system state, it loses information. That is, for a system state $\bS =(S_1, \dots, S_N)$ and action profile $\bA =(A_1, \dots, A_N)$, the sum of the $q$-values do not correspond to the $Q$-value: $\sum_{i=1}^N q_{it}^{\pi}(S_i, A_i)\neq Q_t^{\pi}(\bS, \bA)$.
This introduces a bias in the improvement step of policy iteration.
In Appendix~\ref{s.app.decompbias}, we describe an example of how this bias, $\sum_{i=1}^N q_{it}^{\pi}(S_i, A_i)- Q_t^{\pi}(\bS, \bA)$, can lead to an undesirable behavior. 
Therefore, the decomposition improves the ease of estimation, but comes at a cost of losing information.
In the next section, where we study a stylized model, we establish a performance guarantee based on the decomposed $q$ values.
This guarantee relies on the base policy, $\pi$, to be randomized, and the performance guarantee improves as the number of interventions that $\pi$ gives decreases.
Both of these features, randomization and a small number of interventions, aids in reducing the extent of impact of the bias. 
}






\section{Theoretical Guarantees \edit{for a Representative Stylized Case}} \label{s.theory}
\edit{The full model, presented in the previous section, describes the intended use case for $\DPI$. However, the generality of this model prohibits the ability to establish strong theoretical guarantees.
In this section, we therefore introduce a stylized but representative model of our problem setting, where we are surprisingly able to prove strong performance guarantees.
We first introduce this stylized model in \cref{ss.2statemodel} and describe the form of $\DPI$ in this setting in \cref{ss.simplepolicy}. We then present our performance guarantees in \cref{ss.perfguar} and additional robustness results in \cref{s.robustness}.}

\subsection{Two-State MDP Model} \label{ss.2statemodel}
The state space is $\cS = \{0, 1\}$, where 0 and 1 correspond to an undesired and desired state, respectively.
Under the null action ($A=0$), $p_i$ and $q_i$ represent the probability of transitioning from state 0 to 1 and 1 to 0, respectively.
The intervention ($A=1$) only changes the probability of transitioning from state 0 to 1, which becomes $p_i + \tau_i$.
We assume that $p_i, q_i \in [0, 1/2]$ for all $i$, implying that states are more likely to stay the same than change when there is no intervention.
We assume $\tau_i \in [0, 1-p_i]$ for all $i$, hence an intervention can only increase the probability of switching from state 0 to 1, up to a probability of 1. 
Lastly, the reward is simply equal to the resulting state; i.e.,\ $R_i(S, S', A) = S'$.
\edit{This represents the goal of maximizing the fraction of time that the patient is in the desired state.}
The MDP for one patient is specified by the parameters $(p_i, q_i, \tau_i)$, and this MDP is summarized in Figure~\ref{fig:2_state_mc}.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.55\linewidth]{figs/2_stateMDP_loops}
  \caption{MDP for patient $i$.}
  \label{fig:2_state_mc}
\end{center}
\end{figure}

\edit{
This is a special case of the full model from \cref{s.gmodel}, where we assume the patient MDP takes the simple form described above.
All other aspects of the model remain the same. In particular, the system MDP is derived by combining the patient MDPs via a budget constraint and outreach interventions are costly and must therefore be rationed. Note that the set of possible policies is finite, since both the state and action spaces are finite. Therefore, for any instance, there exists an optimal policy $\OPT$ which maximizes the objective, $\bE[\sumT \sumN S_{i,t+1}]$.

We believe this simplified version of the system is representative and relevant, both for our specific motivating application and behavioral health operations in general. From the perspective of Keheala (as described in \cref{s.intro}), the stylized model captures the salient features of patient behavior in the simplest way possible.
A dataset collected by an RCT run by Keheala revealed that the best single feature that predicted whether a patient will verify on a given day, is simply whether they verified the day before (80.9\% accuracy).
Therefore, the states in the model simply represent whether a patient verified the previous day.
Next, the main objective of the outreach interventions for Keheala is to target patients who have not been verifying, to encourage them change their behavior.
As a result, our model is such that the intervention only impacts the transition from state 0 to state 1.

More broadly, similar Markov models have been used in the literature to model patient behavior.
\cite{mate2020collapsing} and \cite{mate2022field} use the same two-state model and applied it to the setting of TB treatment adherence and maternal, health respectively. \cite{biswas2021learn} also studies maternal health in which they employ a similar MDP with three states. Even the objective function $\bE[\sumT \sumN S_{i,t+1}]$, which captures the 
platform's goal of having its users be in the `desired' state, has has been used in the literature \citep{mate2020collapsing,mate2022field,biswas2021learn}. This type of a two-state Markov model is originally from the literature on communication systems (where it is referred to as a Gilbert-Elliot channel \citep{gilbert1960capacity}) but has also been used in marketing \citep{schmittlein1987counting}. 
}


\subsection{The $\DPI$ Policy for the Two-State MDP Model} \label{ss.simplepolicy}
\edit{
Our main result pertains to $\DPI(\pi)$ for $\pi$ belonging to a class of policies.
Specifically, for $\gamma \in (0, 1)$, let $\RAND(\gamma)$ be the policy in which at every time step, gives an intervention to every patient in state 0 independently with probability $\gamma$.
Denote the intervention values of the policy $\RAND(\gamma)$ by $z^\gamma_{it} \triangleq z^{\RAND(\gamma)}_{it}$
We will provide a performance guarantee for the policy $\DPI(\gamma) \triangleq \DPI(\RAND(\gamma))$.

Our focus is in the regime where $\gamma$ is small (close to 0), and we will see that the theoretical guarantees are stronger when this is the case.
This represents a `budget-constrained' regime; the probability of a patient receiving an intervention is small, or equivalently, the number of interventions given is small relative to the number of patients.
To give intuition on the policy $\DPI(\gamma)$ in this regime, we provide a simple expression for the intervention value in the case where $\gamma \to 0$ and $T = \infty$.
\begin{proposition} \label{prop:z_clean_form}
Suppose $T = \infty$. Then, 
$\lim_{\gamma \to 0} z^{\gamma}_{it}(0) = \tau_i/(p_i + q_i)$ for any $i$ and $t$.
\end{proposition}



A more general version of \cref{prop:z_clean_form} is proven as \cref{lemma:zgamma} in Appendix~\ref{sec:coupling}.
\cref{prop:z_clean_form} says that when $T = \infty$ and $\gamma \to 0$, $\DPI(\gamma)$ orders patients in state 0 by the index $\tau_i/(p_i + q_i)$, and intervenes on those with the highest values.
The index increases with $\tau_i$ and decreases with $p_i$ and $q_i$, which is intuitive. 
$\tau_i$ is the `treatment effect' of an intervention on the probability that the patient switches from state 0 to 1.
The smaller the value of $q_i$, the longer the patient will stay in state 1 once they are there, increasing the `bang-for-the-buck' of an intervention.
As for $p_i$, a larger value means that a patient is likely to move to state 1 \textit{without} an intervention; hence lower priority is given to those with a large $p_i$.

\subsection{Performance Guarantee}\label{ss.perfguar}
Let $\NULL$ be the policy that only takes the null action; i.e.\ $\NULL_t(\bS) = (0, \dots, 0)$ for all $\bS, t$.
As a slight abuse of notation, we use $\OPT, \NULL, \RAND(\gamma)$ and $\DPI(\gamma)$ to also denote the expected reward of those policies.
We now state our main result, a performance guarantee for the policy $\DPI(\gamma)$.
\begin{theorem}[Main Result] \label{thm:approx_ratio_b}
Given an instance of the two-state model, let $\bar{M} = \max_{i \in [N]} \frac{\tau_i (1-p_i-q_i)}{(p_i + q_i)(1-p_i)} > 0$. For any $\gamma \in (0, 1)$,
\begin{align} \label{eq:approx_ratio_b}
\DPI(\gamma)  - \NULL \;\geq\; \frac{1}{2(1+\gamma \bar{M})} \; ( \OPT - \NULL ).
\end{align}	
\end{theorem}

\cref{thm:approx_ratio_b} provides an approximation guarantee for $\DPI(\gamma)$ with respect to the \textit{improvement} over the null policy.
The approximation ratio, $\frac{1}{2(1+\gamma \bar{M})}$ depends on two quantities: $\gamma$ is a parameter of the policy $\RAND(\gamma)$ which is used to compute the intervention values, and $\bar{M}$ is a quantity related to the instance parameters, $\{(p_i, q_i, \tau_i)\}_{i \in [N]}$.
For any fixed instance, the approximation ratio improves as $\gamma$ decreases, approaching 1/2 as $\gamma$ goes to 0.
Therefore, as $\gamma \to 0$, $\DPI(\gamma)$ achieves at least half of the improvement in reward as compared to that of the optimal policy.

With respect to $\bar{M}$, a smaller value implies a stronger guarantee.
Since we assume $p_i, q_i \leq 1/2$, $\bar{M} \leq \max_{i \in [N]} \frac{\tau_i}{p_i + q_i}$.
For $\bar{M}$ to be small, it must be that $p_i + q_i$ is bounded away from 0 for all $i$ --- specifically, patients must not be \textit{completely} sticky, where sticky means that they never switch their state, no matter which state they are in.
}

\cref{thm:approx_ratio_b} is significantly stronger than and implies the \textit{usual} notion of an approximation result, which is the following:
\begin{corollary}[Weaker result]  \label{thm:weaker_result}
For any problem instance of the two-state model,
$\OPT\leq \frac{1}{2(1+\gamma \bar{M})}\DPIs$.
\end{corollary}
Because a patient can transition from state 0 to 1 \textit{without} an intervention (if $p_i > 0$), the $\NULL$ policy can achieve a significant fraction of $\OPT$.
If it was the case that $\NULL$ is more than half of $\OPT$, then \cref{thm:weaker_result} would be vacuous.
On the other extreme, if an intervention was \textit{necessary} for all patients to transition to state 1
(i.e.\ $p_i = 0$ for all $i$), then it would be that $\NULL = 0$, in which case the result of \cref{thm:approx_ratio_b} is equivalent to that of \cref{thm:weaker_result}.

\subsection{Robustness under an Approximate Implementation} \label{s.robustness}
\edit{
Note that \cref{thm:approx_ratio_b} assumes that the intervention values are known exactly.
In practice, these intervention values must be estimated, as discussed in the intended usage in \cref{s.intended_usage}.
We show that this result is robust, in the sense that a policy that \textit{approximately} implements $\DPI(\lambda)$ also yields a performance guarantee.

Suppose $\ALG$ is an \textit{index policy} using indices $z^{\ALG}_{it}(0)$.
That is, $\ALG$ assigns interventions to the patients in state 0 with the largest value of $z^{\ALG}_{it}(0)$.
Then, we show that if the index values $z^{\ALG}_{it}(0)$ approximate the intervention values $z^{\lambda}_{it}(0)$, then $\ALG$ also admits a performance guarantee.

\begin{theorem} \label{corr:approx}
Suppose $\ALG$ is an index policy that uses index values $z^{\ALG}_{it}(0)$ that satisfies, for all $i$ and $t$,
\begin{align} \label{eq.approxvalues2}
\beta_1 z_{it}^{\lambda}(0) \leq z^{\ALG}_{it}(0) \leq \beta_2 z_{it}^{\lambda}(0),
\end{align}
for some $\beta_1 \leq 1$ and $\beta_2 \geq 1$.
Then,
\begin{align*}
\ALG - \NULL \;\geq\; \frac{\beta_1}{\beta_2} \cdot \frac{1}{2(1+\gamma \bar{M})} \; ( \OPT - \NULL ).
\end{align*}
\end{theorem}

The proof can be found in Appendix~\ref{sec:app:pf_approx}.
This result implies that one does not have to run $\DPI$ \textit{exactly} in order to achieve good performance. 
In practice, one may estimate the intervention values from data --- even if the estimation is not perfect, \cref{corr:approx} ensures a performance guarantee. 
}


