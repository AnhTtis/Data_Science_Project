
For most health conditions, long-term outcomes are determined not only by clinical interventions but also by individual habits and behaviors \citep{bosworth2011medication}. A range of behavioral health interventions, often delivered through digital platforms, have been shown to improve outcomes for various health conditions. These interventions aspire to affect users' habits through motivation, education, nudging, or boosting \citep{Ruggeri20Behavioral}. At the same time, such interventions are associated with a variety of direct and indirect costs. This paper is concerned with optimizing the impact of costly interventions through prioritization. 

As a concrete example of a digital health intervention, consider our partner organization, \textit{Keheala}. Keheala operates a digital service promoting medication adherence among patients prescribed with tuberculosis (TB) treatment in Kenya, and has already been shown to have a non-trivial impact on adherence and health outcomes \citep{Yoeli19Digital,Boutilier22Improving}. 
\edit{Increasing adherence to the prescribed course of treatment for TB is important since lack thereof can result in a relapse of the disease, and worse yet, the emergence of multi-drug resistant strains of the TB bacteria. A key feature of Keheala's adherence program is that patients are required to self-verify treatment adherence daily via a mobile phone interface. In addition, Keheala's digital platform comprises of a suite of interventions to further support adherence. Some are on-demand (e.g., leaderboards and information about TB) and some are automated (e.g., daily reminders to adhere to treatment). Most importantly, some are manual (e.g., outreach messages and phone calls). Keheala employs \textit{support sponsors} to operate the platform and reach out to patients who have not self-verified adherence for a predetermined amount of time. The interactions between the platform and the patients result in a feedback loop that presents an opportunity for Keheala to discern the impact of each type of intervention as well as to identify patients most in need of outreach.}



\edit{The Keheala case highlights three salient features that are common among digital services in the context of behavioral health services. 
\begin{enumerate}
	\item Interventions are costly and must be rationed. In general, even automated mobile reminders entail an indirect cost, in that frequent reminders are likely to sensitize patients to the automated messages. In the case of Keheala, calls from support sponsors entail a direct cost. Due to these costs, demand for support sponsor outreach exceeds supply. The support sponsors had a range of responsibilities but on an average day in our data (see more details in \cref{s.casestudy}) their effective capacity to make phone calls to patients was exceeded by the number of patients eligible for such phone calls by a factor of 8.
	\item The short-term metric of interest is a binary measure of compliance among users. For many behavioral health applications, a service provider (e.g., digital platform) collects data from users, aimed at identifying whether they are in a state of compliance with a desired behavior (e.g., medication adherence, exercise routine, correct diet) or not. In the case of Keheala, this binary outcome measure is the daily self-verification of treatment adherence. 
	\item Initially, limited data is collected using an ad-hoc baseline policy. Most digital behavioral health platforms are initially launched (e.g., as part of a pilot study) with heuristic rules of thumb guiding the timing of outreach to each user \citep{Mills20Personalized}. In Keheala's case, an RCT was conducted in which the protocol stated that patients who had not self-verified their adherence for 48 hours should be prioritized for support sponsor outreach. 
\end{enumerate}

This state of affairs prompts the key question we seek to answer: {\em Can one use limited pilot-study data, collected using some ad-hoc baseline policy, to design a practical intervention policy that maximizes the impact of costly interventions?} 
} 




\subsection{What can Reinforcement Learning Offer?}
\edit{Before diving into the development of a new approach to the problem, let us explore an out-of-the-box approach for this problem. 
Natural candidates for personalizing outreach interventions using data are reinforcement learning (RL) algorithms. 
}
As a prototype of the sorts of RL approaches that one might apply to intervention optimization, consider formulating the task at hand as a so-called \textit{contextual bandit}. Specifically, the context of a given patient is their adherence and intervention history so far, and the reward corresponds to whether or not they self-verify treatment adherence in the following day. This reward is assumed to be some unknown, noisy function of context and the chosen action (i.e., whether or not the patient received an intervention). 
Of course, a myopically optimal action may be sub-optimal, but let us ignore this limitation for now. 

Figure \ref{fig:introfig} compares the performance of a common contextual bandit algorithm, Thompson Sampling, with that of the existing heuristic employed by Keheala using a simulation model fit to data from Keheala's initial pilot.\footnote{The model itself was trained by cross-validation with double ML and further validated on a hold out set; see \cref{s.casestudy} for details on this as well as the bandit approach itself.} In parsing this figure, we note that the x-axis corresponds to the number of calls (and thus, also the amount of data that can be collected about the incremental impact of an intervention) in a single day. 
The operating point for the Keheala pilot corresponds to a budget of 26 interventions per day, as indicated by the yellow star.

\begin{figure}[h]
	\begin{center}
		\vspace{-2mm}
		\includegraphics[width=0.75\linewidth]{figs/results_Feb5_nodpi}
		\vspace{-2mm}
		\caption{Average overall verification rate over 50 runs for each policy and budget. The shaded region indicates a 95\% confidence interval. The star represents the operating point for Keheala.}
		\label{fig:introfig}
		\vspace{-6mm}
	\end{center}
\end{figure}

\edit{While we acknowledge that a contextual bandit is only one simple approach to our problem (we will include more sophisticated benchmarks in our numerical analysis in \cref{s.casestudy}) it is illustrative to highlight some of its shortcomings, relative to the criteria we had listed above. Setting aside the obvious issue that a contextual bandit relies on online exploration (whereas we would like to learn from offline data), we make three additional observations. 
First, in general we observe modest improvements. The bandit captures a similar verification rate as the incumbent Keheala policy with 3 fewer interventions per day (relative to the 26 used by Keheala). While meaningful, this is a somewhat marginal improvement.
Second, performance can actually deteriorate, relative to the baseline policy, particularly in the most resource-limited cases. As we further discuss in \cref{ss.results}, this is a data scarcity issue: with a small budget, the bandit simply does not collect enough exploration data to learn an improved policy. This is despite the fact that the Thompson sampling algorithm employed had the benefit of a prior learned on approximately half of the data. 
Third, the contextual bandit is (by design) a myopic approach. In essence, the bandit learns to provide outreach in such a way as to maximize the next day's self-verification rates, which may be highly suboptimal for the long term. While, we could certainly turn to a reinforcement learning algorithm that attempts to learn an optimal policy by measuring the long run impact of an action, the data requirements of such a policy would increase substantively so that the performance degradation we see for the bandit at low budgets is likely to persist at (much) higher budgets.
}

In summary, this leaves us in a peculiar spot---whereas more sophisticated RL algorithms could, in principle, learn an optimal policy, we typically do not have the data budget, or the willingness to risk policy degradation due to exploration, to use such approaches. However, simpler RL algorithms (such as the contextual bandit) provide a relatively marginal improvement. 




\subsection{This Paper}
\edit{In this paper, we propose and evaluate a new algorithm for intervention optimization problems of the type described above. 

Our first contribution is the algorithm itself (\cref{s.model}). Our starting point is a formal model of the rich practical setting of interest where we model patient behaviors using a generic  Markov Decision Process (MDP)---we refer to this as the `full model'. We propose a new approach---inspired by existing techniques---for intervention optimization problems we dub \textit{Decomposed Policy Iteration $(\DPI)$}. Loosely, $\DPI$ can be thought of as approximating a single step of policy iteration in the high dimensional MDP corresponding to our problem. 
Taking as input a dataset of interventions and outcomes under an incumbent policy, 
we estimate the state-action values ($q$-values) of this policy using a prediction algorithm.
Then, at each time step, interventions are assigned to the patients who have the largest increase in their $q$-value if they receive an intervention, compared to if they do not.
Notably, the $q$-values are decomposed at the patient level, which avoids any dependence on an exponentially sized state space.
$\DPI$ is a promising practical solution to intervention optimization problems as it is model-free (i.e., does not require access to a model of the environment) and hence it does not rely on positing a particular behavioral model. 
It also does not rely on online experimentation or updating, as is the case for many RL algorithms.



Our second contribution is to provide surprisingly strong performance guarantees for $\DPI$ when applied to a stylized version of the full problem (\cref{s.theory}). In general, a single step of policy iteration does not provide a meaningful performance guarantee. However, patient dynamics with respect to habitual behavior are not arbitrary. We therefore study the performance of our algorithm when patient behavior is driven by a standard simple model\footnote{We emphasize that $\DPI$ is a generic model-free algorithm that can be used irrespective of the underlying patient behavior model. The behavioral model is used only for the theoretical analysis.} akin to models considered both in the behavioral health \citep{mate2020collapsing,mate2022field,biswas2021learn} and marketing literature \citep{schmittlein1987counting}. For situations in which user behavior is driven by such models, we establish an approximation ratio of the {\em improvement} between an optimal policy and a null policy that does not allocate interventions. Notably, this result is much stronger than a typical approximation guarantee, which compares the \textit{absolute} performance of a policy to the optimal policy. 
Specifically, when the baseline policy used to the collect the data is randomized, the approximation ratio of the improvement approaches $1/2$ as the intervention capacity decreases.
Since the quantities used by $\DPI$ need to be estimated, we prove that our guarantee is robust in the sense that the performance guarantee scales gracefully with respect to errors in these quantities.

Our third contribution is to return to the full model and provide numerical results based on field data (\cref{s.casestudy}). Using the validated simulation setup described in Figure~\ref{fig:introfig}, we find that $\DPI$ would yield the same verification rate as the incumbent Keheala policy {\em with less than half the capacity}. Furthermore, $\DPI$ outperforms the contextual bandit algorithm (described above) for budgets ranging from 50\% to 150\% of the original capacity. Importantly, the performance gains are highest in low capacity scenarios---the most likely paradigm for a future large-scale roll-out of the system in resource-limited settings. This is encouraging for digital behavioral health applications, more generally---particularly given how well the policy performs despite only having access to limited data collected during pilot implementation. 
}












