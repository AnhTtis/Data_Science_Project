\documentclass[nonblindrev]{workingpaperJJB} %

\usepackage[normalem]{ulem}
\usepackage{natbib}
\usepackage{amsmath,bbm}
\usepackage[capitalise]{cleveref}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%
 \usepackage{bibspacing}
 \setlength{\bibitemsep}{.0\baselineskip plus .05\baselineskip minus .05\baselineskip}
 
 \setlength{\pdfpagewidth}{8.5in}
 \setlength{\pdfpageheight}{11in}

 \TheoremsNumberedThrough     %
 \ECRepeatTheorems

 \EquationsNumberedThrough    %


\usepackage{amsfonts}
\usepackage{amsmath}  
\usepackage{amssymb}
\usepackage{changepage}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{color}
\usepackage{filecontents}
\usepackage{subcaption}
\usepackage[customcolors]{hf-tikz}
\usepackage{colortbl}
\usepackage{threeparttable}


\usepackage{algorithm}%
\usepackage{algpseudocode}%

\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}

\newcommand{\added}[1]{{\leavevmode#1}}


\input{commands.tex}
\NewDocumentEnvironment{myproof}{o}
  {\IfNoValueTF{#1}{\paragraph{{Proof.} }} {\paragraph{{#1.} }} }
  {\hfill$\Halmos$}


\begin{document}

 \RUNAUTHOR{Baek et al.}

\RUNTITLE{Policy Optimization for Personalized Interventions}

\EquationsNumberedThrough    %



\TITLE{
Policy Optimization for Personalized Interventions in Behavioral Health}
\ARTICLEAUTHORS{
\AUTHOR{Jackie Baek}\AFF{Stern School of Business, New York University, \EMAIL{baek@stern.nyu.edu}}
\AUTHOR{Justin J. Boutilier}\AFF{Telfer School of Management, University of Ottawa, \EMAIL{boutilier@telfer.uottawa.ca}}
\AUTHOR{Vivek F. Farias}\AFF{Sloan School of Management, Massachusetts Institute of Technology, \EMAIL{vivekf@mit.edu}}
\AUTHOR{J\'{o}nas Oddur J\'{o}nasson}\AFF{Sloan School of Management, Massachusetts Institute of Technology, \EMAIL{joj@mit.edu}}
\AUTHOR{Erez Yoeli}\AFF{Sloan School of Management, Massachusetts Institute of Technology, \EMAIL{eyoeli@mit.edu}}
}
\HISTORY{\today}


\ABSTRACT{




Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize a long-term outcome, where interventions are costly and capacity-constrained. We assume we have access to a historical dataset collected from an initial pilot study. 
We present a new approach for this problem that we dub $\DPI$, which decomposes the state space for a system of patients to the individual level and then approximates one step of policy iteration.
Implementing $\DPI$ simply consists of a prediction task using the dataset, alleviating the need for online experimentation.
$\DPI$ is a generic model-free algorithm that can be used irrespective of the underlying patient behavior model.
We derive theoretical guarantees on a simple, special case of the model that is representative of our problem setting.
When the initial policy used to collect the data is randomized, we establish an approximation guarantee for $\DPI$ with respect to the \textit{improvement} beyond a null policy that does not allocate interventions. We show that this guarantee is robust to estimation errors. We then conduct a rigorous empirical case study using real-world data from a mobile health platform for improving treatment adherence for tuberculosis.
Using a validated simulation model, we demonstrate that $\DPI$ can provide the same efficacy as the status quo approach with approximately \textit{half} the capacity of interventions.
$\DPI$ is simple and easy to implement for an organization aiming to improve long-term behavior through targeted interventions, and this paper demonstrates its strong performance both theoretically and empirically, particularly in resource-limited settings.
}

\KEYWORDS{Health Analytics, Policy Optimization, Reinforcement Learning, Global Health, Behavioral Health, Tuberculosis}
\HISTORY{\today}

\maketitle

\section{Introduction}\label{s.intro}
\input{sections/introduction.tex}


\section{Literature Review}\label{s.litreview}
\input{sections/lit_review.tex}

\section{Full Model and Policy}\label{s.model}
\input{sections/model.tex}


\section{Case Study: TB Treatment Adherence}\label{s.casestudy}
\input{sections/case_study}

\section{Conclusions, Implications, and Future Directions}\label{s.conclusion}
\input{sections/conclusion}


\ACKNOWLEDGMENT{The authors are grateful to Jon Rathauser, founder and CEO of Keheala, for the collaboration. The authors also thank the Keheala Operational Team (Alice, Faith, Edwin, Jacinta, Jill, Lewis, Moreen, Trish).}

\vspace*{-0.5em}
\bibliographystyle{ormsv080}
\bibliography{Keheala}

\clearpage

\setcounter{page}{1} \renewcommand{\theequation}{A\arabic{equation}}
\setcounter{equation}{0}
\renewcommand{\thelemma}{A\arabic{lemma}}
\setcounter{lemma}{0}
\renewcommand{\theproposition}{A\arabic{proposition}}
\setcounter{proposition}{0}
\renewcommand{\thesection}{A\arabic{section}}
\setcounter{section}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\theremark}{A\arabic{remark}}
\setcounter{remark}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}

\section*{E-companion for ``Policy Optimization for Personalized Interventions in Behavioral Health''}

~

\section{Table of Notation} \label{s.app.tablenotation}

\begin{table}[h]
\TableSpaced %
\caption{
\added{List of notation used in the paper and the proofs in \cref{s.app.proof}.}
} \label{tab.notation}
\vspace{2mm}
\begin{center}
\begin{tabular}{@{}cl@{}}
\toprule
Notation & Meaning\\ \midrule
$N$ & Number of patients \\ 
$T$ & Number of time steps \\ 
$B$ & Budget \\ 
$\cS$ & State space for the MDP of a single patient\\ 
$\cA$ & Action space for the MDP of a  single patient\\ 
$\cA^N$ & Action space for the system MDP \\ 
$\mathcal{M}_i$ & MDP for patient $i$ \\
$\mathcal{P}_i(S, S', A)$ & Transition probability function for patient $i$ \\
$R_i(S, S', A)$ & Reward function for patient $i$ \\
$S_{it}^\pi$ & State of patient $i$  at time $t$ under policy $\pi$ \\
$A_{it}^\pi$ & Action for patient $i$  at time $t$ under policy $\pi$ \\
$q_{it}^{\pi}(S, A)$ & Patient-level $q$-value defined in \eqref{eq:q} \\
$\bS, \bA$ & Vector of states and actions for the system-level MDP \\
$Q_{t}^\pi(\bS, \bA) $ & System-level $Q$-value defined in \eqref{eq:Q} \\
$z_{it}^{\pi}(S)$ & Intervention value used for $\DPI(\pi)$ \\
$p_i, g_i, \tau_i$ & Parameters for the two-state model of \cref{ss.2statemodel} \\
$\gamma$ & Probability of an intervention for the policy $\RAND(\gamma)$ \\
$\bar{M}$ & Instance-dependent parameter used in \cref{thm:approx_ratio_b} \\
$V_{it}$ & Whether patient $i$ verifies at time $t$ \\
$P_{it}, G_{it}, K_{it}, W_{it}$ & Bernoulli variables used in the sample path coupling of \cref{sec:coupling} \\
$\bar{z}_{it}(S)$ & Null intervention values defined as $\lim_{\gamma \to 0^+} z_{it}^{\gamma}(S)$ \\
$\DPI_0$ & Index policy that ranks patients by the null intervention values \\
$I_{t}$ & Subset of patients in state 0 at time $t$ \\
$D_t(I)$ & Subset of patients that $\DPI_0$ would choose out of $I \subseteq [N]$ \\
$\ONE(i, t)$ & Policy that chooses patient $i$ once at time $t$ \\
$\tilde{S}_{ir|t}$ & State of patient $i$ at time $r$ under $\ONE(i, t)$ \\
$Z_{it}$ & Number of times state of patient $i$ differ in $\ONE(i, t)$ compared to $\NULL$, defined in \eqref{eq:define_Y} \\
$\nu_i(t)$ & Last time before $t$ that $i$ was in state 0, defined in \eqref{eq:definetau} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Proof of \cref{thm:approx_ratio_b} and \cref{corr:approx}}  \label{s.app.proof}
\input{sections/appendix_modified}


\section{Proofs for \cref{ss.general_guarantees}} \label{s.app.decompbias}
\input{sections/decomp_bias}

\section{Synthetic Simulations} \label{s.app.synthetic simulations}
\input{sections/app_synthetic_sims}


\section{Details on Keheala Case Study} \label{s.app.keheala}
\input{sections/appendix_case_study}





\end{document}