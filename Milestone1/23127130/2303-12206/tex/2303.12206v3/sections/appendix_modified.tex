
We first prove \cref{thm:approx_ratio_b}, where we break down the proof into four steps, where the bulk of the proof lies in the first two steps.
The proof of \cref{corr:approx} follows from steps 3 and 4, which we describe in \cref{sec:app:pf_approx}.
The proofs of all of the steps make use a specific sample path coupling procedure which we describe in the next subsection.

\paragraph{Step 1.}
First, we define \textit{null intervention values} as $\nz_{it}(s) = \lim_{\gamma \to 0^+} z^{\gamma}_{it}(s)$. 
We show that for any algorithm $\ALG$, the difference $\ALG - \NULL$ can be written as a sum of the null intervention values of the patients that were chosen.
\begin{proposition} \label{prop:alg_minus_base}
For any algorithm $\ALG$,
\begin{align} \label{eq:diff_char}
\ALG - \NULL = \bE\left[\sumT \sum_{i \in A^{\ALG}_t} \nz_{it}(0) \right].
\end{align}
\end{proposition}

\paragraph{Step 2.}
Then, we define the policy $\DPI_0$ to be the policy which orders patients in state 0 with respect to the null intervention values, $\nz_{it}(0)$, gives interventions to the $B$ patients with the highest values.
We show that this policy achieves at least half of the optimal improvement over $\NULL$.
\begin{proposition} \label{prop:dpinull_half_result}
For any instance of the two-state model,
\begin{align} \label{eq:approx_ratio_b}
\DPI_0  - \NULL \;\geq\; \frac{1}{2} \; ( \OPT - \NULL ).
\end{align}	
\end{proposition}

\paragraph{Step 3.}
Next, we show that if a policy uses an index rule with indices that approximate the null intervention values, this policy also yields a performance guarantee. 
\begin{proposition} \label{prop:approx_index}
Fix $\alpha_1 \in (0, 1]$ and $\alpha_2 \geq 1$.
Let $\ALG$ be an index policy that uses indices $z^{\ALG}_{it}(0)$ that satisfy $\alpha_1 \nz_{it}(0) \leq z^{\ALG}_{it}(0) \leq \alpha_2 \nz_{it}(0)$ for all $i$ and $t$. Then,
\begin{align} \label{eq:approx_ratio_b}
\ALG  - \NULL \;\geq\; \frac{\alpha_1}{2\alpha_2} \; ( \OPT - \NULL ).
\end{align}	
\end{proposition}

\paragraph{Step 4.}
Lastly, we show that the intervention values $z^{\gamma}_{it}(0)$ satisfy the above, where the corresponding $\alpha$ is a function of both $\gamma$ and the underlying parameters.
\begin{proposition} \label{lemma:rand_null}
Fix a patient $i$, and let $M_i = \frac{\tau_i (1-p_i-g_i)}{(p_i + g_i)(1-p_i)} > 0$.
For any $t \in [T]$, the intervention values $z^{\gamma}_{it}(0)$ and $\nz_{it}(0)$ satisfy the following relationship:
\begin{align}
\frac{1}{1 + \gamma M_i} \nz_{it}(0) \; \leq \; z^{\gamma}_{it}(0) \; \leq \; \nz_{it}(0).
\end{align}
\end{proposition}

Using \cref{lemma:rand_null}, we can apply \cref{prop:approx_index} using $\alpha_1 = 1/(1+\gamma M)$ and $\alpha_2 = 1$ for $M = \max M_i$, which completes the proof of \cref{thm:approx_ratio_b}.
The proofs of Propositions \ref{prop:alg_minus_base}-\ref{lemma:rand_null} can be found in Sections \ref{s.app.p1}-\ref{s.app.p4}.

\paragraph{Terms and notation.} We define a couple of terms and notation that we will use for the proofs.
We say that a patient is `chosen' to mean that the patient received an intervention, and we say that a patient is `available' to mean they are in state 0.
Let $I_t \subseteq [N]$ be the set of patients that are in state  0 at time $t$. Then, $[N] \setminus I_t$ are the patients in state 1 at time $t$.
We use $A_t \subseteq [N]$ to refer to the subset of patients who are chosen (instead of using the notation $A_{it} \in \{0, 1\}$ to denote whether patient $i$ is chosen).
Without loss of generality, we assume that $A_t \subseteq I_t$. 
This is because giving an intervention to a patient in state 1 has no impact on their transitions.
We put $\pi$ in the superscript, $I^\pi_t$ and $A^\pi_t$, to refer to the random variables induced by running policy $\pi$.

We now describe the sample path coupling, and then prove each of the above four propositions in the following subsections.

\subsection{Sample path coupling} \label{sec:coupling}
Fix $\gamma \in (0, 1]$.
We specify a new set of model dynamics that couples different policies through shared random variables.
We will show that these new dynamics are equivalent to the original dynamics specified in \cref{ss.2statemodel}.

\noindent
\textbf{New dynamics.}
At each time $t=1, \dots, T-1$, the following occurs:
\begin{enumerate}
	\item States $S_{it}$ are observed for all patients $i$.
	\item A policy selects a subset of patients $A_{t} \subseteq I_t$ for an intervention.
	\begin{itemize}
		\item For the policy $\RAND(\gamma)$, draw $W_{it} \sim \Bern(\gamma)$ independently for each patient $i$.
		Then, $i \in A_t$  if and only if $S_{it} = 0$ and $W_{it} = 1$.
	\end{itemize}
	\item For every patient $i \in [N]$, draw independent, Bernoulli variables
	$G_{it} \sim \Bern(g_i / (1-p_i))$, $P_{it} \sim \Bern(p_i)$, $K_{it} \sim \Bern(\tau_i/(1-p_i))$.
	\item Patient transitions occur in the following order:
\begin{enumerate}[label=(\roman*)]
	\item \textbf{Return to state 0:} For each patient $i \in I_t(1)$ and $G_{it} = 1$, the patient returns to state 0.
	Let $I_t' = I_t \cup \{i : G_{it} = 1, i \in I_t(1)\}$ be the new set of patients in state 0.
	\item \textbf{Passive transitions to state 1:}  For each patient $i \in I_t'$ and $P_{it} = 1$, the patient goes to state 1.
	Let $I_t'' = I_t' \setminus \{i : P_{it} = 1, i \in I_t'\}$ be the new set of patients in state 0.
	\item \textbf{Active transitions to state 1:}  For each patient $i \in I_t'' \cap A_{t}$ with $K_{it} = 1$, the patient goes to state 1.
	Then, $I_{t+1} = I_t'' \setminus \{i : K_{it} = 1, i \in I_t'' \cap A_{t}\}$ are the set of patients in state 0 at the next time step, and $I_{t+1}(1) = [N] \setminus I_{t+1}$.
\end{enumerate}
	\item We collect the reward $I_{t+1}(1)$.
\end{enumerate}

\textbf{Equivalence.}
We claim that the original model dynamics (from \cref{ss.2statemodel}) is equivalent to these new model dynamics.
To show this, we need to show that the transition probabilities between states are equal under both models.

In the new model, suppose a patient is in state 0.
If they were not chosen, they can only transition to 1 under step (ii), which occurs with probability $p_i$.
If they were chosen, they can transition to 1 under either step (ii) or (iii).
In total, they transition with probability $p_i + (1-p_i) \cdot \tau_i/(1-p_i) = p_i + \tau_i$.
Next, suppose a patient is in state 1.
For the patient to transition to state 0, it must be that $G_{it} = 1$ and $P_{it} = 0$.
This occurs with probability $g_i / (1-p_i) \cdot (1-p_i) = g_i$.
Therefore, the transition probabilities between the two models are equal for all patients.

\textbf{Coupling sample paths of policies.} 
From now on, we assume that all policies are coupled through the variables $P_{it}, G_{it}, K_{it}$.
This coupling immediately gives us the following properties.

\begin{lemma} \label{prop:coupling_property0}
For any policy, if $P_{it} = 1$, then $S_{i, t+1} = 1$.
\end{lemma}

\begin{myproof}
Suppose $P_{it} = 1$ for some $i$ and $t$.
In step 4(ii) of the new dynamics, the set $I_t''$ is defined so that $i \in I_t''$.
In step 4(iii), $I_{t+1}$ is a subset of $I_t''$,  and $I_{t+1}(1) = [N] \setminus I_{t+1}$.
Hence $S_{i, t+1} = 1$.
\end{myproof}

The next property says that if a patient is in state 1 under the $\NULL$ policy, they must also be in state 1 under any other policy.

\begin{lemma} \label{prop:coupling_property}
Let $\ALG$ be any policy. If $S_{it}^{\NULL} = 1$, then $S_{it}^{\ALG} = 1$.
Equivalently, if $S_{it}^{\ALG} = 0$, then $S_{it}^{\NULL} = 0$.
\end{lemma}

\begin{myproof} %
Let $i, t$ be such that $S_{it}^{\NULL} = 1$.
Let $t' = \max\{t' < t : P_{it'} = 1\}$ be the most recent time $P_{it'}$ was 1.
Since $S_{it}^{\NULL} = 1$, $G_{is} = 0$ for every $s \in \{t'+1, \dots, t-1\}$.
Since $P_{it'} = 1$, \cref{prop:coupling_property0} implies $S^{\ALG}_{i, t'+1} = 1$.
Since $G_{is} = 0$ for every $s \in \{t'+1, \dots, t-1\}$, $S_{it}^{\ALG} = 1$.
\end{myproof}

Lastly, using this sample path coupling, we can write an expression for the intervention values $z^{\gamma}_{it}(0)$ and $\nz_{it}(0)$.
\begin{lemma} \label{lemma:zgamma}
The intervention value with respect to $\RAND(\gamma)$ can be written as 
\begin{align} \label{eq:zformula}
z^{\gamma}_{it}(0) = \tau_i \cdot \bE[\min\{\text{Geometric}(p_i + g_i + \gamma \tau_i (1-p_i-g_i)/(1-p_i)), T-t+1\}].
\end{align}
Moreover, the null intervention value can be written as
\begin{align} \label{eq:nzformula}
\nz_{it}(0) = \tau_i \cdot \bE[\min\{\text{Geometric}(p_i + g_i), T-t+1\}].
\end{align}
\end{lemma}


\begin{myproof}[Proof of \cref{lemma:zgamma}]
Recall that $z^{\gamma}_{it}(0) = g_{it}^{\RAND(\gamma)}(0, 1) - g_{it}^{\RAND(\gamma)}(0, 0)$.
Let $E_0 = \{S_{it} = 0, A_{it} = 0\}$ be the event where patient $i$ is in state 0 at time $t$ and $\RAND(\gamma)$ does not choose the patient and let $E_1 = \{S_{it} = 0, A_{it} = 1\}$ be the event where patient $i$ is in state 0 at time $t$ and $\RAND(\gamma)$ does choose the patient.

Conditioned on either $E_0$ or $E_1$, the distribution of the variables $W_{it'}$ for $t' > t$ and  $P_{it'}, G_{it'}, K_{it'}$ for $t' \geq t$ are the same, due to independence.
Therefore, to compute $z^{\gamma}_{it}(0)$, we consider two hypothetical sample paths for patient $i$, one where $E_0$ holds and one where $E_1$ holds, but where all future variables are exactly the same.
We refer to $S^0_{it'},S^1_{it'} \in \{0, 1\}$ as the states under the two sample paths respectively at time $t' > t$.
Let $\zeta = \min\{T+1, \min\{t' > t: S^0_{it'} = S^1_{it'}\}\}$ be the first time after time $t$ that the two states converge, where $\zeta = T+1$ if they never converge. 
The states will always be the same after time $\zeta$ due to the sample path coupling.
Then, $z^{\gamma}_{it}(0) = \bE[\zeta] - t - 1$.

We now write an expression for $z^{\gamma}_{it}(0)$.
For the states to differ at time $t+1$, it must be that $P_{it} = 0$ and $K_{it} = 1$.
After that, the states will converge at time $t'+1$ if (i) $P_{it'} = 1$,  (ii) $G_{it'} = 1$, or (iii) $W_{it'} = 1$ and $K_{it'} = 1$.
Let $\Gamma = \min\{t' > t : P_{it'} = 1 \} - t$ be the length of time from $t$ until $P_{it'} = 1$.
Let $\Gamma' = \min\{t' > t : G_{it'} = 1 \} - t$ be length of time from $t$ until $G_{it'} = 1$.
Let $\Gamma'' = \min\{t' > t : W_{it'} = 1, K_{it'} = 1 \} - t$ be length of time from $t$ until $W_{it'} = 1$ and $K_{it'} = 1$.
Then, $z^{\gamma}_{it}(0)$ can be written as
\begin{align*}
z^{\gamma}_{it}(0) = \Pr(K_{it} = 1) \cdot \Pr(P_{it} = 1) \cdot \bE[\min\{\Gamma, \Gamma', \Gamma'', T-t+1\}].
\end{align*}
The term $\min\{\Gamma, \Gamma', \Gamma''\}$ is a geometric random variable with parameter 
\begin{align*}
&1-(1-\Pr(P_{it} = 1))(1-\Pr(G_{it} = 1))(1-\Pr(W_{it} = 1, K_{it} = 1)) \\
=& 1-(1-p)(1-g/(1-p))(1-\gamma \tau / (1-p)) \\
=& p + g + \gamma \tau (1-p-g)/(1-p).
\end{align*}
Therefore,
\begin{align*} 
z^{\gamma}_{it}(0) = \tau \cdot \bE[\min\{\text{Geometric}(p + g + \gamma \tau (1-p-g)/(1-p)), T-t+1\}].
\end{align*}
\cref{eq:nzformula} follows from taking the limit of the above as $\gamma \to 0$, using the dominated convergence theorem.
\end{myproof}



\subsection{Step 1: Proof of \cref{prop:alg_minus_base}} \label{s.app.p1}
We start with analyzing the left-hand side, $\ALG - \NULL$.
\cref{prop:coupling_property} says that whenever $S^{\ALG}_{it} = 0$, it must be that $S^{\NULL}_{it} = 0$.
Therefore, $\ALG-\NULL$ can be written as the number of times when $S^{\ALG}_{it} = 1$ while $S^{\NULL}_{it} = 0$:
\begin{align}
\ALG - \NULL = \bE\bigg[\sumT \bI(S^{\ALG}_{it'} =1, S^{\NULL}_{it'} = 0) \bigg]
\end{align}

Due to the sample path coupling, $S_{it}^{\ALG} \neq S_{it}^{\NULL}$ can only occur if
$\ALG$ chose patient $i$ at a prior time step, and the states have been different since then (if the states converged, it will stay the same unless $\ALG$ chose the patient again).
Therefore, each time $\bI(S^{\ALG}_{it'} =1, S^{\NULL}_{it'} = 0)$ occurs, it is associated with an intervention by $\ALG$ at a previous time step.
Hence, we will instead represent $\ALG-\NULL$ by summing over all interventions given by $\ALG$, and relating each intervention to how long the states $S^{\ALG}_{it'}$ and $S^{\NULL}_{it'}$ deviate.

\textbf{Defining counterfactual state.}
To formalize this notion, we need to define a couple of new quantities that will play a important role in both this step and step 2 of the proof.
We define a policy $\ONE(i, t)$ to be the same as the $\NULL$ policy, except that it chooses patient $i$ once at time $t$.
Define $\tilde{S}_{i r | t} = S^{\ONE(i, t)}_{ir}$ to be the state of patient $i$ at time $r$ under this policy, which we call the \textit{counterfactual state}.
Then, let $Z_{it}$ be the number of times that the counterfactual state is not equal to the state under $\NULL$:
\begin{align} \label{eq:define_Y}
Z_{it} &= |\{t' \in [T] \; : \; S_{it'}^{\NULL} \neq \tilde{S}_{it' | t} \}|.
\end{align}

Note the following properties:
\begin{itemize}
	\item The two states are always equal before time $t$.
	($S_{it'}^{\NULL} =\tilde{S}_{it' | t}$ for any $t' \leq t$.)
	\item Once the two states converge at some time $t' > t$, they will never diverge again since the policies are the same. (If $S_{it'}^{\NULL} =\tilde{S}_{it' | t}$ for some $t' > t$, then the same holds for any $r > t'$.)
	\item The only way that the states can be different is if the state is 0 under $\NULL$ and 1 under $\ONE(i, t)$, due to \cref{prop:coupling_property}.
\end{itemize}
Therefore, $Z_{it}$ represents exactly the increase in total reward from patient $i$ caused by the intervention at time $t$, compared to $\NULL$.
More specifically, when $S^{\NULL}_{it} = 0$, $Z_{it}$ is the number of time steps that the patient was in state 1 right after time $t$, before it transitioned back to state 0 or the state under the $\NULL$ policy also moved to state 1 (or we reached the last time step).
The above logic allows us to write out an expression for the expected value of $Z_{it}$, conditioned on $S^{\NULL}_{it} = 0$.

\begin{lemma} \label{lemma:zequalsy}
$\bE\big[Z_{it} \;|\; S^{\NULL}_{it} = 0 \big] = \tau_i \cdot \bE[\min\{\text{Geometric}(p_i + g_i, T-t+1\}] = \nz_{it}(0)$.
\end{lemma}

Now, we can write $\ALG-\NULL$ as the sum of $Z_{it}$ values at the times when $i$ was chosen:
\begin{align}
\ALG - \NULL 
&= \bE\bigg[\sumT \sum_{i \in A_t^{\ALG}} Z_{it}\bigg] \label{eq:propRandom} \\
&= \sumT \sum_{i \in [N]} \bE\big[\bI(i \in A_t^{\ALG}) Z_{it}\big] \nonumber \\
&= \sumT \sum_{i \in [N]} \bE\big[\bI(i \in A_t^{\ALG})\big]\; \bE\big[Z_{it} \;|\; i \in A_t^{\ALG}\big]  \nonumber \\
&= \sumT \sum_{i \in [N]} \bE\big[\bI(i \in A_t^{\ALG})\big] \nz_{it}(0) \nonumber \\
&= \bE\bigg[\sumT \sum_{i \in A_t^{\ALG}}  \nz_{it}(0) \bigg], \label{eq:propDet} 
\end{align}
as required.


\begin{myproof}[Proof of \cref{lemma:zequalsy}]
Let $\Gamma_{it} = \min\{t' > t : P_{it'} = 1 \} - t$ be the length of time from $t$ until $P_{it'} = 1$.
Let $\Gamma'_{it} = \min\{t' > t : G_{it'} = 1 \} - t$ be length of time from $t$ until $G_{it'} = 1$.
Then, 
\begin{align} \label{eq:Z_explicit}
Z_{it} = \bI(S_{it}^\NULL = 0, P_{it} = 0, K_{it} = 1) \min\{\Gamma_{it}, \Gamma'_{it}, T-t+1\}.
\end{align}
The indicator represents the fact that $i$ would not transition to state 1 under $\NULL$ ($P_{it} = 0$), but it would transition under $\ONE(i, t)$ ($K_{it} = 1$).
That is, at time $t+1$, the patient is in state 1 under $\ONE$ but in state 0 under $\NULL$.
The $\min\{\Gamma_{it}, \Gamma'_{it}\}$ term counts how long this is the case.
This could end either because patient transitions to state 1 under $\NULL$ (captured by $B_{it}$), or it could be that the patient in $\ONE$ transitions back to state 0 (captured by $L_{it}$).

Note that $\min\{\Gamma_{it}, \Gamma'_{it}\}$ is a geometric random variable with parameter $1-(1-\Pr(P_{it} = 1))(1-\Pr(G_{it} = 1)) = p_i+g_i$.
Therefore,
\begin{align*}
\bE[Z_{it} \;|\; S_{it}^\NULL = 0] 
&= \Pr(P_{it} = 0) \cdot \Pr(K_{it} = 1) \; \bE[\min\{\text{Geometric}(p_i+g_i), T-t+1\}] \\
&= \tau_i \cdot \bE[\min\{\text{Geometric}(p_i+g_i), T-t+1\}].
\end{align*}
Note that the above expression is the same as the one for $\nz_{it}(0)$ from \cref{lemma:zgamma}.
\end{myproof}





\subsection{Step 2: Proof of \cref{prop:dpinull_half_result}}
We prove a more general and stronger version of \cref{prop:dpinull_half_result}, which we state as \cref{thm:step2stronger}.

Define the policy $\DPI_0$ to be the policy which orders patients in state 0 with respect to the null intervention values, $\nz_{it}(0)$, gives interventions to the $B$ patients with the highest values.
Denote by $D_t(I_t^{\ALG}) \subseteq I_t^{\ALG}$ the subset of patients that $\DPI_0$ \textit{would choose} out of $I_t^{\ALG}$, the $B$ patients with the highest null intervention values, $\nz_{it}(0)$.
The next result shows that the sum of null intervention values of the patients in $D_t(I_t^{\ALG})$, will lead to at least half of total sum intervention values for an optimal policy.
\begin{proposition} \label{thm:step2stronger}
For any $\ALG$, 
\begin{align} \label{eq.main_result}
\OPT - \NULL \leq 2 \bE\left[ \sumT \sum_{i \in D_t(I_t^{\ALG})} \yit(0) \right].
\end{align}
\end{proposition}
Note that if $\ALG = \DPI_0$, then $D_t(I_t^{\ALG})$ is simply the patients that $\DPI_0$ chooses at time $t$; in that case, the RHS of \eqref{eq.main_result} equals the RHS of \eqref{eq:diff_char}.
Then, by \cref{prop:alg_minus_base}, the RHS of \eqref{eq.main_result} equals $\DPI_0 - \NULL$, which corresponds exactly to the statement of \cref{prop:dpinull_half_result}.
Hence, \cref{thm:step2stronger}  implies that $\DPI_0$ achieves at least half of the optimal improvement over $\NULL$.


We now prove \cref{thm:step2stronger}.
This proof makes use of the quantity $Z_{it}$, which was defined in the proof of \cref{prop:alg_minus_base}.
Fix any policy $\ALG$.
Recall that $I^{\ALG}_t$ and $I^*_t$ are the set of patients that are in state 0 under $\ALG$ and $\OPT$ respectively at time $t$.
Additionally, $D_t(I_t^{\ALG}) \subseteq I^{\ALG}_t$ are the patients that $\DPI_0$ would choose, out of patients in $I^{\ALG}_t$.
We first decompose the rewards based on whether or not a patient that was chosen in $\OPT$ was available to be chosen under $\ALG$.
\begin{align}
\OPT - \NULL
&= \bE\left[ \sumT \sum_{i \in A^*_t} Z_{it} \right]\nonumber \\
&= \bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} Z_{it} \right]
+ \bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Z_{it} \right].
\label{eq:opt_minus_base_decomp}
\end{align}

For the first term in \eqref{eq:opt_minus_base_decomp},
by definition of $D_t(I_t^{\ALG})$, we have
$\sum_{i \in A^*_t \cap I^{\ALG}_t} \nz_{it}(0) \leq \sum_{i \in D_t(I_t^{\ALG})} \nz_{it}(0)$.
Therefore, 
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} Z_{it} \right]
 = \bE\left[ \sumT \sum_{i \in A^*_t \cap I^{\ALG}_t} \nz_{it}(0) \right]
 \leq 
 \bE\left[ \sumT \sum_{i \in D_t(I_t^{\ALG})}  \nz_{it}(0) \right],
\end{align*}
where the first equality follows the same reasoning as \eqref{eq:propRandom}-\eqref{eq:propDet}.


\cref{thm:step2stronger} then follows from the following result which bounds the second term in \eqref{eq:opt_minus_base_decomp}. This term represents rewards from patients who are in state 0 under $\OPT$ but not under $\ALG$.
\begin{proposition} \label{prop:second_term}
\begin{align} \label{eq:second_term_claim}
\bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Z_{it} \right]	
\leq 
\bE\left[ \sumT \sum_{i \in A^{\ALG}_t} Z_{it} \right]	
\end{align}
\end{proposition}

\subsubsection{Proof of \cref{prop:second_term}.}
The main idea of this result is that for every $Z_{it}$ term that contributes to the LHS of \eqref{eq:second_term_claim}, since that patient is not available under $\ALG$, we have already collected this reward under $\ALG$.
We will show a one-to-one mapping from every $Z_{it}$ term on the LHS to a $Z_{i \nu_i(t)}$ term on the RHS of \eqref{eq:second_term_claim}.

Let $\cI = \{(i, t) : i \in A^*_t \setminus I^{\ALG}_t, K_{it} = 1\}$ be the set of (patient, time) tuples in which patient $i$ is chosen under $\OPT$ but not available under $\ALG$, and moreover, $K_{it} = 1$.
Note that from \eqref{eq:Z_explicit}, $K_{it} = 1$ is a necessary condition for $Z_{it} > 0$. 
Therefore, the LHS of \eqref{eq:second_term_claim} can be written as
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^*_t \setminus I^{\ALG}_t} Z_{it} \right]	
= \bE\left[ \sum_{(i, t) \in \cI} Z_{it} \right].
\end{align*}

Fix $(i, t) \in \cI$.
We have that $S_{it}^{\ALG} = 1$ (since $i \notin I^{\ALG}_t$) and $S_{it}^{\OPT} = 0$ (since $i \in A^*_{t}$).
Define $\nu_i(t) < t$ to be the last time that $i$ was in state 0 under $\ALG$:
\begin{align} \label{eq:definetau}
\nu_i(t) = \max\{ t' < t : S_{it'}^{\ALG} = 0\}.
\end{align}
That is, under $\ALG$, patient $i$ transitioned from state 0 to state 1 between time $\nu_i(t)$ and $\nu_i(t)+1$ and stayed in state 1 since.
We can show that $\nu_i(t)$ satisfies the following property: it must be that $i$ was chosen at time $\nu_i(t)$ under $\ALG$, and moreover, $Z_{i \nu_i(t)}$ is at least $t - \nu_i(t)$.

\begin{claim} \label{claim:properties_of_y}
Let $(i, t) \in \cI$.
Then, $i \in A^{\ALG}_{\nu_i(t)}$, and $Z_{i \nu_i(t)} \geq t - \nu_i(t)$.
\end{claim}	
\begin{myproof}[Proof of \cref{claim:properties_of_y}]
Let $(i, t) \in \cI$.
By definition of $\nu_i(t)$, $S_{i, \nu_i(t)}^{\ALG} = 0$ and $S_{i, \nu_i(t)+1}^{\ALG} = 1$.
To the contrary, suppose $i \notin A_{\nu_i(t)}^{\ALG}$.
Then, $i$ transitioned to state 1 without being chosen, and therefore $P_{i\nu_i(t)} = 1$.
Then, it must be that $i$ transitions to state 1 under $\OPT$ as well; $S_{i, \nu_i(t)+1}^{\OPT} = 1$.
But since $S_{it}^{\OPT} = 0$, $i$ switches back to state 0 before state $t$, which means that this should also happen under $\ALG$ (due to the sample path coupling).
This is a contradiction by the definition of $\nu_i(t)$.
Therefore, $i \in A_{\nu_i(t)}^{\ALG}$.

Moreover, under the same reasoning, it must be that $K_{i\nu_i(t)} = 1$, and that $S_{is}^{\NULL} = 0$ for all $s \in \{\nu_i(t)+1, \dots, t\}$.
Therefore, $Z_{i \nu_i(t)} \geq t - \nu_i(t)$.
\end{myproof}


We next show that the mapping $\nu_i$ is one-to-one.
\begin{claim} \label{claim:1-1}
If $(i, t), (i, s) \in \cI$ for $t \neq s$, then $\nu_i(t) \neq \nu_i(s)$.
\end{claim}
\begin{myproof}[Proof of \cref{claim:1-1}]
Suppose $t < s$ such that $(i, t), (i, s) \in \cI$.
That is, under $\OPT$, patient $i$ was chosen both at time $t$ and $s$, and the patient was already in state 1 under $\ALG$ at both of these times.
Suppose, for contradiction, $\nu_i(t) = \nu_i(s)$.
This means that under $\ALG$, patient $i$ was chosen at time $\nu_i(t)$, and the patient has stayed in state 1 from time $\nu_i(t)+1$ until at least time $s$.

$(i, t), (i, s) \in \cI$ implies $S^{\OPT}_{it} = S^{\OPT}_{is} = 0$.
Since $K_{it} = 1$, the patient transitioned to state 1 at time $t+1$.
Since $S^{\OPT}_{is} = 0$, it must be that there exists a $t' \in \{t+1, \dots, s-1\}$ such that $G_{it'} = 1, P_{it'} = 0$.
But since $S^{\ALG}_{it'} = 1$, the fact that $G_{it'} = 1$ and $P_{it'} = 0$ implies that $S^{\ALG}_{i, t'+1} = 0$, which is a contradiction.
\end{myproof}

Claims \ref{claim:properties_of_y} and \ref{claim:1-1} show that every $(i, t) \in \cI$ maps to one term in the RHS of \eqref{eq:second_term_claim}.
Therefore, the right hand side of \eqref{eq:second_term_claim} is at least
\begin{align*}
\bE\left[ \sumT \sum_{i \in A^{\ALG}_t} Z_{it} \right]	
\geq 
\bE\left[ \sum_{(i, t) \in \cI} Z_{i\nu_i(t)} \right].
\end{align*}
Then, we are done if we can show
\begin{align*} 
\bE\left[ \sum_{(i, t) \in \cI} Z_{it} \right]
\leq
\bE\left[ \sum_{(i, t) \in \cI} Z_{i\nu_i(t)} \right],
\end{align*}
which we can write as
\begin{align}\label{eq:simpler_goal}
\sumT \sumN \bE[Z_{it} \;|\; (i, t) \in \cI\;] \Pr((i, t)\in \cI)
\leq 
\sumT \sumN \bE[Z_{i\nu_i(t)} \;|\; (i, t) \in \cI\;] \Pr((i, t)\in \cI).
\end{align}

The following claim implies \cref{eq:simpler_goal} and finishes the proof of \cref{prop:second_term}.
\begin{claim} \label{claim:compare_ys}
For every $i \in [N]$ and $t \geq 1$,
\begin{align} \label{eq:inner_term}
\bE[Z_{it} \;|\; (i, t)  \in \cI\;] 
\leq 
\bE[Z_{i\nu_i(t)} \;|\; (i, t)  \in \cI\;]
\end{align}
\end{claim}


\begin{myproof}[Proof of \cref{claim:compare_ys}]
Fix $i, t$.
First, we upper bound the LHS of \cref{eq:inner_term}.
\begin{align*}
\bE[Z_{it} \;|\; (i, t)  \in \cI\;]  
&= \bE[Z_{it} \;|\; Z_{it} \geq 1,  (i, t)  \in \cI\;] \Pr(Z_{it} \geq 1 \;|\; (i, t)  \in \cI) \\
&\leq \bE[Z_{it} \;|\; Z_{it} \geq 1,  (i, t)  \in \cI\;].
\end{align*}
Note that conditioned on $Z_{it} \geq 1$, $Z_{it}$ is only a function of $(P_{it'}, G_{it'})_{t' > t}$, the `future' with respect to $t$, and the event $\{(i, t)  \in \cI\} = \{i \in A^*_t \setminus I^{\ALG}_t,  K_{it} = 1\}$ is independent of these future random variables.
Therefore, conditioned on $Z_{it} \geq 1$, $Z_{it}$ is independent of $(i, t)  \in \cI$, and hence
\begin{align}\label{eq:lhs_ub} 
\bE[Z_{it} \;|\; (i, t)  \in \cI\;]  
\leq
\bE[Z_{it} \;|\; Z_{it} \geq 1].
\end{align}

Next, consider the RHS of \eqref{eq:inner_term}.
From \cref{claim:properties_of_y}, $(i, t) \in \cI$ implies $Z_{i \nu_i(t)} \geq t - \nu_i(t)$.
Therefore, 
\begin{align*}
\bE[Z_{i\nu_i(t)} \;|\; (i, t)  \in \cI\;]
= \bE[Z_{i\nu_i(t)} \;|\; (i, t)  \in \cI, Z_{i \nu_i(t)} \geq t - \nu_i(t)\;].
\end{align*}
Similar to before, conditioned on $Z_{i \nu_i(t)} \geq t - \nu_i(t)$, $Z_{it}$ is only a function of $(P_{it'}, G_{it'})_{t' > t}$, the `future' with respect to $t$.
Therefore, conditioned on $Z_{i \nu_i(t)} \geq t - \nu_i(t)$, $Z_{it}$ is independent of $(i, t) \in \cI$, and hence
\begin{align*}
\bE[Z_{i\nu_i(t)} \;|\; (i, t)  \in \cI\;]
&= \bE[Z_{i\nu_i(t)} \;|\; Z_{i \nu_i(t)} \geq t - \nu_i(t)\;] \\
&= \sum_{t' < t} \Pr(\nu_i(t) = t' \;|\; Z_{i \nu_i(t)} \geq t - \nu_i(t)) \bE[Z_{it'} \;|\; Z_{i t'} \geq t - t'] \\ 
&\geq \sum_{t' < t} \Pr(\nu_i(t) = t' \;|\; Z_{i \nu_i(t)} \geq t - \nu_i(t)) \bE[Z_{it'} \;|\; Z_{i t'} \geq 1].
\end{align*}


Note that for $s < t$, $\bE[Z_{is} | Z_{is} \geq 1] \geq \bE[Z_{it} | Z_{it} \geq 1]$.
This is because given the equation for $Z_{it}$ in \eqref{eq:Z_explicit}, the only difference between $Z_{it}$ and $Z_{is}$ is that $Z_{it}$ has a smaller maximum value of $T-t$.
Therefore,
\begin{align}
\bE[Z_{i\nu_i(t)} \;|\; (i, t)  \in \cI\;] 
&\geq  \bE[ Z_{i t}  | Z_{i t} \geq 1] \sum_{t' < t} \Pr(\nu_i(t) = t' \;|\; Z_{i \nu_i(t)} \geq t - \nu_i(t)) \nonumber \\
&= \bE[ Z_{it}  | Z_{it} \geq 1]. \label{eq:rhs_lb} 
\end{align}

Combining \eqref{eq:rhs_lb} and \eqref{eq:lhs_ub} proves the result.
\end{myproof}



\subsection{Step 3: Proof of \cref{prop:approx_index}}

Fix $\alpha_1, \alpha_2$,
and let $\ALG$ be a policy that uses indices that satisfies $\alpha_1 \nz_{it}(0) \leq z^{\ALG}_{it}(0) \leq \alpha_2 \nz_{it}(0)$.
Then, if patient $i$ and $j$ satisfy $z^{\ALG}_{it}(0) \geq z^{\ALG}_{jt}(0)$, then we have $\nz_{it}(0) \geq \frac{\alpha_1}{\alpha_2} \nz_{jt}(0)$.
Therefore, if one chooses patients with the highest values of $z^{\ALG}_{it}(0)$, their null intervention values will be at least an $\alpha_1/\alpha_2$ factor of the patients that have the highest null intervention values.
That is, at any time $t$, we have
\begin{align*}
\sum_{i \in A_t^{\ALG}} \nz_{it}(0) 
\geq 
\frac{\alpha_1}{\alpha_2} \sum_{i \in D_t(I_t^{\ALG}(0))} \nz_{it}(0),
\end{align*}
where $A_t^{\ALG}$ are the patients that $\ALG$ chooses at time $t$, $I_t^{\ALG}$ are the patients in state 0 at time $t$ under $\ALG$, and $D_t(I_t^{\ALG}) \subseteq I_t^{\ALG}$ are the $B$ patients with the largest null intervention values.
Summing over time steps and taking an expectation results in
\begin{align} \label{eq:comparez}
\bE\left[ \sumT \sum_{i \in A_t^{\ALG}} \nz_{it}(0) \right]
\geq 
\alpha \cdot \bE\left[ \sumT \sum_{i \in D_t(I_t^{\ALG})} \nz_{it}(0) \right].
\end{align}
\cref{prop:alg_minus_base} states that the LHS of \cref{eq:comparez} is equal to $\ALG - \NULL$.
Next, \cref{thm:step2stronger} implies that the RHS is at least $\frac{1}{2} (\OPT - \NULL)$.
Combining yields
\begin{align*}
\ALG - \NULL \geq \frac{\alpha}{2} (\OPT - \NULL)
\end{align*}
as desired.


\subsection{Step 4: Proof of \cref{lemma:rand_null}}  \label{s.app.p4}
Fix a patient $i$ and time $t$. In this proof, we remove the subscript $i$ on $p$, $g$ and $\tau$ for convenience.
\cref{lemma:zgamma} gives us an expression for both $z^{\lambda}_{it}(0)$ and $\nz_{it}(0)$:
\begin{align} 
z^{\lambda}_{it}(0) &= \tau \cdot \bE[\min\{\text{Geometric}(p + g + \gamma \tau (1-p-g)/(1-p)), T-t+1\}] \label{eq:z1}\\
\nz_{it}(0) &= \tau \cdot \bE[\min\{\text{Geometric}(p + g, T-t+1\}]. \label{eq:z2}
\end{align}
Since $p+g < 1$, $p + g + \gamma \tau (1-p-g)/(1-p)) > p+g$, hence $z^{\pi}_{it}(0)/\nz_{it}(0) \leq 1$.
Next, to lower bound the ratio $z^{\pi}_{it}(0)/\nz_{it}(0)$, we use the following lemma:
\begin{lemma} \label{lemma:trunc_geometric}
Suppose $X = \text{Geometric}(P)$, $Y = \text{Geometric}(G)$ with $P > G$, and let $T > 0$ be a positive integer.
Then, 
\begin{align*}
\frac{\bE[\min\{X, T\}]}{\bE[\min\{Y, T\}]} \geq \frac{\bE[X]}{\bE[Y]}.
\end{align*}
\end{lemma}

This lemma allows us to lower bound $z^{\pi}_{it}(0)/\nz_{it}(0)$ by considering the expressions \eqref{eq:z1} and \eqref{eq:z2} without the $\min$ with $T-t+1$.
That is, we have
\begin{align*}
\frac{\bE[\text{Geometric}(p + g + \gamma \tau(1-p-g)/(1-p) )]}{\bE[\text{Geometric}(p + g)]}
&= \frac{1}{1 + \gamma \frac{\tau (1-p-g)}{(p+g)(1-p)}}.
\end{align*}
Using \cref{lemma:trunc_geometric} implies
\begin{align*}
z^{\pi}_{it}/\nz_{it} 
&\geq \frac{1}{1 + \gamma \frac{\tau (1-p-g)}{(p+g)(1-p)}}.
\end{align*}

\begin{myproof}[Proof of \cref{lemma:trunc_geometric}]
Let $X = \text{Geometric}(P)$, $Y = \text{Geometric}(G)$ with $P > G$.
We can write an explicit expression for $\bE[\min\{X, T\}]$ as the following:
\begin{align*}
\bE[\min\{X, T\}] 
&= \sum_{k=1}^{T-1} k (1-P)^{k-1} P + T (1-P)^{T-1} \\
&= -P \frac{d}{dP} \left(\sum_{k=1}^{T-1} (1-P)^{k} \right) + T (1-P)^{T-1} \\
&= -P \frac{d}{dP} \left(\frac{1-(1-P)^T}{P}-1 \right) + T (1-P)^{T-1} \\
&= -P  \left(\frac{ T(1-P)^{T-1} P - (1-(1-P)^T) }{P^2}\right) + T (1-P)^{T-1} \\
&= \frac{1}{P} -  \frac{  (1-P)^T }{P} 
\end{align*}

Using this, we get the desired result:
\begin{align*}
	\frac{\bE[\min\{X, T\}] }{\bE[\min\{Y, T\}] } 
	&= \frac{\frac{1}{P} -  \frac{  (1-P)^T }{P}}{\frac{1}{G} -  \frac{  (1-G)^T }{G}} \\
	&= \frac{\frac{1}{P}(1-(1-P)^T)}{ \frac{1}{G}(1-(1-G)^T)} \\
	&\geq \frac{1/P}{1/G} \\
	&=\frac{\bE[X]}{ \bE[Y] },
\end{align*}
where the inequality follows since $P > G$.
\end{myproof}


\subsection{Proof of \cref{corr:approx}} \label{sec:app:pf_approx}

This result follows from applying \cref{prop:approx_index} and \cref{lemma:rand_null}.
Let $\ALG$ be a policy that satisfies \eqref{eq.approxvalues2}.
Then, by \cref{lemma:rand_null}, we have that the following holds for all $i$ and $t$:
\begin{align}
\frac{1}{1 + \gamma M_i} c_1 \nz_{it}(0) \leq z^{\ALG}_{it}(0)\leq c_2 \nz_{it}(0).
\end{align}
Then, we apply \cref{prop:approx_index} with $\alpha_1 = \frac{1}{1 + \gamma M_i} c_1$ and $\alpha_2 = c_2$, and the result follows.












