
Our work relates to the expansive streams of literature on reinforcement learning and (approximate) dynamic programming, as well as the applied operations research literature focusing on improving healthcare delivery in resource-limited settings. Methodologically speaking, most existing solution approaches to problems similar to ours can be classified as either (a) developing policies for a \textit{known} underlying model of behavior or (b) \textit{learning} a policy using data. We summarize these two streams of work in Sections \ref{ss.knownmod} and \ref{ss.unknownmod}, respectively, before discussing prior work on TB treatment as an application area in \cref{ss.litreviewhealth}.

\subsection{Known Model}\label{ss.knownmod}
Our model (described in detail in \cref{s.gmodel}) assumes that every patient behaves according to a Markov decision process (MDP). Even if all the MDP parameters of these models were known exactly, the size of the system state space would be \textit{exponential} in the number of patients: the size of the state space is $|\cS|^N$, where $\cS$ is the state space for one patient and $N$ is the number of patients. As a result, a direct application of dynamic programming techniques such as backwards induction or policy/value iteration would take exponential time and hence is practically infeasible.

As exact methods are infeasible, one can resort to \textit{approximate} dynamic programming (ADP) techniques developed for weakly-coupled MDPs \citep[e.g.,][]{meuleau1998solving,adelman2008relaxations,d2023optimal}. Specifically, the model we study is a \textit{restless bandit}: each patient corresponds to an arm, and there is a budget on the number of arms that can be `pulled' (assigned an intervention) at each time step. The state of each arm evolves as a Markov chain, where its transition probabilities depend on the action taken. It is known that finding the optimal policy to a restless bandit is PSPACE-hard \citep{papadimitriou1994complexity}. There is a large literature on developing algorithms for this problem \citep[e.g., ][]{whittle1988restless,glazebrook2002index,ansell2003whittle,glazebrook2006some,liu2010indexability,guha2010approximation}.
A commonly used policy is the Whittle's index \citep{whittle1988restless}, which is known to be asymptotically optimal under certain strong conditions \citep{weber1990index} and has been shown to have good empirical performance 
\citep{ansell2003whittle,glazebrook2002index,glazebrook2006some}.
However, the Whittle's index have mostly been applied to problems with a single-dimensional parameter, as the computation of the Whittle's index for a single arm is cubic in the number of states \citep{nino2020fast}.
Therefore, even when the parameters are known exactly, the restless bandit is far from a solved problem, especially when dealing with a large or infinitely-sized state space.



\subsection{Unknown Model}\label{ss.unknownmod}
Deriving an optimal policy for an MDP with unknown parameters corresponds to reinforcement learning (RL), a rapidly expanding area of research \citep{sutton2018reinforcement}.
However, a naive application of the RL framework onto our problem results in an exponentially large state space --- this correspondingly results in an exponential blowup in the data requirements to deploy generic RL methods such as Q-learning \citep{jin2018q}.
\added{
Further, we consider the more difficult \emph{offline} RL problem, where we assume access to a dataset collected from another policy and no further access to the environment 
--- see \cite{levine2020offline} for a survey of offline RL (note that this literature studies the generic RL problem, not the restless bandit setting).
In this literature, \cite{brandfonbrener2021offline} shows that performing one step of policy iteration exhibits strong empirical performance, compared to other iterative methods that rely on off-policy evaluation.
Our algorithm also performs one step of policy iteration, but we introduce an additional modification where we \textit{decompose} the value function by patient, to address the issue of the exponential state space.
}


\paragraph{Greedy and multi-armed bandits.}
One approach, as described in the introduction, is to greedily maximize the immediate reward at each time step.
For example, in the treatment adherence setting, one can reach out to patients with the highest increase in their probability of adhering in the next day from the intervention.
A multi-armed bandit is one natural framework that can be used to learn such a policy.
This is the approach used in HeartSteps \citep{lei2017actor,liao2020personalized}, a program to promote physical activity using real-time data collected from wristband sensors.
These papers use a contextual bandit model, where the context represents a user at a particular time step, and they develop a bandit algorithm whose goal is to increase the immediate activity level of the user.
Given the vast literature on contextual bandits, there are a wide variety of algorithms that one can easily plug in.

A fundamental downside of this greedy approach is that it does not capture any potential long-term effects of an action---that is, an action may not only impact a patient's immediate behavior, but their behavior for all future time steps.
One way to address this is to specifically model the type of long-term effect it can have.
For example, \cite{liao2020personalized} introduce a `dosage' variable that models the phenomenon that the treatment effect of an action is often smaller when an action was recently given to that patient in the past.
Similarly, \cite{mintz2020nonstationary} incorporate habituation and recovery dynamics into the bandit framework.
However, these approaches capture only particular types of long-term effects that are explicitly modeled, and there could be other, complex factors that affect the downstream behavior of patients.

Our work does not take this greedy approach, and we aim to learn a policy that maximizes long-term rewards, without specifically modeling the type of long-term effect that an action can have.
We benchmark against a contextual bandit policy, and we observe that incorporating the long-term effects is critical in the behavioral health setting that we study. 

\paragraph{Learning for restless bandits.}
A non-greedy approach to this problem corresponds to the restless bandit model in the unknown parameter regime.
There is a nascent literature develops learning algorithms in this setting. One approach is to adapt algorithms from the multi-armed bandit literature such as UCB \citep{wang2020restless} or Thompson Sampling \citep{jung2019regret}. 
Recent approaches similarly adapt reinforcement learning methods such as Q-learning \citep[e.g., ][]{fu2019towards,avrachenkov2022whittle}. Importantly, these methods are \textit{online} learning algorithms that require continuous exploration and assume that the state space is known and finite. This literature focuses on providing theoretical guarantees of the proposed algorithms, and these algorithms eventually converge to the optimal policy.


Our work differs from the aforementioned literature in \added{four main} ways.
First, we take an \textit{offline} approach, where we leverage existing data to derive a new policy.
This removes the need for exploration, as well as the dependence on a long horizon to obtain an improvement over a baseline policy.
Second, our work does not focus on deriving an \textit{optimal} policy; we derive a practical policy that can be implemented with limited data, and our theoretical results are approximation guarantees to the optimal policy.

\added{Third, we take a \textit{model-free} approach, rather than the commonly used \textit{model-based} approach in restless bandits, in which a simple model of patient behavior is postulated using a small state space.}
For example, \cite{mate2022field} and \cite{biswas2021learn} posit a simple MDP with two and three states respectively for each user, where a state represents the engagement level of the user. 
\cite{aswani2019behavioral} take a slightly different model-based approach in studying weight loss interventions, where they model user behavior via utility functions.
Then, the policy is developed based on these posited models, and therefore these approaches rely heavily on the correctness of the model.
In contrast, our model-free approach is to incorporate as much information as available into the patient's state (which leads to a relatively large state space), and our policy then operates under the assumption that patients in similar states will behave similarly.
\added{Such an approach relies on being able to easily handle a very large, or infinitely-sized state space, which is not the case for for classic restless bandit policies like the Whittle's index.
Our approach is affected by the state space only through a prediction task, which can be performed with any machine learning algorithm (which are usually designed to be able to handle a large state space).
}
In \cref{s.theory}, we use a simple two-state MDP for the purposes of proving a theoretical guarantee for our policy, but the policy is defined irrespective of the underlying patient behavior model.

\added{
Lastly, we note that our technical analysis does not build off of proof ideas from the RL literature --- rather, it borrow ideas from the literature on the online allocation of reusable resources (which, on the surface, seems unrelated to our problem setting).
The key connection relies on interpreting one \emph{patient} as one \emph{reusable resource}. Since we model one patient as being in one of two states (0 or 1), 
we can interpret a patient being in state 0 and state 1 as a reusable resource being \emph{available} and \emph{unavailable} to sell, respectively.
As patients can transition from state 1 to 0, this is analogous to a sold resource returning to the available state.
Our analysis for approximation guarantees builds off of a proof technique used in \cite{gong2021online}.
}



\subsection{Healthcare systems} \label{ss.litreviewhealth}
Finally, from an application perspective, our work contributes to a growing literature focusing on improving healthcare delivery systems in resource-limited settings. Most related to the paper at hand are recent papers focusing on improving TB outcomes in resource-limited settings. Much of this work has been on the policy level, with \cite{Suen14Disease} evaluating strategic alternatives for disease control of multi-drug resistant TB (MDR TB) in India and finding that with MDR TB transitioning from treatment-generated to transmission-generated, rapid diagnosis of MDR TB becomes increasingly important. Similarly, \cite{Suen18Optimal} optimize the timing of sequential tests for TB drug resistance, a necessary step for transitioning patients to second-line treatment. 

Two papers focus on medication adherence. \cite{Suen22Design} tackle the problem of designing patient-level incentives to motivate medication adherence, in situations where adherence is observable but patients have unobserved and heterogeneous preferences for adherence. They first take a modeling approach to design an optimal incentive scheme and then demonstrate that deploying it would be cost effective in the context of TB control in India. Similar to us, \cite{Boutilier22Improving} focus on a behavioral intervention, demonstrating that data describing patient behavior (e.g., patterns of self-verification of treatment adherence, like in the case of Keheala) can be leveraged to predict daily behavioral outcomes as well as health outcomes. They use such predictions to assign patients to risk groups and demonstrate empirically that outreach can be effective, even for patients who are classified as at risk. However, they stop short of prescribing an actionable policy for assigning patient outreach, which is the topic of this paper.
