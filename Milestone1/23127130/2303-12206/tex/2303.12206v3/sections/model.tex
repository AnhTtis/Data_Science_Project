We first formally describe the intended problem setting in its full complexity in \cref{s.gmodel}. 
We then describe our algorithm in \cref{s.decomposedPI} and discuss the intended usage and design choices of the algorithm in \cref{s.intended_usage}.
\added{
In \cref{ss.general_guarantees}, we show, using a counterexample, that our algorithm does not always guarantee an improvement over the baseline policy, but we then demonstrate that an improvement is guaranteed when the baseline policy belongs to a specific class of random policies. All proofs for this section and the next are relegated to appendices.}



\subsection{Model} \label{s.gmodel}
There are $N$ patients and $T$ time steps.
Each patient $i \in [N]$ is associated with a Markov decision process (MDP) represented by $\mathcal{M}_i = (\cS, \cA, \mathcal{P}_i, R_i)$.
$\cS$ is the state space, and let $S_{it} \in \cS $ denote the state of patient $i$ at time $t$.
The action space is $\cA = \{0, 1\}$, where we will refer to action $A_{it} = 0$ as the \textit{null} action, and action $A_{it}=1$ as the \textit{intervention}.
$\mathcal{P}_i(S, S', A) = \Pr(S_{i,t+1} = S' \;|\; S_{it} = S, A_{it} = A)$ is the probability of transition from state $S$ to $S'$ when action $A$ is taken, and $R_i(S, S', A)$ is the immediate reward from this transition.
If $A_{it} = 1$, we say that patient $i$ is \textit{chosen} at time $t$.

Next, we define a \textit{system MDP} by combining the $N$ patient MDPs and adding a budget $B \geq 0$ on the number of interventions that can be given at each time step.
That is, $\Aspace = \{(A_1, \dots, A_N) \subseteq \{0, 1\}^N: \sumN A_i \leq B\}$ is the action space for the system MDP.
The state space is $\cS^N$, 
and a policy $\pi = \{\pi_t \;|\; t \in [T]\}$ maps each state to a distribution over the action space $\Aspace$.
At each time $t = 1, \dots, T$, the following sequence of events occur:
\begin{enumerate}
	\item The state $S_{it} \in  \cS$ is observed for each patient $i \in [N]$.
	\item An action $(A_{1t}, \dots, A_{Nt})$ is drawn from $\pi_t(S_{1t}, \dots, S_{Nt})$.
	\item For each patient $i$, their next state $S_{i,t+1}$ is realized independently according to their transition probabilities $\mathcal{P}_i(S_{it}, \cdot, A_{it})$. We gain the reward $\sumN R_i(S_{it}, S_{i,t+1}, A_{it})$.
\end{enumerate}
Let $\mathbf{S}_1 = (S_{11}, \dots, S_{N1})$ be the starting state profile.
An instance of this problem is represented by $\cI = (N, T, (\mathcal{M}_i)_{i \in [N]}, B, \mathbf{S}_1)$.
We use the name of a policy to denote the expected total reward of that policy.


\subsection{Decomposed Policy Iteration} \label{s.decomposedPI}

We introduce a natural policy closely related to the ubiquitous \textit{policy iteration} (PI) algorithm \citep{howard1960dynamic}.
As the name suggests, policy iteration is an iterative algorithm that maintains a policy and updates it at each iteration to improve its performance.
Our policy, $\DPI$, can be thought of as performing \textit{one step} of policy iteration.
In addition, $\DPI$ differs from vanilla PI in that the algorithm decomposes to the patient level, which allows us to remove dependence on the exponentially sized state and action space.


\subsubsection{Intervention values.}
For any policy $\pi$, let $S^\pi_{it}$ and $A_{it}^\pi$ be the induced random variables corresponding to the state and action, respectively, of patient $i$ at time $t$ under policy $\pi$.
Then, we define 
\begin{align} \label{eq:q}
q_{it}^\pi(S, A) = \bE_{\pi}\left[\sum_{t'=t}^T R(S_{it'}^\pi, S_{i,t'+1}^\pi, A_{it'}^\pi) \;\bigg|\; S_{it}^\pi = S, A_{it}^\pi = A \right],
\end{align}
which represents the expected future reward from patient $i$ when running policy $\pi$, conditioned that they were in state $S$ and were given action $A$ at time $t$.
In the case where $\Pr(S_{it}^\pi = S, A_{it}^\pi = A) = 0$, we define $q_{it}^\pi(S, A) = 0$.
Next, we define an \textit{intervention value}, $z^{\pi}_{it}(S)$, to be the difference in the $q^{\pi}_{it}$ values under $A=1$ and $A=0$:
\begin{align*}
z^\pi_{it}(S) = q_{it}^{\pi}(S, 1) - q_{it}^{\pi}(S, 0).
\end{align*}
$z^\pi_{it}(S)$ is the difference in expected total future reward from patient $i$ under policy $\pi$ when the patient is given the intervention, compared to when they are not.

\subsubsection{Policy.}
Given a \textit{baseline policy} $\pi$, $\DPI(\pi)$ is the policy that gives the intervention to the patients with the highest positive intervention values, up to the budget constraint.
Formally, 
\begin{align*}
\DPI(\pi)_t(S_{1t}, \dots, S_{Nt}) \in \argmax_{A \in \cA_N} \sum_{i \in [N]} A_{i} z^\pi_{it}(S_{it}).
\end{align*}

\subsection{Intended Usage and Design Choices} \label{s.intended_usage}
Recall that our motivating research question is to leverage a limited dataset collected from a pilot study to learn an effective intervention policy for subsequent deployment. $\DPI$ was designed specifically for this setting. The intended usage of $\DPI$ therefore follows three steps. First, run a policy $\pi$ and let $\mathcal{D}$ be the dataset of trajectories of states, actions, and rewards. 
Second, use the dataset $\mathcal{D}$ to compute estimates $\hat{q}_{it}^{\pi}(S, A)$ \added{for ${q}_{it}^{\pi}(S, A)$} using a prediction algorithm. Third, deploy $\DPI(\pi)$ using the estimated intervention values, $\hat{z}^{\pi}_{it}(S) = \hat{q}_{it}^{\pi}(S, 1) - \hat{q}_{it}^{\pi}(S, 0)$.
Note that the main non-trivial step is the second one, the estimation. $\DPI$ was designed in a way to make this step as easy as possible. In particular, $\DPI$ can essentially be described by two design choices, both of which are motivated by the goal of easing estimation:
\begin{enumerate}
		\item[(a)] It performs \textit{one} round of policy iteration.
		\item[(b)] It uses $q$-values that are \textit{decomposed} at the patient level.
\end{enumerate}	
We explain both of these design choices below.

\subsubsection{Design choice (a): A single iteration.}
Given a dataset $\mathcal{D}$ generated from running $\pi$, estimating $q_{it}^\pi(S, A)$ corresponds to a \textit{prediction} problem (also called on-policy evaluation).
Prediction is one of the simplest tasks in reinforcement learning, 
and there exist a myriad of techniques that one can use for this task, such as Monte Carlo methods or temporal difference learning \citep{sutton2018reinforcement,szepesvari2022algorithms}.
Because of this significant existing literature on prediction, our work does not focus on analyzing this step.
In \cref{s.casestudy}, we provide an example of this step using a linear function approximation for the $q$ function.

However, to do \textit{more than one} round of policy iteration, one needs to learn $q^{\pi'}$ for a policy $\pi' \neq \pi$.
Estimating $q^{\pi'}$ from data generated from $\pi$ is an \textit{off-policy evaluation} problem, which is provably hard; \cite{wang2020statistical} show that the sample complexity of off-policy evaluation with a linear function approximation is \textit{exponential} in the time horizon.
Motivated by the hardness of off-policy evaluation, a recent work in the general offline RL literature, \cite{brandfonbrener2021offline}, also proposes the method of doing one-step of policy iteration from offline data and demonstrates strong empirical results of this approach.
Our work is complementary to the empirical findings of \cite{brandfonbrener2021offline},
\added{though we note that our policy is not the same as the one studied in \cite{brandfonbrener2021offline} due to the decomposition, discussed next.}

\subsubsection{Design choice (b): Decomposition.} \label{sss.designchoiceb}
The main difference between $\DPI$ and one iteration of the usual policy iteration (PI) is that $\DPI$ operates on the $q$-function, which is specific to an individual patient.
PI, in contrast, operates on the state space of the system MDP.
Specifically, for a state profile $\bS = (S_1, \dots, S_N)$ and action profile $\bA = (A_1, \dots, A_N)$, PI operates on the function $Q_{t}^\pi(\bS, \bA)$ defined as
\begin{align}  \label{eq:Q}
Q_{t}^\pi(\bS, \bA) = \bE_{\pi}\left[\sum_{t'=t}^T \sumN R(S_{it'}^\pi, S_{i,t'+1}^\pi, A_{it'}^\pi) \;\bigg|\; S_{it}^\pi = S_i, A_{it}^\pi = A_i \; \forall i \in [N] \right].
\end{align}


The advantage of the decomposition (using $q$ rather than $Q$) is in its ease of estimation.
Note that the size of the state and action space for the system MDP is $|\cS|^N$ and $N \choose B$, respectively, whereas the size of the state and action space for a single patient is $|\cS|$ and 2, respectively. Then, if one used a tabular approach to estimating these functions, learning $Q_t$ requires estimating $|\cS|^N {N \choose B}$ quantities, while learning $q_{it}$ for every $i$ requires estimating $2|\cS|N$ quantities. The latter is an \textit{exponentially} fewer number of estimation tasks than the former.

The disadvantage of the decomposition is that since the individual $q$ functions do not take as input the entire system state, it loses information. That is, for a system state $\bS =(S_1, \dots, S_N)$ and action profile $\bA =(A_1, \dots, A_N)$, the sum of the $q$-values do not correspond to the $Q$-value: $\sum_{i=1}^N q_{it}^{\pi}(S_i, A_i)\neq Q_t^{\pi}(\bS, \bA)$.
This introduces a bias in the improvement step of policy iteration.
This can lead to an undesirable behavior where policy iteration does not always result in an improvement \added{(formalized next in \cref{prop:dpi_can_be_worse})}.

Therefore, the decomposition improves the ease of estimation, but comes at a cost of losing information.
In \cref{s.theory}, where we study a stylized model, we establish a performance guarantee based on the decomposed $q$ values.
This guarantee relies on the baseline policy, $\pi$, to be randomized, and the performance guarantee improves as the number of interventions that $\pi$ gives decreases.
Both of these features, randomization and a small number of interventions, aids in reducing the extent of impact of the bias. 

\subsection{\added{Theoretical Guarantees for the Full Model}} \label{ss.general_guarantees}
\added{
Generically, vanilla policy iteration results in a guaranteed improvement in performance at each step \citep{sutton2018reinforcement}.
In our model, one step of policy iteration using the \emph{system} Q-values (\cref{eq:Q}), from policy $\pi$ would result in an improved policy over $\pi$.
However, Proposition \ref{prop:dpi_can_be_worse} shows that this is not always the case for $\DPI(\pi)$ due to the decomposition.

\begin{proposition} \label{prop:dpi_can_be_worse}
There exists an instance $\mathcal{I}$ and a policy $\pi$ where the expected total reward of $\DPI(\pi)$ is strictly smaller than that of $\pi$.
\end{proposition}

Although $\DPI(\pi)$ does not universally guarantee an improvement, the following result shows that this is the case when $\pi$ belongs to a particular class of policies that gives interventions in a random manner.
For $\gamma \in (0, 1)$, let $\RAND(\gamma)$ be the policy in which at every time step, gives an intervention to every patient independently with probability $\gamma$.
This class of policies is an approximation of the policy that was deployed for Keheala.

\begin{theorem} \label{thm:improvement_random}
Let $\gamma \in (0, 1)$ and suppose the budget $B$ satisfies $B \geq \gamma N$.
Then, for any instance, $\DPI(\RAND(\gamma)) \geq \RAND(\gamma)$.
\end{theorem}

While this result demonstrates that our policy results in an improvement if the baseline policy is $\RAND(\gamma)$, the improvement can be marginal, as \cref{thm:improvement_random} does not speak to the \emph{magnitude} of the improvement.
In the next section, we study a special case of MDPs that is inspired by our application, and we provide a different theoretical guarantee which is substantially stronger. 
}

\section{\added{Theoretical Guarantees for a Special Case}} \label{s.theory}
The full model, presented in the previous section, describes the intended use case for $\DPI$. 
However, the generality of this model prohibits the ability to have strong theoretical guarantees.
In this section, we therefore introduce a special case of the model that is inspired by our problem setting,
where we are surprisingly able to prove strong performance guarantees.
We first introduce this stylized model in \cref{ss.2statemodel} and describe the form of $\DPI$ in this setting in \cref{ss.simplepolicy}. We then present our performance guarantees in \cref{ss.perfguar} and additional robustness results in \cref{s.robustness}.

\subsection{Two-State MDP Model} \label{ss.2statemodel}
The state space is $\cS = \{0, 1\}$, where 0 and 1 correspond to an undesired and desired state, respectively.
Under the null action ($A=0$), $p_i$ and $g_i$ represent the probability of transitioning from state 0 to 1 and 1 to 0, respectively.
The intervention ($A=1$) only changes the probability of transitioning from state 0 to 1, which becomes $p_i + \tau_i$.
We assume that $p_i, g_i \in [0, 1/2]$ for all $i$, implying that states are more likely to stay the same than change when there is no intervention.
We assume $\tau_i \in [0, 1-p_i]$ for all $i$, hence an intervention can only increase the probability of switching from state 0 to 1. 
Lastly, the reward is simply equal to the resulting state; i.e.,\ $R_i(S, S', A) = S'$.
This represents the goal of maximizing the fraction of time that the patient is in the desired state.
The MDP for one patient is specified by the parameters $(p_i, g_i, \tau_i)$, and this MDP is summarized in Figure~\ref{fig:2_state_mc}.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.55\linewidth]{figs/2stateMDP}
  \caption{MDP for patient $i$.}
  \label{fig:2_state_mc}
\end{center}
\end{figure}

This is a special case of the full model from \cref{s.gmodel}, where we assume the patient MDP takes the simple form described above.
All other aspects of the model remain the same. In particular, the system MDP is derived by combining the patient MDPs via a budget constraint. Note that the set of possible policies is finite, since both the state and action spaces are finite. Therefore, for any instance, there exists an optimal policy $\OPT$ which maximizes the objective, $\bE[\sumT \sumN S_{i,t+1}]$.

We believe this simplified version of the system is representative and relevant, both for our specific motivating application and behavioral health operations in general. From the perspective of Keheala (as described in \cref{s.intro}), the stylized model captures the salient features of patient behavior in the simplest way possible.
A dataset collected by an RCT run by Keheala revealed that the best single feature that predicted whether a patient will verify on a given day, is simply whether they verified the day before (80.9\% accuracy).
Therefore, the states in the model simply represent whether a patient verified the previous day.
\added{Additionally, according to the Keheala protocol, the subjects eligible for outreach interventions are those who have not been verifying treatment adherence, the objective being to encourage behavior change. Since the protocol does not prescribe outreach to subjects who are in the desired state (i.e., verifying their treatment adherence), our model is such that the intervention only impacts the transition from state 0 to state 1.}


More broadly, similar Markov models have been used in the literature to model patient behavior.
\cite{mate2020collapsing} and \cite{mate2022field} use the same two-state model and apply it to the setting of TB treatment adherence and maternal health respectively. \cite{biswas2021learn} also studies maternal health in which they employ a similar MDP with three states. 
These three works also use the same objective function as we do.
This type of a two-state Markov model is originally from the literature on communication systems (where it is referred to as a Gilbert-Elliot channel \citep{gilbert1960capacity}) but has also been used in marketing \citep{schmittlein1987counting}. 


\subsection{The $\DPI$ Policy for the Two-State MDP Model} \label{ss.simplepolicy}
Our main result pertains to $\DPI(\pi)$ for $\pi$ belonging to a specific class of policies.
Specifically, for $\gamma \in (0, 1)$, let $\RAND(\gamma)$ be the policy in which at every time step, gives an intervention to every patient in state 0 independently with probability $\gamma$.
Denote the intervention values of the policy $\RAND(\gamma)$ by $z^\gamma_{it} \triangleq z^{\RAND(\gamma)}_{it}$.
We will provide a performance guarantee for the policy $\DPI(\gamma) \triangleq \DPI(\RAND(\gamma))$.

Our focus is in the regime where $\gamma$ is small (close to 0), and we will see that the theoretical guarantees are stronger when this is the case.
This represents a `budget-constrained' regime; the probability of a patient receiving an intervention is small, or equivalently, the number of interventions given is small relative to the number of patients.
To give intuition on the policy $\DPI(\gamma)$ in this regime, we provide a simple expression for the intervention value in the case where $\gamma \to 0$ and $T \to \infty$ (the latter limit is only assumed to provide a simple expression).
\begin{proposition} \label{prop:z_clean_form}
Fix a patient $i$ and time step $t$.
Then, $\lim_{T \to \infty} \lim_{\gamma \to 0^+} z^{\gamma}_{it}(0) = \tau_i/(p_i + g_i)$.
\end{proposition}



\added{This result is proven as part of \cref{lemma:zgamma} in Appendix~\ref{sec:coupling}.}
\cref{prop:z_clean_form} says that when $T \to \infty$ and $\gamma \to 0$, $\DPI(\gamma)$ orders patients in state 0 by the index $\tau_i/(p_i + g_i)$, and intervenes on those with the highest values.
The index increases with $\tau_i$ and decreases with $p_i$ and $g_i$, which is intuitive. 
$\tau_i$ is the `treatment effect' of an intervention on the probability that the patient switches from state 0 to 1.
The smaller the value of $g_i$, the longer the patient will stay in state 1 once they are there, increasing the `bang-for-the-buck' of an intervention.
As for $p_i$, a larger value means that a patient is likely to move to state 1 \textit{without} an intervention; hence lower priority is given to those with a large $p_i$.

\subsection{Performance Guarantee}\label{ss.perfguar}
Let $\NULL$ be the policy that only takes the null action; i.e.\ $\NULL_t(\bS) = (0, \dots, 0)$ for all $\bS, t$.
As a slight abuse of notation, we use $\OPT, \NULL, \RAND(\gamma)$ and $\DPI(\gamma)$ to also denote the expected reward of those policies.
We now state our main result, a performance guarantee for the quantity $\DPI(\gamma) - \NULL$, compared to $\OPT - \NULL$.
\begin{theorem}[Main Result] \label{thm:approx_ratio_b}
Given an instance of the two-state model, let $\bar{M} = \max_{i \in [N]} \frac{\tau_i (1-p_i-g_i)}{(p_i + g_i)(1-p_i)} > 0$. For any $\gamma \in (0, 1)$,
\begin{align} \label{eq:approx_ratio_b}
\DPI(\gamma)  - \NULL \;\geq\; \frac{1}{2(1+\gamma \bar{M})} \; ( \OPT - \NULL ).
\end{align}	
\end{theorem}

\cref{thm:approx_ratio_b} provides an approximation guarantee for $\DPI(\gamma)$ with respect to the \textit{improvement} over the null policy.
The approximation ratio, $\frac{1}{2(1+\gamma \bar{M})}$ depends on two quantities: $\gamma$ is a parameter of the policy $\RAND(\gamma)$ which is used to compute the intervention values, and $\bar{M}$ is a quantity related to the instance parameters, $\{(p_i, g_i, \tau_i)\}_{i \in [N]}$.
For any fixed instance, the approximation ratio improves as $\gamma$ decreases, approaching 1/2 as $\gamma$ goes to 0.
Therefore, as $\gamma \to 0$, $\DPI(\gamma)$ achieves at least half of the improvement in reward as compared to that of the optimal policy.

With respect to $\bar{M}$, a smaller value implies a stronger guarantee.
Since we assume $p_i, g_i \leq 1/2$, $\bar{M} \leq \max_{i \in [N]} \frac{\tau_i}{p_i + g_i}$.
For $\bar{M}$ to be small, it must be that $p_i + g_i$ is bounded away from 0 for all $i$ --- specifically, patients must not be \textit{completely} sticky, where sticky means that they never switch their state, no matter which state they are in.

\cref{thm:approx_ratio_b} is significantly stronger than and implies the \textit{usual} notion of an approximation result, which is the following:
\begin{corollary}[Weaker result]  \label{thm:weaker_result}
For any problem instance of the two-state model,
$\DPI(\gamma) \geq  \frac{1}{2(1+\gamma \bar{M})} \OPT$.
\end{corollary}
Because a patient can transition from state 0 to 1 \textit{without} an intervention (if $p_i > 0$), the $\NULL$ policy can achieve a significant fraction of $\OPT$.
If it was the case that $\NULL$ is more than half of $\OPT$, then \cref{thm:weaker_result} would be vacuous.
On the other extreme, if an intervention was \textit{necessary} for all patients to transition to state 1
(i.e.\ $p_i = 0$ for all $i$), then it would be that $\NULL = 0$, in which case the result of \cref{thm:approx_ratio_b} is equivalent to that of \cref{thm:weaker_result}.

\added{We make three observations about our main result. First, the complete proof of \cref{thm:approx_ratio_b} can be found in Appendix~\ref{s.app.proof}, but to provide intuition we note that it relies on a key connection of our model to the problem of online allocation of reusable resources. In that problem, a set of items are sold to arriving customers over time, and the items are `reusable' in that the customer returns the item after some period of time.	One can interpret our problem as one of reusable resources, in which patients are analogous to `items' and the item is `available to sell' if the patient is in state 0, and the item has `been sold' if the patient is in state 1. Giving an intervention to a patient can be thought of as `offering the item in an assortment', in which the item will be bought with some probability. For \cref{thm:approx_ratio_b}, we leverage a sample path coupling proof technique of \cite{gong2021online}, who show a 1/2-approximation for the greedy policy for reusable resources. A direct application of this technique results in the weaker approximation result of \cref{thm:weaker_result}. \cref{thm:approx_ratio_b} requires developing a more intricate sample path coupling across the policies $\NULL$, $\OPT$, and $\DPI(\gamma)$.

Second, we believe the approximation guarantees can potentially be made tighter, although we leave that as an open direction. The reason for this is that the inspiration for our proof technique (the coupling idea used by \cite{gong2021online}) was developed for an \emph{adversarial} online allocation problem. In our setting such an assumption would imply that the set of patients who are in state 0 are adversarially chosen at each round. This is not the case since our model is stochastic, not adversarial, but our proof does not leverage this stochastic structure, and hence the bound may be able to be improved.

Third, with the question of tightness in mind, we run synthetic simulations with the two-state model to empirically evaluate the performance of $\DPI$, which we compare to both a random benchmark and the Whittle's index (which can be exactly computed due to the simplicity of the model). The results are included in Appendix~\ref{s.app.synthetic simulations}. Although the Whittle's index is not necessarily optimal (due to heterogeneous arms and the finite time horizon), it serves as a strong benchmark.
We observe that $\DPI$ greatly outperforms the random policy, and it performs almost identically to the Whittle's index.
}


\subsection{Robustness under an Approximate Implementation} \label{s.robustness}
Note that \cref{thm:approx_ratio_b} assumes that the intervention values are known exactly.
In practice, these intervention values must be estimated, as discussed in the intended usage in \cref{s.intended_usage}.
We show that this result is robust, in the sense that a policy that \textit{approximately} implements $\DPI(\lambda)$ also yields a performance guarantee.

Suppose $\ALG$ is an \textit{index policy} using indices $z^{\ALG}_{it}(0)$.
That is, $\ALG$ assigns interventions to the patients in state 0 with the largest value of $z^{\ALG}_{it}(0)$.
Then, we show that if the index values $z^{\ALG}_{it}(0)$ approximate the intervention values $z^{\lambda}_{it}(0)$, then $\ALG$ also admits a performance guarantee.

\begin{theorem} \label{corr:approx}
Suppose $\ALG$ is an index policy that uses index values $z^{\ALG}_{it}(0)$ that satisfies, for all $i$ and $t$,
\begin{align} \label{eq.approxvalues2}
c_1 z_{it}^{\lambda}(0) \leq z^{\ALG}_{it}(0) \leq c_2 z_{it}^{\lambda}(0),
\end{align}
for some $c_1 \leq 1$ and $c_2 \geq 1$.
Then,
\begin{align*}
\ALG - \NULL \;\geq\; \frac{c_1}{c_2} \cdot \frac{1}{2(1+\gamma \bar{M})} \; ( \OPT - \NULL ).
\end{align*}
\end{theorem}

The proof can be found in Appendix~\ref{sec:app:pf_approx}.
This result implies that one does not have to run $\DPI$ \textit{exactly} in order to achieve good performance. 
In practice, one may estimate the intervention values from data --- even if the estimation is not perfect, \cref{corr:approx} ensures a performance guarantee. 



