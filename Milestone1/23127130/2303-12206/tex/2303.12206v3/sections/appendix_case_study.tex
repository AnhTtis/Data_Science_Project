
\subsection{List of features} \label{s.app.list_features}


\textbf{Static features.}
For each patient, we include the following static covariates: weight, height, age, sex, language, county, HIV positive, and extrapulmonary TB.
There were 6 different counties where we used a one-hot encoding, which resulted in 13 features in total.


\textbf{Condensed history.}
For patient $i$ at time $t$, we summarize their history, $H_{it} = (V_{i1}, A_{i1}, \dots, V_{i, t-1}, A_{i,t-1}, V_{it}) \in \bR^{2t-1}$, using the following features:
\begin{itemize}
	\item Verifications: total so far, total percentage, total last week, X days ago for the last $X \in \{1, \dots, 7\}$ days.
	\item Verification/non-verification streaks (how many days in a row a patient verifies / does not verify): current streak, longest streak
	\item Interventions: total so far, total last week, X days ago for the last $X \in \{1, 2, 3\}$ days.
	\item Number of days on the platform, number of days of treatment left.
\end{itemize}
This results in 21 features in total for the condensed history.
A similar structure of features was used in \cite{Boutilier22Improving} which analyzed the same dataset.



\subsection{Simulation Model Details} \label{s.app.simulation}

We briefly describe the double ML method of \cite{chernozhukov2018double}.
Let $Y \in \bR$ be the outcome variable, $T \in \{0, 1\}$ the treatment, and $X \in \bR^{d}$ the observable features.
The model makes the following structural assumptions:
\begin{align*}
Y &= \tau(X) \cdot T + g(X) + \eps, \\ 
T &= f(X) + \eta,
\end{align*}
where $\bE[\eps | X] = 0, \bE[\eta | X] = 0$, and $\bE[\eps \cdot \eta | X] = 0$.
The goal is to estimate the conditional average treatment effect, $\tau(X)$.
We estimate two functions:
\begin{align*}
q(X) = \bE[Y \;|\; X], \quad
f(X) = \bE[T \;|\; X].
\end{align*}
Then, we compute the residuals
\begin{align*}
\tilde{Y} = Y - q(X), \quad \tilde{T} = T - f(X).
\end{align*}
Lastly, we estimate
\begin{align*}
\hat{\tau} = \argmin_{\tau} \bE[(\tilde{Y} - \tau(X) \cdot \tilde{T})^2].
\end{align*}

For the Keheala case study, we used gradient boosting to estimate $q$ and $f$, and we assumed a linear function for $\tau(X) = \theta^\top X$.

Lastly, we cap $\hat{\tau}(s)$ so that the resulting function $f(s, 1)$ is between 0 and 1.
That is, we let $\tau(s) = \max\{\min\{\hat{\tau}(s), 1 - f(s, 0)\}, -f(s, 0)\}$.







\subsection{Details on the Bandit Algorithm} \label{sec:app:bandit}
The pseudocode for the bandit algorithm used in the experiments can be found in \cref{alg:bandit}.
The bandit is initialized with the offline dataset, the same data that was used to train $\DPI$ -- the $\mathbf{S}(a)$  and $\mathbf{V}(a)$ that is an input in \cref{alg:bandit} comes from the offline data. 
The algorithm uses a Thompson Sampling approach, where a parameter is sampled from the posterior, and the intervention is given to those with the highest intervention values with respect to the sampled parameter. 
In our experiments, we used $\sigma^2 = 1/4$ \added{(we tried several different parameters values and chose the best one)}. 

\begin{algorithm}
\caption{Thompson Sampling Bandit Policy}
\label{alg:bandit}
\begin{algorithmic}[1] %
\Require Budget $B$, $\mathbf{S}(a), \mathbf{V}(a)$ for $a \in \{0, 1\}$, noise $\sigma^2$.
	\For{$t=1, \dots, T$}
	\For{$a \in \{0, 1\}$}
		\State $\hat{\beta}(a) = (\mathbf{S}(a)^{\top} \mathbf{S}(a))^{-1} \mathbf{S}(a)^{\top} \mathbf{V}(a)$ 
		\State $\hat{\Sigma}(a) = \sigma^2 (\mathbf{S}(a)^{\top} \mathbf{S}(a))^{-1}$
		\State $\tilde{\beta}(a) \sim \text{Normal}(\hat{\beta}(a), \hat{\Sigma}(a))$
			\Comment Sample parameter from posterior
	\EndFor
	\For{$i=1, \dots, N$}
	\State $\tilde{z}_{it} = (\tilde{\beta}(1)-\tilde{\beta}(0))^{\top} S_{it}$
	\Comment Sampled intervention values for each patient
	\EndFor
	\State Assign interventions to $B$ patients with the largest $\tilde{z}_{it}$ values.
	\For{$i=1, \dots, N$}
		\Comment Update parameters
		\State $V_{i,t+1} = 1 \text{ if patient $i$ verified at $t+1$, otherwise } 0$
		\If{$A_{it} = 1$}
		\State $\mathbf{S}(1) = \text{Append}(\mathbf{S}(1), S_{it})$
		\State $\mathbf{V}(1) = \text{Append}(\mathbf{V}(1), V_{i,t+1})$
		\Else
		\State $\mathbf{S}(0) = \text{Append}(\mathbf{S}(0), S_{it})$
		\State $\mathbf{V}(0) = \text{Append}(\mathbf{V}(0), V_{i,t+1})$
		\EndIf
    \EndFor
    \EndFor
 \end{algorithmic}
\end{algorithm}


\added{We note that action-centered contextual bandits \citep{greenewald2017action} is a variant of Thompson Sampling that has been used in a similar setting (e.g., HeartSteps study of \cite{liao2020personalized}).
However, this method does not readily apply to our setting due to the budget constraint, hence we use \cref{alg:bandit}.
}

\subsection{Details on the QWI algorithm} \label{sec:app:qwi}

\paragraph{Learning the index.}
For a given $\lambda \in \bR$, we consider a process in which every time action 0 is taken, a reward of $\lambda$ is awarded.
We define $Q(S, A; \lambda)$ as the Q-value from state $S$ and action $A$ under this process.
Then, the $Q$-values satisfy:
\begin{align}
	Q(S, A; \lambda) = r(S, A) + (1-A) \lambda - \beta + \bE_{S' \sim P(S, \cdot, A)}[\max_{a'} Q(S', A'; \lambda)],
\end{align}
where $\beta \in \bR$ is the optimal reward.
Then, the Whittle's index for state $s$ is the value $\lambda(s)$ that satisfies
\begin{align}
	Q(S, 0; \lambda(S)) = Q(S, 1; \lambda(S)).
\end{align}

Recall that we have offline samples $\{(S_i, A_i, v_i, \ell_i, S_i')\}$, where $v_i \in \{0, 1\}$ is the outcome of whether the patient verified, $\ell_i \in \bN$ is the number of days the patient has left, and $S_i'$ is the next state of the patient.
Fix a number $\lambda$, and we will aim to learn $Q(S, A; \lambda)$, which represents the future verification rate.
Initialize $Q_0(\cdot, \cdot; \lambda) = 0$.
Let $\cI(S, A) = \{i : S_i = S, A_i = A\}$ be the data points with state $S$ and action $A$.
For $t = 1, 2, \dots$, update the $Q$-values as:
\begin{align} \label{eq:q_iteration}
Q_t(S, A; \lambda) = \frac{1}{|\cI(S, A)|} \sum_{i \in \cI(S, A)} \frac{1}{\ell_i} \big(v_i  + (1-A_i) \lambda + (\ell_i-1) \max_{a'}Q_{t-1}(S_i', A')\big).
\end{align}	
Then, for every state $u$, to find the $\lambda(S)$ such that $Q(S, 0; \lambda(S)) = Q(S, 1; \lambda(S))$, 
we did a binary search across $\lambda$. 
At each iteration of the binary search, for a given value of $\lambda$, we ran the above method \eqref{eq:q_iteration} for $t=100$ iterations.

\subsection{Details on Prominent Features for $\DPI$} \label{s.app.coefficients}
We describe how \cref{tab.coefs} was generated. 
First, we compute $\hat{\theta}_0$ using the least square regression in \eqref{eq:leastsquares}.
Then, for each sample $(S_{it}, A_{it}, y_{it})$ where $A_{it} = 1$, we create a new target $\tilde{y}_{it} = y_{it} - \hat{\theta}_0^{\top} S_{it}$, which simply subtracts off the prediction from the first regression.
Then, the intervention value is essentially the regression of $S_{it}$ onto the new target values.
Instead of performing the usual least-squares, we first normalize each feature so that they have a standard deviation of 1, and then we perform a Lasso regression \citep{tibshirani1996regression}:
\begin{align*} 
	\tilde{\theta} &\in \argmin_{\theta \in \bR^{34}} \bigg( \sum_{i \in N} \sum_{t=T_{\text{s}}(i)}^{T_{\text{e}}(i)}  \bI(A_{it} = 1)(\tilde{y}_{it} - \theta^\top \tilde{S}_{it})^2 + \lambda ||\theta||^2_1 \bigg),
\end{align*}
where $\tilde{S}$ represent the states after column normalization.
We chose the value of $\lambda$ so that the output $\tilde{\theta}$ has five non-zero entries.
These are the features and their respective sign of the coefficient that is shown in \cref{tab.coefs}.











