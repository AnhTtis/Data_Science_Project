In this section, we revisit the full problem (introduced in \cref{s.model}) and conduct numerical experiments to evaluate the performance of $\DPI$ for its intended use case. Our analysis is motivated by our partner organization, Keheala, which operates a digital health platform to support medication adherence among TB patients in highly resource-constraint settings. Here, we first summarize the state of the global TB epidemic and the Keheala behavioral intervention (\cref{ss.keheala}). We then describe our data sources and the validated simulation model we have developed to test outreach policies (\cref{ss.data} and \cref{ss.sim}). Next, we discuss how our policy, as well as some benchmark policies, can be implemented using Keheala's data (\cref{ss.policies}), before presenting our numerical results (\cref{ss.results}).

\subsection{The global TB epidemic and the Keheala intervention}\label{ss.keheala}
TB remains one of the deadliest communicable diseases in the world, causing 1.6 million deaths in 2021. This is remarkable in light of the fact that effective treatment has been available for over 80 years, with the current WHO guidelines recommending a 6 month regimen of antibiotics for drug-susceptible TB and a more intense regiment for drug-resistant strains \citep{World22Global}. A key limiting factor for curbing the epidemic is lack of patient adherence to these treatment regiments, which increases the probability of infection spreading, drug resistance, and poor health outcomes \citep{garfein2019synchronous}. 

Keheala was designed to provide treatment adherence support to TB patients in resource-limited settings. Their platform operates on the Unstructured Supplementary Service Data (USSD) mobile phone protocol, which importantly allows phones without smart capabilities to access the service. Once a patient has enrolled with Keheala, they are meant to self-verify treatment adherence every day, using their mobile phone. In addition, they have access to a range of services. Some are on-demand, for example educational material about TB or leaderboards for verification rates. Others are automatic, such as adherence reminders, which are sent to patients daily (at their pre-determined medication time) in the absence of verification. In addition, the Keheala protocol is to escalate outreach interventions when patients do not self-verify adherence. It states that after one day of non-adherence patients should receive a customized message to encourage resumed adherence and after two days of non-adherence patients should receive a phone call from a support sponsor. While these support sponsors are full-time employees, they are not healthcare professionals. They are members of the local community who have experience with TB treatment and are therefore familiar with the many contributing factors associated with low treatment adherence, such as side-effects, societal stigma against TB patients, and challenges with refilling prescriptions.

The overall effectiveness of Keheala's combination of services was evaluated in a randomized controlled trial (RCT) in Nairobi, Kenya. The trial demonstrated that Keheala reduced unsuccessful TB treatment outcomes—a composite of loss to follow-up, treatment failure, and death—by roughly two-thirds, as compared to a control group that received the standard of care \citep{Yoeli19Digital}. Given this success, Keheala's primary practical objective is to ensure that enrolled patients remain engaged with the platform through adherence verification.

In this paper, we focus on the final level in Keheala's escalation protocol---support sponsors making phone calls to patients. This part of the outreach was organized through populating a daily list of patients who had not verified treatment adherence for 48 hours. Support sponsors had many responsibilities in operating the platform, but would make phone calls to as many patients on the list as possible on a given day. Since hiring support sponsors is a costly aspect of operating the service, Keheala is interested in implementing a more personalized and targeted approach for prioritizing which patients should receive a phone call on a given day. Being able to maintain a similar performance with fewer support sponsors (or equivalently, serve more patients with the same number of support sponsors) is desirable for any future scale-up of the system.








\subsection{Data sources.}\label{ss.data}
Based on the success of the first RCT, the effectiveness of Keheala was further evaluated in a second RCT\footnote{The trial was approved by the institutional review board of Kenyatta National Hospital and the University of Nairobi. Trial participants or their parents or guardians provided written informed consent. The trial’s protocol and statistical analysis plan were registered in advance with ClinicalTrials.gov (\#NCT04119375).} during 2018-2020. The RCT was conducted in partnership with 902 health clinics distributed across each of Kenya's eight regions, representing a mix of rural and urban clinics. The study included four treatment arms and enrolled over 15,000 patients. We obtained data for 5,433 patients enrolled in the Keheala intervention arm (other arms aimed to independently test specific components of the Keheala intervention). 

As part of the RCT, the study team collected socio-demographic information from all patients. This information includes static covariates such as age, gender, language preferences, location, as well as limited clinical history (see \cref{s.app.list_features} for a full list). In addition, Keheala collected engagement data about each patient during their enrollment in the service. This includes whether a patient verified on a given day, how many reminders they received, and whether they were contacted by a support sponsor. 

After filtering out patients with missing information or not enough data, we conducted all our analysis on 3594 patients. The average patient was enrolled on the platform for 118 days. On an average day, 608 patients were enrolled and 210 of those were eligible for a support sponsor call according to the protocol (i.e., having not verified treatment adherence for the preceding 48 hours). The support sponsors, employed by Keheala, had a range of responsibilities in operating the platform, including making outreach phone calls to the eligible patients. 
The average number of calls made per day was 25.5. Hence, in our analysis, we use a budget of $B$ = 26 as our main point of comparison.
	

\subsection{Simulation Model.}\label{ss.sim}
We build a simulation model that we use to estimate the counterfactual outcomes of different outreach approaches.
The simulator is effectively represented by a single function, $f(S, A) \in [0, 1]$, which denotes the probability that a patient in state $S$ with action $A$ verifies in the next time step.
This function is used to simulate one step transitions for every patient.
We first describe the state space of the patients, describe the exact simulation procedure, and then discuss how we learn $f$ from data and validate the simulator.

\subsubsection{Patient state space.}\label{ss.simstatespace}
For patient $i$, let $X_i \in \bR^{13}$ be their static covariates. 
Let $V_{it} \in \{0, 1\}$ denote whether patient $i$ verified at time $t$, and let $A_{it}\in \{0, 1\}$ denote whether the patient received the intervention at time $t$.
Let $H_{it} = (V_{i1}, A_{i1}, \dots, V_{i, t-1}, A_{i,t-1}, V_{it}) \in \bR^{2t-1}$ be the history of verifications and interventions up to time $t$.
We define a \textit{condensed} history $\tH_{it} \in \bR^{21}$ by summarizing the history $H_{it}$ into 21 features, aiming to capture as much relevant information as possible.
The condensed history contains the patient's recent and overall behavior.
For statistics such as the number of times the patient verified and the number of interventions they have received, we aggregate them over the past week, as well as in total.
We also include information on their verification and non-verification streaks, as well as how long they have been in the platform.
See \cref{s.app.list_features} for a full list of these features.
Then, we define the state of patient $i$ at time $t$ to be $S_{it} = (X_i, \tH_{it}) \in \bR^{34}$.

\subsubsection{Simulation procedure.} 
Given $f$ and an intervention policy $\pi$, we `mimic' the RCT by simulating patient behavior day by day. 
In total, we simulate $T=700$ time steps, where each $t \in [T]$ corresponds to one day between April 2018 to March 2020. We let $T_{s}(i)$ and $T_{e}(i)$ denote the starting and ending time steps that patient $i$ was enrolled in Keheala. Each patient $i$ is then introduced into the system at time $t=T_s(i)$, and removed at time $t=T_e(i)$. We use the observed data from the RCT for their first 7 days in the system to initialize their state. 
Then, in each time period, given the set of patients that were active in the RCT for more than 7 days, we use a policy $\pi$ on these patients to determine who receives sponsor outreach. If a patient $i$ was in state $S_{it}$ at time $t$ and the policy $\pi$ chose action $A_{it}$, we let $V_{i, t+1}$ be 1 with probability $f(S_{it}, A_{it})$, and 0 otherwise (where the randomness is independent across patients and time steps).
Finally, we use $V_{i, t+1}$ to update their state for the next time step. 

\subsubsection{Estimating the $f$ function.} \label{s.learnf} 
Using the state space $\cS$ as described above, we construct the function $f: \cS \times \{0, 1\} \to [0, 1]$ using data from the RCT. Specifically, we learn the two functions $f(S, 0)$ and $f(S, 1)$ separately. For $f(S, 0)$, we train a gradient boosting classifier on the dataset $\{(S_{it}, V_{i,t+1})\}_{i \in N, t \in [T] : A_{it} = 0}$, using $V_{it}$ as the outcome variable. For $f(S, 1)$, we write the function as $f(S, 1) = f(S, 0) + \tau(S)$ and we learn $\tau(S)$ using the double machine learning method of estimating heterogeneous treatment effects \citep{chernozhukov2018double}. See \cref{s.app.simulation} for details on implementing this method.

\subsubsection{Train and test split.} \label{s.traintestsplit}
Importantly, we use a different set of patients to estimate the $f$ function (and to run our simulations) from the set of patients we use to train our policies. In particular, we randomly split all patients from the RCT into two groups, which we call \textit{train} and \textit{test}. We use the \textit{test} set to estimate the $f$ function that forms the basis for the simulation model. We keep the \textit{train} set of patients separate and use it to train policies (see \cref{ss.policies}). This ensures that the policies we evaluate are not learned off of the same dataset that was used to learn the simulator. 
The simulation itself uses the test patients, and we duplicated each patient so that we maintain a similar total number of patients as in the original study.


\subsubsection{Simulation validation.}
We validate the performance of the simulator on a \textit{different} intervention policy than the simulator was trained on, by leveraging the fact that there was variability in the number of interventions given throughout the RCT.
In particular, the average number of interventions given during the first half of the RCT was around double of that of the latter half (45.9 vs. 21.4),
and this variation induces a change in the intervention assignment policy.
Then, dividing the data into halves produces two datasets that are generated using effectively different intervention policies.

To validate the simulator, we use the method from \cref{s.learnf} to learn $f$ using the first dataset, and then validate its performance on the second dataset. 
Using this procedure, the AUCs on the second dataset for $f(S, 0)$ and $f(S, 1)$ were 0.918 and 0.745, respectively.
We also check the calibration of both of these functions, by grouping the samples into bins based on their predicted probability of verifying the next day, and checking whether their actual verification rates.


We group the samples based on the simulated probability of verification into bins with a 10\% range, and we compute the expected calibration error (ECE) \citep{naeini2015obtaining}. For bin $i$, let $o_i$ be the true fraction of positive instances in bin $i$, $e_i$ be the mean of the predicted probabilities of the instances in bin $i$, and $N_i$ be the number of samples in bin $i$. Then, the ECE is defined as
\begin{align}
\text{ECE}	 = \frac{1}{N} \sum_{i=1}^{10} N_i |o_i - e_i|,
\end{align}
where $N$ is the total number of samples.
The expected calibration error was 0.0066 for $f(S, 0)$ and 0.0308 for $f(S, 1)$.
Figure~\ref{f.calibration} displays these bins.

These results demonstrate that the simulator has good performance in mimicking patient behavior. 
As expected, the AUC and the ECE is worse for $f(S, 1)$ compared to $f(S, 0)$; 
this is due to the \textit{significantly} fewer number samples with an intervention in the training data, as well as the increase in variance of doing off-policy estimation.
The training data used for $f(S, 0)$ had 300K samples, while the one used for $f(S, 1)$ had 4.5K samples.
For $f(S, 1)$, the calibration is slightly off for samples with a high probability of verification (bins 0.7-0.9); however, we note that the 0.7-0.9 bins only contain 11.3\% of all samples for $f(S, 1)$.


\begin{figure}[h]%
	\centering
	\subfloat[Calibration for $f(S, 0)$]{{\includegraphics[width=0.48\linewidth]{figs/bar_pred0} }}%
	\subfloat[Calibration for $f(S, 1)$]{{\includegraphics[width=0.48\linewidth]{figs/bar_pred1} }}%
	\vspace{2mm}
	\caption{Calibration plots for $f(S, 0)$ and $f(S, 1)$ for simulation validation. 
		We group the samples based on the simulated probability of verification
		into bins with a 10\% range, which we label by the lower number. 
		For example, the 0.3 bin on the x-axis represents the samples whose probability of verification according to $f$ is in $[0.3, 0.4)$; hence we should expect the actual number of verifications of those samples to be close to 0.35.}
	\label{f.calibration}%
	\vspace{-2mm}
\end{figure}



\subsection{Outreach Policies and Experimental Design}\label{ss.policies}
Using the simulation model described above, we compare the performance of three main policies. For each policy, we vary the budget for outreach interventions per day between 10 and 40. As mentioned before, the average number of sponsor outreaches during a given day of the trial was 26. Importantly, we restrict all policies so that they can only provide an outreach to patients who have not verified for at least two days in a row. This is because that was what was done in the RCT, hence there is no data for how an outreach affects behaviors for those who do not meet this criterion (thus we would not be able to accurately evaluate policies that do not follow this restriction).

We note that attaining improved performance with lower outreach capacity is particularly important for the resource-limited setting at hand as it speaks to the performance achievable during a future scale-up of the system, in which the ratio of patients to support sponsors is likely to be much higher. 
Next, we describe the implemented policies.

\subsubsection{$\DPI$ for Keheala.} 
The first step in operationalizing $\DPI$ is defining the state space for each patient. For this, we use the same condensed state space as described in \cref{ss.simstatespace}, i.e., we define the state of patient $i$ at time $t$ to be $S_{it} = (X_i, \tH_{it}) \in \bR^{34}$ (a full list of these features is included in \ref{s.app.list_features}). Importantly, we note that all of the features of this state space are observable to Keheala at any time $t$, once a patient has been enrolled on the platform for seven days. 

The second step is estimating the $\hz_{it}(S_{it})$ score for each patient at each time period, which is ultimately used to prioritize patients. 
As before, we let $T_{s}(i)$ and $T_{e}(i)$ be the starting and ending time steps that patient $i$ was enrolled in Keheala. Using this notation, we can represent the future verification \emph{rate} for patient $i$ at time $t$ by $y_{it} = \frac{1}{T_{\text{e}}(i)-t} \sum_{r=t+1}^T V_{ir}$.
Then, the data from the RCT can be written in the form $\{(S_{it}, A_{it}, y_{it})\}_{i \in [N], t \in \{T_{s}(i), \dots, T_{e}(i)\}}$, and we can estimate the function $q_{it}^{\baseline}(S, A)$ using this data.
In our implementation, we use a linear function approximation for the verification rate, assuming the form 
\begin{align*}
q_{it}^{\baseline}(S, A) = \langle \theta_A, S \rangle \cdot (T_{\text{e}}(i)-t),
\end{align*}
for each of the two actions $A \in \{0, 1\}$.
The $\langle \theta_A, S \rangle$ term represents the future verification rate, and $T_{\text{e}}(i)-t$ represents the number of days left; combined, $q_{it}^{\baseline}(S, A)$ represents the total number of future verifications.
We note that the state contains information regarding the number of days the patient has been enrolled in Keheala, hence the verification rate is also a function of the time step.

We estimate $\theta_a$ using least squares with an $\ell_2$ regularizer:
\begin{align} \label{eq:leastsquares}
	\hat{\theta}_a &\in \argmin_{\theta \in \bR^{34}} \bigg( \sum_{i \in N} \sum_{t=T_{\text{s}}(i)}^{T_{\text{e}}(i)}  \bI(A_{it} = A)(y_{it} - \theta^\top S_{it})^2 + ||\theta||^2_2 \bigg).
\end{align}

Finally, we compute a patient's estimate of their intervention value at time $t$ as
\begin{align*}
	\hz_{it}(S_{it}) = \langle \htheta_1 - \htheta_0,S_{it} \rangle \cdot (T_{\text{e}}(i)-t), 
\end{align*}
and the resulting policy is to give the intervention to up to $B$ patients with the highest positive $\hz_{it}(S_{it})$ values.





\subsubsection{Bandit.}
The bandit policy aims to choose patients with the highest increase in the probability of next-day verification, using a linear contextual bandit model. In terms of the two-state model from \cref{ss.2statemodel}, the goal is to choose patients with the highest value of $\tau$.
We essentially use the same state space and linear model as was used for $\DPI$, except that the outcome variable is next-day verification, rather than total future verifications.
We first learn a prior using the offline data, and then we run a Thompson sampling style policy, which continually updates the policy with online data.

Specifically, we assume the linear form $V_{i,t+1} = \langle \beta_a, S_{it} \rangle$ for action $a \in \{0, 1\}$, with unknown parameters $\beta_0, \beta_1 \in \bR^{34}$.
The prior on $(\beta_0, \beta_1)$ is initialized as the output of a least-squares regression using the offline data, the same data that was used to train $\DPI$.
At each time step, $(\tilde{\beta_0}, \tilde{\beta_1})$ is sampled from the posterior. 
Then, the policy chooses the $B$ patients with the highest value of $\langle \tilde{\beta_1}, S_{it} \rangle - \langle \tilde{\beta_0}, S_{it} \rangle$.
After the outcome is observed at each time step, the posterior is updated accordingly.
The detailed description on the algorithm can be found in Section~\ref{sec:app:bandit}.

This policy makes use of strictly more data than $\DPI$, since $\DPI$ only uses the offline data. 
In the results, we confirm that this policy indeed learns myopic rewards correctly.
Therefore, this is a very strong benchmark algorithm for optimizing myopic rewards.


\subsubsection{Whittle's index (QWI).} \label{sec:qwi_description}
The next benchmark is the Whittle's index.
The advantage of this method compared to the bandit benchmark is that is is non-myopic.
However, the downside is that computing the Whittle's index requires the model to be known.
To implement Whittle's index in our setting where the model is unknown, we leverage the recent work of \cite{avrachenkov2022whittle} who propose a Q-learning approach to learn the Whittle's index, which we refer to as $\QWI$.
$\QWI$ is an online learning method that simultaneously learns the Q-values as well as the Whittle's index for each state.

There are two main challenges in implementing $\QWI$ in our setting. The first is that the algorithm is an online learning method, and the second is that it requires a finite state space as it learns the Whittle's index for each state separately. 
For the first point, we adapt the algorithm from \cite{avrachenkov2022whittle} to an offline setting so that it can use the same data that is used to train $\DPI$.
For the second point, we define a smaller, finite state space so that $\QWI$ can be implemented.
We define a patient's state at a point in time to be a 3-tuple $(s_1, s_2, s_3)$, where $s_1$ represents the number of times the patient verified in the last week, $s_2$ is the patient's historical total verification rate, and $s_3$ is the number of times that the patient received an intervention in the last week. The values of each of these terms are bucketed into a small number of bins (3 bins for $s_1$ and $s_3$, 5 bins for $s_2$), resulting in 45 states in total. Specifically, the bins for both $s_1$ and $s_2$ are $0, 1$ and $2-7$. For $s_2$, the bins are $0-1\%$, $1-5\%$, $5-20\%$, $20-45\%$, and $45-100\%$. The bin values were chosen to balance the number of samples in each bin. 
This results in 45 states in total.


Based on this state space, we learn the Whittle's index, $\lambda(s) \in \bR$, for each state $s$.
Then, at each point in time, $\QWI$ chooses the patients in states with the highest Whittle's index to give the intervention to.
Further details of the learning algorithm is deferred to Appendix~\ref{sec:app:qwi}. 

\added{
We note that the state space for $\QWI$ is different from that of $\DPI$, due to the computational limitation of $\QWI$. In \cref{ss.state_space_results}, we run simulations where we modify the state space for $\DPI$ to be the same as $\QWI$, so that we can isolate the performance difference to the algorithm rather than the state space.
That said, we believe that the ease of working with an infinite state space is a substantial advantage of $\DPI$.
}




\subsubsection{Baseline.}
The $\baseline$ policy approximates the heuristic followed by Keheala in the two RCTs that have been implemented. In both cases, the protocol was that patients were added to the support sponsor call queue after not verifying for 48 hours. As a result, the order of patients in the queue is effectively random, determined by a combination of their designated medication time (which prompts automated reminders to take the medicine and verify) and the timing of their self-verification. We approximate the resulting outreach policy by selecting $B$ patients out of all those who have not verified for 48 hours, at random. 

\added{
The $\RAND$ policy used in the theoretical results is aimed to be an approximation of $\baseline$. 
The discrepancy between these policies is solely for technical convenience, 
as $\RAND$ is easier to analyze due to the independence across patients.
}


\subsubsection{Null policy.}
\added{
Lastly, we simulate the $\NULL$ policy which does not give any interventions. 
We note that this policy does not depend on the budget parameter $B$.
}

\subsection{Results}\label{ss.results}

The results are shown in Figure~\ref{fig:main_results}. 

\begin{figure}[h]
\begin{center}
\vspace{-3mm}
  \includegraphics[width=1\linewidth]{figs/results_June14_whittle}
  \caption{Average overall verification rate over 50 runs for each policy and budget. 
  The overall verification rate for the $\NULL$ policy was 54.2\%.
  The shaded region indicates a 95\% confidence interval. The star represents the operating point for Keheala.}
  \label{fig:main_results}
\vspace{-6mm}
\end{center}
\end{figure}

\subsubsection{Overall performance.}
The average performance for each budget and policy over 50 runs are shown in Figure~\ref{fig:main_results}, which shows that $\DPI$ clearly outperforms the other policies over a wide range of budget values.
For a practical interpretation of the results, consider \textsf{Baseline} at a budget of 26, the policy and budget that Keheala was operating during the RCT, which results in an overall verification rate of 62.0\%.
By using less than \textit{half} of the budget, $B=12$, $\DPI$ achieves the same verification rate at 62.2\%.
As the costliest aspect of Keheala's system is in hiring staff to provide the interventions, these results imply that they can cut these costs by half to achieve the same outcome.
\added{
The $\NULL$ policy (no interventions) results in a verification rate of 54.2\% (we did not plot this for readability of the figure).
One can interpret this number as a reference benchmark to compare the effectiveness of interventions.
When the budget is 26, $\baseline$ improves over $\NULL$ by 14.6\%, while $\DPI$ improves over $\NULL$ by 20.3\%. 
Therefore, $\DPI$ improves the effectiveness of the interventions over $\baseline$ by 38.3\%.
}

Additionally, we observe that the improvement of $\DPI$ compared to the other policies is especially substantial for smaller budgets. 
This implies that when the number of patients that can be targeted is small, $\DPI$ can correctly identify the set of patients to target that result in the largest gains.
This is especially valuable for scaling up the system.
Indeed, if Keheala wanted to expand to include more patients without linearly increasing their staff costs, then the ratio of budget to the number of patients would decrease, resulting in the regime where $\DPI$ offers major improvements.

The fact that the performance of $\bandit$ policy improves over $\baseline$ as the budget increases is caused by the increase in relevant data.
Note that the number of interventions is small ($\sim 26$) relative to the number of patients in the system at once ($\sim 600$), implying that the number of data points with $A=1$ is much smaller than that of $A=0$. 
Therefore, the main bottleneck in estimation is learning patient behaviors after receiving an intervention.
As the budget increases, the $\bandit$ has access to more data from patients with an intervention, and hence is able to improve its learning. 

$\QWI$ has a slightly inconsistent performance curve relative to the other policies.
Its performance is always better than or similar to $\baseline$, but compared to $\bandit$, it over-performs in the mid-budget regime, but under-performs as the budget increases.
We dive deeper into the types of patients each policy targets in \cref{sss.targetedpatients}, where we provide an explanation for this behavior.  


\added{
One factor that may be contributing to the poor performance of $\QWI$ is the state space that is used.
$\QWI$ uses a discretized state space as described in \cref{sec:qwi_description}, different than the infinitely-sized state space used by $\DPI$.
In \cref{ss.state_space_results}, we run additional experiments where we run $\DPI$ with the same state space as $\QWI$, so that the performance differences can be purely attributed to the algorithm rather than the state space. 
}


\subsubsection{Patient-level verification rates.} 
The overall number of verifications increases under $\DPI$, but how do these rates get impacted at the patient-level?
Fixing the budget to be 26, we compute the verification rate of \textit{each} patient, and we examine the distribution of these patient-level rates. \added{
In Figure~\ref{fig:diff_vrates_all}, we plot how the distribution of patient verification rates shift under the $\DPI$, $\bandit$, and $\QWI$ algorithms, compared to $\baseline$.
We see that under $\DPI$, the distribution shifts in a way that there are fewer patients with verification rates under 50\%, and more patients with a verification rate higher than 50\%.
We see a similar phenomenon for $\QWI$, but the magnitude of the shift is smaller.
$\bandit$ also observes an increase in $>50\%$ verification rates, but there is also an increase of those with very low (0-10\%) verification rates.
These results for $\DPI$ represent a desirable type of shift, where main improvement of $\DPI$ comes from an increase in the number of patients with a high verification rate.
We also provide absolute numbers in \cref{tab.verification_rates}, where we show the percentage of patients whose verification rate is higher than 50\% and 70\% for each of the four algorithms.
Under $\DPI$, the number of patients whose verification rate is above 50\% and 70\% increased relatively by 6.7\% and 5.2\% respectively compared to $\baseline$.
}


\begin{figure}
\centering
\begin{subfigure}{.47\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/2024_dpi-keheala}
  \caption{Comparing $\DPI$ to $\baseline$.}
\end{subfigure}%
\begin{subfigure}{.47\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/2024_bandit-keheala}
  \caption{Comparing $\bandit$ to $\baseline$.}
\end{subfigure} \\
\begin{subfigure}{.47\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figs/2024_qwi-keheala}
  \caption{Comparing $\QWI$ to $\baseline$.}
\end{subfigure}
\caption{
\added{
Differences in the distribution of patient verification rates compared to $\baseline$. 
  The bins represent the difference in the number of patients whose overall verification rate is between $0-10\%$, $10-20\%, \dots, 90-100\%$.
For example, the first bin in (a) shows that there were 28 fewer patients whose verification rate was between 0 and 10\% under $\DPI$, compared to $\baseline$.
 There were 3594 patients in total, and the budget was fixed at 26.
 }
}
  \label{fig:diff_vrates_all}
\end{figure}


\begin{table}[h]
\TableSpaced %
\caption{\added{
The average percentage of patients whose verification rate was over 50\% and 70\% across the four algorithms.
 There were 3594 patients in total, and the budget was fixed at 26.
}} \label{tab.verification_rates}
\vspace{2mm}
\begin{center}
\begin{tabular}{@{}c|cccc@{}}
\toprule
\% patients with \\ verification rate      & \quad $\baseline$ \;  & $\DPI$ & $\bandit$& $\QWI$ \\ \midrule
$\ge 50\%$  &  61.7\%   & 66.1\%  & 63.8\% & 63.9\% \\ 
$\ge 70\%$  &  38.2\%  & 40.2\%  & 39.7\%  & 39.5\% \\ \bottomrule
\end{tabular}
\end{center}
\end{table}



\subsubsection{Description of the targeted patients.} \label{sss.targetedpatients}
In \cref{tab.stats}, we fix the budget to be 26 and we show statistics regarding the state of the targeted patients for each of the four policies.
For example, under $\baseline$, on average, the patient that received an intervention had a treatment effect of 8.8\% with respect to the probability that they will verify the next day.
8.8\% is the `true' average treatment effect, in the sense that the numbers that are averaged are taken directly from the simulation model.


\begin{table}[h]
\TableSpaced %
\caption{
Average statistics of the state of patients who were given an intervention, across the three policies that were run for $B=26$.
(a) is the average value of $f(x, 1) - f(x, 0)$, the increase in probability that the patient verifies the next day when they are given an intervention. 
(b) is the average $f(x, 0)$, the probability that a patient verifies the next day \textit{without} an intervention. 
(c) is the average number of remaining days the patient will be on TB treatment for.
} \label{tab.stats}
\vspace{2mm}
\begin{center}
\begin{tabular}{@{}ccccc@{}}
\toprule
                                                 & $\baseline$ & $\DPI$ & $\bandit$ & $\QWI$ \\ \midrule
\multicolumn{1}{l}{(a) \added{$f(x, 1) - f(x, 0)$}}  & 8.8\%   & 10.6\%    & 13.2\%    & 6.9\%    \\ 
\multicolumn{1}{l}{(b) \added{$f(x, 0)$}}   & 18.2\%  & 22.2\%  & 35.2\%   & 12.2\%      \\ 
\multicolumn{1}{l}{(c) Days on TB treatment remaining} & 68.3     & 109.3   & 92.4    & 69.7      \\ \bottomrule
\end{tabular}
\end{center}
\end{table}



Statistic (a) represents exactly what the $\bandit$ policy optimizes for, the increase in probability of the patient verifying the next day.
The fact that $\bandit$ yields the highest value confirms that indeed, the policy correctly learns what it is supposed to learn.
$\DPI$ chooses patients with a higher one-step treatment effect than $\baseline$, but lower than that of $\bandit$.
Then, the fact $\DPI$ outperforms $\bandit$ in terms of overall verification implies that a myopic strategy of looking only one step ahead is not sufficient.
The next two statistics shed light on why this may be.


Statistic (b) represents the probability that the targeted patient would have verified anyway without an intervention, and we see that the $\bandit$ targets patients with a much higher verify probability than the other policies. 
We plot the entire distribution of this quantity in Figure~\ref{fig:base_probs}, where we see that $\bandit$ often targets those with a relatively high probability ($>45\%$), while $\DPI$ targets those with a relatively low probability ($<15\%$).
This may contribute to the improved performance of $\DPI$, and the reasoning for this can be seen through the two-state model from \cref{ss.2statemodel}, where statistic (b) corresponds to the parameter $p$.
If two patients have the same values of the parameters $g$ and $\tau$ but differing values for $p$, the intervention value is higher when $p$ is smaller (see \cref{prop:z_clean_form}).
This is because the patient with a high value of $p$ is more likely to switch to state 1 at the current time step as well as all future time steps.
As an extreme example, for a patient with $p=0$, they \textit{need} an intervention to switch to state 1, whereas a patient with $p>0$ may switch to state 1 (either now or in the future), without an intervention.
Therefore, an intervention is more likely to be helpful for those with a smaller value of $p$, which $\DPI$ targets.

On the other hand, $\QWI$ takes the above strategy to an extreme, where it targets those with a very low probability of verifying (12.2\%), but on average these patients also do not have a high next-day treatment effect (6.9\%).
This may explain the inconsistent behavior of $\QWI$ as the budget increases. 
The strategy of targeting these patients with a low verify probability and a low treatment effect is reasonably effective in the mid-budget regime; however, as the budget increases, one may also need to judiciously target other types of patients, which $\QWI$ does not do. 





\begin{figure}[h]
\begin{center}
\vspace{-5mm}
  \includegraphics[width=0.54\linewidth]{figs/base_probs_June2024_2}
\vspace{-2mm}
  \caption{
  Histogram of the value of $f(x, 0)$ of targeted patients, the probability that the patients would verify without an intervention.
  This is the entire distribution of the statistic (b) in \cref{tab.stats} for $\bandit$ and $\DPI$.}
  \label{fig:base_probs}
\vspace{-6mm}
\end{center}
\end{figure}

Lastly, statistic (c) is the average number of days a targeted patient has remaining on the platform.
If an intervention positively affects patients for all of their future time steps, then targeting those with longer time left in the system would result in higher benefits. 
The results show that $\DPI$ targets those with the longest days of treatment left.

\subsubsection{Prominent features for $\DPI$.}
\cref{tab.coefs} displays the five most predictive features of the intervention values that $\DPI$ uses to target patients.
These features were found by using Lasso regression with a tuned parameter -- see Section~\ref{s.app.coefficients} for details on the method used.
The results show that the intervention value is lower when the number of previous interventions is higher (first two features), which is intuitive since patients may become fatigued and less receptive when there are too many interventions.
The intervention value is lower when the patient's past verifications is higher (third and fourth features). This is consistent with the analysis in \cref{tab.stats}, where $\DPI$ targets those with a smaller value of $f(x, 0)$.
Lastly, the intervention value increases for patients who are older.


\begin{table}[h]
\TableSpaced %
\caption{
The most predictive features of higher intervention values for $\DPI$, as well as the sign of their coefficient. 
} \label{tab.coefs}
\vspace{2mm}
\begin{center}
\begin{tabular}{@{}lc@{}}
\toprule
\; Feature & Sign of Coefficient  \\ \midrule
\; Interventions: total number & $-$ \\ 
\; Interventions: \# previous week &  $-$ \\
\; Verifications: overall percentage &  $-$ \\
\; Verifications: \# previous week & $-$ \\
\; Age & $+$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\subsection{\added{Robustness of State Space Representation}} \label{ss.state_space_results}
\added{One of the factors attributing to the performance gap between $\DPI$ and $\QWI$ is the differences in state space representation. $\QWI$ requires a finite state space and its computation time scales with the number of states. Hence, we use a relatively small state space for $\QWI$ for our numerical experiments. The ease of using a larger and infinite size state space is an inherent advantage of $\DPI$ over $\QWI$; however, in order to isolate the performance difference caused by the algorithm itself, we run $\DPI$ using the same state space as $\QWI$.

We try two variants of $\DPI$ that differ in the state space used:
\begin{itemize}
	\item \textsf{DecompPI-3}: This uses the same three features used for $\QWI$ (number of times verified in the last week, total historical verification rate, and number of interventions received in the last week), but these features are \textit{not discretized}, and hence the size of the state space is still infinite.
	\item \textsf{DecompPI-3-discrete}: This uses the exact same state space as $\QWI$ --- three features that are discretized in the same way, resulting in 45 states.
\end{itemize}


\begin{figure}[h]
\begin{center}
\vspace{-3mm}
  \includegraphics[width=1\linewidth]{figs/2024_May13_results3D}
  \caption{Average overall verification rate over 50 runs for each policy and budget. The shaded region indicates a 95\% confidence interval. The star represents the operating point for Keheala.}
  \label{fig:DPI3}
\vspace{-6mm}
\end{center}
\end{figure}

The performance of these two algorithms, along with the original $\DPI$ and $\QWI$ policies are shown in Figure~\ref{fig:DPI3}.
The policies \textsf{DecompPI-3-discrete} and $\QWI$ use the exact same state space, and we see that the former consistently outperforms the latter.
This comparison isolates the performance gap induced by the \emph{algorithms}, and the results provide robust evidence on the strength of $\DPI$.
Lastly, we see that \textsf{DecompPI-3} consistently has a strong performance, comparable to that of $\DPI$.
This demonstrates the robustness of $\DPI$ with respect to the feature space, and it also exemplifies the benefit of employing an infinite state space compared to a discretized one.  
}













