\section{Introduction}\label{sec:introduction}


\begin{figure}[!t]
\centerline{\includegraphics[width=0.9\linewidth]{figures/lam_output_examples.pdf}}
\caption{We used ChatGPT (based on GPT-4) as a medical assistant, and queried it for medical advice. Its capability of recalling prior conversation and being able to contextualise a patient's past medical history shows its promising use in medical areas beyond being a simple symptom checker.}
\label{fig:lam_output_examples}
\end{figure}



\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{figures/model_paras.pdf}}
\caption{An overview of general LAMs, presented with their number of parameters and the development timeline. The figure is divided into four sections, from bottom to top, each section showing 1) large language models (LLMs); 2) large vision models (LVMs); 3) large audio models (LAudiMs); and 4) large multi-modal models (LMMs). The sizes of the circles indicate the relative sizes of LAMs.}
\label{fig:model_paras}
\end{figure}


\IEEEPARstart{T}{he} introduction of ChatGPT~\cite{chatgpt2022} has triggered a new wave of development and deployment of Large AI Models (LAMs) recently. Although groundbreaking, the AI community has in fact started creating LAMs much earlier, and it was the seminal work that introduced the Transformer model~\cite{vaswani2017attention} back in 2017 that accelerated the creation of LAMs.

The recent advances in data science and AI algorithms have endowed LAMs with strengthened \textit{generative} and \textit{reasoning} capabilities, significantly distinguishing them from early deep models. As shown in Fig.~\ref{fig:lam_output_examples}, we conversed with ChatGPT and queried it for medical advice, for which it responded with sound general medical knowledge, memory of prior conversation, and a proper level of prudence.



The LAMs that this article discusses are mainly foundation models~\cite{bommasani_opportunities_2022}. However, this article also provides a retrospective of the recent LAMs that are not necessarily considered foundational at their current stage, but are seminal in advancing the future development of LAMs in the fields of biomedicine and health informatics. 



Fig.~\ref{fig:model_paras} shows the timeline of the development of LAMs over the past five years. As shown in Fig.~\ref{fig:model_paras}, the number of parameters of most LAMs reaches beyond billions. The sizes of LAMs developed in the natural language domain predominately outsize those of other domains.  


% One of the premises of developing LAMs is having a massive amount of pretraining data. Fig. shows the sizes of the datasets that have been used to pre-train LAMs. \textcolor{red}{(in the figure, better to identify those bio datasets, or we need a figure to list current data modalities in biomedicine and healthcare)}

One notable bottleneck of developing supervised medical and clinical AI models is that they require annotated data at scale for training a well-functioning model. However, such annotations have to be conducted by domain experts, which is often expensive and time-consuming. This causes the curation of large-scale medical and clinical data with high-quality annotations to be challenging. However, this might not be a bottleneck for LAMs, as they can leverage self-supervision and reinforcement learning in training, relieving the annotation burden and workload of creating large-scale annotated datasets. With the ever-increasing proliferation of medical internet of things such as pervasive wearable sensors, medical and clinical history such as electronic health records (EHRs), prevalent medical imaging for diagnoses such as computed-tomography (CT) scans, the growing genomic sequence discovery, and more, the abundance of biomedical, clinical, and health data fosters the development of the next generation of AI models in the field, which are expected to have a large capacity for modeling the complexity and magnitude of health-related data, and actively engaging and assisting clinical and medical decision making.    


Despite the homogeneity of the model architecture (current LAMs are primarily based on Transformer~\cite{vaswani2017attention}), LAMs inherently are strong learners of heterogeneous data, which are common in the settings of biomedicine and healthcare. Fig.~\ref{fig:data_modalities} shows common biomedical and healthcare data modalities. The multi-modal nature of biomedical and health data provides the natural and promising ground for fostering LAMs in this field.
 

Albeit inspirational, LAMs still face challenges and limitations, and the rapid rise of LAMs brings new opportunities as well as potential pitfalls. This article aims to provide an up-to-date comprehensive review of the recent development of LAMs, with a particular focus on their impacts on the biomedical and health informatics community. The remainder of this article is organized as follows: Section~\ref{sec:background_large_ai_model} describes the background of LAMs in general domains, such as natural language processing (NLP) and computer vision (CV); Section~\ref{sec:applications} discusses current progress and possible applications of LAMs in key sectors of health informatics; Section~\ref{sec:challenges} discusses challenges, limitations and risks of LAMs; Section~\ref{sec:future} points out some potential future directions of advancing LAMs in health informatics, and Section~\ref{sec:conclusions} concludes.  



% \textcolor{red}{Shall we call it Foundation AI Model (FAIM). Foundation Language Models, Foundation Vision Models, Foundation Language-Vision Models, Other Foundation Models.}



% \begin{enumerate}
%     \item \textcolor{red}{refer to ChatGPT and InstructGPT's technical details to refine future outlook and technical descriptions of this paper. They still contain techniques such as human supervision and reinforcement learning.}
%     \item \textcolor{red}{shall we abbreviate Large AI Model as (LAM)??}
%     \item \textcolor{blue}{we need a definition of what big ai model is, and the criteria we used to select which big ai models should be included in this paper.}
%     \item \textcolor{red}{generate some healthcare-related AIGC, e.g. use ChatGPT/StableDiffusion to generate medical AI contents and put them in the introduction for illustration. IDEA-1: let's ask ChatGPT to write a public health policy to contain the spread of an emerging infectious disease. IDEA-2: using a LVM?}  
%     \item a timeline figure showing the time that each seminal big ai model occured
%     \item a figure showing the growing size of the developed big ai models, measured by their parameters
%     \item a figure showing the number of big ai model papers published in the past few years (5 years?)
%     \item mention foundation models (\textcolor{red}{need a reference, track who first defined foundation models, stanford?}), but also indicate this review goes beyond foundation models as e.g., some models certainly cannot be viewed as foundation models like CheXNet.
%     \item say something about how big models can be used in tackling the challenges of a pandemic such as COVID
%     \item better to have a chapter/paragraph to describe the multimodal nature of clinical and healthcare data.
%     \item \textcolor{red}{let's have a figure showing the comparison between the sizes of current clinical/medical datasets and those of datasets used for pretraining big ai models.}
%     \item raise safe, ethical and legal issues of using big ai models in healthcare.
%     \item \textcolor{red}{ChatGPT used Reinforement Learning.}
    
% \end{enumerate}





