\section{Background of Large AI Models}\label{sec:background_large_ai_model}





The burgeoning AI community has devoted much effort to developing large AI models (LAMs) in recent years by leveraging the massive influx of data and computational resources. Based on the pretraining data modality, this article categorizes the current LAMs into four types and defines them as follows:


\begin{enumerate}
    \item Large Language Model (LLM): LLMs are pre-trained on language data and applied to language downstream tasks. Language in different settings can have different interpretations, e.g., protein is the language of life, and code is the language of computers.
    \item Large Vision Model (LVM): LVMs are pre-trained on vision data and applied to vision downstream tasks.
    \item Large Audio Model (LAudiM): LAudiMs are pre-trained on audio data and applied to audio downstream tasks.
    \item Large Multi-modal Model (LMM): LMMs are pre-trained on multi-modal data, e.g., language and vision data, and applied to various single- or multi-modal downstream tasks.
    % \item Others: LAMs that are pretrained on other data modalities, \textcolor{red}{such as audio.}
\end{enumerate}

This section provides an overview of the background of these four types of LAMs in general domains.




% \textcolor{red}{We need a table summarizing the current big models. I will do the edit, for simplicity, let's organize the contents in this way first, covering the following items: 1) name of the model; 2) data source: how much data was used to train the model and what data it was; 3) number of parameters: the number of parameters of the model; 4) tasks: what tasks can the model do. We may need to highlight those big models that have been applied in healthcare in this table; or the table only includes BioLAMs?}

% \textcolor{red}{we also need to mention at the beginning or in this section that LAMs we are talking about are based on Transformer}


\subsection{Large Language Models}






The proposal of the Transformer architecture~\cite{vaswani2017attention} heralds the start of developing large language models (LLMs) in the field of NLP. Since 2018, following the birth of GPT (Generative Pre-trained Transformer)~\cite{radford2018improving} and BERT (Bidirectional Encoder Representations from Transformers)~\cite{devlin2018bert}, the development of LLMs has progressed rapidly.

Broadly speaking, the recent LLMs~\cite{touvron2023llama,chung2022scaling,chowdhery2022palm,hoffmann2022training,smith2022using,scao2022bloom,thoppilan2022lamda,zhang2022opt,ouyang2022training,rae2021scaling,sanh2021multitask,brown2020language,raffel2020exploring,liu2019roberta,radford2019language}, have the following three distinct characteristics: 1) parameter-wise, the number of learnable parameters of an LLM can be easily scaled up to billions; 2) data-wise, a large volume of unlabelled data are used to pre-train an LLM, and the amount can often reach millions or billions if not more; 3) paradigm-wise, LLMs are first pre-trained often with non-supervised learning (e.g., masked language modeling~\cite{devlin2018bert} and next token prediction~\cite{gpt4openai2023}), and then fine-tuned and adapted to various downstream tasks in which they are able to demonstrate impressive performance. 

Recent advances reveal that LLMs are impressive zero-shot, one-shot, and few-shot learners. They are able to extract, summarize, translate, and generate textual information with only a few or even no prompt samples. Furthermore, LLMs manifest impressive reasoning capability, and this capability has been further strengthened recently with prompt engineering techniques such as Chain-of-Thought prompting~\cite{wei2022chain}.



As shown in Fig.~\ref{fig:model_paras}, LLMs such as PaLM~\cite{chowdhery2022palm} have already contained 540 billion parameters, and there was an upsurge of the number of new LLMs in 2022, including the ChatGPT's predecessor InstructGPT~\cite{ouyang2022training}. Despite the general consensus that scaling up the number of parameters and the amount of data will lead to improved performance, which leads to a dominant trend of developing LLMs often with billions of parameters and even trillions of data tokens (e.g., MT-NLG~\cite{smith2022using} was pre-trained with 270 billion tokens, and the training data of RETRO~\cite{borgeaud2022improving} had over 5 trillion tokens), there is currently no concerted agreement within the community that if this continuous growth of model and data size is optimal and necessary~\cite{hoffmann2022training,touvron2023llama}. 


% With LLMs being actively researched in the general domain and demonstrating impressive performance in many fundamental NLP tasks, the biomedical and health community has also started to investigate LLMs and apply them in biomedical, clinical, and healthcare scenarios such as electronic health records (EHRs), with the proposal of a series of BioLLMs, such as BioBERT~\cite{lee2020biobert}, PubMedBERT~\cite{gu2021domain}, GatorTron~\cite{yang2022large}, and Med-PaLM~\cite{singhal2022large}. We discuss in Section~\ref{sec:applications} about the biomedical and health sectors where BioLLMs are having an impact on and transforming the current solutions and paradigms.




To balance the data annotation cost and efficacy, as well as to train an LLM that can better align with human intent, researchers have commonly used reinforcement learning from human feedback (RLHF)~\cite{christiano2017deep} to develop LLMs that can exhibit desired behaviors. The core idea of (RLHF)~\cite{christiano2017deep} is to use human preference datasets to train a Reward Model (RM), which can predict the reward function and be optimized by RL algorithms (e.g. TRPO~\cite{schulman2015trust} or PPO~\cite{schulman2017proximal}). The framework of RLHF has attracted much attention and become a key component of many LLMs, such as WebGPT~\cite{nakano2021webgpt}, InstructGPT~\cite{ouyang2022training}, Sparrow~\cite{glaese2022improving}, and ChatGPT~\cite{chatgpt2022}. Recently, Susano Pinto et al.~\cite{susano2023tuning} have also investigated this reward optimization in vision tasks, which can possibly advance the development of future LVMs using RLHF. RLHF is also adopted in the recent LMM, GPT-4~\cite{gpt4openai2023}.

















% \subsubsection{General LLMs}

% \begin{enumerate}
%     \item GPT~\cite{radford2018improving}: \textcolor{red}{117M parameters (double check)}


%     \item BERT~\cite{devlin2018bert}: BERT-large is 340M parameters. Pretraining data includes 800M words of BooksCorpus~\cite{zhu2015aligning} and 2500M words of English Wikipedia.
    
%     \item GPT-2~\cite{radford2019language}: a 1.5B-parameter Transformer-based model. \textcolor{red}{trained over 8 million documents.}
    
%     \item RoBERTa~\cite{liu2019roberta}: 355M parameters and it uses over 160 GB of text data including BookCorpus

%     \item BART~\cite{lewis2019bart}: a Transformer-based model. Used 160GB pretraining data. the large one has 12 layers in each of its encoder and decoder
    
%     \item T5~\cite{raffel2020exploring}: pretrained on its self-constructed C4 dataset (745GB, $2^{35}$ tokens). Its number of parameters can scale from 60M to 11B.

%     \item GPT-3~\cite{brown2020language}: a 175B-parameter autoregressive language model
    
%     \item T0~\cite{sanh2021multitask}: it is a 11B parameter version of (T5+LM)~\cite{lester2021power}. It investigated zero-shot generalization through multitask prompted training.
    
%     \item Jurassic-1~\cite{lieber2021jurassic}: 178B parameters and pretrained on 300B tokens

%     \item Gopher~\cite{rae2021scaling}: a Transformer-based model with 280B parameters and pretrained on 300B tokens.


%     \item Switch Transformer~\cite{fedus2021switch}: this one actually already has over trillions of parameters back in 2021, but why it has not caught much attention
    
%     \item InstructGPT~\cite{ouyang2022training}: 1.3B - 175B
    
%     \item Codex: openai
    
%     \item RETRO~\cite{borgeaud2022improving}: deepmind. RETRO (retrieval-enhanced transformer) achieves comparable performance to GPT-3, but with 25x fewer parameters. Its retrieval database contains 2 trillion token. Its training data contains over 5 trillion tokens. It is a transformer-based model and its parameters can scale from 150M to 7B.
    
%     \item OPT~\cite{zhang2022opt}: a set of decoder-only pre-trained transformers. Their number of parameters ranges from 125M to 175B.
    
%     \item DRAGON~\cite{yasunaga2022dragon}: same architecture as GreaseLM (360M parameters). It uses BookCorpus~\cite{zhu2015aligning} (6GB of text). For the knowledge graph data, it uses ConceptNet~\cite{speer2017conceptnet}, which has 800K nodes and 2M edges. Its biomedical version uses PubMed (21GB of text), and for the KG data, it uses UMLS~\cite{bodenreider2004unified}, which has 300K nodes and 1M edges. Its biomedical version's LM component was BioLinkBERT-Large.
    
%     \item LaMDA~\cite{thoppilan2022lamda}: 2B to 137B parameter Transformer-based LLM, designed for dialog applications, and were pretrained on 1.56T words of dialog and web data. It also empathizes the \textbf{safety} and \textbf{factual grounding}, and provides solutions. For safety, it uses a classifier fine-tuned with human-annotated data. For factual grounding, it leverages external knowledge resources such as by using an information retrieval system. It suggests by defining some safety objectives can improve safety
    

    
%     \item ChatGPT:
    
%     \item MOSS Chinese ChatGPT


    
%     \item BLOOM~\cite{scao2022bloom}: a 176B-parameter and decoder-only Transformer language model that was trained on the ROOTS corpus containing 46 natural and 13 programming languages.

%     \item MT-NLG~\cite{smith2022using}: 530B parameters and pretrained with 270B tokens

%     \item Chinchilla~\cite{hoffmann2022training}: 70B parameters and 1.4T tokens. It provides a perspective that while increasing the size of model, the data size has also to be scaled up, i.e., the number of tokens used in pretraining, but of course, the quality of data should be maintained while scaling them up.
    
%     \item PaLM~\cite{chowdhery2022palm}: Pathways Language Model, a 540B-parameter and Transformer-based LLM, trained using a dataset containing 780 billion tokens over 100 languages
    
%     \item Flan-PaLM~\cite{chung2022scaling}: It scales to 540B parameters and 1.8K tasks
    
%     \item Galactica~\cite{taylor2022galactica}: Meta AI \textcolor{red}{(this is offline now, and better use it as an example to alarm the potential risks of LLMs)}

%     \item Sparrow~\cite{glaese2022improving}:

%     \item LLaMA~\cite{touvron2023llama}: an auto-regressive model based on Transformer. Its parameters can be scaled to 65B, and were pretrained on 1.4 trillion tokens. It brings up an interesting point that the inference cost of a LAM should also be considered. Like if the model is gigantic, the infrastructure for inference must also be expensive. In most applications, people won't care about training, but only care about inference.
%     \item ChatLLaMA


    
% \end{enumerate}



% \subsubsection{Biomedical and Health LLMs}

% \textcolor{blue}{may need to mention BioNLP~\cite{lewis2020pretrained}, it seems this group of people or its published venue (that workshop) put a lot of efforts. Also Google's Med-PaLM claims only GPT-3 and PaLM can be called LLMs, BERT  series are still small}.




% \begin{enumerate}
%     \item \textbf{BioBERT}~\cite{lee2020biobert}: it essentially continues pretraining of BERT on PubMed Abstracts (4.5B words) and PubMed Central full-text articles (13.5B words).
    
%     \item ClinicalBERT~\cite{alsentzer2019publicly}: 2 million notes in the MIMIC-III v1.4. It is essentially a BERT model that continues to be pretrained on clinical text. ClinicalBERT also continues pretraining based on BioBERT.
    
%     \item SciBERT~\cite{beltagy2019scibert}: pretrained on 1.14M papers (18\% papers are computer science and the rest 82\% are biomedical). The full text of the papers were used and the entire corpus has 3.17B tokens. It pretrained BERT from scratch.
    
%     \item BlueBERT~\cite{peng2019transfer}: it essentially continues to pretrain a BERT model, which has been pretrained on general-domain corpora, on PubMed abstracts (more than 4000M words) and MIMIC-III (more than 500M words) 
    
%     \item BioMegatron~\cite{shin2020biomegatron}: three different sizes: 345 million, 800 million, and 1.2 billion number of parameters. Pretrained using 4.5 billion-word PubMed
%     abstract and 1.6 billion-word PMC full-text corpus.
    
%     \item DARE~\cite{papanikolaou2020dare}: essentially, it is a GPT-2 (774M parameters) pretrained on 500K PubMed abstracts.
    
%     \item BioMedRoBERTa~\cite{gururangan2020don}: it is essentially a RoBERTa, which were continued to be pretrained on 2.68M full-text papers from S2ORC~\cite{lo2019s2orc} (7.55B tokens, 47GB)

%     \item \textbf{BEHRT}~\cite{li2020behrt}: was trained using data of 1.6M patients
    
%     \item \textbf{PubMedBERT}~\cite{gu2021domain}: reveal that domain-specific pretraining from scratch is favourable. Pretrained on 14 million abstracts, 3.2 billion words, 21 GB.
    
%     \item BioELECTRA~\cite{raj2021bioelectra}: essentially pretrain ELECTRA from scratch using PubMed Abstract (4.2B words) and PubMed Central (PMC, 9.6B words)
    
%     \item \textbf{Med-PaLM}~\cite{singhal2022large}: perform \textbf{instruction prompt tuning} on Flan-PaLM 540B to produce Med-PaLM. \textbf{Proposed two medical language benchmarks (MultiMedQA and HealthSearchQA)}
    
%     \item Med-BERT~\cite{rasmy2021med}: it is essentially a BERT model pretrained on Cerner Health Facts (general EHR). 20M of pretraining patients.
    
%     \item BioGPT~\cite{luo2022biogpt}: for biomedical text generation and mining, and pretrained on 15M PubMed abstracts. Its backbone is GPT-2. BioGPT has 347M paramters
    
%     \item BioLinkBERT~\cite{yasunaga2022linkbert}: 110M parameters of the base size, and 340M parameters of the large size (same as bert). Pretrained on PubMed abstracts (21GB)
    
%     \item \textbf{BioMedLM}: Stanford CRFM, originally called PubMedGPT
    
%     \item GatorTron~\cite{yang2022large}: more than 90 billion words (82 billion are de-identified clinical text). the GatorTron-large model has 8.9 billion parameters.

%     \item DRAGON~\cite{yasunaga2022dragon}: it has also been tested on biomedical data.

%     \item Can Large Language Models Reason about Medical Questions?~\cite{lievin2022can}: investigated different prompt engineering for LLMs such as GPT-3.5, InstructGPT and Codex on their performances on medical question answering.
    
% \end{enumerate}


% \subsection{JQ's comments}
% \begin{enumerate}
%     \item need to mention and emphasize the reasoning ability of LLMs
%     \item there is a technique called \textbf{chain of thought prompting}~\cite{wei2022chain} to strengthen the reasoning capabilities of LLMs. Chain of thought prompting only needs a few examples to improve the performance instead of a large amount of training data.
%     \item prompt engineering: \url{https://github.com/dair-ai/Prompt-Engineering-Guide}
%     \item need to mention LLMs' \textbf{few shot, zero-shot learning capabilities}.
%     \item the debate of larger model and larger data size (scaling laws~\cite{hoffmann2022training}), and the point from~\cite{touvron2023llama} that inference budget is also important.
% \end{enumerate}



\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{figures/data_modalities.pdf}}
\caption{Common data modalities in health informatics.}
\label{fig:data_modalities}
\end{figure}








\subsection{Large Vision Models}
% Following the pre-defined taxonomy, we adopt Large Vision Models (LVMs) to denote a category of models which are pre-trained with large-scale vision-only data and capable of processing a large range of vision downstream tasks. 
% \textbf{The ideas of pre-training on large datasets and enlarging the models' capacity have been explored for decades in computer vision to improve the performance. It is the recent breakthrough of self-supervised learning and vision transformers that scales the size of the pre-trained datasets and the model respectively to an unprecedented level and demonstrates promising results. Therefore, we start our discussion with a brief introduction to the previous paradigm and then move to the new paradigm we observed in the field.
% }

% \subsubsection{Pre-training}
It is a common practice in computer vision for better generalization to pre-train a model on a large-scale dataset and then fine-tune it on the target dataset \cite{yosinski2014transferable}. We highlight that the new paradigm distinguishes from the old one in terms of the scale of the pre-training dataset and the method of pre-training. In recent years, the scale of vision dataset has grown up dramatically from millions to billions to enable larger-scale pretraining.
ImageNet-1K \cite{deng2009imagenet} is a canonical, manually curated dataset with 1.28M images for visual pretraining. It is a subset of ImageNet-21K \cite{ridnik1imagenet} (14M images), which has been so far the largest curated image dataset. Nevertheless, the prohibitive cost of curation severely hindered further scaling. 
To push the limit of the scale, several orders of magnitude larger datasets were collected with less or even no curation.
JFT (300M \cite{sun2017revisiting}, 3B \cite{zhai2022scaling} and 4B \cite{Dehghani2023ScalingVT}) and IG (1B \cite{goyal2021self}, 3.5B \cite{mahajan2018exploring} and 3.6B \cite{singh2022revisiting}) are two most popular families of large datasets dedicated to visual pretraining. With the scale increasing, the quality of annotation is compromised and the availability of datasets is becoming limited. 
% Their images were retrieved from web, specifically, Image Search and Instagram \footnote{Instagram: \url{https://www.instagram.com/}} respectively. 

% JFT-300M \cite{sun2017revisiting} consists of 300M images derived from Image Search and algorithmically labeled. Recently, the size of dataset was further extended to be over billions. IG-1B \cite{goyal2021self} randomly fetched 1B images from Instagram without labels. Annotations-1.3B \cite{beal2022billion} collected 1.329B web images and annotated them using content understanding models. IG-3.5B \cite{mahajan2018exploring} and IG-3.6B \cite{singh2022revisiting} both collected Instagram images according to some predefined hashtags yet at slightly different scales. Latest JFT datasets were scaled to have 3B \cite{zhai2022scaling} and 4B \cite{Dehghani2023ScalingVT} unique images. To our best knowledge, JFT-4B is the current largest vision dataset.

% TODO: a table summarizing all modern datasets

% With the scale increasing, the quality of annotation is compromised and the availability of datasets is becoming limited/closed accessed. 
% Both JFT and IG were annotated algorithmically, which unavoidably introduces substantial noise to labels and hence challenges the training methods. For example, the labels of about 20\% data in JFT-300M are noisy \cite{sun2017revisiting}. 
% Moreover, training with billion-scale datasets requires tremendous resource of storage and computation. Even worse, all JFTs and IGs are not publicly available. It is therefore impossible for others to replicate the original result and reuse these data.

% The compromised labels restrict the selection of learning paradigm for pretraining. Datasets like ImageNet are carefully curated to ensure the quality of annotation, so that their labels are usually treated as less or not noisy and hence capable of applying fully-supervised learning \cite{yosinski2014transferable}. In contrast, datasets like JFT and IG exhibit significant noise in their labels and are therefore appropriate to be learnt with weakly-supervised learning techniques \cite{sun2017revisiting, mahajan2018exploring}. The learning task here, albeit full or weak supervision, is usually a single-/multi-labels prediction problem.
% Apart from supervised learning, unsupervised learning, or Self-Supervised Learning (SSL), does not rely on external annotations and is thus technically applicable to all above datasets. This potentially allows further scaling datasets up to be order of magnitude larger. More importantly, SSL achieved impressive performance that is comparable to or even better than its supervised counterpart on some benchmarks \cite{liu_swin_2022, li_efficient_2022}.

Both supervised and unsupervised learning are popular pretraining paradigms in visual domain. 
% Supervised pretraining requires datasets to be labeled either manually or automatically. In general, models are trained to perform a single-/multi-label(s) prediction task \cite{yosinski2014transferable, dosovitskiy_image_2021, singh2022revisiting}. In contrast, unsupervised learning has no rely on labels.
The existing unsupervised pretraining methods can be categorized into three groups: autoregressive modeling, generative modeling, and contrastive learning. Autoregressive modeling trains the model to autoregressively predict the next pixel conditioned on the preceding ones \cite{chen_generative_2020}. Generative modeling trains the model to recover the original image based on its corrupted \cite{chen2021pre} or masked \cite{he_masked_2022} variants. Contrastive learning trains the model to discriminate similar and/or dissimilar instances \cite{chen2020simple, he2020momentum}. 

% autoregressiveK

% masked reconstruction

% instance discrimination

% \subsubsection{Model}
Unlike NLP where transformers dominate, the canonical architecture of LVMs is either based on Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs). They both demonstrate impressive yet close performance on many benchmarks \cite{zhai2022scaling, liu2022convnet}.  

Since the introduction of AlexNet \cite{krizhevsky2017imagenet}, scaling up CNNs has been a direction for visual recognition. 
Among many early attempts \cite{simonyan2014very, szegedy2015going, he2015delving, he2016deep}, ResNet \cite{he2016deep}, as a milestone of deep CNNs, proposed residual connection enabling top-down, direct, gradient flow to effectively train deep networks. 
Later, successive works refined the architectural design of ResNet manually \cite{he2016identity, xie2017aggregated} and automatically \cite{zoph2018learning, radosavovic2020designing} for improved predictive performance.
Given these high-performing baseline models, ideas such as scaling laws \cite{tan2019efficientnet, tan2021efficientnetv2}, large-scale pretraining \cite{kolesnikov2020big, goyal2021self} and efficient parallelism \cite{huang2019gpipe} were explored to scale them up regarding depth, width and input resolution.
Latest models, such as ConvNeXt-XL \cite{liu2022convnet} and InternImage \cite{wang2022internimage}, updated ResNets with ViTs components \cite{liu2021swin} and deformable convolution, respectively. They achieved state-of-the-art accuracy on ImageNet, and InternImage-H (1.08B parameters) is the largest CNN to date.

Vision Transformers have been actively researched recently. Early works ViT \cite{dosovitskiy_image_2021} and iGPT \cite{chen_generative_2020} transferred the transformer architectures \cite{vaswani2017attention, radford2019language}, respectively, from NLP to perform visual tasks with minimal modification. Several works were then proposed to better adapt NLP transformers to visual data. TNT \cite{han_transformer_2021} divided the image patches in ViT into smaller ones for fine-grained processing. Swin Transformer (SwinT) \cite{liu2021swin} proposed a shifted window mechanism to compute self-attention locally, for efficiency, yet allowing cross-window modeling for effectiveness. Recently, ViT-G/14 \cite{zhai2022scaling} and SwinV2-G \cite{liu_swin_2022} employed a set of training techniques to successfully scale ViT and SwinT models to 2B and 3B parameters, respectively. Latest, ViT-22B \cite{Dehghani2023ScalingVT} further scaled ViTs to 22B parameters, the largest vision model to date, using an improved training recipe. We highlight that this scale greatly exceeds the current art of CNNs but is still smaller than many recent LLMs.

% alternating convolutional layers and transformer layers 
% dense vs. mixture of experts

% discussion:

% largest CNNs is much behind Transformers.
% Transformers rely on pretraining to excel

% \textcolor{blue}{JQ: I found the following, can you guys double check? 1) Big Transfer; 2) ResNeXt WSL; 2) GPipe; 4) EfficientNet-L2. Also, there is a dataset called JFT-300M}

% TODO: the key to scale models to be large is 
% \begin{itemize}
%     \item large dataset
%     \item (pre-)training methods at scale
%     \item high-performing base architecture
%     \item efficient scale raw
% \end{itemize}


\subsection{Large Audio Models}
Recent advances in acoustics have also been influenced by the development of LAMs.
% is a self-supervised audio model. 
W2v-BERT~\cite{chung2021w2v} and Wav2Vec~\cite{schneider2019wav2vec} were trained on large amounts of unlabeled audio data, and their resulting representations can be used to enhance acoustic model training.
AudioLM~\cite{borsos2022audiolm} employs a hierarchical approach to modeling audio sequences through a series of connected Transformer models. Hidden-Unit BERT (HuBERT)~\cite{hsu2021hubert} (1B parameters) learns a combined acoustic and language model over the continuous inputs. As a large-scale model for cross-lingual speech representation learning, with up to 2B parameters, XLS-R~\cite{babu2021xls} was trained on a vast corpus of publicly available speech audio (approximately half a million hours) in 128 diverse languages. MusicLM~\cite{agostinelli2023musiclm}, Diffsound~\cite{yang2022discrete}, and AudioGen~\cite{kreuk2023audiogen} can generate high-fidelity music, sound, or audio from text descriptions. Trained on a vast and diverse collection of audio data, Whisper~\cite{radford2022robust} (1.6B parameters) is able to tackle various tasks, such as language identification, speech recognition in multiple languages, and speech translation. Universal Speech Model (USM)~\cite{zhang2023google} (with 2B parameters) demonstrates that leveraging a large unlabeled multilingual dataset for pre-training, followed by fine-tuning on a smaller labeled dataset, can facilitate the identification of under-represented languages. Overall, the application of LAMs to the field of acoustics has led to improvements in speech recognition, cross-lingual speech representation learning, and the generation of high-quality music and sound. 



\subsection{Large Multi-modal Models}
% Large Language-Vision Models (LLVMs) are multi-modal models pretrained on large-scale (paired) vision and language data and can be adapted to various vision and/or language applications. 
% % Language here refers to unstructured text content like image description, which is in contrast to the well-structured labels used in LVMs. 
% LLVMs excel in modeling the complex relationship between vision and language data to solve the cross-modal tasks like Visual Question Answering \cite{yuan_florence_2021} and text-to-image generation \cite{ramesh_zero-shot_2021}. 
Current research on large multi-modal models (LMMs) is mainly focused on two modalities: language and vision. Therefore, this section mainly discusses a particular type of LMM, i.e., large language-vision models (LLVMs).

Prior to the era of large models, language-vision data were dedicated to solving a specific task like image captioning~\cite{ordonez_im2text_2011, young_image_2014, chen_microsoft_2015, krishna_visual_2017, sharma_conceptual_2018} or visual question answering \cite{zhu_visual7w_2016, krishna_visual_2017, hudson_gqa_2019, goyal_making_2019} instead of pretraining large models. For most of them, the scale (thousands or millions) is relatively small and much behind the order of magnitude (billions) typical for pretraining modern large models. Nevertheless, they usually provide richer, fine-grained annotations such as object localization. These extra signals can be explored to design alternative pretraining objectives \cite{li_grounded_2022} for potentially more efficient and/or effective learning.
Later works \cite{thomee_yfcc100m_2016, changpinyo_conceptual_2021, srinivasan_wit_2021, desai_redcaps_2022} pushed the scale of public language-vision datasets to tens of millions, whereas the contemporary LLVMs were mostly pretrained on private datasets on the scale of (hundreds of) millions or billions \cite{radford_learning_2021, yuan_florence_2021, jia_scaling_2021, pham2021combined, chen2022pali}. To reproduce the result based on these private datasets, LAION-5B \cite{schuhmann_laion-5b_2022} was created with 2.3B data entries and, more importantly, available to the public. 

Typical LLVMs adopt a dual-stream architecture. Raw input text and images are first processed separately by respective language and vision models to extract features. 
% The widely-used language encoders include BERT \cite{devlin2018bert} and GPT-2 \cite{radford2019language}, and the common vision encoders are CNNs \cite{he2016deep, tan2019efficientnet} and ViTs \cite{dosovitskiy_image_2021, liu_swin_2021}. 
The features derived from different modalities are then fused into a unified representation through simple concatenation \cite{radford_learning_2021, jia_scaling_2021, yuan_florence_2021, huang_language_2023} or an encoder mapping all features into a joint space \cite{singh_flava_2022, yao_filip_2022, li_blip_2022, su_towards_2022}. 
The interaction between the paired text and images is then learned upon this representation to solve cross-modal tasks, such as visual question answering \cite{yuan_florence_2021} and text-to-image generation \cite{ramesh_zero-shot_2021}. 
Note that not all multi-modal systems follow this paradigm. For example, Visual ChatGPT \cite{wu2023visual} develops a prompt manager to bridge the communication between ChatGPT and a set of pre-trained LVMs. 

% The training objective is computed on the fused representation, together with the extracted features in some cases, to model the interaction between the paired image and text.

To learn cross-modal representations, works such as CLIP \cite{radford_learning_2021}, ALIGN \cite{jia_scaling_2021}, BASIC \cite{pham2021combined}, Florence \cite{yuan_florence_2021} and FILIP \cite{yao_filip_2022} were trained to match the extracted vision and language features in a contrastive manner.
In contrast, SimVLM \cite{wang_simvlm_2022} and KOSMOS-1 \cite{huang_language_2023} concatenated the features from different modalities into one single sequence and maximized the likelihood of the sequence autoregressively. 
Alternatively, FLAVA \cite{singh_flava_2022}, BLIP \cite{li_blip_2022} and M3I \cite{su_towards_2022} pretrained models with multiple objectives.
The overall objective can be composed of various learning tasks, like the aforementioned ones, on cross-modal representations \cite{li_blip_2022, su_towards_2022} or the same learning task on both cross-modal and uni-modal representations \cite{singh_flava_2022}.
Their result suggested that different learning tasks can complement each other and regularizing uni-modal representations simultaneously can be beneficial. 
Furthermore, BLIP-2 \cite{li_blip-2_2023}, PaLI \cite{chen2022pali} and PaLM-E \cite{driess2023palme} (the largest LLVM to date with 562B parameters) improved the underlying image and language encoders for better performance. 
A simple strategy was scaling the image and language backbone models up. 
Latest released GPT-4 \cite{gpt4openai2023} extends its predecessor with the capability of processing visual input, and achieves human-level performance on various benchmarks. 

Recently, LLVMs for text-to-image generation have also attracted much attention. 
DALL-E \cite{ramesh_zero-shot_2021} and Parti \cite{yuscaling} first learned a mapping from images to discrete tokens and then trained a transformer to autoregressively model the joint distribution over the concatenated image and text tokens. 
A different line of work combined LLVMs with diffusion models \cite{croitoru2022diffusion} to improve the generation quality. 
GLIDE \cite{nichol2022glide} adopted CLIP as a classifier to steer the underlying diffusion models. 
unCLIP \cite{ramesh2022hierarchical}, a.k.a. DALL-E-2, Imagen \cite{sahariaphotorealistic} and Stable Diffusion \cite{rombach2021highresolution} employed a pretrained LLM or LLVM to encode text into a sequence of embeddings and conditioned the diffusion process on these embeddings.


% It was pretrained by autoregressive modeling on a mixture of public and private data and then fine-tuned using reinforcement learning from human feedback. 

% \subsection{works}

% \subsubsection{Biomedical and Health LLVMs}

% \begin{enumerate}
%     \item ChatCAD:
% \end{enumerate}

% \subsubsection{JQ's comments}

% \begin{enumerate}
    % \item Language Is Not All You Need: Aligning Perception with Language Models
    % \item BLIP-2~\cite{li2023blip}:
    % \item Stable Diffusion~\cite{rombach2021highresolution}: Stable Diffusion is a latent text-to-image diffusion model. Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder.
    % \item CLIP~\cite{radford_learning_2021}: CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. 
    % \item DALL-E~\cite{ramesh2021zero}: a transformer language model trained to generate images from text captions with precise coherence. 
    % \item Imagen~\cite{saharia2022photorealistic}: a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. 
    % \item Scenic~\cite{dehghani2021scenic}: a codebase with a focus on research around attention-based models for computer vision. Scenic has been successfully used to develop classification, segmentation, and detection models for multiple modalities including images, video, audio, and multimodal combinations of them.
    % \item PaLI~\cite{chen2022pali}:  PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages.
% \end{enumerate}






















% \subsection{Other Large AI Models}

% Beyond language and vision, LAMs are also transforming other domains such as robotics and acoustics.

% Bucker et al. introduced the LANGUAGE TRAJECTORY TRANSFORMER (LATTE), a model capable of adjusting the movements of multiple robot types via natural language inputs \cite{bucker2022latte}. The training and evaluation of LATTE was conducted on 100K synthetic datasets, consisting of original and modified 3D trajectories and user-generated natural language inputs. The real-world experiment showed that LATTE enables the robot to modify the location and velocity of trajectories using both visual and natural language inputs. Bonatti et al. developed a Perception-Action Causal Transformer (PACT) to let robot conduct different tasks from a same starting point \cite{bonatti2022pact}. This model was trained on 840K tokens, which are the expert trajectories collected in Habitat simulator. The results from simulator and real world shows that the model is highly versatile and can help robots with a variety of downstream tasks such as navigation, localization and mapping. A transformer-based generalist robot agent, VIMA has been proposed in 2022 to execute tasks according to the nature language input \cite{jiang2022vima}. 650K experts trajectories were generated in the simulator to perform 100k individual tasks and used to train the model with up to 200M parameters. By converting the robotics tasks into a different serious, robot will able to perform tasks based on human will and visual tokens. 
% According to Reed et al. (2022), DeepMind has created a multi-task execution model, called Gato, which is applicable to simple robot manipulator tasks. The model was trained on a comprehensive dataset of 387K and 15K robot manipulator trajectories in both simulated and real-world settings. The results indicate that Gato enhances robots' generalization of skills and adaptability to various environments. With a predefined specific API, ChartGPT is able to generate code for robots to execute tasks with continues external senor and user natural language input. LAMs are not limited to simple robotic tasks. In addition, they have begun dabbling in some tasks that have long time horizons and high predictive difficulty. 



% Climax is such a foundation model as a pioneering approach to climate modeling and the resolution of a diverse range of climate and weather-related tasks \cite{nguyen2023climax}. By employing data from the CMIP6 database, which comprises a comprehensive collection of climate information spanning the period from 1850 to 2015, the Climax model has the capacity to forecast climate patterns, facilitate climate projections, and perform climate downscaling, in addition to other potential downstream applications.


% \textcolor{blue}{JQ: Microsoft ChatGPT for robotics, and also the climate change and weather work}



% bucker2022latte
% bonatti2022pact
% jiang2022vima

% \textcolor{red}{the following contents are JIANKAI's, 
% need to \textbf{paraphrase}}

% \begin{enumerate}
    % \item Decision Transformer~\cite{sun2021adversarial,chen2021decision}: apply Transformer architecture, and associated advances in language modeling such as GPT-x and BERT to reinforcement learning.  
    % \item MimicPlay~\cite{wang2023mimicplay}:  resort to human play data - video sequences of people freely interacting with the environment using their hands. They introduce a hierarchical learning framework named MimicPlay that learns latent plans from human play data to guide low-level visuomotor control trained on a small number of teleoperated demonstrations. 
    % \item R3M~\cite{nair2022r3m}: visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. They pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment, and an L1 penalty to encourage sparse and compact representations.
    % \item MVP~\cite{Radosavovic2022}:  explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks. train a 307M parameter vision transformer on a massive collection of 4.5M images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning.
    % \item PlaTe~\cite{sun2022plate}: study the problem of how to leverage instructional videos to facilitate the understanding of human decision-making processes, focusing on training a transformer planning model with the ability to plan a goal-directed procedure from real-world videos. 
    % \item CLIPort~\cite{shridhar2022cliport}:
    % \item PerAct~\cite{shridhar2022perceiver}: can learn a single language-conditioned policy for 18 RLBench tasks with 249 unique task variations
    % \item SayCan~\cite{saycan2022arxiv}:
    % \item Robotics Transformer 1 (RT-1)~\cite{brohan2022rt}: trained on 130k tele-operation demonstrations over 13 robots and 744 tasks.
    % \item ChatGPT for Robotics (PromptCraft)~\cite{vemprala2023chatgpt}: outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors.
% \end{enumerate}

% The rapid advancement in LAMs are revolutionizing a wide range of applications~\cite{sun2022locate,sun2022nerf}. 

% The robotics community has actively researched the use of LAMs in various robotic tasks and settings. For example, Decision Transformer~\cite{chen2021decision,sun2021adversarial} applies Transformer architecture, and associated advances in language modeling such as GPT~\cite{brown2020language} and BERT~\cite{devlin2018bert} to reinforcement learning. CLIPort~\cite{shridhar2022cliport} and PerAct~\cite{shridhar2022perceiver} learn a single language-conditioned policy for various tasks using multi-task Transformer.
% LLMs contain a wealth of semantic knowledge about the world~\cite{sun2022plate}, which in principle might be useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. PaLM-SayCan~\cite{saycan2022arxiv} % tackles
% and PaLM-E~\cite{driess2023palme} tackle
% language grounding in robotic affordances with Pathways Language Model (PaLM)~\cite{chowdhery2022palm}.
% Some studies explore the application of large model representation capabilities to robotics. For example, MVP~\cite{Radosavovic2022} explores self-supervised visual pre-training on images from the Internet and egocentric videos for real-world robotic tasks. They trained a 307M parameter vision transformer on a massive collection of 4.5M images, and demonstrated evidently the benefits of scaling visual pre-training for robot learning. Similarly, R3M~\cite{nair2022r3m} shows visual representations pre-trained on diverse egocentric video data (Ego4D~\cite{grauman2022ego4d}) using video-language alignment can enable data-efficient learning of downstream robotic manipulation tasks. MimicPlay~\cite{wang2023mimicplay} further demonstrates that human-play data can enable data-efficient learning of downstream visuomotor control and is helpful for learning latent plans. Robotics Transformer 1 (RT-1)~\cite{brohan2022rt} trains a multi-task transformer model on 130k tele-operation demonstrations over 13 robots and 744 tasks. 

% \subsubsection{Robotics}
% The robotics community has actively researched the use of LAMs in various robotic tasks and settings. Some works apply Transformer architecture and associated advances in language modeling such as GPT~\cite{brown2020language} and BERT~\cite{devlin2018bert}  to reinforcement learning, as shown in Decision Transformer~\cite{chen2021decision,sun2021adversarial}. 
% Some recent research explores the application of large model representation capabilities to robotics. For example, MVP~\cite{Radosavovic2022} explores self-supervised visual pre-training on images from the Internet and egocentric videos for real-world robotic tasks. They trained a 307M parameter vision transformer on a massive collection of 4.5M images, and demonstrated evidently the benefits of scaling visual pre-training for robot learning. Similarly, R3M~\cite{nair2022r3m} shows visual representations pre-trained on diverse egocentric video data (Ego4D~\cite{grauman2022ego4d}) using video-language alignment can enable data-efficient learning of downstream robotic manipulation tasks. MimicPlay~\cite{wang2023mimicplay} demonstrates that human-play data can enable data-efficient learning of downstream visuomotor control and help in learning latent plans. (SL)$^3$~\cite{sharma2021skill} simultaneously finetunes two LLMs, a high-level planning network, and a low-level policy network for actions selection. Pre-trained language models can be further leveraged for more general decision-making problems~\cite{li2022pre,licomposing}.
% % Recently, ChatGPT for Robotics~\cite{vemprala2023chatgpt} develops PromptCraft, a platform that outlines a strategy to combine design principles for prompt engineering and the creation of a high-level function library, allowing ChatGPT to adapt to different robotics tasks, simulators, and form factors. 

% With scaled-up capability, many work has proposed to use only one single LAM to conduct a diverse range of robotic tasks, demonstrating impressive adaptability and generalization skills.
% Gato~\cite{reed2022generalist}, with 1.2 billion parameters and trained on 604 unique tasks, is a generalist agent that utilizes one single large neural model to conduct various downstream robotic tasks. Other studies, such as CLIPort~\cite{shridhar2022cliport} and PerAct~\cite{shridhar2022perceiver}, use multi-task Transformer to learn a single language-conditioned policy for various tasks.
% LLMs contain a wealth of semantic knowledge about the world~\cite{sun2022plate}, which in principle might be useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. PaLM-SayCan~\cite{saycan2022arxiv} % tackles
% and PaLM-E~\cite{driess2023palme} tackle
% language grounding in robotic affordances with Pathways Language Model (PaLM)~\cite{chowdhery2022palm}. Jiang et al.~\cite{jiang2022vima} proposed VIMA, also a generalist robot agent that leverages multimodal prompts to conduct diverse manipulation tasks. Robotics Transformer 1 (RT-1)~\cite{brohan2022rt} trains a multi-task transformer model on 130k tele-operation demonstrations over 13 robots and 744 tasks. Microsoft has utilized ChatGPT to generate execution codes for robots based on natural language instructions, with an aim to transform robotic pipelines from the current \textit{engineer in the loop} to the potential \textit{user in the loop}, which can use high-level language commands to control and fulfill robotic tasks~\cite{vemprala2023chatgpt}. They also proposed AirSim-ChatGPT~\cite{vemprala2023chatgpt}, which combines AirSim~\cite{shah2018airsim} with ChatGPT to serve as a platform to investigate future research on drone navigation using ChatGPT. Furthermore, Sun et al.~\cite{sun2023smart} recently proposed SMART, which pretrains a versatile, generalizable, and resilient model for a wide range of control tasks and shows that the efficacy of self-supervised pretraining seen in vision and language can be extended to control tasks. These studies highlight the potential of LAMs and large model representation capabilities to improve robot learning and performance in various tasks and settings. 

% Here are some possible directions to explore for applying Large AI Models (LAMs) to robotics include:
% \begin{itemize}
%     \item Task-specific fine-tuning: Bridge the gap between large-scale vision and language dataset and the task-specific interactive datasets is important. While LAMs have shown impressive adaptability and generalization skills, they may still require task-specific fine-tuning to achieve optimal performance in a particular robotic task. Researchers should explore techniques for efficiently fine-tuning LAMs to specific tasks while preserving their ability to generalize to new tasks.
%     \item Multimodal inputs: LAMs have been successful in language tasks, but robots typically operate in a multimodal environment that includes visual and tactile inputs. Future research should explore the use of multimodal inputs to improve the performance of LAMs in robotics.
%     \item Transfer learning and generalizability: 
%     % Transfer learning is a promising technique for applying LAMs to robotics. 
%     By pretraining a LAM on a large dataset and then fine-tuning it for a specific robotic task, researchers can take advantage of the LAM's ability to generalize to new tasks while also benefiting from task-specific learning.
%     \item Efficient computing: LAMs are computationally expensive, which can make them challenging to deploy on resource-constrained robotic platforms. Researchers should explore techniques for efficient LAMs such as model compression while preserving their performance to enable their deployment on robotic systems.
%     \item Human-robot interaction: LAMs have the potential to improve human-robot interaction by enabling robots to understand and respond to natural language commands. Future research should explore the use of LAMs for natural language understanding and generation in the context of human-robot interaction.
% \end{itemize}

% Overall, the application of LAMs to robotics is an exciting area of research with the potential to revolutionize the field of robotics and enable more capable and adaptable robotic systems.


% Hence, LLMs, such as GPT (Generative Pre-trained Transformer) models, have the ability to process and generate natural language. This can be extremely useful for robots, as it enables them to communicate with humans more effectively and understand human language better.
% With LLMs, robots can not only understand what humans are saying but also respond appropriately in a way that makes sense to humans. They can also use language to reason and solve problems, making them more autonomous and able to carry out complex tasks.
% A robot that is equipped with a large language model could also reason through complex problems and generate solutions based on its understanding of natural language and the knowledge it has acquired through its training.
% Overall, LLMs have the potential to greatly enhance the capabilities of robots, making them more useful and effective in a wide range of applications, from manufacturing and logistics to healthcare and personal assistance.

% \subsubsection{Acoustics}
% Recent advances in acoustics have also been influenced by the development of LAMs.
% % is a self-supervised audio model. 
% W2v-BERT~\cite{chung2021w2v} and Wav2Vec~\cite{schneider2019wav2vec} is trained on large amounts of unlabeled audio data, and its resulting representations can be used to enhance acoustic model training.
% AudioLM~\cite{borsos2022audiolm} models an audio sequence hierarchically by chaining several Transformer models. Hidden-Unit BERT (HuBERT)~\cite{hsu2021hubert} learns a combined acoustic and language model over the continuous inputs. As a large-scale model for cross-lingual speech representation learning, XLS-R~\cite{babu2021xls} is trained with  up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages. MusicLM~\cite{agostinelli2023musiclm}, Diffsound~\cite{yang2022discrete}, and AudioGen~\cite{kreuk2023audiogen} can generate high-fidelity music, sound, or audios from text descriptions. Whisper~\cite{radford2022robust}  has been trained on an extensive set of varied audio and is designed to handle multiple tasks, including recognizing speech in different languages, translating speech, and identifying languages. Universal Speech Model (USM)~\cite{zhang2023google} demonstrates that utilizing a large unlabeled multilingual dataset to pre-train the model and fine-tuning on a smaller set of labeled data enables the recognition of under-represented languages. Overall, the application of LAMs to the field of acoustics has led to improvements in speech recognition, cross-lingual speech representation learning, and the generation of high-quality music and sound. 

% Several possible future directions for research in acoustics using LAMs include: 
% \begin{itemize}
%     \item Multimodal speech and audio processing: The integration of LAMs with other modalities such as vision and language could help to enhance speech and audio processing.
%     \item Real-time speech and audio processing: Developing LAMs that can operate efficiently in real-time settings will be an important direction for future research.
%     \item Privacy-preserving speech and audio processing:  As speech and audio data are often considered sensitive, there is a growing need for privacy-preserving techniques in speech and audio processing. LAMs that can perform speech and audio processing tasks while preserving privacy could be a valuable contribution to the field.
% \end{itemize}

% \subsubsection{Other Biomedical and Health LAMs}


% \begin{enumerate}
%     \item blind navigation
% \end{enumerate}


% \textbf{\textcolor{red}{Downstream: Robotics, Time-Series (Audio, EEG, etc). Also, let's put LAMs that were pretrained on Language and/or Vision, but their downstream tasks are in for example Robotics in this section}}

% \subsubsection{Other General LAMs}

% \begin{itemize}
%     \item NeurIPS 2022 Foundation Models for Decision Making Workshop: \url{https://openreview.net/group?id=NeurIPS.cc/2022/Workshop/FMDM}
% \end{itemize}

% \subsection{Reinforcement Learning in Large AI Models}
% % \subsubsection{General RL}
% Reinforcement Learning (RL) aims to enhance behavioral decision-making by using interaction experience with the world and evaluative feedback. Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive, and supervised reward signals, RL tackles sequential decision-making problems with sampled, evaluative, and delayed feedback simultaneously~\cite{sutton2018reinforcement}. To balance the data annotation cost and efficacy, as well as to train a LAM that can better align with human intent, researchers have commonly used reinforcement learning from human feedback (RLHF)~\cite{christiano2017deep} to develop LAMs especially LLMs that can exhibit desired behaviors. 

% In the past two decades, a large number of new deep RL (DRL) algorithms have been proposed to solve a wide range of continuous decision-making problems, as follows:
% \begin{enumerate}
%     \item AC~\cite{peters2008natural}: a mainstream algorithm based on policy gradient methods. This algorithm consists of two neural networks: Actor and Critic. Actor one is used to select appropriate actions in the continuous space, and Critic is used to judge whether the actions selected are reasonable, so as to continuously optimize the parameter of the neural networks and find the optimal policy.
%     \item TRPO~\cite{schulman2015trust}: an extension algorithm of the AC, introduced importance sampling and KL divergence constraint to solve the problem of low sampling efficiency. 
%     \item PPO~\cite{schulman2017proximal}: one of the most effective and widely used RL algorithms, proposed an objective function with clipped probability ratios to achieve minibatch updates in multiple epochs. 
% \end{enumerate}

% \subsubsection{LAMs with RL}
 % The core idea of (RLHF)~\cite{christiano2017deep} is to use human preference dataset to train a Reward Model (RM), which can predict the reward function and  be optimized by RL (e.g. TRPO~\cite{schulman2015trust} or PPO~\cite{schulman2017proximal}). By introducing a dynamic KullbackLeibler divergence constraint to the reward function, RLFH can be successfully used to process different tasks, such as continuing text and summarizing text from the CNN/Daily Mail~\cite{2019Fine}. RLHF can fine-tune and align a LLM by using human prior knowledge (i.e., human preference dataset), so as to avoid some extreme, malicious, and dangerous contents. In addition, compared with supervised learning, the human preference dataset needs less efforts to collect. Thus, the framework of RLHF has attracted much attention and become a key component of many LLMs, such as WebGPT~\cite{nakano2021webgpt}, InstructGPT~\cite{ouyang2022training}, Sparrow~\cite{glaese2022improving}, and ChatGPT~\cite{chatgpt2022}. Recently, Susano Pinto et al.~\cite{susano2023tuning} have also investigated this reward optimization in vision tasks, which can possibly advance the development of future LVMs using RLHF. 
% \begin{enumerate}
%     \item WebGPT~\cite{nakano2021webgpt}: with 175B parameters. It uses behavior cloning to fine-tune the GPT-3 model and then uses rejection sampling trained to predict human preferences by the RM. 
%      \item InstructGPT~\cite{ouyang2022training}: based on GPT-3 with 1.3B parameters. Similar to WebGPT, 
%      it mainly includes three parts: Use human preference data to conduct supervised training on GPT3; Train RM through the idea of RLHF; Predict the reward function through the trained RM model and optimize the model strategy through PPO~\cite{schulman2017proximal}.  The training process of ChatGPT is the same as the InstructGPT, but the pre-trained model is GPT-3.5. 
%     \item Sparrow~\cite{glaese2022improving}: based on Dialogue Prompted Chinchilla with 70B parameters. It provides a more correct and harmless pre-trained model by decomposing the requirements for good dialogue and collecting factual evidence.  
%     \item LVMs~\cite{susano2023tuning}: It first uses maximum-likelihood estimation (MLE) model to pre-train the large CV dataset (e.g. COCO) and then uses the REINFORCE algorithm~\cite{williams1992simple} to further tune the MLE model by maximizing the reward function. This method can be used to solve a variety of computer vision tasks, such as object detection, colorization, and image captioning.
% \end{enumerate}

% \textcolor{red}{JQ: I remember in slack we shared a post about RLHF emerging in large vision models??}

% \subsection{Large Datasets \textcolor{red}{(ALL)}}


% \textcolor{red}{shall we have a table that lists which dataset has been used by which models, like one of IJRR papers seen before}


% \subsubsection{Large General-Purpose Datasets \textcolor{red}{(final name TBD)}}
% let's organize datasets based on what type of model has been using them. i.e., a dataset might be used by multiple types of models if it is very large.


% \textbf{LLMs}

% \begin{enumerate}
%     \item Wikipedia and BookCorpus~\cite{zhu2015aligning}
% \end{enumerate}






% \textbf{LVMs}




% \textbf{LLVMs}





% \textbf{Others}








% \subsubsection{Large Biomedical and Health Datasets}

% Same as above \textcolor{red}{(go through the LAM papers to identify large datasets)}

% \textbf{LLMs}


% \begin{enumerate}
%     \item PubMed abstracts
%     \item PubMed Central full-text
%     \item MIMIC-III~\cite{johnson2016mimic}:


% \end{enumerate}



% \textbf{LVMs}

% \begin{enumerate}
%     \item fastMRI: A large-scale dataset of both raw MRI measurements and clinical MRI images.\cite{zbontar2018fastMRI}
%     \item CheXpert: A Large Chest X-Ray Dataset. contains 224,316 chest radiographs of 65,240 patients.
%     \item PadChest~\cite{pooch2020can}: 160,868 chest X-ray
    
%     \item OCTA 500,361600 scans.
%     \item AMOS dataset, AMOS provides 500 CT and 100 MRI scans with voxel-level annotations of 15 abdominal organs, including the spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, pancreas, right adrenal gland, left adrenal gland, duodenum, bladder, prostate/uterus.
%     \item Atlas Dione Dataset
%     \item \textcolor{blue}{shall we put it on table1?}\textbf{MedMNIST(v2)}: consisting of 708,069 2D images and 10,214 3D images
%     \item DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning.(32,735 lesions in 32,120 CT slices from 10,594 studies of 4,427 unique patients)
%     \item The National Library of Medicine presents MedPix.(Database of 53,000 medical images from 13,000 patients with annotations.)
%     \item OASIS The Open Access Series of Imaging Studies (OASIS) is a project aimed at making MRI data sets of the brain freely available to the scientific community. Two datasets are available: a cross-sectional and a longitudinal set.(High clinical value)
%     \item \textbf{Isic Archive} - Melanoma This archive contains 23k images of classified skin lesions. It contains both malignant and benign examples.
%     \item Lung Image Database Consortium (LIDC):a flexible query system that will provide investigators the opportunity to evaluate a wide range of technical parameters and de-identified clinical information within this database that may be important for research applications.
%     \item The ADAM Medical Encyclopedia with Illustrations. Perhaps one of the best illustrated medical works on the internet today, the ADAM Medical Encyclopedia includes over 4,000 articles about diseases, tests, symptoms, injuries, and surgeries. It also contains an extensive library of medical photographs and illustrations to back up those 4,000 articles. These illustrations and articles are free to the public.
%     \item Images from the History of Medicine - This system provides access to the nearly 60,000 images in the prints and photograph collection of the History of Medicine Division (HMD) of the U.S. National Library of Medicine (NLM). The collection includes portraits, pictures of institutions, caricatures, genre scenes, and graphic art in a variety of media, illustrating the social and historical aspects of medicine.
    
%     \item PadChest: A large chest x-ray image dataset with multi-label annotated reports\cite{bustos2020padchest}
% \end{enumerate}




% \textbf{LLVMs}

% \begin{enumerate}
    
%     \item MIMIC-CXR~\cite{johnson2019mimic}: it has chest X-ray images and their corresponding raw radiology reports
% \end{enumerate}



% \textbf{Others}



% \begin{enumerate}
%     \item CMNPD: a comprehensive marine natural products database towards facilitating drug discovery from the ocean.(Drug discovery)\cite{lyu2021cmnpd}
%     \item MIMIC-CXR~\cite{johnson2019mimic}: 377,110 images, 227,835 radiographic studies
%     \item CheXpert~\cite{irvin2019chexpert}: 224,316 chest radiographs, 65,240 patients
%     \item Uni-Mol: A Universal 3D Molecular Representation Learning Framework.(Uni-Mol is composed of two models with the same SE(3)-equivariant transformer architecture: a molecular pretraining model trained by 209M molecular conformations; a pocket pretraining model trained by 3M candidate protein pocket data.)
% \end{enumerate}









% \subsection{Downstream Tasks \textcolor{red}{(ALL)}}

% \subsubsection{Downstream Tasks in General Domain}

% let's list all downstream tasks found of LAMs here to help polish the final contents of this article and make our review more comprehensive. 

% \textbf{LLMs}

% check FLAN, T0, and BIG-bench datasets

% \textbf{LVMs}



% \textbf{LLVMs}



% \textbf{Others}



% \subsubsection{Downstream Tasks in Biomedical and Health Domain}

% Same as above

% \textbf{LLMs}

% \textcolor{blue}{need to mention BLURB~\cite{gu2021domain} since it contains 13 datasets for the six tasks below. Also MultiMedQA~\cite{singhal2022large} that contains MedQA, MedMCQA, PubMedQA, LiveQA, MedicationQA, MMLU clinical topics and HealthSearchQA}

% \begin{enumerate}
%     \item Queation Answering (QA): PubMedQA~\cite{jin2019pubmedqa}, BioASQ~\cite{nentidis2020results}, MedQA~\cite{jin2021disease}, MedMCQA~\cite{pal2022medmcqa}, LiveQA~\cite{abacha2017overview}, MedicationQA~\cite{abacha2019bridging}, MMLU clinical topics~\cite{hendrycks2020measuring}, HealthSearchQA~\cite{singhal2022large}
    
%     \item Named Entity Recognition (NER): BC5-chem~\cite{li2016biocreative}, BC5-disease~\cite{li2016biocreative}, BC5-CDR~\cite{li2016biocreative} \textcolor{red}{(need to double check this BC5-CDR)}, NCBI-disease~\cite{dougan2014ncbi}, BC2GM~\cite{smith2008overview}, BC4CHEMD~\cite{krallinger2015chemdner}, JNLPBA~\cite{collier2004introduction}, LINNAEUS~\cite{gerner2010linnaeus}, Species-800~\cite{pafilis2013species}, 2010 i2b2/VA~\cite{uzuner20112010}, I2B2-2012~\cite{sun2013evaluating}, I2B2-2014~\cite{stubbs2015annotating}, METS-CoV~\cite{zhou2022mets}

%     \item PICO Extraction: EBM PICO~\cite{nye2018corpus}

%     \item Relation Extraction (RE): ChemProt~\cite{krallinger2017overview}, DDI~\cite{herrero2013ddi}, GAD~\cite{bravo2015extraction}, EU-ADR~\cite{van2012eu}


%     \item Sentence Similarity: BIOSSES~\cite{souganciouglu2017biosses}, MedSTS~\cite{wang2020medsts}, ClinicalSTS~\cite{wang20202019}


%     \item Document Classification: HoC~\cite{hanahan2000hallmarks}

%     \item Inference: MedNLI~\cite{romanov2018lessons}

%     \item Sentiment Analysis: METS-CoV~\cite{zhou2022mets}

    
% \end{enumerate}


% \textcolor{blue}{dataset details:}

% \begin{enumerate}
%     \item METS-CoV~\cite{zhou2022mets}: A Dataset of Medical Entity and Targeted Sentiment on COVID-19 Related Tweets, but it only contains 10,000 tweets.

% \end{enumerate}


% \textbf{LVMs}




% \textbf{LLVMs}





% \textbf{Others}

% \begin{enumerate}
%     \item Medical Robotic Tasks: 




    
% \end{enumerate}




