\section{Challenges, Limitations, and Risks}\label{sec:challenges}

\begin{table*}[]
\centering
\caption{Large Biomedical, Clinical, and Health Datasets}
\label{tab:large_bio_dataset}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
Dataset Name            & Details                                                                                                                            & Data Modality & Task                                                                                  \\ \midrule
PubMed Abstract~\cite{pubmedabstract2023} &      a large archive of abstracts of biomedical literature                                                                                                                           & single        & medical language processing                                                           \\
PubMed Central~\cite{pubmedcentral2023}  &   a large archive of full-text biomedical and life sciences literature                                                                                                                              & single        & medical language processing                                                           \\
MIMIC-III~\cite{johnson2016mimic}       &     \begin{tabular}[c]{@{}l@{}}53,423 distinct hospital admissions \\ with detailed EHRs such as medications and physiologic signals \end{tabular}                                                                                                                  &     multiple          &   medical language processing                                                                                    \\
Clinical Practice Research Datalink~\cite{herrett2015data} & \begin{tabular}[c]{@{}l@{}}11.3 million patients covering data on \\ demographics, symptoms, diagnoses, therapies, etc\end{tabular} & multiple
& medical language processing
\\ \midrule
MIMIC-CXR\cite{johnson2019mimic}       & \begin{tabular}[c]{@{}l@{}}65,379 patients, containing 377,110 chest X-ray images,\\ and 227,835 radiology reports\end{tabular} & multiple      & \begin{tabular}[c]{@{}l@{}}medical language processing,\\ medical vision\end{tabular} \\ \midrule

CheXpert~\cite{irvin2019chexpert}        & 65,240 patients, containing 224,316 chest radiographs                                                                           & single        & medical vision                                                                        \\
PadChest~\cite{bustos2020padchest}     & 67,000 patients, containing over 160,000 chest X-ray images                                                                     & single        & medical vision
          \\
MURA~\cite{rajpurkar2017mura}        & 12,173 patients, containing over 40,561 multi-view radiographic images                                                                     & single        & medical vision
          \\
NIH Chest X-Rays~\cite{NIH}        & 30,000 patients, over 112,000 Chest X-ray images                                                                     & single        & medical vision
          \\

BRAX~\cite{reis2022brax}        & 19,351 patients, 24,959 case studies, 40,967 Chest X-ray images with 14 radiological findings                                                                    & multiple        & medical vision
\\
COVIDx~\cite{wang2020covid}        & 13,870 patients containing COVID-19 positive cases, 13,975 CXR images                                                                     & single        & medical vision
\\
Deep Lesion\cite{yan2018deeplesion}        & 4,000 patients, 32,000 lesions CT images                                                                     & single        & medical vision 
\\

MedPix~\cite{MedPix}         & 12,000 patients, 59,000 indexed and curated images                                                                     & multiple        & medical vision
\\
TCIA~\cite{clark2013cancer}         & an open-source archive containing millions of cancer images                                                                     & multiple        & medical vision
          \\
OCTA 500~\cite{li2020ipn}        & 500 patients, over 361,600 retinal scans                                                                     & multiple        & medical vision
          \\
Medical Segmentation Decathlon~\cite{antonelli2022medical}        & 10 organs, 2,642 3D \& 4D data of MRI, MR and CT modalities                                                                     & multiple        & medical vision
          \\
MedMNIST v2~\cite{yang2023medmnist}        & 708,069 2D images and 10,214 3D images                                                                     & multiple        & medical vision
          \\
Isic Archive~\cite{tschandl2018ham10000}        &  23,000 multi-source dermatoscopic images                                                                     & multiple        & medical vision

        \\ \midrule
          
UniParc\cite{uniprot2007universal}        & 250 million protein sequences, 86 billion amino acids                                                                     & single        & protein representation learning
          \\
UniProtKB\cite{uniprot2018uniprot}        & \begin{tabular}[c]{@{}l@{}}an integrated database that contains \\ various protein information with annotations\end{tabular}    & multiple   & protein representation learning
          \\
UniRef100\cite{suzek2015uniref}        & 200 million protein sequences, 80 billion amino acids                                                                     & single        & protein representation learning
          \\
UniRef90\cite{suzek2015uniref}         & similar sequences from UniRef100 clustered at 90\% identity level                                                                     & single        & protein representation learning
          \\
UniRef50\cite{suzek2015uniref}         & similar sequences from UniRef100 clustered at 50\% identity level                                                                     & single        & protein representation learning
          \\
Pfam\cite{mistry2021pfam}       &18,259 protein families and 635 clans    & single   & protein representation learning
\\
Big Fantastic Database \cite{steinegger2018clustering}       & 2.1 billion protein sequences, 393 billion amino acids   & single   & protein representation learning
\\
Observed Antibody Space\cite{kovaltsuk2018observed}        & 558 million antibody sequences                                                                     & single        & antibody representation learning
          \\
PanglaoDB\cite{franzen2019panglaodb}        & 74 tissues with 1,126,580 cells                                                                     & single        & scRNA-seq representation learning
          \\
RNAcentral\cite{rnacentral2021rnacentral}        & 18 million ncRNA sequences and 13 million secondary (2D) structure                                                                    & multiple        & RNA representation learning
          \\ \midrule
ChEMBL\cite{gaulton2012chembl}        & 1,961,462 different compounds and 13,382 targets                                                                     & single        & drug discovery
          \\
ZINC20\cite{irwin2020zinc20}      & 1.4 billion compounds from 310 catalogs from 150 companies                                                                     & single        & drug discovery
          \\
DrugSpaceX\cite{yang2021drugspacex}        & 12,443,292 different compounds                                                                     & single        & drug discovery
          \\
UniChem Database\cite{chambers2013unichem}       & over 22 million compounds from 40 different datasets                                                                    & single        & drug discovery
          \\
PubChem\cite{kim2019pubchem}        & \begin{tabular}[c]{@{}l@{}}114 million compounds, 302 million substances, 302 million bioactivities \\ 35 million literature, 42 million patents\end{tabular}                                                                     & multiple        & drug discovery
          \\
BindingDB Dataset~\cite{yazdani2022binddb}        & contains 2,656,564 data of 8,982 proteins and 1,144,641 molecules                                                                     & multiple        & drug discovery
          \\
DrugOOD Database   \cite{2022drugood}     & a consortium of 96 datasets for AI drug discovery                                                                    & multiple        & drug discovery
          \\
CMNPD   \cite{lyu2021cmnpd}     & 32,000 compounds, 3,400 organisms, 2,700 targets, 72,000 bioactivities                                                                     & multiple        &  drug discovery
          \\
Uni-Mol   \cite{zhou2023unimol}     & \begin{tabular}[c]{@{}l@{}}19 million molecular, 210 million 3D molecular conformations \\3.2 million protein pockets\end{tabular}                                                                  & multiple        &  drug discovery
\\ \bottomrule
\end{tabular}%
}
\end{table*}




Despite the promising outcome of LAMs, there remain many challenges and potential risks in developing and deploying LAMs in biomedical, clinical, and healthcare applications. 



Compared to datasets used to pre-train LAMs in the general domain, as shown in Table~\ref{tab:large_bio_dataset} some current large datasets in health informatics are still relatively small. This is either because the annotation efforts are onerous, which hence restricts the scale of the labeled dataset, or because that particular type of data is in fact hard to obtain. For the former, as LAMs can use self-supervision to leverage unlabelled data, it may not be a hurdle for developing and advancing LAMs in the field, whereas for the latter case, it may not be intractable as well, as LAMs show impressive zero- and few-shot learning capabilities, but more research needs to be conducted to verify the zero- and few-shot learning capabilities of LAMs in the medical and clinical settings. 

Although the volume of unlabelled medical and health data is large, aggregating them and then curating datasets to pre-train LAMs still require proper handling of the underlying ethical, legal, safety, and privacy issues. In biomedical and healthcare scenarios, the robustness and fairness of LAMs are critical. Although the data-driven paradigm underpins the rapid invention of LAMs, biomedical and healthcare data inherently exhibit inequality. Current datasets for developing LAMs are biased against resource-poor countries and rare diseases, but the development will ultimately benefit them the most. This paradox poses challenges and requires continuous monitoring and efforts to ensure future medical LAMs will not develop biases (a recent study~\cite{zhuo2023exploring} revealed that ChatGPT can generate toxic, hazardous, biased contents). More importantly, LAMs such as ChatGPT are also vulnerable to adversarial attacks~\cite{wang2023robustness}. Robustifying LAMs against adversarial attacks requires even more data \cite{li2023data}. These, if not handled precautiously, pose potential risks such as reducing the credibility of LAMs in advising medical and clinical practitioners and the general public, and even more significant privacy issues such as a deployed LAM repeating back training data that potentially expose patient details. 

For many researchers and institutions, the expensive computing infrastructure for developing and deploying LAMs is a practical major roadblock. Therefore, to create a sustainable ecosystem that fosters and docks future LAMs, which could benefit the medical and healthcare society at large, joint and collective efforts from researchers and policymakers are needed. Regulations for applying LAMs in medical and clinical settings should also be implemented, e.g., what level of autonomy should be endowed to LAMs to assist robotic surgery~\cite{yang2017medical} and the accountability of the potential failures from medical LAMs. 

% \textcolor{red}{
% Undoubtedly, these models are transformational and have reduced the time and cost of molecule structure prediction by a large margin. Thereby, this raises the question of whether AI can completely replace experimental methods such as Cryo-EM\cite{bai2015cryo}. We deem that it still falls short from that point. Specifically, the advance of LAMs builds upon big data and large model capacity, which means their ability in predicting unseen types of data could be problematic. For instance, \cite{buel2022can} stated that AlphaFold can barely handle missense mutation on protein structure due to the lack of a corresponding dataset. Furthermore, how can we judge the quality of model prediction for unknown protein structures still remains unclear. In turn, these unverified protein structures cannot be applied to, for example, drug discovery. Therefore, protocols and metrics need to be established to assess their quality and potential impact. We also suggest that, at the current stage, LAMs should stand in a complementary position, together with conventional experimental techniques to benefit molecular biology. LAMs can be re-designed to predict the process of protein folding and reveal their mutual interactions so as to facilitate experimental methods. On the other hand, experimental information such as some physical properties of molecules can be leveraged by LAMs to further improve prediction performance, especially when rare data is available for training (e.g., orphan protein).
% }


The development of medical LAMs also faces transparency challenges. In the general domain, it is not uncommon to see many large-scale datasets that were used to pretrain LAMs remain closed source. The LAMs themselves are also restricted to limited access to outside researchers. These are expected to be more common in biomedical, clinical, and health domains, as the data are often expensive and under stricter scrutiny. Without transparency and more open-source efforts, the development of medical LAMs may only be a carnival and monopoly of a paucity of big companies and institutions.



% \begin{itemize}
%     \item data - scale
%     \begin{itemize}
%         \item more expensive to collect data and curate
%         \item more restrictive usage due to societal and regulartory concerns
%         \item more motivated to not open source
%     \end{itemize}
%     \item model
%     \begin{itemize}
%         \item adapt to the exclusive modality of medical data like EGG
%         \item how to fuse more diverse modalities than just language-vision
%     \end{itemize}
%     \item albeit high-performing and versatility, issues still exist
%     \begin{itemize}
%         \item adversarial robustness
%         \item toxic prompt
%         \item less curated training corpus may bring bias
%     \end{itemize}
% \end{itemize}






% \subsection{Ideas}

% \textcolor{red}{need to identify more challenges, limitations, and risks}

% \begin{enumerate}
    
%     \item ChatGPT coauthored a paper: Open artificial intelligence platforms in nursing education: Tools for academic progress or abuse?~\cite{o2022open}
%     \item Generating scholarly content with ChatGPT: ethical challenges for medical publishing~\cite{liebrenz2023generating}
% \end{enumerate}