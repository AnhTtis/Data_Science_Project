\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}



\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tablefootnote}
\usepackage{soul}



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{}
{Qiu \MakeLowercase{\textit{et al.}}: Large AI Models in Health Informatics: Applications, Challenges, and the Future}
\begin{document}
\title{Large AI Models in Health Informatics: Applications, Challenges, and the Future}
\author{Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, \\Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P.-W. Lo, Bo Xiao, \\Wu Yuan,  \IEEEmembership{Senior Member, IEEE}, Dong Xu, and Benny Lo, \IEEEmembership{Senior Member, IEEE}
% \thanks{This paragraph of the first footnote will contain the date on 
% which you submitted your paper for review. It will also contain support 
% information, including sponsor and financial support acknowledgment. For 
% example, ``This work was supported in part by the U.S. Department of 
% Commerce under Grant BS123456.'' }
% \thanks{The next few paragraphs should contain 
% the authors' current affiliations, including current address and e-mail. For 
% example, F. A. Author is with the National Institute of Standards and 
% Technology, Boulder, CO 80305 USA (e-mail: author@boulder.nist.gov). }
\thanks{Jianing Qiu is with the Department of Computing, Imperial College London, U.K., and he is also with Precision Robotics (Hong Kong) Ltd., Hong Kong (e-mail: jianing.qiu17@imperial.ac.uk).}
\thanks{Lin Li is with the Department of Informatics, King's College London, U.K. (e-mail: lin.3.li@kcl.ac.uk).}
% \thanks{Jiankai Sun is with the Department of Aeronautics and Astronautics, Stanford University, USA (e-mail: jksun@stanford.edu).}
\thanks{Jiankai Sun is with the School of Engineering, Stanford University, USA (e-mail: jksun@stanford.edu).}
\thanks{Jiachuan Peng is with the Department of Engineering Science, University of Oxford (e-mail: jiachuan.peng@seh.ox.ac.uk).}
\thanks{Peilun Shi and Wu Yuan are with the Department of Biomedical Engineering,  The Chinese University of Hong Kong, Hong Kong (e-mails: peilunshi@cuhk.edu.hk and wyuan@cuhk.edu.hk).}
\thanks{Ruiyang Zhang is with Precision Robotics (Hong Kong) Ltd., Hong Kong (e-mail: r.zhang@prhk.ltd).}
% \thanks{Yinzhao Dong is with the Department of Mechanical Engineering, The University of Hong Kong, Hong Kong (e-mail: dongyz@connect.hku.hk).}
\thanks{Yinzhao Dong is with the Faculty of Engineering, The University of Hong Kong, Hong Kong (e-mail: dongyz@connect.hku.hk).}
\thanks{Kyle Lam is with the Department of Surgery and Cancer, Imperial
College London, U.K. (e-mail: k.lam@imperial.ac.uk).}
\thanks{Frank P.-W. Lo is with the Department of Surgery and Cancer, and the Hamlyn Centre for Robotic Surgery, Imperial College London, U.K. (e-mails: po.lo15@imperial.ac.uk).}
\thanks{Bo Xiao is with the Department of Electrical and Electronic Engineering, and the Hamlyn Centre for Robotic Surgery, Imperial College London, U.K. (e-mail: b.xiao@imperial.ac.uk).}
\thanks{Dong Xu is with the Department of Electrical Engineering and Computer Science, and the Christopher S. Bond Life Sciences Center, University of Missouri, USA (e-mail: xudong@missouri.edu).}
\thanks{Benny Lo is with the Department of Surgery and Cancer, and the Hamlyn Centre for Robotic Surgery, Imperial College London, U.K., and he is also with Precision Robotics (Hong Kong) Ltd., Hong Kong (e-mail: benny.lo@imperial.ac.uk).}}

\maketitle

\begin{abstract}
Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which often reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A concrete example is the recent debut of ChatGPT, whose capability has compelled people’s imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our life. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multimodality data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents an up-to-date comprehensive review of large AI models, from background to their applications. We identify seven key sectors that large AI models are applicable and might have substantial influence, including 1) molecular biology and drug discovery; 2) medical diagnosis and decision-making; 3) medical imaging and vision; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges in health informatics, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.
\end{abstract}

\begin{IEEEkeywords}
artificial intelligence; bioinformatics; biomedicine; deep learning; foundation model; health informatics; healthcare
\end{IEEEkeywords}


\input{introduction.tex}
\input{background_large_ai_model.tex}
\input{applications.tex}
\input{challenges.tex}
\input{future.tex}
\input{conclusions.tex}




% \section*{Acknowledgment}

% Checklist:

% \begin{enumerate}
%     \item Own words, no copy and paste.
%     \item Discussion, issues, and mentioned related work in \textbf{Slack} have been addressed and added
%     \item Ask Kyle to generate some clinical contents
%     \item The studies we mentioned are the latest. If the contents are overloaded, remove some early work. 
% \end{enumerate}



\bibliographystyle{IEEEtran}
% \bibliography{reference}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{chatgpt2022}
\BIBentryALTinterwordspacing
OpenAI, ``Chatgpt: Optimizing language models for dialogue,'' 2022. [Online].
  Available: \url{https://openai.com/blog/chatgpt/}
\BIBentrySTDinterwordspacing

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{bommasani_opportunities_2022}
\BIBentryALTinterwordspacing
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.
  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill, E.~Brynjolfsson, S.~Buch,
  D.~Card, R.~Castellon, N.~Chatterji, A.~Chen, K.~Creel, J.~Q. Davis,
  D.~Demszky, C.~Donahue, M.~Doumbouya, E.~Durmus, S.~Ermon, J.~Etchemendy,
  K.~Ethayarajh, L.~Fei-Fei, C.~Finn, T.~Gale, L.~Gillespie, K.~Goel,
  N.~Goodman, S.~Grossman, N.~Guha, T.~Hashimoto, P.~Henderson, J.~Hewitt,
  D.~E. Ho, J.~Hong, K.~Hsu, J.~Huang, T.~Icard, S.~Jain, D.~Jurafsky,
  P.~Kalluri, S.~Karamcheti, G.~Keeling, F.~Khani, O.~Khattab, P.~W. Koh,
  M.~Krass, R.~Krishna, R.~Kuditipudi, A.~Kumar, F.~Ladhak, M.~Lee, T.~Lee,
  J.~Leskovec, I.~Levent, X.~L. Li, X.~Li, T.~Ma, A.~Malik, C.~D. Manning,
  S.~Mirchandani, E.~Mitchell, Z.~Munyikwa, S.~Nair, A.~Narayan, D.~Narayanan,
  B.~Newman, A.~Nie, J.~C. Niebles, H.~Nilforoshan, J.~Nyarko, G.~Ogut, L.~Orr,
  I.~Papadimitriou, J.~S. Park, C.~Piech, E.~Portelance, C.~Potts,
  A.~Raghunathan, R.~Reich, H.~Ren, F.~Rong, Y.~Roohani, C.~Ruiz, J.~Ryan,
  C.~Ré, D.~Sadigh, S.~Sagawa, K.~Santhanam, A.~Shih, K.~Srinivasan,
  A.~Tamkin, R.~Taori, A.~W. Thomas, F.~Tramèr, R.~E. Wang, W.~Wang, B.~Wu,
  J.~Wu, Y.~Wu, S.~M. Xie, M.~Yasunaga, J.~You, M.~Zaharia, M.~Zhang, T.~Zhang,
  X.~Zhang, Y.~Zhang, L.~Zheng, K.~Zhou, and P.~Liang, ``On the {Opportunities}
  and {Risks} of {Foundation} {Models},'' Jul. 2022, arXiv:2108.07258 [cs].
  [Online]. Available: \url{http://arxiv.org/abs/2108.07258}
\BIBentrySTDinterwordspacing

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar \emph{et~al.}, ``Llama: Open
  and efficient foundation language models,'' \emph{arXiv preprint
  arXiv:2302.13971}, 2023.

\bibitem{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma \emph{et~al.}, ``Scaling instruction-finetuned
  language models,'' \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann \emph{et~al.}, ``Palm: Scaling language
  modeling with pathways,'' \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark \emph{et~al.}, ``Training
  compute-optimal large language models,'' \emph{arXiv preprint
  arXiv:2203.15556}, 2022.

\bibitem{smith2022using}
S.~Smith, M.~Patwary, B.~Norick, P.~LeGresley, S.~Rajbhandari, J.~Casper,
  Z.~Liu, S.~Prabhumoye, G.~Zerveas, V.~Korthikanti \emph{et~al.}, ``Using
  deepspeed and megatron to train megatron-turing nlg 530b, a large-scale
  generative language model,'' \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem{scao2022bloom}
T.~L. Scao, A.~Fan, C.~Akiki, E.~Pavlick, S.~Ili{\'c}, D.~Hesslow,
  R.~Castagn{\'e}, A.~S. Luccioni, F.~Yvon, M.~Gall{\'e} \emph{et~al.},
  ``Bloom: A 176b-parameter open-access multilingual language model,''
  \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem{thoppilan2022lamda}
R.~Thoppilan, D.~De~Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du \emph{et~al.}, ``Lamda: Language models for
  dialog applications,'' \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin \emph{et~al.}, ``Opt: Open pre-trained transformer language
  models,'' \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray \emph{et~al.}, ``Training language models to
  follow instructions with human feedback,'' \emph{arXiv preprint
  arXiv:2203.02155}, 2022.

\bibitem{rae2021scaling}
J.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song,
  J.~Aslanides, S.~Henderson, R.~Ring, S.~Young \emph{et~al.}, ``Scaling
  language models: Methods, analysis \& insights from training gopher,''
  \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{sanh2021multitask}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,
  A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja \emph{et~al.}, ``Multitask
  prompted training enables zero-shot task generalization,'' \emph{arXiv
  preprint arXiv:2110.08207}, 2021.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a
  unified text-to-text transformer,'' \emph{The Journal of Machine Learning
  Research}, vol.~21, no.~1, pp. 5485--5551, 2020.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{gpt4openai2023}
O.~(2023), ``Gpt-4 technical report,'' \emph{arXiv preprint arXiv:2303.08774},
  2023.

\bibitem{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, E.~Chi, Q.~Le, and D.~Zhou, ``Chain
  of thought prompting elicits reasoning in large language models,''
  \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem{borgeaud2022improving}
S.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican, G.~B.
  Van Den~Driessche, J.-B. Lespiau, B.~Damoc, A.~Clark \emph{et~al.},
  ``Improving language models by retrieving from trillions of tokens,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2022, pp. 2206--2240.

\bibitem{christiano2017deep}
P.~F. Christiano, J.~Leike, T.~Brown, M.~Martic, S.~Legg, and D.~Amodei, ``Deep
  reinforcement learning from human preferences,'' \emph{Advances in neural
  information processing systems}, vol.~30, 2017.

\bibitem{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz, ``Trust region
  policy optimization,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2015, pp. 1889--1897.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{nakano2021webgpt}
R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain,
  V.~Kosaraju, W.~Saunders \emph{et~al.}, ``Webgpt: Browser-assisted
  question-answering with human feedback,'' \emph{arXiv preprint
  arXiv:2112.09332}, 2021.

\bibitem{glaese2022improving}
A.~Glaese, N.~McAleese, M.~Trebacz, J.~Aslanides, V.~Firoiu, T.~Ewalds,
  M.~Rauh, L.~Weidinger, M.~Chadwick, P.~Thacker \emph{et~al.}, ``Improving
  alignment of dialogue agents via targeted human judgements,'' \emph{arXiv
  preprint arXiv:2209.14375}, 2022.

\bibitem{susano2023tuning}
A.~Susano~Pinto, A.~Kolesnikov, Y.~Shi, L.~Beyer, and X.~Zhai, ``Tuning
  computer vision models with task rewards,'' \emph{arXiv e-prints}, pp.
  arXiv--2302, 2023.

\bibitem{yosinski2014transferable}
J.~Yosinski, J.~Clune, Y.~Bengio, and H.~Lipson, ``How transferable are
  features in deep neural networks?'' \emph{Advances in neural information
  processing systems}, vol.~27, 2014.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{2009 IEEE conference on
  computer vision and pattern recognition}.\hskip 1em plus 0.5em minus
  0.4em\relax Ieee, 2009, pp. 248--255.

\bibitem{ridnik1imagenet}
T.~Ridnik, E.~Ben-Baruch, A.~Noy, and L.~Zelnik-Manor, ``Imagenet-21k
  pretraining for the masses,'' in \emph{Thirty-fifth Conference on Neural
  Information Processing Systems Datasets and Benchmarks Track (Round 1)}.

\bibitem{sun2017revisiting}
C.~Sun, A.~Shrivastava, S.~Singh, and A.~Gupta, ``Revisiting unreasonable
  effectiveness of data in deep learning era,'' in \emph{Proceedings of the
  IEEE international conference on computer vision}, 2017, pp. 843--852.

\bibitem{zhai2022scaling}
X.~Zhai, A.~Kolesnikov, N.~Houlsby, and L.~Beyer, ``Scaling vision
  transformers,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, 2022, pp. 12\,104--12\,113.

\bibitem{Dehghani2023ScalingVT}
M.~Dehghani, J.~Djolonga, B.~Mustafa, P.~Padlewski, J.~Heek, J.~Gilmer,
  A.~Steiner, M.~Caron, R.~Geirhos, I.~M. Alabdulmohsin, R.~Jenatton, L.~Beyer,
  M.~Tschannen, A.~Arnab, X.~Wang, C.~Riquelme, M.~Minderer, J.~Puigcerver,
  U.~Evci, M.~Kumar, S.~van Steenkiste, G.~F. Elsayed, A.~Mahendran, F.~Yu,
  A.~Oliver, F.~Huot, J.~Bastings, M.~Collier, A.~A. Gritsenko, V.~Birodkar,
  C.~N. Vasconcelos, Y.~Tay, T.~Mensink, A.~Kolesnikov, F.~Paveti'c, D.~Tran,
  T.~Kipf, M.~Luvci'c, X.~Zhai, D.~Keysers, J.~Harmsen, and N.~Houlsby,
  ``Scaling vision transformers to 22 billion parameters,'' \emph{ArXiv}, vol.
  abs/2302.05442, 2023.

\bibitem{goyal2021self}
P.~Goyal, M.~Caron, B.~Lefaudeux, M.~Xu, P.~Wang, V.~Pai, M.~Singh,
  V.~Liptchinsky, I.~Misra, A.~Joulin \emph{et~al.}, ``Self-supervised
  pretraining of visual features in the wild,'' \emph{arXiv preprint
  arXiv:2103.01988}, 2021.

\bibitem{mahajan2018exploring}
D.~Mahajan, R.~Girshick, V.~Ramanathan, K.~He, M.~Paluri, Y.~Li, A.~Bharambe,
  and L.~Van Der~Maaten, ``Exploring the limits of weakly supervised
  pretraining,'' in \emph{Proceedings of the European conference on computer
  vision (ECCV)}, 2018, pp. 181--196.

\bibitem{singh2022revisiting}
M.~Singh, L.~Gustafson, A.~Adcock, V.~de~Freitas~Reis, B.~Gedik, R.~P.
  Kosaraju, D.~Mahajan, R.~Girshick, P.~Doll{\'a}r, and L.~Van Der~Maaten,
  ``Revisiting weakly supervised pre-training of visual perception models,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2022, pp. 804--814.

\bibitem{chen_generative_2020}
\BIBentryALTinterwordspacing
M.~Chen, A.~Radford, R.~Child, J.~Wu, H.~Jun, D.~Luan, and I.~Sutskever,
  ``\BIBforeignlanguage{en}{Generative {Pretraining} {From} {Pixels}},'' in
  \emph{\BIBforeignlanguage{en}{Proceedings of the 37th {International}
  {Conference} on {Machine} {Learning}}}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, Nov. 2020, pp. 1691--1703, iSSN: 2640-3498. [Online].
  Available: \url{https://proceedings.mlr.press/v119/chen20s.html}
\BIBentrySTDinterwordspacing

\bibitem{chen2021pre}
H.~Chen, Y.~Wang, T.~Guo, C.~Xu, Y.~Deng, Z.~Liu, S.~Ma, C.~Xu, C.~Xu, and
  W.~Gao, ``Pre-trained image processing transformer,'' in \emph{Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021,
  pp. 12\,299--12\,310.

\bibitem{he_masked_2022}
\BIBentryALTinterwordspacing
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Dollár, and R.~Girshick,
  ``\BIBforeignlanguage{en}{Masked {Autoencoders} {Are} {Scalable} {Vision}
  {Learners}},'' 2022, pp. 16\,000--16\,009. [Online]. Available:
  \url{https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{International
  conference on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2020, pp. 1597--1607.

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick, ``Momentum contrast for
  unsupervised visual representation learning,'' in \emph{Proceedings of the
  IEEE/CVF conference on computer vision and pattern recognition}, 2020, pp.
  9729--9738.

\bibitem{liu2022convnet}
Z.~Liu, H.~Mao, C.-Y. Wu, C.~Feichtenhofer, T.~Darrell, and S.~Xie, ``A convnet
  for the 2020s,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, 2022, pp. 11\,976--11\,986.

\bibitem{krizhevsky2017imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' \emph{Communications of the ACM},
  vol.~60, no.~6, pp. 84--90, 2017.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2015, pp. 1--9.

\bibitem{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Delving deep into rectifiers: Surpassing
  human-level performance on imagenet classification,'' in \emph{Proceedings of
  the IEEE international conference on computer vision}, 2015, pp. 1026--1034.

\bibitem{he2016deep}
------, ``Deep residual learning for image recognition,'' in \emph{Proceedings
  of the IEEE conference on computer vision and pattern recognition}, 2016, pp.
  770--778.

\bibitem{he2016identity}
------, ``Identity mappings in deep residual networks,'' in \emph{Computer
  Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
  October 11--14, 2016, Proceedings, Part IV 14}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2016, pp. 630--645.

\bibitem{xie2017aggregated}
S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He, ``Aggregated residual
  transformations for deep neural networks,'' in \emph{Proceedings of the IEEE
  conference on computer vision and pattern recognition}, 2017, pp. 1492--1500.

\bibitem{zoph2018learning}
B.~Zoph, V.~Vasudevan, J.~Shlens, and Q.~V. Le, ``Learning transferable
  architectures for scalable image recognition,'' in \emph{Proceedings of the
  IEEE conference on computer vision and pattern recognition}, 2018, pp.
  8697--8710.

\bibitem{radosavovic2020designing}
I.~Radosavovic, R.~P. Kosaraju, R.~Girshick, K.~He, and P.~Doll{\'a}r,
  ``Designing network design spaces,'' in \emph{Proceedings of the IEEE/CVF
  conference on computer vision and pattern recognition}, 2020, pp.
  10\,428--10\,436.

\bibitem{tan2019efficientnet}
M.~Tan and Q.~Le, ``Efficientnet: Rethinking model scaling for convolutional
  neural networks,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 6105--6114.

\bibitem{tan2021efficientnetv2}
------, ``Efficientnetv2: Smaller models and faster training,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2021, pp. 10\,096--10\,106.

\bibitem{kolesnikov2020big}
A.~Kolesnikov, L.~Beyer, X.~Zhai, J.~Puigcerver, J.~Yung, S.~Gelly, and
  N.~Houlsby, ``Big transfer (bit): General visual representation learning,''
  in \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK,
  August 23--28, 2020, Proceedings, Part V 16}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2020, pp. 491--507.

\bibitem{huang2019gpipe}
Y.~Huang, Y.~Cheng, A.~Bapna, O.~Firat, D.~Chen, M.~Chen, H.~Lee, J.~Ngiam,
  Q.~V. Le, Y.~Wu \emph{et~al.}, ``Gpipe: Efficient training of giant neural
  networks using pipeline parallelism,'' \emph{Advances in neural information
  processing systems}, vol.~32, 2019.

\bibitem{wang2022internimage}
W.~Wang, J.~Dai, Z.~Chen, Z.~Huang, Z.~Li, X.~Zhu, X.~Hu, T.~Lu, L.~Lu, H.~Li
  \emph{et~al.}, ``Internimage: Exploring large-scale vision foundation models
  with deformable convolutions,'' \emph{arXiv preprint arXiv:2211.05778}, 2022.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2021, pp. 10\,012--10\,022.

\bibitem{dosovitskiy_image_2021}
\BIBentryALTinterwordspacing
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``\BIBforeignlanguage{en}{An {Image} is {Worth} 16x16
  {Words}: {Transformers} for {Image} {Recognition} at {Scale}},'' 2021.
  [Online]. Available:
  \url{https://openreview.net/forum?id=YicbFdNTTy&utm_campaign=f86497ed3a-EMAIL_CAMPAIGN_2019_04_24_03_18_COPY_01&utm_medium=email&utm_source=Deep%20Learning%20Weekly&utm_term=0_384567b42d-f86497ed3a-72965345}
\BIBentrySTDinterwordspacing

\bibitem{han_transformer_2021}
\BIBentryALTinterwordspacing
K.~Han, A.~Xiao, E.~Wu, J.~Guo, C.~XU, and Y.~Wang, ``Transformer in
  {Transformer},'' in \emph{Advances in {Neural} {Information} {Processing}
  {Systems}}, vol.~34.\hskip 1em plus 0.5em minus 0.4em\relax Curran
  Associates, Inc., 2021, pp. 15\,908--15\,919. [Online]. Available:
  \url{https://proceedings.neurips.cc/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html}
\BIBentrySTDinterwordspacing

\bibitem{liu_swin_2022}
\BIBentryALTinterwordspacing
Z.~Liu, H.~Hu, Y.~Lin, Z.~Yao, Z.~Xie, Y.~Wei, J.~Ning, Y.~Cao, Z.~Zhang,
  L.~Dong, F.~Wei, and B.~Guo, ``\BIBforeignlanguage{en}{Swin {Transformer}
  {V2}: {Scaling} {Up} {Capacity} and {Resolution}},'' 2022, pp.
  12\,009--12\,019. [Online]. Available:
  \url{https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{chung2021w2v}
Y.-A. Chung, Y.~Zhang, W.~Han, C.-C. Chiu, J.~Qin, R.~Pang, and Y.~Wu,
  ``W2v-bert: Combining contrastive learning and masked language modeling for
  self-supervised speech pre-training,'' in \emph{2021 IEEE Automatic Speech
  Recognition and Understanding Workshop (ASRU)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2021, pp. 244--250.

\bibitem{schneider2019wav2vec}
S.~Schneider, A.~Baevski, R.~Collobert, and M.~Auli, ``wav2vec: Unsupervised
  pre-training for speech recognition,'' \emph{arXiv preprint
  arXiv:1904.05862}, 2019.

\bibitem{borsos2022audiolm}
Z.~Borsos, R.~Marinier, D.~Vincent, E.~Kharitonov, O.~Pietquin, M.~Sharifi,
  O.~Teboul, D.~Grangier, M.~Tagliasacchi, and N.~Zeghidour, ``Audiolm: a
  language modeling approach to audio generation,'' \emph{arXiv preprint
  arXiv:2209.03143}, 2022.

\bibitem{hsu2021hubert}
W.-N. Hsu, B.~Bolte, Y.-H.~H. Tsai, K.~Lakhotia, R.~Salakhutdinov, and
  A.~Mohamed, ``Hubert: Self-supervised speech representation learning by
  masked prediction of hidden units,'' \emph{IEEE/ACM Transactions on Audio,
  Speech, and Language Processing}, vol.~29, pp. 3451--3460, 2021.

\bibitem{babu2021xls}
A.~Babu, C.~Wang, A.~Tjandra, K.~Lakhotia, Q.~Xu, N.~Goyal, K.~Singh, P.~von
  Platen, Y.~Saraf, J.~Pino \emph{et~al.}, ``Xls-r: Self-supervised
  cross-lingual speech representation learning at scale,'' \emph{arXiv preprint
  arXiv:2111.09296}, 2021.

\bibitem{agostinelli2023musiclm}
A.~Agostinelli, T.~I. Denk, Z.~Borsos, J.~Engel, M.~Verzetti, A.~Caillon,
  Q.~Huang, A.~Jansen, A.~Roberts, M.~Tagliasacchi \emph{et~al.}, ``Musiclm:
  Generating music from text,'' \emph{arXiv preprint arXiv:2301.11325}, 2023.

\bibitem{yang2022discrete}
D.~Yang, J.~Yu, H.~Wang, W.~Wang, C.~Weng, Y.~Zou, and D.~Y. Diffsound,
  ``Discrete diffusion model for text-to-sound generation,'' \emph{arXiv
  preprint arXiv:2207.09983}, vol.~2, 2022.

\bibitem{kreuk2023audiogen}
\BIBentryALTinterwordspacing
F.~Kreuk, G.~Synnaeve, A.~Polyak, U.~Singer, A.~D{\'e}fossez, J.~Copet,
  D.~Parikh, Y.~Taigman, and Y.~Adi, ``Audiogen: Textually guided audio
  generation,'' in \emph{The Eleventh International Conference on Learning
  Representations}, 2023. [Online]. Available:
  \url{https://openreview.net/forum?id=CYK7RfcOzQ4}
\BIBentrySTDinterwordspacing

\bibitem{radford2022robust}
A.~Radford, J.~W. Kim, T.~Xu, G.~Brockman, C.~McLeavey, and I.~Sutskever,
  ``Robust speech recognition via large-scale weak supervision,'' \emph{arXiv
  preprint arXiv:2212.04356}, 2022.

\bibitem{zhang2023google}
Y.~Zhang, W.~Han, J.~Qin, Y.~Wang, A.~Bapna, Z.~Chen, N.~Chen, B.~Li,
  V.~Axelrod, G.~Wang \emph{et~al.}, ``Google usm: Scaling automatic speech
  recognition beyond 100 languages,'' \emph{arXiv preprint arXiv:2303.01037},
  2023.

\bibitem{ordonez_im2text_2011}
\BIBentryALTinterwordspacing
V.~Ordonez, G.~Kulkarni, and T.~Berg, ``{Im2Text}: {Describing} {Images}
  {Using} 1 {Million} {Captioned} {Photographs},'' in \emph{Advances in
  {Neural} {Information} {Processing} {Systems}}, vol.~24.\hskip 1em plus 0.5em
  minus 0.4em\relax Curran Associates, Inc., 2011. [Online]. Available:
  \url{https://papers.nips.cc/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html}
\BIBentrySTDinterwordspacing

\bibitem{young_image_2014}
\BIBentryALTinterwordspacing
P.~Young, A.~Lai, M.~Hodosh, and J.~Hockenmaier, ``From image descriptions to
  visual denotations: {New} similarity metrics for semantic inference over
  event descriptions,'' \emph{Transactions of the Association for Computational
  Linguistics}, vol.~2, pp. 67--78, Feb. 2014. [Online]. Available:
  \url{https://doi.org/10.1162/tacl_a_00166}
\BIBentrySTDinterwordspacing

\bibitem{chen_microsoft_2015}
\BIBentryALTinterwordspacing
X.~Chen, H.~Fang, T.-Y. Lin, R.~Vedantam, S.~Gupta, P.~Dollar, and C.~L.
  Zitnick, ``Microsoft {COCO} {Captions}: {Data} {Collection} and {Evaluation}
  {Server},'' Apr. 2015, arXiv:1504.00325 [cs]. [Online]. Available:
  \url{http://arxiv.org/abs/1504.00325}
\BIBentrySTDinterwordspacing

\bibitem{krishna_visual_2017}
\BIBentryALTinterwordspacing
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.-J. Li, D.~A. Shamma, M.~S. Bernstein, and L.~Fei-Fei,
  ``\BIBforeignlanguage{en}{Visual {Genome}: {Connecting} {Language} and
  {Vision} {Using} {Crowdsourced} {Dense} {Image} {Annotations}},''
  \emph{\BIBforeignlanguage{en}{International Journal of Computer Vision}},
  vol. 123, no.~1, pp. 32--73, May 2017. [Online]. Available:
  \url{https://doi.org/10.1007/s11263-016-0981-7}
\BIBentrySTDinterwordspacing

\bibitem{sharma_conceptual_2018}
\BIBentryALTinterwordspacing
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut, ``Conceptual {Captions}: {A}
  {Cleaned}, {Hypernymed}, {Image} {Alt}-text {Dataset} {For} {Automatic}
  {Image} {Captioning},'' in \emph{Proceedings of the 56th {Annual} {Meeting}
  of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long}
  {Papers})}.\hskip 1em plus 0.5em minus 0.4em\relax Melbourne, Australia:
  Association for Computational Linguistics, Jul. 2018, pp. 2556--2565.
  [Online]. Available: \url{https://aclanthology.org/P18-1238}
\BIBentrySTDinterwordspacing

\bibitem{zhu_visual7w_2016}
\BIBentryALTinterwordspacing
Y.~Zhu, O.~Groth, M.~Bernstein, and L.~Fei-Fei, ``{Visual7W}: {Grounded}
  {Question} {Answering} in {Images},'' 2016, pp. 4995--5004. [Online].
  Available:
  \url{https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Visual7W_Grounded_Question_CVPR_2016_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{hudson_gqa_2019}
\BIBentryALTinterwordspacing
D.~A. Hudson and C.~D. Manning, ``{GQA}: {A} {New} {Dataset} for {Real}-{World}
  {Visual} {Reasoning} and {Compositional} {Question} {Answering},'' 2019, pp.
  6700--6709. [Online]. Available:
  \url{https://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{goyal_making_2019}
\BIBentryALTinterwordspacing
Y.~Goyal, T.~Khot, A.~Agrawal, D.~Summers-Stay, D.~Batra, and D.~Parikh,
  ``\BIBforeignlanguage{en}{Making the {V} in {VQA} {Matter}: {Elevating} the
  {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}},''
  \emph{\BIBforeignlanguage{en}{International Journal of Computer Vision}},
  vol. 127, no.~4, pp. 398--414, Apr. 2019. [Online]. Available:
  \url{https://doi.org/10.1007/s11263-018-1116-0}
\BIBentrySTDinterwordspacing

\bibitem{li_grounded_2022}
\BIBentryALTinterwordspacing
L.~H. Li, P.~Zhang, H.~Zhang, J.~Yang, C.~Li, Y.~Zhong, L.~Wang, L.~Yuan,
  L.~Zhang, J.-N. Hwang, K.-W. Chang, and J.~Gao,
  ``\BIBforeignlanguage{en}{Grounded {Language}-{Image} {Pre}-{Training}},''
  2022, pp. 10\,965--10\,975. [Online]. Available:
  \url{https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{thomee_yfcc100m_2016}
\BIBentryALTinterwordspacing
B.~Thomee, D.~A. Shamma, G.~Friedland, B.~Elizalde, K.~Ni, D.~Poland, D.~Borth,
  and L.-J. Li, ``{YFCC100M}: the new data in multimedia research,''
  \emph{Communications of the ACM}, vol.~59, no.~2, pp. 64--73, Jan. 2016.
  [Online]. Available: \url{https://doi.org/10.1145/2812802}
\BIBentrySTDinterwordspacing

\bibitem{changpinyo_conceptual_2021}
\BIBentryALTinterwordspacing
S.~Changpinyo, P.~Sharma, N.~Ding, and R.~Soricut,
  ``\BIBforeignlanguage{en}{Conceptual {12M}: {Pushing} {Web}-{Scale}
  {Image}-{Text} {Pre}-{Training} {To} {Recognize} {Long}-{Tail} {Visual}
  {Concepts}},'' 2021, pp. 3558--3568. [Online]. Available:
  \url{https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{srinivasan_wit_2021}
\BIBentryALTinterwordspacing
K.~Srinivasan, K.~Raman, J.~Chen, M.~Bendersky, and M.~Najork, ``{WIT}:
  {Wikipedia}-based {Image} {Text} {Dataset} for {Multimodal} {Multilingual}
  {Machine} {Learning},'' in \emph{Proceedings of the 44th {International}
  {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information}
  {Retrieval}}, ser. {SIGIR} '21.\hskip 1em plus 0.5em minus 0.4em\relax New
  York, NY, USA: Association for Computing Machinery, Jul. 2021, pp.
  2443--2449. [Online]. Available:
  \url{https://doi.org/10.1145/3404835.3463257}
\BIBentrySTDinterwordspacing

\bibitem{desai_redcaps_2022}
\BIBentryALTinterwordspacing
K.~Desai, G.~Kaul, Z.~T. Aysola, and J.~Johnson,
  ``\BIBforeignlanguage{en}{{RedCaps}: {Web}-curated image-text data created by
  the people, for the people},'' Jan. 2022. [Online]. Available:
  \url{https://openreview.net/forum?id=VjJxBi1p9zh}
\BIBentrySTDinterwordspacing

\bibitem{radford_learning_2021}
\BIBentryALTinterwordspacing
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever,
  ``\BIBforeignlanguage{en}{Learning {Transferable} {Visual} {Models} {From}
  {Natural} {Language} {Supervision}},'' in
  \emph{\BIBforeignlanguage{en}{Proceedings of the 38th {International}
  {Conference} on {Machine} {Learning}}}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, Jul. 2021, pp. 8748--8763, iSSN: 2640-3498. [Online].
  Available: \url{https://proceedings.mlr.press/v139/radford21a.html}
\BIBentrySTDinterwordspacing

\bibitem{yuan_florence_2021}
\BIBentryALTinterwordspacing
L.~Yuan, D.~Chen, Y.-L. Chen, N.~Codella, X.~Dai, J.~Gao, H.~Hu, X.~Huang,
  B.~Li, C.~Li, C.~Liu, M.~Liu, Z.~Liu, Y.~Lu, Y.~Shi, L.~Wang, J.~Wang,
  B.~Xiao, Z.~Xiao, J.~Yang, M.~Zeng, L.~Zhou, and P.~Zhang, ``Florence: {A}
  {New} {Foundation} {Model} for {Computer} {Vision},'' Nov. 2021,
  arXiv:2111.11432 [cs]. [Online]. Available:
  \url{http://arxiv.org/abs/2111.11432}
\BIBentrySTDinterwordspacing

\bibitem{jia_scaling_2021}
\BIBentryALTinterwordspacing
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung,
  Z.~Li, and T.~Duerig, ``\BIBforeignlanguage{en}{Scaling {Up} {Visual} and
  {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text}
  {Supervision}},'' in \emph{\BIBforeignlanguage{en}{Proceedings of the 38th
  {International} {Conference} on {Machine} {Learning}}}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, Jul. 2021, pp. 4904--4916, iSSN: 2640-3498. [Online].
  Available: \url{https://proceedings.mlr.press/v139/jia21b.html}
\BIBentrySTDinterwordspacing

\bibitem{pham2021combined}
H.~Pham, Z.~Dai, G.~Ghiasi, K.~Kawaguchi, H.~Liu, A.~W. Yu, J.~Yu, Y.-T. Chen,
  M.-T. Luong, Y.~Wu \emph{et~al.}, ``Combined scaling for open-vocabulary
  image classification,'' \emph{arXiv e-prints}, pp. arXiv--2111, 2021.

\bibitem{chen2022pali}
X.~Chen, X.~Wang, S.~Changpinyo, A.~Piergiovanni, P.~Padlewski, D.~Salz,
  S.~Goodman, A.~Grycner, B.~Mustafa, L.~Beyer \emph{et~al.}, ``Pali: A
  jointly-scaled multilingual language-image model,'' \emph{arXiv preprint
  arXiv:2209.06794}, 2022.

\bibitem{schuhmann_laion-5b_2022}
\BIBentryALTinterwordspacing
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~W. Gordon, R.~Wightman, M.~Cherti,
  T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, P.~Schramowski, S.~R.
  Kundurthy, K.~Crowson, L.~Schmidt, R.~Kaczmarczyk, and J.~Jitsev,
  ``\BIBforeignlanguage{en}{{LAION}-{5B}: {An} open large-scale dataset for
  training next generation image-text models},'' Oct. 2022. [Online].
  Available: \url{https://openreview.net/forum?id=M3Y74vmsMcY}
\BIBentrySTDinterwordspacing

\bibitem{huang_language_2023}
\BIBentryALTinterwordspacing
S.~Huang, L.~Dong, W.~Wang, Y.~Hao, S.~Singhal, S.~Ma, T.~Lv, L.~Cui, O.~K.
  Mohammed, B.~Patra, Q.~Liu, K.~Aggarwal, Z.~Chi, J.~Bjorck, V.~Chaudhary,
  S.~Som, X.~Song, and F.~Wei, ``Language {Is} {Not} {All} {You} {Need}:
  {Aligning} {Perception} with {Language} {Models},'' Mar. 2023,
  arXiv:2302.14045 [cs]. [Online]. Available:
  \url{http://arxiv.org/abs/2302.14045}
\BIBentrySTDinterwordspacing

\bibitem{singh_flava_2022}
\BIBentryALTinterwordspacing
A.~Singh, R.~Hu, V.~Goswami, G.~Couairon, W.~Galuba, M.~Rohrbach, and D.~Kiela,
  ``\BIBforeignlanguage{en}{{FLAVA}: {A} {Foundational} {Language} and {Vision}
  {Alignment} {Model}},'' 2022, pp. 15\,638--15\,650. [Online]. Available:
  \url{https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html}
\BIBentrySTDinterwordspacing

\bibitem{yao_filip_2022}
\BIBentryALTinterwordspacing
L.~Yao, R.~Huang, L.~Hou, G.~Lu, M.~Niu, H.~Xu, X.~Liang, Z.~Li, X.~Jiang, and
  C.~Xu, ``\BIBforeignlanguage{en}{{FILIP}: {Fine}-grained {Interactive}
  {Language}-{Image} {Pre}-{Training}},'' Jan. 2022. [Online]. Available:
  \url{https://openreview.net/forum?id=cpDhcsEDC2}
\BIBentrySTDinterwordspacing

\bibitem{li_blip_2022}
\BIBentryALTinterwordspacing
J.~Li, D.~Li, C.~Xiong, and S.~Hoi, ``\BIBforeignlanguage{en}{{BLIP}:
  {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified}
  {Vision}-{Language} {Understanding} and {Generation}},'' in
  \emph{\BIBforeignlanguage{en}{Proceedings of the 39th {International}
  {Conference} on {Machine} {Learning}}}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, Jun. 2022, pp. 12\,888--12\,900, iSSN: 2640-3498. [Online].
  Available: \url{https://proceedings.mlr.press/v162/li22n.html}
\BIBentrySTDinterwordspacing

\bibitem{su_towards_2022}
\BIBentryALTinterwordspacing
W.~Su, X.~Zhu, C.~Tao, L.~Lu, B.~Li, G.~Huang, Y.~Qiao, X.~Wang, J.~Zhou, and
  J.~Dai, ``Towards {All}-in-one {Pre}-training via {Maximizing} {Multi}-modal
  {Mutual} {Information},'' Nov. 2022, arXiv:2211.09807 [cs]. [Online].
  Available: \url{http://arxiv.org/abs/2211.09807}
\BIBentrySTDinterwordspacing

\bibitem{ramesh_zero-shot_2021}
\BIBentryALTinterwordspacing
A.~Ramesh, M.~Pavlov, G.~Goh, S.~Gray, C.~Voss, A.~Radford, M.~Chen, and
  I.~Sutskever, ``\BIBforeignlanguage{en}{Zero-{Shot} {Text}-to-{Image}
  {Generation}},'' in \emph{\BIBforeignlanguage{en}{Proceedings of the 38th
  {International} {Conference} on {Machine} {Learning}}}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, Jul. 2021, pp. 8821--8831, iSSN: 2640-3498. [Online].
  Available: \url{https://proceedings.mlr.press/v139/ramesh21a.html}
\BIBentrySTDinterwordspacing

\bibitem{wu2023visual}
C.~Wu, S.~Yin, W.~Qi, X.~Wang, Z.~Tang, and N.~Duan, ``Visual chatgpt: Talking,
  drawing and editing with visual foundation models,'' \emph{arXiv preprint
  arXiv:2303.04671}, 2023.

\bibitem{wang_simvlm_2022}
\BIBentryALTinterwordspacing
Z.~Wang, J.~Yu, A.~W. Yu, Z.~Dai, Y.~Tsvetkov, and Y.~Cao,
  ``\BIBforeignlanguage{en}{{SimVLM}: {Simple} {Visual} {Language} {Model}
  {Pretraining} with {Weak} {Supervision}},'' Jan. 2022. [Online]. Available:
  \url{https://openreview.net/forum?id=GUrhfTuf_3}
\BIBentrySTDinterwordspacing

\bibitem{li_blip-2_2023}
\BIBentryALTinterwordspacing
J.~Li, D.~Li, S.~Savarese, and S.~Hoi, ``{BLIP}-2: {Bootstrapping}
  {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and
  {Large} {Language} {Models},'' Jan. 2023, arXiv:2301.12597 [cs]. [Online].
  Available: \url{http://arxiv.org/abs/2301.12597}
\BIBentrySTDinterwordspacing

\bibitem{driess2023palme}
D.~Driess, F.~Xia, M.~S.~M. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter,
  A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, W.~Huang, Y.~Chebotar, P.~Sermanet,
  D.~Duckworth, S.~Levine, V.~Vanhoucke, K.~Hausman, M.~Toussaint, K.~Greff,
  A.~Zeng, I.~Mordatch, and P.~Florence, ``Palm-e: An embodied multimodal
  language model,'' in \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{yuscaling}
J.~Yu, Y.~Xu, J.~Y. Koh, T.~Luong, G.~Baid, Z.~Wang, V.~Vasudevan, A.~Ku,
  Y.~Yang, B.~K. Ayan \emph{et~al.}, ``Scaling autoregressive models for
  content-rich text-to-image generation,'' \emph{Transactions on Machine
  Learning Research}, 2022.

\bibitem{croitoru2022diffusion}
F.-A. Croitoru, V.~Hondru, R.~T. Ionescu, and M.~Shah, ``Diffusion models in
  vision: A survey,'' \emph{arXiv preprint arXiv:2209.04747}, 2022.

\bibitem{nichol2022glide}
A.~Q. Nichol, P.~Dhariwal, A.~Ramesh, P.~Shyam, P.~Mishkin, B.~Mcgrew,
  I.~Sutskever, and M.~Chen, ``Glide: Towards photorealistic image generation
  and editing with text-guided diffusion models,'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2022, pp. 16\,784--16\,804.

\bibitem{ramesh2022hierarchical}
A.~Ramesh, P.~Dhariwal, A.~Nichol, C.~Chu, and M.~Chen, ``Hierarchical
  text-conditional image generation with clip latents,'' \emph{arXiv preprint
  arXiv:2204.06125}, 2022.

\bibitem{sahariaphotorealistic}
C.~Saharia, W.~Chan, S.~Saxena, L.~Li, J.~Whang, E.~Denton, S.~K.~S.
  Ghasemipour, R.~Gontijo-Lopes, B.~K. Ayan, T.~Salimans \emph{et~al.},
  ``Photorealistic text-to-image diffusion models with deep language
  understanding,'' in \emph{Advances in Neural Information Processing Systems},
  2022.

\bibitem{rombach2021highresolution}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution
  image synthesis with latent diffusion models,'' 2021.

\bibitem{bai2015cryo}
X.-C. Bai, G.~McMullan, and S.~H. Scheres, ``How cryo-em is revolutionizing
  structural biology,'' \emph{Trends in biochemical sciences}, vol.~40, no.~1,
  pp. 49--57, 2015.

\bibitem{wuthrich2001way}
K.~W{\"u}thrich, ``The way to nmr structures of proteins,'' \emph{Nature
  structural biology}, vol.~8, no.~11, pp. 923--925, 2001.

\bibitem{grimes2018crystallography}
J.~M. Grimes, D.~R. Hall, A.~W. Ashton, G.~Evans, R.~L. Owen, A.~Wagner, K.~E.
  McAuley, F.~von Delft, A.~M. Orville, T.~Sorensen \emph{et~al.}, ``Where is
  crystallography going?'' \emph{Acta Crystallographica Section D: Structural
  Biology}, vol.~74, no.~2, pp. 152--166, 2018.

\bibitem{jumper2021highly}
J.~Jumper, R.~Evans, A.~Pritzel, T.~Green, M.~Figurnov, O.~Ronneberger,
  K.~Tunyasuvunakool, R.~Bates, A.~{\v{Z}}{\'\i}dek, A.~Potapenko
  \emph{et~al.}, ``Highly accurate protein structure prediction with
  alphafold,'' \emph{Nature}, vol. 596, no. 7873, pp. 583--589, 2021.

\bibitem{baek2021accurate}
M.~Baek, F.~DiMaio, I.~Anishchenko, J.~Dauparas, S.~Ovchinnikov, G.~R. Lee,
  J.~Wang, Q.~Cong, L.~N. Kinch, R.~D. Schaeffer \emph{et~al.}, ``Accurate
  prediction of protein structures and interactions using a three-track neural
  network,'' \emph{Science}, vol. 373, no. 6557, pp. 871--876, 2021.

\bibitem{evans2021protein}
R.~Evans, M.~O’Neill, A.~Pritzel, N.~Antropova, A.~Senior, T.~Green,
  A.~{\v{Z}}{\'\i}dek, R.~Bates, S.~Blackwell, J.~Yim \emph{et~al.}, ``Protein
  complex prediction with alphafold-multimer,'' \emph{BioRxiv}, pp. 2021--10,
  2021.

\bibitem{cheng2022fastfold}
S.~Cheng, R.~Wu, Z.~Yu, B.~Li, X.~Zhang, J.~Peng, and Y.~You, ``Fastfold:
  reducing alphafold training time from 11 days to 67 hours,'' \emph{arXiv
  preprint arXiv:2203.00854}, 2022.

\bibitem{wang2022helixfold}
G.~Wang, X.~Fang, Z.~Wu, Y.~Liu, Y.~Xue, Y.~Xiang, D.~Yu, F.~Wang, and Y.~Ma,
  ``Helixfold: An efficient implementation of alphafold2 using paddlepaddle,''
  \emph{arXiv preprint arXiv:2207.05477}, 2022.

\bibitem{li2022uni}
Z.~Li, X.~Liu, W.~Chen, F.~Shen, H.~Bi, G.~Ke, and L.~Zhang, ``Uni-fold: an
  open-source platform for developing protein folding models beyond
  alphafold,'' \emph{bioRxiv}, pp. 2022--08, 2022.

\bibitem{ahdritz2022openfold}
G.~Ahdritz, N.~Bouatta, S.~Kadyan, Q.~Xia, W.~Gerecke, T.~J. O’Donnell,
  D.~Berenberg, I.~Fisk, N.~Zanichelli, B.~Zhang \emph{et~al.}, ``Openfold:
  Retraining alphafold2 yields new insights into its learning mechanisms and
  capacity for generalization,'' \emph{bioRxiv}, pp. 2022--11, 2022.

\bibitem{villegas2023manyfold}
A.~Villegas-Morcillo, L.~Robinson, A.~Flajolet, and T.~D. Barrett, ``Manyfold:
  an efficient and flexible library for training and validating protein folding
  models,'' \emph{Bioinformatics}, vol.~39, no.~1, p. btac773, 2023.

\bibitem{mirdita2022colabfold}
M.~Mirdita, K.~Sch{\"u}tze, Y.~Moriwaki, L.~Heo, S.~Ovchinnikov, and
  M.~Steinegger, ``Colabfold: making protein folding accessible to all,''
  \emph{Nature methods}, vol.~19, no.~6, pp. 679--682, 2022.

\bibitem{rives2021biological}
A.~Rives, J.~Meier, T.~Sercu, S.~Goyal, Z.~Lin, J.~Liu, D.~Guo, M.~Ott, C.~L.
  Zitnick, J.~Ma \emph{et~al.}, ``Biological structure and function emerge from
  scaling unsupervised learning to 250 million protein sequences,''
  \emph{Proceedings of the National Academy of Sciences}, vol. 118, no.~15, p.
  e2016239118, 2021.

\bibitem{uniprot2007universal}
U.~Consortium, ``The universal protein resource (uniprot),'' \emph{Nucleic
  acids research}, vol.~36, no. suppl\_1, pp. D190--D195, 2007.

\bibitem{madani2020progen}
A.~Madani, B.~McCann, N.~Naik, N.~S. Keskar, N.~Anand, R.~R. Eguchi, P.-S.
  Huang, and R.~Socher, ``Progen: Language modeling for protein generation,''
  \emph{arXiv preprint arXiv:2004.03497}, 2020.

\bibitem{article}
A.~Elnaggar, M.~Heinzinger, C.~Dallago, G.~Rehawi, W.~Yu, L.~Jones, T.~Gibbs,
  T.~Feher, C.~Angerer, M.~Steinegger, D.~Bhowmik, and B.~Rost, ``Prottrans:
  Towards cracking the language of lifes code through self-supervised deep
  learning and high performance computing,'' \emph{IEEE Transactions on Pattern
  Analysis and Machine Intelligence}, vol.~PP, pp. 1--1, 07 2021.

\bibitem{steinegger2018clustering}
M.~Steinegger and J.~S{\"o}ding, ``Clustering huge protein sequence sets in
  linear time,'' \emph{Nature communications}, vol.~9, no.~1, p. 2542, 2018.

\bibitem{suzek2015uniref}
B.~E. Suzek, Y.~Wang, H.~Huang, P.~B. McGarvey, C.~H. Wu, and U.~Consortium,
  ``Uniref clusters: a comprehensive and scalable alternative for improving
  sequence similarity searches,'' \emph{Bioinformatics}, vol.~31, no.~6, pp.
  926--932, 2015.

\bibitem{lin2022language}
Z.~Lin, H.~Akin, R.~Rao, B.~Hie, Z.~Zhu, W.~Lu, A.~dos Santos~Costa,
  M.~Fazel-Zarandi, T.~Sercu, S.~Candido \emph{et~al.}, ``Language models of
  protein sequences at the scale of evolution enable accurate structure
  prediction,'' \emph{BioRxiv}, 2022.

\bibitem{wu2022high}
R.~Wu, F.~Ding, R.~Wang, R.~Shen, X.~Zhang, S.~Luo, C.~Su, Z.~Wu, Q.~Xie,
  B.~Berger \emph{et~al.}, ``High-resolution de novo structure prediction from
  primary sequence,'' \emph{BioRxiv}, pp. 2022--07, 2022.

\bibitem{chowdhury2022single}
R.~Chowdhury, N.~Bouatta, S.~Biswas, C.~Floristean, A.~Kharkar, K.~Roy,
  C.~Rochereau, G.~Ahdritz, J.~Zhang, G.~M. Church \emph{et~al.},
  ``Single-sequence protein structure prediction using a language model and
  deep learning,'' \emph{Nature Biotechnology}, vol.~40, no.~11, pp.
  1617--1623, 2022.

\bibitem{chen2022improved}
B.~Chen, Z.~Xie, J.~Qiu, Z.~Ye, J.~Xu, and J.~Tang, ``Improved the protein
  complex prediction with protein language models,'' \emph{bioRxiv}, pp.
  2022--09, 2022.

\bibitem{rao2021msa}
R.~M. Rao, J.~Liu, R.~Verkuil, J.~Meier, J.~Canny, P.~Abbeel, T.~Sercu, and
  A.~Rives, ``Msa transformer,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp. 8844--8856.

\bibitem{ruffolo2021deciphering}
J.~A. Ruffolo, J.~J. Gray, and J.~Sulam, ``Deciphering antibody affinity
  maturation with language models and weakly supervised learning,'' \emph{arXiv
  preprint arXiv:2112.07782}, 2021.

\bibitem{wang2022xtrimoabfold}
Y.~Wang, X.~Gong, S.~Li, B.~Yang, Y.~Sun, C.~Shi, H.~Li, Y.~Wang, C.~Yang, and
  L.~Song, ``xtrimoabfold: Improving antibody structure prediction without
  multiple sequence alignments,'' \emph{arXiv preprint arXiv:2212.00735}, 2022.

\bibitem{yang2022scbert}
F.~Yang, W.~Wang, F.~Wang, Y.~Fang, D.~Tang, J.~Huang, H.~Lu, and J.~Yao,
  ``scbert as a large-scale pretrained deep language model for cell type
  annotation of single-cell rna-seq data,'' \emph{Nature Machine Intelligence},
  vol.~4, no.~10, pp. 852--866, 2022.

\bibitem{chen2022interpretable}
J.~Chen, Z.~Hu, S.~Sun, Q.~Tan, Y.~Wang, Q.~Yu, L.~Zong, L.~Hong, J.~Xiao,
  T.~Shen \emph{et~al.}, ``Interpretable rna foundation model from unannotated
  data for highly accurate rna structure and function predictions,''
  \emph{bioRxiv}, pp. 2022--08, 2022.

\bibitem{rnacentral2021rnacentral}
``Rnacentral 2021: secondary structure integration, improved sequence search
  and new member databases,'' \emph{Nucleic acids research}, vol.~49, no.~D1,
  pp. D212--D220, 2021.

\bibitem{shen2022e2efold}
T.~Shen, Z.~Hu, Z.~Peng, J.~Chen, P.~Xiong, L.~Hong, L.~Zheng, Y.~Wang,
  I.~King, S.~Wang \emph{et~al.}, ``E2efold-3d: End-to-end deep learning method
  for accurate de novo rna 3d structure prediction,'' \emph{arXiv preprint
  arXiv:2207.01586}, 2022.

\bibitem{buel2022can}
G.~R. Buel and K.~J. Walters, ``Can alphafold2 predict the impact of missense
  mutations on structure?'' \emph{Nature Structural \& Molecular Biology},
  vol.~29, no.~1, pp. 1--2, 2022.

\bibitem{burki2020newdrugdesign}
T.~Burki, ``A new paradigm for drug development,'' \emph{The Lancet Digital
  Health}, vol.~2, no.~5, pp. e226--e227, 2020.

\bibitem{wang2019smilesBERT}
S.~Wang, Y.~Guo, Y.~Wang, H.~Sun, and J.~Huang, ``Smiles-bert: large scale
  unsupervised pre-training for molecular property prediction,'' in
  \emph{Proceedings of the 10th ACM international conference on bioinformatics,
  computational biology and health informatics}, 2019, pp. 429--436.

\bibitem{honda2019smilesTRANS}
S.~Honda, S.~Shi, and H.~R. Ueda, ``Smiles transformer: Pre-trained molecular
  fingerprint for low data drug discovery,'' \emph{arXiv preprint
  arXiv:1911.04738}, 2019.

\bibitem{fabian2020molecularBERT}
B.~Fabian, T.~Edlich, H.~Gaspar, M.~Segler, J.~Meyers, M.~Fiscato, and
  M.~Ahmed, ``Molecular representation learning with language models and
  domain-relevant auxiliary tasks,'' \emph{arXiv preprint arXiv:2011.13230},
  2020.

\bibitem{chen2021AGBT}
D.~Chen, K.~Gao, D.~D. Nguyen, X.~Chen, Y.~Jiang, G.-W. Wei, and F.~Pan,
  ``Algebraic graph-assisted bidirectional transformers for molecular property
  prediction,'' \emph{Nature communications}, vol.~12, no.~1, p. 3521, 2021.

\bibitem{rong2020GROVER}
Y.~Rong, Y.~Bian, T.~Xu, W.~Xie, Y.~Wei, W.~Huang, and J.~Huang,
  ``Self-supervised graph transformer on large-scale molecular data,''
  \emph{Advances in Neural Information Processing Systems}, vol.~33, pp.
  12\,559--12\,571, 2020.

\bibitem{bagal2021molgpt}
V.~Bagal, R.~Aggarwal, P.~Vinod, and U.~D. Priyakumar, ``Molgpt: molecular
  generation using a transformer-decoder model,'' \emph{Journal of Chemical
  Information and Modeling}, vol.~62, no.~9, pp. 2064--2076, 2021.

\bibitem{yang2021transformermolegeneration}
L.~Yang, G.~Yang, Z.~Bing, Y.~Tian, Y.~Niu, L.~Huang, and L.~Yang,
  ``Transformer-based generative model accelerating the development of novel
  braf inhibitors,'' \emph{ACS omega}, vol.~6, no.~49, pp. 33\,864--33\,873,
  2021.

\bibitem{honda2019smiles}
S.~Honda, S.~Shi, and H.~R. Ueda, ``Smiles transformer: Pre-trained molecular
  fingerprint for low data drug discovery,'' \emph{arXiv preprint
  arXiv:1911.04738}, 2019.

\bibitem{bradshaw2019MoleculeChef}
J.~Bradshaw, B.~Paige, M.~J. Kusner, M.~Segler, and J.~M. Hern{\'a}ndez-Lobato,
  ``A model to search for synthesizable molecules,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~32, 2019.

\bibitem{grechishnikova2021transformerdonovo}
D.~Grechishnikova, ``Transformer neural network for protein-specific de novo
  drug generation as a machine translation problem,'' \emph{Scientific
  reports}, vol.~11, no.~1, pp. 1--13, 2021.

\bibitem{lee2019deepconv}
I.~Lee, J.~Keum, and H.~Nam, ``Deepconv-dti: Prediction of drug-target
  interactions via deep learning with convolution on protein sequences,''
  \emph{PLoS computational biology}, vol.~15, no.~6, p. e1007129, 2019.

\bibitem{nguyen2021graphdta}
T.~Nguyen, H.~Le, T.~P. Quinn, T.~Nguyen, T.~D. Le, and S.~Venkatesh,
  ``Graphdta: predicting drug--target binding affinity with graph neural
  networks,'' \emph{Bioinformatics}, vol.~37, no.~8, pp. 1140--1147, 2021.

\bibitem{huang2021moltrans}
K.~Huang, C.~Xiao, L.~M. Glass, and J.~Sun, ``Moltrans: molecular interaction
  transformer for drug--target interaction prediction,'' \emph{Bioinformatics},
  vol.~37, no.~6, pp. 830--836, 2021.

\bibitem{chen2021extracting}
D.~Chen, J.~Zheng, G.-W. Wei, and F.~Pan, ``Extracting predictive
  representations from hundreds of millions of molecules,'' \emph{The journal
  of physical chemistry letters}, vol.~12, no.~44, pp. 10\,793--10\,801, 2021.

\bibitem{xiong2021admetlab}
G.~Xiong, Z.~Wu, J.~Yi, L.~Fu, Z.~Yang, C.~Hsieh, M.~Yin, X.~Zeng, C.~Wu, A.~Lu
  \emph{et~al.}, ``Admetlab 2.0: an integrated online platform for accurate and
  comprehensive predictions of admet properties,'' \emph{Nucleic Acids
  Research}, vol.~49, no.~W1, pp. W5--W14, 2021.

\bibitem{li2020mpgnet}
P.~Li, J.~Wang, Y.~Qiao, H.~Chen, Y.~Yu, X.~Yao, P.~Gao, G.~Xie, and S.~Song,
  ``Learn molecular representations from large-scale unlabeled molecules for
  drug discovery,'' \emph{arXiv preprint arXiv:2012.11175}, 2020.

\bibitem{li2021mpg}
------, ``An effective self-supervised framework for learning expressive
  molecular global representations to drug discovery,'' \emph{Briefings in
  Bioinformatics}, vol.~22, no.~6, p. bbab109, 2021.

\bibitem{zhang2021mgbert}
X.-C. Zhang, C.-K. Wu, Z.-J. Yang, Z.-X. Wu, J.-C. Yi, C.-Y. Hsieh, T.-J. Hou,
  and D.-S. Cao, ``Mg-bert: leveraging unsupervised atomic representation
  learning for molecular property prediction,'' \emph{Briefings in
  bioinformatics}, vol.~22, no.~6, p. bbab152, 2021.

\bibitem{lin2022pangu}
X.~Lin, C.~Xu, Z.~Xiong, X.~Zhang, N.~Ni, B.~Ni, J.~Chang, R.~Pan, Z.~Wang,
  F.~Yu \emph{et~al.}, ``Pangu drug model: learn a molecule like a human,''
  \emph{bioRxiv}, pp. 2022--03, 2022.

\bibitem{bai2023interpretable}
P.~Bai, F.~Miljkovi{\'c}, B.~John, and H.~Lu, ``Interpretable bilinear
  attention network with domain adaptation improves drug--target prediction,''
  \emph{Nature Machine Intelligence}, pp. 1--11, 2023.

\bibitem{2022drugood}
Y.~{Ji}, L.~{Zhang}, J.~{Wu}, B.~{Wu}, L.-K. {Huang}, T.~{Xu}, Y.~{Rong},
  L.~{Li}, J.~{Ren}, D.~{Xue}, H.~{Lai}, S.~{Xu}, J.~{Feng}, W.~{Liu},
  P.~{Luo}, S.~{Zhou}, J.~{Huang}, P.~{Zhao}, and Y.~{Bian}, ``{DrugOOD:
  Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug
  Discovery -- A Focus on Affinity Prediction Problems with Noise
  Annotations},'' \emph{arXiv e-prints}, p. arXiv:2201.09637, Jan. 2022.

\bibitem{nvidia}
\BIBentryALTinterwordspacing
N.~Corporation, ``Bionemo,'' 2023, accessed on Month Day, Year. [Online].
  Available: \url{https://www.nvidia.com/en-us/gpu-cloud/bionemo/}
\BIBentrySTDinterwordspacing

\bibitem{kung2023performance}
T.~H. Kung, M.~Cheatham, A.~Medenilla, C.~Sillos, L.~De~Leon, C.~Elepa{\~n}o,
  M.~Madriaga, R.~Aggabao, G.~Diaz-Candido, J.~Maningo \emph{et~al.},
  ``Performance of chatgpt on usmle: Potential for ai-assisted medical
  education using large language models,'' \emph{PLOS Digital Health}, vol.~2,
  no.~2, p. e0000198, 2023.

\bibitem{tiu2022expert}
E.~Tiu, E.~Talius, P.~Patel, C.~P. Langlotz, A.~Y. Ng, and P.~Rajpurkar,
  ``Expert-level detection of pathologies from unannotated chest x-ray images
  via self-supervised learning,'' \emph{Nature Biomedical Engineering}, pp.
  1--8, 2022.

\bibitem{chen2022multi}
Z.~Chen, Y.~Du, J.~Hu, Y.~Liu, G.~Li, X.~Wan, and T.-H. Chang, ``Multi-modal
  masked autoencoders for medical vision-and-language pre-training,'' in
  \emph{Medical Image Computing and Computer Assisted Intervention--MICCAI
  2022: 25th International Conference, Singapore, September 18--22, 2022,
  Proceedings, Part V}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022,
  pp. 679--689.

\bibitem{wang2023chatcad}
S.~Wang, Z.~Zhao, X.~Ouyang, Q.~Wang, and D.~Shen, ``Chatcad: Interactive
  computer-aided diagnosis on medical image using large language models,''
  \emph{arXiv preprint arXiv:2302.07257}, 2023.

\bibitem{li2020behrt}
Y.~Li, S.~Rao, J.~R.~A. Solares, A.~Hassaine, R.~Ramakrishnan, D.~Canoy,
  Y.~Zhu, K.~Rahimi, and G.~Salimi-Khorshidi, ``Behrt: transformer for
  electronic health records,'' \emph{Scientific reports}, vol.~10, no.~1, pp.
  1--12, 2020.

\bibitem{rasmy2021medbert}
L.~Rasmy, Y.~Xiang, Z.~Xie, C.~Tao, and D.~Zhi, ``Med-bert: pretrained
  contextualized embeddings on large-scale structured electronic health records
  for disease prediction,'' \emph{NPJ digital medicine}, vol.~4, no.~1, p.~86,
  2021.

\bibitem{wang2023robustness}
J.~Wang, X.~Hu, W.~Hou, H.~Chen, R.~Zheng, Y.~Wang, L.~Yang, H.~Huang, W.~Ye,
  X.~Geng \emph{et~al.}, ``On the robustness of chatgpt: An adversarial and
  out-of-distribution perspective,'' \emph{arXiv preprint arXiv:2302.12095},
  2023.

\bibitem{jeblick2022chatgpt}
K.~Jeblick, B.~Schachtner, J.~Dexl, A.~Mittermeier, A.~T. St{\"u}ber,
  J.~Topalis, T.~Weber, P.~Wesp, B.~Sabel, J.~Ricke \emph{et~al.}, ``Chatgpt
  makes medicine easy to swallow: An exploratory case study on simplified
  radiology reports,'' \emph{arXiv preprint arXiv:2212.14882}, 2022.

\bibitem{antaki2023evaluating}
F.~Antaki, S.~Touma, D.~Milad, J.~El-Khoury, and R.~Duval, ``Evaluating the
  performance of chatgpt in ophthalmology: An analysis of its successes and
  shortcomings,'' \emph{medRxiv}, pp. 2023--01, 2023.

\bibitem{gu2021domain}
Y.~Gu, R.~Tinn, H.~Cheng, M.~Lucas, N.~Usuyama, X.~Liu, T.~Naumann, J.~Gao, and
  H.~Poon, ``Domain-specific language model pretraining for biomedical natural
  language processing,'' \emph{ACM Transactions on Computing for Healthcare
  (HEALTH)}, vol.~3, no.~1, pp. 1--23, 2021.

\bibitem{ramesh2022improving}
V.~Ramesh, N.~A. Chi, and P.~Rajpurkar, ``Improving radiology report generation
  systems by removing hallucinated references to non-existent priors,'' in
  \emph{Machine Learning for Health}.\hskip 1em plus 0.5em minus 0.4em\relax
  PMLR, 2022, pp. 456--473.

\bibitem{chen2019med3d}
S.~Chen, K.~Ma, and Y.~Zheng, ``Med3d: Transfer learning for 3d medical image
  analysis,'' \emph{arXiv preprint arXiv:1904.00625}, 2019.

\bibitem{zhou2019models}
Z.~Zhou, V.~Sodha, M.~M. Rahman~Siddiquee, R.~Feng, N.~Tajbakhsh, M.~B. Gotway,
  and J.~Liang, ``Models genesis: Generic autodidactic models for 3d medical
  image analysis,'' in \emph{Medical Image Computing and Computer Assisted
  Intervention--MICCAI 2019: 22nd International Conference, Shenzhen, China,
  October 13--17, 2019, Proceedings, Part IV 22}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2019, pp. 384--393.

\bibitem{azizi2021big}
S.~Azizi, B.~Mustafa, F.~Ryan, Z.~Beaver, J.~Freyberg, J.~Deaton, A.~Loh,
  A.~Karthikesalingam, S.~Kornblith, T.~Chen \emph{et~al.}, ``Big
  self-supervised models advance medical image classification,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, pp. 3478--3488.

\bibitem{zhou2020C2L}
H.-Y. Zhou, S.~Yu, C.~Bian, Y.~Hu, K.~Ma, and Y.~Zheng, ``Comparing to learn:
  Surpassing imagenet pretraining on radiographs by comparing image
  representations,'' in \emph{Medical Image Computing and Computer Assisted
  Intervention--MICCAI 2020: 23rd International Conference, Lima, Peru, October
  4--8, 2020, Proceedings, Part I 23}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 398--407.

\bibitem{zhang2022convirt}
Y.~Zhang, H.~Jiang, Y.~Miura, C.~D. Manning, and C.~P. Langlotz, ``Contrastive
  learning of medical visual representations from paired images and text,'' in
  \emph{Machine Learning for Healthcare Conference}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2022, pp. 2--25.

\bibitem{huang2021gloria}
S.-C. Huang, L.~Shen, M.~P. Lungren, and S.~Yeung, ``Gloria: A multimodal
  global-local representation learning framework for label-efficient medical
  image recognition,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 3942--3951.

\bibitem{sowrirajan2021moco}
H.~Sowrirajan, J.~Yang, A.~Y. Ng, and P.~Rajpurkar, ``Moco pretraining improves
  representation and transferability of chest x-ray models,'' in \emph{Medical
  Imaging with Deep Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2021, pp. 728--744.

\bibitem{chen2021transunet}
J.~Chen, Y.~Lu, Q.~Yu, X.~Luo, E.~Adeli, Y.~Wang, L.~Lu, A.~L. Yuille, and
  Y.~Zhou, ``Transunet: Transformers make strong encoders for medical image
  segmentation,'' \emph{arXiv preprint arXiv:2102.04306}, 2021.

\bibitem{zhang2021transfuse}
Y.~Zhang, H.~Liu, and Q.~Hu, ``Transfuse: Fusing transformers and cnns for
  medical image segmentation,'' in \emph{Medical Image Computing and Computer
  Assisted Intervention--MICCAI 2021: 24th International Conference,
  Strasbourg, France, September 27--October 1, 2021, Proceedings, Part I
  24}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2021, pp. 14--24.

\bibitem{valanarasu2021medT}
J.~M.~J. Valanarasu, P.~Oza, I.~Hacihaliloglu, and V.~M. Patel, ``Medical
  transformer: Gated axial-attention for medical image segmentation,'' in
  \emph{Medical Image Computing and Computer Assisted Intervention--MICCAI
  2021: 24th International Conference, Strasbourg, France, September
  27--October 1, 2021, Proceedings, Part I 24}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2021, pp. 36--46.

\bibitem{xie2021cotr}
Y.~Xie, J.~Zhang, C.~Shen, and Y.~Xia, ``Cotr: Efficiently bridging cnn and
  transformer for 3d medical image segmentation,'' in \emph{Medical Image
  Computing and Computer Assisted Intervention--MICCAI 2021: 24th International
  Conference, Strasbourg, France, September 27--October 1, 2021, Proceedings,
  Part III 24}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2021, pp.
  171--180.

\bibitem{hatamizadeh2022unetr}
A.~Hatamizadeh, Y.~Tang, V.~Nath, D.~Yang, A.~Myronenko, B.~Landman, H.~R.
  Roth, and D.~Xu, ``Unetr: Transformers for 3d medical image segmentation,''
  in \emph{Proceedings of the IEEE/CVF winter conference on applications of
  computer vision}, 2022, pp. 574--584.

\bibitem{cao2023swinunet}
H.~Cao, Y.~Wang, J.~Chen, D.~Jiang, X.~Zhang, Q.~Tian, and M.~Wang,
  ``Swin-unet: Unet-like pure transformer for medical image segmentation,'' in
  \emph{Computer Vision--ECCV 2022 Workshops: Tel Aviv, Israel, October 23--27,
  2022, Proceedings, Part III}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2023, pp. 205--218.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for
  biomedical image segmentation,'' in \emph{Medical Image Computing and
  Computer-Assisted Intervention--MICCAI 2015: 18th International Conference,
  Munich, Germany, October 5-9, 2015, Proceedings, Part III 18}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2015, pp. 234--241.

\bibitem{tang2022self}
Y.~Tang, D.~Yang, W.~Li, H.~R. Roth, B.~Landman, D.~Xu, V.~Nath, and
  A.~Hatamizadeh, ``Self-supervised pre-training of swin transformers for 3d
  medical image analysis,'' in \emph{Proceedings of the IEEE/CVF Conference on
  Computer Vision and Pattern Recognition}, 2022, pp. 20\,730--20\,740.

\bibitem{shaker2022unetrplus}
A.~Shaker, M.~Maaz, H.~Rasheed, S.~Khan, M.-H. Yang, and F.~S. Khan, ``Unetr++:
  Delving into efficient and accurate 3d medical image segmentation,''
  \emph{arXiv preprint arXiv:2212.04497}, 2022.

\bibitem{chambon2022adapting}
P.~Chambon, C.~Bluethgen, C.~P. Langlotz, and A.~Chaudhari, ``Adapting
  pretrained vision-language foundational models to medical imaging domains,''
  \emph{arXiv preprint arXiv:2210.04133}, 2022.

\bibitem{wang2022medclip}
Z.~Wang, Z.~Wu, D.~Agarwal, and J.~Sun, ``Medclip: Contrastive learning from
  unpaired medical images and text,'' \emph{arXiv preprint arXiv:2210.10163},
  2022.

\bibitem{bemyeyes2023}
\BIBentryALTinterwordspacing
``Be my eyes,'' 2023. [Online]. Available: \url{https://www.bemyeyes.com/}
\BIBentrySTDinterwordspacing

\bibitem{lee2020biobert}
J.~Lee, W.~Yoon, S.~Kim, D.~Kim, S.~Kim, C.~H. So, and J.~Kang, ``Biobert: a
  pre-trained biomedical language representation model for biomedical text
  mining,'' \emph{Bioinformatics}, vol.~36, no.~4, pp. 1234--1240, 2020.

\bibitem{alsentzer2019publicly}
E.~Alsentzer, J.~R. Murphy, W.~Boag, W.-H. Weng, D.~Jin, T.~Naumann, and
  M.~McDermott, ``Publicly available clinical bert embeddings,'' \emph{arXiv
  preprint arXiv:1904.03323}, 2019.

\bibitem{shin2020biomegatron}
H.-C. Shin, Y.~Zhang, E.~Bakhturina, R.~Puri, M.~Patwary, M.~Shoeybi, and
  R.~Mani, ``Biomegatron: Larger biomedical domain language model,''
  \emph{arXiv preprint arXiv:2010.06060}, 2020.

\bibitem{gururangan2020don}
S.~Gururangan, A.~Marasovi{\'c}, S.~Swayamdipta, K.~Lo, I.~Beltagy, D.~Downey,
  and N.~A. Smith, ``Don't stop pretraining: Adapt language models to domains
  and tasks,'' \emph{arXiv preprint arXiv:2004.10964}, 2020.

\bibitem{rasmy2021med}
L.~Rasmy, Y.~Xiang, Z.~Xie, C.~Tao, and D.~Zhi, ``Med-bert: pretrained
  contextualized embeddings on large-scale structured electronic health records
  for disease prediction,'' \emph{NPJ digital medicine}, vol.~4, no.~1, p.~86,
  2021.

\bibitem{raj2021bioelectra}
K.~raj Kanakarajan, B.~Kundumani, and M.~Sankarasubbu, ``Bioelectra: pretrained
  biomedical text encoder using discriminators,'' in \emph{Proceedings of the
  20th Workshop on Biomedical Language Processing}, 2021, pp. 143--154.

\bibitem{yasunaga2022linkbert}
M.~Yasunaga, J.~Leskovec, and P.~Liang, ``Linkbert: Pretraining language models
  with document links,'' \emph{arXiv preprint arXiv:2203.15827}, 2022.

\bibitem{luo2022biogpt}
R.~Luo, L.~Sun, Y.~Xia, T.~Qin, S.~Zhang, H.~Poon, and T.-Y. Liu, ``Biogpt:
  generative pre-trained transformer for biomedical text generation and
  mining,'' \emph{Briefings in Bioinformatics}, vol.~23, no.~6, 2022.

\bibitem{singhal2022large}
K.~Singhal, S.~Azizi, T.~Tu, S.~S. Mahdavi, J.~Wei, H.~W. Chung, N.~Scales,
  A.~Tanwani, H.~Cole-Lewis, S.~Pfohl \emph{et~al.}, ``Large language models
  encode clinical knowledge,'' \emph{arXiv preprint arXiv:2212.13138}, 2022.

\bibitem{yang2022large}
X.~Yang, A.~Chen, N.~PourNejatian, H.~C. Shin, K.~E. Smith, C.~Parisien,
  C.~Compas, C.~Martin, A.~B. Costa, M.~G. Flores \emph{et~al.}, ``A large
  language model for electronic health records,'' \emph{npj Digital Medicine},
  vol.~5, no.~1, p. 194, 2022.

\bibitem{wei2022emergent}
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama,
  M.~Bosma, D.~Zhou, D.~Metzler \emph{et~al.}, ``Emergent abilities of large
  language models,'' \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem{agrawal2022large}
M.~Agrawal, S.~Hegselmann, H.~Lang, Y.~Kim, and D.~Sontag, ``Large language
  models are few-shot clinical information extractors,'' \emph{arXiv preprint
  arXiv:2205.12689}, 2022.

\bibitem{lievin2022can}
V.~Li{\'e}vin, C.~E. Hother, and O.~Winther, ``Can large language models reason
  about medical questions?'' \emph{arXiv preprint arXiv:2207.08143}, 2022.

\bibitem{priorauth2023}
\BIBentryALTinterwordspacing
``Fairway health - process prior authorization faster,'' 2023. [Online].
  Available:
  \url{https://www.ycombinator.com/launches/IIu-fairway-health-process-prior-authorization-faster}
\BIBentrySTDinterwordspacing

\bibitem{patel2023chatgpt}
S.~B. Patel and K.~Lam, ``Chatgpt: the future of discharge summaries?''
  \emph{The Lancet Digital Health}, 2023.

\bibitem{shue2023empowering}
E.~Shue, L.~Liu, B.~Li, Z.~Feng, X.~Li, and G.~Hu, ``Empowering beginners in
  bioinformatics with chatgpt,'' \emph{bioRxiv}, pp. 2023--03, 2023.

\bibitem{dai2023chataug}
H.~Dai, Z.~Liu, W.~Liao, X.~Huang, Z.~Wu, L.~Zhao, W.~Liu, N.~Liu, S.~Li,
  D.~Zhu \emph{et~al.}, ``Chataug: Leveraging chatgpt for text data
  augmentation,'' \emph{arXiv preprint arXiv:2302.13007}, 2023.

\bibitem{mitchell2023detectgpt}
E.~Mitchell, Y.~Lee, A.~Khazatsky, C.~D. Manning, and C.~Finn, ``Detectgpt:
  Zero-shot machine-generated text detection using probability curvature,''
  \emph{arXiv preprint arXiv:2301.11305}, 2023.

\bibitem{korngiebel2021considering}
D.~M. Korngiebel and S.~D. Mooney, ``Considering the possibilities and pitfalls
  of generative pre-trained transformer 3 (gpt-3) in healthcare delivery,''
  \emph{NPJ Digital Medicine}, vol.~4, no.~1, p.~93, 2021.

\bibitem{chen2020tracking}
E.~Chen, K.~Lerman, E.~Ferrara \emph{et~al.}, ``Tracking social media discourse
  about the covid-19 pandemic: Development of a public coronavirus twitter data
  set,'' \emph{JMIR public health and surveillance}, vol.~6, no.~2, p. e19273,
  2020.

\bibitem{peng2022clustering}
J.~Peng, P.~Shi, J.~Qiu, X.~Ju, F.~P.-W. Lo, X.~Gu, W.~Jia, T.~Baranowski,
  M.~Steiner-Asiedu, A.~K. Anderson \emph{et~al.}, ``Clustering egocentric
  images in passive dietary monitoring with self-supervised learning,'' in
  \emph{2022 IEEE-EMBS International Conference on Biomedical and Health
  Informatics (BHI)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  01--04.

\bibitem{qiu2023egocentric}
J.~Qiu, F.~P.-W. Lo, X.~Gu, M.~L. Jobarteh, W.~Jia, T.~Baranowski,
  M.~Steiner-Asiedu, A.~K. Anderson, M.~A. McCrory, E.~Sazonov, M.~Sun,
  G.~Frost, and B.~Lo, ``Egocentric image captioning for privacy-preserved
  passive dietary intake monitoring,'' \emph{IEEE Transactions on Cybernetics},
  2023.

\bibitem{popkin2019dynamics}
B.~M. Popkin, C.~Corvalan, and L.~M. Grummer-Strawn, ``Dynamics of the double
  burden of malnutrition and the changing nutrition reality,'' \emph{The
  Lancet}, 2019.

\bibitem{nguyen2023climax}
T.~Nguyen, J.~Brandstetter, A.~Kapoor, J.~K. Gupta, and A.~Grover, ``Climax: A
  foundation model for weather and climate,'' \emph{arXiv preprint
  arXiv:2301.10343}, 2023.

\bibitem{secoli2022modular}
R.~Secoli, E.~Matheson, M.~Pinzi, S.~Galvan, A.~Donder, T.~Watts, M.~Riva,
  D.~D. Zani, L.~Bello, and F.~Rodriguez~y Baena, ``Modular robotic platform
  for precision neurosurgery with a bio-inspired needle: System overview and
  first in-vivo deployment,'' \emph{Plos one}, vol.~17, no.~10, p. e0275686,
  2022.

\bibitem{d2022emotion}
G.~D’Onofrio, L.~Fiorini, A.~Sorrentino, S.~Russo, F.~Ciccone, F.~Giuliani,
  D.~Sancarlo, and F.~Cavallo, ``Emotion recognizing by a robotic solution
  initiative (emotive project),'' \emph{Sensors}, vol.~22, no.~8, p. 2861,
  2022.

\bibitem{qiu2022egocentric}
J.~Qiu, L.~Chen, X.~Gu, F.~P.-W. Lo, Y.-Y. Tsai, J.~Sun, J.~Liu, and B.~Lo,
  ``Egocentric human trajectory forecasting with a wearable camera and
  multi-modal fusion,'' \emph{IEEE Robotics and Automation Letters}, vol.~7,
  no.~4, pp. 8799--8806, 2022.

\bibitem{sun2022locate}
J.~Sun, B.~Zhou, M.~J. Black, and A.~Chandrasekaran, ``Locate: End-to-end
  localization of actions in 3d with transformers,'' \emph{arXiv preprint
  arXiv:2203.10719}, 2022.

\bibitem{asgharian2022review}
P.~Asgharian, A.~M. Panchea, and F.~Ferland, ``A review on the use of mobile
  service robots in elderly care,'' \emph{Robotics}, vol.~11, no.~6, p. 127,
  2022.

\bibitem{chen2021decision}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch, ``Decision transformer: Reinforcement learning
  via sequence modeling,'' \emph{Advances in neural information processing
  systems}, vol.~34, pp. 15\,084--15\,097, 2021.

\bibitem{nair2022r3m}
S.~Nair, A.~Rajeswaran, V.~Kumar, C.~Finn, and A.~Gupta, ``R3m: A universal
  visual representation for robot manipulation,'' \emph{CoRL}, 2022.

\bibitem{grauman2022ego4d}
K.~Grauman, A.~Westbury, E.~Byrne, Z.~Chavis, A.~Furnari, R.~Girdhar,
  J.~Hamburger, H.~Jiang, M.~Liu, X.~Liu \emph{et~al.}, ``Ego4d: Around the
  world in 3,000 hours of egocentric video,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  18\,995--19\,012.

\bibitem{wang2023mimicplay}
C.~Wang, L.~Fan, J.~Sun, R.~Zhang, L.~Feifei, D.~Xu, Y.~Zhu, and A.~Anandkumar,
  ``Mimicplay: Long-horizon imitation learning by watching human play,''
  \emph{ArXiv}, 2022.

\bibitem{sun2022plate}
J.~Sun, D.-A. Huang, B.~Lu, Y.-H. Liu, B.~Zhou, and A.~Garg, ``Plate:
  Visually-grounded planning with transformers in procedural tasks,''
  \emph{IEEE Robotics and Automation Letters}, vol.~7, no.~2, pp. 4924--4930,
  2022.

\bibitem{vemprala2023chatgpt}
\BIBentryALTinterwordspacing
S.~Vemprala, R.~Bonatti, A.~Bucker, and A.~Kapoor, ``Chatgpt for robotics:
  Design principles and model abilities,'' Microsoft, Tech. Rep. MSR-TR-2023-8,
  February 2023. [Online]. Available:
  \url{https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/}
\BIBentrySTDinterwordspacing

\bibitem{reed2022a}
\BIBentryALTinterwordspacing
S.~Reed, K.~Zolna, E.~Parisotto, S.~G. Colmenarejo, A.~Novikov, G.~Barth-maron,
  M.~Gim{\'e}nez, Y.~Sulsky, J.~Kay, J.~T. Springenberg, T.~Eccles, J.~Bruce,
  A.~Razavi, A.~Edwards, N.~Heess, Y.~Chen, R.~Hadsell, O.~Vinyals, M.~Bordbar,
  and N.~de~Freitas, ``A generalist agent,'' \emph{Transactions on Machine
  Learning Research}, 2022, featured Certification. [Online]. Available:
  \url{https://openreview.net/forum?id=1ikK0kHjvj}
\BIBentrySTDinterwordspacing

\bibitem{shridhar2022cliport}
M.~Shridhar, L.~Manuelli, and D.~Fox, ``Cliport: What and where pathways for
  robotic manipulation,'' in \emph{Conference on Robot Learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 894--906.

\bibitem{shridhar2022perceiver}
------, ``Perceiver-actor: A multi-task transformer for robotic manipulation,''
  \emph{arXiv preprint arXiv:2209.05451}, 2022.

\bibitem{saycan2022arxiv}
M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn, C.~Fu,
  K.~Gopalakrishnan, K.~Hausman, A.~Herzog, D.~Ho, J.~Hsu, J.~Ibarz, B.~Ichter,
  A.~Irpan, E.~Jang, R.~J. Ruano, K.~Jeffrey, S.~Jesmonth, N.~Joshi, R.~Julian,
  D.~Kalashnikov, Y.~Kuang, K.-H. Lee, S.~Levine, Y.~Lu, L.~Luu, C.~Parada,
  P.~Pastor, J.~Quiambao, K.~Rao, J.~Rettinghouse, D.~Reyes, P.~Sermanet,
  N.~Sievers, C.~Tan, A.~Toshev, V.~Vanhoucke, F.~Xia, T.~Xiao, P.~Xu, S.~Xu,
  M.~Yan, and A.~Zeng, ``Do as i can and not as i say: Grounding language in
  robotic affordances,'' in \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{jiang2022vima}
Y.~Jiang, A.~Gupta, Z.~Zhang, G.~Wang, Y.~Dou, Y.~Chen, L.~Fei-Fei,
  A.~Anandkumar, Y.~Zhu, and L.~Fan, ``Vima: General robot manipulation with
  multimodal prompts,'' \emph{arXiv preprint arXiv:2210.03094}, 2022.

\bibitem{brohan2022rt}
A.~Brohan, N.~Brown, J.~Carbajal, Y.~Chebotar, J.~Dabis, C.~Finn,
  K.~Gopalakrishnan, K.~Hausman, A.~Herzog, J.~Hsu \emph{et~al.}, ``Rt-1:
  Robotics transformer for real-world control at scale,'' \emph{arXiv preprint
  arXiv:2212.06817}, 2022.

\bibitem{yang2017medical}
G.-Z. Yang, J.~Cambias, K.~Cleary, E.~Daimler, J.~Drake, P.~E. Dupont, N.~Hata,
  P.~Kazanzides, S.~Martel, R.~V. Patel \emph{et~al.}, ``Medical
  robotics—regulatory, ethical, and legal considerations for increasing
  levels of autonomy,'' \emph{Science Robotics}, vol.~2, no.~4, 2017.

\bibitem{yip2019robot}
M.~Yip and N.~Das, ``Robot autonomy for surgery,'' in \emph{The Encyclopedia of
  MEDICAL ROBOTICS: Volume 1 Minimally Invasive Surgical Robotics}.\hskip 1em
  plus 0.5em minus 0.4em\relax World Scientific, 2019, pp. 281--313.

\bibitem{9341382}
M.~Ginesi, D.~Meli, A.~Roberti, N.~Sansonetto, and P.~Fiorini, ``Autonomous
  task planning and situation awareness in robotic surgery,'' in \emph{2020
  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  2020, pp. 3144--3150.

\bibitem{8794159}
C.~Shin, P.~W. Ferguson, S.~A. Pedram, J.~Ma, E.~P. Dutson, and J.~Rosen,
  ``Autonomous tissue manipulation via surgical robot using learning based
  model predictive control,'' in \emph{2019 International Conference on
  Robotics and Automation (ICRA)}, 2019, pp. 3875--3881.

\bibitem{saeidi2022autonomous}
H.~Saeidi, J.~D. Opfermann, M.~Kam, S.~Wei, S.~L{\'e}onard, M.~H. Hsieh, J.~U.
  Kang, and A.~Krieger, ``Autonomous robotic laparoscopic surgery for
  intestinal anastomosis,'' \emph{Science robotics}, vol.~7, no.~62, p.
  eabj2908, 2022.

\bibitem{dupont2021decade}
P.~E. Dupont, B.~J. Nelson, M.~Goldfarb, B.~Hannaford, A.~Menciassi, M.~K.
  O’Malley, N.~Simaan, P.~Valdastri, and G.-Z. Yang, ``A decade retrospective
  of medical robotics research from 2010 to 2020,'' \emph{Science Robotics},
  vol.~6, no.~60, p. eabi8017, 2021.

\bibitem{10065461}
R.~Zhang, J.~Chen, Z.~Wang, Z.~Yang, Y.~Ren, P.~Shi, J.~Calo, K.~Lam,
  S.~Purkayastha, and B.~Lo, ``A step towards conditional autonomy - robotic
  appendectomy,'' \emph{IEEE Robotics and Automation Letters}, vol.~8, no.~5,
  pp. 2429--2436, 2023.

\bibitem{pubmedabstract2023}
\BIBentryALTinterwordspacing
``Pubmed abstract,'' 2023. [Online]. Available:
  \url{https://pubmed.ncbi.nlm.nih.gov/download/}
\BIBentrySTDinterwordspacing

\bibitem{pubmedcentral2023}
\BIBentryALTinterwordspacing
``Pubmed central,'' 2023. [Online]. Available:
  \url{https://www.ncbi.nlm.nih.gov/pmc/}
\BIBentrySTDinterwordspacing

\bibitem{johnson2016mimic}
A.~E. Johnson, T.~J. Pollard, L.~Shen, L.-w.~H. Lehman, M.~Feng, M.~Ghassemi,
  B.~Moody, P.~Szolovits, L.~Anthony~Celi, and R.~G. Mark, ``Mimic-iii, a
  freely accessible critical care database,'' \emph{Scientific data}, vol.~3,
  no.~1, pp. 1--9, 2016.

\bibitem{herrett2015data}
E.~Herrett, A.~M. Gallagher, K.~Bhaskaran, H.~Forbes, R.~Mathur, T.~Van~Staa,
  and L.~Smeeth, ``Data resource profile: clinical practice research datalink
  (cprd),'' \emph{International journal of epidemiology}, vol.~44, no.~3, pp.
  827--836, 2015.

\bibitem{johnson2019mimic}
A.~E. Johnson, T.~J. Pollard, S.~J. Berkowitz, N.~R. Greenbaum, M.~P. Lungren,
  C.-y. Deng, R.~G. Mark, and S.~Horng, ``Mimic-cxr, a de-identified publicly
  available database of chest radiographs with free-text reports,''
  \emph{Scientific data}, vol.~6, no.~1, p. 317, 2019.

\bibitem{irvin2019chexpert}
J.~Irvin, P.~Rajpurkar, M.~Ko, Y.~Yu, S.~Ciurea-Ilcus, C.~Chute, H.~Marklund,
  B.~Haghgoo, R.~Ball, K.~Shpanskaya \emph{et~al.}, ``Chexpert: A large chest
  radiograph dataset with uncertainty labels and expert comparison,'' in
  \emph{Proceedings of the AAAI conference on artificial intelligence},
  vol.~33, no.~01, 2019, pp. 590--597.

\bibitem{bustos2020padchest}
A.~Bustos, A.~Pertusa, J.-M. Salinas, and M.~de~la Iglesia-Vay{\'a},
  ``Padchest: A large chest x-ray image dataset with multi-label annotated
  reports,'' \emph{Medical image analysis}, vol.~66, p. 101797, 2020.

\bibitem{rajpurkar2017mura}
P.~Rajpurkar, J.~Irvin, A.~Bagul, D.~Ding, T.~Duan, H.~Mehta, B.~Yang, K.~Zhu,
  D.~Laird, R.~L. Ball \emph{et~al.}, ``Mura: Large dataset for abnormality
  detection in musculoskeletal radiographs,'' \emph{arXiv preprint
  arXiv:1712.06957}, 2017.

\bibitem{NIH}
\BIBentryALTinterwordspacing
N.~I. of~Health Chest X-Ray~Dataset, ``Nih chest x-rays,'' 2018, accessed on
  Month Day, Year. [Online]. Available:
  \url{https://www.kaggle.com/datasets/nih-chest-xrays/data}
\BIBentrySTDinterwordspacing

\bibitem{reis2022brax}
E.~P. Reis, J.~P. de~Paiva, M.~C. da~Silva, G.~A. Ribeiro, V.~F. Paiva,
  L.~Bulgarelli, H.~M. Lee, P.~V. Santos, V.~M. Brito, L.~T. Amaral
  \emph{et~al.}, ``Brax, brazilian labeled chest x-ray dataset,''
  \emph{Scientific Data}, vol.~9, no.~1, p. 487, 2022.

\bibitem{wang2020covid}
L.~Wang, Z.~Q. Lin, and A.~Wong, ``Covid-net: A tailored deep convolutional
  neural network design for detection of covid-19 cases from chest x-ray
  images,'' \emph{Scientific reports}, vol.~10, no.~1, pp. 1--12, 2020.

\bibitem{yan2018deeplesion}
K.~Yan, X.~Wang, L.~Lu, and R.~M. Summers, ``Deeplesion: automated mining of
  large-scale lesion annotations and universal lesion detection with deep
  learning,'' \emph{Journal of medical imaging}, vol.~5, no.~3, pp.
  036\,501--036\,501, 2018.

\bibitem{MedPix}
\BIBentryALTinterwordspacing
N.~L. of~Midecine, ``The national library of medicine presents medpix,''
  unknown, accessed on Month Day, Year. [Online]. Available:
  \url{https://medpix.nlm.nih.gov/home}
\BIBentrySTDinterwordspacing

\bibitem{clark2013cancer}
K.~Clark, B.~Vendt, K.~Smith, J.~Freymann, J.~Kirby, P.~Koppel, S.~Moore,
  S.~Phillips, D.~Maffitt, M.~Pringle \emph{et~al.}, ``The cancer imaging
  archive (tcia): maintaining and operating a public information repository,''
  \emph{Journal of digital imaging}, vol.~26, pp. 1045--1057, 2013.

\bibitem{li2020ipn}
M.~Li, Y.~Zhang, Z.~Ji, K.~Xie, S.~Yuan, Q.~Liu, and Q.~Chen, ``Ipn-v2 and
  octa-500: Methodology and dataset for retinal image segmentation,''
  \emph{arXiv preprint arXiv:2012.07261}, 2020.

\bibitem{antonelli2022medical}
M.~Antonelli, A.~Reinke, S.~Bakas, K.~Farahani, A.~Kopp-Schneider, B.~A.
  Landman, G.~Litjens, B.~Menze, O.~Ronneberger, R.~M. Summers \emph{et~al.},
  ``The medical segmentation decathlon,'' \emph{Nature communications},
  vol.~13, no.~1, p. 4128, 2022.

\bibitem{yang2023medmnist}
J.~Yang, R.~Shi, D.~Wei, Z.~Liu, L.~Zhao, B.~Ke, H.~Pfister, and B.~Ni,
  ``Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical
  image classification,'' \emph{Scientific Data}, vol.~10, no.~1, p.~41, 2023.

\bibitem{tschandl2018ham10000}
P.~Tschandl, C.~Rosendahl, and H.~Kittler, ``The ham10000 dataset, a large
  collection of multi-source dermatoscopic images of common pigmented skin
  lesions,'' \emph{Scientific data}, vol.~5, no.~1, pp. 1--9, 2018.

\bibitem{uniprot2018uniprot}
U.~Consortium \emph{et~al.}, ``Uniprot: the universal protein knowledgebase,''
  \emph{Nucleic acids research}, vol.~46, no.~5, p. 2699, 2018.

\bibitem{mistry2021pfam}
J.~Mistry, S.~Chuguransky, L.~Williams, M.~Qureshi, G.~A. Salazar, E.~L.
  Sonnhammer, S.~C. Tosatto, L.~Paladin, S.~Raj, L.~J. Richardson
  \emph{et~al.}, ``Pfam: The protein families database in 2021,'' \emph{Nucleic
  acids research}, vol.~49, no.~D1, pp. D412--D419, 2021.

\bibitem{kovaltsuk2018observed}
A.~Kovaltsuk, J.~Leem, S.~Kelm, J.~Snowden, C.~M. Deane, and K.~Krawczyk,
  ``Observed antibody space: a resource for data mining next-generation
  sequencing of antibody repertoires,'' \emph{The Journal of Immunology}, vol.
  201, no.~8, pp. 2502--2509, 2018.

\bibitem{franzen2019panglaodb}
O.~Franz{\'e}n, L.-M. Gan, and J.~L. Bj{\"o}rkegren, ``Panglaodb: a web server
  for exploration of mouse and human single-cell rna sequencing data,''
  \emph{Database}, vol. 2019, 2019.

\bibitem{gaulton2012chembl}
A.~Gaulton, L.~J. Bellis, A.~P. Bento, J.~Chambers, M.~Davies, A.~Hersey,
  Y.~Light, S.~McGlinchey, D.~Michalovich, B.~Al-Lazikani \emph{et~al.},
  ``Chembl: a large-scale bioactivity database for drug discovery,''
  \emph{Nucleic acids research}, vol.~40, no.~D1, pp. D1100--D1107, 2012.

\bibitem{irwin2020zinc20}
J.~J. Irwin, K.~G. Tang, J.~Young, C.~Dandarchuluun, B.~R. Wong,
  M.~Khurelbaatar, Y.~S. Moroz, J.~Mayfield, and R.~A. Sayle, ``Zinc20—a free
  ultralarge-scale chemical database for ligand discovery,'' \emph{Journal of
  chemical information and modeling}, vol.~60, no.~12, pp. 6065--6073, 2020.

\bibitem{yang2021drugspacex}
T.~Yang, Z.~Li, Y.~Chen, D.~Feng, G.~Wang, Z.~Fu, X.~Ding, X.~Tan, J.~Zhao,
  X.~Luo \emph{et~al.}, ``Drugspacex: a large screenable and synthetically
  tractable database extending drug space,'' \emph{Nucleic acids research},
  vol.~49, no.~D1, pp. D1170--D1178, 2021.

\bibitem{chambers2013unichem}
J.~Chambers, M.~Davies, A.~Gaulton, A.~Hersey, S.~Velankar, R.~Petryszak,
  J.~Hastings, L.~Bellis, S.~McGlinchey, and J.~P. Overington, ``Unichem: a
  unified chemical structure cross-referencing and identifier tracking
  system,'' \emph{Journal of cheminformatics}, vol.~5, no.~1, p.~3, 2013.

\bibitem{kim2019pubchem}
S.~Kim, J.~Chen, T.~Cheng, A.~Gindulyte, J.~He, S.~He, Q.~Li, B.~A. Shoemaker,
  P.~A. Thiessen, B.~Yu \emph{et~al.}, ``Pubchem 2019 update: improved access
  to chemical data,'' \emph{Nucleic acids research}, vol.~47, no.~D1, pp.
  D1102--D1109, 2019.

\bibitem{yazdani2022binddb}
M.~Yazdani-Jahromi, N.~Yousefi, A.~Tayebi, E.~Kolanthai, C.~J. Neal, S.~Seal,
  and O.~O. Garibay, ``Attentionsitedti: an interpretable graph-based model for
  drug-target interaction prediction using nlp sentence-level relation
  classification,'' \emph{Briefings in Bioinformatics}, vol.~23, no.~4, p.
  bbac272, 2022.

\bibitem{lyu2021cmnpd}
C.~Lyu, T.~Chen, B.~Qiang, N.~Liu, H.~Wang, L.~Zhang, and Z.~Liu, ``Cmnpd: a
  comprehensive marine natural products database towards facilitating drug
  discovery from the ocean,'' \emph{Nucleic Acids Research}, vol.~49, no.~D1,
  pp. D509--D515, 2021.

\bibitem{zhou2023unimol}
\BIBentryALTinterwordspacing
G.~Zhou, Z.~Gao, Q.~Ding, H.~Zheng, H.~Xu, Z.~Wei, L.~Zhang, and G.~Ke,
  ``Uni-mol: A universal 3d molecular representation learning framework,'' in
  \emph{The Eleventh International Conference on Learning Representations},
  2023. [Online]. Available: \url{https://openreview.net/forum?id=6K2RM6wVqKu}
\BIBentrySTDinterwordspacing

\bibitem{zhuo2023exploring}
T.~Y. Zhuo, Y.~Huang, C.~Chen, and Z.~Xing, ``Exploring ai ethics of chatgpt: A
  diagnostic analysis,'' \emph{arXiv preprint arXiv:2301.12867}, 2023.

\bibitem{li2023data}
\BIBentryALTinterwordspacing
L.~Li and M.~W. Spratling, ``Data augmentation alone can improve adversarial
  training,'' in \emph{The Eleventh International Conference on Learning
  Representations}, 2023. [Online]. Available:
  \url{https://openreview.net/forum?id=y4uc4NtTWaq}
\BIBentrySTDinterwordspacing

\bibitem{dou2021federated}
Q.~Dou, T.~Y. So, M.~Jiang, Q.~Liu, V.~Vardhanabhuti, G.~Kaissis, Z.~Li, W.~Si,
  H.~H. Lee, K.~Yu \emph{et~al.}, ``Federated deep learning for detecting
  covid-19 lung abnormalities in ct: a privacy-preserving multinational
  validation study,'' \emph{NPJ digital medicine}, vol.~4, no.~1, p.~60, 2021.

\bibitem{qin2021efficient}
D.~Qin, J.-J. Bu, Z.~Liu, X.~Shen, S.~Zhou, J.-J. Gu, Z.-H. Wang, L.~Wu, and
  H.-F. Dai, ``Efficient medical image segmentation based on knowledge
  distillation,'' \emph{IEEE Transactions on Medical Imaging}, vol.~40, no.~12,
  pp. 3820--3831, 2021.

\bibitem{hou2023geneturing}
W.~Hou and Z.~Ji, ``Geneturing tests gpt models in genomics,'' \emph{bioRxiv},
  pp. 2023--03, 2023.

\bibitem{yu2021reinforcement}
C.~Yu, J.~Liu, S.~Nemati, and G.~Yin, ``Reinforcement learning in healthcare: A
  survey,'' \emph{ACM Computing Surveys (CSUR)}, vol.~55, no.~1, pp. 1--36,
  2021.

\bibitem{carusi2023medical}
A.~Carusi, P.~D. Winter, I.~Armstrong, F.~Ciravegna, D.~G. Kiely, A.~Lawrie,
  H.~Lu, I.~Sabroe, and A.~Swift, ``Medical artificial intelligence is as much
  social as it is technological,'' \emph{Nature Machine Intelligence}, pp.
  1--3, 2023.

\end{thebibliography}



\end{document}
