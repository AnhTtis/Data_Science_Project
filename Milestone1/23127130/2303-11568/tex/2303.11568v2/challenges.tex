\section{Challenges, Limitations, and Risks}\label{sec:challenges}




Despite the promising outcome of LAMs, there remain many challenges and potential risks in developing and deploying LAMs in biomedical, clinical, and healthcare applications. 



\subsubsection{Data}
Most existing public datasets for health informatics are much smaller (please refer to Fig.~\ref{fig:LAM_key_feature_summary} and Table~\ref{tab:LAM_dataset}~\footnote{References to the datasets in Table~\ref{tab:LAM_dataset} and methods in Table~\ref{tab:LAM_sota} can be found in https://github.com/Jianing-Qiu/Awesome-Healthcare-Foundation-Models.}) than those used in general domains and thus are likely insufficient to unlock the full potential of LAMs in biomedical and health scenarios. Building large-scale high-quality medical datasets are particularly challenging because (1) curation requires domain-expertise to identify data of clinical relevance, and quality assurance is very important with health data; (2) some data modalities like MRI require special devices to collect, which is inefficient and expensive; (3) the collected data may not be allowed to publish or use for training because of consent, legal and privacy issues. Furthermore, the training strategy RLHF of some LLMs like ChatGPT requires even more intense engagement of human experts.



\subsubsection{Computation}
Training, or even fine-tuning, contemporary LAMs is extremely expensive in terms of time and resource consumption, which is beyond the budget of most researchers and organizations \cite{ding2023parameter}. Taking LLaMa as an example, an LLM with 65B parameters, it took about 21 days on 2048 A100 GPUs to train the model once on a dataset of 1.4T tokens \cite{touvron2023llama}. Furthermore, even inference can be prohibitively costly due to the model size, making it impractical for most hospitals to deploy these LAMs locally using their computing devices at hand. 



\subsubsection{Reliability}
The reliability threshold for translation into clinical practice is significantly higher~\cite{gilbert2023large}. Despite the impressive performance, LLMs are still far from reliable \cite{shen2023chatgpt} and prone to hallucinate \cite{gpt4openai2023, lee2023benefits}, i.e., generating factually-incorrect yet plausible content which misleads users. In addition, the unsatisfactory robustness of LAMs impairs their credibility. LLMs are known to be sensitive to prompts \cite{nori2023capabilities}. LLMs as well as LMs for other modalities remain vulnerable to out-of-distribution and adversarial examples \cite{shen2023chatgpt, wang2023robustness}. Improving the robustness of LAMs may require even more data \cite{li2023data}. Therefore, caution is highly required when using LAMs in healthcare practice to alleviate the potential danger of over-reliance. In addition, LAMs, especially LLMs were trained offline, in many clinical and health scenarios, using up-to-date information is critical.





\subsubsection{Privacy}
First, LAMs have been reported to have excessive capacity to memorize their training data \cite{carlini2021extracting}, and more importantly, it is viable to extract sensitive information in the memorized data using direct prompts \cite{carlini2021extracting, carlini2022quantifying}. This was later mitigated by fine-tuning LAMs to refuse to answer such prompts \cite{li2023multi}. However, Li et al. \cite{li2023multi} also show that this mitigation can be bypassed through tricky prompts called jailbreaking. Moreover, membership inference attacks \cite{shokri2017membership} could reveal if a sample is in the training set, e.g., if a patient is in a cancer dataset. It has been recently demonstrated to work even on the latest large diffusion models \cite{duan2023diffusion}. 

Second, the information provided by users to query LLM-integrated applications may be leaked. According to the data policy of OpenAI \cite{openaidatapolicy2023}, they store the data that users provide to ChatGPT or DALL-E to train their models. Unfortunately, it has been reported that the stored personal information can be leaked incidentally by a ``chat history" bug \cite{openaichatbug2023} or deliberately by indirect prompt injection attack \cite{greshake2023more}.



\subsubsection{Fairness}
LAMs are data-driven approaches so they could learn any bias from the training data. Unfortunately, bias widely exists in the delivery of healthcare \cite{hall2015implicit} and also the data collected in this process \cite{obermeyer2019dissecting, cirillo2020sex, char2018implementing}. Machine learning models trained on such data are reported to mimic human bias against race \cite{obermeyer2019dissecting}, gender \cite{cirillo2020sex}, politics \cite{rutinowski2023self}, etc. In addition to these conventional biases, LLMs present language bias as well, i.e., they perform better in particular languages like English but worse in others \cite{zhuo2023exploring} because training data is dominated by a few languages.




\subsubsection{Toxicity}
Current LAMs, even LLMs explicitly trained with alignment, do not understand and represent human ethics \cite{hendrycks2020aligning}. LLMs are reported to produce hate speech \cite{gehman2020realtoxicityprompts} that causes offensive and psychologically harmful content and even incites violence. Secondly, LAMs may endorse unethical or harmful views and behaviors \cite{hendrycks2020aligning} and motivate users to perform. Lastly, LAMs can be used intentionally to facilitate harmful activities like spreading disinformation and encouraging criminal activities. Although some countermeasures like filtering are applied, they can be circumvented by prompt injection \cite{zhuo2023exploring}.

\subsubsection{Transparency}


Recently, some impactful LAMs like ChatGPT and Med-PaLM 2 chose not to disclose the complete technical details, the pre-trained models, and the used data. This makes it impossible for others to independently reproduce, improve upon and audit their methods. This transparency threat for LAMs can be more serious in healthcare as many medical data is private and models built upon them are not allowed to be open sourced.




\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{figures/Future_direction.eps}}
\caption{Future directions of LAMs in health informatics}
\label{fig:Future_direction}
\vspace{-0.5cm}
\end{figure}



\subsubsection{Interpretability}
LAMs inherently lack interpretability due to their extremely dense hidden layers. Even worse, the behavior of LAMs can be meaningless \cite{daras2022discovering, wang2023investigating}, hard to predict \cite{kojima2022large, elhage2021mathematical} and thus mysterious. For example, DALL-E 2 generates the images of physical objects with absurd prompts (e.g., ``Apoploe vesrreaitais" for birds) \cite{daras2022discovering}; the reasoning ability of LLMs can be improved by simply adding a text of ``Letâ€™s think step by step" to prompts \cite{kojima2022large}. There has been little progress towards explaining LAMs. Chain-of-thought prompts provide one way to reveal the intermediate reasoning steps behind an output, but it remains unclear whether the generated description of reasoning reflects the model's true internal reasoning. Alternatively, mechanistic interpretability methods \cite{elhage2021mathematical} reverse-engineer the computation of LAMs to illuminate the model's internal mechanism of reasoning.



\subsubsection{Sustainability}
Despite many benefits, LAMs, if abused, will negatively impact the sustainability of our society. LAMs consume lots of computation resource \cite{touvron2023llama} and energy \cite{patterson2021carbon} and emit tons of carbon \cite{patterson2021carbon} in all activities in their lifecycle (from training to deployment) because of their scale. For example, as estimated by \cite{patterson2022carbon}, training a GPT-3 model consumes 1287 MWh and emits 552 tons of \ensuremath{\mathrm{CO_2}}. As the paradigm moves towards LAMs in healthcare, more and more research is expected to be conducted based on LAMs, which could be environmentally unfriendly due to the cost and carbon emission if right practices \cite{patterson2021carbon} are not established.



\subsubsection{Regulation}
Regulation is needed to ensure responsible LAMs especially when some of the above issues cannot be technically addressed. Particularly, data collection and usage should be governed to protect the rights of data owner such as copyright, privacy and ``being forgotten" \cite{villaronga2018humans}. The liability of LAMs' creators/owners for the possible harm caused by the model's output should be clarified. LAMs should be deployed in critical healthcare services only if regulatory approval is obtained and standardized safety assessment is passed. Regulation today is much behind the development of technology for LAMs, even for more general AI. Our webpage lists some major legislation for reference if readers want to know more about AI regulation. 














