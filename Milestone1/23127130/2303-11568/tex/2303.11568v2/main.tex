\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tablefootnote}
\usepackage{soul}
\usepackage{multirow}
\usepackage{caption}

% \usepackage{titlesec}

% \titlespacing\section{0pt}{1pt plus 1pt minus 1pt}{1pt plus 0pt minus 1pt}
% \titlespacing\subsection{0pt}{1pt plus 1pt minus 1pt}{1pt plus 0pt minus 1pt}
% \titlespacing\subsubsection{0pt}{1pt plus 1pt minus 1pt}{1pt plus 0pt minus 1pt}

% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
% \setlength{\belowcaptionskip}{-7.5pt}



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{}
{Qiu \MakeLowercase{\textit{et al.}}: Large AI Models in Health Informatics: Applications, Challenges, and the Future}
\begin{document}
\title{Large AI Models in Health Informatics: Applications, Challenges, and the Future}
\author{Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, \\Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P.-W. Lo, Bo Xiao, \\Wu Yuan,  \IEEEmembership{Senior Member, IEEE}, Ningli Wang, Dong Xu, and Benny Lo, \IEEEmembership{Senior Member, IEEE}
% \thanks{This paragraph of the first footnote will contain the date on 
% which you submitted your paper for review. It will also contain support 
% information, including sponsor and financial support acknowledgment. For 
% example, ``This work was supported in part by the U.S. Department of 
% Commerce under Grant BS123456.'' }
% \thanks{The next few paragraphs should contain 
% the authors' current affiliations, including current address and e-mail. For 
% example, F. A. Author is with the National Institute of Standards and 
% Technology, Boulder, CO 80305 USA (e-mail: author@boulder.nist.gov). }
\thanks{Jianing Qiu is with the Department of Computing, Imperial College London, U.K., and he is also with the Department of Biomedical Engineering, The Chinese University of Hong Kong, Hong Kong SAR. He was with Precision Robotics (Hong Kong) Ltd., Hong Kong SAR (e-mail: jianing.qiu17@imperial.ac.uk).}
\thanks{Lin Li is with the Department of Informatics, King's College London, U.K. (e-mail: lin.3.li@kcl.ac.uk).}
\thanks{Jiankai Sun is with the School of Engineering, Stanford University, USA (e-mail: jksun@stanford.edu).}
\thanks{Jiachuan Peng is with the Department of Engineering Science, University of Oxford, U.K. (e-mail: jiachuan.peng@seh.ox.ac.uk).}
\thanks{Peilun Shi and Wu Yuan are with the Department of Biomedical Engineering,  The Chinese University of Hong Kong, Hong Kong SAR (e-mails: peilunshi@cuhk.edu.hk and wyuan@cuhk.edu.hk).}
\thanks{Ruiyang Zhang is with Precision Robotics (Hong Kong) Ltd., Hong Kong SAR (e-mail: r.zhang@prhk.ltd).}
\thanks{Yinzhao Dong is with the Faculty of Engineering, The University of Hong Kong, Hong Kong SAR (e-mail: dongyz@connect.hku.hk).}
\thanks{Kyle Lam is with the Department of Surgery and Cancer, Imperial
College London, U.K. (e-mail: k.lam@imperial.ac.uk).}
\thanks{Frank P.-W. Lo  and Bo Xiao are with the Hamlyn Centre for Robotic Surgery, Imperial College London, U.K. (e-mails: po.lo15@imperial.ac.uk and b.xiao@imperial.ac.uk).}
\thanks{Ningli Wang is with Beijing Tongren Eye Center, Beijing Tongren Hospital, Capital Medical University, and also with Beijing Ophthalmology \& Visual Sciences Key Laboratory, Beijing, China (e-mail: wningli@vip.163.com)}
\thanks{Dong Xu is with the Department of Electrical Engineering and Computer Science, and the Christopher S. Bond Life Sciences Center, University of Missouri, USA (e-mail: xudong@missouri.edu).}
\thanks{Benny Lo is with the Facualty of Medicine, Imperial College London, U.K., and he is also with Precision Robotics (Hong Kong) Ltd., Hong Kong SAR (e-mail: benny.lo@imperial.ac.uk).}
\thanks{Corresponding authors: Wu Yuan and Benny Lo}
}

\maketitle






\begin{abstract}
Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people’s imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.
\end{abstract}

\begin{IEEEkeywords}
artificial intelligence; bioinformatics; biomedicine; deep learning; foundation model; health informatics; healthcare; medical imaging 
\end{IEEEkeywords}


\input{introduction.tex}
\input{background_large_ai_model.tex}
\input{applications.tex}
\input{challenges.tex}
\input{future.tex}
\input{conclusions.tex}




\bibliographystyle{IEEEtran}
% \bibliography{reference}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{chatgpt2022}
\BIBentryALTinterwordspacing
OpenAI, ``Chatgpt: Optimizing language models for dialogue,'' 2022. [Online].
  Available: \url{https://openai.com/blog/chatgpt/}
\BIBentrySTDinterwordspacing

\bibitem{kirillov2023segment}
A.~Kirillov \emph{et~al.}, ``Segment anything,'' \emph{arXiv preprint
  arXiv:2304.02643}, 2023.

\bibitem{vaswani2017attention}
A.~Vaswani \emph{et~al.}, ``Attention is all you need,'' \emph{NeurIPS},
  vol.~30, 2017.

\bibitem{gpt4openai2023}
O.~(2023), ``Gpt-4 technical report,'' \emph{arXiv:2303.08774}, 2023.

\bibitem{lee2023ai}
P.~Lee, C.~Goldberg, and I.~Kohane, \emph{The AI Revolution in Medicine: GPT-4
  and Beyond}.\hskip 1em plus 0.5em minus 0.4em\relax Pearson Education,
  Limited, 2023.

\bibitem{gulshan2016development}
V.~Gulshan, L.~Peng, M.~Coram \emph{et~al.}, ``Development and validation of a
  deep learning algorithm for detection of diabetic retinopathy in retinal
  fundus photographs,'' \emph{jama}, vol. 316, no.~22, pp. 2402--2410, 2016.

\bibitem{bommasani_opportunities_2022}
R.~Bommasani \emph{et~al.}, ``On the opportunities and risks of foundation
  models,'' \emph{arXiv:2108.07258}, 2021.

\bibitem{radford2018improving}
A.~Radford \emph{et~al.}, ``Improving language understanding by generative
  pre-training,'' 2018.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,''
  \emph{arXiv:1810.04805}, 2018.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard \emph{et~al.}, ``Llama: Open and efficient
  foundation language models,'' \emph{arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
H.~Touvron, L.~Martin, K.~Stone \emph{et~al.}, ``Llama 2: Open foundation and
  fine-tuned chat models,'' \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre \emph{et~al.}, ``Scaling instruction-finetuned
  language models,'' \emph{arXiv:2210.11416}, 2022.

\bibitem{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin \emph{et~al.}, ``Palm: Scaling language
  modeling with pathways,'' \emph{arXiv:2204.02311}, 2022.

\bibitem{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch \emph{et~al.}, ``Training compute-optimal
  large language models,'' \emph{arXiv:2203.15556}, 2022.

\bibitem{smith2022using}
S.~Smith \emph{et~al.}, ``Using deepspeed and megatron to train megatron-turing
  nlg 530b, a large-scale generative language model,'' \emph{arXiv:2201.11990},
  2022.

\bibitem{scao2022bloom}
T.~L. Scao, A.~Fan, C.~Akiki \emph{et~al.}, ``Bloom: A 176b-parameter
  open-access multilingual language model,'' \emph{arXiv:2211.05100}, 2022.

\bibitem{thoppilan2022lamda}
R.~Thoppilan, D.~De~Freitas, J.~Hall \emph{et~al.}, ``Lamda: Language models
  for dialog applications,'' \emph{arXiv:2201.08239}, 2022.

\bibitem{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal \emph{et~al.}, ``Opt: Open pre-trained
  transformer language models,'' \emph{arXiv:2205.01068}, 2022.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang \emph{et~al.}, ``Training language models to follow
  instructions with human feedback,'' \emph{arXiv:2203.02155}, 2022.

\bibitem{rae2021scaling}
J.~W. Rae, S.~Borgeaud, T.~Cai \emph{et~al.}, ``Scaling language models:
  Methods, analysis \& insights from training gopher,''
  \emph{arXiv:2112.11446}, 2021.

\bibitem{sanh2021multitask}
V.~Sanh, A.~Webson, C.~Raffel \emph{et~al.}, ``Multitask prompted training
  enables zero-shot task generalization,'' \emph{arXiv:2110.08207}, 2021.

\bibitem{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans \emph{et~al.}, ``Chain of thought prompting
  elicits reasoning in large language models,'' \emph{arXiv:2201.11903}, 2022.

\bibitem{borgeaud2022improving}
S.~Borgeaud, A.~Mensch, J.~Hoffmann \emph{et~al.}, ``Improving language models
  by retrieving from trillions of tokens,'' in \emph{ICML}.\hskip 1em plus
  0.5em minus 0.4em\relax PMLR, 2022, pp. 2206--2240.

\bibitem{christiano2017deep}
P.~F. Christiano, J.~Leike, T.~Brown \emph{et~al.}, ``Deep reinforcement
  learning from human preferences,'' \emph{NeurIPS}, vol.~30, 2017.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal \emph{et~al.}, ``Proximal policy
  optimization algorithms,'' \emph{arXiv:1707.06347}, 2017.

\bibitem{glaese2022improving}
A.~Glaese, N.~McAleese, M.~Trebacz \emph{et~al.}, ``Improving alignment of
  dialogue agents via targeted human judgements,'' \emph{arXiv:2209.14375},
  2022.

\bibitem{susano2023tuning}
A.~Susano~Pinto, A.~Kolesnikov, Y.~Shi \emph{et~al.}, ``Tuning computer vision
  models with task rewards,'' \emph{arXiv e-prints}, pp. arXiv--2302, 2023.

\bibitem{yosinski2014transferable}
J.~Yosinski \emph{et~al.}, ``How transferable are features in deep neural
  networks?'' \emph{NeurIPS}, vol.~27, 2014.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{CVPR}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2009, pp. 248--255.

\bibitem{ridnik1imagenet}
T.~Ridnik \emph{et~al.}, ``Imagenet-21k pretraining for the masses,'' in
  \emph{NeurIPS}.

\bibitem{sun2017revisiting}
C.~Sun \emph{et~al.}, ``Revisiting unreasonable effectiveness of data in deep
  learning era,'' in \emph{ICCV}, 2017, pp. 843--852.

\bibitem{zhai2022scaling}
X.~Zhai \emph{et~al.}, ``Scaling vision transformers,'' in \emph{CVPR}, 2022,
  pp. 12\,104--12\,113.

\bibitem{mahajan2018exploring}
D.~Mahajan \emph{et~al.}, ``Exploring the limits of weakly supervised
  pretraining,'' in \emph{ECCV}, 2018, pp. 181--196.

\bibitem{singh2022revisiting}
M.~Singh \emph{et~al.}, ``Revisiting weakly supervised pre-training of visual
  perception models,'' in \emph{CVPR}, 2022, pp. 804--814.

\bibitem{chen_generative_2020}
M.~Chen, A.~Radford, R.~Child \emph{et~al.},
  ``\BIBforeignlanguage{en}{Generative {Pretraining} {From} {Pixels}},'' in
  \emph{\BIBforeignlanguage{en}{ICML}}.\hskip 1em plus 0.5em minus 0.4em\relax
  PMLR, Nov. 2020.

\bibitem{assran2023self}
M.~Assran, Q.~Duval, I.~Misra \emph{et~al.}, ``Self-supervised learning from
  images with a joint-embedding predictive architecture,'' in \emph{CVPR},
  2023, pp. 15\,619--15\,629.

\bibitem{chen2021pre}
H.~Chen, Y.~Wang, T.~Guo \emph{et~al.}, ``Pre-trained image processing
  transformer,'' in \emph{CVPR}, 2021, pp. 12\,299--12\,310.

\bibitem{he_masked_2022}
K.~He, X.~Chen, S.~Xie \emph{et~al.}, ``\BIBforeignlanguage{en}{Masked
  {Autoencoders} {Are} {Scalable} {Vision} {Learners}},'' in
  \emph{\BIBforeignlanguage{en}{CVPR}}, 2022.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for
  contrastive learning of visual representations,'' in \emph{ICML}, 2020, pp.
  1597--1607.

\bibitem{dosovitskiy_image_2021}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov \emph{et~al.}, ``An {Image} is {Worth}
  16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale},'' in
  \emph{ICLR}, 2021.

\bibitem{han_transformer_2021}
K.~Han, A.~Xiao, E.~Wu \emph{et~al.}, ``Transformer in {Transformer},'' in
  \emph{NeurIPS}, vol.~34, 2021, pp. 15\,908--15\,919.

\bibitem{liu2021swin}
Z.~Liu \emph{et~al.}, ``Swin transformer: Hierarchical vision transformer using
  shifted windows,'' in \emph{ICCV}, 2021, pp. 10\,012--10\,022.

\bibitem{liu_swin_2022}
Z.~Liu, H.~Hu, Y.~Lin \emph{et~al.}, ``Swin {Transformer} {V2}: {Scaling} {Up}
  {Capacity} and {Resolution},'' in \emph{ICCV}, 2022.

\bibitem{Dehghani2023ScalingVT}
M.~Dehghani, J.~Djolonga, B.~Mustafa \emph{et~al.}, ``Scaling vision
  transformers to 22 billion parameters,'' \emph{ArXiv}, vol. abs/2302.05442,
  2023.

\bibitem{liu2022convnet}
Z.~Liu \emph{et~al.}, ``A convnet for the 2020s,'' in \emph{CVPR}, 2022, pp.
  11\,976--11\,986.

\bibitem{wang2023internimage}
W.~Wang, J.~Dai, Z.~Chen \emph{et~al.}, ``Internimage: Exploring large-scale
  vision foundation models with deformable convolutions,'' in \emph{CVPR},
  2023.

\bibitem{dai2021coatnet}
Z.~Dai, H.~Liu, Q.~V. Le, and M.~Tan, ``Coatnet: Marrying convolution and
  attention for all data sizes,'' \emph{NeurIPS}, vol.~34, pp. 3965--3977,
  2021.

\bibitem{d2021convit}
S.~d’Ascoli \emph{et~al.}, ``Convit: Improving vision transformers with soft
  convolutional inductive biases,'' in \emph{ICML}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2021, pp. 2286--2296.

\bibitem{kolesnikov2020big}
A.~Kolesnikov \emph{et~al.}, ``Big transfer (bit): General visual
  representation learning,'' in \emph{ECCV}, 2020, pp. 491--507.

\bibitem{tan2019efficientnet}
M.~Tan \emph{et~al.}, ``Efficientnet: Rethinking model scaling for
  convolutional neural networks,'' in \emph{ICML}, 2019, pp. 6105--6114.

\bibitem{tan2021efficientnetv2}
------, ``Efficientnetv2: Smaller models and faster training,'' in \emph{ICML},
  2021, pp. 10\,096--10\,106.

\bibitem{goyal2021self}
P.~Goyal \emph{et~al.}, ``Self-supervised pretraining of visual features in the
  wild,'' \emph{arXiv:2103.01988}, 2021.

\bibitem{huang2019gpipe}
Y.~Huang \emph{et~al.}, ``Gpipe: Efficient training of giant neural networks
  using pipeline parallelism,'' \emph{NeurIPS}, vol.~32, 2019.

\bibitem{radford2021learning}
A.~Radford \emph{et~al.}, ``Learning transferable visual models from natural
  language supervision,'' in \emph{ICML}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2021, pp. 8748--8763.

\bibitem{jia_scaling_2021}
C.~Jia, Y.~Yang, Y.~Xia \emph{et~al.}, ``\BIBforeignlanguage{en}{Scaling {Up}
  {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy}
  {Text} {Supervision}},'' in \emph{\BIBforeignlanguage{en}{ICML}}, Jul. 2021.

\bibitem{yuan_florence_2021}
L.~Yuan, D.~Chen, Y.-L. Chen \emph{et~al.}, ``Florence: {A} {New} {Foundation}
  {Model} for {Computer} {Vision},'' Nov. 2021, arXiv:2111.11432 [cs].

\bibitem{schuhmann_laion-5b_2022}
C.~Schuhmann, R.~Beaumont, R.~Vencu \emph{et~al.},
  ``\BIBforeignlanguage{en}{{LAION}-{5B}: {An} open large-scale dataset for
  training next generation image-text models},'' in
  \emph{\BIBforeignlanguage{en}{NeurIPS}}, Oct. 2022.

\bibitem{singh_flava_2022}
A.~Singh, R.~Hu, V.~Goswami \emph{et~al.}, ``\BIBforeignlanguage{en}{{FLAVA}:
  {A} {Foundational} {Language} and {Vision} {Alignment} {Model}},'' in
  \emph{\BIBforeignlanguage{en}{CVPR}}, 2022, pp. 15\,638--15\,650.

\bibitem{wang_simvlm_2022}
Z.~Wang, J.~Yu, A.~W. Yu \emph{et~al.}, ``\BIBforeignlanguage{en}{{SimVLM}:
  {Simple} {Visual} {Language} {Model} {Pretraining} with {Weak}
  {Supervision}},'' in \emph{\BIBforeignlanguage{en}{ICLR}}, Jan. 2022.

\bibitem{chen2022pali}
X.~Chen \emph{et~al.}, ``Pali: A jointly-scaled multilingual language-image
  model,'' \emph{arXiv:2209.06794}, 2022.

\bibitem{li_blip-2_2023}
J.~Li \emph{et~al.}, ``{BLIP}-2: {Bootstrapping} {Language}-{Image}
  {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language}
  {Models},'' Jan. 2023, arXiv:2301.12597 [cs].

\bibitem{huang_language_2023}
S.~Huang, L.~Dong, W.~Wang \emph{et~al.}, ``Language {Is} {Not} {All} {You}
  {Need}: {Aligning} {Perception} with {Language} {Models},'' Mar. 2023,
  arXiv:2302.14045 [cs].

\bibitem{li_blip_2022}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi, ``\BIBforeignlanguage{en}{{BLIP}:
  {Bootstrapping} {Language}-{Image} {Pre}-training for {Unified}
  {Vision}-{Language} {Understanding} and {Generation}},'' in
  \emph{\BIBforeignlanguage{en}{ICML}}.\hskip 1em plus 0.5em minus 0.4em\relax
  PMLR, Jun. 2022, pp. 12\,888--12\,900, iSSN: 2640-3498.

\bibitem{pham2021combined}
H.~Pham, Z.~Dai, G.~Ghiasi \emph{et~al.}, ``Combined scaling for
  open-vocabulary image classification,'' \emph{arXiv e-prints}, pp.
  arXiv--2111, 2021.

\bibitem{ramesh_zero-shot_2021}
A.~Ramesh, M.~Pavlov, G.~Goh \emph{et~al.},
  ``\BIBforeignlanguage{en}{Zero-{Shot} {Text}-to-{Image} {Generation}},'' in
  \emph{\BIBforeignlanguage{en}{ICML}}, Jul. 2021.

\bibitem{yuscaling}
J.~Yu, Y.~Xu, J.~Y. Koh \emph{et~al.}, ``Scaling autoregressive models for
  content-rich text-to-image generation,'' \emph{Transactions on Machine
  Learning Research}, 2022.

\bibitem{ramesh2022hierarchical}
A.~Ramesh, P.~Dhariwal, A.~Nichol \emph{et~al.}, ``Hierarchical
  text-conditional image generation with clip latents,''
  \emph{arXiv:2204.06125}, 2022.

\bibitem{rombach2021highresolution}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution
  image synthesis with latent diffusion models,'' 2021.

\bibitem{nichol2022glide}
A.~Q. Nichol \emph{et~al.}, ``Glide: Towards photorealistic image generation
  and editing with text-guided diffusion models,'' in \emph{ICML}, 2022, pp.
  16\,784--16\,804.

\bibitem{saharia2022photorealistic}
C.~Saharia \emph{et~al.}, ``Photorealistic text-to-image diffusion models with
  deep language understanding,'' \emph{arXiv:2205.11487}, 2022.

\bibitem{sohl2015deep}
J.~Sohl-Dickstein \emph{et~al.}, ``Deep unsupervised learning using
  nonequilibrium thermodynamics,'' in \emph{ICML}.\hskip 1em plus 0.5em minus
  0.4em\relax PMLR, 2015, pp. 2256--2265.

\bibitem{wu2023visual}
C.~Wu, S.~Yin, W.~Qi \emph{et~al.}, ``Visual chatgpt: Talking, drawing and
  editing with visual foundation models,'' \emph{arXiv:2303.04671}, 2023.

\bibitem{girdhar2023imagebind}
R.~Girdhar, A.~El-Nouby, Z.~Liu \emph{et~al.}, ``Imagebind: One embedding space
  to bind them all,'' in \emph{CVPR}, 2023, pp. 15\,180--15\,190.

\bibitem{zhang2023meta}
Y.~Zhang, K.~Gong, K.~Zhang \emph{et~al.}, ``Meta-transformer: A unified
  framework for multimodal learning,'' \emph{arXiv preprint arXiv:2307.10802},
  2023.

\bibitem{bai2015cryo}
X.-C. Bai \emph{et~al.}, ``How cryo-em is revolutionizing structural biology,''
  \emph{Trends in biochemical sciences}, vol.~40, no.~1, pp. 49--57, 2015.

\bibitem{wuthrich2001way}
K.~W{\"u}thrich, ``The way to nmr structures of proteins,'' \emph{Nature
  structural biology}, vol.~8, no.~11, pp. 923--925, 2001.

\bibitem{grimes2018crystallography}
J.~M. Grimes \emph{et~al.}, ``Where is crystallography going?'' \emph{Acta
  Crystallographica Section D: Structural Biology}, vol.~74, no.~2, pp.
  152--166, 2018.

\bibitem{jumper2021highly}
J.~Jumper, R.~Evans, A.~Pritzel \emph{et~al.}, ``Highly accurate protein
  structure prediction with alphafold,'' \emph{Nature}, vol. 596, no. 7873, pp.
  583--589, 2021.

\bibitem{evans2021protein}
R.~Evans \emph{et~al.}, ``Protein complex prediction with alphafold-multimer,''
  \emph{BioRxiv}, pp. 2021--10, 2021.

\bibitem{madani2020progen}
A.~Madani, B.~McCann, N.~Naik \emph{et~al.}, ``Progen: Language modeling for
  protein generation,'' \emph{arXiv:2004.03497}, 2020.

\bibitem{article}
A.~Elnaggar \emph{et~al.}, ``Prottrans: Towards cracking the language of lifes
  code through self-supervised deep learning and high performance computing,''
  \emph{IEEE TPAMI}, vol.~PP, pp. 1--1, 07 2021.

\bibitem{steinegger2018clustering}
M.~Steinegger and J.~S{\"o}ding, ``Clustering huge protein sequence sets in
  linear time,'' \emph{Nature communications}, vol.~9, no.~1, p. 2542, 2018.

\bibitem{suzek2015uniref}
B.~E. Suzek \emph{et~al.}, ``Uniref clusters: a comprehensive and scalable
  alternative for improving sequence similarity searches,''
  \emph{Bioinformatics}, vol.~31, no.~6, pp. 926--932, 2015.

\bibitem{lin2023evolutionary}
Z.~Lin \emph{et~al.}, ``Evolutionary-scale prediction of atomic-level protein
  structure with a language model,'' \emph{Science}, vol. 379, no. 6637, pp.
  1123--1130, 2023.

\bibitem{wu2022high}
R.~Wu, F.~Ding, R.~Wang \emph{et~al.}, ``High-resolution de novo structure
  prediction from primary sequence,'' \emph{BioRxiv}, pp. 2022--07, 2022.

\bibitem{baek2021accurate}
M.~Baek \emph{et~al.}, ``Accurate prediction of protein structures and
  interactions using a three-track neural network,'' \emph{Science}, vol. 373,
  no. 6557, pp. 871--876, 2021.

\bibitem{Chen2023.07.05.547496}
B.~Chen, X.~Cheng, L.~ao~Gengyang \emph{et~al.}, ``xtrimopglm: Unified
  100b-scale pre-trained transformer for deciphering the language of protein,''
  \emph{bioRxiv}, 2023.

\bibitem{elnaggar2023ankh}
A.~Elnaggar \emph{et~al.}, ``Ankh: Optimized protein language model unlocks
  general-purpose modelling,'' \emph{bioRxiv}, pp. 2023--01, 2023.

\bibitem{chen2022interpretable}
J.~Chen, Z.~Hu, S.~Sun \emph{et~al.}, ``Interpretable rna foundation model from
  unannotated data for highly accurate rna structure and function
  predictions,'' \emph{bioRxiv}, pp. 2022--08, 2022.

\bibitem{rnacentral2021rnacentral}
``Rnacentral 2021: secondary structure integration, improved sequence search
  and new member databases,'' \emph{Nucleic acids research}, vol.~49, no.~D1,
  pp. D212--D220, 2021.

\bibitem{fu2022ufold}
L.~Fu, Y.~Cao, J.~Wu, Q.~Peng, Q.~Nie, and X.~Xie, ``Ufold: fast and accurate
  rna secondary structure prediction with deep learning,'' \emph{Nucleic acids
  research}, vol.~50, no.~3, pp. e14--e14, 2022.

\bibitem{shen2022e2efold}
T.~Shen, Z.~Hu, Z.~Peng \emph{et~al.}, ``E2efold-3d: End-to-end deep learning
  method for accurate de novo rna 3d structure prediction,''
  \emph{arXiv:2207.01586}, 2022.

\bibitem{buel2022can}
G.~R. Buel and K.~J. Walters, ``Can alphafold2 predict the impact of missense
  mutations on structure?'' \emph{Nature Structural \& Molecular Biology},
  vol.~29, no.~1, pp. 1--2, 2022.

\bibitem{tiu2022expert}
E.~Tiu, E.~Talius, P.~Patel \emph{et~al.}, ``Expert-level detection of
  pathologies from unannotated chest x-ray images via self-supervised
  learning,'' \emph{Nature Biomedical Engineering}, pp. 1--8, 2022.

\bibitem{wang2023chatcad}
S.~Wang \emph{et~al.}, ``Chatcad: Interactive computer-aided diagnosis on
  medical image using large language models,'' \emph{arXiv:2302.07257}, 2023.

\bibitem{zhao2023chatcad+}
Z.~Zhao, S.~Wang, J.~Gu \emph{et~al.}, ``Chatcad+: Towards a universal and
  reliable interactive cad using llms,'' \emph{arXiv preprint
  arXiv:2305.15964}, 2023.

\bibitem{li2023chatdoctor}
Y.~Li, Z.~Li, K.~Zhang \emph{et~al.}, ``Chatdoctor: A medical chat model
  fine-tuned on a large language model meta-ai (llama) using medical domain
  knowledge,'' \emph{Cureus}, vol.~15, no.~6, 2023.

\bibitem{thawkar2023xraygpt}
O.~Thawkar, A.~Shaker, S.~S. Mullappilly \emph{et~al.}, ``Xraygpt: Chest
  radiographs summarization using medical vision-language models,'' \emph{arXiv
  preprint arXiv:2306.07971}, 2023.

\bibitem{wang2023huatuo}
H.~Wang, C.~Liu, N.~Xi \emph{et~al.}, ``Huatuo: Tuning llama model with chinese
  medical knowledge,'' \emph{arXiv preprint arXiv:2304.06975}, 2023.

\bibitem{vaid2023foundational}
A.~Vaid, J.~Jiang, A.~Sawant \emph{et~al.}, ``A foundational vision transformer
  improves diagnostic performance for electrocardiograms,'' \emph{NPJ Digital
  Medicine}, vol.~6, no.~1, p. 108, 2023.

\bibitem{li2020behrt}
Y.~Li, S.~Rao, J.~R.~A. Solares \emph{et~al.}, ``Behrt: transformer for
  electronic health records,'' \emph{Scientific reports}, vol.~10, no.~1, pp.
  1--12, 2020.

\bibitem{rasmy2021medbert}
L.~Rasmy, Y.~Xiang, Z.~Xie \emph{et~al.}, ``Med-bert: pretrained contextualized
  embeddings on large-scale structured electronic health records for disease
  prediction,'' \emph{NPJ digital medicine}, vol.~4, no.~1, p.~86, 2021.

\bibitem{shi2023generalist}
P.~Shi, J.~Qiu, S.~M.~D. Abaxi \emph{et~al.}, ``Generalist vision foundation
  models for medical imaging: A case study of segment anything model on
  zero-shot medical segmentation,'' \emph{Diagnostics}, vol.~13, no.~11, p.
  1947, 2023.

\bibitem{wu2023medical}
J.~Wu, R.~Fu, H.~Fang \emph{et~al.}, ``Medical sam adapter: Adapting segment
  anything model for medical image segmentation,'' \emph{arXiv preprint
  arXiv:2304.12620}, 2023.

\bibitem{wang2022medclip}
Z.~Wang, Z.~Wu, D.~Agarwal, and J.~Sun, ``Medclip: Contrastive learning from
  unpaired medical images and text,'' \emph{arXiv:2210.10163}, 2022.

\bibitem{huang2023visual}
Z.~Huang \emph{et~al.}, ``A visual--language foundation model for pathology
  image analysis using medical twitter,'' \emph{Nature Medicine}, pp. 1--10,
  2023.

\bibitem{chen2019med3d}
S.~Chen, K.~Ma, and Y.~Zheng, ``Med3d: Transfer learning for 3d medical image
  analysis,'' \emph{arXiv:1904.00625}, 2019.

\bibitem{chambon2022adapting}
P.~Chambon \emph{et~al.}, ``Adapting pretrained vision-language foundational
  models to medical imaging domains,'' \emph{arXiv:2210.04133}, 2022.

\bibitem{huang2023stu}
Z.~Huang, H.~Wang, Z.~Deng \emph{et~al.}, ``Stu-net: Scalable and transferable
  medical image segmentation models empowered by large-scale supervised
  pre-training,'' \emph{arXiv preprint arXiv:2304.06716}, 2023.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder \emph{et~al.}, ``Language models are few-shot
  learners,'' \emph{NeurIPS}, vol.~33, pp. 1877--1901, 2020.

\bibitem{pubmedabstract2023}
\BIBentryALTinterwordspacing
``Pubmed abstract,'' 2023. [Online]. Available:
  \url{https://pubmed.ncbi.nlm.nih.gov/download/}
\BIBentrySTDinterwordspacing

\bibitem{pubmedcentral2023}
\BIBentryALTinterwordspacing
``Pubmed central,'' 2023. [Online]. Available:
  \url{https://www.ncbi.nlm.nih.gov/pmc/}
\BIBentrySTDinterwordspacing

\bibitem{lee2020biobert}
J.~Lee \emph{et~al.}, ``Biobert: a pre-trained biomedical language
  representation model for biomedical text mining,'' \emph{Bioinformatics},
  vol.~36, no.~4, pp. 1234--1240, 2020.

\bibitem{alsentzer2019publicly}
E.~Alsentzer \emph{et~al.}, ``Publicly available clinical bert embeddings,''
  \emph{arXiv}, 2019.

\bibitem{shin2020biomegatron}
H.-C. Shin, Y.~Zhang, E.~Bakhturina \emph{et~al.}, ``Biomegatron: Larger
  biomedical domain language model,'' \emph{arXiv:2010.06060}, 2020.

\bibitem{gururangan2020don}
S.~Gururangan, A.~Marasovi{\'c}, S.~Swayamdipta \emph{et~al.}, ``Don't stop
  pretraining: Adapt language models to domains and tasks,''
  \emph{arXiv:2004.10964}, 2020.

\bibitem{rasmy2021med}
L.~Rasmy, Y.~Xiang, Z.~Xie, C.~Tao, and D.~Zhi, ``Med-bert: pretrained
  contextualized embeddings on large-scale structured electronic health records
  for disease prediction,'' \emph{NPJ digital medicine}, vol.~4, no.~1, p.~86,
  2021.

\bibitem{raj2021bioelectra}
K.~raj Kanakarajan, B.~Kundumani, and M.~Sankarasubbu, ``Bioelectra: pretrained
  biomedical text encoder using discriminators,'' in \emph{Proceedings of the
  20th Workshop on Biomedical Language Processing}, 2021, pp. 143--154.

\bibitem{gu2021domain}
Y.~Gu, R.~Tinn, H.~Cheng \emph{et~al.}, ``Domain-specific language model
  pretraining for biomedical natural language processing,'' \emph{HEALTH},
  vol.~3, no.~1, pp. 1--23, 2021.

\bibitem{yasunaga2022linkbert}
M.~Yasunaga, J.~Leskovec, and P.~Liang, ``Linkbert: Pretraining language models
  with document links,'' \emph{arXiv:2203.15827}, 2022.

\bibitem{luo2022biogpt}
R.~Luo \emph{et~al.}, ``Biogpt: generative pre-trained transformer for
  biomedical text generation and mining,'' \emph{Briefings in Bioinformatics},
  vol.~23, no.~6, 2022.

\bibitem{singhal2022large}
K.~Singhal, S.~Azizi, T.~Tu \emph{et~al.}, ``Large language models encode
  clinical knowledge,'' \emph{arXiv:2212.13138}, 2022.

\bibitem{yang2022large}
X.~Yang, A.~Chen, N.~PourNejatian \emph{et~al.}, ``A large language model for
  electronic health records,'' \emph{npj Digital Medicine}, vol.~5, no.~1, p.
  194, 2022.

\bibitem{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis \emph{et~al.}, ``Lora: Low-rank adaptation of
  large language models,'' \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{han2023medalpaca}
T.~Han, L.~C. Adams, J.-M. Papaioannou \emph{et~al.}, ``Medalpaca--an
  open-source collection of medical conversational ai models and training
  data,'' \emph{arXiv preprint arXiv:2304.08247}, 2023.

\bibitem{wei2022emergent}
J.~Wei \emph{et~al.}, ``Emergent abilities of large language models,''
  \emph{arXiv}, 2022.

\bibitem{agrawal2022large}
M.~Agrawal, S.~Hegselmann, H.~Lang, Y.~Kim, and D.~Sontag, ``Large language
  models are few-shot clinical information extractors,''
  \emph{arXiv:2205.12689}, 2022.

\bibitem{singhal2023towards}
K.~Singhal, T.~Tu, J.~Gottweis \emph{et~al.}, ``Towards expert-level medical
  question answering with large language models,'' \emph{arXiv preprint
  arXiv:2305.09617}, 2023.

\bibitem{lievin2022can}
V.~Li{\'e}vin, C.~E. Hother, and O.~Winther, ``Can large language models reason
  about medical questions?'' \emph{arXiv:2207.08143}, 2022.

\bibitem{patel2023chatgpt}
S.~B. Patel and K.~Lam, ``Chatgpt: the future of discharge summaries?''
  \emph{The Lancet Digital Health}, 2023.

\bibitem{priorauth2023}
\BIBentryALTinterwordspacing
``Fairway health - process prior authorization faster,'' 2023. [Online].
  Available:
  \url{https://www.ycombinator.com/launches/IIu-fairway-health-process-prior-authorization-faster}
\BIBentrySTDinterwordspacing

\bibitem{kung2023performance}
T.~H. Kung, M.~Cheatham, A.~Medenilla \emph{et~al.}, ``Performance of chatgpt
  on usmle: Potential for ai-assisted medical education using large language
  models,'' \emph{PLOS Digital Health}, vol.~2, no.~2, p. e0000198, 2023.

\bibitem{shue2023empowering}
E.~Shue, L.~Liu, B.~Li, Z.~Feng, X.~Li, and G.~Hu, ``Empowering beginners in
  bioinformatics with chatgpt,'' \emph{bioRxiv}, pp. 2023--03, 2023.

\bibitem{dai2023chataug}
H.~Dai, Z.~Liu, W.~Liao \emph{et~al.}, ``Chataug: Leveraging chatgpt for text
  data augmentation,'' \emph{arXiv:2302.13007}, 2023.

\bibitem{mitchell2023detectgpt}
E.~Mitchell \emph{et~al.}, ``Detectgpt: Zero-shot machine-generated text
  detection using probability curvature,'' \emph{arXiv:2301.11305}, 2023.

\bibitem{lin2022pangu}
X.~Lin, C.~Xu, Z.~Xiong \emph{et~al.}, ``Pangu drug model: learn a molecule
  like a human,'' \emph{bioRxiv}, pp. 2022--03, 2022.

\bibitem{korngiebel2021considering}
D.~M. Korngiebel and S.~D. Mooney, ``Considering the possibilities and pitfalls
  of generative pre-trained transformer 3 (gpt-3) in healthcare delivery,''
  \emph{NPJ Digital Medicine}, vol.~4, no.~1, p.~93, 2021.

\bibitem{chen2020tracking}
E.~Chen, K.~Lerman, E.~Ferrara \emph{et~al.}, ``Tracking social media discourse
  about the covid-19 pandemic: Development of a public coronavirus twitter data
  set,'' \emph{JMIR public health and surveillance}, vol.~6, no.~2, p. e19273,
  2020.

\bibitem{peng2022clustering}
J.~Peng \emph{et~al.}, ``Clustering egocentric images in passive dietary
  monitoring with self-supervised learning,'' in \emph{IEEE-EMBS BHI}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 01--04.

\bibitem{qiu2023egocentric}
J.~Qiu \emph{et~al.}, ``Egocentric image captioning for privacy-preserved
  passive dietary intake monitoring,'' \emph{IEEE Transactions on Cybernetics},
  2023.

\bibitem{popkin2019dynamics}
B.~M. Popkin, C.~Corvalan, and L.~M. Grummer-Strawn, ``Dynamics of the double
  burden of malnutrition and the changing nutrition reality,'' \emph{The
  Lancet}, 2019.

\bibitem{nguyen2023climax}
T.~Nguyen, J.~Brandstetter, A.~Kapoor \emph{et~al.}, ``Climax: A foundation
  model for weather and climate,'' \emph{arXiv:2301.10343}, 2023.

\bibitem{bi2023accurate}
K.~Bi, L.~Xie, H.~Zhang \emph{et~al.}, ``Accurate medium-range global weather
  forecasting with 3d neural networks,'' \emph{Nature}, pp. 1--6, 2023.

\bibitem{wang2023foundation}
Z.~Wang, C.~Liu, S.~Zhang \emph{et~al.}, ``Foundation model for endoscopy video
  analysis via large-scale self-supervised pre-train,'' \emph{arXiv preprint
  arXiv:2306.16741}, 2023.

\bibitem{d2022emotion}
G.~D’Onofrio, L.~Fiorini, A.~Sorrentino \emph{et~al.}, ``Emotion recognizing
  by a robotic solution initiative (emotive project),'' \emph{Sensors},
  vol.~22, no.~8, p. 2861, 2022.

\bibitem{qiu2022egocentric}
J.~Qiu, L.~Chen, X.~Gu \emph{et~al.}, ``Egocentric human trajectory forecasting
  with a wearable camera and multi-modal fusion,'' \emph{IEEE Robotics and
  Automation Letters}, vol.~7, no.~4, pp. 8799--8806, 2022.

\bibitem{asgharian2022review}
P.~Asgharian, A.~M. Panchea, and F.~Ferland, ``A review on the use of mobile
  service robots in elderly care,'' \emph{Robotics}, vol.~11, no.~6, p. 127,
  2022.

\bibitem{seenivasan2023surgicalgpt}
L.~Seenivasan, M.~Islam, G.~Kannan \emph{et~al.}, ``Surgicalgpt: End-to-end
  language-vision gpt for visual question answering in surgery,'' \emph{arXiv
  preprint arXiv:2304.09974}, 2023.

\bibitem{vemprala2023chatgpt}
\BIBentryALTinterwordspacing
S.~Vemprala, R.~Bonatti, A.~Bucker, and A.~Kapoor, ``Chatgpt for robotics:
  Design principles and model abilities,'' Microsoft, Tech. Rep. MSR-TR-2023-8,
  February 2023. [Online]. Available:
  \url{https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/}
\BIBentrySTDinterwordspacing

\bibitem{reed2022a}
S.~Reed \emph{et~al.}, ``A generalist agent,'' \emph{TMLR}, 2022.

\bibitem{shridhar2022cliport}
M.~Shridhar \emph{et~al.}, ``Cliport: What and where pathways for robotic
  manipulation,'' in \emph{Conference on Robot Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2022, pp. 894--906.

\bibitem{shridhar2022perceiver}
------, ``Perceiver-actor: A multi-task transformer for robotic manipulation,''
  \emph{arXiv:2209.05451}, 2022.

\bibitem{saycan2022arxiv}
M.~Ahn, A.~Brohan, N.~Brown \emph{et~al.}, ``Do as i can and not as i say:
  Grounding language in robotic affordances,'' in \emph{arXiv:2204.01691},
  2022.

\bibitem{driess2023palme}
D.~Driess, F.~Xia, M.~S.~M. Sajjadi \emph{et~al.}, ``Palm-e: An embodied
  multimodal language model,'' in \emph{arXiv:2303.03378}, 2023.

\bibitem{jiang2022vima}
Y.~Jiang, A.~Gupta, Z.~Zhang \emph{et~al.}, ``Vima: General robot manipulation
  with multimodal prompts,'' \emph{arXiv:2210.03094}, 2022.

\bibitem{brohan2022rt}
A.~Brohan, N.~Brown, J.~Carbajal \emph{et~al.}, ``Rt-1: Robotics transformer
  for real-world control at scale,'' \emph{arXiv:2212.06817}, 2022.

\bibitem{ding2023parameter}
N.~Ding, Y.~Qin, G.~Yang \emph{et~al.}, ``Parameter-efficient fine-tuning of
  large-scale pre-trained language models,'' \emph{Nature Machine
  Intelligence}, vol.~5, no.~3, pp. 220--235, 2023.

\bibitem{gilbert2023large}
S.~Gilbert, H.~Harvey, T.~Melvin \emph{et~al.}, ``Large language model ai
  chatbots require approval as medical devices,'' \emph{Nature Medicine}, pp.
  1--3, 2023.

\bibitem{shen2023chatgpt}
X.~Shen, Z.~Chen, M.~Backes, and Y.~Zhang, ``In chatgpt we trust? measuring and
  characterizing the reliability of chatgpt,'' \emph{arXiv preprint
  arXiv:2304.08979}, 2023.

\bibitem{lee2023benefits}
P.~Lee, S.~Bubeck, and J.~Petro, ``Benefits, limits, and risks of gpt-4 as an
  ai chatbot for medicine,'' \emph{New England Journal of Medicine}, vol. 388,
  no.~13, pp. 1233--1239, 2023.

\bibitem{nori2023capabilities}
H.~Nori, N.~King, S.~M. McKinney, D.~Carignan, and E.~Horvitz, ``Capabilities
  of gpt-4 on medical challenge problems,'' \emph{arXiv preprint
  arXiv:2303.13375}, 2023.

\bibitem{wang2023robustness}
J.~Wang, X.~Hu, W.~Hou \emph{et~al.}, ``On the robustness of chatgpt: An
  adversarial and out-of-distribution perspective,'' \emph{arXiv:2302.12095},
  2023.

\bibitem{li2023data}
L.~Li and M.~W. Spratling, ``Data augmentation alone can improve adversarial
  training,'' in \emph{ICLR}, 2023.

\bibitem{carlini2021extracting}
N.~Carlini, F.~Tramer, E.~Wallace, M.~Jagielski, A.~Herbert-Voss, K.~Lee,
  A.~Roberts, T.~B. Brown, D.~Song, U.~Erlingsson \emph{et~al.}, ``Extracting
  training data from large language models.'' in \emph{USENIX Security
  Symposium}, vol.~6, 2021.

\bibitem{carlini2022quantifying}
N.~Carlini, D.~Ippolito, M.~Jagielski, K.~Lee, F.~Tramer, and C.~Zhang,
  ``Quantifying memorization across neural language models,'' \emph{arXiv
  preprint arXiv:2202.07646}, 2022.

\bibitem{li2023multi}
H.~Li, D.~Guo, W.~Fan, M.~Xu, and Y.~Song, ``Multi-step jailbreaking privacy
  attacks on chatgpt,'' \emph{arXiv preprint arXiv:2304.05197}, 2023.

\bibitem{shokri2017membership}
R.~Shokri, M.~Stronati, C.~Song, and V.~Shmatikov, ``Membership inference
  attacks against machine learning models,'' in \emph{2017 IEEE symposium on
  security and privacy (SP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2017, pp. 3--18.

\bibitem{duan2023diffusion}
J.~Duan, F.~Kong, S.~Wang, X.~Shi, and K.~Xu, ``Are diffusion models vulnerable
  to membership inference attacks?'' \emph{arXiv preprint arXiv:2302.01316},
  2023.

\bibitem{openaidatapolicy2023}
\BIBentryALTinterwordspacing
``How your data is used to improve model performance,'' 2023. [Online].
  Available:
  \url{https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance}
\BIBentrySTDinterwordspacing

\bibitem{openaichatbug2023}
\BIBentryALTinterwordspacing
``March 20 chatgpt outage: Here’s what happened,'' 2023. [Online]. Available:
  \url{https://openai.com/blog/march-20-chatgpt-outage}
\BIBentrySTDinterwordspacing

\bibitem{greshake2023more}
K.~Greshake, S.~Abdelnabi, S.~Mishra \emph{et~al.}, ``More than you've asked
  for: A comprehensive analysis of novel prompt injection threats to
  application-integrated large language models,'' \emph{arXiv preprint
  arXiv:2302.12173}, 2023.

\bibitem{hall2015implicit}
W.~J. Hall, M.~V. Chapman, K.~M. Lee \emph{et~al.}, ``Implicit racial/ethnic
  bias among health care professionals and its influence on health care
  outcomes: a systematic review,'' \emph{American journal of public health},
  vol. 105, no.~12, pp. e60--e76, 2015.

\bibitem{obermeyer2019dissecting}
Z.~Obermeyer, B.~Powers, C.~Vogeli, and S.~Mullainathan, ``Dissecting racial
  bias in an algorithm used to manage the health of populations,''
  \emph{Science}, vol. 366, no. 6464, pp. 447--453, 2019.

\bibitem{cirillo2020sex}
D.~Cirillo, S.~Catuara-Solarz, C.~Morey \emph{et~al.}, ``Sex and gender
  differences and biases in artificial intelligence for biomedicine and
  healthcare,'' \emph{NPJ digital medicine}, vol.~3, no.~1, p.~81, 2020.

\bibitem{char2018implementing}
D.~S. Char, N.~H. Shah, and D.~Magnus, ``Implementing machine learning in
  health care—addressing ethical challenges,'' \emph{The New England journal
  of medicine}, vol. 378, no.~11, p. 981, 2018.

\bibitem{rutinowski2023self}
J.~Rutinowski, S.~Franke, J.~Endendyk, I.~Dormuth, and M.~Pauly, ``The
  self-perception and political biases of chatgpt,'' \emph{arXiv:2304.07333},
  2023.

\bibitem{zhuo2023exploring}
T.~Y. Zhuo, Y.~Huang, C.~Chen \emph{et~al.}, ``Exploring ai ethics of chatgpt:
  A diagnostic analysis,'' \emph{arXiv:2301.12867}, 2023.

\bibitem{hendrycks2020aligning}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Critch, J.~Li, D.~Song, and
  J.~Steinhardt, ``Aligning ai with shared human values,'' \emph{arXiv preprint
  arXiv:2008.02275}, 2020.

\bibitem{gehman2020realtoxicityprompts}
S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith,
  ``Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models,'' \emph{arXiv preprint arXiv:2009.11462}, 2020.

\bibitem{daras2022discovering}
G.~Daras and A.~G. Dimakis, ``Discovering the hidden vocabulary of dalle-2,''
  \emph{arXiv preprint arXiv:2206.00169}, 2022.

\bibitem{wang2023investigating}
Y.~Wang, P.~Shi, and H.~Zhang, ``Investigating the existence of" secret
  language''in language models,'' \emph{arXiv preprint arXiv:2307.12507}, 2023.

\bibitem{kojima2022large}
T.~Kojima, S.~S. Gu, M.~Reid \emph{et~al.}, ``Large language models are
  zero-shot reasoners,'' \emph{Advances in neural information processing
  systems}, vol.~35, pp. 22\,199--22\,213, 2022.

\bibitem{elhage2021mathematical}
N.~Elhage, N.~Nanda, C.~Olsson \emph{et~al.}, ``A mathematical framework for
  transformer circuits,'' \emph{Transformer Circuits Thread}, vol.~1, 2021.

\bibitem{patterson2021carbon}
D.~Patterson \emph{et~al.}, ``Carbon emissions and large neural network
  training,'' \emph{arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{patterson2022carbon}
D.~Patterson, J.~Gonzalez, U.~H{\"o}lzle \emph{et~al.}, ``The carbon footprint
  of machine learning training will plateau, then shrink,'' \emph{Computer},
  vol.~55, no.~7, 2022.

\bibitem{villaronga2018humans}
E.~F. Villaronga, P.~Kieseberg, and T.~Li, ``Humans forget, machines remember:
  Artificial intelligence and the right to be forgotten,'' \emph{Computer Law
  \& Security Review}, vol.~34, no.~2, pp. 304--313, 2018.

\bibitem{lecun2022path}
Y.~LeCun, ``A path towards autonomous machine intelligence version 0.9. 2,
  2022-06-27,'' 2022.

\bibitem{liu2023pre}
P.~Liu, W.~Yuan, J.~Fu \emph{et~al.}, ``Pre-train, prompt, and predict: A
  systematic survey of prompting methods in natural language processing,''
  \emph{ACM Computing Surveys}, vol.~55, no.~9, pp. 1--35, 2023.

\bibitem{leslie2019understanding}
D.~Leslie, ``Understanding artificial intelligence ethics and safety,''
  \emph{arXiv preprint arXiv:1906.05684}, 2019.

\bibitem{siddiqui2022metadata}
S.~A. Siddiqui, N.~Rajkumar, T.~Maharaj \emph{et~al.}, ``Metadata archaeology:
  Unearthing data subsets by leveraging training dynamics,'' in \emph{ICLR},
  2022.

\bibitem{huang2023survey}
X.~Huang \emph{et~al.}, ``A survey of safety and trustworthiness of large
  language models through the lens of verification and validation,''
  \emph{arXiv:2305.11391}, 2023.

\end{thebibliography}


\end{document}
