\section{Background of Large AI Models}\label{sec:background_large_ai_model}





The burgeoning AI community has devoted much effort to developing large AI models (LAMs) in recent years by leveraging the massive influx of data and computational resources. Based on the pre-training data modality, this article categorizes the current LAMs into three types and defines them as follows:


\begin{enumerate}
    \item Large Language Model (LLM): LLMs are pre-trained on language data and applied to language downstream tasks. Language in different settings can have different interpretations, e.g., protein is the language of life, and code is the language of computers.
    \item Large Vision Model (LVM): LVMs are pre-trained on vision data and applied to vision downstream tasks.
    \item Large Multi-modal Model (LMM): LMMs are pre-trained on multi-modal data, e.g., language and vision data, and applied to various single- or multi-modal downstream tasks.
\end{enumerate}

This section provides an overview of the background of these three types of LAMs in general domains.







\subsection{Large Language Models}






The proposal of the Transformer architecture~\cite{vaswani2017attention} heralds the start of developing large language models (LLMs) in the field of NLP. Since 2018, following the birth of GPT (Generative Pre-trained Transformer)~\cite{radford2018improving} and BERT (Bidirectional Encoder Representations from Transformers)~\cite{devlin2018bert}, the development of LLMs has progressed rapidly.

Broadly speaking, the recent LLMs~\cite{touvron2023llama,touvron2023llama2,chung2022scaling,chowdhery2022palm,hoffmann2022training,smith2022using,scao2022bloom,thoppilan2022lamda,zhang2022opt,ouyang2022training,rae2021scaling,sanh2021multitask}, have the following three distinct characteristics: 1) parameter-wise, the number of learnable parameters of an LLM can be easily scaled up to billions; 2) data-wise, a large volume of unlabelled data are used to pre-train an LLM, and the amount can often reach millions or billions if not more; 3) paradigm-wise, LLMs are first pre-trained often with weakly- or self-supervised learning (e.g., masked language modeling~\cite{devlin2018bert} and next token prediction~\cite{gpt4openai2023}), and then fine-tuned or adapted to various downstream tasks such as question answering and dialogue in which they are able to demonstrate impressive performance. 

Recent advances reveal that LLMs are impressive zero-shot, one-shot, and few-shot learners. They are able to extract, summarize, translate, and generate textual information with only a few or even no prompt/fine-tuning samples~\cite{gpt4openai2023}. Furthermore, LLMs manifest impressive reasoning capability, and this capability can be further strengthened with prompt engineering techniques such as Chain-of-Thought prompting~\cite{wei2022chain}.

There was an upsurge in the number of new LLMs from 2022 onwards. Despite the general consensus that scaling up the number of parameters and the amount of data will lead to improved performance, which leads to a dominant trend of developing LLMs often with billions of parameters (e.g., LLMs such as PaLM~\cite{chowdhery2022palm} have already contained 540 billion parameters) and even trillions of data tokens (e.g., LLaMa 2 was pre-trained with 2 trillion tokens~\cite{touvron2023llama2}, and the training data of RETRO~\cite{borgeaud2022improving} had over 5 trillion tokens), there is currently no concerted agreement within the community that if this continuous growth of model and data size is optimal~\cite{hoffmann2022training,touvron2023llama}, and there is also lacking a verified universal scaling law.







To balance the data annotation cost and efficacy, as well as to train an LLM that can better align with human intent, researchers have commonly used reinforcement learning from human feedback (RLHF)~\cite{christiano2017deep} to develop LLMs that can exhibit desired behaviors. The core idea of RLHF is to use human preference datasets to train a Reward Model (RM), which can predict the reward function and be optimized by RL algorithms (e.g., Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}). The framework of RLHF has attracted much attention and become a key component of many LLMs, such as InstructGPT~\cite{ouyang2022training}, Sparrow~\cite{glaese2022improving}, and ChatGPT~\cite{chatgpt2022}. Recently, Susano Pinto et al.~\cite{susano2023tuning} have also investigated this reward optimization in vision tasks, which can possibly advance the development of future LVMs using RLHF. 































\subsection{Large Vision Models} \label{sec: background lvms}


In computer vision, it has been a common practice for years to first pre-train a model on a large-scale dataset and then fine-tune it on the dataset of interest (usually smaller than the one for pre-training) for improved generalization \cite{yosinski2014transferable}. The fundamental changes driving this evolution of large models lie in \textbf{the scale of pre-training datasets and models}, and \textbf{the pre-training methods}. ImageNet-1K (1.28M images) \cite{deng2009imagenet} and -21K (14M images) \cite{ridnik1imagenet} used to be canonical datasets for visual pre-training. ImageNet is manually curated for high-quality labeling, but the prohibitive cost of curation severely hindered further scaling. To push the scale beyond ImageNet's, datasets like JFT (300M \cite{sun2017revisiting} and 3B \cite{zhai2022scaling} images) and IG (3.5B \cite{mahajan2018exploring} and 3.6B \cite{singh2022revisiting} images) were collected from the web with less or no curation. The quality of annotation is therefore compromised, and the accessibility of datasets become limited because of copyright issues.

The compromised annotation requires the pre-training paradigm to shift from supervised learning to weakly-/self-supervised learning or unsupervised learning. The latter methods include autoregressive modeling, generative modeling and contrastive learning. Autoregressive modeling trains the model to autoregressively predict the next pixel conditioned on the preceding pixels \cite{chen_generative_2020}. Generative modeling trains the model to reconstruct the entire original image, or some target regions within it \cite{assran2023self}, from its corrupted \cite{chen2021pre} or masked \cite{he_masked_2022} variants. Contrastive learning trains the model to discriminate similar and/or dissimilar data instances \cite{chen2020simple}.



Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) are two major architectural families of LVMs. For vision transformers, pioneering works ViT \cite{dosovitskiy_image_2021} and iGPT \cite{chen_generative_2020} transferred the transformer architectures from NLP to CV with minimal modification, but the resulting architectures incur high computational complexity, which is quadratic to the image size. Later, works like TNT \cite{han_transformer_2021} and Swin Transformer \cite{liu2021swin} were proposed to better adapt transformers to visual data. Recently, ViT-G/14 \cite{zhai2022scaling}, SwinV2-G \cite{liu_swin_2022} and ViT-22B \cite{Dehghani2023ScalingVT} substantially scaled the vision transformers up using a bag of training tricks to achieve state-of-the-art (SOTA) accuracy on various benchmarks. While ViTs may seem to gain more momentum than CNNs in developing LVMs, to improve CNNs, the latest works such as ConvNeXt \cite{liu2022convnet} and InternImage \cite{wang2023internimage} redesigned CNN architecture with inspirations from ViTs and achieved SOTA accuracy on ImageNet. This refutes the previous statement that CNNs are inferior to ViTs. Apart from the above, recent works like CoAtNet \cite{dai2021coatnet} and ConViT \cite{d2021convit} merge CNNs and ViTs to form new hybrid architectures.
Note that ViT-22B is the largest vision model to date, whose scale is significantly larger than that (1.08B) of the current art of CNNs (InternImage) but is still much behind that of the contemporary LLMs.


Architecturally speaking, LVMs are largely-scaled-up variants of their base architectures. How they are scaled up can significantly impact the final performance. Simply increasing the depth by repeating layers vertically may be suboptimal \cite{kolesnikov2020big}, so a line of studies \cite{tan2019efficientnet, tan2021efficientnetv2, wang2023internimage} investigate the rules for effective scaling. Furthermore, scaling the model size up is usually combined with larger-scale pre-training \cite{kolesnikov2020big, goyal2021self} and efficient parallelism \cite{huang2019gpipe} for improved performance.

LVMs also transform other fundamental computer vision tasks beyond classification. The latest breakthrough in segmentation task is SAM \cite{kirillov2023segment}. SAM is built with a ViT-H image encoder (632M), a prompt encoder and a transformer-based mask decoder that predicts object masks from the output of the above two encoders.
Prompts can be points or bounding boxes in images or text. 
SAM demonstrates a remarkable zero-shot generalization ability to segment unseen objects and images. Furthermore, to train SAM, a largest segmentation dataset to date, SA-1B, with over 1B masks is constructed.








\subsection{Large Multi-modal Models} \label{sec: background lmm}


This section describes large multi-modal models (LMMs). While the primary focus is on one type of LMMs: large vision-language models (LVLMs), multi-modality beyond vision and language is also summarized in the end.





Training LVLMs like CLIP \cite{radford2021learning} requires more than hundreds of millions of image-text pairs. Such large amount of data was often closed source \cite{radford2021learning, jia_scaling_2021, yuan_florence_2021}. Until recently, LAION-5B \cite{schuhmann_laion-5b_2022} was created with 5.85B data samples, matching the size of the largest private dataset while being available to the public.





LVLMs usually adopt a dual-stream architecture: input text and image are processed separately by their respective encoders to extract features. For representation learning, the features from different modalities are then aligned through contrastive learning \cite{radford2021learning, jia_scaling_2021, yuan_florence_2021} or fused into a unified representation through another encoder on the top of all extracted features \cite{singh_flava_2022, wang_simvlm_2022, chen2022pali, li_blip-2_2023, huang_language_2023}. Typically, the entire model, including unimodal encoder and multi-modal encoder if have, is pre-trained on the aforementioned large-scale image-text datasets, and fine-tuned on the downstream tasks or to carry out zero-shot tasks without fine-tuning. The pre-training objectives can be multi-modal tasks only or with unimodal tasks (see Section~\ref{sec: background lvms}). Common multi-modal pre-training tasks contain 
image-text contrastive learning \cite{radford2021learning, jia_scaling_2021, yuan_florence_2021}, image-text matching \cite{singh_flava_2022, li_blip_2022, li_blip-2_2023}, autoregressive modeling \cite{huang_language_2023, wang_simvlm_2022}, masked modeling \cite{singh_flava_2022}, image-grounded text generation \cite{li_blip_2022, li_blip-2_2023}, etc. Recent studies suggest that scaling the unimodal encoders up \cite{pham2021combined, chen2022pali} and pre-training with multiple objectives across uni- and multi-modalities \cite{li_blip_2022, li_blip-2_2023} can substantially benefit multi-modal representation learning.  





Recently, LVLMs made a major breakthrough in text-to-image generation. There are generally two classes of methods for such a task: autoregressive model \cite{ramesh_zero-shot_2021, yuscaling, ramesh2022hierarchical} and diffusion model \cite{rombach2021highresolution, ramesh2022hierarchical, nichol2022glide, saharia2022photorealistic}. Autoregressive model, like introduced in Section~\ref{sec: background lvms}, first concatenates the tokens (returned by some encoders) of text and images together and then learns a model to predict the next item in the sequence. In contrast, diffusion model first perturbs an image with random noise progressively until the image becomes complete noise (forward diffusion process) and then learns a model to gradually denoise the completely noisy image to restore the original image (reverse diffusion process) \cite{sohl2015deep}. Text description is first encoded by a separate encoder and then integrated into the reverse diffusion process as the input to the model so that the image generation can be conditioned on the text prompt. It is common to reuse those pre-defined LLM and LVM architectures and/or their pre-trained parameters as the aforementioned encoders. The scale of these encoders and the generator can significantly impact the quality of generation and the ability of language understanding \cite{yuscaling, saharia2022photorealistic}. 


The paradigm of bridging language and vision modalities can be beyond learning, e.g., using LLM to instruct other LVMs to perform vision-language tasks \cite{wu2023visual}. Beyond vision and language, recent development in LMMs seeks to unify more modalities under one single framework, e.g., ImageBind~\cite{girdhar2023imagebind} combines six whereas Meta-Transformer~\cite{zhang2023meta} unifies twelve modalities.























