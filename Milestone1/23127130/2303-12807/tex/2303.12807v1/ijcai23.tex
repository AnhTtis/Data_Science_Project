%%%% ijcai23.tex

\typeout{IJCAI--23 Instructions for Authors}

% These are the instructions for authors for IJCAI-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai23.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{float} 
%\usepackage{sub}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{nameref,hyperref}
\usepackage{arydshln}
\usepackage{amsmath,amsfonts,amssymb,amsthm,version}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
	/TemplateVersion (IJCAI.2023.0)
}

\title{Granular-ball Optimization Algorithm}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
	Shuyin Xia$^1$
	\and
	Jiancu Chen$^{1,2}$\and
	Bin Hou$^{1}$\And
	Guoyin Wang$^1$
	\affiliations
	$^1$Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China\\
	$^2$College of Computer Science and Engineering, Chongqing Three Gorges University, Chongqing, China
	\emails
	xiasy@cqupt.edu.cn
}
%\fi

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		The existing intelligent optimization algorithms are designed based on the finest granularity, i.e., a point. This leads to weak global search ability and inefficiency. To address this problem, we proposed a novel multi-granularity optimization algorithm, namely granular-ball optimization algorithm (GBO), by introducing granular-ball computing. GBO uses many granular-balls to cover the solution space. Quite a lot of small and fine-grained granular-balls are used to depict the important parts, and a little number of large and coarse-grained granular-balls are used to depict the inessential parts. Fine multi-granularity data description ability results in a higher global search capability and faster convergence speed. In comparison with the most popular and state-of-the-art algorithms, the experiments on twenty benchmark functions demonstrate its better performance. The faster speed, higher approximation ability of optimal solution, no hyper-parameters, and simpler design of GBO make it an all-around replacement of most of the existing popular intelligent optimization algorithms.
	\end{abstract}
	
	\section{Introduction}
	In the field of scientific research and engineering, we often need to calculate the global optimal solution of functions, including unimodal and multi-modal functions. \cite{hussien2020comprehensive,rashedi2018comprehensive,huband2006review,weile1997genetic}. Multi-modal functions are more difficult, it is easy to fall into local optimal solution and the calculation cost is high. To solve these problems, researchers have put forward a lot of solutions. 
	
	In order to avoid falling into a local optimal solution as much as possible, Bergh et al. designed a cooperative particle swarm optimization (CPSO) \cite{van2004cooperative}, which uses multiple populations to work together and shares information. Yang et al. \cite{yang2016adaptive} proposed an adaptive continuous ant colony optimization algorithm, which uses a local search strategy based on Gaussian distribution. It provides a good balance between the global and local, and will not fall into the local optimal solution prematurely. Zhang et al. proposed an adaptive particle swarm optimization (IDPSO) \cite{zhang2013improved}, which dynamically set the evolution direction of each particle by adjusting three parameters. Zhang et al. \cite{zhang2020dynamic} proposed a particle swarm optimization algorithm based on dynamic neighborhood. It use a dynamic neighborhood selection mechanism to determine the position update direction of particles. It is better balance the relationship between global exploration and local exploitation. Wang et al. \cite{wang2013particle} used an adaptive mutation strategy to help the captured particles escape from the local optimal solutions, which can improve the mutation performance of PSO effectively. Qu et al.\cite{qu2012niching} designed a new local search technique, which improves the probability of finding the global optimal solution.
	
	To reduce the computational cost, Song et al. improved the three components of individual selection, population diversity, and adaptive parameters in the particle swarm optimization algorithm \cite{song2019adaptive}. Different particles performs different search tasks, and they shares information. Zhou et al. proposed a bee algorithm \cite{zhou2016bees} used a local search method to speed up the local search. It showed faster computational speed and higher computational accuracy in computing multi-modal functions. Cheng et al. \cite{cheng2017evolutionary} designed an adaptive peak detection method. First, they find out the peaks where the optimal solution may exist. Then, they do a fine search in this region and a coarse search in other regions. The method can reduce unnecessary computations in some regions. Li et al.\cite{li2020multimodal} proposed a multi-modal whale optimization algorithm. They used localization and Gaussian sampling to improve searchability. Li et al. \cite{li2017loser} designed a competition-based fireworks algorithm. It uses competition as an interactive way and compares fireworks according to their current status and progress, continuously eliminating the losers. This method greatly improves the computational efficiency of multi-modal functions.	
	
	Our world is displayed in a multi-granularity way. From the perspective of granular computing or granular computing \cite{xia2019granular,wang2017dgcc}, the existing intelligent optimization algorithms are optimized based on the most fine-grained points, so they do not have the efficiency and global characterization ability of multi-granularity. To this end, we propose a novel optimization algorithm inspired by granular-ball computing. The main contributions of the paper are as follows:
	
	\begin{itemize}
		\item We propose a multi-granularity optimization algorithm GBO which can simulate the multi-granularity distribution of data. Different from the existing intelligence optimization algorithms, its optimization process is based on granular-balls with different sizes, rather than sampling points. This leads to better convergence performance than other algorithms.
		\item In GBO, the unimportant part of the solution space is covered by a small amount of coarse-grained granular-balls, which makes GBO avoid a large number of search calculations. So, it has a faster convergence speed than the existing intelligent optimization algorithms.  
		\item In GBO, the important part of the solution space is covered by a large amount of fine-grained granular-balls, which makes GBO have better search ability of the optimal solution than the existing intelligent optimization algorithms.
		\item GBO contains no hyper-parameters. So, GBO is completely adaptive, which is a characteristics that the existing intelligence optimization algorithms do not have. Not relying on any hyper-parameters also lead to higher stability than the existing algorithms.		
		\item GBO is very simple, and easy to be implemented.

	\end{itemize}
	
	The rest of this paper is organized as follows: we introduce related works in Section \ref{sec:related work}. Section \ref{sec:Method} introduces the structure and details of granular-ball optimization algorithm. Experimental results and analysis are provided in Section \ref{sec:Experimental Settings}. We conclude the whole work and discuss the future work in Section \ref{sec:conclusions and future work}. 
	
	\section{Related Work about Granular-ball Computing}\label{sec:related work}
	In 1982, Chen \cite{chen1982topological} pointed out that the brain gives priority to recognizing a ``wide range'' of contour information in image recognition, and human cognition has the characteristics of ``global precedence''. It is different from the major existing artificial intelligence algorithms, which take the most fine-grained points as input. The human brain's global precedence cognition is efficient and robust, and is very beneficial for improving the performance of the existing artificial intelligence algorithms. Wang \cite{wang2017dgcc} first introduced the large-scale cognitive rule into granular computing and proposed multi-granular cognitive computing. Xia and Wang \cite{xia2020fast,xia2022gbsvm,xia2022efficient} further used hyperspheres of different sizes to represent "grains", and proposed granular-ball computing, in which a large granular-ball represents coarse granularity, while a small granular-ball represents fine granularity. Granular-ball computing was initially used to deal with classification problems successfully. A schematic diagram of appling granular-ball computing into classification problems is shown in Fig. \ref{splitofgranulation}.
	
	\begin{figure}[!ht]
		\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\centering
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{gb1.jpg}
			\caption{}
			\label{gb1}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{gb2.jpg}
			\caption{}
			\label{gb2}
		\end{subfigure}
		\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{gb3.jpg}
			\caption{}
			\label{gb3}
		\end{subfigure}
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{gb4.jpg}
			\caption{}
			\label{gb4}
		\end{subfigure}
		\caption{The granular-ball splitting generation process. $A_1$, $A_2$, $B_1$, and $B_2$ are noise points.}
		\label{splitofgranulation}
	\end{figure}
	
	As shown in Fig. \ref{splitofgranulation}, the granular-ball in Fig. \ref{splitofgranulation}(\subref{gb1}) is the coarsest one. At this time, its quality of is not high and continues to split into two fine granular-balls(as shown in Fig. \ref{splitofgranulation}(\subref{gb2})). At this point, the quality of the granular-ball still cannot meet the requirements, and it will continue to split until the splitting process converges (as shown in Fig. \ref{splitofgranulation}(\subref{gb3}). Fig. \ref{splitofgranulation}(\subref{gb4}) is the result after the data is transformed into granular-balls, and the blue is the decision curve obtained after the training and learning of granular-balls.	Since the four fine-grained noise points in Fig. \ref{splitofgranulation}(\subref{gb3}) do not affect the label of the coarse-grained granular-ball, the learning process based on granular-ball is robust. As the number of granular-balls in Fig. \ref{splitofgranulation}(\subref{gb4}) is far less than the number of samples in Fig. \ref{splitofgranulation}(\subref{gb1}), the learning based on granular-ball is efficient.	The generation model of granular-ball is as follows.
	
	Given a data set $D = {x_i(i=1, 2,\dots, n)}$, where $n$ is the number of samples on $D$. Granular balls $GB_1, GB_2,\dots, GB_m$ are used to cover and represent the data set $D$. Suppose the number of samples in the $j^{th}$ granular-ball $GB_j$ is expressed as $|GB_j|$, then its coverage degree can be expressed as $ {\textstyle \sum_{i=1}^{m}}\left ( \left | GB_j \right |  \right ) /n $. The basic model of granular-ball coverage can be expressed as
	\begin{equation}\label{eqGB}%\Vert\Vert
		\setlength{\abovedisplayskip}{6pt}
		\setlength{\belowdisplayskip}{3pt}
		\begin{split}
			min \ \ \lambda _1 \ast n /{\sum_{j=1}^{m}}\left ( \left | GB_j \right |  \right ) /n + \lambda _2 \ast m,  \\
			s.t. \ \ quality(GB_j) \ge T, \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
		\end{split}
	\end{equation}
	where $\lambda _1$ and $\lambda _2$ are the corresponding weight coefficients, and $m$ the number of granular balls. When other factors remain unchanged, the higher the coverage, the less the sample information is lost, and the more the number of granular-balls, the the characterization is more accurate. Therefore, the minimum number of granular-balls should be considered to obtain the maximum coverage degree when generating granular-balls. By adjusting the parameters $\lambda _1$ and $\lambda _2$, the optimal granular-ball generation results can be obtained to minimize the value of the whole equation. In most cases, the two items in the objective function do not affect each other and do not need trade off, so $\lambda _1$ and $\lambda _2$ are set to 1 by default. Granular-ball computing can fit arbitrarily distributed data \cite{xia2019granular,xia2022gbsvm}.
	
	Shuyin Xia and Guoyin Wang et al. proposed the GBSVM method \cite{xia2019granular,xia2022gbsvm}. There is no the finest granularity point $x_i$ in GBSVM, and $x_i$ is replaced with a granular-ball with a center $c_i$ and radius $r_i$. A large $r_i$ represents a coarse granularity, and a smaller value represents a  fine granularity. GBSVM is the first multi-granularity classifier model that is not based on point-input (i.e., containing $x_i$), and exhibits better efficiency and robustness than the traditional classifier at the same time. Granulular-ball computing is also applied into many other learning methods to improve their generalizability or efficiency, such as rough sets \cite{shuyin2022gbrs}, sampling for classification \cite{xia2021granular}, fuzzy sets \cite{xia2022fuzzy}, and so on. In this paper, we want to propose a multi-granularity optimization algorithm based on the idea of granular computing, and make it have the multi-granularity global optimization ability and high performance in efficiency.
	
	
	\section{Method}\label{sec:Method}
	\subsection{Motivation}\label{sec:Motivation}	
	The comparison between traditional intelligent optimization algorithms and GBO is shown in Fig. \ref{motivationPic}. As shown in Fig. \ref{motivationPic}(\subref{motivation1}), the existing intelligent optimization algorithms are designed based on the finest granularity, i.e., point samples. By continuously generating new samples, the objective function value of the optimal sample is constantly close to the optimal solution. In the minimum optimization problem, the value of the minimum sample function is decreasing. The whole process is based on the calculation of the most fine-grained samples. In contrast, granular-ball optimization is based on coarse-grained and multi-grained. The same number of granular-balls can better describe and cover the solution space. Therefore, in comparison with those traditional optimization algorithms based on the most fine-grained sample, it can obtain higher convergence speed and approximation ability of the optimal solution.
	 
	\begin{figure}[H]
		\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\begin{subfigure}{0.45\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{mov01.jpg}
			\caption{}
			\label{motivation1}
		\end{subfigure}
		\qquad
		\begin{subfigure}{0.45\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{mov02.jpg}
			\caption{}
			\label{motivation2}
		\end{subfigure}
		\caption{Comparison between traditional intelligent optimization algorithms and GBO in a minimum optimization problem. (\subref{motivation1}) $X_i$ and $f(X_i), i=1, 2, \dots, n$ denote a sampled point and its function value. (\subref{motivation2}) GBO based on coarse granularity. $GB_j$ and $\hat{f}(GB_j), j=1, 2,\dots, m$ denote a granular-ball and its function value.}
		\label{motivationPic}
	\end{figure}

	In order to simulate the global precedence characteristics of human cognition at the beginning of the algorithm. As the process decribed in Fig. \ref{splitofgranulation}. First, we use a granular-ball to cover the whole solution space of the function. Then, with the idea from coarse-grained to fine-grained, the granular-balls are split. In granular-ball computing, the larger the granular-ball, the higher the efficiency, but it is more likely to lead to the neglect of details and the loss of precision. The smaller the granular-ball, the more times repeated calculation will reduce the efficiency. Therefore, the whole process involves three research issues including how to represents a granular-ball, how to measure the quality of each granular-ball, and how to design the splitting strategy of granular-ball.
	
	\subsection{GBO algorithm Framework}
	Aiming at dealing with three above problems involved in GBO, we firstly designed the quality evaluation method of a granular-ball.
	
	\subsubsection{Quality Evaluation of a Granular-ball}
	Our goal is to search the global minimum of the function, so the smaller the objective function value of a granular-ball, the higher its quality. Assuming that the center of the granular-ball is the origin of the coordinate axis, and the intersection points between the granular-ball and the coordinate axis are boundary points. A granular-ball can be represented using its boundary points \cite{xia2021granular}, which can be expressed in a parsed way. The optimal value of a granular-ball is the minimum function value of all the boundary points. The objective function value of a granular-ball is defined as follows.
	
	\begin{definition}
		Given a granular-ball $GB$, its radius and center are $r$ and $c=[c_1, c_2, \dots, c_d]$, respectively, where $c_i$ represents the value on the $i^{th}$ dimension. Then, the boundary points on the $i^{th}$ dimension, $BP_i(GB)$ is defined as:
		\begin{equation}\label{eqpoint}%\Vert\Vert
		\begin{split}
		\setlength{\abovedisplayskip}{6pt}
		\setlength{\belowdisplayskip}{3pt}
		BP_i(GB) = [c_1, c_2, \dots, c_{i}+r, \dots, c_d] \\
		\cup [c_1, c_2, \dots, c_{i}-r, \dots, c_d].
		\end{split}
		\end{equation}
		The objective function value of $GB$, $\hat{f}(GB)$, can be expressed as:
		\begin{equation}\label{equality}%\Vert\Vert
		\begin{split}
		\setlength{\abovedisplayskip}{6pt}
		\setlength{\belowdisplayskip}{3pt}
		\hat{f}(GB) = min(f(bp)), bp \in BP_i(GB), i=1,2,\dots,d.
		\end{split} 
		\end{equation}
		\label{defgb}
		\vspace{-0.3cm}
	\end{definition}
	
	According to (\ref{eqpoint}), for a granular-ball $GB$, the center of $GB$ is used as the origin and moves distance $r$ along the coordinate axis on the $i^{th}$ dimension; as the direction of movement includes positive and negative, two boundary points are generated on the dimension. According to (\ref{equality}), $\hat{f}(GB)$ is equal to the minimum function value of its boundary points. As the minimum optimization problem is dealt with in this paper, the reciprocal of the objective function value of a granular-ball can be used as its quality. The smaller the objective function value of a granular-ball, the higher its quality. If the maximum optimization problem is dealt with, the quality of a granular-ball can be directly set to its objective function value. 
	
	\subsubsection{Granular-ball Splitting Strategy}
	
	Like Fig. \ref{splitofgranulation}, the splitting process is shown in Fig. \ref{splitflow}. At the beginning of GBO, it generates an initial granular-ball to fully cover the solution space, and performs the first round of granular-ball splitting. Given a $d$-dimension object function $f(x_1, x_2, \dots, x_d), x_k = [-a_k,a_k], k=1, 2, \dots, d$, the radius of the initial granular-ball is equal to $R_0=\sqrt[k]{a_1^2+a_2^2+\dots+a_k^2}$. In the whole splitting process, for a granular-ball $GB$ with its center $c$ and radius $r$, its splitting is in its boundary points. In order to cover the solution space of $GB$ with some smallest balls as possible, its sub-ball is defined in Def. (\ref{def1}). According to Def. (\ref{def1}), the radius of its $j^{th}$ sub-ball $r_j$ is equal to $0.5*r$. Besides, the center point of a sub-ball $c_j$ is set as the midpoint of the line between the $j^{th}$ boundary point in $BP_j(GB)$ and the center point of $GB$, i.e., $0.5*(c+p),p \in BP_j(GB)$. As the number of its boundary points is equal to $2d$, $GB$ will be split into $2d$ sub-balls. Then, by comparing the quality of each sub-ball and the granular-ball, to decide whether to continue splitting. If the quality of a sub-ball is worse than $GB$, the sub-ball will stop splitting; in contrary, the sub-ball will be split. The splitting process will be converged until each sub-ball stop splitting.
	\begin{figure}[bpht!]
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\centering
		\centering
		\includegraphics[width=3in]{splitflow.jpg}
		\caption{Flow chart of single round splitting of granular-ball.}
		\label{splitflow}
	\end{figure}
	
	
	\begin{definition}
		Given a granlar-ball $GB$ with its center as $c$ and radius as $r$ respectively. A granular-ball with its center as $sc$ and radius as $sr$ is called a sub-ball of $GB$ if:
		\begin{equation}\label{eq1uality}%\Vert\Vert
		sc = 1/2*(c+bp), bp \in BP_i(GB), i=1,2,\dots,d
		\end{equation}
		\begin{equation}
		sr = 1/2*r.
		\end{equation}
		\label{def1}
		\vspace{-0.3cm}
	\end{definition}
	
	Take the function of one variable $y=x^2+2x-1, x \in [-3,3]$ as an example.	First,  as shown in Fig. \ref{thoughtofgranulation}(\subref{fig3a}), at the beginning of the algorithm, a granular-ball $A$ is used to cover the whole solution space and its function value is calculated according to (\ref{eq1uality}). Then, as shown in Fig. \ref{thoughtofgranulation}(\subref{fig3b}), we split the initial granular-ball into four sub-balls, $A_1$, $A_2$, $A_3$, and $A_4$, in its four boundary points and calculate their function values respectively according to (\ref{eq1uality}). As only $\hat{f}(A_3)$ is not larger than $\hat{f}(A)$, $A_3$ is continuously split in Fig. \ref{thoughtofgranulation}(\subref{fig3c}). As only $\hat{f}(A_{32})$ is not larger than $\hat{f}(A_3)$, $A_{32}$ is continuously split in Fig. \ref{thoughtofgranulation}(\subref{fig3d}). As $\hat{f}(A_{31})$, $\hat{f}(A_{33})$, and $\hat{f}(A_{34})$ are all larger than $\hat{f}(A_3)$, GBO converges and all granular balls stop splitting. At this time, it can be observed that many small granular-balls are distributed at the minimum value of the function curve, and the objective function values of those balls can describe the minimum value of the function well.
	
	\begin{figure}[ht]
		\centering
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{fig3_a.png}
			\caption{}
			\label{fig3a}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{fig3_b.png}
			\caption{}
			\label{fig3b}
		\end{subfigure}
		\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{fig3_c.png}
			\caption{}
			\label{fig3c}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1\linewidth]{fig3_d.png}
			\caption{}
			\label{fig3d}
		\end{subfigure}
		\caption{The diagram of granular-ball splitting. (\subref{fig3a}) The initial granular-ball $A$. (\subref{fig3b}) The result after $A$ is split. (\subref{fig3c}) The result after $A_3$ is split. (\subref{fig3d}) The result after $A_{32}$ is split.}
		\label{thoughtofgranulation}
	\end{figure}
	
	GBO uses coarse-grained granular-balls instead of fine-grained sample points to search in the solution space, so the optimization process is more efficient than the existing methods. Besides, this coarse-to-fine splitting process covers the entire solution space, and can describe the overall multi-granularity distribution characteristics of data. As shown in Fig. \ref{thoughtofgranulation}(\subref{fig3d}), the unimportant solution space is represented by a small number of coarse granular-balls, while the important solution space is represented by a large number of fine granular-balls. This multi-granularity description ability enables GBO to obtain better solutions than the existing methods. In addition, the whole process has no parameters. To sum up, the multi-granularity optimization process makes GBO efficient, accurate and adaptive. This will be further demonstrated in the experimental results. The GBO algorithm is provided in Algorithm \ref{algorithm1}.

	\begin{algorithm}[h]
		\caption{Granular-ball Optimization Algorithm.}
		\label{algorithm1}
		\textbf{Input}: The objective function $f(x_1, x_2, \dots, x_d)$, $x_{i} \in [-a_i, a_i] $;\\
		\textbf{Output}: The minimum function value $f^*$ and the corresponding boundary point $bp^*$;
		\begin{algorithmic}[1] %[1] enables line numbers
			\STATE $CurrentSet$ =\text{\O}; $SubSet$ = \text{\O}; 
			\STATE Generate initial granular-ball $GB$ to cover the solution space, and $CurrentSet$ = $CurrentSet \cup GB$;
			\STATE $f^*$ = $\hat{f}(GB)$; ${bp}^*$= the boundary point that generates $\hat{f}(GB)$;
			\FOR{each granular-ball $GB_i$ in $CurrentSet$}
			\IF {$\hat{f}(GB_i)<f^*$}
			\STATE $f^*=\hat{f}(GB_i)$; ${bp}^*$= the corresponding boundary point that generates $\hat{f}(GB)$;
			\ENDIF
			\STATE Split $GB_i$ and its sub-balls are denoted $GB_iSub$;
			\FOR{each granular-ball $GB_iSub_j$ in $GB_iSub$}
			\IF {$\hat{f}(GB_iSub_j) < \hat{f}(GB_i)$}
			\STATE $SubSet$ = $SubSet \cup GB_iSub_j$;
			\ENDIF
			\ENDFOR
			\ENDFOR
			\IF {$SubSet$ $\neq$ \text{\O} }
			\STATE $CurrentSet = SubSet$; $SubSet = \text{\O}$; goto Step 4;
			\ELSE
			\STATE return $f^*$ and $bp^*$;
			\ENDIF			
		\end{algorithmic}
	\end{algorithm}
	
	\subsubsection{Improved Quality Evaluation of a Granular-ball}
	In Def. (\ref{eqpoint}), the quality of a granular-ball is determined by its boundary points. However, in this way, the ability to describe the solution space inside the granular-ball may not be good enough. Therefore, more sub-balls are used to improve the quality of a granular-ball, the improved objective function value of a granular-ball is defined as follows.
	
	\begin{table*}[htbp!]
		\setlength{\abovecaptionskip}{0.1cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\tiny
		\centering
		\caption{The twenty benchmark functions used in our experiment. Opt represents the theoretical global minimum of the function.}
		\begin{tabular}{lp{8cm}p{2.6cm}p{0.7cm}p{1cm}}
			\hline
			%\toprule[1pt]
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%\textwidth 是每一行的宽度.[0.1\textwidth]设定单元格宽度
			% [c]  单元格文本居中对齐
			% {name} 单元格内容
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\makebox[0.2\textwidth][l]{\textbf{Function Name}} & \makebox[0.2\textwidth][l]{\textbf{Function Expression}} & \makebox[0.2\textwidth][l]{\textbf{Define Fields}} & \makebox[0.01\textwidth][l]{\textbf{Opt}}              \\
			%\midrule[0.6pt]
			\hline
			Sphere Model          & $f_{1}=\sum_{i=1}^{n}x_{i}^{2}$ & $x_i \in [-100, 100]$   &  0   \\
			Schwefel's Problem 1.2 & $f_{2}= {\textstyle \sum_{i=1}^{n}} ( {\textstyle \sum_{j=1}^{i}}x_j)^2$ & $x_i \in [-100, 100]$   &0\\
			Generalized Rosenbrock’s Function & $f_{3}= {\textstyle \sum_{i=1}^{n-1}} [100(x_{i+1}-x_i^2)^2+(x_i-1)^2]$ & $x_i \in [-30, 30]$     &0\\
			Quartic Function i.e. Noise& $f_{4}= {\textstyle \sum_{i=1}^{n}} ix_i^4+random[0,1)$ & $x_i \in [-1.28, 1.28]$ &0\\
			Drop-Wave Function     &  $f_5=-\frac{1+\cos ( 12\sqrt{x_{1}^{2} + x_{2}^{2} } ) }{0.5 ( x_{1}^{2} + x_{2}^{2} ) +2}$ & $x_i \in [-5.12, 5.12]$ & -1 \\
			Levy Function N. 13   &  $f_6=\sin^2 (3\pi x_1)+(x_1-1)^2\left [ 1+sin^2(3\pi x_2) \right ]+(x_2-1)^2 [1+ sin^2(2\pi x_2) ]$ & $x_1 \in [-10, 10]$ & 0 \\
			Matyas Function       & $f_{7}=0.26(x_{1}^{2}+x_{2}^{2})-0.48x_1x_2$ &$x_i \in [-10, 10]$ & 0 \\
			Three-Hump Camel Function& $f_{8}=2x_{1}^{2} -1.05x_{1}^{4}+\frac{x_{1}^{6}}{6} +x_1x_2+x_{2}^{2}$ &$x_i \in [-10, 10]$ & 0 \\
			Goldstein-Price Function& $f_{9}=[ 1+(x_1+x_2+1)^2(19-14x_1+3x_1^2-14x_2+6x_1x_2+3x_2^2) ] \times [ 30+(2x_1-3x_2)^2(18-32x_1+12x_1^2+48x_2-36x_1x_2+27x_2^2) ] $ &$x_i \in [-2, 2]$ & 3 \\
			Schaffer Function N. 2& $f_{10}=0.5+\frac{\sin^2(x_1^2-x_2^2)-0.5}{[1+0.001(x_1^2+x_2^2)]^2} $ &$x_i \in [-100, 100]$ & 0 \\
			Generalized Rastrigin's Function & $f_{11}=\sum_{i=1}^{30} \left [ x_{i}^{2}-10\cos (2\pi x_i) +10 \right ] $ &$x_i \in [-5.12,5.12]$ & 0 \\
			Easom Function        & $f_{12}=-\cos(x_1)\cos(x_2)\exp(-(x_1-\pi)^2-(x_2-\pi)^2) $ &$x_i \in [-100, 100]$ & -1 \\
			Sum of Different Powers Function& $f_{13}= \sum_{i=1}^{d}|x_i|^{i+1}$ &$x_i \in [-1,1]$ & 0 \\
			Rastrigin Function    & $f_{14}=10d+\sum_{i=1}^{d}[x_i^2-10\cos(2\pi x_i)]  $ &$x_i \in [-5.12, 5.12]$ & 0 \\
			Sum Squares Function  & $f_{15}=\sum_{i=1}^{d} ix_{i}^{2} $ &$x_i \in [-10, 10]$ & 0 \\
			Generalized Griewank's Function & $f_{16}=\frac{1}{4000} \sum_{i=1}^{30} x_{i}^{2} -\prod_{i=1}^{30}\cos (\frac{x_i}{\sqrt{i} } )+1 $ &$x_i \in [-600,600] $ & 0 \\
			Rotated Hyper-Ellipsoid Function & $f_{17}=\sum_{i=1}^{d}\sum_{j=1}^{i}  x_{j}^{2} $ &$x_i \in [-65.536, 65.536]$ & 0 \\
			Bohachevsky Function1 & $f_{18}=x_{1}^{2} +2x_{2}^{2} -0.3\cos(3\pi x_1)-0.4\cos(4\pi x_2)+0.7$ &$x_i \in [-100, 100]$ & 0 \\
			Bohachevsky Function2 & $f_{19}= x_{1}^{2}+ 2x_{2}^{2}-0.3\cos (3\pi x_1)\cos (4\pi x_2)+0.3$ &$x_i \in [-100,100]$ & 0 \\
			Bohachevsky Function3 & $f_{20}= x_{1}^{2}+ 2x_{2}^{2}-0.3\cos (3\pi x_1 +4 \pi x_2)+0.3$ &$x_i \in [-100,100]$ & 0 \\
			%\bottomrule[1pt]
			\hline
		\end{tabular}
		\textbf{\label{tab1}}
	\end{table*}

	\begin{definition}
		\label{def3}
		Given a granular-ball $GB$ with its radius and center as $r$ and $c$. A granular-ball $GB^{'}$ with its radius $r'$ and $c'$ is called the concentric sub-ball of $GB$ if $r'<r$ and $c'=c$. 
		The concentric sub-balls of $GB$ contains $GB_1^{'}, GB_2^{'}, \dots, GB_p^{'}$, where $p$ denotes the number of the sub-balls. The improved objective function value of $GB$, $\tilde{f}(GB)$, can be expressed as:
		\vspace{-0.1cm}
		\begin{equation}\label{}%\Vert\Vert
		\setlength{\abovedisplayskip}{6pt}
		\setlength{\belowdisplayskip}{3pt}
		\tilde{f}(GB) = min(\hat{f}(GB_i^{'})), i=1,2,\dots,p.
		\end{equation}
		\vspace{-0.3cm}
	\end{definition}

    In Def. \ref{def3}, for a granular-ball, its concentric sub-balls can be used to improve the description of the solution space in the granular-ball. For a granular-ball with center as $c$ and radius as $r$, its concentric sub-balls can be generated in the following ways: their centers are set to $c$, and their radii are generated by increasing from 0 to $r$ in a certain step. So, the core problem is how to set the step size. An intuitive way, as shown in  Fig. \ref{variable-lengthradius}(\subref{fig5a}), is to set the step length to a fixed value, 1. However, this method will cause a lot of repeated calculations.

	\begin{figure}[!ht]
		\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\begin{subfigure}{0.49\linewidth}
			\raggedleft
			\includegraphics[width=1\linewidth]{fig5_a.png}
			\caption{}
			\label{fig5a}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\raggedleft
			\includegraphics[width=1\linewidth]{fig5_b.png}
			\caption{}
			\label{fig5b}
		\end{subfigure}
		\caption{(\subref{fig5a}) Schematic diagram of granular-ball splitting with fixed step size of $1$. (\subref{fig5b}) Schematic diagram of granular-ball splitting based on primes. All points represent the boundary points of those sub-balls generated in multiple rounds splitting. All the computations of the objective function are in these points. Repeated calculation occur in those red points, and not in the green points.}
		\label{variable-lengthradius}
	\end{figure}

	In Fig. \ref{variable-lengthradius}(\subref{fig5a}), all calculated points in $r=2$ will be re-computed in $r=8$. Similarly, all points of $r=8$ will be repeated in $r=16$. In order to reduce double counting, we design a radius generation method based on prime numbers. In this method, for a granular-ball with radius as $R$, all primes of the range $(0, R)$ are respectively selected as the radii of its concentric granular-balls. As a prime is divisible only by $1$ and itself, there is no double computing; as shown in (\subref{fig5b}), there are no red points. 
	
	\section{Experiments} \label{sec:Experimental Settings}
	
	We evaluate the performance of the algorithms on twenty benchmark functions in comparison with those most widely used and the-state-of-the-art evolutionary algorithms, including PSO\cite{kennedy1995particle},DE\cite{storn1996differential}, AFSA\cite{li2003new}, GA\cite{holland1992genetic}, SA\cite{van1987simulated} and FWA\cite{li2017loser}. Tab. \ref{tab1} lists the basic information of the twenty benchmark functions including function name, function expression, define fields, and optimal value. A more detailed description for each function is provided in Appendix. $f_1$-$f_4$ are unimodal functions; $f_4$ is a noisy quartic function, and the variable $random$ in $f_4$ is uniformly distributed in $(0,1)$. $f_5$-$f_{20}$ are multi-modal functions. The hardware environment of the experiments is a PC with an Intel Core i7-6700HQ CPU @3.4GHz with 8G RAM; the experimental software is Python 3.9.All the results are conducted under the same conditions. To reduce the influence of the randomness of the bionic algorithm, we run ten times for each function and take the average, and the best results are displayed in bold.
	
	\subsection{Parameter Setting}
	The parameters of the comparison algorithms used in the experiments are shown in Tab. \ref{tab2}. All parameters of the comparison algorithms are set to default values. It should be noted that GBO does not contain any hyper-parameters. This leads to better performance of GBO in stability and adaptiveness in comparison with other algorithms.  
	\begin{table}[H]
		\setlength{\abovecaptionskip}{0.1cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\centering
		\tiny
		\caption{Parameter information. N denotes the number of parameters.}
		\begin{tabular}{lll}
			\hline
			\textbf{Algorithms} & \textbf{N}  & \textbf{Parameter items}                     \\ \hline
			GBO		  & 0			 & \textbf{-}                        \\
			PSO       & 7            & $size\_pop$, $max\_iter$, $w$, $c_1$, $c_2$, $lb$, $ub$ \\
			DE        & 6            & $size\_pop$, $max\_iter$, $prob\_mut$, $F$, $lb$, $ub$\\
			AFSA      & 7            & $size\_pop$, $max\_iter$, $max\_try\_num$, $step$, $visual$, $q$, $delta$\\
			GA        & 6            & $size\_pop$, $max\_iter$, $prob\_mut$, $lb$, $ub$, $precision$ \\
			SA        & 7            & $x_0$, $T\_max$, $T\_min$, $L$, $max\_stay\_counter$, $lb$, $ub$\\
			FWA       & 7            & $size\_pop$, $max\_iter$, $\lambda$, $\alpha$, $mu\_rate$, $c_a$, $c_r$\\ \hline
		\end{tabular}
		\label{tab2}
	\end{table}    
	
	\subsection{Effectiveness}
	In this section, the convergence error of the algorithms is compared, and the experimental results are shown in Tab. \ref {tab3}. The convergence error represents the difference between the optimization result and the theoretical optimal value. The smaller the error, the better the optimization result. 
	
	\begin{table}[!ht]
		\setlength{\abovecaptionskip}{0.1cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\tiny
		\centering
		\caption{Solution comparison between GBO and baselines on $f_1$–$f_{20}$. Min denotes the theoretical minimum.}
		\setlength{\tabcolsep}{0.8mm}{
			\begin{tabular}{lllllllll}
				\hline
				\textbf{Func}   & \textbf{Min}  & \textbf{GBO} & \textbf{PSO}      & \textbf{DE}       & \textbf{AFSA}     & \textbf{GA}		& \textbf{SA}     &\textbf{FWA} \\ \hline
				$f_1$   		&     0         & \textbf{0}    &   7.98E-14        &  9.32E-20         & 1.85E-09          &  4.31E-04         &  8.83E-09       & 6.20E-31 \\
				$f_2$   		&     0         & \textbf{0}    &   1.23E-13        &  1.91E-09         & 8.22E-10          &  1.76E-01         &  8.54E-09       & 6.42E-31 \\
				$f_{3}$ 		&     0         & \textbf{0}    &   4.09E+00        &  3.11E-02         & 5.60E-08          &  3.31E+00         & \textbf{0.00E+00}& 2.55E-30 \\
				$f_{4}$ 		&     0         & \textbf{2.30E-05} &   5.85E-04    &  4.08E-01         & 4.63E-03          &  6.12E-01         &  2.98E-04        & 3.70E-01 \\
				$f_5$    		&    -1         & \textbf{-1}   &   -9.94E-01       &  -9.99E-01        & -9.41E-01         &  -9.04E-01        &  -9.94E-01       & \textbf{-1.00E+00} \\
				$f_6$    		&     0         & \textbf{1.35E-31}&   9.86E-15     &  5.80E-22         & 3.16E-08          &  4.40E-02         & \textbf{1.35E-31}& 2.42E-31 \\
				$f_{7}$  		&     0         & \textbf{0}    &   3.27E-16        &  2.39E-09         & 1.18E-10          &  2.88E-02         &  2.26E-09        & 1.63E-33 \\
				$f_{8}$ 		&     0         & \textbf{0}    &   5.22E-16        &  2.07E-16         & 2.86E-09          &  8.98E-02         &  2.00E-09        & 2.05E-32 \\
				$f_{9}$ 		&     3         & \textbf{3}    & \textbf{3.00E+00} & \textbf{3.00E+00} & \textbf{3.00E+00} &  5.70E+00         & \textbf{3.00E+00}& \textbf{3.00E+00} \\
				$f_{10}$ 		&     0         & \textbf{0}    &   2.22E-17        &  1.76E-06         & 1.31E-12          &  8.37E-02         &  1.04E-09        & \textbf{0.00E+00} \\
				$f_{11}$ 		&     0         & \textbf{0}    &  2.13E-14         & \textbf{0.00E+00} & 4.73E-07          &  1.09E+00         &  7.15E-09        & \textbf{0.00E+00} \\
				$f_{12}$ 		&    -1         &  -6.78E-01    & \textbf{-1.00E+00}& \textbf{-1.00E+00}& -2.15E-05         &  -2.00E-01        &  -7.09E-01       & -3.18E-01 \\
				$f_{13}$ 		&     0         & \textbf{0}    &  1.18E-18         &  4.75E-28         & 2.41E-11          &  5.90E-08         &  1.35E-10        & 1.27E-20 \\
				$f_{14}$ 		&     0         & \textbf{0}    &  1.15E-13         & \textbf{0.00E+00} & 9.29E-07          &  1.09E+00         &  4.46E-09        & \textbf{0.00E+00} \\
				$f_{15}$ 		&     0         & \textbf{0}    &  6.23E-16         &  5.64E-22         & 1.82E-09          &  5.22E-05         &  1.73E-09        & 2.27E-32 \\
				$f_{16}$ 		&     0         & \textbf{0}    &  3.70E-03         &  5.76E-06         & 2.23E-09          &  2.34E-01         &  2.96E-03        & 4.14E-04 \\
				$f_{17}$ 		&     0         & \textbf{0}    &  1.50E-14         &  1.02E-20         & 1.23E-09          &  1.51E-03         &  8.34E-09        & 2.83E-31 \\
				$f_{18}$ 		&     0         & \textbf{0}    &   6.87E-13        & \textbf{0.00E+00} & 9.26E-08          &  1.88E-01         &  3.90E-08        & \textbf{0.00E+00} \\
				$f_{19}$ 		&     0         & \textbf{0}    &  1.01E-12         &  1.59E-14         & 1.02E-09          &  1.79E-01         &  4.54E-08        & \textbf{0.00E+00} \\
				$f_{20}$ 		&     0         & \textbf{0}    &  4.39E-12         &  6.24E-08         & 1.86E-09          &  2.00E-01         &  4.22E-08        & \textbf{0.00E+00} \\
				\hline
		\end{tabular}}
		\label{tab3}
	\end{table}
	
	As shown in Tab. \ref{tab3}, the errors of GBO for $f_1$-$f_3$, $f_5$, $f_7$-$f_{11}$, and $f_{13}$-$f_{20}$ functions are all equal to $0$, which indicates the solutions of GBO are exactly consistent with the theoretical optimal values; in comparison, all other algorithms can not achieve this point. Besides, the solutions of GBO are much better than those of other comparison algorithms in almost all cases only except the case on $f_{12}$, no matter on which unimodal and multi-modal functions. Furthermore, even on $f_{12}$, the converge performance in the error has reached above average level, and better than those of AFSA, GA, SA, and FWA. In general, as shown in Tab. \ref{tab3}, the convergence accuracy of GBO algorithm is significantly higher than that of other algorithms. The reason is that, as GBO can simulate the multi-granularity distribution of data, the important part of the solution space is covered by a large amount of fine-grained granular-balls, which makes GBO have better search ability of the optimal solution than the existing intelligent optimization algorithms. In addition, another phenomenon is that the error of FWA is the closest to GBO and less than other algorithms. But at the same time, our subsequent experiments show that the time cost of FWA is also the highest due to the high frequency of optimization search; in contrast, while maintaining the minimum error, GBO is also significantly ahead of other algorithms in efficiency.
	
	\subsection{Efficiency}
	
	In this section, the convergence speed of the algorithms is compared, and their running time is shown in Tab. \ref {tab4}. As shown in Tab. \ref {tab4}, it can be observed that GBO is much faster than other algorithms on almost all the datasets except $f_{13}$ and $f_{16}$. Even on $f_{13}$ and $f_{16}$, the running time of GBO on the two functions (i.e.,$0.11s$ and $0.08s$ respectively) is also far below the average level, and only very close to the best case, ranking second. The reason is that, as GBO can simulate the multi-granularity distribution of data, the unimportant part of the solution space is covered by a small amount of coarse-grained granular-balls, which makes GBO avoid a large number of search calculations. So, it has a faster convergence speed than the existing intelligent optimization algorithms. 
	
	\begin{table}[!ht]
		\setlength{\abovecaptionskip}{0.3cm}
		\setlength{\belowcaptionskip}{0.3cm}
		\tiny
		\centering
		\caption{The comparison of running time (s).}
		\setlength{\tabcolsep}{3mm}
		\begin{tabular}{llllllll}
			\hline
			\textbf{Func}	& \textbf{GBO}     & \textbf{PSO}      & \textbf{DE}       & \textbf{AFSA}     & \textbf{GA}       & \textbf{SA}    &\textbf{FWA} \\ \hline
			$f_1$   		&   \textbf{0.00}&   0.02          &  0.07           & 7.96            &  0.16           &  0.52        & 73.64 \\
			$f_2$   		&   \textbf{0.01}&   0.02          &  0.06           & 7.71            &  0.15           &  0.57        & 77.95 \\
			$f_{3}$ 		&   \textbf{0.00}&   0.02          &  0.08           & 7.32            &  0.16           &  0.57        & 86.98 \\
			$f_{4}$ 		&   \textbf{0.00}&   0.02          &  0.07           & 2.75            &  0.14           &  0.55        & 89.92 \\
			$f_5$    		&   \textbf{0.00}&   0.04          &  0.15           & 3.14            &  0.19           &  0.62        & 132.86 \\
			$f_6$    		&   \textbf{0.03}&   0.05          &  0.16           & 12.43           &  0.20           &  0.69        & 161.20 \\
			$f_{7}$  		&   \textbf{0.00}&   0.02          &  0.07           & 8.30            &  0.15           &  0.55        & 83.18 \\
			$f_{8}$ 		&   \textbf{0.00}&   0.03          &  0.09           & 9.29            &  0.15           &  0.59        & 95.65 \\
			$f_{9}$ 		&   \textbf{0.04}&\textbf{0.04}    &  0.14           & 10.17           &  0.19           &  0.66        & 150.97 \\
			$f_{10}$ 		&   \textbf{0.01}&   0.03          &  0.12           & 11.74           &  0.17           &  0.57        & 118.19 \\
			$f_{11}$ 		&   \textbf{0.00}&   0.04          &  0.13           & 10.71           &  0.18           &  0.58        & 127.70 \\
			$f_{12}$ 		&   \textbf{0.03}&   0.04          &  0.14           & 2.70            &  0.18           &  0.55        & 137.22 \\
			$f_{13}$ 		&   0.11         &   \textbf{0.03} &  0.11           & 10.41           &  0.17           &  0.51        & 108.52 \\
			$f_{14}$ 		&   \textbf{0.00}&   0.03          &  0.13           & 10.82           &  0.19           &  0.56        & 126.30 \\
			$f_{15}$ 		&   \textbf{0.00}&   0.02          &  0.07           & 8.02            &  0.15           &  0.49        & 79.80 \\
			$f_{16}$ 		&   0.08         &\textbf{0.05}    &  0.17           & 14.95           &  0.21           &  0.65        & 150.48 \\
			$f_{17}$ 		&   \textbf{0.00}&   0.02          &  0.07           & 8.39            &  0.15           &  0.50        & 81.94 \\
			$f_{18}$ 		&   \textbf{0.01}&   0.04          &  0.13           & 10.22           &  0.20           &  0.58        & 125.41 \\
			$f_{19}$ 		&   \textbf{0.01}&   0.03          &  0.12           & 9.65            &  0.21           &  0.56        & 122.22 \\
			$f_{20}$ 		&   \textbf{0.04}&\textbf{0.04}    &  0.11           & 9.63            &  0.20           &  0.55        & 105.82 \\
			\hline
		\end{tabular}
		\label{tab4}
	\end{table}
	
	\subsection{Stability and Adaptiveness}
	
	In this section, two unimodal functions $f_{3}$ and $f_{4}$, and two complex multi-modal functions $f_{5}$ and $f_{11}$ are selected for verify the stability of the comparison algorithms. The function diagram of $f_{3}$, $f_{4}$, $f_{5}$, and $f_{11}$ are shown in Fig. \ref{functions}. Each algorithm is executed for 10 rounds in four functions and count its optimization error in Fig. \ref{fsresultfig}, and the average error of the ten rounds in Fig. \ref{figavgerror}. As the running time of different algorithms is quite different, in order to display conveniently in the same figure, we have taken the logarithm of the running time with base 10 and displayed it in Fig. \ref{fsresultfig}. Besides, as some convergence error values of GBO are equal to $0$, we add a minimum value $1E-31$ to all errors in the process of taking logarithm. The raw data of running time is provided in Tab. 1 in the appendix.
	
	\begin{figure}[!ht]
		\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\centering
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=0.9\linewidth]{f4.png}
			\caption{}
			\label{f3}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=0.9\linewidth]{f5.png}
			\caption{}
			\label{f4}
		\end{subfigure}
		\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=0.9\linewidth]{f7.png}
			\caption{}
			\label{f5}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=0.9\linewidth]{f21.png}
			\label{Function diagram of f11}
			\caption{}
			\label{f11}
		\end{subfigure}
		\caption{Function diagrams of $f_{3}$, $f_{4}$, $f_{5}$, and $f_{11}$, corresponding to (\subref{f3}), (\subref{f4}), (\subref{f5}) and (\subref{f11}) respectively.}
		\label{functions}
	\end{figure}
	
	\begin{figure}[!ht]
		\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\centering
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1.0\linewidth]{f3result.png}
			\caption{}
			\label{f3result}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1.0\linewidth]{f4result.png}
			\caption{}
			\label{f4result}
		\end{subfigure}
		\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1.0\linewidth]{f5result.png}
			\caption{}
			\label{f5result}
		\end{subfigure}
		%\qquad
		\begin{subfigure}{0.49\linewidth}
			\centering
			\includegraphics[width=1.0\linewidth]{f11result.png}
			\caption{}
			\label{f11result}
		\end{subfigure}
		\caption{The error of GBO and baselines running ten times on function $f_{3}$, $f_{4}$, $f_{5}$, and $f_{11}$, corresponding to (\subref{f3result}), (\subref{f4result}), (\subref{f5result}) and (\subref{f11result}).} 
		\label{fsresultfig}
	\end{figure}
	
	As shown in Fig. \ref{fsresultfig}(\subref{f3result}), (\subref{f5result}), and (\subref{f11result}), the error of GBO is much smaller and stable than other algorithms. The instability of GBO in Fig. \ref{fsresultfig}(\subref{f4result}) comes from the using of a random function in $f_4$. In general, GBO has the best performance in stability. The experimental results about the average error in Fig. \ref{figavgerror} also support this conclusion. The reason is that GBO does not contain any hyper-parameters and random factors.	In addition, not containing any hyper-parameters also makes GBO completely adaptive, which is an important characteristics that other intelligence optimization algorithms does not have.
	
	\begin{figure}[!ht]
		\setlength{\abovecaptionskip}{0.cm}
		\setlength{\belowcaptionskip}{0.1cm}
		\centering
		\includegraphics[width=3in]{avg_error}
		\caption{The average error of running ten rounds of the comparison algorithms for $f_{3}$, $f_{4}$, $f_{5}$, and $f_{11}$ functions.}
		\label{figavgerror}
	\end{figure}
	
	\section{Conclusions and Future Work} \label{sec:conclusions and future work} 
	This paper presents a multi-granularity optimization algorithm GBO which can simulate the multi-granularity distribution of data. Different from the existing intelligence optimization algorithms, its optimization process is based on granular-balls with different sizes, rather than sampling points.  This leads to better convergence performance both in convergence speed and convergence accuracy than the existing algorithms. GBO contains no hyper-parameters, so it is completely adaptive and more stable than other algorithms. The experimental results in 20 benchmark functions show that GBO is significantly better than the existing algorithms in terms of both convergence speed and accuracy.
	
	At present, GBO is just a new and simple basic method. In future work, we will further develop it and improve its performance to solve high-dimensional and multi-modal function optimization. In addition, a better ball splitting strategy may bring higher convergence accuracy, which deserves further study.
	
%	\clearpage
	\section{Appendix}
	
	%\subsection{Benchmark functions}
	\begin{itemize}
		\item  Sphere Model
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{1}(x)=\sum_{i=1}^{n}x_{i}^{2},\ \  x_i \in [-100, 100]\ \ \ \ \ \ \\
					min(f_1)=f_1(0,0)=0.\ \ \ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Schwefel's Problem 2.22
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{2}(x)={\textstyle \sum_{i=1}^{n}} ( {\textstyle \sum_{j=1}^{i}}x_j)^2,\ \ x_i \in [-100, 100]\\
					min(f_2)=f_2(0,0)=0.\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Generalized Rosenbrock’s Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{3}(x)={\textstyle \sum_{i=1}^{n-1}} [100(x_{i+1}-x_i^2)^2+(x_i-1)^2],\\
					x_i \in [-30, 30], min(f_3)=f_3(1,1)=0.\ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Quartic Function i.e. Noise
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{4}(x)={\textstyle \sum_{i=1}^{n}} ix_i^4+random[0,1),\ \ \ \ \ \ \ \ \ \\
					x_i \in [-1.28, 1.28], min(f_4)=f_4(0,0)=0.\nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Cross-in-Tray Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{5}(x)= -\frac{1+\cos ( 12\sqrt{x_{1}^{2} + x_{2}^{2} } ) }{0.5 ( x_{1}^{2} + x_{2}^{2} ) +2}\ \ \ \ \ \ \ \ \ \ \ \ \ \\
					x_i \in [-5.12, 5.12],\ \ min(f_5)=f_5(0,0)=-1.\nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Levy Function N. 13
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{6}(x)= \sin^2 (3\pi x_1)+(x_1-1)^2\left [ 1+sin^2(3\pi x_2) \right ]\\+(x_2-1)^2 [1+ sin^2(2\pi x_2) ]\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
					x_i \in [-10, 10],\ \ min(f_6)=f_6(1,1)=0.\ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Matyas Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{7}(x)= 0.26(x_{1}^{2}+x_{2}^{2})-0.48x_1x_2\ \ \ \ \ \ \\
					x_i \in [-10, 10],\ \ min(f_7)=f_7(0,0)=0.\nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Three-Hump Camel Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{8}(x)= 2x_{1}^{2} -1.05x_{1}^{4}+\frac{x_{1}^{6}}{6} +x_1x_2+x_{2}^{2}\ \ \ \ \ \\
					x_i \in [-10, 10],\ \ min(f_{8})=f_{8}(0,0)=0.\ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Goldstein-Price Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{9}(x)= [ 1+(x_1+x_2+1)^2(19-14x_1+3x_1^2\\-14x_2+6x_1x_2+3x_2^2) ]
					\times [ 30+(2x_1-3x_2)^2\\(18-32x_1+12x_1^2+48x_2-36x_1x_2+27x_2^2) ]\\
					x_i \in [-2, 2],\ \ min(f_{9})=f_{9}(0,-1)=3.\ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Styblinski-Tang Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{10}(x)= 0.5+\frac{\sin^2(x_1^2-x_2^2)-0.5}{[1+0.001(x_1^2+x_2^2)]^2}\ \ \ \ \ \ \\
					x_i \in [-100, 100],\ \ min(f_{10})=f_{10}(0,0)=0.\nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Generalized Rastrigin's Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{18}(x)= \sum_{i=1}^{30} \left [ x_{i}^{2}-10\cos (2\pi x_i) +10 \right ]\ \ \ \ \ \ \ \ \\
					x_i \in [-5.12,5.12], \ \ min(f_{18})=f_{18}(0,0)=0.\nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Easom Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{12}(x)= -\cos(x_1)\cos(x_2)\exp(-(x_1-\pi)^2-(x_2-\pi)^2)\\
					x_i \in [-100, 100],\ \ min(f_{12})=f_{12}(\pi,\pi)=-1.\ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Sum of Different Powers Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{13}(x)= \sum_{i=1}^{d}|x_i|^{i+1},x_i \in [-1,1]\ \ \ \ \ \ \\
					min(f_{13})=f_{13}(0,0)=0.\ \ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Rastrigin Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{14}(x)= 10d+\sum_{i=1}^{d}[x_i^2-10\cos(2\pi x_i)]\ \ \ \ \ \ \ \ \\
					x_i \in [-5.12, 5.12],\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
					min(f_{14})=f_{14}(0.00034,0.00014)=0.\ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Sum Squares Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{15}(x)= \sum_{i=1}^{d} ix_{i}^{2},\ \ x_i \in [-10, 10]\ \ \ \ \ \\
					min(f_{15})=f_{15}(0,0)=0.\ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Generalized Griewank's Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{16}(x)= \frac{1}{4000} \sum_{i=1}^{30} x_{i}^{2} -\prod_{i=1}^{30}\cos (\frac{x_i}{\sqrt{i} } )+1\ \ \ \ \ \ \\
					x_i \in [-600,600], \ \ min(f_{16})=f_{16}(0,0)=0.\nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Rotated Hyper-Ellipsoid Function
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{17}(x)= \sum_{i=1}^{d}\sum_{j=1}^{i}  x_{j}^{2},\ \ x_i \in [-65.536, 65.536]\\
					min(f_{17})=f_{17}(0,0)=0.\ \ \ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Bohachevsky Function1
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{11}(x)= x_{1}^{2} +2x_{2}^{2} -0.3\cos(3\pi x_1)-0.4\cos(4\pi x_2)+0.7\\
					x_i \in [-100, 100],\ \ min(f_{11})=f_{11}(0,0)=0.\ \ \ \ \ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Bohachevsky Function2
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{19}(x)= x_{1}^{2}+ 2x_{2}^{2}-0.3\cos (3\pi x_1)\cos (4\pi x_2)+0.3\\
					x_i \in [-100,100],\ \ min(f_{19})=f_{19}(0,0)=0.\ \ \ \ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
		\item  Bohachevsky Function3
		\begin{center}
			\begin{equation}
				\begin{split}
					f_{20}(x)= x_{1}^{2}+ 2x_{2}^{2}-0.3\cos (3\pi x_1 +4 \pi x_2)+0.3\\
					x_i \in [-100,100],\ \ min(f_{20})=f_{20}(0,0)=0.\ \ \ \nonumber
				\end{split}
			\end{equation}
		\end{center}
	\end{itemize}
	
	%% The file named.bst is a bibliography style file for BibTeX 0.99c
	\bibliographystyle{named}
	\bibliography{ijcai23}
	
\end{document}