\section{Related Work}
Previous studies on autonomous navigation for field robots~\cite{thuilot2002automatic, griepentrog2006autonomous, kayacan2018high, reid2000agricultural, cordesses2000combine} have primarily relied on GPS with RTK correction combined with IMU. This is due to the accuracy of GPS with RTK correction which can reach centimeter level accuracy. However, this approach is limited by the need of having reliable GPS signals, which may be difficult to obtain in certain circumstances (\eg on the area with dense vegetation and tall crops). In such failure cases, dead-reckoning using IMU alone may not be reliable, especially on the fields with uneven surfaces and slopes. Obtaining an RTK signal correction is also necessary for GPS-based navigation to reach high accuracy, which can be expensive and impractical in many situations. Additionally, GPS-based navigation also requires long preparation and surveying the field to obtain a pre-defined map of the field structures and determine the path ahead of time. Furthermore, this map may not always be reliable due to changes in terrain conditions, weather, and season. 
These limitations make it challenging to apply GPS-based navigation to new fields and environments, hindering its widespread use. 

Therefore, researchers have investigated other sensors to supplement or replace GPS. 
For instance, LiDAR has been utilized to provide point cloud spatial information that enables the leveraging of the structure of rows or plants for in-field navigation~\cite{higuti2019under}. 
Some LiDAR-based methods perform line fitting (\eg RANSAC,  Hough transform, and PEARL~\cite{isack2012energy}) to identify the plant row structures from the point cloud~\cite{barawid2007development,malavazi2018lidar,nehme2021lidar}. 
Others directly use the point cloud as measurements to filter-based localization (\eg particle filter) and navigate at the center of the crops~\cite{hiremath2014laser}.
%However, point cloud-based methods can be sensitive to  point cloud density and may not generalize well to different plants and fields, as the density and structure of crops can vary and thus affect the distribution of the point cloud.
Although LiDAR provides accurate spatial information, the state of the art LiDAR with high density is expensive and cost prohibitive for consumer products.
Moreover, point cloud-based methods may struggle to adapt to different plants and fields, as the density and structure of crops can vary and thus affect the distribution of the point cloud.
These methods are also sensitive to the stage (\eg early or late) of the crops and plant size. 
For example, crops in their early stages may have smaller and less prominent structures that are harder to detect with LiDAR. Indeed, these methods have only been shown to work well for relatively tall vegetation with prominent structures, such as vineyards and maize, and may not work well for row crops that lack prominent structures, such as carrots.
More importantly, LiDAR does not provide semantic information that can be useful for improving the robustness of the system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\cite{barawid2007development} uses laser scanner to navigate orchard row crops by using Hough transform to detect orchard rows as straight line
%\cite{malavazi2018lidar} perform line extractions from 2D LiDAR point clouds using a PEARL~\cite{isack2012energy} geometric fitting based method and apply additional filter and post processing steps for crop detection and extract line path to follow.
%\cite{nehme2021lidar} applies Hough transform to point cloud input for line fitting in vineyard and identify the structure of the field.
%\cite{hiremath2014laser} uses particle filter and LiDAR measurement to localize the robot and navigate at the center of the crops. 
%%%%%%%%%%%%%%%%%%%%%

%Thus, many works leverage camera image information to perform vision-based navigation.
Thus, vision-based navigation, using image input from the camera for path guidance, has become increasingly popular in recent years.
Some of these works involve using image input from the camera to detect plant rows and lines for path guidance.
For instance, \cite{ahmadi2022towards, ahmadi2020visual} use classical image signal processing techniques (such as thresholding color values) to extract vegetation masks for row crop plants, which are then subjected to pixel clustering and post-processing steps to obtain a line for navigation guidance. 
While classical signal processing techniques may effectively extract vegetation masks for crop plants (short plants forming clear and simple lines), these techniques may not work well in more complicated plant and field scenarios.
Thus, deep learning-based techniques have also been used to address this challenge.
\cite{aghi2021deep} trains a canopy segmentation model MobileNetV3, and then performs pixel clustering on the segmentation results to obtain a cluster of empty space, which is identified as the road path.
However, this method relies on having only one empty space cluster, thus may not work well in several cases, \eg, early stages of growth or sparse vegetation. 
Moreover, the feedback on control commands is done in pixel space, which may not accurately represent the robot's movements. Working solely in pixel space limits the ability to sense distance and represent changes in elevation. This may result to errors in planning control commands, especially in complex environments such as varying terrain features that encounter slopes or elevation changes. Our proposed approach avoids relying explicitly on the crop patterns and adapts variant field environment with the generalizability of the machine learning model during the path detection. The detected path is transferred to the Bird Eye View (BEV) before applying the controller, and thus reduces the limitation of working soly on the image plane.
Alternatively, \cite{sivakumar2021learned} uses a truncated ResNet-18 and performs direct regression on the relative pose (heading and placement) of the robot to the center of the path, which is then used as a feedback for the controller.
However, these deep-learning based methods require manual annotation of numerous training images, which can be laborious to annotate. Our proposed method utilizes automaticlly generated annotations to train the model, allowing the algorithm to be smoothly deployed in the foreign fields and easily re-calibrated as needed.
Deep reinforcement learning models~\cite{martini2022position,zhou2014vision} have also gained attraction recently. 
However, RL-based approaches require many interactions with the environment and are typically trained only on simulations. RL models also tend to overfit to specific environments.

Finally, most works on in-field autonomous navigation primarily focus on line tracking (walking between rows of plants). 
However, for a robot to navigate fully autonomously in the field, it must also be able to switch rows to next rows of plants after finishing a line.
Some recent works~\cite{ahmadi2022towards,ahmadi2020visual} have proposed methods to perform row switching. 
For example, \cite{ahmadi2020visual} uses the back camera to detect the next row using SIFT detection after finishing a line and then moves backward to the next row. \cite{ahmadi2022towards} applies a similar technique to detect the next row, but employs a robot that can move sideways, thus simplifying the row-switching maneuver.
However, these methods work are only applicable to crop fields where plants laying low on the ground and do not work for vineyards or more complicated fields.  Additionally, the demonstration of row-switching in \cite{ahmadi2020visual} shows that the robot steps/walks over the crop plants during the process, which may damage the crop or not even viable for certain plants, such as those in vineyards.


%\input{dump/related-ref}
%%%%%%%%%%%%%%%%%%%

