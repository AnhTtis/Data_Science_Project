\section{Introduction}
Viticulture has been known as a labor-intensive production throughout its 8,000-year history. The development and application of agriculture machinery and pesticides have changed the picture of fieldwork by providing effective assistance in the field management tasks, and revealed the demand of further reducing the agricultural labor requirement by moving toward the next generation full automation of fieldwork. 

Many intelligent actuation systems have been developed for automating the planting, pruning, and harvesting \cite{ly2015fully,botterill2017robot,luo2018vision}. Recently the exploration of novel robotic perception system for disease management and yield prediction tasks have also gained attention because of the development of modern computer vision technology \cite{liu2022near,lopes2016vineyard,victorino2019grapevine}. In addition to these task-oriented automation, robotic systems need to accurately and autonomously navigate in the field with adaption to ever-changing field conditions. Currently, navigation systems based on real time kinematics (RTK)-enabled global navigation satellite system (GNSS) and inertial measurement unit (IMU) have been widely used in both research projects and commercial applications. However, RTK-GPS based systems heavily rely on fast and stable communication (e.g., radio) with a base station. When communication interference occurs (e.g., poor visibility to satellites due to tall crops), the RTK-GPS signals will be interrupted, influencing or even ceasing robot operations. Moreover, if a mobile base station is used and moved unexpectedly, the rover localization will be dramatically changed, leading to potential safety risks such as a collision in the field.

Vision-based navigation solutions have been extensively studied in recent days because of 1) increased imaging resolution and modalities (e.g., depth cameras) with affordable cost for resolving needed information in navigation and 2) breakthroughs in deep learning (DL) that offer new tools for image analysis and interpretation for navigation. Many recent vision-based navigation systems require abundant human annotations of certain objects in the field (e.g., crops and crop rows) to train a DL model for finding a traversal path. While these systems have achieved certain success, challenges remain in i) costly data annotation for model training and ii) substantial variations among crops (e.g., corns vs grapes) and cropping systems (e.g., row cropping vs trellised cropping systems) for generalizing a model/system developed from one crop to another. These become major barriers for adopting the vision-based navigation solutions in agriculture.

This work aimed to provide a learning-based visual navigation system that can be easily and reliably deployed in the field. We proposed to directly output the traversal path in image frames, and automatically generate the training annotation by projecting the RTK-GPS paths onto the images. The RTK GPS would be required only during the setup phase for training data collection and could be removed during the deployment phase for regular operations, reducing the cost of training and applying a DL for various crops and cropping systems. We integrated the pretrained model into a full navigation framework consisting of row tracking and switching modules for autonomous navigation in the vineyard.

