\section{Experiment}

\begin{figure*}[t]
  \centering
  \includegraphics[trim={0 0 0 0},clip,scale=1.2]{Figures/examples_lr.png}
  \caption{Sample pictures of model inference result. Images of successful detection are labeled in green and images of failed detection are labeled in red. The approximate position and orientation of detected paths are annotated in red.}
  \label{examples}
\end{figure*}

The experiments took place at three different vineyards with \emph{varying conditions} at Cornell AgriTech, Geneva, NY, including Crittenden Pathology vineyard (CRT), VitisGen3 vineyard (VG3), and research north vineyard (RN). CRT and VG3 are on an approximately 10$^{\circ}$ slope and next to each other. While CRT and VG3 share similar terrain and soil conditions, they contain grapevines in very different growth stages (20 years in CRT and newly planted in VG3) and therefore visual appearance. RN is 1.5 km away and has a relatively flat surface and sandy soil. Crop rows in all three vineyards are generally straight lines. The row length is 120m, 90m and 70m for CRT, VG3 and RN, respectively.

%%To train the path detection network, we collected a set of \emph{training data} in CRT field from row 6 up to row 8 (refer to \autoref{trajectory}) in November 2022.
A \emph{training dataset} was established using data from CRT in November 2022 when grapvines still had canopies.
The robot was driven using an RTK-enabled waypoint navigation to collect RGB-D images from the front camera along with RTK-GPS and IMU information. The collected data were used to \emph{automatically} generate a training dataset with ground-truth annotation (as described in Section \ref{sec:Path Estimation on Image Frame}).
Additionally, we introduced deviation from the RTK-guided path when driving the robot to increase the training data variance. Specifically, we manually override the navigation command and change the heading of the robot in the middle of the row for about 10s.
The network was trained using the Adam~\cite{kingma2014adam} optimizer with a batch size of 64, a base learning rate of $1\times 10^{-5}$, and a weight decay of $0.004$. The training process on 4560 RGB-D images (640$\times$480) took approximately 24 hours for 1500 epochs using 2 RTX3090 GPUs. The model weights with the best validation performance were used in all successive tests.

Two tiers of tests were conducted to evaluate the developed navigation framework. The first-tier tests were to evaluate the performance of the path estimation network (Section \ref{sec:Path Detection Evaluation}) as the core component of the framework, and they were perfomed in CRT only. The second-tier tests were to assess the full navigation framework (Section \ref{sec:Autonomous Navigation Evaluation}), and they were conducted in all three vineyards to evaluate the robustness and generalizability of the navigation framework \ref{sec:Generalization Test}. All tests were performed in February 2023 that was three months apart from the training data collection time, furthering the difference between training and testing data to demonstrate the robustness and generalizability of the model and framework.

%The experiments were conducted at the Cornell Pathology Vineyards of Cornell AgriTech in Geneva, NY in the year of 2022 and 2023. The experiments were done in three different fields: crittenden (CRT), vitisgen3 (VG3), and research north (RN). CRT and VG3 are close to each other and are on a roughly 10$^{\circ}$ slope, and RN is about 1.5 km away from them and is relatively flat. The crop rows in all three fields are overall straight with the lengths of around 120m, 90m and 70m for CRT, VG3 and RN respectively.  The training data was collected on November 8th, 2022, in field one. The training data collection started from the east end of row 6 the CRT and ended at the first 10m of row 8. The validation data consisted of the rest of row 8. The robot was driven by the RTK-GPS-IUM based navigation program. Deviations were introduced by manual control override for increasing data variance. During the data collection, the front D435 camera was used, outputting RGB-D image of 640x480 resolution at 6 FPS. The training was conducted on a desktop computer with two Nvidia RTK 3090 GPU and one AMD 3970X CPU. The batch size was set to 64. The Adam optimizer with base learning rate of 1E-5 and weight decay of 0.004 were used. The weight that yielded the lowest loss on the validation dataset was selected. The training process took around 24 hours. After the training, all the evaluation experiments were conducted 3 months later using the same weight.

\subsection{Path Detection Evaluation}\label{sec:Path Detection Evaluation}
Correct identification of a path was the key to the success of the path detection model and successive navigation, especially when a robot oriented away from the row direction. To evaluate this, the robot was manually driven in the middle of a row to perform a $360^{\circ}$ in-place turn and collect inference results from the path detection model. The detected path heading was compared with the robot's heading (from the IMU of the SMART7 system) with respect to the reference path that was generated by pre-surveyed RTK GPS points. 

\input{sections/table1}
The model could provide satisfactory path detection for a robot heading up to $\pm25^{\circ}$ away from the row direction (Table \ref{table1}). When the robot heading was beyond this range, the traversal path was not fully visible in images, yielding unusable results. For most of the valid heading angles, the average heading deviation was below $2.5^{\circ}$, which was comparable to a previous study ($1.99^{\circ}$ average heading error) on vision navigation in a corn field~\cite{sivakumar2021learned}. The average deviation generally increased as the angle between the robot heading and the crop row increased. At larger angles, the collected images started to include information from adjacent rows, resulting in false detection. This issue may be partially addressed during a growing season when grapevine canopies would block the view of adjacent rows.

These patterns were observed from representative images (Figure \ref{examples}). 
When the robot headed straight down to the row (Figure \ref{examples} a) or within the heading deviation range ($\sim20^{\circ}$ away from the row in Figure \ref{examples} b), the model could accurately generate a high quality heatmap and identify the correct moving path. However, when the robot headed out of the deviation range (i.e., greater than $25^{\circ}$), one side of the row became invisible in the image, and the model started to be influenced by the features from the next row (Figure \ref{examples} c). In addition to the heading deviation effect, tall weeds and vacant space due to dead grapevines introduced challenges for the model to correctly detect the traversable path (Figure \ref{examples} d and Figure \ref{examples} e). While these special cases were occasional in vineyards and generally did not hamper the continuation of navigation, some large navigation errors were observed due to suboptimal path detection results. These could be mostly improved by fine-tuning the model with the automatic annotation without significant additional effort.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Autonomous Navigation Evaluation}\label{sec:Autonomous Navigation Evaluation}
Field trials were conducted on February 16, 2023 in all three vineyards to quantitatively analyze the navigation framework performance. In each vineyard, four replication trials were conducted using the developed vision-based navigation framework to simulate the periodical and repetitive fieldwork needs. For the vision-based navigation, all cameras were set to the resolution of 640 $\times$ 480 at 15 FPS. During the trial, the RTK-GPS locations were separately logged for trajectory analysis purposes. One reference trial with RTK-enabled waypoint navigation was conducted for comparison purposes.

In CRT, the trial started from the end (east side) of row 7 and stopped at the first 10m (east side) of row 11, consisting of 4 full row tracking maneuvers and 4 row switching maneuvers (2 on each side of the vineyard). Although row 7 and 8 were included in the training and validation dataset that was collected 3 months ago, the robot traversed through these two rows from the opposite directions with different visual features. 

Positional and heading deviations between robot moving trajectories and predefined crop row paths were measured and used as metrics for performance evaluation. The positional deviation was defined as the distance from the current robot position to the predefined crop row path, and the heading deviation was defined as the robot heading with respect to the crop path direction. In these analyses, the robot heading was calculated using GPS records. To avoid potential noises in the calculation, the collected GPS records were downsampled to 1Hz. To quantify the algorithm performance at row tracking and row switching stages individually, the data near the end of the rows were separately analyzed from the data in the middle of the rows. The end of the row region was defined by 12m radians circles from the crop row endpoints.

\input{sections/table2}
Among the four replication trials, two trials were successfully completed without any human intervention, whereas another two trials required human intervention once for correct row switching to complete the trials (\autoref{table3} and left panel in \autoref{trajectory}). This demonstrated that the developed navigation framework generally provided a satisfactory performance, especially for row tracking (no human intervention along a total of 1920 m testing distance). However, row switching presented challenges for the current framework occasionally. In trial 2, during the transitioning from row 9 to row 10 (west end), the noisy depth information from the side camera failed to initiate the row switching module to state 1, and therefore a human intervention was needed to correctly turn the robot for successive navigation. In trial 4, during the transitioning from row 10 to row 11 (east end), the robot deviated too far away from the optimal path while navigating using the back camera, and as a result the robot mis-detected the next row as the current row in state 3. A human intervention was required to direct the robot back to the correct row.

The positional and heading deviation distributions were similar cross the 4 trials, indicating stable performance for repetitive runs on the same path (\autoref{table3}). While the position deviations for the vision based navigation were higher than those of the RTK-enabled waypoint navigation, the heading deviations were comparable. This was likely due to the fact that angular errors were more visible in image frames especially at further distances to the camera, while the positional offset between the image center-line and the crop row center-line tended to vanish as the distance increased due to image projection. The deviations during row switching (especially when the robot entered or exited a row) were also higher than those during row tracking deviations because of uncertainties caused by the path detection using the back camera. When tracking using back camera, the BEV controller needed to extend the detected path to the front of the robot. This extension increased the precision requirement for the detection and decreased the stability of the controller. This issue could be solved by improving the detection accuracy and the controller design.

While large positional deviations occurred very occasionally during the field trials, the maximal positional deviations were observed for common challenges (Left panel in Figure \ref{trajectory}). For trial 1, 2, and 4, the maximum deviation happened at roughly the same location where a ditch was created by previous tractor operations (red circle in the left panel in Figure \ref{trajectory}), introducing terrain differences for robot maneuver. For trial 3, the maximum deviation happened around a vacant space where several adjacent grapevines were dead and removed (white circle in the left panel in Figure \ref{trajectory}). This vacant space could influence the path detection model to generate a suboptimal path with a large positional deviation.     

Overall, the trial results demonstrated that our vision-based navigation framework was able to continuously guide the robot traversing through a vineyard with minimum human intervention, even during the winter time that presented additional terrain challenges. With extensive grapevine canopies (therefore better visual and depth information of a crop row) during the growing season and improved row switching, one could expect the framework to continuously perform well as an all-season navigation solution in the future. 

\begin{figure*}[t]
  \centering
  \includegraphics[trim={3.5cm 5cm 4cm 4.8cm},clip,scale=0.75]{Figures/Result_trajectory.png}
  \caption{Autonomous navigation evaluation and generalization evaluation trajectories. Left: Five trials in CRT with positions of maximum deviations of vision based row tracking labeled. Right: Generalization evaluation trajectories in RN and VG3. Easting coordinates are flipped along the Northing axis.}
  \label{trajectory}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization Test}\label{sec:Generalization Test}
To evaluate the robustness and generalizability of the navigation framework in unseen vineyards, additional field trials were conducted on February 17th, 2023 in VG3 and RN. For each vineyard, the trial included 3 row tracking maneuvers and 2 row switching maneuvers (1 on each end of the field). Only one trial was performed with vision-based navigation. The data logging and evaluation processes were identical to those in CRT field trials.

\input{sections/table3}

Both trials successfully completed without any human intervention (Table \ref{table3} and Figure \ref{trajectory}) and achieved comparable positional and heading deviations to those in CRT, indicating a consistent satisfactory performance in unseen vineyards without model fine-tuning and parameter re-tuning. It should be noted that RN is oriented in the North-South direction, which is different from CRT and VG3 along the East-West direction. Operating in RN required the camera to directly facing the Sun. The experiment results indicated that no significant interference was caused during row tracking. However, the robot spent longer time to turn into the third row facing the Sun. This was likely caused by the delayed auto-exposure adjustment and could be potentially solved by optimizing the imaging system.
