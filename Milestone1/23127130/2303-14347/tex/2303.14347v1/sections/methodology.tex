\section{Method}
%Our proposed navigation algorithm was a state machine consisting two state: row tracking and row switching. The navigation algorithm assumed that the robot was initially pointed down to the row, and started from row tracking. The robot changed to row switching state when the low-precision GPS indicated that the robot was approaching the end of the row. The row switching algorithm drove the robot to align to the next row and then handed over to the row tracking state.


%\begin{figure*}[t]
%  \centering
%  \includegraphics[trim={4cm 6.5cm 3.5cm 5.5cm},clip,scale=0.72]{Figures/Pipeline.png}
%  \caption{Overall design of our vision based navigation approach. The algorithm alternatively switch between row tracking and row switching state to traverse through the field}
%  \label{pipeline}
%\end{figure*}


\begin{figure*}[t]
  \centering
  \includegraphics[trim={1.2cm 8.5cm 5cm 5.8cm},clip,scale=0.65]{Figures/Pipeline1.png}
  \caption{Path detection and BEV controller for row tracking.}
  \label{pipeline1}
\end{figure*}


\begin{figure}[h]
  \centering
  \includegraphics[trim={15cm 7.5cm 1.5cm 0.5cm},clip,scale=0.5]{Figures/Pipeline2.png}
  \caption{Overall design of our vision based navigation approach. The algorithm alternatively switch between row tracking and row switching state to traverse through the field.}
  \label{pipeline}
\end{figure}

Our autonomous navigation framework comprises two main modules: row tracking and row switching. The framework starts the row tracking module by assuming that a robot will start at the beginning of the first row and head towards the row. As the robot approaches the end of the current row, the framework will use the row switching module to transition the robot into the next row and reverse to the row tracking module. The framework continues this iteration until all rows in a navigation sequence are traversed. 

%\subsection{System design}
%The navigation solution was designed for the differential drive non-holonomic system. Specifically, we integrated our method on a Clearpath Husky based UGV system. The system contained 4 Intel Realsense D435 cameras, one facing foward, one facing backward and two facing each side. The cameras were installed roughly on the centerline of the robot and on a shaft about 1m away from the ground. A SWIFT GPS was added to provide state switching information. All required sensors were connected to an Nvidia Jetson Xavier AGX computer, which execute the algorithm and send the velocity commands to the Husky robot base. The proposed method would work generally in robotic systems that have required equipment and can perform zero-point turns.

We implemented and evaluated our navigation framework using a custom robot developed for vineyard research and management (Figure \ref{robot}). The robot consisted of a non-holonomic differential drive mobile robot base (Husky, Clearpath Robotics Inc., Canada), four RGB-D cameras (Realsense D435, Intel Corp., USA), and two sets of GNSS systems. The RGB-D cameras were mounted approximately one meter above the ground, with one facing in direction. A low-cost GPS system (Duro, Swift Navigation, USA) was used to provide DGPS signals with up to 5m accuracy to aid in transitioning between row tracking and row switching. A high-end GNSS-IMU system (SMART7, NovAtel Inc., Canada) was used to acquire RTK GPS with centimeter accuracy for training annotation generation and performance evaluation. Both GPS systems worked at 10 Hz. All sensors were connected to an embedded computer (Jetson Xavier AGX, Nvidia Corp., USA) that executed the navigation algorithm and communicated with the robot base for control.


%%%%JM fig%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{Figures/robot2.png}
  \caption{Robotic platform used for data collection, deployment and evaluation. Only two out of four cameras are shown due to viewing angle}
  \label{robot}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Row Tracking}
\subsubsection{Path Detection on Image Frame} \label{sec:Path Estimation on Image Frame}
We proposed a novel learning-based approach to estimate the path of the robot without manual annotation. The path detection in an image was considered as the prediction of a \emph{path traversability heatmap} in which the pixel intensity represented the path preference. The predicted heatmap was used to generate an efficient path for a robot to follow. Pixels with the highest value of individual rows in the heatmap were selected to generate the path in the image frame. The ResNet-18 network \cite{he2016deep} was adapted to estimate the path traversability heatmap of a given RGB-D image. Specifically, the last average pooling layer and all fully convolution layers were replaced with two transposed convolution layers and one single output channel convolution layer (i.e., $M\times N \times1$). This modification enabled the generation of a high-resolution (half size of an input image) heatmap of path preference. Additionally, the first convolution layer of the network was revised to accept four channels (RGB-D) instead of three (RGB), allowing an effective integration of color and depth information for path traversability estimation, especially in complex field conditions. The network was trained using a binary cross-entroy loss between predicted and ground-truth heatmaps.\\

An automatic method was developed to generate ground-truth heatmaps. The robot system would be firstly deployed in a vineyard for collecting RGB-D images from the front camera along with GNSS-IMU information using a waypoint-based navigation with the RTK GPS enabled. The collected GPS paths were projected into the images using the equation \ref{proj_eq}:
\begin{equation}
\boldsymbol{x_c} = \boldsymbol{P} \boldsymbol{[R|t]} \boldsymbol{x_w}
\label{proj_eq}
\end{equation}
where $\boldsymbol{x_w}$ is the GPS path to be projected, $\boldsymbol{[R|t]}$ is the translation between world frame and local camera 3D frame obtained from the GNSS-IMU observation, $\boldsymbol{P}$ is the camera projection matrix, and $\boldsymbol{x_c}$ is the projected path in the image frame

A Gaussian kernel with $\sigma=15$ was applied around the projected paths to generate ground-truth heatmaps for network training. While our approach still required the acquisition of training images, image annotation was fully automatic to make network training and validation much more affordable and scalable. This is a key difference compared with other learning-based methods~\cite{aghi2021deep,sivakumar2021learned} that required laborious annotation of image labels such as canopy/vegetation masks and/or crop row paths or horizon lines.

%Given an RGB-D image from the front camera, our network estimates a \emph{path traversability heatmap} that assigns a high value to pixels that represent preferred paths for the robot to follow. We train the network using a binary cross-entropy loss to between the predicted and ground-truth heatmaps. From the predicted heatmap, one can easily obtain an efficient path to guide the robot. 
%previously here...
%To implement this, we adopt the ResNet-18 \cite{he2016deep} as our backbone and modify it correspondingly. Specifically, we truncate the network (removing the last average pooling and fully convolution layers) and add two transposed convolution layers and one 1-D convolution layer. By doing so, we can obtain a high-resolution heatmap that accurately represents the preferred path of the robotAdditionally, the first convolution layer of the network is modified to accept four channels (RGB-D) instead of three (RGB). This allows us to effectively utilize the depth information, which is crucial for accurately estimating the path traversability in complex environments.One advantage of directly detecting the path in the image frame is that we can leverage the rich information provided by the RGB-D data to improve the accuracy and robustness of the path estimation.

%The key innovation of our approach was that the algorithm directly detected the target traversing path in the image frame. To achieve this, we designed the network to output a global heatmap, similar to the one used for the key point detection tasks. The high values of the heatmap indicate the pixels of preferred traversing place and should together form the shape of a line or a curve in the image. For the network design, we modified the ResNet-18 \cite{he2016deep} network by truncating the average pooling and fully convolution layers in the end and added two transposed convolution layers and one 1-D convolution layer. This modification reduced the output channel to one, which is required for the heatmap, and enlarged the output resolution. Additionally, we also modified the first convolution layer of the network to accept input of 4 channels. We fed the RGB and depth image into the network by concatenating the 1 channel depth image directly as the 4th channel of the original RGB image. This modification forced the network to internally process the RGB and depth information computationally effectively. Only the front camera was used to provide navigation input in this stage. 


%Another key advantage of directly predicting preferred path of the robot is it allows us to obtain training labels without manual annotation.
%We propose an automatic annotation method that leverages the robot's path recorded from data collection to generate our training labels. 
%Specifically, during the training data collection, an operator manually drives the robot in the field to collect the necessary training images. 
%Upon data collection, we then project the recorded GPS path into the image space using camera projection parameters and apply a Gaussian kernel around the projected path to obtain a ground-truth heatmap.
%It is worth noting that any learning-based works~\cite{aghi2021deep,sivakumar2021learned} including ours will require the robot to be driven in the field to collect training images. 
%However, the key difference is our approach do not require any manual annotation upon data collection, whereas existing works rely on laborious manual annotation to obtain the necessary training labels, such as vegetation masks, canopy segmentation labels, and horizon lines.
%Furthermore, it is important to emphasize that we require the use of an RTK-GPS-IMU system only for data collection and not for deployment. This means that we can tolerate occasional RTK-GPS signal loss, as we can just select training data with good signals. Overall, our proposed method for automatic annotation provides a more efficient and reliable way to generate training labels, reducing the need for manual annotation and improving the training process. Figure \ref{annotation} illustrates the automatic annotation process.



\begin{figure}[h]
  \centering
  \includegraphics[trim={4.5cm 8.5cm 7cm 5cm},clip,scale=0.42]{Figures/annotation.png}
  \caption{Automatic annotation generation process. The GPS path is projected onto the image frame to generate ground truth label. Image is sampled from the training dataset}
  \label{annotation}
\end{figure}


%Unlike most of the methods described previously, the training annotation of our approach was generated automatically from the RTK-GPS-IMU system with little human effort. To generate the training annotation, the robot first traversed through the pre-determined waypoints in the field using the high precision RTK-GPS-IMU system. The robot position, orientation and camera image were recorded. After the data collection, the GPS paths were projected onto the image using robot positional information and camera projection parameters. In the end, a Gaussian kernel was applied to the projected path on the image frame to generate the ground truthing heatmaps. Although training annotation required RTK-GPS-IMU system, this process only required limited data and only needed to be conducted once when deploying in the foreign field. The robot would navigate independent of the RTK-GPS-IMU system as long as there were no significant changes in the field. 

%We employ a binary cross-entropy loss to train the network.

\subsubsection{BEV Controller}
%During the post-processing stage, the path detected by the network in the image frame was first projected to the Bird Eye view (BEV). The current simplified BEV projection assumed that the camera was always on a fixed height parallel to the ground. This assumption was largely violated due to the rough terrain in the field, but the experimental result indicated that the error was overall neglect-able, and the assumption can be lifted with little modification. After the path was extracted in the BEV, a trajectory follower with a proportional–derivative (PD) controller and feedback Linearization was applied to generate the forwarding and turning velocity commands. Controlling from BEV prevented distorted deviation calculation due to image projection effect.

We designed our controller in the bird-eye's view (BEV) representation rather than in the image space. This was because 1) the BEV representation depicted the robot’s movements with limited effects due to camera pose changes and 2) the BEV representation provided a stable and consistent view of the environment, improving the accuracy and robustness of the controller. Therefore, the extracted path from a given image was transformed to the bird-eye view's representation using homography projection. The resultant path was used as a reference for path following control via a proportional–derivative (PD) controller. Linearization feedback was applied to generate the linear and angular velocity commands for the robot base.


\subsection{Row Switching}
The framework transitioned to the \emph{row switching} module when the robot approached the end of a row. The low-cost GPS was used to roughly estimate the distance between the robot and the pre-surveyed end point of that row. If the distance was less than a threshold, the module transitioning would be triggered to change the robot from state 0 to state 1 in \autoref{pipeline}. In this study, we set an arbitrary threshold of 12 m by considering the GPS accuracy (up to 5 m) and grapevine block length. The use of the low-cost GPS was to relax many difficult requirements that would otherwise hinder the widespread adoption of developed framework and robot. In state 1, the robot started to use the back camera to perform row tracking because the front camera would lose sight of that row. To accurately determine turning point for row switching, one of the side cameras (depending on the turning direction) was used to obtain side view depth distribution. The depth values from the side view increased at the end of a row compared with that in a row.     

%To trigger this transition process (from state 0 to state 1 in \autoref{pipeline}), the low-cost GPS provided a \emph{coarse location}, particularly to check the \emph{rough} distance between the robot and pre-surveyed end of the row waypoints.
%It is worth noting that this GPS signal is just used to \emph{coarsely} check if the robot is nearing the end of the row (\ie within 7m distance), as a GPS signal without RTK correction can provide only up to 5m accuracy. 
%By using just a low-cost GPS without RTK correction, we can relax many difficult requirements that would otherwise hinder the widespread adoption of the robot.
%To accurately determine the end of the row turning point for row switching, we also utilize the signals from the side cameras, taking advantage of the fact that the depth distribution from the sides becomes further away at the end of the row.
%Additionally, as the robot approaches the end of the row, the front camera may lose sight of the row. 
%In such case, the robot navigates uses the back camera to perform row tracking navigation.

%Once the GPS indicates that the distance between the robot and the way point at the end of the row was smaller than a 7m threshold, the row switching state is activated. The GPS along only provides localization up to 5m localization accuracy. Therefore it cannot be used to safely determine the end of the row turning point for row switching. To ensure the collision avoidance, the robot continues to drive until several seconds after the side depth camera indicates that no close range objects in front of the camera. The distance and time threshold are hyperparameters decided based on the field setup and robot traversing speed. During this time, the robot navigate with the same row tracking algorithm but uses the back camera, as the front camera may have lost the sight of the row. The trajectories detected by the back camera are extended forward in the BEV to apply the same controller. This process is represented by state 0 in the row switching part of the Figure \ref{pipeline}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Once the turning point was identified, the robot performed the row switching process as follows:
\begin{enumerate}[label=(\alph*)]
    \item After the robot reached the turning point, it executed a 90$^{\circ}$ in-place turn (state 2 in \autoref{pipeline}). 
    Images from the side camera (the same one used for module transitioning) was fed to the path estimation network (described in Section \ref{sec:Path Estimation on Image Frame}) to identify the line of the \emph{recently completed row} (as feedback), from which the relative angle of robot rotation was estimated. The 90$^\circ$ turning concluded once the side camera aligned with the recently completed row, namely, a row path was identified in the center of a side view image.

    \item Now that the robot was considered perpendicular to the grapevine rows and moved straight forward towards the beginning of the next row (state 3 in \autoref{pipeline}). Side camera images were used to identify the line of the \emph{next row} and measure the relative location of the robot to this new row. The robot stopped when it arrived at the \emph{next} row, indicated by the row detected at the \emph{center} of a side view image.

    \item Finally, the robot performed another 90$^{\circ}$ in-place turn to align its orientation with the new row (state 4 of \autoref{pipeline}). This turning process was identical to the first turning (step a), but the front camera (instead of the side camera) was used for feedback because of a better view to the row during the process. After completing this turn, the robot headed towards the new row and switched back to the row tracking module for operations.
\end{enumerate}


%The row switching algorithm primarily feed images from different cameras into the path detection network to sense the robot relative position to the rows, with the assumption that the detected path from the row tracking phase generally reflect the crop row position and orientation. When the turning point was reached, the robot first start a 90$^{\circ}$ constant rate zero-point-turn. During this process, the side camera was used to measure the turning angle, and stop the turn when it was pointed toward the row. To measure the turning angle, the data from the side camera is fed into the path detection network, and the angle is measured between the generated path and the camera orientation. This process is illustrated as the state 2 in the row switching part of the Figure \ref{pipeline}.

%After the robot is perpendicular to the crop rows, it then moves forward to the next row. The same side camera facing toward the crop row is used to stop the robot to align to the target row. The robot stops moving when the side camera image yield a valid detected path approximately at the center of the camera. The stopping criteria is  delayed by a timer to ensure that the robot moves out of the current row first. This process is illustrated as the state 3 in the row switching part of the Figure \ref{pipeline}. Finally, the robot performed another 90$^{\circ}$ turn to align itself to the row. The process of alignment is the same as state 2 but the front camera is used for angle measurement, as is illustrated by the state 4 in the row switching part of the Figure \ref{pipeline}. After the last 90$^{\circ}$ the row switching maneuver is completed and row tracking algorithm takes control of the robot. 