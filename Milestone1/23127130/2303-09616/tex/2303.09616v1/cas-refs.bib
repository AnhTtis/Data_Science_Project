
@article{li_estimating_2017,
	abstract = {An important statistical task in disease mapping problems is to identify divergent regions with unusually high or low risk of disease. Leave-one-out cross-validatory (LOOCV) model assessment is the gold standard for estimating predictive p-values that can flag such divergent regions. However, actual LOOCV is time-consuming because one needs to rerun a Markov chain Monte Carlo analysis for each posterior distribution in which an observation is held out as a test case. This paper introduces a new method, called integrated importance sampling (iIS), for estimating LOOCV predictive p-values with only Markov chain samples drawn from the posterior based on a full data set. The key step in iIS is that we integrate away the latent variables associated the test observation with respect to their conditional distribution without reference to the actual observation. By following the general theory for importance sampling, the formula used by iIS can be proved to be equivalent to the LOOCV predictive p-value. We compare iIS and other three existing methods in the literature with two disease mapping datasets. Our empirical results show that the predictive p-values estimated with iIS are almost identical to the predictive p-values estimated with actual LOOCV and outperform those given by the existing three methods, namely, the posterior predictive checking, the ordinary importance sampling, and the ghosting method by Marshall and Spiegelhalter (2003). Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
	author = {Li, Longhai and Feng, Cindy X. and Qiu, Shi},
	copyright = {All rights reserved},
	doi = {10.1002/sim.7278},
	file = {/Users/lol553/Zotero/storage/JW3VU7N6/dmpvalue_17feb2.pdf;/Users/lol553/Zotero/storage/W85ZPBT9/Li_et_al-2017-Statistics_in_Medicine.pdf;/Users/lol553/Zotero/storage/42W8BQMX/abstract.html;/Users/lol553/Zotero/storage/9SVRXPZJ/1603.html},
	issn = {1097-0258},
	journal = {Statistics in Medicine},
	keywords = {Cross-validation,disease mapping,ghosting method,importance sampling,MCMC,posterior predictive p-value},
	langid = {english},
	month = jan,
	number = {14},
	pages = {2220--2236},
	title = {Estimating Cross-Validatory Predictive p-Values with Integrated Importance Sampling for Disease Mapping Models},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.7278/abstract},
	urldate = {2017-03-15},
	volume = {36},
	year = {2017},
	Bdsk-Url-1 = {http://onlinelibrary.wiley.com/doi/10.1002/sim.7278/abstract},
	Bdsk-Url-2 = {https://doi.org/10.1002/sim.7278}}

@article{piironen_comparison_2017,
	abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
	author = {Piironen, Juho and Vehtari, Aki},
	doi = {10.1007/s11222-016-9649-y},
	file = {/Users/lol553/Zotero/storage/BSB85UHI/Piironen and Vehtari - 2016 - Comparison of Bayesian predictive methods for mode.pdf;/Users/lol553/Zotero/storage/LIC5JHST/Piironen and Vehtari - 2016 - Comparison of Bayesian predictive methods for mode.pdf;/Users/lol553/Zotero/storage/UTRRT7V8/Piironen and Vehtari - 2017 - Comparison of Bayesian predictive methods for mode.pdf;/Users/lol553/Zotero/storage/9KWB5GWV/s11222-016-9649-y.html;/Users/lol553/Zotero/storage/AL5YRARQ/s11222-016-9649-y.html;/Users/lol553/Zotero/storage/KIRYXU3R/s11222-016-9649-y.html},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	keywords = {Computer Science - Learning,Statistics - Methodology},
	langid = {english},
	month = may,
	number = {3},
	pages = {711--735},
	title = {Comparison of {{Bayesian}} Predictive Methods for Model Selection},
	url = {https://link.springer.com/article/10.1007/s11222-016-9649-y},
	urldate = {2017-04-26},
	volume = {27},
	year = {2017},
	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s11222-016-9649-y},
	Bdsk-Url-2 = {https://doi.org/10.1007/s11222-016-9649-y}}

@article{vehtari_pareto_2015,
	abstract = {Importance weighting is a convenient general way to adjust for draws from the wrong distribution, but the resulting ratio estimate can be noisy when the importance weights have a heavy right tail, as routinely occurs when there are aspects of the target distribution not well captured by the approximating distribution. More stable estimates can be obtained by truncating the importance ratios. Here we present a new method for stabilizing importance weights using generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios.},
	archiveprefix = {arXiv},
	author = {Vehtari, Aki and Gelman, Andrew},
	eprint = {1507.02646},
	eprinttype = {arxiv},
	file = {/Users/lol553/Zotero/storage/5LI2UYIQ/Vehtari and Gelman - 2015 - Pareto Smoothed Importance Sampling.pdf;/Users/lol553/Zotero/storage/SNLK5SN6/1507.html},
	journal = {arXiv:1507.02646 [stat]},
	keywords = {Statistics - Computation,Statistics - Methodology},
	month = jul,
	primaryclass = {stat},
	title = {Pareto {{Smoothed Importance Sampling}}},
	url = {http://arxiv.org/abs/1507.02646},
	urldate = {2015-07-20},
	year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1507.02646}}

@article{vehtari_practical_2017,
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	doi = {10.1007/s11222-016-9696-4},
	file = {/Users/lol553/Zotero/storage/96268KLS/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf},
	issn = {1573-1375},
	journal = {Statistics and Computing},
	langid = {english},
	month = sep,
	number = {5},
	pages = {1413--1432},
	title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
	url = {https://doi.org/10.1007/s11222-016-9696-4},
	urldate = {2023-02-15},
	volume = {27},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-016-9696-4}}

@article{smith_prediction_2022,
	abstract = {Predictive modeling uncovers knowledge and insights regarding a hypothesized data generating mechanism (DGM). Results from different studies on a complex DGM, derived from different data sets, and using complicated models and algorithms, are hard to quantitatively compare due to random noise and statistical uncertainty in model results. This has been one of the main contributors to the replication crisis in the behavioral sciences. The contribution of this paper is to apply prediction scoring to the problem of comparing two studies, such as can arise when evaluating replications or competing evidence. We examine the role of predictive models in quantitatively assessing agreement between two datasets that are assumed to come from two distinct DGMs. We formalize a distance between the DGMs that is estimated using cross validation. We argue that the resulting prediction scores depend on the predictive models created by cross validation. In this sense, the prediction scores measure the distance between DGMs, along the dimension of the particular predictive model. Using human behavior data from experimental economics, we demonstrate that prediction scores can be used to evaluate preregistered hypotheses and provide insights comparing data from different populations and settings. We examine the asymptotic behavior of the prediction scores using simulated experimental data and demonstrate that leveraging competing predictive models can reveal important differences between underlying DGMs. Our proposed cross-validated prediction scores are capable of quantifying differences between unobserved data generating mechanisms and allow for the validation and assessment of results from complex models.},
	author = {Smith, Anna L. and Zheng, Tian and Gelman, Andrew},
	doi = {10.1007/s11222-022-10154-7},
	file = {/Users/lol553/Zotero/storage/EXUMYEYF/Smith et al. - 2022 - Prediction scoring of data-driven discoveries for .pdf},
	issn = {1573-1375},
	journal = {Statistics and Computing},
	langid = {english},
	month = dec,
	number = {1},
	pages = {11},
	title = {Prediction Scoring of Data-Driven Discoveries for Reproducible Research},
	url = {https://doi.org/10.1007/s11222-022-10154-7},
	urldate = {2023-01-26},
	volume = {33},
	year = {2022},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-022-10154-7}}

@article{gelman_understanding_2014,
	abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
	author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
	doi = {10.1007/s11222-013-9416-2},
	file = {/Users/lol553/Zotero/storage/8GN2Z4UZ/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf;/Users/lol553/Zotero/storage/FUT5A66M/fulltext.html;/Users/lol553/Zotero/storage/RNDXPK5A/s11222-013-9416-2.html;/Users/lol553/Zotero/storage/ZB99HL24/s11222-013-9416-2.html},
	issn = {0960-3174, 1573-1375},
	journal = {Statistics and Computing},
	keywords = {AIC,Artificial Intelligence (incl. Robotics),Bayes,Cross-validation,DIC,Prediction,Probability and Statistics in Computer Science,Statistical Theory and Methods,Statistics and Computing/Statistics Programs,WAIC},
	langid = {english},
	month = nov,
	number = {6},
	pages = {997--1016},
	title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
	url = {http://link.springer.com/article/10.1007/s11222-013-9416-2},
	urldate = {2015-01-13},
	volume = {24},
	year = {2014},
	Bdsk-Url-1 = {http://link.springer.com/article/10.1007/s11222-013-9416-2},
	Bdsk-Url-2 = {https://doi.org/10.1007/s11222-013-9416-2}}


@book{collett_modelling_2015,
	Author = {Collett, David},
	File = {/Users/judywu/Zotero/storage/GUJB38IX/Collett - 2015 - Modelling survival data in medical research.pdf;/Users/judywu/Zotero/storage/RLNIHB94/9781498731690.html},
	Publisher = {{Chapman and Hall/CRC}},
	Title = {Modelling Survival Data in Medical Research},
	Year = {2015}}
	
	
@article{CoxD.R.1968AGDo,
issn = {0035-9246},
abstract = {Residuals are usually defined in connection with linear models. Here a more general definition is given and some asymptotic properties found. Some illustrative examples are discussed, including a regression problem involving exponentially distributed errors and some problems concerning Poisson and binomially distributed observations.},
journal = {Journal of the Royal Statistical Society. Series B, Methodological},
pages = {248--275},
volume = {30},
publisher = {Royal Statistical Society},
number = {2},
year = {1968},
title = {A General Definition of Residuals},
copyright = {1968 The Authors},
language = {eng},
address = {London},
author = {Cox, D. R. and Snell, E. J.},
keywords = {Gaussian distributions ; Linear models ; Linear regression ; Mathematical independent variables ; Modeling ; Parametric models ; Random sampling ; Random variables ; Statistical variance ; Statistics},
}

@article{therneau_martingale-based_1990,
	Abstract = {Abstract.  Graphical methods based on the analysis of residuals are considered for the setting of the highly-used Cox (1972) regression model and for the Anders},
	Author = {Therneau, Terry M. and Grambsch, Patricia M. and Fleming, Thomas R.},
	Doi = {10.1093/biomet/77.1.147},
	File = {/Users/judywu/Zotero/storage/FV2GBVXB/Therneau et al. - 1990 - Martingale-based residuals for survival models.pdf;/Users/judywu/Zotero/storage/EIMLJQVY/271076.html},
	Issn = {0006-3444},
	Journal = {Biometrika},
	Language = {en},
	Month = mar,
	Number = {1},
	Pages = {147-160},
	Title = {Martingale-Based Residuals for Survival Models},
	Volume = {77},
	Year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1093/biomet/77.1.147}}

@book{therneau_modeling_2013,
	Abstract = {This is a book for statistical practitioners, particularly those who design and analyze studies for survival and event history data. Its goal is to extend the toolkit beyond the basic triad provided by most statistical packages: the Kaplan-Meier estimator, log-rank test, and Cox regression model. Building on recent developments motivated by counting process and martingale theory, it shows the reader how to extend the Cox model to analyse multiple/correlated event data using marginal and random effects (frailty) models. It covers the use of residuals and diagnostic plots to identify influential or outlying observations, assess proportional hazards and examine other aspects of goodness of fit. Other topics include time-dependent covariates and strata, discontinuous intervals of risk, multiple time scales, smoothing and regression splines, and the computation of expected survival curves. A knowledge of counting processes and martingales is not assumed as the early chapters provide an introduction to this area. The focus of the book is on actual data examples, the analysis and interpretation of the results, and computation. The methods are now readily available in SAS and S-Plus and this book gives a hands-on introduction, showing how to implement them in both packages, with worked examples for many data sets. The authors call on their extensive experience and give practical advice, including pitfalls to be avoided. Terry Therneau is Head of the Section of Biostatistics, Mayo Clinic, Rochester, Minnesota. He is actively involved in medical consulting, with emphasis in the areas of chronic liver disease, physical medicine, hematology, and laboratory medicine, and is an author on numerous papers in medical and statistical journals. He wrote two of the original SAS procedures for survival analysis (coxregr and survtest), as well as the majority of the S-Plus survival functions. Patricia Grambsch is Associate Professor in the Division of Biostatistics, School of Public Health, University of Minnesota. She has collaborated extensively with physicians and public health researchers in chronic liver disease, cancer prevention, hypertension clinical trials and psychiatric research. She is a fellow the American Statistical Association and the author of many papers in medical and statistical journals.},
	Author = {Therneau, Terry M. and Grambsch, Patricia M.},
	Isbn = {978-1-4757-3294-8},
	Keywords = {Mathematics / Applied,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Medical / Biostatistics},
	Language = {en},
	Month = nov,
	Publisher = {{Springer Science \& Business Media}},
	Shorttitle = {Modeling {{Survival Data}}},
	Title = {Modeling {{Survival Data}}: {{Extending}} the {{Cox Model}}},
	Year = {2013}}

@book{mccullagh_generalized_1989,
	Abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
	Author = {McCullagh, P. and Nelder, John A.},
	Isbn = {978-0-412-31760-6},
	Keywords = {Mathematics / Probability \& Statistics / General},
	Language = {en},
	Month = aug,
	Publisher = {{CRC Press}},
	Title = {Generalized {{Linear Models}}, {{Second Edition}}},
	Year = {1989}}

@article{grambsch_proportional_1994-1,
  ids = {grambsch\_proportional\_1994},
  title = {Proportional Hazards Tests and Diagnostics Based on Weighted Residuals},
  author = {Grambsch, Patricia M. and Therneau, Terry M.},
  year = {1994},
  volume = {81},
  pages = {515--526},
  publisher = {{Oxford University Press}},
  file = {/Users/lol553/Zotero/storage/8NDHN62Z/Grambsch and Therneau - 1994 - Proportional hazards tests and diagnostics based o.pdf;/Users/lol553/Zotero/storage/CG5CDJ3Z/Grambsch and Therneau - 1994 - Proportional hazards tests and diagnostics based o.pdf;/Users/lol553/Zotero/storage/PYHKZWEP/257037.html;/Users/lol553/Zotero/storage/SWFXV2C5/257037.html},
  journal = {Biometrika},
  number = {3}
}

@article{LiLonghai2015Acpe,
issn = {0960-3174},
abstract = {Looking at predictive accuracy is a traditional method for comparing models. A natural method for approximating out-of-sample predictive accuracy is leave-one-out cross-validation (LOOCV)—we alternately hold out each case from a full dataset and then train a Bayesian model using Markov chain Monte Carlo without the held-out case; at last we evaluate the posterior predictive distribution of all cases with their actual observations. However, actual LOOCV is time-consuming. This paper introduces two methods, namely iIS and iWAIC, for approximating LOOCV with only Markov chain samples simulated from a posterior based on a
full
dataset. iIS and iWAIC aim at improving the approximations given by importance sampling (IS) and WAIC in Bayesian models with possibly correlated latent variables. In iIS and iWAIC, we first integrate the predictive density over the distribution of the latent variables associated with the held-out without reference to its observation, then apply IS and WAIC approximations to the integrated predictive density. We compare iIS and iWAIC with other approximation methods in three kinds of models: finite mixture models, models with correlated spatial effects, and a random effect logistic regression model. Our empirical results show that iIS and iWAIC give substantially better approximates than non-integrated IS and WAIC and other methods.},
journal = {Statistics and computing},
pages = {881--897},
volume = {26},
publisher = {Springer US},
number = {4},
year = {2015},
title = {Approximating cross-validatory predictive evaluation in Bayesian latent variable models with integrated IS and WAIC},
copyright = {Springer Science+Business Media New York 2015},
language = {eng},
address = {New York},
author = {Li, Longhai and Qiu, Shi and Zhang, Bei and Feng, Cindy X},
keywords = {Analysis ; Article ; Artificial Intelligence ; Markov processes ; Mathematics and Statistics ; Models ; Monte Carlo method ; Probability and Statistics in Computer Science ; Statistical Theory and Methods ; Statistics ; Statistics and Computing/Statistics Programs},
}

@article{MarshallE.C.2003Acpc,
issn = {0277-6715},
abstract = {When fitting complex hierarchical disease mapping models, it can be important to identify regions that diverge from the assumed model. Since full leave‐one‐out cross‐validatory assessment is extremely time‐consuming when using Markov chain Monte Carlo (MCMC) estimation methods, Stern and Cressie consider an importance sampling approximation. We show that this can be improved upon through replication of both random effects and data. Our approach is simple to apply, entirely generic, and may aid the criticism of any Bayesian hierarchical model. Copyright © 2003 John Wiley & Sons, Ltd.},
journal = {Statistics in medicine},
pages = {1649--1660},
volume = {22},
publisher = {John Wiley & Sons, Ltd},
number = {10},
year = {2003},
title = {Approximate cross-validatory predictive checks in disease mapping models},
copyright = {Copyright © 2003 John Wiley & Sons, Ltd.},
language = {eng},
address = {Chichester, UK},
author = {Marshall, E. C. and Spiegelhalter, D. J.},
keywords = {Bayes Theorem ; Bayesian ; Biological and medical sciences ; cross-validation ; disease mapping ; Humans ; Lip Neoplasms - epidemiology ; Medical sciences ; Models Statistical ; predictive model checks ; Predictive Value of Tests ; Risk Assessment ; Scotland - epidemiology ; Small-Area Analysis},
}

@article{MarshallE.C.2007IoiB,
issn = {1936-0975},
journal = {Bayesian analysis},
volume = {2},
number = {2},
year = {2007},
title = {Identifying outliers in Bayesian hierarchical models: a simulation-based approach},
language = {eng},
author = {Marshall, E. C. and Spiegelhalter, D. J.},
}

@article{MCGILCHRISTC.A1991RwFi,
issn = {0006-341X},
abstract = {In studies of survival, the hazard function for each individual may depend on observed risk variables but usually not all such variables are known or measurable. This unknown factor of the hazard function is usually termed the individual heterogeneity or frailty. When survival is time to the occurrence of a particular type of event and more than one such time may be obtained for each individual, frailty is a common factor among such recurrence times. A model including frailty is fitted to such repeated measures of recurrence times.},
journal = {Biometrics},
pages = {461--466},
volume = {47},
publisher = {Biometric Society},
number = {2},
year = {1991},
title = {Regression with Frailty in Survival Analysis},
copyright = {Copyright 1991 The Biometric Society},
language = {eng},
address = {Malden, MA},
author = {MCGILCHRIST, C. A and AISBETT, C. W},
keywords = {Adolescent ; Adult ; Aged ; Analytical estimating ; Biological and medical sciences ; Biometrics ; Biometry ; Catheters ; Censorship ; Child ; Female ; Fundamental and applied biological sciences. Psychology ; General aspects ; Humans ; Infection - etiology ; Infections ; Kidneys ; Male ; Maximum likelihood estimation ; Middle Aged ; Models Statistical ; Preliminary estimates ; Proportional Hazards Models ; Regression Analysis ; Renal Dialysis - adverse effects ; Statistical variance ; Survival Analysis},
}

@article{MCGILCHRISTC.A1991RwFi,
issn = {0006-341X},
abstract = {In studies of survival, the hazard function for each individual may depend on observed risk variables but usually not all such variables are known or measurable. This unknown factor of the hazard function is usually termed the individual heterogeneity or frailty. When survival is time to the occurrence of a particular type of event and more than one such time may be obtained for each individual, frailty is a common factor among such recurrence times. A model including frailty is fitted to such repeated measures of recurrence times.},
journal = {Biometrics},
pages = {461--466},
volume = {47},
publisher = {Biometric Society},
number = {2},
year = {1991},
title = {Regression with Frailty in Survival Analysis},
copyright = {Copyright 1991 The Biometric Society},
language = {eng},
address = {Malden, MA},
author = {MCGILCHRIST, C. A and AISBETT, C. W},
keywords = {Adolescent ; Adult ; Aged ; Analytical estimating ; Biological and medical sciences ; Biometrics ; Biometry ; Catheters ; Censorship ; Child ; Female ; Fundamental and applied biological sciences. Psychology ; General aspects ; Humans ; Infection - etiology ; Infections ; Kidneys ; Male ; Maximum likelihood estimation ; Middle Aged ; Models Statistical ; Preliminary estimates ; Proportional Hazards Models ; Regression Analysis ; Renal Dialysis - adverse effects ; Statistical variance ; Survival Analysis},
}

@article{KaragrigoriouAlex2011FMiS,
series = {Journal of Applied Statistics},
issn = {0266-4763},
journal = {Journal of Applied Statistics},
pages = {2988--2989},
volume = {38},
publisher = {Taylor & Francis},
number = {12},
year = {2011},
title = {Frailty Models in Survival Analysis},
copyright = {Copyright Alex Karagrigoriou 2011},
language = {eng},
author = {Karagrigoriou, Alex},
}

@article{henderson_analysis_2001,
  title = {Analysis of {{Multivariate Survival Data}}. {{Philip Hougaard}}, {{Springer}}, {{New York}}, 2000. {{No}}. of Pages: Xvii+542. {{Price}}: \$84.95. {{ISBN}} 0-387-98873-4},
  shorttitle = {Analysis of {{Multivariate Survival Data}}. {{Philip Hougaard}}, {{Springer}}, {{New York}}, 2000. {{No}}. of Pages},
  author = {Henderson, Rob},
  year = {2001},
  month = aug,
  volume = {20},
  pages = {2533--2534},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.938},
  file = {/Users/judywu/Zotero/storage/HI3NFWP4/Henderson - 2001 - Analysis of Multivariate Survival Data. Philip Hou.pdf},
  journal = {Statist. Med.},
  language = {en},
  number = {16}
}


@book{duchateau_frailty_2008,
  title = {The Frailty Model},
  author = {Duchateau, Luc and Janssen, Paul},
  year = {2008},
  series = {Statistics for Biology and Health},
  publisher = {{Springer Verlag}},
  address = {{New York}},
  isbn = {978-0-387-72834-6},
  langid = {english},
  lccn = {QA276 .D795 2008},
  keywords = {Congresses,Statistics,Survival Analysis},
  annotation = {OCLC: ocn156812994},
  file = {/Users/judywu/Zotero/storage/ETIS3LJM/Duchateau and Janssen - 2008 - The frailty model.pdf}
}

@article{hougaard_frailty_1995,
  title = {Frailty Models for Survival Data},
  author = {Hougaard, Philip},
  year = {1995},
  volume = {1},
  pages = {255--273},
  issn = {1380-7870, 1572-9249},
  doi = {10.1007/BF00985760},
  abstract = {A frailty model is a random effects model for time variables, where the random effect (the frailty) has a multiplicative effect on the hazard. It can be used for univariate (independent) failure times, i.e. to describe the influence of unobserved covariates in a proportiOnal hazards model. More interesting, however, is to consider multivariate (dependent) failure times generated as conditionally independent times given the frailty. This approach can be used both for survival times for individuals, like twins or family members, and for repeated events for the same individual. The standard assumption is to use a gamma distribution for the frailty, but this is a restriction that implies that the dependence is most important for late events. More generally, the distribution can be stable, inverse Gaussian, or follow a power variance function exponential family. Theoretically, large differences are seen between the choices. In practice, using the largest model makes it possible to allow for more general de\textasciitilde ndence structures, without making the formulas too complicated.},
  file = {/Users/judywu/Zotero/storage/JGTNZA5X/Hougaard - 1995 - Frailty models for survival data.pdf},
  journal = {Lifetime Data Anal},
  language = {en},
  number = {3}
}


@article{SCHOENFELDDAVID1982Prft,
issn = {0006-3444},
abstract = {Residuals are defined for the proportional hazards regression model introduced by Cox (1972). These residuals can be plotted against time to test the proportional hazards assumption. Histograms of these residuals can be used to examine fit and detect outlying covariate values.},
journal = {Biometrika},
pages = {239--241},
volume = {69},
publisher = {Oxford University Press},
number = {1},
year = {1982},
title = {Partial residuals for the proportional hazards regression model},
copyright = {Copyright 1982 Biometrika Trust},
language = {eng},
author = {SCHOENFELD, DAVID},
keywords = {Censoring ; Failure time data ; Health hazards ; Miscellanea ; Proportional hazard ; Regression analysis ; Residual ; Residual neoplasm},
}

@article{LiLonghai2021Mdfc,
issn = {0277-6715},
abstract = {Residuals in normal regression are used to assess a model's goodness‐of‐fit (GOF) and discover directions for improving the model. However, there is a lack of residuals with a characterized reference distribution for censored regression. In this article, we propose to diagnose censored regression with normalized randomized survival probabilities (RSP). The key idea of RSP is to replace the survival probability (SP) of a censored failure time with a uniform random number between 0 and the SP of the censored time. We prove that RSPs always have the uniform distribution on (0, 1) under the true model with the true generating parameters. Therefore, we can transform RSPs into normally distributed residuals with the normal quantile function. We call such residuals by normalized RSP (NRSP residuals). We conduct simulation studies to investigate the sizes and powers of statistical tests based on NRSP residuals in detecting the incorrect choice of distribution family and nonlinear effect in covariates. Our simulation studies show that, although the GOF tests with NRSP residuals are not as powerful as a traditional GOF test method, a nonlinear test based on NRSP residuals has significantly higher power in detecting nonlinearity. We also compared these model diagnostics methods with a breast‐cancer recurrent‐free time dataset. The results show that the NRSP residual diagnostics successfully captures a subtle nonlinear relationship in the dataset, which is not detected by the graphical diagnostics with CS residuals and existing GOF tests.},
journal = {Statistics in medicine},
pages = {1482--1497},
volume = {40},
publisher = {Wiley Subscription Services, Inc},
number = {6},
year = {2021},
title = {Model diagnostics for censored regression via randomized survival probabilities},
copyright = {2020 John Wiley & Sons, Ltd.},
language = {eng},
address = {England},
author = {Li, Longhai and Wu, Tingxuan and Feng, Cindy},
keywords = {Cox‐Snell residual ; goodness‐of‐fit ; Mathematical models ; Medical research ; model checking ; Probability ; quantile residual ; Regression analysis ; residual diagnostics ; Survival analysis},
}


@article{BalanTheodorA2020Atof,
issn = {0962-2802},
abstract = {The hazard function plays a central role in survival analysis. In a homogeneous population, the distribution of the time to event, described by the hazard, is the same for each individual. Heterogeneity in the distributions can be accounted for by including covariates in a model for the hazard, for instance a proportional hazards model. In this model, individuals with the same value of the covariates will have the same distribution. It is natural to think that not all covariates that are thought to influence the distribution of the survival outcome are included in the model. This implies that there is unobserved heterogeneity; individuals with the same value of the covariates may have different distributions. One way of accounting for this unobserved heterogeneity is to include random effects in the model. In the context of hazard models for time to event outcomes, such random effects are called frailties, and the resulting models are called frailty models. In this tutorial, we study frailty models for survival outcomes. We illustrate how frailties induce selection of healthier individuals among survivors, and show how shared frailties can be used to model positively dependent survival outcomes in clustered data. The Laplace transform of the frailty distribution plays a central role in relating the hazards, conditional on the frailty, to hazards and survival functions observed in a population. Available software, mainly in R, will be discussed, and the use of frailty models is illustrated in two different applications, one on center effects and the other on recurrent events.},
journal = {Statistical Methods in Medical Research},
pages = {3424--3454},
volume = {29},
publisher = {SAGE Publications},
number = {11},
year = {2020},
title = {A tutorial on frailty models},
copyright = {The Author(s) 2020},
language = {eng},
address = {London, England},
author = {Balan, Theodor A and Putter, Hein},
keywords = {Correlated failure times ; Frailty ; frailty models ; Hazards ; Heterogeneity ; Random effects ; random effects models ; Recurrent ; Recurrent events ; Review ; Statistical models ; Survival ; Survival analysis ; unobserved heterogeneity},
}


@article{hillis_residual_1995,
	title = {Residual plots for the censored data linear regression model},
	volume = {14},
	issn = {0277-6715},
	doi = {10.1002/sim.4780141808},
	abstract = {To be consistent, censored data linear regression estimators typically require a correctly specified linear regression function and independent and identically distributed errors. For uncensored data one can assess these model assumptions informally by examining plots of the residuals against the independent variables or fitted values. In this paper I propose plots for censored data analogous to these uncensored data residual plots. One can use such plots in the same way as their uncensored data counterparts for checking model assumptions; if the model assumptions are correct, then the plots should exhibit a random scatter. I show that the proposed plots are useful in selecting a linear regression model for the Stanford heart transplant data.},
	language = {eng},
	number = {18},
	journal = {Statistics in Medicine},
	author = {Hillis, Stephen L.},
	year = {1995},
	keywords = {Linear Models;},
	pages = {2023--2036},
}


@article{residual_mixturecuremodel_2017,
	Abstract = {Summary Model diagnosis, an important issue in statistical modeling, has not yet been addressed adequately for cure models. We focus on mixture cure models in this work and propose some residual-based methods to examine the fit of the mixture cure model, particularly the fit of the latency part of the mixture cure model. The new methods extend the classical residual-based methods to the mixture cure model. Numerical work shows that the proposed methods are capable of detecting lack-of-fit of a mixture cure model, particularly in the latency part, such as outliers, improper covariate functional form, or nonproportionality in hazards if the proportional hazards assumption is employed in the latency part. The methods are illustrated with two real data sets that were previously analyzed with mixture cure models.},
	Author = {Peng, Yingwei and Taylor, Jeremy M. G.},
	Doi = {10.1111/biom.12582},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12582},
	Journal = {Biometrics},
	Number = {2},
	Pages = {495-505},
	Title = {Residual-Based Model Diagnosis Methods for Mixture Cure Models},
	Volume = {73},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1111/biom.12582}}
	
@article{residual_Tree-Structured_survival,
	Abstract = {Abstract Extensions of various non-parametric regression techniques (for example, additive models, trees, MARS) have been devised for right censored survival data. These approaches directly handle the difficulties posed by censoring. However, it is possible to bypass these difficulties by utilizing standard non-parametric regression procedures applied with (say) martingale residuals as outcome. Analytic correspondences between the direct and residual-based approaches have been established for additive models while more qualitative comparisons have been provided for MARS. Here we develop such correspondences for tree-structured regression. In particular, we provide an analytic relationship between logrank and martingale residual sum-of-squares split functions that explains the widely observed similarity of the resultant trees. Further investigation is provided by simulation and an illustrative example using time to AIDS with data deriving from a Western Australian HIV cohort study. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd.},
	Author = {Kele{\c s}, S{\"u}nd{\"u}z and Segal, Mark R.},
	Doi = {10.1002/sim.981},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.981},
	Journal = {Statistics in Medicine},
	Keywords = {censoring AIDS,martingale residuals,recursive partitioning,survival trees},
	Number = {2},
	Pages = {313-326},
	Title = {Residual-Based Tree-Structured Survival Analysis},
	Volume = {21},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1002/sim.981}}
	
@article{residual_ph_interval_censored,
	Abstract = {Summary. We develop diagnostic tools for use with proportional hazards models for interval-censored survival data. We propose counterparts t o the Cox-Snell, Lagakos (or martingale), deviance, and Schoenfeld residuals. Many of the properties of these residuals carry over to the interval-censored case. In particular, the interval-censored versions of the Lagakos and Schoenfeld residuals may be derived as components of suitable score statistics. The Lagakos residuals may be used to check regression relationships, while the Schoenfeld residuals can help to detect nonproportional hazards in semiparametric models. The methods apply to parametric models and to the semiparametric model with discrete observation times.},
	Author = {Farrington, C. P.},
	Doi = {10.1111/j.0006-341X.2000.00473.x},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2000.00473.x},
	Journal = {Biometrics},
	Keywords = {Interval-censoring,Proportional hazards,Regression diagnostic,Residual,Survival},
	Number = {2},
	Pages = {473-482},
	Title = {Residuals for Proportional Hazards Models with Interval-Censored Survival Data},
	Volume = {56},
	Year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1111/j.0006-341X.2000.00473.x}}

@article{deviance_normal_scores,
	Abstract = {{{We discuss the use of normal order statistics plots, based on deviance residuals, to check distributional assumptions in regression models. Continuous and discrete error distributions are considered, as are censored data. Misspecified error distributions and discrimination between competing models are discussed, with an example.}}},
	Author = {Davison, A. C. and Gigli, A.},
	Doi = {10.1093/biomet/76.2.211},
	Eprint = {http://oup.prod.sis.lan/biomet/article-pdf/76/2/211/736860/76-2-211.pdf},
	Issn = {0006-3444},
	Journal = {Biometrika},
	Month = jun,
	Number = {2},
	Pages = {211-221},
	Title = {Deviance Residuals and Normal Scores Plots},
	Volume = {76},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1093/biomet/76.2.211}}
	
@article{lin_checking_1993,
	Abstract = {[This paper presents a new class of graphical and numerical methods for checking the adequacy of the Cox regression model. The procedures are derived from cumulative sums of martingale-based residuals over follow-up time and/or covariate values. The distributions of these stochastic processes under the assumed model can be approximated by zero-mean Gaussian processes. Each observed process can then be compared, both visually and analytically, with a number of simulated realizations from the approximate null distribution. These comparisons enable the data analyst to assess objectively how unusual the observed residual patterns are. Special attention is given to checking the functional form of a covariate, the form of the link function, and the validity of the proportional hazards assumption. An omnibus test, consistent against any model misspecification, is also studied. The proposed techniques are illustrated with two real data sets.]},
	Author = {Lin, D. Y. and Wei, L. J. and Ying, Z.},
	Doi = {10.2307/2337177},
	Issn = {00063444},
	Journal = {Biometrika},
	Number = {3},
	Pages = {557-572},
	Title = {Checking the {{Cox Model}} with {{Cumulative Sums}} of {{Martingale}}-{{Based Residuals}}},
	Volume = {80},
	Year = {1993},
	Bdsk-Url-1 = {https://doi.org/10.2307/2337177}}


@article{law_residual_2017,
  title = {Residual Plots for Linear Regression Models with Censored Outcome Data: {{A}} Refined Method for Visualizing Residual Uncertainty},
  shorttitle = {Residual Plots for Linear Regression Models with Censored Outcome Data},
  author = {Law, Martin and Jackson, Dan},
  year = {2017},
  month = apr,
  volume = {46},
  pages = {3159--3171},
  issn = {0361-0918},
  doi = {10.1080/03610918.2015.1076470},
  abstract = {Residual plots are a standard tool for assessing model fit. When some outcome data are censored, standard residual plots become less appropriate. Here, we develop a new procedure for producing residual plots for linear regression models where some or all of the outcome data are censored. We implement two approaches for incorporating parameter uncertainty. We illustrate our methodology by examining the model fit for an analysis of bacterial load data from a trial for chronic obstructive pulmonary disease. Simulated datasets show that the method can be used when the outcome data consist of a variety of types of censoring.},
  file = {/Users/lol553/Zotero/storage/RC47472F/Law and Jackson - 2017 - Residual plots for linear regression models with c.pdf;/Users/lol553/Zotero/storage/5M4ZNYSS/03610918.2015.html},
  journal = {Communications in Statistics - Simulation and Computation},
  keywords = {62-09,Bootstrapping,Censoring,Linear models,Multiple imputation,Regression diagnostics,Residuals},
  number = {4}
}


@article{shepherd_probability-scale_2016-1,
  ids = {shepherd\_probability-scale\_2016},
  title = {Probability-Scale Residuals for Continuous, Discrete, and Censored Data},
  author = {Shepherd, Bryan E. and Li, Chun and Liu, Qi},
  year = {2016},
  month = dec,
  volume = {44},
  pages = {463--479},
  issn = {0319-5724},
  doi = {10.1002/cjs.11302},
  abstract = {We describe a new residual for general regression models, defined as pr(Y* {$<$} y) - pr(Y* {$>$} y), where y is the observed outcome and Y* is a random variable from the fitted distribution. This probability-scale residual can be written as E \{sign(y, Y*)\} whereas the popular observed-minus-expected residual can be thought of as E(y - Y*). Therefore, the probability-scale residual is useful in settings where differences are not meaningful or where the expectation of the fitted distribution cannot be calculated. We present several desirable properties of the probability-scale residual that make it useful for diagnostics and measuring residual correlation, especially across different outcome types. We demonstrate its utility for continuous, ordered discrete, and censored outcomes, including current status data, and with various models including Cox regression, quantile regression, and ordinal cumulative probability models, for which fully specified distributions are not desirable or needed, and in some cases suitable residuals are not available. The residual is illustrated with simulated data and real datasets from HIV-infected patients on therapy in the southeastern United States and Latin America.},
  file = {/Users/lol553/Zotero/storage/45KAADFA/Shepherd et al. - 2016 - Probability-scale residuals for continuous, discre.pdf;/Users/lol553/Zotero/storage/UCJRHRQM/Shepherd et al. - 2016 - Probability-scale residuals for continuous, discre.pdf},
  journal = {The Canadian journal of statistics = Revue canadienne de statistique},
  keywords = {Diagnostics,Generalized linear model,HIV,MSC 2010: Primary 62G99,quantile regression,rank statistics,secondary 62N99,survival analysis},
  number = {4},
  pmcid = {PMC5364820},
  pmid = {28348453}
}


@article{vaupel_impact_1979,
  title = {The Impact of Heterogeneity in Individual Frailty on the Dynamics of Mortality},
  author = {Vaupel, James W. and Manton, Kenneth G. and Stallard, Eric},
  year = {1979},
  month = aug,
  volume = {16},
  pages = {439--454},
  issn = {0070-3370, 1533-7790},
  doi = {10.2307/2061224},
  abstract = {Life table methods are developed for populations whose members differ in their endowment for longevity. Unlike standard methods, which ignore such heterogeneity, these methods use different calculations to construct cohort, period, and individual life tables. The results imply that standard methods overestimate current life expectancy and potential gains in life expectancy from health and safety interventions, while underestimating rates of individual aging, past progress in reducing mortality, and mortality differentials between pairs of populations. Calculations based on Swedish mortality data suggest that these errors may be important, especially in old age.},
  file = {/Users/judywu/Zotero/storage/RVK5E6VX/Vaupel et al. - 1979 - The impact of heterogeneity in individual frailty .pdf},
  journal = {Demography},
  language = {en},
  number = {3}
}


@article{clayton_model_2022,
  title = {A {{Model}} for {{Association}} in {{Bivariate Life Tables}} and {{Its Application}} in {{Epidemiological Studies}} of {{Familial Tendency}} in {{Chronic Disease Incidence}}},
  author = {Clayton, D G},
  year = {2022},
  pages = {12},
  abstract = {The application of Cox's (1972) regression model for censored survival data to epidemiological studies of chronic disease incidence is discussed. A related model for association in bivariate survivorship time distributions is proposed for the analysis of familial tendency in disease incidence. The possible extension of the model to general multivariate survivorship distributions is indicated.},
  file = {/Users/judywu/Zotero/storage/6C3I5VSP/Clayton - 2022 - A Model for Association in Bivariate Life Tables a.pdf},
  language = {en}
}


@article{HanagalDD2015Msdu,
issn = {0962-2802},
journal = {Statistical methods in medical research},
pages = {936--936},
volume = {24},
publisher = {SAGE Publications},
number = {6},
year = {2015},
title = {Modeling survival data using frailty models},
copyright = {The Author(s) 2011},
language = {eng},
address = {London, England},
author = {Hanagal, DD},
keywords = {Frailty ; Survival analysis},
}



@Manual{survival-package,
    title = {A Package for Survival Analysis in R},
    author = {Terry M Therneau},
    year = {2022},
    note = {R package version 3.3-1},
    url = {https://CRAN.R-project.org/package=survival},
  }

  @article{duchateau_penalized_2004,
  title = {Penalized {{Partial Likelihood}} for {{Frailties}} and {{Smoothing Splines}} in {{Time}} to {{First Insemination Models}} for {{Dairy Cows}}},
  author = {Duchateau, Luc and Janssen, Paul},
  year = {2004},
  month = sep,
  volume = {60},
  pages = {608--614},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2004.00209.x},
  abstract = {In many epidemiological studies time to event data are clustered and the physiolog tionship between (time-dependent) covariates and the log hazard is often not linear as assumed model. Introducing frailties in the Cox model can account for the clustering of the data and s splines can be used to describe nonlinear relations. These two extensions of the Cox model are jointly and it is shown how penalized partial likelihood techniques can be used to fit the exten We demonstrate the need for such a model to study the relation between the physiological cova ureum and protein concentration and the log hazard of first insemination in dairy cows, with t clusters.},
  file = {/Users/judywu/Zotero/storage/D6JZBKTZ/Duchateau and Janssen - 2004 - Penalized Partial Likelihood for Frailties and Smo.pdf},
  journal = {Biometrics},
  language = {en},
  number = {3}
}

@article{ripatti_estimation_2000-1,
  title = {Estimation of {{Multivariate Frailty Models Using Penalized Partial Likelihood}}},
  author = {Ripatti, Samuli and Palmgren, Juni},
  year = {2000},
  month = dec,
  volume = {56},
  pages = {1016--1022},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2000.01016.x},
  abstract = {There exists a growing literature on the estimation of gamma distributed multiplicative shared frailty models. There is, however, often a need to model more complicated frailty structures, but attempts to extend gamma frailties run into complications. Motivated by hip replacement data with a more complicated dependence structure, we propose a model based on multiplicative frailties with a multivariate log-normal joint distribution. We give a justification and an estimation procedure for this generally structured frailty model, which is a generalization of the one presented by McGilchrist (1993, Biometrics 49, 221-225). The estimation is based on Laplace approximation of the likelihood function. This leads to estimating equations based on a penalized fixed effects partial likelihood, where the marginal distribution of the frailty terms determines the penalty term. The tuning parameters of the penalty function, i.e., the frailty variances, are estimated by maximizing an approximate profile likelihood. The performance of the approximation is evaluated by simulation, and the frailty model is fitted to the hip replacement data.},
  file = {/Users/judywu/Zotero/storage/6M7N8SH7/Ripatti and Palmgren - 2000 - Estimation of Multivariate Frailty Models Using Pe.pdf},
  journal = {Biometrics},
  language = {en},
  number = {4}
}


@article{mcgilchrist_reml_1993,
  title = {{{REML Estimation}} for {{Survival Models}} with {{Frailty}}},
  author = {McGilchrist, C. A.},
  year = {1993},
  month = mar,
  volume = {49},
  pages = {221},
  issn = {0006341X},
  doi = {10.2307/2532615},
  abstract = {A method of estimation for generalised mixed models is applied to the estimation of regression parameters in proportional hazards models for failure times when there are repeated observations of failure on each subject. The subject effect is incorporated into the model as a random frailty term. Best linear unbiased predictors are used as an initial step in the computation of maximum likelihood and residual maximum likelihood estimates.},
  journal = {Biometrics},
  language = {en},
  number = {1}
}

@article{1972DoPC,
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society. Series B, Methodological},
pages = {202--220},
volume = {34},
number = {2},
year = {1972},
title = {Discussion on Professor Cox's Paper},
copyright = {1972 The Authors},
language = {eng},
}

@article{lin_breslow_2007,
  title = {On the {{Breslow}} Estimator},
  author = {Lin, D. Y.},
  year = {2007},
  month = dec,
  volume = {13},
  pages = {471--480},
  issn = {1380-7870, 1572-9249},
  doi = {10.1007/s10985-007-9048-y},
  abstract = {In his discussion of Cox's (1972) paper on proportional hazards regression, Breslow (1972) provided the maximum likelihood estimator for the cumulative baseline hazard function. This estimator is commonly used in practice. The estimator has also been highly valuable in the further development of Cox regression and semiparametric inference with censored data. The present paper describes the Breslow estimator and its tremendous impact on the theory and practice of survival analysis.},
  file = {/Users/judywu/Zotero/storage/3LXZ6QMH/Lin - 2007 - On the Breslow estimator.pdf},
  journal = {Lifetime Data Anal},
  language = {en},
  number = {4}
}









