\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{abstract}
\usepackage{appendix}
\usepackage[colorlinks,linkcolor=black]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{enumerate}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\numberwithin{equation}{section}
\usepackage{booktabs}
\renewcommand{\baselinestretch}{1.3}
\usepackage{cases}

\title{\textbf{Alternating Direction Method of Multipliers Based on $\ell_{2,0}$-norm for Multiple Measurement Vector Problem}}

\author{Zekun Liu, Siwei Yu\thanks{Corresponding author: siweiyu@hit.edu.cn}\\School of Mathematics, Harbin Institute of Technology 
}

\date{}

\begin{document}

\maketitle

\begin{abstract} 
In this paper, we propose an alternating direction method of multipliers (ADMM)-based optimization algorithm to achieve better undersampling rate for multiple measurement vector (MMV) problem. The core is to introduce the $\ell_{2,0}$-norm sparsity constraint to describe the joint-sparsity of the MMV problem, which is different from the widely used $\ell_{2,1}$-norm constraint in the 
\\
existing research. In order to illustrate the better performance of $\ell_{2,0}$-norm, first this paper proves the equivalence of the sparsity of the row support set of a matrix and its $\ell_{2,0}$-norm. Afterward, the MMV problem based on $\ell_{2,0}$-norm is proposed. Moreover, building on the Kurdyka-Lojasiewicz property, this paper establishes that the sequence generated by ADMM globally converges to the optimal point of the MMV problem. Finally, the performance of our algorithm and comparison with other algorithms under different conditions is studied by simulated examples.
\end{abstract}


\section{Introduction}

Multiple measurement vector (MMV) problem\cite{1} has received considerable attention in the field of compressed sensing\cite{11,12}. In the single measurement vector (SMV) problem, our task is to recover a vector $x$ from $y=Ax$ where $A\in \mathbb{R}^{m\times n}(m<n)$ and $y\in \mathbb{R}^{m}$ is given. Since $m<n$, the problem is ill-posed, with the prior information that $x$ is sparse enough\cite{13}, we can solve
\begin{equation}\label{equ:1.1}
\begin{aligned}
        &\min_{x\in \mathbb{R}^{n}}\left \| x \right \|_{0}\\
        &s.t.\enspace y=Ax,
\end{aligned}
\end{equation}
where $\left \| \cdot \right \|_{0}$ of a vector is the number of its nonzero entries to obtain the unique solution. However, problem (\ref{equ:1.1}) is NP-hard, it is hard to be solved directly. A usual approach to overcome this is to relax the $\ell_{0}$-norm to the $\ell_{1}$-norm which is known as the basis pursuit\cite{14}:
\begin{equation}\label{equ:1.2}
\begin{aligned}
        &\min_{x\in \mathbb{R}^{n}}\left \| x \right \|_{1}\\
        &s.t.\enspace y=Ax.
\end{aligned}
\end{equation}
It is a convex optimization problem which can be solved much more efficiently, and it yeilds the same solution as problem (\ref{equ:1.1}) under suitable conditions\cite{12}.

As an extension of SMV problem, MMV problem considers the case where a group of measurement vectors obtained from a group of single vectors which are jointly sparse, that is
\begin{displaymath}
    y^{j}=Ax^{j}, j=1,2,\cdots,J,
\end{displaymath}
where all of the vectors $x^{j}$ have nonzero entries at the same locations. Such a problem has many applications, including hyperspectral imagery\cite{15}, computing sparse solutions to linear inverse problems\cite{16}, Neuromagnetic imaging\cite{17}, Source localization\cite{18}, and equalization of sparse communication channels\cite{19}. Denote $Y=(y^{1},y^{2},\cdots,y^{J})\in \mathbb{R}^{m\times J}$, MMV problem is to recover $X=(x^{1},x^{2},\cdots,x^{J})$ from the observations $Y=AX$ where $X$ has nonzero entries in only a small number of rows. The most widely studied approach is the one based on $\ell_{2,1}$-norm minimization\cite{16,7,20}:
\begin{equation}\label{equ:1.3}
\begin{aligned}
        &\min_{X\in \mathbb{R}^{n\times J}}\left \| X \right \|_{2,1}\\
        &\quad s.t.\enspace Y=AX,
\end{aligned}
\end{equation}
where the generalized $\ell_{p,q}$-norm of a matrix $X$ is defined as
\begin{displaymath}
    \left \| X \right \|_{p,q}:=(\sum_{i=1}^{n}\left \|X^{i} \right \|_{p}^{q})^{\frac{1}{q}},
\end{displaymath}
and $X^{i}$ is the $i$th row of $X$. Benefited from the additional joint-sparsity property, the recovery performance in MMV problem is better than that in SMV problem\cite{7,20}.

In the past decades, several recovery algorithms for the MMV problem have been proposed\cite{7,16,21,22}. Algorithm in \cite{21} employ the greedy pursuit strategy to recover the signals since the joint-sparsity recovery problem is NP-hard. As a similar convex relaxation technique in the SMV problem (\ref{equ:1.2}), researches in \cite{16,7,20} study the $\ell_{2,1}$-norm to solve the MMV problem. In the application for hyperspectral imagery, works in \cite{15} apply the alternating direction method of multipliers to solve problem (\ref{equ:1.3}). Studies have shown that most theoretical results on the relaxation of the SMV problem can be extended to the MMV problem\cite{7}. However, as (\ref{equ:1.2}) is a convex relaxation of (\ref{equ:1.1}) which yields the same solution only under suitable conditions\cite{12}, so (\ref{equ:1.3}) is only a convex relaxation of $\ell_{2,0}$-norm minimization problem. (\ref{equ:1.3}) can not obtain the accurate solution under some bad situations while $\ell_{2,0}$-norm minimization problem can. Hence there exist some drawbacks which limit the use of previous algorithms.

In this paper, instead of considering the widely used convex relaxation $\ell_{2,1}$-norm minimization problem, we directly study the original $\ell_{2,0}$-norm minimization problem in the MMV problem. We show the equivalence of the joint-sparsity property and the $\ell_{2,0}$-norm in the MMV problem. Then we reformulate the MMV problem via the sparsity constraint in \cite{7}. Next we propose our algorithm called MMV-ADMM-$\ell_{2,0}$ by applying the alternating direction method of multipliers\cite{8} to our reformulated problem. Theoretical analysis shows that our algorithm is globally convergent to the unique optimal point, which is the solution of the MMV problem. Compared with existing algorithms, our algorithm can solve the MMV problem when the sparsity of the signals is large or number of sensors is small, and it can achieve a better undersampling rate.

The paper is organized as follows: In Section 2, we overview some definitions used for further analysis. We propose our problem by reformulating the MMV problem in Section 3. In Section 4, we give our algorithm called MMV-ADMM-$\ell_{2,0}$ and discuss the subproblems of it in detail. Section 5 establishes the convergence results on our algorithm. In Section 6, we design experiments to test the validity of our algorithm and compare it with other algorithms. We close our paper with the conclusions in Section 7.


\section{Preliminaries}

In this section, we give some notations and preliminaries used for further analysis. For any vector $x=
\left ( x_{1},x_{2},\cdots,x_{N} \right )^{T}\in \mathbb{R}^{N}$, the sparse support set of $x$ is defined by
\begin{displaymath}
Supp(x)=\left \{ i|x_{i} \ne 0 \right \} \subseteq \left \{1,2,\cdots,N  \right \}.
\end{displaymath}
Recall that the assumption of the multiple measurement vector (MMV) problem \cite{1} for a joint-sparse regression is that the set of all vectors $\left \{ x^{j}\right \}_{j=1}^{J} \in \mathbb{R}^{N}$ has the same sparse support set, which means
\begin{displaymath}
    Supp(x^{1})=Supp(x^{2})=\cdots=Supp(x^{J}).
\end{displaymath}
\textbf{Definiton 2.1:} For a set of vectors $\left \{ x^{j}\right \}_{j=1}^{J} \in \mathbb{R}^{N}$, the common sparse support set is defined by
\begin{displaymath}
    Supp(\left \{ x^{j}\right \}_{j=1}^{J})=\bigcup_{j=1}^{J} Supp(x^{j}).
\end{displaymath}
In fact, although real datasets are almost impossible to satisfy the above assumption, MMV problem works well when real datas are joint-sparse enough\cite{7}. 

Let us recall a few definitions for further analysis\cite{2,3,4,5}.
\\
\textbf{Definition 2.2:} Let $f:\mathbb{R}^{m\times n}\to \mathbb{\bar{R}}$ be a generalized real function.
\begin{enumerate}[(i)]
\item For a nonempty set $\mathcal{X}$, we call $f$ proper to $\mathcal{X}$ if there exist $x\in \mathcal{X}$ such that $f(x)<+\infty$ and for any $x\in \mathcal{X}, f(x)>-\infty$.
\item For a proper function $f$, the domain is defined through
\begin{displaymath}
    domf=\left \{ x|f(x)<+\infty \right \}.
\end{displaymath}
\item For a generalized real function $f$, we call $f$ a lower semicontinuous function if for any $x\in \mathbb{R}^{m\times n}, \liminf_{y\to x}f(y)\ge f(x)$.
\item For a generalized real function $f$, we call $f$ a closed function if its epigraph 
\begin{displaymath}
    epif=\left \{ (x,t)\in \mathbb{R}^{m\times n}\times \mathbb{R}|f(x)\le t\right \}
\end{displaymath}
is a closed set.
\end{enumerate}

Denote $f$ be a generalized real function, $f$ is a lower semicontinuous function \textit{if and only if} $f$ is a closed function. 
\\
\textbf{Definition 2.3:} Let $f:\mathbb{R}^{m\times n}\to \mathbb{R}$ be a matrix function, if there exist a matrix $G\in \mathbb{R}^{m\times n}$, such that for any direction $V\in \mathbb{R}^{m\times n}$ 
\begin{equation}\label{equ:2.1}
    \lim_{t \to 0} \frac{f(X+tV)-f(X)-t\left \langle G,V \right \rangle }{t} =0,
\end{equation}
then we call $f$ is Gâteaux differentiable and $G$ is the gradient of $f$ at $X$.
\\
\textbf{Definition 2.4:} Let $f:\mathbb{R}^{m\times n}\to \mathbb{R}\cup \left \{ +\infty \right \}$ be a proper lower semicontinuous function.
\begin{enumerate}[(i)]
    \item The Fréchet subdifferential of $f$ at $x\in domf$, written $\hat{\partial}f(x)$, is defined by
    \begin{displaymath}
        \hat{\partial}f(x)=\left \{ z\in \mathbb{R}^{m\times n}\mid \liminf_{y\ne x,y\to x} \frac{f(y)-f(x)-\left \langle z,y-x \right \rangle }{\left \| y-x \right \| }\ge 0   \right \}.
    \end{displaymath}
    When $x\notin domf$, set $\hat{\partial}f(x)=\emptyset$.
    \item The subdifferential of $f$ at $x\in domf$, written $\partial f(x)$, is defined by
    \begin{displaymath}
        \partial f(x)=\left \{ z\in \mathbb{R}^{m\times n}\mid \exists x_{k}\to x,f(x_{k})\to f(x),z_{k}\in  \hat{\partial}f(x_{k}),with\enspace z_{k}\to z \right \}.
    \end{displaymath}
    \item A point that satisfies $0\in \partial f(x)$ is called a critical point, and the set of critical points of $f$ is denoted by $critf$.
\end{enumerate}

From Definition 2.4, we can conclude that
\begin{enumerate}[(i)]
    \item If $f$ is proper lower semicontinuous and $g$ is continuously differentiable, then $\partial (f+g)(x)=\partial f(x)+\nabla g(x)$ for any $x\in domf$.
    \item A necessary condition for $x\in domf$ to be a minimizer of $f$ is $x\in critf$.
\end{enumerate}

Consider the general constrained optimization problem:
\begin{equation}\label{equ:2.2}
    \begin{aligned}
        &\min_{x} \enspace f(x), \\
        &s.t. \quad c_{i}(x) \le 0, \enspace i\in \mathcal{I},\\
        &\qquad  \, \, c_{i}(x)=0, \enspace i\in \mathcal{E},
    \end{aligned}
\end{equation}
where $\mathcal{I}$ and $\mathcal{E}$ are respectively the inequality and the equality constraints subscript set.
\\
\textbf{Definition 2.5:} We say that $(x^{\ast},\lambda^{\ast})$ is a Karush-Kuhn-Tucker (KKT)
point of the optimization problem (\ref{equ:2.2}) if it satisfies the Karush-Kuhn-Tucker (KKT) conditions:
\begin{equation}\label{equ:2.3}
    \begin{aligned}
        &0\in \partial f(x^{\ast})+\sum_{i\in \mathcal{I}\cup \mathcal{E}}\lambda_{i}^{\ast}\partial c_{i}(x^{\ast}),\\
        &c_{i}(x^{\ast})=0,\enspace i\in \mathcal{E},\\
        &c_{i}(x^{\ast})\le 0,\enspace i\in \mathcal{I},\\
        &\lambda_{i}^{\ast}\ge 0,\enspace i \in \mathcal{I},\\
        &\lambda_{i}^{\ast}c_{i}(x^{\ast})=0,\enspace i\in \mathcal{I}.
    \end{aligned}
\end{equation}

For any subset $S\subseteq \mathbb{R}^{m\times n}$ and any point $x\in \mathbb{R}^{m\times n}$, the distance from $x$ to $S$ is defined through
\begin{displaymath}
    d(x,S)=\inf_{y\in S}\left \| y-x \right \|.
\end{displaymath}
When $S=\emptyset$, set $d(x,S)=+\infty$ for all $x$.

Denote $\Phi _{\eta }$ be the class of all continuous concave functions $\varphi:\left [ 0,\eta \right )\to \mathbb{R}_{+}$ which satisfy:
\begin{enumerate}[(i)]
    \item $\varphi (0)=0$;
    \item $\varphi \in C^{1}(0,\eta)$ and is continuous at 0;
    \item ${\varphi}'(s)>0, \enspace s\in (0,\eta)$.
\end{enumerate}

Let us recall some important results on the powerful Kurdyka–Łojasiewicz (KL) property\cite{6}.
\\
\textbf{Definition 2.6:} Let $f:\mathbb{R}^{m\times n}\to \mathbb{R}\cup \left \{ +\infty \right \} $ be a proper lower semicontinuous function. For $-\infty <a<b\le +\infty$, define
\begin{displaymath}
    \left [ a<f<b  \right ]=\left \{x\in \mathbb{R}^{m\times n}|a<f(x)<b  \right \}.
\end{displaymath}
We say that $f$ has the KL property at $x^{\ast}\in dom \, \partial f:=\left \{x\in \mathbb{R}^{m\times n}|\partial f(x) \ne \emptyset \right \}$ if there exist $\eta \in (0,+\infty]$, a neighborhood $U$ of $x^{\ast}$, and a function $\varphi \in \Phi_{\eta}$, such that for all $x \in U\cap [f(x^{\ast})<f(x)<f(x^{\ast})+\eta]$, the Kurdyka–Łojasiewicz inequality holds
\begin{displaymath}
    {\varphi}'(f(x)-f(x^{\ast}))d(0,\partial f)\ge 1.
\end{displaymath}
If $f$ has the KL property at each point of $dom \, \partial f$, then we call $f$ a KL function.
\\
\textbf{Definition 2.7:} A subset $S$ of $\mathbb{R}^{m\times n}$ is a semi-algebraic set if there exist polynomial functions $f_{ij},g_{ij}:\mathbb{R}^{m\times n}\to \mathbb{R}$ for all $1\le i\le p,1\le j\le q$ such that 
\begin{displaymath}
    S=\bigcup_{i=1}^{p}\bigcap_{j=1}^{q}\left \{x\in  \mathbb{R}^{m\times n}:f_{ij}(x)=0,g_{ij}(x)>0 \right \}.
\end{displaymath}
A function $f:\mathbb{R}^{m\times n}\to \mathbb{R}\cup \left \{ +\infty \right \}$ is called semi-algebraic if its graph
\begin{displaymath}
    graphf=\left \{(x,t)\in \mathbb{R}^{m\times n}\times\mathbb{R}:f(x)=t \right \}
\end{displaymath}
is a semi-algebraic set of $\mathbb{R}^{m\times n}\times\mathbb{R}$. 

From Definition 2.7, we can find a few useful properties of semi-algebra:
\begin{enumerate}[(i)]
    \item The class of semi-algebraic sets is stable under finite unions, finite intersections, complementation and Cartesian products.
    \item The following functions are all semi-algebraic functions:
    \begin{itemize}
        \item Real polynomial functions.
        \item Indicator functions of semi-algebraic sets.
        \item Composition of semi-algebraic functions.
        \item The image of a semi-algebraic set $A\subseteq \mathbb{R}^{m\times n}\times\mathbb{R}$ by the projection $\pi:\mathbb{R}^{m\times n}\times\mathbb{R}\to \mathbb{R}^{m\times n}$ is semi-algebraic.
    \end{itemize}
\end{enumerate}

It is remarkable that a semi-algebraic function is also a KL function.
\\
\textbf{Proposition 2.1:} Let $f:\mathbb{R}^{m\times n}\to \mathbb{R}\cup \left \{ +\infty \right \} $ be a proper lower semicontinuous function. If $f$ is semi-algebraic, then it satisfies the KL property at any point of $dom\, \partial f$.


\section{Problem formulation}

First, we propose a proposition which is useful to describe the MMV problem in mathematical language.
\\
\textbf{Proposition 3.1:} For a set of vectors $\left \{  s^{j}\right \}_{j=1}^{J}\in \mathbb{R}^{N}$, denote $S=(s^{1},s^{2},\cdots,s^{J})$, then $\left \{  s^{j}\right \}_{j=1}^{J}$ has the common sparse support set with sparsity less than $k$ $if$ $and$ $only$ $if$ $\left \| S \right \|_{2,0}\le k$.
\\
\textbf{Proof:} Denote the common sparse support set of $\left \{  s^{j}\right \}_{j=1}^{J}$ as $Supp$, its sparsity $Card(Supp)=m\le k$. We know $S=(s_{ij})_{N\times J}$, denote $S_{r}\in \mathbb{R}^{N}$ is the vector whose element is the $\ell_{2}$-norm of the row vectors of $S$: let $S_{r}[i]$ represent the $i$th element of $S_{r}$, then
\begin{displaymath}
    S_{r}[i]=(\sum_{j=1}^{J}s_{ij}^{2})^{\frac{1}{2}},i=1,2,\cdots,N.
\end{displaymath}
From the definition of $\ell_{2,0}$-norm we know $\left \| S \right \|_{2,0}=\left \| S_{r} \right \|_{0}$. For any $i\in Supp$, there must exist $j_{0}\in \left \{ 1,2,\cdots,J \right \}$ such that $s_{ij_{0}}\ne 0$, therefore
\begin{displaymath}
    S_{r}[i]=(\sum_{j=1}^{J}s_{ij}^{2})^{\frac{1}{2}}=(\sum_{j=1,j\ne j_{0}}^{J}s_{ij}^{2}+s_{ij_{0}}^{2})^{\frac{1}{2}}\ge \left | s_{ij_{0}} \right |>0.
\end{displaymath}
For any $i\notin Supp$,
\begin{displaymath}
    s_{ij}=0,j=1,2,\cdots,J,
\end{displaymath}
hence
\begin{displaymath}
    S_{r}[i]=(\sum_{j=1}^{J}s_{ij}^{2})^{\frac{1}{2}}=0.
\end{displaymath}
Above all,
\begin{displaymath}
    \left \| S \right \|_{2,0}=\left \| S_{r} \right \|_{0}=Card(Supp)=m\le k.
\end{displaymath}
This completes the necessity.

For the sufficiency, assume $\left \| S \right \|_{2,0}=\left \| S_{r} \right \|_{0}=m\le k.$
\\
For any $i\in Supp(S_{r})$,
\begin{displaymath}
     S_{r}[i]=(\sum_{j=1}^{J}s_{ij}^{2})^{\frac{1}{2}}>0,
\end{displaymath}
there must exist $j_{0}\in \left \{ 1,2,\cdots,J \right \}$ such that $s_{ij_{0}}\ne 0$, therefore
\begin{displaymath}
    \bigcup_{j=1}^{J}Supp(s^{j})\supseteq Supp(S_{r}).
\end{displaymath}
For any $i\notin Supp(S_{r})$,
\begin{displaymath}
    S_{r}[i]=(\sum_{j=1}^{J}s_{ij}^{2})^{\frac{1}{2}}=0,
\end{displaymath}
thus $s_{ij}=0,j=1,2,\cdots,J$, hence
\begin{displaymath}
    Supp(s^{j})\subseteq Supp(S_{r}),j=1,2,\cdots,J.
\end{displaymath}
Therefore,
\begin{displaymath}
    \bigcup_{j=1}^{J}Supp(s^{j})\subseteq Supp(S_{r}).
\end{displaymath}
Above all,
\begin{displaymath}
    \bigcup_{j=1}^{J}Supp(s^{j})=Supp(S_{r}).
\end{displaymath}
Which means $Supp(S_{r})$ is the common sparse support set of $\left \{  s^{j}\right \}_{j=1}^{J}$, while
\begin{displaymath}
    Card(Supp(S_{r}))=\left \| S_{r} \right \|_{0}=\left \| S \right \|_{2,0}=m\le k.
\end{displaymath}
\noindent This completes the whole proof. \hfill $\Box$ 
\\
Next we convert the MMV problem to an optimization problem with the help of Proposition 3.1.

Assuming there are $J$ sensors, the sparse vectors after sparse representation are $s_{1},s_{2},\cdots,s_{J}$ where $s_{j}\in \mathbb{R}^{N}$ for all $j\in \left \{ 1,2,\cdots,J \right \}$, denote $S=(s_{1},s_{2},\cdots,s_{J})\in \mathbb{R}^{N\times J}$. Assuming the sensing matrix $\Phi \in \mathbb{R}^{M\times N}(M<N)$, the measurement vectors are $y_{1},y_{2},\cdots,y_{J}$ where $y_{j}=\Phi s_{j}\in \mathbb{R}^{M}$ for all $j\in \left \{ 1,2,\cdots,J \right \}$, denote $Y=(y_{1},y_{2},\cdots,y_{J})\in \mathbb{R}^{M\times J}$. By Proposition 3.1, minimizing the joint-sparsity of $\left \{ s_{j} \right \}_{j=1}^{J}$ is equivalent to minimizing $\left \| S \right \|_{2,0}$. Therefore the MMV problem can be described as
\begin{equation}\label{equ:3.1}
    \begin{aligned}
        &\min_{S\in \mathbb{R}^{N\times J}} \left \| S \right \|_{2,0}\\
        &\quad s.t.\quad Y=\Phi S,
    \end{aligned}
\end{equation}
The following theorem offers the prerequisite to make a further conversion of problem (\ref{equ:3.1}).
\\
\textbf{Theorem 3.1\cite{7}:} For $\Phi \in \mathbb{R}^{M\times N}$ and $S\in \mathbb{R}^{N\times J}$, if $Y=\Phi S$ and 
\begin{equation}\label{equ:3.2}
    \left \| S \right \|_{2,0}<\frac{Spark(\Phi)+Rank(Y)-1}{2},
\end{equation}
where $Spark(\Phi)$ is the smallest number of columns of $\Phi$ that are linearly dependent and $Rank(Y)$ denotes the rank of $Y$, then matrix $S$ will be the unique solution of problem (\ref{equ:3.1}).

From Theorem 3.1, the precondition of the MMV problem that all of the vectors share the same sparse support set can be relaxed, we just require their matrix $S$ satisfies (\ref{equ:3.2}).

In consideration of the odevity of $Spark(\Phi)+Rank(Y)$ and the measurement error between $Y$ and the real $\Phi S$, we set 
\begin{equation}\label{1}
    s=\left \lfloor \frac{Spark(\Phi)+Rank(Y)-2}{2} \right \rfloor,
\end{equation}
where $\left \lfloor \cdot \right \rfloor$ is the floor operator. Problem (\ref{equ:3.1}) can be converted to
\begin{equation}\label{equ:3.3}
    \begin{aligned}
        &\min_{S\in \mathbb{R}^{N\times J}} \left \| Y-\Phi S \right \|_{F}^{2}\\
        &\quad s.t.\quad \left \| S \right \|_{2,0}\le s.
    \end{aligned}
\end{equation}
Problem (\ref{equ:3.3}) is a non-convex constrained optimization problem. Introduce the indicator function
\begin{displaymath}
    \mathcal{I}_{\mathcal{M}}(X)=\begin{cases}
 0, & \text{ \textit{if} } X\in \mathcal{M} \\
 +\infty, & \text{ \textit{if} } X\notin \mathcal{M}
\end{cases}
\end{displaymath}
to the set
\begin{displaymath}
    \mathcal{M}=\left \{ X\in \mathbb{R}^{N\times J}:\left \| X \right \|_{2,0}\le s \right \}
\end{displaymath}
to move the non-convex constraint to the objective function, and introduce matrix $B\in \mathbb{R}^{N\times J}$ as an auxiliary variable of $S$ to reformulate problem (\ref{equ:3.3}) as
\begin{equation}\label{equ:3.4}
    \begin{aligned}
        &\min_{B,S\in \mathbb{R}^{N\times J}} \left \| Y-\Phi S \right \|_{F}^{2}+\mathcal{I}_{\mathcal{M}}(B)\\
        &\quad s.t.\quad \quad B-S=0.
    \end{aligned}
\end{equation}
Now we get the problem formulation (\ref{equ:3.4}) to describe the MMV problem.


\section{Algorithm}

Problem (\ref{equ:3.4}) is a two block optimization problem with linear constraint, we apply the alternating direction method of multipliers (ADMM)\cite{8} to solve it.

The augmented Lagrangian function associated with problem (\ref{equ:3.4}) is defined as
\begin{equation}\label{equ:4.1}
    \mathcal{L}_{\rho}(B,S,L)= \left \| Y-\Phi S \right \|_{F}^{2}+\mathcal{I}_{\mathcal{M}}(B)+\left \langle L,B-S  \right \rangle +\frac{\rho}{2}\left \| B-S \right \|_{F}^{2},
\end{equation}
where $L\in \mathbb{R}^{N\times J}$ is the Lagrangian dual multiplier of the equation constraint, $\left \langle \cdot,\cdot  \right \rangle$ is the inner product, $\rho>0$ is the penalty parameter to measure the distance between $B$ and $S$.

According to ADMM scheme, an approximate solution of problem (\ref{equ:3.4}) can be obtained by minimizing one variable with others fixed in a Gauss-Seidel manner as follows:
\begin{equation}\label{equ:4.2}
    \begin{aligned}
        \left\{\begin{matrix}
 B^{k+1}=&\mathop{\arg\min}\limits_{B\in \mathbb{R}^{N\times J}}\mathcal{L}_{\rho}(B,S^{k},L^{k}),\\
 S^{k+1}=&\mathop{\arg\min}\limits_{S\in \mathbb{R}^{N\times J}}\mathcal{L}_{\rho}(B^{k+1},S,L^{k}),\\
L^{k+1}=&L^{k}+\tau \rho (B^{k+1}-S^{k+1}),
\end{matrix}\right.
    \end{aligned}
\end{equation}
where $\tau>0$ is the step size.

Next we solve the subproblems in equation (\ref{equ:4.2}) one by one.

\subsection{Update B}

When the variables $S$ and $L$ are fixed,
\begin{equation}\label{equ:4.3}
    \begin{split}
        B^{k+1}&=\mathop{\arg\min}\limits_{B\in \mathbb{R}^{N\times J}}\mathcal{L}_{\rho}(B,S^{k},L^{k})\\
        &=\mathop{\arg\min}\limits_{B\in \mathbb{R}^{N\times J}}\left \{  \mathcal{I}_{\mathcal{M}}(B)+\left \langle L^{k},B-S^{k}  \right \rangle +\frac{\rho}{2}\left \| B-S^{k} \right \|_{F}^{2} \right \} \\
        &=\mathop{\arg\min}\limits_{B\in \mathcal{M}}\left \| B-S^{k}+\frac{L^{k}}{\rho} \right \|_{F}^{2}\\
        &=\mathcal{P}_{\mathcal{M} }(S^{k}-\frac{L^{k}}{\rho}),
    \end{split}
\end{equation}
where $\mathcal{P}_{\mathcal{M}}(\cdot)$ is the projection operator on set $\mathcal{M}$.

In detail, if $\left \| S^{k}-\frac{L^{k}}{\rho} \right \|_{2,0}\le s$, then $B^{k+1}=S^{k}-\frac{L^{k}}{\rho}$, otherwise truncate $S^{k}-\frac{L^{k}}{\rho}$ with the rows whose $\ell_{2}$-norm is the top $s$ large preserved.


\subsection{Update S}

When the variables $B$ and $L$ are fixed,
\begin{equation}\label{equ:4.4}
    \begin{split}
        S^{k+1}&=\mathop{\arg\min}\limits_{S\in \mathbb{R}^{N\times J}}\mathcal{L}_{\rho}(B^{k+1},S,L^{k})\\
        &=\mathop{\arg\min}\limits_{S\in \mathbb{R}^{N\times J}}\left \{ \left \| Y-\Phi S \right \|_{F}^{2}  +\left \langle L^{k},B^{k+1}-S\right \rangle +\frac{\rho}{2}\left \| B^{k+1}-S\right \|_{F}^{2} \right \} \\
        &=\mathop{\arg\min}\limits_{S\in \mathbb{R}^{N\times J}}\left \{ \left \| Y-\Phi S \right \|_{F}^{2}+\frac{\rho}{2}\left \| B^{k+1}-S+\frac{L^{k}}{\rho} \right \|_{F}^{2} \right \},
    \end{split}
\end{equation}
Denote $f(X)=\left \| Y-\Phi X \right \|_{F}^{2}+\frac{\rho}{2}\left \| B^{k+1}-X+\frac{L^{k}}{\rho} \right \|_{F}^{2}$, by the first-order optimal condition for differentiable optimization problem, we know
\begin{displaymath}
    S^{k+1}=\mathop{\arg\min}\limits_{S\in \mathbb{R}^{N\times J}}f(S) \Longleftrightarrow  \nabla f(S^{k+1})=0.
\end{displaymath}
From Definition 2.3, it is easy to calculate
\begin{displaymath}
    \nabla f(S^{k+1})=(2\Phi^{T}\Phi+\rho I)S^{k+1}-2\Phi^{T}Y-\rho B^{k+1}-L^{k},
\end{displaymath}
According to the knowledge of linear algebra, we know $\Phi^{T}\Phi$ is semi-positive definite, the identity matrix $I$ is positive definite, combing the penalty parameter $\rho>0$ we can conclude that $2\Phi^{T}\Phi+\rho I$ is positive definite, therefore it is invertible.

Hence by solving the linear equation $\nabla f(S^{k+1})=0$ we obtain
\begin{equation}\label{equ:4.5}
    S^{k+1}=(2\Phi^{T}\Phi+\rho I)^{-1}(2\Phi^{T}Y+\rho B^{k+1}+L^{k}).
\end{equation}


\subsection{Update L}

When the variables $B$ and $S$ are fixed, set the step size $\tau=1$, the Lagrangian dual multiplier $L$ can be updated by 
\begin{equation}\label{equ:4.6}
    L^{k+1}=L^{k}+\rho (B^{k+1}-S^{k+1}).
\end{equation}


\subsection{Convergence criterion}

We give the convergence criterion as shown below, reasons for setting the convergence criterion this way will be discussed in the following section.

Assuming $\left \{ (B^{k},S^{k},L^{k}) \right \}$ is the sequence generated by ADMM procedure (\ref{equ:4.2}), the convergence criterion is shown as follows:
\begin{equation}\label{equ:4.7}
    \begin{aligned}
        \left\{\begin{matrix}
 \left \| B^{k}-S^{k} \right \|_{F}\to 0,\\
 \left \| S^{k+1}-S^{k} \right \|_{F}\to 0,\\
\left \| L^{k} \right \|_{F}\to 0.
\end{matrix}\right.
    \end{aligned}
\end{equation}
\\
We call our algorithm for solving problem (\ref{equ:3.4}) the MMV-ADMM-$\ell_{2,0}$, the entire algorithm is summarized in Algorithm 1.
\\
\begin{tabular*}{\hsize}{@{}@{\extracolsep{\fill}}l@{}} 
    \toprule[1.5pt] 
        \textbf{Algorithm1} MMV-ADMM-$\ell_{2,0}$ \\
        \midrule  
            \textbf{Input:} The measurement matrix $Y$, the sensing matrix $\Phi$; \\
            \textbf{Output:} The reconstructed sparse matrix $\Hat{S}$; \\
            \quad \textbf{1:} Initialize: $B^{0},S^{0},L^{0},s,\rho,k=0$; \\
            \quad \textbf{2:} \textbf{while} not converged \textbf{do} \\
            \quad \textbf{3:} \quad Fix $S$ and update $B$ by:\\
            \quad \quad
             $B^{k+1}=\mathcal{P}_{\mathcal{M} }(S^{k}-\frac{L^{k}}{\rho});$
            \\
            \quad \textbf{4:} \quad Fix $B$ and update $S$ by:\\
            \quad \quad
                $S^{k+1}=(2\Phi^{T}\Phi+\rho I)^{-1}(2\Phi^{T}Y+\rho B^{k+1}+L^{k});$
            \\
            \quad \textbf{5:} \quad Update the Lagrangian multiplier $L$:\\
            \quad \quad 
            $L^{k+1}=L^{k}+\rho (B^{k+1}-S^{k+1});$
            \\
            \quad \textbf{6:} \quad Update $k:k=k+1$.
            \\
            \quad \textbf{7:} \textbf{end while} \\
            \quad \textbf{8:} \textbf{return} $\Hat{S}=B^{k}$.\\
        \bottomrule  
\end{tabular*}

It is noticeable that $2\Phi^{T}\Phi+\rho I$ is unchanged in iteration, hence the inverse of it just need to be calculated only once out of the iteration. Actually, there is an obvious way to accelerate our algorithm. When updating $S$ by (\ref{equ:4.5}), we need to calculate the inverse of an $N\times N$ matrix, which will cost lots of time when $N$ is large. Since $M<N$ in compressed sensing, we can use the Sherman-Morrison-Woodbury (SMW) formula\cite{24,25} to simplify the calculation of the inverse. Specifically,by SMW-formula we have
\begin{equation}\label{equ:4.8}
    (2\Phi^{T}\Phi+\rho I)^{-1}=\frac{I}{\rho}-\frac{2\Phi^{T}(I+\frac{2\Phi \Phi^{T}}{\rho})^{-1}\Phi}{\rho^{2}}.
\end{equation}
It converts the calculation of an $N\times N$ matrix inverse to 
the inverse of an $M\times M$ matrix, where $M<N$ in compressed sensing, this will save the running time.We call our algorithm MMV-ADMM-$\ell_{2,0}$-SMW when using (\ref{equ:4.8}) to update $S$.


\section{Convergence analysis}

Algorithm 1 is a two-block ADMM for non-convex problem, the convergence of non-convex ADMM is still an open problem. However, due to the KL property of the objective function in problem (\ref{equ:3.4}), we can establish the global convergence of Algorithm 1.

First, we prove some properties of the objective function in problem (\ref{equ:3.4}).
\\
\textbf{Proposition 5.1:} The $\ell_{2,0}$-norm of a matrix is a proper lower semicontinuous function.
\\
\textbf{Proof:} First we prove that the $\ell_{0}$-norm of a vector is a lower semicontinuous function. Denote
\begin{align*}
 f:&\mathbb{R}^{N}\to \mathbb{R}  \\
&x\mapsto \left \| x \right \|_{0}.
\end{align*}
For any $x=(x_{1},x_{2},\cdots,x_{N})^{T} \in \mathbb{R}^{N}$, assume 
\begin{displaymath}
    f(x)=\left \| x \right \|_{0}=s,
\end{displaymath}
\begin{displaymath}
    Supp(x)=\left \{ l_{1},l_{2},\cdots,l_{s} \right \} \subseteq \left \{ 1,2,\cdots,N \right \},
\end{displaymath}
when $y^{k}=(y_{1}^{k},y_{2}^{k},\cdots,y_{N}^{k})^{T} \overset{k\to \infty}{\longrightarrow}x$, for 
\begin{displaymath}
\varepsilon=\frac{\min_{j=1,2,\cdots,s}\left | x_{l_{j}} \right |}{2}>0,
\end{displaymath}
there exist $K\in \mathbb{N}$ such that 
\begin{displaymath}
    \left | y_{i}^{k}-x_{i} \right |<\varepsilon, i=1,2,\cdots,N \text{ for all } k>K. 
\end{displaymath}
Hence when $k>K$,
\begin{displaymath}
    y_{l_{j}}^{k}\ne 0, j=1,2,\cdots,s,
\end{displaymath}
which means $f(y^{k})=\left \| y^{k} \right \|_{0}\ge s.$

According to the sign-preserving property of limit, we have $\liminf_{k\to \infty}f(y^{k})\ge s=f(x)$. By Definition 2.2, $f$ is a lower semicontinuous function.

For any $S\in \mathbb{R}^{N\times J}$, $S$ can be expressed by its row vector as $S=(\alpha_{1},\alpha_{2},\cdots,\alpha_{N})^{T}$, denote
\begin{align*}
 g:&\mathbb{R}^{N\times J}\to \mathbb{R}^{N}  \\
&S\mapsto (\left \| \alpha_{1} \right \|_{2},\left \| \alpha_{2} \right \|_{2},\cdots,\left \| \alpha_{N} \right \|_{2})^{T}.
\end{align*}
Then the $\ell_{2,0}$-norm of a matrix 
\begin{align*}
 h:&\mathbb{R}^{N\times J}\to \mathbb{R}  \\
&S\mapsto \left \| S \right \|_{2,0}
\end{align*}
is the composition of $f$ and $g$, $i.e.$ $h=f\circ g$.

From the continuity of the $\ell_{2}$-norm we know that $g$ is a continuous function, combining $f$ a lower semicontinuous function, we can conclude that for any $S\in \mathbb{R}^{N\times J}$, $S^{k}\to S$, we have
\begin{displaymath}
    \liminf_{k\to \infty}h(S^{k})=\liminf_{k\to \infty}f(g(S^{k}))\ge f(\lim_{k\to \infty}g(S^{k}))=f(g(S))=h(S),
\end{displaymath}
hence the $\ell_{2,0}$-norm of a matrix is a lower semicontinuous function.

Apparently $\ell_{2,0}$-norm is proper, hence it is a proper lower semicontinuous function.
\hfill $\Box$

From the equivalence of semicontinuous function and closed function, we get the following Corollary 5.1 immediately.
\\
\textbf{Corollary 5.1:} The $\ell_{2,0}$-norm of a matrix is a closed function.
\\
\textbf{Corollary 5.2:} The indicator function 
\begin{displaymath}
    \mathcal{I}_{\mathcal{M}}(X)=\begin{cases}
 0, & \text{ \textit{if} } X\in \mathcal{M} \\
 +\infty, & \text{ \textit{if} } X\notin \mathcal{M}
\end{cases}
\end{displaymath}
to the set
$\mathcal{M}=\left \{ X\in \mathbb{R}^{N\times J}:\left \| X \right \|_{2,0}\le s \right \}$ is also a proper semicontinuous function.
\\  
\textbf{Proof:} We conclude that $\mathcal{I}_{\mathcal{M}}(X)$ is a closed function. In fact, for any sequence $(A_{k},t_{k})\in epi\mathcal{I}_{\mathcal{M}}$, we have $t_{k}\ge \mathcal{I}_{\mathcal{M}}(A_{k})$, by Definition 2.2, we just need to prove that if $(A_{k},t_{k})\to (A,t)$, then $t\ge \mathcal{I}_{\mathcal{M}}(A)$.

From $t_{k}\to t$ we know when $k$ is large enough, there are only two cases:

if $\left \| A_{k} \right \|_{2,0}>s$, then $t_{k}=\mathcal{I}_{\mathcal{M}}(A_{k})=+\infty$, from $t_{k}\to t$ we have $t=+\infty$, hence $t\ge \mathcal{I}_{\mathcal{M}}(A)$.

if $\left \| A_{k} \right \|_{2,0}\le s$, then $t_{k}\ge \mathcal{I}_{\mathcal{M}}(A_{k})=0$, from $t_{k}\to t$ we have $t\ge 0$. Through Corollary 5.1 we know $\ell_{2,0}$-norm of a matrix is a closed function, hence

\begin{center}
    $\left.
\begin{aligned}
&(A_{k},s)\in epi\left \| \cdot \right \|_{2,0}  \\
&(A_{k},s)\to (A,s)
\end{aligned}
\right\}
\Longrightarrow
(A,s)\in epi\left \| \cdot \right \|_{2,0},$
\end{center}
which means $\left \| A \right \|_{2,0}\le s$, therefore $t\ge0= \mathcal{I}_{\mathcal{M}}(A)$.

Above all $t\ge \mathcal{I}_{\mathcal{M}}(A)$, this complishes the proof that $\mathcal{I}_{\mathcal{M}}(X)$ is closed. It is apparent $\mathcal{I}_{\mathcal{M}}(X)$ is proper, hence $\mathcal{I}_{\mathcal{M}}(X)$ is also a proper semicontinuous function.\hfill $\Box$
\\
\textbf{Proposition 5.2:} The $\ell_{2,0}$-norm of a matrix is a KL function.
\\
\textbf{Proof:} Researches in \cite{6} suggested that both $\left \| \cdot \right \|_{0}$ and $\left \| \cdot \right \|_{p}$ with $p\in \mathbb{Q}_{+}$ are semi-algebraic. Denote functions $f,g,h$ the same as definition in Proposition 5.1, For any $S\in \mathbb{R}^{N\times J}$, $S$ can be expressed by its row vector as $S=(\alpha_{1},\alpha_{2},\cdots,\alpha_{N})^{T}$,
\begin{displaymath}
    g(S)=(\left \| \alpha_{1} \right \|_{2},\left \| \alpha_{2} \right \|_{2},\cdots,\left \| \alpha_{N} \right \|_{2})^{T}=\prod_{i=1}^{N} \left \{ \left \| \alpha_{i} \right \|_{2} \right \}
\end{displaymath}
is the Cartesian product of the $\ell_{2}$-norm, while semi-algebra is stable under Cartesian product, we know $g$ is also semi-algebraic.
The composition of semi-algebraic functions are also semi-algebraic, hence $h=f\circ g$ is also semi-algebraic.

In Proposition 5.1, we prove $h$ is proper lower semicontinuous, hence we can conclude that $h$ is a KL function by Proposition 2.1.\hfill $\Box$
\\
\textbf{Corollary 5.3:} The indicator function 
\begin{displaymath}
    \mathcal{I}_{\mathcal{M}}(X)=\begin{cases}
 0, & \text{ \textit{if} } X\in \mathcal{M} \\
 +\infty, & \text{ \textit{if} } X\notin \mathcal{M}
\end{cases}
\end{displaymath}
to the set
$\mathcal{M}=\left \{ X\in \mathbb{R}^{N\times J}:\left \| X \right \|_{2,0}\le s \right \}$ is also a KL function.
\\  
\textbf{Proof:} First we prove that $\mathcal{M}$ is semi-algebraic. Since $\left \| \cdot \right \|_{2,0}\in \mathbb{N}$, we can express $\mathcal{M}$ as
\begin{displaymath}
    \mathcal{M}=\left \{ X\in \mathbb{R}^{N\times J}:\left \| X \right \|_{2,0}\le s \right \}=\bigcup_{t=0}^{s}\left \{ X\in \mathbb{R}^{N\times J}:\left \| X \right \|_{2,0}=t \right \}.
\end{displaymath}
From Proposition 5.2 we know the $\ell_{2,0}$-norm is a semi-algebraic function, hence we can conclude through Definition 2.7 that
\begin{displaymath}
    graph\left \| \cdot \right \|_{2,0}=\left \{ (X,t):\left \| X \right \|_{2,0}=t \right \}
\end{displaymath}
is a semi-algebraic set.

Denote the projection operator
\begin{align*}
 \pi:&\mathbb{R}^{N\times J}\times \mathbb{R}\to \mathbb{R}^{N\times J}  \\
&(X,t)\mapsto X,
\end{align*}
then 
\begin{displaymath}
    \mathcal{M}=\bigcup_{t=0}^{s}\pi(graph\left \| \cdot \right \|_{2,0}).
\end{displaymath}
While semi-algebra is stable under finite union and the projection operator $\pi$, we have $\mathcal{M}$ is a semi-algebraic set.

Hence the indicator function $\mathcal{I}_{\mathcal{M}}(X)$ to the semi-algebraic set $\mathcal{M}$ is also semi-algebraic, in Corollary 5.2 we prove that it is also proper lower semicontinuous, therefore by Proposition 2.1 $\mathcal{I}_{\mathcal{M}}(X)$ is a KL function.\hfill $\Box$
\\
Next we will establish the global convergence of Algorithm 1 based on the works in \cite{9}.

In \cite{9} the authors consider the optimization problem with the following form
\begin{equation}\label{equ:5.1}
    \begin{aligned}
        &\min_{x,y} f(x)+g(y)\\
        &s.t.\quad Ax+y=b,
    \end{aligned}
\end{equation}
where $f$ is a proper lower semicontinuous function, $g$ is a continuous differentiable function with $\nabla g$ is Lipschitz continuous with modulus $L>0$.

To ensure the convergence, the authors make the following assumptions.
\begin{enumerate}[(i)]
    \item The two minimization subproblems in (\ref{equ:4.2}) possess solutions,
    \item The penalty parameter $\rho>2L$,
    \item $A^{T}A\succeq \mu I$ for some $\mu>0$.
\end{enumerate}

Set $A=-I,x=B,y=S,b=0,f(B)=\mathcal{I}_{\mathcal{M}}(B),g(S)=\left \| Y-\Phi S \right \|_{F}^{2}$ in (\ref{equ:5.1}), we get problem (\ref{equ:3.4}). It is apparent that problem (\ref{equ:3.4}) satisfies the form of (\ref{equ:5.1}) and the above assumptions.

Next, we refer to the sufficient conditions in \cite{9} to guarantee the sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 is bounded.
\\
\textbf{Lemma 5.1\cite{9}:} Let $\left \{ (x^{k},y^{k},\lambda^{k}) \right \}$ be the sequence generated by the classic ADMM procedure of (\ref{equ:5.1}), suppose that 
\begin{equation}\label{equ:5.2}
    \bar{g}:=\inf_{y}\left \{ g(y)-\frac{1}{2L}\left \| \nabla g(y) \right \|^{2} \right \}>-\infty.
\end{equation}
If one of the following statements is true:
\begin{enumerate}[(i)]
    \item $\liminf_{\left \|x  \right \|\to +\infty}f(x)=+\infty$;
    \item $\inf_{x}f(x)>-\infty$ and $\liminf_{\left \|y  \right \|\to +\infty}g(y)=+\infty$.
\end{enumerate}
Then, we have $\left \{ (x^{k},y^{k},\lambda^{k}) \right \}$ is bounded.

The main result in \cite{9} is the convergence of the classic ADMM procedure of (\ref{equ:5.1}) as shown below.
\\
\textbf{Theorem 5.1\cite{9}:} Let $\left \{ w^{k}=(x^{k},y^{k},\lambda^{k}) \right \}$ be the sequence generated by the classic ADMM procedure of (\ref{equ:5.1}) which is assumed to be bounded. Suppose that $f$ and $g$ are KL functions,then $\left \{ w^{k} \right \}$ has finite length, that is
\begin{displaymath}
    \sum_{k=0}^{+\infty}\left \| w^{k+1}-w^{k} \right \|<+\infty,
\end{displaymath}
and as a consequence, $\left \{ w^{k} \right \}$ converges to a critical point of the augmented Lagrangian function of (\ref{equ:5.1}). Moreover, if the assumptions in Lemma 5.1 hold, $\left \{ w^{k} \right \}$ converges to the KKT point of (\ref{equ:5.1}).

As an application of the above theorem, now we give the global convergence of Algorithm 1.
\\
\textbf{Theorem 5.2:} The sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 globally converges to the KKT point of problem (\ref{equ:3.4}).
\\
\textbf{Proof:} First we prove by Lemma 5.1 that the sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 is bounded.

For $g(S)=\left \| Y-\Phi S \right \|_{F}^{2}$, we have $\nabla g(S)=2\Phi^{T}(\Phi S-Y)$ and $L=2\left \| \Phi  \right \|_{F}^{2}$. We show that
\begin{displaymath}
    \bar{g}:=\inf_{S}\left \{ g(S)-\frac{1}{2L}\left \| \nabla g(S) \right \|^{2} \right \}\ge 0.
\end{displaymath}
For any $S\in \mathbb{R}^{N\times J}$,
\begin{displaymath}
    \left \| Y-\Phi S \right \|_{F}^{2}-\frac{1}{2L}\left \| 2\Phi^{T}(\Phi S-Y) \right \|_{F}^{2}\ge \left \| Y-\Phi S \right \|_{F}^{2}-\frac{2\left \| \Phi  \right \|_{F}^{2}}{L}\left \| Y-\Phi S \right \|_{F}^{2}=(1-\frac{2\left \| \Phi  \right \|_{F}^{2}}{L})\left \| Y-\Phi S \right \|_{F}^{2}=0.
\end{displaymath}
Hence the condition (\ref{equ:5.2}) is satisfied. Moreover, it is easy to verify that the condition (ii) in Lemma 5.1 holds. Hence the sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 is bounded.

From Corollary 5.3 we have that $f(B)=\mathcal{I}_{\mathcal{M}}(B)$ is a KL function, while $g(S)=\left \| Y-\Phi S \right \|_{F}^{2}$ is a real polynomial function, it is a semi-algebraic function, therefore by Proposition 2.1 it is also a KL function.

All the conditions in Theorem 5.1 are satisfied, hence the sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 globally converges to the KKT point of (\ref{equ:3.4}).\hfill $\Box$

At last, we give the theorem about settings for the convergence criterion (\ref{equ:4.7}).
\\
\textbf{Theorem 5.3:} The sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 is nearly convergent to the optimal point of problem (\ref{equ:3.4}) \textit{if and only if} it satisfies the convergence criterion (\ref{equ:4.7}).
\\
\textbf{Proof:} In research \cite{10},the authors point out that for the cardinality-constrained optimization problem
\begin{equation}\label{equ:5.3}
    \begin{aligned}
        &\min_{x} f(x)\\
        & s.t. \enspace x\in \mathcal{X},\left \| x \right \|_{0}\le k,
    \end{aligned}
\end{equation}
where $f$ denotes a continuously differentiable function, $k>0$ is a given natural number and $\mathcal{X}$ is a polyhedral convex set, the usual KKT conditions are necessary optimality conditions.

It is easy to see that problem (\ref{equ:3.3}) satisfies the form of (\ref{equ:5.3}), while problem (\ref{equ:3.4}) is equivalent to problem (\ref{equ:3.3}), we know that the optimal point of problem (\ref{equ:3.4}) is also its KKT point.

Therefore, if the sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ generated by Algorithm 1 converges to the optimal point $(B^{\ast},S^{\ast},L^{\ast})$, then $(B^{\ast},S^{\ast},L^{\ast})$ is a KKT point of problem (\ref{equ:3.4}). The Lagrangian function of problem (\ref{equ:3.4}) is
\begin{equation}\label{equ:5.4}
    \mathcal{L}(B,S,L)=\left \| Y-\Phi S \right \|_{F}^{2}+\mathcal{I}_{\mathcal{M}}(B)+\left \langle L,B-S \right \rangle,
\end{equation}
it satisfies the KKT conditions at $(B^{\ast},S^{\ast},L^{\ast})$, therefore
\begin{equation}\label{equ:5.5}
    \begin{cases}
 0 = \nabla_{S} \mathcal{L}(B^{\ast},S^{\ast},L^{\ast})=2\Phi^{T}(\Phi S^{\ast}-Y)-L^{\ast}, \\
 0 \in \partial_{B}\mathcal{L}(B^{\ast},S^{\ast},L^{\ast})=\partial_{B}\mathcal{I}_{\mathcal{M}}(B^{\ast})+L^{\ast}, \\
 0 =B^{\ast}-S^{\ast}.
    \end{cases}
\end{equation}
From $\left \{ (B^{k},S^{k},L^{k}) \right \}\to (B^{\ast},S^{\ast},L^{\ast})$ and the third formula of (\ref{equ:5.5}), we can conclude that $\left \| B^{k}-S^{k} \right \|_{F}\to 0$.

Denote $f$ the same as definition in subsection 4.2, when we update $S$ we have 
\begin{displaymath}
    \nabla f(S^{k+1})=(2\Phi^{T}\Phi+\rho I)S^{k+1}-2\Phi^{T}Y-\rho B^{k+1}-L^{k}=0,
\end{displaymath}
Let $k\to +\infty$ we obtain $2\Phi^{T}(\Phi S^{\ast}-Y)-L^{\ast}=0$, which means the first formula of (\ref{equ:5.5}) is satisfied in Algorithm 1 without any further conditions.

From the rule of updating $B$ in (\ref{equ:4.3}), we know that $\mathcal{I}_{\mathcal{M}}(B^{k})=0$ is always satisfied during iteration in Algorithm 1, while Corollary 5.2 tells us that $\mathcal{I}_{\mathcal{M}}(X)$ is a proper lower semicontinuous function, we have
\begin{displaymath}
    0\le \mathcal{I}_{\mathcal{M}}(B^{\ast})\le \liminf_{k\to +\infty}\mathcal{I}_{\mathcal{M}}(B^{k})=0.
\end{displaymath}
Hence limit the space to all the iteration points generated by Algorithm 1 and its accumulation, we have $\mathcal{I}_{\mathcal{M}}(X)\equiv 0$, thus $\partial_{B}\mathcal{I}_{\mathcal{M}}(B^{\ast})=0$, therefore the second formula of (\ref{equ:5.5}) holds \textit{if and only if} $L^{\ast}=0$, which is equivalent to $\left \|L^{k}  \right \|_{F}\to 0$.

So far we prove that the KKT conditions (\ref{equ:5.5}) is equivalent to the first and the third formula in (\ref{equ:4.7}). From $S^{k}\to S^{\ast}$ we have $\left \| S^{k+1}-S^{k} \right \|_{F}\to 0$, which is the second formula in (\ref{equ:4.7}). Hence the optimal point $(B^{\ast},S^{\ast},L^{\ast})$ must satisfy the convergence criterion (\ref{equ:4.7}). This completes the necessity of the theorem.

For the sufficiency of the theorem, the convergence criterion for a generalized constrained optimization problem\cite{4} is that the distance between the adjacent iteration points tends to zero and the KKT conditions at the current iteration point are nearly satisfied. We have proved that the first and the third formula in (\ref{equ:4.7}) is equivalent to the KKT conditions, hence the convergence criterion (\ref{equ:4.7}) ensures the KKT conditions satisfied, combining the second formula in (\ref{equ:4.7}), which tells that the distance between the adjacent iteration points tends to zero, we know that the convergence criterion (\ref{equ:4.7}) guarantees that the sequence $\left \{ (B^{k},S^{k},L^{k}) \right \}$ converges to a KKT point of (\ref{equ:3.4}). Denote the KKT point as $(B^{\ast},S^{\ast},L^{\ast})$, we say that it is also the optimal point of (\ref{equ:3.4}).

In fact, at the KKT point $(B^{\ast},S^{\ast},L^{\ast})$ we have proved $L^{\ast}=0$ and (\ref{equ:5.5}) satisfied, hence from the first formula of (\ref{equ:5.5}), we can conclude that 
\begin{equation}\label{equ:5.6}
    \Phi^{T}(\Phi S^{\ast}-Y)=0.
\end{equation}
 It is known that the sensing matrix $\Phi\in \mathbb{R}^{M\times N}$ in the MMV problem is a row full rank matrix, therefore $\Phi^{T}$ is a column full rank matrix, thus from (\ref{equ:5.6}) we know that $\Phi S^{\ast}-Y=0$. We have proved that $\mathcal{I}_{\mathcal{M}}(B^{\ast})=0$, which means $\left \| B^{\ast} \right \|_{2,0}\le s$, while $B^{\ast}=S^{\ast}$, we have $\left \| S^{\ast} \right \|_{2,0}\le s$ too. Therefore the KKT point $(B^{\ast},S^{\ast},L^{\ast})$ satisfies $Y=\Phi S^{\ast},\left \| S^{\ast} \right \|_{2,0}\le s$. By Theorem 3.1 we know that $S^{\ast}$ is the unique optimal point of (\ref{equ:3.1}), according to the equivalence of (\ref{equ:3.1}) and (\ref{equ:3.4}), we can conclude that $(B^{\ast},S^{\ast},L^{\ast})$ is also the unique optimal point of (\ref{equ:3.4}).

 Hence when the convergence criterion satisfied, the sequence generated by Algorithm 1 converges to the optimal point of (\ref{equ:3.4}).

 This completes the proof.\hfill $\Box$


\section{Numerical simulations}

In this section we design several numerical experiments to test the validity of our algorithm and theorems we proposed, then we compare our algorithm with other existing algorithms to explore the performance and advantages of our MMV-ADMM-$\ell_{2,0}$. First, we describe our experiment setup. Second, we evaluate the recovery quality and speed of our proposed algorithm. Third, we test the validity of our proposed convergence criterion. Fourth, we study how percentage of successful recovery changes as a function of the sparsity $K$. Fifth, we make experiments to see the influence of undersampling rate on recovery. Sixth, we compare the performance of different algorithms with different number of sensors. Seventh, we give the influence of sample dimensions $N$ on recovery. At last, we compare the performance of MMV-ADMM-$\ell_{2,0}$ and MMV-ADMM-$\ell_{2,0}$-SMW when sample dimension $N$ is large.

\subsection{Experiment setup}

In all of the experiments, performance is analyzed on synthetically generated datasets, and averaged over 100 independent trials. Our datasets are generated as follows: sensing matrix $\Phi\in \mathbb{R}^{M\times N}$ is $i.i.d.$ Gaussian random matrix with unit-norm columns, the ground truth of the recovery problem $S\in  \mathbb{R}^{N\times J}$ is generated by two steps, first randomly select $K$ rows with nonzero entries, then generate the entries of those $K$ rows independently from $\mathcal{N}(0,1)$. Denote $\hat{S}$ as the solution obtained from recovery algorithms. The recovery quality is measured by comparing the root-mean-square error (RMSE)
\begin{displaymath}
    RMSE=\frac{\left \| \hat{S}-S \right \|_{F}}{\sqrt{NJ}},
\end{displaymath}        
and the average running time. When $RMSE<10^{-5}$ we say this recovery is successful.

For comparison to MMV-ADMM-$\ell_{2,0}$, we test two traditional algorithms with good performance for the MMV problem, SOMP\cite{21} and MFOCUSS\cite{16}. We also include an algorithm called MMV-SPG which is based on $\ell_{2,1}$-norm to solve the MMV problem, it is an algorithm that comes form a solver for large-scale sparse reconstruction called SPGL1\cite{23}. As the most direct comparison, we introduce the same ADMM scheme algorithm based on $\ell_{2,1}$-norm called MMV-ADMM-$\ell_{2,1}$\cite{15} to our experiments to see the change of performance after $\ell_{2,1}$-norm replaced by $\ell_{2,0}$-norm.

The parameter settings for all the algorithms are shown in Table \ref{tab:1}.

\begin{table}[hbt!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        Algorithm & Parameters \\\hline
        SOMP\cite{21} & Sparsity $K$ \\\hline
        MFOCUSS\cite{16} & $\lambda=10^{-10}$ \\\hline
        MMV-SPG\cite{23} & Not required \\\hline
        MMV-ADMM-$\ell_{2,1}$\cite{15} & $\lambda=10^{-6},\rho=10^{-5},MaxIter=10^{3}$ \\\hline
        MMV-ADMM-$\ell_{2,0}$ & $\rho=1,MaxIter=10^{3}$ \\\hline
    \end{tabular}
    \caption{Parameter settings for all of the algorithms in experiments}
    \label{tab:1}
\end{table}



\subsection{Validity of MMV-ADMM-$\ell_{2,0}$}

As the first experiment, we evaluate the recovery quality of our MMV-ADMM-$\ell_{2,0}$. We apply MMV-ADMM-$\ell_{2,0}$ without convergence criterion ($i.e.$ iterates to $MaxIter=10^{3}$) on the datasets of size $N=500,M=150,K=50,J=10$ over 10 random repetitions. The experiment results are presented in Table \ref{tab:2}. We can see that the $RMSE$ of MMV-ADMM-$\ell_{2,0}$ is close to the computer precision, and the running time is less than 1 seconds, which tells us that our algorithm can recover the sparse signals precisely and efficiently.

\begin{table}[hbt!]
    \centering
    \begin{tabular}{|c|c|c|}
     \hline
        Data & RMSE & Time(s) \\ \hline
         1 & 8.9904e-16 & 0.6610 \\ \hline
         2 & 9.1155e-16 & 0.6290 \\ \hline
         3 & 9.1562e-16 & 0.5604 \\ \hline
         4 & 8.4978e-16 & 0.5965 \\ \hline
         5 & 9.1531e-16 & 0.6480 \\ \hline
         6 & 9.0638e-16 & 0.6051 \\ \hline
         7 & 9.9356e-16 & 0.5715 \\ \hline
         8 & 9.1655e-16 & 0.5679 \\ \hline
         9 & 7.7866e-16 & 0.6154 \\ \hline
         10 & 7.6170e-16 & 0.6352 \\ \hline
         Mean & 8.8484e-16 & 0.6090 \\ \hline
         Std & 6.9661e-17 & 0.0350  \\ \hline
    \end{tabular}
    \caption{The averaged recovery quality over 10 random experiments ($N=500,M=150,K=50,J=10$)}
    \label{tab:2}
\end{table}


\subsection{Test for convergence criterion}

In the second experiment, we test the convergence criterion (\ref{equ:4.7}) given by Theorem 5.3. We set $N=500,M=150,K=50,J=10$. Our task is to observe the averaged change of convergence criterion (\ref{equ:4.7}) in iteration over 10 random repetitions. The experiment results of the change of convergence criterion in 200 iterations are shown in Figure \ref{fig:1}, which satisfy our analysis in Theorem 5.3.
 
\begin{figure}[hbt!]
	
	\begin{minipage}{0.32\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{1cc200.jpg}}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{2cc200.jpg}}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{3cc200.jpg}}
	\end{minipage}
	\caption{The change of convergence criterion in iterations averaged over 10 experiments ($N=500,M=150,K=50,J=10$)}
	\label{fig:1}
\end{figure}

Then we study the influence of the convergence criterion (\ref{equ:4.7}) on our algorithm. When the convergence criterion (\ref{equ:4.7}) is less than $10^{-6}$, we say that our algorithm has found an approximately optimal solution and then stops the iteration. As shown in Table \ref{tab:3}, with the convergence criterion, our algorithm can successfully recover the sparse signals in less times.

In the following experiments, we apply our algorithm with the convergence criterion.

\begin{table}[hbt!]
    \centering
    \begin{tabular}{|c|cc|cc|}
 \hline
\multicolumn{1}{|l|}{data} & \multicolumn{2}{l|}{Without convergence criterion} & \multicolumn{2}{l|}{With convergence criterion} \\ \hline
           & RMSE & Time(s) & RMSE & Time(s) \\ \hline
         1 & 8.2155e-16 & 0.5765 & 9.4005e-10 & 0.1560 \\ \hline
         2 & 9.0799e-16 & 0.5193 & 7.4102e-10 & 0.1179  \\ \hline
         3 & 7.6515e-16 & 0.4940 & 6.9234e-10 & 0.1116  \\ \hline
         4 & 8.4516e-16 & 0.5109 & 8.8110e-10 & 0.1132  \\ \hline
         5 & 8.5083e-16 & 0.5278 & 7.4768e-10 & 0.1269  \\ \hline
         6 & 9.4163e-16 & 0.5697 & 8.0561e-10 & 0.1508  \\ \hline
         7 & 8.2227e-16 & 0.5152 & 6.5987e-10 & 0.1108  \\ \hline
         8 & 8.9286e-16 & 0.5053 & 7.4714e-10 & 0.1069  \\ \hline
         9 & 7.6474e-16 & 0.4863 & 8.1573e-10 & 0.0949  \\ \hline
         10 & 8.0768e-16 & 0.4796 & 7.8128e-10 & 0.1024  \\ \hline
         Mean & 8.4199e-16 & 0.5185 & 7.8118e-10 & 0.1191  \\ \hline
         Std & 5.8515e-17 & 0.0324 & 8.4008e-11 & 0.0200   \\ \hline
    \end{tabular}
    \caption{The averaged recovery quality over 10 random experiments with and without convergence criterion($N=500,M=150,K=50,J=10$)}
    \label{tab:3}
\end{table}


\subsection{Performance with different $K$}

In this experiment, we study how sparsity influence the performance of all these algorithms. Set $N=500,M=150,J=10$ and let $\frac{M}{K}$ ranges from 1.2 to 5.1 with step size 0.3, observe the percentage of successful recovery as a function of $\frac{M}{K}$ when applying different algorithms, the experiment results are shown in Figure \ref{fig:2}.

\begin{figure}[hbt!]
	
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{KS2.jpg}}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{KT2.jpg}}
	\end{minipage}
	\caption{The recovery quality of different algorithms with different sparsity $K$ ($N=500,M=150,J=10$)}
	\label{fig:2}
\end{figure}

From Figure \ref{fig:2}, we can see that all these algorithms fail to recover the original signals when the sparsity of signals is large ($i.e.$ $\frac{M}{K}\le1.5$). However, when sparsity is not so large, $i.e.$ $1.8\le \frac{M}{K}\le 2.4$, our algorithm can successfully recover all signals over 100 experiments, while the other algorithms perform bad. In addition, the running time of our algorithm is close to the others, they are all fast enough. Hence we can conclude that our algorithm performs better than others for the MMV problem when the original signals are not sparse enough.


\subsection{Performance with different undersampling rate $\frac{N}{M}$}

One of the most significant aims of compressed sensing is to reduce the number of measurements that it can still obtain good recovery quality. To illustrate our algorithm has better performance in undersampling rate compared with others, we study the percentage of successful recovery with different undersampling rate. We set $N=500,K=50,J=10$, the undersampling rate $\frac{N}{M}$ ranges from 1.6 to 8 with step size 1.6, the results of this experiment are shown in Figure \ref{fig:3}.

\begin{figure}[hbt!]
	
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{MS2.jpg}}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{MT2.jpg}}
	\end{minipage}
	\caption{The recovery quality of different algorithms with different undersampling rate $\frac{N}{M}$ ($N=500,K=50,J=10$)}
	\label{fig:3}
\end{figure}

In Figure \ref{fig:3}, we can see that when the undersampling rate $\frac{N}{M}\ge 6.4$, all of the algorithms fail to recover the original signals, but when $\frac{N}{M}= 4.8$, our algorithm can still recover all signals, while MFOCUSS can only recover about 94\% signals and others perform poor. For the running time, we just need to consider the case of successful recovery, and we can see that the running time of our algorithm is about the average of these algorithms. Hence we can say that our algorithm has the better undersampling rate compared with others, which is the key of compressed sensing.


\subsection{Performance with different number of sensors $J$}

In this experiment, we study how recovery quality is affected by the number of sensors $J$. Set $N=500,M=150,K=50$ and let $J$ ranges from 1 (multiplied by 2 per step) to 32. The results are shown in Figure \ref{fig:4}.

\begin{figure}[hbt!]
	
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{JS2.jpg}}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{JT2.jpg}}
	\end{minipage}
	\caption{The recovery quality of different algorithms with different number of sensors $J$ ($N=500,M=150,K=50$)}
	\label{fig:4}
\end{figure}

From Figure \ref{fig:4} we can find that when the number of sensors is 1, $i.e.$ a SVM problem, all of the algorithms can not successfully recover all signals, but our algorithm can recover about 82\% signals, while others perform really bad in this case. When $J\ge2$, our algorithm can successfully recover all signals, however, others successfully recover all signals at least when $J\ge 4$. And the running time of our algorithm is about the average of all these algorithms, which are all fast enough to recover the original signals. This experiment tells us that our algorithm performs good even though the number of sensors is small, while other algorithms do not have the advantage.


\subsection{Performance with different sample dimensions $N$}

We will test the recovery quality of our algorithm for high dimension problem. Set $J=10$, test $N=100,500,1000,1500,3000$ with $\frac{N}{M}=3$, $\frac{M}{K}=3.3$. Results are shown in Figure \ref{fig:5}.

\begin{figure}[hbt!]
	
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{NS2.jpg}}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{NT2.jpg}}
	\end{minipage}
	\caption{The recovery quality of different algorithms with different sample dimensions $N$ ($\frac{N}{M}=3,\frac{M}{K}=3.3,J=10$)}
	\label{fig:5}
\end{figure}

Figure \ref{fig:5} tells us that MFOCUSS, MMV-ADMM-$\ell_{2,1}$ and our algorithm all perform good no matter how dimension changes. And the running time of MFOCUSS and our algorithm is close, but MMV-ADMM-$\ell_{2,1}$ costs more time when $N$ is large. It tells that our algorithm is also suitable when dimension is high.

\subsection{Performance with SMW-formula}

At last, we design the experiment to illustrate the advantage of MMV-ADMM-$\ell_{2,0}$ with SMW-formula when $N$ is large. Set $N=5000,M=1500,K=500,J=10$, and observe $RMSE$ and running time over 10 random experiments.

\begin{table}[hbt!]
    \centering
    \begin{tabular}{|c|cc|cc|}
 \hline
\multicolumn{1}{|l|}{data} & \multicolumn{2}{l|}{MMV-ADMM-$\ell_{2,0}$} & \multicolumn{2}{l|}{MMV-ADMM-$\ell_{2,0}$-SMW} \\ \hline
           & RMSE & Time(s) & RMSE & Time(s) \\ \hline
         1 & 2.5689e-10 & 12.5324 & 2.3941e-10 & 11.5070 \\ \hline
         2 & 2.6148e-10 & 13.4957 & 2.4949e-10 & 12.0098  \\ \hline
         3 & 2.5017e-10 & 13.3999 & 2.6850e-10 & 12.7097  \\ \hline
         4 & 2.5103e-10 & 13.8419 & 2.5707e-10 & 12.2158  \\ \hline
         5 & 2.4778e-10 & 13.6656 & 2.4574e-10 & 12.1301  \\ \hline
         6 & 2.4846e-10 & 13.5132 & 2.5370e-10 & 12.0958  \\ \hline
         7 & 2.4829e-10 & 13.5649 & 2.4438e-10 & 12.4199  \\ \hline
         8 & 2.5644e-10 & 14.2626 & 2.5308e-10 & 12.4762  \\ \hline
         9 & 2.6028e-10 & 13.6527 & 2.5655e-10 & 12.3948  \\ \hline
         10 & 2.4507e-16 & 13.8025 & 2.5998e-10 & 12.3597  \\ \hline
         Mean & 2.5259e-10 & 13.5732 & 2.5279e-10 & 12.2319  \\ \hline
         Std & 5.7252e-12 & 0.4394 & 8.4480e-12 & 0.3283   \\ \hline
    \end{tabular}
    \caption{The averaged recovery quality over 10 random experiments with and without SMW-formula($N=5000,M=1500,K=500,J=10$)}
    \label{tab:4}
\end{table}

As shown in Table \ref{tab:4}, with the SMW-formula, MMV-ADMM-$\ell_{2,0}$-SMW can use less time to obtain solutions with the same precision as MMV-ADMM-$\ell_{2,0}$, and the time advantage is more distinct when $N$ is larger.   


\section{Conclusions}

In this paper, we propose MMV-ADMM-$\ell_{2,0}$, an alternating direction method of multipliers based on $\ell_{2,0}$-norm for the MMV problem. In order to apply ADMM scheme to the MMV problem, we reformulate the MMV problem by Proposition 3.1 and Theorem 3.1\cite{7}, and convert it to problem (\ref{equ:3.4}). Instead of relaxing $\ell_{2,0}$-norm to $\ell_{2,1}$-norm, we directly solve the $\ell_{2,0}$-norm problem, which is the most different part with other works\cite{16,7,20}. Then we establish the global convergence of our algorithm and give its convergence criterion in Theorem 5.2, Theorem 5.3 based on works in \cite{9,10}. In numerical simulations, we test the validity of our algorithm and its convergence criterion, the experiment results are consistent with our theory. In comparing against the traditional algorithms, one algorithm based on $\ell_{2,1}$-norm, and another ADMM scheme algorithm based on $\ell_{2,1}$-norm, we find that MMV-ADMM-$\ell_{2,0}$ can solve a larger range of MMV problems even in bad situations such as high sparsity or few sensors. Moreover, MMV-ADMM-$\ell_{2,0}$ has the larger undersampling rate compared with other algorithms, especially comparied with MMV-ADMM-$\ell_{2,1}$, which is the key of compressed sensing.



\bibliographystyle{unsrt}
\bibliography{references}




\end{document}
