% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref}
\usepackage{xcolor}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{amsmath}
\usepackage{multirow}

\begin{document}
%
\title{Deep Learning-Based Detection of Motion- Affected k-Space Lines for T2*-Weighted MRI}
\titlerunning{Learning-Based Detection of Motion-Affected k-Space Lines}

\titlerunning{Preprint: submitted to MICCAI 2023}

% \author{Anonymous}
% \institute{Anonymous Organization\\
% \email{***@***.***}}
\author{Hannah Eichhorn\inst{1, 2}
\and Kerstin Hammernik\inst{2,3}
\and Veronika Spieker\inst{1,2}
\and Samira M. Epp\inst{2,4}
\and Daniel Rueckert\inst{2,3}
\and Christine Preibisch\inst{2}
\and Julia A. Schnabel\inst{1,2,5}
}

\authorrunning{H. Eichhorn et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

\institute{Helmholtz Munich, Germany 
\and Technical University of Munich, Germany 
\and Imperial College London, United Kingdom 
\and Ludwig-Maximilians-University, Germany
\and King’s College London, United Kingdom\\
\email{hannah.eichhorn@helmholtz-munich.de}}
% \institute{Institute of Machine Learning in Biomedical Imaging, Helmholtz Munich, Germany 
% \and TUM School of Computation, Information and Technology, Technical University of Munich, Germany 
% \and Lab for Artificial Intelligence in Healthcare and Medicine, Technical University of Munich, Germany 
% \and Department of Computing, Imperial College London, United Kingdom 
% \and Department of Neuroradiology, Neuroimaging Center, Technical University of Munich, Germany
% \and Graduate School of Systemic Neurosciences, Ludwig-Maximilians-University, Germany
% \and Department of Neuroradiology, School of
% Medicine, Technical University of Munich, Germany
% \and School of Biomedical Imaging and Imaging Sciences, King’s College London, United Kingdom\\
% \email{hannah.eichhorn@helmholtz-muenchen.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%The abstract should briefly summarize the contents of the paper in 15--250 words.
T$_2$*-weighted gradient echo MR imaging is strongly impacted by subject head motion due to motion-related changes in B$_0$ inhomogeneities. Within the oxygenation-sensitive mqBOLD protocol, even mild motion during the acquisition of the T$_2$*-weighted data propagates into errors in derived quantitative parameter maps. 
In order to correct these images without the need of repeated measurements, we propose to learn a classification of motion-affected k-space lines.
To test this, we perform realistic motion simulations including motion-induced field inhomogeneity changes for supervised training. To detect the presence of motion in each phase encoding line, we train a convolutional neural network, leveraging the multi-echo information of the T$_2$*-weighted images.  
The proposed network accurately detects motion-affected k-space lines for simulated displacements of $\geq$0.5~mm (accuracy on test set: $92.5~\%$). Finally, we show example reconstructions where we include these classification labels as weights in the data consistency term of an iterative reconstruction procedure, opening up exciting opportunities of k-space line detection in combination with more powerful reconstruction methods.


\keywords{Brain MRI \and Motion Artefacts  \and Motion Correction \and Deep Learning.}
\end{abstract}

%check for correct anonymization: https://conferences.miccai.org/2022/files/downloads/MICCAI2022-Submitting-to-MICCAI-Avoiding-Desk-Reject.pdf

%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
T$_2$* mapping, as part of the multi-parametric quantitative BOLD (mq-BOLD) protocol \cite{Hirsch_2014}, enables oxygenation sensitive brain magnetic resonance imaging (MRI) and is more affordable and less invasive than positron emission tomography techniques. Promising applications of the mqBOLD technique comprise stroke, glioma and internal carotid artery stenosis \cite{Gersing_2015,Kaczmarz_2021,Preibisch_2017}.
Motion artefacts, however, remain a major challenge for brain MRI. T$_2$*-weighted acquisitions, like gradient echo (GRE) MRI, show a high sensitivity towards magnetic field inhomogeneities and are thus particularly affected by subject head motion \cite{Magerkurth_2011}. Due to the increasing impact of motion with increasing echo times, motion severely affects the signal decay over several echoes 
%and thus, the accuracy of the T$_2$* quantification from multi-echo GRE data 
\cite{Noth_2014}. Artefacts 
%in T$_2$*-weighted data 
have been shown to propagate from the T$_2$* mapping towards derived parameters, such as the susceptibility related R2’ relaxation rate, even for mild motion with less than 1~mm average displacements during the scans \cite{Eichhorn_2023_dean}.
%and finally the oxygen extraction fraction, which can be derived via an analytic model \cite{Hirsch_2014}. 
This underlines the need of intra-scan motion correction (MoCo) for T$_2$*-weighted GRE data. The current approach of retrospective MoCo during mqBOLD processing relies on partially repeated acquisitions, which almost doubles the overall acquisition time \cite{Noth_2014}. 

Over the last years, deep learning solutions have shown promising results for correcting motion in various MR applications. The MoCo problem has been approached as an image denoising problem, using convolutional neural networks as well as generative adversarial networks \cite{Chatterjee_2020,Johnson_2019,Kustner_2019,Pawar_2019}. However, acting purely on image data, these methods cannot guarantee data consistency, which might hinder their translation into clinical practice. 
Enforcing data consistency is only possible when working with raw k-space data and combining MoCo with the image reconstruction process. For brain MRI, approaches vary from including convolutional neural networks into model-based MoCo \cite{Haskell_2019,Hossbach_2022}, over statistical bootstrap methods \cite{Oh_2021}, to detecting motion events followed by a learning-based unrolled iterative reconstruction \cite{Rotman_2021}. The latter idea of motion detection and undersampled reconstruction has also been demonstrated purely deep learning-based for cardiac ECG mistriggering artefacts \cite{Oksuz_2020}, enabling end-to-end training with relevant downstream tasks. 
In the context of T$_2$* quantification, Xu et al. \cite{Xu_2022} demonstrate that image-based MoCo of GRE data can be performed end-to-end with deep learning based T$_2$* mapping. Yet, to the best of our knowledge no data-consistent MoCo method for T2*-weighted GRE data exists so far.

Inspired by the work of Oksuz et al. \cite{Oksuz_2020}, we propose to learn a detection algorithm for motion-affected k-space measurements, which enables motion-sensitive weighting of the raw GRE data in the following reconstruction. 
Our contributions are three-fold: 
\begin{enumerate}
    \item To enable supervised training, we carry out realistic motion simulations. In contrast to state-of-the-art methods, we do not only include rigid-body transformations, but also B$_0$ inhomogeneities as second-order motion effects.
    \item We leverage the multi-echo information of the T$_2$*-weighted brain dataset with a 3D convolutional neural network to classify individual k-space lines.
    % We train a convolutional neural network to classify individual k-space lines, leveraging the multi-echo information of the T$_2$*-weighted dataset.
    \item To demonstrate the potential of our work, we include motion corrected images where this motion information is included as a weighting mask in the data consistency term of an iterative reconstruction procedure.
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%
% Methods
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}

\subsection{Motion Simulation}
%Motion simulation was carried out using the MRI singlecoil forward model in the presence of rigid-body motion, resulting in the motion-corrupted k-space $y$:
The MRI singlecoil forward model in the presence of rigid-body motion includes the Fourier transform $\mathcal{F}$, the sampling mask $\mathbf{M}_t$ and the three parameter rotation and translation transforms, $\mathbf{R}_t$ and $\mathbf{T}_t$, which are applied to the  motion-free image $x$ for each time point $t$ \cite{Atkinson_2023}:
\begin{equation}
    y = \sum_{t=1}^{T} \mathbf{M}_t \mathcal{F} \mathbf{T}_t\mathbf{R}_t x.
\end{equation}
We further include the position-dependent magnetic field inhomogeneity map $\mathbf{\omega_{B_0}}_t$ as second-order motion effect, dependent on the echo time $T_E$:
\begin{equation}
    y = \sum_{t=1}^{T} \mathbf{M}_t \mathcal{F} e^{-2i\pi \mathbf{\omega_{B_0}}_t T_E} \mathbf{T}_t\mathbf{R}_t x.
\end{equation}
%with the ground-truth image $x$ without noticable motion, the Fourier transform $\mathcal{F}$, the three parameter rotation and translation transforms, $\mathbf{R}_t$ and $\mathbf{T}_t$, the sampling mask $\mathbf{M}_t$,  the position-dependent magnetic field inhomogeneity map $\mathbf{\omega_{B$_0$}}_t$ and the echo time $T_E$ for each time point $t$. 

Based on this, motion is simulated by rigidly transforming the complex MRI data 
%using separately acquired motion curves 
and subsequently merging different motion states in k-space. In this process, the actual multi-slice acquisition scheme is considered, i.e. the ordering of phase encoding lines is included in the sampling mask $\mathbf{M}_t$. Additionally, for time points with average displacements of more than 0.5~mm, magnetic field inhomogeneities are incorporated by multiplying the image with a B$_0$ map that was modified by adding random image gradients with deviations of max. 5~Hz~\cite{Liu_2018}. 
%For the latter we chose deviations of max. 5~Hz as upper limit based on a maximum head position-dependent field inhomogeneity perturbation of 10~Hz measured at 7~T \cite{Liu_2018}.
%with a randomly modified B$_0$ map, with deviations of max. 5~Hz. We chose this 
A visualisation of the procedure is included in the Supplementary Material (Fig.~S1).

In order to not inadvertently introduce additional noise by simulating - what is clinically considered to be - negligible motion, simulation is only performed for phase encoding (PE) lines, where the average displacement of a 64~mm sphere exceeds a certain threshold value $d_{min}$. This threshold is varied between 0.25~mm and 1.0~mm in the following experiments.
To avoid the need of registration for full-reference metrics with respect to the original image, the motion curves are transformed in a way that the median motion state - measured by the average displacement - is in the zero-position. 


\subsection{k-Space Line Classification Network}
As visualised in Fig.~\ref{fig:Network_architecture}, the proposed network consists of five repetitions of 3D convolution, batch normalisation, ReLU activation, dropout and max pooling layers, followed by a fully connected, a dropout and a sigmoid activation layer. Convolution layers are implemented with a kernel size of $3\times3\times3$. Max pooling is performed in the echo and readout dimensions. 

\begin{figure}[t]
    \centering
    \includegraphics[width=120mm]{Fig/3D_Network.pdf}
    \caption{Proposed network architecture. Input $y$ of the network is the motion corrupted k-space. The 219,380 trainable parameters are updated based on the loss calculated between the predicted and the target classification, $m^{pr}$ and $m^{ta}$, which are visualised in black or white for the presence or absence of motion in each PE line.}
    \label{fig:Network_architecture}
\end{figure}

%\subsubsection*{Network Training}
The network was trained for 300 epochs with Adam using a learning rate of $5 \times 10^{-4}$ and a weighted cross entropy loss between prediction $m^{pr}$ and target $m^{ta}$:
\begin{equation}
    L(m^{pr}, m^{ta}) = - m^{ta} \log (m^{pr} + \epsilon) \ - \ \omega \cdot (1-m^{ta}) \log((1-m^{pr}) + \epsilon).
\end{equation} 
Batch sizes of 32 and 64, weighting factors, $\omega$, between 1 and 5, L2 regularisation parameters between $1 \times 10^{-5}$ and $5 \times 10^{-2}$ and dropout probabilities between 10~$\%$ and 30~$\%$ were tested and the best configuration was chosen based on the validation dataset (64 / 5 / $1 \times 10^{-3}$ /  20~$\%$). The average runtime of the network training was 14 hours.
The computations were performed on an NVIDIA RTX A6000, using  Python 3.8.12 and PyTorch 1.13.0 (code available at: \url{https://github.com/HannahEichhorn/T2starLineDet}).



\subsection{Data}
Motion simulation for training and evaluation of the proposed network is performed on 132 complex, coil-combined T$_2$*-weighted datasets of 59 young and healthy volunteers, which can be expected to not considerably move during the scanning. These datasets originally show no noticeable motion artefacts. The unpublished mqBOLD data originates from four ongoing studies investigating brain oxygen metabolism on a 3T Philips Elition MR scanner (Philips Healthcare, Best, The Netherlands), using a multi-slice GRE sequence (12 echoes, 30-36 slices, TE1/$\Delta$TE = 5/5~ms, TR=1910-2291~ms, $\alpha$=30°, voxel size: $2\times2\times3$~mm$^3$). The ongoing studies have been approved by the local ethics committee (approval numbers ********).
%(approval numbers 472/16 S, 446/21 S, 165/21 S, 382/18 S) 
%and written informed consent was obtained from all volunteers.
The available datasets are divided subject-wise into train, validation and test sets (74/18/24 datasets). 
%Additionally, exemplary complex, coil-combined T$_2$*-weighted datasets from the same ongoing studies (same acquisition parameters), which show noticeable motion artefacts, were used for qualitatively evaluating the proposed approach on real motion data.

To base the simulations on real patient motion, 132 motion curves of length 235~s are extracted from 62 separately acquired functional MRI time series (unpublished data from two ongoing studies, independent cohorts from above imaging cohorts). These are divided into train, validation and test sets ($N=$ 88/20/24 motion curves), keeping the motion metrics of the different sets as equal as possible. Principal component analysis (PCA) is used to determine the largest modes of variations of the train motion curves $pc_i(t)$, which are combined with the mean curve $\overline{T(t)}$ for generating augmented training samples \cite{Cootes_1995}:
\begin{equation}
    T_{new}(t) = \overline{T(t)} + \sum_{i=1}^{0.2\cdot N} \omega_i \cdot pc_i(t).
\end{equation}
For each complex image in the training set, six motion curves are generated for simulation. Slices with less than 30~$\%$ brainmask voxels are excluded. This results in a total of 8,922 slices for training, 392 for validation and 492 for testing. 



\subsubsection*{Preprocessing}
The complex data are normalised for each line individually: 
\begin{equation}
    y^{n}_{epr} =  \frac{y_{epr}}{\sqrt{\sum_{ep} \mid y_{epr} \mid ^2}},
\end{equation}
with $e$, $p$ and $r$ indicating the echo, phase encoding and readout dimension. Real and imaginary parts are fed into the network on different channels. 

The target classification labels are calculated by thresholding and inverting the average displacement of a 64~mm sphere at the acquisition time of the corresponding PE line, using the simulation threshold value $d_{min}$. The target masks are averaged across all 12 echoes, since these have been acquired within 60~ms and are thus assumed to have been acquired during the same motion state.



\subsection{Weighted Iterative Reconstruction}
Inspired by the work of Jiang et al. \cite{Jiang_2018}, who include weights into the data consistency term of the reconstruction model for soft-gating, we include the estimated motion classification as weights into the data consistency (DC) term of the singlecoil MRI reconstruction model with total variation (TV) regularisation:
\begin{equation}
    x^* = \arg \min_{x} \frac{1}{2} \parallel \mathbf{W} (\mathbf{A}x-y) \parallel_2^2 + \lambda \parallel \mathbf{\Phi} x \parallel_1.
    \label{eq:TVrecon}
\end{equation}
$\mathbf{W}$ represents a diagonal matrix containing empirical weighting factors 0.25 or 1 for the presence or absence of motion in each PE line, 
$\mathbf{A} = \mathcal{F}$ the forward operator, $\lambda$ the regularisation parameter and $\mathbf{\Phi}$ the finite differences operator. Eq.~\ref{eq:TVrecon} can be solved for the optimal $x^*$ with a proximal gradient algorithm \cite{Hammernik_2020}. 
%The derivation of the proximal operator to the DC term is provided in the Supplementary Material. 

\subsection{Evaluation}
The predicted classification labels are evaluated based on accuracy, corresponding to the rate of correctly predicted lines.
For a more detailed interpretation, the rate of non-detected lines (ND) is calculated as the fraction of lines with motion ($m^{ta}=0$), which the network does not detect ($m^{pr}=1$). Similarly, the rate of wrongly-detected lines (WD) is calculated as the fraction of lines without motion ($m^{ta}=1$), which the network classifies as motion-corrupted ($m^{pr}=0$).


%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments and Results}
In order to determine the minimal level of motion, which the proposed network is able to detect, we train the network using four datasets with different simulation thresholds $d_{min}$. Fig.~\ref{fig:Performance_different_thresholds} compares accuracy, ND and WD rates of these networks' predictions on unseen test data (with the same simulation thresholds as for training). 
%
\begin{figure}
    \centerline{\includegraphics[width=120mm]{Fig/Performance_diff_thr.pdf}}
    \caption{Test accuracy, rates of non-detected (ND) and wrongly-detected (WD) lines for varying thresholds in the motion simulation of train and test data. Mean values are visualised by horizontal lines and numbers provided above each violin plot. All metrics show a decreasing classification performance with decreasing simulation thresholds.}
    \label{fig:Performance_different_thresholds}
\end{figure}
%
The performance in terms of accuracy, ND and WD rate decreases for training and testing the networks with decreasing simulation thresholds. When changing the simulation threshold from 0.5~mm to 0.25~mm, the accuracy drops from $92.28~\%$ to $56.31~\%$, which is mainly driven by an increase of the WD rate from $5.25~\%$ to $41.02~\%$. This indicates that the network is not able to correctly classify motion-affected PE lines when motion is simulated for time points with less than 0.5~mm average displacement of a 64~mm sphere. In the following, we only show results for data simulated with a threshold of 0.5~mm as minimal level of detectable motion. Please note that the performances below can be expected to increase when using larger simulation thresholds. 

In Fig.~\ref{fig:Examples_Simulated_Motion} we demonstrate how the predicted line-wise classification labels can be used for correcting motion-corrupted images by including the classifications as weights in the DC term of the reconstruction model with TV regularisation (Eq.~\ref{eq:TVrecon}).
% The predicted line-wise classification labels are used for correcting motion-corrupted images by including them as weights in the DC term of the reconstruction model (Eq.~\ref{eq:TVrecon}). Fig.~\ref{fig:Examples_Simulated_Motion} shows examples of weighted reconstructions with TV regularisation for simulated data. 
The corresponding motion curves are provided in the Supplementary Material (Fig.~S2).
%
\begin{figure}[t]
    \centering
    \includegraphics[width=120mm]{Fig/Example_recons.pdf}
    \caption{Demonstration of weighted reconstructions with TV regularisation for simulated data with very mild and slightly stronger motion (top/bottom row, mean displacement during whole scan: 0.50/0.89~mm). From left to right: images with simulated motion, TV reconstructions weighted with predicted labels by the network as well as with ground truth (GT) labels, and corresponding original images (without noticeable motion). Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) \cite{Wang_2004} with respect to the original images are given on top of the images. Green arrows indicate an area with subtly reduced artefacts in the weighted reconstructions.}
    \label{fig:Examples_Simulated_Motion}
\end{figure}
%
 Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) \cite{Wang_2004} with respect to the original images (without noticeable motion) are included in Fig.~\ref{fig:Examples_Simulated_Motion} as quantitative measures of image quality. For the example with very mild motion (mean displacement of 0.50~mm), no visual differences between the different scenarios are recognizable. SSIM is slightly increased and PSNR slightly decreased for the weighted reconstruction with both predicted and target label. For simulations with slightly stronger simulated motion (mean displacement of 0.89~mm), we observe a subtle visual decrease of motion artefacts in the weighted reconstruction with target labels and to a certain extent also in the reconstruction with predicted labels. This is in agreement with an increase in PSNR and SSIM for both scenarios. However, the weighted reconstructions appear slightly over-smooth compared to the original image.


%%%%%%%%%%%%%%%%%%%%%%%%
% Discussion
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
T$_2$* mapping based on multi-echo GRE data is highly sensitive to motion, even for displacements smaller than the voxel size of 2~mm \cite{Eichhorn_2023_dean}. Thus, MoCo might be required even for cooperative patients. In this work, we have proposed to split the MoCo problem into two steps: first, we have developed a convolutional neural network to detect the presence or absence of motion for every PE k-space line. Second, we have proposed to use these line-wise classification labels as weights in the DC term of a TV reconstruction procedure. For this, we have simulated realistic motion artefacts both by basing our simulations on real recorded patient motion and, with respect to MR physics, by including motion-induced B$_0$ inhomogeneity changes in addition to rigid-body motion parameters.

The majority of deep learning based MoCo techniques has been developed for correcting movements considerably larger than a voxel size \cite{Haskell_2019,Johnson_2019,Kustner_2019}. In our first experiment, we have investigated the extent of motion that can be detected by the proposed approach. For this, we have varied the simulation threshold for training and test data. We have been able to classify displacements down to 0.5~mm, corresponding to one quarter of the voxel size, with an accuracy of $92.3~\%$. For smaller simulation thresholds, we have observed a clear performance drop. 
For motion simulation we have used rigid-body motion curves extracted from real fMRI time series. For time points with average displacements of less than 0.5~mm the precision of co-registering the fMRI series is also questionable. Thus, a certain degree of noise in the motion data for such small displacements can be expected. Using such motion information in the simulation might lead to an overestimation of motion artefacts, which justifies to simulate motion only for time points with displacements larger than 0.5~mm. 

The accuracy of the proposed line detection approach might be further increased by incorporating additional information, e.g. from neighboring slices \cite{Chatterjee_2020}, or feeding back errors from downstream tasks, like the T$_2$* quantification in image-space \cite{Oksuz_2020,Xu_2022}. However, when considering the two error rates separately, for the following reconstruction task a small rate of non-detected lines is becoming more important than a small rate of wrongly-detected lines.
Thus, the achieved ND and WD rates of the proposed approach (ND = $2.5~\%$, WD = $5.3~\%$) might be sufficient if the reconstruction method is powerful enough.

To demonstrate how the detected motion information can be used for motion correction, we have included the classification labels as weighting factors in the DC term of an iterative reconstruction with TV regularisation. Please note that for the images shown in Fig.~\ref{fig:Examples_Simulated_Motion} we used classification labels from the network trained with the minimal detectable motion threshold $d_{min}=0.5$~mm, since very subtle motion needs to be corrected for precise T$_2$* quantification. Better results might have been achieved by using labels from the more accurate networks trained with larger thresholds. Since these examples have been shown as proof-of-principle, we have also provided images reconstructed with target labels as best-case scenario. For the case with very mild motion, no differences have been visually recognisable. For the case with slightly stronger motion, the proposed approach has reduced motion artefacts visually and in terms of SSIM and PSNR. However, the TV reconstructions have shown some over-smoothness compared to the original image without noticeable motion. Over-smoothing is a common problem when using TV regularisation, though different regularisers can be expected to result in better reconstructions. In future work we plan to combine the proposed method of using line-wise classifications as DC weights with e.g. learning an unrolled optimisation scheme~\cite{Hammernik_2018,Schlemper_2018}. 

One limitation of our work is that we have demonstrated the performance of the proposed method for simulated data only. An evaluation using real motion corrupted data remains for future work, particularly paired in-vivo experiments would be preferable for a quantitative evaluation. Moreover, we have used coil-combined k-space data that were recovered from reconstructed MRI data. The use of multi-coil raw data can be expected to improve the performance, especially for the reconstruction due to redundancies between different coils. Within this work, reconstruction examples have been included as proof-of-principle, but multi-coil raw k-space data will be acquired for future developments.


\subsubsection*{Conclusion}
%\textbf{Conclusion}
We have presented a convolutional neural network which classifies motion-affected k-space lines for T$_2$*-weighted GRE data. We have performed realistic motion simulations, using motion curves extracted form real fMRI data and including motion induced changes in B$_0$ inhomogeneities, and have been able to detect displacements down to 0.5~mm (a quarter of a voxel). We have demonstrated how this motion information can be included in the data consistency term of an iterative reconstruction procedure, the expansion of which is a promising direction for future work.

%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
HE and VS are supported in part by the Helmholtz Association under the joint research school "Munich School for Data Science - MUDS". CP receives funding from Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Projektnummer 395030489. Data acquisition was funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (ERC Starting Grant, ID 759659).
% Funding
% Data sources


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}

% @inproceedings{Eichhorn_2023_dean,
%  author = {Eichhorn, Hannah and Hammernik, Kerstin and Epp, Samira M. and Karampinos, Dimitrios C. and Schnabel, Julia A. and Preibisch, Christine},
%  title = {Investigating the Impact of Motion and Associated {B0} Changes on Oxygenation Sensitive {MRI} through Realistic Simulations},
%  volume = {31},
%  booktitle = {Proc. Intl. Soc. Mag. Reson. Med},
%  year = {2023},
% }


\section*{Supplementary Material}
\setcounter{figure}{0}
\renewcommand{\figurename}{Fig.}
\renewcommand{\thefigure}{S\arabic{figure}}

%
\begin{figure}
    \centerline{\includegraphics[width=130mm]{Fig/Suppl_Motion_Simulation.pdf}}
    \caption{Visualisation of the motion simulation approach. (1) Rigid-body transformations are applied to the complex original image for each time point. (2) A phase term with randomly generated B$_0$ inhomogeneities is multiplied to the transformed images. The results are transformed into k-space. (3) k-Space lines from the individual motion states at all time points are merged into the final simulated k-space while accounting for the multi-slice acquisition scheme. The final image is obtained through an inverse Fourier transform.}
    \label{fig:Suppl_Motion_Sim}
\end{figure}
%

%
\begin{figure}
    \centerline{\includegraphics[width=130mm]{Fig/Suppl_Motion_Curves.pdf}}
    \caption{Motion parameter curves used in the simulation of the example images shown in Fig.~3 of the main article. Motion curves for the case with very mild motion are shown on the left and for the case with slightly stronger motion on the right (mean displacements during the whole scan: 0.50/0.89~mm).
    Translations along x-, y- and z-axis are shown in the top row, rotations around x-, y- and z-axis in the bottom row. Grey background represents time points with displacements, d, larger than 0.5~mm, corresponding to time points, for which motion was simulated.}
    \label{fig:Suppl_Motion_curves}
\end{figure}
%

%
\begin{figure}
    \centerline{\includegraphics[width=120mm]{Fig/Suppl_Example_recons_diff_thr.pdf}}
    \caption{For weighted reconstructions with larger simulation thresholds, $d_{min}§=$ 0.75~mm and 1.0~mm, similar patterns are observable as for $d_{min}§=$ 0.5~mm (Fig. 3 of the main article, example with mean displacement of 0.89~mm). Motion artefacts are subtly reduced by the weighted reconstruction with ground truth (GT) and predicted labels.
    Image quality in terms of PSNR and SSIM is in general higher than for $d_{min}§=$ 0.5~mm, since motion has been simulated for fewer time points. Please refer to Fig. 3 of the main article for a description of the figure content.}
    \label{fig:Suppl_Examples_recons}
\end{figure}
%

\end{document}
