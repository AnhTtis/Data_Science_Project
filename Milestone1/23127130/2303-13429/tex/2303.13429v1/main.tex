\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{upgreek}
\usepackage{enumitem}
% algorithms
\usepackage[ruled]{algorithm}
\usepackage{algorithmic}
\usepackage{authblk}
\usepackage[round]{natbib}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor={blue},citecolor={bred},urlcolor={blue}}
\setcitestyle{round,comma}
\usepackage{cleveref}
\usepackage{xcolor}

\definecolor{bred}{rgb}{0.8,0,0}

% shorthand commands for colors
\newcommand{\cred}[1]{\textcolor{red}{#1}}
\newcommand{\cblue}[1]{\textcolor{blue}{#1}}
\newcommand{\cgreen}[1]{\textcolor{green}{#1}}

\def\x{{\mathbf x}}
\def\y{{\mathbf y}}
\def\z{{\mathbf z}}
\def\cA{{\mathcal A}}
\def\cB{{\mathcal B}}
\def\cBR{{\mathcal B}_{\mathbb{R}}}
\def\cF{{\mathcal F}}
\def\cM{{\mathcal M}}
\def\cX{{\mathcal X}}
\def\cP{{\mathcal P}}
\def\sX{{\mathsf X}}
\def\sY{{\mathsf Y}}
\def\cY{{\mathcal Y}}
\def\bR{{\mathbb R}}
\def\bS{{\mathbb S}}
\def\bC{{\mathbb C}}
\def\bE{{\mathbb E}}
\def\bP{{\mathbb P}}
\def\sP{{\mathsf P}}
\def\bN{{\mathbb N}}
\def\bZ{{\mathbb Z}}
\def\NPDF{{\mathcal N}}
\def\PO{{\mathcal PO}}
\def\cL{{\mathcal L}}
\def\cI{{\mathcal I}}
\def\cJ{{\mathcal J}}
\def\f0{{\mathbf 0}}
\def\bI{{\mathbb I}}
\def\s{{\sigma}}
\def\md{{\mathrm{d}}}
\def\Tr{{\operatorname{Tr}}}
\def\vect{{\operatorname{vec}}}
\DeclareMathOperator{\ent}{ent}
\def\real{\mathbb{R}}

\newtheorem{thm}{Theorem}

\newtheorem{defn}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{prob}{Problem}
\newtheorem{algo}{Algorithm}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{assmp}{Assumption}

\theoremstyle{definition}

\newtheorem{exper}{Experiment}
\newtheorem{remark}{Remark}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{soln}{Solution}
\newtheorem{note}{Note}
\newtheorem*{note*}{Note}

\newtheorem{assumptionH}{\textbf{H}\hspace{-3pt}}
\Crefname{assumptionH}{\textbf{H}\hspace{-3pt}}{\textbf{H}\hspace{-3pt}}
\crefname{assumptionH}{\textbf{H}}{\textbf{H}}

\newtheorem{assumptionA}{\textbf{A}\hspace{-3pt}}
\Crefname{assumptionA}{\textbf{A}\hspace{-3pt}}{\textbf{H}\hspace{-3pt}}
\crefname{assumptionA}{\textbf{A}}{\textbf{A}}

\def\rmd{\mathrm{d}}
\newcommand{\norm}[1]{\ensuremath{\Vert #1 \Vert}}


% notes
\usepackage[textwidth=1.8cm, textsize=scriptsize]{todonotes}
\newcommand{\deniz}[1]{\todo[color=blue!20]{#1 {\bf da}}}
\newcommand{\tim}[1]{\todo[color=red!20]{#1 {\bf tj}}}
\newcommand{\fra}[1]{\todo[color=green!20]{#1 {\bf fc}}}
\newcommand{\fral}[1]{\todo[inline, color=green!20]{#1 {\bf fc}}}
\setlength{\marginparwidth}{2cm}


\title{Interacting Particle Langevin Algorithm for Maximum Marginal Likelihood Estimation}
\author[1]{\"O. Deniz Akyildiz}
\author[2]{Francesca Romana Crucinio}
\author[4,6]{Mark Girolami}
\author[3]{Tim Johnston}
\author[3,4,5]{Sotirios Sabanis}


\affil[1]{Department of Mathematics, Imperial College London, UK}
\affil[2]{CREST, ENSAE, Paris, France}
\affil[3]{School of Mathematics, University of Edinburgh, UK}
\affil[4]{The Alan Turing Institute, UK}
\affil[5]{National Technical University of Athens, Greece}
\affil[6]{Department of Engineering, University of Cambridge, UK}
\affil[ ]{{\textcolor{red}{\footnotesize \texttt{deniz.akyildiz@imperial.ac.uk}, \footnotesize \texttt{francesca.crucinio@ensae.fr}, \footnotesize \texttt{T.Johnston-4@sms.ed.ac.uk}, \texttt{mag92@cam.ac.uk}, \texttt{s.sabanis@ed.ac.uk}}}}

\begin{document}
\maketitle
\begin{abstract}
We study a class of interacting particle systems for implementing a marginal maximum likelihood estimation (MLE) procedure to optimize over the
parameters of a latent variable model. To do so, we propose a continuous-time interacting particle system which can be seen as a Langevin diffusion over an
extended state space, where the number of particles acts as \textit{the inverse temperature} parameter in classical settings for optimisation. Using Langevin diffusions, we
prove nonasymptotic concentration bounds for the optimisation error of the maximum marginal likelihood estimator
in terms of the number of particles in the particle system, the number of iterations of the algorithm, and the
step-size parameter for the time discretisation analysis.
\end{abstract}
% \tableofcontents



\section{Introduction}
Parameter estimation in the presence of hidden, latent, or unobserved variables is key in modern statistical practice. This setting arises in
multiple applications such as frequentist inference \citep{casella2021statistical},  empirical Bayes \citep{carlin2000empirical} and most notably latent variable models to model
complex datasets such as images \citep{bishop2006pattern}, text \citep{blei2003latent}, audio \citep{smaragdis2006probabilistic}, and graphs \citep{hoff2002latent}.
Latent variable models (LVMs) allow us to handle complex datasets by decomposing them into a set of trainable parameters and a set of latent
variables which correspond to the underlying structure of the data. However, learning models with hidden, unobserved, or latent variables
 is challenging as, in order to find a maximum likelihood estimate (MLE), we first need to tackle an intractable integral.

Broadly speaking, the problem of parameter inference in models with latent variables
 can be formalized as a maximum marginal likelihood estimation problem \citep{dempster1977maximum}.
In this setting, there are three main ingredients of the probabilistic model: The parameters, the latent variables, and the observed
data. More precisely, in maximum marginal likelihood estimation we consider a probabilistic model with observed data $y$ and likelihood
$p_\theta(x, y)$ parametrised by $\theta\in \real^{d_\theta}$, where $x\in\real^{d_x}$ is a latent variable which cannot be observed.
Our aim is to find $\theta^\star$ maximising the marginal likelihood $p_\theta(y):=\int_{\real^{d_x}} p_\theta(x, y)\rmd x$. In the literature, the
standard way to solve maximum marginal likelihood estimation problems is to use the Expectation Maximisation (EM)
algorithm \citep{dempster1977maximum}, an iterative procedure which is guarantee to converge to a (potentially local) maximiser of $p_\theta(y)$. The EM algorithm consists of two steps, where the first step is an expectation step (\textit{E-step}) w.r.t. \textit{latent variables}
 while the second step is a maximisation step (\textit{M-step}) w.r.t. \textit{parameters}. While initially this algorithm was mainly implemented in
 the case where both steps are exact, it has been shown that the EM algorithm can be implemented with approximate steps, e.g., using Monte Carlo methods for
 the E-step \citep{wei1990monte, sherman1999conditions}, approximating the expectation step. Similarly, the M-step can be approximated using
 numerical optimisation algorithms \citep{meng1993maximum, liu1994ecme}, e.g., gradient descent \citep{lange1995gradient}. The ubiquity of latent variable models and the effectiveness of
 the EM algorithm compared to its alternatives has led to a flurry of research in the last decades for its approximate implementation. In particular,
 a class of algorithms called Monte Carlo EM (MCEM) algorithms \citep{wei1990monte} have been extensively analysed and used in the literature, see, e.g.,
  \citet{celeux1992stochastic, chan1995monte, sherman1999conditions, booth1999maximizing, cappe1999simulation}. This method replaces the E-step with Monte Carlo integration.
  However, since the expectation in the E-step is w.r.t. posterior distribution of the latent variables given data, the perfect Monte Carlo integration
  is often intractable. As a result, a plethora of approximations using Markov chain Monte Carlo (MCMC) for the E-step have been proposed and the convergence properties
  of the resulting algorithms have been studied, see, e.g., \citet{delyon1999convergence, fort2011convergence, fort2003convergence, caffo2005ascent, atchade2017perturbed}.
  Standard MCMC methods, however, may be hard to implement and slow to converge, particularly when the dimension of the latent
variables is high. This is due to the fact that standard MCMC (Metropolis-based) kernels may be hard to calibrate and may get stuck in local modes easily.
To alleviate such problems, a number of \textit{unadjusted} Markov chain Monte Carlo methods have been proposed, most notably, the unadjusted Langevin algorithm (ULA)
 \citep{durmus2017nonasymptotic,  dalalyan2017further, dalalyan2017theoretical} based on Langevin diffusions \citep{roberts1996exponential}. These approaches are based on a simple discretisation
 of the Langevin stochastic differential equation (SDE) and have become a popular choice for MCMC methods in high dimensional setting with
 favourable theoretical properties, see, e.g., \citet{durmus2019high, vempala2019rapid, durmus2019analysis, brosse2019tamed, dalalyan2019user, chewi2022analysis} and references therein.

Naturally, MCMC methods based on unadjusted Markov kernels like ULA have been also used in the context of EM. In particular, \citet{de2021efficient} studied the SOUL algorithm
 which uses ULA (or, more generally, inexact Markov kernels) in order to draw (approximate) samples from the posterior distribution of the latent variables and approximate the E-step.
 This algorithm proceeds in a \textit{coordinate-wise} manner, first running a sequence of ULA steps to obtain samples, then approximating the E-step with these samples, and finally
  moving to the M-step. The convergence analysis has been done under convexity and mixing assumptions but the proofs are based on a particular selection of step-sizes which
  obfuscates the general applicability of the results. More recently, \citet{kuntz2022scalable} showed that interacting Langevin dynamics can substantially improve the performance.
  In particular, \citet{kuntz2022scalable} used an \textit{interacting} particle system to approximate the E-step, rather than running a ULA chain. This approach has significantly
  improved empirical results, however, \citet{kuntz2022scalable} provided a limited theoretical analysis, in particular, precise nonasymptotic bounds on the convergence rate of the algorithm
   were not provided.

In the optimisation literature algorithms based on interacting particle systems are often used in place of gradient based methods
(e.g. \citet{pinnau2017consensus, kennedy1995particle, totzeck2020consensus, akyildiz2020parallel}, see also \citet{grassi2021particle} for a recent review)
or to speed up convergence in gradient-based methods (e.g. \citet{borovykh2021optimizing}).
Particle methods based on sequential Monte Carlo have been successfully employed in the context of maximum likelihood estimation
(\cite{johansen2008particle, duan2017maximum} ;see also \citet[Chapter 6]{finke2015thesis} for a review).
However, to the best of our knowledge, the use of interacting particle systems combined with Langevin dynamics in the context of maximum
likelihood estimation has not been widely explored,
except for the recent developments in \citet{kuntz2022scalable} which paved the way for the present work. \\

\noindent\textbf{Contributions.} In this paper, we propose and analyse a new algorithm for maximum marginal likelihood estimation in the context of latent variable models. Our algorithm forms an
interacting particle system (IPS) which is based on a time discretisation of a continuous time IPS to approximate the maximum marginal likelihood estimator for models with
incomplete or hidden data. Our method uses a similar continuous time system to that of \citet{kuntz2022scalable} but with a crucial difference in the parameter $(\theta)$ dimension. Specifically, similar to
\citet{kuntz2022scalable}, our IPS model is defined over a set of particles $\{X_1, \ldots, X_N\}$, where $X_i\in \real^{d_x}$, and a parameter $\theta\in \real^{d_\theta}$. However, we inject noise into the dynamics of $\theta$, hence obtain an SDE for $\theta$ rather than a deterministic ordinary differential equation (ODE). This allows us to obtain a system with the invariant measure of the form
\begin{equation}
\pi^N_\star(\theta, x_1, ..., x_N) \propto e^{-\sum_{i=1}^N U(\theta, x_i)}
\end{equation}
where $U(\theta, x) := -\log p_\theta(x, y)$ with fixed data $y$. We prove that the $\theta$-marginal of this density concentrates around $\theta^\star$ as $N \to \infty$ allowing us to obtain
nonasymptotic results for the algorithm.
It follows that any method of sampling from $\pi^N_\star$ (keeping only the $\theta$-component) will be a method for maximum-marginal likelihood estimation. Our approach leverages the rich connection between
Langevin dynamics and optimisation (see, e.g., \citet{dalalyan2017further, raginsky2017non, zhang2023nonasymptotic}) and opens up further avenues for proving new results. As the first nonasmyptotic results under the
interacting setting, our results are obtained in the case where $U$ is gradient Lipschitz and obeys a strong convexity condition,
but we believe that similar results can be obtained under much weaker (nonconvex) conditions \citep{zhang2023nonasymptotic}. This direction is left for future work.

The remainder of this paper is organized as follows. In Section~\ref{sec:optimisation}, we set up our framework and summarize the key contributions, the proposed particle system and resulting algorithm, and explain in detail the intuition for our main result.  In Section~\ref{sec: analysis}, we prove our results with full proofs and provide global error bounds.
Proofs of a technical nature will be postponed to Appendix~\ref{app:ergodicity}--\ref{app:gf}. We supply this section by reviewing closely related work in Section~\ref{sec:related}, making our contributions and main ideas clearer. We then provide in Section~\ref{sec:logistic} an example on logistic regression where we show our results are applicable to this case. Finally, we conclude in Section~\ref{sec:conclusion}.




\subsection{Notation}

We endow $\real^d$ with the Borel $\sigma$-field $\mathcal{B}(\real^d)$ with respect to
the Euclidean norm $\norm{\cdot}$. 
%We denote by $\mathrm{C}(\real^d)$ the set of continuous functions defined over $\real^d$ and by $\mathrm{C}^n(\real^d)$ the set of $n$-times differentiable functions defined over $\real^d$ for any $n \in \mathbb{N}s$.  
For all continuously differentiable functions $f$,
we denote by $\nabla f$ the gradient. Furthermore, if $f$ is twice continuously differentiable we
denote by $\nabla^2f$ its Hessian and by $\Delta f$ its Laplacian.
For any $p \in \mathbb{N}$ we denote by
$\mathcal{P}_p(\real^d) = \{\pi \in \mathcal{P}(\real^d):\int_{\real^d}
  \norm{x}_p^p \rmd \pi(x) < +\infty\}$ the set of probability measures over
$\mathcal{B}(\real^d)$ with finite $p$-th moment. For ease of notation, we define
$\mathcal{P}(\real^d) = \mathcal{P}_0(\real^d)$ the set of probability measures over
$\mathcal{B}(\real^d)$ and endow this space with the topology of weak convergence.  For any $\mu,\nu\in\mathcal{P}_p(\real^d)$ we define the
$p$-Wasserstein distance $W_p(\mu, \nu)$ between $\mu$ and $\nu$ by
\begin{equation*}
    W_p(\mu, \nu) =\left( \inf_{\gamma \in \mathbf{T}(\mu,\nu)} \int_{\real^d\times \real^d}  \norm{x -y}_p^p \rmd \gamma (x,y)\right)^{1/p}
\end{equation*}
where
$\mathbf{T}(\mu, \nu)=\{\gamma\in\mathcal{P}(\real^d\times
  \real^d):\gamma(A \times \real^d) = \mu(A),\ \gamma(\real^d \times
  A) = \nu(A)\ \forall A \in \mathcal{B}(\real^d)\}$ denotes the set of all transport
plans between $\mu$ and $\nu$. In the following, we metrise $\mathcal{P}_p(\real^d)$ with $W_p$.

In the following we adopt the convention that continuous time processes are denoted by bold letters while their time discretisations are not.

\section{Background and main results}
\label{sec:optimisation}
In this section, we introduce the problem and technical background of relevant work.
\subsection{Maximum marginal likelihood estimation and the EM algorithm}
In this section, we introduce the problem of maximum marginal likelihood estimation in models with latent, hidden, or unobserved variables. 

Let $p_\theta(x, \cdot)$ be a joint probability density function of the latent variable $x$ and fixed observed the observed data. We assume a fixed observation $y\in \real^{d_y}$ and define the negative log-likelihood as
\begin{equation*}
    U(\theta, x) := - \log p_\theta(x, y).
\end{equation*}
The goal of maximum marginal likelihood estimation is to find the parameter $\theta^\star$ that maximises the marginal likelihood \citep{dempster1977maximum}
\begin{equation}\label{eq:def:k}
k(\theta) := p_\theta(y) = \int p_\theta(x, y) \rmd x = \int e^{-U(\theta, x)}\rmd x,
\end{equation}
where $y \in \real^{d_y}$ is the observed (fixed) data. Here $p_\theta(y)$ can be interpreted as a function of $\theta$ for fixed observation $y$ as standard in Bayesian literature. Therefore, the problem of maximum marginal likelihood estimation can be seen as maximisation of an intractable integral \eqref{eq:def:k}.

\subsection{A Particle Langevin SDE and its discretisation}
While optimizing a function in a standard setup can be done with standard stochastic optimization techniques in the convex case or with Langevin diffusions in the nonconvex case \citep{zhang2023nonasymptotic}, when the function to be optimised is an intractable integral as in \eqref{eq:def:k}, the problem becomes highly non-trivial. This stems from the fact that any gradient estimate w.r.t. $\theta$ will require sampling $x$ variables to estimate the integral. But since sampling from the unnormalised measure $e^{-U(\theta, x)}$ for fixed $\theta$ in this setting is typically intractable, the samples drawn using numerical schemes would be approximate which would incur bias on the gradient estimates. This is the precise problem investigated in prior works, see, e.g., \citet{atchade2017perturbed} or \citet{de2021efficient} for using MCMC chains to sample from $e^{-U(\theta, x)}$ with fixed $\theta$ and use these samples to estimate the gradient of $k(\theta)$. This approach requires non-trivial assumptions on step-sizes of optimisation schemes and sometimes not even computationally tractable as the sample sizes used to estimate gradients may need to increase over iterations.

A different approach was considered in \citet{kuntz2022scalable} which used a \textit{particle system}. In other words, at iteration $n$, \citet{kuntz2022scalable} proposed to use $N$ particles $X^{i, N}_n$ to estimate the gradient of $k(\theta)$ which is then minimised using a gradient step. Inspired by this approach, in this work, we propose an interacting Langevin SDE
\begin{align}
    \rmd \bm{\theta}^N_t &= -\frac{1}{N}\sum_{j=1}^N \nabla_{\theta} U(\bm{\theta}^N_t, \bm{X}_t^{j, N})\rmd t+ \sqrt {\frac{2}{N}}\rmd \bm{B}_t^{0,N}, \label{eq:ContIPS_theta} \\
    \rmd \bm{X}_t^{i, N} &= -\nabla_x U(\bm{\theta}^N_t, \bm{X}_t^{i, N})\rmd t + \sqrt{2}\rmd \bm{B}_t^{i, N},\label{eq:ContIPS_x}
\end{align}
for $i=1, \dots, N$, where $\{(\bm{B}_t^{i,N})_{t \geq 0}\}_{i =0}^N$ is a family of independent
Brownian motions. This interacting particle system (IPS) is closely related to the one introduced in \citet{kuntz2022scalable}, with one major difference in Eq.~\eqref{eq:ContIPS_theta} where we add the noise term $\sqrt{2/N} \rmd \bm{B}_t^{0,N}$. Despite seemingly small, the addition of the noise term in the $\theta$-component unlocks the way for proving precise nonasymptotic results as we will summarise in Section~\ref{sec:main_results} after introducing our algorithm.

The IPS given in Eqs.~\eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} can be discretised in a number of ways. In this work, we consider a simple Euler-Maruyama discretisation. This can be defined, given $(\theta_0, X_0^{1:N}) \in \real^{d_\theta}\times(\real^d)^{\otimes N}$ and for any $n \in \mathbb{N}$, as
\begin{align}
    \theta_{n+1} &= \theta_n - \frac{\gamma}{N}\sum_{j=1}^N \nabla_{\theta} U(\theta_n, X_n^{j, N}) + \sqrt {\frac{2\gamma}{N}}\xi_{n+1}^{0,N}, \label{eq:IPS_disc_theta}\\
    X_{n+1}^{i, N} &= X_n^{i, N} - \gamma \nabla_x U(\theta_n, X_n^{i, N}) + \sqrt{2\gamma} \xi_{n+1}^{i, N},\label{eq:IPS_disc_x}
\end{align}
where $\gamma>0$ is a time discretisation parameter and $\xi_n^{i,N}:= \bm{B}^{i,N}_{n \gamma}-\bm{B}^{i,N}_{(n-1)\gamma}$ are iid Gaussians for $i=0, \dots, N$.
Algorithm~\ref{algo:ipsem} summarises our method to approximate $\theta^\star$. Interacting particle systems approximating MKVSDEs have been actively studied during the past decades
\citep{sznitman1991topics, bossy1997stochastic, eberle2016reflection} and have been recently popularized in the machine learning community to
train neural networks (e.g., see, \citet{de2020quantitative, nitanda2022convex, hu2019mean}). We call our resulting method Interacting Particle Langevin Algorithm (IPLA).

As in the case of our IPS, this algorithm is similar to the one proposed in \citet{kuntz2022scalable} (see Algorithm~1 therein) but with the crucial difference that we inject
noise in the $\theta$-direction as well. This seemingly small algorithmic difference enables us to obtain a number of results about the invariant measure
of the corresponding continuous-time IPS and the nonasymptotic convergence of the algorithm to the optimal solution. 

\begin{algorithm}[t]
\caption{Interacting Particle Langevin Algorithm (IPLA)}\label{algo:ipsem}
\begin{algorithmic}
\STATE{\textbf{Require:} $N, \gamma, \pi_{\mathrm{init}} \in \cP(\real^{d_\theta})\times\cP((\real^d)^N)$}
\STATE{Draw $(\theta_0, \{X_0^{i,N}\}_{i=1}^N)$ from $\pi_{\mathrm{init}}$}
\FOR{$n=1:n_T$}
\STATE{$\theta_{n+1} = \theta_n - \frac{\gamma}{N}\sum_{j=1}^N \nabla_{\theta} U(\theta_n, X_n^{j, N}) + \sqrt {\frac{2\gamma}{N}}\xi_{n+1}^{0,N}$ and \\
    $X_{n+1}^{i, N} = X_n^{i, N} - \gamma \nabla_x U(\theta_n, X_n^{i, N}) + \sqrt{2\gamma} \xi_{n+1}^{i, N}$}
\ENDFOR
\RETURN $\theta_{n_T + 1}$
\end{algorithmic}
\end{algorithm}

\subsection{The proof plan and the main result}\label{sec:main_results}
In this section, we summarise the main ideas behind our analysis of the scheme given in \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x} and our final result which quantifies the convergence of the sequence $(\theta_n)_{n\geq 1}$ to the minimiser of $k$, namely $\theta^\star$. A standard result from the literature shows that finding minimisers of $k$ also solves the problem of sampling from the posterior distribution $p_{\theta^\star}(x|y)$, see, e.g., \citet{neal1998view} or \citet[Theorem~1]{kuntz2022scalable}.

In what follows, we will sketch our results on the invariant measure, the convergence of the IPS \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} and the discrete algorithm \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x}. These results will extend proofs from a standard Langevin diffusion to the much more complicated particle systems we consider. A key aspect of our theoretical framework is that we consider the rescaled version of our particle system given as
\begin{align}\label{eq:z_rescaling_intro}
Z^N_n:=(\theta_n, N^{-1/2}X^{i,N}_n,...,N^{-1/2}X^{i,N}_n),
\end{align}
which proves much easier to analyse, and causes no loss in precision since we shall be considering only the convergence of the $\theta_n$ component. 

Our proof can be summarised in a few main steps we describe below.

\subsubsection{Invariant measure}\label{subsec:inv}
After showing that the IPS has a strong unique solution in Prop.~\ref{prop:e!}, our first result (in Prop.~\ref{prop:invariant_measure}) shows that, as mentioned in the Introduction, the IPS \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} leaves
\begin{align*}
    \pi^N_\star(\theta, x_1, ..., x_N) \propto e^{-\sum_{i=1}^N U(\theta, x_i)}
\end{align*}
invariant. This is a crucial result as this measure has concentration properties that will be very useful in the next section.

\subsubsection{Concentration of the invariant measure around the minimiser}\label{subsec:concentration}
The usefulness of proving results in Section~\ref{subsec:inv} becomes apparent in our setup with the crucial observation that the $\theta$-marginal of $\pi_\star^N$ \textit{concentrates} around $\theta^\star \in \arg \min_{\theta \in \bR^{d_\theta}} k(\theta)$.

To show this, we denote the $\theta$-marginal of $\pi_\star^N$ as $\pi_\Theta^N$ and we prove (in Prop.~\ref{prop:concentration}) that
\begin{equation*}
W_2(\pi^N_\Theta, \delta_{\theta^\star})\leq \sqrt{\frac{2d_\theta}{\mu N}},
\end{equation*}
i.e., the $\theta$-marginal concentrates around the Dirac measure located at the minimiser $\theta^\star$. This allows us to derive optimisation errors in our final result.

\subsubsection{Convergence rate of the IPS \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x}}\label{subsec:conv_rate_IPS}
Our next result shows that, not only $\pi_\star^N$ is the invariant measure, but under appropriate smoothness and convexity assumptions which will be made clear in Section~\ref{sec: analysis}, we show (in Prop.~\ref{prop: conv to inv}) that under appropriate conditions on the initial condition, we have
\begin{align*}
    W_2(\mathcal{L}_{\theta_0}(\bm{\theta}_t^N), \pi_\Theta^N) \leq e^{-\mu t} \left(C_0 + \frac{d_xN + d_\theta}{N \mu}\right)^{1/2}
\end{align*}
where $C_0 < \infty$ is the initialisation error (see Prop.~\ref{prop: conv to inv} for its precise form) and $\pi_\Theta^N$ is the $\theta$-marginal of the measure associated to the law of the IPS \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} at time $t$. Prop.~4 in particular is akin to the standard results on Langevin diffusions \citep{durmus2017nonasymptotic, durmus2019high}, but proved for the law of the entire particle system which is much harder to prove and can only be achieved using the rescaling \eqref{eq:z_rescaling_intro} mentioned above.

\subsubsection{Discretisation analysis}
While above results prove that the law of the IPS SDE in \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} would concentrate around the minimiser in $\theta$ marginal, we still have to handle the discretisation error of the algorithm \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x}. In particular, we prove in Prop.~\ref{prop: discr error 1} that
\begin{equation*}
W_2(\mathcal{L}(\theta_n), \mathcal{L}(\bm{\theta}_{n\gamma}) \leq C_1 (1+\sqrt{d_\theta/N + d_x}) \gamma^{1/2},
\end{equation*}
where $\gamma$ is the step-size given in \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} and $C_1$ is the constant independent of problem parameters. This proves that the law of the algorithm \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x} stays close to the law of the SDE \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} in a certain sense.
\subsubsection{The main result}
Recall that our main aim is to show that the sequence $(\theta_n)_{n\geq 1}$ \textit{optimises} $k(\theta)$, in order words $\theta_n$ converges to $\theta^\star$ (which is unique by strong convexity). Precisely, we use the following equality and then triangle inequality
\begin{align*}
\mathbb{E}\left[\Vert \theta_n-\theta^\star \Vert^2\right]^{1/2} &= W_2(\delta_{\theta^\star}, \mathcal{L}(\theta_n))\\
&\leq W_2(\delta_{\theta^\star}, \pi^N_\Theta)+ W_2(\pi^N_\Theta, \mathcal{L}(\bm{\theta}_{n\gamma}))+W_2(\mathcal{L}(\bm{\theta}_{n\gamma}), \mathcal{L}_{\theta_0}(\theta_n)),
\end{align*}
to arrive at a \textit{splitting} of the optimisation error into three terms which we know how to bound as explained in previous subsections.

Using this, we prove in Theorem~\ref{thm:main} that $\theta$-iterates in Algorithm~\ref{algo:ipsem} satisfy the following nonasymptotic convergence result
\begin{equation*}
\mathbb{E}\left[\Vert \theta_n-\theta^\star \Vert^2 \right]^{1/2}\leq \sqrt{\frac{2d_\theta}{N\mu}}+e^{-\mu n \gamma} \left(C_0 + \left(\frac{d_xN + d_\theta}{N \mu}\right)^{1/2}\right) + C_1 (1+\sqrt{d_\theta/N + d_x})\gamma^{1/2},
\end{equation*}
where $\mu$ is the strong convexity constant, $\gamma$ is the time discretisation parameter, $N$ is the number of particles,
%$t$ is the number of iterations% 
and $C_1$ is a constant independent of $N,n, d_\theta, d_x$ defined for $\gamma$ sufficiently small. This
result is the first of its kind for the IPS based methods for the MMLE problem.

\section{Analysis}\label{sec: analysis}
As mentioned above, the results we would like to achieve require smoothness and convexity assumptions on $U$ which we summarise in the following section.

\subsection{Assumptions}

Before proceeding further and introducing our approach, we collect here all the assumptions that we will use in the remainder of this paper.
First, we assume that $\nabla U(\theta,x)$ is Lipschitz in both variables.
\begin{assumptionA}\label{Lipschitz assump} Let $v = (\theta, x)$ and $v' = (\theta', x')$. We assume that there exist $L_\theta, L_x>0$ such that
\begin{align*}
    \|\nabla U(v) - \nabla U(v')\| \leq L_\theta \|\theta-\theta'\| + L_x \|x-x'\|.
\end{align*}
so that $\nabla U$ is overall Lipschitz with constant $L\leq \sqrt{2} \max \{L_\theta, L_x \}$.
\end{assumptionA}
We also assume that the following strong convexity condition holds.
\begin{assumptionA} \label{Conv assump} Let $v = (\theta, x)$. Then, there exists $\mu>0$ such that
\begin{align*}
 \left\langle v - v', \nabla U(v) - \nabla U(v')\right\rangle \geq     \mu \|v - v'\|^2,
\end{align*}
for all $v, v^\prime\in\real^{d_\theta}\times\real^{d_x}$.
\end{assumptionA}

\Cref{Lipschitz assump} is common in the literature on interacting particle systems and guarantees that both the interacting particle system \eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x} and its time discretisation \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x} are stable (see, e.g., \citet{sznitman1991topics, bossy1997stochastic}).
\Cref{Conv assump} guarantees that $U$ has a unique minimizer $v^\star = (\theta^\star, x^\star)$.
\begin{remark}
 Observe that the strong monotonicity assumption ~\Cref{Conv assump} on $\nabla U$ is equivalent to the assertion that $U$ is strongly convex, which in turns implies a quadratic lower bound on the growth of $U$. Therefore $e^{-U}$ is absolutely integrable, and so the Leibniz integral rule for differentiation under the
integral sign (e.g. \citet[Theorem 16.8]{billingsley1995measure}) guarantees that
\begin{align}\label{eq: derivative of k}
    \nabla k(\theta) = \nabla \int_{\real^{d_x}} e^{-U(\theta,x)}\rmd x= -\int_{\real^{d_x}} \nabla_\theta U(\theta, x)e^{-U(\theta,x)}\rmd x.
\end{align}
\end{remark}

\subsection{Continuous-time process and its invariant measure}
Recall our continuous-time interacting SDE introduced in Eqs.~\eqref{eq:ContIPS_theta}--\eqref{eq:ContIPS_x}
\begin{equation}
\label{eq: IPS noised}
    \rmd \bm{\theta}^N_t = -\frac{1}{N}\sum_{j=1}^N \nabla_{\theta} U(\bm{\theta}^N_t, \bm{X}_t^{j, N})\rmd t+ \sqrt {\frac{2}{N}}\rmd \bm{B}_t^{0,N}, \qquad \rmd \bm{X}_t^{i, N} = -\nabla_x U(\bm{\theta}^N_t, \bm{X}_t^{i, N})\rmd t + \sqrt{2}\rmd \bm{B}_t^{i, N},
\end{equation}
In what follows, we show that~\eqref{eq: IPS noised} admits a unique solution under~\Cref{Lipschitz assump}.
\begin{prop}
\label{prop:e!} Let \Cref{Lipschitz assump} hold. Then, for any initial condition $(\theta_0, x_0^{1:N})$ there exist a unique strong solution to~\eqref{eq: IPS noised}.
\end{prop}
\begin{proof} This follows since the coefficients of the SDE \eqref{eq: IPS noised} is Lipschitz. See, for instance, \citet[Theorem 2.9]{karatzas1991brownian}.
\end{proof}

Having shown that~\eqref{eq: IPS noised} admits a unique solution, we now investigate its invariant measure.
\begin{prop}[Invariant measure]
\label{prop:invariant_measure}
For any $N\in\mathbb{N}$, the measure $\pi_\star^N(\theta, x_1, ..., x_N) \propto e^{-\sum^N_{i=1} U(\theta, x_i)}$ is an invariant measure for the IPS~\eqref{eq: IPS noised}.
\end{prop}
\begin{proof}
Recall that for an $\mathbb{R}^d$-valued SDE with drift $b:\mathbb{R}^d\to\mathbb{R}^d$ and $\sigma:\mathbb{R}^d \to\mathbb{R}^{d\times d}$, if we define the diffusion tensor $D:=\frac{1}{2}\sigma\sigma^T$, the probability density $\rho_t \in \cP(\real^d)$ of the solution obeys the Fokker-Plank PDE given as
\begin{equation}\label{eq:fp}
\partial_t \rho = -\nabla \rho \cdot b -\rho \sum_{i=1}^d \partial_{x_i} b_i+\sum_{i,j=1}^d \partial_{x_i,x_j} (D_{ij} \rho).
\end{equation}
The invariance of $\pi_\star^N$ then follows from showing that the RHS of \eqref{eq:fp} vanishes for $\rho$ equal to the density of $\pi_\star^N$. Specifically, for the system \eqref{eq: IPS noised} (considered as one SDE), we have $d=d_\theta+Nd_x$ and 
\begin{equation}
\label{eq:fp_drift}
b(\theta, x_1,\dots, x_N):= (-N^{-1}\sum_{j=1}^N\nabla_\theta U(\theta, x_j), -\nabla_xU(\theta, x_1). \dots, -\nabla_xU(\theta, x_N)),
\end{equation}
and $D$ is a constant diagonal matrix with diagonal entries $D_{ii}=N^{-1}$ for $i=1,...,d_\theta$ and $D_{ii}=1$ for the remainder. Let $\rho(\theta, x_1, x_2,..., x_N) =  e^{-\sum^N_{i=1} U(\theta, x_i)}/C$. We can then calculate the first term of the Fokker-Plank equation \eqref{eq:fp} as
\begin{equation}
  -\nabla \rho \cdot b = -\biggr(N^{-1}\biggr \Vert \sum_{i=1}^N \nabla_\theta U(\theta, x_i) \biggr\Vert^2  + \sum^N_{i=1}\Vert \nabla_x U(\theta, x_i) \Vert^2  \biggr) \rho.
\end{equation}
Next, we write
\begin{align*}
    \bar{x} = (x_1, \ldots, x_N)^\top,
\end{align*}
and note that $\text{dim}(\bar{x}) = N d_x$. We then denote each element of this vector with $\bar{x}_i$ and write
\begin{align*}
    -\rho \sum_{i=1}^{d_\theta} \partial_{\theta_i} b_i -\rho \sum_{i=d_\theta + 1}^{d_\theta + Nd_x} \partial_{\bar{x}_i} b_i& = \biggr(N^{-1} \sum_{i=1}^N \Delta_\theta U(\theta, x_i) +\sum_{i=1}^N \Delta_x U(\theta, x_i) \biggr )\rho.
\end{align*}
For the final term of \eqref{eq:fp}, note that $D_{ij} = 0$ whenever $i \neq j$. This implies, we can write this term as
\begin{align*}
 &\sum_{i=1}^{d_\theta} \partial^2_{\theta_i}(D_{ii} \rho) + \sum_{i=d_\theta + 1}^{d_\theta + Nd_x} \partial^2_{\bar{x}_i}(D_{ii} \rho) = N^{-1}\sum^N_{i=1}\Delta_\theta \rho +\sum^N_{i=1}\Delta_{x_i} \rho \\
 & =\bigg ( -N^{-1}\sum_{i=1}^N \Delta_\theta U(\theta, x_i)+N^{-1}\biggr \Vert \sum_{i=1}^N \nabla_\theta U(\theta, x_i) \biggr\Vert^2 - \sum_{i=1}^N \Delta_x U(\theta, x_i)     +\sum^N_{i=1}\Vert \nabla_x U(\theta, x_i) \Vert^2\biggr)\rho,
\end{align*}
so that summing these three terms we see that $\rho_t=\pi_\star^N$ for all $t\geq 0$ is solution to the Fokker-Plank equation for \eqref{eq: IPS noised}, and therefore an invariant measure.
\end{proof}
\subsection{Concentration around the minimiser}
Proposition~\ref{prop:invariant_measure} shows that the IPS~\eqref{eq: IPS noised} has an invariant measure which admits
\begin{equation}
\label{eq:mu_marginal_theta}
\pi^N_\Theta (\theta) \propto \int_{\real^{d_x}}... \int_{\real^{d_x}} e^{-\sum^N_{i=1} U(\theta, x_i)} \rmd x_1 \rmd x_2 \dots \rmd x_N = \biggr(\int_{\real^{d_x}} e^{-U(\theta, x)} \rmd x \biggr)^N = k(\theta)^N
\end{equation}
as $\theta$-marginal. This observation is key to showing that the IPS \eqref{eq: IPS noised} can act as a \textit{global optimiser} of $k(\theta)$, more precisely $\log k(\theta)$. Let $\kappa(\theta) = -\log k(\theta)$ and note
\begin{align}\label{eq:target_kappa}
    \pi_\Theta^N(\theta) \propto e^{-N \kappa(\theta)},
\end{align}
which concentrates around the maximiser of $\kappa(\theta)$, hence the minimiser of $k(\theta)$, as $N \to \infty$. This is a classical setting in global optimisation, see, e.g., \citet{hwang1980laplace} where $N$ acts as the \textit{inverse temperature} parameter. Here we stress that our number of particles produces an equivalent effect as the classical inverse temperature parameter, hence we do not need an explicit inverse temperature parameter in this setting. In the next proposition we provide quantitative rates on how $\pi^N_{\Theta}$ concentrates around the minimiser of~\eqref{eq:def:k}. Note that concentration results of this kind hold in more general contexts, see Proposition 3.4 of \citet{pmlr-v65-raginsky17a} or see \citet{zhang2023nonasymptotic}.



\begin{prop}[Concentration Bound]
\label{prop:concentration}
Let $\pi^N_\Theta$ be as in \eqref{eq:mu_marginal_theta} and $\theta^\star$ be the maximiser of $k(\theta)$. Then, under~\Cref{Conv assump}, one has the bound
\begin{equation*}
W_2(\pi^N_\Theta, \delta_{\theta^\star})\leq \sqrt{\frac{2d_\theta}{\mu N}}.
\end{equation*}
\end{prop}
\begin{proof}
First we define the measure $\pi^1$, i.e., the target measure for $N=1$. Observe that $\pi^1$ is $\mu$-strongly log-concave, since $\pi^1 \propto e^{-U(\theta, x)}$ and $U$ is strongly convex by~\Cref{Conv assump}. Therefore, by the Pr\'ekopaâ€“Leindler
inequality (see \citet[Theorem 7.1]{gardner2002brunn} for instance), $\pi^1_\Theta$ is $\mu$-strongly log-concave, i.e.,
\begin{align}
\label{eq:V_convex}
 \left\langle \theta - \theta', \nabla \kappa(\theta) - \nabla \kappa(\theta')\right\rangle \geq     \mu \|\theta - \theta'\|^2,
\end{align}
where $\kappa(\theta) = -\log k(\theta)$ for all $\theta, \theta^\prime\in\Theta$, with $\mu$ given in~\Cref{Conv assump}.
As a result, since $\pi^N_\Theta (\theta) \propto \biggr(\int_{\mathbb{R}^{d_x}}e^{-U(\theta,x)} \rmd x \biggr)^N$ one can write $\pi^N_\Theta \propto e^{-N \kappa}$ as shown in \eqref{eq:target_kappa}.

Next, we show that the Langevin SDE with drift $-\nabla \kappa$ would converge. It is well-known that the law of the overdamped Langevin SDE 
\begin{equation*}
\rmd \bm{L}_t = -\nabla \kappa(\bm{L}_t) \rmd t+\sqrt{\frac{2}{N}}\rmd \bm{B}_t, \qquad \bm{L}_0 = \theta^\star,
\end{equation*}
converges to $\pi^N_\Theta$ \citep{roberts1996exponential}. Therefore, using Ito's formula, we have
\begin{equation*}
\mathbb{E}\left[ \Vert \bm{L}_t - \theta^\star \Vert^2 \right]=  \biggr (\mathbb{E}\left[\int^t_0  2\langle \nabla \kappa(\bm{L}_s), \bm{L}_s - \theta^\star \rangle \rmd s\right]+ \frac{2d_\theta}{N}t \biggr ),
\end{equation*}
which gives
\begin{equation*}
\frac{\rmd}{\rmd t}\mathbb{E}\left[ \Vert \bm{L}_t - \theta^\star \Vert^2 \right]=  2 \mathbb{E}\left[ \langle \nabla \kappa(\bm{L}_t), \bm{L}_t - \theta^\star \rangle\right]+\frac{2d_\theta}{N}.
\end{equation*}
Observing that $\theta^\star$ is a minimizer of $\kappa$, and so $\nabla \kappa(\theta^\star)=0$, we can apply~\eqref{eq:V_convex}, and obtain
\begin{align*}
\frac{\rmd}{\rmd t}\mathbb{E}\left[ \Vert \bm{L}_t - \theta^\star \Vert^2 \right]&=  2\mathbb{E}\left[ \langle \nabla \kappa(\bm{L}_t)-\nabla \kappa(\theta^\star), \bm{L}_t - \theta^\star \rangle\right]+ \frac{2d_\theta}{N} \\
&\leq - 2 \mu \mathbb{E}\left[  \Vert \bm{L}_t - \theta^\star \Vert^2 \right]+\frac{2d_\theta}{N}.
\end{align*}
Therefore, since $\Vert \bm{L}_0-\theta^\star \Vert=0$ one has
\begin{equation*}
\mathbb{E}\left[  \Vert \bm{L}_t - \theta^\star \Vert^2\right] \leq \frac{2d_\theta}{N\mu}(1-e^{-\mu t}),
\end{equation*}
from which we obtain
\begin{equation*}
W_2(\pi^N_\Theta, \delta_{\theta^\star} )= \lim_{t \to \infty} W_2( \mathcal{L}(\bm{L}_t),  \delta_{\theta^\star} ) \leq \lim_{t \to \infty} \mathbb{E}\left[  \Vert \bm{L}_t - \theta^\star \Vert^2\right]^{1/2},
\end{equation*}
where $\mathcal{L}(\bm{L}_t)$ denotes the law of $\bm{L}_t$, and 
the result follows.
\end{proof}


\subsection{Convergence of $\theta$-marginal}
We now exploit the connection between~\eqref{eq: IPS noised} and standard Langevin diffusions to establish exponential ergodicity of~\eqref{eq: IPS noised}.

As mentioned before, recall that we can define the rescaled version of our particle system
\begin{align*}
    \bm{Z}_t^N = (\bm{\theta}_t^N, N^{-1/2} \bm{X}_t^{1, N}, \ldots, N^{-1/2} \bm{X}_t^{N,N}).
\end{align*}
With the help of this rescaled IPS system, we have the following exponential ergodicity result:
\begin{prop} \label{prop: conv to inv} Let \Cref{Lipschitz assump} and \Cref{Conv assump} hold. Then, for any $N\in\mathbb{N}$ and any initial condition $(\theta_0,x_0^{1:N}) \in\real^{d_\theta}\times (\real^{d_x})^N$
 we have
\begin{align*}
    W_2(\mathcal{L}_{\theta_0}(\bm{\theta}_t^N), \pi_{\Theta}^N) \leq e^{-\mu t} \left(\|z_0 - z^\star\| + \left(\frac{d_xN + d_\theta}{N \mu}\right)^{1/2}\right),
\end{align*}
where $\mu$ is given in~\Cref{Conv assump}, $v^*=(\theta^*, x^*)$ is the minimiser of $U$ and
\begin{align*}
z_0&=(\theta_0, N^{-1/2} x_0^1, \ldots, N^{-1/2} x_0^N), \\
    z^\star &= (\theta^\star, N^{-1/2} x^\star, \ldots, N^{-1/2} x^\star).
\end{align*}
\end{prop}
\begin{proof}
See Appendix~\ref{sec:proof:conv_to_inv}.
\end{proof}
We note that the proof uses the contraction of the rescaled particle system which implies the convergence of the $\theta$-marginal (since the $\theta$-coordinate is not modified).


\subsection{Discretisation Error}\label{subsec: disc error}
The final part of analysis will consist of discretisation analysis of the discrete-time scheme \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x}, i.e., the error analysis of this scheme w.r.t. to the \eqref{eq: IPS noised}.
Although the $L^2$ convergence rates presented here are well established given the regularity assumed for $\nabla U$, we present full arguments in order to demonstrate that our bounds are independent of the number of particles $N$.

For the sake of proving convergence bounds for the discretisation it is convenient to extend the original definition of the discretisation to a time continuous process which agrees with the the original Euler-Maruyama scheme at times $t \in \mathbb{N}$ (we shall use the same notation since this new process agrees with the old one at integer times):
\begin{align}\label{eq: interpolation}
\theta_{t} &= \theta_0-\frac{1}{N}\sum_{j=1}^N\int^{t\gamma}_0  \nabla_{\theta} U(\theta_{\lfloor \frac{s}{\gamma} \rfloor}X_{\lfloor \frac{s}{\gamma} \rfloor}^{j, N}) \rmd s+\sqrt{\frac{2}{N}}\int^{t\gamma}_0 \rmd\bm{B}^{0,N}_s, \\
X_t^{i, N} &= X_0^{i, N} - \int^{t\gamma}_0 \nabla_x U(\theta_{\lfloor \frac{s}{\gamma} \rfloor}, X_{\lfloor \frac{s}{\gamma} \rfloor}^{i, N}) \rmd s + \sqrt{2}\int^{t\gamma}_0 \rmd\bm{B}_s^{i, N}.\label{eq: interpolation_X}
\end{align}

Noting then that $\theta_t$ is an estimate for $\bm{\theta}_{\gamma t}$, we can bound the error between this continuous time process and~\eqref{eq: IPS noised}. 

\begin{prop} \label{prop: discr error 1}
Let \Cref{Lipschitz assump},~\Cref{Conv assump} hold. Then for every $\gamma_0 \in (0, \min\{L^{-1}, 2\mu^{-1}\})$ there exists a constant $C>0$ independent of $t,n,N,\gamma$ such that for every $\gamma \in (0, \gamma_0)$ one has
\begin{equation} \label{eq: Lipschtiz disc error}
\mathbb{E}\left[\Vert \theta_n -\bm{\theta}_{n\gamma} \Vert^2\right]^{1/2} \leq C_1 (1+\sqrt{d_\theta/N+d_x})\gamma^{1/2},
\end{equation}
for all $n\in\mathbb{N}$.
\end{prop}
\begin{proof}
See Appendix~\ref{app:time_disc_1}.
\end{proof}


\subsection{Global Error}

Combining the results obtained in the previous sections, we are able to give precise bounds on the accuracy of Algorithm~\ref{algo:ipsem} in terms of $N$, $\gamma$, $n$ and the convexity properties of $U$.
\begin{thm}
\label{thm:main}
Let \Cref{Lipschitz assump},~\Cref{Conv assump} hold. Then for every $\gamma_0 \in (0, \min\{L^{-1}, 2\mu^{-1}\})$ there exists a constant $C_1 > 0$ independent of $t,n,N,\gamma$ such that for every $\gamma \in (0, \gamma_0)$ one has
\begin{align*}
\mathbb{E}\left[\Vert \theta_n-\theta^\star \Vert^2\right]^{1/2} \leq \sqrt{\frac{2d_\theta}{N\mu}}+e^{-\mu n \gamma} \left(\|z_0 - z^\star\| + \left(\frac{d_xN + d_\theta}{N \mu}\right)^{1/2}\right)+C_1 (1+\sqrt{d_\theta/N +d_x})\gamma^{1/2},
\end{align*}
for all $n\in\mathbb{N}$.
\end{thm}
\begin{proof}
Let us denote by $\mathcal{L}(\theta_n)$ the law of $\theta_n$ obtained with Algorithm~\ref{algo:ipsem}. Then, we can decompose the expectation into a term describing the concentration of the $\pi^N_\Theta$ around $\theta^\star$, a term describing the convergence of~\eqref{eq: IPS noised} to its invariant measure, and a term describing the error induced by the time discretisation:
\begin{align*}
\mathbb{E}\left[\Vert \theta_n-\theta^\star \Vert^2\right]^{1/2} &= W_2(\delta_{\theta^\star}, \mathcal{L}(\theta_n))\\
&\leq W_2(\delta_{\theta^\star}, \pi^N_\Theta)+ W_2(\pi^N_\Theta, \mathcal{L}(\bm{\theta}_{n\gamma}))+W_2(\mathcal{L}(\bm{\theta}_{n\gamma}), \mathcal{L}_{\theta_0}(\theta_n))\\
&\leq \sqrt{\frac{2d_\theta}{N\mu}}+e^{-\mu n \gamma} \left(\|z_0 - z^\star\| + \left(\frac{d_xN + d_\theta}{N \mu}\right)^{1/2}\right) + C_1(1+\sqrt{d_\theta/N +d_x})\gamma^{1/2},
\end{align*}
where the first equality stems from the fact that $\delta_{\theta^\star}$ is a deterministic measure, the second inequality is a triangle inequality for Wasserstein distances, and the last inequality follows combining Proposition~\ref{prop:concentration}, \eqref{eq:ergocidity_theta} and Proposition~\ref{prop: discr error 1}.
\end{proof}

\section{Related work}\label{sec:related}


\subsection{Maximum Marginal Likelihood Estimation with Gradient Flows}

Our approach is based on observing that in order to maximise the marginal likelihood~\eqref{eq:def:k} one can employ standard tools from Langevin-based optimisation. \citet{neal1998view} and, more recently, \cite{kuntz2022scalable} observe that EM can be seen as application of coordinate ascent to the free-energy functional $F:\real^{d_\theta}\times\cP(\real^{d_x})\to \mathbb{R}$
\begin{align*}
    F(\theta, \nu) := - \int U(\theta, x)\nu( x)\rmd x - \int \log\left( \nu(x)\right)\nu( x)\rmd x.
\end{align*}
The approach of \cite{kuntz2022scalable} leverages recent advances in minimisation of functionals over spaces of probability measures \citep{otto2001geometry, jordan1998variational} to obtain an IPS approximating the minimiser $(\theta^\star, \nu^\star)\in \mathbb{R}^{d_\theta}\times\cP(\real^{d_x})$ of $F$. In this setting, $\theta^\star$ corresponds to the MMLE estimate, while $\nu^\star = p_{\theta^\star}(x|y)$ is the posterior
distribution of the latent variable $x$ given the observed data $y$.
\citet[Theorem 1--2]{kuntz2022scalable} establish that minimising $k$ in~\eqref{eq:def:k} is equivalent to minimising $F$.

The minimisation of $F$ is achieved by deriving a Wasserstein gradient flow for $F$, which can be seen as an extension of standard gradient flows to spaces of probability measures. The Wasserstein gradient flow for $F$ is given by a Fokker-Plank PDE~\eqref{eq:fp} with drift
\begin{align*}
b(\theta, x, \nu) = \left(-\int_{\real^{d_x}} \nabla_\theta U(\theta, x)\nu(x)\rmd x, -\nabla_x U(\theta, x)\right),
\end{align*}
where $\nu$ denotes the law of $x$, and diffusion tensor $D$ given by a constant diagonal matrix with diagonal entries $D_{ii} = 0$ for $i=1, \dots, d_\theta$ and $D_{ii} = 1$ for the remainder. We refer to
\citet[Appendix B]{kuntz2022scalable} for the derivation of this Fokker-Plank equation and \citet{ambrosio2008gradient} for a full treatment of Wasserstein gradient flows.

The associated SDE is a McKean--Vlasov SDE (MKVSDE), whose drift coefficient depends on the law $\nu_t$ of the solution $X_t$
\begin{align} \label{eq: Cont EM}
    \rmd \bm{\theta}_t = \left[-\int_{\real^{d_x}} \nabla_{\theta} U(\bm{\theta}_t, x)\nu_t(x)\rmd x\right] \rmd t, \qquad \rmd \bm{X}_t = -\nabla_x U(\bm{\theta}_t, X_t)\rmd t + \sqrt{2}\rmd \bm{B}_t,
\end{align}
where $(\bm{B}_t)_{t \geq 0}$ is a $d_x$-dimensional Brownian motion.
As shown in \citet[Theorem 3]{kuntz2022scalable}, under strong convexity assumption~\eqref{eq: Cont EM} will converge
exponentially fast to $\theta^\star$ and the corresponding posterior distribution $p_{\theta^\star}(x|y)$.

The MKVSDE~\eqref{eq: Cont EM} gives rise to an IPS closely related to that in~\eqref{eq: IPS noised}
\begin{equation}
\label{eq:juan_sde}
    \rmd \bm{\theta}^N_t = -\frac{1}{N}\sum_{j=1}^N \nabla_{\theta} U(\bm{\theta}^N_t, \bm{X}_t^{j, N})\rmd t, \qquad \rmd \bm{X}_t^{i, N} = -\nabla_x U(\bm{\theta}^N_t, \bm{X}_t^{i, N})\rmd t + \sqrt{2}\rmd \bm{B}_t^{i, N},
\end{equation}
for $i=1, \dots, N$, where $\{(\bm{B}_t^{i,N})_{t \geq 0}\}_{i =0}^N$ is a family of independent
Brownian motions. 
The main difference between~\eqref{eq: IPS noised} and the MKVSDE above is that the $\theta$-component evolves as on ODE rather than a SDE, as a consequence the system~\eqref{eq:juan_sde} does not have $\pi^N_\star$ as invariant measure, nor it can be easily cast as a Langevin diffusion targeting a given distribution.

Despite this difference, the IPS~\eqref{eq: IPS noised} and~\eqref{eq:juan_sde} are closely related; in Appendix~\ref{app:gf} we show that~\eqref{eq: IPS noised} converges to the MKVSDE~\eqref{eq: Cont EM}, a similar result can be established for~\eqref{eq:juan_sde} using standard tools from the literature on IPS (see, e.g., \citet{sznitman1991topics, bossy1997stochastic}).

\section{Application to Bayesian logistic regression}\label{sec:logistic}
Consider the Bayesian logistic regression in \citet[Section 4.1]{kuntz2022scalable} (see also \citet[Section 4.1]{de2021efficient}), where
\begin{align*}
    k(\theta) = (2\uppi\sigma^2)^{-d_x/2}\int_{\real^{d_x}} \left(\prod_{j=1}^{d_y}s(v_j^Tx)^{y_j}(1-s(v_j^Tx))^{1-y_j}\right)\exp\left(-\norm{x-\theta\bm{1}_{d_x}}^2/(2\sigma^2)\right)\rmd x,
\end{align*}
where $s(u):=e^u/(1+e^u)$.
As shown in \citet[Lemma S18]{de2021efficient}, we have
\begin{align*}
    U(\theta, x) = (d_x/2)\log(2\uppi\sigma^2) - \sum_{j=1}^{d_y}\left(y_j\log(s(v_j^Tx))+(1-y_j)\log(s(-v_j^Tx))\right)+\frac{\norm{x-\theta\bm{1}_{d_x}}^2}{2\sigma^2}.
\end{align*}
\citet[Proposition 1, Appendix F.2]{kuntz2022scalable} show that $U$ is strongly convex.
To see that $\nabla U$ also satisfies the Lipschitz assumption, notice that
\begin{align*}
    \nabla_\theta  U(\theta, x) &= -\langle \bm{1}_{d_x}, x-\theta\bm{1}_{d_x}\rangle/\sigma^2\\
    \nabla_x U(\theta, x) &= \frac{x-\theta\bm{1}_{d_x}}{\sigma^2} - \sum_{i=1}^{d_y}\left(y_j-s(v_j^Tx)\right)v_j.
\end{align*}
Then, 
\begin{align*}
    \norm{\nabla_\theta  U(\theta, x)-\nabla_\theta  U(\theta', x')}^2 &= \sigma^{-4}\left(d_x(\theta-\theta')+\sum_{i=1}^{d_x}(x_i'-x_i)\right)^2\\
    &\leq 2\sigma^{-4}d_x^2\left(\norm{\theta-\theta'}^2+\norm{x'-x}^2\right)
\end{align*}
and $\nabla_x U(\theta, x)$ is Lipschitz continuous in both variables since it is linear in $\theta$ and Lipschitz continuous in $x$ as shown in \citet[Section 4.1]{de2021efficient}.

\section{Conclusions}\label{sec:conclusion}
We have proved convergence rates and nonasymptotic bounds for a class of numerical schemes that aim at solving the maximum marginal likelihood problem. Our proof techniques introduce a novel way to tackle this problem with such schemes and paves the way of a new class of results which can be proved using a similar machinery. In particular, we plan to extend this work in various directions, including relaxed Lipschitz assumptions and for nonconvex (disspative) $U$.
\section*{Acknowledgements}
This work has been supported by The Alan Turing Institute through the Theory and Methods Challenge Fortnights event ``Accelerating generative models and nonconvex optimisation",
which took place on 6-10 June 2022 and 5-9 Sep 2022 at The Alan Turing Institute headquarters. M.G. was supported by EPSRC grants EP/T000414/1, EP/R018413/2, EP/P020720/2, EP/R034710/1, EP/R004889/1, and a Royal Academy of Engineering Research Chair.
F.R.C. and O.D.A. acknowledge support from the EPSRC (grant \#  EP/R034710/1). 


\bibliographystyle{agsm}
\bibliography{sample}

\cleardoublepage


\appendix


\section{Supporting Lemmata}

Let us denote, for any $\theta\in\real^{d_\theta}$ and $\nu\in\cP(\real^{d_x})$, $g(\theta, \nu):= \int_{\real^{d_x}} \nabla_{\theta} U(\theta, x^\prime)\nu(x^\prime)\rmd x^\prime$.

\begin{lemma}
\label{lem:b_lip}
Under~\Cref{Lipschitz assump}, the function $g:\real^{d_\theta} \times \cP(\real^{d_x}) \to \mathbb{R}^{d_theta} $ is Lipschitz continuous, i.e.,
\begin{align*}
    \|g(\theta_1, \nu_1) - g(\theta_2, \nu_2)\| \leq L_{\theta}\left( \|\theta_1 - \theta_2\| +W_1(\nu_1, \nu_2)\right).
\end{align*}
In addition, under~\Cref{Conv assump}, we have, for fixed $\nu \in \cP(\cX)$,
\begin{align*}
    \mu \|\theta_1 - \theta_2\|^2 \leq \langle \theta_1 - \theta_2, g(\theta_1, \nu) - g(\theta_2, \nu)\rangle.
\end{align*}
\end{lemma}
\begin{proof}
We have
\begin{align*}
  \|g(\theta_1, \nu_1) - g(\theta_2, \nu_2)\| &\leq \|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_1, x^\prime)\nu_1(x^\prime)\rmd x^\prime - \int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\nu_2(x^\prime)\rmd x^\prime\|.
\end{align*}
We can decompose the above and use~\Cref{Lipschitz assump}
\begin{align*}
 &  \left\|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_1, x^\prime)\nu_1(x^\prime)\rmd x^\prime - \int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\nu_2(x^\prime)\rmd x^\prime\right\|\\
 &\qquad\leq \left\|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_1, x^\prime)\nu_1(x^\prime)\rmd x^\prime - \int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\nu_1(x^\prime)\rmd x^\prime\right\|  \\
 &\qquad+ \left\|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\left[\nu_1(x^\prime)-\nu_2(x^\prime)\right]\rmd x^\prime\right\|.
\end{align*}
Since $\nabla U(\theta,x)$ is $L_x$-Lipschitz,  $\nabla U(\theta,x)/L_x$ is $1$-Lipschitz and we can use the dual representation of $W_1$ distance to obtain 
\begin{align}
    \label{eq:lemma1}
    \left\|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\left[\nu_1(x^\prime)-\nu_2(x^\prime)\right]\rmd x^\prime\right\| &\leq L_xW_1(\nu_1, \nu_2).
\end{align}
Using again the Lipschitz continuity of $\nabla U(\theta, x)$ we also have
\begin{align}
    \label{eq:lemma2}
    \left\|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_1, x^\prime)\nu_1(x^\prime)\rmd x^\prime - \int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\nu_1(x^\prime)\rmd x^\prime\right\| \leq L_\theta \|\theta_1 - \theta_2\|.
\end{align}
Combining~\eqref{eq:lemma1} and~\eqref{eq:lemma2}, we obtain
\begin{align*}
 &   \left\|\int_{\real^{d_x}} \nabla_{\theta} U(\theta_1, x^\prime)\nu_1(x^\prime)\rmd x^\prime - \int_{\real^{d_x}} \nabla_{\theta} U(\theta_2, x^\prime)\nu_2(x^\prime)\rmd x^\prime\right\| \leq L_\theta\|\theta_1 - \theta_2\| +L_\theta W_1(\nu_1, \nu_2).
 \end{align*}
% To obtain the second statement, note that, using~\Cref{Conv assump},
% \begin{align*}
%     \langle \theta_1 - \theta_2, g(\theta_1, \nu) - g(\theta_2, \nu)\rangle & = \int_{\real^{d_x}}\langle \theta_1 - \theta_2, \nabla_\theta U(\theta_1, x^\prime) - \nabla_\theta U(\theta_2, x^\prime)\rangle \nu(x^\prime)\rmd x^\prime\\
%     &\geq \mu\norm{\theta_1-\theta_2}^2.
% \end{align*}
\end{proof}

\section{Proofs of Section~\ref{sec: analysis}}
\label{app:ergodicity}

Recall that we write continuous time processes such as \eqref{eq: IPS noised} in bold, and their Euler-Maryama discretisation as normal letters. Recall as well the continuous interpolation \eqref{eq: interpolation}--\eqref{eq: interpolation_X} of discrete-time scheme \eqref{eq:IPS_disc_theta}--\eqref{eq:IPS_disc_x}, for which we use the same notation since they agree at integer points (so that $\theta_t$ is an approximation of $\bm{\theta}_{\gamma t}$). Finally let us define, similarly to \eqref{eq: Z}, the rescaled continuous interacting particle system
\begin{equation}
\label{eq:Zt}
\bm{Z}^N_t:=(\bm{\theta}^N_t, N^{-1/2}\bm{X}^{1,N}_t, \dots, N^{-1/2}\bm{X}^{N,N}_t).
\end{equation}
We also denote the target measure of the rescaled system as $\pi_{\star,z}^N$. Finally, for notational convenience we define
\begin{equation}\label{eq: Y defn}
\bm{V}^{i,N}_t=(\bm{\theta}^N_t, \bm{X}^{i,N}_t), \;\;\;V^{i,N}_t=(\theta^N_t, X^{i,N}_t)
\end{equation}

\subsection{Proof of Proposition~\ref{prop: conv to inv}}\label{sec:proof:conv_to_inv}
\begin{proof}
The proof has 5 steps which we outline below.

\begin{enumerate}[label=(\roman*)]
\item  Let $v^\star= (\theta^\star, x^\star)$ be the minimiser of $U$, so $\nabla_\theta U(v^\star)$ and $\nabla_x U(v^\star)$ are vectors with identically zero entries. Let $z^\star=(\theta^\star, \frac{1}{\sqrt{N}}x^\star, ..., \frac{1}{\sqrt{N}}x^\star)$. We first bound $\bE\left[\|\bm{Z}^N_t - z^\star\|^2\right]$
    \begin{align*}
\bE\left[\|\bm{Z}^N_t - z^\star\|^2 \right]&= \|z_0 - z^\star\|^2 - 2 \bE \left[ \int_0^t \left\langle \bm{\theta}^N_s - \theta^\star, \frac{1}{N}\sum_{i=1}^N(\nabla_\theta U(\bm{V}^{i,N}_s) - \nabla_\theta U(v^\star))\right\rangle\rmd s \right] + 2 \frac{(N d_x + d_\theta)t}{N}\\
&-2\frac{1}{N}\sum_{i=1}^N \bE\left[ \int_0^t \left\langle \bm{X}^{i,N}_s - x^\star, (\nabla _xU(\bm{V}^{i,N}_s) - \nabla_x U(v^\star))\right\rangle\rmd s \right] \\
& = \|z_0 - z^\star\|^2 - 2 \frac{1}{N}\sum_{i=1}^N\int_0^t \bE \left[\left\langle \bm{V}^{i,N}_s - v^\star, \nabla U(\bm{V}^{i,N}_s) - \nabla U(v^\star)\right\rangle \right]\rmd s+ 2 \frac{(N d_x + d_\theta)t}{N} \\
&\leq \|z_0 - z^\star\|^2 - 2 \mu\int_0^t \bE \Vert  \bm{Z}^N_s-z^\star \Vert^2 \rmd s+ 2 \frac{(N d_x + d_\theta)t}{N}. 
\end{align*}
We compute derivatives of both sides
\begin{align*}
    \frac{\md \bE\left[\|\bm{Z}^N_t - z^\star\|^2\right]}{\md t} &= -2 \bE \left[\left\langle \bm{Z}^N_t - z^\star, \nabla U(\bm{Z}^N_t) - \nabla U(z^\star)\right\rangle \right]+ 2 \frac{N d_x + d_\theta}{N},\\
    &\leq -2 \mu \bE\left[\|\bm{Z}^N_t - z^\star\|^2 \right]+ 2 \frac{N d_x + d_\theta}{N}.
\end{align*}
Consider next
\begin{align*}
    \frac{\md e^{2\mu t} \bE\left[\|\bm{Z}^N_t - z^\star\|^2\right]}{\md t} &= 2\mu e^{2 \mu t} \bE^{z_0}\left[\|\bm{Z}^N_t - z^\star\|^2\right] + e^{2\mu t} \frac{\md}{\md t}\bE^{z_0}\left[\|\bm{Z}^N_t - z^\star\|^2\right], \\
    &\leq 2\mu e^{2\mu t} \bE^{z_0}\left[\|\bm{Z}^N_t - z^\star\|^2\right] + e^{2\mu t} (-2\mu \bE^{z_0}\left[\|\bm{Z}^N_t - z^\star\|^2\right] + 2 \frac{N d_x + d_\theta}{N}, \\
    &= 2e^{2 \mu t}  \frac{N d_x + d_\theta}{N}.
\end{align*}
Integrating both sides gives
\begin{align*}
    \bE^{z_0}\left[\|\bm{Z}^N_t - z^\star\|^2 \right]e^{2 \mu t} - \|z_0 - z^\star\|^2 \leq 2\frac{N d_x + d_\theta}{N} \int_0^t e^{2\mu s} \md s.
\end{align*}
This implies that
\begin{align}
\label{eq:concentration1}
    \bE^{z_0}\left[\|\bm{Z}^N_t - z^\star\|^2\right] \leq e^{-2\mu t} \|z_0 - z^\star\|^2 + \frac{N d_x + d_\theta}{N \mu}(1 - e^{-2\mu t}).
\end{align}

\item Now we use the fact that $\pi_\star^N$ is the invariant measure of our SDE, i.e., $\pi_\star^N P_t^N = \pi_\star^N$ for the Markov kernel of our particle system $P_t^N$. It then follows by the definition \eqref{eq:Zt} that the rescaled system has a rescaled invariant measure, that we denote $\pi^N_{\star,z}$, so that if we denote the rescaled transition kernel of $Z^N_t$ as $\tilde{P}_t^N$ then one has $\pi^N_{\star,z} \tilde{P}_t^N = \pi^N_{\star,z}$. Hence by \eqref{eq:concentration1} one can write
\begin{align*}
    \int_{\real^{d_\theta+N d_x }} \|z- z^\star\|^2 \pi^N_{\star,z}(\md z)&= \int_{\real^{d_\theta+N d_x }} \int_{\real^{d_\theta+N d_x }} \|z- z^\star\|^2 \tilde{P}_t^N(z', \md z)\pi^N_{\star,z} (\md z') \\
    &=\int_{\real^{d_\theta+N d_x }} \bE^{z'}\left[\|\bm{Z}^N_t - z^\star\|^2\right] \pi^N_{\star,z}(\md z'), \\
    &\leq e^{-2\mu t} \int_{\real^{d_\theta+N d_x }} \|z- z^\star\|^2 \pi^N_{\star,z}(\md z)+ \frac{N d_x + d_\theta}{N \mu}(1 - e^{-2\mu t}),
\end{align*}
which implies that
\begin{align*}
    \int_{\real^{d_\theta+N d_x }} \|z- z^\star\|^2 \pi^N_{\star,z}(z)\md z\leq \frac{d_\theta+N d_x}{N \mu}.
\end{align*}
\item  Now let us show that \eqref{eq: IPS noised} is a contractive flow with respect to the Wasserstein distance. Indeed
\begin{align*}
    \bE^{z_0,\tilde{z}_0}\left[\|\bm{Z}^N_t - \tilde{\bm{Z}}_t^N\|^2\right] & = \Vert z_0-\tilde{z}_0 \Vert^2+
  \frac{2}{N} \sum^N_{i=1}\mathbb{E}\left[\int^t_0  \langle \nabla _\theta U(\bm{V}^{i,N}_s)-\nabla _\theta U(\tilde{\bm{V}}^{i,N}_s), \bm{\theta}_s-\tilde{\bm{\theta}}_s \rangle \rmd s\right]\\
   &+   \frac{2}{N}\sum^N_{i=1} \mathbb{E}\left[\int^t_0 \langle \nabla _xU(\bm{V}^{i,N}_s)-\nabla _x U(\tilde{\bm{V}}^{i,N}_s), \bm{X}^{i,N}_s-\tilde{\bm{X}}^{i,N}_s \rangle \rmd s \right]\\
   & =   \frac{2}{N}\sum^N_{i=1} \mathbb{E}\left[\int^t_0 \langle \nabla U(\bm{V}^{i,N}_s)-\nabla  U(\tilde{\bm{V}}^{i,N}_s), (\bm{\theta}^N_s, \bm{X}^{i,N}_s)-(\bm{\tilde{\theta}}^N_s, \tilde{\bm{X}}^{i,N}_s) \rangle \rmd s\right],
\end{align*}
so that we can differentiate and apply the convexity assumption~\Cref{Conv assump} to each term in the sum to obtain
\begin{align*}
  \frac{\rmd}{\rmd t}  \bE^{z_0,\tilde{z}_0}\left[\|\bm{Z}^N_t - \tilde{\bm{Z}}_t^N\|^2\right] & \leq -2\mu \mathbb{E}\left[\Vert \bm{Z}^N_s - \tilde{\bm{Z}}_s \Vert^2\right],
\end{align*}
so that
\begin{equation*}
    \bE^{z_0,\tilde{z}_0}\left[\|\bm{Z}^N_t - \tilde{\bm{Z}}_t^N\|^2\right] \leq e^{-2 \mu t} \|z_0 - \tilde{z}_0\|^2,
\end{equation*}
and therefore by definition
\begin{align*}
    W_2(\delta_{z_0} \tilde{P}_t^N, \delta_{\tilde{z}_0} \tilde{P}_t^N) \leq e^{-\mu t} \|z_0 - \tilde{z}_0\|.
\end{align*}
\item Finally, we combine all the steps above and write
\begin{align*}
    W_2(\delta_{z_0} \tilde{P}_t^N, \pi_{\star,z}^N) &\leq W_2(\delta_{z^\star}\tilde{P}_t^N, \delta_{z_0} \tilde{P}_t^N) + W_2(\delta_{z^\star}
\tilde{P}_t^N, \pi_{\star,z}^N) \\
    % &\leq e^{-\mu t} \|z_0 - z^\star\| + \bE^{z^\star}[\|\bm{Z}^N_t - z\|^2]^{1/2}, \quad \quad z\sim \pi_{\star,z}^N) \\
    &\leq e^{-\mu t} \|z_0 - z^\star\| + \bE^{z^\star, z}[\|\bm{Z}^N_t - \tilde{\bm{Z}}_t^N\|^2]^{1/2}, \quad \quad z\sim \pi_{\star,z}^N \\
    % &\leq e^{-\mu t} \|z_0 - z^\star\| + \sqrt{\int \int \|w - z\|^2 P_t^N(z^\star, \md w) \pi_{\star,z}^N)(\md z)},\\
    & \leq  e^{-\mu t} \|z_0 - z^\star\| + e^{-\mu t}\bE[\|z^\star - z\|^2]^{1/2},\\
    &\leq e^{-\mu t} \left\lbrace \|z_0 - z^\star\| + \left(\frac{N d_x + d_\theta}{N\mu}\right)^{1/2}\right\rbrace.
\end{align*}
\item Steps (i)--(iv) establish exponential ergodicity of the process $\bm{Z}_t^N$ to the \textit{rescaled measure} $\pi_{\star,z}^N$. This is different than $\pi_\star^N$, however, since the rescaling only affects $x$-coordinates, the $\theta$-marginal of $\pi_{\star,z}^N$ coincides with $\pi_\Theta^N$. As a consequence of this result, since the Wasserstein distance decreases upon projection to a linear subspace, we additionally have that
\begin{align}
\label{eq:ergocidity_theta}
    W_2(\mathcal{L}_{\theta_0}(\bm{\theta}^N_t), \pi_{\Theta}^N) \leq e^{-\mu t} \left(\|z_0 - z^\star\| + \left(\frac{d_xN + d_\theta}{N \mu}\right)^{1/2}\right),
\end{align}
where $\mathcal{L}_{\theta_0}$ denotes the law of $\bm{\theta}^N_t$ with initial condition $\theta_0$.
\end{enumerate}
\end{proof}

\subsection{Euler--Maruyama discretisation}\label{app:time_disc_1}

\subsubsection{Moment bound}
We first prove an increment bound which will be crucial for bounding the numerical discretisation error.
\begin{lemma}\label{lem: numerics bdds}
Let \Cref{Lipschitz assump},~\Cref{Conv assump} hold. Then for every $\gamma_0 \in (0, \min\{L^{-1}, 2\mu^{-1}\})$ there exists a constant $C>0$ independent of $t,n,N,\gamma$ such that for every $\gamma \in (0, \gamma_0)$, $n \in \mathbb{N}$ and $t\in [n,n+1]$ one has
\begin{equation}\label{eq: lem bound 2}
    E[\lvert \lvert Z^N_t-Z^N_n \rvert \rvert^2]\leq C(1+d_\theta/N+d_x)\gamma.
\end{equation}

\end{lemma}
\begin{proof}
For this it shall be marginally more convenient to consider the original discrete definition of $Z^N_n$ in \eqref{eq:z_rescaling_intro}. Furthermore let $v^\star$ and $z^*$ be as in Proposition \ref{prop: conv to inv}. We begin by showing
\begin{equation}\label{eq: lem bound 1}
E[\lvert \lvert Z^N_n \rvert \rvert^2]\leq C(1+d_\theta/N+d_x), 
\end{equation}
for a constant $C>0$ independent of $N$, $n$, $d_\theta$ and $d_x$. Firstly we recall that a standard Gaussian on $\mathbb{R}^d$ has second moment equal to $d$, so that using the definition \eqref{eq: Y defn} of the process $V^{i,N}_t$ one has
\begin{align}\label{eq: moment calc}
\mathbb{E}\left[\Vert Z^N_{n+1} -z^\star\Vert^2 \right]&\leq \mathbb{E}\left[\Vert Z^N_n -z^\star\Vert^2\right] + \frac{\gamma^2}{N^2}\mathbb{E}\left[\biggr (\sum_{i=1}^N \Vert \nabla_{\theta} U(V^{i,N}_n) \Vert \biggr )^2 \right] + \frac{2\gamma d_\theta}{N}-\frac{2\gamma}{N}\langle \theta_n-\theta^\star,  \sum_{j=1}^N E \left[\nabla_{\theta} U(V_n^{j, N} )\rangle \right] \nonumber \\
&+\frac{1}{N}\sum_{i=1}^N \biggr (  \gamma^2 \mathbb{E}\left[\Vert \nabla_x U(V_n^{i, N}) \Vert^2\right]+ 2\gamma d_x- 2\gamma \mathbb{E}\left[\langle X_n^{i, N}-x*, \nabla_x U(V_n^{i, N}) \rangle \right]\biggr ).
\end{align}
Now note that 
\begin{equation}
\frac{1}{N}\biggr(\sum_{i=1}^N\langle \theta, \nabla_\theta U(\theta, x_i) \rangle+ \langle x_i, \nabla_x U(\theta, x_i) \rangle \biggr) = \frac{1}{N}\sum_{i=1}^N \langle (\theta, x_i), \nabla U(\theta, x_i) \rangle,
\end{equation}
and furthermore by Young's inequality
\begin{equation}
\frac{1}{N^2}\biggr (\sum_{i=1}^N \Vert \nabla_{\theta} U(\theta, x_i) \Vert \biggr )^2 \leq \frac{1}{N}\sum_{i=1}^N \Vert \nabla_{\theta} U(\theta, x_i) \Vert ^2,
\end{equation}
so that since by definition $\nabla U(v^\star)=0$ one has
\begin{align}
\mathbb{E}\left[\Vert Z^N_{n+1} -z^\star\Vert^2 \right]&\leq  \mathbb{E}\left[\Vert Z^N_n -z^\star\Vert^2\right]- \frac{2\gamma}{N}\mathbb{E}\left[\sum_{i=1}^N \langle V^{i,N}_N-v^\star, \nabla U(V^{i,N}_n) -\nabla U(v^\star)\rangle\right] \nonumber \\
&+ \gamma(\frac{2d_\theta}{N}+2\gamma d_x)+\frac{\gamma^2}{N} \biggr (\mathbb{E}\left[\biggr (\sum_{i=1}^N \Vert \nabla U(V^{i,N}_n) -\nabla U(v^\star)\Vert ^2\biggr )\right] .
\end{align}
Furthermore, since $U$ is convex, we may use co-coercivity, given as
\begin{equation}
\langle \nabla U(x)-\nabla U(y),x-y \rangle \geq \frac{1}{L}\Vert \nabla U(x)-\nabla U(y) \Vert^2,
\end{equation}
for every $x,y \in \mathbb{R}^d$, to conclude for every $\gamma < \frac{1}{L}$ 
\begin{equation}
- \frac{\gamma}{N}\mathbb{E}\left[\sum_{i=1}^N \langle V^{i,N}_N-v^\star, \nabla U(V^{i,N}_n) -\nabla U(v^\star)\rangle\right]+\frac{\gamma^2}{N} \biggr (\mathbb{E}\left[\biggr (\sum_{i=1}^N \Vert \nabla U(V^{i,N}_n) -\nabla U(v^\star)\Vert ^2\biggr ) \right ] \leq 0.
\end{equation}
Therefore by the convexity assumption \Cref{Conv assump}
\begin{align}
\mathbb{E}\left[\Vert Z^N_{n+1} -z^\star\Vert^2 \right]&\leq  \mathbb{E}\left[\Vert Z^N_n -z^\star\Vert^2\right]- \frac{\gamma}{N}\mathbb{E}\left[\sum_{i=1}^N \langle V^{i,N}_N-v^\star, \nabla U(V^{i,N}_n) -\nabla U(v^\star)\rangle\right] \nonumber \\
&+ \gamma(\frac{2d_\theta}{N}+2\gamma d_x)\nonumber \\
&\leq  (1-\mu\gamma) \mathbb{E}\left[\Vert Z^N_n -z^\star\Vert^2\right]+ 2\gamma(\frac{d_\theta}{N}+d_x),
\end{align}
so that using standard bounds on recursive sequence, for every $\gamma <\frac{2}{\mu}$ one obtains that
\begin{align}
\mathbb{E}\left[\Vert Z^N_{n+1} -z^\star\Vert^2 \right]\leq  (1-\mu \gamma)^{n+1}\Vert Z^N_{0} -z^\star\Vert^2+ \mu^{-1}(\frac{d_\theta}{N}+ d_x),
\end{align}
and as a result
\begin{align} \label{eq: scheme L2 bound}
\mathbb{E}\left[\Vert Z^N_{n} \Vert^2 \right]& \leq 2\mathbb{E}\left[\Vert Z_n -z^\star\Vert^2 \right]+2 \Vert z^\star\Vert^2 \nonumber \\
&\leq C(1+\frac{d_\theta}{N}+ d_x).
\end{align}
Now to bound the increment one can similarly calculate
\begin{align}
\mathbb{E}\left[\Vert Z^N_{n+1} -Z^N_n \Vert^2 \right]&\leq  - \frac{2\gamma}{N}\mathbb{E}\left[\sum_{i=1}^N \langle V^{i,N}_N, \nabla U(V^{i,N}_n) \rangle\right] \nonumber \\
&+ 2\gamma(\frac{d_\theta}{N}+d_x)+\frac{\gamma^2}{N}  \mathbb{E}\left[\sum_{i=1}^N \Vert \nabla U(V^{i,N}_n) 
\Vert ^2\right] ,
\end{align}
so that applying Cauchy-Schwartz, Young's inequality and the Lipschitz assumption \Cref{Lipschitz assump} one has
\begin{align}
\mathbb{E}\left[\Vert Z^N_{n+1} -Z^N_n \Vert^2 \right]&\leq  C\gamma(1+\frac{1}{N}\sum_{i=1}^N E[\Vert V^{i,N}_n \Vert^2]).
\end{align}
Therefore, since $\frac{1}{N}\sum_{i=1}^N\Vert V^{i,N}_n \Vert^2 = \Vert Z^N_n\Vert^2$, the result follows from \eqref{eq: scheme L2 bound}.
\end{proof}

\subsubsection{Proof of Proposition~\ref{prop: discr error 1}}
\begin{proof}
In order to prove~\eqref{eq: Lipschtiz disc error} we consider the convergence of the entire rescaled discretized IPS $Z^N_n$ to the original continuous rescaled process $\bm{Z}^N_t$. In particular, noting that $Z^N_n$ is an approximation of $\bm{Z}^N_{n\gamma}$, we shall prove that
\begin{equation} \label{eq: Lipschitz disc error sys}
\mathbb{E}\left[\Vert Z^N_n -\bm{Z}^N_{n\gamma} \Vert^2\right] \leq C \gamma,
\end{equation}
at which point~\eqref{eq: Lipschtiz disc error} follows immediately. The interpolated process $Z^N_t$ given as
\begin{align}\label{eq: Z}
Z^N_t:=(\theta_t, N^{-1/2}X^{i,N}_t,...,N^{-1/2}X^{i,N}_t).
\end{align}
via the interpolated discretisation \eqref{eq: interpolation} is an It\^o process, so we can apply It\^o's formula for $x \mapsto \lvert x \rvert^2$ to obtain
\begin{align*}
\lvert\lvert Z^N_t - \bm{Z}^N_{\gamma t} \rvert\rvert^2 &= -\frac{1}{N} \sum_{i=1}^N  \int^{t \gamma}_0 \langle \nabla_{\theta} U(\theta_{\lfloor \frac{s}{\gamma} \rfloor},X_{\lfloor \frac{s}{\gamma} \rfloor}^{i, N}) - \nabla_{\theta} U(\bm{\theta}_{s}, \bm{X}^{i,N}_{s}), \theta_s - \bm{\theta}_s \rangle \rmd s \nonumber \\ 
& - \frac{1}{N} \sum_{i=1}^N\int^{t \gamma}_0 \langle \nabla_x U(\theta_{\lfloor \frac{s}{\gamma} \rfloor},X_{\lfloor \frac{s}{\gamma} \rfloor}^{i, N}) - \nabla_x U(\bm{\theta}_{s}, \bm{X}^{i,N}_{s}), X^{i,N}_s - \bm{X}^{i,N}_{s} \rangle \rmd s  \\ 
&= -\frac{1}{N} \sum_{i=1}^N  \int^{t \gamma}_0 \langle \nabla U(V^{i,N}_{\lfloor \frac{s}{\gamma} \rfloor}) - \nabla U(\bm{V}_{s}), V^{i,N}_s - \bm{V}^{i,N}_{s} \rangle \rmd s ,
\end{align*}
where $V^{i,N}_t:=(\theta_t, X^{i,N}_t)$ and $\bm{V}^{i,N}_t:=(\bm{\theta}_t, \bm{X}^{i,N}_t)$ for $i=1,2,...,N$ and $t>0$. For convenience let us define 
\begin{equation} \label{eq: dif process}
e^{i,N}_t:= V^{i,N}_t - \bm{V}^{i,N}_{\gamma t}, \;\; e_t = Z^N_t - \bm{Z}^N_{\gamma t}, 
\end{equation}
so that in particular $\frac{1}{N}\sum^N_{i=1} \Vert e^{i,N}_t \Vert^2 = \Vert e_t \Vert^2$. Then one can add and subtract terms in order to obtain
\begin{align}\label{eq: dif1}
\frac{\rmd}{\rmd t}\Vert e_t \Vert^2 &= -\frac{\gamma}{N}\sum_{i=1}^N    \langle \nabla U(V^{i,N}_t) - \nabla U(\bm{V}_{\gamma t}), e_t \rangle \nonumber \\ 
& -\frac{\gamma}{N}\sum_{i=1}^N \langle \nabla U(V^{i,N}_{\lfloor \frac{t}{\gamma} \rfloor}) - \nabla U(V^{i,N}_t), e_t \rangle \nonumber \\
&:=r_1(t)+r_2(t).
\end{align}
We then control $r_1(t)$ in the same way as in the proof of Proposition~\ref{prop:concentration}, and the discretisation error $r_2(n)$ using standard bounds for stochastic processes. Specifically, for the former we have by the convexity assumption~\Cref{Conv assump} that
\begin{align}\label{eq: e1}
r_1(t)&:= -\frac{\gamma}{N} \sum_{i=1}^N \langle \nabla U(V^{i,N}_t) - \nabla U(\bm{V}_{\gamma t}), e^{i,N}_t \rangle  \nonumber \\
& \leq -\mu \frac{\gamma}{N} \sum_{i=1}^N  \Vert V_t^{i, N} - \bm{V}^{i,N}_{\gamma t}\Vert^2   \nonumber \\
& \leq -\mu \gamma \Vert e_t \Vert^2 .
\end{align}
For the second part, the Cauchy-Schwarz inequality, Young's inequality and~\Cref{Lipschitz assump} give
\begin{align}\label{eq: e2}
r_2(t)& := \frac{1}{N}\sum_{i=1}^N \langle \nabla U(V^{i,N}_t)-\nabla U(V^{i,N}_{\lfloor \frac{t}{\gamma} \rfloor}) , e^{i,N}_t \rangle \nonumber \\
&\leq \frac{1}{2N}\sum_{i=1}^N  (\frac{1}{2\mu}\Vert \nabla U(V^{i,N}_{\lfloor \frac{t}{\gamma} \rfloor}) - \nabla U(V^{i,N}_t) \rvert\rvert^2 + \frac{\mu}{2}\Vert e^{i,N}_t \Vert^2 ) \nonumber \\
& \leq  C \Vert Z_{\lfloor \frac{t}{\gamma} \rfloor} - Z^N_t \Vert^2 +\frac{\mu}{2} \Vert e_t\Vert^2 , 
\end{align}
so that combining~\eqref{eq: e1} and~\eqref{eq: e2} into~\eqref{eq: dif1} and taking expectation one obtains
\begin{align}\label{eq: dif2}
\frac{\rmd}{\rmd t } \mathbb{E}\left[\Vert e_t \Vert^2\right] \leq \gamma \biggr (-\frac{\mu }{2}\mathbb{E}\left[\Vert e_t\Vert\right] +C  \mathbb{E}\left[\Vert Z^N_t-Z_{\lfloor \frac{t}{\gamma} \rfloor}  \Vert^2\right]  \biggr),
\end{align}
and therefore by Lemma \ref{lem: numerics bdds} one has
\begin{align*}
\frac{\rmd}{\rmd t}\mathbb{E}\left[ \Vert e_t \Vert^2\right] \leq \gamma \biggr (-\frac{\mu }{2} \mathbb{E}\left[ \Vert e_t \Vert^2\right] +C(1+d_\theta/N +d_x)\gamma  \biggr).
\end{align*}
Finally we apply the comparison Lemma to obtain
\begin{equation*}
\mathbb{E}\left[ \Vert e_t \Vert^2\right] \leq C(1+d_\theta/N +d_x)\gamma.
\end{equation*}
\end{proof}



\section{Convergence to Wasserstein Gradient Flow}
\label{app:gf}

Under~\Cref{Lipschitz assump}, standard results on the $N\to \infty$ limit of interacting particle systems (e.g. \citep[Theorem 1.4]{sznitman1991topics}), allows us to show the following convergence of the particle system~\eqref{eq: IPS noised} to~\eqref{eq: Cont EM} at rate $N^{-1/2}$. 
If follows that the addition of the noise in the $\theta$ component does not undermine the gradient flow behaviour of~\eqref{eq: IPS noised}.


In the following, we explicitly distinguish $\theta$-component of the solution of the gradient flow ODE~\eqref{eq: Cont EM} and the $\theta$-component of the IPS~\eqref{eq: IPS noised} and denote them by $\bm{\theta}_t$ and $\bm{\theta}_t^{1,N}$, respectively.
\begin{prop}[Propagation of chaos]
\label{prop:poc}
Under~\Cref{Lipschitz assump}, for any (exchangeable) initial condition $(\theta_0^{1,N}, X_0^{1:N})$ such that $(\theta_0^{1,N}, X_0^{j, N}) = (\theta_0, X_0)$ for $j=1,\dots, N$ with $\mathbb{E}\left[\vert \theta_0\vert^2+\vert X_0\vert^2\right]<\infty$, we have for any $T \geq 0$
  \begin{equation}
  \label{eq:poc}
      \mathbb{E}\left[\sup_{t \in [0,T]} \left(\norm{\bm{\theta}_t - \bm{\theta}_t^{1,N}}+\norm{\bm{X}_t - \bm{X}_t^{j,N}}\right)\right] \leq \frac{\sqrt{2}(\sqrt{C_T}(L_\theta+L_x)+\sqrt{T})e^{T\max\{2L_\theta, L_\theta+L_x\}}}{N^{1/2}}
  \end{equation}
  where $C_T:= \sup_{t\leq T}\mathbb{E}\left[\vert \bm{\theta}_t\vert^2+\vert \bm{X}_t\vert^2\right]<\infty$, for any $j=1, \dots, N$.
\end{prop}
\begin{proof}
The propagation of chaos estimates in~\eqref{eq:poc} follow using again a contraction argument as in \citep[Theorem 1.4]{sznitman1991topics}. For completeness, we reproduce the argument of \citep[Theorem 1.4]{sznitman1991topics} for~\eqref{eq: IPS noised}.

Let us denote by $\nu_s$ the law of $\bm{X}_s$ and by $\nu_s^N$ the empirical measure $\nu_s^N:=N^{-1}\sum_{j=1}^N\delta_{\bm{X}_s^{j,N}}$.
Consider $N$ independent copies of~\eqref{eq: Cont EM}, $\widetilde{\bm{X}}_t^{1:N}$, and the corresponding empirical measure $\tilde{\nu}_t^N:=N^{-1}\sum_{j=1}^N\delta_{\widetilde{\bm{X}}_t^{j,N}}$.
We can decompose
\begin{align*}
    \norm{g(\bm{\theta}_s, \nu_s)-g(\bm{\theta}_s^{1,N}, \nu_s^N)} & \leq \norm{g(\bm{\theta}_s, \nu_s)-g(\bm{\theta}_s^{1,N}, \tilde{\nu}_s^N)}+\norm{g(\bm{\theta}_s, \tilde{\nu}^N_s)-g(\bm{\theta}_s^{1,N}, \nu_s^N)}\\
    &\leq \norm{g(\bm{\theta}_s, \nu_s)-g(\bm{\theta}_s, \tilde{\nu}_s^N)}+L_\theta\left(\norm{\bm{\theta}_s-\bm{\theta}_s^{1,N}}+W_1(\tilde{\nu}_s^N, \nu_s^N)\right)\\
    &\leq \norm{g(\bm{\theta}_s, \nu_s)-g(\bm{\theta}_s, \tilde{\nu}_s^N)}+L_\theta\left(\norm{\bm{\theta}_s-\bm{\theta}_s^{1,N}}+\norm{\widetilde{\bm{X}}_s^{1:N}-\bm{X}_s^{1:N}}\right)
\end{align*}
where we used the Lipschitz continuity of $g$ established in Lemma~\ref{lem:b_lip} and the fact that $\tilde{\nu}_s^N, \nu_s^N$ are empirical measures.

Putting the above together and using \citep[Theorem 66]{protter2005stochastic} we obtain
\begin{align*}
    \mathbb{E}\left[\sup_{s \in [0,t]} \left(\norm{\bm{\theta}_s - \bm{\theta}_s^{1,N}}+\norm{\bm{X}_s - \bm{X}_s^{j,N}}\right)\right] &\leq \int_0^t 2L_\theta\mathbb{E}\left[\norm{\bm{\theta}_s - \bm{\theta}_s^{1,N}}\right] + (L_x+L_\theta)\mathbb{E}\left[\norm{\bm{X}_s - \bm{X}_s^{j,N}}\right]\rmd s\\
    &+\int_0^t \mathbb{E}\left[\norm{g(\bm{\theta}_s, \nu_s) - g(\bm{\theta}_s, \tilde{\nu}_s^N)}\right]\rmd s + \sqrt{\frac{2}{N}}\mathbb{E}\left[\vert\int_0^t \rmd \bm{B}_s\vert\right].
\end{align*}
We use the definition of $g$ and Jensen's inequality to write
\begin{align}
\label{eq:poc_var1}
\mathbb{E}\left[\norm{g(\bm{\theta}_s, \nu_s) - g(\bm{\theta}_s, \tilde{\nu}_s^N)}\right] & = \mathbb{E}\left[\norm{\int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - N^{-1}\sum_{k=1}^N \nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{k,N})}\right]\\
&\leq \mathbb{E}^{1/2}\left[\norm{\int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - N^{-1}\sum_{k=1}^N \nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{k,N})}^2\right].\notag
\end{align}
Expanding the above we have
\begin{align*}
&\mathbb{E}\left[\norm{\int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - N^{-1}\sum_{k=1}^N \nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{k,N})}^2\right]\\
&=\frac{1}{N^2}\sum_{r_1, r_2=1}^N \mathbb{E}\left[\langle\int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - \nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{r_1,N}), \int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - \nabla_{\theta}U(\bm{\theta}_s, \tilde{X}_s^{r_2,N})\rangle\right]\\
&=\frac{1}{N} \mathbb{E}\left[\norm{\int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - \nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{1,N})}^2\right],
\end{align*}
since by independence of the $\tilde{\bm{X}}_s^{j,N}$ the second expectation is 0 whenever $r_1\neq r_2$ and the $\tilde{\bm{X}}_s^{j,N}$ are all identically distributed.
For simplicity, assume now that $d_\theta=1$ (for multidimensional spaces the computation below apply component-wise), then
\begin{align*}
    \mathbb{E}\left[\norm{\int \nabla_{\theta}U(\bm{\theta}_s, x')\nu_s(x')\rmd x' - \nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{1,N})}^2\right] &= \mathrm{var}\left(\nabla_{\theta}U(\bm{\theta}_s, \tilde{\bm{X}}_s^{1,N})\right)\\
    &\leq 2L^2\mathbb{E}\left[\bm{\theta}_s^2+\norm{\tilde{\bm{X}}_s^{1,N}}^2\right]\\
    &\leq 2L^2 C_T,
\end{align*}
where we used~\Cref{Lipschitz assump} and the fact that the variance of Lipschitz functions is bounded. Combining this with~\eqref{eq:poc_var1} we obtain
\begin{align*}
    \mathbb{E}\left[\norm{g(\bm{\theta}_s, \nu_s) - g(\bm{\theta}_s, \tilde{\nu}_s^N)}\right] \leq N^{-1/2}\sqrt{2C_T}L.
\end{align*}
For the last term we apply It\^o isometry and Jensen's inequality to get
\begin{align*}
    \mathbb{E}\left[\vert\int_0^t \rmd \bm{B}_s\vert\right] &\leq \mathbb{E}^{1/2}\left[\vert\int_0^t \rmd \bm{B}_s\vert^2\right]=\sqrt{t}.
\end{align*}
Using the bounds above we have
\begin{align*}
  & \mathbb{E}\left[\sup_{s \in [0,t]} \left(\norm{\bm{\theta}_s - \bm{\theta}_s^{1,N}}+\norm{\bm{X}_s - \bm{X}_s^{j,N}}\right)\right] \\
  &\qquad\qquad \leq  \max\{2L_\theta, L_\theta+L_x\}\int_{0}^t \mathbb{E}\left[\sup_{u \in [0,s]} \left(\norm{\bm{\theta}_u - \bm{\theta}_u^{1,N}}+\norm{\bm{X}_u - \bm{X}_u^{j,N}}\right)\right]\rmd s\\
  &\qquad\qquad+\sqrt{2}N^{-1/2}(\sqrt{C_T}(L_\theta+L_x)+\sqrt{t}),
\end{align*}
we conclude the proof using Gr\"{o}nwall's lemma
\begin{align*}
    \mathbb{E}\left[\sup_{t \in [0,T]}\left(\norm{\bm{\theta}_t - \bm{\theta}_t^{1,N}}+\norm{\bm{X}_t - \bm{X}_t^{j,N}}\right)\right] & \leq  \frac{\sqrt{2}(\sqrt{C_T}(L_\theta+L_x)+\sqrt{T})e^{T\max\{2L_\theta, L_\theta+L_x\}}}{N^{1/2}}.
\end{align*}
\end{proof}

\end{document}
