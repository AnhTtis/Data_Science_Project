\section{Experiment}
\label{sec:experiment}

\subsection{Datasets and Evaluation Protocols}
Our experiments use models pre-trained on the ImageNet21k~\cite{imagenet} dataset without specified, and we follow prior works to train and evaluate the model on CIFAR100~\cite{cifar} and ImageNet-R~\cite{imagenet-r} benchmarks.

\noindent\textbf{CIFAR100} is an extensively used dataset in prior continual learning (CL) works, containing 100 classes, each class with 500 training and 100 test images of size 32$\times$32$\times$3. 

\noindent\textbf{ImageNet-R} is first introduced to CL by Wang et al.~\cite{dual_prompt}, including 200 subcategories of ImageNet~\cite{imagenet}, but its samples are in different styles, such as cartoon, graffiti, and origami. There are also some hard examples from ImageNet that standard models, \emph{e.g.}, ResNet~\cite{resnet}, fail to classify. The original dataset is split into the train set with 24000 samples and the test set with 6000 samples, and the number of training and testing samples varies between classes.

We follow prior works to split the dataset into 10 tasks, and all tasks have the same number of classes, \emph{i.e.}, 10 for CIFAR100 and 20 for ImageNet-R. We evaluate the model by the widely used incremental metrics: last incremental accuracy $A_{N}$ and average incremental accuracy $\bar{A}_{N} = \frac{1}{N} \sum_{i=1}^N A_i$, where $N$ is the total number of tasks (\emph{i.e.}, 10), and $A_i$ is formally defined as:
\begin{equation}
A_i = \frac{1}{\lvert \mathcal{D}_{1:i}^{test} \rvert}
\sum_{(\mathbf{x}, y) \in \mathcal{D}_{1:i}^{test}} 
\mathds{1} \left(\hat{y} = y\right), 
\end{equation}
where $\mathds{1}(\cdot)$ is the indicator function that maps the boolean value to $\{0, 1\}$, $\mathcal{D}_{1:i}^{test}$ is the test set of all seen tasks so far, $\hat{y}$ and $y$ are predicted and ground truth labels of input $\mathbf{x}$. \textit{We ran all experiments 3 times with different class orders and report the mean and standard deviation of these 3 runs}.

\input{table/benchmark_cifar100}

\subsection{Implementation and Training Details} 
To make fair comparisons, we consider state-of-the-art approaches~\cite{l2p, dual_prompt} based on pre-trained models like our LAE and using the PyTorch code released by Jaeho Lee\footnote{https://github.com/JH-LEE-KR} to conduct experiments. The joint fine-tuning (Joint-FT) and the naive sequential fine-tuning (Seq-FT) usually recognized as the upper and lower bounds of CIL are implemented in our codebase, referring to the code of Jaeho Lee. We also compare with recent work ESN~\cite{esn}, using its official PyTorch code. We chose three types of representative PET modules and two sizes per type for our baseline and LAE framework, where \textit{the size denotes the down-projection dimension of the Adapter, the rank of LoRA, or the length of the Prefix} described in Sec.~\ref{sec:preliminaries}. We assume that only a single PET module is attached to the pre-trained model in the previous discussion for convenience, in practice, multiple PET modules are inserted into the Attention blocks of Transformers or the convolution blocks of ConvNets in the shallow layers, following DualPrompt~\cite{dual_prompt}.

The training strategy of our baseline and LAE framework is the same as DualPrompt, \emph{i.e.}, training the model with Adam optimizer for 5 and 50 epochs, and constant learning rate 0.03 and 0.005 based on batch size 256, for CIFAR100 and ImageNet-R, respectively. The EMA algorithm's weight decay $\alpha$ defined in Eq.~(\ref{eq:ema}) is empirically set to 0.9999 in all experiments. The freezing epochs of PET modules are set to 3 and 30 for CIFAR100 and ImageNet-R, respectively.
The data augmentation is consistent with that used in model pre-training. We train Joint-FT and Seq-FT with the recommended fine-tuning strategy of ViT~\cite{vit}, but the number of training epochs is the same as ours. More details can be found in the supplementary materials.

\input{table/benchmark_imagenet-r}

\subsection{Benchmark Results}
\noindent\textbf{CIFAR100} benchmark results are present in Tab.~\ref{table:cifar100}. All approaches use the same ViT-B/16~\cite{vit} model pre-trained on the ImageNet21k~\cite{imagenet} dataset. The numerical suffix of the PET module denotes its size (\emph{i.e.}, down-projection dimension or length). L2P and DualPrompt are state-of-the-art approaches that adopt a pool to store Prompt or Prefix. However, the accuracy of their prompt selection gradually declines with the increase in the number of learning tasks and the prompts for different tasks appear homogeneous (see supplementary). Therefore, our baseline is very naive but achieves comparable performance to L2P and DualPrompt, and our LAE framework with all 6 PET modules consistently surpasses DualPrompt and ESN by about 1.5\% in last incremental accuracy $A_{10}$.  
Although PET modules have different performances in the baseline, they achieve better and same level performance in our LAE, mainly due to the calibration of adaptation speed. In particular, DualPrompt has 3-10x more learnable parameters than our LAE.

\input{figure/benchmark}

\noindent\textbf{ImageNet-R} benchmark is more difficult than CIFAR100, but it can better demonstrate the advantages of our LAE framework. From the results shown in Tab.~\ref{table:imagenet-r}, our baseline can only achieve comparable performance to DualPrompt when using Prefix. This is because the Adapter and LoRA adapt to a new task faster than Prefix, which is enlarged in the ImageNet-R dataset but successfully addressed by the adaption speed calibration of our LAE framework. Thus, we can see that our LAE achieves more than 3.5\% performance improvement over DualPrompt in terms of the last incremental accuracy $A_{10}$, which is also scaled up compared to the easier CIFAR100 dataset. We can also observe that the size of the PET modules has little impact on performance in our LAE framework, and our LAE is more robust to the class order, while our baseline has a relatively large variance between different class orders.

The real-world CL is an endless procedure, and the performance of each learning phase is equally important to the AI system. So, we also plot the task-by-task incremental accuracy in Fig.~\ref{fig:cifar_benchmark} and \ref{fig:imgr_benchmark}. We can observe that our LAE with all three types of PET modules performs better than L2P and DualPrompt at almost all learning phases. \textit{Our LAE outperforms others by a wider margin in the 20-task experiments presented in the supplementary material, highlighting its ability to handle long-term CL scenarios}.

\subsection{Ablation Study}
Our LAE consists of three main novel designs, \emph{i.e.}, learning, accumulation, and ensemble, so we ablate on them and report the results in Tab.~\ref{tab:ablation}. The first and last rows are our baseline and LAE framework, respectively. The performance drops the most when removing our learning with calibrated speed, demonstrating that it contributes the most to our LAE. Accumulation and Ensemble are also important to our LAE, without them the last incremental accuracy decreases by 2.15\%. The sixth row indicates our LAE inference with the Offline PET module only, whose $A_{10}$ is even better than the inference by our expert ensemble. As illustrated in Fig.~\ref{fig:ens_ada}, inference by expert ensemble performs better than inference with the Online or Offline PET module alone in the earlier learning phases. However, as the number of learned tasks increases, the advantage of expert ensemble over the Offline PET module gradually decreases, partly due to the performance of the old tasks dominating the overall performance. Nonetheless, inference with expert ensemble yields more robust performance in most cases.

We also conducted ablation experiments on the calibration made to Prefix. We can see from Tab.~\ref{tab:ablation_prefix} that both gradient compensation and learnable scaling parameters individually lead to significant improvements in performance. 
Moreover, when used together, the performance gain is approximately equal to the sum of the gains achieved by using each of them separately, indicating that their contributions to the performance are independent of each other.

\input{table/ablation_lae}

\input{figure/ablation_inference}

\subsection{Attach Position of PET Modules}
Our LAE inserts PET modules directly into the first 5 Transformer blocks, following the DualPrompt. As shown in Fig.~\ref{fig:ablation_position_and_number} (left), inserting the PET modules in the shallowest position produces better results than inserting them in deeper positions, which is consistent with the observation in DualPrompt. Additionally, Fig.~\ref{fig:ablation_position_and_number} (right) shows that inserting PET modules in the first 6 Transformer blocks achieves the best performance while inserting them in the first 5 Transformer blocks (\emph{i.e.}, the default setting of our LAE) also results in nearly the same performance.

\input{table/ablation_calibration}

\input{figure/ablation_position}

\subsection{Results on Transformer variant and ConvNet}
Prefix and Prompt are not flexible enough to be applied to ConvNets and Transformer variants, while our LAE is model architecture generalizable due to the ability to leverage various PET modules. We choose the Swin Transformer~\cite{swin} and ConvNeXt~\cite{convnext} to validate our LAE.

\smallskip
\noindent\textbf{Swin Transformer} is a representative window-based Vision Transformer, but L2P and DualPrompt cannot be directly applied to it because the inserted tokens may disrupt the proper division of the windows. Therefore, we only compare our LAE with our baseline when using Swin Transformer, and report the results in Tab.~\ref{table:swin}. We can see that our LAE achieves better performance with Swin-B than with ViT-B/16, which is mainly due to the superior performance of Swin-B. Moreover, our LAE significantly improves the performance of our baseline on all two datasets.

\smallskip
\noindent\textbf{ConvNeXt} is a modern ConvNet for the 2020s that outperforms Swin Transformer by incorporating several novel designs into the standard ResNet~\cite{resnet}. Similarly, we compare our LAE with the baseline in Tab.~\ref{table:convnext}. The Adapter's down and up projections are implemented using $1$$\times$$1$ convolution layers. Both our baseline and LAE achieve significantly better performance using ConvNeXt-B compared to using ViT-B/16 and Swin-B, highlighting LAE's strengths beyond being limited to Transformers. Our LAE consistently improves the performance of our baseline on both datasets.

\input{table/swin_vit}

\input{table/convnext}