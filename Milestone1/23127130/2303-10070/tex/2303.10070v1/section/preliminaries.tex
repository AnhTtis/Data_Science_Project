\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Continual Learning Formulation}
We focus on Continual Learning with incremental classes, \ie, Class-Incremental Learning (CIL),  
where a model sequentially learns tasks $\mathcal{T}$$:=$$\{\mathcal{T}_1, \mathcal{T}_2, \cdots, \mathcal{T}_n\}$, the $i^{th}$ task $\mathcal{T}_i$ has $|\mathcal{T}_i|$ categories, the train set of $\mathcal{T}_i$ is denoted as $\mathcal{D}_{i}$, and the categories are non-overlapping between tasks. The model $f(\cdot;\boldsymbol{\theta}, \boldsymbol{\phi})$ predicts the category label $y \in \mathcal{Y}$ for a given sample $\mathbf{x} \in \mathcal{X}$ of the learned tasks, where $\mathcal{Y}$ is all seen categories, $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ are parameters of feature extractor and classification head, respectively. In this paper, the feature extractor is a pre-trained model parameterized by $\boldsymbol{\theta}_{pre}$ attached with the Parameter-Efficient Tuning module parameterized by $\boldsymbol{\theta}_{pet}$, and $\boldsymbol{\phi}$$=$$\operatorname{concatenate}(\boldsymbol{\phi}_{old}, \boldsymbol{\phi}_{new})$, where $\boldsymbol{\phi}_{old}$ and $\boldsymbol{\phi}_{new}$ are classifiers of all learned tasks $\mathcal{T}_{1:i}$ and the current learning task $\mathcal{T}_i$. Since $\boldsymbol{\theta}_{pre}$ and $\boldsymbol{\phi}_{old}$ are kept fixed during learning a new task, we may omit them for concise in the rest of this paper. 

\subsection{Parameter-Efficient Tuning Revisit}
Parameter-Efficient Tuning (PET) keeps the pre-trained model frozen and tunes a small number of additional learnable parameters, called PET module in the paper. Below we revisit several representative PET modules, in which $g$ is the module that PET attached to, $\mathbf{e}$ and $\mathbf{h}$ are input and output of the orginal $g$ and $\mathbf{h}'$ is output of $g$ attached with PET.

\smallskip
\noindent\textbf{Adapter}~\cite{adapter_tuning} is a small module that can be inserted to any layer (\ie, $g$) of the pre-trained model. As shown in Fig.~\ref{fig:framework} (b), the adapter is generally a residual block composed of a down-projection with parameters $\mathbf{W}_{down}$, a nonlinear activation function $\sigma(\cdot)$, and an up-projection with parameters $\mathbf{W}_{up}$. The two projections can be convolution~\cite{res_adapter} for CNN or linear~\cite{adapter_tuning} layers  for Transformer architectures, respectively. We formulate the adapter as follows:
\begin{equation}
\label{eq:seq_adapter}
    \mathbf{h}' = \mathbf{h} + \sigma(\mathbf{h} \ast \mathbf{W}_{down}) \ast \mathbf{W}_{up},
\end{equation}
where the $\ast$ is matrix multiplication or convolution operation, $\sigma$ is the activation function.
Alternatively, the adapter can also be parallel with $g$ like a residual branch~\cite{side_tuning, uniview_pet}:
\begin{equation}
\label{eq:parallel_adapter}
    \mathbf{h}' = \mathbf{h} + \sigma(\textbf{e} \ast \mathbf{W}_{down}) \ast \mathbf{W}_{up}.
\end{equation}
\smallskip
\noindent\textbf{LoRA}~\cite{lora}
assumes the change of parameters is in a low-rank space when tuning the pre-trained model on a downstream task. For a linear layer with weight $\mathbf{W} \in \mathbb{R}^{d \times d'}$, the weight updates $\Delta \mathbf{W}$ can be decomposed into the multiplication of two small matrices:
\begin{equation}
\Delta \mathbf{W} = \mathbf{W}_{down} \mathbf{W}_{up},
\end{equation}
where $\mathbf{W}_{down} \in \mathbb{R}^{d \times r}$ and $\mathbf{W}_{up} \in \mathbb{R}^{r \times d'}$. For the convolution layer, the updates can be reshaped into the kernel shape.
Finally, LoRA modifies the forward pass of the adapted layer into the following form:
\begin{equation}
\label{eq:lora}
\textbf{h}' = \textbf{h} + \textbf{e} \ast (\mathbf{W}_{down} \mathbf{W}_{up}),
\end{equation}
where $\ast$ is matrix multiplication or convolution operation,
the bias and reshape operation are omitted for conciseness. Since LoRA adapts the weight of $g$, the weight updates can be merged into $g$ to reduce the inference latency.

\smallskip
\noindent\textbf{Prefix}~\cite{prefix_tuning} and \textbf{Prompt}~\cite{prompt_tuning} are learnable tokens prepended to the input of a transformer block or keys and values of the attention module. Given two sets of prefix tokens $\mathbf{P}_k, \mathbf{P}_v \in \mathbb{R}^{l \times d}$ the attention module is modified as:
\begin{equation}
\begin{aligned}
\mathbf{h}' = \operatorname{Attn}\left(
        \mathbf{x} \mathbf{W}_q, 
        [\mathbf{P}_k, \mathbf{e} \mathbf{W}_k],  
        [\mathbf{P}_v, \mathbf{e} \mathbf{W}_v]
        \right),
\end{aligned}
\end{equation}
where $[\cdot, \cdot]$ is  concatenate, and $\operatorname{Attn}$ is defined as:
$$
\operatorname{Attn}\left(\mathbf{Q}, \mathbf{K}, \mathbf{V} \right) := \operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d}}\right) \mathbf{V},
$$
and the multi-head mechanism is omitted for conciseness.

In this paper, we follow \cite{uniview_pet} to add a learnable scale parameter $s$ to the parallel Adapter (Eq.~\ref{eq:parallel_adapter}) and LoRA (Eq.~\ref{eq:lora}) respectively to obtain:
\begin{equation}
\label{eq:sp_adapter}
\mathbf{h}' = \mathbf{h} + s \cdot \sigma(\mathbf{e} \ast \mathbf{W}_{down}) \ast \mathbf{W}_{up},
\end{equation}
%--------
\begin{equation}
\label{eq:sp_lora}
\mathbf{h}' = \mathbf{h} + s \cdot \mathbf{e} \ast (\mathbf{W}_{down} \mathbf{W}_{up}).
\end{equation}
The Eqs. (\ref{eq:sp_adapter}) and (\ref{eq:sp_lora}) are general forms of Eqs. (\ref{eq:parallel_adapter}) and (\ref{eq:lora}), and they degenerate to Eq. (\ref{eq:parallel_adapter}) and (\ref{eq:lora}) when $s$ is the constant $1$. We use these PETs in Eqs.~(\ref{eq:sp_adapter}) and~(\ref{eq:sp_lora}) rather than Eqs.~(\ref{eq:seq_adapter}) and~(\ref{eq:lora}) in our experiments (Sec.~\ref{sec:experiment}).

In addition to the three PET modules described above, there are many others, such as AdaptBias~\cite{adaptbias}, Compacter~\cite{compacter}, and AdapterFormer~\cite{adapter_former}, and there will be new and superior PET methods in future as well. All of them can be applied to our CL framework (see Sec.~\ref{sec:framework}), as long as they are compatible with the pre-trained model.