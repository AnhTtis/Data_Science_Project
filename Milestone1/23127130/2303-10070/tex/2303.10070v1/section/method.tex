\section{Methodology}
\subsection{Naive Baseline}
We construct a baseline by leveraging pre-trained models and PET techniques, following the naive sequential fine-tuning (Seq-FT) that was usually considered as the lower bound of CIL. Intuitively, our baseline creates a PET module and attaches it to the pre-trained model, then sequentially learns tasks in the same way as Seq-FT but keeping the pre-trained model frozen. We chose the local cross-entropy loss (CE) rather than the global CE loss as the learning objective for Seq-FT and our baseline because local CE empirically performs better than global CE when using a large pre-trained model~\cite{l2p,dual_prompt,esn}. The local CE is the standard CE computed on categories of the current task:
\begin{equation}
\label{eq:loss_lce}
\mathcal{L} = \frac{1}{|\mathcal{D}_i|} \sum_{(\mathbf{x}, y) \in \mathcal{D}_i} \mathcal{L}_{ce}(\operatorname{mask}(f(\mathbf{x};\boldsymbol{\theta}, \boldsymbol{\phi})), y),
\end{equation}
where $y$ is the ground truth label of the input $\mathbf{x}$ in the current training set $\mathcal{D}_i$, $\operatorname{mask}(\cdot)$ is a function that filters out the logits of old categories. The Eq.~(\ref{eq:loss_lce}) falls back to global CE when $\operatorname{mask}(\cdot)$ is removed. Although our baseline is very naive, the performance is comparable to the state-of-the-art DualPrompt when using the same Prefix module.

\subsection{Proposed Framework} 
\label{sec:framework}
LAE builds upon our network architecture generalizable baseline and additionally introduces three novel designs, yielding a robust framework that can readily reshape any PET methods into a competitive Continual Learning approach. In the following, we will delve into these three key aspects of LAE: learning, accumulation, and ensemble.

\smallskip
\noindent\textbf{Learning with calibrated speed}. 
We observed that \textit{PET modules vary in their speed for acquiring new knowledge, leading to disparities in performance}.
In theory, adapting the PET module to a new task too quickly can lead to overfitting and result in worse catastrophic forgetting, whereas slower adaptation can maintain the model's stability but limit its plasticity. We argue that aligning the adaptation speeds of different PET modules is crucial for transforming them into an efficient and robust CL approach in a unified way. To address this, we propose calibrating PET modules to align their adaptation speeds.

Moreover, as the same PET module $\boldsymbol{\theta}_{pet}$ is shared by new and old tasks, the changes made to $\boldsymbol{\theta}_{pet}$ for the new task can cause forgetting for the old tasks. Therefore, slowing down the change in $\boldsymbol{\theta}_{pet}$, \eg, by reducing the learning rate (see supplementary), can help mitigate catastrophic forgetting.
However, finding a learning rate that works well for all situations is not easy, and simply adjusting the number of training epochs cannot properly solve this problem.
To tackle this challenge, we propose a flexible \emph{classifier-first learning strategy}
to calibrate the $\boldsymbol{\theta}_{pet}$'s adaptation speed relative to $\boldsymbol{\phi}_{new}$. 
Specifically, at the beginning of training, we only learn $\boldsymbol{\phi}_{new}$ with $\boldsymbol{\theta}_{pet}$ frozen; then after $\boldsymbol{\phi}_{new}$ has sufficiently learned and the loss has significantly decreased, we jointly learn both $\boldsymbol{\phi}_{new}$ and $\boldsymbol{\theta}_{pet}$.

According to the study by He et al.~\cite{uniview_pet}, the Prefix can be equivalently transformed into a similar form to the Adapter:
\begin{equation}
\label{eq:prefix_uni_form}
\mathbf{h}’ \leftarrow(1-\lambda(\mathbf{e})) 
    \mathbf{h} + \lambda(\mathbf{e}) \sigma\left(
      \mathbf{e} \mathbf{W}_{1}
    \right) \mathbf{W}_{2},
\end{equation}
in which $\mathbf{W}_{1}$$=$$\mathbf{W}_q \mathbf{P}_k^{\top}$, $\mathbf{W}_{2}$$=$$\mathbf{P}_v$, $\sigma$$=$$\text{softmax}$ and
\begin{equation}
\label{eq:lambda_x} 
    \lambda(\mathbf{e}) = \frac{\sum_i \exp \left(\mathbf{e} \mathbf{W}_q \mathbf{P}_k^{\top}\right)_i}{\sum_i \exp \left(\mathbf{e} \mathbf{W}_q \mathbf{P}_k^{\top}\right)_i + \sum_j \exp \left(\mathbf{e} \mathbf{W}_q \mathbf{W}_k^{\top} \mathbf{C}^{\top}\right)_j}.
\end{equation}
As $\mathbf{P}_k$ usually contains much fewer tokens than input $\mathbf{C}$, $\lambda(\mathbf{e})$ is often a small positive number close to 0, which impacts the gradient of Prefix tokens $\mathbf{P}_v$:
\begin{equation}
\label{eq:prefix_grad} 
    \frac{\partial{\mathcal{L}}}{\partial{\mathbf{P}_v}} =
    (\frac{\partial{\mathbf{h}’}}{\partial{\mathbf{P}_v}})^{\top}
    \frac{\partial{\mathcal{L}}}{\partial{\mathbf{h}’}}
    = \lambda(\mathbf{e}) (\sigma\left(\mathbf{e} \mathbf{W}_{1} \right))^{\top} \frac{\partial{\mathcal{L}}}{\partial{\mathbf{h}’}}.
\end{equation}

Therefore, the gradient of $\mathbf{P}_v$ is significantly smaller than $\mathbf{W}_{up}$ of the corresponding Adapter parameterized by $\mathbf{W}_{down}$$=$$\mathbf{W}_{1}$ and $\mathbf{W}_{up}$$=$$\mathbf{W}_{2}$, 
and we can arrive at a similar conclusion regarding $\mathbf{P}_k$ and $\mathbf{W}_{down}$. Then, we can easily observe that Prefix adapts to the new task much slower than Adapter. This is partly why the prompts for different tasks in prior approaches~\cite{l2p, dual_prompt} are prone to be homogeneous (see supplementary). Here we align Prefix with Adapter by compensating its gradient by $\frac{1}{\lambda(\mathbf{e})}$ and adding two learnable scaling parameters $s^k$ and $s^v$, calibrating the Prefix described by Eq.~(\ref{eq:prefix_uni_form}) into the following form: 
\begin{equation}
\label{eq:prefix_uni_form_improved}
\mathbf{h}’ \leftarrow(1-\lambda(\mathbf{e})) 
    \mathbf{h} + \sigma\left(
      s^k \cdot \mathbf{e} \mathbf{W}_{1}
    \right) (s^v \cdot \mathbf{W}_{2}).
\end{equation}
The adaptation speed of the calibrated Prefix is nearly equivalent to the Adapter depicted in Eq.~(\ref{eq:sp_adapter}) and elevates its performance to be on par with the Adapter. For other PET modules, we can also analyze them specifically and then calibrate their adaptation speeds to align with Adapter. 

By aligning the adaptation speed of PET modules and calibrating their adaptation speed relative to the classifiers, our framework achieves a better and more consistent stability-plasticity balance with various PET modules.

\smallskip
\noindent\textbf{Accumulation of multi-task knowledge}. 
The PET module $\boldsymbol{\theta}_{pet}$ is designed to continuously adapt to new tasks, making the model more proficient in dealing with novel tasks. However, this adaptation process can result in the model gradually forgetting how to handle older tasks. To address this issue, we propose to create an additional expert for older tasks to complement the expert for newer tasks, drawing inspiration from the Complementary Learning System~\cite{cls_theory, cls_theory_updated} of the human brain, which involves the hippocampus rapidly learning new knowledge and the neocortex integrating learned knowledge in an offline manner over time. We achieve this by duplicating the online PET module $\boldsymbol{\theta}_{pet}^{on}$ (\emph{i.e.}, the $\boldsymbol{\theta}_{pet}$ in the baseline) attached to the model as the offline PET module $\boldsymbol{\theta}_{pet}^{off}$ after the model has learned the first task. The $\boldsymbol{\theta}_{pet}^{off}$ slowly accumulates the learned knowledge when the model learns a new task by an accumulation function, and we empirically find the simple Exponential Moving Average (EMA) algorithm works well for our LAE:
\begin{equation}
\label{eq:ema}
\boldsymbol{\theta}_{pet}^{off} \leftarrow \alpha \cdot \boldsymbol{\theta}_{pet}^{off} + (1 - \alpha) \cdot \boldsymbol{\theta}_{pet}^{on},
\end{equation}
where $\alpha \in (0, 1)$ is a large (\emph{i.e.}, close to 1) weight decay. 

This way, the neocortex-like offline PET module gradually integrates the learned knowledge in a slow offline manner, while the hippocampus-like online PET module continues to rapidly learn new knowledge. Then, we can obtain two experts for newer tasks and older tasks with $\boldsymbol{\theta}_{pet}^{on}$ and $\boldsymbol{\theta}_{pet}^{off}$, respectively. However, the task a sample belongs to is unknown during inference, we need to devise a method to effectively utilize both experts for inference.

\smallskip
\noindent\textbf{Ensemble of two expert models}.
Two expert models constructed with $\boldsymbol{\theta}_{pet}^{on}$ and $\boldsymbol{\theta}_{pet}^{off}$ are respectively proficient at handling newer and older tasks. Instead of inference only using the online or offline expert model, we integrate their outputs to obtain the prediction for an inference sample.

A classifier can be viewed as an energy model when we define the unnormalized negative log probability as the energy function~\cite{ebm_tutorial}. The optimization goal of the energy model is to minimize the energy of the model on the data distribution of its learning task. Previous research~\cite{eb_ood} has shown that the energy of an energy model trained on one data domain is generally very high on other data domains. Therefore, the Eq.~(\ref{eq:loss_lce}) actually continuously minimizes the energy of the $\boldsymbol{\phi}_{new}$  on the new task. Even if old data is not used during training, the energy of the old data on $\boldsymbol{\phi}_{new}$ will be very high, as demonstrated by the recent work ESN~\cite{esn}. 

Likewise, as the $\boldsymbol{\theta}_{pet}^{on}$ and $\boldsymbol{\theta}_{pet}^{off}$ respectively contain relatively more novel and historical knowledge, theoretically, the energy produced by $\boldsymbol{\theta}_{pet}^{on}$ for the sample of the newer tasks should be smaller than that produced by $\boldsymbol{\theta}_{pet}^{off}$, and the vice versa for the sample of older tasks. Therefore, choosing the prediction result with the lowest energy as the final prediction of an inference sample seems like a simple but effective solution. However, in practice, we find that normalizing the energy produced by $\boldsymbol{\theta}_{pet}^{on}$ and $\boldsymbol{\theta}_{pet}^{off}$ before ensemble yields more robust results. Therefore, we adopt the following ensemble algorithm instead:
\begin{equation}
\label{eq:ensemble_method}
f_{ens}(\mathbf{o}^{on}, \mathbf{o}^{off}) := \operatorname{max}\left(
  \operatorname{\sigma}\left(\mathbf{o}^{on}\right), 
  \operatorname{\sigma}\left(\mathbf{o}^{off}\right)
\right),
\end{equation}
where $\operatorname{\sigma}$ is the softmax function, $\mathbf{o}^{on}$ and $\mathbf{o}^{off}$ are outputs of the online and offline expert models (\emph{i.e.}, $f(\cdot;\boldsymbol{\theta}_{pet}^{on}, \boldsymbol{\phi})$ and $f(\cdot;\boldsymbol{\theta}_{pet}^{off}, \boldsymbol{\phi})$) for an inference sample, respectively.

\smallskip
As illustrated in Fig.~\ref{fig:framework}, in our LAE framework, the model learns a new task with $\boldsymbol{\theta}_{pet}^{on}$ and accumulates the learned knowledge to $\boldsymbol{\theta}_{pet}^{off}$, the two experts favored by newer and older tasks are ensembled to get the final prediction for an inference sample. Our LAE can be applied to the pre-trained model in any network architecture as long as the PET modules are compatible with the model.