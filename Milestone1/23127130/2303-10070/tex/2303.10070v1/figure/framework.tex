\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{file/framework.pdf}}
\caption{\textbf{Illustration of our LAE framework.} The left (a) is the training process, the right (c) is the inference flow, and the middle (b) lists some representative Parameter-Efficient Tuning (PET) modules attached to a transformer attention block, modules connected with dashed lines are optional and we use one of the PET modules in the experiments. There is no residual connection in Parallel Adapter. ``Online'' is a PET module to learn knowledge from the new task and ``Offline'' is a PET module to accumulate knowledge. The pre-trained model is omitted in the inference flow for concise.
} 
\label{fig:framework}
\end{center}
\vskip -0.4in
\end{figure*}