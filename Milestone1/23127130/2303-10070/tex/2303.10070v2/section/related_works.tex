\section{Related Works}
\label{sec:rws}

\smallskip
\noindent\textbf{Parameter-Efficient Tuning.} As an efficient alternative to full fine-tuning, Adapter-Tuning~\cite{adapter_tuning} was first proposed to transfer large pre-trained Language models to downstream tasks. Inspired by textual prompting, Prompt-Tuning~\cite{prompt_tuning} and Prefix-Tuning~\cite{prefix_tuning} insert learnable tokens to adapt to the new task. More advanced methods~\cite{bitfit,adaptbias,lora,compacter} achieved comparable or superior performance to full fine-tuning, and keep the same inference cost by merging the additional learnable parameters to the original pre-trained model. Following the step of successful Vision Transformers~\cite{vit,swin}, VPT~\cite{vpt} and AdapterFormer~\cite{adapter_former} have been proposed to solve visual transfer learning problems. Prompt-Tuning and Prefix-Tuning depend on the transformer architecture because they modify input or hidden tokens. Adapter and its variants are network architecture generalizable since they are new modules that can be implemented in forms compatible with pre-trained models. All types of Parameter-Efficient Tuning modules can be integrated into our LAE framework as long as they are suitable for the pre-trained model, but we focus on the representative Adapter~\cite{adapter_tuning}, LoRA~\cite{lora}, and Prefix~\cite{prefix_tuning} in this paper.

\noindent\textbf{Continual Learning.} The central problem of Continual Learning (CL) is fighting catastrophic forgetting~\cite{catastrophic_forgetting}. Memory-based approaches~\cite{icarl,ucir,podnet,aanets,alssum2023smile} save a subset of learned samples into a memory buffer and replay them when learning a new task. Memory-Free approaches do not rely on old samples that may raise privacy concerns, they dynamically expand the network or isolate parameters for different tasks~\cite{pnn,den,l2g,cpg}, regularize the network parameters that are important to learned tasks~\cite{lwf,ewc,pi,mas}, and replay generative or synthetic data~\cite{dgr,deepinversion,abd,r-dfcil}. Conventional CL approaches learn tasks from scratch using a randomly initialized model, while pre-trained models have received little attention from CL researchers until recently. Two pioneering works~\cite{l2p,dual_prompt} introduce Prompt-Tuning to CL and achieve much higher incremental performance than previous approaches, demonstrating the advantage of using pre-trained models in CL. Side-Tuning~\cite{side_tuning} adopts a technique similar to Adapter-Tuning but requires the task identity of the inference sample. In this paper, we propose a unified framework for Memory-Free CL that can incorporate various types of PET modules.
Particularly, we focus on practical Class-Incremental Learning with the potential to extend our LAE to other CL scenarios in future work.