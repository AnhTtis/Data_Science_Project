In the supplementary materials, we further validate the proposed LAE framework by providing the following:
\begin{itemize}
  \item Section~\ref{sec:exp_details}: Additional Experimental Details.
  \item Section~\ref{sec:exp_results}: Additional Experimental Results.
  \item Section~\ref{sec:prompt_pool}: Investigation on Prompt Learning and Selection from Pool in Prompt-Pool-based Approaches.
\end{itemize}

\section{Additional Experimental Details}
\label{sec:exp_details}
\noindent\textbf{Data Augmentation}. We adopt a very simple data augmentation strategy for training, following L2P~\cite{l2p} and DualPrompt~\cite{dual_prompt}.
\emph{1)} Images are randomly resized to $224\times224$ using the bilinear interpolation algorithm. \emph{2)} Images are normalized by min-max (for ViT~\cite{vit}) or standard deviation (for Swin Transformer~\cite{swin} and ConvNeXt~\cite{convnext}) normalization. \emph{3)} Images are randomly flipped from horizontal. During inference, images are resized to $256\times256$ and cropped to $224\times224$ from central. All other approaches take the same data augmentation strategy as ours for fair comparisons.
The PyTorch-like code is present in Algorithm~\ref{alg:data_aug}.

\noindent\textbf{Hyper-Parameter}. Our LAE introduced  two additional hyper-parameters, \ie, the weight decay $\alpha$ of the Exponential Moving Average (EMA) algorithm and freezing epochs of the online Parameter-Efficient Tuning (PET) module. We did not intentionally search for these parameters and set $\alpha$ to a value very close to 1, such as the default value $0.9999$ we used. The number of freezing epochs can be determined by the change in loss after freezing the online PET module and is typically set to the value where the loss no longer decreases. We set this value to 3 for CIFAR100 and scaled it proportionally for ImageNet-R, on all of which our LAE achieved superior performance than other competitors.

\input{algorithm/data_augmentation}

\input{algorithm/training}

\noindent\textbf{Training, Inference and Evaluation}. The training and inference of our LAE framework are very easy to implement, the PyTorch-like pseudocode is provided in Algorithms~\ref{alg:lae_train}.  It is important to note that our evaluation metric $A_{10}$ (Equation 15 in the paper) is slightly different from the following metric used by original L2P and DualPrompt:
\begin{equation}
  A_{10} = \frac{1}{10} \sum_{j=1}^{10}
  \frac{1}{\lvert \mathcal{D}_{j}^{test} \rvert}
  \sum_{(\mathbf{x}, y) \in \mathcal{D}_{j}^{test}}
  \mathds{1} \left(\hat{y} = y\right),
\end{equation}
where $\mathcal{D}_{j}^{test}$ is the test set of the $j^{th}$ task. We train and evaluate on three different class orders, while L2P, DualPrompt, and ESN~\cite{esn} only evaluate on one class order in their original papers. Additionally, ESN uses a different pre-trained checkpoint from L2P and DualPrompt, but we correct this issue when using its code. The above differences lead to slightly different experimental results reported in their original papers from the data reported by us.

\input{table/benchmark_cifar100_t20}

\input{table/benchmark_imagenet-r_t20}

\section{Additional Experimental Results}
\label{sec:exp_results}
\noindent\textbf{20-Task Benchmark Results}.
To further validate the efficacy of our LAE in longer-term Continual Learning scenarios, we split the CIFAR100~\cite{cifar} and ImageNet-R~\cite{imagenet} datasets into 20 tasks, each containing 5 (for CIFAR100) or 10 (for ImageNet-R) classes. We then conducted experiments and reported the mean and standard deviation of three runs in different class orders in Tables~\ref{table:cifar100_t20} and~\ref{table:imagenet-r_t20}.
Similar to the 10-task experiments in the paper, we plot the task-by-task evaluation results in Figure~\ref{fig:benchmark_cifar_t20} and~\ref{fig:benchmark_imgr_t20} for the 20-task experiments on CIFAR100 and ImageNet-R. From these tables and figures, we can observe a wider performance gap between our LAE and other competitors compared to the 10-task experiments, suggesting that our LAE is more effective at mitigating forgetting and achieving a better stability-plasticity balance in longer-term Continual Learning.

\input{figure/benchmark_cifar100_t20}

\input{table/benchmark_domainnet_t5}

\smallskip
\noindent\textbf{Comparison with CODA-Prompt}.
The contemporary CODA-Prompt~\cite{coda_prompt} approach demonstrates remarkable performance. Nevertheless, upon reviewing the authors' released code, we identified three potential sources of unfair comparison:
1) A distinct ImageNet-R train-test split in contrast to DualPrompt.
2) The model is pretrained on ImageNet-21k and subsequently fine-tuned on ImageNet-1K.
3) Varied training strategies, such as the number of epochs and learning rates.
Our initial experiments reveal that when utilizing DualPrompt's train-test split, CODA-Prompt consistently underperforms our LAE. To ensure a fair evaluation, we adopt CODA-Prompt's settings for our experiments and extend our assessment to the DomainNet~\cite{domainnet} dataset. All results are showcased in Table~\ref{table:coda_comparasion}, where we present average forgetting rates instead of average incremental accuracy.

\input{table/ablation_alpha}

\smallskip
\noindent\textbf{Sensitive Analysis on EMA's Weight Decay}.
Weight decay $\alpha$ plays an important role in the knowledge accumulation of the offline PET module. A small value can lead to the integration of too much unstable new knowledge during the learning process, while a large value can result in the offline PET module being unable to effectively absorb new knowledge. In all of our experiments in the paper, we set the weight decay of EMA to 0.9999, which is the default value in the timm\footnote{https://github.com/huggingface/pytorch-image-models} library. Our experimental results in Table~\ref{table:ablation_alpha} demonstrate that this value yields the best performance.

\smallskip
\noindent\textbf{Memory and computation complexity}.
Our LAE requires two forward passes (one with $\boldsymbol{\theta}{pet}^{off}$ and the other with $\boldsymbol{\theta}{pet}^{on}$) per inference sample, yielding computational costs on par with L2P, DualPrompt, and the contemporary approach CODA-Prompt. Additionally, due to the constant number of parameters maintained across all tasks, LAE introduces fewer new parameters, as illustrated in Table~\ref{tab:param_com}.

\input{table/params_comparasion}

\input{table/eval_with_10prompts}

\input{table/eval_per_task_with_10prompts}

\section{Prompt Learning and Selection from Pool}
\label{sec:prompt_pool}
L2P~\cite{l2p} and DualPrompt~\cite{dual_prompt} are two representative approaches that leverage prompt tuning~\cite{prompt_tuning} to address the problem of Continual Learning. L2P first proposes to use a pool to store prompts shared across tasks, where a set of prompts that match the sample are selected from the pool to predict the sample's label. In contrast, DualPrompt directly learns a set of task-specific E-Prompts for each task and stores them in the pool. During inference, the best-matched prompts (\ie, the prompts learned for the task that the sample belongs to) are selected for the given sample.

The performance of these approaches is influenced by two key factors. \emph{1)} The ability to learn optimal prompts for each task is crucial for achieving better plasticity, \ie, the ability to learn new knowledge. Better performance can be achieved only by sufficiently learning new knowledge while retaining as much previous knowledge as possible. \emph{2)} the ability to accurately select the best-matched prompts for the inference sample is more critical. Because even if optimal prompts are learned for each task, inference using the wrong prompts can still result in poor prediction results. Following, we take DualPrompt as an example to investigate these two abilities of prompt-pool-based approaches.

To begin with, we assume that DualPrompt can learn the optimal E-prompts for each task. We then evaluate whether it can accurately select the right E-prompts during inference. As shown in Figure~\ref{fig:e-prompt_acc}, we observe that the E-prompts selected by DualPrompt are completely accurate after learning the first task. However, as the number of learned tasks increases, the accuracy of the prompt selection gradually decreases. By the time the 10th task is learned, the selection accuracy drops to below 50\%. Therefore, we conclude that if DualPrompt cannot address this issue, it is difficult to apply it to longer-term Continual Learning scenarios.

\input{figure/dp_selection_acc}

In addition, according to Figure 3 in our paper, we observe that DualPrompt performs worse than our LAE when learning the first task, regardless of whether our LAE uses Adapter~\cite{adapter_tuning} with fewer parameters, the LoRA~\cite{lora} with the equivalent number of parameters, or Prefix~\cite{prefix_tuning} (\ie, DualPrompt's E-Prompt) with slightly more parameters than DualPrompt. This indicates that Prompt/Prefix may not be as effective as Adapter and LoRA in learning new knowledge on these two datasets, as well as DualPrompt could not learn the optimal E-Prompts for the first task because they did not calibrate the Prefix like our LAE. This suggests that it is necessary to explore different Parameter-Efficient Tuning (PET) methods and calibrate PET modules.

Moreover, our naive baseline only uses one set of Prefixes, while DualPrompt learns a set of Prefixes for each task, totaling 10 sets, yet they achieve similar performance. We evaluate all 10 tasks using the 10 sets of E-Prompts learned by DualPrompt separately, and the results in Table~\ref{table:dp_metrics} show that the differences in the last and average incremental accuracy using the 2nd-10th sets of E-Prompts are very small. Table~\ref{table:dp_10tasks} presents the prediction results of each task using each set of E-Prompts, for most tasks, the prediction results using the 2nd-10th sets of E-Prompts are very close. These analyses reveal that from the learning of the second task, task-specific E-Prompts tend to become homogeneous. According to our analysis in the paper, an important reason for this is that the adaptation speed of the Prefix is much slower than classifiers and other PET modules (\ie, Adapter~\cite{adapter_tuning} and LoRA~\cite{lora}).