\section{Conclusion}
This paper thoroughly studied the novel Continual Learning (CL) paradigm that starts with a pre-trained model and continuously adapts the model to arriving tasks utilizing general Parameter-Efficient Tuning (PET) methods. We constructed a naive baseline that achieved performance comparable to the prior state-of-the-art approaches. We proposed the Learning-Accumulation-Ensemble (LAE) framework by introducing three novel designs to the baseline. Our LAE can convert any PET method into an efficient CL approach without accessing any old data. We conducted extensive experiments to validate the effectiveness of our LAE, and the results demonstrated that our LAE significantly outperforms the previous state-of-the-art approaches.

\noindent\textbf{Limitations}. There are still some limitations that need to be improved in the future, such as how to accumulate knowledge more efficiently and better ensemble expert models. Moreover, due to the lack of large-scale datasets that do not overlap with the pre-training dataset, our LAE has not been verified in CL scenarios with a larger number of tasks.

\noindent Overall, this paper provides a new solution for Memory-Free CL and offers some theoretical and experimental references for future research on this novel CL paradigm.