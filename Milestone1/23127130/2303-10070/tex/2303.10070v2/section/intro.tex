\vspace{-15pt}
\section{Introduction}
\label{sec.introduction}
Continual Learning (CL) of new knowledge is an essential ability for AI models in the constantly changing world. However, neural networks often suffer from catastrophic forgetting~\cite{catastrophic_forgetting,cfrp}, in which previously learned knowledge is forgotten when the model incorporates novel information. Although many works have been devoted to reducing forgetting, such as dynamic networks~\cite{pnn,den,l2g,cpg}, regularization~\cite{lwf,ewc,pi,mas}, and memory replay~\cite{icarl,ucir,podnet,aanets,alssum2023smile}, their performance still falls short of practical requirements.

\input{figure/overview}

Recently, pre-training and downstream adaptation techniques have opened up new opportunities and challenges for CL. Basically, these techniques ~\cite{bert, gpt, moco, mae, ufo} pre-train a deep model on large-scale data and then adapt the pre-trained model to novel tasks.
We observe that downstream adaptation and CL are important for each other.
On the one hand, in realistic AI systems, pre-trained models sometimes needs to be adapted to multiple downstream tasks sequentially, yielding the need of CL. On the other hand, recent efforts~\cite{l2p, dual_prompt,esn} show that the ``pre-training $\rightarrow$ downstream adaptation'' techniques can boost CL performance.

Specifically, L2P~\cite{l2p}, DualPrompt~\cite{ dual_prompt}, and ESN~\cite{esn} all use a popular adaptation technique named Parameter-Efficient-Tuning (PET).
Generally, PET adapts pre-trained models to downstream tasks with much fewer learnable parameters, as well as fewer resources.
Though these approaches have advanced the state-of-the-art in CL, they still have some limitations. 1) They are all constrained to a specific PET method, \emph{i.e.}, prompt tuning,
limiting their flexibility, considering that prompt can only cooperate with transformers and does not accommodate other network architectures. 2) Most of them rely on selecting task-specific parameters (the prompt tokens, in particular) for each individual task. The selection tends to be noisy with increasing task numbers and the task-specific prompts appear homogeneous, according to our investigation in the supplementary.

To circumvent these issues, this paper proposes Learning-Accumulation-Ensemble (LAE), a unified CL framework resort to the general Parameter-Efficient Tuning (PET). LAE is not restricted to Prompt, but can also utilize various other PET modules as shown in Fig.~\ref{fig:framework} (b).
Given a PET method, our LAE directly reshapes it for CL with three steps, \emph{i.e.}, learning, accumulation, and ensemble.

$\bullet$ \textbf{1) Learning with calibrated speed.}
The pre-trained model adapts to the new task by tuning an online PET module. To accommodate various PET methods, a key challenge is that different PET modules have different \textit{adaptation speeds} (for novel tasks), as well as different \textit{forgetting speeds} (for historical tasks). In response, we design an adaptation calibration strategy, based on the gradient analysis for different PET modules. We empirically show that this calibration strategy aligns different PET against each other and is critical for LAE to be a unified framework.

$\bullet$ \textbf{2) Accumulation of multi-task knowledge.} After adapting the pre-trained model to a new task,
the parameters in the online PET module are prone to the current novel task and may not fit historical tasks. Instead of memorizing multiple sets of PET modules and selecting some subsets (as in L2P and DualPrompt) for individual tasks, LAE accumulates all the knowledge of already-seen tasks into a single offline PET module through momentum update. This simple accumulation avoids noisy selection and is competent for \textit{alleviating catastrophic forgetting, especially when the amount of learned tasks is large} (Figs.~\ref{fig:benchmarks} and~\ref{fig:ens_ada} in Sec.~\ref{sec:experiment}).

$\bullet$ \textbf{3) Ensemble of two expert models.}
The online and offline PET modules respectively contain more novel and historical knowledge, therefore, two expert models constructed with them are correspondingly better at handling newer and older tasks. Instead of inference only using the online or offline expert model, we integrate the outputs of two expert models by an energy indicator (detailed in Sec.~\ref{sec:framework}) to obtain the prediction for an inference sample from any learned task. This expert ensemble strategy helps our framework to achieve a more robust performance compared to inference using one of the expert models alone.

\smallskip
\noindent The contributions of this paper are summarized as follows:
\begin{itemize}
  \item We thoroughly investigate the novel Continual Learning paradigm that constantly adapts a pre-trained model to novel tasks using general Parameter-Efficient Tuning (PET) methods, and propose a unified Learning-Accumulation-Ensemble (LAE) framework.
  \vskip -0.5in
  \item Our LAE framework reshapes a given PET method into a competitive Memory-Free Continual Learning approach with three novel designs: Learning with calibrated speed, Accumulation of multi-task knowledge, and Ensemble of two expert models constructed with online and offline PET modules.
  \vskip -0.5in
  \item We conduct extensive experiments on CIFAR100 and ImageNet-R benchmarks, on all of which, our LAE consistently achieves superior incremental performance than previous state-of-the-art approaches.
\end{itemize}