\begin{algorithm*}[t]
  \caption{{Training and Inference Code (PyTorch-like)}}
  \label{alg:lae_train}
  \definecolor{codeblue}{rgb}{0.25,0.5,0.25}
  \lstset{
    backgroundcolor=\color{white},
    basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
    columns=fullflexible,
    breaklines=true,
    captionpos=b,
    commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
    keywordstyle=\fontsize{7.2pt}{7.2pt},
  }

  \begin{lstlisting}[language=python]
# model: the pre-trained model; pet_on: online PET module; pet_off: offline PET module;

def train(model, pet_on, pet_off, dataloader, optimizer, task_id, alpha):
    model = attach(model, pet_on)
    for e in range(MAX_EPOCHS):
        if e == 0 and not_the_first_task(task_id):
            freeze(pet_on)
        elif e == NUM_FREEZING_EPOCHS:
            unfreeze(pet_on)
        
        for input, target in dataloader:
            pred = mask(model(input), task_id) # Eq. (8) in the paper
            loss = cross_entropy(pred, target) # Eq. (8) in the paper
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            ema_update(pet_off, pet_on, alpha) # Eq. (13) in the paper

def inference(model, pet_on, pet_off, input):
    pred_on, pred_off = attach(model, pet_on)(input), attach(model, pet_off)(input) 
    pred = max(softmax(pred_on, dim=-1), softmax(pref_off, dim=-1)) # Eq. (14) in the paper
    return argmax(pred)
\end{lstlisting}
\end{algorithm*}