\begin{table}
  \renewcommand\arraystretch{1.0}
  \caption{20-Task Benchmark Results on CIFAR100. The PET modules are inserted into the first 5 transformer blocks of the standard ViT-B/16 pre-trained on the ImageNet21k dataset. The ``5, 10, 20'' indicate the size of PET modules.}
  \vskip -0.25in
  \label{table:cifar100_t20}
  \begin{center}
    \resizebox{\columnwidth}{!}{
      \begin{tabular}{lcccr}
        \toprule
        Approach                      & PET Module & $A_{20}$ (↑)            & $\bar{A}_{20}$ (↑)      \\
        \midrule
        L2P~\cite{l2p}                & Prompt     & 80.10$\pm$0.72          & 85.29$\pm$0.50          \\
        DualPrompt~\cite{dual_prompt} & Prefix20   & 82.02$\pm$0.32          & 89.50$\pm$0.11          \\
        ESN~\cite{esn}                & Prompt     & 80.56$\pm$0.94          & 90.47$\pm$1.19          \\
        \midrule
        \multirow{6}{*}{LAE (Ours)}   & Adapter5   & 83.89$\pm$0.60          & \textbf{92.35}$\pm$0.55 \\
                                      & Adapter10  & 83.81$\pm$0.35          & 92.32$\pm$0.57          \\
                                      & LoRA5      & 83.92$\pm$0.36          & 92.15$\pm$0.47          \\
                                      & LoRA10     & 83.35$\pm$0.20          & 91.71$\pm$0.88          \\
                                      & Prefix10   & 83.82$\pm$0.18          & 92.07$\pm$0.72          \\
                                      & Prefix20   & \textbf{83.93}$\pm$0.28 & 92.21$\pm$0.53          \\
        \bottomrule
      \end{tabular}
    }
  \end{center}
  \vskip -0.2in
\end{table}