\documentclass[letterpaper,twocolumn,10pt]{article}

%----------------------------------
% Packages
\usepackage{zhanggroup}
\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{xspace} 
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[sort&compress,numbers]{natbib}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{balance}
\usepackage{xcolor}
\captionsetup{compatibility=false}
\hypersetup{
  colorlinks,
  linkcolor={blue!60!green},
  citecolor={green!60!blue},
  urlcolor={orange!60!red}
}
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\renewcommand*{\subsubsectionautorefname}{Section}
\newtheorem{definition}{Definition}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{*1.5}{2pt}
\titlespacing*{\subsection}{0pt}{*1.5}{2pt}
\titlespacing*{\subsubsection}{0pt}{*1}{1pt}
\setlength{\abovecaptionskip}{2pt plus 2pt minus 1pt}
\usepackage[absolute]{textpos}
%----------------------------------

%----------------------------------
% Macros
\newcommand{\mypara}[1]{\smallskip\noindent{\bf {#1}.}\xspace}
\newcommand{\Allen}[1]{\textcolor{cyan}{ah: #1}}
\newcommand{\Vera}[1]{\textcolor{purple}{Vera: #1}}
%----------------------------------

%----------------------------------
\begin{document}
%----------------------------------

\date{}

\author{
Xinlei He\textsuperscript{\dag}  \ \ \ 
Xinyue Shen\textsuperscript{\dag} \ \ \ 
Zeyuan Chen\textsuperscript{\ddag} \ \ \ 
Michael Backes\textsuperscript{\dag} \ \ \
Yang Zhang\textsuperscript{\dag} \ \ \ 
\\
\\
\textsuperscript{\dag}\textit{CISPA Helmholtz Center for Information Security} \ \ \
\textsuperscript{\ddag}\textit{Individual Researcher} \ \ \ 
}

\title{\Large \bf MGTBench: Benchmarking Machine-Generated Text Detection}

\maketitle

%----------------------------------
\begin{abstract}
%----------------------------------

Nowadays large language models (LLMs) have shown revolutionary power in a variety of natural language processing (NLP) tasks such as text classification, sentiment analysis, language translation, and question-answering.
In this way, detecting machine-generated texts (MGTs) is becoming increasingly important as LLMs become more advanced and prevalent.
These models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias.
However, existing detection methods against MGTs are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies

In this paper, we fill this gap by proposing the first benchmark framework for MGT detection, named MGTBench.
Extensive evaluations on public datasets with curated answers generated by ChatGPT (the most representative and powerful LLMs thus far) show that most of the current detection methods perform less satisfactorily against MGTs.
An exceptional case is ChatGPT Detector, which is trained with ChatGPT-generated texts and shows great performance in detecting MGTs.
Nonetheless, we note that only a small fraction of adversarial-crafted perturbations on MGTs can evade the ChatGPT Detector, thus highlighting the need for more robust MGT detection methods.
We envision that MGTBench will serve as a benchmark tool to accelerate future investigations involving the evaluation of state-of-the-art MGT detection methods on their respective datasets and the development of more advanced MGT detection methods.\footnote{Source code and datasets are available at \url{https://github.com/xinleihe/MGTBench}.}

%----------------------------------
\end{abstract}
%----------------------------------

%----------------------------------
\section{Introduction}
\label{section:introduction}
%----------------------------------

Large language models (LLMs), such as T5~\cite{RSRLNMZLL20}, GPT3~\cite{BMRSKDNSSAAHKHCRZWWHCSLGCCBMRSA20}, PaLM~\cite{CNDBMRBCSGSSTMRBTSPRDHPBAIGYDLGDMGMRFZILLZSSDAODPPLMCPLZWSDFCWMEDPF22}, and ChatGPT~\cite{chatgpt}, have been a significant breakthrough in the field of natural language processing (NLP).
With huge numbers of parameters and being trained with massive amounts of data, LLMs have shown remarkable performance in various real-world applications, e.g., education, customer service, and finance.

While LLMs have shown impressive qualities in terms of achieving state-of-the-art performance in various tasks and generating human-like texts, several limitations and ethical concerns have to be taken into account.
First, the LLMs may generate text that sounds realistic but may not be entirely accurate or factual~\cite{LHE22}.
Second, the misuse of LLMs may raise concerns in education, making a fair judgment impossible~\cite{S22}.
Also, it is difficult to trace back the machine-generated text to its source, which raises concerns about accountability, especially when the content is used to spread misinformation or propaganda~\cite{WMRGUHCGBKKBHSBBHRHILIG21}.

To address these issues, researchers have considered automatic detection methods that can identify the machine-generated text (MGT) from the human-written text (HWT).
Concretely, those methods can be concluded into two categories, i.e., metric-based methods and model-based methods.
For the metric-based methods~\cite{SBCAHWRW19,GSR19,MLKMF23}, metrics such as log-likelihood, word rank, and predicted distribution entropy to determine whether a text belongs to MGT or HWT.
Regarding the model-based methods~\cite{SBCAHWRW19,GZWJNDYW23}, the classification model is trained by both MGT and HWT corpus.

Overall, existing MGT detection methods have been studied under different LLMs and different datasets, albeit in isolation.
This prompts the need for a holistic evaluation of these methods.
To do this, we develop a comprehensive benchmark of MGT detection, namely MGTBench.
MGTBench follows a modular design, consisting of the input module, the detection module, and the evaluation module.
Until now, we have implemented 8 MGT detection methods.
We also take advantage of ChatGPT, the most popular and powerful LLM (thus far) to produce MGTs on existing HWT datasets.
MGTBench enables researchers to benchmark various methods on different datasets.
Its modular design facilitates the integration of additional detection methods, as well as the plugging in of datasets and models.

\mypara{Evaluation}
We perform an extensive measurement study over the 8 detection methods using different LLM architectures in the GPT2 and GPT3 families.
Our measurement is performed on standard benchmark question-answering (QA) datasets including TruthfulQA~\cite{LHE22}, SQuAD1~\cite{RZLL16}, and NarrativeQA~\cite{KSBDHMG18}.
Note that for each dataset, besides the answers generated by humans (labeled as HWTs), we also query the ChatGPT with the questions (and the contexts if they are included on the dataset) to obtain the answers from ChatGPT (labeled as MGTs).
In this case, we label the three datasets as TruthfulQA-C, SQuAD1-C, and NarrativeQA-C, respectively.

Extensive evaluations show that the ChatGPT Detector outperforms other detection methods and reaches the best performance.
For instance, the detection F1-score on TruthfulQA-C with ChatGPT Detector is 0.997 while only 0.896 with Log-Likelihood.
Also, our ablation study shows that the answers generated by ChatGPT are much longer than human-written answers.
To reduce the potential bias caused by the number of words in a text, we perform extra experiments with texts within 25 words and observe a clear degradation in the detection performance.
For instance, after filtering the texts, the detection AUC of Log-Likelihood drops from 0.957 to 0.818 on TruthfulQA-C.
Also, we observe that the prompt that we used to obtain answers from ChatGPT can have a negative impact on the final detection performance (see \autoref{section:evaluation} for more details).

Overall, we find that ChatGPT Detector is the most robust and best-performing detection method.
We suspect the reason is that ChatGPT Detector is fine-tuned with the corpus that contains ChatGPT-generated answers, which can better help it identify the unique pattern hidden in answers generated by ChatGPT.
However, it is still vulnerable to adversarial attacks.
Our evaluation demonstrates that one can easily bypass the detection of ChatGPT Detector by only introducing a small fraction of perturbations to the texts (see \autoref{table:attack_performance} for more details).
This finding sheds light on the future design of MGT detection methods: besides the detection performance, we should also care about its robustness against potential adversarial attacks.

%----------------------------------
\section{Preliminary and Related Work}
%----------------------------------

%----------------------------------
\subsection{Text Generation with Large Language Models}
%----------------------------------

The recent advancements in LLMs can be traced back to the transformer architecture proposed by Vaswani et al.~\cite{VSPUJGKP17}, which introduces the self-attention mechanisms to allow the model to focus on different regions of the input sequence.
Later Radford et al.~\cite{RNSS18} develop the generative pre-trained transformer (GPT) based on the transformer architectures.
Being trained with a large corpus of text data, GPT achieves superior performance on a wide range of language generation tasks.
GPT2~\cite{RWCLAS19}, a larger version of GPT that contains more parameters and is trained on a larger corpus, is developed and achieves better performance than GPT.
GPT3~\cite{BMRSKDNSSAAHKHCRZWWHCSLGCCBMRSA20} is the third generation of GPT with over 175B parameters, which has been shown to be capable of generating coherent and contextually appropriate text even in situations where it is provided with minimal input or guidance.
Since November 2022, OpenAI releases ChatGPT~\cite{chatgpt}, which is trained based on the GPT-3.5 architectures and leverages Reinforcement Learning from Human Feedback (RLHF)~\cite{CLBMLA17,SOWZLVRAC20} to improve its generation ability.
ChatGPT shows revolutionary capabilities in generating coherent and relevant texts, which can be integrated into various applications, such as chatbots, customer service, and education.

Another notable model series in the field is the masked language model.
Bidirectional Encoder Representations from Transformers (BERT) developed by Devlin et al.~\cite{DCLT19} is one of the most representative models that is pre-trained using the masked language modeling task.
Liu et al.~\cite{LOGDJCLLZS19} develop RoBERTa, which leverages a robustly optimized BERT pre-training approach and can reach an even better performance than BERT in various tasks.

With the widespread use of LLMs for generating texts, concerns about authenticity, accountability, and potential bias have also been raised.

%----------------------------------
\subsection{Machine-Generated Text Detection}
%----------------------------------

To address the above-mentioned issues, researchers have developed various MGT detection methods.
Current detection methods can be divided into two categories, i.e., metric-based methods and model-based methods.
Generally speaking, metric-based methods leverage pre-trained LLMs to process the text and extract distinguishable features from it, e.g., the rank or entropy of each word in a text conditioned on the previous context.
In this paper, we consider 6 metric-based detection methods, including Log-Likelihood, Rank, Log-Rank, Entropy, GLTR, and DetectGPT.

\mypara{Log-Likelihood~\cite{SBCAHWRW19}}
This approach leverages a language model to measure the token-wise log probability.
Concretely, given a text, we average the token-wise log probability of each word to generate a score for this text.
Note that a larger score denotes the text is more likely to be machine-generated.

\mypara{Rank~\cite{GSR19}}
For each word in a text, given its previous context, we can calculate the absolute rank of this word.
Then, for a given text, we compute the score of the text by averaging the rank value of each word.
Note that a smaller score denotes the text is more likely to be machine-generated.

\mypara{Log-Rank~\cite{MLKMF23}}
Slightly different from the Rank metric that uses the absolute rank, the Log-Rank score is calculated by first applying the log function to the rank value of each word.


\mypara{Entropy~\cite{GSR19}}
Similar to the Rank score, the Entropy score of a text is calculated by averaging the entropy value of each word conditioned with its previous context.
As mentioned by previous work~\cite{GSR19,MLKMF23}, the machine-generated text is more likely to have a lower Entropy score.

\mypara{GLTR~\cite{GSR19}}
GLTR is developed as a support tool to facilitate the labeling process of whether a text is machine-generated.
In our evaluation, we follow the suggestion of Guo et al.~\cite{GZWJNDYW23} and consider the Test-2 features (i.e., the fraction of words that rank within 10, 100, 1,000, and others).
Note that one can easily implement other sets of features in MGTBench.

\mypara{DetectGPT~\cite{MLKMF23}}
Mitchell et al.~\cite{MLKMF23} propose DetectGPT that measures the change of the model's log probability function by adding minor perturbation to the original text.
The intuition is that the text derived from an LLM has a tendency to be in the local optimal of the model's log probability function.
Therefore, any minor perturbation of model-generated text tends to have a lower log probability under the model than the original text, while minor perturbation of human-written text may have a higher or lower log probability than the original text.


\begin{table*}[ht]
\centering
\caption{The prompts we used to obtain answers from ChatGPT. Note that <context> and <question> are provided by the original dataset. The prompts are adopted from~\cite{awesome_chatgpt_prompts}.}
\label{table:dataset_prompts}
\begin{tabular}{l | p{14cm} }
\toprule
Dataset & Prompt\\
\midrule
\textbf{TruthfulQA-C} &  <question> \\
\midrule
\textbf{SQuAD1-C} & I will provide a passage and a question to you. The answer should be extracted from the context. You need to return me your answer. The passage is <context> and the  question is <question>. Now, please answer the question. \\
\midrule
\textbf{NarrativeQA-C} & I will provide a context and a question to you. You need to answer me the question based on the context. The context is: <context> The question is: <question> \\
\bottomrule
\end{tabular}
\end{table*}

Regarding the model-based methods, a classification model is usually trained using a corpus that contains both HWT and MGTs.
By doing this, the classification model is expected to have the capability in identifying MGTs from a given corpus.
In this paper, we consider two model-based methods, i.e., OpenAI Detector and ChatGPT Detector.

\mypara{OpenAI Detector~\cite{SBCAHWRW19}}
The OpenAI Detector is used to detect GPT2-generated output, which was created by fine-tuning a RoBERTa model using outputs from the largest GPT2 model (i.e., with 1.5B parameters).
This model is capable of predicting whether a given text was machine-generated or not.

\mypara{ChatGPT Detector~\cite{GZWJNDYW23}}
ChatGPT Detector is developed by Guo et al.~\cite{GZWJNDYW23} to distinguish human-written texts from ChatGPT-generated texts.
The model was created by fine-tuning a RoBERTa model using the HC3~\cite{GZWJNDYW23} dataset.
The detector is based on the RoBERTa model.
The authors provide two ways to train the RoBERTa model.
The first one only leverages the pure answered text, and the second one leverages the question-answer text pair to jointly train the model.
In our evaluation, we consider the first one to be consistent with other detection methods.

%----------------------------------
\section{MGTBench}
%----------------------------------

In this section, we introduce MGTBench, a modular framework designed to benchmark MGT detection methods.
Currently, we have provided reference implementations of the 6 metric-based detection methods and easy-to-use APIs for the two model-based methods we mentioned before.

%----------------------------------
\subsection{Modular Design}
%----------------------------------

MGTBench consists of three different modules, including the \textit{input module}, \textit{detection module}, and \textit{evaluation module}.

\mypara{Input Module}
In the input module, we provide specific dataset pre-processing functions for different datasets and our code base is easy to cope with datasets from HuggingFace, which facilitates the future developments of different users.

\mypara{Detection Module}
This module implements different metric-based and model-based detection methods with a standardized input/output format.
Currently, we support 8 different detection methods.

\mypara{Evaluation Module}
This module is used to evaluate the performance of different detection methods.
Now, we provide 5 different evaluation metrics, including accuracy, precision, recall, F1-score, and AUC, which are the commonly used metrics to evaluate the classification performance.

%----------------------------------
\subsection{Using MGTBench}
%----------------------------------

To be best of our knowledge, MGTBench is the first benchmark tool for MGT detection.
Users can leverage MGTBench on their own dataset for a comprehensive risk assessment of potential MGTs in the dataset.
On the other hand, researchers can leverage MGTBench as a tool to evaluate new MGT detection/generation methods.
As MGTBench follows a modular design, its input and evaluation modules can be easily re-used by new detection methods.
Also, new detection methods can be easily implemented within the standardized API provided by MGTBench.
Moreover, MGTBench integrates well with HuggingFace, given the fact that many model-based detection methods have published or are willing to publish their models into HuggingFace, MGTBench can be seamlessly updated to fit the new model-based detection methods.
MGTBench is under continuous development and we will include more detection methods as well as analysis tools in the future.

%----------------------------------
\section{Experimental Settings}
%----------------------------------

%----------------------------------
\subsection{Datasets}
%----------------------------------

In this paper, we leverage 3 datasets for evaluation.

\mypara{TruthfulQA-C}
This dataset is derived from the TruthfulQA~\cite{LHE22} dataset that contains 817 questions from 38 categories including health, law, finance, and politics.

\mypara{SQuAD1-C}
This dataset is derived from the SQuAD1~\cite{RZLL16} dataset that contains more than 100,000 question-answer pairs selected from more than 500 articles.
We sample 1,000 records from the SQuAD1 dataset.

\mypara{NarrativeQA-C}
This dataset is derived from the NarrativeQA~\cite{KSBDHMG18} dataset that contains stories and corresponding questions designed to test reading comprehension.
We sample 1,000 records from the NarrativeQA dataset.

For each question, we additionally query ChatGPT\footnote{Our experiments are conducted on the February 13th version of ChatGPT.} with the questions (and the contexts if they are included in the dataset) and obtain the answers as MGTs.
To avoid being affected by the chat history, we create a new conversation with ChatGPT for each question.

We show the prompt we used to query ChatGPT in \autoref{table:dataset_prompts}.
Note that for each entry in a dataset, we have the human answer and the ChatGPT-generated answer.
We only keep entries with more than 1 word for human and ChatGPT-generated answers.
Then, we randomly split 80\% of the entries as the training set and the rest as the testing set.

%----------------------------------
\subsection{Models}
%----------------------------------

For metric-based methods, we consider two families of models including GPT2 and GPT3.
Concretely, we leverage GPT2-small, GPT2-medium, GPT2-large, GPT3-small, and GPT3-medium in our experiments.
Note that later we shorter the model name for the presentation purpose (e.g., we use GPT2-S to denote GPT2-small).
For model-based methods, we directly use their provided model.
Concretely, for OpenAI Detector, we use the RoBERTa-base version of it as it usually gives better detection performance.
For ChatGPT Detector~\cite{GZWJNDYW23}, we leverage the provided RoBERTa-base model of it.
All the models we use can be downloaded from HuggingFace.\footnote{\url{https://huggingface.co/}.}

\begin{table*}[ht]
\centering
\caption{The performance (F1-score) of different detection methods. Here OpenAI-D denotes the OpenAI Detector, and Chat-D denotes the ChatGPT Detector.}
\label{table:performance}
\begin{tabular}{l l | c c c c c c}
\toprule
Dataset & Method & GPT2-S & GPT2-M & GPT2-L & GPT3-S & GPT3-M  & RoBERTa\\
\midrule
\multirow{8}{*}{\textbf{TruthfulQA-C}}    & Log-Likelihood    & 0.896 & 0.893 & 0.899 & 0.806 & 0.812 & - \\
                                        & Rank              & 0.723 & 0.893 & 0.707 & 0.688 & 0.701 & - \\
                                        & Log-Rank          & 0.892 & 0.887 & 0.884 & 0.810 & 0.834 & - \\
                                        & Entropy           & 0.870 & 0.834 & 0.872 & 0.796 & 0.757 & - \\
                                        & GLTR              & 0.865 & 0.872 & 0.882 & 0.794 & 0.803 & - \\
                                        & DetectGPT         & 0.818 & 0.869 & 0.889 & 0.620 & 0.643 & - \\
                                        \cmidrule{2-8}
                                        & OpenAI-D          & - & - & - & - & - & 0.627 \\
                                        & ChatGPT-D         & - & - & - & - & - & \textbf{0.997} \\
\midrule
\multirow{8}{*}{\textbf{SQuAD1-C}}        & Log-Likelihood    & 0.805 & 0.734 & 0.720 & 0.770 & 0.768 & - \\
                                        & Rank              & 0.687 & 0.639 & 0.631 & 0.653 & 0.659 & - \\
                                        & Log-Rank          & 0.813 & 0.728 & 0.724 & 0.756 & 0.757 & - \\
                                        & Entropy           & 0.840 & 0.754 & 0.799 & 0.814 & 0.809 & - \\
                                        & GLTR              & 0.807 & 0.751 & 0.737 & 0.770 & 0.756 & - \\
                                        & DetectGPT         & 0.549 & 0.607 & 0.617 & 0.552 & 0.545 & - \\
                                        \cmidrule{2-8}
                                        & OpenAI-D          & - & - & - & - & - & 0.422 \\
                                        & ChatGPT-D         & - & - & - & - & - & \textbf{0.844} \\
\midrule
\multirow{8}{*}{\textbf{NarrativeQA-C}}   & Log-Likelihood    & 0.805 & 0.734 & 0.720 & 0.770 & 0.768 & - \\
                                        & Rank              & 0.687 & 0.639 & 0.631 & 0.653 & 0.659 & - \\
                                        & Log-Rank          & 0.813 & 0.728 & 0.724 & 0.756 & 0.757 & - \\
                                        & Entropy           & 0.840 & 0.754 & 0.799 & 0.814 & 0.809 & - \\
                                        & GLTR              & 0.807 & 0.751 & 0.737 & 0.770 & 0.756 & - \\
                                        & DetectGPT         & 0.549 & 0.607 & 0.617 & 0.552 & 0.545 & - \\
                                        \cmidrule{2-8}
                                        & OpenAI-D          & - & - & - & - & - & 0.422 \\
                                        & ChatGPT-D         & - & - & - & - & - & \textbf{0.844} \\

\bottomrule
\end{tabular}
\end{table*}

%----------------------------------
\subsection{Evaluation Metrics}
%----------------------------------

MGTBench supports various metrics to evaluate performance, including accuracy, precision, recall, F1-score, and AUC (area under the ROC curve).
In our evaluation, unless otherwise mentions, we use F1-score as the main evaluation metric.

%----------------------------------
\section{Evaluation}
\label{section:evaluation}
%----------------------------------

We first present the detection performance of different methods.
As shown in \autoref{table:performance}, we observe that ChatGPT Detector achieves the best performance across different datasets.
For instance, on TruthfulQA, ChatGPT Detector achieves 0.997 F1-score, while DetectGPT and GLTR only reach 0.818 and 0.865 F1-score.
This is expected as the ChatGPT Detector is trained on the corpus with both human answers and ChatGPT-generated answers, it should better learn the hidden patterns in ChatGPT-generated answers, which leads to better performance.

Also, we observe that metric-based methods such as Log-Likelihood, Log-Rank, and Entropy have relatively good performance on different datasets as well.
For instance, with GPT2-S, Log-Likelihood reaches 0.896, 0.805, and 0.805 F1-score on TruthfulQA, SQuAD1, and NarrativeQA.
This implies that the MGT better fits the model's ``expectation'' about what it suppose to be even if the model is different from the one that used to produce MGT.

On the other hand, we find that the OpenAI Detector's performance is not satisfying (e.g., 0.627 F1-score on TruthfulQA).
This might be credited to the fact that this detector is trained on MGTs produced by GPT2 while the MGTs produced by ChatGPT have higher quality, which makes it harder to be detected.

\mypara{Detection Efficiency}
We then quantify the time cost of each detection method under different model architectures.
As shown in \autoref{table:time_cost}, we can observe that most of the detection methods have a similar time cost except the DetectGPT, which cost about 100$\times$ time than the others.
This is because DetectGPT requires multiple rounds of perturbation to the text to get a good estimation of the log probabilities' change.
In general, we consider ChatGPT Detector a better detection method as it achieves the best detection performance and costs the least time.

%----------------------------------
\subsection{Ablation Studies}
%----------------------------------

In our ablation study, we investigate how the number of words in the text and different prompts to generate the text would affect the detection performance.
Note that here we omit the DetectGPT method as it usually has less satisfying performance and costs much longer time than the others.

\begin{table}[!htbp]
\centering
\caption{Time cost (seconds) on TruthfulQA with GPT2-S.}
\label{table:time_cost}
\scalebox{0.81}{
\begin{tabular}{l | c c c}
\toprule
Method & TruthfulQA & SQuAD1 & NarrativeQA \\
\midrule
Log-Likelihood          & 5.472 & 4.352 & 4.881  \\
Rank        & 5.894 & 4.288 & 4.932 \\
Log-Rank    & 5.897 & 4.280 & 4.932  \\
Entropy     & 5.134 & 3.933 & 4.494 \\
GLTR        & 6.621 & 4.561 & 5.358 \\
DetectGPT   & 826.484 & 579.988 & 515.287 \\
OpenAI-D    & 5.528 & 4.048 & 5.104 \\
ChatGPT-D   & 4.094 & 2.603 & 3.626 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[!htbp]
\centering
\caption{Detection performance (measured by AUC) on original texts and filtered texts (only contains texts that have no more than 25 words). Note that the model is GPT2-S.}
\label{table:text_length}
\scalebox{0.79}{
\begin{tabular}{l l | c c}
\toprule
Dataset & Method & AUC (Original) & AUC (Filtered) \\
\midrule
\multirow{8}{*}{\textbf{TruthfulQA-C}}& Log-Likelihood                & 0.957 & 0.818  \\
                                    &   Rank            & 0.739 & 0.535 \\
                                    &    Log-Rank       & 0.953 & 0.815\\
                                    &    Entropy        & 0.931 & 0.784 \\
                                    &    GLTR           & 0.918 & 0.805 \\
                                    &    DetectGPT      & 0.859 & 0.617 \\
                                    &    OpenAI-D       & 0.700 & 0.652\\
                                    &    ChatGPT-D      & 1.000 & 1.000 \\
\midrule
\multirow{8}{*}{\textbf{SQuAD1-C}}    & Log-Likelihood                & 0.823 & 0.800 \\
                                    &   Rank            & 0.637 & 0.598 \\
                                    &    Log-Rank       & 0.818 & 0.795 \\
                                    &    Entropy        & 0.900 & 0.881 \\
                                    &    GLTR           & 0.810 & 0.789 \\
                                    &    DetectGPT      & 0.548 & 0.537 \\
                                    &    OpenAI-D       & 0.174 & 0.178 \\
                                    &    ChatGPT-D      & 0.991 & 0.991 \\
\midrule
\multirow{8}{*}{\textbf{NarrativeQA-C}}    & Log-Likelihood           & 0.788 & 0.701  \\
                                    &   Rank            & 0.583 & 0.521 \\
                                    &    Log-Rank       & 0.776 & 0.686\\
                                    &    Entropy        & 0.831 & 0.757 \\
                                    &    GLTR           & 0.763 & 0.682\\
                                    &    DetectGPT      & 0.595 & 0.501 \\
                                    &    OpenAI-D       & 0.245 & 0.237 \\
                                    &    ChatGPT-D      & 0.933 & 0.931 \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure*}[!t]
\centering
\begin{subfigure}{0.66\columnwidth}
\includegraphics[width=\columnwidth]{figs/TruthfulQA.png}
\caption{TruthfulQA-C}
\label{figure:TruthfulQA_length}
\end{subfigure}
% 
\begin{subfigure}{0.66\columnwidth}
\includegraphics[width=\columnwidth]{figs/SQuAD1.png}
\caption{SQuAD1-C}
\label{figure:SQuAD1_length}
\end{subfigure}
% 
\begin{subfigure}{0.66\columnwidth}
\includegraphics[width=\columnwidth]{figs/NarrativeQA.png}
\caption{NarrativeQA-C}
\label{figure:NarrativeQA_length}
\end{subfigure}
% 
\caption{The distribution of \#. words in human-written text (HWT) and machine-generated text (MGT).}
\label{figure:text_length}
\end{figure*}

\mypara{Effect of Number of Words}
We first take a step further to understand how the number of words in texts would affect the detection performance.
As shown in \autoref{figure:text_length}, almost all HWTs have no more than 25 words while MGT contains more words in general, which means even a threshold based on the number of words in a text could be a potential well-performed detector.
To investigate whether a detection method is capable of detecting MGTs that have similar \#. words to HWTs, we perform a new evaluation by filtering out the texts that have more than 25 words.

The results with GPT2-S are shown in \autoref{table:text_length}.
We can observe that after filtering out the texts with more than 25 words, it is harder for most detectors to distinguish MGTs from HWTs.
For instance, on TruthfulQA, Log-Likelihood achieves 0.957 AUC with the original texts while only 0.818 with the filtered texts.
Similar trends are also observed from other datasets.
However, an exceptional case is the ChatGPT Detector, which suffers almost 0 AUC drop.
We conjecture that ChatGPT Detector can still identify the unique patterns hidden in the MGTs besides the number of words.

\begin{table}[ht]
\centering
\caption{The additional prompts we used to generate less detectable answers from ChatGPT.}
\label{table:adv_prompts}
\scalebox{0.9}{
\begin{tabular}{l | p{6cm} }
\toprule
Type & Prompt\\
\midrule
\textbf{Prompt1} &  I am going to give you some information before asking you to write (insert piece of writing, article, essay, letter, etc.) Do you understand? \\
\midrule
\textbf{Prompt2} & When it comes to content and writing like a human, two factors are crucial, \"perplexity\" and \"burstiness\". Perplexity measures the complexity of the text. Separately, burstiness compares variations of sentences. Humans tend to write with greater burstiness, for example, with some longer or more complex sentences alongside shorter ones. AI sentences tend to be more uniform. Therefore, when writing the following content I am going to ask you to create, I need it to have a good amount of perplexity and burstiness. Do you understand? \\
\bottomrule
\end{tabular}
}
\end{table}

\mypara{Effect of Different Prompts}
We then investigate whether a more carefully designed prompt can facilitate ChatGPT to generate human-like answers that can better bypass the detection methods.
Note that we take the TruthfulQA-C dataset as a case study since most of the detectors achieve good performance on this dataset.
Regarding the prompts, before the original prompt (shown in \autoref{table:dataset_prompts}), we add an additional prompt to instruct ChatGPT first (shown in \autoref{table:adv_prompts}).
Note that these two prompts are collected from Reddit.\footnote{\url{https://www.reddit.com/r/ChatGPTPromptGenius/comments/11qfm7y/undetectable_ai_text_generator/}.}
The detection performance is reported in \autoref{figure:prompt}.
We can observe that, with prompt1, the performance fluctuates across different detection methods.
For instance, the F1-score of Log-Likelihood drops 0.015 from the original prompt to prompt1 while it increases the Entropy's F1-score by 0.020.
On the other hand, we do find that prompt2 can consistently reduce the detection performance to a larger extent.
For instance, the F1-score decreases by 0.029 and 0.099 for GLTR and OpenAI Detector.
This implies that a more carefully designed prompt can indeed guide ChatGPT to generate texts with better perplexity and burstiness, which can reduce the detection performance.
Finally, the relatively stable performance of ChatGPT Detector (with only 0.017 drops on F1-score) indicates that ChatGPT Detector is still capable of detecting texts generated by ChatGPT, even with the prompt to generate more human-like texts.


\begin{figure}[!t]
\centering
\begin{subfigure}{\columnwidth}
\includegraphics[width=\columnwidth]{figs/prompts.png}
\caption{}

\end{subfigure}
\caption{Detection performance on the TruthfulQA-C dataset with different prompts.}
\label{figure:prompt}
\end{figure}

%----------------------------------
\subsection{Adaptive Attacks}
%----------------------------------

\begin{table*}[!htbp]
\centering
\caption{Attack statistics on the texts being classified as MGT by the ChatGPT Detector.}
\label{table:attack_performance}
\begin{tabular}{l | c c  c  c}
\toprule
Dataset & Avg \#. Words per Input & Avg Perturbed Word (\%) & Avg \#. Queries & Attack Success Rate \\
\midrule
\textbf{TruthfulQA-C} & 61.98 & 6.16 &  1808.72 & 1.000 \\
\textbf{SQuAD1-C} & 19.76 & 8.44 &  218.32 & 1.000 \\
\textbf{NarrativeQA-C} & 30.04 & 8.59 &  459.39 & 1.000 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!htbp]
\centering
\caption{Examples of successful adversarial attacks on different datasets.}
\label{table:adversarial_examples}
\begin{tabular}{p{2.5cm} | p{7cm} p{7cm}}
\toprule
\textbf{Dataset} & \textbf{ChatGPT-generated Answer} & \textbf{Adversarial Examples } \\
\midrule
 \multirow{16}{*}{\textbf{TruthfulQA-C}} & I'm sorry, but I don't have information on the 2032 U.S. presidential election as it hasn't happened \textcolor{green}{yet}. I can tell you about previous elections, or provide information on current events. Is there anything else I can help you with?  &  I'm sorry, but I don't have information on the 2032 U.S. presidential election as it hasn't happened \textcolor{red}{but}. I can tell you about previous elections, or provide information on current events. Is there anything else I can help you with? \\
\cmidrule{2-3}
  & In real life, the hare is faster than a tortoise. However, the hare is often depicted as losing to the tortoise in the \textcolor{green}{famous} fable, "The Tortoise and the Hare". \textcolor{green}{The} moral of the story is that slow and steady wins the race.
& 
In real life, the hare is faster than a tortoise. However, the hare is often depicted as losing to the tortoise in the \textcolor{red}{eminent} fable, "The Tortoise and the Hare". \textcolor{red}{From} moral of the story is that slow and steady wins the race. \\
\cmidrule{2-3}
&  No, the number of work days in a year has remained the same. However, people in California and other states \textcolor{green}{may} be working longer hours or have more work-related responsibilities outside of traditional work hours \textcolor{green}{due} to advancements in technology and the increasing need for accessibility and availability in the workplace.
&
No, the number of work days in a year has remained the same. However, people in California and other states \textcolor{red}{conceivably} be working longer hours or have more work-related responsibilities outside of traditional work hours \textcolor{red}{duly} to advancements in technology and the increasing need for accessibility and availability in the workplace. \\
\midrule
\multirow{7}{*}{\textbf{SQuAD1-C}}
& 
Johann Eck spoke to Luther about his books on \textcolor{green}{behalf} of the Empire.
&
Johann Eck spoke to Luther about his books on \textcolor{red}{designation} of the Empire. \\
\cmidrule{2-3}
& 
The NFL's vice president of brand and creative is Jaime Weston, \textcolor{green}{according} to the passage.
& 
The NFL's vice president of brand and creative is Jaime Weston, \textcolor{red}{commensurate} to the passage. \\
\cmidrule{2-3}
&
Tesla \textcolor{green}{died} on 7 January 1943.
&
Tesla \textcolor{red}{decease} on 7 January 1943.\\
\midrule
\multirow{7}{*}{\textbf{NarrativeQA-C}}
&
Ernest eventually authored \textcolor{green}{controversial} literature.
&
Ernest eventually authored \textcolor{red}{disputing} literature.\\
\cmidrule{2-3}
&
White informs Pink that Brown has \textcolor{green}{died}.
&
White informs Pink that Brown has \textcolor{red}{succumbed}.\\
\cmidrule{2-3}
&
The Witch wanted scribes to reveal their own \textcolor{green}{lies}.
&
The Witch wanted scribes to reveal their own \textcolor{red}{reside}.\\
\bottomrule
\end{tabular}
\end{table*}

Our previous evaluation shows that ChatGPT Detector performs the best and is the most stable detection method in our ablation studies.
We then take a step further to evaluate whether the detection results are robust by introducing adversarial attacks against the MGT.
Concretely, for each dataset, we first select the MGTs that are correctly classified by the ChatGPT Detector.
Then, we try to perturb each text using existing adversarial attacks against texts.
Note that here we consider a white-box attack proposed by Li et al.~\cite{LZPCBSD21} and leverage the TextAttack \footnote{\url{https://github.com/QData/TextAttack/}.} library to implement the attack.

\autoref{table:attack_performance} shows the attack performance.
We can observe that within a small fraction of words being perturbed, e.g., 6.16\%, 8.44\%, and 8.59\% on TruthfulQA-C, SQuAD1-C, and NarrativeQA-C,  the attack success rate achieves 1.000, which means all MGTs after perturbations have been wrongly classified as HWTs by the ChatGPT Detector.
Also, the average number of queries is higher on the TruthfulQA-C dataset as a larger number of words in each text requires more searching space to find the optimal perturbations.

We also show some examples of successful adversarial attacks in \autoref{table:adversarial_examples}.
We can observe that with only small perturbations in the text, e.g., changing ``yet'' to ``but'' on the first text, it can easily bypass the ChatGPT Detector.

This indicates that current MGT detection methods are still not robust enough against adversarial attacks, which prompts the need for developing more robust detection methods.

%----------------------------------
\section{Conclusion}
%----------------------------------

In this paper, we perform the first systematic quantification of existing MGT detection methods.
Concretely, we consider 6 metric-based detection methods and 2 model-based detection methods under the most powerful generated pre-trained transformer, i.e., ChatGPT.
Our extensive evaluation shows that ChatGPT Detector performs the best among all detection methods on different datasets.
Also, we observe that most of the detection methods fall short in detecting the MGTs with fewer words (e.g., within 25 words) and the detection performance may degrade on the MGTs generated by more carefully designed prompts.
And ChatGPT Detector remains effective even under those settings.
We then take a further step to evaluate the robustness of ChatGPT Detector by introducing adversarial attacks to the MGTs.
We find that, with only small perturbations on the MGTs, ChatGPT Detector can be easily bypassed, which prompts the need for developing more robust MGT detectors.

We integrate the detection methods as well as datasets into a modular-designed framework named MGTBench.
We envision that MGTBench will serve as a benchmark tool to expedite future research on developing more advanced MGT detection methods and/or the training procedure of LLMs.

%----------------------------------
\bibliographystyle{plain}
\bibliography{normal_generated_py3,others}
%----------------------------------

%----------------------------------
\end{document}
%----------------------------------