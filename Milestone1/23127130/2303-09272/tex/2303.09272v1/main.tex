\documentclass[sigconf]{acmart}

\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{filecontents}
\usepackage{footnote}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{url}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{listings}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{balance}
\usepackage{lipsum} 
\usepackage{indentfirst}
\usepackage[htt]{hyphenat}
\usepackage{tcolorbox}


\AtBeginEnvironment{tcolorbox}{\small}

\settopmatter{printacmref=true, printccs=true, printfolios=true}
%\settopmatter{printacmref=false, printccs=false, printfolios=true}
%\setcopyright{none}
%\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\captionsetup[figure]{font=small,labelfont={bf, small}}
\captionsetup[table]{font=small,labelfont={bf, small}}

\usepackage{siunitx}
\sisetup{
    % load=prefixed,
    binary-units=true,
    group-separator={,},
    group-minimum-digits=4,
    group-digits=integer,
    group-four-digits=true,
    detect-weight=true,
    detect-family=true
  }

\def\eg{\emph{e.g.,}\xspace}
\def\etc{\emph{etc.}\xspace}
\def\ie{\emph{i.e.,}\xspace}
\def\etal{\emph{et al.}\xspace}
\def\vs{\emph{vs.}\xspace}
\def\cf{\emph{cf.}\xspace}

\newcommand{\one}{({\em i}\/)}
\newcommand{\two}{({\em ii}\/)}
\newcommand{\three}{({\em iii}\/)}
\newcommand{\four}{({\em iv}\/)}
\newcommand{\five}{({\em v}\/)}
\newcommand{\six}{({\em vi}\/)}
\newcommand{\seven}{({\em vii}\/)}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\newcommand{\tofill}[0]{\textcolor{blue}{\textbf{[to be filled]~}}}
\newcommand{\new}[1]{\textcolor{purple}{#1}}
\newcommand{\jason}[1]{\textit{\textcolor{red}{[Jason: #1]}}}
\newcommand{\tina}[1]{\textbf{\textcolor{blue}{Tina: #1}}}
\newcommand{\chamikara}[1]{\textit{\textcolor{violet}{[Chamikara: #1]}}}
\newcommand{\Chehara}[1]{\textit{\textcolor{brown}{[Chehara: #1]}}}
\newcommand{\Humphrey}[1]{\textit{\textcolor{cyan}{[Humphrey: #1]}}}
\newcommand{\Haonan}[1]{\textit{\textcolor{olive}{[Haonan: #1]}}}
\newcommand{\Jiamin}[1]{\textit{\textcolor{teal}{[Jiamin: #1]}}}


\setlength{\textfloatsep}{10pt}
\renewcommand{\tabcolsep}{5pt}

\widowpenalty=10000
\clubpenalty=10000
\brokenpenalty=10000

    \captionsetup[table]{skip=3pt}
\setlength{\textfloatsep}{10pt}
\renewcommand{\tabcolsep}{3pt}
    


%\renewcommand\footnotetextcopyrightpermission[1]{} 
%\settopmatter{printacmref=false}
%\settopmatter{printfolios=true}

\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[WWW '23 Companion]{Companion Proceedings of the ACM Web Conference 2023}{April 30-May 4, 2023}{Austin, TX, USA}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion), April 30-May 4, 2023, Austin, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3543873.3587321}
\acmISBN{978-1-4503-9419-2/23/04}

\begin{document}



\title{Copyright Protection and Accountability of Generative AI: \\Attack, Watermarking and Attribution}

%\title{Copyright Protection of AI-Generated Images and Accountability of Generative AI Models: Attack, Watermarking and Attribution}


%%% old version
\begin{comment}
\author{Haonan Zhong$^{\ast}$$^{\mathsection}$, Jiaming Chang$^{\ast}$$^{\mathsection}$, Ziyue Yang$^{\dagger}$$^\mathsection$, Tingmin Wu$^{\ddagger}$, \and  Pathum Chamikara Mahawaga Arachchige$^{\ddagger}$, Chehara Pathmabandu$^{\ddagger}$, and 
Minhui Xue$^{\ddagger}$} 

\affiliation{
\institution{$^{\ast}$University of New South Wales, Australia}
\institution{$^{\dagger}$The Australian National University,  Australia}
\institution{$^{\ddagger}$CSIRO's Data61, Australia}
\country{}
}
\end{comment}

\author{Haonan Zhong$^{1}$$^{\mathsection}$, Jiamin Chang$^{1}$$^{\mathsection}$, Ziyue Yang$^{2}$$^\mathsection$, Tingmin Wu$^{3}$, \and  Pathum Chamikara Mahawaga Arachchige$^{3}$, Chehara Pathmabandu$^{3}$, and 
Minhui Xue$^{3}$} 

\affiliation{
\institution{$^{1}$University of New South Wales, Australia}
\institution{$^{2}$The Australian National University,  Australia}
\institution{$^{3}$CSIRO's Data61, Australia}
\country{}
}

\renewcommand \authors{Haonan Zhong, Jiamin Chang, Ziyue Yang, Tingmin Wu,  Pathum Chamikara Mahawaga Arachchige, Chehara Pathmabandu, and Minhui Xue}

\renewcommand{\shortauthors}{Zhong et al.}


\begin{abstract}
   % Generative Adversarial Networks (GANs) have become increasingly popular in recent years for image generation, 
   %
    Generative AI (e.g., Generative Adversarial Networks – GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance tracing on training sets.
    
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002978.10003022</concept_id>
<concept_desc>Security and privacy~Software and application security</concept_desc>
<concept_significance>500</concept_significance>
</concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Software and application security}

\maketitle 

\def\thefootnote{$^{\mathsection}$}\footnotetext{The first three authors made equal contributions, and the work was done at CSIRO's Data61, Australia.}\def\thefootnote{\arabic{footnote}}
\pagestyle{plain}


\section{Introduction}

%Generative Adversarial Networks (GANs) have been rapidly advancing in the past decade in image generation. However, as GANs become more prevalent in research and industry, a pressing issue has emerged: how to protect the Intellectual Property Right (IRP) of images and models generated by GANs. 
%
The field of Generative AI (e.g., Generative Adversarial Networks – GANs) has made significant advancements in recent times.  These advancements have led Generative AI to improve its capacity to produce highly realistic content such as artwork and images~\cite{zhang_styleswin_2022}. Hence, there is widespread utilization of Generative AI in both research and industry, leading to significant concerns regarding the protection of Intellectual Property Rights (IPR).
%    
%Widespread concerns have arisen over the different dimensions of using deep generative networks without the consent of copyright content owners. 
%
There is a surge of GAN-based image generative model usage for creating content that can infringe upon existing copyrights, spoof security systems, and undermine the rights of content owners through the spread of misinformation, affecting personal reputations. Consequently, AI-Generated images are facing lawsuits over alleged copyright violations. One of the notable examples of a legal dispute involving AI-Generated images includes the lawsuit filed by Getty Images and a group of artists against AI art generators.\footnote{https://www.businessinsider.com/ai-art-artists-getty-images-lawsuits-stable-diffusion-2023-1?op=1}
%
Additionally, producing state-of-the-art generative models requires substantial computational resources and large datasets. These issues have generated growing interest in research on protecting and verifying the ownership of the GAN models and training sets.


Previous copyright protection approaches for images and GANs include attacks (e.g., adversarial noise), watermarking, and attribution techniques. However, the literature focuses mainly on Deep Neural Networks (DNNs) and has a limited focus on developing copyright protection frameworks of GANs~\cite{chen_copy_2022}. 

Our research aims to fill this gap by conducting a comprehensive analysis of the effectiveness of the existing copyright protection approaches for GANs. The proposed framework assists in identifying the key factors that determine the effectiveness of copyright protection techniques through the analysis of various GAN models. In order to accomplish this goal, we explore and answer the following three research questions.


    \begin{itemize} [itemsep=0pt,partopsep=0pt, leftmargin=*]
        
        \item \textbf{RQ1:} Can current adversarial attacks  effectively prevent copyright violations of input images? 
        
        \item \textbf{RQ2:} 
        %
        %Is it possible to protect and verify the IPR
        %
        Can the IPR of GANs and their training sets be protected and verified through watermarking?
        %
        %of GAN models and training sets through watermarks?
        
       \item  \textbf{RQ3:} Can current attribution methods effectively support IPR protection and trace toxic generative AI model accountability through source attribution of output images?
    \end{itemize}

\begin{figure}[tp]
\centering
\includegraphics[width=0.75\linewidth]{figures/Flow.jpg}
\vspace{-0.2cm}
\caption{The proposed GAN and image copyright protection and accountability approach evaluation framework.}
%Different copyright mechanisms and the Corresponding Components of GANs}
\label{fig:flow}
\vspace{-0.2cm}
\end{figure}


\section{Experimental Setup}

This section provides the details of the experimental setup of the proposed work. Figure~\ref{fig:flow} provides a schematic representation of the framework utilized for evaluating the IPR protection methods on GANs.
%
%comprehensive picture of the IPR protection in GANs. 
%
To evaluate the effectiveness of different IPR approaches using the proposed evaluation framework, we utilize a range of datasets, including Horse2Zebra~\cite{zhu_unpaired_2017} for unconditional translation models and CelebA~\cite{liu2015faceattributes} and CelebAHQ~\cite{CelebAMask-HQ} for conditional translation and generation models. One of the main objectives of our work is to benchmark copyright protection techniques on GANs and their associated input sets (images) and outputs.  We performed this analysis on the following list of GAN architectures --- \textit{(i)} \textbf{Generation Models:} StyleGANv2~\cite{Karras2019stylegan2}, StyleGAN3~\cite{Karras2021}, StyleSwin~\cite{zhang_styleswin_2022}, DDGAN~\cite{xiao2022tackling}; \textit{(ii)} {\textbf{{Translation Models:}}
\emph{(a) Unconditional:} CycleGAN~\cite{zhu_unpaired_2017}, AttentionGAN-v2 (AttGAN)~\cite{tang2021attentiongan}, CUT~\cite{park2020cut}; \emph{(b) Conditional:} StarGAN~\cite{choi2018stargan}, StarGAN-v2~\cite{choi2020starganv2}.


%This list represents a diverse range of GAN architectures, enabling us to generalize the findings. 
   
\begin{comment}
    \begin{itemize} [itemsep=0pt,partopsep=0pt, leftmargin=*]
       \item{\textbf{Generation Models:}} 
       
       \footnotesize{StyleGAN2~\cite{Karras2019stylegan2}, StyleGAN3\cite{Karras2021}, StyleSwin~\cite{zhang_styleswin_2022}, DDGAN~\cite{xiao2022tackling}}
       
       \normalsize
       \item{\textbf{{Translation Models:}}}
           \begin{itemize}
               \footnotesize 
               \item \emph{Unconditional:} 
                    CycleGAN~\cite{zhu_unpaired_2017}, AttentionGAN-v2~\cite{tang2021attentiongan}, CUT~\cite{park2020cut}
                    
               \item \emph{Conditional:}
                    StarGAN~\cite{choi2018stargan}, StarGAN-v2~\cite{choi2020starganv2} 
           \end{itemize}
    \end{itemize}

    \end{comment}

\begin{table}[tp]
\normalfont
\centering
\caption{Performance of imperceptible Adversarial Attack on Different GANs. We followed the experimental configurations utilized in~\cite{ruiz2020disrupting} and set the regularization coefficient ($\epsilon$) to $0.05$ during all the experiments to enable a fair comparison. FID: Fréchet Inception Distance ~\cite{heusel_gans_2017}.}
\label{tab:table2}
 \resizebox{0.8\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Dataset} & \textbf{L1} & \textbf{L2} & \textbf{FID} \\ \hline
CUT & \multirow{3}{*}{Horse2Zebra} & 0.5229 & 0.4278 & 37.2135 \\ \cline{1-1} \cline{3-5} 
AttentionGAN &  & 0.5590 & 0.6153 & 227.4701 \\ \cline{1-1} \cline{3-5} 
CycleGAN &  & 0.4512 & 0.3084 & 74.4684 \\ \hline
StarGANv2 & \multirow{2}{*}{CelabAHQ $256 \times 256$} & 0.3553 & 0.2901 & 20.1240 \\ \cline{1-1} \cline{3-5} 
StarGAN &  & 1.0459 & 1.3470 & 251.7915 \\ \hline
StarGAN & CelabA $128 \times 128$ & 0.9288 & 1.1167 & 166.7310 \\ \hline
\end{tabular}
}
\end{table}

\section{RQ1: Adversarial Attacks}

   % \subsection{Related Work}
    
        The copyright protection of input images is crucial in defending against the malicious modifications of images utilizing GANs. 
        %
        %One promising prevention mechanism mainly focuses on deploying adversarial perturbations on the input images, which prevents the creation of fake images.
        %
        One promising approach to counter fake image generation is to conduct adversarial perturbations on input images.
        
        \noindent \textbf{Related work.} There has been extensive discussion on the application of invisible adversarial perturbations to input images to disrupt the illicit use of generative models for fake image generation. Previous research has successfully implemented attacks on both unconditional and conditional image-translation models. Consequently, adversarial attacks have become a promising tool for ensuring that copyrighted images are not subjected to unauthorized modifications by GANs.
        Hence, we examine whether current adversarial watermarks can lead to satisfactory copyright protection against the illicit use of the input images. 
        %
        %We use an adversarial watermark produced using Projected Gradient Descent (PGD) trained using model outputs introduced by Ruiz \ et al’s work~\cite{ruiz2020disrupting} over input images, as its methodology is widely used in this domain
        %
        For this task, we employ adversarial watermarks generated via the utilization of the Projected Gradient Descent (PGD) algorithm trained with model outputs, as proposed in Ruiz \etal's work~\cite{ruiz2020disrupting}. This technique is widely established in the field of image watermarking and has been frequently employed as one of the benchmarked approaches in many recent attacks~\cite{huang_cmua-watermark_2022}. 


    \noindent \textbf{Experimental results and discussion.}
        We employ adversarial watermarks to generate adversarial examples to attack models that have not been assessed qualitatively or quantitatively. We measured the disturbance levels by calculating the L1 and L2 norms and FID~\cite{heusel_gans_2017} on the original output of the generative network and the output generated with adversarial input examples. 

        \begin{figure}[tp]
        \centering
        \includegraphics[width=0.94\linewidth]{figures/attack_h2z.png}
    \vspace{-2mm}
        \caption{Original and attacked output for unconditional translation GANs using the Horse2Zebra dataset.}
        \label{fig:attack_h2z}
        
        \includegraphics[width=0.97\linewidth]{figures/attack_starGAN_quality.png}
            \vspace{-2mm}
        \caption{Original and attacked output for StarGAN with different image qualities.}
        \vspace{1mm}
        \label{fig:attack_starGAN_quality}

        \includegraphics[width=0.94
        \linewidth]{figures/attack_starganv2.png}
        \vspace{-2mm}
        \caption{Original and attacked output for StarGANv2 using the CelebAHQ dataset.}
        \label{fig:attack_starganv2}
        %\vspace{-1mm}
        \end{figure}

        As shown in Table~\ref{tab:table2}, the adversarial attack causes significant disruptions in outputs. However, there is a complex interplay of factors  that has not been investigated before. Consequently, to enable a more in-depth understanding of the similarity between original and generated images, we use FID~\cite{heusel_gans_2017} together with L1 and L2 norm calculations. Through the experiments, adversarial attacks on images with high resolution deliver a better attack performance compared to the attacks on lower resolution images (Figure~\ref{fig:attack_starGAN_quality}). Additionally, the models with better performance (AttentionGAN and CycleGAN) will lead to more perceivable disruptions (Figure~\ref{fig:attack_h2z}).

        Figure~\ref{fig:attack_h2z} also illustrates that AttentionGAN suffered from the highest level of distortion, with the background of the images being significantly affected.
        %
        We conjecture that this is related to the attention mechanism of AttentionGAN
        and it is due to the presence of noise perturbing the model's ability to distinguish the objects of interest from the noisy background.
        
        %Lastly, the output of StarGANv2 has drawn our attention, given the very low FID.
        %
        The output generated by StarGANv2 gathered a significant interest due to the low FID value.
        Figure~\ref{fig:attack_starganv2} demonstrates that the distortion caused by the adversarial attack is less significant when the face is clearly visible. However, in both cases, the style of the source inputs is preserved. Hence, we conclude that the disruption is more targeted to the source characteristic preservation~\cite{choi2020starganv2}, while less effective in preventing style reconstruction. However, further investigation into the relevant loss is necessary to confirm this hypothesis. Furthermore, it shows the limitation of FID in assessing the distortions when the style of images resembles the source image.

\vspace{1mm}     
\begin{center}
\resizebox{0.99\columnwidth}{!}{

\begin{tcolorbox}
\textbf{RQ1. Takeaways:}
\begin{itemize} [leftmargin=*]

\item Recent developments in invisible adversarial attack-based watermarks on input images have facilitated the effective defence against the unauthorized use of images.

\item New metrics are needed to comprehensively assess the effectiveness of attacks, as current metrics exhibit limitations in accurately reporting distortions.

\item Adversarial attack watermarks can be less effective in applications focusing on style transfer compared to source characteristic preservation in image-to-image translation.
\end{itemize}
\end{tcolorbox}}
\end{center}

%\vspace{-5mm}
\section{RQ2: Watermarks}

    %\subsection{Related Work}
        Watermarking is crucial for the copyright protection of GAN models and training sets, given the cost of efforts and resources required to develop advanced GAN architectures, gather comprehensive datasets and train those models rigorously. The goal of watermarking GANs is to embed watermarks within a model and later retrieve them from the generated content. These watermarks can be later used as evidence in the event of unauthorized utilization of copyrighted models and datasets.
        
        \noindent \textbf{Related work.} There are two potential approaches that could be employed to achieve watermarking. One extensively researched method uses DNNs to create a watermark network. This approach involves training a watermark encoder to embed the watermark while training the model and a decoder to retrieve the watermark from the generated content. Yu \etal~\cite{yu_artificial_2021} approached this concept by training an encoder and a decoder first, and then superimposing the fingerprints on training sets for GAN training. The watermark will be attached to the GAN, so the fingerprints can be detected in the generated images. Fei \etal~\cite{fei_supervised_2022} extended this approach by integrating a decoder and an encoder into GAN training. For our analysis, we chose Yu \etal's approach~\cite{yu_artificial_2021} for our evaluation, as it provides IPR protection strategies on both training sets and models. 

        %Another method adopted by 
        Another branch proposed by Ong \etal~\cite{ong_protecting_2021} is to embed the ownership information into the model as a regularization term so that a trigger input into the suspect model will generate an output with the watermark. It essentially creates a backdoor for the copyright owners to employ a trigger image to verify whether a black-box API is using their models.
        
    \noindent \textbf{Experimental results and discussion.} We utilized Yu's method~\cite{yu_artificial_2021} as a representative CNN-based watermarking technique and the Ong~\cite{ong_protecting_2021} to create a model watermark backdoor. 
    %
    %When implementing~\cite{yu_artificial_2021}, we examine watermarks' performance are attached across epochs.
    %
    During the implementation of the watermarks, we evaluated their performance across multiple epochs to determine their optimal configurations.
    
        \begin{figure}[tp]
        \centering
        \includegraphics[width=1\linewidth]{figures/watermark.png}
        \vspace{-0.5cm}
        \caption{Bit accuracy of watermarking detection based on Yu \etal \cite{yu_artificial_2021}. (a) Bit accuracy of watermark retrieval on Horse2Zebra (CycleGAN, CUT, and AttentionGAN-v2) and CelebA (StarGAN and StyleGANv2). (b) Bit accuracy at different epochs of GAN training.}
    
        \label{fig: watermark_pp}
        \vspace{0.2cm}
        \includegraphics[width=0.9\linewidth]{figures/training_process.png}
        \vspace{-0.2cm}
        \caption{Original, trigger, and watermarked images of watermark injection method on unconditional translation models.}
        \label{fig: ipr_concat}
        \end{figure}
    
        

        Figure~\ref{fig: watermark_pp} shows the result of dataset watermarking on different models. We used the watermarked datasets to train models and conducted watermark detection. We find that it is challenging to watermark translation models, whereas the performance of the generation model in terms of bit accuracy is improved. We assume that it is due to the generation model's higher reflection of the original training sets in comparison to translation models. 
        Moreover, watermarks tend to be learned during the early training stages of generation GAN model training, regardless of the proportion of the watermarked images in the training set. This feature is important since it indicates that watermarking models can be a less demanding task than expected, given that current models can be easily watermarked by being retrained on the watermarked datasets.
        
        We also utilized the regularization approach on translation models and used trigger inputs to test the performance of all three unconditional translation models. Overall, the results generated from the regularization approach show that the trigger image can successfully produce a watermarked output without affecting the performance of the models. However, the CUT model fails to reproduce the watermark image. We assume that it is due to its patch-wise training mechanism. This issue raises the importance of an in-depth investigation into the limitations of current watermarking methods to achieve IPR protection.

%\vspace{2mm}
        %\resizebox{1\columnwidth}{!}{
        \begin{tcolorbox}%[h]
        \textbf{RQ2. Takeaways:}
        \begin{itemize} [leftmargin=*]
        
        \item Recent developments in watermarks have led to improved protection of GAN models. However,  IPR protection for training sets remains a prevalent concern.
        
        \item Watermark detection capabilities of generation models trained on watermarked training sets are better than those of translation models.
        
        \end{itemize}
        \end{tcolorbox}%}

\section{RQ3: Attribution}

   % \subsection{Related Work}

        The attribution of GAN architectures 
        %
        %is the final resort for copyright protection.
        %
        also plays a crucial role in copyright protection.
        %
        It allows IPR owners to differentiate between real images 
 and fake images, and their source models by attributing fake images based on the architectural characteristics of the underlying model. There are two broad categories of attribution methods: spectral and contrastive.
    
       \noindent \textbf{Related work.} The utilization of spectral analysis was proven to be effective in DeepFake detection and model attribution. Consequently, detectors based on discrete cosine transformation (DCT)~\cite{frank_leveraging_2020} were proposed and validated. Contrastive attribution is another approach used to determine the modifications a GAN model has made to a generated image. This is achieved by conducting analyses on local feature differences between original and generated images.  Yang~\etal~\cite{yang2022deepfake} discussed patch-wise contrastive learning to generate predictions based on the model fingerprints.
        
   % \subsection{Experiment Results and Discussion}
        \begin{table}[tp]
    \centering
    \caption{The confusion matrix for attribution  of ``Translation GANs'' using Horse2Zebra with spectral analysis (Rows: True Values, Columns: Predicted Values).}
    \label{tab:generation}
    \resizebox{0.6\columnwidth}{!}{
    \begin{tabular}{|l|l|l|l|l|}
    \hline
     & \textit{\textbf{AttGAN}} & \textit{\textbf{CUT}} & \textit{\textbf{CycleGAN}} & \textit{\textbf{Real}} \\ \hline
    \textit{\textbf{AttGAN}} & \textbf{209} & 3 & 0 & 25 \\ \hline
    \textit{\textbf{CUT}} & 5 & \textbf{221} & 7 & 4 \\ \hline
    \textit{\textbf{CycleGAN}} & 0 & 2 & \textbf{233} & 2 \\ \hline
    \textit{\textbf{Real}} & 19 & 2 & 0 & \textbf{216} \\ \hline
    \textbf{Precision} & \textbf{0.8970} & \textbf{0.9693} & \textbf{0.9708} & \textbf{0.8745} \\ \hline
    \end{tabular}
    }
    \vspace{2mm}
    \caption{The confusion matrix for attribution for ``Generation GANs'' on 5000 images generated using CelebAHQ with spectral analysis (Rows: true values, Columns: predicted values).}
    \label{tab:translation}
    \resizebox{0.9\columnwidth}{!}{
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    \textbf{} & \textit{\textbf{DD-GAN}} & \textit{\textbf{StarGAN}} & \textit{\textbf{StarGANv2}} & \textit{\textbf{StyleGAv2}} & \textit{\textbf{StyleSwin}} & \textit{\textbf{Real}} \\ \hline
    \textit{\textbf{DD-GAN}} & \textbf{934} & 13 & 0 & 33 & 0 & 20 \\ \hline
    \textit{\textbf{StarGAN}} & 4 & \textbf{996} & 0 & 0 & 0 & 0 \\ \hline
    \textit{\textbf{StarGANv2}} & 0 & 1 & \textbf{992} & 0 & 3 & 4 \\ \hline
    \textit{\textbf{StyleGANv2}} & 3 & 1 & 0 & \textbf{992} & 2 & 2 \\ \hline
    \textit{\textbf{StyleSwin}} & 0 & 0 & 12 & 10 & \textbf{962} & 16 \\ \hline
    \textit{\textbf{Real}} & 36 & 6 & 43 & 61 & 77 & \textbf{777} \\ \hline
    \textbf{Precision} & \textbf{0.956} & \textbf{0.9794} & \textbf{0.9475} & \textbf{0.9051} & \textbf{0.9215} & \textbf{0.9487} \\ \hline
    \end{tabular}
    }
    \end{table}
    
    \noindent \textbf{Experimental results and discussion.}
    %We reported the confusion matrix for the attribution spectral analysis based on~\cite{frank_leveraging_2020}.
    %
    Following the approach and extending the scope outlined in Frank \etal's %\ et al's 
    %
    work~\cite{frank_leveraging_2020}, we generated the confusion matrix for attribution spectral analysis. %For each translation model, we produce 237 fake images from Horse2Zebra, and we generate 5000 fake images from each generative model using CelebAHQ to examine the attribution performance.
    %
    For this analysis, we generated 237 fake images from the Horse2Zebra dataset for each translation model, and generated 5,000 fake images from the CelebAHQ for each generative model.

    
    
    %
    The results indicate that the accuracy generated by spectral analysis is high when trained on different datasets. The results are as expected, given that discrete cosine transformation produces distinct fingerprints for each model (Figure~\ref{fig: spectrum}). 
    %
    %We also observed that the likelihood of the images generated by more capable models being confused with real images is high (Table~\ref{tab:translation}).
    %
    The empirical results further confirm that the models with more capabilities exhibit better performance in generating fake images that are more realistic (refer to Table~\ref{tab:translation}).
    
    
    We are conducting an ongoing experiment on the contrastive method and attribution evasion proposed by Dong \etal~\cite {dong_think_2022}. However, the evasion pipeline requires modifications to the network, which contradicts the requirement of the watermark to remain  intact during IPR protection. Hence,  the authors are not motivated to incorporate spectral loss into their model.

    %\resizebox{0.9\columnwidth}{!}{
    \begin{tcolorbox}
    \textbf{RQ3. Takeaways:}
    \begin{itemize} [leftmargin=*]
    
    \item 
    %
    %We can conclude that 
    %
    Classifiers that utilize spectral domain fingerprints can achieve IPR protection of the model through the attribution of the output images.
    \item Spectral features can be harder to be traced when the models generate more realistic outputs.
    \end{itemize}
    \end{tcolorbox}%}
%\vspace{-2mm}
\section{Conclusion}

This study conducted a preliminary evaluation of the state-of-the-art IPR protection methods for GANs and their training sets (images).     
    %
    %Overall, our experiments indicate 
    %
    %We found that with the recent development of adversarial attacks, watermarks, and attribution methods, IPR protection has become a promising field where most of the techniques are effective for copyright protection.
    %
    The experimental results demonstrate that the recently developed IPR protection techniques such as adversarial attacks, watermarking, and attribution are largely effective for copyright protection and accountability tracing of GAN models. 
    %
    %However, we also identified that IPR protection for training data, which is one of the most controversial aspects in the public discussions on generative models, is still largely empty.
    %
    However, our findings reveal that the current techniques for protecting the IPR of the training sets (the original copyrighted images) are inadequate.
    %
    %Further studies into this topic will be valuable for
    %
    We also highlight the importance of further analysis of recent generative AI models that are capable of generating realistic images utilizing the unauthorized styles and works of IPR owners.     Furthermore, this paper underscores the importance of developing a unified pipeline to assess the performance of adversarial attack watermarks and measure the IPR protection levels and accountability tracing of generative AI. Our ongoing research aims to integrate 
    %
    %develop metrics integrating results from different domains of IPR protection in GANs.
    %
    findings from various domains of IPR protection across a plethora of generative AI models to develop a comprehensive set of metrics. The findings derived from this preliminary study will inform lawmakers and practitioners of ethical generative AI adoption. 
    
    \begin{figure}[tp]
    %\vspace{-8mm}
    \includegraphics[width=1\linewidth]{figures/spectrum.png}
    \vspace{-4mm}
    \caption{Spectral Imprints Produced by GANs using DCT.}
    \label{fig: spectrum}
       \vspace{-3mm}
    \end{figure}
    

    



    
%\vspace{-1.35mm}


\input{main.bbl}


\end{document}

