
@inproceedings{chen_copy_2022,
	title = {Copy, {Right}? {A} {Testing} {Framework} for {Copyright} {Protection} of {Deep} {Learning} {Models}},
	shorttitle = {Copy, {Right}?},
	abstract = {Deep learning models, especially those large-scale and high-performance ones, can be very costly to train, demanding a considerable amount of data and computational resources. As a result, deep learning models have become one of the most valuable assets in modern artificial intelligence. Unauthorized duplication or reproduction of deep learning models can lead to copyright infringement and cause huge economic losses to model owners, calling for effective copyright protection techniques. Existing protection techniques are mostly based on watermarking, which embeds an owner-specified watermark into the model. While being able to provide exact ownership verification, these techniques are 1) invasive, i.e., they need to tamper with the training process, which may affect the model utility or introduce new security risks into the model; 2) prone to adaptive attacks that attempt to remove/replace the watermark or adversarially block the retrieval of the watermark; and 3) not robust to the emerging model extraction attacks. Latest fingerprinting work on deep learning models, though being non-invasive, also falls short when facing the diverse and ever-growing attack scenarios.In this paper, we propose a novel testing framework for deep learning copyright protection: DEEPJUDGE. DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Advantages of DEEPJUDGE include: 1) non-invasive, as it works directly on the model and does not tamper with the training process; 2) efficient, as it only needs a small set of seed test cases and a quick scan of the two models; 3) flexible, i.e., it can easily incorporate new testing metrics or test case generation methods to obtain more confident and robust judgement; and 4) fairly robust to model extraction attacks and adaptive attacks. We verify the effectiveness of DEEPJUDGE under three typical copyright infringement scenarios, including model finetuning, pruning and extraction, via extensive experiments on both image classification and speech recognition datasets with a variety of model architectures.},
	journal = {SP},
	author = {Chen, Jialuo and Wang, Jingyi and Peng, Tinglan and Sun, Youcheng and Cheng, Peng and Ji, Shouling and Ma, Xingjun and Li, Bo and Song, Dawn},
	year = {2022},
	annote = {[TLDR] A novel testing framework for deep learning copyright protection: DEEPJUDGE quantitatively tests the similarities between two deep learning models: a victim model and a suspect model, which leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspects model is a copy of the victim model.},
	file = {Submitted Version:/Users/humphreyyang/Zotero/storage/47FPYCMA/Chen et al. - 2022 - Copy, Right A Testing Framework for Copyright Pro.pdf:application/pdf},
}

@article{karnouskos_artificial_2020,
	title = {Artificial {Intelligence} in {Digital} {Media}: {The} {Era} of {Deepfakes}},
	volume = {1},
	issn = {2637-6415},
	shorttitle = {Artificial {Intelligence} in {Digital} {Media}},
	abstract = {The recent practical advances realized by artificial intelligence, have also given rise to the phenomenon of deepfakes, which can be considered as a form of fake news. Deepfakes is the phenomenon of creation of realistic digital products, and a plethora of videos has emerged over the last two years in social media. Especially, the low technical expertise and equipment required to create deepfakes, means that such content can be easily produced by anyone and distributed online. The societal implications are significant and far-reaching. This article investigates the deepfakes via multiangled perspectives that include media and society, media production, media representations, media audiences, gender, law, and regulation, as well as politics. Some key implications of these viewpoints are identified and critically discussed. The results indicate that as a society, we are not ready to deal with the emergence of deepfakes at any level. That we have not witnessed any severe impacts so far is due to their early stage of development, which shows imperfections to address the issue, a combination of technology, education, training, and governance is urgently needed.},
	number = {3},
	journal = {IEEE Transactions on Technology and Society},
	author = {Karnouskos, Stamatis},
	year = {2020},
	keywords = {Artificial intelligence (AI), Deep learning, deepfakes, digital media, Games, Information integrity, Media, Social networking (online), society, Videos},
	file = {IEEE Xplore Abstract Record:/Users/humphreyyang/Zotero/storage/EVWRYVUH/9123958.html:text/html},
}

@article{franceschelli_copyright_2022,
	title = {Copyright in generative deep learning},
	volume = {4},
	journal = {Data \& Policy},
	author = {Franceschelli, Giorgio and Musolesi, Mirco},
	year = {2022},
	note = {ISBN: 2632-3249
Publisher: Cambridge University Press},
}


@article{tolosana_deepfakes_2020,
	title = {Deepfakes and beyond: {A} {Survey} of face manipulation and fake detection},
	issn = {15662535},
	shorttitle = {Deepfakes and beyond},
	language = {en},
	journal = {Information Fusion},
	author = {Tolosana, Ruben and Vera-Rodriguez, Ruben and Fierrez, Julian and Morales, Aythami and Ortega-Garcia, Javier},
	year = {2020},
	file = {Full Text PDF:/Users/humphreyyang/Zotero/storage/DQ23BQ2V/Tolosana et al. - 2020 - Deepfakes and beyond A Survey of face manipulatio.pdf:application/pdf}
}



@inproceedings{madry_towards_2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	language = {en},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2018},
	file = {Full Text PDF:/Users/humphreyyang/Zotero/storage/WH9TEMTI/Madry et al. - 2023 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf},
}


@article{wang_deceiving_2020,
	title = {Deceiving {Image}-to-{Image} {Translation} {Networks} for {Autonomous} {Driving} {With} {Adversarial} {Perturbations}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	abstract = {Deep neural networks (DNNs) have achieved impressive performance on handling computer vision problems. However, it has been found that DNNs are vulnerable to adversarial examples. For such reason, adversarial perturbations have been recently studied in several respects. However, most previous works have focused on image classification tasks, and it has never been studied regarding adversarial perturbations on Image-to-image (Im2Im) translation tasks, showing great success in handling paired and/or unpaired mapping problems in the field of autonomous driving and robotics. This letter examines different types of adversarial perturbations that can fool Im2Im frameworks for autonomous driving purposes. We propose both quasi-physical and digital adversarial perturbations that can make Im2Im models yield unexpected results. We then empirically analyze these perturbations and show that they generalize well under both paired for image synthesis and unpaired settings for style transfer. We also validate that there exist some perturbation thresholds over which the Im2Im mapping is disrupted or impossible. The existence of these perturbations reveals that there exist crucial weaknesses in Im2Im models. Lastly, we show that our methods illustrate how perturbations affect the quality of outputs, pioneering the improvement of the robustness of current SOTA networks for autonomous driving.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wang, Lin and Cho, Wonjune and Yoon, Kuk-Jin},
	year = {2020},
	annote = {[TLDR] This letter proposes both quasi-physical and digital adversarial perturbations that can make Im2Im models yield unexpected results, and empirically analyzes these perturgations to show that they generalize well under both paired for image synthesis and unpaired settings for style transfer.},
	file = {Submitted Version:/Users/humphreyyang/Zotero/storage/PEVPTD6D/Wang et al. - 2020 - Deceiving Image-to-Image Translation Networks for .pdf:application/pdf},
}

@inproceedings{bashkirova_adversarial_2019,
	title = {Adversarial {Self}-{Defense} for {Cycle}-{Consistent} {GANs}},
	abstract = {The goal of unsupervised image-to-image translation is to map images from one domain to another without the ground truth correspondence between the two domains. State-of-art methods learn the correspondence using large numbers of unpaired examples from both domains and are based on generative adversarial networks. In order to preserve the semantics of the input image, the adversarial objective is usually combined with a cycle-consistency loss that penalizes incorrect reconstruction of the input image from the translated one. However, if the target mapping is many-to-one, e.g. aerial photos to maps, such a restriction forces the generator to hide information in low-amplitude structured noise that is undetectable by human eye or by the discriminator. In this paper, we show how such self-attacking behavior of unsupervised translation methods affects their performance and provide two defense techniques. We perform a quantitative evaluation of the proposed techniques and show that making the translation model more robust to the self-adversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to low-amplitude perturbations.},
	author = {Bashkirova, D. and Usman, Ben and Saenko, Kate},
	year = {2019},
	annote = {[TLDR] Making the translation model more robust to the self-adversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to low-amplitude perturbations.},
	file = {Full Text PDF:/Users/humphreyyang/Zotero/storage/6WCK95DV/Bashkirova et al. - 2019 - Adversarial Self-Defense for Cycle-Consistent GANs.pdf:application/pdf},
}

@inproceedings{ruiz2020disrupting,
  title={Disrupting deepfakes: Adversarial attacks against conditional image translation networks and facial manipulation systems},
  author={Ruiz, Nataniel and Bargal, Sarah Adel and Sclaroff, Stan},
  journal={CVPR},
  year={2020},
}

@article{yeh_disrupting_2020,
	title = {Disrupting {Image}-{Translation}-{Based} {DeepFake} {Algorithms} with {Adversarial} {Attacks}},
	abstract = {DeepNude, a deep generative software based on im-age-to-image translation algorithm, excelling in undressing photos of humans and producing realistic nude images. Although the software was later purged from the Internet, image translation algorithms such as CycleGAN, pix2pix, or pix2pixHD can easily be applied by anyone to recreate a new version of DeepNude. This work addresses the issue by introducing a novel aspect of image translating algorithms, namely the possibility of adversarially attacking these algorithms. We modify the input images by the adversarial loss, and thereby the edited images would not be counterfeited easily by these algorithms. The proposed technique can provide a guideline to future research on defending personal images from malicious use of image translation algorithms.},
	journal = {WACVW},
	author = {Yeh, Chin-Yuan and Chen, Hsi-Wen and Tsai, Shang-Lun and Wang, Shang-De},
	year = {2020},
	annote = {[TLDR] This work addresses the issue of image translation algorithms by introducing a novel aspect of image translating algorithms, namely the possibility of adversarially attacking these algorithms, and modifying the input images by the adversarial loss, and thereby the edited images would not be counterfeited easily by these algorithms.}
 }

@article{wang_deceiving_2020-1,
	title = {Deceiving {Image}-to-{Image} {Translation} {Networks} for {Autonomous} {Driving} {With} {Adversarial} {Perturbations}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	abstract = {Deep neural networks (DNNs) have achieved impressive performance on handling computer vision problems. However, it has been found that DNNs are vulnerable to adversarial examples. For such reason, adversarial perturbations have been recently studied in several respects. However, most previous works have focused on image classification tasks, and it has never been studied regarding adversarial perturbations on Image-to-image (Im2Im) translation tasks, showing great success in handling paired and/or unpaired mapping problems in the field of autonomous driving and robotics. This letter examines different types of adversarial perturbations that can fool Im2Im frameworks for autonomous driving purposes. We propose both quasi-physical and digital adversarial perturbations that can make Im2Im models yield unexpected results. We then empirically analyze these perturbations and show that they generalize well under both paired for image synthesis and unpaired settings for style transfer. We also validate that there exist some perturbation thresholds over which the Im2Im mapping is disrupted or impossible. The existence of these perturbations reveals that there exist crucial weaknesses in Im2Im models. Lastly, we show that our methods illustrate how perturbations affect the quality of outputs, pioneering the improvement of the robustness of current SOTA networks for autonomous driving.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wang, Lin and Cho, Wonjune and Yoon, Kuk-Jin},
	year = {2020},
	annote = {[TLDR] This letter proposes both quasi-physical and digital adversarial perturbations that can make Im2Im models yield unexpected results, and empirically analyzes these perturgations to show that they generalize well under both paired for image synthesis and unpaired settings for style transfer.},
	file = {Submitted Version:/Users/humphreyyang/Zotero/storage/7BVSD3G6/Wang et al. - 2020 - Deceiving Image-to-Image Translation Networks for .pdf:application/pdf},
}


@inproceedings{huang_cmua-watermark_2022,
	title = {{CMUA}-{Watermark}: {A} {Cross}-{Model} {Universal} {Adversarial} {Watermark} for {Combating} {Deepfakes}},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{CMUA}-{Watermark}},
	abstract = {Malicious applications of deepfakes (i.e., technologies generating target facial attributes or entire faces from facial images) have posed a huge threat to individuals' reputation and security. To mitigate these threats, recent studies have proposed adversarial watermarks to combat deepfake models, leading them to generate distorted outputs. Despite achieving impressive results, these adversarial watermarks have low image-level and model-level transferability, meaning that they can protect only one facial image from one specific deepfake model. To address these issues, we propose a novel solution that can generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a large number of facial images from multiple deepfake models. Specifically, we begin by proposing a cross-model universal attack pipeline that attacks multiple deepfake models iteratively. Then, we design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models. Moreover, we address the key problem in cross-model optimization with a heuristic approach to automatically find the suitable attack step sizes for different models, further weakening the model-level conflict. Finally, we introduce a more reasonable and comprehensive evaluation method to fully test the proposed method and compare it with existing ones. Extensive experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by multiple deepfake models while achieving a better performance than existing methods. Our code is available at https://github.com/VDIGPKU/CMUA-Watermark.},
	journal = {AAAI},
	author = {Huang, Hao and Wang, Yongtao and Chen, Zhaoyu and Zhang, Yuze and Li, Yuheng and Tang, Zhi and Chu, Wei and Chen, Jingdong and Lin, Weisi and Ma, Kai-Kuang},
	year = {2022},
	annote = {[TLDR] A cross-model universal attack pipeline that attacks multiple deepfake models iteratively and design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models is proposed.},
	file = {Full Text PDF:/Users/humphreyyang/Zotero/storage/HRGW2MXJ/Huang et al. - 2022 - CMUA-Watermark A Cross-Model Universal Adversaria.pdf:application/pdf},
}

@article{wang_generative_2021,
	title = {Generative {Adversarial} {Networks} in {Computer} {Vision}: {A} {Survey} and {Taxonomy}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Generative {Adversarial} {Networks} in {Computer} {Vision}},
	abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN\_Review.},
	number = {2},
	journal = {ACM Computing Surveys},
	author = {Wang, Zhengwei and She, Qi and Ward, Tomás E.},
	year = {2021},
	keywords = {architecture-variants, computer vision, Generative adversarial networks, loss-variants, stabilizing training},
	file = {Full Text PDF:/Users/humphreyyang/Zotero/storage/3HTTTM6U/Wang et al. - 2021 - Generative Adversarial Networks in Computer Vision.pdf:application/pdf},
}

@inproceedings{CelebAMask-HQ,
  title={MaskGAN: Towards Diverse and Interactive Facial Image Manipulation},
  author={Lee, Cheng-Han and Liu, Ziwei and Wu, Lingyun and Luo, Ping},
  journal = {CVPR},
  year={2020}
}


@inproceedings{zhu_unpaired_2017,
	title = {Unpaired image-to-image translation using cycle-consistent adversarial networks},
	journal = {ICCV},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
}

@inproceedings{choi2020starganv2,
  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},
  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},
  journal = {CVPR},
  year={2020}
}

@inproceedings{parmar_aliased_2022,
	title = {On aliased resizing and surprising subtleties in gan evaluation},
	journal = {CVPR},
	author = {Parmar, Gaurav and Zhang, Richard and Zhu, Jun-Yan},
	year = {2022},
}

@inproceedings{Karras2019stylegan2,
  title     = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  journal = {CVPR},
  year      = {2020}
}

@inproceedings{Karras2021,
  author = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title = {Alias-Free Generative Adversarial Networks},
  journal = {NeurIPS},
  year = {2021}
}

@inproceedings{zhang_styleswin_2022,
	title = {Styleswin: {Transformer}-based gan for high-resolution image generation},
	journal = {CVPR},
	author = {Zhang, Bowen and Gu, Shuyang and Zhang, Bo and Bao, Jianmin and Chen, Dong and Wen, Fang and Wang, Yong and Guo, Baining},
	year = {2022},
}

@inproceedings{xiao2022tackling,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion GANs},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
journal = {ICLR},
year={2022}
}

@article{tang2021attentiongan,
  title={AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks},
  author={Tang, Hao and Liu, Hong and Xu, Dan and Torr, Philip HS and Sebe, Nicu},
  journal = {IEEE TNNLS},
  year={2021} 
}

@inproceedings{park2020cut,
  title={Contrastive Learning for Unpaired Image-to-Image Translation},
  author={Taesung Park and Alexei A. Efros and Richard Zhang and Jun-Yan Zhu},
  journal = {ECCV},
  year={2020}
}

@inproceedings{choi2018stargan,
author={Yunjey Choi and Minje Choi and Munyoung Kim and Jung-Woo Ha and Sunghun Kim and Jaegul Choo},
title={StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
journal = {CVPR},
year={2018}
}


@inproceedings{fei_supervised_2022,
	title = {Supervised gan watermarking for intellectual property protection},
	isbn = {9798350309676},
	journal = {IEEE WIFS},
	publisher = {IEEE},
	author = {Fei, Jianwei and Xia, Zhihua and Tondi, Benedetta and Barni, Mauro},
	year = {2022}
}

@inproceedings{yu_artificial_2021,
	title = {Artificial {Fingerprinting} for {Generative} {Models}: {Rooting} {Deepfake} {Attribution} in {Training} {Data}},
	shorttitle = {Artificial {Fingerprinting} for {Generative} {Models}},
        journal =  {ICCV},
	language = {en},
	author = {Yu, Ning and Skripniuk, Vladislav and Abdelnabi, Sahar and Fritz, Mario},
	year = {2021},
	file = {Full Text PDF:/Users/humphreyyang/Zotero/storage/3TY3FI85/Yu et al. - 2021 - Artificial Fingerprinting for Generative Models R.pdf:application/pdf},
}

@inproceedings{yu_responsible_2022,
	title = {Responsible {Disclosure} of {Generative} {Models} {Using} {Scalable} {Fingerprinting}},
	journal =  {ICLR},
	author = {Yu, Ning and Skripniuk, Vladislav and Chen, Dingfan and Davis, Larry S. and Fritz, Mario},
	year = {2022},
}


@inproceedings{ong_protecting_2021,
	title = {Protecting intellectual property of generative adversarial networks from ambiguity attacks},
	journal = {CVPR},
	author = {Ong, Ding Sheng and Chan, Chee Seng and Ng, Kam Woh and Fan, Lixin and Yang, Qiang},
	year = {2021},
}

@inproceedings{yang2022deepfake,
  title={Deepfake Network Architecture Attribution},
  author={Yang, Tianyun and Huang, Ziyao and Cao, Juan and Li, Lei and Li, Xirong},
  journal = {AAAI},
  year={2022}
}


@inproceedings{dzanic_fourier_2020,
	title = {Fourier spectrum discrepancies in deep network generated images},
	journal = {NeurIPS},
	author = {Dzanic, Tarik and Shah, Karan and Witherden, Freddie D.},
	year = {2020},
}

@inproceedings{frank_leveraging_2020,
	title = {Leveraging frequency analysis for deep fake image recognition},
	isbn = {2640-3498},
	journal = {ICML},
	author = {Frank, Joel and Eisenhofer, Thorsten and Schönherr, Lea and Fischer, Asja and Kolossa, Dorothea and Holz, Thorsten},
	year = {2020},
}


@inproceedings{heusel_gans_2017,
	title = {Gans trained by a two time-scale update rule converge to a local nash equilibrium},
	journal = {NeurIPS},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	year = {2017},
}


@inproceedings{dong_think_2022,
	title = {Think {Twice} {Before} {Detecting} {GAN}-generated {Fake} {Images} from their {Spectral} {Domain} {Imprints}},
	journal = {CVPR},
	author = {Dong, Chengdong and Kumar, Ajay and Liu, Eryun},
	year = {2022},
}

@inproceedings{liu2015faceattributes,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  journal = {ICCV},
  year = {2015} 
}