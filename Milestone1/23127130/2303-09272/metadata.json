{
    "arxiv_id": "2303.09272",
    "paper_title": "Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution",
    "authors": [
        "Haonan Zhong",
        "Jiamin Chang",
        "Ziyue Yang",
        "Tingmin Wu",
        "Pathum Chamikara Mahawaga Arachchige",
        "Chehara Pathmabandu",
        "Minhui Xue"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.CR",
        "cs.MM"
    ],
    "abstract": "Generative AI (e.g., Generative Adversarial Networks - GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance tracing on training sets.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09272v1"
    ],
    "publication_venue": null,
    "doi": "10.1145/3543873.3587321"
}