
\begin{abstract}

% By design, convolutional architectures allot equal computational budgets to each pixel in the input image. However, this may be suboptimal in applications with strict computational constraints, such as mobile computing, autonomous navigation, and VR/AR.

Many perception systems in mobile computing, autonomous navigation, and AR/VR face strict compute constraints that are particularly challenging for high-resolution input images.
Previous works propose nonuniform downsamplers that "learn to zoom" on salient image regions, reducing compute while retaining task-relevant image information.
However, for tasks with spatial labels (such as 2D/3D object detection and semantic segmentation), such distortions may harm performance.
In this work (LZU), we "learn to zoom" in on the input image, compute spatial features, and then "unzoom" to revert any deformations.
To enable efficient and differentiable unzooming, we approximate the zooming warp with a piecewise bilinear mapping that is invertible.
LZU can be applied to any task with 2D spatial input and any model with 2D spatial features, and we demonstrate this versatility by evaluating on a variety of tasks and datasets: \emph{object detection} on Argoverse-HD, \emph{semantic segmentation} on Cityscapes, and \emph{monocular 3D object detection} on nuScenes.
Interestingly, we observe boosts in performance even when high-resolution sensor data is unavailable, implying that LZU can be used to "learn to upsample" as well.
Code and additional visuals are available at \url{https://tchittesh.github.io/lzu/}.

%% Previous version of the abstract

% Many perceptual systems in spaces such as mobile computing, autonomous navigation, and VR/AR face strict computational constraints that limit the maximum processable image resolution.
% Previous works target the case when higher-resolution sensor data is available and propose intelligent nonuniform downsamplers to combat information loss when downsampling. These methods achieve superior performance relative to uniform downsampling by "learning to zoom" at salient image regions and thereby retaining more task-relevant information.
% However, when labels are spatially distributed (e.g. detection, segmentation), task-specific adjustments to the model and loss are required in order to account for spatial distortions introduced by the downsampler.
% In this work, we generalize these methods by introducing an "unzoom" operation to efficiently revert these spatial deformations.
% We can apply our zoom-compute-unzoom framework whenever the base model contains a spatial feature map, making our method highly flexible and task-agnostic.
% We demonstrate the versatility of LZU on a variety of tasks: \emph{object detection} with RetinaNet and Faster-RCNN on Argoverse-HD and COCO, \emph{semantic segmentation} with PSPNet on Cityscapes, and \emph{monocular 3D detection} with FCOS3D on NuScenes.
% In all settings, we observe increased performance over uniform downsampling with minimal additional latency.
% Furthermore, we observe boosts in performance even when high-resolution sensor data is unavailable, demonstrating the broad applicability of our framework as a spatial attention mechanism.

%% Previous version of the abstract

% FOVEA introduces an intelligent downsampling technique for video object detection that achieves superior accuracy-latency tradeoffs relative to uniform downsampling by retaining more information (sampling more pixels) in "salient‚Äù regions. This is crucial in safety-critical applications like AV perception, in which high-resolution camera input is often available, but must be downsampled due to latency constraints. A key limitation of FOVEA is its irreversible spatial deformation, which necessitates adjustments to the loss and ultimately makes pixel-wise tasks like segmentation infeasible. 

\end{abstract}
