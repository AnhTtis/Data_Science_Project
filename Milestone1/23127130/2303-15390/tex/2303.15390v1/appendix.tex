\appendix

\section{Appendix}

\subsection{Bilinear Transformations}
\label{sec:bilinear-maps}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/bilinear-map.png}
\caption{
Geometric interpretation of bilinear transformations. Suppose we have such a transformation from the unit square to an arbitrary quadrilateral. Given coordinates $(u, v)$ in the unit square, if we draw lines in the quadrilateral such that
$u = \frac{|\bx_b \bx_{bl}|}{|\bx_{br} \bx_{bl}|} = \frac{|\bx_t \bx_{tl}|}{|\bx_{tr} \bx_{tl}|}$ and
$v = \frac{|\bx_l \bx_{bl}|}{|\bx_{tl} \bx_{bl}|} = \frac{|\bx_r \bx_{br}|}{|\bx_{tr} \bx_{br}|}$,
they will intersect at $\mathrm{BilinearTransformation}(u, v)$.
}
\label{fig:bilinear-map}
\end{figure}

Our construction in Section~\ref{sec:warp-inv} assumes prior knowledge of bilinear transformations. Bilinear transformations have actually been widely studied in the context of computer graphics~\cite{wolberg1990digital}. Here, for unfamiliar readers, we outline its definition and inverse formulation.

For simplicity, consider a bilinear transformation that maps the unit square to the quadrilateral with corners $\bx_{bl}, \bx_{br}, \bx_{tl}, \bx_{tr}$. True to its name, the transformation is defined \textit{within} the square via bilinear interpolation:
\begin{align}
    \mathrm{BilinearTransformation}(u, v) &= \bx_{bl} + (\bx_{br} - \bx_{bl})u \nonumber \\
    + (\bx_{tl} - \bx_{bl})v + (\bx_{tr} -& \bx_{br} - \bx_{tl} + \bx_{bl}) uv.
\end{align}
Interestingly, this transformation also has a geometric interpretation, shown in Figure~\ref{fig:bilinear-map}.

Now, consider the inverse of this mapping. Given a point $(x, y)$ in the quadrilateral, we want to find the point $(u, v)$ in the unit square that maps to it. A full derivation is given in~\cite{wolberg1990digital}, but if we define the following scalars
\begin{align}
    (a_0, b_0) &= \bx_{bl} \\
    (a_1, b_1) &= \bx_{br} - \bx_{bl} \\
    (a_2, b_2) &= \bx_{tl} - \bx_{bl} \\
    (a_3, b_3) &= \bx_{tr} - \bx_{br} - \bx_{tl} + \bx_{bl} \\
    c_0 &= a_1 (b_0 - y) + b_1 (x - a_0) \\
    c_1 &= a_3 (b_0 - y) + b_3 (x - a_0) + a_2 b_1 - a_2 b_1 \\
    c_2 &= a_3 b_2 - a_2 b_3,
\end{align}
then the solution $(u, v)$ must satisfy
\begin{equation}
\label{eq:bilinear-sol-1}
    c_2v^2 + c_1v + c_0 = 0
\end{equation}
and
\begin{equation}
\label{eq:bilinear-sol-2}
    u = \frac{x - a_0 - a_2 v}{a_1 + a_3 v}.
\end{equation}

Applying the quadratic formula on Equation~\ref{eq:bilinear-sol-1}, we can solve for $v$. Then, we can substitute into Equation~\ref{eq:bilinear-sol-2} to find $u$. Given a point $(x, y)$ in the quadrilateral, this will produce exactly one pair of solutions $(u, v)$ in the unit square (there may be extraneous solutions with $u$ or $v$ negative or greater than $1$).

Although these results assume a mapping from the unit square, they extend naturally to our use case. Recall from Section~\ref{sec:warp-inv} that $\mathrm{\widetilde\calT}_{ij}$ is a bilinear transformation from rectangle $R_{ij}$ to quadrilateral $\calT[R_{ij}]$. We can apply all previous results, simply by normalizing the coordinates within $R_{ij}$.

\subsection{Efficient Inversion of Nonseparable Warps}
\label{sec:nonsep-inversion}

\begin{figure*}[t]
\centering
\includegraphics[width=1.8\columnwidth]{figures/unzoom.png}
\caption{
% \deva{Given a saliency map rendered as grayscale image,  LZU computes downsampled grid locations represented by the $11 \times 11$ red corners, thus resamping a high-res input into a small $11 \times 11$ canvas. After processing the downsampled spatial canvas with a task-specific network, LZU upsamples spatial features back to the original resolution.  To do so, for each output grid location (represented as a green point), LZU first computes {\em which} quadrilateral it falls into. Given this quadraliteral with 4 corners, LZU then interpolates the feature value at that (green) output grid location by taking a (bi)linear combination of corner features~\cite{wolberg1990digital}.}
Unzooming in the nonseparable case. $\calT_{\mathrm{LZ}}$ is approximated as $\widetilde\calT_{\mathrm{LZ}}$, which is a $(h-1) \times (w-1)$ tiling of bilinear transformations (left).
We wish to compute $\widetilde\calT_{\mathrm{LZ}}^{-1}(\bx)$ at each green point $\bx \in \mathrm{Grid}(H'', W'')$, where $H'' \times W''$ is the desired output size (middle).
For the $ij$-th tile, we consider the set of all candidate green point $\bx$ within the enclosing blue box (right) and apply the corresponding inverse bilinear transformation. We set $\widetilde\calT_{\mathrm{LZ}}^{-1}(\bx) = \mathrm{BilinearTransformation}_{ij}^{-1}(\bx)$ only if it falls in the $ij$-th grid rectangle $R(i,j)$.
}
\label{fig:unzoom-nonsep}
\end{figure*}

In Section~\ref{sec:lzu}, we detail how to efficiently invert separable zooms $\calT_{\mathrm{LZ,sep}}$. To invert nonseparable zooms $\calT_{\mathrm{LZ}}$, 
it no longer suffices to invert each axis. We must instead reason in the full 2D space.

Suppose we have a nonseparable zoom $\calT_{\mathrm{LZ}}$. We compute $\calT_{\mathrm{LZ}}[\mathrm{Grid}(h, w)]$ for small $h, w$ and use this to approximate the forward zoom as $\widetilde\calT_{\mathrm{LZ}}$, an $(h-1) \times (w-1)$ piecewise tiling of bilinear maps. Now, to unzoom to a desired output resolution of $H'' \times W''$, we must evaluate $\widetilde\calT_{\mathrm{LZ}}[\mathrm{Grid}(H'', W'')]$. That is, for each $\bx \in \mathrm{Grid}(H'', W'')$, we must determine which of the $(h-1)(w-1)$ quadrilateral pieces it falls in and apply the corresponding inverse bilinear map. Recall from Appendix~\ref{sec:bilinear-maps} that applying an inverse bilinear map amounts to solving a quadratic.

In our actual implementation, we parallelize operations as much as possible. For the $ij$-th tile, instead of first determining which points $\bx \in \mathrm{Grid}(H'', W'')$ are inside of it and then applying the $ij$-th inverse bilinear map, we implement it the other way around. We consider a set of candidate interior points, apply the $ij$-th inverse bilinear map to all of them, and keep only those with a valid solution. The candidate points are those falling inside the axis-aligned rectangle enclosing that tile. The full procedure is described in Algorithm~\ref{alg:unzoom} and visualized in Figure~\ref{fig:unzoom-nonsep}. 

Our implementation takes about $12.6$ms to invert a nonseparable warp with $(h, w) = (31, 51)$ and an output shape $(H'', W'')=(600, 960)$, as in our Argoverse-HD~\cite{li2020towards} experiments. While this is not fast enough to support favorable accuracy-latency tradeoffs, we believe that further optimization (\eg using custom CUDA operations) may change this.

\begin{spacing}{1.2}
\begin{algorithm}
  \footnotesize
  \caption{Inverting nonseparable zooms $\calT_{\mathrm{LZ}}$. \\{\footnotesize In practice, we make the following optimizations. We vectorize the loop on line 13. We also fix $B_{ij}$ to be the max size over choices of $(i,j)$, allowing us to implement the loop on line 4 using batch-processing.}}
  \label{alg:unzoom}
  \setstretch{1.35}
    \begin{algorithmic}[1]
      \LineComment{See Appendix~\ref{sec:nonsep-inversion} for the algorithm setup and meaning of variables.}
      \Function{Unzoom}{$\calT_{\mathrm{LZ}}[\mathrm{Grid}(h, w)], (H'', W'')$}
        \State Initialize $\calT_{\mathrm{LZ}}^{-1}(\bx) = (0, 0)$ for all $\bx \in \mathrm{Grid}(H'', W'')$
        \For{$(i, j) \in [h-1] \times [w-1]$}
        
            \LineComment{corners of $R(i,j)$}
            \State $\bx_{tl}', \bx_{tr}' = 
            \left( \frac{i}{h-1}, \frac{j-1}{w-1} \right),
            \left( \frac{i}{h-1}, \frac{j}{w-1} \right)$
            \State $\bx_{bl}', \bx_{br}' = 
            \left( \frac{i}{h-1}, \frac{j-1}{w-1} \right),
            \left( \frac{i}{h-1}, \frac{j}{w-1} \right)$
            \LineComment{corners of $ij$-th quadrilateral tile}
            \State $\bx_{tl}, \bx_{tr} = \calT_{\mathrm{LZ}}^{-1}\left(\bx_{tl}' \right),
            \calT_{\mathrm{LZ}}^{-1}\left(\bx_{tr}' \right)$
            \State $\bx_{bl}, \bx_{br} =
            \calT_{\mathrm{LZ}}^{-1}\left(\bx_{bl}' \right),
            \calT_{\mathrm{LZ}}^{-1}\left(\bx_{br}' \right)$
            \LineComment{top-left and bottom-right corners of rectangle}
            \LineComment{enclosing the quadrilateral tile}
            \State $\bc_{tl} = \min(\bx_{tl}, \bx_{tr}, \bx_{bl}, \bx_{br})$
            \State $\bc_{br} = \max(\bx_{tl}, \bx_{tr}, \bx_{bl}, \bx_{br})$
            \LineComment{set of all candidate points in the $ij$-th tile}
            \State $B_{ij} = \left\{ \bx \in \mathrm{Grid}(H'', W'') : \bc_{tl} \leq \bx \leq \bc_{br} \right\}$
            \For{$\bx \in B_{ij}$}
                \State $\bx' = \mathrm{BilinearTransformation}^{-1}_{ij}(\bx)$
                \If{$\bx_{tl}' \leq \bx' \leq \bx_{br}'$}
                \State $\calT_{\mathrm{LZ}}^{-1}(\bx) = \bx'$
                \EndIf
            \EndFor
        \EndFor
        \State \Return $\calT^{-1}_{\mathrm{LZ}}$
      \EndFunction

    \end{algorithmic}
\end{algorithm}
\end{spacing}

\subsection{Analysis of our Warping Approximations}
\label{sec:approximations}

We make two approximations in our formulation.
First, to ensure that the composition of forward and inverse warps is truly the identity function, we use the approximate forward warp $\widetilde\calT$ in place of the true forward warp $\calT$.
This trades latency for how well $\widetilde\calT$ zooms in on the intended regions of interest.
See Figure~\ref{fig:warp_approx} for a visualization of this effect.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/approx-fig.png}
    \caption{Quality of the approximate forward warp $\widetilde\calT$ as we decrease the dimensions $h \times w$ of our piecewise bilinear approximation. 
    $\widetilde\calT$ degrades noticeably once $h, w$ decrease beyond a certain threshold.
    }
    \label{fig:warp_approx}
\end{figure*}

Second, we use bilinear downsampling to approximate the inverse at lower resolutions after feature pyramid networks~\cite{lin2017feature}.
This approximation is surprisingly effective!
For our fixed LZU model on 2D detection, we calculate that the error between $\widetilde\calT^{-1}$ and the doubly approximated inverse $\widehat\calT_d^{-1}$ given by
\begin{equation}
    \underset{\bx\in\mathrm{Grid}(H'/d, W'/d)}{\mathrm{avg}} \left\|\widetilde\calT^{-1}(\bx) - \widehat\calT_d^{-1}(\bx)\right\|_2
\end{equation}
is only $0.0274$, $0.0065$, $0.0028$ pixels at $d=2$, $4$, and $8$!

\subsection{Sensitivity to Saliency}
\label{sec:sens-saliency}

Fixed LZU is quite robust to choice of saliency.
Details on how we computed our saliency maps are given in Appendix~\ref{sec:impl-details}.
For 2D detection, we performed a grid search at $0.5$x scale to determine the saliency hyperparameters.
The results show that LZU is quite robust to choice of saliency, as long as the warp is not too strong (see Table~\ref{tab:saliency_sensitivity}).
For semantic segmentation and 3D detection, we chose saliency one-shot, which suggests that these fixed LZU models are also robust to choice of saliency.

Our grid search suggests that adaptive LZU is also robust to choice of hyperparameters than fixed LZU.
However, these saliency hyperparameters, chosen at $0.5$x scale, struggle to generalize to $0.75$x and $1$x scale (see Table~\ref{tab:det-avhd}). As a result, for adaptive warps, it may be necessary to tune saliency at each scale.

\begin{table}[t]
\centering
% \resizebox{0.8\columnwidth}{!}{%
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\footnotesize
\centering
\vspace{0.5em}
\begin{minipage}{.5\linewidth}
\centering
LZU, fixed
\begin{tabular}{rccccc}
    \toprule
    & \multicolumn{4}{c}{Attraction Kernel fwhm} \\
    \cmidrule(r){2-5}
    Amp.~~ & 4 & 10 & 16 & 22 \\
    \cmidrule(r){1-1}\cmidrule(r){2-5}
    1&\bf\textcolor[rgb]{0.479087719298246, 0.702951496388029, 0.54205572755418}{34.6}&\bf\textcolor[rgb]{0.446789473684211, 0.685386996904025, 0.51761919504644}{34.8}&\bf\textcolor[rgb]{0.349894736842106, 0.632693498452013, 0.44430959752322}{35.4}&\bf\textcolor[rgb]{0.301447368421052, 0.606346749226006, 0.40765479876161}{35.7}\\
    5&\bf\textcolor[rgb]{0.495236842105263, 0.711733746130031, 0.55427399380805}{34.5}&\bf\textcolor[rgb]{0.462938596491228, 0.694169246646027, 0.529837461300309}{34.7}&\bf\textcolor[rgb]{0.802070175438597, 0.87859649122807, 0.786421052631579}{32.6}&\bf\textcolor[rgb]{0.479087719298246, 0.702951496388029, 0.54205572755418}{34.6}\\
    10&\bf\textcolor[rgb]{0.446789473684211, 0.685386996904025, 0.51761919504644}{34.8}&\bf\textcolor[rgb]{0.511385964912281, 0.720515995872033, 0.56649226006192}{34.4}&\bf\textcolor[rgb]{0.85051754385965, 0.904943240454077, 0.82307585139319}{32.3}&\bf\textcolor[rgb]{0.495236842105263, 0.711733746130031, 0.55427399380805}{34.5}\\
    50&\bf\textcolor[rgb]{0.414491228070176, 0.667822497420021, 0.4931826625387}{35.0}&\bf\textcolor[rgb]{0.656728070175439, 0.799556243550052, 0.67645665634675}{33.5}&\bf\textcolor[rgb]{0.9	,0.58	,0.54}{31.4}&\bf\textcolor[rgb]{0.624429824561403, 0.781991744066047, 0.652020123839009}{33.7}\\
    100&\bf\textcolor[rgb]{0.414491228070176, 0.667822497420021, 0.4931826625387}{35.0}&\bf\textcolor[rgb]{0.608280701754387, 0.773209494324046, 0.63980185758514}{33.8}&\bf\textcolor[rgb]{0.9	,0.58,	0.54}{31.3}&\bf\textcolor[rgb]{0.705175438596491, 0.825902992776058, 0.713111455108359}{33.2}\\
    \bottomrule
\end{tabular}
\end{minipage}%
\begin{minipage}{.5\linewidth}
\centering
LZU, adaptive
\begin{tabular}{rccccc}
    \toprule
    & \multicolumn{4}{c}{Attraction Kernel fwhm} \\
    \cmidrule(r){2-5}
    Amp.~~ & 4 & 10 & 16 & 22 \\
    \cmidrule(r){1-1}\cmidrule(r){2-5}
    1&\bf\textcolor[rgb]{0.430640350877193, 0.676604747162023, 0.50540092879257}{34.9}&\bf\textcolor[rgb]{0.253, 0.58, 0.371}{36}&\bf\textcolor[rgb]{0.31759649122807, 0.615128998968008, 0.41987306501548}{35.6}&\bf\textcolor[rgb]{0.333745614035088, 0.62391124871001, 0.43209133126935}{35.5}\\
    5&\bf\textcolor[rgb]{0.527535087719299, 0.729298245614035, 0.57871052631579}{34.3}&\bf\textcolor[rgb]{0.398342105263158, 0.659040247678019, 0.48096439628483}{35.1}&\bf\textcolor[rgb]{0.689026315789475, 0.817120743034056, 0.70089318885449}{33.3}&\bf\textcolor[rgb]{0.398342105263158, 0.659040247678019, 0.48096439628483}{35.1}\\
    10&\bf\textcolor[rgb]{0.672877192982457, 0.808338493292054, 0.68867492260062}{33.4}&\bf\textcolor[rgb]{0.527535087719299, 0.729298245614035, 0.57871052631579}{34.3}&\bf\textcolor[rgb]{0.866666666666667, 0.913725490196078, 0.835294117647059}{32.2}&\bf\textcolor[rgb]{0.575982456140351, 0.755644994840042, 0.6153653250774}{34.0}\\
    50&\bf\textcolor[rgb]{0.608280701754387, 0.773209494324046, 0.63980185758514}{33.8}&\bf\textcolor[rgb]{0.705175438596491, 0.825902992776058, 0.713111455108359}{33.2}&\bf\textcolor[rgb]{0.85051754385965, 0.904943240454077, 0.82307585139319}{32.3}&\bf\textcolor[rgb]{0.543684210526316, 0.738080495356037, 0.590928792569659}{34.2}\\
    100&\bf\textcolor[rgb]{0.575982456140351, 0.755644994840042, 0.6153653250774}{34.0}&\bf\textcolor[rgb]{0.640578947368421, 0.79077399380805, 0.664238390092879}{33.6}&\bf\textcolor[rgb]{0.85051754385965, 0.904943240454077, 0.82307585139319}{32.3}&\bf\textcolor[rgb]{0.624429824561403, 0.781991744066047, 0.652020123839009}{33.7}\\
    \bottomrule
\end{tabular}
\end{minipage}
% }
\caption{Grid search over hyperparameters for 2D object detection 
 (see Appendix~\ref{sec:impl-details}) shows that LZU is quite robust to choice of saliency. 
% ``Attraction Range" denotes the fwhm of attraction kernels $k_x,k_y$ and ``Amp" denotes the KDE amplitude $a$. 
For comparison, uniform downsampling yields an AP of $32.2$.}
\label{tab:saliency_sensitivity}
\end{table}

\subsection{Additional Results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/spatial_accuracy.png}
    \caption{Visualizations of the per-pixel accuracy difference between the uniform and the LZU models at each resolution. The largest gains occur at the central horizontal strip, largely corresponding with the saliency map used for segmentation. However, performance also \textit{decreases} at the top and bottom of the image. Thus, using a fixed saliency map must be conscious decision to trade performance at low saliency regions for high saliency regions. We hypothesize that adaptive saliency maps could lead to more uniform gains.}
    \label{fig:spatial_accuracy}
\end{figure}

In Figure~\ref{fig:spatial_accuracy}, we plot the spatial accuracy of our CityScapes~\cite{cordts2016cityscapes} models.

\subsection{Implementation Details}
\label{sec:impl-details}

Our experiments are implemented using open-source libraries MMDetection~\cite{mmdetection}, MMSegmentation~\cite{mmseg2020}, and MMDetection3D~\cite{mmdet3d2020}, all released under the Apache 2.0 License.
% We run experiments on a compute cluster of GeForce RTX 2080 Ti's.
We use GeForce RTX 2080 Ti's for training and training, which takes at most $5$ GPU-days for any given model, but the precise amount varies by model and task. We perform all timing experiments with a batch size of 1 on a single GPU.

\paragraph{Argoverse-HD~\cite{li2020towards} Detection}
As done in FOVEA~\cite{thavamani2021fovea}, for our uniform downsampling experiments, we finetune a COCO-pretrained model for 3 epochs with the random left-right image flip augmentation, a batch size of 8, momentum SGD with a learning rate of 0.005, momentum of 0.9, weight decay of 1e-4, learning rate warmup for 1000 iterations, and a per-iteration linear learning rate decay~\cite{li2019budgeted}.

For both LZU models, we use a learning rate of $0.01$ and keep all other hyperparameters identical to the baseline. To "zoom", we use a $31 \times 51$ saliency map and the separable anti-cropping formulation $\calT_{\mathrm{LZ,sep,ac}}$ (as proposed in~\cite{thavamani2021fovea} and discussed in Section~\ref{sec:saliency-downsampling}).

For the fixed saliency LZU model, we use Gaussian distance kernels $k_x$ and $k_y$ of full-width-half-maximum (fwhm) $22$. 
To generate the fixed saliency map, we use kernel density estimation (KDE) on all training bounding boxes with hyperparameters amplitude $a=1$ and bandwidth $b=64$. For details on the effects of $a$ and $b$, refer to~\cite{thavamani2021fovea}.

For the adaptive saliency LZU model, we use Gaussian distance kernels $k_x$ and $k_y$ of fwhm 10. To generate adaptive saliency, we use KDE on detections from the previous frame with $a=1$ and $b=64$. When training, to simulate motion, we jitter bounding boxes by $\calN(0, 7.5)$ pixels horizontally and $\calN(0, 3)$ pixels vertically.

For each LZU experiment, we run a grid search at $0.5$x scale over separable/nonseparable, amplitude $a=1,5,10,50,100$, and distance kernel's $\text{fwhm}=4,10,16,22$ to determine optimal settings. This is done using an $80/20$ split of the train set, so as to not overfit on the real validation set. We generate this split such that locations between splits are disjoint. All other hyperparameters are chosen one-shot.

% \paragraph{Synthetic Video COCO~\cite{lin2014microsoft} Detection}
% We train the uniform downsampling baseline using default MMDetection~\cite{mmdetection} hyperparameters.
% That is, we train from an ImageNet-pretrained backbone for $12$ epochs with random left-right flip augmentation, a batch size of $16$, momentum SGD with a learning rate of $0.01$, momentum of $0.9$, weight decay of 1e-4, learning rate warmup for $500$ iterations, and step learning schedule of multiplier $0.1$ at epochs $8$ and $11$.

% For our adaptive LZU model, we keep the same training hyperparameters as the uniform downsampling baseline. For "zoom" hyperparameters, we use mostly the same configuration that was determined optimal for our Argoverse-HD~\cite{li2020towards} experiments. That is, we use the separable anti-cropping version $\calT_{\mathrm{LZ,sep,ac}}$ with Gaussian distance kernels $k_x$ and $k_y$ of fwhm $10$. To generate saliency, we use KDE with amplitude $a=1$ and bandwidth $b=64$ (see~\cite{thavamani2021fovea} for details). Finally, we train and test with a jitter of $\calN(0, 10)$ pixels in the x and y directions. However, since COCO images have variable aspect ratio, we are unable to maintain a fixed size saliency map. Instead, we scale the shorter side length to $31$. More precisely, given an input image of size $H \times W$, we generate a saliency map of size $\lfloor Hs\rfloor  \times \lfloor Ws\rfloor$ with a scale factor of $31 \cdot \max(H,W) / \min(H,W)$.

\paragraph{Cityscapes~\cite{cordts2016cityscapes} Segmentation}
We train the uniform downsampling baseline using mostly the default hyperparameters from MMSegmentation~\cite{mmseg2020}.
The only changes are to the data augmentation pipeline and the evaluation frequency.
Comprehensively, we train with just the photometric distortion augmentation (random adjustments in brightness, contrast, saturation, and hue), a batch size of 16, momentum SGD with a learning rate of 0.01, momentum of 0.9, weight decay of 5e-4, and a polynomial learning rate schedule with power 0.9. To account for overfitting, we validate our performance at 10 equally spaced intervals on a 500-image subset of the training dataset (and train on the others). For all experiments, we train for 80K iterations, with the exception of $64\times64$, which we train for 20K iterations due to rapid overfitting.

For the fixed LZU model, we use the same training hyperparameters as the baseline. To "zoom", we use the separable anti-cropping formulation $\calT_{\mathrm{LZ,sep,ac}}$ with a $45 \times 45$ saliency map and Gaussian distance kernels $k_x$, $k_y$ of fwhm $15$. To generate the fixed saliency, we aggregate ground truth semantic boundaries over the train set. Precisely, we define boundaries to be pixels which differ from at least one of its eight neighbors. We compute semantic boundaries for each $256 \times 256$ ground truth segmentation, assign boundaries an intensity of $200$ and background an intensity of $1$, and average pool down to a $45 \times 45$ saliency map. The semantic boundary intensity value was chosen qualitatively (for producing a reasonably strong warp) and tested one-shot.
 
\paragraph{nuScenes~\cite{nuScenes} 3D Detection}
We train the uniform downsampling baseline using all default hyperparameters from MMDetection3D~\cite{mmdet3d2020}, except the learning rate, which we reduce for stability. 
Specifically, we train for 12 epochs with the random left-right flip augmentation, a batch size of 16, momentum SGD with a learning rate of 0.001, momentum of 0.9, weight decay of 1e-4, doubled learning rate on bias parameters with no weight decay, L2 gradient clipping, a step learning rate schedule with drops at epochs 8 and 11, and a linear learning rate warmup for the first 500 iterations.

For the fixed LZU model, we use the same training hyperparameters as the baseline. To "zoom", we use the separable anti-cropping formulation $\calT_{\mathrm{LZ,sep,ac}}$ with a $27 \times 48$ saliency map and Gaussian distance kernels $k_x$, $k_y$ of fwhm $10$. To generate the fixed saliency, we project 3D bounding boxes into the image plane and reuse the same KDE formulation with the same hyperparameters ($a=1$ and $b=64$) as used in 2D detection. These are all chosen and evaluated one-shot.

% \subsection{Code}

% We include code for our nuScenes~\cite{nuScenes} 3D detection experiments at \texttt{lzu\_nuScenes\_code.zip} in our supplementary materials.
% We plan to release publicly release code for other tasks as well.
