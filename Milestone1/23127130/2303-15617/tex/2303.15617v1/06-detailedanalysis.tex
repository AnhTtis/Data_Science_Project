\section{Regret Analysis}

First, we derive an expression for $\hat{b}^i_t$. 
\begin{lemma}
Under the baseline estimation procedure of OL-DRM Algorithm \ref{alg:olb}, for any $t > 1$,
\beq \hat{b}^i_{t+1} =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k q^i_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t q^i_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}.\nonumber \eeq  %+ \delta b^i_t 
\label{lem:ls-exp}
\end{lemma}

\begin{proof}
Let 
\[ \Phi = \left[ \begin{array}{cc} \sum_{k = 1}^{t} p^2_k & -\sum_{k = 1}^t p_k \\  -\sum_{k = 1}^t p_k &  t \end{array}\right]. \nonumber \]
The determinant of matrix $\Phi$ is given by
\[ \tn{Det}(\Phi) = t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2. \nonumber \]
Now, given how $p_t$ is defined (Eq. \eqref{eq:pr-dr-mul}), $\left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2 > 0$ for $k = 1$ and any $t > 1$. Therefore, $\Phi$ is invertible for any $t > 1$. Therefore, from standard least-squares estimation solution, it follows that
\[
\left[ \begin{array}{c} \hat{b}^{e,i}_{1,t+1} \\ \hat{b}^{e,i}_{t+1} \end{array}\right] = \Phi^{-1} \left[ \begin{array}{c} -\sum_{k =1}^t p_k q^i_k \\ \sum_{k=1}^t q^i_k \end{array}\right]. \nonumber \]
By using the standard formula for the inverse of a matrix, which for a given matrix $A$ is $\tn{Adj}(A)^\top/\tn{Det}(A)$, we get that, for any $t > 1$,
\begin{align}
& \left[ \begin{array}{c} \hat{b}^{e,i}_{1,t+1} \\ \hat{b}^{e,i}_{t+1} \end{array}\right] = \frac{\left[ \begin{array}{cc} t & \sum_{k = 1}^t p_k \\  \sum_{k = 1}^t p_k & \sum_{k = 1}^{t} p^2_k \end{array}\right]}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \left[ \begin{array}{c} -\sum_{k =1}^t p_k q^i_k \\ \sum_{k=1}^t q^i_k \end{array}\right], \nonumber  \\
& \tn{i.e.}, ~ \hat{b}^{e,i}_{t+1} =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k q^i_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t q^i_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}
\end{proof}

Next, we derive an expression for the optimal consumer response given by Eq. \eqref{eq:optcondec}. 
\begin{lemma}
The optimal consumer response under OL-DRM Algorithm \ref{alg:olb} is given by
\begin{align}
    & q^{*i}_t = b^i_t - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t, ~~ \widetilde{\Delta}^i_t = \frac{1}{d^i}\sum_{j = 1}^{m} p_{t+j}\frac{\partial \hat{b}^i_{t+j}}{\partial q^i_t}, \nonumber \\
    & \frac{\partial \hat{b}^i_{t+j}}{\partial q^i_t} =  \frac{- \sum_{s = 1}^{t+j-1} p_s p_t + \sum_{s=1}^{t+j-1} p_s^2}{(t+j-1) \sum_{s = 1}^{t+j} \left(p_s - \frac{1}{t+j-1}\sum_{l=1}^{t+j-1} p_l\right)^2}. \nonumber 
\end{align} 
\label{lem:opt-con-resp}
\end{lemma}

\begin{proof}
Recall that the optimal consumer decision is given by
\begin{align}
q^{*i}_t = \arg \max_{q^i_{t:t+m}} \mathbb{E}_{\epsilon^i_{t+1:t+m}} [J(q^i_{t:t+m})], ~~  \nonumber  \\
J(q^i_{t:t+m}) = \left(\sum_{j = 1}^{m}U^i_{t+j}(q^i_{t+j})\right). \nonumber 
\end{align}
The general expectation in Eq. \eqref{eq:optcondec} reduces to the specific expectation in the equation above because (i) the price sequence for DR is set deterministically, and (ii) the fact that, at time $t$, the only randomness in the baselines to be assigned in the future, $\hat{b}^i_{t+j}$ for $j \in [1,m]$, is in the $q^i_k$s for $k \in [t+1,t+m]$; refer to Lemma \ref{lem:opt-con-resp} for the expression for $\hat{b}^i_{t}$. 

Next, we observe that the only term in $U^i_{t+j}(q^i_{t+j})$ that is a function of $q^i_t$ for all $j \in [1,m]$ is the assigned baseline $\hat{b}^i_{t+j}$; see Lemma \ref{lem:opt-con-resp} for the dependence of  $\hat{b}^i_{t+j}$ on $q^i_t$. Given that $J(q^i_{t:t+m})$ is concave in $q^i_{t+j}$s and $q^i_t$, it follows that $\mathbb{E}_{\epsilon^i_{t+1:t+m}} [J(q^i_{t:t+m})]$ is also concave in $q^i_t$. Therefore, given the concavity, by applying first order condition for optimality, we get that the optimal $q^{i*}_t$ satisfies
\[ a^i + \epsilon^i_t - d^i q^{*i}_t - (p_0 + p_t) + d^i \widetilde{\Delta}^i_t = 0. \nonumber \] 
The final expression follows from here. \end{proof}

\begin{remark}
The optimal consumer response has the following terms: (i) $b^i_t$, the standard response when not participating in DR, (ii) the second term is the reduction incentivized by the reward/kWh $p_t$, and (iii) the third term is the inflation in consumption that arises from the incentive to inflate future baseline assignments. 
\end{remark}

In the next lemma, we derive an upper bound for the regret in terms of the cumulative error in the baseline estimation.
\begin{lemma}
The regret under the OL-DRM Algorithm \ref{alg:olb} is
\begin{align}
& R_T = \sum_{t=1}^T (c-p_t)\widetilde{\Delta}_t + \sum_{t=1}^T \frac{(\delta p)^2 e^{-2t}}{d} \nonumber \\
& + \sum_{t=1}^Tp_t\mathbb{E}[\hat{b}_t - \tilde{b}] + P_o, \nonumber 
\end{align} 
where $1/d = \sum_{i=1}^N 1/d^i$, ~ $\widetilde{\Delta}_t = \sum_{i=1}^N  \widetilde{\Delta}^i_t, P_o = \sum_i P^i_o$.
\label{lem:reg-1}
\end{lemma}
\begin{proof}
In the following, for simplicity, we use $q^i_t$ as the notation for the consumption on day $t$ instead of $q^{*i}_t$. Let 
\[  C^{*i}_t = cs^{*i}_t + p^{*} (\tilde{b}^i - s^{*i}_t). \nonumber \]
Also, let
\[ C^{i}_t = cq^{i}_t + p_t (\hat{b}^i_t - q^{i}_t). \nonumber  \]
Then,
\begin{align}
& C^{i}_t -  C^{*i}_t = cq^{i}_t + p_t (\hat{b}^i_t - q^{i}_t) - cs^{*i}_t - p^{*} (\tilde{b}^i - s^{*i}_t) \nonumber \\
& = (c-p_t)q^i_t -(c-p^{*})s^{*i}_t + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i) \nonumber \\
& = c(q^i_t - s^{*i}_t) + p^{*}s^{*i}_t -p_tq^i_t + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i). \nonumber 
\end{align}
From Eq. \eqref{eq:con-c} and Lemma \ref{lem:opt-con-resp},
\[
s^{*i}_t  = b^i_t - \frac{p^{*}}{d^i}, ~
q^{i}_t = b^i_t - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t. \nonumber \]
Therefore,
\begin{align}
& C^{i}_t -  C^{*i}_t = c \widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t)  + p^{*}s^{*i}_t -p_tq^i_t \nonumber \\
& + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i) \nonumber \\
& =  c \widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t) + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i) \nonumber \\
& + p^{*}\left( b^i_t - \frac{p^{*}}{d^i}\right) - p_t\left(b^i_t - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t \right). \nonumber
\end{align}
Taking expectation on both sides
\begin{align}
& \mathbb{E}[C^{i}_t] -  \widetilde{C}^{*i}_t =  c \widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t) + (p_t - p^{*})\tilde{b}^i \nonumber \\
& + p^{*}\left( \tilde{b}^i - \frac{p^{*}}{d^i}\right) - p_t\left(\tilde{b}^i - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t \right) + \mathbb{E}[p_t(\hat{b}^i_t - \tilde{b}^i)]  \nonumber \\
& = (c-p_t)\widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t) + \frac{1}{d^i}\left( p_t^2 - {p^{*}}^2\right) \nonumber \\
& + \mathbb{E}[p_t(\hat{b}^i_t - \tilde{b}^i)] \nonumber \\
& = (c-p_t)\widetilde{\Delta}^i_t + \frac{(\delta p)^2 e^{-2t}}{d^i} + \mathbb{E}[p_t(\hat{b}^i_t - \tilde{b}^i)]. \nonumber 
\end{align}
Therefore,
\[ \mathbb{E}[\widetilde{C}_t] -  \widetilde{C}^{*}_t = (c-p_t)\widetilde{\Delta}_t + \frac{(\delta p)^2 e^{-2t}}{d} + \mathbb{E}[p_t(\hat{b}_t - \tilde{b})]. \nonumber \] 
The final result follows from here.
\end{proof}

\begin{remark}
The regret has the following terms: (i) the first term reflects the increase in the power purchase cost from consumption inflation and the decrease in DR payments from consumption inflation, (ii) the second term reflects the exploration cost that arises from the deviation from the optimal price, (iii) the third term reflects the increase in DR payments from baseline inflation, and (iv) the final term is the total payment made to the consumers upfront.  
\end{remark}

% \begin{lemma}
% The regret under the OL-DRM Algorithm \ref{alg:olb} is
% \begin{align}
% & R_T = \sum_{t=1}^T (c-p_t)\widetilde{\Delta}_t + \sum_{t=1}^T \frac{(\delta p)^2 t^{-2\delta}}{d}\tn{I}\{t ~\tn{is even}\} \nonumber \\
% & + \sum_{t=1}^Tp_t\mathbb{E}[\hat{b}_t - \tilde{b}], \nonumber 
% \end{align} 
% where $1/d = \sum_{i=1}^N 1/d^i$, ~ $\widetilde{\Delta}_t = \sum_{i=1}^N  \widetilde{\Delta}^i_t$.
% \label{lem:reg-1}
% \end{lemma}
% \begin{proof}
% In the following, for simplicity, we use $q^i_t$ as the notation for the consumption on day $t$ instead of $q^{*i}_t$. Let 
% \[  C^{*i}_t = cs^{*i}_t + p^{*} (\tilde{b}^i - s^{*i}_t). \nonumber \]
% Also, let
% \[ C^{i}_t = cq^{i}_t + p_t (\hat{b}^i_t - q^{i}_t). \nonumber  \]
% Then,
% \begin{align}
% & C^{i}_t -  C^{*i}_t = cq^{i}_t + p_t (\hat{b}^i_t - q^{i}_t) - cs^{*i}_t - p^{*} (\tilde{b}^i - s^{*i}_t) \nonumber \\
% & = (c-p_t)q^i_t -(c-p^{*})s^{*i}_t + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i) \nonumber \\
% & = c(q^i_t - s^{*i}_t) + p^{*}s^{*i}_t -p_tq^i_t + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i). \nonumber 
% \end{align}
% From Eq. \eqref{eq:con-c} and Lemma \ref{lem:opt-con-resp},
% \[
% s^{*i}_t  = b^i_t - \frac{p^{*}}{d^i}, ~
% q^{i}_t = b^i_t - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t. \nonumber \]
% Therefore,
% \begin{align}
% & C^{i}_t -  C^{*i}_t = c \widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t)  + p^{*}s^{*i}_t -p_tq^i_t \nonumber \\
% & + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i) \nonumber \\
% & =  c \widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t) + (p_t - p^{*})\tilde{b}^i + p_t(\hat{b}^i_t - \tilde{b}^i) \nonumber \\
% & + p^{*}\left( b^i_t - \frac{p^{*}}{d^i}\right) - p_t\left(b^i_t - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t \right). \nonumber
% \end{align}
% Taking expectation on both sides
% \begin{align}
% & \mathbb{E}[C^{i}_t] -  \widetilde{C}^{*i}_t =  c \widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t) + (p_t - p^{*})\tilde{b}^i \nonumber \\
% & + p^{*}\left( \tilde{b}^i - \frac{p^{*}}{d^i}\right) - p_t\left(\tilde{b}^i - \frac{p_t}{d^i} + \widetilde{\Delta}^i_t \right) + \mathbb{E}[p_t(\hat{b}^i_t - \tilde{b}^i)]  \nonumber \\
% & = (c-p_t)\widetilde{\Delta}^i_t + \frac{c}{d_i}(p^{*} - p_t) + \frac{1}{d^i}\left( p_t^2 - {p^{*}}^2\right) \nonumber \\
% & + \mathbb{E}[p_t(\hat{b}^i_t - \tilde{b}^i)] \nonumber \\
% & = (c-p_t)\widetilde{\Delta}^i_t + \frac{(\delta p)^2 t^{-2\delta}}{d^i}\tn{I}\{t ~\tn{is even}\} + \mathbb{E}[p_t(\hat{b}^i_t - \tilde{b}^i)]. \nonumber 
% \end{align}
% Therefore,
% \[ \mathbb{E}[\widetilde{C}_t] -  \widetilde{C}^{*}_t = (c-p_t)\widetilde{\Delta}_t + \frac{(\delta p)^2 t^{-2\delta}}{d}\tn{I}\{t ~\tn{is even}\} + \mathbb{E}[p_t(\hat{b}_t - \tilde{b})]. \nonumber \] 
% The final result follows from here.
% \end{proof}

Next, we bound a key term that contributes to the consumption inflation term. %We note from the above result that the consumption inflation term is the main contributing term to the regret, since, it is the main contributor in all the terms except the exploration cost term. Define,
\[ 
\Delta_{t,k} := \frac{p_{t+1}}{d}\left[ \frac{-\sum_{s = 1}^{t}p_s p_{t-k} + \sum_{s = 1}^{t}p_s^2}{t \sum_{s=1}^t (p_s - \frac{\sum_{l=1}^t p_l}{t})^2}\right].
\label{eq:Delta-t}
\]
\begin{lemma}
Under the OL-DRM Algorithm \ref{alg:olb}, for any $t > 1$
\[ \Delta_{t,k} = \mathcal{O}\left( (p^{*}+ \delta p) t^{ -1} \right). \nonumber \]
\label{lem:delta-t}
\end{lemma}
\begin{proof}
From how $p_t$ is defined (Eq. \eqref{eq:pr-dr-mul}), we get that 
\begin{align}
& \sum_{s=1}^{t} \left(p_s^2 - p_s p_{t-k} \right) \leq \sum_{s = 1}^{t} \left((p^* + \delta p e^{-s})^2 \right. \nonumber \\
& \left. - (p^* + \delta p (e)^{-s}) p^*\right). \nonumber 
%\label{eq:lem:delta-t:eq1}
\end{align}
Simplifying the above expression, we get
\begin{align}
& \sum_{s=1}^{t} p_s^2 - p_s p_{t-k} \leq \sum_{s = 1}^{t} \left( (\delta p) p^* e^{-s} + \delta p^2 e^{-2s} \right) \nonumber\\
& = \mathcal{O}\left((\delta p) p^* + (\delta p)^2\right). \nonumber 
\end{align}
%The same bound holds when $t$ is even. The steps are exactly the same except that when $t$ is even the sum on the right of Eq. \eqref{eq:lem:delta-t:eq1} extends till $t/2$ instead of $(t-1)/2$. 

Next, we bound the denominator of $\Delta_{t,k}$. %Similarly, when $t$ is odd,
\begin{align}
& t \sum_{s=1}^{t} \left(p_s - \frac{\sum_{l=1}^t p_l}{t}\right)^2 = t \left(\sum_{s=1}^{s=t} p^2_s \right) - \left( \sum_{s=1}^t p_s\right)^2 \nonumber\\
& = t \sum_{s = 1}^{t} \left(p^* + \delta p e^{-s}\right)^2 - \left(\sum_{s=1}^t p_s \right)^2 \nonumber\\
& = t^2 {p^*}^2 + 2tp^*\delta p \sum_{s = 1}^{t} e^{-s} + t \sum_{s = 1}^{t} \left(\delta p\right)^2 e^{-2s} \nonumber\\
&   - \left(\sum_{s=1}^t p_s \right)^2. \nonumber 
%\label{eq:lem:delta-t:eq2}
\end{align}
The term $\left(\sum_{s=1}^t p_s \right)^2 $ can be expanded as
\begin{align} 
& \left(\sum_{s=1}^t p_s \right)^2  = t^2 {p^*}^2 + 2 t p^* \delta p \sum_{s = 1}^{t} \delta p e^{-s} + \left(\sum_{s = 1}^{t}\delta p e^{-s}\right)^2. \nonumber 
\end{align}
Combining the two, we get
\begin{align}
& t \sum_{s=1}^{t} \left(p_s - \frac{\sum_k p_k}{t}\right)^2  =   t \sum_{s = 1}^{t} \left(\delta p\right)^2 e^{-2s} - \left(\sum_{s = 1}^{t}\delta p e^{-s}\right)^2 \nonumber \\
& \geq \frac{t\left(\delta p\right)^2(e^{2t}-1)}{2e^{2t}} - \left(\delta p\right)^2\frac{(e^{2t}-1)}{e^{2t}} = \mathcal{O}\left((\delta p)^2t\right). \nonumber 
\label{eq:din-Dt}
\end{align}
Combining the upper bound for the numerator and the lower bound for denominator, we get the final result.
\end{proof}

The bound on the consumption inflation term $\widetilde{\Delta}_t$ follows from adding $m$ terms of the type bounded in Lemma \ref{lem:delta-t}; see Lemma \ref{lem:opt-con-resp}. In the next lemma, we derive an upper bound for the cumulative error in the baseline estimation and the payment $P_o$. 
% \begin{remark}
% We note that $\Delta_{t,k}$ decreases as $\mathcal{O}(t^{-1})$ and that the consumption inflation term $\widetilde{\Delta}_t$ is the summation of $m$ terms of the type $\Delta_{t,k}$. Therefore, the consumption inflation term decreases to zero with time, clearly suggesting that the incentives to inflate one's consumption decreases with time. This is necessary, as without this property, the SO cannot achieve a ``No Regret" condition.
% \end{remark}

% \begin{lemma}
% Under the OL-DRM Algorithm \ref{alg:olb}, for all $t \geq 4$,
% \[ \Delta_{t,k} = \mathcal{O}\left( t^{\delta -1}\frac{4^{1-2\delta}}{(2^{1-2\delta}-1)} \right). \nonumber \]
% \label{lem:delta-t}
% \end{lemma}
% \begin{proof}
% When $t$ is odd, from how $p_t$ is defined (Eq. \eqref{eq:pr-dr-mul}), we get that 
% \begin{align}
% & \sum_{s=1}^{t} \left(p_s^2 - p_s p_{t-k} \right) \leq \sum_{s = 1}^{(t-1)/2} \left((p^* + \delta p (2 s)^{-\delta})^2 \right. \nonumber \\
% & \left. - (p^* + \delta p (2 s)^{-\delta}) p^*\right). 
% \label{eq:lem:delta-t:eq1}
% \end{align}
% Simplifying the above expression, we get
% \begin{align}
% & \sum_{s=1}^{t} p_s^2 - p_s p_{t-k} \leq \sum_{s = 1}^{(t-1)/2} \left( (\delta p) \ p^* (2 s)^{-\delta} + \delta p^2 (2 s)^{-2\delta} \right) \nonumber\\
% & \leq  \frac{(\delta p) p^* (t-1)^{1-\delta}}{(1-\delta)} + \frac{(\delta p)^2 (t-1)^{1-2\delta}}{(1-2\delta)} \nonumber \\
% & \leq \frac{\left( (\delta p) p^* + (\delta p)^2\right) t^{1-\delta}}{(1-2\delta)} = \mathcal{O}\left(\frac{t^{1-\delta}}{(1-2\delta)}\right). \nonumber 
% \end{align}
% The same bound holds when $t$ is even. The steps are exactly the same except that when $t$ is even the sum on the right of Eq. \eqref{eq:lem:delta-t:eq1} extends till $t/2$ instead of $(t-1)/2$. 

% Next, we bound the denominator of $\Delta_{t,k}$. Similarly, when $t$ is odd,
% \begin{align}
% & t \sum_{s=1}^{t} \left(p_s - \frac{\sum_{l=1}^t p_l}{t}\right)^2 = t \left(\sum_{s=1}^{s=t} p^2_s \right) - \left( \sum_{s=1}^t p_s\right)^2 \nonumber\\
% & = t \sum_{s = 1}^{(t - 1)/2} \left(p^* + \delta p (2 s)^{-\delta}\right)^2 +  t \sum_{s = 1}^{(t + 1)/2} p^{*2} - \left(\sum_s p_s \right)^2 \nonumber\\
% & = t^2 {p^*}^2 + 2tp^*\delta p \sum_{s = 1}^{(t - 1)/2} (2 s)^{-\delta} \nonumber\\
% & + t \sum_{s = 1}^{(t - 1)/2} \left(\delta p (2 s)^{-\delta} \right)^2 - \left(\sum_{s=1}^t p_s \right)^2. 
% \label{eq:lem:delta-t:eq2}
% \end{align}
% The term $\left(\sum_s p_s \right)^2 $ can be expanded as
% \begin{align} 
% & \left(\sum_s p_s \right)^2  = t^2 {p^*}^2 + 2 t p^* \delta p \sum_{s = 1}^{(t - 1)/2} \delta p (2 s)^{-\delta} \nonumber \\
% & + \left(\sum_{s = 1}^{(t - 1)/2}\delta p (2 s)^{-\delta}\right)^2. \nonumber 
% \end{align}
% Combining the two, we get
% \begin{align}
% & t \sum_{s=1}^{t} \left(p_s - \frac{\sum_k p_k}{t}\right)^2  =  t \sum_{s = 1}^{(t - 1)/2} \left(\delta p (2 s)^{-\delta} \right)^2 \nonumber \\
% & -  \left(\sum_{s = 1}^{(t - 1)/2}\delta p (2 s)^{-\delta}\right)^2 \nonumber\\
% & \geq \frac{(\delta p)^2 t (t -1)^{1 - 2\delta}}{2 (1 - 2\delta)} - \frac{t (\delta p)^2}{4^\delta (1 - 2\delta)} - \frac{(\delta p)^2 (t - 1)^{2(1 - \delta)}}{4(1-\delta)} \nonumber\\
% & \geq \frac{(\delta p)^2 t (t -1)^{1 - 2\delta}}{4 (1 - 2\delta)} - \frac{t (\delta p)^2}{4^\delta (1 - 2\delta)} \nonumber \\
% & \geq \frac{(\delta p)^2t}{4(1-2\delta)}\left( (t -1)^{1 - 2\delta} - 1\right) \nonumber \\
% & \geq \frac{(\delta p)^2t}{4(1-2\delta)}\left( \frac{t^{1 - 2\delta}}{2^{1 - 2\delta}} - 1\right), ~ \forall ~ t \geq 2,  \nonumber \\
% & \geq \frac{(\delta p)^2t}{4(1-2\delta)}\left( \frac{t^{1 - 2\delta}}{2^{1 - 2\delta}} - \frac{t^{1 - 2\delta}}{4^{1 - 2\delta}}\right), ~ \forall ~ t \geq 4, \nonumber \\
% & \geq \mathcal{O}\left(\frac{t^{2 - 2\delta}(2^{1-2\delta}-1)}{(1-2\delta)4^{1-2\delta}}\right), ~ \forall ~ t \geq 4. \nonumber
% \label{eq:din-Dt}
% \end{align}
% The same holds when $t$ is even. The steps to show this are exactly the same as when $t$ is odd. Combining the upper bound for the numerator and the lower bound for denominator, we get the final result.
% \end{proof}

%We recall that this is the third and the fourth term in the regret.
\begin{lemma}
Under the OL-DRM Algorithm \ref{alg:olb}, for $T > 1$,
\[ \sum_{t=1}^Tp_t\mathbb{E}[\hat{b}_t - \tilde{b}] + P_o = \mathcal{O}\left((\log{T})^2\right) . \nonumber \]
\label{lem:error-basest}
\end{lemma}
\begin{proof}
Let $\delta b^i_{t+1} = \left( \frac{\sum_{k=1}^t p^{*} \delta p e^{-k} \widetilde{\Delta}^i_k}{\sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \right).$
Then, note that $\sum_t p_t \delta b^i_t = P^i_o$. From Lemma \ref{lem:ls-exp}, we have that
\[
\hat{b}^i_{t+1} =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k q^i_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t q^i_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
Therefore, taking expectation on both sides, we get
\begin{align}
& \mathbb{E}[\hat{b}^i_{t+1}] =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \mathbb{E}[q^i_k] + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \mathbb{E}[q^i_k] }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}
Therefore,
\begin{align}
& \mathbb{E}[\hat{b}^i_{t+1}] = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \left(\tilde{b}^i - \frac{p_k}{d^i}+ \widetilde{\Delta}^i_k\right)}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \nonumber \\
& + \frac{\sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \left(\tilde{b}^i - \frac{p_k}{d^i} + \widetilde{\Delta}^i_k\right) }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}
First, we observe that 
\[ \tilde{b}^i = \frac{-\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \tilde{b}^i + t\tilde{b}^i\sum_{k = 1}^{t} p^2_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
Therefore, it follows that
\begin{align}
& \mathbb{E}[\hat{b}^i_{t+1}] - \tilde{b}^i = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \widetilde{\Delta}^i_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \widetilde{\Delta}^i_k}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}
Therefore,
\begin{align}
& \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \widetilde{\Delta}_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \widetilde{\Delta}_k}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}
In the proof of Lemma \ref{lem:delta-t}, we showed that the denominator is lower bounded as 
\[ t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2 \geq \mathcal{O}(t). \nonumber \]
Also, from the definition of $\widetilde{\Delta}_k$ and Lemma \ref{lem:delta-t}, it follows that
\[ \widetilde{\Delta}_k \leq \mathcal{O}\left(m k^{-1}\right). \nonumber \]
Now, we can simplify $\mathbb{E}[\hat{b}_{t+1} - \tilde{b}] $ further as
\begin{align}
& \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] \nonumber \\
& \leq \frac{\left(\sum_{k = 1}^{t}  p^* \delta p e^{-k}  + \left( \delta p e^{-k}  \right)^2 \right) \sum_{k = 1}^{t}\widetilde{\Delta}_k}{\mathcal{O}(t)} - \delta b^i_{t+1} \nonumber \\
& = \mathcal{O}\left( \frac{\sum_{k = 1}^{t}\widetilde{\Delta}_k}{t}\right) - \delta b^i_t \leq \mathcal{O}\left( \frac{\log(T)}{t}\right) - \delta b^i_{t+1}. \nonumber 
\end{align}
Therefore,
\[ \sum_{t = 1}^T p_t\mathbb{E}[\hat{b}_{t} - \tilde{b}] + P_o \leq (\log{T})^2. \nonumber \]
Then, combining Lemma \ref{lem:reg-1} and Lemma \ref{lem:error-basest}, we get the final regret result. Next, we show that {\it individual rationality} is satisfied by our algorithm. 

{\bf Individual Rationality}: 

For convenience, we remove the superscript $i$ from all variables. Let $\mathbb{E}[\epsilon^2_t] = \sigma^2_t$. We consider $U^{*}$ first. 
\begin{align}
& \mathbb{E}[U^{*}] \nonumber \\
& = \mathbb{E}\left[(a+\epsilon_t)\left(\tilde{b}+\frac{\epsilon_t}{d}\right) - \frac{d}{2}\left(\tilde{b} + \frac{\epsilon_t}{d}\right)^2 - p_0\left( \tilde{b} + \frac{\epsilon_t}{d}\right) \right]\nonumber \\
& = a\tilde{b} + \frac{\sigma^2_t}{2d} -\frac{d\tilde{b}^2}{2} - p_0\tilde{b}. \nonumber 
\end{align}
From Lemma \ref{lem:opt-con-resp}, we recall that $q_t = \tilde{b} + \frac{\epsilon_t}{d} -\frac{p_t}{d} + \widetilde{\Delta}_t$. Hence,
\begin{align}
& \mathbb{E}[U_t(q_t)] = a\tilde{b} + a\left( \widetilde{\Delta}_t - \frac{p_t}{d}\right) -\frac{d}{2} \left( \tilde{b} + \widetilde{\Delta}_t - \frac{p_t}{d}\right)^2 - p_0\tilde{b} \nonumber \\
& -p_0\left(\widetilde{\Delta}_t - \frac{p_t}{d} \right) + p_t\left(\frac{p_t}{d} - \widetilde{\Delta}_t\right) + p_t\mathbb{E}[\hat{b}_t - \tilde{b}] +  \frac{\sigma^2_t}{2d}\nonumber \\
& = a\tilde{b} - p_0\tilde{b} -\frac{d\tilde{b}^2}{2} -\frac{d}{2}\left(\widetilde{\Delta}_t - \frac{p_t}{d}\right)^2 \nonumber \\
& + p_t\left(\frac{p_t}{d} - \widetilde{\Delta}_t\right) + p_t\mathbb{E}[\hat{b}_t - \tilde{b}] +  \frac{\sigma^2_t}{2d}. \nonumber 
\end{align}
Therefore,
\begin{align}
& \mathbb{E}[U_t(q_t)] - \mathbb{E}[U^{*}] = p_t\mathbb{E}[\hat{b}_t - \tilde{b}] + p_t\left(\frac{p_t}{d}- \widetilde{\Delta}_t\right)  \nonumber \\  
& -\frac{d}{2}\left(\widetilde{\Delta}_t - \frac{p_t}{d}\right)^2 = p_t\mathbb{E}[\hat{b}_t - \tilde{b}] + \frac{p^2_t}{2d} -\frac{d\widetilde{\Delta}^2_t}{2}. \nonumber 
\end{align}  

Now, following the proof of Lemma \ref{lem:error-basest},
\begin{align}
& \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] \nonumber \\
& = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \widetilde{\Delta}_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \widetilde{\Delta}_k}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}

Expanding the second term,
\begin{align} 
& \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \widetilde{\Delta}_k = \sum_{k = 1}^{t} \left(p^{*} + \delta p e^{-k}\right)^2 \sum_{k=1}^t \widetilde{\Delta}_k \nonumber \\
& = t {p^{*}}^2\sum_{k=1}^t \widetilde{\Delta}_k + 2p^{*}\delta p \sum_{k = 1}^t e^{-k}\sum_{k=1}^t \widetilde{\Delta}_k \nonumber \\
& + (\delta p)^2\sum_{k = 1}^t e^{-2k}\sum_{k=1}^t \widetilde{\Delta}_k. \nonumber  
\end{align}

Similarly, expanding the first term,
\begin{align}
& -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \widetilde{\Delta}_k \nonumber \\
& = -\sum_{k = 1}^t (p^{*} + \delta p e^{-k}) \sum_{k =1}^t (p^{*} + \delta p e^{-k}) \widetilde{\Delta}_k \nonumber\\
& = -t{p^{*}}^2 \sum_{k =1}^t \widetilde{\Delta}_k -t p^{*} \sum_{k =1}^t \delta p e^{-k} \widetilde{\Delta}_k -\sum_{k =1}^t \delta p e^{-k} \sum_{k =1}^t p^{*} \widetilde{\Delta}_k \nonumber \\
& -\sum_{k =1}^t \delta p e^{-k} \sum_{k =1}^t \delta p e^{-k} \widetilde{\Delta}_k. \nonumber 
\end{align}
Therefore,
\begin{align}
& \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] \nonumber \\
& = \frac{ p^{*}\delta p \sum_{k = 1}^t e^{-k}\sum_{k=1}^t \widetilde{\Delta}_k + (\delta p)^2\sum_{k = 1}^t e^{-2k}\sum_{k=1}^t \widetilde{\Delta}_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \nonumber \\
& + \frac{-t p^{*} \sum_{k =1}^t \delta p e^{-k} \widetilde{\Delta}_k-(\delta p)^2 \sum_{k =1}^t e^{-k} \sum_{k =1}^t e^{-k}}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
\end{align}
Using the bound on $\widetilde{\Delta}_k$,
\begin{align}
& \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] \nonumber \\
& = \frac{ p^{*}\delta p \sum_{k = 1}^t e^{-k}\sum_{k=1}^t \widetilde{\Delta}_k + (\delta p)^2\sum_{k = 1}^t e^{-2k}\sum_{k=1}^t \widetilde{\Delta}_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \nonumber \\
& + \frac{-(\delta p)^2 \sum_{k =1}^t e^{-k} \sum_{k =1}^t e^{-k}}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} - \delta b_{t+1}  \nonumber \\
& \geq -\mathcal{O}\left(\frac{1}{t}\right) - \delta b_{t+1}. \nonumber 
\end{align}
As in the proof of Lemma \ref{lem:error-basest},
\[ \widetilde{\Delta}^2_k \leq \mathcal{O}\left(m^2 k^{-1}\right). \nonumber \]
Therefore, $\exists$ some $t_0$ and some $\delta p = \mathcal{O}(1)$ such that sufficiently large such that, for all $t' > t_0$
\begin{align}
& \sum_{t = 1}^{T}\mathbb{E}[U_t(q_t)] - \mathbb{E}[U^{*}] + P_o
\geq  \nonumber \\
& \sum_{t = 1}^{T}\left( -\mathcal{O}\left(\frac{1}{t}\right) + \frac{{p^{*}}^2}{2d} -\frac{d\widetilde{\Delta}^2_t}{2}\right)  \nonumber\\
&  \geq -\mathcal{O}\left(\log{T}\right) + \mathcal{O}\left(T\right) > 0. \nonumber 
\end{align} 
% \begin{align}
% & \sum_{t = 1}^{t'}\mathbb{E}[U_t(q_t)] - \mathbb{E}[U^{*}] 
% \geq  \nonumber \\
% & \sum_{t = 1}^{t'}\left( -\mathcal{O}\left(\frac{1}{t}\right) + \frac{{p^{*}}^2}{2d} - \mathcal{O}\left(\frac{p^{*}(p^{*}+\delta p)^2}{\delta p}\right) -\frac{d\widetilde{\Delta}^2_t}{2}\right)  \nonumber\\
% &  \geq -\mathcal{O}\left(\log{t'}\right) + \mathcal{O}\left(t'\right) > 0. \tn{\red{?}} \nonumber 
% \end{align} 
Individual rationality follows from here. \end{proof}

% In the next lemma, we derive an upper bound for the cumulative error in the baseline estimation
% \begin{lemma}
% Under the OL-DRM Algorithm \ref{alg:olb}, for all $t \geq 4$,
% \[ \sum_{t=1}^T\mathbb{E}[\hat{b}_t - \tilde{b}] = \mathcal{O}\left(T^{2\delta}\right) . \nonumber \]
% \label{lem:error-basest}
% \end{lemma}
% \begin{proof}
% From Lemma \ref{lem:ls-exp}, we have that
% \[
% \hat{b}^i_{t+1} =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k q^i_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t q^i_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
% Therefore, taking expectation on both sides, we get
% \[
% \mathbb{E}[\hat{b}^i_{t+1}] =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \mathbb{E}[q^i_k] + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \mathbb{E}[q^i_k] }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
% Therefore,
% \begin{align}
% & \mathbb{E}[\hat{b}^i_{t+1}] = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \left(\tilde{b}^i - \frac{p_k}{d^i}+ \widetilde{\Delta}^i_k\right)}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \nonumber \\
% & + \frac{\sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \left(\tilde{b}^i - \frac{p_k}{d^i} + \widetilde{\Delta}^i_k\right) }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber 
% \end{align}
% First, we observe that 
% \[ \tilde{b}^i = \frac{-\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \tilde{b}^i + t\tilde{b}^i\sum_{k = 1}^{t} p^2_k }{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
% Therefore, it follows that
% \[
% \mathbb{E}[\hat{b}^i_{t+1}] - \tilde{b}^i = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \widetilde{\Delta}^i_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \widetilde{\Delta}^i_k}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
% Therefore,
% \[
% \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] = \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k \widetilde{\Delta}_k + \sum_{k = 1}^{t} p^2_k \sum_{k=1}^t \widetilde{\Delta}_k}{t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2}. \nonumber \]
% From the proof of Lemma \ref{lem:delta-t}, we know that the denominator is lower bounded as 
% \begin{align}
% & t \sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2 \geq \mathcal{O}\left(\frac{t^{2 - 2\delta}(2^{1-2\delta}-1)}{(1-2\delta)4^{1-2\delta}}\right) \nonumber \\
% & = \mathcal{O}\left(t^{2 - 2\delta}\right), ~ \tn{for all} ~ t \geq 4.\nonumber
% \end{align} 
% In addition, from the definition of $\widetilde{\Delta}_k$ and Lemma \ref{lem:delta-t}, it follows that
% \[ \widetilde{\Delta}_k \leq \mathcal{O}\left( t^{\delta -1}\frac{m4^{1-2\delta}}{(2^{1-2\delta}-1)} \right). \nonumber \]
% Now, we can simplify $\mathbb{E}[\hat{b}_{t+1} - \tilde{b}]$ further as
% \begin{align}
% & \mathbb{E}[\hat{b}_{t+1} - \tilde{b}] \nonumber \\
% & \leq \frac{\left(\sum_{s = 1}^{(t-1)/2} 2 p^* \delta p (2s)^{-\delta}  + \left( \delta p (2s)^{-\delta}  \right)^2 \right) \sum_{s = 1}^{t}\widetilde{\Delta}_s}{O(t^{2 - 2\delta})}. \nonumber 
% \end{align}

% \end{proof}
