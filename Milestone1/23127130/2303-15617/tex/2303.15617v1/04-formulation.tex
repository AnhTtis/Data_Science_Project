\section{Demand Response Formulation}

We consider the problem of managing Demand Response (DR) in an online setting, where the consumers' utility functions are unknown to the System Operator (SO) and the SO has to learn the necessary consumer relevant parameters online. In a typical demand response program, the SO recruits consumers for demand response and calls them to provide load curtailment on certain days. To incentivize the consumers to curtail, the SO typically pays a certain price (reward/kWh) for the load curtailment the consumers provide. Therefore, the consumer's response depends on the incentive or the reward to reduce, which is the price set by the SO. In addition to the payment for the DR services, the SO incurs an additional cost for serving the final consumption after load curtailment. Thus, the total cost for the SO depends on the payments for the DR services and the cost to serve the final load after the curtailment.

%It is clear from the setup described above that one of the decision variables for the SO is the price (reward/kWh) that it pays for the load curtailment. 
Typically, the SO can only observe the final consumption and not the load curtailment. Therefore, in addition to the price (reward/kWh), the SO needs to specify a baseline consumption to quantify the load curtailment. Baselines are estimates of the power that would be consumed had the consumer not been called to provide load curtailment. The SO, typically, announces the mechanism to assign the consumer's baseline to the consumers participating in the DR program. Thus, the SO's DR policy is the procedure to set the price (reward/kWh) and the baseline. The objective of the System Operator is to choose a DR policy that minimizes its overall cost. %We observe that lower the price, the lesser the DR service and higher overall cost because of the cost to service the loads. It is also conceivable that the payment required to elicit full curtailment can be very high that the overall cost to do so is very high. Therefore, the price for DR service cannot be too high or too low and there is an optimal price for the payment.

We note that it is impossible for the SO to avoid under payment or over payment for the load curtailment without the knowledge of the consumer's correct baseline, which the SO need not know apriori. Here, we consider the setting, where the SO learns to set the correct baseline during the course of the DR program. The price and the baseline that the SO sets can vary from one day to the other and can be adapted depending on the response that the SO observes over its operation. The SO on any day has the following information: (i) the price for DR on all the previous days (ii) the baseline set for all the previous days and (iii) the final consumption of the consumers on all the previous days. The SO can use this information to set the price and the baseline consumption. Since the SO has to learn with the observations made on the fly and there is a cost that the SO incurs every day, this problem in effect is an online learning problem.

Like in a typical online learning problem, the SO has to balance the exploration and exploitation trade-off. Specifically, in this context, the SO cannot afford to set a constant price throughout to learn the correct baseline. This is because the total payment is a function of the assigned baseline, which creates an incentive for consumers to modify their consumption so as to inflate their future baselines and thence their future payments. Therefore, the SO has to be strategic in how the prices are set initially so as to infer the correct baseline over time. This is the exploration part. The exploration thus has to be balanced against deviating from the optimal outcome so as to ensure that on average the SO does not deviate from the best outcome. %The classical trade-off between exploration and exploitation in online learning manifests in this manner in this specific problem. %The challenge is to explore sufficiently and at the same time not deviate significantly on an average. 
We characterize the effectiveness of the SO's DR policy, as is done typically in online learning, through a metric called {\it regret}, which in our case is the difference between the cumulative cost over a set of days and the achievable optimal cost with the full baseline information over the same set of days. Our objective is to develop an online learning scheme that achieves sub-linear regret and thereby achieve an outcome that on average converges to the best DR outcome. %Regret is a metric that is used commonly in 

\subsection{DR Setting}

We index the days by $t$. We denote the number of consumers participating in the DR program by $N$. The SO, before any given day, decides whether to call a DR event or not. If it decides to call a DR event, it assigns a baseline $\hat{b}^i_t$ to the $i$th consumer participating in the DR program and the price for DR $p_t$ (reward/kWh) prior to day $t$. %The baseline is an estimate of the counterfactual consumption that the consumer would consume when it is not called to provide DR service. 
The consumers are paid at the price $p_t$ for the reduction of consumption from the assigned baseline. The price $p_t$ is a reward or incentive for the consumer to reduce its consumption. The price and the baseline is set by the SO using the following information: (i) the price for DR on all the previous days (ii) the baseline set for all the previous days and (iii) the final consumption of the consumers on all the previous days. Thus, the SO can adapt its price and the baselines online as it makes newer observations. As in any DR program, the SO announces the procedure for setting the price for DR and the baseline prior to the first day, which is the SO's DR policy. % and announces the price for DR for a certain number of days in advance.

\subsection{Consumer Model}
\label{sec:con-mod}

We denote the electricity consumption of a consumer $i$ on day $t$ by $q^i_t$. Then, the payment $P$ to consumer $i$ for curtailing from $\hat{b}^i_t$ is given by
\beq
P^i_t = p_t(\hat{b}^i_t - q^i_t).  \nonumber 
\label{eq:reward}
\eeq

We denote the utility that the consumer derives from the electricity consumption $q^i_t$ by
\begin{equation}
u^i_t = u^i(q^i_t,\epsilon^i_t) = \left( a^i + \epsilon^i_t \right) q^i_t - \frac{d^i (q^i_t)^2 }{2},  \nonumber 
\label{eq:con-ut}
\end{equation}
where %p_0$ is the retail price and 
$\epsilon^i_t$ is a zero mean random variable and models the unpredictability or the uncertainty in the consumer's behavior. The assumption is that, by day $t$, the consumers observe their respective $\epsilon^i_t$s and that this information is private to them. 

As in a typical power market, the consumers pay a retail price to the electricity provider for their daily consumption. We denote the retail price that the consumers pay by $p_0$. Therefore, the net utility to consumer $i$ on day $t$ is given by
\beq
U^i_t(q^i_t) = u^i_t - p_0q^i_t + P^i_t. \nonumber 
\label{eq:netut-con}
\eeq

The correct average baseline $\tilde{b}^i$ for a consumer $i$ can be derived from the consumer's utility function. Following the definition that the correct baseline is the optimal consumption when the consumer is not called to provide DR, the correct average baseline for a consumer $i$ is given by
\beq
\tilde{b}^i =  \mathbb{E}_{\epsilon^i_t} b^i_t = \frac{a^i - p_0}{d^i}, ~~ \tn{where} ~ b^i_t = \frac{a^i + \epsilon^i_t - p_0}{d^i}. \nonumber 
\label{eq:av-bas}
\eeq 
The optimal consumption in the hypothetical case when the set baselines are fixed to the correct values and do not depend on the past consumption can be derived by minimizing $U^i_t$ individually. Therefore, the consumption for this hypothetical case is given by
\beq
s^i_t(p_t) = \argmin_{q^i_t} U^i_t(q^i_t) = \frac{a^i + \epsilon^i_t - p_0 - p_t}{d^i}.   
\label{eq:con-c}
\eeq

% It follows that the true average baseline $b$ of the consumer is given by, 
% \beq
% b^i = \frac{a - p_0}{d}
% \label{eq:av-bas}
% \eeq 

% From strict concavity of $u^i$ we get,
% \beq
% q^i_t(p_t) = \frac{a^i + \epsilon^i_t - p_0 - p_t}{d^i}  
% \label{eq:con-c}
% \eeq 

{\it Consumer's Optimal Decision}: In a DR setting, since a consumer's current consumption determines the future baseline and payments, the consumer typically has an incentive to modify its consumption to influence the future baselines and the DR payments. To model this effect, we consider the setting, where a consumer's current decision is also determined by its effect on the outcome of the next $m$ days. In this setting, the optimal consumer response on a day $t$ is given by
\beq
q^{*i}_t = \arg \max_{q^i_{t:t+m}} \mathbb{E}\sum_{s = t}^{t+m}U^i_s(q^i_s),  
\label{eq:optcondec}
\eeq
where expectation is over all randomness in $\epsilon^i_s, p_s, \hat{b}^i_s$ for all $s > t$. 

\subsection{System Operator's Objective}

The system operator's decision variables on a day $t$ are the price for DR and the baselines, which we collectively denote by $(p_t, \hat{b}^{1:N}_t)$. %We denote the sequence of decisions over a period $[t_1, t_2]$ by $\pi_{t_1:t_2}$. 
We denote the aggregate of the assigned baselines on a day $t$ by
\beq
\hat{b}_t = \sum_{i=1}^{N} \hat{b}^i_t. \nonumber 
\label{eq:aggbase}
\eeq
Similarly, we denote the aggregate consumption on a day $t$ by
\beq
q_t = \sum_{i=1}^{N} q^i_t. \nonumber 
\label{eq:aggcon}
\eeq

Typically, the SO has to procure power from an external market to serve the demand of the consumers. Therefore, the SO incurs a cost for procuring the power consumed by the consumers. We denote the cost of procuring an unit of power by $c$. Therefore, the total cost that is incurred by the SO on a day $t$ is the sum of the purchase cost and the cost for DR:
\beq
C_t(p_t, \hat{b}_t) = c q_t + p_t (\hat{b}_t - q_t). \nonumber
\label{eq:so-cost}
\eeq
Therefore, the SO's expected cost on day $t$ conditioned on the set baseline and the price is given by
\beq
\widetilde{C}_t(p_t, \hat{b}_t) = \mathbb{E}[C_t(p_t, \hat{b}_t) \vert p_t, \hat{b}_t] =  c \tilde{q}_t + p_t (\hat{b}_t - \tilde{q}_t), \nonumber 
\label{eq:so-cost}
\eeq
where $\tilde{q}_t  = \mathbb{E} q_t$. In the analysis, we assume that consumer chooses $q_t$ according to Eq. \eqref{eq:optcondec}. % $\tilde{q}_t  = \mathbb{E}_{\epsilon^{1:N}_{t}} q_t$, and the dependence of $q_t$ on $\epsilon^{1:N}_t$ follows from Eq. \eqref{eq:optcondec}. 

{\bf SO's Objective:} Let the price that minimizes SO's expected cost when the baselines are set to the correct values be denoted by $p^{*}$. Then, the consumption under this price, with the baselines set to the correct values, is given by $s^{*i}_t = s^i_t(p^{*})$ for all $i$. Therefore, the optimal expected cost for the SO, when the baselines are set to the correct values, is given by
\beq
\widetilde{C}^{*}_t  =  c \tilde{s}^{*}_t + p^{*} (\tilde{b} - \tilde{s}^{*}_t), \nonumber 
\label{eq:mincost} 
\eeq
where $\tilde{s}^{*}_t = \mathbb{E}_{\epsilon_t} s^{*}_t$ and $s^{*}_t =  \sum_{i = 1}^{N} s^{*i}_t$, $\tilde{b} =  \sum_{i = 1}^{N} \tilde{b}^{i}$. Since the primary objective of the SO is not to inflate the baseline and over pay, it is reasonable to define the regret with respect to the total optimal cost when the baselines are set to the correct values. Therefore, we define the SO's expected regret over a time period $T$, under a DR policy, as
\beq
R_T = \sum_{t = 1}^{T} \left( \mathbb{E} [\widetilde{C}_t(p_t, \hat{b}_t)]  - \widetilde{C}^{*}_t\right). \nonumber  
\label{eq:regret}
\eeq
The SO's objective is to prescribe a DR policy such that
\beq
\lim_{T \rightarrow \infty} \frac{R_T}{T} = 0 ~~ (\tn{No Regret}). \nonumber 
\label{eq:so-obj}
\eeq
The SO has to achieve zero regret on average while ensuring that the consumer's individual rationality is satisfied on average, i.e.,
\begin{align} 
& \lim_{T \rightarrow \infty} \frac{\mathbb{E}[\sum_{t = 1}^T U^i_t(q^i_t)] - TU^{*i}}{T} \geq 0~ \forall ~ i, \nonumber \\
& ~ (\tn{Individual Rationality}), \nonumber \\
& ~ \tn{where} ~ U^{*i} = \mathbb{E}[u^i(s^i_t(0),\epsilon^i_t) - p_0s^i_t(0)].   % \widetilde{U}^i_t(\tilde{b}^i), \nonumber \\
%& ~ \widetilde{U}^i_t(\cdot) = \mathbb{E}[ U^i_t(\cdot)].
\label{eq:con-indrat}
\end{align}

\begin{remark}[Individual Rationality]
The individual rationality condition in Eq. \eqref{eq:con-indrat} is essential, since, otherwise the SO can set a very low baseline and under pay the consumers for the DR services. Thus, this condition is essential in the formulation. Moreover, the consumer will not participate in the DR program if the consumer does not receive a benefit that is on average at least as much as the benefit when not participating in the DR program. Therefore, to enforce this constraint, we set $U^{*i}$ in the individual rationality condition as the optimal expected utility for a consumer when not participating in the DR program.
\end{remark}

\begin{remark}[Regret Definition]
%The critical aspect of the regret definition is the cost to be compared with.
The question is whether $\widetilde{C}^{*}_t$ is appropriate as the cost to be compared with in the regret. It can be shown that if the SO inflates the baseline by a certain quantity $\Delta b > 0$ then the optimal expected cost necessarily increases till the incentives for the consumers to participate in the DR program are positive. %Now, it is possible that the the SO gains and the consumers do not loose even while understating the baseline. 
Therefore, given that the primary objective is to mitigate over payment while ensuring individual rationality, it follows that $\widetilde{C}^{*}_t$ is the right candidate for the cost to be compared with. 
\end{remark}



