\section{Introduction}

Demand Response (DR) programs \cite{Albadi2008} are potentially powerful tools to modulate the demand for electricity in a wide variety of situations. For example, at certain times such as mid-afternoons on hot summer days, the supply of additional electric power is scarce and expensive. At these times, it is more cost-effective to reduce demand than to increase supply to maintain power balance. Another scenario is a grid with high renewable penetration. Here, DR promises to be a better alternative compared to other expensive and polluting reserves to balance the variability in renewable generation. Realizing its potential, the 2005 Energy Policy Act provided the Congressional mandate to promote DR in organized wholesale electricity markets. The FERC order 745 \cite{federal2011demand} met this mandate by prescribing that demand response resource owners should be allowed to offer their demand reduction as if it were a supply resource rather than a bid to reduce demand so that the market operates fairly.

Dynamic pricing based DR programs %\cite{Joskow2012, chakraborty2017distributed} 
%\cite{chakraborty2017distributed} 
can ideally achieve market efficiency, but they require complex metering and communication infrastructure to achieve this, which raises their implementation costs %\cite{mathieu2013residential, borenstein2002dynamic}
\cite{mathieu2013residential}. Furthermore, consumers may not be responsive to dynamic pricing \cite{faruqui2011dynamic}. Alternatively, consumers could be signaled to reduce consumption and paid for their load reductions. Such schemes are referred to as Incentive-based DR programs or Demand Reduction programs. There are two key components of any incentive-based DR program: (a) a baseline against which demand reduction is measured, (b) a payment scheme for agents who reduce their consumption from the baseline.

Thus, incentive-based DR programs require an established baseline against which consumer's load reduction is measured. The baseline is an estimate of the consumption when the consumer is not participating in the DR program. There are several ways to approach the estimation of baseline. One could, for example, use data to estimate the baseline. For example, the California Independent System Operator (CAISO) uses the average of the consumption on the ten most recent non-event days as the baseline estimate~\cite{CaisoDR2017}. %The CAISO method also uses a morning adjustment factor to account for any variability in consumption pattern during the day of the DR event from the past. 
Data driven approaches can be broadly classified as (a) averaging, (b) regression, and (c) control group methods. Typically, these methods are prone to baseline manipulation %\cite{chao2010price,wolak2007residential,chao2013incentive, vuelvas2017rational}
\cite{vuelvas2017rational}. There have have been reported cases in the past where the participants have artificially inflated their baseline for increasing payments \cite{gaming-examples}. Another class of approaches are based on {\it mechanism design}, where the consumers are elicited to report their baselines \cite{muthirayan2019mechanism, muthirayan2019minimal, satchidanandan2022two}. These approaches rely on suitably designed payment schemes to ensure that the manipulation or gaming of the reporting is minimal. 


While many data-driven approaches for estimating baseline have been proposed in the literature \cite{coughlin2008estimating, grimm2008evaluating, mathieu2011quantifying, wijaya2014bias, nolan2015challenges, weng2015probabilistic, zhang2016cluster, nolan2015challenges, zhou2016forecast, hatton2016statistical, wang2018synchronous}, they typically consider the offline setting where sufficient data is available for estimation prior to the start of the DR program. The limitation is that these approaches cannot be used when the data is limited or when the underlying conditions can change. The mechanism design approaches can avoid the need for learning, but have limitations because the consumers can be unwilling to reveal their baselines or can even be unaware of their baselines. These considerations necessitate approaches that can learn online while running the DR program without needing the consumers to report any information. 

% necessitating {\it online learning}, where online learning refers to learning while interacting with the environment whose characteristics are to be learned.

%Current methods to establish the baseline raise several concerns. One major concern is that the consumers have an incentive to artificially inflate their baseline to increase their profits \cite{chao2010price,wolak2007residential,chao2013incentive, vuelvas2017rational}. Cases have been reported where the participants artificially inflated their baseline for increasing payments \cite{gaming-examples}. Fairness can also be a concern. Consider, for instance, an agent who happens to be on vacation during a DR event and receives a payment for load reduction without suffering any hardship. This can be perceived as unfair by other agents who curtail their consumption and suffer some disutility. 

{\bf Contribution}: In this work, we consider the problem of managing DR resources where a participating consumer's baseline is to be estimated online, i.e., while running the DR program. We consider the setting where the DR program can only learn from the consumption data that it gathers over the course of time. % the consumers need not report any information and therefore %where large amount of historical data is available and the challenge is only of estimation. and thus the consumers can interfere with the estimation to inflate their baselines. %We consider the problem where the DR program manager has to learn online the baseline of the consumers to manage its DR resources cost efficiently. 
The unique challenge of our setting is that the DR program manager has to simultaneously learn the consumers' baselines and optimize its operating costs with the information it gathers along the way. This makes this problem a typical online learning problem. % where typically the learner has to simultaneously learn and optimize some objective. 
Therefore, the exploration-exploitation trade-off in any online learning problem applies to our problem as well. The added complexity in a setting like ours is the incentive the consumers can have to interfere with the learning in order to inflate the baseline estimation. We formulate the online learning DR problem that incorporates all these characteristics and propose a  online learning DR scheme for this problem. We show that our method achieves $\mathcal{O}((\log{T})^2)$ regret with respect to the optimal operating cost over $T$ days of the DR program and is individually rational for the consumers to participate with an upfront payment for participation. Our main contribution is an online learning scheme for (incentive-based) DR that converges to the optimal cost with a very low regret and is at the same time individually rational. Ours is also the first work to formally study incentive-based DR as an online learning problem and present algorithms and regret guarantees.

