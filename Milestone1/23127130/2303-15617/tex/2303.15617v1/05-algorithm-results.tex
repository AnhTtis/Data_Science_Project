\section{Online Learning DR Mechanism}

In this section, we discuss our algorithm and present the properties of our algorithm formally. %We first discuss the single consumer case and then the multiple consumer case.

%\subsection{Single Consumer}

%In this section, we describe our online learning algorithm for the single consumer case. Here, for convenience, we drop the superscript $i$ from all the consumer specific variables. The algorithm for the multiple consumer is a trivial extension of the single consumer case. 

% {\bf DR Policy}: We recall that $q_1, q_2, q_3,....$ denote the sequence of consumption by the consumer and $p_1, p_2, . . . $ denote the sequence of price set by the SO for DR. The SO, at the end of a day $t$, computes $\hat{b}_{1,t+1}$ and $\hat{b}_{t+1}$ according to
% \begin{equation}
% \left[ \begin{array}{c} \hat{b}^e_{1,t+1} \\ \hat{b}^e_{t+1} \end{array}\right] =\arg \min_{\hat{b},\hat{b}_1} \sum_{k=1}^t (q_k - (\hat{b} - \hat{b}_1 p_k))^{2}. 
% \label{eq:ls-est}
% \end{equation}
% From \eqref{eq:ls-est} it follows that
% \begin{equation}
% \left[ \begin{array}{c} \hat{b}_{1,t+1} \\ \hat{b}_{t+1} \end{array}\right] = \left[ \begin{array}{cc} \sum_{k = 1}^{t} p^2_k & \sum_{k = 1}^t p_k \\  \sum_{k = 1}^t p_k &  t \end{array}\right]^{-1} \left[ \begin{array}{c} \sum_{k =1}^t p_k q_k \\ \sum q_k \end{array}\right]. \nonumber 
% \label{eq:ls-est-1}
% \end{equation}
% Therefore, it follows that
% \begin{align}
% & \left[ \begin{array}{c} \hat{b}_{1,t+1} \\ \hat{b}_{t+1} \end{array}\right] = \frac{\left[ \begin{array}{cc} t & -\sum_{k = 1}^t p_k \\  -\sum_{k = 1}^t p_k & \sum_{k = 1}^{t} p^2_k \end{array}\right]}{t \sum_{k} \left(p_k - \frac{\sum_k p_k}{t}\right)^2} \left[ \begin{array}{c} \sum_{k =1}^t p_k q_k \\ \sum q_k \end{array}\right]. \nonumber  \\
% & \tn{i.e.} ~ \hat{b}_{t+1} =  \frac{ -\sum_{k = 1}^t p_k \sum_{k =1}^t p_k q_k + \sum_{k = 1}^{t} p^2_k \sum q_k }{t \sum_{k} \left(p_k - \frac{\sum_{k=1}^t p_k}{t}\right)^2}
% \label{eq:ls-est-2}
% \end{align}
% The SO then assigns $\hat{b}_{t+1} = \hat{b}^{e}_{t+1}$ as the baseline for day $t+1$ and calls the consumer to provide DR service. The price $p_t$ for DR is given by 
% \begin{equation}
% p_t = p^* + \delta p e^{-t} ,
% \label{eq:pr-dr-mul}
% \end{equation}
% where $\delta p$ is a constant. In addition, the SO also pays a payment $P_o$ to the consumer upfront. This payment is needed for meeting the individual rationality condition. Later, we give the specific form of this payment.
% \begin{equation}
% p_t = \left\{ \begin{array}{cc} p^* & \ \text{If} \ t \ \text{is odd} \\ p^* + \delta p  \ t^{-\delta} & \ \text{If} \ t \ \text{is even} \end{array} \right. ,
% \label{eq:pr-dr}
% \end{equation}
% where $\delta p $ and $\delta$ are constants to be specified later. %and $p^{*} = c/2$.



%\subsection{Multiple Consumer Case}

%The multiple consumer case is a trivial extension of the single consumer case. In this case, 
We recall that $q_1, q_2, q_3,....$ denote the sequence of consumption by the consumer and $p_1, p_2, . . . $ denote the sequence of price set by the SO for DR. The SO, at the end of a day $t$,  calculates a $\hat{b}^i_{1,t+1}$ and $\hat{b}^i_{t+1}$ for each consumer $i$ by
\begin{equation}
\left[ \begin{array}{c} \hat{b}^{e,i}_{1,t+1} \\ \hat{b}^{e,i}_{t+1} \end{array}\right] =\arg \min_{\hat{b},\hat{b}_1} \sum_{k=1}^t (q^i_k - (\hat{b} - \hat{b}_1 p_k))^{2}. 
\label{eq:ls-est-mulcon}
\end{equation}
The SO then assigns %$\hat{b}^i_{t+1} = \hat{b}^{e,i}_{t+1} + \delta b^i_t$, 
$\hat{b}^i_{t+1} = \hat{b}^{e,i}_{t+1}$ as the baseline for day $t+1$ to the $i$th consumer and calls all the consumers to provide DR service. The price $p_t$ for DR is given by 
\begin{equation}
p_t = p^* + \delta p e^{-t},
\label{eq:pr-dr-mul}
\end{equation}
where $\delta p$ is a constant. In addition, the SO also pays a payment $P_o$ to the consumer upfront. This payment is needed for meeting the individual rationality condition. Later, we give the specific form of this payment. %As in the single consumer case, here too, the SO pays a fixed amount $P^i_o$ to every consumer $i$ upfront.
% \begin{equation}
% p_t = \left\{ \begin{array}{cc} p^* & \ \text{If} \ t \ \text{is odd} \\ p^* + \delta p  \ t^{-\delta} & \ \text{If} \ t \ \text{is even} \end{array} \right.,
% \label{eq:pr-dr-mul}
% \end{equation}
% where $\delta p$ and $\delta$ are constants to be specified later.

\LinesNumberedHidden{\begin{algorithm}[]
\DontPrintSemicolon
\KwInput{$N, P^i_o$ for each $i \in [1,N]$}

Make the payment $P^i_o$ to each $i \in [1,N]$.

Announce the price sequence for the DR program as given by Eq. \eqref{eq:pr-dr-mul} and the process of baseline estimation.

{\bf Initialize} $\hat{b}^i_1$ for each $i \in [1,N]$ arbitrarily.

\For{$t = [1,T]$}{

Assign $\hat{b}^i_t$ as the baseline for each $i \in [1,N]$.

Set $p_t$ as the price for DR.

Receive the demand request $q^i_t$ from each consumer $i \in [1,N]$.

Serve $q^i_t$ to each consumer $i \in [1,N]$.

Incur the purchase cost $cq_t$ and the DR cost $p_t (\hat{b}_t - q_t)$.

Update $\hat{b}^i_t \rightarrow \hat{b}^i_{t+1}$ for each $i \in [1,N]$ according to Eq. \eqref{eq:ls-est-mulcon}.
}
\caption{Online Learning DR Mechanism (OL-DRM)}
\label{alg:olb}
\end{algorithm}}

\begin{remark}[Optimal Price for DR]
For the consumer utility model considered here \eqref{eq:con-ut}, it is straightforward to show that the optimal price $p^{*}$, given by the condition $\frac{d\widetilde{C}_t(p_t, \tilde{b})}{dp_t} = 0$, is $p^* = c/2$. 
\end{remark}

% \begin{remark}[Baseline Assignment]
% The baseline procedure given by Eq. \eqref{eq:ls-est} is motivated by the fact that the myopic optimal response to DR price $p_t$, as given by Eq. \eqref{eq:con-c}, is of the form $q_t = \tilde{b} + \epsilon_t - \tilde{b}_1 p_t$, where $\tilde{b}$ is the average consumption of the consumer had the consumer not been called for DR. Therefore, $\tilde{b}$ represents the correct average baseline and it follows that $\hat{b}_{t+1}$ can serve as an estimation of the correct average baseline. This is why the SO assigns $\hat{b}_{t+1}$ as the baseline for day $t+1$. %We note that the actual consumption $q_t$ will not be of this form, but rather determined by Eq. \eqref{eq:optcondec}. In spite of this, we later show through our analysis that over time this leads to the correct estimate and the desired regret performance.  
% \end{remark}

\begin{remark}[Exploration Strategy]
The prescribed policy for the SO explores by perturbing the price from the optimal price $p^{*}$. These perturbations cannot be persistent and therefore the prescribed policy reduces the perturbations with time. The decreasing of the perturbation is the balancing part of the exploration necessary to achieve sub-linear regret or ``No Regret".
\end{remark}

% Next we present the regret guarantee for our algorithm.
% \begin{theorem}
% Under the Algorithm OL-BE (Algorithm \ref{alg:olb}), for $\delta = 1/4, \delta p = \mathcal{O}(1)$, 
% \beq R_T = \mathcal{O}(T^{1/2}), \nonumber \eeq
% and {\it individual rationality} holds for each $i \in [1,N]$.
% \label{thm:olb}
% \end{theorem}

\begin{definition}
\[\widetilde{\Delta}^i_t = \frac{1}{d^i}\sum_{j = 1}^{m} \frac{p_{t+j}\left(- \sum_{s = 1}^{t+j-1} p_s p_t + \sum_{s=1}^{t+j-1} p_s^2\right)}{(t+j-1) \sum_{s = 1}^{t+j} \left(p_s - \frac{1}{t+j-1}\sum_{l=1}^{t+j-1} p_l\right)^2}. \nonumber \]
\[ P^i_o = \sum_{t=1}^T p_t\left( \frac{\sum_{k=1}^t p^{*} \delta p e^{-k} \widetilde{\Delta}^i_k}{\sum_{k=1}^t \left(p_k - \frac{\sum_{l=1}^t p_l}{t}\right)^2} \right). \nonumber \]
\label{def:payment}
\end{definition}
Next, we present the regret guarantee for our algorithm.
\begin{theorem}
Under the Algorithm OL-DRM (Algorithm \ref{alg:olb}), with $P^i_o$ given by Definition \ref{def:payment}, for $\delta p = \mathcal{O}(1)$, and $T > 1$,
\beq R_T = \mathcal{O}((\log{T})^2), \nonumber \eeq
and {\it individual rationality} holds for each $i \in [1,N]$. 
\label{thm:olb}
\end{theorem}
We give the detailed analysis in the next section. We observe that the regret guarantee for our approach leads to the desired ``No Regret" and individually rational outcome. 

\begin{remark}[Approach]
Our approach is the online equivalent of the regression approach to estimate baseline. An alternate approach is to not call the consumers for a certain number of days and use the consumption on these days to set the baseline for the future. This is the online equivalent of the averaging approach to estimate baseline. It can be shown that this approach leads to $\mathcal{O}(T^{1/3})$ regret. Our result, therefore, provides theoretical justification that regression type approaches can be superior to averaging type approaches.
\end{remark}

%\begin{remark}[Assumption]
%Our algorithm requires that $d^i$ is known. The parameter $d_i$ is proportional to the price elasticity of demand. The knowledge of the price elasticity of demand is not sufficient to establish the baseline because these are independent parameters. Therefore, the central technical challenge does not go away even with this assumption. This parameter appears in the term $\delta b^i_t$ of our algorithm, which is the baseline correction term. From the proof, it is evident that the primary role of the correction term is to guarantee individual rationality. It is a subject of future work to remove the need for knowing $d_i$. %We note that the averaging type online approach will not require this assumption.
%\end{remark}

\begin{remark}[Extensions]
Our setting assumes that the consumer utility model is quadratic, and the consumer's decision depends only on a finite horizon $m$. Our approach can be trivially extended to the infinite horizon case, where the future benefits are discounted by a discounting factor. For this case, the algorithm requires no change except for the definition of $P^i_o$. We can use the same proof technique to analyse this case to show that the same regret is achievable. The extension to a general utility model is a subject of future work. 
\end{remark}
