@misc{kojima2022largelanguagemodels,
    author = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
    title = {Large Language Models are Zero-Shot Reasoners},
    year = 2022,
    url = {https://openreview.net/forum?id=e2TBb5y0yFf}
}

@misc{rae2021scalinglanguagemodels,
    author = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
    title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
    year = 2021,
    url = {https://arxiv.org/abs/2112.11446}
}

@misc{bérard2019naverlabseurope,
    author = {Alexandre Bérard and Ioan Calapodescu and Claude Roux},
    title = {Naver Labs Europe's Systems for the WMT19 Machine Translation Robustness   Task},
    year = 2019,
    url = {https://arxiv.org/abs/1907.06488}
}

@misc{wei2021finetunedlanguage,
    author = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
    title = {Finetuned Language Models are Zero-Shot Learners},
    year = 2021,
    url = {https://openreview.net/forum?id=gEZrGCozdqR}
}

@misc{cronen-townsend2002predictingqueryperformance,
    author = {Stephen Cronen-Townsend and Yun Zhou and W. Bruce Croft},
    title = {Predicting query performance.},
    year = 2002,
    url = {https://doi.org/10.1145/564376.564429},
    doi = {10.1145/564376.564429}
}

@misc{raiber2014queryperformanceprediction,
    author = {Fiana Raiber and Oren Kurland},
    title = {Query-performance prediction: setting the expectations straight.},
    year = 2014,
    url = {https://doi.org/10.1145/2600428.2609581},
    doi = {10.1145/2600428.2609581}
}

@misc{ueffing2005wordlevelconfidence,
    author = {Nicola Ueffing and Hermann Ney},
    title = {Word-Level Confidence Estimation for Machine Translation using Phrase-Based Translation Models},
    year = 2005,
    url = {https://www.aclweb.org/anthology/H05-1096/}
}

@misc{fomicheva2020unsupervisedquality,
    author = {Marina Fomicheva and Shuo Sun and Lisa Yankovskaya and Frédéric Blain and Francisco Guzmán and Mark Fishel and Nikolaos Aletras and Vishrav Chaudhary and Lucia Specia},
    title = {Unsupervised Quality Estimation for Neural Machine Translation},
    year = 2020,
    url = {https://arxiv.org/abs/2005.10608}
}

@misc{schick2023toolformerlanguage,
    author = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
    title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
    year = 2023,
    url = {https://arxiv.org/abs/2302.04761}
}

@misc{lyu2023faithfulchainof,
    author = {Qing Lyu and Shreya Havaldar and Adam Stein and Li Zhang and Delip Rao and Eric Wong and Marianna Apidianaki and Chris Callison-Burch},
    title = {Faithful Chain-of-Thought Reasoning},
    year = 2023,
    url = {https://arxiv.org/abs/2301.13379}
}

@misc{ye2023largelanguagemodels,
    author = {Yunhu Ye and Binyuan Hui and Min Yang and Binhua Li and Fei Huang and Yongbin Li},
    title = {Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning},
    year = 2023,
    url = {https://arxiv.org/abs/2301.13808}
}

@misc{ott2023thoughtsourceacentral,
    author = {Simon Ott and Konstantin Hebenstreit and Valentin Liévin and Christoffer Egeberg Hother and Milad Moradi and Maximilian Mayrhauser and Robert Praas and Ole Winther and Matthias Samwald},
    title = {ThoughtSource: A central hub for large language model reasoning data},
    year = 2023,
    url = {https://arxiv.org/abs/2301.11596}
}

@misc{nye2021showyourwork,
    author = {Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
    title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
    year = 2021,
    url = {https://arxiv.org/abs/2112.00114}
}

@misc{lewis2020retrievalaugmented,
    author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
    title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
    year = 2020,
    url = {https://papers.nips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}
}

@misc{nogueira2021investigatingthelimitations,
    author = {Rodrigo Nogueira and Zhiying Jiang and Jimmy Lin},
    title = {Investigating the Limitations of Transformers with Simple Arithmetic Tasks},
    year = 2021,
    url = {https://arxiv.org/abs/2102.13019}
}

@misc{sutskever2014sequencetosequence,
    author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
    title = {Sequence to Sequence Learning with Neural Networks},
    year = 2014,
    url = {https://arxiv.org/abs/1409.3215}
}

@misc{wolf2019huggingfacestransformers,
    author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
    title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
    year = 2019,
    url = {https://arxiv.org/abs/1910.03771}
}

@misc{anonymous2022crammingtraininga,
    author = {Anonymous},
    title = {Cramming: Training a language model on a single GPU in one day},
    year = 2022,
    url = {https://openreview.net/forum?id=gUL6zYN4Uaf}
}

@misc{perez2022discoveringlanguage,
    author = {Ethan Perez and Sam Ringer and Kamilė Lukošiūtė and Karina Nguyen and Edwin Chen and Scott Heiner and Craig Pettit and Catherine Olsson and Sandipan Kundu and Saurav Kadavath and Andy Jones and Anna Chen and Ben Mann and Brian Israel and Bryan Seethor and Cameron McKinnon and Christopher Olah and Da Yan and Daniela Amodei and Dario Amodei and Dawn Drain and Dustin Li and Eli Tran-Johnson and Guro Khundadze and Jackson Kernion and James Landis and Jamie Kerr and Jared Mueller and Jeeyoon Hyun and Joshua Landau and Kamal Ndousse and Landon Goldberg and Liane Lovitt and Martin Lucas and Michael Sellitto and Miranda Zhang and Neerav Kingsland and Nelson Elhage and Nicholas Joseph and Noemí Mercado and Nova DasSarma and Oliver Rausch and Robin Larson and Sam McCandlish and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Brown and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Jack Clark and Samuel R. Bowman and Amanda Askell and Roger Grosse and Danny Hernandez and Deep Ganguli and Evan Hubinger and Nicholas Schiefer and Jared Kaplan},
    title = {Discovering Language Model Behaviors with Model-Written Evaluations},
    year = 2022,
    url = {https://arxiv.org/abs/2212.09251}
}

@misc{kalyan2021howmuchcoffee,
    author = {Ashwin Kalyan and Abhinav Kumar and Arjun Chandrasekaran and Ashish Sabharwal and Peter Clark},
    title = {How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI},
    year = 2021,
    url = {https://arxiv.org/abs/2110.14207}
}

@misc{honovich2022unnaturalinstructions,
    author = {Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
    title = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
    year = 2022,
    url = {https://arxiv.org/abs/2212.09689}
}

@misc{zhong2017seq2sqlgenerating,
    author = {Victor Zhong and Caiming Xiong and Richard Socher},
    title = {Seq2SQL: Generating Structured Queries from Natural Language using   Reinforcement Learning},
    year = 2017,
    url = {https://arxiv.org/abs/1709.00103}
}

@misc{pasupat2015compositionalsemantic,
    author = {Panupong Pasupat and Percy Liang},
    title = {Compositional Semantic Parsing on Semi-Structured Tables},
    year = 2015,
    url = {https://arxiv.org/abs/1508.00305}
}

@misc{brown2020languagemodelsare,
    author = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    title = {Language Models are Few-Shot Learners},
    year = 2020,
    url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@misc{chen2021evaluatinglargelanguage,
    author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
    title = {Evaluating Large Language Models Trained on Code},
    year = 2021,
    url = {https://arxiv.org/abs/2107.03374}
}

@misc{liu2021tapextablepre,
    author = {Qian Liu and Bei Chen and Jiaqi Guo and Morteza Ziyadi and Zeqi Lin and Weizhu Chen and Jian-Guang Lou},
    title = {TAPEX: Table Pre-training via Learning a Neural SQL Executor},
    year = 2021,
    url = {https://openreview.net/forum?id=O50443AsCP}
}

@misc{lewis2020bartdenoisingsequence,
    author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
    title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    year = 2020,
    url = {https://www.aclweb.org/anthology/2020.acl-main.703/},
    doi = {10.18653/v1/2020.acl-main.703}
}

@misc{mallen2022whennotto,
    author = {Alex Mallen and Akari Asai and Victor Zhong and Rajarshi Das and Hannaneh Hajishirzi and Daniel Khashabi},
    title = {When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories},
    year = 2022,
    url = {https://arxiv.org/abs/2212.10511}
}

@misc{wang2022executionbasedevaluation,
    author = {Zhiruo Wang and Shuyan Zhou and Daniel Fried and Graham Neubig},
    title = {Execution-Based Evaluation for Open-Domain Code Generation},
    year = 2022,
    url = {https://arxiv.org/abs/2212.10481}
}

@misc{lai2022ds1000a,
    author = {Yuhang Lai and Chengxi Li and Yiming Wang and Tianyi Zhang and Ruiqi Zhong and Luke Zettlemoyer and Scott Wen-tau Yih and Daniel Fried and Sida Wang and Tao Yu},
    title = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
    year = 2022,
    url = {https://arxiv.org/abs/2211.11501}
}

@misc{agashe2019juicealarge,
    author = {Rajas Agashe and Srinivasan Iyer and Luke Zettlemoyer},
    title = {JuICe: A Large Scale Distantly Supervised Dataset for Open Domain   Context-based Code Generation},
    year = 2019,
    url = {https://arxiv.org/abs/1910.02216}
}

@misc{yin2022naturallanguageto,
    author = {Pengcheng Yin and Wen-Ding Li and Kefan Xiao and Abhishek Rao and Yeming Wen and Kensen Shi and Joshua Howland and Paige Bailey and Michele Catasta and Henryk Michalewski and Alex Polozov and Charles Sutton},
    title = {Natural Language to Code Generation in Interactive Data Science Notebooks},
    year = 2022,
    url = {https://arxiv.org/abs/2212.09248}
}

@misc{zhong2022reasoningoverhybrid,
    author = {Wanjun Zhong and Junjie Huang and Qian Liu and Ming Zhou and Jiahai Wang and Jian Yin and Nan Duan},
    title = {Reasoning over Hybrid Chain for Table-and-Text Open Domain QA},
    year = 2022,
    url = {https://arxiv.org/abs/2201.05880}
}

@misc{zamani2022retrievalenhanced,
    author = {Hamed Zamani and Fernando Diaz and Mostafa Dehghani and Donald Metzler and Michael Bendersky},
    title = {Retrieval-Enhanced Machine Learning.},
    year = 2022,
    url = {https://doi.org/10.1145/3477495.3531722},
    doi = {10.1145/3477495.3531722}
}

@misc{parisi2022talmtoolaugmented,
    author = {Aaron Parisi and Yao Zhao and Noah Fiedel},
    title = {TALM: Tool Augmented Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2205.12255}
}

@misc{hashimoto2018aretrieveand,
    author = {Tatsunori B. Hashimoto and Kelvin Guu and Yonatan Oren and Percy Liang},
    title = {A Retrieve-and-Edit Framework for Predicting Structured Outputs},
    year = 2018,
    url = {https://arxiv.org/abs/1812.01194}
}

@misc{yin2018learningtomine,
    author = {Pengcheng Yin and Bowen Deng and Edgar Chen and Bogdan Vasilescu and Graham Neubig},
    title = {Learning to Mine Aligned Code and Natural Language Pairs from Stack   Overflow},
    year = 2018,
    url = {https://arxiv.org/abs/1805.08949}
}

@misc{asai2022taskawareretrieval,
    author = {Akari Asai and Timo Schick and Patrick Lewis and Xilun Chen and Gautier Izacard and Sebastian Riedel and Hannaneh Hajishirzi and Wen-tau Yih},
    title = {Task-aware Retrieval with Instructions},
    year = 2022,
    url = {https://arxiv.org/abs/2211.09260}
}

@misc{mitchell2022memorybasedmodel,
    author = {Eric Mitchell and Charles Lin and Antoine Bosselut and Christopher D. Manning and Chelsea Finn},
    title = {Memory-Based Model Editing at Scale},
    year = 2022,
    url = {https://arxiv.org/abs/2206.06520}
}

@misc{cao2021editingfactualknowledge,
    author = {Nicola De Cao and Wilker Aziz and Ivan Titov},
    title = {Editing Factual Knowledge in Language Models},
    year = 2021,
    url = {https://aclanthology.org/2021.emnlp-main.522/},
    doi = {10.18653/v1/2021.emnlp-main.522}
}

@misc{izacard2022atlasfewshot,
    author = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
    title = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2208.03299}
}

@misc{bueno2022inducednaturallanguage,
    author = {Mirelle Bueno and Carlos Gemmell and Jeffrey Dalton and Roberto Lotufo and Rodrigo Nogueira},
    title = {Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2208.11445}
}

@misc{hofstätter2022fidlightefficient,
    author = {Sebastian Hofstätter and Jiecao Chen and Karthik Raman and Hamed Zamani},
    title = {FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation},
    year = 2022,
    url = {https://arxiv.org/abs/2209.14290}
}

@misc{gao2022palprogramaided,
    author = {Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
    title = {PAL: Program-aided Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2211.10435}
}

@misc{anonymous2022teachingalgorithmic,
    author = {Anonymous},
    title = {Teaching Algorithmic Reasoning via In-context Learning},
    year = 2022,
    url = {https://openreview.net/forum?id=6dlC7E1H_9}
}

@misc{alipoormolabashi2022supernaturalinstructions,
    author = {Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and Arjun Ashok and Arut Selvan Dhanasekaran and Eshaan Pathak and Haizhi Lai and Kirby Kuznia and Krima Doshi and Maitreya Patel and Mihir Parmar and Mirali Purohit and Phani Rohitha Kaza and Ravsehaj Singh Puri and Savan Doshi and Sumanta Patro and Tanay Dixit and Xudong Shen},
    title = {Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
    year = 2022,
    url = {https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.340/}
}

@misc{efrat2022lmentryalanguage,
    author = {Avia Efrat and Or Honovich and Omer Levy},
    title = {LMentry: A Language Model Benchmark of Elementary Language Tasks},
    year = 2022,
    url = {https://arxiv.org/abs/2211.02069}
}

@misc{anonymous2022measuringandnarrowing,
    author = {Anonymous},
    title = {Measuring and Narrowing the Compositionality Gap in Language Models},
    year = 2022,
    url = {https://openreview.net/forum?id=PUwbwZJz9dO}
}

@misc{ma2022opendomainquestion,
    author = {Kaixin Ma and Hao Cheng and Xiaodong Liu and Eric Nyberg and Jianfeng Gao},
    title = {Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge},
    year = 2022,
    url = {https://arxiv.org/abs/2210.12338}
}

@misc{chung2022scalinginstruction,
    author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
    title = {Scaling Instruction-Finetuned Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2210.11416}
}

@misc{xie2022unifiedskgunifying,
    author = {Tianbao Xie and Chen Henry Wu and Peng Shi and Ruiqi Zhong and Torsten Scholak and Michihiro Yasunaga and Chien-Sheng Wu and Ming Zhong and Pengcheng Yin and Sida I. Wang and Victor Zhong and Bailin Wang and Chengzu Li and Connor Boyle and Ansong Ni and Ziyu Yao and Dragomir Radev and Caiming Xiong and Lingpeng Kong and Rui Zhang and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
    title = {UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2201.05966}
}

@misc{chen2022largelanguagemodels,
    author = {Wenhu Chen},
    title = {Large Language Models are few(1)-shot Table Reasoners},
    year = 2022,
    url = {https://arxiv.org/abs/2210.06710}
}

@misc{chen2020hybridqaadataset,
    author = {Wenhu Chen and Hanwen Zha and Zhiyu Chen and Wenhan Xiong and Hong Wang and William Wang},
    title = {HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},
    year = 2020,
    url = {https://arxiv.org/abs/2004.07347}
}

@misc{chen2019tabfactalarge,
    author = {Wenhu Chen and Hongmin Wang and Jianshu Chen and Yunkai Zhang and Hong Wang and Shiyang Li and Xiyou Zhou and William Yang Wang},
    title = {TabFact: A Large-scale Dataset for Table-based Fact Verification},
    year = 2019,
    url = {https://arxiv.org/abs/1909.02164}
}

@misc{chen2020openquestionanswering,
    author = {Wenhu Chen and Ming-Wei Chang and Eva Schlinger and William Wang and William W. Cohen},
    title = {Open Question Answering over Tables and Text},
    year = 2020,
    url = {https://arxiv.org/abs/2010.10439}
}

@misc{andrejczuk2022tabletotext,
    author = {Ewa Andrejczuk and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Yasemin Altun},
    title = {Table-To-Text generation and pre-training with TabT5},
    year = 2022,
    url = {https://arxiv.org/abs/2210.09162}
}

@misc{zhu2022reasonchainqatext,
    author = {Minjun Zhu and Yixuan Weng and Shizhu He and Kang Liu and Jun Zhao},
    title = {ReasonChainQA: Text-based Complex Question Answering with Explainable Evidence Chains},
    year = 2022,
    url = {https://arxiv.org/abs/2210.08763}
}

@misc{wang2022behaviorclonedtransformers,
    author = {Ruoyao Wang and Peter Jansen and Marc-Alexandre Côté and Prithviraj Ammanabrolu},
    title = {Behavior Cloned Transformers are Neurosymbolic Reasoners},
    year = 2022,
    url = {https://arxiv.org/abs/2210.07382}
}

@misc{wang2021learningtosynthesize,
    author = {Bailin Wang and Wenpeng Yin and Xi Victoria Lin and Caiming Xiong},
    title = {Learning to Synthesize Data for Semantic Parsing},
    year = 2021,
    url = {https://arxiv.org/abs/2104.05827}
}

@misc{shi2020onthepotential,
    author = {Tianze Shi and Chen Zhao and Jordan Boyd-Graber and Hal Daumé and Lillian Lee},
    title = {On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries},
    year = 2020,
    url = {https://arxiv.org/abs/2010.11246}
}

@misc{eisenschlos2020understandingtables,
    author = {Julian Martin Eisenschlos and Syrine Krichene and Thomas Müller},
    title = {Understanding tables with intermediate pre-training},
    year = 2020,
    url = {https://arxiv.org/abs/2010.00571}
}

@misc{khashabi2020unifiedqacrossing,
    author = {Daniel Khashabi and Sewon Min and Tushar Khot and Ashish Sabharwal and Oyvind Tafjord and Peter Clark and Hannaneh Hajishirzi},
    title = {UNIFIEDQA: Crossing Format Boundaries with a Single QA System},
    year = 2020,
    url = {https://www.aclweb.org/anthology/2020.findings-emnlp.171/}
}

@misc{andor2019givingberta,
    author = {Daniel Andor and Luheng He and Kenton Lee and Emily Pitler},
    title = {Giving BERT a Calculator: Finding Operations and Arguments with Reading   Comprehension},
    year = 2019,
    url = {https://arxiv.org/abs/1909.00109}
}

@misc{chemmengath2021topictransferable,
    author = {Saneem Ahmed Chemmengath and Vishwajeet Kumar and Samarth Bharadwaj and Jaydeep Sen and Mustafa Canim and Soumen Chakrabarti and Alfio Gliozzo and Karthik Sankaranarayanan},
    title = {Topic Transferable Table Question Answering},
    year = 2021,
    url = {https://arxiv.org/abs/2109.07377}
}

@misc{jiang2022omnitabpretraining,
    author = {Zhengbao Jiang and Yi Mao and Pengcheng He and Graham Neubig and Weizhu Chen},
    title = {OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering},
    year = 2022,
    url = {https://arxiv.org/abs/2207.03637}
}

@misc{wei2022chainofthought,
    author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
    title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
    year = 2022,
    url = {https://arxiv.org/abs/2201.11903}
}

@misc{rugery2020attentionisall,
    author = {Pierrick RUGERY},
    title = {Attention is all you need},
    year = 2020,
    url = {https://becominghuman.ai/attention-is-all-you-need-16bf481d8b5c}
}

@misc{raffel2020exploringthelimits,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li 0133 and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.},
    year = 2020,
    url = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{herzig2018decouplingstructure,
    author = {Jonathan Herzig and Jonathan Berant},
    title = {Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing},
    year = 2018,
    url = {https://www.aclweb.org/anthology/D18-1190/},
    doi = {10.18653/v1/D18-1190}
}
