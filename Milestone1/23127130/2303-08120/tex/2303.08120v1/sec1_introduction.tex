
\begin{figure*}[t]
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
% &
\rotatebox{90}{\hspace{1.2mm} \scriptsize{Frame 1}   }&

\includegraphics[height=0.069\linewidth]{Figure/teaser2/oldmovie_00000.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/oldanimie_00024.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/timelapse_00000.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/slowmo0.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/colorization_00000.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/spatial_00000.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/intrinsic_00000.jpg}&
\includegraphics[height=0.069\linewidth, width=0.112\linewidth]{Figure/teaser2/00014.jpg}
\\
% Frame 2&
\rotatebox{90}{\hspace{1.2mm}  \scriptsize{Frame 2}  }&

\includegraphics[height=0.069\linewidth]{Figure/teaser2/oldmovie_00078.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/oldanimie_00025.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/timelapse_00006.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/slowmo1.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/colorization_00010.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/spatial_00020.jpg}&
\includegraphics[height=0.069\linewidth]{Figure/teaser2/intrinsic_00020.jpg}&
\includegraphics[height=0.069\linewidth, width=0.112\linewidth]{Figure/teaser2/00015.jpg}
\\
& Old movies&Old cartoons & Time-lapse& Slow-motion& Colorization&  White balance& Intrinsic&Text-to-video\\
\end{tabular}
\vspace{-0.5em}
\caption{\textbf{Videos with flickering artifacts.} Flickering artifacts exist in unprocessed videos, including old movies, old cartoons, time-lapse videos, and slow-motion videos. Besides, some processing algorithms~\cite{zhang2016colorful,he2010single,bell2014intrinsic} can introduce the flicker to temporally consistent unprocessed videos. Synthesized videos from video generations approaches~\cite{singer2022make,zhou2022magicvideo,DBLP:conf/cvpr/RenW22} also might be temporal inconsistent. \textit{Blind deflickering} aims to remove various types of flickering with only an unprocessed video as input.}
\label{fig:flicker_video}
\vspace{-1em}
\end{figure*}

\vspace{-3em}

\section{Introduction}
% What is the problem? Why is it important?
% Temporal consistent videos are preferred for human but many videos suffer from flickering due to various reasons.
A high-quality video is usually temporally consistent, but many videos suffer from flickering artifacts for various reasons, as shown in Figure~\ref{fig:flicker_video}. For example, the brightness of old movies can be very unstable since some old cameras with low-quality hardware cannot set the exposure time of each frame to be the same~\cite{delon2010stabilization}. Besides, high-speed cameras with very short exposure time can capture the high-frequency (e.g., 60 Hz) changes of indoor lighting~\cite{kanj2017flicker}. Effective processing algorithms such as enhancement~\cite{marki2016bilateral,DBLP:journals/corr/abs-2012-05228}, colorization~\cite{zhang2016colorful,lei2019fully}, and style transfer~\cite{li2017universal} might bring flicker when applied to temporally consistent videos. Videos from video generations approaches~\cite{singer2022make,zhou2022magicvideo,DBLP:conf/cvpr/RenW22} also might contain flickering artifacts. Since temporally consistent videos are generally more visually pleasing, removing the flicker from videos is highly desirable in video processing~\cite{xie2020video,zhang2019deepcolor,chu2018temporally,chen2019seeing,claus2019videnn} and computational photography. 


In this work, we are interested in a general approach for deflickering: (1) it is agnostic to the patterns or levels of flickering (e.g., old movies, high-speed cameras, processing artifacts), (2) it only takes a single flickering video and does not require other guidance (e.g., flickering types, extra consistent videos). That is to say, this model is blind to flickering types and guidance, and we name this task as \textit{blind deflickering}. Thanks to the blind property, blind deflickering has very wide applications.

% Why is the problem challenge? What makes it challenging? % How far has existing work done? Why hasn't the problem be solved?

Blind deflickering is very challenging since it is hard to enforce temporal consistency across the whole video without any extra guidance. Existing techniques usually design specific strategies for each flickering type with specific knowledge. For example, for slow-motion videos captured by high-speed cameras, prior work~\cite{kanj2017flicker} can analyze the lighting frequency. For videos processed by image processing algorithms, blind video temporal consistency~\cite{DBLP:conf/nips/dvp,lei2022deep} obtains long-term consistency by training on a temporally consistent unprocessed video. However, the flickering types or unprocessed videos are not always available, and existing flickering-specific algorithms cannot be applied in this case. One intuitive solution is to use the optical flow to track the correspondences. However, the optical flow from the flickering videos is not accurate, and the accumulated errors of optical flow are also increasing with the number of frames due to inaccurate estimation~\cite{bonneel2015blind}.

With two key observations and designs, we successfully propose the first approach for \textit{blind deflickering} that can remove various flickering artifacts without extra guidance or knowledge of flicker. First, we utilize a unified video representation named neural atlas~\cite{kasten2021layered} to solve the major challenge of solving long-term inconsistency. This neural atlas tracks all pixels in the video, and correspondences in different frames share the same pixel in this atlas. Hence, a sequence of temporally consistent frames can be obtained by sampling from the shared atlas. Secondly, while the frames from the shared atlas are consistent, the structures of images are flawed: the neural atlas cannot easily model dynamic objects with large motion; the optical flow used to construct the atlas is imperfect. Hence, we propose a neural filtering strategy to take the treasure and throw the trash from the flawed atlas. A neural network is trained to learn the invariant under two types of distortion, which mimics the artifacts in the atlas and the flicker in the video, respectively. At test time, this network works well as a filter to preserve the consistency property and block the artifacts from the flawed atlas.

% What does the experiments say?
We construct the first dataset containing various types of flickering videos to evaluate the performance of blind deflickering methods faithfully. Extensive experimental results show the effectiveness of our approach in handling different flicker. Our approach also outperforms blind video temporal consistency methods that use an extra input video as guidance on a public benchmark. 

Our contributions can be summarized as follows:\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item We formulate the problem of blind deflickering and construct a deflickering dataset containing diverse flickering videos for further study.
\item We propose the first blind deflickering approach to remove diverse flicker. We introduce the neural atlas to the deflickering problem and design a dedicated strategy to filter the flawed atlas for satisfying deflickering performance.
\item Our method outperforms baselines on our dataset and even outperforms methods that use extra input videos on a public benchmark.

\end{itemize}
