\section{Method}
\label{sec:background}
\subsection{Overview}
\label{subsec:overview}

Let $\{I_t\}_{t=1}^T$ be the input video sequences with flickering artifacts where $T$ is the number of video frames. Our approach aims to remove the flickering artifacts to generate a temporally consistent video $\{O_t\}_{t=1}^T$. Flicker denotes a type of temporal inconsistency that correspondences in different frames share different features (e.g., color, brightness). The flicker can be either global or local in the spatial dimension, either long-term or short-term in the temporal dimension.



Figure~\ref{fig:framework} shows the framework of our approach. We tackle the problem of blind deflickering through the following key designs: 
$(i)$ We first propose to use a single \textit{neural atlas (Section~\ref{subsec:atlas})} for the deflickering task. 
$(ii)$ We design a \textit{neural filtering (Section~\ref{subsec:filter})} strategy for the neural atlas as the single atlas is inevitably flawed. 


% \textcolor{gray}{Enforcing long-term consistency across the whole video is challenging in blind deflickering. First, most architecture in video processing can only take a small number of frames as input, which results in a limited receptive field and is insufficient for long-term consistency. Secondly, optical flow estimation is more challenging for videos with flickering artifacts. As discussed in Bonneel et al.~\cite{bonneel2015blind}, simply warping the first frame with the optical flow to the last frame in a recurrent way is infeasible due to the accumulated errors of estimated flow. At last, unlike Lei et al.~\cite{DBLP:conf/nips/dvp}, blind deflickering methods do not have a temporally consistent video as guidance. 
% }




\subsection{Flawed Atlas Generation}
\label{subsec:atlas}

% What is the neural atlas?

% Why single layer?


\noindent\textbf{Motivation.}
A good blind deflickering model should have the capacity to track correspondences across all the video frames. Most architectures in video processing can only take a small number of frames as input, resulting in a limited receptive field that is insufficient for long-term consistency. To this end, we introduce neural atlases~\cite{kasten2021layered} to the deflickering task as we observe it perfectly fits this task. A neural atlas is a unified and concise representation of all the pixels in a video. As shown in Figure~\ref{fig:framework}(a), let $p =(x,y,t) \in \mathbb{R}^3 $ be a pixel that locates at $(x,y)$ in frame $I_t$. Each pixel $p$ is fed into a mapping network $\mathbb M$ that predicts a 2D coordinate $(u^p,v^p)$ representing the corresponding position of the pixel in the atlas. Ideally, the correspondences in different frames should share a pixel in the atlas, even if the color of the input pixel is different. That is to say, temporal consistency is ensured. 


% as correspondences in different frames share the pixel in the atlas.


% For a perfect atlas, every pixel in video frames can find a position in the atlas. 
% With this key design, our approach can solve a major challenge in blind deflickering: long-term temporal consistency without the guidance of consistent video. 
% In this part, we provide the details for the generation of the atlas. 
\noindent\textbf{Training.} Figure~\ref{fig:framework}(a) shows the pipeline to generate the atlas. For each pixel $p$, we have:
% \red{(why?)
\begin{align}
(u^p,v^p)&=\mathbb M(p), \\
c^p & = \mathbb{A}(\phi(u^p),\phi(v^p)).
\end{align}
The 2D coordinate $(u^p,v^p)$ is fed into an atlas network $\mathbb A$ with positional encoding function $\phi(\cdot)$ to estimate the RGB color $c^p$ for the pixel. The mapping network $\mathbb M$ and the atlas network $\mathbb A$ are trained jointly by optimizing the loss between the original RGB color and the predicted color $c^p$. 
Besides the reconstruction term, a consistency loss is also employed to encourage the corresponding pixels in different frames to be mapped to the same atlas position. We follow the implementation of loss functions in \cite{kasten2021layered}. 
% Note that the use of continuous implicit functions in extracting the atlas enables us to use noisy optical flow and deal with occlusion. 
% The predicted 2D coordinate $(u^p,v^p)$ from correspondences in different frames is trained to be consistent with a consistency loss. 
% This help us that we can use noise optical flow with continuous implicit function.
% actually this is a key difference with Task-agonistic temporal learning. 
After training the networks $\mathbb M$ and $\mathbb A$, we can reconstruct the videos $\{A_t\}_{t=1}^T$ by providing all the coordinates of pixels in the whole video. The reconstructed atlas-based video $\{A_t\}_{t=1}^T$ is temporally consistent as all the pixels are mapped from a single atlas. In this work, we utilize the temporal consistency property of this video for blind deflickering. 



Considering the trade-off between performance and efficiency, we only use a single-layer atlas to represent the whole video, although using two layers (background layer and foreground layer)  or multiple layers of atlases might slightly improve the performance. First, in practice, we notice the number of layers is quite different, which varies from a single layer to multiple layers (more than two), making it challenging to apply them to diverse videos automatically. Besides, we notice that artifacts and distortion are inevitable for many scenes, as discussed in \cite{kasten2021layered}. We present how to handle the artifacts in the flawed atlas with the following network designs in Secion~\ref{subsec:filter}.




\subsection{Neural Filtering and Refinement}
\label{subsec:filter}

\noindent \textbf{Motivation.} 
The neural atlas contains not only the treasure but also the trash. In Section~\ref{subsec:atlas}, we argue that an atlas is a strong cue for blind deflickering since it can provide consistent guidance across the whole video. However, the reconstructed frames from the atlas are flawed. First, as analyzed in NLA~\cite{kasten2021layered}, it cannot perform well when the object is moving rapidly, or multiple layers might be required for multiple objects. For blind deflickering, we need to remove the flicker and avoid introducing new artifacts. Secondly, the optical flow obtained by the flickering video is not accurate, which leads to more flaws in the atlas. At last, there are still some natural changes, such as shadow changes, and we hope to preserve the patterns. 

Hence, we design a \textit{neural filtering} strategy that utilizes the promising temporal consistency property of the atlas-based video and prevents the artifacts from destroying our output videos.




\begin{figure}[t]
\centering
\begin{tabular}{@{}c@{}}
% \includegraphics[width=0.9\linewidth]{LaTeX/Figures/sec4/framework.jpg}
\includegraphics[width=1.0\linewidth]{Figure/method/augumentation_pipeline_2_crop.pdf}
\end{tabular}
\vspace{-0.5em}
\caption{\textbf{Training pipeline of our neural filtering strategy.} We apply two transformations to a clean image $X$ for mimicking the flickering input frame and the flawed atlas frame.}
\label{fig:NeuralFilter}
\vspace{-1em}
\end{figure}

% why we use single-layer atlas instead of multi-layer atlas
% 1) multi need annotation
% 2) multi still have artifact (optinal) -> with an ablation study is good
% \paragraph{Discussion.} While our training scheme is inspired by NeuralAtlas, we choose different designs to achieve our goal. 
\noindent \textbf{Training Strategy.} 
Figure~\ref{fig:framework}(b) shows the framework to use the atlas. Given an input video $\{I_t\}_{t=1}^T$ and an atlas $A$, in every iteration, we get one frame $I_t$ from the video and input it to the filter network $\mathbb F$:
\begin{align}
    O_t^f = {\mathbb F}(I_t, A_t),
\end{align}
where $A_t$ is obtained by fetching pixels from the shared atlas $A$ with the coordinates of $I_t$. 

We design a dedicated training strategy for the flawed atlas, as shown in Figure~\ref{fig:NeuralFilter}. We train the network using only a single image $X$ instead of consecutive frames. In the training time, we apply a transformation $\tau_a(\cdot)$ to distort the appearance, including color, saturation, and brightness of the image, which mimics the flickering pattern in $I_t$. We apply another transformation $\tau_s(\cdot)$ to distort the structures of the image, mimicking the distortion of a flawed atlas-based frame in $A_t$.  
At last, the network is trained by minimizing the L2 loss function $\mathcal{L}$ between prediction ${\mathbb F}(\tau_a(X), \tau_s(X))$ and the clean ground truth $X$ (i.e., the image before augmentation):
\begin{align}
    \mathcal{L} = ||{\mathbb F}(\tau_a(X), \tau_s(X);\theta_{\mathbb F}) - X||_2^2,
\end{align}
where $\theta_{\mathbb F}$ is the parameters of filtering network $\mathbb F$. 

The network tends to learn the invariant part from two distorted views respectively. Specifically, ${\mathbb F}$ learns the structures from the input frame $I_t$ and the appearance (e.g., brightness, color) from the atlas frame $A_t$ as they are invariant to the structure distortion $\tau_s$. At the same time, the distortion of $\tau_s(X)$ would not be passed through the network $\mathbb F$. With this strategy, we achieve the goal of neural filtering with the flawed atlas.

Note that while this network ${\mathbb F}$ only receives one frame, long-term consistency can be enforced since the temporal information is encoded in the atlas-based frame $A_t$.

\noindent \textbf{Local refinement.}
\label{subsec:local}
The video frames $\{O_t^f\}_{t=1}^T$ are consistent with each other globally in both the short term and the long term. However, it might contain local flicker due to the misalignment between input and atlas-based frames. Hence, we use an extra local deflickering network to refine the results further. Prior work has shown that local flicker can be well addressed by a flow-based regularization. We thus choose a lightweight pipeline~\cite{lai2018learning} with modification. As shown in Figure~\ref{fig:framework}(b), we predict the output frame $O_t$ by providing two consecutive frames $O_t^f, O_{t-1}^f$ and previous output $O_{t-1}$ to our local refinement network ${\mathbb L}$. Two consecutive frames are firstly followed by a few convolution layers and then fused with the $O_{t-1}$. The local flickering network is trained with a simple temporal consistency loss $\mathcal L_{local} $ to remove local flickering artifacts:
\begin{align}
    \mathcal{L}_{local}(O_t,O_{t-1}) &=  ||M_{t,t-1} \odot (O_t - \hat{O}_{t-1})||_1 ,
\end{align}
where $\hat{O}_{t-1}$ is obtained by warping the $O_{t-1}$ with the optical flow from frame $t$ to frame $t-1$. $M_{t,t-1}$ is the corresponding occlusion mask. For the frames without local artifacts, the output should $O_t$ be the same as $O_t^f$. Hence, we also provide a reconstruction loss by minimizing the distance between $O_t$ and $O_t^f$ to regularize the quality.

% According to our experiments, while using this local deflickering network individually achieves poor performance, combining it with our atlas-guided network improves the performance effectively. 

\noindent \textbf{Implementation details.}
The network ${\mathbb F}$ is trained on the MS-COCO dataset~\cite{coco} as we only need images for training. We train it for $20$ epochs with a batch size of $8$. For the network ${\mathbb L}$, we train it on the processed DAVIS dataset~\cite{davis,lai2018learning} for $50$ epochs with a batch size of $8$. We adopt the Adam optimizer and set the learning rate to 0.0001.
% We notice train
% For more details, please refer to supplementary materials.
% We train the local network $l$ for 100 epochs with a batch size 8. We adopt the Adam optimizer and set the learning rate to 0.0001. According to our experiments, while using this local deflickering network individually achieve poor performance, combining it with our atlas-guided network improves the performance effectively. 




\begin{table*}[t]
\centering
\begin{tabular}{cc}%
\hfill
\begin{minipage}{0.62\textwidth}
\centering
\renewcommand{\arraystretch}{1.1}
% \resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{lccccc}
\toprule
% \hline
% \rowcolor{white}
% \multirow{2}{*}{Task} 
 \cellcolor{white}Video type    & \multicolumn{3}{c}{$E_{warp} \downarrow$} & \multicolumn{2}{c}{ PSNR $\uparrow$}\\
% \rowcolor{white}
 \cellcolor{white} & {Raw video} & ConvLSTM &  {Ours} & ConvLSTM &  {Ours} \\
 \hline
 % \midrule
 Synthetic & 0.163 & 0.148 & \textbf{0.088} & 21.84 & \textbf{26.46} \\
  \qquad \footnotesize{-- $W = 1$}& 0.199 & 0.168 & \textbf{0.091} & 19.84 & \textbf{27.75} \\
 \qquad \footnotesize{-- $W = 3$}& 0.158 & 0.151 & \textbf{0.086} & 21.71 & \textbf{26.40} \\
 \qquad \footnotesize{-- $W = 10$} & 0.132 & 0.124 & \textbf{0.086} &  23.98 & \textbf{25.21} \\
 % \hline
 \hline
 Processed & 0.128 & 0.118 & \textbf{0.094} & -- & --\\

 \bottomrule
\end{tabular}
% }
\caption*{(a) Quantitative comparison} 

\label{fig:plogp}
\end{minipage}
&
\begin{minipage}{0.33\textwidth}
\centering
\renewcommand{\arraystretch}{1.1}
% \vspace{6mm}
% \resizebox{\linewidth}{!}{ 
% \begin{tabular}{a@{\hspace{3mm}}b@{\hspace{5mm}}b}
\begin{tabular}{lbb}
\toprule
% \hline
% \rowcolor{white}
\cellcolor{white}Video type & \multicolumn{2}{c}{Preference rate} \\
\cellcolor{white} & ConvLSTM & Ours  \\
% \midrule
\hline
{Old movies} & 38.0\% & \textbf{62.0\%} \\
{Old cartoons} & 33.6\% & \textbf{66.4\%} \\
Time-lapse & 31.9\% & \textbf{68.1\%} \\
Slow-motion & 29.0\% & \textbf{71.0\%} \\
\hline
Average & 33.7\% & \textbf{66.3\%} \\
\bottomrule
% \hline
\end{tabular}
% }
% \vspace{0mm}
\caption*{(b) User study} 
\end{minipage}%
\end{tabular}
% \vspace{-0.6em}
\caption{\textbf{Comparison to baselines.} We provide the (a) quantitative comparison for processed and synthetic videos since we have the high-quality optical flow for computing the evaluation metric. The warping errors of our approach are much smaller than the baseline, and our results are more similar to the ground truth on synthetic data, according to the PSNR. For the other real-world videos that cannot provide high-quality optical flow, we provide the (b) user study results for comparison. Our results are preferred by most users. } 
\label{table:comparison_ourdata}
% \vspace{-2mm}
\end{table*}


\begin{figure*}[t]
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}


\rotatebox{90}{\small \hspace{2mm}  }&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/input/00174.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/convlstm/00174.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/atlas/00175_resize.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/ours/00174.jpg}\\
\rotatebox{90}{\small \hspace{2mm}  }&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/input/00192.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/convlstm/00192.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/atlas/00193_resize.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/Lily/ours/00192.jpg}\\

\rotatebox{90}{\small \hspace{2mm}  }&
\includegraphics[width=0.230\linewidth]{Figure/results/synthetic_camel/input/00039.jpg}&
\includegraphics[width=0.230\linewidth]
{Figure/results/synthetic_camel/convlstm/00039.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/synthetic_camel/atlas/00039.jpg}&
\includegraphics[width=0.230\linewidth,height=0.130\linewidth]{Figure/results/synthetic_camel/ours/00039.jpg}\\

\rotatebox{90}{\small \hspace{2mm}  }&
\includegraphics[width=0.230\linewidth]{Figure/results/synthetic_camel/input/00089.jpg}&
\includegraphics[width=0.230\linewidth]
{Figure/results/synthetic_camel/convlstm/00089.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/synthetic_camel/atlas/00089.jpg}&
\includegraphics[width=0.230\linewidth,height=0.130\linewidth]{Figure/results/synthetic_camel/ours/00089.jpg}\\

\rotatebox{90}{\small \hspace{2mm}  }&
\includegraphics[width=0.230\linewidth]{Figure/results/old_movie_machester/input/00009.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/old_movie_machester/convlstm/00009.jpg}&
\includegraphics[width=0.230\linewidth,height=0.126\linewidth]{Figure/results/old_movie_machester/atlas/00009.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/old_movie_machester/ours/00009.jpg}\\

\rotatebox{90}{\small \hspace{2mm}  }&
\includegraphics[width=0.230\linewidth]{Figure/results/old_movie_machester/input/00057.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/old_movie_machester/convlstm/00057.jpg}&
\includegraphics[width=0.230\linewidth,height=0.126\linewidth]
{Figure/results/old_movie_machester/atlas/00057.jpg}&
\includegraphics[width=0.230\linewidth]{Figure/results/old_movie_machester/ours/00057.jpg}\\

&Input & \small{ConvLSTM}  & \small{Ours atlas only} & \small{Ours} \\
\end{tabular}
\\
% \vspace{-0.5em}
\caption{\textbf{Qualitative comparisons to baselines.} Our results outperforms the baseline ConvLSTM significantly on various flickering videos. We highly encourage readers to see videos in our project website.}

\label{fig:baselines}
\vspace{-1em}
\end{figure*}

