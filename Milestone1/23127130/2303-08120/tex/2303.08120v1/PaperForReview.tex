% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{diagbox}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{graphbox} 
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{collcell}
\definecolor{mygray}{gray}{.9}

\newcolumntype{a}{>{\columncolor{mygray}}l}
\newcolumntype{b}{>{\columncolor{white}}c}

\usepackage{color}
\definecolor{citecolor}{HTML}{0071bc}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=citecolor,bookmarks=false]{hyperref}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2460} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\usepackage{color}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Blind Video Deflickering by Neural Filtering with a Flawed Atlas}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\author{Chenyang Lei$^{1,2}$\thanks{Equal contribution} 
\quad  Xuanchi Ren$^{3,4}$\footnotemark[1]
\quad  Zhaoxiang Zhang$^1$ 
\quad  Qifeng Chen$^5$\\
$^1$CAIR, HKISI-CAS  \quad $^2$Princeton University  \quad $^3$University of Toronto \quad $^4$Vector Institute \quad $^5$HKUST \\
}
% \maketitle

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
\vspace{-1.5 em}
\renewcommand\arraystretch{0.5} 
\centering
% \begin{tabular}{c@{\hspace{1mm}}c@{\hspace{0.5mm}}c@{\hspace{0.5mm}}c}
% \rotatebox{90}{\small \hspace{10mm} Input } &
% \includegraphics[width=0.32\linewidth]{Figure/teaser/Input/Betty/00000.jpg}&
% \includegraphics[width=0.32\linewidth]{Figure/teaser/Input/Betty/00009.jpg}&
% \includegraphics[width=0.32\linewidth]{Figure/teaser/Input/Betty/00035.jpg}\\
% \rotatebox{90}{\small \hspace{10mm} Output }&
% \includegraphics[width=0.32\linewidth]{Figure/teaser/Ours/Betty/00000.jpg}&
% \includegraphics[width=0.32\linewidth]{Figure/teaser/Ours/Betty/00009.jpg}&
% \includegraphics[width=0.32\linewidth]{Figure/teaser/Ours/Betty/00035.jpg}\\
% \end{tabular}

\begin{tabular}{c@{\hspace{1mm}}c@{\hspace{0.5mm}}c@{\hspace{0.5mm}}c@{\hspace{0.5mm}}c}
\rotatebox{90}{\small \hspace{8mm} Input } &
\includegraphics[width=0.24\linewidth]{Figure/teaser/Input/Betty/00000.jpg}&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Input/Betty/00009.jpg}&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Input/Betty/00035.jpg}&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Input/Betty/00071.jpg}\\

\rotatebox{90}{\small \hspace{6mm} Output }&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Ours/Betty/00000.jpg}&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Ours/Betty/00009.jpg}&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Ours/Betty/00035.jpg}&
\includegraphics[width=0.24\linewidth]{Figure/teaser/Ours/Betty/00071.jpg}\\
\end{tabular}

\vspace{-0.5em}
\captionof{figure}{\textbf{Blind deflickering performance of our approach on \textit{Betty Boop} (1934).} Many videos can have flickering artifacts for various
reasons. Our approach takes only the input video and removes the flicker automatically without any extra guidance.}
% \vspace{-2mm}
\label{fig:teaser}
\end{center}
}]

{
  \renewcommand{\thefootnote}%
    {\fnsymbol{footnote}}
  \footnotetext[1]{Equal contribution.}
}

\begin{abstract}
Many videos contain flickering artifacts; common causes
of flicker include video processing algorithms, video generation
algorithms, and capturing videos under specific situations.
Prior work usually requires specific guidance such
as the flickering frequency, manual annotations, or extra
consistent videos to remove the flicker. In this work, we
propose a general flicker removal framework that only receives
a single flickering video as input without additional
guidance. Since it is blind to a specific flickering type or
guidance, we name this “blind deflickering.” The core of
our approach is utilizing the neural atlas in cooperation
with a neural filtering strategy. The neural atlas is a unified
representation for all frames in a video that provides
temporal consistency guidance but is flawed in many cases.
To this end, a neural network is trained to mimic a filter
to learn the consistent features (e.g., color, brightness) and
avoid introducing the artifacts in the atlas. To validate our
method, we construct a dataset that contains diverse real-world
flickering videos. Extensive experiments show that
our method achieves satisfying deflickering performance
and even outperforms baselines that use extra guidance on
a public benchmark. The source code is publicly available at \url{https://chenyanglei.github.io/deflicker}.
\end{abstract}



%%%%%%%%% BODY TEXT
\input{sec1_introduction}
\input{sec2_relatedwork}
\input{sec3_method}
\input{sec4_dataset}
\input{sec5_experiment}

\section{Conclusion}
In this paper, we define a problem named \textit{blind deflickering} that can remove diverse flickering artifacts without knowing the specific flickering type and extra guidance. We propose the first dedicated approach for this task. The core of our approach is to adopt a neural atlas with a neural filtering strategy. The neural atlas concisely extracts all pixels in the videos and provides strong guidance to enforce long-term consistency, but it is flawed in many regions. We then use a neural network to filter the flaws of the atlas for satisfying performance. We conduct extensive experiments to evaluate the deflickering performance. Results show that our approach outperforms baselines significantly on different datasets, and controlled experiments validate the effectiveness of our key designs.


\section*{Acknowledgement}
\noindent This work was supported by the InnoHK program.

% \clearpage

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
% supplement
% detailed introduction of atlas paper

\end{document}