\section{Related Work}

\noindent \textbf{Task-specific deflikcering.} Different strategies are designed for specific flickering types. Kanj et al.~\cite{kanj2017flicker} propose a strategy for high-speed cameras. Delon et al.~\cite{delon2010stabilization} present a method for local contrast correction, which can be utilized for old movies and biological film sequences. Xu et al.~\cite{DBLP:conf/eccv/XuAH22} focus on temporal flickering artifacts from GAN-based editing. Videos processed by a specific image-to-image translation algorithm~\cite{isola2017image,CycleGAN2017,he2010single,zhang2016colorful,lei2020polarized,bell2014intrinsic,li2017universal,ouyang2021neural} can suffer from flickering artifacts, and blind video temporal consistency~\cite{bonneel2015blind,lang2012practical,lai2018learning,DBLP:conf/nips/dvp,yao2017occlusion,lei2022deep} is designed to remove the flicker for these processed videos. These approaches are blind to a specific image processing algorithm. However, the temporal consistency of generated frames is guided by a temporal consistent unprocessed video. Bonneel et al.\cite{bonneel2015blind} compute the gradient of input frames as guidance. Lai et al.\cite{lai2018learning} input two consecutive input frames as guidance. Lei et al.~\cite{DBLP:conf/nips/dvp} directly learn the mapping function between input and processed frames. While these approaches achieve satisfying performance on many tasks, a temporally consistent video is not always available. For example, for many flickering videos such as old movies and synthesized videos from video generation methods~\cite{ho2022imagen,singer2022make,zhou2022magicvideo}, the original videos are temporal inconsistent. A concurrent preprint~\cite{abs-2206-03753} attempts to eliminate the need for unprocessed videos during inference. Nonetheless, it still relies on optical flow for local temporal consistency and does not study other types of flickering videos. Our approach has a wider application compared with blind temporal consistency. 

Also, some commercial software~\cite{revision,flicker_free} can be used for deflickering by integrating various task-specific deflickering approaches. However, these approaches require users to have a knowledge background of the flickering types. Our approach aims to remove this requirement so that more videos can be processed efficiently for most users.





\noindent \textbf{Video mosaics and neural atlas.}
Inheriting from panoramic image stitching~\cite{BrownL07}, video mosaicing is a technique that organizes video data into a compact mosaic representation, especially for dynamic scenes.
It supports various applications, including video compression~\cite{IraniAH95}, video indexing~\cite{IraniA98}, video texture~\cite{AgarwalaZPACCSS05}, 2D to 3D conversion~\cite{RiberaCKLN12, SchnyderWS11}, and video editing~\cite{Rav-AchaKRF08}. 
Building video mosaics based on homography warping often fails to depict motions. To handle the dynamic contents, researchers compose foreground and background regions by spatio-temporal masks~\cite{CorreaM10}, or blend the video into a multi-scale tapestry in a patch-based manner~\cite{BarnesGSF10}. 
However, these approaches heavily rely on image appearance information and thus are sensitive to lighting changes and flicker. 

Recently, aiming for consistent video editing, Kasten et al.~\cite{kasten2021layered} propose Neural Layered Atlas (NLA), which decomposes a video into a set of atlases by learning mapping networks between atlases and video frames.
Editing on the atlas and then reconstructing frames from the atlas can achieve consistent video editing.
Follow-up work validates its power on text-driven video stylization~\cite{abs-2206-12396,text2live} and face video editing~\cite{LiuCLLJFG22}. 
Directly adopting NLA to blind deflickering tasks is not trivial and is mainly limited by two-fold: 
\textit{(i)} its performance on the complex scene is still not satisfying with notable artifacts.
\textit{(ii)} it requires segmentation masks as guidance for decomposing dynamic objects, and each dynamic object requires an additional mapping network. 
To facilitate automatic deflickering, we use a single-layered atlas without the need for segmentation masks by designing an effective neural filtering strategy for the flawed atlas. Our proposed strategy is also compatible with other atlas generation techniques. 


\begin{figure*}[t]
\centering
\begin{tabular}{@{}c@{}}
\includegraphics[width=1.0\linewidth]{Figure/method/merge_pipeline_5_crop.pdf}
\end{tabular}
\vspace{-0.5em}
\caption{\textbf{The framework of our approach}. We first generate an atlas as a unified representation of the whole video, providing consistent guidance for deflickering. Since the atlas is flawed, we then propose a neural filtering strategy to filter the flaws. }
\label{fig:framework}
\vspace{-1.2em}
\end{figure*}

\noindent \textbf{Implicit image/video representations.}
With the success of using the multi-layer perceptron (MLP) as a continuous implicit representation for 3D geometry~\cite{MeschederONNG19,nerf,ParkFSNL19}, such representation has gained popularity for representing images and videos~\cite{DBLP:conf/nips/SitzmannMBLW20, NERV, LiNSW21, TancikSMFRSRBN20}. Follow-up work extends these models to various tasks, such as image super-resolution~\cite{ChenL021}, video decomposition~\cite{DBLP:conf/cvpr/YeL0KS22}, and semantic segmentation~\cite{DBLP:conf/eccv/HuCXBCPW22}. 
In our work, we follow \cite{kasten2021layered} to employ a coordinate-based MLP to represent the neural atlases. 
