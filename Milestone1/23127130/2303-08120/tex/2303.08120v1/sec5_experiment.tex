\section{Experiments}

\subsection{Evaluation Setup} 

\noindent \textbf{Dataset.} We mainly use our constructed Blind Deflickering Dataset for evaluation. Details are presented in Section~\ref{sec:dataset}.


\noindent \textbf{Evaluation metric.} We measure the temporal inconsistency based on the warping error used in DVP~\cite{DBLP:conf/nips/dvp} that considers both short-term and long-term warping errors for quantitative evaluation. Given a pair of frames $O_t$ and $O_s$, the warping error $E_{pair}$ can be calculated by:
\begin{align}
&E_{pair}(O_t,O_{s}) =  ||M_{t,s} \odot (O_t - \hat{O}_{s})||_1,    \\
&E_{warp}^t=E_{pair}(O_t,O_{t-1}) + E_{pair}(O_t, O_1),
\end{align}
where $\hat{O}_{s}$ is obtained by warping the $O_{s}$ with the optical flow from frame $t$ to frame $s$. $M_{t,s}$ is the occlusion mask from frame $t$ and frame $s$. For each frame $t$, the warping error $E_{warp}^t$ is computed with the previous and first frames in the video. We compute the warping error for all the frames in a video. 


\subsection{Comparisons to Baselines}
\noindent \textbf{Baseline.} As our approach is the first method for blind deflickering, no existing public method can be used for comparison. Thus, we design a baseline inspired by blind video temporal consistency approaches: \textit{ConvLSTM}, modified from Lai et al.~\cite{lai2018learning}. Specifically, we replace the consistent input pair of frames with flickered pair frames, and we retrain the ConvLSTM on their training dataset~\cite{lai2018learning}. 




\noindent \textbf{Results.} Table~\ref{table:comparison_ourdata}(a) provides quantitative results on two types of videos where we can estimate high-quality optical flow from consistent videos for computing the warping errors. Our results are consistently better than the baseline. In Table~\ref{table:comparison_ourdata}(b), we conduct a user study on Amazon Mechanical Turk following an A/B test protocol~\cite{ChenK17} to evaluate the perceptual preference between the main baseline ConvLSTM and our method. Each user needs to choose a video with better perceptual quality from videos processed by our method and the baseline. We use all real-world videos that cannot obtain high-quality optical flow for quantitative evaluation.
In total, we have $20$ users and $50$ pairs of comparisons.
Our method outperforms the baseline significantly in all tasks. 
Figure~\ref{fig:baselines} shows the qualitative comparisons between our approach and the baseline. Our approach removes various types of flicker, and our results are more temporal consistent than baselines.



\noindent \textbf{Comparisons to blind temporal consistency {methods}.} 
The comparison between our approach and blind video temporal consistency methods is unfair since our approach \textit{does not} require extra videos and baselines \textit{use} extra input videos as guidance. However, we still provide the comparison results for reference. Specifically, we use three state-of-the-art baselines, including Bonneel et al.~\cite{bonneel2015blind}, Lai et al.~\cite{lai2018learning} and DVP~\cite{DBLP:conf/nips/dvp}. Table~\ref{table:MainComparison} shows the quantitative results between our approach and baselines. The warping error of our approach on various processed videos is lower than all the baselines, which indicates that our approach is more temporal consistent even without the input video guidance. 
\begin{table}[t]
\begin{center}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l@{\hspace{3em}}c}
\toprule
\multicolumn{1}{l}{Method} & $E_{warp} \downarrow$  \\
\midrule
Ours without atlas \& neural filtering & 0.131  \\
Ours without local refinement & 0.090  \\
Ours full model & \textbf{{0.088}} \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-1.2em}
\caption{ \textbf{Quantitative results of ablation study}. The atlas and neural filtering strategy reduce temporal inconsistency significantly. The local refinement makes a slight difference in warping error but does improve perceptual performance.}
\label{table:ablation}
\vspace{-1em}
\end{table}

\subsection{Ablation Study}

We present the flawed atlas in the third column of  {Figure~\ref{fig:baselines}}. While the temporal consistency between frames is perfect, the atlas-based frames contain many artifacts and distortions. Hence, designing dedicated strategies is essential for blind deflickering. In this part, we analyze the importance of our two modules, respectively: (1) \textit{neural filtering} denotes the atlas generation and neural filtering since they cannot be split. (2) \textit{local refinement} denotes the local refinement module.


\noindent \textbf{Neural filtering.} We first analyze the importance of neural filtering by removing this part. We direct apply our local refinement module on the flickering input videos. As shown in Table~\ref{table:ablation}, removing the neural filtering module significantly increases temporal inconsistency. 

\noindent \textbf{Local refinement.} As shown in Table~\ref{table:ablation}, removing the local refinement network degraded the quantitative performance slightly. In Figure~\ref{fig:refine}, we show some local regions in the frames are temporal inconsistent. While these regions are small and the warping errors are only slightly reduced, this local flickering does hurt the perceptual performance, as shown in our supplementary materials.




\begin{figure}[t]
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}

\includegraphics[width=0.320\linewidth]{Figure/results/experts/expert3/input/00020_rec.png}&
\includegraphics[width=0.320\linewidth]{Figure/results/experts/expert3/deflicker/00020.jpg}&
\includegraphics[width=0.320\linewidth]{Figure/results/experts/expert3/ours/00020.jpg} \\

\includegraphics[width=0.320\linewidth]{Figure/results/experts/expert3/input/00021_rec.png}&
\includegraphics[width=0.320\linewidth]{Figure/results/experts/expert3/deflicker/00021.jpg}&
\includegraphics[width=0.320\linewidth]{Figure/results/experts/expert3/ours/00021.jpg} \\
Input & Human experts & Ours \\

\end{tabular}
\vspace{-0.5em}
\caption{\textbf{Comparison to human experts}.
Input: the lower frame is redder than the upper one. Our method achieves comparable performance with human experts in this case. }%More comparisons are provided in the supplementary materials.
\label{fig:exper}
\vspace{-1.2em}
\end{figure}

\subsection{Comparisons to Human Experts}


We compare our approach to human experts that use commercial software for deflickering. We adopt the RE:Vision DE:Flicker commercial software~\cite{revision} for comparison, following Bonneel et al.~\cite{bonneel2015blind}. Specifically, we obtain the official demos processed by experts and compare them. Figure~\ref{fig:exper} shows the qualitative comparison results. We can see that our approach can obtain competitive results in a fully-automatic manner. Besides, as discussed in Bonneel et al.~\cite{bonneel2015blind}, the videos processed by new users are usually low-quality. 

\subsection{Discussion and Future Work}
\noindent\textbf{Potential applications.} Our model can be applied to all evaluated types of flickering videos. Besides, while our approach is designed for videos, it is possible to apply \textit{Blind Deflickering} for other tasks (e.g., novel view synthesis~\cite{nerf,xie2022high}) where flickering artifacts exist. 

\noindent\textbf{Temporal consistency beyond our scope.} Solving the temporal inconsistency of video content is beyond the scope of deflickering. For example, the contents obtained by video generation algorithms can be very different. Large scratches in old films can destroy the contents and result in unstable videos, which require extra restoration technique~\cite{wan2022bringing}. We leave the study for removing these temporally inconsistent artifacts to the future work.





