\section{Blind Deflickering Dataset}
\label{sec:dataset}
% What?
%
\begin{table*}[t]
\small
\centering
\renewcommand{\arraystretch}{1.1}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
% \hline
\toprule
% \multicolumn{1}{c}{Task}
\multirow{2}{*}{Task} & \multicolumn{5}{c}{$E_{warp} \downarrow$} \\
 & {Processed} & {Bonneel et al.}~\cite{bonneel2015blind} & Lai et al.~\cite{lai2018learning} & DVP~\cite{DBLP:conf/nips/dvp} &  {Ours} \\ 
\hline
\rowcolor{mygray}
w/o Extra Consistent Guidance & -- & {\color{red} \xmark} & {\color{red} \xmark} & {\color{red} \xmark} &  {\color{green} \cmark} \\
\hline
Dehazing~\cite{he2010single} & 0.120 & 0.128 & 0.136 & \underline{0.109} & \textbf{0.106} \\
Spatial White Balancing~\cite{hsu2008light}  & 0.087 & 0.081 & 0.098 & \underline{0.073} & \textbf{0.062}\\
Colorization~\cite{zhang2016colorful} & 0.109 & \underline{0.096} & {0.100} & {0.097} & \textbf{0.084} \\
Enhancement~\cite{gharbi2017deep} & 0.125 & {0.105} & 0.115 & \underline{0.102} & \textbf{0.093} \\
CycleGAN~\cite{CycleGAN2017} & 0.124 & 0.113 & 0.117 & \underline{0.103} & \textbf{0.099} \\
Intrinsic Decomposition & 0.131 & {0.085} & 0.108 & \underline{0.071} & \textbf{0.069} \\
Style Transfer & 0.202 & {0.161} & 0.177 &  \underline{0.143} & \textbf{0.142} \\
\hline
Average Score & 0.128  & {0.110} & 0.122 & \underline{0.100} & \textbf{0.094} \\
% \hline
\bottomrule
\end{tabular}
\vspace{-1mm}
\caption{\textbf{Qualitative comparison with blind video temporal consistency methods that use input videos as extra guidance.} While our approach does not use input videos as guidance, our method achieves better numerical performance compared with the baselines. }
\label{table:MainComparison}\
\vspace{-1.6em}
\end{table*}


We construct the first publicly available dataset for blind deflickering. 

\noindent \textbf{Real-world data.}
We first collect real-world videos that contain various types of flickering artifacts. Specifically, we collect five types of real-world flickering videos: 
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item \textit{Old movies} contain complicated flickering patterns. The flicker is caused by multiple reasons, such as unstable exposure time and aging of film materials. Hence, the flickering can be high-frequency or low-frequency, globally and locally. 
    \item \textit{Old cartoons} are similar to old movies, but the structures differ from natural videos.
    \item\textit{Time-lapse} videos capture a scene for a long time, and the environment illumination usually changes a lot. 
    \item \textit{Slow-motion} videos can capture high-frequency changes in lighting. 
    \item \textit{Processed} videos denote the videos processed by various processing algorithms. The patterns of flicker are decided by the specific algorithm. We follow the setting in \cite{lai2018learning}. 
\end{itemize}


\begin{figure}[t]
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}


\rotatebox{90}{\small \hspace{0mm} w/o local  refinement}&
\includegraphics[height=0.34\linewidth]{wo_0_rec.png}&
\includegraphics[height=0.34\linewidth]{Figure/ready/wo_ready.png}\\
\rotatebox{90}{\small \hspace{7mm} Full model }&
\includegraphics[height=0.34\linewidth]{full_0_rec.png}&
\includegraphics[height=0.34\linewidth]{Figure/ready/full_ready.png}\\
\end{tabular}

\vspace{-0.5em}
\caption{\textbf{Ablation study for local refinement module.} The local refinement network is vital to remove the local flickering. Cropped images and their difference maps are placed at the third column.}
\vspace{-1.1em}
\label{fig:refine}
\end{figure}

\noindent \textbf{Synthetic data.}
While real-world videos are good for evaluating perceptual performance, they do not have ground truth for quantitative evaluation. Hence, we create a synthetic dataset that provides ground truth for quantitative analysis. Let $\{G_t\}_{t=1}^T$ be the clean video frames, the flickered video $\{G_t\}_{t=1}^T$ can be obtained by adding the flickering artifacts $F_t$ for each frame at time $t$:
\begin{align}
    I_t = G_t + F_t, 
\end{align}
where $\{F_t\}_{t=1}^T$ is the synthesized flickering artifacts. For the temporal dimension, we synthesize both short-term and long-term flicker. Specifically, we set a window size $W$, which denotes the number of frames that share the same flickering artifacts. We set the window size $W$ as 1, 3, 10 respectively.

\noindent\textbf{Summary.} We provide $20$, $10$, $10$, $10$, $157$, and $90$ for old movies, old cartoons, slow-motion videos, time-lapse videos, processed videos, and synthetic videos.





