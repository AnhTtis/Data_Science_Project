% TODO only just swapped out documentclass, but should probably find a proper ToG template

%\documentclass[conference]{IEEEtran}
\documentclass[lettersize,journal]{IEEEtran}


% \newcommand{\KOT}[1]{{\tt{\textcolor{orange}{JKO}(}{#1}{)}}}
% \newcommand{\TODO}[1]{{\tt{\textcolor{red}{TODO}(}{#1}{)}}}

\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{float}
%%\usepackage[ruled]{algorithm2e}
\usepackage{hyperref}
\usepackage{multirow}

\usepackage{booktabs}

\begin{document}

\title{Proof Number Based  Monte-Carlo Tree Search}
%\title{Combining Monte-Carlo Tree Search with Proof-Number Search}




% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{\IEEEauthorblockN{Elliot Doe, Mark H. M. Winands, Dennis J. N. J. Soemers and Cameron Browne}
% \IEEEauthorblockA{\textit{Games Group, Department of Advanced Computing Sciences, Maastricht University} \\
% Maastricht, The Netherlands\\ e.doe@student.maastrichtuniversity.nl, \{m.winands, dennis.soemers, cameron.browne\}@maastrichtuniversity.nl}}
% }


\author{\IEEEauthorblockN{Elliot Doe\IEEEauthorrefmark{1}, Mark H. M. Winands\IEEEauthorrefmark{1}, Jakub Kowalski\IEEEauthorrefmark{2},\\ Dennis J. N. J. Soemers\IEEEauthorrefmark{1}, Daniel G\'{o}rski\IEEEauthorrefmark{2}, and Cameron Browne\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}\textit{Department of Advanced Computing Sciences, Maastricht University}\\
e.doe@student.maastrichtuniversity.nl, \{m.winands, dennis.soemers, cameron.browne\}@maastrichtuniversity.nl}
\and
\IEEEauthorblockA{\IEEEauthorrefmark{2}\textit{Faculty of Mathematics and Computer Science, University of Wroc{\l}aw}\\\{jakub.kowalski, daniel.gorski\}@cs.uni.wroc.pl}
}



% \author{
% \IEEEauthorblockN{Elliot Doe, Mark H. M. Winands, Dennis J. N. J. Soemers and Cameron Browne}

% \IEEEauthorblockA{\textit{Games Group, Department of Advanced Computing Sciences, Maastricht University} \\
% e.doe@student.maastrichtuniversity.nl, \{m.winands, dennis.soemers, cameron.browne\}@maastrichtuniversity.nl}
% \and 
% \IEEEauthorblockN{Jakub Kowalski, Daniel G\'{o}rski}

% \IEEEauthorblockA{\textit{University of Wroc{\l}aw, Faculty of Mathematics and Computer Science} \\
% \{jakub.kowalski, daniel.gorski\}@cs.uni.wroc.pl}
% }



% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}



\maketitle

\begin{abstract}
This paper proposes a new game search algorithm, PN-MCTS, that combines Monte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These two algorithms have been successfully applied for decision making in a range of domains.
We define three areas where the additional knowledge provided by the proof and disproof numbers gathered in MCTS trees might be used: final move selection, solving subtrees, and the UCT formula. We test all possible combinations on different time settings, playing against vanilla UCT MCTS on several games: Lines of Action ($7$$\times$$7$ and $8$$\times$$8$), MiniShogi, Knightthrough, Awari, and Gomoku. Furthermore, we extend this new algorithm to properly address games with draws, like Awari, by adding an additional layer of PNS on top of the MCTS tree.
The experiments show that PN-MCTS confidently outperforms MCTS in 5 out of 6 game domains (all except Gomoku), achieving win rates up to  96.2\% for Lines of Action.
\end{abstract}


\begin{IEEEkeywords}
Monte-Carlo Tree Search, Proof-Number Search, MCTS Solver
\end{IEEEkeywords}



% This paper is an extension of work originally presented in conference name [

% Intro, że dodajemy coś co jest significant improvement over the methods presented in  [bazowy papier]

% This paper proposes a new variant of Monte-Carlo Tree Search, obtained by combining it with Proof Number Search. Proposed algo


\section{Introduction}

 
 Monte-Carlo Tree Search (MCTS)~\cite{coulom06,kocsis06b} is a best-first search method guided by the results of Monte-Carlo simulations, well-established in game AI.  Using the results of previous simulations, the method gradually builds up a game tree in memory and increasingly becomes better at accurately estimating the values of the most promising moves. MCTS has substantially advanced the state of the art in several deterministic game domains \cite{mctssurvey}, in particular
Go \cite{Silver2017mastering}, but other board games include  Amazons~\cite{Lorentz08}, Hex
\cite{arneson10}, Lines of Action \cite{winands10},  and the ones of the  General Game
Playing competition~\cite{bjornsson09}. MCTS has even increased the level of competitive agents in board games with challenging properties such as multi-player (e.g., Chinese Checkers \cite{sturtevant08}) and uncertainty (e.g., Kriegspiel \cite{ciancarini10} and Scotland Yard  \cite{nijssen12tciaig}).


%Games have always been a great place for the development of artificial intelligence (AI). They are problems with typically difficult (if at all possible) to prove solutions, but also simple rules. Within the domain of games, there are also various distinctions that are made between games. There are games with perfect or imperfect information, stochastic or deterministic outcomes, singleplayer or multiplayer elements and various other parameters that all require different approaches. In the domain of abstract games, sequential games with perfect information, there are many different agents that perform well. For a long time $\alpha\beta$ algorithms were the go-to for abstract games \cite{Junghanns98arethere}. In 2006, Kocsis and Szepesv{\'a}ri proposed an algorithm called Monte-Carlo Tree Search (MCTS) for the domain of Go \cite{mcts}. MCTS has since seen much success in various domains and in various forms. MCTS has been implemented in most abstract games with various levels of success \cite{swiechowski2021monte}.

In tactical games, where the main line towards the winning
position is typically narrow with many non-progressing alternatives, MCTS may often lead to an erroneous outcome because the nodes' values in the tree do not converge fast enough to their game-theoretic value. To mitigate this effect, MCTS variants have been proposed that integrate concepts of minimax search \cite{winands08b,winands11,LanctotWPS14,baier2015}. 


Another promising direction would be the incorporation of  Proof-Number Search (PNS) \cite{allis94} in MCTS. PNS and its variants \cite{vandenHerik2008} have been proposed to prove endgames faster than traditional minimax. PNS variants  have been successfully applied to a large number of domains including
Chess \cite{breuker}, Othello \cite{nagai}, Shogi \cite{nagai}, Lines of Action (LOA) \cite{Winands04}, Go \cite{Kishimoto05b}, Checkers \cite{schaeffer07a}, Connect6 \cite{Wu10}, and the multi-player game Rolit \cite{Saito10}.  All PNS variants share two features: (1) they are algorithms for solving binary goals, such as proving a win or a loss for a game position, and (2) they rely on the concept of proof and disproof numbers.


This paper proposes a new variant, called PN-MCTS, that combines the strengths of MCTS and PNS with each other. The main ideas are to incorporate proof and disproof numbers in the UCT mechanism \cite{kocsis06b} of MCTS, and use them for solving subtrees in a similar fashion as MCTS Solver does \cite{winands08b}.
To investigate PN-MCTS performance, we performed game-playing experiments in five two-player, zero-sum board games: Lines of Action, Gomoku, MiniShogi, Knightthrough, and Awari, using a variety of different time settings.





 
%This thesis tries to answer the problem statement: 
%\begin{itemize}
    %\item Is the information gained from the (dis)proof number worth the additional computation needed?
%\end{itemize}
%% Additionally, different methods of incorporating (dis)proof number into MCTS are tested to see which performs best. 
%To investigate the problem statement, the thesis tries to answer the following two research questions:
%\begin{enumerate}
    %\item What is the computational cost of finding and storing the proof and disproof number during a regular MCTS run?
    %\item What, if any, difference in playing strength can be seen when the proof and disproof number are used in the selection step of MCTS?
%\end{enumerate}
This paper is an extension of work originally presented in \cite{doe2022combining} and contains significant improvements over the methods described there. 
The paper adds a new PN-MCTS enhancement that benefits from solving subtrees and performed extensive tests checking all combinations of proposed enhancements.  New experiments cover a wider area of possible settings, and they are computed based on four times more games per test, which notably increases their confidence.
We also propose a novel extension of the originally proposed PN-MCTS algorithm that is able to perform well in games with draws.


The remainder of the paper is organized as follows. First, MCTS and PNS are discussed in Sections \ref{sec:MCTS} and \ref{sec:PNS}, respectively. Next, we propose a single-layer PN-MCTS in Section \ref{sec:PN-MCTS}. Subsequently, we empirically evaluate the proposed algorithm in the following two sections. First, establishing proper parameter values, and then conducting main experiments testing all combinations of previously-defined enhancements.
In Section~\ref{sec:PNMCTS2}, we address the issue of games with draws, proposing an extension of PN-MCTS that can handle them successfully. Finally, Section \ref{sec:Conc} gives conclusions and an outlook on future research. 






%\section{Search Methods}
%This section describes the two search methods that are investigated for this thesis. For the scope of this thesis, only the basic versions of these two methods are considered. Subsection A. details Monte-Carlo Tree Search (MCTS) and more specifically the four steps that make up a typical cycle of this search method. Proof-Number Search, the second search method, is explained in Subsection B.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monte-Carlo Tree Search}\label{sec:MCTS}

Monte-Carlo Tree Search (MCTS) \cite{coulom06,kocsis06b} is a best-first search method that does
not require a positional evaluation function. It is based on a
randomized exploration of the search space. Using the results of
previous explorations, the algorithm gradually builds up a game
tree in memory, and increasingly becomes better at accurately
estimating the values of the most promising moves. MCTS consists of four strategic steps, repeated as long as there is
time left \cite{chaslot08}. The steps, outlined in \figurename~\ref{fig:mcts}, are as follows.\\

%Monte-Carlo Tree Search (MCTS) is a search technique that relies on random simulations rather than a heuristic evaluation function \cite{mcts}\cite{mcts2}. MCTS will first randomly explore the search space and build up its information by using random playouts as an alternative to an evaluation function. Once it has gathered enough information, MCTS will not only search the tree randomly, but also try to exploit promising branches of the search tree. Even once more information has been gained however, the method will still actively search out unexplored areas of the search space. This balance between exploiting the most promising branches whilst still exploring the tree for new opportunities is what makes MCTS as powerful as it is. Figure \ref{fig:MCTS_figure} illustrates how an iteration of MCTS works schematically. There are four steps that MCTS goes through. The selection, expansion, playout and backpropagation steps in that order. Those four steps are described in more detail in the following subsections.
\begin{figure}[hb]
    \centerline{\includegraphics[width= \columnwidth]{scheme4.pdf}}
    \caption{Outline of Monte-Carlo Tree Search.}
    \label{fig:mcts}
\end{figure}

\textbf{Selection Step.} In the first step, a  child is selected to be searched based on previously gathered information.
The selection step controls the balance between exploitation and exploration. On the
one hand, the task consists of selecting the move that leads
to the best results so far (exploitation). On the other hand, the less promising moves still have to be tried, due to the uncertainty of
the simulations (exploration).

Several \textit{selection strategies} \cite{mctssurvey}  have been suggested for MCTS such as BAST, EXP3, UCB1-Tuned, but the most popular one is based on the UCB1 algorithm \cite{auerfinit02}, called  UCT (\textbf{U}pper \textbf{C}onfidence Bounds applied to
\textbf{T}rees) \cite{kocsis06b}. UCT works as follows. Let $I$ be the set of nodes immediately reachable from the
current node $p$. The selection strategy selects the child $b$ of
node $p$ that satisfies Formula \ref{eq:UCT}:


 \begin{equation}
 \label{eq:UCT}
 \mathit{b\in \mathrm{argmax}_{i \in I} \left(v_i + C \times \sqrt{\frac{\ln{n_p}}{n_i}}\right)},
 \end{equation}

\noindent where $v_i$ is the value of the node $i$, $n_i$ is the
visit count of $i$, and $n_p$ is the visit count of $p$. $C$ is a
parameter constant, which can be tuned experimentally (e.g., $C=\sqrt{2}$).
In the case of a tie, the tie is broken randomly. This process is repeated until a node is reached that has not yet fully been expanded.\\


%In this step, MCTS decides how to traverse the search space. As stated previously, MCTS tries to both exploit states of the game space where the search technique has been performing well and explore areas of the search tree that have not been visited much yet. A commonly used formula for determining which child node should be traversed is the UCT (Upper Confidence Bound for Trees) formula \cite{mcts}. The exploitation part of the formula is based on an evaluation function. For this thesis, the evaluation function of $wins/totalVisits$ is used. The exploration part of the formula promotes the child nodes which have been visited relatively fewer times than their sibling nodes. The formula for the exploration part of UCT is as follows: $\sqrt{\frac{\ln N_i}{n_i}}$. These two parts are put together to create the UCT formula that looks as follows:
%\begin{equation} \label{eq:UCT_formula}
%UCT = \frac{w_i}{n_i} + C \sqrt{\frac{\ln N_i}{n_i}}
%\end{equation}
%Here $w_i$ is the number of wins, $n_i$ is the number of visits of a node, $N_i$ is the number of visits in the parent node and $C$ is an adjustable parameter that this thesis refers to as the UCT constant. Here $C$ is  set to $\sqrt{2}$. Once the UCT value for each child node is calculated, the child with the highest value is picked. In the case of a tie, the tie is broken randomly. This process is repeated until a node is reached that has not yet fully been expanded. 
\textbf{Expansion Step.} As previously stated, the selection step continues until a node is reached that has not yet expanded all of its children. Among the children that have not been stored in tree, one is selected uniformly at random. This node $L$ is then added as a  new leaf node and is subsequently investigated.\\

\textbf{Play-out Step.} From the leaf node the play-out step is performed. Moves are selected in self-play until the end of the game is reached. This step might consist of playing plain random moves or -- better -- semi-random moves chosen according to a \textit{simulation strategy}.\\


\textbf{Backpropagation Step.} In the final step, the result \textit{R}
of a play-out $k$ is backpropagated from the leaf node $L$, through the
previously traversed nodes, all the way up to the root. The result is scored positively $(R_k=+1)$ if the game is won, and negatively
  $(R_k=-1)$ if the game is lost. Draws lead to a result $R_k=0$. A \emph{backpropagation strategy} is applied to the  \textit{value} $v_i$
  of a node $i$. Here, it is computed by taking the average of the results of all simulated games made through this node \cite{coulom06},
   i.e., $v_i=(\sum_k R_k ) / n_i$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof-Number Search}\label{sec:PNS}

 Proof-Number Search (PNS) is a best-first search method especially suited for finding the
game-theoretic value in game trees \cite{allis94}. Its aim is to prove
a particular goal. In the context of this paper, the goal is to prove a forced win for the player to move. A tree can have three values:
\textit{true}, \textit{false}, or \textit{unknown}.  In the case of a
forced win, the tree is \textit{proven}  and its value is true. In
the case of a forced loss or draw, the tree is \textit{disproven}
and its value is false. Otherwise, the value of the tree is unknown.
As long as the value of the root is unknown, the most-promising node
is expanded. Just like MCTS, PNS
does not need a domain-dependent heuristic evaluation function to
determine the most-promising node \cite{allis94}. In PNS this
node is usually called the \textit{most-proving} node. PNS
selects the most-proving node using two criteria: (1) the shape of
the search tree (the branching factor of every internal node) and
(2) the values of the leaves. These two criteria enable PNS to
treat game trees with a non-uniform branching factor efficiently.


\begin{figure}[ht]
\centerline{\includegraphics[width=\columnwidth]{pntree} }
\caption{An AND/OR tree with proof and disproof numbers}
 \label{pntree}
\end{figure}

Below we explain PNS on the basis of the AND/OR tree depicted
in Fig. \ref{pntree}, in which a square denotes an OR node, and a
circle denotes an AND node. The numbers to the right of a node
denote the proof number (upper) and disproof
number  (lower). A \textit{proof number}
(\emph{pn}) represents the minimum number of leaf nodes, which have
to be proven in order to prove the node. Analogously, a
\textit{disproof number} (\emph{dpn}) represents the minimum number
of leaf nodes that have to be disproved in order to disprove the
node. Because the goal of the tree is to prove a forced win, winning
nodes are regarded as proven. Therefore, they have $pn=0$ and
$dpn=\infty$ (e.g., node \textit{i}). Lost or drawn
nodes are regarded as disproven (e.g., nodes \textit{f} and
\textit{k}). They have $pn =\infty$ and $dpn=0$.
Unknown leaf nodes have  $pn=1$ and $dpn=1$ (e.g.,
nodes \textit{g}, \textit{h}, \textit{j}, and \textit{l}).  The \emph{pn} of an
internal OR node is equal to the minimum of its children's proof
numbers, since to prove an OR node it suffices to prove one child.
The \emph{dpn} of an internal OR node is equal to the sum of
its children's disproof numbers, since to disprove an OR node all
the children have to be disproven. The  \emph{pn} of an internal AND node is equal to the sum of its children's
proof numbers, since to prove an AND node all the children have to
be proved. The \emph{dpn} of an AND node is equal to the
minimum of its children's disproof numbers, since to disprove an AND
node it suffices to disprove one child. 

The procedure of selecting the most-proving node to expand next is as
follows. The algorithm starts at the root. Then, at each OR node the child with
the smallest \emph{pn} is selected as successor, and at each AND
node the child with the smallest  \emph{dpn} is selected as
successor. Finally, when a leaf node is reached, it is expanded
(which makes the leaf node an internal node) and the newborn
children are evaluated. This is called
\textit{immediate evaluation}. The selection of the most-proving
node (\textit{j}) in Fig. \ref{pntree} is given by the bold path.



%The second method used in this thesis is Proof-Number Search (PNS) \cite{Allis1994SearchingFS}\cite{pns2}. This search technique only works in AND/OR-Trees. OR-nodes in the game tree are those where the player has the move and the AND-nodes are those where the opponent has the move. PNS uses so-called proof and disproof numbers to function. A proof number provides a lower bound for how many nodes are necessary to be proved before the current node itself is be proved. A disproof number is the same except it tells at least how many nodes must be disproved to disprove a node. Proving a node means that from the board state of a current node, there is a possible route down the tree that proves the chosen prove goal. The proof goal is a binary goal that the search tries to prove. In this thesis, the proof goal is a victory (so not a loss or a tie). In a state where the proving player has the move, a node can be proven simply by proving one of its child nodes. In a state where the opponent has the move, the only way to prove a node is if every option they have results in a win for the proving player. Thus, the proof and disproof numbers are calculated differently depending on if the PNS is in an OR-node or an AND-node. The code to set the proof and disproof numbers are outlined in Figure \ref{fig:PNS_figure}.
%\begin{figure}
    %\centerline{\includegraphics[width= \columnwidth]{Proof_Disproof Pseudocode Figure.png}}
    %\caption{(Dis)proof Number Pseudocode \cite{Allis1994SearchingFS}}
    %\label{fig:PNS_figure}
%\end{figure}
%
%The search tree is built up by traversing the tree by using the proof and disproof number. At an OR-node the child with the lowest proof number will be picked as it is the closest to proving the tree. At an AND-node the opposite happens and PNS travels down the tree through the node with the lowest disproof number. Once an unexplored node is reached, all its children are immediately created and evaluated. These values are then used to backpropagate a new proof and disproof number through the tree up to the root. As stated before, a node is only proved if it can prove the proof goal. However, it can take time before the tree is expanded far enough to the point of reaching endgame situations. Thus, when a node is evaluated and cannot be evaluated as a win or a loss yet, it will be given a default proof and disproof number (typically both are set to 1). If a node is evaluated to be proven, the proof number is set to 0 (because no more nodes need to be proved to prove this node as it is already proved), and the disproof number is set to infinity (there is no way to disprove this node). If instead the node is evaluated to be disproven, the method does the opposite and sets the proof number to infinity and the disproof number to 0. This loop of traversing the tree and checking proof and disproof numbers is continued until the root node is proved or disproved.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PN-MCTS Algorithm}\label{sec:PN-MCTS}
To properly determine a feasible approach to incorporating (dis)proof numbers into MCTS, it is first important to consider what information the proof and disproof numbers bring. As explained in the previous section about PNS, a (dis)proof number provides a lower bound for the number of nodes that still have to be (dis)proven to prove the current node. 

In PNS, these lower bounds determine which leaf node would be investigated further. In MCTS,  the selection step has a similar function. In the default  MCTS implementation, this would use the UCT formula (\ref{eq:UCT}). Thus, a natural way to combine MCTS and PNS would be to combine these two ways of selecting promising leaf nodes and modify the basic UCT formula with knowledge gained from PNS. 

A direct consequence of tracking this lower bound is knowledge about the proven/disproven subtrees. Although for any game of proper size we should not expect the game root to be proven, it may be possible that near the endgame, the current state became provable. Also, many subtrees can be (dis)proven within the MCTS tree during a search, so there is no need to revisit them again during the expansion phase.

Therefore, we introduce three PNS-based types of MCTS enhancements, regarding \emph{Final move selection}, \emph{Solving subtrees}, and \emph{UCT formula}. They can be applied independently, in the form of flags (each enhancement turned on/off). 



\subsection{Final Move Selection}
There are two most common ways to select a move to play after the times for MCTS computations run out. One (used in this paper's experiments) is to select a move leading to the root child note with the most visits, and the other is to prioritize the node with the best value (average score) \cite{chaslot08}.

We propose a simple yet effective enhancement. Due to the proof numbers stored in nodes, we know which of the subtrees rooted in these nodes are proven to be a win for our player. Thus, if during the final move selection there is a move leading to a proven child, we should always select it regardless of the number of visits / average score in this node.

We may be tempted to use disproof numbers in a similar way, forbidding the selection of such nodes if any alternative exists, but please note that if the game has more than two outcomes, all proven outcomes other than a win will count as disproven. E.g., we will not distinguish a node that is a loss from one that leads to a draw. We present a solution to this issue in Section~\ref{sec:PNMCTS2}.


\subsection{Solving Subtrees}

Proof and disproof numbers can be seen as an extension of a solver \cite{winands08b}, a successful MCTS enhancement that is obligatory in most applications. MCTS-Solver backpropagates information from leaf nodes being terminal states and marks subtrees as won, lost, or unresolved. PN-MCTS computes a superset of this information, so it can be used in the same way -- to save the computational effort and skip revisiting already (dis)proven subtrees. 

In practical applications, usually an additional parameter $T$ is introduced to solve some issues that UCT has during selection step when some children of a node are solved. Thus, a node is omitted during the UCT selection only if the number of visits in this node exceeds the treshold $T$ \cite{winands08b}. In the case of all our experiments, if the solver enhancement is on, $T$ is equal to 5.



\subsection{UCT}

The third enhancement proposed in this paper is a modification of basic UCT. By adjusting UCT to also use proof and disproof numbers, nothing else about MCTS can to be changed, though the information from the (dis)proof number can still influence the decision making process.

The final consideration then is how to use (dis)proof numbers in UCT. 
%The size of the proof and disproof numbers themselves do not technically have much meaning relative to each other. For example, a node with proof number 100 is not ten times worse than a node with proof number 10. This is because the node with proof number 100 may have just been investigated more often already. The fact that the proof and disproof numbers do not have much meaning relative to each other means they are more difficult to add directly into the UCT formula. 
The magnitudes of differences amongst (dis)proof numbers technically do not have much meaning. For example, a node with a proof number of 100 is not necessarily ten times worse than a node with a proof number of 10. The node with the proof number of 100 may just have been investigated more often already. The fact that the magnitudes of differences between (dis)proof numbers do not have much meaning makes it difficult to directly use them in the UCT formula.

Instead of using the proof or disproof numbers directly in the formula, this paper proposes that the (dis)proof numbers are used to determine a ranking amongst all the nodes. The ranking is similar to the one of PNS as explained in Section \ref{sec:PNS}. At an OR node the child node with the lowest proof number would get the best ranking because that is the node that would be selected in regular PNS. 
For example, if there are 30 child nodes, the one that would be picked according to PNS gets a rank of 1. 
At an AND-node, the node with the lowest disproof number is picked. The next best ranking node would be the node PNS would pick if the original best ranking node was not an option (so the second lowest (dis)proof number would get a ranking of 2, whereas the worst option would get 30 for this example). Ties are awarded the same rank. Finally, this rank can then be normalized to be in the range of [0, 1]. 

Normalization allows the resulting value to be in a range that is similar to the values that might come out of the exploitation or exploration parts of the UCT formula. To normalize, the ranking (called $pnRank$ in the formula) is divided by the highest rank of any of the children. Finally, to control the influence of the addition, a parameter is added (called $C_{pn}$ in the formula).

Putting all that together gives the following adjusted UCT formula, referred to as the UCT-PN formula:

\begin{scriptsize}
\begin{equation} \label{UCT-PN_formula}
 \mathit{b\in \mathrm{argmax}_{i \in I} \left(v_i + C \sqrt{\frac{\ln{n_p}}{n_i}} + C_{pn} \left( 1 - \frac{pnRank_{i}}{\mathrm{argmax}_{j \in I} (pnRank_{j})} \right) \right)},
%%UCT_{pn} = \frac{w_i}{n_i} + C \sqrt{\frac{\ln N_i}{n_i}} + 
\end{equation}
\end{scriptsize}

\noindent where $C_{pn}$ is the PN-Parameter that can be adjusted, $pnRank$ is the rank of a specific node (lowest rank is best node according to PNS), and $argmax_{j \in I} (pnRank_{j})$ is the highest (and thus worst) rank of any child node. The rest of the variables are the same as the regular UCT Formula \ref{eq:UCT}. This paper uses the term PN-MCTS to refer to any MCTS variant that uses the UCT-PN formula instead of the base UCT formula for its selection step.

%This is the approach that is investigated in this paper. There are other uses for the (dis)proof numbers that could be considered. (Dis)proof numbers can be used to for instance prune parts of the search tree that have already been proven or disproven. This is however not investigated for the scope of this thesis.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Environment}

Here, we briefly introduce the domains used to test our algorithm.
We outline  the General Game Game Playing system Ludii, in which all algorithms and games were implemented, and explain the choice for this system.
Then, we present the rules of the five games used as a test domain to compare our PN-MCTS with the basic MCTS.

\subsection{Ludii General Game System}\label{ludii}

The Ludii General Game System \cite{Piette2020Ludii} is a general game playing framework, which provides an environment for developers to test their implementation of general game playing agents. The system includes over 1,000 games described in its general game description language, and implementations of various standard algorithms and enhancements (such as several variants of MCTS).
It has a single, unified API for the development of intelligent agents, based on a forward model (with functions to generate lists of legal actions, generate successor states, and so on) and standardized state and action representations.
Ludii has been demonstrated \cite{Piette2020Ludii} to process games faster than the previous state-of-the-art general game playing framework based on Stanford's general game description language \cite{Love_2008_GDL}, which is important for the playing strength of tree search algorithms such as MCTS.
%This system was chosen as the testing environment for this thesis for several reasons. First of all, the framework has implementations of both the basic MCTS and PNS agents. These implementations could be used as a base for the enhancements suggested in this thesis. Additionally, using the Ludii system facilitates trying the new algorithm on various games.

\subsection{Game Domains}\label{domains}

Two-player adversarial games are well suited to PNS as it structures its knowledge as AND/OR-trees. The list of games that fulfill this condition is still rather large. 
%To narrow the list down even more, only domains that have been proven to be effective for both MCTS and PNS are considered.
To narrow the list down even more, only domains in which both MCTS and PNS have shown to be effective are considered.
If either MCTS or PNS does not perform well in a domain, the combination of the two will probably not be very effective.
% To test this hypothesis, one such game is also chosen for testing. 
From the remaining list of games that fits the requirements and desirable qualities, five games are chosen: Lines of Action, Awari, MiniShogi, Gomoku, and Knightthrough. 
% Lines of Action (LOA) is the primary domain for most of the experiments. The other games are used in two of the experiments. 
Each of the games is briefly described below. 

\subsubsection{Lines of Action}
%%The primary domain for the experiments is Lines of Action (LOA). 

The rules of Lines of Action (LOA) are as follows \cite{sackson69}. It is played on an
8$\times$8 board by two sides, Black and White. Each side has twelve
pieces at its disposal. The black pieces are placed along the top and bottom rows of the board, while the white pieces are placed in
the left- and right-most files of the board. The players alternately move a piece, starting with Black. A piece moves in a straight line, exactly as many squares as there are pieces of either color anywhere along the line of movement. A player may jump over its own pieces, but not the
opponent's, although opposing pieces are captured by landing on them.
The goal of the players is to be the first  to create a configuration on the board in which all own pieces are connected in
one unit. The connections within the unit may be either orthogonal or diagonal.

There are two main reasons why LOA was chosen as the main test domain. Both MCTS and PNS have been extensively tested on LOA \cite{vandenHerik2008,winands10}, and the game board has an adjustable size. The default board is 8$\times$8, but  smaller sizes such as 7$\times$7 can also be used. The advantage of the smaller board sizes is that the game reaches endgame states much quicker. PNS works best in endgame scenarios where game states can be proven or disproven in fewer steps. Thus, by testing on various board sizes, the experiments can test whether PN-MCTS has a better performance when endgame states require fewer steps to be reached.

\subsubsection{Awari}
Awari is a Mancala or sowing game \cite{awari}. It is is played on a 2$\times$6 board and with counters. The goal of the game is to capture as many counters as possible. To capture counters, a player must end their sow in the opponent's row and in a hole with 2 or 3 counters (including the piece used to sow). Sowing is a process where a player takes all the counters from a hole in their row and deposits them one by one into adjacent holes until none are left. In Awari, sowing goes counter-clockwise. The game is over once none of the holes contain more than 1 counter. The player who captured most counters wins, or in case both players have an equal amount, the game ends in a draw.  Awari is a suitable test domain for PN-MCTS as Mancala variants have been used as testbed for PNS \cite{allis94} and MCTS \cite{LanctotWPS14} in the past.

\subsubsection{MiniShogi}
MiniShogi is a variant of the old Japanese game called Shogi and was invented around 1970 by Shigenobu Kusumoto. The game has various pieces each of which have their own rules for movement. The goal is to capture the opponent's King with these pieces. Pieces can be promoted by moving them towards the opponent's side of the board. Opponent pieces can be captured by moving a piece onto an enemy piece. Captured pieces can re-enter the game as a turn. MiniShogi differs from regular Shogi in the following ways: it is played on a 5$\times$5 board, it has fewer pieces than the original, and features a smaller promotion area. Shogi endgames have  been one of the main test domains of PNS \cite{KishimotoW0S12}, whereas MCTS has become the dominating search technique for this game \cite{SilHub18General}.

\subsubsection{Gomoku}
Gomoku is a connection game \cite{gomoku}. The goal of the game is to make a row of exactly 5 stones of the same color. Gomoku is played on a 15$\times$15 board, with black and white stones. 
%The game starts as follows. One player places 2 black stones and 1 white stone anywhere on the board. The other player can then either choose to play as Black, add another white piece and play as White or place two additional stones and let the first player choose which color to play. For the rest of the game players take turns placing pieces of their color somewhere on the board. 
Players take turns placing pieces of their color on any empty position of the board.
The game ends once a player places a stone that gives them a line of exactly 5 pieces (lines of more than 5 pieces do not count). Lines may be orthogonal or diagonal. PNS has been applied in Gomoku before \cite{allis96}, and MCTS variants have also been developed for this game \cite{TangZSL16}.

\subsubsection{Knightthrough}
The game of Knightthrough is a variant of Breakthrough \cite{handscomb01}. It is played on an 8$\times$8 board. Each player has 16 pieces in the first two rows of their side of the board (opposing sides). Every piece moves like a knight in chess. This means each piece may move 1 square in one non-diagonal direction and then 2 squares in a perpendicular direction. Pieces may be captured by landing on them. Knights can jump over other pieces (both friendly and opponent). The goal of the game is to reach the opponent's edge of the board (the row furthest from the player) with one of their knights. A player can also win by capturing all opposing pieces. Knightthrough has been used to test MCTS in a general-game-playing context \cite{SironiLW20}. Its original variant Breakthrough has served as a test bed for PNS variants \cite{saffidineJC11}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{sec:PremExp}

This section outlines the experiments that have been conducted for the initial parameter tuning and measurement of computational efficiency difference between the compared algorithms. 

The variant of PN-MCTS with final move selection and UCT-PN formula (solver enhancement is disabled) is tested against the basic UCT MCTS implementation from the Ludii system\footnote{\url{https://github.com/Ludeme/LudiiExampleAI}}. PN-MCTS extension has been built on the same code base. For both agents the MCTS $C$ parameter is set to $\sqrt{2}$.
All of the experiments described in this section have been run on a Intel(R) Core(TM) i7-10750H CPU  with 16 GB of RAM.   

For every result, if the error margins are presented, they mean a 95\%-confidence interval. Also, in all tests player positions are swapped so that the agents play both sides equally.





\subsection{PN-Parameter}
First, we want to tune the $C_{pn}$ parameter. Values of $C_{pn} \in \{0.0, 0.1, 0.5, 1.0, 2.0, 5.0,  10^6\}$ are tested, each with 250 games against the base MCTS. One of the tested $C_{pn}$ configurations is set to an arbitrarily high number, such that the PN-MCTS performs practically the same as a regular PNS would, with the exception that ties in proof and disproof number are broken by MCTS instead of randomly. The experiments are executed with 1 second per turn.  This experiment is conducted in the game of LOA on the board sizes 7$\times$7 and 8$\times$8. By playing on two board sizes, results can be compared to determine if larger search spaces, longer games, and later endgame situations have different trends in which parameters are best for the PN-MCTS.

Figure \ref{fig:pnConstantLOAComparison} displays the win rates of PN-MCTS with varying $C_{pn}$ against MCTS. The trend in win rate of PN-MCTS is fairly similar for both board sizes. 
If $C_{pn}$ nears 0, such as when it is 0.1, the PN-MCTS will function more like the basic MCTS with final move selection (so it makes up for decreased efficiency).
% If $C_{pn}$ nears 0, such as when it is 0.1, the PN-MCTS will function more like the basic MCTS. % ACTUALLY NOT TRUE ^^'
% If $C_{pn}$ was 0, the two agents would be the same and a win rate of approximately 50\% would be expected. Thus it seems that as $C_{pn}$ nears 0, so too does the win rate near 50\%. 
Another low point for both board sizes is when $C_{pn}$ is $10^6$. When $C_{pn}$ is that high, the agent starts behaving similar to PNS. From the results, it seems that a basic PNS still performs better than basic MCTS on the smaller 7$\times$7 board, but not on the regular 8$\times$8 board. This is expected as PNS is most effective in endgame play. Endgame play is reached sooner on a 7$\times$7 board and thus (MCTS-)PNS performs better more quickly.
As for the highest win rates, on a 7$\times$7 board, a $C_{pn}$ of 2.0 performs best, recording a win rate of 91.2\% over basic MCTS. On the 8$\times$8 board, there is a different optimal setting. A $C_{pn}$ of 1.0 appears to be the optimal one with a win rate of 83.2\%. 

$C_{pn}$ represents the impact that the (dis)proof numbers have on the basic MCTS. So on the smaller board size, where PNS performs better than MCTS, a larger influence of the proof and disproof number performs better. This is only true to some degree, as when $C_{pn}$ reaches 5.0, the win rate drops as it starts to converge toward the score of a pure PNS. 

\begin{figure}[t]
    \centering
    \includegraphics[width= \columnwidth]{pnComparisonBarFigure5.png}
    \caption{PN-Parameter tuning. PN-MCTS against MCTS for 7$\times$7 and \& 8$\times$8 LOA (250 games, 1s per turn).}
    \label{fig:pnConstantLOAComparison}
\end{figure}

To give an idea how a pure PNS would behave on its own without any CPU overhead, we conducted an experiment with $C_{pn}=10^6$ and a fixed number of 1000 simulations per move in  LOA 8$\times$8. Here PN-MCTS won only 9 out 100 games against the regular MCTS. For the same number of simulations, but for $C_{pn}=1$, PN-MCTS won 99 out 100 games. These results validate that PNS on its own is weaker than MCTS, but when combined together in PN-MCTS the algorithm may outperform MCTS.



\subsection{Overhead Estimation}
This experiment investigates the cost of obtaining, and continuously updating, the proof and disproof numbers. PN-MCTS is constructed as an augmentation of the basic MCTS implementation provided by the Ludii system to ensure that any difference in performance is solely due to the proposed enhancement. To obtain the cost specifically, PN-MCTS and the original MCTS are both run for a fixed amount of time from the initial positions of several different games. The number of simulations that the two search techniques are able to perform are tracked and noted.

%\subsubsection{Results} %\subsubsection{Overhead Cost}
In Table \ref{tab:simsPerSecond} the average number of simulations per second is given for PN-MCTS and MCTS, as well as the ratio between these two numbers, for five games. These numbers were measured over a period of 30 seconds for Knightthrough, and 100 seconds for the remaining games. A shorter duration was used for Knightthrough because, due to its large branching factor and relatively high baseline number of simulations per second, PN-MCTS tended to run out of memory  at higher time settings. The increased memory usage is because the implementation of PN-MCTS fully expands all children of nodes at once -- allowing for the computation of (dis)proof numbers and rankings for all children-- whereas the standard MCTS only expands one child per simulation.
%The table reveals that bookkeeping the proof and disproof number for every node causes a drop of around 14\% in the number of simulations. 
The table reveals that PN-MCTS has a relatively mild overhead measuring from the initial game state. In most games ratios are relatively close to $0.9$), with the most notable drop in performance being in Knightthrough. However, as the game progresses, the overhead increases, which resembles in the later experiments.
%While this number does give an indication of the overhead, it should also be noted that the overhead here might be higher than average. 
While this number does give an indication of the overhead, it should also be noted that the overhead here might be higher than in usual experiments; this experiment measured overhead over 100 seconds (or 30 for Knightthrough), whereas other experiments run tree searches only for up to 2 seconds. 
%This is because of how PN is updated during backpropagation. 
The (dis)proof numbers have to be recalculated on backpropagation, thus the larger the tree, the higher the backpropagation has to go and the higher the processing cost. 
% However, the (dis)proof numbers for ancestors of a leaf node are only recalculated if the change in (dis)proof number has an effect on the (dis)proof numbers of the ancestors. So even if the game tree goes quite deep (such as in this experiment), the overhead cost should not be much higher than in usual cases. Still, the overhead cost will be different every turn depending on the current board state. 

\begin{table}[H]
\begin{center}
    \caption{Average number of simulations per second}
    \label{tab:simsPerSecond}
    \begin{tabular}{@{}lrrr@{}}
    \toprule
    %& \multicolumn{2}{c}{Simulations per second} & \\
    %\cmidrule(lr){2-3}
    Game & PN-MCTS & MCTS & Ratio \\
    \midrule
    LOA 7$\times$7 & 312.89 & 341.34 & 0.92 \\
    LOA 8$\times$8 & 172.84 & 188.95 & 0.91 \\
    Awari & 898.82 & 913.69 & 0.98 \\
    MiniShogi & 83.40 & 89.51 & 0.93 \\
    %%Gomoku & 863.00 & 16,102.80 & 0.05 \\
    Knightthrough & 1104.23 & 1342.30 & 0.82 \\
    \bottomrule
    \end{tabular}
\end{center}
\end{table}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PN-MCTS Experiments}

In this section, we present two groups of experiments. The goal of the first one is to measure the influence of each enhancement of PN-MCTS on the overall result. Thus, we will test all combinations of these enhancements using 1 second per turn timelimits. The second experiment aims to show the trends of the algorithm given different timelimits. For this test, we chose the most successful variants of PN-MCTS based on the previous experiment.

We used 1,000 games for each test (with 500 games per side), and $C_{pn}=1$ . All of the experiments described in this section have been run on an Intel(R) Core(TM) i7I7-11700K CPU with 2$\times$ 16GB Patriot Viper 3200MHz RAM.  

To encode which PN-MCTS enhancement is turned on, we use the following notation. Final move selection is encoded by \verb|F|, Solver by \verb|S|, and UCT-PN formula by \verb|U|. If an enhancement is off, the \verb|x| symbol is used instead. Thus, \verb|xSx| encodes a variant where only PN-solver is used, while \verb|FSU| is a variant with all proposed enhancements enabled.

% PROCESOR        Intel I7-11700K
% CHŁODZENIE CPU        ENDORFY Fera 5 Dual Fan
% PŁYTA GŁÓWNA        MSI MAG Z590 TOMAHAWK WIFI
% PAMIĘĆ RAM        Patriot Viper 32GB (2x 16GB) 3200MHz
% SSD         Samsung 500GB M.2 PCIe Gen4 NVMe 980 PRO
% OBUDOWA        SilentiumPC Armis AR1 SPC173
% ZASILACZ         be quiet! Pure Power 11 600W

\subsection{PN-MCTS Variants}

The results of the experiment are presented in Table~\ref{tab:variants}. There are two main observations. One is that the behavior of the PN-based enhancements is very game dependent, and as it may be very beneficial for some games, it may also be very harmful for others. 

Second is that, indeed, proposed enhancements are mostly improvements, and turning them on increases the winrates. It is not always the case, but the \verb|FSU| variant with all enhancements is the best combination in 4 out of 6 tested domains. 
%Also note, that the reduced speed of the PN-MCTS is significant and results in severe losses everywhere. 
To make the comparison meaningful, in the \verb|xxx| variant, none of the enhancements are used, but all data required to use them is computed.


\begin{table*}[t]
  \centering
\caption{Comparison of PN-MCTS variants (1000 games, 1s per turn, $C_{pn}=1$). Enhancements encoding -- Final move selection: \texttt{F}, Solver: \texttt{S}, UCT-PN: \texttt{U}, no enhancement: \texttt{x}.}
\label{tab:variants}
\begin{tabular}{c||c||c|c|c||c|c|c||c}
\toprule
 \multirow{2}{*}{Game domain}& \multicolumn{8}{c}{PN-MCTS variant} \\
     & \verb|xxx|        & \verb|Fxx| & \verb|xSx| & \verb|xxU| & \verb|FSx| & \verb|FxU| & \verb|xSU|  & \verb|FSU| \\ \hline 
LOA $7\times 7$ & $13.1\pm2.08$ & $46.0\pm3.09$ & $18.6\pm2.40$ & $33.1\pm2.86$ & $65.6\pm2.95$ & $90.4\pm1.83$ & $36.2\pm2.95$ & $\textbf{93.2}\pm1.56$\\ 
LOA $8\times 8$ & $14.5\pm2.18$ & $45.2\pm3.09$& $22.6\pm2.59$ & $37.4\pm2.96$ & $66.4\pm2.93$ & $83.2\pm2.32$ & $42.9\pm3.04$ & $\textbf{90.8}\pm1.79$ \\
MiniShogi       & $10.1\pm1.87$ & $48.2\pm3.10$& $11.4\pm1.97$ & $15.9\pm2.27$ & $53.2\pm3.09$ & $64.5\pm2.97$ & $15.7\pm2.26$ & $\textbf{65.8}\pm2.94$ \\
Knightthrough   & $13.9\pm2.15$ &$52.3\pm3.10$ & $18.2\pm2.39$ & $24.5\pm2.67$ & $59.2\pm3.05$ & $56.4\pm3.08$ & $29.7\pm2.83$ & $\textbf{66.8}\pm2.92$  \\
Awari           & $30.0\pm2.56$&  $34.4\pm2.77$ & $26.4\pm2.54$ & $48.0\pm2.65$ & $38.0\pm2.94$  & $\textbf{49.8}\pm2.72$ & $35.2\pm2.87$ & $40.6\pm3.01$ \\
Gomoku          & $ 0.01\pm0.20$ & $2.8\pm1.02$ & $ 0.0\pm 0.0$ & $0.01\pm0.20$ & $ 3.6\pm1.16$ &$\textbf{4.4}\pm1.27$ & $ 0.0\pm 0.0$ & $ 3.6\pm1.16$ \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Final Move Selection}

Although the benefits of this enhancement are apparent, and it has always been a crucial part of MCTS-Solver \cite{winands08b}, its impact was never measured alone, apart from the selection part of the solver. Our experiments measure this impact, as we can directly compare combinations of enhancements with, and without final move selection: \verb|xxx| and \verb|Fxx|, \verb|xSx| and \verb|XSx|, \verb|xxU| and \verb|FxU|, finally \verb|xSU| and \verb|FSU|.

There are a few interesting observations here, the first one being that this is, in general, the most impactful improvement. It can improve winrate by over 55\% (LOA $7\times7$, \verb|xSU| and \verb|FSU|, \verb|xxU| and \verb|FxU|). Also for LOA, MiniShogi, and Knightthrough, the winratio is always improved by at least 30\%. 

Thus, it mostly helps if we already have a strong, well-playing algorithm. Final move selection is indeed well-paired with the solver, as the optimized selection phase makes proven subtrees reach the current root node faster. Lastly, it does not benefit so much drawish games as Awari, as it does not influence decision of when to draw instead of possibly losing. We come back to this topic in the next section.

\subsubsection{Solving Subtrees}

This enhancement is usually beneficial, but most often provides relatively small ($\leq 5\%$) improvement. Its greater positive impact is for LOA when going from \verb|Fxx| to \verb|FSx|. However, for some testcases solver tends to be harmful. The highest negative impact observed, over $-9\%$ is in Awari, between \verb|FxU| to \verb|FSU|.

\subsubsection{UCT-PN}

For all the tested games and relevant variant pairs, extending UCT formula with UCT-PN proves to be beneficial (with some ,,draws'' in case of Gomoku). All overall-best results for each game contain this enhancement. In 14 out of 24 comparable variants it resulted in winrates increased by more than 10\%. The highest observed improvement, 44.4\%, is for LOA $7\times7$, \verb|Fxx| and \verb|FxU|.  

On the one hand, these results should be expected, as the test set contains games where PNS has proven to be effective. On the other, it is not necessarily true that combining two valid approaches will result in an even better one. Also note that because UCT-PN uses a ranking system, it requires sorting nodes, which is the most costly part of the entire proposed extension.




\subsection{Time Trends}

To properly judge the usefulness of the proposed MCTS variant it is necessary to measure the effect of time and estimate the trends. We have selected three overall best-behaving variants: \verb|FSx|, \verb|FSU|, \verb|FxU|, and run them for different timelimits: 0.125, 0.25, 0.5, 1, and 2 seconds. The results of this experiment are presented in Table~\ref{tab:timetrends}.

Generally, MCTS-based algorithms perform better with more time. This means that any increase in PN-MCTS performance is not only a general improvement but also one relative to the inherent improvement that any enhanced MCTS would see with more time. If this was not the case, the win rate would not go up, and instead, stay roughly the same. 

Generally, the more time both algorithms have, the greater impact of the performance difference, and relatively pure MCTS makes much more simulations. However, the PN component of the PN-MCTS has the most impact in endgame positions, so with more time it is higher chance to reach them sooner. The trends depend on the balance of each of these factors for each of the tested domains.

For Lines of Actions, independently of the board size, there is generally a clear improving tendency for \verb|FSU|, \verb|FxU| variants. Results start from certain wins and after doubling the time the results are better or similar.  The \verb|FSx| variant has no such clear tendencies, going up and down. For MiniShogi, \verb|FSx| also behaves differently, with peak winrates at the lowest and highest time settings, while the remaining two variants have peaked around 1 second turn time. In Knighthrough, there is no universal tendency, and the behavior depends on the variant. The general trend for Awari is that with more time, the results of PN-MCTS are worse for all variants tested. Apart from the fact that PN-MCTS fails badly at Gomoku, the winrates are better for the  low time settings. Also, an interesting observation is that \verb|FSx| variant achieves the best results for small times compared to \verb|FxU|, which is better when 1 and 2 second clock is used.

\begin{table*}[t]
  \centering
\caption{Comparison of  best PN-MCTS variants in different time settings (1000 games, $C_{pn}=1$).}
\label{tab:timetrends}
\begin{tabular}{c||c|c|c|c|c|}
\toprule
\multicolumn{6}{c}{LOA $7\times 7$} \\ \hline
Variant & $1/8s$ & $1/4s$ & $1/2s$ & $1s$ & $2s$  \\ 
\verb|FSx| & $63.3\pm2.99$ & $70.9\pm2.82$ & $67.2\pm2.91$ & $65.6\pm2.95$ & $58.4\pm3.06$\\
\verb|FSU| & $85.4\pm2.19$ & $90.4\pm1.83$ & $91.9\pm1.69$ & $93.2\pm1.56$ & $91.0\pm1.78$\\
\verb|FxU| & $72.0\pm2.78$ & $77.3\pm2.60$ & $83.5\pm2.30$ & $90.4\pm1.83$ & $90.6\pm1.80$\\ \hline
\multicolumn{6}{c}{LOA $8\times 8$} \\ \hline
Variant & $1/8s$ & $1/4s$ & $1/2s$ & $1s$ & $2s$  \\ 
\verb|FSx| & $51.9\pm3.10$ & $57.5\pm3.07$ & $65.6\pm2.95$ & $66.4\pm2.93$ & $59.0\pm3.05$\\
\verb|FSU| & $60.0\pm3.04$ & $72.7\pm2.76$ & $85.4\pm2.19$ & $90.8\pm1.79$ & $96.2\pm1.19$\\
\verb|FxU| & $59.2\pm3.05$ & $58.0\pm3.06$ & $73.4\pm2.74$ & $83.2\pm2.32$ & $91.5\pm1.73$\\ \hline
\multicolumn{6}{c}{MiniShogi} \\ \hline
Variant & $1/8s$ & $1/4s$ & $1/2s$ & $1s$ & $2s$  \\ 
\verb|FSx| & $57.4\pm3.07$ & $53.9\pm3.09$ & $52.4\pm3.10$ & $53.2\pm3.09$ & $60.3\pm3.03$\\
\verb|FSU| & $58.1\pm3.06$ & $57.4\pm3.07$ & $63.0\pm2.99$ & $65.8\pm2.94$ & $64.5\pm2.97$\\
\verb|FxU| & $57.8\pm3.06$ & $56.8\pm3.07$ & $59.7\pm3.04$ & $64.5\pm2.97$ & $59.1\pm3.05$\\ \hline
\multicolumn{6}{c}{Knightthrough} \\ \hline
Variant & $1/8s$ & $1/4s$ & $1/2s$ & $1s$ & $2s$  \\ 
\verb|FSx| & $79.9\pm2.49$ & $73.0\pm2.75$ & $58.6\pm3.05$ & $59.2\pm3.05$ & $68.4\pm2.88$ \\
\verb|FSU| & $83.8\pm2.28$ & $77.7\pm2.58$ & $62.9\pm3.00$ & $66.8\pm2.92$ & $68.2\pm2.89$ \\
\verb|FxU| & $50.4\pm3.10$ & $54.1\pm3.09$ & $56.9\pm3.07$ & $56.4\pm3.08$ & $50.5\pm3.10$ \\ \hline
\multicolumn{6}{c}{Awari} \\ \hline
Variant & $1/8s$ & $1/4s$ & $1/2s$ & $1s$ & $2s$  \\ 
\verb|FSx| & $39.9\pm3.01$ & $39.4\pm2.99$ & $38.1\pm2.93$ & $38.0\pm2.94$ & $32.7\pm2.84$\\
\verb|FSU| & $56.6\pm3.02$ & $51.9\pm3.07$ & $41.6\pm3.02$ & $40.6\pm3.01$ & $33.6\pm2.90$\\
\verb|FxU| & $64.8\pm2.78$ & $57.6\pm2.84$ & $55.0\pm2.71$ & $49.8\pm2.72$ & $48.0\pm2.59$\\ \hline
\multicolumn{6}{c}{Gomoku} \\ \hline
Variant & $1/8s$ & $1/4s$ & $1/2s$ & $1s$ & $2s$ \\ 
\verb|FSx| & $13.5\pm2.12$ & $10.0\pm1.86$ & $2.4\pm0.95$ & $3.6\pm1.16$ & $2.6\pm0.99$ \\
\verb|FSU| & $11.3\pm1.96$ & $ 8.4\pm1.72$ & $4.3\pm1.26$ & $3.6\pm1.16$ & $1.5\pm0.75$\\
\verb|FxU| & $13.0\pm2.09$ & $ 8.8\pm1.76$ & $4.4\pm1.27$ & $4.4\pm1.27$ & $1.9\pm0.85$\\ 
\bottomrule
\end{tabular}
\end{table*}




%
%\begin{table}[H]
%\begin{center}
    %\caption{Time Setting Experiments: PN-MCTS against MCTS, LOA 7$\times$7}
    %\label{tab:timeLOA7x7}
    %\begin{tabular}{|c|c|}
    %\hline
    %Time per turn (s) & Winrate\% PN-MCTS\\
    %\hline
    %0.1 & 65.2($\pm$5.90)\\
    %\hline
    %0.5 & 85.6($\pm$4.35)\\
    %\hline
    %1.0 & 87.6($\pm$4.09)\\
    %\hline
    %2.0 & 92.4($\pm$3.28)\\
    %\hline
    %5.0 & 91.6($\pm$3.44)\\
    %\hline
%
    %% \hline
    %% Time Allowance (s) & 0.1 & 0.5 & 1.0 & 2.0 & 5.0\\
    %% \hline
    %% Winrate\% PN-MCTS & 65.2 & 85.6 & 87.6 & 92.4 & 91.6\\
    %% \hline
    %\end{tabular}
%\end{center}
%\end{table}
%
%\begin{table}[H]
%\begin{center}
    %\caption{Time Setting Experiments: PN-MCTS against MCTS, LOA 8$\times$8}
    %\label{tab:timeLOA8x8}
    %\begin{tabular}{|c|c|}
    %\hline
    %Time per turn (s) & Winrate\% PN-MCTS\\
    %\hline
    %0.1 & 59.2($\pm$6.09)\\
    %\hline
    %0.5 & 68.0($\pm$5.78)\\
    %\hline
    %1.0 & 83.2($\pm$4.63)\\
    %\hline
    %2.0 & 91.2($\pm$3.51)\\
    %\hline
    %5.0 & 94.0($\pm$2.94)\\
    %\hline
    %% \hline
    %% Time Allowance (s) & 0.1 & 0.5 & 1.0 & 2.0 & 5.0\\
    %% \hline
    %% Winrate\% PN-MCTS & 59.2 & 68.0 & 83.2 & 91.2 & 94.0\\
    %% \hline
    %\end{tabular}
%\end{center}
%\end{table}



%%\subsubsection{UCT-Parameter}
%Table \ref{tab:mctsLOA7x7} shows the result of the experiments in which the UCT-parameter is varied. The table does not show any significant differences between the different parameter settings. It may be because PN-MCTS already inherently does its own exploration, making the additional exploration function less relevant. In any case the $C$ does not seem to make a big impact on the win rate of PN-MCTS. Thus, leaving this parameter at $\sqrt{2}$ for the other experiments is reasonable.
%
%\begin{table}[h!!]
%\begin{center}
    %\caption{UCT-Parameter Experiment: PN-MCTS against MCTS in 7$\times$7 LOA }
    %\label{tab:mctsLOA7x7}
    %\begin{tabular}{|c|c|}
    %\hline
    %$C$ & Win rate\% PN-MCTS\\
    %\hline
    %0.5 & 90.0($\pm$5.88)\\
    %\hline
    %1.0 & 90.0($\pm$5.88)\\
    %\hline
    %$\sqrt{2}$ & 89.0($\pm$6.13)\\
    %\hline
    %1.75 & 88.0($\pm$6.37)\\
    %\hline
    %2.0 & 92.0($\pm$5.32)\\
    %\hline
    %
    %% \hline
    %% $C$ & 0.5 & 1.0 & $\sqrt{2}$ & 1.75 & 2.0\\
    %% \hline
    %% Winrate\% PN-MCTS & 90.0 & 90.0 & 89.0 & 88.0 & 92.0\\
    %% \hline
    %\end{tabular}
%\end{center}
%\end{table}

%Due to the scope of this paper, there will not be more investigation into the specifics of what makes a domain more suitable to use PN-MCTS. This type of investigation would likely require a full array of experiments as was done in the earlier sections in the LOA domain. 
%Still, this table does show that PN-MCTS has merit in more domains than just LOA if the conditions and settings are good.

% \begin{table}[h!!]
% \begin{center}
%     \caption{Domain Experiment: Win rate\% of PN-MCTS in various games at various time settings against basic MCTS}
%     \label{tab:domains}
%     \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     Game domain & 0.1s & 1.0s & 2.0s\\
%     \hline\hline
%     LOA 8$\times$8 & 65.2($\pm$5.90) & 87.6($\pm$4.09) & \textbf{92.4($\pm$3.28)}\\
%     \hline
%     MiniShogi & \textbf{86.0($\pm$4.30)} & 76.8($\pm$5.23) & 67.2($\pm$5.82)\\
%     \hline
%     Knightthrough & 46.0($\pm$6.18) & 60.4($\pm$6.06) & \textbf{63.6($\pm$5.96)}\\
%     \hline
%     Awari & \textbf{61.2($\pm$6.04)} & 49.8($\pm$6.20) & 31.1($\pm$5.74)\\
%     \hline
%     Gomoku & \textbf{32.4($\pm$5.80)} & 9.6($\pm$3.65) & 11.2($\pm$3.91)\\
%     \hline
    
%     \end{tabular}
% \end{center}
% \end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two-Layer PN-MCTS}\label{sec:PNMCTS2}

As already mentioned, PN-MCTS in the form presented above does not handle properly games with draws: proven nodes are won nodes, but disproven nodes can be drawn or lost. However, we have a solution for this issue.

Instead of remembering just a proof and disproof number for each MCTS tree node, we remember two additional values: second-layer proof number and second-layer disproof number. These data are processed exactly as the first-layer (dis)proof numbers described before, with a tiny difference: now the semantic of our proof is ,,not-lost''.  Thus, the game-theoretic values of each (dis)proof number are presented in Table~\ref{tab:2layersemantic}.

\begin{table}[h!!]
\begin{center}
    \caption{Game-theoretical values for two-layer PN-MCTS}
    \label{tab:2layersemantic}
    \begin{tabular}{c|c|c}
    \toprule
     & Proven nodes & Disproven nodes  \\ \hline
    First-Layer & Won & Drawn, Lost \\ 
    Second-Layer & Won, Drawn & Lost \\ 
    \bottomrule
    \end{tabular}
\end{center}
\end{table}



\subsection{Second-Layer Enhancements}


We have to decide how each of the previously proposed enhancements can benefit from the additional PN layer. The simplest case is for the solver part. The only difference is that instead of marking as resolved nodes that are proven on the first layer, we skip them if they are proven on the second layer, as it is easier to prove the superset. The remaining improvements are described in their respective sections accompanied by the experiments testing their effectiveness.

All the experiments are performed on Awari, as it is the only game  from our test domain with a significant number of draws, with a turn limit of 1 second.


\subsection{UCT-PN Rank Sorting}

For the UCT-PN part, there are multiple methods how for handling this new knowledge. One that seems straightfroward is to enhance the Formula~\ref{UCT-PN_formula}, and introduce a new parameter $C_{pn2}$ that will rank nodes and influence the equation ,,independently'' on the first layer. There are two downsides of this approach. One is that it introduces a new parameter that requires additional tuning. Second, in our opinion worse, is that it requires separate sorting of nodes to compute ranks, which will heavily influence the computational efficiency, which is already a problem. 

Thus, our proposed solution is to modify the sorting mechanism, and use second-layer (dis) proof numbers as a tiebreaker. To ensure that this idea works, we took all variants that use UCT-PN formula and compare them with the corresponding results of the single-layer PN-MCTS from the Table~\ref{tab:variants}. All variants use the final move selection algorithm described before, taking into account only first-layer proven nodes. The results of this experiment are presented in Table~\ref{tab:2layer1}.


\begin{table}[h!!]
\begin{center}
    \caption{Comparison of sorting method in UCT-PN formula (Awari, 1000 games, $C_{pn}=1$, 1 s per turn)}
    \label{tab:2layer1}
    \begin{tabular}{c|c|c}
    \toprule
     \multirow{2}{*}{Variant}& \multicolumn{2}{c}{UCT-PN formula} \\
     & First-Layer & Second-Layer  \\ \hline
    \texttt{xxU} & $48.0\pm2.65$ & $45.2\pm2.61$ \\ 
    \texttt{xSU} & $35.2\pm2.87$ & $40.7\pm2.68$ \\ 
    \texttt{FxU} & $49.8\pm2.72$ & $48.6\pm2.70$  \\ 
    \texttt{FSU} & $40.6\pm3.01$ & $46.4\pm2.83$ \\ 
    \bottomrule
    \end{tabular}
\end{center}
\end{table}


As we can see, on the variants where this new sorting method is paired with solver, the results are confidently better. In the remaining cases, they are slightly worse, but their confidence bounds are overlapping, so they are not strictly worse either. Thus, although the proposed second-layer UCT-PN is not a general improvement, it may lead to better results for some variants.


\subsection{Final Move Selection Contempt Factor}

Additional knowledge provided by second-layer PN allows us to choose a move that leads to a proven draw if one is available. However, it is not a straightforward decision when to do so, as by choosing such a move, we usually give up any chances to win a game.

Thus, the so-called \emph{Contempt Factor} is used to determine when to prefer draw over the other lines of play. In our case, if the root score is strictly smaller than a contempt factor, then the proven draw child is selected. 

To test the impact of this parameter, we chose two variants that gave the best results in the previous experiment: \texttt{FxU}, \texttt{FSU}, and extend it for different values of contempt factor: $<$-1, -0.2, 0, 0.2, 0.4, 0.6. Note that the possible outcomes of the game are -1, 0, and 1, thus the contempt factor less than -1 behaves exactly as single-layer PN-MCTS final move selection. The results of this experiment are presented in Table~\ref{tab:2layer2}.

\begin{table*}[t]
\centering
\caption{Comparison of various contempt factor values for Double-Layer PN-MCTS (Awari, 1000 games, $C_{pn}=1$, 1 s per turn).}
\label{tab:2layer2}
\begin{tabular}{c||c|c|c|c|c|c}
\toprule
\multirow{2}{*}{Variant}& \multicolumn{6}{c}{Contempt Factor} \\
 & $<$-1 & -0.2 & 0.0 & 0.2 & 0.4 & 0.6\\ \hline 
\verb|FxU|  & $48.6\pm2.70$ & $46.7\pm2.62$ & $\textbf{54.9}\pm2.66$ & $49.9\pm2.63$ & $50.0\pm2.61$ & $47.1\pm2.68$ \\
\verb|FSU|  & $46.4\pm2.83$ & $46.4\pm2.71$ & $\textbf{50.0}\pm2.74$ & $47.2\pm2.72$ & $45.6\pm2.74$ & $47.3\pm2.72$ \\
\bottomrule
\end{tabular}
\end{table*}

As we can see, the contempt factor has a non-negligible impact on the algorithm's performance. The right value, which in the case of both tested variants would be around 0.0, increases the scores of 3.6--6.3\% compared to the standard single-layer PN final move selection.

Thus, both improvements introduced by extending PN-MCTS by another layer of Proof-Number Search seem to have a positive impact on the algorithm behavior, increasing its winrate given the right conditions and parameter values.
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Research}\label{sec:Conc}

This paper has proposed PN-MCTS, a new Monte-Carlo Tree Search enhancement that combines 
Proof-Number Search and MCTS. We proposed several variants of the algorithm 
by taking advantage of the additional knowledge in a few areas: during the final move selection, during MCTS selection phase, and in the UCT formula. In particular, the ranking of the nodes according to their (dis)proof numbers is used to bias the UCT formula. 


While there is a computational cost necessary to obtain the proof and disproof numbers, for the right domain and parameter values, the benefits can outweigh the costs. 
The results show that for Lines of Action, MiniShogi, and Knightthrough, one can choose a variant of PN-MCTS that outperforms MCTS for all time settings tested, some of them reaching a winrate up to 96\%. However, all proposed ideas failed on Gomoku, where our algorithm did not exceed even 15\%. An interesting case is Awari, where many tested variants are losing or drawing, but, especially for time limits below 1 second per move, PN-MCTS can confidently win.

Because PNS can only deal with binary outcomes, in the basic PN-MCTS implementation the disproof number is the same for a draw and a loss, potentially steering the search in the wrong direction. We aimed to resolve this issue by introducing a second layer of proof numbers to distinguish draws from losses. With this, Double-Layer PN-MCTS was able to win in Awari for the same variant and time settings that was previously drawish.


 
%PN-MCTS might require less time per turn to be effective if the domain has reaches endgame board states sooner. It can also be concluded that the optimal choice for $C_{pn}$ is domain specific and relies on how far removed the endgame board states are from the initial board states.

%For future research, there are a number of potential approaches that could further improve this combination of PNS and MCTS. There is the option to use the regular UCT first and only switch to UCT-PN once either enough simulations are done or once endgame board states are reached, as that is when PN actually provides useful information.   
%Research can also be performed to investigate which domains and parameters are optimal for PN-MCTS and why. A final potential research line could be to investigate if the proof and disproof numbers could also be combined with versions of MCTS that are already enhanced. There are many MCTS variations nowadays and it might that the proof and disproof number could increase the performance of some of them further.


There are multiple directions for future research. One is to test PN-MCTS on more domains and to investigate the reason why it does (not) work for certain games. Concurrently, one could test more parameter values with different time settings, and develop alternative methods of combining UCT formula with PN values. Although the ranking idea worked best from what we have tested so far, further studies on te subject are required. 
Eventually, we could search for more advancement enhancements taking advantage of the concept of multi-layer PN-MCTS. In particular, extend the two-layers variant into a multi-layer one, to handle games with multiple outcomes. Alternatively, this concept can be turned out to handle multiple players by computing proven subtrees against each of them separately.


\section{Acknowledgments}

This research is partially funded by the European Research Council as part of the Digital Ludeme Project (ERC Consolidator Grant \#771292).

This work was supported in part by the National Science Centre, Poland, under project number 2021/41/B/ST6/03691 (Jakub Kowalski).
%(Jakub Kowalski, Marek Szykuła).

\bibliographystyle{IEEEtran} 
\bibliography{refs,references} 

\end{document}
