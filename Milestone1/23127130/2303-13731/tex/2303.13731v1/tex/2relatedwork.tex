\section{Related Work}
Our work belongs to the visual analytics attempts towards more interpretable deep learning (DL), with a special focus on interpreting multi-head self-attention from transformers. We thus review earlier works from these two aspects.

\textit{\textbf{Visualizations for DL.}} 
A plethora of visualization works have been introduced for the interpretation of deep neural networks recently~\cite{liu2016towards,strobelt2017lstmvis,wang2018ganviz,wang2018dqnviz,jin2022gnnlens}.
We refer readers to recent surveys~\cite{hohman2018visual, choo2018visual} for a thorough review of these works. Lately, deep transformers demonstrate superior performance than other DL models on 1D sequential data, and multiple visualization works have been introduced for their interpretations~\cite{kovaleva2019revealing,park2019sanvis,vig2019multiscale,jaunet2021visqa, derose2020attention,li2021t3}. The success of transformers has also been extended to 2D images with the seminal work of vision transformers (ViTs)\cite{dosovitskiy2020image}. However, to the best of our knowledge, no comprehensive visual analyses have been conducted to demystify this type of powerful yet complex models, especially how attention works in the 2D image context. Our work tries to fill this gap.

\textbf{\textit{Attention Visualization.}} 
The attention mechanism~\cite{bahdanau2014neural} has been used extensively in DL, especially NLP-related tasks, to learn what target tokens the source tokens should ``look at''. Essentially, attention is a matrix where each cell denotes the attention magnitude that the source token (row) pays to the target (column). Popular attention visualization techniques include flow maps~\cite{dong2020interactive, strobelt2018s}, parallel coordinates plots (PCPs)~\cite{vig2019multiscale}, and heatmaps~\cite{jaunet2021visqa, park2019sanvis,aflalo2022vl}. For example, the flow maps used by Dong et al.~\cite{dong2020interactive} connect the source and target tokens with curves, the widths of which denote the attention strengths. 
Vig~\cite{vig2019multiscale} arranges the source and target tokens along two parallel axes (i.e., a simplified PCP) and connects them with line segments in between to show the attention patterns.  
Heatmaps are used extensively in NLP~\cite{abnar2020quantifying,kovaleva2019revealing,li2021t3}, where the attention strengths are directly encoded into the color of each heatmap cell. 
There are also customized visual designs for attention visualizations~\cite{derose2020attention, jaunet2021visqa}. For instance, DeRose et al.~\cite{derose2020attention} extract an ``attention graph'' from the attentions across layers of a BERT model and arrange the graph into a radial layout to present the propagation of attentions layer-by-layer. The resulting visualization, named Attention Flows, helps to easily analyze and compare attentions from two transformer models. 

For \textit{attention patterns} (in individual transformer heads), researchers have discovered some typical ones~\cite{park2019sanvis}, analyzed their occurrence in different tasks~\cite{kovaleva2019revealing}, 
compared the patterns between low and high-performing models~\cite{jin2022visual}, and related them with the corresponding heads' importance~\cite{li2021t3}. 
However, these works all focus on 1D sequential data. Attentions learned from images with a 2D spatial context have much richer patterns that are difficult to be identified and summarized manually. Our work intends to efficiently discover them.
For \textit{attention strengths} between patches within a ViT head, \textit{mean attention distance} has been introduced in previous works~\cite{cao2020behind, dosovitskiy2020image, raghu2021vision}, which is a sum of the attentions between patches weighted by their spatial distance. This single aggregated value holistically reflects each head's attention strength, but also averages out many spatial details. Here, we introduce the \textit{attention strength vector} to comprehensively profile the spatial distribution of attention strengths.




