\section{Discussion, Limitations, and Future Work}

Despite many visual interpretation works for DL, \textit{\textbf{the unique values of our work}} come from the following perspectives. 
First, our work presents a comprehensive interpretation of ViTs and discloses insightful findings.
For example, heads with strong self-attentions
are dominantly important. Lower- and higher-layer heads show different local/global attention strengths. Also, we summarize all possible attention patterns between patches. These insights open the hood of ViTs and deepen model designers' understanding.
Second, our interpretation triggers model improvement ideas, e.g., pruning heads with repeating patterns. Thus, improving ViTs with our derived insights would be a direct follow-up work.
Lastly, although we focus only on the classification task, we believe our interpretations are transferable to ViT-based detection/generation tasks~\cite{khan2021transformers}, as those tasks also significantly rely on the multi-head self-attentions of ViTs.

\textbf{\textit{Head-Centric v.s. Image-Centric.}}
We want to emphasize that all our analyses are \textit{head-centric}, and each head's behavior is analyzed in one and across all images.
Specifically, for \textit{head importance}, we provide each head's \textit{local} importance on one image and \textit{global} importance over all images (Sec.~\ref{sec:head_imp_vis}). For \textit{head attention strength}, we present a head's attention strengths in one image (Fig.~\ref{fig:teaser}-C2) and its strength distribution over all images (Fig.~\ref{fig:teaser}-C3). For \textit{head attention pattern}, the two-axes/heatmap (Fig.~\ref{fig:teaser}-D3, D4) shows the attention pattern of a head from one image, while the scatterplot in Fig.~\ref{fig:teaser}-D1 lays out the head's attention pattern over all images. From a different perspective, we believe \textit{image-centric} analysis would also lead to insightful findings, e.g., checking if the heads show similar patterns for images of the same class. We plan to explore this direction in the future.

\textbf{\textit{Performance.}} 
To guarantee the exploration interactivity, we have pre-computed some of the visualization data.
For example, the head importance metrics are computed offline as they can take hours. The partial pruning in Fig.~\ref{fig:teaser}-B2 is performed online and each computation takes about 0.6 seconds on an Nvidia Titan RTX GPU. The head attention strengths and the tSNE layout for head attention patterns are both computed offline as they only need to be computed once and directly plugged into our system.
%
In terms of storage, the raw attention weights consume the most space, ranging from 11 GB to 178 GB, depending on the number of heads in the studied ViT. Other data (e.g., images, probabilities, tSNE results) take about 300 MB in total.

\textbf{\textit{Limitations and Future Work.}} Our head importance analysis relies on leave-one-out ablations, which do not consider the interaction between heads. In some cases, one head could be important only if another head is pruned. The analysis can be further extended to higher-order interactions, which is our planned future work. 
Second, our current analysis focuses on the attentions between two consecutive attention layers only. In the future, we would like to explore attention aggregation methods, e.g.,~\cite{abnar2020quantifying}, to interpret heads' impact across multiple layers.
Lastly, we plan to investigate if the head importance, head attention strengths, and head attention patterns show any class-specific or dataset-specific trends. This will help to diagnose class-related performance issues and validate our findings in more datasets.
