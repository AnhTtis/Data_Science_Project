\section{Methodology and Visualization System}
\label{sec:system}

Our visual analytics system (Fig.~\ref{fig:teaser}) contains four components: the \imageoverview{}, the \headimportance{}, the \attnstrength{}, and the \attnpattern{}. The \imageoverview{} (Fig.~\ref{fig:teaser}A) lays out image instances based on their heads' importance vector, providing an entry point for the exploration. The remaining three views (Fig.~\ref{fig:teaser}B-D) are designed to meet the three requirements.

\subsection{The Image Overview}
The \imageoverview{} (Fig.~\ref{fig:teaser}A) uses tSNE+scatterplot to provide an overview of the images. 
Each point represents an image, and its color denotes the class label. The coordinates of each point are the dimensionality reduction result of the corresponding image's \textit{head importance vector}, i.e., a $(l{\times}n)$-dimensional vector with the corresponding head's importance (Eq.~\ref{eq:prob}) at each dimension.
The tSNE layout based on this vector clusters images with similar head importance together, guiding users' exploration (\textbf{R1.3}). For example, there is a small cluster in the top-right corner, which immediately catches users' attention during exploration (see Sec.~\ref{sec:case_study}). 

Clicking on any point or providing an ID in the top-right input box will select the corresponding image into the other three views. Inside each view, the analysis can focus on the selected image or be extended to all other images.

\subsection{The Head Importance View}
\label{sec:head_importance}
We define several metrics to quantify the importance of a head. 
These metrics are generated by ``leave-one-out'' ablations, i.e., encoding a head's importance by the changes in the final output (\textit{model-level impact}) or next-layer activations (\textit{layer-level impact}) after pruning the head. Pruning a head is conducted by setting its attention matrix ($A$ in Eq.~\ref{eq:single_head}) to 0.
Similar head/neuron importance analysis through ablation studies has been widely adopted in NLP, e.g.,~\cite{bau2018identifying,michel2019aresixteen,hao2021self}.

\subsubsection{Importance to the Model's Output (\textbf{R1.1})}
\label{sec:global_importance}
We propose two \textit{model-level} importance metrics for each head.
One reflects the probability change of the true class (Eq.~\ref{eq:prob}); the other encodes the Jensen-Shannon Divergence (JSD) between the two probability distributions (Eq.~\ref{eq:jsd}) before and after a head is pruned.
Mathematically, $ViT()$ denotes the well-trained model, which takes an image as input and outputs a probability distribution, i.e., $\textbf{P} {=} ViT(img)$. $ViT_{i, j}()$ is the same model but the $j$th head from the $i$th layer has been pruned, and $\textbf{P}_{i,j} {=}ViT_{i, j}(img)$. $idx_{label}$ is the image's true class index. The importance of head $(i, j)$ is:
\begin{equation}\small
\label{eq:prob}
   I^{prob}_{i,j} = \textbf{P}{[idx_{label}]}-\textbf{P}_{i,j}[idx_{label}]
\end{equation}
\vspace{-0.15in}
\begin{equation}\small
\label{eq:jsd}
   I^{JSD}_{i,j} = JSD(\textbf{P}||\textbf{P}_{i,j}) 
\end{equation}

\subsubsection{Importance to the Attention Layer (\textbf{R1.1})}
\label{sec:local_importance}
Assessing only the changes in final outputs cannot reflect a head's importance in its attention layer. As our experts noticed, pruning an important head may significantly change the corresponding layer's output (i.e., $z$ in Eq.~\ref{eq:multi_head}), but show minor changes to the final probabilities. This is because heads from later layers may compensate for the contribution of the pruned head, concealing its importance.

To identify the important heads in each attention layer, we propose two \textit{layer-level} importance metrics, which are defined by the cosine distance ($D_{cos}$) between the immediate layer activations before and after a head is pruned. As shown in Fig.~\ref{fig:architecture}, the attention layer's output $z$ is a $(1{+}p^2){\times}h$ matrix, containing the activations of the \texttt{CLS} (the first $1{\times}h$) and patch tokens (the later $p^2{\times}h$). Our layer-level metrics measure the importance of the \texttt{CLS} and patch tokens separately.
%
For \texttt{CLS}, the metric ($I_{i,j}^\texttt{CLS}$) reflects the cosine distance between the two \texttt{CLS} activations. For patch tokens, the metric ($I_{i,j}^{patch}$) similarly computes the cosine distances and averages the distances over all patches.
Mathematically ($z$ and $z'$ are the layer's output before and after pruning),
\begin{equation}\small
    I_{i,j}^{{CLS}} = D_{cos}(z[0], z'[0])
    \label{eq:cls}
\end{equation}
\begin{equation}\small
    I_{i,j}^{patch} =\frac{1}{p^2}\Sigma_{i=1}^{p^2} D_{cos}(z[i], z'[i])
    \label{eq:patch}
\end{equation}


\subsubsection{Head Pruning Modes (\textbf{R1.2})}

Once the important heads are identified, we further dissect their importance by partially pruning them.
\begin{figure}[tbh]
    \centering
    \includegraphics[width=.88\columnwidth]{fig/pruning_modes.pdf}
    \vspace{-0.1in}
    \caption{Pruning modes. (A) The attention matrix is divided into four regions. (B) Different pruning modes set one/multiple regions to 0.}
    \label{fig:pruning_modes}
\end{figure}

As shown in Fig.~\ref{fig:pruning_modes}A, the attention matrix of a head can be divided into four regions based on the source and target tokens: $\texttt{CLS} {\to} \texttt{CLS}$, $\texttt{CLS} {\to} patches$, $patches {\to} \texttt{CLS}$, and $patches {\to} patches$.
Regions $\texttt{CLS}{\to} patches$ and $patches {\to} \texttt{CLS}$ are considered together, as they both encode the interaction between the \texttt{CLS} and patch tokens.
Six pruning modes are defined by setting different regions % of the attention matrix 
to zero (Fig.~\ref{fig:pruning_modes}B), i.e., mode 0 is the original head without pruning; mode 1 prunes the head completely; modes $2{\sim}5$ are additional cases where only the striped regions are pruned. Showing the impacts from these modes attributes the head's importance to individual regions.

\subsubsection{Visualization}
\label{sec:head_imp_vis}
The \headimportance{} (Fig.~\ref{fig:teaser}B) visualizes our proposed metrics and pruning modes. First, Fig.~\ref{fig:teaser}-B1 uses a line chart to present the four head importance metrics for a single selected image (i.e., the heads' \textit{local} importance to an image). The horizontal axis represents all the $l{\times}n$ heads, and the vertical axis denotes a metric's value, where the dropdown widget enables users to switch among the four metrics. Note that for $I_{i,j}^{prob}$, we directly show the value of $\textbf{P}_{i,j}[idx_{label}]$ (instead of the difference in Eq.~\ref{eq:prob}) as it is more intuitive. When no image is selected (e.g., at the beginning of exploration), the curve in this view shows the average value of the selected metric over all images. Meanwhile, a blue band surrounding the curve denotes the standard deviation of the metric's values (see Fig.~\ref{fig:importance_summary}). The mean and standard deviation reflect the \textit{global} importance over all images, guiding users to select globally important heads. 

Second, after a head is selected from Fig.~\ref{fig:teaser}-B1 (by dragging the vertical line), the bar-chart in Fig.~\ref{fig:teaser}-B2 shows the selected importance metric (y-axis) in different pruning modes (x-axis), further dissecting the head's importance. For example, Fig.~\ref{fig:teaser}-B2 reveals that the importance of the selected head originates from the patch tokens solely, and pruning \texttt{CLS}-related attentions shows no impact.

Lastly, Fig.~\ref{fig:teaser}-B3 shows the top-5 predicted probabilities for the selected image, in the current pruning. If the true label is among the top 5, it will be highlighted in bold.

\subsection{The Attention Strength View}
The attention strength of a head characterizes the spatial distributions of the attention strength across all patches, which answers why the head is important by disclosing where it makes the patches focus.

\subsubsection{Attention Strength Over $k$-Hop Neighbors (\textbf{R2.1})}


We define a $p$-dimensional ($p$D) \textit{attention strength vector}, $\textbf{s}$, for each head, which profiles the average attention strength of all patches to their $k$-hop neighbors ($k\in [0, p{-}1]$), i.e., 
\vspace{-0.1in}
\begin{equation}\small
\label{eq:khop}
    \textbf{s}=\frac{1}{p^2}\Sigma_i\Sigma_j \textbf{s}^{(i,j)},\  i{\in}[0, p{-}1],\ j{\in}[0, p{-}1],
\end{equation}
\vspace{-0.13in}
\begin{equation}
\textbf{s}^{(i,j)} = <s^{(i,j)}_0, s^{(i,j)}_1, s^{(i,j)}_2, \dots, s^{(i,j)}_{p - 1}>,
\end{equation}
where $s^{(i,j)}_k$ denotes the average attention from patch $(i,j)$ to its $k$th hop neighbors in the 2D domain.
\begin{figure}[tbh]
    \centering
    \includegraphics[width=.98\columnwidth]{fig/attention_strengths.pdf}
    \vspace{-0.1in}
    \caption{Computing the attention strength vector for a single head.}
    \label{fig:attention_strengths}
\end{figure}

Without loss of generality, Fig.~\ref{fig:attention_strengths} shows the computation of $\textbf{s}$ when $p{=}5$. Starting from patch $(0,0)$, $s^{(0,0)}_0$ is the attention that patch $(0,0)$ paid to itself (i.e., 0-hop attention); $s^{(0,0)}_1$ is the sum of the attentions paid to its 1-hop neighbors in yellow divided by the number of neighbors (i.e., 3); $s^{(0,0)}_2$ is the total attentions paid to its 2-hop neighbors in green divided by the number of neighbors (i.e., 5); and so on so forth. To the end, we get a $5$D vector for patch (0, 0), i.e., $\textbf{s}^{(0,0)}$. Repeating this computation to all patches, we get $25$ $5$D attention strength vectors, one for each patch. Their average is the head's \textit{attention strength vector}, i.e., $\textbf{s}$.

Note that some patches may not have certain hops of neighbors, e.g., patch $(2,2)$ in Fig.~\ref{fig:attention_strengths} does not have $3$-hop or $4$-hop neighbors (marked as `$\times$'). Therefore, $s^{(2,2)}_3$ and $s^{(2,2)}_4$ will not be counted when computing the corresponding element of vector $\textbf{s}$. In other words, the denominator in Eq.~\ref{eq:khop} is not $p^2$ for all elements of $\textbf{s}$; some will have a smaller denominator due to the missing neighbors.

\subsubsection{Visualization}
The \attnstrength{} (Fig.~\ref{fig:teaser}C) presents all heads' attention strength with three components. The first component (Fig.~\ref{fig:teaser}-C1) presents an overview of all heads for the selected image through a scatterplot. Each point in the scatterplot is a head. Its horizontal position (as well as its color) reflects the layer that the head is from. Its vertical position denotes the entropy of the head's attention strength vector $\textbf{s}$ (normalized). The entropy of $\textbf{s}$ reflects if the head's attention strength is localized on a certain-hop of neighbors (low-entropy, one element's value dominates the vector) or spread across all $k$-hop neighbors (high-entropy, all elements' values are similar). From the overview, there is an obvious trend of the heads across layers (\textbf{R2.2}), i.e., heads from higher layers attend more evenly to all patches, whereas lower-layer heads attend either locally or globally.


Second, after a head is selected by clicking the corresponding point in Fig.~\ref{fig:teaser}-C1, its attention strength vector is presented as a bar chart in Fig.~\ref{fig:teaser}-C2 (\textbf{R2.1}). In the current visualization, we can see that all patches in the selected head attend only to themselves (i.e., all attention strengths are distributed to the 0-hop neighbors).

Lastly, the area plot in Fig.~\ref{fig:teaser}-C3 presents the distribution of entropy values for the selected head over all images (\textbf{R2.3}). For example, the currently selected head has a small entropy (Fig.~\ref{fig:teaser}-C1) as all patches attend to 0-hop neighbors only (Fig.~\ref{fig:teaser}-C2). Fig.~\ref{fig:teaser}-C3 further reveals that the head has consistently low entropy across all images, reflected by the peak on the left corner. The vertical line over the area plot marks the head's entropy for the currently selected image, reflecting how much the head's attention strength for the current image varies from its strength for other images. 

\subsection{The Attention Pattern View}

The attention pattern of a head reflects how tokens are attending to each other. We want to summarize the possible patterns of all heads to deepen the understanding of ViTs. 

\subsubsection{Unsupervised Pattern Identification (\textbf{R3.1})}
\label{sec:pattern_generation}
Due to the functionality difference between the \texttt{CLS} and patch tokens, we treat them separately and learn their respective patterns.
Specifically, given an input image and one of its heads, the corresponding attention matrix $A$ is of shape ${(1{+}p^2){\times}(1{+}p^2)}$ (Fig.~\ref{fig:architecture}). 
We separate $A$ into \texttt{CLS}-related attentions $A_{\texttt{CLS}} {=} concat(A[0, :], A[1:, 0]) {\in} \mathbb{R}^{2p^2 {+} 1}$ and patch attentions $A_{patch} {=} A[1:, 1:] {\in} \mathbb{R}^{p^2 \times p^2}$, as shown in Fig.~\ref{fig:attention_pattern_vis}A.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=\columnwidth]{fig/attention_pattern_vis.pdf}
    \vspace{-0.25in}
    \caption{(A) Each attention matrix is separated into \texttt{CLS}-related attentions ($A_{\texttt{CLS}}$, blue) and patch attentions ($A_{\texttt{patch}}$, orange). (B) $A_{\texttt{CLS}}$ from all images is visualized through tSNE+scatterplot. (C) For $A_{\texttt{patch}}$ from all images, we train an autoencoder to cluster them. (D, E) $A_{\texttt{CLS}}$ and $A_{\texttt{patch}}$ can be mapped back to the image as a mask.}
    \label{fig:attention_pattern_vis}
\end{figure}

\textbf{\texttt{CLS} Attention Patterns.}
The \texttt{CLS}-related attentions ($A_{\texttt{cls}}$) concatenates the $\texttt{CLS}{\to}\texttt{CLS}$, $\texttt{CLS}{\to} patches$, and $patches{\to}\texttt{CLS}$ regions (Fig.~\ref{fig:attention_pattern_vis}A), and its size is $(2p^2{+}1)$. If we have $m$ images, each generates $l{\times}n$ attention matrices from the $l{\times}n$ heads, we will have $m{\times}l{\times}n$ such vectors. Using tSNE, we project them from $(2p^2{+}1)$D to 2D and present them with a scatterplot (Fig.~\ref{fig:attention_pattern_vis}B). Attention heads with similar \texttt{CLS} attention patterns will be clustered together.

\textbf{Patch Attention Patterns.} We applied the same method to the patch attentions $A_{patch} {=} A[1:, 1:] {\in} \mathbb{R}^{p^2 {\times} p^2}$, but the resulting tSNE layout could not clearly separate/cluster dissimilar/similar patch attention patterns. We believe this is caused by the much higher dimensionality of the patch attentions and tried to fix it with several remedies. For example, we used max pooling to spatially shrink $A_{patch}$ before tSNE, and tried to apply PCA on $A_{patch}$ before tSNE. Both solutions did not yield much performance gain.

In the end, we came up with an autoencoder (AE)-based learning solution (Fig.~\ref{fig:attention_pattern_vis}C). \textit{First}, as we care more about the attention pattern, instead of the magnitude, we binarize $A_{patch}$ using a cutoff, e.g., setting top 1\% values to 1 and the rest to 0. This enhances the patterns and makes them easier to learn. \textit{Second}, using the $m{\times}l{\times}n$ binarized $A_{patch}$, we train an AE. The AE has two symmetric subnetworks, i.e., the encoder and decoder, each with two convolutional layers and one fully-connected layer. \textit{Third}, using the latent representations from the well-trained AE's bottleneck layer, we conduct tSNE layout. The layout shows obvious clusters, exposing different attention patterns.

\subsubsection{Visualization}
The \attnpattern{} adopts the ``overview+details'' exploration strategy to visualize the attention patterns.

The \textbf{overview} presents all heads from all images ($m{\times}l{\times}n$ in total) through tSNE+scatterplot (\textbf{R3.1}, Fig.~\ref{fig:teaser}-D1), as explained in Sec.~\ref{sec:pattern_generation}. 
The tSNE layout could be either for the \texttt{CLS} attentions ($A_{\texttt{CLS}}$) or for the patch attentions ($A_{patch}$). The top-right toggle enables this switch.
To be scalable, we allow users to convert the scatterplot into a density plot, and the top-left toggle controls this. For example, the background density contours in Fig.~\ref{fig:teaser}-D1 present the distribution of all the $m{\times}l{\times}n$ heads, as a context. When an image of interest is selected from the \imageoverview{} (Fig.~\ref{fig:teaser}A), its $l{\times}n$ heads will be shown on top of the density plot as points, the color of each reflects its layer.

The \textbf{details} of the attention matrix for a selected head are shown in the right of Fig.~\ref{fig:teaser}D (\textbf{R3.2}). 
An attention matrix denotes the attention between ($1{+}p^2$) tokens, and we present it in two different manners (Fig.~\ref{fig:teaser}-D3, D4).

Fig.~\ref{fig:teaser}-D3 lists all ($1{+}p^2$) tokens as two rows (top: source tokens, bottom: target tokens) and uses lines with light to dark color to encode the attention magnitude. Blue and orange are used to color $A_{\texttt{CLS}}$ and $A_{patch}$ respectively. Showing all the $(1{+}p^2){\times}(1{+}p^2)$ lines would make the view very cluttered. Therefore, we enable users to specify a threshold, the lines with associated attention value below which will be disabled. The histogram in Fig.~\ref{fig:teaser}-D5 shows the distribution of the $(1{+}p^2){\times}(1{+}p^2)$ values, guiding users to specify the threshold by dragging the vertical bar on top of the histogram. The current threshold in Fig.~\ref{fig:teaser}-D5 is 1\%, indicating only the top 1\% lines are visible. 
From the dark vertical lines in Fig.~\ref{fig:teaser}-D3, we can easily see that all tokens (both \texttt{CLS} and patch tokens) strongly attend to themselves.

Fig.~\ref{fig:teaser}-D4 presents the attention matrix through a heatmap (row: source token; column: target token). The four parts of the heatmap have been illustrated in Fig.~\ref{fig:pruning_modes}A. For $A_{patch}$ in the bottom-right corner, one pixel represents one attention value. For $A_{\texttt{CLS}}$, as one pixel is barely visible for the single row and column of attention values, we augment them to take 10 pixels. The color mapping is consistent with that in Fig.~\ref{fig:teaser}-D3 (blue: $A_{\texttt{CLS}}$; orange: $A_{patch}$). 
From the heatmap, we can observe a clear diagonal pattern, indicating that the patch tokens attend strongly to themselves. The \texttt{CLS} token also strongly attends to itself, as the top-left cell is in dark blue. Meanwhile, \texttt{CLS} also attends to different patch tokens, but the attention magnitude is very small (light blue or white color in the top row). The threshold specified from the histogram in Fig.~\ref{fig:teaser}-D5 also applies to this heatmap.

The reason for presenting the attention matrix in two visualizations is to leverage their respective advantages.
The heatmap shows the attention patterns more intuitively, whereas the two-axes view can better present the attention relationship between the \texttt{CLS} and patch tokens (one example is shown later in Fig.~\ref{fig:bottom}). The two-axes view is also better than the heatmap in terms of interacting with tokens, e.g., it can easily highlight a token of interest (see Fig.~\ref{fig:case_head137}).
Apart from these two, we have also considered other visualizations in our early design stages. For example, we tried to overlay arrows on top of the heatmap to show the attention direction or embed patch pixels into the heatmap. However, both designs are not easily scalable to our problem size.

\textbf{Image Context.} To intuitively present the patch-related attentions, we need to map the patches back onto the 2D image. For $\texttt{CLS}{\to}patches$ (shape: $1{\times}p^2$) and $patches{\to}\texttt{CLS}$ (shape: $p^2{\times}1$) attentions, we reshape them to a $p{\times}p$ square, scale the square to $w{\times}w$, and overlay it on top of the image as a transparency mask (Fig.~\ref{fig:attention_pattern_vis}D, stronger attention${\to}$more transparent). For the $patches{\to}patches$ attentions (shape: $p^2{\times}p^2$), we reshape individual row/column into a $p{\times}p$ square, scale it to $w{\times}w$, and overlay it on top of the image as a mask to show the attention from a token to all others (a row) or vise versa (a column), Fig.~\ref{fig:attention_pattern_vis}E.
The three images in Fig.~\ref{fig:teaser}-D2 (top-bottom) show the original image, image+source attention mask, and image+target attention mask. 
Hovering over individual source/target tokens from the top/bottom axis in Fig.~\ref{fig:teaser}-D3 will update the source/target attention masks dynamically (e.g., Fig.~\ref{fig:case_head10_cls}D,  Fig.~\ref{fig:case_head137}C).
