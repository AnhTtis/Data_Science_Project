\section{Case Study and Experts' Feedback}
\label{sec:case_study}

We use multiple case studies, conducted together with deep learning experts, to show the capability of our system. The experts' feedback is summarized at the end of Sec.~\ref{sec:feedback}.


For ViTs, we explored four pre-trained ViTs with different image resolutions ($w$), numbers of layers ($l$) and heads ($n$)~\cite{dosovitskiy2020image}. As our findings are consistent across them, we focus our illustrations on one model only but include results of the other three in our Appendix. The parameters of the focused model are: $w{=}224$, $l{=}12$, $n{=}12$, and $p{=}14$.

For datasets, we used 1000 images sampled from the validation set of ImageNet~\cite{russakovsky2015imagenet}. The images are from 20 classes (10 classes with the best and 10 classes with the worst predictions), each has 50 images. 
We have also explored our system with another dataset. However, as the model-level findings from the two datasets are mostly similar, we include the exploration of the other dataset in our Appendix.

\subsection{Head Importance}
\label{sec:case_head_importance}
Fig.~\ref{fig:teaser}A shows the 1000 images, laid out by their $l{\times}n$-dimensional head importance vector (each dimension is $\textbf{P}_{i,j}[idx_{label}]$ in Eq.~\ref{eq:prob}). From the layout, we found images are arranged clockwise with an increasing true-class probability. 

\begin{figure}[tbh]
    \centering
    \includegraphics[width=\columnwidth]{fig/importance_summary.pdf}
    \vspace{-0.2in}
    \caption{The mean (blue curve) and standard deviation (blue band) of the four head importance metrics computed over all images.}
    \label{fig:importance_summary}
\end{figure}

Fig.~\ref{fig:importance_summary} shows the four head importance metrics aggregated over all images. Both \textit{model-level} metrics (`Probability', `JSD') reflect head $4$ and $9$ from layer 0 are very important. The very wide band of the `Probability' plot is due to our choice of images with the best and worst performances.
The two \textit{layer-level} metrics (`\texttt{CLS} Distance', `Patch Distance') show heads 9 and 4 contribute significantly to the changes of \texttt{CLS} and patch representations, respectively.

Moreover, the \textit{layer-level} metrics show more oscillations, identifying important heads that cannot be identified from the \textit{model-level} metrics.
For example, removing layer 1 head 4 barely changes the final predictions but significantly affects the layer activations. As shown in the `Patch Distance' plot, the mean for this head is large and its standard deviation is small, 
indicating the head is important to the layer across images.
This head has a fixed function of making all patches attend to the patch above themselves (explained later in Fig.~\ref{fig:tsne_pattern}F). The \textit{model-level} metrics cannot identify it as heads from later layers show similar functionalities (i.e., head 3 from layer 2, explained later in Fig.~\ref{fig:tsne_pattern}J), hiding its importance. Also, an increasing standard deviation is observed from the two \textit{layer-level} metrics, indicating that higher-layer heads' importance is more influenced by image contents.

By coordinately exploring the \imageoverview{} and \headimportance{}, we find image clusters, to which, individual heads are very important (e.g., heads 9 and 4 from layer 0 are important to images in Fig.~\ref{fig:teaser}-A1, A2). To analyze their importance, we randomly select an image (ID: 712, label: \texttt{macaw}) from one cluster for further exploration. The three heads that are very important to the selected image are: \headfour{}, \headnine{}, and \headfive{} (Fig.~\ref{fig:teaser}-B1). 

Fig.~\ref{fig:teaser}-B2 shows the partial pruning results for \headfour{}. The probability drops only if the $patches{\to}patches$ attentions %(the bottom-right region) 
are pruned, and pruning \texttt{CLS}-related attentions has little impact. This attributes head 4's importance to its attentions between patches.  Fig.~\ref{fig:teaser}-B3 shows the top-five probabilities are very low when this head is pruned, and \texttt{macaw} (the true label) is not among them.

Fig.~\ref{fig:teaser}-B4 and B5 show the partial pruning results for \headnine{} and \headfive{}, respectively. From them, \headnine{} is important due to the \texttt{CLS} self-attention ($\texttt{CLS}{\to}\texttt{CLS}$); \headfive{} is important due to the attentions between \texttt{CLS} and patches ($\texttt{CLS}{\to}patches$ and $patches{\to}\texttt{CLS}$).

\subsection{Attention Strength}
\label{sec:case_strength}
Next, we examine why the heads are important through their attention strengths. The attention strength analysis is for patch tokens only (\texttt{CLS} has no spatial information), so our analysis focuses on \headfour{}.
%
Fig.~\ref{fig:teaser}-C1 shows the attention strength overview of all heads. \headfour{} (in the red circle) has the smallest entropy. Fig.~\ref{fig:teaser}-C2 shows the head's $p$D $k$-hop neighborhood vector. From it, all patches' attention strengths focus on the $0$-hop neighbor, i.e., the head makes all patches strongly attend to themselves. It is reasonable that such a head is important in lower layers, as no details should be overlooked at the beginning. From Fig.~\ref{fig:teaser}-C3, we also notice that this head's functionality is consistent across all images, as its entropies for different images are always low (distributed dominantly to the left).

Fig.~\ref{fig:teaser}-C4, C5 show the attention strengths of the other two important heads. Their importance majorly comes from \texttt{CLS}-related attentions, and the attention strengths (for patch tokens) are scattered across all hops of neighbors (the bar chart) and varying across images (the area plot).
\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.97\columnwidth]{fig/attention_strengths_system.pdf}
    \vspace{-0.1in}
    \caption{Lower-layer heads can distribute patches' attention strength only to their near neighbors (A, B), or evenly to all $k$-hop neighbors (C). Higher-layer heads can only do the latter (D, E).}
    \label{fig:attention_strength}
\end{figure}


The overview in Fig.~\ref{fig:teaser}-C1 and Fig.~\ref{fig:attention_strength} also reveals the attention strength distribution of heads over layers. 
In general, lower-layer heads can make patches attend strongly to their local regions, e.g., $0$-hop or $1$-hop neighbors (the low entropy heads in Fig.~\ref{fig:attention_strength}A-B). Low-layer heads can also make patches evenly distribute their attention across the entire image (e.g., Fig.~\ref{fig:attention_strength}C). For higher-layer heads, patches only attend globally with similar attention strengths across all $k$-hop neighbors, e.g., Fig.~\ref{fig:attention_strength}D-E. 
A similar overview can always be observed no matter which image is selected.
This observation is consistent with the original claims about attention strengths~\cite{dosovitskiy2020image}
(see details about~\cite{dosovitskiy2020image} in our Appendix).

\subsection{Attention Pattern}
\label{sec:case_pattern}

The attention patterns further answer why \headfour{} is important. From the \attnpattern{}, i.e., the vertical lines in Fig.~\ref{fig:teaser}-D3 and the diagonal pattern in Fig.~\ref{fig:teaser}-D4, all patches in this head strongly attend to themselves, echoing our earlier findings from the \attnstrength{}. 

\begin{figure}[tbh]
    \centering
    \includegraphics[width=\columnwidth]{fig/case_head5_patterns.pdf}
    \vspace{-0.3in}
    \caption{Exploring heads that have similar attention patterns with \headfour{} to explain why it is important while others are not.}
    \label{fig:case_head5_patterns}
\end{figure}

Are there heads with attention patterns similar to {\textit{head 4}}? If yes, why is only {\textit{head 4}} so important? To answer these questions, we zoom into the red dashed region of the tSNE layout in Fig.~\ref{fig:teaser}-D1. The zoomed-in details are shown in Fig.~\ref{fig:case_head5_patterns}A. From it, we explore heads close to {\textit{head 4}} and check their attention patterns. Fig.~\ref{fig:case_head5_patterns}B-C show two of them, where the vertical lines indicate the patches also majorly attend to themselves in these two heads. However, different from {\textit{head 4}}, the self-attentions in these two heads are not always strong, i.e., all lines in Fig.~\ref{fig:teaser}-D3 are in dark orange, but most lines in Fig.~\ref{fig:case_head5_patterns}B-C are in light orange.
%(the visibility thresholds in all three views are 1\%).
Moreover, the two heads' functionality is not as consistent as that of {\textit{head 4}} across images (comparing Fig.~\ref{fig:teaser}-C3 and Fig.~\ref{fig:case_head5_patterns}D-E).

\begin{figure}[tbh]
    \centering
    \includegraphics[width=\columnwidth]{fig/case_head10_cls.pdf}
    \vspace{-0.2in}
    \caption{The attention pattern of \headnine{}.}
    \label{fig:case_head10_cls}
\end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{fig/case_head137.pdf}
    \vspace{-0.25in}
    \caption{The $\texttt{CLS}{\to}patches$ attention in \headfive{}.}
    \label{fig:case_head137}
\end{figure}


\begin{figure*}[tbh]
    \centering
    \includegraphics[width=\textwidth]{fig/tsne_pattern.pdf}
    \vspace{-0.3in}
    \caption{(A/B) tSNE layouts of heads using their patch/\texttt{CLS} attention patterns. Content-agnostic heads from lower layers show similar patterns across images, and thus each forms an isolated cluster no matter they are laid out by the patch (C-J, M, N) or \texttt{CLS} (G-J, O-Q) patterns. Content-relevant heads from higher layers show dissimilar patterns for different images. These heads are scattered in both layouts (K, L, R).
    }
    \label{fig:tsne_pattern}
\end{figure*}

\begin{figure*}[tbh]
 \centering 
 \includegraphics[width=\textwidth]{fig/pattern.pdf}
  \vspace{-0.3in}
 \caption{The 13 possible attention patterns between image patches presented in the (top) image space, (middle) two-axes, and (bottom) heatmap visualization. Each pattern is a mix of one/multiple of the four basic patterns: \textit{diagonal} (A-K), \textit{horizontal} (J, K), \textit{vertical} (L), and \textit{block} (M). The first two are \textit{content-agnostic} and often appear in lower-layer heads; the latter two are \textit{content-relevant} and often occur in higher-layer heads. }
 \label{fig:pattern}
 \vspace{-0.1in}
\end{figure*}

For \headnine{}, we have known its importance comes from the $\texttt{CLS}{\to}\texttt{CLS}$ attention in Fig.~\ref{fig:teaser}-B4. Visualizing its attention patterns, we can see the dark blue vertical line in Fig.~\ref{fig:case_head10_cls}B and the dark blue cell in the top-left corner of Fig.~\ref{fig:case_head10_cls}C, echoing the importance of \texttt{CLS}'s self-attention. Meanwhile, the $\texttt{CLS}{\to}patches$ and $patches{\to}\texttt{CLS}$ attentions are not noticeable. The $patches{\to}patches$ attentions (the orange lines in Fig.~\ref{fig:case_head10_cls}B and the vertical pattern in Fig.~\ref{fig:case_head10_cls}C), as well as the masked source and target images (Fig.~\ref{fig:case_head10_cls}D), show no obvious extracted features, confirming the less importance of the patch-related attentions. Also, we noticed that the \texttt{CLS} attention pattern of {\textit{head 9}} is very unique, as it is the only head (of the selected image) in the isolated cluster of Fig.~\ref{fig:case_head10_cls}A (the tSNE layout of all heads' $A_\texttt{CLS}$). The background contour shows the head 9 from other images, reflecting that head 9's functionality is fixed across images. 

For \headfive{}, its importance is from the $patches$${\to}$ $\texttt{CLS}$ and
$\texttt{CLS}{\to}patches$ attentions (Fig.~\ref{fig:teaser}-B5). In Fig.~\ref{fig:case_head137}A, the $patches{\to}\texttt{CLS}$ attentions (the leftmost column) are not noticeable, but the $\texttt{CLS}{\to}patches$ attentions (the top row) show four major regions, indicating the \texttt{CLS} attends to four groups of patches. Hovering over the \texttt{CLS} token on the top axis of Fig.~\ref{fig:case_head137}B, the four groups of target patches are highlighted in the masked target image (Fig.~\ref{fig:case_head137}C). From it, the four regions accurately extract the \texttt{macaw}'s wings in blue.

\subsubsection{Attention Pattern Summary}
\label{sec:attn_pattern_summary}
Our explorations identified different attention patterns. This section provides an exhaustive summary.

\textbf{Patch Attention ($A_{Patch}$) Patterns.}
Fig.~\ref{fig:tsne_pattern}A shows the tSNE layout of all heads' patch attentions from the 1000 images (Fig.~\ref{fig:teaser}-D1 is the density plot of it). 
The points from an isolated cluster often represent the same head from different images, verifying the head's fixed functionality.
Exploring individual heads in the \attnpattern{}, we summarize them into 13 patterns in Fig.~\ref{fig:pattern}. The three rows of the illustrative figure show how each patch attends to others in the image space (top), the two-axes (middle), and the heatmap (bottom). From the last row, each of the 13 patterns is a combination of four basic patterns, i.e., \textit{\textbf{diagonal}}, \textit{\textbf{horizontal}}, \textit{\textbf{vertical}}, and \textit{\textbf{block}}.

Fig.~\ref{fig:pattern}A-K include the \textit{\textbf{diagonal} pattern}. Fig.~\ref{fig:pattern}A denotes self-attention. Fig.~\ref{fig:pattern}B-C show each patch attends to its left or right patch (one cell off the diagonal), where the gaps indicate the leftmost or rightmost patches without further ones to attend to. Each patch in Fig.~\ref{fig:pattern}D-E attends to the patch above or below itself, i.e., the white-squares shift from the diagonal for a row of patches.
%
The four heads in Fig.~\ref{fig:tsne_pattern}C-F (all from layer 1) show examples where each patch attends to its right, bottom, left, and top patch, respectively.
The four heads in Fig.~\ref{fig:tsne_pattern}G-J (all from layer 2) show similar patterns, revealing the repeating functionalities.
Coming to middle attention layers, patches can attend to multiple patches above and/or below themselves (Fig.~\ref{fig:pattern}F-H). Fig.~\ref{fig:tsne_pattern}M-N show two such examples from layer 4. Fig.~\ref{fig:pattern}I includes the counter-diagonal patterns, in which the left/right patches symmetrically attend to the right/left ones in the same row. 

Fig.~\ref{fig:pattern}J-K contain the \textit{\textbf{horizontal} pattern} (mixed with the diagonal pattern). The pattern indicates that a patch attends to multiple patches before/after itself in the same row. 

Fig.~\ref{fig:pattern}L shows the \textit{\textbf{vertical} pattern}, i.e.,  multiple source patches (heatmap rows) attend to the same target patches (heatmap columns). This usually indicates the target patches include important semantics to the class (e.g., the \texttt{cat} face region of a \texttt{cat} image, Fig.~\ref{fig:tsne_pattern}K-L). The pattern often occurs in higher-layer heads, as indicated by the yellow/brown color in Fig.~\ref{fig:tsne_pattern}A (the big chaotic cluster in the center).

Fig.~\ref{fig:pattern}M shows the \textit{\textbf{block} pattern}, i.e., patches in a local region mutually attend to each other (e.g., the face patches of a \texttt{cat} attend to its body patches and vice versa). 
Similar to segmentation, the attentions extract the object's pixels.

Attention patterns can also be categorized into \textit{\textbf{content-agnostic}} and \textit{\textbf{content-relevant}}. The \textit{diagonal} and \textit{horizontal} patterns are often agnostic to the image content, e.g., heads in Fig.~\ref{fig:tsne_pattern}C-J. The same head from all images forms an isolated cluster in Fig.~\ref{fig:tsne_pattern}A.
The \textit{vertical} and \textit{block} patterns are content-relevant, as their position depends on the content of images. They are the big chaotic cluster in Fig.~\ref{fig:tsne_pattern}A.
Content-agnostic patterns often occur in lower layers, whereas content-relevant patterns often occur in higher layers.


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.95\columnwidth]{fig/bottom.pdf}
    \vspace{-0.12in}
    \caption{Each patch attends to the one beneath it, e.g., patch 0 (row 0, column 0) attends to patch 14 (row 1, column 0,  $p{=}14$). The bottom row of patches all attend to \texttt{CLS} since there is no further patch.}
    \label{fig:bottom}
\end{figure}

\textbf{\texttt{CLS} Attention ($A_{\texttt{CLS}}$) Patterns.}
The \texttt{CLS}-related attention patterns follow a similar layer-wise trend. 
As shown in Fig.~\ref{fig:tsne_pattern}B (its density plot is in Fig.~\ref{fig:case_head10_cls}A), heads in lower layers form clear clusters, indicating the \texttt{CLS}'s attentions in them are more content-agnostic.
For example, Fig.~\ref{fig:tsne_pattern}P shows dominantly strong $\texttt{CLS}{\to}\texttt{CLS}$ attentions; Fig.~\ref{fig:tsne_pattern}Q shows strong $patches{\to}\texttt{CLS}$ attentions.
In higher layers, the attentions are more content-relevant, e.g., in Fig.~\ref{fig:tsne_pattern}R (the top row), the \texttt{CLS} focuses only on specific image patches.

The four heads with isolated clusters in Fig.~\ref{fig:tsne_pattern}A, G-J also form four isolated clusters in their \texttt{CLS} layout (Fig.~\ref{fig:tsne_pattern}B, G-J). By coordinately exploring the patch and \texttt{CLS} patterns, we found the boundary patches without further patch to attend in these heads will attend to \texttt{CLS}. For example, in Fig.~\ref{fig:tsne_pattern}H, all patches attend to the patch beneath themselves. The bottom row of patches have no patches beneath them, so they attend to \texttt{CLS} (Fig.~\ref{fig:bottom}A). The two-axes view of this head is shown in Fig.~\ref{fig:bottom}B, which is consistent with the pattern in Fig.~\ref{fig:tsne_pattern}H (and the inset). 
This explains how information is passed across patches row-by-row to \texttt{CLS} for classification.

\subsection{Head Attention Diagnosis}
Our coordinated system also helps to diagnose the roles of different heads (especially the important ones) in mispredictions. We brief two example cases in this section.

\textit{\textbf{Case 1:}} Fig.~\ref{fig:case_image355}A shows an image from the \texttt{overskirt} class, but the ViT performs badly on its prediction (Fig.~\ref{fig:case_image355}B). From the \headimportance{}, we notice a sharp increase in the true label probability when head 9 of layer 11 is pruned (Fig.~\ref{fig:case_image355}C). The partial pruning result of this head (Fig.~\ref{fig:case_image355}D) reflects that the probability of \texttt{overskirt} will \textit{increase} only if the attentions between \texttt{CLS} and patches are pruned.
As the $patches{\to}\texttt{CLS}$ attentions are very small in Fig.~\ref{fig:case_image355}E (the first column), we hover over the $\texttt{CLS}{\to}patches$ attentions in Fig.~\ref{fig:case_image355}F. The masked target image (Fig.~\ref{fig:case_image355}G) shows that the \texttt{CLS} attends strongly to the background fences. Pruning such a misleading head will help the model focus more on the right features, leading to a probability increase. The importance of this head also explains why the top two classes in the original predictions (Fig.~\ref{fig:case_image355}B) are \texttt{fences}.
\begin{figure}[tbh]
    \centering
    \includegraphics[width=\columnwidth]{fig/case_image355.pdf}
    \vspace{-0.3in}
    \caption{The head 9 from layer 11 incorrectly attends to the background fence. Pruning it will increase the true class's probability.}
    \label{fig:case_image355}
\end{figure}

\textit{\textbf{Case 2:}} The ViT correctly predicts the image in Fig.~\ref{fig:case_image483}A to be \texttt{sunglass}, but also assigns a high probability to \texttt{jersey}. 
We found two important heads in the last layer but with opposite effects (Fig.~\ref{fig:case_image483}B). Pruning heads 9 and 10 from layer 11 separately leads to a big decrease and increase of the \texttt{sunglass} probability.
From the partial pruning results, we found the attentions between \texttt{CLS} and patches dominate the decrease (Fig.~\ref{fig:case_image483}C) or increase (Fig.~\ref{fig:case_image483}D). We then visualize the $\texttt{CLS}{\to}patches$ attentions and the corresponding masked images. 
For head 9, the \texttt{CLS} focuses on the face region, whereas for head 10, the \texttt{CLS} attends solely to the jersey region. Heads 9 and 10 contribute largely to the probability of \texttt{sunglass} and \texttt{jersey} respectively. Pruning them increases the opposite class's probability (Fig.~\ref{fig:case_image483}C, D). These details significantly deepens the understanding of how ViT works.
\begin{figure}[tbh]
    \centering
    \includegraphics[width=\columnwidth]{fig/case_image483.pdf}
    \vspace{-0.3in}
    \caption{Heads 9 and 10 (layer 11) extract the \texttt{sunglass} and \texttt{jersey} features respectively, impacting the corresponding classes' probability.}
    \label{fig:case_image483}
\end{figure}

\vspace{-0.15in}

\subsection{Domain Experts' Feedback}
\label{sec:feedback}
The above case studies were conducted with 7 deep learning experts in separate sessions, using the protocol of \textit{guided exploration+think-aloud discussions}. All experts are researchers with 5+ years of experience in deep learning. Five experts ($E_1{\sim}E_5$) have participated in our requirement analysis. The other two ($E_6{\sim}E_7$) had no knowledge about our visualization system until the case study sessions.


In general, all experts confirmed the importance of the three focused topics and appreciated our findings. $E_2$, $E_4$, and $E_5$ enjoyed the system's interactivity, especially the linked visualizations, which helped them connect the dots for comprehensive interpretations of important heads. Their existing visualization tools with piece-by-piece analysis fall short of such coordinated explorations.
Using our system, $E_1$ and $E_3$ obtained an overview of ViTs' head attention patterns for the first time. Both experts found our findings intriguing and had thorough discussions on the analogy between CNNs and ViTs.
For example, patches in Fig.~\ref{fig:tsne_pattern}G always attend to their right patch. The $3{\times}3$ CNN filter \inlinegraphics{fig/filter.pdf} also aggregates the right pixel's value to the current pixel. So, the ViT head and CNN filter share equivalent functions. 
Similar equivalence analysis can also be extended to other heads/filters.
$E_7$ pointed out similar learning trends across layers of CNNs and ViTs, i.e., lower-layer heads/filters focus on local features, whereas higher layers aggregate the output of lower layers to extract object-level information. 
$E_7$ also found the interactions between the \texttt{CLS} and patches very insightful. In Fig.~\ref{fig:bottom}A, all patches attend to the patch below them and the last row attends to the \texttt{CLS}. This is similar to propagating the information top-down, and the final accumulated results are passed to \texttt{CLS} for classification.
$E_6$ was interested in the content-agnostic/relevant attentions and believed they could be adopted for anomaly detection.


The experts also pointed out some insufficiency of the current system. First, $E_7$ initially thought only the three heads in Fig.~\ref{fig:teaser}-B1 were important to the prediction of image 712, which was misleading as we only pruned one head at a time and did not consider the dependency between heads. Second, both $E_1$ and $E_7$ worked on token pruning of transformers (instead of head pruning). They liked our image masks in disclosing the tokens' semantics but also wanted to see similar saliency maps highlighting the importance of individual patches through ablations. These comments provide promising future directions for us to explore.  

