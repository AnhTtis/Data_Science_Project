@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{dong2020interactive,
  title={Interactive Attention Model Explorer for Natural Language Processing Tasks with Unbalanced Data Sizes},
  author={Dong, Zhihang and Wu, Tongshuang and Song, Sicheng and Zhang, Mingrui},
  booktitle={2020 IEEE Pacific Visualization Symposium},
  pages={46--50},
  year={2020},
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{wang2018dqnviz,
  title={Dqnviz: A visual analytics approach to understand deep q-networks},
  author={Wang, Junpeng and Gou, Liang and Shen, Han-Wei and Yang, Hao},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={25},
  number={1},
  pages={288--298},
  year={2018},
  publisher={IEEE}
}

@article{choo2018visual,
  title={Visual analytics for explainable deep learning},
  author={Choo, Jaegul and Liu, Shixia},
  journal={IEEE Comput. Graph. Appl.},
  volume={38},
  number={4},
  pages={84--92},
  year={2018},
  publisher={IEEE}
}

@article{hohman2018visual,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={25},
  number={8},
  pages={2674--2693},
  year={2018},
  publisher={IEEE}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1",
    year = "2019",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 volume = {33},
 year = {2020}
}

@article{liu2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  year      = {2019},
}

@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1748--1764},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{li2019enhancing,
 author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting},
 volume = {32},
 year = {2019}
}

@article{michel2019aresixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM Computing Surveys},
  year={2021},
  publisher={ACM New York, NY}
}

@inproceedings{hao2021self,
  title={Self-attention attribution: Interpreting information interactions inside transformer},
  author={Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12963--12971},
  year={2021}
}

@inproceedings{chen2022denoising,
  title={Denoising Self-Attentive Sequential Recommendation},
  author={Chen, Huiyuan and Lin, Yusan and Pan, Menghai and Wang, Lan and Yeh, Chin-Chia Michael and Li, Xiaoting and Zheng, Yan and Wang, Fei and Yang, Hao},
  booktitle={Proceedings of the 16th ACM Conference on Recommender Systems},
  pages={92--101},
  year={2022}
}

@inproceedings{kovaleva2019revealing,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga  and
      Romanov, Alexey  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "EMNLP-IJCNLP",
    year = "2019",
    pages = "4365--4374",
    abstract = "BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT{'}s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.",
}

@inproceedings{abnar2020quantifying,
    title = "Quantifying Attention Flow in Transformers",
    author = "Abnar, Samira  and
      Zuidema, Willem",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    pages = "4190--4197",
    abstract = "In the Transformer model, {``}self-attention{''} combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
}

@inproceedings{voita2019analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    pages = "5797--5808",
    abstract = "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",
}

@inproceedings{dosovitskiy2020image,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  booktitle = {9th International Conference on Learning Representations (ICLR)},
  year      = {2021},
}

@inproceedings{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  booktitle={NeurIPS},
  volume={34},
  pages={12116--12128},
  year={2021}
}

@article{hohman2019s,
  title={Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations},
  author={Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng Polo},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={26},
  number={1},
  pages={1096--1106},
  year={2019},
  publisher={IEEE}
}

@article{wang2018ganviz,
  title={Ganviz: A visual analytics approach to understand the adversarial game},
  author={Wang, Junpeng and Gou, Liang and Yang, Hao and Shen, Han-Wei},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={24},
  number={6},
  pages={1905--1917},
  year={2018},
  publisher={IEEE}
}


@inproceedings{park2019sanvis,
  title={Sanvis: Visual analytics for understanding self-attention networks},
  author={Park, Cheonbok and Na, Inyoup and Jo, Yongjang and Shin, Sungbok and Yoo, Jaehyo and Kwon, Bum Chul and Zhao, Jian and Noh, Hyungjong and Lee, Yeonsoo and Choo, Jaegul},
  booktitle={2019 IEEE Visualization Conference (VIS)},
  pages={146--150},
  year={2019},
  organization={IEEE}
}


@article{derose2020attention,
  title={Attention flows: Analyzing and comparing attention mechanisms in language models},
  author={DeRose, Joseph F and Wang, Jiayao and Berger, Matthew},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={27},
  number={2},
  pages={1160--1170},
  year={2020},
  publisher={IEEE}
}

@inproceedings{vig2019multiscale,
  title={A Multiscale Visualization of Attention in the Transformer Model},
  author={Vig, Jesse},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={37--42},
  year={2019}
}

@article{olah2016attention,
  title={Attention and augmented recurrent neural networks},
  author={Olah, Chris and Carter, Shan},
  journal={Distill},
  volume={1},
  number={9},
  pages={e1},
  year={2016}
}

@article{jaunet2021visqa,
  title={VisQA: X-raying Vision and Language Reasoning in Transformers},
  author={Jaunet, Th{\'e}o and Kervadec, Corentin and Vuillemot, Romain and Antipov, Grigory and Baccouche, Moez and Wolf, Christian},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={28},
  number={1},
  pages={976--986},
  year={2021},
  publisher={IEEE}
}

@inproceedings{cao2020behind,
  title={Behind the scene: Revealing the secrets of pre-trained vision-and-language models},
  author={Cao, Jize and Gan, Zhe and Cheng, Yu and Yu, Licheng and Chen, Yen-Chun and Liu, Jingjing},
  booktitle={European Conference on Computer Vision},
  pages={565--580},
  year={2020},
}

@inproceedings{li2020does,
  title={What does bert with vision look at?},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5265--5275},
  year={2020}
}

@article{strobelt2018s,
  title={Seq2seq-vis: A visual debugging tool for sequence-to-sequence models},
  author={Strobelt, Hendrik and Gehrmann, Sebastian and Behrisch, Michael and Perer, Adam and Pfister, Hanspeter and Rush, Alexander M},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={25},
  number={1},
  pages={353--363},
  year={2018},
  publisher={IEEE}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{russakovsky2015imagenet,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
volume={115},
number={3},
pages={211-252}
}


@article{bilal2017convolutional,
  title={Do convolutional neural networks learn class hierarchy?},
  author={Bilal, Alsallakh and Jourabloo, Amin and Ye, Mao and Liu, Xiaoming and Ren, Liu},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={24},
  number={1},
  pages={152--162},
  year={2017},
  publisher={IEEE}
}

@article{liu2016towards,
  title={Towards better analysis of deep convolutional neural networks},
  author={Liu, Mengchen and Shi, Jiaxin and Li, Zhen and Li, Chongxuan and Zhu, Jun and Liu, Shixia},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={23},
  number={1},
  pages={91--100},
  year={2016},
  publisher={IEEE}
}

@article{ramprasaath2016gradcam,
  author    = {Ramprasaath R. Selvaraju and
               Abhishek Das and
               Ramakrishna Vedantam and
               Michael Cogswell and
               Devi Parikh and
               Dhruv Batra},
  title     = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks
               via Gradient-based Localization},
  journal   = {CoRR},
  year      = {2016},
  eprinttype = {arXiv},
}
@ARTICLE{kwon2019retainvis,
  author={Kwon, Bum Chul and Choi, Min-Je and Kim, Joanne Taery and Choi, Edward and Kim, Young Bin and Kwon, Soonwook and Sun, Jimeng and Choo, Jaegul},
  journal={IEEE Trans. Vis. Comput. Graphics}, 
  title={RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records}, 
  year={2019},
  volume={25},
  number={1},
  pages={299-309},
  }
  
@inproceedings{ming2017understanding,
  title={Understanding hidden memories of recurrent neural networks},
  author={Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
  booktitle={2017 IEEE VAST},
  pages={13--24},
  year={2017},
}

@article{strobelt2017lstmvis,
  title={Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks},
  author={Strobelt, Hendrik and Gehrmann, Sebastian and Pfister, Hanspeter and Rush, Alexander M},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={24},
  number={1},
  pages={667--676},
  year={2017},
  publisher={IEEE}
}

@article{wang2019deepvid,
  title={Deepvid: Deep visual interpretation and diagnosis for image classifiers via knowledge distillation},
  author={Wang, Junpeng and Gou, Liang and Zhang, Wei and Yang, Hao and Shen, Han-Wei},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={25},
  number={6},
  pages={2168--2180},
  year={2019},
  publisher={IEEE}
}


@article{jin2022gnnlens,
  title={Gnnlens: A visual analytics approach for prediction error diagnosis of graph neural networks},
  author={Jin, Zhihua and Wang, Yong and Wang, Qianwen and Ming, Yao and Ma, Tengfei and Qu, Huamin},
  journal={IEEE Trans. Vis. Comput. Graphics},
  year={2022},
  publisher={IEEE}
}

@article{liu2022visualizing,
  title={Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding},
  author={Liu, Zipeng and Wang, Yang and Bernard, J{\"u}rgen and Munzner, Tamara},
  journal={IEEE Trans. Vis. Comput. Graphics},
  volume={28},
  number={6},
  pages={2500--2516},
  year={2022},
  publisher={IEEE}
}

@article{ying2019gnnexplainer,
  title={Gnnexplainer: Generating explanations for graph neural networks},
  author={Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{jin2022visual,
  title={A visual analytics system for improving attention-based traffic forecasting models},
  author={Jin, Seungmin and Lee, Hyunwook and Park, Cheonbok and Chu, Hyeshin and Tae, Yunwon and Choo, Jaegul and Ko, Sungahn},
  journal={IEEE Trans. Vis. Comput. Graphics},
  year={2022},
  publisher={IEEE}
}

@inproceedings{
bau2018identifying,
title={Identifying and Controlling Important Neurons in Neural Machine Translation},
author={Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass},
booktitle={International Conference on Learning Representations},
year={2019}
}

@inproceedings{li2021t3,
    title = "T3-Vis: visual analytic for Training and fine-Tuning Transformers in {NLP}",
    author = "Li, Raymond  and
      Xiao, Wen  and
      Wang, Lanjun  and
      Jang, Hyeju  and
      Carenini, Giuseppe",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2021",
    pages = "220--230",
}

@inproceedings{aflalo2022vl,
  title={VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers},
  author={Aflalo, Estelle and Du, Meng and Tseng, Shao-Yen and Liu, Yongfei and Wu, Chenfei and Duan, Nan and Lal, Vasudev},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21406--21415},
  year={2022}
}

