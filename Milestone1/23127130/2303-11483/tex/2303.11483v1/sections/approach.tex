% !TEX root = ../main.tex
\section{Approach}
\label{sec:approach}

\begin{table*}[]
 \caption{Evaluation of different sketch to image methods meeting content distance from the conceptual sketch (lower is better), similarity to real images (FID, lower is better), and structural diversity of designs (higher is better). }
     \centering
     \begin{tabular}{c|c|c|c}
         \textbf{Method} & \textbf{Content Distance $\downarrow$} & \textbf{FID $\downarrow$} & \textbf{Structural Diversity $\uparrow$} \\
         \hline 
          MUNIT &  $0.43 \pm 0.15 $ & $188.05$ & $ 0.37 \pm 0.22 $ \\
          Pix2pix & $ 0.54 \pm 0.08$ & $294.23$ & $0.0 \pm 0.0$ \\
          ControlNet & $ 0.54 \pm 0.12$ & $190.48$ & $ 0.80 \pm 0.36$ \\
          Conceptual Sketches &  $\mathbf{0.07 \pm 0.02}$ & $357.58$ & $ 0.0 \pm 0.0$\\
          \hline
     \end{tabular}
     
     \label{tab:experiment}
 \end{table*}

In our work, we have identified several evaluation automated measures that can act as proxies for the generated renders desiderata:
\begin{itemize}
    \item \textbf{Content distance}: L1 distance between the MUNIT \cite{munit_ref} content encoder of the sketch and the rendered designs;
    \item \textbf{Frechet Inception Distance} (FID) \cite{FID_ref} to measure the realism of the candidate designs;
    \item \textbf{Structural diversity}, computed using Structural Similarity Index Measure (SSIM) \cite{SSIM_ref}.
\end{itemize}

\noindent\textbf{Content distance} is our proxy for determining if the overall intent of a generated design matches those of a sketch.
To measure this, we use Multimodal Unsupervised Image-to-Image Translation (MUNIT) which is an autoencoder model that uses cyclic consistency to generate neural encoders for the style and content of an image \cite{neural_style_ref}.  Style refers to the distribution of pixels within small patches of an image.  Content refers to the broader structure of an image, and is invariant across application of multiple different styles.  For example, an image of a building painted blue will have a different style compared to the same building being painted red, but have similar contents.  This also extends to lower level structural details that may be captured by the style coding.  
MUNIT employs a cyclic loss \cite{cyclegan_ref} to translate between the content of an image and the variety of different renderings that can stem from it.  
In our case, we use MUNIT to learn how to identify the content from a sketch and how it applies to different building images.
% 

 \begin{figure}[h]
\begin{center}
\includegraphics[width=1.\columnwidth]{figures/munit_content.pdf}
\end{center}
\caption{Capturing content versus style: The content, broad structural information, of the original sketch is combined with multiple styles encoding different low-level pixel information to produce several visually different results.}
\label{fig:munit_content}
\end{figure}

Figure \ref{fig:munit_content} illustrates how content is preserved across multiple different renders.  Here a trained MUNIT network creates three visually distinct renders from a single source sketch. 

\noindent\textbf{Frechet Inception Distance (FID)} is the predominant measure used to measure the fidelity of generated images with a corpus of in-domain real images.  This is done by computing from the feature activation statistics of an Inception network \cite{inception_ref} over the corpus of real images and synthetic images.  The Frechet divergence (the squared Wasserstein distance) between the distribution of visual features between real and synthetic images is used to determine how unrealistic a given generative method is.

%%

\noindent\textbf{Structural diversity} is taken as the inverse of the Structural Similarity Index Measure (SSIM) \cite{SSIM_ref}. SSIM specifically aims at capturing structural relationships, where structure is represented by spatially close pixels that are also strongly correlated.  For each set of designs generated from a conceptual sketch, we take the mean of the pairwise SSIMs for all renders from a given conceptual sketch. 

Inception score \cite{inception_ref} is another popular measure used to assess the quality and diversity of generated images.  However, this score is computed from feature activation statistics in an Inception network, and features corresponding to palette and low-level texture changes may be captured and treated as indicating diversity.  The stronger assumptions about structure in SSIM thus are a better match.
%
%%
%
We explore the use of several image-to-image models for our design goals.
These models are: 1) Pix2pix \cite{pix2pix2017}, 2) MUNIT \cite{munit_ref}, and 3) ControlNet \cite{controlnet_ref}.
% \begin{itemize}
%     \item Pix2pix \cite{pix2pix2017}
%     \item MUNIT \cite{munit_ref}
%     \item ControlNet \cite{controlnet_ref}
% \end{itemize}

Pix2pix is a framework that uses conditional generative adversarial networks to learn transformations from one domain of images to another.  
%% 
Both Pix2pix and MUNIT were trained using a dataset of $80,000$ architectural images drawn from Flickr\footnote{https://www.flickr.com/} and Unsplash\footnote{https://unsplash.com/}.  
%%
ControlNet is a state-of-the art method that uses sketches as guidance cues for controlling the inference in a pre-trained diffusion model.  By forcing the generated images match the constraints provided by the sketches, ControlNet can leverage existing highly-trained diffusion models without having to fine-tune or retrain the model.  For these experiments, we used the scribble maps guidance with the prompt "a photograph of a building."
%% 
A major challenge is the lack of a large scale corpus of architectural conceptual sketches.  
Previous studies on sketch-to-image generation have synthesized sketch-like images from images using heuristic methods such as Canny Edge Detection \cite{canny_edge_ref} or the Hough transform \cite{hough_ref}.  Neural methods have also been trained to generate edges based on specific criteria.  Commonly used examples of this class include Holistically-Nested Edge Detection (HED) \cite{hed_ref} and DexiNed \cite{dexined_ref}.  For our training models, we used edges detected via DexiNed to generate corresponding sketch images for our data, as these most visually resemble hand-drawn sketches.  





%  \begin{figure}[]
% \begin{center}
% \includegraphics[width=\columnwidth]{figures/pix2pix_averaging.pdf}
% \end{center}
% \caption{Averaging effects can be observed from a sample of renders produced by Pix2pix.}
% \label{fig:pix2pix_averaging}
% \end{figure}