
\subsection{Hyperparameters of compared methods}
\paragraph{Face image restoration.}
For ILO, we found that optimizing the first 5 layers of the generative network offered the best trade-off between image quality and consistency with the observation.
Hence, we optimize the 5 first layers for 100 iterations each. 
This choice is different from the official implementation, where they only optimize the 4 first layers for a lower number of iterations, trading restoration performance for speed.
For DPS, we set the scale hyper-parameter $\zeta'$ (described in subsection C.2 in\cite{chung2022diffusion})  to $\zeta'=1$ for the deblurring and super-resolution experiments reported in this paper.


\paragraph{Natural images restoration - Deblurring.}
For the three tested methods, we use the official implementation provided by the authors, along with the pretrained models.
For EPLL, we use the default parameters in the official implementation.

For GS-PnP, using the notation of the paper, we use the suggested hyperparameter $\lambda_\nu=0.1$ for the motion blur kernels %
and $\lambda_\nu=0.75$ for the Gaussian kernels.

For PnP-MMO, we use the denoiser trained on $\sigma_{den}=0.007$. 
On deblurring with $\sigma=2.55$ we use the default parameters in the implementation. 
for higher noise levels ($\sigma=7.65$; $\sigma=12.75$), and we  set the strength of the gradient step as $\gamma=\sigma_{den}/(2\sigma||h||)$, where $h$ corresponds to the blur kernel.

\paragraph{Natural images restoration- Inpainting.}
For EPLL, we use the default parameters provided in the authors matlab code.
For GS-PnP, after a grid-search, we chose to set $\lambda_\nu=1$ and $\sigma_{denoiser}=10$.