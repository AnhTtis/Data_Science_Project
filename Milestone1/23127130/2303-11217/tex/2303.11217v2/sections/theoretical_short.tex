We now analyse the convergence of Algorithm~\ref{algo:final}. Following the work of~\cite{attouch2010proximal},  the alternate optimization scheme converges if  $\qp{\z|x}=\pt{\z|\x}$ and the greedy optimization scheme in Algorithm~\ref{algo:hierarchical_latent_reg} actually solves $\min_{\z} %
J_2(\x, \z)$. %
In practice, it is difficult to verify if these hypotheses hold. 
We propose to theoretically study algorithm~\ref{algo:final}, and next verify empirically that the assumptions are met.

In section~\ref{ssec:pnp}, we reformulate Algorithm~\ref{algo:final} as a Plug-and-Play algorithm, where the HVAE reconstruction takes the role of the denoiser. 
Then we study in section~\ref{ssec:fp_conv} the fixed-point convergence of the algorithm. %
Finally, section~\ref{ssec:num_conv} contains numerical experiments with the patch HVAE architecture proposed in section~\ref{ssec:patchVDVAE}. We empirically show that the patch architecture satisfies the aforementioned technical  assumptions and then illustrate the numerical  convergence and the stability of our alternate algorithm. 


\subsection{Plug-and-Play HVAE}\label{ssec:pnp}
In this section we  make the assumption that the HVAE decoder is Gaussian with a constant variance on its diagonal~\eqref{eq:decoder_constant}.
We rely on the proximal operator of a convex function~$f$ that is defined as $\prox_f(\x) = \arg\min_{\bm{u}} f(\bm{u}) + \frac{1}{2}||\x - \bm{u}||^2$.

\begin{proposition}
    \label{prop:pnp_vae}
    Assume the decoder is defined as in~\eqref{eq:decoder_constant}.
    Denote $\HVAE(\x, \tauvec) := \mut{\enc{\x}}$. Then the alternate scheme described in Algorithm~\ref{algo:final} writes
    \begin{equation}
        \label{eq:pnp-vae}
        \x_{k+1} = \prox_{\gamma^2f}\left(\HVAE\left(\x_k, \tauvec\right)\right).
    \end{equation}
\end{proposition}
From relation~\eqref{eq:pnp-vae}, algorithm~\ref{algo:final} is a Plug-and-Play Half-Quadratic Splitting method~\cite{ryu2019plug} where the role of the denoiser is played by the reconstruction $\HVAE\left(\x_k, \tauvec\right)$.
We now derive from relation~\eqref{eq:pnp-vae} sufficient conditions to establish the convergence of the iterations. %
\subsection{Fixed-point convergence}\label{ssec:fp_conv}
Let us denote $\T$ the operator corresponding to one iteration of \eqref{eq:pnp-vae}:
   $ \T(\x) = \prox_{\gamma^2}f\left(\HVAE\left(\x, \tauvec\right)\right)$.
The Lipschitz constant of $\T$ can then be expressed as a function of $f$ and the HVAE reconstruction operator $\HVAE\left(\x_k, \tauvec\right)$.
\begin{proposition}[Proof in supplementary]
    \label{eq:prop_lipschitz}
   Assume that the decoder has a constant variance $\Vti{\z} = \frac{1}{\gamma^2}\id$ for all $\z$; and  the autoencoder with latent regularization is $L_{\tauvec}$-Lipschitz, {\em i.e.} $\forall \bf{u}$, $\bf{v} \in \R^n$: $
            || \HVAE\left(\bf{u}, \tauvec\right) - \HVAE\left(\bf{v}, \tauvec\right) || \leq L_{\tauvec} ||\bf{u} -\bf{v}||$.
   Then, denoting as $\lambda_{\min}$ the smallest eigenvalue of $A^tA$, we have
    \begin{equation}\label{eq:lips_const}
        ||\T({\bf u}) - \T({\bf v})|| \leq \frac{\sigma^2}{\gamma^2\lambda_{\min} + \sigma^2}L_{\tau}||{\bf u} -{\bf v}||.
    \end{equation}
\end{proposition}


\begin{corollary}\label{cor:convergence}
If $\HVAE\left(\x_k, \tauvec\right)$  is $L_{\tauvec}<1$-Lipschitz, then iterations~\eqref{eq:pnp-vae} converge.
\end{corollary}


\begin{proof}
If  $L_{\tauvec} < 1$, then $\HVAE\left(\x_k, \tauvec\right)$ is a contraction. Hence $T$ is also a contraction form proposition~\ref{eq:prop_lipschitz} and consequently, Banach theorem ensures the convergence of the iteration $\x_{k+1} = T(\x_k)$  to a fixed point of $T$.
 \end{proof}
 

\begin{proposition}[Proof in supplementary]
    \label{prop:fixed_point}
    $\x^{\star}$ is a fixed point of $\T$ if and only if:
    \begin{equation}
        \label{eq:fixed_point}
        \nabla f(\x^{\star}) =  \frac{1}{\gamma^2}\left(\HVAE\left(\x^{\star}, \tauvec\right)-\x^{\star}\right)
    \end{equation}
\end{proposition}



Proposition~\ref{prop:fixed_point} characterizes the solution of the latent-regularization scheme, in the case where the HVAE reconstruction is a contraction.
Under mild assumptions, the fixed point condition can be stated as a critical point condition
$$\nabla f(\x^*)+\nabla g(\x^*)=0,$$
of the objective function $f(\x) + g(\x) = - \log p(\y|\x) - \log \ptt{\x}$, where  the tempered prior is  the marginal
$ \ptt{\x} := \int \ptt{\x,\z} d\z $
of the joint tempered prior defined in~\eqref{eq:hvae_joint_model_temp}.
This result, detailed in the supplementary, follows from an interpretation of $\HVAE(\x,\tauvec)$ as an MMSE denoiser. As a consequence Tweedie's formula provides the link between the right-hand side of equation~\eqref{eq:fixed_point} and $\nabla g$.



\subsection{Numerical convergence with PatchVDVAE }\label{ssec:num_conv}
We illustrate the numerical convergence of Algorithm~\ref{algo:final}. We first analyse the Lipschitz constant of the HVAE reconstruction with the PatchVDVAE architecture proposed  in section~\ref{ssec:patchVDVAE}. Then we study the empirical convergence of the algorithm and show that it outperforms the baseline optimisation of the joint MAP~\eqref{eq:J1} with the Adam optimizer.

\noindent
{\bf   Lipschitz constant of the HVAE reconstruction.}
Corollary~\ref{cor:convergence} establishes the fixed point convergence of our proposed optimization algorithm under the hypothesis that the reconstruction with latent regularization is a contraction, \emph{i.e.} $L_{\tauvec} < 1$. 
We now show thanks to an empirical estimation of the Lipschitz constant $L_{\tauvec}$ that our PatchVDVAE network empirically satisfies such a property when applied to noisy images.  %
We present  in figure~\ref{fig:hist_lipschitz} the histograms of the ratios $r={||\HVAE(\bf{u}, \tauvec)-\HVAE(\bf{v}, \tauvec)||}/{||\bf{u}-\bf{v}||}$, where $\bf{u}$ and $\bf{v}$ are natural images extracted from the BSD dataset and corrupted with  white Gaussian noise. These ratios give a lower bound for the true Lipschitz constant $L_{\tauvec}$.
Although it is possible to set different temperature $\tau_l$ at each level, we fixed a constant temperature amongst all levels to limit the number of hyperparameters.
We realized tests for 3 temperatures $\tau \in \{0.6, 0.8, 0.99\}$, and 3 noise levels $\sigma \in \{0, 25, 50\}$.
On clean images ($\sigma=0$), the distribution of ratios in close to $1$. This suggests that the HVAE is well trained and accurately  models clean images. In some rare case, a ratio $r\geq 1$ is observed for clean images. This indicates that the reconstruction is not a contraction everywhere, in particular on the manifold of clean images. 

On noisy images $\sigma>0$, the reconstruction behaves as a contraction, as the ratio $r<1$ is always  observed. Moreover, reducing the temperature of the latent regularization $\tau$ increases the strength of the contraction. This suggests that with the trained PatchVDVAE architecture, the hypothesis $L_{\tauvec}<1$ in  Corollary~\ref{cor:convergence} holds for noisy images.

\begin{figure}
    \includegraphics[width=\columnwidth,trim={0 0cm 0 .4cm},clip]{data/hist.pdf}\vspace{-0.2cm}
    \caption{\label{fig:hist_lipschitz}Numerical estimation of the Lipschitz constant of PatchVDVAE reconstruction  with different temperatures $\tau$. We present the histogram of ratio values $\frac{||\HVAE(\bf{u}, \tau)-\HVAE(\bf{v}, \tau)||}{||\bf{u}-\bf{v}||}$, where $\bf{u}$ and $\bf{v}$ are  natural images corrupted with white Gaussian noise of different standard deviations $\sigma$. For noisy images ($\sigma>0)$, the observed Lipschitz constant is always less than $1$.\vspace{-0.2cm}}
\end{figure}

\noindent
{\bf   Convergence of Algorithm~\ref{algo:final}}
We now illustrate the effectiveness of PnP-HVAE through comparisons with the optimization of the objective $J_1(\x_k,\z_k)$ in~\eqref{eq:J1} using  the Adam algorithm~\cite{kingma2014adam} for two  learning rates $lr \in \{0.01, 0.001\}$.  The left plot in figure~\ref{fig:conv} shows that Adam is able to estimate a better minimum of $J_1$. However, our alternate algorithm requires a smaller number of iterations to converge. %

On the other hand, as illustrated by the right plot in figure~\ref{fig:conv}, the use of Adam involves numerical instabilities. Oscillations of the ratio $ L_k :=\frac{||\T\left(\x_{k+1}\right) - \T\left(\x_{k}\right)||}{||\x_{k+1} - \x_{k}||} %
$ are even increased  with larger learning rates, whereas our method provides a stable  sequence of iterates.

More important, we finally exhibit the better quality of the restorations obtained with our alternate algorithm  %
on inpaiting, deblurring and super-resolution of face images. In these experiments, we used the hierarchical VDVAE model~\cite{child2021very}  trained on the FFHQ dataset \cite{karras2019style}.
Figure~\ref{fig:face_restoration} (see $2$nd and $4$th columns) and table~\ref{table:comp_ILO} (PSNR, SSIM and LPIPS scores) illustrate that the quality of the images restored with our alternate optimization algorithm is higher than the ones obtained with Adam. This suggests that for image restoration purposes, our optimization method is able to find a more relevant local minimum of $J_1$ than Adam.



\begin{figure}[t]
    \label{fig:convergence}
    \centering
    \begin{tabular}{cc}
    \hspace{-.2cm}\includegraphics[height=.33\linewidth]{{data/adam_vs_jpmap/butterfly_T_0.70_J1_2}.pdf}&\hspace{-.2cm}\includegraphics[height=.33\linewidth]{{data/adam_vs_jpmap/butterfly_T_0.70_Lk_2}.pdf}\vspace{-0.4cm}
    \end{tabular}
    \caption{\label{fig:conv} Comparison of the convergence of PnP-HVAE algorithm~\ref{algo:final} with respect to the baseline Adam optimizer, on a deblurring problem.
     Left (Convergence of the function value): PnP-HVAE converges faster to a minimum of the joint posterior  $J_1(\x_k,\z_k)$  in~\eqref{eq:J1}. 
   Right (Convergence of iterates $\x_k$): PnP-HVAE is more stable than Adam.\vspace{-0.1cm}}
\end{figure}


\begin{figure}[ht]
    \center
    \includegraphics[width=0.95\columnwidth]{figures/demo_face_restoration_wdps.pdf}\vspace{-0.1cm}
    \caption{\label{fig:face_restoration}
        Visual comparaison of image restoration methods based on deep generative models. We studied $3$  tasks on face images:  inpainting (top), deblurring (middle), super-resolution (bottom). 
        Contrary to the optimization of the objective~\eqref{eq:J1} with Adam, our alternate algorithm generates realistic results, on par with ILO~\cite{daras2021intermediate}, while remaining consistant with the observation.\vspace{-0.3cm}
    }
\end{figure}



\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1}}
\begin{table}[ht]\small
    \centering
        \begin{tabular}{x{1.15cm}x{1.5cm}x{.65cm}x{.65cm}x{.65cm}x{1cm}}
           & &PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & time (s) \\
        \hline    
        \multirow{2}{*}{SR $\times 4$}&Adam & $28.56$ & $0.75$ & $0.38$ & $\underline{26}$  \\
       \multirow{2}{*}{$\sigma=3$}& ILO & $\underline{28.80}$ & $\underline{0.78}$ & $\mathbf{0.17}$ & $34$\\
       & PnP-HVAE & $\mathbf{29.32}$ & $\mathbf{0.82}$ & $0.28$ & $\mathbf{15}$\\ 
       & DPS & $27.53$ & $0.76$ & $\underline{0.21}$ & $153$\\
        \hline 
        Deblurring &Adam & $26.69$ & $0.75$ & $0.27$ &$ \underline{12}$  \\
        (motion) &ILO & $\underline{29.01}$ & $\underline{0.80}$ & $\underline{0.20}$ & $34$ \\
        $\sigma=8$&PnP-HVAE & $\mathbf{30.40}$ & $\mathbf{0.84}$ & $\mathbf{0.16}$ & $\mathbf{10}$\\
         & DPS & $28.70$ & $\underline{0.80}$ & $0.23$& $142$\\
        \hline 
         Deblurring &Adam & $\underline{30.17}$ & $\underline{0.83}$ & $\underline{0.21}$& $\underline{12}$\\
         (Gaussian) &ILO & $29.12$ & $0.79$ & $\mathbf{0.17}$ & $34$\\
        $\sigma=8$&PnP-HVAE& $\mathbf{30.81}$ & $\mathbf{0.86}$ & $0.24$ & $\mathbf{10}$\\
        & DPS & $29.14$ & $0.81$ & $ $0.24$ $ & $142$\\
        \end{tabular}\vspace*{-0.1cm}
    \caption{\label{table:comp_ILO}Quantitative evaluation on face restoration. Best results in {\bf bold}, second best \underline{underlined}.\vspace{-0.2cm}}
\end{table}