

\subsection{Global minimum of the hierarchical Gaussian negative log-likelihood}\label{sec:algo1-global-min}

In this section we show that under certain conditions Algorithm~\ref{algo:hierarchical_latent_reg} actually computes the global minimum of $J_2(\x,\z)$ with respct to $\z$.
To reach that conclusion we first decompose the objective function into several terms (equation~\eqref{eq:J2decomp} in proposition~\ref{prop:J2decomp}). Since many of these terms do not depend on $\z$ we conclude that
$$ \arg\min_{\z} J_2(\x,\z) = \arg\min_{\z} A(\z) + B(\z).$$

Furthermore, since the second term ($B(\z)$) only depends on $\z$ via the determinant of the encoder and decoder covariances, we have that under assumption~\ref{hypo:vpcov}
$$ \arg\min_{\z} J_2(\x,\z) = \arg\min_{\z} A(\z).$$

Finally, proposition~\ref{prop:global-min} shows that a functional of the form $A(\z)$ reaches its global minimum exactly at the point $E_{\tauvec}(\x)$ computed by Algorithm~\ref{algo:hierarchical_latent_reg}. Hence under assumption~\ref{hypo:vpcov} we have that
$$ \arg\min_{\z} J_2(\x,\z) = \arg\min_{\z} A(\z) = E_{\tauvec}(\x).$$



\begin{proposition}\label{prop:J2decomp}
	The objective $J_2(\x,\z)$ in equation~\eqref{eq:J2} can be decomposed as
	\begin{align}\label{eq:J2decomp}
		J_2(\x,\z) = f(\x)-\log\pd{\x} + A(\z) + B(\z) + C
	\end{align}
	where
	\begin{align}
		A(\z) &:= \sum_{l=0}^{L-1} A_l(\zl,\z_{<l}) \label{eq:A}\\
		B(\z) &:= \sum_{l=0}^{L-1} B_l(\z_{<l}) \\
		C      &:=  \sum_{l=0}^{L-1} C_l 
	\end{align}
	and
	\begin{align}
		A_l(\zl,\zll) &:= \| \zl - m_l(\zll)\|^2_{S_l^{-1}(\zll)} \label{eq:Al} \\
		B_l(\zll)      &:= \frac12 \log \det(S_l^{-1}(\zll)) + \frac12 (1-\lambda_l)\log \det(S_{p,l}^{-1}(\zll)) \\
		C_l             &:= \frac{d_l}{2} (\log \lambda_l -\lambda_l\log(2\pi))
	\end{align}
    and
    \begin{align}
    	m_{p,l}(\zll) & := \mu_{\theta,l}(\zll)           &   S_{p,l}(\zll) &:= \Sigma^{-1}_{\theta,l}(\zll) \label{eq:mplSpl} \\
    	m_{q,l}(\zll) &:=  \mu_{\phi,l}(\zll)               &   S_{q,l}(\zll) &:= \Sigma^{-1}_{\phi,l}(\zll)  \label{eq:mqlSql} \\
    	m_l(\zll) &:= S_{q,l}(\zll) m_{q,l}(\zll) + \lambda_l S_{p,l}(\zll) m_{p,l}(\zll) &
    	S_l(\zll) &:= S_{q,l}(\zll) + \lambda_l S_{p,l}(\zll)  \label{eq:gaussianprod}  %
    \end{align}
\end{proposition}

\begin{proof}
	First observe that $\qp{\zl|\x,\zll}$ and $\pt{\zl | \zll}$ are multivariate Gaussians as stated in equation~\eqref{eq:gaussian_hvae}. Also $\pt{\zl | \zll}^{\lambda_l}$ behaves like a Gaussian with a different normalization constant, namely
	$$  \pt{\zl | \zll}^{\lambda_l} = \N(\zl;m_{p,l},\lambda_l^{-1}S^{-1}_{p,l}) D_l  $$
	where the missing normalization constant is
	$$ D_l = (2\pi)^{-\frac{d_l}{2}(1-\lambda_l)}  \lambda_l^{-\frac{d_l}{2}} \det(S_{p,l}^{-1})^{-\frac{1}{2}(1-\lambda_l)}$$
	Now $  \qp{\zl | \x, \zll} \pt{\zl | \zll}^{\lambda_l} $ is the product of two Gaussians times the correcting term $D_l$. Since the product of two Gaussians is a Gaussian we obtain
	$$  \qp{\zl | \x, \zll} \pt{\zl | \zll}^{\lambda_l}  = \N(\zl; m_l, S_l^{-1}) D_l $$
	with mean and variance given by equation \eqref{eq:gaussianprod}.
	Taking  $-\log$ in the previous expression, we get $A_l + B_l + C_l$ by grouping into $A_l$ the  terms depending on both $\zl$ and $\zll$, in $B_l$ those depending only on $\zll$, and into $C_l$ the constant terms.
\end{proof}


\begin{assumption}[Volume-preserving covariances]\label{hypo:vpcov}
	The covariance matrices of the HVAE have constant determinant (not depending on $\zll$, although this constant may depend on the hierarchy level $l$)
	\begin{align}
		\det(\Sigma_{\phi,l}(\zll,\x)) &= c_l(\x)  \\
		\det(\Sigma_{\theta,l}(\zll)) &= d_l 
	\end{align}
\end{assumption}




\begin{proposition}[Algorithm~\ref{algo:hierarchical_latent_reg} computes the global minimum of $J_2(\x,\z)$ with respect to $\z$]\label{prop:global-min}
	Under Assumption~\ref{hypo:vpcov} minimizing $J_2(\x,\z)$ w.r.t. $\z$ is equivalent to minimizing  $A(\z)$ defined in equations~\eqref{eq:A}~and~\eqref{eq:Al}, \emph{i.e.}
	$$ \arg\min_{\z} J_2(\x,\z) = \arg\min_{\z} A(\z).$$
	In addition the global minimum of $A(\z_0, \dots, \z_{L-1})$ is given by the recursion computed by Algorithm~\ref{algo:hierarchical_latent_reg}, namely:
	    \begin{equation}
		\label{eq:hierarchical_solution}
		\begin{cases}
			\z_0^{\star} &=  m_0 \\
			\z_{l}^{\star} &= m_{l}(\z_{<l}^{\star}) \quad \text{for $l \in \{1, \dots, L-1\}$}
		\end{cases}
	\end{equation}
	where $\z_{<l}^{\star} = (\z_0^{\star}, \dots, \z_{l-1}^{\star})$, and $m_l(\zll)$ as defined in equations~\eqref{eq:mplSpl}~to~\eqref{eq:gaussianprod}. Put another way, $\z^{\star} = E_{\tauvec}(\x)$ as computed by Algorithm~\ref{algo:hierarchical_latent_reg}.
\end{proposition}



\begin{proof}
According to the decomposition of $J_2$ into several terms (equation~\eqref{eq:J2decomp} in proposition~\ref{prop:J2decomp}), we observe that many of these terms do not depend on $\z$. Therefore we conclude that
$$ \arg\min_{\z} J_2(\x,\z) = \arg\min_{\z} A(\z) + B(\z).$$

Furthermore, since the second term ($B(\z)$) only depends on $\z$ via the determinant of the encoder and decoder covariances, and these determinants do not depend on $\z$ under assumption~\ref{hypo:vpcov}, we conclude the first part of the proposition, namely
$$ \arg\min_{\z} J_2(\x,\z) = \arg\min_{\z} A(\z).$$

Now let's find the global minimum of $A(\z)$.

It is clear that $A(\z_0, \dots, \z_{L-1}) \geq 0$ for all $\z_0, \dots, \z_{L-1}$. 
It is also simple to verify that:
\begin{align} \nonumber
   & A(\z_1^{\star}, \dots, \z_{L-1}^{\star})\\ =&  ||m_0 - m_0||^2_{S_1^{-1}} + \sum_{l=1}^{L-1} ||m_{l}(\z_{<j}^{\star}) - m_l(\z_{<j}^{\star})||^2_{S_l^{-1}(\z_{<l}^{\star})} \nonumber\\
    =&0.
\end{align}
Therefore the minimum value of $A$ is reached in $\z^{\star}$.
Furthermore, for any $\z \neq \z^{\star}$, let us denote by $k$ the first value in $\{0, \dots, L-1\}$ such that $\z_k \neq m_k(\z_{<k}^{\star})$. 
Then, 
\begin{align}
    A(\z_0, \dots, \z_{L-1}) \geq ||\z_k - m_k(\z_{<k}^{\star})||^2_{S_k^{-1}(\z_{<k}^{\star})} > 0,
\end{align}
which implies that $\z^{\star}$ is the unique minimum of $A$.
\end{proof}

\paragraph{Discussion on assumption 1 (volume preserving covariance)}
We showed in proposition 5 that, under assumption 1, Algorithm~\ref{algo:hierarchical_latent_reg} computes the global minimum of $J_2(\x,\z)$ with respect to $\z$.
 When optimizing $z_{l}$ in~\eqref{eq:J2decomp} we only consider the impact of $\zl$ on the distance to the Gaussian mean in $A(\z)$, while ignoring its impact on the covariance volumes in the subsequent levels in the terms $B_{l'}(\z_{<l'})$, for $l' > l$. 
 If the covariance volumes are constant as stated in assumption 1, the value of $\zl$ has no impact on the covariance volumes of the subsequent levels, and algorithm~\ref{algo:hierarchical_latent_reg} gives the global minimizer of $J_2(\x, .)$ with respect to $\z$.
In practice, the HVAE we use does not enforce the covariance matrices of $p(\zl|\z_{<l})$ and $q(\zl|\z_{<l}, \x)$ to have constant volume.
However, the experiment in figure~\ref{fig:volume-preserving-covariance} shows that the variation of $B_{l+1}(\z_{<l+1})$ is negligible in front of $A_{l}(\zl)$.
Hence, we can reasonably expect algorithm~\ref{algo:hierarchical_latent_reg} to yield the minimum of $J_2(\x,\z)$ with respect to $\z$.
For future works, we could explicitly enforce assumption 1 in the HVAE design.

\begin{figure}
    \centering
    \includegraphics*[width=0.7\columnwidth]{figures/array_covlogdet_distance.png}\vspace{-0.3cm}
    \caption{\small%
    Evolution of {$B_{l+1}=\log \det S_{l+1}^{-1}(z_{<l+1})$} as a function of the distance ${A_{l}} = \|z_l - \mu_l(z_{<l})||^2_{{S_l^{-1}}(z_{<l})}$. (experiment made on VDVAE).\vspace{-0.5cm}%
    }\label{fig:volume-preserving-covariance}
 \end{figure}



\subsection{Proof of Proposition~\ref{eq:prop_lipschitz} (Lipschitz constant of one iteration)}\label{app:prop_lipschitz}

\begin{proof}
    For a decoder with constant covariance $\Vti{\z} = \frac{1}{\gamma^2}\id$, we have:
    \begin{equation}
        \T(\x) = \left(\hspace{-2pt}A^tA+\frac{\sigma^2}{\gamma^2}\id\right)^{\hspace{-2pt}-1}\hspace{-3pt}
        \left(\hspace{-2pt}
            A^t\y+\frac{\sigma^2}{\gamma^2}\mut{\enc{\x}}
        \hspace{-2pt}\right)
    \end{equation}
    and then :
    \begin{equation}
        ||T({\bf u}) - T({\bf v})|| \leq \left|\left|\left(A^tA+\frac{\sigma^2}{\gamma^2}\id\right)^{-1}\right|\right|\frac{\sigma^2L_{\tau}}{\gamma^2}||{\bf u} -{\bf v}|| .
    \end{equation}
    To conclude the proof, we use that for an invertible matrix~M, $||M^{-1}||=\frac{1}{\bf{\sigma}_{min}(M)}$,  where $\bf{\sigma}_{min}(M)$ is the smallest eigenvalue of $M$.
  We also use the fact that $\alpha$ is an eigenvalue of $A^tA+\frac{\sigma^2}{\gamma^2}\id$ if and only if $\alpha=\lambda +\frac{\sigma^2}{\gamma^2}$ for an eigenvalue $\lambda\geq 0$ of the positive definite matrix $A^tA$.
\end{proof}



\subsection{Proof of Proposition~\ref{prop:fixed_point} (fixed point of PnP-HVAE)}\label{app:fixed_point}

\begin{proof}
	$\x^*$ is a fixed point of $T$ if and only if $\x^*=T(\x^*)$.
	Recalling the definition of $ \T(\x) := \prox_{\gamma^2}f\left(\HVAE\left(\x, \tauvec\right)\right)$, and the definition of proximal operator $\prox_{\gamma^2 f}(\x) = \arg\min_{\bm{t}} \gamma^2f(\bm{u}) + \frac{1}{2}||\x - \bm{t}||^2$, the fixed point condition is equivalent to
	$$ \x^* = \arg\min_{\bm{t}} \frac12 \|\bm{t}-\HVAE\left(\x^*, \tauvec\right)\|^2 + \gamma^2 f(\bm{t}).$$
	Since $f$ is convex the above condition is equivalent to
	$$ \x^*-\HVAE\left(\x^*, \tauvec\right) + \gamma^2 \nabla f(\x^*) = 0.$$
	Rearranging the terms we obtain equation~\eqref{eq:fixed_point}.
\end{proof}

Under mild assumptions the above result can be restated as follows:
$\x^*$ is a fixed point of $T$ if and only if 
$$\nabla f(\x^*)+\nabla g(\x^*)=0,$$
\emph{i.e.} whenever $\x^*$ is a \emph{critical point} of the objective function $f(\x) + g(\x) = - \log p(\y|\x) - \log \ptt{\x}$, where the tempered prior is defined as the marginal
$$ \ptt{\x} = \int \ptt{\x,\z} d\z $$
of the joint tempered prior defined in equation~\eqref{eq:hvae_joint_model_temp}.

This is shown in the next section.

\subsection{Fixed points are critical points}\label{app:fixed_points_are_critical_points}

In this section we characterize fixed points of Algorithm~\ref{algo:final} as critical points of a posterior density (a necessary condition to be a MAP estimator), under mild conditions. Before we formulate this caracterization we need to review in more detail a few facts about HVAE training, temperature scaling and our optimization model.

\paragraph{HVAE training.}
In section 3.1 we introduced how VAEs in general (and HVAEs in particular) are trained. As a consequence an HVAE embeds a joint prior
\begin{equation}  \label{eq:joint_prior_decoder}
	\pt{\x,\z} := \pt{\x|\z}\pt{\z} 
\end{equation}
from which we can define a marginal prior on $\x$
\begin{equation}  \label{eq:x-prior}
	\pt{\x} := \int \pt{\x,\z} d\z. 
\end{equation}

In addition, from the ELBO maximization condition in \eqref{eq:perfect_vae} and Bayes theorem we can obtain an alternative expression for the joint prior, namely
\begin{equation} \label{eq:joint_prior_encoder}
	\pt{\x,\z}  = \qp{\z|\x} \pd{\x}. 
\end{equation}

\paragraph{Temperature scaling.}
After training we reduce the temperature by a factor $\tauvec$, which amounts to replacing $\pt{\z}$ by 
$$ \ptt{\z} := \prod_{l=0}^{L-1} \frac{\pt{\zl|\z_{<l}}^{\frac{1}{\tau_l^2}}}{Z_l} $$
as shown in equation~\eqref{eq:hvae_joint_model_temp}, leading to the joint tempered prior
\begin{equation}\label{eq:joint_tprior_decoder}
	\ptt{\x,\z} :=  \pt{\x|\z}\ptt{\z}.
\end{equation}
The corresponding marginal tempered prior on $\x$ becomes
\begin{equation}  \label{eq:x-tprior}
	\ptt{\x} := \int \ptt{\x,\z} d\z
\end{equation}
and the corresponding posterior is
\begin{equation}  \label{eq:tposterior-decoder}
	\ptt{\z|\x} := \ptt{\x,\z} / \ptt{\x}.
\end{equation}

The joint tempered prior also has an alternative expression (based on the encoder). Indeed substituting $\pt{\x|\z}$ from equations~\eqref{eq:joint_prior_decoder}~and~\eqref{eq:joint_prior_encoder} into \eqref{eq:joint_tprior_decoder} we obtain
\begin{equation}\label{eq:joint_tprior_encoder}
	\ptt{\x,\z} = \frac{\ptt{\z}}{\pt{\z}} \qp{\z|\x}\pd{\x}.
\end{equation}
Substituting this result into definition~\eqref{eq:tposterior-decoder} we obtain an alternative expression for the tempered posterior
\begin{equation}
	\ptt{\z|\x} = \qp{\z|\x}\pd{\x}/\pt{\z}.
\end{equation}

\paragraph{Optimization model.}
Since we are using a scaled prior $\ptt{\x}$ encoded in our HVAE to regularize the inverse problem, the ideal optimization objective we would like to minimize is
\begin{equation}\label{eq:xMAP}
	U(\x) := \underbrace{-\log p(\y|\x)}_{f(\x)} \underbrace{- \log \ptt{\x}}_{g(\x)}.
\end{equation}

Since $\ptt{\x}$ is intractable our algorithm seeks to minimize a relaxed objective (see equation~\eqref{eq:J1}). Nevertheless, under certain conditions (to be specified below) this is equivalent to minimizing the ideal objective \eqref{eq:xMAP}.

\paragraph{Fixed-point characterization.}
We start by characterizing $\nabla \log \ptt{\x}$ in terms of an HVAE-related denoiser (Proposition~\ref{prop:tweedie}). Then we relate this denoiser to the quantity $\HVAE(\x,\tau)$ that is computed by our algorithm (Proposition~\ref{prop:denoiser-caracterization}). As a consequence we obtain that the fixed point condition in Proposition~\ref{prop:fixed_point} can be written as $\nabla U(\x)=0$ (see Corollary~\ref{cor:critical-point}).

\begin{proposition}[Tweedie's formula for HVAEs.]\label{prop:tweedie}
For an HVAE with Gaussian decoder $\pt{\x|\z} = \N(\x;\mut{\z},\gamma^2 I)$, the following denoiser based on the HVAE with tempered prior
\begin{equation}\label{eq:HVAE-denoiser}
	\Dtt{\x} := \int \mut{\z} \ptt{\z|\x} d\z
\end{equation}
satisfies Tweeedie's formula
\begin{equation}\label{eq:tweedie}
	\Dtt{\x} - \x = \gamma^2 \nabla \log \ptt{x} = - \gamma^2 \nabla g(\x).
\end{equation}
\end{proposition}

\begin{proof}
From the definition of $\ptt{\x}$ in equation~\eqref{eq:x-tprior} we have that
$$ \nabla\log\ptt{\x} = \frac{1}{\ptt{\x}}\int  \nabla_{\x} \pt{\x|\z} \ptt{\z} d\z.$$
From the pdf of the Gaussian decoder $\pt{\x|\z}$ its gradient writes
$$ \nabla_{\x} \pt{\x|\z}  = - \frac{1}{\gamma^2} (\x-\mut{\z})\pt{\x|\z}.$$
Replacing this in the previous equation we get
\begin{align*}
	\nabla\log\ptt{\x} & = \frac{1}{\gamma^2} \int (\mut{\z}-\x) \frac{\pt{\x|\z}\ptt{\z}}{\ptt{\x}}d\z \\
	& =  \frac{1}{\gamma^2} \int (\mut{\z}-\x) \ptt{\z|\x}d\z \\
	& =  \frac{1}{\gamma^2} \left( \int \mut{\z}\ptt{\z|\x}d\z - \x\right).
\end{align*}
In the second step we used the definitions of the joint tempered prior $\ptt{\x,\z}$~\eqref{eq:joint_tprior_decoder} and the tempered posterior $\ptt{\z|\x}$~\eqref{eq:tposterior-decoder}.
The last step follows from the fact that $\int \ptt{\z|\x} d\z = 1$ according to definitions~\eqref{eq:tposterior-decoder}~and~\eqref{eq:x-tprior}.
Finally applying the definition of the denoiser $\Dtt{\x}$ in the last expression we obtain Tweedie's formula~\eqref{eq:tweedie}.
\end{proof}

Under suitable assumptions the denoiser defined above coincides with $\HVAE(\x,\tauvec)$ computed by our algorithm.

\begin{assumption}[Deterministic encoder]\label{hyp:deterministic-encoder}
	The covariance matrices of the encoder defined in equation~\eqref{eq:gaussian_hvae} are 0, \emph{i.e.} $\Sigma_{\phi,l}(\z_{<l},\x)=0$ for $l=0,\dots,L-1$.
	Put another way $\qp{\z|\x}=\delta_{E_{\tauvec}(\x)}(\z)$ is a Dirac centered at $E_{\tauvec}(\x)$.
\end{assumption}


\begin{proposition}\label{prop:denoiser-caracterization}
	Under Assumption~\ref{hyp:deterministic-encoder} the function $\HVAE(\x,\tauvec)$ computed by Algorithm~\ref{algo:final} coincides with the denoiser $\Dtt{\x}$ defined in equation~\eqref{eq:HVAE-denoiser}.
\end{proposition}

\begin{proof}
	$\HVAE(\x,\tauvec)$ is defined in Proposition~\ref{prop:fixed_point} as
	$$ \HVAE(\x,\tauvec) = \mut{E_\tau(\x)}.$$
	
	First observe that for a deterministic encoder we also have $\ptt{\z|\x} = \delta_{E_{\tauvec}(\x)}(\z)$.
	Indeed for any test function $h$:
	\begin{align*} 
		\int h(\z) \ptt{\z|\x} d\z & =  \int h(\z)\qp{\z|\x} \pd{\x} /\pt{\z} d\z \\
		& = h(E_{\tauvec}(\x))  \underbrace{\pd{\x} /\pt{E_{\tauvec}(\x)}}_{Z(\x)}.
	\end{align*}
	And the normalization constant $Z(\x)$ should be equal to 1 because $\int \ptt{\z|\x} d\z=Z(\x)=1$.
	Hence $\ptt{\z|\x} = \qp{\z|\x} = \delta_{E_{\tauvec}(\x)}(\z)$.
	
	Finally applying the definition of $\Dtt{\x}$ we obtain
	\begin{align*} 
		\Dtt{\x} & = \int \mut{\z} \ptt{\z|\x} d\z = \mut{E_{\tauvec}(\x)} \\
		& = \HVAE(\x,\tau).
	\end{align*}
\end{proof}

Combining Propositions~\ref{prop:denoiser-caracterization},~\ref{prop:fixed_point}~and~\ref{prop:tweedie} we obtain a new characterization of fixed points as critical points.

\begin{corollary}\label{cor:critical-point}
	Under Assumption~\ref{hyp:deterministic-encoder} $\x^*$ is a fixed point of $T$ if and only if
	\begin{equation}
		\nabla f(\x^*) + \nabla g(\x^*) = 0
	\end{equation}
   where $g(\x) = - \log \ptt{\x}$.
\end{corollary}

\begin{proof}
	From Proposition~\ref{prop:tweedie} we have that
	$$- \nabla g(\x) = \frac{1}{\gamma^2} \left(\Dtt{\x} - \x\right).$$
	From Proposition~\ref{prop:denoiser-caracterization} we have that (under Assumption~\ref{hyp:deterministic-encoder}) $\Dtt{\x} = \HVAE(\x,\tau)$. In combination with the previous result:
	$$- \nabla g(\x) = \frac{1}{\gamma^2} \left(\HVAE(\x,\tau) - \x\right).$$
	Finally, Proposition~\ref{prop:fixed_point} allows to conclude that
	$$ - \nabla g(\x) = \nabla f(\x).$$
\end{proof}



