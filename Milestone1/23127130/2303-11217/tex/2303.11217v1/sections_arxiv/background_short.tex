This work  exploits the capacity of HVAEs to model complex image distributions~\cite{child2021very,vahdat2020nvae}.
 We review the properties of the loss function used to train a VAE
(section~\ref{sec:perfect_vae}), then we detail the generalization of VAE to hierarchical VAE (section~\ref{ssec:hvae}), and present the temperature scaling approach to monitor the quality of generated images (section~\ref{ssec:temp}).

\subsection{VAE training} \label{sec:perfect_vae}

Variational autoencoders (VAE)  have been introduced in~\cite{kingma2013auto} to model complex data distributions. VAEs are trained to fit a parametric probability distribution in the form of a latent variable model:
\begin{equation}
    \pt{\x} = \int{\pt{\x|\z}\pt{\z}d\z},
\end{equation}
where $\pt{\x|\z}$ is a probabilistic decoder, and $\pt{\z}$ corresponds to the prior distribution over the model latent variable $\z$. 
A VAE is also composed of a probabilistic encoder $\qp{\z|\x}$, whose role is to approximate the posterior of the latent model $\pt{\z|\x}$, which is usually intractable.

The generative model parameters $\theta$ and the approximate posterior parameters $\phi$ of a VAE are jointly trained by maximizing the evidence lower bound (ELBO)~\cite{kingma2013auto, rezende2014stochastic} on a training data distribution $\pd{\x}$.
\begin{equation}
    \label{eq:elbo}
    \mathcal{L}\left(\x; \theta, \phi\right)\hspace{-1pt} =\hspace{-1pt} \E{\qp{\z|\x}}{\log\pt{\x|\z}} - \KL{\qp{\z|\x}}{\pt{\z}}.
\end{equation}
The ELBO expectation on $\pd{\x}$ is upper-bounded by the negative entropy of the data distribution,
and, when the upper-bound is reached we have that~\cite{zhao2017learning}:
\begin{equation}
    \label{eq:perfect_vae}
    \KL{\pd{\x}\qp{\z|\x}}{\pt{\x}\pt{\z|\x}} = 0.
\end{equation}


\subsection{Hierarchical variational autoencoders}\label{ssec:hvae}

The ability of hierarchical VAEs to model complex distributions is due to their hierarchical structure imposed in the latent space.
 The latent variable of HVAE is partitioned into $L$ subgroups $\z = (\z_0, \z_1, \cdots, \z_{L-1})$, and the prior and the encoder are respectively defined as: 
\begin{align}
    \pt{\z} &= \prod_{l=1}^{L-1} \pt{\z_l|\z_{<l}}\pt{\z_0}  \label{eq:hierarchical_prior}\\
    \qp{\z|\x} &= \prod_{l=1}^{L-1} \qp{\z_l|\z_{<l},\x}\qp{\z_0|\x}.    \label{eq:hierarchical_encoder}
\end{align}

We consider a specific class of HVAEs with gaussian conditional distributions for the encoder and the decoder
\begin{equation}
    \label{eq:gaussian_hvae}
    \hspace*{-0.25cm}\begin{cases}
        \pt{\z_l|\z_{<l}} &= \mathcal{N}\!\left(\z_l; \mutl{\z_{<l}}\hspace{-1pt}\hspace{-1pt}\Sigma_{\theta, l}(\z_{<l})\right) \\
        \qp{\z_l|\z_{<l}, \x}\mkern-18mu &= \mathcal{N}\!\left(\z_l; \mupl{\z_{<l}, \x}\hspace{-1pt},\hspace{-1pt} \Sigma_{\phi, l}(\z_{<l}, \x)\right)\hspace{-1pt},
    \end{cases}
\end{equation} 
where $\mu_{\theta, 0}$ and $\Sigma_{\theta, 0}$ can either be trainable or non-trainable constants, and the remaining mean vectors ($\mu_{\theta,l}$, and $\mu_{\phi,l}$, for $l>0$) and covariance matrices ($\Sigma_{\theta,l}$ and $\Sigma_{\phi,l}$, for $l>0$)  are parametrized by neural networks. \footnote{
	Note that for the special case $l=0$, $\z_{<l}$ is empty, meaning that 
	$\Sigma_{\theta, 0}(\z_{<0})=\Sigma_{\theta, 0}$ is actually a constant,
	$\qp{\z_0|\z_{<0},\x}=\qp{\z_0|\x}$ is only conditioned on $\x$, etc.
}
In this work, we consider models with a gaussian decoder:
\begin{equation}
    \label{eq:decoder_constant}
    \pt{\x|\z} = \N\left(\x; \mut{\z}, \gamma^2 I\right).
\end{equation}


\subsection{Temperature scaling}\label{ssec:temp}
As demonstrated in~\cite{vahdat2020nvae,child2021very}, sampling the latent variables $\z_l$ from a prior with reduced temperature improves the visual quality of the generated images from the VAE. 
In practice, this is done by multipling the covariance matrix of the gaussian distribution $\pt{\z_l|\z_{<l}}$ by a factor $\tau_l<1$.
This factor $\tau_l$ is called temperature because of its link to statistical physics.
Reducing the temperature of the priors amounts to defining the auxilliary model:
\begin{equation}
    \label{eq:hvae_joint_model_temp}
    p_{\theta, \tauvec}\left(\z_0, \cdots, \z_{L-1}, \x\right) = \prod_{l=0}^{L-1} \frac{\pt{\z_l|\z_{<l}}^{\frac{1}{\tau_l^2}}}{\tau_l^{\frac{d_l}{2}}}\pt{\x|\z_{<L}},
\end{equation}
where $\tauvec := \left(\tau_0, \cdots, \tau_{L-1}\right)$ gives the temperature for each level of the  hierarchy, and $d_l$ is the dimension of the latent variable $\z_l$.
In the following, we use this  temperature-scaled model to balance the regularization of our inverse problem.
