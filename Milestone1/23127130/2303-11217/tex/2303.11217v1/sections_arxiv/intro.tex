
In this work, we study linear inverse problems 
\begin{equation}
    \label{eq:invprob}
 \y = A\x + \e
\end{equation}
in which  $\y \in \R^m$ is the degraded observation, $\x \in \R^d$ the original signal we wish to retrieve, $A\in \R^{m\times d}$ is an observation matrix
and $\e \sim \N(0, \sigma^2 I)$ is an additive Gaussian noise. 
Many image restoration tasks can be formulated as~\eqref{eq:invprob}, including deblurring, super-resolution or inpainting.
 
With the development of deep learning in computer vision, image restoration have known significant progress.
The most straight-forward way to exploit deep learning for solving image inverse problems is to train a neural network  to map degraded images to their clean version in a supervised fashion.
However, this type of approach requires a large amount of training data, and it lacks flexibility, as one network is needed for each different inverse problem.


An alternate approach is to use deep latent variable generative models such as GANs or VAEs and to compute the Maximum-a-Posterior (MAP) estimator in the latent space:

\begin{equation}
    \label{eq:map}
    \hat{\z} = \arg\max_{\z} \log p\left(\y|G(\z)\right) + \log p\left(\z\right),
\end{equation}
where $\z$ is the latent variable and $G$ is the generative network~\cite{bora2017compressed,menon2020pulse}.
In~\eqref{eq:map} the likelihood $p\left(\y|G(\z)\right)$ is related to the forward model~\eqref{eq:invprob}, and $p\left(\z\right)$ corresponds to the prior distribution over the latent space.
After solving~\eqref{eq:map}, the solution of the inverse problem is defined as $\hat{\x}=G\left(\hat{\z}\right)$.
The latent optimization methods~\eqref{eq:map} provides high-quality solutions that are guaranteed to be in the range of a generative network. 
However, this implies highly non-convex problems~\eqref{eq:map} due to the complexity of the generator 
and the obtained  solutions  may lack of consistency with the degraded observation~\cite{Saharia2021}.
Although  the convergence of latent optimization algorithms has been studied in the  literature, existing convergence guarantees are either restricted to specific settings, or rely on assumptions that are hard to verify.


In this work, we propose an algorithm that exploits the strong prior of deep generative model while providing realistic convergence guarantees.
We  consider a specific type of deep generative model, the hierarchical variational autoencoder (HVAE).
 HVAE gives state-of-the-art results on image generation benchmarks~\cite{vahdat2020nvae, child2021very,hazami2022efficient,luhman2022optimizing}, and provides an encoder that will be key in the design of our proposed method.


As HVAE  model differs significantly from concurrent models architecture, it is necessary to design algorithms adapted to their specific structure.
The latent space dimension of HVAE is significantly higher than the image dimension.
 Hence, imposing the solution to lie in the image of the generator is not sufficient enough to regularize inverse problems.
Indeed, it has been observed that HVAEs can perfectly reconstruct out-of-domain images~\cite{havtorn2021hierarchical}. 
Consequently, we propose to constrain the latent variable of the solution to lie in the high probability area of the HVAE prior distribution.
This can be done efficiently by controlling the variance of the prior over the latent variables.


The common practice of optimizing the latent variables of the generative model with backpropagation is impractical due to the high dimensionality of the hierarchical latent space. Instead, we exploit the HVAE encoder to define an alternating algorithm~\cite{gonzalez2022solving} to optimize the joint distribution over the image and its latent variable


To derive convergence guarantees for our algorithm, we show that it can be reformulated as a Plug-and-Play (PnP) method~\cite{venkatakrishnan2013plug}, which alternates between an application of the proximal operator of the data-fidelity term, and a reconstruction by the HVAE.
Under this perspective, we give sufficient condition to ensure the convergence of our method, and we  provide an explicit characterization of the fixed-point
of the iterations. 
Motivated by the parallel with PnP methods, we name our method PnP-HVAE.



\subsection{Contributions and outline}
In this work, we introduce PnP-HVAE, a method for regularizing 
image restoration problems with a hierarchical variational autoencoder.
Our approach exploits the expressiveness of a deep HVAE generative model and its capacity to provide a strong prior on specialized datasets,  as well as convergence guarantees of Plug-and-Play  methods and their ability to 
deal with natural images of any size. 


After a review of related works (section~\ref{sec:related}) and of the  background on HVAEs (section~\ref{sec:back}), our contributions are the following. 
\newline\noindent$\bullet$   In section~\ref{sec:alternate}, we introduce PnP-HVAE, an algorithm to solve inverse problems with a HVAE prior.  
PnP-HVAE optimizes a joint posterior on image and latent variables without backpropagation through the generative network. It 

can be viewed as a generalization of JPMAP~\cite{gonzalez2022solving} to hierarchical VAEs,  with additional control of the regularization.
\newline\noindent$\bullet$   In section~\ref{sec:convergence}, we demonstrate the convergence of PnP-HVAE under hypotheses on the autoencoder reconstruction. Numerical experiments illustrate that the technical hypotheses are empirically met on noisy images with our proposed architecture. We also  exhibit the  better convergence properties of our alternate algorithm with respect to the use of Adam for optimizing the joint posterior objective.
\newline\noindent$\bullet$   In section~\ref{sec:expe}, we demonstrate the effectiveness of PnP-HVAE through image restoration experiments and comparisons on
(i) faces images using the pre-trained VDVAE model from~\cite{child2021very}; and (ii) natural images using the proposed PatchVDVAE architecture trained on natural image patches. 





