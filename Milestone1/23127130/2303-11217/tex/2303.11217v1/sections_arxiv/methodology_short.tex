In this section  we introduce our  Plug-and-Play method using a Hierarchical VAE  prior (PnP-HVAE) to solve generic image inverse problems. Building on top of the JPMAP framework~\cite{gonzalez2022solving}, we propose a joint model  over  the image and its latent variable that we optimize in an alternate way. By doing so, we take advantage of the HVAE encoder to avoid backpropagation through the generative network.
Although our motivation is similar to JPMAP, PnP-HVAE overcomes two of its limitations, that are the lack of control, and the limitation to simple and non-hierarchical VAEs.
We show in section~\ref{ssec:HJP} that the strength of the regularization of the tackled inverse problem can be monitored by tuning the temperature of the prior in the latent space. In section~\ref{ssec:encoder}, we  propose an approximation of the joint posterior distribution using the hierarchical VAE encoder. 
In section~\ref{ssec:optim}, we present our final algorithm that includes a new greedy scheme to optimize the latent variable of the HVAE.



\subsection{Tempered hierarchical joint posterior}\label{ssec:HJP}
The linear image degradation model~\eqref{eq:invprob} yields  
\begin{equation}
    \label{eq:f}
    p(\y|\x) \propto e^{-f(\x)}, \quad f(\x) = \frac{1}{2\sigma^2}||A\x-\y||^2.
\end{equation}
Solving the underlying image inverse problem in a bayesian framework requires an a priori distribution $p(\x)$ over clean images and studying the posterior distribution of $\x$ knowing its degraded observation $\y$. 
In this work, the image prior is given by a hierarchical VAE and we exploit the joint posterior model $p(\z,\x|\y)$. 
From the HVAE latent variable model with reduced temperature $p_{\theta, \bf{\tau}}\left(\z_0, \cdots, \z_{l-1}, \x\right)$ as defined in \eqref{eq:hvae_joint_model_temp}, 
we define the associated tempered joint model as:
\begin{equation}
    \label{eq:full_joint_model}
    p\left(\z
    , \x, \y\right) 
    := p_{\theta, \bf{\tau}}\left(\z_0, \cdots, \z_{L-1}, \x\right)p\left(\y|\x\right).
\end{equation}

Following the JPMAP idea from~\cite{gonzalez2022solving}, we aim at finding the couple $(\x,\z)$ that maximizes the joint posterior $p(\x, \z|\y)$: 
\begin{equation}
    \label{eq:joint_map}
    \min_{\x, \z} - \log p(\x, \z|\y) .
\end{equation}
Although we are only interested in finding the image $\x$,  the joint Maximum A Posteriori (MAP) criterion \eqref{eq:joint_map} makes it possible to derive an optimization scheme that only relies on forward calls of the HVAE. 
Using  Bayes' rule and the definition of the tempered HVAE joint model~\eqref{eq:full_joint_model} and~\eqref{eq:hvae_joint_model_temp}, the logarithm of the joint posterior rewrites:

\begin{align}
  &\log p(\x, \z|\y)\log p(\y) \\ =&\log p(\y|\x)  + \sum_{l=0}^{L-1} \log\frac{\pt{\z_l|\z_{<l}}^{\frac{1}{\tau_l^2}}}{\tau_l^{\frac{d_l}{2}}} + \log \pt{\x|\z_{<L}}\nonumber.
\end{align}


Since $p(\y)$ is constant,  
finding the joint MAP estimate \eqref{eq:joint_map} amounts to minimizing the following criterion:
\begin{align}
    \min_{\x,\z}J_1\left(\x, \z\right):=- \sum_{l=1}^{L-1}\frac{1}{\tau_l^2}\log \pt{\z_l|\z_{<l}}+f(\x) -\log \pt{\x|\z_{<L}}.
    \label{eq:J1}
\end{align}
Notice that the temperature of the prior over the latent space $\tau_l$ controls the weight of the regularization over the latent variable $\z_l$.
Optimizing~\eqref{eq:J1} w.r.t. $\x$ is tractable, whereas the minimization  w.r.t.$\z$ requires a backpropagation through the decoder $\log \pt{\x|\z_{<L}}$ that is impractical due to the high dimensionality and the hierarchical structure of the HVAE latent space.

\subsection{Encoder approximation of the joint posterior}\label{ssec:encoder}
Using the encoder $q_\phi$, we can reformulate the joint MAP problem~\eqref{eq:J1} in a form that is more  convenient to optimize with respect to $\z$.
Indeed, for a HVAE with enough capacity and trained to optimality, relation~\eqref{eq:perfect_vae} holds, and  we get
\begin{equation}
    \pt{\x|\z} = \frac{\qp{\z|\x}\pd{\x}}{\pt{\z}} \label{eq:decoder_approx}.
\end{equation}

Thus, by introducing the decoder expression~\eqref{eq:decoder_approx} in the full model~\eqref{eq:full_joint_model}, we have:
\begin{align}
    \nonumber
    p\left(\z, \x, \y\right)=
    \prod_{l=0}^{L-1} \frac{1}{\tau^{\frac{d_l}{2}}}\frac{\qp{\z_l|\z_{<l}, \x}}{\pt{\z_l|\z_{<l}}^{1-\tau_l^{-2}}}p(\y|\x)\pd{\x}.
\end{align} 

Denoting $\lambda_l = \frac{1}{\tau_l^2}-1$, we reformulate the joint MAP problem~\eqref{eq:joint_map} as a joint MAP problem over the encoder model:
\begin{align}
    %\nonumber
        \min_{\x,\z}\hspace{-1pt}J_2(\x, \z)\hspace{-2pt} :=  \hspace{-1pt}-\hspace{-4pt}\sum_{l=0}^{L-1} \hspace{-3pt}\left(\log\qp{\z_l|\x, \z_{<l}} \hspace{-2pt}+\hspace{-2pt} \lambda_l\hspace{-1pt}\log\pt{\z_l|\z_{<l}}\hspace{-1pt}\right)+f(\x)- \log\pd{\x}\label{eq:J2}.
\end{align}


\subsection{Alternate optimization with PnP-HVAE}\label{ssec:optim}
We introduce an alternate  scheme to minimize~\eqref{eq:joint_map} that  sequentially optimizes with respect to $\x$ and  to $\z$.
For a linear degradation model and a gaussian decoder~\eqref{eq:invprob}, the criterion $J_1(\x, \z)$ in~\eqref{eq:J1} is convex in $\x$ and its global minimum is

$\x = \left(A^tA+\frac{\sigma^2}{\gamma^2}\id\right)^{-1}
        \hspace{-4pt}\left(
            A^t\y+\frac{\sigma^2}{\gamma^2}\mut{\z}\right)$.
Next we propose to compute an approximate solution of the problem $\min_{\z} J_2(\x,\z)$ with the greedy 
 algorithm~\ref{algo:hierarchical_latent_reg}.

\begin{algorithm}
    \caption{Hierarchical encoding with latent regularization to minimize~\eqref{eq:J2} w.r.t. $\z$ for a fixed $\x$}\label{algo:hierarchical_latent_reg}
        \begin{algorithmic}\small
            \Require image $\x$; HVAE ($\phi,\theta$); temperature $\tau_l$;  $\lambda_l=\frac{1}{\tau_l^2}-1$
        \For{$0\leq l < L$}
      \State $S_q \gets \Vpil{{\z}_{<l}, \x}$; $m_q\gets\mupl{{\z}_{<l}, \x}$
        \Comment{Encoder}

        \State $S_p \gets \Vtil{{\z}_{<l}}$; $m_p\gets\mutl{{\z}_{<l}}$
        \Comment{Prior}

        \State $\z_l\gets \left(S_q + \lambda_lS_p\right)^{-1} 
        \left(S_qm_q + \lambda_lS_pm_p\right)$       

        \EndFor
            \State \textbf{return} $\enc{\x}=(\hat{\z}_{0},\hat{\z}_{1}, \cdots, \hat{\z}_{L-1})$
            \end{algorithmic}
\end{algorithm}
    
In algorithm~\ref{algo:hierarchical_latent_reg}, the latent variables $\z_l$ are determined in a hierarchical fashion starting from the coarsest to the finest one. 
As defined in relations~\eqref{eq:gaussian_hvae}, the conditionals $\qp{\z_l|\x, \z_{<l}}$ and $\pt{\z_l|\z_{<l}}$ are gaussians. Therefore, the minimization at each step can be viewed as an interpolation between the mean of the encoder $\qp{\z_l|\x, \z_{<l}}$ and the prior $\pt{\z_l|\z_{<l}}$, given some weights conditioned by the covariance matrices and the temperature $\tau_l$.
The solution of each minimization problem in $\z_l$ is given by:
\begin{align}
    \hat{\z_l} \hspace{-1pt}= &\hspace{-1pt}\left(\Vpil{\hat{\z}_{<l}, \x} + \lambda_l\Vtil{\hat{\z}_{<l}}\right)^{-1} \\
    &\hspace{-1pt}\left(\Vpil{\hat{\z}_{<l}, \x}\mupl{\hat{\z}_{<l}, \x} \hspace{-2pt}+ \hspace{-2pt}\lambda_l\Vtil{\hat{\z}_{<l}}\mutl{\hat{\z}_{<l}}\hspace{-1pt}\right)\nonumber
\end{align}
where $\lambda_l=1/{\tau_l^2}-1$. 
In the following, we denote as $\hat{\z} := \enc{\x}$ the output of the hierarchical encoding of Algorithm~\ref{algo:hierarchical_latent_reg}.
In Appendix~\ref{sec:algo1-global-min} we show that this algorithm finds the global optimum of $J_2(\x,\cdot)$ under mild assumptions.

The final PnP-HVAE procedure to solve an inverse problem with the HVAE prior is presented in Algorithm~\ref{algo:final}.
\begin{algorithm}[!h]
    \caption{PnP-HVAE - Restoration  by solving~\eqref{eq:J1}}\label{algo:final}
    \begin{algorithmic}\small
        \State $k \gets 0$; $res \gets + \infty$; 
        initialize $\x^{(0)}$
        \While{$res > tol$}
\State \textcolor{gray}{\% $\min_{\z} J_2(\x^{(k)},\z)$}
	\Comment{Optimize~\eqref{eq:J2} w.r.t. $\z$ using Alg. \ref{algo:hierarchical_latent_reg}}
	\State $\z^{(k+1)} = E_{\tauvec} (\x^{(k)})$
        \State \textcolor{gray}{\% $\min_{\x} J_1(\x,\z^{(k+1)})$}
        \Comment{Optimize~\eqref{eq:J1} w.r.t. $\x$}
        \State $\x^{(k+1)} = \left(A^tA+\frac{\sigma^2}{\gamma^2}\id\right)^{-1}
        \hspace{-4pt}\left(
            A^t\y+\frac{\sigma^2}{\gamma^2}\mut{\z^{(k+1)}}
        \right)$ 
        \State $res \gets ||\x^{(k+1)}-\x^{(k)}||$;  $k \gets k+1$
        \EndWhile
        \State \Return $\x^{(k)}$
    \end{algorithmic}
\end{algorithm}

