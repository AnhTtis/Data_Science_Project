
CNN methods for regularizing inverse problems can be classified in two main categories: regularization with denoisers (Plug-and-Play) and regularization with generative models. 


\subsection{Plug-and-Play methods} 

Plug-and-Play (PnP) and RED methods~\cite{venkatakrishnan2013plug,romano2017little} make use of a (deep) denoising method as a proxy to encode the local information over the prior distribution.
The denoiser is plugged in an optimization algorithm such as Half-Quadratic Splitting or ADMM in order to solve the inverse problem.
PnP algorithms come with theoretical convergence guarantees by imposing certain conditions on the denoiser network~\cite{ryu2019plug, pesquet2021learning, hurault2022gradient}.
These approaches provide state-of-the-art results on a wide variety of image modality thanks to the excellent performance of the currently available deep denoiser architectures~\cite{zhang2021plug}. 
However, PnP methods are only implicitly related to a probabilistic model, and they provide limited performance for challenging structured problems such as the inpainting of large occlusions.\\



\subsection{Deep generative models for inverse problems}
Generative models represent an explicit  image prior that can  be used to regularize ill-posed inverse problems~\cite{bora2017compressed, latorre2019fast, menon2020pulse, daras2021intermediate,  oberlin2021regularizing, pan2021exploiting,song2021solving}. 
They are latent variable models parametrized by neural networks, optimized to fit a training data distribution~\cite{kingma2013auto, goodfellow2020generative, dinh2016density, ho2020denoising}.


\noindent
{\bf  Convergence issues.} 
Regularization with generative models~\eqref{eq:map} 
 involves highly non-convex optimization over  latent variables \cite{bora2017compressed,menon2020pulse,oberlin2021regularizing}. 
As a consequence, the convergence guarantees of existing methods remain to be established~\cite{shah2018solving,raj2019gan,holden2022bayesian}. 
Convergent methods have only been proposed for restricted uses cases.
In compressed sensing problems with Gaussian measurement matrices, one can show that the objective function has a few critical points and design an algorithm to find the global optimum~\cite{hand2018phase,huang2021provably}. With a prior given by a VAE, and under technical hypothesis on the encoder and the VAE architecture, the Joint Posterior Maximization  with Autoencoding Prior (JPMAP) framework of~\cite{gonzalez2022solving} 
 converges toward a minimizer of the joint posterior $p(\x,\z|\y)$  of the image $\x$ and latent $\z$  given the  observation $\y$. 
JPMAP is nevertheless only designed for VAEs of limited expressiveness, with a simple  fixed gaussian prior distribution over the latent space. This makes it impossible to use this approach for anything other than toy examples.\\




\noindent
{\bf  Genericity issues.} When a highly structured dataset with fixed image size  is available ({\em e.g.}  face images~\cite{karras2019style}), deep generative models 
produce image restorations of impressive quality for severely ill-posed problems, such as 
super-resolution with huge upscaling factors.


For natural images, deep priors have \hbox{been improved by} 
normalizing flows \cite{rezende2015variational,dinh2016density}, GANs~\cite{goodfellow2020generative},  score-based  \cite{song2020score} and diffusion models \cite{ho2020denoising,Saharia2021,Saharia2022}. Note that the  use of  diffusion models in PnP is still based on  approximations~\cite{Song2023-pigdm,Chung2022}, assumptions~\cite{Kawar2022,Meng2022} or empirical algorithms~\cite{Lugmayr2022}. 
While GAN models were considered SOTA, modern  hierarchical VAE (HVAE) architectures were shown to display  quality on par with GANs \cite{child2021very,vahdat2020nvae}, 
while being 
 able to perfectly reconstruct out-of-domain images~\cite{havtorn2021hierarchical}. 
Integrating HVAE in a convergent scheme for natural image restoration of any size raises several theoretical and methodological challenges, as the image model of HVAE is  the push-forward of a {\em causal cascade of latent distributions}.

