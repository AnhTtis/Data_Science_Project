\documentclass[a4paper]{article}

\usepackage[a4paper]{geometry}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
% Include other packages here, before hyperref.
\usepackage{mydefs}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{tabularx,rotating}
\usepackage{wrapfig}
\usepackage{authblk}
%\usepackage{multicolumn}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usepgfplotslibrary{external} 
%\tikzexternalize

% Store counters to follow in supplementary
\include{xref-save}

%\pgfmathparse

\newcommand{\np}[1]{\textcolor{purple}{#1}}
\newcommand{\jean}[1]{\textcolor{red}{#1}}
\newcommand{\andres}[1]{\textcolor{blue}{#1}}
\newcommand{\ah}[1]{\textcolor{olive}{#1}}
\newcommand{\id}{\operatorname{Id}}
\newcommand{\diag}{\operatorname{diag}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission


\begin{document}

%%%%%%%%% TITLE
\title{Inverse problem regularization with hierarchical variational autoencoders\vspace{-0.2cm}}

\author[1]{Jean Prost}
\author[2]{Antoine Houdard}
\author[3]{Andrés Almansa}
\author[1]{Nicolas Papadakis}
%
\affil[1]{Univ. Bordeaux, CNRS, Bordeaux INP, IMB, UMR 5251, F-33400 Talence, France}
\affil[2]{Ubisoft La Forge}
\affil[3]{Université Paris Cité, CNRS, MAP5, UMR 8145, F-75013 Paris, France}

\date{}
\maketitle



%%%%%%%%% ABSTRACT 
\begin{abstract}
\input{sections_arxiv/abstract.tex}\vspace{-0.2cm}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\input{sections_arxiv/intro.tex}

\section{Related works}\label{sec:related}
\input{sections_arxiv/related_works.tex} 



\section{Background on  variational autoencoders}\label{sec:back}
\input{sections_arxiv/background_short.tex}



\section{Regularization with HVAE Prior}
\label{sec:alternate}
\input{sections_arxiv/methodology_short.tex} 

\section{Convergence analysis}\label{sec:convergence}
\input{sections_arxiv/theoretical_short.tex}



\section{Image restoration results}\label{sec:expe}
\input{sections_arxiv/experiments.tex}

\section{Conclusion}
\input{sections_arxiv/conclusion.tex}


\section*{Acknowledgements}
This study has  been carried out with financial support from the French Research Agency through the PostProdLEAP project (ANR-19-CE23-0027-01).


\input{sections_arxiv/appendix.tex}

\clearpage


\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{agustsson2017ntire}
Eirikur Agustsson and Radu Timofte.
\newblock Ntire 2017 challenge on single image super-resolution: Dataset and
  study.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pages 126--135, 2017.

\bibitem{altekruger2022patchnr}
Fabian Altekr{\"u}ger, Alexander Denker, Paul Hagemann, Johannes Hertrich,
  Peter Maass, and Gabriele Steidl.
\newblock Patchnr: Learning from small data by patch normalizing flow
  regularization.
\newblock {\em arXiv preprint arXiv:2205.12021}, 2022.

\bibitem{attouch2010proximal}
H{\'e}dy Attouch, J{\'e}r{\^o}me Bolte, Patrick Redont, and Antoine Soubeyran.
\newblock Proximal alternating minimization and projection methods for
  nonconvex problems: An approach based on the kurdyka-{\l}ojasiewicz
  inequality.
\newblock {\em Mathematics of operations research}, 35(2):438--457, 2010.

\bibitem{bora2017compressed}
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros~G Dimakis.
\newblock Compressed sensing using generative models.
\newblock In {\em International Conference on Machine Learning}, pages
  537--546. PMLR, 2017.

\bibitem{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock {\em arXiv preprint arXiv:1809.11096}, 2018.

\bibitem{child2021very}
Rewon Child.
\newblock Very deep vaes generalize autoregressive models and can outperform
  them on images.
\newblock {\em arXiv preprint arXiv:2011.10650}, 2020.

\bibitem{Chung2022}
Hyungjin Chung, Jeongsol Kim, Michael~T. Mccann, Marc~L. Klasky, and Jong~Chul
  Ye.
\newblock {Diffusion Posterior Sampling for General Noisy Inverse Problems}.
\newblock In {\em (ICLR) International Conference on Learning Representations},
  pages 1--28, sep 2023.

\bibitem{daras2021intermediate}
Giannis Daras, Joseph Dean, Ajil Jalal, and Alex Dimakis.
\newblock Intermediate layer optimization for inverse problems using deep
  generative models.
\newblock In {\em International Conference on Machine Learning}, pages
  2421--2432. PMLR, 2021.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems},
  34:8780--8794, 2021.

\bibitem{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{gonzalez2022solving}
Mario Gonz{\'a}lez, Andr{\'e}s Almansa, and Pauline Tan.
\newblock Solving inverse problems by joint posterior maximization with
  autoencoding prior.
\newblock {\em SIAM Journal on Imaging Sciences}, 15(2):822--859, 2022.

\bibitem{goodfellow2020generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em Communications of the ACM}, 63(11):139--144, 2020.

\bibitem{hand2018phase}
Paul Hand, Oscar Leong, and Vlad Voroninski.
\newblock Phase retrieval under a generative prior.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{havtorn2021hierarchical}
Jakob~D Havtorn, Jes Frellsen, S{\o}ren Hauberg, and Lars Maal{\o}e.
\newblock Hierarchical vaes know what they don’t know.
\newblock In {\em International Conference on Machine Learning}, pages
  4117--4128. PMLR, 2021.

\bibitem{hazami2022efficient}
Louay Hazami, Rayhane Mama, and Ragavan Thurairatnam.
\newblock Efficient-vdvae: Less is more.
\newblock {\em arXiv preprint arXiv:2203.13751}, 2022.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{holden2022bayesian}
Matthew Holden, Marcelo Pereyra, and Konstantinos~C Zygalakis.
\newblock Bayesian imaging with data-driven priors encoded by neural networks.
\newblock {\em SIAM Journal on Imaging Sciences}, 15(2):892--924, 2022.

\bibitem{huang2021provably}
Wen Huang, Paul Hand, Reinhard Heckel, and Vladislav Voroninski.
\newblock A provably convergent scheme for compressive sensing under random
  generative priors.
\newblock {\em Journal of Fourier Analysis and Applications}, 27:1--34, 2021.

\bibitem{hurault2022gradient}
Samuel Hurault, Arthur Leclaire, and Nicolas Papadakis.
\newblock Gradient step denoiser for convergent plug-and-play.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 4401--4410, 2019.

\bibitem{karras2020analyzing}
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
  Timo Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 8110--8119, 2020.

\bibitem{Kawar2022}
Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song.
\newblock {Denoising Diffusion Restoration Models}.
\newblock In {\em ICLR Workshop on Deep Generative Models for Highly Structured
  Data}, volume 2020-Decem, jan 2022.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{latorre2019fast}
Fabian Latorre, Volkan Cevher, et~al.
\newblock Fast and provable admm for learning with generative priors.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{levin2009understanding}
Anat Levin, Yair Weiss, Fredo Durand, and William~T Freeman.
\newblock Understanding and evaluating blind deconvolution algorithms.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 1964--1971. IEEE, 2009.

\bibitem{Lim_2017_CVPR_workshops}
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung~Mu Lee.
\newblock Enhanced deep residual networks for single image super-resolution.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops}, July 2017.

\bibitem{liu2018large}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Large-scale celebfaces attributes (celeba) dataset.
\newblock {\em Retrieved August}, 15(2018):11, 2018.

\bibitem{Lugmayr2022}
Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and
  Luc {Van Gool}.
\newblock {RePaint: Inpainting using Denoising Diffusion Probabilistic Models}.
\newblock In {\em (CVPR) Conference on Computer Vision and Pattern
  Recognition}, pages 11451--11461. IEEE, jun 2022.

\bibitem{luhman2022optimizing}
Eric Luhman and Troy Luhman.
\newblock Optimizing hierarchical image vaes for sample quality.
\newblock {\em arXiv preprint arXiv:2210.10205}, 2022.

\bibitem{MartinFTM01}
D. Martin, C. Fowlkes, D. Tal, and J. Malik.
\newblock A database of human segmented natural images and its application to
  evaluating segmentation algorithms and measuring ecological statistics.
\newblock In {\em Proc. 8th Int'l Conf. Computer Vision}, volume~2, pages
  416--423, July 2001.

\bibitem{Meng2022}
Xiangming Meng and Yoshiyuki Kabashima.
\newblock {Diffusion Model Based Posterior Sampling for Noisy Linear Inverse
  Problems}.
\newblock nov 2022.

\bibitem{menon2020pulse}
Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.
\newblock Pulse: Self-supervised photo upsampling via latent space exploration
  of generative models.
\newblock In {\em Proceedings of the ieee/cvf conference on computer vision and
  pattern recognition}, pages 2437--2445, 2020.

\bibitem{oberlin2021regularizing}
Thomas Oberlin and Mathieu Verm.
\newblock Regularization via deep generative models: an analysis point of view.
\newblock In {\em 2021 IEEE International Conference on Image Processing
  (ICIP)}, pages 404--408, 2021.

\bibitem{pan2021exploiting}
Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen~Change Loy, and Ping Luo.
\newblock Exploiting deep generative prior for versatile image restoration and
  manipulation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  44(11):7474--7489, 2021.

\bibitem{pesquet2021learning}
Jean-Christophe Pesquet, Audrey Repetti, Matthieu Terris, and Yves Wiaux.
\newblock Learning maximally monotone operators for image recovery.
\newblock {\em SIAM Journal on Imaging Sciences}, 14(3):1206--1237, 2021.

\bibitem{prakash2021interpretable}
Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, and Florian Jug.
\newblock Interpretable unsupervised diversity denoising and artefact removal.
\newblock {\em arXiv preprint arXiv:2104.01374}, 2021.

\bibitem{prost2021learning}
Jean Prost, Antoine Houdard, Andr{\'e}s Almansa, and Nicolas Papadakis.
\newblock Learning local regularization for variational image restoration.
\newblock In {\em Scale Space and Variational Methods in Computer Vision: 8th
  International Conference, SSVM 2021, Virtual Event, May 16--20, 2021,
  Proceedings}, pages 358--370. Springer, 2021.

\bibitem{raj2019gan}
Ankit Raj, Yuqi Li, and Yoram Bresler.
\newblock Gan-based projector for faster recovery with convergence guarantees
  in linear inverse problems.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5602--5611, 2019.

\bibitem{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International conference on machine learning}, pages
  1530--1538. PMLR, 2015.

\bibitem{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em International conference on machine learning}, pages
  1278--1286. PMLR, 2014.

\bibitem{romano2017little}
Yaniv Romano, Michael Elad, and Peyman Milanfar.
\newblock The little engine that could: Regularization by denoising (red).
\newblock {\em SIAM J. on Im. Sc.}, 10(4):1804--1844, 2017.

\bibitem{stylegan2pytorch}
Kim~Seonghyeon (rosinality).
\newblock stylegan2-pytorch.
\newblock \url{https://github.com/rosinality/stylegan2-pytorch}, 2020.

\bibitem{ryu2019plug}
Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao
  Yin.
\newblock Plug-and-play methods provably converge with properly trained
  denoisers.
\newblock In {\em International Conference on Machine Learning}, pages
  5546--5557. PMLR, 2019.

\bibitem{Saharia2022}
Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim
  Salimans, David Fleet, and Mohammad Norouzi.
\newblock {Palette: Image-to-Image Diffusion Models}.
\newblock In {\em (SIGGRAPH) Special Interest Group on Computer Graphics and
  Interactive Techniques Conference Proceedings}, number~1, pages 1--10, New
  York, NY, USA, aug 2022. ACM.

\bibitem{Saharia2021}
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David~J. Fleet, and
  Mohammad Norouzi.
\newblock {Image Super-Resolution via Iterative Refinement}.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages 1--14, apr 2021.

\bibitem{shah2018solving}
Viraj Shah and Chinmay Hegde.
\newblock Solving linear inverse problems using gan priors: An algorithm with
  provable guarantees.
\newblock In {\em 2018 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 4609--4613. IEEE, 2018.

\bibitem{Song2023-pigdm}
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz.
\newblock {Pseudoinverse-Guided Diffusion Models for Inverse Problems}.
\newblock In {\em (ICLR) International Conference on Learning Representations},
  2023.

\bibitem{song2021solving}
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon.
\newblock Solving inverse problems in medical imaging with score-based
  generative models.
\newblock {\em arXiv preprint arXiv:2111.08005}, 2021.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{vahdat2020nvae}
Arash Vahdat and Jan Kautz.
\newblock Nvae: A deep hierarchical variational autoencoder.
\newblock {\em Advances in neural information processing systems},
  33:19667--19679, 2020.

\bibitem{venkatakrishnan2013plug}
Singanallur~V Venkatakrishnan, Charles~A Bouman, and Brendt Wohlberg.
\newblock Plug-and-play priors for model based reconstruction.
\newblock In {\em 2013 IEEE Global Conference on Signal and Information
  Processing}, pages 945--948. IEEE, 2013.

\bibitem{zhang2021plug}
Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van~Gool, and Radu Timofte.
\newblock Plug-and-play image restoration with deep denoiser prior.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  44(10):6360--6376, 2021.

\bibitem{zhao2017learning}
Shengjia Zhao, Jiaming Song, and Stefano Ermon.
\newblock Learning hierarchical features from deep generative models.
\newblock In {\em International Conference on Machine Learning}, pages
  4091--4099. PMLR, 2017.

\bibitem{zoran2011learning}
Daniel Zoran and Yair Weiss.
\newblock From learning models of natural image patches to whole image
  restoration.
\newblock In {\em 2011 international conference on computer vision}, pages
  479--486. IEEE, 2011.

\end{thebibliography}





\end{document}