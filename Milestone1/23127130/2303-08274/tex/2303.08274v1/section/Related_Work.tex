
\section{Related work} \label{sec:intro}
\paragraph{Point Cloud Segmentation.}
Deep learning methods for point cloud processing can be categorized into three types: projection-based methods \cite{Tatarchenko2018Tangent3D, Wu_2019,Wu_2018, Boulch_2018, Jaritz_2019}, voxel-based methods \cite{Choy20194DNetworks,Graham_2015, Park_2022}, and point-based methods \cite{Charles_2017,Qi2017PointNet++:Space, Zhao2019Pointweb:Processing, Hu2019RandLA-Net:Cloudsb, Jiang2018PointSIFT:Segmentation, Li2018, Boulch_2020}. Recent work on point-based methods mostly adopts an encoder-decoder paradigm while applying various sub-sampling methods to expand the receptive field for point features. A variety of local aggregation modules were developed, including convolution-based methods \cite{Thomas2019KPConv:Cloudsb, Boulch_2020}, graph-based methods \cite{Landrieu2018Large-scaleGraphs, Landrieu2019PointLearning}, and attention-based methods \cite{Hu2019RandLA-Net:Clouds, Zhao_2021}. Meanwhile, different downsampling methods \cite{Chen2022SASA:Detection, Dovrat_2019, Yan2020PointASNL:Sampling}, upsampling \cite{Qiu2021SemanticFusion, Varney_2022}, and post-processing methods \cite{Hu_2020, Lu_2021} have been proposed to enhance the network's robustness. The inherent permutation invariance of attention operations makes them well-suited for point cloud learning, leading to the application of transformer operations in point clouds \cite{Zhao_2021, Engel_2021, Xie_2020}, following their success in 2D and NLP. However, global self-attention is impractical due to massive computational costs, leading to recent work utilizing local-scale self-attention to learn point features. Point Transformer \cite{Zhao_2021} proposed a vector self-attention with k nearest neighbor points for feature aggregation and introduced the concept of learnable positional embedding, which has proved to be very powerful across various tasks. Nevertheless, local attention has a limited receptive field and cannot explicitly model long-range dependencies. Thus, Stratified Transformer \cite{Lai_2022} was developed and adopted a shifted window strategy to include long-range contexts. Similarly, Fast Point Transformer \cite{Park_2022} utilized a voxel hashing-based architecture to enhance computational efficiency. However, these data-agnostic methods only select receptive fields based on spatial information, which could fail to focus on relevant areas.
\vspace{-5mm}
\paragraph{Geometric Partition}
Geometric partitions are an over-segmentation technique for point clouds that groups point with similar geometric features. Over the years, several methods have been developed to learn hand-crafted features of point clouds and utilize clustering or graph partition methods to generate meaningful partitions \cite{Papon_2013, Guinard2017WeaklyClouds, Lin_2018}. However, the performance of these methods is limited by the handcrafted feature of point clouds. SPNet \cite{Hui_2021} introduced a superpoint center updating scheme that generates superpoints with the supervision of point cloud labels. Although very efficient, SPNet fails to produce intuitive partitions beyond grouping points with the same semantic label together. On the other hand, the supervised superpoint method uses MLP to learn point features and combines points with graph-structured deep metric learning \cite{Landrieu2018Large-scaleGraphs}. Nevertheless, Geometric partition techniques offer impressive outcomes in finding geometric shapes in point clouds. However, despite their impressive performance, there have been few attempts to utilize these techniques in the deep learning framework \cite{Landrieu2018Large-scaleGraphs, Landrieu2019PointLearning}. In this regard, we propose GeoSpark, which leverages the geometry information stored in the partition to enhance feature aggregation and downsampling.
