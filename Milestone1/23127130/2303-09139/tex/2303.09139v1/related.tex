\section{Related Work}
Over the last two decades, a large body of works on the multi-agent narrow passage navigation problem in motion planning has emerged. 

Widely used sampling algorithms such as RRT~\cite{lavalle2001randomized} and PRM~\cite{kavraki1994probabilistic} can work in high-dimensional configuration spaces by, looking for feasible motion plans, and extensions including RRT$^*$~\cite{karaman2011anytime} and FMT$^*$~\cite{janson2015fast} can find (nearly) optimal trajectories. These algorithms have been extended to handle nonholonomic agents \cite{webb2013kinodynamic,doi:10.1177/0278364915614386}. Unfortunately, both theoretical analysis \cite{doi:10.1177/0278364917714338} and empirical studies \cite{szkandera2020narrow} have shown that such algorithms incur extremely high computational overheads. Indeed, narrow passages significantly reduce the set of the lookout \cite{619371}, which is crucial to the efficacy of sampling, while the complexity of optimal motion planning grows exponentially with the number of agents \cite{doi:10.1177/0278364917714338}. Almost all these algorithms are offline and inappropriate for time-critical applications such as autonomous driving.

Local navigation techniques use a set of heuristic rules to generate moving directions. These methods incur a much lower computational cost but sacrifice completeness or feasibility. In practice, however, they can have a high success rate under certain assumptions. Successful local navigation algorithms include the dynamic windows~\cite{fox1997dynamic}, reciprocal velocity obstacles (RVO)~\cite{van2011reciprocal,6225166,bareiss2015generalized}, and potential fields~\cite{koren1991potential,6943069}. All these methods were originally proposed for holonomic robots and extensions to differential drive models have been proposed. It is noteworthy that RVO and its variants can provide a collision-free guarantee, which allows agents to alter their moving directions or come to a full stop before collisions. This feature of RVO typically produces a yielding behavior allowing agents to move around local obstacles and continue towards the goal. However, the ambient space required for such yielding behaviors is generally larger for nonholonomic robots than holonomic ones, making RVO-based methods less successful in differential drive models and narrow passages. 

A different category of methods, known as centralized, global algorithms~\cite{yu2013multi,luna2011push,yu2018effective}, involves discretizing the agent motions on a grid or a graph-like structure. Graph search algorithms can then be used to find optimal~\cite{sharon2015conflict}, near optimal~\cite{yu2018effective}, or feasible trajectories~\cite{yu2015pebble} for large groups of agents within a relatively small computational budget. However, these methods are mostly designed for holonomic robots, and extensions to nonholonomic cases are far from trivial while their computational cost cannot meet real-time requirements. Our method can be interpreted as a special kind of centralized algorithm on the medial axis graph of the free space, on which we plan the POIs. The low-level yielding actions are then generated using local navigation techniques within each POI.

Finally, we have noticed some recent works~\cite{sun2017no,bareiss2015generalized,barrett2013teamwork,trautman2015robot} apply data-driven techniques to multi-agent navigation problems. By presenting agents with examples of optimal solutions in challenging scenarios, some learned policies can outperform analytic techniques. These techniques are parallel and orthogonal to our contribution. We speculate that learning-based techniques can be used as the local navigator in our method to generate high-quality yielding behaviors in large open areas. However, these methods incur a high computational cost in the training phase and re-training is required when the environment changes. The results of learned navigation policies are also sensitive to training parameters and network architectures. These potential drawbacks inspire us to design low-cost algorithms based on existing local navigation algorithms, with a higher success rate.

%Also, here are some noticed drawbacks of existed reinforcement learning technologies in multi agents motion planning applications. First of all, it take hours or even days to train agents in a certain scene, in which the shape of obstacles have to be known. It means if the shape of obstacles are changed, then they have to upload the data and recalculate the whole plan. Secondly, most of them are centralized methods, in other words, they assume agents building stable communication during navigation. Finally, another common knowledge is, the data driven methods are hard to be debug. More precisely, if agents got stuck or collided, there's a strong possibility that the training results miss some "holes" which do not cover current scene or agents' configuration states, while these "holes" are hard to be tracing during the training processing. These essential defects of learning technology could produce unpredictable results in robotics motion planning.