\section{Problem Description}\label{sec:problem_description}
We begin by describing the problem of BO for composite functions in subsection \ref{sec:desc_BO}. In subsection \ref{sec:demand_model} we describe the dynamic pricing problem and model the revenue function as a function composition to which BO methods for composite functions can be applied. 
\subsection{BO for Function Composition}\label{sec:desc_BO}
We consider the problem of optimizing $g(\textbf{x}) = h(f_1(\textbf{x}), f_2(\textbf{x}), \ldots , f_M(\textbf{x}))$ where $g : \mathcal{X} \rightarrow \mathrm{R}$, $f_i : \mathcal{X} \rightarrow \mathrm{R}$, $h:\mathrm{R}^{M} \rightarrow \mathrm{R}$ and $\mathcal{X} \subseteq \mathrm{R}^d$. We assume each $f_i$ is a black-box expensive-to-evaluate continuous function and $h$ is known and cheap to evaluate. The optimization problem that we consider is 
\begin{equation}\label{eq:problemdesc}
  \max_{\textbf{x} \in \mathcal{X}} ~h(f_1(\textbf{x}), f_2(\textbf{x}), \ldots, f_M(\textbf{x})). 
\end{equation}
We want to solve Problem~\ref{eq:problemdesc} in an iterative manner where in the $n^{\text{th}}$ iteration, we can use the previous observations $\{\textbf{x}_i, f_1(\textbf{x}_i), \ldots, f_M(\textbf{x}_i\})\}^{n-1}_{i=1}$ to request a new observation $\{\textbf{x}_n, f_1(\textbf{x}_1), \ldots, f_M(\textbf{x}_n)\}$.

A vanilla BO algorithm applied to this problem would first assume a prior GP model on $g$, denoted by $\mathcal{GP}(\mu(\cdot), K(\cdot, \cdot))$ where $\mu$ and $K$ denote the mean and covariance function of the prior model. Given some function evaluations, an updated posterior GP model is obtained. A suitable acquisition function, such as EI or PI can be used, to identify the next query point. For example, in the $n+1^{\text{th}}$ update round, one would first use the $n$ available observations $(g(\textbf{x}_1), g(\textbf{x}_2), \ldots, g(\textbf{x}_n))$ to update the GP model to  $\mathcal{GP}(\mu^{(n)}(\cdot), K^{(n)}(\cdot, \cdot))$   where $\mu^{(n)}(\cdot)$ is the posterior mean function and $K^{(n)}(\cdot, \cdot)$ is the posterior covariance function, see \cite{Rasmussen2005} for more details. The acquisition function then uses this posterior model to identify the next query location $\textbf{x}_{n+1}$. In doing so, vanilla BO ignores the values of the member functions in the composition $h$.
%while building a posterior.

BO for composite function, on the other hand, takes advantage of the available information about $h$, and its easy-to-compute nature. Astudillo and Frazier~\cite{https://doi.org/10.48550/arxiv.1906.01537} model the constituent functions of the composition by a single multi-output function $\textbf{f}(\textbf{x}) = (f_1(\textbf{x}), \ldots, f_M(\textbf{x}))$ and then model the uncertainty in $\textbf{f}(\textbf{x})$ using a multi-output Gaussian process to optimize $h({\textbf{f}(\textbf{x})})$. Since the prior over $f$ is modelled as a MOGP, the proposed method tries to capture the correlations between different components of the multi-output function $\textbf{f}(\textbf{x})$. Note that the proposed EI and PI-based acquisition functions are required to be computed using Monte Carlo sampling. Furthermore, a sample from the posterior distribution is obtained by first sampling an $n$ variate normal distribution, then scaling it by the lower Cholesky factor and then centering it with the mean of the posterior GP. Two problems arise due to this: \begin{enumerate*} 
    \item Such simulation based averaging approach increases the time complexity of the procedure  linearly with the number of samples taken for averaging and
    \item calculation of the lower Cholesky factor increases the function's time complexity cubically with the number of data points. 
\end{enumerate*}
These factors render the algorithm unsuitable, particularly for problems with large number of member functions or for problems with large dimensions.


% \textbf{Expand on this, why sampling and inversion is needed, describe their acquisiton in 1-2 lines, enumerate drawbacks of astuldo work, 1) computationally expensive 2) inversion 3) 1&2 make it slow and not suitable for large dimension problems  }


To alleviate these problems, in this work, we model the constituent functions using independent GPs. This modelling approach allows us to train GPs for each output independently and hence the posterior GP update can be parallelized. We propose two acquisition functions, cEI which is based on the EI algorithm and cUCB, which is based on the GP-UCB algorithm \cite{Srinivas_2012}. Our cEI acquisition function is similar in spirit to the EI-CF acquisition function of \cite{https://doi.org/10.48550/arxiv.1906.01537} but is less computationally intensive owing to the independent GP model. Since we have independent one dimensional GP model for each constituent function, sampling points from the posterior GP does not require computing the Cholesky factor (and hence matrix inversion), something that is needed in the case of high-dimensional GP's of \cite{https://doi.org/10.48550/arxiv.1906.01537}.  
This greatly reduces the complexity of the MC sampling steps of our algorithm (see section \ref{sec:our_approach} for more details). However, the cEI acquisition function still suffers from the drawback of requiring Monte Carlo averaging. 
To alleviate this problem, we propose a UCB based acquisition function that uses the current mean plus scaled variance of the posterior GP at a point as a surrogate for the constituent function at that point. As shown by Srinivas et al.~\cite{Srinivas_2012}, while the mean term in the surrogate guides exploitation, it is the variance of the posterior GP at a point that allows for suitable exploration. The scaling of the variance term is controlled in such a way that it balances the trade off between exploration and exploitation. In Section~\ref{sec:experiments}, we illustrate the utility of our method, first for standard test functions and then as an application to dynamic pricing problem. Our algorithms, especially the cUCB one, outperforms not only vanilla BO but also those proposed in Astudillo and Frazier~\cite{https://doi.org/10.48550/arxiv.1906.01537}.

%and does not require matrix inversion, resulting in the complexity for computing the variance for each function reducing from cubic to constant time. Our UCB variant helps us eliminate Monte Carlo sampling while computing the acquisition function. 
% \textbf{describe 1-2 lines what are the main advantages of doing this over atuldo}. In this work, we also propose a a UCB-style algorithm which helps us reduce the computational complexity of the process.
% \textbf{Refer to GP UCB paper here, again say in detail how the UCB offers benefits, other than not requiring matrix inversion are there more benefits ? The sigma term in UCB allows for better exploration?} Our method does not require the computation of an inverse matrix. 

% \textbf{Vanilla BO}
%  One possible approach to solve this is using a vanilla Bayesian Optimization of $g(x)$, we assume $g$ to be drawn from a GP prior probability distribution. It would use an acquisition function like EI to optimize the GP prior over $f$ and ignore the observations on $f_i$ in the process.

% \textbf{Astuldo}
% We want to solve Problem~\ref{eq:problemdesc} in an iterative manner. In the $k^{th}$ iteration, we can use the previous observations $\{x_i, f_1(x_i), \ldots, f_N(x_N)\}^{k-1}_{i=1}$ to request a new observation $\{x_k, f_1(x_k), \ldots, f_N(x_k)\}$.

% \textbf{Drawbacks of Astuldo}
% Multi output GP, computationally intensive, acquisition functions require matrix inversion etc 

% \textbf{Motivation to our algorithms }
% Independent GP, UCB type acquisition function that does not need inversion. Performance gains we see in terms of run time.
% more details section 3

% \textbf{Move this para to vani9lla BO, add mathematical details, see prabu paper or fobo paper}
% While traditional BO would model a GP over the function $g$ as described in Section~\ref{sec:problem_description} and ignore the values of the member functions in the composition of $h$ while building a prior, our idea is to take advantage of the information we have about the cheap to evaluate function, $h$. Traditional BO would then look to use standard acquisition functions such as Expected Improvement (EI) or Probability of Improvement (PI) to find the point at which the model predicts the best results should be. That point would be evaluated next and the result would be used to update the GP and suggest the next point to evaluate. This method works well with standard functions but would ignore the domain information we have about $h$ when optimizing a composite function.


\subsection{Bayesian Optimization for dynamic pricing}\label{sec:demand_model}
We consider Bayesian Optimization for two types of revenue optimization problems. The first problem optimizes the revenue per customer where customers are characterized by their willingness-to-pay distribution (which is unknown). In the second problem, we assume a parametric demand model (the functional form is assumed to be unknown) and optimizes the associated revenue. 

In the first model, we assume that an arriving customer has an associated random variable, $V$, with complimentary cumulative distribution function $\bar{F}$, indicating its maximum willingness to pay for the item sold. For an item on offer at a price $p$, an arriving customer purchases it with the probability 
\begin{equation}\label{eq:fp}
    d(p):= \bar{F}(p) = \text{Pr}\{V \geq p\}  .
\end{equation}
In this case, when the product is on offer at a price $p$, the revenue per customer $r(p)$ is given by $r(p) = p \bar{F}(p)$. The revenue function is a composition of the price and demand or purchase probability and we assume that the distribution of the purchase probability i.e., $F$ is not known and also expensive to estimate. One could perform a vanilla BO algorithm by having a GP model on $r(p)$ itself. However to exploit the known nature of the revenue function, we will apply our function composition method by instead having a GP on  $\bar{F}(p)$ and demonstrate its superiority over vanilla BO.  

In the second model, we assume that the true demand $d(p)$ for a commodity at price $p$ has a functional form. This forms the ground truth model that governs the demand, but we assume that the functional form for this demand is not known to the manager optimizing the revenue. In our experiments, we assume linear, logit, Booth and Matyas functional forms for the demand (see section \ref{sec:experiments} for more details.)
Along similar lines, one could build more sophisticated demand models to account for external factors (such as supply chain issues, customer demographics or inventory variables), something that we leave for future explorations. 
%in the model by modelling demand as a parametric function of price
%\begin{equation}
 %   d(p, \boldsymbol{z}):= \bar{F}_{\boldsymbol{z}}(p) = \text{Pr}\{V \geq p\}
%\end{equation}
%where $z$ is the function parameter over $\bar{F}$ which can evolve over time with changing conditions.

Note that we make some simplifying assumptions about the retail environment in these two models and our experiments. We assume a non-competitive monopolistic market with an unlimited supply of the product and no marginal cost of production. However, these assumptions can easily be relaxed by changing the ground truth demand model appropriately, which are used in the experiments to reflect these aspects. The fact that we use a GP model as a surrogate for the unknown demand model offers it the ability to model a diverse class of demand functions under diverse problem settings. We do not discuss these aspects further but focus on the following simple yet meaningful experimental examples that one typically encounters in revenue management problems.  
%We also assume that the price for a product can be adjusted instantaneously and that customer feedback is received in real time. Moreover, we also assume that it is feasible to quote different prices to the customer simultaneously as well and expose them to customers with identical natures. 
%We now describe the experimental setups to which we apply our composite BO algorithms. 
In the following, $\textbf{p}$ denotes the price vector:
\begin{enumerate}
    \item \textbf{Independent demand model:} A retailer supplies its product to two different regions whose customer markets behave independently from each other. Thus, the same product has independent and different demand functions (and hence different optimal prices) in different geographical regions and under such black-box demand models for the two regions $(d_1, d_2)$. The retailer is interested in finding the optimal prices, leading to the optimization of the following function: $g(\textbf{p}) = p_1d_1(p_1) + p_2d_2(p_2)$. 
    \item \textbf{Correlated demand model:} Assume that a retailer supplies two products at prices $p_1$ and $p_2$ and the demand for the two products is correlated and influenced by the price for the other product. 
    %product to two regions, but their customer market does not behave independently from each other, leading to the objective function 
    Such a scenario can be modelled by a revenue function of the form $g(\textbf{p}) = p_1d_1(\textbf{p}) + p_2d_2(\textbf{p})$. Consider the  example where the prices of business and economy class tickets can influence the demand in each segment. Similarly, the demand for a particular dish in a fast food chain might be influenced by the prices for other dishes.
    \item \textbf{Identical price model: } In this case, the retailer is compliant with having a uniform price across locations. However, the demand function across different locations could be independent at each of these locations, leading to the following objective function: $g(p) = pd_1(p) + pd_2(p)$. This scenario can be used to model different demand functions for different population segments in their age, gender, socio-economic background, etc. 
\end{enumerate}
%Note that the first scenario is a special case of the second scenario where function $d_i$ is only dependent on $p_i$.