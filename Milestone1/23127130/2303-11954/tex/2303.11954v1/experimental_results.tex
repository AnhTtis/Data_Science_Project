\section{Experiments and Results}\label{sec:experiments}
In this section, we compare the results of our cUCB and cEI algorithms with Vanilla EI, Vanilla UCB and the state-of-the-art BO for Composite Functions (BO-CF)~\cite{https://doi.org/10.48550/arxiv.1906.01537} using HOGP~\cite{maddox2021bayesian} in terms of loss in regret and runtime of the algorithms. We first compare our methods on 3 test functions and then move on to show their applications to three different pricing scenarios. Our code is available \href{https://github.com/kjain1810/Bayesian-Optimization-for-Function-Compositions-with-Applications-to-Dynamic-Pricing}{here}.

Our algorithms are implemented with the help of the BoTorch framework~\cite{botorch} and use the APIs provided by them to declare and fit the GP models. We assume noiseless observations for our results in this section, and the same results can be obtained when we add Gaussian noise to the problem with a fixed mean and variance. We start with the same 10 initial random points and run our BO algorithms for 70 iterations. We use a system with 96 Intel Xeon Gold 6226R CPU @2.90GHz and 96GB of memory shared between the CPUs.

We compare the performance of different algorithms based on the log of mean minimum regret till each iteration, averaged over 100 runs. In a single BO run, the regret at iteration $i$ in the $k^{th}$ run is defined as $l_i^k = g^* - g(\textbf{x}_i)$ where $g^*$ is the global maximum of the objective function. The minimum regret at iteration $i$ in the $k^{th}$ run is defined as $m_i^k = \min_{1 \leq j \leq i}l_j^k$ and the final metric at iteration $i$ averaged across 100 runs is calculated as 
$$r_i = \log_{10}\left(\frac{1}{100}\sum_{k=1}^{100}m_i^k\right).$$

\subsection{Results on Test Functions}
\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/regret_langermann.png}
  \caption{Langermann function}
  \label{fig:langermann}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/regret_dixonprice.png}
  \caption{Dixon-Price function}
  \label{fig:dixonprice}
\end{subfigure}
\caption{Log regret for test functions with a composite nature}
\label{fig:testfunctions}
\end{figure}

We first asses our algorithms on three standard test functions~\cite{sfu}:
\subsubsection{Langermann Function:} 
To express this function as a function composition, we consider each outer iteration of the Langermann function to be a separate constituent function, that is, 
$$f_i(\textbf{x}) = \exp\left(-\frac{1}{\pi}\sum\limits_{j=1}^d(x_j-A_{ij})^2\right)\cos\left(\pi\sum\limits_{j=1}^d(x_j-A_{ij})\right).$$
The composition for this will be $h(\{f_i(\textbf{x})\}) = \sum_{i=1}^mc_if_i(\textbf{x})$ with $d=2$, $c=(1,2,5,2,3)$, $m=5$, $A=((3,5),(5,2),(2,1),(1,4),(7,9))$ and domain $\mathcal{X}=[0, 10]^2$. Note that the terms differ only in the columns of hyperparameter $A$ for different member functions and thus, should have a high covariance.

\subsubsection{Dixon-Price Function} In this function, we take the term associated with each dimension of the input to be a separate constituent, that is,
$$f_1(\textbf{x}) = (x_1 - 1)^2, f_i(\textbf{x}) = i(2x_i^2-x_{i-1})^2$$
The composition for this function will be $h(\{f_i(\textbf{x})\}) = \sum_{i=1}^df_i(\textbf{x})$ with $d=5$ and domain $\mathcal{X}=[-10, 10]^d$. Since only consecutive terms in this function share one variable, the member functions do have a non-zero covariance but it will not be as high as in the Langermann function.

\subsubsection{Ackley Function} Here, we build a more complex composition function by considering the terms in the exponents as the member functions, resulting in
$$f_1(\textbf{x})=\sqrt{\frac{1}{d}\sum\limits_{i=1}^dx_i^2} \text{ and } f_2(\textbf{x})=\frac{1}{d}\sum\limits_{i=1}^d\cos(cx_i)$$
$$h(\textbf{x}) = -a\exp(-bf_1(\textbf{x}))-\exp(f_2(\textbf{x}))+a+\exp(1)$$
with $d=5$, $a=20$, $b-0.2$, $c=2\pi$ and domain $\mathcal{X}=[-32.768, 32.768]^d$.

\subsubsection{Results} Figure~\ref{fig:langermann},~\ref{fig:dixonprice} and~\ref{fig:regret_ackely} compares the results of different algorithms. Vanilla EI and UCB algorithms do not consider the composite nature of the function while BO-CF and our methods use the composition defined above. Even with the high covariance between the members in Langermann function, cUCB outperforms BO-CF while the cEI algorithm has a similar performance level. However, when that covariance reduces in the Dixon-Price function, the cEI algorithm performs better than BO-CF while the cUCB algorithm significantly outperforms it. Figure~\ref{fig:regret_ackely} shows that our algorithms work well with complicated composition functions as well and both, cUCB and cEI, outperform BO-CF.

\subsection{Results for Demand Pricing Experiments}
\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/regret_ackley.png}
  \caption{Ackley function}
  \label{fig:regret_ackely}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/regret_pricing1.png}
  \caption{Independent demand model}
  \label{fig:pricing1}
\end{subfigure}
\caption{Log of expected regret for pricing tasks}
\label{fig:testfunctions}
\end{figure}

We now test our approach on the demand models discussed in Section~\ref{sec:demand_model}.
\subsubsection{Independent Demand Model:} Recall that in this model, we allow the price at each location to be different and model the demand in a region to depend only on the price therein. We consider 4 regions where each region has a parametric demand functions and randomly chosen parameters. Particularly, we assume that two regions have  a logit demand function ($d(p) = \frac{e^{-z_1-z_2p}}{1+e^{-z_1-z_2p}}$) with $z_1\in\left[1.0, 2.0\right]$, $z_2\in\left[-1.0. 1.0\right]$ and the other two regions have a linear demand function ($d(p) = z_1-z_2p$) with $z_1\in\left[0.75, 1\right]$ and $z_2\in[2/3, 0.75]$. The domain for this model is $\mathcal{X}=[0,1]^4$ and the composition is 
The composition function looks $h(d_1(\textbf{p}), d_2(\textbf{p}), d_3(\textbf{p}), d_4(\textbf{p})) = \sum_{i=1}^4p_id_i(p_i)$.
%  like Equation(\ref{eq:firstdemand}) and domain space is $\mathcal{X}=[0,1]^4$
% \begin{equation}\label{eq:firstdemand}
%     h(d_1(\textbf{p}), d_2(\textbf{p}), d_3(\textbf{p}), d_4(\textbf{p})) = \sum\limits_{i=1}^4p_id_i(p_i)
% \end{equation}
\subsubsection{Correlated Demand Model:} In this example, we assume that different products are for sale at different prices, and there is a certain correlation between demand for different products via their prices. We consider the case of 2 products where one of the product has a demand function governed by the Matyas function~\cite{sfu}. We assume that the demand for the second product is governed by the Booth function~\cite{sfu}. More specifically, the function composition and constituent functions are as below, where the domain for the problem is $\mathcal{X}=[0,10]^2$:
\begin{align*}
    d_1(\textbf{p}) &= 8(100 - \text{Matyas}(\textbf{p})) \\
    d_2(\textbf{p}) &= 1154 - \text{Booth}(\textbf{p})
\end{align*}
where Matyas function is defined as $\text{Matyas}(\textbf{x}) = 0.26(x_1^2+x_2^2)-0.48p_1p_2$. Similarly, Booth function is defined as $\text{Booth}(\textbf{x}) = (x_1 + 2x_2 -7)^2 + (2x_1+x_2-5)^2$. The composition for this will be $h(d_1(\textbf{p}), d_2(\textbf{p})) = p_1d_1(\textbf{p}) + p_2d_2(\textbf{p})$.
\subsubsection{Identical price Model:} In this example, we assume that a commodity is sold for same price at two different regions but the willingness to pay variable for customers in the two regions is different. We assume that the willingness to pay distribution in one region follows  exponential distribution with $\lambda = 5.0$. n the other region, this is assumed to be a gamma distribution with $\alpha=10.0, \beta=10.0$ The resulting function composition is given by :
$h(d_1(p), d_2(p)) = pd_1(p) + pd_2(p)$.
\subsubsection{Results} Figures~\ref{fig:pricing1}, \ref{fig:pricing2} and \ref{fig:pricing_3} compare the results of these dynamic pricing models for the different BO algorithms. 
Our algorithms perform well even in higher dimensions of input and member functions with cUCB marginally outperforming BO-CF in the first model. cUCB matches the minimum regret in the second model and converges to it much faster than BO-CF. In the case of the third model, having independent GP's performs better than BO-CF with cEI.
\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/regret_pricing_2.png}
  \caption{Correlated demand model}
  \label{fig:pricing2}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{plots/regret_pricing_3.png}
  \caption{Identical price model}
  \label{fig:pricing_3}
\end{subfigure}
\caption{Log of expected regret for pricing tasks}
\label{fig:testfunctions}
\end{figure}
\begin{table}[t]
\centering
\begin{tabular}{lccccccc}
    \hline
    Task & EI & UCB & cEI & cUCB & HOGP\\
    \hline
    Langermann function & 1.74 & 1.73 & 9.71 & 9.34 & 36.30 \\
    Dixon-Price function & 1.70 & 1.69 & 14.47 & 12.71 & 41.72 \\
    Ackley function & 1.63 & 1.55 & 11.56 & 11.37 & 47.91 \\
    Independent demand model & 2.05 & 2.03 & 7.85 & 7.12 & 19.12 \\
    Correlated demand model & 3.91 & 3.29 & 9.63 & 9.19 & 53.01 \\
    Identical price model & 2.55 & 2.38 & 7.27 & 7.81 & 24.37 \\
    \hline
\end{tabular}
\caption{Run-time for 70 iterations across algorithms in seconds}
\label{tab:runtimes}
\end{table}
\subsection{Runtime Comparisons with State of the Art}
Along with the performance of our algorithms being superior in terms of regret, the methodology of training independent GPs is significantly faster in terms of run time. As shown in Table~\ref{tab:runtimes}, both of our algorithms are between 3 to 4 times faster than BO-CF using HOGP on average and their run time increases linearly with the number of member functions in the composition when compared to vanilla EI and UCB.
By not having to compute the inverse matrix for estimating the lower Cholesky factor of the covariance matrix, we gain large improvements in run time.
The elimination of inverse matrix computation 
while estimating with the help of lower Cholesky factor of the covariance matrix results in the large improvement in run time over BO-CF. Also note that our UCB variant is marginally faster than the EI variant as well due to the elimination of MC sampling in the process.