\section{Proposed Method}\label{sec:our_approach}
% \subsection{Overview}
% \textbf{Change this according to section 2 changes }
As discussed in Section~\ref{sec:problem_description}, we propose that instead of having a single GP model over $g$, we have $M$ different GP models over each constituent function in the composition. Each prior GP model will be updated using GP regression whenever the observations of the constituent functions are available. A suitably designed acquisition function would then try to find the optimal point when the constituent functions should all be evaluated at, in the next iteration. For ease of notation, we use the shorthand $h(\{f_i(\textbf{x})\})$ to denote $h(f_1(\textbf{x}), \ldots, f_M(\textbf{x}))$ in the subsequent sections.

\subsection{Statistical Model and GP regression}
Let $f_i^{1:n},~ i\in\{1,2,\ldots,M\}$ denote the function evaluations of the member functions at locations $\{\textbf{x}_1,\textbf{x}_2, \ldots, \textbf{x}_n\}$ denotes as $\textbf{x}^{1:n}$. In the input space $\mathcal{X} \subset \mathbb{R}^d$, let $\mathcal{GP}(\mu^{(n)}_i, k^{(n)}_i)$ be the posterior GP over the function $f_i$ where $\mu^{(n)}_i : \mathcal{X} \rightarrow \mathbb{R}$ is the posterior mean function, $k^{(n)}_i : \mathcal{X} \times \mathcal{X} \rightarrow \mathcal{R}$ is the positive semi-definite covariance function and the variance of the function is denoted by $\sigma_i^{(n)}(\textbf{x})$. The superscript $n$ is used to denote the fact that the posterior update accounts for $n$ function evaluations made till now.
For each such GP, the underlying prior is a combination of a constant mean function $\mu_i \in \mathbb{R}$ and the squared exponential function $k_i$
$$k_i(\textbf{x}, \textbf{x}') = \sigma^2\exp\left(-\frac{(\textbf{x}-\textbf{x}')^T(\textbf{x}-\textbf{x}')}{2l_i^2}\right)$$
The kernel matrix $K_i$ is then defined as
$$\textbf{K}_i:=\begin{bmatrix}
k_i(\textbf{x}_1, \textbf{x}_1) & k_i(\textbf{x}_1, \textbf{x}_2) & \dots & k_i(\textbf{x}_1, \textbf{x}_n) \\
\vdots & \vdots & \vdots & \vdots\\
k_i(\textbf{x}_n, \textbf{x}_1) & k_i(\textbf{x}_n, \textbf{x}_2) & \dots & k_i(\textbf{x}_n, \textbf{x}_n)\\
\end{bmatrix}$$
and with abuse of notation define $\textbf{K}=\textbf{K}+\lambda^2I$ (to account for noise in the function evaluations). The posterior distribution on the function $f_i(\textbf{x})$ at any input $\textbf{x}\in\mathcal{X}$~\cite{Rasmussen2005} is given by
$$P(f_i(\textbf{x})|\textbf{x}^{1:n}, f_i^{1:n}) = \mathcal{N}(\mu^{(n)}_i(\textbf{x}), \sigma_i^{(n)}(\textbf{x}) + \lambda^2), ~~ ~ i\in\{1,2,\ldots,M\} \mbox{~where~}$$
\begin{align*}
  \mu^{(n)}_i(\textbf{x}) &= \mu^{(0)} + \textbf{k}_i^T\textbf{K}_i^{-1}(f_i^{1:n} - \mu_i(\textbf{x}^{1:n})) \\ 
  \sigma_i^{(n)}(\textbf{x}) &= k_i(\textbf{x}, \textbf{x})-\textbf{k}_i^T\textbf{K}_i^{-1}\textbf{k}_i \\
  \textbf{k}_i &= \left[k_i(\textbf{x}, \textbf{x}_1) \hspace{5pt}\ldots\hspace{5pt} k_i(\textbf{x}, \textbf{x}_n)\right].
\end{align*}
\begin{algorithm}\label{alg:ei}
    \centering
    \caption{cEI: Composite BO using EI based acquisition function} \label{alg:all_ei}
    \begin{algorithmic}[1]
        \Require $T \xleftarrow{}$ Budget of iterations
        \Require $h(\cdot), f_1(\cdot), \ldots, f_M(\cdot) \xleftarrow{}$ composition and member functions
        \Require $\textbf{X} = \{\textbf{x}_1, \ldots, \textbf{x}_s\} \xleftarrow{} s$ starting points
        \Require $\textbf{F} = \{(f_1(\textbf{x}), \ldots, f_M(\textbf{x}))\}_{\textbf{x} \in \textbf{X}} \xleftarrow{}$  function evaluations at starting points
        \For{$n = s + 1, \ldots, s + T$}
            \For{$i = 1, \ldots, M$}
                \State Fit model $\mathcal{GP}(\mu_i^{(n)}(\cdot), K_i^{(n)}(\cdot, \cdot))$ using evaluations of $f_i$ at points in $\textbf{X}$
            \EndFor
            \State Find new point $\textbf{x}_n$ by optimizing $\text{cEI}(\textbf{x}, L)$ (defined below) 
            \State Get $(f_1(\textbf{x}_n), \ldots, f_M(\textbf{x}_n))$
            \State Augment the data $(f_1(\textbf{x}_n), \ldots, f_M(\textbf{x}_n))$ into $\textbf{F}$ and update $\textbf{X}$ with $\textbf{x}_n$
        \EndFor
        \Function{cEI}{$\textbf{x}, L$}
            \For{$l = 1, \ldots, L$}
                \State Draw $M$ samples $Z_{(l)} \sim \mathcal{N}_M(0_M, I_M)$
                \State Compute $\alpha^{(l)} := \{h(\{\mu^{(n)}_i(\textbf{x}) + \sigma_i^{(n)}(\textbf{x})Z^{(l)}_i\}) - g^*_n\}^+$ 
            \EndFor
            \State \textbf{return} $E_n(\textbf{x})$ = $\frac{1}{L}\Sigma_{l=1}^L\alpha^{(l)}$
        \EndFunction
     \end{algorithmic}
\end{algorithm}
\subsection{cEI and cUCB Acquisition Functions}
For any fixed point $\textbf{x}\in\mathcal{X}$, we use the information about the composition function $h$ to estimate $g$ by first estimating the value of each member function at $\textbf{x}$. However, this is not a straightforward task and needs to be performed in a way similar to the vanilla EI acquisition using Monte Carlo sampling. We propose to use the following acquisition function, that we call as cEI.
\begin{equation}\label{eq:ei_compute}
    E_n(\textbf{x}) = \mathbb{E}_n\left[{h(\{\mu^{(n)}_i(\textbf{x}) + \sigma^{(n)}_i(\textbf{x})Z_i\})} - g_n^*\right]^+  
\end{equation}
where $Z$ is drawn from an $M$-variate normal distribution and $g_n^*$ is the best value observed so far. This acquisition function is similar to EI as we subtract the best observation, $g^*$, so far and only consider negative terms to be 0.  Assuming independent GPs over the functions allows constant time computation of the variance at $\textbf{x}$. However, since each function $f_i$ is being considered an independent variable with mean $\mu_i^{(n)}(\cdot)$ and variance $\sigma_i^{(n)}(\cdot)$, the calculation of $E_n(\boldsymbol{x})$ does not have a closed form and thus, the expectation needs to be evaluated empirically with sampling. Algorithm~\ref{alg:all_ei} provides the complete procedure for doing BO with this acquisition function.

To alleviate this complexity in estimating the acquisition function, we propose a novel UCB-style acquisition function. This function estimates the value of each member function using the GP priors over them and controls the exploration and exploitation factor with the help of the hyperparameter $\lambda_n$: 
\begin{equation}\label{eq:ucb_compute}
   U_n(\textbf{x}) = h(\{\mu^{(n)}_i(\textbf{x}) + \beta_n \sigma^{(n)}_i(\textbf{x})\}) 
\end{equation}
Algorithm~\ref{alg:all_ucb} gives the complete details for using this acquisition function. The user typically starts with a high value for $\beta$ to promote exploration and reduces iteratively to exploit the low reget regions it found. For our experiments, we start with $\beta=1$ and exponentially decay it in each iteration by a factor of 0.99. 
\begin{algorithm}\label{alg:ei}
    \centering
    \caption{cUCB: Composite BO using UCB based acquisition function} \label{alg:all_ucb}
    \begin{algorithmic}[1]
        \Require $T \xleftarrow{}$ Budget of iterations
        \Require $h(\cdot), f_1(\cdot), \ldots, f_M(\cdot) \xleftarrow{}$ composition and member functions
        \Require $\textbf{X} = \{\textbf{x}_1, \ldots, \textbf{x}_s\} \xleftarrow{} s$ starting points
        \Require $\textbf{F} = \{(f_1(\textbf{x}), \ldots, f_M(\textbf{x}))\}_{\textbf{x} \in \textbf{X}} \xleftarrow{}$  function evaluations at starting points
        \Require $\beta\xleftarrow{}$ Exploration factor
        \For{$n = s + 1, \ldots, s + T$}
            \For{$i = 1, \ldots, M$}
                \State Fit model $\mathcal{GP}(\mu_i^{(n)}(\cdot), K_i^{(n)}(\cdot, \cdot))$ using evaluations of $f_i$ at points in $\textbf{X}$
            \EndFor
            \State Find new point $\textbf{x}_n$ suggested by the composition function using Eq.~\ref{eq:ucb_compute}
            \State Get $(f_1(\textbf{x}_n), \ldots, f_M(\textbf{x}_n))$
            \State Augment the data $(f_1(\textbf{x}_n), \ldots, f_M(\textbf{x}_n))$ into $\textbf{F}$ and update $\textbf{X}$ with $\textbf{x}_n$
            \State Update $\beta$
        \EndFor
     \end{algorithmic}
\end{algorithm}
