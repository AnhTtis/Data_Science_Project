
\documentclass{article} % For LaTeX2e

\usepackage[dvipsnames, table]{xcolor}

\usepackage{iclr2024_conference}

\usepackage{times}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{slashbox}
\usepackage{subcaption}
\usepackage{graphicx}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{wrapfig}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\TODO}[1]{{\color{red}#1}}
\newcommand{\makeblue}[1]{{\color{blue}#1}}
\newcommand{\makered}[1]{{\color{red}#1}}
\usepackage{xspace}
  \newcommand{\eg}{e.\,g.\xspace}
  \newcommand{\ie}{i.\,e.\xspace}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{setspace}
\newcommand{\model}{\textsf{FairPol}\xspace}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{pifont}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
%Theorems % Algorithms
\usepackage{amssymb,amsmath,amsthm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}


\title{Fair Off-Policy Learning from Observational Data}


\author{Dennis Frauen, Valentyn Melnychuk \& Stefan Feuerriegel \\
LMU Munich\\
Munich Center for Machine Learning\\
\texttt{\{frauen,melnychuk,feuerriegel\}@lmu.de} \\
}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Algorithmic decision-making in practice must be fair for legal, ethical, and societal reasons. To achieve this, prior research has contributed various approaches that ensure fairness in machine learning predictions, while comparatively little effort has focused on fairness in decision-making, specifically off-policy learning. In this paper, we propose a novel framework for \emph{fair off-policy learning}: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different -- potentially discriminatory -- behavioral policy. For this, we first formalize different fairness notions for off-policy learning. We then propose a neural network-based framework to learn optimal policies under different fairness notions. We further provide theoretical guarantees in the form of generalization bounds for the finite-sample version of our framework. We demonstrate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data. Altogether, our work enables algorithmic decision-making in a wide array of practical applications where fairness must be ensured.  
\end{abstract}

\section{Introduction}

Algorithmic decision-making in practice must avoid discrimination and thus be fair to meet legal, ethical, and societal demands \citep{Nkonde.2019, DeArteaga.2022, CorbettDavies.2023}. For example, in the U.S., the Fair Housing Act and Equal Credit Opportunity Act stipulate that decisions must not be subject to systematic discrimination by gender, race, or other attributes deemed as sensitive. 

However, research from different areas has provided repeated evidence that algorithmic decision-making is often \emph{not} fair. A prominent example is Amazon's tool for automatically screening job applicants that was used between 2014 and 2017 \citep{Dastin.2018}. It was later discovered that the underlying algorithm generated decisions that were subject to systematic discrimination against women and thus resulted in a ceteris paribus lower probability of women being hired. 

Ensuring fairness in off-policy learning is subject to inherent challenges. The reason is that off-policy learning is based on \emph{observational data that observational data may ingrain existing bias from historical decision-making}.\footnote{The term ``bias'' can have different meanings. Here, we use bias can refer to algorithmic bias, where algorithms discriminate against individuals from certain sensitive groups. This is in contrast to the statistical bias of estimators, e.g., due to confounded data.} Hence, one challenge is that the resulting policy must be fair despite the observational data being collected under a different -- potentially discriminatory -- behavioral policy. Furthermore, one may erroneously think that a na{\"i}ve approach to achieving fairness in algorithmic decision-making is to simply omit the sensitive attribute from the observational data. For instance, to avoid bias against women, one would prevent off-policy learning from having access to a variable that stores the gender of an individual. However, in observational data, other variables may act as proxies for gender, and, hence, the learned policy may still lead to discrimination due to the underlying data-generating process \citep{Kilbertus.2017}. Hence, a custom approach for handling sensitive attributes in off-policy learning is needed.

In this paper, we propose a novel framework for \emph{fair off-policy learning from observational data}. Specifically, we learn fair decision rules from observational data where the observational may be collected under a different -- potentially discriminatory -- behavioral policy. To the best of our knowledge, ours is the first neural approach to fair off-policy learning. 
%Detailed motivation of value fairness vs action fairness
 In our off-policy learning framework, fairness may enter at two different stages, namely with respect to both the action and the policy value. 
\begin{enumerate}[leftmargin=7.5mm]
\item \emph{Fairness with respect to the action} ensures that individuals with different sensitive attributes but otherwise equal characteristics receive the same decision. In other words, the choice of action is independent of the sensitive attribute. For example, in credit lending, this means that a woman and a man, each with the same academic subject, have the same chance that their student loan is approved. We later refer to this notion as ``action fairness''. 
\item \emph{Fairness with respect to the policy value} allows us to express fairness in the way that we consider the utility (i.e., the policy value) for each sensitive group. Hence, individuals with different sensitive attributes achieve, on average, a similar utility. For example, this may allow governments to account for the fact that some sub-populations have been historically underrepresented. Hence, as women have a lower propensity than men to pursue academic careers in subjects related to technology, governments may want to strategically incentivize women through student loans so that the long-term benefit for society is maximized. We refer to this notion as ``value fairness''. Later, we introduce two variants of value fairness that build upon envy-free fairness and max-min fairness.  
\end{enumerate}

Our \textbf{contributions}\footnote{Code is available at \url{https://anonymous.4open.science/r/FairPol-402C}.} are three-fold. (1)~We first introduce different fairness notions tailored to our setting, namely fair off-policy learning from observational data. (2)~We then propose a neural framework, called \model, to learn optimal policies under these fairness notions. Specifically, we leverage fair representation learning 
 in combination with custom training objectives so that the resulting policies satisfy our fairness notions.  (3)~We provide theoretical learning guarantees in the form of generalizations bounds for \model. We also evaluate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data.

\section{Related work}
\label{sec:related_work}

We provide an overview on related work on off-policy learning from observational data, both in the standard machine learning and algorithmic fairness literature. For further background on algorithmic fairness and fairness in utility-based decision models (e.g., reinforcement learning), we refer to Appendix~\ref{app:rw}.

\textbf{Off-policy learning:}
Off-policy learning typically aims to determine optimal policies from observational data by maximizing the so-called {policy value} \citep[e.g.,][]{Kallus.2018, Athey.2021}. The policy value is a causal quantity, which can be identified from observational data under certain assumptions (see Section~\ref{sec:problem_setting}). There are three standard methods for estimating the policy value: (1)~The direct method (DM) \citep{Qian.2011, Bennett.2020}; (2)~The inverse propensity score weighted (IPW) method \citep{Kallus.2018}; and (3)~The doubly robust (DR) \citep{Athey.2021, Chernozhukov.2022}. Several works propose extensions of the three standard methods for specific settings, such as unobserved confounding \citep{Kallus.2018c, Bennett.2019b} or distribution shifts \citep{Hatt.2022b, Kallus.2022}, or overlap violations \citep{Kallus.2021b}. Different from our work, none of the above works deals with algorithmic fairness in off-policy learning.

\textbf{Algorithmic fairness for off-policy learning from observational data:}
Recently, some works aimed at learning fair decision rules from observational data. However, existing works have crucial differences from our paper as they make strong assumptions or have other critical restrictions. To this end, our paper is thus the first to integrate algorithmic fairness in off-policy learning for general policies.   

One literature stream rests on the assumption that the structural causal graph of the decision problem is known and then seeks to block specific causal pathways that are deemed unfair \citep{Nabi.2019,Nilforoshan.2022}. However, approaches from this literature stream require \emph{exact knowledge} of the causal structure of the decision problem. That is, the underlying structural causal model of the data-generating process must be a~prior known. In contrast to that, we do \emph{not} make such strong assumption (i.e., exact knowledge of the underlying causal structure) as this is rarely the case in practice.

Closest to our framework is the work by \citet{Viviano.2023}. Therein, the authors study fair off-policy learning but only for {Pareto-optimal} policies but \emph{not} for general policies. As such, the work has two crucial differences from ours. (1)~\citet{Viviano.2023} are restricted to \emph{Pareto-optimal} policies, whereas we study \emph{general} policies. Because of this, both have different learning objectives, which is also why \citet{Viviano.2023} is \emph{not} applicable as a baseline in our work. On the contrary, our decision problem requires both tailored fairness notions and a tailored learning algorithm. (2)~\citet{Viviano.2023} are limited to \emph{linear} policies, whereas we later use a \emph{neural} approach to learn complex, non-linear policies. 

\textbf{Research gap:} In sum, there is -- to the best of our knowledge -- no neural framework for fair off-policy learning of general policies. Hence, there are also no baselines that are applicable later, as there are no previous works that consider our fairness notions for off-policy learning.

\section{Problem setting}\label{sec:problem_setting}

We build upon the standard setting for policy learning from observational data \citep[e.g.,][]{Kallus.2018,Athey.2021}. We consider observational data $(X_i, S_i, A_i, Y_i)_{i=1}^n$ sampled i.i.d. from a data-generating process $(X, S, A, Y) \sim \mathbb{P}$, which consists of covariates $X \in \mathcal{X} \subseteq \R^p$, sensitive attributes $S \in \mathcal{S}$, an action $A \in \{0,1\}$, and an outcome $Y \in \R$.\footnote{In the literature on causal machine learning, actions are oftentimes also called treatments \citep[e.g.,][]{Curth.2021}. Throughout our manuscript, we prefer the term ``action'' as it directly relates to the decision-making literature.} For example, in credit lending, one could model the credit score of an applicant by $X$, the gender or age as a sensitive attribute $S$, a decision $A$ whether to approve or reject the loan, and a profit $Y$ for the lending institution. The causal graph from our setting is shown in Fig.~\ref{fig:causal_graph}. Note that modeling the action $A$ as a binary variable is consistent with previous literature \citep[e.g.,][]{Kallus.2018, Kallus.2018c, Athey.2021, Hatt.2022b} and is common for decision-making in a wide range of practical applications such as, e.g., automated hiring, credit lending, and ad targeting \citep[e.g.,][]{Smith.2023, Yoganarasimhan.2022, Kozodoi.2022}.

\begin{wrapfigure}{r}{0.15\textwidth}
  \centering
\vspace{-0.5cm}
\includegraphics[width=1\linewidth]{figures/causal_graph.pdf}
\caption{Causal graph. We allow for arbitrary dependence between $X$, $S$.}
\label{fig:causal_graph}
\end{wrapfigure}

We make use of the Neyman-Rubin potential outcomes framework \citep{Rubin.1978} and denote $Y(a)$ as the potential outcome, which would have been observed if the action had been set to $A = a$. Formally, a policy is a measurable function $\pi \colon \mathcal{X} \times \mathcal{S} \to [0,1]$, which maps an individual with covariates $(X, S)$ onto a probability of receiving an action. The policy value of $\pi$ is then defined as
\begin{equation}
V(\pi) = \E[Y^\pi] = \E[\pi(X, S) \, Y(1) + (1 - \pi(X, S)) \, Y(0)].
\end{equation}
Note that we cannot directly estimate the policy value because, for each observation, only one of the potential outcomes is observed in the observational data. This is known as the fundamental problem of causal inference \citep{Pearl.2009}. However, we can impose the following standard assumptions in order to identify the policy value $V(\pi)$ from observational data \citep{Rubin.1974}.

\begin{assumption}[Standard causal inference assumptions]\label{ass:identification}
\label{ass:iv}
We assume: (i)~\emph{consistency:} $Y(A) = Y$; (ii)~\emph{positivity:} $0 < \mathbb{P}(A=1 \mid X = x) < 1$ for all $x \in \mathcal{X}$; and (iii)~\emph{strong ignorability:} $Y(0), Y(1) \indep A \mid X$.
\end{assumption}

\noindent
Under Assumption~\ref{ass:identification}, the policy value is identified by $V(\pi) = \E_\mathcal{D}[\psi^\mathrm{m}(\pi, \mathcal{D})]$, with observational data $\mathcal{D} = (X, S, A, Y)$ and where $\psi^\mathrm{m}(\pi, \mathcal{D})$ is one of the following three policy scores:
\begin{align}
\psi^{\mathrm{DM}}(\pi, \mathcal{D}) &= \pi(X, S) \, \mu_1(X, S) + (1 - \pi(X, S)) \, \mu_0(X, S) , \label{eq:pscore}\\
\psi^{\mathrm{IPW}}(\pi, \mathcal{D}) &= \frac{A \, \pi(X, S) + (1 - A) \, (1 - \pi(X, S))}{A \, \pi_b(X, S) + (1 - A) \, (1 - \pi_b(X, S))}Y , \qquad\qquad \text{ and } \label{eq:pscore_ipw}\\
\psi^{\mathrm{DR}}(\pi, \mathcal{D}) &= \psi^{\mathrm{DM}}(\pi, \mathcal{D}) + \frac{A \, \pi(X, S) + (1 - A) \, (1 - \pi(X, S)) }{A \, \pi_b(X, S) + (1 - A) \, (1 - \pi_b(X, S))} \left(Y - \mu_A(X, S)\right),\label{eq:pscore_dr}
\end{align}
which refer to the direct method (DM), the inverse propensity score weighted (IPW) method, and the doubly robust (DR) method, and where $\mu_j(X, S) = \E[Y \mid X, S, A = j], j \in \{0,1\}$, are the outcome regression functions and where $\pi_b(X, S) = \mathbb{P}(A = 1 \mid X, S)$ is the so-called {propensity score} (i.e., behavioral policy).

\textbf{Task:} In standard off-policy learning, the objective is to find a policy from observational data that maximizes the policy value via 
\begin{equation}\label{eq:unfair}
    \pi^{{\mathrm{uf}}} \in \argmax_{\pi \in \Pi} V(\pi),
\end{equation}
where $\Pi$ is some predefined class of policies. For example, $\Pi$ may contain all policies parameterized by some neural network. Any policy that satisfies Eq.~\eqref{eq:unfair} is an \emph{optimal unrestricted policy}, as it does not give any special considerations to the sensitive covariates $S$ when maximizing the policy value. In special cases, the optimal unrestricted policy may coincide with a policy that satisfies the desired fairness notion but, in practice, it will generally not. In many situations, the optimal unrestricted policy will lead to discrimination because of which fairness must be explicitly enforced. 

\section{Fairness notions for off-policy learning}

We now introduce different fairness notions that are tailored to off-policy learning. Specifically, fairness may enter off-policy learning at two different stages, namely with respect to (1)~the action and (2)~the policy value. We refer to them as (1)~\emph{action fairness} and (2)~\emph{value fairness}, respectively. The former, action fairness, prohibits discrimination with respect to the selected action, while the latter, value fairness, prohibits discrimination with respect to the expected utility (i.e., the policy value). Both have inherent advantages in practice, and the actual choice depends on the underlying business goals as well as legal and ethical contexts. 

{\tiny$\blacksquare$}\,\textbf{Action fairness:} The objective in action fairness is that individuals with different sensitive attributes but otherwise equal characteristics receive the same action. For example, in credit lending, this implies that a policy $\pi$ should assign the same action $A = A'$ to two individuals with the same covariates $X = X'$ but different gender $S \neq S'$. In other words, the choice of actions should be independent of the sensitive attribute. We formalize this in the following definition. 
\begin{definition}[Action fairness]
\emph{A policy $\pi^{\mathrm{af}} \in \Pi$ fulfills \emph{action fairness} if it is not a function of $S$ and $\pi^{\mathrm{af}}(X) \indep S$, that is, the recommended action should be independent of the sensitive attribute. A policy $\pi^{\mathrm{af}}$ that fulfills action fairness is optimal if it satisfies $\pi^{\mathrm{af}} \in \argmax_{\pi \in \Pi_{\mathrm{af}}} V(\pi)$, 
where $\Pi_{\mathrm{af}} = \{\pi \in \Pi \,|\, \pi \textrm{ fulfills action fairness}\}$.}
\end{definition}

Action fairness is the equivalent of \emph{demographic parity} for decision-making \citep{Hardt.2016}. It ensures that individuals who only differ with respect to their sensitive attributes (and covariates correlated to them) receive the same decision. 
As such, action fairness is relevant in many applications such as hiring or credit lending where legal frameworks mandate that decisions may not discriminate against certain sensitive attributes \citep{Barocas.2016, Kleinberg.2019}.

{\tiny$\blacksquare$}\,\textbf{Value fairness:} The rationale behind {value fairness} is that different sub-populations defined by the sensitive attribute may benefit differently from a policy. Hence, we now express fairness with respect to the policy value and thus ensure that individuals with different sensitive attributes achieve, on average, a similar utility. Generally, value fairness may be desired in practice where social welfare shall be considered. Examples can be governments that award scholarships, job trainings, or free health insurance with the aim to strategically promote historically underrepresented groups but where the underrepresented groups benefit from such treatments at an overproportionate rate. 

To formalize value fairness, let us denote the conditional policy value $V_s(\pi) = \E[\psi^\mathrm{m}(\pi, \mathcal{D}) \mid S = s]$, where we condition on the sensitive attribute $S = s$. In the following, we introduce two variants of value fairness with different aims: (1)~{envy-free fairness} and (2)~{max-min fairness}. The former, envy-free fairness, ensures that the conditional policy values $V_s(\pi)$, $s \in \{ 0, 1 \}$, do not differ more than some predefined level $\alpha$ between the sub-populations. The latter, max-min fairness, ensures that the worst-case conditional policy value across sub-populations is being maximized.

\vspace{0.2cm}
\begin{definition}[Envy-free fairness]
\emph{A policy $\pi \in \Pi$ fulfills \emph{envy-free fairness} with level $\alpha \geq 0$ if $|V_s(\pi) - V_{s^\prime}(\pi)| \leq \alpha \textrm{ for all } s, s^\prime \in S$. We denote the set of envy-free policies by $\Pi(\alpha) = \{\pi \in \Pi \,|\, \pi \textrm{ is envy free with level } \alpha \}$. An envy-free policy $\pi^\alpha$ is optimal if
$\pi^\alpha \in \argmax_{\pi \in \Pi(\alpha)} V(\pi)$.}
\end{definition} 

\begin{definition}[Max-min fairness]
\emph{A policy $\pi^{\mathrm{mm}} \in \Pi$ fulfills \emph{max-min fairness} if it minimizes the worst-case policy value for the sensitive attributes, that is, $\pi^{\mathrm{mm}} \in \argmax_{\pi \in \Pi} \inf_{s \in S} V_s(\pi)$.}
\end{definition}

The above definitions of value fairness are inspired by previous literature on resource allocation \citep[e.g.,][]{Arnsperger.1994, Bertsimas.2011}, and we here adopt them here to off-policy learning, that is, learning from observational data. Envy-free fairness allows decision-makers to control for disparities in the utility between the sensitive groups by fixing $\alpha$. Max-min fairness seeks the best possible worst-case policy value.

{\tiny$\blacksquare$}\,\textbf{Combining action fairness and value fairness:} Both action fairness and value fairness can be combined in off-policy learning so that the obtained policies fulfill both notions simultaneously. To this end, one simply replaces the policy class $\Pi$ with $\Pi_{\mathrm{af}}$. This thus restricts the policy class to all policies that fulfill action fairness, and, as a result, one obtains policies that fulfill both notions. 

Combining action fairness and value fairness has also theoretical implications, which we discuss in the following. In fact, it turns out that the notion of max-min fairness only yields a useful fairness notion when it is used in combination with action fairness. We show this in the following Lemma~\ref{lem:maxmin_unfair}.

\begin{lemma}\label{lem:maxmin_unfair}
Let $\Pi$ the set of all measurable policies $\pi \colon \mathcal{X} \times \mathcal{S} \to [0,1]$. Then, there exists a policy that fulfills max-min fairness, i.e., $\pi^\ast \in \argmax_{\pi \in \Pi} \inf_{s \in S} V_s(\pi)$ which is also an optimal unrestricted policy (i.e., a solution to Eq.~(\ref{eq:unfair})).
\end{lemma}

We now turn to the relationship between envy-free fairness and max-min fairness when combined with action fairness. As it turns out, under action fairness and some further conditions, max-min fairness can be seen as a special case of envy-free fairness with $\alpha = 0$. This is stated in Lemma~\ref{lem:envy_maxmin}. 

\vspace{0.2cm}
\begin{lemma}\label{lem:envy_maxmin}
Let $\mathrm{ITE}(x, s^\ast) = \mu_1(x, s^\ast) - \mu_0(x, s^\ast)$ denote the individual treatment effect for an individual with covariates $(x, s^\ast)$. We further assume that $\mathcal{S} = \{0, 1\}$ is binary, and let $(\pi^{\mathrm{mm}}, s^\ast)$ be a solution to $\max_{\pi \in \Pi} \inf_{s \in S} V_s(\pi)$, and let $\pi^{\mathrm{mm}}(x)$ fulfill action fairness. Furthermore, we assume that there exists a set of covariates $V \subseteq \mathcal{X}$ with $\mathbb{P}(X \in V \mid S = s) > 0$ such that: either $\mathrm{ITE}(x, s^\ast) > 0$ and $\pi^{\mathrm{mm}}(x) < 1$; or $\mathrm{ITE}(x, s^\ast) < 0$ and $\pi^{\mathrm{mm}}(x) > 0$ for all $x \in V$, $s \in \mathcal{S}$. Then, $\pi^{\mathrm{mm}}$ fulfills envy-free fairness with $\alpha=0$. Further, all optimal policies that both satisfy action fairness and envy-free fairness with $\alpha = 0$ also fulfill max-min fairness.
\end{lemma}

We provide an additional discussion of the assumptions from Lemma~\ref{lem:envy_maxmin} in Appendix~\ref{app:assumptions}. Furthermore, we provide a toy example to discuss our fairness notions in Appenidx~\ref{app:toy_example}.





\section{Neural framework for off-policy learning}\label{sec:framework}

We propose our neural framework, called \model, which learns optimal action and/ or value fair policies in two steps (see Fig.~\ref{fig:model}). In \textbf{Step~1}, we ensure action fairness by restricting the underlying policy class $\Pi$ to a subset of policies $\Pi_\mathrm{af} \subseteq \Pi$ (Sec.~\ref{sec:step1}). In \textbf{Step~2}, we ensure value fairness by changing the underlying learning objective (Sec.~\ref{sec:step2}). We provide theoretical results in Sec.~\ref{sec:theory}. 

\subsection{Step 1: Fair representation learning for action fairness} 
\label{sec:step1}

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.4cm}
  \centering
\includegraphics[width=1\linewidth]{figures/model_architecture.pdf}
\caption{Overview of \model which provides an instantiation of our framework with neural networks.}
\label{fig:model}
\vspace{-0.2cm}
\end{wrapfigure}
To obtain $\Pi_\mathrm{af}$, we build upon the idea of fair representation learning \citep[e.g.,][]{Zemel.2013, Madras.2018} but adapt it to our task of fair off-policy learning. We first learn a fair representation $\Phi \colon \mathcal{X} \to \R^k$ of the data so that $\Phi(X) \indep S$, but where $\Phi(X)$ is still predictive of the outcome $Y$. This ensures that any policy based on $\Phi(X)$ satisfies action fairness but is still effective in achieving a large policy value. In our implementation, we parameterize $\Phi$ by neural networks that are trained with two adversarial objectives. As a result, $\Phi$ essentially yields a policy class that is restricted to all policies with action fairness, that is, $\Pi^\Phi_\mathrm{af} = \{\pi_\theta \circ \Phi \mid \theta \in \Theta\}$.

We use three feed-forward neural networks to learn the representation $\Phi$: (1)~a base representation network $\Phi_{\theta_\Phi}$ that takes the non-sensitive attributes $X$ as input and outputs the representation; (2)~an outcome prediction network $G^Y_{\theta_Y}$ that predicts the outcome $Y$ based on the representation $\Phi$; and (3)~a sensitive attribute network $G^S_{\theta_S}$ that predicts the sensitive attribute $S$ based on the representation. Here $\theta_\Phi$, $\theta_Y$, and $\theta_S$ denote the neural network parameters. The base representation network $\Phi_{\theta_\Phi}$ serves as basis to construct the fair representation, while $G^Y_{\theta_Y}$ and $G^S_{\theta_S}$ allow us to ensure predictiveness of $Y$ and non-predictiveness of $S$.

We proceed as follows to find the optimal parameters $\hat{\theta}_\Phi$, $\hat{\theta}_Y$, and $\hat{\theta}_S$. We optimize an objective consisting of three parts: 
(1)~The outcome loss $\mathcal{L}_Y$ ensures that to our representation $\Phi$ and the outcome prediction network are predictive of the outcome $Y$. For this, we minimize 
$\mathcal{L}_Y(\theta_\Phi, \theta_Y) = \frac{1}{n} \sum_{i=1}^n \left(G^Y_{\theta_Y}\left(\Phi_{\theta_\Phi}(X_i)\right) - Y_i \right)^2$.
(2)~The sensitivity loss $\mathcal{L}_S$ learns the parameters of the sensitive attribute network, i.e., $G^S_{\theta_S}$, so that it is predictive of $S$. We thus minimize $\mathcal{L}_S(\theta_\Phi, \theta_S) = \frac{1}{n} \sum_{i=1}^n\mathrm{CE}\left(G^S_{\theta_S}\left(\Phi_{\theta_\Phi}(X_i)\right), S_i\right)$,
where $\mathrm{CE}$ denotes the categorical cross-entropy loss. 
(3)~The confusion loss  $\mathcal{L_\mathrm{conf}}$, guided by the sensitive attribute network, aims to render the representation $\Phi$ non-predictive of $S$. We thus minimize $\mathcal{L_\mathrm{conf}}(\theta_\Phi, \theta_S) = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^{|\mathcal{S}|} - \frac{1}{|\mathcal{S}|}\log \left(G^S_{\theta_S}\left[\Phi_{\theta_\Phi}(X_i)\right]^j\right)$,
where $[\cdot]^j$ is the $j$-th element of a vector. 

Both the sensitivity loss and the confusion loss are adversarial to each other. This is crucial for the following reasons: the sensitive attribute network $G^S_{\theta_S}$ is trained to correctly classify the sensitive attribute by minimizing $\mathcal{L}_S(\theta_\Phi, \theta_S)$ with respect to $\theta_S$, while the base representation network $\Phi_{\theta_\Phi}$ tries to ``confuse'' the sensitive attribute network by minimizing $\mathcal{L_\mathrm{conf}}(\theta_\Phi, \theta_S)$ with respect to $\theta_\Phi$, i.e., forcing the sensitive attribute network to predict a uniform distribution of the sensitive attribute. This ensures that the learned representation becomes non-predictive of the sensitive attribute $S$. Taken together, the overall objective is
\begin{equation}\label{eq:loss_overall}
    \hat{\theta}_\Phi, \hat{\theta}_Y = \argmin_{\theta_\Phi, \theta_Y}  \mathcal{L}_Y(\theta_\Phi, \theta_Y) + \gamma \mathcal{L_\mathrm{conf}}(\theta_\Phi, \hat{\theta}_S); \qquad \qquad \hat{\theta}_S = \argmin_{\theta_S}\gamma \mathcal{L}_S(\hat{\theta}_\Phi, \theta_S);
\end{equation}
where $\gamma$ is a parameter that weights the different parts in the loss function. The objective in Eq.~\eqref{eq:loss_overall} is also known as counterfactual domain confusion loss \citep{Melnychuk.2022}. We later train the two adversarial objectives from Eq.~\eqref{eq:loss_overall} via an iterative gradient-based solver. For further details on our learning algorithm, we refer to Appendix~\ref{app:algorithm}.

\subsection{Step 2: Learning objectives for value fairness}
\label{sec:step2}
We now specify the learning objectives in our framework and how these vary according to the different notions of value fairness. To do so, we first propose model-agnostic objectives and then describe how we incorporate these into Step~2 of \model.

\textbf{Model-agnostic objectives}: In expectation, the policy value is defined as $V(\pi) = \E_\mathcal{D}[\psi^\mathrm{m}(\pi, \mathcal{D})]$, where $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$. Further, the conditional policy value is defined as $V_s(\pi) = \E[\psi^\mathrm{m}(\pi, \mathcal{D}) \mid S = s] = \E[\psi^\mathrm{m}(\pi, \mathcal{D}) \frac{\mathbbm{1}(S = s)}{\mathbb{P}(S = s)}]$, where $\mathbbm{1}(\cdot)$ denotes the indicator function. Hence, we can estimate these quantities by replacing the expectations with finite sample averages. Then, the empirical policy value becomes
$\hat{V}^\mathrm{m}(\pi) = \frac{1}{n} \sum_{i=1}^n \psi^\mathrm{m}(\pi, \mathcal{D}_i)$, ,
and the empirical conditional policy value becomes
$\hat{V}^\mathrm{m}_s(\pi) = \frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{\hat{p}_n(s)} \, \psi^\mathrm{m}(\pi, \mathcal{D}_i) \quad \textrm{with} \quad \hat{p}_n(s) = \frac{\sum_{j=1}^n \mathbbm{1}(S_j = s)}{n}$. 
The optimal unconstrained policy can be obtained via
$\hat{\pi} \in \argmax_{\pi \in \Pi} \hat{V}^\mathrm{m}(\pi)$.

We now state the learning objectives for (1)~envy-free fairness and (2)~max-min fairness:
(1)~For envy-free fairness, we reformulate the optimization problem over the class of envy-free policies into an optimization problem over an unconstrained policy class. We further replace the population quantities $V(\pi)$ and $V_s(\pi)$ with their corresponding estimates $\hat{V}^\mathrm{m}(\pi)$ and $\hat{V}^\mathrm{m}_s(\pi)$. We thus yield
\begin{equation}\label{eq:envyfree_est}
    \hat{\pi}^\lambda \in \argmax_{\pi \in \Pi} \hat{V}^\mathrm{m}_\lambda(\pi) \quad \textrm{with} \quad \hat{V}^\mathrm{m}_\lambda(\pi) = \hat{V}^\mathrm{m}(\pi)  - \lambda \max_{s, s\prime \in \mathcal{S}} |\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi)|,
\end{equation}
where $\lambda > 0$ is a hyperparameter controlling envy-free fairness and where larger values correspond to more fair policies.
(2)~For max-min fairness, we proceed analogously and obtain
\begin{equation}\label{eq:maxmin_est}
    \hat{\pi}^{\mathrm{mm}} \in \argmax_{\pi \in \Pi} \min_{s \in S} \hat{V}^\mathrm{m}_s(\pi).
\end{equation}

\textbf{Incorporating value fairness in \model:}
The second step of \model is to optimize the empirical policy value. Here, we optimize against the previously introduced learning objectives. Depending on whether action fairness is enforced, we optimize the learning objectives over all policies in $\Pi$ or the subset $\Pi^\Phi_\mathrm{af}$ of policies with action fairness.  Hence, once the representation $\hat{\Phi} = \Phi_{\hat{\theta}_\Phi}$ is trained, we optimize our objectives for value fairness over the policy class $\Pi^{\hat{\Phi}}_\mathrm{af} = \{\pi_\theta \circ \hat{\Phi} \mid \theta \in \Theta\}$. Here, we parametrize $\pi_\theta$ by a neural network with parameters $\theta \in \Theta$ that takes the representation $\hat{\Phi}(X)$ as input and outputs a policy recommendation $\pi_\theta(\hat{\Phi}(X)) \in [0,1]$. Formally, we thus optimize the policy via
$\max_{\theta \in \Theta} \hat{V}^\mathrm{m}(\pi_\theta), \qquad
\max_{\theta \in \Theta} \hat{V}^\mathrm{m}_\lambda(\pi_\theta), \qquad \textrm{or} \qquad \max_{\theta \in \Theta} \min_{s \in S} \hat{V}^\mathrm{m}_s(\pi_\theta)$, depending on whether there is no value fairness, envy-free fairness, or max-min fairness, respectively.\footnote{Note that the policies that fulfill no value fairness are either the optimal unrestricted policies or the policies that fulfill action fairness.} 

\textbf{Implementation details:}
In our \model implementation, we use feed-forward neural networks with dropout and exponential linear unit activation functions for the base representation network, the outcome prediction network, and the sensitive attribute network. We use Adam \citep{Kingma.2015} for the optimization in both Steps 1 and 2. We further follow best practices for hyperparameter tuning. We first split the data into a training and validation set, and we then perform hyperparameter tuning using a grid search. All evaluations are based on the test set so that we capture the out-of-sample performance on unseen data.
Additional details for our framework are in Appendix~\ref{app:hyperparam}. 

\subsection{Generalization bounds}
\label{sec:theory}

In the following, we derive generalization bounds for the finite-sample version of our framework under a standard boundedness assumption (see Appendix~\ref{app:proofs}). There, we quantify the deviation of the proposed finite-sample policy estimators from their respective population quantities. 
Note that the derivations also hold for action fairness where one would simply need to replace $\Pi$ by $\Pi_{\mathrm{af}}$. 

\vspace{0.2cm}
\begin{theorem}[Generalization bounds]\label{thrm:bounds} Let $p(s) = \mathbb{P}(S = s) \geq \nu$ for all $s \in \mathcal{S}$ and some $\nu > 0$. Let $p, p_1, p_2 > 0$ and let $K_\mathrm{m}$ denote a constant that depends on the estimation method $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$. Assume that, for $\ell(n, p_2) = 1 - \nu + \sqrt{\log\left(|\mathcal{S}|/{p_2}\right)/2}$, it holds that $\frac{1}{\sqrt{n}}\ell(n, p_2) < \nu$. Then, the following three statements hold: (i) With probability at least $1 - p$ it holds that
\begin{equation}\label{eq:bound_uf}
\resizebox{.4\hsize}{!}{$V(\pi) \geq \hat{V}^\mathrm{m}(\pi) - 2 C K_\mathrm{m} \left(R_n(\Pi) + \sqrt{\frac{8 \log\left(\frac{2}{p}\right)}{n}}\right)$}
\end{equation}
for all $\pi \in \Pi$. (ii)~With probability at least $1 - p_1 - p_2$, we have
\begin{equation}\label{eq:bound_envy_free}
\resizebox{.6\hsize}{!}{$V_\lambda(\pi) \geq \hat{V}^\mathrm{m}_\lambda(\pi) - 2 C K_\mathrm{m} \frac{2+\nu}{\nu}\left( R_n(\Pi) +\sqrt{\frac{8 \log\left(\frac{4 |\mathcal{S}|}{p_1}\right)}{n}} + \frac{2}{(2+\nu) \sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right)$}
\end{equation}
for all $\pi \in \Pi$. (iii)~With probability at least $1-p_1 - p_2$ it holds that \begin{equation}\label{eq:bound_max_min}
\resizebox{.8\hsize}{!}{
    $\min_{s \in \mathcal{S}} V_s(\pi) \geq \min_{s \in \mathcal{S}} \hat{V}^\mathrm{m}_s(\pi) - \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\left(\frac{2 |\mathcal{S}|}{p_1}\right)}{n}} + \frac{1}{\sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right)$}
\end{equation}
for all $\pi \in \Pi$.
\end{theorem}
\vspace{-0.3cm}
Theorem~\ref{thrm:bounds} shows that, with sufficient sample size, the oracle policy objectives $\hat{V}^\mathrm{m}(\pi)$, $\hat{V}^\mathrm{m}_\lambda(\pi)$, and $\min_{s \in \mathcal{S}} V_s(\pi)$ are with high probability lower bounded than their empirical counterpart if the policy class $\Pi$ has as vanishing Rademacher complexity $R_n(\Pi)$. In this paper, we consider policy classes parametrized by neural networks, which  have a $\sqrt{n}$-vanishing Rademacher complexity $R_n(\Pi) = \mathcal{O}\left(n^{-1/2}\right)$. We also observe that the bounds for value fairness in Theorem~\ref{thrm:bounds} depend on the number $|\mathcal{S}|$ of possible values the sensitive attribute can attain, which is one of the main differences to the unrestricted bound.

\section{Experiments}
\label{sec:experiments}

We follow common procedures in causal machine learning \citep[e.g.,][]{Shalit.2017, Curth.2021} and perform experiments using both simulated and real-world data. Note that there are no suitable baselines for our setting apart from na{\"i}ve off-policy learning without fairness.

\subsection{Experiments using simulated data}
\vspace{-0.3cm}
\textbf{Experimental setup:}  
We generate a simulated dataset with $n=3000$ observations inspired by a credit-lending problem (see Appendix\ref{app:data_sim} for details). Throughout our experiments, we estimate the policies using the data from a training set (80\%) and evaluate the policies using the data from a test set (20\%) to compare the out-of-sample performance.
We perform all evaluations using the following performance metrics: (1)~We report the policy value $\hat{V}^\mathrm{m}(\pi)$. This thus corresponds to the objective function in off-policy learning that is maximized under the fairness constraints (thus: larger values are better). (2)~We additionally report the policy value by different sub-populations given by the conditional policy value $\hat{V}^\mathrm{m}_s(\pi)$ for $s=0$ and $s=1$. We provide the results for our framework across all three different policy scores, namely $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$ from Eq.~\eqref{eq:pscore}, Eq.~\eqref{eq:pscore_ipw}, and Eq.~\eqref{eq:pscore_dr}, respectively. Of note, we cannot compare our framework against other methods since suitable baselines that can deal with fair off-policy learning over general policies are missing.

\begin{wraptable}{r}{0.7\textwidth}
\vspace{-0.4cm}
\caption{Results for simulated data.}
\label{tab:results_sim_af}
\centering
\footnotesize
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
{Approach} & \multicolumn{3}{c}{Policy value} & {Action fairness}\\
\cmidrule(lr){2-4}
& {Overall} & {$S = 0$} & {$S = 1$} \\
\midrule
\textsc{Baselines} & & & \\
\quad Optimal unrestricted policy &$1.24 \pm 0.03$& $0.74 \pm 0.03$ & $1.46 \pm 0.06$ & $2.42 \pm 0.20 $ \\
\quad Oracle action fairness &$1.03 \pm 0.02$& $0.01 \pm 0.07$ & $1.46 \pm 0.06$ & $0.00 \pm 0.00 $\\
\midrule
\textsc{Our \model (only action fairness)} & & & \\
\quad \model with $\mathrm{m} = \mathrm{DM}$ &$1.02 \pm 0.02$ & $0.01 \pm 0.06$ &$1.45 \pm 0.06$ & $0.21 \pm 0.05$ \\
\quad \model with $\mathrm{m} = \mathrm{IPW}$ &$ 1.01 \pm 0.03$ & $0.02 \pm 0.05$ &$1.43 \pm 0.07$ & $0.24 \pm 0.06$ \\
\quad \model with $\mathrm{m} = \mathrm{DR}$ &$1.01 \pm 0.03$ & $0.02 \pm 0.05$ &$1.43 \pm 0.07$ & $0.23 \pm 0.05$\\
\textsc{Our \model with envy-free fairness} & & & \\
\quad \model with $\mathrm{m} = \mathrm{DM}$ &$0.87 \pm 0.17$ &$0.61 \pm 0.07$ & $0.99 \pm 0.24$ & $0.38 \pm 0.22$\\
\quad \model with $\mathrm{m} = \mathrm{IPW}$ &$0.87 \pm 0.05$ &$0.32 \pm 0.17$ & $1.11 \pm 0.14$ & $0.79 \pm 0.30 $ \\
\quad \model with $\mathrm{m} = \mathrm{DR}$ &$0.86 \pm 0.06$ &$0.34 \pm 0.17$ & $1.09 \pm 0.15$ & $0.75 \pm 0.32$\\
\midrule
\textsc{Our \model with max-min fairness } & & & \\
\quad \model with $\mathrm{m} = \mathrm{DM}$ &$0.73 \pm 0.03$ &$0.73 \pm 0.03$ & $0.73 \pm 0.03$ & $0.00 \pm 0.00$\\
\quad \model with $\mathrm{m} = \mathrm{IPW}$ &$0.73 \pm 0.03$ &$0.73 \pm 0.03$ & $0.73 \pm 0.03$& $0.00 \pm 0.01$ \\
\quad \model with $\mathrm{m} = \mathrm{DR}$ &$0.73 \pm 0.03$ &$0.73 \pm 0.03$ & $0.73 \pm 0.03$ & $0.00 \pm 0.01$\\
\bottomrule
\multicolumn{4}{l}{Reported: mean $\pm$ standard deviation ($\times 10$) on test set over 5 runs.}
\end{tabular}}
\vspace{-0.4cm}
\end{wraptable}

\textbf{Results for action fairness:}
We now examine whether our framework is effective in learning policies that fulfill action fairness. (1)~We first report an optimal unrestricted policy that has access to the ground-truth outcome functions from the data-generating process and acts as the maximum achievable policy value for comparison. (2)~We further estimate an oracle policy that fulfills action fairness with access to the ground-truth outcome functions. It should be regarded as an upper bound for the policy value that can be achieved under action fairness. (3)~We compare our \model for action fairness, setting $\gamma = 0.5$. We report three different variants of our \model by varying the underlying policies scores $\mathrm{m}$, namely DM, IPW, and DR.

The results are in Table~\ref{tab:results_sim_af}. Besides policy values, we also report the performance in terms of action fairness, which we calculate via $\E[\pi(X_u, X_{s=1}, S=1) - \pi(X_{u}, X_{s=0}, S=0)]$. We make the following observations. First, the optimal unrestricted policy has the largest policy value but fails to achieve action fairness, as expected. Second, the policy value for the oracle policy with action fairness is lower, and, by definition, the action fairness achieves a score of zero. Third, we find that our \model is effective in achieving action fairness.
Fourth, we find that our \model attains a policy value that is close to the upper bound given by the oracle policy with action fairness, which corroborates the effectiveness of our framework. Finally, we find that our \model achieves a similar performance regardless of the underlying policy score (i.e., DM, IPW, and DR) and thus appears robust with respect to the choice of policy score.

\textbf{Results for value fairness:}
We further assess whether our framework is effective in learning policies that fulfill value fairness. Here, we report results from our framework with action fairness for three different fairness notions: (1)~no value fairness, (2)~envy-free fairness ($\lambda = 0.5$), and (3)~max-min fairness. We set $\gamma = 0.5$ and provide a sensitivity analysis for the parameter later. 

\begin{wrapfigure}{l}{0.6\textwidth}
  \centering
  \vspace{-0.5cm}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_envy_free_lambda.pdf}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_envy_free_gamma.pdf}
\end{subfigure}
    \vspace{-0.4cm}
\caption{Sensitivity analysis for the envy-free parameter $\lambda$ (left) and the regularization parameter $\gamma$ (right).}
\label{fig:parameter}
    \vspace{-0.4cm}
\end{wrapfigure}
The results are in Table~\ref{tab:results_sim_af}. We arrive at the following conclusions. First, the optimal unrestricted policy has the largest overall policy value, as expected. Second, our \model for envy-free fairness achieves a smaller overall policy due to the fairness constraints. 
Third, our \model for max-min fairness is effective in achieving the desired fairness notion. It achieves a larger worst-case policy value compared to the optimal unrestricted policy and a lower difference between groups.
In summary, the experimental results confirm the effectiveness of our empirical framework in enforcing the proposed fairness notions.

We also examine the sensitivity to the envy-freeness parameter $\lambda$. We compare the policy value from our \model for different values of $\lambda \in [0, 2]$. The results are shown in Fig.~\ref{fig:parameter} (left). As expected, the policy value decreases and the difference between the policy values for the two sub-groups $S \in \{0,1\}$ becomes smaller for larger $\lambda$. Furthermore, the results remain robust for different choices of $\gamma$ (Fig.~\ref{fig:parameter}, right). 

\subsection{Experiments using real-world data}

\begin{wraptable}{r}{0.7\textwidth}
\vspace{-0.5cm}
\caption{Results for real-world data.}
\label{t:results_real_af}
\centering
\footnotesize
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
{Approach} & \multicolumn{3}{c}{Policy value} & {Action fairness}\\
\cmidrule(lr){2-4}
& {Overall} & {$S = \mathrm{male}$} & {$S = \mathrm{female}$} \\
\midrule
{Optimal unrestricted policy} & $0.137 \pm 0.005$ & $0.101 \pm 0.008$ &  $0.165 \pm 0.003$ & $0.129 \pm 0.007$ \\
{\model (only action fairness)} & $0.130 \pm 0.004$ & $0.093 \pm 0.006$ & $0.160 \pm 0.003$ & $0.015 \pm 0.001$ \\
\model with {envy-free fairness} & $0.130 \pm 0.004$ & $0.093 \pm 0.006$ & $0.160 \pm 0.003$ & $0.067 \pm 0.007$ \\
\model with {max-min fairness} & $0.131 \pm 0.008$ & $0.100 \pm 0.011$ & $0.157 \pm 0.008$& $0.057 \pm 0.010$ \\
\bottomrule
\multicolumn{5}{l}{Reported: mean $\pm$ standard deviation on test set over 5 random runs}
\end{tabular}}
\vspace{-0.3cm}
\end{wraptable}
\textbf{Experimental setup:}
We now demonstrate the applicability of our framework to real-world, medical data. 
We use medical data from the Oregon health insurance experiment \citep{Finkelstein.2012}. The Oregon health insurance experiment took place in 2008. As part of it, around 30,000 low-income, uninsured adults in Oregon were offered free health insurance through Medicaid. We use our framework to learn fair policies that assign Medicaid to minimize the total costs for medical care of an individual, while avoiding discrimination with respect to gender. 
Besides gender, we include five additional variables as possible confounders. Details are Appendix~\ref{app:data}. \model is based on $\gamma = 0.5$ (for action fairness) and the double robust method $\mathrm{m} = \mathrm{DR}$. For envy-free fairness, we set $\lambda = 0.3$. Here, we do not know the ground-truth outcomes for real-world data, and, hence, we estimate the policy values on the test data. To quantify action fairness, we report Spearman's rank correlation coefficient between the sensitive attribute (gender) and the policy predictions on the test data.

\begin{wrapfigure}{l}{0.6\textwidth}
\vspace{-0.5cm}
  \centering
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_pred_age.pdf}
\end{subfigure}%
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_pred_visits.pdf}
\end{subfigure}
\vspace{-0.5cm}
\caption{Comparison of the estimated policies averaged over 20 random runs. Visualized are the policy predictions (i.e., the outputs of the respective policies).}
\label{fig:pred}
\vspace{-0.5cm}
\end{wrapfigure}

\textbf{Results for action and value fairness:}
($V^\textrm{m}_{\mathrm{female}}(\pi)$).   
The results are shown in Table~\ref{t:results_real_af}. Again, the optimal unrestricted policy has the largest empirical policy value but does not satisfy action fairness. In contrast to that, we find that our \model is again effective in achieving action fairness. This comes at the cost of a slightly lower overall policy value. As expected, both \model with envy-free fairness and with max-min fairness have a lower policy value than the optimal unrestricted policy. However, the max-min policy is effective in both increasing the worst-case policy value and decreasing the difference in policy values across both sub-populations. This demonstrates the effectiveness of our \model to obtain a policy with the best possible worst-case policy value on real-world data.

\textbf{Insights:}
We now examine the outputs of the respective policies. We the averaged outputs over (i)~age and (ii)~the number of previous emergency department visits of an individual (Fig.~\ref{fig:pred}). 
Both policies tend to recommend Medicaid for the majority of individuals. Furthermore, both policy outputs are lower for individuals with a smaller number of emergency department visits. This is reasonable as such individuals may be less at risk of accumulating medical debt compared to individuals with an extensive medical history. \model with max-min fairness outputs slightly lower predictions for older individuals and for individuals with no or few emergency visits. Hence, there seem to exist some male individuals with few emergency visits or higher age for which free health insurance has only little positive effect. 

\textbf{Conclusion:} In this paper, we proposed a novel framework for fair off-policy learning from observational data. Our framework is widely applicable to algorithmic decision-making in practice where fairness must be ensured. We refer to Appendix~\ref{app:discussion} for a detailed discussion.


\clearpage

\bibliography{bibliography.bib}
\bibliographystyle{iclr2024_conference}

\clearpage

\appendix

\section{Extended related work}\label{app:rw}

\subsection{Algorithmic fairness for machine learning predictions} 

Extensive work has developed algorithmic fairness for machine learning predictions, which refers to computational approaches that enforce certain constraints on predictions so that similarly-situated individuals also receive similar predictions. In the following, we provide a brief overview of the different concepts and fairness notions. We refer to \citet{Chouldechova.2020} and \citet{Mitchell.2021} for a detailed overview. We emphasize that the following fairness notions are developed for predictions and \emph{not} for off-policy learning.

A major literature branch deals with fairness notions that prevent systematic differences in predictions across different groups that are defined by some sensitive attributes (e.g., gender or race) \citep[e.g.,][]{Dwork.2012,Hardt.2016, Madras.2018,CorbettDavies.2023}.This can be achieved, for example, by enforcing independence between the sensitive attribute and the predictions (i.e., statistical parity) or ensuring a similar classification performance for the different sensitive groups (e.g., similar type-I/II error rates). Approaches for group-level fairness have been extended to specific settings, such as for data with unobserved sensitive attributes \citep{Kallus.2019b} and for censored training data \citep{Kallus.2018f}. Beyond group-level fairness, there are also notions at the individual level as well as notions that are based on a causal lens (called causal fairness); see \citet{Chouldechova.2020}. Note that, even though off-policy learning itself is a causal problem, our setting later is different from the literature on causal fairness: the standard literature on causal fairness uses causal theory (e.g., structural causal models) to define fairness notions \citep[e.g.,][]{Kilbertus.2017, Kusner.2017, Nabi.2018}, while we introduce fairness to a specific causal decision problem (off-policy learning).

A popular approach to achieve group-level fairness in practice is to remove the algorithmic bias incorporated in the training data by producing a new, fair representation of the data \citep[e.g.,][]{Zemel.2013, Locatello.2019}. For this, one typically uses neural networks that learn such a fair representation, and, then, one uses the fair representation as input to the actual prediction model. For instance, statistical parity can be achieved by producing a new representation of the data that is non-predictive of the sensitive attributes using probabilistic models \citep{Creager.2019} or adversarial learning methods \citep{Madras.2018}.

\subsection{Algorithmic fairness for utility-based decision models}

A different literature stream has developed fairness notions that account for the utility of individuals who are subject to decisions. Such fairness notions have been integrated into traditional decision problems and thus outside of machine learning. Examples are, for instance, resource allocation \citep{Bertsimas.2011,Bertsimas.2013, Rea.2021b} and pricing \citep{Kallus.2021c, Cohen.2022}. Here, a common fairness notion is envy-free fairness, which is fulfilled if an individual receives an allocation that has the same (or a higher) utility as the allocation of any other individual. Hence, decisions are envy-free if all players receive a share of resources that is equally good from their perspective \citep{Arnsperger.1994}. Another fairness notion is max-min fairness, which is grounded in Rawlsian justice and which seeks to maximize the minimum utility that a player can obtain \citep{Bertsimas.2011}. However, to the best of our knowledge, the aforementioned notions -- envy-free fairness and max-min fairness -- have only been used for traditional resource allocations and have not yet been adapted to off-policy learning from observational data, which is one of our contributions later.      

Prior literature also considered algorithmic fairness in specialized settings. Examples are ranking tasks such as from recommender systems \citep[e.g.,][]{Singh.2019c} or risk-averse approaches to bound worst-case outcomes \citep[e.g.,][]{Fang.2022}. Even others consider algorithmic fairness in reinforcement learning. Here, fair policies can be obtained by customizing the reward function \citep{Jabbari.2017, Jiang.2019, Yu.2022} or by optimizing social welfare functions \citep{Siddique.2020, Zimmer.2021}. However, these works focus on Markov decision processes (MDPs), whereas we focus on learning policies in non-sequential settings that are not restricted to MDPs. 

\clearpage

\section{Proofs}\label{app:proofs}

\subsection{Proof of Lemma~\ref{lem:maxmin_unfair}}

\begin{proof}
For each sensitive attribute $s \in \mathcal{S}$, we construct $\pi^\ast(\cdot, s) \in \argmax_{\pi \in \Pi_{\mathcal{X}}} V_s(\pi)$, where $\Pi_{\mathcal{X}} = \{\pi \colon \mathcal{X} \to [0,1] \mid \pi \textrm{ measurable} \} $. By definition, it holds that $V_s(\pi) \leq V_s(\pi^\ast)$ for any policy $\pi \in \Pi$ and, hence, $\inf_{s \in S} V_s(\pi) \leq \inf_{s \in S} V_s(\pi^\ast)$, which means that $\pi^\ast$ is a policy fulfilling max-min fairness. At the same time, due to $V_s(\pi) \leq V_s(\pi^\ast)$, it holds that
\begin{equation}
    V(\pi) = \int_{\mathcal{S}} V_s(\pi) \, \mathbb{P}(S = s) \,  \diff s \leq \int_{\mathcal{S}} V_s(\pi^\ast) \, \mathbb{P}(S = s) \diff s = V(\pi^\ast).
\end{equation}
Thus, the policy $\pi^\ast$ is an optimal unrestricted policy. 
\end{proof}

\subsection{Proof of Lemma~\ref{lem:envy_maxmin}}

\begin{proof}
We first show that $V_0(\pi^{\mathrm{mm}}) = V_{1}(\pi^{\mathrm{mm}})$, i.e., $\pi^{\mathrm{mm}}$ is envy-free with $\alpha=0$. Let us assume w.l.o.g. that $V_0(\pi^{\mathrm{mm}}) < V_{1}(\pi^{\mathrm{mm}})$. By our assumption, we can find an $\epsilon > 0$ such the policy $\pi^\prime$ defined by
\begin{equation}
   \pi^\prime(x) = \left\{
\begin{array}{ll}
\pi^{\mathrm{mm}}(x) + \epsilon \, \mathrm{sign}\{\mathrm{ITE}(x, 0)\} , \qquad & \quad \textrm{if } x \in V, \\
\pi^{\mathrm{mm}}(x) , & \quad \textrm{if } x \in \mathcal{X} \setminus V, \\
\end{array}
\right.
\end{equation}
satisfies $V_{1}(\pi^\prime) > V_0(\pi^{\mathrm{mm}})$. By construction of $\pi^\prime$ and our assumption, we yield
\begin{align}
V_{0}(\pi^\prime) &= \int_\mathcal{X} \, \pi^\prime(x) \, \mathrm{ITE}(x, 0) \, \mathbb{P}(x \mid S=0) + \mu_0(x, 0) \, \mathbb{P}(x \mid S=0) \, \diff x \\ &> \int_\mathcal{X} \pi^{\mathrm{mm}}(x) \, \mathrm{ITE}(x, 0) \, \mathbb{P}(x \mid S=0) + \mu_0(x, 0) \, \mathbb{P}(x \mid S=0) \,  \diff x = V_0(\pi^{\mathrm{mm}}). 
\end{align}
This implies
\begin{equation}
    \min\{V_{0}(\pi^\prime), V_{1}(\pi^\prime)\} > \min\{V_{0}(\pi^{\mathrm{mm}}), V_{1}(\pi^{\mathrm{mm}})\},
\end{equation}
which is a contradiction to the assumption that $\pi^{\mathrm{mm}}$ fulfills max-min fairness. Hence, $\pi^{\mathrm{mm}}$ fulfills envy-free fairness.

Now, let $\pi^{0}$ be an optimal policy that satisfies both action fairness and envy-free fairness. Let us further assume that $\pi^{0}$ does not fulfill max-min fairness. We then yield
\begin{equation}
    V(\pi^{0}) = \mathbb{P}(S = 0) \, V_0(\pi^{0}) + \mathbb{P}(S = 1) \, V_1(\pi^{0}) < \mathbb{P}(S = 0) \, V_0(\pi^{\mathrm{mm}}) + \mathbb{P}(S = 1) \, V_1(\pi^{\mathrm{mm}}) = V(\pi^{\mathrm{mm}}),
\end{equation}
which is a contradiction because $\pi^{\mathrm{mm}}$ fulfills envy-free fairness and $\pi^{0}$ is optimal. 
\end{proof}

\subsection{Proof of generalization bounds}

As a first step, we impose the following boundedness assumptions.

\begin{assumption}[Boundedness]\label{ass:boundedness}
We assume there exist constants $C, \xi, \nu > 0$ such that (i)~the outcomes are bounded with $|Y| \leq C$ almost surely, (ii)~the propensity score is bounded away from $0$ and $1$, i.e., $\mathbb{P}(\xi \leq \pi_b(X, S) \leq 1 - \xi) = 1$, and (iii)~$S$ has full support on $\mathcal{S}$, i.e., $p(s) = \mathbb{P}(S = s) \geq \nu$ for all $s \in \mathcal{S}$ and some $\nu > 0$.
\end{assumption}
\vspace{0.2cm}

Assumption~\ref{ass:boundedness} is standard in the off-policy learning literature \citep{Kallus.2018} and allows to derive theoretical guarantees. In particular, generalization bounds have been derived in settings with unobserved confounding \citep{Kallus.2018c} and distribution shifts \citep{Hatt.2022b}. In the following, we leverage techniques from \citep{Kallus.2018c}. Our bounds depend on the Rademacher complexity $R_n(\Pi) = \sum_{\epsilon \in \{-1, +1\}^n} \sup_{\pi \in \Pi} \left| \frac{1}{n} \sum_{i=1}^n \epsilon_i \pi(X_i)\right|$, which is a measure to characterize the complexity of the policy class $\Pi$.

In this section, we provide proof of our generalization bounds, namely Theorem~\ref{thrm:bounds}. In our proof, we later leverage ideas from Theorem~1 in \cite{Kallus.2018}; however, adapting these to our setting is not straightforward, and several additional arguments must be made. To this end, we begin with three auxiliary lemmas.

\vspace{0.3cm}
\begin{lemma}\label{lem:bounded_diff}
Let $T^\mathrm{m}(s, \mathcal{D}) = \sup_{\pi \in \Pi} |\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}(\pi, \mathcal{D}_i) - V_s(\pi)|$. Then, $T^\mathrm{m}(s, \cdot)$ satisfies the bounded difference inequality with $\frac{4C}{n p(s)} K_\mathrm{m}$, where $K_\mathrm{m}$ is a constant depending on $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$.
\end{lemma}

\begin{proof}
It holds that
\begin{align}
    &\left|T^\mathrm{m}(s, \mathcal{D}) - T^\mathrm{m}(s, \mathcal{D}^\prime)\right| \\ 
    \overset{(1)}{\leq} & \sup_{\pi \in \Pi} \left| \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}(\pi, \mathcal{D}_i) - V_s(\pi) \right| -  \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i^\prime = s)}{p(s)} \psi^\mathrm{m}(\pi, \mathcal{D}_i^\prime) - V_s(\pi) \right|\right|\\
    \overset{(2)}{\leq} & \frac{1}{n p(s)} \sup_{\pi \in \Pi} \left|\sum_{i=1}^n \mathbbm{1}(S_i = s) \psi^\mathrm{m}(\pi, \mathcal{D}_i) - \mathbbm{1}(S_i^\prime = s) \psi^\mathrm{m}(\pi, \mathcal{D}_i^\prime) \right| \\
    = & \frac{1}{n p(s)} \sup_{\pi \in \Pi} \left( \left| \psi^\mathrm{m}(\pi, \mathcal{D}_j)\right| + \left| \psi^\mathrm{m}(\pi, \mathcal{D}_j^\prime)\right| \right),
\end{align}
where (1) follows from a property of the supremum and (2) follows from the reverse triangle inequality.
It remains to bound $\left|\psi^\mathrm{m}(\pi, \cdot)\right|$ for $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$. For $\mathrm{m} = \mathrm{DM}$, we get
\begin{equation}
\left| \psi^{\mathrm{DM}}(\pi, \mathcal{D}_j)\right| \leq |\mu_1(X, S)| + |\mu_0(X, S)| \leq 2 C .
\end{equation}
For $\mathrm{m} = \mathrm{IPW}$, we get
\begin{equation}
\left| \psi^{\mathrm{IPW}}(\pi, \mathcal{D}_j)\right| \leq \frac{|Y|}{|A \pi_b(X, S) + (1 - A) (1 - \pi_b(X, S))|}\leq \frac{C}{\eta} .
\end{equation}
Finally, for $\mathrm{m} = \mathrm{DR}$, we get
\begin{equation}
\left| \psi^{\mathrm{DR}}(\pi, \mathcal{D}_j)\right| \leq \left| \psi^{\mathrm{DM}}(\pi, \mathcal{D}_j)\right| +  \frac{|Y - \mu_A(X, S)|}{|A \pi_b(X, S) + (1 - A) (1 - \pi_b(X, S))|}\leq 2C \left(\frac{\eta + 1}{\eta} \right).
\end{equation}
Therefore, we arrive at
\begin{equation}
    \left|T^\mathrm{m}(s, \mathcal{D}) - T^\mathrm{m}(s, \mathcal{D}^\prime)\right| \leq \frac{4C}{n p(s)} K_\mathrm{m},
\end{equation}
with $K_{\mathrm{DM}} = 1$, $K_{\mathrm{IPW}} = \frac{1}{2 \eta}$, and $K_{\mathrm{DR}} = \frac{\eta + 1}{\eta}$.
\end{proof}

\vspace{0.3cm}
\begin{lemma} \label{lem:t_bound}
With probability of at least $1 - p$, it holds that
\begin{equation}
T^\mathrm{m}(s, \mathcal{D}) \leq \frac{2 C K_\mathrm{m}}{p(s)} \left(R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p}}{n}}\right).
\end{equation}
\end{lemma}

\begin{proof}
Lemma~\ref{lem:bounded_diff} allows us to apply McDiarmid's inequality, resulting in
\begin{equation}
    \mathbb{P}\left(T^\mathrm{m}(s, \mathcal{D}) - \E\left[ T^\mathrm{m}(s, \mathcal{D})\right] \geq \epsilon  \right) \leq \exp\left(- \frac{n p(s)^2 \epsilon^2}{8C^2 K_\mathrm{m}^2} \right).
\end{equation}
Equivalently, with probability of at least $1 - p_1$, it holds that
\begin{equation}
T^\mathrm{m}(s, \mathcal{D}) \leq \E\left[ T^\mathrm{m}(s, \mathcal{D})\right] + \frac{2 C K_\mathrm{m}}{p(s)} \sqrt{\frac{2 \log\frac{1}{p_1}}{n}}.
\end{equation}
By a standard symmetrization argument, we yield
\begin{equation}
\E\left[ T^\mathrm{m}(s, \mathcal{D})\right] \leq \E\left[ \frac{1}{2^n} \sum_{\epsilon \in \{-1,1\}^n} \sup_{\pi \in \Pi} \left|\frac{1}{n} \sum_{i=1}^n \epsilon_i \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}((\pi, \mathcal{D}_i) \right|\right] \leq \frac{2C K_\mathrm{m}}{p(s)} \E\left[R_n(\Pi)\right].
\end{equation}
Here, the Rademacher complexity $R_n(\Pi)$ satisfies the bounded differences with $\frac{2}{n}$, and we thus obtain
\begin{equation}
    \mathbb{P}\left(R_n(\Pi) - \E\left[R_n(\Pi)\right] \leq - \epsilon \right) \leq \exp\left(- \frac{n \epsilon^2}{2}\right) .
\end{equation}
This implies that, with probability of at least $1 - p_2$, it holds that
\begin{equation}
\E\left[R_n(\Pi)\right] \leq  R_n(\Pi) + \sqrt{\frac{2 \log\frac{1}{p_2}}{n}}.
\end{equation}
By setting $p_1 = p_2 = \frac{p}{2}$ and applying the union bound, we yield
\begin{equation}
T^\mathrm{m}(s, \mathcal{D}) \leq \frac{2 C K_\mathrm{m}}{p(s)} \left(R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p}}{n}}\right)
\end{equation}
with probability of at least $1-p$.
\end{proof}

\noindent
In the next step, we use Lemma~\ref{lem:t_bound} to derive a bound on the absolute estimation error $\left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|$ that holds uniformly over all policies and sensitive attributes. This is stated in Lemma~\ref{lem:v_bound_uniform}.

\vspace{0.3cm}
\begin{lemma} \label{lem:v_bound_uniform}
Let $ \ell(n, p_2) = 1 - \nu + \sqrt{\frac{\log\frac{|\mathcal{S}|}{p_2}}{2}}$. Let us further assume that $\frac{\ell(n, p_2)}{\sqrt{n}} < \nu$. Then, with probability of at least $1-p_1 - p_2$, it holds that
\begin{equation}
    \sup_{\pi \in \Pi} \sup_{s \in \mathcal{S}} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right| \leq \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\frac{2 |\mathcal{S}|}{p_1}}{n}} + \frac{1}{\sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right).
\end{equation}
\end{lemma}

\begin{proof}
We yield
\begin{align}
& \quad \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right| \\
    & = \sup_{\pi \in \Pi} \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{\hat{p}_n(s)} \psi^\mathrm{m}((\pi, \mathcal{D}_i) - V_s(\pi)\right| \\
    & \leq \left|\frac{1}{\hat{p}_n(s)} - \frac{1}{p(s)} \right| \sup_{\pi \in \Pi} \left| \sum_{i=1}^n \mathbbm{1}(S_i = s)\psi^\mathrm{m}((\pi, \mathcal{D}_i) \right| + \sup_{\pi \in \Pi} \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}((\pi, \mathcal{D}_i) - V_s(\pi)\right| \\
    & \leq \frac{2 C K_\mathrm{m}}{p(s)} \frac{\left|\hat{p}_n(s) - p(s) \right|}{\hat{p}_n(s)} +  T^\mathrm{m}(s, \mathcal{D}).
\end{align}
The absolute difference $\left|\hat{p}_n(s) - p(s) \right|$ satisfies bounded differences with constant $\frac{1}{n}$ because
\begin{equation}
  \left|\left|\hat{p}_n(s) - p(s) \right| - \left|\hat{p}^\prime_n(s) - p(s) \right| \right|  \leq \left|\hat{p}_n(s) - \hat{p}^\prime_n(s) \right| \leq \frac{\left|\mathbbm{1}(S_j = s) - \mathbbm{1}(S^\prime_i = s) \right|}{n} \leq \frac{1}{n}.
\end{equation}
Hence, McDiamid's inequality implies 
\begin{equation}
\mathbb{P}\left(\left|\hat{p}_n(s) - p(s) \right| - \E\left[\left|\hat{p}_n(s) - p(s) \right| \right] \geq \epsilon \right) \leq \exp\left(-2n\epsilon^2\right).
\end{equation}
Thus, with probability at least $1-p_2$ it holds that
\begin{align}
\left|\hat{p}_n(s) - p(s) \right| &\leq \E\left[\left|\hat{p}_n(s) - p(s) \right| \right] + \sqrt{\frac{\log\frac{1}{p_2}}{2n}} \\
& \overset{(1)}{\leq} \frac{1}{n} \sqrt{\E\left[\left(n\hat{p}_n(s) - np(s) \right)^2 \right]} + \sqrt{\frac{\log\frac{1}{p_2}}{2n}} \\
& \overset{(2)}{=} \sqrt{\frac{p(s)\left(1 - p(s)\right)}{n}} + \sqrt{\frac{\log\frac{1}{p_2}}{2n}} = \ell(n, p_2, s) \\
& \leq \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right),
\end{align}
where (1) follows by applying Jensen's inequality and (2) by noting that $n \hat{p}_n(s) \sim \mathrm{Binomial}(n, p(s))$ with expectation $n p(s)$ and standard deviation $\sqrt{n p(s) (1 - p(s))}$.

The above also implies that, with probability of at least $1-p_2$, we obtain
\begin{equation}
\hat{p}_n(s) \geq p(s) - \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right) \geq \nu - \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right) > 0,
\end{equation}
whenever $\frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right) < \nu$. Putting everything together via the union bound, we obtain with probability of at least $1 - p_1 - p_2$ that
\begin{equation}
\sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|
\leq \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p_1}}{n}} + \frac{1}{\sqrt{n}} \left( \frac{1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}}{\nu - \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right)} \right)\right).
\end{equation}
The result follows by applying the union bound over all $s\in \mathcal{S}$.
\end{proof}

In the following, we use Lemma~\ref{lem:v_bound_uniform} to prove the generalization bounds. Specifically, we provide the proofs for the envy-free generalization bound from Eq.~\eqref{eq:bound_envy_free}, the max-min generalization bound from Eq.~\eqref{eq:bound_max_min}, and the unrestricted generalization bound from Eq.~\eqref{eq:bound_uf}.

\textbf{Proof of Eq.~\eqref{eq:bound_envy_free}:}

\begin{proof}
It follows that
\begin{align}
& \quad \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}} \left|\hat{V}^\mathrm{m}(\pi) - \lambda \big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big|   - V(\pi) + \lambda \big|V_s(\pi) - V_{s^\prime}(\pi) \big| \right| \\
& \leq \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi) \right| + \lambda \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}}\left|\big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big| - \big|V_s(\pi) - V_{s^\prime}(\pi) \big| \right| \\
& \leq \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi) \right| + 2 \lambda \sup_{\pi \in \Pi} \sup_{s \in \mathcal{S}} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|.
\end{align}

Hence, with probability of at least $1 - p_1 - p_2$, we yield
\begin{align}
& \quad \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}} \left|\hat{V}^\mathrm{m}(\pi) - \lambda \big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big|   - V(\pi) + \lambda \big|V_s(\pi) - V_{s^\prime}(\pi) \big| \right| \\
&\leq 2 C K_\mathrm{m} \frac{2+\nu}{\nu}\left( R_n(\Pi) +\sqrt{\frac{8 \log\frac{4 |\mathcal{S}|}{p_1}}{n}} + \frac{2}{(2+\nu) \sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right)
\end{align}
using Lemma~\ref{lem:v_bound_uniform}. The theorem follows from
\begin{equation}
    \hat{V}^\mathrm{m}_\lambda(\pi) \leq V_\lambda(\pi) + \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}} \left|\hat{V}^\mathrm{m}(\pi) - \lambda \big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big|   - V(\pi) + \lambda \left|V_s(\pi) - V_{s^\prime}(\pi) \right| \right|.
\end{equation}
\end{proof}


\textbf{Proof of Eq.~\eqref{eq:bound_max_min}:}

\begin{proof}
The triangle inequality implies that
\begin{equation}
    \hat{V}^\mathrm{m}_s(\pi) \leq V_s(\pi) + \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|.
\end{equation}

Hence, Lemma~\ref{lem:v_bound_uniform} yields with probability of at least $1-p_1 - p_2$ for all $s \in \mathcal{S}$, $\pi \in \Pi$:
\begin{equation}
    V_s(\pi) \geq \hat{V}^\mathrm{m}_s(\pi) - \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\frac{2 |\mathcal{S}|}{p_1}}{n}} + \frac{1}{\sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right).
\end{equation}
The result follows by applying the minimum over $s$ on both sides.
\end{proof}

\textbf{Proof of Eq.~\eqref{eq:bound_uf}:}

\begin{proof}
It follows that
\begin{equation}
    \hat{V}^\mathrm{m}(\pi) \leq V(\pi) + \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi)\right|.
\end{equation}
With the same proof as in Lemma~\ref{lem:v_bound_uniform}, we can show, with probability of at least $1-p$, that
\begin{equation}
\sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi) \right| \leq 2 C K_\mathrm{m} \left(R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p}}{n}}\right).
\end{equation}
\end{proof}

\clearpage

\section{Toy example to differentiate fairness notions}
\label{app:toy_example}

In the following, we provide a toy example based on which we discuss the differences between the above fairness notions. For this, we consider algorithmic decision-making in credit lending where applications for student loans are evaluated. We consider two covariates for students, namely their gender and average grade (GPA) given by $\mathit{Gender} \in \{\textrm{Female}, \textrm{Male}\}$ and $\mathit{GPA} \in \{\textrm{Low}, \textrm{High}  \}$. We consider $\mathit{Gender}$ as the sensitive attribute $S$. The outcome $Y$ is the expected change in salary, that is, whether it increases ($=1$), remains the same ($=0$), or decreases ($=-1$) as a result of the study program. For the purpose of our toy example, we make further assumptions regarding the distribution of covariates and expected outcomes. To this end, Table~\ref{t:toy_ex1} reports the probability of observing an individual from each sub-population (column~3), the outcome when a student receives the loan ($\mu_1$), and the outcome when a student does not receive the loan ($\mu_0$). Then, the overall effect of the action (i.e., the student loan) is given by $\mu_1 - \mu_0$. As can be seen, the action of receiving a loan benefits males with a high GPA while it has a negative effect for all other sub-groups.  

In Table~\ref{t:toy_ex1}, we report the policy value under different decision policies. (Details for calculating the policy values in our toy example are in Appendix~\ref{app:toy_example}). First, we report the optimal unrestricted policy ($\pi^{\mathrm{u}}$). This policy gives student loans only to males with high GPA but not to any other student. The reason is that the sub-population of males with high GPA is the only one with a positive effect (i.e., $\mu_1 - \mu_0 = 1$). Second, we report an optimal policy under action fairness ($\pi^{\mathrm{af}}$). It chooses the same action for both males and females with high (low) GPA. Hence, the action taken by $\pi^{\mathrm{af}}$ does not depend on gender and thus fulfills action fairness. Third, envy-free fairness ($\pi^\alpha$) and max-min fairness ($\pi^{\mathrm{mm}}$) assign loans only to males with a high GPA. In particular, the max-min policy coincides with the optimal unrestricted policy, as implied by Lemma~\ref{lem:maxmin_unfair}.

We further consider policies for envy-free fairness and max-min fairness that are combined with action fairness, so that always both action fairness and value fairness are satisfied (columns 10 and 11). Here, the policies assign actions to males and females with high GPA in order to fulfill action fairness. In addition, both policies assign actions only to a fraction of the overall population. This is seen by the fact that the policy outputs are $\frac{\alpha+1}{3}$ and $\frac{1}{3}$, respectively, and thus below 1. We further note that some of the policies can coincide as stipulated in Lemma~\ref{lem:envy_maxmin}. For $\alpha = 0$, the policy combining action fairness and envy-free fairness is identical to the policy combing action fairness and max-min fairness. For $\alpha = 2$, the policy combining action fairness and envy-free fairness is identical to the policy for action fairness ($\pi^{\mathrm{af}}$).

\begin{table}[ht]
\caption{Toy example comparing the different fairness notions for off-policy learning.}
\centering
\label{t:toy_ex1}
\footnotesize
\resizebox{1\textwidth}{!}{
\begin{tabular}{P{0.8cm} P{0.8cm} P{1.5cm} P{1.3cm} P{1.3cm} P{0.6cm} P{0.6cm} P{0.6cm} P{0.6cm} P{1.3cm} P{1.3cm}}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\multicolumn{3}{c}{\textbf{Data}}& \multicolumn{2}{c}{\textbf{Expected outcome }} & \multicolumn{4}{c}{\textbf{Policies}} & \multicolumn{2}{c}{\textbf{Combined policies}} \\
& & & & & & & & & \multicolumn{2}{c}{(with action fairness)} \\
\cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
Gender & GPA & Probability & $\mu_1$ & $\mu_0$ & $\pi^{\mathrm{u}}$ & $\pi^{\mathrm{af}}$ & $\pi^{\mathrm{\alpha}}$ & $\pi^{\mathrm{mm}}$ &  $\pi^{\alpha}$ & $\pi^{\mathrm{mm}}$\\
\midrule
Female & Low& $0.1$ & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
Male& Low & $0.4$& $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
Female& High & $0.1$ & $-1$& $1$ & $0$ & $1$ & $0$ &$0$ & $\frac{\alpha + 1}{3}$& $\frac{1}{3}$ \\
Male & High & $0.4$& $1$ & $0$ & $1$ & $1$ & $1$ &$1$ &$\frac{\alpha + 1}{3}$ &$\frac{1}{3}$\\
\bottomrule
\multicolumn{11}{l}{\tiny \emph{Legend}: $\pi^{\mathrm{u}}$: optimal unrestricted; $\pi^{\mathrm{af}}$: action fairness; $\pi^{\alpha}$: envy-free fairness; $\pi^{\mathrm{mm}}$: max-min fairness}
\end{tabular}}
\end{table}


\textbf{Derivations:} We denote the levels of gender with $\mathrm{F}$ (female) and $\mathrm{M}$ (male), and the levels of GPA with $\mathrm{L}$ (low) and $\mathrm{H}$ (high). We first calculate the conditional policy value $V_\mathrm{F}$ for females
\begin{align}
    V_\mathrm{F}(\pi) & = \pi(\mathrm{F}, \mathrm{L}) \mu_1(\mathrm{F}, \mathrm{L}) + (1 - \pi(\mathrm{F}, \mathrm{L})) \mu_0(\mathrm{F}, \mathrm{L}) + \pi(\mathrm{F}, \mathrm{H}) \mu_1(\mathrm{F}, \mathrm{H}) + (1 - \pi(\mathrm{F}, \mathrm{H})) \mu_0(\mathrm{F}, \mathrm{H}) \\
    & = (1 - \pi(\mathrm{F}, \mathrm{L})) - \pi(\mathrm{F}, \mathrm{H}) + (1 - \pi(\mathrm{F}, \mathrm{H})) \\
    &= 2 - \pi(\mathrm{F}, \mathrm{L}) - 2 \pi(\mathrm{F}, \mathrm{H})
\end{align}
and $V_\mathrm{M}$ for males
\begin{align}
    V_\mathrm{M}(\pi) & = \pi(\mathrm{M}, \mathrm{L}) \mu_1(\mathrm{M}, \mathrm{L}) + (1 - \pi(\mathrm{M}, \mathrm{L})) \mu_0(\mathrm{M}, \mathrm{L}) + \pi(\mathrm{M}, \mathrm{H}) \mu_1(\mathrm{M}, \mathrm{H}) + (1 - \pi(\mathrm{M}, \mathrm{H})) \mu_0(\mathrm{M}, \mathrm{H}) \\
    & = 1 - \pi(\mathrm{M}, \mathrm{L}) + \pi(\mathrm{M}, \mathrm{H}).
\end{align}
The overall policy value is
\begin{align}
    V(\pi) & = 0.2 V_\mathrm{F}(\pi) + 0.8 V_M(\pi) \\
    &= 1.2 + 0.8\pi(\mathrm{M}, \mathrm{H}) -0.8 \pi(\mathrm{M}, \mathrm{L}) - 0.2\pi(\mathrm{F}, \mathrm{L}) - 0.4 \pi(\mathrm{F}, \mathrm{H}).
\end{align}
Hence, the optimal unrestricted policy is $\pi^\mathrm{u}(\mathrm{M}, \mathrm{H}) = 1$, $\pi^\mathrm{u}(\mathrm{M}, \mathrm{L}) = 0$, $\pi^\mathrm{u}(\mathrm{F}, \mathrm{H}) = 0$, and $\pi^\mathrm{u}(\mathrm{F}, \mathrm{L}) = 0$. 

The difference of the conditional policy value is
\begin{align}
    \Delta(\pi) = |V_\mathrm{F}(\pi) - V_\mathrm{M}(\pi)| & = |1 - \pi(\mathrm{F}, \mathrm{L}) - 2 \pi(\mathrm{F}, \mathrm{H}) - \pi(\mathrm{M}, \mathrm{H}) + \pi(\mathrm{M}, \mathrm{L})|.
\end{align}
It holds that $\Delta(\pi^\mathrm{u}) = 0$ which implies that the policy $\pi^\alpha$ with $\alpha$-envy-free fairness coincides with $\pi^\mathrm{u}$.

For the optimal policy $\pi^\mathrm{af}$ with action fairness, the policy value simplifies to
\begin{align}
    V(\pi^\mathrm{af})  &= 1.2 + 0.8\pi^\mathrm{af}(\mathrm{H}) -0.8 \pi^\mathrm{af}(\mathrm{L}) - 0.2\pi^\mathrm{af}(\mathrm{L}) - 0.4 \pi^\mathrm{af}(\mathrm{H}) \\
    &= 1.2 + 0.4\pi^\mathrm{af}(\mathrm{H}) - \pi^\mathrm{af}(\mathrm{L}),
\end{align}
which means that $\pi^\mathrm{af}(\mathrm{L}) = 0$ and $\pi^\mathrm{af}(\mathrm{H}) = 1$.
For the policy $\pi^{\mathrm{af} + \alpha}$ with both action fairness and envy-free fairness, we obtain
\begin{align}
    \Delta(\pi^{\mathrm{af} + \alpha}) = |1 - 3 \pi^{\mathrm{af} + \alpha}(\mathrm{H})| \leq \alpha,
\end{align}
which yields $\pi^{\mathrm{af} + \alpha}(\mathrm{L}) = 0$ and $\pi^{\mathrm{af} + \alpha}(\mathrm{H}) = \frac{\alpha + 1}{3}$. Finally, the policy $\pi^\mathrm{mm}$ with max-min fairness maximizes
\begin{align}
    \min\{V_\mathrm{F}(\pi^\mathrm{mm}), V_\mathrm{M}(\pi^\mathrm{mm})\} = \min\{2 - \pi^\mathrm{mm}(\mathrm{L}) - 2\pi^\mathrm{mm}(\mathrm{H}), \, 1 - \pi^\mathrm{mm}(\mathrm{L}) + \pi^\mathrm{mm}(\mathrm{H}) \},
\end{align}
which implies $\pi^\mathrm{mm}(\mathrm{L}) = 0$ and $\pi^\mathrm{mm}(\mathrm{H}) = 1/3$.

\clearpage

\section{Discussion of the assumptions in Lemma~\ref{lem:envy_maxmin}}
\label{app:assumptions}

In this section, we provide additional details regarding the assumptions in Lemma~\ref{lem:envy_maxmin}. In essence, Lemma~\ref{lem:envy_maxmin} holds if the max-min solution $(\pi^{\mathrm{mm}}, s^\ast)$ outputs stochastic actions in an area $V$ of the covariate space where the policy value could be improved by choosing a deterministic action. In the toy example from the previous section, this is the case, and, hence, the max-min and the envy-free policy with $\alpha = 0$ coincide. In the following, we study a second toy example where $\pi^{\mathrm{mm}}$ does not coincide with any envy-free policy $\pi^{\mathrm{af} + \alpha}$.

\subsection{Toy example}

The same data from Table~\ref{t:toy_ex1} is shown in Table~\ref{t:toy_ex2} with different ITEs. Now, the action benefits all groups, but males have larger expected outcomes than females. Furthermore, old males receive a larger benefit from the action than all other groups. Hence, even though the action benefits all groups, the difference in policy values for males and females will increase by giving performing action on old male patients. The max-min policy $\pi^{\mathrm{mm}}$ simply recommends action to everyone, as it aims to maximize the policy value $V_{\textrm{Female}}(\pi^{\mathrm{mm}})$ for females (worst-case). In contrast, the $\pi^{\mathrm{af} + \alpha}$ restricts action on older patients in order to decrease the disparity of conditional policy values $V_{\textrm{Female}}(\pi^{\mathrm{af} + \alpha})$ and $V_{\textrm{Male}}(\pi^{\mathrm{af} + \alpha})$ (envy-free).

\begin{table}[ht]
\caption{Toy example comparing the different fairness notions for off-policy learning.}
\centering
\label{t:toy_ex2}
\footnotesize
\resizebox{1\textwidth}{!}{
\begin{tabular}{P{0.8cm} P{0.8cm} P{1.5cm} P{1.3cm} P{1.3cm} P{0.6cm} P{0.6cm} P{0.6cm} P{0.6cm} P{1.3cm} P{1.3cm}}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\multicolumn{3}{c}{\textbf{Data}}& \multicolumn{2}{c}{\textbf{Expected outcome }} & \multicolumn{4}{c}{\textbf{Policies}} & \multicolumn{2}{c}{\textbf{Combined policies}} \\
& & & & & & & & & \multicolumn{2}{c}{(with action fairness)} \\
\cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
Gender & GPA & Probability & $\mu_1$ & $\mu_0$ & $\pi^{\mathrm{u}}$ & $\pi^{\mathrm{af}}$ & $\pi^{\mathrm{\alpha}}$ & $\pi^{\mathrm{mm}}$ &  $\pi^{\alpha}$ & $\pi^{\mathrm{mm}}$\\
\midrule
Female & Low& $0.1$ & $0$ & $-1$ & $1$ & $1$ & $1$ & $1$ & $1$ & $1$ \\
Male& Low & $0.4$& $1$ & $0$ & $1$ & $1$ & $\frac{\alpha}{3}$ & $1$ & $1$ & $1$ \\
Female& High & $0.1$ & $0$& $-1$ & $1$ & $1$ & $1$ &$1$ & $\alpha - 2$& 1 \\
Male & High & $0.4$& $2$ & $0$ & $1$ & $1$ & $\frac{\alpha}{3}$ &$1$ &$\alpha - 2$ &1\\
\bottomrule
\multicolumn{11}{l}{\tiny \emph{Legend}: $\pi^{\mathrm{u}}$: optimal unrestricted; $\pi^{\mathrm{af}}$: action fairness; $\pi^{\alpha}$: envy-free fairness; $\pi^{\mathrm{mm}}$: max-min fairness}
\end{tabular}}
\end{table}

\subsection{Derivation of toy example}

We proceed as in the example from our main paper (see Appendix~\ref{app:toy_example} for details) and calculate the conditional policy values
\begin{align}
    V_\mathrm{F}(\pi) & = -(1 - \pi(\mathrm{F}, \mathrm{L})) - (1 - \pi(\mathrm{F}, \mathrm{H})) \\
    &=  \pi(\mathrm{F}, \mathrm{L}) + \pi(\mathrm{F}, \mathrm{H}) - 2
\end{align}
and
\begin{align}
    V_\mathrm{M}(\pi) & = \pi(\mathrm{M}, \mathrm{L})  + 2 \pi(\mathrm{M}, \mathrm{H}).
\end{align}
The overall policy value is
\begin{align}
    V(\pi) & = 0.2 V_\mathrm{F}(\pi) + 0.8 V_\mathrm{M}(\pi) \\
    &= 1.6\pi(\mathrm{M}, \mathrm{H}) +0.8 \pi(\mathrm{M}, \mathrm{L}) + 0.2\pi(\mathrm{F}, \mathrm{L}) +0.2 \pi(\mathrm{F}, \mathrm{H}) - 0.4
\end{align}

We immediately obtain the optimal unrestricted policy is $\pi^\mathrm{u} \equiv \pi^\mathrm{af} \equiv \pi^\mathrm{mm} \equiv 1$ because all policy terms are positive. To obtain policies $\pi^{\mathrm{af} + \alpha}$ and $\pi^{\alpha}$ with envy-free fairness, we write the constraints as
\begin{align}\label{eq:app:policy_contraint1}
    \Delta(\pi^\alpha) = |2 + \pi^\alpha(\mathrm{M}, \mathrm{L})  + 2 \pi^\alpha(\mathrm{M}, \mathrm{H}) - \pi^\alpha(\mathrm{F}, \mathrm{L}) - \pi^\alpha(\mathrm{F}, \mathrm{H})| \leq \alpha
\end{align}
and 
\begin{align}\label{eq:app:policy_contraint2}
    \Delta(\pi^{\mathrm{af} + \alpha}) = 2 + \pi^{\mathrm{af} + \alpha}(\mathrm{H}) \leq \alpha,
\end{align}
where $\pi^{\mathrm{af} + \alpha}$ again denotes the policy that fulfills both action fairness and envy-free fairness. Eq.~\eqref{eq:app:policy_contraint2} implies $\pi^{\mathrm{af} + \alpha}(\mathrm{H}) = \alpha - 2$ (for $\alpha \geq 2$) and $\pi^{\mathrm{af} + \alpha}(\mathrm{L}) = 1$. Eq.~\eqref{eq:app:policy_contraint1} yields a linear constrained optimization problem with solution $\pi^\alpha(\mathrm{F}, \mathrm{L}) = \pi^\alpha(\mathrm{F}, \mathrm{H}) = 1$ and $\pi^\alpha(\mathrm{M}, \mathrm{L}) = \pi^\alpha(\mathrm{M}, \mathrm{H}) = \alpha / 3$.

\clearpage

\section{Learning algorithm for \model}
\label{app:algorithm}

Algorithm~\ref{alg:fair-policy} provides the learning algorithm for \model. The algorithm consists of two consecutive steps, namely the fair representation learning step and the policy learning step. For the policy learning step, the specification of the policy loss is needed, namely one of no value, envy-free fairness, and max-min fairness: $\mathcal{L}_\pi^{\mathrm{m}}(\cdot) \in \{\hat{V}^\mathrm{m}(\cdot), \hat{V}^\mathrm{m}_\lambda(\cdot), and \min_{s \in S} \hat{V}^\mathrm{m}_s(\cdot)\}$.


\begin{algorithm}
    \caption{Learning algorithm for \model}
    \label{alg:fair-policy}
    \footnotesize
    \begin{algorithmic}
        \State {\bfseries Input:} hyperparameters of representation networks (number of epochs $n_{r}$, learning rate $\eta_r$, action fairness parameter $\gamma$), hyperparameters of policy network (number of epochs $n_{p}$, learning rate $\eta_p$, policy loss $\mathcal{L}_\pi^{\mathrm{m}}(\cdot)$) 
        \State Initialize $\theta_Y^{(0)}, \theta_\Phi^{(0)}, \theta_S^{(0)} \sim$ Kaiming-Uniform \Comment{Step 1. Fitting the representation networks}
        \For{$k = 1$ {\bfseries to} $n_{r}$ }
            \State Forward pass of the base representation, outcome prediction, and sensitive attribute networks with $\theta_Y^{(k-1)}, \theta_\Phi^{(k-1)}, \theta_S^{(k-1)}$
            \State $\theta_Y^{(k)} \gets \theta_Y^{(k-1)} - \eta_r \nabla_{\theta_Y} \big[ \mathcal{L}_Y(\theta_\Phi^{(k-1)}, \theta_Y^{(k-1)}) \big]$ 
            \State $\theta_\Phi^{(k)} \gets \theta_\Phi^{(k-1)} - \eta_r \nabla_{\theta_\Phi} \big[ \mathcal{L}_Y(\theta_\Phi^{(k-1)}, \theta_Y^{(k-1)}) + \gamma \mathcal{L_\mathrm{conf}}(\theta_\Phi^{(k-1)}, \theta_S^{(k-1)}) \big]$
            \State Forward pass of the base representation and sensitive attribute networks with $\theta_\Phi^{(k)}, \theta_S^{(k-1)}$
            \State $\theta_S^{(k)} \gets \theta_S^{(k-1)} - \eta_r \nabla_{\theta_S} \big[ \gamma \mathcal{L}_S(\theta_\Phi^{(k)}, \theta_S^{(k-1)}) \big]$
        \EndFor
        \State $\hat{\theta}_\Phi \gets \theta_\Phi^{(n_{r})}$

        \State Initialize $\theta^{(0)} \sim$ Kaiming-Uniform \Comment{Step 2. Fitting the policy network}
        \For{$k = 1$ {\bfseries to} $n_{p}$ } 
            \State Forward pass of the policy network with $\theta^{(k-1)}$
            \State $\theta^{(k)} \gets \theta^{(k-1)} - \eta_p \nabla_{\theta} \big[\mathcal{L}_\pi^\mathrm{m}(\pi_{\theta^{(k-1)}}(\hat{\Phi}(X)))\big]$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\clearpage

\section{Hyperparameter tuning}\label{app:hyperparam}

We followed best practices in causal machine learning \citep[e.g.,][]{Bica.2021, Curth.2021} and performed extensive hyperparameter tuning for \model. We split the data into a training set (80\%) and a validation set (10\%). We then performed 30 random grid search iterations and chose the set of parameters that minimized the respective training loss on the validation set. In particular, the tuning procedure was the same for all baselines, which ensures that the performance differences reported in Section~\ref{sec:experiments} are  not due to larger flexibility but are due to the different methods themselves.

We performed hyperparameter tuning for all neural networks in \model, i.e., the different representation networks and the policy network. For the real-world data, we also used TARNet \citep{Shalit.2017} in order to estimate the nuisance parameters. We first performed hyperparameter tuning for TARNet and for the representation networks, before tuning the policy neural networks by using the input from the tuned neural networks. The tuning ranges for the hyperparameter are shown in Table~\ref{tab:hyper_sim} (simulated data) and Table~\ref{tab:hyper_real} (real-world data). 

\begin{table}[ht]
\caption{Hyperparameter tuning ranges (simulated data).}
\centering
\label{tab:hyper_sim}
\footnotesize
\begin{tabular}{lll}
\toprule
\textsc{Neural network} & \textsc{Hyperparameter} & \textsc{Tuning range} \\
\midrule
All neural networks&Dropout probability & $0$, $0.1$, $0.2$ \\
&Batch size &$32$, $64$, $128$  \\
& Epochs & 400 \\
\midrule
Representation networks & Learning rate & 0.0001, 0.0005, 0.001, 0.005\\
 & Hidden layer / representation size & 2, 5, 10\\
& Weight decay & 0, 0.001 \\
Policy network & Learning rate & 0.00005, 0.0001, 0.0005, 0.001 \\
 & Hidden layer size  & 5, 10, 15, 20 \\
 & Weight decay & 0\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Hyperparameter tuning ranges (real-world data).}
\centering
\label{tab:hyper_real}
\footnotesize
\begin{tabular}{lll}
\toprule
\textsc{Neural network} & \textsc{Hyperparameter} & \textsc{Tuning range} \\
\midrule
All neural networks&Dropout probability & $0$, $0.1$, $0.2$, $0.3$ \\
&Batch size &$32$, $64$, $128$  \\
\midrule
TARNet & Learning rate & 0.0001, 0.0005, 0.001, 0.005\\
 & Hidden layer sizes & 5, 10, 20, 30\\
& Weight decay & 0 \\
& Epochs & 200 \\

Representation networks & Learning rate & 0.0001, 0.0005, 0.001, 0.005\\
 & Hidden layer / representation size & 2, 5, 10\\
& Epochs & 400 \\
& Weight decay & 0, 0.001 \\
Policy network & Learning rate & 0.00005, 0.0001, 0.0005, 0.001 \\
 & Hidden layer size  & 5, 10, 15, 20 \\
& Epochs & 300 \\
 & Weight decay & 0\\
\bottomrule
\end{tabular}
\end{table}

The tables include both the hyperparameter ranges shared across all neural networks and the network-specific hyperparameters. For reproducibility purposes, we report the selected hyperparameters as \emph{.yaml} files.\footnote{Codes are in the supplementary materials and at \url{https://anonymous.4open.science/r/FairPol-402C}.}

\clearpage

\section{Details regarding simulated data}
\label{app:data_sim}

Here, we provide details regarding our synthetic data generation. We consider a decision problem from credit lending where loans are approved based on covariates of the customers, yet where algorithmic decision-making must not discriminate by gender. To this end, we denote the sensitive attribute by $S \in \{0,1\}$ and generate data as follows. We simulate two covariates $X_\mathrm{u} \in \R$ and $X_s \in \R$ via
\begin{equation}
    S \sim \mathrm{Bernoulli}(p_s), \qquad  X_u \sim \mathcal{U}[-1, 1], \qquad \text{ and } \qquad X_s \mid S = s \sim \mathcal{U}[s - 1, s],
\end{equation}
where $\mathcal{U}[-1, 1]$ is the uniform distribution over the interval $[-1, 1]$. Thus, $X_u$ is independent of $S$, while $X_s$ is correlated with $S$. In practice, $X_u$ can be, e.g., a credit score (which gives the probability of repaying a loan yet which is independent of gender), while $X_s$ can be, e.g., income (which is often correlated with gender). We further generate actions (decisions on whether a loan was approved or not) via
\begin{equation}
  A \mid X_u = x_u, X_s = x_s, S = s ~\sim \mathrm{Bernoulli}(p) \quad \text{with} \quad p = \sigma(\sin(2 x_u) + \sin(2 X_s) + \sin(2 s)),
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function. Finally, we generate outcomes
\begin{equation}
  Y = \mathbbm{1}\{A = 1\} \left(\mathbbm{1}\{X_u < 0.5\}\sin(4X_s - 2) + \mathbbm{1}\{X_u > 0.5\}(0.6 \, S - 0.3)\right) + \epsilon,
\end{equation}
where $\epsilon \sim \mathcal{N}(0, 0.1)$. In our example, the outcomes could correspond to the profit for the lending institution. We sample a dataset of $n=3000$ sample from the data generating process and split the data into a training set (80\%) and a test set (20\%).

\clearpage

\section{Details regarding real-world data}
\label{app:data}

In our experiment with real-world data, we use data from the Oregon health insurance experiment (OHIE)\footnote{The dataset is available here: https://www.nber.org/programs-projects/projects-and-centers/oregon-health-insurance-experiment} \citep{Finkelstein.2012}. The OHIE was conducted as a large-scale experiment among public health to assess the effect of health insurance on several outcomes such as health or economic status. In 2008, a lottery draw offered low-income, uninsured adults in Oregon participation in a Medicaid program, providing health insurance. Chosen individuals were offered free health insurance. After a period of 12 months, a survey was conducted to evaluate several outcomes of the participants.

In our analysis, the decision to sign up for the Medicaid program is the action $A$, and the overall out-of-pocket cost for medical care within the last 6 months is the outcome $Y$. The sensitive covariate $S$ we consider is gender. Furthermore, we include the following covariates $X$: age, the number of people the individual signed up with, the week the individual signed up, the number of emergency visits before the experiment, and language. We extract $n=24,646$ observations from the OHIE data and plot the histograms of all variables in Fig.~\ref{fig:descriptive}. 

We split the data randomly into a train (0.7\%), validation (0.1\%), and test set (0.2\%) and perform hyperparameter tuning using the validation set. The evaluation metrics are then computed using the test set.

\begin{figure}[H]
\centering
\includegraphics[width=.8\linewidth]{figures/plot_descriptive.pdf}
\caption{Histograms of marginal distributions (real-world data).}
\label{fig:descriptive}
\end{figure}

\clearpage

\section{Discussion}\label{app:discussion}


% relevance

Unfair decisions can have detrimental consequences for individuals because of which ethical and legal frameworks require that algorithmic decision-making must ensure fairness \citep{Barocas.2016, Kleinberg.2019}. Hence, potential applications benefiting from fairness for algorithmic decision-making are vast and include healthcare, lending, and criminal justice, among many others \citep{DeArteaga.2022}. For instance, in the U.S., the Equal Credit Opportunity Act mandates that lending decisions are fair for individuals of different gender, race, and other sensitive groups, while the Fair Housing Act enforces similar principles for housing rentals and purchases. As such algorithmic decision-making must avoid discrimination of individuals and thus generate decisions that are regarded as fair.


We addressed the problem of fairness in algorithmic decision-making by learning fair policies from observational data. Our framework has three main benefits that make it appealing for use in practice: (1)~Our framework is directly applicable even in settings where observational data ingrain historical discrimination. Despite such historical discrimination, our framework can still obtain a fair policy. This is relevant for practice as there is a growing awareness that many data sources are biased and that it is often challenging or infeasible to remove bias in historical data \citep{CorbettDavies.2023}. (2)~Our framework comes with a scalable machine learning instantiation based on a custom neural network (\model). Hence, practitioners can effectively generate fair policies from high-dimensional and non-linear observational data. (3)~Our framework is flexible in the sense that it supports different fairness notions. Practitioners can thus adapt our framework to the underlying fairness goals as well as the legal and ethical contexts and thus choose a suitable fairness notion. Together, our framework fulfills crucial fairness demands in many applications from practice (e.g., automated hiring, credit lending, and ad targeting).


Our work contributes to the literature in several ways. First, our work connects to off-policy learning \citep[e.g.,][]{Kallus.2018, Athey.2021}. While there is a growing body of literature that uses off-policy learning for managerial decision-making such as pricing and ad targeting \citep[e.g.,][]{Smith.2023, Yoganarasimhan.2022, Yang.2023}, we add by offering a new framework with fairness guarantees. In particular, our work fills an important gap in the literature in that we are able to learn fair policies from discriminatory observational data. Second, there is extensive literature on algorithmic fairness that focuses on machine learning predictions \citep[e.g.,][]{Hardt.2016, Kusner.2017, Nabi.2018}, whereas we contribute to algorithmic fairness for decision-making from observational data, specifically, off-policy learning. Third, fairness notions such as envy-free fairness or max-min fairness have been used in traditional, utility-based decision models such as those from resource allocation and pricing \citep[e.g.,][]{Bertsimas.2011,Kallus.2021c, Cohen.2022}. We build upon these fairness notions but integrate them into off-policy learning. 


\textbf{Conclusion:} In this paper, we proposed a novel framework for fair off-policy learning from observational data. For this, we introduced fairness notions tailored to off-policy learning, then developed a flexible instantiation of our framework using machine learning (\model), and finally provided theoretical guarantees in form of generalization bounds. Our framework is widely applicable to algorithmic decision-making in practice where fairness must be ensured.

\end{document}
