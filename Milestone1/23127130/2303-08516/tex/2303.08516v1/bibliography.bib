% This file was created with Citavi 6.10.0.0

@article{Arnsperger.1994,
 author = {Arnsperger, Christian},
 year = {1994},
 title = {Envy-freeness and distributive justice},
 pages = {155--186},
 volume = {8},
 number = {2},
 journal = {Journal of Economic Surveys},
 file = {j.1467-6419.1994.tb00098.x:Attachments/j.1467-6419.1994.tb00098.x.pdf:application/pdf}
}


@article{Athey.2021,
 abstract = {In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application-specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.},
 author = {Athey, Susan and Wager, Stefan},
 year = {2021},
 title = {Policy learning with observational data},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {133--161},
 volume = {89},
 number = {1},
 journal = {Econometrica},
 file = {1702.02896:Attachments/1702.02896.pdf:application/pdf}
}


@article{Barocas.2016,
 author = {Barocas, Solon and Selbst, Andrew D.},
 year = {2016},
 title = {Big data's disparate impact},
 pages = {671--732},
 volume = {104},
 journal = {California Law Review},
 file = {Big Data's Disparate Impact:Attachments/Big Data's Disparate Impact.pdf:application/pdf}
}


@inproceedings{Bennett.2019b,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Bennett, Andrew and Kallus, Nathan},
 title = {Policy Evaluation with Latent Confounders via Optimal Balance},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-policy-evaluation-with-latent-confounders-via-optimal-balance-Paper:Attachments/NeurIPS-2019-policy-evaluation-with-latent-confounders-via-optimal-balance-Paper.pdf:application/pdf}
}


@inproceedings{Bennett.2020,
 abstract = {Recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. But, efficient policy evaluation need not yield efficient estimation of policy parameters. We consider the estimation problem given by a weighted surrogate-loss classification reduction of policy learning with any score function, either direct, inverse-propensity weighted, or doubly robust. We show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. We draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semiparametric model. In light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. We propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically.},
 author = {Bennett, Andrew and Kallus, Nathan},
 title = {Efficient policy learning from surrogate-loss classification reductions},
 url = {http://arxiv.org/pdf/2002.05153v1},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 booktitle = {ICML},
 year = {2020},
 file = {2002.05153:Attachments/2002.05153.pdf:application/pdf}
}


@article{Bertsimas.2011,
 abstract = {TeX output 2011.03.04:1223},
 author = {Bertsimas, Dimitris and Farias, Vivek F. and Trichakis, Nikolaos},
 year = {2011},
 title = {The price of fairness},
 pages = {17--31},
 volume = {59},
 number = {1},
 journal = {Operations Research},
 doi = {10.1287/opre.1100.0865},
 file = {The Price of Fairness:Attachments/The Price of Fairness.pdf:application/pdf}
}


@article{Bertsimas.2013,
 author = {Bertsimas, Dimitris and Farias, Vivek F. and Trichakis, Nikolaos},
 year = {2013},
 title = {Fairness, efficiency, and flexibility in organ allocation for kidney transplantation},
 pages = {73--87},
 volume = {61},
 number = {1},
 journal = {Operations Research},
 doi = {10.1287/opre.1120.1138},
 file = {opre.1120.1138:Attachments/opre.1120.1138.pdf:application/pdf}
}


@article{Bhutta.2022,
 author = {Bhutta, Neil and Hizmo, Aurel and Ringo, Daniel},
 year = {2022},
 title = {How much does racial bias affect mortgage lending? {E}vidence from human and algorithmic credit decisions},
 keywords = {automated underwriting;credit score;discrimination;Fair lending;G28;mortgage lendingG21;R30;R51},
 pages = {1--44},
 volume = {2022},
 number = {-067},
 journal = {Finance and Economics Discussion Series},
 doi = {10.17016/FEDS.2022.067},
 file = {SSRN-id4276067:Attachments/SSRN-id4276067.pdf:application/pdf}
}


@inproceedings{Bica.2021,
 author = {Bica, Ioana and Jarrett, Daniel and {van der Schaar}, Mihaela},
 title = {Invariant causal imitation learning for generalizable policies},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-invariant-causal-imitation-learning-for-generalizable-policies-Paper:Attachments/NeurIPS-2021-invariant-causal-imitation-learning-for-generalizable-policies-Paper.pdf:application/pdf}
}


@article{Chernozhukov.2022,
 abstract = {Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where moment conditions have zero derivative with respect to first steps. We show that orthogonal moment functions can be constructed by adding to identifying moments the nonparametric influence function for the effect of the first step on identifying moments. Orthogonal moments reduce model selection and regularization bias, as is very important in many applications, especially for machine learning first steps.  We give debiased machine learning estimators of functionals of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that adding to identifying moments the nonparametric influence function provides a general construction of orthogonal moments, including regularity conditions, and show that the nonparametric influence function is robust to additional unknown functions on which it depends. We give a general approach to estimating the unknown functions in the nonparametric influence function and use it to automatically debias estimators of functionals of high dimensional conditional location learners. We give a variety of new doubly robust moment equations and characterize double robustness. We give general and simple regularity conditions and apply these for asymptotic inference on functionals of high dimensional regression quantiles and dynamic discrete choice parameters with high dimensional state variables.},
 author = {Chernozhukov, Victor and Escanciano, Juan Carlos and Ichimura, Hidehiko and Newey, Whitney K. and Robins, James M.},
 year = {2022},
 title = {Locally robust semiparametric estimation},
 url = {http://arxiv.org/pdf/1608.00033v4},
 keywords = {Mathematics - Statistics;Semiparametric inference;Statistics - Theory;Stochastic Equicontinuity;Two-step estimation},
 pages = {1501--1535},
 volume = {90},
 number = {4},
 journal = {Econometrica},
 file = {1608.00033:Attachments/1608.00033.pdf:application/pdf}
}


@article{Chouldechova.2020,
 abstract = {The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.},
 author = {Chouldechova, Alexandra and Roth, Aaron},
 year = {2020},
 title = {A snapshot of the frontiers of fairness in machine learning},
 url = {http://arxiv.org/pdf/1810.08810v1},
 keywords = {Computer Science - Computer Science and Game Theory;Computer Science - Data Structures and Algorithms;Computer Science - Learning;Statistics - Machine Learning},
 pages = {82--89},
 volume = {63},
 number = {5},
 journal = {Communications of the ACM},
 file = {1810.08810:Attachments/1810.08810.pdf:application/pdf}
}


@article{Cohen.2022,
 abstract = {Management Science 0.0},
 author = {Cohen, Maxime C. and Elmachtoub, Adam N. and Lei, Xiao},
 year = {2022},
 title = {Price Discrimination with Fairness Constraints},
 keywords = {fairness;personalization;price discrimination;social welfare},
 pages = {8536--8552},
 volume = {68},
 number = {12},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.2022.4317},
 file = {mnsc.2022.4317:Attachments/mnsc.2022.4317.pdf:application/pdf}
}


@article{CorbettDavies.2023,
 author = {Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
 year = {2023},
 title = {The measure and mismeasure of fairness: A critical review of fair machine learning},
 journal = {arXiv preprint},
 file = {fair-ml:Attachments/fair-ml.pdf:application/pdf}
}


@article{Cowgill.2020,
 author = {Cowgill, Bo and Tucker, Catherine},
 year = {2020},
 title = {Algorithmic fairness and economics},
 journal = {Columbia Business School Research Paper},
 file = {SSRN-id3361280:Attachments/SSRN-id3361280.pdf:application/pdf}
}


@inproceedings{Creager.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Creager, Elliot and Madras, David and Jacobsen, Joern-Henrik and Weiss, Marissa A. and Swersky, Kevin and Pitassi, Tohiann and Zemel, Richard},
 title = {Flexibly Fair Representation Learning by Disentanglement},
 keywords = {ICML;Machine learning},
 booktitle = {ICML},
 year = {2019},
 file = {creager19a:Attachments/creager19a.pdf:application/pdf}
}


@inproceedings{Curth.2021,
 abstract = {The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed meta-learning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes.},
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Nonparametric estimation of heterogeneous treatment effects: From theory  to learning Algorithms},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2021},
 file = {2101.10943:Attachments/2101.10943.pdf:application/pdf}
}


@misc{Dastin.2018,
 author = {Dastin, Jeffrey},
 year = {2018},
 title = {Amazon scraps secret {AI} recruiting tool that showed bias against women},
 url = {https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G}
}


@article{DeArteaga.2022,
 abstract = {The extensive adoption of business analytics (BA) has brought financial gains and increased efficiencies. However, these advances have simultaneously drawn attention to rising legal and ethical challenges when BA inform decisions with fairness implications. As a response to these concerns, the emerging study of algorithmic fairness deals with algorithmic outputs that may result in disparate outcomes or other forms of injustices for subgroups of the population, especially those who have been historically marginalized. Fairness is relevant on the basis of legal compliance, social responsibility, and utility; if not adequately and systematically addressed, unfair BA systems may lead to societal harms and may also threaten an organization's own survival, its competitiveness, and overall performance. This paper offers a forward-looking, BA-focused review of algorithmic fairness. We first review the state-of-the-art research on sources and measures of bias, as well as bias mitigation algorithms. We then provide a detailed discussion of the utility-fairness relationship, emphasizing that the frequent assumption of a trade-off between these two constructs is often mistaken or short-sighted. Finally, we chart a path forward by identifying opportunities for business scholars to address impactful, open challenges that are key to the effective and responsible deployment of BA.},
 author = {De-Arteaga, Maria and Feuerriegel, Stefan and Saar-Tsechansky, Maytal},
 year = {2022},
 title = {Algorithmic fairness in business analytics: Directions for research and practice},
 keywords = {Computer Science - Artificial Intelligence},
 pages = {3749--3770},
 volume = {31},
 number = {10},
 journal = {Production and Operations Management},
 file = {2207.10991:Attachments/2207.10991.pdf:application/pdf}
}


@inproceedings{Dwork.2012,
 author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Tohiann and Reingold, Omer and Zemel, Richard},
 title = {Fairness through awareness},
 keywords = {Computational complexity and cryptography;Computer Applications;Computer science;Computer Systems Organization;Computing Methodologies;Data;Design and analysis of algorithms;Information Systems;Mathematics of Computing;Numerical analysis;Software;Theory of Computation},
 booktitle = {ITCS},
 year = {2012},
 file = {2090236.2090255:Attachments/2090236.2090255.pdf:application/pdf}
}


@article{Fang.2022,
 author = {Fang, Ethan X. and Wang, Zhaoran and Wang, Lan},
 year = {2022},
 title = {Fairness-oriented learning for optimal individualized treatment rules},
 keywords = {Individualized Treatment Regime;Nonconvex Problem;Quantile},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2021.2008402},
 file = {Fairness Oriented Learning for Optimal Individualized Treatment Rules:Attachments/Fairness Oriented Learning for Optimal Individualized Treatment Rules.pdf:application/pdf}
}


@article{Finkelstein.2012,
 abstract = {In 2008, a group of uninsured low-income adults in Oregon was selected by lottery to be given the chance to apply for Medicaid. This lottery provides an opportunity to gauge the effects of expanding access to public health insurance on the health care use, financial strain, and health of low-income adults using a randomized controlled design. In the year after random assignment, the treatment group selected by the lottery was about 25 percentage points more likely to have insurance than the control group that was not selected. We find that in this first year, the treatment group had substantively and statistically significantly higher health care utilization (including primary and preventive care as well as hospitalizations), lower out-of-pocket medical expenditures and medical debt (including fewer bills sent to collection), and better self-reported physical and mental health than the control group.},
 author = {Finkelstein, Amy and Taubman, Sarah and Wright, Bill and Bernstein, Mira and Gruber, Jonathan and Newhouse, Joseph P. and Allen, Heidi and Baicker, Katherine},
 year = {2012},
 title = {The oregon health insurance experiment: Evidence from the first year},
 pages = {1057--1106},
 volume = {127},
 number = {3},
 journal = {The Quarterly Journal of Economics},
 doi = {10.1093/qje/qjs020},
 file = {qje{\_}qjs020:Attachments/qje{\_}qjs020.pdf:application/pdf}
}


@inproceedings{Hardt.2016,
 abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.  In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.  We illustrate our notion using a case study of FICO credit scores.},
 author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
 title = {Equality of opportunity in supervised learning},
 keywords = {Computer Science - Learning},
 booktitle = {NeurIPS},
 year = {2016},
 file = {NIPS-2016-equality-of-opportunity-in-supervised-learning-Paper:Attachments/NIPS-2016-equality-of-opportunity-in-supervised-learning-Paper.pdf:application/pdf}
}


@inproceedings{Hatt.2022b,
 abstract = {Learning personalized decision policies that generalize to the target population is of great relevance. Since training data is often not representative of the target population, standard policy learning methods may yield policies that do not generalize target population. To address this challenge, we propose a novel framework for learning policies that generalize to the target population. For this, we characterize the difference between the training data and the target population as a sample selection bias using a selection variable. Over an uncertainty set around this selection variable, we optimize the minimax value of a policy to achieve the best worst-case policy value on the target population. In order to solve the minimax problem, we derive an efficient algorithm based on a convex-concave procedure and prove convergence for parametrized spaces of policies such as logistic policies. We prove that, if the uncertainty set is well-specified, our policies generalize to the target population as they can not do worse than on the training data. Using simulated data and a clinical trial, we demonstrate that, compared to standard policy learning methods, our framework improves the generalizability of policies substantially.},
 author = {Hatt, Tobias and Tschernutter, Daniel and Feuerriegel, Stefan},
 title = {Generalizing off-policy learning under sample selection bias},
 url = {http://arxiv.org/pdf/2112.01387v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {UAI},
 year = {2022},
 file = {2112.01387:Attachments/2112.01387.pdf:application/pdf}
}


@inproceedings{Jabbari.2017,
 abstract = {We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness},
 author = {Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Roth, Aaron},
 title = {Fairness in reinforcement learning},
 keywords = {Computer Science - Learning},
 booktitle = {ICML},
 year = {2017},
 file = {jabbari17a:Attachments/jabbari17a.pdf:application/pdf}
}


@inproceedings{Jiang.2019,
 author = {Jiang, Jiechuan and Lu, Zongqing},
 title = {Learning fairness in multi-agent systems},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-learning-fairness-in-multi-agent-systems-Paper:Attachments/NeurIPS-2019-learning-fairness-in-multi-agent-systems-Paper.pdf:application/pdf}
}


@inproceedings{Kallus.2018,
 abstract = {We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.},
 author = {Kallus, Nathan},
 title = {Balanced policy evaluation and learning},
 booktitle = {NeurIPS},
 year = {2018},
 file = {Kallus 2018 - Balanced Policy Evaluation and Learning:Attachments/Kallus 2018 - Balanced Policy Evaluation and Learning.pdf:application/pdf}
}


@inproceedings{Kallus.2018c,
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Confounding-robust policy improvement},
 booktitle = {NeurIPS},
 year = {2018},
 file = {neurips{\_}2018{\_}appendix:Attachments/neurips{\_}2018{\_}appendix.pdf:application/pdf;NeurIPS-2018-confounding-robust-policy-improvement-Paper:Attachments/NeurIPS-2018-confounding-robust-policy-improvement-Paper.pdf:application/pdf}
}


@inproceedings{Kallus.2018d,
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Policy evaluation and optimization with continuous treatments},
 booktitle = {AISTATS},
 year = {2018},
 file = {kallus18a:Attachments/kallus18a.pdf:application/pdf}
}


@inproceedings{Kallus.2018f,
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Residual unfairness in fair machine learning from prejudiced data},
 booktitle = {ICML},
 year = {2018},
 file = {kallus18a:Attachments/kallus18a.pdf:application/pdf}
}


@article{Kallus.2019b,
 abstract = {The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, to construct models that predict the protected class from proxy variables, such as surname and geolocation. We show that even with such data, a variety of common disparity measures are generally unidentifiable, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the tightest-possible set of all possible true disparities that are consistent with the data (and possibly any assumptions). We further provide optimization-based algorithms for computing and visualizing these sets and statistical tools to assess sampling uncertainty. Together, these enable reliable and robust assessments of disparities -- an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.},
 author = {Kallus, Nathan and Mao, Xiaojie and Zhou, Angela},
 year = {2021},
 title = {Assessing algorithmic fairness with unobserved protected class using data combination},
 url = {http://arxiv.org/pdf/1906.00285v2},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Statistics - Machine Learning},
 pages = {1591--2376},
 volume = {68},
 number = {3},
 issn = {0025-1909},
 journal = {Management Science},
 file = {1906.00285:Attachments/1906.00285.pdf:application/pdf}
}


@article{Kallus.2021b,
 author = {Kallus, Nathan},
 year = {2021},
 title = {More efficient policy learning via optimal retargeting},
 keywords = {Efficient policy learning;Individualized treatment regimes;Optimization;Overlap},
 pages = {646--658},
 volume = {116},
 number = {534},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2020.1788948},
 file = {01621459.2020:Attachments/01621459.2020.pdf:application/pdf}
}


@inproceedings{Kallus.2021c,
 abstract = {We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a {\textquotedbl}triple bottom line{\textquotedbl}: personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.},
 author = {Kallus, Nathan and Zhou, Angela},
 title = {Fairness, welfare, and equity in personalized pricing},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {FAccT},
 year = {2021},
 file = {2012.11066:Attachments/2012.11066.pdf:application/pdf}
}


@inproceedings{Kallus.2022,
 abstract = {Off-policy evaluation and learning (OPE/L) use offline observational data to make better decisions, which is crucial in applications where experimentation is necessarily limited. OPE/L is nonetheless sensitive to discrepancies between the data-generating environment and that where policies are deployed. Recent work proposed distributionally robust OPE/L (DROPE/L) to remedy this, but the proposal relies on inverse-propensity weighting, whose regret rates may deteriorate if propensities are estimated and whose variance is suboptimal even if not. For vanilla OPE/L, this is solved by doubly robust (DR) methods, but they do not naturally extend to the more complex DROPE/L, which involves a worst-case expectation. In this paper, we propose the first DR algorithms for DROPE/L with KL-divergence uncertainty sets. For evaluation, we propose Localized Doubly Robust DROPE (LDR$^2$OPE) and prove its semiparametric efficiency under weak product rates conditions. Notably, thanks to a localization technique, LDR$^2$OPE only requires fitting a small number of regressions, just like DR methods for vanilla OPE. For learning, we propose Continuum Doubly Robust DROPL (CDR$^2$OPL) and show that, under a product rate condition involving a continuum of regressions, it enjoys a fast regret rate of {\$}\mathcal{O}(N{\^{}}{-1/2}){\$} even when unknown propensities are nonparametrically estimated. We further extend our results to general $f$-divergence uncertainty sets. We illustrate the advantage of our algorithms in simulations.},
 author = {Kallus, Nathan and Mao, Xiaojie and Wang, Kaiwen and Zhou, Zhengyuan},
 title = {Doubly robust distributionally roust off-policy evaluation and learning},
 url = {http://arxiv.org/pdf/2202.09667v1},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 booktitle = {ICML},
 year = {2022},
 file = {2202.09667:Attachments/2202.09667.pdf:application/pdf}
}


@article{Kennedy.2022,
 abstract = {In this review we cover the basics of efficient nonparametric parameter estimation (also called functional estimation), with a focus on parameters that arise in causal inference problems. We review both efficiency bounds (i.e., what is the best possible performance for estimating a given parameter?) and the analysis of particular estimators (i.e., what is this estimator's error, and does it attain the efficiency bound?) under weak assumptions. We emphasize minimax-style efficiency bounds, worked examples, and practical shortcuts for easing derivations. We gloss over most technical details, in the interest of highlighting important concepts and providing intuition for main ideas.},
 author = {Kennedy, Edward H.},
 year = {2022},
 title = {Semiparametric doubly robust targeted double machine learning: A review},
 url = {http://arxiv.org/pdf/2203.06469v1},
 keywords = {Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2203.06469:Attachments/2203.06469.pdf:application/pdf}
}


@inproceedings{Kilbertus.2017,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Kilbertus, Niki and Rojas-Carulla, Mateo and Parascandolo, Gianbattista and Hardt, Moritz and Janzig, Dominik and Sch{\"o}lkopf, Bernhard},
 title = {Avoiding Discrimination through Causal Reasoning},
 booktitle = {NeurIPS},
 year = {2017},
 file = {NIPS-2017-avoiding-discrimination-through-causal-reasoning-Paper:Attachments/NIPS-2017-avoiding-discrimination-through-causal-reasoning-Paper.pdf:application/pdf}
}


@inproceedings{Kingma.2015,
 abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
 author = {Kingma, Diederik P. and Ba, Jimmy},
 title = {Adam: A method for stochastic optimization},
 booktitle = {ICLR},
 year = {2015}
}


@article{Kleinberg.2019,
 author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Sunstein, Cass R.},
 year = {2019},
 title = {Discrimination in the age of algorithms},
 pages = {113--174},
 volume = {10},
 journal = {Journal of Legal Analysis},
 doi = {10.1093/jla/laz001},
 file = {laz001:Attachments/laz001.pdf:application/pdf}
}


@article{Kozodoi.2022,
 abstract = {The rise of algorithmic decision-making has spawned much research on fair machine learning (ML). Financial institutions use ML for building risk scorecards that support a range of credit-related decisions. Yet, the literature on fair ML in credit scoring is scarce. The paper makes three contributions. First, we revisit statistical fairness criteria and examine their adequacy for credit scoring. Second, we catalog algorithmic options for incorporating fairness goals in the ML model development pipeline. Last, we empirically compare different fairness processors in a profit-oriented credit scoring context using real-world data. The empirical results substantiate the evaluation of fairness measures, identify suitable options to implement fair credit scoring, and clarify the profit-fairness trade-off in lending decisions. We find that multiple fairness criteria can be approximately satisfied at once and recommend separation as a proper criterion for measuring the fairness of a scorecard. We also find fair in-processors to deliver a good balance between profit and fairness and show that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. The codes corresponding to the paper are available on GitHub.},
 author = {Kozodoi, Nikita and Jacob, Johannes and Lessmann, Stefan},
 year = {2022},
 title = {Fairness in credit scoring: Assessment, implementation and profit implications},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 pages = {1083--1094},
 volume = {297},
 number = {3},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 file = {2103.01907:Attachments/2103.01907.pdf:application/pdf}
}


@article{Kraus.2020,
 abstract = {European Journal of Operational Research, 281 (2020) 628-641. doi:10.1016/j.ejor.2019.09.018},
 author = {Kraus, Mathias and Feuerriegel, Stefan and Oztekin, Asil},
 year = {2020},
 title = {Deep learning in business analytics and operations research: {M}odels, applications and managerial implications},
 keywords = {Analytics;Deep Learning;Deep neural networks;Managerial implications;Research agenda},
 pages = {628--641},
 volume = {281},
 number = {3},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 doi = {10.1016/j.ejor.2019.09.018},
 file = {1-s2.0-S0377221719307581-main:Attachments/1-s2.0-S0377221719307581-main.pdf:application/pdf}
}


@inproceedings{Kusner.2017,
 abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
 author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
 title = {Counterfactual fairness},
 url = {http://arxiv.org/pdf/1703.06856v3},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2017},
 file = {1703.06856:Attachments/1703.06856.pdf:application/pdf}
}


@article{Lambrecht.2019,
 abstract = {Management Science 2019.65:2966-2981},
 author = {Lambrecht, Anja and Tucker, Catherine},
 year = {2019},
 title = {Algorithmic bias? {A}n empirical study of apparent gender-based discrimination in the display of {STEM} career ads},
 keywords = {algorithmic bias;Algorithms;artificial intelligence;online advertising},
 pages = {2966--2981},
 volume = {65},
 number = {7},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.2018.3093},
 file = {mnsc.2018.3093:Attachments/mnsc.2018.3093.pdf:application/pdf}
}


@inproceedings{Locatello.2019,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Locatello, Francesco and Abbati, Gabriele and Rainforth, Tom and Bauer, Stefan and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
 title = {On the fairness of disentangled representations},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-on-the-fairness-of-disentangled-representations-Paper:Attachments/NeurIPS-2019-on-the-fairness-of-disentangled-representations-Paper.pdf:application/pdf}
}


@inproceedings{Madras.2018,
 author = {Madras, David and Creager, Elliot and Pitassi, Tohiann and Zemel, Richard},
 title = {Learning adversarially fair and transferable representations},
 booktitle = {ICML},
 year = {2018},
 file = {madras18a:Attachments/madras18a.pdf:application/pdf}
}


@inproceedings{Melnychuk.2022,
 abstract = {Estimating counterfactual outcomes over time from observational data is relevant for many applications (e.g., personalized medicine). Yet, state-of-the-art methods build upon simple long short-term memory (LSTM) networks, thus rendering inferences for complex, long-range dependencies challenging. In this paper, we develop a novel Causal Transformer for estimating counterfactual outcomes over time. Our model is specifically designed to capture complex, long-range dependencies among time-varying confounders. For this, we combine three transformer subnetworks with separate inputs for time-varying covariates, previous treatments, and previous outcomes into a joint network with in-between cross-attentions. We further develop a custom, end-to-end training procedure for our Causal Transformer. Specifically, we propose a novel counterfactual domain confusion loss to address confounding bias: it aims to learn adversarial balanced representations, so that they are predictive of the next outcome but non-predictive of the current treatment assignment. We evaluate our Causal Transformer based on synthetic and real-world datasets, where it achieves superior performance over current baselines. To the best of our knowledge, this is the first work proposing transformer-based architecture for estimating counterfactual outcomes from longitudinal data.},
 author = {Melnychuk, Valentyn and Frauen, Dennis and Feuerriegel, Stefan},
 title = {Causal transformer for estimating counterfactual outcomes},
 url = {http://arxiv.org/pdf/2204.07258v2},
 keywords = {Computer Science - Learning;counterfactual inference;personalized medicine;Statistics - Machine Learning;transformer;treatment effect estimation},
 booktitle = {ICML},
 year = {2022},
 file = {2204.07258 (1):Attachments/2204.07258 (1).pdf:application/pdf}
}


@article{Mitchell.2021,
 abstract = {Annu. Rev. Stat. Appl. 2021.8:141-163},
 author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
 year = {2021},
 title = {Algorithmic fairness: Choices, assumptions, and definitions},
 keywords = {Algorithmic Fairness;decision theory;Machine learning;predictive modeling;statistical learning},
 pages = {141--163},
 volume = {8},
 number = {1},
 journal = {Annual Review of Statistics and Its Application},
 file = {annurev-statistics-042720-125902:Attachments/annurev-statistics-042720-125902.pdf:application/pdf}
}


@inproceedings{Nabi.2018,
 abstract = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 author = {Nabi, Razieh and Shpitser, Ilya},
 title = {Fair Inference on Outcomes},
 keywords = {Knowledge Representation and Reasoning Track},
 booktitle = {AAAI},
 year = {2018},
 file = {11553-Article Text-15081-1-2-20201228:Attachments/11553-Article Text-15081-1-2-20201228.pdf:application/pdf}
}


@inproceedings{Nabi.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
 title = {Learning Optimal Fair Policies},
 keywords = {Algorithmic Fairness;causal inference;Dynamic Treatment Regimes;Fair Policies;Mediation Analysis;Optimal Policies;Policy Learning},
 booktitle = {ICML},
 year = {2019},
 file = {nabi19a:Attachments/nabi19a.pdf:application/pdf}
}


@inproceedings{Nilforoshan.2022,
 abstract = {Recent work highlights the role of causality in designing equitable decision-making algorithms. It is not immediately clear, however, how existing causal conceptions of fairness relate to one another, or what the consequences are of using these definitions as design principles. Here, we first assemble and categorize popular causal definitions of algorithmic fairness into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions \emph{almost always} -- in a measure theoretic sense -- result in strongly Pareto dominated decision policies, meaning there is an alternative, unconstrained policy favored by every stakeholder with preferences drawn from a large, natural class. For example, in the case of college admissions decisions, policies constrained to satisfy causal fairness definitions would be disfavored by every stakeholder with neutral or positive preferences for both academic preparedness and diversity. Indeed, under a prominent definition of causal fairness, we prove the resulting policies require admitting all students with the same probability, regardless of academic qualifications or group membership. Our results highlight formal limitations and potential adverse consequences of common mathematical notions of causal fairness.},
 author = {Nilforoshan, Hamed and Gaebler, Johann and Shroff, Ravi and Goel, Sharad},
 title = {Causal conceptions of fairness and their consequences},
 url = {http://arxiv.org/pdf/2207.05302v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computers and Society;Computer Science - Learning;ICML;Machine learning},
 booktitle = {ICML},
 year = {2022},
 file = {2207.05302:Attachments/2207.05302.pdf:application/pdf}
}


@article{Nkonde.2019,
 author = {Nkonde, Mutale},
 year = {2019},
 title = {Is AI bias a corporate social responsibility issue?},
 journal = {Harvard Business Review},
 file = {ai{\_}bias:Attachments/ai{\_}bias.pdf:application/pdf}
}


@article{Obermeyer.2019,
 abstract = {Science 2019.366:447-453},
 author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
 year = {2019},
 title = {Dissecting racial bias in an algorithm used to manage the health of populations},
 pages = {447--453},
 volume = {366},
 number = {6464},
 journal = {Science},
 doi = {10.1530/ey.17.12.7},
 file = {science.aax2342:Attachments/science.aax2342.pdf:application/pdf}
}


@book{Pearl.2009,
 abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
 author = {Pearl, Judea},
 year = {2009},
 title = {Causality},
 address = {New York City},
 publisher = {{Cambridge University Press}},
 isbn = {9780521895606},
 file = {Pearl 2009 - Causality:Attachments/Pearl 2009 - Causality.pdf:application/pdf}
}


@article{Qian.2011,
 abstract = {Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [11]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is non-concave in the parameters. Furthermore there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges we consider estimation based on l(1) penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule.},
 author = {Qian, Min and Murphy, Susan A.},
 year = {2011},
 title = {Performance guarantees for individualized treatment rules},
 keywords = {62H99;62J07;62P10;Decision making;l1-penalized least squares;value},
 pages = {1180--1210},
 volume = {39},
 number = {2},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/10-AOS864},
 file = {10-AOS864:Attachments/10-AOS864.pdf:application/pdf}
}


@article{Rea.2021b,
 abstract = {Production and Operations Management 2021.30:2304-2320},
 author = {Rea, David and Froehle, Craig and Masterson, Suzanne and Stettler, Brian and Fermann, Gregory and Pancioli, Arthur},
 year = {2021},
 title = {Unequal but fair: {I}ncorporating distributive justice in operational allocation models},
 pages = {2304--2320},
 volume = {30},
 number = {7},
 journal = {Production and Operations Management},
 doi = {10.1111/poms.13369},
 file = {Reaetal-UnequalButFair-POM2021:Attachments/Reaetal-UnequalButFair-POM2021.pdf:application/pdf}
}


@article{Rubin.1974,
 author = {Rubin, Donald B.},
 year = {1974},
 title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
 pages = {688--701},
 volume = {66},
 number = {5},
 issn = {0022-0663},
 journal = {Journal of Educational Psychology},
 doi = {10.1037/h0037350}
}


@article{Rubin.1978,
 author = {Rubin, Donald B.},
 year = {1978},
 title = {Bayesian inference for causal effects: The role of randomization},
 keywords = {Potential outcomes},
 pages = {34--58},
 volume = {6},
 number = {1},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/aos/1176344064},
 file = {1176344064:Attachments/1176344064.pdf:application/pdf}
}


@inproceedings{Shalit.2017,
 abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a {\textquotedbl}balanced{\textquotedbl} representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
 author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
 title = {Estimating individual treatment effect: Generalization bounds and  algorithms},
 keywords = {causal effects;Computer Science - Artificial Intelligence;Computer Science - Learning;counterfactual inference;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2017},
 file = {Individual{\_}treatment{\_}generalization{\_}bounds:Attachments/Individual{\_}treatment{\_}generalization{\_}bounds.pdf:application/pdf}
}


@inproceedings{Siddique.2020,
 abstract = {Proceedings of the International Conference on Machine Learning 2020},
 author = {Siddique, Umer and Weng, Paul and Zimmer, Matthieu},
 title = {Learning fair policies in multiobjective (deep) reinforcement learning with average and discounted rewards},
 keywords = {ICML;Machine learning},
 booktitle = {ICML},
 year = {2020},
 file = {siddique20a:Attachments/siddique20a.pdf:application/pdf}
}


@inproceedings{Singh.2019c,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Singh, Ashudeep and Joachims, Thorsten},
 title = {Policy learning for fairness in ranking},
 booktitle = {NeurIPS},
 year = {2019},
 file = {singh{\_}joachims{\_}19a:Attachments/singh{\_}joachims{\_}19a.pdf:application/pdf}
}


@article{Smith.2022,
 abstract = {Marketing Science 0.0},
 author = {Smith, Adam N. and Seiler, Stephan and Aggarwal, Ishant},
 year = {2022},
 title = {Optimal price targeting},
 keywords = {choice models;heterogeneity;Machine learning;personalization;targeting},
 journal = {Marketing Science},
 file = {mksc.2022.1387:Attachments/mksc.2022.1387.pdf:application/pdf}
}


@inproceedings{Tschernutter.2022,
 abstract = {Personalized treatment decisions have become an integral part of modern medicine. Thereby, the aim is to make treatment decisions based on individual patient characteristics. Numerous methods have been developed for learning such policies from observational data that achieve the best outcome across a certain policy class. Yet these methods are rarely interpretable. However, interpretability is often a prerequisite for policy learning in clinical practice. In this paper, we propose an algorithm for interpretable off-policy learning via hyperbox search. In particular, our policies can be represented in disjunctive normal form (i.e., OR-of-ANDs) and are thus intelligible. We prove a universal approximation theorem that shows that our policy class is flexible enough to approximate any measurable function arbitrarily well. For optimization, we develop a tailored column generation procedure within a branch-and-bound framework. Using a simulation study, we demonstrate that our algorithm outperforms state-of-the-art methods from interpretable off-policy learning in terms of regret. Using real-word clinical data, we perform a user study with actual clinical experts, who rate our policies as highly interpretable.},
 author = {Tschernutter, Daniel and Hatt, Tobias and Feuerriegel, Stefan},
 title = {Interpretable off-policy learning via hyperbox search},
 url = {http://arxiv.org/pdf/2203.02473v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2022},
 file = {2203.02473:Attachments/2203.02473.pdf:application/pdf}
}


@misc{UnitedNations.2015,
 author = {{United Nations}},
 year = {2015},
 title = {{S}ustainable {D}evelopment {G}oals: {T}he sustainable development agenda},
 url = {https://www.un.org/sustainabledevelopment/development-agenda/}
}


@article{Viviano.2023,
 abstract = {One of the major concerns of targeting interventions on individuals in social welfare programs is discrimination: individualized treatments may induce disparities across sensitive attributes such as age, gender, or race. This paper addresses the question of the design of fair and efficient treatment allocation rules. We adopt the non-maleficence perspective of first do no harm: we select the fairest allocation within the Pareto frontier. We cast the optimization into a mixed-integer linear program formulation, which can be solved using off-the-shelf algorithms. We derive regret bounds on the unfairness of the estimated policy function and small sample guarantees on the Pareto frontier under general notions of fairness. Finally, we illustrate our method using an application from education economics.},
 author = {Viviano, Davide and Bradic, Jelena},
 year = {2023},
 title = {Fair policy targeting},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 journal = {Journal of the American Statistical Association},
 file = {2005.12395:Attachments/2005.12395.pdf:application/pdf}
}


@article{Yang.2023,
 abstract = {Decision-makers often want to target interventions (e.g., marketing campaigns) so as to maximize an outcome that is observed only in the long-term. This typically requires delaying decisions until the outcome is observed or relying on simple short-term proxies for the long-term outcome. Here we build on the statistical surrogacy and off-policy learning literature to impute the missing long-term outcomes and then approximate the optimal targeting policy on the imputed outcomes via a doubly-robust approach. We apply our approach in large-scale proactive churn management experiments at The Boston Globe by targeting optimal discounts to its digital subscribers to maximize their long-term revenue. We first show that conditions for validity of average treatment effect estimation with imputed outcomes are also sufficient for valid policy evaluation and optimization; furthermore, these conditions can be somewhat relaxed for policy optimization. We then validate this approach empirically by comparing it with a policy learned on the ground truth long-term outcomes and show that they are statistically indistinguishable. Our approach also outperforms a policy learned on short-term proxies for the long-term outcome. In a second field experiment, we implement the optimal targeting policy with additional randomized exploration, which allows us to update the optimal policy for each new cohort of customers to account for potential non-stationarity. Over three years, our approach had a net-positive revenue impact in the range of {\$}4-5 million compared to The Boston Globe's current policies.},
 author = {Yang, Jeremy and Eckles, Dean and Dhillon, Paramveer and Aral, Sinan},
 year = {2023},
 title = {Targeting for long-term outcomes},
 url = {http://arxiv.org/pdf/2010.15835v1},
 keywords = {Computer Science - Learning;Statistics - Applications;Statistics - Machine Learning},
 issn = {0025-1909},
 journal = {Management Science},
 file = {2010.15835:Attachments/2010.15835.pdf:application/pdf}
}


@article{Yoganarasimhan.2022,
 abstract = {Management Science 0.0},
 author = {Yoganarasimhan, Hema and Barzegary, Ebrahim and Pani, Abhishek},
 year = {2022},
 title = {Design and evaluation of optimal free trials},
 keywords = {digital marketing;field experiment;free trials;Machine learning;personalization;policy evaluation;Software as a Service;targeting},
 issn = {0025-1909},
 journal = {Management Science},
 file = {mnsc.2022.4507:Attachments/mnsc.2022.4507.pdf:application/pdf}
}


@inproceedings{Yu.2022,
 abstract = {Long-term fairness is an important factor of consideration in designing and deploying learning-based decision systems in high-stake decision-making contexts. Recent work has proposed the use of Markov Decision Processes (MDPs) to formulate decision-making with long-term fairness requirements in dynamically changing environments, and demonstrated major challenges in directly deploying heuristic and rule-based policies that worked well in static environments. We show that policy optimization methods from deep reinforcement learning can be used to find strictly better decision policies that can often achieve both higher overall utility and less violation of the fairness requirements, compared to previously-known strategies. In particular, we propose new methods for imposing fairness requirements in policy optimization by regularizing the advantage evaluation of different actions. Our proposed methods make it easy to impose fairness constraints without reward engineering or sacrificing training efficiency. We perform detailed analyses in three established case studies, including attention allocation in incident monitoring, bank loan approval, and vaccine distribution in population networks.},
 author = {Yu, Eric Yang and Qin, Zhizhen and Lee, Min Kyung and Gao, Sicun},
 title = {Policy optimization with advantage regularization for long-term Fairness  in decision systems},
 url = {http://arxiv.org/pdf/2210.12546v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computers and Society;Computer Science - Learning},
 booktitle = {NeurIPS},
 year = {2022},
 file = {2210.12546:Attachments/2210.12546.pdf:application/pdf}
}


@inproceedings{Zemel.2013,
 abstract = {Proceedings of the International Conference on Machine Learning 2013},
 author = {Zemel, Richard and Wu, Yu Ledell and Swersky, Kevin and Pitassi, Tohiann and Dwork, Cynthia},
 title = {Learning Fair Representations},
 keywords = {clustering;fairness;latent representations;privacy},
 booktitle = {ICML},
 year = {2013},
 file = {zemel13:Attachments/zemel13.pdf:application/pdf}
}


@inproceedings{Zimmer.2021,
 abstract = {Proceedings of the International Conference on Machine Learning 2021},
 author = {Zimmer, Matthieu and Glanois, Clarie and Siddique, Umer and Weng, Paul},
 title = {Learning fair policies in decentralized cooperative  multi-agent reinforcement learning},
 keywords = {Actor-Critic;Decentralized;fairness;ICML;Multi-agent},
 booktitle = {ICML},
 year = {2021},
 file = {zimmer21a:Attachments/zimmer21a.pdf:application/pdf}
}


