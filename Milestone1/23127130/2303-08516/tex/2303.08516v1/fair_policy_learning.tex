%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Author template for Management Science (mnsc) for articles with e-companion (EC)
%% Mirko Janc, Ph.D., INFORMS, mirko.janc@informs.org
%% ver. 0.95, December 2010
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%input blindrev for anonymous submission
\documentclass[mnsc]{informs_neutral} % current default for manuscript submission
%\documentclass[mnsc,nonblindrev]{informs3}

\OneAndAHalfSpacedXI % current default line spacing
%%\OneAndAHalfSpacedXII 
%%\DoubleSpacedXII
%%\DoubleSpacedXI

% If hyperref is used, dvi-to-ps driver of choice must be declared as
%   an additional option to the \documentstyle. For example
%\documentclass[dvips,mnsc]{informs3}      % if dvips is used
%\documentclass[dvipsone,mnsc]{informs3}   % if dvipsone is used, etc.

% Private macros here (check that there is no clash with the style)

% Natbib setup for author-year style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%
 
%Custom packages
\usepackage{booktabs}       % professional-quality tables
\usepackage{amssymb,amsmath}
\usepackage{bbm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{array}
\usepackage{float}
\usepackage{placeins}

%Three main theorems
\declaretheorem[name=Theorem]{thrm}

%Custom commands
\newcommand{\TODO}[1]{{\color{red}#1}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

\usepackage{xspace}
\newcommand{\model}{\textsf{FairPol}\xspace}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
\ECRepeatTheorems

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.
\MANUSCRIPTNO{MS-0001-1922.65}

%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
\RUNAUTHOR{Frauen and Feuerriegel}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{Fair policy learning from observational data}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{Fair off-policy learning from observational data}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
\AUTHOR{Dennis Frauen}
\AFF{Institute of AI in Management, LMU Munich, Germany \& Munich Center for Machine Learning (MCML), Germany, \EMAIL{frauen@lmu.de}} %, \URL{}}
\AUTHOR{Valentyn Melnychuk}
\AFF{Institute of AI in Management, LMU Munich, Germany \& Munich Center for Machine Learning (MCML), Germany, \EMAIL{melnychuk@lmu.de}} %, \URL{}}
\AUTHOR{Stefan Feuerriegel}
\AFF{Institute of AI in Management, LMU Munich, Germany,  \& Munich Center for Machine Learning (MCML), Germany, \EMAIL{feuerriegel@lmu.de}}
% Enter all authors
} % end of the block

\ABSTRACT{%
Businesses and organizations must ensure that their algorithmic decision-making is fair in order to meet legislative, ethical, and societal demands. For example, decision-making in automated hiring must not discriminate with respect to gender or race. To achieve this, prior research has contributed approaches that ensure algorithmic fairness in machine learning predictions, while comparatively little effort has focused on algorithmic fairness in decision models, specifically off-policy learning. In this paper, we propose a novel framework for \emph{fair} off-policy learning: we learn decision rules from observational data under different notions of fairness, where we explicitly assume that observational data were collected under a different -- potentially biased -- behavioral policy. For this, we first formalize different fairness notions for off-policy learning. We then propose a machine learning approach to learn optimal policies under these fairness notions. Specifically, we reformulate the fairness notions into unconstrained learning objectives that can be estimated from finite samples. Here, we leverage machine learning to minimize the objective constrained on a fair representation of the data, so that the resulting policies satisfy our fairness notions. We further provide theoretical guarantees in form of generalization bounds for the finite-sample version of our framework. We demonstrate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data. As a result, our work enables algorithmic decision-making in a wide array of practical applications where fairness must ensured.  
}%

% Fill in data. If unknown, outcomment the field
\KEYWORDS{off-policy learning, algorithmic fairness, observational data, neural networks, machine learning} %\HISTORY{This paper was
%first submitted on April 12, 1922 and has been with the authors for
%83 years for 65 revisions.}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}

%Why fairness is important

The widespread use of algorithmic decision-making in businesses and organizations has given rise to the imperative that the generated decisions must avoid discrimination and should be fair \citep{Nkonde.2019, Cowgill.2020, CorbettDavies.2023}. Besides ethical and societal concerns, fairness is directly mandated in many applications by government regulations and laws \citep{Barocas.2016, Kleinberg.2019}. For example, in the U.S., the Fair Housing Act and Equal Credit Opportunity Act stipulate that decisions must not be subject to systematic discrimination by gender, race, or other attributes deemed as sensitive. In addition, ensuring fairness is also regarded as integral for the long-term well-being of businesses and organization \citep{DeArteaga.2022}, and, to this end, reducing inequality and improving fairness are key for the global development, as stipulated by the United Nations' Sustainable Development Goals \citep{UnitedNations.2015}.

%Evidence of discrimination

However, research from different areas of management science has provided repeated evidence that algorithmic decision-making is often unfair. A prominent example is Amazon's tool for automatically screening job applicants that was used between 2014 and 2017 \citep{Dastin.2018}. It was later discovered that the underlying algorithm generated decisions that were subject to systematically discrimination against women and thus resulted in a ceteris paribus lower probability of women being hired. Another example is advertising, where, in practice, algorithms tend to show ads with job offerings from science and engineering to men and not to women \citep{Lambrecht.2019}. In credit lending, people from ethnic minorities were found to have a ceteris paribus significantly lower chance of their loan being approved when automated decision-making is used \citep{Bhutta.2022}. Against this backdrop, there is a direct need by businesses and organizations for algorithmic decision-making that guarantees fairness.

%Recap on existing fairness streams

A growing stream of literature, commonly referred to as ``algorithmic fairness'', develops computational approaches for ensuring fairness (see Section~\ref{sec:related_work} for an overview). On the one hand, there are works aimed at algorithmic fairness in machine learning where the objective is to ensure fairness in {predictions} \citep[e.g.,][]{Dwork.2012,Hardt.2016, Nabi.2018, Kallus.2018f, Kallus.2019b, Madras.2018, CorbettDavies.2023}. Here, a common approach is to first select a sensitive attribute (e.g., gender) and then enforce mathematical constraints so that the prediction performance cannot systematically deviate for different sub-populations belonging to the sensitive attribute (e.g., women vs. men). On the other hand, there are works that aim at fairness in traditional, utility-based decision models. Examples are decision problems from resource allocation \citep[e.g.,][]{Arnsperger.1994, Bertsimas.2011, Bertsimas.2013} or pricing \citep[e.g.,][]{Kallus.2021c, Cohen.2022}, where additional constraints are introduced to fulfill that decisions are fair. There are also works that consider fairness in reinforcement learning \citep{Siddique.2020, Zimmer.2021}. However, these assume a Markov decision process (MDP) for sequential decision-making, while, in contrast, few works have focused on non-sequential, off-policy learning from {observational data}. In the latter case, existing approaches require strong assumptions on the underlying causal structure of the data \citep{Nabi.2019} or restrict themselves to Pareto-optimal policies \citep{Viviano.2023}. However, to the best of our knowledge, no work has developed fair off-policy learning for general policies.

%Challanges for fairness and observational data

Ensuring fairness in off-policy learning is subject to inherent challenges. The reason is that off-policy learning is based on observational data and that observational data may ingrain existing bias from historical decision-making.\footnote{Note that ``bias'' can have different meanings. Bias can refer to algorithmic bias, where algorithms discriminate against individuals from certain sensitive groups \citep{DeArteaga.2022}. In contrast, there is statistical bias of estimators, e.g., due to confounded data. Unless stated otherwise, we refer to algorithmic bias throughout this manuscript.} Hence, one challenge is that the resulting policy must be fair despite that the observational data were collected under a different -- potentially biased -- behavioral policy. Furthermore, one may think that a na{\"i}ve approach to achieving algorithmic fairness in algorithmic decision-making is by simply omitting the sensitive attribute from the observational data. For instance, to avoid bias against women, one may want to prevent off-policy learning from having access to a variable that stores the gender of an individual. However, in observational data, other variables may act as proxies for gender, and, hence, the resulting policy may still lead to discrimination due to the underlying data-generating process \citep{Kilbertus.2017}. Hence, a custom approach for handling sensitive attributes in off-policy learning are needed.

%High level overview of contributions

In this paper, we propose a novel framework for fair off-policy learning from observational data. Specifically, we learn fair decision rules from observational data where the observational may be collected under a different -- potentially biased -- behavioral policy. Our contributions are five-fold. (1)~We first introduce different fairness notions that are tailored to our setting, namely fair off-policy learning from observational data. (2)~We then propose our empirical framework to learn optimal policies under these fairness notions where we reformulate the fairness notions into unconstrained learning objectives. Specifically, we leverage machine learning to minimize the objective constrained on a fair representation of the data, so that the resulting policies satisfy our fairness notion. (3)~We present an instantiation of our framework in form of a custom neural network called \model that is both flexible and scalable. (4)~We provide theoretical learning guarantees in form of generalizations bounds for our framework. (5)~We finally evaluate the effectiveness of our framework through extensive numerical experiments using both simulated and real-world data.

%Detailed motivation of value fairness vs action fairness

In our off-policy learning framework, fairness may enter at two different stages, namely with respect to both the action and the policy value. (1)~The former, fairness with respect to the action, ensures that individuals with different sensitive attributes but otherwise equal characteristics receive the same decision. In other words, the choice of the action is independent of the sensitive attribute. For example, in credit lending, this means that a women and a men, each with the same academic degree, have the same chance that their student loan is approved. We later refer to this notion as ``action fairness''. (2)~The latter, fairness with respect to the policy value, allows us to express fairness with respect utility (i.e., the policy value) for each sensitive group, so that individuals with different sensitive attributes achieve, on average, a similar utility. For example, in credit lending, this may allow governments to account for the fact that some sub-populations have been historically underrepresented. Hence, as women have a lower propensity than men to pursue academic careers in subjects related to technology, governments may want to strategically incentivize women through student loans so that the long-term utility of society is maximized. We refer to this notion as ``value fairness''. Later, we introduce two variants of value fairness that build upon envy-free fairness and max-min fairness.  

Our work has direct implications for both research and practice. For research, we add to a growing stream of algorithmic fairness. Prior literature has extensively focused on fairness in machine learning predictions \citep[e.g.,][]{Dwork.2012,Hardt.2016} and in traditional decision models such as resource allocation and pricing \citep[e.g.,][]{Arnsperger.1994, Bertsimas.2011, Kallus.2021c, Cohen.2022}. Different from that, we present a novel framework for fair off-policy learning from observational data. For this reason, we contribute fairness notions that are specifically tailored to off-policy learning. We also provide theoretical guarantees in form of generalization bounds for the finite-sample version of our framework. In practice, our work supports businesses and organizations that seek to promote fairness in their algorithmic decision-making due to legislative, ethical, and societal reasons. Here, our framework is beneficial as businesses and organizations can use their existing -- and potentially biased -- data and can still obtain a fair policy. Potential applications of our framework are thus vast and include, e.g., automated hiring, credit lending, and ad targeting. 

% outline

The remainder of this paper is structured as follows. We provide a background on off-policy learning and algorithmic fairness in Section~\ref{sec:related_work} and therein identify fair off-policy learning as an important research gap. In Section~\ref{sec:problem_setting}, we present our decision problem of fair off-policy learning and introduce tailored fairness notions. We then propose our empirical framework including our neural network instantiation called \model (Section~\ref{sec:framework}) and derive theoretical guarantees in terms of generalization bounds (Section~\ref{sec:theory}). In Section~\ref{sec:experiments}, we perform extensive numerical experiments using both simulated and real-world data to demonstrate the effectiveness of \model for fair off-policy learning. Finally, Section~\ref{sec:discussion} discusses implications for fair decision-making in research and practice.

\section{Related work}
\label{sec:related_work}

Our work is related to both off-policy learning and algorithmic fairness. We review both research streams in the following and describe how our work expands over the existing literature. 

\subsection{Off-policy learning}\label{sec:rw_off_policy_learning}

Off-policy learning typically aims to determine optimal policies from observational data by maximizing the so-called {policy value} \citep[e.g.,][]{Kallus.2018, Athey.2021}. The policy value is a causal quantity, which can be identified from observational data under certain assumptions (see Section~\ref{sec:problem_setting}). There are three standard methods for estimating the policy value: (1)~The direct method (DM) estimates the outcome functions and plugs them into the regression-adjustment formula \citep{Qian.2011, Bennett.2020}. (2)~The inverse propensity score weighted (IPW) method estimates the propensity score to re-weight the data \citep{Kallus.2018}. IPW usually performs better than DM when the propensity scores are easier to estimate than the outcome functions \citep{Curth.2021}. (3)~The doubly robust (DR) method is based on the efficient influence function of the policy value and estimates both the outcome functions and the propensity score \citep{Athey.2021, Chernozhukov.2022}. The advantage of DR is that it still provides a consistent estimator even if one of the two models for outcome functions or propensity score is misspecified \citep{Kennedy.2022}.

Several works propose extensions of the three standard methods for specific settings. For example, \citet{Kallus.2018c} and \citet{Bennett.2019b} develop methods to handle unobserved confounders. \citet{Kallus.2018d} consider continuous actions. \citet{Hatt.2022b} and \citet{Kallus.2022} learn policies under distribution shifts. Even other account for overlap violations \citep{Kallus.2021b}, interpretable policies \citep{Tschernutter.2022}, and long-term outcomes \citep{Yang.2023}. Different from our work, none of the above works deals with algorithmic fairness in off-policy learning.

\subsection{Algorithmic fairness for machine learning predictions} 

Extensive work has developed algorithmic fairness for machine learning predictions, which refers to computational approaches that enforce certain constraints on predictions so that similarly-situated individuals also receive similar predictions. In the following, we provide a brief overview about the different concepts and fairness notions. We refer to \citet{Chouldechova.2020} and \citet{Mitchell.2021} for a detailed overview. We emphasize that the following fairness notions are developed for predictions and \emph{not} for off-policy learning.

%Group level fairness

A major branch deals with fairness notions that aim to achieve that predictions do not systematically differ across groups that are defined by some sensitive attributes (e.g., gender or race). Such fairness notions at the group level are common in the context of classification problems, where the underlying idea is to modify the classification method in order to remove disparities between different sensitive groups \citep[e.g.,][]{Dwork.2012,Hardt.2016, Madras.2018,CorbettDavies.2023}. For example, this can be achieved by enforcing independence between the sensitive attribute and the predictions (i.e., statistical parity) or ensuring a similar classification performance for the different sensitive groups (e.g., similar type-I/II error rates). Approaches for group-level fairness have been extended to specific settings, such as for data with unobserved sensitive attributes \citep{Kallus.2019b} and for censored training data \citep{Kallus.2018f}. Beyond group-level fairness, there are also notions at the individual level as well as notions that are based on a causal lens; see \citet{Chouldechova.2020}. Note that, even though off-policy learning itself is a causal problem, our setting later is different from the literature on causal fairness: We introduce fairness to a specific causal decision problem (off-policy learning), whereas the standard literature on causal fairness uses causal theory (e.g., structural causal models) to define fairness notions \citep[e.g.,][]{Kilbertus.2017, Kusner.2017, Nabi.2018}.

A popular approach to achieve group-level fairness in practice is to remove the algorithmic bias incorporated in the training data by producing a new, fair representation of the data \citep[e.g.,][]{Zemel.2013, Locatello.2019}. Formally, one adapts neural networks to learn such a fair representation and then use the fair representation as input for the actual prediction model. For instance, statistical parity can be achieved by producing a new representation of the data that is non-predictive of the sensitive attributes using probabilistic models \citep{Creager.2019} or adversarial learning methods \citep{Madras.2018}.

\subsection{Algorithmic fairness for utility-based decision models}

A different literature stream has developed fairness notions that account for the utility of individuals who are subject to decisions. Such fairness notions have been integrated into traditional decision problems and thus outside of machine learning. Examples are, e.g., resource allocation \citep{Bertsimas.2011,Bertsimas.2013, Rea.2021b} and pricing \citep{Kallus.2021c, Cohen.2022}. Here, a common notion is envy-free fairness, which is fulfilled if an individual receives an allocation that has the same (or a higher) utility as the allocation of any other individual. Hence, decisions are envy-free if all players receive a share of resources that is equally good from their perspective \citep{Arnsperger.1994}. Another fairness notion is max-min fairness, which is grounded in Rawlsian justice and which seeks to maximize the minimum utility that a player can obtain \citep{Bertsimas.2011}. However, to the best of our knowledge, the aforementioned notions -- envy-free and max-min fairness -- have only been used for traditional resource allocations and have not yet been adapted for off-policy learning from observational data.      

Prior literature also considered algorithmic fairness in specialized settings. Examples are ranking tasks such as from recommender systems \citep[e.g.,][]{Singh.2019c} or risk-averse approaches to bound worst-case outcomes \citep[e.g.,][]{Fang.2022}. Even others consider algorithmic fairness in reinforcement learning. Here, fair policies can be obtained by customizing the reward function \citep{Jabbari.2017, Jiang.2019, Yu.2022} or by optimizing social welfare functions \citep{Siddique.2020, Zimmer.2021}. However, these work focus on Markov decision processes (MDPs), whereas we focus on learning policies in non-sequential settings not restricted to MDPs. 

\subsection{Algorithmic fairness for decision making from observational data}

Recently, some works aimed at learning fair decision rules from observational data. However, existing works have crucial differences from our paper as they make strong assumptions or have other critical restrictions. To this end, our paper is therefore the first to integrate algorithmic fairness in off-policy learning for general policies.   

One literature stream rests on the assumption that the structural causal graph of the decision problem is known and then seek to block specific causal pathways that are deemed unfair \citep{Nabi.2019,Nilforoshan.2022}. However, approaches from this literature stream require exact knowledge of the causal structure of the decision problem. That is, the underlying structural causal model of the data-generating process must be a~prior known. In contrast to that, we do not require knowledge of the underlying causal structure as this is rarely the case in practice.

Closest to our framework is the work by \citet{Viviano.2023}. Therein, the authors study fair off-policy learning but only for Pareto-optimal policies. As such, the work has two crucial differences to ours. First, \citet{Viviano.2023} are restricted to Pareto-optimal polices, whereas we study general policies. Because of this, both have different learning objectives, which is also why \citet{Viviano.2023} is \emph{not} applicable as a baseline in our work. On the contrary, our decision problem requires both tailored fairness notions and a tailored learning algorithm. Second, \citet{Viviano.2023} are limited to linear policies, whereas we later use neural networks to learn complex, non-linear policies. In sum, no work has -- to the best of our knowledge -- incorporated fairness in off-policy learning for general policies. This presents our contribution.

\section{Problem setting}\label{sec:problem_setting}

In this section, we review the decision problem in off-policy learning and further introduce custom fairness notions.

\subsection{Standard off-policy learning}

We build upon the standard setting for policy learning from observational data \citep[e.g.,][]{Kallus.2018,Athey.2021}. We consider observational data $(X_i, S_i, A_i, Y_i)_{i=1}^n$ sampled i.i.d. from a data-generating process $(X, S, A, Y) \sim \mathbb{P}$, which consists of covariates $X \in \mathcal{X} \subseteq \R^p$, sensitive attributes $S \in \mathcal{S}$, an action $A \in \{0,1\}$, and an outcome $Y \in \R$.\footnote{In the literature on causal machine learning, actions are oftentimes also called treatments \citep[e.g.,][]{Curth.2021}. Throughout our manuscript, we prefer the term ``action'' as it directly relates to the decision-making literature.} For example, in credit lending, one could model the credit score of an applicant by $X$, the gender or age as a sensitive attribute $S$, a decision $A$ whether to approve or reject the loan, and a profit $Y$ for the lending institution \citep{Kozodoi.2022}. The causal graph from our setting is shown in Fig.~\ref{fig:causal_graph}. Note that modeling the action $A$ as a binary variable is consistent with previous literature \citep[e.g.,][]{Kallus.2018, Kallus.2018c, Athey.2021, Hatt.2022b} and is common for decision-making in a wide range of practical applications such as, e.g., automated hiring, credit lending, and ad targeting \citep[e.g.,][]{Smith.2022, Yoganarasimhan.2022, Kozodoi.2022}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.15\linewidth]{figures/causal_graph.pdf}
\caption{The causal graph for our problem setting. Both the covariates $X$ and the sensitive attribute $S$ may act as confounders for the relationship between action $A$ and outcome $Y$. We also allow for arbitrary causal relationships between $X$ and $S$.}
\label{fig:causal_graph}
\end{figure}

We make use of the Neyman-Rubin potential outcomes framework \citep{Rubin.1978} and denote $Y(a)$ as the potential outcome, which would have been observed if the action had been set to $A = a$. Formally, a policy is a measurable function $\pi \colon \mathcal{X} \times \mathcal{S} \to [0,1]$, which maps an individual with covariates $(X, S)$ onto a probability of receiving an action. The policy value of $\pi$ is then defined as
\begin{equation}
V(\pi) = \E[Y^\pi] = \E[\pi(X, S) \, Y(1) + (1 - \pi(X, S)) \, Y(0)].
\end{equation}
Note that we cannot directly estimate the policy value because, for each observation, only one of the potential outcomes is observed in the observational data. This is known as the fundamental problem of causal inference \citep{Pearl.2009}. However, we can impose the following standard assumptions in order to identify the policy value $V(\pi)$ from observational data \citep{Rubin.1974}.

\begin{assumption}[Standard causal inference assumptions]\label{ass:identification}
\label{ass:iv}
We assume: (i)~\emph{consistency:} $Y(A) = Y$; (ii)~\emph{positivity:} $0 < \mathbb{P}(A=1 \mid X = x) < 1$ for all $x \in \mathcal{X}$; and (iii)~\emph{strong ignorability:} $Y(0), Y(1) \indep A \mid X$.
\end{assumption}

\noindent
Under Assumption~\ref{ass:identification}, the policy value is identified by
\begin{equation}
    V(\pi) = \E_\mathcal{D}[\psi^\mathrm{m}(\pi, \mathcal{D})],
\end{equation}
with observational data $\mathcal{D} = (X, S, A, Y)$ and where $\psi^\mathrm{m}(\pi, \mathcal{D})$ is one of the following three policy scores (see Section~\ref{sec:rw_off_policy_learning}):
\begin{align}
\psi^{\mathrm{DM}}(\pi, \mathcal{D}) &= \pi(X, S) \, \mu_1(X, S) + (1 - \pi(X, S)) \, \mu_0(X, S) , \label{eq:pscore}\\
\psi^{\mathrm{IPW}}(\pi, \mathcal{D}) &= \frac{A \, \pi(X, S) + (1 - A) \, (1 - \pi(X, S))}{A \, \pi_b(X, S) + (1 - A) \, (1 - \pi_b(X, S))}Y , \qquad \text{ and } \label{eq:pscore_ipw}\\
\psi^{\mathrm{DR}}(\pi, \mathcal{D}) &= \psi^{\mathrm{DM}}(\pi, \mathcal{D}) + \frac{A \, \pi(X, S) + (1 - A) \, (1 - \pi(X, S)) }{A \, \pi_b(X, S) + (1 - A) \, (1 - \pi_b(X, S))} \left(Y - \mu_A(X, S)\right),\label{eq:pscore_dr}
\end{align}
which refer to the direct method (DM), the inverse propensity score weighted (IPW) method, and the doubly robust (DR) method, and where $\mu_j(X, S) = \E[Y \mid X, S, A = j], j \in \{0,1\}$, are the outcome regression functions and where $\pi_b(X, S) = \mathbb{P}(A = 1 \mid X, S)$ is the so-called {propensity score} (i.e., behavioral policy).

In standard off-policy learning, the objective is to find a policy from observational data that maximizes the policy value via 
\begin{equation}\label{eq:unfair}
    \pi^{{\mathrm{uf}}} \in \argmax_{\pi \in \Pi} V(\pi),
\end{equation}
where $\Pi$ is some predefined class of policies. For example, $\Pi$ may contain all policies parameterized by some neural network. Any policy that satisfies Eq.~\eqref{eq:unfair} is an \emph{optimal unrestricted policy}, as it does not give any special considerations to the sensitive covariates $S$ when maximizing the policy value. In special cases, the optimal unrestricted policy might coincide with fairness restricted policies (see next section). In practice however, not enforcing any restrictions will in most situations lead to discrimination against sensitive groups in order to maximize the overall policy value.

\subsection{Fairness notions for off-policy learning}

We now introduce different fairness notions that are tailored to off-policy learning. Specifically, fairness may enter off-policy learning at two different stages, namely with respect to the action and the policy value. We refer to them as ``action fairness'' and ``value fairness'', respectively. The former, action fairness, prohibits discrimination with respect to the selected action, while the latter, value fairness, prohibits discrimination with respect to the expected utility (i.e., the policy value). Both have inherent advantages in practice, and the actual choice depends on the underlying goals as well as legal and ethical context. As an illustrative example, action fairness may beneficial in credit lending where, ceteris paribus, a company may be required by law approve loans from women and men with equal probability. In contrast, value fairness may be used by governments that award scholarships to broaden university access and thus to promote social advances. Here, the net-benefit of such scholarship is typically much greater for individuals from historically underrepresented groups, so that decision-makers may prefer to consider the utility of such scholarships. We formally define both fairness notions -- action fairness and policy fairness -- in the following.

\subsubsection{Action fairness.}

The objective in action fairness is that individuals with different sensitive attributes but otherwise equal characteristics receive the same action. For example, in credit lending, this implies that a policy $\pi$ should assign the same action $A = A'$ to two individuals with the same covariates $X = X'$ but different gender $S \neq S'$. In other words, the choice of actions should be independent of the sensitive attribute. We formalize this in the following definition. 

\vspace{0.2cm}
\begin{definition}[Action fairness]
\emph{A policy $\pi^{\mathrm{af}} \in \Pi$ fulfills action fairness if it is not a function of $S$ and $\pi^{\mathrm{af}}(X) \indep S$, that is, the recommended action should be independent of the sensitive attribute. A policy $\pi^{\mathrm{af}}$ that fulfills action fairness is optimal if it satisfies
\begin{equation}\label{eq:af}
    \pi^{\mathrm{af}} \in \argmax_{\pi \in \Pi_{\mathrm{af}}} V(\pi), 
\end{equation}
where $\Pi_{\mathrm{af}} = \{\pi \in \Pi \,|\, \pi \textrm{ fulfills action fairness}\}$.}
\end{definition}
\vspace{0.2cm}

Action fairness ensures that individuals, which only differ with respect to their sensitive attributes (and covariates correlated to them), receive the same decision. Action fairness thus ensures that the policy cannot exploit information contained within the sensitive attributes. As such, action fairness may be relevant in many applications such as hiring or credit lending where legal frameworks mandate that decisions may not discriminate by certain sensitive attributes \citep{Barocas.2016, Kleinberg.2019}.

\subsubsection{Value fairness.}

The rationale behind {value fairness} is that different sub-populations defined by the sensitive attribute may benefit differently from a policy. Hence, we now express fairness with respect to the policy value and thus ensure that individuals with different sensitive attributes achieve, on average, a similar utility. Generally, value fairness may be desired in practice where social welfare shall be considered. Examples can be governments that award scholarships, job trainings, or free health insurance with the aim to strategically promote historically underrepresented groups but where the underrepresented groups benefit from such treatments at an overproportionate rate. 

To formalize value fairness, let us denote the conditional policy value $V_s(\pi) = \E[\psi^\mathrm{m}(\pi, \mathcal{D}) \mid S = s]$, where we condition on the sensitive attribute $S = s$. In the following, we introduce two variants of value fairness -- i.e., (1)~{envy-free fairness} and (2)~{max-min fairness} -- with different aims. The former, envy-free fairness, ensures that the conditional policy values $V_s(\pi)$, $s \in \{ 0, 1 \}$, do not differ more than some predefined level $\alpha$ between the sub-populations. The latter, max-min fairness, ensures that the worst-case conditional policy value across sub-populations is being maximized.

%  inspired by other decision problems \citep{Bertsimas.2011}

\vspace{0.2cm}
\begin{definition}[Envy-free fairness]
\emph{A policy $\pi \in \Pi$ fulfills envy-free fairness with level $\alpha \geq 0$ if $|V_s(\pi) - V_{s^\prime}(\pi)| \leq \alpha \textrm{ for all } s, s^\prime \in S$. We denote the set of envy-free policies by $\Pi(\alpha) = \{\pi \in \Pi \,|\, \pi \textrm{ is envy free with level } \alpha \}$. An envy-free policy $\pi^\alpha$ is optimal if
\begin{equation}\label{eq:envy_free}
    \pi^\alpha \in \argmax_{\pi \in \Pi(\alpha)} V(\pi).
\end{equation}}
\end{definition}

\vspace{0.2cm}
\begin{definition}[Max-min fairness]
\emph{A policy $\pi^{\mathrm{mm}} \in \Pi$ fulfills max-min fairness if it minimizes the worst-case policy value for the sensitive attributes, that is,
\begin{equation}\label{eq:maxmin}
    \pi^{\mathrm{mm}} \in \argmax_{\pi \in \Pi} \inf_{s \in S} V_s(\pi).
\end{equation}}
\end{definition}
\vspace{0.2cm}

%Comparison of value fairness with the literature
The above definitions of value fairness are inspired by previous literature on resource allocation \citep[e.g.,][]{Arnsperger.1994, Bertsimas.2011}, yet we carefully adapt them here to off-policy learning, that is, learning from observational data. Envy-free fairness allows decision-makers to control for disparities in the utility between the sensitive groups by fixing $\alpha$. Max-min fairness does not limit disparities in the utility between both sensitive groups but, instead, seeks the best possible worst-case policy value. It can further be shown that max-min fairness is the unique notion that satisfies a collection desirable properties such as Pareto-optimality and affine invariance \citep{Bertsimas.2011}. 

\subsubsection{Combining action fairness and value fairness.}
\label{sec:combination}

Both action fairness and value fairness can be combined in off-policy learning so that the obtained policies fulfill both notions at the same. To this end, one simply replaces the policy class $\Pi$ in Eq.~\eqref{eq:envy_free} and Eq.~\eqref{eq:maxmin} by $\Pi_{\mathrm{af}}$. This thus restricts the policy class to all policies that fulfill action fairness and, as a result, one obtains policies that fulfill both notions. 

Combining fairness notions has also theoretical implications, which we discuss in the following. In fact, it turns out that the notion of max-min fairness only yields a useful fairness notion when it is used in combination with action fairness. We show this in the following Lemma~\ref{lem:maxmin_unfair}. The lemma essentially states that the set of policies that fulfill our notion of max-min fairness is so wide that it includes some optimal unrestricted policies. For practice, this means that decision-makers interested in max-min fairness will additionally need to enforce action fairness. We also reflect this later in our experiments where all evaluations based on max-min fairness are in combination with action fairness.

\begin{lemma}\label{lem:maxmin_unfair}
Let $\Pi$ the set of all measurable policies $\pi \colon \mathcal{X} \times \mathcal{S} \to [0,1]$. Then, there exists a policy that fulfills max-min fairness, i.e., $\pi^\ast \in \argmax\limits_{\pi \in \Pi} \inf\limits_{s \in S} V_s(\pi)$ which is also an optimal unrestricted policy (i.e., a solution to Eq.~(\ref{eq:unfair})).
\end{lemma}
\proof{Proof.}
For each sensitive attribute $s \in \mathcal{S}$, we construct $\pi^\ast(\cdot, s) \in \argmax_{\pi \in \Pi_{\mathcal{X}}} V_s(\pi)$, where $\Pi_{\mathcal{X}} = \{\pi \colon \mathcal{X} \to [0,1] \mid \pi \textrm{ measurable} \} $. By definition, it holds that $V_s(\pi) \leq V_s(\pi^\ast)$ for any policy $\pi \in \Pi$ and, hence, $\inf_{s \in S} V_s(\pi) \leq \inf_{s \in S} V_s(\pi^\ast)$, which means that $\pi^\ast$ is a policy fulfilling max-min fairness. At the same time, due to $V_s(\pi) \leq V_s(\pi^\ast)$, it holds that
\begin{equation}
    V(\pi) = \int_{\mathcal{S}} V_s(\pi) \, \mathbb{P}(S = s) \,  \diff s \leq \int_{\mathcal{S}} V_s(\pi^\ast) \, \mathbb{P}(S = s) \diff s = V(\pi^\ast).
\end{equation}
Thus, the policy $\pi^\ast$ is an optimal unrestricted policy. \hfill\Halmos
\endproof
\vspace{0.2cm}

We now turn to the relationship between envy-free fairness and max-min fairness when combined with action fairness. As it turns out, under action fairness and some further conditions, max-min fairness can be seen as a special case of envy-free fairness with $\alpha = 0$. This is stated in Lemma~\ref{lem:envy_maxmin}. 

\vspace{0.2cm}
\begin{lemma}\label{lem:envy_maxmin}
Let $\mathrm{ITE}(x, s^\ast) = \mu_1(x, s^\ast) - \mu_0(x, s^\ast)$ denote the individual treatment effect for an individual with covariates $(x, s^\ast)$. We further assume that $\mathcal{S} = \{0, 1\}$ is binary, and let $(\pi^{\mathrm{mm}}, s^\ast)$ be a solution to Eq.~\eqref{eq:maxmin}, and let $\pi^{\mathrm{mm}}(x)$ fulfill action fairness. Furthermore, we assume that there exists a set of covariates $V \subseteq \mathcal{X}$ with $\mathbb{P}(X \in V \mid S = s) > 0$ such that: either $\mathrm{ITE}(x, s^\ast) > 0$ and $\pi^{\mathrm{mm}}(x) < 1$; or $\mathrm{ITE}(x, s^\ast) < 0$ and $\pi^{\mathrm{mm}}(x) > 0$ for all $x \in V$, $s \in \mathcal{S}$. Then, $\pi^{\mathrm{mm}}$ fulfills envy-free fairness with $\alpha=0$. Further, all optimal policies that both satisfy action fairness and envy-free fairness with $\alpha = 0$ also fulfill max-min fairness.
\end{lemma}
\proof{Proof.}
We first show that $V_0(\pi^{\mathrm{mm}}) = V_{1}(\pi^{\mathrm{mm}})$, i.e., $\pi^{\mathrm{mm}}$ is envy-free with $\alpha=0$. Let us assume w.l.o.g. that $V_0(\pi^{\mathrm{mm}}) < V_{1}(\pi^{\mathrm{mm}})$. By our assumption, we can find an $\epsilon > 0$ such the policy $\pi^\prime$ defined by
\begin{equation}
   \pi^\prime(x) = \left\{
\begin{array}{ll}
\pi^{\mathrm{mm}}(x) + \epsilon \, \mathrm{sign}\{\mathrm{ITE}(x, 0)\} , \qquad & \quad \textrm{if } x \in V, \\
\pi^{\mathrm{mm}}(x) , & \quad \textrm{if } x \in \mathcal{X} \setminus V, \\
\end{array}
\right.
\end{equation}
satisfies $V_{1}(\pi^\prime) > V_0(\pi^{\mathrm{mm}})$. By construction of $\pi^\prime$ and our assumption, we yield
\begin{align}
V_{0}(\pi^\prime) &= \int_\mathcal{X} \, \pi^\prime(x) \, \mathrm{ITE}(x, 0) \, \mathbb{P}(x \mid S=0) + \mu_0(x, 0) \, \mathbb{P}(x \mid S=0) \, \diff x \\ &> \int_\mathcal{X} \pi^{\mathrm{mm}}(x) \, \mathrm{ITE}(x, 0) \, \mathbb{P}(x \mid S=0) + \mu_0(x, 0) \, \mathbb{P}(x \mid S=0) \,  \diff x = V_0(\pi^{\mathrm{mm}}). 
\end{align}
This implies
\begin{equation}
    \min\{V_{0}(\pi^\prime), V_{1}(\pi^\prime)\} > \min\{V_{0}(\pi^{\mathrm{mm}}), V_{1}(\pi^{\mathrm{mm}})\},
\end{equation}
which is a contradiction to the assumption that $\pi^{\mathrm{mm}}$ fulfills max-min fairness. Hence, $\pi^{\mathrm{mm}}$ fulfills envy-free fairness.

Now, let $\pi^{0}$ be an optimal policy that satisfies both action fairness and envy-free fairness. Let us further assume that $\pi^{0}$ does not fulfill max-min fairness. We then yield
\begin{equation}
    V(\pi^{0}) = \mathbb{P}(S = 0) \, V_0(\pi^{0}) + \mathbb{P}(S = 1) \, V_1(\pi^{0}) < \mathbb{P}(S = 0) \, V_0(\pi^{\mathrm{mm}}) + \mathbb{P}(S = 1) \, V_1(\pi^{\mathrm{mm}}) = V(\pi^{\mathrm{mm}}),
\end{equation}
which is a contradiction because $\pi^{\mathrm{mm}}$ fulfills envy-free fairness and $\pi^{0}$ is optimal. 
\hfill\Halmos
\endproof
\vspace*{0.2cm}

We provide an additional discussion on the assumptions from Lemma~\ref{lem:envy_maxmin} in Appendix~\ref{app:assumptions}.


\subsubsection{Toy example.}

In the following, we provide a toy example based on which we discuss the differences between the above fairness notions. Specifically, we consider algorithmic decision-making in credit lending where applications for student loans are evaluated. We consider two covariates for students, namely their gender and average grade (GPA). These are given by $\mathit{Gender} \in \{\textrm{Female} , \textrm{Male}\}$ and $\mathit{GPA} \in \{\textrm{Low} , \textrm{High}  \}$. We consider $\mathit{Gender}$ as the sensitive attribute $S$. The outcome $Y$ is the expected change in salary, that is, whether it increases ($=1$), remains the same ($=0$), or decreases ($=-1$) as a result of the study program. For the purpose of our toy example, we make further assumptions regarding the distribution of covariates and expected outcomes. To this end, Table~\ref{t:toy_ex1} reports the probability of observing an individual from each sub-population (column~3), the outcome when a student receives the loan ($\mu_1$), and the outcome when a student does not receive the loan ($\mu_0$). Then, the overall effect of the action (i.e., the student loan) is given by $\mu_1 - \mu_0$. As can be seen, the action of receiving a loan benefits males with a high GPA while it has a negative effect for all other groups.  

% Unfair vs action fair

In Table~\ref{t:toy_ex1}, we report the policy value under different decision policies. (Details for calculating the policy values in our toy example are in Appendix~\ref{app:toy_example}). First, we report the optimal unrestricted policy ($\pi^{\mathrm{u}}$). This policy gives student loans only to males with high GPA but not to any other student. The reason is that the the sub-population of males with high GPA is the only one with a positive effect (i.e., $\mu_1 - \mu_0 = 1$). Second, we report an optimal policy under action fairness ($\pi^{\mathrm{af}}$). It chooses the same action for both males and females with high (low) GPA. Hence, the action taken by $\pi^{\mathrm{af}}$ does not depend on gender and thus fulfills action fairness. Third, envy-free fairness ($\pi^\alpha$) and max-min fairness ($\pi^{\mathrm{mm}}$) assign loans only to males with a high GPA. In particular, the max-min policy coincides with the optimal unrestricted policy, as implied by Lemma~\ref{lem:maxmin_unfair}.

\begin{table*}[ht]
\caption{Toy example comparing the different fairness notions for off-policy learning.}
\centering
\label{t:toy_ex1}
\footnotesize
\begin{tabular}{P{0.8cm} P{0.8cm} P{1.5cm} P{1.3cm} P{1.3cm} P{0.6cm} P{0.6cm} P{0.6cm} P{0.6cm} P{1.3cm} P{1.3cm}}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\multicolumn{3}{c}{\textbf{Data}}& \multicolumn{2}{c}{\textbf{Expected outcome }} & \multicolumn{4}{c}{\textbf{Policies}} & \multicolumn{2}{c}{\textbf{Combined policies}} \\
& & & & & & & & & \multicolumn{2}{c}{(with action fairness)} \\
\cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
Gender & GPA & Probability & $\mu_1$ & $\mu_0$ & $\pi^{\mathrm{u}}$ & $\pi^{\mathrm{af}}$ & $\pi^{\mathrm{\alpha}}$ & $\pi^{\mathrm{mm}}$ &  $\pi^{\alpha}$ & $\pi^{\mathrm{mm}}$\\
\midrule
Female & Low& $0.1$ & $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
Male& Low & $0.4$& $0$ & $1$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
Female& High & $0.1$ & $-1$& $1$ & $0$ & $1$ & $0$ &$0$ & $\frac{\alpha + 1}{3}$& $\frac{1}{3}$ \\
Male & High & $0.4$& $1$ & $0$ & $1$ & $1$ & $1$ &$1$ &$\frac{\alpha + 1}{3}$ &$\frac{1}{3}$\\
\bottomrule
\multicolumn{11}{l}{\tiny \emph{Legend}: $\pi^{\mathrm{u}}$: optimal unrestricted; $\pi^{\mathrm{af}}$: action fairness; $\pi^{\alpha}$: envy-free fairness; $\pi^{\mathrm{mm}}$: max-min fairness}
\end{tabular}
\end{table*}

% max-min and envy-free
We further consider policies for envy-free fairness and max-min fairness that are combined with action fairness, so that always both action fairness and value fairness are satisfied (columns 10 and 11). Here, the policies assign actions to males and females with high GPA in order to fulfill action fairness. In addition, both polices assign actions only for a fraction of the overall population. This is seen by the fact that the policy outputs are $\frac{\alpha+1}{3}$ and $\frac{1}{3}$, respectively, and thus below 1. We further note that some of the polices can coincide as stipulated in Lemma~\ref{lem:envy_maxmin}. For $\alpha = 0$, the policy combining action fairness and envy-free fairness is identical to the policy combing action fairness and max-min fairness. For $\alpha = 2$, the policy combining action fairness and envy-free fairness is identical to policy for action fairness ($\pi^{\mathrm{af}}$).

\section{Empirical framework for off-policy learning}\label{sec:framework}

In this section, we describe our empirical framework for estimating fair policies from finite observational data. We first give a general overview (Section~\ref{sec:overview}) outlining the different ways how we incorporate action fairness and value fairness. We then state the underlying learning objectives (Section~\ref{sec:obj_value}). Finally, we propose an instantation of our framework based on machine learning, which we call \model (Section~\ref{sec:model}). 

\subsection{Overview} 
\label{sec:overview}

In our framework, we aim to determine a policy $\pi \in \Pi$ that maximizes the empirical policy value $\hat{V}^\mathrm{m}(\pi)$ under different fairness notions, namely (1)~action fairness which prohibits a direct influence from the sensitive attributes on the actions and (2)~value fairness which takes the utility for each sensitive group into account. We address both in different ways. 

For (1)~action fairness, we restrict the underlying policy class. Specifically, we search our policy no longer of a general class of policies given by $\Pi$ but over a subset of policies $\Pi_\mathrm{af} \subseteq \Pi$ that full action fairness. To obtain $\Pi_\mathrm{af}$, we build upon the idea of fair representation learning \citep[e.g.,][]{Zemel.2013,Madras.2018} but adapt it to our task of fair off-policy learning. Specifically, we map our input data $X$ onto a new representation $\Phi(X)$ via some function $\Phi(\cdot)$. Here, the representation must aim for two objectives. On the one hand, it should generate data that is independent of the sensitive attributes $S$, that is, $\Phi(X) \indep S$. On the other hand, the new representation should still be predictive of the outcome $Y$. We thus later yield two, adversarial objectives that we optimize using an iterative gradient-based solver. Later, we model $\Phi$ via neural networks due to their flexibility for modeling complex, non-linear relationships. 

For (2)~value fairness, we change the underlying learning objective. Specifically, we no longer optimize against the empirical policy value, but we reformulate the optimization problem so that we optimize against policies that fulfill envy-free fairness or max-min fairness. We present the objectives in the next section, which we then numerically optimize using gradient-based methods. Notwithstanding, both action fairness and value fairness can be combined by simply following both of the above ways, so that one restricted the policy class for action fairness and then changes the learning objective for value fairness. 


Finally, we combine both (1) and (2) and propose a machine learning instantiation of \model. Therein, we model $\Phi$ via neural networks as able to model complex non-linear relationships, and we further use iterative gradient-based solvers for solving the underlying optimization problems. 

% and allow us to construct fair representation by using an adversarial objective.


\subsection{Learning objectives}
%\subsection{Empirical policy values}
\label{sec:obj_value}

We now specify the learning objectives in our framework and how these vary according to the different notions for value fairness. In expectation, the policy value is defined as $V(\pi) = \E_\mathcal{D}[\psi^\mathrm{m}(\pi, \mathcal{D})]$, where $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$. Further, the conditional policy value is defined as $V_s(\pi) = \E[\psi^\mathrm{m}(\pi, \mathcal{D}) \mid S = s] = \E[\psi^\mathrm{m}(\pi, \mathcal{D}) \frac{\mathbbm{1}(S = s)}{\mathbb{P}(S = s)}]$, where $\mathbbm{1}(\cdot)$ denotes the indicator function. Hence, we can estimate these quantities by replacing the expectations with finite sample averages. Then, the empirical policy value becomes
\begin{equation}\label{eq:pvalue_est}
\hat{V}^\mathrm{m}(\pi) = \frac{1}{n} \sum_{i=1}^n \psi^\mathrm{m}(\pi, \mathcal{D}_i) ,
\end{equation}
and the empirical conditional policy value becomes
\begin{equation}\label{eq:pvalue_cond_est}
\hat{V}^\mathrm{m}_s(\pi) = \frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{\hat{p}_n(s)} \, \psi^\mathrm{m}(\pi, \mathcal{D}_i) \quad \textrm{with} \quad \hat{p}_n(s) = \frac{\sum_{j=1}^n \mathbbm{1}(S_j = s)}{n}.
\end{equation}
By the law of large numbers and Slutsky's lemma, $\hat{V}^\mathrm{m}(\pi)$ and $\hat{V}^\mathrm{m}_s(\pi)$ are consistent for $V(\pi)$ and $V_s(\pi)$. The optimal unconstrained policy can be obtained via
\begin{equation}
    \hat{\pi} \in \argmax_{\pi \in \Pi} \hat{V}^\mathrm{m}(\pi).
\end{equation}

We now state the learning objectives for (1)~envy-free fairness and (2)~max-min fairness:
\begin{quote}\begin{quote}
\begin{enumerate}
\item For envy-free fairness, we reformulate the optimization problem from Eq.~\eqref{eq:envy_free} over the class of envy-free policies into an optimization problem over an unconstrained policy class. We further replace the population quantities $V(\pi)$ and $V_s(\pi)$ with their corresponding estimates $\hat{V}^\mathrm{m}(\pi)$ and $\hat{V}^\mathrm{m}_s(\pi)$. We thus yield
\begin{equation}\label{eq:envyfree_est}
    \hat{\pi}^\lambda \in \argmax_{\pi \in \Pi} \hat{V}^\mathrm{m}_\lambda(\pi) \quad \textrm{with} \quad \hat{V}^\mathrm{m}_\lambda(\pi) = \hat{V}^\mathrm{m}(\pi)  - \lambda \max_{s, s\prime \in \mathcal{S}} |\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi)|,
\end{equation}
where $\lambda > 0$ is a hyper-parameter, controlling the envy-free fairness: higher values correspond to fairer policies.
\item For max-min fairness, we proceed analogously. We thus yield
\begin{equation}\label{eq:maxmin_est}
    \hat{\pi}^{\mathrm{mm}} \in \argmax_{\pi \in \Pi} \min_{s \in S} \hat{V}^\mathrm{m}_s(\pi) ,
\end{equation}
\end{enumerate}
so that we yield the best worst-case policy value. 
\end{quote}\end{quote}


\subsection{Neural network instantiation}
\label{sec:model}


We instantiate our framework \model based on neural networks to learn fair policies. In \model, we make use of neural networks for several reasons. First, neural networks provide a flexible machine learning model that allows us to learn complex, non-linear relationships. Second, neural networks have further achieved a state-of-the-art performance in many practical applications \citep[e.g.,][]{Kraus.2020}. Third, they allow for a principled approach to address adversarial learning.  

Our \model proceeds along two steps (see Figure~\ref{fig:model}): (1)~We first learns a fair representation $\Phi \colon \mathcal{X} \to \R^k$ of the data such that $\Phi(X) \indep S$ but $\Phi(X)$ is still predictive of the outcome $Y$, where $k$ is the size of the representation. This ensures that any policy based on $\Phi(X)$ satisfies action fairness but is still effective in achieving a large policy value. In our implementation, we parameterize $\Phi$ with a neural network that is trained with two adversarial objectives. As result, $\Phi$ essentially yields gives a policy class that is restricted to all polices with action fairness, that is, $\Pi^\Phi_\mathrm{af} = \{\pi_\theta \circ \Phi \mid \theta \in \Theta\}$. (2)~We then optimize against the learning objectives from the previous section. Specifically, the learning objectives vary depending on whether we have no value fairness (Eq.~\eqref{eq:pvalue_est}), envy-free fairness (Eq.~\eqref{eq:envyfree_est}), or max-min fairness (Eq.~\eqref{eq:maxmin_est}). Depending on whether action fairness is enforced, the learning objectives then optimize overall all policies in $\Pi$ or the subset of policies with action fairness (i.e., $\Pi^\Phi_\mathrm{af}$). We formalize both steps in the following.  


\begin{figure}[ht]
\centering
\includegraphics[width=0.55\linewidth]{figures/model_architecture.pdf}
\caption{Our \model framework instantiated with neural networks.}
\label{fig:model}
\end{figure}

\subsubsection{Step 1: Fair representation learning.}

We use three feed-forward neural networks to learn the representation $\Phi$: (1)~a base representation network $\Phi_{\theta_\Phi}$ that takes the non-sensitive attributes $X$ as input and outputs the representation, (2)~an outcome prediction network $G^Y_{\theta_Y}$ that predicts the outcome $Y$ based on the representation $\Phi$, and (3)~a sensitive attribute network $G^S_{\theta_S}$ that predicts the sensitive attribute $S$ based on the representation. Here $\theta_\Phi$, $\theta_Y$, and $\theta_S$ denote the neural network parameters. The base representation network $\Phi_{\theta_\Phi}$ serves as basis to construct the fair representation, while $G^Y_{\theta_Y}$ and $G^S_{\theta_S}$ allow us to ensure predictiveness of $Y$ and non-predictiveness of $S$.

We proceed as follows to find the optimal parameters $\hat{\theta}_\Phi$, $\hat{\theta}_Y$, and $\hat{\theta}_S$. We optimize an objective consisting of three parts: 
\begin{quote}\begin{quote}
\begin{enumerate}
\item The outcome loss $\mathcal{L}_Y$ ensures that to our representation $\Phi$ and the outcome prediction network are predictive of the outcome $Y$. For this, we minimize 
\begin{equation}\label{eq:loss_outcome}
\mathcal{L}_Y(\theta_\Phi, \theta_Y) = \frac{1}{n} \sum_{i=1}^n \left(G^Y_{\theta_Y}\left(\Phi_{\theta_\Phi}(X_i)\right) - Y_i \right)^2 .
\end{equation}
\item The sensitivity loss $\mathcal{L}_S$ learns the parameters of the sensitive attribute network, i.e., $G^S_{\theta_S}$, so that it is predictive of $S$. We thus minimize
\begin{equation}\label{eq:loss_sens}
    \mathcal{L}_S(\theta_\Phi, \theta_S) = \frac{1}{n} \sum_{i=1}^n\mathrm{CE}\left(G^S_{\theta_S}\left(\Phi_{\theta_\Phi}(X_i)\right), S_i\right),
\end{equation}
where $\mathrm{CE}$ denotes categorical cross-entropy. 
\item The confusion loss  $\mathcal{L_\mathrm{conf}}$, guided by the sensitive attribute network, aims to render the representation $\Phi$ non-predictive of $S$. We thus minimize
\begin{equation}\label{eq:loss_confusion}
    \mathcal{L_\mathrm{conf}}(\theta_\Phi, \theta_S) = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^{|\mathcal{S}|} - \frac{1}{|\mathcal{S}|}\log \left(G^S_{\theta_S}\left[\Phi_{\theta_\Phi}(X_i)\right]^j\right),
\end{equation}
where $[\cdot]^j$ is a $j$-th element of a vector. 
\end{enumerate}
\end{quote}\end{quote}
Both the sensitivity and the confusion losses are adversarial to each other. This is crucial: the sensitive attribute network $G^S_{\theta_S}$ is trained to correctly classify the sensitive attribute by minimizing $\mathcal{L}_S(\theta_\Phi, \theta_S)$ with respect to $\theta_S$, while the base representation network $\Phi_{\theta_\Phi}$ tries to ``confuse'' the sensitive attribute network by minimizing $\mathcal{L_\mathrm{conf}}(\theta_\Phi, \theta_S)$ with respect to $\theta_\Phi$, i.e., forcing the sensitive attribute network to predict a uniform distribution of the sensitive attribute. This ensures that the learned representation becomes non-predictive of the sensitive attribute $S$. Taken together, the overall objective is
\begin{equation}\label{eq:loss_overall}
    \hat{\theta}_\Phi, \hat{\theta}_Y = \argmin_{\theta_\Phi, \theta_Y}  \mathcal{L}_Y(\theta_\Phi, \theta_Y) + \gamma \mathcal{L_\mathrm{conf}}(\theta_\Phi, \hat{\theta}_S); \qquad \qquad \hat{\theta}_S = \argmin_{\theta_S}\gamma \mathcal{L}_S(\hat{\theta}_\Phi, \theta_S);
\end{equation}
where $\gamma$ is a parameter that weights the different parts in the loss function. The objective in Eq.~\eqref{eq:loss_overall} is also known as the counterfactual domain confusion loss \citep{Melnychuk.2022}. We later train the two adversarial objectives from Eq.~\eqref{eq:loss_overall} via iterative gradient-based optimization. For further details on our learning algorithm, we refer to Appendix~\ref{app:algorithm}.

\subsubsection{Step 2: Optimization of empirical policy value.}

The second step is to optimize the empirical policy value. Hence, once the representation $\hat{\Phi} = \Phi_{\hat{\theta}_\Phi}$ is trained, we optimize our objectives for value fairness over the policy class $\Pi^{\hat{\Phi}}_\mathrm{af} = \{\pi_\theta \circ \hat{\Phi} \mid \theta \in \Theta\}$. Here, we propose to parametrize $\pi_\theta$ with a neural network with parameters $\theta \in \Theta$ that takes the representation $\hat{\Phi}(X)$ as input and outputs a policy recommendation $\pi_\theta(\hat{\Phi}(X)) \in [0,1]$. Here, the use of neural networks is again beneficial as it allows us to handle high-dimensional input $X$ and further obtain complex, non-linear policies. Formally, we thus optimize the policy via
\begin{equation}\label{eq:learning_objectives}
\max_{\theta \in \Theta} \hat{V}^\mathrm{m}(\pi_\theta), \qquad
    \max_{\theta \in \Theta} \hat{V}^\mathrm{m}_\lambda(\pi_\theta), \qquad \textrm{or} \qquad \max_{\theta \in \Theta} \min_{s \in S} \hat{V}^\mathrm{m}_s(\pi_\theta),
\end{equation}
depending on whether there is no value fairness\footnote{Note that the policies that fulfill no value fairness are either the optimal unrestricted policy or the policy that fulfills action fairness.}, envy-free fairness, or max-min fairness, respectively. 

\subsection{Implementation details}

In our implementation, we use feed-forward neural networks with dropout and exponential linear unit activation functions for each neural network in model. We use Adam \citep{Kingma.2015} for the optimization in both step 1 and 2. We further follow best-practice for hyperparameter tuning. To this end, we first split the data into a training and validation set, and we then perform hyperparameter tuning using a grid search following best practices. All evaluations are based on the test set, so that we capture the out-of-sample performance on unseen data. {Code is in the supplementary materials and available at \url{https://anonymous.4open.science/r/FairPol-163C}.}

Additional details for our framework are in Appendix~\ref{app:hyperparam}. Of note, \model is applicable to all policy scores $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$, namely, the direct method (DM), the inverse propensity score weighted (IPW) method, and the doubly robust (DR) method. Here, we refer to Eq.~\eqref{eq:pscore}, Eq.~\eqref{eq:pscore_ipw}, and Eq.~\eqref{eq:pscore_dr}, respectively, for the definitions.

\section{Theoretical results}
\label{sec:theory}

In the following, we derive generalization bounds for the finite-sample version of our framework. Specifically, we quantify the deviation of the proposed finite-sample policy estimators from their respective population quantities. We later provide derivations for the three policy objectives, namely no value fairness (Theorem~\ref{thrm:bound:uf}), envy-free-fairness (Theorem~\ref{thrm:bound:envy_free}), and max-min fairness (Theorem~\ref{thrm:bound:max_min}). Note that the derivations also hold for action fairness where one would simply need to replace $\Pi$ by $\Pi_{\mathrm{af}}$. As a first step, we impose the following boundedness assumptions.

\vspace{0.2cm}
\begin{assumption}[Boundedness]\label{ass:boundedness}
We assume there exist constants $C, \xi, \nu > 0$ such that (i) the outcomes are bounded with $|Y| \leq C$ almost surely, (ii) the propensity score is bounded away from $0$ and $1$, i.e., $\mathbb{P}(\xi \leq \pi_b(X, S) \leq 1 - \xi) = 1$, and (iii) $S$ has full support on $\mathcal{S}$, i.e., $p(s) = \mathbb{P}(S = s) \geq \nu$ for all $s \in \mathcal{S}$ and some $\nu > 0$.
\end{assumption}
\vspace{0.2cm}

Assumption~\ref{ass:boundedness} is standard in the off-policy learning literature \citep{Kallus.2018} and allows to derive theoretical guarantees. In particular, generalization bounds have been derived in settings with unobserved confounding \citep{Kallus.2018c} and distribution shifts \citep{Hatt.2022b}. In the following, we leverage techniques from \citep{Kallus.2018c} to derive generalization bounds for unrestricted, envy-free, and max-min fair policies. Our bounds depend on the Rademacher complexity $R_n(\Pi) = \sum_{\epsilon \in \{-1, +1\}^n} \sup_{\pi \in \Pi} \left| \frac{1}{n} \sum_{i=1}^n \epsilon_i \pi(X_i)\right|$, which is a measure to characterize the complexity of the policy class $\Pi$.

\vspace{0.2cm}
\begin{restatable}[Unrestricted generalization bound]{thrm}{unfair}\label{thrm:bound:uf}
For any $p > 0$ and a constant $K_\mathrm{m}$ depending on the estimation method $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$, it holds with probability at least $1 - p$ that
\begin{equation}
V(\pi) \geq \hat{V}^\mathrm{m}(\pi) - 2 C K_\mathrm{m} \left(R_n(\Pi) + \sqrt{\frac{8 \log\left(\frac{2}{p}\right)}{n}}\right)
\end{equation}
for all $\pi \in \Pi$.
\end{restatable}
\proof{Proof.}
See Section~\ref{sec:proofs}.
\endproof

\vspace{0.2cm}
\begin{restatable}[Envy-free generalization bound]{thrm}{envyfree}\label{thrm:bound:envy_free}
Let $p_1, p_2 > 0$ and assume that for $\ell(n, p_2) = 1 - \nu + \sqrt{\frac{\log\left(\frac{|\mathcal{S}|}{p_2}\right)}{2}}$ it holds that $\frac{1}{\sqrt{n}}\ell(n, p_2) < \nu$. Then, with probability at least $1 - p_1 - p_2$ we have
\begin{equation}
V_\lambda(\pi) \geq \hat{V}^\mathrm{m}_\lambda(\pi) - 2 C K_\mathrm{m} \frac{2+\nu}{\nu}\left( R_n(\Pi) +\sqrt{\frac{8 \log\left(\frac{4 |\mathcal{S}|}{p_1}\right)}{n}} + \frac{2}{(2+\nu) \sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right)
\end{equation}
for all $\pi \in \Pi$.
\end{restatable}
\proof{Proof.}
See Section~\ref{sec:proofs}.
\endproof
\vspace{0.2cm}

\begin{restatable}[Max-min generalization bound]{thrm}{maxmin}\label{thrm:bound:max_min}
Under the assumption in Theorem~\ref{thrm:bound:envy_free} it holds 
with probability at least $1-p_1 - p_2$ that
\begin{equation}
    \min_{s \in \mathcal{S}} V_s(\pi) \geq \min_{s \in \mathcal{S}} \hat{V}^\mathrm{m}_s(\pi) - \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\left(\frac{2 |\mathcal{S}|}{p_1}\right)}{n}} + \frac{1}{\sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right)
\end{equation}
for all $\pi \in \Pi$.
\end{restatable}
\proof{Proof.}
See Section~\ref{sec:proofs}.
\endproof
\vspace{0.2cm}

Theorems~\ref{thrm:bound:uf}--\ref{thrm:bound:max_min} show that, with sufficient sample size, the oracle policy objectives $\hat{V}^\mathrm{m}(\pi)$, $\hat{V}^\mathrm{m}_\lambda(\pi)$, and $\min_{s \in \mathcal{S}} V_s(\pi)$ are with high probability lower bounded than their empirical counterpart if the policy class $\Pi$ has as vanishing Rademacher complexity $R_n(\Pi)$. In this paper, we consider policy classes parametrized by neural networks, which  have a $\sqrt{n}$-vanishing Rademacher complexity $R_n(\Pi) = \mathcal{O}\left(n^{-1/2}\right)$. We also observe that the bounds for value fairness in Theorem~\ref{thrm:bound:envy_free} and \ref{thrm:bound:max_min} depend on the number $|\mathcal{S}|$ of possible values the sensitive attribute can attain, which is in contrast to the unrestricted bound in Theorem~\ref{thrm:bound:uf}.

Note that, in Theorem~\ref{thrm:bound:uf}-\ref{thrm:bound:max_min}, we consider the (unknown) oracle nuisance parameters (such as $\mu_i(X, S)$ and $\pi_b(X,S)$) when computing $\psi^\mathrm{m}(\pi, \mathcal{D}_i)$. In practice, these need to be estimated from the observational data before policy learning. However, we omit this in our theorems because it has been shown that replacing oracle nuisance parameters with estimates does not affect the leading term in the convergence rate of the policy value \citep{Athey.2021}. 

\section{Numerical experiments}
\label{sec:experiments}

We perform an extensive series of numerical experiments using both simulated and real-world data. To evaluate the effectiveness of our \model, simulated data are beneficial as we obtain access to the ground-truth outcomes and can thus benchmark the performance and is consistent with prior work from causal machine learning \citep[e.g.,][]{Shalit.2017, Curth.2021}, while real-world data allow us to demonstrate the applicability to algorithmic decision-making in practice. 

\subsection{Experiments using simulated data}

\subsubsection{Experimental setup.} 

We consider a decision problem from credit lending where loans are approved based on covariates of the customers, yet where algorithmic decision-making must not discriminate by gender. To this end, we denote the sensitive attribute by $S \in \{0,1\}$ and generate data as follows. We simulate two covariates $X_\mathrm{u} \in \R$ and $X_s \in \R$ via
\begin{equation}
    S \sim \mathrm{Bernoulli}(p_s), \qquad  X_u \sim \mathcal{U}[-1, 1], \qquad \text{ and } \qquad X_s \mid S = s \sim \mathcal{U}[s - 1, s],
\end{equation}
where $\mathcal{U}[-1, 1]$ is the uniform distribution over the interval $[-1, 1]$. Thus, $X_u$ is independent of $S$ while $X_s$ is correlated with $S$. This reflects practice where $X_u$ can be, e.g., a credit score (which gives the probability of repaying a loan yet which is independent of gender) and where $X_s$ can be, e.g., income (which is often correlated with gender). We further generate actions, that is, decisions on whether a loan was approved or not via
\begin{equation}
  A \mid X_u = x_u, X_s = x_s, S = s ~\sim \mathrm{Bernoulli}(p) \quad \text{with} \quad p = \sigma(\sin(2 x_u) + \sin(2 X_s) + \sin(2 s)),
\end{equation}
where $\sigma(\cdot)$ denotes the sigmoid function. Finally, we generate outcomes
\begin{equation}
  Y = \mathbbm{1}\{A = 1\} \left(\mathbbm{1}\{X_u < 0.5\}\sin(4X_s - 2) + \mathbbm{1}\{X_u > 0.5\}(0.6 \, S - 0.3)\right) + \epsilon,
\end{equation}
where $\epsilon \sim \mathcal{N}(0, 0.1)$. In our example, the outcomes could correspond to the profit for the lending institution. 

We sample a dataset with $n=3000$ observations based on above procedure and split the data into a training set (80\%) and a test set (20\%). Throughout our experiments, we estimate the policies using the data from the training set and evaluate the policies using the data from the test set to compare the out-of-sample performance. We repeat our analysis over 5 runs and later report the performance by the mean and the corresponding standard deviation. 

We perform all evaluations using the following performance metrics: (1)~We report the policy value $\hat{V}^\mathrm{m}(\pi)$, where $\mathrm{m}$ is DR. This thus corresponds to the objective function in off-policy learning that is maximized under the fairness constraints (thus: larger values are better). (2)~We additionally report the policy value by different sub-populations given by the conditional policy value $\hat{V}^\mathrm{m}_s(\pi)$ for $s=0$ and $s=1$, where $\mathrm{m}$ is DR. We provide the results for our framework across all three different policy scores, all namely $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$ from Eq.~\eqref{eq:pscore}, Eq.~\eqref{eq:pscore_ipw}, and Eq.~\eqref{eq:pscore_dr}, respectively. Of note, we cannot compare our framework against other methods since suitable baselines that can deal with our decision problems are missing.

\subsubsection{Results for action fairness.} 

We now examine whether our framework is effective in learning policies that fulfill action fairness. To do so, we compare the results for different policies as follows. (1)~We first report an optimal unrestricted policy. This baseline has no fairness constraints and has further access to the ground-truth outcome functions from the data generating process, which allows us to the compute the maximum possible policy value for comparison. (2)~We further estimate an oracle policy that fulfills action fairness. This baseline has again access to the ground-truth outcome functions and, hence, should be regarded as a upper bound for the policy value that can be achieved under action fairness. (3)~We compare our \model for action fairness. We set $\gamma = 0.5$. We report three different variants of our \model by varying the underlying polices scores $\mathrm{m}$, namely, DM, IPW, and DR. We repeat again that there is no previous work on fair off-policy learning over general policies. Hence, there are also no suitable baselines against which we can compare our framework.

The results are in Table~\ref{tab:results_sim_af}. Besides policy values, we also report the performance in terms of action fairness, which we calculate via $\E[\pi(X_u, X_{s=1}, S=1) - \pi(X_{u}, X_{s=0}, S=0)]$.\footnote{$S$ is ignored as an input if the policy should fulfill action fairness.} We make the following observations. First, the optimal unrestricted policy has the largest policy value but fails to achieve action fairness, as expected. Second, the policy value for the oracle policy with action fairness is lower, and, by definition, the action fairness achieves a score of zero. Third, we find that our \model is effective in achieving action fairness. In particular, the performance in terms of action fairness of our framework is around 12 times lower than corresponding value for the optimal unrestricted policy. This confirms the effectiveness of framework in removing the dependence between the sensitive attribute and the policy predictions. The performance in terms of action fairness slightly above zero for our \model, which can be expected due to the fact we evaluate our framework out-of-sample. Fourth, we find that our \model attains a policy value that is close to the upper bound given by the oracle policy with action fairness, which corroborates the effectiveness of our framework. Finally, we find that our \model achieves a similar performance regardless of the underlying policy score (i.e., DM, IPW, and DR), showing robustness with respect to the choice of policy score.

\begin{table}[ht]
\caption{Results comparing action fairness using simulated data.}
\label{tab:results_sim_af}
\centering
\footnotesize
\begin{tabular}{lcccc}
\toprule
{Approach} & \multicolumn{3}{c}{Policy value} & {Action fairness}\\
\cmidrule(lr){2-4}
& {Overall} & {$S = 0$} & {$S = 1$} \\
\midrule
\textsc{Baselines} & & & \\
\quad Optimal unrestricted policy &$1.24 \pm 0.03$& $0.74 \pm 0.03$ & $1.46 \pm 0.06$ & $2.42 \pm 0.20 $ \\
\quad Oracle action fairness &$1.03 \pm 0.02$& $0.01 \pm 0.07$ & $1.46 \pm 0.06$ & $0.00 \pm 0.00 $\\
\midrule
\textsc{\model} & & & \\
\quad \model with $\mathrm{m} = \mathrm{DM}$ &$1.02 \pm 0.02$ & $0.01 \pm 0.06$ &$1.45 \pm 0.06$ & $0.21 \pm 0.05$ \\
\quad \model with $\mathrm{m} = \mathrm{IPW}$ &$ 1.01 \pm 0.03$ & $0.02 \pm 0.05$ &$1.43 \pm 0.07$ & $0.24 \pm 0.06$ \\
\quad \model with $\mathrm{m} = \mathrm{DR}$ &$1.01 \pm 0.03$ & $0.02 \pm 0.05$ &$1.43 \pm 0.07$ & $0.23 \pm 0.05$\\
\bottomrule
\multicolumn{4}{l}{Reported: mean $\pm$ standard deviation ($\times 10$) on test set over 5 runs.}
\end{tabular}
\end{table}

\subsubsection{Results for value fairness.}

We further assess whether our framework is effective in learning policies that fulfill value fairness. Here, we report results from our framework with action fairness for three different fairness notions: (1)~no value fairness, (2)~envy-free fairness ($\lambda = 0.5$), and (3)~max-min fairness. We set $\gamma = 0.5$ and provide a sensitivity analysis for the parameter later later. Recall that Lemma~\ref{lem:envy_maxmin} essentially states that max-min fairness is only practical if it used in combination with action fairness. 

The results are in Table~\ref{tab:results_sim_vf}. We arrive at the following conclusions. First, the optimal unrestricted policy has the largest overall policy value, as expected. Second, our \model for envy-free fairness achieves a smaller overall policy due to the fairness constraints. Here, we also report an envy-free metric which is the difference in conditional policy values, confirming that it is effective in satisfying the fairness notion. Third, our \model for max-min fairness is effective in achieving the desired fairness notion. In other words, it achieves a larger worst-case policy value compared to the optimal unrestricted policy and a lower difference between groups. Here, the policy value amounts to 0.73, regardless of the sensitive group. By design, our \model also enforces action fairness ($\gamma = 0.5$). We achieve a mean $0.38$ of action fairness for the envy-free policy, and mean $0.13$ action fairness for max-min policy. This is considerable lower than the corresponding value of the unrestricted policy ($2.42$) for the optimal unrestricted policy. Fourth, we observed relatively low standard deviations, which corroborate that the robustness of our framework. In summary, the experimental results confirm the effectiveness of our empirical framework in enforcing the proposed fairness notions.

\begin{table}[ht]
\caption{Results comparing value fairness using simulated data.}
\label{tab:results_sim_vf}
\centering
\footnotesize
\begin{tabular}{lcccc}
\toprule
{Approach} & \multicolumn{3}{c}{Policy value} & Envy-free metric\\
\cmidrule(lr){2-4}
& {Overall} & {$S = 0$} & {$S = 1$} & \\
\midrule
\textsc{Baselines} & & & \\
\quad Optimal unrestricted policy &$1.24 \pm 0.03$& $0.74 \pm 0.03$ & $1.46 \pm 0.06$ & $0.73 \pm 0.06 $ \\
\quad Oracle action fairness &$1.03 \pm 0.02$& $0.01 \pm 0.07$ & $1.46 \pm 0.06$ & $1.45 \pm 0.12 $\\
\midrule
\textsc{\model with envy-free fairness} & & & \\
\quad \model with $\mathrm{m} = \mathrm{DM}$ &$0.87 \pm 0.17$ &$0.61 \pm 0.07$ & $0.99 \pm 0.24$ & $0.38 \pm 0.22$\\
\quad \model with $\mathrm{m} = \mathrm{IPW}$ &$0.87 \pm 0.05$ &$0.32 \pm 0.17$ & $1.11 \pm 0.14$ & $0.79 \pm 0.30 $ \\
\quad \model with $\mathrm{m} = \mathrm{DR}$ &$0.86 \pm 0.06$ &$0.34 \pm 0.17$ & $1.09 \pm 0.15$ & $0.75 \pm 0.32$\\
\midrule
\textsc{\model with max-min fairness } & & & \\
\quad \model with $\mathrm{m} = \mathrm{DM}$ &$0.73 \pm 0.03$ &$0.73 \pm 0.03$ & $0.73 \pm 0.03$ & $0.00 \pm 0.00$\\
\quad \model with $\mathrm{m} = \mathrm{IPW}$ &$0.73 \pm 0.03$ &$0.73 \pm 0.03$ & $0.73 \pm 0.03$& $0.00 \pm 0.01$ \\
\quad \model with $\mathrm{m} = \mathrm{DR}$ &$0.73 \pm 0.03$ &$0.73 \pm 0.03$ & $0.73 \pm 0.03$ & $0.00 \pm 0.01$\\
\bottomrule
\multicolumn{4}{l}{Reported: mean $\pm$ standard deviation ($\times 10$) on test set over 5 runs.}
\end{tabular}
\end{table}


Finally, we examine the sensitivity to the envy-freeness parameter $\lambda$. For this purpose, we compare the policy value from our \model for different values of $\lambda \in [0, 2]$. The results are shown in Fig.~\ref{fig:parameter} (left). As expected, the policy value tends to decrease for larger $\lambda$ and thus when the fairness notion becomes stricter. Similarly, the difference between the policy values for the two subgroups $S=0$ and $S=1$ becomes smaller for larger $\lambda$ and thus and thus achieves the desired result. Furthermore, we study the robustness to the regularization parameter $\gamma$; see Fig.~\ref{fig:parameter} (right). Here, for different choices of $\gamma$, the results remain robust.

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_envy_free_lambda.pdf}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/plot_envy_free_gamma.pdf}
\end{subfigure}
\caption{Sensitivity analysis for the envy-free parameter $\lambda$ (left) and the regularization parameter $\gamma$ (right).}
\label{fig:parameter}
\end{figure}

\subsection{Experiments using real-world data}

\subsubsection{Experimental setup.} 

We now demonstrate the applicability of our framework to real-world, medical data. For this, we consider a decision problem from public health where governments seek to offer free healthcare to individuals who may otherwise not enroll in health insurance. As an example, the state of Oregon has previously experimented with such practice where it provided free health insurance to some individuals through Medicaid \citep{Finkelstein.2012}. The underlying rationale is that free health insurance can improve the health status of individuals and lower their medical debts, which is beneficial as it leads to improved health conditions, a more productive workforce, and lower costs for the healthcare system in the long run. To this end, the decision problem is to minimize the medical debt of low-income individuals but further ensure fairness with respect to gender. Notwithstanding, ensuring fairness in healthcare is non-trivial as algorithmic decision-making in medicine is often biased \citep{Obermeyer.2019}.

We use medical data from the Oregon health insurance experiment \citep{Finkelstein.2012}. The Oregon health insurance experiment took place in 2008, and around 30,000 low-income, uninsured adults in Oregon were offered free health insurance through Medicaid. Details for the dataset are in Appendix~\ref{app:data}.

We use our framework to learn fair policies that assign Medicaid to minimize the total cost for the individual medical care, while avoiding discrimination with respect to gender. Importantly, even though the Medicaid was assigned randomly, individuals had to actively sign up in order to participate. Therefore, gender can still influence both the Medicaid participation and the health outcome, resulting in a possibly confounded action-outcome relationship that violates fairness. Besides gender, we include five additional variables as possible confounders (e.g., age and language). Details are Appendix~\ref{app:data}. In the following, we again evaluate our framework separately for both action fairness and value fairness. A strength of our framework is that it can account for different fairness notions that can be potentially relevant, while we leave the actual choice to decision-makers in practice. 

\subsubsection{Results for action fairness.} 

We compare the optimal unrestricted policy against our \model with action fairness. Here, \model  is based on $\gamma = 0.5$ and the double robust method $\mathrm{m} = \mathrm{DR}$. We again report the overall policy value ($\hat{V}^\textrm{m}(\pi)$), and the policy value for the male sub-population ($V^\textrm{m}_{\mathrm{male}}(\pi)$) and for the female sub-population ($V^\textrm{m}_{\mathrm{female}}(\pi)$). In contrast to our experiment on simulated data, we do not know the ground-truth outcomes for real-world data, and, hence, we estimate the overall policy values using our estimators from Eq.~\eqref{eq:pvalue_est} and \eqref{eq:pvalue_cond_est} on the test data. To quantify action fairness, we report the Spearman's rank correlation coefficient between the sensitive attribute (gender) and the policy predictions on the test data.  

The results are shown in Table~\ref{t:results_real_af}. Again, the optimal unrestricted policy has the largest empirical policy value but does not satisfy action fairness. In contrast to that, we find that our \model is again effective in achieving action fairness. This comes at the cost of a slightly lower overall policy value.

\begin{table}[ht]
\caption{Results for action fairness using real-world data.}
\label{t:results_real_af}
\centering
\footnotesize
\begin{tabular}{lcccc}
\toprule
{Approach} & \multicolumn{3}{c}{Policy value} & {Action fairness}\\
\cmidrule(lr){2-4}
& {Overall} & {$S = \mathrm{male}$} & {$S = \mathrm{female}$} \\
\midrule
{Optimal unrestricted policy} & $0.137 \pm 0.005$ & $0.101 \pm 0.008$ &  $0.165 \pm 0.003$ & $0.129 \pm 0.007$ \\
{\model} & $0.130 \pm 0.004$ & $0.093 \pm 0.006$ & $0.160 \pm 0.003$ & $0.015 \pm 0.001$ \\
\bottomrule
\multicolumn{5}{l}{Reported: mean $\pm$ standard deviation on test set over 5 random runs}
\end{tabular}
\end{table}


\subsubsection{Results for value fairness.}

Table~\ref{t:results_real_vf} evaluates \model for value fairness. We use the doubly-robust method $\mathrm{m} = \mathrm{DR}$ and set $\gamma = 0.5$. For envy-free fairness, we set $\lambda = 0.3$. As expected, both \model with envy-free fairness and with max-min fairness have a lower policy value than the optimal unrestricted policy. However, the max-min policy is effective in both increasing the worst-case policy value and decreasing the difference in policy values across both sub-populations. This demonstrates the effectiveness of our \model to obtain a policy with best possible worst-case policy value on real-world data.

\begin{table}[ht]
\caption{Results for value fairness using real-world data.}
\label{t:results_real_vf}
\centering
\footnotesize
\begin{tabular}{lcccc}
\toprule
{Approach} & \multicolumn{3}{c}{Policy value} & {Envy-free metric}\\
\cmidrule(lr){2-4}
& {Overall} & {$S = \mathrm{male}$} & {$S = \mathrm{female}$} \\
\midrule
Optimal unrestricted policy & $0.137 \pm 0.005$ & $0.101 \pm 0.008$ &  $0.165 \pm 0.003$ & $0.129 \pm 0.007$\\
\model with {envy-free fairness} & $0.130 \pm 0.004$ & $0.093 \pm 0.006$ & $0.160 \pm 0.003$ & $0.067 \pm 0.007$ \\
\model with {max-min fairness} & $0.131 \pm 0.008$ & $0.100 \pm 0.011$ & $0.157 \pm 0.008$& $0.057 \pm 0.010$ \\
\bottomrule
\multicolumn{5}{l}{Reported: mean $\pm$ standard deviation on test set over 5 random runs}
\end{tabular}
\end{table}

\subsubsection{Insights.}

To better understand the estimated polices, we now examine the the corresponding policy predictions, i.e., the outputs of the respective policies. We average the predictions for each policy on the test data conditional on (i)~age and (ii)~the number of previous emergency department visits of an individual. We plot the results in Fig.~\ref{fig:pred}. Here, we illustrate the differences between decision logic in the optimal unrestricted policy and our \model with max-min fairness. In general, the policy predictions are large for both policies. In other words, both policies tend to recommend Medicaid for the majority of individuals. Thus, medicaid seems to benefit most of the population in that it reduces medical debt.  Furthermore, both policy predictions are lower for individuals with a smaller number of emergency department visits. This is reasonable as such individuals may be less at risk to accumulate medical debt compared to individuals with an extensive medical history. \model with max-min fairness outputs slightly lower predictions for older individuals and for individuals with no or few emergency visits. Hence, there seem to exist some male individuals with few emergency visits or higher age for which free health insurance has only little positive effect. 

%Medicaid does not affect positively. 

\begin{figure}[ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figures/plot_pred_age.pdf}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figures/plot_pred_visits.pdf}
\end{subfigure}
\caption{Comparison of the estimated policies. Visualized are the policy prediction (i.e., the outputs of the respective policies).}
\label{fig:pred}
\end{figure}

\subsection{Combining action and value fairness}

In our experiments, we showed that \model can effectively combine action fairness and value fairness. As we discussed earlier, max-min fairness is only practical if it is combined with action fairness (see Section~\ref{sec:combination} and Lemma~\ref{lem:maxmin_unfair}). To this end, all of the above evaluations are based on \model where both fairness notions are combined and thus demonstrates the effectiveness for this case. In sum, this highlights the flexibility of our \model as it allows one to integrate a variety of fairness notions that are of relevance for algorithmic decision-making in practice. 

\section{Discussion}\label{sec:discussion}

% relevance

Unfair decisions can have detrimental consequences for individuals because of which ethical and legal frameworks require that algorithmic decision-making must ensure fairness \citep{Barocas.2016, Kleinberg.2019}. Applications where fairness for algorithmic decision-making are vast and include healthcare, lending, and criminal justice, among many others \citep{DeArteaga.2022}. For example, in the U.S., the Equal Credit Opportunity Act mandates that lending decisions are fair for individuals of different gender, race, and other sensitive groups, while the Fair Housing Act enforces similar principles for housing rentals and purchases. Hence, while the use of algorithmic decision-making may have several operational benefits, it must avoid discrimination of individuals and thus generate decisions that are regarded as fair.

We addressed the problem of fairness in algorithmic decision-making by learning fair policies from observational data. Our framework has three main benefits which make it appealing for use in practice: (1)~Our framework is directly applicable even in settings where the observational data ingrain historically bias. Despite that, our framework can still obtain a fair policy. This is relevant for practice as there is a growing awareness that many data sources are biased and that it is often challenging or infeasible to remove bias in historical data \citep{CorbettDavies.2023}. (2)~Our framework comes with a scalable machine learning instantiation based on a custom neural network (\model). Thereby, one can effectively generate fair policies from high-dimensional and non-linear observational data. (3)~Our framework is flexible in the sense that it supports different fairness notions. Practitioners can thus adapt our framework to the underlying fairness goals as well as the legal and ethical context and thus choose a suitable fairness notion. Together, our framework fulfills crucial fairness demands in many applications from practice (e.g., automated hiring, credit lending, and ad targeting).

Our work contributes to the literature in several ways. First, our work connects to off-policy learning \citep[e.g.,][]{Kallus.2018, Athey.2021}. While there is a growing body of literature that uses off-policy learning for managerial decision-making such as for pricing and ad targeting \citep[e.g.,][]{Smith.2022, Yoganarasimhan.2022, Yang.2023}, we add by offering a new framework for off-policy learning with fairness guarantees. In particular, our work fills an important gap in the literature in that we are able to learn fair policies from biased observational data. Second, there is extensive literature from algorithmic fairness that focuses on machine learning predictions \citep[e.g.,][]{Hardt.2016, Kusner.2017, Nabi.2018}, whereas we contribute to algorithmic fairness for decision-making from observational data, specifically, off-policy learning. Third, fairness notions such as envy-free or max-min fairness have been used in traditional, utility-based decision models such as those from resource allocation and pricing \citep[e.g.,][]{Bertsimas.2011,Kallus.2021c, Cohen.2022}. We build upon these fairness notions but extend them for off-policy learning. 

% from both algorithmic fairness and causal machine learning and contributes to both areas

\section{Concluding remarks}

In this paper, we proposed a novel framework for fair off-policy learning from observational data. For this, we introduced fairness notions tailored to off-policy learning, then developed a flexible instantiation of our framework using machine learning (\model), and finally provided theoretical guarantees in form of generalization bounds. Our framework is widely applicable to algorithmic decision-making in practice where fairness must be ensured.

\section{Proof of generalization bounds}
\label{sec:proofs}

In this section, we provide the proof of our generalization bounds, namely, Theorem~\ref{thrm:bound:envy_free} and Theorem~\ref{thrm:bound:max_min}. In our proof, we later leverage ideas from Theorem~1 in \cite{Kallus.2018}; however, adapting these to our setting is not straightforward, and several additional arguments must be made. To this end, we begin with three auxiliary lemmas.

\vspace{0.3cm}
\begin{lemma}\label{lem:bounded_diff}
Let $T^\mathrm{m}(s, \mathcal{D}) = \sup_{\pi \in \Pi} |\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}(\pi, \mathcal{D}_i) - V_s(\pi)|$. Then, $T^\mathrm{m}(s, \cdot)$ satisfies the bounded difference inequality with $\frac{4C}{n p(s)} K_\mathrm{m}$, where $K_\mathrm{m}$ is a constant depending on $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$.
\end{lemma}

\proof{\textbf{Proof.}}
It holds that
\begin{align}
    &\left|T^\mathrm{m}(s, \mathcal{D}) - T^\mathrm{m}(s, \mathcal{D}^\prime)\right| \\ 
    \overset{(1)}{\leq} & \sup_{\pi \in \Pi} \left| \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}(\pi, \mathcal{D}_i) - V_s(\pi) \right| -  \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i^\prime = s)}{p(s)} \psi^\mathrm{m}(\pi, \mathcal{D}_i^\prime) - V_s(\pi) \right|\right|\\
    \overset{(2)}{\leq} & \frac{1}{n p(s)} \sup_{\pi \in \Pi} \left|\sum_{i=1}^n \mathbbm{1}(S_i = s) \psi^\mathrm{m}(\pi, \mathcal{D}_i) - \mathbbm{1}(S_i^\prime = s) \psi^\mathrm{m}(\pi, \mathcal{D}_i^\prime) \right| \\
    = & \frac{1}{n p(s)} \sup_{\pi \in \Pi} \left( \left| \psi^\mathrm{m}(\pi, \mathcal{D}_j)\right| + \left| \psi^\mathrm{m}(\pi, \mathcal{D}_j^\prime)\right| \right),
\end{align}
where (1) follows from a property of the supremum and (2) follows from the reverse triangle inequality.
It remains to bound $\left|\psi^\mathrm{m}(\pi, \cdot)\right|$ for $\mathrm{m} \in \{\mathrm{DM}, \mathrm{IPW}, \mathrm{DR}\}$. For $\mathrm{m} = \mathrm{DM}$, we get
\begin{equation}
\left| \psi^{\mathrm{DM}}(\pi, \mathcal{D}_j)\right| \leq |\mu_1(X, S)| + |\mu_0(X, S)| \leq 2 C .
\end{equation}
For $\mathrm{m} = \mathrm{IPW}$, we get
\begin{equation}
\left| \psi^{\mathrm{IPW}}(\pi, \mathcal{D}_j)\right| \leq \frac{|Y|}{|A \pi_b(X, S) + (1 - A) (1 - \pi_b(X, S))|}\leq \frac{C}{\eta} .
\end{equation}
Finally, for $\mathrm{m} = \mathrm{DR}$, we get
\begin{equation}
\left| \psi^{\mathrm{DR}}(\pi, \mathcal{D}_j)\right| \leq \left| \psi^{\mathrm{DM}}(\pi, \mathcal{D}_j)\right| +  \frac{|Y - \mu_A(X, S)|}{|A \pi_b(X, S) + (1 - A) (1 - \pi_b(X, S))|}\leq 2C \left(\frac{\eta + 1}{\eta} \right).
\end{equation}
Therefore, we arrive at
\begin{equation}
    \left|T^\mathrm{m}(s, \mathcal{D}) - T^\mathrm{m}(s, \mathcal{D}^\prime)\right| \leq \frac{4C}{n p(s)} K_\mathrm{m},
\end{equation}
with $K_{\mathrm{DM}} = 1$, $K_{\mathrm{IPW}} = \frac{1}{2 \eta}$, and $K_{\mathrm{DR}} = \frac{\eta + 1}{\eta}$.
\hfill\Halmos
\endproof

\vspace{0.3cm}
\begin{lemma} \label{lem:t_bound}
With probability of at least $1 - p$, it holds that
\begin{equation}
T^\mathrm{m}(s, \mathcal{D}) \leq \frac{2 C K_\mathrm{m}}{p(s)} \left(R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p}}{n}}\right).
\end{equation}
\end{lemma}

\noindent
\proof{\textbf{Proof.}}
Lemma~\ref{lem:bounded_diff} allows us to apply McDiarmid's inequality, resulting in
\begin{equation}
    \mathbb{P}\left(T^\mathrm{m}(s, \mathcal{D}) - \E\left[ T^\mathrm{m}(s, \mathcal{D})\right] \geq \epsilon  \right) \leq \exp\left(- \frac{n p(s)^2 \epsilon^2}{8C^2 K_\mathrm{m}^2} \right).
\end{equation}
Equivalently, with probability of at least $1 - p_1$, it holds that
\begin{equation}
T^\mathrm{m}(s, \mathcal{D}) \leq \E\left[ T^\mathrm{m}(s, \mathcal{D})\right] + \frac{2 C K_\mathrm{m}}{p(s)} \sqrt{\frac{2 \log\frac{1}{p_1}}{n}}.
\end{equation}
By a standard symmetrization argument, we yield
\begin{equation}
\E\left[ T^\mathrm{m}(s, \mathcal{D})\right] \leq \E\left[ \frac{1}{2^n} \sum_{\epsilon \in \{-1,1\}^n} \sup_{\pi \in \Pi} \left|\frac{1}{n} \sum_{i=1}^n \epsilon_i \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}((\pi, \mathcal{D}_i) \right|\right] \leq \frac{2C K_\mathrm{m}}{p(s)} \E\left[R_n(\Pi)\right].
\end{equation}
Here, the Rademacher complexity $R_n(\Pi)$ satisfies the bounded differences with $\frac{2}{n}$, and we thus obtain
\begin{equation}
    \mathbb{P}\left(R_n(\Pi) - \E\left[R_n(\Pi)\right] \leq - \epsilon \right) \leq \exp\left(- \frac{n \epsilon^2}{2}\right) .
\end{equation}
This implies that, with probability of at least $1 - p_2$, it holds that
\begin{equation}
\E\left[R_n(\Pi)\right] \leq  R_n(\Pi) + \sqrt{\frac{2 \log\frac{1}{p_2}}{n}}.
\end{equation}
By setting $p_1 = p_2 = \frac{p}{2}$ and applying the union bound, we yield
\begin{equation}
T^\mathrm{m}(s, \mathcal{D}) \leq \frac{2 C K_\mathrm{m}}{p(s)} \left(R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p}}{n}}\right)
\end{equation}
with probability of at least $1-p$.
\hfill\Halmos
\endproof
\vspace{0.3cm}

\noindent
In the next step, we use Lemma~\ref{lem:t_bound} to derive a bound on the absolute estimation error $\left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|$ that holds uniformly over all policies and sensitive attributes. This is stated in Lemma~\ref{lem:v_bound_uniform}.

\vspace{0.3cm}
\begin{lemma} \label{lem:v_bound_uniform}
Let $ \ell(n, p_2) = 1 - \nu + \sqrt{\frac{\log\frac{|\mathcal{S}|}{p_2}}{2}}$. Let us further assume that $\frac{\ell(n, p_2)}{\sqrt{n}} < \nu$. Then, with probability of at least $1-p_1 - p_2$, it holds that
\begin{equation}
    \sup_{\pi \in \Pi} \sup_{s \in \mathcal{S}} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right| \leq \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\frac{2 |\mathcal{S}|}{p_1}}{n}} + \frac{1}{\sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right).
\end{equation}
\end{lemma}

\proof{\textbf{Proof.}}
We yield
\begin{align}
& \quad \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right| \\
    & = \sup_{\pi \in \Pi} \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{\hat{p}_n(s)} \psi^\mathrm{m}((\pi, \mathcal{D}_i) - V_s(\pi)\right| \\
    & \leq \left|\frac{1}{\hat{p}_n(s)} - \frac{1}{p(s)} \right| \sup_{\pi \in \Pi} \left| \sum_{i=1}^n \mathbbm{1}(S_i = s)\psi^\mathrm{m}((\pi, \mathcal{D}_i) \right| + \sup_{\pi \in \Pi} \left|\frac{1}{n} \sum_{i=1}^n \frac{\mathbbm{1}(S_i = s)}{p(s)} \psi^\mathrm{m}((\pi, \mathcal{D}_i) - V_s(\pi)\right| \\
    & \leq \frac{2 C K_\mathrm{m}}{p(s)} \frac{\left|\hat{p}_n(s) - p(s) \right|}{\hat{p}_n(s)} +  T^\mathrm{m}(s, \mathcal{D}).
\end{align}
The absolute difference $\left|\hat{p}_n(s) - p(s) \right|$ satisfies bounded differences with constant $\frac{1}{n}$ because
\begin{equation}
  \left|\left|\hat{p}_n(s) - p(s) \right| - \left|\hat{p}^\prime_n(s) - p(s) \right| \right|  \leq \left|\hat{p}_n(s) - \hat{p}^\prime_n(s) \right| \leq \frac{\left|\mathbbm{1}(S_j = s) - \mathbbm{1}(S^\prime_i = s) \right|}{n} \leq \frac{1}{n}.
\end{equation}
Hence, McDiamid's inequality implies 
\begin{equation}
\mathbb{P}\left(\left|\hat{p}_n(s) - p(s) \right| - \E\left[\left|\hat{p}_n(s) - p(s) \right| \right] \geq \epsilon \right) \leq \exp\left(-2n\epsilon^2\right).
\end{equation}
Thus, with probability at least $1-p_2$ it holds that
\begin{align}
\left|\hat{p}_n(s) - p(s) \right| &\leq \E\left[\left|\hat{p}_n(s) - p(s) \right| \right] + \sqrt{\frac{\log\frac{1}{p_2}}{2n}} \\
& \overset{(1)}{\leq} \frac{1}{n} \sqrt{\E\left[\left(n\hat{p}_n(s) - np(s) \right)^2 \right]} + \sqrt{\frac{\log\frac{1}{p_2}}{2n}} \\
& \overset{(2)}{=} \sqrt{\frac{p(s)\left(1 - p(s)\right)}{n}} + \sqrt{\frac{\log\frac{1}{p_2}}{2n}} = \ell(n, p_2, s) \\
& \leq \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right),
\end{align}
where (1) follows by applying Jensen's inequality and (2) by noting that $n \hat{p}_n(s) \sim \mathrm{Binomial}(n, p(s))$ with expectation $n p(s)$ and standard deviation $\sqrt{n p(s) (1 - p(s))}$.

The above also implies that, with probability of at least $1-p_2$, we obtain
\begin{equation}
\hat{p}_n(s) \geq p(s) - \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right) \geq \nu - \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right) > 0,
\end{equation}
whenever $\frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right) < \nu$. Putting everything together via the union bound, we obtain with probability of at least $1 - p_1 - p_2$ that
\begin{equation}
\sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|
\leq \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p_1}}{n}} + \frac{1}{\sqrt{n}} \left( \frac{1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}}{\nu - \frac{1}{\sqrt{n}}\left(1 - \nu + \sqrt{\frac{\log\frac{1}{p_2}}{2}}\right)} \right)\right).
\end{equation}
The result follows by applying the union bound over all $s\in \mathcal{S}$.
\hfill\Halmos
\endproof
\vspace{0.3cm}

In the following, we use Lemma~\ref{lem:v_bound_uniform} to prove the generalization bounds. Specifically, we provide the proofs for the envy-free generalization bound (Theorem~\ref{thrm:bound:envy_free}), the max-min generalization bound (Theorem~\ref{thrm:bound:max_min}), and the unrestricted generalization bound (Theorem~\ref{thrm:bound:uf}).

\envyfree*

\proof{\textbf{Proof.}}
It follows that
\begin{align}
& \quad \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}} \left|\hat{V}^\mathrm{m}(\pi) - \lambda \big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big|   - V(\pi) + \lambda \big|V_s(\pi) - V_{s^\prime}(\pi) \big| \right| \\
& \leq \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi) \right| + \lambda \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}}\left|\big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big| - \big|V_s(\pi) - V_{s^\prime}(\pi) \big| \right| \\
& \leq \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi) \right| + 2 \lambda \sup_{\pi \in \Pi} \sup_{s \in \mathcal{S}} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|.
\end{align}

Hence, with probability of at least $1 - p_1 - p_2$, we yield
\begin{align}
& \quad \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}} \left|\hat{V}^\mathrm{m}(\pi) - \lambda \big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big|   - V(\pi) + \lambda \big|V_s(\pi) - V_{s^\prime}(\pi) \big| \right| \\
&\leq 2 C K_\mathrm{m} \frac{2+\nu}{\nu}\left( R_n(\Pi) +\sqrt{\frac{8 \log\frac{4 |\mathcal{S}|}{p_1}}{n}} + \frac{2}{(2+\nu) \sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right)
\end{align}
using Lemma~\ref{lem:v_bound_uniform}. The theorem follows from
\begin{equation}
    \hat{V}^\mathrm{m}_\lambda(\pi) \leq V_\lambda(\pi) + \sup_{\pi \in \Pi} \sup_{s, s^\prime \in \mathcal{S}} \left|\hat{V}^\mathrm{m}(\pi) - \lambda \big|\hat{V}^\mathrm{m}_s(\pi) - \hat{V}^\mathrm{m}_{s^\prime}(\pi) \big|   - V(\pi) + \lambda \left|V_s(\pi) - V_{s^\prime}(\pi) \right| \right|.
\end{equation}
\hfill\Halmos
\endproof

\maxmin*

\proof{\textbf{Proof.}}
The triangle inequality implies that
\begin{equation}
    \hat{V}^\mathrm{m}_s(\pi) \leq V_s(\pi) + \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}_s(\pi) - V_s(\pi)\right|.
\end{equation}

Hence, Lemma~\ref{lem:v_bound_uniform} yields with probability of at least $1-p_1 - p_2$ for all $s \in \mathcal{S}$, $\pi \in \Pi$:
\begin{equation}
    V_s(\pi) \geq \hat{V}^\mathrm{m}_s(\pi) - \frac{2 C K_\mathrm{m}}{\nu} \left( R_n(\Pi) + \sqrt{\frac{8 \log\frac{2 |\mathcal{S}|}{p_1}}{n}} + \frac{1}{\sqrt{n}} \left( \frac{\ell(n, p_2)}{\nu - \frac{1}{\sqrt{n}}\ell(n, p_2)} \right)\right).
\end{equation}
The result follows by applying the minimum over $s$ on both sides.
\hfill\Halmos
\endproof

\unfair*

\proof{\textbf{Proof.}}
It follows that
\begin{equation}
    \hat{V}^\mathrm{m}(\pi) \leq V(\pi) + \sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi)\right|.
\end{equation}
With the same proof as in Lemma~\ref{lem:v_bound_uniform}, we can show, with probability of at least $1-p$, that
\begin{equation}
\sup_{\pi \in \Pi} \left|\hat{V}^\mathrm{m}(\pi) - V(\pi) \right| \leq 2 C K_\mathrm{m} \left(R_n(\Pi) + \sqrt{\frac{8 \log\frac{2}{p}}{n}}\right).
\end{equation}
\hfill\Halmos
\endproof


\bibliography{bibliography}
\bibliographystyle{informs2014.bst}

%% Here starts the e-companion (EC)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{APPENDICES}

~

\begin{center}
\Huge\vspace{1cm}
\textbf{Online Appendix}
\vspace{1cm}
\end{center}
\newpage

\section{Derivations for the example (Table~\ref{t:toy_ex1})}
\label{app:toy_example}
We denote the levels of gender with $\mathrm{F}$ (female) and $\mathrm{M}$ (male), and the levels of GPA with $\mathrm{L}$ (low) and $\mathrm{H}$ (high). We first calculate the conditional policy value $V_\mathrm{F}$ for females
\begin{align}
    V_\mathrm{F}(\pi) & = \pi(\mathrm{F}, \mathrm{L}) \mu_1(\mathrm{F}, \mathrm{L}) + (1 - \pi(\mathrm{F}, \mathrm{L})) \mu_0(\mathrm{F}, \mathrm{L}) + \pi(\mathrm{F}, \mathrm{H}) \mu_1(\mathrm{F}, \mathrm{H}) + (1 - \pi(\mathrm{F}, \mathrm{H})) \mu_0(\mathrm{F}, \mathrm{H}) \\
    & = (1 - \pi(\mathrm{F}, \mathrm{L})) - \pi(\mathrm{F}, \mathrm{H}) + (1 - \pi(\mathrm{F}, \mathrm{H})) \\
    &= 2 - \pi(\mathrm{F}, \mathrm{L}) - 2 \pi(\mathrm{F}, \mathrm{H})
\end{align}
and $V_\mathrm{M}$ for males
\begin{align}
    V_\mathrm{M}(\pi) & = \pi(\mathrm{M}, \mathrm{L}) \mu_1(\mathrm{M}, \mathrm{L}) + (1 - \pi(\mathrm{M}, \mathrm{L})) \mu_0(\mathrm{M}, \mathrm{L}) + \pi(\mathrm{M}, \mathrm{H}) \mu_1(\mathrm{M}, \mathrm{H}) + (1 - \pi(\mathrm{M}, \mathrm{H})) \mu_0(\mathrm{M}, \mathrm{H}) \\
    & = 1 - \pi(\mathrm{M}, \mathrm{L}) + \pi(\mathrm{M}, \mathrm{H}).
\end{align}
The overall policy value is
\begin{align}
    V(\pi) & = 0.2 V_\mathrm{F}(\pi) + 0.8 V_M(\pi) \\
    &= 1.2 + 0.8\pi(\mathrm{M}, \mathrm{H}) -0.8 \pi(\mathrm{M}, \mathrm{L}) - 0.2\pi(\mathrm{F}, \mathrm{L}) - 0.4 \pi(\mathrm{F}, \mathrm{H}).
\end{align}

Hence, the optimal unrestricted policy is $\pi^\mathrm{u}(\mathrm{M}, \mathrm{H}) = 1$, $\pi^\mathrm{u}(\mathrm{M}, \mathrm{L}) = 0$, $\pi^\mathrm{u}(\mathrm{F}, \mathrm{H}) = 0$, and $\pi^\mathrm{u}(\mathrm{F}, \mathrm{L}) = 0$. 

The difference of the conditional policy value is
\begin{align}
    \Delta(\pi) = |V_\mathrm{F}(\pi) - V_\mathrm{M}(\pi)| & = |1 - \pi(\mathrm{F}, \mathrm{L}) - 2 \pi(\mathrm{F}, \mathrm{H}) - \pi(\mathrm{M}, \mathrm{H}) + \pi(\mathrm{M}, \mathrm{L})|.
\end{align}
It holds that $\Delta(\pi^\mathrm{u}) = 0$ which implies that the $\alpha$-envy free policy $\pi^\alpha$ conincides with $\pi^\mathrm{u}$.

For the optimal action fair policy $\pi^\mathrm{af}$ the policy value simplifies to
\begin{align}
    V(\pi^\mathrm{af})  &= 1.2 + 0.8\pi^\mathrm{af}(\mathrm{H}) -0.8 \pi^\mathrm{af}(\mathrm{L}) - 0.2\pi^\mathrm{af}(\mathrm{L}) - 0.4 \pi^\mathrm{af}(\mathrm{H}) \\
    &= 1.2 + 0.4\pi^\mathrm{af}(\mathrm{H}) - \pi^\mathrm{af}(\mathrm{L}),
\end{align}
which means that $\pi^\mathrm{af}(\mathrm{L}) = 0$ and $\pi^\mathrm{af}(\mathrm{H}) = 1$.
For the action fair envy-free policy $\pi^{\mathrm{af} + \alpha}$ we obtain
\begin{align}
    \Delta(\pi^{\mathrm{af} + \alpha}) = |1 - 3 \pi^{\mathrm{af} + \alpha}(\mathrm{H})| \leq \alpha,
\end{align}
which yields $\pi^{\mathrm{af} + \alpha}(\mathrm{L}) = 0$ and $\pi^{\mathrm{af} + \alpha}(\mathrm{H}) = \frac{\alpha + 1}{3}$. Finally, the max-min policy $\pi^\mathrm{mm}$ maximizes
\begin{align}
    \min\{V_\mathrm{F}(\pi^\mathrm{mm}), V_\mathrm{M}(\pi^\mathrm{mm})\} = \min\{2 - \pi^\mathrm{mm}(\mathrm{L}) - 2\pi^\mathrm{mm}(\mathrm{H}), \, 1 - \pi^\mathrm{mm}(\mathrm{L}) + \pi^\mathrm{mm}(\mathrm{H}) \},
\end{align}
which implies $\pi^\mathrm{mm}(\mathrm{L}) = 0$ and $\pi^\mathrm{mm}(\mathrm{H}) = 1/3$.

\newpage

\section{Discussion on the assumptions in Lemma~\ref{lem:envy_maxmin}}
\label{app:assumptions}

In this section, we provide additional details regarding the assumptions in Lemma~\ref{lem:envy_maxmin}. In essance, Lemma~\ref{lem:envy_maxmin} holds if the max-min solution $(\pi^{\mathrm{mm}}, s^\ast)$ (which is a solution to Eq.~\eqref{eq:maxmin}) outputs stochastic actions in an area $V$ of the covariate space where the policy value could be improved by choosing a deterministic action. In the toy example from the previous section this is the case, and hence the max-min and the envy-free policy with $\alpha = 0$ coincide. In the following, we study a secpmd toy example where $\pi^{\mathrm{mm}}$ does not coincide with any envy-free policy $\pi^{\mathrm{af} + \alpha}$.

\subsection{Toy example}

The same data from Table~\ref{t:toy_ex1} is shown in Table~\ref{t:toy_ex2} with different ITEs. Now, action benefits all groups, but males have larger expected outcomes than females. Furthermore, old males receive a larger benefit from the action than all other groups. Hence, even though the action benefits all groups, the difference in policy values for males and females will increase by giving performing action on old male patients. The max-min policy $\pi^{\mathrm{mm}}$ simply recommends action to everyone, as it aims to maximize the policy value $V_{\textrm{Female}}(\pi^{\mathrm{mm}})$ for females (worst-case). In contrast, the $\pi^{\mathrm{af} + \alpha}$ restricts action on older patients in order to decrease the disparity of conditional policy values $V_{\textrm{Female}}(\pi^{\mathrm{af} + \alpha})$ and $V_{\textrm{Male}}(\pi^{\mathrm{af} + \alpha})$ (envy-free).

\begin{table*}[ht]
\caption{Toy example comparing the different fairness notions for off-policy learning.}
\centering
\label{t:toy_ex2}
\footnotesize
\begin{tabular}{P{0.8cm} P{0.8cm} P{1.5cm} P{1.3cm} P{1.3cm} P{0.6cm} P{0.6cm} P{0.6cm} P{0.6cm} P{1.3cm} P{1.3cm}}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\multicolumn{3}{c}{\textbf{Data}}& \multicolumn{2}{c}{\textbf{Expected outcome }} & \multicolumn{4}{c}{\textbf{Policies}} & \multicolumn{2}{c}{\textbf{Combined policies}} \\
& & & & & & & & & \multicolumn{2}{c}{(with action fairness)} \\
\cmidrule(lr){1-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9} \cmidrule(lr){10-11}
Gender & GPA & Probability & $\mu_1$ & $\mu_0$ & $\pi^{\mathrm{u}}$ & $\pi^{\mathrm{af}}$ & $\pi^{\mathrm{\alpha}}$ & $\pi^{\mathrm{mm}}$ &  $\pi^{\alpha}$ & $\pi^{\mathrm{mm}}$\\
\midrule
Female & Low& $0.1$ & $0$ & $-1$ & $1$ & $1$ & $1$ & $1$ & $1$ & $1$ \\
Male& Low & $0.4$& $1$ & $0$ & $1$ & $1$ & $\frac{\alpha}{3}$ & $1$ & $1$ & $1$ \\
Female& High & $0.1$ & $0$& $-1$ & $1$ & $1$ & $1$ &$1$ & $\alpha - 2$& 1 \\
Male & High & $0.4$& $2$ & $0$ & $1$ & $1$ & $\frac{\alpha}{3}$ &$1$ &$\alpha - 2$ &1\\
\bottomrule
\multicolumn{11}{l}{\tiny \emph{Legend}: $\pi^{\mathrm{u}}$: optimal unrestricted; $\pi^{\mathrm{af}}$: action fairness; $\pi^{\alpha}$: envy-free fairness; $\pi^{\mathrm{mm}}$: max-min fairness}
\end{tabular}
\end{table*}

\subsection{Derivation of toy example}
We proceed as in Example 1 and calculate the conditional policy values
\begin{align}
    V_\mathrm{F}(\pi) & = -(1 - \pi(\mathrm{F}, \mathrm{L})) - (1 - \pi(\mathrm{F}, \mathrm{H})) \\
    &=  \pi(\mathrm{F}, \mathrm{L}) + \pi(\mathrm{F}, \mathrm{H}) - 2
\end{align}
and
\begin{align}
    V_\mathrm{M}(\pi) & = \pi(\mathrm{M}, \mathrm{L})  + 2 \pi(\mathrm{M}, \mathrm{H}).
\end{align}

The overall policy value is
\begin{align}
    V(\pi) & = 0.2 V_\mathrm{F}(\pi) + 0.8 V_\mathrm{M}(\pi) \\
    &= 1.6\pi(\mathrm{M}, \mathrm{H}) +0.8 \pi(\mathrm{M}, \mathrm{L}) + 0.2\pi(\mathrm{F}, \mathrm{L}) +0.2 \pi(\mathrm{F}, \mathrm{H}) - 0.4
\end{align}

We immediately obtain, the optimal unrestricted policy is $\pi^\mathrm{u} \equiv \pi^\mathrm{af} \equiv \pi^\mathrm{mm} \equiv 1$ because all policy terms are positive. To obtain envy-free policies $\pi^{\mathrm{af} + \alpha}$ and $\pi^{\alpha}$, we write down the constraints as
\begin{align}\label{eq:app:policy_contraint1}
    \Delta(\pi^\alpha) = |2 + \pi^\alpha(\mathrm{M}, \mathrm{L})  + 2 \pi^\alpha(\mathrm{M}, \mathrm{H}) - \pi^\alpha(\mathrm{F}, \mathrm{L}) - \pi^\alpha(\mathrm{F}, \mathrm{H})| \leq \alpha
\end{align}
and 
\begin{align}\label{eq:app:policy_contraint2}
    \Delta(\pi^{\mathrm{af} + \alpha}) = 2 + \pi^{\mathrm{af} + \alpha}(\mathrm{H}) \leq \alpha.
\end{align}
Eq.~\eqref{eq:app:policy_contraint2} implies $\pi^{\mathrm{af} + \alpha}(\mathrm{H}) = \alpha - 2$ (for $\alpha \geq 2$) and $\pi^{\mathrm{af} + \alpha}(\mathrm{L}) = 1$. Eq.~\eqref{eq:app:policy_contraint1} yields a linear constrained optimization problem with solution $\pi^\alpha(\mathrm{F}, \mathrm{L}) = \pi^\alpha(\mathrm{F}, \mathrm{H}) = 1$ and $\pi^\alpha(\mathrm{M}, \mathrm{L}) = \pi^\alpha(\mathrm{M}, \mathrm{H}) = \alpha / 3$.

\newpage

\section{\model learning algorithm}\label{app:algorithm}
Algorithm~\ref{alg:fair-policy} provides the learning algorithm of \model. The algorithm consists of two consecutive steps, namely the fair representation learning step and the the policy learning step. For the policy learning step, the specification of the policy loss is needed, namely one of no value, envy-free, and max-min fairness: $\mathcal{L}_\pi^{\mathrm{m}}(\cdot) \in \{\hat{V}^\mathrm{m}(\cdot), \hat{V}^\mathrm{m}_\lambda(\cdot), \min_{s \in S} \hat{V}^\mathrm{m}_s(\cdot)\}$.


\begin{algorithm}
    \caption{\model: fair representation and policy learning via iterative gradient descent}\label{alg:fair-policy}
    \begin{algorithmic}
        \State {\bfseries Input:} representation networks hyperparameters (number of epochs $n_{r}$, learning rate $\eta_r$, action fairness parameter $\gamma$), policy network hyperparameters (number of epochs $n_{p}$, learning rate $\eta_p$, policy loss $\mathcal{L}_\pi^{\mathrm{m}}(\cdot)$) 
        \State Initialize $\theta_Y^{(0)}, \theta_\Phi^{(0)}, \theta_S^{(0)} \sim$ Kaiming-Uniform \Comment{Step 1. Fitting the representation networks}
        \For{$k = 1$ {\bfseries to} $n_{r}$ }
            \State Forward pass of the base representation / outcome prediction / sensitive attribute networks with $\theta_Y^{(k-1)}, \theta_\Phi^{(k-1)}, \theta_S^{(k-1)}$
            \State $\theta_Y^{(k)} \gets \theta_Y^{(k-1)} - \eta_r \nabla_{\theta_Y} \big[ \mathcal{L}_Y(\theta_\Phi^{(k-1)}, \theta_Y^{(k-1)}) \big]$ 
            \State $\theta_\Phi^{(k)} \gets \theta_\Phi^{(k-1)} - \eta_r \nabla_{\theta_\Phi} \big[ \mathcal{L}_Y(\theta_\Phi^{(k-1)}, \theta_Y^{(k-1)}) + \gamma \mathcal{L_\mathrm{conf}}(\theta_\Phi^{(k-1)}, \theta_S^{(k-1)}) \big]$
            \State Forward pass of the base representation / sensitive attribute networks with $\theta_\Phi^{(k)}, \theta_S^{(k-1)}$
            \State $\theta_S^{(k)} \gets \theta_S^{(k-1)} - \eta_r \nabla_{\theta_S} \big[ \gamma \mathcal{L}_S(\theta_\Phi^{(k)}, \theta_S^{(k-1)}) \big]$
        \EndFor
        \State $\hat{\theta}_\Phi \gets \theta_\Phi^{(n_{r})}$

        \State Initialize $\theta^{(0)} \sim$ Kaiming-Uniform \Comment{Step 2. Fitting the policy network}
        \For{$k = 1$ {\bfseries to} $n_{p}$ } 
            \State Forward pass of the policy network with $\theta^{(k-1)}$
            \State $\theta^{(k)} \gets \theta^{(k-1)} - \eta_p \nabla_{\theta} \big[\mathcal{L}_\pi^\mathrm{m}(\pi_{\theta^{(k-1)}}(\hat{\Phi}(X)))\big]$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\newpage

\section{Hyperparameter tuning}\label{app:hyperparam}

We followed best practices in causal machine learning \citep[e.g.,][]{Bica.2021, Curth.2021} and performed extensive hyperparameter tuning for \model. We split the data into a training set (80\%) and a validation set (10\%). We then performed 30 random grid search iterations and chose the set of parameters that minimized the respective training loss on the validation set. In particular, the tuning procedure was the same for all baselines, which ensures that the performance differences reported in Section~\ref{sec:experiments} are  not due to larger flexibility but are due to the different methods themselves.

We performed hyperparameter tuning for all neural networks in \model, i.e., the representation networks and the policy network. For the real-world data, we also used TARNet \citep{Shalit.2017} in order to estimate the nuisance parameters. We first performed hyperparameter tuning for TARNet and for the representation networks, before tuning the policy neural networks by using the input from the tuned neural networks. The tuning ranges for the hyperparameter are shown in Table~\ref{tab:hyper_sim} (simulated data) and Table~\ref{tab:hyper_real} (real-world data). 

\begin{table}[ht]
\caption{Hyperparameter tuning ranges (simulated data).}
\centering
\label{tab:hyper_sim}
\begin{tabular}{lll}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\textsc{Neural network} & \textsc{Hyperparameter} & \textsc{Tuning range} \\
\midrule
All neural networks&Dropout probability & $0$, $0.1$, $0.2$ \\
&Batch size &$32$, $64$, $128$  \\
& Epochs & 400 \\
\midrule
Representation networks & Learning rate & 0.0001, 0.0005, 0.001, 0.005\\
 & Hidden layer / representation size & 2, 5, 10\\
& Weight decay & 0, 0.001 \\
Policy network & Learning rate & 0.00005, 0.0001, 0.0005, 0.001 \\
 & Hidden layer size  & 5, 10, 15, 20 \\
 & Weight decay & 0\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Hyperparameter tuning ranges (real-world data).}
\centering
\label{tab:hyper_real}
\begin{tabular}{lll}
\noalign{\smallskip} \toprule \noalign{\smallskip}
\textsc{Neural network} & \textsc{Hyperparameter} & \textsc{Tuning range} \\
\midrule
All neural networks&Dropout probability & $0$, $0.1$, $0.2$, $0.3$ \\
&Batch size &$32$, $64$, $128$  \\
\midrule
TARNet & Learning rate & 0.0001, 0.0005, 0.001, 0.005\\
 & Hidden layer sizes & 5, 10, 20, 30\\
& Weight decay & 0 \\
& Epochs & 200 \\

Representation networks & Learning rate & 0.0001, 0.0005, 0.001, 0.005\\
 & Hidden layer / representation size & 2, 5, 10\\
& Epochs & 400 \\
& Weight decay & 0, 0.001 \\
Policy network & Learning rate & 0.00005, 0.0001, 0.0005, 0.001 \\
 & Hidden layer size  & 5, 10, 15, 20 \\
& Epochs & 300 \\
 & Weight decay & 0\\
\bottomrule
\end{tabular}
\end{table}

The tables include both the hyperparameter rangers shared across all neural networks and the network-specific hyperparameters. For reproducibility purposes, we report the selected hyperparameters as \emph{.yaml} files.\footnote{Codes are in the supplementary materials and at \url{https://anonymous.4open.science/r/FairPol-163C}.}



\newpage
~\newpage

\section{Details regarding real-world data}
\label{app:data}

In our experiment with real-world data, we use data from the Oregon health insurance experiment (OHIE)\footnote{Data available here: https://www.nber.org/programs-projects/projects-and-centers/oregon-health-insurance-experiment} \citep{Finkelstein.2012}. The OHIE was conducted as a large-scale experiment among public health to assess the effect of health insurance on several outcomes such as health or economic status. In 2008, a lottery draw offered low-income, uninsured adults in Oregon participation in a Medicaid program, providing health insurance. Chosen individuals were offered free health insurance. After a period of 12 months, a survey was conducted to evaluate several outcomes of the participants.

In our analysis, the decision to sign up for the Medicaid program is the action $A$, and the overall out-of-pocket cost for medical care within the last 6 months is the outcome $Y$. The sensitive covariate $S$ we consider is gender. Furthermore, we include the following covariates $X$: age, the number of people the individual signed up with, the week the individual signed up, the number of emergency visits before the experiment, and language. We extract $n=24,646$ observations from the OHIE data and plot the histograms of all variables in Fig.~\ref{fig:descriptive}. 
\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{figures/plot_descriptive.pdf}
\caption{Histograms of marginal distributions (real-world data).}
\label{fig:descriptive}
\end{figure}
\end{APPENDICES}

We split the data randomly into a train (0.7\%), validation (0.1\%), and test set (0.2\%) and perform hyperparameter tuning using the validation set. The evaluation metrics are then computed using the test set.


%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%

