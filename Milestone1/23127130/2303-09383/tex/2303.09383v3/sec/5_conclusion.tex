\section{Conclusions}
\label{sec:conclusions}
With the rapid development of Augmented Reality (AR) and Virtual Reality (VR) technologies, there is an increasing demand for predicting and understanding human gaze behavior \cite{park2021mosaic,kaplanyan2019deepfovea,park2019advancing}, with scanpath prediction being a challenging task. For those AR/VR applications requiring a high input resolution ($360^\circ$), discretizing fixations into a coarse grid incurs a non-negligible loss in accuracy.
In this work we presented HAT, a generic attention scanpath prediction model. Built from a simple dense prediction framework \cite{cheng2021per}, HAT circumvents the drawbacks of discretizing fixations as in prior state of the arts \cite{yang2020predicting,chen2021predicting,yang2022target}. Inspired by the human vision system, HAT uses a novel foveated working memory which dynamically updates its knowledge about the scene as it changes its fixation. We show that HAT achieves new SOTA performance, not only in predicting free-viewing fixation scanpaths, but also scanpaths in target-present and target-absent search. In demonstrating this broad scope, our HAT model sets a new bar in the computational attention of attention control. 

% Models that predict top-down attention (TP/TA search fixations), modulated by an external goal, have wide applicability to attention-centric HCI. For example, 
% faster attention-based rendering that leverages the prediction of a user's attention as they play a VR/AR game and home robots incorporating search-fixation-prediction models will be better at inferring a user's need (i.e., their search target).
% Home robots incorporating search-fixation-prediction models will be better able to infer a users' need (i.e., their search target) and autonomous driving systems can attend to image input like an undistracted driving expert.
% Applications of FV attention prediction exist in foveated rendering \cite{kaplanyan2019deepfovea} and online video streaming \cite{park2021mosaic}.

\myheading{Acknowledgement}. {The authors would like to thank Xianyu Chen for providing the source code for PathGAN. This project was supported by US National Science Foundation Awards IIS-1763981, IIS-2123920, NSDF DUE-2055406, and the SUNY2020 Infrastructure Transportation Security Center, and a gift from Adobe.}