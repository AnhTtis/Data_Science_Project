\section{Introduction}
\label{sec:intro}
Attention, a cognitive process that allows humans to selectively allocate their limited cognitive resources to specific regions of the visual world, plays a crucial role in human perception system. Understanding and predicting human (visual) attention will enable numerous applications such as assistive technologies that can anticipate a person's needs and intents, perceptions system that can prioritize processing regions of human interest and enhancing the accuracy and speed of various visual tasks (e.g., object detection), and image/video compression that allocates more resources to encoding and transmitting high-attention regions, optimizing the use of bandwidth.

Human attention control can take two broad forms. One is bottom-up, meaning that attention saliency signals are computed from the visual input and used to prioritize shifts of attention. The same visual input should therefore lead to the same shifts of bottom-up attention. The second type of attention is top-down, meaning that a task or goal is used to control attention. Given a kitchen scene, very different fixations are observed depending on whether a person is searching for a clock or a microwave oven \cite{zelinsky2021predicting}. These two types of attention control spawned two separate literatures on gaze fixation prediction (the accepted measure of attention), one where studies use a free-viewing task to study questions of bottom-up attention and the other using a goal-directed task (typically, visual search) to study top-down attention control.
Consequently, most models have been designed to address {\it either} bottom-up {\it or} top-down attention, not both. {\it Can a single model architecture predict both bottom-up and top-down attention control?}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/teaser2.pdf}
  \caption{Given an image, the proposed {\bf HAT} is able to predict scanpaths under three settings target-present search for TV; target-absent scanpath for sink; and free viewing. Importantly, HAT outperforms previous state-of-the-art scanpath prediction methods on multiple datasets across three settings: target-present, target-absent visual search and free viewing, that were studied separately.}
  \label{fig:teaser}
\end{figure}

Our answer to this question is HAT, a Human Attention Transformer that generally predicts scanpaths of fixations, meaning that it can be applied to both top-down visual search and bottom-up free viewing tasks (\Cref{fig:teaser}). Devising a unified model architecture capable of predicting both bottom-up and top-down attention control is nontrivial: 1) predicting human fixation scanpaths requires the model to have a spatio-temporal understanding of the fixated image contents and their relationship to the external goals; and 2) predicting top-down and bottom-up attention requies the model to capture both low-level features and high-level semantics of the input image. 
HAT addresses these issues by using a novel transformer-based design and a simplified foveated retina. Together, these components forge a novel paradigm, constituting a form of {\it dynamically-updating visual working memory}.
Traditional approaches have leaned on recurrent neural networks (RNNs) to uphold a dynamically updated hidden vector conveying information across fixations \cite{chen2021predicting,sun2019visual,zelinsky2019benchmarking,assens2018pathgan}.
Alternatively, simulations of a foveated retina have combined multi-resolution information at pixel \cite{zelinsky2019benchmarking}, feature \cite{yang2022target}, or semantic levels \cite{yang2020predicting}.
However, these methods present drawbacks: RNNs sacrifice interpretability, while multi-resolution simulations fall short in capturing crucial temporal and spatial information integral for scanpath prediction.

In addressing these challenges, we leverage a computational attention mechanism \cite{vaswani2017attention} to dynamically assimilate spatial, temporal, and visual information acquired at each fixation into working memory~\cite{oberauer2019working, olivers2020attention}. This empowers HAT to discern a set of task-specific attention weights for amalgamating information from working memory and forecasting human attention control. 
This innovative mechanism sheds light on the intricate relationship between human attention and working memory~\cite{desimone1995neural, gazzaley2012top}, rendering HAT not only cognitively plausible but also ensuring the interpretability of its predictions. Furthermore, in contrast to prior methods \cite{yang2020predicting,yang2022target,chen2021predicting}, HAT treats scanpath prediction as a sequence of dense prediction tasks with per-pixel supervision, successfully avoiding the need for discretizing fixations. This enhances the method's efficacy, particularly in scenarios involving high-resolution imagery.

% Besides, previous scanpath prediction methods \cite{yang2020predicting,yang2022target,chen2021predicting} utilized a coarse grid for fixation discretization, reducing accuracy due to limited gaze data. This limitation diminishes the method's effectiveness in scenarios with high-resolution imagery. While raising the grid resolution seems like a straightforward solution, it exponentially expands the action space, posing challenges in training reinforcement learning networks due to the substantial increase in required data. In contrast, HAT approaches scanpath prediction as a series of dense prediction tasks with per-pixel supervision, effectively sidestepping the discretization issue.

% Humans have a foveated retina, meaning that we receive high-resolution input from our central vision (where we are currently fixated) and progressively lower resolution inputs with increasing peripheral eccentricity. By actively shifting our high-resolution central vision, we accumulate with each fixation the information that we need to perform a task (e.g., visual search) or explore a scene (free viewing). Two classes of methods exist for representing this dynamic fixation-by-fixation acquisition of information by human attention. One class uses a recurrent neural network (RNN) to maintain a dynamically updated hidden vector that conveys information across fixations \cite{chen2021predicting,sun2019visual,zelinsky2019benchmarking,assens2018pathgan}. The other class simulates a foveated retina by combining multi-resolution information at the pixel level \cite{zelinsky2019benchmarking}, feature level \cite{yang2022target} or semantic level \cite{yang2020predicting}. Both classes of methods, however, have drawbacks: RNNs lack interpretability and multi-resolution methods of simulating a foveated retina  \cite{zelinsky2019benchmarking,yang2020predicting,yang2022target} fail to capture critical temporal and spatial information useful for scanpath prediction.

% To address these problems, HAT uses a novel transformer-based design and a simplified foveated retina that collectively work to create a form of {\it dynamically-updating visual working memory}. A computational attention mechanism \cite{vaswani2017attention} integrates the spatial and temporal visual information acquired at each fixation into working memory~\cite{oberauer2019working, olivers2020attention}, enabling HAT to learn a set of task-specific attention weights for aggregating information from working memory and predicting human attention control. This mechanism hints at the relationship between human attention and working memory~\cite{desimone1995neural, gazzaley2012top}, making HAT cognitively plausible and its predictions interpretable. 

To demonstrate HAT's generality, we predict scanpaths
under three settings, target-present (TP) and target-absent (TA) visual search, and free-viewing (FV), covering both top-down and bottom-up attention. %, and in two datasets (COCO-Search18 \cite{chen2021coco} and COCO-FreeView \cite{Chen_2022_CVPR}). %Previous scanpath prediction models focused on visual search
In  previous work predicting search scanpaths \cite{yang2020predicting,yang2022target,chen2021predicting}, separate models were trained for the TP and TA settings. HAT is a single model establishing new SOTA in both TP and TA search-scanpath prediction. When trained with FV scanpaths, HAT also achieves top performance relative to baselines. HAT advances SOTA in cNSS by 95\%, 94\% and 104\% under the TP, TA and FV settings on the COCO-Search18 dataset \cite{chen2021coco} and the COCO-FreeView dataset \cite{Chen_2022_CVPR}, respectively.
% On these benchmarks, HAT performs similarly or better than the more specialized SOTA architectures.

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We propose HAT, a novel transformer architecture integrating visual information at two different eccentricities (approximating a foveated retina) to predict the spatial and temporal allocation of human attention.

    % resolution of the output
    \item We formulate scanpath prediction as a sequential dense prediction task without fixation discretization, making HAT applicable to high-resolution input.
    
    \item The HAT architecture can be broadly applied to different attention control tasks, evidenced by the SOTA scanpath predictions in both visual search and free-viewing tasks. 
    % We also demonstrate that HAT's predictions of human attention are highly interpretable.
    
    % Merge to second bullet
    \item HAT's attention predictions offer high interpretability, making it useful for studying gaze behavior.
\end{enumerate}
% {\bf (1)} We propose HAT, a novel transformer architecture  integrating visual information at two different eccentricities (approximating a foveated retina) to predict spatial and temporal allocation of human attention, the fixation scanpath;
% {\bf (2)} We show that our single HAT architecture can be broadly applied to both bottom-up and top-down forms of attention control, as demonstrated by SOTA scanpath predictions in TP and TA search and FV settings; and
% {\bf (3)} We propose formulating scanpath prediction as a dense prediction task, so that HAT is applied to high-resolution input and remove the need for fixation discretization.
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/model_overview.pdf}
  \caption{{\bf HAT overview.} We use encoder-decoder CNNs to extract two sets of feature maps $P_1$ and $P_4$ of different spatial resolutions. A working memory with a capacity of $\lambda$ tokens is constructed by combining all feature vectors from $P_1$ with the feature vectors of $P_4$ at previously fixated locations, representing information extracted from the periphery and central fovea. A transformer encoder is used to dynamically update the working memory at every new fixation. Then, HAT produces $N$ per-task queries of dimension $C$ (e.g., clock search and mouse search), with each learning to aggregates task-specific information from the shared working memory for predicting the fixations for its own task. Finally, the updated queries are convolved with $P_4$ to yield the fixation heatmaps after a MLP layer, and projected to the termination probabilities in parallel. Note, although this figure depicts visual search, the framework also applies for free viewing.}
  \label{fig:overview}
\end{figure*}