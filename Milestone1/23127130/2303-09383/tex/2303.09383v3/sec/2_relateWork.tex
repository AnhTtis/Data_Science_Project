\section{Related Work}
\label{sec:relateWork}
\textbf{Saliency prediction.}
Predicting and understanding human gaze control has been a topic of interest for decades in psychology~\cite{yarbus1967eye, findlay2001visual, zelinsky2008theory, wolfe2017five}, but it has only recently attracted the researcher's attention in computer vision. In particular, Itti's seminal work \cite{itti2000saliency} on the saliency model has triggered a lot of interest on human attention modeling in computer vision community and facilitated many other studies identifying and modeling the salient visual features of an image (i.e., saliency prediction) ~\cite{borji2013state,kummerer2017understanding, kruthiventi2017deepfix,huang2015salicon,kummerer2014deep, cornia2018predicting,jiang2015salicon,jetley2016end, borji2015salient, masciocchi2009everyone, berg2009free,wang2017deep,wang2019revisiting}. However, the scope of these work is often narrowly focused on predicting human natural eye-movements without a specific visual task (i.e., free-viewing), ignoring another important form of attention control, goal-directed attention. Moreover, saliency models only model the spatial distribution of fixations and do not predict the temporal order between fixations. Scanpath prediction is more challenging problem because it requires predicting not only \textit{where} a fixation will be, but also \textit{when} it will be there.
\\
\textbf{Scanpath prediction.}
Many existing scanpath prediction deep neural networks (DNN) focus on predicting the free-viewing scanpaths \cite{assens2017saltinet,assens2018pathgan,sun2019visual,kummerer2022deepgaze}, primarily due to their close connection to saliency modeling. However, these models are inherently constrained in their ability to capture the full spectrum of human attention control, particularly goal-directed attention---a fundamental cognitive process that underlies various everyday visual tasks such as navigation and motor control. 
% However, these free-viewing scanpath prediction models only capture a limited aspect of human attention control, and do not generalize to goal-directed attention, which underlies everyday visual tasks like navigation and motor control.
Although goal-directed human attention has been studied for decades~\cite{yarbus1967eye, land2009looking,wolfe1998visual} in cognitive science (mainly in the context of visual search~\cite{najemnik2005optimal, torralba2006contextual, zelinsky2008theory}), the development of DNNs for goal-directed scanpath prediction lags behind those designed for free-viewing tasks, partly due to the lack of data.
To tackle this problem, \citet{chen2021coco} created the first large-scale goal-directed gaze dataset with 18 search targets, COCO-Search18.
In \cite{yang2020predicting}, an inverse reinforcement learning model showed superior performance on COCO-Search18 in predicting TP scanpaths. Later, \citet{chen2021predicting} showed that a reinforcement learning model directly optimized on the scanpath similarity metric can predict VQA scanpaths, as well as on TP search scanpaths. \citet{yang2022target,rashidi2020optimal} also proposed a more generalized scanpath prediction model that can be applied to both target-present and target-absent visual search scanpaths. 
Most recently, a transformer-based scanpath prediction model, Gazeformer \cite{mondal2023gazeformer}, further advanced the TP search scanpath prediction performance on COCO-Search18. However, none of these work have demonstrated the generalizability to all three settings (i.e., TP, TA and FV). In this work, we design a generic scanpath model that generalizes to both free-viewing and visual search tasks. 
% Contrary to the previous approach (mostly CNN-based), we leverage the power of the Transformer architecture~\cite{vaswani2017attention} and a dynamically-updating working memory, which collectively helps to learn a complex spatio-temporal fixation representations that can be applied to various visual tasks. We also approach scanpath prediction as a dense prediction problem, eliminating the need for discretizing the fixations into lower-resolution grid space, which causes an inevitable loss of precision in the fixation prediction as in previous methods \cite{chen2021predicting,yang2020predicting,yang2022target}. Our model is similar to IVSN \cite{zhang2018finding} in using a top-down query to match features in a feature map for each visual task (free-viewing or search). However, a critical difference is that in IVSN the query is directly extracted from a given task-related image; whereas in our setting HAT automatically learn the task-specific queries from the training fixations using a uniquely designed Transformer architecture.
\\
\textbf{Scanpath Transformers.}
The transformative game-changing impact of Transformers \cite{vaswani2017attention} has been widely recognized in natural language processing and beyond. In computer vision, Transformers have demonstrated outstanding capabilities across a wide range of computer vision tasks, such as image recognition \cite{dosovitskiy2020image,liu2021swin,touvron2021training}, object detection \cite{carion2020end,zhu2020deformable} and image segmentation \cite{ranftl2021vision,cheng2022masked,xie2021segformer}.
\citet{mondal2023gazeformer} introduced Gazeformer, a Transformer-based model specifically designed for zero-shot visual search scanpath prediction. In contrast, our proposed model is generic, capable of predicting both visual search and free-viewing scanpaths. Additionally, our model diverges from other Transformer-based architectures by drawing inspiration from the human vision system. It incorporates a novel foveation module simulating a simplified foveated retina, thereby establishing a dynamic visual working memory for enhanced scanpath prediction.

% \myheading{Scanpath datasets.}

% Our focus on scanpath prediction here will be in the context of visual search and free viewing. Although we believe that our approach is general, scanpath prediction scenarios such as VQA \cite{chen2021predicting} and driving \cite{baee2021medirl} are currently outside the scope of our work.

% Human attention can be broadly categorized into top-down (visual search) attention and bottom-up (free-viewing) attention. For free-viewing scanpath, early models \cite{itti2000saliency,walther2006modeling} predicts a static saliency map and employ a winner-take-all heuristic to sample fixations in order. 

% These models are designed for free-viewing scanpath prediction and it is nontrivial to adapt them for visual search with multiple tasks,
% \footnote{A naive solution is to train a separate model for each visual search task, which is inefficient and costly.}. 










% \myheading{Dense prediction.}
% Dense prediction \cite{ranftl2021vision,ronneberger2015u,long2015fully,cheng2021per,cheng2022masked} concerns the problem of predicting a label for every pixel in the image. Dense prediction networks often employ a encoder-decoder architecture. The encoder (a.k.a. backbone) progressively compresses the image into lower-resolutional feature maps; the decoder reverses the process by upsampling the feature maps and converting them to the final prediction with a resolution same as or close to the resolution of the input image.
% Here, to avoid fixation discretization as in \cite{zelinsky2019benchmarking,yang2020predicting,yang2022target,chen2021predicting}, HAT, inspired by the dense prediction architectures in \cite{cheng2022masked,cheng2021per}, predicts a fixation probability for each pixel given the image input and the fixation history. Notably, HAT equips the dense prediction network with a foveated retina which {\it dynamically} integrates the information acquired through fixations for scanpath prediction.