% \clearpage
% \setcounter{page}{1}
% \maketitlesupplementary

\section{Experiments on OSIE and MIT1003}

\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
\toprule 
 & SS & cIG & cNSS & cAUC \\
\midrule
Human consistency & 0.380 & - & -\\\midrule
Chen \etal~\cite{chen2021predicting} & 0.326 & -1.526 & 2.288 & 0.920\\
HAT & {\bf 0.386} & {\bf 2.434} & {\bf 4.515} & {\bf 0.973}\\
\bottomrule
\end{tabular}
\caption{{\bf Comparing free-viewing scanpath prediction algorithms on OSIE} (rows) using multiple scanpath metrics (columns). The best results are highlighted in bold.}
\label{tb:osie}
\end{center}
\end{table}


\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
\toprule 
 & SS & cIG  & cNSS & cAUC \\
\midrule
Human consistency & 0.363 & - & - & -\\\midrule
Chen \etal~\cite{chen2021predicting} & 0.260 & 0.042 & 1.408 & 0.927\\
% {HAT w/o pretrain} & 0.324 & 0.762 & 2.116 & 0.941\\
HAT & {\bf 0.364} & {\bf 1.311} & {\bf 2.966} & {\bf 0.956}\\
\bottomrule
\end{tabular}
\caption{Comparing free-viewing scanpath prediction algorithms (rows) on {\bf MIT1003 training set using 5-fold cross validation} using multiple scanpath metrics (columns). The best results are highlighted in bold.}
\label{tb:mit1003}
\end{center}
\end{table}

To further validate the effectiveness of our proposed HAT in free-viewing scanpath prediction, we compare HAT to the previous state-of-the-art method in free-viewing scanpath prediction,  \citet{chen2021predicting}, using the OSIE dataset \cite{xu2014predicting} and the MIT1003 dataset \cite{judd2009learning}. Here we only report SS, cIG, cNSS and cAUC and do not use SemSS because free-viewing attention is bottom-up and does not rely on semantics. Moreover, OSIE and MIT1003 do not contain pixel-wise segmentation annotation which is required in SemSS. \cref{tb:osie} and \cref{tb:mit1003} consistently show that HAT surpasses \citet{chen2021predicting} in all metrics by a large margin especially in cIG and cNSS on both free-viewing datasets. The results are consistent with our findings in Tab. 3 of the main text---HAT accurately predicts the scanpaths (reflected by SS), with well-calibrated confidence (as evidenced by the high cIG and cNSS). 
Additionally, we compare HAT to the best alternative overall, \citet{chen2021predicting}, by evaluating the models trained using COCO-FreeView on an {\it unseen} dataset MIT1003 in \cref{tb:mit1003_unseen}. The results show that HAT outperforms \citet{chen2021predicting} in all metrics and with significant improvement in cIG, cNSS and cAUC. This suggests that Chen~\etal's model is prone to be overconfident, whereas HAT better calibrates the confidence in predicting free-viewing fixations and thus provides a more robust prediction of human attention with better generalizability to unseen datasets.

\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
\toprule
 & SS & cIG  & cNSS & cAUC \\
\midrule
Human consistency & 0.363 & - & - & -\\\midrule
Chen \etal~\cite{chen2021predicting} & 0.210	&-9.735	&0.186	&0.750\\
% HAT w/o pretrain & 0.225 & 0.573	&{1.841}	&{0.930}\\
HAT & {\bf 0.251}	& {\bf 1.052}	& {\bf 2.577} & {\bf 0.951}\\
\bottomrule
\end{tabular}
\caption{{\bf Generalization to an unseen dataset MIT1003}. Both models are trained on COCO-Freeview. The best results are in bold.}
\label{tb:mit1003_unseen}
\end{center}
\end{table}


\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cc|ccc}
\toprule 
 & \multicolumn{2}{c|}{Target-present} & \multicolumn{2}{c}{Target-absent} \\
 \midrule
Target & Seen & Unseen  & Seen & Unseen \\
\midrule
Bottle & Others & Food & Others & Kitchen\\
Bowl & Others & Kitchen & Others & Kitchen\\
Car & Indoor & Outdoor & Vehicle & Others\\
Chair & Others & Kitchen & Indoor & Outdoor\\
Clock & Others & Building & Others & Office\\
Cup & Others & Office & Food & Others\\
Fork & - & - & Others & Bathroom\\
Keyboard & Office & Others & Others & Bedroom\\
Knife & Food & Others & Others & Bathroom\\
Laptop & Others & Living & Others & Living\\
Microwave & Kitchen & Others & Kitchen & Others\\
Mouse & Office & Others & Others & Office\\
Oven & - & - & Others & Living\\
Potted plant & Indoor & Outdoor & Others & Food\\
Sink & Others & Kitchen & Indoor & Outdoor\\
Stop sign & - & - & Others & Vehicle\\
Toilet & Indoor & Outdoor & Others & Bedroom\\
TV & Others & Office & Indoor & Outdoor\\

\bottomrule
\end{tabular}
\caption{\textbf{Data split for scene-to-scene training and testing.} COCO-Search-18 includes 12 scenes: outdoor, street, building, vehicle, food, eatery, kitchen, bathroom, bedroom, living-room (living), dining-room, office. \textbf{Others} in the table includes some of the scenes excluding the unseen scene. \textbf{Outdoor} in the table includes outdoor, street, building and vehicle. In target-present, fork, oven and stop sign are not splittable because they only contain one scene, so we remove them from testing.}
\label{tb:partition}
\end{center}
\end{table}
\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{c|c|cccc}
\toprule 
 \multicolumn{2}{c|}{} & SS & cIG & cNSS & cAUC \\
\midrule
\multirow{4}{*}{TP} & Human(Seen) & 0.520 & - & -\\
 & HAT(Seen) & 0.499 & 2.074 & 5.032 & 0.976\\
  \cmidrule{2-6}
 & Human(Unseen) & 0.546 & - & -\\
 & HAT(Unseen) & 0.481 & 2.454 & 4.537 & 0.973\\\midrule
 \multirow{4}{*}{TA} & Human(Seen) & 0.364 & - & -\\
 & HAT(Seen) & 0.368 & 1.586 & 2.852 & 0.955\\
 \cmidrule{2-6}
 & Human(Unseen) & 0.416 & - & -\\
 & HAT(Unseen) & 0.416 & 2.075 & 3.115 & 0.962\\
\bottomrule
\end{tabular}
\caption{Quantitative result of scene-to-scene generalization on target-present and target-absent task. The first four columns are analysis of scene-to-scene target-present search, and the last four columns are analysis of scene-to-scene target-absent search. Each search contains human consistency and testing results on seen (first two columns of the search) and unseen scenes (last two columns of the search).}
\label{tb:scene2scene-metrics}
\end{center}
\end{table}


\begin{figure*}[t]
  \centering
    {\includegraphics[width=\linewidth]{images/scene2scene.pdf}}\label{fig:scene2scene}
    \caption{\textbf{Visual search scanpath visualization for unseen scenes predictions.} The first row is human consistency and the second row is test result on unseen scenes. The first three columns are trained for target-present tasks for mouse, cup and car search, the second three columns are trained for target-absent tasks for chair, microwave and bowl search. }
  \label{fig:scene2scene}
\end{figure*}econd ro
\section{Scene-to-scene Generalization.} 
To further demonstrate the generalization ability of HAT to unseen scenes, we re-partition the COCO-Search18 dataset \cite{chen2021coco} by {\bf scenes}. The new partition contains two test sets: one test set shares the scenes as the training set and the other test set only contains unseen (new) scenes from the training set. We partition COCO-Search18 for each category independently (shown in \cref{tb:partition}). For instance, for the target-absent microwave search task, the training set only contains kitchen scenes while the unseen test set has a variety of other scenes including living rooms, dining rooms, bedrooms and outdoor scenes. To further ensure the unseen images do not exist in the training set, we remove the unseen images for all tasks from the training set because some tasks share the same image stimuli. In the new partition, the target-present set consists of 2170 training images, 568 testing images of unseen scenes, and 273 testing images of seen scenes. The target-absent set consists of 2299 training images, 378 testing images of unseen scenes, and 282 testing images of seen scenes. 

\cref{tb:scene2scene-metrics} presents results of HAT in predicting the TP and TA search scanpaths under both seen and unseen novel scenes. We use human consistency as the baseline. For both tasks, the gap between human consistency and HAT in seen test set is smaller than that in the unseen test set, which is expected. Importantly, HAT's performance on the unseen scenes is on par with human consistency in TA setting although worse in TP setting. Note that the performance difference between seen and unseen human consistency of TA setting is due to the fact that human consistency on the TA test data with seen scenes in the new partition is low. These results suggest that HAT learns to extrapolate from scene to scene and generalize well on novel scenes in visual search scanpath prediction.
The visualization of predicted scanpaths in \cref{fig:scene2scene} further reinforces our observations. By comparing HAT's predicted scanpaths with the ground-truth human scanpaths for unseen scenes in both TP and TA settings, we unveil HAT's robust generalization. For example, in a TP car search task, HAT trained on {\it indoor} scenes successfully located the car on the top-left corner in an {\it outdoor} scene much like humans do. Similarly, when addressing microwave searches under target-absent conditions—with the training set exclusively comprising kitchen scenes—HAT demonstrates significant generalization. This is evident in its proficient extension of predictive capabilities to living-room scenes, as showcased in the fifth column of \cref{fig:scene2scene}. These findings underscore HAT's consistent and robust generalization across diverse scenes, emphasizing its reliable performance in a spectrum of visual search scenarios.
% By comparing the human and HAT prediction, for instance, the car search in target present, HAT generalized well when training on cars in the indoor scene and testing on cars in the outdoor scene. For microwave search in target-absent, the training set only contains the kitchen scene, and it generalizes well in the living-room scene as shown in the fifth column in \cref{fig:scene2scene}. Thses indicate that HAT reliably generalizes across different scenes, underlining its robustness in varied visual search scenarios. 
% Under this partitioning, HAT will predict scanpaths for scenes it has not encountered during training, thereby demonstrating its ability to generalize to new data. Furthermore, this also indicates HAT's ability to adapt to typical scenes, such as unexpected scenarios like a fork in the bathroom or tv in the outdoor environments. The visualization of results \cref{fig:scene2scene} and quantitative result in \cref{tb:scene2scene-metrics} indicates that HAT reliably generalizes across different scenes, underlining its robustness in varied visual search scenarios. 



\section{Individual Scanpath Recall}
The importance of predicting personalized scanpath lies in the fact that each person’s unique life experiences shape their individual mental representations of scenes, resulting in personalized perceptions. Therefore, testing the model's ability to generate diversified scanpaths is crucial to learn individual perceptions and to avoid potential biases. To this end, we compute {\it scanpath recall} to measure the extent of individual representation within the model's predictions. For a human scanpath in the stimuli, we consider it to be covered if its sequence score with at least one prediction is higher than the threshold $\tau$. The ratio of covered human scanpaths to all human scanpaths is the recall of the stimuli. \cref{tb:recall} presents the average recall and sequence score of HAT and Gazeformer in both target-present and target-absent search scanpath prediction. For each visual stimuli, we sample 10 scanpaths from the fixation density map and set $\tau$ to the human consistency of sequence score (i.e., $\tau=0.5$ for TP and $\tau=0.381$ for TA). We can see that HAT outperforms Gazeformer in both recall and sequence score by a large margin in target-absent scanpath prediction. This is consistent with our findings in Sec. 4.1 of the main text. Although Gazeformer has a slightly higher sequence score than HAT in TP setting, HAT outperforms Gazeformer significantly in scanpath recall. This implies that HAT better captures the entire scanpath distribution from multiple subjects whereas Gazeformer tends to overfit to an ``average person", thereby repeatedly sampling similar scanpaths given the same image input. 
% \begin{algorithm}
% \caption{Calculate Recall}
% \label{alg:coverage}
% \begin{algorithmic}[t]
% \STATE \textbf{Input:} A list of sampled scanpath predictions $\hat{L}$, a list of ground truth scanpaths $L$, threshold $\tau$, number of scanpaths $N$
% \STATE \textbf{Output:} The recall of $\hat{L}$: $Recall$
% \STATE $coverage \leftarrow 0$
% \STATE $Score(\hat{l}, l)$: compute sequence score of two scanpaths
% \FOR{$l_i$ in $L$}
%     \STATE $S_i = [Score(\hat{l_1}, l_i), Score(\hat{l_2}, l_i), ...]$
% \ENDFOR
% \FOR{$i$ in $N$}
%     \IF{$max(S_i) \geq \tau$}
%         \STATE $TP \leftarrow coverage + 1$
%     \ENDIF
% \ENDFOR
% \STATE $Recall = \frac{coverage}{N}$
% \RETURN $Recall$
% \end{algorithmic}
% \end{algorithm}
\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cc|cc}
\toprule 
\multirow{2}{*}{}
 & \multicolumn{2}{c|}{Target-present} & \multicolumn{2}{c}{Target-absent} \\
\cmidrule{2-5}
 & Recall & SS  & Recall & SS \\
\midrule
Gazeformer~\cite{mondal2023gazeformer} & 0.563 & {\bf 0.489} & 0.428 & 0.357\\
HAT & {\bf 0.727} & {0.453} & {\bf 0.750} & {\bf 0.381}\\
\bottomrule
\end{tabular}
\caption{Recall and sequence score comparison between Gazeformer and HAT.}
\label{tb:recall}
\end{center}
\end{table}

\section{Additional Ablation Study}
\label{sec:add_ablation}
\setlength{\tabcolsep}{1.5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{c|c|ccccc}
\toprule 
 Pixel enc. & Pixel dec. & SemSS  & SS & cIG & cNSS & cAUC\\ 
\midrule
R50& MSD & 0.382 & 0.402 & 1.686 & 3.103 & 0.961 \\
R50& FPN & 0.367 & 0.388 & 1.582 & 2.908 & 0.958\\
R101& MSD & 0.372 & 0.397 & 1.598 & 2.998 & 0.961\\
Swin-B & {MSD} & 0.382 & 0.405 & 1.645 & 3.103 & 0.962\\
\bottomrule
\end{tabular}
\caption{{\bf Comparing different pixel encoder and pixel decoder} in HAT. The ablation experiments are done on the target-absent set of COCO-Search18.}
\label{tb:feat_ext}
\end{center}
\end{table}

\begin{table}[t]
% \vspace{-0.3cm}
\begin{center}
\begin{tabular}{ccc|cccccc}
\toprule
% \hline 
heads & $\alpha$ & $\beta$ & SemSS & SS & cIG & cNSS & cAUC\\
\midrule
4 & 2 & 4 & \textbf{0.382}  & \textbf{0.402} & \textbf{1.686} & \textbf{3.103} & {0.961} \\
8 & 2 & 4 & 0.375  & 0.390 &  1.310 & 2.826 & 0.961\\
4 & 2 & 2 & 0.381 & 0.401 & 1.129 & 2.633 & 0.960\\
4 & 1 & 4 & 0.378 & 0.393 & 1.566 & 3.046 & \textbf{0.962}\\
\bottomrule
\end{tabular}
\caption{Hyperparameters ablation using
COCO-Search18 TA set.}
\label{tb:params}
\end{center}
\end{table}


\begin{table}[t]
\begin{center}
\begin{tabular}{c|cc}
\toprule 
& Target-present & Target-absent \\
\midrule
Dense & \textbf{0.470} & \textbf{0.403}\\
Regression &  0.452 & 0.330\\
\bottomrule
\end{tabular}

\caption{Comparison between HAT's dense prediction paradigm and Gazeformer's regression paradigm on COCO-Search18 using HAT's architecture.}
\label{tb:regression}
\end{center}
\end{table}

% \begin{table}[t]
% \begin{center}
% \begin{tabular}{c|cc|cc}
% \toprule 
% \multirow{2}{*}{}
%  & \multicolumn{2}{c|}{Target-present} & \multicolumn{2}{c}{Target-absent} \\
% \cmidrule{2-5}
% & Dense & Regression & Dense & Regression\\
% \midrule 
% % SemSS  & \textbf{0.543}& 0.537 & \textbf{0.418} & 0.393\\
% SS & \textbf{0.470} & 0.452 & \textbf{0.403} & 0.330\\
% \bottomrule
% \end{tabular}

% \caption{HAT's dense prediction paradigm vs. Gazeformer's regression paradigm on COCO-Search18.}
% \label{tb:regression}
% \end{center}
% \end{table}

\begin{figure*}[t]
  \centering
    \subfloat[Target-absent laptop search]{\includegraphics[width=.49\linewidth]{images/TA_vis_ins2.pdf}\label{fig:ta_vis2}}
    \subfloat[Target-absent car search]{\includegraphics[width=.49\linewidth]{images/TA_vis_ins1.pdf}\label{fig:ta_vis1}}
    \caption{Visualization of the {\bf predicted scanpath, peripheral contribution map and fixation heatmap} (columns) of HAT for target-absent (a) laptop and (b) car visual search examples at every fixation (rows). We also include the predicted termination probability $\tau$ for each step on the left. The model terminates searching if $\tau>0.5$.}
    \label{fig:peri_cont}
\end{figure*}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/attn_weights.pdf}
  \caption{{\bf Peripheral tokens vs foveal tokens} under TP, TA and FV settings (from left to right). 
    The top three figures visualize the temporal change of the contribution of peripheral and foveal memory tokens in predicting human attention. Here the contribution is measured by the attention weight from the last cross-attention layer of the aggregation module in HAT. X-axis shows the token index, with 0 representing all peripheral tokens (by summing the attention weights of all peripheral tokens) and $i>0$ being the $i$-th foveal token. Y-axis indicates temporal fixation step from first to max number of fixation steps allowed for each task. The bottom three figures show the spatial distribution of the attention weights of all peripheral tokens, averaged over the temporal axis. The brighter the color, the larger is the contribution.}
  \label{fig:attn_weights}
\end{figure}


% \subsection{Backbone.}
In this section, we provide further ablation on HAT. First we ablate the backbones of HAT. We perform the ablation experiments using the target-absent (TA) visual search fixation prediction task on the TA set of COCO-Search18. By default, HAT uses ResNet-50 \cite{he2016deep} as the pixel encoder and MSD \cite{zhu2020deformable} as the pixel decoder. However, HAT is also compatible with other architectures. Hence, in \cref{tb:feat_ext}, we evaluate HAT with different pixel encoders and decoders. Three pixel encoders: ResNet-50 (R50), ResNet-101 (R101) \cite{he2016deep} and Swin Transformer \cite{liu2021swin} (we use the base model, Swin-B); and two pixel decoders: FPN \cite{lin2017feature} and MSD \cite{zhu2020deformable}, are evaluated. One can observe that MSD is better than FPN as the pixel decoder and HAT performs the best when using R50 and Swin-B as the pixel encoder. Notice that the performance gap between different pixel encoders is small, suggesting that the performance of HAT is robust to the choice of different pixel encoder architectures. 
More importantly, all of these configurations of HAT significantly outperforms all baselines in Tab. 2 of the main text.

In \cref{tb:params}, we also present HAT's results with varied hyperparameters: the number of attention heads in the transformer module of HAT, $\alpha$ and $\beta$ of \eqref{eq:loss_fn}, demonstrating HAT's robustness w.r.t. difference choices of hyperparameters. Notably, the choice of $(4, 2,4)$ in the three ablated hyperparameters achieves the best performance.

\cref{tb:regression} compares HAT's DP task with Gazeformer's Reg task using HAT in TP and TA settings. The proposed DP outperforms Reg in both settings, especially in TA setting. This aligns with our findings in Tab. 1-3 of the main text which show that Gazeformer's Reg paradigm, assuming a Gaussian fixation distribution, is less effective for TA and FV scanpaths.

\section{Additional Qualitative Analysis}

\subsection{Model interpretability}
\myheading{Peripheral contribution map visualization.}
In Sec. 4.2 of the main text, we showed that the peripheral contribution maps in HAT can be leveraged to interpret the model's behaviors using a target-present search example. We also observe a similar pattern in the target-absent (TA) setting (see \cref{fig:peri_cont}). In \cref{fig:ta_vis2}, we see that in a TA laptop search task pixels of {\it table} and {\it keyboard} contribute significantly in predicting human fixations as tables and keyboards can provide spatial cues for the laptop. This reveals a unique factor that guides visual search attention---anchor objects \cite{boettcher2018anchoring}. In \cref{fig:ta_vis1}, we see that TA car search fixations are attracted to truck pixels as trucks and cars are closely related concepts that are considered as distractors.


\myheading{Peripheral vs foveal.}
We also \textit{collectively} analyze the contribution of peripheral tokens and foveal tokens in predicting human attention control under the TP, TA and FV settings, separately. \cref{fig:attn_weights} visualizes the temporal change of contributions of all peripheral tokens collectively and the foveal token in predicting human attention averaged over all test images. We observe that the peripheral tokens contribute the most in predicting TP fixations across all fixations (forming the yellow column on the left). This is because in TP images there is a strong target signal available in the visual periphery to guide attention. Contrast this with FV fixations, where the contribution of the peripheral tokens diminishes over the temporal space and the only the current foveal token has a strong and consistent contribution (a clear red diagonal line). An interpretation of this pattern is that people have only a poor memory of what they viewed in previous fixations and their attention is controlled by salient pixels within a local neighborhood around the current fixation. Interestingly, for TA fixations we also observe a diminishing contribution of the peripheral tokens over the temporal space, but not as pronounced. Moreover, as more fixations are made, the contribution of recent fixations increases, approaching the pattern in FV. This suggests that the later fixations of a TA scanpath behave like a FV scanpath, which confirms a finding in \cite{Chen_2022_CVPR}. Lastly, the bottom row visualizes the contribution of each individual peripheral token (averaged over the temporal axis), where we see peripheral tokens encode a strong center bias for FV fixations, whereas TA fixations show a weaker center bias and TP fixations show no obvious center bias at all, again as expected and confirming previous suggestion. This showcases the potential for HAT to make highly interpretable predictions of human attention control.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.\linewidth]{images/cat_peri.pdf}
  \caption{{\bf Categorical peripheral contribution map of visual search fixations}. We show the contribution map of the peripheral tokens for two categories (rows): car and bottle, in target-present and target-absent settings (columns). We measure the contribution of each peripheral token by the attention weights from the last cross-attention layer of the aggregation module in HAT, averaged over the temporal axis of all testing data in COCO-Search18 \cite{chen2021coco}. The brighter the color, the larger the contribution.}
  \label{fig:cat}
\end{figure}
\begin{figure*}[t]
  \centering
{\includegraphics[width=1.0\linewidth]{images/fail_case.pdf}\label{fig:failcase}}
\caption{\textbf{Failure cases.} The first row is two failure cases for laptop and tv search, respectively. The second row is two failure cases for sink and clock search, respectively. The third row is two failure cases for free viewing. }
    \label{fig:failcase}
\end{figure*}
\myheading{Target prior.}
A natural question arising from this observation is whether the peripheral tokens of TA and TP fixations encode a {\it target prior}--spatial distribution of the possible target location. To answer this question, we visualize the category-wise peripheral contribution maps for TP and TA fixations by averaging the attention weights (on the peripheral tokens) of the last cross-attention layer over all testing fixations for each target category. As shown in \cref{fig:cat}, the category-specific peripheral contribution map does not provide a clear evidence of TA and TP peripheral contribution map being a target prior, but we find some target-specific pattern, e.g., the contribution is pronounced around the bottom horizontal area for ``car" and around the vertical area for ``bottle", which may represent the spatial prior of each category. 

\subsection{Failure cases analysis}

Our analysis of failure cases offers insights for future research. A scanpath prediction would be taken as a failure case if its sequence score falls below 50\% of the human consistency of its stimulus. Under this criterion, we find some common features of failure cases. For target-present, the ambiguity of the target object  often leads to a decline in HAT performance. For instance, in the first row of \cref{fig:failcase}, the laptop in the first case has a very similar color to the table and its surrounding objects. In the second case, the TV is indistinguishable even to an individual. Both scenarios present an ambiguous visual representation of the target, complicating the prediction of the scanpath during visual searches. For target-absent, it is hard for HAT to learn the perception pattern of human when the human scanpaths are very short. In free-viewing, from the visualization in the third row of \cref{fig:failcase}, HAT only allocates a few fixations to text in the image, which is opposite to human perception. This discrepancy is attributed to the limitations of the image encoder and decoder in capturing text features.

\begin{figure*}[t]
  \centering
  %\includegraphics[width=1.0\linewidth]{images/vis_TP.pdf}
  \includegraphics[width=1.0\linewidth]{images/new_vis_tp.pdf}
  \caption{{\bf Target-present scanpath visualization}. We show the scanpaths of seven methods (rows) for four different targets (columns) which are bottle, stop sign, microwave and knife. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL and detector, we visualize the first 6 fixations.}
  \label{fig:sps_tp}
\end{figure*}

\begin{figure*}[t]
  \centering
  %\includegraphics[width=1.0\linewidth]{images/vis_TA.pdf}
  \includegraphics[width=1.0\linewidth]{images/new_vis_ta.pdf}
  \caption{{\bf Target-absent scanpath visualization}. We show the scanpaths of seven methods (rows) for four different targets (columns) which are bottle, stop sign, microwave and knife. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL and detector, we visualize the first 6 fixations.}
  \label{fig:sps_ta}
\end{figure*}

\begin{figure*}[t]
  \centering
  %\includegraphics[width=1.0\linewidth]{images/vis_FV.pdf}
  \includegraphics[width=1.0\linewidth]{images/new_vis_fv.pdf}
  \caption{{\bf Free-viewing scanpath visualization}.  We show the scanpaths of seven methods (rows) for four example images. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL and detector, we visualize the first 15 fixations.}
  \label{fig:sps_fv}
\end{figure*}
\subsection{Scanpath visualization}
We further visualize additional scanpaths for human (ground truth), our HAT, Gazeformer \cite{mondal2023gazeformer}, FFMs \cite{yang2022target}, Chen \etal \cite{chen2021predicting}, IRL \cite{yang2020predicting}, and a heuristic method (target detector for visual search and saliency heuristic for free viewing) in the TP, TA, and FV settings. \cref{fig:sps_tp} shows the TP scanpaths. In all examples, HAT shows superior performance in predicting the human fixation trajectory not only when humans correctly fixate on the target, but also when their attention is distracted by other visually similar objects. For example, in the last column of \cref{fig:sps_tp} when the task is to find a knife, HAT is the only model that correctly predicts the fixation on the metallic object (because knives are usually metallic), whereas other methods either missed the target or did not show any distractions to the metallic object. This shows the capacity of HAT in modeling human attention control in visual search. \cref{fig:sps_ta} shows that HAT learns to leverage the context cues in predicting target-absent fixations, e.g., when the search target is microwave, HAT correctly predicted the fixations on the counter-top and table, where microwaves are often found. Similarly, HAT also generates the most human-like scanpaths in free-viewing task (see \cref{fig:sps_fv}), capturing all important aspects of scanpaths, such as the locations (where), the semantics (what), and the order (when) of the fixations.  


\section{Implementation details}
\subsection{Network structure.}
HAT has four modules as shown in Fig. 2 of the main text. By default, the feature extraction module employs ResNet-50 \cite{he2016deep} as the pixel encoder and MSDeformAttn \cite{zhu2020deformable} as the pixel decoder. \cref{{sec:add_ablation}} presents the results with other pixel encoders, ResNet-101 and Swin Transformer \cite{liu2021swin}, and pixel decoder, FPN \cite{lin2017feature}.
The number of channels of the feature maps $C$ is set to 256. For the foveation module, the transformer encoder has three layers. The transformer decoder in the aggregation module has six layers (i.e,. $L=6$). All transformer encoder and decoder layers in HAT have 4 attention heads. The number of queries $N=18$ for visual search scanpath prediction as COCO-Search18 contains 18 target categories and $N=1$ for free-viewing scanpath prediction. Finally, the MLP in the fixation prediction module has two linear layers with 512 hidden dimensions and a ReLU activation function.

\subsection{Training settings.}
Following \cite{yang2022target,yang2020predicting}, we resize all images to $320{\times} 512$ for computational efficiency during training and inference. We use the AdamW \cite{loshchilov2017decoupled} with the learning rate of $0.0001$ and train HAT for 30 epochs with a batch size of 32. No data augmentation is used during training. Note that we keep the pixel encoder fixed during training and we use the COCO-pretrained weights for panoptic segmentation from \cite{cheng2022masked} as an initialization for the pixel encoder and pixel decoder.
Following \cite{yang2020predicting}, we set the maximum length of each predicted scanpath to 6 and 10 (excluding the initial fixation) for target-present and target-absent search scanpath prediction, respectively. For free viewing, the maximum scanpath length is set to 20.

\subsection{Additional details on heuristic baselines} 
\textbf{Detector}: The detector network consists of a feature pyramid network (FPN) for feature extraction (1024 channels) with a ResNet50 pretrained on ImageNet as the backbone and two convolution layers with batch normalization and a ReLU activation layer in between for detection of 18 different targets. The kernel size and hidden dimension of the first convolutional layer is 3 and 128, respectively. The detector network predicts a 2D spatial probability map of the target from the image input and is trained using the ground-truth location of the target. Another similar baseline is \textbf{Fixation Heuristic}.
This network shares exactly the same network architecture with the detector baseline but it is trained with behavioral fixations in the form of spatial fixation density map, which is generated from 10 subjects on the training images.

\subsection{Scanpath generation}
Most methods except human consistency generate a new spatial priority map or action map at every step, while the predicted priority map is fixed over all steps for the Detector, Fixation Heuristic and IVSN baselines. Prior works like \cite{yang2020predicting} measure model performance based on multiple randomly sampled scanpaths, which, however, can be unfairly bias toward models that repeatedly sample the best (greedy) scanpath. Therefore, in this work we directly compare different methods using their best predictions. When generating scanpaths, for all methods we follow \cite{yang2022target} and predict one scanpath for each testing image in a greedy fashion---a fixation location is determined by selecting the most probable fixation location in the predicted priority map. At evaluation, we compare the predicted ``greedy" scanpath against all GT scanpaths which helps measure how well a model (at its best) captures human scanpath consistency.

\subsection{Implementation of cIG}
cIG denotes the amount of information gain from the predicted fixation map (the model is provided with all previous fixations) over a baseline in predicting the ground-truth fixation. Here, the baseline is a  fixation density map constructed by averaging the smoothed density (with a Gaussian kernel of one degree of visual angle) maps of all training fixations. For target-present and target-absent visual search settings, we use a (target) category-wise fixation density map, following \cite{yang2022target}. For the heuristic models (i.e., target detector and saliency heuristic) which apply the winner-take-all strategy on a static fixation map to generate the scanpath prediction, we use the same static fixation map for all fixations in a scanpath to compute cIG, cNSS and cAUC. To obtain the predicted fixation maps for Chen \etal's model~\cite{chen2021predicting}, we use the ground-truth fixation map (Gaussian smoothed with a kernel size of 2) as input to obtain the predicted action map for the next fixation (i.e., the predicted fixation map). Note that all predicted fixation maps in computing cIG, cNSS and cAUC, are resized to $320\times512$ for fair comparison.

\section{A note on human consistency}
For the same image, there are multiple ground-truth scanpaths from different human subjects. 
As in \cite{yang2020predicting,yang2022target}, for each image human consistency is computed by averaging the similarities of every pair of human scanpaths. A model's performance, however, is measured by the average similarity of the predicted \enquote{mean} scanpath to every human scanpath. 
Consider scanpaths as 2D points whose similarity can be measured by Euclidean distance. The average pairwise similarity between these points (human consistency) is smaller than the average similarity of these points to their arithmetic mean (model performance).
This explains how it is possible for a good model to exceed the human consistency. Taking a triangle as analogy: the average distance of a point (predicted scanpath) within the triangle to all vertices can be smaller than the average edge length (human consistency).

\section{Further discussion on applications}
\label{sec:app} 
Models that predict top-down attention (TP/TA search fixations), modulated by an external goal, have wide applicability to attention-centric HCI. For example, 
faster attention-based rendering that leverages the prediction of a user's attention as they play a VR/AR game and home robots incorporating search-fixation-prediction models will be better at inferring a user's need (i.e., their search target).
Home robots incorporating search-fixation-prediction models will be better able to infer a users' need (i.e., their search target) and autonomous driving systems can attend to image input like an undistracted driving expert.
Applications of FV attention prediction exist in foveated rendering \cite{kaplanyan2019deepfovea} and online video streaming \cite{park2021mosaic}.