\section{Experiments}
\label{sec:experiments}
% In this section, we describe our experiments to study the effectiveness of HAT for scanpath prediction on both visual search and free-viewing tasks. Then, demonstrate the interpretability of HAT's prediction by leveraging the unique design of HAT. Finally, we ablate different components of HAT to understand their roles for modeling human attention control.\\
\textbf{Datasets.}
We train and evaluate HAT using four datasets: COCO-Search18 \cite{chen2021coco}, COCO-FreeView \cite{Chen_2022_CVPR}, MIT1003 \cite{judd2009learning} and OSIE \cite{xu2014predicting}. COCO-Search18 is a large-scale visual search dataset containing human scanpaths in searching for 18 different object target and it has two parts: target-present and target-absent. In total, there are 3101 target-present images and 3101 target-absent images in COCO-Search18, each viewed by 10 subjects. Following \cite{yang2022target}, we treat the target-present part and target-absent part of COCO-Search18 as two separate datasets and train models on them independently.
COCO-FreeView is a ``sibling" dataset of COCO-Search18 but with free-viewing scanpaths. COCO-FreeView contains the same images with COCO-Search18, each viewed by 10 subjects in a free-viewing setting. 
MIT1003 is a widely-used free-viewing dataset containing 1003 natural images. OSIE is a free-viewing gaze dataset with rich semantic-level annotations, containing 700 natural indoor and outdoor images. Each image in MIT1003 and OSIE is viewed by 15 subjects.\\
\textbf{Evaluation metrics.}
To measure the performance, we mainly analyze the scanpath prediction models from two aspects: 1) how similar the predicted scanpaths are to the  human scanpaths; and 2) how accurate a model predicts the next fixation {\it given all previous fixations}. 
To measure the scanpath similarity, we use a commonly adopted metric, {sequence score (SS)} \cite{borji2013analysis} and its variant {semantic sequence score (SemSS)} \cite{yang2022target}. SS transforms the scanpaths into sequences of fixation cluster IDs and then compares them using a string matching algorithm \cite{needleman1970general}. Different from SS, SemSS transforms a scanpath into a string of semantic labels of the fixated pixels. 
% Following \cite{yang2022target}, we also report the SS of scanpaths truncated at 4 new fixations, denoted as {\bf SS(4)}.
For next fixation prediction, we follow \cite{kummerer2021state,yang2022target,kummerer2022deepgaze} and report the conditional saliency metrics, {cIG, cNSS and cAUC}, which measure how well a predicted fixation probability map of a model predicts the ground-truth (next) fixation when the model is provided with the fixation history of the scanpath in consideration, using the widely used saliency metrics, IG, NSS and AUC \cite{bylinskii2018different}. For fair comparison, we follow \cite{yang2022target} and predict one scanpath for each testing image, step by step selecting the most probable fixation location as the next fixation.\\
\textbf{Baselines.}
We first compare our model against several heuristic baselines. Following prior works \cite{yang2020predicting,zelinsky2019benchmarking,yang2022target,chen2021predicting,kummerer2022deepgaze}, the {human consistency}, an oracle where we use one viewer's scanpath to predict the scanpath of another, is reported as a gold-standard model. Second, we compare to a {fixation heuristic} method---a ConvNet trained to predict human fixation density maps, from which we select fixations sequentially with inhibition of return. For visual search scanpaths, we further include a {detector} baseline, which is similar to the fixation heuristic, but trained on target-present images of COCO-Search18 to predict a target detection probability map. For both fixation heuristic and detector baselines, we use the winner-take-all strategy to generate scanpaths.
Furthermore, we compare HAT to the previous state-of-the-art models of scanpath prediction: {IVSN} \cite{zhang2018finding}, PathGAN \cite{assens2018pathgan}, {IRL} \cite{yang2020predicting}, {Chen \etal} \cite{chen2021predicting}, DeepGaze III \cite{kummerer2022deepgaze}, {FFMs} \cite{yang2022target} and GazeFormer \cite{mondal2023gazeformer}.
Note that IVSN only applies for visual search tasks, and unlike other methods, IVSN is designed for zero-shot search scanpath prediction, hence is not trained with any gaze data. DeepGaze III only applies for free-viewing scanpaths and is trained with the SALICON dataset \cite{jiang2015salicon} and MIT1003 \cite{judd2009learning}.\\
\textbf{Implementation details.}
We use ResNet-50 \cite{he2016deep} as the pixel encoder and MSDeformAttn \cite{zhu2020deformable} as the pixel decoder. For the foveation module, the transformer encoder has three layers. The transformer decoder in the aggregation module has six layers (i.e,. $L=6$). All transformer encoder and decoder layers in HAT have 4 attention heads. The MLP in the fixation prediction module has two linear layers with 512 hidden dimensions and a ReLU activation function.
We use the AdamW \cite{loshchilov2017decoupled} with the learning rate of $0.0001$ and train HAT for 30 epochs with a batch size of 128. All images are resized to $320{\times} 512$ for computational efficiency during training and inference. Following \cite{yang2020predicting}, we set the maximum length of each predicted scanpath to 6 and 10 (excluding the initial fixation) for target-present and target-absent search scanpath prediction, respectively. For free viewing, the maximum scanpath length is set to 20.
For more implementation details, please refer to the supplement.

\setlength{\tabcolsep}{2.5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
 &  SemSS  & SS & cIG  & cNSS & cAUC\\ 
\midrule
Human consistency & 0.500 & 0.500 & - & - & -\\\midrule
Detector & 0.523 & 0.449 & 0.182 & 2.346 & 0.905\\
Fixation heuristic & 0.506 & 0.437&1.107&	2.186 &  0.917\\
IVSN \cite{zhang2018finding} & 0.368 & 0.326 & -0.192 & 1.318 & 0.901\\
PathGAN \cite{assens2018pathgan} & 0.280 & 0.239 & - & - & -\\
IRL \cite{yang2020predicting} & 0.486 & 0.422 & -9.709 & 1.977 & 0.913\\
Chen~\etal \cite{chen2021predicting} & 0.518 & 0.445 & -1.273 &2.606 & 0.956\\
FFMs \cite{yang2022target} & 0.500 & {0.451} & 1.548 & 2.376 & 0.932 \\
Gazeformer \cite{mondal2023gazeformer} & 0.499 & \bf{0.489} & - & - & -\\
\textbf{HAT} (ours) & \bf{0.543} & 0.470 & \bf{2.399} & \bf{5.086} & \bf{0.977}\\
\bottomrule
\end{tabular}
\caption{{\bf Target-present search scanpath prediction comparison} on the {target-present} test set of COCO-Search18. We highlight the best results in bold.}
\label{tb:tp_rst}
\end{center}
\vspace{-0.35cm}
\end{table}
\setlength{\tabcolsep}{2.5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|ccccc}
\toprule
 &  SemSS  & SS & cIG  & cNSS & cAUC\\ 
\midrule
Human consistency & 0.372 & 0.381 & - & - & -\\\midrule
Detector & 0.332 & 0.321 & -0.516 & 0.446 & 0.783\\
Fixation heuristic & 0.309 & 0.298	&	-0.599&	0.405 & 0.798\\
IVSN \cite{zhang2018finding} & 0.279 & 0.260 & -0.219 & 0.884 & 0.867\\
PathGAN \cite{assens2018pathgan} & 0.315 & 0.250 & - & - & -\\
IRL \cite{yang2020predicting} & 0.329 & 0.319 & 0.032 & 1.202& 0.893\\
Chen~\etal \cite{chen2021predicting} & 0.340 & 0.331 & -3.278 & 1.600& 0.925\\
FFMs \cite{yang2022target} & 0.376 & 0.372 & 0.729 & 1.524 & 0.916 \\
Gazeformer \cite{mondal2023gazeformer} & 0.374 & 0.357 & - & - & -\\
\textbf{HAT} (ours) & \bf{0.382} & \bf{0.402} & \bf{1.686} & \bf{3.103} & \bf{0.961}\\
% \hline
% {Joint HAT (ours)} \\
\bottomrule
\end{tabular}
\caption{{\bf Target-absent search scanpath prediction comparison} on the {target-absent} test set of COCO-Search18. We highlight the best results in bold.}
\label{tb:ta_rst}
\end{center}
\vspace{-0.35cm}
\end{table}
\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccccccc}
\toprule
& SS & cIG  & cNSS & cAUC\\
\midrule
Human consistency & 0.349  & - & - & -\\\midrule
Fixation heuristic & 0.329 & 0.319 & 1.621 & 0.930\\
PathGAN \cite{assens2018pathgan} & 0.181 & - & - & -\\
IRL \cite{yang2020predicting} & 0.300 & -0.213 & 1.018 & 0.888\\
Chen \etal \cite{chen2021predicting} & {0.365} & -1.263 & 1.655 & 0.922\\
DeepGaze III \cite{kummerer2022deepgaze} & 0.339 & 0.140 & 1.418 & 0.910\\
FFMs \cite{yang2022target} &	0.329&	0.329&	1.432 & 0.918\\
% {HAT w/o pretrain} & 0.359 & {1.171}&	{2.841} & {0.947}\\ 
Gazeformer \cite{mondal2023gazeformer} & 0.280 & - & - & - \\
HAT & {\bf 0.369} & {\bf 1.485} & {\bf 3.382} & {\bf 0.965}\\
\bottomrule
\end{tabular}
\caption{{\bf Comparing free-viewing scanpath prediction algorithms} (rows) using multiple metrics (columns) on the test set of COCO-FreeView. The best results are highlighted in bold.}
\label{tb:fv_rst}
\end{center}
\vspace{-0.35cm}
\end{table}

\subsection{Main Results}
\textbf{Target-present search.}
We compare HAT with previous scanpath prediction models under the target-present (TP) setting using the target-present part of the COCO-Search18 dataset in \cref{tb:tp_rst}. HAT consistently outperforms all other predictive methods in predicting TP human scanpaths in nearly all metrics. The simple heuristic baselines (i.e., detector and fixation heuristic) perform quite well on TP scanpath prediction by predicting the location of the target or fixation density map as in 60\% of the TP trials of COCO-Search18 humans can locate the target within 2 fixations. However, they have low scores on saliency metrics (i.e., cIG, cNSS and cAUC) as they ignore the inter-dependencies between fixations. Compared to FFMs \cite{yang2022target} and Chen \etal \cite{chen2021predicting} which have high saliency scores, HAT further improves the performance significantly for all metrics. Particularly, HAT is better than Chen \etal \cite{chen2021predicting} (the second best) in cNSS by 95\%. HAT slightly lags behind the most recent GazeFormer \cite{mondal2023gazeformer} in SS but is significantly better in semSS. We also demonstrate in the supplement that HAT learns the entire scanpath distribution from multiple subjects whereas GazeFormer overfits to the \enquote{average person} and fails to predict scanpaths from different subjects. 
Moreover, HAT surpasses the human consistency in semSS, suggesting that HAT well captures the semantics behind fixations.\\
\textbf{Target-absent search.}
For target-absent (TA) search scanpath prediction, we compare HAT to different approaches on the TA test set of COCO-Search18 in \cref{tb:ta_rst}. Different from TP search results shown in \cref{tb:tp_rst}, we see in \cref{tb:ta_rst} that the gap between heuristic methods to human consistency is much larger for TA search, demonstrating that TA search scanpath prediction is a more challenging task than TP scanpath prediction. Indeed, the predominant influence on human attention in TP search (i.e., the target) is now absent \cite{Chen_2022_CVPR}, making other factors such as the spatial cues provided by the anchor objects \cite{boettcher2018anchoring}, the contextual cues from global scene understanding \cite{torralba2006contextual} and object co-occurrence \cite{mack2011object} stand out. The discernment of these factors necessitates a robust semantic understanding of the input image. \cref{tb:ta_rst} shows that HAT sets a new state-of-the-art at \textbf{all} metrics, outperforming the previous state-of-the-art (Chen \etal \cite{chen2021predicting}) by 94\% in cNSS. More importantly, HAT achieves a sequence score surpassing human consistency for the first time. These results suggest that comparing to other methods HAT better captures the semantics of the image and learns the relation between other objects and targets.
\\
\textbf{Free-viewing.}
In addition to visual search, HAT can predict free-viewing scanpaths by treating free-viewing as a standalone task. In \cref{tb:fv_rst}, we compare HAT with the baselines using COCO-FreeView. Note that Detector and IVSN are excluded here as the free-viewing fixations are not tasked to searching for a target like visual search. HAT outperforms all other methods in cIG, cNSS and cAUC, especially HAT is 351\% and 104\% better than the second best (FFMs and Chen \etal \cite{chen2021predicting}) in cIG, cNSS, respectively. This reaffirms the effectiveness of HAT as a generic framework for scanpath prediction. We further validated the effectiveness of our proposed HAT using OSIE \cite{xu2014predicting} and MIT1003 \cite{judd2009learning}, and the  generalizability of HAT to new scenes, please refer to the supplement for detailed results.


% \setlength{\tabcolsep}{6pt}
% \begin{table}[t]
% \caption{{\bf Comparing different choices of Transformer decoder layers $L$} in HAT. The ablation experiments are done on the target-absent set of COCO-Search18. The best results are highlighted in bold.}
% \label{tb:trans_dec}
% \begin{center}
% \begin{tabular}{c|ccccc}
% \toprule
%  $L$ & SemSS  & SS & cIG & cNSS & cAUC\\ 
% \midrule
% 1 & & & 0.& \\
% 3 & \\
% 6 & \\
% 9 & \\

% \bottomrule
% \end{tabular}

% \end{center}
% \end{table}



\subsection{Qualitative Analysis}\label{sec:qual}

% \begin{figure*}[t]
%   \centering
%     \subfloat[Target-present search scanpaths for ``bottle" and ``knife".]{\includegraphics[width=\linewidth]{images/vis_line_tp.pdf}\label{fig:vis_tp}}\\
%     \subfloat[Target-absent search scanpaths for ``bottle" and ``stop sign".]{\includegraphics[width=\linewidth]{images/vis_line_ta.pdf}\label{fig:vis_ta}}\\
%     \hfill
%     \subfloat[Free viewing scanpaths.]{\includegraphics[width=0.95\linewidth]{images/vis_line_fv.pdf}\label{fig:vis_fv}}
%     \caption{{\bf Visualization of the ground-truth human scanpaths and predicted scanpaths of different methods (columns).} Three different settings (rows) including (a) target-present search for bottle and knife, (b) target-absent search for bottle and stop sign and (c) free viewing are shown from the top to bottom. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL, detector and fixation heuristic, we visualize the first 6 fixations for visual search and 15 for free viewing. The rightmost column shows the predicted scanpaths of the heuristic methods (detector for visual search and fixation heuristic for free-viewing).}
%   \label{fig:vis}
% \end{figure*}
\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/vis3.pdf}
  \caption{\textbf{Visualization of the ground-truth human scanpaths
and predicted scanpaths of different methods (columns).} Three different settings (rows) including target-present bottle search, target-absent stop sign search and free viewing are shown from the top to bottom. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL, detector and fixation heuristic, we visualize the first 6 fixations for visual search and 15 for free viewing. The rightmost column shows the predicted scanpaths of the heuristic methods (detector 630 for visual search and fixation heuristic for free-viewing)}
  \label{fig:vis_small}
\end{figure}
\textbf{Scanpath visualization.}
In this section, we qualitatively compare the predicted scanpaths of different methods to each other and to the ground-truth human scanpaths in the TP, TA and FV settings. As shown in \cref{fig:vis_small}, when searching for bottles in the TP setting, HAT not only correctly predicted the terminal fixation on the heavily-occluded target, but also predicted fixations on all the distractor objects that look similar to the target, like humans do. Other methods either missed the distractor objects or failed to find the target. Similarly, for the TA stop sign search, HAT was the only one that looked at both sides of the road in searching for a stop sign like the human subject would, showing a use of semantic and context cues to control attention. In the FV setting, HAT also predicted the most human-alike scanpaths among all methods in (1) the fixation locations (where), (2) the semantics (what), and (3) the order (when) of the fixations. More scanpath visualizations can be found in the supplement.\\
\begin{figure}[t]
  \centering
{\includegraphics[width=1.0\linewidth]{images/TP_vis_ins2.pdf}\label{fig:tp_peri_cont}}
\caption{Visualization of the {\bf predicted scanpath, peripheral contribution map and fixation heatmap} (columns) of HAT for target-present laptop visual search examples at every fixation (rows). We also include the predicted termination probability $\tau$ for each step on the left. The model terminates searching if $\tau>0.5$.}
\label{fig:peri_cont}
\vspace{-0.35cm}
\end{figure}
\\
\textbf{Model interpretability.}
A distinctive attribute of HAT lies in its interpretability, facilitated by the computational attention mechanism and the foveation module design. HAT enables quantitatively measuring the contribution of both peripheral and foveal tokens to fixation allocation. The contribution of a token is computed as the attention
weight from the last cross-attention layer of the aggregation module in HAT. By computing the normalized contribution of each peripheral token, we create a \textit{peripheral contribution map}, which offers insights for the human gaze behavior.
We further analyze how the peripheral contribution map evolves across a sequence of fixations. \cref{fig:peri_cont} shows the predicted scanpath, peripheral contribution maps and predicted fixation heatmaps of HAT in a TP laptop search task. We observe that the encoded periphery features not only align with the location of the next fixation (e.g., when the occluded laptop is encoded in the left-bottom periphery, the model makes a fixation to the target and terminates the search), but also provides the {\it contextual cues} where a target might be located (e.g., near the keyboard and the monitor where a laptop is usually found). We also observe a similar pattern for the TA setting (see illustration in the supplement). In the supplement, we also collectively analyze the contribution of peripheral and foveal tokens in predicting human attention control, which has shown that the peripheral vision plays different roles under different settings. These all have demonstrated that HAT can make highly \textit{interpretable} predictions.\\

\subsection{Ablation studies}\label{sec:ablation}
We ablate HAT under the TA setting as TA search fixations exhibit characteristics of both target-present search fixations and free-viewing fixations \cite{Chen_2022_CVPR}.\\
\textbf{Peripheral and foveal tokens.}
We verify the effectiveness of peripheral tokens and foveal tokens by ablating them one at a time. It is shown in \cref{tb:ablation} that ablating any one of them incur a performance drop over all metrics. This suggests that all of these components contribute to the superior performance of HAT. In comparison, removing foveal tokens incurs a larger performance drop (cIG decreases by 30\%). This decline is expected as foveal tokens embody the knowledge accumulated from prior fixations. Without them, HAT can be regarded as a static fixation density map predictor, akin to the fixation heuristic baseline. Conversely, the removal of  peripheral tokens has a relatively minor effect, possibly attributed to the adaptive capacity of foveal tokens ($P_2$) compensating for information loss in peripheral tokens ($P_1$) during training.\\
\textbf{Output resolution.}
HAT has a default output resolution of $80\times128$ due to the convolution with the high-resolution feature map $P_4$ (see \cref{fig:overview}). In \cref{tb:ablation} (last row), we change the convolution operant from $P_4$ to $P_2$ to yield an output resolution of $20\times 32$, same as FFMs \cite{yang2022target} and IRL \cite{yang2020predicting} but smaller than Chen~\etal \cite{chen2021predicting} ($30\times40$). 
Despite that a reduced resolution incurs a noticeable performance drop in HAT, HAT still outperforms prior state-of-the-art FFMs with the same output resolution and \citet{chen2021predicting} using a higher output resolution. This underscores HAT's effectiveness and design flexibility. Additional ablations can be found in the supplement.

\setlength{\tabcolsep}{2.5pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|ccccc}
\toprule 
 &  SemSS  & SS & cIG & cNSS & cAUC\\ 
\midrule
baseline ($\htimesw{80}{128}$) & \bf{0.382} & \bf{0.402} & \bf{1.686} & \bf{3.103} & \bf{0.961}\\
\hline
-- peripheral tokens & 0.375 & 0.396 & 1.600 & 3.003 & 0.960\\
-- foveal tokens & 0.358 & 0.385 & 1.179 & 2.380 & 0.948\\
\hline
low-res ($\htimesw{20}{32}$) & 0.374 & 0.389 & 1.534 & 2.760 & 0.955\\
\bottomrule
\end{tabular}
\caption{{\bf Ablation study} of HAT. These experiments are done on the TA set of COCO-Search18. The best results are in bold.}
\label{tb:ablation}
\end{center}
\vspace{-0.35cm}
\end{table}

% \setlength{\tabcolsep}{2.5pt}
% \begin{table}[t]
% \begin{center}
% \begin{tabular}{l|ccccc}
% \toprule 
%  &  SemSS  & SS & cIG & cNSS & cAUC\\ 
% \midrule
% baseline & \bf{0.382} & \bf{0.402} & \bf{1.686} & \bf{3.103} & \bf{0.961}\\
% \midrule
% -- peripheral tokens & 0.411 & 0.396 & 1.600 & 3.003 & 0.960\\
% -- foveal tokens & 0.394 & 0.385 & 1.179 & 2.380 & 0.948\\
% \bottomrule
% \end{tabular}
% \caption{{\bf Ablation study} of HAT. These experiments are done on the TA set of COCO-Search18. The best results are in bold.}
% \label{tb:ablation}
% \end{center}
% \vspace{-0.35cm}
% \end{table}
% \setlength{\tabcolsep}{4pt}
% \begin{table}[t]
% \begin{center}
% \begin{tabular}{c|ccccc}
% \toprule 
%  Res. &  SemSS  & SS & cIG & cNSS & cAUC\\ 
% \midrule
% {$\htimesw{80}{128}$} & \bf{0.382} & \bf{0.402} & \bf{1.686} & \bf{3.103} & \bf{0.961}\\
% % \midrule
% $\htimesw{20}{32}$ & 0.410 & 0.389 & 1.534 & 2.760 & 0.955\\
% % FFMs & $\htimesw{20}{32}$ & 0.376 & 0.372 & 0.729 & 1.524 & 0.916 \\
% % Chen~\etal & $\htimesw{30}{40}$ & 0.340 & 0.331 & -3.278 & 1.600& 0.925\\
% \bottomrule
% \end{tabular}
% \caption{{\bf Comparing different output resolutions} of HAT using the COCO-Search18 TA set. The best results are in bold.}
% \label{tb:resolution}
% \end{center}
% \vspace{-0.35cm}
% \end{table}