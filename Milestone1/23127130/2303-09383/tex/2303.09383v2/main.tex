\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{makecell}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\newcommand{\myheading}[1]{\vspace{1ex}\noindent \textbf{#1}}
\newcommand{\htimesw}[2]{\mbox{$#1$${\times}$$#2$}}
\newcommand{\zhibo}[1]{{\color{red}\textbf{[Zhibo: #1]}}}
\newcommand{\yg}[1]{{\color{red}\textbf{[Suggested Edit: #1]}}}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{5509} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}
\input{definitions}
%%%%%%%%% TITLE
\title{Predicting Human Attention using Computational Attention}

\author{Zhibo Yang\and
Sounak Mondal\and
Seoyoung Ahn\and
Gregory Zelinsky\and 
Minh Hoai\and
Dimitris Samaras\\
Stony Brook University
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
% The accurate prediction of human attention is crucial for enhancing human-computer interaction systems to anticipate the needs and intentions of users. However, attention control involves both bottom-up and top-down mechanisms, which have been traditionally studied separately in free-viewing and visual search literature. To address this, we propose HAT, the Human Attention Transformer, a new state-of-the-art model that can predict both bottom-up and top-down attention control. HAT outperforms previous models in predicting the scanpath of fixations during both target-present and target-absent search, and achieves comparable or superior performance in predicting scanpaths during taskless free-viewing. This success is attributed to the innovative transformer-based architecture and simplified foveated retina used in HAT, which collectively create a spatiotemporal-aware, dynamic visual working memory. Unlike previous methods that use a grid prediction approach, which can result in information loss about fixations, HAT predicts scanpaths in a dense prediction manner. Our proposed model sets a new standard in computational attention, which emphasizes both effectiveness and generality. HAT's demonstrated scope and applicability will likely inspire the development of new attention models that can better predict human behavior in various attention-demanding scenarios.

%The prediction of human attention will enable HCI systems to better anticipate a person's needs and intents. 
%Attention control takes bottom-up and top-down forms that are typically studied in separate free-viewing and visual search literature, respectively. Here we propose HAT, a Human Attention Transformer, that can predict both bottom-up and top-down attention control. 
%We present Human Attention Transformer (HAT), a model capable of predicting both bottom-up and top-down attention control, the two forms of human attention traditionally studied in separate free-viewing and visual search literatures. 
Most models of visual attention are aimed at predicting \textit{either} top-down or bottom-up control, as studied using different visual search and free-viewing tasks. We propose Human Attention Transformer (HAT), a single model predicting \textit{both} forms of attention control. HAT is the new state-of-the-art (SOTA) in predicting the scanpath of fixations made during target-present \textit{and} target-absent search, and matches or exceeds SOTA in the prediction of \enquote{taskless} free-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel transformer-based architecture and a simplified foveated retina that collectively create a spatio-temporal awareness akin to the dynamic visual working memory of humans. Unlike previous methods that rely on a coarse grid of fixation cells and experience information loss due to fixation discretization, HAT features a dense-prediction architecture and outputs a dense heatmap for each fixation, thus avoiding discretizing fixations. HAT sets a new standard in computational attention, which emphasizes both effectiveness and generality. HAT's demonstrated scope and applicability will likely inspire the development of new attention models that can better predict human behavior in various attention-demanding scenarios.

%We present Human Attention Transformer (HAT), a model capable of predicting both bottom-up and top-down attention control, two forms of attention control traditionally studied in separate free-viewing and visual search literature. 
%Attention control takes bottom-up and top-down forms that are typically studied in separate free-viewing and visual search literatures, respectively. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
The prediction of human attention is crucial for Human-Computer Interaction (HCI) systems to anticipate a person's needs and intents, but human attention is not a singular thing and its control can take at least two broad forms. One is bottom-up, meaning that attention saliency signals are computed from the visual input and used to prioritize shifts of attention. The same visual input should therefore lead to the same shifts of bottom-up attention. The second type of attention is top-down, meaning that a task or goal is used to control attention. Given a kitchen scene, very different fixations are observed depending on whether a person is searching for a clock or a microwave oven \cite{zelinsky2021predicting}. These two types of attention control spawned two separate literatures on gaze fixation prediction (the accepted measure of attention), one where studies use a free-viewing task to study questions of bottom-up attention and the other using a goal-directed task (typically, visual search) to study top-down attention control.
Consequently, most models have been designed to address {\it either} bottom-up or top-down attention, not both. {\it Can a single model architecture predict both bottom-up and top-down attention control?}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/teaser2.pdf}
  \caption{Given an image, the proposed {\bf HAT} is able to predict scanpaths under three settings target-present search for TV; target-absent scanpath for sink; and free viewing. Importantly, HAT outperforms previous state-of-the-art scanpath prediction methods on multiple datasets across three settings: target-present, target-absent visual search and free viewing, that were studied separately.}
  \label{fig:teaser}
\end{figure}
Our answer to this question is HAT, a Human Attention Transformer that generally predicts scanpaths of fixations, meaning that it can be applied to both top-down visual search and bottom-up free viewing tasks (\Fref{fig:teaser}). %To study this question, we propose Human Attention Transformer (HAT) that unifies these bottom-up and top-down forms of attention control by predicting the scanpath (a sequence of fixations) made during both visual search and free viewing. 
Critical to HAT's effectiveness and scope is 
a novel transformer-based design and a simplified foveated retina that collectively work to create a form of {\it dynamically-updating visual working memory}.
Previous methods either use a recurrent neural network (RNN) to maintain a dynamically updated hidden vector that conveys information across fixations \cite{chen2021predicting,sun2019visual,zelinsky2019benchmarking,assens2018pathgan}, or simulate a foveated retina by combining multi-resolution information at the pixel level \cite{zelinsky2019benchmarking}, feature level \cite{yang2022target}, or semantic level \cite{yang2020predicting}. These methods, however, have drawbacks: RNNs lack interpretability and multi-resolution methods of simulating a foveated retina  \cite{zelinsky2019benchmarking,yang2020predicting,yang2022target} fail to capture critical temporal and spatial information useful for scanpath prediction.
To address these problems, a computational attention mechanism \cite{vaswani2017attention} is used to dynamically integrate the spatial, temporal and visual information acquired at each fixation into working memory~\cite{oberauer2019working, olivers2020attention}, enabling HAT to learn a set of task-specific attention weights for aggregating information from working memory and predicting human attention control. This mechanism hints at the relationship between human attention and working memory~\cite{desimone1995neural, gazzaley2012top}, making HAT cognitively plausible and its predictions interpretable. 



In addition, previous work \cite{yang2020predicting,yang2022target,chen2021predicting} addressed the problem of scanpath prediction using (inverse) reinforcement learning, where a {coarse} grid is often used to discretize the fixations into categorical actions due to the limited amount of gaze data. However, this discretization of fixations leads to reduced accuracy of the fixation prediction, thereby minimizing the usefulness of the method in applications where high-resolution imagery is available as input. A seemingly simple remedy to this problem would be to increase the resolution of the discretization grid. However, this would quadratically increase the dimension of the action space, making it challenging to train the networks due to the need for much larger amounts of training data to covering a much larger action space. HAT, on the other hand, treats the scanpath prediction problem as a sequence of dense prediction tasks with per-pixel supervision, thus avoiding the discretization problem.

% Humans have a foveated retina, meaning that we receive high-resolution input from our central vision (where we are currently fixated) and progressively lower resolution inputs with increasing peripheral eccentricity. By actively shifting our high-resolution central vision, we accumulate with each fixation the information that we need to perform a task (e.g., visual search) or explore a scene (free viewing). Two classes of methods exist for representing this dynamic fixation-by-fixation acquisition of information by human attention. One class uses a recurrent neural network (RNN) to maintain a dynamically updated hidden vector that conveys information across fixations \cite{chen2021predicting,sun2019visual,zelinsky2019benchmarking,assens2018pathgan}. The other class simulates a foveated retina by combining multi-resolution information at the pixel level \cite{zelinsky2019benchmarking}, feature level \cite{yang2022target} or semantic level \cite{yang2020predicting}. Both classes of methods, however, have drawbacks: RNNs lack interpretability and multi-resolution methods of simulating a foveated retina  \cite{zelinsky2019benchmarking,yang2020predicting,yang2022target} fail to capture critical temporal and spatial information useful for scanpath prediction.

% To address these problems, HAT uses a novel transformer-based design and a simplified foveated retina that collectively work to create a form of {\it dynamically-updating visual working memory}. A computational attention mechanism \cite{vaswani2017attention} integrates the spatial and temporal visual information acquired at each fixation into working memory~\cite{oberauer2019working, olivers2020attention}, enabling HAT to learn a set of task-specific attention weights for aggregating information from working memory and predicting human attention control. This mechanism hints at the relationship between human attention and working memory~\cite{desimone1995neural, gazzaley2012top}, making HAT cognitively plausible and its predictions interpretable. 

To demonstrate HAT's generality, we predict scanpaths % evaluate HAT's scanpath predictions 
under three settings, target-present (TP) search, target-absent (TA) search, and free-viewing (FV), covering both top-down and bottom-up attention. %, and in two datasets (COCO-Search18 \cite{chen2021coco} and COCO-FreeView \cite{Chen_2022_CVPR}). %Previous scanpath prediction models focused on visual search
In the previous work predicting search scanpaths \cite{yang2020predicting,yang2022target,chen2021predicting}, separate models were trained for the TP and TA settings. HAT is a single model establishing new SOTAs in both TP and TA search-scanpath prediction. When trained with FV scanpaths, HAT also achieves top performance relative to baselines. HAT advances SOTA in cNSS by 83\%, 58\% and 72\% under the TP, TA and FV settings on the COCO-Search18 dataset \cite{chen2021coco} and the COCO-FreeView dataset \cite{Chen_2022_CVPR}, respectively.
% On these benchmarks, HAT performs similarly or better than the more specialized SOTA architectures.

Our contributions can be summarized as follows:
\begin{enumerate}\denselist
    \item We propose HAT, a novel transformer architecture integrating visual information at two different eccentricities (approximating a foveated retina) to predict the spatial and temporal allocation of human attention (the fixation scanpath).
    
    \item We show that our HAT architecture can be broadly applied to different attention control tasks, as demonstrated by SOTA scanpath predictions in the TP and TA and FV settings. We also demonstrate that HAT's predictions of human attention are highly interpretable.
    
    % resolution of the output
    \item We remove the need for fixation discretization and formulate scanpath prediction as a sequential dense prediction task, making HAT applicable to high-resolution input.
    % Merge to second bullet
    % \item We demonstrate that HAT's predictions of human attention are highly interpretable.%the potential for HAT to make highly interpretable predictions of human attention control.
\end{enumerate}
% {\bf (1)} We propose HAT, a novel transformer architecture  integrating visual information at two different eccentricities (approximating a foveated retina) to predict spatial and temporal allocation of human attention, the fixation scanpath;
% {\bf (2)} We show that our single HAT architecture can be broadly applied to both bottom-up and top-down forms of attention control, as demonstrated by SOTA scanpath predictions in TP and TA search and FV settings; and
% {\bf (3)} We propose formulating scanpath prediction as a dense prediction task, so that HAT is applied to high-resolution input and remove the need for fixation discretization.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/model_overview.pdf}
  \caption{{\bf HAT overview.} We use encoder-decoder CNNs to extract two sets of feature maps $P_1$ and $P_4$ of different spatial resolutions. A working memory is constructed by combining all feature vectors from $P_1$ with the feature vectors of $P_4$ at previously fixated locations, representing information extracted from the periphery and central fovea. A transformer encoder is used to dynamically update the working memory at every new fixation. Then, HAT produces $N$ per-task queries $Q$ (e.g., clock search and mouse search), with each learning to aggregates task-specific information from the shared working memory for predicting the fixations for its own task. Finally, the updated queries are convolved with $P_4$ to yield the fixation heatmaps after a MLP layer, and projected to the termination probabilities in parallel. Note, although this example uses visual search, the framework also works for free-viewing scanpaths with $N=1$.}
  \label{fig:overview}
\end{figure*}

\section{Related Work}
Predicting and understanding human gaze control has been a topic of interest for decades in psychology~\cite{yarbus1967eye, findlay2001visual, zelinsky2008theory}, but it has only recently attracted the researcher's attention in computer vision \cite{akbas2017object,jonnalagadda2021foveater} In particular, Itti's seminal work \cite{itti2000saliency} on the saliency model has triggered a lot of interest on human attention modeling in computer vision community and facilitated many other studies identifying and modeling the salient visual features of an image to predict natural human eye-movement behavior ~\cite{borji2013state,kummerer2017understanding, kruthiventi2017deepfix,huang2015salicon,kummerer2014deep, cornia2018predicting,jiang2015salicon,jetley2016end, borji2015salient, masciocchi2009everyone, berg2009free}. However, the scope of these work is often narrowly focused on predicting human natural eye-movements without a specific visual task (i.e., free-viewing), ignoring another important form of attention control, such as goal-directed attention. In addition, existing saliency models (for which training code is available, if applicable, e.g., ~\cite{itti2000saliency,walther2006modeling}) only model the spatial distribution of fixations and do not predict the temporal order between fixations (i.e., scanpath). Scanpath prediction is more challenging problem because it requires predicting not only \textit{where} a fixation will be, but also \textit{when} it will be there. % could affect a person's allocation of attention \cite{wolfe2017five,borji2013state} at every fixation. 

To tackle these limitations, Chen \etal~\cite{chen2021coco} created COCO-Search18, a large-scale goal-directed gaze dataset. In that work, eye movements were collected from the participants who were asked to search for a target object from the visual input where a target might or might not exist (target present and target absent conditions, respectively). This paradigm, called categorical visual search, is extensively studied in psychology to understand human goal-directed attention control~\cite{wolfe1998visual}. Early visual search datasets either have multiple targets \cite{gilani2015pet} in an image or only contain one or two target categories~\cite{ehinger2009modelling,zelinsky2019benchmarking}. COCO-Search18 extended it to 18 targets creating a large enough eye-movement dataset that enables training deep-learning models to predict goal-directed human scanpaths. In \cite{yang2020predicting}, an inverse reinforcement learning (IRL) model proposed which showed superior performance on COCO-Search18 in predicting target-present scanpaths.
Most recently, the same research group \cite{yang2022target} proposed a more generalized scanpath prediction model using foveated feature maps that can be applied to target-absent as well. However, their generalizability to free-viewing prediction has never been interrogated.

Chen~\etal \cite{chen2021predicting} showed that a reinforcement learning model directly optimized on the scanpath similarity metric can predict scanpaths on VQA task, as well as on free-viewing and target-present visual search tasks. Similar to their work, we design a generic scanpath model that generalizes to free-viewing and visual search tasks (both target-present and target-absent). Contrary to the previous approach (mostly CNN-based), we leverage the power of the Transformer architecture~\cite{vaswani2017attention} and a dynamically-updating working memory, which collectively helps to learn a complex spatio-temporal fixation representations that can be applied to various visual tasks. We also approach scanpath prediction as a dense prediction problem, eliminating the need for discretizing the fixations into lower-resolution grid space, which causes an inevitable loss of precision in the fixation prediction as in previous methods \cite{chen2021predicting,yang2020predicting,yang2022target}. Our model is similar to IVSN \cite{zhang2018finding} in using a top-down query to match features in a feature map for each visual task (free-viewing or search). However, a critical difference is that in IVSN the query is directly extracted from a given task-related image; whereas in our setting HAT automatically learn the task-specific queries from the training fixations.



% Our focus on scanpath prediction here will be in the context of visual search and free viewing. Although we believe that our approach is general, scanpath prediction scenarios such as VQA \cite{chen2021predicting} and driving \cite{baee2021medirl} are currently outside the scope of our work.

% Human attention can be broadly categorized into top-down (visual search) attention and bottom-up (free-viewing) attention. For free-viewing scanpath, early models \cite{itti2000saliency,walther2006modeling} predicts a static saliency map and employ a winner-take-all heuristic to sample fixations in order. 

% These models are designed for free-viewing scanpath prediction and it is nontrivial to adapt them for visual search with multiple tasks,
% \footnote{A naive solution is to train a separate model for each visual search task, which is inefficient and costly.}. 










% \myheading{Dense prediction.}
% Dense prediction \cite{ranftl2021vision,ronneberger2015u,long2015fully,cheng2021per,cheng2022masked} concerns the problem of predicting a label for every pixel in the image. Dense prediction networks often employ a encoder-decoder architecture. The encoder (a.k.a. backbone) progressively compresses the image into lower-resolutional feature maps; the decoder reverses the process by upsampling the feature maps and converting them to the final prediction with a resolution same as or close to the resolution of the input image.
% Here, to avoid fixation discretization as in \cite{zelinsky2019benchmarking,yang2020predicting,yang2022target,chen2021predicting}, HAT, inspired by the dense prediction architectures in \cite{cheng2022masked,cheng2021per}, predicts a fixation probability for each pixel given the image input and the fixation history. Notably, HAT equips the dense prediction network with a foveated retina which {\it dynamically} integrates the information acquired through fixations for scanpath prediction.

\section{Approach}
In this section, we first formulate scanpath prediction as a sequence of dense prediction tasks using behavior cloning. We then introduce our proposed transformer-based model, {\bf HAT}, for scanpath prediction. Finally, we describe how we train HAT and use it for fast inference.

\subsection{Preliminaries}
% Scanpath prediction is often modelled as an imitation learning problem where a model is trained to generate scanpaths that are similar to human scanpaths. Inverse reinforcement learning \cite{yang2020predicting,yang2022target} and behavior cloning methods \cite{zelinsky2021predicting,de2022scanpathnet} have been proposed to predict human scanpaths. These methods often break down scanpath prediction into a sequential prediction of the next fixation given the image input and all previous fixations. To simplify the problem, fixations are often discretized using a coarse grid and models are trained to output a {\it categorical} probability distribution of fixation grids. For instance, Yang~\etal \cite{yang2022target} discretize the fixations into a \htimesw{20}{32} grid and use the grid center as the predicted fixation at inference. However, we argue that discretizing fixations would hurt the accuracy of the fixation prediction model especially for high-resolutional input where fixations are likely to fall into the same grid.

%To avoid the precision loss incurred by grid discretization in existing fixation prediction methods \cite{zelinsky2019benchmarking,yang2020predicting,chen2021predicting,yang2022target}, we formulate scanpath prediction as a sequential dense prediction task without discretizing the fixations. 
To avoid the precision loss caused by grid discretization present in prior fixation prediction methods \cite{zelinsky2019benchmarking,yang2020predicting,chen2021predicting,yang2022target}, we formulate scanpath prediction as a sequential prediction of pixel coordinates. 
Given a $H{\times}W$ image and the initial fixation $f_0$ (often set as the center of an image), a scanpath prediction model aims to predict a sequence of human-like fixation locations $\{f_1, \cdots, f_n\}$, with each fixation $f_i$ being a pixel location in the image. Note that $n$ is variable that may be different for each scanpath due to the different termination criteria of different human viewers. To model the uncertainty in human attention allocation, existing methods \cite{yang2022target,yang2020predicting,chen2021predicting,zelinsky2019benchmarking} often predict a probability distribution over a coarse grid of fixation locations at each step. HAT takes the same spirit but outputs a dense fixation heatmap. Specifically, HAT outputs a heatmap $Y_i\in[0,1]^{H{\times} W}$ with each pixel value indicating the chance of the pixel being fixated in the next fixation. In addition, HAT also outputs a termination probability $\tau_i\in[0,1]$ indicating how likely the model should terminate the scanpath at the current step $i$. To sample a fixation, we apply $L_1$-normalization on $Y_i$. In the following, we omit the subscript $i$ for brevity.
% \mhoai{This 2nd last sentence is not clear to me. What's the purpose of $L_1$-normalization???}

\subsection{Human Attention Transformer}
\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/working_mem.pdf}
  \caption{{\bf Working memory construction.} We construct the working memory by starting with the visual embeddings (\enquote{what}) flattened from $P_1$ over the spatial axes and selected from $P_4$ at previous fixation locations. Scale embedding is introduced to capture scale information. Spatial embeddings and temporal embeddings are further added to the tokens to enhance the \enquote{where} and \enquote{when} signals. At every new fixation (marked in red), we simply add a new foveal token while keeping other tokens unchanged.}
  \label{fig:foveation}
\end{figure}


HAT is a novel transformer-based model for scanpath prediction. At each fixation, HAT outputs a set of prediction pairs $\{(Y_t, \tau_t)\}_{t=1}^T$ where $t$ indicates a task, which could be a visual search task (e.g., clock search and mouse search) or a free-viewing task. \Fref{fig:overview} shows an overview of the proposed model. HAT consists of four modules: 1) a feature extraction module that extracts a feature pyramid with multi-resolutional feature maps corresponding to information extracted at different eccentricities \cite{yang2022target,rashidi2020optimal}; 2) a foveation module which maintains a dynamical working memory representing the information acquired through fixations; 3) an aggregation module that selectively aggregates the information in the working memory using attention mechanism for each task; 4) a fixation prediction module that predicts the fixation heatmap $Y_t$ and termination probability $\tau_t$ for each task $t$.

\myheading{The feature extraction module}
consists of a pixel encoder (e.g., ResNet \cite{he2016deep}, a Swin transformer \cite{liu2021swin}), and a pixel decoder (e.g., FPN \cite{lin2017feature} and deformable attention \cite{zhu2020deformable}). Taking a $H{\times} W$ image as input, the pixel encoder encodes the input image into a high-semantic but low-resolution feature map. 
%The pixel decoder gradually up-samples the low-resolution feature map to higher resolution by a scale factor of two and constructs a pyramid of four multi-scale feature maps, 
The pixel decoder up-samples the feature map several times, each time by a scale factor of two, to construct a pyramid of four multi-scale feature maps 
denoted as $P=\{P_1, \cdots, P_4\}$, where $P_1\in \mathbb{R}^{C{\times}\frac{H}{32}{\times}\frac{W}{32}}$, $P_4\in \mathbb{R}^{C{\times}\frac{H}{4}{\times}\frac{W}{4}}$, and $C$ is the channel dimension.

\myheading{The foveation module}
% Humans are known to have a foveated retina which samples the visual information in the view in a non-uniform manner---pixels around the fixation are sampled at high resolution (i.e., foveal vision) while the pixel resolution gradually drop off as its distance from the fixation point increases (i.e., peripheral vision). The foveal vision is used to scrutinize details of the scene or objects, whereas the peripheral vision provides broad, spatial information of the scene. Inspired by \cite{yang2022target} which apply foveation on the feature pyramid, 
constructs a {\it dynamic} working memory using the feature maps $P_1$ and $P_4$ to represent the information a person acquires from the peripheral and foveal vision, respectively. We discard medium-grained feature maps $P_2$ and $P_3$ in computing the peripheral representation for computational efficiency. We did not observe performance improvement after adding $P_2$ in the peripheral tokens (see \Sref{sec:ablation} for details). Finally, we apply a Transformer encoder \cite{vaswani2017attention} to dynamically update the working memory with the information acquired at a new fixation.

\Fref{fig:foveation} illustrates the construction of the working memory. 
The working memory consists of two parts: peripheral tokens and foveal tokens. We first flatten the low-resolution feature map $P_1$ over the spatial axes to obtain the peripheral visual embeddings $V^p\in\mathbb{R}^{(\frac{H}{32}\cdot\frac{W}{32}){\times} C}$. Feature vectors in $P_4$ at each fixation location are selected as the foveal visual embeddings $V^f\in\mathbb{R}^{k{\times} C}$, where $k$ is number of previous fixations. For simplicity, we round the fixation to its nearest position in $P_4$.
Then we add a learnable {\bf scale} embedding to each token to discern the scale/resolution of the visual embeddings. As the spatial information is shown to be important in predicting human scanpath (e.g., center bias and inhibition of return \cite{wang2010searching}), we enrich the peripheral and foveal tokens with their 2D {\bf spatial} information in the image. 
% We use the 2D sinusoidal position encoding \cite{li2021learnable} to encode the 2D spatial location of each token in the image space. 
Specifically, we create a lookup table of 2D sinusoidal position embeddings \cite{li2021learnable} $G\in\mathbb{R}^{H{\times} W{\times} C}$ by concatenating the 1D sinusoidal positional encoding of the horizontal and vertical coordinates of each pixel location. For a visual embedding at position $(i, j)$ of a given feature map of stride $S$ ($S=32$ for $P_1$ and $S=4$ for $P_4$), its position encoding is defined by the element at position $(t_i, t_j)$ in $G$ where $t_i=\lfloor i\cdot S\rfloor$ and $t_j=\lfloor j\cdot S\rfloor$. 
% Not only knowing \enquote{what} and \enquote{where} have been seen in previous fixations is important for predicting the next fixation, also important is \enquote{when} the pixel has been seen as information acquired in distant past fixations would has less effect on the current gaze behavior (i.e., fading memory). 
% The fading memory effect is more prominent in target-absent visual search tasks and free-viewing task where the scanpaths are long. To this end, 
Furthermore, we add to each foveal token the {\bf temporal} embedding, a learnable vector, according to its fixation index to capture the temporal order among previous fixations.


\myheading{The aggregation module}
% We represent each task (a visual search task or the free-viewing task) with a task-specific search query. 
is a transformer decoder \cite{vaswani2017attention} that selectively aggregates information from the working memory into the task-specific queries $Q\in \mathbb{R}^{N{\times} C}$, where $N$ is the number of tasks (e.g., $N=18$ for COCO-Search18 \cite{chen2021coco} and $N=1$ for free-viewing datasets). The transformer decoder has $L$ layers, with each layer consisting of a cross-attention layer, a self-attention layer and a feed-forward network (FFN). Different from the standard transformer decoder \cite{vaswani2017attention}, we follow \cite{cheng2022masked} and switch the order of cross-attention and self-attention module.
Firstly, each zero-initialized task query selectively gathers the information in working memory acquired through previous fixations using cross-attention. Then, the self-attention layer followed by a FFN is applied to exchange information in different queries which could boost the contextual cues \cite{chun1998contextual} in each query. 
% Note that unlike DETR \cite{carion2020end} where queries form a unordered set and thus matching queries with the ground-truth annotations is required to compute the loss, here the queries can be considered as an ordered set of kernels. By convolving with the high-resolution feature map, each kernel yields a fixation heatmap indicating where to look at next for a specific task.

\myheading{The fixation prediction module} yields the final prediction---a fixation heatmap $\hat{Y}_t$ and a termination probability $\hat{\tau}_t$ for each task $t$. For the termination prediction, a linear layer followed by a sigmoid activation is applied on top of each updated query $q_t\in Q$:
\begin{equation}
    \hat{\tau}_t = \text{sigmoid}(Wq_t^T+b),
\end{equation}
where $W$ and $b$ are the parameters of the linear layer. For the fixation heatmap prediction, a Multi-Layer Perceptron (MLP) with two hidden layers first transforms $q_t$ into a task embedding, which is then convolved with the high-resolution feature map $P_4$ to get the fixation heatmap $\hat{Y}_t$ after a sigmoid layer:
\begin{equation}\label{eq:heatmap}
    \hat{Y}_t=\text{sigmoid}(P_4 \odot \text{MLP}(q_t)),
\end{equation}
where $\odot$ denotes the pixel-wise dot product operation. Finally, we upsample $\hat{Y}_t$ to the image resolution.
Note that the predictions for all tasks, i.e., $\hat{Y}\in\mathbb{R}^{N{\times} H{\times} W}$ and $\hat{\tau}\in\mathbb{R}^{N{\times}1}$, are yielded in parallel.

\subsection{Training and Inference}
\myheading{Training loss.}
We follow \cite{zelinsky2019benchmarking} and use behavior cloning to train HAT. The problem of scanpath prediction is broken down into learning a mapping from the input triplet of an image, a sequence of previous fixations, and a task to the output pair of a fixation heatmap and a termination probability. Given the predicted fixation heatmaps $\hat{Y}\in\mathbb{R}^{N{\times} H{\times} W}$ and termination probabilities $\hat{\tau}\in\mathbb{R}^{N{\times}1}$, the training loss is only calculated for its ground-truth task $t$:
\begin{equation}
    \mathcal{L}=\mathcal{L}_\text{fix}(\hat{Y}_t, Y)+\mathcal{L}_\text{term}(\hat{\tau}_t, \tau),
\end{equation} 
where $Y\in[0,1]^{H{\times} W}$ and $\tau\in\{0,1\}$ are the the ground-truth fixation heatmap and termination label for task $t$, respectively. We compute $Y$ by smoothing the ground-truth fixation map with a  Gaussian kernel with the kernel size being one degree of visual angle. $\mathcal{L}_\text{fix}$ denotes the fixation loss and is computed using pixel-wise focal loss \cite{lin2017focal,law2018cornernet}:
\begin{equation}\label{eq:focal}
\begin{aligned}
\mathcal{L}_\text{fix} = \frac{-1}{HW}\sum_{i,j}
\begin{cases} 
    (1-\hat{Y}_{ij})^\alpha\log(\hat{Y}_{ij}) & \textrm{if } Y_{ij}=1,\\[5pt]
    \begin{gathered}
    (1-{Y}_{ij})^\beta(\hat{Y}_{ij})^\alpha\\
    \log(1-\hat{Y}_{ij}) 
    \end{gathered}& \text{otherwise},
\end{cases}
\end{aligned}
\end{equation}
where $Y_{ij}$ represents the value of $Y$ at location $(i, j)$ and $\alpha=1$ and $\beta=4$.
$\mathcal{L}_\text{term}$ is the termination loss and is computed by applying a binary cross entropy (negative log-likelihood) loss, i.e., 
\begin{equation}
    \mathcal{L}_\text{term}=-\omega\cdot\tau\log(\hat{\tau}_t)- (1-\tau)\log(1-\hat{\tau}_t),
\end{equation}
where $\omega$ is a weight to balance the loss of positive and negative training examples since there are many more  negative labels than positive labels for training a termination prediction, especially for target-absent visual search and free-viewing tasks where scanpath are long. We set $\omega$ to be the ratio of the number of negative training instances to the number of positive ones.

% \myheading{Auxiliary task.}
% Following \cite{yang2022target}, we also use object center map detection as an auxiliary task to train HAT for visual search tasks. To predict the 80 object center maps (one for each object category in COCO \cite{lin2014microsoft}), we simply append 80 additional object detection queries in the aggression module of HAT and output one center map for each object detection query using \Eref{eq:heatmap}. As in \cite{law2018cornernet,zhou2019objects}, we use pixel-wise focal loss (\Eref{eq:focal}) to train the center map detection task. One merit of HAT is that the aggression module automatically exchange information among different queries, which implicitly enhance the fixation queries with both features of the target and the contextual objects that might provide positional cues for locating the target.

\myheading{Inference.}
Similarly to \cite{yang2020predicting,yang2022target,chen2021predicting}, HAT also generates scanpaths autoregressively, but in an efficient way. Given an image, HAT only computes the image pyramid $P$ and peripheral tokens once. For a new fixation, a foveal token is constructed and appended to the working memory after which the aggregation module and fixation prediction module yield the fixation heatmaps and termination predictions for all tasks in parallel.


\setlength{\tabcolsep}{6pt}
\begin{table*}[t]
\begin{center}
\begin{tabular}{l|ccccc|ccccc}
% \toprule
& \multicolumn{5}{c|}{Target-present} &
\multicolumn{5}{c}{Target-absent} \\ %\cline{2-11} 
 &  SemSS  & SS & cIG  & cNSS & cAUC & SemSS  & SS & cIG  & cNSS & cAUC\\ 
\Xhline{3\arrayrulewidth}
Human consistency & 0.526 & 0.500 & - & - & - & 0.410 & 0.381 & - & - & -\\\hline
Detector & 0.545 & 0.451 & 0.182 & 2.346 & 0.905 & 0.369 & 0.321 & -0.516 & 0.446 & 0.783\\
Fixation heuristic &0.530&	0.437&1.107&	2.186 &  0.917 & 0.347	& 0.298	&	-0.599&	0.405 & 0.798\\
IVSN \cite{zhang2018finding} & 0.393 & 0.326 & -0.192 & 1.318 & 0.901 & 0.284 & 0.222 & -0.219 & 0.884 & 0.867\\
PathGAN \cite{assens2018pathgan} & 0.310 & 0.244 & - & - & - & 0.349 & 0.250 & - & - & -\\
IRL \cite{yang2020predicting} & 0.491 & 0.422 & -9.709 & 1.977 & 0.913 & 0.301 & 0.319 & 0.032 & 1.202& 0.893\\
Chen \etal~\cite{chen2021predicting} & 0.536 & 0.445 & -1.273 &2.606 & 0.956 & 0.381 & 0.331 & -3.278 & 1.600& 0.925\\
FFMs \cite{yang2022target} & 0.529 & {\bf 0.451} & 1.548 & 2.376 & 0.932 & 0.389 & 0.372 & 0.729 & 1.524 & 0.916 \\
{HAT (ours)} & {\bf 0.554} & {0.444} & {\bf 2.259} & {\bf 4.769}  & {\bf 0.970} & {\bf 0.412} & {\bf 0.388} & {\bf 1.314} & {\bf 2.528}  & {\bf 0.945}\\
\hline
{Joint HAT (ours)} & {\bf 0.575} &	{\bf 0.468}&	{\bf 2.301}& 4.687 & {\bf 0.977} & {\bf 0.421}&	{\bf 0.394}&	{\bf 1.372}& {\bf 2.624} & {\bf 0.948}
% \bottomrule
\end{tabular}
\caption{{\bf Comparing visual search scanpath prediction algorithms} (rows) using multiple scanpath metrics (columns) on the target-present test set and the 
target-absent test set of COCO-Search18. All metrics are the higher the better. We highlight the best results in bold. Note that \cite{yang2022target} reported SemSS only using the 80 COCO object categories, here we also include the stuff categories (background) in COCO-Stuff \cite{caesar2018coco}, which results in a lower SemSS for the same method. \enquote{Joint} means training jointly with TP and TA scanpaths.}
\label{tb:vs_rst}
\end{center}
\end{table*}
\section{Experiments}
In this section, we describe our experiments to study the effectiveness of HAT for scanpath prediction on both visual search and free-viewing tasks.% Then, we ablate different components of HAT to understand their roles for modeling human attention control.
 
\myheading{Datasets.}
We train and evaluate HAT using four datasets: COCO-Search18 \cite{chen2021coco}, COCO-FreeView \cite{Chen_2022_CVPR}, MIT1003 \cite{judd2009learning} and OSIE \cite{xu2014predicting}. COCO-Search18 is a large-scale visual search dataset containing both target-present and target-absent human scanpaths in searching for 18 different object target. COCO-Search18 contains 3101 target-present images and 3101 target-absent images selected from COCO \cite{lin2014microsoft}, each viewed by 10 subjects. 
COCO-FreeView is a \enquote{sibling} dataset of COCO-Search18 but with free-viewing scanpaths. COCO-FreeView contains the same images with COCO-Search18, each viewed by 10 subjects in a free-viewing setting. 
MIT1003 is a widely-used free-viewing dataset containing 1003 natural images. OSIE is a also free-viewing gaze dataset but with rich semantic-level annotations, containing 700 natural indoor and outdoor scenes. Each image in MIT1003 and OSIE is viewed by 15 subjects.

\myheading{Evaluation metrics.}
To measure the performance, we mainly analyze the scanpath prediction models from two aspects: 1) how similar the predicted scanpaths are to the  human scanpaths; and 2) how accurate a model predicts the next fixation {\it given all previous fixations}. 
To measure the scanpath similarity, we use a commonly adopted metric, {sequence score (SS)} \cite{borji2013analysis} and its variant {semantic sequence score (SemSS)} \cite{yang2022target}. SS transforms the scanpaths into sequences of fixation cluster IDs and then compares them using a string matching algorithm \cite{needleman1970general}. Different from SS, SemSS transforms a scanpath into a string of semantic labels of the fixated pixels. 
% Following \cite{yang2022target}, we also report the SS of scanpaths truncated at 4 new fixations, denoted as {\bf SS(4)}.
For next fixation prediction, we follow \cite{kummerer2021state,yang2022target,kummerer2022deepgaze} and report the conditional saliency metrics, {cIG, cNSS and cAUC}, which measure how well a predicted fixation probability map of a model predicts the ground-truth (next) fixation when the model is provided with the fixation history of the scanpath in consideration, using the widely used saliency metrics, IG, NSS and AUC \cite{bylinskii2018different}. For fair comparison, we follow \cite{yang2022target} and predict one scanpath for each testing image, step by step selecting the most probable fixation location as the next fixation.

\myheading{Other models.}
We first compare our model against several heuristic baselines. Following prior works \cite{yang2020predicting,zelinsky2019benchmarking,yang2022target,chen2021predicting,kummerer2022deepgaze}, the {human consistency}, an oracle where we use one viewer's scanpath to predict the scanpath of another, is reported as a gold-standard model. Second, we compare to a {fixation heuristic} method, a ConvNet trained to predict human fixation density maps, from which we sample fixations sequentially with inhibition of return. For visual search scanpaths, we further include a {detector} baseline, which is similar to the fixation heuristic, but trained on target-present images of COCO-Search18 to output target detection probability maps. For the fixation heuristic and detector baselines, we use the winner-take-all strategy to generate scanpaths.
Furthermore, we compare HAT to the previous state-of-the-art models of scanpath prediction: {IVSN} \cite{zhang2018finding}, {IRL} \cite{yang2020predicting}, {Chen \etal} \cite{chen2021predicting}, DeepGaze III \cite{kummerer2022deepgaze} and {FFMs} \cite{yang2022target}.
Note that IVSN only applies for visual search tasks, and unlike other methods, IVSN is designed for zero-shot search scanpath prediction, hence is not trained with any gaze data. DeepGaze III only applies for free-viewing scanpaths and is trained with the SALICON dataset \cite{jiang2015salicon} and MIT1003 \cite{judd2009learning}.
% A zero-shot variant of the detector baseline, {IVSN} \cite{zhang2018finding}, is also included for comparison. We follow \cite{ding2022efficient} for the implementation of IVSN in COCO-Search18. 
% Although these models are originally proposed for predicting visual search scanpath, they can be directly applied for free-viewing scanpath prediction. We retrained these models on COCO-FreeView for free-viewing scanpath prediction.


\subsection{Implementation Details}
\myheading{Network structure.}
HAT has four modules as shown in \Fref{fig:overview}. For the feature extraction module, we use ResNet-50 \cite{he2016deep} as our pixel encoder and MSDeformAttn \cite{zhu2020deformable} as the pixel decoder (results with other pixel encoders, ResNet-101 and Swin Transformer \cite{liu2021swin}, and pixel decoder, FPN \cite{lin2017feature}, can be found in the supplement).
The number of channels of the feature maps $C$ is set to 256. For the foveation module, the transformer encoder has three layers. The transformer decoder in the aggregation module has six layers (i.e,. $L=6$). The number of queries $N=18$ for visual search scanpath prediction (as COCO-Search18 contains 18 target categories) and $N=1$ for free-viewing scanpath prediction. Finally, the MLP in the fixation prediction module has two hidden layers of 128 dimensions and a ReLU activation function.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/vis3.pdf}
  \caption{{\bf Visualization of the ground-truth human scanpaths and predicted scanpaths of different methods (columns).} Three different settings (rows) including target-present bottle search, target-absent stop sign search and free viewing are shown from the top to bottom. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL, detector and fixation heuristic, we visualize the first 6 fixations for visual search and 15 for free viewing. The rightmost column shows the predicted scanpaths of the heuristic methods (detector for visual search and fixation heuristic for free-viewing).}
  \label{fig:vis}
\end{figure}
\myheading{Training settings.}
We follow \cite{yang2022target,yang2020predicting} and resize all images to $320{\times} 512$ for computational efficiency during training and inference. We use the AdamW \cite{loshchilov2017decoupled} with the learning rate of $0.0001$ and train HAT for 30 epochs with a batch size of 128. No data augmentation is used during training. Note that the pixel  encoder and pixel decoder are kept fixed during training and we use the COCO-pretrained weights for panoptic segmentation from \cite{cheng2022masked}.
Following \cite{yang2020predicting}, we set the maximum length of each predicted scanpath to 6 and 10 (excluding the initial fixation) for target-present and target-absent search scanpath prediction, respectively. For free viewing, the maximum scanpath length is set to 20. More details can be found in the supplement.

\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccccccc}
% \toprule
& SS & cIG  & cNSS & cAUC\\
\Xhline{3\arrayrulewidth}
Human consistency & 0.349  & - & - & -\\\hline
Fixation heuristic & 0.329 & 0.319 & 1.621 & 0.930\\
PathGAN \cite{assens2018pathgan} & 0.181 & - & - & -\\
IRL \cite{yang2020predicting} & 0.300 & -0.213 & 1.018 & 0.888\\
Chen \etal~\cite{chen2021predicting} & {\bf 0.365} & -1.263 & 1.655 & 0.922\\
DeepGaze III \cite{kummerer2022deepgaze} & 0.339 & 0.140 & 1.418 & 0.910\\
FFMs \cite{yang2022target} &	0.329&	0.329&	1.432 & 0.918\\
{HAT} & 0.359 &{\bf 1.171}&	{\bf 2.841} & {\bf 0.947}\\
% \bottomrule
\end{tabular}
\caption{{\bf Comparing free-viewing scanpath prediction algorithms} (rows) using multiple metrics (columns) on the test set of COCO-FreeView. The best results are highlighted in bold.}
\label{tb:fv_rst}
\end{center}
\end{table}

\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
% \toprule
 & SS & cIG  & cNSS & cAUC \\
\Xhline{3\arrayrulewidth}
Human consistency & 0.363 & - & - & -\\\hline
Chen \etal~\cite{chen2021predicting} & 0.210	&-9.735	&0.186	&0.750\\
{HAT} & {\bf 0.225}	&{\bf 0.573}	&{\bf 1.841}	&{\bf 0.930}\\
% \bottomrule
\end{tabular}
\caption{Generalization to an unseen dataset MIT1003. Both models are trained on COCO-Freeview. The best results are in bold.}
\label{tb:mit1003}
\end{center}
\end{table}


% \setlength{\tabcolsep}{6pt}
% \begin{table*}[t]
% \begin{center}
% \begin{tabular}{l|ccc|ccc|ccc}
% % \toprule
% & \multicolumn{3}{c|}{COCO-Freeview test} &
% \multicolumn{3}{c|}{OSIE} &
% \multicolumn{3}{c}{MIT1003}\\ %\cline{2-11} 
%  &  SS  & cNSS & cAUC & SS  & cNSS & cAUC & SS   & cNSS & cAUC\\ 
% \Xhline{3\arrayrulewidth}
% Human consistency & 0.349  & - & - & 0.380  & - & - & 0.363  & - & -\\\hline
% Fixation heuristic & 0.329 & 0.319 & 1.621 & 0.930 & 0.329 & 0.319 & 1.621 & 0.930 & 0.329\\
% PathGAN \cite{assens2018pathgan} & 0.181 & - & - & -\\
% IRL \cite{yang2020predicting} & 0.300 & -0.213 & 1.018 & 0.888\\
% Chen \etal~\cite{chen2021predicting} & {\bf 0.365} & -1.263 & 1.655 & 0.922\\
% FFMs \cite{yang2022target} &	0.329&	0.329&	1.432 & 0.918\\
% {HAT} & 0.359 &{\bf 1.171}&	{\bf 2.841} & {\bf 0.947}\\
% % \bottomrule
% \end{tabular}
% \caption{{\bf Comparing free-viewing scanpath prediction algorithms} (rows) using multiple scanpath metrics (columns) on the target-absent test set of COCO-Search18. The best results are highlighted in bold.}
% \label{tb:fv_rst}
% \end{center}
% \end{table*}

\subsection{Main Results}
\myheading{Visual Search.}
In \Tref{tb:vs_rst}, we compare HAT with SOTA visual search scanpath prediction models in both target-present (TP) and target-absent (TA) settings on the COCO-Search18 test set. When trained separately on TP and TA scanpaths, HAT consistently outperforms all other methods in predicting TP and TA scanpaths. In TP setting, HAT achieves the best performance in four out of five metrics. Although FFM and Detector are better in SS, HAT achieves a higher SemSS, suggesting that HAT better understands the semantics behind fixations. Moreover, compared to the previous state of the art, Chen \etal~\cite{chen2021predicting}, HAT improves the cNSS by 83\% and 58\% in TP and TA settings, respectively. Importantly, when trained jointly with TP and TA scanpaths (never been done before), HAT further improves the performance in predicting both TP and TA scanpaths, surpassing or matching with human consistency in most metrics. 

\myheading{Free-viewing.}
Beyond visual search, HAT can also predict free-viewing scanpath by regarding the free-viewing as a single standalone task. In \Tref{tb:fv_rst}, we compare HAT with the baselines except Detector and IVSN as the free-viewing fixations are not tasked to searching for a target like visual search. HAT outperforms all other methods in cIG, cNSS and cAUC, especially HAT is 256\% and 72\% better than the second best (FFMs and \cite{chen2021predicting}) in cIG, cNSS, respectively. This reaffirms the effectiveness of HAT as a generic framework for scanpath prediction. We further compare HAT to the best alternative overall, Chen \etal \cite{chen2021predicting}, by evaluating the models trained using COCO-FreeView on an {\it unseen} dataset MIT1003 in \Tref{tb:mit1003}.
The results show that HAT outperforms \cite{chen2021predicting} in all metrics and with significant improvement in cIG, cNSS and cAUC. This suggests that other models like the predictions of FFMs and Chen~\etal's model are prone to be overconfident, whereas HAT better calibrates the confidence in predicting free-viewing fixations and thus provides a more robust prediction of human attention with better generalizability to unseen datasets.
Our additional experiments on OSIE and MIT1003 further confirm our findings. Please refer to the supplement for detailed results.
% Comparing to \cite{chen2021predicting} which is trained using reinforcement learning, HAT is trained under behavior cloning framework, making it hard to capture long-term planning in free-viewing setting where scanpaths are much longer than visual search. We leave the exploration of HAT with (inverse) reinforcement learning as our future work. 

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/attn_weights.pdf}
  \caption{{\bf Peripheral tokens vs foveal tokens} under TP, TA and FV settings (from left to right). 
%   On the top we visualize the contribution of the peripheral and foveal tokens in human attention control over the temporal space. The y-axis is the the number of previous fixations and the x-axis is the index of the tokens with 0 indicating all peripheral tokens and $i>0$ meaning that the $i$-th foveal token. At the bottom we visualize the {\it relative} contribution of each peripheral token.The contribution is measure by the attention weights of the last cross-attention layer of the aggregation module in HAT, averaged over the testing data. Brighter color means larger contribution.
    The top three figures visualize the temporal change of the contribution of peripheral and foveal memory tokens in predicting human attention. Here the contribution is measured by the attention weight from the last cross-attention layer of the aggregation module in HAT. X-axis shows the token index, with 0 representing all peripheral tokens (by summing the attention weights of all peripheral tokens) and $i>0$ being the $i$-th foveal token. Y-axis indicates temporal fixation step from first to max number of fixation steps allowed for each task. The bottom three figures show the spatial distribution of the attention weights of all peripheral tokens, averaged over the temporal axis. The brighter the color, the larger is the contribution.}
  \label{fig:attn_weights}
\end{figure}

\subsection{Qualitative Analysis}
\myheading{Scanpath visualization.}
In this section, we qualitatively compare the predicted scanpaths of different methods to each other and to the ground-truth human scanpaths in the TP, TA and FV settings. As shown in \Fref{fig:vis}, when searching for bottles in the TP setting, HAT not only correctly predicted the terminal fixation on the heavily-occluded target, but also predicted fixations on all the distractor objects that look similar to the target, like humans do. Other methods either missed the distractor objects or failed to find the target. Similarly, for the TA stop sign search, HAT was the only one that looked at both sides of the road in searching for a stop sign like the human subject would, showing a use of semantic and context cues to control attention. In the FV setting, HAT also predicted the most human-alike scanpaths among all methods in (1) the fixation locations (where), (2) the semantics (what), and (3) the order (when) of the fixations. This demonstrates that HAT captures the all three aspects (what, where and when) of human fixations. More scanpath visualizations can be found in the supplement.

\myheading{Model interpretability.}
We also qualitatively analyzed the contribution of peripheral tokens and foveal tokens in predicting human attention control under the TP, TA and FV settings, separately. As shown in \Fref{fig:attn_weights}, the peripheral tokens contribute the most in predicting TP fixations across all fixations (forming the yellow column on the left). This is because in TP images there is a strong target signal available in the visual periphery to guide attention. Contrast this with FV fixations, where the contribution of the peripheral tokens diminishes over the temporal space and the only the current foveal token has a strong and consistent contribution (a clear red diagonal line). An interpretation of this pattern is that people have only a poor memory of what they viewed in previous fixations and their attention is controlled by salient pixels within a local neighborhood around the current fixation. Interestingly, for TA fixations we also observe a diminishing contribution of the peripheral tokens over the temporal space, but not as pronounced. Moreover, as more fixations are made, the contribution of recent fixations increases, approaching the pattern in FV. This suggests that the later fixations of a TA scanpath behave like a FV scanpath, which confirms a finding in \cite{Chen_2022_CVPR}. Lastly, the bottom row visualizes the contribution of each individual peripheral token (averaged over the temporal axis), where we see peripheral tokens encode a strong center bias for FV fixations, whereas TA fixations show a weaker center bias and TP fixations show no obvious center bias at all, again as expected and confirming previous suggestion. This showcases the potential for HAT to make highly interpretable predictions of human attention control. Category-wise analysis for TP and TA can be found in the supplement.

\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
 &  SemSS  & SS & cIG & cNSS \\ 
\Xhline{3\arrayrulewidth}
$P_1$ (HAT) &  {\bf 0.412}	& {\bf 0.388} & 1.314 & 2.528 \\
$P_2$  &0.410&	0.385&	{\bf 1.321}&	{\bf 2.533}\\
$P_1 + P_2$ & 0.411	&0.386	&1.306	& 2.487\\\hline
% w/o detection loss & \\
w/o transformer enc. & 0.399&	0.377&	1.249& 	2.442\\
w/o spatial emb. & 0.400&	0.380&	1.040& 	2.299\\
w/o scale emb. & 0.403&	0.380&	1.283&	2.453\\
w/o temporal emb. & 0.411	&0.383	&1.136	& 2.244\\
% w/o self-att. & \\
w/o peripheral tokens & 0.404 & 0.374 & 1.248 & 2.465\\
w/o foveal tokens & 0.393 & 0.374 & 0.778 & 1.893\\
% \bottomrule
\end{tabular}
\caption{{\bf Ablation study} of HAT. These experiments are done on the TA set of COCO-Search18. The best results are in bold.}
\label{tb:ablation}
\end{center}
\vspace{-.1cm}
\end{table}

\subsection{Ablation studies}\label{sec:ablation}
We perform ablation studies of HAT under the TA setting.
%as target-absent search fixations carries the characteristics of both target-present search fixations and free-viewing fixations \cite{Chen_2022_CVPR}. 
% \mhoai{I don't undrestand the above sentence.}
In \Tref{tb:ablation}, we first verify our choice of extracting the peripheral tokens from $P_1$ in the foveation module of HAT. We train HAT with three different sources of peripheral tokens: 1) $P_1\in \mathbb{R}^{C{\times}\frac{H}{32}{\times}\frac{W}{32}}$ (160 peripheral tokens as $H=320$ and $W=512$); 2) $P_2\in \mathbb{R}^{C{\times}\frac{H}{16}{\times}\frac{W}{16}}$ (640 tokens); 3) $P_1+P_2$ where we concatenate the tokens for $P_1$ and $P_2$ (800 tokens in total). We observe that three different options have similar performance. We choose $P_1$ in our implementation because of its computational efficiency and smaller memory footprint.

We further verify the effectiveness of each component in the foveation module, including the transformer encoder, 2D spatial embedding, scale embedding and temporal embedding, peripheral tokens and foveal tokens, by ablating them one at a time. It is shown in \Tref{tb:ablation} that ablating any of these components incur a performance drop over all metrics. This suggests that all of these components contribute to the superior performance of HAT. Among these components, removing foveal tokens incurs the largest performance drop (cIG decreases by nearly 41\%). This is understandable because foveal tokens represent the knowledge acquired in previous fixations, without which HAT can be considered as a fixation density map predictor like the fixation heuristic baseline.
In addition, spatial embedding and temporal embedding also contribute significantly to the performance of HAT, without them cIG drops by 21\% and 14\%, respectively. This indicates that knowing \enquote{where} and \enquote{when} visual information is acquired is important for predicting scanpath.
Please refer to the supplementary material for more ablation studies of HAT.% such as the number of Transformer encoder/decoder layers.

\section{Conclusions and Discussion}
With the rapid development of Augmented Reality (AR) and Virtual Reality (VR) technologies, there is an increasing demand for predicting and understanding human gaze behavior \cite{park2021mosaic,kaplanyan2019deepfovea,park2019advancing}, with scanpath prediction being a challenging task. For those AR/VR applications requiring a high input resolution ($360^\circ$), discretizing fixations into a coarse grid incurs a non-negligible loss in accuracy.
In this work we presented HAT, a generic attention scanpath prediction model. Built from a simple dense prediction framework \cite{cheng2021per}, HAT circumvents the drawbacks of discretizing fixations as in prior state of the arts \cite{yang2020predicting,chen2021predicting,yang2022target}. Inspired by the human vision system, HAT uses a novel foveated working memory which dynamically updates its knowledge about the scene as it changes its fixation. We show that HAT achieves new SOTA performance, not only in predicting free-viewing fixation scanpaths, but also scanpaths in target-present and target-absent search. In demonstrating this broad scope, our HAT model sets a new bar in the computational attention of attention control. 
%We show that HAT obtains top performance in predicting both target-present and target-absent search scanpath, as well as free-viewing scanpath. 
%We hope our work would spur a joint efforts in understanding both visual search gaze behavior and free-viewing gaze behavior as well as their relationship.

% TODOs:
% \begin{itemize}
%     \item create a teaser image
%     \item visualize attention weights
%     \item Train models on high-resolution input (1050x1680)
%     \item Test methods on OSIE
% \end{itemize}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib,pubs_iccv}
}

\input{suppl}
\end{document}
