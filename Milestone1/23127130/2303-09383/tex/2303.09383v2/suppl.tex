\cleardoublepage

\appendix
\section{Appendix}

This document provides further implementation details (\Sref{sec:impl}), additional results from ablation studies (\Sref{sec:abl}), experiments on OSIE and MIT1003 (\Sref{sec:osie}), additional qualitative analysis (\Sref{sec:qua}) and further discussion about the applications of the proposed HAT model (\Sref{sec:app}).


\section{Implementation details}\label{sec:impl}
\myheading{Training details.}
Follow \cite{yang2022target,chen2021predicting,yang2020predicting}, we resize the images in all datasets (COCO-Search18, COCO-FreeView, OSIE and MIT1003) to a fixed resolution ($320{\times} 512$ in this paper) for computational efficiency during training and inference. The ground-truth human fixations are remapped (linearly) onto the resized image. For COCO-Search18, COCO-FreeView and OSIE, we use their official data split for training and testing. For the MIT1003 dataset, we perform 5-fold cross validation on the 1003 images in MIT1003, i.e., training on 800 images (and the associated human scanpaths) and testing on the rest 203 images in each fold. We report the averaged results (over 5 folds) for the MIT1003 dataset.
We use the AdamW \cite{loshchilov2017decoupled} with the learning rate of $0.0001$ and train HAT for 30 epochs with a batch size of 128. No data augmentation is used during training. Note that the pixel encoder and pixel decoder are kept fixed during training and we use the COCO-pretrained weights for panoptic segmentation from \cite{cheng2022masked}. In summary, the following parameters in HAT are trained end-to-end using the loss defined in Eq. (3) of the main paper: the Transformer encoder in the foveation module; all task-specific queries and the Transformer decoder in the aggression module; and the linear layer (W, b) and MLP in the fixation prediction module.
Following \cite{yang2020predicting}, we set the maximum length of each predicted scanpath to 6 and 10 (excluding the initial fixation) for target-present and target-absent search scanpath prediction (COCO-Search18), respectively. For free-viewing datasets (COCO-FreeView, OSIE and MIT1003), the maximum scanpath length is set to 20.

\myheading{Implementation of cIG.}
cIG denotes the amount of information gain from the predicted fixation map (the model is provided with all previous fixations) over a baseline in predicting the ground-truth fixation. Here, the baseline is a  fixation density map constructed by averaging the smoothed density (with a Gaussian kernel of one degree of visual angle) maps of all training fixations. For target-present and target-absent visual search settings, we use a (target) category-wise fixation density map, following \cite{yang2022target}. For the heuristic models (i.e., target detector and saliency heuristic) which apply the winner-take-all strategy on a static fixation map to generate the scanpath prediction, we use the same static fixation map for all fixations in a scanpath to compute cIG, cNSS and cAUC. To obtain the predicted fixation maps for Chen \etal's model~\cite{chen2021predicting}, we use the ground-truth fixation map (Gaussian smoothed with a kernel size of 2) as input to obtain the predicted action map for the next fixation (i.e., the predicted fixation map). Note that all predicted fixation maps in computing cIG, cNSS and cAUC, are resized to $\htimesw{320}{512}$ for fair comparison.

\myheading{Inference speed.} On a single RTX A6000 GPU, it takes about 1 second and 18GB GPU memory for HAT to process a batch of 128 images at each fixation step (i.e., 0.008 secs for each image). The total time for generating a scanpath depends on the scanpath length. For instance, on average HAT takes 0.048 secs to generate 128 target-absent scanpaths whose average length is about 6 fixations.

\section{More Ablation Studies with HAT}\label{sec:abl}
% \myheading{Feature extraction module}
By default, HAT uses ResNet-50 \cite{he2016deep} as the pixel encoder and MSD \cite{zhu2020deformable} as the pixel decoder. However, HAT is also compatible with other architectures. Hence, in \Tref{tb:feat_ext}, we evaluate HAT with different pixel encoders and decoders. Three pixel encoders: ResNet-50 (R50), ResNet-101 (R101) \cite{he2016deep} and Swin Transformer (we use the base model, Swin-B) \cite{liu2021swin}), and two pixel decoders: FPN \cite{lin2017feature} and MSD \cite{zhu2020deformable}, are evaluated. One can observe that MSD is slightly better than FPN as the pixel decoder and HAT performs the best when using R50 as the pixel encoder. Notice that the difference between different configurations is small, suggesting that the performance of HAT is robust to the choice of different pixel encoder and decoder architectures. 
More importantly, all of these configurations of HAT significantly outperforms all baselines in the main text (see Table 1 of the main text).

In \Tref{tb:trans_dec}, we ablate the Transformer decoder in the aggregation module of HAT by using different numbers of Transformer decoder layers: $L=1, 3, 6, 9$. By default, HAT uses 6 layers of Transformer decoder, i.e., $L=6$. By varying $L$, we observe no significant performance change in HAT with $L=6$ being slightly better than other configurations in SS and cIG. This indicates that HAT's superior performance does not depend on the choice of hyper-parameter $L$.

\section{Experiments on OSIE and MIT1003}\label{sec:osie}
To further validate the effectiveness of our proposed HAT in free-viewing scanpath prediction, we compare HAT to the previous state-of-the-art method in free-viewing scanpath prediction, Chen~\etal \cite{chen2021predicting} (see Tab. 2 of the main text), using the OSIE dataset \cite{xu2014predicting} and the MIT1003 dataset \cite{judd2009learning}. Here we only report SS, cIG, cNSS and cAUC and do not use SemSS because OSIE and MIT1003 do not contain pixel-wise segmentation annotation which is required in SemSS. \Tref{tb:osie} and \Tref{tb:mit1003} consistently show that HAT surpasses Chen et al.~\cite{chen2021predicting} in all three metrics by a large margin especially in cIG and cNSS on both free-viewing datasets. The results are consistent with our findings in Table 2 of the main text---HAT accurately predicts the scanpaths (reflected by SS), with well-calibrated confidence (as evidenced by the high cIG and cNSS).

\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{c|c|cccc}
 Pixel enc. & Pixel dec. & SemSS  & SS & cIG & cNSS \\ 
\Xhline{3\arrayrulewidth}
R50& MSD & {\bf 0.412}	& {\bf 0.388} & {\bf 1.314} & {\bf 2.528} \\
R50& FPN & 0.404 & 0.376 & 1.283 & 2.465\\
R101& MSD & 0.404 & 0.383 & 1.238 & 2.467\\
Swin-B & MSD & 0.406 & 0.382 &1.289 & 2.508\\

% \bottomrule
\end{tabular}
\caption{{\bf Comparing different pixel encoder and pixel decoder} in HAT. The ablation experiments are done on the target-absent set of COCO-Search18. The best results are highlighted in bold.}
\label{tb:feat_ext}
\end{center}
\end{table}

\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{c|cccc}
 $L$ & SemSS  & SS & cIG & cNSS \\ 
\Xhline{3\arrayrulewidth}
1 & {\bf 0.413} & 0.386 & 1.277 & 2.536\\
3 & 0.412 & 0.384 & 1.301 & {\bf 2.557}\\
6 & {0.412}	& {\bf 0.388} & {\bf 1.314} & 2.528\\
9 & 0.411 & 0.384 & 1.304 & 2.528\\

% \bottomrule
\end{tabular}
\caption{{\bf Comparing different choices of Transformer decoder layers $L$} in HAT. The ablation experiments are done on the target-absent set of COCO-Search18. The best results are highlighted in bold.}
\label{tb:trans_dec}
\end{center}
\end{table}


\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
% \toprule
 & SS & cIG & cNSS & cAUC \\
\Xhline{3\arrayrulewidth}
Human consistency & 0.380 & - & -\\\hline
Chen \etal~\cite{chen2021predicting} & 0.326 & -1.526 & 2.288 & 0.920\\
% FFMs \cite{yang2022target} & 0.464&	0.329&	0.329&	1.432 \\
{HAT} & {\bf 0.333} & {\bf 1.519} & {\bf 2.780} & {\bf 0.942}\\
% \bottomrule
\end{tabular}
\caption{{\bf Comparing free-viewing scanpath prediction algorithms on OSIE} (rows) using multiple scanpath metrics (columns). The best results are highlighted in bold.}
\label{tb:osie}
\end{center}
\end{table}


\setlength{\tabcolsep}{6pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{l|cccc}
% \toprule
 & SS & cIG  & cNSS & cAUC \\
\Xhline{3\arrayrulewidth}
Human consistency & 0.363 & - & - & -\\\hline
Chen \etal~\cite{chen2021predicting} & 0.260 & 0.042 & 1.408 & 0.927\\
{HAT} & {\bf 0.324} & {\bf 0.762} & {\bf 2.116} & {\bf 0.941}\\
% \bottomrule
\end{tabular}
\caption{Comparing free-viewing scanpath prediction algorithms (rows) on {\bf MIT1003 training set using 5-fold cross validation} using multiple scanpath metrics (columns). The best results are highlighted in bold.}
\label{tb:mit1003}
\end{center}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.\linewidth]{images/cat_peri.pdf}
  \caption{{\bf Peripheral contribution map of visual search fixations}. We show the contribution map of the peripheral tokens for two categories (rows): car and bottle, in target-present and target-absent settings (columns). We measure the contribution of each peripheral token by the attention weights from the last cross-attention layer of the aggregation module in HAT, averaged over the temporal axis of all testing data in COCO-Search18 \cite{chen2021coco}. The brighter the color, the larger the contribution.}
  \label{fig:cat}
\end{figure}

% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=1.0\linewidth]{images/TP_vis_ins1.pdf}
%   \caption{Visualization of the {\bf predicted scanpath, peripheral contribution map and fixation heatmap} (columns) of HAT for a {\bf target-present car search} example at every fixation (rows). We also include the predicted termination probability $\tau$ for each step. The model stop searching if $\tau>0.5$. The target bounding boxes are marked in red on the left column.}
%   \label{fig:tp_vis1}
% \end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/TP_vis_ins2.pdf}
   \caption{Visualization of the {\bf predicted scanpath, peripheral contribution map and fixation heatmap} (columns) of HAT for a {\bf target-present laptop search} example at every fixation (rows). We also include the predicted termination probability $\tau$ for each step on the left. The model stop searching if $\tau>0.5$.}
  \label{fig:tp_vis2}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/TA_vis_ins1.pdf}
   \caption{Visualization of the {\bf predicted scanpath, peripheral contribution map and fixation heatmap} (columns) of HAT for a {\bf target-absent car search} example at every fixation (rows). We also include the predicted termination probability $\tau$ for each step on the left. The model stop searching if $\tau>0.5$.}
  \label{fig:ta_vis1}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/TA_vis_ins2.pdf}
   \caption{Visualization of the {\bf predicted scanpath, peripheral contribution map and fixation heatmap} (columns) of HAT for a {\bf target-absent laptop search} example at every fixation (rows). We also include the predicted termination probability $\tau$ for each step on the left. The model stop searching if $\tau>0.5$.}
  \label{fig:ta_vis2}
\end{figure*}

\section{Additional Qualitative Analysis}\label{sec:qua}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/vis_TP.pdf}
  \caption{{\bf Target-present scanpath visualization}. We show the scanpaths of six methods (rows) for four different targets (columns) which are bottle, stop sign, microwave and knife. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL and detector, we visualize the first 6 fixations.}
  \label{fig:sps_tp}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/vis_TA.pdf}
  \caption{{\bf Target-absent scanpath visualization}. We show the scanpaths of six methods (rows) for four different targets (columns) which are bottle, stop sign, microwave and knife. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL and detector, we visualize the first 6 fixations.}
  \label{fig:sps_ta}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{images/vis_FV.pdf}
  \caption{{\bf Free-viewing scanpath visualization}.  We show the scanpaths of six methods (rows) for four example images. The final fixation of each scanpath is highlighted in red circle. For methods without termination prediction, i.e., IRL and detector, we visualize the first 15 fixations.}
  \label{fig:sps_fv}
\end{figure*}

\myheading{Peripheral contribution map.}
In Section 4.3 of the main text, we showed that there exists a strong center bias signal in the peripheral tokens of free-viewing (FV) fixations, whereas the center bias is less obvious in the peripheral tokens of target-absent (TA) and target-present (TP) fixations. A natural question arising from this observation is whether the peripheral tokens of TA and TP fixations encode a {\it target prior}--spatial distribution of the possible target location. To answer this question, we first visualize the category-wise peripheral contribution maps for TP and TA fixations by averaging the attention weights (on the peripheral tokens) of the last cross-attention layer over all testing fixations for each target category. As shown in \Fref{fig:cat}, the category-specific peripheral contribution map does not provide a clear evidence of TA and TP peripheral contribution map being a target prior, but we find some target-specific pattern, e.g., the contribution is pronounced around the bottom horizontal area for \enquote{car} and around the vertical area for \enquote{bottle}, which may represent the spatial prior of each category. 

We further analyze how the peripheral contribution map evolves across a sequence of fixations. \Fref{fig:tp_vis2} shows the peripheral contribution maps when searching for a laptop (middle column) with the corresponding predicted scanpath (left column) and fixation heatmaps (right column). Each row represents each fixation step. We observe that the encoded periphery features not only align with the location of the next fixation (e.g., when the occluded laptop is encoded in the left-bottom periphery, the model makes a fixation to the target and terminates the search), but also provides the {\it contextual cues} where a target might be located (e.g., near the keyboard and the monitor where a laptop is usually found). We also observe a similar pattern in the TA setting, where the peripheral contributions are higher for target-like objects (e.g., truck in car search, \Fref{fig:ta_vis1}) and for contextual cues (table and keyboard for laptop search, \Fref{fig:ta_vis2}) 

% One can see that only the contribution of the peripheral tokens near the current fixation that encode the car features are high, showing that HAT learns a set of target-specific attention weights that sample visual information from the peripheral tokens for locating the target in question and terminating the search. 

% In \Fref{fig:tp_vis2}, we show a target-present laptop search example where the target is heavily occluded. We observe that initially the contribution is high around the keyboard and monitor which are often hint a laptop nearby. Interestingly, after the second fixation made to the keyboard and monitor on the right of the image, their contribution reduces and the contribution of the real target pixels increases as fixations shifted to the left. This demonstrates that HAT successfully models the contextual cues in visual search fixations. 
% For the TA setting, we observe a similar phenomenon in \Fref{fig:ta_vis1} and \Fref{fig:ta_vis1} that the contribution of target-alike objects (e.g., truck in the TA car search in \Fref{fig:ta_vis1}) and objects with contextual cues (table and keyboard in the TA laptop search in \Fref{fig:ta_vis1}) are high.


\myheading{Scanpath visualization.}
We further visualize additional scanpaths for human (ground truth), our HAT, FFMs \cite{yang2022target}, Chen \etal \cite{chen2021predicting}, IRL \cite{yang2020predicting}, and a heuristic method (target detector for visual search and saliency heuristic for free viewing) in the TP, TA, and FV settings. \Fref{fig:sps_tp} shows the TP scanpaths. In all examples, HAT shows superior performance in predicting the human fixation trajectory not only when humans correctly fixate on the target, but also when their attention is distracted by other visually similar objects. For example, in the last column of \Fref{fig:sps_tp} when the task is to find a knife, HAT is the only model that correctly predicts the fixation on the metallic object (because knives are usually metallic), whereas other methods either missed the target or did not show any distractions to the metallic object. This shows the capacity of HAT in modeling human attention control in visual search. \Fref{fig:sps_ta} shows that HAT learns to leverage the context cues in predicting target-absent fixations, e.g., when the search target is microwave, HAT correctly predicted the fixations on the counter-top and table, where microwaves are often found. Similarly, HAT also generates the most human-like scanpaths in free-viewing task (see \Fref{fig:sps_fv}), capturing all important aspects of scanpaths, such as the locations (where), the semantics (what), and the order (when) of the fixations.  

 \section{Further discussion on applications}
 \label{sec:app} Models that predict top-down attention (TP/TA search fixations), modulated by an external goal, have wide applicability to attention-centric HCI. For example, 
faster attention-based rendering that leverages the prediction of a user's attention as they play a VR/AR game and home robots incorporating search-fixation-prediction models will be better at inferring a user's need (i.e., their search target).
Home robots incorporating search-fixation-prediction models will be better able to infer a users' need (i.e., their search target) and autonomous driving systems can attend to image input like an undistracted driving expert.
Applications of FV attention prediction exist in foveated rendering \cite{kaplanyan2019deepfovea} and online video streaming \cite{park2021mosaic}.

% it can be observed that HAT not only correctly predicted the fixations on target location and its termination, but also predicted the fixations that are distracted to the objects that look similar to the target. 
% For example, in the last column of \Fref{fig:sps_tp} where the task is to find a knife, HAT is the only method that correctly predicted the human fixations on the metallic object (above the pizza) as knifes are often metallic, whereas other methods either missed the target or ignored the distractors. This shows the capability of HAT in modeling human attention control in visual search which is constrained by the low-resolutional peripheral vision. \Fref{fig:sps_ta} visualizes the TA scanapths. One can observe that HAT learns to leverage the context cues in predicting target-absent fixations: in the third column of \Fref{fig:sps_ta} where the search target is microwave, HAT correctly predicted the fixations on the counter-top and table, where microwaves are often found.
% Similarly for the free-viewing scanpaths as shown in \Fref{fig:sps_fv}, HAT also predicts the most human-alike scanpaths among all methods in (1) the fixation locations (where), (2) the semantics (what), and (3) the order (when) of the fixations. This demonstrates that HAT is able to capture the all three aspects (what, where and when) of human fixations. 
