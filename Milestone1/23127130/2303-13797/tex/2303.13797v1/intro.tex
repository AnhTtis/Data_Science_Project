\section{Introduction}
\label{intro}


\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{fig/fig2.pdf}
  \caption{Overview of \ourmodel. The unsupervised personalization phase is at the core of the proposed framework.
  }
  \vspace{-5pt}
  \label{fig:overview}
\end{figure*}

Task-oriented dialog systems 
%(e.g.,~Amazon Alexa)
% FJ through --> using below
provide users with the ability to carry out tasks, such as reserving a table at a restaurant, using natural language~\cite{wen2017network}.
%Recently, researchers have increasingly focused on training end-to-end task-oriented dialog systems, as opposed to the pipeline approach~\cite{hosseini2020simple,bordes2016learning}.
% FJ Add references after '... pipeline approach' below
Contrary to the pipeline approach~\cite{chen2017survey}, researchers have increasingly focused on training end-to-end task-oriented dialog systems recently~\cite{hosseini2020simple,bordes2016learning}.
Such models generate responses exclusively based on the task-specific context of the dialog. Consequently, these models fail to adapt their responses to the diverse user personalities~\cite{joshi2017personalization}.
% FJ consider removing 'according' above
% FJ consider removing 'current' below, it's redundant
Specifically, state-of-the-art task-oriented dialog systems struggle to \myNum{i} adapt their conversation flows according to the active user’s personality, \myNum{ii} adjust their linguistic style, and \myNum{iii} handle ambiguities~\cite{herzig2017neural}.
% 
In addition to presenting the choices to the user in an arbitrary or sequential order without taking the personality of the active user into account, task-oriented dialog systems use only task-specific, dull language.
% 
It has been shown that adapting to the interlocutor improves communication efficiency~\cite{brown1965work,brown1987theory,kroger1992rules}.
% 
Personalized task-oriented dialog systems can leverage profile information to expedite the interaction by understanding user’s actual information needs promptly, generate tailored responses by adapting linguistic variations, and properly address ambiguities by contextualizing nuanced queries -- a step towards delivering more human-like interactions~\cite{mo2018personalizing,yang2018investigating}. 
Personalizing task-oriented dialog systems without compromising the task completion accuracy is the focus of this work.


Earlier works use pre-training user profiles for intermediate supervision, as well as memory networks with copy mechanisms~\cite{qian2017assigning,herzig2017neural}.
The authors in~\cite{joshi2017personalization,luo2019learning} encode user information and conversation history using memory networks in an end-to-end fashion.
To synthesize personalized responses, \cite{zhang2019learning} utilized dynamic and static attention mechanisms in the end-to-end memory network.
For each user profile, these works require enormous amounts of labeled training data, which is time-consuming, expensive, and nearly impossible to acquire.
Recently, pre-trained language models have shown zero-shot capabilities in the natural language understanding and natural language generation tasks~\cite{du2021glam,brown2020language}, which suggests the possibility of developing personalized task-oriented dialog systems without
%the need for 
requiring labeled training data for each target user profile.
However, successfully exploiting the users' profiles and synthesizing personalized responses with no (or few) labeled training examples is a demanding task.

We introduce a novel framework for building \textbf{P}ersonalized \textbf{T}ask-\textbf{o}riented \textbf{D}ialog Systems, {\ourmodel}, that leverages the
%power of 
pre-trained language models (LMs), zero-shot (as well as few-shot) learning, and deep reinforcement learning.
% FJ Consider using {\ourmode} instead of 'the proposed framework' below and elsewhere
Guided by the proximal policy optimization (PPO) algorithm~\cite{schulman2017proximal,baselines} and a zero-shot generalizable reward function, the proposed framework
%FJ Consider '... and make them adapt to ...' instead of '... are capable of adapting to ...'
can personalize task-oriented dialog systems
%that are capable of adapting 
to diverse user profiles in an unsupervised fashion.
Figure~\ref{fig:overview} presents an overview of the framework that works in three phases and uses a pre-trained GPT-2~\cite{radford2019language} as a backbone model.
A task-specific training 
(e.g., reserving a table)
is performed in the first phase. Task-specific training datasets are generally available for a wide range of tasks in many domains~\cite{lee2021sgd,zang-etal-2020-multiwoz}, whereas personalized counterparts are 
%more or less 
practically
impossible to obtain. 
To overcome this challenge, we employ the unsupervised personalization phase. 
The deep reinforcement learning-based phase initializes a personalized GPT model from the task-specific GPT model (i.e., trained in phase one).
Then, it trains personalized GPT model based on \myNum{i} the appropriateness of the generated response for the given user profile, quantified by the zero-shot generalizable reward function; and \myNum{ii} fidelity of the response to the task, measured by the KL divergence between the responses generated by the task-specific and personalized models. Using the above signals, the PPO algorithm is employed to perform policy gradients.


We also propose a new reward function that allows quantifying the quality of the generated personalized responses not only for previously seen user profiles, but also for newly emerging unseen profiles. 
The zero-shot generalizable reward function uses pre-trained sentence transformers and contrastive representation learning to score the suitability of the response for the active user profile. To the best of our knowledge, this is the \emph{first work} that can  adapt the responses of task-oriented dialog systems to diverse user profiles in an unsupervised fashion.
To further improve the performance of the personalized task-oriented dialog systems, an \emph{optional} few-shot fine-tuning phase is introduced. This phase uses a few labeled training examples to adjust the responses for the given user profile, that can be employed or skipped depending on the availability of the labeled training data. 
Moreover, the number of shots can also be adjusted depending on the quantity of the available training examples.  

We perform thorough experimental evaluations on the 
only publicly available benchmark, 
\myspecial{personalized bAbI dialogue}
benchmark, for five tasks and up to 180 distinct user profiles in the restaurant domain.
The experimental results show that our proposed framework outperforms state-of-the-art supervised personalization models, even when given access to zero labeled training instances (i.e., few-shot fine-tuning phase is skipped). 
We also demonstrate that the proposed personalization approach achieves a competitive performance when compared to a strong supervised GPT-2 baseline model on the BLEU-4 and ROUGE-2 measures. 
% FJ Consider 'Furthermore, our human study confirms that {\ourmodel} is superior to existing supervised approaches.' instead of below.
Furthermore, the human study confirms the competitiveness of our unsupervised personalization framework to the other supervised approaches.

This work's contributions are summarized below:
\begin{itemize}[leftmargin=1.2\parindent,labelindent=-1pt, itemsep=-1pt]

    \item We propose an end-to-end framework for personalizing task-oriented dialog systems in an unsupervised way. To the best of our knowledge, this is the first work that has the unsupervised personalization capabilities.
    
    \item We introduce a zero-shot generalizable reward function that can guide the policy of the personalized task-oriented dialog systems to generate rich and personalized responses even for the unseen user profiles.
    
    \item We perform extensive experimental analysis using \myspecial{personalized bAbI dialogue} dataset and show that our framework consistently outperforms state-of-the-art supervised personalization models for up to $180$ unique user profiles on five tasks.
\end{itemize}