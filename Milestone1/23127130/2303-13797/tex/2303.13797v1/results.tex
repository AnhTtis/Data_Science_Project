\section{Results}
\label{results}

\begin{table*}[t!]
\caption{BLEU scores and ROUGE scores for personalization for all five tasks.}
\label{tab:bleu}
\centering
\begin{tabular}{llcc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Approach}} & \multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c|}{\textbf{Task 1}} & \multicolumn{2}{c|}{\textbf{Task 2}} & \multicolumn{2}{c|}{\textbf{Task 3}} & \multicolumn{2}{c|}{\textbf{Task 4}} & \multicolumn{2}{c}{\textbf{Task 5}} \\ \cline{3-12} 
                                   &                                  & BLEU           & ROUGE           & BLEU           & ROUGE           & BLEU           & ROUGE           & BLEU           & ROUGE           & BLEU           & ROUGE          \\ \hline
\multirow{6}{*}{Supervised}          & Mem2Seq-org                      & 60.12            & 64.82             & 65.54            & 69.83             & 57.74            & 62.73             & 59.07            & 63.32             & 64.23            & 59.39            \\
                                   & Mem2Seq-split                    & 60.30            & 63.82             & 64.92            & 68.60             & 58.07            & 62.43             & 59.20            & 63.03             & 64.11            & 58.73            \\
                                   & Mem2Seq-att                      & 62.26            & 71.17             & 67.15            & 75.84             & 59.84            & 69.59             & 61.29            & 69.74             & 66.02            & 66.17            \\
                                   & GLMP                             & 61.25            & 70.81             & 66.40            & 75.46             & 59.07            & 68.93             & 59.66            & 70.13             & 64.91            & 65.74            \\
                                   & CoMemNN                          & 68.67            & 77.71             & 73.83            & 82.67             & 65.77            & 75.72             & 67.58            & 76.85             & 72.23            & 72.53            \\
                                   & Supervised-GPT                   & \textbf{75.71}            & \underline{78.42}             & \textbf{80.61}            & \textbf{83.38}             & \underline{73.21}            & \textbf{76.46}             & \textbf{74.64}            & \underline{77.11}             & \textbf{80.01}            & \underline{73.61}            \\ \hline
Unsupervised           & PToD-0  (This work)                         & 70.84            & 75.02             & 75.75            & 79.85             & 68.44            & 72.93             & 69.72            & 73.69             & 75.12            & 70.21            \\
\hline
\multirow{2}{*}{Few-shot}          & Few-shot GPT                      & 40.21            & 46.71             & 33.17            & 39.32             & 27.17            & 22.78             & 39.20            & 33.25             & 24.12            & 29.31            \\
       & {\ourmodel}   (This work)                          & \underline{75.64}            & \textbf{78.46}             & \underline{80.55}            & \underline{83.29}             & \textbf{73.24}            & \underline{76.37}             & \underline{74.52}            & \textbf{77.13}             & \underline{79.92}            & \textbf{73.65}    \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\end{table*}

In this section, we present quantitative as well as qualitative analysis. We first present results on the task completion and then demonstrate that our proposed framework consistently outperforms SOTA supervised personalization models for the personalization task.


\subsection{Quantitative Analysis}
\label{results-quantitative}
\stitle{Task Completion.}
Despite the fact that the core task in this work is personalization, the personalized models should not compromise the accuracy of task completion for adapting their behaviors for the profiles of the users. 
Keeping it in mind, we report the results for task completion in Tables~\ref{tab:f1} that presents F1 scores for all five tasks for all the competing models.
In terms of task completion, all the models show competitive performance except MemNN-split. 
The main reason for all the models showing great performance for task completion is that the user never drops out of the conversation, even if the system keeps providing the user with unwanted recommendations or never adapts the linguistic style according to the user. 
Since, the system eventually completes the task (i.e., the user is too patient which is not the case in the real-world), the F1 score is high for all the competing models. 
Though, the margin is not big, the best models are supervised-GPT and {\ourmodel} (i.e., this work). For example, on tasks one and three, the proposed {\ourmodel} performs the best, and on the remaining three tasks, supervised-GPT shows the best performance. 

It is critical to emphasize that the proposed {\ourmodel} was trained using only 20 labeled training examples in phase three, whereas the supervised-GPT was trained on the complete training set. 
Moreover, we observe that PToD-0 variant (i.e., that was not trained in phase three) has comparable performance when compared to the SOTA personalization models. Last but not least, the few-shot GPT (that skipped phase two training and used only 20 training examples in phase three) baseline does not show good performance for task five as compared to other models.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.98\linewidth]{fig/shots.pdf}
  \caption{Performance of the {\ourmodel} for different number of shots for all five tasks.
  }
  \label{fig:shots}
  \vspace{-5pt}
\end{figure}


\stitle{Personalization.}
Table~\ref{tab:bleu} presents BLEU-4 and ROUGE-2 scores for all the competing models on all five tasks. For all the tasks, the proposed {\ourmodel} achieves the best performance or insignificant performance difference from supervised-GPT baseline. 
Excluding supervised-GPT model, the proposed {\ourmodel} outperforms all other SOTA response generation methods by at least 19.95\% on BLEU-4 and 9.74\% on ROUGE-2 metrics.
Similarly, the other variant PToD-0 that was not trained on any labeled training examples, still outperforms all the competing models including CoMemNN (which is a response selection model) for BLEU score. 
Since CoMemNN does not generate responses, it has advantage to get better BLEU and ROUGE scores as compared to the response generation approaches. 
Moreover, the few-shot GPT baseline shows the worst performance, since it was trained with only 20 labeled examples in the phase three and phase two (i.e., unsupervised personalization) was skipped. The poor performance of the few-shot GPT baseline highlights the critical role of the phase two.


Figure~\ref{fig:shots} presents the performance of the proposed personalization framework, when provided with different number of training examples in phase three. 
Generally, we notice that as the number of training examples are increased, the performance improves, which highlights the importance of the supervision. However, we noticed that the performance does not get much better beyond 20 examples. That is almost the point, when {\ourmodel} is as good as supervised-GPT model (i.e., trained on full training set). 


\begin{figure}[t!]
  \centering
  \includegraphics[width=0.98\linewidth]{fig/all_reward.pdf}
  \caption{Mean reward across unsupervised personalization phase for all five tasks.
  }
  \label{fig:reward}
  \vspace{-5pt}
\end{figure}


The unsupervised personalization phase is at the core of the proposed framework, we provide more details about it in Figure~\ref{fig:reward}. 
Since all five tasks vary in terms of difficulty, we present the mean reward of the models for each task, as the training progresses in phase two. 
The general trend is that the mean reward starts at 0 (e.g., at episode 0), which is obvious because the responses at the beginning of this phase were not tailored for the given user profile. 
Then, depending on the difficulty of the task, we notice that the respective models start approaching to 1.0 (e.g., after 100,000 episodes). 
We know that the task five (i.e., conduct full personalized dialog) is the most challenging task and the mean reward throughout the training process also signifies that. Similarly, we also notice that the tasks that involve adapting only linguistic styles (e.g., task two), the respective models start to achieve higher mean reward quickly as compared to the tasks that require meaningful recommendations or need to resolve nuances (e.g., task three). 

\begin{table}[t!]
\caption{Average scores of the user study.}
\label{tab:user_study}
\centering
\begin{tabular}{llll}
\toprule
Method         & Fluent & Appropriate & Rank \\
\hline
Reference Response   & 4.92   & 4.87        & 2.41 \\
Supervised GPT & 4.93   & 4.85        & 2.52 \\
PToD-0 (This work)        & 4.91   & 4.86        & 2.62 \\
{\ourmodel}  (This work)          & 4.92   & 4.85        & 2.45 \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\end{table}


\subsection{Qualitative Analysis}
\label{results-qualitative}
In this experiment, we randomly selected 300 responses generated by supervised-GPT (i.e., the best model among the supervised competitors), PToD-0 (i.e., used zero labeled training examples), and {\ourmodel} (i.e., used 20 labeled training examples) along with the reference responses and asked human annotators to rate them (i.e., 1 to 5, 5 being the best) for fluency and appropriateness of the response for the given user profile. 
Moreover, we also asked the annotators to rank the responses for personalization to the given user profile. Each response was rated by three annotators. Table~\ref{tab:user_study} presents average scores for fluency, appropriateness of the response, and average rank among the responses. 
All the models (including reference) achieve high scores on the fluency and appropriateness of the response for the given user profile. 
Moreover, there is not a significant difference among the average scores. Similarly, almost all were ranked similar as reference responses. 
For example, responses generated from every model are ranked at all the places, i.e., $1^{st}$ to $4^{th}$ place. 
In summary, results from human study show that the responses of all the models are as a good as reference responses. 
It is important to remind that the supervised-GPT was trained on the full training set, whereas our proposed PToD-0 and {\ourmodel} were trained using zero and 20 labeled training examples, respectively.

We also observe that the PToD-0 model had slightly lower BLEU and ROUGE scores as compared to {\ourmodel} and supervised-GPT, whereas in the human study it showed equally outstanding performance. Upon further investigation, we noticed that the responses generated by the PToD-0 are identical to that of supervised-GPT and {\ourmodel}. The PToD-0 model did not use the ``words'' (or n-grams) in the reference responses. For example, a perfectly acceptable response generated by PToD-0, ``What should the price be, madam?'' did not get good BLEU or ROUGE scores, because the reference response happened to be, ``Madam! which price range are you looking for?''.  
