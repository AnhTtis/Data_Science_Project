\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.96\linewidth]{fig/zs.pdf}
  \caption{Overview of the training and inference process for the zero-shot generalizable reward function.
  }
  \vspace{-10pt}
  \label{fig:zs}
\end{figure*}

\section{Personalization Framework: \ourmodel}
\label{model}
This work presents a new framework for developing personalized dialog systems that works in three phases. 
A pre-trained GPT-2 model serves as the backbone model for the framework.
In the first phase, the base GPT-2 model is optimized via task-specific training.  
The phase two, referred to as unsupervised personalization phase, employs deep reinforcement learning to adapt the system responses to a wide range of user profiles guided by the zero-shot generalizable reward function (i.e., presented in Figure~\ref{fig:zs}) and the trained task-specific GPT model. 
The \emph{optional} phase three fine-tunes the personalized GPT model using a few supervised training examples to further improve the performance. Figure~\ref{fig:overview} summarizes the proposed unsupervised personalization framework.


\subsection{Phase One: Task-specific Training}
\label{sec:phaseone}

We leverage the power of the pre-trained language models by intializing the phase one of our framework with a pre-trained GPT-2 model. The details of the pre-trained model are as follows. 
The model~\cite{radford2019language} was pre-trained on the WebText dataset and has 774 million parameters. 
Using byte pair encoding, the vocabulary size is 50,257 tokens; capitalization and punctuation were preserved~\cite{sennrich2015neural}. 
The model is built on the transformer's decoder stack~\cite{vaswani2017attention}, and it has 36 layers, 20 heads, and an embedding size of 1280.
The task-specific training of the model is performed using causal language modeling (see Section ~\ref{sec:lm} for details). 
Figure~\ref{fig:task} presents the task-specific training of the model.
Given a dialog context $\bigC_\smallt$, user's current utterance $\bigU_\smallt$, and (optional) knowledge base search result tuples $\bigK$ at turn $\smallt$, the probability of system's response $\bigS_\smallt$ with length $\smalln$ can be defined as:
$$
p(\bigS_\smallt | \bigC_\smallt, \bigU_\smallt, \bigK) = \prod_{i=1}^\smalln p(\smalls_\smalli | \smalls_{<\smalli}, \bigC_\smallt, \bigU_\smallt, \bigK)
$$

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.96\linewidth]{fig/task_model.pdf}
  \caption{The task-specific training of the GPT-2 model.
  }
  \vspace{-12pt}
  \label{fig:task}
\end{figure}

We train the model by calculating the cross-entropy loss by maximizing the log-likelihood of the system response conditioned on the dialog context, user's input, and knowledge base tuples. If the task does not require interaction with the knowledge base, the search query is not performed nor the generation is conditioned on the resultant tuples.
%from the knowledge base. 
The output of phase one is the trained task-specific GPT model. %It is important to highlight that this trained model cannot (automatically) adapt to different user profiles,  in its current state.  

\subsection{Phase Two: Unsupervised Personalization}
\label{sec:phasetwo}
This phase initializes the personalized GPT model with the trained task-specific GPT model (i.e., output of phase one). 
The personalized GPT model is trained for personalization in the unsupervised way. 
The two critical training signals are provided by \myNum{i} the zero-shot generalizable reward function that quantifies whether the output of the personalized model is apprpriate for the given user profile; and \myNum{ii} the KL divergence between the personalized and task-specific model's distributions to ensure that the output of the personalized model does not deviate too much from the task-specific model (i.e., it still accomplishes the task with high accuracy). 
%

In the following, we describe the details of the novel reward function and KL divergence. Then, we detail the training process for the unsupervised personalization phase.


\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.96\linewidth]{fig/model.pdf}
  \caption{Phase two of the framework: Unsupervised Personalization.
  }
  \label{fig:personalization}
  \vspace{-5pt}
\end{figure*}


\stitle{Zero-shot Generalizable Reward Function.}
The zero-shot generalization is enabled by the unsupervised representations provided by the powerful pre-trained language model MPNet and the contrastive loss function~\cite{hadsell2006dimensionality}. 
The training and inference process of the reward function is shown in Figure~\ref{fig:zs}.
At a dialog turn $\smallt$, we concatenate the dialog context $\bigC_\smallt$, user's current input $\bigU_\smallt$, the (optional) knowledge base search result tuples $\bigK$, and the system's response $\bigS^{\smalli}_\smallt$ for the user $\smalli$ and acquire their representation $\bigH^{\smalli}_\smallt$. Similarly, we encode the user profile information $\bigP_\smallj$ for the user $\smallj$ to get a corresponding representation $\bigU^\smallj$.
If a pair of encodings had a positive corresponding label (i.e., the system response is appropriate for the given user profile), then the contrastive loss function would reduce their distance, and if a negative label were given, it would increase their distance. We generate positive training examples by setting $\smalli == \smallj$ and negative examples are generated by setting $\smalli \neq \smallj$. The training loss can be defined as:
$$
\mathcal{L}_{\smalli,\smallj}
  =-\log{
  \frac{\text{exp}\left(\bigH^{\smalli}_\smallt \bigcdot \bigU^\smallj /\tau\right)}{\sum\limits_{\smallq \in \bigQ }\text{exp}\left(\bigH^{\smalli}_\smallt \bigcdot  \bigU^\smallq / \tau\right)}
  }
$$
where the $\bigcdot$ represents the scoring function, $\tau\in\mathcal{R}^+$ is a scalar parameter for temperature, and $\bigQ$ is the set of negative pairs, i.e., $\smalli \neq \smallj$.
To train a classifier that works in the zero-shot setting, we select a subset of user profiles (i.e., \emph{seen} profiles) and use them to train the classifier. 
The pre-trained MPNet has the capability to generate rich, accurate, and high-quality embeddings even for the \emph{unseen} user profiles or unseen knowledge base entries, since both the user profile and knowledge base tuples are described using natural language. 
For example, the model can produce precise embeddings for an unseen user profile who prefers ``kosher'' food, because it has already learned the contextual usage of a large number of words (e.g., MPNet has a vocabulary size of 30,527) in the pre-training process.
The scoring function learns to score close to one, the matching pairs (i.e., the system response is appropriate for the given profile), and zero otherwise.

Our zero-shot generalizable reward function follows the Sentence-BERT~\cite{reimers2019sentence} that employs siamese and triplet network structures~\cite{schroff2015facenet}, leverages contrastive loss, and dot product is used as the scoring function. 
To generate input encoding, we use the pre-trained \myspecial{all-mpnet-base-v2} that has been trained on over one billion training pairs and produces 768 dimensional normalized embeddings for the input by \myspecial{mean pooling}.
For every positive training pair, two negative training examples are generated.
At inference time, the trained zero-shot generalizable reward function provides a scalar reward, $\smallr \in $~[0,1] that quantifies the suitability of the system's responses for both previously seen and newly emerging unseen user profiles.
%

\stitle{KL Divergence.}
To ensure that the personalized policy does not diverge too much from the trained task-specific model, we use an additional reward signal by calculating the KL divergence between the personalized policy and the task-specific policy (i.e., the model trained in phase one).
That is, keeping close to the task-specific model is rewarded, whereas big KL divergences are penalized.
We denote the distributions of the task-specific and personalized models by $p_{\one}$ and $p_{\two}$, respectively. At dialog turn $\smallt$, the KL divergence can be calculated as:
$$
KL =
\mathbb{E}_{\bigS^\smalli_\smallt \sim p_{\two}} [
\log
p_{\two}(\bigS^\smalli_\smallt | \bigP^\smalli, \bigC_\smallt, \bigU_\smallt, \bigK)
- \log
p_{\one}(\bigS_\smallt | \bigC_\smallt, \bigU_\smallt, \bigK)
]
$$
where $\bigS_\smallt$ is the task-specific response and $\bigS^\smalli_\smallt$ is the system's response adapted for the user $\smalli$. The final $reward$ can be combined as given below:
$$
reward = \smallr + \beta \times KL
$$
where $\beta \in [0, -1]$ is the penalty coefficient and decides the weight of the KL divergence. We use
adaptive KL Penalty coefficient and
initialize $\beta = -0.2 $ in our experiments .


\stitle{Training Details.}
To start with the unsupervised personalization phase, we initialize our personalized model $p_\two$ = $p_\one$ and then adapt $p_\two$ to synthesize the personalized responses for a wide range of user profiles using deep reinforcement learning.
The personalized model is fine-tuned via PPO algortihm from~\cite{baselines} with the final $reward$ (i.e., a combination of KL divergence and a score from zero-shot generalizable reward function). 
The expected reward for a response $\bigS^\smalli_\smallt$ for the user $\smalli$ at a dialog turn $\smallt$ can be written as:
$$
\mathbb{E}_{p_\two}[reward] = \mathbb{E}_{\bigU_\smallt \sim \omega, \bigS^\smalli_\smallt \sim p_\two(\cdot | \bigP^\smalli, \bigC_\smallt, \bigU_\smallt, \bigK)} [reward(\bigP^\smalli, \bigS^\smalli_\smallt)]
$$
where $\omega$ represents a given task, the model $p_\two$ is being trained for. The personalized model is trained for up to 600,000 episodes using Adam optimizer~\cite{kingma2014adam} with a learning rate of $1.41 \times 10^{-5}$.

The output of this phase is a personalized model that can generate responses that are not only specific to the task, but are also adapted for the given user profile. It is important to recall that the unsupervised personalization phase does not use any personalized variants of the responses for training the model. It is exclusively trained in the unsupervised setting, guided by the zero-shot generalizable reward function and KL divergence between the distributions of the task-specific and personalized models.


\begin{table*}[t!]
\centering
\caption{Datasets statistics.}
\label{tab:dataset}
\begin{tabular}{llccccc}
\toprule
\multicolumn{2}{l}{\textbf{Dataset} }           & \multicolumn{1}{c}{\textbf{Task 1}} & \multicolumn{1}{c}{\textbf{Task 2}} & \multicolumn{1}{c}{\textbf{Task 3}}  & \multicolumn{1}{c}{\textbf{Task 4}} & \multicolumn{1}{c}{\textbf{Task 5} } \\ \hline
\multirow{2}{*}{bAbI dialogue}              & Number of dialogs         & \multicolumn{1}{c}{4000}                                 & \multicolumn{1}{c}{4000}                                 & \multicolumn{1}{c}{4000}                                 & \multicolumn{1}{c}{4000}                                 & \multicolumn{1}{c}{4000}                                 \\
%                                           & \# Training Dialogs & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 \\
%                                           & \# Validation Dialogs & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 \\
%                                           & \# Test Dialogs & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 \\
%                                           & \# Test-OOV Dialogs & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 & \multicolumn{1}{c}{1000}                                 \\
                                           & Avg. dialog turns & \multicolumn{1}{c}{6.0}                                 & \multicolumn{1}{c}{9.5}                                 & \multicolumn{1}{c}{9.9}                                 & \multicolumn{1}{c}{3.5}                                 & \multicolumn{1}{c}{18.4}                                 \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Personalized\\  bAbI dialogue\end{tabular}} & Number of dialogs         &  24000                                                    &    24000                                                  &    48000                                                  &     24000                                                 &    48000                                                  \\
%& \# Training Dialogs & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}  \\
%& \# Validation Dialogs & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}  \\
%& \# Test Dialogs & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}  \\
%& \# Test-OOV Dialogs & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}                                 & \multicolumn{1}{c}{6000}                                 & \multicolumn{1}{c}{12000}  \\
                                            & Avg. dialog turns &   6.0                                                   &   9.5                                                   &  11.8                                                    &     3.5                                                 &   20.3                                                   \\
                                            & Number of user profiles        &6                                                      &6                                                      &    180                                                  &   6                                                   & 180 \\
                                            & 
                                            Avg. dialogs per profile        &   4000                                                   &   4000                                                   &   267                                                   &    4000                                                  &  267                         \\ \bottomrule
\end{tabular}
\vspace{-9pt}
\end{table*}

\subsection{Phase Three: Few-shot Fine-tuning}
The optional phase three uses a few labeled training examples to calibrate the personalized model (i.e., trained in phase two in the unsupervised setting) for the given user profile in the supervised setting. 
The probability for system's response $\bigS^\smallj_\smallt$ with length $\smalln$, for a given user $\smallj$, at dialog turn $\smallt$ can be defined as:
$$
p(\bigS^\smallj_\smallt | \bigP^\smallj, \bigC_\smallt, \bigU_\smallt, \bigK) = \prod_{i=1}^\smalln p(\smalls_\smalli | \smalls_{<\smalli}, \bigP^\smallj, \bigC_\smallt, \bigU_\smallt, \bigK)
$$

We call this phase \emph{optional}, since it can be employed or skipped based on the availability of the labeled variants for the given user profile.
Moreover, the number of shots can also be adjusted depending on the quantity of the available training examples. In our experiments, we present results with the following number of shots: 0 (i.e., we skip this phase), 1, 5, 10, and 20. 


