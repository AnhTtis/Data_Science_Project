\section{Preliminaries}
\label{problem}
\subsection{Problem Formulation}
In a multi-turn task-oriented dialogue, 
$\bigU_\smallt$ is an input from the user and $\bigS_{\smallt}$ is a system's response 
at a turn \smallt. 
To generate a response $\bigS_{\smallt}$, all previous turns are concatenated to prepare dialog context $\bigC_\smallt = [\bigU_0, \bigS_0, \cdots,  \bigU_{\smallt-1}, \bigS_{\smallt-1}]$ and passed to the system as input along with the user’s current input $\bigU_\smallt$.
In a personalized task-oriented dialog system, at turn \smallt, the goal is to synthesize a response $\bigS^{\smalli}_\smallt$ adapted for a user profile $\bigP^\smalli \in \bigP = \{\bigP^0, \bigP^1, \cdots\}$.
The system’s response $\bigS^{\smalli}_\smallt$ is generated by conditioning on dialog context $\bigC_\smallt$, user’s current utterance $\bigU_\smallt$, and profile information $\bigP_\smalli$ for user $\smalli$, concatenated as a single sequence.
$$
\bigS^{\smalli}_\smallt = \ptod ([\bigP^\smalli ;  \bigC_\smallt ; \bigU_\smallt])
$$
In traditional (i.e., supervised) personalized task-oriented dialog systems, at turn \smallt, we are given $\smallm$ variants of the system response adapted for each user as: 
$ \{(\bigU_\smallt, \bigS^{\smalli}_\smallt)\}_{i=1}^\smallm$ for all $\smallm$ user profiles to train the models. 
The major disadvantage of such an approach is the unscalable requirement of having a large number of labeled training examples for each user profile; such data acquisition is expensive and time-consuming.
To overcome this challenge, we assume that, at turn $\smallt$, profile-specific response $\bigS^{\smalli}_\smallt $ $\forall \smalli$ is not available for model's supervision (i.e., unsupervised personalization). 
To allow for handling up to $\infty$ user profiles, we assume that the user profile is described via natural language text, in contrast to previous works that encode the features of the user profile via one-hot encoding and limits the model's expansion to new profile features. Naturally, describing user profiles using natural language takes care of the case where only partial information about a user profile is available.
Moreover, some tasks require interaction with knowledge base, we define the knowledge base tuples as $\bigK = [\smallk_1, \smallk_2, \cdots , \smallk_{\smalll}]$, where each tuple $\smallk_\smallb$ is defined using natural language and passed as additional input to the model where needed.


\subsection{Pre-trained Language Models}
\label{sec:lm}
% self-supervisedly
The Language models (e.g., GPT-2~\cite{radford2019language}, BERT~\cite{devlin2018bert}) are  trained utilizing massive amounts of text data in the unsupervised way. Since these models have millions of parameters, they have the capability to effectively capture both general semantic and syntactic information.
In this work, we utilize the pre-trained GPT-2 and MPNet~\cite{song2020mpnet} models.
We use GPT-2 as a base model, perform task-specific training, and then further train the model to synthesize personalized responses in an supervised way, guided by the novel reward function. 
The GPT-2 model has achieved state-of-the-art performance on many natural language generation benchmarks including conversation question answering~\cite{reddy2019coqa}, text summarization~\cite{nallapati2016abstractive}, and machine translation~\cite{lample2017unsupervised}, among others. 

We train a zero-shot generalizable reward function to score the acceptability of the generated responses for the given user profile using a contrastive loss function. 
The novel reward function uses 
%pre-trained sentence-Bert~\cite{reimers2019sentence} to acquire semantically accurate sentence embeddings. 
%We use the 
pre-trained MPNet~\cite{song2020mpnet} as a basic building block to acquire semantically accurate embeddings.
%in sentence-Bert.
%The sentence-Bert uses BERT~\cite{devlin2018bert} as a basic building block.
%The sentence-Bert model has produced cutting-edge results on semantic textual
%similarity benchmarks, such as sentiment prediction~\cite{pang2005seeing,socher2013recursive}, opinion polarity~\cite{wiebe2005annotating}, and paraphrase detection~\cite{dolan2004unsupervised}.
The MPNet model has produced cutting-edge results on several natural language processing tasks including GLUE~\cite{wang2018glue}, SQuAD~\cite{rajpurkar2016squad,rajpurkar2018know}, RACE~\cite{lai2017race}, and sentiment prediction~\cite{maas2011learning} benchmarks.
%
%In the following, we provide a brief overview of the causal language modeling (i.e., employed by GPT-2), masked language modeling, and next sentence prediction (i.e., used by BERT).
%
In the following, we provide a brief overview of the GPT-2 and MPNet models.


%\stitle{Causal Language Modeling.} 
\stitle{GPT-2.} 
The GPT-2 model is pre-trained for autoregressive generation (i.e., predicting the next word) on the WebText dataset (i.e., 40 GB of text) and adapts a transformer-based neural architecture~\cite{vaswani2017attention}.
Suppose we have a natural language sequence  $(\smalls_1, \cdots , \smalls_\smalln)$ where symbol $\smalls_\smalli$ is drawn from a fixed set of symbols. The sequential ordering of language leads to factorizing the joint probabilities over symbols as a product of conditional probabilities~\cite{bengio2003neural}, as given below.
$$
p(\smalls) = \prod_{i=1}^\smalln p(\smalls_\smalli | \smalls_1, \cdots ,\smalls_{\smalli-1})
$$
Using this approach, it is possible to estimate $p(\smalls)$ and any conditionals of the form 
$p(\smalls_{\smalli - k}, \cdots , \smalls_\smalli | \smalls_1, \cdots , \smalls_{\smalli - k - 1} )$, and perform tractable sampling. 
%Accordingly, the GPT-2 model is trained with parameters $\theta$ to minimize the negative log-likelihood over the entire dataset:
%$$
%- \sum_{i=1}^{n} \log {p_{\theta}(\smalls_{\smalli} | \smalls_1, \cdots ,\smalls_{\smalli-1})} 
%$$
  
\stitle{MPNet.} 
BERT does not account for interdependence among predicted tokens, whereas complete position information is not used by XLNet~\cite{yang2019xlnet}, though dependency among tokens is considered. 
The MPNet model exploits the benefits of masked language modeling (MLM) (i.e., employed by BERT) and permuted language modeling (PLM) (i.e., used by XLNet) and eliminates their shortcomings.
It brings out the best of both worlds: by using PLM, it exploits the predicted token's dependencies, and, at the same time, uses the full position information of a sentence from MLM to enable a full view of the sentence.
It has been pre-trained on BooksCorpus~\cite{zhu2015aligning}, OpenWebText, CC-News, Stories~\cite{trinh2018simple}, and Wikipedia (i.e., over 160GB data).
For a given sequence $(\smalls_1, \cdots, \smalls_\smalln)$, where permutations of set $\{1, \cdots, \smalln\}$ is represented by $\mathcal{Z}_\smalln$, 
the $\smallt$-th element of $z$ by $z_\smallt$,
the first $t-1$ element of $z$ by $z_{<t}$,
the number of non-predicted tokens by $c$, and
the mask tokens $\rm{[M]}$ in position $z_{>c}$ by $M_{z_{>c}}$.
The MPNet is trained for the following objective:
$$
\mathbb{E}_{z \in \mathcal{Z}_n} \sum\limits_{t=c+1}^{n} \log p(\smalls_{z_\smallt}|\smalls_{z_{<\smallt}}, M_{z_{>c}};\theta)
$$


\subsection{Reinforcement Learning Paradigm}
The reinforcement learning paradigm has been extensively studied for unsupervised learning.
Methods that use policy gradients compute an estimator of the gradient and then plug it into a stochastic gradient ascent algorithm.
It is common to optimize the policy $\pi$ by maximizing the expected reward $r \in \realR$ for the generated sequence $\bigY = (y_1, \cdots, y_n)$ with length $n$, given the input sequence $\bigX = (x_1, \cdots, x_m) $ with length $m$, that is sampled from data distribution $\bigD$. We can optimize the expected reward as follows:
$$
\mathbb{E}_{\pi}[\smallr] = \mathbb{E}_{x \sim \bigD, y \sim \pi(\cdot | x)} [r(x, y)]
$$

%
The PPO algorithm introduced clipped surrogate objective, in addition to, the penalty on the KL divergence.
The objective function is modified using the KL divergence penalty, instead of making it a hard constraint like in the trust region policy optimization algorithms~\cite{schulman2015trust}. The PPO updates its policy, at step $k$ via:
$$
\theta_{k+1} = \arg\max_\theta \mathbb{E}_{s,a \sim \pi_{\theta_k}} [ \mathcal{L} (s, a, \theta_k, \theta)]
$$ 
where $s$ and $a$ represent the state and action, respectively.
In this work, we employ PPO algorithm~\cite{baselines} to perform policy gradients, that has been shown to be scalable (e.g., for large language models), data-efficient, and robust (i.e., without excessive hyperparameter tuning)~\cite{bellemare2013arcade}. 



