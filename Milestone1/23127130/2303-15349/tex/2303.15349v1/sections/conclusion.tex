\section{Conclusion}
We introduced Information Maximizing Curriculum (IMC), a novel approach to learning mixture of experts models (MoE). IMC is a curriculum-based approach that allows each expert to select its own subset of the training data for learning. The curriculum allows the MoE model to automatically ignore data points that it can not represent, which reduces the susceptibility to local optima and in particular to mode-averaging, a common problem associated with maximum likelihood-based optimization for multimodal density estimation. The maximization of the entropy of the joint curriculum of all experts incentivizes the MoE to cover all data samples.
IMC is able to adapt the model complexity online by adding more experts during training which is enabled by the proposed objective. We motivated our objective for a single expert and generalized it to the MoE case. 
We showed that our method is able outperform existing optimization schemes for MoE and state-of-the art generative models on challenging multimodal conditional density estimation problems. In particular, we employed behavior learning tasks to show that IMC is able \textit{i)} avoid mode averaging and \textit{ii)} extract all modes present in the data distribution.
 % The maximization of the entropy of the joint curriculum of all experts incentivizes the MoE to cover the all data samples. We showed that our method is able outperform existing optimization schemes for MoE and state-of-the art generative models on challenging multimodal conditional density estimation problems.  
 % In contrast, the number of experts has to be specified a-priori for exisiting optimization schemes, making it difficult to find suitable model complexities. 

Despite introducing a lower bound on the per-component entropy of the curricula, which helps to 
tune the curriculum pacing $\eta$, we find that it can still be difficult to determine an appropriate value. In future work, we plan to tackle this issue.  
% Even though the constraint optimization formulation helps tune the entropy scaling factor $\eta$, during our experiments we noticed that 

% We introduced Information Maximizing Curriculum (IMC), a novel approach to learning mixture of experts models (MoE) in conditional density estimation with highly multi modal data distributions. IMC is a curriculum-based approach that allows each expert to select its own subset of the training data for learning. This curriculum optimization allows the MoE model to automatically ignore data points from modes it can not represent, which reduces the commonly present mode-averaging and local optimum problem for maximum likelihood-based optimization. IMC is able to adapt the model complexity by adding more experts during training which is only possible due to the proposed objective. 
% In particular, we proposed a maximization of a lower bound to a joint entropy objective, which, together with the curriculum optimization, incentivizes the MoE to cover the whole data. We started to present our objective for a single expert and generalized it to the MoE. We further proposed a constraint optimization to ease tuning the entropy scaling coefficient for the per-component-curriculum entropy. 
% Our results on thorough empirical evaluations which include highly multi modal robot data collected by humans show that IMC is able to \textit{i)} avoid mode averaging and \textit{ii)} extract all modes present in the data distribution while outperforming state-of-the-art methods. 

% Even though the constraint optimization formulation helps tune the entropy scaling factor $\eta$, during our experiments we noticed that it can still be difficult to determine the optimal value. In future work, we plan to tackle this issue. 


%We introduced Information Maximizing Curriculum (IMC), a novel approach for training mixtures of experts that combines the information projection with curriculum learning to avoid problems with common maximum likelihood optimization schemes. In particular, 
%We proposed a new objective that builds on the information projection as well as curriculum learning. 
%\begin{itemize}
%    \item Why did we propose this approach? 1) Avoid mode averaging 2) Avoid local maxima 3) finding an appropriate model complexity is difficult
%    \item What did we do? We motivated our objective using a single expert. We generalized the concept to mixtures of experts. 
%    \item We empirically showed that our approach is able to avoid mode averaging and  cover all modes in the data distribution.
%\end{itemize}