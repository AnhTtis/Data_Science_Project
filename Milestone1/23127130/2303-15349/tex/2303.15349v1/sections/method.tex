\section{Information Maximizing Curriculum}
\label{section:imc}
% In this section, we propose IMC, a novel algorithm for training mixtures of experts that builds on the information projection as well as self-paced learning to generate a curriculum for the individual experts that allows the model to ignore samples that it is not able to represent. To that end, we formalize an objective for a single expert...
% In this section, we propose IMC, a novel algorithm for training mixtures of experts which leverages curriculum learning and in particular self-paced learning to soft-assign samples to expert models. Building on the information projection for performing such an assignment allows the model to ignore samples that it is not able to represent resulting in reduced outlier-sensitivity and averaging problems compared to existing moment-projection based approaches. 
In this section, we propose Information Maximizing Curriculum (IMC), a novel algorithm for training mixtures of experts. We motivate our optimization objective using a single expert model. Next, we generalize the objective to support an arbitrary number of experts. Thereafter, we discuss the optimization scheme and algorithmic details. Lastly, we explain how the model is used at inference time.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/illustration_fig.tex}
\subsection{Objective for a Single Self-Paced Expert}
\label{sec:single_expert}
We propose an objective that jointly learns a curriculum $p(\mathcal{D})$ and a parameterized expert distribution $p_{\vtheta}(\mathbf{y}|\mathbf{x})$ with parameters $\vtheta$. The curriculum is a categorical distribution $p(\mathcal{D}_n)$ over samples of a dataset $\mathcal{D} = \{(\mathbf{x}_n, \mathbf{y}_n)\}_{n=1}^N$, assigning probability mass to samples according to the performance of the expert. 
% Moreover, the curriculum expands as the expert specializes to a subset of the data. 
To allow the curriculum to ignore samples that the expert cannot represent, we build on the I-projection (see Equation \ref{eq:i_proj}). We therefore formulate the objective as
% We formulate this as
\begin{equation}
     \max_{p(\mathcal{D}), \vtheta}\ \E_{p(\mathcal{D})}[\log p_{\vtheta}(\mathbf{y}|\mathbf{x})] + \eta \mathcal{H}(\mathcal{D}),
    \label{eq:single_expert_obj}
\end{equation}
which is optimized for $p(\mathcal{D})$ and $\vtheta$ in an alternating fashion. We additionally introduced a trade-off factor $\eta$ that determines the pacing of the curriculum. For $\eta \rightarrow \infty$ the curriculum becomes uniform, exposing all samples to the expert and hence reducing to maximum likelihood estimation for $\vtheta$. In contrast, if $\eta \rightarrow 0$ the curriculum concentrates on samples where the expert log-likelihood $\log p_{\vtheta}(\mathbf{y}|\mathbf{x})$ is highest.
The objective can be solved in closed form for $p(\mathcal{D})$, i.e., 
\begin{equation*}
    p^*(\mathcal{D}_n) \propto p_{\vtheta}(\mathbf{y}_n|\mathbf{x}_n)^{1/\eta}.
\end{equation*}
Maximizing the objective w.r.t $\vtheta$ reduces to a weighted maximum-likelihood estimation for $\vtheta$, that is,
\begin{equation*}
    \vtheta^* = \argmax_{\vtheta} \sum_n p(\mathcal{D}_n)\log p_{\vtheta}(\mathbf{y}_n|\mathbf{x}_n).
\end{equation*}
The expert thus specializes to samples selected by the curriculum which in turn selects samples that lie within the representational capacity of the expert. As a result, the expert is paced by its own performance, thus the name \textit{self-paced}. The optimization is repeated until the curriculum converges, meaning that the representational capacity of the expert is exhausted. The curriculum allows experts to ignore samples that they cannot represent which makes the model less sensitive to noise, outliers, and averaging over parts of the target function with high complexity, such as discontinuities. These properties are illustrated in Figure \ref{fig:illustration_m} and \ref{fig:illustration_i}.
% For a visualization see Figure \ref{fig:outlier_zero_forc}. 
However, the downside of a self-pacing expert is the arbitrarily poor performance at samples that are ignored. This problem is alleviated by introducing multiple experts that specialize to different subsets of the data. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Objective for a Mixture of Self-Paced Experts}
\label{sec:mmoe}
% \textbf{Objective for a Modular Mixture of Specialized Experts.}
Assuming limited complexity, a single expert is likely to ignore a large amount of samples due to the zero-forcing property of the I-projection. Using multiple curricula and experts that specialize to different subsets of the data is hence a natural extension to the single expert model. To that end, we make two major modifications to Equation \ref{eq:single_expert_obj}: Firstly, we use a mixture model with multiple components $o$ where each component has its own curriculum, i.e., $p(\mathcal{D}) = \sum_o p(o) p(\mathcal{D}|o)$. Secondly, we employ an expert per component $p_{\vtheta_o}(\mathbf{y}|\mathbf{x},o)$, that is paced by the corresponding curriculum. The resulting objective function is given by
\begin{equation}
    J(\vpsi) = \E_{p(o)}\E_{p(\mathcal{D}|o)}[\log p_{\vtheta_o}(\mathbf{y}|\mathbf{x},o)] + \eta \mathcal{H}(\mathcal{D}),
    \label{eq:before_decomposition}
\end{equation}
where $\vpsi$ summarizes the dependence on $p(o)$, $\{p(\mathcal{D}|o)\}_o$ and $\{\vtheta_o\}_o$.
However, Equation \ref{eq:before_decomposition} is difficult to optimize as the entropy of the mixture model prevents us from updating each the curriculum of each component independently. Similar to \cite{arenz2018efficient}, we introduce an auxiliary distribution $q(o|\mathcal{D})$ to decompose the objective function into a lower bound $L(\vpsi, q)$ and an expected $\KL$ term, that is,
\begin{equation}
   J(\vpsi) = L(\vpsi, q) + \eta \E_{p(\mathcal{D})}\KL(p(o|\mathcal{D}) \Vert q(o|\mathbf{x})),
   \label{eq:decomposition}
\end{equation}
with $p(o|\mathcal{D}) = p(\mathcal{D}|o)p(o)/p(\mathcal{D})$ and 
\begin{equation*}
    L(\vpsi, q) = \E_{p(o)}\big[\underbrace{\E_{p(\mathcal{D}|o)}[R_o(\mathcal{D})] + \eta\mathcal{H}(\mathcal{D}|o)}_{J_o(p(\mathcal{D}|o), \vtheta_o)}]\big] + \eta \mathcal{H}(o), %+ \KL ( p(x|o) \Vert p_{c_o}(x|o) )
\end{equation*}
with $R_o(\mathcal{D}) = \log p_{\vtheta_o}(\mathbf{y}|\mathbf{x},o) + \eta \log q(o|\mathcal{D})$, allowing for independent updates for $p(\mathcal{D}|o)$ and $\vtheta_o$ by maximizing the per-component objective function $J_o(p(\mathcal{D}|o), \vtheta_o)$. Similar to the expectation-maximization algorithm, we introduced $q(o|\mathcal{D})$ as auxiliary distribution. Note that the lower bound decomposition holds for any distribution  $q(o|\mathcal{D})$.
%[Mention why we use the notation $R$. Make a connection to policy search here. Do we need to proof this result?] 
A derivation can be found in Appendix \ref{appendix:lb_decomp}.
 Since $\E_{p(\mathcal{D})}\KL(p(o|\mathcal{D}) \Vert q(o|\mathbf{x})) \geq 0$, $L$ is a lower bound on $J$ for $\eta \geq 0$. 
 Please note that the per-component objective function $J_o$ is very similar to Equation \ref{eq:single_expert_obj}, i.e., the different experts specialize to samples selected by their curriculum $p(\mathcal{D}|o)$. However, $J_o$ additionally contains $\log q(o|\mathcal{D})$ which prevents different curricula from assigning probability mass to the same samples. This property is further illustrated in Figure \ref{fig:illustration_joint}. %The decomposition used in Equation \ref{eq:decomposition} is closely related to the decomposition used for the expectation-maximization (EM) algorithm \cite{dempster1977maximum}.
 %[Mention that EM is for the forward KL whereas we use the decomposition on the reverse KL] 
 We follow the optimization scheme of the EM algorithm, that is, we iteratively maximize (M-step) and tighten the lower bound (E-step).
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maximizing the Lower Bound (M-Step)}
\label{sec:m_step}
We maximize the lower bound $L(\vpsi, q)$ with respect to the mixture weights $p(o)$, curricula $p(\mathcal{D}|o)$ and expert parameters $\vtheta_o$.  We find closed form solutions for both, $p(o)$ and $p(\mathcal{D}|o)$ given by
\begin{equation}
\label{eq:opt_mw}
    p^*(o) \propto \exp\big( \E_{p(\mathcal{D}|o)}[{R_o(\mathcal{D})}/{\eta}] + \mathcal{H}(\mathcal{D}|o)\big),
\end{equation}
and 
\begin{equation*}
    p^*(\mathcal{D}_n|o) \propto \tilde{p}(\mathcal{D}_n|o) = \exp\big( {R_o(\mathcal{D}_n)}/{\eta}\big),
\end{equation*}
where $\tilde{p}(\mathcal{D}_n|o)$ are the optimal unnormalized curricula $p^*(\mathcal{D}_n|o)$. However, due to the hierarchical structure of $L(\vpsi, q)$ we implicitly optimize for $p(o)$ when updating the curricula. 
This result is frequently used throughout this work and is formalized in Proposition \ref{thm:implicit}. A proof is found in Appendix \ref{proof:implicit}.
\begin{proposition}
\label{thm:implicit}
Let $p^*(o)$ and $\tilde{p}(\mathcal{D}|o)$ be the optimal mixture weights and unnormalized curricula for maximizing $L(\vpsi, q)$. It holds that
\begin{equation*}
    p^*(o) = \sum_n \tilde{p}(\mathcal{D}_n|o) / \sum_o \sum_n \tilde{p}(\mathcal{D}_n|o).
\end{equation*}
\end{proposition}
The implicit updates of the mixture weights render the computation of $p^*(o)$ obsolete, reducing the optimization to computing the optimal (unnormalized) curricula $\tilde{p}(\mathcal{D}|o)$ and expert parameters $\vtheta^*_o$.
% Indeed, the mixture weights are not required for training the mixture of experts. Consequently, the optimization reduces to computing the optimal curricula $p^*(\mathcal{D}|o)$ and expert parameters $\vtheta^*_o$.
% \begin{proof}
% A proof is found in Appendix XX.
% \end{proof}
Maximizing the lower bound with respect to the expert parameters results in a weighted maximum likelihood estimation, i.e.,  
\begin{equation}
\label{eq:expert_update}
  \vtheta^*_o =\argmax_{\vtheta_o} \ \sum_n p(\mathcal{D}_n|o) \log p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n,o),
\end{equation}
where the curricula $p(\mathcal{D}_n|o)$ assign sample weights.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tightening the Lower Bound (E-Step)}
\label{sec:e_step}
Tightening of the lower bound (also referred to as E-step) is done by minimizing the expected Kullback-Leibler divergence in Equation \ref{eq:decomposition}. Using the properties of the KL divergence, it can easily be seen that the lower bound is tight if for all $n\in \{1,...,N\}$ $q(o|\mathbf{x}_n) = p(o|\mathcal{D}_n)$ holds. To obtain $p(o|\mathcal{D}_n)$ we leverage Bayes' rule, that is, $p(o|\mathcal{D}_n)=p^*(o)p^*(\mathcal{D}_n|o)/\sum_o p^*(o)p^*(\mathcal{D}_n|o)$. Using Proposition \ref{thm:implicit} we find that
% \begin{corollary}
% \label{thm:gating}
\begin{equation*}
    p(o|\mathcal{D}_n) = \tilde{p}(\mathcal{D}_n|o) / \sum_o \tilde{p}(\mathcal{D}_n|o).
\end{equation*}
% \end{corollary}
 Please note that the lower bound is tight after every E-step as the KL divergence is set to zero. Thus, increasing the lower bound $L$ maximizes the original objective $J$ assuming that updates of $\vtheta_o$ are not decreasing the expert log-likelihood $\log p_{\vtheta_o}(\mathbf{y}|\mathbf{x},o)$.
 % [Mention that since we can make the lower bound tight, we increase the original objective $J$ when increasing the lower bound $L$]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Automatic Per-Component Curriculum Pacing}
\label{sec:dual}
Choosing a fixed curriculum pacing value $\eta$ for applications where entropy and expert log-likelihood values change heavily during training can be a limiting assumption and might lead to sub-optimal results. Moreover, pacing all curricula with the same $\eta$ can result in curricula that cover large subsets of the data while others degenerate, i.e., only cover few samples. 
In order to obtain an adaptive per-component curriculum pacing and alleviate problems with degrading curricula, we enforce a lower bound $\mathcal{H}_{\text{min}}$ on the entropy of the individual curricula. To that end, we frame the per-component objective $J_o$ as constraint optimization problem, giving
\begin{equation*}
    \argmax_{p(\mathcal{D}|o)} \ \E_{p(\mathcal{D}|o)}[R_o(\mathcal{D})] + \eta \mathcal{H}(\mathcal{D}|o), \ \text{s.t.} \ \mathcal{H}(\mathcal{D}|o) \geq  \mathcal{H}_{\text{min}}.
\end{equation*}
Using Lagrange duality \cite{boyd2004convex} we obtain a closed form solution by optimizing the Lagrangian function given by
\begin{equation*}
    p^*_{\xi_o}(\mathcal{D}|o) \propto \exp\Big( \frac{R_o(\mathcal{D})}{\xi_o + \eta}\Big),
\end{equation*}
with per-component Lagrangian multiplier $\xi_o$. The optimal value $\xi^*_o$ is obtained by minimizing the Lagrangian dual function $g(\xi_o)$, that is,
% \begin{equation*}
    $\xi_o^* = \argmin_{\xi_o > 0} \ g(\xi_o)$
% \end{equation*}
with
\begin{equation*}
% \resizebox{0.45\textwidth}{!}{
    g(\xi_o) = \xi_o \Big( \log \sum_n \exp\Big( \frac{R_o(\mathcal{D}_n)}{\xi_o + \eta}\Big)
    -  \mathcal{H}_{\text{min}}\Big),
    % }
\end{equation*}
and thus $p^*(\mathcal{D}|o) = p^*_{\xi^*_o}(\mathcal{D}|o)$. Please note that $\xi_o^*$ is obtained using a convex numerical optimizer. 
% For a derivation of the Lagrangian dual function see Appendix \ref{appendix:derivations:lagrange}. 
The minimum per-component entropy $\mathcal{H}_{\text{min}}$ is intuitive to choose as it translates into the number of samples $N_s$ that a single curriculum should cover (uniformly) by setting $\mathcal{H}_{\text{min}} = \log N_s$. 

% [Quick intro. Motivate chapter new since we have a soft and hard constraint now]
% Choosing the curriculum pacing $\eta$ is difficult in applications where entropy and expert log-likelihood values change heavily during training. Furthermore, pacing all curricula with the same $\eta$ value for all components can be a limiting assumption and might lead to sub-optimal results. Moreover, for datasets that require an intractable amount of components to cover all samples it can be preferable to trade-off the performance with computational complexity to enforce less components to cover all samples. To address these points we frame the per-component objective as constraint optimization problem to enforce a minimum per-component entropy $\mathcal{H}_{\text{min}}$, giving
% \begin{equation*}
%     \argmax_{p(\mathcal{D}|o)} \ \E_{p(\mathcal{D}|o)}[R_o(\mathcal{D})] + \eta \mathcal{H}(\mathcal{D}|o), \ \text{s.t.} \ \mathcal{H}(\mathcal{D}|o) \geq  \mathcal{H}_{\text{min}}.
% \end{equation*}
% Using Lagrange duality (\cite{boyd2004convex}) we obtain a closed form solution for $p(\mathcal{D}|o)$ by optimizing the Lagrangian function given by
% \begin{equation*}
%     p^*_{\xi_o}(\mathcal{D}|o) \propto \exp\Big( \frac{R_o(\mathcal{D})}{\xi_o + \eta}\Big),
% \end{equation*}
% with per-component dual variable $\xi_o$. The optimal dual variable $\xi^*_o$ is obtained by minimizing the Lagrangian dual function $g(\xi_o)$, that is,
% % \begin{equation*}
%     $\xi_o^* = \argmin_{\xi_o > 0} \ g(\xi_o)$
% % \end{equation*}
% with
% \begin{equation*}
% % \resizebox{0.45\textwidth}{!}{
%     g(\xi_o) = \xi_o \Big( \log \sum_n \exp\Big( \frac{R_o(\mathcal{D}_n)}{\xi_o + \eta}\Big)
%     -  \mathcal{H}_{\text{min}}\Big),
%     % }
% \end{equation*}
% and thus $p^*(\mathcal{D}|o) = p^*_{\xi^*_o}(\mathcal{D}|o)$. Please note that $\xi_o^*$ is obtained using a convex numerical optimizer. 
% % For a derivation of the Lagrangian dual function see Appendix \ref{appendix:derivations:lagrange}. 
% The minimum per-component entropy $\mathcal{H}_{\text{min}}$ is intuitive to choose as it translates into the number of samples $N_s$ that a single component should cover (uniformly) by setting $\mathcal{H}_{\text{min}} = \log N_s$. 
%Furthermore, the Lagrangian dual formulation introduces a per-component dual variable $\xi_o$ and therefore allows for varying entropy scaling factors between components.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Adding Components to the Model}
\subsection{Adding Components and Algorithmic Details}
\label{section:adding_o}
In contrast to mixtures of experts trained by either EM or backpropagation, we adapt the model complexity online by adding new components during the training procedure. Using such an online scheme for maximum likelihood based approaches is difficult as existing components are forced to cover all samples, making it challenging to initialize new components that improve the overall model performance. 

We initialize the model with a single component and gradually increase the model complexity by adding more components to the mixture of experts model. Moreover, we use the convergence of the lower bound $L(\vpsi, q)$ as criterion for adding new components to the model. After each M-step, the lower bound is evaluated using Corollary \ref{thm:lowerbound}. See Appendix \ref{proof:lowerbound} for a proof.
\begin{corollary} 
\label{thm:lowerbound}
Consider the setup used in Proposition \ref{thm:implicit}. For $p^*(o) \in \vpsi$ and  $\{p^*(\mathcal{D}|o)\}_o \in \vpsi$ it holds that
\begin{equation*}
    L\big(\vpsi, q\big)  = \eta \log \sum_o \sum_n \tilde{p}(\mathcal{D}_n|o).
\end{equation*}
\end{corollary}
% \begin{equation*}
% % L(\vpsi, q)  = \log \sum_o \sum_n \tilde{p}(\mathcal{D}_n|o).
% L\big(p^*(o), \{p^*(\mathcal{D}|o)\}_o,\{\vtheta_o\}_o, q\big)  = \log \sum_o \sum_n \tilde{p}(\mathcal{D}_n|o).
% \end{equation*}
Hence, if the difference between two subsequent iterations $(i)$ and $(i-1)$ falls below a threshold $\epsilon$, that is, 
\begin{equation*}
   |\Delta L| = |L^{(i)}(\vpsi, q)- L^{(i-1)}(\vpsi, q)| \leq \epsilon,
\end{equation*}
a new component $o_{\text{new}}$ is added. The full training procedure is outlined in Algorithm \ref{algo:imc_training}.
% In order to adapt the complexity of our model we  
% [Explain the criteria for adding components here and finish this chapter with pseudocode. Explain the choice of experts here as well. Mention that learning the variance of the Gaussians leads to instabilities. Mention that targets are standardized. Hence, the variance is set to 1. Cite a paper that states that optimizing Gaussian likelihoods is unstable.]
\input{sections/pseudocode.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inference}
\label{section:inference}
In order to perform inference, i.e., sampling from the model or computing expectations, we need to access the gating distribution for arbitrary inputs $\mathbf{x}$ which is not possible as $p(o|\mathcal{D})$ is only defined for samples contained in $\mathcal{D}$. We therefore leverage Corollary \ref{thm:gating} to learn an inference network $g_{\vphi}(o|\mathbf{x})$ with parameters $\vphi$ by minimizing the KL divergence between $p(o|\mathcal{D})$ and $g_{\vphi}(o|\mathbf{x})$ under the joint curriculum $p(\mathcal{D})$ (see Appendix \ref{proof:gating} for a proof).
\begin{corollary} 
\label{thm:gating}
Consider the setup used in Proposition \ref{thm:implicit}. For $p(o|\mathcal{D}) \propto p^*(o)p^*(\mathcal{D}|o)$ and $p({\mathcal{D}})=\sum_o p^*(o)p^*(\mathcal{D}|o)$ it holds that
    \begin{align*}
    &\min_{\vphi} \E_{p({\mathcal{D}})}\KL\big(p(o|\mathcal{D})\Vert g_{\vphi}(o|\mathbf{x})\big) \\ = \ 
    & \max_{\vphi} \sum_n \sum_o \tilde{p}(\mathcal{D}_n|o) \log g_{\vphi}(o|\mathbf{x}_n).
\end{align*}
\end{corollary}
Training a gating network can be cumbersome for applications where we often switch between training and inference (e.g. due to new data acquisition), 
since adding components requires learning a new gating network from scratch due to varying network sizes. We counteract this problem by using an output dimension equal to the maximal number of components $N_{o, \textrm{max}}$ in the MoE model. Using a masking layer, i.e., a binary vector that sets the current number of components $N_{o}$ to $1$ and $0$ otherwise allows to preserve the learned representation for existing components when adding a new component.

% using a modular gating network which has output dimension equal to the maximal number of components $N_{o, \textrm{max}}$ that we intend to add to the mixture model. Additionally, we employ a mask that is multiplied with the gating output before applying the softmax transformation. The mask is a binary vector where the first $N_o$ entries are $1$ and the rest $0$. Hence, when adding a component, we simply set the corresponding mask entry to $1$, allowing us to preserve the learned representation for the other components.

% In order to perform inference, i.e., sample from the model or compute expectations, we need to access the gating distribution for arbitrary inputs $\mathbf{x}$ which is not possible as $p(o|\mathcal{D})$ is only defined for samples contained in $\mathcal{D}$. We therefore learn a inference network $g_{\vphi}(o|\mathbf{x})$ with parameters $\vphi$ by minimizing the KL divergence between $p(o|\mathcal{D})$ and $g_{\vphi}(o|\mathbf{x})$ under the joint curriculum $p(\mathcal{D})$, that is,
% \begin{align*}
%     &\min_{\vphi} \E_{p({\mathcal{D}})}\KL\big(p(o|\mathcal{D})\Vert g_{\vphi}(o|\mathbf{x})\big) \\ = \ 
%     & \max_{\vphi} \sum_n \sum_o \tilde{p}(\mathcal{D}|o) \log g_{\vphi}(o|\mathbf{x}_n).
% \end{align*}
% [PROOF. (Mention that uncovered samples do not impact the gating optimization?)]
% The inference network replaces the gating distribution $p(o|\mathbf{x})$ for inference, that is
% \begin{equation}
%     p(\mathbf{y}|\mathbf{x}) = \sum_o g_{\vphi}(o|\mathbf{x}) p_{\vtheta}(\mathbf{y}|\mathbf{x},o).
%     \label{eq:marg_lh}
% \end{equation}
% [Remove the following part since the modular gating network leads to performance issues. Instead, mention that training the inference network is only done once at the end of training and therefore does not interfer with the concept of modularity.]
% Training a gating network can be cumbersome for applications where we often switch between training and inference (e.g. due to new data acquisition), 
% since adding components would require learning a new gating network from scratch due to varying output dimensions. We counteract this problem by using a modular gating network which has output dimension equal to the maximal number of components $N_{o, \textrm{max}}$ that we intend to add to the mixture model. Additionally, we employ a mask that is multiplied with the gating output before applying the softmax transformation. The mask is a binary vector where the first $N_o$ entries are $1$ and the rest $0$. Hence, when adding a component, we simply set the corresponding mask entry to $1$, allowing us to preserve the learned representation for the other components.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%