\section{Introduction}
% \begin{itemize}
%     \item Given data a common task is to fit a model to a data distribution
%     \item This is commonly done by minimizing a similarity measure, typically the forward KL divergence, also known as moment projection or maximum likelihood estimation
%     \item If the data distribution is multimodal the moment projection forces the model to average over modes if it has insufficient complexity, which can lead to arbitrarily poor solutions.
%     \item The information projection minimizes the reverse KL divergence and is zero forcing. [elaborate here].
%     \item Moreover, the Moment projection is often paired with the assumption that samples stem from the same data distribution (i.e., the i.i.d. assumption). As a consequence, all samples are assigned equal importance, rendering the model prone to outliers and leading to an unnatural learning progression as easy samples are treated equally as difficult ones. This is contradictory to the way humans learn, where skills are often learned on a basic level before gradually increasing the difficulty to become an expert.
%     \item Curriculum learning (CL) \cite{bengio2009curriculum} tries to close this gap by assigning different weights to samples according to their difficulty. While early work relies on a manual assignment of these weights, more recent work automatically generates the curriculum by adapting to the pace of the learner \cite{jiang2014self}. [Mention that this is called self-paced curriculum learning]
%     \item We introduce ...
% \end{itemize}
% Using supervised learning to fit a model to a data distribution is a common task in machine learning
% % A common task in machine learning is to fit a model to a data distribution induced by some dataset. 
% which typically done by minimizing the forward Kullback-Leibler (KL) divergence, also known as moment projection or maximum-likelihood estimation. If the data distribution is multimodal the moment projection forces the model to average over modes if it has insufficient complexity, which can lead to arbitrarily poor solutions. In contrast, minimizing the reverse KL divergence, that is, the information projection, ignores non-representable modes, leading to good generative models.
% \begin{itemize}
%     \item Mention that advantage of the zero forcing property (focuses on parts where the model is able to represent -> creates good generative models, but some areas with high prob mass might be ignored)
%     \item Mention why it is commonly not used for optimization - difficulty in the optimization (maybe cite EIM)
%     \item citations?
%     \item should we do an enumeration of problems and address them when we introduce our method?
% \end{itemize}
Using supervised learning to fit a model to a data distribution is a common task in machine learning
% A common task in machine learning is to fit a model to a data distribution induced by some dataset. 
which typically done by minimizing the forward Kullback-Leibler (KL) divergence, also known as moment projection or maximum-likelihood estimation. If the data distribution is multimodal the moment projection forces the model to average over modes if it has insufficient complexity, which can lead to arbitrarily poor solutions. In contrast, minimizing the reverse KL divergence, that is, the information projection, ignores non-representable modes, leading to good generative models.

Moreover, common optimization objectives make the i.i.d. assumption, i.e., that samples stem from the same data distribution \cite{bishop2006pattern}.
% (i.e., the i.i.d. assumption). 
As a consequence, all samples are assigned equal importance, rendering the model prone to outliers and leading to an unnatural learning progression as easy samples are treated equally as difficult ones. This is contradictory to the way humans learn, where skills are often learned on a basic level before gradually increasing the difficulty to become an expert. Curriculum learning tries to close this gap by assigning different weights to samples according to their difficulty \cite{bengio2009curriculum}. While early work relies on a manual assignment of these weights, more recent work, named self-paced learning, automatically generates the curriculum by adapting to the pace of the learner \cite{jiang2014self, kumar2010self, zhang2015self, klink2020self, klink2020self2, celik2022specializing}.

In this work, we propose \textit{information maximizing curriculum} (IMC), a novel approach for training mixtures of experts (MoEs) \cite{jacobs1991adaptive} that builds on the information projection as well as self-paced learning. MoEs are powerful models, 
that leverage a divide-and-conquer approach to conditional density estimation by assigning experts to smaller sub-tasks. They are capable of learning highly multimodal distributions but are commonly trained by maximizing the likelihood via expectation maximization \cite{dempster1977maximum}, making them susceptible to mode averaging. 
Furthermore, finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. In contrast, IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, maximizing the entropy of the joint curriculum of all experts ensures that they specialize to different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Mixtures of experts (MoEs) are powerful models, that leverage a divide-and-conquer approach to conditional density estimation by assigning experts to smaller sub-tasks. They are capable of learning highly complex multimodal distributions but are commonly trained by maximizing the likelihood via expectation maximization \cite{dempster1977maximum}. It is well known that maximum likelihood estimation corresponds to a moment projection which causes the model to average over modes that it cannot represent, leading to poor generative capabilities. Moreover, finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. 


% Despite this drawback, the moment projection is often paired with the assumption that that samples stem from the same data distribution \cite{bishop2006pattern}. As a consequence, easy and difficult samples are weighted equally, rendering the model prone to outliers. % we could mentoin here that the mode averaging propoerty can be problematic in applications such as behavior learning.

Despite this drawback, 
% a more general problem is that 
common optimization objectives assume that samples stem from the same data distribution \cite{bishop2006pattern}. As a consequence, all samples are assigned equal importance, rendering the model prone to outliers and leading to an unnatural learning progression as easy samples are treated equally as difficult ones. This is contradictory to the way humans learn, where skills are often learned on a basic level before gradually increasing the difficulty to become an expert. Whilst this (i.i.d.) assumption is justified for most problems, certain applications such as... assuption invalid...

In this work, we propose \textit{information maximizing curriculum} (IMC), a novel approach for training mixtures of experts (MoEs) that combines an information projection with curriculum learning (CL) to address the aforementioned problems with existing optimizing schemes. The information projection minimizes the reverse KL divergence which forces the model to ignore non-representable modes, leading to good generative capabilities. In addition, CL assigns weight to samples according to their difficulty \cite{bengio2009curriculum}, resulting in reduced outlier sensitivity. While early work relies on a manual assignment of these weights, more recent work that goes by self-paced learning, automatically generates the curriculum by adapting to the pace of the learner \cite{kumar2010self}. 

IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, the information projection on the joint curriculum of all experts ensures that they specialize to different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model.
 


% In this work, we propose \textit{information maximizing curriculum} (IMC), a novel approach for training mixtures of experts (MoEs) that builds on the information projection as well as self-paced curriculum learning.  MoE are powerful models, capable of learning highly multimodal distributions but are commonly trained using the moment projection making them susceptible to mode averaging [why is that bad - leads to suboptimal solutions]. Furthermore,  finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. In contrast, IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, maximizing the entropy of the joint curriculum of all experts ensures that they specialize to different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model.

In this work... tackle both problems using a novel objective and epirically show that iid invalid... we base our objective on the I-projection and build on curriculu learning.

The I-Projection is ....., and therefore is an appropriate chosse

Curriculum  learning distinguishes between difficult ....

On highly multimodal human collected behavior learning tasks, we show that out method is able to outperform... 

We show that our method is able to outperform xyz on challengin conditional density estimation tasks, wwhere the iid assumption is violated. We consider highly multimodal human collected behavior learning tasks. 


Curriculum learning tries to close this gap by assigning different weights to samples according to their difficulty \cite{bengio2009curriculum}. While early work relies on a manual assignment of these weights, more recent work, named self-paced learning, automatically generates the curriculum by adapting to the pace of the learner \cite{jiang2014self, kumar2010self, zhang2015self, klink2020self, klink2020self2, celik2022specializing}.


Furthermore, finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. In contrast, IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, maximizing the entropy of the joint curriculum of all experts ensures that they specialize to different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% [should we mention the concept of MoEs briefly on a high level here? yes] capable of learning highly multimodal distributions but suffer from the aforementioned problems [due to their optimization objective. repeat m projection and assumptions]
% % are commonly trained using the moment projection making them susceptible to mode averaging [why is that bad - leads to suboptimal solutions]. 
% Furthermore, finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. In contrast, IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, maximizing the entropy of the joint curriculum of all experts ensures that they specialize to different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model.

% Most machine learning approaches leverage the moment projection for fitting a model to a data distribution. If the data distribution is multimodal the moment projection forces the model to average over modes if it has insufficient complexity, which can lead to arbitrarily poor solutions. In contrast, the information projection ignores modes that the model is not able to represent [Not sure if the I-projection should be introduced here]. Moreover, the moment projection [paired with the iid assumption] typically assigns all data samples equal importance rendering the model prone to outliers and leading to an unnatural learning progression as easy samples are treated equally as difficult ones. This is contradictory to the way humans learn, where skills are often learned on a basic level before gradually increasing the difficulty to become an expert. [Introduce I projection here?] Curriculum learning (CL) tries to close this gap by assigning different weights to samples according to their difficulty. While early work relies on a manual assignment of these weights, more recent work automatically generates the curriculum by adapting to the pace of the learner \cite{jiang2014self, kumar2010self, zhang2015self, klink2020self, klink2020self2, celik2022specializing}.

% In this work, we propose \textit{information maximizing curriculum} (IMC), a novel approach for training mixtures of experts (MoEs) that builds on the information projection as well as self-paced curriculum learning.  MoE are powerful models, capable of learning highly multimodal distributions but are commonly trained using the moment projection making them susceptible to mode averaging [why is that bad - leads to suboptimal solutions]. Furthermore,  finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. In contrast, IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, maximizing the entropy of the joint curriculum of all experts ensures that they specialize to different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model.
% [Outline; We show that...good regression performance. Ability to learn highly multimodal distributions][Contribution statement?]



%%%% Suggestion:
%A common task in machine learning is fitting a model to an arbitrary data distribution. Typically, the parameters of the model are optimized by maximizing the model's likelihood under the given data points, which is also known as the moment projection (M-projection). This maximization can lead to arbitrarily poor solutions if the chosen model can not represent the data distribution due to insufficient complexity, which most notably occurs for multi-modal data distributions. The moment projection assigns all data samples equal importance such that the model tries to cover the whole data even for outliers, in which case the model averages over modes. [Here we could already mention the probability forcing property]. 

% In contrast, minimizing the reverse Kullback-Leibler (KL) [Footnote for a small definition of what KL is?] Divergence between the model and the data distribution does not suffer from the mode averaging problem as it forces the model to concentrate on modes it can represent only [Introduce zero forcing?]. This minimization is known as information projection (I-projection). However, optimizing models under this objective is not straightforward. 

% Getting a good representation of a data distribution requires having a powerful model that is capable of representing highly multimodal distributions. Mixtures of experts (MoE) can represent arbitrarily complex data distributions with enough amount of experts, where each expert is responsible for a distinct region of the input space. This suggests that they are a natural choice for the model, however, it is not straightforward to train these models. Prior work usually trained MoE using the M-projection, which suffers from the aforementioned drawbacks since the number of experts has to be specified a-priori which is difficult to choose. Using the I-Projection to train these models is, therefore, more appropriate and additionally allows for easily adapting the model complexity during training due to the mode-seeking property. However, training MoE with the I-Projection is not straightforward. 

%Additionally, for both techniques, the model will see the whole data set during training, which is contradictory to how humans learn where tasks are learned on a basic level before gradually increasing the difficulty to eventually becoming an expert. Curriculum learning (CL) tries to close this gap by assigning different weights to samples according to their difficulty and hence gradually guides the model during learning. While early work relies on a manual assignment of these weights, more recent work automatically generates the curriculum by adapting to the pace of the learner. [cite]

%In this work, we propose \textit{information maximizing curriculum} (IMC), a novel approach for training mixtures of experts (MoEs) that builds on the information projection as well as self-paced curriculum learning. We leverage MoEs because of their powerful model complexity and use the I-Projection to avoid the shortcomings of the M-Projection. We propose an objective that allows each expert to automatically pace its curriculum by choosing the data points it can represent and specialize while retaining the properties of the I-Projection. Moreover, maximizing the entropy of the curriculum of all experts ensures that they specialize in different subsets of the data allowing for an online adaptation of the model complexity by adding experts to the model. 

%[Outline; We show that...good regression performance. Ability to learn highly multimodal distributions][Contribution statement?]

