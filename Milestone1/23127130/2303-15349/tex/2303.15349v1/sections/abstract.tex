\begin{abstract}
% credit to chatGPT :)
Mixtures of Experts (MoE) are known for their ability to learn complex conditional distributions with multiple modes. However, despite their potential, these models are challenging to train and often tend to produce poor performance, explaining their limited popularity. Our hypothesis is that this under-performance is a result of the commonly utilized maximum likelihood (ML) optimization, which leads to mode averaging and a higher likelihood of getting stuck in local maxima.
We propose a novel curriculum-based approach to learning mixture models in which each component of the MoE is able to select its own subset of the training data for learning. This approach allows for independent optimization of each component, resulting in a more modular architecture that enables the addition and deletion of components on the fly, leading to an optimization less susceptible to local optima. The curricula can ignore data-points from modes not represented by the MoE, reducing the mode-averaging problem. To achieve a good data coverage, we couple the optimization of the curricula with a joint entropy objective and optimize a lower bound of this objective.
We evaluate our curriculum-based approach on a variety of multimodal behavior learning tasks and demonstrate its superiority over competing methods for learning MoE models and conditional generative models. %Our approach utilizes relatively simple deep neural network (DNN) architectures, which are cheap at inference time, as components and still achieves improved performance.
%Mixture of Expert (MoE) models are powerful models for learning conditional distributions with several modes. However, MoE models are hard to train and often yield sub-optimal prediction models in comparison to current SOTA methods and are hence not very popular. We postulate that the rather poor performance is caused by the commonly used maximum likelihood (ML) optimization for MoE models, which causes mode averaging and is prone to getting stuck in local maxima.  
%We propose a curriculum-based approach to learning mixture models where each component can select its own  subset of the training data to be used for learning. The curricula allow independent optimization of each component, enabling modular architectures where components can be added and deleted on the fly, resulting in an optimization less prone to local optima. Moreover, data-points from modes not represented by the MoE can be ignored by the curriculum which diminishes the mode-averaging problem. The optimization of the curricula is coupled by an entropy objective which, given sufficient components, attempts to cover the whole dataset with responsible components.   
% We test our approach on various multi-modal imitation learning tasks and show that, already with rather simple DNN architectures used as components, it outperforms current SOTA methods for learning MoE models as well as more general conditional generative models. 
 
\end{abstract}