\section{Experiment Setup}
\label{appendix:experiment_setup}
\subsection{Environments and Datasets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Obstacle Avoidance}
\label{appendix:obs_avoidance}
%\begin{figure}[htb!]%[t]
        % \centering
%        \begin{minipage}[t!]{0.235\textwidth}
%            \centering 
%            \includegraphics[width=\textwidth]{supplementary_material/figures/oa_env_final.png}
%        \end{minipage}
%         \hfill
%         \begin{minipage}[t!]{0.235\textwidth}
%            \centering 
%            \includegraphics[width=.6\textwidth]{supplementary_material/figures/Fig:ICML23:obsavoid_data.pdf}
%        \end{minipage}
%         \hfill
%        \caption[]
%        {\small Obstacle avoidance environment. The right figure shows 6 out of 24 modes for this task.
%        }
%        \label{fig:obs_avoid_env}
%    \end{figure}

\begin{figure}[htb!]%[t]
        \begin{minipage}[t!]{0.235\textwidth}
            \includegraphics[width=\textwidth]{supplementary_material/figures/obsavoid_data.pdf}
        \end{minipage}
        \hfill
        \begin{minipage}[t!]{0.235\textwidth}
            \includegraphics[width=\textwidth]{supplementary_material/figures/contexts.pdf}
        \end{minipage}
        \hfill
        \caption[]{\small The left figure shows 6 out of 24 ways of completing the obstacle avoidance task. The right figure shows all 30 initial block configurations used for the block pushing task.}
        \label{fig:obs_hbp}
\end{figure}
    
\textbf{Dataset.} The obstacle avoidance dataset contains $96$ trajectories resulting in a total of $7.3$k $(\mathbf{x}, \mathbf{y})$ pairs. The inputs $\mathbf{x} \in \mathbb{R}^{4}$ contain the end-effector position and velocity in Cartesian space. Please note that the height of the robot is fixed. The targets $\mathbf{y} \in \mathbb{R}^{2}$ represent the desired position of the robot. The data is recorded such that there are an equal amount of trajectories for all $24$ ways of avoiding the obstacles and reaching the target line. For successful example trajectories see Figure \ref{fig:obs_hbp}.

\textbf{Performance Metrics.} The \textit{success rate} indicates the number of end-effector trajectories that successfully reach the target line (indicated by green color in Figure \ref{fig:planar_reacher_vis}). The \textit{entropy} 
\begin{equation*}
    \mathcal{H}_{24}(\vtau) = - \sum_\vtau p(\vtau) \log_{24} p(\vtau),
\end{equation*}
is computed for successful trajectories $\vtau$. To assess the model performance, we simulate $1000$ end-effector trajectories. We count the number of successful trajectories for each way of completing the task. From that, we calculate a categorical distribution $p(\vtau)$ which is used to compute the entropy. By the use of $\log_{24}$ we make sure that $\mathcal{H}_{24}(\vtau) \in [0, 1]$. If a model is able to discover all modes in the data distribution with equal probability, its entropy will be close to $1$. In contrast, $\mathcal{H}_{24}(\vtau) = 0$ if a model only learns one solution.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Block Pushing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%        \centering
%        \begin{minipage}[t!]{0.235\textwidth}
%            \centering 
%            \includegraphics[width=\textwidth]{supplementary_material/figures/hbp_env_final.png}
%        \end{minipage}
%        \hfill
%        \begin{minipage}[t!]{0.235\textwidth}
%            \centering 
%            \includegraphics[width=\textwidth]{supplementary_material/figures/contexts.pdf}
%        \end{minipage}
%         \hfill
%        \caption[ ]
%        {\small \textbf{Block pushing environment.}}
%        \label{fig:box_env}
%    \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{appendix:block_pushing}
\textbf{Dataset.} The block pushing dataset contains $480$ trajectories resulting in a total of $100$k $(\mathbf{x}, \mathbf{y})$ pairs. The inputs $\mathbf{x} \in \mathbb{R}^{16}$ contain the desired position and velocity of the robot in addition to the position and orientation of the green and red block. Please note that the orientation of the blocks is represented as quaternion number system. Please note that the height of the robot is fixed. The targets $\mathbf{y} \in \mathbb{R}^{2}$ represent the desired position of the robot. For all $30$ initial block configurations $\mathbf{c}$, i.e., position and orientation, we record four trajectories for all (four) push sequences. This task is similar to the one proposed in \cite{florence2022implicit}. However, they use a deterministic controller to record the data whereas we use human demonstrators which increases the difficulty of the task significantly due to the inherent versatility in human behavior.  

\textbf{Performance Metrics.} The \textit{success rate} indicates the number of end-effector trajectories $\vtau$ that successfully push both blocks to different target zones. To assess the model performance on non-successful trajectories, we consider the \textit{distance error}, that is, the euclidean distance from the blocks to the target zones at the final block configuration of an end-effector trajectory. As there are a total of four push sequences (see Figure \ref{fig:planar_reacher_vis}) we use the expected \textit{entropy}
\begin{equation*}
    \E_{p(\mathbf{c})}\mathcal{H}_{4}(\vtau|\mathbf{c}) = - \sum_{\mathbf{c}} p(\mathbf{c}) \sum_{\vtau} p(\vtau|\mathbf{c}) \log_{4} p(\vtau| \mathbf{c}),
\end{equation*}
to quantify a model's ability extract the modes in the data distribution. Please note that we set $p(\mathbf{c}) = 1/30$ as we sample $30$ block configurations uniformly from a configuration space (see Figure \ref{fig:obs_hbp}). For each $\mathbf{c}$ we simulate $16$ end-effector trajectories. For a given configuration, we count how often each of the four push-sequences is executed successfully and use the result to calculate a categorical distribution $p(\vtau| \mathbf{c})$. Once repeated for all $30$ configurations , we compute $\E_{p(\mathbf{c})}\mathcal{H}_{4}(\vtau|\mathbf{c})$. Using $\log_4$ we make sure that the expected entropy is upper bounded by $1$. This bound is achieved if a model is able to execute each of the push sequences with equal probability for all configurations. If a model only executes one sequence successfully, the entropy is $0$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Franka Kitchen} 
\textbf{Dataset.} The Franka kitchen environment was introduced by \citet{gupta2019relay}. It contains $566$ human-collected trajectories resulting in a total of $128$k $(\mathbf{x}, \mathbf{y})$ pairs. 
The inputs $\mathbf{x} \in \mathbb{R}^{30}$ contain information about position and orientation of the task-relevant objects in the environment. The targets $\mathbf{y} \in \mathbb{R}^{9}$ represent the signals to control the robot and the gripper. The dataset comprises sequences that successfully solve $4$ out of $7$ tasks in different orders.   

\textbf{Performance Metrics.} First, we consider the \textit{success rate} for a different number of tasks solved. We additionally compute the \textit{entropy} over task sequences. 
This is computed using $100$ simulated robot trajectories. For trajectories with a single task solved, we count how frequently each of the tasks is executed. From that we calculate a categorical distribution which is then used for computing the entropy. We generalize this concept to more successful task completions, by calculating a categorical distribution over all $7^k$ possible task sequences for $k$ task completions.
\label{appendix:fk_kitchen}
\begin{figure}[htb!]%[t]
        % \centering
        \begin{minipage}[t!]{0.35\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{supplementary_material/figures/kitchen_env.png}
        \end{minipage}
         \hfill
        \caption[]
        {\small \textbf{Franka kitchen environment.} 
        }
        \label{fig:fk_env}
    \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Table Tennis}
\textbf{Dataset.} The table tennis dataset contains $5000$ $(\mathbf{x}, \mathbf{y})$ pairs. The inputs $\mathbf{x} \in \mathbb{R}^{4}$ contain the coordinates of the initial and target ball position as projection on the table. Movement primitives (MPs) \cite{paraschos2013probabilistic} are used to describe the joint space trajectories of the robot manipulator using two basis functions per joint and thus $\mathbf{y} \in \mathbb{R}^{14}$.

\textbf{Metrics.} To evaluate the different algorithms on the demonstrations recorded using the table tennis environment quantitatively, we employ two performance metrics: The \textit{success rate} and the \textit{distance error}. The success rate is the percentage of strikes where the ball is successfully returned to the opponentâ€™s side. The distance error, is the distance between the target position and landing position of the ball for successful strikes.
% \begin{figure}[htb!]%[t]
%         % \centering
%         \begin{minipage}[t!]{0.4\textwidth}
%             \centering 
%             \includegraphics[width=\textwidth]{supplementary_material/figures/Fig_MT_table_tennis_env.pdf}
%         \end{minipage}
%          \hfill
%         \caption[]
%         {\small \textbf{Table tennis environment.} 
%         }
%         \label{fig:tt_env}
%     \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{IMC Details and Hyperparameter}
IMC employs a parameterized inference network and conditional Gaussian distributions to represent experts. For the latter, we also use a fixed variance of $1$ and parameterize the means as shallow neural networks. 
In every M-step, we optimize the experts for $5$ epochs. We add a new component after performing $5$ E- and M-steps or if the lower bound $L(\vpsi, q)$ converges. Moreover, we use automatic per-component curriculum pacing (Section \ref{sec:dual}) for all experiments. For the table tennis and obstacle avoidance task, we experimented with experts with $1$ and $2$ layer neural networks. We found that using $1$ layer with $32$ neurons performs best on the table tennis task and $2$ layer with $64$ neurons for the obstacle avoidance task. For the block pushing and Franka kitchen experiments, we considered $3$ and $4$ layers. We found that $3$ layers with $64$ neurons yield the best results for the block pushing task and $4$ layer with $100$ for Franka kitchen. For all experiments except the kitchen task, we used an effective number of samples $N_s = 50$ to calculate the minimal per-component entropy $\mathcal{H}_{\text{min}}$. For the kitchen task, we used $N_s = 150$. We found that an expert learning rate of $10^{-3}$ leads to good results on all experiments. For the inference network, we used a fixed set of parameters that are listed in Table \ref{table:imc_hp}. For the entropy scaling factor $\eta$ we performed a hyperparameter sweep using Bayesian optimization. The respective values are $\eta = 1/30$ for obstacle avoidance, $\eta \approx 45$ for block pushing and $\eta = 1$ for Franka kitchen and $\eta = 1$ for table tennis. To find an appropriate model complexity we added up to $50$ components for the obstacle avoidance and table tennis task and $30$ for the block pushing and Franka kitchen task. For the former two, the best results were obtained using $50$ components. For the latter $10$ components were sufficient to achieve the results reported in the main manuscript. We always evaluated the model after adding $5$ components.
\begin{table}[htb!]
\caption{\textbf{IMC Hyperparameter.}}
\label{table:imc_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ll}
\toprule
    Parameter & Value \\ 
    \midrule
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Entropy scaling ($\eta$)
    % & $[10^{-2}, 10^{2}]$
    % \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert learning rate
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert batchsize
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert variance ($\sigma^2_\mathbf{y}$)
    & $1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net hidden layer
    & $4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net hidden units
    & $200$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net epochs
    & $800$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net learning rate
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net batchsize
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{supplementary_material/baselines.tex}