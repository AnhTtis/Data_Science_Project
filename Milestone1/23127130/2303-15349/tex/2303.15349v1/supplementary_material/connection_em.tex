\section{Connection to Expectation Maximization}
\label{section:connection_em}
In this section we want to highlight the commonalities and differences between our algorithm and the expectation-maximization (EM) algorithm for mixtures of experts. First, we look at the updates of the variational distribution $q$. Next, we compare the expert optimization. Lastly, we take a closer look at the optimization of the gating distribution.

The EM algorithm sets the variational distribution during the E-step to 
\begin{equation}
    q(o|\mathbf{x}_n) = p(o|\mathbf{x}_n, \mathbf{y}_n) = \frac{p_{\vtheta}(\mathbf{y}_n|\mathbf{x}_n, o)p(o|\mathbf{x}_n)}{\sum_o p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n, o)p(o|\mathbf{x}_n)},
   \label{eq:comp_em_e_step}
\end{equation}
for all samples $n$ and components $o$.
In the M-step, the gating distribution $p(o|\mathbf{x})$ is updated such that the KL divergence between $q(o|\mathbf{x})$ and $p(o|\mathbf{x})$ is minimized. Using the properties of the KL divergence, we obtain a global optimum by setting $p(o|\mathbf{x}_n) = q(o|\mathbf{x}_n)$ for all $n$ and all $o$. This allows us to rewrite Equation \ref{eq:comp_em_e_step} using the recursion in $q$, giving
\begin{equation*}
    q(o|\mathbf{x}_n)^{(i+1)} = \frac{p_{\vtheta}(\mathbf{y}_n|\mathbf{x}_n, o)q(o|\mathbf{x}_n)^{(i)}}{\sum_o p_{\vtheta}(\mathbf{y}_n|\mathbf{x}_n, o)q(o|\mathbf{x}_n)^{(i)}},
\end{equation*}
where $(i)$ denotes the iteration of the EM algorithm. The update for the variational distribution of the IMC algorithm is given by
\begin{align*}
    q(o|\mathcal{D}_n)^{(i+1)} &= \frac{\tilde{p}(\mathcal{D}_n|o)^{(i+1)}}{\sum_o \tilde{p}(\mathcal{D}_n|o)^{(i+1)}} \\
    % &= \frac{\exp(\log p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n, o)/\eta + \log q(o|\mathcal{D}_n)^{(i)}}{\sum_o \exp(\log p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n, o)/\eta + \log q(o|\mathcal{D}_n)^{(i)}} \\
    & = \frac{p_{\vtheta}(\mathbf{y}_n|\mathbf{x}_n, o)^{1/\eta}q(o|\mathcal{D}_n)^{(i)}}{\sum_o p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n, o)^{1/\eta}q(o|\mathcal{D}_n)^{(i)}}.
\end{align*}
Consequently, we see that $q(o|\mathbf{x}) =  q(o |\mathcal{D})$ for $\eta = 1$. 
However, the two algorithms mainly differ in the M-step for the experts: The EM algorithm uses the variational distribution to assign weights to samples, i.e.
\begin{equation*}
    \max_{\vtheta_o} \ \sum_{n=1}^N q(o|\mathbf{x}_n)\log p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n, o),
\end{equation*}
whereas IMC uses the curricula as weights, that is, 
\begin{equation*}
    \max_{\vtheta_o} \ \sum_{n=1}^N p(\mathcal{D}_n|o)\log p_{\vtheta_o}(\mathbf{y}_n|\mathbf{x}_n, o).
\end{equation*}
This subtle difference shows the properties of moment and information projection: In the EM algorithm each sample $\mathbf{x}_n$ contributes to the expert optimization as $\sum_o q(o|\mathbf{x}_n) = 1$. However, if all curricula ignore the $n$th sample, it will not have impact on the expert optimization. Assuming that the curricula ignore samples which the corresponding experts are not able to represent, IMC prevents experts having to average over `too hard' samples. Furthermore, this results in reduced outlier sensitivity as they are likely to be ignored for the expert optimization. Lastly, we highlight the difference between the gating optimization: Assuming that both algorithms train a gating network $g_{\vphi}(o|\mathbf{x})$ we have
\begin{equation*}
    \max_{\vphi}  \ \sum_n \sum_o q(o|\mathbf{x}_n) \log g_{\vphi}(o|\mathbf{x}_n),
\end{equation*}
for the EM algorithm and 
\begin{equation*}
   \max_{\vphi} \  \sum_n \sum_o \tilde{p}(\mathcal{D}_n|o) \log g_{\vphi}(o|\mathbf{x}_n),
\end{equation*}
for IMC. Similar to the expert optimization, EM includes all samples to fit the parameters of the gating network, whereas IMC ignores samples where the unnormalized curriculum weights $\tilde{p}(\mathcal{D}_n|o)$ are zero for all components.