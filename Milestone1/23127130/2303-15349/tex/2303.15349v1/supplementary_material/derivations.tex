\section{Derivations}
\label{appendix:derivations}
\subsection{Lower Bound Decomposition}
\label{appendix:lb_decomp}
To arrive at Equation \ref{eq:decomposition} by marginalizing over the latent variable $o$ for the entropy of the joint curriculum, i.e.,
\begin{align*}
    \mathcal{H}(\mathcal{D}) & = - \sum_n p(\mathcal{D}_n) \log p(\mathcal{D}_n) \\ &
    = - \sum_n p(\mathcal{D}_n) \sum_o p(o|\mathcal{D}_n) \log p(\mathcal{D}_n)
\end{align*}
Next, we use Bayes' theorem, that is, $p(\mathcal{D}_n) = p(o)p(\mathcal{D}_n|o)/ p(o|\mathcal{D}_n)$, giving 
\begin{align*}
\mathcal{H}(\mathcal{D}) = &
- \sum_n p(\mathcal{D}_n) \sum_o p(o|\mathcal{D}_n) \big(
\log p(o) + \log p(\mathcal{D}_n|o)  \\
&- \log p(o|\mathcal{D}_n)
\big).
\end{align*}
Moreover, we add and subtract the log auxiliary distribution $\log q(o|\mathcal{D}_n)$ which yields
\begin{align*}
\mathcal{H}(\mathcal{D}) = &
- \sum_n p(\mathcal{D}_n) \sum_o p(o|\mathcal{D}_n) \big(
\log p(o) + \log p(\mathcal{D}_n|o)  \\
&- \log p(o|\mathcal{D}_n) + \log q(o|\mathcal{D}_n) - \log q(o|\mathcal{D}_n)
\big).
\end{align*}
Rearranging the terms leads and writing the sums in terms of expectations we arrive at
\begin{align*}
    \mathcal{H}(\mathcal{D}) = &
    \E_{p(o)}\big[
    \E_{p(o|\mathcal{D})}[
    \log q(o|\mathcal{D})
    ] 
    + \mathcal{H}(\mathcal{D}|o)
    \big]  + \mathcal{H}(o) \\ & + \KL\big(p(o|\mathcal{D})\Vert q(o|\mathcal{D})\big).
\end{align*}
Lastly, multiplying $\mathcal{H}(\mathcal{D})$ with $\eta$ and adding $\E_{p(o)}\E_{p(\mathcal{D}|o)}[\log p_{\vtheta_o}(\mathbf{y}|\mathbf{x},o)]$
we arrive at Equation \ref{eq:decomposition} which concludes the derivation.