\subsection{Baselines and Hyperparameter}
We now briefly mention the baselines and their hyperparameters. We used Bayesian optimization to tune the most important hyperparameters. 

\textbf{Mixture of Experts trained with Expectation-Maximization (EM).}
The architecture of the mixture of experts model trained with EM \cite{jacobs1991adaptive} is identical to the one optimized with IMC: We employ a parameterized inference network and conditional Gaussian distributions to represent experts. For the latter, we also use a fixed variance of $1$ and parameterize the means as shallow neural networks. For all experiments, we train the model for $100$ EM steps or until the lower bound on the marginal likelihood converges. Equal to IMC, EM trains each expert for $5$ epochs per M-step. However, the two approaches differ in the optimization scheme: While IMC trains the inference network once, EM updates the parameters in every M-step. We therefore optimize the inference network for $8$ epochs resulting in a total of $800$ epochs using $100$ expectation maximization steps which is equivalent to the number used for IMC. Moreover, EM initialized all components at the beginning of the training whereas IMC incrementally adds components during training. For the table tennis and obstacle avoidance task we experimented using experts with $1$ and $2$ layer neural networks. We found that using $1$ layer with $64$ neurons performed best. For the block pushing and Franka kitchen experiments we considered $3$ and $4$ layers. We found that $3$ layers with $100$ neurons yield the best results.
To find an appropriate model complexity we tested up to $50$ components for the obstacle avoidance and table tennis task and $30$ for the block pushing and Franka kitchen task. For the former two, the best results were obtained using $25$ and $50$ components respectively. For the latter $20$ and $25$ components.
 For the remaining hyperparamter choices see Table \ref{table:em_hp}.
\begin{table}[htb!]
\caption{\textbf{EM Hyperparameter.}}
\label{table:em_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ll}
\toprule
    Parameter & Value \\ 
    \midrule
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert learning rate
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert batchsize
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert variance ($\sigma^2_\mathbf{y}$)
    & $1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net hidden layer
    & $4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net hidden units
    & $200$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net epochs
    & $800$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net learning rate
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference net batchsize
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Mixture Density Network (MDN).} 
The mixture density network \cite{bishop1994mixture} uses a shared backbone neural network with multiple heads for predicting component indices as well as the expert likelihood. For the experts, we employ conditional Gaussians with a fixed variance. The model likelihood is maximized in an end-to-end fashion using stochastic gradient ascent. We experimented with different backbone and expert architectures. However, we found that the MDN is not able to partition the input space in a meaningful way, often resulting in sub-optimal outcomes, presumably due to mode averaging. 
To find an appropriate model complexity we tested up to $50$ expert heads for the obstacle avoidance and table tennis task and $30$ for the block pushing and Franka kitchen task. We found that the number of experts heads did not significantly influence the results, further indicating the the MDN is not able to utilize multiple experts to solve sub-tasks. We additionally experimented with a version of the MDN that adds an entropy bonus to the objective \cite{zhou2020movement} to encourage more diverse and multimodal solutions. However, we did not find significant improvements compared to the standard version of the MDN. For a list of hyperparameter choices see \ref{table:mdn_hp}. 
\begin{table}[htb!]
\caption{\textbf{MDN Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:mdn_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.49\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\ 
    \midrule
    Expert hidden layer
    & $\{1, 2\}$
    & $1, 1, 1, 1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert hidden units
    & $\{30, 50\}$
    & $50, 30, 30, 50$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Backbone hid. layer
    & $\{2, 3, 4, 6, 8, 10\}$
    & $3, 2, 4, 3$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Backbone hid. units
    & $\{50, 100, 150, 200\}$
    & $200, 200, 200, 200$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate $\times 10^{-3}$
    & $[0.1, 1]$
    & $5.949, 7.748, 1.299, 2.577$
    \\
    Expert variance ($\sigma^2_\mathbf{y}$)
    & $-$
    & $1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
 }
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Denoising Diffusion Probabilistic Models (DDPM).} 
We consider the denoising diffusion probabilistic model proposed by \cite{ho2020denoising}. Following common practice we parameterize the model as neural network with a sinusoidal positional encoding for the diffusion steps \cite{vaswani2017attention}. Moreover, we use the a cosine-based variance scheduler proposed by \cite{nichol2021improved}. For further details on hyperparameter choices see Table \ref{table:ddpm_hp}. 
% For the block push task, we use 150 hidden units, 16 diffusion steps and 6 hidden layers. For the kitchen task, we use 200 hidden units, 16 diffusion steps and 8 hidden layers.
% For the obstacle avoidance task, we use 200 hidden units, 16 diffusion steps and 6 hidden layers.
\begin{table}[htb!]
\caption{\textbf{DDPM Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order). }
\label{table:ddpm_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.49\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\
    \midrule
    Hidden layer
    & $\{4,6,8, 10, 12\}$
    & $6, 6, 8, 6$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden units
    & $\{50, 100, 150, 200\}$
    & $200, 150, 200, 200$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Diffusion steps
    & $\{5, 15, 25, 50\}$
    & $15, 15, 15, 15$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Variance scheduler
    & $-$
    & cosine
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Normalizing Flow (NF).} For all experiments, we build the normalizing flow by stacking masked autoregressive flows \cite{papamakarios2017masked} paired with permutation layers \cite{papamakarios2021normalizing}. As base distribution, we use a conditional isotropic Gaussian. Following common practice, we optimize the model parameters by maximizing its likelihood. 
See Table \ref{table:nf_hp} for a list of hyperparamters.
% For the kitchen task, we use 200 hidden units, 0.000462 learning rate and 4 num flows.
% For the block push task, we use 150 hidden units, 0.00045 learning rate and 6 num flows.
% For the obstacle avoidance task, we use 100 hidden units, 0.000743 learning rate and 6 num flows.
\begin{table}[htb!]
\caption{\textbf{NF Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:nf_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.49\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\
    \midrule
    Num. flows
    & $\{4,6,8, 10, 12\}$
    & $6, 6, 4, 4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden units per flow
    & $\{50, 100, 150, 200\}$
    & $100, 150, 200, 150$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate $\times 10^{-4}$
    & $[0.01, 10]$
    & $7.43, 4.5, 4.62, 7.67$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Conditional Variational Autoencoder (CVAE).} We consider the conditional version of the autoencoder proposed by \citet{sohn2015learning}. We parameterize the encoder and decoder with a neural network with mirrored architecture. Moreover, we consider an additional scaling factor ($\beta$) for the KL regularization in the lower bound objective of the VAE as suggested by \citet{higgins2017beta}. 
% For the kitchen task, we use 4 hidden layers, 100 hidden units, 16 latent dimension and 0.452 $\KL$ scaling. For the block push task, we use 10 hidden layers, 150 hidden units, 16 latent dimension and 1.008 $\KL$ scaling. For the obstacle avoidance task, we use 8 hidden layers, 100 hidden units, 32 latent dimension and 1.641 $\KL$ scaling.
\begin{table}[htb!]
\caption{\textbf{CVAE Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:cave_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.49\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\ 
    \midrule
    Hidden layer
    & $\{4,6,8, 10, 12\}$
    & $8, 10, 4, 4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden units
    & $\{50, 100, 150, 200\}$
    & $100, 150, 100, 100$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Latent dimension
    & $\{4, 16, 32, 64\}$
    & $32, 16, 16, 16$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $\KL$ scaling ($\beta$)
    & $[10^{-3}, 10^{2}]$
    & $1.641, 1.008, 0.452, 0.698$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Implicit Behavior Cloning (IBC).} 
IBC was proposed by \citet{florence2022implicit} and uses energy-based models to learn a joint distribution over inputs and targets. Following common practice we parameterize the model as neural network. Moreover, we use the version that adds a gradient penalty to the InfoNCE loss \cite{florence2022implicit}. For sampling, we use gradient-based Langevin MCMC \cite{du2019implicit}. Despite our effort, we could not achieve good results with IBC. A list of hyperparameters is shown in Table \ref{table:ibc_hp}.
% For the obstacle avoidance task, we use 200 hidden dim, 4 hidden layers, 0.1662 noise scale and 44 train samples.

\begin{table}[htb!]
\caption{\textbf{IBC Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task and the table tennis task (in this order). We do not get any good results for the block push task and the Franka kitchen task.}
\label{table:ibc_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.49\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value\\ 
    \midrule
    Hidden dim
    & $\{50,100,150,200,256\}$
    & $200, 256$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden layers
    & $\{4,6,8,10\}$
    & $4,6$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Noise scale
    & $[0.1, 0.5]$
    & $0.1662,0.1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Train samples
    & $[8, 64]$
    & $44, 8$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Noise shrink
    & $-$
    & $0.5$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Train iterations
    & $-$
    & $20$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference iterations
    & $-$
    & $40$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-4}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batch size
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Epochs
    & $-$
    & $1000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Behavior Transformer (BET).} Recently, \citet{shafiullah2022behavior} proposed the behavior transformer which employs a minGPT transformer \cite{brown2020language} to predict targets by decomposing them into cluster centers and residual offsets. To obtain a fair comparison, we compare our method to the version with no history. A comprehensive list of hyperparameters is shown in Table \ref{table:bet_hp}.
% For the obstacle avoidance task, we use 3 transformer blocks, 1 offset loss scale, 96 embedding width, 50 number of bins and 4 attention heads. For the block push task, we use 4 transformer blocks, 1 offset loss scale, 72 embedding width, 10 number of bins and 4 attention heads. For the kitchen task, we use 6 transformer blocks, 1 offset loss scale, 120 embedding width, 64 number of bins and 6 attention heads.
\begin{table}[htb!]
\caption{\textbf{BET Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:bet_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.49\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value\\ 
    \midrule
    Transformer blocks
    & $\{2,3,4,6\}$
    & $3, 4, 6, 2$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Offset loss scale
    & $\{1.0,100.0,1000.0\}$
    & $1.0, 1.0, 1.0, 1.0$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Embedding width
    & $\{48,72,96,120\}$
    & $96, 72, 120, 48$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Number of bins
    & $\{8,10,16,32,50,64\}$
    & $50, 10, 64, 64$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Attention heads
    & $\{4,6\}$
    & $4, 4, 6, 4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Context size
    & $-$
    & $1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Training epochs
    & $-$
    & $500$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batch size
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-4}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%