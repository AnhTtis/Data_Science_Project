\section{Related Work}
\label{section:rw}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \textbf{Mixtures of Experts.}
% The mixture of experts model was first proposed by \citet{jacobs1991adaptive} and used expectation maximization \cite{dempster1977maximum} for optimizing the model parameters. 
% Several studies are dedicated to increasing the flexibility of the model \cite{jordan1994hierarchical, waterhouse1998classification, bishop2012bayesian}.
% On another note, \citet{bishop1994mixture} introduced the mixture density network (MDN) which uses a neural network whose parameters are shared between gating and experts, allowing for an end-to-end training using the backpropagation algorithm \cite{rumelhart1986learning}. \citet{zhou2020movement} extended the MDN by adding an entropy bonus to the optimization objective to encourage more diverse and multimodal solutions. 
% All of these works maximize the likelihood to optimize the model parameters which corresponds to a moment projection. This differs from our approach which is inspired by the information projection. Recently, \citet{becker2020expected} introduced expected information maximization (EIM), an approach for computing the expected information projection based on samples from a dataset.
% While EIM was mainly introduced in the context of density estimation, the authors mention that the algorithm is also applicable to conditional models. However, EIM relies on an intermediate density ratio estimation step, causing stability issues and preventing scaling to high dimensional problems. We, therefore, do not consider EIM as a competitive baseline. For an elaborate survey on mixture of experts models, the reader is referred to \citet{yuksel2012twenty} and \citet{masoudnia2014mixture}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Imitation Learning.}
A variety of algorithms in imitation learning \cite{osa2018algorithmic, argall2009survey} can be grouped into two categories: Inverse reinforcement learning \cite{zakka2022xirl, ziebart2008maximum}, which extracts a reward function from demonstrations and optimizes a policy subsequently, and behavioral cloning, which directly extracts a policy from demonstrations.
Many works approach the problem of imitation learning by considering behavior cloning as a distribution-matching problem, in which the state distribution induced by the policy is required to align with the state distribution of the expert data.
Some methods \cite{ho2016generative, fu2018learning} are based on adversarial methods inspired by Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}. A policy is trained to imitate the expert while a discriminator learns to distinguish between fake and expert data. However, these methods are not suitable for our case as they involve interacting with the environment during training.
Other approaches focus on purely offline training and use various policy representations such as energy-based models \cite{florence2022implicit}, normalizing flows \cite{papamakarios2021normalizing, singh2021parrot}, conditional variational autoencoders (CVAEs) \cite{sohn2015learning, mees2022matters}, transformers \cite{shafiullah2022behavior}, or diffusion models \cite{song2019generative, ho2020denoising, song2021scorebased, chi2023diffusion, pearce2023imitating, reuss2023goal}. These models can represent multi-modal expert distributions but are optimized based on the M-Projection, which leads to a performance decrease. 
Recent works \cite{freymuth2022inferring, li2023curriculum} have proposed training Mixture of Experts models with an objective similar to ours. However, the work by \cite{freymuth2022inferring}  requires environment-specific geometric features, which is not applicable in our setting, whereas the work by \cite{li2023curriculum} considers linear experts and the learning of skills parameterized by motion primitives \cite{paraschos2013probabilistic}. For a detailed differentiation between our work and the research conducted by \cite{li2023curriculum}, please refer to Appendix \ref{section:connection_mlcur}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Curriculum Learning.}
The authors of \cite{bengio2009curriculum} introduced curriculum learning (CL) as a new paradigm for training machine learning models by gradually increasing the difficulty of samples that are exposed to the model. Several studies followed this definition \cite{spitkovsky2009baby, soviany2022curriculum, chen2015webly, tudor2016hard, pentina2015curriculum, shi2015recurrent, zaremba2014learning}. Other studies used the term curriculum learning for gradually increasing the model complexity \cite{karras2017progressive, morerio2017curriculum, sinha2020curriculum} or task complexity \cite{caubriere2019curriculum, florensa2017reverse, lotter2017multi, sarafianos2017curriculum}. 
All of these approaches assume that the difficulty-ranking of the samples is known a-priori.
In contrast, we consider dynamically adapting the curriculum according to the learning progress of the model which is known as self-paced learning (SPL). 
Pioneering work in SPL was done in \cite{ kumar2010self} which is related to our work in that the authors propose to update the curriculum as well as model parameters iteratively. However, their method is based on maximum likelihood which is different from our approach. Moreover, their algorithm is restricted to latent structural support vector machines. 
% Less related work in SPL include work by \citet{jiang2014self} which encourages sample diversity in the curriculum and \citet{zhang2015self} which incorporates prior knowledge in the curriculum. More recent work applied SPL to the domain of reinforcement learning \cite{klink2020self, klink2020self2, celik2022specializing}.
For a comprehensive survey on curriculum learning, the reader is referred to \cite{soviany2022curriculum}.

