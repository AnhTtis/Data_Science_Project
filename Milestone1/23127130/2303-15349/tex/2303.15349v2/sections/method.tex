\section{Information Maximizing Curriculum}
\label{section:imc}
% In this section, we propose IMC, a novel algorithm for training mixtures of experts that builds on the information projection as well as self-paced learning to generate a curriculum for the individual experts that allows the model to ignore samples that it is not able to represent. To that end, we formalize an objective for a single expert...
% In this section, we propose IMC, a novel algorithm for training mixtures of experts which leverages curriculum learning and in particular self-paced learning to soft-assign samples to expert models. Building on the information projection for performing such an assignment allows the model to ignore samples that it is not able to represent resulting in reduced outlier-sensitivity and averaging problems compared to existing moment-projection based approaches. 
In this section, we propose Information Maximizing Curriculum (IMC), a novel algorithm for training mixtures of expert polices. We motivate our optimization objective using a single policy. Next, we generalize the objective to support learning a mixture of experts policy. Thereafter, we discuss the optimization scheme and provide algorithmic details. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/illustration_fig.tex}
\subsection{Objective for a Single Expert Policy}
\label{sec:single_expert}
We propose an objective that jointly learns a curriculum $\cur$ and a parameterized policy $p_{\vtheta}(\act|\obs)$ with parameters $\vtheta$. The curriculum is a categorical distribution over samples of a dataset $ \{(\obs_n, \act_n)\}_{n=1}^N$, assigning probability mass to samples according to the performance of the policy. 
% Moreover, the curriculum expands as the expert specializes to a subset of the data. 
To allow the curriculum to ignore samples that the policy cannot represent, we build on the I-projection (see Equation \ref{eq:i_proj}). We, therefore, formulate the objective function as
% We formulate this as
\begin{equation}
     \tilde{J}(\cur, \vtheta) = \excur[\log p_{\vtheta}(\act|\obs)] + \eta \entcur,
    \label{eq:single_expert_{\comp}bj}
\end{equation}
which is optimized for $\cur$ and $\vtheta$ in an alternating fashion using coordinate ascent \cite{boyd2004convex}. We additionally introduce a trade-off factor $\eta$ that determines the pacing of the curriculum. For $\eta \rightarrow \infty$ the curriculum becomes uniform, exposing all samples to the policy and hence reducing to maximum likelihood estimation for $\vtheta$. In contrast, if $\eta \rightarrow 0$ the curriculum concentrates on samples where the policy log-likelihood $\log p_{\vtheta}(\act|\obs)$ is highest.
The objective can be solved in closed form for $\cur$ (see Appendix \ref{appendix:derivations}), resulting in 
\begin{equation*}
    \optcurn \propto p_{\vtheta}(\act_n|\obs_n)^{1/\eta}.
\end{equation*}
Maximizing the objective w.r.t $\vtheta$ reduces to a weighted maximum-likelihood estimation, that is,
\begin{equation*}
    \vtheta^* = \argmax_{\vtheta} \sum_n \curn\log p_{\vtheta}(\act_n|\obs_n).
\end{equation*}
The optimization is repeated until reaching a local maximum, indicating that the curriculum has selected a fixed set of samples where the policy attains high log-likelihoods $\log p_{\vtheta}(\act|\obs)$. Proposition \ref{thm:single} establishes convergence guarantees, the details of which are elaborated upon in Appendix \ref{proof:conv_single}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}
\label{thm:single}
Let $\tilde{J}$ be defined as in Equation \ref{eq:single_expert_{\comp}bj} and $0 < \eta < \infty$. Under mild assumptions on $p_{\vtheta}$ and the optimization scheme for $\vtheta$, it holds for all $i$, denoting the iteration index of the optimization process, that
\begin{equation*}
\tilde{J}(\cur^{(i+1)}, \vtheta^{(i+1)}) \geq \tilde{J}(\cur^{(i)}, \vtheta^{(i)}),
\end{equation*}
where equality indicates that the algorithm has converged to a local optimum.
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The ability to ignore samples for which the policy is not able to achieve good performance alleviates the mode-averaging problem.
% However, the downside of a single expert policy is the arbitrarily poor performance at samples that are ignored. This problem is alleviated by introducing multiple experts that specialize in different subsets of the data. 
The capacity to disregard samples where the policy cannot achieve satisfactory performance mitigates the mode-averaging problem. Nevertheless, a drawback of employing a single expert policy is the potential for significantly suboptimal performance on the ignored samples. This limitation is overcome by introducing multiple experts, each specializing in different subsets of the data.

% The curriculum allows the policy to ignore samples for which it cannot achieve high log-likelihoods, thus alleviating the mode-averaging problem. 
% These properties are illustrated in Figure \ref{fig:illustration_m} and \ref{fig:illustration_i}.
% For a visualization see Figure \ref{fig:outlier_zero_forc}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Objective for a Mixture of Experts Policy}
\label{sec:mmoe}
% \textbf{Objective for a Modular Mixture of Specialized Experts.}
Assuming limited complexity, a single expert policy is likely to ignore a large amount of samples due to the zero-forcing property of the I-projection. Using multiple curricula and policies that specialize to different subsets of the data is hence a natural extension to the single policy model. To that end, we make two major modifications to Equation \ref{eq:single_expert_{\comp}bj}: Firstly, we use a mixture model with multiple components $z$ where each component has its own curriculum, i.e., $\cur = \sum_{\comp} p(\comp) \curpc$. Secondly, we employ an expert policy per component $p_{\vtheta_{\comp}}(\act|\obs,\comp)$, that is paced by the corresponding curriculum $\curpc$. The resulting objective function is given by
\begin{equation}
    J(\vpsi) = \exz\excurpc[\log p_{\vtheta_{\comp}}(\act|\obs,\comp)] + \eta \entcur,
    \label{eq:before_decomposition}
\end{equation}
where $\vpsi$ summarizes the dependence on $p(\comp)$, $\{\curpc\}_{\comp}$ and $\{\vtheta_{\comp}\}_{\comp}$.
However, Equation \ref{eq:before_decomposition} is difficult to optimize as the entropy of the mixture model prevents us from updating the curriculum of each component independently. Similar to \cite{arenz2018efficient}, we introduce an auxiliary distribution $q(\comp|\oa)$ to decompose the objective function into a lower bound $L(\vpsi, q)$ and an expected $\KL$ term, that is,
\begin{equation}
   J(\vpsi) = L(\vpsi, q) + \eta \excur\KL(p(\comp|\oa) \Vert q(\comp|\oa)),
   \label{eq:decomposition}
\end{equation}
with $p(\comp|\oa) = p(\oa|\comp)p(\comp)/\cur$ and 
\begin{equation*}
    L(\vpsi, q) = \exz\big[\underbrace{\excurpc[R_{\comp}(\oa)] + \eta\mathcal{H}(\oa|\comp)}_{J_{\comp}(p(\oa|\comp), \vtheta_\comp)}]\big] + \eta \mathcal{H}(\comp), %+ \KL ( p(x|\comp) \Vert p_{c_{\comp}}(x|\comp) )
\end{equation*}
with $R_{\comp}(\oan) = \log p_{\vtheta_{\comp}}(\act_n|\obs_n,\comp) + \eta \log q(\comp|\oan)$, allowing for independent updates for $p(\oa|\comp)$ and $\vtheta_{\comp}$ by maximizing the per-component objective function $J_{\comp}(p(\oa|\comp), \vtheta_\comp)$. 
% Note that the lower bound decomposition holds for any distribution  $q(\comp|\oa)$.
%[Mention why we use the notation $R$. Make a connection to policy search here. Do we need to proof this result?] 
A derivation can be found in Appendix \ref{appendix:lb_decomp}.
 Since $\E_{\cur}\KL(p(\comp|\oa) \Vert q(\comp|\obs)) \geq 0$, $L$ is a lower bound on $J$ for $\eta \geq 0$. 
 Please note that the per-component objective function $J_{\comp}$ is very similar to Equation \ref{eq:single_expert_{\comp}bj}, with $J_{\comp}$ including an additional term, $\log q(\comp|\oa)$, which serves the purpose of preventing different curricula from assigning probability mass to the same set of samples: Specifically, a component $z$ is considered to `cover' a datapoint $(\obs_n, \act_n)$ when $q(z|\obs_n, \act_n) \approx 1$. Since $\sum_z q(z|\obs_n, \act_n) = 1$, it follows that for other components $z' \neq z$ it holds that $q(z'|\obs_n, \act_n) \approx 0$. Consequently, $\log q(z'|\obs_n, \act_n) \rightarrow -\infty$, implying that $R_{z'}(\obs_n, \act_n) \rightarrow -\infty$. As a result, the other curricula effectively disregard the datapoint, as $p(\obs_n, \act_n|z') \propto \exp R_{z'}(\obs_n, \act_n) \approx 0$.
 % This property is further illustrated in Figure \ref{fig:illustration_joint}. 
 %The decomposition used in Equation \ref{eq:decomposition} is closely related to the decomposition used for the expectation-maximization (EM) algorithm \cite{dempster1977maximum}.
 %[Mention that EM is for the forward KL whereas we use the decomposition on the reverse KL] 
 
 We follow the optimization scheme of the expectation-maximization algorithm \cite{dempster1977maximum}, that is, we iteratively maximize (M-step) and tighten the lower bound (E-step) $L(\vpsi, q)$.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maximizing the Lower Bound (M-Step)}
\label{sec:m_step}
We maximize the lower bound $L(\vpsi, q)$ with respect to the mixture weights $p(\comp)$, curricula $p(\oa|\comp)$ and expert policy parameters $\vtheta_{\comp}$.  We find closed form solutions for both, $p(\comp)$ and $p(\oa|\comp)$ given by
\begin{equation}
\label{eq:opt_mw}
    p^*(\comp) \propto \exp\big( \excurpc[{R_{\comp}(\oa)}/{\eta}] + \mathcal{H}(\oa|\comp)\big), 
    \text{ and } 
    \tilde{p}(\oan|\comp) = \exp\big( {R_{\comp}(\oan)}/{\eta}\big),
\end{equation}
where $\tilde{p}(\oan|\comp)$ are the optimal unnormalized curricula, such that holds $p^*(\oan|\comp) = \tilde{p}(\oan|\comp) / \sum_n \tilde{p}(\oan|\comp)$ . However, due to the hierarchical structure of $L(\vpsi, q)$ we implicitly optimize for $p(\comp)$ when updating the curricula. 
This result is frequently used throughout this work and formalized in Proposition \ref{thm:implicit}. A proof can be found in Appendix \ref{proof:implicit}.
% This result is frequently used throughout this work and is formalized in Proposition \ref{thm:implicit}. A proof is found in Appendix \ref{proof:implicit}.
%
\begin{proposition}
\label{thm:implicit}
Let $p^*(\comp)$ and $\tilde{p}(\oa|\comp)$ be the optimal mixture weights and unnormalized curricula for maximizing $L(\vpsi, q)$. It holds that
\begin{equation*}
    p^*(\comp) = \sum_n \tilde{p}(\oan|\comp) / \sum_{\comp} \sum_n \tilde{p}(\oan|\comp).
\end{equation*}
\end{proposition}
The implicit updates of the mixture weights render the computation of $p^*(\comp)$ obsolete, reducing the optimization to computing the optimal (unnormalized) curricula $\tilde{p}(\oa|\comp)$ and expert policy parameters $\vtheta^*_{\comp}$. In particular, this result allows for training the policy using mini-batches and thus greatly improves the scalability to large datasets as explained in Section \ref{section:algo_details}.
% Indeed, the mixture weights are not required for training the mixture of experts. Consequently, the optimization reduces to computing the optimal curricula $p^*(\oa|\comp)$ and expert parameters $\vtheta^*_{\comp}$.
% \begin{proof}
% A proof is found in Appendix XX.
% \end{proof}
Maximizing the lower bound with respect to $\vtheta^*_{\comp}$ results in a weighted maximum likelihood estimation, i.e.,  
\begin{equation}
\label{eq:expert_update}
  \vtheta^*_{\comp} =\argmax_{\vtheta_{\comp}} \ \sum_n \tilde{p}(\oan|\comp) \log p_{\vtheta_{\comp}}(\act_n|\obs_n,\comp),
\end{equation}
where the curricula $\tilde{p}(\oan|\comp)$ assign sample weights. For further details on the M-step, including derivations of the closed-form solutions and the expert parameter objective see Appendix \ref{appendix:expert_obj}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tightening the Lower Bound (E-Step)}
\label{sec:e_step}
Tightening of the lower bound (also referred to as E-step) is done by minimizing the expected Kullback-Leibler divergence in Equation \ref{eq:decomposition}. Using the properties of the KL divergence, it can easily be seen that the lower bound is tight if for all $n\in \{1,...,N\}$ $q(\comp|\obs_n) = p(\comp|\oan)$ holds. To obtain $p(\comp|\oan)$ we leverage Bayes' rule, that is, $p(\comp|\oan)=p^*(\comp)p^*(\oan|\comp)/\sum_{\comp} p^*(\comp)p^*(\oan|\comp)$. Using Proposition \ref{thm:implicit} we find that
% \begin{corollary}
% \label{thm:gating}
\begin{equation*}
    p(\comp|\oan) = \tilde{p}(\oan|\comp) / \sum_{\comp} \tilde{p}(\oan|\comp).
\end{equation*}
% \end{corollary}
 Please note that the lower bound is tight after every E-step as the KL divergence is set to zero. Thus, increasing the lower bound $L$ maximizes the original objective $J$ assuming that updates of $\vtheta_{\comp}$ are not decreasing the expert policy log-likelihood $\log p_{\vtheta_{\comp}}(\act|\obs,\comp)$.
 % [Mention that since we can make the lower bound tight, we increase the original objective $J$ when increasing the lower bound $L$]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Automatic Per-Component Curriculum Pacing}
% \label{sec:dual}
% Choosing a fixed curriculum pacing value $\eta$ for applications where entropy and expert log-likelihood values change heavily during training can be a limiting assumption and might lead to sub-optimal results. Moreover, pacing all curricula with the same $\eta$ can result in curricula that cover large subsets of the data while others degenerate, i.e., only cover few samples. 
% In order to obtain an adaptive per-component curriculum pacing and alleviate problems with degrading curricula, we enforce a lower bound $\mathcal{H}_{\text{min}}$ on the entropy of the individual curricula. To that end, we frame the per-component objective $J_{\comp}$ as constraint optimization problem, giving
% \begin{equation*}
%     \argmax_{p(\oa|\comp)} \ \excur[R_{\comp}(\oa)] + \eta \mathcal{H}(\oa|\comp), \ \text{s.t.} \ \mathcal{H}(\oa|\comp) \geq  \mathcal{H}_{\text{min}}.
% \end{equation*}
% Using Lagrange duality \cite{boyd2004convex} we obtain a closed form solution by optimizing the Lagrangian function given by
% \begin{equation*}
%     p^*_{\xi_{\comp}}(\oa|\comp) \propto \exp\Big( \frac{R_{\comp}(\oa)}{\xi_{\comp} + \eta}\Big),
% \end{equation*}
% with per-component Lagrangian multiplier $\xi_{\comp}$. The optimal value $\xi^*_{\comp}$ is obtained by minimizing the Lagrangian dual function $g(\xi_\comp)$, that is,
% % \begin{equation*}
%     $\xi_{\comp}^* = \argmin_{\xi_{\comp} > 0} \ g(\xi_\comp)$
% % \end{equation*}
% with
% \begin{equation*}
% % \resizebox{0.45\textwidth}{!}{
%     g(\xi_\comp) = \xi_{\comp} \Big( \log \sum_n \exp\Big( \frac{R_{\comp}(\oan)}{\xi_{\comp} + \eta}\Big)
%     -  \mathcal{H}_{\text{min}}\Big),
%     % }
% \end{equation*}
% and thus $p^*(\oa|\comp) = p^*_{\xi^*_{\comp}}(\oa|\comp)$. Please note that $\xi_{\comp}^*$ is obtained using a convex numerical optimizer. 
% % For a derivation of the Lagrangian dual function see Appendix \ref{appendix:derivations:lagrange}. 
% The minimum per-component entropy $\mathcal{H}_{\text{min}}$ is intuitive to choose as it translates into the number of samples $N_s$ that a single curriculum should cover (uniformly) by setting $\mathcal{H}_{\text{min}} = \log N_s$. 

% [Quick intro. Motivate chapter new since we have a soft and hard constraint now]
% Choosing the curriculum pacing $\eta$ is difficult in applications where entropy and expert log-likelihood values change heavily during training. Furthermore, pacing all curricula with the same $\eta$ value for all components can be a limiting assumption and might lead to sub-optimal results. Moreover, for datasets that require an intractable amount of components to cover all samples it can be preferable to trade-off the performance with computational complexity to enforce less components to cover all samples. To address these points we frame the per-component objective as constraint optimization problem to enforce a minimum per-component entropy $\mathcal{H}_{\text{min}}$, giving
% \begin{equation*}
%     \argmax_{p(\oa|\comp)} \ \excur[R_{\comp}(\oa)] + \eta \mathcal{H}(\oa|\comp), \ \text{s.t.} \ \mathcal{H}(\oa|\comp) \geq  \mathcal{H}_{\text{min}}.
% \end{equation*}
% Using Lagrange duality (\cite{boyd2004convex}) we obtain a closed form solution for $p(\oa|\comp)$ by optimizing the Lagrangian function given by
% \begin{equation*}
%     p^*_{\xi_{\comp}}(\oa|\comp) \propto \exp\Big( \frac{R_{\comp}(\oa)}{\xi_{\comp} + \eta}\Big),
% \end{equation*}
% with per-component dual variable $\xi_{\comp}$. The optimal dual variable $\xi^*_{\comp}$ is obtained by minimizing the Lagrangian dual function $g(\xi_\comp)$, that is,
% % \begin{equation*}
%     $\xi_{\comp}^* = \argmin_{\xi_{\comp} > 0} \ g(\xi_\comp)$
% % \end{equation*}
% with
% \begin{equation*}
% % \resizebox{0.45\textwidth}{!}{
%     g(\xi_\comp) = \xi_{\comp} \Big( \log \sum_n \exp\Big( \frac{R_{\comp}(\oan)}{\xi_{\comp} + \eta}\Big)
%     -  \mathcal{H}_{\text{min}}\Big),
%     % }
% \end{equation*}
% and thus $p^*(\oa|\comp) = p^*_{\xi^*_{\comp}}(\oa|\comp)$. Please note that $\xi_{\comp}^*$ is obtained using a convex numerical optimizer. 
% % For a derivation of the Lagrangian dual function see Appendix \ref{appendix:derivations:lagrange}. 
% The minimum per-component entropy $\mathcal{H}_{\text{min}}$ is intuitive to choose as it translates into the number of samples $N_s$ that a single component should cover (uniformly) by setting $\mathcal{H}_{\text{min}} = \log N_s$. 
%Furthermore, the Lagrangian dual formulation introduces a per-component dual variable $\xi_{\comp}$ and therefore allows for varying entropy scaling factors between components.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Adding Components to the Model}
\subsection{Algorithmic Details}
\label{section:algo_details}
\textbf{Convergence Guarantees.} Proposition \ref{thm:multiple}
establishes convergence guarantees for the mixture of experts policy objective $J$. The proof mainly relies on the facts that IMC has the same convergence guarantees as the EM algorithm and that Proposition \ref{thm:single} can be transferred to the per-component objective $J_z$. The full proof is given in Appendix \ref{proof:conv_single}.

\begin{proposition}
\label{thm:multiple}
Let ${J}$ be defined as in Equation \ref{eq:decomposition} and $0 < \eta < \infty$. Under mild assumptions on $p_{\vtheta_z}$ and the optimization scheme for $\vtheta_z$, it holds for all $i$, denoting the iteration index of the optimization process, that
\begin{equation*}
{J}(\vpsi^{(i+1)}) \geq {J}(\vpsi^{(i)}),
\end{equation*}
where equality indicates that the IMC algorithm has converged to a local optimum.
\end{proposition}


\textbf{Stopping Criterion.} We terminate the algorithm if either the maximum number of training iterations is reached or if the lower bound $L(\vpsi, q)$ converges, i.e., 
\begin{equation*}
   |\Delta L| = |L^{(i)}(\vpsi, q)- L^{(i-1)}(\vpsi, q)| \leq \epsilon,
\end{equation*}
with threshold $\epsilon$ and two subsequent iterations $(i)$ and $(i-1)$.  The lower bound can be evaluated efficiently using Corollary \ref{thm:lowerbound}.
\begin{corollary}[thm:implicit]
\label{thm:lowerbound}
Consider the setup used in Proposition \ref{thm:implicit}. For $p^*(\comp) \in \vpsi$ and  $\{p^*(\oa|\comp)\}_{\comp} \in \vpsi$ it holds that
\begin{equation*}
    L\big(\vpsi, q\big)  = \eta \log \sum_{\comp} \sum_n \tilde{p}(\oan|\comp).
\end{equation*}
\end{corollary}
% \begin{equation*}
% % L(\vpsi, q)  = \log \sum_{\comp} \sum_n \tilde{p}(\oan|\comp).
% L\big(p^*(\comp), \{p^*(\oa|\comp)\}_{\comp},\{\vtheta_{\comp}\}_{\comp}, q\big)  = \log \sum_{\comp} \sum_n \tilde{p}(\oan|\comp).
% \end{equation*}
See Appendix \ref{proof:lowerbound} for a proof.

\textbf{Inference.} In order to perform inference, i.e., sample actions from the policy, we need to access the gating distribution for arbitrary observations $\obs \in \mathcal{O}$ which is not possible as $p(\comp|\oa)$ is only defined for observations contained in the dataset $\oa$. We therefore leverage Corollary \ref{thm:gating} to learn an inference network $g_{\vphi}(\comp|\obs)$ with parameters $\vphi$ by minimizing the KL divergence between $p(\comp|\obs)$ and $g_{\vphi}(\comp|\obs)$ under $p(\obs)$ (see Appendix \ref{proof:gating} for a proof).
\begin{corollary}[thm:implicit]
\label{thm:gating}
Consider the setup used in Proposition \ref{thm:implicit}. 
%  For $p(\comp|\oa) \propto p^*(\comp)p^*(\oa|\comp)$ and 
% $p({\oa})=\sum_{\comp} p^*(\comp)p^*(\oa|\comp)$
% $p({\comp, \obs})= \int_{\mathcal{A}} p({\comp, \oa}) \text{d}\act$
It holds that
    \begin{equation*}
    \min_{\vphi} \E_{p({\obs})}\KL\big(p(\comp|\obs)\Vert g_{\vphi}(\comp|\obs)\big)  = \ 
     \max_{\vphi} \sum_n \sum_{\comp} \tilde{p}(\oan|\comp) \log g_{\vphi}(\comp|\obs_n).
\end{equation*}
\end{corollary}
Once trained, the inference network can be used for computing the exact log-likelihood as $p(\act|\obs) = \sum_{\comp} g_{\vphi}(\comp|\obs) p_{\vtheta_{\comp}}(\act|\obs, \comp)$ or sampling a component index, i.e.,  $z' \sim g_{\vphi}(\comp|\obs)$. 


\textbf{Mini-Batch Updates.} 
%  M- and E-step only require access to the unnormalized curricula $\tilde{p}(\oa|\comp)$. As a consequence, we do not need to compute the normalization constant $\sum_n \tilde{p}(\oan|\comp)$, allowing to complete the training procedure using mini-batches, rather than the entire dataset. This enables the algorithm to efficiently scale to large datasets. The full training procedure is outlined in Algorithm \ref{algo:imc_training}.
Due to Proposition \ref{thm:implicit}, the M- and E-step in the training procedure only rely on the unnormalized curricula $\tilde{p}(\oa|\comp)$. Consequently, there is no need to compute the normalization constant $\sum_n \tilde{p}(\oan|\comp)$. This allows us to utilize mini-batches instead of processing the entire dataset, resulting in an efficient scaling capability for large datasets. Please refer to Algorithm \ref{algo:imc_training} for a detailed description of the complete training procedure.
\begin{wrapfigure}{R}{0.63\textwidth}
\vspace{-0.85cm}
    \begin{minipage}{0.63\textwidth}
        \input{sections/pseudocode.tex}
    \end{minipage}
    % \vspace{-1.3cm}
\end{wrapfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% using a modular gating network which has output dimension equal to the maximal number of components $N_{o, \textrm{max}}$ that we intend to add to the mixture model. Additionally, we employ a mask that is multiplied with the gating output before applying the softmax transformation. The mask is a binary vector where the first $N_{\comp}$ entries are $1$ and the rest $0$. Hence, when adding a component, we simply set the corresponding mask entry to $1$, allowing us to preserve the learned representation for the other components.

% In order to perform inference, i.e., sample from the model or compute expectations, we need to access the gating distribution for arbitrary inputs $\obs$ which is not possible as $p(\comp|\oa)$ is only defined for samples contained in $\oa$. We therefore learn a inference network $g_{\vphi}(\comp|\obs)$ with parameters $\vphi$ by minimizing the KL divergence between $p(\comp|\oa)$ and $g_{\vphi}(\comp|\obs)$ under the joint curriculum $\cur$, that is,
% \begin{align*}
%     &\min_{\vphi} \E_{p({\oa})}\KL\big(p(\comp|\oa)\Vert g_{\vphi}(\comp|\obs)\big) \\ = \ 
%     & \max_{\vphi} \sum_n \sum_{\comp} \tilde{p}(\oa|\comp) \log g_{\vphi}(\comp|\obs_n).
% \end{align*}
% [PROOF. (Mention that uncovered samples do not impact the gating optimization?)]
% The inference network replaces the gating distribution $p(\comp|\obs)$ for inference, that is
% \begin{equation}
%     p(\act|\obs) = \sum_{\comp} g_{\vphi}(\comp|\obs) p_{\vtheta}(\act|\obs,\comp).
%     \label{eq:marg_lh}
% \end{equation}
% [Remove the following part since the modular gating network leads to performance issues. Instead, mention that training the inference network is only done once at the end of training and therefore does not interfer with the concept of modularity.]
% Training a gating network can be cumbersome for applications where we often switch between training and inference (e.g. due to new data acquisition), 
% since adding components would require learning a new gating network from scratch due to varying output dimensions. We counteract this problem by using a modular gating network which has output dimension equal to the maximal number of components $N_{o, \textrm{max}}$ that we intend to add to the mixture model. Additionally, we employ a mask that is multiplied with the gating output before applying the softmax transformation. The mask is a binary vector where the first $N_{\comp}$ entries are $1$ and the rest $0$. Hence, when adding a component, we simply set the corresponding mask entry to $1$, allowing us to preserve the learned representation for the other components.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%