\begin{abstract}
Imitation learning uses data for training policies to solve complex tasks. However, when the training data is collected from human demonstrators, it often leads to multimodal distributions because of the variability in human actions. Most imitation learning methods rely on a maximum likelihood (ML) objective to learn a parameterized policy, but this can result in suboptimal or unsafe behavior due to the mode-averaging property of  the ML objective. In this work, we propose \textit{Information Maximizing Curriculum}, a curriculum-based approach that assigns a weight to each data point and encourages the model to specialize in the data it can represent, effectively mitigating the mode-averaging problem by allowing the model to ignore data from modes it cannot represent. To cover all modes and thus, enable diverse behavior, we extend our approach to a mixture of experts (MoE) policy, where each mixture component selects its own subset of the training data for learning. A novel, maximum entropy-based objective is proposed to achieve full coverage of the dataset, thereby enabling the policy to encompass all modes within the data distribution. We demonstrate the effectiveness of our approach on complex simulated control tasks using diverse human demonstrations, achieving superior performance compared to state-of-the-art methods.

% Imitation learning is a promising approach for teaching robots new skills. However, when demonstrations show the same task being performed in multiple ways, the resulting multi-modal action distributions can pose significant challenges. Most imitation learning methods rely on a maximum likelihood objective for model optimization, which can lead to mode-averaging in the action space and thus suboptimal or even dangerous behavior. In this paper, we propose \textit{Information Maximizing Curriculum}, a curriculum-based approach that assigns a weight to each data point and encourages the model to specialize in the data it can represent. Our approach significantly alleviates the mode-averaging problem by allowing the model to ignore data from modes it cannot represent, leading to safer behavior. We extend our approach to a Mixture of Experts model to learn versatile skills, where each component selects its own subset of training data while maximizing coverage of the full dataset through a maximum entropy objective. We evaluate our approach on complex simulated control tasks using versatile human demonstrations and show its superiority over state-of-the-art methods.

% Mixtures of Experts (MoE) are known for their ability to learn complex conditional distributions with multiple modes. However, despite their potential, these models are challenging to train and often tend to produce poor performance, explaining their limited popularity. Our hypothesis is that this under-performance is a result of the commonly utilized maximum likelihood (ML) optimization, which leads to mode averaging and a higher likelihood of getting stuck in local maxima.
% We propose a novel curriculum-based approach to learning mixture models in which each component of the MoE is able to select its own subset of the training data for learning. This approach allows for independent optimization of each component, resulting in a more modular architecture that enables the addition and deletion of components on the fly, leading to an optimization less susceptible to local optima. The curricula can ignore data-points from modes not represented by the MoE, reducing the mode-averaging problem. To achieve a good data coverage, we couple the optimization of the curricula with a joint entropy objective and optimize a lower bound of this objective.
% We evaluate our curriculum-based approach on a variety of multimodal behavior learning tasks and demonstrate its superiority over competing methods for learning MoE models and conditional generative models. 

%Our approach utilizes relatively simple deep neural network (DNN) architectures, which are cheap at inference time, as components and still achieves improved performance.
%Mixture of Expert (MoE) models are powerful models for learning conditional distributions with several modes. However, MoE models are hard to train and often yield sub-optimal prediction models in comparison to current SOTA methods and are hence not very popular. We postulate that the rather poor performance is caused by the commonly used maximum likelihood (ML) optimization for MoE models, which causes mode averaging and is prone to getting stuck in local maxima.  
%We propose a curriculum-based approach to learning mixture models where each component can select its own  subset of the training data to be used for learning. The curricula allow independent optimization of each component, enabling modular architectures where components can be added and deleted on the fly, resulting in an optimization less prone to local optima. Moreover, data-points from modes not represented by the MoE can be ignored by the curriculum which diminishes the mode-averaging problem. The optimization of the curricula is coupled by an entropy objective which, given sufficient components, attempts to cover the whole dataset with responsible components.   
% We test our approach on various multi-modal imitation learning tasks and show that, already with rather simple DNN architectures used as components, it outperforms current SOTA methods for learning MoE models as well as more general conditional generative models. 
 
\end{abstract}