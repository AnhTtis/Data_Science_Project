\section{Preliminaries}
\definecolor{forestgreen4416044}{RGB}{44,160,44}
\definecolor{steelblue31119180}{RGB}{31,119,180}
Our approach heavily builds on minimizing Kullback-Leibler divergences as well as mixtures of expert policies. Hence, we will briefly review both concepts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Moment and Information Projection}
The Kullback-Leibler (KL) divergence \cite{kullback1951information} is a similarity measure for probability distributions and is defined as $\KL ( p\Vert p') = \sum_{\mathbf{x}} p(\mathbf{x}) \log p(\mathbf{x})/p'(\mathbf{x})$ for a discrete random variable $\mathbf{x}$. Due to its asymmetry, the KL divergence offers two different optimization problems for fitting a model distribution $p$ to a target distribution $p^*$ \cite{murphy2012machine}, that is,
\begin{equation*}
\begin{aligned}
\underbrace{\argmin_{p}\KL ( p^*\Vert p)}_{\text{M(oment)-Projection}} \quad
\end{aligned}
 \begin{minipage}[t!]{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/m_proj_vis.pdf}
            % \captionof{figure}{Information}
            \vspace{0.03cm}
\end{minipage}
\begin{aligned}
\quad \text{and} \quad 
\underbrace{\argmin_{p}\KL  ( p \Vert p^*)}_{\text{I(nformation)-Projection}}. \quad
\end{aligned}
 \begin{minipage}[t!]{0.2\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/i_proj_vis.pdf}
            \vspace{0.05cm}
            % \captionof{figure}{Moment}
\end{minipage}
\end{equation*}
% \begin{equation*}
% \begin{aligned}
% \underbrace{\min_{p}\KL ( {p^*}\Vert {p}) = \max_p \E_{p^*}[\log p(\obs)]}_{\text{M(oment)-Projection}}
% \end{aligned}
% \end{equation*}
% % \vspace{-0.7cm}
% and 
% \begin{equation*}
% \begin{aligned}
% \underbrace{{\min_{p}\KL  ( p \Vert {p^*})
% = \max_p \E_{p}[\log p^*(\obs)] + \mathcal{H}(\obs)}
% }_{\text{I(nformation)-Projection}}.
% \end{aligned}
% \end{equation*}
The M-projection - or equivalently maximum likelihood estimation (MLE) \cite{bishop2006pattern} - is \textit{probability forcing}, meaning that the model is optimized to match the moments of the target distribution, causing it to average over modes that it cannot represent. In contrast, the I-projection is \textit{zero forcing} which leads the model to ignore modes of the target distribution that it is not able to represent. The I-projection can be rewritten as maximum entropy problem, i.e., 
\begin{equation}
\label{eq:i_proj}
    \argmax_p\  \E_{p(\mathbf{x})}[\log p^*(\mathbf{x})] + \mathcal{H}(\mathbf{x}).
\end{equation}
Using this formulation, it can be seen that the optimization balances between fitting the target distribution and keeping the entropy $\mathcal{H}(\mathbf{x}) = - \sum_{\mathbf{x}}p(\mathbf{x})\log p(\mathbf{x})$ high. 
% [Rewriting the KL as maximization problem.. expand the reverse KL.. Trade-off between fitting target density and match entropy. A trade of factor is often introduced to handle this exploration exploitation trade-off (cite max-ent RL). $\mathcal{H}(\obs) = - \sum_{\obs}p(\obs)\log p(\obs)$]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mixtures of Expert Policies}
Mixtures of expert policies are conditional discrete latent variable models. Given some observation $\obs\in \mathcal{O}$ and action $\act \in \mathcal{A}$, the marginal likelihood is decomposed into individual components, i.e.,
\begin{equation*}
    p(\act|\obs) = \sum_{\comp} p(\comp|\obs) p(\act|\obs, \comp).
\end{equation*}
Here, $\mathcal{O}$ and $\mathcal{A}$ denote the observation and action space respectively, and $\comp$ stands for the discrete latent variable that indexes distinct components within the mixture. 


The gating $p(\comp|\obs)$ is responsible for soft-partitioning the observation space $\mathcal{O}$ into sub-regions where the corresponding experts $p(\act|\obs, \comp)$ approximate the target density. Typically the experts and the gating are parameterized and learned by maximizing the marginal likelihood via expectation-maximization (EM) \cite{dempster1977maximum} or gradient ascent \cite{bishop1994mixture}.
In order to sample actions, that is, $\act' \sim p(\act|\obs')$ for some observation $\obs'$, we first sample a component index from the gating, i.e., $\comp' \sim p(\comp|\obs')$. The component index selects the respective expert to obtain $\act' \sim p(\act|\obs', \comp')$.
% Using Bayes' rule, the gating can be further decomposed as $p(o|\obs) \propto p(o) p(\obs|o)$. Typically, the input components $p(\obs|o)$ and mixture weights $p(o)$ are learned implicitly by optimizing a parameterized gating network \cite{jacobs1991adaptive, bishop1994mixture}. [To make the last part less random we can say that the gating is defined by mixture model using Bayes' rule; Explain sample procedure]
