\section{Introduction}
% INCLUDE CITATIONS. 
% Robotic systems hold immense potential for enhancing various aspects of our daily lives, from industrial automation to healthcare assistance. However, their widespread adoption is hindered by the challenges associated with programming complex and versatile behaviors. 
% Imitation or policy learning is a promising technique to learn from human demonstrators. By leveraging demonstrations, robots can acquire a wide range of skills and behaviors exhibited by humans. However, imitating human behavior presents several difficulties due to the inherent versatility of human actions.

% One key challenge in imitation learning is the multimodal nature of the data obtained from human demonstrators. These modes can arise due to differences in personal preferences, individual expertise, or alternative problem-solving strategies.

% It is well known that the commonly used maximum likelihood estimation corresponds to a moment projection which causes the policy to average over modes that it cannot represent, often leading to suboptimal or dangerous behavior. 

% To address the mode-averaging problem, we propose \textit{Information Maximizing Curriculum} (IMC) a novel curriculum-based approach. Our approach introduces a weighted optimization scheme, where data samples are assigned curriculum weights, which are updated using an information projection. The information projection minimizes the reverse KL divergence which forces the policy to ignore non-representable modes leading to safe behavior while achieving the desired task.

% To equip the learned policy with a rich set of versatile skills, we extend our approach to a Mixture of Experts (MoE) policy. Each mixture component within the MoE policy selects its own subset of training data for learning, allowing for specialization in different modes. By maximizing the entropy of the joint curriculum, we ensure that all data samples are covered by the policy.

% We show that our method is able to outperform state-of-the-art policy learning algorithms on complex simulated control tasks where data is collected by human demonstrators. In our experiments, we assess the ability of the models to \textit{i)} avoid mode averaging and \textit{ii)} cover all modes present in the data distribution.  
% Include that we compare against other moe policies which use the same model but different learning algorithm?


Equipping agents with well-performing policies has long been a prominent focus in machine learning research. Imitation learning (IL) \cite{osa2018algorithmic} offers a promising technique to mimic human behavior by leveraging expert data, without the need for intricate controller design, additional environment interactions, or complex reward shaping to encode the target behavior. The latter are substantial advantages over reinforcement learning techniques \cite{kaelbling1996reinforcement, sutton2018reinforcement} that rely on reward feedback. However, a significant challenge in IL lies in handling the multimodal nature of data obtained from human demonstrators, which can stem from differences in preferences, expertise, or problem-solving strategies.
Conventionally, maximum likelihood estimation (MLE) is employed to train a policy on expert data. It is well-known that MLE corresponds to the moment projection \cite{murphy2012machine}, causing the policy to average over modes in the data distribution that it cannot represent. Such mode averaging can lead to unexpected and potentially dangerous behavior
% (Figure \ref{fig:intro_m})
. We address this critical issue by introducing \textit{Information Maximizing Curriculum} (IMC), a novel curriculum-based approach.

In IMC, we view imitation learning as a conditional density estimation problem and present a mathematically sound weighted optimization scheme. Data samples are assigned curriculum weights, which are updated using an information projection. The information projection minimizes the reverse KL divergence, forcing the policy to ignore modes it cannot represent  \cite{murphy2012machine}. As a result, the optimized policy ensures safe behavior while successfully completing the task. % (Figure \ref{fig:intro_i}).

Yet, certain modes within the data distribution may remain uncovered with a single expert policy. To address this limitation and endow the policy with the diverse behavior present in the expert data, we extend our approach to a mixture of expert (MoE) policy. Each mixture component within the MoE policy selects its own subset of training data, allowing for specialization in different modes. Our objective maximizes the entropy of the joint curriculum, ensuring that the policy covers all data samples. 

We show that our method is able to outperform state-of-the-art policy learning algorithms and MoE policies trained using competing optimization algorithms on complex multimodal simulated control tasks where data is collected by human demonstrators. In our experiments, we assess the ability of the models to \textit{i)} avoid mode averaging and \textit{ii)} cover all modes present in the data distribution.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure*}[t]
%         \centering
%         \begin{minipage}[t!]{0.32\textwidth}
%             \centering
%             \includegraphics[width=.7\textwidth]{figures/illustrations/fig1_expert.pdf}
%             \subcaption[]{{Expert demonstrations}}
%             \label{fig:intro_expert}
%         \end{minipage}
%         \hfill
%         \begin{minipage}[t!]{0.32\textwidth}
%             \centering
%             \includegraphics[width=.7\textwidth]{figures/illustrations/fig1_m.pdf}
%             \subcaption[]{{Moment projection}}
%             \label{fig:intro_m}
%         \end{minipage}
%         \hfill
%         \begin{minipage}[t!]{0.32\textwidth}
%             \centering
%             \includegraphics[width=.7\textwidth]{figures/illustrations/fig1_i.pdf}
%             \subcaption[]{{Information projection}}
%             \label{fig:intro_i}
%         \end{minipage}
%         \hfill
%         \caption{Visual illustration of versatile expert demonstrations (a) and the behavior of a policy trained computing the  moment- (b) and  information (c) projection. The moment projection is probability forcing: The policy is forced to average over the expert data. The information projection is zero-forcing: The policy is forced to ignore unrepresentable expert data.}
%         \label{fig:intro}
%     \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







% Mixtures of experts (MoEs) are powerful models, that leverage a divide-and-conquer approach to conditional density estimation by assigning experts to smaller sub-tasks. They are capable of representing highly complex multimodal distributions but are inherently hard to train which often yields poor performance due to a sub-optimal optimization outcome. We hypothesize that these problems are due to training by maximizing the likelihood via gradient ascent \cite{bishop1994mixture} or expectation maximization \cite{dempster1977maximum}. It is well known that maximum likelihood estimation corresponds to a moment projection which causes the model to average over modes that it cannot represent, leading to poor generative capabilities \cite{murphy2012machine}. Moreover, these methods are often susceptible to poor solutions found due to local maxima, and finding an appropriate model complexity is difficult as the number of experts has to be specified a-priori. 
% % Despite these drawbacks, common optimization objectives assign equal weights to all samples rendering the model prone to outliers and harming generalization \cite{bengio2009curriculum}. 


% % Despite this drawback, the moment projection is often paired with the assumption that that samples stem from the same data distribution \cite{bishop2006pattern}. As a consequence, easy and difficult samples are weighted equally, rendering the model prone to outliers. % we could mentoin here that the mode averaging propoerty can be problematic in applications such as behavior learning.

% %Despite this drawback, 
% % a more general problem is that 
% %common optimization objectives assume that samples stem from the same data distribution (i.i.d. assumption). As a consequence, all samples are assigned equal importance, rendering the model prone to outliers and leading to an unnatural learning progression as easy samples are treated equally as difficult ones. 
% %This is contradictory to the way humans learn, where skills are often learned on a basic level before gradually increasing the difficulty to become an expert.

% In this work, we propose \textit{Information Maximizing Curriculum} (IMC), a novel approach for training mixtures of experts that combines the information projection \cite{murphy2012machine} with curriculum learning (CL) to address the aforementioned problems with existing optimizing schemes. The information projection minimizes the reverse KL divergence which forces the model to ignore non-representable modes, leading to good generative models that are able to produce high quality samples. IMC assigns weights to samples according to their difficulty, resulting in reduced outlier sensitivity and better generalization capabilities \cite{bengio2009curriculum}. 
% % While early work relies on a manual assignment of these weights, more recent work that goes by self-paced learning, automatically generates the curriculum by adapting to the pace of the learner \cite{kumar2010self}. 

% IMC employs a curriculum for each expert, which adapts to their performance and allows them to specialize on samples that they are able to represent. Moreover, the information projection is employed to compute the joint curriculum of all experts, which results in components that specialize to different subsets of the data. The curriculum also enables a modular architecture  capable of online adaptation of the model complexity by adding experts to the model.
 
% % In this work... tackle both problems using a novel objective and epirically show that iid invalid... we base our objective on the I-projection and build on curriculu learning.

% We show that our method is able to outperform state-of-the-art generative models on challenging multimodal conditional density estimation problems. In particular, we focus on complex behavior learning tasks where data is collected by human demonstrators. The inherent versatility in human behavior leads to highly multimodal data distributions. In our experiments, we assess the ability of the models to \textit{i)} avoid mode averaging and \textit{ii)} extract all modes present in the data distribution.  

%Despite the performance we particularly focus whether all models can cover the different modes in the data, which we include in our performance metrics.
%In all experiments, we assess the models ability to learn the inherent versatility in human behavior   focus on the question of whether our method can 