\section{Algorithm Details \& Ablation Studies}
\label{appendix:algo_details}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inference Details}
\label{appendix:inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We provide pseudocode to further clarify the inference procedure of our proposed method (see Algorithm \ref{algo:imc_inference}). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[htb]
\caption{IMC Action Generation}
\begin{algorithmic}[1]
% \Procedure{Roy}{$a,b$}       \Comment{This is a test}
    \STATE \textbf{Require:} Curriculum weights $\{\tilde{p}(\obs_n, \act_n|z) \ | \ n \in \{1,...,N\}, z \in \{1,...,N_z\}\}$
    \STATE \textbf{Require:} Expert parameter $\{\vtheta_z | \ z \in \{1,...,N_z\}\}$
    \STATE \textbf{Require:} New observation $\mathbf{o}^*$ 
    % \State \textbf{Require:} Variances $\sigma^2_l, \sigma^2_y$
    % \State Initialize $\tilde{p}^{(0)}(x_i|z)$, $\vtheta_z$ and $c_z$
    \IF{\textbf{not} parameter\_updated}
    \STATE $\vphi^* \leftarrow \argmax_{\vphi} \sum_n \sum_z \tilde{p}(\obs_n, \act_n|z) \log g_{\vphi}(z|\mathbf{o}_n)$
    \STATE parameter\_updated $\leftarrow$ True
    \ENDIF
    \STATE Sample $z' \sim g_{\vphi^*}(z|\mathbf{o}^*)$ 
    \STATE Sample $\mathbf{a}' \sim p_{\vtheta_z}(\mathbf{a}|\mathbf{o}^*,z')$
    % $\sim $p(\mathbf{a}|\mathbf{o}^*) = \sum_z g_{\vphi^*}(z|\mathbf{o}^*)p_{\vtheta_z}(\mathbf{a}|\mathbf{o}^*,z)$
    \STATE \textbf{Return} $\mathbf{a}'$
\end{algorithmic}
\label{algo:imc_inference}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Component Initialization}
% \begin{enumerate}
%     \item Motivate the incremental component initialization scheme
%     \item Formalize the incremental component initialization scheme
%     \item Visualize the incremental component initialization scheme
% \end{enumerate}
% We introduced the incremental component-adding scheme mainly for two reasons:
% \begin{enumerate}
%     \item MoGE performance is heavily dependent on the initialization of gating/experts. Using a random initialization, therefore, leads to high variance across different runs. Using heuristics such as K-means which is often used for initializing MoEs in the EM setting helps but can still lead to poor local optima. IMC alleviates problems with local optima by initializing new components at samples that lack coverage by existing components. 
%     \item Incrementally adding components makes the model selection process, i.e., finding a good model capacity for a given task, easier, as it is not necessary to train the model from scratch for a varying number of components.
% \end{enumerate}
% The effectiveness of such a heuristic can be seen by the results in the table below where we compared IMC with incremental component adding scheme, IMC where all components are initialized at the beginning of training using the K-means algorithm. The results for the obstacle avoidance and table tennis task are generated using 30 components. The rest of the hyperparameters are equal to the ones listed in the main manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Expert Design Choices}
\label{appendix:experts}
\textbf{Distribution.} In our mixture of experts policy, we employ Gaussian distributions with a fixed variance to represent the individual experts. This choice offers several benefits in terms of likelihood calculation, optimization and ease of sampling:

To perform the M-Step for the curricula (Section \ref{sec:m_step}), exact log-likelihood computation is necessary. This computation becomes straightforward when using Gaussian distributions. Additionally, when Gaussian distributions with fixed variances are employed to represent the experts, the M-Step for the experts simplifies to a weighted squared-error minimization. Specifically, maximizing the weighted likelihood reduces to minimizing the weighted squared error between the predicted actions and the actual actions.

The optimization problem for the expert update can be formulated as follows:
\begin{equation*}
  \vtheta^*_{\comp} =\argmax_{\vtheta_{\comp}} \ \sum_n \tilde{p}(\obs_n, \act_n|\comp) \log p_{\vtheta_{\comp}}(\act_n|\obs_n,\comp)
 = \argmin_{\vtheta_{\comp}} \ \sum_n \tilde{p}(\obs_n, \act_n|\comp) \|\vmu_{\vtheta_z}(\mathbf{o}_n) - \mathbf{a}_n\|_2^2.
\end{equation*}

This optimization problem can be efficiently solved using gradient-based methods. Lastly, sampling from Gaussian distributions is well-known to be straightforward and efficient.

\textbf{Parameterization.}
We experimented with two different parameterizations of the Gaussian expert means $\vmu_{\vtheta_z}$, which we dub \textit{single-head} and \textit{multi-head}: For single-head, there is no parameter sharing between the different experts. Each expert has its own set of parameters $\vtheta_\comp$. As a result, we learn $N_\comp$ different multi-layer perceptrons (MLPs) $\vmu_{\vtheta_z}: \mathbb{R}^{|\mathcal{O}|} \rightarrow \mathbb{R}^{|\mathcal{A}|}$, where $N_\comp$ is the number of mixture components. In contrast, the multi-head parameterization uses a global set of parameters ${\vtheta}$ for all experts and hence allows for feature sharing. We thus learn a single MLP $\vmu_{\vtheta}: \mathbb{R}^{|\mathcal{O}|} \rightarrow \mathbb{R}^{N_\comp \times |\mathcal{A}|}$.

To compare both parameterizations, we conducted an ablation study where we evaluate the MoE policy on obstacle avoidance, table tennis and Franka kitchen. In order to have a similar number of parameters, we used smaller MLPs for single-head, that is, $1-4$ layers whereas for multi-head we used a $6$ layer MLP. The results are shown in Table \ref{table:expert_param} and are generated using 30 components for the obstacle avoidance and table tennis task. The remaining hyperparameters are equal to the ones listed in the main manuscript. For Franka kitchen, we report the cumulative success rate and entropy for a different number of completed tasks. We report the mean and standard deviation calculated across 10 different seeds. Our findings indicate that, in the majority of experiments, the single-head parameterization outperforms the mutli-head alternative. Notably, we observed a substantial performance disparity, especially in the case of Franka kitchen.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht!]
\caption{\textbf{Expert Parameterization Ablation}: We compare IMC with single- and multi-head expert parameterization. For further details, please refer to the accompanying text.}
\label{table:expert_param}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcc|cc|cc}
\toprule
\rule{0pt}{2ex} &
\multicolumn{2}{c}{\textbf{Obstacle Avoidance}} &
\multicolumn{2}{c}{\textbf{Table Tennis}} &
\multicolumn{2}{c}{\textbf{Franka Kitchen}}  \\
Architecture & 
success rate ($\uparrow$) & Entropy ($\uparrow$) & 
success rate ($\uparrow$) &  Distance Err. ($\downarrow$) & 
success rate ($\uparrow$) & Entropy ($\uparrow$)\\
\midrule
%
single-head & 
% ------------------------------------ %
$0.899 \scriptstyle{\pm 0.035}$ &
$0.887 \scriptstyle{\pm 0.043}$ &
% ------------------------------------ %
$0.812 \scriptstyle{\pm 0.039}$ &
$0.168 \scriptstyle{\pm 0.007}$ &
% ------------------------------------ %
$3.644 \scriptstyle{\pm 0.230}$ &
$6.189 \scriptstyle{\pm 1.135}$ \\
%
multi-head & 
% ------------------------------------ %
${0.855 \scriptstyle{\pm 0.053}}$ &
${0.930 \scriptstyle{\pm 0.031}}$ &
% ------------------------------------ %
${0.870 \scriptstyle{\pm 0.017}}$ &
${0.153 \scriptstyle{\pm 0.007}}$ &
% ------------------------------------ %
$3.248\scriptstyle{\pm 0.062}$ &
$4.657 \scriptstyle{\pm 0.312}$ \\

%
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Expert Complexity.} 
We conducted an ablation study to evaluate the effect of expert complexity on the performance of the IMC algorithm. The study involved varying the number of hidden layers in the single-head expert architecture while assessing the IMC algorithm's performance on the Franka kitchen task using the cumulative success rate and entropy. The results, presented in Figure \ref{fig:expert_complexity_ablation}, were obtained using the hyperparameters specified in the main manuscript. Mean and standard deviation were calculated across 5 different seeds. Our findings demonstrate a positive correlation between expert complexity and achieved performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht!]
        \centering
        \begin{minipage}[t!]{0.8\textwidth}
            \centering
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_sr_fk_expert_ablation.pdf}
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_ent_fk_expert_ablation.pdf}
        \end{minipage}
         \hfill
        \caption[ ]
        { \textbf{Expert Complexity Ablation:} Evaluation of the IMC algorithm on the Franka kitchen task with varying numbers of hidden layers in the single-head expert architecture.}
        \label{fig:expert_complexity_ablation}
    \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{wrapfigure}{r}{0.3\textwidth}
%   \begin{center}
%     \includegraphics[width=\linewidth]{results/kitchen/fk_success.pdf}
%   \end{center}
%   \caption{Birds}
% \end{wrapfigure}
\subsection{Component Utilization}
We further analyze how IMC utilizes its individual components. Specifically, we assess the entropy of the weights assigned to these components, denoted as $\mathcal{H}(z)$ and defined as $\mathcal{H}(z) = - \sum_z p(z) \log p(z)$. Maximum entropy occurs when the model allocates equal importance to all components to solve a task, which implies that $p(z) = 1/ N_z$. Conversely, if the model relies solely on a single component, the entropy $\mathcal{H}(z)$ equals zero.
%
We conduct the study on the Franka kitchen task, evaluating it through cumulative success rates and entropy. The results are shown in Figure \ref{fig:ent_p_z_fk}. We computed the mean and standard deviation based on data collected from 5 different seeds. Our findings reveal that IMC exhibits substantial component utilization even when dealing with a large number of components, denoted as $N_z$.
%
\subsection{Number of Components Sensitivity}
To investigate how the algorithm responds to changes in the number of components $N_z$, we performed a comprehensive ablation study. Figure \ref{fig:succ_p_z_fk} shows the success rate of IMC on the Franka kitchen task using a varying number of components. Our findings indicate that the algorithm's performance remains stable across different values of $N_z$. This robust behavior signifies that the algorithm can adeptly accommodate varying numbers of components without experiencing substantial performance fluctuations. Hence, opting for a higher number of components primarily entails increased computational costs, without leading to overfitting issues.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht!]
        \centering
         \begin{minipage}[t!]{.8\textwidth}
            \centering
            \begin{minipage}[t!]{.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{supplementary_material/ablations/franka/ent_p_z_fk.pdf}
            \subcaption[]{}
            \label{fig:ent_p_z_fk}
            \end{minipage}
            \begin{minipage}[t!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{supplementary_material/ablations/franka/succ_p_z_fk.pdf}
            \subcaption[]{}
            \label{fig:succ_p_z_fk}
            \end{minipage}
        \end{minipage}
        % \begin{minipage}[t!]{0.65\textwidth}
        %     \centering
        %     \includegraphics[width=.49\textwidth]{supplementary_material/ablations/franka/ent_p_z_fk.pdf}
        %     \subcaption[]{}
        %     \includegraphics[width=.49\textwidth]{supplementary_material/ablations/franka/succ_p_z_fk.pdf}
        %     \subcaption[]{}
        % \end{minipage}
         \hfill
        \caption[ ]
        {Figure \ref{fig:ent_p_z_fk} shows the component utilization of IMC: The entropy of IMC's mixture weights $p(z)$ is close to the entropy of a uniform distribution $1/N_z$ indicating that the algorithm uses all components for solving a task, even for a high number of components $N_z$. }
        \label{fig:ent_z_ablation}
    \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Curriculum Pacing Sensitivity}
To examine the algorithm's sensitivity to the curriculum pacing parameter $\eta$, we conducted an ablation study. Figure \ref{fig:eta_ablation} presents the results obtained using 30 components for the obstacle avoidance and table tennis tasks, while maintaining the remaining hyperparameters as listed in the main manuscript. For the Franka kitchen task, we analyzed the cumulative success rate and entropy across varying numbers of completed tasks. The mean and standard deviation were calculated across 5 different seeds. Our findings reveal that the optimal value for $\eta$ is dependent on the specific task. Nevertheless, the algorithm exhibits stable performance even when $\eta$ values differ by an order of magnitude.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht!]
        \centering
        \begin{minipage}[t!]{0.49\textwidth}
            \centering
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_sr_oa_eta_ablation.pdf}
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_ent_oa_eta_ablation.pdf}
            \subcaption[]{{Obstacle avoidance}}
            \label{fig:ablation_performance_obstacle_avoidance}
        \end{minipage}
                \hfill
        \begin{minipage}[t!]{0.49\textwidth}
            \centering
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_sr_tt_eta_ablation.pdf}
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_ent_tt_eta_ablation.pdf}
            \subcaption[]{{Table tennis}}
            \label{fig:ablation_performance_table_tennis}
        \end{minipage}
         \hfill
        \begin{minipage}[t!]{0.49\textwidth}
            \centering
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_sr_hbp_eta_ablation.pdf}
            \includegraphics[width=.49\textwidth]{supplementary_material/ablations/Fig_ICML23_ent_hbp_eta_ablation.pdf}
            \subcaption[]{{Franka kitchen}}
            \label{fig:ablation_performance_hbp}
        \end{minipage}
         \hfill
        \caption[ ]
        { \textbf{Curriculum Pacing Sensitivity:} Sensitivity analysis of the IMC algorithm for performance metrics in the obstacle avoidance, table tennis, and Franka kitchen tasks, considering varying curriculum pacing ($\eta$) values. The results illustrate the mean and standard deviation across 5 different seeds.}
        \label{fig:eta_ablation}
    \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Stopping Criterion} 
% We terminate the algorithm if either the maximum number of training iterations is reached or if the lower bound $L(\vpsi, q)$ converges, i.e., 
% \begin{equation*}
%    |\Delta L| = |L^{(i)}(\vpsi, q)- L^{(i-1)}(\vpsi, q)| \leq \epsilon,
% \end{equation*}
% with threshold $\epsilon$ and two subsequent iterations $(i)$ and $(i-1)$.  The lower bound can be evaluated efficiently using Corollary \ref{thm:lowerbound}.
% \begin{corollary} 
% \label{thm:lowerbound}
% Consider the setup used in Proposition \ref{thm:implicit}. For $p^*(\comp) \in \vpsi$ and  $\{p^*(\obs \act|\comp)\}_{\comp} \in \vpsi$ it holds that
% \begin{equation*}
%     L\big(\vpsi, q\big)  = \eta \log \sum_{\comp} \sum_n \tilde{p}(\obs_n, \act_n|\comp).
% \end{equation*}
% \end{corollary}
% % \begin{equation*}
% % % L(\vpsi, q)  = \log \sum_{\comp} \sum_n \tilde{p}(\obs_n, \act_n|\comp).
% % L\big(p^*(\comp), \{p^*(\obs \act|\comp)\}_{\comp},\{\vtheta_{\comp}\}_{\comp}, q\big)  = \log \sum_{\comp} \sum_n \tilde{p}(\obs_n, \act_n|\comp).
% % \end{equation*}
% See Appendix \ref{proof:lowerbound} for a proof.