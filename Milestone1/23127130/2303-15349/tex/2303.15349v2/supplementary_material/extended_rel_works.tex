\section{Extended Related Work}
\subsection{Connection to ML-Cur}
\label{section:connection_mlcur}
This section elaborates on the differences between this work and the work by \cite{li2023curriculum}, as both studies leverage curriculum learning paired with Mixture of Experts to facilitate the acquisition of diverse skills. However, it's essential to discern several noteworthy distinctions between our approach and theirs:

Firstly, Li et al. primarily focus on linear policies and a gating distribution with limited flexibility, constraining the expressiveness of their learned policies. In contrast, our approach allows for the use of arbitrarily non-linear neural network parameterizations for both, expert policies and gating.

Another divergence lies in the handling of mini-batches during training. While our algorithm accommodates the use of mini-batches, Li et al.'s method does not support this feature. The ability to work with mini-batches can significantly enhance the efficiency and scalability of the learning process, especially when dealing with extensive datasets or intricate environments.

Additionally, Li et al.'s evaluation primarily centers around relatively simple tasks, that do not require complex manipulations, indicating potential limitations in terms of task complexity and applicability. In contrast, our work is designed to address more intricate and challenging environments, expanding the range of potential applications and domains.

Lastly, it's worth noting that Li et al. specifically focus on the learning of skills parameterized by motion primitives \cite{paraschos2013probabilistic}. In contrast, our framework offers the versatility to encompass various skill types and parameterizations, broadening the scope of potential applications and use cases.



\subsection{Connection to Expectation Maximization}
\label{section:connection_em}
In this section we want to highlight the commonalities and differences between our algorithm and the expectation-maximization (EM) algorithm for mixtures of experts. First, we look at the updates of the variational distribution $q$. Next, we compare the expert optimization. Lastly, we take a closer look at the optimization of the gating distribution.

The EM algorithm sets the variational distribution during the E-step to 
\begin{equation}
    q(z|\mathbf{o}_n) = p(z|\mathbf{o}_n, \mathbf{a}_n) = \frac{p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)p(z|\mathbf{o}_n)}{\sum_z p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)p(z|\mathbf{o}_n)},
   \label{eq:comp_em_e_step}
\end{equation}
for all samples $n$ and components $z$.
In the M-step, the gating distribution $p(z|\mathbf{o})$ is updated such that the KL divergence between $q(z|\mathbf{o})$ and $p(z|\mathbf{o})$ is minimized. Using the properties of the KL divergence, we obtain a global optimum by setting $p(z|\mathbf{o}_n) = q(z|\mathbf{o}_n)$ for all $n$ and all $z$. This allows us to rewrite Equation \ref{eq:comp_em_e_step} using the recursion in $q$, giving
\begin{equation*}
    q(z|\mathbf{o}_n)^{(i+1)} = \frac{p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)q(z|\mathbf{o}_n)^{(i)}}{\sum_z p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)q(z|\mathbf{o}_n)^{(i)}},
\end{equation*}
where $(i)$ denotes the iteration of the EM algorithm. The update for the variational distribution of the IMC algorithm is given by
\begin{align*}
    q(z|\obs_n, \act_n)^{(i+1)} &= \frac{\tilde{p}(\obs_n, \act_n|z)^{(i+1)}}{\sum_z \tilde{p}(\obs_n, \act_n|z)^{(i+1)}} 
    % &= \frac{\exp(\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)/\eta + \log q(z|\obs_n, \act_n)^{(i)}}{\sum_z \exp(\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)/\eta + \log q(z|\obs_n, \act_n)^{(i)}} \\
     = \frac{p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)^{1/\eta}q(z|\obs_n, \act_n)^{(i)}}{\sum_z p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)^{1/\eta}q(z|\obs_n, \act_n)^{(i)}}.
\end{align*}
Consequently, we see that $q(z|\mathbf{o}) =  q(z |\obs \act)$ for $\eta = 1$. 
However, the two algorithms mainly differ in the M-step for the experts: The EM algorithm uses the variational distribution to assign weights to samples, i.e.
\begin{equation*}
    \max_{\vtheta_z} \ \sum_{n=1}^N q(z|\mathbf{o}_n)\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z),
\end{equation*}
whereas IMC uses the curricula as weights, that is, 
\begin{equation*}
    \max_{\vtheta_z} \ \sum_{n=1}^N p(\obs_n, \act_n|z)\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z).
\end{equation*}
This subtle difference shows the properties of moment and information projection: In the EM algorithm each sample $\mathbf{o}_n$ contributes to the expert optimization as $\sum_z q(z|\mathbf{o}_n) = 1$. However, if all curricula ignore the $n$th sample, it will not have impact on the expert optimization. Assuming that the curricula ignore samples that the corresponding experts are not able to represent, IMC prevents experts from having to average over `too hard' samples. Furthermore, this results in reduced outlier sensitivity as they are likely to be ignored for expert optimization. Lastly, we highlight the difference between the gating optimization: Assuming that both algorithms train a gating network $g_{\vphi}(z|\mathbf{o})$ we have
\begin{equation*}
    \max_{\vphi}  \ \sum_n \sum_z q(z|\mathbf{o}_n) \log g_{\vphi}(z|\mathbf{o}_n),
\end{equation*}
for the EM algorithm and 
\begin{equation*}
   \max_{\vphi} \  \sum_n \sum_z \tilde{p}(\obs_n, \act_n|z) \log g_{\vphi}(z|\mathbf{o}_n),
\end{equation*}
for IMC. Similar to the expert optimization, EM includes all samples to fit the parameters of the gating network, whereas IMC ignores samples where the unnormalized curriculum weights $\tilde{p}(\obs_n, \act_n|z)$ are zero for all components.