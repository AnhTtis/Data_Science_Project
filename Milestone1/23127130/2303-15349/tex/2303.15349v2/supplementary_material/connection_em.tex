\section{Connection to Expectation Maximization}
\label{section:connection_em}
In this section we want to highlight the commonalities and differences between our algorithm and the expectation-maximization (EM) algorithm for mixtures of experts. First, we look at the updates of the variational distribution $q$. Next, we compare the expert optimization. Lastly, we take a closer look at the optimization of the gating distribution.

The EM algorithm sets the variational distribution during the E-step to 
\begin{equation}
    q(z|\mathbf{o}_n) = p(z|\mathbf{o}_n, \mathbf{a}_n) = \frac{p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)p(z|\mathbf{o}_n)}{\sum_z p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)p(z|\mathbf{o}_n)},
   \label{eq:comp_em_e_step}
\end{equation}
for all samples $n$ and components $z$.
In the M-step, the gating distribution $p(z|\mathbf{o})$ is updated such that the KL divergence between $q(z|\mathbf{o})$ and $p(z|\mathbf{o})$ is minimized. Using the properties of the KL divergence, we obtain a global optimum by setting $p(z|\mathbf{o}_n) = q(z|\mathbf{o}_n)$ for all $n$ and all $z$. This allows us to rewrite Equation \ref{eq:comp_em_e_step} using the recursion in $q$, giving
\begin{equation*}
    q(z|\mathbf{o}_n)^{(i+1)} = \frac{p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)q(z|\mathbf{o}_n)^{(i)}}{\sum_z p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)q(z|\mathbf{o}_n)^{(i)}},
\end{equation*}
where $(i)$ denotes the iteration of the EM algorithm. The update for the variational distribution of the IMC algorithm is given by
\begin{align*}
    q(z|\obs_n, \act_n)^{(i+1)} &= \frac{\tilde{p}(\obs_n, \act_n|z)^{(i+1)}}{\sum_z \tilde{p}(\obs_n, \act_n|z)^{(i+1)}} 
    % &= \frac{\exp(\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)/\eta + \log q(z|\obs_n, \act_n)^{(i)}}{\sum_z \exp(\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)/\eta + \log q(z|\obs_n, \act_n)^{(i)}} \\
     = \frac{p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)^{1/\eta}q(z|\obs_n, \act_n)^{(i)}}{\sum_z p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z)^{1/\eta}q(z|\obs_n, \act_n)^{(i)}}.
\end{align*}
Consequently, we see that $q(z|\mathbf{o}) =  q(z |\obs \act)$ for $\eta = 1$. 
However, the two algorithms mainly differ in the M-step for the experts: The EM algorithm uses the variational distribution to assign weights to samples, i.e.
\begin{equation*}
    \max_{\vtheta_z} \ \sum_{n=1}^N q(z|\mathbf{o}_n)\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z),
\end{equation*}
whereas IMC uses the curricula as weights, that is, 
\begin{equation*}
    \max_{\vtheta_z} \ \sum_{n=1}^N p(\obs_n, \act_n|z)\log p_{\vtheta_z}(\mathbf{a}_n|\mathbf{o}_n, z).
\end{equation*}
This subtle difference shows the properties of moment and information projection: In the EM algorithm each sample $\mathbf{o}_n$ contributes to the expert optimization as $\sum_z q(z|\mathbf{o}_n) = 1$. However, if all curricula ignore the $n$th sample, it will not have impact on the expert optimization. Assuming that the curricula ignore samples that the corresponding experts are not able to represent, IMC prevents experts from having to average over `too hard' samples. Furthermore, this results in reduced outlier sensitivity as they are likely to be ignored for expert optimization. Lastly, we highlight the difference between the gating optimization: Assuming that both algorithms train a gating network $g_{\vphi}(z|\mathbf{o})$ we have
\begin{equation*}
    \max_{\vphi}  \ \sum_n \sum_z q(z|\mathbf{o}_n) \log g_{\vphi}(z|\mathbf{o}_n),
\end{equation*}
for the EM algorithm and 
\begin{equation*}
   \max_{\vphi} \  \sum_n \sum_z \tilde{p}(\obs_n, \act_n|z) \log g_{\vphi}(z|\mathbf{o}_n),
\end{equation*}
for IMC. Similar to the expert optimization, EM includes all samples to fit the parameters of the gating network, whereas IMC ignores samples where the unnormalized curriculum weights $\tilde{p}(\obs_n, \act_n|z)$ are zero for all components.