\section{Proofs}
\label{appendix:proofs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{thm:single} and \ref{thm:multiple}}
\label{proof:conv_single}
\textbf{Convergence of the Single Expert Objective (Proposition \ref{thm:single}).}
We perform coordinate ascent on $\tilde{J}$ which is guaranteed to converge to a stationary point if updating each coordinate results in a monotonic improvement of \cite{boyd2004convex}. For fixed expert parameters we find the unique $p(\obs, \act)$ that maximizes $\tilde{J}$ \cite{levine2018reinforcement} (see Section \ref{sec:single_expert}) and hence we have $\tilde{J}(p(\obs, \act)^{(i)}, \vtheta) \geq \tilde{J}(p(\obs, \act)^{(i-1)}, \vtheta)$ where $i$ denotes the iteration. Under suitable assumptions ($\log p_{\vtheta}$ is differentiable, its gradient is 
$L$-Lipschitz, $\vtheta_{\comp}$ is updated using gradient ascent and the learning rate is chosen such that the descent lemma \cite{bertsekas1997nonlinear} holds), it holds that 
$\tilde{J}(p(\obs, \act), \vtheta^{(i)}) \geq \tilde{J}(p(\obs, \act), \vtheta^{(i-1)})$. Hence, we are guaranteed to converge to a stationary point of $\tilde{J}$. $\qed$

\textbf{Convergence of the Mixture of Experts Objective (Proposition \ref{thm:multiple}).}
As we tighten the lower bound $L$ in every E-step, it remains to show that $L(\vpsi^{(i)},q) \geq L(\vpsi^{(i-1)},q)$ to prove that $J(\vpsi)$ is guaranteed to converge to a stationary point, with $\vpsi =\{p(\comp), \{\curpc\}_{\comp}, \{\vtheta_{\comp}\}_{\comp}\}$. To that end, we again perform a coordinate ascent on $L$ to show a monotonic improvement in every coordinate. Note that we find the unique $p(\comp)$ and $\{\curpc\}_{\comp}$ that maximize $L$ via Proposition \ref{thm:implicit} and Equation \ref{eq:opt_mw}. Analogously to the proof of Proposition \ref{thm:single} we can show monotonic improvement of $L$ in $\{\vtheta_{\comp}\}_{\comp}$  under suitable assumptions on $\log p_{\vtheta_z}$. $\qed$

\textbf{Remark.}
Although \textit{stochastic} gradient ascent does not guarantee strictly monotonic improvements in the objective function $J$, our empirical observations suggest that $J$ indeed tends to increase monotonically in practice as shown by Figure \ref{fig:lb_fk}.
\begin{figure*}[ht!]
        \centering
        \begin{minipage}[h!]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{supplementary_material/Fig_ICML23_lb.pdf}
        \end{minipage}
         \hfill
        \caption[ ]
        {\textbf{Convergence of the Mixture of Experts Objective.} Objective function value $J$ for $100$ training iterations $i$ on Franka Kitchen.}
        \label{fig:lb_fk}
    \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{thm:implicit}}
\label{proof:implicit}
Expanding the entropy in Equation \ref{eq:opt_mw} we obtain
\begin{equation*}
    p^*(\comp) \propto \exp\big( \E_{p^*(\obs, \act|\comp)}[{R_\comp(\obs, \act)}/{\eta} - \log p^*(\obs, \act|\comp)] \big).
\end{equation*}
Using $p^*(\obs, \act|\comp) = \tilde{p}(\obs, \act|\comp)/ \sum_n \tilde{p}(\obs_n, \act_n|\comp)$ yields
\begin{align*}
    p^*(\comp) \propto & \exp\big( \E_{p^*(\obs, \act|\comp)}[{R_\comp(\obs, \act)}/{\eta} - \log \tilde{p}(\obs, \act|\comp) + \log  \sum_n \tilde{p}(\obs_n, \act_n|\comp)] \big).
\end{align*}
Next, leveraging that $\log \tilde{p}(\obs, \act|\comp) = {R_\comp(\obs_n, \act_n)}/{\eta} $ we see that
\begin{equation*}
    p^*(\comp) \propto  \exp\big( \E_{p^*(\obs, \act|\comp)}[\log  \sum_n \tilde{p}(\obs_n, \act_n|\comp)] \big)
    =  \sum_n \tilde{p}(\obs_n, \act_n|\comp),
\end{equation*}
which concludes the proof. $\qed$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Corollary \ref{thm:lowerbound}}
\label{proof:lowerbound}
We start by rewriting the lower bound as $L(\vpsi, q) = $
\begin{align*}
    \E_{p^*(\comp)}\big[\E_{p^*(\obs, \act|\comp)}[R_\comp(\obs, \act) - \eta \log p^*(\obs, \act|\comp)]- \eta \log p^*(\comp)\big].
\end{align*}
Using $p^*(\obs, \act|\comp) \propto \tilde{p}(\obs, \act|\comp)$ and Proposition \ref{thm:implicit} we obtain
\begin{align*}
    L(\vpsi, q) = & \E_{p^*(\comp)}\big[\E_{p^*(\obs, \act|\comp)}[R_\comp(\obs, \act) - \eta \log \tilde{p}(\obs, \act|\comp) \\
    + & \eta \log  \sum_n \tilde{p}(\obs_n, \act_n|\comp) ] -\eta \log \sum_n \tilde{p}(\obs_n, \act_n|\comp)
    +  \eta \log \sum_\comp \sum_n \tilde{p}(\obs_n, \act_n|\comp) \big]
\end{align*}
With $\eta \log \tilde{p}(\obs, \act|\comp) = {R_\comp(\obs_n, \act_n)}$ all most terms cancel, giving
\begin{align*}
    L(\vpsi, q) = & \E_{p^*(\comp)}\big[\eta \log \sum_\comp \sum_n \tilde{p}(\obs_n, \act_n|\comp)\big]  =  \eta \log \sum_\comp \sum_n \tilde{p}(\obs_n, \act_n|\comp),
\end{align*}
which concludes the proof. $\qed$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Corollary \ref{thm:gating}}
\label{proof:gating}
\begin{align*}
& \min_{\phi} \mathbb{E}_{p(\obs)}D_{\text{KL}}(p(z|\obs)\|g_{\phi}(z|\obs))
=
\max_{\phi} \int_{\mathcal{O}}\sum_z p(\obs,z) \log {g_{\phi}(z|\obs)} \text{d}\obs
\\= &
\max_{\phi} \int_{\mathcal{O}} \int_{\mathcal{A}} \sum_z p(\obs,\act,z) \log {g_{\phi}(z|\obs)} \text{d}\act \text{d}\obs
= 
\max_{\phi} \int_{\mathcal{O}} \int_{\mathcal{A}} p(\obs,a) \sum_z p(z|\obs,\act) \log {g_{\phi}(z|\obs)} \text{d}\act \text{d}\obs
\\= &
\min_{\phi} \mathbb{E}_{p(\obs,\act)}D_{\text{KL}}(p(z|\obs,\act)\|g_{\phi}(z|\obs)).
\end{align*}

Expanding the expected KL divergence, we get 
\begin{align*}
    &\min_{\vphi} \E_{p({\obs, \act})}\KL\big(p(\comp|\obs, \act)\Vert g_{\vphi}(\comp|\obs)\big)  =  \min_{\vphi} \sum_n p({\obs_n, \act_n}) \sum_\comp p(\comp|\obs_n, \act_n) \log \frac{p(\comp|\obs_n, \act_n)}{g_{\vphi}(\comp|\obs_n)}.
\end{align*}
Noting that $p(\comp|{\obs_n, \act_n})$ is independent of $\vphi$ we can rewrite the objective as
\begin{equation*}
    \max_{\vphi} \sum_n p({\obs_n, \act_n}) \sum_\comp p(\comp|\obs_n, \act_n) \log {g_{\vphi}(\comp|\obs_n)}.
\end{equation*}
Using that $p(\comp|\obs, \act) = \tilde{p}(\obs, \act|\comp)/ \sum_\comp \tilde{p}(\obs, \act|\comp)$ together with $p({\obs, \act})=\sum_\comp p^*(\comp)p^*(\obs, \act|\comp)$ yields
\begin{align*}
    \max_{\vphi} \sum_n \sum_\comp p^*(\comp)p^*(\obs_n, \act_n|\comp) \sum_\comp \frac{\tilde{p}(\obs_n, \act_n|\comp)}{\sum_\comp\tilde{p}(\obs_n, \act_n|\comp)}  \log {g_{\vphi}(\comp|\obs_n)}.
\end{align*}
Using Proposition \ref{thm:implicit} we can rewrite $p^*(\comp)p^*(\obs, \act|\comp)$ as $\tilde{p}(\obs, \act|\comp) / \sum_\comp \sum_n \tilde{p}(\obs_n, \act_n|\comp)$. Since the constant factor $1 / \sum_\comp \sum_n \tilde{p}(\obs_n, \act_n|\comp)$ does not affect the optimal value of $\vphi$ we obtain
\begin{align*}
    & \max_{\vphi} \sum_n \sum_\comp \tilde{p}(\obs_n, \act_n|\comp)   \sum_\comp \frac{\tilde{p}(\obs_n, \act_n|\comp)}{\sum_\comp\tilde{p}(\obs_n, \act_n|\comp)}  \log {g_{\vphi}(\comp|\obs_n)} 
    = \max_{\vphi} \sum_n   \sum_\comp {\tilde{p}(\obs_n, \act_n|\comp)}  \log {g_{\vphi}(\comp|\obs_n)},
\end{align*}
which concludes the proof. $\qed$

%     \begin{align*}
%     &\min_{\vphi} \E_{p({\obs, \act})}\KL\big(p(\comp|\obs, \act)\Vert g_{\vphi}(\comp|\obs)\big) \\ = \ 
%     & \max_{\vphi} \sum_n \sum_\comp \tilde{p}(\obs_n, \act_n|\comp) \log g_{\vphi}(\comp|\obs_n).
% \end{align*}


% \begin{equation*}
%     p^*(\comp) \propto \exp\big( \E_{p(\obs, \act|\comp)}[{R_\comp(\obs, \act)}/{\eta}] + \mathcal{H}(\obs, \act|\comp)\big),
% \end{equation*}
% and 
% \begin{equation*}
%     p^*(\obs_n, \act_n|\comp) \propto \tilde{p}(\obs_n, \act_n|\comp) = \exp\big( {R_\comp(\obs_n, \act_n)}/{\eta}\big),
% \end{equation*}