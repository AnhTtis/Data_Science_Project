\section{Derivations}
\label{appendix:derivations}
\subsection{Lower Bound Decomposition}
\label{appendix:lb_decomp}
To arrive at Equation \ref{eq:decomposition} by marginalizing over the latent variable $o$ for the entropy of the joint curriculum, i.e.,
\begin{align*}
    \mathcal{H}(\obs, \act) & = - \sum_n p(\obs_n, \act_n) \log p(\obs_n, \act_n) \\ &
    = - \sum_n p(\obs_n, \act_n) \sum_\comp p(\comp|\obs_n, \act_n) \log p(\obs_n, \act_n)
\end{align*}
Next, we use Bayes' theorem, that is, $p(\obs_n, \act_n) = p(\comp)p(\obs_n, \act_n|\comp)/ p(\comp|\obs_n, \act_n)$, giving 
\begin{align*}
\mathcal{H}(\obs, \act) = &
- \sum_n p(\obs_n, \act_n) \sum_\comp p(\comp|\obs_n, \act_n) \big(
\log p(\comp) + \log p(\obs_n, \act_n|\comp)  - \log p(\comp|\obs_n, \act_n)
\big).
\end{align*}
Moreover, we add and subtract the log auxiliary distribution $\log q(\comp|\obs_n, \act_n)$ which yields
\begin{align*}
\mathcal{H}(\obs, \act) = &
- \sum_n p(\obs_n, \act_n) \sum_\comp p(\comp|\obs_n, \act_n) \big(
\log p(\comp) + \log p(\obs_n, \act_n|\comp)  \\
&- \log p(\comp|\obs_n, \act_n) + \log q(\comp|\obs_n, \act_n) - \log q(\comp|\obs_n, \act_n)
\big).
\end{align*}
Rearranging the terms leads and writing the sums in terms of expectations we arrive at
\begin{align*}
    \mathcal{H}(\obs, \act) = &
    \E_{p(\comp)}\big[
    \E_{p(\comp|\obs, \act)}[
    \log q(\comp|\obs, \act)
    ] 
    + \mathcal{H}(\obs, \act|\comp)
    \big]  + \mathcal{H}(\comp)  + \KL\big(p(\comp|\obs, \act)\Vert q(\comp|\obs, \act)\big).
\end{align*}
Lastly, multiplying $\mathcal{H}(\obs, \act)$ with $\eta$ and adding $\E_{p(\comp)}\E_{p(\obs, \act|\comp)}[\log p_{\vtheta_\comp}(\mathbf{y}|\mathbf{x},\comp)]$
we arrive at Equation \ref{eq:decomposition} which concludes the derivation.

\subsection{M-Step Objectives}
\label{appendix:expert_obj}
\textbf{Closed-Form Curriculum Updates.}  
In order to derive the closed-form solution to Equation \ref{eq:opt_mw} (RHS) we solve
\begin{equation*}
   \max_{p(\obs, \act|\comp)} \  J_{\comp}(p(\obs, \act|\comp), \vtheta_\comp)= \max_{p(\obs, \act|\comp)} \ \E_{p(\obs, \act|\comp)}[R_{\comp}(\obs, \act)] + \eta\mathcal{H}(\obs, \act|\comp) \quad \text{subject to} \quad \sum_n p(\obs, \act) = 1. 
\end{equation*}
Following the procedure of constrained optimization, we write down the Lagrangian function \cite{boyd2004convex} as
\begin{equation*}
    \mathcal{L}(p, \lambda) = \sum_n p(\obs_n, \act_n|\comp) R_{\comp}(\obs_n, \act_n) - \eta \sum_n p(\obs_n, \act_n|\comp) \log p(\obs_n, \act_n|\comp) + \lambda (\sum_n p(\obs_n, \act_n|\comp) - 1),
\end{equation*}
where $\lambda$ is the Lagrangian multiplier. As $p$ is discrete, we solve for the optimal entries of $p(\obs_n, \act_n|\comp)$, that is, $p^{\prime}(\obs_n, \act_n, \lambda|z) = \argmax_{p } \mathcal{L}(p, \lambda)$. Setting the partial derivative of $\mathcal{L}(p, \lambda)$ with respect to $p$ zero, i.e.,
\begin{align*}
    & \frac{\partial}{\partial p(\obs_n, \act_n|\comp)} \mathcal{L}(p, \lambda) = R_{\comp}(\obs_n, \act_n) - \eta \log p(\obs_n, \act_n|\comp) - \eta + \lambda \overset{!}{=} 0.
\end{align*}
 yields
$
     p^{\prime}(\obs_n, \act_n, \lambda|z) = \exp \big( R_{\comp}(\obs_n, \act_n) - \eta + \lambda \big) / \eta.
$

Plugging $p^\prime$ back in the Lagrangian gives the dual function $g(\lambda)$, that is,
\begin{equation*}
    g(\eta) = \mathcal{L}(p^{\prime}, \lambda) =  - \eta + \eta \sum_n  \exp \big( R_{\comp}(\obs_n, \act_n) - \eta + \lambda \big) / \eta.
\end{equation*}
Solving for $\lambda^* = \argmin_{\lambda \geq 0} g(\lambda)$ equates to
\begin{align*}
 & \frac{\partial}{\partial \lambda} g(\lambda) = -1 + \eta \sum_n  \exp \big( R_{\comp}(\obs_n, \act_n) - \eta + \lambda \big)/ \eta \overset{!}{=} 0 \\
 \ \iff \ & \lambda^* = -\log\Big( \eta \sum_n  \exp \big( R_{\comp}(\obs_n, \act_n) - \eta\big)/ \eta
\Big).
\end{align*}
Finally, substituting $\lambda^*$ into $p^{\prime}$ we have
\begin{equation*}
    p^*(\obs_n, \act_n|\comp) = p^{\prime}(\obs_n, \act_n, \lambda^*|\comp) = \frac{\exp \big( R_{\comp}(\obs_n, \act_n) / \eta \big)}{\sum_n \exp \big(R_{\comp}(\obs_n, \act_n) / \eta \big) },
\end{equation*}
which concludes the derivation. The derivation of the optimal mixture weights $p^*(z)$ works analogously.

\textbf{Expert Objective.}
In order to derive the expert objective of Equation \ref{eq:expert_update} we solve $\max_{\vtheta_\comp} \  J_{\comp}(p(\obs, \act|\comp), \vtheta_\comp)=$
\begin{equation*}
     \max_{\vtheta_\comp} \  
    \sum_n p(\obs_n, \act_n|\comp) \Big(\log p_{\vtheta_{\comp}}(\act_n|\obs_n,\comp) + \eta \log q(\comp|\obs_n, \act_n) -  \eta \log p(\obs_n, \act_n|\comp)\Big).
\end{equation*}
Noting that $q(\comp|\obs_n, \act_n)$ and $p(\obs_n, \act_n|\comp)$ are independent of $\vtheta_\comp$ and ${p}(\obs_n, \act_n|\comp) = \tilde{p}(\obs_n, \act_n|\comp) / \sum_n \tilde{p}(\obs_n, \act_n|\comp)$ we find that 
\begin{equation*}
   \max_{\vtheta_\comp} \  J_{\comp}(p(\obs, \act|\comp), \vtheta_\comp) = 
     \max_{\vtheta_\comp} \  
    \sum_n \frac{\tilde{p}(\obs_n, \act_n|\comp)}{\sum_n \tilde{p}(\obs_n, \act_n|\comp)} \log p_{\vtheta_{\comp}}(\act_n|\obs_n,\comp).
\end{equation*}
Noting that $\sum_n \tilde{p}(\obs_n, \act_n|\comp)$ is a constant scaling factor concludes the derivation. 