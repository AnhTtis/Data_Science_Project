\subsection{Baselines and Hyperparameter}
We now briefly mention the baselines and their hyperparameters. We used Bayesian optimization to tune the most important hyperparameters. 

\textbf{Mixture of Experts trained with Expectation-Maximization (EM).}
The architecture of the mixture of experts model trained with EM \cite{jacobs1991adaptive} is identical to the one optimized with IMC: We employ a parameterized inference network and conditional Gaussian distributions to represent experts with the same hyperparameters as shown in Table \ref{table:imc_hp}. Furthermore, we initialized all responsibilities as $p(z|\mathbf{o}) = 1/N_z$, where $N_z$ is the number of components.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Mixture Density Network (MDN).} 
The mixture density network \cite{bishop1994mixture} uses a shared backbone neural network with multiple heads for predicting component indices as well as the expert likelihood. For the experts, we employ conditional Gaussians with a fixed variance. The model likelihood is maximized in an end-to-end fashion using stochastic gradient ascent. We experimented with different backbones and expert architectures. However, we found that the MDN is not able to partition the input space in a meaningful way, often resulting in sub-optimal outcomes, presumably due to mode averaging. 
To find an appropriate model complexity we tested up to $50$ expert heads. We found that the number of experts heads did not significantly influence the results, further indicating that the MDN is not able to utilize multiple experts to solve sub-tasks. We additionally experimented with a version of the MDN that adds an entropy bonus to the objective \cite{zhou2020movement} to encourage more diverse and multimodal solutions. However, we did not find significant improvements compared to the standard version of the MDN. For a list of hyperparameter choices see \ref{table:mdn_hp}. 
\begin{table}[htb!]
\caption{\textbf{MDN Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:mdn_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\ 
    \midrule
    Expert hidden layer
    & $\{1, 2\}$
    & $1, 1, 1, 1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Expert hidden units
    & $\{30, 50\}$
    & $50, 30, 30, 50$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Backbone hid. layer
    & $\{2, 3, 4, 6, 8, 10\}$
    & $3, 2, 4, 3$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Backbone hid. units
    & $\{50, 100, 150, 200\}$
    & $200, 200, 200, 200$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate $\times 10^{-3}$
    & $[0.1, 1]$
    & $5.949, 7.748, 1.299, 2.577$
    \\
    Expert variance ($\sigma^2$)
    & $-$
    & $1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
 }
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Denoising Diffusion Probabilistic Models (DDPM).} 
We consider the denoising diffusion probabilistic model proposed by \cite{ho2020denoising}. Following common practice we parameterize the model as residual MLP \cite{pearce2023imitating} with a sinusoidal positional encoding \cite{vaswani2017attention} for the diffusion steps. Moreover, we use the cosine-based variance scheduler proposed by \cite{nichol2021improved}. For further details on hyperparameter choices see Table \ref{table:ddpm_hp}. 
% For the block push task, we use 150 hidden units, 16 diffusion steps and 6 hidden layers. For the kitchen task, we use 200 hidden units, 16 diffusion steps and 8 hidden layers.
% For the obstacle avoidance task, we use 200 hidden units, 16 diffusion steps and 6 hidden layers.
\begin{table}[htb!]
\caption{\textbf{DDPM Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task, and the table tennis task (in this order). }
\label{table:ddpm_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\
    \midrule
    Hidden layer
    & $\{4,6,8, 10, 12\}$
    & $6, 6, 8, 6$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden units
    & $\{50, 100, 150, 200\}$
    & $200, 150, 200, 200$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Diffusion steps
    & $\{5, 15, 25, 50\}$
    & $15, 15, 15, 15$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Variance scheduler
    & $-$
    & cosine
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Normalizing Flow (NF).} For all experiments, we build the normalizing flow by stacking masked autoregressive flows \cite{papamakarios2017masked} paired with permutation layers \cite{papamakarios2021normalizing}. As base distribution, we use a conditional isotropic Gaussian. Following common practice, we optimize the model parameters by maximizing its likelihood. 
See Table \ref{table:nf_hp} for a list of hyperparameters.
% For the kitchen task, we use 200 hidden units, 0.000462 learning rate and 4 num flows.
% For the block push task, we use 150 hidden units, 0.00045 learning rate and 6 num flows.
% For the obstacle avoidance task, we use 100 hidden units, 0.000743 learning rate and 6 num flows.
\begin{table}[htb!]
\caption{\textbf{NF Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:nf_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\
    \midrule
    Num. flows
    & $\{4,6,8, 10, 12\}$
    & $6, 6, 4, 4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden units per flow
    & $\{50, 100, 150, 200\}$
    & $100, 150, 200, 150$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate $\times 10^{-4}$
    & $[0.01, 10]$
    & $7.43, 4.5, 4.62, 7.67$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Conditional Variational Autoencoder (CVAE).} We consider the conditional version of the autoencoder proposed in \cite{sohn2015learning}. We parameterize the encoder and decoder with a neural network with mirrored architecture. Moreover, we consider an additional scaling factor ($\beta$) for the KL regularization in the lower bound objective of the VAE as suggested in \cite{higgins2017beta}. 
% For the kitchen task, we use 4 hidden layers, 100 hidden units, 16 latent dimension and 0.452 $\KL$ scaling. For the block push task, we use 10 hidden layers, 150 hidden units, 16 latent dimension and 1.008 $\KL$ scaling. For the obstacle avoidance task, we use 8 hidden layers, 100 hidden units, 32 latent dimension and 1.641 $\KL$ scaling.
\begin{table}[htb!]
\caption{\textbf{CVAE Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:cave_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value \\ 
    \midrule
    Hidden layer
    & $\{4,6,8, 10, 12\}$
    & $8, 10, 4, 4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden units
    & $\{50, 100, 150, 200\}$
    & $100, 150, 100, 100$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Latent dimension
    & $\{4, 16, 32, 64\}$
    & $32, 16, 16, 16$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $\KL$ scaling ($\beta$)
    & $[10^{-3}, 10^{2}]$
    & $1.641, 1.008, 0.452, 0.698$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-3}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Max. epochs
    & $-$
    & $2000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batchsize
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Implicit Behavior Cloning (IBC).} 
IBC was proposed in \cite{florence2022implicit} and uses energy-based models to learn a joint distribution over inputs and targets. Following common practice we parameterize the model as neural network. Moreover, we use the version that adds a gradient penalty to the InfoNCE loss \cite{florence2022implicit}. For sampling, we use gradient-based Langevin MCMC \cite{du2019implicit}. Despite our effort, we could not achieve good results with IBC. A list of hyperparameters is shown in Table \ref{table:ibc_hp}.
% For the obstacle avoidance task, we use 200 hidden dim, 4 hidden layers, 0.1662 noise scale and 44 train samples.

\begin{table}[htb!]
\caption{\textbf{IBC Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task and the table tennis task (in this order). We do not get any good results for the block push task and the Franka kitchen task.}
\label{table:ibc_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value\\ 
    \midrule
    Hidden dim
    & $\{50,100,150,200,256\}$
    & $200, 256$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Hidden layers
    & $\{4,6,8,10\}$
    & $4,6$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Noise scale
    & $[0.1, 0.5]$
    & $0.1662,0.1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Train samples
    & $[8, 64]$
    & $44, 8$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Noise shrink
    & $-$
    & $0.5$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Train iterations
    & $-$
    & $20$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Inference iterations
    & $-$
    & $40$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-4}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batch size
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Epochs
    & $-$
    & $1000$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Behavior Transformer (BET).} Behavior transformers were recently proposed in \cite{shafiullah2022behavior}. The model employs a minGPT transformer \cite{brown2020language} to predict targets by decomposing them into cluster centers and residual offsets. To obtain a fair comparison, we compare our method to the version with no history. A comprehensive list of hyperparameters is shown in Table \ref{table:bet_hp}.
% For the obstacle avoidance task, we use 3 transformer blocks, 1 offset loss scale, 96 embedding width, 50 number of bins and 4 attention heads. For the block push task, we use 4 transformer blocks, 1 offset loss scale, 72 embedding width, 10 number of bins and 4 attention heads. For the kitchen task, we use 6 transformer blocks, 1 offset loss scale, 120 embedding width, 64 number of bins and 6 attention heads.
\begin{table}[htb!]
\caption{\textbf{BET Hyperparameter.} The `Value' column indicates sweep values for the obstacle avoidance task, the block pushing task, the Franka kitchen task and the table tennis task (in this order).}
\label{table:bet_hp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
    Parameter & Sweep & Value\\ 
    \midrule
    Transformer blocks
    & $\{2,3,4,6\}$
    & $3, 4, 6, 2$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Offset loss scale
    & $\{1.0,100.0,1000.0\}$
    & $1.0, 1.0, 1.0, 1.0$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Embedding width
    & $\{48,72,96,120\}$
    & $96, 72, 120, 48$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Number of bins
    & $\{8,10,16,32,50,64\}$
    & $50, 10, 64, 64$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Attention heads
    & $\{4,6\}$
    & $4, 4, 6, 4$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Context size
    & $-$
    & $1$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Training epochs
    & $-$
    & $500$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Batch size
    & $-$
    & $512$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Learning rate
    & $-$
    & $10^{-4}$
    \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \bottomrule
  \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Parameter Comparison}
% Mention that we compared all the parameters of the models
% \begin{table}[htb!]
% \caption{\textbf{Model Parameter Comparison.} To the left of `$/$' is the optimal number of model parameters determined by running a sweep. To the right of `$/$' is the maximum number of parameters that were tested in the sweep.}
% \label{table:model_params}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \resizebox{.9\textwidth}{!}{%
% \begin{tabular}{lcccc}
% \toprule
%      & Obstacle Avoidance & Block Pushing & Table Tennis & Franka Kitchen \\ 
%     \midrule
%     MDN
%     & $91k/372k$
%     & $53k/375k$
%     & $91k/372k$
%     & $136k/378k$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     EM
%     & $137k/362k$
%     & $570k/1096k$
%     & $193k/401k$
%     & $737k/1162k$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     DDPM
%     & $206k/464k$
%     & $119k/466k$
%     & $211k/468k$
%     & $295k/472k$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     NF
%     & $255k/1982k$
%     & $595k/2068k$
%     & $397k/2069k$
%     & $740k/2220k$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     CVAE
%     & $162k/965k$
%     & $443k/970k$
%     & $79k/970k$
%     & $83k/979k$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     IBC
%     & $162k/659k$
%     & $-$
%     & $399k/663k$
%     & $-$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     BET
%     & $350k/1070k$
%     & $256k/1071k$
%     & $103k/1162k$
%     & $1127k/1127k$
%     \\
%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     IMC
%     & $362k/362k$
%     & $221k/416k$
%     & $162k/162k$
%     & $471k/1162k$
%     \\
%     \bottomrule
%   \end{tabular}
% }
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%