{
    "arxiv_id": "2303.15349",
    "paper_title": "Information Maximizing Curriculum: A Curriculum-Based Approach for Training Mixtures of Experts",
    "authors": [
        "Denis Blessing",
        "Onur Celik",
        "Xiaogang Jia",
        "Moritz Reuss",
        "Maximilian Xiling Li",
        "Rudolf Lioutikov",
        "Gerhard Neumann"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG"
    ],
    "abstract": "Mixtures of Experts (MoE) are known for their ability to learn complex conditional distributions with multiple modes. However, despite their potential, these models are challenging to train and often tend to produce poor performance, explaining their limited popularity. Our hypothesis is that this under-performance is a result of the commonly utilized maximum likelihood (ML) optimization, which leads to mode averaging and a higher likelihood of getting stuck in local maxima. We propose a novel curriculum-based approach to learning mixture models in which each component of the MoE is able to select its own subset of the training data for learning. This approach allows for independent optimization of each component, resulting in a more modular architecture that enables the addition and deletion of components on the fly, leading to an optimization less susceptible to local optima. The curricula can ignore data-points from modes not represented by the MoE, reducing the mode-averaging problem. To achieve a good data coverage, we couple the optimization of the curricula with a joint entropy objective and optimize a lower bound of this objective. We evaluate our curriculum-based approach on a variety of multimodal behavior learning tasks and demonstrate its superiority over competing methods for learning MoE models and conditional generative models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15349v1"
    ],
    "publication_venue": null
}