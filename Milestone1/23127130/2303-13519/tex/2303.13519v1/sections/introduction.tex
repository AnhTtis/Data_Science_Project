\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/fig1.pdf}
    \caption{Prior work~\cite{lin2022learning,lei2021less} learns step representations from single short video clips, independent of the task, thus lacking knowledge of task structure. Our model, VideoTaskformer, learns step representations for masked video steps through the global context of all surrounding steps in the video, making our learned representations aware of task semantics and structure.}
    \label{fig:overview}
\end{figure}

Picture this, you’re trying to build a bookshelf by watching a YouTube video with several intricate steps. You’re annoyed by the need to repeatedly hit pause on the video and you’re unsure if you have gotten all the steps right so far. 
Fortunately, you have an interactive assistant that can guide you through the task at your own pace, verifying each step as you perform it and interrupting you if you make a mistake. 
A composite task such as ``\emph{making a bookshelf}'' involves multiple fine-grained activities such as ``\emph{drilling holes}'' and ``\emph{adding support blocks}.'' Accurately categorizing these activities requires not only recognizing the individual steps that compose the task but also understanding the task structure, which includes the temporal ordering of the steps and multiple plausible ways of executing a step (e.g., one can beat eggs with a fork or a whisk). 
An ideal interactive assistant has both a high-level understanding of a broad range of tasks, as well as a low-level understanding of the intricate steps in the tasks, their temporal ordering, and the multiple ways of performing them.

As seen in Fig.~\ref{fig:overview}, prior work~\cite{lei2021less,lin2022learning} models  step representations of a single step independent of the overall task context. 
This might not be the best strategy, given that steps for a task are related, and the way a step is situated in an overall task may contain important information about the step. To address this, we pre-train our model with a masked modeling objective that encourages the step representations to capture the \emph{global context} of the entire video. Prior work lacks a benchmark for detecting mistakes in videos, which is a crucial component of verifying the quality of instructional video representations. We introduce a mistake detection task and dataset for verifying if the task in a video is executed correctly---i.e. if each step is executed correctly and in the right order.    

% Our goal is to learn representations for the steps in the instructional video which capture semantics of the task being performed such that each step representation contains information about the surrounding context (other steps in the task). To achieve this, we train a video model VideoTaskformer using a BERT style masked modeling objective which predicts masked out steps in the input video.

Our goal is to learn representations for the steps in the instructional video which capture semantics of the task being performed such that each step representation contains information about the surrounding context (other steps in the task). To this end, we train a model VideoTaskformer, using a masked step pre-training approach for learning step representations in instructional videos. We learn step representations jointly for a whole video, by feeding multiple steps to a transformer, and masking out a subset. The network learns to predict labels for the masked steps given just the visual representations of the remaining steps. The learned contextual representations improve performance on downstream tasks such as forecasting steps, classifying steps, and recognizing procedures. 

% \trevor{Intro to this para was too chatty.  I'm going to rewrite to make slighly more formal...hope that's ok} 
Our approach of modeling steps further enables a new method for mistake identification. Recall, our original goal was to assist a user following an instructional video. We synthetically generate a mistakes dataset for evaluation using the step annotations in COIN~\cite{tang2019coin}. We consider two mistake types: mistakes in the steps of a task, and mistakes in the ordering of the steps of a task. For the first, we randomly replace the steps in a video with steps from a similar video. For the second, we re-order the steps in a task. We show that our network is capable of detecting both mistake types and outperforms prior methods on these tasks. 

Additionally, we evaluate representations learned by VideoTaskformer on three existing benchmarks: step classification, step forecasting, and procedural activity recognition on the COIN dataset. Our experiments show that learning step representation through masking pre-training objectives improves the performance on the downstream tasks. We will release code, models, and the mistake detection dataset and benchmark to the community.
%\trevor{also reference code?}





