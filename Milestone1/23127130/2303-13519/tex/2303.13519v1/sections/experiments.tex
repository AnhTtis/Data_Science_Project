\section{Experiments}
\begin{table*}[h]
\centering
\footnotesize
\begin{tabular}{lllc}
\toprule 
Model   & Pre-training Supervision & Pre-training Dataset  & Acc (\%)
\\ \midrule
TSN (RGB+Flow)~\cite{tang2020comprehensive}  & Supervised: action labels & Kinetics      & 36.5* \\ 
S3D~\cite{miech2020end} & Unsupervised: MIL-NCE on ASR  & HT100M      & 37.5* \\ 
\hline
ClipBERT~\cite{lei2021less} & Supervised: captions & COCO + Visual Genome & 30.8\\
VideoCLIP~\cite{xu2021videoclip} & Unsupervised: NCE on ASR  & HT100M & 39.4 \\ 
SlowFast~\cite{feichtenhofer2019slowfast}  & Supervised: action labels  & Kinetics & 32.9 \\
TimeSformer~\cite{bertasius2021space} & Supervised: action labels & Kinetics & 48.3 \\
LwDS: TimeSformer~\cite{bertasius2021space} & Unsupervised: $k$-means on ASR & HT100M & 46.5 \\
LwDS: TimeSformer  & Distant supervision & HT100M & 54.1 \\ \hline
VideoTF (SC)  & Unsupervised: NN on ASR & HT100M & 47.0\\
\textbf{VideoTF (DM)}  & \textbf{Distant supervision} & HT100M &  \textbf{54.8} \\
\textbf{VideoTF (SC)}   & \textbf{Distant supervision} & HT100M &  \textbf{56.5} \\
\bottomrule
\end{tabular}
\caption{\textbf{Step classification.} We compare to the accuracy scores for all baselines. VideoTF (SC) pre-trained with step classification loss on distant supervision from WikiHow achieves state-of-the-art performance on the downstream step classification task. We report baseline results from \cite{lin2022learning}. * indicates results by fine-tuning on COIN}
\label{tab:step}
\end{table*}

\begin{table*}[h]
\centering
\footnotesize
\setlength{\tabcolsep}{1pt}
\begin{tabular}{lllcc}
\toprule
Downstream Model & Base Model  & Pre-training Supervision & Pre-training Dataset  & Acc (\%)
\\ \midrule
TSN (RGB+Flow)~\cite{tang2020comprehensive} & Inception~\cite{szegedy2016rethinking} & Supervised: action labels & Kinetics      & 73.4* \\ 
Transformer & S3D~\cite{miech2020end}  & Unsupervised: MIL-NCE on ASR  & HT100M     & 70.2* \\ 
Transformer & ClipBERT~\cite{lei2021less} & Supervised: captions & COCO + Visual Genome & 65.4 \\
Transformer & VideoCLIP~\cite{xu2021videoclip} & Unsupervised: NCE on ASR  & HT100M      &  72.5\\ 
Transformer & SlowFast~\cite{feichtenhofer2019slowfast}  & Supervised: action labels  & Kinetics & 71.6 \\
Transformer & TimeSformer~\cite{bertasius2021space}   & Supervised: action labels & Kinetics      & 83.5 \\
LwDS: Transformer & TimeSformer~\cite{bertasius2021space}   & Unsupervised: $k$-means on ASR & HT100M      & 85.3 \\
LwDS: Transformer & TimeSformer   & Distant supervision & HT100M     &  88.9 \\
LwDS: Transformer w/ KB Transfer & TimeSformer  & Distant supervision & HT100M     & 90.0 \\ \hline
VideoTF (SC; fine-tuning) w/ KB Transfer & TimeSformer & Unsupervised: NN on ASR & HT100M & 81.2\\
VideoTF (SC; linear-probe) w/ KB Transfer & TimeSformer & Distant supervision & HT100M & 83.1\\
VideoTF (DM; linear-probe) w/ KB Transfer & TimeSformer & Distant supervision & HT100M &  85.7\\
VideoTF (SC) w/ KB Transfer & TimeSformer & Distant supervision & HT100M & 90.5\\
\textbf{VideoTF (DM) w/ KB Transfer} & \textbf{TimeSformer} & Distant supervision & \textbf{HT100M} & \textbf{91.0} \\
\bottomrule
\end{tabular}
\centering
\caption{Accuracy of different methods on the \textbf{procedural activity recognition} dataset.}
\label{tab:proc}
\end{table*}


\begin{table*}[!htbp]
\centering
\footnotesize
\begin{tabular}{lllllll}
\toprule
Downstream Model & Base Model  & Pre-training Supervision & Pre-training Dataset  & Acc (\%)
\\ \midrule
Transformer & S3D~\cite{miech2020end}  & Unsupervised: MIL-NCE on ASR  & HT100M     & 28.1 \\ 
Transformer & SlowFast~\cite{feichtenhofer2019slowfast}  & Supervised: action labels  & Kinetics     & 25.6 \\
Transformer & TimeSformer~\cite{bertasius2021space}   & Supervised: action labels & Kinetics      & 34.7 \\
LwDS: Transformer & TimeSformer~\cite{bertasius2021space}   & Unsupervised: $k$-means on ASR & HT100M      & 34.0 \\
LwDS: Transformer w/ KB Transfer & TimeSformer  & Distant supervision & HT100M     &  39.4 \\ \hline
VideoTF (SC; fine-tuned) w/ KB Transfer & TimeSformer & Unsupervised: NN on ASR & HT100M & 35.1\\
VideoTF (SC; linear-probe) w/ KB Transfer & TimeSformer & Distant supervision & HT100M & 39.2\\
VideoTF (DM; linear-probe) w/ KB Transfer & TimeSformer & Distant supervision & HT100M &  40.1\\
VideoTF (SC) w/ KB Transfer & TimeSformer & Distant supervision & HT100M & 41.5\\
\textbf{VideoTF (DM) w/ KB Transfer} & \textbf{TimeSformer} & Distant supervision & \textbf{HT100M} & \textbf{42.4} \\
\bottomrule
\end{tabular}
\vspace{-.1cm}
\caption{Accuracy of different methods on the \textbf{short-term step forecasting}  dataset.}
\vspace{-.2cm}
\label{tab:sfore}
\end{table*}


\begin{table*}[!htbp]
\centering
\footnotesize
\begin{tabular}{lllllll}
\toprule
Downstream Model & Base Model  & Pre-training Supervision & Pre-training Dataset  & Acc (\%)
\\ \midrule
Transformer (ASR text) w/ Task label & MPNet & & & 39.0\\
Transformer & SlowFast~\cite{feichtenhofer2019slowfast}  & Supervised: action labels  & Kinetics  & 15.2\\
Transformer & TimeSformer~\cite{bertasius2021space}   & Supervised: action labels & HT100M &  17.0\\
Transformer w/ Task label & TimeSformer~\cite{bertasius2021space}   & Supervised: action labels & HT100M &  40.1\\
LwDS: Transformer w/ Task label & TimeSformer   & Distant supervision & HT100M     &  41.3 \\\hline
VideoTF (DM) & TimeSformer & Distant supervision & HT100M & 40.2\\
\textbf{VideoTF (DM) w/ Task label} & \textbf{TimeSformer} & Distant supervision & \textbf{HT100M} & \textbf{46.4} \\
\bottomrule
\end{tabular}
\vspace{-.1cm}
\caption{Accuracy of different methods on the \textbf{long-term step forecasting} dataset.}
\vspace{-.2cm}
\label{tab:lfore}
\end{table*}


\begin{table*}[!htbp]
\centering
\footnotesize
% \setlength{\tabcolsep}{1.2pt}
\begin{tabular}{llllcc}
\toprule
\multirow{2}{*}{Downstream Model} & \multirow{2}{*}{Base Model}  & \multirow{2}{*}{Pre-training Supervision} & \multirow{2}{*}{Pre-training Dataset}  & \multicolumn{2}{c}{Mistake Detection} \\
\multicolumn{4}{c}{} & \textbf{Step} & \textbf{Order}
\\ \midrule
Transformer (ASR text) w/~Task label & MPNet \cite{song2020mpnet} & & & 34.2 & 33.4\\
Transformer w/~Task Label & SlowFast~\cite{feichtenhofer2019slowfast}  & Supervised: action labels  & Kinetics     &  28.6 & 26.1\\
Transformer w/~Task label & TimeSformer~\cite{bertasius2021space}   & Supervised: action labels & HT100M  & 36.0 & 34.7 \\
LwDS: Transformer & TimeSformer & Distant supervision & HT100M     &  17.1 & 11.2\\ 
LwDS: Transformer w/~Task Label & TimeSformer & Distant supervision & HT100M     & 37.6 & 31.8\\ \hline
VideoTF (SC) & TimeSformer & Distant supervision & HT100M & 20.1 & 15.4\\
VideoTF (DM) w/~Task label & TimeSformer & Distant supervision & HT100M & 40.8 & 34.0\\
\textbf{VideoTF (SC; fine-tuned) w/~Task label} & \textbf{TimeSformer} & Distant supervision & \textbf{HT100M} & \textbf{41.7} & \textbf{35.4}\\
\bottomrule
\end{tabular}
\vspace{-.1cm}
\caption{Accuracy of different methods on the \textbf{mistake step detection} test dataset.}
\vspace{-.2cm}
\label{tab:mis}
\end{table*}

We evaluate VideoTaskformer~(VideoTF) and compare it with existing baselines on 6 downstream tasks: step classification, procedural activity recognition, step forecasting, mistake step detection, mistake ordering detection, and long term forecasting. Results are on the datasets described in Sec.~\ref{sec:dataset}.

\subsection{Baselines} 
We compare our method to state-of-the-art video representation learning models for action/step recognition. We fine-tune existing models in a similar fashion to ours on the 6 downstream tasks. We briefly describe the best performing baseline, Learning with Distant Supervision (LwDS)~\cite{lin2022learning}.

\noindent\textbullet~\textbf{TimeSformer~(LwDS)~\cite{lin2022learning}.} In this baseline model, the TimeSformer backbone is pre-trained on HowTo100M using the Distribution Matching loss (but without any masking of steps as in our model). Next, a single-layer transformer is fine-tuned on top of the pre-trained representations from the base model for each downstream task. 

\noindent\textbullet~\textbf{TimeSformer w/ KB transfer~(LwDS)~\cite{lin2022learning}.} For procedural activity recognition and step forecasting, the LwDS baseline is modified to include knowledge base transfer via retrieval of most relevant facts from the knowledge base to assist the downstream task. We also include results by adding the same KB transfer component to our method, referenced as  w/ KB Transfer. 

\noindent\textbullet~\textbf{Steps from clustering ASR text.} As an alternative to the weak supervision from WikiHow, we introduce an unsupervised baseline that relies only on the transcribed speech (ASR text) to obtain steps. \cite{narasimhan2022tl} introduced an approach to segment a video into steps by clustering visual features along the time axis. It divides the video into non-overlapping segments and groups adjacent video segments together based on a similarity threshold. We adopt a similar approach but in the text space. We compute sentence embeddings for the ASR sentences and group adjacent sentences if their similarity exceeds the average similarity of all sentences across the entire video. We include ablations with different thresholds in the Supplemental.     

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/res.pdf}
    \caption{\textbf{Qualitative results.} We show qualitative results of our method on 4 tasks. The step labels are not used during training and are only shown here for illustrative purposes.}
    \label{fig:res}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/res2.pdf}
    \caption{\textbf{Qualitative comparison.} We compare results from our method VideoTF to the baseline LwDS on the short-term forecasting task. Step labels are not passed to the model as input and are only for reference.}
    \label{fig:res2}
\end{figure}

\subsection{Ablations}
We evaluate our design choices by ablating different components of our model.  

\noindent\textbullet~\textbf{Base model.} We report results for different base video models for pre-training: S3D \cite{miech2020end}, SlowFast \cite{feichtenhofer2019slowfast}, TimeSformer \cite{bertasius2021space} trained on HT100M, and TimeSformer trained on Kinetics. For short-term step forecasting, procedural activity recognition, and step classification, the results are from \cite{lin2022learning}. 

\noindent\textbullet~\textbf{Loss function.} For pre-training VideoTF, we test both the loss functions, Step Classification (SC), and Distribution Matching (DM) described in Sec.~\ref{sec:method}.  

\noindent\textbullet~\textbf{Modalities.} For mistake step detection and long-term forecasting tasks, we tried replacing video features with ASR text during fine-tuning. The base model is a language model for embedding sentences in the ASR text and is kept fixed. The ASR text embeddings for all the segments of the video are fed as input to the downstream model, a basic single-layer transformer, which is fine-tuned to each of the tasks. 

\noindent\textbullet~\textbf{Task label.} For mistake detection and long-term forecasting tasks, we include the task name, e.g. \emph{``Install a Ceiling Fan''}, as input to the downstream model. We compute the sentence embedding of the task label and append it to the list of video tokens fed as input to the model. This domain knowledge provides additional context which boosts the performance on these challenging downstream tasks.


\noindent\textbullet~\textbf{Linear-probe vs Fine-tuning.} In linear-probe evaluation, only the $f_\text{head}$ layer is fine-tuned to each downstream task and in the fine-tuning setting, all the layers of the segment transformer $f_\text{trans}$ are fine-tuned. 

\subsection{Results}
\noindent \textbf{Quantitative Results.} We compare our approach to several baselines on all downstream tasks. For all the downstream tasks, the downstream segment transformer is fine-tuned, except for linear-probe where we keep our pre-trained model fixed and only train a linear head on top of it for each downstream task.

On the step classification task in Tab.~\ref{tab:step}, VideoTF with step classification loss outperforms LwDS~\cite{lin2022learning} by ~2\%, indicating that step representations learned with global context also transfer well to a task that only looks at local video clips. In procedural activity recognition (Tab.~\ref{tab:proc}), we see that distribution matching loss works slightly better than step classification loss and our fine-tuned model achieves 1\% improvement over the best baseline. For short-term forecasting in Tab.~\ref{tab:sfore}, we achieve a 3\% improvement over LwDS and our unsupervised pre-training using NN with ASR outperforms previous unsupervised methods. We also note that linear-probe performance is competitive in Tab.~\ref{tab:proc} and outperforms baselines in Tab.~\ref{tab:sfore}. VideoTF with achieves a strong improvement of 5\% over LwDS on the long-term forecasting task, 4\% on mistake step detection, and 4\% on mistake ordering detection. Adding task labels improves performance on all three tasks. 

Additionally, we evaluate our approach on the activity recognition task in EPIC Kitchens-100 and include results in the Supplemental. We also report our models performance on the step localization task in COIN.   

\noindent \textbf{Qualitative Results.} Fig.~\ref{fig:res} shows qualitative results of our model VideoTF on the mistake detection tasks. Fig.~\ref{fig:res} (A) shows a result on mistake step detection, where our model's input is the sequence of video clips on the left and it correctly predicts the index of the mistake step ``2'' as the output. In (B), the order of the first two steps is swapped and our model classifies the sequence as incorrectly ordered. In (C), for the long-term forecasting task, the next 5 steps predicted by our model match the ground truth and in (D), for the short-term forecasting task, the model predicts the next step correctly given the past 2 steps. In Fig.~\ref{fig:res2} we show an example result of our method compared to the baseline LwDS on the short-term forecasting  task. Our method correctly predicts the next step as ``remove air nozzle'' since it has acquired knowledge of task structure whereas the baseline predicts the next step incorrectly as ``install valve cap.''
