\section{Learning Task Structure through Masked Modeling of Steps}
\label{sec:method}
Our goal is to learn task-aware step representations from a large corpus of instructional videos. To this end, we develop VideoTaskformer, a video model pre-trained using a \bert~\cite{devlin2019bert} style masked modeling loss. In contrast to \bert and \vbert~\cite{sun2019videobert}, we perform masking at the step level, which encourages the network to learn step embeddings that encapsulate the semantics and temporal ordering of steps within the task. 

Our framework consists of two steps: pre-training and fine-tuning. During pre-training, VideoTaskformer is trained on weakly labeled data on the pre-training task. For fine-tuning, VideoTaskformer is first initialized with the pre-trained parameters, and a subset of the parameters is fine-tuned using labeled data from the downstream tasks. Each downstream task yields a separate fine-tuned model.

We first provide an overview of the pre-training approach before delving into details of the individual components.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/model.pdf}
    \caption{\textbf{VideoTaskformer Pre-training~(Left).} VideoTaskformer $f_{\text{VT}}$ learns step representations for the masked out video clip $v_i$, while attending to the other clips in the video. It consists of a video encoder $f_{\text{vid}}$, a step transformer $f_{\text{trans}}$, and a linear layer $f_{\text{head}}$, and is trained using weakly supervised step labels. \textbf{Downstream Tasks~(Right).} We evaluate step representations learned from VideoTaskformer on 6 downstream tasks. 
    }
    \label{fig:model}
\end{figure*}

\noindent \textbf{Overview.} Our approach for pre-training VideoTaskformer is outlined in Fig.~\ref{fig:model}. Consider an instructional video $V$ consisting of $K$ video clips $v_{i}, i \in [1, \dots, K]$ corresponding to $K$ steps in the video. A step $v_{i} \in \mathbb{R}^{L \times H \times W \times 3}$ is a sequence of $L$ consecutive frames depicting a step, or semantic component of the task. For example, for the task ``\emph{Making a french toast}'', examples of steps include ``\emph{Whisk the batter}'', and ``\emph{Dip bread in batter}.'' We train a video model VideoTaskformer $f_{\text{VT}}$ to learn step representations. We mask out a few clips in the input $V$ and feed it to $f_{\text{VT}}$ which learns to predict step labels for the masked-out clips. We evaluate the embeddings learned by our pre-training objective on 6 downstream tasks: step classification, procedural activity recognition, step forecasting, mistake step detection, mistake ordering detection, and long term forecasting.

% We encode the video frames for a step using the Timesformer model and feed all of its output step embeddings to a step transformer, while masking out a few steps. The Transformer predicts the step label for all the masked-out steps. This way, the network is encouraged to learn context-aware step embeddings for all steps across all videos.  

Below, we provide more details on how we pre-train VideoTaskformer using a masked step modeling loss, followed by fine-tuning details on the downstream tasks.

\subsection{Pre-training VideoTaskformer with Masked Step Modeling} 
We extend masked language modeling techniques used in \bert and \vbert to learn step representations for instructional videos. While \bert and \vbert operate on language and visual tokens respectively, VideoTaskformer operates on clips corresponding to steps in an instructional video. By predicting weakly supervised natural language step labels for masked out clips in the input video, VideoTaskformer learns semantics and long-range temporal interactions between the steps in a task. Unlike prior works wherein step representations are learned from local short video snippets corresponding to the step, our step representations are from the entire video with all the steps as input and capture \emph{global context} of the video.  

% \noindent \textbf{Masked Step Modeling.} 
% \newcommand{\xxl}{V_{\setminus i}}
% Let $V = \{v_1, \dots, v_K\}$ denote the visual clips corresponding to $K$ steps in video $V$. The input to VideoTaskformer is $V_{\setminus i}$, which is the input sequence $V$ with the $i\text{th}$ step masked out (i.e., replaced with a \texttt{MASK} token).
% \[
% \xxl = (v_1, \ldots, v_{i-1}, \mathrm{MASK}, v_{i+1}, \ldots, v_K)
% \] 
% We define a joint probability distribution over the steps in a video as follows:
% \[
% p(V|\theta) = \frac{1}{Z(\theta)} \prod_{i=1}^K p(V_{\setminus i}|\theta)
% \]
% where $p(V_{\setminus i})$ is the probability of step $i$ given the model parameters $\theta$, and $Z$ is the partition function.

% The probability of each step is defined as:
% \[
% p(V|\theta) = V_i^T f_{\text{VT}_\theta}(\xxl)
% \]
% where $V_i$ is the one-hot vector for the $i$'th step and $f_{\text{VT}_\theta}(\xxl)$ is the learned step embedding. 

% The function $f_{\text{VT}}$, denotes our model VideoTaskformer and ${\theta}$ are the model parameters. It consists of a video encoder model $f_{\text{vid}}$ for each video clip $v_i$ corresponding to a step, followed by a step transformer $f_{\text{t}}$,  and a linear layer $f_{\text{head}}$. Note that the input to the model is the entire video $V$, consisting of all the $K$ steps as a tensor in $\mathbb{R}^{K \times L \times H \times W \times 3}$ where $K$ denotes the number of steps, $L$ is the number of frames in each step with spatial resolution $H \times W \times 3$. The masked step $v_i$ in $V$ is replaced by a mask token. The model $f_{\text{VT}}(\xxl)$ returns an $K \times D$ tensor, where $D$ is the size of the output of each transformer node.

% The model $f_{\text{VT}}$ is trained to maximize the log-likelihood of the correct step label $y_i$:
% \[
% \mathcal{L} = \frac{1}{K} \sum_{i=1}^K \log p(y_i|V, \theta)
% \]

\noindent \textbf{Masked Step Modeling.}
Let $V = \{v_1, \dots, v_K\}$ denote the visual clips corresponding to $K$ steps in video $V$. The goal of our our Masked Step Modeling pre-training setup is to encourage VideoTaskformer to learn representations of clips $v_i$ that are aware of the semantics of the corresponding step and the context of the surrounding task. To this end, the task for pre-training is to predict categorical natural language step labels for the masked out steps. While we do not have ground truth step labels, we use the weak supervision procedure proposed by \cite{lin2022learning} to map each clip $v_i$ to a distribution over step labels $p(y_i \mid v_i)$ by leveraging the noisy ASR annotations associated with each clip. The distribution $p(y_i \mid v_i)$ is a categorical distribution over a finite set of step labels $Y$. More details are provided in Sec.~\ref{sec:impl}.

Let $M \subseteq [1, \ldots, K]$ denote some subset of clip indices (where each index is included in $M$ with some masking probability $r$, a hyperparameter). Let $V_{\setminus M}$ denote a partially masked-out sequence of clips: the same sequence as $V$ except with clips $v_i$ masked out for all $i \in M$.

Let $f_{\text{VT}}$ represent our VideoTaskformer model with parameters $\theta$. $f_{\text{VT}}$ is composed of a video encoder model $f_{\text{vid}}$ which encodes each clip $v_i$ independently, followed by a step transformer $f_{\text{trans}}$ operating over the sequence of clip representations, and finally a linear layer $f_{\text{head}}$ (which includes a softmax). The input to the model is an entire video (of size $K \times L \times H \times W \times 3$) and the output is of size $K \times S$ (where $S$ is the output dimension of the linear layer).

We pre-train $f_{\text{VT}}$ by inputting a masked video $V_{\setminus M}$ and predicting step labels $y_{i}$ for each masked-out clip $v_i$, as described below. For the downstream tasks, we extract step-aware representations using $f_{\text{VT}}$ by feeding an unmasked video $V$ to the model. We then extract the intermediate outputs of $f_{\text{trans}}$ (which are of size $K \times D$, where $D$ is the output embedding size).

% \noindent \textbf{Learning.} To train our model $f_{\text{VT}}$, we require step labels for every video clip $v_i$ in the input video. We describe how we obtain these step labels and the approximate target distribution of video clips over step labels $P(y_s|v_i)$, in Sec.~\ref{sec:impl}. We consider the two different training objectives to train the network: (1) step classification and (2) distribution matching. Our choice of these objectives is inspired by Lin \emph{et al.}~\cite{lin2022learning}, and we describe them below for the context of masked step modeling.

To predict step labels for masked-out steps at pre-training time, we consider two training objectives: (1) step classification, and (2) distribution matching. We describe them below in the context of Masked Step Modeling.

% \noindent \textbf{Step classification loss.}
% We train $f_{\text{VT}}(\xxl) \to [0,1]^S$ to classify each \emph{masked out} video segment in the input into one of the $S$ possible steps, where $S = \{s_1, \ldots, s_N$\}. 
% Let $n^*,s^*$ be the index of the step (describing a timestep and step index) that best describes segment $v_i$ according to our target distribution $P(y_s|v_i)$. 
% We use cross-entropy loss to train $f_{\text{VT}}$ to classify video segment $v_i$ into class $(n^*,s^*)$:
% \begin{equation}
%     \min _\theta~-\log\left( \left[ f_{\text{VT}}(\xxl; \theta)  \right]_{(n^*,s^*)} \right)
% \end{equation}
% where $\theta$ denotes the learning parameters of the video model. The model uses a softmax activation function in the last layer to define a proper distribution over the steps, such that $\sum_{n,s} \left[ f_{\text{VT}}(\xxl; \theta)  \right]_{(n,s)}  = 1$. The loss above is shown for a single masked segment. However, during training, we mask 15\% of the $K$ segments in a video and optimize the objective by averaging over a mini-batch of video segments sampled from the entire collection in each iteration~\cite{lin2022learning}.

\noindent \textbf{Step classification loss.}
We use the outputs of $f_{\text{VT}}$ to represent an $S$-dimensional prediction distribution over steps, where $S=|Y|$. We form the target distribution by placing all probability mass on the best textual step description $y_i^*$ for each clip $v_i$ according to the weak supervision process. That is, 
\begin{equation}
    y_i^* = \argmax_{y \in Y} p(y \mid v_i).
\end{equation}
We calculate the cross entropy between the predicted and target distributions for each masked out clip, yielding the following expression:
\begin{equation}
    -\log([f_{\text{VT}}(V_{\setminus M})]_{j})
\end{equation}
where $j$ is the index of $y_i^*$ in $Y$, i.e., such that $y_i^* = Y_j$. To get the final training objective for a single masked video $V_{\setminus M}$, we sum over all indices $i \in M$, and minimize with respect to $\theta$.

\noindent \textbf{Distribution matching loss.} For this objective, we treat the distribution of step labels $p(y_i \mid v_i)$ from weak supervision as the target distribution for each clip $v_i$. We then compute the KL Divergence between the prediction distribution $f_{\text{VT}}(V_{\setminus M})$ and the target distribution $p(y_i \mid v_i)$ as follows:
\begin{equation}
    \sum_{j'=1}^S p(Y_{j'} \mid v_i) \log{\frac{p(Y_{j'} \mid v_i)}{[f_{\text{VT}}(V_{\setminus M})]_{j'}}}
\end{equation}
We sum over all $i \in M$ and minimize with respect to $\theta$.
Following \cite{lin2022learning}, we use only the top-$k$ steps in $p(y_i \mid v_i)$ and set the probability of the remaining steps to 0.

%We also train the step classification model $f_{\text{VT}}(\xxl)$ to minimize the KL-Divergence between the predicted distribution $f_{\text{VT}}(\xxl)$ and the target distribution $P(y_s|v_i)$ in Eq.~\ref{eq:asr_step}:
% \begin{equation} 
%     \min _\theta \sum_{n,s} P(y_s|v_i) \log \frac{P(y_s|v_i)}{\left[ f_{\text{VT}}(\xxl; \theta)  \right]_{(n,s)}}.
% \end{equation}

Lin \emph{et al.}~\cite{lin2022learning} show that the distribution matching loss results in a slight improvement over step classification loss. For VideoTaskformer, we find both objectives to have similar performance and step classification outperforms distribution matching on some downstream tasks. 

% The final equations for VideoTaskformer are:

% \begin{align}
%     \label{eq:ivsum}
%     y'_{v_i} &= f_{\text{VT}}(\xxl)\\
%     &= f_{\text{head}}(f_{\text{trans}}(\text{concat}(f_{\text{vid}}(\xxl)))) \; \forall \; i \in K\\
%     \mathcal{L}_{KL} &= \sum_{i \in \mathcal{N}} \text{KLDivergence}\: (\left[ f_{\text{VT}}(\xxl; \theta)  \right],P(y_s|v_i) \\
%     \mathcal{L}_{SC} &= \sum_{i \in \mathcal{N}} \text{CrossEntropy}\: (y_{v_i}; y'_{v_i})
%     \label{eq:mse}
% \end{align}

We use $f_{\text{VT}}$ as a feature extractor (layer before softmax) to extract step representations for new video segments. 

\subsection{Downstream Tasks}
\label{sec:tasks}

To show that the step representations learned by VideoTaskformer capture task structure and semantics, we evaluate the representations on 6 downstream tasks---3 new tasks which we introduce (mistake step detection, mistake ordering detection, and long-term step forecasting) and 3 existing benchmarks (step classification, procedural activity recognition, and short-term step forecasting). We describe the dataset creation details for our 3 new benchmarks in Sec.~\ref{sec:dataset}.

% What is a good way to determine if a video model can fully comprehend the semantics of a task from an instructional video? Here we propose one way of evaluating this - checking if the video model can detect irregularities in an instructional video. \ning{I dislike using } We introduce and evaluate our method on two new benchmarks, Mistake step detection and Mistake ordering detection.
% In this work, we introduce a new evaluation benchmark for detecting incorrect steps in videos. We propose two simple strategies to synthetically introduce mistakes in videos. Given videos from the COIN dataset where we know the start and end of each step, we construct the following datasets:
% (i) Mistake order detection: randomly re-order the steps to generate a new video where the steps are in the incorrect order
% (ii) Mistake step detection: replace a randomly chosen step in the video with a step from another video

\noindent\textbf{Mistake Detection.} A critical aspect of step representations that are successful at capturing the semantics and structure of a task is that, from these representations, \textit{correctness} of task execution can be verified. We consider two axes of correctness: content (what steps are portrayed in the video) and ordering (how the steps are temporally ordered). We introduce 2 new benchmark tasks to test these aspects of correctness.

\noindent\textbullet~\textbf{Mistake step detection.} 
The goal of this task is to identify which step in a video is incorrect. More specifically, each input consists of a video $V = \{v_{1}, \dots, v_{K}\}$ with $K$ steps.  $V$ is identical to some unaltered video $V_1$ that demonstrates a correctly executed task, except that step $v_j$ (for some randomly selected $j \in [1, \dots, K]$) is replaced with a random step from a different video $V_2$. The goal of the task is to predict the index $j$ of the incorrect step in the video.

\noindent\textbullet~\textbf{Mistake ordering detection.}
In this task, the goal is to verify if the steps in a video are in the correct temporal order. The input consists of a video $V = \{v_{1}, \dots, v_{K}\}$ with $K$ steps. There is a 50\% probability that $V$ is identical to some (correctly ordered) video $V_1 = \{v^1_{1}, \dots, v^1_{K}\}$, and there is a 50\% probability that the steps are randomly permuted. That is, $v_i = v^1_{\pi_i}$ for some random permutation $\pi$ of indices $[1,\dots, K]$. The goal of the task is to predict whether the steps are ordered correctly or are permuted. 

\noindent\textbf{Step Forecasting.} As another way to evaluate how learned step representations capture task structure, we test the capabilities of our model in anticipating future steps given one or more clips of a video.   

\noindent\textbullet~\textbf{Short-term forecasting.}
Consider a video $V = \{v_1, \dots, v_n, v_{n+1}, \dots v_K\}$ where $v_i$ denotes a step, and $V$ has step labels $\{y_1, \dots, y_K\}$, where $y_i \in Y$, the finite set of all step labels in the dataset. Short-term forecasting involves predicting the step label $y_{n+1}$ given the previous $n$ segments $\{v_1, \dots, v_n\}$~\cite{lin2022learning}.   

\noindent\textbullet~\textbf{Long-term step forecasting.} We introduce the challenging task of long-term step forecasting. Given a single step $v_i$ in a video $V = \{v_1, \dots, v_K\}$ with step labels $\{y_1, \dots, y_K\}$, the task is to predict the step labels for the next 5 steps, i.e. $\{y_{i+1}, y_{i+2}, \ldots, y_{i+5}\}$. This task is particularly challenging since the network receives very little context---just a single step---and needs to leverage task information learned during training from watching multiple different ways of executing the same task.    

\noindent\textbf{Procedural Activity Recognition.} The goal of this task is to recognize the procedural activity (i.e., task label) from a long instructional video. The input to the network is all the $K$ video clips corresponding to the steps in a video, $V = \{v_1, \dots, v_K\}$. The task is to predict the video task label $t \in \mathcal{T}$ where $\mathcal{T}$ is the set of all task labels for all the videos in the dataset.

\noindent\textbf{Step Classification.} In this task, the goal is to predict the step label $y_i \in Y$ given the video clip corresponding to step $v_i$ from a video $V = \{v_1, \dots, v_K\}$. No context other than the single clip is given. Therefore, this task requires fine-grained recognition capability, which would benefit from representations that contain information about the context in which a step gets performed.

For all of the above tasks, we use the step and task label annotations as supervision. We show the ``zero-shot'' performance of VideoTaskformer by keeping the video model $f_{\text{vid}}$ and the transformer layer $f_{\text{trans}}$ fixed and only fine-tuning a linear head $f_{\text{head}}$ on top of the output representations. Additionally, we also show fine-tuning results where we keep the base video model $f_{\text{vid}}$ fixed and fine-tune the final transformer $f_{\text{trans}}$ and the linear layer $f_{\text{head}}$ on top of it. The network is fine-tuned using cross-entropy loss with supervision from the step labels for all downstream tasks. 

\subsection{Implementation Details}
\label{sec:impl}
\noindent \textbf{Step labels from Weak Supervision.} To train VideoTaskformer, we require step annotations, i.e., step labels with start and end timestamps in the video, for a large corpus of instructional videos. Unfortunately, this is difficult to obtain manually and datasets that provide these annotations, like COIN and CrossTask, are small in size ($\sim$10K videos). To overcome this issue,  the video speech transcript can be mapped to steps in WikiHow and used as a weak form of supervision \cite{lin2022learning}. The intuition behind this is that WikiHow steps are less noisy compared to transcribed speech.   

The WikiHow dataset contains a diverse array of articles with step-by-step instructions for performing a range of tasks. Denote the steps across all $T$ tasks in WikiHow as $s = \{s_1, \ldots, s_N$\}, where $s_n$ represents the natural language title of the $n$th step in $s$, and $N$ is the number of steps across all tasks in WikiHow. Each step $s_n$ contain a lengthy language-based description which we denote as $y_n$. 

Consider a video with $K$ sentences in the automatic speech transcript denoted as $\{a_1, \ldots, a_K\}$. Each sentence is accompanied by a $\{start, end\}$ timestamp to localize it in the video. This yields $K$ corresponding video segments denoted as $\{v_1, \ldots, v_K\}$. Each video segment $v_i$ is a sequence of $F$ RGB frames having spatial resolution $H \times W$.
To obtain the step label for a segment $v_i$,  the corresponding sentence in the transcript $a_i$ is used to find the distribution of the nearest steps in the WikiHow repository.
Following~\cite{lin2022learning}, we approximate this distribution using a textual similarity measure $\text{sim}$ between $y_n$ and $a_i$:

% \begin{equation}
%     P(y_s|v_i) \approx \frac{\exp{(\text{sim} (a_i,y_s))}}{\sum_{t,s} \exp{(\text{sim} (a_i,y_s))}}.
%     \label{eq:asr_step}
% \end{equation}

\begin{equation}
    P(y_n|v_i) \approx \frac{\exp{(\text{sim} (a_i,y_n))}}{\sum_{n'} \exp{(\text{sim} (a_i,y_{n'}))}}.
    \label{eq:asr_step}
\end{equation}

The authors of \cite{lin2022learning} found it best to search over all the steps across all tasks (i.e., all $y_n$), rather than the set of steps for the specific task referenced in the video. The  similarity function $\text{sim}$ is formulated as a dot product between language embeddings obtained from a pre-trained language model. 


\noindent \textbf{Language model.} To compare WikiHow steps to the transcribed speech via the $\text{sim}$ function, we follow the same setup as in Lin \emph{et al.}~\cite{lin2022learning}. For a fair comparison to the baseline, we use MPNet (paraphrase-mpnet-base-v2)  to extract sentence embeddings $\in \mathbb{R}^{768}$. 

\noindent \textbf{Video model.} VideoTaskformer is a TimeSformer model with a two-layer transformer. Following \cite{lin2022learning}, the TimeSformer is initialized with ViT \cite{dosovitskiy2020image} pre-trained on ImageNet-21K, and is trained on subsampled clips from HowTo100M (with 8 frames sampled uniformly from each 8-second span).

We include additional implementation details in the Supplemental. 

