\section{Datasets and Evaluation Metrics}
\label{sec:dataset}
% insert figure from dataset 
% insert dataset statistics - number of videos (train/val/test), number of mistakes, easy/hard mistakes  

\noindent \textbf{Pre-training.} For pre-training, we use videos and transcripts from the HowTo100M (HT100M)~\cite{miech2019howto100m} dataset and steps from the WikiHow dataset~\cite{bertasius2021space}. HT100M contains 136M video clips from 1.2M long narrated instructional videos, spanning 23k activities such as ``gardening'' and ``personal care.'' The WikiHow dataset contains 10,588 steps collected from 1059 WikiHow articles which are sourced from the original dataset~\cite{koupaee2018wikihow}. 

\noindent \textbf{Evaluation.} 
All evaluation benchmarks use videos and step annotations from the COIN dataset~\cite{tang2019coin}. COIN consists of 11,827 videos related to 180 different tasks and provides step labels with start and end timestamps for every video. We use a subset of 11,104 videos that were available to download.

As described in Sec. \ref{sec:tasks}, we introduce 3 new benchmark tasks in this work: mistake step detection, mistake ordering detection, and long-term step forecasting. 

\noindent \textbf{\textit{Mistake Step Detection.}} For creating the mistake step detection dataset, for every video in the COIN dataset, we randomly replace one step with a step from a different video. The network predicts the index of the mistake step. We use the same train/validation/test splits as in COIN and report average accuracy of predicting the mistake step index on the test set. 

\noindent \textbf{\textit{Mistake Ordering Detection.}} We synthetically create the mistake ordering detection dataset by randomly shuffling the ordering of the steps in a given video, for 50\% of the videos and train the network to predict whether the steps are in the right order or not. While creating the
dataset, we repeatedly shuffle the input steps until the shuffled “mistake” order is different from the original valid order. Additionally, we compare the shuffled “mistake” order
across all the videos in the same task, to ensure it doesn’t
match any other video’s correct ordering of steps. Despite these two pre-processing checks, there might be noise in the dataset. We report average prediction accuracy on the test split.

\noindent \textbf{\textit{Long-term step forecasting.}} Given a video clip corresponding to a single step, long-term step forecasting involves predicting the step class label for the next 5 consecutive steps. If there are fewer than 5 next steps we append NULL tokens to the sequence. We compute classification accuracy as the number of correct predictions out of the total number of predictions, ignoring NULL steps. We again use the same splits in the COIN dataset. 

Additionally, we evaluate on 3 existing benchmarks: \textbf{\textit{step classification}}~\cite{tang2019coin} - predicts the step class label from a single video clip containing one step, \textbf{\textit{procedural activity recognition}}~\cite{tang2019coin} - predicts the procedure/task label given all the steps in the input video, and \textbf{\textit{short-term step forecasting}}~\cite{lin2022learning} - predicts the class of the step in the next segment given as input the sequence of observed video segments up to that step (excluded).  





