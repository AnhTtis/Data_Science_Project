\section{Related Works}

\noindent \textbf{Instructional Video Datasets and Tasks.} Large-scale narrated instructional video datasets~\cite{damen2018scaling,miech2019howto100m,tang2019coin,zhou2018towards,zhukov2019cross} have paved the way for learning joint video-language representations and task structure from videos. More recently, datasets such as Assembly-101 dataset~\cite{Sener_2022_CVPR} and Ikea ASM~\cite{ben2021ikea} provide videos of people assembling and disassembling toys and furniture. Assembly-101 also contains annotations for detecting mistakes in the video. Some existing benchmarks for evaluating representations learned on instructional video datasets include step localization in videos~\cite{damen2018scaling,tang2019coin}, step classification~\cite{damen2018scaling,tang2019coin,zhukov2019cross}, procedural activity recognition~\cite{tang2019coin}, and step forecasting~\cite{lin2022learning}. In our work, we focus on a broad range of instructional videos found in HowTo100M~\cite{miech2019howto100m} and evaluate the learned representations on the downstream tasks in COIN~\cite{tang2019coin} dataset. We additionally introduce 3 new benchmarks for detecting mistakes in instructional videos and forecasting long-term activities. 

\noindent \textbf{Procedure Learning from Instructional Videos.} Recent works have attempted to learn procedures from instructional videos~\cite{bansal2022my, chang2020procedure, lin2022learning, qian2022svip, wang2022multimedia}. Most notably, \cite{chang2020procedure} generates a sequence of actions given a start and a goal image. \cite{bansal2022my} finds temporal correspondences between key steps across multiple videos while \cite{qian2022svip} distinguishes pairs of videos performing the same sequence of actions from negative ones. \cite{lin2022learning} uses distant supervision from WikiHow to localize steps in instructional videos. Contrary to prior works, our step representations are aware of the task structure as we learn representations globally for all steps in a video jointly, as opposed to locally, as done in past works. 

\noindent \textbf{Video Representation Learning.} There has been significant improvement in video action recognition models over the last few years \cite{arnab2021vivit,fan2021multiscale,feichtenhofer2019slowfast,liu2022video}. All of the above methods look at trimmed videos and focus on learning short-range atomic actions. In this work, we build a model that can learn longer and more complex actions, or steps, composed of multiple short-range actions. For example, the first step in Fig.~\ref{fig:overview}, \emph{``Make batter''}, is composed of several atomic actions such as \emph{``pour flour''} and \emph{``whisk''}. There have also been works~\cite{lin2022learning,miech2020end,qiu2017learning,sun2019videobert,xu2015discriminative} which learn representations for longer video clips containing semantically more complex actions. Our work falls into this line of work.     


