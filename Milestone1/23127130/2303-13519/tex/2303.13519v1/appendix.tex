% \documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{lipsum}

% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref} 

% % Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \usepackage[font={small}]{caption}
% \usepackage{float}

% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}
% \newcommand\trevor[1]{\textcolor{magenta}{[TD: #1]}}
% \newcommand\ning[1]{\textcolor{blue}{[Ning: #1]}}
% \newcommand\blfootnote[1]{%
%     \begingroup
%     \renewcommand\thefootnote{}\footnote{#1}%
%     \addtocounter{footnote}{-1}%
%     \endgroup
% }
% \DeclareMathOperator*{\argmin}{argmin}
% \DeclareMathOperator*{\argmax}{argmax}

% \def\bert{BERT\xspace}
% \def\vbert{VideoBERT\xspace}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).


% \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{8756} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% % Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

% \begin{document}

% \title{Supplemental Materials: \\ Learning and Verification of Task Structure in Instructional Videos}
% \maketitle

\newpage
\clearpage

\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thetable}{T\arabic{table}}
\renewcommand{\thefigure}{F\arabic{figure}}

\section*{Supplementary Materials}

In this section, we describe additional implementation details of our method and provide more qualitative results and comparisons on all the 6 downstream tasks. 

\section{Implementation Details}
\noindent\textbf{Pre-training.} The base video model is a Timesformer~\cite{bertasius2021space} model with a ViT backbone initialized with ImageNet-21K ViT pretraining~\cite{dosovitskiy2020image}. We pre-train our model on 64 A100 GPUs for 20 epochs which takes ~120 hours for all the videos in the HowTo100M dataset. We use a batch size of 64 videos (1 video per GPU), each consisting of 12 segments. To train the model, we use SGD optimizer with momentum and weight decay. The learning rate is set to 0.01 and is decayed using a step learning rate policy of 10\% decay at steps 15 and 19. We perform a second round of pretraining for 15 epochs using AdamW~\cite{loshchilov2018decoupled} with a learning rate of 0.00005. 

We use a 15\% masking ratio during pre-training. Segment transformer $f_{\text{trans}}$ is a two layer transformer with 12 video segments as input. Each segment consists of 8 embedding vectors extracted from a series of 8 adjacent 8-second clips from the input video (spanning a total of 64 seconds). It has a 768 embedding dimnesion and 12 heads, along with learnable positional encodings at the beginning. The WikiHow knowledgebase has 10588 step classes all of which are used for training the network with step classification loss. For obtaining the distant supervision from WikiHow and mapping ASR text to step labels in the WikiHow knowledge base, we follow the setup described in ~\cite{lin2022learning}.   

\vspace{2mm}
\noindent\textbf{Fine-tuning.} For mistake step detection, mistake ordering detection, long term and short term step forecasting, and procedural activity recognition the input consists of 12 segments from the video. We fine-tune only the segment transformer $f_{\text{trans}}$ and the linear head $f_{\text{head}}$ using cross entropy loss, while the keeping the base TimeSformer video model $f_{\text{vid}}$ as a fixed feature extractor. We use a learning rate of 0.005 with a step decay of 10\% and train the network for 50 epochs using sgd optimizer.       

For the step classification task, we only fine-tune the linear head, while keeping both the base video model and the 2 layer segment transformer fixed. We use a learning rate of 0.005 with a step decay of 10\% and train the network for 50 epochs using sgd optimizer.

\section{Additional Quantitative Results}
\noindent\textbf{Activity Recognition.} In Tab.~\ref{tab:epic}, we include results for activity recognition on EPIC-KITCHENS-100 by fine-tuning our pre-trained model for noun, verb, and action recognition tasks. We outperform all baselines on noun recognition, and are on par with MoViNet~(\emph{Kondratyuk et al., CVPR 2021}) on action recognition.  

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lllccc}
\toprule 
Model & Action (\%) & Verb (\%) & Noun (\%) 
\\ \midrule
MoViNet & \textbf{47.7} & \textbf{72.2} & 57.3  \\
LwDS: TimeSformer  & 44.4 & 67.1 & 58.1 \\ \hline
\textbf{VideoTaskformer (SC)} & \textbf{47.6} & 70.4 & \textbf{59.8} \\
\bottomrule
\end{tabular}
\caption{\textbf{Activity Recognition on EPIC-KITCHENS-100.}}
\vspace{-.1cm}
\label{tab:epic}
\vspace{-.2cm}
\end{table}

\vspace{2mm}
\noindent\textbf{Evaluating on step localization}: We evaluate our pre-trained embeddings on the action segmentation task in COIN. Following previous work, we train a linear head on top of our fixed features and predict action labels for non-overlapping 1-second 
input video segments. LwDS attains 67.6\% on this task, and our method achieves 69.1\%.


\noindent\textbf{Step labels as input}: Our method uses visual features since step labels are not always available during inference. Nevertheless, for the purpose of comparison, we assume we have access to ground-truth step labels during inference and include results for all tasks. The results shown in Tab.~\ref{tab:epic} are from training a single layer transformer on the COIN train set and evaluating on the test set, i.e. there is no pre-training. As expected, using step labels makes the task much simpler and it outperforms using visual features. However, adding task label information to visual features improves performance significantly for all the tasks.   
\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lcccccc}
\toprule 
\multirow{2}{*}{Task} & \multicolumn{2}{c}{Step Labels(\%)} & \multicolumn{2}{c}{Visual Features (\%)} \\
& - & w/ Task label & - & w/ Task label \\ \midrule
Short term forecasting & 65 & 68 & 20 & 49 \\
Long term forecasting & 50 & 53 & 14 & 40\\ 
Mistake Ordering & 80 & 82 & 60 & 65\\
Mistake Step & 64 & 68 & 28 & 33\\
\bottomrule
\end{tabular}
\caption{\textbf{Step labels vs Visual features.}}
\vspace{-.1cm}
\label{tab:epic}
\vspace{-.3cm}
\end{table}

\section{Additional Qualitative Results}
\noindent\textbf{Step Classification.} We compare results from our method VideoTaskformer, to the baseline LwDS~\cite{lin2022learning} in Fig.~\ref{fig:sc}. Since our model was trained on the entire video by masking out segments, it has a better understanding of the relationship between different steps in the same task, i.e. learned representations are \emph{``context-aware''}. As a result, it is better at distinguishing the steps within a task and correctly classifies all the steps in the four examples shown here. LwDS on the other hand incorrectly classifies all of the steps. For reference, we show a keyframe from the correct video step clip corresponding to the incorrect step class chosen by LwDS. The input image clips and the correct clips for the LwDS predictions are closely related and contain similar objects of interest, they correspond to different stages of the task and contain different actions. Since our model learns step representations \emph{``globally''} from the whole video, it is able to capture these subtle differences. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/res_sc.pdf}
    \caption{\textbf{Step classification.} We qualitatively compare results from our method (VideoTaskeformer) to the baseline LwDS on the step classification task. While the inputs are video clips, we only show a keyframe from the clip for visualization purposes. Correct predictions (VideoTaskformer) are shown in green and incorrect predictions (LwDS) are in red. We also show a frame from the clip corresponding to the incorrect prediction made by LwDS.}
    \label{fig:sc}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/res_mo.pdf}
    \caption{\textbf{Mistake Order Detection.} Qualitative comparison of results from VideoTaskformer to LwDS. Step and task labels shown along with the input are for visualization purpose only. Correct answers are shown in green and incorrect answers in red.}
    \label{fig:mo}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/res_ms.pdf}
    \caption{\textbf{Mistake Step Detection.} Qualitative comparison of results from VideoTaskformer to LwDS. Step and task labels shown along with the input are for visualization purpose only. Correct answers are shown in green and incorrect answers in red.}
    \label{fig:ms}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/res3.pdf}
    \caption{Qualitative results for \textbf{procedural activity recognition, short term step forecasting, and long term step forecasting}. Step and task labels shown along with the input are for visualization purpose only. Correct answers are shown in green and incorrect answers in red.}
    \label{fig:res3}
\end{figure*}

\vspace{2mm}
\noindent\textbf{Mistake Ordering Detection.} Fig.~\ref{fig:mo} compares results of our method VideoTaskformer to the baseline LwDS on the mistake ordering detection task. We show two examples, \emph{``lubricate a lock''} and \emph{``change guitar string''}, where the steps in the input are swapped as shown by red arrows. Our method correctly detects that the input steps are in the incorrect order whereas the baseline predicts the ordering to be correct. As seen, detecting the order requires a high level understanding of the task structure, which our model learns through masking. 

\vspace{2mm}
\noindent\textbf{Mistake Step Detection.} Qualitative comparison on the mistake step detection task is shown in Fig.~\ref{fig:ms}. The input consists of video clip steps for the task \emph{``change battery of watch''}. The second step is swapped with an incorrect step from a different task. Our method correctly identifies the index of the mistake step 1, whereas the baseline predicts 3 which is incorrect. We show the correct step for visualization purposes. 

\vspace{2mm}
\noindent\textbf{Procedural Activity Recognition.} A result is shown in Fig.~\ref{fig:res3}. VideoTaskformer's representations are context-aware and can identify the right task given the sequence of clips, \emph{``paste car sticker''}. The baseline misidentifies the task as an incorrect similar task, \emph{``remove scratches from windshield''}.

\vspace{2mm}
\noindent\textbf{Short-term Step Forecasting.} Fig.~\ref{fig:res3} shows an input consisting of two clips corresponding to the first two steps for the task \emph{``open lock with paper clips''}. The clips are far apart temporally, so the model needs to understand broader context of the task to predict what the next step is. Our method VideoTaskformer correctly identifies the next step as \emph{``insert paper clip into lock''} whereas the baseline incorrectly predicts a step \emph{``install the new doorknob''} from another task.

\vspace{2mm}
\noindent\textbf{Long-term Step Forecasting.} In Fig.~\ref{fig:res3} we compare the future steps predicted by our model and the baseline LwDS on the long-term step forecasting task. Both models only receive a single clip as input, corresponding to the first step \emph{``unscrew the screws used to fix the screen''} of the task \emph{``replace laptop screen''}. Our model predicts all the next 4 ground-truth steps correctly, and in the right order. The baseline on the other hand predicts steps from the same task but in the incorrect order. 

All of the above qualitative results further support the effectiveness of learning step representations through masking, and show that our learned step representations are \emph{``context-aware''} and possess \emph{``global''} knowledge of task-structure.  

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \end{document}