\section{Introduction}

\begin{figure}[t]
  \centering
     \includegraphics[width=\linewidth]{figure/teaser_v2.pdf}
   \caption{\textbf{Illustration.} UrbanGIRAFFE generates a photorealistic image given a sampled panoptic prior in the form of a semantic voxel grid and object layout. Our method enables diverse controllability regarding camera pose, instance, and stuff.}
   \label{fig:overview}
   \vspace{-0.2cm}
\end{figure}

Generating photorealistic urban scenes has many applications in simulation, gaming and virtual reality. 
Unfortunately, designing diverse urban scenes with novel 3D visual content is typically expensive and time-consuming as it requires the expertise of professional artists. 

Recent advances in generative models have demonstrated a promising direction to reduce the cost via learning to generate images from data.
Ideally, the generated scenes should be controllable in terms of camera pose and 3D content. For example, the camera should be able to move freely in the scene with six degrees of freedom. The poses of instantiated objects (\eg, cars) should be able to be manipulated independently. Furthermore, the layout of the scene should be controllable.

There are many attempts to generate photorealistic urban images. 
Several methods study semantic image synthesis to transfer a 2D semantic segmentation map to an RGB urban scene image~\cite{Isola2017CVPR,Park2019CVPRa,Schonfeld2021ICLR}. However, when changing the camera poses, the generated images across multiple frames may not be consistent using such 2D generative models.
Recently, 3D-aware generative models have witnessed a rapid progress by lifting the generation process to the 3D space. Despite achieving multi-view consistency, most existing 3D-aware generative models are limited to object-centric images, e.g., faces and cars~\cite{Schwarz2020NIPS,Chan2021CVPR,Chan2022CVPR}. 
There are a few attempts to generate scene images in a compositonal manner~\cite{Liao2020CVPRa, Niemeyer2021CVPR,Nguyen-Phuoc2020NEURIPS,Xu2022ARXIV}. However, all these methods struggle to learn a good geometry of the background and hence do not support large camera movement, e.g., moving the camera along the road. Another line of work enables camera movement but ignores the compositional nature of the scene, thus lacking controllability of the 3D content~\cite{DeVries2021ICCV,Bautista2022NEURIPS}.





In this paper, we propose UrbanGIRAFFE to address the challenging task of compositional and controllable 3D-aware image synthesis of urban scenes, see \figref{fig:overview}. Our key idea is to leverage scene-level but coarse 3D panoptic prior, simplifying the task of learning complex geometry through 2D supervision and incorporating semantic information for scene editing. The panoptic prior, including semantic voxel grids of uncountable stuff and bounding boxes of countable objects, can be obtained from existing datasets~\cite{Liao2022PAMI} or inferred from pre-trained models~\cite{Cao2022CVPR}.
Specifically, our model represents the scene as compositional neural feature fields consisting of stuff, objects, and sky.  %
We propose a semantic voxel-conditioned stuff generator, effectively preserving the semantic and geometry information provided by the prior. In terms of objects, we follow GIRAFFE~\cite{Niemeyer2021CVPR} to generate objects in canonical space by leveraging the object layout prior.  We further model the sky and far regions using a sky generator.
With all three generators, we render a composited feature map via volume rendering and upsample it to the target image using a neural renderer. 
For the complicated urban scenes, we observe that training with an adversarial loss on the full image alone is insufficient. We additionally employ an adversarial loss applied to objects and a reconstruction loss to the stuff image regions to improve the image fidelity.

Our contributions are as follows. i) We propose to study the challenging task of 3d-aware generative models for urban scenes with diverse controllability in terms of large camera movement, objects manipulation and stuff editing.  ii) We leverage coarse 3D panoptic prior to address this challenging task and design compositional generative radiance fields that leverages the prior information effectively.
iii) Our method demonstrates state-of-the-art performance compared to existing methods on both synthetic and real-world datasets, including the challenging KITTI-360 dataset. 
