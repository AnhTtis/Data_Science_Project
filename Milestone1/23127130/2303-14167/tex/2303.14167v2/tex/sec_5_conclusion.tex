\section{Conclusion}
\label{sec:conclusion}

We propose UrbanGIRAFFE to tackle controllable 3D-aware image synthesis for challenging urban scenes. By effectively incorporating 3D panoptic prior, our model decomposes the scene into stuff, objects, and sky. Our compositional generative model enables diverse controllability regarding large camera viewpoint change, semantic layout, and object manipulation. We believe that our method pushes the frontier of 3D-aware generative models for unbounded scenes with complex geometry. In future work, it can be augmented with a semantic voxel generator for sampling novel scenes. Further, our method does not disentangle light from ambient color, which is worth investigating to enable lighting control.

