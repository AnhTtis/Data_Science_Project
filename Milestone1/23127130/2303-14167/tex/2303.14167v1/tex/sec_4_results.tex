\input{gfx/results/comp_baseline_kitti360}

\input{gfx/results/comp_baseline_clevr}

\section{Experimental Results}
\label{sec:experimenmt}
In this section, we first compare our method to several 2D and 3D baselines on both synthetic and real-world datasets. Subsequently, we design a number of controllable urban scene editing experiments to evaluate the preferences of our synthesis model with regards to controllability and fidelity. We further conduct ablation studies to better understand the influence of different architectural components. 


\input{gfx/results/tables/baseline_comp}


\boldparagraph{Datasets} We conduct experiments on two multi-object datasets with diverse backgrounds. %
\textbf{KITTI-360}~\cite{Liao2022PAMI} is an outdoor sub-urban dataset containing complex scene geometry.
Furthermore, scenes in KITTI-360 are replete with highlights and shadows, causing the appearance of the same object to vary greatly in different scenes. 
KITTI-360 provides coarse 3D bounding primitives in cuboids and spheres for both stuff and objects. We consider cars as objects since cars are important for driving scenarios. For stuff regions, we simply convert the coarse 3D bounding primitives to semantic voxel grids.
We further create an augmented \textbf{CLEVR-W} dataset following CLEVR~\cite{Johnson2017CVPR}. In contrast to existing methods~\cite{Niemeyer2021CVPR,Xu2022ARXIV} that places objects on a simple flat background in CLEVR, we introduce walls into the background. We consider the wall and the floor as stuff regions.
Please refer to the supplementary material for additional information regarding the CLEVR-W dataset.

\boldparagraph{Baselines} We compare our approach to two state-of-the-art models GIRAFFE~\cite{Niemeyer2021CVPR} and GSN~\cite{DeVries2021ICCV} for 3D-aware image synthesis. To further evaluate the fidelity of the synthesized image, we additionally compare our method with a state-of-the-art 2D method, StyleGAN2~\cite{Karras2020CVPRa}.


\input{gfx/results/control_full_kitti360}

\boldparagraph{Metrics}
 We report the FID~\cite{Heusel2017NIPS} and KID~\cite{Binkowski2018ICLR} scores to quantify image quality. We use 5k real and fake samples to calculate the FID and KID score.


\subsection{Comparison to the State of the Art}

\boldparagraph{Quantitative Comparison}
\tabref{tab:comp_baselines} shows the quantitative comparison on KITTI-360 and CLEVR-W. Note that GSN requires training on sequential frames, thus we omit GSN on the CLEVR-W dataset which does not contain sequential data. The quantitative comparison shows that our method greatly outperforms existing state-of-the-art 3D methods regarding image fidelity and is comparable to the 2D baseline.  

\boldparagraph{Qualitative Comparison}
We compare our method with GIRAFFE and GSN on KITTI-360 in \figref{fig:com_baseline_kitti360} with the camera moving forward. 
Note that GIRAFFE struggles to learn the complicated background geometry of urban scenes. This distracts the GAN training, thus leading to low-quality results even in a static scenario (the first row). Compared with GIRAFFE, GSN's scene representation is built upon a local 2D feature map, enabling it to model relatively complex 3D scenes. Therefore, GSN performs better in the static scenario, but the image quality drops dramatically as the camera moves forward. 
As a comparison, our method is conditioned on a 3D semantic voxel grid, thus enabling  photorealistic and consistent 3D-aware image synthesis even with a large camera moving distance. 

\figref{fig:com_baselines_clevr} shows the qualitative comparison with GIRAFFE on CLEVR-W. We conduct various experiments including stuff editing (\eg, editing the height of the wall or moving it closer to the objects), object rearrangement, and camera viewpoint manipulation. Note that GIRAFFE performs well on the foreground objects but still lags behind on the background. In contrast, our method can keep high fidelity and 3D consistency under these experiments, which clearly outperforms the baseline method. 






\input{gfx/results/ablation_kitti360}

\input{gfx/results/comp_gt_kitti360}

\subsection{Controllable Urban Scene Generation} 
We now demonstrate the diverse controllability of our model in terms of stuff editing, object editing and camera viewpoint control.

\boldparagraph{Stuff Editing}
Our semantic-conditioned stuff generator enables fine-grained stuff editing by modifying the conditioning semantic voxel. As shown in \figref{fig:stuff_editing}. We can transfer stuff semantics like ``Road to Grass'' and ``Building to Tree''. It is also possible to edit the occupancy of the voxel grids, \eg, ``Lower building'' and ``Move tree''. All these stuff editings are achieved by modifying the semantic voxel grid without additional optimization.

It is worth mentioning that, in the ``Building To Tree'' example,  the shadow of the road also changes to a large degree after the editing. This suggests that our method not only allows for photorealistic and semantically-align urban scene generation but also learns the implicit relationship between the shadow condition and semantic layout. 

\boldparagraph{Object Editing}
Next, we conduct various experiments on object editing in \figref{fig:object_editing}. As in GIRAFFE~\cite{Niemeyer2020ARXIV}, we can add/delete objects, and control their appearance, rotation, and translation.
Our object experiments with object editing do not affect the appearance of other scene parts, suggesting that our method can disentangle objects from the complex background by leveraging the panoptic prior.

\boldparagraph{Camera Control}
Finally,  \figref{fig:camera_control} shows that our method also allows for large viewpoint control, including large rotation in azimuth and polar angles as well as in-plane rotation. 
We can also change the camera's focal length, successfully capturing a photorealistic wide-angle image. 




\subsection{Ablation Study}

To verify our design choices, we conduct ablation studies on the KITTI-360 dataset, and evaluate both image-level and patch-level FID/KID scores in  \tabref{tab:ablation_study}.
 

\input{gfx/results/tables/ablation_comp}



\boldparagraph{Reconstruction Loss}
We first validate the role of the reconstruction loss. After removing the reconstruction loss, the FID and KID scores drop significantly  (w/o $\cL_{recon}$).  This is unsurprising as the reconstruction loss provides stronger supervision to align the generated scenes with the ground truth. \figref{fig:ablation} shows that removing the reconstruction loss can also lead to reasonable performance, but yields more artifacts. Moreover, reconstruction loss is particularly important for infrequently encountered semantic classes.  
For example, removing reconstruction loss results in the model rendering the ``rail track'' as grass, while the full model can render it with the corresponding semantic meaning faithfully (see 3rd row of \figref{fig:ablation}). Note that our full model can  maintain high fidelity while still exhibiting differences from the ground truth image, see \figref{fig:ablation_comp_gt}.
These findings suggest our full model can produce diverse results instead of simply remembering the entire dataset.

\boldparagraph{Object Discriminator}
Next, we exclude the adversarial loss $\cL_{adv}^{\bP}$ applied to object patches and train the object generator solely through the image adversarial loss $\cL_{adv}^{\bI}$. As shown in \tabref{tab:ablation_study}, removing $\cL_{adv}^{\bP}$ significantly increases the patch FID$_{\bP}$ and KID$_{\bP}$. This can also be seen from the qualitative results in \figref{fig:ablation}, where the cars are of lower quality when removing the object discriminator.
It is worth noting that FID$_\bI$ is less affected, indicating that in scenes where the proportion of objects pixels is not large, the global adversarial training cannot provide enough supervision to optimize objects which we actually care, and hence introducing  $\cL_{adv}^{\bP}$ is important to improve visual quality.


\boldparagraph{All Stuff} Lastly, we remove the object generator $G_\theta^{obj}$ and use the stuff generator to represent the full scene except for the sky (w/o $G_\theta^{obj}$), similar to the GSN approach. This can also be considered as a generative version of GANCraft~\cite{Hao2021ICCV}. As shown in \figref{fig:ablation}, the quality of objects drops significantly. This verifies the importance of decomposing stuff and objects to learn high-fidelity object generation. Note that $\cL_{adv}^{\bP}$ is also not applied in this experiment as there is no information of object instance.



