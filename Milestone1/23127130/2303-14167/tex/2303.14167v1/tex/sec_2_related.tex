\section{Related Work}
\label{sec:related_work}



\boldparagraph{Conditional Image synthesis}
In recent years, Generative Adversarial Networks~\cite{Goodfellow2014NIPS, Karras2019CVPR,Karras2020CVPRa,Karras2020NeurIPS,Karras2021NIPS,SauerS022SIGGRAPH} have achieved impressive results in photorealistic image synthesis. As it is not straightforward to control the generated images of unconditional GANs, many attempts have been made for conditional image synthesis. A line of works generates images conditioned on a 2D semantic segmentation map~\cite{Isola2017CVPR,Park2019CVPRa,Schonfeld2021ICLR}. Instead of requiring per-pixel semantic annotation, another line of methods generates images following an image layout in the form of 2D bounding boxes~\cite{Zhao2020IJCV,Yang2022CVPR,He2021CVPR} or learned  blobs~\cite{Epstein2022ECCV}. 
When changing the camera viewpoint, the generated images across different views are typically not multi-view consistent, as discussed in~\cite{Hao2021ICCV}. We instead learn a 3D-aware conditional generative model that leads to better consistency with the underlying 3D representation.



\boldparagraph{3D-Aware Image Synthesis}
3D-aware generative models have received growing attention recently. While early works learn to generate 3D voxel grids~\cite{Nguyen-Phuoc2019ICCV,Henzler2019ICCV}, recent methods achieve high-fidelity 3D-aware image synthesis leveraging neural radiance fields as the underlying 3D representation~\cite{Schwarz2020NIPS,Chan2022CVPR,Chan2021CVPR,Schwarz2022NEURIPS,Deng2022CVPR,Xu2022CVPR,gu2021stylenerf}. Empowered by 3D-aware generative models, many promising applications has been demonstrated, including semantic editing~\cite{Sun2022CVPR,Sun2022TOG}, relighting~\cite{Tan2022SIGGRAPH, Lee2022ARXIV}, single-view reconstruction~\cite{Cai2022CVPR,Muller2022CVPR} and articulated human generation~\cite{Zhang2022ECCV,Noguchi2022ECCV,Bergman2022ARXIV,Hong2022ARXIV}. However, all aforementioned methods focus on object-centric scenes and assume that the object lies in a canonical object coordinate system. Thus, it is non-trivial to extend these methods to complex, unaligned urban scenes. 
GSN~\cite{DeVries2021ICCV} and GAUDI~\cite{Bautista2022ARXIV} propose to generate unbounded indoor scenes. However, both ignore the compositionality of the scene, thus making it harder to achieve high visual fidelity and do not support editing of the scene content. 

A few works exploit the compositionality of 3D scenes to generate scenes containing multiple objects~\cite{Liao2020CVPRa,Xu2022ARXIV,Nguyen-Phuoc2020NEURIPS,Niemeyer2021CVPR,Xue2022CVPR}. 
As they consider the compositionality of foreground objects only, these methods are incapable of modeling complex background geometry in urban scenes. A concurrent work, DiscoScene~\cite{Xu2022ARXIV}, also study 3D-aware generative model of urban scenes. Despite achieving high-fidelity image synthesis, DiscoScene does not support camera control or stuff editing in urban scenes.

\boldparagraph{Neural Radiance Fields}
We proposed to present the scene as compositional neural feature fields. Exploiting implicit neural representations~\cite{Mescheder2019CVPR, Park2019CVPR}, NeRF~\cite{Mildenhall2020ECCV} has enabled impressive novel view synthesis by training a single model for each scene. Many exciting works have shown its potential in real-time rendering~\cite{Reiser2021ICCV, mueller2022instant}, geometric reconstruction~\cite{Wang2021NIPS, Wang2022ARXIV}, semantic segmentation~\cite{Zhi2021ICCV,Fu2022THREEDV,Kundu2022CVPR},  and view synthesis from sparse input~\cite{Wang2021ibrnet,yu2021pixelnerf,Chen2021ICCVmvs}.
It has been shown that NeRF can also be extended to model unbounded urban scenes ~\cite{Rematas2022CVPR, Ost2021CVPR} and scale to city level~\cite{Tancik2022CVPR,Xiangli2021ARXIV,Xiangli2022ECCV}. 
While all these methods focus on reconstructing existing scenes, we aim to learn a conditional generative model that can generate urban images conditioned on different panoptic layouts. A more related work, GANCraft~\cite{Hao2021ICCV}, aims to generate a scene based on semantic voxels, yet it also requires per-scene optimization. In contrast, our generative model allows for stuff editing by manipulating the semantic voxels.