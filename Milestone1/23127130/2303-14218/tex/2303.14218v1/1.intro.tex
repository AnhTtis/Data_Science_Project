\section{Introduction}
As a common atmospheric phenomenon, haze noticeably degrades the quality of photographed images, severely limiting the performance of subsequent high-level visual tasks such as vehicle re-identification~\cite{chen2022sjdl} and scene understanding~\cite{sakaridis2018model}. Similar to the emergence of other image restoration task solvers~\cite{wen2019single,zhang2019fast,du2020blind,du2023dsdnet}, valid image dehazing techniques are required for handling vision-based applications.
\begin{figure}[t]
	%	\captionsetup{justification=centering}
	\includegraphics[width=\linewidth]{fig/teaser.pdf}
	\captionsetup{justification=justified}
	\caption{Upper panel: Examination for contrastive regularization based on three difficulty levels of the negatives in the consensual contrastive space. Lower panel: Illustration of contrastive samples in the consensual and non-consensual spaces.}
	\label{fig:teaser}\vspace{-6mm}
\end{figure}

\vspace{-4mm}Deep learning based methods have achieved tremendous success in single image dehazing and can be roughly categorized into two classes: physics-free methods~\cite{chen2019gated, hong2020distilling,dong2020multi,guo2022image} and physics-aware methods~\cite{ren2016single,cai2016dehazenet,dong2020physics,chen2021psd}. Regarding the former, most of them usually use ground-truth images with predicted restorations to enforce L1/L2 distance-based consistency and also involve various regularizations~\cite{zhang2018densely,liu2019griddehazenet} as additional constraints to cope with the ill-posed property. Notice that all of those regularizations ignore the information from negative images as a lower bound, contrastive regularization (CR)~\cite{wu2021contrastive} is proposed to introduce different hazy images as negatives and the ground-truth image as the positive and further uses contrastive learning~\cite{He2020,hadsell2006dimensionality} to guarantee a closed solution space. Moreover, it is shown that better performances can be achieved when using more negatives since diverse degraded patterns are included as cues. However, the issue is that the contents of those negatives are distinct from the positive, and their embeddings may be too distant, leaving the solution space still under-constricted. 

To remedy this issue, a natural idea is to use the negatives in the \textit{consensual} contrastive space\footnote{In this space, the contents of the negatives are identical to the positive sample, except for the haze distribution. Here, we use the terms (non-)consensual contrastive space and (non-)consensual space interchangeably, and a negative in the consensual space is denoted as a consensual negative.} (see the lower panel in Fig.~\ref{fig:teaser}) as better lower-bound constraints, which can be easily assembled from the hazy input and the corresponding restorations by other existing methods. In such cases, the negatives can be ``closer'' to the positive than those in the non-consensual space since the diversity of such negatives is more associated with the haze (or haze residue) rather than any other semantics. However, an intrinsic dilemma arises when the embedding of a negative is too close to that of the positive, as its pushing force to an anchor (\ie, the prediction) may cancel out the pulling force of the positive. Such a learning difficulty can confuse the anchor to move towards the positive, especially in the early training stage. 

This intuition is further examined in the upper panel of Fig.~\ref{fig:teaser}. We use FFA-Net~~\cite{qin2020ffa} as baseline (row (a)) and SOTS-indoor~\cite{li2018benchmarking} as the testing dataset to explore the impact of the negatives in the consensual space with diverse difficulty. Specifically, we define the difficulty of the negatives into three levels: easy (E), hard (H), and ultra-hard (U). We adopt the hazy input as the easy negative, and use a coarse strategy to distinguish between the latter two types, \ie, whether the PSNR of the negative is greater than 30. First, in the single-negative case (row (b)-(d)), an interesting finding is that using a hard sample as negative achieves the best performance compared to the other two settings, and using an ultra-hard negative is even worse than the baseline. This reveals that a ``close'' negative has the potential to promote the effectiveness of the dehazing model, but not the closer the better due to the learning difficulty. While in the multi-negative case\footnote{We give each negative the same weight in the regularization under this case, and we omit the cases of E=0, which would drastically decrease the performance. We will discuss the reason for this in Sec.~\ref{section:A}.} (row (e)-(g)), we have observed that comprehensively covering negatives with different difficulty levels, including ultra-hard samples, can lead to the best performance. It implies the negatives at different difficulty levels can all contribute to the training phase. These observations motivate us to explore how to wisely arrange the multiple negative pairs in a consensual space into the CR during training.

Moving on to the realm of physics-aware deep models, most of them utilize the atmospheric scattering model~\cite{mccartney1976optics,nayar1999vision} in the raw space, without fully exploring the beneficial feature-level information. PFDN~\cite{dong2020physics} is the only work that attempts to express the physics model as a basic unit in the network. The unit is designed as a shared structure to predict the latent features corresponding to the atmospheric light and transmission map. Nevertheless, the former is usually assumed to be homogeneous while the latter is non-homogeneous, and thus their features cannot be approximated in the same way. Therefore, it is still an open problem how to accurately realize the interpretability of the feature space of the deep network using the physics model, which is another aspect we are interested in.

In this paper, we propose a curricular contrastive regularization using hazy or restored images as negatives in the consensual space for image dehazing to address the first issue. Informed by our analysis, which suggests that the difficulty of consensual negatives can impact the effectiveness of the regularization, we present a curriculum learning strategy to arrange these negatives to mitigate learning ambiguity. Specifically, we split the negatives into three types (\ie, easy, hard, and ultra-hard) and assign different weights to corresponding negative pairs in CR. Meanwhile, the difficulty levels of the negatives are dynamically adjusted as the anchor moves towards the positive in the representation space during training. In this way, the proposed regularization can facilitate the dehazing models to be stably optimized in a more compact solution space.

We propose a physics-aware dual-branch unit (PDU) regarding the second issue. The PDU approximates the features corresponding to both the atmospheric light and transmission map in dual branches, considering the physical characteristics of both factors. The features of the latent clear image can thus be synthesized more precisely in line with the physics model. Finally, we establish C$^2$PNet, our dehazing network that deploys the PDU into a cascaded backbone with curricular contrastive regularization.

In summary, our key contributions are as follows:
\begin{itemize}
	\item \vspace{-2mm}We propose a novel C$^2$PNet for haze removal that employs curricular contrastive regularization and enforces physics-based prior in the feature space. Our method outperforms SOTAs in both synthetic and real-world scenarios. In particular, we achieve significant PSNR boosts of 3.94dB and 1.50dB on the SOTS-indoor and SOTS-outdoor datasets, respectively.
	
	\item \vspace{-2mm}The proposed regularization adopts a unique consensual negative-based approach for dehazing and incorporates a self-contained curriculum learning strategy that dynamically calibrates the priority and difficulty levels of the negatives. It is also proven to enhance the performance of SOTAs as a generalized regularization technique, surpassing previous related strategies. 
	
	\item \vspace{-2mm}With careful consideration of the characteristics of factors involved, we built the PDU based on an unprecedented expression of the physics model. This innovative design promotes feature transmission and extraction in the feature space, guided by physics priors.
	
\end{itemize}



