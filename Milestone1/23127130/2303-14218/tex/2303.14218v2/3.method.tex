\section{Method}
\label{section:A}
\subsection{Overview}
Our goals are two-fold: 1) to promote the interpretability of the feature space for haze removal and 2) to establish a more concise solution space using of contrastive samples. Fig.~\ref{fig:method} illustrates the detailed structure of our C$^2$PNet. To achieve our first goal, we design a physics-aware dual-branch unit that is derived from the atmospheric scattering model. Regarding our second aim, we tailor a contrastive regularization using consensual negatives, along with a self-contained curriculum learning strategy to deal with the learning difficulty. Note that our curricular contrastive regularization is network-agnostic, making it applicable to other dehazing networks.
%\\
%\textbf{Physics-aware Dual-branch Unit.}
\subsection{Physics-aware Dual-branch Unit}
The atmospheric scattering model is commonly used to describe the formation of a hazy image $I$. It can be mathematically formulated as $I(x)=T(x)J(x)+(1-T(x))A$, where $J$ represents the clear image, $T$ is the transmission map, $A$ indicates the atmospheric light, and $x$ denotes the index of pixels. As both $T$ and $A$ are unknown, haze removal is a highly ill-posed problem. Raw space based methods directly estimate the two unknown factors, which can easily lead to cumulative errors. In contrast, imposing physics priors in the feature space can encourage the interpretability that aligns with the hazing process, without relying on the ground truths of $T$ and $A$. Inspired by FDU~\cite{dong2020physics}, we propose a physics-aware dual-branch Unit (PDU) that is derived from the physics model in the feature space, as shown in Fig.~\ref{fig:block}. 
\begin{figure}[t]
	\center
	\includegraphics[width=\linewidth]{fig/PDU.pdf}
	\caption{The architecture of the proposed PDU.}
	\label{fig:block}\vspace{-6mm}	
\end{figure}

To begin with, we reformulate the physics model to represent the clear image $J$ as follows:
\begin{equation}
	\begin{aligned}
		J(x)&=I(x)\frac{1}{T(x)}+A(1-\frac{1}{T(x)})\\
		&=I(x)\frac{1}{T(x)}+A-A\frac{1}{T(x)}.
	\end{aligned}
    \label{equ:scatter}
\end{equation}
Then extracting features via kernel $k$, Eq.~\eqref{equ:scatter} can be reformulated as follows:
\begin{equation}
	k\circledast J=k\circledast(I\odot\frac{1}{T})+k\circledast A-k\circledast(A\odot\frac{1}{T}), 
	\label{equ:conv}
\end{equation}
%Similar to the derivation in FDU, further
where $\circledast$ indicates the convolution operator and $\odot$ denotes the Hadamard product.  Consequently, we respectively introduce the matrix-vector forms of $k$, $J$, $I$, $A$, $\frac{1}{T}$, \ie, $\bm{K}$, $\bm{J}$, $\bm{I}$, $\bm{A}$ and $\bm{D}$, and Eq.~\eqref{equ:conv} can be rewritten as 
\begin{equation}
	\bm{KJ}=\bm{KDI}+\bm{KA}-\bm{KDA}.
\end{equation}
Such a reformulation can be given by a few steps of algebra operations. Note that the diagonal vector of the diagonal matrix $\bm{D}$ corresponds to the vectorized form of $\frac{1}{T}$.

Next, we can decompose the matrix $\bm{KD}$ into a product of two matrices $\bm{QK}$. As $\bm{K}$, $\bm{D}$ and $\bm{Q}$ are all unknown, implementing this decomposition can be indicated as solving an underdetermined system of equations, which can guarantee the existence of $\bm{Q}$. And then, we have
\begin{equation}	
	\bm{KJ}=\bm{Q}(\bm{KI})+\bm{KA}-\bm{Q}(\bm{KA}).
	\label{equ:Q}		
\end{equation}

We can denote $\tilde{\bm{A}}$ as an approximation of the features $\bm{KA}$ that correspond to the atmospheric light and $\tilde{\bm{t}}$ as an approximation of $\bm{Q}$, which is associated with the transmission map. Furthermore, $\bm{KI}$ and $\bm{KJ}$ can be viewed as the extracted features of a hazy image and its corresponding clear image, respectively. Based on Eq.~\eqref{equ:Q}, and assuming that the channel number of the features $\tilde{\bm{t}}$ matches that of the input features $\bm{M}$, we can calculate the physics-aware features $\tilde{\bm{J}}$ by 
\begin{equation}	
	\begin{aligned}
		\tilde{\bm{J}}&=\bm{M}\odot\tilde{\bm{t}}+\tilde{\bm{A}}-\tilde{\bm{A}}\odot\tilde{\bm{t}}\\		&=\bm{M}\odot\tilde{\bm{t}}+\tilde{\bm{A}}(\bm{1}-\tilde{\bm{t}}),
	\end{aligned}
    \label{equ:final}
\end{equation}
where $\bm{1}$ indicates a matrix whose elements are all ones. 

Note that the second term on the right-hand side of Eq.~\eqref{equ:final} involves a synergistic action between $\tilde{\bm{A}}$ and $\tilde{\bm{t}}$ that is ignored by FDU. Then we can explicitly build the PDU based on Eq.~\eqref{equ:final}. One branch in PDU (see the upper part of Fig.~\ref{fig:block}) is used to produce $\tilde{\bm{A}}$. As the atmospheric light is usually assumed to be homogeneous, we use global average pooling (GAP($\cdot$)) to eliminate unnecessary information in the feature space. And $\tilde{\bm{A}}$ is produced by
\begin{equation}	
	\tilde{\bm{A}}=H(\sigma(\textrm{Conv}^N(\textrm{ReLU}(\textrm{Conv}^{\frac{N}{8}}(\textrm{GAP}(\bm{M})))))),		
\end{equation}
where $\sigma(\cdot)$ is the Sigmoid function, $H(\cdot)$ denotes a replication operation, $\textrm{Conv}^N(\cdot)$ is the convolutional layer with $N$ kernels, and $N$ is set to 64.

On the other hand, we cannot apply GAP$(\cdot)$ for the approximation of $\bm{Q}$ due to a loss of information, as the transmission map is non-homogeneous. Therefore, in the lower branch in Fig.~\ref{fig:block}, we choose to extract $\tilde{\bm{t}}$ using a sequence of convolutional layers, which is given by
\begin{equation}	
	\tilde{\bm{t}}=\sigma(\textrm{Conv}^N(\textrm{ReLU}(\textrm{Conv}^{\frac{N}{8}}(\textrm{Conv}^{N}(\bm{M}))))).		
\end{equation}

With the proposed PDU, interpretable features $\tilde{\bm{J}}$ can be generated from the input features $\bm{M}$ for restoring hazy images. Unlike FDU, which uses a shared structure with GAP$(\cdot)$ to predict latent features that are simultaneously correlated to both $T$ and $A$, the PDU attentively incorporates the corresponding physical characteristics of these two factors. This approach allows for more useful features to be estimated in a dual interactive paradigm. 

%\textbf{Curricular Contrastive Regularization.}
\subsection{Curricular Contrastive Regularization} 
Regarding the canonical contrastive regularization for image dehazing, the anchor is the recovered result by the dehazing network, the positive is the ground truth, and the negatives include a hazy input and multiple hazy images that are non-consensual with the positive. The target of this regularization $R$ is to minimize the L1 distance between the embeddings of the anchor and the positive while maximizing their distance from the negatives, which is given by
\begin{equation}
	R=\sum_{i=1}^n\xi_i\frac{||V_i(J)-V_i(f(I,\theta))||_1}{\sum_{q=1}^r||V_i(U_q)-V_i(f(I,\theta))||_1+E_i},		
\end{equation}
where $E_i=||V_i(I)-V_i(f(I,\theta))||_1$, $f(\cdot,\theta)$ indicates the dehazing network with parameters $\theta$, $V_i(\cdot), i=1,2,\cdots,n$ extracts the $i$th hidden features from the pre-trained VGG-19~\cite{simonyan2014very}, the number of non-consensual negatives $\{U_q\}$ is $r$, and $\{\xi_{i}\}$ is the set of hyperparameters. As illustrated in Fig.~\ref{fig:CC}, the introduced contrast between the anchor and non-consensual negatives cannot provide a satisfactory lower bound of the solution space. The non-consensual negatives are typically distantly located from the positive, leading to an under-constricted solution space that limits the quality of the restorations.

Based on our analysis of Fig.~\ref{fig:teaser}, we propose a novel contrastive regularization for haze removal that utilizes negatives in the consensual space, which can be restored results from other dehazing models. Our straightforward aim is to push the anchor far away from better-quality negatives. However, two critical problems arise: 1) how to define the difficulty of different negatives and 2) how to arrange these negatives according to their difficulty during training. 
\begin{figure}[t]
	\center
	\includegraphics[width=\linewidth]{fig/CC.pdf}
	\caption{Illustration of curricular contrastive regularization.}
	\label{fig:CC}\vspace{-4mm}
\end{figure}

To solve both issues, we incorporate a curriculum learning strategy into contrastive regularization. We define the difficulty of the negatives into three levels: easy, hard, and ultra-hard. For easy negative, we use the hazy input consistently. The difficulty levels of the other negatives are dynamically determined during training. Specifically, we measure the average PSNR performance of the network before every epoch begins. In the $t$th epoch, a negative is defined as an ultra-hard sample when its PSNR is higher than the network performance, or as a hard negative otherwise. 

To properly arrange these negatives, we weigh them differently according to their difficulty levels. First, the weight of easy negative is fixed and largest. This is because although hard and ultra-hard negatives may contribute to a more compact solution space, they can also cause learning ambiguity. To ensure that the resultant force is towards the positive such that the anchor is shifted in the desired direction, we give the easy negative a weight that is large enough. In practice, we set this weight to the number of the non-easy negatives $z$. Second, the weight of a non-easy negative $S_q$ at the $t$th epoch is defined as follows: 
\begin{small}
\begin{equation}
	W_t(S_q) = \left\{
	\begin{array}{rcl}
		1+\gamma, &\textrm{avgPSNR}(f(\{I_g\},\theta_{t-1}))\geq \textrm{PSNR}(S_q),\\
		1-\gamma, & \textrm{otherwise},\\
	\end{array} \right. 
	\label{equ:beta}
\end{equation}
\end{small}
where $\{I_g\}$ denotes the hazy input dataset, $q=1,2,\cdots,z$ is the index of the non-easy negatives, and $\gamma$ is a hyperparameter. The weights of the hard and the ultra-hard negatives are set to $1+\gamma$ and $1-\gamma$, respectively. This means that the weight of a hard negative is larger than that of an ultra-hard negative, allowing the hard negative to provide a greater force and alleviating the potential learning ambiguity. Furthermore, the flexibility of this strategy in determining the difficulty levels enables ultra-hard negatives to become hard ones in the later stage of training (see Fig.~\ref{fig:CC}). This makes sense because as the quality of the anchor improves, the ambiguity caused by ultra-hard samples is reduced, and their importance should be strengthened. In this way, the hard and ultra-hard negatives can be viewed as better lower bounds for effectively constraining the solution space. Then, our curricular contrastive regularization $R^*$is formulated as follows:
\begin{small}
\begin{equation}
	R^*=\sum_{i=1}^n\xi_i\frac{||V_i(J)-V_i(f(I,\theta))||_1}{\sum_{q=1}^zW_t(S_q)||V_i(S_q)-V_i(f(I,\theta))||_1+z\cdot E_i}.
	\label{equ:R}	
\end{equation}
\end{small}

Finally, our total objective $\cal L$, which consists of an L1 norm based fidelity term and our contrastive curricular regularization, is given by
\begin{equation}
	{\cal L}=||J-f(I,\theta)||_1+\lambda R^*.	
\end{equation}
%\\


\subsection{Network Architecture}
\begin{table*}[t]
	\caption{Quantitative Evaluations with the state-of-the-art methods on the synthetic and real-world datasets.}
	\centering
	\small
	\begin{tabular}{c||c||c|c||c|c||c|c||c|c||c}
		\toprule
		\multirow{2}*{Method} &\multirow{2}*{Venue\&Year}&\multicolumn{2}{c||}{SOTS-indoor} &\multicolumn{2}{c||}{SOTS-outdoor} &\multicolumn{2}{c||}{Dense-Haze} &\multicolumn{2}{c||}{NH-Haze2} &\multirow{2}*{\#Params} \\
		\cmidrule(lr){3-4}
		\cmidrule(lr){5-6}
		\cmidrule(lr){7-8}
		\cmidrule(lr){9-10}		
		&&PSNR&SSIM&PSNR&SSIM&PSNR&SSIM&PSNR&SSIM&\\
		\midrule
		DCP~\cite{he2010single}&TPAMI2010&16.62&0.8179&19.13&0.8148&11.01&0.4165&11.68&0.6475&-\\
		
		DehazeNet~\cite{cai2016dehazenet}&TIP2016&21.14&0.8472&22.46&0.8514&9.48&0.4383&11.77&0.6217&0.01M\\
		
		AODNet~\cite{li2017aod}&ICCV2017&19.06&0.8504&20.29&0.8765&12.82&0.4683&12.33&0.6311&0.002M\\	
		
		DM2F-Net~\cite{Deng2019}&ICCV2019&34.29&0.9728&34.50&0.9815&14.99&0.5640&20.46&0.8217&92.14M\\
		
		GCANet~\cite{chen2019gated}&WACV2019&30.06&0.9596&22.76&0.8887&12.62&0.4208&18.79&0.7729&0.70M\\
		
		GDN~\cite{liu2019griddehazenet}&ICCV2019&32.16&0.9836&30.86&0.9819&14.96&0.5326&19.26&0.8046&0.96M\\	
		
		MSBDN~\cite{dong2020multi}&CVPR2020&32.77&0.9812&34.81&0.9857&15.13&0.5551&20.11&0.8004&31.35M\\	
		
		FFA-Net~\cite{qin2020ffa}&AAAI2020&36.39&0.9886&33.57&0.9840&12.22&0.4440&20.00&0.8225&4.46M\\	
		
		AECR-Net~\cite{wu2021contrastive}&CVPR2021&37.17&0.9901&-&-&15.80&0.4660&20.68&0.8282&2.61M\\
		
		MAXIM-2S~\cite{tu2022maxim}&CVPR2022&38.11&0.9908&34.19&0.9846&-&-&-&-&14.1M\\	
		
		DeHamer~\cite{guo2022image}&CVPR2022&36.63&0.9881&35.18&0.9860&16.62&0.5602&19.18&0.7939&132.45M\\	
		
		UDN~\cite{hong2022uncertainty}&AAAI2022&38.62&0.9909&34.92&0.9871&-&-&-&-&4.25M\\		
		\midrule
		\textbf{C$^2$PNet}   &&\textbf{42.56}&\textbf{0.9954}&\textbf{36.68}&\textbf{0.9900}&\textbf{16.88}&\textbf{0.5728}&\textbf{21.19}&\textbf{0.8334}&7.17M\\		
		\bottomrule
	\end{tabular}
	\label{tab:quantitative}
\end{table*}
\begin{figure*}[t]
	\centering
	%	\footnotesize
	\setlength{\abovecaptionskip}{0cm}
	\setlength{\tabcolsep}{0.05em}
	\setlength{\fboxrule}{1pt}
	\setlength{\fboxsep}{0pt}
	\begin{tabular}{cccccccc}			   		
		PSNR / SSIM& $18.09 /  0.7459 $ & $31.55 / 0.9793$ & $34.41 / 0.9811$ & $36.69 / 0.9838$ & $37.10 / 0.9825$ & $41.20 / 0.9914$ & $\infty / 1$ \\			
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/hazy.png} &
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/aod.png} &
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/gdn.png} &
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/ffa.png} &
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/maxim.png} &
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/dehamer.png} &
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/ours.png}&
		\includegraphics[width=.12\linewidth]{fig/indoor/rect/clear.png}\\	
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/hazy.png}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/aod.png}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/gdn.png}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/ffa.png}}&
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/maxim.png}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/dehamer.png}}&
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/ours.png}}&
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/indoor/crop/clear.png}}\\
		Hazy Image &AODNet~\cite{li2017aod}&GDN~\cite{liu2019griddehazenet}&FFA-Net~\cite{qin2020ffa}&MAXIM~\cite{tu2022maxim}&DeHamer~\cite{guo2022image}&C$^2$PNet (Ours)&GT
	\end{tabular}
	\caption{Visual results of SOTS-indoor dataset by different methods. (Zoom in for better view.)
	}
	\label{fig:indoor}
\end{figure*}

Our C$^2$PNet adopts an FFA-Net-like backbone because: 1) FFA-Net has a simple structure that cascades several FA blocks without any other redundant modules, and 2) the FA block is simple and has been proven to be practical. Since the proposed PDU mainly focuses on refining spatial information, we deploy it into each FA block by replacing the PA module. In this way, the features are enforced to conform to the hazing process before being fed into the subsequent module. Note that all other network parameters of C$^2$PNet are identical to those of FFA-Net, except for the PDUs.


