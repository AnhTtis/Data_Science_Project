\section{Experiments}
\label{exp}
\begin{figure*}[t]
	\centering
	%	\footnotesize
	\setlength{\abovecaptionskip}{0cm}
	\centering
	\setlength{\tabcolsep}{0.05em}
	\setlength{\fboxrule}{1pt}
	\setlength{\fboxsep}{0pt}
	\begin{tabular}{cccccccc}
		PSNR / SSIM& 17.16 / 0.8792  & 23.12 / 0.9598 & 26.64 / 0.9757 & 27.39 / 0.9718 & 29.94 / 0.9758 & 31.10 / 0.9859 & $\infty$ / 1 \\	   		
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/hazy.jpg} &
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/aod.jpg} &
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/gdn.png} &
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/ffa.png} &
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/maxim.png} &
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/dehamer.png} &
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/ours.png}&
		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/clear.png}\\	
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/hazy.jpg}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/aod.jpg}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/gdn.png}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/ffa.png}}&
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/maxim.png}} &
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/dehamer.png}}&
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/ours.png}}&
		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/clear.png}}\\
		Hazy Image &AODNet~\cite{li2017aod}&GDN~\cite{liu2019griddehazenet}&FFA-Net~\cite{qin2020ffa}&MAXIM~\cite{tu2022maxim}&DeHamer~\cite{guo2022image}&C$^2$PNet (Ours)&GT
	\end{tabular}
	\caption{Visual results of SOTS-outdoor dataset by different methods. (Zoom in for better view.)
	}
	\label{fig:outdoor}
\end{figure*}


%	}
%	\label{fig:outdoor}
%\end{figure*}



%\begin{figure*}[t]
%	\centering
%	%	\footnotesize
%	\setlength{\abovecaptionskip}{0cm}
%	\centering
%	\setlength{\tabcolsep}{0.05em}
%	\setlength{\fboxrule}{1pt}
%	\setlength{\fboxsep}{0pt}
%	\begin{tabular}{cccccccc}
%		PSNR / SSIM& 17.16 / 0.8792  & 23.12 / 0.9598 & 26.64 / 0.9757 & 27.39 / 0.9718 & 29.94 / 0.9758 & 31.10 / 0.9859 & $\infty$ / 1 \\	   		
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/hazy.jpg} &
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/aod.jpg} &
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/gdn.png} &
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/ffa.png} &
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/maxim.png} &
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/dehamer.png} &
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/ours.png}&
%		\includegraphics[width=.12\linewidth]{fig/outdoor/rect/clear.png}\\	
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/hazy.jpg}} &
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/aod.jpg}} &
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/gdn.png}} &
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/ffa.png}}&
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/maxim.png}} &
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/dehamer.png}}&
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/ours.png}}&
%		\fcolorbox{red}{red}{\includegraphics[width=.117\linewidth]{fig/outdoor/crop/clear.png}}\\
%		Hazy Image &AODNet&GDN&FFA-Net&MAXIM-2S&DeHamer&C$^2$PDN (Ours)&GT
%	\end{tabular}
%	\caption{Visual results of SOTS-outdoor dataset by different methods. (Zoom in for better view.)


\subsection{Experimental Settings}
\textbf{Implementation Details.} We implement C$^2$PNet using Pytorch 1.11.0 on an NVIDIA RTX 3090 GPU. Adam optimizer is used with exponential decay rates $\beta_1=0.9$ and $\beta_2=0.999$. The initial learning rate is set to 0.0001 and is scheduled by cosine annealing strategy~\cite{he2019bag}. The batch size is set to 2. We empirically set the penalty parameters $\lambda$ to 0.2, and $\gamma$ to 0.25 for 200 epochs. We follow CR~\cite{wu2021contrastive} that set the L1 distance in Eq.\eqref{equ:R} after the latent features of the 1st, 3rd, 5th, 9th and 13th layers from the fixed pre-trained VGG-19, and their corresponding weights $\xi_i, i=1,\cdots,5$ to $\frac{1}{32},\frac{1}{16},\frac{1}{8},\frac{1}{4},$ and 1, respectively.
%\\

\textbf{Datasets.} For fair comparisons, we evaluate the proposed method on synthetic datasets and real-world datasets. RESIDE~\cite{li2018benchmarking} is a widely used benchmark dataset. Among the five subsets, we select ITS and OTS as our training datasets and SOTS-indoor and SOTS-outdoor as our testing datasets for synthetic image dehazing. We also use two real-world datasets: Dense-Haze~\cite{ancuti2019dense} and NH-Haze2~\cite{ancuti2021ntire} for real image dehazing.
%\\

\textbf{Competitors and Evaluation Metrics.} We compare our method with the prior-based method (\eg, DCP~\cite{he2010single}), physical model based methods(\eg, DehazeNet~\cite{cai2016dehazenet}, AOD-Net~\cite{li2017aod}, and DM2F-Net~\cite{Deng2019}), and hazy-to-clear image translation based methods (\eg, GDN~\cite{liu2019griddehazenet}, GCANet~\cite{chen2019gated}, FFA-Net~\cite{qin2020ffa}, MSBDN~\cite{dong2020multi}, AECR-Net~\cite{wu2021contrastive}, MAXIM-2S~\cite{tu2022maxim}, DeHamer~\cite{guo2022image}, and UDN~\cite{hong2022uncertainty}). We utilize the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) to evaluate the performance.
\begin{figure*}[t]
	\centering
	\setlength{\abovecaptionskip}{0cm}
	\setlength{\fboxrule}{1pt}
	\setlength{\fboxsep}{0pt}
	\setlength{\tabcolsep}{0.05em}	
	\begin{tabular}{cccccccc}
		PSNR / SSIM& 11.56 / 0.4480  &17.81 / 0.5828 & 19.35 / 0.6666 & 19.83 / 0.6114 & 22.82 / 0.6312 & 22.94 / 0.6776 & $\infty$ / 1 \\	   		
		\includegraphics[width=.12\linewidth]{fig/NH19/hazy.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH19/AODNet.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH19/GDN.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH19/FFANet.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH19/AECRNet.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH19/Dehamer.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH19/ours.jpg}&
		\includegraphics[width=.12\linewidth]{fig/NH19/GT.jpg}\\			
%		Hazy Image &AODNet~\cite{li2017aod}&GDN~\cite{liu2019griddehazenet}&FFA-Net~\cite{qin2020ffa}&AECRNet~\cite{wu2021contrastive}&DeHamer~\cite{guo2022image}&C$^2$PNet (Ours)&GT
	\end{tabular}	
	\label{fig:NH19}
	
	\centering
	\setlength{\tabcolsep}{0.05em}	
	\begin{tabular}{cccccccc}
		PSNR / SSIM& 12.22 / 0.5895  &19.30 / 0.7741 & 18.09 / 0.8145 & 19.74 / 0.8312 & 17.21 / 0.7673 & 20.09 / 0.8281 & $\infty$ / 1 \\	   		
		\includegraphics[width=.12\linewidth]{fig/NH21/hazy.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH21/AODNet.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH21/GDN.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH21/FFANet.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH21/AECRNet.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH21/Dehamer.jpg} &
		\includegraphics[width=.12\linewidth]{fig/NH21/ours.jpg}&
		\includegraphics[width=.12\linewidth]{fig/NH21/GT.jpg}\\			
		Hazy Image &AODNet~\cite{li2017aod}&GDN~\cite{liu2019griddehazenet}&FFA-Net~\cite{qin2020ffa}&AECRNet~\cite{wu2021contrastive}&DeHamer~\cite{guo2022image}&C$^2$PNet (Ours)&GT
	\end{tabular}	
	\caption{Visual results of Dense-Haze (top) and NH-Haze2 (bottom) datasets by different methods. (Zoom in for better view.)}\vspace{-2mm}
	\label{fig:NH21}
\end{figure*}

%\\
\subsection{Comparison with SOTAs}
\textbf{Results on Synthetic Datasets.}
Regarding the evaluation of synthetic datasets, Table.~\ref{tab:quantitative} reports the average PSNR and SSIM values of different competitors for SOTS-indoor and SOTS-outdoor datasets. Our C$^2$PNet achieves the best performance on both datasets compared to other SOTAs, with 42.56dB PSNR and 0.9954 SSIM in SOTS-indoor, and 36.68dB PSNR and 0.9900 SSIM in SOTS-outdoor. Specifically, our method outperforms the second-best method UDN by a significant margin on SOTS-indoor, \ie, 3.94dB PSNR and 0.0045 SSIM. Moreover, our method achieves at least 1.50dB PSNR and 0.0029 SSIM performance gains on SOTS-outdoor. In addition, we respectively visualize the recovered images from the SOTS-indoor and the SOTS-outdoor datasets by different methods in Fig.~\ref{fig:indoor} and Fig.~\ref{fig:outdoor}. It can be observed that AODNet and GDN fail to remove most of the haze, while FFA-Net, MAXIM-2S, and DeHamer suffer from severe color distortion, and their results still contain some artifacts. Instead, our method generates the most natural restoration that preserves more details and involves fewer color distortions. Note that we can adjust the number of blocks in our network to balance the performance and the number of parameters. More details are included in the supplementary.
%\\

\textbf{Results on Real-world Datasets.}
We also evaluate the proposed C$^2$PNet on real-world datasets including Dense-Haze and NH-Haze2 datasets, summarizing the quantitative results in Table~\ref{tab:quantitative}. It is worth noting that removing haze from real-world images is much more challenging than from synthetic images. Nevertheless, our method outperforms all the other competitors on both datasets in terms of PSNR and SSIM. We also visualize the results in Fig.~\ref{fig:NH21}. Despite the reconstructions of all the comparisons generally being far from good, our method produces the most desired image that succeeded in removing most of the haze. 

\vspace{-2mm}\subsection{Ablation Study}\vspace{-2mm}
\begin{table}[t]
	\caption{Ablation study on C$^2$PNet with different modules and regularizations on SOTS-indoor dataset.}
	\centering
	\small
	\begin{tabular}{c||c|c}
		\toprule
		Model&PSNR&SSIM\\	
		\midrule
		base (FFA-Net) &36.39&0.9886\\
		
		base+FDU&36.59&0.9894\\
		
		base+PDU  &38.30&0.9914\\		
		
		base+PDU+CR(non-consensual,1:10)  &41.32&0.9947\\	
		
		base+PDU+CR(consensual,1:7)+w/o CL  &42.09&0.9951\\		
		\midrule
		\textbf{Ours (1:7)} &\textbf{42.56}&\textbf{0.9954}\\
		\bottomrule
	\end{tabular}\vspace{-5mm}
	\label{tab:ablation}
\end{table}

\begin{table*}[t]
	\caption{Evaluation of applying curricular contrastive regularization into SOTAs.}\vspace{-2mm}
	\centering	
	\resizebox{0.85\textwidth}{!}{
		\begin{tabular}{c|c|c|c||c||c|c|c|c|c}
			\toprule
			\multicolumn{4}{c||}{Regularization}&\multirow{2}*{Metric}&\multicolumn{5}{c}{Method}\\
			\cmidrule(lr){1-4}		
			\cmidrule(lr){6-10}
			CR&Space&CL&Rate&&GCANet~\cite{chen2019gated}&GDN~\cite{liu2019griddehazenet}&MSBDN~\cite{dong2020multi}&FFANet~\cite{qin2020ffa}&DMTNet~\cite{liu2021synthetic}\\
			\midrule
			\multirow{2}*{\XSolidBrush}&\multirow{2}*{N/A}&\multirow{2}*{N/A}&\multirow{2}*{N/A}&PSNR&30.06&32.16&33.79&36.39&28.53\\
			
			&&&&SSIM&0.9596&0.9836&0.9835&0.9886&0.96\\
			\midrule
			\multirow{2}*{\Checkmark}&\multirow{2}*{\footnotesize{non-consensual}}&\multirow{2}*{\XSolidBrush}&\multirow{2}*{1:10}&PSNR&29.83&33.36&34.74&37.21&30.88\\
			&&&&SSIM&0.9611&0.9867&0.9859&0.9920&0.9785\\
			\midrule
			\multirow{2}*{\Checkmark}&\multirow{2}*{\footnotesize{consensual}}&\multirow{2}*{\XSolidBrush}&\multirow{2}*{1:7}&PSNR&29.91&34.91& 34.95&38.93&31.16\\
			&&&&SSIM&0.9612&\textbf{0.9892}&0.9865&0.9936&0.9772\\
			\midrule
			\multirow{2}*{\Checkmark}&\multirow{2}*{\footnotesize{consensual}}&\multirow{2}*{\footnotesize{self-paced}}&\multirow{2}*{1:7}&PSNR&30.05&35.20&35.17&38.98&31.56\\
			&&&&SSIM&0.9596&0.9889&0.9861&0.9936&0.9776\\
			\midrule
			\multirow{2}*{\Checkmark}&\multirow{2}*{\footnotesize{\textbf{consensual}}}&\multirow{2}*{\footnotesize{\textbf{ours}}}&\multirow{2}*{\textbf{1:7}}&PSNR&\textbf{30.76}&\textbf{35.46}&\textbf{35.31}&\textbf{39.24}&\textbf{31.63}\\
			&&&&SSIM&\textbf{0.9668}&0.9880&\textbf{0.9875}&\textbf{0.9937}&\textbf{0.9791}\\ 		
			\bottomrule
	\end{tabular}}\vspace{-2mm}
	\label{tab:generality}
\end{table*}
In this section, we analyze the effectiveness of the different components of the proposed C$^2$PNet, including PDU, consensual negatives-based contrastive regularization (consensual CR), and curricular contrastive regularization (C$^2$R). Our \textit{base} network is FFA-Net, and subsequently, we establish five variants including 1) \textbf{base+FDU}: Replacing the PA module with FDU in the FA block. 2) \textbf{base+PDU}: Replacing the PA module with PDU in the FA block. 3) \textbf{base+PDU+CR(non-consensual, 1:10)}: Adding canonical contrastive regularization to base+PDU, with the rate between positive and negative samples being 1:10. 4) \textbf{base+PDU+CR(consensual, 1:7)+w/o CL}: Adding consensual CR without our curriculum strategy (CL) to base+PDU, with the rate between positive and negative samples being 1:7. 5) \textbf{Ours}: The full model of our C$^2$PNet. We list the results in Table~\ref{tab:ablation}, using the ITS dataset for training and SOTS-indoor for testing.
%\\

\textbf{Effectiveness of PDU.} The architecture of PDU is derived from Eq.~\eqref{equ:final} with a consideration of the physical characteristics of $A$ and $T$, which introduces a dual-branch interaction for the prediction of both factors. Since the features corresponding to $A$ and $T$ are disentangled by our PDU, the latent structural feature-level information is excavated more accurately. As a result, in Table~\ref{tab:ablation} we can see that the PDU achieves 1.71dB and 1.91dB gains over base+FDU and the base network, respectively. 

\textbf{Effectiveness of consensual CR.} 
We follow the same setting as non-consensual CR that considers at most 10 negatives due to the practicability towards training time and GPU memory limitations, and we use the optimal numbers of negatives for a fair comparison, \ie, 7 (consensual CR) vs. 10 (non-consensual CR). It can be observed that consensual CR remarkably boosts the performance against base+PDU and base+PDU+CR (non-consensual, 1:10) with PSNR improvements of 3.79dB and 0.77dB, respectively. Note that our training time is accelerated to 137 hours in contrast to 200 hours for non-consensual CR (1:10). These facts reinforce the superiority of consensual CR. More analysis can be found in the supplementary. 

%\\

\textbf{Effectiveness of C$^2$R.} Our full network employs the proposed CL strategy into consensual CR during training and performs the best in comparison with all the variants. Compared to base+PDU+CR(consensual, 1:7)+w/o CL, C$^2$PNet achieves an increase of 0.57dB in PSNR, revealing the effectiveness of the proposed C$^2$R.




\subsection{Generality Analysis for C$^2$R}
\vspace{-2mm}To further verify the generality of our C$^2$R, we apply it to different SOTA methods and compare it with several other universal regularizations. The results are summarized in Table~\ref{tab:generality}. Our method achieves significant improvements in PSNR and SSIM on all five SOTAs compared to other regularizations, except for a slight decrease of 0.0012 in SSIM compared to consensual CR on GDN. Specifically, our C$^2$R enhances the performances of the five baseline models with average PSNR improvements of 0.70-3.30dB, and is superior to CR (non-consensual,1:10) as a regularization term by average PSNR improvements of 0.93-2.10dB. In particular, compared to the popular self-paced CL strategy~\cite{Kumar2010}, our CL method yields a maximum increase of 0.71dB in PSNR. The possible reason is that using the self-paced strategy will feed the negatives into the regularization stage by stage, leading to 1) a two-level split of difficulty without considering the ultra-hard negatives and 2) all the introduced negatives share the same weight. However, as we analyzed before, both hard and ultra-hard samples can provide useful information for regularization during training, and the corresponding weights need to be delicately assigned separately.

\vspace{-2mm}\section{Discussion and Limitation}\vspace{-2mm}
An important advantage of negatives from existing dehazing models is the post-dehazing priors embedded in the recoveries, such as the distribution of the haze residue, which can indicate a more challenging pattern that is difficult to remove. This can provide valuable information to the model during training. However, as most existing methods perform poorly in real-world scenarios, it is hard to collect high-quality images as the non-easy (especially ultra-hard) negatives. This may limit the capacity of our model, despite achieving promising performance on real-world dehazing.

%\begin{table*}[t]
%	\caption{PSNR~(dB) and SSIM results on SOTS, Dense-Haze and NH-HAZE2. The top-2 performances are marked in \textcolor{red}{red} and \textcolor{blue}{blue}.}
%	\centering
%	\begin{tabular}{c||c|c||c|c||c|c||c|c|c}
%		\toprule
%		\multirow{2}*{Method} &\multicolumn{2}{c||}{SOTS-indoor} &\multicolumn{2}{c||}{SOTS-outdoor} &\multicolumn{2}{c||}{Dense-Haze} &\multicolumn{2}{c||}{NH-HAZE2} &\multirow{2}*{\#Params} \\
%		\cmidrule(lr){2-3}
%		\cmidrule(lr){4-5}
%		\cmidrule(lr){6-7}
%		\cmidrule(lr){8-9}		
%		&PSNR&SSIM&PSNR&SSIM&PSNR&SSIM&PSNR&SSIM&\\
%		\midrule
%		DCP \cite{he2010single}&16.62&0.8179&19.13&0.8148&11.01&0.4165&11.68&0.6475&-\\
%		
%		DehazeNet \cite{Cai2016dehazenet}&21.14&0.8472&22.46&0.8514&9.48&0.4383&11.77&0.6217&0.01M\\
%		
%		AODNet \cite{li2017aod}&19.06&0.8504&20.29&0.8765&12.82&0.4683&12.33&0.6311&0.002M\\	
%				
%		GCANet  \cite{chen2019gated} &30.06&0.9596&22.76&0.8887&12.62&0.4208&18.79&0.7729&0.70M\\
%		
%		GridDehazeNet \cite{liu2019griddehazenet} &32.16&0.9836&30.86&0.9819&14.96&0.5326&19.26&0.8046&0.96M\\	
%			
%		MSBDN \cite{dong2020multi} &32.77&0.9812&34.81&0.9857&15.13&0.5551&20.11&0.8004&31.35M\\	
%			
%		FFA-Net\cite{qin2020ffa} &36.39&0.9886&33.57&0.9840&12.22&0.4440&20.45&0.8043&4.46M\\	
%		
%		AECR-Net\cite{wu2021contrastive} &37.17&0.9901&-&-&15.80&0.4660&20.25&0.8331&2.61M\\
%		
%		MAXIM-2S~\cite{tu2022maxim}   &38.11&0.9908&34.19&0.9846&-&-&-&-&14.1M\\	
%		
%		DeHamer \cite{DeHamer2022} &36.63&0.9881&35.18&0.9860&16.62&0.5602&19.18&0.7939&132.45M\\	
%		
%	    UDN \cite{hong2022uncertainty} &38.62&0.9909&34.92&0.9871&-&-&-&-&4.25M\\		
%		\midrule
%		\textbf{C$^2$PDN}   &\textbf{42.56}&\textbf{0.9954}&\textbf{36.68}&\textbf{0.9900}&\textbf{16.92}&\textbf{0.5783}&\textbf{21.19}&\textbf{0.8334}&7.17M\\		
%		\bottomrule
%	\end{tabular}
%	\label{tab:color}\vspace{-3mm}
%\end{table*}


%\begin{figure*}[t]
%	\centering
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH19/hazy.png}		
%		\caption{Hazy Input}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH19/AODNet.png}	
%		\caption{AODNet}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH19/GDN.png}
%		\caption{GDN}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH19/FFANet.png}
%		\caption{FFANet}
%	\end{subfigure}
%    \begin{subfigure}[t]{0.12\linewidth}
%    	\centering
%    	\includegraphics[width=\textwidth]{fig/NH19/AECRNet.png}		
%    	\caption{AECRNet}
%    \end{subfigure}
%    \begin{subfigure}[t]{0.12\linewidth}
%    	\centering
%    	\includegraphics[width=\textwidth]{fig/NH19/Dehamer.png}	
%    	\caption{DeHamer}
%    \end{subfigure}
%    \begin{subfigure}[t]{0.12\linewidth}
%    	\centering
%    	\includegraphics[width=\textwidth]{fig/NH19/ours.png}
%    	\caption{C$^2$PDN}
%    \end{subfigure}
%    \begin{subfigure}[t]{0.12\linewidth}
%    	\centering
%    	\includegraphics[width=\textwidth]{fig/NH19/GT.png}
%    	\caption{GT}
%    \end{subfigure}
%	\caption{Dense-Haze. (Zoom in for better view.)}
%	\label{fig:NH19}
%\end{figure*}





%\begin{figure*}[t]
%	\centering
%	\setlength{\tabcolsep}{0.05em}	
%	\begin{tabular}{cccccccc}
%		PSNR / SSIM& 12.59 / 0.7155  &22.48 / 0.8547 & 24.13 / 0.9074 & 22.13 / 0.8879 & 22.82 / 0.8861 & 24.62 / 0.9147 & $\infty$ / 1 \\	   		
%		\includegraphics[width=.12\linewidth]{fig/NH21/hazy.png} &
%		\includegraphics[width=.12\linewidth]{fig/NH21/AODNet.png} &
%		\includegraphics[width=.12\linewidth]{fig/NH21/GDN.png} &
%		\includegraphics[width=.12\linewidth]{fig/NH21/FFANet.png} &
%		\includegraphics[width=.12\linewidth]{fig/NH21/AECRNet.png} &
%		\includegraphics[width=.12\linewidth]{fig/NH21/Dehamer.png} &
%		\includegraphics[width=.12\linewidth]{fig/NH21/ours.png}&
%		\includegraphics[width=.12\linewidth]{fig/NH21/GT.png}\\			
%		Hazy Image &AODNet&GDN&FFANet&AECRNet&DeHamer&C$^2$PDN (Ours)&GT
%	\end{tabular}	
%	\caption{NH-Haze2. (Zoom in for better view.)}
%	\label{fig:NH21}
%\end{figure*}


%\begin{figure*}[t]
%	\centering
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/hazy.png}		
%		\caption{Hazy Input}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/AODNet.png}	
%		\caption{AODNet}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/GDN.png}
%		\caption{GDN}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/FFANet.png}
%		\caption{FFANet}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/AECRNet.png}		
%		\caption{AECRNet}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/Dehamer.png}	
%		\caption{DeHamer}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/ours.png}
%		\caption{C$^2$PDN}
%	\end{subfigure}
%	\begin{subfigure}[t]{0.12\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{fig/NH21/GT.png}
%		\caption{GT}
%	\end{subfigure}
%	\caption{NH-Haze2. (Zoom in for better view.)}
%	\label{fig:NH21}
%\end{figure*}



%\begin{table}[t]
%	\caption{The effect of the number of blocks.}
%	\centering
%	\begin{tabular}{c||c|c|c|c}
%		\toprule
%		Method&\#Blocks&PSNR&SSIM&\#Params\\	
%	    \midrule		
%		\multirow{6}*{C$^2$PDN}&6&37.88&0.9926&2.41M\\
%		
%		&12&40.41&0.9945&4.62M\\
%	
%	    &18&42.33&0.9952&6.82M\\
%	    &19&42.56&0.9954&7.17M\\
%	    &20&42.69&0.9954&7.53M\\
%	    &24&43.37&0.9957&9.02M\\
%		\midrule
%		FFANet&-&36.39&0.9886&4.46M\\
%		AECRNet&-&37.17&0.9901&2.61M\\
%		MAXIM-2S&-&38.11&0.9908&14.1M\\
%	    DeHamer&-&36.63&0.9881&132.45M\\	
%		\bottomrule
%	\end{tabular}
%	\label{tab:12}
%\end{table}
%
%
%\begin{table}[t]
%	\caption{The effect of the number of negative samples.}
%	\centering
%	\begin{tabular}{c||c|c|c|c|c|c}
%		\toprule
%		\#Neg.&1&2&4&7&8&10\\	
%		\midrule		
%		PSNR&&42.08&&\textbf{42.56}&&41.91\\
%		\midrule
%		SSIM&&0.9952&&\textbf{0.9954}&&0.9951\\		
%		\bottomrule
%	\end{tabular}
%	\label{tab:12}
%\end{table}
%
%\begin{table}[t]
%	\caption{Performance on SOTS-indoor with respect to different values of the hyperparameter $\lambda$. $\lambda=0$ means to average the contributions of different negative samples.}
%	\centering
%	\begin{tabular}{c||c|c|c|c|c|c}
%		\toprule
%		$\lambda$&0&0.25&0.3&0.4&0.5&0.75\\	
%		\midrule		
%		PSNR&&\textbf{}&&&&\\
%		\midrule
%		SSIM&&\textbf{}&&&&\\		
%		\bottomrule
%	\end{tabular}
%	\label{tab:12}
%\end{table}

