\section{Related Work}
\label{related}
\begin{figure*}[t]
	\center
	\includegraphics[width=\linewidth]{fig/framework.pdf}
	\caption{Illustration of our C$^2$PNet for single image dehazing.}
	\label{fig:method}\vspace{-4mm}
\end{figure*}
\textbf{Single Image Dehazing.} 
Traditional single image dehazing methods are mainly based on an atmospheric scattering model~\cite{mccartney1976optics}. They focus on designing hand-crafted priors such as the dark channel prior~\cite{he2010single} and color attenuation prior~\cite{zhu2015fast}. However, these priors may not be powerful enough to characterize complex scenes in practice. Early learning-based methods~\cite{cai2016dehazenet,ren2016single} use deep neural networks to predict the transmission map and atmospheric light in the physics model to obtain a latent clear image. However, inaccuracies in the estimations may accumulate, hindering the reliable inference of the haze-free image. With the advent of large haze datasets~\cite{li2018benchmarking}, data-driven methods~\cite{chen2021psd,liu2021synthetic,qin2020ffa,guo2022image} have been developed rapidly. FFANet~\cite{qin2020ffa} introduces feature attention (FA) blocks that leverage both channel and pixel attention to improve haze removal. DeHamer~\cite{guo2022image} combines CNN and Transformer for image dehazing, which can aggregate long-term attention in Transformer and local attention in CNN features. Note that these methods do not consider the physics of the hazing process. Further, Dong \etal~propose a feature dehazing unit (FDU)~\cite{dong2020physics} derived based on the physics model. To the best of our knowledge, this work is the only one that considers the physics model in the feature space, avoiding the cumulative errors that occur in the raw space. However, FDU uses a shared structure to predict those unknown factors without considering their different physical characteristics. To solve this problem, we re-understand the physics model and construct a novel physics-aware dual-branch unit for image dehazing.

\textbf{Contrastive Learning.} 
In recent, contrastive learning has been broadly employed in high-level visual tasks~\cite{grill2020bootstrap,He2020,chen2020simple,guo2022hcsc}. The idea behind contrastive learning is to pull an anchor point closer to a positive point while simultaneously pushing it away from a negative point through a contrastive loss. However, there are only a few works that have applied contrastive learning to low-level vision problems. CR~\cite{wu2021contrastive} is one of the representative works, which introduces the concept of negative points for image dehazing. By considering the negative information as a lower bound of the solution space, CR can exploit both positive and negative information for training. However, most of the negatives are non-consensual and thus distantly represented from the positive, resulting in an under-constrained solution space. We aim to solve this issue with a novel curricular contrastive regularization approach that uses consensual negatives.
%\\

\textbf{Curriculum Learning.}
Inspired by the cognitive systems of humans, Elman~\cite{elman1993learning} emphasizes the importance of starting small in neural network training, which may be considered a prototype of curriculum learning. Later, Bengio \etal ~\cite{bengio2009curriculum} formally propose the curriculum learning strategy to arrange the training samples according to their difficulty. Nowadays, curriculum learning has been successfully applied to various cases including vision and language tasks~\cite{schroff2015facenet,korbar2018cooperative,duan2020curriculum,zhang2021flexmatch}. Building on our analysis that different consensual negatives exhibit varying learning difficulty, the question arises of how to arrange these samples during training. We propose to solve this issue via a self-contained curriculum learning strategy.


