\section{Introduction}
\label{sec:intro}
Semantic segmentation is a fundamental computer vision task with applications in numerous domains such as autonomous driving~\cite{cordts2016cityscapes,siam2017deep}, scene understanding~\cite{sless2019road}, surveillance~\cite{tseng2021person} and medical diagnosis~\cite{chen2020deep,hesamian2019deep}. As the advent of deep learning has significantly advanced the state-of-the-art, many new application areas have come to light and continue to do so too. This growth has brought and continues to bring exciting domain-specific datasets for segmentation tasks~\cite{islam2020semantic,li2020mas3k,Bodenstedt2018,liu2020fsd,WelinderEtal2010}. 

Today, the process of establishing machine learning-based segmentation models for any new application is relatively well understood and standard. Only once an image dataset is gathered and curated, can machine learning models be trained and validated. In contrast, building appropriate datasets is known to be difficult, time-consuming, and yet paramount. Beyond the fact that collecting images can be tedious, a far more challenging task is producing ground-truth segmentation annotations to subsequently train  (semi) supervised machine learning models. This is mainly because producing segmentation annotations often remains a manual task. As reported in~\cite{Bearman16}, generating segmentation annotations for a single PASCAL image~\cite{pascal-voc-2012} takes over 200 seconds on average. This implies over 250 hours of annotation time for a dataset containing a modest 5'000 images. What often further exacerbates the problem for domain-specific datasets is that only the dataset designer, or a small group of individuals, have enough expertise to produce the annotations (\eg, doctors, experts, etc.), making crowd-sourcing ill-suited. 
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{datasets.pdf}
\caption{Illustration of different semantic segmentation applications; OCT: Pathologies of the eye in OCT images, SUIM: Underwater scene segmentation~\cite{islam2020semantic}, Cityscape: street level scene segmentation~\cite{cordts2016cityscapes}, PASCAL VOC: natural object segmentation.}
\label{fig:datasets}
\end{figure*}

To overcome this challenge, different paradigms have been suggested over the years. Approaches such as Active Learning~\cite{Cai21,Casanova2020Reinforced,Konyushkova15} aim to iteratively identify subsets of images to annotate so as to yield highly performing models. Transfer learning has also proved to be an important tool in reducing annotation tasks~\cite{Ding2019,heker2020joint,kolesnikov2020big,koleshnikov2021,Liang2020,menegola2017}. For instance, \cite{Mensink}~show that training segmentation models from scratch is often inferior to using pre-training models derived from large image classification datasets, even when the target application domain differs from the source domain. Finally, weakly-supervised methods~\cite{ahn2018learning,Papandreou15} combine pixel-wise annotations with other weak annotations that are faster to acquire, thereby reducing the annotation burden. In particular, Papandreou~\etal~\cite{Papandreou15} showed that combinations of strong and weak annotations (\eg, bounding boxes, keypoints, or image-level tags) delivered competitive results with a reduced annotation effort. In this work, we rely on these observations and focus on the weakly supervised segmentation setting.

In the frame of designing annotation campaigns, weakly-supervised approaches present opportunities for efficiency as well. Instead of completely spending a budget on a few expensive annotations, weakly-supervised methods allow a proportion of the budget to be allocated to inexpensive, or weak, labels. That is, one could spend the entire annotation budget to manually segment available images, but would ultimately lead to relatively few annotations. Conversely, weak annotations such as image-level labels are roughly 100~times cheaper to gather than their segmentation counterparts~\cite{Bearman16}. Thus, a greater number of weakly-annotated images could be used to train segmentation models at an equal cost. In fact, under a fixed budget, allocating a proportion of the budget to inexpensive image-level class labels has been shown to yield superior performance compared to entirely allocating a budget to segmentation labels~\cite{Bearman16}.

Yet, allocating how an annotation budget should be distributed among strong and weak annotations is challenging, and inappropriate allocations may severely impact the quality of the final segmentation model. For example, spending the entire budget on image-level annotations will clearly hurt the performance of a subsequent segmentation model. Instead, a naive solution would be to segment and classify a fixed proportion of each (\eg,~say 80\% - 20\%). Knowing what proportion to use for a given dataset is unclear, however. Beyond this, there is no reason why the same fixed proportion would be appropriate across different datasets or application domains. That is, it would be highly unlikely that the datasets shown in Fig.~\ref{fig:datasets} all require the same proportion of strong and weak annotations to yield optimal segmentation models.

Despite its importance, choosing the best proportion of annotation types remains a largely unexplored research question. Weakly-supervised and transfer-learning methods generally assume that the annotation campaign and the model training are independent and that all annotations are simply available at training time. While active learning methods do alternate between annotation and training, they focus on choosing optimal samples to annotate rather than choosing the right type of annotations. Moreover, most active learning methods ignore constraints imposed by an annotation budget. More notable, however, is the recent work of Mahmood {\it et.~al.}~\cite{mahmood2022, mahmood2022optimizing} which aims to determine what weak and strong annotation strategy is necessary to achieve a target performance level. While noteworthy, this objective differs from that here, whereby given a fixed budget, what strategy is best suited for a given new dataset?

To this end, we propose a novel method to find an optimal budget allocation strategy in an online manner. Using a collection of unlabeled images and a maximum budget, our approach selects strong and weak annotations, constrained by a given budget, that maximize the performance of the subsequent trained segmentation model. To do this, our method iteratively alternates between partial budget allocations, label acquisition, and model training. At each step, we use the annotations performed so far to train multiple models to estimate how different proportions of weak and strong annotations affect model performance. A Gaussian Process models these results and maps the number of weak and strong annotations to the expected model improvement. Computing the Pareto optima between expected improvement and costs, we choose a new sub-budget installment and its associated allocation so to yield the maximum expected improvement. We show in our experiments that our approach is beneficial for a broad range of datasets, and illustrate that our dynamic strategy allows for high performances, close to optimal fixed strategies that cannot be determined beforehand.


%---------------------------------------%

% The goal of our work is to find this data-specific optimal budget allocation in an online manner. We claim that the annotation procedure and the model training should not be independent but are, in fact, closely intertwined. Following this idea, our algorithm iteratively alternates between partial budget allocation and model training. At each step, our method uses the annotations available so far to train multiple models and estimate how different proportions of weak and strong annotations affect model performance. A Gaussian process fitted to these results models a map from the number of weak and strong annotations to the expected model improvement and used to determine how to spend the  next budget tranche in a way that yields the maximum expected improvement. We show in our experiments that datasets annotated with our approach achieve performances very close to those of the optimal fixed strategy, even though this strategy cannot be known in advance. 

% \PMN{To prevent the obvious complaint from bad reviewers, I'd like to clarify early in the text that this method is far from active learning. However, this paragraph is more suitable for the conclusion section\ldots} While this method could be reminiscent of active learning, our work differs from it in fundamental ways: we focus on choosing the best annotation strategies for arbitrary samples and, unlike active learning, do not care or search for the best samples to annotate. In this sense, our work is orthogonal to active learning, and both methods can be easily combined. This is, however, not the goal of this paper and a direction for future research.

% Typically in these methods, a combination of annotation types  can be used at training time. 
% Our work relies on some of the key concepts of training with weak supervision, active learning \PMN{Claiming that our method is inspired by active learning might lead an average reviewer to demand a comparison with active learning methods.} and transfer learning \PMN{Why transfer learning?} in order to determine what kind of annotations should be gathered when building a domain-specific dataset for segmentation.



% Specifically, we focus on the setting where we wish to build a segmentation model for a novel application and where the annotations must be gathered from scratch (see Fig~\ref{fig:datasets}). Naturally, one could manually segment all images available, but given any reasonable annotation budget, this would strongly limit the number of annotations produced. Conversely, weak annotations such as image level classification annotations are roughly 100 times cheaper to gather than their segmentation counterparts~\cite{Bearman16}, and have been shown to be beneficial for segmentation models where the target domain differs from the domain of the pre-trained network~\cite{Mensink}. Compared to segmentation annotations, this implies that much larger quantities of weakly-annotated images could be used to train segmentation models at an equal cost. Less obvious however is that using a mixture of both fully and weak annotations has also been shown to yield superior performances for fixed budgets~\cite{Bearman16}. %In this work, we will explicitly consider the case where segmentation anno, as well as image-level classification annotations, are potentially available \PMN{I do not understand the intention of this sentence. Our assumption is precisely that segmentation and classification labels are not available}. 

% With the option of collecting segmentation and classification annotations for a new dataset, a natural question for the dataset designer is ``Given a fixed annotation budget for this new dataset, what proportion of the budget should be allocated to manual segmentation or classification annotations?'' While choosing to segment and classify a fixed proportion of each is not particularly difficult (\ie, say 80\% - 20\%), knowing what proportion to use for a given dataset is however unclear and there is no reason to believe that a fixed proportion would be appropriate regardless of the domain application. That is, there is no reason why the same proportion of annotation types would be optimal for both the OCT and SUIM datasets shown in Fig.~\ref{fig:datasets}. Hence, the question at the heart of this work is whether a dedicated annotation strategy can be derived for a given new application domain, whereby providing full and weak annotations that yield high-performing segmentation models. 

% To this end, we propose a novel approach to collect segmentation and classification annotations for a novel segmentation task. At the core of our method lies an adaptive algorithm that sequentially looks to estimate how a partial budget should be spent to maximize the segmentation performance of a subsequent training model. To do this, we model the expected utility of a given number of potential segmentation and classification annotations using a Gaussian Process, and use this to estimate what proportion yields the maximum expected improvement in the segmentation model. By doing this step by step, we show in our experiments that our approach produces annotated datasets that allow high-performing segmentation performances for a number of different budgets and domains. Beyond this, we show that by applying our method, we ensure that downstream segmentation methods reach near high-level performances even though the optimal fixed annotation allocation cannot be known in advance. 

%\PMN{We should remove this paragraph.}
%The remainder of the paper is organized as follows: Sec.~\ref{sec:related} introduces a number of relevant related works and the core of our approach is detailed in Sec.~\ref{sec:method}. We then explain our experimental setup in Sec.~\ref{sec:experiments}, as well as the results achieved without approach and the compared strategies in Sec.~\ref{sec:results}. Sec.~\ref{sec:conclusion} provides the conclusion of our work.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
