Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{chen2020deep,
author = {Chen, Chen and Qin, Chen and Qiu, Huaqi and Tarroni, Giacomo and Duan, Jinming and Bai, Wenjia and Rueckert, Daniel},
journal = {Frontiers in Cardiovascular Medicine},
pages = {25},
publisher = {Frontiers},
title = {{Deep learning for cardiac image segmentation: a review}},
year = {2020}
}
@article{mustafa2021supervised,
abstract = {Transfer learning is a standard technique to improve performance on tasks with limited data. However, for medical imaging, the value of transfer learning is less clear. This is likely due to the large domain mismatch between the usual natural-image pre-training (e.g. ImageNet) and medical images. However, recent advances in transfer learning have shown substantial improvements from scale. We investigate whether modern methods can change the fortune of transfer learning for medical imaging. For this, we study the class of large-scale pre-trained networks presented by Kolesnikov et al. on three diverse imaging tasks: chest radiography, mammography, and dermatology. We study both transfer performance and critical properties for the deployment in the medical domain, including: out-of-distribution generalization, data-efficiency, sub-group fairness, and uncertainty estimation. Interestingly, we find that for some of these properties transfer from natural to medical images is indeed extremely effective, but only when performed at sufficient scale.},
archivePrefix = {arXiv},
arxivId = {2101.05913},
author = {Mustafa, Basil and Loh, Aaron and Freyberg, Jan and MacWilliams, Patricia and Wilson, Megan and McKinney, Scott Mayer and Sieniek, Marcin and Winkens, Jim and Liu, Yuan and Bui, Peggy and Prabhakara, Shruthi and Telang, Umesh and Karthikesalingam, Alan and Houlsby, Neil and Natarajan, Vivek},
eprint = {2101.05913},
journal = {arXiv preprint arXiv:2101.05913},
month = {jan},
title = {{Supervised Transfer Learning at Scale for Medical Imaging}},
url = {http://arxiv.org/abs/2101.05913},
year = {2021}
}
@inproceedings{lin2016scribblesup,
author = {Lin, Di and Dai, Jifeng and Jia, Jiaya and He, Kaiming and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {3159--3167},
title = {{Scribblesup: Scribble-supervised convolutional networks for semantic segmentation}},
year = {2016}
}
@inproceedings{Zamir2018,
abstract = {Do visual tasks have a relationship, or are they unre-lated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values ; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for model-ing the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2 3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxo-nomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
archivePrefix = {arXiv},
arxivId = {1804.08328v1},
author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1804.08328v1},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Zamir et al. - 2018 - Taskonomy Disentangling Task Transfer Learning.pdf:pdf},
month = {jun},
pages = {3712--3722},
title = {{Taskonomy: Disentangling Task Transfer Learning}},
url = {http://taskonomy.vision/},
year = {2018}
}
@inproceedings{siam2017deep,
author = {Siam, Mennatullah and Elkerdawy, Sara and Jagersand, Martin and Yogamani, Senthil},
booktitle = {2017 IEEE 20th international conference on intelligent transportation systems (ITSC)},
organization = {IEEE},
pages = {1--8},
title = {{Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges}},
year = {2017}
}
@inproceedings{Adhikari2018,
author = {Adhikari, Bishwo and Peltomaki, Jukka and Puura, Jussi and Huttunen, Heikki},
booktitle = {2018 7th European Workshop on Visual Information Processing (EUVIP)},
doi = {10.1109/EUVIP.2018.8611732},
pages = {1--6},
title = {{Faster Bounding Box Annotation for Object Detection in Indoor Scenes}},
year = {2018}
}
@article{Liang2020,
abstract = {Background and Objective: Computer aided diagnosis systems based on deep learning and medical imaging is increasingly becoming research hotspots. At the moment, the classical convolutional neural network generates classification results by hierarchically abstracting the original image. These abstract features are less sensitive to the position and orientation of the object, and this lack of spatial information limits the further improvement of image classification accuracy. Therefore, how to develop a suitable neural network framework and training strategy in practical clinical applications to avoid this problem is a topic that researchers need to continue to explore. Methods: We propose a deep learning framework that combines residual thought and dilated convolution to diagnose and detect childhood pneumonia. Specifically, based on an understanding of the nature of the child pneumonia image classification task, the proposed method uses the residual structure to overcome the over-fitting and the degradation problems of the depth model, and utilizes dilated convolution to overcome the problem of loss of feature space information caused by the increment in depth of the model. Furthermore, in order to overcome the problem of difficulty in training model due to insufficient data and the negative impact of the introduction of structured noise on the performance of the model, we use the model parameters learned on large-scale datasets in the same field to initialize our model through transfer learning. Results: Our proposed method has been evaluated for extracting texture features associated with pneumonia and for accurately identifying the performance of areas of the image that best indicate pneumonia. The experimental results of the test dataset show that the recall rate of the method on children pneumonia classification task is 96.7{\%}, and the f1-score is 92.7{\%}. Compared with the prior art methods, this approach can effectively solve the problem of low image resolution and partial occlusion of the inflammatory area in children chest X-ray images. Conclusions: The novel framework focuses on the application of advanced classification that directly performs lesion characterization, and has high reliability in the classification task of children pneumonia.},
annote = {- Not pretrained on imagenet, but chestxray},
author = {Liang, Gaobo and Zheng, Lixin},
doi = {10.1016/J.CMPB.2019.06.023},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Liang, Zheng - 2020 - A transfer learning method with deep residual network for pediatric pneumonia diagnosis.pdf:pdf},
issn = {0169-2607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Deep learning,Image classification,Pneumonia,Residual network,Transfer Learning},
month = {apr},
pages = {104964},
pmid = {31262537},
publisher = {Elsevier},
title = {{A transfer learning method with deep residual network for pediatric pneumonia diagnosis}},
volume = {187},
year = {2020}
}
@article{jones1998efficient,
author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
journal = {Journal of Global optimization},
number = {4},
pages = {455--492},
publisher = {Springer},
title = {{Efficient global optimization of expensive black-box functions}},
volume = {13},
year = {1998}
}
@inproceedings{siddiqui2020viewal,
author = {Siddiqui, Yawar and Valentin, Julien and Nie{\ss}ner, Matthias},
booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
pages = {9433--9443},
title = {{Viewal: Active learning with viewpoint entropy for semantic segmentation}},
year = {2020}
}
@inproceedings{ahn2018learning,
author = {Ahn, Jiwoon and Kwak, Suha},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {4981--4990},
title = {{Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation}},
year = {2018}
}
@article{Mockus1978,
author = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
journal = {Towards Global Optimization},
number = {117-129},
pages = {2},
publisher = {Amsterdam: Elsevier},
title = {{The Application of Bayesian Methods for Seeking the Extremum}},
volume = {2},
year = {1978}
}
@inproceedings{tang2018normalized,
author = {Tang, Meng and Djelouah, Abdelaziz and Perazzi, Federico and Boykov, Yuri and Schroers, Christopher},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {1818--1827},
title = {{Normalized cut loss for weakly-supervised cnn segmentation}},
year = {2018}
}
@inproceedings{koleshnikov2021,
author = {Kolesnikov, Alexander and Dosovitskiy, Alexey and Weissenborn, Dirk and Heigold, Georg and Uszkoreit, Jakob and Beyer, Lucas and Minderer, Matthias and Dehghani, Mostafa and Houlsby, Neil and Gelly, Sylvain and Unterthiner, Thomas and Zhai, Xiaohua},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
year = {2021}
}
@article{2019LiTS,
archivePrefix = {arXiv},
arxivId = {cs.CV/1901.04056},
author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"{u}}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`{a}}, Jana and Lowengrub, John and Meine, Hans and {Hendrik Moltz}, Jan and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"{u}}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"{o}}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and {Efrain Humpire Mamani}, Gabriel and van Ginneken, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and Ben-Cohen, Avi and Klang, Eyal and Amitai, Marianne M and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and Lev-Cohain, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H},
eprint = {1901.04056},
journal = {arXiv e-prints},
keywords = {Computer Science - Computer Vision and Pattern Rec},
month = {jan},
pages = {arXiv:1901.04056},
primaryClass = {cs.CV},
title = {{The Liver Tumor Segmentation Benchmark (LiTS)}},
year = {2019}
}
@inproceedings{howard2019searching,
author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Others},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
pages = {1314--1324},
title = {{Searching for mobilenetv3}},
year = {2019}
}
@article{hesamian2019deep,
author = {Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Kennedy, Paul},
journal = {Journal of digital imaging},
number = {4},
pages = {582--596},
publisher = {Springer},
title = {{Deep learning techniques for medical image segmentation: achievements and challenges}},
volume = {32},
year = {2019}
}
@inproceedings{zhou2016learning,
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {2921--2929},
title = {{Learning deep features for discriminative localization}},
year = {2016}
}
@article{Simpson2019,
abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts.},
archivePrefix = {arXiv},
arxivId = {1902.09063v1},
author = {Simpson, Amber L and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and {Van Ginneken}, Bram and Kopp-Schneider, Annette and Landman, Bennett A and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M and Bilic, Patrick and Christ, Patrick F and Do, Richard K G and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H and Jarnagin, William R and Mchugo, Maureen K and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and Cardoso, M Jorge},
eprint = {1902.09063v1},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Simpson et al. - 2019 - Google DeepMind. 10. Imaging Biomarkers and Computer-aided Diagnosis Lab, Radiology and Imag-ing Sciences, Natio.pdf:pdf},
journal = {Memorial Sloan Kettering Cancer Center},
number = {9},
title = {{A large annotated medical image dataset for the development and evaluation of segmentation algorithms}},
volume = {12},
year = {2019}
}

@article{Bodenstedt2018,
  author    = {Sebastian Bodenstedt and
               Max Allan and
               Anthony Agustinos and
               Xiaofei Du and
               Luis C. Garc{\'{\i}}a{-}Peraza{-}Herrera and
               Hannes Kenngott and
               Thomas Kurmann and
               Beat P. M{\"{u}}ller{-}Stich and
               S{\'{e}}bastien Ourselin and
               Daniil Pakhomov and
               Raphael Sznitman and
               Marvin Teichmann and
               Martin Thoma and
               Tom Vercauteren and
               Sandrine Voros and
               Martin Wagner and
               Pamela Wochner and
               Lena Maier{-}Hein and
               Danail Stoyanov and
               Stefanie Speidel},
  title     = {Comparative evaluation of instrument segmentation and tracking methods
               in minimally invasive surgery},
  journal   = {ArXiv},
  volume    = {abs/1805.02475},
  year      = {2018},
  eprinttype = {arXiv},
  eprint    = {1805.02475},

}

@article{chen2017rethinking,
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
journal = {arXiv preprint arXiv:1706.05587},
title = {{Rethinking atrous convolution for semantic image segmentation}},
year = {2017}
}
@inproceedings{Ranganathan2017,
author = {Ranganathan, Hiranmayi and Venkateswara, Hemanth and Chakraborty, Shayok and Panchanathan, Sethuraman},
booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2017.8297020},
pages = {3934--3938},
title = {{Deep active learning for image classification}},
year = {2017}
}
@article{Benenson2019LargeScaleIO,
author = {Benenson, Rodrigo and Popov, Stefan and Ferrari, Vittorio},
journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {11692--11701},
title = {{Large-Scale Interactive Object Segmentation With Human Annotators}},
year = {2019}
}
@inproceedings{he2019,
annote = {- Imagenet pretraining doesn't help, it's the same as random init but it takes less time},
author = {He, Kaiming and Girshick, Ross and Doll{\'{a}}r, Piotr},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
pages = {4918--4927},
title = {{Rethinking imagenet pre-training}},
year = {2019}
}
@inproceedings{song2019box,
author = {Song, Chunfeng and Huang, Yan and Ouyang, Wanli and Wang, Liang},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages = {3136--3145},
title = {{Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation}},
year = {2019}
}
@article{kushner1964,
abstract = {A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.},
author = {Kushner, H J},
doi = {10.1115/1.3653121},
issn = {0021-9223},
journal = {Journal of Basic Engineering},
number = {1},
pages = {97--106},
title = {{A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise}},
url = {https://doi.org/10.1115/1.3653121},
volume = {86},
year = {1964}
}
@article{Esteva2017,
abstract = {An artificial intelligence trained to classify images of skin lesions as benign lesions or malignant skin cancers achieves the accuracy of board-certified dermatologists.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
doi = {10.1038/nature21056},
issn = {1476-4687},
journal = {Nature},
number = {7639},
pages = {115--118},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {https://doi.org/10.1038/nature21056},
volume = {542},
year = {2017}
}
@inproceedings{cordts2016cityscapes,
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {3213--3223},
title = {{The cityscapes dataset for semantic urban scene understanding}},
year = {2016}
}
@article{Zhang2021,
abstract = {Weakly supervised semantic segmentation is receiving great attention due to its low human annotation cost. In this paper, we aim to tackle bounding box supervised semantic segmentation, i.e., training accurate semantic segmentation models using bounding box annotations as supervision. To this end, we propose Affinity Attention Graph Neural Network (A 2 GNN). Following previous practices, we first generate pseudo semantic-aware seeds, which are then formed into semantic graphs based on our newly proposed affinity Convolutional Neural Network (CNN). Then the built graphs are input to our A 2 GNN, in which an affinity attention layer is designed to acquire the short-and long-distance information from soft graph edges to accurately propagate semantic labels from the confident seeds to the unlabeled pixels. However, to guarantee the precision of the seeds, we only adopt a limited number of confident pixel seed labels for A 2 GNN, which may lead to insufficient supervision for training. To alleviate this issue, we further introduce a new loss function and a consistency-checking mechanism to leverage the bounding box constraint, so that more reliable guidance can be included for the model optimization. Experiments show that our approach achieves new state-of-the-art performances on Pascal VOC 2012 datasets (val: 76.5{\%}, test: 75.2{\%}). More importantly, our approach can be readily applied to bounding box supervised instance segmentation task or other weakly supervised semantic segmentation tasks, with state-of-the-art or comparable performance among almot all weakly supervised tasks on PASCAL VOC or COCO dataset. Our source code will be available at https://github.com/zbf1991/A2GNN.},
archivePrefix = {arXiv},
arxivId = {2106.04054v1},
author = {Zhang, Bingfeng and Xiao, Jimin and Jiao, Jianbo and Wei, Yunchao and Zhao, Yao},
doi = {10.1109/TPAMI.2021.3083269},
eprint = {2106.04054v1},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Index Terms-Weakly supervised,graph neural network,semantic segmentation},
pages = {1--1},
title = {{Affinity Attention Graph Neural Network for Weakly Supervised Semantic Segmentation}},
url = {https://github.com/zbf1991/A2GNN. https://ieeexplore.ieee.org/document/9440699/},
year = {2021}
}
@inproceedings{papadopoulos2014,
abstract = {Training an object class detector typically requires a large set of images annotated with bounding-boxes, which is expensive and time consuming to create. We propose novel approach to annotate object locations which can substantially reduce annotation time. We first track the eye movements of annotators instructed to find the object and then propose a technique for deriving object bounding-boxes from these fixations. To validate our idea, we collected eye tracking data for the trainval part of 10 object classes of Pascal VOC 2012 (6,270 images, 5 observers). Our technique correctly produces bounding-boxes in 50{\%}of the images, while reducing the total annotation time by factor 6.8{\{}$\backslash$texttimes{\}} compared to drawing bounding-boxes. Any standard object class detector can be trained on the bounding-boxes predicted by our model. Our large scale eye tracking dataset is available at groups.inf.ed.ac.uk/calvin/eyetrackdataset/.},
address = {Cham},
author = {Papadopoulos, Dim P and Clarke, Alasdair D F and Keller, Frank and Ferrari, Vittorio},
booktitle = {Computer Vision -- ECCV 2014},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10602-1},
pages = {361--376},
publisher = {Springer International Publishing},
title = {{Training Object Class Detectors from Eye Tracking Data}},
year = {2014}
}
@book{Rasmussen2006,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Rasmussen, Williams - Unknown - Gaussian Processes for Machine Learning.pdf:pdf},
isbn = {026218253X},
pages = {266},
publisher = {The MIT Press},
title = {{Gaussian Processes for Machine Learning}},
url = {www.GaussianProcess.org/gpml},
year = {2006}
}
@inproceedings{pmlr-v102-geyer19a,
abstract = {Transfer learning has been an important ingredient of state-of-the-art deep learning models. In particular, it has significant impact when little data is available for the target task, such as in many medical imaging applications. Typically, transfer learning means pre-training the target model on a related task which has sufficient data available. However, often pre-trained models from several related tasks are available, and it would be desirable to transfer their combined knowledge by automatic weighting and merging. For this reason, we propose T-IMM (Transfer Incremental Mode Matching), a method to leverage several pre-trained models, which extends the concept of Incremental Mode Matching from lifelong learning to the transfer learning setting. Our method introduces layer wise mixing ratios, which are learned automatically and fuse multiple pre-trained models before fine-tuning on the new task. We demonstrate the efficacy of our method by the example of brain tumor segmentation in MRI (BRATS 2018 Challange). We show that fusing weights according to our framework, merging two models trained on general brain parcellation can greatly enhance the final model performance for small training sets when compared to standard transfer methods or state-of the art initialization. We further demonstrate that the benefit remains even when training on the entire Brats 2018 data set (255 patients).},
annote = {- Transfer learning with a different thing, not imagenet},
author = {Geyer, Robin and Corinzia, Luca and Wegmayr, Viktor},
booktitle = {Proceedings of The 2nd International Conference on Medical Imaging with Deep Learning},
editor = {Cardoso, M Jorge and Feragen, Aasa and Glocker, Ben and Konukoglu, Ender and Oguz, Ipek and Unal, Gozde and Vercauteren, Tom},
pages = {185--196},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Transfer Learning by Adaptive Merging of Multiple Models}},
url = {https://proceedings.mlr.press/v102/geyer19a.html},
volume = {102},
year = {2019}
}
@incollection{mccloskey1989catastrophic,
author = {McCloskey, Michael and Cohen, Neal J},
booktitle = {Psychology of learning and motivation},
pages = {109--165},
publisher = {Elsevier},
title = {{Catastrophic interference in connectionist networks: The sequential learning problem}},
volume = {24},
year = {1989}
}
@article{chen2018priors,
author = {Chen, Zexun and Wang, Bo},
journal = {Neurocomputing},
pages = {1702--1710},
publisher = {Elsevier},
title = {{How priors of initial hyperparameters affect Gaussian process regression models}},
volume = {275},
year = {2018}
}
@inproceedings{deng2009imagenet,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
booktitle = {2009 IEEE conference on computer vision and pattern recognition},
organization = {Ieee},
pages = {248--255},
title = {{Imagenet: A large-scale hierarchical image database}},
year = {2009}
}
@incollection{Raghub,
abstract = {Transfer learning from natural image datasets, particularly IMAGENET, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to IMAGENET architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings.},
annote = {- Transfer learning offers limited performance gains in small models},
author = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Raghu et al. - Unknown - Transfusion Understanding Transfer Learning for Medical Imaging.pdf:pdf},
publisher = {Curran Associates Inc.},
title = {{Transfusion: Understanding Transfer Learning for Medical Imaging}},
url = {https://papers.nips.cc/paper/2019/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html},
year = {2019}
}
@inproceedings{kolesnikov2020big,
author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
booktitle = {European conference on computer vision},
organization = {Springer},
pages = {491--507},
title = {{Big transfer (bit): General visual representation learning}},
year = {2020}
}
@techreport{WelinderEtal2010,
author = {Welinder, P and Branson, S and Mita, T and Wah, C and Schroff, F and Belongie, S and Perona, P},
institution = {California Institute of Technology},
number = {CNS-TR-2010-001},
title = {{Caltech-UCSD Birds 200}},
year = {2010}
}
@inproceedings{sun2017,
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages = {843--852},
title = {{Revisiting unreasonable effectiveness of data in deep learning era}},
year = {2017}
}
@article{Zhai2021,
abstract = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45{\%} top-1 accuracy. The model also performs well on few-shot learning, for example, attaining 84.86{\%} top-1 accuracy on ImageNet with only 10 examples per class.},
archivePrefix = {arXiv},
arxivId = {2106.04560},
author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
eprint = {2106.04560},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Zhai et al. - 2021 - Scaling Vision Transformers.pdf:pdf},
journal = {arXiv preprint arXiv:2106.04560},
month = {jun},
title = {{Scaling Vision Transformers}},
url = {https://arxiv.org/abs/2106.04560v1},
year = {2021}
}
@misc{pascal-voc-2012,
author = {Everingham, M and Van{\~{}}Gool, L and Williams, C K I and Winn, J and Zisserman, A},
howpublished = {http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html},
title = {{The PASCAL Visual Object Classes Challenge 2012 VOC2012 Results}}
}
@article{heker2020joint,
author = {Heker, Michal and Greenspan, Hayit},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Heker, Greenspan - Unknown - Medical Imaging with Deep Learning 2020 1-5 MIDL 2020-Short Paper Joint Liver Lesion Segmentation and Class.pdf:pdf},
journal = {arXiv preprint arXiv:2004.12352},
title = {{Joint liver lesion segmentation and classification via transfer learning}},
year = {2020}
}
@inproceedings{islam2020semantic,
author = {Islam, Md Jahidul and Edge, Chelsey and Xiao, Yuyang and Luo, Peigen and Mehtaz, Muntaqim and Morse, Christopher and Enan, Sadman Sakib and Sattar, Junaed},
booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
organization = {IEEE},
pages = {1769--1776},
title = {{Semantic segmentation of underwater imagery: Dataset and benchmark}},
year = {2020}
}
@inproceedings{Konyushkova_2018_CVPR,
author = {Konyushkova, Ksenia and Uijlings, Jasper and Lampert, Christoph H and Ferrari, Vittorio},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {jun},
title = {{Learning Intelligent Dialogs for Bounding Box Annotation}},
year = {2018}
}
@inproceedings{lee2019ficklenet,
author = {Lee, Jungbeom and Kim, Eunji and Lee, Sungmin and Lee, Jangho and Yoon, Sungroh},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages = {5267--5276},
title = {{Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference}},
year = {2019}
}
@article{Ding2019,
abstract = {Purpose To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine 18 (18F) fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers. Materials and Methods Prospective 18F-FDG PET brain images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on 90{\%} of ADNI data set and tested on the remaining 10{\%}, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and t-distributed stochastic neighbor embedding. Results The algorithm achieved area under the ROC curve of 0.98 (95{\%} confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of AD in the independent test set (82{\%} specificity at 100{\%} sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance (57{\%} [four of seven] sensitivity, 91{\%} [30 of 33] specificity; P {\textless} .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain. Conclusion By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82{\%} specificity at 100{\%} sensitivity, an average of 75.8 months prior to the final diagnosis. {\textcopyright} RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Larvie in this issue.},
annote = {PMID: 30398430},
author = {Ding, Yiming and Sohn, Jae Ho and Kawczynski, Michael G and Trivedi, Hari and Harnish, Roy and Jenkins, Nathaniel W and Lituiev, Dmytro and Copeland, Timothy P and Aboian, Mariam S and {Mari Aparici}, Carina and Behr, Spencer C and Flavell, Robert R and Huang, Shih-Ying and Zalocusky, Kelly A and Nardo, Lorenzo and Seo, Youngho and Hawkins, Randall A and {Hernandez Pampaloni}, Miguel and Hadley, Dexter and Franc, Benjamin L},
doi = {10.1148/radiol.2018180958},
journal = {Radiology},
number = {2},
pages = {456--464},
title = {{A Deep Learning Model to Predict a Diagnosis of Alzheimer Disease by Using 18F-FDG PET of the Brain}},
url = {https://doi.org/10.1148/radiol.2018180958},
volume = {290},
year = {2019}
}
@inproceedings{li2020mas3k,
author = {Li, Lin and Rigall, Eric and Dong, Junyu and Chen, Geng},
booktitle = {International Symposium on Benchmarking, Measuring and Optimization},
organization = {Springer},
pages = {194--212},
title = {{MAS3K: An Open Dataset for Marine Animal Segmentation}},
year = {2020}
}
@article{Ki2021,
abstract = {Weakly supervised learning attempts to construct predictive models by learning with weak supervision. In this paper, we concentrate on weakly supervised object localization and semantic segmentation tasks. Existing methods are limited to focusing on narrow discriminative parts or overextending the activations to less discriminative regions even on backgrounds. To mitigate these problems, we regard the background as an important cue that guides the feature activation to cover the entire object to the right extent, and propose two novel objective functions: 1) contrastive attention loss and 2) foreground consistency loss. Contrastive attention loss draws the foreground feature and its dropped version close together and pushes the dropped foreground feature away from the background feature. Foreground consistency loss favors agreement between layers and provides early layers with a sense of objectness. Using both losses leads to balanced improvements over localization and segmentation accuracy by boosting activations on less discriminative regions but restraining the activation in the target object extent. For better optimizing the above losses, we use the non-local attention blocks to replace channel-pooled attention leading to enhanced attention maps considering the spatial similarity. Finally, our method achieves state-of-the-art localization performance on CUB-200-2011, ImageNet, and OpenImages benchmarks regarding top-1 localization accuracy, MaxBoxAccV2, and PxAP. We also demonstrate the effectiveness of our method in improving segmentation performance measured by mIoU on the PASCAL VOC dataset.},
annote = {They get predictions in 41x41, then upsample and refine with conditional random fields.},
author = {Ki, Minsong and Uh, Youngjung and Lee, Wonyoung and Byun, Hyeran},
doi = {10.1016/j.neucom.2021.03.023},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Ki et al. - 2021 - Contrastive and consistent feature learning for weakly supervised object localization and semantic segmentation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Contrastive learning,Foreground consistency,Localization,Segmentation,Weakly supervised learning},
month = {jul},
pages = {244--254},
publisher = {Elsevier B.V.},
title = {{Contrastive and consistent feature learning for weakly supervised object localization and semantic segmentation}},
url = {https://doi.org/10.1016/j.neucom.2021.03.023},
volume = {445},
year = {2021}
}
@inproceedings{sless2019road,
author = {Sless, Liat and {El Shlomo}, Bat and Cohen, Gilad and Oron, Shaul},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
pages = {0},
title = {{Road scene understanding by occupancy grid learning from sparse radar clusters using semantic segmentation}},
year = {2019}
}
@article{Mensink,
abstract = {Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e. pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 1200 transfer experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations for practitioners.},
annote = {In general, accross datasets, segmentation helps object detection but not vice versa.
-﻿Transfer learning effects are larger for small target training sets. 
-﻿The source domain including the target is more important than the number of source samples.},
archivePrefix = {arXiv},
arxivId = {2103.13318v1},
author = {Mensink, Thomas and Uijlings, Jasper and Kuznetsova, Alina and Gygli, Michael and Ferrari, Vittorio},
eprint = {2103.13318v1},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Mensink et al. - Unknown - Factors of Influence for Transfer Learning across Diverse Appearance Domains and Task Types.pdf:pdf},
journal = {arXiv preprint arXiv:2103.13318},
keywords = {Computer Vision,Index Terms-Transfer Learning},
title = {{Factors of Influence for Transfer Learning across Diverse Appearance Domains and Task Types}},
year = {2021}
}
@inproceedings{menegola2017,
annote = {Imagenet pretraining},
author = {Menegola, Afonso and Fornaciali, Michel and Pires, Ramon and Bittencourt, Fl{\'{a}}via Vasques and Avila, Sandra and Valle, Eduardo},
booktitle = {2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)},
organization = {IEEE},
pages = {297--300},
title = {{Knowledge transfer for melanoma screening with deep learning}},
year = {2017}
}
@article{swersky2013multi,
author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
journal = {Advances in neural information processing systems},
title = {{Multi-task bayesian optimization}},
volume = {26},
year = {2013}
}
@inproceedings{andriluka2018fluid,
author = {Andriluka, Mykhaylo and Uijlings, Jasper R R and Ferrari, Vittorio},
booktitle = {Proceedings of the 26th ACM international conference on Multimedia},
pages = {1957--1966},
title = {{Fluid annotation: a human-machine collaboration interface for full image annotation}},
year = {2018}
}
@inproceedings{wang2017chestxray,
annote = {ChestX-ray14 dataset},
author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)},
pages = {3462--3471},
title = {{ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases}},
year = {2017}
}
@article{tseng2021person,
author = {Tseng, Chien-Hao and Hsieh, Chia-Chien and Jwo, Dah-Jing and Wu, Jyh-Horng and Sheu, Ruey-Kai and Chen, Lun-Chi},
journal = {Journal of Sensors},
publisher = {Hindawi},
title = {{Person Retrieval in Video Surveillance Using Deep Learning--Based Instance Segmentation}},
volume = {2021},
year = {2021}
}
@inproceedings{jiang2019integral,
author = {Jiang, Peng-Tao and Hou, Qibin and Cao, Yang and Cheng, Ming-Ming and Wei, Yunchao and Xiong, Hong-Kai},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
pages = {2070--2079},
title = {{Integral object mining via online attention accumulation}},
year = {2019}
}
@article{Bergstra2012,
author = {Bergstra, James and Bengio, Yoshua},
journal = {Journal of machine learning research},
number = {2},
title = {{Random search for hyper-parameter optimization.}},
volume = {13},
year = {2012}
}
@inproceedings{papadopoulos2017extreme,
author = {Papadopoulos, Dim P and Uijlings, Jasper R R and Keller, Frank and Ferrari, Vittorio},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages = {4930--4939},
title = {{Extreme clicking for efficient object annotation}},
year = {2017}
}
@inproceedings{domhan2015speeding,
author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
booktitle = {Twenty-fourth international joint conference on artificial intelligence},
title = {{Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves}},
year = {2015}
}
@inproceedings{ahn2019weakly,
author = {Ahn, Jiwoon and Cho, Sunghyun and Kwak, Suha},
booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
pages = {2209--2218},
title = {{Weakly supervised learning of instance segmentation with inter-pixel relations}},
year = {2019}
}
@inproceedings{ronneberger2015u,
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {International Conference on Medical image computing and computer-assisted intervention},
organization = {Springer},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
year = {2015}
}
@article{liu2020fsd,
author = {Liu, Shenlan and Liu, Xiang and Huang, Gao and Feng, Lin and Hu, Lianyu and Jiang, Dong and Zhang, Aibin and Liu, Yang and Qiao, Hong},
journal = {arXiv preprint arXiv:2002.03312},
title = {{FSD-10: a dataset for competitive sports content analysis}},
year = {2020}
}
@article{Tajbakhsh2016,
abstract = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
archivePrefix = {arXiv},
arxivId = {1706.00712},
author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
doi = {10.1109/TMI.2016.2535302},
eprint = {1706.00712},
file = {:Users/javier/Library/Application Support/Mendeley Desktop/Downloaded/Tajbakhsh et al. - 2016 - Convolutional Neural Networks for Medical Image Analysis Full Training or Fine Tuning.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Carotid intima-media thickness,computer-aided detection,convolutional neural networks,deep learning,fine-tuning,medical image analysis,polyp detection,pulmonary embolism detection,video quality assessment},
month = {may},
number = {5},
pages = {1299--1312},
pmid = {26978662},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?}},
volume = {35},
year = {2016}
}
@inproceedings{Zhang2016,
abstract = {Focus of this paper is on the prediction accuracy of multidimensional functions at an inaccessible point. The paper explores the possibility of extrapolating a high-dimensional function using multiple one-dimensional converging lines. The main idea is to select samples along lines towards the inaccessible point. Multi-dimensional extrapolation is thus transformed into a series of one-dimensional extrapolations that provide multiple estimates at the inaccessible point. We demonstrate the performance of converging lines using Kriging to extrapolate a two-dimensional drag coefficient function. Post-processing of extrapolation results from different lines based on Bayesian theory is proposed to combine the multiple predictions. Selection of lines is also discussed. The method of converging lines proves to be more robust and reliable than two-dimensional Kriging surrogate for the example.},
author = {Zhang, Yiming and Kim, Nam Ho and Park, Chanyoung and Haftka, Raphael T.},
booktitle = {Volume 2B: 41st Design Automation Conference},
doi = {10.1115/DETC2015-47689},
isbn = {978-0-7918-5708-3},
month = {aug},
publisher = {American Society of Mechanical Engineers},
title = {{Function Extrapolation at One Inaccessible Point Using Converging Lines}},
url = {https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2015/57083/Boston, Massachusetts, USA/251314},
year = {2015}
}
@inproceedings{fan2020learning,
author = {Fan, Junsong and Zhang, Zhaoxiang and Song, Chunfeng and Tan, Tieniu},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages = {4283--4292},
title = {{Learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation}},
year = {2020}
}
@article{snoek2012practical,
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
journal = {Advances in neural information processing systems},
title = {{Practical bayesian optimization of machine learning algorithms}},
volume = {25},
year = {2012}
}
@inproceedings{Joshi2009,
author = {Joshi, Ajay J and Porikli, Fatih and Papanikolopoulos, Nikolaos},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206627},
pages = {2372--2379},
title = {{Multi-class active learning for image classification}},
year = {2009}
}

@InProceedings{Bearman16,
author="Bearman, Amy
and Russakovsky, Olga
and Ferrari, Vittorio
and Fei-Fei, Li",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="What's the Point: Semantic Segmentation with Point Supervision",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="549--565",
abstract="The semantic image segmentation task presents a trade-off between test time accuracy and training time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain; image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of {\$}{\$}12.9{\backslash},{\backslash}{\%}{\$}{\$}mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.",
isbn="978-3-319-46478-7"
}
@InProceedings{Papandreou15,
author = {Papandreou, George and Chen, Liang-Chieh and Murphy, Kevin P. and Yuille, Alan L.},
title = {Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@inproceedings{Casanova2020Reinforced,
title={Reinforced active learning for image segmentation},
author={Arantxa Casanova and Pedro O. Pinheiro and Negar Rostamzadeh and Christopher J. Pal},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgC6TNFvr}
}

@INPROCEEDINGS{Konyushkova15,
author={Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},
booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
title={Introducing Geometry in Active Learning for Image Segmentation}, 
year={2015},
pages={2974-2982},
}

@INPROCEEDINGS{Cai21,  
author={Cai, Lile and Xu, Xun and Liew, Jun Hao and Sheng Foo, Chuan},  
booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Revisiting Superpixels for Active Learning in Semantic Segmentation with Realistic Annotation Costs},   
year={2021},  
pages={10983-10992},  
}

@article{mahmood2022optimizing,
   abstract = {Modern deep learning systems require huge data sets to achieve impressive performance, but there is little guidance on how much or what kind of data to collect. Over-collecting data incurs unnecessary present costs, while under-collecting may incur future costs and delay workflows. We propose a new paradigm for modeling the data collection workflow as a formal optimal data collection problem that allows designers to specify performance targets, collection costs, a time horizon, and penalties for failing to meet the targets. Additionally, this formulation generalizes to tasks requiring multiple data sources, such as labeled and unlabeled data used in semi-supervised learning. To solve our problem, we develop Learn-Optimize-Collect (LOC), which minimizes expected future collection costs. Finally, we numerically compare our framework to the conventional baseline of estimating data requirements by extrapolating from neural scaling laws. We significantly reduce the risks of failing to meet desired performance targets on several classification, segmentation, and detection tasks, while maintaining low total collection costs.},
   author = {Rafid Mahmood and James Lucas and Jose M. Alvarez and Sanja Fidler and Marc T. Law},
   journal = {Advances in Neural Information Processing Systems (NeurIPS)},
   month = {10},
   title = {Optimizing Data Collection for Machine Learning},
   url = {http://arxiv.org/abs/2210.01234},
   year = {2022},
}

@article{mahmood2022,
   abstract = {Given a small training data set and a learning algorithm , how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.},
   author = {Rafid Mahmood and James Lucas and David Acuna and Daiqing Li and Jonah Philion and Jose M Alvarez and Zhiding Yu and Sanja Fidler and Marc T Law},
   journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
   pages = {275-284},
   title = {How Much More Data Do I Need? Estimating Requirements for Downstream Tasks},
   url = {https://openaccess.thecvf.com/content/CVPR2022/html/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.html},
   year = {2022},
}
