\section{Experimental setup}
\label{sec:experiments}

To validate our approach, we evaluated it on four different datasets, while comparing its performance to a set of typical fixed budget allocation strategies. In addition, we explore the impact of different hyper parameters on the overall performance of the method. 
\begin{figure*}[h]
\centering
\includegraphics[width=0.96\textwidth]{main_result_notshared.pdf}
\caption{
Performance of our method (orange line) on OCT, PASCAL VOC, SUIM and Cityscapes datasets. Shaded region is computed from three seeds. Fixed strategies are shown in blue. Red points show the {\it estimated-best-fixed} strategy with $B_0$.  Labels expressed as percentage of the budget allocated to segmentation. Note that the first budget $B$ fulfills $B\gg B_0$ in all cases. 
}
\label{fig:main_results}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=0.96\textwidth]{alphas_notshared.pdf}
\caption{Mean of our method with $\alpha_s = \{5, 12, 25, 50\}$ on Cityscapes (orange, line). Shaded region is computed from three seeds. Fixed strategies are shown in blue. Labels expressed as percentage of the budget allocated to segmentation.
}
\label{fig:alpha_results}
\end{figure*}

\subsection{Datasets} 
We chose a collection of datasets with different image modalities, including a medical dataset as they often suffer from data and annotation scarcity. In this context, they represent a typical new application domain where our method could be particularly helpful. In each case, we enumerate the number of images for which classification or segmentation images can be sampled by a method:
\begin{description}
    %\item[Spleen~\cite{Simpson2019}:] 3'650~chest CT scans with pixel-wise annotations of the spleen. The spleen is visible in 1'051~scans. 250~scans that included the spleen were randomly selected for the test split, and the remaining 3'400~scans were used for training, whereby 801~scans contained the spleen. Class-labels for the 3'400~scans were automatically extracted from the pixel-wise annotations and used in the classification task. The 801~scans with a visible spleen were used for the segmentation task.
    \item[Augmented PASCAL VOC 2012~\cite{pascal-voc-2012}:] 5'717~classification and 10'582~segmentation natural images with 21~classes for training. The validation sets contain 1'449 segmented images.
    \item[SUIM~\cite{islam2020semantic}:] training set consists of 1'525 underwater images with annotations for 8 classes. For evaluation, we used a separate split of 110 additional images. The classification labels were estimated from the segmentation ground-truth as a multi-label problem by setting the class label to~1 if the segmentation map contained at least one pixel assigned to that class.
    \item[Cityscapes~\cite{cordts2016cityscapes}:] 2'975 annotated images for both classification and segmentation are available for training. We test on the official Cityscapes validation set, which contains 500 images. 
    \item[OCT:] 22'723 Optical Coherence Tomography~(OCT) images with classification annotations and 1,002~images with pixel-wise annotations corresponding to 4 different types of retinal fluid for segmentation. We split the data into 902~training images and 100~test images.
\end{description}

\subsection{Baseline strategies.} We compared our method to ten different {\it fixed} budget allocation strategies. Each of these randomly sample images for classification and segmentation annotations according to a specified and fixed proportion. We denote these policies by the percentage dedicated to segmentation annotations: $B_0$: $50\%, 55\%, \ldots, 95\%$ with increases in 5\%. For fair comparison, the strategies are computed from the budget $B_0$.

In addition, we consider an {\it estimated-best-fixed} budget allocation strategy, whereby the method estimates what fixed budget should be used for a given dataset. This is done by using the initial budget $B_0$ to compute the best performing fixed strategy (mentioned above) and then using this fixed strategy for the annotation campaign until budget $B$ is reached. This strategy represents an individual that chooses to explore all fixed strategies for an initial small budget and then exploit it. 

\subsection{Implementation details.} 
{\bf Weakly supervised segmentation model:} To train a segmentation model that uses both segmentation and classifications, we first train the models with the weakly-annotated data~$\T_c$ until convergence and then with the segmentation data~$\T_s$. We use the U-Net segmentation model~\cite{ronneberger2015u} for  OCT, and the DeepLabv3 model~\cite{chen2017rethinking} with a ResNet50 backbone on the SUIM, PASCAL, and Cityscapes. For the U-Net, a classification head is appended at the end of the encoding module for the classification task. For the DeepLab-like models, we train the entire backbone on the classification task and then add the ASPP head for segmentation. In all cases, we use the cross-entropy loss for classification and the average of the Dice loss and the cross-Entropy loss for segmentation. While we choose this training strategy for its simplicity, other cross-task or weakly supervised alternatives could have been used as well~\cite{ahn2018learning,Papandreou15}. Additional details are provided in the supplementary materials.

Note that all models are randomly initialized to maximize the impact of classification labels, as Imagenet-pretraining shares a high resemblance to images in PASCAL and Cityscapes. Failing to do so would lead to classification training not adding significant information and may even hurt performance due to catastrophic forgetting~\cite{mccloskey1989catastrophic}.

%% Our method requires multiple evaluations of the scoring function~$m$ to produce the samples, $\mathcal{M},$ to train the~GP. Evaluating~$m(\T_c, \T_s)$ involves training a model with the approach described above and measuring its performance on the test set. This process can be very time-consuming and is repeated multiple times (depending on the number of new samples per iteration~$M$) with training datasets of increasing size. To alleviate this computational burden, we used the fast surrogate model MobilenetV3~\cite{howard2019searching} instead of U-Net/DeepLabv3 for the scoring function~$m$, enabling faster estimation of strategies. We empirically found that the performance of the surrogate model correlates well with that of the full models, which supports this choice (see details in the supplementary material). Additionally, we progressively decrease~$M$ from 10 to 1 at each iteration of our method as the size of the training datasets increases in order to keep the computational time within reasonable limits. In all cases, the reported results were measured for a full model trained with the best final strategy~$(C, S)$ found by our method.

%\jgt{In order to keep computational time within reasonable limits, we approximate the simulation workflow by interpolating from a ground truth surface. With this procedure, we avoid re-training a model with different subsamples and evaluating its score. At each iteration step, after solving for the best combination of data, we use the interpolation to retrieve the evaluation score as a function of the datasets share.} \JGT{Do we need to explain how we interpolate?}

{\bf Hyperparameters: }
%Table~\ref{tab:configurations} summarizes the hyperparameters of our method used for all datasets. 
We measured costs in terms of class-label equivalents setting~$\alpha_c=1$ and leaving only $\alpha_s$ as a hyperparameter of our method. We set~$\alpha_s=12$ for all datasets following previous studies on crowdsourced annotations~\cite{Bearman16}. We predict the first GP surface with 8\% of the dataset for both classification and segmentation. This quantity is reduced for OCT classification and VOC segmentation due to the high number of labels available. In all cases, we fixed the number of iterative steps to 8 and set the learning rate of the GP to 0.1.
%\begin{table}[h]
%\centering
%\begin{tabular}{lcccccc}
%\toprule
%\textbf{Dataset} & $C_0 (\%)$ & $S_0 (\%)$ & \textbf{Optimizer} & \textbf{Batch size} \\ \midrule
%\textbf{OCT} & 4 & 8 &  Adam & 8 \\
%\textbf{VOC} & 8 & 6 &  SGD & 16 \\
%\textbf{SUIM} & 8 & 8 &  SGD & 8 \\ 
%\textbf{Cityscapes} & 8 & 8 & SGD & 16  \\ \bottomrule
%\end{tabular}
%\caption{Hyperparameters and conditions for all experiments.}
%\label{tab:configurations}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%


