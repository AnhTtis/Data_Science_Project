\section{Related work}
\label{sec:related}

\subsection{Weak annotations for segmentation}
Weakly supervised semantic segmentation (WSSS) relies on coarser annotations, such as bounding boxes~\cite{song2019box}, scribbles~\cite{lin2016scribblesup,tang2018normalized} or image-level classification labels~\cite{ahn2019weakly}, to train a segmentation network. WSSS methods have often employed saliency maps as weak annotations for segmentation models, as these are typically obtained from CAM~\cite{zhou2016learning}, which leverages image-level classification annotation. These methods then focus on refining the saliency maps with a variety of techniques~\cite{fan2020learning,lee2019ficklenet}. Others make use of attention to achieve coarse segmentations~\cite{jiang2019integral,Ki2021}. Conversely, \cite{Zhang2021} combined annotations in the form of bounding boxes and image-level labels to accurately generate image graphs, to be used by a graph neural network to predict node values corresponding to pixel labels. In this context, the work in~\cite{mahmood2022} and~\cite{mahmood2022optimizing} are close to this one, whereby their objective is to determine what annotation strategy over annotation types is likely to yield a target performance level. 

\subsection{Transfer learning}
Due to the limited availability of annotated image data in some domains, it is now common to use neural networks pre-trained on large image classification tasks~\cite{deng2009imagenet} for subsequent target tasks. Specifically, in cases where the target task has limited data or annotations, this has been shown to be particularly advantageous. Among others, this practice is now widely used in medical imaging and has been linked to important performance gains after fine-tuning~\cite{Ding2019,Esteva2017,menegola2017,Tajbakhsh2016,kolesnikov2020big}.

%The effect of cross-domain transfer learning has not been studied in depth, and several studies challenge the aforementioned belief. In~\cite{he2019}, it is shown that \MG{I observe the use of ``pre-train'' and ``pretrain''. We need to decide. I would say pre-train.} pre-training only affects in shorter convergence times compared to random initialization, but it does not necessarily convert into better performance. On the same line,~\cite{Raghub} shows that transferring offers limited performance benefit on two large scale medical imaging datasets, and that smaller architectures can perform comparably to large (Imagenet pre-trained) models. On the other hand,~\cite{mustafa2021supervised} compares transfer performance of large-scale pre-trained networks on three medical imaging datasets, and shows that cross-domain transfer learning is effective when performed at scale.

Efforts are now pivoting towards the use of in-domain pre-training, avoiding the leap of faith that is often taken with Imagenet~\cite{heker2020joint,Liang2020}. In~\cite{Liang2020}, the model is pre-trained on ChestX-ray14~\cite{wang2017chestxray} to more accurately detect pneumonia in chest X-ray images from children. In~\cite{heker2020joint}, the authors show that joint classification and segmentation training, along with pre-training on other medical datasets that have domain similarity, increases segmentation performances with respect to the segmentation using Imagenet-based pre-training.

Alternatively, cross-task methods seek to transfer features learned on one task (\eg~ classification, normal estimation, etc.) to another, usually more complex one. Along this line, Taskonomy~\cite{Zamir2018} explored transfer learning capabilities among a number of semantic tasks and built a task similarity tree that provided a clustered view of how much information is available when transferring to other tasks. Similarly,~\cite{Mensink} performed an extensive study of cross-task transfer capabilities for a variety of datasets, reaching the conclusion that Imagenet pre-training outperforms random initialization in all cases, but further training on related tasks or domains also brings additional benefits.

%Transfer learning
%- Places where imagenet pretraining has been used
%- Where imagenet pretraining doesn't seem to help much
%- Where in domain pretraining helps
%- Something about "Supervised Transfer Learning at Scale for Medical Imaging"
%- Taskonomy, as an alternative with cross-task

%% I have found this: "Budget-aware Semi-Supervised Semantic and Instance Segmentation", and I think we should add it somewhere. One of their conclusions supports our paper.

\subsection{Active learning}
In active learning, the goal is to train a model while querying an oracle to label new samples that are expected to improve the model's accuracy. In computer vision, it has been applied to image classification~\cite{Joshi2009,Ranganathan2017} or semantic segmentation~\cite{andriluka2018fluid,Benenson2019LargeScaleIO,siddiqui2020viewal} among others. As a byproduct, Active learning has also been used as a way to reduce labeling time. For example,~\cite{Konyushkova_2018_CVPR} describes a method that couples Reinforcement Learning and Active Learning to derive the shortest sequence of annotation actions that will lead to object detection within an image. Others have focused on speeding up this process via eye-tracking~\cite{papadopoulos2014} or extreme clicking~\cite{papadopoulos2017extreme}. As such, Active Learning is related to the present work in the sense that our approach is adaptive but differs in that our method determines what annotations types should be collected under a constrained budget instead of predicting at each time step which samples should be added to the annotated set.

%\subsubsection{Bayesian Optimization}
%Introduced in~\cite{kushner1964}, Bayesian optimization has become a very popular method to optimize expensive black-box functions. Commonly used for hyperparameter optimization in deep neural networks, as opposed to random search~\cite{Bergstra2012}. Since then, a myriad of works have employed this technique and proposed improvements~\cite{domhan2015speeding,swersky2013multi} in the search for efficiency.

%We follow the traditional approach of Bayesian optimization, and use a surrogate model to predict performance in unexplored points. Then, we use the expected improvement acquisition function, which has a closed form for Gaussian distributions~\cite{Mockus1978}, to select the next sampling vertex. The expected improvement function is based on a tradeoff between high expected performance and high uncertainty. While sampling the point with highest expected performance seems reasonable, high uncertainty points can provide valuable information about the underlying surface \MG{This is hard for me to understand}, and therefore they should be considered.