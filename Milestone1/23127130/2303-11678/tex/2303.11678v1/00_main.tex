% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{tikz}
\usepackage{comment}
\usepackage{color}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7583} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\graphicspath{{Figures/}}
\input{defs}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Full or Weak annotations? \\An adaptive strategy for budget-constrained annotation campaigns}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not % both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{Javier Gamazo Tejero$^1$, Martin S. Zinkernagel$^2$, Sebastian Wolf$^2$\\
Raphael Sznitman$^1$, Pablo MÃ¡rquez Neila$^1$\\
{\small     $^1$University of Bern, $^2$Inselspital Bern, Switzerland}\\
{\tt\small \{javier.gamazo-tejero, raphael.sznitman, pablo.marquez\}@unibe.ch}\\
{\tt\small \{martin.zinkernagel, sebastian.wolf\}@insel.ch}
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Annotating new datasets for machine learning tasks is tedious, time-consuming, and costly. For segmentation applications, the burden is particularly high as manual delineations of relevant image content are often extremely expensive or can only be done by experts with domain-specific knowledge. Thanks to developments in transfer learning and training with weak supervision, segmentation models can now also greatly benefit from annotations of different kinds. However, for any new domain application looking to use weak supervision, the dataset builder still needs to define a strategy to distribute full segmentation and other weak annotations. Doing so is challenging, however, as it is a priori unknown how to distribute an annotation budget for a given new dataset. To this end, we propose a novel approach to determine annotation strategies for segmentation datasets, whereby estimating what proportion of segmentation and classification annotations should be collected given a fixed budget. To do so, our method sequentially determines proportions of segmentation and classification annotations to collect for budget-fractions by modeling the expected improvement of the final segmentation model. We show in our experiments that our approach yields annotations that perform very close to the optimal for a number of different annotation budgets and datasets.
% that allow high-performing segmentation performances

\end{abstract}

%%%%%%%%% BODY TEXT

\input{01_introduction}
\input{02_related}
\input{03_method}
\input{04_experiments}
\input{05_results}
\input{06_conclusion}

\newpage


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{00_main}
}

%%%%%%%%% SUPPLEMENTARY
\newpage
\onecolumn
\appendix
\input{01_supp_implementation}
\input{02_supp_alpha}
\input{05_supp_average_performance}
\input{03_supp_surfaces}
\input{04_supp_gt}

\end{document}
