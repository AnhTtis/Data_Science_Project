\section{Guaranteed PE for classes of nonlinear systems}\label{sec_main2}
\subsection{Hammerstein systems}\label{sec_Ham}
Consider an unknown Hammerstein system of the form
\begin{equation}
	\begin{aligned}
		x_{k+1} = Ax_k + B\gamma(u_k), \quad y_k = Cx_k + D\gamma(u_k),
	\end{aligned}\label{eqn_HamSys}
\end{equation}
with $(A,B)$ controllable, and $x_k\in\mathbb{R}^n,\,u_k\in\mathbb{R}^m,\,y_k\in\mathbb{R}^p$ are the state, input and output vectors, respectively. Furthermore, $\gamma:\mathbb{R}^m\to\mathbb{R}^{\bar{m}}$ is an unknown nonlinear function which satisfies $\gamma(\mathbf{0})=\mathbf{0}$ without loss of generality. An extension of Theorem \ref{thm_FL} to the class of Hammerstein systems appeared in \cite[Proposition 5]{Berberich20}, assuming that $\gamma_i$, $i\in\mathbb{Z}_{[1,\bar{m}]}$, belong to the span of a given set of $r$ linearly independent basis functions $\psi_j:\mathbb{R}^m\to\mathbb{R}$ which satisfy\footnote{Functions with $\psi_j(\mathbf{0})\neq0$ can be suitably shifted by a constant.} $\psi_j(\mathbf{0})=0$, $j\in\mathbb{Z}_{[1,r]}$. In this case, PE must be imposed on the sequence of basis functions. In this section, we show how this can be done only by the design of the physical input $u$.\par
We denote the stacked vector of the basis functions by $\Psi(u_k)=\begingroup
\setlength\arraycolsep{1pt}\begin{bmatrix} 	\psi_1(u_k) & \cdots & \psi_r(u_k) \end{bmatrix}^\top\endgroup$. For $\lambda_j\in\mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, we define the following square matrix $\Lambda\in\mathbb{R}^{r\times r}$
\begin{equation}
	\begin{aligned}
		\Lambda &= \begin{bmatrix}
			\Psi(\lambda_1) & \Psi(\lambda_2) & \cdots & \Psi(\lambda_r)
		\end{bmatrix}.
	\end{aligned}\label{eqn_Lambda}
\end{equation}
The following theorem provides conditions for an input sequence $\{u_k\}_{k=0}^{N-1}$ that is guaranteed to result in a PE sequence of basis functions $\{\hat{\Psi}_k\}_{k=0}^{N-1}$, where $\hat{\Psi}_k\coloneqq\Psi(u_k)$.
%The following theorem provides conditions on the input $\{u_k\}_{k=0}^{N-1}$ such that the sequence of basis functions $\{\Psi(u_k)\}_{k=0}^{N-1}$ is PE.
\begin{theorem}\label{thm_PEinputHam}
	Given $L\in\mathbb{Z}_{>0}$ and any $r$ linearly independent basis functions $\psi_j:\mathbb{R}^m\to\mathbb{R}$ which satisfy $\psi_j(\mathbf{0})=0$, let $N\geq(r+1)L-1$. Let $\{u_k\}_{k=0}^{N-1}$ take the form
	\begin{equation}
		u_k = \begin{cases}
			\lambda_j, \qquad &k=jL-1, \quad \forall j\in\mathbb{Z}_{[1,r]},\\
			\mathbf{0}, \qquad &\textup{otherwise},
		\end{cases}\label{eqn_PEinputHam}
	\end{equation}
	where $\lambda_j\in\mathbb{R}^m$ are such that $\Lambda$ in \eqref{eqn_Lambda} is invertible. Then it holds that $\textup{rank}(\mathscr{H}_L(\hat{\Psi})) = rL.$
\end{theorem}
\begin{proof}
	Since $\Lambda$ is invertible, each block row of $\mathscr{H}_L(\hat{\Psi})$ below contains $r$ linearly independent columns
		\begin{align}
			&\mathscr{H}_L(\hat{\Psi}) =\label{eqn_fullrankbasisfcns}\\
			&\begingroup % keep the change local
			\setlength\arraycolsep{2pt}
			\begin{bmatrix}
				\begingroup % keep the change local
				\setlength\arraycolsep{2pt}
				\begin{matrix}
					\mathbf{0} & \mathbf{0} & \cdots & \Psi(\lambda_1) & \boldsymbol{\cdots}\\
					\vdots & \vdots & \reflectbox{$\ddots$} & \vdots & \boldsymbol{\cdots}\\
					\mathbf{0} & \Psi(\lambda_1) & \cdots & \mathbf{0} & \boldsymbol{\cdots}\\
					\Psi(\lambda_1) & \mathbf{0} & \cdots & \mathbf{0} & \boldsymbol{\cdots}
				\end{matrix}
				\endgroup
				&
				\begingroup % keep the change local
				\setlength\arraycolsep{2pt}
				\begin{matrix}
					\mathbf{0} & \mathbf{0} & \cdots & \Psi(\lambda_r)\\
					\vdots & \vdots & \reflectbox{$\ddots$} & \vdots\\
					\mathbf{0} & \Psi(\lambda_r) & \cdots & \mathbf{0}\\
					\Psi(\lambda_r) & \mathbf{0} & \cdots & \mathbf{0}
				\end{matrix}
				\endgroup
			\end{bmatrix}\hspace{-1mm}.
			\endgroup\notag
		\end{align}
		Therefore, each block row has rank $r$. Furthermore, due to the structure of $\mathscr{H}_L(\hat{\Psi})$, all block rows are linearly independent from each other. Since there exist $L$ such block rows, it follows that rank$(\mathscr{H}_L(\hat{\Psi}))=rL$.
\end{proof}

In order to find the values of $\lambda_j$ which make $\Lambda$ invertible, one can formulate a feasibility problem as follows
\begin{equation}
	\begin{aligned}
		\textup{find}\quad\lambda_1,\ldots,\lambda_r, \qquad
		\textup{s.t.}\quad\textup{rank}(\Lambda)=r.
	\end{aligned}\label{eqn_feasibility}
\end{equation}
Such a problem can be solved offline, e.g., iteratively as in \cite{Sun17} or by reformulating it as a regularized unconstrained nonlinear least squares problem \cite{Markovsky13_SLRA}. Solving \eqref{eqn_feasibility} only requires knowledge of the user-defined basis functions. The following theorem shows that for \textit{any} given linearly independent basis functions, \eqref{eqn_feasibility} always has a feasible solution.
\begin{theorem}\label{thm_alwaysexistslambda}
	Given any set of $r$ linearly independent basis functions $\psi_j:\mathbb{R}^m\to\mathbb{R}$, there exist $\lambda_j\in\mathbb{R}^m$, for $j\in\mathbb{Z}_{[1,r]}$, such that $\Lambda$ in \eqref{eqn_Lambda} is invertible.
\end{theorem}
\begin{proof}
	Suppose for contradiction that the maximum rank that $\Lambda$ can attain for arbitrary choices of $\lambda_j \in \mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, is given by $d<r$. 	Consider such a choice of $\lambda_j \in \mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, which results in rank$(\Lambda)=d$. Then, the matrix $\begin{bmatrix}\Lambda & \Psi(\bar\lambda)\end{bmatrix}$ has the same image as $\Lambda$ (and hence also rank $d$) for any choice of $\bar\lambda \in \mathbb{R}^m$. %This is the case since otherwise, we already would have reached a contradiction by defining $\bar\Lambda$ by (i) deleting one column of $\Lambda$ without decreasing the rank (which is possible since $d<r$) and (ii) adding $\Psi(\bar\lambda)$ as last column, which would result in rank$(\bar\Lambda)=d+1$.
	
	Since rank$(\Lambda)=d<r$, there exists a non-zero vector $\rho~\in~\mathbb{R}^r$ such that $\rho^\top \Lambda = 0$. Since as discussed above, $\begin{bmatrix}\Lambda & \Psi(\bar\lambda)\end{bmatrix}$ has the same image as $\Lambda$, we also have $\rho^\top\Psi(\bar\lambda)=0$ for arbitrary $\bar\lambda \in \mathbb{R}^m$, i.e.,
	\begin{equation}
		\rho_1\psi_1(\bar\lambda) + \cdots + \rho_r\psi_r(\bar\lambda) =0,\qquad \forall \bar\lambda \in \mathbb{R}^m.\label{eqn_LIbasisfcns}
	\end{equation}
This, however, contradicts linear independence of the basis functions, thus proving that there exists $\lambda_j\in\mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, such that $\Lambda$ is invertible.
\end{proof}

Theorem \ref{thm_PEinputHam} provides sufficient conditions on the input $u$ such that a sequence of linearly independent basis functions is persistently exciting. In the next subsection, we extend this result to the class of SISO flat systems.
\subsection{SISO flat nonlinear systems}\label{sec_flatPE}
Consider an unknown SISO flat system of the form
\begin{equation}
	\begin{aligned}
		x_{k+1} = f(x_k,u_k), \quad y_k = h(x_k),
	\end{aligned}\label{eqn_flatsys}
\end{equation}
where \(x_k\in\mathbb{R}^n, u_k,\,y_k\in\mathbb{R}\) and $f:\mathbb{R}^n\times\mathbb{R}\to\mathbb{R}^n$, $h:~\mathbb{R}^n\to~\mathbb{R}$ are smooth unknown functions with $f(\mathbf{0},0)=\mathbf{0}$ and $h(\mathbf{0})=0$. Let $f_O^j(x_k)$ denote the $j-$th iterated composition of the undriven dynamics $f(x_k,0)$.\par
Since the system is flat (i.e., has a well defined relative degree equal to the system dimension $n$, cf. \cite{AlsaltiBerLopAll2021}), it can be transformed into the discrete-time normal form provided that $0\in\textup{Im}\left(h(f_O^{n-1}(f(x,\cdot)))\right)$ holds for all $x\in\mathbb{R}^n$ (cf. \cite{MonacoNor1987} for more details). This means that there exists an invertible (w.r.t. ${v}_k$) control law ${u}_k=q({x}_k,{v}_k)$, with $q:\mathbb{R}^n\times\mathbb{R}\to\mathbb{R}$ and an invertible coordinate transformation $\xi_k=T({x}_k)$, such that the system is written~as
\begin{equation}
	\begin{matrix}
		\xi_{k+1} = {A}\xi_k + {B}{v}_k, \qquad
		{y}_k = {C}\xi_k,
	\end{matrix}
	\label{BINF}%
\end{equation}%
where $\xi_k\in\mathbb{R}^{n}$ is defined as $\xi_k = y_{[k,k+n-1]}^\top$. Further, ${A},\,{B},\,{C}$ are in the Brunovsky canonical form (cf. \cite{AlsaltiBerLopAll2021}). The synthetic input $v_k$ takes the form
\begin{equation}
	\begin{aligned}
		v_k = h(f_O^{n-1}(f(x_k,u_k))).
	\end{aligned}\label{eqn_expressionforv}
\end{equation}

In the extension of the fundamental lemma to flat systems \cite{AlsaltiBerLopAll2021} and for designing controllers from data in \cite{DePersis22}, persistence of excitation is imposed on a sequence of $r$ basis functions which contain $h\circ f_O^{n-1}\circ f$ in their span. To check this condition, one performs an experiment of length $N\geq (r+1)L-1$, collects the corresponding state or output measurements and then verifies the rank of the resulting Hankel matrix.

In this section, we illustrate how one can enforce persistence of excitation of order one of a \textit{particular choice} of basis functions \textit{a priori}. A specific choice of basis functions may, in general, not contain the unknown nonlinearity \eqref{eqn_expressionforv} in its span. Nonetheless, enforcing PE of order one of such basis functions is still useful for, e.g., designing locally stabilizing controllers for unknown SISO flat systems provided that the basis functions result in a sufficiently good local approximation of \eqref{eqn_expressionforv} (cf. \cite[Section VII.B]{DePersis22}). 

As discussed above, the map from $u$ to $v$ is invertible. This, along with the fact that $f(\mathbf{0},0)=\mathbf{0}$ and $h(\mathbf{0})=0$, implies that a non-zero input applied to the system from zero initial conditions results in a non-zero value of $v$ in \eqref{eqn_expressionforv}. Moreover, invertibility implies that for all $\delta_1,\delta_2\in\mathbb{R}$, the following holds
\begin{equation}
	\delta_1\hspace{-0.5mm}\neq \hspace{-0.5mm}\delta_2 \hspace{-1mm}\iff\hspace{-1mm} h(f_O^{n-1}(f(\mathbf{0},\delta_{1})))\hspace{-0.5mm}\neq\hspace{-0.5mm} h(f_O^{n-1}(f(\mathbf{0},\delta_{2}))).\label{eqn_uniquenessofv}
\end{equation}

We exploit this fact to prove the following lemma, which will be needed later for the main result of this subsection.
\begin{lemma}\label{lemma_vandermonde}
	For $t\in\mathbb{Z}_{>0}$ let $\delta_j\neq0$, $j\in\mathbb{Z}_{[1,t]}$, be mutually distinct values and define $v_{\delta_j}\coloneqq h(f_O^{n-1}(f(\mathbf{0},\delta_j)))$. Then, the following matrix is invertible:
\end{lemma}
\begin{equation}
	\Omega = \begin{bmatrix}
		v_{\delta_1} & v_{\delta_2} & \cdots & v_{\delta_t}\\
		v_{\delta_1}^2 & v_{\delta_2}^2 & \cdots & v_{\delta_t}^2\\
		\vdots & \vdots & \ddots & \vdots\\
		v_{\delta_1}^{t} & v_{\delta_2}^{t} & \cdots & v_{\delta_t}^{t}
	\end{bmatrix}.\label{eqn_OmegaVandermonde}
\end{equation}
\begin{proof}
	Since for $j\in\mathbb{Z}_{[1,t]}$, $\delta_j\neq0$ are mutually distinct values, it follows that the corresponding values $v_{\delta_j}$ are also distinct and non-zero (compare the discussion above Lemma~\ref{lemma_vandermonde}). The matrix $\Omega$ can be written as $\Omega = V^\top \Delta$, where $V\in\mathbb{R}^{t\times t}$ is a square Vandermonde matrix composed of the distinct $v_{\delta_j}$ and, hence, invertible \cite{Macon58} and $\Delta\in\mathbb{R}^{t\times t}$ is a diagonal matrix containing $v_{\delta_j}$. The proof is concluded by noting that $V$ and $\Delta$ are invertible matrices.
\end{proof}

Consider the following choice of basis functions
%\footnote{The powers are defined element-wise, i.e., $\xi_k^t=[\xi_{1,k}^t \,\, \cdots \,\, \xi_{n,k}^t]^\top$.}
\begin{align}
	&\Phi(\xi_k,u_k) = \label{eqn_specificchoice}\begingroup
	\setlength\arraycolsep{2.5pt}\begin{bmatrix}
		u_k & u_k^2 & \cdots & u_k^t& \xi_k^\top & (\xi_k^2)^\top & \cdots & (\xi_k^t)^\top 
	\end{bmatrix}^\top\endgroup\hspace{-1mm},
\end{align}
where the powers are defined element-wise, i.e., $\xi_k^t=[\xi_{1,k}^t \,\, \cdots \,\, \xi_{n,k}^t]^\top$. This choice represents monomials in the state and input up to some finite order $t\in\mathbb{Z}_{>0}$. In the following theorem, we show how to choose input sequences $\{u_k^{(j)}\}_{k=0}^{N_j-1}$, $j\in\mathbb{Z}_{[1,t]}$, such that the resulting sequences of basis functions $\{\hat{\Phi}_k^{(j)}\}_{k=0}^{N_j-1}$ (with $\hat{\Phi}_k^{(j)}\coloneqq\Phi(\xi_k^{(j)},u_k^{(j)})$) are collectively persistently exciting of order one, i.e., that the following mosaic Hankel matrix has full row rank
%\footnote{We use $\{\Phi_k^{(j)}\}_{k=0}^{N_j-1}$ to denote the sequences of basis functions, where $\Phi_k^{(j)}\coloneqq\Phi(\xi_k^{(j)},u_k^{(j)})$.}
\begin{align}
	&\mathcal{H}_1(\varphi) = \begin{bmatrix}
		\mathscr{H}_1(\hat{\Phi}^{(1)}) & \cdots & \mathscr{H}_1(\hat{\Phi}^{(t)})
	\end{bmatrix},
\end{align}
where $\varphi=\begingroup
\setlength\arraycolsep{2pt}\begin{bmatrix}
	(\hat{\Phi}^{(1)})^\top & \cdots & (\hat{\Phi}^{(t)})^\top
\end{bmatrix}^\top\endgroup$. A topic for future work is to extend the results of Theorem \ref{thm_aprioriFL} below to other choices of basis functions and higher orders of (collective) PE.
\begin{theorem}\label{thm_aprioriFL}
	For $t\in\mathbb{Z}_{>0}$, let $\delta_j\neq0$, $j\in~\mathbb{Z}_{[1,t]}$, be mutually distinct values chosen such that $U~=~\begingroup
	\setlength\arraycolsep{2pt}\begin{bmatrix}
		\delta_{[1,t]} & \delta_{[1,t]}^2 & \cdots & \delta_{[1,t]}^t
	\end{bmatrix}\endgroup$ is invertible. For $N_j\geq n+1$ and the basis functions in \eqref{eqn_specificchoice}, let $\{u_k^{(j)}\}_{k=0}^{N_j-1}$ take the form
	\begin{equation}
		u_k^{(j)} = \begin{cases}
			\delta_j, \quad & k=0, \qquad \forall j\in\mathbb{Z}_{[1,t]},\\
			0,\quad &\textup{otherwise}.
		\end{cases}\label{eqn_PEinputSISOflat}
	\end{equation}
	If \eqref{eqn_PEinputSISOflat} are applied to \eqref{eqn_flatsys} starting from zero initial conditions, then it holds that $\textup{rank}(\mathcal{H}_1(\varphi))=t(n+1)$.
\end{theorem}
\begin{proof}
	Without loss of generality, let $N_j=n+1$ for all $j\in\mathbb{Z}_{[1,t]}$. For $c\in\mathbb{Z}_{[0,n-1]}$, we define\footnote{With slight abuse of notation, we also use $v_{\delta_j,0}=v_{\delta_j}$.} $v_{\delta_j,[0,c]}~\coloneqq~\begingroup
	\setlength\arraycolsep{1pt}\begin{bmatrix}
		h(f_O^{n-1}(f(\mathbf{0},\delta_{j}))) \,\, \cdots \,\, h(f_O^{n+c-1}(f(\mathbf{0},\delta_{j})))
	\end{bmatrix}^\top\endgroup\hspace{-1.5mm}$. Next, we partition the mosaic Hankel matrix $\mathcal{H}_1(\varphi)$ as in \eqref{eqn_mosaicFL} (see next page),
\begin{figure*}[!t]
	\normalsize
	\begin{align}
		&\mathcal{H}_1(\varphi) \hspace{-1mm} \coloneqq \hspace{-1mm} \begin{bmatrix}
			H_{u} \\ H_{\xi} \end{bmatrix} \hspace{-1mm}= \hspace{-1mm}\begingroup
		\setlength\arraycolsep{2.25pt}\begin{bmatrix}
			\delta_1 & 0 & 0 & \cdots & 0 & \boldsymbol{\cdots} & \delta_t & 0 & 0 & \cdots & 0\\[-1ex]
			\vdots & \vdots & \vdots & \ddots & \vdots& \boldsymbol{\cdots} & \vdots & \vdots & \vdots & \ddots & \vdots\\[-0.5ex]
			\delta_1^t & 0 & 0 & \cdots & 0 & \boldsymbol{\cdots}& \delta_t^t & 0& 0 & \cdots & 0\\
			\hline
			\mathbf{0} & \begin{pmatrix}
				\mathbf{0}_{n-1\times1}\\[-0.5ex] v_{\delta_1}
			\end{pmatrix} & \begin{pmatrix}
				\mathbf{0}_{n-2\times1}\\[-0.5ex] v_{\delta_1,[0,1]}
			\end{pmatrix} & \cdots & v_{\delta_1,[0,n-1]} & \boldsymbol{\cdots} & \mathbf{0} & \begin{pmatrix}
				\mathbf{0}_{n-1\times1}\\[-0.5ex] v_{\delta_t}
			\end{pmatrix}& \begin{pmatrix}
				\mathbf{0}_{n-2\times1}\\[-0.5ex] v_{\delta_t,[0,1]}
			\end{pmatrix} & \cdots & v_{\delta_t,[0,n-1]}\\[-0.5ex]
			\vdots & \vdots & \vdots & \cdots & \vdots& \boldsymbol{\cdots} & \vdots & \vdots & \vdots & \cdots & \vdots\\[-1ex]
			\mathbf{0} & \begin{pmatrix}
				\mathbf{0}_{n-1\times1}\\[-0.5ex] v_{\delta_1}
			\end{pmatrix}^t & \begin{pmatrix}
				\mathbf{0}_{n-2\times1}\\[-0.5ex] v_{\delta_1,[0,1]}
			\end{pmatrix}^t & \cdots & v_{\delta_1,[0,n-1]}^t & \boldsymbol{\cdots} & \mathbf{0} & \begin{pmatrix}
				\mathbf{0}_{n-1\times1}\\[-0.5ex] v_{\delta_t}
			\end{pmatrix}^t& \begin{pmatrix}
				\mathbf{0}_{n-2\times1}\\[-0.5ex] v_{\delta_t,[0,1]}
			\end{pmatrix}^t & \cdots & v_{\delta_t,[0,n-1]}^t
		\end{bmatrix}\endgroup\hspace{-1mm}.\label{eqn_mosaicFL}
	\end{align}%
	\hrulefill
	\vspace{-1em}
\end{figure*}%
	where $H_u\in\mathbb{R}^{t\times t(n+1)}$ and $H_\xi\in\mathbb{R}^{tn\times t(n+1)}$. Notice that $H_u$ has $t$ linearly independent columns whose transpose are the rows of the invertible matrix $U$.
	Moreover, notice that the structure of $H_{\xi}$ follows (i) from the fact that \eqref{BINF} is in the Brunovsky canonical form (cf. \eqref{BINF}) and (ii) from the choice of the basis function \eqref{eqn_specificchoice}. Using the columns of $H_\xi$, one can construct $n$ submatrices $\bar{H}_{i,\xi}\in\mathbb{R}^{tn\times t}$, $i\in\mathbb{Z}_{[1,n]}$, of the form\footnote{For clarity, $\mathbf{0}_{\ell\times1}$ denotes an $\ell-$dimensional vector of zeros.}
	\begin{equation*}
		\bar{H}_{i,\xi} = \begingroup
		\setlength\arraycolsep{1.5pt}\begin{bmatrix}
			\begin{pmatrix}
				\mathbf{0}_{n-i\times1}\\ v_{\delta_1,[0,i-1]}
			\end{pmatrix} & \begin{pmatrix}
				\mathbf{0}_{n-i\times1}\\ v_{\delta_2,[0,i-1]}
			\end{pmatrix} & \cdots & \begin{pmatrix}
				\mathbf{0}_{n-i\times1}\\ v_{\delta_t,[0,i-1]}
			\end{pmatrix}\\
			\vdots & \vdots & \vdots & \vdots\\
			\begin{pmatrix}
				\mathbf{0}_{n-i\times1}\\ v_{\delta_1,[0,i-1]}
			\end{pmatrix}^{\hspace{-0.25mm}t} & \begin{pmatrix}
				\mathbf{0}_{n-i\times1}\\ v_{\delta_2,[0,i-1]}
			\end{pmatrix}^{\hspace{-0.25mm}t} & \cdots & \begin{pmatrix}
				\mathbf{0}_{n-i\times1}\\ v_{\delta_t,[0,i-1]}
			\end{pmatrix}^{\hspace{-0.25mm}t}
		\end{bmatrix}\endgroup.
	\end{equation*}
	
	Each matrix of this form has $t$ rows given by the rows of the matrix $\Omega$ in \eqref{eqn_OmegaVandermonde}. Since the assumptions of Lemma~\ref{lemma_vandermonde} are satisfied, it follows that $\Omega$ is inverible and hence, each matrix $\bar{H}_{i,\xi}$ has rank~$t$. Notice that the columns of each matrix $\bar{H}_{i,\xi}$ are linearly independent with respect to the columns of any other  $\bar{H}_{j,\xi}$, $i\neq j\in\mathbb{Z}_{[1,n]}$. This holds because the rows of $\Omega$ appear in different rows in each $\bar{H}_{i,\xi}$ as well as the structure in which the block rows $\mathbf{0}_{n-i\times1}$ appear in each submatrix. This implies that rank$(H_\xi)=tn$. Finally, due to the structure of $H_u$ and $H_\xi$, it follows that rank$(\mathcal{H}_1(\varphi))=t(n+1)$.
%	 since the remaining columns of $H_\xi$ that do not appear in any $\bar{H}_{i,\xi}$ only contain zero entries
\end{proof}

In Section \ref{sec_examples}, we use the results of Theorem \ref{thm_aprioriFL} to perform multiple experiments on a SISO flat system and use the collected data to design a stabilizing nonlinear controller (cf. \cite{DePersis22}). In the following section, we study a more general class of nonlinear systems and show existence of sparse inputs that guarantee collective PE of basis functions.