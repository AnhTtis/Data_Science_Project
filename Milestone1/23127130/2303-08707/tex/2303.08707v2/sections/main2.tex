\section{PE for Hammerstein systems}\label{sec_main2}
Consider an unknown Hammerstein system of the form
\begin{equation}
	\begin{aligned}
		x_{k+1} = Ax_k + B\gamma(u_k), \quad y_k = Cx_k + D\gamma(u_k),
	\end{aligned}\label{eqn_HamSys}
\end{equation}
with $(A,B)$ controllable, $x_k\in\mathbb{R}^n,\,u_k\in\mathbb{R}^m,$ and $y_k\in\mathbb{R}^p$. Furthermore, $\gamma:\mathbb{R}^m\to\mathbb{R}^{\bar{m}}$ is an unknown nonlinear function which satisfies $\gamma(\mathbf{0})=\mathbf{0}$ without loss of generality. An extension of Theorem \ref{thm_FL} to the class of Hammerstein systems appeared in \cite{Berberich20}, assuming that $\gamma_i$, $i\in\mathbb{Z}_{[1,\bar{m}]}$, belong to the span of a given set of $r$ basis functions $\psi_j:\mathbb{R}^m\to\mathbb{R}$, $j\in\mathbb{Z}_{[1,r]}$, which satisfy $\psi_j(\mathbf{0})=0$ (note that functions with $\psi_j(\mathbf{0})\neq0$ can be suitably shifted by a constant). In this case, PE must be imposed on the sequence of basis functions. In this section, we show how this can be done only by the design of the physical input $u$.\par
%
We denote the stacked vector of the basis functions by $\Psi(u_k)=\begingroup
\setlength\arraycolsep{1pt}\begin{bmatrix} 	\psi_1(u_k) & \cdots & \psi_r(u_k) \end{bmatrix}^\top\endgroup$. For $\lambda_j\in\mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, we define the following square matrix $\Lambda\in\mathbb{R}^{r\times r}$
\begin{equation}
	\begin{aligned}
		\Lambda &= \begin{bmatrix}
			\Psi(\lambda_1) & \Psi(\lambda_2) & \cdots & \Psi(\lambda_r)
		\end{bmatrix}.
	\end{aligned}\label{eqn_Lambda}
\end{equation}
The following theorem provides conditions for an input sequence $\{u_k\}_{k=0}^{N-1}$ that is guaranteed to result in a PE sequence of basis functions $\{\hat{\Psi}_k\}_{k=0}^{N-1}$, where $\hat{\Psi}_k\coloneqq\Psi(u_k)$.
\begin{theorem}\label{thm_PEinputHam}
	Given $L\in\mathbb{Z}_{>0}$ and any $r$ linearly independent basis functions $\psi_j:\mathbb{R}^m\to\mathbb{R}$ which satisfy $\psi_j(\mathbf{0})=0$, let $N\geq(r+1)L-1$. Let $\{u_k\}_{k=0}^{N-1}$ take the form
	\begin{equation}
		u_k = \begin{cases}
			\lambda_j, \qquad &k=jL-1,\\
			\mathbf{0}, \qquad &\textup{otherwise},
		\end{cases}\label{eqn_PEinputHam}
	\end{equation}
	where $j\in\mathbb{Z}_{[1,r]}$. If $\lambda_j\in\mathbb{R}^m$ are such that the matrix $\Lambda$ in \eqref{eqn_Lambda} is invertible, then it holds that $\textup{rank}(\mathscr{H}_L(\hat{\Psi})) = rL.$
\end{theorem}
\begin{proof}
	Let $\{\tilde{u}_k\}_{k=0}^{N-1}$, $\tilde{u}_k\in\mathbb{R}^r$, take the form of \eqref{eqn_PEinputLTI}. Then by Theorem~\ref{thm_PEinputLTI} it holds that rank$(\mathscr{H}_L(\tilde{u}))=rL$. Next, notice that $\mathscr{H}_L(\hat{\Psi}) = (I_L\otimes \Lambda)\mathscr{H}_L(\tilde{u})$, where $\otimes$ denotes the Kronecker product. Since $\Lambda$ is invertible by assumption, it follows that $(I_L\otimes\Lambda)$ is a square full rank matrix and hence rank$(\mathscr{H}_L(\hat{\Psi}))=\textup{rank}(\mathscr{H}_L(\tilde{u}))=rL$.
\end{proof}

In order to find the values of $\lambda_j$ which make $\Lambda$ invertible, one can formulate a feasibility problem as follows
\begin{equation}
	\begin{aligned}
		\textup{find}\quad\lambda_1,\ldots,\lambda_r, \qquad
		\textup{s.t.}\quad\textup{rank}(\Lambda)=r.
	\end{aligned}\label{eqn_feasibility}
\end{equation}
This is equivalent to requiring that $\Lambda^\top \Lambda\succ0$. Such a problem can be solved offline, e.g., iteratively as in \cite{Sun17} or by reformulating it as a regularized unconstrained nonlinear least squares problem \cite{Markovsky13_SLRA}. Solving \eqref{eqn_feasibility} only requires knowledge of the user-defined basis functions. The following theorem shows that for \textit{any} given set of linearly independent basis functions, \eqref{eqn_feasibility} is feasible.
\begin{theorem}\label{thm_alwaysexistslambda}
	Given any set of $r$ linearly independent basis functions $\psi_j:\mathbb{R}^m\to\mathbb{R}$, there exist $\lambda_j\in\mathbb{R}^m$, for $j\in\mathbb{Z}_{[1,r]}$, such that $\Lambda$ in \eqref{eqn_Lambda} is invertible.
\end{theorem}
\begin{proof}
	Suppose by contradiction that the maximum rank that $\Lambda$ can attain for arbitrary $\lambda_j \in \mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, is $d<r$. Consider such a choice of $\lambda_j \in \mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, which results in rank$(\Lambda)=d<r$. Then, there exists a non-zero vector $\rho~\in~\mathbb{R}^r$ such that $\rho^\top \Lambda = 0$. Moreover, for any $\bar\lambda \in \mathbb{R}^m$, the matrix $\begin{bmatrix}\Lambda & \Psi(\bar\lambda)\end{bmatrix}$ has the same rank as $\Lambda$. Hence, it holds that $\rho^\top\Psi(\bar\lambda)=0$ for arbitrary $\bar\lambda \in \mathbb{R}^m$, i.e.,
		\begin{equation}
			\rho_1\psi_1(\bar\lambda) + \cdots + \rho_r\psi_r(\bar\lambda) =0,\qquad \forall \bar\lambda \in \mathbb{R}^m.\label{eqn_LIbasisfcns}
		\end{equation}
		This, however, contradicts linear independence of the basis functions, thus proving that there exists $\lambda_j\in\mathbb{R}^m$, $j\in\mathbb{Z}_{[1,r]}$, such that $\Lambda$ is invertible.
\end{proof} 

In the next section, we study the class of locally reachable nonlinear systems at the origin and show existence of sparse inputs that guarantee collective PE of basis functions. For SISO flat systems (which are locally reachable), we show how to design those inputs such that PE of any order $L>0$ can be guaranteed for a specific choice of basis functions.