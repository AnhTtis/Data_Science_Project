This section presents the comprehensive evaluation of our SEM through a series of experiments.
To begin with, we describe the implementation details, and then we conduct experiments on four widely-used standard benchmarks for image matching.
Moreover, a range of ablation studies are conducted to validate the effectiveness of each individual component.

\subsection{Implementation Details}\label{4.1}

Our SEM was implemented using PyTorch~\cite{paszke2019pytorch} and was trained on the MegaDepth dataset~\cite{li2018megadepth}.
During the training phase, we input images with a size of $832 \times 832$. Our CNN extractor is a deepened ResNet-18~\cite{he2016deep} with features at $1/32$ resolution.
We set the band width $s_0$ in Epipolar Attention to $10$, and the number of anchor point $N_A$ in Structured Feature Extractor is set to $32$.
The matching threshold $\sigma_h$ in Iterable Epipolar Coarse Matching equals to $0.5$, and the threshold in the matching module before refinement is set to $0.2$.
We use $4$ iterartions in  Iterative Epipolar Coarse Matching module.
Our network is trained for 15 epochs with a batch size of $8$, using the Adam~\cite{kingma2014adam} optimizer with an initial learning rate of $1 \times 10^{-3}$.
% To efficiently establish epipolar attention, we implemented a highly optimized general sparse attention operator based on CUDA.

\begin{table}[ht]
	\centering
	\small
	\caption{Evaluation on HPatches~\cite{balntas2017hpatches} for homography estimation.}
	\label{tab:HPatches_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c l c c c c}
			\hline
            \multirow{2}{*}{Category} &\multicolumn{1}{c}{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{Homography est. AUC} &\multirow{2}{*}{matches} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{3-5}
                    & &{@3px}  &{@5px}  &{@10px} \\
			\hline
            \multirow{5}{*}{Detector-based} &D2Net~\cite{dusmanu2019d2}+NN &23.2 &35.9 &53.6 &0.2K \\

			                                &R2D2~\cite{r2d2}+NN &50.6 &63.9 &76.8 &0.5K \\

                                            &DISK~\cite{tyszkiewicz2020disk}+NN &52.3 &64.9 &78.9 &1.1K \\

                                            &SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &53.9 &68.3 &81.7 &0.6K \\
                                            &Patch2Pix~\cite{zhou2021patch2pix} &46.4 &59.2 &73.1 &1.0K \\
            \hline
            \multirow{6}{*}{Detector-free} &Sparse-NCNet~\cite{rocco2020efficient} &48.9 &54.2 &67.1 &1.0K \\ 

                                            &COTR~\cite{jiang2021cotr} &41.9 &57.7 &74.0 &1.0K \\
                                            &DRC-Net~\cite{li20dualrc} &50.6 &56.2 &68.3 &1.0K \\
                                            &LoFTR~\cite{sun2021loftr} &65.9 &75.6 &84.6 &1.0K \\
                                            &PDC-Net+~\cite{truong2021pdc} &66.7 &76.8 &85.8 &1.0k \\
			                                &\textbf{SEM(ours)}       &\bf 69.6 &\bf 79.0  &\bf87.1  &1.0K   \\
			\hline
		\end{tabular}
	}
 \vspace{-3mm}
\end{table}

\subsection{Homography Estimation}\label{4.2}
\textbf{Dataset and Metric.}
HPatches~\cite{balntas2017hpatches} is a widely used benchmark for evaluating image matching algorithms. In line with the methodology presented in~\cite{dusmanu2019d2}, we have selected 56 sequences featuring significant viewpoint changes and 52 sequences with substantial illumination variation to assess the performance of our SEM, which was trained on the MegaDepth~\cite{li2018megadepth} dataset. We have adopted the same evaluation protocol used in the LoFTR approach~\cite{sun2021loftr}. In terms of metrics, we report the area under the cumulative curve (AUC) of the corner error distance up to 3, 5, and 10 pixels. In addition, we have limited the maximum number of output matches to 1,000 as LoFTR~\cite{sun2021loftr}.

\textbf{Results.}
Table~\ref{tab:HPatches_result} shows that our SEM sets a new state-of-the-art and notably performs better on HPatches~\cite{balntas2017hpatches} under all error threshold. This is a strong demenstration of the effectiveness of our method.
SEM outperforms the previous state-of-the-art method by a margin of $\textbf{2.9\%}$, $\textbf{2.2\%}$ and $\textbf{1.3\%}$ under 3, 5, 10 pixels respectively.
This shows that our proposed Structured Feature Extractor can effectively model the geometric prior of the image content under illumination changes, while Epipolar Attention/Matching can effectively filter out the effect of irrelevant regions due to the viewpoint variations.



% In Table~\ref{tab:HPatches_result}, we can see that our SEM achieves new state-of-the-art performance on HPatches~\cite{balntas2017hpatches} under all error thresholds, which strongly proves the effectiveness of our method.
% SEM outperforms the best method before (LoFTR~\cite{sun2021loftr}), achieving a large margin of $\textbf{5.8\%}$ under 3 pixels, $\textbf{4.7\%}$ under 5 pixels, and $\textbf{3.4\%}$ under 10 pixels.
% Thanks to the proposed spot-guided aggregation module and adaptive scaling module, our method can yield more accurate matches under extreme viewpoint and illumination variations.
% % limiting the number of matches.  
% % Table~\ref{tab:HPatches_result} summarizes the performance comparison between our ASTR and state-of-the-art image matching methods on HPatches dataset.

\subsection{Relative Pose Estimation}\label{4.3}
\textbf{Dataset and Metric.}
To evaluate the performance of our SEM in relative pose estimation, we conduct experiments on two datasets, MegaDepth~\cite{li2018megadepth} and ScanNet~\cite{dai2017scannet}.
MegaDepth~\cite{li2018megadepth} is a large-scale outdoor dataset consisting 1 million internet images of 196 different outdoor scenes, reconstructed by COLMAP~\cite{schonberger2016structure}.
Depth maps as intermediate results can be used to obtain ground truth matches.
We follow the same testing pairs and use the same 1500 image pairs as LoFTR~\cite{sun2021loftr}.
All test images are resized to 1216 in their longer dimension.
On the other hand, ScanNet~\cite{dai2017scannet} is usually used to validate the performance of indoor pose estimation, which is composed of monocular sequences with ground truth poses and depth maps.
Wide baselines and extensive textureless regions in image pairs make ScanNet benchmark challenging. 
All test images are resized to $640 \times 480$.
It is worth noting that we evaluate our SEM trained on MegaDepth\cite{li2018megadepth} on ScanNet~\cite{dai2017scannet}.
We report the AUC of pose error at thresholds $(5^{\circ}, 10^{\circ}, 20^{\circ})$ in both benchmark as LoFTR~\cite{sun2021loftr}.
Our SEM with proposed modules achieves state-of-the-art performance on both datasets.

\begin{table}[t]
	\centering
	\small
	\caption{Evaluation on MegaDepth~\cite{li2018megadepth} for outdoor relative position estimation.}
	\label{tab:MegaDepth_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c l c c c c}
			\hline
            \multirow{2}{*}{Category} &\multicolumn{1}{c}{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{3-5}
                    & &{@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
			\hline
            \multirow{2}{*}{Detector-based} &SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &42.2 &59.0 &73.6 \\
            
                                            &SP~\cite{detone2018superpoint}+SGMNet~\cite{chen2021learning} &40.5 &59.0 &73.6 \\
            \hline
            \multirow{7}{*}{Detector-free}  &DRC-Net~\cite{li20dualrc} &27.0 &42.9 &58.3 \\
                                            
                                            &PDC-Net+(H)~\cite{truong2021pdc}  &43.1  &61.9  &76.1 \\
                                            
                                            &LoFTR~\cite{sun2021loftr} &52.8 &69.2 &81.2 \\                          
                                            
                                            &MatchFormer~\cite{wang2022matchformer} &53.3 &69.7 &81.8 \\
                                            
                                            &QuadTree~\cite{tang2022quadtree} & 54.6 & 70.5 & 82.2 \\
                                            
                                            &ASpanFormer~\cite{chen2022aspanformer} &55.3 &71.5 &83.1 \\

			                                &\textbf{SEM(ours)}       &\textbf{58.0} & \textbf{72.9} &\ \textbf{83.7}  \\
			\hline
		\end{tabular}
	}
\vspace{-3mm}
\end{table}

\begin{table}[t]
	\centering
	\small
	\caption{Evaluation on ScanNet~\cite{dai2017scannet} for indoor relative position estimation. * indicates models trained on MegaDepth~\cite{li2018megadepth}.}
	\label{tab:ScanNet_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c l c c c c}
			\hline
            \multirow{2}{*}{Category} &\multicolumn{1}{c}{\multirow{2}{*}{Method}} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{3-5}
                    & &{@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
			\hline
            \multirow{3}{*}{Detector-based} &D2-Net~\cite{dusmanu2019d2}+NN &5.3 &14.5 &28.0 \\

											&SP~\cite{detone2018superpoint}+OANet~\cite{zhang2019learning} &11.8 &26.9 &43.9 \\
											
											&SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &16.2 &33.8 &51.8 \\
            \hline
            \multirow{5}{*}{Detector-free}  &DRC-Net~\cite{li20dualrc}* &7.7 &17.9 &30.5 \\                         
            
                                            &MatchFormer~\cite{wang2022matchformer}* &15.8  &32.0   &48.0 \\

											&LoFTR-OT~\cite{sun2021loftr}* &16.9 &33.6 &50.6 \\
                                            
                                            % &Quadtree~\cite{tang2022quadtree}* &19.0 &37.3  &53.5 \\
                                            % &ASpanFormer~\cite{chen2022aspanformer}* & & & \\

			                                &\textbf{SEM(ours)*}       & \textbf{18.7} & \textbf{36.6} & \textbf{52.9}  \\
			\hline
		\end{tabular}
	}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/match_exp.pdf}
    \vspace{-6mm}
	\caption{
	% Qualitative results of dense matching on MegaDepth~\cite{li2018megadepth} and ScanNet~\cite{dai2017scannet}.
        Visual qualitative comparison of MatchFormer~\cite{wang2022matchformer} and our SEM on MegaDepth~\cite{li2018megadepth} dataset.
	% The green color indicates epipolar error within $5 \times 10^{-4}$ for indoor scenes and $1 \times 10^{-4}$ for outdoor scenes (in the normalized image coordinates).
    }\label{fig:qualitative}
    \vspace{-3mm}
\end{figure}

\textbf{Results.}
As Table~\ref{tab:MegaDepth_result} illustrated, our SEM show an outstanding performance on MegaDepth~\cite{li2018megadepth} compared to other methods.
Respectively, our method outperformances the previous best method with $\textbf{2.7\%}$ and $\textbf{1.4\%}$ in AUC$@5^{\circ}$ and AUC$@10^{\circ}$.
This result shows the outstanding performance of our method on outdoor scenes.
Furthermore, Table~\ref{tab:ScanNet_result} compares the performance of our proposed SEM with other state-of-the-art models on ScanNet~\cite{dai2017scannet} dataset.
Similar with MegaDepth~\cite{li2018megadepth}, our method ranks the first although the model is not trained on ScanNet~\cite{dai2017scannet} dataset, which demonstrates the strong generalization ability of our proposed SEM.
The main challenge of indoor dataset lies in the widespread presence of textureless and repetitive texture regions, where structured information is critical.
Our method generalizes well in indoor dataset thanks to the utilization of geometry prior in proposed Structured Feature Extractor and Epipolar Attention and Matching.
% To further demonstrate the effectiveness of our SEM, we provide a visual comparison of the matching results with other models in Figure~\ref{fig:qualitative}.
% Notably, our approach performs better in handling challenges such as textureless areas and repetitive patterns.
In order to more strongly demonstrate the effectiveness of our proposed SEM, Figure~\ref{fig:qualitative} provides a qualitative result compared with other methods. Our method has an outstanding performance on textureless and reppetitive texture regions.

% Comparison results in Fig.~\ref{fig:comparison} verify that our ASTR follows the consistency principle to obtain accurate matching results.

% our spot-guided aggreagtion module has the ability to focus the interaction area of most pixels to the correct area.
% In contrast, the regions of linear attention are very scattered.
\subsection{Visual Localization}\label{4.4}
\textbf{Dataset and Metric.}
To evaluate the performance of our SEM in visual localization, we use the InLoc~\cite{taira2018inloc} dataset, which consists of 9972 RGBD images.
Among them, 329 RGB images are selected as queries for visual localization.
InLoc~\cite{taira2018inloc} presents challenges such as textureless regions and repetitive patterns under large viewpoint changes.
We evaluate the performance of our SEM trained on MegaDepth~\cite{li2018megadepth} in the same way as LoFTR~\cite{sun2021loftr}.
The metric used in InLoc~\cite{taira2018inloc} measures the percentage of images registered within given error thresholds.

\begin{table}[t]
	\centering
	\small
	\caption{Visual localization evaluation on the InLoc~\cite{taira2018inloc} benchmark.}
	\label{tab:Inloc_result}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{l c c}
			\hline
            \multicolumn{1}{c}{\multirow{2}{*}{Method}} & DUC1 & DUC2 \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{2-3}
                     & \multicolumn{2}{c}{$\left(0.25m, 10^\circ\right)$ / $\left(0.5m, 10^\circ\right)$ / $\left(1m, 10^\circ\right)$} \\
			\hline
            % \multirow{1}{*}{Detector-based} & SP~\cite{detone2018superpoint}+SuperGlue~\cite{sarlin2020superglue} &  49.0 / 68.7 / 80.8 & 53.4 / 77.1 / 82.4  \\
            LoFTR~\cite{sun2021loftr} & 47.5 / 72.2 / 84.8 & 54.2 / 74.8 / \textbf{85.5} \\
            MatchFormer~\cite{wang2022matchformer} & 46.5 / 73.2 / 85.9 & \textbf{55.7} / 71.8 / 81.7 \\
            ASpanFormer~\cite{chen2022aspanformer} & 51.5 / 73.7 / 86.4 & 55.0 / 74.0 / 81.7 \\
            \textbf{SEM(ours)} & \textbf{52.0} / \textbf{74.2} / \textbf{87.4} & 50.4 / \textbf{76.3} / 83.2 \\
			\hline
		\end{tabular}
	}
\end{table}

\textbf{Results.}
Our method's performance on the InLoc~\cite{taira2018inloc} benchmark is summarized in Table~\ref{tab:Inloc_result}, where it achieves the best performance on DUC1 and performs comparably to state-of-the-art methods on DUC2.
This demonstrates our method's strong generalization ability in visual localization, even in challenging indoor scenes.

\subsection{Ablation Study}\label{4.5}

\begin{table}[t]
	\centering
	\small
	\caption{Ablation Study of each component on MegaDepth~\cite{li2018megadepth}. SF indicates Structured Feature extractor, and EAM refers to Epipolar Attention and Matching.}
	\label{tab:ablation_study}
	\vspace{-3mm}
	\scalebox{0.75}{
		\begin{tabular}{c c c c c c c}
    		\hline
    		% \multirow{2}{*}{Method} & 
			\multirow{2}{*}{Index} & \multirow{2}{*}{Multi-Level} & \multirow{2}{*}{SF} & \multirow{2}{*}{EAM}
		    &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{5-7}
                & & & & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
            % \hline
		    % LoFTR~\cite{sun2021loftr} & & & & & 45.9 & 63.3 & 76.8 \\
		    \hline
		    % \multirow{4}{*}{ASTR(ours)}
		      1 & & & & 45.6 & 62.2 & 75.3 \\
		      2 & \checkmark & & & 46.7 & 63.1 & 76.3  \\
		      3 & \checkmark & \checkmark & & 47.3 & 64.3 & 76.8 \\
		      4 & \checkmark & \checkmark & \checkmark & \bf 48.1  & \bf 64.7 & \bf 77.4 \\
		     \hline
		\end{tabular}
	}
\end{table}

To deeply analyze and evaluate the effectiveness of each component in SEM, we conducted detailed ablation studies on MegaDepth~\cite{li2018megadepth}.
Here, we use images with a size of 544 for both training and evaluation.
In Table~\ref{tab:ablation_study}, we gradually added the proposed components to the baseline to analyze their impact.
% The baseline (Index-1) we used is slightly different from LoFTR~\cite{sun2021loftr}.
% More details can be found in Supplementary Material.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/vis_exp.pdf}
    \vspace{-6mm}
	\caption{
        Visualization of the epipolar banded areas corresponding to some points and matching results on MegaDepth~\cite{li2018megadepth} and ScanNet~\cite{dai2017scannet}.
        Points and corresponding epipolar regions are identified by the same color.
    }\label{fig:vis_exp}
\vspace{-3mm}
\end{figure}

\textbf{Effectiveness of Structured Feature Extractor.}
According to ~\cite{chen2022aspanformer}, we learn that the multi-level strategy is useful in local matching, which is verfied by the results of Index-2 and Index-1.
% Comparing the results of Index-2 and Index-1, we can see that the performance of our model is improved with the Structured Feature Extractor
Comparing the results of Index-2 and Index-1, we find that the proposed Structured Feature Extractor helps during the inference phase, increasing performance with an improvement of
+0.6 AUC@$5^\circ$, +1.2 AUC@$10^\circ$ and +0.5 AUC@$20^\circ$.
% We divide the spot-guided aggregation module into multi-level cross attention and spot-guided attention for ablation studies.
% We first add vanilla cross attention layers at 1/32 resolution to the baseline (Index-2 in Table~\ref{tab:ablation_study}).
% Comparing the results of Index-2 and Index-1, we conclude that 1/32 resolution global interaction across images is beneficial for image matching.
% Then, in Index-3, linear attention layers at 1/8 resolution are substituted for the spot-guided attention layers.
% The performance of Index-3 is improved compared with Index-2, which verifies the effectiveness of our spot-guided attention.
% In Figure~\ref{fig:spot_vis}, we visualize vanilla and our spot-guided cross attention maps for contrast, showing that spot-guided attention can indeed avoid interference from unrelated areas.
% Please refer to Supplementary Material for ablation studies on values of $l$ and $k$.

\begin{table}[t]
	\centering
	\small
	\caption{Ablation Study with different $s_0$ in Epipolar Attention on MegaDepth~\cite{li2018megadepth}.}
	\label{tab:s_sigma_result}
	\vspace{-3mm}
	% \scalebox{0.75}{
	% 	\begin{tabular}{c c c c}
 %    		\hline
 %    		% \multirow{2}{*}{Method} & 
	% 		\multirow{2}{*}{$\sigma_h$($s_0=10$)} &\multicolumn{3}{c}{Pose estimation AUC} \\
	% 		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
	% 		\cline{2-4}
 %                & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
 %            % \hline
	% 	    % LoFTR~\cite{sun2021loftr} & & & & & 45.9 & 63.3 & 76.8 \\
	% 	    \hline
	% 	    % \multirow{4}{*}{ASTR(ours)}
	% 	   %    1  & 46.0 & 62.7 & 76.2 \\
	% 		  % 2  & 47.5 & 64.0 & 77.1\\
	% 	   %    3  & 47.3 & 63.8 & 76.7 \\
	% 	   %    4  & \bf 47.7 & \bf 64.5 & \bf 77.4\\
	% 		  % 5  & 47.1 & 63.7 & 77.0\\
	% 		  % 6  & 46.9 & 63.6 & 76.6\\
	% 	     \hline
	% 	\end{tabular}
	% }
	\scalebox{0.85}{
		\begin{tabular}{c c c c}
    		\hline
    		% \multirow{2}{*}{Method} & 
			\multirow{2}{*}{$s_0$} &\multicolumn{3}{c}{Pose estimation AUC} \\
			% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
			\cline{2-4}
                & {@$5^\circ$}  &{@$10^\circ$}  &{@$20^\circ$} \\
            \hline
            5 & 45.6 &  62.7 & 76.2 \\
            10 & \textbf{48.1} & \textbf{64.7} & \textbf{77.4} \\
            15 & 47.5 & 64.3 & 77.2 \\
            20 & 46.7 & 62.4 & 76.4 \\
		     \hline
		\end{tabular}
	}
        \vspace{-3mm}
\end{table}

% To maximize the effectiveness of our spot-guided attention, we explore how to set suitable parameters $l$ and $k$.
% First, in the setting of Index-3, we fix $l=5$ and vary $k$ from 1 to 6.
% After observing the results in Table~\ref{tab:k_l_result}, the performance drops when $k$ is smaller than 4 or larger than 4.
% Then, we fix $k=4$ and vary $l$ from 3 to 9.
% As shown in Table~\ref{tab:k_l_result}, we find that the model achieves the best performance at $l=5$.
% The reason may be that the spot area is too small to provide sufficient information from another image when using small $k$ or $l$.
% With large $k$ or $l$, for a certain pixel, some matching areas of low confidence or dissimilar points will damage its feature aggregation.



\textbf{Effectiveness of Epipolar Attention.}
As shown in Table~\ref{tab:ablation_study}, We replace the original linear attention and global matching with our Epipolar Attention and Epipolar Matching in Index-4.
Comparing the results of Index-4 and Index-3, we can see that the performance is improved, which indicates the effectiveness of our proposed Epipolar Attention and Epipolar Matching.
In Figure~\ref{fig:vis_exp}, we visualize the matching results for several pair of images. Specifically, we randomly select points in the reference image and identify their corresponding epipolar band regions (marked by same color) in the source image.
For the points in the source image, the corresponding area is also found in the reference image.
The attention is computed within these points and their corresponding band regions.
The visualization results show that our module can effectively utilize the relative camera poses to filter out irrelevant regions.
To further explore the effect of eliminating irrelevant regions, we explored the effect of different $s_0$ on MegaDepth dataset.
As Table~\ref{tab:s_sigma_result} shows, an appropriate value for $s_0$ is crucial as it can balance the filtering of unwanted regions and tolerance to camera pose estimation errors.
If $s_0$ is too small, it may lead to the exclusion of correct matches when the initial camera pose estimation is imprecise.
On the other hand, if $s_0$ is too large, it may not effectively filter out irrelevant regions.
This shows that the modules we designed play the role we expected.
