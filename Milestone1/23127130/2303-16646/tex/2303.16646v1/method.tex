\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/pipeline.pdf}
    \caption{
    The architecture of our SEM.
    Our SEM consists of two stages: Iterative Epipolar Coarse Matching stage and refine Matching stage.
    Here, ``Attention'' means vanilla self-and-cross attention, ``Linear Attention'' means linear self-and-cross attention.
    Please refer to the text for detailed architecture.
    }\label{fig:method}
    \vspace{-3 mm}
\end{figure*}
In this section, we introduce our proposed Structured Epipolar Matcher (SEM) for local feature matching.
The overall architecture is illustrated in Figure~\ref{fig:method}.
Here we first present the overall scheme of SEM in Section~\ref{sec:3.1}.
In Section~\ref{sec:3.2}, we describe our Iterative Epipolar Coarse Matching in detail.
Then we explain our Structured Feature Extractor in Section~\ref{sec:3.3} and our Epipolar Attention/Matching in Section~\ref{sec:3.4}.
At last, Section~\ref{sec:3.5} states the loss used in our SEM. 

\subsection{Overview}\label{sec:3.1}
As shown in Figure~\ref{fig:method}, the proposed SEM mainly consists of two stages, including a Iterative Epipolar Coarse Matching stage and refine Matching stage.
Here we provide a concise overview of the complete process.
For a pair of input images, reference image $I_{ref}$ and source image $I_{src}$, we first use a Feature Pyramid Network (FPN)~\cite{lin2017feature} to extract multi-level features of each image $\{f_{1/i}^{ref},f_{1/i}^{src}\}$.
Then the feature maps with the size of ${1/32}$ and ${1/8}$ are sent into the Iterative Epipolar Coarse Matching stage.
% In Iterable Epipolar Coarse Matching, we first apply one self-and-cross attention layer for initializing the feature maps ${f_{1/32}^{ref},f_{1/32}^{src}}$ and one self-and-cross linear attention layer for initializing the feature maps ${f_{1/8}^{ref},f_{1/8}^{src}}$.
% Two 
With the help of ${1/32}$ feature maps, coarse matching matrix $M$ are obtained from the feature maps with the size of ${1/8}$ through a Structured Feature Extractor, Epipolar Attention and Epipolar Matching.
Finally, we refine our coarse matching results in refine matching, which is the same as LoFTR~\cite{sun2021loftr}.

\subsection{Iterative Epipolar Coarse Matching}\label{sec:3.2}
Considering that global feature can help fine feature to locate correct matching area, we use a multi-level strategy in Iterative Epipolar Coarse Matching.
First, we apply one self-and-cross attention layer on the feature maps $\{f_{1/32}^{ref},f_{1/32}^{src}\}$ and one self-and-cross linear attention layer on the feature maps $\{f_{1/8}^{ref},f_{1/8}^{src}\}$.
After that, features at two scales are updated from each other. 
This progress can be described as following:
\begin{equation}\label{eq:epc0}
    \begin{aligned}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
    &\hat{f}_{1/32}^{t} = f_{1/32}^{t} + \mathrm{Conv_{1 \times 1}}(\mathrm{Down}(f_{1/8}^{t})), t \in \{ref, src\}
    \\
    &\hat{f}_{1/8}^{t} = f_{1/8}^{t} + \mathrm{Conv_{1 \times 1}}(\mathrm{Up}(f_{1/32}^{t})), t \in \{ref, src\}
    \end{aligned}
\end{equation}
where $\hat{f}_{1/32}^{t}$ and $\hat{f}_{1/8}^{t}$ denotes the fused features with the size of ${1/32}$ and ${1/8}$.
The purpose of the initialization is to establish the global connection in image or cross images.
Next, we enter the process of iterative matching.
During iterative process, $f_{1/32}^{ref}$ and $f_{1/32}^{src}$ are updated by self-and-cross attention layers to mainatin the global connection.
Coarse matching matrix $M \in \mathbb{R}^{HW \times HW}$ is initialized by computing pointwise similarity from flattened $f_{1/8}^{ref}$ and $f_{1/8}^{src}$ and a dural-softmax operator:
\begin{equation}\label{eq:epc1}
    \begin{aligned}
    % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
    &S(i,j) = \tau \langle f_{1/8}^{ref}(i), f_{1/8}^{src}(j) \rangle,
    \\
    &\noindent M(i, j) = \mathrm{softmax}(S(i, :))(i,j) \cdot \mathrm{softmax}(S(:, j))(i,j),
    \end{aligned}
\end{equation}
 where $i$ is a pixel in image $I_{ref}$ and $j$ is a pixel in image $I_{src}$.
 $\tau$ means the temperature coefficient.
 $S \in R^{HW \times HW}$ is the similarity matrix.
As features on appearance, $f_{1/8}^{ref}$ and $f_{1/8}^{src}$ are supplied by structured feature from Structured Feature Extractor and become more discriminating.
And then, in Epipolar Attention, we interact $f_{1/8}^{ref}$ and $f_{1/8}^{src}$ via cross attention within corresponding epipolar banded area, which will be detailed in Section~\ref{sec:3.4}.
After updating features between different scales, in Epipolar Matching, we use $f_{1/8}^{ref}$ and $f_{1/8}^{src}$ to rewrite matching matrix $M$ along epipolar banded areas, which is a sparse similarity calculation with a low computational cost.
After several iterations of epipolar matching, we obtain accurate coarse matching results.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/Structured_Feature_Extractor.pdf}
    \caption{
    The schematic of our Structural Feature Extractor.
    We take the black point to examplify the construction of our structural feature, and we show the way to fuse appearance feature and structural feature.
    The red points means the selected anchor points.
    }\label{fig:Structural Feature Extractor}
    \vspace{-3 mm}
\end{figure}

\subsection{Structured Feature Extractor}\label{sec:3.3}
As human beings, when we search for correspondences between a pair of images, we do not just compare pixels.
Instead, we first look for conspicuous and easily matched anchor points in the images.
By utilizing the relative positional relationships between pixels and anchor points, human beings can easily overcome the challenges posed by textureless and repetitive texture areas.
This process is also inspiring for learning-based approaches.
As shown in Figure~\ref{fig:motivation_s}, even though the appearance features of points $B$ and $C$ are not discriminating enough, suitable anchor points can be selected, and the relative positions can be utilized to build structured features, which can compensate for the shortage of appearance features, resulting in better correspondences in textureless and repetitive texture regions.

We propose the \textbf{Structured Feature Extractor} for above purpose.
First we define the confidence of correspondence:
\begin{equation}\label{eq:def_of_conf}
\mathrm{conf}(p_i)=\max \left\{ M (i, j) \mid j\in\{1,2,\cdots,HW\} \right\},
\end{equation}
where $p_i$ is the pixel in reference image.
As shown in Figure~\ref{fig:Structural Feature Extractor}, given two image feature maps, $f^{ref}$ and $f^{src}$, we first refer to matching matrix $M$ to find high-confidence correspondence over a threshold of $\sigma_h$.
Among these correspondences, we randomly select $N_A$ pairs as anchor points (marked in red), denote as 
\begin{equation}\label{eq:def_of_anchor}
\begin{aligned}
 & \mathcal{A}^{ref}=\left\{ \left( x^{ref}_i, y^{ref}_i \right) \mid i=1,2,\cdots,N_A \right\}, \\
 & \mathcal{A}^{src}=\left\{ \left( x^{src}_i, y^{src}_i \right) \mid i=1,2,\cdots,N_A \right\}. \\
\end{aligned}
\end{equation}
Without loss of generality, we talk about the reference feature map $f^{ref}$. For any point $Q$ in reference image whose coordinate is $(x,y)$, we calculate the coordinate differences and Euclidean distances from $Q$ to all anchor points:
\begin{equation}\label{eq:p_anchor_diff}
\begin{aligned}
\Delta X^{ref} &= \left(x-x^{ref}_1,x-x^{ref}_2,\cdots,x-x^{ref}_{N_A} \right), \\
\Delta Y^{ref} &= \left(y-y^{ref}_1,y-y^{ref}_2,\cdots,y-y^{ref}_{N_A} \right), \\
D^{ref} &= \sqrt{\left(\Delta X\right)^2+ \left(\Delta Y\right)^2} = \left( d^{ref}_1, d^{ref}_2,\cdots,d^{ref}_{N_A} \right). \\
\end{aligned}
\end{equation}
The structured feature $f^{ref}_{sf}$ is then defined as
% \begin{small}
\begin{equation}\label{eq:def_sf}
\begin{aligned}
    & \Delta X^{ref}_n = \mathrm{norm} \left(\Delta X^{ref}\right), \\
    & \Delta Y^{ref}_n = \mathrm{norm} \left(\Delta Y^{ref}\right), \\
    & D^{ref}_n = \mathrm{norm} \left(\Delta X^{ref}\right), \\
    & f^{ref}_{sf}(x,y) = \Delta X^{ref}_n \Vert \Delta Y^{ref}_n \Vert D^{ref}_n, \\
\end{aligned}
\end{equation}
% \end{small}
where $\Vert$ is concatenate operation, $\mathrm{norm}$ is the $L_1$ normalization operation. $f^{src}_{sf}(x,y)$ is calculated by similar process.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/Norm.pdf}
    \caption{
    After scaling and rotating the image, the relative positional relationship between the anchor points (marked as {\color{red} red}) and the given point (marked as black).
    }\label{fig:norm}
    \vspace{-3 mm}
\end{figure}

Why do we need $D$ and $L_1$ normalization to model the relative position instead of only using $\Delta X$ and $\Delta Y$?
A suitable structured feature should satisfy both \textbf{scaling invariance} and \textbf{rotational invariance}.
As shown in Figure~\ref{fig:norm}, when an image is resized, $\Delta X$, $\Delta Y$ and $D$ are all scaled proportionally.
A $L_1$ normalization can solve the problems caused by scaling variation and make the structured feature more robust.
Also, considering the rotation of the images, $\Delta X$ and $\Delta Y$ will also change despite the normalization.
This is the motivation why we introduce $D$, which is rotational invariant with the normalization. 

Finally, the fused feature $f^{src}_{fused}$ can be obtained by applying a MLP to fuse the appearance feature and structural feature:
\begin{equation}\label{eq:feat_fusing}
f^{src}_{fused}(x,y)=\mathrm{MLP}\left( f^{src}(x,y) \Vert f^{src}_{sf}(x,y) \right)
\end{equation}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{graph/Epipolar_Matching.pdf}
    \caption{
    The illustration of our Epipolar Attention and Epipolar Matching.
    }\label{fig:Epipolar_Matching}
    \vspace{-3 mm}
\end{figure}

\subsection{Epipolar Attention and Matching}\label{sec:3.4}
Existing methods mostly perform global interaction between reference and source images, which will introduce irrelevant noisy points.
In Figure~\ref{fig:motivation_e}, the point $F$ has an negative impact on the matching result of point $D$.
Therefore, we attempt to remove irrelevant regions during feature updating and matching as soon as possible.
But how?
We think of some ``good" points that are easily to match correctly.
With the help of epipolar geometry prior, these ``good" points can filter out irrelevant areas and reduce candidate regions for other points.
Next, we will introduce the preliminaries of attention mechanism and epipolar line in Section~\ref{sec:3.4.1}.
Finally, we states our epipolar attention and matching in Section~\ref{sec:3.4.2}.

\subsubsection{Attention and Epipolar Line}\label{sec:3.4.1}

Vanilla attention calculation can be devided into 3 steps.
Inputs are converted to query $Q$, key $K$ and value $V$ by MLPs.
To start with, the attention map is computed by dot production from query $Q$ and key $K$.
After the softmax operator, the attention map becomes the weight matrix. 
Finally, the output is weighted sum of value $V$ with the weight matrix above.
The whole process can be expressed as 
\begin{equation}\label{eq:epc3}
    \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(QK^T)V.
\end{equation}
The attention map can produce high computational and memory consumption, which is overwhelming for computer vision tasks.
To resolve the problem above, Katharopoulos~\cite{katharopoulos2020transformers} propose linear attention, which approximates the softmax operator with the product of two kernel functions $\phi(\cdot) = \mathrm{elu}(\cdot) + 1$ and uses the commutative law of matrix multiplication:
\begin{equation}\label{eq:epc4}
    \mathrm{Linear\_attention}(Q, K, V) = \phi (Q) (\phi (K^T)V),
\end{equation}
However, some experiments~\cite{germain2021visual,chen2021learning} have proved that linear attention will cause a slight decline in model performance.

In Figure~\ref{fig:Epipolar_Matching}, $P$ is a point in the real world, $P_0$ and $P_1$ is a pair of matching points, which are also the projections of $P$ on the reference and source image.
$M$ and $N$ are optical centers of the reference and source camera.
$N_0$ is the projection of $N$ on the reference image.
$M_1$ is the projection of $M$ on the source image.
$N_0$ and $M_1$ are a pair of epipolar points.
$P_0N_0$ and $P_1M_1$ are a pair of epipolar lines.
According to epipolar constraint, for any point in $P_0N_0$, its matching point on the source image will definitely fall on $P_1M_1$.

\subsubsection{Epipolar Attention and Matching}\label{sec:3.4.2}
In Figure~\ref{fig:Epipolar_Matching}, given Mating Matrix $M$, we first pick up ``good" matching pairs whose confidence is above a threshold $\sigma_h$.
With these matching pairs, we can obtain the relative pose of two images $\{R,T\}$.
Without loss of generality, we take the point $P_0$ as an example to show the way to find the candidate matching area of $P_0$.
The coordinate of $P_0$ on the reference image is $(x,y)^T$.
Then we find a point $\widetilde{P}$ on $MP$ whose reference camera coordinate is $K_{ref}^{-1} \cdot (x,y,1)^T$, where $K_{ref}$ denotes the intrinsics of the reference camera.
And the source camera coordinate of $\widetilde{P}$ is $R \cdot K_{ref}^{-1} \cdot (x,y,1)^T + T$.
Project $\widetilde{P}$ to the source image plane and we get $\widetilde{P}_1$.
The coordinate of $\widetilde{P}_1$ on the source image $c_{\widetilde{P}_1}$is 
\begin{equation}\label{eq:epc5}
\begin{aligned}
& p_{\widetilde{P}_1} = K_{src} \cdot (R \cdot K_{ref}^{-1} \cdot (x,y,1)^T + T),
\\
& c_{\widetilde{P}_1} = (\frac{p_{\widetilde{P}_1}(1)}{p_{\widetilde{P}_1}(2)}, \frac{p_{\widetilde{P}_1}(0)}{p_{\widetilde{P}_1}(2)}),
\end{aligned}
\end{equation}
where $K_{src}$ is the intrinsics of the source camera.
Meanwhile, the coordinate of $M_1$ on the source image $c_{M_1}$is 
\begin{equation}\label{eq:epc6}
\begin{aligned}
& p_{M_1} = K_{src} \cdot T,
\\
& c_{M_1} = (\frac{p_{M_1}(1)}{p_{M_1}(2)}, \frac{p_{M_1}(0)}{p_{M_1}(2)}).
\end{aligned}
\end{equation}
Now we can easily get the slope $k$ and intercept $b$ of $M_1\widetilde{P}_1$:
\begin{equation}\label{eq:epc7}
\begin{aligned}
& k = \frac{p_{\widetilde{P}_1}(1) - p_{M_1}(1)}{p_{\widetilde{P}_1}(0) - p_{M_1}(0)},
\\
& b = p_{M_1}(1) - k \cdot p_{M_1}(0).
\end{aligned}
\end{equation}
Considering that the initial relative pose $\{R,T\}$ may be inaccurate, we expand the epipolar region to a banded region with tolerance $s_0$, whose boundaries can be represented as $\{k,b - s_0\}$ and $\{k,b + s_0\}$.
At this point, we finally locate the candidate matching area of $P_0$.
% And $P_0$ performs cross attention with points in its corresponding candidate region.
The candidate matching area of points on the source image can be obtained in the similar way.
Each point performs cross attention with points in its corresponding candidate region.
For later updating matching matrix $M$, each point is also matched against points in its corresponding candidate region.

\subsection{Loss Function}\label{sec:3.5}

Our loss function mainly includes two parts, iterative coarse matching loss and refine matching loss.
Iterative coarse matching loss $L_i$ is mainly used to supervise the matching matrix $M$ updated by each iteration:
\begin{equation}\label{eq:epc11}
\noindent L_i = \sum_{k} - \frac{1}{\left\lvert M^{gt}_c \right\rvert} \sum_{(i,j) \in M^{gt}_c} \log M^k(i,j),
\end{equation}
where $M^{gt}_c$ is the ground truth matches at coarse resolution.
$M^k(i,j)$ denotes the predicted matching matrix in the $k$-th iteration.
Fine matching loss $L_f$ is a weighted $L_2$ loss same as LoFTR~\cite{sun2021loftr}.
Our final loss is:
\begin{equation}\label{eq:epc14}
\noindent L_{total} = L_i + L_f.
\end{equation}
% Without loss of generality, we take the reference image as an example to 
% Then for the same point $p$, its reference camera coordinate $p^ref$ and 



% Given an image pair $I_{ref}$ and $I_{src}$, to start with, we extract multi-scale feature maps of each image through a shared Feature Pyramid Network (FPN)~\cite{lin2017feature}.
% Here, we denote feature maps with the size of $1/i$ as $F^{1/i} = \{F^{1/i}_{ref}, F^{1/i}_{src}\}$.
% Then, $F^{1/32}$ and $F^{1/8}$ are fed into the spot-guided aggregation module for coarse-matching and depth maps.
% % The acquisition of coarse matching is divided into two stages
% Here, the coarse matching result is acquired in three phases.
% First, we need to compute the similarity matrix, which can be given by $S(i,j) = \tau \langle f_{1/8}^{ref}(i), f_{1/8}^{src}(j) \rangle$ with flattened features, where $\tau$ is the temperature coefficient.
% Then we perform dural-softmax operator on $S$ to calculate matching matrix $\mathrm{P_c}$:
% \begin{equation}\label{eq:epc7}
%     \noindent \mathrm{P_c}(i, j) = \mathrm{softmax}(S(i, :)) \cdot \mathrm{softmax}(S(:, j)).
% \end{equation}
% Finally, we use the mutual nearest neighbor strategy and the threshold $\theta_c$ to filter out the coarse-matching result $M_c$.
% % After that, we calculate depth maps with size of ${1/8}$ from coarse-matching results.
% According to depth information and coarse-matching result, we can crop different size grids on the high-resolution feature map $F^{1/2}$.
% % We can crop different size grids on high resolution feature map $F^{1/2}$ according to coarse-matching results and depth information.
% After linear self and cross attention layers, features of the cropped grids are used to produce the final fine-level matching result.
% % In sopt-guided aggregation module, we first interacte globally across images to update $F^{1/8}$ and $F^{1/32}$.
% % After that, we conduct vanilla cross attention on $F^{1/32}$ and
% \subsection{Spot-Guided Aggregation Module}\label{sec:3.2}
% Correct matching always satisfies the local matching consistency, i.e., the matching points of two similar adjacent pixels are also close to each other in the other image.
% When humans establish dense matches between two images, they will first scan through the two images quickly and keep in mind some landmarks that are easier to match correctly.
% For those trouble points similar to surrounding landmarks, it is not easy to obtain correct matches in the beginning.
% But now, they can focus attention around the matching points of landmarks to revisit trouble points' matches.
% In this way, more correctly matched landmarks are obtained.
% After several iterations of the above process, eventually, they will get the matching result for the whole image.
% Inspired by this idea, we design  a spot-guided aggregation module.
% % Section~\ref{3.2.1} demonstrates the design of the entire spot-guided aggregation module.
% Section~\ref{3.2.1} introduces the preliminaries of vanilla attention and linear attention.
% Section~\ref{3.2.2} describes our proposed spot-guided attention mechanism.
% Section~\ref{3.2.3} demonstrates the design of the entire spot-guided aggregation module.


% \subsubsection{Preliminaries}\label{3.2.1}
% % As an important part in transformer,
% The calculation of vanilla attention requires three inputs: query $Q$, key $K$, and value $V$.
% % we can obtain the affinity matrix with Scaled Dot-Product Attention.
% % Furthermore, the weighted sum of the value is computed through the following equation:
% The output of vanilla attention is a weighted sum of the value, where the weight matrix is determined by the query and its corresponding key.
% The process can be described as
% \begin{equation}\label{eq:epc1}
%     \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(QK^T)V.
% \end{equation}
% However, in vision tasks, the size of the weight matrix $\mathrm{softmax}(QK^T)$ increases quadratically as the image resolution grows.
% When the image resolution is large, the memory and computational cost of vanilla attention is unacceptable.
% To solve this problem, 
% Linear attention ~\cite{katharopoulos2020transformers} is proposed to replace the softmax operator with the product of two kernel functions:
% \begin{equation}\label{eq:epc2}
%     \mathrm{Linear\_attention}(Q, K, V) = \phi (Q) (\phi (K^T)V),
% \end{equation}
% where $\phi(\cdot) = \mathrm{elu}(\cdot) + 1$.
% Since the number of feature channels is much smaller than the number of image pixels, the computational complexity is reduced from quadratic to linear.
% % Although linear attention makes it possible to process high-resolution images, some empirical studies show that linear attention does not work well.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{graph/ASTR_spot.pdf}
%     \caption{
%     The illustration of our spot-guided attention.
%     Here, Sel. Score is the result of element-wise multiplication of Sim. Score and Conf. Score.
%     % $p$ only interacts with spot areas on the source image.
%     }\label{fig:spot}
% \end{figure}

% \subsubsection{Spot-Guided Attention}\label{3.2.2}
% It is known from the local matching consistency that the matching points of similar adjacent pixels are also close to each other.
% In Figure~\ref{fig:spot}, we illustrate the case that the reference image as query aggregates features from the source image.
% % As shown in Fig 3, for any pixel A on the reference feature map,
% Given reference and source feature maps $F^{1/8} = \{f_{1/8}^{ref}, f_{1/8}^{src}\}$, 
% we compute a matching matrix $P_s$ across images.
% % {\color{red}We illustrate the case that the reference image as query retrieves information from the source image.}
% % {\color{blue}We illustrate the case in Fig.~\ref{fig:spot} that the reference image as query aggregates features from the source image.}
% % % As shown in Fig 3, for any pixel A on the reference feature map,
% % It is known from the local matching consistency that the matching points of similar adjacent pixels are also close to each other.
% % Thus, for any pixel on the reference feature map, we should focus the search range on matching regions of local high-quality similar points.
% For any pixel $p$ in Figure~\ref{fig:spot}, we first compute the similarity score $\mathrm{S_{sim}}(p)$ between $p$ and the pixels in the $l \times l$ area around $p$.
% Specifically, the similarity score can be obtained as
% % \begin{equation}\label{eq:epc3}
% % \noindent S(p_i) = \langle F^{1/8}_{ref.}(p), F^{1/8}_{ref.}(p_i) \rangle,
% %     % Score_S(p) = \mathop{softmax}\limits_{i}(\{S(p_i)\}_{p_i \in N(p)}),
% %     % Sim(p) = \langle F^{1/8}_{ref.}(p), F^{1/8}_{ref.}(p_i) \rangle,
% % \end{equation}
% \begin{small}
% \begin{equation}\label{eq:ep3}
%     % S(p_i) = \langle F^{1/8}_{ref.}(p), F^{1/8}_{ref.}(p_i) \rangle,
% \noindent \mathrm{S_{sim}}\left(p\right) = \mathop{\mathrm{softmax}}\limits_{i}\left(\left\{\langle f_{1/8}^{ref}(p), f_{1/8}^{src}(p_i) \rangle\right\}_{p_i \in N(p)}\right),
% \end{equation}
% \end{small}
% % Sim(p) = \langle F^{1/8}_{ref.}(p), F^{1/8}_{ref.}(p_i) \rangle,
% % $Sim(p)$ denotes the similarity score of pixel p in reference image.
% where $\langle \cdot, \cdot \rangle$ is the inner product, and $N(p)$ is the set of pixels in the $l \times l$ field around pixel $p$. 
% % (except pixel $p$).
% In addition, we should also consider the reliability of points in $N(p)$.
% % The confidence score can be viewed as the similarity score between each pixel on the reference image
% For each $p_i \in N(p)$, confidence can be viewed as the highest similarity to all pixels on the source images.
% % within the last search range of $p_i$.
% Meanwhile, we can also get the matching point position of $p_i$, denoted as $\mathrm{Loc}(p_i)$.
% Hence, $\mathrm{Loc}(p_i)$ and confidence score $\mathrm{S_{conf}}(p)$ can be computed in the following way:
% % \begin{equation}\label{eq:epc5}
% % \noindent C(p_i) = \mathop{max}\limits_{j}\mathop{softmax}\limits_{j}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% %     % Score_C(p) = {Conf(p_i)}_{p_i \in N(p)},
% % \end{equation}
% % \begin{equation}\label{eq:epc6}
% % \noindent P(p_i) = \mathop{argmax}\limits_{j}\mathop{softmax}\limits_{j}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% %     % Score_C(p) = {Conf(p_i)}_{p_i \in N(p)},
% % \end{equation}
% \begin{small}
% \begin{equation}\label{eq:epc4}
%     \begin{aligned}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
%     % &\mathrm{P_s}\left(p_i,:\right) =\mathop{\mathrm{softmax}}\limits_{j}\left(\left\{\langle f_{1/8}^{ref}(p_i), f_{1/8}^{src}(p_j) \rangle\right\}_{p_j \in I_{src}}\right),
%     % \\
%     % &\mathrm{C}\left(p_i\right) = \mathop{\mathrm{max}}\limits_{j}\left(\mathrm{P_s}\left(p_i,:\right)\right),
%     % \\
%     &\mathrm{S_{conf}}\left(p\right) = \left\{\mathop{\mathrm{max}}\left(\mathrm{P_s}\left(p_i,:\right)\right)\right\}_{p_i \in N(p)}.
%     \\
%     &\mathrm{Loc}\left(p_i\right) = \mathop{\mathrm{argmax}}\left(\mathrm{P_s}\left(p_i,:\right)\right),
%     % \\
%     % &\mathrm{S_{conf}}\left(p\right) = \left\{\mathop{\mathrm{max}}\left(\mathrm{P_s}\left(p_i,:\right)\right)\right\}_{p_i \in N(p)}.
%     \end{aligned}
% \end{equation}
% \end{small}
% % where $I_{src}$ represents the pixels on .
% % where $\mathrm{P_s}(p_i,:)$ denotes the matching probability in $I_{src}$.
% % In particular, the initial search range of each point is the entire image.
% Combining two scores, we select $p$ and top-k points $\mathrm{Topk}(p)$ whose matching points are used as seed points $\mathrm{Seed}(p)$:
% % \begin{equation}\label{eq:epc8}
% %     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% % \noindent topk(p) = topk\{Score_S(p) \cdot Score_C(p)\},
% % \end{equation}
% \begin{equation}\label{eq:epc5}
%     \begin{aligned}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
%     &\mathrm{Topk}(p) = \{p\} \cup \mathrm{topk}\{\mathrm{S_{sim}}(p) \cdot \mathrm{S_{conf}}(p)\},
%     \\
%     &\mathrm{Seed}(p) = \{\mathrm{Loc}(q)\}_{q \in \mathrm{Topk}(p)},%\mathrm{topk}\{\mathrm{Score_S}(p) \cdot \mathrm{Score_C}(p)\}},
%     \end{aligned}
% \end{equation}
% Following that, we extend $l \times l$ regions centered on these seed points $\mathrm{Seed}(p)$ on $I_{src}$, which are the spot areas of $p$.
% Finally, cross attention is performed between $p$ and corresponding spot areas.
% After exchanging the source image and the reference image, the source feature map is updated in the same way. 

% %  and expand $k \times k$ areas around the matching points of these points
% \subsubsection{Spot-Guided Feature Aggregation}\label{3.2.3}
% For the input features $F^{1/32}$ and $F^{1/8}$, $F^{1/32}$ is updated by vanilla cross attention, and $F^{1/8}$ is updated by linear cross attention for initialization.
% Then, two features of different resolutions are fed into the spot-guided aggregation blocks.
% In each block, $F^{1/32}$ and $F^{1/8}$ are first fused into each other in the following way:
% % \begin{equation}\label{eq:epc10}
% %     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% % \noindent \hat{F}^{1/32} = F^{1/32} + Conv_{1 \times 1}(Down(F^{1/8})),
% % \end{equation}
% \begin{equation}\label{eq:epc6}
%     \begin{aligned}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
%     &\hat{F}^{1/32} = F^{1/32} + \mathrm{Conv_{1 \times 1}}(\mathrm{Down}(F^{1/8})),
%     \\
%     &\hat{F}^{1/8} = F^{1/8} + \mathrm{Conv_{1 \times 1}}(\mathrm{Up}(F^{1/32})),
%     \end{aligned}
% \end{equation}
% % \begin{equation}\label{eq:epc10}
% %     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% % \noindent \hat{F}^{1/32} = F^{1/32} + Conv_{1 \times 1}(Down(F^{1/8})),
% % \end{equation}
% where $\hat{F}^{1/32}$ and $\hat{F}^{1/8}$ are features after fusion. $\mathrm{Down}(\cdot)$ and $\mathrm{Up}(\cdot)$ are downsampling and upsampling.
% And then, $\hat{F}^{1/32}$ aggregate features across images by vanilla attention.
% In the meantime, $\hat{F}^{1/8}$ aggregate features across images by spot-guided attention.
% After four spot-guided aggregation blocks, $1/32$-resolution features are fused into $1/8$-resolution features, which are used to obtain the coarse-matching result $M_c$.

% % , which are flattened to $f_{1/8}^{ref} \in R^{m \times c}$ and $f_{1/8}^{src} \in R^{n \times c}$.

% % Similarity matrix can be given by $S(i,j) = \tau \langle f_{1/8}^{ref}(i), f_{1/8}^{ref}(j) \rangle$, where $\tau$ is the temperature coefficient.
% % Then we perform dural-softmax operator on $S$ to calculate probability matrix $\mathrm{P_c}$:
% % \begin{equation}\label{eq:epc7}
% %     \noindent \mathrm{P_c}(i, j) = \mathrm{softmax}(S(i, :)) \cdot \mathrm{softmax}(S(:, j)).
% % \end{equation}
% % Finally, we use the mutual nearest neighbor strategy and the threshold $\theta_c$ to filter out the coarse-matching result $M_c$.
% % to produce the final coarse-matching matrix with size of $1/8$.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{graph/ASTR_adaptive_scaling.pdf}
%     \caption{
%     The illustration of our adaptive scaling module.
%     On the left is the reference image, whose optical center is $C_{ref}$.
%     On the right is the source image, whose optical center is $C_{src}$.
%     $x_i$ and $x_j$ are the projections of the real-world point $X$.
%     }\label{fig:adaptive_scaling}
% \end{figure}

% \subsection{Adaptive Scaling Module}\label{3.3}
% At the fine stage, previous methods usually crop fixed-size grids based on the coarse matching result.
% When there is a large scale variation, fine matching may fail since the ground-truth matching points are out of grids.
% Thus, we refer to depth information to adaptively crop grids of different sizes between images.
% Section~\ref{3.3.1} describes the way to obtain depth information from the coarse-matching result.
% Section~\ref{3.3.2} demonstrates the process of adaptively cropping grids.

% \subsubsection{Depth Information}\label{3.3.1}

% % Since we have already obtained coarse-level matching result, the relative pose of the two pictures $\{R, T\}$
% With the coarse-level matching result, we can obtain the relative pose of two images $\{R, T\}$ through RANSAC~\cite{fischler1981random}.
% It should be noted that the $T$ calculated here has a scale uncertainty, i.e., $T_{real} = \alpha T$, where $\alpha$ is the scale factor.
% Given the image coordinates of any pair of matching points $\{x_i, x_j\}$ from coarse-level matching result, they satisfy the following equation:
% \begin{equation}\label{eq:epc8}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% % \noindent d_j K^{-1}_j \binom{x_j^T}{1} = d_i R K^{-1}_i \binom{x_i^T}{1} + \alpha T ,
% \noindent d_j K^{-1}_j (x_j, 1)^T = d_i R K^{-1}_i (x_i, 1)^T + \alpha T ,
% \end{equation}
% where $d_i$ and $d_j$ are the depth values of $x_i$ and $x_j$.
% $K_i$ and $K_j$ are corresponding camera intrinsics.
% We let $p_i = R K^{-1}_i (x_i, 1)^T$ and $p_j = K^{-1}_j (x_j, 1)^T$.
% From Equation~\eqref{eq:epc8} it can be deduced that:
% \begin{equation}\label{eq:epc9}
% \begin{aligned}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% &d_j p_j = d_i p_i + \alpha T, \\
% \Rightarrow &\left\{
%     \begin{array}{lr}
%     (\cfrac{d_j}{\alpha}) p_j \wedge p_i = 0 + T \wedge p_i, \\
%     0 = (\cfrac{d_i}{\alpha}) p_i \wedge p_j + T \wedge p_j,
%     \end{array}
% \right.
% \\
% \Rightarrow &\left\{
%     \begin{array}{lr}
%     \cfrac{d_j}{\alpha} = \mathrm{mean}(\mathrm{div}(T \wedge p_i, p_j \wedge p_i)), \\
%     \cfrac{d_i}{\alpha} = \mathrm{mean}(\mathrm{div}(- T \wedge p_j, p_i \wedge p_j)),
%     \end{array}
% \right.
% % \noindent \frac{d_j}{\alpha} = \frac{div(T \wedge p_i, p_j \wedge p_i)}{3},
% % \\
% % \noindent \frac{d_i}{\alpha} = \frac{div(- T \wedge p_j, p_i \wedge p_j)}{3},
% \end{aligned}
% \end{equation}
% where $\wedge$ indicates outer product.
% $\mathrm{div}(\cdot, \cdot)$ denotes element-wise division between two vectors.
% $\mathrm{mean}(\cdot)$ is the scalar mean of each component of a vector.
% In this way, we have obtained depth information of $x_i$ and $x_j$ with scale uncertainty.

% \subsubsection{Adaptive Scaling Strategy}\label{3.3.2}
% As shown in Figure~\ref{fig:adaptive_scaling}, $x_i$ and $x_j$ are a pair of matching points at the coarse stage.
% $d_i$ and $d_j$ are depth values of $x_i$ and $x_j$.
% To begin with, we crop a $s_i \times s_i$ region centered on $x_i$.
% When the scale changes too much, the correct matching point $\widetilde{x_j}$ may be beyond the $s_i \times s_i$ region around $x_j$.
% Because everything looks small in the distance and big on the contrary, the size of cropped grid $s_j$ should satisfy:
% \begin{equation}\label{eq:epc10}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent \frac{s_j}{s_i} = \frac{d_i}{d_j} = (\frac{d_i}{\alpha})(\frac{d_j}{\alpha})^{-1},
% \end{equation}
% Following the above approach, we can crop different sizes of grids adaptively according to the scale variation.
% % Features in the grids centered on $x_i$ and $x_j$ are further fed into linear self and cross layers.
% After the same refinement as LoFTR~\cite{sun2021loftr}, we get the final matching position $\widetilde{x_j}$ of $x_i$.
% % Please refer to Supplementary Material for more details.

% \subsection{Loss Function}\label{3.3}
% Our loss function mainly consists of three parts, spot matching loss, coarse matching loss, and fine matching loss.
% Spot matching loss is the cross entropy loss to supervise the matching matrix during spot-guided attention:
% \begin{equation}\label{eq:epc11}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent L_s = - \frac{1}{\left\lvert M^{gt}_c \right\rvert} \sum_{(i,j) \in M^{gt}_c} \log \mathrm{P_s}(i,j),
% \end{equation}
% where $M^{gt}_c$ is the ground truth matches at coarse resolution.
% %  over the spot-guided attention areas.
% Coarse matching loss is also the cross entropy loss to supervise the coarse matching matrix:
% \begin{equation}\label{eq:epc12}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent L_c = - \frac{1}{\left\lvert M^{gt}_c \right\rvert} \sum_{(i,j) \in M^{gt}_c} \log \mathrm{P_c}(i,j).
% \end{equation}
% % where $M^{gt}_c$ is the ground truth matches at coarse resolution.
% Fine matching loss $L_f$ is a weighted $L_2$ loss same as LoFTR~\cite{sun2021loftr}.
% % \begin{equation}\label{eq:epc13}
% %     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% % \noindent L_f = \frac{1}{\left\lvert M^{gt}_f \right\rvert} \sum_{(i,j) \in M^{gt}_f} \frac{1}{\sigma ^2(i)}{\left\lVert j - j_{gt}\right\rVert}_2 ,
% % \end{equation}
% % where $M^{gt}_f$ is the ground truth matches at fine resolution and $\sigma^2(i)$ means the total variance of the heatmap.
% Therefore, our total loss is:
% \begin{equation}\label{eq:epc14}
%     % Conf(p_i) = \mathop{max}\limits_{i}\mathop{softmax}\limits_{i}(\{\langle F^{1/8}_{ref.}(p_i), F^{1/8}_{src.}(p_j) \rangle\}_{p_j \in R(p_i)}),
% \noindent L_{total} = L_s + L_c + L_f.
% \end{equation}

% 结构化特征的部分

% As human beings, when we search for correspondences between a pair of images, we do not just compare pixels.
% Instead, we first look for conspicuous and easily matched anchor points in the images.
% By utilizing the relative positional relationships between pixels and anchor points, human beings can easily overcome the challenges posed by textureless and repetitive texture areas.
% This process is also inspiring for learning-based approaches.
% As shown in the motivation figure, even though the appearance features of points $B$ and $C$ are not discriminating enough, suitable anchor points can be selected, and the relative positions can be utilized to build structural features, which can compensates for the shortage of appearance features, resulting in better correspondences in textureless and repetitive texture regions.

% We propose the \textbf{Structural Feature Extractor} module for above purpose. Given two image feature maps, $F^{ref}$ and $F^{src}$, we first use a matcher to find high-confidence correspondence with a threshold of $\sigma_h$. Among these correspondences, we randomly select $N_A$ pairs as anchor points, denote as
% \begin{equation}\label{eq:def_of_anchor}
% \begin{aligned}
%  & A^{ref}=\left\{ \left( x^{ref}_i, y^{ref}_i \right) \mid i=1,2,\cdots,N_A \right\}, \\
%  & A^{src}=\left\{ \left( x^{src}_i, y^{src}_i \right) \mid i=1,2,\cdots,N_A \right\}. \\
% \end{aligned}
% \end{equation}
% Without loss of generality, we talk about the reference feature map $F^{ref}$. As figure for each point $P(x,y)$ in reference image, we calculate the coordinate differences and Euclidean distances from $P$ to all anchor points:
% \begin{equation}\label{eq:p_anchor_diff}
% \begin{aligned}
% \Delta \mathbf{X}^{ref} &= \left(x-x^{ref}_1,x-x^{ref}_2,\cdots,x-x^{ref}_{N_A} \right), \\
% \Delta \mathbf{Y}^{ref} &= \left(y-y^{ref}_1,y-y^{ref}_2,\cdots,y-y^{ref}_{N_A} \right), \\
% \mathbf{D}^{ref} &= \sqrt{\left(\Delta \mathbf{X}\right)^2+ \left(\Delta \mathbf{Y}\right)^2}. \\
% \end{aligned}
% \end{equation}
% The structural feature is then defined as
% \begin{equation}\label{eq:def_sf}
% \mathbf{F}^{ref}_{sf}(x,y)=\Delta \mathbf{X}^{ref} \Vert \Delta \mathbf{Y}^{ref} \Vert \mathbf{D}^{ref},
% \end{equation}
% where $\Vert$ is concatenate operation. $\mathbf{F}^{src}_{sf}(x,y)$ is calculated by similar process.

% Finally, a MLP is applied to fuse the appearance features and structural features:
% \begin{equation}\label{eq:feat_fusing}
% \mathbf{F}^{src}_{fused}(x,y)=\textbf{MLP}\left( \mathbf{F}^{src}(x,y) \Vert \mathbf{F}^{src}_{sf}(x,y) \right)
% \end{equation}


% denoted as A^ref_1, A^ref_2, A^src_1, and A^src_2 in the two images, respectively. Without loss of generality, we choose a point P(x,y) in the ref image, and the Euclidean distances between its coordinates and those of all anchor points in I^ref are calculated. By utilizing the relative position relationship between pixels and anchor points, we construct structured features to compensate for the lack of apparent features in textureless and poor texture regions.
