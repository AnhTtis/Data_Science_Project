\documentclass{article}
\usepackage{arxiv}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage{amssymb}
\usepackage{thmtools, thm-restate}
\usepackage{hyperref}
\usepackage{bm}
\usepackage[title]{appendix}
\usepackage[explicit,compact]{titlesec}             % Fixing the Appendix Display
\usepackage{dsfont}
\usepackage[square]{natbib}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}

\newcommand{\mo}{\textcolor{red}{Mohammad}}

\newcommand{\step}[1]{\vspace{+5pt} \textbf{#1} \\}
\newcommand{\cor}{\mathrm{cor}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\renewcommand{\u}{\mathbf{u}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bnu}{\bm{\nu}}
\newcommand{\bN}{\bm{N}}
\newcommand{\indicator}[2]{\mathds{1}_{#1}\big({#2}\big)}
\newcommand{\apprx}{\mathrm{apprx}}
\newcommand{\residue}{\mathcal{O}\Big(\frac{1}{\sqrt{\min \{N_1, N_2\}}}\Big)}

\newcommand{\RMSet}[2]{\mathcal{R}_{\mu,#1}^{#2}(\mu_{#1}, \nu_{#1})}
\newcommand{\RNSet}[2]{\mathcal{R}_{\nu,#1}^{#2}(\mu_{#1}, \nu_{#1})}


\usepackage{cases}

\newcommand{\order}[1]{\mathcal{O}\Big(#1\Big)}

\newcommand{\dist}[1]{\mathrm{dist}( #1 )}
\newcommand{\distH}[1]{\mathrm{dist}_\mathrm{H}( #1 )}

\newcommand{\todo}{\textcolor{red}{TODO!}}

\newcommand{\Co}[1]{\mathrm{Co}(#1)}

\newcommand{\lowervalue}{\underline{J}}
\newcommand{\loweralpha}{\underline{\alpha}}
\newcommand{\uppervalue}{\bar{J}}
\newcommand{\upper}{\bar{\beta}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\allowdisplaybreaks

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{criterion}{Criterion}
\newtheorem{assertion}{Assertion}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}


% Fancy Single Letters
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\pr}[1]{\mathbb{P}\left\{ #1 \right\}}

% operators
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right \rvert}
\newcommand{\expectation}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\expct}[2]{\mathbb{E}_{#2}\left[#1\right]}

% Probability related
\newcommand{\dtv}[1]{\mathrm{d}_{\mathrm{TV}} \big( #1 \big)}
    
% Fancy Numbers
\newcommand{\vectorone}{\mathds{1}}

% Formatting
\newcommand{\halfspace}{\kern 0.2em}

% Comments
\newcommand{\sg}[1]{\textcolor{blue}{Scott: #1}}
\newcommand{\pn}[1]{\textcolor{red}{Panos: #1}}
\newcommand{\edit}[1]{\textcolor{blue}{#1}}

\newcommand{\figref}[1]{Figure~\ref{#1}}

\title{\LARGE \bf  Zero-Sum Games between Mean-Field Teams: A Common Information and Reachability based Analysis
}



\author{
	Yue Guan%
	\thanks{Yue Guan is a PhD student with the School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, USA. Email:
		{\tt\small yguan44@gatech.edu}}%}
	\qquad Mohammad Afshari%
	\thanks{Mohammad Afshari is a Postdoctoral Fellow with the Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA. Email:
		{\tt\small mafshari@gatech.edu}}
 %	\qquad Ali Pakniyat%
 %	\thanks{Ali Pakniyat is an Assistant Professor with the Department of Mechanical Engineering, University of Alabama, Tuscaloosa, AL, USA.
%{\tt\small apakniyat@ua.edu}}
 	\qquad Panagiotis Tsiotras%
 	\thanks{Panagiotis Tsiotras is the David \& Andrew Lewis Chair Professor with the School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, USA. Email: {\tt\small tsiotras@gatech.edu}}
}


\begin{document}
\maketitle


\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
    This work studies the behaviors of two competing teams in a discrete environment, each having a large number of agents. 
    A novel zero-sum mean-field team game framework is proposed, where the team-level interactions are captured as a zero-sum game while the dynamics within each team is formulated as a mean-field team problem.
    Following the ideas from the mean-field game literature, we first approximate the large-population team game with its limiting case when the number of agents tends to infinity.
    Leveraging the common information approach, we introduce two fictitious coordinators, one for each team, such that the infinite-population game is transformed into an equivalent zero-sum game between the two coordinators.
    We study the optimal coordinator strategies and translate them back to feedback strategies  the original agents can deploy.
    We prove that the strategies are $\epsilon$-optimal for the original finite-population team game, regardless of the strategy utilized by the opponent team.
    Furthermore, we show that the suboptimality diminishes when the size of both teams approaches infinity.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Multi-agent decision-making arises in many applications, ranging from warehouse robots~\citep{li2021lifelong}, surveillance missions~\citep{tian2020search} to organizational economics~\citep{gibbons2013handbook}. 
While most of the literature formulates the problems under the cooperative team framework, where all agents share a common objective, results on mixed collaborative-competitive team behaviors are relatively sparse. 
In this work, we consider a competitive team game, where two teams, each consisting of a large number of intelligent agents, compete at the team level, while the agents within each team collaborate. 
Such hierarchical interactions are of particular interest to military operations~\citep{tyler2020autonomous} and other multi-agent systems that operate in adversarial environments.

There are two major challenges when trying to solve such competitive team problems:
\begin{enumerate}
    % \item The desirable decentralization leads to discrepancy in the information available to individual agents. 
    % While agents need to cooperate or compete depending on the teams, they have different information about the state of the environment.
    % Due to this discrepancy in the information structure, dynamic programming cannot be directly applied to team problems~\citep{arabneydi2015team}. 
    \item Team problems are \emph{computationally} challenging since the solution complexity increases exponentially with the number of agents. 
    Consequently, it is difficult to obtain tractable solutions for team problems with large populations. 
    %
    \item Competitive team problems are \emph{conceptually} difficult due to the unknown nature of the opponent team.
    In particular, one may want to impose additional assumptions on the opponent team to obtain tractable solutions, but these assumptions may not hold in practice. 
    It is thus unclear whether one can directly deploy approximation techniques that are commonly seen in the large-population game literature.
\end{enumerate}

The scalability challenge is also present in dynamic games; however, it has been successfully resolved for a class of models known as  mean-field games (MFGs)~\citep{huang2006large,lasry2007mean}.
The salient feature of mean-field games is that the agents are weakly coupled in their dynamics and rewards, and coupling is only through the mean field (i.e., the empirical distribution of the agents). 
When the population is large, the game can be approximately solved by considering the infinite population limit, at which point the interactions among agents are reduced to a game between a representative agent and the infinite population. 

Motivated by the results in the MFG literature, we investigate team-optimal solutions in zero-sum mean-field team games. 
We focus on discrete-time models with finite state and action spaces.
In particular, the problem formulation assumes two teams, each having a \emph{finite} number of homogeneous agents;
the dynamics of the agents and the team rewards are coupled through the mean-field of the states. 
We consider a zero-sum reward at the team level, under which one team's gain is another team's loss.
Furthermore, we assume a \textit{mean-field sharing} information structure~\citep{arabneydi:2014}; that is, each agent observes its local state and the mean-fields of both its own team and the opponent team. 
In some applications, such as adversarial resource allocation~\citep{shishika2022dynamic}, the distribution of an agent's own team can be accessed easily, while the distribution of the opponent team can be obtained through some filtering.
In other applications, it is possible to obtain the distribution of an agent's own team using distributed consensus algorithms~\citep{olfati2006belief}.

In the single-team setting, an approach based on the common-information technique~\citep{arabneydi:2014} was developed to provide a dynamic programming decomposition by exploiting the mean-field sharing structure.
In general, team optimal control problems with mean-field sharing belong to the NEXP complexity class~\citep{bernstein2002complexity}.
To obtain tractable solutions, the method developed in~\citep{arabneydi:2014} assumes that all homogeneous agents within the team deploy the same strategy prescribed by a fictitious coordinator.
The identical strategy assumption significantly simplifies the problem, and it has been shown that such a restriction does not lead to a sub-optimal solution under linear-quadratic models~\citep{arabneydi2015team}.
In the competitive team setting, however, although one may restrict strategies used by agents within its team to be identical, making the same assumption for the opponent team is not justified, as doing so may lead to a significant underestimation of the opponent's capabilities.
This leads to the second challenge before.

In this work, we assume that one team deploys a team strategy that is identical across all agents. 
We  prove that by restricting the opponent team to identical strategies, we end up with an $\epsilon$-suboptimal team strategy.
However, the $\epsilon$-suboptimality diminishes at the rate $\mathcal{O}(1 / \sqrt{\min\{N_1, N_2\}})$, where $N_1$ and $N_2$ are the number of agents within each team.
The key intuition behind this result leverages the fact that the state and action spaces are finite. 
Consequently, regardless of the strategies applied by the opponent agents (e.g., different agents may apply different strategies), one can construct an identical strategy, whose action selection distribution at each state mimics the fraction of opponent agents that apply each action at that state.
One can show that the constructed identical strategy induces a controlled dynamics similar to that of the original opponent team strategy.
Furthermore, the difference diminishes as the number of agents approaches infinity.
Through reachability-based analysis, we conclude that the class of opponent teams deploying identical strategies is rich enough.
Thus, we can restrict our attention to such opponent class strategies and still obtain satisfactory team performance.



Based on the above result, we extend the common-information approach~\citep{arabneydi:2014} to the zero-sum team game setting and treat the mean-fields of the two teams as common information.
We consider the limiting case where both teams have infinite population, and we introduce two fictitious coordinators, one for each team. 
The fictitious coordinator of a team observes the mean-fields of both teams and prescribes an identical policy (decision rule at a time step) for all agents within its team; 
each agent then selects its action according to the prescribed policy and its observed individual state (see~\figref{fig:schematic}).
The infinite-population team game can then be cast into an equivalent zero-sum game between the two coordinators, where the joint state is the  joint mean-fields and the actions are the local feedback policies that can be prescribed by the coordinator.
We prove that the optimal value functions (max-min and min-max) of the coordinator game are Lipschitz continuous, which then allows us to provide performance guarantees in terms of the original finite-population game.  

In summary, the contributions of this work are four-fold: 
(i) a novel formulation of zero-sum mean-field team games in discrete environments;
(ii) simplification of large-population games to equivalent coordinator games;
(iii) analysis of the optimal value functions and strategies;
and
(iv) derivation of the sub-optimality bounds for the proposed approach.

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.75\linewidth]{Figures/MFTG_Schematic.png}
    \caption{A schematic of the proposed common-information approach for the mean-field zero-sum team games.}
    \label{fig:schematic}
\end{figure}


Some important notations used in this paper is presented in the following Table~\ref{table:notation}.

\begin{table}[h!]
  \begin{center}
    \caption{Important Notations}
    \label{table:notation}
    \begin{tabular}{l|l|l}
      \toprule % <-- Toprule here
      \textbf{Notation} & \textbf{Description} & \textbf{Population Size}\\
      \midrule % <-- Midrule here
      $N_1$ / $N_2$ / $N$ & Number of agents in Blue / Red team / whole system & Finite\\
      $\rho$ & Population ratio $N_1 / (N_1+N_2) $ & Finite \\ 
      $\x_{i,t}^{N_1} (x_{i,t}^{N_1})$ / $\y_{j,t}^{N_2} (y_{j,t}^{N_2})$ & State of Blue/Red agent (realization)& Finite \\
      $\bmu_t^{N_1}(\mu_t^{N_1})$ / $\bnu_t^{N_2} (\nu_t^{N_2})$ & Empirical distribution of Blue/Red team (realization)& Finite \\
      % $\mu_t^{N_1}$ / $\nu_t^{N_2}$ & Empirical distribution of Blue/Red team (realized)& Finite \\  
      $\mu_t^{\rho}$ / $\nu_t^{\rho}$ & Mean-Field of Blue/Red team & Infinite \\  
      $\phi$ / $\psi$ & Identical Blue/Red team strategies & Finite\\
      $\phi^\rho$ / $\psi^\rho$ & Identical Blue/Red team strategies & Infinite\\  
      % $\Phi$ / $\Psi$ & Set of identical Blue/Red team strategies, Both finite and infinite population\\
      $\phi^{N_1}$ / $\psi^{N_2}$ & Arbitrary Blue/Red team strategies &  Finite\\
      % $\Phi^{N_1}$ / $\Psi^{N_2}$ & Set of arbitrary Blue/Red team strategies, Finite population\\
      $\pi_t$ / $\sigma_t$ & Identical Blue/Red team local policy & Infinite\\
      $\alpha_t$ / $\beta_t$ & Blue/Red coordinator policy & Infinite\\
      $r^N_t$ / $r^\rho_t$ & Game reward - empirical distribution / mean-field & Finite / Infinite\\
      % $r^\rho_t$ & Game reward - mean-field induced & Infinite\\
      $J^{N}$ /$J^{\rho}$ & Team game value & Finite / Infinite\\
      $J^\rho_\cor$ & Coordinator game value & Infinite\\
      $\R^\rho_{\mu}$ / $\R^\rho_{\nu}$ & Reachablity correspondence for Blue/Red team mean-field  \qquad  & Infinite \\
      \bottomrule % <-- Bottomrule here
    \end{tabular}
  \end{center}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Related Work}

\paragraph{Mean-field games.}

The mean-field game model was introduced in~\citep{huang2006large, huang2007large, lasry2007mean} to address the scalability issues in large population games. 
The salient feature of  mean-field games is that \textit{selfish} agents are weakly-coupled in their dynamics and rewards only through the distribution of all the agents. 
If the population is sufficiently large, then an approximately optimal solution for these models can be obtained by solving the infinite population limit, and the solution in this limiting case is the mean-field equilibrium (MFE). 
The infinite-population game is easier to solve since, when the population is asymptotically large, the action of a single agent does not affect the mean-field. 
In the continuous setting, the MFE is characterized by a Hamilton-Jacobi-Bellman equation (HJB) coupled with a transport equation~\citep{lasry2007mean}. 
The HJB equation describes the optimality conditions for the strategy of the representative agent, and the transport equation captures the evolution of the mean-field. 
The existence and uniqueness of the MFE have been established in~\citep{huang2006large}.
Two desirable features of the MFE are:
(i) the resultant strategy is decentralized and identical across all agents, i.e., an agent selects its action only based on the local information of its own state and no information regarding the mean-field is needed; 
and (ii) the complexity of the MFE solution does not depend on the number of controllers. 
The mean-field results have been extended to discrete environments in~\citep{cui2021approximately,guan2022shaping}, using entropy-regularization.
We refer the readers to~\citep{lauriere2022learning} for a detailed overview of the main results in the mean-field game literature. 

The main differences between our setup and the current mean-field game literature are the following:
(i) we seek team-optimal control strategies, while MFG seeks Nash equilibrium strategies. 
In particular, we provide performance guarantees when the whole opponent team deviates, while MFG only considers single-agent deviation.
(ii) we assume that each agent observes the mean-fields of both teams, while MFG model does not make this assumption. 
The MFE works without observation on the mean-field since it assumes all agents apply the same strategy and when the number of agents goes to infinity, the mean-field becomes a deterministic process. 
However, due to the lack of knowledge on the opponent team strategy, we need the feedback on the mean-fields in this work to adapt to strategies deployed by the opponent team.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Markov team games.}

Different from mean-field games, team problems focus on fully cooperative scenarios, where all agents share a common reward function.
From a game-theoretic perspective, this cooperative setting can also be viewed as a special case of Markov potential games~\citep{zazo2016dynamic}, with the potential function being the common rewards. 
When all agents have access to the present and past observations along with the past actions of all other agents, such a system
is equivalent to a centralized one, which thus enables the use of single-agent algorithms, such as value iteration or Q-learning~\citep{bertsekas1996neuro}.
%
However, such a centralized system suffers from scalability issues, since the joint state and action spaces grow exponentially with the number of agents. 
Furthermore, in many applications, decentralization is desirable or even required, due to reasons such as limited communication bandwidth, privacy, etc.
This work, therefore, investigates the decentralized team control problem under the information structure known as the mean-field sharing~\citep{arabneydi:2014}.

Information structure is one of the most important characteristics of a multi-agent system, since it largely determines the tractability of the corresponding decentralized decision problem~\citep{4048470,ho1980team}.
In~\citep{witsenhausen1971separation}, information structures are classified into three classes: classical, quasi-classical and non-classical.
The characterizing feature of the classical information structure is centralization of information, i.e., all agents know the information available to all agents acting in the past. 
A system is called quasi-classical or partially nested, if the following condition holds: 
If agent $i$ can influence agent $j$, then agent $j$ must know the information available to agent $i$.
All other information structures are non-classical. 
In the team  gameliterature, information structures that are commonly seen include 
state sharing~\citep{aicardi1987decentralized}, 
belief sharing~\citep{yuksel2009stochastic}, 
partial-history sharing~\citep{nayyar2013decentralized}, 
mean-field sharing~\citep{arabneydi:2014}, etc. 
This work, in particular, assumes a mean-field sharing structure, where each agent observes its local state and the mean-fields of both teams. 
As the agents do not receive information regarding the other agents' individual states, the mean-field sharing is a non-classic information structure,  thus posing a challenge to attain tractable solutions.

As decentralized team problems belongs to the NEXP complexity class~\citep{bernstein2002complexity},
no efficient algorithm is available, in general. 
Nonetheless, it is possible to develop a dynamic programming decomposition for specific information structures. 
Three such generic approaches are discussed in~\citep{mahajan2012information}: namely,
person-by-person approach,
designer's approach,
and common-information approach. 
While we discuss in detail the common-information approach next, we refer the interested reader to~\citep{mahajan2012information} for the other two approaches.

Team problems in a mixed competitive-cooperative setting have also been considered.
For example, \citep{lagoudakis2002learning} proposed a centralized learning algorithm for zero-sum Markov games in which each side is composed of multiple agents collaborating against an opponent team of agents. 
Later, in~\citep{hogeboom2023zero, sanjari2022nash}, the authors studied the information structure and the existence of equilibria in games involving teams against teams. 
Most relevant to this work, \citep{Kartik2021} considered a general model of zero-sum stochastic games between two competing teams, where the information available to each agent can be decomposed into common and private information. 
Exploiting this specific information structure, an expanded game between two virtual players (fictitious coordinators) is constructed based on the sufficient statistics known as the common information belief. 
The authors showed that the expanded game can be solved via dynamic programming, and the upper and lower values of the expanded games provide bounds for the original game values.
Although we utilize the similar common information approach in~\citep{Kartik2021}, this work focuses more on the large-population aspect of the problem, i.e., the mean-field limits of the team games, the performance guarantees under the mean-field approximation, etc.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Common-information approach.} 

The common information-based approach was proposed in~\citep{nayyar2013decentralized} to investigate optimal strategies in decentralized stochastic control problems with partial history-sharing information structure. 
In this model, each agent shares part of its information with other agents at each time step. Based on the common information available to all agents, a fictitious coordinator is designed. It is shown that the optimal control problem at the coordinator level is a centralized Partially Observable Markov Decision Process (POMDP) and can be solved using dynamic programming. The coordinator solves the POMDP problem and chooses prescriptions that map each agent's local information to its actions. The common information approach was used to solve decentralized stochastic control problems including the control sharing information structure~\citep{mahajan2011}, mean-field teams~\cite{arabneydi:2014}, and Linear Quadratic Gaussian (LQG) systems~\citep{mahajan2015linear}.

In this work, we treat the mean fields of both (infinite-population) teams as common information. 
Instead of having a single coordinator, we introduce two fictitious coordinators, one for each team, and, consequently, formulate an equivalent zero-sum game between the two coordinators. 
Since we consider the equivalent coordinator game at the infinite-population limit, the two mean fields provide enough information to characterize the status of the system under the mean-field dynamics, which leads to a full state information structure instead of a partial observability information structure.
Although the original environment is discrete, the resultant zero-sum coordinator game has continuous state space (the mean fields are continuous distributions) and continuous action spaces (the prescriptions assigned by the coordinators consist of distribution over action space).
Since the focus of this work is the theoretical analysis of the problem, the numerical solution to such continuous zero-sum games is left for future work.


\paragraph{Mean-field teams.}

The theory of teams was first investigated in~\citep{marschak1955elements,radner1962team}. In such problems, there are multiple agents that have access to different information but try to minimize a common cost function. 
The mean-field team formulation~\citep{arabneydi:2014} considers the optimal stochastic problem in a team with a large population of agents. 
It transforms the team problem into a hierarchical control structure, where a fictitious coordinator broadcasts a prescription based on the distribution of agents, and the individual agents then act according to this prescription.


% \paragraph{Population dynamics}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}

We consider a discrete-time system with two \textit{large} teams of agents that operate over a finite time horizon $T\in \mathbb{N}$. 
The Blue Team has $N_1$ homogeneous agents, and the Red team has $N_2$ homogeneous agents. 
%
The state of Blue agent $i \in \{1,\ldots, N_1\}$, at time $t$, is denoted by $x^{N_1}_{i,t} \in \X$, where $\X$ is the finite individual state space for the Blue Team (independent of $i$ and $t$). 
Let $u^{N_1}_{i,t} \in \U$ denote the action taken by Blue agent $i$ at time $t$, where $\U$ is the finite individual action space for the Blue Team.
Similarly, we use $y^{N_2}_{j,t} \in \Y$ and $v^{N_2}_{j,t} \in \V$ to denote the individual state and action of Red agent $j \in \{1,\ldots, N_2\}$.
%
The joint state of the Blue Team is denoted as $x^{N_1}_t = \big(x^{N_1}_{1,t},\ldots, x^{N_1}_{N_1,t}\big)$. 
We use the short hand notation $x^{N_1}_{-i,t} = \big(x^{N_1}_{1,t},\ldots,x^{N_1}_{i-1,t},x^{N_1}_{i+1,t}, \ldots, x^{N_1}_{N_1,t}\big)$ to denote the joint state of the Blue Team except Blue agent $i$.
Similarly, the joint control of the Blue Team is denoted as $u^{N_1}_t = \big(u^{N_1}_{1,t},\ldots, u^{N_1}_{N_1,t}\big)$.
The notation extends to the Red team's joint state $y^{N_2}_t$ and joint control action $v^{N_2}_t$.
% The state of the whole system is then given by
% \begin{equation*}
%     \mathbf{z}_t^{N_1,N_2} = (\x^{N_1}_t,\y^{N_2}_t) \in \X^{N_1} \times \Y^{N_2}.
% \end{equation*}

\paragraph{Coupled dynamics.}

The response of each individual agent is coupled with the response of the other agents through its dynamics.
We use $f^i_t\big(x^{N_1}_{i,t+1}|x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{-i,t}, y^{N_2}_{t})$ to denote the probability of Blue agent $i$ transitioning to state $x^{N_1}_{i,t+1}$ given its current state $x^{N_1}_{i,t}$, its current action $u^{N_1}_{i,t}$, all other Blue agents' states $x^{N_1}_{-i,t}$ and the Red team  state $y^{N_2}_{t}$. 
Similarly, the transition function for a Red agent $j$ is given by $g^j_t\big(y^{N_2}_{j,t+1} | y^{N_2}_{j,t}, v^{N_2}_{j,t}, y^{N_2}_{-j,t}, x^{N_1}_{t}\big)$.

\vspace{+5pt}

Similar to the standard assumptions made in the mean-field game literature~\citep{huang2006large}, we start with a pair-wise weakly coupled dynamics as follow
\begin{subequations}
\label{eqn:weakly-dynamics}
\begin{align}
    f^{i,N}_t\big(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{-i,t}, y^{N_2}_{t}) &= 
    \frac{ \sum_{k=1}^{N_1}f_{1,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{k,t} )}{N} +
    \frac{ \sum_{k=1}^{N_2}f_{2,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y^{N_2}_{k,t} )}{N},
    \\
    g^{j,N}_t\big(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, y^{N_2}_{-j,t}, x^{N_1}_{t}\big) &=  
    \frac{\sum_{k=1}^{N_1}g_{1,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, x^{N_1}_{k,t} )}{N}  +
    \frac{\sum_{k=1}^{N_2}g_{2,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t},  y^{N_2}_{k,t} )}{N}.
\end{align}
\end{subequations}
where $N = N_1 + N_2$.

% To ensure that we end up with valid transition probabilities, we define the weakly coupled dynamics as 
% \begin{equation}
% \label{eqn:weakly-dynamics}
% \begin{aligned}
%     f^{i,N}_t\big(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, \x^{N_1}_{-i,t}, \y^{N_2}_{t}) &= \frac{\bar{f}^{i,N}_t\big(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, \x^{N_1}_{-i,t}, \y^{N_2}_{t})}{
%     \sum_{x}\bar{f}^{i,N}_t\big(x\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, \x^{N_1}_{-i,t}, \y^{N_2}_{t})
%     },
%     \\
%     g^{j,N}_t\big(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, \y^{N_2}_{-j,t}, \x^{N_1}_{t}\big) &= \frac{ \bar{g}^{j,N}_t\big(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, \y^{N_2}_{-j,t}, \x^{N_1}_{t}\big)}{
%      \sum_{y}\bar{g}^{j,N}_t\big(y \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, \y^{N_2}_{-j,t}, \x^{N_1}_{t}\big)
%     },
% \end{aligned}
% \end{equation}

To ensure that $f^i_t$ and $g^j_t$ are valid transition functions, we require that $f_{1,t}, f_{2,t}, g_{1,t},$ and $g_{2,t}$ are themselves valid transition functions. 
For example, we require that $\sum_{x\in \X} f_{1,t}(x\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{k,t} ) = 1$ for all $t \leq T$, $x^{N_1}_{i,t}, x^{N_1}_{k,t} \in \X$ and $u^{N_1}_{i,t} \in \U$.
Then, we have
\begin{align*}
    \sum_{x \in \X} f^i_t\big(x \big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{-i,t}, y^{N_2}_{t})
    &= \frac{ \sum_{k=1}^{N_1} \sum_{x} f_{1,t}(x\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{k,t} )}{N} +
    \frac{ \sum_{k=1}^{N_2} \sum_{x} f_{2,t}(x\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y^{N_2}_{k,t} )}{N} \\
    &= \frac{N_1 +N_2}{N} = 1.
\end{align*}

In the following discussions, we will use bold font to represent random variables and  normal font to represent their corresponding realization. 
For example, $\x_{i,t}^{N_1}$ is the random variable that represents the state of Blue agent $i$ at time step $t$, while  $x_{i,t}^{N_1}$ is a realized value of the random variable $\x_{i,t}^{N_1}$.
% We use the notation $x \sim p(\cdot)$ to denote that the value $x$ is draw from the distribution $p$. 
% With a slight abuse of notation, we use $\x \sim p(\cdot)$ to denote that the random variable $\x$ has distribution $p$.
We use $\mathcal{F}_t$ to denote the sigma-algebra generated by the joint state and action random variables $\{\x^{N_1}_0, \y^{N_2}_0,\u^{N_1}_0,\v^{N_2}_0, \ldots, \x^{N_1}_t, \y^{N_2}_t\}$.
Similarly, $\mathcal{F}_t^+$ denotes the sigma algebra generated by $\{\x^{N_1}_0, \y^{N_2}_0,\u^{N_1}_0,\v^{N_2}_0, \ldots, \x^{N_1}_t, \y^{N_2}_t, \u^{N_1}_t,\v^{N_2}_t\}$.
Specifically, $\F_t$ captures the system history up to the state realization at time step $t$, while $\F_t^+$ further includes the action realization at $t$.


\begin{remark}
    Under the above definition of the random variables, the dynamics in~\eqref{eqn:weakly-dynamics} can be interpreted as follows.
    \begin{equation}
        \mathbb{P}\big(\x^{N_1}_{i,t+1} = x^{N_1}_{i,t+1}\big \vert \x^{N_1}_{i,t} = x^{N_1}_{i,t}, \u^{N_1}_{i,t} = u^{N_1}_{i,t}, \x^{N_1}_{-i,t}=x^{N_1}_{-i,t}, \y^{N_2}_{t}=y^{N_2}_{t})=f^{i,N}_t\big(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{-i,t}, y^{N_2}_{t})
    \end{equation}
\end{remark}

It is obvious that the team with more agents has a larger influence on the dynamics.
Consequently, when we perform the mean-field approximation (taking $N_1$ and $N_2$ to infinity), we need to preserve the ratio between $N_1$ and $N_2$ to properly represent the mean dynamics. 
Therefore, we introduce the factor $\rho$ to capture the ratio between the size of the two populations. 
Formally, 
\begin{equation}
    \label{eqn:rho-def}
    \rho = \frac{N_1}{N}.
\end{equation}

Note that the weakly coupled dynamics in~\eqref{eqn:weakly-dynamics} is indifferent to the ordering of the entries in $x_{t}^{N_1}$ and $y_{t}^{N_2}$. 
Furthermore, we assume  that all agents share the same structure for the weakly-coupled dynamics.
Consequently, agents within each team are indistinguishable from each other in terms of their individual dynamics.
Henceforth, we will drop the superscripts $i$ and $j$ and simply use $f^N_t$ and $g^N_t$ to denote the individual dynamics of the Blue and the Red agents within a finite $N$-agent system.


\paragraph{Team rewards.}

In a team game setting, agents in the same team share the same reward. 
In other words, an agent strives for the interest of its team instead of its own benefit.  
%
In our formulation, the two teams are competing (zero-sum), while within each team, the agents collaborate and receive the same team rewards.
Similar to the dynamics, we consider weakly-coupled \textit{team rewards} that are indifferent to the ordering of the agents in the same team,
\begin{equation}\label{eqn:team-rewards}
    r_t^{N}(x_t^{N_1},y_t^{N_2}) = \frac{1}{N_1} \sum_{k=1}^{N_1}r_{1,t}(x^{N_1}_{k,t} ) - 
    \frac{1}{N_2} \sum_{k=1}^{N_2}r_{2,t}(y^{N_2}_{k,t} ).
\end{equation}

We let the Blue team be the \textit{maximizing team}, aiming to maximize the \textit{expected cumulative rewards}.
The Red team is then assigned as the \textit{minimizing team}. 

\begin{remark}
    Note that in~\eqref{eqn:team-rewards} we use $N_1$ and $N_2$ as the normalizing factors. 
    However, the results derived later can be easily extended to normalization with $N = N_1 + N_2$ .
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Empirical distributions.}

Since the population within each team is homogeneous and the dynamics in~\eqref{eqn:weakly-dynamics} and the rewards in~\eqref{eqn:team-rewards} do not differentiate between agents, 
the ordering of the agents is irrelevant to the agent's decision making. 
% For example, when a Blue agent $i$ is making its decision, the following two scenarios are equivalent: (i) Blue agent $1$ on state $x_3$ and Blue agent $2$ on state $x_4$, and (ii) Blue agent $2$ on state $x_3$ and Blue agent~$1$ on state $x_4$. 
Consequently, one can discard the information identifying each agent.  


To remove irrelevant information, we leverage the \textit{empirical distribution} to characterize the system in a tractable manner. 
We denote the empirical distribution of the states of all agents in the Blue team and the Red team at time $t$ as $\bm{\mu}^{N_1}_t$ and $\bm{\nu}^{N_2}_t$, respectively.

%\clearpage

\begin{definition}  
    \label{def:empirical-dist}
    The \textit{empirical distribution} for the Blue and Red teams are defined, respectively, as
    \begin{subequations}\label{eqn:mean-field}
        \begin{align}
            \bm{\mu}^{N_1}_t(x) &= \frac{1}{N_1} \sum_{k=1}^{N_1} \mathds{1}_x(\x^{N_1}_{k,t}), ~~~ x \in \X,\\
            \bm{\nu}^{N_2}_t(y) &= \frac{1}{N_2} \sum_{k=1}^{N_2} \mathds{1}_y(\y^{N_2}_{k,t}), ~~~ y \in \Y,
        \end{align}
    \end{subequations}
    where $\mathds{1}_{a}(\cdot)$ is the indicator function.
\end{definition}

We have that $\bm{\mu}^{N_1}_t \in \mathcal{P}(\X)$ and $\bm{\nu}^{N_2}_t \in \mathcal{P}(\Y)$ almost surely, where $\mathcal{P}(\X)$ contains all probability measures over the space $\X$, and $\P(\Y)$ is similarly defined. 
%
Since the state spaces $\X$ and $\Y$ are finite, we have $\P(\X) = \Delta_{|\X|}$ and $\P(\Y) = \Delta_{|\Y|}$, where $\Delta_n$ is the $n$-dimensional standard simplex.
The \textit{empirical distribution flows} of the Blue and Red teams are then defined as $\bm{\mu}^{N_1} = \{\bm{\mu}^{N_1}_t\}_{t=0}^T$ and $\bm{\nu}^{N_2} = \{\bm{\nu}^{N_2}_t\}_{t=0}^T$ respectively.
Furthermore, we have that $\bm{\mu}^{N_1}_t \in \F_t$ and $\bm{\nu}^{N_2}_t \in \F_t$, that is, the random variables $\bm{\mu}^{N_1}_t$ and $\bm{\mu}^{N_2}_t$ are $\F_t$-measurable.
The empirical distribution captures the fraction of the (finite) population of agents at each state and at each timestep.
%
Since the transition dynamics of each individual agent is stochastic, the empirical distributions $\bm{\mu}_t^{N_1}$ and $\bm{\nu}_t^{N_2}$ are random variables.
One can view the empirical distribution of a team as an \textit{aggregated state} of the actual team state $\x^{N_1}_t$ or $\y^{N_2}_t$ at time $t$, with the information on the agents' ordering removed.
% However, due to the stochastic nature of the empirical distribution, its exact distribution may be hard to propagate over time, especially when the number of agents get large. 
% The difficulty of propagating the distribution motivates the mean-field approximation, which will be introduced in the next section.

We use the total variation $\mathrm{d}_{\mathrm{TV}}$ as the norm in the spaces of the probability measures $\P(\X)$ and $\P(\Y)$~\citep{chung2001course}.
Since $\X$ and~$\Y$ are finite state spaces, the total variation is directly related to the 1-norm. 
\begin{definition}
    \label{def:dtv}
    For a finite space $\X$, the total variation between two probability measures $\mu_t, \mu_t' \in \P(\X)$ is given by
    \begin{equation}
        \dtv{\mu_t, \mu_t'} = \frac{1}{2}\sum_{x \in \X} \abs{\mu_t(x) - \mu'_t(x)}.
    \end{equation}
\end{definition}

Using the empirical distributions in \eqref{eqn:mean-field}, one can equivalently express the weakly-coupled dynamics in~\eqref{eqn:weakly-dynamics} as
\begin{align}
f_t &\big(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{-i,t}, y^{N_2}_{t})   \label{eqn:dynamics-1}\\
&= \frac{N_1}{N}\frac{ \sum_{k=1}^{N_1}f_{1,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x^{N_1}_{k,t} )}{N_1} +
    \frac{N_2}{N}\frac{ \sum_{k=1}^{N_2}f_{2,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y^{N_2}_{k,t} )}{N_2} \nonumber\\  
        &= \frac{N_1}{N}\frac{ \sum_{k=1}^{N_1}\sum_{x\in\X} \indicator{x}{x^{N_1}_{k,t}}f_{1,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x )}{N_1} +
    \frac{N_2}{N}\frac{ \sum_{k=1}^{N_2}\sum_{y\in\Y} \indicator{y}{y^{N_2}_{k,t}}f_{2,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y )}{N_2} \nonumber \\   
    & =
    \rho\sum_{x\in \X}\mu^{N_1}_t(x) f_{1,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, x ) +
    (1-\rho)\sum_{y\in \Y} \nu_t^{N_2}(y)f_{2,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y ) \nonumber\\
    &= f^\rho_t\big(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t},  \mu^{N_1}_t, \nu^{N_2}_t) ,   \nonumber
\end{align}
where $\rho = {N_1}/{N}$.
Similarly, for the Red team, we have
\begin{equation}
\label{eqn:dynamics-2}
\begin{aligned}
g_t\big(y^{N_2}_{j,t+1} \big \vert &y^{N_2}_{j,t}, v^{N_2}_{j,t}, y^{N_2}_{-j,t}, x^{N_1}_{t}\big)
    =g^{\rho}_t\big(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, \mu^{N_1}_t, \nu^{N_2}_t\big) \\
    &=  
    \rho\sum_{x\in \X}\mu^{N_1}_t(x)
    g_{1,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, x )  +
    (1-\rho)\sum_{y\in \Y} \nu_t^{N_2}(y)g_{2,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t},  y ).
\end{aligned}
\end{equation}

Note that the team with more agents has a larger influence on the dynamics.
Similarly, the team reward in~\eqref{eqn:team-rewards} can be equivalently expressed as 
\begin{equation}\label{eqn:mf-team-rewards}
\begin{aligned}
   r_t^{N}(x_t^{N_1},y_t^{N_2}) &= \frac{1}{N_1} \sum_{k=1}^{N_1}r_{1,t}(x^{N_1}_{k,t} ) - 
    \frac{1}{N_2} \sum_{k=1}^{N_2}r_{2,t}(y^{N_2}_{k,t} ) \\
    &= \sum_{x\in\X}\mu^{N_1}_t(x)\; r_{1,t}(x) - 
    \sum_{y\in \Y} \nu^{N_2}_t(y) \;r_{2,t}(y) =  r_t^{\rho}(\mu_t^{N_1},\nu_t^{N_2}).
    \end{aligned}
\end{equation}

One can now clearly see that the empirical distribution of each team serves as an aggregated state of that team. 
At the team level, there is no need to keep track of which specific agent is at which state; instead, keeping track of what fraction of the population is at each state is enough to characterize the team.
In the literature, such a system is commonly referred to as a mean-field coupled system~\citep{lauriere2022learning}.

\begin{remark}
    It has been shown in~\citep{arabneydi2017new} that any controlled Markov chain with partially exchangeable agents, irrespective of the information structure, is equivalent to a mean-field coupled system with the same information structure.
    Consequently, one can generalize the dynamics and rewards in~\eqref{eqn:weakly-dynamics} and~\eqref{eqn:team-rewards} by assuming exchangibility of agents in each team and still arrive at the mean-field coupled system in~\eqref{eqn:dynamics-1} and~\eqref{eqn:mf-team-rewards}.
\end{remark}




\paragraph{Control strategies.}
We assume a mean-field sharing information structure as follows~\citep{arabneydi2015team}. 
At each time step $t$, Blue agent $i$ observes its own state $x_{i,t}^{N_1}$ and the empirical distributions of both teams $\bm{\mu}_t^{N_1}$ and $\bm{\nu}_t^{N_2}$.
Similarly, Red agent $j$ observes $y_{j,t}^{N_2}$, $\bm{\mu}_t^{N_1}$ and $\bm{\nu}_t^{N_2}$.
We further assume that these observations are perfect, i.e., there is no measurement noise. 

Since the empirical distribution can be viewed as an aggregated state of the teams, 
we consider the following mixed Markov policy for the Blue agent $i$:
\begin{equation}
    \label{eqn:ed-blue-policy}
    \phi^{N_1}_{i,t}: \U \times \X \times \P(\X) \times \P(\Y) \to [0,1],
\end{equation}
where $\phi^{N_1}_{i,t}(u|x^{N_1}_{i,t}, \mu^{N_1}_t,\nu^{N_2}_t)$ represents the probability that agent $i$ selects action $u$ given its state $x_{i,t}^{N_1}$ and the team distributions $\mu^{N_1}_t$ and $\nu^{N_2}_t$. 
We say that a policy is admissible if it satisfies $\sum_{u \in \U}\phi^{N_1}_{i,t}(u|x^{N_1}_{i,t}, \mu^{N_1}_t,\nu^{N_2}_t) = 1$ for all $x^{N_1}_{i,t} \in \X$, $\mu^{N_1}_t \in \P(\X)$ and $\nu^{N_2}_t \in \P(\Y)$.
A Blue individual strategy $\phi^{N_1}_{i}$ is then defined as a time sequence $\phi^{N_1}_{i} = \{\phi^{N_1}_{i, t}\}_{t=0}^T$.
We denote the set of admissible Blue individual strategies as $\Phi_t$, and the admissible set of Blue strategies is denoted as $\Phi$.

The policy for Red agent $j$ is similarly defined as 
\begin{equation}
    \label{eqn:ed-red-policy}
    \psi^{N_2}_{j,t}: \V \times \Y \times \P(\X) \times \P(\Y) \to [0,1],
\end{equation}
and the Red individual strategy is denoted as a time sequence $\psi^{N_2}_{j} = \{\psi^{N_2}_{j, t}\}_{t=0}^T$.
The admissible policy and strategy sets for the Red agents are denoted as $\Psi_t$ and $\Psi$, respectively.


We refer to the collection $\phi^{N_1} = \{\phi^{N_1}_i\}_{i=1}^{N_1}$ as the  Blue team strategy consisting of strategies used by each Blue agent.
We denote the set of admissible Blue team strategies as $\Phi^{N_1}$. 
The Red team strategy $\psi^{N_2}$ and its admissible set $\Psi^{N_2}$ are defined similarly. 


\begin{definition}
    \label{def:identical-policy}
    We say that the Blue team applies an identical team strategy if the strategies used by all Blue agents are identical, i.e., $\phi^{N_1}_{i_1,t} = \phi^{N_1}_{i_2,t}$ for all $i_1,i_2 \in \{1,\ldots,N_1\}$ and all $t \in \{0, \ldots, T-1\}$. 
    With a slight abuse of notation, we denote both the identical Blue team strategy and the identical strategy used by each Blue agent as $\phi$. 
    We further denote the admissible set of identical Blue team strategies as $\Phi$.
\end{definition}

Similarly, when the Red team applies an identical team strategy, we denote both the Red team policy and the identical Red agent strategy as $\psi$, and the admissible set of identical Red team strategies is denoted as $\Psi$.

Due to symmetry, in the rest of the paper we analyze the game \emph{from the perspective of the Blue team}, and we make the following assumption for the Blue team.
\begin{assumption}\label{assm:ident-control-blue}
The Blue team applies an identical team strategy. 
\end{assumption}


In general, Assumption~\ref{assm:ident-control-blue} may result in degraded performance~\citep{arabneydi:2014}.
However, it is a standard approach taken in the literature for large-scale systems for reasons of simplicity, fairness, and robustness~\citep{shi:2012,wu:2010}.
Furthermore, since the Blue team is ``our" team, we have complete control over its strategy.
Consequently, we regard Assumption~\ref{assm:ident-control-blue} as justified.
The main question is, whether we can make a similar assumption for the opponent team to further simplify the problem.


\paragraph{Expected rewards.}
We use the expected rewards as the metric to evaluate each team's performance. 
Assuming that the Blue team deploys an identical team strategy $\phi \in \Phi$ and the Red team deploys an \emph{arbitrary} strategy $\psi^{N_2} \in \Psi^{N_2}$, the whole system becomes a controlled Markov process. 
We define the expected cumulative reward as the expectation over the sample space of the empirical distribution trajectories under the given strategies. Specifically,
\begin{equation}\label{eqn:expected-cum-reward}
    J^{N, \phi, \psi^{N_2}} \big(\mu^{N_1}_0, \nu^{N_2}_0 \big) = 
    \expct{
    \sum_{t=0}^{T} r^{\rho}_t(\bm{\mu}^{N_1}_t, \bm{\nu}^{N_2}_t)|\bm{\mu}^{N_1}_0=\mu^{N_1}_0, \bm{\nu}^{N_2}_t= \nu^{N_2}_0
    }{\phi, \mathbf{\psi}^{N_2}}.
\end{equation}

As the maximizing team, the Blue team aims to maximize the expected reward in~\eqref{eqn:expected-cum-reward} by selecting the team strategy $\phi \in \Phi$.
The Red team minimizes through the team strategy $\psi^{N_2} \in \Psi^{N_2}$.
When the Blue team considers its worst-case performance, we have the following max-min optimization problem:
\begin{equation}\label{eqn:origin-optimization}
    \underline{J}^{N*} = \max_{\mathbf{\phi} \in \Phi} ~ \min_{\mathbf{\psi}^{N_2} \in \Psi^{N_2}} ~~ J^{N, \phi, \psi^{N_2}},
\end{equation}

where $\underline{J}^{N*}$ is the lower game value for the finite-population game.
Since the game value may not always exist (maximin differs from minimax)~\citep{elliott1972existence, fleming1989existence}, instead of considering equilibrium solutions, we consider the following optimality condition for the Blue team strategy. 
\begin{definition}
    \label{def:team-optimal}
    An identical Blue team strategy $\phi^* \in \Phi$ is optimal if
    \begin{equation}
        \underline{J}^{N*}  = \min_{\mathbf{\psi}^{N_2} \in \Psi^{N_2}} J^{N, \phi^*, \psi^{N_2}}.
        % \geq \min_{\mathbf{\psi}^{N_2} \in \Psi^{N_2}} J^{N, \phi, \psi^{N_2}}
        % \qquad \forall \; \phi \in \Phi.
    \end{equation} 
\end{definition}
In other words, the optimal Blue team strategy $\phi^*$ guarantees the highest worst-case performance against any Red team strategy.
A relaxed version of the above optimality condition is the $\epsilon$-team optimality. 
\begin{definition}
    \label{def:eps-team-optimal}
    An identical Blue team strategy $\phi^* \in \Phi$ is $\epsilon$-optimal if
    \begin{equation}
        \underline{J}^{N*}  \geq \min_{\mathbf{\psi}^{N_2} \in \Psi^{N_2}} J^{N, \phi^*, \psi^{N_2}} \geq \underline{J}^{N*} - \epsilon.
    \end{equation} 
\end{definition}



After making the identical-strategy assumption for the Blue team, one natural question is whether one can make the same assumption on the Red team to further simplify the problem. 
As discussed in the introduction, it is not clear whether such an assumption is justified, since one does not have knowledge of the opponent team's strategy structure. 
However, we will show that such assumption on the opponent team is justified in the zero-sum mean-field team setting. 
In particular, we will construct an identical Blue team strategy $\phi^* \in \Phi$ under the assumption that the opponent team deploys an identical strategy $\psi \in \Psi$, and we will show that $\phi^*$ is still an $\epsilon$-optimal Blue team strategy (against any opponent team strategy $\psi^{N_2} \in \Psi^{N_2}$). 
Furthermore, the suboptimality diminishes as the populations of both teams approaches infinity. 
The key step toward the desired result is the mean-field approximation, which is discussed in the next section.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Road map of the solution approach}

\section{Mean-Field Approximation}

In the previous sections, we have shown that the weakly-coupled dynamics provides a potential to describe the population dynamics as in~\eqref{eqn:dynamics-1} and~\eqref{eqn:dynamics-2}, by expressing the dynamics with the coupling terms on the empirical distributions.
However, the empirical distributions are sequences of random vectors over time, which could be difficult to simulate or compute analytically, in general. 
Therefore, there is a motivation to consider the limiting case, when the number of agents of both teams goes to infinity.
With an infinite population, the empirical distributions become deterministic due to the law of large numbers~\citep{cui2021approximately}. 
The deterministic distribution of the infinite population is referred to as the \textit{mean-field}, and using the mean-field to approximate the empirical distribution of a finite system is known as the mean-field approximation~\citep{huang2006large}.
The mean-field approximation is motivated by the following intuition from statistical mechanics: as the number of particles grows to infinity, the particles become essentially indistinguishable from one another while each individual particle becomes negligible. 
%
Consequently, a single particle's statistical properties can effectively represent and approximate the empirical distribution produced by all particles.
Similarly, in the mean-field game literature, the empirical distribution of agents in a large population can be approximated by a \emph{typical agent}'s state distribution. 

In this section, we extend the mean-field ideas to the zero-sum mean-field team game setting, and formulate the mean-field team game at the infinite-population limit. 
Following the standard mean-field approach, we further restrict our attention to the case when the Red team applies an identical team strategy.
We will show in Section~\ref{sec:performance} that restricting the opponent's strategy to identical team strategies only induces a small decrease in the Blue team's performance, even when the Blue team faces a Red team that does not use an identical strategy.

\paragraph{Mean-field.}
% When the number of agents in \textit{both} teams approaches infinitely, the limiting game constitutes the mean-field zero-sum team game.
% Since the ratio $\rho = \frac{N_1}{N}$ plays a significant role in the dynamics $f_t^\rho$ and $g_t^\rho$, we preserve this ratio when taking the limit.  
The mean-field of the Blue team is defined as the empirical distribution of an infinite Blue population. 
We denote the mean-field of the Blue team at time $t$ as $\mu^\rho_t$. 
The mean-field of the Red team is similarly defined, and it is denoted as $\nu^\rho_t$.
Since the ratio $\rho = N_1 / N$ plays a significant role in the dynamics $f_t^\rho$ and $g_t^\rho$, we preserve this ratio when constructing the infinite-population game.  



At the infinite population limit, we only consider Blue and Red teams that apply \textit{identical team strategies}.
To differentiate from the identical team strategies $\phi$ and $\psi$ used in the finite-population games, the identical strategies used by the two infinite-population teams are denoted as $\phi^\rho$ and $\psi^\rho$, respectively. Formally, the policies at each time step are mappings that take the form of 
\begin{subequations}
\label{eqn:mf-policy}
\begin{align}
    \label{eqn:mf-blue-policy}
    \phi^{\rho}_{t}: \U \times \X \times \P(\X) \times \P(\Y) \to [0,1],\\
    \label{eqn:mf-red-policy}
    \psi^{\rho}_{t}: \V \times \Y \times \P(\X) \times \P(\Y) \to [0,1],
\end{align}
\end{subequations}
such that $\phi^{\rho}_{t}(u_t|x_t,\mu^\rho_t, \nu^\rho_t)$ gives the probability that a typical Blue agent would select action $u_t$ given its current state $x_t$ and the mean-fields $\mu^\rho_t$ and  $\nu^\rho_t$.

% Due to the law of large number, the mean-field of a team coincides with the state distribution of a typical agent, also known as the representative agent. 
% Consequently, we define the mean-field as follows. 

Formally, we define the mean-field flow as a deterministic sequence of distributions that coincides with the state distribution of a \textit{typical agent}. 
\begin{definition}
    The mean-field flows of the two teams $\mu^\rho = \{\mu^\rho_t\}_{t=0}^T$ and $\nu^\rho= \{\nu^\rho_t\}_{t=0}^T$ are deterministic time sequences. 
    Under the identical control strategies $\phi^\rho$ and $\psi^\rho$, the mean-field flows propagate according to the following coupled deterministic dynamics:
    \begin{subequations}
    \label{eqn:mf-dynamics}
    \begin{align}
        \mu^\rho_{t+1} (x') &= \sum_{x \in \X} \left[\sum_{u \in \U} f^\rho_t(x'|x, u, \mu_t, \nu_t )\; \phi^\rho_t(u|x,\mu^\rho_t, \nu^\rho_t) \right] \mu^\rho_t(x), \\
        \nu^\rho_{t+1} (y') &= \sum_{y \in \Y} \left[\sum_{v \in \V}g^\rho_t(y'|y, v, \mu^\rho_t, \nu^\rho_t ) \; \psi^\rho_t(v|y,\mu^\rho_t, \nu^\rho_t)\right] \nu^\rho_t(y),
    \end{align}
\end{subequations}
where the initial distributions $\mu^\rho_0$ and $\nu^\rho_0$ are assumed to be given.
\end{definition}

The above deterministic mean-field dynamics can be expressed in a compact matrix form as 
\begin{equation}
    \begin{aligned}
        \label{eqn:coordinator-dynamics}
        \mu^\rho_{t+1} &= \mu^\rho_{t} F_t^{\rho}(\mu^\rho_t, \nu^\rho_t,\phi^\rho_t), \\
        \nu^\rho_{t+1} &= \nu^\rho_t G^\rho_t(\mu^\rho_t, \nu^\rho_t,\psi^\rho_t),
    \end{aligned}
\end{equation}
where the matrix $F_t^\rho \in \mathbb{R}^{|\X| \times |\X|}$ is the transition matrix for a typical Blue agent's distribution under the policy $\phi^\rho_t$, with its entries given by $\left[F_t^{\rho}(\mu^\rho_t, \nu^\rho_t,\phi_t)\right]_{pq} = \sum_{u \in \U}f_t(q|p, u, \mu^\rho_t, \nu^\rho_t ) \; \phi_t(u|p, \mu_t^\rho, \nu_t^\rho)$. 
The matrix $G_t^\rho$ is defined similarly.
Note that, once the control strategies are fixed, the dynamics of a typical agent becomes a Markov chain, and the matrices $F^\rho_t$ and $G^\rho_t$ are the transition matrices corresponding to the Blue and Red typical agents, respectively.

The following proposition provides justifications for introducing the mean-field dynamics as an approximation of the system dynamics.

\begin{restatable}{proposition}{mfp}
    Consider a finite $N$-population and its corresponding infinite-population system.
    At time $t$, suppose that the empirical distributions of the finite population are the same as the mean-fields of the infinite population, i.e., $\bmu^{N_1}_t = \mu^\rho_t$ and $\bnu^{N_2}_t = \nu^{\rho}_t$, and both systems apply the identical control strategy pair $(\phi_t, \psi_t)$, then the following holds:
    \begin{align*}
        \expectation{\dtv{\bmu_{t+1}^{N_1}, \mu^{\rho}_{t+1}}\vert \bmu_{t}^{N_1} = \mu_t^\rho, \bnu_{t}^{N_2} = \nu_t^\rho} = \order{\frac{1}{\sqrt{N_1}}},\\
        \expectation{\dtv{\bnu_{t+1}^{N_2}, \nu^{\rho}_{t+1}}\vert \bmu_{t}^{N_1} = \mu_t^\rho, \bnu_{t}^{N_2} = \nu_t^\rho} = \order{\frac{1}{\sqrt{N_2}}},
    \end{align*}
    where 
    $\mu^\rho_{t+1} = \mu^\rho_{t} F_t^{\rho}(\mu^\rho_t, \nu^\rho_t,\phi_t)$ and
    $\nu^\rho_{t+1} = \nu^\rho_t G^\rho_t(\mu^\rho_t, \nu^\rho_t,\psi_t)$ are propagated according to the dynamics in~\eqref{eqn:coordinator-dynamics}.
\end{restatable}

\begin{proof}
    See Appendix~\ref{appdx:prop-mf-apprx}
\end{proof}

\paragraph{Mean-field rewards.}

The reward in~\eqref{eqn:team-rewards} can also be easily extended from the empirical distribution to the mean-field, and we have
\begin{equation}\label{eqn:mf-rewards}
    r^\rho_t(\mu^\rho_t,\nu^\rho_t)  = \sum_{x\in\X}\mu^\rho_t(x)\; r_{1,t}(x) - 
    \sum_{y\in \Y} \nu^\rho_t(y) \;r_{2,t}(y).
\end{equation}

\begin{restatable}{proposition}{rrl}
    \label{rmk:reward-L-cont}
    The reward function is Lipschitz continuous, with Lipschitz constant 
    $L_{r} = 2 \max \{r_{1,\max}, r_{2,\max}\}$, 
    where $r_{1,\max} = \max_{x, t} \abs{r_{1,t}(x)}$ and $r_{2,\max} = \max_{y,t} \abs{r_{2,t}(y)}$.
    Formally, for all $\mu, \mu' \in \P(\X)$, $\nu, \nu' \in \P(\Y)$ and $t \in \{0, \ldots T\}$, we have that
    \begin{equation}
        \abs{r^\rho_t(\mu, \nu) - r^\rho_t(\mu', \nu')} \leq L_{r} \big(\dtv{\mu, \mu'} + \dtv{\nu, \nu'} \big).
    \end{equation}
\end{restatable}

% \begin{proof}
%     See Appendix~\ref{appdx-sec:Continuity}.
% \end{proof}

% \begin{remark}
%     \label{rmk:reward-linear}
%     The reward function is linear with respect to $\mu_t^\rho$ and $\nu_t^\rho$.
%     % which implies that it is also a concave-convex function.
% \end{remark}

For this infinite-population game, the cumulative reward induced by the strategy pair $(\phi^\rho, \psi^\rho) \in \Phi \times \Psi$ is given by 
\begin{equation}
    \label{eqn:mf-optimization-phi-psi}
    J^{\rho, \mathbf{\phi}^\rho, \mathbf{\psi}^\rho}(\mu^\rho_0, \nu^\rho_0) = \sum_{t=0}^{T} r_t^\rho(\mu^\rho_t, \nu^\rho_t),
\end{equation}
where the propagation of the mean-fields $\mu^\rho_t$ and $\nu^\rho_t$ are subject to the dynamics~\eqref{eqn:mf-dynamics}.
The corresponding lower game value from the max-min optimization is defined as
\begin{equation}\label{eqn:mf-optimization}
    \underline{J}^{\rho*} (\mu^\rho_0, \nu^\rho_0) = \max_{\mathbf{\phi}^\rho\in \Phi} \; \min_{\mathbf{\psi}^\rho \in \Psi} ~ J^{\rho, \mathbf{\phi}^\rho, \mathbf{\psi}^\rho}(\mu^\rho_0, \nu^\rho_0).
\end{equation}

The lower game value provides the worst-case performance for the Blue team, which is maximizing. 
The Red team would instead use the upper game value based on the following min-max optimization
\begin{equation}\label{eqn:mf-optimization-upper}
    \bar{J}^{\rho*} (\mu^\rho_0, \nu^\rho_0) =  \min_{\mathbf{\psi}^\rho \in \Psi} \max_{\mathbf{\phi}^\rho \in \Phi}~ J^{\rho, \mathbf{\phi}^\rho, \mathbf{\psi}^\rho}(\mu^\rho_0, \nu^\rho_0).
\end{equation}

Notice that there is no expectation taken in the cumulative reward in~\eqref{eqn:mf-optimization-phi-psi}, since the mean-field dynamics is deterministic. 
This observation motivates the approximation of the stochastic finite-population game with the deterministic infinite-population game. 
To facilitate the mean-field approximation in a mean-field team setting, we further investigate the optimization in~\eqref{eqn:mf-optimization} and exploit its deterministic nature.   

\subsection{Reachable Sets} \label{sec:reachable-set}

Due to the deterministic dynamics in~\eqref{eqn:coordinator-dynamics}, 
designing the policies $\phi_t^\rho$ and $\psi^\rho_t$ at time $t$ is equivalent to selecting the desirable next mean-fields given the current mean-fields. 
Consequently, we want to examine the set of mean-fields that can be achieved at the next timestep, starting from the current mean-fields $\mu^\rho_t$ and $\nu^\rho_t$.
We use $\pi_t : \U \times \X \to [0,1]$ to denote the identical local policies of the Blue agents, which are open-loop with respect to the mean-fields. 
Specifically, $\pi_t (u|x)$ is the probability that a Blue agent selects action $u$ at state $x$ regardless of the current mean-fields of the two teams.
We say a local policy is admissible if $\sum_u \pi_t (u|x) = 1$ for all state $x \in \X$.
The set of all admissible local policies is denoted as $\Pi_t$. 
Similarly, $\sigma_t : \V \times \Y \to [0,1]$ and $\Sigma_t$ are the (identical??) local policy and the feasible set for the Red team agents.

Given local policies $\pi_t$ and $\sigma_t$, the mean-fields of the two teams propagate in a deterministic manner just as in~\eqref{eqn:mf-dynamics}, but with the identical team policies replaced by the identical local policies. 
Formally, the propagation rule under the local policies is governed by
\begin{subequations}
\label{eqn:mf-dynamics-local-policy}
\begin{align}
    \mu^\rho_{t+1} (x_{t+1}) &= \sum_{x_t \in \X} \left[ \sum_{u_t \in \U}f^\rho_t(x_{t+1}|x_t, u_t, \mu^\rho_t, \nu^\rho_t ) \; \pi_t(u_t|x_t)\right]\mu^\rho_t(x_t), \label{eqn:mf-dynamics-local-policy-mu}\\
    \nu^\rho_{t+1} (y_{t+1}) &= \sum_{y_t \in \Y} \left[ \sum_{v_t \in \V}g^\rho_t(y_{t+1}|y_t, v_t, \mu^\rho_t, \nu^\rho_t ) \; \sigma_t(v_t|y_t)\right] \nu^\rho_t(y_t).\label{eqn:mf-dynamics-local-policy-nu}
\end{align}
\end{subequations}
Similarly to~\eqref{eqn:coordinator-dynamics}, the above deterministic propagation can be expressed in a matrix form as 
\begin{equation}
    \begin{aligned}
        \label{eqn:coordinator-dynamics-matrix}
        \mu^\rho_{t+1} &= \mu^\rho_{t} F_t^{\rho}(\mu^\rho_t, \nu^\rho_t,\pi_t), \\
        \nu^\rho_{t+1} &= \nu^\rho_t G^\rho_t(\mu^\rho_t, \nu^\rho_t,\sigma_t),
    \end{aligned}
\end{equation}
where the entries of the matrices are given by $\left[F_t^{\rho}(\mu_t, \nu_t,\pi_t)\right]_{pq} = \sum_{u \in \U}f_t(q|p, u, \mu_t, \nu_t ) \; \pi_t(u|p)$, and similarly for the $G_t^\rho$ matrix.
Note that these two matrices are both transition matrices, and they correspond to the transition probability of a typical agent under the local policies.

\vspace{+5pt}
To facilitate the analysis later, we provide the following definition of the reachable sets.

\begin{definition}
    \label{def:reachable-set}
    The Blue reachable set starting from the mean-fields $\mu_t$ and $\nu_t$ is defined to be the set consisting of all the next Blue team mean-fields $\mu_{t+1}$ that can be achieved with a feasible local policy $\pi_t \in \Pi_t$. Formally, 
    \begin{equation}
        \label{eqn:blue-reachable-set-def}
        \RMSet{t}{\rho} = \left\{\mu_{t+1} \;\vert\; \exists \pi_t \in \Pi_t \text{ such that } \mu_{t+1} = \mu_t F^{\rho}_t(\mu_t,\nu_t, \pi_t)\right\}.
    \end{equation}
    Similarly, the Red reachable set is defined as 
    \begin{equation}
        \label{eqn:red-reachable-set-def}
         \RNSet{t}{\rho} = \left\{\nu_{t+1} \; \vert \; \exists \sigma_t \in \Sigma_t \text{ such that } \nu_{t+1} = \nu_t G^{\rho}_t(\mu_t,\nu_t, \sigma_t)\right\}.
    \end{equation}
\end{definition}

\begin{remark}
    The Blue reachable set can be equivalently written as
    $\RMSet{t}{\rho} = \bigcup_{\pi_t \in \Pi_t} \{\mu_t F^\rho_t(\mu_t, \nu_t, \pi_t)\}$.
    Note that $\RMSet{t}{\rho} \subseteq \P(\X)$. 
\end{remark}

\begin{remark}
    For any Blue team policy $\phi^\rho_t$, the next mean-field $\mu_{t+1}^\rho$ propagated from $\mu_t^\rho$ and $\nu_t^\rho$ using~\eqref{eqn:mf-dynamics} is equal to the one propagated using~\eqref{eqn:mf-dynamics-local-policy} with a local policy $\pi_t$ such that $\pi_t(u|x) = \phi^\rho_t(u|x, \mu^\rho_t, \nu^\rho_t)$ for all $x \in \X$ and $u \in \U$.
    Consequently, regardless of the identical Blue team policy, the next Blue mean-field is always within the reachable set. 
\end{remark}


 



We will regard the reachable sets as set-valued functions (correspondences)~\citep{freeman2008robust}. 
In this case, we write $\R^\rho_{\mu,t} : \P(\X) \times \P(\Y) \rightsquigarrow \P(\X)$,
and similarly $\R^\rho_{\nu,t} : \P(\X) \times \P(\Y) \rightsquigarrow \P(\Y)$.



\subsection{Approximation Error}

Recall that in the mean-field setting, we assumed that both the Blue and Red teams apply identical team strategies.
At this point, it is unclear whether such an assumption on the opponent Red team is justified. 
In this subsection, we will show that the reachable set constructed from the mean-field dynamics under the identical-strategy assumption for both teams is actually rich enough. 
That is, there is a mean-field in the reachable set for the Red team that can approximate the empirical distribution resulting from arbitrary actions applied by the finite-population Red team.

Recall that $\mathcal{F}^+_t = \{\x^{N_1}_0, \y^{N_2}_0,\u^{N_1}_0,\v^{N_2}_0, \ldots, \x^{N_1}_t, \y^{N_2}_t, \u^{N_1}_t, \v^{N_2}_t\}$ captures the history of the $N$-agent system 
without the identical-strategy assumption on the Red team.
That is, 
the history $\F_t^+$ does not assume that the Red team deploys an identical strategy.
%
Note that the empirical distributions $\bmu^{N_1}_t$ and $\bnu^{N_2}_t$ are functions of the random variables in $\F^+_t$.
Furthermore, the actions $\u_t^{N_1}$ and $\v_t^{N_2}$ selected at time $t$ are also random variables in $\F^+_t$.
We present the following lemma to justify the use of mean-fields to approximate the empirical distributions. 

\begin{restatable}{lemma}{mfa}
    \label{lmm:mf-apprx}
    Let $\mathcal{F}^+_t = \{\x^{N_1}_0, \y^{N_2}_0,\u^{N_1}_0,\v^{N_2}_0, \ldots, \x^{N_1}_t, \y^{N_2}_t, \u^{N_1}_t, \v^{N_2}_t\}$ be the history of the $N$-agent system  under \emph{arbitrary strategies} up to timestep $t$ with actions realized.
    Consider the policy for the Red team
    \begin{equation}
    \label{appdx-eqn:local-policy-apprx}
    \bm{\sigma}_{\apprx,t} (v|y) =
    \mathds{1}_{\bm{\nu}^{N_2}_t(y) > 0}
    \frac{\sum_{j=1}^{N_2}\mathds{1}_{y}(\y_{j,t}^{N_2}) \mathds{1}_v(\v_{j,t}^{N_2})}{\sum_{k=1}^{N_2}\mathds{1}_y (\y_{k,t}^{N_2})} + \mathds{1}_{\bm{\nu}^{N_2}_t(y) = 0} \frac{1}{|\V|}.
\end{equation}
Define the next mean-field achieved when all Red agents apply $\bm{\sigma}_{\apprx, t}$ as
\begin{equation}    \label{eqn:apprx-blue-next-mf}
    \bm{\nu}_{\apprx, t+1} (y') = \sum_{y \in \Y} \left[\sum_{v \in \V} g^\rho_t(y'|y, v, \bm{\mu}^{N_1}_t, \bm{\nu}^{N_2}_t )\; \bm{\sigma}_{\apprx,t}(v|y) \right] \bm{\nu}^{N_2}_t(y).
\end{equation}
Then, the distance between the next Red empirical distribution achieved with Red actions $\v_t^{N_2}$ and the next Red mean-field induced by $\bm{\sigma}_{\apprx, t}$ is bounded as
\begin{equation}
    \label{eqn:mf-apprx-rate}
    \mathbb{E}\left[ \dtv{
    \bm{\nu}_{t+1}^{N_2}, \bm{\nu}_{\apprx,t+1}}\vert \F^+_t\right] = \mathcal{O}(\frac{1}{\sqrt{N_2}}).
\end{equation}
\end{restatable}

\begin{proof}
    See Appendix~\ref{appdx:mf-reachability}.
\end{proof}

    The local policy $\bm{\sigma}_{\apprx, t}$ in \eqref{appdx-eqn:local-policy-apprx} mimics the behaviors of the $N_2$ Red agents at time $t$, such that $\bm{\sigma}_{\apprx,t} (v|y)$ equals the fraction of Red agents on state $y$ that applied action $v$. 
    The indicator functions in~\eqref{eqn:apprx-blue-next-mf} ensure that the constructed local policy is admissible when $\bm{\nu}_t^{N_2}(y) = \sum_{j=1}^{N_2} \indicator{y}{\y_{j,t}^{N_2}} = 0$.

    Since the actions have been realized in $\F_t^+$, the conditional expectation in~\eqref{eqn:mf-apprx-rate} captures only the stochasticity in the system dynamics, instead of the randomness in potentially mixed policies.

The approximation result~\eqref{eqn:mf-apprx-rate} in Lemma~\ref{lmm:mf-apprx} implies the following error bound in terms of the reachable set:
\begin{equation}
    \label{eqn:mf-apprx-rset}
    \mathbb{E}\left[\inf_{\bm{\nu}^\rho_{t+1} \in \mathcal{R}^\rho_{\nu,t}(\bm{\mu}_t^{N_1}, \bm{\nu}_t^{N_2})} \dtv{
    \bm{\nu}_{t+1}^{N_2}, \bm{\nu}^\rho_{t+1}
    } \Big \vert \; \F^+_t\right] \leq \mathcal{O}(\frac{1}{\sqrt{N_2}}).
\end{equation}
Note that one can take another expectation to account for the randomness coming from the mixed strategies applied by the agents and still get the same order for the error bounds. 
In other words, given the current distributions $\mu^{N_1}_t$ and $\nu_t^{N_2}$ of the finite population team game, and regardless of the policies used by the Red agents (potentially different policies for different agents), there is always a mean-field $\nu_t$ in the reachable set (propagated using identical policies for all agents) that is $\mathcal{O}(1 /\sqrt{N_2})$-close to the next empirical distribution generated by these arbitrary policies (see \figref{fig:apprx} for an illustration). 
This observation justifies the identical strategy assumption and the use of a mean field to approximate the empirical distribution when the population is sufficiently large.
% Formal statements regarding performance loss due to using mean-field approximation and assuming identical policy are presented in Section~\ref{sec:performance}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\linewidth]{Figures/Approximation_Illustration.pdf}
    \caption{An illustration of Lemma~\ref{lmm:mf-apprx}.}
    \label{fig:apprx}
\end{figure}

The following lemma characterizes the discrepancy between a mean-field and its corresponding empirical distribution under an identical local policy. 

\begin{restatable}{lemma}{mff}
    \label{lmm:mff}
    Let $\F_t = \{\x_0^{N_1}, \y_0^{N_2}, \u_0^{N_1}, \v_0^{N_2}, \ldots, \x_t^{N_1}, \y_t^{N_2}\}$ be the history of the $N$-agent system up to timestep $t$. 
    Let $\bmu_t^{N_1}$ and $\bnu_t^{N_2}$ be the empirical distribution at the current time step $t$.
    Suppose that all the Blue agents apply the local policy $\pi_t$. 
    Then,
    \begin{equation}
        \expct{\dtv{\bm{\mu}^{N_1}_{t+1}, \bm{\mu}^\rho_{t+1}}\vert \F_t}{} = \order{\frac{1}{\sqrt{N_1}}},
    \end{equation}
    where $\bm{\mu}^{N_1}_{t+1}$ is the next empirical distribution induced by $\pi_t$ and $\bm{\mu}^\rho_{t+1} = \bm{\mu}^{N_1}_{t} F^\rho_t(\bm{\mu}^{N_1}_t, \bm{\nu}^{N_2}_t, \pi_t) \in \F_t$
    is the propagated mean-field from the current empirical distribution. 
\end{restatable}
\begin{proof}
    See Appendix~\ref{appdx:mf-reachability}.
\end{proof}

\section{Equivalent Zero-Sum Game Between Coordinators}
In this section, we consider the infinite population game, where both teams have an infinite number of agents.
Such infinite-population games have deterministic dynamics, providing a useful approximation to the finite-population stochastic game. 
We use the common information approach~\citep{nayyar2013decentralized} to introduce an equivalent zero-sum game between two ficiticious coordinators. 
% Then we find the solution to the centralized problem and translate the obtained solution back to the decentralized policies in~\eqref{eqn:decent-control-1} and~\eqref{eqn:decent-control-2}. 
In our formulation, the team mean-fields $\mu_t^\rho$ and $\nu_t^{\rho}$ serve as the common information.
We assume that the fictitious coordinators have perfect measurements of the mean-fields at each time step. 
Each coordinator prescribes a local feed-back policy at each timestep and communicates this policy to all agents within its team.
The details are presented below.

\subsection{Equivalent Centralized System}

Following~\citep{nayyar2013decentralized}, we construct a fictitious centralized \textit{coordinated system} for each team as follows.
We refer to the decision maker in the coordinated system as the \textit{coordinator}. 
At time $t$, the Blue coordinator of the Blue team observes the mean-fields of both teams at time $t$ and chooses a local policy $\pi_t:\U \times \X \to [0,1]$ as:
\begin{equation}
    \pi_t = \alpha_t \big(\mu^\rho_t,\nu^\rho_t \big),
\end{equation}
where  $\alpha_t: \P(\X) \times \P(\Y) \to \Pi_t$ is a deterministic \textit{coordination policy} used by the Blue coordinator, and $\pi_t(u_t|x_t) \triangleq \alpha_t(\mu^\rho_t,\nu^\rho_t)(u_t|x_t)$ gives the probability that a Blue agent selects action $u_t$ given its current state $x_t$.
Note that the local policy $\pi_t$ is slightly different from the policy $\phi^\rho_t$ in~\eqref{eqn:mf-policy}, as $\pi_t$ is a $\X$-state feedback policy and it is only implicitly dependent on the mean-fields through the coordination policy $\alpha_t$. 
In other words, the coordinator observes the mean-fields and pre-processes the information when selecting the policy according to $\alpha_t$. 
Since the state space $\X$ and the action space $\U$ are both finite, the admissible set of local policies is given by $\Pi_t = (\Delta_{|\U|})^{|\X|}$.


Similarly to the Blue coordinator, the Red coordinator observes the mean-fields of both teams and selects a local policy $\sigma_t: \V \times \Y \to [0,1]$ for the Red agents according to 
\begin{equation}
    \sigma_t = \beta_t \big(\mu^\rho_t, \nu^\rho_t\big).
\end{equation}
and the admissible set of Red local policies is given by $\Sigma_t = (\Delta_{|\V|})^{|\Y|}$.

The collection $\alpha =\big(\alpha_1, \ldots, \alpha_T\big)$ is called the \textit{coordination strategy}
for the Blue team. 
Similarly, we refer to the collection $\beta=\big(\beta_1, \ldots, \beta_T\big)$ as the Red coordination strategy.
We denote the set of admissible coordination strategies as $\A$ and $\B$ for the Blue and Red teams, respectively.

After a local policy $\pi_t$ is chosen by the Blue coordinator at time $t$, it is communicated to all Blue agents. 
A typical Blue agent then uses its local state $x_t$ and the local policy $\pi_t$ to generate its control action.
Similarly, the Red coordinator selects $\sigma_t$ and broadcasts it to all Red agents. The Red agents then select their actions according to $\sigma_t$.

\begin{remark}
    \label{rmk:equivalent-policy}
    A Blue coordination strategy $\alpha \in \A$ induces an identical team strategy $\phi \in \Phi$ for the finite population game according to the rule
    \begin{equation}
        \phi_{t}(u|x_{i,t}^{N_1}, \mu^{N_1}_t, \nu^{N_2}_t) = \underbrace{\alpha_t(\mu^{N_1}_t, \nu^{N_2}_t)}_{\displaystyle{\pi_t}}(u|x_{i,t}^{N_1}).
    \end{equation}
\end{remark}




% \begin{remark}
% The deterministic dynamics is linear with respect to the local policies. 
% Formally, for any pair of local policies $\pi_t, \pi'_t$ and $\theta \in [0,1]$, we have
% \begin{equation}
%     F_t(\mu_t, \nu_t, \theta \pi_t + (1-\theta) \pi'_t) = \theta F_t(\mu_t, \nu_t, \pi_t) + (1-\theta) F_t(\mu_t, \nu_t, \pi'_t).
% \end{equation}
% Similar linearity results applies to the Red mean-field dynamics.
% \end{remark}

Under a pair of \textit{deterministic} coordination strategies, the mean-fields propagate as
\begin{subequations}
\label{eqn:cor-mf-dynamics}
\begin{alignat}{2}
    \mu^\rho_{t+1} &= \mu^\rho_t F_t^\rho (\mu^\rho_t, \nu^\rho_t, \alpha_t(\mu^\rho_t,\nu^\rho_t)), 
    \\
    \nu^\rho_{t+1} &=\nu^\rho_t G^\rho_t(\mu^\rho_t, \nu^\rho_t, \beta_t(\mu^\rho_t,\nu^\rho_t)).
\end{alignat}
\end{subequations}
For notational simplicity, we use the shorthand notation $F_t^\rho (\mu^\rho_t, \nu^\rho_t, \alpha_t)$ to denote $F_t^\rho (\mu^\rho_t, \nu^\rho_t, \alpha_t(\mu^\rho_t,\nu^\rho_t))$. A similar notation extends to $G^\rho_t$.

% For notation simplicity, we denote the \textit{joint mean-field} as $\xi^\rho_t = (\mu^\rho_t, \nu^\rho_t)$ and write the above deterministic dynamics as 
% \begin{equation}
%     \label{eqn:joint-mf-dynamics}
%     \xi^\rho_{t+1} = H^\rho_t(\xi^\rho_t, \alpha_t,\beta_t),
% \end{equation}
% where $\xi^\rho_t \in \P(\X) \times \P(Y) \subseteq \mathbb{R}^{|\X| + |\Y|}$. 

The equivalent centralized system can be viewed as a zero-sum game played between the two coordinators, 
where the game state is the joint mean-field $(\mu_t^\rho, \nu_t^\rho)$,
and the actions are the local policies $\pi_t$ and $\sigma_t$ selected by the coordinators. 
Formally, the zero-sum coordinated team game can be defined via a tuple $\langle \P(\X) \times \P(Y), \Pi_t, \Sigma_t, F^\rho_t, G^\rho_t, r^\rho_t, T\rangle$.
In particular, the continuous game state space is $\P(\X) \times \P(Y)$, and the continuous action spaces are $\Pi_t$ and $\Sigma_t$ for the Blue and Red coordinators, respectively. 
The deterministic dynamics of the game is given in~\eqref{eqn:cor-mf-dynamics}. 
Finally, the reward structure $r^\rho_t$ is given in~\eqref{eqn:mf-rewards}, and the horizon $T$ is the same as in the original game. 



Given two coordination strategies $\alpha \in \A$ and $\beta \in \B$, the expected cumulative rewards of the coordinator game can be computed through dynamic programming as 
\begin{subequations}
\begin{align}
    J_{\cor,T}^{\rho, \alpha, \beta}(\mu^\rho_T, \nu^\rho_T) &= r^\rho_T(\mu^\rho_T, \nu^\rho_T), \\
    J_{\cor,t}^{\rho, \alpha, \beta}(\mu^\rho_t, \nu^\rho_t) &= r^\rho_t(\mu^\rho_t, \nu^\rho_t) +  J_{\cor,t+1}^{\rho, \alpha, \beta}\big(\mu^\rho_{t} F_{t}^\rho(\mu^\rho_{t}, \nu^\rho_{t}, \alpha_t),  \nu^\rho_{t} G_{t}^\rho(\mu^\rho_{t}, \nu^\rho_{t}, \beta_t)\big), ~~~ t = 0, \ldots, T-1.
\end{align}
\end{subequations}

\begin{remark}
    \label{rmk:value-DP-relation}
    We denote the induced value of the game starting from $\mu^\rho_0$ and $\nu^\rho_0$ as $J_{\cor}^{\rho, \alpha, \beta}(\mu^\rho_0, \nu^\rho_0)$; 
    it is related to the above dynamic program as follows:
    \begin{equation}
        \label{eqn:value-DP-relation}
        J_{\cor}^{\rho, \alpha, \beta}(\mu^\rho_0, \nu^\rho_0) = J_{\cor,0}^{\rho, \alpha, \beta}(\mu^\rho_0, \nu^\rho_0).
    \end{equation}
\end{remark}


In the next section, we examine the properties of the max-min (lower) and min-max (upper) game values of the coordinator game.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Value Functions of the Coordinator Game}
\label{sec:optimal-value}

Similar to the standard two-player zero-sum games, we use a backward induction scheme to find the lower and upper values of the coordinator game. 
In general, the max-min value need not be equal to the min-max value~\citep{elliott1972existence}.
Consequently, from the Blue coordinator's perspective, we consider the lower value (max-min).

The lower value provides the highest guaranteed performance for the maximizing (Blue) team in a worst-case scenario.
The terminal lower value at time $T$ is defined as 
\begin{equation}
    \label{eqn:J-ab-T}
    \lowervalue_{\cor,T}^{\rho *}(\mu^\rho_T, \nu^\rho_T) = r^\rho_T(\mu^\rho_T, \nu^\rho_T).
\end{equation}
For all previous timesteps $t=0,\ldots, T-1$, the two coordinators optimize their cumulative reward function by choosing their actions (i.e., local policies broadcasted to their own team agents) $\pi_t$ and $\sigma_t$.
Consequently, for all $t=0,\ldots,T-1,$ we have
\begin{equation}
    \label{eqn:j-ab-t}
    \lowervalue_{\cor,t}^{\rho*}(\mu^\rho_t, \nu^\rho_t) = r^\rho_t(\mu^\rho_t, \nu^\rho_t) +
    \max_{\pi_t \in \Pi_t}~ \min_{\sigma_t \in \Sigma_t}
    \lowervalue_{\cor,t+1}^{\rho*}\big(\mu^\rho_t F^\rho_t(\mu^\rho_t, \nu^\rho_t, \pi_t), \nu^\rho_t G^\rho_t(\mu^\rho_t, \nu^\rho_t, \sigma_t)\big).
\end{equation}

For timesteps $t=0,\ldots, T-1$, the optimal Blue team coordination policies can then be easily extracted from the optimal value functions:
\begin{align}
    \label{eqn:optimal-blue-policy}
    \alpha^*_t(\mu^\rho_t, \nu^\rho_t) &\in \argmax_{\pi_t \in \Pi_t}~ \min_{\sigma_t \in \Sigma_t}
    \lowervalue_{\cor,t+1}^{\rho*}\big(\mu^\rho_t F_t^{\rho}(\mu^\rho_t, \nu^\rho_t,\pi_t), \nu_t^\rho G^\rho_t(\mu^\rho_t, \nu^\rho_t, \sigma_t)\big).
\end{align}


% Similarly, for the Red team coordinator, the upper values are computed as 
% \begin{subequations}
%     \begin{align}
%     &\uppervalue_{\cor,T}^{\rho *}(\mu^\rho_T, \nu^\rho_T) = r^\rho_T(\mu^\rho_T, \nu^\rho_T),
%     \\
%     &\uppervalue_{\cor,t}^{\rho*}(\mu^\rho_t, \nu^\rho_t) = r_t(\mu^\rho_t, \nu^\rho_t) +
%     \min_{\sigma_t \in \Sigma_t} \max_{\pi_t \in \Pi_t} 
%     \uppervalue_{\cor,t+1}^{\rho*}\big(\mu^\rho_t F^\rho_t(\mu^\rho_t, \nu^\rho_t, \pi_t), \nu^\rho_t G^\rho_t(\mu^\rho_t, \nu^\rho_t, \sigma_t)\big),~~ t=0,\ldots,T-1,
%     \end{align}
% \end{subequations}
% and the optimal Red team coordination policy is given by
% \begin{align}
%     \label{eqn:optimal-red-policy}
%     \beta^*_t(\mu^\rho_t, \nu^\rho_t) &\in  \argmin_{\sigma_t \in \Sigma_t}~\max_{\pi_t \in \Pi_t}
%     \uppervalue_{\cor,t+1}^{\rho*}\big(\mu^\rho_t F_t^{\rho}(\mu^\rho_t, \nu^\rho_t,\pi_t), G^\rho_t(\mu^\rho_t, \nu^\rho_t, \sigma_t)\big).
% \end{align}

However, several technical details were skipped before the introduction of the max-min optimization in~\eqref{eqn:j-ab-t}. 
Two important questions one may ask are: (i) whether the max-min is attainable, and (ii) when the game value exists, i.e., whether max-min equals to min-max. 
We answer these questions in the following subsections, by addressing the continuity and convexity/concavity of the value function. 
Before we answer these two questions, we first present an equivalent form of the optimization problem in~\eqref{eqn:J-ab-T} and~\eqref{eqn:j-ab-t} by leveraging the reachable set introduced in Definition~\ref{def:reachable-set}.


\subsection{Equivalent Form of Value Functions} 

Exploiting the deterministic mean-field dynamics, we can change the optimization domains in~\eqref{eqn:J-ab-T} and~\eqref{eqn:j-ab-t} from the policy spaces to the corresponding reachable sets. 
One can easily see that the following lower value propagation scheme is equivalent to the one in~\eqref{eqn:J-ab-T} and~\eqref{eqn:j-ab-t}.
\begin{equation}
    \label{eqn:lower-j-ab-t-rset}
    \begin{aligned}
        \lowervalue_{\cor,T}^{\rho*}(\mu^\rho_T, \nu^\rho_T) &= r^\rho_T(\mu^\rho_T, \nu^\rho_T)\\
        \lowervalue_{\cor,t}^{\rho*}(\mu^\rho_t, \nu^\rho_t) &= r^\rho_t(\mu^\rho_t, \nu^\rho_t) +
        \max_{\mu_{t+1}^\rho \in \mathcal{R}_{\mu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}~ \min_{\nu_{t+1}^\rho \in \mathcal{R}_{\nu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu^\rho_{t+1}, \nu^\rho_{t+1}), \quad t= 0,\ldots, T-1.
    \end{aligned}
\end{equation}

% Similarly, the upper value propagation scheme can be transformed to the following reachable-set-based optimization problem 
% \begin{equation}
%     \label{eqn:upper-j-ab-t-rset}
%     \begin{aligned}
%         \uppervalue_{\cor,T}^{\rho*}(\mu^\rho_T, \nu^\rho_T) &= r^\rho_T(\mu^\rho_T, \nu^\rho_T)\\
%         \uppervalue_{\cor,t}^{\rho*}(\mu^\rho_t, \nu^\rho_t) &= r^\rho_t(\mu^\rho_t, \nu^\rho_t) +
%         \min_{\nu_{t+1}^\rho \in \mathcal{R}_{\nu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}~
%         \max_{\mu_{t+1}^\rho \in \mathcal{R}_{\mu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}
%         \uppervalue_{\cor,t+1}^{\rho*}(\mu^\rho_{t+1}, \nu^\rho_{t+1}), \quad t= 0,\ldots, T-1.
%     \end{aligned}
% \end{equation}

In the later analysis, we will mainly focus on the reachable set based optimization for the optimal values.
The reachable set formulation has two major advantages:
(i) the reachable sets have lower dimension than the action spaces\footnote{For the Blue coordinator, the reachable set is a subset of $\P(\X)$, while its action space is the set of all admissible local policies given by $(\Delta_{|\U|})^{|\X|}$.},
and (ii) the reachability-based optimization allows us to apply Lemma~\ref{lmm:mf-apprx} and study the performance loss due to the identical-strategy assumption on the Red team, which was introduced by the mean-field approximation. 


\subsection{Lipschitz Continuity of the Value Functions}

In this section, we study the continuity of the optimal value function in~\eqref{eqn:lower-j-ab-t-rset} with respect to the two mean-fields.
Clearly, the continuity of the value function depends on the continuity of the two reachability correspondences $\R^\rho_{\mu,t}$ and $\R^\rho_{\nu,t}$, which governs the optimization domains. 
% Intuitively, only when a slight change in the mean-fields leads to a small change in the reachable sets, there can guarantees for the continuity of the value function. 
%
To properly study the continuity of the reachability correspondences, we use the Hausdorff distance to measure the distance between two sets. 
\begin{definition}[Hausdorff distance]
    For a normed space $(\X, \norm{\cdot})$, the Hausdorff distance between two sets $A, B \subseteq \X$ is defined as
    \begin{equation}
        \distH{A,B} = \max\left\{\sup_{a \in A} \inf_{b \in B} \norm{a-b}, \sup_{a \in B} \inf_{b \in A} \norm{a-b}\right\}.
    \end{equation}
\end{definition}

Based on the Hausdorff distance, we have the following notion of Lipschitz continuity for correspondences~\citep{freeman2008robust}.
\begin{definition}
    A correspondence $\Gamma: \X \rightsquigarrow \Y$ is $L_\Gamma$-Lipschitz continuous under the Hausdorff distance if, for all $x, y \in \X$, it satisfies that
    \begin{equation}
        \distH{\Gamma(x), \Gamma(y)} \leq L_\Gamma \norm{x-y}.
    \end{equation}
\end{definition}

Observe that the optimal value function in~\eqref{eqn:lower-j-ab-t-rset} takes the following form:
$f(x,y) = \max_{p\in \Gamma(x,y)} \min_{q\in \Theta(x,y)} g(p,q)$,
which can be viewed as an extension of the the marginal function~\citep{freeman2008robust} to the max-min case.
We present a continuity result for this type of marginal functions in Lemma~\ref{lmm:min-max-marginal-L-cont}. 
The proof of this result is delayed till Appendix~\ref{appdx-sec:Continuity}.


\begin{restatable}{lemma}{mmm}
    \label{lmm:min-max-marginal-L-cont}
    Consider two compact-valued correspondences $\Gamma: \X \times \Y \rightsquigarrow \X$ and $\Theta: \X \times \Y \rightsquigarrow \Y$ that are Lipschitz continuous with Lipschitz constants $L_\Gamma$ and $L_\Theta$, respectively.
    Given a $L_g$-Lipschitz continuous
    real-valued function $g: \X \times \Y \to \mathbb{R}$, the max-min marginal function
    \begin{equation}
        \label{eqn:max-min-marginal-def}
        f(x,y) = \max_{p\in \Gamma(x,y)} \min_{q\in \Theta(x,y)} g(p,q)
    \end{equation}
    is Lipschitz continuous with Lipschitz constant $L_g (L_\Gamma + L_\Theta)$. 
\end{restatable}




Next, we present a Lipschitz continuity result for the reachability correspondences based on the deterministic mean-field dynamics.
The proof of Lemma~\ref{lmm:R-L-cont} is postponed to Appendix~\ref{appdx-sec:Continuity-RSet}.

\begin{restatable}{lemma}{RLC}
    \label{lmm:R-L-cont}
    The reachability correspondences $\mathcal{R}_{\mu,t}$ and $\mathcal{R}_{\nu,t}$ in~\eqref{eqn:blue-reachable-set-def} and~\eqref{eqn:red-reachable-set-def} satisfy the following inequalities for all $\mu_t, \mu'_t \in \P(\X)$ and $\nu_t, \nu'_t \in \P(\Y)$,
    \begin{equation}
        \label{eqn:reachability-continuity-1}
        \begin{aligned}
            \distH{\R^\rho_{\mu,t}(\mu_t,\nu_t), \R^\rho_{\mu,t}(\mu'_t,\nu'_t)} \leq \big(\rho |\X| +1\big) \dtv{\mu_t, \mu'_t} + (1-\rho) |\X|  \dtv{\nu_t, \nu'_t}, \\
            \distH{\R^\rho_{\nu,t}(\mu_t,\nu_t), \R^\rho_{\nu,t}(\mu'_t,\nu'_t)} \leq \rho |\Y| \dtv{\mu_t, \mu'_t} + \big((1-\rho) |\Y| + 1\big) \dtv{\nu_t, \nu'_t}.
        \end{aligned}
    \end{equation}
\end{restatable}

For notational simplicity, we denote the Lipschitz constants of the reachability correspondences as
\begin{align*}
    L_{R_{\mu,t}} = \max\left\{\rho |\X| +1, (1-\rho) |\X| \right\}\quad 
    \text{and} \quad
    L_{R_{\nu,t}} = \max\left\{\rho |\Y| , (1-\rho) |\Y| + 1 \right\}.
\end{align*}
With the above constants, it follows directly from~\eqref{eqn:reachability-continuity-1} that
\begin{equation}
    \label{eqn:reachability-continuity}
    \begin{aligned}
            \distH{\R^\rho_{\mu,t}(\mu_t,\nu_t), \R^\rho_{\mu,t}(\mu'_t,\nu'_t)} \leq L_{R_{\mu,t}} \big(\dtv{\mu_t, \mu'_t} +  \dtv{\nu_t, \nu'_t}\big), \\
            \distH{\R^\rho_{\nu,t}(\mu_t,\nu_t), \R^\rho_{\nu,t}(\mu'_t,\nu'_t)} \leq L_{R_{\nu,t}} \big( \dtv{\mu_t, \mu'_t} + \dtv{\nu_t, \nu'_t}\big).
    \end{aligned}
\end{equation}

Since the terminal rewards are Lipschitz (see Remark~\ref{rmk:reward-L-cont}), we can prove, through an inductive argument, the following corollary regarding the continuity of the optimal value function. 
A detailed proof is given Appendix~\ref{appdx-sec:Continuity}.

\begin{restatable}{theorem}{vlc}
    \label{cor:value-L-cont}
    The optimal lower value function $\lowervalue^{\rho*}_{\cor, t}$ and the optimal upper value function $\uppervalue^{\rho*}_{\cor, t}$ are both Lipschitz continuous. 
    Formally, for all $\mu_{t}^\rho,\mu^{\rho\prime}_t \in \P(\X)$ and $\nu_{t}^\rho,\nu^{\rho\prime}_t \in \P(\Y)$, the lower value satisfies 
    \begin{equation}
        \label{eqn:L-cont-cor-value}
        \abs{\lowervalue^{\rho*}_{\cor, t}(\mu^\rho_{t}, \nu^\rho_t) - \lowervalue^{\rho*}_{\cor, t}(\mu^{\rho\prime}_{t}, \nu^{\rho\prime}_t)} \leq 
        L_{J,t}\big(\dtv{\mu^\rho_t, \mu_t^{\rho\prime}}+\dtv{\mu^\rho_t, \nu_t^{\rho\prime}}\big),
    \end{equation}
    where the Lipschitz constant $L_{J,t}$ is given by
    \begin{equation}
        \label{eqn:value-L-constant}
        L_{J,t} = L_r \big( 1+ \sum_{k=t}^{T-1} \prod_{\tau =t}^{k} (L_{\R^\rho_{\mu,\tau}} + L_{\R^\rho_{\nu,\tau}})\big)
    \end{equation}
\end{restatable}

Theorem~\ref{cor:value-L-cont} ensures that the max-min optimization in~\eqref{eqn:lower-j-ab-t-rset} and the min-max optimization in~\eqref{eqn:upper-j-ab-t-rset} are well defined, that is, the maximum and minimum are attainable.



\subsection{Existence of the Game Value}

In general, the game value may not exist for the continuous coordinator game, i.e., the upper and the lower values may not be equal. 
In this section, we show that the game value exists for a special class of mean-field team games, where the dynamics of each individual agent is independent of other agents' states (both its teammates and its opponents).
It is left as a future research direction to obtain more general conditions that ensure the existence of the game value. 


\begin{definition}[Independent dynamics]
    We say that the weakly-coupled dynamics in~\eqref{eqn:weakly-dynamics} are independent if they satisfy 
    \begin{subequations}
        \label{eqn:independent-dynamics}
        \begin{align}
            f_{1,t} (x_{i,t+1}^N | x_{i,t}^{N_1}, u_{i,t}^{N_1}, x) &= f_{2,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y) = f_{t}(x_{i,t+1}^N | x_{i,t}^{N_1}, u_{i,t}^{N_1}), \quad  x\in \X, y \in \Y,\label{eqn:independent-blue-dynamics}
            \\
            g_{1,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, x )  &=
            g_{2,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t},  y) = g_{t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}), \quad  x\in \X, y \in \Y. \label{eqn:independent-red-dynamics}
        \end{align}
    \end{subequations}
\end{definition}


The following theorem ensures the existence of the game value under independent dynamics. 

\begin{theorem}[Existence of game value]
    \label{thm:game-value}
    Under the independent dynamics in~\eqref{eqn:independent-dynamics}, the game value exists. 
    That is, the lower game value and the upper game value coincide. 
\end{theorem}
One can show the concavity-convexity of the optimal value under independent dynamics. 
The above theorem is then a direct result of the minimax theorem. 
We delay the detailed proof of Theorem~\ref{thm:game-value} to Appendix~\ref{appdx-sec:game-value}.




% allows us to apply the minimax theorem and ensures that the max-min value equals the min-max value. 
% Consequently, additional strengthened conditions are required to ensure the existence of the game value. 



% \subsubsection{Counter Example}
% \todo
% Consider the following simple mean-field team game with horizon $H=1$, such that 
% \begin{enumerate}
%     \item The population ratio is given by $\rho = 0.5$;
%     \item The states spaces are given as: $\X = \{x_1, x_2\}$ and $\Y = \{y_1\}$;
%     \item The action spaces are singletons, i.e. $\U = \{u_1\}$ and $\V = \{v_1\}$;
%     \item The Blue dynamics are given by:
%     \begin{alignat*}{2}
%         f_{1,0}(x_1|x_1, u_1, x_1) &= 1     \qquad \qquad   f_{2,0}(x_1|x_1, u_1, y_1) &= 1\\
%         f_{1,0}(x_1|x_1, u_1, x_2) &= 0     \qquad \qquad   f_{2,0}(x_1|x_2, u_1, y_1) &= 1\\
%         f_{1,0}(x_1|x_2, u_1, x_1) &= 0     \qquad \qquad   f_{2,0}(x_2|x_1, u_1, y_1) &= 0\\
%         f_{1,0}(x_1|x_2, u_1, x_2) &= 1     \qquad \qquad   f_{2,0}(x_2|x_2, u_1, y_1) &= 0\\
%         f_{1,0}(x_2|x_1, u_1, x_1) &= 0\\
%         f_{1,0}(x_2|x_1, u_1, x_2) &= 1\\
%         f_{1,0}(x_2|x_2, u_1, x_1) &= 1\\
%         f_{1,0}(x_2|x_2, u_1, x_2) &= 0
%     \end{alignat*}
%     \item The Red dynamics are given by:
%     \begin{align*}
%         g_{1,0}(y_1|y_1, v_1, x_1) &= 1 \\
%         g_{1,0}(y_1|y_1, v_1, x_2) &= 1 \\
%         g_{2,0}(y_1|y_1, v_1, y_1) &= 1
%     \end{align*}
%     \item The reward is given by:
%     \begin{alignat}{2}
%         r_{1,0}(x_1) &= 0 \qquad r_{1,0}(x_2) &= 0 \\
%         r_{1,1}(x_1) &= 1 \qquad r_{1,0}(x_2) &= 2 \\
%         r_{2,0}(y_1) & = 0 & \\
%         r_{2,1}(y_1) & = 0. &
%     \end{alignat}
% \end{enumerate}

% Since both action spaces are singletons, the system is degenerate and is simply a Markov chain with no decision to make. 
% Consequently, the mean-field reachable sets for both Blue and Red team are also singletons. 


% Similar to the analysis done in the previous subsection, 


\section{Performance Guarantees}  \label{sec:performance}

In the previous section we obtained an optimal Blue coordination strategy $\alpha^*$ from~\eqref{eqn:optimal-blue-policy}.
Note that $\alpha^*$ is obtained under the mean-field setting, i.e., both teams have an infinite number of agents and all agents in each team apply identical strategies.
In this section, we analyze the performance guarantees for $\alpha^*$ for the \emph{finite-population} Blue team, when the \emph{finite-population} Red team deploys a team strategy $\psi^{N_2} \in \Psi^{N_2}$ allowing different Red agents to use different agent strategies. 


To this end, let the Red team apply strategies $\psi^{N_2} = (\psi^{N_2}_1, \ldots \psi^{N_2}_{N_2}) \in \Psi^{N_2}$ as in~\eqref{eqn:ed-red-policy}. 
Recall that a Blue coordination strategy induces an equivalent identical Blue team strategy (see Remark~\ref{rmk:equivalent-policy}).
With a slight abuse of notation, we denote the finite-population performance induced by the optimal coordination strategy $\alpha^*$ and some Red strategies $\psi^{N_2} \in \Psi^{N_2}$ as 
\begin{equation*}
    J_{0}^{N, \alpha^*, \psi^{N_2}}(\mu^{N_1}_0, \nu^{N_2}_0) = \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[\sum_{t =0}^T r^\rho_t(\bm{\mu}^{N_1}_t, \bm{\nu}^{N_2}_t) \vert \bm{\mu}^{N_1}_0 = \mu^{N_1}_0, \bm{\nu}^{N_2}_0 = \nu^{N_2}_0\Big].
\end{equation*}
Through dynamic programming, we can compute the above induced value through a backward propagation scheme 
\begin{subequations}
    \begin{align}
        J_{T}^{N, \alpha^*, \psi^{N_2}}(\mu^{N_1}_T, \nu^{N_2}_T) 
        &= r^\rho_T(\mu^{N_1}_T, \nu^{N_2}_T) \\
        J_{t}^{N, \alpha^*, \psi^{N_2}}(\mu^{N_1}_t, \nu^{N_2}_t) 
        &= r^\rho_t(\mu^{N_1}_t, \nu^{N_2}_t) + \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[J_{t+1}^{N, \alpha^*, \psi^{N_2}}(\bm{\mu}^{N_1}_{t+1}, \bm{\nu}^{N_2}_{t+1}) \; \big \vert  \; \bm{\mu}^{N_1}_t = \mu^{N_1}_t, \bm{\nu}^{N_2}_t = \nu^{N_2}_t \Big], \\
        & \qquad \qquad\qquad \qquad \qquad \qquad\qquad \qquad \qquad\qquad\qquad \qquad\qquad\qquad\qquad \forall t = 0, \ldots, T-1. \nonumber
    \end{align}
\end{subequations}


We have the following theorem regarding the performance guarantees for the optimal Blue coordination strategy. 
\begin{theorem}
    The optimal Blue coordination strategy $\alpha^*$ obtained from~\eqref{eqn:optimal-blue-policy} is an $\epsilon$-optimal Blue team strategy. Formally, we have for all distributions $\mu\in \P(\X)$ and $\nu \in \P(\nu)$
    \begin{equation}
        \label{eqn:performance-bounds}
        \underline{J}^{N*}(\mu, \nu) \geq \min_{\psi^{N_2} \in \Psi^{N_2}} J^{N,\alpha^*,\psi^{N_2}} (\mu, \nu) \geq \underline{J}^{N*}(\mu, \nu) - \residue.
    \end{equation}
    % where $\underline{J}^{N*} = \max_{\phi \in \Phi} \min_{\psi^{N_2} \in \Psi^{N_2}} J^{N,\phi,\psi^{N_2}}$ as defined in~\eqref{eqn:origin-optimization}.
\end{theorem}

\begin{proof}
    Due to Remark~\ref{rmk:equivalent-policy}, the Blue coordination strategy $\alpha^*$ induces an equivalent identical Blue team strategy $\phi^* \in \Phi$.
    Consequently, we have
    \begin{align*}
        \underline{J}^{N*}(\mu, \nu) &= \max_{\phi \in \Phi} \min_{\psi^{N_2} \in \Psi^{N_2}} J^{N,\phi,\psi^{N_2}}(\mu, \nu) \\
        &\geq \min_{\psi^{N_2} \in \Psi^{N_2}} J^{N,\phi^*,\psi^{N_2}}(\mu, \nu) = \min_{\psi^{N_2} \in \Psi^{N_2}} J^{N,\alpha^*,\psi^{N_2}}(\mu, \nu),
    \end{align*}
    which leads to the first inequality in~\eqref{eqn:performance-bounds}.
    
    For the second inequality, we break it down into two lemmas:
    First, Lemma~\ref{lmm:coordination-esp-nash} states that $\min_{\psi^{N_2}\in \psi^{N_2}} J^{N, \alpha^*, \psi^{N_2}} \geq \lowervalue_{\cor}^{\rho*} - \epsilon_1$;
    and, second, 
 Lemma~\ref{lmm:cor-apprx-err} shows that $\lowervalue_{\cor}^{\rho*} \geq \lowervalue^{N*} - \epsilon_2$;
    finally, it is shown that both error terms are of order $\residue$.
    Combining the two lemmas, we obtain the desired result.
\end{proof}



\begin{lemma}
    \label{lmm:coordination-esp-nash}
    The optimal Blue coordination strategy $\alpha^*$ obtained from~\eqref{eqn:optimal-blue-policy} guarantees that, for all distributions $\mu \in \P(\X)$ and $\nu \in \P(\Y)$,
    %
    %\pn{the use of $\rho$ superscript indicates an inf population?}
    %\sg{Yes. Here we are just comparing the performance of the finite-population system and inf-population system starting from the same distributions $\mu$ and $\nu$.}
    \begin{equation}
        \label{eqn:deviation-performance-bound}
        \min_{\psi^{N_2}\in \psi^{N_2}} J^{N, \alpha^*, \psi^{N_2}}(\mu, \nu) \geq \lowervalue_{\cor}^{\rho*}(\mu, \nu) - \residue.
    \end{equation}
\end{lemma}

In other words, the above lemma states that when the Blue team deploys the strategy $\alpha^*$, the Red team can only minimally improve its performance by switching from an identical team strategy to an arbitrary team strategy.
Furthermore, the performance improvement diminishes as the populations of both teams approach infinity. 

\begin{proof}
    The proof is constructed based on induction. 
    In the rest of the proof, we consider an arbitrary Red team strategy $\psi^{N_2} \in \Psi^{N_2}$.
    
    \textit{Base case:}
    At the terminal timestep $T$, since there is no decision to be made, both value functions are equal to the terminal reward and are thus the same.
    %
 %   \pn{why?}\sg{an explanation added.}
    %
    Formally,
    \begin{equation*}
        J_{T}^{N, \alpha^*, \psi^{N_2}}(\mu_T, \nu_T) = \lowervalue_{\cor,T}^{\rho*}(\mu_T, \nu_T)
        = r^\rho_T(\mu_T, \nu_T), \quad \mu_T \in \P(\X), ~ \nu_T \in \P(\Y).
    \end{equation*} 
    
    \textit{Inductive hypothesis: }
    Assume that at time step $t+1$, the following holds for all distributions $\mu_{t+1}$ and $\nu_{t+1}$
    %
%    \pn{is the $O$ term positive? why the minus sign?}
 %   \sg{this is just the inductive hypothesis, which is valid since the base case satisfies the following equation. We will show that the O term is positive and the minus sign in the induction step.}
    \begin{equation}
        J_{t+1}^{N, \alpha^*, \psi^{N_2}} (\mu_{t+1}, \nu_{t+1}) \geq 
        \lowervalue_{\cor, t+1}^{\rho*}(\mu_{t+1}, \nu_{t+1}) - \residue.
    \end{equation}
    
    \textit{Induction: }
    At timestep $t$, for given distributions $\mu_t$ and $\nu_t$, define $\mu_{t+1}^{*}$ as
    \begin{equation*}
        \mu_{t+1}^* = \mu_{t} F(\mu_{t}, \nu_{t}, \alpha^*).
    \end{equation*}
    Note that, from the optimality of $\alpha^*$, we have
    \begin{equation}
        \label{eqn:mu-star-def}
        \mu_{t+1}^* \in \argmax_{\mu_{t+1} \in \mathcal{R}_{\mu,t}^\rho(\mu_t,\nu_t)}~ \min_{\nu_{t+1} \in \mathcal{R}_{\nu,t}^\rho(\mu_t,\nu_t)}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu_{t+1}, \nu_{t+1}).
    \end{equation}
    Let $\bm{\nu}_{\apprx,t+1} \in \F^+_t$ be the Red mean-field constructed based on $\psi^{N_2}$ via~\eqref{eqn:apprx-blue-next-mf}. 
    Then, we have
    \begin{flalign}
        & J_{t}^{N, \alpha^*, \psi^{N_2}}(\mu_t, \nu_t)
        =r^\rho_t(\mu_t, \nu_t) + \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[J_{t+1}^{N, \alpha^*, \psi^{N_2}}(\bm{\mu}^{N_1}_{t+1}, \bm{\nu}^{N_2}_{t+1}) \; \big \vert  \; \bm{\mu}^{N_1}_t = \mu_t, \bm{\nu}^{N_2}_t = \nu_t \Big]\\
        & = r^\rho_t(\mu_t, \nu_t) + \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[J_{t+1}^{N, \alpha^*, \psi^{N_2}}(\bm{\mu}^{N_1}_{t+1}, \bm{\nu}^{N_2}_{t+1}) \big\vert \F_t\Big]
        \label{eqn:main-1}\\
        &\stackrel{\text{(i)}}{\geq}  r^\rho_t(\mu_t, \nu_t) + \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[\lowervalue_{\cor, t+1}^{\rho*}(\bm{\mu}^{N_1}_{t+1}, \bm{\nu}^{N_2}_{t+1}) \big \vert \F_t\Big] -\residue
        \label{eqn:main-2}\\
        &\stackrel{\text{(ii)}}{=}  r^\rho_t(\mu_t, \nu_t) + \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[ \expectation{\lowervalue_{\cor, t+1}^{\rho*}(\bm{\mu}^{N_1}_{t+1}, \bm{\nu}^{N_2}_{t+1}) \big \vert \F_t^+} \big\vert \F_t\Big] -\residue \label{eqn:main-25}\\
        &\stackrel{\text{(iii)}}{\geq}  r^\rho_t(\mu_t, \nu_t) -\residue \label{eqn:main-3} \\
        & ~~~~~~~~~ + \mathbb{E}_{\alpha^*, \psi^{N_2}}\Bigg[ \expectation{\lowervalue_{\cor, t+1}^{\rho*}({\mu}^{*}_{t+1}, \bm{\nu}_{\apprx, t+1}) - L_{J, t+1} \bigg(\dtv{{\mu}^*_{t+1}, \bm{\mu}_{t+1}^{N_1}} + \dtv{\bm{\nu}_{\apprx,t+1}, \bm{\nu}^{N_2}_{t+1}}\bigg)\big \vert \F_t^+} \big\vert \F_t\Bigg] \nonumber
        \\
        % &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad  \nonumber\\
        &\stackrel{\text{(iv)}}{=}  r^\rho_t(\mu_t, \nu_t) + 
        \mathbb{E}_{\alpha^*, \psi^{N_2}}\Big[ \expectation{\lowervalue_{\cor, t+1}^{\rho*}({\mu}^{*}_{t+1}, \bm{\nu}_{\apprx, t+1})\big \vert \F_t^+} \big\vert \F_t\Big] -\residue \label{eqn:main-4} \\
        &\qquad \qquad - L_{J, t+1}  \underbrace{\mathbb{E}_{\alpha^*}\Big[\dtv{{\mu}^*_{t+1}, \bm{\mu}_{t+1}^{N_1}} \big\vert \F_t\Big]}_{=\mathcal{O}(\frac{1}{\sqrt{N_1}}) \text{ due to Lemma~\ref{lmm:mff}}} - 
        L_{J, t+1} \mathbb{E}_{\psi^{N_2}}\Big[ \underbrace{\expectation{\dtv{\bm{\nu}_{\apprx,t+1}, \bm{\nu}^{N_2}_{t+1}} \big \vert \F_t^+} }_{=\mathcal{O}(\frac{1}{\sqrt{N_2}}) \text{ due to Lemma~\ref{lmm:mf-apprx}}}\big\vert \F_t\Big]
        \nonumber \\
        % &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad  \nonumber\\
        & \stackrel{\text{(v)}}{=}  r^\rho_t(\mu_t, \nu_t) + 
        \mathbb{E}_{\alpha^*, \psi^{N_2}}\Bigg[ \expectation{\lowervalue_{\cor, t+1}^{\rho*}(\mu^{*}_{t+1}, \bm{\nu}_{\apprx, t+1})\big \vert \F_t^+} \big\vert \F_t\Big] - \residue \label{eqn:main-5}\\
        &\stackrel{\text{(vi)}}{\geq}  r^\rho_t(\mu_t, \nu_t) + \min_{\nu_{t+1}\in \mathcal{R}_{\nu,t}^\rho(\mu_t, \nu_t)}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu^*_{t+1}, \nu_{t+1}) -\residue\label{eqn:main-6}\\
        &\stackrel{\text{(vii)}}{=}  r^\rho_t(\mu_t, \nu_t) +
        \max_{\mu_{t+1} \in \mathcal{R}_{\mu,t}^\rho(\mu_t, \nu_t)}~ \min_{\nu_{t+1}\in \mathcal{R}_{\nu,t}^\rho(\mu_t, \nu_t)}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu_{t+1}, \nu_{t+1}) -\residue \label{eqn:main-7}\\
        &\stackrel{\text{viii}}{=}  \lowervalue_{\cor,t}^{\rho*}(\mu_{t}, \nu_{t}) -\residue. \label{eqn:main-8}
    \end{flalign}
    For inequality (i), we used the inductive hypothesis; 
    equality (ii) comes from the rule of total expectation;
    for inequality (iii), we utilize the Lipschitz continuity of the coordinator value function presented in~\eqref{eqn:L-cont-cor-value}; 
    %
    %\sg{The minus sign: if function $f(x,y)$ is L-Lipschitz, we have $f(x',y') \geq f(x, y) - L (|x-x'|,|y-y'|)$, based on the way I define Lipschitz continuity for function with two arguments. See~\eqref{eqn:L-cont-cor-value}.}
    %
    equality (v) is due to  Lemma~\ref{lmm:mf-apprx} and Lemma~\ref{lmm:mff} and we combine the two error terms with Lipschitz constant $L_{j, t+1}$ into the residue term;
    inequality (vi) is due to the minimization. 
    In other words, $\lowervalue_{\cor, t+1}^{\rho*}(\mu^{*}_{t+1}, \bnu_{\apprx, t+1}) \geq \min_{\nu_{t+1}\in \mathcal{R}_{\nu,t}^\rho(\mu_t, \nu_t)}
    \lowervalue_{\cor,t+1}^{\rho*}(\mu^*_{t+1}, \nu_{t+1})$ almost surely, since $\bm{\nu}_{\apprx,t+1}$ is in the reachable set by construction;
    the equality (vii) comes from the definition of $\mu^*_{t+1}$ in~\eqref{eqn:mu-star-def},
    and the final equality in~\eqref{eqn:main-8} is simply the definition of $\lowervalue^{\rho*}_{\cor,t}$, which completes the induction.

    Since the Red team strategy $\psi^{N_2} \in \Psi^{N_2}$ is arbitrary, we have that, for all $\mu \in \P(\X)$ and $\nu \in \P(\Y)$,
    \begin{align*}
         \min_{\psi^{N_2}\in \psi^{N_2}} J^{N, \alpha^*, \psi^{N_2}}&(\mu, \nu) = \min_{\psi^{N_2}\in \psi^{N_2}} J^{N, \alpha^*, \psi^{N_2}}_{0}(\mu, \nu) \\
         &\geq \lowervalue_{\cor,0}^{\rho*}(\mu, \nu) - \residue
         =\lowervalue_{\cor}^{\rho*}(\mu, \nu) - \residue.
    \end{align*}
    
\end{proof}


\begin{lemma}
    \label{lmm:cor-apprx-err}
    The following inequality holds for all distributions $\mu  \in \P(\X)$ and $\nu  \in \P(\Y)$
    \begin{equation}
        \label{eqn:cor-apprx-err-bound}
        \lowervalue_{\cor}^{\rho*}(\mu , \nu ) \geq \lowervalue^{N*}(\mu , \nu )- \residue.
    \end{equation}
\end{lemma}

\begin{proof}
    We prove the lemma through an inductive argument. 
    
    \textit{Base case:}
    At the terminal timestep $T$, the two value functions are the same. Thus, we have 
    \begin{equation*}
        \lowervalue_{\cor,T}^{\rho*}(\mu _T, \nu_T)
         = \underline{J}_{T}^{N*}(\mu _T, \nu_T), \quad  \mu _T \in \P(\X) \text{ and } \nu_T \in \P(\Y).
    \end{equation*} 

    \textit{Inductive hypothesis:}
    Assume that, at time step $t+1$, the following holds for all $\mu _{t+1} \in \P(\X)$ and $\nu_{t+1} \in \P(\Y)$
    \begin{equation}
        \lowervalue_{\cor, t+1}^{\rho*}(\mu _{t+1}, \nu_{t+1}) \geq 
        \underline{J}_{t+1}^{N*} (\mu _{t+1}, \nu_{t+1})- \residue.
    \end{equation}

    \textit{Induction:}
    Consider arbitrary $\mu _{t} \in \P(\X)$ and $\nu_{t} \in \P(\Y)$. 
    Denote $\mu_{t+1} = \mu _t F^\rho_t(\mu _t, \nu_t, \phi_t)$ and $\nu_{t+1} = \nu_t G^\rho_t(\mu _t, \nu_t, \psi_t)$. Then, we have
    \begin{align*}
        \underline{J}_{t}^{N*} &(\mu _{t}, \nu_{t}) 
        = \max_{\phi_t \in \Phi_t} \min_{\psi^{N_2}_t \in \Psi^{N_2}_t} r^\rho_{t} (\mu _{t}, \nu_{t}) + \mathbb{E}_{\phi_t,\psi_t^{N_2}} \Big[\underline{J}_{t+1}^{N*} (\bmu _{t+1}, \bnu_{t+1}) \vert \F_t \Big]\\
        & \stackrel{\text{(i)}}{\leq}   r^\rho_{t} (\mu _{t}, \nu_{t}) + \max_{\phi_t \in \Phi_t} \min_{\psi_t^{N_2} \in \Psi^{N_2}_t}\mathbb{E}_{\phi_t,\psi_t^{N_2}} \Big[\lowervalue_{\cor, t+1}^{\rho*}(\bmu _{t+1}, \bnu_{t+1}) \vert \F_t \Big] +\residue \\
        & \stackrel{\text{(ii)}}{\leq}  r^\rho_{t} (\mu _{t}, \nu_{t}) + \max_{\phi_t \in \Phi_t} \; \min_{\psi_t \in \Psi_t} \; \mathbb{E}_{\phi_t,\psi_t} \Big[\lowervalue_{\cor, t+1}^{\rho*}(\bmu _{t+1}, \bnu_{t+1}) \vert \F_t \Big] +\residue\\
        &\stackrel{\text{(iii)}}{=} r^\rho_{t} (\mu _{t}, \nu_{t}) + \max_{\pi_t \in \Pi_t} \; \min_{\sigma_t \in \Sigma_t} \; \mathbb{E}_{\pi_t,\sigma_t} \Big[\lowervalue_{\cor, t+1}^{\rho*}(\bmu _{t+1}, \bnu_{t+1}) \vert \F_t \Big] +\residue\\
        &\stackrel{\text{(iv)}}{\leq} r^\rho_{t} (\mu _{t}, \nu_{t}) +\residue \\
        &+ \max_{\pi_t \in \Pi_t} \; \min_{\sigma_t \in \Sigma_t} \; \mathbb{E}_{\pi_t,\sigma_t} \Big[\lowervalue_{\cor, t+1}^{\rho*}(\mu_{t+1}, \nu_{t+1}) + L_{J,t} \Big( \dtv{\bmu _{t+1},\mu_{t+1}} + \dtv{\bnu_{t+1},\nu_{t+1}} \Big)\vert \F_t \Big] \nonumber \\
        &= r^\rho_{t} (\mu _{t}, \nu_{t}) +\residue \\
        &+ \max_{\pi_t \in \Pi_t} \; \min_{\sigma_t \in \Sigma_t} \; \lowervalue_{\cor, t+1}^{\rho*}(\mu_{t+1}, \nu_{t+1}) + \underbrace{\mathbb{E}_{\pi_t,\sigma_t} \Big[ L_{J,t} \Big( \dtv{\bmu _{t+1},\mu_{t+1}} + \dtv{\bnu_{t+1},\nu_{t+1}} \Big)\vert \F_t \Big]}_{=\residue \text{ due to Lemma~\ref{lmm:mff}}} \nonumber \\
        &\stackrel{\text{(v)}}{=} r^\rho_{t} (\mu _{t}, \nu_{t})+ \max_{\pi_t \in \Pi_t} \; \min_{\sigma_t \in \Sigma_t} \; \lowervalue_{\cor, t+1}^{\rho*}(\mu_{t+1}, \nu_{t+1}) +\residue\\
        &\stackrel{\text{(vi)}}{=}  \lowervalue_{\cor, t}^{\rho*}(\mu_{t}, \nu_{t}) +\residue.
    \end{align*}
    For inequality (i), we used the inductive hypothesis; 
    for inequality (ii), we reduced the optimization domain of the Red team to the identical policy space;
    equality (iii) is due to the fact that given the current mean-fields, optimizing the team policy is equivalent to selecting the local policies (actions for the coordinators) that maximize or minimize the value;
    inequality (iv) is a result of the Lipschitz continuity of $\underline{J}^{\rho}_{\cor,t}$;
    equality (v) is a result of Lemma~\ref{lmm:mff};
    finally, equality vi is from the definition of the coordinator game value.
\end{proof}

% \section{Sensitivity regarding the Population Ratio}

% \todo
% \begin{equation*}
%     \underline{J}^{\hat{\rho}*}_{\cor,t} (\mu, \nu) \geq \underline{J}^{\rho*}_{\cor,t} (\mu, \nu) - 2\abs{\rho - \hat{\rho}} \Big(\sum_{\tau=t}^{T-1} L_{J,\tau} \Big)
% \end{equation*}

% \begin{equation*}
%     \bar{J}^{\hat{\rho}*}_{\cor,t} (\mu, \nu) \leq \bar{J}^{\rho*}_{\cor,t} (\mu, \nu) + 2\abs{\rho - \hat{\rho}} \Big(\sum_{\tau=t}^{T-1} L_{J,\tau} \Big)
% \end{equation*}

\section{Conclusion}
In this work, we formulated large-population zero-sum team games under the mean-field sharing information structure. 
Through a mean-field-type argument, we first approximated the large-population game with its limiting case where both teams have an infinite number of agents.
Leveraging the common information approach, we introduced two fictitious coordinators, one for each team, so that the infinite-population game is transformed into an equivalent zero-sum game between the two coordinators. 
The optimal coordinator strategies are constructed through dynamic programming and are translated back to feedback strategies the original agents deploy. 
Even though the optimal strategies are constructed assuming that the opponent team follows the same fictitious-coordinator method, we proved that the strategies constructed are still $\epsilon$-optimal for the original finite-population system and for the general class of strategies that the opponent may deploy.
Finally, we show that the suboptimality diminishes when the size of both teams approaches infinity.
Future work will investigate numerical aspects of the problem, i.e., the development of efficient algorithms to solve the continuous zero-sum coordinator game.

\section*{Acknowledgments:} 
%
The authors acknowledge fruitful discussions with A. Pakniyat.


\clearpage
\begin{appendices}

\section{Supporting Lemmas}
The following lemma is a modified version of the $\ell_2$ weak law of large numbers~\citep{chung2001course} that is adapted to accommodate the notion of empirical distributions used in this work.
This lemma will be used extensively in the later proofs. 
\begin{lemma}
    \label{lmm:l2-weak-law}
    Let $\x_1, \ldots, \x_N$ be $N$ i.i.d. random variables (vectors) taking values in a finite set $\X$.
    Suppose $\x_i$ is distributed according to $p \in \mathcal{P}(\X)$, such that $\mathbb{P}(\x_i = x) = p(x)$.
    Define the empirical distribution as
    \begin{equation*}
        \bm{\mu}^N(x) = \frac{1}{N}\sum_{i=1}^N \indicator{x}{\x_i}.
    \end{equation*}
    Then,
    \begin{equation}
        \mathbb{E}[\dtv{\bm{\mu}^N, p}] =\frac{1}{2} \sqrt{\frac{|\X|}{N}}.    
    \end{equation}
\end{lemma}
\begin{proof}
    Consider the random variables $\x_{x, i} = \indicator{x}{\x_i}$, which are i.i.d. across $i$ with mean $p(x)$.
    % As $\mu^N(x)$ is the sample mean of $X_{x, i}$, the strong law of large numbers implies $\mu^N(x) \xrightarrow{a.s.} p(x)$ as $N \to \infty$. 
    Since $\x_{x,i}$ only takes a value of 0 and 1, we have $\mathbb{E}[\x_{x,i}^2] = \mathbb{E}[\x_{x,i}]$. 
    The variance of $\x_{x,i}$ is then given by $\mathrm{Var}(\x_{x, i}) = p(x) - p(x)^2$.
    Furthermore, define $\bmu^N$ and $p$ to be the augmented vectors with elements $\mu^N(x)$ and $p(x)$, $x \in \mathcal{X}$ respectively. Then,
    \begin{align*}
        \expectation{\lVert \bm{\mu}^N - p \rVert_2^2 } &= \expectation{\sum_{x\in \X} \abs{\bm{\mu}^N(x) - p(x)}^2}
        = \mathbb{E}\Big[\sum_{x \in \X}\Big\vert\frac{1}{N}\sum_{i=1}^N (\x_{x,i}-p(x))\Big \vert ^2\Big]\\
        &= \frac{1}{N} \sum_{x \in \X} \mathrm{Var}(\x_{x,i})
        = \frac{1}{N} \sum_{x \in \X} (p(x) - p(x)^2)\\
        &= \frac{1}{N}(1 - \norm{p}_2^2) \leq \frac{1}{N}.
    \end{align*}
    By Jensen's inequality, we have $\expectation{\lVert \bm{\mu}^N - p \rVert_2} \leq \frac{1}{\sqrt{N}}$.
    Furthermore, due to equivalence of norms in the finite dimensional space $\X$, we have $\norm{\bm{\mu}^N - p}_1 \leq \sqrt{|\X|} \norm{{\bm{\mu}^N - p}}_2$ almost surely, where $|\X|$ is the cardinality of the set $\X$. 
    It then follows that 
    \begin{align*}
         \mathbb{E}[\dtv{\bm{\mu}^N, p}] = \frac{1}{2} \expectation{\norm{\bm{\mu}^N - p}_1} \leq \frac{1}{2} \sqrt{\frac{|\X|}{N}}.
    \end{align*}

\end{proof}

\begin{lemma}
    \label{appdx-lmm:sum}
    Suppose $a_1, \ldots, a_n$ are positive numbers. Then,
    \begin{equation*}
        \sum_{i=1}^n a_i \leq \sqrt{n} \left( \sqrt{\sum_{i=1}^n a_i^2} \right) .
    \end{equation*}
\end{lemma}
\begin{proof}
    By the positiveness of $a_i$ and Cauchy-Schwarz inequality, we have
    \begin{equation*}
        \sum_{i=1}^n a_i = \sum_{i=1}^n |a_i| \cdot 1 \leq \sqrt{\sum_{i=1}^n a_i^2} \sqrt{\sum_{i=1}^n 1} = \sqrt{n} \sqrt{\sum_{i=1}^n a_i^2}.
    \end{equation*}
\end{proof}



\section{Proof of the Approximation Results}


\subsection{Proof of the Approximation Proposition}
\label{appdx:prop-mf-apprx}
\mfp*

\begin{proof}
    Due to symmetry, we only prove the result for the Blue team.
    Let  $N_{1,t}^x$ be the number of Blue agents that are at state $x$ and time $t$ in the finite-population system. Then, from $\bmu^{N_1}_t = \mu_t^\rho$, we get that $N_{1,t}^x = N_1 \mu^\rho_t(x)$.
    Since all Blue agents apply an identical policy $\phi_t$, the next states for these $N_{1,t}^x$ agents are i.i.d..
    Specifically, if agent $i$ is one of these $N_{1,t}^x$ agents, the conditional distribution of its next state $\x^{N_1}_{i,t+1}$ is given by
    \begin{equation}
        \label{appdx-eqn-001}
        \mathbb{P}\left[\x_{i,t+1}^{N_1} = x' \big\vert , \x_{i,t}^{N_1} = x, \bmu^{N_1}_t=\mu^\rho_t, \bnu^{N_2}_t=\nu^\rho_t \right] = \sum_{u} f_t^\rho(x'|x,u,\mu^\rho_t, \nu^\rho_t) \phi_t(u|x, \mu^\rho_t, \nu^\rho_t).
    \end{equation}
    Define the empirical distribution of these $N_{1,t}^x$ Blue agents at time $t+1$ as
    \begin{equation*}
        \bmu_{t+1}^{N_{1,t}^x}(x') = \frac{1}{N_{1,t}^x} \sum_{i=1}^{N_1} \indicator{x}{\x_{i,t}^{N_1}}\indicator{x'}{\x_{i,t+1}^{N_1}}.
    \end{equation*}
    Note that, 
    \begin{equation}
        \label{appdx-eqn:001}
        \begin{aligned}
        \bmu_{t+1}^{N_1}(x') &= \frac{1}{N_1}\sum_{i = 1}^{N_1} \indicator{x'}{\x_{i,t+1}^{N_1}} = \frac{1}{N_1} \sum_{x\in \X} \sum_{i=1}^{N_1} \indicator{x}{\x_{i,t}^{N_1}}\indicator{x'}{\x_{i,t+1}^{N_1}} = \sum_{x\in\X} \frac{N_{1,t}^x}{N_1} \bmu_{t+1}^{N_{1,t}^x}(x').
        \end{aligned}
    \end{equation}

    
    Further define $\mu_{t+1}^{x}(x') = \sum_{u} f_t^\rho(x'|x,u,\mu^\rho_t, \nu^\rho_t) \phi_t(u|x, \mu^\rho_t, \nu^\rho_t)$ according to~\eqref{appdx-eqn-001}.
    From the mean-field dynamics~\eqref{eqn:mf-dynamics}, we have
    \begin{equation}
    \label{appdx-eqn:002}
        \begin{aligned}
            \mu^\rho_{t+1} (x') &= \sum_{x \in \X} \left[\sum_{u \in \U} f^\rho_t(x'|x, u, \mu_t, \nu_t )\; \phi_t(u|x,\mu^\rho_t, \nu^\rho_t) \right] \mu^\rho_t(x) \\
            &=\sum_{x \in \X}  \mu_t^\rho(x) \mu^x_{t+1}(x')
            = \sum_{x \in \X} \frac{N_{1,t}^{x}}{N_1} \mu^x_{t+1}(x').
        \end{aligned}
    \end{equation}


    
    From Lemma~\ref{lmm:l2-weak-law}, we have
    \begin{equation}
        \label{appdx-eqn:apprx-err-001}
        \mathbb{E} \left[\dtv{\bmu^{N_{1,t}^{x}}_{t+1}, \mu^{x}_{t+1}}\Big \vert \bmu^{N_1}_t=\mu^\rho_t, \bnu^{N_2}_t=\nu^\rho_t \right] \leq \frac{1}{2} \sqrt{\frac{|\X|}{N_{1,t}^x}}.
    \end{equation}
    Combining~\eqref{appdx-eqn:001}, \eqref{appdx-eqn:002} and~\eqref{appdx-eqn:apprx-err-001}, we have
    \begin{align*}
        \expct{\dtv{\bmu_{t+1}^{N_1}, \mu^\rho_{t+1}}\big\vert \bmu_{t}^{N_1} = \mu_t^\rho, \bnu_{t}^{N_2} = \nu_t^\rho}{} &\leq \sum_{x} \frac{N_1^x}{N_1}\expectation{\dtv{\bmu_{t+1}^{N^x_{1,t}}, \mu^x_{t+1}}\big\vert \bmu_{t}^{N_1} = \mu_t^\rho, \bnu_{t}^{N_2} = \nu_t^\rho} \\
        &\leq \sum_{x} \frac{N_1^x}{2N_1}\sqrt{\frac{|\X|}{N_{1,t}^x}}
        \leq \frac{|\X|}{2} \sqrt{\frac{1}{N_1}},
    \end{align*}
    where the last inequality is a result of Lemma~\ref{appdx-lmm:sum}.
 %   \pn{check last formula - compare with (73)}\sg{checked and updated}
\end{proof}

\subsection{Reachability Analysis of the Mean-field Approximation}
\label{appdx:mf-reachability}
\mfa*

\begin{proof}
Note that if $\bm{\nu}_t^{N_2}(y) = 0$, then the state $y$ has no contribution to the next mean-field propagated via~\eqref{eqn:apprx-blue-next-mf}. 
Consequently, we can assume that, for all states $y \in \Y$, the distribution $\bm{\nu}_t^{N_2} (y) > 0$, without loss of generality. 
For the rest of the proof, we ignore the second term in~\eqref{appdx-eqn:local-policy-apprx}, whose sole purpose is to ensure that the constructed local policy is admissible%
\footnote{Note that $\bnu_t^{N_2}(y)>0$ implies $\sum_{k}^{N_2}\indicator{y}{\y_{k,t}^{N_2}} >0$.}.


To ease the notation, we introduce the following random variable to denote the number of Red agents at time $t$ that are at state~$y$ and apply action~$v$.
    \begin{equation*}
        \bm{N}_{2,t}^{y,v} = \sum_{j=1}^{N_2}\mathds{1}_y(\y_{j,t}^{N_2}) \mathds{1}_v(\v_{j,t}^{N_2}).
    \end{equation*}
    Similarly, the number of Red agents that are at state $y$ is denoted as 
    \begin{equation*}
        \bm{N}_{2,t}^{y} = \sum_{j=1}^{N_2}\mathds{1}_y(\y_{j,t}^{N_2}) = N_2 \bm{\nu}^{N_2}_t(y).
    \end{equation*}
    Then, the identical policy in~\eqref{appdx-eqn:local-policy-apprx} can be equivalently written as 
    \begin{equation*}
        \bm{\sigma}_{\apprx, t}(v|y) = \frac{\bm{N}_{2,t}^{y,v}}{\bm{N}_{2,t}^{y}}.
    \end{equation*}
    
    Define 
    \begin{equation}
        \bm{\nu}_{t+1}^{\bm{N}_{2,t}^{y,v}} (y') = \frac{1}{\bm{N}_{2,t}^{y,v}} \sum_{j=1}^{N_2} \mathds{1}_y(\y_{j,t}^{N_2}) \mathds{1}_v(\v_{j,t}^{N_2}) \mathds{1}_{y'}(\y_{j,t+1}^{N_2}),
    \end{equation} 
    which represents the fraction of the $\bm{N}_{2,t}^{y,v}$ Red agents that apply action $v$ at state $y$ and reach $y'$ at the next timestep.
    Conditioned on $\F^+_t$, the next states of these $\bm{N}_{2,t}^{y,v}$ Red agents are i.i.d..
    Furthermore, from the dynamics of the game, if agent $j$ is among these $\bm{N}_{2,t}^{y,v}$ agents, its next state $\y^{N_2}_{j,t+1}$ satisfies 
    % \begin{equation}
    %     \mathbb{E}\left[\mu_{t+1}^{N_{1}^{y,u}}(y')|\F^+_t\right] = f^\rho_t(y'|y,u,\mu_t^{N_2},\nu_t^{N_2}).
    % \end{equation}
    \begin{equation}
        \mathbb{P}\left[\y^{N_2}_{j,t+1} = y'|\F^+_t\right] = g^\rho_t(y'|y,v,\bmu_t^{N_1},\bnu_t^{N_2}).
    \end{equation}
    Let $\bnu^{y,v}_{t+1}(y') = g^\rho_t(y'|y,v,\bm{\mu}_t^{N_1},\bm{\nu}_t^{N_2})$, which is $\F_{t}^+$-measurable. 
    From Lemma~\ref{lmm:l2-weak-law}, we have 
    \begin{equation}
        \label{appdx-eqn:mf-apprx-1}
        \mathbb{E}\left[\dtv{\bm{\nu}^{\bm{N}_{2,t}^{y,v}}_{t+1}, \bnu^{y,v}_{t+1}}\Big \vert \F^+_t\right] \leq \frac{1}{2} \sqrt{\frac{|\Y|}{\bm{N}_{2,t}^{y,u}}}.
    \end{equation}
    
    Under the policy $\bm{\sigma}_{\apprx, t}$ constructed using~\eqref{appdx-eqn:local-policy-apprx}, we have \begin{align*}
        \bm{\nu}_{\apprx, t+1}(y') &= \sum_{y\in \Y} \sum_{v \in \V} \left[ g^\rho_t(y'|y,v, \bm{\mu}_t^{N_1}, \bm{\nu}_t^{N_2}) \frac{\sum_{j=1}^{N_2}\mathds{1}_{y}(\y_{j,t}^{N_2}) \mathds{1}_v(\v_{j,t}^{N_2})}{\sum_{k=1}^{N_2}\mathds{1}_y (\y_{k,t}^{N_2})}\right] \bm{\nu}^{N_2}_t(y)\\
        &= \sum_{y\in \Y} \sum_{v \in \V} \left[ \bnu_{t+1}^{y,v}(y') \frac{\bN_{2,t}^{y,v}}{N_2\bnu_t^{N_2}(y)}\right] \bnu_t^{N_2}(y) \\
        &=\sum_{y,v} \frac{\bN_{2,t}^{y,v}}{N_2} \bnu_{t+1}^{y,v}(y').
    \end{align*}
    From the definition of $\bnu^{N_2}_{t+1}$, we also have, almost surely, that
    \begin{equation*}
        \bnu_{t+1}^{N_2}(y') 
        = \frac{\sum_{j=1}^{N_1} \indicator{y'}{\y_{j,t+1}^{N_2}}}{N_2}
        =\frac{\sum_{j=1}^{N_1} \sum_{y,v} \indicator{y'}{\y_{j,t+1}^{N_2}} \indicator{y}{\y_{j,t}^{N_2}}\indicator{v}{\v_{j,t}^{N_2}}}{N_2} 
        = \sum_{y,v} \frac{\bN_{2,t}^{y,v}}{N_2} \bnu_{t+1}^{\bN_{2,t}^{y,v}}(y').
    \end{equation*}
    Thus,
    \begin{equation}
        \label{appdx-eqn:2-b}
        \begin{aligned}
            \mathbb{E}\left[ \dtv{
            \bnu_{t+1}^{N_2}, \bnu_{\apprx,t+1}
            }\vert \mathcal{F}^+_t\right] &\stackrel{\text{(i)}}{\leq} \sum_{y, v} \frac{\bN_{2,t}^{y,v}}{N_2}
            \mathbb{E}\left[\dtv{\bnu_{t+1}^{\bN_{2,t}^{y,v}}, \bnu_{t+1}^{y,v}}\big \vert \F^+_t\right] \\
            &\stackrel{\text{(ii)}}{\leq} \sum_{y, v} \frac{\bN_{2,t}^{y,v}}{2N_2} \sqrt{\frac{|\Y|}{\bm{N}_{2,t}^{y,v}}}\\
            &= \frac{\sqrt{|\Y|}}{2N_2} \sum_{y, v}\sqrt{\bN_{2,t}^{y,v}} \\
            &\stackrel{\text{(iii)}}{\leq} \frac{\sqrt{|\Y|}}{2N_2} \sqrt{|\Y||\V|}\sqrt{\sum_{y,v}\bN_{2,t}^{y,v}} = \frac{|\Y|\sqrt{|\V|}}{2} \sqrt{\frac{1}{N_2}},
        \end{aligned}
    \end{equation}
    where inequality (i) comes from the triangle inequality, and inequality (ii) comes from~\eqref{appdx-eqn:mf-apprx-1}.
    Note that $\sum_{y,u} \bN_{2,t}^{y,u} = N_2$ and plugging in $a_i = \sqrt{\bN_{2,t}^{y,u}}$ from Lemma~\ref{appdx-lmm:sum}, we have inequality (iii).   
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mff*

\begin{proof}
    Similarly to the proof of Lemma~\ref{lmm:mf-apprx}, let  $\bN_{1,t}^x = N_1 \bmu^{N_1}_t(x)$ be the number of agents at state $x$ and at time $t$.
    Since all agents apply an identical policy $\pi_t$, the next states $\x^{N_1}_{i,t+1}$ for these agents are i.i.d. conditioned on $\F_t$. 
    Specifically, if agent $i$ is one of these $\bN_{1,t}^x$ agents, the conditional distribution of its next state $\x^{N_1}_{i,t+1}$ is given by
    \begin{equation*}
        \mathbb{P}\left[\x_{i,t+1}^{N_1} = x' \big\vert \F_t\right] = \sum_{u} f_t^\rho(x'|x,\bmu_t^{N_1}, \bnu_t^{N_2}) \pi_t(u|x).
    \end{equation*}
    Define 
    \begin{align*}
        \bmu_{t+1}^{\bN_{1,t}^x}(x') &= \frac{1}{\bN_{1,t}^x} \sum_{i=1}^{N_1} \indicator{x}{\x_{i,t}^{N_1}}\indicator{x'}{\x_{i,t+1}^{N_1}},\\
         \bmu_{t+1}^{x}(x') &= \sum_{u} f_t^\rho(x'|x,\bmu_t^{N_1}, \bnu_t^{N_2}) \pi_t(u|x).
    \end{align*}
    Note that $\bmu_{t+1}^{x}$ is $\F_t$-measurable. From Lemma~\ref{lmm:l2-weak-law}, we have
    \begin{equation}
        \label{appdx-eqn:apprx-err-2}
        \mathbb{E} \left[\dtv{\bmu^{\bN_{1,t}^{x}}_{t+1}, \bmu^{x}_{t+1}}\Big \vert \F_t\right] \leq \frac{1}{2} \sqrt{\frac{|\X|}{\bN_{1,t}^x}}.
    \end{equation}
    Recall that the next mean-field propagated from the current empirical distribution $\bmu^{N_1}_t$ under the local policy $\pi_t$ is given by
    \begin{equation}
        \bmu_{t+1}(x') = \sum_{x}\left[\sum_{u} f_t^\rho(x'|x,\bmu_t^{N_1}, \bnu_t^{N_2}) \pi_t(u|x)\right] \bmu^{N_1}_t(x).
    \end{equation}
    Since $\bmu_t^{N_1}(x) = \bN_{1,t}^x / N_1$, we have $\bmu_{t+1} = (\sum_x \bN_{1,t}^x \bmu^{x}_{t+1} )/{N_1}$.
    It can also been shown that $\bmu^{N_1}_{t+1} = (\sum_x \bN_{1,t}^x\bmu^{\bN^x_{1,t}}_{t+1})/{N_1}$.
    Thus, using~\eqref{appdx-eqn:apprx-err-2}, we have
    \begin{equation*}
        \expct{\dtv{\bmu_{t+1}^{N_1}, \bmu_{t+1}}\big\vert \F_t}{} \leq \sum_{x} \frac{\bN_1^x}{N_1}\expectation{\dtv{\bmu_{t+1}^{\bN^x_{1,t}}, \bmu^x_{t+1}}\big\vert \F_t} 
        \leq \sum_{x} \frac{\bN_1^x}{2N_1}\sqrt{\frac{|\X|}{\bN_{1,t}^x}}
        \leq \frac{|\X|}{2} \sqrt{\frac{1}{N_1}},
    \end{equation*}
 %   \pn{check last formula - compare with (73)}\sg{checked and updated}
    which follows the same techniques used in~\eqref{appdx-eqn:2-b}.
\end{proof}

\section{Proof of the Continuity Results}
\label{appdx-sec:Continuity}

\rrl*
\begin{proof}
    Note that
    \begin{align*}
        \abs{r^\rho_t(\mu,\nu)-r^\rho_t(\mu',\nu')} &= \abs{\sum_{x} r_{1,t}(x) (\mu(x)- \mu'(x)) - \sum_{y} r_{2,t}(y) (\nu(y)- \nu'(y))}\\
        &\leq \abs{\sum_{x} r_{1,t}(x) (\mu(x)- \mu'(x))} + \abs{\sum_{y} r_{2,t}(y) (\nu(y)- \nu'(y))}\\
        &\leq \sum_{x} \abs{r_{1,t}(x)} \abs{\mu(x)- \mu'(x)} + \sum_{y} \abs{r_{2,t}(y)}\abs{\nu(y)- \nu'(y)}\\
        &\leq r_{1,\max} \sum_{x} \abs{\mu(x)- \mu'(x)} + r_{2,\max}\sum_{y} \abs{\nu(y)- \nu'(y)}\\
        &\leq \max\{r_{1,\max}, r_{2,\max}\} \big(2\dtv{\mu, \mu'} + 2\dtv{\nu, \nu'}\big).
    \end{align*}
\end{proof}

\begin{lemma}
    \label{appdx-lmm:min-lip}
    Let $g:\X \times \Y \to \mathbb{R}$ be a continuous function, such that $\abs{g(p,q) - g(p',q)}\leq L_g \norm{p-p'}$ for all $p,p' \in \X$ and $q\in \Y$.
    Then,  the following holds for all compact set $Q \subseteq \Y$:
    \begin{equation*}
        \abs{\min_{q \in Q} g(p,q) - \min_{q'\in Q} g(p',q')} \leq L_g \norm{p-p'}.
    \end{equation*}
\end{lemma}
\begin{proof}
    Since $Q$ is compact and $g$ is continuous, the two minimization problems are well-defined.
    Let $h(q) = g(p,q)$ and $h'(q) = g(p', q)$. 
    From the Lipschitz continuity of $g$, we have for all $q \in \Y$, $\abs{h(q) - h'(q)} \leq L_g \norm{p-p'}.$
    Let $q^* \in \argmin_{q \in Q} h(q)$, then we have
    \begin{align*}
        \min_{q'\in Q} g(p',q') - \min_{q \in Q} g(p,q) &=  \min_{q'\in Q} h'(q') -\min_{q \in Q} h(q) = \min_{q'\in Q} h'(q') - h(q^*)\\
        &\leq h'(q^*) - h(q^*) \leq L_g\norm{p-p'}.
    \end{align*}
    Similarly, let $q^{\prime *} \in \argmin_{q' \in Q} h'(q')$, then we have
    \begin{align*}
        \min_{q \in Q} g(p,q) -\min_{q'\in Q} g(p',q') &=  \min_{q \in Q} h(q) -\min_{q'\in Q} h'(q') = \min_{q\in Q} h(q) - h'(q^{\prime *})\\
        &\leq h(q^{\prime *}) - h'(q^{\prime *}) \leq L_g\norm{p-p'}.
    \end{align*}
\end{proof}


\mmm*

% \textcolor{red}{$ | g(p,q) - g(p',q')| \le L_g ( \|p-p'\| + \|q-q'\|)$}

\begin{proof}
    Let $h(x, y, p) = \min_{q\in \Theta(x,y)}g(p,q) $. 
    Since $g(p,q)$ is continuous and $\Theta(x,y)$ is compact, the minimization is well-defined for each $p$.
    Fix $p \in \X$, and consider $x,x' \in \X$ and $ y,y'\in \Y$. 
    The Lipschitz continuity of $\Theta$ implies that
    \begin{equation}
        \distH{\Theta(x,y), \Theta(x',y')} \leq L_\Theta(\norm{x-x'}+ \norm{y-y'}).
        \label{eqn:marginal-3}
    \end{equation}
    For simplicity, let $\epsilon = (\norm{x-x'}+ \norm{y-y'})$.
    Consider a minimizer $q^* \in \argmin_{q \in \Theta(x,y)} g(p,q)$. 
    Inequality~\eqref{eqn:marginal-3} implies that there exists $q^{*\prime} \in \Theta(x',y')$, such that $\norm{q^* - q^{*\prime}}\leq L_\Theta \epsilon$. 
    Since $g$ is Lipschitz, it follows that
    \begin{equation*}
        \abs{g(p,q^*) - g(p, q^{*\prime})} \leq L_g L_\Theta \epsilon.
    \end{equation*}
    Then,
    \begin{align*}
        h(x,y,p) = \min_{q\in \Theta(x,y)} g(p,q) = g(p, q^*) &\geq g(p,q^{*\prime}) - L_g L_\Theta \epsilon \\
        &\geq \min_{q'\in \Theta(x',y')} g(p,q')- L_g L_\Theta\epsilon = h(x',y',p) - L_g L_\Theta \epsilon.
    \end{align*}
    Through a similar argument, we can show that $h(x', y', p) \geq h(x,y,p) - L_g L _\theta \epsilon$, and thus we have
    \begin{equation}
        \abs{h(x,y,p) - h(x',y',p)} \leq L_g L_\Theta \epsilon, \label{eqn:marginal-0}
    \end{equation}
    which implies that $h(x,y,p)$ is $L_g L_\theta$-Lipschitz continuous with respect to its $x$ and $y$ arguments. 
    
    Next, fix $(x, y)$ and consider $p, p' \in \Y$. It follows from Lemma~\ref{appdx-lmm:min-lip} that
    %
%    \pn{not sure about this step} \sg{added a new lemma.}
%
    \begin{align}
        \label{eqn:marginal-2}
        \abs{h(x,y,p) - h(x,y,p')} = \abs{\min_{q \in \Theta(x,y)} g(p,q) - \min_{q' \in \Theta(x,y)}g(p', q')} \leq L_g \norm{p-p'},
    \end{align}
    where the inequality comes form the Lipschitz continuity of $g$.
    Thus, $h(x,y,p)$ is $L_g$-Lipschitz continuous with respect to its $p$ argument.
    
    Finally, consider $x,x' \in \X$ and $ y,y'\in \Y$, it follows that
    %
     %  \pn{not sure about this step} \sg{updated.}
    \begin{align}
        \big\vert f(x,y) &- f(x',y')\big\vert   = \abs{\max_{p \in \Gamma(x,y)} h(x,y,p)- \max_{p' \in \Gamma(x',y')} h(x',y',p')}\\
        &\leq \abs{\max_{p \in \Gamma(x,y)} h(x,y,p)- \max_{p \in \Gamma(x,y)} h(x',y',p)} + 
        \abs{\max_{p \in \Gamma(x,y)} h(x',y',p)- \max_{p' \in \Gamma(x',y')} h(x',y',p')} \label{eqn:marginal-1}\\
        &\leq L_g L_\Theta \epsilon + L_\Gamma L_g \epsilon
        = L_g(L_\Theta + L_\Gamma) \epsilon,
    \end{align}
    where the first term in~\eqref{eqn:marginal-1} is bounded using Lemma~\ref{appdx-lmm:min-lip} and the Lipschitz constant derived in~\eqref{eqn:marginal-0}; the second term in~\eqref{eqn:marginal-1} is bounded using the Lipschitz constant in~\eqref{eqn:marginal-2} and the same techniques used to derive~\eqref{eqn:marginal-0}. 
\end{proof}

\vlc*
\begin{proof}
    The proof is given by induction.
    
    \textit{Base case: } Recall that at time $T$, the optimal coordinator value function is given by  
    \begin{equation*}
        \lowervalue_{\cor,T}^{\rho*}(\mu^\rho_T, \nu^\rho_T) = r^\rho_T(\mu^\rho_T, \nu^\rho_T).
    \end{equation*}
    From Remark~\ref{rmk:reward-L-cont}, the Lipschitz constant for $\lowervalue_{\cor, T}^{\rho*}$ is $L_r$.
    
    \vspace{+5pt}
    \textit{Inductive hypothesis: }
    Suppose that the Lipschitz constant is given by~\eqref{eqn:value-L-constant} at time $t+1$.
    
    \vspace{+5pt}
    \textit{Induction: }
    Recall the definition of the optimal coordinator value at time $t$:
    \begin{equation*}
        \lowervalue_{\cor,t}^{\rho*}(\mu^\rho_t, \nu^\rho_t) = r_t(\mu^\rho_t, \nu^\rho_t) +
        \max_{\mu_{t+1}^\rho \in \mathcal{R}_{\mu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}~ \min_{\nu_{t+1}^\rho \in \mathcal{R}_{\nu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu^\rho_{t+1}, \nu^\rho_{t+1}),
    \end{equation*}
    where the second term takes the form of a max-min marginal function defined in~\eqref{eqn:max-min-marginal-def}, where $\Gamma = \R^\rho_{\mu,t}$, $\Theta = \R^\rho_{\nu,t}$ and $g = \lowervalue_{\cor,t+1}^{\rho*}$, while the first term is Lipschitz continuous with Lipschitz constant $L_r$.
    Applying Lemma~\ref{lmm:min-max-marginal-L-cont}, it follows that, for all $\mu^\rho_t, \mu^{\rho\prime}_t \in \P(\X)$ and $\nu^\rho_t, \nu^{\rho\prime}_t \in \P(\Y)$,
    %
  %  \pn{I believe the subscripts are messed up in these equations}
    % \sg{updated.}
    \begin{align*}
        &\Big \vert \lowervalue_{\cor,t}^{\rho*}(\mu^\rho_t, \nu^\rho_t) - \lowervalue_{\cor,t}^{\rho*}(\mu^{\rho\prime}_t, \nu^{\rho\prime}_t)\Big \vert  \leq \abs{r_t(\mu^\rho_t, \nu^\rho_t) - r_t(\mu^{\rho\prime}_t, \nu^{\rho\prime}_t)} + \\
        & \abs{
        \max_{\mu_{t+1}^\rho \in \mathcal{R}_{\mu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}~ \min_{\nu_{t+1}^\rho \in \mathcal{R}_{\nu,t}^\rho(\mu_t^\rho,\nu_t^\rho)}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu^\rho_{t+1}, \nu^\rho_{t+1})
        -
        \max_{\mu_{t+1}^{\rho\prime} \in \mathcal{R}_{\mu,t}^{\rho\prime}(\mu_t^{\rho\prime},\nu_t^{\rho\prime})}~ \min_{\nu_{t+1}^{\rho\prime} \in \mathcal{R}_{\nu,t}^{\rho\prime}(\mu_t^{\rho\prime},\nu_t^{\rho\prime})}
        \lowervalue_{\cor,t+1}^{\rho*}(\mu^{\rho\prime}_{t+1}, \nu^{\rho\prime}_{t+1})}\\
        &\leq \bigg(L_r + (L_{\R^\rho_{\mu,t}} + L_{\R^\rho_{\nu,t}}) L_r \Big( 1+ \sum_{k=t+1}^{T-1} \prod_{\tau =t+1}^{k} (L_{\R^\rho_{\mu,\tau}} + L_{\R^\rho_{\nu,\tau}})\Big)\bigg) \Big(\dtv{\mu^\rho_t, \mu^{\rho\prime}_t} + \dtv{\nu^\rho_t, \nu^{\rho\prime}_t}\Big)\\
        & = \bigg(L_r \Big( 1+ (L_{\R^\rho_{\mu,t}} + L_{\R^\rho_{\nu,t}}) + \sum_{k=t+1}^{T-1} (L_{\R^\rho_{\mu,t}} + L_{\R^\rho_{\nu,t}}) \prod_{\tau =t+1}^{k}  (L_{\R^\rho_{\mu,\tau}} + L_{\R^\rho_{\nu,\tau}})\Big)\bigg) \Big(\dtv{\mu^\rho_t, \mu^{\rho\prime}_t} + \dtv{\nu^\rho_t, \nu^{\rho\prime}_t}\Big)\\
        &=\bigg(L_r \Big( 1+ (L_{\R^\rho{_\mu,t}} + L_{\R^\rho_{\nu,t}}) + \sum_{k=t+1}^{T-1} \prod_{\tau =t}^{k} (L_{\R^\rho_{\mu,\tau}} + L_{\R^\rho_{\nu,\tau}})\Big)\bigg) \Big(\dtv{\mu^\rho_t, \mu^{\rho\prime}_t} + \dtv{\nu^\rho_t, \nu^{\rho\prime}_t}\Big) \\
        &=\bigg(L_r \Big( 1+ \sum_{k=t}^{T-1} \prod_{\tau =t}^{k} (L_{\R^\rho_{\mu,\tau}} + L_{\R^\rho_{\nu,\tau}})\Big)\bigg) \Big(\dtv{\mu^\rho_t, \mu^{\rho\prime}_t} + \dtv{\nu^\rho_t, \nu^{\rho\prime}_t}\Big),
    \end{align*}
    which completes the induction.
\end{proof}

\subsection{Continuity of the Reachable Correspondences}

\begin{definition}[Pure local policy]
    \label{appdx-def:pure-policy}
    An admissible Blue local policy $\pi_t \in \Pi_t$ is said to be pure if $\pi_t(u|x) \in \{0,1\}$ for all $u \in \U$ and $x \in \X$.
    We use $\hat{\Pi}_t = \{\hat{\pi}^k\}_{k=1}^{|\U|^{|\X|}}$ to denote the set of pure Blue policies. 
    The pure Red local policies are defined similarly.
\end{definition}


\label{appdx-sec:Continuity-RSet}
\RLC*
\begin{proof}
    Due to symmetry, we only present the proof for the Blue reachability correspondence. 
    From Proposition~\ref{appdx-lmm:Rset-convex-hull}, we know that the reachable set can be characterized as the convex hull
    \begin{equation*}
        \R^\rho_{\mu,t}(\mu_t, \nu_t) = \Co{\{\mu_t F_t^\rho(\mu_t,\nu_t, \hat{\pi}^k\}_{k=1}^{|\U|^{|\X|}}},
    \end{equation*}
    where $\hat{\pi}^k$ is a pure local policy defined in Definition~\ref{appdx-def:pure-policy}.
    In other words, all the extreme points of the reachable set are the next mean-fields that can be induced by the pure policies. 
    Lemma~\ref{appdx-lmm:vertex-perturb} states that for each extreme policy $\hat{\pi}_k$, if one perturbs the current mean-fields slightly, the change of the next induced mean-field is bounded, i.e., 
    \begin{equation*}
        \dtv{\mu_t F^\rho_t(\mu_t, \nu_t, \hat{\pi}_t), \mu_t' F^\rho_t(\mu'_t, \nu'_t, \hat{\pi}^k)} \leq \big(\rho |\X| + 1\big) \dtv{\mu_t, \mu'_t} + (1-\rho) |\X| \dtv{\nu_t, \nu'_t}.
    \end{equation*}
    %Note that these constants are independent of $\hat{\pi}_t^k$.
    The above result implies that if one perturbs the current mean-field slightly, the extreme points of the reachable sets also change slightly.
    Note that the reachable sets are bounded polytopes, which means that a reachable set is completely characterized by its extreme points. 
    Intuitively, a slight change in the extreme points of a bounded polytope implies that the polytope only moves slightly. 
    This intuition is formalized in Lemma~\ref{appdx-lmm:vertex-movement}, with which we conclude that 
    \begin{equation}
        \distH{\mathcal{R}_{\mu,t}(\mu_t, \nu_t), \mathcal{R}_{\mu,t}(\mu'_t, \nu'_t))} \leq \big(\rho |\X|+1\big) \dtv{\mu_t, \mu'_t} + (1-\rho) |\X|  \dtv{\nu_t, \nu'_t}.
    \end{equation}

\end{proof}



\begin{proposition}
    \label{appdx-lmm:Rset-convex-hull}
    The Blue reachable set is characterized as 
    \begin{equation}
        \label{appdx-eqn:Rset-characterization}
        \R^\rho_{\mu,t}(\mu_t, \nu_t) = \Co{\{\mu_t F_t^\rho(\mu_t,\nu_t, \hat{\pi}^k\}_{k=1}^{|\U|^{|\X|}}},
    \end{equation}
    where $\hat{\pi}^k$ are pure Blue local policies, and $\mathrm{Co}$ denotes the convex hull. 
    The Red reachable set can be characterized similarly.
\end{proposition}

\begin{proof}
    Due to symmetry, we only present the proof for the characterization of the Blue reachable set.
    
    First, note that the set of admissible policies $\Pi_t$ can be characterized as the convex hull of the pure policies, i.e., $\Pi_t = \Co{\{\hat{\pi}^k\}_k}$.
    
    Next, we show that the mean-field propagation rule is linear with respect to the policy. 
    Consider arbitrary current mean-fields $\mu_t$ and $\nu_t$,
    and consider a policy $\pi_t = \lambda \pi_t^1 + (1-\lambda) \pi_t^2$ for some $\lambda \in (0,1)$ and $\pi_t^1, \pi_t^2 \in \Pi_t$.
    Then, the next mean-field is given by
    \begin{align*}
        \mu_{t+1}(x') &= \sum_{x} \left[\sum_u f^\rho_t(x'|x, \mu_t, \nu_t) \pi_t(u|x)\right]\mu_t(x) \\
        &= \lambda \sum_{x} \left[\sum_u f^\rho_t(x'|x, \mu_t, \nu_t) \pi^1_t(u|x)\right]\mu_t(x) + (1-\lambda) \sum_{x} \left[\sum_u f^\rho_t(x'|x, \mu_t, \nu_t) \pi^2_t(u|x)\right]\mu_t(x),
    \end{align*}
    which then implies
    \begin{equation}
        \label{appd-eqn:linear-mf-dynamics}
        \mu_{t+1} = \mu_t F^\rho_t(\mu_t, \nu_t, \pi_t) = \lambda \mu_t F^\rho_t(\mu_t, \nu_t, \pi^1_t) + (1-\lambda) \mu_t F^\rho_t(\mu_t, \nu_t, \pi^2_t).
    \end{equation}
    
  %  With the above observations one can easily show the characterization in~\eqref{appdx-eqn:Rset-characterization  through double inclusion. 
    
    Consider $\mu_{t+1} \in \R^\rho_{\mu,t}(\mu_t, \nu_t)$. Then there exists a policy $\pi_t$ such that $\mu_{t+1} = \mu_{t}F^\rho_t(\mu_t, \nu_t, \pi_t)$.
    Since the admissible policy set is the convex hull of pure policies, we have $\pi_t = \sum_k \lambda_k \hat{\pi}^k$, where $\lambda_k \geq 0$ and $\sum_k \lambda_k = 1$.
    It follows directly from~\eqref{appd-eqn:linear-mf-dynamics} that
    %
    %\pn{check these equations} \sg{added equation~\eqref{appd-eqn:linear-mf-dynamics} to explain.}
    %
    \begin{equation}
        \mu_{t+1} = \sum_{k} \lambda_k \mu_{t}F^\rho_t(\mu_t, \nu_t, \hat{\pi}^k) \in \Co{\{\mu_t F_t^\rho(\mu_t,\nu_t, \hat{\pi}^k\}_{k=1}^{|\U|^{|\X|}}}.
    \end{equation}
    
    Consider a point $\mu_{t+1} \in \Co{\{\mu_t F_t^\rho(\mu_t,\nu_t, \hat{\pi}^k\}_{k=1}^{|\U|^{|\X|}}}$. 
    By definition, we have 
    \[\mu_{t+1} = \sum_{k} \lambda_k  \mu_t F_t^\rho(\mu_t,\nu_t, \hat{\pi}^k)
    =\mu_t F_t^\rho(\mu_t,\nu_t, \pi_t),\]
    where $\pi_t = \sum_k {\lambda_k \hat{\pi}^k} \in \Pi_t$, which implies $\mu_{t+1} \in \RMSet{t}{\rho}$    
\end{proof}

\begin{lemma}
    \label{appdx-lmm:vertex-perturb}
    Consider an extreme policy $\hat{\pi} \in \hat{\Pi}_t$ and arbitrary mean-fields $\mu_t, \mu'_t \in \P(\X)$ and $\nu_t, \nu'_t\in \P(\Y)$. The following bound holds: 
    \begin{equation*}
        \dtv{\mu_t F^\rho_t(\mu_t, \nu_t, \hat{\pi}), \mu_t' F^\rho_t(\mu'_t, \nu'_t, \hat{\pi})} \leq \big(\rho |\X| + 1\big) \dtv{\mu_t, \mu'_t} + (1-\rho) |\X| \dtv{\nu_t, \nu'_t}.
    \end{equation*}

\end{lemma}
\begin{proof}
    Using the triangle inequality, we have that
    \begin{align*}
        \dtv{\mu_t F^\rho_t(\mu_t, \nu_t, \hat{\pi}), \mu_t' F^\rho_t(\mu'_t, \nu'_t, \hat{\pi})} &\leq \underbrace{\dtv{\mu_t F^\rho_t(\mu_t, \nu_t, \hat{\pi}), \mu_t' F^\rho_t(\mu_t, \nu_t, \hat{\pi})}}_{A} \\
        & \qquad \qquad \qquad \qquad \qquad \qquad + \underbrace{\dtv{\mu'_t F^\rho_t(\mu_t, \nu_t, \hat{\pi}), \mu_t' F^\rho_t(\mu'_t, \nu'_t, \hat{\pi})}}_{B}. 
    \end{align*}
    
    Since $F^\rho_t$ is a stochastic matrix, it is a non-expansive operator under the 1-norm, and hence
    $A \leq \dtv{\mu_t, \mu'_t}$. 
    Next, we bound the term $B$. 
    Define $f_{1,t,\max} = \max_{x',z,u,x} f_{1,t}(x'|z,u,x)$ and $f_{2,t,\max} = \max_{x',z,u,y} f_{2,t}(x'|z,u,y)$.
    Plugging in the definition of $F_t^\rho$ in~\eqref{eqn:coordinator-dynamics-matrix} we have
    \begin{align*}
        B &= \frac{1}{2} \norm{\mu'_t (F^\rho_t(\mu_t, \nu_t, \hat{\pi}) -F^\rho_t(\mu'_t, \nu'_t, \hat{\pi}))}_1\\
        & = \frac{1}{2} \sum_{x'} \Bigg\vert\sum_{z} \mu'_t(z) \bigg[\sum_{u}\Big(\rho \sum_x  f_{1,t}(x'|z,x,u) \mu_t(x) + (1-\rho) \sum_y f_{2,t}(x'|z,u,y)\nu_t(y)\Big)\hat{\pi}(u|z)\bigg]\\
        & \qquad \quad -\sum_{z} \mu'_t(z) \bigg[\sum_{u}\Big(\rho \sum_x  f_{1,t}(x'|z,x,u) \mu'_t(x) + (1-\rho) \sum_y f_{2,t}(x'|z,u,y)\nu'_t(y)\Big)\hat{\pi}(u|z)\bigg]
        \Bigg\vert\\
        &\leq \frac{1}{2} \sum_{x',z} \mu_t'(z) \bigg[\sum_{u}\Big(\rho \sum_x  f_{1,t}(x'|z,x,u) \big \vert \mu_t(x)-\mu'_t(x)\big \vert + (1-\rho) \sum_y f_{2,t}(x'|z,u,y) \big \vert \nu_t(y) - \nu'_t(y) \big \vert \Big)\hat{\pi}(u|z)\bigg]\\
        &\leq \frac{1}{2}\sum_{x'} \sum_{z} \mu_t'(z) \bigg[\sum_{u}\Big(\rho f_{1,t,\max} \sum_x   \abs{\mu_t(x)-\mu'_t(x)} + (1-\rho) f_{2,t, \max} \sum_y \abs{\nu_t(y) - \nu'_t(y)}\Big)\hat{\pi}(u|z)\bigg]\\
        &=\sum_{x'} \sum_{z} \mu_t'(z) \bigg[\sum_{u}\Big(\rho \, f_{1,t,\max} \, \dtv{\mu_t, \mu'_t} + (1-\rho) \, f_{2,t, \max} \, \dtv{\nu_t, \nu'_t}\Big)\hat{\pi}(u|z)\bigg]\\
        &= \Big(\rho |\X| f_{1,t,\max}\Big) \dtv{\mu_t, \mu'_t} + \Big((1-\rho) |\X| f_{2,t, \max}\Big) \dtv{\nu_t, \nu'_t}.
     \end{align*}
     
     
    Combining the bounds and noting that $f_{1,t,\max}$ and $f_{2,t,\max}$ are both less than one, we have 
    \begin{equation*}
        \dtv{\mu_t F^\rho_t(\mu_t, \nu_t, \hat{\pi}), \mu_t' F^\rho_t(\mu'_t, \nu'_t, \hat{\pi})} \leq \big(\rho |\X| + 1\big) \dtv{\mu_t, \mu'_t} + (1-\rho) |\X| \dtv{\nu_t, \nu'_t}.
    \end{equation*}
    
\end{proof}

    
\begin{lemma}
    \label{appdx-lmm:vertex-movement}
    Suppose that two sets of points $\{x_i\}_{i=1}^N$ and $\{y_i\}_{i=1}^N$ satisfy  $\norm{x_i - y_i} \leq \epsilon$ for all $i$. 
    Then, the corresponding convex hulls satisfy
    \begin{equation*}
        \distH{\Co{\{x_i\}_{i=1}^N}, \Co{\{y_i\}_{i=1}^N}} \leq \epsilon.
    \end{equation*}
\end{lemma}

\begin{proof}
    Consider a point $x \in \Co{\{x_i\}_{i=1}^N}$. 
    By definition, there exists a set of non-negative coefficients $\{\lambda_i\}_{i=1}^N$, such that $x = \sum_{i} \lambda_i x_i$ and $\sum_i \lambda_i = 1$.
    Let $y = \sum_i \lambda_i y_i  \in \Co{\{y_i\}_{i=1}^N}$.
    We then have 
    \begin{equation*}
        \norm{x-y} \leq \sum_{i} \lambda_i \norm{x_i - y_i} \leq \epsilon.
    \end{equation*}
    Since $x$ is arbitrary, the above inequality implies 
    \[\sup_{x\in \Co{\{x_i\}_{i=1}^N}} \inf_{y \in \Co{\{y_i\}_{i=1}^N}} \norm{x-y} \leq \epsilon.\]
    Through a similar argument, we can conclude that $\sup_{y\in \Co{\{y_i\}_{i=1}^N}} \inf_{x \in \Co{\{x_i\}_{i=1}^N}} \norm{x-y} \leq \epsilon,$ which completes the proof.
\end{proof}





\section{Proof of Existence of Game Values}
\label{appdx-sec:game-value}

We first examine the concavity of the reachability correspondences. 
\begin{definition}[\citep{kuroiwa1996convexity}]
    Let $\X$, $\Y$ and $\Z$ be convex sets. 
    The correspondence $\Gamma: \X \times \Y \rightsquigarrow \Z$ is convex with respect to $\X$ if for all $x_1, x_2 \in \X$, $y \in \Y$,  $z_1 \in \Gamma(x_1, y)$, $z_2 \in \Gamma(x_2, y)$, and $\lambda \in (0,1)$, there exists $z \in \Gamma(\lambda x_1 + (1-\lambda) x_2, y)$ such that 
    \begin{equation*}
        z = \lambda z_1 + (1-\lambda) z_2.
    \end{equation*}
\end{definition}

\begin{remark}
    The above definition of convexity is equivalent to  the following
    \begin{equation}
        \label{eqn:convex-correspondence-def}
        \Gamma(\lambda x_1 + (1-\lambda) x_2, y) \supseteq \{z| z=\lambda z_1 + (1-\lambda) z_2 \, \text{ where } \, z_1 \in \Gamma(x_1, y), z_2 \in \Gamma(x_2, y)\},
    \end{equation}
    for all $\lambda \in (0,1)$, $x_1,x_2 \in \X$ and $y \in \Y$.  
\end{remark}

\begin{definition}
    Let $\X$, $\Y$ and $\Z$ be convex. 
    The correspondence $\Gamma: \X \times \Y \rightsquigarrow \Z$ is constant with respect to $\Y$ if for all $x \in \X$ and $y_1, y_2 \in \Y$, the following holds:
    \begin{equation}
        \label{eqn:constant-correspondence-def}
        \Gamma(x, y_1) = \Gamma(x, y_2).
    \end{equation}
    
\end{definition}

We have the following concave-convex result for a max-min marginal function in~\eqref{eqn:max-min-marginal-def}. 
\begin{restatable}{lemma}{lcc}
    \label{lmm:concave-convex}
    Consider two compact-valued correspondences $\Gamma: \X \times \Y \rightsquigarrow  \X$ and $\Theta: \X \times \Y \rightsquigarrow \Y$. 
    Let $\Gamma$ be convex with respect to $\X$ and constant with respect to $\Y$, and let $\Theta$ be constant with respect to $\X$ and convex with respect to $\Y$.
    Let $g: \X \times \Y \to \mathbb{R}$ be concave-convex. 
    Then, the max-min marginal function $f(x, y) = \max_{x\in \Gamma(x,y)} \min_{y \in \Theta(x,y)} g(x,y)$ is also concave-convex.
\end{restatable}

\begin{proof}
    Fix $y \in \Y$, and consider $x_1, x_2 \in \X$ and $\lambda \in (0,1)$. Denote $x = \lambda x_1 + (1-\lambda) x_2$. It follows that
    \begin{align*}
        f(\lambda x_1 + (1-\lambda)x_2, y ) &= \max_{p \in \Gamma(\lambda x_1 + (1-\lambda)x_2, y)} \; \min_{q\in \Theta(\lambda x_1 + (1-\lambda)x_2, y)} g(p,q)\\
        &\stackrel{\text{(i)}}{\geq} \max_{\substack{p_1 \in \Gamma(x_1,y)\\ p_2 \in \Gamma(x_2,y)}} \min_{q\in \Theta(x, y)} g(\lambda p_1 + (1-\lambda)p_2, q)\\
        &\stackrel{\text{(ii)}}{\geq}\max_{\substack{p_1 \in \Gamma(x_1,y)\\ p_2 \in \Gamma(x_2,y)}} \min_{q\in \Theta(x,y)} (\lambda g(p_1,q) + (1-\lambda)g(p_2, q)) \\
        &\stackrel{\text{(iii)}}{\geq} \max_{\substack{p_1 \in \Gamma(x_1,y)\\ p_2 \in \Gamma(x_2,y)}}  (\underbrace{\min_{q_1\in \Theta(x,y)}\lambda g(p_1,q_1)}_{A(p_1)} + \underbrace{\min_{q_2\in \Theta(x,y)}(1-\lambda)g(p_2, q_2)}_{B(p_2)}) \\
        & \stackrel{\text{(iv)}}{=} \lambda \max_{p_1 \in \Gamma(x_1,y)} \min_{q_1 \in \Theta(x,y)} g(p_1, q_1) + (1-\lambda) \max_{p_2 \in \Gamma(x_2,y)} \min_{q_2 \in \Theta(x,y)} g(p_2, q_2)\\
        & \stackrel{\text{(v)}}{=} \lambda \max_{p_1 \in \Gamma(x_1,y)} \min_{q_1 \in \Theta(x_1,y)} g(p_1, q_1) + (1-\lambda) \max_{p_2 \in \Gamma(x_2,y)} \min_{q_2 \in \Theta(x_2,y)} g(p_2, q_2)\\
        &= \lambda f(x_1, y) + (1-\lambda) f(x_2, y),
    \end{align*}
    %
    %\pn{need to check these}
    %
    where inequality (i) 
    %\sg{should I change (i) to (I)?} 
    holds from restricting the maximization domain using the convexity of $\Gamma$ in~\eqref{eqn:convex-correspondence-def}; 
    inequality (ii) holds from the assumption that $g$ is concave with respect to its $p$-argument; 
    inequality (iii) is the result of distributing the minimization;
    in equality (v), we distribute the maximization due to the independence between the terms $A(p_1)$ and $B(p_2)$;
    equality (v) is due to $\Theta$ being constant with respect to $\X$.
    The above result implies the concavity of $f$ with respect to its $x$-argument.
    
    Fix $x \in \X$, and let $y_1, y_2 \in \Y$ and $\lambda \in (0,1)$. Denote $y = \lambda y_1 + (1-\lambda) y_2$. Then,
    \begin{align*}
        f(x, \lambda y_1 + (1-\lambda) y_2) &= \max_{p \in \Gamma(x,\lambda y_1 + (1-\lambda) y_2)} \; \min_{q\in \Theta(x,\lambda y_1 + (1-\lambda) y_2)} g(p,q)\\
        &\leq \max_{p\in \Gamma(x,y)} \min_{\substack{q_1\in \Theta(x,y_1) \\ q_2 \in \Theta(x, y_2)}} g(p, \lambda q_1 + (1-\lambda) q_2)\\
        &\leq \max_{p\in \Gamma(x,y)} \min_{\substack{q_1\in \Theta(x, y_1) \\ q_2 \in \Theta(x, y_2)}}(\lambda g(p,q_1) + (1-\lambda)g(p, q_2)) \\
        & \leq \lambda \max_{p_1 \in \Gamma(x, y)} \min_{q_1 \in \Theta(x,y_1)} g(p_1, q_1) + (1-\lambda) \max_{p_2 \in \Gamma(x, y)} \min_{q_2 \in \Theta(x,y_2)} g(p_2, q_2)\\
        & = \lambda \max_{p_1 \in \Gamma(x, y_1)} \min_{q_1 \in \Theta(x,y_1)} g(p_1, q_1) + (1-\lambda) \max_{p_2 \in \Gamma(x, y_2)} \min_{q_2 \in \Theta(x,y_2)} g(p_2, q_2)\\
        &= \lambda f(x, y_1) + (1-\lambda) f(x, y_2),
    \end{align*}
    which implies that $f$ is convex with respect to its $y$-argument.
\end{proof}


The following lemma shows the convexity of the reachability correspondences $\R^\rho_{\mu,t}$ and $\R^\rho_{\nu,t}$ under independent dynamics.

\begin{restatable}{lemma}{lrc}
    \label{lmm:rset-indep-dynamics}
    Suppose that the dynamics in~\eqref{eqn:weakly-dynamics} for each individual agent is independent of other agent's state, i.e.,
    the dynamics satisfies 
    \begin{align*}
        f_{1,t} (x_{i,t+1}^N | x_{i,t}^{N_1}, u_{i,t}^{N_1}, x) &= f_{2,t}(x^{N_1}_{i,t+1}\big \vert x^{N_1}_{i,t}, u^{N_1}_{i,t}, y) = f_{t}(x_{i,t+1}^N | x_{i,t}^{N_1}, u_{i,t}^{N_1}), \quad x\in \X, y \in \Y,
        \\
        g_{1,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}, x )  &=
        g_{2,t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t},  y) = g_{t}(y^{N_2}_{j,t+1} \big \vert y^{N_2}_{j,t}, v^{N_2}_{j,t}), \quad x\in \X, y \in \Y. 
    \end{align*}
    Then, the reachability correspondences satisfy
    \begin{subequations}
        \begin{align}
            \R^\rho_{\mu,t}(\mu_t, \nu_t) &= \R^\rho_{\mu,t}(\mu_t, \nu'_t),\quad \nu_t, \nu'_t \in \P(\Y), \label{eqn:constant-blue-reachability}
            \\
            \R^\rho_{\nu,t}(\mu_t, \nu_t) &= \R^\rho_{\nu,t}(\mu'_t, \nu_t),\quad \mu_t, \mu'_t \in \P(\X).
            \label{eqn:constant-red-reachability}
        \end{align}
    \end{subequations}
    Furthermore, for all $\lambda \in [0,1]$, 
    \begin{subequations}
        \begin{align}
            \R^\rho_{\mu,t}\Big( \lambda \mu_t + (1-\lambda) \mu'_t, \nu_t \Big) &= \left\{
            \bar{\mu}_{t+1} = \lambda\mu_{t+1} + (1-\lambda)\mu_{t+1}' \big \vert \mu_{t+1} \in \R^\rho_{\mu,t}(\mu_t, \nu_t), \mu'_{t+1} \in \R^\rho_{\mu,t}(\mu'_t, \nu_t)
            \right\}, \label{eqn:concave-blue-reachability}
            \\
            \R^\rho_{\nu,t}\Big(\mu_t, \lambda \nu_t + (1-\lambda) \nu'_t \Big) &= \left\{\bar{\nu}_{t+1} = 
            \lambda\nu_{t+1} + (1-\lambda)\nu_{t+1}' \big \vert \nu_{t+1} \in \R^\rho_{\nu,t}(\mu_t, \nu_t), \nu'_{t+1} \in \R^\rho_{\nu,t}(\mu_t, \nu'_t)
            \right\}. \label{eqn:concave-red-reachability}
        \end{align}
    \end{subequations}
\end{restatable}


\begin{proof}
    Due to symmetry, we only prove the results for the Blue reachability correspondence.
    The results for the Red team can be obtained through a similar argument. 
    
    Consider an arbitrary pair of distributions $\mu_t \in \P(\X)$ and $\nu_t \in \P(\Y)$. 
    If $\mu_{t+1} \in \R_{\mu,t}^\rho (\mu_t, \nu_t)$ under the independent dynamics in~\eqref{eqn:independent-dynamics},  there exists a local policy $\pi_t \in \Pi_t$ such that
    \begin{equation*}
        \mu_{t+1}(x') = \sum_{x \in \X} \Big[\sum_{u \in \U} f_t(x'|x,u)\pi_t(u|x)\Big] \mu_t(x),
    \end{equation*}
    which is independent of $\nu_t$. 
    Consequently, 
    \begin{equation*}
        \R_{\mu,t}^\rho (\mu_t, \nu_t) = \R_{\mu,t}^\rho (\mu_t, \nu'_t), \quad \forall~ \mu_t \in \P(\X) \text{ and } \forall~\nu_t, \nu'_t \in \P(\Y).
    \end{equation*}
    
    Next, we prove the property in~\eqref{eqn:concave-blue-reachability}.
    Consider two Blue mean-fields $\mu_t, \mu'_t \in \P(\X)$ and a Red mean-field $\nu_t \in \P(\Y)$. 
    
    Let $\bar{\mu}_{t+1} \in \R^\rho_{\mu,t}(\lambda \mu_t + (1-\lambda)\mu'_t, \nu_t)$. 
    From the definition of the reachable set, there exists a local policy $\pi_t \in \Pi_t$ such that
    \begin{align*}
        \bar{\mu}_{t+1}(x') &= \sum_{x \in \X} \Big[\sum_{u \in \U} f_t(x'|x,u) \pi_t(u|x) \Big] (\lambda \mu_t(x) + (1-\lambda)\mu_t'(x))  \\
        &=\lambda \underbrace{\sum_{x \in \X} \Big[\sum_{u \in \U} f_t(x'|x,u) \pi_t(u|x) \Big]  \mu_t(x) }_{\mu_{t+1}(x')}
        +(1-\lambda) \underbrace{\sum_{x \in \X} \Big[\sum_{u \in \U} f_t(x'|x,u) \pi_t(u|x) \Big]  \mu_t'(x)}_{\mu'_{t+1}(x')},
    \end{align*}
    where $\mu_{t+1}\in \R^\rho_{\mu,t} (\mu_t,\nu_t)$ and $\mu'_{t+1}\in \R^\rho_{\mu,t} (\mu'_t,\nu_t)$.
    Hence, we have 
    \[\R^\rho_{\mu,t}(\lambda \mu_t + (1-\lambda)\mu'_t, \nu_t) \subseteq \left\{
            \bar{\mu}_{t+1} = \lambda\mu_{t+1} + (1-\lambda)\mu_{t+1}' \big \vert \mu_{t+1} \in \R^\rho_{\mu,t}(\mu_t, \nu_t), \mu'_{t+1} \in \R^\rho_{\mu,t}(\mu'_t, \nu_t)
            \right\}.\]
    
    \vspace{+5pt}
    
    Let now $\bar{\mu}_{t+1} = \lambda \mu_{t+1} + (1-\lambda) \mu_{t+1}'$, where $\mu_{t+1} \in \R^\rho_{\mu,t}(\mu_t, \nu_t)$ and $\mu_{t+1}' \in \R^\rho_{\mu,t}(\mu_t', \nu_t)$. 
    Further, define $\bar{\mu}_t = \lambda \mu_t + (1-\lambda) \mu'_t$.
    From the definition of the reachable set, there exists admissible local policies $\pi_t$ and $\pi'_t$ such that 
    \begin{align*}
        \bar{\mu}_{t+1} &= \lambda \sum_{x \in \X} \Big[\sum_{u \in \U} f_t(x'|x,u) \pi_t(u|x) \Big]  \mu_t(x)
        +(1-\lambda) \sum_{x \in \X} \sum_{u \in \U} f_t(x'|x,u) \pi'_t(u|x) \Big]  \mu_t'(x) \\
        &= \sum_{x \in \X} \sum_{u \in \U} f_t(x'|x,u) \Big[ \lambda \pi_t(u|x) \mu_t(x) + (1-\lambda) \pi'_t(u|x) \mu'_t(x)\Big] \\
        &= \sum_{x \in \X} \sum_{u \in \U} \mathds{1}_{\bar{\mu}_t(x) >0} \; f_t(x'|x,u) \Big[ \lambda \pi_t(u|x) \mu_t(x) + (1-\lambda) \pi'_t(u|x) \mu'_t(x)\Big] \\
        & + \sum_{x \in \X} \sum_{u \in \U} \mathds{1}_{\bar{\mu}_t(x) =0}  \; f_t(x'|x,u) \underbrace{\Big[ \lambda \pi_t(u|x) \mu_t(x) + (1-\lambda) \pi'_t(u|x) \mu'_t(x)\Big]}_{=0 \, \footnotemark} \\
        &= \sum_{x \in \X} \sum_{u \in \U} \mathds{1}_{\bar{\mu}_t(x) >0} \; f_t(x'|x,u) \frac{ \lambda \pi_t(u|x) \mu_t(x) + (1-\lambda) \pi'_t(u|x) \mu'_t(x)}{\bar{\mu}_t(x)} \bar{\mu}_t(x) \\
        &+\sum_{x \in \X} \sum_{u \in \U} \mathds{1}_{\bar{\mu}_t(x) =0} \; f_t(x'|x,u) \underbrace{\Big[ \lambda \pi_t(u|x) + (1-\lambda) \pi'_t(u|x) \Big] \bar{\mu}_{t}(x)}_{=0} \\
        &= \sum_{x \in \X} \left[\sum_{u \in \U} f_t(x'|x,u) \bar{\pi}_t(u|x)\right] \bar{\mu}_t(x),
    \end{align*}
    \footnotetext{Since the condition $\bar{\mu}_t(x)=0$ implies that $\mu_t(x) =0$ and $\mu_t'(x) = 0$.}
    
    where the ``averaged" local policy $\bar{\pi}_t$ is given by
    \begin{equation*}
        \bar{\pi}_t (u|x) = \mathds{1}_{\bar{\mu}_t(x) > 0}  \frac{\lambda\pi_t(u|x)\mu_t(x) + (1-\lambda) \pi'_t(u|x) \mu'_t(x)}{\lambda \mu_t(x) + (1-\lambda) \mu'_t(x)} + \mathds{1}_{\bar{\mu}_t(x) = 0} \Big(\lambda\pi_t(u|x) + (1-\lambda) \pi'_t(u|x) \Big).
    \end{equation*}
    Consequently, we have
    \begin{equation*}
        \bar{\mu}_{t+1} \in \R^\rho_{\mu,t} (\lambda \mu_t + (1-\lambda) \mu_{t}', \nu_t).
    \end{equation*}
     Hence,  
     \[\R^\rho_{\mu,t}(\lambda \mu_t + (1-\lambda)\mu'_t, \nu_t) \supseteq \left\{
            \bar{\mu}_{t+1} = \lambda\mu_{t+1} + (1-\lambda)\mu_{t+1}' \big \vert \mu_{t+1} \in \R^\rho_{\mu,t}(\mu_t, \nu_t), \mu'_{t+1} \in \R^\rho_{\mu,t}(\mu'_t, \nu_t)
            \right\}.\]
    
    
\end{proof} 

Since the rewards in~\eqref{eqn:mf-rewards} are linear with respect to the mean-fields, the above characterization of the reachability correspondences under decoupled dynamics enables us to show that the optimal value function is concave-convex. 

\begin{corollary}
    \label{thm:concave-convex}
    Under independent dynamics, both the lower and upper game values %in~\eqref{eqn:lower-j-ab-t-rset} and~\eqref{eqn:upper-j-ab-t-rset} 
    are concave-convex at each timestep. 
\end{corollary}
\begin{proof}
    Note that the lower (upper) game value has the form of a max-min (min-max) marginal function. 
    Furthermore, the game value at time $T$ is directly the mean-field reward, which is linear with respect to both the Blue and Red mean-fields.
    One can then provide an inductive proof leveraging Lemma~\ref{lmm:concave-convex} and Lemma~\ref{lmm:rset-indep-dynamics}.
\end{proof}

\begin{theorem}
    Under independent dynamics, the game value exists. 
    That is, the lower game value and the upper game value coincides. 
\end{theorem}
\begin{proof}
    From Theorem~\ref{thm:concave-convex}, we have that the lower game value is concave-convex, together with the compactness of the reachable sets, one can apply the minimax theorem~\cite{owen2013game}, which guarantees that the max-min optimal value is the same as the min-max optimal value, and thus ensures the existence of the game value.
\end{proof}






% \section{Proof of Lemma~\ref{lmm:approx}}
% \begin{definition}
%     \label{def:strategy-lipschitz}
%     A Blue policy $\phi$ is Lipschitz continuous with respect to the mean-fields if for arbitrary mean-field flows $\mu, \mu' \in \mathcal{M}$ and $\nu,\nu'\in \mathcal{N}$, the following condition holds for all time step $t$,
%     \begin{equation}
%         \max_{x \in \X} \; \dtv{\phi_t(x, \mu_t,\nu_t), \phi_t(x, \mu'_t,\nu'_t)} \leq L_\phi \Big(\dtv{\mu_t, \mu'_t} + \dtv{\nu_t,\nu'_t}\Big),
%     \end{equation}
%     where $L_\phi$ is the Lipschitz constant for the Blue policy $\phi$.
%     The Lipschitz continuity of a Red policy $\psi$ is defined similarly, and we use $L_\psi$ as its Lipschitz constant. 
%     % Similarly, Red policy $\psi$ is Lipschitz continuous with Lipschitz constant $L_\psi$ if for arbitrary mean-field flows $\mu, \mu' \in \mathcal{M}$ and $\nu,\nu'\in \mathcal{N}$ and for all timesteps $t$,  it satisfies
%     % \begin{equation}
%     %     \max_{y \in \Y} \; \dtv\left(\psi_t(y, \mu_t,\nu_t), \psi_t(y, \mu'_t,\nu'_t)\right) \leq L_\psi \Big(\dtv\left(\mu_t, \mu'_t\right) + \dtv\left(\nu_t,\nu'_t\right)\Big).
%     % \end{equation}
% \end{definition}

% \label{appdx:proof-approx}
% \approx*
% \begin{proof}
%     Let $\mathcal{F}_t = \{\x^{N_1}_0, \y^{N_2}_0,\u^{N_1}_0,\v^{N_2}_0, \ldots, \x^{N_1}_t, \y^{N_2}_t, \u^{N_1}_t, \v^{N_2}_t\}$ be the history of the system up to time $t$.
%     Note that the empirical distributions are functions of the random variables in $\F_t$, that is $\mu^{N_1}_t = \frac{1}{N_1}\sum_{i=1}^{N_1} x^{N_1}_{i,t} \in \F_t$ and $\nu^{N_2}_t = \frac{1}{N_2}\sum_{j=1}^{N_2} y^{N_2}_{j,t} \in \F_t$.
%     We provide a proof of the lemma based on induction. 
    
    
%     \textit{Base Case:}
%     Under the assumption that the initial state distributions of Blue agents and Red agents are i.i.d. and are sampled according to some initial mean-fields $\mu^\rho_0$ and $\nu^\rho_0$. Formally,
%     \begin{alignat*}{2}
%         x^{N_1}_{i,0} &\sim \mu^\rho_0 \quad \forall \; i \in N_1, \quad \text{and} \quad
%          y^{N_2}_{j,0} &\sim \nu^\rho_0 \quad \forall \; j \in N_2.
%     \end{alignat*}
    
%     From Lemma~\ref{lmm:l2-weak-law} and treating $\mu^\rho_0$ as $p$, we directly have
%     \begin{equation*}
%         \expectation{\dtv{\mu^\rho_0, \mu_0^{N_1}}} \leq \frac{1}{2}\sqrt{\frac{|\X|}{N_1}} \leq \residue.
%     \end{equation*}
%     Through a similar argument, we get the desired error bounds for the Red team.
    
%     \textit{Inductive hypothesis: } 
%     Suppose at some timestep $t \geq 1$, we have both empirical distributions satisfy the bounds
%     \begin{align*}
%         \expectation{\dtv{\mu^\rho_t, \mu_t^{N_1}}} &= \order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}, \\
%         \expectation{\dtv{\nu^\rho_t, \nu_t^{N_2}}} &= \order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}.
%     \end{align*}
    
%     \vspace{+5pt}
%     \textit{Induction step: } 
%     We want to show that the same bounds holds at timestep $t+1$. 
%     Due to the symmetry between the Blue and Red teams, we will only show the results for the Blue empirical distribution $\mu^{N_1}_{t+1}$. 
%     First, we break down the error bounds into three components, and we will analyze them individually.
%     \begin{equation}
%     \label{appdx-eqn:mf-approx-3-terms}
%         \begin{aligned}
%             &\expectation{\dtv{\mu_{t+1}^{N_1}, \mu^\rho_{t+1}}} = \expectation{\dtv{\mu_{t+1}^{N_1}, \; \mu^\rho_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}} \\
%             & \leq 
%             \underbrace{\expectation{\dtv{\mu_{t+1}^{N_1}, \; \mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t))}}}_{A} +
%             \underbrace{\expectation{\dtv{\mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t), \; \mu^{N_1}_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}}}_{B}\\
%             &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad +\underbrace{\expectation{\dtv{\mu^{N_1}_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t), \; \mu^\rho_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}}}_{C}.
%         \end{aligned}
%     \end{equation}

    
%     For term $A$, we rewrite it using the tower rule, 
%     \begin{equation}
%         A = \expectation{\expectation{\dtv{\mu_{t+1}^{N_1}, \; \mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t)}\vert \F_t}}.
%     \end{equation}
%     Note that $\mu^{N_1}_t, \nu^{N_2}_t \in \F_t$ and $\expectation{\mu^{N_1}_{t+1}\vert \F_t} = \mu^N_t F^\rho(\mu^{N_1}_t,\nu^{N_1}_t)$. 
%     Through a similar argument used in Lemma~\ref{lmm:mff}, we have that, with probability 1,
%     \begin{equation}
%         \expectation{\dtv{\mu_{t+1}^{N_1}, \; \mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t)}\vert \F_t} \leq \frac{1}{2}\sqrt{\frac{|\X|}{N_1}}. 
%     \end{equation}
%     Consequently, term $A$ satisfies
%     \begin{equation}
%         A = \expectation{\expectation{\dtv{\mu_{t+1}^{N_1}, \; \mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t)}\vert \F_t}} \leq \frac{1}{2}\sqrt{\frac{|\X|}{N_1}} = \order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}.
%     \end{equation}
    
%     For term $B$, we first rewrite it as
%     \begin{equation}
%         B = \frac{1}{2} \expectation{\norm{\mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t) -  \mu^{N_1}_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}_1}.
%     \end{equation}
    
%     From Lemma~\ref{appdx-lmm:P-1} and the assumption on the Lipschitz continuity of the policies, we have 
%     \begin{equation*}
%         \norm{\mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t) -  \mu^{N_1}_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}_1 \leq |\X|(1+ |\U|L_\phi + |\V| L_\psi) \left(\norm{\mu_t^{N_1} - \mu^\rho_t}_1 + \norm{\nu_t^{N_2} - \nu^\rho_t}_1\right).
%     \end{equation*}
%     Together with the inductive hypothesis, we have, with probability 1, that 
%     \begin{equation*}
%         \norm{\mu^{N_1}_t F_t^{\rho}(\mu^{N_1}_t,\nu^{N_1}_t, \phi^\rho_t) -  \mu^{N_1}_t F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}_1= \order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}.
%     \end{equation*}
%     Consequently, $B = \order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}$.
    
%     Finally, for term $C$, we rewrite it as
%     \begin{equation*}
%         C = \frac{1}{2} \expectation{\norm{(\mu^{N_1}_t - \mu^\rho_t) F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}_1}.
%     \end{equation*}
%     Since all stochastic matrix is a non-expansive operator in 1-norm (see Lemma~\ref{appdx-lmm:P-2}), we have that 
%     $\norm{(\mu^{N_1}_t - \mu^\rho_t) F_t^{\rho}(\mu^\rho_t,\nu^\rho_t, \phi^\rho_t)}_1 \leq \norm{\mu^{N_1}_t - \mu^\rho_t}_1$. Together with the inductive hypothesis, we again have 
    
%     \begin{equation*}
%         C \leq \frac{1}{2} \expectation{\norm{\mu_t^{N_1}-\mu^\rho_t}_1} =\order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}
%     \end{equation*}
    
%     With all three terms of the desired order, we have shown that $\expectation{\dtv{\mu_{t+1}^{N_1} - \mu_{t+1}}} = \order{\frac{1}{\sqrt{\min\{N_1, N_2\}}}}$, which completes the induction step. 
%     The proof for the Red team empirical distribution can be constructed in the a similar manner. 
% \end{proof}


% The following auxiliary lemmas are used in the proof of Lemma~\ref{lmm:approx}.
% \begin{lemma}
% \label{appdx-lmm:P-1}
%     Given two distributions $\mu, \mu' \in \P(\X)$ and two distributions $\nu, \nu' \in \P(\Y)$. 
%     For arbitrary $\xi \in \P(\X)$ and Lipschitz continuous Blue policy $\phi$ with Lipschitz constant $L_\phi$, we have
%     \begin{equation*}
%         \norm{\xi \Big(F^{\rho}(\mu,\nu,\phi) - F^{\rho}(\mu',\nu',\phi)\Big)} \leq |\X|(1+ \rho |\U|L_\phi + (1-\rho)|\V| L_\psi) \left(\norm{\mu-\mu'}_1 + \norm{\nu-\nu'}_1\right),
%     \end{equation*}
%     where $L_\phi$ is the Lipschitz constant for the Blue policy $\phi$.
% \end{lemma}
% \begin{proof}
%     We first bound the element-wise difference of the transition matrix $F^\rho$ due to deviating from $(\mu,\nu)$ to $(\mu',\nu')$.
%     \begin{align*}
%         \Big\vert\big[F^\rho(\mu,\nu,\phi)\big]_{ji} - \big[F^\rho&(\mu',\nu',\phi)\big]_{ji}\Big\vert \\
%         &=
%         \Bigg\vert \rho \sum_{u} \sum_x f_1(i|j,u,x)\phi(u|j, \mu,\nu)\mu(x) +
%         (1-\rho)\sum_{u} \sum_y f_2(i|j,u,y)\phi(u|j, \mu,\nu)\nu(y)\\
%         &-\left(\rho\sum_{u'} \sum_{x'} f_1(i|j,u',x')\phi(u'|j, \mu',\nu')\mu'(x') +
%         (1-\rho)\sum_{u'} \sum_y' f_2(i|j,u',y')\phi(u'|j, \mu',\nu')\nu'(y)\right) \Bigg \vert \\
%         & \leq \rho \underbrace{\abs{\sum_u \sum_x f_1(i|j, u, x) \Big(\phi(u|j, \mu,\nu) \mu(x) - \phi(u|j, \mu',\nu')\mu'(x)\Big)}}_{D} \\
%         &\qquad \qquad \qquad \qquad \qquad +(1-\rho) \underbrace{\abs{\sum_u \sum_y f_2(i|j, u, y) \Big(\phi(u|j, \mu,\nu) \nu(y) - \phi(u|j, \mu',\nu')\nu'(y)\Big)}}_{E}.
%     \end{align*}
%     Since terms $D$ and $E$ has symmetric structures, we only analyze term $D$.
%     \begin{align*}
%         D &\leq \underbrace{\abs{\sum_u \sum_x f_1(i|j, u, x) \Big(\phi(u|j, \mu,\nu) \mu(x) - \phi(u|j, \mu,\nu)\mu'(x)\Big)}}_{D_1}\\
%         &\qquad \qquad \qquad \qquad \qquad \qquad 
%         + \underbrace{\abs{\sum_u \sum_x f_1(i|j, u, x) \Big(\phi(u|j, \mu,\nu) \mu'(x) - \phi(u|j, \mu',\nu')\mu'(x)\Big)}}_{D_2}
%     \end{align*}
%     For $D_1$, we have
%     \begin{align*}
%         D_1 &\leq \sum_{u} \phi(u|j,\mu,\nu) \sum_x f_1(i|j,u, x) \abs{\mu(x)-\mu'(x)} \\
%         &\leq \sum_{u} \phi(u|j,\mu,\nu) \sum_x \abs{\mu(x)-\mu'(x)} = \norm{\mu-\mu'}_1.
%     \end{align*}
%     For $D_2$, we have
%     \begin{align*}
%         D_2 &\leq \sum_u \sum_x \mu'(x) f_1(i|j,u, x) \abs{\phi(u|j, \mu,\nu) - \phi(u|j, \mu',\nu')}\\
%         &\leq \sum_u \sum_x \mu'(x) f_1(i|j,u, x) L_\phi \big(\norm{\mu-\mu'}_1 +\norm{\nu-\nu'}_1 \big)\\
%         &\leq \sum_u \sum_x \mu'(x) L_\phi \big(\norm{\mu-\mu'}_1 +\norm{\nu-\nu'}_1 \big)\\
%         &= |\U|L_\phi \big(\norm{\mu-\mu'}_1 +\norm{\nu-\nu'}_1 \big).
%     \end{align*}
%     Similarly, one can show that the $E$ term has the following bounds:
%     \begin{equation}
%         E \leq \norm{\nu-\nu'}_1 + |\V|L_\phi \big(\norm{\mu-\mu'}_1 +\norm{\nu-\nu'}_1 \big).
%     \end{equation}
%     Consequently, we have
%     \begin{align*}
%         \Big\vert\big[F^\rho(\mu,\nu)\big]_{ji} - \big[F^\rho(\mu',\nu')\big]_{ji}\Big\vert 
%         &\leq (\rho+ \rho|\U|L_\phi + (1-\rho)|\V| L_\psi) \norm{\mu_t^{N_1} - \mu_t}_1 \\
%         & \qquad \qquad +(1-\rho+ \rho|\U|L_\phi + (1-\rho)|\V| L_\psi) \norm{\nu_t^{N_2} - \nu_t}_1. \\
%         & \leq (1 + \rho|\U|L_\phi + (1-\rho)|\V| L_\psi) \Big(\norm{\mu_t^{N_1} - \mu_t}_1 + \norm{\nu_t^{N_2} - \nu_t}_1\Big)
%     \end{align*}
%     Finally, we go back the statement in the lemma. 
%     \begin{align*}
%          \norm{\xi \Big(F^\rho(\mu,\nu) - F^\rho(\mu',\nu')\Big)}_1 
%          &\leq \sum_{i} \sum_j \xi(j) \abs{[F^\rho(\mu,\nu)]_{ji}-[F^\rho(\mu',\nu')]_{ji}}\\
%          &\leq \sum_{i} \sum_j \xi(j)  (1+ |\U|L_\phi + |\V| L_\psi) \left(\norm{\mu_t^{N_1} - \mu_t}_1 + \norm{\nu_t^{N_2} - \nu_t}_1\right)\\
%          & = |\X|  (1+ \rho|\U|L_\phi + (1-\rho)|\V| L_\psi) \left(\norm{\mu_t^{N_1} - \mu_t}_1 + \norm{\nu_t^{N_2} - \nu_t}_1\right)
%     \end{align*}
    
    
% \end{proof}


% \begin{lemma}
% \label{appdx-lmm:P-2}
% Given an arbitrary right stochastic matrix $P \in \mathbb{R}^{N\times N}$ and two arbitrary probability distributions $\mu, \nu \in \Delta_{N}$. 
% We have that $\norm{\mu P - \nu P}_1 \leq \norm{\mu - \nu}$.
% \end{lemma}
% \begin{proof}
% Since the row sum of right stochastic matrix is unity, we have
% \begin{align*}
%     \norm{\mu P - \nu P}_1 &= \sum_{j} \abs{\sum_i \left[P\right]_{ij} (\mu_i - \nu_i)}\\
%     &\leq \sum_j \sum_i \left[P\right]_{ij} \abs{\mu_i - \nu_i}
%     = \sum_i \abs{\mu_i - \nu_i} \sum_j \left[P\right]_{ij} \\
%     &= \sum_i \abs{\mu_i - \nu_i} = \norm{\mu_i - \nu_i}_1.
% \end{align*}
% \end{proof}

\end{appendices}



%\clearpage
\bibliographystyle{apalike}
\bibliography{references}

\end{document}

\clearpage
\section{Scenarios}
\subsection{Grid World Battle}
We consider a battle game played between two teams over a (finite) grid world illustrated in Figure~\ref{fig:grid-battle-demo} where a team of Red agents tries to break through the line of defense of the Blue team and reach the high value target (green cell).
Each individual agent's state is characterized by its position and its activation status. 
Each agent exerts a control force (action) on itself to steer its trajectory. Aside from the control force, each of the rest of the agents in the environment exerts a influence force on that agent. 
This inter-agent force depends on the distance between the agents, as well as the types of the agents. 
The agent then makes its probabilistic transition to the next cell based on the total force. 
The activation status is also influenced by other agents. 
All agents within a neighborhood of the agent can `vote' to change the status of that agent (activate or deactivate).
That agent's status changes probabilistically based on the votes it receives. If an agent is deactivated, it is frozen and does not have influence on other agents' states until it is reactivated. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.3\linewidth]{Figures/grid-battle-demo.png}
    \caption{An illustrative example of the grid world battle scenario.}
    \label{fig:grid-battle-demo}
\end{figure}






We denote the collection of cells as $\mathcal{C}$, where each cell $c \in \mathcal{C} $ corresponds to a 2D position $p_c = \Theta(c)\in \mathbb{R}^2$. 
With a slight abuse of notation, we use $c$ to denote the position $p_c$ as well.
The state of Blue agent $i$ is given by $x^{N_1}_{i,t}=(c_{i,t}^x, h_{i,t}^x)$, where $c_{i,t}^x \in \mathcal{C}$ is the position state of the agent within the grid world, and $h^x_{i,t} \in \{0, 1\}$ is the status state of the agent. 
If $h^x_{i,t} = 0$, the agent is inactive and will have no effect on the dynamics of the system, and vice versa.
Similarly, the state of Red agent $j$ is given by $y^{N_2}_{j,t}=(c_{j,t}^y, h_{j,t}^y)$.
The action spaces $\U$ and $\V$ are finite sets of 2D vectors, that can be interpreted as the force applied by an agent to steer itself. 
For simplicity, we assume that $\U$ is a finite subset of a unit ball, and then $u^{N_1}_{i,t} \in \U$ is the force that Blue agent $i$ applies at timestep $t$.

The state transition is decomposed into two components: (i) the spatial transition and (ii) the status state transition. 
For example, under the weakly coupled dynamics, the influence of Red agent $j$ on the Blue agent $i$ is given by
\begin{equation}
    f_2(x^{N_1}_{i, t+1}|x^{N_1}_{i, t}, u^{N_1}_{i, t}, y^{N_2}_{j, t}) = f_2^c(c^{x,N_1}_{i, t+1}|c^{x,N_1}_{i, t}, h^{x,N_1}_{i,t}, u^{N_1}_{i, t}, c^{y,N_2}_{j, t}, h^{y,N_2}_{j, t})
    f_2^h(h^{x,N_1}_{i, t+1}|c^{x,N_1}_{i, t}, h^{x,N_1}_{i,t}, c^{y,N_2}_{j, t}, h^{y,N_2}_{j, t}),
\end{equation}
where $f_2^c$ corresponds to the spatial transition and $f_2^h$ is the status transition. 
We will discuss the two transitions separately in the following two subsections. 

\subsubsection{Spatial transitions}
The interaction among agents are induced by a pair-wise particle force. 
Consider a Blue agent $i$ and a Red agent $j$, given their state $x^{N_1}_{i,t}$ and $y^{N_2}_{j,t}$, 
the force the Red agent exert on the Blue agent is given by
\begin{equation}
    \vec{f}_{2}(y^{N_2}_{j,t}, x^{N_1}_{i,t}) = \frac{K_{2,1} h_{j,t}^y (c_{i,t}^x - c_{j,t}^y)}{\max \left\{\norm{c_{i,t}^x - c_{j,t}^y}^3, 1 \right\}}.
\end{equation}
The first argument of the force function is the agent that applies the force and the second argument corresponds to the agent that receives the force.
The coefficient $K_{2,1}$ is a parameter that control how strong the influence a Red agent has over a Blue agent, and the sign of the coefficient decides whether the force is attractive or repelling. 
The max operator in the denominator ensures that the force is well defined even if the two agents are at the same location (cell), in which case, there is no force. 
The force is multiplied with the factor $h_{j,t}^y$, which implies that if the Red agent $j$ is inactive, then it does not exert any force on any of the agents.
The rest of the interaction forces are defined as
\begin{align*}
    \vec{f}_{1}(x^{N_1}_{j,t}, x^{N_1}_{i,t}) &= \frac{K_{1,1} h_{j,t}^x (c_{i,t}^x - c_{j,t}^x)}{\max \left\{\norm{c_{i,t}^x - c_{j,t}^x}^3, 1 \right\}},\\
    \vec{g}_{1}(x^{N_1}_{i,t}, y^{N_2}_{j,t}) &= \frac{K_{1,2} h_{i,t}^x ( c_{j,t}^y- c_{i,t}^x)}{\max \left\{\norm{c_{j,t}^y- c_{i,t}^x}^3, 1 \right\}},\\
    \vec{g}_{2}(y^{N_2}_{i,t}, y^{N_2}_{j,t}) &= \frac{K_{2,2} h_{i,t}^y (c_{j,t}^y -c_{i,t}^y)}{\max \left\{\norm{c_{j,t}^y -c_{i,t}^y}^3, 1 \right\}}.
\end{align*}


Due to the symmetry, we explain the dynamics due to the force Red agent $j$ exert on Blue agent $i$.
The total force due to the action of Blue agent $i$ and the force from Red agent $j$ is then computed as 
\begin{equation}
    \vec{f}_{\mathrm{total}}(y^{N_2}_{j,t}, x^{N_1}_{i,t}, u^{N_1}_{i,t}) = \vec{f}_{2}(y^{N_2}_{j,t}, x^{N_1}_{i,t}) + u^{N_1}_{i,t}.
\end{equation}
Note that with the action within the unit ball, the norm of total force is upper bounded by $K_{2,1} + 1$.


We consider four possible directions to travel under this joint force: $e_1 = [1, 0]$, $e_2 = [0, 1]$, $e_3 = [-1, 0]$ and $e_4 = [0, -1]$.
Suppose the angle between $\vec{f}_{\mathrm{total}}(y^{N_2}_{j,t}, x^{N_1}_{i,t}, u^{N_1}_{i,t})$ and a direction $e_i$ is less than $\frac{\pi}{2}$, then the Blue agent $i$ has two possible next positions under this joint force: one is $c^x_{i,t}$ and the other is $c^x_{i,t} + e_i$. 
If $c^x_{i,t} + e_i$ is outside $\mathcal{C}$ or $\vec{f}_{\mathrm{total}} = 0$ or $h^{x, N_1}_{i,t}=0$, then the Blue agent will stay at its current location.
Formally, we have
\begin{subnumcases}{f_2^c(c^{x,N_1}_{i, t+1}|c^{x,N_1}_{i, t}, h^{x,N_1}_{i,t}, u^{N_1}_{i, t}, c^{y,N_2}_{j, t}, h^{y,N_2}_{j, t})}
        1   & $\text{if } c^{x,N_1}_{i, t+1}=c^{x,N_1}_{i, t} \text{ and } c^{x,N_1}_{i, t}+e_i \notin \mathcal{C}  $ \label{case:outside}
        \\
        1   & $\text{if } c^{x,N_1}_{i, t+1}=c^{x,N_1}_{i, t} \text{ and }
        \vec{f}_{\mathrm{total}} = 0 $ \label{case:zero-total}
        \\
        1   & $\text{if } c^{x,N_1}_{i, t+1}=c^{x,N_1}_{i, t} \text{ and }
        h^{x,N_1}_{i,t} = 0 $ \label{case:inactive-x}
        \\
        \frac{\Vert\vec{f}_{\mathrm{total}}\Vert}{K_{2,1} +1} & $\text{if } c^{x,N_1}_{i, t+1}=c^{x,N_1}_{i, t}+e_i  \text{ and } c^{x,N_1}_{i, t}+e_i \in \mathcal{C}$  \label{case:transition-forward}
        \\
        1 - \frac{\Vert\vec{f}_{\mathrm{total}}\Vert}{K_{2,1} +1} & $\text{if } c^{x,N_1}_{i, t+1}=c^{x,N_1}_{i, t}  \text{ and otherwise.} $ \label{case:transition-stay}
\end{subnumcases}
Note that a larger total force implies a higher transition probability to the next state in the force direction. 
Consequently, if the Red agent $j$ exerts a larger force on the Blue agent, it has a larger influence on the Blue agent's dynamics. 
As a result, the Red team needs to strategically position their agents to influence the Blue team's dynamics and steer the system into the Red team's advantage. See Figure~\ref{fig:battle-field-force-demo} for an illustration. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/battle-field-force.png}
    \caption{A demonstration of the spatial transition concept. 
    The colored cells represents the cells that has a positive probability support at the next timestep.
    The first plot corresponds to the scenario in~\eqref{case:outside}, where the total force points to positions outside the domain. 
    The second plot corresponds to the scenario in~\eqref{case:zero-total}, where the total force is zero. 
    The third plot corresponds to the scenario in~\eqref{case:inactive-x}, where the Blue agent is inactive. 
    The forth plot corresponds to the normal transitions in~\eqref{case:transition-forward} and~\eqref{case:transition-stay}.
    The last plot corresponds to the scenario where the Red agent is inactive, and consequently, the transition is decided solely based on the control force of the Blue agent. }
    \label{fig:battle-field-force-demo}
\end{figure}

\subsection{Status transitions}
For the status transition, we let the agent have a negative effect on opposing team's status, while a positive effect on its teammates. 
Furthermore, the influence is active only if the two agents are close enough.
For $f_2^h$, we have
\begin{subnumcases}{f_2^h(h^{x,N_1}_{i, t+1}|c^{x,N_1}_{i, t}, h^{x,N_1}_{i,t}, c^{y,N_2}_{j, t}, h^{y,N_2}_{j, t}) =}
    1 & $\text{if } h^{x,N_1}_{i, t+1} = h^{x,N_1}_{i, t} \text{ and } h^{y,N_2}_{j, t} = 0$ \label{case:status-trans-adv-inactive}
    \\
    1 & $\text{if } h^{x,N_1}_{i, t+1} = 0 \text{ and } \mathrm{dist}(c^{x,N_1}_{i, t}, c^{y,N_2}_{j, t}) \leq \rho$ \label{case:status-trans-adv-in-range}
    \\
    1 & $\text{if } h^{x,N_1}_{i, t+1} = h^{x,N_1}_{i, t} \text{ and } \mathrm{dist}(c^{x,N_1}_{i, t}, c^{y,N_2}_{j, t}) > \rho $ \label{case:status-trans-adv-out-range}
    \\
    0 & \text{otherwise}.
\end{subnumcases}
Figure~\ref{fig:status-adv} illustrates the transitions defined above. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/status-adversarial-demo.png}
    \caption{A demonstration of the status transition under an adversarial agent.
    The first plot corresponds to the scenario in~\eqref{case:status-trans-adv-inactive}, where the Red agent is inactive and has no effect on the status of the Blue agent. 
    The second and third plots correspond to the scenario in~\eqref{case:status-trans-adv-in-range}, where the Red agent is in the effective range, and tries to deactivate the Blue agent. 
    The fourth plot shows the scenario in~\eqref{case:status-trans-adv-out-range}, where the active Red agent outside its effective range and has no impact on the Blue agent's status.
    }
    \label{fig:status-adv}
\end{figure}

The transition $f_1^h$ captures the dynamics of status state due to interactions between two Blue agents, and it is defined as
\begin{subnumcases}{f_1^h(h^{x,N_1}_{i, t+1}|c^{x,N_1}_{i, t}, h^{x,N_1}_{i,t}, c^{x,N_1}_{k, t}, h^{x,N_1}_{k, t}) =}
    1 & $\text{if } h^{x,N_1}_{i, t+1} = h^{x,N_1}_{i, t} \text{ and } h^{x,N_1}_{k, t} = 0$ \label{case:status-trans-team-inactive}
    \\
    1 & $\text{if } h^{x,N_1}_{i, t+1} = 1 \text{ and } \mathrm{dist}(c^{x,N_1}_{i, t}, c^{x,N_1}_{k, t}) \leq \rho$ \label{case:status-trans-team-in-range}
    \\
    1 & $\text{if } h^{x,N_1}_{i, t+1} = h^{x,N_1}_{i, t} \text{ and } \mathrm{dist}(c^{x,N_1}_{i, t}, c^{x,N_1}_{k, t}) > \rho $ \label{case:status-trans-team-out-range}
    \\
    0 & \text{otherwise}.
\end{subnumcases}

Figure~\ref{fig:status-team} illustrates the transitions defined above. 
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/status-teammate-demo.png}
    \caption{A demonstration of the status transition under an adversarial agent.
    The first plot corresponds to the scenario in~\eqref{case:status-trans-team-inactive}, where the Blue agent $k$ is inactive and has no effect on the status of the Blue agent $i$. 
    The second and third plots correspond to the scenario in~\eqref{case:status-trans-team-in-range}, where the Blue agent $k$ is in the effective range, and tries to reactivate the Blue agent $i$. 
    The fourth plot shows the scenario in~\eqref{case:status-trans-team-out-range}, where the active Blue agent $k$ outside its effective range and has no impact on the Blue agent $i$'s status.
    }
    \label{fig:status-team}
\end{figure}

\subsubsection{Rewards}
The reward is given based on the fraction of Red agents that enters the high value region. 
Formally, the reward is given as 
\begin{equation}
    r_t^{N_1,N_2}(\x^{N_1}_t, \y^{N_2}_t) = \frac{1}{N_2} \sum_{j=1}^{N_2} \mathds{1}(\y^{N_2}_{j,t} \in \mathrm{HTR}).
\end{equation}



