\documentclass[12pt]{article}
\usepackage{geometry, booktabs, subcaption}
\usepackage{titlesec}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{amssymb,amsmath}
\usepackage{graphics}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=30mm,
	top=30mm,
	right=30mm,
	bottom=30mm,
}


\usepackage[square]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=black]{hyperref}
\usepackage{mathrsfs,amsthm,amsmath}
\usepackage{amssymb,multirow}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{dsfont}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{bm}
\usepackage{hyperref}

\usepackage{bbm}
\usepackage{bm}
\RequirePackage[OT1]{fontenc}
\usepackage{tikz}
\usepackage[misc,geometry]{ifsym}
\def\ab{\mathbf{a}}
\def\bb{\mathbf{b}}
\def\Sdb{\mathbf{x}}
\def\yb{\mathbf{y}}
\def\tb{\mathbf{t}}
\def\vb{\mathbf{v}}
\def\wb{\mathbf{w}}
\def\ub{\mathbf{u}}
\def\zb{\mathbf{z}}
\def\Sdib{\boldsymbol{\Sdi}}
\def\thetab{\boldsymbol{\theta}}
\def\taub{\boldsymbol{\tau}}

\def\rd{\mathbb{R}^d}
\def\rk{\mathbb{R}^k}
\def\hrd{\mathbb{R}^d_{+}}
\def\naturals{\mathbb{N}}
\def\rationals{\mathbb{Q}}
\def\reals{\mathbb{R}}

\def\ps{\Theta}
\def\reals{\mathbb{R}}
\def\probabilityspace{(\Omega, \Fcr, \P)}
\def\ss{\mathbb{X}}
\def\samplespace{\mathbb{X}}
\def\ssa{\mathscr{X}}
\def\psa{\mathscr{T}}
\newcommand{\ptilde}{\tilde p}
\newcommand{\mutilde}{\tilde \mu}
\newcommand{\rhotilde}{\tilde \rho}
\newcommand{\tautilde}{\tilde \tau}
\newcommand{\ibp}{\mathrm{IBP}}
\newcommand{\bern}{\mathrm{Be}}
\newcommand{\prob}{\mathrm{Pr}}
\newcommand{\tsp}{{\operatorname{T-SP}}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\indicator}{\mathrm{I}}

\renewcommand{\mid}{\,|\,}


\def\samplespace{\mathbb{X}}
\def\samplesigmafield{\mathscr{X}}
\def\pmsfield{\mathscr{M}}
\def\ppms{\mathbb{P}}
\def\ppmsfield{\mathscr{P}}
\def\fptheta{f(\cdot\ |\ \theta)}
\allowdisplaybreaks



\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\lesssup}{\lambda-ess\,sup}
\newcommand{\Space}{\mathbb{M}}
\newcommand{\SpaceMetric}{(\mathbb{M}, \ud_{\mathbb{M}})}
\newcommand{\PmsS}{\mathcal{P}(\mathbb{M})}
\newcommand{\Hunoz}{\mathrm{H}^1_{\ast}(0,1)}
\newcommand{\Huno}{\mathrm{H}^1(0,1)}
\newcommand{\wz}{w^{(0)}}




\newcommand{\Dc}{\mathcal{D}}

\newcommand{\bbC}{\mathbb{C}}
\newcommand{\indic}{\mathds{1}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\X}{\mathbb{X}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Sd}{\mathbb{S}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\D}{\mathrm{D}}
\newcommand{\Kbb}{\mathbb{K}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Dcal}{\mathcal{D}}


\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathds{E}}
\newcommand{\C}{\mathds{C}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathds{P}}


\newcommand{\bbsf}{\mathsf{B}}
\newcommand{\ccsf}{\mathsf{C}}
\newcommand{\ddsf}{\mathsf{D}}
\newcommand{\eesf}{\mathsf{E}}
\newcommand{\ffsf}{\mathsf{F}}
\newcommand{\ggsf}{\mathsf{G}}
\newcommand{\hhsf}{\mathsf{H}}
\newcommand{\iisf}{\mathsf{I}}
\newcommand{\llsf}{\mathsf{L}}
\newcommand{\mmsf}{\mathsf{M}}
\newcommand{\nnsf}{\mathsf{N}}
\newcommand{\oosf}{\mathsf{O}}
\newcommand{\ppsf}{\mathsf{P}}
\newcommand{\qqsf}{\mathsf{Q}}
\newcommand{\rrsf}{\mathsf{R}}
\newcommand{\sssf}{\mathsf{S}}
\newcommand{\ttsf}{\mathsf{T}}
\newcommand{\uusf}{\mathsf{U}}
\newcommand{\vvsf}{\mathsf{V}}
\newcommand{\wwsf}{\mathsf{W}}
\newcommand{\zzsf}{\mathsf{Z}}



\newcommand{\Bcr}{\mathscr{B}}
\newcommand{\Fcr}{\mathscr{F}}
\newcommand{\Dcr}{\mathscr{D}}
\newcommand{\Icr}{\mathscr{I}}
\newcommand{\Lcr}{\mathscr{L}}
\newcommand{\Pcr}{\mathscr{P}}
\newcommand{\Sdcr}{\mathscr{X}}
\newcommand{\Mcr}{\mathscr{M}}
\newcommand{\Ycr}{\mathscr{Y}}
\newcommand{\Acr}{\mathscr{A}}
\newcommand{\Gcr}{\mathscr{G}}
\newcommand{\Vcr}{\mathscr{V}}
\newcommand{\Ccr}{\mathscr{C}}

\newcommand{\calC}{\mathscr{C}}

\newcommand{\Sdf}{\mathfrak{X}}
\newcommand{\Sf}{\mathfrak{\sigma}}
\newcommand{\Wuno}{\mathcal{W}_1}
\newcommand{\Wdue}{\mathcal{W}_2}
\newcommand{\Wp}{\mathcal{W}_p}

\def\rd{\mathbb{R}^d}
\def\naturals{\mathbb{N}}
\def\reals{\mathbb{R}}
\def\simind{\stackrel{\mbox{\scriptsize{ind}}}{\sim}}
\def\simiid{\stackrel{\mbox{\scriptsize{iid}}}{\sim}}

\newcommand{\kksf}{\mathsf{K}}
\newcommand{\PP}{\mathds{P}}
\newcommand{\ddr}{\mathrm{d}}
\newcommand{\EE}{\mathds{E}}
\newcommand{\var}{\textsf{Var}}
\newcommand{\psf}{\mathscr{T}}
\newcommand{\pms}{\mathcal{P}(\Theta)}
\newcommand{\pmstwo}{\mathcal{P}_2(\Theta)}
\newcommand{\pmsp}{\mathcal{P}_p(\Theta)}
\newcommand{\randomparameter}{\tilde{\theta}}
\newcommand{\randommeasure}{\tilde{\mathfrak{p}}}
\newcommand{\randomtau}{\tilde{\tau}}
\newcommand{\empiric}{\mathfrak{e}_n}
\newcommand{\pfrak}{\mathfrak{p}}
\newcommand{\ee}{\textsf{E}}
\newcommand{\pp}{\textsf{P}}
\newcommand{\pfrakz}{\mathfrak{p}_0}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\ind}{\mathds{1}}
\newcommand{\Cpw}{\mathfrak{C}_2}
\newcommand{\Bdual}{\mathbb{B}^{\ast}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defi}[thm]{Definition}
\newtheorem{notation}[thm]{Notation}
\newtheorem{prp}[thm]{Proposition}
\newtheorem{lm}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{remark}[thm]{Remark}
\newtheorem{example}[thm]{Example}
\newtheorem{assum}[thm]{Assumptions}
\newcommand{\remarks}[1]{\marginpar{\scriptsize{\sc #1}}}

\usepackage{xparse}
\NewDocumentCommand{\evalat}{sO{\big}mm}{%
  \IfBooleanTF{#1}
   {\mleft. #3 \mright|_{#4}}
   {#3#2|_{#4}}%
}



\usepackage{fancyhdr}
%\pagestyle{fancy}
%\lhead{Beraha and Favaro}
%\rhead{Random measure priors for sketches}


\title{Random measure priors in Bayesian frequency recovery from sketches}






\author[1]{Mario Beraha}
\author[2]{Stefano Favaro}
\affil[1]{\normalsize{Department of Economics and Statistics, University of Torino}} 
\affil[2]{\normalsize{Department of Economics and Statistics, University of Torino and \newline Collegio Carlo Alberto}}


\begin{document}

\maketitle


\begin{abstract}
Given a lossy-compressed representation, or sketch, of data with values in a set of symbols, the frequency recovery problem considers the estimation of the empirical frequency of a new data point. This is a classical problem in computer science, with recent studies applying Bayesian nonparametrics (BNPs) to develop learning-augmented versions of the popular count-min sketch (CMS) recovery algorithm. In this paper, we present a novel BNP approach to frequency recovery, which is not built from the CMS but still relies on a sketch obtained by random hashing. Assuming data to be modeled as random samples from an unknown discrete distribution, which is endowed with a Poisson-Kingman (PK) prior, we provide the posterior distribution of the empirical frequency of a symbol, given the sketch, from which estimates are obtained as mean functionals. An application of our result is presented for the Dirichlet process (DP) and Pitman-Yor process (PYP) priors, and in particular: i) we characterize the DP prior as the sole PK prior featuring a property of sufficiency with respect to the sketch, leading to a simple posterior distribution; ii) we identify a large sample regime under which the PYP prior leads to a simple approximation of the posterior distribution. We further develop our BNP approach to a general “traits” formulation of the frequency recovery problem, not yet studied in the CMS literature, in which data belong to more than one symbol, referred to as traits, and exhibit nonnegative integer levels of associations with each trait. In particular, by modeling data as random samples from a generalized Indian buffet process, we provide the posterior distribution of the empirical frequency level of a trait, given the sketch. This result is then applied under the assumption of a Poisson and Bernoulli distribution for the levels of associations, leading to a simple posterior distribution and a simple approximation of the posterior distribution, respectively. 

\end{abstract}

\textbf{Keywords}: Bayesian nonparametrics; Dirichlet process; frequency recovery; generalized Indian buffet process; Pitman-Yor process; sketch; Poisson-Kingman prior; random hashing





\section{Introduction}

Recovering the empirical frequency of an object given a lossy-compressed representation, or sketch, of a large dataset is a classical problem in computer science \citep{Mis(82),Alo(99),Man(02),Kar(03),Cha(04),Cor(05),Ind(06)}. In its common ``species" formulation, the frequency recovery problem considers $n\geq1$ data points $(x_{1},\ldots,x_{n})$, with each $x_{i}$ taking one value in a (possibly infinite) set $\mathbb{S}$ of ``species" labels or symbols, and it relies on a sketch of the $x_{i}$'s to recover the number of occurrences of a new $x_{n+1}$ in $(x_{1},\ldots,x_{n})$, namely the empirical frequency
\begin{displaymath}
f_{x_{n+1}}=\sum_{i=1}^{n}I(x_{i}=x_{n+1}),
\end{displaymath}
with $I(\cdot)$ being the indicator function. The problem of recovering $f_{x_{n+1}}$ from a sketch of  $(x_{1},\ldots,x_{n})$ is known to be relevant in diverse fields, including statistical machine learning in high-dimensional feature spaces \citep{Shi(09),Agg(10)}, cybersecurity in tracking password popularity \citep{Sch(10)}, web and social network data analysis \citep{Son(09),Cor(17)}, natural language processing \citep{Goy(12)}, sequencing analysis in biological sciences \citep{Zha(14),Ber(16),Sol(16),Mar(19),Leo(20)}, and privacy-protecting data analysis \citep{Dwo(10),Mel(16),Cor(17),Cor(18),Koc(20)}. In general, sketching is practically motivated by memory limitations, as large numbers of distinct symbols in a dataset may otherwise be computationally expensive to analyze, or by privacy constraints, in situations where the data contain sensitive information. We refer to \citet[Chapter 3]{Cor(20)} for an up-to-date review on frequency recovery from sketches, and generalizations thereof.

The count-min sketch (CMS) \citep{Cor(05)} is arguably the most popular algorithm to recover $f_{x_{n+1}}$. It relies on a sketch obtained by $D\geq1$ different $J$-wide independent random hash functions $h_{k}:\mathbb{S}\rightarrow\{1,\ldots,J\}$, for $k=1,\ldots, D$ and $J\geq1$. Each function maps $(x_{1},\ldots,x_{n})$ into one of the $J$ buckets, defining  a sketch $\mathbf{C}_{D,J}\in\mathbb{N}_{0}^{D\times J}$ whose $(k,j)$-th element $C_{k,j}$ counts the data points mapped by the $k$-th hash function into the $j$-th bucket. Then, based on $\mathbf{C}_{D,J}$, the CMS recovers $f_{x_{n+1}}$ by taking the smallest count among the $D$ buckets into which $x_{n+1}$ is mapped, i.e.
\begin{equation}\label{cms}
\hat{f}_{x_{n+1}}=\min\{C_{1,h_{1}(x_{n+1})},\ldots,C_{D,h_{D}(x_{n+1})}\}.
\end{equation}
The CMS has been the subject of numerous studies, with a recent interest in the use of statistical models to better exploit the data \citep[Chapter 3]{Cor(20)}. In such a context, the work of \citet{Cai(18)} is one of the most original contributions, introducing a Bayesian nonparametric (BNP) approach to obtain learning-augmented versions of the CMS. In particular, the $x_{i}$'s are assumed to be modeled as a random sample $(X_{1},\ldots,X_{n})$ from an unknown discrete distribution, which is endowed with a Dirichlet process (DP) prior \citep{Fer(73)}, and then estimate $f_{X_{n+1}}$ are obtained as mean functionals of the posterior distribution of $f_{X_{n+1}}$ given $(C_{1,h_{1}(X_{n+1})},\ldots,C_{D,h_{D}(X_{n+1})})$, e.g. the mean, median and mode. Besides allowing to include some a priori knowledge on the data into the recovery problem, the BNP approach provides a natural way to assess uncertainty of estimates through the posterior distribution. See \citet{Dol(21),Dol(23)} for an extension to the more general Pitman-Yor process (PYP) prior \citep{Pit(97)}, which led to a novel learning-augmented CMS for power-law data.

\subsection{Our contributions} 

In this paper, we present a novel BNP approach to frequency recovery. While the approach of \citet{Cai(18)} is built from the CMS, hence the interest in a posterior distribution with respect to the same information of $\mathbf{C}_{D,J}$ as in \eqref{cms}, our approach considers BNP estimation of $f_{X_{n+1}}$ from a sketch obtained by a single random hash function, say $\mathbf{C}_{J}=(C_{1},\ldots,C_{J})\in\mathbb{N}_{0}^{J}$, computing the posterior distribution of  $f_{X_{n+1}}$ with respect to $\mathbf{C}_{J}$. Within a (model-based) statistical perspective of the frequency recovery problem, our approach appears more natural than that of \citet{Cai(18)}, which may be questionable with respect to: i) the practical usefulness of combining a statistical model for the $x_{i}$'s with the sketch $\mathbf{C}_{D,J}$ from multiple hash functions, being such a sketch designed for a (model-free) recovery algorithm; ii) the use of  a posterior distribution with respect to the sole $C_{k,h_{k}(X_{n+1})}$'s that may determine a loss of information, unless the $C_{k,h_{k}(X_{n+1})}$'s are somehow sufficient to estimate $f_{X_{n+1}}$. For the broad class of Poisson-Kingman priors \citep{Pit(03)}, which includes both the DP and PYP, we provide a closed-form expression for the posterior distribution of $f_{X_{n+1}}$ given $\mathbf{C}_{J}$ and the bucket in which $X_{n+1}$ is hashed, i.e. $h(X_{n+1})$. An application of our result to the DP prior shows that the posterior distribution depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$, thus proving that the BNP approach of \citet{Cai(18)} relies on a sufficient statistic of $\mathbf{C}_{D,J}$; then, we characterize the DP prior as the sole PK prior that satisfies such a property of sufficiency. For the PYP, we show that the posterior distribution is intractable to be evaluated even for moderately large sample sizes, with a computational costs that scales exponentially in $J$; then, we identify a large sample regime that leads to a simple approximation of the posterior distribution, depending on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$. In general, our results show that, for priors beyond the DP, the BNP approach to frequency recovery leads to non-trivial computational challenges in the evaluation of the posterior distribution of $f_{X_{n+1}}$.

We further develop our BNP approach to a general ``traits" formulation of the frequency recovery problem, not yet studied in the CMS literature, in which data belong to more than one symbol, referred to as traits, and exhibit nonnegative integer levels of associations with each trait \citep{Cam(18)}. For examples, single cell data contain multiple genes with their expression levels, members of a social networks have friends to which they send multiple messages, documents contain different topics with their words. Here, we consider $n\geq1$ data points $(x_{1},\ldots,x_{n})$, with each $x_{i}$ taking a value in a set $\mathbb{S}^{\infty}\times\mathbb{N}_{0}^{\infty}$ of traits with their levels, and assume the $x_{i}$'s to be modeled as a random sample $(X_{1},\ldots,X_{n})$ from the generalized Indian buffet process \citep{Jam(17)}. That is, the $X_{i}$'s are random counting measures, say $X_{i}=\sum_{k\geq1}A_{i,k}\delta_{w_{k}}$, whose distribution is determined by: i) a distribution $G_{A}$ for the level of association $A_{i,k}\in\mathbb{N}_{0}$ of the trait $w_{k}\in\mathbb{S}$, which depends on a parameter $J_{k}>0$, for $k\geq1$; ii) the distribution of a completely random measure (CRM) as a prior distribution for the $J_{k}$'s. Based on a sketch $\mathbf{C}_{J}\in\mathbb{N}_{0}^{J}$ of $(X_{1},\ldots,X_{n})$, still obtained by a single random hash function, we provide the posterior distribution of the empirical frequency level of a trait of a new $X_{n+1}$, given $\mathbf{C}_{J}$ and the bucket in which such a trait is hashed. It turns out that any CRM prior leads to a posterior distribution that depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$, showing a property of sufficiency as in the ``species" formulation under the DP prior. We apply our result with $G_{A}$ being a Poisson distribution, which leads to a simple expression for the posterior distribution, and then we show how to use such an expression to approximate the  posterior distribution with $G_{A}$ being a Bernoulli distribution. Our results show how the BNP approach to frequency recovery in the ``species" formulation admits a natural extension to the ``traits" formulation, for which we are not aware of any (model-free) algorithmic approach. This is a further evidence of the flexibility of the BNP approach to frequency recovery.

\subsection{Organization of the paper}

The paper is structured as follows. In Section \ref{sec2} we consider the ``species" formulation of frequency recovery: i) under PK priors, we compute the posterior distribution of the empirical frequency of a symbol; ii) we apply this result to the DP and the PYP priors, showing their peculiar properties within PK priors; iii) we present a numerical illustration. In Section \ref{sec3} we consider the ``traits" formulation of frequency recovery: i) under CRM priors, we compute the posterior distribution of the empirical frequency level of a trait; ii) we apply this result to the Poisson and the Bernoulli distribution for the level of association of traits; iii) we present a numerical illustration. Section  \ref{sec4} contains a discussion of our work, and some directions for future research. Proofs are deferred to Appendices.


\section{BNP frequency recovery: ``species" formulation}\label{sec2}

For $n\geq1$, let $(x_{1},\ldots,x_{n})$ be data points with each $x_{i}$ taking a value in $\mathbb{S}$. The $x_{i}$'s are not observable, and instead we have access only to a sketch of them, which is obtained through random hashing \citep[Chapter 5 and Chapter 15]{Mit(17)}. 
In particular, for an integer $J \geq 1$, let $h$ be a random hash function of width $J$, which is defined as a random mapping from $\mathbb{S}$ to $[J] = \{1,\ldots,J\}$ chosen from a pairwise independent hash family $\mathcal{H}_{J}$. That is, $h: \mathbb{S} \to [J]$, and, for any $j_1,j_2 \in [J]$ and fixed $x_1,x_2 \in \mathbb{S}$ such that $x_1 \neq x_2$,
\begin{displaymath}
\text{Pr}[h(x_{1})=j_{1},\,h(x_{2})=j_{2}]= \frac{1}{J^{2}}.
\end{displaymath}
The pairwise independence of $\mathcal{H}_{J}$, also known as strong universality, implies uniformity, meaning that $\Pr[h(x)=j]=J^{-1}$ for $j=1,\ldots,J$. The process of hashing $(x_{1},\ldots,x_{n})$  through $h$ produces a random vector $\mathbf{C}_{J}=(C_{1},\ldots,C_{J})\in\mathbb{N}_{0}^{J}$, which is referred to as sketch, whose $j$-th element (bucket) is
\begin{displaymath}
C_{j} = \sum_{i=1}^{n} I( h(x_i) = j),
\end{displaymath}
such that $\sum_{1\leq j\leq J} C_{j} = n$. In general, the sketch $\mathbf{C}_{J}$ has a smaller (physical) size than $(x_{1},\ldots,x_{n})$ due to the collisions of the $x_{i}$'s induced by random hashing \citep[Chapter 3]{Cor(20)}. The above sketch is a special version of the sketch $\mathbf{C}_{D,J}\in\mathbb{N}_{0}^{D\times J}$ at the basis of the CMS of \citet{Cor(05)}, which simultaneously sketches the same data points $x_{i}$'s using a collection of $D\geq1$ independent random hash functions from $\mathcal{H}_{J}$. We refer to \citet[Chapter 3]{Cor(20)} for further details on $\mathbf{C}_{D,J}$, and to \citet{Cai(18)} for the use of $\mathbf{C}_{D,J}$ under a BNP model for the $x_{i}$'s.

Based of a sketch $\mathbf{C}_{J}$ of $(x_{1},\ldots,x_{n})$, we consider the problem of BNP estimation of the empirical frequency $f_{x_{n+1}}$. Following ideas first developed in \citet{Cai(18)} and \citet{Dol(23)}, the BNP approach to frequency recovery relies on two modeling assumptions: i) the data points $(x_{1},\ldots,x_{n})$ are modeled as a random sample $\mathbf{X}_{n}=(X_{1},\ldots,X_{n})$ from an unknown discrete distribution $P$, which is endowed by a nonparametric prior $\mathscr{P}$; ii) the hash family $\mathcal{H}_{J}$ is independent of $P$. Accordingly, we write the following BNP model:
\begin{align}\label{eq:exchangeable_model_hash} 
\begin{split}
  C_{j} &\,=\, \sum_{i=1}^{n} I( h(X_i) = j)\qquad j=1,\ldots,J\\[0.2cm]
  h&\,\sim\,\mathcal{H}_{J}\\[0.2cm]
  X_1,\ldots,X_{n}\,|\,P &\,\simiid\, P\\[0.2cm]
  P&\,\sim\,\mathscr{P}.
\end{split}
\end{align}
Under the model \eqref{eq:exchangeable_model_hash}, the problem of recovering the empirical frequency $f_{X_{n+1}}$ from the sketch consists in the computation of the posterior distribution of $f_{X_{n+1}}$ given $\mathbf{C}_{J}$ and the buckets in which $X_{n+1}$ is hashed, i.e. $h(X_{n+1})$. Below, we compute such a posterior distribution assuming that $\mathscr{P}$ belongs to the broad class of PK priors, and then we apply our result to the DP prior and the PYP prior, which are arguably the most popular PK priors.

\subsection{Main result}

Before stating our main result, it is useful to recall the definition of PK prior \citep{Pit(03)}. See also \citet[Chapter 3]{Pit(06)}. Consider a completely random measure (CRM) $\mutilde$ on $\mathbb{S}$, which is a random element with values on the space of bounded measure on $(\mathbb{S},\mathcal{S})$, such that for $k\geq1$ and for a collection of disjoint Borel sets $A_1, \ldots, A_k \in \mathcal{S}$ the random variables $\mutilde(A_{1}),\ldots,\mutilde(A_{k})$ are independent \citep{Kin(67)}.  We consider CRMs of the form
\begin{displaymath}
\mutilde(\cdot) = \int_{\R_+} s \tilde N(\dd s,\, \cdot) = \sum_{k \geq 1} J_k \delta_{W_k}(\cdot),
\end{displaymath}
where $\tilde N = \sum_{k \geq 1} \delta_{(J_k, W_k)}$ is a Poisson random measure on $\R_+ \times \mathbb{S}$ with L\'evy intensity $\nu(\dd s, \dd x)$, which characterizes the distribution of $\tilde{\mu}$ in terms its random jumps $J_{k}$'s and random locations $W_{k}$'s \citep{Kin(67),Kin(93)}. We focus on homogeneous L\'evy intensities, namely measures of the form $\nu(\dd s, \, \dd x) = \theta \rho(s) \dd s \, G_0(\dd x)$ where $\theta > 0$ is a parameter, $G_0$ is a nonatomic probability measure on $\mathbb{S}$ and $\rho(s) \dd s$ is a measure on $\R_+$ such that $\int_{\R_+} \rho(s) \dd s = +\infty$ and
\begin{equation}\label{eq_cond}
\psi(u) = \int_{\R_+}(1 - e^{-us}) \rho(s) \dd s < +\infty
\end{equation}
for all $u>0$, which ensure that $0 < \mutilde (\mathbb{S}) < +\infty$ almost surely \citep{Pit(03),Reg(03)}. We write $\mutilde \sim \mbox{CRM}(\theta, \rho, G_0)$ to denote an homogeneous CRM on $\mathbb{S}$. A PK prior is defined as the law of a suitable ``normalization" of an homogeneous CRM with respect to its total mass \citep{Pit(03)}.

\begin{defi}\label{pk}
Let $\mutilde \sim \mbox{CRM}(\theta, \rho, G_0)$ with $T = \mutilde(\mathbb{S}) \sim f_T$ and let $\text{PK}(\theta, \rho \mid T=t)$ be the conditional distribution of ``normalized" random jumps $(J_{k}/T)_{k\geq1}$ given $T=t$. If $g(\mutilde) \equiv g(T)$ such that $\E[g(T)] = 1$, then a PK prior with parameter $(\theta, \rho,  g f_T, G_0)$ is the law of the (discrete) random probability measure
\begin{displaymath}
P(\cdot)=\frac{\tilde{\mu}(\cdot)}{T}=\sum_{k\geq1}\frac{J_{k}}{T}\delta_{W_{k}}(\cdot),
\end{displaymath}
on $\mathbb{S}$, where the random probabilities $(J_{k}/T)_{k\geq1}$ are distributed as the PK distribution $\text{PK}(\theta, \rho, g f_T) = \int_{\R_+} \text{PK}(\theta, \rho \mid T=t) g(t) f_T(t) \dd t$ and the random location $(W_{k})_{k\geq1}$, independent of $(J_{k}/T)_{k\geq1}$, are independent and identically distributed as $G_{0}$.
\end{defi}

Under the model \eqref{eq:exchangeable_model_hash} with $\mathscr{P}$ being a PK prior, i.e. $P\sim\text{PK}(\theta, \rho,  g f_T, G_0)$, the next theorem provides an expression for the posterior distribution of $f_{X_{n+1}}$ given $\mathbf{C}_{J}$ and $h(X_{n+1})$. Because of the broadness of the class of PK priors, this is a general result that can be applied upon suitable specifications of the measure $\rho$ and the function $g$. For instance, PK priors includes the class of (homogeneous) normalized CRM priors, which is obtained by setting $g$ as the identity function \citep{Jam(02),Pru(02),Pit(03),Reg(03)}. In general, it is sufficient to consider $g(t) \propto t^{-\gamma} e^{-\beta t}$ to recover from Definition \ref{pk} the most popular priors in BNPs. Common choices of $\mutilde$ are the Gamma CRM and the $\alpha$-Stable CRM \citep{Kin(75)}, as they provide PK priors with a flexible tail behaviour, ranging from geometric tail to heavy power-law tails, respectively.  Under the Gamma CRM, Definition \ref{pk} provides a generalization of the DP prior, which is obtained by setting $g$ as the identity function. Under the $\alpha$-Stable CRM, Definition \ref{pk} provides a generalization of the normalized $\alpha$-Stable prior \citep{Kin(75)}, which is obtained by setting $g$ as the identity function, and it also includes the PYP prior and the normalized Gamma process prior \citep{Jam(02),Lij(05),Lij(07)}. We refer to \cite[Chapter 3]{Pit(06)} and \citet{Lij(10)} for other examples of PK priors, with applications in BNPs. Applications of the next theorem to the DP and the PYP priors will be considered below, showing their peculiar features within the class of PK priors.

\begin{thm}\label{main_pk}
Let $\mathbf{C}_{J}$ be an observable sketch of $\mathbf{X}_{n}$ under the model \eqref{eq:exchangeable_model_hash} with $P \sim \text{PK}(\theta, \rho, g f_T,G_{0})$ and $g(t) \propto t^{-\gamma} e^{-\beta t}$, and let $X_{n+1}$ be an additional (unobservable) random sample. With $\psi$ being defined in \eqref{eq_cond}, if $\phi^{(n)}(u) = (-1)^{n} \frac{\dd^n}{\dd u^n} e^{-\theta/J \psi(u)}$ and  $\kappa(u, n) = \int_{\R_+} e^{-un} s^n \rho(s) \dd s$, then for $l=0,1, \ldots, c_j$
\begin{multline}\label{post_pk}
\prob[f_{X_{n+1}}=l\,|\,\mathbf{C}_{J}=\mathbf{c},h(X_{n+1})=j]\\
\quad=\frac{\theta}{J} \binom{c_j}{l} \frac{\int_{\R_+} u^{n + \gamma}  \phi^{(c_j - l)}(u + \beta) 
 \prod_{k\neq j} \phi^{(c_k)}(u + \beta)  \, \kappa(u + \beta, l+1) \dd u 
 }{
  \int_{\R_+ } u^{n+\gamma}   \phi^{(c_j + 1)}(u + \beta) 
  \prod_{k\neq j} \phi^{(c_k)}(u + \beta)  \dd u}.
\end{multline}
\end{thm}

See Appendix \ref{app:proof_main} for the proof of Theorem \ref{main_pk}. Theorem \ref{main_pk} provides a BNP solution of the problem of recovery $f_{X_{n+1}}$, in the sense that an estimator of $f_{X_{n+1}}$, with respect to a suitable loss function, is obtained as a mean functional of the posterior distribution \eqref{main_pk}. Also credible intervals may be derived. The use of a squared loss function is arguably the most common choice, leading the posterior mean as a BNP  estimator of  $f_{X_{n+1}}$, i.e. 
\begin{displaymath}
\hat{f}_{X_{n+1}}=\sum_{l=0}^{c_j}l \, \text{Pr}[f_{X_{n+1}}=l\,|\,\mathbf{C}_{J}=\mathbf{c},h(X_{n+1})=j].
\end{displaymath}
Theorem \ref{main_pk} extends the main results of \citet{Cai(18)} and \citet{Dol(23)} with respect to both the use of the sketch and the specification of the prior distribution. As for the sketch, \citet[Proposition 1]{Cai(18)} and \citet[Theorem 1 and Theorem 2]{Dol(23)} provide a posterior distribution of $f_{X_{n+1}}$ with respect to the sole bucket $C_{h(X_{n+1})}$ in which $X_{n+1}$ is hashed, whereas Theorem \ref{main_pk} considers the whole sketch $\mathbf{C}_{J}$ and where $X_{n+1}$ is hashed, i.e. $h(X_{n+1})$.  As for the prior, \citet{Cai(18)} and \citet{Dol(23)} focussed on the DP prior and PYP prior, obtaining posterior distributions by means of  peculiar conjugacy or quasi-conjugacy properties of the priors, whereas here we consider a general PK prior. As discussed in \citet{Dol(23)}, since the DP and PYP priors are the sole quasi-conjugate PK priors, the use of conjugacy properties is a strong limitation in view of considering more general prior distributions. Our proof of Theorem \ref{main_pk} overcomes this limitation, by avoiding the use of any form of conjugacy for the prior.

\subsection{Results under the DP prior}

We consider Definition \ref{pk} with $\mutilde$ being a Gamma CRM, i.e. $\rho(s)=s^{-1}\exp\{-s\}$, and $g$ being be the identity function. For this choice of $(\rho,g)$, $P\sim\text{PK}(\theta, \rho,  g f_T, G_0)$ is a DP with mass $\theta>0$ and base measure $G_0$ \citep{Fer(73),Pit(03)}. For short, we write $P\sim\text{DP}(\theta,G_{0})$. Then, the posterior distribution of $f_{X_{n+1}}$ given $\mathbf{C}_{J}$ and $h(X_{n+1})$ is obtained by an application of Theorem \ref{main_pk}. Denote by $(a)_{(n)}$ the rising factorial of $a$ of order $n$, i.e. $(a)_{(n)}=\prod_{0\leq i\leq n-1}(a+i)$ with the proviso $(a)_{(0)}:=1$ \citep[Chapter 2]{Cha(05)}. Then, from \eqref{post_pk}, for $l=0,1,\ldots,c_{j}$
\begin{equation}\label{post_dp}
	\text{Pr}[f_{X_{n+1}}=l\,|\,\mathbf{C}_{J}=\mathbf{c},h(X_{n+1})=j] = \frac{\theta}{J}\frac{(c_{j}-l+1)_{(l)}}{\left(\frac{\theta}{J}+c_{j}-l\right)_{(l+1)}}.
\end{equation}
See Appendix \ref{app:dp_cor} for the proof of \eqref{post_dp} from \Cref{main_pk}, and Appendix \ref{app:dp_teo} for an alternative proof based on finite-dimensional properties of the DP. It is easy to show that \eqref{post_dp} is a Beta-Binomial distribution, namely a Binomial distribution in which the probability of success at each of the $c_{j}$  trials is a Beta random variable with parameter $(1,\theta/J)$, say $B_{1,\theta/J}$. That is, if $F_{X_{n+1}}$ is a random variable distributed as \eqref{post_dp}, then by de Finetti theorem $c_{j}^{-1}F_{X_{n+1}}$ converges (weakly) to a $B_{1,\theta/J}$ as $c_{j}\rightarrow+\infty$. The posterior distribution \eqref{post_dp} depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$. That is, under the DP prior, $C_{h(X_{n+1})}$ is a sufficient statistic for estimating $f_{X_{n+1}}$ from $\mathbf{C}_{J}$, thus making \eqref{post_dp} equivalent to the posterior distribution computed in \citet[Proposition 1]{Cai(18)}. The next theorem shows that such an equivalence is a peculiar feature of the DP prior, characterizing the DP prior as the unique PK prior for which the posterior distribution of $f_{X_{n+1}}$ with respect to $C_{h(X_{n+1})}$ is equivalent to the posterior distribution with respect to $\mathbf{C}_{J}$ and $h(X_{n+1})$.

\begin{thm}\label{char_dp}
Under the setting of Theorem \ref{main_pk}, the DP prior is the sole PK prior for which the posterior distribution \eqref{post_pk} depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$.
\end{thm}

See Appendix \ref{app:char_proof} for the proof of Theorem \ref{char_dp}. To conclude with the DP prior, we observe that a concrete application of the posterior distribution \eqref{post_dp} requires to estimate the unknown prior's parameter $\theta>0$ from the sketch $\mathbf{C}_{J}$. \citet{Cai(18)} proposed an empirical Bayes approach to estimate $\theta$, which relies on the following finite-dimensional projective property of $P\sim\text{DP}(\theta,G_{0})$:  if $\{B_1,\ldots,B_{k}\}$ is a measurable $k$-partition of $\mathcal{S}$, for $k\geq1$, then $(P(B_1), \ldots, P(B_k))$ is distributed as a Dirichlet distribution with parameter $(\theta G_{0}(B_{1}),\ldots,\theta G_{0}(B_{k}))$ \citep{Fer(73),Reg(01)}. According to the finite-dimensional projective property, and the assumption that $\mathcal{H}_{J}$ is independent of $P$, $\mathbf{C}_{J}$ is distributed as a Dirichlet-Multinomial distribution, i.e.
\begin{align}\label{marg}
&\text{Pr}[\mathbf{C}_{J}=\mathbf{c}]=\frac{n!}{(\theta)_{(m)}}\prod_{j=1}^{J}\frac{(\frac{\theta}{J})_{(c_{j})}}{c_{j}!}.
\end{align}
The fact that the distribution of $\mathbf{C}_{J}$ has a simple closed-form allows for an easy implementation of Bayesian estimation of $\theta$. \citet{Cai(18)} adopted an empirical Bayes approach, which consists in estimating $\theta$ with the value that maximizes, over $\theta$, the (marginal) likelihood \eqref{marg}. Such a value is then used as a plug-in estimate in the posterior distribution \eqref{post_dp}. Alternatively, a fully Bayes approach can be applied by placing a prior distribution on $\theta$ and evaluating the induced posterior distributions through Monte Carlo sampling.

\subsection{Results under the PYP prior}

We consider Definition \ref{pk} with $\theta=1$, $\mutilde$ being an $\alpha$-Stable CRM, i.e. $\rho(s)=(\alpha/\Gamma(1-\alpha)) s^{-1-\alpha}$ for $\alpha\in(0,1)$ and $g(t)=(\Gamma(\gamma+1)/\Gamma(\gamma/\alpha+1)t^{-\gamma}$ for $\gamma>-\alpha$, with $\Gamma(\cdot)$ being the Gamma function, i.e. $\Gamma(x)=\int_{(0,+\infty)}z^{x-1}\text{e}^{-z}\ddr z$ for $x>0$. For this choice of $(\theta,\rho,g)$, $P\sim\text{PK}(\theta, \rho,  g f_T, G_0)$ is a PYP with discount $\alpha$, mass $\gamma$ and base measure  $G_0$ \citep{Pit(03)}. For short, we write $P\sim\text{PYP}(\alpha,\gamma,G_{0})$. Then, the posterior distribution of $f_{X_{n+1}}$ given $\mathbf{C}_{J}$ and $h(X_{n+1})$ is obtained by an application of Theorem \ref{main_pk}. Denote by $\mathscr{C}(n,k;s)$ the generalized factorial of $n$ of order $k$, i.e.
\begin{displaymath}
	\mathscr{C}(n,k;s)=\frac{1}{k!}\sum_{i=0}^{k}(-1)^{i}{n\choose i}(-is)_{(n)}
\end{displaymath}
\citep[Chapter 2 and Chapter 3]{Cha(05)}. Then, from \eqref{post_pk}, for $l=0,1,\ldots,c_{J}$
\begin{multline}\label{post_pyp}
\text{Pr}[f_{X_{n+1}}=l\,|\,\mathbf{C}_{J}=\mathbf{c},h(X_{n+1})=j]\\
     \quad= \frac{\gamma}{J} \binom{c_j}{l} (1 - \alpha)_{(l)} \frac{\sum_{\bm i \in \mathcal S(\bm c, j, -l)} \frac{\Gamma\left(\frac{\gamma + \alpha}{ \alpha} + |\bm i|\right)}{J^{|\bm i|}} \prod_{k=1}^J \mathscr{C}(c_k - l \delta_{k,j}, i_k; \alpha)
    }{
    \sum_{\bm i \in \mathcal S(\bm c, j, 1)}\frac{ \Gamma\left(\frac{\gamma}{\alpha} + |\bm i|\right)}{J^{|\bm i|}}  \prod_{k=1}^J \mathscr{C}(c_k + \delta_{k,j}, i_k; \alpha)
    },
\end{multline}
where $\mathcal S(\bm c, j, q)$ is the Cartesian product of the index sets $\{\{0, \ldots, c_k + \delta_{k, j} q\},\ k = 1, \ldots, J\}$, and for a multi-index $\bm i \in \mathcal S(\bm c, j, q)$, $|\bm i| = \sum_{k=1}^J i_k$. See Appendix \ref{app:post_pyp} for the proof of \eqref{post_pyp}. Note that the posterior distribution \eqref{post_pyp} depends on the whole sketch $\mathbf{C}_{J}$, with the size of $\mathcal S(\bm c, j, q)$ blowing up exponentially both in $J$ and in $n$, e.g. if $J = 10$ and $c_j = 5$ for all $j$, then  $|\mathcal S(\bm c, j, q) | \approx 60 \times 10^6$. This makes  \eqref{post_pyp} intractable to be evaluated even for moderately large sample sizes, as it involves summations over a potentially very large number of generalized factorial coefficients, depending on $J$. We refer to Appendix \ref{app:computations_pyp} for some approaches to evaluate \eqref{post_pyp}, leading to some non-trivial computational challenges. The next theorem provides a large sample (qualitative) approximation of \eqref{post_pyp}, which depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$.

\begin{thm}\label{char_pyp}
The posterior distribution \eqref{post_pyp} is such that, if $n \rightarrow +\infty$ and $c_{k}\rightarrow+\infty$ simultaneously for any $k \neq j$ while $c_j$ is fixed, then for any fixed $l\geq0$
\begin{align}\label{post_pyp_asyn}
&\prob[f_{X_{n+1}}=l\,|\,\mathbf{C}_{J}=\mathbf{c},h(X_{n+1})=j] \\
&\notag\quad\rightarrow\gamma{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma}{\alpha}+J\right)(\gamma+J\alpha)_{(c_{j}-l)}}{\Gamma\left(\frac{\gamma}{\alpha}+J-1\right)(\gamma+J\alpha-\alpha)_{(c_{j}+1)}}.
\end{align}
\end{thm}

See Appendix \ref{asypyp} for the proof of Theorem \ref{char_dp}. As for the DP prior, a concrete application of the posterior distribution \eqref{post_pyp}, as well as its large sample asymptotic approximation arising from \eqref{post_pyp_asyn}, requires to estimate the unknown prior's parameter $(\alpha,\gamma)$ from the sketch $\mathbf{C}_{J}$. As discussed in \cite{Dol(23)}, under the PYP prior the distribution of $\mathbf{C}_{J}$ has not a simple closed-form expression, thus preventing the use of an empirical Bayes or fully Bayes approach to estimate $(\alpha,\gamma)$. 
\cite{Dol(23)} adopted a likelihood-free approach based on the Wasserstein distance \citep{Ber(19)}, whereby  independent data sets $\mathbf{X}^{\prime}_n$ are sampled from $P\sim\text{PYP}(\alpha^\prime,\gamma^\prime,G_{0})$ and subsequently sketched into $\mathbf{C}^{\prime}_{J}$  with the same hash function $h$. The sketches $\mathbf{C}_{J}$ and $\mathbf{C}^{\prime}_{J}$ are then compared by means of the 1-Wasserstein distance and the empirical estimates for $(\alpha,\gamma)$ are defined as those minimizing (a suitable approximation of) the Wasserstein distance between $\mathbf{C}^{\prime}_{J}$  and $\mathbf{C}_{J}$. This approach is computationally demanding, as it requires to compute the Wasserstein distance, which has a computational cost of $\mathcal O(J^3)$, and to simulate $\mathbf{X}^{\prime}_n$ whose cost scales super-linearly with $n$, depending on the parameters' values. Since the objective function is not differentiable,  \cite{Dol(23)} made use of Bayesian Optimization to find the estimates for the parameters. Here, instead, we propose to consider the first $n^\prime \ll n$ observations, sketch them through $h$ and estimate their frequency through the expected value of \eqref{post_pyp_asyn}. Minimizing the mean absolute error of the frequency recovery with respect to the parameters is easy since the loss function is differentiable (almost everywhere) and can be done with standard software packages.

\subsection{Numerical illustrations}

We present an illustration for the posterior distributions under the DP prior and the PYP prior, i.e. Equation \eqref{post_dp} and Equation \eqref{post_pyp}, respectively. We set $n=50$ and $J=10$, and we consider four  sketched datasets, say $\mathbf C_{J}^{\text{\tiny{(1)}}}$, $\mathbf C_{J}^{\text{\tiny{(2)}}}$, $\mathbf C_{J}^{\text{\tiny{(3)}}}$, and $\mathbf C_{J}^{\text{\tiny{(4)}}}$, whose values are reported in Table \ref{tab:sketch_simu}. In particular: i) for the scenario $\mathbf C_{J}^{\text{\tiny{(1)}}}$, the values $C^{\text{\tiny{(1)}}}_j$'s are constant across $j$; ii) for the scenario $\mathbf C_{J}^{\text{\tiny{(2)}}}$, the values $C^{\text{\tiny{(2)}}}_j$'s decay exponentially in $j$; iii) for the scenario $\mathbf C_{J}^{\text{\tiny{(3)}}}$, the values $C^{\text{\tiny{(3)}}}_j$'s decay linearly in $j$; iv) for scenario $\mathbf C_{J}^{\text{\tiny{(4)}}}$ the values  $C^{\text{\tiny{(4)}}}_j$'s are either equal to nine, five, or one. Moreover, we assume that $X_{n+1}$ is mapped into the bucket $h^{(i)}(X_{n+1})$ such that  $C^{\text{\tiny{(i)}}}_{h^{(i)}(X_{n+1})} = 5$ for $i=1,2,3,4$. We consider a PYP prior with parameter $\gamma=1$ and parameter $\alpha = 0,\, 0.1,\, 0.3,\, 0.5$; recall that the PYP prior with parameter $\alpha=0$ and $\gamma>0$ coincides with the DP prior with parameter $\gamma$. 
Figure \ref{fig:py_post} summarizes the posterior distribution of $f_{X_{n+1}}$ for the three different scenarios. Given the sufficiency of $C_{h(X_{n+1})}$ under the DP prior, the corresponding posterior distributions do not change across the different scenarios; moreover, the posterior distributions under the DP prior are the most concentrated on larger values. We observe that, for all the different scenarios, increasing the parameter $\alpha$ corresponds to pushing the posterior to lower values. Finally, there are some differences in the posterior distributions under the PYP priors across the different scenarios, although these are not so evident from the plots. In particular, for $\alpha = 0.3$ and $\alpha=0.5$, under setting $i=2$ the posterior mass assigned to 5 is slightly larger than in the other settings.


\begin{table}[t]
\centering
  \begin{tabular}{c | c c c c c c c c c c}
     $\mathbf C_{J}^{\text{\tiny{(1)}}}$ & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5  \\[0.2cm]
     $\mathbf C_{J}^{\text{\tiny{(2)}}}$ & 14 & 10 & 7 & 5 & 4 &3 & 2 & 2 & 2 & 1   \\[0.2cm]
     $\mathbf C_{J}^{\text{\tiny{(3)}}}$ & 10 & 9 & 8 & 7 & 5 & 4 & 3 & 2 & 1 & 1  \\[0.2cm]
      $\mathbf C_{J}^{\text{\tiny{(4)}}}$ & 9 & 9 & 9 & 5 & 5 & 5 & 5 & 1 & 1 & 1  \\[0.2cm]
  \end{tabular}
  \caption{\label{tab:sketch_simu} Empirical frequencies of the $j$-th bucket (by column) in each simulated scenario (by row).}
  \end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/posterior_dp_py_full}
  \caption{Posterior distribution of $f_{X_{n+1}}$ in the four different scenarios. In each plot, the blue, orange, green, and red bars correspond to $\alpha$ equal to $0$, $0.1$, $0.3$, and $0.5$ respectively, while $\gamma = 1$. }
  \label{fig:py_post}
\end{figure}

As a further illustration of our results, we consider $n=500,000$ data points generated from a Dirichlet process with parameter $\theta = 5, 10, 20, 100$, and from a Zipf distribution with parameter $c=1.18, 1.54, 1.82, 2.22$, which is typically used to model frequencies with power-law tail behavior. In particular, we recall that the parameter $c$ controls the tails of the Zipf distribution, with lower values of $c$ corresponding to heavier tails (i.e., larger fractions of types with a low frequency). We assume a DP prior, and estimate its total mass by maximizing \eqref{marg} using the Python package \texttt{JAX}. We let $J$ vary between $100$ and $5,000$. We assess the goodness of \eqref{post_dp} by comparing the a posteriori expected value with the true frequency of each token in the dataset, and compute the mean absolute error (MAE) stratified by the true frequency of the tokens. As shown in Figure \ref{fig:dp_simu}, in the well specified settings the MAEs for low and mid frequency tokens decrease rapidly with $J$ especially when the parameter $\theta$ is small. On the other hand, for data generated from the Zipf distribution, the MAEs are considerably larger, especially when $c$ is small. Such a behaviour is not surprising, since the DP prior is known to be suitable for modeling discrete distributions with geometric tails, thus not being able to capture the power-law behavior of the Zipf distribution.

Finally, we consider the same synthetic datasets of \cite{Dol(23)}, which consist of $n=500,000$ samples from the Zipf  distribution with parameter $c=1.18, 1.54, 1.82, 2.22$. We make use of \eqref{post_pyp_asyn} to estimate the frequencies via the posterior expectation, and then we compare estimating the parameters via the likelihood-free approach in \cite{Dol(23)} (specifically, we refer to their estimated values) and our method based on minimizing the recovery error on the first 10,000 samples. Results are displayed in Figure \ref{fig:py_asyn_simu}. Note that, in order to satisfy the asymptotic regime of Theorem \ref{char_pyp}, we fix $J$ to smaller values than the previous simulation. It is clear that the proposed method of estimation yields to a substantial improvement for the frequency recovery problem. Moreover, the use of a PYP prior results in much lower MAEs compared to a DP prior, especially for low frequency tokens. Interestingly, while under the DP prior larger values of the parameter $c$ correspond to better performance, under the PYP it is the opposite. This can be explained by the fact that when $c$ is large, there will be a small number of data points that contribute to most of the total mass, typically referred to as heavy hitters. Hence, when predicting the frequency of the heavy hitters, the asymptotic regime under which \eqref{post_pyp_asyn} is derived does not hold, as it can be the case that $c_j$ is comparable to $n$ for some $j$. Similarly, if $J$ is large, it might happen that $c_j \approx 0$ for some values of $j$, which also justifies the fact that for $c=1.82$ the performance of deteriorates as the Zipf's exponent $c$ increases.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/dp_dp_simu}
  \includegraphics[width=\linewidth]{images/dp_zipf_simu}
  \caption{MAEs as a function of $J$ for the frequency recovery problem under the DP prior. Top row: data simulated from a Dirichlet process. Bottom row:  data generated from the Zipf distribution.}
  \label{fig:dp_simu}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/py_zipf_simu}
  \caption{MAEs as a function of $J$ for the frequency recovery problem of Zipfian data under the PYP prior. Solid lines correspond to prior's parameters $(\alpha,\theta)$ estimated using our method, whereas dashed lines correspond to prior's parameters $(\alpha,\theta)$ estimated as in \cite{Dol(23)}.}
  \label{fig:py_asyn_simu}
\end{figure}


\section{BNP frequency recovery: ``traits" formulation}\label{sec3}
The ``traits" formulation of the frequency recovery problem generalizes the ``species" formulation by assuming  that data belong to more than one symbol, referred to as traits, and exhibit nonnegative integer levels of associations with each trait. We assume $n\geq1$ data points $(x_{1},\ldots,x_{n})$ to be modeled as a random sample $\mathbf{X}_{n}=(X_{1},\ldots,X_{n})$, whose BNP model will be specified below, with each $X_i$ taking a value in the set  $\mathbb S^{\infty} \times \N_0^{\infty}$. That is, we write $X_i = ((\tilde Y_{i,j}, \tilde A_{i,j}), j=1, \ldots, K_i)$, with the $\tilde A_{i,j}$'s being the levels of associations in $\mathbb{N}_{0}$ and the $\tilde Y_{i,j}$'s being the traits' labels in $\mathbb{S}$. The sketch of $\mathbf{X}_{n}$ is performed at the trait level: a random hash function $h:\mathbb{S}\rightarrow\{1,\ldots,J\}$ from the hash family $\mathcal{H}_{J}$ is applied to the traits $\tilde{Y}_{i,j}$'s of $\mathbf{X}_{n}$, in such a way that each $\tilde Y_{i,j}$ is hashed in a bucket whose counter $C_{h(\tilde Y_{i,j})}$ is incremented by the corresponding level of association $\tilde A_{i,j}$, for $j=1,\ldots,K_{i}$ and $i=1,\ldots,n$. Such a process of hashing $\mathbf{X}_{n}$ through $h$ produces a sketch $\mathbf{C}_{J}=(C_{1},\ldots,C_{J})$ whose $j$-th bucket contains the sum of the levels of associations of the traits that are hashed in $j$, for $j=1\ldots,J$. Denoting by $Y_{n+1, \ell}$ a trait belonging to a new data point $X_{n+1}$, we define the empirical frequency level of $Y_{n+1, \ell}$ as
\begin{equation}\label{eq:trait_rec1}
  f_{Y_{n+1, \ell}} = \sum_{i=1}^n \sum_{j=1}^{K_i} \tilde A_{i, j} I(\tilde Y_{i,j} = Y_{n+1, \ell}).
\end{equation}
Based on the sketch $\mathbf{C}_{J}$ of $\mathbf{X}_{n}$, we consider the problem of BNP estimation of $f_{Y_{n+1, \ell}}$, assuming that $\mathbf{X}_{n}$ is a random sample from the generalized Indian buffet process \citep{Jam(17)}. Under this modeling assumption, the frequency levels of ``traits" are sufficient statistics for the $X_{i}$'s \citep{Cam(18)}, as were the frequencies of ``species" within the BNP model for the $X_{i}$'s in Section \ref{sec2}.

Topic modeling is one of the most popular applications of ``traits" allocations. In particular, consider the problem of classifying documents into topics using a multinomial naive Bayes classifier. Let $(X_i, T_i)$ be a data point, where $T_i \in \{1, \ldots, M\}$ is a topic label. Let $n_j$ be the number of the documents in each topic. Under the naive Bayes assumption, the classification rule is
\begin{equation}\label{eq:multnb}
  \prob[T_i = m \mid X_i = \{(y_{i,j}, a_{i,j})\}_{j=1}^{K_i}] \propto \prob[T_i = m]\prod_{j=1}^{K_i} (\pi^m_{y_{i,j}})^{a_{i, j}},
\end{equation}
where $\pi^m_{y_{i,j}}$ is the relative frequency of $y_{i,j}$ in the subset of the documents for which $T_i = m$. Then, one may consider a sketched version of \eqref{eq:multnb}, in which $\pi^m_{y_{i,j}}$ is replaced by the Bayesian estimator $\hat f^m_{y_{i,j}} / n_j$ from sketched data. Another application is in feature engineering, by computing sketched ``tf-idfs''. In information retrieval, tf-idf is a prominent pre-processing step, which consists in weighting the $\tilde A_{i,k}$'s by the (logarithm of the) inverse of the number of documents displaying the trait $\tilde Y_{i,k}$. That is, the level of association for $\tilde Y_{i, k}$ is
\begin{equation}\label{tf-idfs}
    \frac{\tilde A_{i,k}}{\sum_{j=1}^{K_i} \tilde A_{i, j}} \log \frac{n}{\sum_{i=1}^n I(\tilde Y_{i, k} \in X_i)}.
\end{equation}
It is also common wisdom that such tf-idfs can be used in place of the raw frequencies in \eqref{eq:multnb} \citep{Ren(03)}. One may consider a sketched version of \eqref{tf-idfs}, in which the counter is incremented by one instead of $\tilde A_{i, k}$ so that $f_{Y_{n+1}, \ell}$ is exactly the count of how many documents contain word $Y_{n+1}$. Finally, we mention the work by \cite{Zho(16)}, which combines some ideas from the naive Bayes approach with a BNP model for ``traits" allocations.

\subsection{BNP model and main result}

It is useful to start with the definition of the generalized Indian buffet process, which is arguably one of the most popular framework for BNP modeling of ``traits" allocations. See also \citet{Bro(15)}, \citet{Bro(18)}, and references therein, for a detailed account of  BNP modeling of ``traits" allocations, and generalizations thereof.
\begin{defi}
  Let $\mutilde \sim \text{CRM}(\theta, \rho, G_0)$, that is $\mutilde= \sum_{k \geq 1} J_k \delta_{W_k}$, and let $G_A$ be a probability mass function over $\N_0$. We say that a random variable $X$ given $\mutilde$ is distributed as a generalized Indian buffet process with parameter $G_A$ if 
\begin{displaymath}
      X = \sum_{k \geq 1} A_{k} \delta_{W_k},
\end{displaymath}
where $(A_{k})_{k\geq1}$ is independent of $(W_{k})_{k\geq1}$ and such that $A_{k} \mid J_k \sim G_A(J_k)$ for $k\geq1$.
\end{defi}

For short, we write  $X \mid \mutilde \sim \mbox{IBP}(G_A \mid \mutilde)$ to denote that $X$ is distributed according to a generalized Indian buffet process with parameter $G_A$. Note that $X$ is an infinite random measure, though we assume that only a finite number of the $A_{i,k}$'s are nonzero. The representation of $X$ as a collection of displayed traits with their corresponding levels of association follows by letting 
$\{(\tilde Y_{ j}, \tilde A_{j})\}_j =  \{(W_k \, : A_{k} > 0, A_{k})\}_k$. Accordingly, we write the BNP model:
\begin{align}\label{eq:trait_model_hash} 
  \begin{split}
    C_{j} &\,=\, \sum_{i=1}^{n} \sum_{k \geq 1} A_{i,k} I( h(W_k) = j) \qquad j=1,\ldots,J\\[0.2cm]
    h&\,\sim\,\mathcal{H}_{J}\\[0.2cm]
    X_1,\ldots,X_{n}\,|\,\mutilde &\,\simiid\, \mbox{IBP}(G_A \mid \mutilde)\\[0.2cm]
    \mutilde&\,\sim\,\mbox{CRM}(\theta, \rho, G_0).
  \end{split}
\end{align}
Under the model \eqref{eq:trait_model_hash}, the total weighted frequency in \eqref{eq:trait_rec1} can be expressed as 
\[
  f_{Y_{n+1, \ell}} = \sum_{i=1}^m Z_i(Y_{n+1, \ell}).
\]
Let $\mathbf{C}_{J}=(C_{1},\ldots,C_{J})$ be the sketch obtained through random hashing of $\mathbf{X}_{n}$. Denote by $D_j = h^{-1}(\{j\}) = \{\omega \in \mathbb S \, : h(\omega) = j\}$ for $j=1,\ldots,J$. For a new data point $X_{n+1}$, let $Y_{n+1, \ell}$ be the trait belonging to $X_{n+1}$ whose frequency we are interested in, and let $B_j = X_{n+1}(D_j)$ be the increment to the $j$-th bucket of the sketch given by $X_{n+1}$, for $j=1,\ldots,J$. Let $\mathbf B = (B_1, \ldots, B_J)$.

In the ``species" formulation of the frequency recovery problem, the posterior distribution \eqref{post_pk} takes on the interpretation of the conditional probability that $X_{n+1}$ appeared $l+1$ times in the sample of size $n+1$ given that the sketch of $(X_1, \ldots, X_{n+1})$ is $(C_1, \ldots, C_j + 1, \ldots, C_J)$ and $h(X_{n+1}) = j$. Such a conditional probability has a natural counterpart in the ``traits" formulation, namely
\begin{equation}\label{eq:trait_rec_all}
  \prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j, \mathbf C_J = \mathbf c_J, \mathbf B = \mathbf b \right],
\end{equation}
from which the posterior distribution of $f_{Y_{n+1,\ell}}$ follows from Bayes' theorem. For the conditional probability \eqref{eq:trait_rec_all}, the next proposition shows that $(C_{h(Y+1, \ell)}, B_{h(Y+1, \ell)})$ is sufficient, with respect to $(h(Y_{n+1,\ell}), \mathbf C_J, \mathbf B)$, to estimate $f_{Y_{n+1,\ell}}$.
\begin{prp}\label{prop:rec_trait_all}
Let $\mathbf{C}_{J}$ be an observable sketch of $\mathbf{X}_{n}$ under the model \eqref{eq:trait_model_hash}, and let $X_{n+1}$ be an additional (unobservable) random sample.  Then, for $l=0, \ldots, c$
  \begin{align}\label{eq:trait_rec_single}
   &\prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j, \mathbf C_J = \mathbf c_J, \mathbf B = \mathbf b \right]\\
      &\notag\quad=\prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j,  C_{j} = c, B_{j} = b \right].
  \end{align}
\end{prp}

See Appendix \ref{app:prof_trait_all} for the proof of \Cref{prop:rec_trait_all}. Interestingly, Proposition \ref{prop:rec_trait_all} points out a remarkable difference between the ``species" formulation and the ``traits" formulation of the frequency recovery problem. In the ``species" formulation, Theorem \ref{char_dp} shows how the DP prior is the sole PK prior for which the  posterior distribution of $f_{X_{n+1}}$, given $\mathbf{C}_{J}$ and $h(X_{n+1})$, is equivalent to the posterior distribution of $f_{X_{n+1}}$ given $C_{h(X_{n+1})}$. Instead, in the ``traits" formulation, Proposition \ref{prop:rec_trait_all} shows how for any CRM prior the posterior distribution of $f_{Y_{n+1,\ell}}$, given $\mathbf{C}_{J}$, $\mathbf{B}$ and $h(Y_{n+1,\ell})$, coincides with the posterior distribution of $f_{Y_{n+1,\ell}}$, given $C_{h(X_{n+1})}$ and $B_{h(X_{n+1})}$. That is, in the ``traits" formulation, all CRM priors satisfies a property of sufficiency analogous to that of the DP prior in the  ``species" formulation. The next proposition provides an explicit expression for the posterior distribution \eqref{eq:trait_rec_single}.
\begin{thm}\label{teo:trait_general}
Let $\mathbf{C}_{J}$ be an observable sketch of $\mathbf{X}_{n}$ under the model \eqref{eq:trait_model_hash} and let $X_{n+1}$ be an additional (unobservable) random sample. Then, for $l=0, \ldots, c$
\begin{align}\label{eq:post_trait}
  & \prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j,  C_{j} = c, B_{j} = b \right]\\
  &\notag \quad= \frac{\theta}{J}\frac{\prob\left[ \sum_{k \geq 1} \sum_{i=1}^n A^\prime_{i, k} = c - l, \sum_{k \geq 1}  A^\prime_{n+1, k} = b - a\right]}{\prob\left[ \sum_{k \geq 1} \sum_{i=1}^n A^\prime_{i, k} = c, \sum_{k \geq 1}  A^\prime_{n+1, k} = b\right]}  \\
  &\notag \quad\quad \times  \int_{\R_+} \prob \left[ \sum_{i=1}^n \tilde A_{i} = l, \tilde A_{n+1} = a \mid s \right] \rho(s) \dd s,
\end{align}
where
\begin{itemize}
  \item[i)] $\tilde A_{1} \ldots, \tilde A_{n+1} \mid s \simiid G_A(\cdot \mid s)$;
  \item[ii)] the $A^\prime_{ik}$'s are the projection on the second coordinate of the points of $N^\prime = \{(J^\prime_k, (A^\prime_{i,k})_{i=1}^{n+1})\}_{k \geq 1}$ which is a Poisson process on $\R_+ \times \N_0^{n+1}$ with L\'evy intensity
  \[
      \frac{\theta}{J} \left[\prod_{i=1}^{n+1} G_A(\dd a_i \mid s)\right] \rho(s) \dd s.
  \]
\end{itemize}
\end{thm}

See Appendix \ref{app:proof_trait_general} for a proof of Theorem \ref{teo:trait_general}. In general, the application of Theorem \ref{teo:trait_general} requires to specify the CRM prior, through its L\'evy intensity, and the distribution $G_{A}$ for the levels of associations of traits. Hereafter, we apply Theorem \Cref{teo:trait_general} with $G_{A}$ being the Poisson distribution, which leads to a simple expression for  \eqref{eq:post_trait}, and then we show how to use such an expression as an approximation of \eqref{eq:post_trait} with $G_{A}$ being a Bernoulli distribution. 

\subsection{Results under $G_{A}$ Poisson}\label{sec:poisson_traits}
We consider $G_A(\cdot \mid J_k)$ to be the Poisson distribution with parameters $r J_k$, for fixed $r > 0$. In the Poisson setting, we can take advantage of the closeness of the Poisson distribution under convolution, which leads to a remarkable simplification of the posterior distribution \eqref{eq:post_trait}.

\begin{prp}\label{prop:poisson_traits}
Let $\mathbf{C}_{J}$ be an observable sketch of $\mathbf{X}_{n}$ under the model \eqref{eq:trait_model_hash} with $G_A(J_k)$ being the Poisson distribution with parameter $rJ_k$ for a fixed $r > 0$, and let $X_{n+1}$ be an additional (unobservable) random sample. Furthermore, let $\psi$ be the function defined in \eqref{eq_cond}, and let $\phi^{(n)}(u) = (-1)^{n} \frac{\dd^n}{\dd u^n} e^{-\theta/J \psi(u)}$ and  $\kappa(u, n) = \int_{\R_+} e^{-un} s^n \rho(s) \dd s$. Then, for $l=0, \ldots, c$ it holds
  \begin{align}\label{post_poiss}
    &\prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j,  C_{j} = c, B_{j} = b \right] \\
    &\notag\quad=\frac{\theta}{J} \binom{c}{l} \binom{b}{a}  \frac{\phi^{(c-l+b-a)}((n+1) r) }{\phi^{(c+b)}((n+1) r)} \kappa(l+a, (n+1) r).
  \end{align}
\end{prp}

See Appendix \ref{app:proof_poisson} for a proof of \Cref{prop:poisson_traits}. We further specialize \Cref{prop:poisson_traits} by assuming that $\mutilde$ is a Gamma CRM, i.e. $\rho(s) = s^{-1} \exp\{-s\}$. Under this assumption, the posterior distribution \eqref{post_poiss} simplifies to
\begin{align}\label{eq:pois_gamma}
&\prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j,  C_{j} = c, B_{j} = b \right] \\
  &\notag\quad=\frac{\theta}{J}\binom{c}{l} \binom{b}{a} (l + a - 1)! \frac{\Gamma(\theta/J + c + b - l - a)}{\Gamma(\theta/J + c + b)}.
\end{align}
See Appendix \ref{app:poisson} for the proof of \eqref{eq:pois_gamma}. As a generalization of the Gamma CRM we may consider the generalized Gamma CRM, i.e. $\rho(s) = \alpha \Gamma(1-\alpha)^{-1} s^{- \alpha - 1} e^{-\tau s}$ for $\alpha\in[0,1)$ and $\tau>0$ \citep{Bri(99)}. See also \citet{Pit(03)} and references therein. Under this assumption, the posterior distribution \eqref{post_poiss} simplifies to
\begin{align}\label{eq:pois_gg}
&\prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = a \mid h(Y_{n+1,\ell}) = j,  C_{j} = c, B_{j} = b \right] \\
 &\notag\quad= \frac{\theta}{J} \binom{c}{l} \binom{b}{a}\frac{\alpha (1 - \alpha)_{(l + a -1)}}{(\tau + (n+1)r)^{-\alpha + l + a}} \frac{
        \sum_{i=1}^{c-l+b-a} \left(\frac{\theta}{J}\right)^i \frac{\calC(c - l + b -a, i; \alpha)}{(\tau + (n+1)r)^{c - l + b -a - \alpha i}} 
    }{
    \sum_{i=1}^{c+b} \left(\frac{\theta}{J}\right)^i \frac{\calC(c + b , i; \alpha)}{(\tau + (n+1)r)^{c + b - \alpha i}}
    }.
\end{align}
See Appendix \ref{app:poisson} for the proof of \eqref{eq:pois_gg}. Both the posterior distributions displayed in \eqref{eq:pois_gamma} and \eqref{eq:pois_gg} are simple, and they can easily implemented for any $n$.

As discussed in Section \ref{sec2}, the application of the posterior distribution \eqref{eq:pois_gamma} or \eqref{eq:pois_gg} requires to estimate the unknown parameter $r$, as well as the unknown prior's parameters, from the sketch $\mathbf{C}_{J}$. Because of the closeness under convolution of the Poisson distribution, it is easy to check that the distribution of $\mathbf C_j$ under model \eqref{eq:trait_model_hash} with $G_A(J_k)$ being the Poisson distribution with parameter $rJ_k$ equals to the distribution of the following hierarchical model
\begin{equation}\label{eq:cj_model}
\begin{aligned}
	C_j \mid T^\prime_j & \quad \simind \quad \mbox{Poi}(nrT^\prime_j)\\[0.2cm]
	T^\prime_j & \quad \simind \quad f_{D_j} (\theta, \rho, G_0),
\end{aligned}
\end{equation}
where $f_{D_j}(\theta, \rho, G_0)$ is the distribution of $\mutilde(D_j)$. In particular, if $\mutilde$ is a Gamma CRM, then $f_{D_j}$ is the Gamma distribution with parameters $(\theta/J, 1)$. Hence, 
\begin{equation}\label{like_ibp}
	\prob[\mathbf  C_J = \mathbf c_J] = \frac{(nr)^n}{(1 + nr)^{\theta + n}} \prod_{j=1}^J \frac{\left(\frac{\theta}{J}\right)_{(c_j)}}{c_j!},
\end{equation}
such that the parameters $\theta$ and $r$ may be estimated with the values that maximize, over $(\theta,r)$, the (marginal) likelihood \eqref{like_ibp}. If $\mutilde$ is a generalized Gamma CRM, then $f_{D_j}$ is a generalized Gamma distribution, which is not conjugate to the Poisson distribution, thus making the distribution of the sketch $\mathbf{C}_{J}$ not available in closed form. Then, a likelihood-free approach, along the same lines discussed in Section \ref{sec2} for the PYP prior, can be considered.

\subsection{Results under $G_{A}$ Bernoulli}

Now, we consider $G_A(\cdot \mid J_k)$ to be the Bernoulli distribution with parameter $J_k$. Under this assumption, the levels of associations of traits simply refer to their presence or absence. For this reason, traits are typically referred to as features. In the Bernoulli setting, the conditional distribution of the random variable $S_n = \sum_{k \geq 1} \sum_{i=1}^k A^\prime_{i, k}$ in \eqref{eq:post_trait}, given $\mutilde$, is the distribution of the sum of independent Bernoulli random variables with parameters $J^\prime_1, \ldots, J^\prime_1, J^\prime_2, \ldots, J^\prime_2, \ldots$. where each $J^\prime_k$ appears exactly $n$ times. This is typically referred to as the Poisson-Binomial distribution. \citep{Che(97)}. Similarly, the random variable $Z = \sum_{k \geq 1} A^\prime_{n+1, k}$ is distributed as a Poisson-Binomial distribution with parameter $J^\prime_1, J^\prime_2, \ldots$. While the Poisson-Binomial distribution has a complicated expression, one may combine Le Cam's theorem \citep{LeCam(60)} with the results in Section \ref{sec:poisson_traits} to obtain an approximation of \eqref{eq:post_trait} in the Bernoulli setting. For notation's sake, let $S_n$ and $Z$ as before, and introduce $\tilde S_n$ and $\tilde Z$ such that, given $T^\prime = \sum_{k \geq 1} J^\prime_k$, $\tilde S_n \mid T^\prime \sim \mbox{Poi}(nT^\prime)$ and $\tilde Z \mid T^\prime \sim \mbox{Poi}(T^\prime)$.
We first observe that the posterior distribution of $f_{Y_{n+1, \ell}}$ in \eqref{eq:post_trait} is proportional to
\begin{align*}
 & \prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = 1 \mid h(Y_{n+1,\ell}) = j,  C_{j} = c, B_{j} = b \right]\\
  &\quad\propto\prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}), S_n =  c-l, Z = b-1\right].
\end{align*}
In the next theorem, we provide an approximation of the distribution of $f_{Y_{n+1,\ell}}$ by replacing $S_n$ and $Z$ with $\tilde S_n$ and $\tilde Z$ respectively. We also obtain an estimate of the error introduced on the joint distribution on the right-hand side

\begin{thm}\label{prop:bern_traits}
Let $\mathbf{C}_{J}$ be an observable sketch of $\mathbf{X}_{n}$ under the model \eqref{eq:trait_model_hash} with $G_A(J_k)$ being the Bernoulli distribution with parameter $J_k$, and let $X_{n+1}$ be an additional (unobservable) random sample. Furthermore, let $\psi$ be the function defined in \eqref{eq_cond}, and let $\phi^{(n)}(u) = (-1)^{n} \frac{\dd}{\dd u} e^{-\theta/J \psi(u)}$ and  $\kappa(u, n) = \int_{\R_+} e^{-un} s^n \rho(s) \dd s$. Then, the posterior distribution of $f_{Y_{n+1, \ell}}$ can be approximated by the distribution of the random variable $\tilde f_{Y_{n+1, \ell}}$ such that, for $l=0, \ldots, c$:
  \begin{align}\label{eq:post_features}
    & \prob\left[\tilde f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = 1\right]\\
     & \notag\quad= \prob\left[f_{Y_{n+1,\ell}} = l, X_{n+1}(Y_{n+1,\ell}) = 1 \mid \tilde S_n =  c-l, \tilde Z = b-1\right] \\
     & \notag\quad\propto
    (c - l + 1)_{(l)} \binom{n}{l} \phi^{(c+b - l - 1)}(n+1) \int (s^{l+1} - s^{n+1}) \rho(s) \dd s.
    \end{align}
  Moreover, the total variation distance between the vectors of random variables $[f_{Y_{n+1, \ell}}, \allowbreak X_{n+1}(Y_{n+1, \ell}),S_n, Z]$ and $[f_{Y_{n+1, \ell}}, X_{n+1}(Y_{n+1, \ell}), \tilde S_n, \tilde Z]$
 is upper bounded by
  \[
    \frac{2\theta}{J} \int_{\R_+} e^{-\psi(u)} \kappa(u, 2) \dd u.
  \]
\end{thm}

\subsection{A numerical illustration}

Assuming the Poisson setting, we compare the posterior distribution of $f_{Y_{n+1, \ell}}$, under a Gamma CRM prior and a generalized Gamma CRM prior.  First, we observe that, if $a = b = 1$, then the posterior distribution in \eqref{eq:pois_gamma} under the Gamma CRM prior coincides with the posterior distribution of $f_{X_{n+1}}$ under the DP prior in \eqref{post_dp}. In all other cases, the posterior distribution of $f_{Y_{n+1, \ell}}$ is ``enriched'' by the information carried by $X_{n+1}(Y_\ell) = a$ and $X_{n+1}(D_{h(Y_{n+1,\ell})}) = b$. In particular, these quantities can inform the relative frequency of the trait $Y_{n+1,\ell}$ inside the bucket $h(Y_{n+1,\ell})$. \Cref{fig:poi_post} shows a comparison of the posterior distributions under a Gamma CRM prior and a generalized Gamma CRM prior, with different parameters. From such a figure, it is interesting to notice how the different prior specifications lead to substantially different posterior when
$X_{n+1}(Y_{n+1, \ell})$ and are small. Moreover, observe how for a fixed value of $X_{n+1}(D_{h(Y_{n+1,\ell})}) = b$, values of $X_{n+1}(Y_\ell)$ that are close to $b$ push the posterior distribution to large values, as this means that the trait $Y_{n+1,\ell}$ is common inside that bucket. Similarly, if $X_{n+1}(Y_\ell)$ is close to zero (or significantly smaller than $b$), this will push the posterior of $f_{Y_{n+1, \ell}}$ close to zero as well.


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{images/poi_post_new.pdf}
  \caption{Posterior distribution of the empirical frequency level $f_{Y_{n+1,\ell}}$, assuming the Poisson setting with a Gamma CRM prior (blue line) and with a generalized Gamma CRM prior (orange and green lines). In all the panels, $a$ and $b$ vary, while we fix $c=50$, $m=1000$, $J=50$, $\theta=0.3$, $\tau = 1$, $r=1$, and $\alpha = 0.25, 0.75$ for the orange and green lines respectively.}
  \label{fig:poi_post}
\end{figure}


\section{Discussion}\label{sec4}

This paper introduced a BNP approach to frequency recovery, both in the common ``species" formulation and in a more general ``traits" formulation of the problem. While the works \citet{Cai(18)} and \citet{Dol(21),Dol(23)} applied BNPs to develop learning-augmented versions of the CMS under the DP and PYP priors, our approach is not built from the CMS, and considers the broad class of PK priors. For a PK prior, in Theorem \ref{main_pk} we provided the posterior distribution of the empirical frequency of a symbol $X_{n+1}$, given the sketch $\mathbf{C}_{J}$, which has been then applied to show peculiar properties of the DP and the PYP priors: i) Theorem \ref{char_dp} characterized the DP prior as the sole PK prior for which the posterior distribution depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$; ii) under the PYP prior, Theorem \ref{char_pyp} identified a large sample regime to obtain an asymptotic approximation of the posterior distribution that depends on $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$. The ``traits" formulation of the frequency recovery problem is a further novelty with respect to \citet{Cai(18)} and \citet{Dol(21),Dol(23)}, and also with respect to the CMS literature, showing the flexibility of the BNP approach to deal with general data structures. In particular, for a CRM prior, in Theorem \ref{teo:trait_general} we provided the posterior distribution of the empirical frequency level of a trait $Y_{n+1, \ell}$,  given the sketch $\mathbf{C}_{J}$, showing how the Poisson process formulation of CRMs leads to a dependence of $\mathbf{C}_{J}$ only through $C_{h(Y_{n+1, \ell})}$. This result has been applied to the Poisson and Bernoulli distribution for the levels of associations of traits: i) Proposition \ref{prop:poisson_traits} showed how, under the Gamma and generalized Gamma CRM priors, the Poisson distribution lead to simple posterior distributions; ii) for the Bernoulli distribution, Theorem \ref{prop:bern_traits} introduced a simple approximation of the posterior distribution.

Assuming the true data to be available, PK priors are widely used in BNPs, mostly for their mathematical tractability, leading to posterior inferences that are simple, computationally efficient and scalable to massive data sets \citep{Lij(10)}. Our work shows how this desirable property of PK priors is no longer valid in BNP inference from sketched data, which poses additional challenges and constraints in the choice of the prior distribution. In particular, we showed how the PYP prior, which is arguably the most popular PK prior generalizing the DP, leads to a posterior distribution that is intractable to be evaluated, even for moderately large sample sizes. In general, our results show that the BNP approach to frequency recovery leads to non-trivial computational challenges in the evaluation of the posterior distribution under priors beyond a DP, which paves the way to: i) further investigating exact and approximate algorithms for an efficient (numerical) evaluation of the posterior distribution under the PYP prior; ii) further studying large sample approximations, possibly with (reliable) error bounds, of the posterior distribution under the PYP prior; iii) proposing different priors, with the aim of obtaining simple posterior distributions, while exhibiting more flexible tail behaviors than the DP. Differently from the ``species" formulation of the frequency recovery problem, our results in the ``traits" formulation show a much larger flexibility with respect to the choice of a prior distributions, leading to posterior distributions that are tractable to be evaluated for large sample sizes, at least assuming a Poisson distribution for the levels of associations of traits. Such a desirable property is determined by the Poisson process formulation of CRMs, which determines a posterior distribution that depends on the sketch $\mathbf{C}_{J}$ only through $C_{h(X_{n+1})}$.

By considering the ``traits" formulation of the frequency recovery problem, we provided a further evidence of the flexibility of the BNP approach to deal with a broad range of discrete data structures. We argue that the BNP approach also allows for a great flexibility with respect to the object of interest, due to the use of the posterior distribution as the main tool to obtain estimates. For instance, our BNP approach may be extended to deal with more general quantities than the empirical frequency of a new data point, i.e. $f_{x_{n+1}}$. Of notable interest in the CMS literature is the problem of recovering the (cumulative) empirical frequency of $s\geq1$ new data points \citep[Chapter 3]{Cor(20)}. To describe such a problem, it is useful to consider the ``species" formulation of the frequency recovery problem. In particular, for $n\geq1$ let $(x_{1},\ldots,x_{n})$ be a collection of $\mathbb{S}$-valued data points, and for a positive integer $J$ let $h:\mathbb{S}\rightarrow\{1,\ldots,J\}$ be a random hash function from the hash family $\mathcal{H}_{J}$. Assuming $(x_{1},\ldots,x_{n})$ to be available through the sketch $\mathbf{C}_{J}$, the goal is to recover the vector of empirical frequencies $(f_{x_{n+1}},\ldots,f_{x_{n+s}})$ of $s$ new data points $(x_{n+1},\ldots,x_{m+s})$ in $(x_{1},\ldots,x_{n})$, with the $r$-th empirical frequency $f_{x_{n+r}}$ being defined as $f_{x_{n+r}}=\sum_{1\leq i\leq n}I(x_{i}=x_{m+r})$, for $r=1,\ldots, s$. Then, the (cumulative) empirical frequency is obtained by summing the $f_{x_{n+r}}$'s. In principle, the arguments of Theorem \ref{main_pk} may be easily extended to provide the posterior distribution of $(f_{X_{n+1}},\ldots,f_{X_{n+s}})$, given $\mathbf{C}_{J}$ and the buckets in which the $X_{n+r}$'s are hashed, though we expect such a posterior distribution to have a rather complicated form, even under the DP prior. We refer to \citet{Dol(23)} for a discussion in the context of learning-augmented versions of the CMS. 



\appendix


\section{Proofs}

\subsection{Proof of \Cref{main_pk}}\label{app:proof_main}

By the definition of conditional probability, we have that
\begin{equation}\label{eq:sketch_post_fract}
  \prob\left[ f_{X_{n+1}} = l \mid \bm C = \bm c, h(X_{n+1}) = j\right] = \frac{\prob\left[ f_{X_{n+1}} = l, \bm C = \bm c, h(X_{n+1}) = j\right]}{\prob\left[ \bm C = \bm c, h(X_{n+1}) = j\right]} 
\end{equation}
Let $g(t) = Z_T t^{-\gamma} e^{-\beta t}$, where $Z_T$ is a normalizing constant. 

\paragraph{Denominator}
We have 
\begin{align}
  & \prob[\bm C = \bm c, h(X_{n+1}) = j] \nonumber \\
  & \qquad = \binom{n}{c_1, \ldots, c_J} \prob\Big[X_1 \in D_j, \ldots, X_{c_j} \in D_j, X_{n+1} \in D_j,  \nonumber \\
  & \hspace{4cm} X_{i} \in D_1, i \in [c_j: c_j + c_1], \ldots, X_{i} \in D_j, i \in [\sum_{k=1}^{J-1} c_k, n] \Big] \nonumber \\
  & \qquad = \binom{n}{c_1, \ldots, c_J} \E\left[ P(D_j)^{c_j + 1} \prod_{k \neq j} P(D_k)^{c_k} \right] \label{eq:denom_probs} 
\end{align}
From \eqref{eq:denom_probs}, writing $P(\cdot) := \mutilde(\cdot) / \mutilde(\Sd)$ and the expression of $g(t) = g(\mutilde(\Sd))$
\begin{align*}
   & \prob[\bm C = \bm c, h(X_{n+1}) = j] = \binom{n}{c_1, \ldots, c_j} \E\left [P(D_j)^{c_j + 1} \prod_{k \neq j} P(D_k)^{c_k}\right] \\
   & \qquad = Z_T  \binom{n}{c_1, \ldots, c_j} \E\left[\mutilde(\Sd)^{-n  - 1- \gamma} e^{-\beta \mutilde(\Sd)}  \mutilde(D_j)^{c_j + 1} \prod_{k \neq j} \mutilde(D_k)^{c_k}\right] \\
   & \qquad = Z_T  \binom{n}{c_1, \ldots, c_j} \int_{\R_+ } \frac{u^{n+\gamma}}{\Gamma(n + \gamma + 1)} \E\left[e^{-(u + \beta) \mutilde(D_j)} \mutilde(D_j)^{c_j + 1} \right] \\
   & \qquad \qquad \qquad \times \prod_{k \neq j} \E\left[e^{-(u + \beta) \mutilde(D_k)} \mutilde(D_k)^{c_k} \right] \\
   & \qquad = Z_T \binom{n}{c_1, \ldots, c_j} \int_{\R_+ } \frac{u^{n+\gamma}}{\Gamma(n + \gamma + 1)} (-1)^{c+1} \frac{\dd^{c_j+1}}{\dd z^{c_j+1}} e^{- \theta \psi(z)/J}|_{(u + \beta)}   \\
   & \hspace{4cm} \prod_{k\neq j} (-1)^{c_k}\frac{\dd^{c_k}}{\dd z^{c_k}} e^{- \theta \psi(z)/J}|_{(u + \beta)} \dd u
\end{align*}
where the second equality follows from the definition of Poisson-Kingman model and the third one from the Gamma identity $\mutilde(\Sd) ^{-n -1 - \gamma} = \int_{\R_+} u^{n+\gamma} / \Gamma(n + \gamma + 1) e^{-u \mutilde(\Sd)} \dd x$, an application of Fubini's theorem, and the independence property of CRMs. Finally the last equality follows from the properties of the exponential function and the definition of Laplace exponent of the CRM.

\paragraph{Numerator}

Let $B_{\omega}$ denote a ball of radius $\varepsilon$ around $\omega \in \Sd$. 
We have
\begin{align*}
    &\prob\left[ \sum_{i=1}^n \indicator_{(X_{n+1})}(X_i) = l, \bm C = \bm c, h(X_{n+1}) = j\right] = \\
    & \qquad \lim_{\varepsilon \rightarrow 0} \int_{D_j} \prob\left[\sum_{i=1}^n \indicator_{B_{\omega^*}}(X_i) = l, \bm C = \bm C, X_{n+1} \in B_{\omega^*} \right] \dd \omega^*
\end{align*}
We consider the integrand,
\begin{align}
    & \prob\left[\sum_{i=1}^n \indicator_{B_{\omega^*}}(X_i) = l, \sum_{i=1}^n \indicator_{h(\omega^*)}(h(X_i)) = c, X_{n+1} \in B_{\omega^*} \right] \nonumber \\
    & \qquad = \binom{n}{l}\binom{n-l}{c_1, \ldots, c_j - l, \ldots c_J} \prob\Big[X_1 \in B_{\omega^*} \ldots X_l \in B_{\omega^*}, X_{n+1} \in B_{\omega^*}, \nonumber \\
    & \hspace{5cm} X_{l+1} \in D_j \setminus B_{\omega^*} \ldots, X_{c_j} \in D_j \setminus B_{\omega^*}, \nonumber \\ 
    &  \hspace{5cm} X_{i} \in D_1, i \in [c_j: c_j + c_1], \ldots, X_{i} \in D_j, i \in [\sum_{l=1}^{J-1} c_l, n] \Big] \nonumber \\
     & \qquad = \frac{1}{l!}\binom{n}{c_1, \ldots, c_j - l, \ldots c_J} \E\left[P(B_{\omega^*})^{l+1} P(D_j \setminus B_{\omega^*})^{c_j-l} \prod_{k \neq j} P(D_k)^{c_k} \right] \label{eq:num_probs}
\end{align}
To evaluate the expected value, we proceed as in the case of the denominator and write
\begin{align*}
    &  \E\left[P(B_{\omega^*})^{l+1} P(D_j \setminus B_{\omega^*})^{c_j-l} \prod_{k \neq j} P(D_k)^{c_k} \right] \\
    & \qquad  = Z_T \int_{\R_+ } \frac{u^{n + \gamma}}{\Gamma(n + \gamma + 1)} \E \left[ e^{- (u + \beta) \mutilde(B_{\omega^*})} \mutilde(B_{\omega^*})^{l+1} \right]  \\
    & \qquad \qquad \times 
    \E \left[ e^{- (u + \beta) \mutilde(D_j \setminus B_{\omega^*})}  \mutilde(D_j \setminus B_{\omega^*})^{c-l}  \right]
    \prod_{k \neq j} \E\left[e^{-(u + \beta) \mutilde(D_k)} \mutilde(D_k)^{c_k} \right]
    \dd u
\end{align*}
The two latter expected values above can be computed as in the denominator case.
For the first expectation instead, letting $\varepsilon \rightarrow 0$ we have
\[
    \E \left[ e^{- (u + \beta) \mutilde(B_{\omega^*})} \mutilde(B_{\omega^*})^{l+1} \right] \rightarrow \kappa(u + \beta, l+1) \theta G_0(\dd \omega^*)
\]
where $\kappa(u, l) = \int_{\R_+} s^{l} e^{-us} \rho(s) \dd s$. This can be verified using, for instance, Lemma 1 in \cite{Cam(19)}.
Hence, the numerator equals
\begin{align*}
    & \prob\left[ \sum_{i=1}^n \indicator_{(X_{n+1})}(X_i) = l, \bm C = \bm c, h(X_{n+1}) = j\right] \\
    & \qquad \frac{1}{l!}\binom{n}{c_1, \ldots, c_j - l, \ldots c_J}\frac{Z_T}{\Gamma(n + \gamma + 1)} \\
    & \hspace{2cm} \times\int_{D_j} \int_{\R_+} u^{n + \gamma} (-1)^{c_j - l} \left(\frac{\dd^{c_j-l}}{\dd (u+\beta)^{c_j-l}} e^{- \theta / J \psi(u+\beta)}\right) \times \\ 
    & \hspace{3cm}\times \prod_{k\neq j} (-1)^{c_k}\frac{\dd^{c_k}}{\dd z^{c_k}} e^{- \theta \psi(z)/J}|_{(u + \beta)} \kappa(u + \beta, l+1) \theta \dd u G_0(\dd \omega^*)  
\end{align*}
where we can further integrate with respect to $\dd \omega^*$ and observe $\int_{D_j} G_0(\dd \omega^*) = 1 / J$ thanks to the universality assumption on the hash function $h$.
Combining numerator and denominator, and using the definition of $\phi^{(n)}(u)$, yields the result.

\subsection{Proof of \Cref{char_dp}}\label{app:char_proof}

Recalling the definition of $\psi^{(n)}(u)$, the claim of the theorem entails that
\[
    \frac{\int_{\R_+} u^{n + \gamma} \frac{\dd^{c_j-l}}{\dd (z)^{c_j-l}} e^{- \theta / J \psi(z)}|_{u+\beta}
    \prod_{k\neq j} \frac{\dd^{c_k}}{\dd z^{c_k}} e^{-\theta \psi(z)/J}|_{(u + \beta)} \kappa(u + \beta, l+1) \dd u }{\int_{\R_+ } u^{n+\gamma} \frac{\dd^{c_j+1}}{\dd z^{c_j+1}} e^{- \theta \psi(z)/J}|_{(u + \beta)}  \prod_{k\neq j} \frac{\dd^{c_k}}{\dd z^{c_k}} e^{- \theta \psi(z)/J}|_{(u + \beta)} \dd u} = f(n, c_j, l)
\]
where $f$ is an unknown function which cannot depend on $c_k, k \neq j$.
Then
\begin{multline*}
    \int_{\R_+} u^{n+ \gamma} \frac{\dd^{c_j-l}}{\dd (z)^{c_j-l}} e^{- \theta / J \psi(z)}|_{u+\beta}
    \prod_{k\neq j} \frac{\dd^{c_k}}{\dd z^{c_k}} e^{-\theta \psi(z)/J}|_{(u + \beta)} \kappa(u + \beta, l+1) \dd u = \\
    f(n, c_j, l) \int_{\R_+ } u^{n+\gamma} \frac{\dd^{c_j+1}}{\dd z^{c_j+1}} e^{- \theta \psi(z)/J}|_{(u + \beta)}  \prod_{k\neq j} \frac{\dd^{c_k}}{\dd z^{c_k}} e^{- \theta \psi(z)/J}|_{(u + \beta)} \dd u
\end{multline*}
or equivalently
\begin{multline*}
     \int_{\R_+} u^{n + \gamma} \prod_{k\neq j} \frac{\dd^{c_k}}{\dd z^{c_k}} e^{- \theta \psi(z)/J}|_{(u + \beta)} \times \\
     \left( \frac{\dd^{c_j-l}}{\dd (z)^{c_j-l}} e^{- \theta / J \psi(z)}|_{u+\beta} \kappa(u + \beta, l+1) - f(n, c_j, l) \frac{\dd^{c_j+1}}{\dd z^{c_j+1}} e^{- \theta \psi(z)/J}|_{(u + \beta)}  \right) \dd u = 0
\end{multline*}
Since the above equality must hold true for all values of $c_k, k \neq j$, it is easy to see that
\begin{equation}\label{eq:diffeq}
     \frac{\dd^{c_j-l}}{\dd (z)^{c_j-l}} e^{- \theta / J \psi(z)}|_{u+\beta} \kappa(u + \beta, l+1) - f(n, c_j, l) \frac{\dd^{c_j+1}}{\dd z^{c_j+1}} e^{- \theta \psi(z)/J}|_{(u + \beta)} \equiv 0
\end{equation}
for all $u \geq 0$.

The simplest nontrivial case is when $c_j = 1, l=0$ (indeed note that if $c_j = 0$, $l=0$ by definition and we get $f(n, c_j, l) = 1$).
Now, note that
\begin{align*}
    \frac{\dd}{\dd z} e^{- \theta / J \psi(z)} &= e^{- \theta / J \psi(z)} \left(- \frac{\theta}{J}\right) \frac{\dd}{\dd z} \psi(z) \\
    \frac{\dd^2}{\dd z^2} e^{- \theta / J \psi(z)} &= e^{- \theta / J \psi(z)} \left[\left(- \frac{\theta}{J}\right) \frac{\dd}{\dd z} \psi(z)\right]^2 + e^{- \theta / J \psi(z)} \left(- \frac{\theta}{J}\right) \frac{\dd^2}{\dd z^2} \psi(z)
\end{align*}
Plugging these into \eqref{eq:diffeq} we get
\begin{multline*}
    e^{- \theta / J \psi(u+\beta)} \Bigg\{  - \frac{\theta}{J} \frac{\dd}{\dd z} \psi(z)|_{ u+\beta} \kappa(u+\beta, 1) + \\
     - f(n, c_j, l) \left( \left[\left(- \frac{\theta}{J}\right) \frac{\dd}{\dd z} \psi(z)\right]^2_{u+\beta} \right) - \frac{\theta}{J}  \frac{\dd^2}{\dd z^2} \psi(z)|_{u+\beta}  \Bigg\} = 0
\end{multline*}
Given the positivity of the exponential function, we can set the term in the curly brackets equal to zero.
Recalling that $\dd / \dd z \, \psi(z) = - \kappa(z, 1)$, the term in the curly brackets above reduces to
\[
     \frac{\theta}{J} \kappa(u + \beta, 1)^2 - f(n, c_j, l) \left( \frac{\theta^2}{J^2} \kappa(u + \beta, 1)^2 + \frac{\theta}{J} \frac{\dd}{\dd z} \kappa(z, 1)|_{u + \beta}\right) = 0
\]
which entails
\[
    \frac{\dd}{\dd z} \kappa(z, 1) = - \left(\frac{\theta}{J}  - \frac{1}{f(m, c_j, l)} \right) \kappa(z, 1)^2.
\]
Letting $w : = \left(\frac{\theta}{J}  - \frac{1}{f(n, c_j, l)}\right)^{-1}$, the differential equation above can be seen to have solution
\[
    \kappa(z, 1) = \frac{1}{c + \frac{z}{w}} = \frac{w}{\tau + z}
\]
where $c$ is an arbitrary constant and $\tau = c w$.

Let now $c_j = l = 0$ and recall that in this case $f \equiv 1$. Plugging these into \eqref{eq:diffeq} we get
\begin{align*}
    e^{- \theta / J \psi(u+\beta)} \kappa(u+\beta, 1) - \frac{\dd}{\dd z} e^{- \theta / J \psi(z)} &= 0 
\end{align*}
which leads to
\[
    e^{- \theta / J \psi(u+\beta)} \left(\frac{w}{\tau + z} + \frac{\theta}{J} \frac{\dd }{\dd z} \psi(z) \right)_{| u + \beta} = 0
\]
Setting the term in the parentheses equal to zero, we obtain that
\begin{equation}\label{eq:lap_exp}
    \psi(z) = K \log(\tau + z).
\end{equation}
Hence, we have shown that if $\prob\left[ f_{X_{n+1}} = l \mid \bm C = \bm c, h(X_{n+1}) = j\right]$ does not depend on $c_k$, $k \neq j$, the CRM $\mutilde$ must have L\'evy exponent \eqref{eq:lap_exp}.

Let now $\mutilde^\prime$ be a CRM with L\'evy exponent $\psi$ as above. We first note that, without loss of generality, we can set $K=1$ since setting $K \neq 1$ the Laplace transform
\[
    \E e^{- z \mutilde^\prime(A)} = e^{- K \log(\tau + z) \theta G_0(A)} = (\tau + z)^{-K\theta G_0(A)} =: F^\prime(z)
\]
simply amounts to rescaling the total mass parameter.

We now show that $\tau$ must necessarily be equal to one.
Note that if $\mutilde_G$ is a Gamma process, then its L\'evy exponent is $\log(1 + z)$, which is \eqref{eq:lap_exp} but shifted of a term $(1 - \tau)$:
\begin{equation}\label{eq:laplace_shift}
     \E e^{- z \mutilde_G(A)} = (1 + z)^{-K\theta G_0(A)} =  F^\prime(z + (1 - \tau))
\end{equation}
Let $f_A$ be the probability density function of $\mutilde_G(A)$, and $f^\prime_A$ be the probability density function of $\mutilde^\prime(A)$. By the properties of the Laplace transform, \eqref{eq:laplace_shift} is equivalent to
\[
    f_A(t) = e^{(\tau - 1)t} f^\prime_A(t), \quad \text{for all } t,
\]
which is clearly impossible if $\tau \neq 1$ since we must have that both $f_A$ and $f^\prime_A$ must integrate to one since they are probability density functions.
Hence, we have shown that if $\prob\left[ f_{X_{n+1}} = l \mid \bm C = \bm c, h(X_{n+1}) = j\right]$ does not depend on $c_k, k \neq j$, the underlying CRM must be a Gamma process.

We are left with two degrees of freedom: namely the parameters $\gamma$ and $\beta$ defining the change of measure in the Poisson-Kingman model.
However, note that if $\tilde \mu$ is a Gamma process, the resulting PK model with $g(t) \propto t^{-\gamma} e^{-\beta t}$ is still a Dirichlet process. This can be checked, for instance, starting from Eq. (177) in \cite{Jam(02)}. This concludes the proof.


\subsection{Proof of Theorem \ref{char_pyp}}\label{asypyp}

The proof relies on the use of an asymptotic property for a class of (normalized) generalized factorial coefficients. In particular, according to \citet[Lemma 2]{Dol(20)}, for any $z>0$ it holds
\begin{equation}\label{asym_gfc_n}
\lim_{n\rightarrow+\infty}\frac{z^{k}\mathscr{C}(n,k;s)}{\sum_{k=1}^{n}z^{k}\mathscr{C}(n,k;s)}=\text{e}^{-z}\frac{z^{k-1}}{(k-1)!}.
\end{equation}
We start by rewriting the numerator and the denominator of the posterior distribution \eqref{post_pyp}. With regards to the numerator of \eqref{post_pyp}, we can write the following:
\begin{align}\label{num_posterior}
&\frac{\gamma}{J}{c_{j}\choose l}(1-\alpha)_{(l)}\\
&\notag\quad\quad\times\sum_{i_{1}=1}^{c_{1}}J^{-i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)\cdots\sum_{i_{j}=1}^{c_{j}-l}J^{-i_{j}}\mathscr{C}(c_{j}-l,i_{j};\alpha)\cdots\sum_{i_{J}=1}^{c_{J}}J^{-i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)\\
&\notag\quad\quad\quad\times\Gamma\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+\cdots+i_{J}\right)\\
&\notag\frac{\gamma}{J}{c_{j}\choose l}(1-\alpha)_{(l)}\Gamma\left(\frac{\gamma+\alpha}{\alpha}\right)\left(\prod_{1\leq s\neq j\leq J}\sum_{i_{s}=1}^{c_{s}}\frac{1}{J^{i_{s}}}\mathscr{C}(c_{s},i_{s};\alpha)\right)\\
&\notag\quad\quad\times\sum_{i_{j}=1}^{c_{j}-l}\left(\frac{1}{J}\right)^{i_{j}}\mathscr{C}(c_{j}-l,i_{j};\alpha)\sum_{i_{1}=1}^{c_{1}}\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{1})}\frac{\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}{\sum_{i_{1}=1}^{c_{1}}\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}\cdots\\
&\notag\quad\quad\quad\cdots\times\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\cdots\\
&\notag\quad\quad\quad\quad\cdots\times\sum_{i_{J}=1}^{c_{J}}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\frac{\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}{\sum_{i_{J}=1}^{c_{J}}\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}.
\end{align}
By means of the same arguments, we can write the denominator of \eqref{post_pyp} as follows:
\begin{align}\label{den_posterior}
&\sum_{i_{1}=1}^{c_{1}}J^{-i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)\cdots\sum_{i_{j}=1}^{c_{j}+1}J^{-i_{j}}\mathscr{C}(c_{j}+1,i_{j};\alpha)\cdots\sum_{i_{J}=1}^{c_{J}}J^{-i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)\\
&\notag\quad\times\Gamma\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+\cdots+i_{J}\right)\\
&\notag\quad=\Gamma\left(\frac{\gamma}{\alpha}\right)\left(\prod_{1\leq s\neq j\leq J}\sum_{i_{s}=1}^{c_{s}}\frac{1}{J^{i_{s}}}\mathscr{C}(c_{s},i_{s};\alpha)\right)\\
&\notag\quad\quad\times\sum_{i_{j}=1}^{c_{j}+1}\left(\frac{1}{J}\right)^{i_{j}}\mathscr{C}(c_{j}+1,i_{j};\alpha)\sum_{i_{1}=1}^{c_{1}}\left(\frac{\gamma}{\alpha}\right)_{(i_{1})}\frac{\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}{\sum_{i_{1}=1}^{c_{1}}\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}\cdots\\
&\notag\quad\quad\quad\cdots\times\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\cdots\\
&\notag\quad\quad\quad\quad\cdots\times\sum_{i_{J}=1}^{c_{J}}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\frac{\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}{\sum_{i_{J}=1}^{c_{J}}\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}.
\end{align}
Then, by combining the numerator \eqref{num_posterior} with the denominator \eqref{den_posterior}, we can write the posterior distribution \eqref{post_pyp} as
\begin{align}\label{main_num_den}
&\text{Pr}[f_{X_{n+1}}=l\,|\,\mathbf{C}_{J}=\mathbf{c},h(X_{n+1})=j]\\
&\notag\quad=\frac{\gamma}{J}{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma+\alpha}{\alpha}\right)}{\Gamma\left(\frac{\gamma}{\alpha}\right)}\frac{N_{\gamma,\alpha,J}(l;c_{1},\ldots,c_{J})}{D_{\gamma,\alpha,J}(c_{1},\ldots,c_{J})},
\end{align}
where
\begin{align}\label{num_rewrite}
&N_{\gamma,\alpha,J}(l;c_{1},\ldots,c_{J})\\
&\notag\quad=\sum_{i_{j}=1}^{c_{j}-l}\left(\frac{1}{J}\right)^{i_{j}}\mathscr{C}(c_{j}-l,i_{j};\alpha)\sum_{i_{1}=1}^{c_{1}}\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{1})}\frac{\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}{\sum_{i_{1}=1}^{c_{1}}\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}\cdots\\
&\notag\quad\quad\cdots\times\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\cdots\\
&\notag\quad\quad\quad\cdots\times\sum_{i_{J}=1}^{c_{J}}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\frac{\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}{\sum_{i_{J}=1}^{c_{J}}\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}
\end{align}
and
\begin{align}\label{den_rewrite}
&D_{\gamma,\alpha,J}(c_{1},\ldots,c_{J})\\
&\notag\quad=\sum_{i_{j}=1}^{c_{j}+1}\left(\frac{1}{J}\right)^{i_{j}}\mathscr{C}(c_{j}+1,i_{j};\alpha)\sum_{i_{1}=1}^{c_{1}}\left(\frac{\gamma}{\alpha}\right)_{(i_{1})}\frac{\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}{\sum_{i_{1}=1}^{c_{1}}\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}\cdots\\
&\notag\quad\quad\cdots\times\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\cdots\\
&\notag\quad\quad\quad\cdots\times\sum_{i_{J}=1}^{c_{J}}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\frac{\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}{\sum_{i_{J}=1}^{c_{J}}\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}.
\end{align}
Now, we apply repeatedly \eqref{asym_gfc_n} to both \eqref{num_rewrite} and \eqref{den_rewrite}, starting from the last terms. In particular, for the summation in $i_{J}=1,\ldots,c_{J}$ of \eqref{num_rewrite}, we have that
\begin{align}\label{first_step_num}
&\lim_{c_{J}\rightarrow+\infty}\sum_{i_{J}=1}^{c_{J}}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\frac{\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}{\sum_{i_{J}=1}^{c_{J}}\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}\\
&\notag\quad=\sum_{i_{J}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{J}-1}}{(i_{J}-1)!}\\
&\notag\quad=\text{e}^{-\frac{1}{J}}\frac{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}+1}},
\end{align}
and along the same arguments, for the summation in $i_{J}=1,\ldots,c_{J}$ of \eqref{den_rewrite}, we have
\begin{align}\label{first_step_den}
&\lim_{c_{J}\rightarrow+\infty}\sum_{i_{J}=1}^{c_{J}}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\frac{\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}{\sum_{i_{J}=1}^{c_{J}}\left(\frac{1}{J}\right)^{i_{J}}\mathscr{C}(c_{J},i_{J};\alpha)}\\
&\notag\quad=\sum_{i_{J}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}\right)_{(i_{J})}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{J}-1}}{(i_{J}-1)!}\\
&\notag\quad=\text{e}^{-\frac{1}{J}}\frac{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}+1}}.
\end{align}
Now, we consider the summation in $i_{J-1}=1,\ldots,c_{J-1}$ of \eqref{num_rewrite}, with \eqref{first_step_num}, i.e.,
\begin{align*}
&\lim_{c_{J-1}\rightarrow+\infty}\sum_{i_{J-1}=1}^{c_{J-1}}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(i_{J-1})}\text{e}^{-\frac{1}{J}}\frac{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}+1}}\\
&\quad\quad\times\frac{\left(\frac{1}{J}\right)^{i_{J-1}}\mathscr{C}(c_{J-1},i_{J-1};\alpha)}{\sum_{i_{J-1}=1}^{c_{J-1}}\left(\frac{1}{J}\right)^{i_{J-1}}\mathscr{C}(c_{J-1},i_{J-1};\alpha)}\\
&\quad=\sum_{i_{J-1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(i_{J-1})}\text{e}^{-\frac{1}{J}}\frac{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-1}+1}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{J-1}-1}}{(i_{J-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{2}{J}}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}\sum_{i_{J-1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(i_{J-1}+1)}\frac{\left(\frac{1}{J-1}\right)^{i_{J-1}-1}}{(i_{J-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{2}{J}}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(2)}}{\left(\frac{J-2}{J-1}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}\\
&\quad=\text{e}^{-\frac{2}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(2)}}{\left(1-\frac{2}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}.
\end{align*}
Similarly, consider the summation in $i_{J-1}=1,\ldots,c_{J-1}$ of \eqref{den_rewrite}, with \eqref{first_step_den}, i.e., 
\begin{align*}
&\lim_{c_{J-1}\rightarrow+\infty}\sum_{i_{J-1}=1}^{c_{J-1}}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(i_{J-1})}\text{e}^{-\frac{1}{J}}\frac{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}+1}}\\
&\quad\quad\times\frac{\left(\frac{1}{J}\right)^{i_{J-1}}\mathscr{C}(c_{J-1},i_{J-1};\alpha)}{\sum_{i_{J-1}=1}^{c_{J-1}}\left(\frac{1}{J}\right)^{i_{J-1}}\mathscr{C}(c_{J-1},i_{J-1};\alpha)}\\
&\quad=\sum_{i_{J-1}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(i_{J-1})}\text{e}^{-\frac{1}{J}}\frac{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-1}+1}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{J-1}-1}}{(i_{J-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{2}{J}}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}\sum_{i_{J-1}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(i_{J-1}+1)}\frac{\left(\frac{1}{J-1}\right)^{i_{J-1}-1}}{(i_{J-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{2}{J}}}{\left(1-\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(2)}}{\left(\frac{J-2}{J-1}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}\\
&\quad=\text{e}^{-\frac{2}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}\right)_{(2)}}{\left(1-\frac{2}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{J-2}+2}}.
\end{align*}
By proceeding recursively, for the summation in $i_{j+1}=1,\ldots,c_{j+1}$ of \eqref{num_rewrite} is such that 
\begin{align}\label{general_nun}
&\lim_{c_{j+1}\rightarrow+\infty}\sum_{i_{j+1}=1}^{c_{j+1}}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(i_{j+1})}\text{e}^{-\frac{J-j-1}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j+1}\right)_{(J-j-1)}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j+1}+J-j-1}}\\
&\notag\quad\quad\times\frac{\left(\frac{1}{J}\right)^{i_{j+1}}\mathscr{C}(c_{j+1},i_{j+1};\alpha)}{\sum_{i_{j+1}=1}^{c_{j+1}}\left(\frac{1}{J}\right)^{i_{j+1}}\mathscr{C}(c_{j+1},i_{j+1};\alpha)}\\
&\notag\quad=\sum_{c_{j+1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(i_{j+1})}\text{e}^{-\frac{J-j-1}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j+1}\right)_{(J-j-1)}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j+1}+J-j-1}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{j+1}-1}}{(i_{j+1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-j}{J}}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\sum_{i_{j+1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(i_{j+1}+J-j-1)}\frac{\left(\frac{1}{j+1}\right)^{i_{j+1}-1}}{(i_{j+1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-j}{J}}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(\frac{j}{j+1}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\\
&\notag\quad=\text{e}^{-\frac{J-j}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+J-j}},
\end{align}
and, along the same lines, the summation in $i_{j+1}=1,\ldots,c_{j+1}$ of \eqref{den_rewrite} is such that 
\begin{align}\label{general_den}
&\lim_{c_{j+1}\rightarrow+\infty}\sum_{i_{j+1}=1}^{c_{j+1}}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(i_{j+1})}\text{e}^{-\frac{J-j-1}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j+1}\right)_{(J-j-1)}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j+1}+J-j-1}}\\
&\notag\quad\quad\times\frac{\left(\frac{1}{J}\right)^{i_{j+1}}\mathscr{C}(c_{j+1},i_{j+1};\alpha)}{\sum_{i_{j+1}=1}^{c_{j+1}}\left(\frac{1}{J}\right)^{i_{j+1}}\mathscr{C}(c_{j+1},i_{j+1};\alpha)}\\
&\notag\quad=\sum_{c_{j+1}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(i_{j+1})}\text{e}^{-\frac{J-j-1}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j+1}\right)_{(J-j-1)}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j+1}+J-j-1}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{j+1}-1}}{(i_{j+1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-j}{J}}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\sum_{i_{j+1}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(i_{j+1}+J-j-1)}\frac{\left(\frac{1}{j+1}\right)^{i_{j+1}-1}}{(i_{j+1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-j}{J}}}{\left(1-\frac{J-j-1}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(\frac{j}{j+1}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\\
&\notag\quad=\text{e}^{-\frac{J-j}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\\
\end{align}
Now, we consider the summation in $i_{j-1}=1,\ldots,c_{j-1}$ of \eqref{num_rewrite}, with \eqref{general_nun}, i.e.,
\begin{align*}
&\lim_{c_{j-1}\rightarrow+\infty}\sum_{i_{j-1}=1}^{c_{j-1}}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j-1})}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\\
&\quad\quad\times\text{e}^{-\frac{J-j}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\frac{\left(\frac{1}{J}\right)^{i_{j-1}}\mathscr{C}(c_{j-1},i_{j-1};\alpha)}{\sum_{i_{j-1}=1}^{c_{j-1}}\left(\frac{1}{J}\right)^{i_{j-1}}\mathscr{C}(c_{j-1},i_{j-1};\alpha)}\\
&\quad=\sum_{i_{j-1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j-1})}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\\
&\quad\quad\times\text{e}^{-\frac{J-j}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{j-1}-1}}{(i_{j-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{J-j+1}{J}}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}\sum_{i_{j-1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j-1}+i_{j}+J-j)}\frac{\left(\frac{1}{j}\right)^{i_{j-1}-1}}{(i_{j-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{J-j+1}{J}}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j}+J-j+1)}}{\left(\frac{j-1}{j}\right)^{\frac{\gamma+\alpha}{\gamma}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}\\
&\quad=\text{e}^{-\frac{J-j+1}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j}+J-j+1)}}{\left(\frac{j-1}{J}\right)^{\frac{\gamma+\alpha}{\gamma}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}.
\end{align*}
Similarly, consider the summation in $i_{j-1}=1,\ldots,c_{j-1}$ of \eqref{den_rewrite}, with \eqref{general_den}, i.e., 
\begin{align*}
&\lim_{c_{j-1}\rightarrow+\infty}\sum_{i_{j-1}=1}^{c_{j-1}}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j-1})}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\\
&\quad\quad\times\text{e}^{-\frac{J-j}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\frac{\left(\frac{1}{J}\right)^{i_{j-1}}\mathscr{C}(c_{j-1},i_{j-1};\alpha)}{\sum_{i_{j-1}=1}^{c_{j-1}}\left(\frac{1}{J}\right)^{i_{j-1}}\mathscr{C}(c_{j-1},i_{j-1};\alpha)}\\
&\quad=\sum_{i_{j-1}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j-1})}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-1}\right)_{(i_{j})}\\
&\quad\quad\times\text{e}^{-\frac{J-j}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}\right)_{(J-j)}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j}+J-j}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{j-1}-1}}{(i_{j-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{J-j+1}{J}}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}\sum_{i_{j-1}\geq1}\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j-1}+i_{j}+J-j)}\frac{\left(\frac{1}{j}\right)^{i_{j-1}-1}}{(i_{j-1}-1)!}\\
&\quad=\frac{\text{e}^{-\frac{J-j+1}{J}}}{\left(1-\frac{J-j}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j}+J-j+1)}}{\left(\frac{j-1}{j}\right)^{\frac{\gamma}{\gamma}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}\\
&\quad=\text{e}^{-\frac{J-j+1}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}+\cdots+i_{j-2}\right)_{(i_{j}+J-j+1)}}{\left(\frac{j-1}{J}\right)^{\frac{\gamma}{\gamma}+i_{j}+i_{1}+\cdots+i_{j-2}+J-j+1}}.
\end{align*}
By proceeding recursively, we arrive to the summation in $i_{1}=1,\ldots,c_{1}$ of \eqref{num_rewrite}, i.e. 
\begin{align}\label{final_num}
&\lim_{c_{1}\rightarrow+\infty}\sum_{i_{1}=1}^{c_{1}}\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{1})}\text{e}^{-\frac{J-2}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}\right)_{(i_{j}+J-2)}}{\left(\frac{2}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+i_{1}+J-2}}\frac{\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}{\sum_{i_{1}=1}^{c_{1}}\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}\\
&\notag\quad=\sum_{i_{1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{1})}\text{e}^{-\frac{J-2}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}+i_{1}\right)_{(i_{j}+J-2)}}{\left(\frac{2}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+i_{1}+J-2}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{1}-1}}{(i_{1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-1}{J}}}{\left(\frac{2}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+J-1}}\sum_{i_{1}\geq1}\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{1}+i_{j}+J-2)}\frac{\left(\frac{1}{2}\right)^{i_{1}-1}}{(i_{1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-1}{J}}}{\left(\frac{2}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+J-1}}\frac{\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{j}+J-1)}}{\left(\frac{1}{2}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+J-1}}\\
&\notag\quad=\text{e}^{-\frac{J-1}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{j}+J-1)}}{\left(\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+J-1}}
\end{align}
and, along the same lines, we arrive to the summation in $i_{1}=1,\ldots,c_{1}$ of \eqref{den_rewrite}, i.e. 
\begin{align}\label{final_den}
&\lim_{c_{1}\rightarrow+\infty}\sum_{i_{1}=1}^{c_{1}}\left(\frac{\gamma}{\alpha}\right)_{(i_{1})}\text{e}^{-\frac{J-2}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}\right)_{(i_{j}+J-2)}}{\left(\frac{2}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+i_{1}+J-2}}\frac{\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}{\sum_{i_{1}=1}^{c_{1}}\left(\frac{1}{J}\right)^{i_{1}}\mathscr{C}(c_{1},i_{1};\alpha)}\\
&\notag\quad=\sum_{i_{1}\geq1}\left(\frac{\gamma}{\alpha}\right)_{(i_{1})}\text{e}^{-\frac{J-2}{J}}\frac{\left(\frac{\gamma}{\alpha}+i_{1}\right)_{(i_{j}+J-2)}}{\left(\frac{2}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+i_{1}+J-2}}\text{e}^{-\frac{1}{J}}\frac{\left(\frac{1}{J}\right)^{i_{1}-1}}{(i_{1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-1}{J}}}{\left(\frac{2}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+J-1}}\sum_{i_{1}\geq1}\left(\frac{\gamma}{\alpha}\right)_{(i_{1}+i_{j}+J-2)}\frac{\left(\frac{1}{2}\right)^{i_{1}-1}}{(i_{1}-1)!}\\
&\notag\quad=\frac{\text{e}^{-\frac{J-1}{J}}}{\left(\frac{2}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+J-1}}\frac{\left(\frac{\gamma}{\alpha}\right)_{(i_{j}+J-1)}}{\left(\frac{1}{2}\right)^{\frac{\gamma}{\alpha}+i_{j}+J-1}}\\
&\notag\quad=\text{e}^{-\frac{J-1}{J}}\frac{\left(\frac{\gamma}{\alpha}\right)_{(i_{j}+J-1)}}{\left(\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+J-1}}.
\end{align}
We $\mathbf{c}_{-j} = (c_1, \ldots, c_{j-1}, c_{j+1}, \ldots, c_J)$ and indicate by 
$\mathbf{c}_{-j} \rightarrow +\infty$  the limit situation when $c_k \rightarrow +\infty$ simultaneously for $k \neq j$ while $c_j$ is fixed. Then,  according to the asymptotics obtained in \eqref{final_num} and \eqref{final_den}, from \eqref{main_num_den} we can write
\begin{align*}
&\lim_{\mathbf{c}_{-j} \rightarrow +\infty} \frac{\gamma}{J}{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma+\alpha}{\alpha}\right)}{\Gamma\left(\frac{\gamma}{\alpha}\right)}\frac{N_{\gamma,\alpha,J}(l;c_{1},\ldots,c_{J})}{D_{\gamma,\alpha,J}(c_{1},\ldots,c_{J})}\\
&\quad= \frac{\gamma}{J}{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma+\alpha}{\alpha}\right)}{\Gamma\left(\frac{\gamma}{\alpha}\right)}\frac{\sum_{i_{j}=1}^{c_{j}-l}\left(\frac{1}{J}\right)^{i_{j}}\mathscr{C}(c_{j}-l,i_{j};\alpha)\text{e}^{-\frac{J-1}{J}}\frac{\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{j}+J-1)}}{\left(\frac{1}{J}\right)^{\frac{\gamma+\alpha}{\alpha}+i_{j}+J-1}}}{\sum_{i_{j}=1}^{c_{j}+1}\left(\frac{1}{J}\right)^{i_{j}}\mathscr{C}(c_{j}+1,i_{j};\alpha)\text{e}^{-\frac{J-1}{J}}\frac{\left(\frac{\gamma}{\alpha}\right)_{(i_{j}+J-1)}}{\left(\frac{1}{J}\right)^{\frac{\gamma}{\alpha}+i_{j}+J-1}}}\\
&\quad=\gamma{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma+\alpha}{\alpha}\right)}{\Gamma\left(\frac{\gamma}{\alpha}\right)}\frac{\sum_{i_{j}=1}^{c_{j}-l}\mathscr{C}(c_{j}-l,i_{j};\alpha)\left(\frac{\gamma+\alpha}{\alpha}\right)_{(i_{j}+J-1)}}{\sum_{i_{j}=1}^{c_{j}+1}\mathscr{C}(c_{j}+1,i_{j};\alpha)\left(\frac{\gamma}{\alpha}\right)_{(i_{j}+J-1)}}\\
&\quad=\gamma{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma+\alpha}{\alpha}+J-1\right)}{\Gamma\left(\frac{\gamma}{\alpha}+J-1\right)}\frac{\sum_{i_{j}=1}^{c_{j}-l}\mathscr{C}(c_{j}-l,i_{j};\alpha)\left(\frac{\gamma+\alpha}{\alpha}+J-1\right)_{(i_{j})}}{\sum_{i_{j}=1}^{c_{j}+1}\mathscr{C}(c_{j}+1,i_{j};\alpha)\left(\frac{\gamma}{\alpha}+J-1\right)_{(i_{j})}}\\
&\quad=\gamma{c_{j}\choose l}(1-\alpha)_{(l)}\frac{\Gamma\left(\frac{\gamma+\alpha}{\alpha}+J-1\right)}{\Gamma\left(\frac{\gamma}{\alpha}+J-1\right)}\frac{(\gamma+\alpha+J\alpha-\alpha)_{(c_{j}-l)}}{(\gamma+J\alpha-\alpha)_{(c_{j}+1)}},
\end{align*}
where the summations over $i_{j}$ follows from Equation 2.49 in \citet{Cha(05)}. See also \citet[Theorem 2.14]{Cha(05)}. This completes the proof.

\subsection{Proof of \Cref{prop:rec_trait_all}}\label{app:prof_trait_all}
Without loss of generality, assume that $A_{i,k} \in \mathbb N_0 := \{0, 1, \ldots\}$.
From the Poisson process representation of CRMs and the marking theorem \citep{Kin(93)}, $\tilde N := \{(\omega_k, J_k, (A_{i, k})_{i=1}^{n+1})\}_{k \geq 1}$ is a Poisson process on $\mathbb S \times \R_+ \times \N_0^{n+1}$.
Consider now thinned processes $\tilde N_j$, $j=1, \ldots, J$ obtained from $\tilde N$ by taking only those points for which $\omega_k \in D_j$. By the coloring theorem \citep{Kin(93)}, the $\tilde N_j$'s are independent Poisson processes.

Now observe that the random variables $f_{Y_{n+1,\ell}}$, $X_{n+1}(Y_{n+1,\ell})$ depend only on $\tilde N_{h(Y_{n+1,\ell})}$. Similarly, each $(C_j, B{j})$ depends only on $\tilde N_j$ and the independence is preserved also when marginalizing the $\tilde N_j$'s.
Hence, we can conclude that $(f_{Y_{n+1,\ell}}$, $X_{n+1}(Y_{\ell}), C_{h(Y_{n+1,\ell})}, B_{h(Y_{\ell})})$ are 
independent of all the other $C_k$'s ($k \neq h(Y_{\ell})$) and all of $B_{k}$ ($k \neq h(Y_{\ell})$), which yields the proof.


\subsection{Proof of Theorem \ref{teo:trait_general}}\label{app:proof_trait_general}

We evaluate \eqref{eq:trait_rec_single} by computing the numerator and denominator separately.
In the following, we will denote by $D_\omega$ the preimage of $h(\omega)$, for $\omega \in \mathbb S$.

\paragraph{Denominator}

From the Poisson process representation of CRMs and the marking theorem, we have that $N := \left\{(\omega_k, S_k, \{A_{i, k}\}_{i=1}^{n+1} \right\}_{k \geq 1}$ is a Poisson process on $\mathbb S \times \R_+ \times \mathbb N^{n+1}$ with intensity
\[
    \theta G_0(\dd w) \left[\prod_{i=1}^{n+1} G_A(\dd a_i \mid s)\right] \rho(s) \dd s
\]
Then by the colouring theorem \citep[see, e.g., Chapter 5 in][]{Kin(93)}, we have that selecting from $N$ only those points for which $\omega_k \in D_j$ (i.e., the points whose features' hashes coincide with the hash of $Y_{n+1, \ell}$), leads to a point process $N^\prime := \left\{(\omega^\prime_k, S^\prime_k, \{A^\prime_{i, k}\}_{i=1}^{n+1} \right\}_{k \geq 1}$ which is Poisson on $D_j \times \R_+ \times \mathbb N^{n+1}$ with intensity
\begin{equation}\label{eq:int_rescaled}
    \frac{\theta}{J} \bar G_j(\dd w) \left[\prod_{i=1}^{n+1} G_A(\dd a_i \mid s)\right] \rho(s) \dd s
\end{equation}
where $\bar G_j(\dd w)$ is $G_0$ truncated on $D_{\omega^*}$ and re-normalized.
Then, 
\begin{align*}
    & \prob\left[ C_{j} = c, B_j = b \right] = \\
    & \qquad = \prob\left[ \sum_{k \geq 1} \indicator[\omega_k \in D_{\omega^*}] \sum_{i=1}^n A_{i, k} = c, \sum_{k \geq 1} \indicator[\omega_k \in D_{\omega^*}]  A_{i, n+1} = b \right] \\
    & \qquad = \prob\left[ \sum_{k \geq 1} \sum_{i=1}^m A^\prime_{i, k} = c, \sum_{k \geq 1}  A^\prime_{n+1, k} = b\right]
\end{align*}
Observe that $\omega_k$ is not involved in the last probability, so we can marginalize with respect to it and consider the point process $\{J^\prime_k, (A^\prime_{i,k})_{i=1}^{n+1}\}_{k \geq 1}$ on $\R_+ \times \mathbb N_0^{n+1}$ with intensity $\theta J^{-1} \left[\prod_{i=1}^{n+1} G_A(\dd a_i \mid s)\right] \rho(s) \dd s$.

\paragraph{Numerator}

We start by considering $Y_{n+1, \ell} = \omega^*$ fixed.
Let $B_{\omega^*}$ be a ball of radius $\varepsilon$ centered in $\omega^*$.
We compute
\[
  \prob\left[\sum_{i=1}^n X_i(B_{\omega^*}) = l, X_{n+1}(B_{\omega^*}) = a, C_j = c, B_j = b, h(\omega^*)=j \right]
\]
which converges to the numerator in \eqref{eq:trait_rec_single} as $\varepsilon \rightarrow 0$.
By the definition of $B_j$ and $C_j$ we have
\begin{align}
  &\prob\left[\sum_{i=1}^n X_i(B_{\omega^*}) = l, X_{n+1}(B_{\omega^*}) = a, C_j = c, B_j = b \right] \label{eq:trait_num_temp} \\
  & \qquad = \prob\Big[\sum_{i=1}^n X_i(B_{\omega^*}) = l, X_{n+1}(B_{\omega^*}) = a, \nonumber \\
  & \hspace{1cm}
  \sum_{i=1}^n X_i(D_{\omega^*} \setminus B_{\omega^*}) = (c-l), X_{n+1}(D_{\omega^*} \setminus B_{\omega^*}) = (b-a)\Big]\nonumber 
\end{align}
We further have that $(X_i(B_{\omega^*}))_{i \geq 1}$ is a collection of random variables independent of $(X_i(D_{\omega^*} \setminus B_{\omega^*}) )_{i \geq 1}$. Therefore, we can consider the first two events and the last two events separately.

For the last two, arguing as in the denominator case, we have that as $\varepsilon \rightarrow 0$
\begin{multline*}
\prob\Big[
  \sum_{i=1}^n X_i(D_{\omega^*} \setminus B_{\omega^*}) = (c-l), X_{n+1}(D_{\omega^*} \setminus B_{\omega^*}) = (b-a)\Big] \\
\longrightarrow \prob\left[ \sum_{k \geq 1} \sum_{i=1}^n A^\prime_{i, k} = c - l, \sum_{k \geq 1}  A^\prime_{n+1, k} = b - a\right]
\end{multline*}
where the $A^\prime_{ik}$'s come from the Poisson process $\{J^\prime_k, (A^\prime_{i,k})_{i=1}^{n+1}\}_{k \geq 1}$ on $\R_+ \times \mathbb N_0^{n+1}$ with intensity $\theta J^{-1} \left[\prod_{i=1}^{n+1} G_A(\dd a_i \mid s)\right] \rho(s) \dd s$.

For the first two events instead, an application of Campbell's theorem yields
\begin{align*}
  & \prob\Big[\sum_{i=1}^n X_i(B_{\omega^*}) = l, X_{n+1}(B_{\omega^*}) = a \Big] \\
  & \qquad = \E\left[\int I\left[\left\{\sum_{i=1}^n a_i\right\} = l\right] I[a_{n+1} = a] I[\omega \in B_{\omega^*}] N(\dd s \, \dd \bm a \, \dd \omega) \right] \\
  & \qquad = \theta G_0(B_{\omega^*}) \int \prob \left[ \sum_{i=1}^n \tilde A_{i} = l, \tilde A_{n+1} = a \mid s \right] \rho(s) \dd s
\end{align*}
The proof concludes by letting $\varepsilon \rightarrow 0$ and noting that $\int_{D_j} G_0(\dd \omega^*) = J^{-1}$. 


\subsection{Proof of \Cref{prop:poisson_traits}}\label{app:proof_poisson}

The closeness of the Poisson distribution under convolution entails that $\sum_{i=1}^n \tilde A_{i} \mid s \sim \mbox{Poi}(nrs)$.
Similarly, $\sum_{k \geq 1}\sum_{i=1}^n A^\prime_{i, k} \sim \mbox{Poi}(nrT^\prime)$ where $T^\prime = \sum_{k \geq 1} J^\prime_k$.
Then
\begin{align*}
  & \int_{\R_+} \prob \left[ \sum_{i=1}^n \tilde A_{i} = l, \tilde A_{n+1} = a \mid s \right] \rho(s) \dd s  \\
  & \qquad\qquad= \int_{\R_+} \frac{1}{l!} \frac{1}{a!} (nr s)^l e^{-nrs} (rs)^a e^{-rs} \rho(s) \dd s \\
  & \qquad\qquad = \frac{n^{l} r^{l + a}}{l! a!} \kappa(l+a, (n+1) r).
\end{align*}
Moreover
\begin{align*}
  & \prob\left[ \sum_{k \geq 1} \sum_{i=1}^n A^\prime_{i, k} = c, \sum_{k \geq 1}  A^\prime_{n+1, k} = b \right] \\
  & \qquad \qquad= \frac{n^c r^{c+b}}{c! b!}  \E_{T^\prime} \left[e^{-(n+1)rT^\prime} (T^\prime)^{c+b} \right]\\
  & \qquad \qquad=\frac{n^c r^{c+b}}{c! b!}  (-1)^{c+b} \frac{\dd^{c+b}}{\dd ((n+1)r)^{c+b}} \E_{\mu^\prime}[e^{-(n+1)rT^\prime}] \\
  & \qquad \qquad = \frac{n^c r^{c+b}}{c! b!} (-1)^{c+b} \frac{\dd^{c+b}}{\dd z^{c+b}} \exp\left(- \theta/J \psi(z) \right)\big|_{(n+1)r}
\end{align*}
In a similar fashion,
\begin{equation}\label{eq:proof_poi_num}
  \begin{aligned}
  & \prob\left[ \sum_{k \geq 1} \sum_{i=1}^n A^\prime_{i, k} = c - l, \sum_{k \geq 1}  A^\prime_{n+1, k} = b - a\right] \\
  & \qquad \qquad \frac{n^{c-l} r^{c - l+b - a}}{(c-l)! (b-a)!} (-1)^{c-l+b-a} \frac{\dd^{c-l+b-a}}{\dd z^{c-l+b-a}} \exp\left(- \theta/J \psi(z) \right)\big|_{(n+1)r}
  \end{aligned}
\end{equation}
Combining these expressions together yields the proof.

\subsection{Proof of \Cref{prop:bern_traits}} 

Observe that $(J^\prime_k)_{k \geq 1}$ in \Cref{teo:trait_general} is a Poisson process on $\R_+$ with intensity $\theta/J \rho(s) \dd s$.
Consider now the numerator in \eqref{eq:post_trait}.
Let $S_{n} := \sum_{k \geq 1} \sum_{i=1}^n A^\prime_{i,k}$ and $Z:= \sum_{k \geq 1} A^{\prime}_{n+1, k}$. 
We have that, conditionally to $(J^\prime_k)_{k \geq 1}$, $S_{n,k}$ is Poisson-binomial distributed with paramterers $J^\prime_1, \ldots, J^\prime_1, J^\prime_2, \ldots, J^\prime_2, \ldots$, where each $J^\prime_k$ appears exactly $n$ times. Similarly, $Z \mid (J^\prime_k)_{k \geq 1}$ is Poisson-binomial with parameters $J_1, J_2, \ldots$.
Let $\tilde S_n \mid (J^\prime_k)_{k \geq 1} \sim \mbox{Poi}(n T^\prime)$ and $\tilde Z \mid (J^\prime_k)_{k \geq 1} \sim \mbox{Poi}(T^\prime)$ where $T^\prime = \sum_{k} J^\prime_k$.
Then, by \cite{LeCam(60)}, conditionally to $(J^\prime_k)_{k \geq 1}$, $S_n \approx \tilde S_n$, $Z \approx \tilde Z$. Hence
\[
  \prob[S_n = c-l, Z = b- 1] \approx \prob[\tilde S_n = c-l, \tilde Z = b- 1]
\]
Then, from \eqref{eq:proof_poi_num} we get
\begin{multline*}
  \prob[\tilde S_n = c-l, \tilde Z = b - 1] = \\ \frac{n^{c-l} r^{c - l+ b - 1}}{(c-l)! (b-1)!} (-1)^{c-l+b-1} \frac{\dd^{c-l+b-1}}{\dd z^{c-l+b-1}} \exp\left(- \theta/J \psi(z) \right)\big|_{(n+1)r}.
\end{multline*}

Moreover,
\begin{align*}
  & \prob \left[ \sum_{i=1}^n \tilde A_{i} = l, \tilde A_{n+1} = 1 \mid s \right] \\
  & \qquad = \binom{n}{l} \prob[\tilde A_{1} = 1, \ldots, \tilde A_{l} = 1, \tilde A_{n+1} = 1, \tilde A_{l+1} = 0, \ldots, \tilde A_{n}=0] \\
  & \qquad = \binom{n}{l} s^{n+1} (1 - s)^{m - l} = \binom{n}{l} \left(s^{l+1} - s^{n+1} \right)
\end{align*}
Integrating the resulting expression with respect to $\rho(s) \dd s$ and ignoring multiplicative terms yields \eqref{eq:post_trait}.


To prove the error bound, for ease of notation, let $f_Y \equiv f_{Y_{n+1, \ell}}$, $X \equiv X_{n+1}(Y_{n+1, \ell})$ and let $M^\prime= (J_k)_{k\geq 1}$.
Furthermore, observe that $(f_Y, X)$ is independent of $(S_n, Z)$ and $(\tilde S_n, \tilde Z)$.
Then
\begin{align*}
  & TV\left(\left[f_Y, X, S_n, Z\right], \left[f_Y, X, \tilde S_n, \tilde Z\right]\right) \\
  & \quad = \sum_{l, x, s, z} \Big| \prob\left[f_Y = l, X = x, S_n = s, Z = z\right] - \prob\left[f_Y = l, X = x, \tilde S_n = s, \tilde Z = z\right] \Big| \\
  &\quad = \sum_{l, x, s, z} \prob\left[f_Y = l, X= x \right] \Big| \prob\left[S_n = s, Z = z\right] -  \prob\left[\tilde S_n = s, \tilde  Z = z\right]\Big| \\
  & \quad \leq  \sum_{s, z}  \Big| \E \prob\left[S_n = s, Z = z \mid M^\prime \right] -  \E \prob\left[\tilde S_n = s, \tilde  Z = z \mid M^\prime \right]\Big| \\
  & \quad \leq \E \sum_{s, z} \Big| \prob\left[S_n = s, Z = z \mid M^\prime \right] -  \prob\left[\tilde S_n = s, \tilde  Z = z \mid M^\prime \right]\Big|
\end{align*}
where the last inequality follows from Jensen's inequality and an application of the Fubini theorem.
Then by the conditional independence between $S_n$ and $Z$, and $\tilde S_n$ and $\tilde Z$ we get
\begin{align*}
  &  \E \sum_{s, z} \Big| \prob\left[S_n = s, Z = z \mid M^\prime \right] -  \prob\left[\tilde S_n = s, \tilde  Z = z \mid M^\prime \right]\Big| \\
& \qquad =  \E\left[TV(S_n \mid M^\prime, \tilde S_n \mid M^\prime) + TV(Z \mid M^\prime, \tilde Z \mid M^\prime) \right]
\end{align*}
To upper bound the error, we start from Eq. (5.5) in \cite{Ste(94)} so that 
\begin{align*}
  TV(S_n \mid M^\prime, \tilde S_n \mid M^\prime) &\leq \frac{(1 - e^{-nT^\prime})}{nT^\prime} n \sum_{k \geq 1} J_k^{\prime 2} \\
  TV(Z \mid M^\prime, \tilde Z \mid M^\prime) &\leq \frac{(1 - e^{-T^\prime})}{T^\prime}  \sum_{k \geq 1} J_k^{\prime 2}
\end{align*}
so that
\begin{align*}
 & \E\left[TV(S_n \mid M^\prime, \tilde S_n \mid M^\prime) + TV(Z \mid M^\prime, \tilde Z \mid M^\prime) \right] \\
& \qquad \leq \E \left[\frac{2}{T^\prime}  \sum_{k \geq 1} J_k^{\prime 2} \right] = 2 \int_{\R_+} \E \left[e^{-u T^\prime} \sum_{k \geq 1} J_k^{\prime 2}\right] \dd u \\
& \qquad = 2 \int_{\R_+} \E \left[ \int_{\R_+} e^{-u \int_{\R_+} z M^\prime(\dd z)} s^2 M^\prime(\dd z)\right] \dd u
\end{align*}
where the last equality follows by identifying $M^\prime$ with the random counting measure $\sum_{k \geq 1} \delta_{J^\prime_k}$.
Then, an application of the Mecke equation \citep[see, e.g., Theorem 4.1 in][]{Las(18)} yields
\begin{align*}
  & 2 \int_{\R_+} \E \left[ \int_{\R_+} e^{-u \int_{\R_+} z M^\prime(\dd z)} s^2 M^\prime(\dd z)\right] \dd u =  \\
  & \qquad  = \frac{2\theta}{J} \int_{\R_+} \int_{\R_+} \E[e^{- u T^\prime}] e^{-us} s^2 \rho(s) \dd s.
\end{align*}
The proof follows by noticing that, by the L\'evy-Kintchine representation, $\E[e^{- u T^\prime}] = e^{-\psi(u)}$ and from the definition of $\kappa(u, n)$.






\section{Details about the Dirichlet Process case}

\subsection{Proof of Equation \eqref{post_dp} as a special case}\label{app:dp_cor}

The DP is obtained by normalizing a Gamma process, whose L\'evy intensity is $\theta s^{-1} e^{-s} \dd s G_0(\dd x)$.
Hence,
\begin{align*}
    \psi(u) &= \int_{\R_+}(1 - e ^{-us}) \rho(s) \dd s = \int_{\R_+}(1 - e ^{-us}) s^{-1} e^{-s}\dd s \\
    &= \int_{\R_+} \int_{\R_+} (1 - e ^{-us}) e^{-ts} e^{-s} \dd t \, \dd s \\
    &= \int_{\R_+} \frac{u}{(t+1)(t+u+1)} \dd t = \log(1 + u) 
\end{align*}
where the third equality follows from the identity $s^{-1} = \int_{\R_+} e^{-ts} \dd t$ and the fourth one by an application of Fubini's theorem.
Then it follows
\[
  (-1)^n \frac{\dd^u}{\dd u^n} e^{-\theta \psi(u)} = \frac{\Gamma(\theta + n)}{\Gamma(\theta)} (1 + u)^{- \theta -n},  
\]
Moreover, $k(l, u) = \Gamma(l) / (u+1)^{l}$.
Hence, the integral at the numerator of \eqref{post_pk} can be evaluated as
\begin{align*}
   &\frac{\Gamma(\theta/J + c_j - l)}{\Gamma(\theta/J)} \prod_{k \neq j} \frac{\Gamma(\theta/J + c_k)}{\Gamma(\theta/J)} \Gamma(l+1)  \int_{\R_+} u^{n} + (1 + u)^{-\theta - n - 1} \\
   & \qquad = \frac{\Gamma(\theta/J + c_j - l)}{\Gamma(\theta/J)} \prod_{k \neq j} \frac{\Gamma(\theta/J + c_k)}{\Gamma(\theta/J)} \Gamma(l+1) \frac{\Gamma(n+1)}{\Gamma(\theta + n + 1)}
\end{align*}
Similarly, the integral at the denominator of \eqref{post_pk}  equals
\begin{align*}
   &\frac{\Gamma(\theta/J + c_j + 1)}{\Gamma(\theta/J)} \prod_{k \neq j} \frac{\Gamma(\theta/J + c_k)}{\Gamma(\theta/J)}  \int_{\R_+} u^{n} + (1 + u)^{-\theta - n - 1} \\
   & \qquad = \frac{\Gamma(\theta/J + c_j + 1)}{\Gamma(\theta/J)} \prod_{k \neq j} \frac{\Gamma(\theta/J + c_k)}{\Gamma(\theta/J)} \frac{\Gamma(n+1)}{\Gamma(\theta + n + 1)}
\end{align*}
Combining these expressions together, we have
\[
    \prob\left[ f_{X_{n+1}} = l \mid \bm C = \bm c, h(X_{n+1}) = j\right] = \frac{\theta}{J} \binom{c_j}{l} l! \frac{\Gamma(\theta/J + c_j - l)}{\Gamma(\theta / J + c_j + 1)}.
\]

\subsection{Proof of Equation \eqref{post_dp} from the finite dimensional laws}\label{app:dp_teo}

In this proof, we exploit only the original characterization of the DP in terms of its finite-dimensional distributions. That is, given $\theta >0$ and $G_0$ a probability measure on $(\Sd)$, $P$ is a Dirichlet process with mean measure $\theta G_0$ if and only if, for any $n > 0$ and any $n$ measurable partition $A_1, \ldots, A_n$ of $\X$, 
\begin{equation}\label{eq:dp_def}
    (P(A_1), \ldots, P(A_n)) \sim \mathrm{Dir}_n (\theta G_0(A_1), \ldots, \theta G_0(A_n))
\end{equation}
where $\mathrm{Dir}_n$ denotes the $n-1$ dimensional Dirichlet distribution.

We argue as in \Cref{app:proof_main}. 
To compute the denominator in \eqref{eq:sketch_post_fract}, from \eqref{eq:denom_probs}, \eqref{eq:dp_def}
\begin{align*}
   & \prob[\bm C = \bm c, h(X_{n+1}) = j] \nonumber \\
   & \qquad = \binom{n}{c_1, \ldots, c_J}  \frac{\Gamma(\theta)}{\Gamma(\theta + n + 1)} \frac{\Gamma(\theta / J + c_j + 1)}{\Gamma(\theta / J)} \prod_{k \neq j} \frac{\Gamma(\theta / J + c_k)}{\Gamma(\theta / J)} \nonumber
\end{align*}
where we also exploited the e uniformity of the hash function which ensures that $G_0(D_\ell) = G_{0}(h^{-1}(\{\ell\})) = 1 / J$ for any $\ell = 1, \ldots, J$.

Similarly, to compute the numerator, consider \eqref{eq:num_probs}. Since $P$ is a Dirichlet process,
\begin{align*}
    & \E\left[P(B_{\omega^*})^{l+1} P(D_j \setminus B_{\omega^*})^{c_j-l} \prod_{k \neq j} P(D_k)^{c_k} \right] \\
    & \qquad = \frac{1}{l!}\binom{n}{c_1, \ldots, c_j - l, \ldots c_J} \frac{\Gamma(\theta)}{\Gamma(\theta + n + 1)} \frac{\Gamma(\alpha G_0(B_{\omega^*}) + l + 1)}{\Gamma(\theta G_0(B_{\omega^*}))} \\
    & \qquad \qquad \times \frac{\Gamma(\theta G_0(D_j \setminus B_{\omega^*}) + c_j - l)}{\Gamma(\theta G_0(D_j \setminus B_{\omega^*}))} \prod_{k \neq j} \frac{\Gamma(\theta / J + c_k)}{\Gamma(\theta / J)}
\end{align*}
We now let $\varepsilon \rightarrow 0$. First note that, of course
\[
    \frac{\Gamma(\theta G_0(D_j \setminus B_{\omega^*}) + c_j - l)}{\Gamma(\theta G_0(D_j \setminus B_{\omega^*}))} \rightarrow \frac{\Gamma(\theta G_0(D_j) + c_j - l)}{\Gamma(\theta G_0(D_j))} = \frac{\Gamma(\theta / J + c_j - l)}{\Gamma(\theta / J)} 
\]
To evaluate the limit of  $\Gamma(\theta G_0(B_{\omega^*}) + l + 1) / \Gamma(\theta G_0(B_{\omega^*}))$, we first unroll the numerator using the recurrence relation $\Gamma(z + 1) = z \Gamma(z)$ $l$ times so that
\[
    \frac{\Gamma(\theta G_0(B_{\omega^*}) + l + 1)}{\Gamma(\theta G_0(B_{\omega^*}))} = (\theta G_0(B_{\omega^*}) + l) \cdots (\theta G_0(B_{\omega^*})) = \theta \Gamma(l+1) G_0(B_{\omega^*}) + o(G_0(B_{\omega^*}))
\]
letting now $\varepsilon \rightarrow 0$, we can ignore higher order infinitesimals and get that
\[
    \frac{\Gamma(\theta G_0(B_{\omega^*}) + l + 1)}{\Gamma(\theta G_0(B_{\omega^*}))} \rightarrow \theta \Gamma(l+1) G_0(\dd \omega^*),
\]
which leads to
\begin{align*}
    & \prob\left[ \sum_{i=1}^n \indicator_{(X_{n+1})}(X_i) = l, \bm C = \bm c, h(X_{n+1}) = j\right] \\
    & \qquad = \int_{D_j} \prob\left[\sum_{i=1}^n \indicator_{X_{n+1}}(X_i) = l, \bm C = \bm c, X_{n+1} \in \dd \omega^* \right] \\
    & \qquad = \frac{1}{l!}\binom{n}{c_1, \ldots, c_j - l, \ldots c_J} \frac{\Gamma(\theta)}{\Gamma(\theta + n + 1)} \theta \Gamma(l+1) G_0(B_{\omega^*}) \\
    & \qquad \qquad \times \frac{\Gamma(\theta/J + c_j - l)}{\Gamma(\theta / J)} \prod_{k \neq j} \frac{\Gamma(\theta / J + c_k)}{\Gamma(\theta / J)} \int_{D_j} G_0(\dd \omega^*)
\end{align*}
So that integration with respect to $\dd \omega^*$ is now straightforward and $\int_{D_j} G_0(\dd \omega^*) = 1 / J$.

Combining numerator and denominator yields the proof.


\section{Details About the PYP Case}

\subsection{Proof of \eqref{post_pyp}}\label{app:post_pyp}

In the case of a PYP, we have $\beta = 0$, $\psi(u) = u^{\alpha}$ and 
\[
    \kappa(u, l) = \alpha u^{\alpha - l} \frac{\Gamma(l - \alpha)}{\Gamma(1 - \alpha)} = \alpha u^{\alpha - l} (1 - \alpha)_{(l-1)}
\]
where $(a)_{(b)}$ is the $b$-th rising factoral, i.e. $(a)_{(b)} = a (a+1) \cdots (a + b -1)$.
Using the Faa di Bruno formula, we further have \citep[see, e.g., Lemma 1 in][]{Cam(19)}
\begin{align*}
    (-1)^c \frac{\dd^c}{\dd u^c} e^{-u^{\alpha} / J} &= e^{-u^{\alpha} / J} \sum_{i=1}^c \left(\frac{1}{J}\right)^i \sum_{(*)} \frac{1}{i!} \binom{c}{k_1 \cdots k_i} \prod_{j=1}^i \kappa(u, k_j) \\
    &= e^{-u^\alpha /J} \sum_{i=1}^c \left(\frac{1}{J}\right)^i \frac{\alpha^i}{u^{c - \alpha i} i!} \sum_{(*) } \binom{c}{k_1 \cdots k_i} \prod_{j=1}^i (1 - \alpha)_{k_j - 1} \\
    &= e^{-u^\alpha /J} \sum_{i=1}^c \left(\frac{1}{J}\right)^i \frac{1}{u^{c - \alpha i}} \mathscr{C}(c, i; \alpha)
\end{align*}
where $\mathscr{C}(c, i; \alpha)$ is the generalized factorial coefficient and $(*)$ denotes the summation over positive integers $(k_1, \ldots, k_i)$ such that $\sum_{j=1}^i k_j = c$.

Recall the definition of the multi-index set $S(\bm c, j, q)$ as in the main text below \eqref{post_pyp}.
Then, the integral at the numerator of \eqref{post_pk} equals
\begin{align*}
    & \int_{\R_+} u^{b+\gamma} e^{-u^{\alpha} / J} \sum_{i_j = 1}^{c_j - l} J^{-i_j} \frac{\mathscr{C}(c_j - l, i_j; \alpha)}{u^{c_j - l - \alpha i_j}} \\
    & \hspace{2cm} \times \Bigg\{ \prod_{k \neq j}  e^{-u^{\alpha} / J} \sum_{i_k = 1}^{c_k} J^{-i_k} \frac{\mathscr{C}(c_k, i_k; \alpha)}{u^{c_k - \alpha i_k}} \Bigg\} \alpha u^{\alpha - l - 1} (1 - \alpha)_{(l)} \dd u\\
    & = \sum_{i_1 = 1}^{c_1} \cdots \sum_{i_j = 1}^{c_j - l} \cdots \sum_{i_J = 1}^{c_J} J^{- \sum_k i_k} \mathscr{C}(c_j - l, i_j; \alpha) \\
    & \hspace{2cm} \times \prod_{k \neq j} \mathscr{C}(c_k, i_k; \alpha)  \alpha (1- \alpha)_{(l)} \int_{\R_+} e^{-u^\alpha} u^{\gamma - 1 + \alpha \sum_k i_k + \alpha} \dd u\\
    & = \sum_{i_1 = 1}^{c_1} \cdots \sum_{i_j = 1}^{c_j - l} \cdots \sum_{i_J = 1}^{c_J} J^{- \sum_k i_k} \mathscr{C}(c_j - l, i_j; \alpha) \\ 
    & \hspace{2cm} \times \prod_{k \neq j} \mathscr{C}(c_k, i_k; \alpha) (1 - \alpha)_{(l)} \Gamma\left(\frac{\gamma + \alpha}{\alpha} + \sum_k i_k \right) \\
    &= (1 - \alpha)_{(l)} \sum_{\bm i \in \mathcal S(\bm c, j, -l)} \Gamma\left(\frac{\gamma + \alpha}{ \alpha} + |\bm i|\right) J^{-|\bm i|} \prod_{k=1}^J \mathscr{C}(c_k - l \delta_{k,j}, i_k; \alpha).
\end{align*}
Similarly, the integral at the denominator  of \eqref{post_pk} equals
\begin{align*}
   & \sum_{i_1 = 1}^{c_1} \cdots \sum_{i_j = 1}^{c_j+1} \cdots \sum_{i_J = 1}^{c_J} J^{- \sum_k i_k} \mathscr{C}(c_j +1, i_j; \alpha) \prod_{k \neq j} \mathscr{C}(c_k, i_k; \alpha)  \Gamma\left(\frac{\gamma}{\alpha} + \sum_k i_k \right) \\
    &= \sum_{\bm i \in \mathcal S(\bm c, j, 1)} \Gamma\left(\gamma / \alpha + |\bm i|\right) J^{-|\bm i|} \prod_{k=1}^J \mathscr{C}(c_k + \delta_{k,j}, i_k; \alpha).
\end{align*}
Combining these gives \eqref{post_pyp}.

\subsection{Monte Carlo Estimation of \eqref{post_pyp}}\label{app:computations_pyp}

As in \cite{Dol(23)}, we note that the posterior distribution in \eqref{post_pyp} can be equivalently expressed as a ratio of expectations as follows:
\begin{multline}\label{eq:rec_nrm_py_mc}
    \prob\left[ f_{X_{n+1}} = l \mid \bm C = \bm c, h(X_{n+1}) = j\right] = \\ \frac{\gamma}{J} \binom{c_j}{l} (1 - \alpha)_{(l)} \frac{(\gamma)_{(c_j -l)}}{(\gamma)_{(c_j + 1)}} \frac{\E\left[\frac{ \left(\frac{\gamma + \alpha}{\gamma} \right)_{K^{\mathcal S(\bm c, j, -l) }_{\boldsymbol \cdot}}}{J^{K^{\mathcal S(\bm c, j, -l) }_{\boldsymbol \cdot}} \prod_{k=1}^J \left(\frac{\gamma}{\alpha}\right)_{K_{c_k - l \delta_{k,j}}}} \right]}{\E\left[\frac{ \left(\frac{\gamma }{\gamma} \right)_{K^{\mathcal S(\bm c, j, 1) }_{\boldsymbol \cdot}}}{J^{K^{\mathcal S(\bm c, j, 1) }_{\boldsymbol \cdot}} \prod_{k=1}^J \left(\frac{\gamma}{\alpha}\right)_{K_{c_k + \delta_{k,j}}}} \right]}
\end{multline}
where $K_m$ is number of distinct values in a sample of size $m$ from a PYP and
 $K^{\mathcal S(\bm c, j, -l) }_{\boldsymbol \cdot} := \sum_{k=1}^J K_{c_k - l \delta_{k, j}}$.

To prove \eqref{eq:rec_nrm_py_mc}, recall that the number of distinct elements $K_m$ in a sample of size $m$ from a PYP has distribution
\[
    \prob\left[K_m = k\right] = \frac{\left(\frac{\gamma}{\alpha} \right)_{(k)}}{(\gamma)_{(m)}} \mathscr{C}(m, k; \alpha).
\]

Consider now \eqref{post_pyp} and note that $\Gamma\left(a + b\right) = (a)_{(b)} \Gamma(a)$.
Then
\begin{align*}
    &\prob\left[ f_{X_{n+1}} = l \mid \bm C = \bm c, h(X_{n+1}) = j\right] = \\
    & \qquad = \frac{\gamma}{J} \binom{c_j}{l} (1 - \alpha)_{(l)} \frac{\sum_{\bm i \in \mathcal S(\bm c, j, -l)} \Gamma\left(\frac{\gamma + \alpha}{ \alpha} + |\bm i|\right) J^{-|\bm i|} \prod_{k=1}^J \mathscr{C}(c_k - l \delta_{k,j}, i_k; \alpha)
    }{
    \sum_{\bm i \in \mathcal S(\bm c, j, 1)} \Gamma\left(\gamma / \alpha + |\bm i|\right) J^{-|\bm i|} \prod_{k=1}^J \mathscr{C}(c_k + \delta_{k,j}, i_k; \alpha)} \\
    & \qquad = \frac{\gamma}{J} \binom{c_j}{l} (1 - \alpha)_{(l)} \frac{(\gamma)_{(c_j -l)}}{(\gamma)_{(c_j + 1)}}  \frac{\sum_{\bm i \in \mathcal S(\bm c, j, -l)} \frac{\left(\frac{\gamma + \alpha}{\alpha}\right)_{(|\bm i|)}}{J^{|\bm i|} \prod_{h=1}^J \left(\frac{\gamma}{\alpha} \right)_{(i_h)}}  \prod_{k=1}^J \frac{\left(\frac{\gamma}{\alpha}\right)_{(i_k)}}{(\gamma)_{(c_k - l \delta_{k, j})}}\mathscr{C}(c_k - l \delta_{k,j}, i_k; \alpha)
    }{
    \sum_{\bm i \in \mathcal S(\bm c, j, 1)} \frac{\left(\frac{\gamma}{\alpha}\right)_{(|\bm i|)}}{J^{|\bm i|} \prod_{h=1}^J \left(\frac{\gamma}{\alpha} \right)_{(i_h)}}  \prod_{k=1}^J \frac{\left(\frac{\gamma}{\alpha}\right)_{(i_k)}}{(\gamma)_{(c_k + \delta_{k, j})}}\mathscr{C}(c_k - l \delta_{k,j}, i_k; \alpha)
    } \\
     & \qquad = \frac{\gamma}{J} \binom{c_j}{l} (1 - \alpha)_{(l)} \frac{(\gamma)_{(c_j -l)}}{(\gamma)_{(c_j + 1)}}  \frac{\sum_{\bm i \in \mathcal S(\bm c, j, -l)} \frac{\left(\frac{\gamma + \alpha}{\alpha}\right)_{(|\bm i|)}}{J^{|\bm i|} \prod_{h=1}^J \left(\frac{\gamma}{\alpha} \right)_{(i_h)}}  \prod_{k=1}^J \prob\left[K_{c_k - l \delta_{k,j}} = i_k\right]
    }{
    \sum_{\bm i \in \mathcal S(\bm c, j, 1)} \frac{\left(\frac{\gamma}{\alpha}\right)_{(|\bm i|)}}{J^{|\bm i|} \prod_{h=1}^J \left(\frac{\gamma}{\alpha} \right)_{(i_h)}}  \prod_{k=1}^J 
    \prob\left[K_{c_k + \delta_{k,j}} = i_k\right]
    }\\ 
    & \qquad = \frac{\gamma}{J} \binom{c_j}{l} (1 - \alpha)_{(l)} \frac{(\gamma)_{(c_j -l)}}{(\gamma)_{(c_j + 1)}} \frac{\E\left[\frac{ \left(\frac{\gamma + \alpha}{\gamma} \right)_{K^{\mathcal S(\bm c, j, -l) }_{\boldsymbol \cdot}}}{J^{K^{\mathcal S(\bm c, j, -l) }_{\boldsymbol \cdot}} \prod_{k=1}^J \left(\frac{\gamma}{\alpha}\right)_{K_{c_k - l \delta_{k,j}}}} \right]}{\E\left[\frac{ \left(\frac{\gamma }{\gamma} \right)_{K^{\mathcal S(\bm c, j, 1) }_{\boldsymbol \cdot}}}{J^{K^{\mathcal S(\bm c, j, 1) }_{\boldsymbol \cdot}} \prod_{k=1}^J \left(\frac{\gamma}{\alpha}\right)_{K_{c_k + \delta_{k,j}}}} \right]}
\end{align*}


\section{Details about the Poisson-IBP}\label{app:poisson}

\paragraph{Proof of Equation \eqref{eq:pois_gamma}}
It follows from $\psi(u) = \log(1 + u)$ and $\kappa(u, n) = (n-1)! / (u + 1)^n$.

\paragraph{Proof of \eqref{eq:pois_gg}} 

We have
\begin{align*}
    \psi(u) &= \frac{\alpha}{\Gamma(1 - \alpha)} \int_{\R_+}(1 - e ^{-us}) \rho(s) \dd s =  \frac{\alpha}{\Gamma(1 - \alpha)} \int_{\R_+}(1 - e ^{-us}) s^{-1 - \alpha} e^{-\tau s}\dd s \\
    &=  \frac{\alpha}{\Gamma(1 - \alpha)} \int_{\R_+} \int_{\R_+} (1 - e ^{-us}) \frac{t^\alpha e^{-ts}}{\Gamma(\alpha + 1)} e^{-s} \dd t \, \dd s \\
    &=  \frac{\alpha}{\Gamma(1 - \alpha)} \int_{\R_+} \frac{t^\alpha u}{(t+1)(t+u+1)} \dd t = \frac{\alpha \Gamma(\alpha) \Gamma(1 - \alpha)}{\Gamma(1 - \alpha) \Gamma(\alpha + 1)} \left[(\tau + u)^\alpha - \tau^\alpha \right] \\
    &= \left[(\tau + u)^\alpha - \tau^\alpha \right],
\end{align*}
and
\[
    \kappa(l, u) = \alpha \frac{\Gamma(l - \alpha)}{\Gamma(1 - \alpha)} (\tau + u)^{\alpha - l} = \alpha (1 - \alpha)_{(l-1)}(\tau + u)^{\alpha - l}
\]
Denoting by $(*)$ the summation over positive integers $(k_1, \ldots, k_i)$ such that $\sum_{j=1}^i k_j = n$, we have 
\begin{align*}
    \frac{\dd^n}{\dd u^n} e^{- \theta \psi(u) / J} &= e^{\theta/J \tau^\alpha} \frac{\dd^n}{\dd u^n} e^{- \theta/J (\tau + u)^\alpha} \\
    &= e^{-\theta/J \psi(u)} \sum_{i=1}^n \left(\frac{\theta}{J}\right)^{i} \sum_{(*)} \frac{1}{i!} \binom{n}{k_1, \ldots, k_i} \prod_{j=1}^i \alpha (\tau + u)^{\alpha - k_j} (1 - \alpha)_{k_j - 1} \\
    &= e^{-\theta/J \psi(u)} \sum_{i=1}^n \left(\frac{\theta}{J}\right)^{i} \frac{\calC(n, i; \alpha)}{(\tau + u)^{n - \alpha i}}.
\end{align*}
Plugging these in the expression of \Cref{prop:poisson_traits} leads to \eqref{eq:pois_gg}.






\section*{Acknowledgements}

Stefano Favaro wishes to thank Emanuele Dolera and Matteo Sesia for stimulating discussions on sketches and generalizations thereof. Mario Beraha and Stefano Favaro received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme under grant agreement No 817257. Stefano Favaro gratefully acknowledge the financial support from the Italian Ministry of Education, University and Research (MIUR), ``Dipartimenti di Eccellenza" grant 2018-2022.



\begin{thebibliography}{100}

\bibitem[Aggarwal and Yu(2010)]{Agg(10)}
\textsc{Aggarwal, C. and Yu, P.} (2010). On classification of high-cardinality data streams. In \textit{Proceedings of the 2010 SIAM International Conference on Data Mining}.

\bibitem[Alon et al.(1999)]{Alo(99)}
\textsc{Alon, N., Matias, Y. and Szegedy, M.} (1999). The space complexity of approximating the frequency moments. \textit{Journal of Computer and System Sciences} \textbf{58}, 137--147.

\bibitem[Berger et al.(2016)]{Ber(16)}
\textsc{Berger, B., Daniels, N.M., and Yu, Y.W.} (2016). Computational biology in the 21st century: scaling with compressive algorithms. \textit{Communication of the ACM} \textbf{59}, 72--80.

\bibitem[Bernton et al.(2019)]{Ber(19)}
\textsc{Bernton, E., Jacob, P.E., Gerber, M. and Robert, C.P.} (2019). On parameter estimation with the Wasserstein distance. \textit{Information and Inference: a Journal of the IMA} \textbf{8}, 657--676.

\bibitem[Brix(1999)]{Bri(99)}
\textsc{Brix, A.} (1999). Generalized gamma measures and shot-noise Cox processes. \textit{Advances in Applied Probability} \textbf{31}, 929--953.

\bibitem[Broderick et al.(2015)]{Bro(15)}
\textsc{Broderick, T., Mackey, L., Paisley, J., and Jordan, M.I.} (2015). Combinatorial clustering and the beta negative binomial process. \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence} \textbf{37}, 290--306.

\bibitem[Broderick et al.(2018)]{Bro(18)}
\textsc{Broderick, T., Wilson, A.C, and Jordan, M.I.} (2018). Posteriors, conjugacy, and exponential families for completely random measures. \textit{Bernoulli} \textbf{24}, 3181--3221.

\bibitem[Cai et al.(2018)]{Cai(18)}
\textsc{Cai, D., Mitzenmacher, M., and Adams, R. P.} (2018) A Bayesian nonparametric view on count-min sketch. In \textit{Advances in Neural Information Processing Systems}.

\bibitem[Camerlenghi et al.(2019)]{Cam(19)}
\textsc{Camerlenghi, F., A. Lijoi, P. Orbanz, and I. Pr{\"u}nster} (2019) Distribution theory for hierarchical processes. \textit{The Annals of Statistics} \textbf{47}, 67--92.

\bibitem[Campbell et al.(2018)]{Cam(18)}
\textsc{Campbell, T., Cai, D. and Broderick. T.} (2018) Exchangeable trait allocations. \textit{Electronic Journal of Statistics} \textbf{12}, 2290--2322.

\bibitem[Charalambides(2005)]{Cha(05)}
\textsc{Charalambides, C.} (2005) \textit{Combinatorial methods in discrete distributions.} Wiley.

\bibitem[Charikar et al.(2004)]{Cha(04)}
\textsc{Charikar, M., Chen, K. and Farach-Colton, M.} (2002) Finding frequent items in data streams. \textit{Theoretical Computer Science} \textbf{312}, 3--15

\bibitem[Chen and Liu (1997)]{Che(97)}
\textsc{Chen, S. X. and Liu, J. S.} (1997) Statistical applications of the Poisson-Binomial and conditional Bernoulli distributions. \textit{Statistica Sinica} \textbf{7}, 875--892.

\bibitem[Cormode(2017)]{Cor(17)}
\textsc{Cormode, G.} (2017). Data sketching. \textit{Communication of the ACM} \textbf{60}, 48--55.

\bibitem[Cormode et al.(2018)]{Cor(18)}
\textsc{Cormode, G., Jha, S., Kulkarni, T., Li, N., Srivastava D. and Wang, T.} (2018). Privacy at scale: local differential privacy in practice. In \textit{Proceedings of the International Conference on Management}.

\bibitem[Cormode and Muthukrishnan(2005)]{Cor(05)}
\textsc{Cormode, G. and Muthukrishnan, S.} (2005). An improved data stream summary: the count-min sketch and its applications. \textit{Journal of Algorithms} \textbf{55}, 58--75.

\bibitem[Cormode and Yi(2020)]{Cor(20)}
\textsc{Cormode, G. and Yi, K.} (2020). \textit{Small summaries for big data}. Cambridge University Press.

\bibitem[Dolera and Favaro(2020)]{Dol(20)}
\textsc{Dolera, E. and Favaro, S.} (2020). A Berry-Esseen theorem for Pitman’s $\alpha$-diversity. \textit{The Annals of Applied Probability} \textbf{30}, 847--869.

\bibitem[Dolera et al.(2021)]{Dol(21)}
\textsc{Dolera, E., Favaro, S. and Peluchetti, S.} (2021). A Bayesian nonparametric approach to count-min sketch under power-law data stream. In \textit{International Conference on Artificial Intelligence and Statistics}.

\bibitem[Dolera et al.(2023)]{Dol(23)}
\textsc{Dolera, E., Favaro, S. and Peluchetti, S.} (2023). Learning-augmented count-min sketches via Bayesian nonparametrics. \textit{Journal of Machine Learning Research} \textbf{24}, 1--60.

\bibitem[Dwork et al.(2010)]{Dwo(10)}
\textsc{Dwork, C. and Naor, M. and Pitassi, T. and Rothblum, G. and Yekhanin, S.} (2010). Pan-private streaming algorithms. In \textit{Proceedings of the Symposium on Innovations in Computer Science}.

\bibitem[Ferguson(1973)]{Fer(73)}
\textsc{Ferguson, T.S.} (1973). A Bayesian analysis of some nonparametric problems. \textit{The Annals of Statistics} \textbf{1}, 209--230.

\bibitem[Goyal et al.(2012)]{Goy(12)}
\textsc{Goyal, A., Daum\'e, H. and Cormode, G.} (2012). Sketch algorithms for estimating point queries in NLP. In \textit{Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}.

\bibitem[Indyk(2006)]{Ind(06)}
\textsc{Indyk, P.} (2006). Stable distributions, pseudorandom generators, embeddings and data stream computation. \textit{Journal of the ACM} \textbf{53}, 307--325.

\bibitem[James(2002)]{Jam(02)}
\textsc{James, L.F.} (2002). Poisson process partition calculus with applications to exchangeable models and Bayesian nonparametrics. \textit{Preprint  arXiv:math/0205093}.

\bibitem[James(2017)]{Jam(17)}
\textsc{James, L.F.} (2017). Bayesian Poisson calculus for latent feature modeling via generalized Indian buffet process priors \textit{The Annals of Statistics} \textbf{45}, 2016--2045.

\bibitem[Karp et al.(2003)]{Kar(03)}
\textsc{Karp, R., Shenker, S. and Papadimitriou, C.H.} (2003). A simple algorithm for finding frequent elements in streams and bags. \textit{ACM Transactions on Database Systems} \textbf{28}, 51--53.

\bibitem[Kingman(1967)]{Kin(67)}
\textsc{Kingman, J.F.C} (1967). Completely random measures.\textit{Pacific Journal of Mathematics} \textbf{21}, 59--78.

\bibitem[Kingman(1975)]{Kin(75)}
\textsc{Kingman, J.F.C} (1975). Random discrete distributions. \textit{Journal of the Royal Statistica Society Series B} \textbf{37}, 1--22.

\bibitem[Kingman(1993)]{Kin(93)}
\textsc{Kingman, J.F.C} (1993). \textit{Poisson processes}. Oxford University Press, Oxford.

\bibitem[Kockan et al.(2020)]{Koc(20)}
\textsc{Kockan, C., Zhu, K., Dokmai, N., Karpov, N., Kulekci, M.O., Woodruff, D.P, and Sahinalp, S.C.} (2020). Sketching algorithms for genomic data analysis and querying in a secure enclave. \textit{Nature methods} \textbf{17}, 295--301.

\bibitem[Last and Penrose(2018)]{Las(18)}
\textsc{Last, G. and Penrose, M.} (2018). Lectures on the Poisson process. \textit{Cambridge University Press}.

\bibitem[Le Cam (1960)]{LeCam(60)}
\textsc{Le Cam, L.} (1960). An approximation theorem for the Poisson-Binomial distribution. \textit{Pacific Journal of Mathematics} \textbf{10}, 1181--1197.

\bibitem[Leo Elworth et al.(2020)]{Leo(20)}
\textsc{Leo Elworth, L.A., Wang, Q., Kota, P.K., Barberan, C.J., Coleman, B., Balaji, A., Gupta, G., Baraniuk, R.G., Shrivastava, A. and Treangen, T.J.} (2020).  To petabytes and beyond: recent advances in probabilistic and signal processing algorithms and their application to metagenomics. \textit{Nucleic Acids Research} \textbf{48} 5217--5234.

\bibitem[Lijoi and Pr\"unster(2010)]{Lij(10)}
\textsc{Lijoi, A. and Pr\"unster, I.} (2010). Models beyond the Dirichlet process. In \textit{Bayesian Nonparametrics}, Hjort, N.L., Holmes, C.C. M\"uller, P. and Walker, S.G. Eds. Cambridge University Press.

\bibitem[Lijoi et al.(2005)]{Lij(05)}
\textsc{Lijoi, A., Mena, R. H., and Pr\"unster, I.} (2005). Hierarchical mixture modeling with normalized inverse-Gaussian
priors. \textit{Journal of the American Statistical Association} \textbf{100}, 1278--1291.

\bibitem[Lijoi et al.(2007)]{Lij(07)}
\textsc{Lijoi, A., Mena, R.H. and Pr\"unster, I.} (2007). Controlling the reinforcement in Bayesian nonparametric mixture models. \textit{Journal of the Royal Statistica Society Series B} \textbf{69}, 715-740.

\bibitem[Manku and Motwani(2002)]{Man(02)}
\textsc{Manku, G.S. and Motwani, R.} (2002). Approximate frequency counts over data streams. In \textit{Proceedings of the International Conference on Very Large Data Bases}.

\bibitem[Marcais et al.(2019)]{Mar(19)}
\textsc{Marcais, G., Solomon, B., Patro, R. and Kingsford, C} (2019). Sketching and sublinear data structures in genomics. \textit{Annual Review of Biomedical Data Science} \textbf{89}, 669--682

\bibitem[Melis et al.(2016)]{Mel(16)}
\textsc{Melis, L., Danezis, G., and Cristofaro, E.D.} (1982). Efficient private statistics with succinct sketches. In \textit{Proceedings of the NDSS Symposium}.

\bibitem[Misra and Gries(1982)]{Mis(82)}
\textsc{Misra, J. and Gries D.} (1982).  Finding repeated elements. \textit{Science of computer programming} \textbf{2}, 143--152.

\bibitem[Mitzenmacher and Upfal(2017)]{Mit(17)}
\textsc{Mitzenmacher, M. and Upfal, E.} (2017). \textit{Probability and computing: randomization and probabilistic techniques in algorithms and data analysis}. Cambridge University Press.

\bibitem[Pitman and Yor(1997)]{Pit(97)}
\textsc{Pitman, J. and Yor, M.} (1997). The two parameter Poisson-Dirichlet distribution derived from a stable subordinator. \textit{The Annals of Probability} \textbf{25}, 855--900.

\bibitem[Pitman(2003)]{Pit(03)}
\textsc{Pitman, J.} (2003). Poisson-Kingman partitions. In \textit{Science and Statistics: A Festschrift for Terry Speed}, Goldstein, D.R. Eds. Institute of Mathematical Statistics.

\bibitem[Pitman(2006)]{Pit(06)} 
\textsc{Pitman, J.} (2006). \textit{Combinatorial Stochastic Processes.} Ecole d'Et\'e de Probabilit\'es de Saint-Flour XXXII. Lecture notes in mathematics, Springer - New York.

\bibitem[Pr\"unster(2002)]{Pru(02)}   
\textsc{Pr\"unster, I.} (2002). \textit{Random probability measures derived from increasing additive processes and their application to Bayesian statistics}. PhD Thesis, University of Pavia.

\bibitem[Pitman and Yor(1997)]{Pit(97)}
\textsc{Pitman, J. and Yor, M.} (1997). The two parameter Poisson-Dirichlet distribution derived from a stable subordinator. \textit{The Annals of Probability} \textbf{25}, 855--900.

\bibitem[Rennie et al.(2003)]{Ren(03)}
\textsc{Rennie, J. D. and Shih, L. and Teevan, J. and Karger, D. R.} (2003). Tackling the poor assumptions of naive Bayes text classifiers. \textit{Proceedings of the 20th International Conference on Machine Learning}.

\bibitem[Schechter et al.(2010)]{Sch(10)}
\textsc{Schechter, S., Herley, C. and Mitzenmacher, M.} (2010). Popularity is everything: a new approach to protecting passwords from Statistical-Guessing attacks. \textit{USENIX Workshop on Hot Topics in Security} \textbf{10}.

\bibitem[Shi et al.(2009)]{Shi(09)}
\textsc{Shi, Q., Petterson, J., Dror, G., Langford, J., Smola, A. and Vishwanathan, S.} (2009). Hash kernels for structured data. \textit{Journal of Machine Learning Research} \textbf{10}, 2615--2637. 

\bibitem[Solomon and Kingsford(2016)]{Sol(16)}
\textsc{Solomon, B. and Kingsford, C.} (2016). Fast search of thousands of short-read sequencing experiments. \textit{Nature Biotechnology} \textbf{34}, 300--302.

\bibitem[Song et al.(2009)]{Son(09)}
\textsc{Song, H.H., Cho, T.W., Dave, V., Zhang, Y. and Qiu, L.} (2009). Scalable proximity estimation and link prediction in online social networks. In \textit{Proceedings of the ACM SIGCOMM Conference on Internet Measurement}.

\bibitem[Regazzini(2001)]{Reg(01)}
\textsc{Regazzini, E.} (2001). \textit{Foundations of Bayesian statistics and some theory of Bayesian nonparametric methods.} Lecture Notes, Stanford University.

\bibitem[Regazzini et al.(2003)]{Reg(03)}
\textsc{Regazzini, E., Lijoi, A. and Pr\"unster, I.} (2003). Distributional results for means of normalized random measures with independent increments.
 \textit{The Annals of Statistics}. \textbf{31}, 560--585.

\bibitem[Steele (1994)]{Ste(94)}
\textsc{Steele J. M.} (1994). Le Cam's inequality and Poisson approximations. \textit{The American Mathematical Monthly}. \textbf{101}, 48--54.
 
\bibitem[Zhang et al.(2014)]{Zha(14)}
\textsc{Zhang, Q., Pell, J., Canino-Koning, R., Howe, A.C. and Brown, C.T.} (2014). These are not the k-mers you are looking for: efficient online $k$-mer counting using a probabilistic data structure. \textit{PloS one} \textbf{9}.

\bibitem[Zhou et al.(2016)]{Zho(16)}
\textsc{Zhou, M. and Padilla, O. H. M and Scott, J. G.} (2016). Priors for random count matrices derived from a family of negative binomial processes. \textit{Journal of the American Statistical Association} \textbf{111}, 1144--1156.

\end{thebibliography}

\end{document}
