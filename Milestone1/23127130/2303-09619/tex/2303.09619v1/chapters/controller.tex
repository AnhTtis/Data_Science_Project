% \acrfull{mpc} is an advanced form of closed-loop control that calculates the optimal input sequence for a dynamic-sytem to reach a desired state.
% \acrshort{mpc} works really well with complex, non-linear, \acrshort{mimo} systems that may have interactions between their inputs and outputs.
% It is also able to handle constraints on the system's input, state and output.
% This enables the controller to simulate the bounded inputs of a realistic model as well as implement a collision avoidance system inside the controller itself.

% \acrshort{mpc} works by using a mathematical model of the plant to predict the system's behavior in the future.
% At time $t$, the system's state is sampled by means of various sensors and a control strategy is calculated to minimize a cost function over a short horizon $[t, t+T]$.
% An optimization algorithm is used to calculate a sequence of control inputs $u_k$ that will drive the system to a state that minimizes the cost function.
% Only the first step of the control strategy, $u_0$, is used to drive the system.
% Then, in the next control iteration, the plant's state is sampled again and a new control strategy is calculated.
% This continuous shifting of the horizon to the future, eventually drives the system to the desired state.

% \insertfig{1_mpc}{Block diagram of a \acrshort{mpc} controller}

% In this project, the chasers need to maintain a fixed pose relative to the \gls{target} and avoid collision with the rest of the chasers and the target.
% To achieve this, both the future states of the chasers and the target need to be predicted and their relative pose needs to be maintained in every time-step of the horizon.
% Details about the system dynamics of both a \gls{chaser} and the target in can be found in Section \ref{ch:chap3:sec2} and the cost function that implements this concept in Section \ref{ch:chap3:sec3}.
% Additional constraints are added to the optimization problem to ensure collision avoidance.
% Details about the constraints enforced on the optimizer can be found in Section \ref{ch:chap3:sec4}.

% To sum up, an \acrshort{mpc} controller uses the following principles:
% \begin{itemize}
%     \item A model of the controlled dynamic system.
%     \item A cost function $J$ over the horizon $[t, t+T]$.
%     \item An optimization algorithm that minimizes $J$ using the control input $u$ while obeying a set of constraints.
% \end{itemize}

As previously mentioned, the control architecture uses a MPC backbone.
In this section, the cost function and constraints used to formulate the MPC will be presented.




% \subsection{Euler Method} \label{ch:chap3:euler}
% Now the system dymanics need to be descretized and solved to be used with the MPC controller.
% For that purpose, the Euler method is used (Equation \ref{eq:3_euler_method}).
% The local error of this method is proportinal to the square of the step size, and the global error proportional to the step size.
% This means that a smaller step size will result in a more accurate solution but for the same horizon duration, smaller step size will result in higher computational complexity and thus an increase on processing time.
% A good balance between time-step and horizon length is necessary to obtain a good \acrshort{mpc} prediction and keep the calculation time low.

% \begin{equation} \label{eq:3_euler_method}
%     \begin{array}{c}
%         \text{For a dynamic system defined as: } \dot{x} = f(x,u)\\
%         x_{k+1} = x_k + T_s f(x_k,u_k)
%     \end{array}
% \end{equation}

% From Equation \ref{eq:3_chaser_dynamics}, it is obvious that the control inputs that will be used in the descritized equations for each chaser are:

% \begin{equation}
%     \begin{split}
%         u   &= [F_x, F_y, F_z, \tau_x, \tau_y, \tau_z]^T\\
%             &= [\bm{F}, \bm{\tau}]^T
%     \end{split}
% \end{equation}

\subsection{Cost Function} \label{ch:chap3:sec3}

% As mentioned in Section \ref{ch:chap3}, a cost function $J$ must be determined for the \acrshort{mpc} controller to work.
% This cost function will be minimized by the optimization algorithm using $u$ to drive the plant to the desired state.

% Since the mission requires each chaser $i$ to be at a pose ${x_{ref}^i}$ relative to the target, the cost function could be defined as the distance between the desired pose ${x_{ref}^i}$ and the chaser's pose $p_{ch} = [\bm{x}, \bm{q}]$.
% But $ref^i$ changes while the target is moving during the time horizon.
% Therefore, the MPC controller predicts the movement of the target as well, and uses these predicted future poses to calculate what the desired pose $ref^i_k$ will be in every time-step $k$ of the horizon.
% An appropriate cost function, that uses these predictions to calculate an optimal control sequence is $J$ in \eqref{eq:3_cost_function}.

The chasers state vectors are defined as $x^i = [p^i, q^i]^\top$ and the corresponding control action as $u^i = [F^i, \tau^i]^\top$.
The system dynamics are discretized with a sampling time of $dt$ using the forward Euler method to obtain $x^i_{k+1} = \zeta(x^i_k, u^i_k)$. 
The target's state vector is defined as $x^{tar} = [p^{tar}, q^{tar}]$, where $p^{tar}$ is the position and $q^{tar}$ the quaternion attitude represention of the target.
They are descritized in a manner similar to the chaser's states.
The reference poses for the chasers are calculated by transfering the target's body frame using $x^{{off}_i} = [p^{{off}_i}, q^{{off}_i}]$, where $p^{{off}_i}$ is a translation and $q^{{off}_i}$ is a rotation quaternion.
These calculations are performed as such: $p^{{ref}_i}_k = p^{tar}_k + q^{tar}_k \otimes p^{{off}_i} \otimes {q^{tar}_k}^\ast$ and $q^{{ref}_i}_k = q^{tar}_k q^{{off}_i}$.
This discrete model is used as the prediction model of the NMPC.
The prediction is performed over a receding horizon of $D = T/dt$ steps, where $T$ is the horizon duration in seconds.

A cost function is defined such that, when minimized in the current time and the predicted horizon, an optimal set of control actions $u^i_k$ will be calculated.
Let $x_{k+j|k}$ and $x^{tar}_{k+j|k}$ be the predicted chaser and target states at time step $k+j$ respectively, calculated in time step $k$.
The corresponding control actions are $u_{k+j|k}$ and reference states $x^{{ref}_i}_{k+j|k}$.
Also, let $\bm{x}_k$ and $\bm{u}_k$ be the predicted states and control actions for the whole horizon duration calculated at time step $k$.
The cost function is formulated as follows:

\begin{multline} \label{eq:3_cost_function}
    J(\bm{x}_k, \bm{u}_k, u_{k-1|k}) = \sum_{i=0}^{N-1} \{ \sum_{j=0}^{D} \{ \\ 
    \underbrace{(1 - \frac{\alpha \cdot k}{D} )}_\text{Falloff \%} \cdot [\underbrace{{\| p^{{ref}_i}_{k+j|k} - p^i_{k+j|k} \|}^2 Q_p}_\text{Position cost} + \underbrace{{\| q^i_{k+j|k} \otimes {q^{{ref}_i}_{k+j|k}}^\ast \|}^2 Q_q}_\text{Orientation cost}] \\
    + \underbrace{\| u^i_{ref} - u^i_{k+j|k} \|^2 Q_u \}}_\text{Input cost} \\ 
    + \underbrace{{\| p^{{ref}_i}_{k+D|k} - p^i_{k+D|k} \|}^2 Q^f_p + {\| q^i_{k+D|k} \otimes {q^{{ref}_i}_{k+D|k}}^\ast \|}^2 Q^f_q}_\text{Final state cost} \}
\end{multline}

where $Q_p, Q^f_p \in \mathbb{R}^{3\times3}$, $Q_q, Q^f_q \in \mathbb{R}^{4\times4}$ and $Q_u \in \mathbb{R}^{6\times6}$ are positive definite weight matrices for the position and orientation states and inputs respectively. 
In \eqref{eq:3_cost_function}, the first term represents the state cost which penalizes deviation from the reference state at time-step $k$, $x^{{ref}_i}_k$.
The Falloff term $\alpha$, is an adaptive weight to penalize overshoot errors.
% The idea was that error propagation of velocity measurements can accumulate and become proportional to the horizon duration.
The second term represents the input cost which penalizes deviation from the steady-state input $u_{ref} = 0$ that describes constant-velocity movement.
Finally, the final state-cost applies an extra penalty for deviation of the state from the reference at the end of the horizon period.

% where %\footnote{Powers in these equations are element-wise operations}: 
% \begin{itemize}
%     % \item $N =$ The number of chasers
%     % \item $D = T / dt$ The horizon length
%     % \item $J_{p_{i,k}} = Q_p \cdot \left\{ \begin{array}{c} 
%     %     ({ref}_k - p_{i_k})^2 \text{ , for $x$, $\dot{x}$ and $\omega$}\\ 
%     %     (p_{i_k} \otimes {ref}_k^*)^2 \text{ , for $\bm{q}$}
%     % \end{array} \right. $ \\The cost representing the position error of chaser $i$ on time $k$
%     \item $Q_p, Q_{fp} \in \mathbb{R}^{3\times3}$, $Q_q, Q_{fq} \in \mathbb{R}^{4\times4}$ and $Q_u \in \mathbb{R}^{6\times6}$ are positive definite weight matrices for the position and orientation states and inputs respectively.
%     \item $J_{u_{i,k}} = Q_u \cdot u_{i_k}$ The control cost on time $k$ for chaser $i$
%     \item $J_{f_{i}} = Q_f \cdot J_{i_{T}}$ The cost representing the final state error of chaser $i$
%     \item $Q_p, Q_u, Q_f =$ Weight matrices for the cost functions
%     \item ${ref}_k =$ The predicted pose of the \gls{target} at time $k$
%     \item $\alpha$ = Falloff percentage
% \end{itemize}

\subsection{Constraints} \label{ch:chap3:sec4}

% Appropriate optimization constraints must force the controller to respect the hardware limits for the control inputs \eqref{eq:3_constr_max}.
% They can also force the optimizer to limit the plant output to avoid collisions.
A minimum chaser-chaser and chaser-target distance $d_{min}$ is enforced by \eqref{eq:3_constr_dist1} and \eqref{eq:3_constr_dist2}.
\eqref{eq:3_constr_dist3} prevents the chasers from moving too far away from the target by setting their maximum distance to $d_{max}$.
The implemented constraints are the following:
\begin{subequations}
    \begin{equation}\label{eq:3_constr_max}
        u^i_{k+j|k} = [\bm{F}^i_{k+j|k}, \bm{\tau}^i_{k+j|k}] \in [-F_{max}, F_{max}] \times [-\tau_{max}, \tau_{max}]
    \end{equation}
    
    \begin{equation}\label{eq:3_constr_dist1}
        \dist{(p^{tar}_{k+j|k}, p^m_{k+j|k})} \geq d_{min}\; \forall\; j \in [0, D]
    \end{equation}
    
    \begin{equation}\label{eq:3_constr_dist2}
        \dist{(p^m_{k+j|k}, p^n_{k+j|k})} \geq d_{min} \forall\; j \in [0, D],\; m \neq n
    \end{equation}
    
    \begin{equation}\label{eq:3_constr_dist3}
        \dist{(p^{{ref}_m}_{k+j|k}, p^m_{k+j|k})} \leq d_{max}\; \forall\; j \in [0, D]
    \end{equation}
\end{subequations}
where $dist$ is a function of Eucledean distance.

\subsection{Optimizer} \label{ch:chap3:sec5}

% An optimization problem is defined as the calculation of the extrema of an objective function $f(x)$ over a set of real variables $x$.
% The problem might be subject to a set of conditions defined as a system of equalities and inequalities called constraints.
% \eqref{eq:3_optimization} is the mathematic representation of the problem where $m,p \in \mathbb{Z^+}$ and $f, g_i, h_j$ are real functions on $X \subseteq \mathbb{R}^n$.
% \acrfull{nlp} is the process of solving an optimization problem where at least one of $f, g_i, h_j$ is non-linear \cite{math_programming}.

% \begin{equation} \label{eq:3_optimization}
%     \begin{split}
%         \min_{x \in X \subseteq \mathbb{R}^n} \quad & f(x)\\
%         \text{subject to} \quad & g_i(x) \ge 0 \; \forall i \in \{1, \dots, m\}\\
%         & h_j(x) = 0 \; \forall j \in \{1, \dots, p\}
%     \end{split}
% \end{equation}

\acrfull{open} was used as the MPC cost function optimizer.
\acrshort{open} is a framework developed by \cite{opengen} and is based on the PANOC \citep{panoc} optimization algorithm.
PANOC is an extremely fast, the state-of-the-art optimizer for real-time, embedded applications.
A powerful symbolic mathematics library called CasADi \citep{casadi} is used to define the optimization problem and perform under-the-hood operations.
% OpEn runs in Rust, a programming language that is ideal for embeded, real-time applications and can be interfaced for use with many high level programming languages.
% These make it the state-of-the-art option for real-time applications such as robotics, autonomous vehicles and \acrshort{uav}s where running an optimizer is necessary to close a control loop in every time-step.

% In contrast to other popular libraries, \acrshort{open} uses an algorithm called PANOC.
% PANOC is an algorithm introduced by \cite{panoc} that involves only very simple iterations making it extremely fast.
% It is fundamentally different from other popular, iterative algorithms as it implements a method called proximal averaged Newton-type method.
% PANOC is capable of solving \acrshort{nlp} problems in the form described by \eqref{eq:3_nlp_panoc}.
% (As described in the library's \href{https://alphaville.github.io/optimization-engine/docs/open-intro}{documentation page}).
% \begin{equation}\label{eq:3_nlp_panoc}
%     \begin{split}
%         \mathbb{P}(p) \; \text{:} \; \min_{u \in \mathbb{R}^{n_u}} \quad & f(u, p)\\
%         \text{subject to} \quad & u \; \in \; U\\
%         & F_1(u, p) \; \in \; C\\
%         & F_2(u, p) \; = \; 0
%     \end{split}
% \end{equation}
% Where $u \in \mathbb{R}^{n_u}$ is the vector decision variables of the problem, $p \in \mathbb{R}^{n_p}$ is the parameter vector and $F_1$ and $F_2$ describe two types of constraints handled with different methods in the algorithm.