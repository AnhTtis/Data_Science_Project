
\section{Method}

\subsection{Preliminaries}

In this paper, we adopt volume rendering to obtain the depth map for the model training. In the novel view synthesis task \cite{nerf, v4d}, researchers usually use a multilayer perceptron network (MLP) to learn a mapping function to map the 3D position $(x,y,z)$ and view direction $(\theta, \phi)$ into the volume density $\sigma$ and the RGB color $c$. For rendering the RGB value in a pixel, we can use the approximate volume rendering in NeRF \cite{nerf} to do the weighted sum on the sampled point on the ray. The rendering function is defined in the equation (\ref{rendering}): 
\begin{equation}
    \hat{c} = \sum_{i=1}^{W}{T_{i}(1-exp({-\sigma _{i}}{\delta _{i}}  )  ) c_{i} }, 
     \label{rendering}
\end{equation} 
where $\hat{c}$ is the rendered RGB value, ${T_{i} = exp\left ( -\sum_{j=1}^{i-1} {\sigma _{j}}{\delta _{j}}  \right ) }, {\delta _{i}} = t_{i+1} - t_{i}$ is the adjacent sampled points' distance, $i$ is the sampled point along the ray, and $W$ is the number of the sampled points. If we want to obtain the depth information of the related pixel, we can replace the RGB value in the equation (\ref{rendering}) with the sampled point's distance $t_{i}$ as shown in equation (\ref{weight}). In this way, we can use the ground truth depth map to do the supervision training. 
 \begin{equation}
    % \vspace{-1.0em}
    \hat{d} = \sum_{i=1}^{W}{T_{i}(1-exp({-\sigma _{i}}{\delta _{i}}  ) )t_{i} }.
     \label{weight}
\end{equation} 
The NeRF's model learns geometry by the multi-view constraint while, in our setting, the geometry is from the image that is achieved by the CNN model, as introduced below.

\subsection{Model design (SimpleOccupancy)} \label{3.2}

The problem setting of this work is defined in Figure \ref{fig:task}. Given the surrounding-view images with the relative camera pose to the vehicle framework, we design an end-to-end neural network $Q$ to predict the 3D occupancy map, where we formulate it as  $Q$: $(I^1,I^2,I^3,..., I^n) \rightarrow V^{x \times y \times z}$, where $n$ is the number of the surrounding images and $x,y,z$ represents the resolution of the final output of the voxel. We present the overview of the proposed framework in Figure \ref{fig:Overview}, and give the details as follows.


%总体描述问题设置，以及框架

\textbf{Encoder} For the image feature extraction, we use the ResNet \cite{resnet} as the backbone. We provide the experiment on ResNet 50 and 101 with the pre-trained model from ImageNet \cite{pytorch, krizhevsky2012imagenet}. The final feature map is with the shape $C \times H/4 \times W/4 $, where $H$ and $W$ are the input resolution of the image and $C=64$ is the channel number. 

\textbf{From the image feature to 3D volume} For both the BEV perception and 3D occupancy estimation, a critical step is to transform the image feature from the 2D image space to the 3D volume space. In this work, we adopt the simplest manner as used in the Simple-BEV \cite{simplebev}. Specifically, we define a set of 3D points and back-project the 3D points to the 2D image feature planes and do the sampling by the bilinear interpolation. For the overlap region of the adjacent camera, we do the mean average for the sampled feature. Besides, we also investigate the other two 2D to 3D transformations, LSS \cite{lss} and Query \cite{wei2023surroundocc}, but we do not observe the benefit of using them as shown in the experiment section.


\textbf{3D volume space learning} The parameter-free transformation by the bilinear interpolation does not have any position prior, which means that the feature on the rays of the frustum is identical. Therefore, it is a highly ill-posed setting to infer the 3D geometry from the surrounding-view images with only a little overlap. The ill-posed setting requires stronger feature aggregation ability to achieve the 3D occupancy prediction. For the 3D feature learning, we adapt the 3D convolution network based on the hourglass structure from HybridNet \cite{HybirdNet} in the stereo-matching task. Differently, we discard the multiple 3D volume output from the stacked hourglass architecture because we do not find the performance improvement with the default architecture, and rendering for the multiple depth maps also needs more computational resources. The detailed network structure is placed in the supplementary material.  

\textbf{Occupancy probability} After the 3D volume space feature aggregation, we have the final voxel $V^{x \times y \times z}$. The occupancy probability value is from the Sigmoid function (\ref{sigomid}). 
% can use the volume rendering to render the final output $V^{x \times y \times z}$ into the depth map for supervision training, where the final voxel could have two representations, the density and the occupancy probability. For the density, it is the same as the NeRF model in that the larger value means the higher density of the scene. But the density is not normalized between 0 and 1, and in practice, it is uneasy to select the threshold to determine whether the voxel grid is being occupied or not. Therefore, to obtain a better occupancy representation, we first use the Sigmoid function to obtain the probability value as formulated in equation (\ref{sigomid}), and then for rendering the depth map, we adopt the accumulation sum of the depth ray as done in \cite{VoxelNet}. 
\begin{equation}
    Probability = Sigmoid(\sigma).
     \label{sigomid}
\end{equation} 


\subsection{Model evaluation} \label{3.3}

\begin{figure}[t!]
\centering
{
\includegraphics[width=0.48\textwidth]{fig/metric.png}
}
\caption{\textbf{We use a collection of key points to represent the 3D space and evaluate it based on sample points.} The classification label is shown on upper side while the bottom side compares two metrics - the classification metric and our newly proposed discrete depth metric - in two different prediction cases. Note that the unknown point is not involved in classification metric but in the proposed discrete depth metric. It is evident that the discrete depth metric accurately reflects the cost associated with each prediction.}
\label{fig:label}
\end{figure}

To the best of our knowledge, our study represents pioneering research in the field of surrounding-view 3D occupancy estimation and concurrent evaluation. As a result, we find it imperative to explore suitable metrics for assessing the performance of our model, particularly considering the available datasets - Nuscenes and DDAD datasets. Our approach aims to leverage existing datasets for 3D occupancy estimation and evaluation in a manner similar to depth estimation - with simplicity and efficiency in mind. To achieve this goal, we have identified key factors and essential considerations that should be taken into account during the evaluation process:
(1) the current available outdoor dataset with the point cloud as the ground truth label is sparse, especially for the far-end space. Without huge efforts of human labeling, we can not obtain the dense voxel label as in \cite{semantickitti}. Therefore, we can not evaluate the whole voxel space and could only do the evaluation on the known space. (2) About the known space, we can only determine the space between the lidar center and the point cloud. (3) The 3D occupancy and voxel representation is a discrete representation, which means the quantization error is unavoidable, but we can determine the affordable quantization error in the autonomous driving scenario. (4) The evaluation should be feasible for the common datasets and easily be conducted for the study. Therefore, given the aforementioned considerations, we examine two evaluation metrics in this study: the classification metric and the discrete depth metric. These metrics are also associated with two training 
fashions in the ablation study as shown in Figure \ref{fig:label}. 

 
\textbf{Occupancy label generation} Inspired by NeRF \cite{nerf}, we use the stratified sampling strategy to set the label for the free space. First, we set Lidar's position as the origin of all the rays for all the point clouds. Different from the even stride sampling, we use a fixed number of sample points (30), which means that, for closer point clouds, the sampling space is denser, and for far-end point clouds, the sampling space is sparser. The used sampling strategy has advantages in that it allows for a more balanced distribution of positive and negative labels and maintains a higher standard for closer objects in the driving scenes. The occupied space is represented by point cloud voxelization with a resolution of 0.4 m. The above operation can be easily performed using the Open3D \cite{open3d} library. 

\textbf{Classification metric} By representing the known space by a set of key points, we can perform the evaluation as the classification task with binary classification metrics, as shown in Figure \ref{fig:label}. The classification metrics are commonly used in 3D semantic scene completion tasks \cite{occupancysurvey, monoscene, voxformer}. However, we observe that classification metrics are not perfect since they can only evaluate the known space in our setting. We give two cases of evaluation along a ray in Figure \ref{fig:label}. We can see that classification metric is not sensitive to case 1. Even though the first predicted occupancy point is far away from the ground truth occupancy (lidar point), classification metric still gives a high score. Conversely, in case 2, if the network is unable to predict the first occupancy point in known space, all of the classification metrics produce a low score except for the accuracy metric. This is unfair if the first predicted occupancy point is next to the actual occupancy point. For a more detailed explanation, please refer to the supplementary material.

\textbf{Discrete depth metric} Recognizing the limitations of classification metric, we introduce discrete depth metric, which provides a more accurate assessment of predictions. Our approach involves dense sampling evaluation points along the ray with an interval (0.2 m) and a maximum distance (52 m). If all the prediction along the ray is empty, we set the last point as the first predicted occupancy point.
The discrete depth error is then calculated as the distance between the first predicted occupancy point and point cloud along the ray. By utilizing this criterion, we can perform evaluations in a manner similar to depth map error assessments. Following depth estimation \cite{surrounddepth}, we report occupancy evaluation with the following metrics, including error metric, (Abs Rel, Sq Rel, RMSE, RMSE log) and accuracy metric, $\delta<t: \% \text { of } d \text { s.t. } \max \left(\frac{\hat{d}}{d^*}, \frac{d^*}{\hat{d}}\right)=\delta<t$. The detailed definition is presented in supplementary material.


\subsection{Model optimization} \label{3.4}

Based on the available depth map and the generated occupancy label as introduced before, we investigate two different training manners. The first one is depth loss, the depth map supervision by the volume rendering. The other one directly calculates the binary classification loss based on the obtained label in the known space, where we consider binary cross entropy loss and L1 loss.

\textbf{Depth loss} With the depth map from the volume rendering, we can train the network the same as the depth estimation task. Following \cite{newcrfs}, we use the Scale-Invariant Logarithmic (SILog) loss \cite{eigen2014depth} to supervise the training. Specifically, the depth map loss is defined as: 
\begin{equation}
\mathcal{L}_{depth}=\alpha \sqrt{\frac{1}{M} \sum_i \Delta d_i^2-\frac{\lambda}{M^2}\left(\sum_i \Delta d_i\right)^2},
\end{equation}
where $\Delta d_i=\log \hat{d}_i-\log d_i^*$, $d_i^*$ is the ground truth depth and $\hat{d}_i$ is the predicted depth. $M$ is the number of valid pixels. We set $\lambda = 0.85$ and $\alpha = 10$ the same as \cite{newcrfs}.


\textbf{Classification loss} Following the common practice, we use the binary cross entropy (BCE) loss function to train the model as follows: 
\begin{equation}
\mathcal{L}_{BCE}=-\frac{1}{N} \sum_{i=1}^N y_i \cdot \log \left(\hat{y_i}\right)+\left(1-y_i\right) \cdot \log \left(1-\hat{y_i}\right),
\end{equation}
where $y_i$ is the ground truth label and $\hat{y_i}$ is the binary probability prediction. Note that we used to try the combination of the BCE loss and Dice loss \cite{dice} to balance the occupied and empty labels, but we did not observe the improvement. Besides, we also investigate to use of the L1 loss direct work on sampled points as below: 
\begin{equation}
\mathcal{L}_{L1}=\frac{1}{N}\sum_{i=1}^{N}L_{1}\left ( 1-p_{i}  \right ) + \frac{1}{K} \omega \sum_{j=1}^{K}L_{1}\left ( 0-p_{j}  \right ),        
\end{equation}
where $p_{i}$ is the probability value based on the point cloud position, and $p_{j}$ is the probability value from the sampled point in the empty space. $N$ is the number of the valid point cloud and $K$ is the number of sampled points in the empty space. $\omega = 5 $ is the hyperparameter for balancing occupied and empty labels with the search range from 1 to 10.0.

