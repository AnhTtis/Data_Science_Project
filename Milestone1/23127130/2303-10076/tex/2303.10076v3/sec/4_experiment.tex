

%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.76}{
% \resizebox{0.78\hsize}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{14}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Discrete depth metric}
\tabularnewline
\hline

 & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   & Res101  &  LSS \cite{lss} &  Query \cite{wei2023surroundocc} & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log  $\downarrow$ &  $\delta<1.25$ $\uparrow$ & $\delta<1.25^2$ $\uparrow$ & $\delta<1.25^3$ $\uparrow$
\tabularnewline
\hline

(1) & \Checkmark &  & & & & & 0.208 &	2.684	& 9.510 &	0.339 &	\underline{0.678}	& \underline{0.874}	& \underline{0.938}


\tabularnewline
% \hline

(2) & & \Checkmark & & & & & 0.221 & 3.204 & 10.283	&  0.396 &	0.668 &	0.849 &	0.914


\tabularnewline
% \hline

(3) & & & \Checkmark & & & & 0.225	& 3.289 & 10.461 &	0.409 &	0.668 &	0.845 &	0.911


\tabularnewline

(4) & \Checkmark & \Checkmark & & & & & 0.208	 &  \underline{2.654}  & 	9.769 & 	0.349 & 	0.653 & 	0.862	 & 0.933


\tabularnewline

(5) & \Checkmark & & \Checkmark & & & &  0.210 & 2.781 & 9.994 &	0.362 &		0.654	&	0.855 &		0.924



\tabularnewline
\hline

(6) & \Checkmark & & & \Checkmark & & &  \underline{0.206}	& \textbf{2.597} & \textbf{9.353}	& \textbf{0.329}	& \textbf{0.685} &	\textbf{0.878} &	\textbf{0.942}


\tabularnewline

(7) & \Checkmark & & & \Checkmark $^*$ &  &  & 0.265 &	3.933 &	10.356 &	0.369 &	0.620&	0.840&	0.923


\tabularnewline

(8)& \Checkmark & & & \Checkmark & \Checkmark & & 	\textbf{0.205}	& \textbf{2.597}	& \underline{9.470}	& \underline{0.331}	& 0.676	& 0.872 &	\underline{0.938}


\tabularnewline
% \hline

(9)& \Checkmark & & & \Checkmark & &\Checkmark &  0.208 & 	2.683 &  9.647	&  0.343 &  0.673	&  0.869	& 0.935

\tabularnewline
\hline
-- &  \Checkmark &  \multicolumn{5}{c|}{SurroundOcc \cite{wei2023surroundocc}}  & 0.209	& 2.748 &	9.455 &	0.337 &	0.687	& 0.873	& 0.937



\tabularnewline
\hline

 \multicolumn{14}{c}{Nuscenes \cite{nuscenes}}  
 \tabularnewline
\hline

-- &  \Checkmark &  \multicolumn{5}{c|}{SurroundOcc \cite{wei2023surroundocc}} & 0.361	& 6.089	& 9.726	& 0.391	& 0.585	& 0.820	& 0.923

\tabularnewline
\hline

(6) & \Checkmark & & & \Checkmark & &  & 0.205 &	2.044 &	6.495 &	0.289 &	0.718 &	0.922 &	0.966


\tabularnewline
\hline

\end{tabular}
}
\vspace{0.2cm}
\caption{The ablation study of the proposed method in the proposed discrete depth metric for occupancy evaluation. Experiment (7) means do not use the pretrained model from ImageNet. Experiment (6) is the optimal setting. $\uparrow$ means the value higher is better and $\downarrow$  means lower is better. The number with bold typeface means the best and the number with the underline is the second. }
\label{t:3D occupancy}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{4}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{1}{c}{Experiment setting}  &  \multicolumn{3}{|c}{Depth metric}
\tabularnewline
\hline

    & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & $\delta<1.25$ $\uparrow$ 
\tabularnewline
\hline

(1)  & 0.149 &	\underline{1.001} &	\underline{0.798}

\tabularnewline
% \hline

(2) &  0.175	& 1.725	&	0.763


\tabularnewline
% \hline

(3) &  0.168&	1.570	&0.735


\tabularnewline


(4)  &  0.150 &	1.028	&	0.790


\tabularnewline
% \hline

(5) & 	  0.151 &	1.056	&0.790 


\tabularnewline
\hline

(6) &  	\textbf{0.147} &	\textbf{0.996} &	\textbf{0.801}



\tabularnewline
% \hline


(7) & 	0.198	& 1.555	& 0.709

\tabularnewline
% \hline


(8) & 	\underline{0.148} &	1.013	& 0.797

\tabularnewline
% \hline


(9) & 	 0.150  &	1.008  &	\underline{0.798}

\tabularnewline
\hline
SurroundOcc \cite{wei2023surroundocc}  & 0.152	 & 1.022	 & 0.797


\tabularnewline
\hline

 \multicolumn{4}{c}{Nuscenes \cite{nuscenes}}  
 \tabularnewline
\hline

SurroundOcc \cite{wei2023surroundocc} & 0.117	& 0.591	& 0.886

  \tabularnewline
\hline

 (6) & 	0.116  &	0.542	&	0.888


\tabularnewline
\hline



\end{tabular}
}
% \setlength{\abovecaptionskip}{2cm}
% \setlength{\belowcaptionskip}{2cm}
\vspace{0.3cm}
\caption{The ablation study of the proposed method in the depth metric. The full table is presented in the supplementary material.}
\label{t:depth metric}
\end{table}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%


\section{Experiment}
Currently, there is a lack of established baseline work for surrounding-view 3D occupancy estimation. Therefore, we present the first attempt at addressing this gap by evaluating our proposed framework on DDAD and Nuscenes datasets. Our experiments consist of two parts. First, we conduct a detailed ablation study on the proposed framework to investigate the characteristics of the loss function and network architecture. Second, as we can obtain the depth map through volume rendering, we establish a benchmark for depth metrics by comparing our results with those of monocular depth estimation methods under the supervision setting.


\subsection{Datasets}

% \textbf{DDAD} \cite{ddad} is a largescale dataset with dense ground-truth depth maps. Specifically, this dataset includes 12,650 training samples, which means that it has 75,900 images for six cameras. The validation set contains 3,950 samples (15,800 images). We only consider the distance up to 52m, which is a reasonable range referred from the 3D semantic completion task \cite{monoscene} and BEV perception task \cite{bevformer}. The DDAD dataset has a denser point cloud compared with the Nuscenes dataset, which could provide a more justice evaluation, so we conduct the ablation study experiment based on the DDAD dataset.

\textbf{DDAD} \cite{ddad} is a largescale dataset with dense ground-truth depth maps. Specifically, this dataset includes 12,650 training samples. The validation set contains 3,950 samples. We only consider the distance up to 52m, which is a reasonable range referred from the 3D semantic completion task \cite{monoscene} and BEV perception task \cite{bevformer}. The DDAD dataset has a denser point cloud compared with Nuscenes dataset, which could provide a more equitable evaluation, so we conduct an ablation study experiment based on the DDAD dataset.

\textbf{Nuscenes} \cite{nuscenes} has 1000 sequences of diverse scenes, where each sequence is approximately 20 seconds in duration. We split the training and testing set the same as the depth estimation task \cite{surrounddepth}, including 20096 samples for training and 6019 samples for validation. The distance is also limited to 52m for consideration.

For the above two datasets, we generate the occupancy and empty space label from the raw point cloud and use the projected depth map 
for training and testing. Specifically, we first define the $Z$, $Y$, and $X$ as depth, height, and width, respectively. For the similar perception range as SemanticKITTI \cite{semantickitti}, the valid point cloud range is set as $Z\in \left ( -52m, 52m \right )$, $Y \in \left ( 0m, 6m \right )$, and $X\in \left ( -52m, 52m \right )$ for the training and testing. With the final voxel resolution in $256\times256\times16$, the size of the resolutions of the grid is (0.41m, 0.41m, 0.38m). At last, we set the best threshold for obtaining the score of discrete depth metric from the search range from 0 to 1 with an interval of 0.05.

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{fig/result_new.jpg}
    % \includegraphics[width=1\textwidth]{figure/main.png}
    \caption{\textbf{The visualization for the ablation study of the proposed method (DDAD dataset \cite{ddad}).} The first row: the surrounding images and the rendered depth maps. For the second row: based on the occupancy label, we present the binary prediction, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The third row is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. The BCE loss, L1 loss, Depth loss, and Full model are related to the experiment setting (2), (3), (1), and (6) in Table \ref{t:3D occupancy} and \ref{t:depth metric}, respectively. Note that we omit the prediction under the 0.4 m for better visualization. Best viewed in color.}
    \label{fig:Result_ablation}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
Method & Occupancy & Depth & Abs Rel &  Sq Rel &  RMSE &  RMSE log &   $\delta<1.25$  &   $\delta<1.25^{2}$  &  $\delta<1.25^{3}$ & Inference time (s)
\tabularnewline

\hline
 \multicolumn{11}{c}{DDAD \cite{ddad}}  

\tabularnewline
\hline

Monodepth2 \cite{monodepth2} &  &  \Checkmark &  0.148  &   1.003  &   4.722  &   0.219  &   0.795  &   0.935  &   0.973 &  \textbf{0.029}

\tabularnewline
\hline

New-CRFs \cite{newcrfs} &  &  \Checkmark &  \textbf{0.130}  &  \textbf{0.824}  &   \textbf{4.169}  &   \textbf{0.195}  &   \textbf{0.840}  &   \textbf{0.951}  &   \textbf{0.979} & 0.065

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} &  &  \Checkmark &   0.151  &   1.021  &   4.766  &   0.221  &   0.789  &   0.935  &   0.974 & 0.099

\tabularnewline
\hline

SurroundOcc \cite{wei2023surroundocc} & \Checkmark &  \Checkmark &  0.152	 & 1.022	 & 4.668	 & 0.220	 & 0.797	 & 0.933	 & 0.972 & 0.305

\tabularnewline
\hline

Ours & \Checkmark &  \Checkmark &  0.147&	0.996	&4.738	& 0.220	& 0.801	& 0.935	& 0.972 & 0.230



\tabularnewline

\hline
 \multicolumn{11}{c}{Nuscenes \cite{nuscenes}}  
\tabularnewline
\hline

Monodepth2 \cite{monodepth2} &  &  \Checkmark &  0.131  &   0.728  &   3.479  &   0.201  &   0.845  &   0.942  &   0.974 & \textbf{0.024}

\tabularnewline
\hline

New-CRFs \cite{newcrfs} &  &  \Checkmark & \textbf{0.115}  &  0.618  &   3.193  &  0.185  &   0.872  &   0.951  &  0.977  &  0.058

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} &  &  \Checkmark &  0.128  &   0.760  &   3.442  &   0.198  &   0.856  &   0.946  &   0.975 & 0.102

\tabularnewline
\hline

SurroundOcc \cite{wei2023surroundocc} & \Checkmark &  \Checkmark & 0.117 & 0.591	& \textbf{2.623}	& 0.180	& 0.886	& 0.950	& 0.975 & 0.305

\tabularnewline
\hline

Ours & \Checkmark &  \Checkmark & 0.116	& \textbf{0.542}	&  2.784  & \textbf{0.179}  &	\textbf{0.888} & \textbf{0.955} &	\textbf{0.978} & 0.196

% 0.116 &	0.542	& 2.784 &	0.179 &	0.888 &	0.955 &	0.978

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.3cm}
\caption{The benchmark in depth map metric with monocular depth estimation methods. The number with bold typeface means the best.}
\label{t:depth2}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation detail}
Our approach is implemented with Pytorch \cite{pytorch}. We resize the RGB image into $336\times672$ before putting them into the neural network. The depth map is rendered with the resolution $224\times352$. We use the Adam optimizer \cite{adam} with $\beta 1 = 0.9$ and $\beta 2 = 0.999$. The learning rate of the 2D CNN is set as $1e-4$ and the 3D CNN is $1e-3$. We train the networks with 12 epochs and do the learning rate decay with 0.1 at the 10th epoch. All the experiments have been conducted on the NVIDIA A 100 (40 GB) GPU.

\subsection{The ablation study for the proposed framework}


We present the ablation study result of the proposed method in Table \ref{t:3D occupancy} (discrete depth metric) and Table \ref{t:depth metric} (depth metric). We mainly investigate the characteristics of the different loss functions and the network design as follows:


\textbf{Loss functions analysis} First, we conduct the experiments (1-5) with different loss functions based on the lighter encoder, Resnet 50. We can see that the depth loss outperforms the classification loss for both the discrete depth metric and the depth metric in Table \ref{t:3D occupancy} and Table \ref{t:depth metric}. Accompanying with Figure \ref{fig:Result_ablation} marked with white circles in the depth map and the dense occupancy map, we can observe that the classification losses easily produce the floater in the region close to the sky. We conclude that the training with classification loss could not handle these regions behind the point cloud (unknown region). Reversely, the depth loss could largely prevent this situation because the rendering fashion cloud implicitly optimizes these regions with the sampled points along the whole ray. On the other hand, the depth loss from rendering more easily produced the long tail false positive prediction, as marked with the green circles. The long tail false positive prediction usually happens at the foreground and background intersection, which is not in the concerned driving region. We further investigate the combination of depth loss and classification loss in experiments (4) and (5). However, the combined loss is a little worse than the depth loss. It may own that the classification loss hurts the prediction in the sky region, as analyzed above.

In terms of the metric design, the prediction case 2 in Figure \ref{fig:label} usually happens as marked with yellow circles, where the network fails to predict the first occupancy at the end of the point cloud, but in the real situation, the first occupied point is very close to the point cloud from the visualization of the dense occupancy. Therefore, the proposed discrete depth metric could have a more justice assessment for the 3D occupancy estimation. 


\textbf{Network design} Building upon the depth loss, we delve into the investigation of network design. Note that in this section, our focus is not on introducing a highly specific network component but rather on exploring the performance of existing modules within the context of BEV perception under our proposed framework. At first, we try with a larger encoder by replacing ResNet 50 with ResNet 101, and the model performs better. Note that, in the experiment (7), the model gets a severe performance drop without initializing the model pre-trained from on ImageNet, which hints that finding a better pretrain strategy associated with 3D occupancy estimation may further boost the performance. The used parameter-free interpolation to recover 3D feature space is the most straightforward and efficient manner. In experiment (8), we also try to use the unprojection manner proposed in LSS \cite{lss}, which estimates the depth distribution first, and then forms the 3D volume with the weighted feature based on the distribution information. However, we overlooked the benefit of LSS's unprojection manner on the 3D occupancy estimation task, which gives us the information that doing the depth distribution estimation may cause a larger learning space for the following 3D CNN feature aggregation module, especially under the imperfect depth distribution estimation. Similarly, we have not observed the performance gain with the try on the Query method \cite{wei2023surroundocc}, which is based on the transformer with cross attention. 

% \textbf{SurroundOcc \cite{wei2023surroundocc}} We also implement the depth loss to the concurrent work SurroundOcc, where our network is performing better than it for both the performance (Table \ref{t:3D occupancy}) and computational cost (Table \ref{t:depth2}).

\textbf{Nuscenes} We implement the optimal experiment setting (6) to Nuscenes dataset. The visualization result and analysis are placed in the supplement due to limited space.

% In a brief summary, from Table \ref{t:3D occupancy} and Table \ref{t:depth metric}, we can conclude that the proposed discrete depth metric for 3D occupancy evaluation is related to the depth metric. In general, with better depth map results and the 3D occupancy prediction is also better. From Figure \ref{fig:Result_ablation}, we can see that the model could predict reasonable layout of the driving scenes, but some details are still missing, and the far-end prediction is still not good. We discuss the improvement in future work. 


\subsection{The benchmark for the depth estimation}
In the surrounding-view setting, we build a new benchmark in terms of the depth map metric for comparison with the monocular depth estimation methods. Monodepth2 \cite{monodepth2} is a well-recognized self-supervision monocular depth estimation method and New-CRFs \cite{newcrfs} is the state-of-the-art supervision method. Surrounddepth \cite{surrounddepth} is a self-supervision work that has been introduced in the related work. For the supervision setting in this work, we train the above three monocular depth estimation methods with the same loss function used in \cite{newcrfs}. 

From Table \ref{t:depth2}, we can observe that our method is competitive with the monocular depth estimation methods. For the DDAD dataset, New-CRFs \cite{newcrfs} achieves the best result, matching its performance in single image depth estimation. The performance for Monodepth2 \cite{monodepth2}, Surrounddepth \cite{surrounddepth}, and ours is similar.  For the Nuscenes dataset, our method achieves the best result. The visualization result is presented in the supplementary material.  

The disadvantage of our method is that the inference time is higher than the monocular depth estimation methods. We further analyze the inference time for each component of the system. For the DDAD dataset: 2D CNN (31) + 2D to 3D interpolation (3) + 3D CNN (101) + Rendering (95) = 230 (ms). We learn that the 3D feature aggregation and the rendering of the depth map occupied the main time. Note that Rendering is for depth maps and is ignorable if we only want the occupancy estimation.


\subsection{Discussion}
\textbf{Concurrent work} The motivation behind our approach to 3D occupancy estimation aligns with concurrent works \cite{wei2023surroundocc,wang2023openoccupancy, tong2023scene} in the field. Among them, we select SurroundOcc \cite{wei2023surroundocc} as a representative for discussion. First, We implement the depth loss to SurroundOcc, where our network is performing better than it for both the performance (Table \ref{t:3D occupancy}) and computational cost (Table \ref{t:depth2}). Second, as depicted in Figure \ref{fig:label}, our method adopts a point-level training approach, while the concurrent works employ voxel-level training. This distinction allows us to achieve a finer level of granularity in our predictions compared to theirs. To explore the potential benefits of our strategy for concurrent work, we conduct an experiment using our approach as a pretraining step. Specifically, we train SurroundOcc on point-level semantic labels generated by our sampling strategy, and then we fine-tune the network using voxel-level semantic labels from SurroundOcc. More implementation details are placed in the supplement. The results in Table \ref{t:pretrain} demonstrate that our point-level pretraining effectively improves the network's performance. 

\textbf{Limitation and future work} We place this part in supplement material due to the limited space.



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{c|c|c|c}

% \hline
%      \multicolumn{4}{c}{SurroundOcc \cite{wei2023surroundocc}}  
% \tabularnewline
\hline
 Method & Experiment setting  &  SC IoU  &  SSC IoU
\tabularnewline
\hline

 \multirow{3}*{SurroundOcc \cite{wei2023surroundocc}} & Original   & 31.49	 & 20.30

\tabularnewline

~ & W/O pretrain   &	31.29   & 20.08

\tabularnewline

~ & W pretrain   &	\textbf{32.55}  &  \textbf{20.85}



\tabularnewline
\hline

\end{tabular}
}

\vspace{0.3cm}
\caption{The ablation study on SurroundOcc with point level pretrain. Original is from the original paper. W/O pretrain represents our reimplement result. W/ pretrain means using our point level pretrain. The number with bold typeface means the best.}

\label{t:pretrain}
\end{table}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%



% \section{Limitation and future work}
% In this simple attempt for 3D occupancy estimation, we still do not introduce sequence information as did in \cite{tesla} and the BEV perception tasks \cite{bevformer, liu2022petrv2}. It is a promising direction to improve performance by fusing the sequence information. Besides, the current voxel resolution is relatively coarse with 0.4 m, which is a good beginning for the research purpose due to limited computation resources. In the future, we will explore a higher resolution such as 0.2 m \cite{monoscene}. 


\section{Conclusion}
In this paper, we give a simple attempt for 3D occupancy estimation in the surrounding-view setting for autonomous driving. We demonstrated the effectiveness of the entire pipeline through novel network design, loss function investigation, and model evaluation. We hope this simple and effective work will inspire followers, and we will release the code to encourage further research in this field.  
