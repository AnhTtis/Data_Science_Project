\clearpage
\setcounter{page}{1}
\maketitlesupplementary


% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.

\begin{abstract}
In this supplementary material, we provide more implementation details, experiment results, analysis, and further discussion on the limitation and future work.
\end{abstract}



%%%%%%%%% BODY TEXT
\section{More implementation details}

\textbf{The detailed architecture of the 3D CNN.} For the volume feature aggregation, we adapt the 3D CNN from the HybirdNet \cite{HybirdNet} as shown in Figure \ref{fig:3DCNN}. The difference is that we discard the multiple 3D volume output from the stacked hourglass architecture because we do not find the performance improvement with the default architecture, and rendering for the multiple depth maps also need more computational resource. Please find the attached code for a more detailed design of the 3D CNN. 

\textbf{Depth map benchmark.} To build up the depth map benchmark, we implement the same training strategy for Monodepth2 \cite{monodepth2}, New-CRFs \cite{newcrfs}, and Surrounddepth \cite{surrounddepth}. Specifically, we train the networks with 15 epochs and do the learning rate decay with 0.1 at the 12th epoch for the DDAD dataset. For the Nuscenes dataset, we train the networks with 10 epochs and do the learning rate decay with 0.1 at the 8th epoch as this dataset is faster to converge, which is also observed in Surrounddepth \cite{surrounddepth}. 

\textbf{The detailed implementation on SurroundOcc \cite{wei2023surroundocc}.}
For our benchmark (Table 1 and Table 2 in the main paper), we made the following adjustments:
1. We set the final channel to 1 instead of 18, as our focus is on pure geometry prediction without considering semantic information.
2. To ensure a fair comparison, we set the final output size to $256\times256\times16$, which is the same as ours.
3. We used the same learning rate as ours, as we found that the original learning rate in the reference paper led to inferior results.

Regarding the pretrain experiment (Table 4 in the main paper), we followed these steps:
1. We generated point-level labels by sampling from the point cloud with semantic labels provided from the dataset used by SurroundOCC \cite{fong2022panoptic}. The sampling process is depicted in Figure 3 in the main paper, where the sampled points (except the point cloud itself) are assigned to the empty category.
2. We performed classification training based on the generated point-level labels for the pretraining phase.
3. Once the pretraining was completed, we finetuned the network using dense voxel labels to obtain the final scores.


\textbf{Detailed definition of the classification metric.} By representing the scenes with a set of labeled key points, we can use the classification metric to evaluate the model, where we define the prediction with: True Positive $(TP)$, False Positive $(FP)$, True Negative $(TN)$, False Negative $(FN)$. Following the common practice, we report the F1 score $((2\times P \times R)/(P+R))$, Precision $(P = TP/(TP+FP))$, Recall $(R=TP/(TP+FN))$ and Accuracy $((TP+TN)/(TP+FP+TN+FN))$. The IOU (Intersection-Over-Union) metric is calculated from the average IOU $(TP / (TP + FN + FP))$ of each class.


\textbf{Detailed definition of (discrete) depth map metric.} Following the depth estimation task \cite{surrounddepth}, we report the (discrete) depth map evaluation with the following metrics, 
\begin{equation}
\begin{split}
\text {Abs Rel:} \frac{1}{|M|} \sum_{d \in M}\left|\hat{d}-d^*\right| / d^* , \\ 
\text {Sq Rel:} \frac{1}{|M|} \sum_{d \in M}\left\|\hat{d}-d^*\right\|^2 / d^*, \\
\text { RMSE: } \sqrt{\frac{1}{|M|} \sum_{d \in M}|| \hat{d}-d^* \|^2}, \\
\text { RMSE log: } \sqrt{\frac{1}{|M|} \sum_{d \in M}|| \log \hat{d}-\log d^* \|^2} , \\
\delta<t: \% \text { of } d \text { s.t. } \max \left(\frac{\hat{d}}{d^*}, \frac{d^*}{\hat{d}}\right)=\delta<t,
\end{split}
\end{equation}
where the definition of symbols is identical to that of the main paper. 



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{l|c|c|c|c|c|c|c}

\hline
 \multicolumn{8}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{2}{c}{Experiment setting}  &  \multicolumn{3}{|c}{Discrete depth metric}  & \multicolumn{3}{|c}{Depth metric}
\tabularnewline
\hline

    &  & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & $\delta<1.25$ $\uparrow$ & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & $\delta<1.25$ $\uparrow$ 
\tabularnewline
\hline

\multirow{2}*{Two channels} & $\mathcal{L}_{BCE}$  & 0.375 & 8.327 & 0.628  & 0.172 & 1.658 & 0.723

\tabularnewline
~ & $\mathcal{L}_{depth}$  & 0.295 &	5.422 & 0.629 &  \textbf{0.151}  & 1.055 & 0.790


\tabularnewline
\hline
 \multirow{2}*{Single channel} & $\mathcal{L}_{BCE} (\omega = 0.5)$ & 0.379 & 6.383 & 0.626 &  0.176 & 1.699 & 0.710 

\tabularnewline

~ & $\mathcal{L}_{depth} (\omega = 0.5)$  &  0.305	 &  5.629	& 0.617 &  0.152 &  \textbf{1.034} & \textbf{0.795}

\tabularnewline
\hline

 \multirow{2}*{Single channel}  & $\mathcal{L}_{BCE} (\omega = 0.2)$ & 	0.235 &	3.395  & 0.647 & 0.176 & 1.699 & 0.710 

\tabularnewline

~ & $\mathcal{L}_{depth} (\omega = 0.05)$  &  \textbf{0.210} &	\textbf{2.724} & \textbf{0.674}  & 0.152 &  \textbf{1.034} & \textbf{0.795}


\tabularnewline
\hline



\end{tabular}
}
% \setlength{\abovecaptionskip}{2cm}
% \setlength{\belowcaptionskip}{2cm}
\vspace{0.3cm}
\caption{The comparison of the output channel for the final occupancy prediction. $\omega$ is the threshold for the occupancy prediction, where the probability value is larger than the $\omega$ would be regarded as occupancy. For the single-channel setting, different thresholds have different occupancy predictions, resulting in different discrete depth values. The depth metric is identical to different thresholds since it is not determined by a certain threshold. The number with bold typeface means the best.}
\label{t:output channel}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%





\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{fig/3DCNN_new.png}
    \caption{\textbf{The architecture of the proposed 3D CNN feature aggregation module.}}
    \label{fig:3DCNN}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{fig/channel.jpg}
    \caption{\textbf{The comparison of the output channel for the final occupancy prediction.} The upper part is the comparison of binary cross entropy (BCE) loss. The lower part is the comparison of the depth loss. }
    \label{fig:channelcomparison}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.76}{
% \resizebox{0.78\hsize}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{14}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Depth metric}
\tabularnewline
\hline

 & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   & Res101  &  LSS \cite{lss} &  Query \cite{wei2023surroundocc} & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log  $\downarrow$ &  $\delta<1.25$ $\uparrow$ & $\delta<1.25^2$ $\uparrow$ & $\delta<1.25^3$ $\uparrow$
\tabularnewline
\hline

(1) & \Checkmark &  & & & & & 0.149 &	\underline{1.001} &	4.782 &	\underline{0.222} &	\underline{0.798} &	\underline{0.934} &	\underline{0.972}



\tabularnewline
% \hline

(2) & & \Checkmark & & & & & 0.175	&1.725	&5.954&	0.289	&0.763&	0.903	&0.949



\tabularnewline
% \hline

(3) & & & \Checkmark & & & & 0.168&	1.570	&6.580	&0.310	&0.735&	0.882&	0.936



\tabularnewline

(4) & \Checkmark & \Checkmark & & & & & 0.150 &	1.028	&4.868&	0.225	&0.790 &	0.931 &	0.971



\tabularnewline

(5) & \Checkmark & & \Checkmark & & & &  0.151&	1.056	&4.937	&0.230 &	0.790 &	0.929&	0.970




\tabularnewline
\hline

(6) & \Checkmark & & & \Checkmark & & &  \textbf{0.147} &	\textbf{0.996} &	\underline{4.738}  &	\textbf{0.220} &	\textbf{0.801}	& \textbf{0.935} &	\underline{0.972}



\tabularnewline

(7) & \Checkmark & & & \Checkmark $^*$ & & & 0.198	& 1.555	& 5.761	& 0.275	& 0.709	& 0.888	& 0.951


\tabularnewline

(8)& \Checkmark & & & \Checkmark & \Checkmark & & 	\underline{0.148} &	1.013 &	4.797	& \underline{0.222}	& 0.797 &	\underline{0.934} &	\underline{0.972}



\tabularnewline
% \hline

(9)& \Checkmark & & & \Checkmark & &\Checkmark &  0.150 &	1.008 &	\textbf{4.737} &	\textbf{0.220} &	\underline{0.798} &	\textbf{0.935} &	\textbf{0.973}


\tabularnewline
\hline
-- &  \Checkmark &  \multicolumn{5}{c|}{SurroundOcc \cite{wei2023surroundocc}}  & 0.152	 & 1.022	 & 4.668	 & 0.220	 & 0.797	 & 0.933	 & 0.972


\tabularnewline
\hline

 \multicolumn{14}{c}{Nuscenes \cite{nuscenes}}  
 \tabularnewline
\hline

-- &  \Checkmark &  \multicolumn{5}{c|}{SurroundOcc \cite{wei2023surroundocc}} & 0.117	& 0.591	& 2.623	& 0.180	& 0.886	& 0.950	& 0.975

 \tabularnewline
\hline
 
(6) & \Checkmark & & & \Checkmark & & &  0.116 &	0.542	& 2.784 &	0.179 &	0.888 &	0.955 &	0.978

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.2cm}
\caption{The ablation study of the proposed method in the depth metric. Experiment (7) means do not use the pretrained model from ImageNet. Experiment (6) is the optimal setting. $\uparrow$ means the value higher is better and $\downarrow$  means lower is better. The number with bold typeface means the best, and the number with the underline is the second. }
\label{t:depth metric full}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%



\section{More experiment results}


\textbf{The comparison of the output channel for the final occupancy prediction.} In this work, the occupancy prediction can be obtained in two ways. The first approach (single channel), presented in the main paper, utilizes the Sigmoid function and a threshold to determine whether the sampled point is occupied. The second approach involves using two channels for the final output and applying Softmax and Argmax operations to obtain the occupancy prediction. Regarding the two channels setting, we can still render the depth map by selecting the value of the occupancy channel after the Softmax function. Our experiment found that the first approach achieves better results in the discrete depth metric and performs similarly in the depth metric, as indicated in Table \ref{t:output channel}. We observe the results are similar if set the single channel's threshold $\omega  = 0.5$ compared with the two-channel setting, which means the single-channel and two-channel settings are equivalent. Meanwhile, in the single channel setting, it would produce more false negative predictions with a higher threshold. Therefore, we could select a suitable threshold for better occupancy performance in discrete depth metrics. We visualize a scene in Figure \ref{fig:channelcomparison} for both BCE loss and depth loss for reference. Note that this experiment is from our relatively early experiment, and we slightly adjust the network, so the numerical result and visualization are slightly different from the main paper. It still gives you the information for the comparison of output channels.



\textbf{The full table for the ablation study of the proposed network in the depth metric.} We present the full table for the ablation study of the proposed network in the depth metric in Table \ref{t:depth metric full}. The conclusion is the same as in the main paper, which confirms the effectiveness of the proposed pipeline. 


\textbf{More visualization results for DDAD \cite{ddad} and Nuscenes \cite{nuscenes} datasets.} More 3D occupancy visualization results for DDAD dataset are presented in Figure \ref{fig:ddad}, and the results for Nuscenes are presented in Figure \ref{fig:nuscenes}. We can learn that the model could predict the detailed layout of the surrounding scenes. We hope this baseline work could inspire researchers to further develop this relatively new task. For the sequence prediction visualization, please find the attached GIF.



\textbf{Depth map visualization for the depth estimation benchmark.} The depth map visualization results for the DDAD dataset are shown in Figure \ref{fig:ddad_depth}, and Figure \ref{fig:nuscenes_depth} shows the depth map results for the Nuscenes dataset. Note that the depth map results in the Nuscenes dataset have some wave-like pattern due to the point cloud's sparseness. The result in the DDAD dataset does not have this problem with a more dense point cloud for training. The quantitative and qualitative results show that the depth map from volume rendering is competitive with the current well-developed monocular depth estimation methods. Besides, the chosen rendering pipeline can be easily extended to incorporate sequence fusion in the future. This would help address the current ill-posed problem setting of inferring 3D geometry from a single image, which is a limitation of current monocular depth estimation methods. 

\textbf{Analysis for the point-level training and the voxel-level training.}
For the purpose of pretraining, we employ a classification loss on the sampled points with corresponding semantic labels. However, it is important to note that this approach does not optimize the regions that lidar rays cannot pass through, such as the observable region and the area close to the sky. As shown in Figure \ref{fig:pretrain}, the outcome of training with sparse point-level labels is not optimal. In the upper part of the view, we observe that vegetation and manmade structures dominate, as these classes typically appear near the top. Consequently, the network tends to predict all regions close to the sky as either vegetation or manmade. Despite this limitation, the network demonstrates reasonable predictions for the remaining parts of the scene, which proves to be a valuable initialization for subsequent fine-tuning and leads to improved performance, as indicated in Table 4 in the main paper.


\textbf{Analysis for the Chamfer distance and the proposed discrete distance metric.}
The Chamfer distance is widely used for the evaluation in 3D reconstruction, which is based on finding the closest distance between the prediction and the ground truth (GT). As illustrated in Figure \ref{fig:chamfer}, we find that the chamfer distance is not a good option for the evaluation of autonomous driving. The closest voxel is easy to be the plane ground rather than the GT vehicle. Therefore, the proposed discrete depth metric is a better choice, which is an ego-centric evaluation. 


\section{Limitation and future work}
In this simple attempt for 3D occupancy estimation, we still do not introduce sequence information as did in \cite{tesla} and the BEV perception tasks \cite{bevformer, liu2022petrv2}. It is a promising direction to improve performance by fusing the sequence information. Besides, the current voxel resolution is relatively coarse with 0.4 m, which is a good beginning for the research purpose due to limited computation resources. In the future, we will explore a higher resolution, such as 0.2 m \cite{monoscene}. At last, it has been proved that the rendering pipeline is also applicable to the semantic information \cite{zhi2021place}. Therefore, it is worth trying to incorporate 2D semantic supervision for 3D semantic scene learning.






\begin{figure*}
    \centering
    \includegraphics[width=0.92\textwidth]{fig/ddad_new_1.jpg}
    \caption{\textbf{More 3D occupancy visualization results for DDAD dataset.} The result is under the full model (experiment setting (6)). For the third column, we present the binary prediction based on the generated labels, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The fourth column is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. Note that we omit the prediction under 0.4 m for better visualization. Best viewed in color.}
    \label{fig:ddad}
\end{figure*}


% \textbf{Visualization results for Nuscenes dataset} More 3D occupancy visualization results for Nuscenes dataset are presented in Figure \ref{fig:nuscenes}.

\begin{figure*}
    \centering
    \includegraphics[width=0.92\textwidth]{fig/nuscene_new_1.jpg}
   \caption{\textbf{The 3D occupancy visualization results for Nuscenes dataset.} The result is under the full model (experiment setting (6)). For the third column, we present the binary prediction based on the generated labels, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The fourth column is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. Note that we omit the prediction under 0.4 m for better visualization. Best viewed in color.}
    \label{fig:nuscenes}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.93\textwidth]{fig/ddad_depth_new_1.jpg}
    \caption{\textbf{The depth map visualization results for DDAD dataset.} For the camera's order in the surrounding images and the rendered depth maps, the first row is front left, front and front right, and the second row is back left, back, and back right. Best viewed in color.}
    \label{fig:ddad_depth}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{fig/nuscene_depth_new_1.jpg}
    \caption{\textbf{The depth map visualization results for Nuscenes dataset.} For the camera's order in the surrounding images and the rendered depth maps, the first row is front left, front and front right, and the second row is back left, back, and back right.  Best viewed in color.}
    \label{fig:nuscenes_depth}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{fig/Pretrain.png}
    \caption{\textbf{The comparison for point-level supervision by our sampling strategy and the dense voxel-level supervision from SurroundOcc \cite{wei2023surroundocc}.} }
    \label{fig:pretrain}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{fig/Chamfer.png}
    \caption{\textbf{The analysis for the Chamfer distance and the proposed discrete distance metric.} }
    \label{fig:chamfer}
\end{figure*}


