\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}

% 
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{makecell}
\usepackage{multirow}
% \usepackage{booktabs}
% \usepackage{marvosym}
% \usepackage{pifont}
% \usepackage{url}
\usepackage{bbding}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}

% Support for easy cross-referencing
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).



\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2903} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}


%%%%%%%%% TITLE
\title{A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving}


\author{\textbf{$\rm Wanshui \; Gan$} $^{1, 2}$ \\
\and
\textbf{$\rm Ningkai \; Mo$} $^{3}$ \\
\and
\textbf{$\rm Hongbin \; Xu$} $^{4} $ \\
\and
\textbf{$\rm Naoto \; Yokoya$} $^{1, 2}$ \\
\and
$\rm ^1 The \; University \; of \; Tokyo, ^2 RIKEN$ \\
$\rm ^3 Shenzhen \; Institute \; of \; Advanced \; Technology,\ Chinese \; Academy \; of \; Sciences$ \\
$\rm ^4 South \; China \; University \; of \; Technology$ \\
${ \rm \{wanshuigan, nk.mo19941001, hongbinxu1013\}@gmail.com,  yokoya@k.u-tokyo.ac.jp}$}


\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
% \footnotetext[1]{The first two authors contributed equally and should be regarded as co-first authors.}
% \footnotetext[2]{Corresponding author.}



% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
The task of estimating 3D occupancy from surrounding-view images is an exciting development in the field of autonomous driving, following the success of Bird's Eye View (BEV) perception. This task provides crucial 3D attributes of the driving environment, enhancing the overall understanding and perception of the surrounding space. However, there is still a lack of a baseline to define the task, such as network design, optimization, and evaluation. In this work, we present a simple attempt for 3D occupancy estimation, which is a CNN-based framework designed to reveal several key factors for 3D occupancy estimation. In addition, we explore the relationship between 3D occupancy estimation and other related tasks, such as monocular depth estimation, stereo matching, and BEV perception (3D object detection and map segmentation), which could advance the study on 3D occupancy estimation. For evaluation, we propose a simple sampling strategy to define the metric for occupancy evaluation, which is flexible for current public datasets. Moreover, we establish a new benchmark in terms of the depth estimation metric, where we compare our proposed method with monocular depth estimation methods on the DDAD and Nuscenes datasets. The relevant code will be available in \href{https://github.com/GANWANSHUI/SimpleOccupancy}{https://github.com/GANWANSHUI/SimpleOccupancy}.
% We demonstrate this baseline work with extensive experiments to reduce the burden for future research in this field.
\end{abstract}

% https://github.com/GANWANSHUI/SimpleOccupancy

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

3D scene understanding is a challenging mission for autonomous driving, especially only relying on the camera. In recent years, Bird's Eye View (BEV) perception, including 3D detection \cite{liu2022petr} and map segmentation \cite{li2022hdmapnet} is getting a lot of attention with the advantage of doing the 3D task estimation in the 2D feature plane and is beneficial for downstream tasks such as prediction and planning \cite{bevsurvey, arnold2019survey}. However, some vital information for driving safety is ignored in the BEV tasks, such as an unrecognizable obstacle. Therefore, reconstructing the 3D geometry of the driving scenes is a longstanding task for autonomous driving. 

To obtain the 3D geometry information, depth estimation from RGB images, such as monocular depth estimation \cite{monosurvery} and stereo matching \cite{poggi2021synergies}, has been well investigated. While depth maps can provide 3D geometry information at the pixel level, we need to project them into the point cloud format in 3D space, and multiple post-processing procedures are required as depth maps may be inconsistent in the local region \cite{tesla}, which is not a straightforward manner for the 3D perception in autonomous driving. For better geometry representation in driving scenarios, occupancy estimation has gained attention in the industrial community. It has shown superiority over the representation in the BEV space \cite{tesla}. However, for research purposes, there is still a lack of a baseline method for the public to measure the progress of occupancy estimation. Therefore, we explore a baseline for 3D occupancy estimation, starting from the surrounding-view setting, like BEV perception. 

As an initial attempt in this field, we investigate the baseline in terms of network design, optimization, and evaluation. For the network design, as shown in Figure \ref{fig:task}, the final output representation for 3D occupancy estimation is different from monocular depth estimation and stereo matching. The network architecture of the 3D occupancy estimation is similar to stereo matching, which means that the experiences from the stereo matching task can be adapted to occupancy estimation to reduce the burden in the network design. Therefore, we design the pipeline to resemble stereo matching closely and investigate a CNN-based framework as the baseline. In terms of optimization, we investigate two training fashions, including rendering-based \cite{nerf} and classification-based loss functions. For evaluation, we conduct experiments on two well-known datasets, DDAD and Nuscenes \cite{ddad, nuscenes}, for broader recognition. Besides, due to the lack of dense 3D occupancy labels for the two datasets, we propose a novel distance-based metric for occupancy evaluation, which is inspired by the sampling strategy in volume rendering \cite{nerf}. Our experimental results demonstrate that the proposed metric is more equitable compared to alternative options, such as classification metrics. Additionally, the proposed metric boasts flexibility as it solely relies on the point cloud as the ground truth, thereby eliminating any additional burdens when implementing the metric on similar datasets.


% In terms of optimization, we use volume rendering \cite{nerf} to produce the depth map from the occupancy grid and then calculate the loss from the predicted depth map and the ground truth. Besides, an L1 voxel grid loss is investigated to corporate with the depth map loss for better performance. For the evaluation, we implement the experiment on two well-known datasets, DDAD and Nuscenes \cite{ddad, nuscenes}, for broader recognition. Besides, due to the lack of the 3D occupancy label for the two datasets, we propose a new metric for the occupancy evaluation, which is inspired by the sampling strategy in the volume rendering \cite{nerf}. The proposed metric is flexible as it only relies on the point cloud as the ground truth, so there is no other burden to implement the metric on other similar datasets. 

In summary, the main contributions of this work are as follows.
\begin{itemize}
    % 压缩itemize的间距，默认的占用空间太多
    \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \item Our study introduces a novel network design, loss design, and performance evaluation to investigate surrounding-view 3D occupancy estimation for the first time.
    \item To evaluate our proposed framework, we establish an occupancy metric for both the DDAD and Nuscenes datasets, and demonstrate the efficacy of the proposed metric for 3D occupancy evaluation. Furthermore, we connect 3D occupancy estimation to the monocular depth estimation task and establish a new ranking benchmark for DDAD and Nuscenes datasets based on the depth estimation metric.
    \item Through extensive qualitative and quantitative experiments, we demonstrate the effectiveness of our proposed method as a universal solution that can utilize existing datasets to do the 3D occupancy estimation and evaluation as simple as depth estimation.
\end{itemize}

\begin{figure}[t!]
\centering
{
% \includegraphics[width=0.48\textwidth]{figure/task.jpg}
\includegraphics[width=\linewidth]{occupancy.png}
}
\caption{\textbf{The overall pipeline comparison of monocular depth estimation, stereo matching, and 3D occupancy estimation}.}
\label{fig:task}
\end{figure}

\section{Related Work}

\subsection{Depth estimation}
% Depth estimation is another way for the 3D perception, and monocular depth estimation and stereo matching are common settings in autonomous driving \cite{wang2019pseudo}. 
For monocular depth estimation, it is usually implemented with a 2D U-Net architecture, as Figure \ref{fig:task} shown \cite{monodepth2,newcrfs}. More recently, the surrounding-view depth estimation has been explored, which is not just limited to a single image context \cite{fsm,surrounddepth}. FSM \cite{fsm} uses the spatio-temporal contexts, poses consistency constraints, and designed photometric loss to learn a single network generating dense and scale-aware depth map. Differently, Surrounddepth \cite{surrounddepth} adopts structure-from-motion to extract scale-aware pseudo depths to pretrain the models for the scale-aware result. This work mainly discusses 3D occupancy estimation, but we compare the depth metric with monocular depth estimation methods. 
% We would like to reveal the gap for different depth map generation (i.e. the depth map from volume rendering and from U-Net architecture). 

In terms of stereo matching, we could obtain the real scale depth map by estimating disparity between the stereo images. The state-of-the-art methods usually used 3D convolution neural networks (CNN) to do cost aggregation \cite{leastereo,cspn,xu2022attention,psmnet,HybirdNet}. Likewise, we also used the 3D CNN to do 3D volume aggregation for the final occupancy representation. 

% However, the 3D CNN cannot be used naively for the 3D volume space, and we will give detailed instructions in the method section. 

\subsection{BEV perception}
We identify that the perception task from the bird's eye view has a common step as the 3D occupancy estimation. Both of them require feature space transformation, where the BEV perception task is from the image space to the BEV plane \cite{bevsurvey}, and 3D occupancy estimation is from the image space to the 3D volume space. LSS \cite{lss} implicitly projects the image feature to the 3d space by learning the latent depth distribution. DETR3D \cite{detr3d} and BEVFormer \cite{bevformer} define a 3D query in the 3D space and use the transformer attention mechanism to query the 2D image feature. ImVoxelNet \cite{imvoxelnet} and Simple-BEV \cite{simplebev} build the 3D volume by the bilinear interpolation of the projection location at the 2D image feature plane, which is an efficient and parameter-free unprojection manner. Therefore, for simplicity in this baseline work, we adapt the parameter-free unprojection manner to build the 3D volume the same as Simple-BEV. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8 \textwidth]{network.png}
    \vspace{0.2cm}
    \caption{\textbf{The overview of the proposed Simple 3D Occupancy network (SimpleOccupancy).} Given the surrounding image, we first extract the image feature by the shared 2D CNN and then use the parameter-free interpolation to obtain the 3D volume. With the position prior guidance, the 3D CNN could effectively aggregate the 3D feature in the volume space (Section \ref{3.2}). At last, we train the proposed network by the depth map loss on the rendered depth map or the classification loss based on the occupancy label (Section \ref{3.3} and \ref{3.4}).}
    \label{fig:Overview}
\end{figure*}

\subsection{Occupancy estimation}
There have been some works representing the scene in an occupancy (voxel) format \cite{occupancysurvey,mescheder2019occupancy,lange2022lopr}. Recently, the industrial community \cite{tesla} reveals that the occupancy representation could be easily combined with the semantic information achieving instance-level prediction. Besides, it could predict the occupancy flow by considering the sequence frame, which could be regarded as the scene flow in the instance level \cite{sceneflow}. Following \cite{tesla}, this personal blog gives a try to use volume rendering to train the occupancy representation in a self-supervision manner, which can not reconstruct the dynamic object and requires the velocity information of the vehicle \cite{VoxelNet}. For the geometry of driving scene, MonoScene \cite{monoscene} and Voxformer \cite{voxformer} explore the 3D semantic scene completion from a single image. This approach proposes to reconstruct the road scene from a monocular image by the deep implicit function \cite{roddick2021road}. Different from \cite{monoscene,roddick2021road, voxformer}, we are under the surrounding-view setting, which is more challenging and meets the need for full perception in the driving scenarios. We need to point out that this baseline work is inspired by the above two works \cite{tesla,VoxelNet} from industrial community, where we contribute a new CNN-based framework, a novel evaluation metric and set up the benchmark for both the 3D occupancy metric and depth map metric in the public datasets \cite{ddad, nuscenes}. 
    
At the time of writing this work, we found a preprint work TPVFormer \cite{huang2023tri}, which is also for 3D occupancy estimation under the surrounding-view setting. Apart from the network architecture, the distinguishing difference is that we are investigating the pure geometry estimation (without considering the semantic information) from the novel optimization manner along with a novel evaluation metric. Our research investigates a more fundamental 3D perception task as close as depth estimation.  
% This concurrent work could be regarded as the extension of the BEVFormer \cite{bevformer} from BEV perception to 3D occupancy estimation. 


\section{Method}

\subsection{Preliminaries}

In this paper, we adopt volume rendering to obtain the depth map for the model training. In the novel view synthesis task \cite{nerf, v4d}, researchers usually use a multilayer perceptron network (MLP) to learn a mapping function to map the 3D position $(x,y,z)$ and view direction $(\theta, \phi)$ into the volume density $\sigma$ and the RGB color $c$. For rendering the RGB value in a pixel, we can use the approximate volume rendering in NeRF \cite{nerf} to do the weighted sum on the sampled point on the ray. The rendering function is defined in the equation (\ref{rendering}): 
\begin{equation}
    \hat{c} = \sum_{i=1}^{W}{T_{i}(1-exp({-\sigma _{i}}{\delta _{i}}  )  ) c_{i} }, 
     \label{rendering}
\end{equation} 
where $\hat{c}$ is the rendered RGB value, ${T_{i} = exp\left ( -\sum_{j=1}^{i-1} {\sigma _{j}}{\delta _{j}}  \right ) }, {\delta _{i}} = t_{i+1} - t_{i}$ is the adjacent sampled points' distance, $i$ is the sampled point along the ray, and $W$ is the number of the sampled points. If we want to obtain the depth information of the related pixel, we can replace the RGB value in the equation (\ref{rendering}) with the sampled point's distance $t_{i}$ as shown in equation (\ref{weight}). In this way, we can use the ground truth depth map to do the supervision training. 
 \begin{equation}
    % \vspace{-1.0em}
    \hat{d} = \sum_{i=1}^{W}{T_{i}(1-exp({-\sigma _{i}}{\delta _{i}}  ) )t_{i} }.
     \label{weight}
\end{equation} 
The NeRF's model learns geometry by the multi-view constraint while, in our setting, the geometry is from the image that is achieved by the CNN model, as introduced below.

\subsection{Model design (SimpleOccupancy)} \label{3.2}

The problem setting of this work is defined in Figure \ref{fig:task}. Given the surrounding-view images with the relative camera pose to the vehicle framework, we design an end-to-end neural network $Q$ to predict the 3D occupancy map, where we formulate it as  $Q$: $(I^1,I^2,I^3,..., I^n) \rightarrow V^{x \times y \times z}$, where $n$ is the number of the surrounding images and $x,y,z$ represents the resolution of the final output of the voxel. We present the overview of the proposed framework in Figure \ref{fig:Overview}, and give the details as follows.


%总体描述问题设置，以及框架

\textbf{Encoder} For the image feature extraction, we use the ResNet \cite{resnet} as the backbone. We provide the experiment on the ResNet 50 and 101 with the pre-trained model provided by Pytorch \cite{pytorch}. The final feature map is with the shape $C \times H/4 \times W/4 $, where $H$ and $W$ are the input resolution of the image and $C=64$ is the channel number. 

\textbf{From the image feature to 3D volume} For both the BEV perception and 3D occupancy estimation, a critical step is to transform the image feature from the 2D image space to the 3D volume space. In this baseline work, we adopt the simplest manner as used in the Simple-BEV \cite{simplebev}. The parameter-free transformation by the bilinear interpolation does not have any position prior, which means that the feature on the rays of the frustum is identical. Therefore, it is a highly ill-posed setting to infer the 3D geometry from the surrounding-view images with only a little overlap. 

\textbf{3D volume space learning} The ill-posed setting requires stronger feature aggregation ability to achieve the 3D occupancy prediction. For the 3D feature learning, we adapt the 3D convolution network based on the hourglass structure from HybridNet \cite{HybirdNet} in the stereo-matching task. 
The transformer-based methods \cite{voxformer, huang2023tri,tesla} use the position encoding to query the feature in the image space. Differently, after obtaining the 3D volume, we further learn position embedding to guide the 3D feature aggregation. The learned position embedding conducts the point-wise multiplication with the obtained 3D volume. The detailed network structure is placed in the supplementary material.  

% However, it is unworkable to naively adopt the 3D CNN from HybridNet, where the neural network can not be optimized. The first observation is that we train the neural network through depth supervision from the volume rendering. This work \cite{dvgo} shows that the Softplus activation function could explore the density very close to 0 and is critical to optimize the voxel density directly. Therefore, we use Softplus to replace Relu, and the network could be optimized normally. Besides, we also try other activation functions, such as SiLU and ELU, which do not work either.

% Though the network could be optimized with the Softplus activation function, the performance is not good due to the lack of any position prior in the 3D volume space. Therefore, we explore the position prior with two formats, position embedding and position encoding. For the position embedding, we initialize a 3D embedding with the same shape as the 3D volume and directly do the point-wise multiplication with the 3D volume. For position encoding, we first initialize a 3D voxel with the 3D position coordinate and map the 3D position coordinate to a higher dimension, which is for capturing the high-frequency geometry details. This position encoding operation is common in the novel view synthesis task (i.e., NeRF) \cite{nerf}, but in this task, we directly concatenate the 3D position encoding voxel with the 3D volume, and then use the 3D CNN to aggregate the 3D feature. The positional encoding we used in 3D CNN is more like the combination of the NeRF's high-frequency function and the discrete positions prior in Transformer architecture \cite{transformer}.

\textbf{Occupancy probability} After the 3D volume space feature aggregation, we have the final voxel $V^{x \times y \times z}$. The occupancy probability value is from the Sigmoid function (\ref{sigomid}).

% can use the volume rendering to render the final output $V^{x \times y \times z}$ into the depth map for supervision training, where the final voxel could have two representations, the density and the occupancy probability. For the density, it is the same as the NeRF model in that the larger value means the higher density of the scene. But the density is not normalized between 0 and 1, and in practice, it is uneasy to select the threshold to determine whether the voxel grid is being occupied or not. Therefore, to obtain a better occupancy representation, we first use the Sigmoid function to obtain the probability value as formulated in equation (\ref{sigomid}), and then for rendering the depth map, we adopt the accumulation sum of the depth ray as done in \cite{VoxelNet}. 
\begin{equation}
    Probability = Sigmoid(\sigma).
     \label{sigomid}
\end{equation} 


\subsection{Model evaluation} \label{3.3}

\begin{figure}[t!]
\centering
{
\includegraphics[width=0.48\textwidth]{metric.png}
}
\caption{\textbf{We use a collection of key points to represent the 3D space and evaluate it based on sample points.} The classification label is shown on the upper side while the bottom side compares two metrics - the classification metric and our newly proposed discrete depth metric - in two different prediction cases. It is evident that the discrete depth metric accurately reflects the cost associated with each prediction.}
\label{fig:label}
\end{figure}

To the best of our knowledge, we are the first work to explore the surrounding-view 3D occupancy estimation and the evaluation at the same time. Note that the concurrent work TPVFormer \cite{huang2023tri} did not investigate the evaluation in empty space. Therefore, we need to investigate the suitable metric to evaluate the model based on the available datasets, Nuscenes and DDAD datasets. We list the following key factors and facts that should be considered for the evaluation:
(1) the current available outdoor dataset with the point cloud as the ground truth label is sparse, especially for the far-end space. Without huge efforts of human labeling, we can not obtain the dense voxel label as in \cite{semantickitti}. Therefore, we can not evaluate the whole voxel space and could only do the evaluation on the known space. (2) About the known space, we can only determine the space between the lidar center and the point cloud. (3) The 3D occupancy and voxel representation is a discrete representation, which means the quantization error is unavoidable, but we can determine the affordable quantization error in the autonomous driving scenario. 
%and the maximum quantization error is the resolution of the voxel.
(4) The evaluation should be feasible for the common datasets and easily be conducted for the study. Our goal with this work is to explore a pipeline that can leverage existing datasets used for depth estimation to perform 3D occupancy estimation and evaluation. Therefore, given the aforementioned considerations, we examine two evaluation metrics in this study: the classification metric and the discrete depth metric. These metrics are also associated with two training 
fashions in the ablation study as shown in Figure \ref{fig:label}. 

 
\textbf{Occupancy label generation} Inspired by NeRF \cite{nerf}, we use the stratified sampling strategy to set the label for the free space. First, we set Lidar's position as the origin of all the rays for all the point clouds. Different from the even stride sampling, we use a fixed number of sample points (30), which means that, for closer point clouds, the sampling space is denser, and for far-end point clouds, the sampling space is sparser. The used sampling strategy has advantages in that it allows for a more balanced distribution of positive and negative labels and maintains a higher standard for closer objects in the driving scenes. The occupied space is represented by the down-sampled point cloud, where we set the down-sample size as 0.4 m. The above operation can be easily performed using the Open3D \cite{open3d} library. 

\textbf{Classification metric} By representing the known space by a set of key points, we can perform the evaluation as the classification task with binary classification metrics, as shown in Figure \ref{fig:label}. The classification metrics are commonly used in 3D semantic scene completion tasks \cite{occupancysurvey, monoscene, voxformer}. However, we observe that classification metrics are not perfect since they can only evaluate the known space in our setting. We give two cases of evaluation along a ray in Figure \ref{fig:label}. We can see that classification metric is not sensitive to case 1. Even though the first predicted occupancy point is far away from the ground truth occupancy (lidar point), classification metric still gives a high score. Conversely, in case 2, if the network is unable to predict the first occupancy point in known space, all of the classification metrics produce a low score except for the accuracy metric. This is unfair if the first predicted occupancy point is next to the actual occupancy point. For a more detailed explanation, please refer to the supplementary material.

\textbf{Discrete depth metric} Recognizing the limitations of classification metric, we introduce discrete depth metric, which provides a more accurate assessment of predictions. Our approach involves dense sampling evaluation points along the ray with an interval (0.2 m) and a maximum distance (52 m). If all the prediction along the ray is empty, we set the last point as the first predicted occupancy point.
The discrete depth error is then calculated as the distance between the first predicted occupancy point and point cloud along the ray. By utilizing this criterion, we can perform evaluations in a manner similar to depth map error assessments. Following depth estimation \cite{surrounddepth}, we report occupancy evaluation with the following metrics, including error metric, (Abs Rel, Sq Rel, RMSE, RMSE log) and accuracy metric, $\delta<t: \% \text { of } d \text { s.t. } \max \left(\frac{\hat{d}}{d^*}, \frac{d^*}{\hat{d}}\right)=\delta<t$. The detailed definition is presented in supplementary material.


\subsection{Model optimization} \label{3.4}

Based on the available depth map and the generated occupancy label as introduced before, we investigate two different training manners. The first one is depth loss, the depth map supervision by the volume rendering. The other one directly calculates the binary classification loss based on the obtained label in the known space, where we consider binary cross entropy loss and L1 loss.

\textbf{Depth loss} With the depth map from the volume rendering, we can train the network the same as the depth estimation task. Following \cite{newcrfs}, we use the Scale-Invariant Logarithmic (SILog) loss \cite{eigen2014depth} to supervise the training. Specifically, the depth map loss is defined as: 
\begin{equation}
\mathcal{L}_{depth}=\alpha \sqrt{\frac{1}{M} \sum_i \Delta d_i^2-\frac{\lambda}{M^2}\left(\sum_i \Delta d_i\right)^2},
\end{equation}
where $\Delta d_i=\log \hat{d}_i-\log d_i^*$, $d_i^*$ is the ground truth depth and $\hat{d}_i$ is the predicted depth. $M$ is the number of valid pixels. We set $\lambda = 0.85$ and $\alpha = 10$ the same as \cite{newcrfs}.


\textbf{Classification loss} Following the common practice, we use the binary cross entropy loss function to train the model as follows: 
\begin{equation}
\mathcal{L}_{BCE}=-\frac{1}{N} \sum_{i=1}^N y_i \cdot \log \left(\hat{y_i}\right)+\left(1-y_i\right) \cdot \log \left(1-\hat{y_i}\right),
\end{equation}
where $y_i$ is the ground truth label and $\hat{y_i}$ is the prediction. Besides, we also investigate to use of the L1 loss direct work on sampled points as below: 
\begin{equation}
\mathcal{L}_{L1}=\frac{1}{N}\sum_{i=1}^{N}L_{1}\left ( 1-p_{i}  \right ) + \frac{1}{K} \omega \sum_{j=1}^{K}L_{1}\left ( 0-p_{j}  \right ),        
\end{equation}
where $p_{i}$ is the probability value based on the point cloud position, and $p_{j}$ is the probability value from the sampled point in the empty space. $N$ is the number of the valid point cloud and $K$ is the number of sampled points in the empty space. $\omega = 5 $ is the hyperparameter for balancing occupied and empty labels with the search range from 1 to 10.0.



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
% \resizebox{0.78\hsize}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{14}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Discrete depth metric}
\tabularnewline
\hline

 & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   & Prior &  Res101 &  LSS \cite{lss}  & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log  $\downarrow$ &  $\delta<1.25$ $\uparrow$ & $\delta<1.25^2$ $\uparrow$ & $\delta<1.25^3$ $\uparrow$
\tabularnewline
\hline

(1) & \Checkmark &  & & & & & 0.210	& 2.724 &	9.595 &	0.343 &	0.674 &	0.869 &	0.936

\tabularnewline
% \hline

(2) & & \Checkmark & & & & & 0.235 &	3.395 &	10.692	& 0.418 &	0.647 &	0.837 &	0.908

\tabularnewline
% \hline

(3) & & & \Checkmark & & & & 0.231 &	3.916 &	12.365	& 0.413 &	0.650 &	0.834 &	0.905

\tabularnewline
\hline

(4) & \Checkmark & & & \Checkmark & & & \textbf{0.206} &	\textbf{2.644} &	\underline{9.457} &	\underline{0.342} &	\underline{0.686} &  \underline{0.875} &	\underline{0.937}

\tabularnewline
% \hline

(5)& \Checkmark & & & \Checkmark & \Checkmark & & \textbf{0.206}	& 2.655 &	\textbf{9.422} &	\textbf{0.339} &	\textbf{0.690} &	\textbf{0.877} &	\textbf{0.939}

\tabularnewline
% \hline

(6)& \Checkmark & & & \Checkmark & \Checkmark $^*$  & & 0.264 &	3.940 &	10.512 &	0.378 &	0.617 &	0.836 &	0.918


\tabularnewline
% \hline


(7) & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \underline{0.207} &	\underline{2.651} &	9.684 &	0.348 &	0.668 &	0.863 &	0.930

\tabularnewline
\hline

 \multicolumn{14}{c}{Nuscenes \cite{nuscenes}}  
 \tabularnewline
\hline
 
(5) & \Checkmark & & & \Checkmark & \Checkmark & & 0.537 &	13.710 &	13.666	 & 0.541 &	0.485 &	0.719 &	0.850

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.2cm}
\caption{The ablation study of the proposed method in the proposed discrete depth metric for occupancy evaluation. Experiment (6) means do not use the official pretrained model for ResNet 101. Experiment (5) is the full model. $\uparrow$ means the value higher is better and $\downarrow$  means lower is better. The number with bold typeface means the best and the number with the underline is the second. }
\label{t:3D occupancy}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{4}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{1}{c}{Experiment setting}  &  \multicolumn{3}{|c}{Depth metric}
\tabularnewline
\hline

    & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & $\delta<1.25$ $\uparrow$ 
\tabularnewline
\hline

(1)  & 	0.152 &	1.034  & 0.795 

\tabularnewline
% \hline

(2) & 0.239 & 3.358 &	0.744 


\tabularnewline
% \hline

(3) & 0.176 & 1.699 & 0.710 


\tabularnewline
\hline

(4)  & \underline{0.150} &	\underline{1.019}  &	\underline{0.798} 


\tabularnewline
% \hline

(5) & \textbf{0.148}	& \textbf{1.000}  &	\textbf{0.800}	


\tabularnewline
% \hline

(6) &  0.204 &	1.663 &	0.702 



\tabularnewline
% \hline


(7) & \textbf{0.148}	& 1.029 & \underline{0.798}	


\tabularnewline
\hline

 \multicolumn{4}{c}{Nuscenes \cite{nuscenes}}  
 \tabularnewline
\hline
 
 (5) & 0.118 & 	0.579  & 	0.891 


\tabularnewline
\hline



\end{tabular}
}
% \setlength{\abovecaptionskip}{2cm}
% \setlength{\belowcaptionskip}{2cm}
\vspace{0.3cm}
\caption{The ablation study of the proposed method in the depth metric. The full table is presented in the supplementary material.}
\label{t:depth metric}
\end{table}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%



% The experiment setting is the same as Table \ref{t:3D occupancy}. The number with bold typeface means the best and the number with the underline is the second.

% %%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[t]
% \centering
% \scalebox{0.78}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% % \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% % located in center x11 

% \hline
%  \multicolumn{11}{c}{DDAD \cite{ddad}}  
% \tabularnewline
% \hline

%  \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Depth metric}
% \tabularnewline
% \hline

%  & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   &  Prior &  Res101 &  LSS \cite{lss}  & Abs Rel & Sq Rel & RMSE & RMSE log &  $\delta<1.25$ & $\delta<1.25^2$ & $\delta<1.25^3$
% \tabularnewline
% \hline

% (1) & \Checkmark &  & & & & & 	0.152 &	1.034 &	4.792 &	\underline{0.223} &	0.795 &	0.933 &	\underline{0.972}


% \tabularnewline
% % \hline

% (2) & & \Checkmark & & & & & 0.239 & 3.358 &	6.779 &	0.313 &	0.744 &	0.884 &	0.937


% \tabularnewline
% % \hline

% (3) & & & \Checkmark & & & & 0.176 & 1.699 & 6.923 &	0.323 &	0.710 &	0.868 &	0.928


% \tabularnewline
% \hline

% (4) & \Checkmark & & & \Checkmark & & & \underline{0.150} &	\underline{1.019} &	\underline{4.775} &	\underline{0.223} &	\underline{0.798} &	\underline{0.934} &	\underline{0.972}


% \tabularnewline
% % \hline

% (5)& \Checkmark & & & \Checkmark & \Checkmark & & \textbf{0.148}	& \textbf{1.000}	 &  \textbf{4.770}  &	\textbf{0.220}  &	\textbf{0.800}	& \textbf{0.935} &	\textbf{0.973}


% \tabularnewline
% % \hline

% (6)& \Checkmark & & & \Checkmark & \Checkmark^*  & & 0.204 &	1.663 &	5.903 &	0.282 &	0.702 &	0.880 &	0.947



% \tabularnewline
% % \hline


% (7) & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \textbf{0.148}	& 1.029 &	4.845 &	0.225	& \underline{0.798}	& 0.933	& 0.970


% \tabularnewline
% \hline

%  \multicolumn{11}{c}{Nuscenes \cite{nuscenes}}  
%  \tabularnewline
% \hline
 
%  (5) & \Checkmark & & & \Checkmark & \Checkmark & & 0.118 & 	0.579 & 	2.630 & 	0.178 & 	0.891 & 	0.954 & 	0.977


% \tabularnewline
% \hline



% \end{tabular}
% }

% \caption{The ablation study of the proposed network in the depth metric. The number with bold typeface means the best and the number with the underline is the second. The full table is presented in the in the supplementary material.}
% \label{t:depth metric}
% \end{table*}
% %%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%


\section{Experiment}
Currently, there is a lack of established baseline work for surrounding-view 3D occupancy estimation. Therefore, we present the first attempt at addressing this gap by evaluating our proposed framework on DDAD and Nuscenes datasets. Our experiments consist of two parts. First, we conduct a detailed ablation study on the proposed framework to investigate the characteristics of the loss function and network design. Second, as we can obtain the depth map through volume rendering, we establish a benchmark for depth metrics by comparing our results with those of monocular depth estimation methods under the supervision setting.


\subsection{Datasets}

% \textbf{DDAD} \cite{ddad} is a largescale dataset with dense ground-truth depth maps. Specifically, this dataset includes 12,650 training samples, which means that it has 75,900 images for six cameras. The validation set contains 3,950 samples (15,800 images). We only consider the distance up to 52m, which is a reasonable range referred from the 3D semantic completion task \cite{monoscene} and BEV perception task \cite{bevformer}. The DDAD dataset has a denser point cloud compared with the Nuscenes dataset, which could provide a more justice evaluation, so we conduct the ablation study experiment based on the DDAD dataset.

\textbf{DDAD} \cite{ddad} is a largescale dataset with dense ground-truth depth maps. Specifically, this dataset includes 12,650 training samples. The validation set contains 3,950 samples. We only consider the distance up to 52m, which is a reasonable range referred from the 3D semantic completion task \cite{monoscene} and BEV perception task \cite{bevformer}. The DDAD dataset has a denser point cloud compared with Nuscenes dataset, which could provide a more equitable evaluation, so we conduct an ablation study experiment based on the DDAD dataset.

\textbf{Nuscenes} \cite{nuscenes} has 1000 sequences of diverse scenes, where each sequence is approximately 20 seconds in duration. We split the training and testing set the same as the depth estimation task \cite{surrounddepth}, including 20096 samples for training and 6019 samples for validation. The distance is also limited to 52m for consideration.

For the above two datasets, we generate the occupancy and empty space label from the raw point cloud and use the projected depth map 
for training and testing. Specifically, we first define the $Z$, $Y$, and $X$ as depth, height, and width, respectively. For the similar perception range as SemanticKITTI \cite{semantickitti}, the valid point cloud range is set as $Z\in \left ( -52m, 52m \right )$, $Y \in \left ( 0m, 6m \right )$, and $X\in \left ( -52m, 52m \right )$ for the training and testing. With the final voxel resolution in $256\times256\times16$, the size of the resolutions of the grid is (0.41m, 0.41m, 0.38m). At last, we set the best threshold for obtaining the score of discrete depth metric from the search range from 0 to 1 with an interval of 0.05.

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{result.jpg}
    % \includegraphics[width=1\textwidth]{figure/main.png}
    \caption{\textbf{The visualization for the ablation study of the proposed method (DDAD dataset \cite{ddad}).} The first row: the surrounding images and the rendered depth maps. For the second row: based on the occupancy label, we present the binary prediction, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The third row is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. The BCE loss, L1 loss, Depth loss, and Full model are related to the experiment setting (2), (3), (1), and (5) in Table \ref{t:3D occupancy} and \ref{t:depth metric}, respectively. Note that we omit the prediction under the 0.4 m for better visualization. Best viewed in color.}
    \label{fig:Result_ablation}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
Method & Occupancy & Depth & Abs Rel &  Sq Rel &  RMSE &  RMSE log &   $\delta<1.25$  &   $\delta<1.25^{2}$  &  $\delta<1.25^{3}$ & Inference time (s)
\tabularnewline

\hline
 \multicolumn{11}{c}{DDAD \cite{ddad}}  

\tabularnewline
\hline

Monodepth2 \cite{monodepth2} & \XSolid &  \Checkmark &  0.148  &   1.003  &   4.722  &   0.219  &   0.795  &   0.935  &   0.973 &  \textbf{0.029}

\tabularnewline
\hline

New-CRFs \cite{newcrfs} & \XSolid &  \Checkmark &  \textbf{0.130}  &  \textbf{0.824}  &   \textbf{4.169}  &   \textbf{0.195}  &   \textbf{0.840}  &   \textbf{0.951}  &   \textbf{0.979} & 0.065

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} & \XSolid &  \Checkmark &   0.151  &   1.021  &   4.766  &   0.221  &   0.789  &   0.935  &   0.974 & 0.099

\tabularnewline
\hline

Ours & \Checkmark &  \Checkmark &  0.148 &	1.00 &	4.770 &	0.220 &	0.800 & 0.935 &	0.973 & 0.230


\tabularnewline

\hline
 \multicolumn{11}{c}{Nuscenes \cite{nuscenes}}  
\tabularnewline
\hline

Monodepth2 \cite{monodepth2} & \XSolid &  \Checkmark &  0.131  &   0.728  &   3.479  &   0.201  &   0.845  &   0.942  &   0.974 & \textbf{0.024}

\tabularnewline
\hline

New-CRFs \cite{newcrfs} & \XSolid &  \Checkmark & \textbf{0.115}  &  0.618  &   3.193  &  0.185  &   0.872  &   0.951  &  \textbf{0.977}  &  0.058

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} & \XSolid &  \Checkmark &  0.128  &   0.760  &   3.442  &   0.198  &   0.856  &   0.946  &   0.975 & 0.102

\tabularnewline
\hline

Ours & \Checkmark &  \Checkmark & 0.118	& \textbf{0.579}	&  \textbf{2.63}  & \textbf{0.178}  &	\textbf{0.891} & \textbf{0.954} &	\textbf{0.977} & 0.196

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.3cm}
\caption{The benchmark in depth map metric with monocular depth estimation methods. The number with bold typeface means the best.}
\label{t:depth2}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation detail}
Our approach is implemented with Pytorch \cite{pytorch}. We resize the RGB image into $336\times672$ before putting them into the neural network. The depth map is rendered with the resolution $224\times352$. We use the Adam optimizer \cite{adam} with $\beta 1 = 0.9$ and $\beta 2 = 0.999$. The learning rate of the 2D CNN is set as $1e-4$ and the 3D CNN is $1e-3$. We train the networks with 12 epochs and do the learning rate decay with 0.1 at the 10th epoch. All the experiments have been conducted on the NVIDIA A 100 (40 GB) GPU.

\subsection{The ablation study for the proposed framework}


We present the ablation study result of the proposed method in Table \ref{t:3D occupancy} (discrete depth metric) and Table \ref{t:depth metric} (depth metric). We mainly investigate the characteristics of the different loss functions and the network design as follows:


\textbf{Loss functions analysis} First, we conduct the experiment with different loss functions based on the baseline model. We can see that the depth loss outperforms the classification for both the discrete depth metric and the depth metric in Table \ref{t:3D occupancy} and Table \ref{t:depth metric}. Accompanying with Figure \ref{fig:Result_ablation} marked with white circles in the depth map and the dense occupancy map, we can observe that the classification losses easily produce the floater, especially in the region close to the sky, because the training could not handle these regions behind the point cloud. Reversely, the depth loss could largely prevent this situation because the rendering fashion cloud implicitly optimizes these regions with the sampled points along the whole ray. But there is still some extreme section, such as at the top of the ego vehicle, exists the floater problem, which never has the sample point to optimize that region. On the other hand, we could observe that the classification loss could produce sharper occupancy boundaries, and the depth loss from rendering more easily produced the long tail false positive prediction as marked with the 
green circles. The long tail false positive prediction usually happens at the intersection of the foreground and the background, which also happens in the stereo matching task \cite{tosi2021smd}. For pixel-level (ray level) prediction, depth supervision with volume rendering is similar to the disparity estimation by the softmax operation. The solution in \cite{tosi2021smd} may help to address the long tail false positive prediction problem, which deserves to investigate in the future.

In terms of the metric design, the prediction case 2 in Figure \ref{fig:label} usually happens as marked with yellow circles, where the network fails to predict the first occupancy at the end of the point cloud, but in the real situation, the first occupied point is very close to the point cloud from the visualization of the dense occupancy. Therefore, the proposed discrete depth metric could have a more justice assessment for the 3D occupancy estimation. 


\textbf{Network design} Based on the depth loss, we further investigate the network design. To alleviate the highly ill-posed problem setting, we learn position embedding to guide the 3D feature aggregation. From Table \ref{t:3D occupancy} and Table \ref{t:depth metric}, we can see that experiment (4) with position prior achieves better results. Furthermore, by replacing ResNet 50 with ResNet 101, the model gets another performance gain. Note that, in experiment (6), the model got a severe performance drop without initializing the pretrain model from Pytorch, which hints that finding a better pretrain strategy associated with 3D occupancy estimation may further boost the performance. The used parameter-free interpolation to recover 3D feature space is the most straightforward and efficient manner. In addition, we also try to use the unprojection manner proposed in LSS \cite{lss}, which estimates the depth distribution first, and then forms the 3D volume with the weighted feature based on the distribution information. However, we overlooked the benefit of LSS's unprojection manner on the 3D occupancy estimation task, which gives us the information that doing the depth distribution estimation may cause a larger learning space for the following 3D CNN feature aggregation module, especially under the imperfect depth distribution estimation.



\textbf{Nuscenes} We also implement the experiment setting (5) to the Nuscenes dataset. The visualization result and analysis are placed in the supplement due to limited space.

In a brief summary, from Table \ref{t:3D occupancy} and Table \ref{t:depth metric}, we can conclude that the proposed discrete depth metric for 3D occupancy evaluation is related to the depth metric. In general, with better depth map results and the 3D occupancy prediction is also better. From Figure \ref{fig:Result_ablation}, we can see that the model could predict the reasonable layout of the driving scenes, but some details are still missing, and the far-end prediction is still not good. We discuss the improvement in the future work section. 


\subsection{The benchmark for the depth estimation}
In the surrounding-view setting, we build a new benchmark in terms of the depth map metric for comparison with the monocular depth estimation methods. Monodepth2 \cite{monodepth2} is a well-recognized self-supervision monocular depth estimation method and New-CRFs \cite{newcrfs} is the state-of-the-art supervision method. Surrounddepth \cite{surrounddepth} is a self-supervision work that has been introduced in the related work. For the supervision setting in this work, we train the above three monocular depth estimation methods with the same loss function used in \cite{newcrfs}. 

From Table \ref{t:depth2}, we can observe that our method is competitive with the monocular depth estimation methods. For the DDAD dataset, New-CRFs \cite{newcrfs} achieves the best result, matching its performance in single image depth estimation. The performance for Monodepth2 \cite{monodepth2}, Surrounddepth \cite{surrounddepth}, and ours is similar.  For the Nuscenes dataset, our method achieves the best result. The visualization result is presented in the supplementary material.  

The disadvantage of our method is that the inference time is higher than the monocular depth estimation methods. We further analyze the inference time for each component of the system. For the DDAD dataset: 2D CNN (31) + 2D to 3D interpolation (3) + 3D CNN (101) + Rendering (95) = 230 (ms). We learn that the 3D feature aggregation and the rendering of the depth map occupied the main time. Note that Rendering is for depth maps and is ignorable if we only want the occupancy estimation.


% but Surroudndepth has a larger inference time because it uses the cross-view transformer to fuse the neighboring image. Our method needs 0.171 s for the single inference, which is mainly from the ResNet 101 feature extractor and the 3D CNN aggregation module. The visualization result is presented in the supplementary material.  


\section{Limitation and future work}
In this simple attempt for 3D occupancy estimation, we still do not introduce the sequence information as did in \cite{tesla} and the BEV perception tasks \cite{bevformer, liu2022petrv2}. It is a promising direction to improve performance by fusing the sequence information. Besides, the current voxel resolution is relatively coarse with 0.4 m, which is a good beginning for the research purpose due to limited computational resources. In the future, we will explore a higher resolution such as 0.2 m \cite{monoscene}. Besides, as observed in Figure \ref{fig:Result_ablation}, the classification loss generally produces sharper boundary than depth loss but with the floater problem, we would investigate the combination of these two training manners to see the benefit. 

% and we believe the proposed framework and the evaluation metric are suitable as the baseline reference. 


\section{Conclusion}
In this paper, we established a baseline for 3D occupancy estimation in the surrounding-view setting for autonomous driving. We demonstrated the effectiveness of the entire pipeline through novel network design, loss function investigation, and model evaluation. We hope this baseline work will inspire followers, and we will release the code to encourage further research in this field.  


% \clearpage

%%%%%%%%% ABSTRACT
% \begin{abstract}
% We present more implementation details and results in this supplementary material. 

% \end{abstract}

%%%%%%%%% BODY TEXT
\section{Appendix}
\textbf{The detailed architecture of the 3D CNN.} For the volume feature aggregation, we adapt the 3D CNN from the HybirdNet \cite{HybirdNet} as shown in Figure \ref{fig:3DCNN}. The differences are below: (1) We introduce the position embedding in the early stage of the 3D volume feature, which is helpful in reasoning the distance information. Specifically, we use a five layers MLP to learn position prior from the position (xyz) guidance. (3) We discard the multiple 3D volume output from the stacked hourglass architecture because we do not find the performance improvement with the default architecture, and rendering for the multiple depth maps also need more computational resource. Please find the attached code for a more detailed design of the 3D CNN. 


\textbf{The comparison of the output channel for the final occupancy prediction.} In this work, the occupancy prediction can be obtained in two ways. The first approach (single channel), presented in the main paper, utilizes the Sigmoid function and a threshold to determine whether the sampled point is occupied. The second approach involves using two channels for the final output and applying Softmax and Argmax operations to obtain the occupancy prediction. Regarding the two channels setting, we can still render the depth map by selecting the value of the occupancy channel after the Softmax function. Our experiment found that the first approach achieves better results in the discrete depth metric and performs similarly in the depth metric, as indicated in Table \ref{t:output channel}. We observe the results are similar if set the single channel's threshold $\omega  = 0.5$ compared with the two-channel setting, which means the single-channel and two-channel settings are equivalent. Meanwhile, in the single channel setting, it would produce more false negative predictions with a higher threshold. Therefore, we could select a suitable threshold for better occupancy performance in discrete depth metrics. We visualize a scene in Figure \ref{fig:channelcomparison} for both BCE loss and depth loss for reference.   




%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{l|c|c|c|c|c|c|c}

\hline
 \multicolumn{8}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{2}{c}{Experiment setting}  &  \multicolumn{3}{|c}{Discrete depth metric}  & \multicolumn{3}{|c}{Depth metric}
\tabularnewline
\hline

    &  & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & $\delta<1.25$ $\uparrow$ & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & $\delta<1.25$ $\uparrow$ 
\tabularnewline
\hline

\multirow{2}*{Two channels} & $\mathcal{L}_{BCE}$  & 0.375 & 8.327 & 0.628  & 0.172 & 1.658 & 0.723

\tabularnewline
~ & $\mathcal{L}_{depth}$  & 0.295 &	5.422 & 0.629 &  \textbf{0.151}  & 1.055 & 0.790


\tabularnewline
\hline
 \multirow{2}*{Single channel} & $\mathcal{L}_{BCE} (\omega = 0.5)$ & 0.379 & 6.383 & 0.626 &  0.176 & 1.699 & 0.710 

\tabularnewline

~ & $\mathcal{L}_{depth} (\omega = 0.5)$  &  0.305	 &  5.629	& 0.617 &  0.152 &  \textbf{1.034} & \textbf{0.795}

\tabularnewline
\hline

 \multirow{2}*{Single channel}  & $\mathcal{L}_{BCE} (\omega = 0.2)$ & 	0.235 &	3.395  & 0.647 & 0.176 & 1.699 & 0.710 

\tabularnewline

~ & $\mathcal{L}_{depth} (\omega = 0.05)$  &  \textbf{0.210} &	\textbf{2.724} & \textbf{0.674}  & 0.152 &  \textbf{1.034} & \textbf{0.795}


\tabularnewline
\hline



\end{tabular}
}
% \setlength{\abovecaptionskip}{2cm}
% \setlength{\belowcaptionskip}{2cm}
\vspace{0.3cm}
\caption{The comparison of the output channel for the final occupancy prediction. $\omega$ is the threshold for the occupancy prediction, where the probability value is larger than the $\omega$ would be regarded as occupancy. For the single-channel setting, different thresholds have different occupancy predictions, resulting in different discrete depth values. The depth metric is identical to different thresholds since it is not determined by a certain threshold. }
\label{t:output channel}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%




\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{channel.jpg}
    \caption{\textbf{The comparison of the output channel for the final occupancy prediction.} The upper part is the comparison of binary cross entropy (BCE) loss. The lower part is the comparison of the depth loss. }
    \label{fig:channelcomparison}
\end{figure*}




\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{3DCNN.png}
    \caption{\textbf{The architecture of the proposed 3D CNN feature aggregation module.}}
    \label{fig:3DCNN}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{14}{c}{DDAD \cite{ddad}}  
\tabularnewline
\hline

 \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Depth metric}
\tabularnewline
\hline

 & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   &  Prior &  Res101 &  LSS \cite{lss}  & Abs Rel & Sq Rel & RMSE & RMSE log &  $\delta<1.25$ & $\delta<1.25^2$ & $\delta<1.25^3$
\tabularnewline
\hline

(1) & \Checkmark &  & & & & & 	0.152 &	1.034 &	4.792 &	\underline{0.223} &	0.795 &	0.933 &	\underline{0.972}


\tabularnewline
% \hline

(2) & & \Checkmark & & & & & 0.239 & 3.358 &	6.779 &	0.313 &	0.744 &	0.884 &	0.937


\tabularnewline
% \hline

(3) & & & \Checkmark & & & & 0.176 & 1.699 & 6.923 &	0.323 &	0.710 &	0.868 &	0.928


\tabularnewline
\hline

(4) & \Checkmark & & & \Checkmark & & & \underline{0.150} &	\underline{1.019} &	\underline{4.775} &	\underline{0.223} &	\underline{0.798} &	\underline{0.934} &	\underline{0.972}


\tabularnewline
% \hline

(5)& \Checkmark & & & \Checkmark & \Checkmark & & \textbf{0.148}	& \textbf{1.000}	 &  \textbf{4.770}  &	\textbf{0.220}  &	\textbf{0.800}	& \textbf{0.935} &	\textbf{0.973}


\tabularnewline
% \hline

(6)& \Checkmark & & & \Checkmark & \Checkmark $^*$  & & 0.204 &	1.663 &	5.903 &	0.282 &	0.702 &	0.880 &	0.947



\tabularnewline
% \hline


(7) & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \textbf{0.148}	& 1.029 &	4.845 &	0.225	& \underline{0.798}	& 0.933	& 0.970


\tabularnewline
\hline

 \multicolumn{14}{c}{Nuscenes \cite{nuscenes}}  
 \tabularnewline
\hline
 
 (5) & \Checkmark & & & \Checkmark & \Checkmark & & 0.118 & 	0.579 & 	2.630 & 	0.178 & 	0.891 & 	0.954 & 	0.977


\tabularnewline
\hline



\end{tabular}
}
\vspace{0.3cm}
\caption{The ablation study of the proposed network in the depth metric. Experiment (6) means do not use the official pretrained model for ResNet 101 (*). Experiment (5) is the full model. The number with bold typeface means the best and the number with the underline is the second.  }
\label{t:depth metric full}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%



\textbf{Depth map benchmark.} To build up the depth map benchmark, we implement the same training strategy for Monodepth2 \cite{monodepth2}, New-CRFs \cite{newcrfs}, and Surrounddepth \cite{surrounddepth}. Specifically, we train the networks with 15 epochs and do the learning rate decay with 0.1 at the 12th epoch for the DDAD dataset. For the Nuscenes dataset, we train the networks with 10 epochs and do the learning rate decay with 0.1 at the 8th epoch as this dataset is faster to converge, which is also observed in Surrounddepth \cite{surrounddepth}. 

\textbf{Detailed definition of the classification metric.} By representing the scenes with a set of labeled key points, we can use the classification metric to evaluate the model, where we define the prediction with: True Positive $(TP)$, False Positive $(FP)$, True Negative $(TN)$, False Negative $(FN)$. Following the common practice, we report the F1 score $((2\times P \times R)/(P+R))$, Precision $(P = TP/(TP+FP))$, Recall $(R=TP/(TP+FN))$ and Accuracy $((TP+TN)/(TP+FP+TN+FN))$. The IOU (Intersection-Over-Union) metric is calculated from the average IOU $(TP / (TP + FN + FP))$ of each class.

% The AUC and F1 scores would be more focused as they could handle the imbalanced label situation, where the AUC could effectively reflect the classification ability of the model, and the F1 score is a comprehensive metric with the consideration for both precision and recall. 

\textbf{Detailed definition of (discrete) depth map metric.} Following the depth estimation task \cite{surrounddepth}, we report the (discrete) depth map evaluation with the following metrics, 
\begin{equation}
\begin{split}
\text {Abs Rel:} \frac{1}{|M|} \sum_{d \in M}\left|\hat{d}-d^*\right| / d^* , \\ 
\text {Sq Rel:} \frac{1}{|M|} \sum_{d \in M}\left\|\hat{d}-d^*\right\|^2 / d^*, \\
\text { RMSE: } \sqrt{\frac{1}{|M|} \sum_{d \in M}|| \hat{d}-d^* \|^2}, \\
\text { RMSE log: } \sqrt{\frac{1}{|M|} \sum_{d \in M}|| \log \hat{d}-\log d^* \|^2} , \\
\delta<t: \% \text { of } d \text { s.t. } \max \left(\frac{\hat{d}}{d^*}, \frac{d^*}{\hat{d}}\right)=\delta<t,
\end{split}
\end{equation}
where the definition of symbols is identical to that of the main paper. 

% The advantage of the proposed discrete depth metric is not only could cope with the limitation of the classification but is also good for evaluating the error with the relative measurement (Abs Rel, Sq Rel). This is helpful to evaluate the occupancy closer to the ego vehicle.


% \section{More results}

\textbf{The full table for the ablation study of the proposed network in the depth metric.} We present the full table for the ablation study of the proposed network in the depth metric in Table \ref{t:depth metric full}. The conclusion is the same as in the main paper, which confirms the effectiveness of the proposed pipeline. 


\textbf{More visualization results for DDAD \cite{ddad} and Nuscenes \cite{nuscenes} datasets.} More 3D occupancy visualization results for DDAD dataset are presented in Figure \ref{fig:ddad}, and the results for Nuscenes are presented in Figure \ref{fig:nuscenes}. We can learn that the model could predict the detailed layout of the surrounding scenes. We hope this baseline work could inspire researchers to further develop this relatively new task. For the sequence prediction visualization, please find the attached video.

\begin{figure*}
    \centering
    \includegraphics[width=0.92\textwidth]{ddad.jpg}
    \caption{\textbf{More 3D occupancy visualization results for DDAD dataset.} The result is under the full model (experiment setting (5)). For the third column, we present the binary prediction based on the generated labels, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The fourth column is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. Note that we omit the prediction under 0.4 m for better visualization. Best viewed in color.}
    \label{fig:ddad}
\end{figure*}


% \textbf{Visualization results for Nuscenes dataset} More 3D occupancy visualization results for Nuscenes dataset are presented in Figure \ref{fig:nuscenes}.

\begin{figure*}
    \centering
    \includegraphics[width=0.92\textwidth]{nuscene.jpg}
   \caption{\textbf{The 3D occupancy visualization results for Nuscenes dataset.} The result is under the full model (experiment setting (5)). For the third column, we present the binary prediction based on the generated labels, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The fourth column is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. Note that we omit the prediction under 0.4 m for better visualization. Best viewed in color.}
    \label{fig:nuscenes}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{ddad_depth.jpg}
    \caption{\textbf{The depth map visualization results for DDAD dataset.} For the camera's order in the surrounding images and the rendered depth maps, the first row is front left, front and front right, and the second row is back left, back, and back right. Best viewed in color.}
    \label{fig:ddad_depth}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{nuscene_depth.jpg}
    \caption{\textbf{The depth map visualization results for Nuscenes dataset.} For the camera's order in the surrounding images and the rendered depth maps, the first row is front left, front and front right, and the second row is back left, back, and back right.  Best viewed in color.}
    \label{fig:nuscenes_depth}
\end{figure*}



\textbf{Depth map visualization for the depth estimation benchmark.} The depth map visualization results for the DDAD dataset are shown in Figure \ref{fig:ddad_depth}, and Figure \ref{fig:nuscenes_depth} shows the depth map results for the Nuscenes dataset. Note that the depth map results in the Nuscenes dataset have some wave-like pattern due to the point cloud's sparseness. The result in the DDAD dataset does not have this problem with a more dense point cloud for training. The quantitative and qualitative results show that the depth map from volume rendering is competitive with the current well-developed monocular depth estimation methods. Besides, the chosen rendering pipeline can be easily extended to incorporate sequence fusion in the future. This would help address the current ill-posed problem setting of inferring 3D geometry from a single image, which is a limitation of current monocular depth estimation methods. 



\clearpage



{\small
% \input{reference.bbl}
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}