

%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.76}{
% \resizebox{0.78\hsize}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{14}{c}{DDAD \cite{ddad} (Supervised learning)}  
\tabularnewline
\hline

 \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Discrete depth metric}
\tabularnewline
\hline

 & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   & Res101  &  LSS \cite{lss} &  Query \cite{wei2023surroundocc} & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log  $\downarrow$ &  $\delta<1.25$ $\uparrow$ & $\delta<1.25^2$ $\uparrow$ & $\delta<1.25^3$ $\uparrow$
\tabularnewline
\hline

(1) & \Checkmark &  & & & & & 0.208 &	2.684	& 9.510 &	0.339 &	\underline{0.678}	& \underline{0.874}	& \underline{0.938}


% \tabularnewline
% % \hline

% -- & \Checkmark $^*$ &  & & & & & 0.213 &	2.788 	& 10.059 &	0.372 &	0.631	& 	0.857 &  0.928

\tabularnewline
% \hline

(2) & & \Checkmark & & & & & 0.221 & 3.204 & 10.283	&  0.396 &	0.668 &	0.849 &	0.914


\tabularnewline
% \hline

(3) & & & \Checkmark & & & & 0.225	& 3.289 & 10.461 &	0.409 &	0.668 &	0.845 &	0.911


\tabularnewline

(4) & \Checkmark & \Checkmark & & & & & 0.208	 &  \underline{2.654}  & 	9.769 & 	0.349 & 	0.653 & 	0.862	 & 0.933


\tabularnewline

(5) & \Checkmark & & \Checkmark & & & &  0.210 & 2.781 & 9.994 &	0.362 &		0.654	&	0.855 &		0.924



\tabularnewline
\hline

(6) & \Checkmark & & & \Checkmark & & &  \underline{0.206}	& \textbf{2.597} & \textbf{9.353}	& \textbf{0.329}	& \textbf{0.685} &	\textbf{0.878} &	\textbf{0.942}


\tabularnewline

(7) & \Checkmark & & & \Checkmark $^*$ &  &  & 0.265 &	3.933 &	10.356 &	0.369 &	0.620&	0.840&	0.923


\tabularnewline

(8)& \Checkmark & & & \Checkmark & \Checkmark & & 	\textbf{0.205}	& \textbf{2.597}	& \underline{9.470}	& \underline{0.331}	& 0.676	& 0.872 &	\underline{0.938}


\tabularnewline
% \hline

(9)& \Checkmark & & & \Checkmark & &\Checkmark &  0.208 & 	2.683 &  9.647	&  0.343 &  0.673	&  0.869	& 0.935

\tabularnewline
\hline
-- &  \Checkmark &  \multicolumn{5}{c|}{SurroundOcc \cite{wei2023surroundocc}}  & 0.209	& 2.748 &	9.455 &	0.337 &	0.687	& 0.873	& 0.937

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.2cm}
\caption{The ablation study of the proposed method in the proposed \textbf{discrete depth metric} for occupancy evaluation. Experiment (7) means do not use the pretrained model from ImageNet. Experiment (6) is the optimal setting. $\uparrow$ means the value higher is better and $\downarrow$  means lower is better. The number with bold typeface means the best and the number with the underline is the second. }
\label{t:3D occupancy}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.76}{
% \resizebox{0.78\hsize}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{14}{c}{DDAD \cite{ddad} (Supervised learning)}  
\tabularnewline
\hline

 \multicolumn{7}{c}{Experiment setting}  &  \multicolumn{7}{|c}{Depth metric}
\tabularnewline
\hline

 & $\mathcal{L}_{depth}$  &  $\mathcal{L}_{BCE}$  &  $\mathcal{L}_{L1}$   & Res101  &  LSS \cite{lss} &  Query \cite{wei2023surroundocc} & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log  $\downarrow$ &  $\delta<1.25$ $\uparrow$ & $\delta<1.25^2$ $\uparrow$ & $\delta<1.25^3$ $\uparrow$
\tabularnewline
\hline

(1) & \Checkmark &  & & & & & 0.149 &	\underline{1.001} &	4.782 &	\underline{0.222} &	\underline{0.798} &	\underline{0.934} &	\underline{0.972}

% \tabularnewline
% % \hline
% -- & \Checkmark $^*$ &  & & & & &  0.158  &   1.089  &   5.171  &   0.238  &   0.769  &   0.926  &   0.969 

\tabularnewline

(2) & & \Checkmark & & & & & 0.175	&1.725	&5.954&	0.289	&0.763&	0.903	&0.949

\tabularnewline
% \hline
(3) & & & \Checkmark & & & & 0.168&	1.570	&6.580	&0.310	&0.735&	0.882&	0.936

\tabularnewline

(4) & \Checkmark & \Checkmark & & & & & 0.150 &	1.028	&4.868&	0.225	&0.790 &	0.931 &	0.971

\tabularnewline

(5) & \Checkmark & & \Checkmark & & & &  0.151&	1.056	&4.937	&0.230 &	0.790 &	0.929&	0.970

\tabularnewline
\hline

(6) & \Checkmark & & & \Checkmark & & &  \textbf{0.147} &	\textbf{0.996} &	\underline{4.738}  &	\textbf{0.220} &	\textbf{0.801}	& \textbf{0.935} &	\underline{0.972}

\tabularnewline

(7) & \Checkmark & & & \Checkmark $^*$ & & & 0.198	& 1.555	& 5.761	& 0.275	& 0.709	& 0.888	& 0.951

\tabularnewline

(8)& \Checkmark & & & \Checkmark & \Checkmark & & 	\underline{0.148} &	1.013 &	4.797	& \underline{0.222}	& 0.797 &	\underline{0.934} &	\underline{0.972}

\tabularnewline
% \hline

(9)& \Checkmark & & & \Checkmark & &\Checkmark &  0.150 &	1.008 &	\textbf{4.737} &	\textbf{0.220} &	\underline{0.798} &	\textbf{0.935} &	\textbf{0.973}


\tabularnewline
\hline
-- &  \Checkmark &  \multicolumn{5}{c|}{SurroundOcc \cite{wei2023surroundocc}}  & 0.152	 & 1.022	 & 4.668	 & 0.220	 & 0.797	 & 0.933	 & 0.972

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.2cm}
\caption{The ablation study of the proposed method in the \textbf{depth metric}. Experiment (7) means do not use the pretrained model from ImageNet. Experiment (6) is the optimal setting. $\uparrow$ means the value higher is better and $\downarrow$  means lower is better. The number with bold typeface means the best, and the number with the underline is the second. }
\label{t:depth metric}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.76}{
% \resizebox{0.78\hsize}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
 \multicolumn{7}{c|}{Method}  & \multicolumn{7}{c}{Nuscenes \cite{nuscenes}}  
\tabularnewline
\hline
 \multicolumn{14}{c}{Supervised learning, Discrete depth metric}
\tabularnewline
\hline

 \multicolumn{7}{c|}{--} & Abs Rel $\downarrow$ & Sq Rel $\downarrow$ & RMSE $\downarrow$ & RMSE log  $\downarrow$ &  $\delta<1.25$ $\uparrow$ & $\delta<1.25^2$ $\uparrow$ & $\delta<1.25^3$ $\uparrow$

 \tabularnewline
\hline

  \multicolumn{7}{c|}{SurroundOcc \cite{wei2023surroundocc}} & 0.361	& 6.089	& 9.726	& 0.391	& 0.585	& 0.820	& 0.923

\tabularnewline
\hline

\multicolumn{7}{c|}{Ours} & \textbf{0.205} & \textbf{2.044} &	\textbf{6.495} &	\textbf{0.289} &	\textbf{0.718} &	\textbf{0.922} &	\textbf{0.966}

\tabularnewline
\hline
\multicolumn{14}{c}{Self-supervised learning, Discrete depth metric}  

\tabularnewline
\hline
 \multicolumn{7}{c|}{SurroundOcc \cite{wei2023surroundocc}} & 0.431	& 8.483 &	12.200 &	0.490 & 0.542	& 0.726 & 0.839


\tabularnewline
\hline
\multicolumn{7}{c|}{Ours$^{\dagger}$} & 0.570	 &  8.835  & 13.453	&  1.152	& 0.211	& 	0.403 & 0.530

\tabularnewline
\hline
\multicolumn{7}{c|}{Ours} &	0.211 &  \textbf{2.460} & 7.687	& 0.384	& 0.702	& 0.855	& 0.914

\tabularnewline
\hline
\multicolumn{7}{c|}{Ours (Density)} & 	0.226	&  3.900	&  7.876	&  0.371 & 	0.736	&  0.871	&  0.927

\tabularnewline
\hline
\multicolumn{7}{c|}{Ours (SDF)} &  \textbf{0.203}	& 2.703 & \textbf{7.534} & \textbf{0.350} &	\textbf{0.732} & \textbf{0.872} & \textbf{0.928}

\tabularnewline
\hline

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \multicolumn{14}{c}{Supervised learning, Depth metric}

 \tabularnewline
\hline

\multicolumn{7}{c|}{SurroundOcc \cite{wei2023surroundocc}} & 0.117 & 0.591	& \textbf{2.623}	& 0.180	& 0.886	& 0.950	& 0.975
\tabularnewline
\hline

\multicolumn{7}{c|}{Ours} & 0.116	& \textbf{0.542}	&  2.784  & \textbf{0.179}  &	\textbf{0.888} & \textbf{0.955} &	\textbf{0.978}

\tabularnewline
\hline
\multicolumn{14}{c}{Self-supervised learning, Depth metric}  

\tabularnewline
\hline
 \multicolumn{7}{c|}{SurroundOcc \cite{wei2023surroundocc}} & 0.274	&  3.405	&  6.127 &  0.345	&  0.705	&  0.854	&  0.920


\tabularnewline
\hline
\multicolumn{7}{c|}{Ours$^{\dagger}$} &	0.725  &   8.174  &  14.013  &   1.344  &   0.011  &   0.024  &   0.043

\tabularnewline
\hline
 \multicolumn{7}{c|}{Ours} & 0.227 &  3.437 &  5.463	 &  0.305	 &  \textbf{0.789}	 &  \textbf{0.899}	 &  0.940
 
\tabularnewline
\hline
\multicolumn{7}{c|}{Ours (Density)} & 	\textbf{0.199}	& \textbf{2.261} &	\textbf{5.396} &	\textbf{0.302}  &	0.775 &	0.893 &	\textbf{0.941}

\tabularnewline
\hline
 \multicolumn{7}{c|}{Ours (SDF)} & 0.210	& 2.655 &	5.540 &	 0.306 &	0.777 & 	0.892  &	 0.939

\tabularnewline
\hline

\end{tabular}
}
\vspace{0.2cm}
\caption{The ablation study of the proposed method in the proposed discrete depth metric and depth metric. Our network is the optimal setting in Experiment (6) in Table \ref{t:3D occupancy}. Our$^{\dagger}$ represents using the PoseNet in \cite{surrounddepth} to predict the 6D pose of two input frames. Our (Density) means the final output is density value without passing the sigmoid function. Our (SDF) means the final output is signed distance value. $\uparrow$ means the value higher is better and $\downarrow$  means lower is better. The number with bold typeface means the best. }
\label{t:nuscene 3D occupancy}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%


\section{Experiment}
We validate our proposed framework through extensive experiments utilizing the DDAD and Nuscenes datasets. Our experimental evaluation comprises four key components.
Firstly, we conduct a comprehensive ablation study on the proposed framework, delving into the intricacies of the supervised loss function and network architecture.
Subsequently, we delve into the domain of self-supervised learning within the Nuscenes dataset, shedding light on its impact on 3D reconstruction performance.
In the third phase, leveraging volume rendering for depth map acquisition, we establish a benchmark for depth metrics. This entails a comparative analysis between our outcomes and those of monocular depth estimation methods.
Finally, we discuss our work with the recent semantic 3D occupancy estimation method. Our experiments conclusively demonstrate that our point-level optimization approach could benefit semantic 3D occupancy estimation in an effective pretraining strategy.


\subsection{Datasets}

\textbf{DDAD} \cite{ddad} is a largescale dataset with dense ground-truth depth maps. Specifically, this dataset includes 12,650 training samples. The validation set contains 3,950 samples. We only consider the distance up to 52m, which is a reasonable range referred from the 3D semantic completion task \cite{monoscene} and BEV perception task \cite{bevformer}. The DDAD dataset has a denser point cloud compared with Nuscenes dataset, which could provide a more equitable evaluation, so we conduct an ablation study experiment based on the DDAD dataset.

\textbf{Nuscenes} \cite{nuscenes} has 1000 sequences of diverse scenes, where each sequence is approximately 20 seconds in duration. We split the training and testing set the same as the depth estimation task \cite{surrounddepth}, including 20096 samples for training and 6019 samples for validation. The distance is also limited to 52m for consideration.

For the above two datasets, we generate the occupancy and empty space label from the raw point cloud and use the projected depth map 
for training and testing. Specifically, we first define the $Z$, $Y$, and $X$ as depth, height, and width, respectively. For the similar perception range as SemanticKITTI \cite{semantickitti}, the valid point cloud range is set as $Z\in \left ( -52m, 52m \right )$, $Y \in \left ( 0m, 6m \right )$, and $X\in \left ( -52m, 52m \right )$ for the training and testing. With the final voxel resolution in $256\times256\times16$, the size of the resolutions of the grid is (0.41m, 0.41m, 0.38m). To evaluate the representation with occupancy probability, we set the best threshold for obtaining the score of discrete depth metric from the search range from 0 to 1 with an interval of 0.05 in the test set. For the representation with the signed distance function (SDF), we set the best threshold from the search range from -0.5 to 0.5 for the discrete depth metric's score  in the test set. 

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{fig/result_new_new.jpg}
    % \includegraphics[width=1\textwidth]{figure/main.png}
    \caption{\textbf{The visualization for 3D occupancy and depth estimation ablation study of the proposed method (DDAD dataset \cite{ddad}).} The first row: the surrounding images and the rendered depth maps. For the second row: based on the occupancy label, we present the binary prediction, where the red, green, and white colors mean the false negative, true positive, and false positive, respectively. The third row is the dense occupancy prediction in the voxel grid, where the darker color means the occupancy is closer to the ego vehicle. The BCE loss, L1 loss, Depth loss, and Full model are related to the experiment setting (2), (3), (1), and (6) in Table \ref{t:3D occupancy} and \ref{t:depth metric}, respectively. Note that we omit the prediction under the 0.4 m for better visualization. Best viewed in color.}
    \label{fig:Result_ablation}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{fig/SD.png}
    \caption{\textbf{The visualization for self-supervised learning in the representation of density and signed distance function, Nuscenes \cite{nuscenes}. The mesh for density is extracted with the threshold of 0.5. }}
    \label{fig:nuscene_ablation}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
% \begin{tabular}{l|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}|p{1cm}<{\centering}}
% located in center x11 

\hline
Method & Occupancy & Depth & Abs Rel &  Sq Rel &  RMSE &  RMSE log &   $\delta<1.25$  &   $\delta<1.25^{2}$  &  $\delta<1.25^{3}$ & Inference time (s)
\tabularnewline

\hline
 \multicolumn{11}{c}{DDAD \cite{ddad} (Supervised learning)}  

\tabularnewline
\hline

Monodepth2 \cite{monodepth2} &  &  \Checkmark &  0.148  &   1.003  &   4.722  &   0.219  &   0.795  &   0.935  &   0.973 &  \textbf{0.029}

\tabularnewline
\hline

New-CRFs \cite{newcrfs} &  &  \Checkmark &  \textbf{0.130}  &  \textbf{0.824}  &   \textbf{4.169}  &   \textbf{0.195}  &   \textbf{0.840}  &   \textbf{0.951}  &   \textbf{0.979} & 0.065

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} &  &  \Checkmark &   0.151  &   1.021  &   4.766  &   0.221  &   0.789  &   0.935  &   0.974 & 0.099

\tabularnewline
\hline

SurroundOcc \cite{wei2023surroundocc} & \Checkmark &  \Checkmark &  0.152	 & 1.022	 & 4.668	 & 0.220	 & 0.797	 & 0.933	 & 0.972 & 0.305

\tabularnewline
\hline

Ours & \Checkmark &  \Checkmark &  0.147&	0.996	&4.738	& 0.220	& 0.801	& 0.935	& 0.972 & 0.230


\tabularnewline
\hline

 \multicolumn{11}{c}{Nuscenes \cite{nuscenes} (Supervised learning)}  
\tabularnewline
\hline

Monodepth2 \cite{monodepth2} &  &  \Checkmark &  0.131  &   0.728  &   3.479  &   0.201  &   0.845  &   0.942  &   0.974 & \textbf{0.024}

\tabularnewline
\hline

New-CRFs \cite{newcrfs} &  &  \Checkmark & \textbf{0.115}  &  0.618  &   3.193  &  0.185  &   0.872  &   0.951  &  0.977  &  0.058

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} &  &  \Checkmark &  0.128  &   0.760  &   3.442  &   0.198  &   0.856  &   0.946  &   0.975 & 0.102

\tabularnewline
\hline

SurroundOcc \cite{wei2023surroundocc} & \Checkmark &  \Checkmark & 0.117 & 0.591	& \textbf{2.623}	& 0.180	& 0.886	& 0.950	& 0.975 & 0.305

\tabularnewline
\hline

Ours & \Checkmark &  \Checkmark & 0.116	& \textbf{0.542}	&  2.784  & \textbf{0.179}  &	\textbf{0.888} & \textbf{0.955} &	\textbf{0.978} & 0.196

% 0.116 &	0.542	& 2.784 &	0.179 &	0.888 &	0.955 &	0.978
\tabularnewline
\hline

\multicolumn{11}{c}{Nuscenes \cite{nuscenes} (Self-supervised learning)}  
\tabularnewline
\hline

Monodepth2 \cite{monodepth2} &  &  \Checkmark &  0.258  &   4.566  &   5.650  &   0.316  &   0.785  &   0.894  &   0.935 & --

\tabularnewline
\hline

New-CRFs \cite{newcrfs} &  &  \Checkmark &  0.230  &   3.931  &   5.361  &   0.304  &   \textbf{0.812}  &   \textbf{0.904}  &   0.939  & --

\tabularnewline
\hline

Surrounddepth \cite{surrounddepth} &  &  \Checkmark & 0.215  &   3.127  &   \textbf{5.277}  &   \textbf{0.294}  &   0.797  &   0.903  &   \textbf{0.943} &  --

\tabularnewline
\hline

SurroundOcc \cite{wei2023surroundocc} & \Checkmark &  \Checkmark &  0.274  &   3.405  &   6.127  &   0.345  &   0.705  &   0.854  &   0.920 & --
\tabularnewline
\hline

% Ours (Probability) & \Checkmark &  \Checkmark &  & & & & & & & 
% \tabularnewline
% \hline

Ours & \Checkmark &  \Checkmark &  \textbf{0.199}	& \textbf{2.261} &	5.396 &	0.302  &	0.775 &	0.893 &	0.941  & --
\tabularnewline
\hline





\end{tabular}
}
\vspace{0.3cm}
\caption{The benchmark in depth map metric with monocular depth estimation methods. The number with bold typeface means the best.}
\label{t:depth2}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%




% %%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[t]
% \centering
% \scalebox{0.78}{
% \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}

% \hline
% Method & Occupancy & Depth & Abs Rel &  Sq Rel &  RMSE &  RMSE log &   $\delta<1.25$  &   $\delta<1.25^{2}$  &  $\delta<1.25^{3}$ & Inference time (s)
% \tabularnewline
% \hline

% \multicolumn{11}{c}{Nuscenes \cite{nuscenes}}  
% \tabularnewline
% \hline

% Monodepth2 \cite{monodepth2} &  &  \Checkmark &  0.332  &  10.809  &   7.907  &   0.352  &   0.776  &   0.883  &   0.926 

% \tabularnewline
% \hline

% New-CRFs \cite{newcrfs} &  &  \Checkmark & 0.295  &   9.358  &   7.571  &   0.339  &   0.802  &   0.894  &   0.930

% \tabularnewline
% \hline

% Surrounddepth \cite{surrounddepth} &  &  \Checkmark &  0.320  &  10.811  &   7.936  &   0.345  &   0.783  &   0.889  &   0.930 

% \tabularnewline
% \hline

% % SurroundOcc \cite{wei2023surroundocc} & \Checkmark &  \Checkmark & 0.117 & 0.591	& \textbf{2.623}	& 0.180	& 0.886	& 0.950	& 0.975 & 0.305

% % \tabularnewline
% % \hline



% Ours & \Checkmark &  \Checkmark & 0.224  &   3.383  &   7.165  &   0.333  &   0.753  &   0.877  &   0.930  & 0.1609

% \tabularnewline
% \hline

% Ours (W / Sky) & \Checkmark &  \Checkmark & 0.252  &   5.136  &   6.975  &   0.328  &   0.767  &   0.888  &   0.934  & 0.2403

% \tabularnewline
% \hline

% Ours (W / Sky, 101) & \Checkmark &  \Checkmark &  0.250  &   5.341  &   6.964  &   0.324  &   0.776  &   0.892  &   0.936 &



% \tabularnewline
% \hline

% \end{tabular}
% }
% \vspace{0.3cm}
% \caption{The benchmark in depth map metric with monocular depth estimation methods. All the method is trained with self-supervision loss with a maximum depth of 80 m. The number with bold typeface means the best.}
% \label{t:selfdepth2}
% \end{table*}
% %%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%






\subsection{Implementation detail}
Our approach is implemented with Pytorch \cite{pytorch}. We resize the RGB image into $336\times672$ before putting them into the neural network. The depth map is rendered with the resolution $224\times352$. We use the Adam optimizer \cite{adam} with $\beta 1 = 0.9$ and $\beta 2 = 0.999$. The learning rate of the 2D CNN is set as $1e-4$ and the 3D CNN is $1e-3$. We train the networks with 12 epochs and do the learning rate decay with 0.1 at the 10th epoch. All the experiments have been conducted on the NVIDIA A 100 (40 GB) GPU.

\subsection{The ablation study for supervised loss and network architecture}

We present the ablation study result of the proposed method in Table \ref{t:3D occupancy} (discrete depth metric) and Table \ref{t:depth metric} (depth metric). We mainly investigate the characteristics of the different supervised loss functions and the network design as follows:


\textbf{Supervised loss functions analysis} First, we conduct the experiments (1-5) with different loss functions based on the lighter encoder, Resnet 50. We can see that the depth loss ($\mathcal{L}_{depth}$) outperforms the classification losses ($\mathcal{L}_{BCE}$ and $\mathcal{L}_{L1}$) for both the discrete depth metric and the depth metric in Table \ref{t:3D occupancy} and Table \ref{t:depth metric}, in the experiment setting (1-3). Accompanying with Figure \ref{fig:Result_ablation} marked with white circles in the depth map and the dense occupancy map, we can observe that the classification losses easily produce the floater in the region close to the sky. We conclude that the training with classification loss could not handle these regions behind the point cloud (unknown region). Reversely, the depth loss could largely prevent this situation because the rendering fashion cloud implicitly optimizes these regions with the sampled points along the whole ray. On the other hand, the depth loss from rendering more easily produced the long tail false positive prediction, as marked with the green circles. The long tail false positive prediction usually happens at the foreground and background intersection, which is not in the concerned driving region. We further investigate the combination of depth loss and classification loss in experiments (4) and (5). However, the combined loss is a little worse than the depth loss. It may own that the classification loss hurts the prediction in the sky region, as analyzed above.

In terms of the metric design, the prediction case 2 in Figure \ref{fig:label} usually happens as marked with yellow circles, where the network fails to predict the first occupancy at the end of the point cloud, but in the real situation, the first occupied point is very close to the point cloud from the visualization of the dense occupancy. Therefore, the proposed discrete depth metric could have a more justice assessment for the 3D occupancy estimation. 


\textbf{Network design} Building upon the depth loss, we delve into the investigation of network design. Note that in this section, our focus is not on introducing a highly specific network component but rather on exploring the performance of existing modules within the context of BEV perception under our proposed framework. At first, we try with a larger encoder by replacing ResNet 50 with ResNet 101, and the model performs better. Note that, in the experiment (7), the model gets a severe performance drop without initializing the model pre-trained from on ImageNet, which hints that finding a better pretrain strategy associated with 3D occupancy estimation may further boost the performance. The used parameter-free interpolation to recover 3D feature space is the most straightforward and efficient manner. In experiment (8), we also try to use the unprojection manner proposed in LSS \cite{lss}, which estimates the depth distribution first, and then forms the 3D volume with the weighted feature based on the distribution information. However, we overlooked the benefit of LSS's unprojection manner on the 3D occupancy estimation task, which gives us the information that doing the depth distribution estimation may cause a larger learning space for the following 3D CNN feature aggregation module, especially under the imperfect depth distribution estimation. Similarly, we have not observed the performance gain with the try on the Query method \cite{wei2023surroundocc}, which is based on the transformer with cross attention. 

% \textbf{SurroundOcc \cite{wei2023surroundocc}} We also implement the depth loss to the concurrent work SurroundOcc, where our network is performing better than it for both the performance (Table \ref{t:3D occupancy}) and computational cost (Table \ref{t:depth2}).

% \textbf{Nuscenes} We implement the optimal network architecture in experiment setting (6) to Nuscenes dataset. The visualization result and analysis are placed in the supplement due to limited space.

% In a brief summary, from Table \ref{t:3D occupancy} and Table \ref{t:depth metric}, we can conclude that the proposed discrete depth metric for 3D occupancy evaluation is related to the depth metric. In general, with better depth map results and the 3D occupancy prediction is also better. From Figure \ref{fig:Result_ablation}, we can see that the model could predict reasonable layout of the driving scenes, but some details are still missing, and the far-end prediction is still not good. We discuss the improvement in future work. 


\subsection{Self-supervised learning and 3D reconstruction}

For our investigation into self-supervised learning and 3D reconstruction, we perform experiments on the Nuscenes dataset \cite{nuscenes}. This choice was motivated by the higher accuracy of the provided 6D pose for the two subsequent frameworks, as opposed to the DDAD dataset \cite{ddad}. The results of this exploration are summarized in Table \ref{t:nuscene 3D occupancy}, where we also include results from supervised learning for comparison. This supervised setting mirrors the conditions of experiment (6) in Table \ref{t:3D occupancy}.

Our initial observation reveals a persistent gap between supervised and self-supervised learning, even when employing ground truth pose information from the sensor. Notably, as shown in Figure \ref{fig:scale_ambiguity}, the use of pose from PoseNet yields notably inferior results due to the inherent scale ambiguity between depth estimation and 6D pose transformation. 
We then delved into the final output representation in the 3D volume space. Table \ref{t:nuscene 3D occupancy} highlights that the Signed Distance Function (SDF) representation outperforms in discrete depth metrics, indicating its superiority in 3D reconstruction, as also evidenced by the marked improvement in the region enclosed by the red rectangle in Figure \ref{fig:nuscene_ablation}. Conversely, the SDF representation yields a less favorable result in the depth metric, as demonstrated in Table \ref{t:nuscene 3D occupancy} and visually represented by the yellow rectangle in Figure \ref{fig:nuscene_ablation}. This discrepancy can be attributed to the relatively challenging nature of optimizing signed distance values, while the density and probability representations offer more flexibility for optimization.
However, it's worth noting that this soft and flexible characteristic is less conducive to mesh extraction at specific thresholds. This is exemplified in Figure \ref{fig:nuscene_ablation}, where the mesh output in scene 1 is reasonable for density representation, but the result in scene 2 is suboptimal. Consequently, for the specific purpose of 3D reconstruction, the SDF representation emerges as the superior choice.


\subsection{The benchmark for the depth estimation}
In the surrounding-view setting, we build a new benchmark in terms of the depth map metric for comparison with the monocular depth estimation methods. Monodepth2 \cite{monodepth2} is a well-recognized self-supervised monocular depth estimation method and New-CRFs \cite{newcrfs} is the state-of-the-art supervised method. Surrounddepth \cite{surrounddepth} is a self-supervised work that has been introduced in the related work. For the supervised learning in this work, we train the above three monocular depth estimation methods with the same loss function used in \cite{newcrfs}. For self-supervised learning, we use the loss function in Equation \ref{self-supervised loss} for all the compared methods.

From Table \ref{t:depth2}, we can observe that our method is competitive with the monocular depth estimation methods. For the DDAD dataset, New-CRFs \cite{newcrfs} achieves the best result, matching its performance in single image depth estimation. The performance for Monodepth2 \cite{monodepth2}, Surrounddepth \cite{surrounddepth}, and ours is similar. For the Nuscenes dataset, our method achieves the best result in supervised learning and competitive performance in self-supervised learning. In Figure \ref{fig:ddad_depth_main}, We present the depth map visualization on the DDAD dataset, and more visualization result is presented in the supplementary material.  

The disadvantage of our method is that the inference time is higher than the monocular depth estimation methods. We further analyze the inference time for each component of the system. For the DDAD dataset: 2D CNN (31) + 2D to 3D interpolation (3) + 3D CNN (101) + Rendering (95) = 230 (ms). We learn that the 3D feature aggregation and the rendering of the depth map occupied the main time. Note that Rendering is for depth maps and is ignorable if we only want the occupancy estimation.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/scale_ambiguity.png}
    \caption{\textbf{The comparison for depth map trained based on the pose from PoseNet and ground truth (GT) pose.} }
    \label{fig:scale_ambiguity}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=0.98\textwidth]{fig/ddad_depth_main.jpg}
    \caption{\textbf{The depth map visualization results for DDAD dataset.} For the camera's order in the surrounding images and the rendered depth maps, the first row is front left, front and front right, and the second row is back left, back, and back right. Best viewed in color.}
    \label{fig:ddad_depth_main}
\end{figure*}




\subsection{Discussion on semantic 3D occupancy estimation}

\textbf{SurroundOcc in our framework} The motivation behind our approach to 3D occupancy estimation aligns with concurrent works \cite{wei2023surroundocc,wang2023openoccupancy, tong2023scene} in the field. Among them, we select SurroundOcc \cite{wei2023surroundocc} as a representative for discussion limited by the computational cost. First, We implement the same loss function to SurroundOcc with the similar training strategy, where the detailed configuration is in the supplemental material. From Table \ref{t:3D occupancy} to Table \ref{t:depth metric}, we could find that our network is performing better than it for both the performance and computational cost. Note that, during the experiment, we observe that SurroundOcc is relatively difficult to optimize in self-supervised learning and easily falls into the sub-optimal results, hence, with worse results. 

\textbf{Pretrain strategy for SurroundOcc} Second, as depicted in Figure \ref{fig:label}, our method adopts a point-level training approach, while the concurrent works employ voxel-level training. This distinction allows us to achieve a finer level of granularity in our predictions compared to theirs. To explore the potential benefits of our strategy for concurrent work, we conduct an experiment using our approach as a pretraining step. Specifically, we train SurroundOcc on point-level semantic labels generated by our sampling strategy, and then we fine-tune the network using voxel-level semantic labels from SurroundOcc. More implementation details are placed in the supplement. The results in Table \ref{t:pretrain} demonstrate that our point-level pretraining effectively improves the network's performance. As shown in Figure \ref{fig:pretrain}, the outcome of training with sparse point-level labels is not optimal. In the upper part of the view, we observe that vegetation and manmade structures dominate, as these classes typically appear near the top. Consequently, the network tends to predict all regions close to the sky as either vegetation or manmade. Despite this limitation, the network demonstrates reasonable predictions for the remaining parts of the scene, which proves to be a valuable initialization for subsequent fine-tuning and leads to improved performance.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/Pretrain_1.png}
    \caption{\textbf{The comparison for point-level supervision by our sampling strategy directly from the point cloud label and the dense voxel-level supervision from SurroundOcc \cite{wei2023surroundocc}.} }
    \label{fig:pretrain}
\end{figure}


\section{Limitation and future work}
In this simple framework for 3D occupancy estimation, we still do not introduce sequence information as did in \cite{tesla} and the BEV perception tasks \cite{bevformer, liu2022petrv2}. It is a promising direction to improve performance by fusing the sequence information. Besides, the current voxel resolution is relatively coarse with 0.4 m, which is a good beginning for the research purpose due to limited computation resources. In the future, we will explore a higher resolution, such as 0.2 m in \cite{monoscene}, which should also benefit the 3D reconstruction. 



%%%%%%%%%%%%%%%%%%%%%%For %%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{c|c|c|c}

% \hline
%      \multicolumn{4}{c}{SurroundOcc \cite{wei2023surroundocc}}  
% \tabularnewline
\hline
 Method & Experiment setting  &  SC IoU  &  SSC IoU
\tabularnewline
\hline

 \multirow{3}*{SurroundOcc \cite{wei2023surroundocc}} & Original   & 31.49	 & 20.30

\tabularnewline

~ & W/O pretrain   &	31.29   & 20.08

\tabularnewline

~ & W pretrain   &	\textbf{32.55}  &  \textbf{20.85}



\tabularnewline
\hline

\end{tabular}
}

\vspace{0.3cm}
\caption{The ablation study on SurroundOcc with point level pretrain. Original is from the original paper. W/O pretrain represents our reimplement result. W/ pretrain means using our point level pretrain. The number with bold typeface means the best.}

\label{t:pretrain}
\end{table}
%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%



\section{Conclusion}
In this paper, we give a simple framework for 3D occupancy estimation, depth estimation, and 3D reconstruction in the surrounding-view setting for autonomous driving. We demonstrated the effectiveness of the entire pipeline through novel network design, loss function investigation, and model evaluation. We hope this simple and effective work will inspire followers, and we will release the code to encourage further research in this field.  
