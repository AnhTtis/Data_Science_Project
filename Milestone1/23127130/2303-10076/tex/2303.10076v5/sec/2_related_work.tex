\section{Related Work}

\subsection{Depth estimation}
% Depth estimation is another way for the 3D perception, and monocular depth estimation and stereo matching are common settings in autonomous driving \cite{wang2019pseudo}. 
For monocular depth estimation, it is usually implemented with a 2D U-Net architecture, as Figure \ref{fig:task} shown \cite{monodepth2,newcrfs}. More recently, the surrounding-view depth estimation has been explored, which is not just limited to a single image context \cite{fsm,surrounddepth}. FSM \cite{fsm} uses the spatio-temporal contexts, poses consistency constraints, and designed photometric loss to learn a single network generating dense and scale-aware depth map. Differently, Surrounddepth \cite{surrounddepth} adopts structure-from-motion to extract scale-aware pseudo depths to pretrain the models for the scale-aware result. This work mainly discusses 3D occupancy estimation, but we compare the depth metric with monocular depth estimation methods. 
% We would like to reveal the gap for different depth map generation (i.e. the depth map from volume rendering and from U-Net architecture). 

In terms of stereo matching, we could obtain the real scale depth map by estimating disparity between the stereo images. The state-of-the-art methods usually use 3D convolution neural networks (CNN) to do cost aggregation \cite{leastereo,cspn,xu2022attention,psmnet,HybirdNet}. Likewise, we also use the 3D CNN to do 3D volume aggregation for the final occupancy representation. 

% However, the 3D CNN cannot be used naively for the 3D volume space, and we will give detailed instructions in the method section. 

\subsection{BEV perception}
We identify that the perception task from the bird's eye view has a common step as the 3D occupancy estimation. Both of them require feature space transformation, where the BEV perception task is from the image space to the BEV plane \cite{bevsurvey}, and 3D occupancy estimation is from the image space to the 3D volume space. LSS \cite{lss} implicitly projects the image feature to the 3D space by learning the latent depth distribution. DETR3D \cite{detr3d} and BEVFormer \cite{bevformer} define a 3D query in the 3D space and use the transformer attention mechanism to query the 2D image feature. ImVoxelNet \cite{imvoxelnet} and Simple-BEV \cite{simplebev} build the 3D volume by the bilinear interpolation of the projection location at the 2D image feature plane, which is an efficient and parameter-free unprojection manner. Therefore, for simplicity in this baseline work, we adapt the parameter-free unprojection manner to build the 3D volume the same as Simple-BEV. 

\begin{figure*}
    \centering
    \includegraphics[width=0.99 \textwidth]{fig/Network_self.png}
    \vspace{0.2cm}
    \caption{\textbf{The overview of the proposed Simple 3D Occupancy estimation framework (SimpleOccupancy).} Given the surrounding image, we first extract the image feature by the shared 2D CNN and then use the parameter-free interpolation to obtain the 3D volume. The following 3D CNN could effectively aggregate the 3D feature in the volume space (Section \ref{3.2}). At last, we train the proposed network for both the supervised learning from the sparse point cloud and the self-supervised learning with photometric consistency loss. (Section \ref{3.4}).}
    % by the depth map loss on the rendered depth map or the classification loss based on the occupancy label (Section \ref{3.3} and \ref{3.4}).}
    \label{fig:Overview}
\end{figure*}


\subsection{Image-based 3D reconstruction}
There have been a series of methods for rendering-based 3D reconstruction \cite{scenerf, behindthescenes,volrecon,sat2density, streetsurf}. VolRecon \cite{volrecon} and StreetSurf \cite{streetsurf} use the signed distance function for the scene reconstruction, while VolRecon requires the multi-view image as the input and StreetSurf is not a generalizing method, only for the single scene reconstruction. Sat2density \cite{sat2density} uses volume rendering to transform the satellite image into the density to help the ground view synthesis. The recent works \cite{scenerf, behindthescenes} explore the volume rendering for the depth estimation in single-view image in the autonomous driving scene. Different from them, our task is more challenging for the 3D reconstruction from the surrounding-view image and we investigate the proposed framework in a more comprehensive manner, ranging from supervised and self-supervised learning, and also, jointly investigating the 3D occupancy estimation, depth estimation, and 3D reconstruction.

\subsection{Occupancy estimation}
There have been some works representing the scene in an occupancy (voxel) format \cite{occupancysurvey,mescheder2019occupancy,lange2022lopr, huang2023tri}. Recently, the industrial community \cite{tesla} reveals that the occupancy representation could be easily combined with the semantic information achieving instance-level prediction. Besides, it could predict the occupancy flow by considering the sequence frame, which could be regarded as the scene flow in the instance level \cite{sceneflow}. Following \cite{tesla}, this personal blog gives a try to use volume rendering to train the occupancy representation in a self-supervision manner, which can not reconstruct the dynamic object and requires the velocity information of the vehicle \cite{VoxelNet}. For the geometry of driving scene, MonoScene \cite{monoscene} and Voxformer \cite{voxformer} explore the 3D semantic scene completion from a single image. This approach proposes to reconstruct the road scene from a monocular image by the deep implicit function \cite{roddick2021road}. Different from \cite{monoscene,roddick2021road, voxformer}, we are under the surrounding-view setting, which is more challenging and meets the need for full perception in the driving scenarios. We need to point out that this baseline work is inspired by the above two works \cite{tesla,VoxelNet} from industrial community, where we contribute a new CNN-based framework, a novel evaluation metric and set up the benchmark for both the 3D occupancy metric and depth map metric in the public datasets \cite{ddad, nuscenes}. 
    
Upon completing this study, we discovered several relevant preprint works \cite{wei2023surroundocc,wang2023openoccupancy, tong2023scene}, exploring the 3D occupancy estimation in a surrounding-view setting. These works share a similar motivation of constructing dense voxel labels for semantic occupancy prediction, which involves merging multi-frame point clouds with human annotations to handle dynamic objects and extensive post-processing. In contrast, our focus is solely on pure geometry prediction without incorporating semantic information. Our method operates at a point level based on the sampling technique depicted in Figure \ref{fig:label}. This difference allows our work to do the 3D reconstruction at the mesh level and offers pretraining benefits to enhance the concurrent works, as presented in the experiment section. 
