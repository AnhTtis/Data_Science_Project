\documentclass[11pt,a4paper]{article}
%\usepackage{}
%\usepackage{amsfonts}
%\usepackage{mathrsfs}
%\usepackage{multirow}
\usepackage{makecell,booktabs}
%\usepackage{graphicx}
\usepackage{subfigure}
 \usepackage{epsfig}
 \usepackage{epstopdf}
%\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsbsy,amsmath,latexsym,amsfonts, epsfig, color, authblk, amssymb, graphics, bm, caption}
\usepackage{algorithm}
\usepackage{cases}
%\usepackage{cite}
\usepackage[colorlinks, citecolor=blue]{hyperref}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{malgorithm}{Algorithm}[section]
\newtheorem{aalgorithm}{Algorithm}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{condition}{Condition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{append}{Appendix}[section]
\newtheorem{alemma}{Lemma}
\newtheorem{atheorem}{Theorem}
\newtheorem{aproposition}{Proposition}
\newtheorem{aremark}{Remark}
\newtheorem{acorollary}{Corollary}
\newenvironment{aproof}{{\noindent}}{\hfill$\Box$\medskip}
\newenvironment{proof}{{\noindent \bf Proof:}}{\hfill$\Box$\medskip}

\definecolor{lred}{rgb}{1,0.8,0.8}
\definecolor{lblue}{rgb}{0.8,0.8,1}
\definecolor{dred}{rgb}{0.6,0,0}
\definecolor{dblue}{rgb}{0,0,0.5}
\definecolor{dgreen}{rgb}{0,0.5,0.5}

\title{An inexact LPA for DC composite optimization and application to matrix completions with outliers}

\author{Ting Tao\footnote{(\href{mailto:taoting@fosu.edu.cn}{taoting@fosu.edu.cn}) School of Mathematics, Foshan University, Foshan },\ \	
 Ruyu Liu\footnote{(\href{mailto:maruyuliu@mail.scut.edu.cn}{maruyuliu@mail.scut.edu.cn}) School of Mathematics, South China University of Technology, Guangzhou},\ \ {\rm and}\ \	 	
 Shaohua Pan\footnote{(\href{mailto:shhpan@scut.edu.cn}{shhpan@scut.edu.cn}) School of Mathematics, South China University of Technology}}
%--------------------------------------------------------------------------------------------------
 \begin{document}

 \maketitle

\begin{abstract}
	This paper concerns a class of DC composite optimization problems which, as an extension of convex composite optimization problems and DC programs with nonsmooth components, often arises in robust factorization models of low-rank matrix recovery. For this class of nonconvex and nonsmooth problems, we propose an inexact linearized proximal algorithm (iLPA) by computing in each step an inexact minimizer of a strongly convex majorization constructed with a partial linearization of their objective functions at the current iterate, and establish the convergence of the generated iterate sequence under the Kurdyka-\L\"ojasiewicz (KL) property of a potential function. In particular, by leveraging the composite structure, we provide a verifiable condition for the potential function to have the KL property of exponent $1/2$ at the limit point, so for the iterate sequence to have a local R-linear convergence rate. Finally, we apply the proposed iLPA to a robust factorization model for matrix completions with outliers and non-uniform sampling, and numerical comparison with the Polyak subgradient method confirms its superiority in terms of computing time and quality of solutions.
\end{abstract}

\noindent
{\bf Keywords:}\ DC composite optimization problems, inexact LPA, global convergence, KL property, matrix completions with outliers
%----------------------------------------------------------------------------------------
\section{Introduction}\label{sec1}

Let $\mathbb{X},\mathbb{Y}$ and $\mathbb{Z}$ be the Euclidean vector spaces with the inner product $\langle\cdot,\cdot\rangle$ and its induced norm $\|\cdot\|$, and let $\overline{\mathbb{R}}\!:=(-\infty,\infty]$ be the extended real number set. We are interested in the DC composite problem
\begin{equation}\label{prob}
 \min_{x\in\mathbb{X}}\,\Phi(x)\!:=\vartheta_1(F(x))-\vartheta_2(G(x))+h(x),
\end{equation}
where $F\!:\mathbb{X}\to\!\mathbb{Y}, G\!:\mathbb{X}\to\mathbb{Z},\vartheta_1\!:\mathbb{Y}\to\mathbb{R},\vartheta_2\!:\mathbb{Z}\to\mathbb{R}$ and $h\!:\mathbb{X}\to\overline{\mathbb{R}}$ satisfy the basic assumption:
\begin{assumption}\label{ass0}
\begin{enumerate}
 \item[(i)] $F$ and $G$ are differentiable on an open set $\mathcal{X}\supset{\rm dom}\,h$, and their differential mappings $F'$ and $G'$ are strictly continuous on $\mathcal{X}$;

  \item[(ii)] $\vartheta_1,\vartheta_2$ are lower semicontinuous (lsc) convex functions, $h$ is a proper lsc convex function that is continuous relative to ${\rm dom}\,h\ne\emptyset$;

  \item[(iii)] the function $\Phi$ is lower bounded, i.e., $\inf_{x\in\mathbb{X}}\Phi(x)>-\infty$.
\end{enumerate}
\end{assumption}

 Problem \eqref{prob} covers the case that $h$ is an indicator of a closed convex set though it is required to be continuous relative to its domain in Assumption \ref{ass0} (ii). This class of nonconvex and nonsmooth problems, as an extension of convex composite optimization problems \cite{Burke95,Fletcher82} and DC programs with nonsmooth components \cite{LeTai18,Pham97}, not only provides a unified framework for studying the theory of many important classes of optimization such as amenable optimization, convex conic optimization and convex inclusions (see \cite{BS00,RW98}), but also has extensive applications in many fields such as machine learning, statistics, financial optimization and telecommunication (see \cite{LeTai05,LeTai18}). For example, in machine learning, the robust factorization model of low-rank matrix recovery takes the form of \eqref{prob}:
\begin{align}\label{SCAD-loss}
 \min_{U\in\mathbb{R}^{n_1\times r},V\in\mathbb{R}^{n_2\times r}}\Phi(U,V):=	 \vartheta(\mathcal{A}(UV^{\top})-b)+\lambda(\|U\|_{2,1}+\|V\|_{2,1}),
\end{align}
 where $\vartheta:\mathbb{R}^m\to\mathbb{R}$ is a DC function to promote sparsity, $\mathcal{A}:\mathbb{R}^{n_1\times n_2}\to\mathbb{R}^{m}$ is a sampling operator, $b\in\mathbb{R}^{m}$ is an observation vector, $\|\cdot\|_{2,1}$ denotes the column $\ell_{2,1}$-norm of a matrix, and $\lambda>0$ is a regularization parameter. Such $\vartheta$ includes the SCAD, the MCP and the capped $\ell_1$-norm (see \cite{Fan01,Gong13,Zhang10}), which are shown to be the equivalent DC surrogates of the zero-norm \cite{ZhangPan22}. Due to the sparsity induced by $\vartheta$, the associated loss term $\vartheta(\mathcal{A}(\cdot)-b)$ copes well with outliers involved in the observation vector.

%-----------------------------------------------------------------------------------
\subsection{Related works}\label{sec1.1}

Model \eqref{prob} with $\vartheta_2\!\equiv 0$ and $h\!\equiv 0$ is the popular convex composite optimization problem \cite{Burke95,Fletcher82}. For this class of composite problems, the Gauss-Newton method is a classical one for which the global quadratic convergence of the iterate sequence was got in \cite{Burke95} by assuming that $C:=\mathop{\arg\min}_{y\in\mathbb{Y}}\vartheta_1(y)$ is a set
of weak sharp minima for $\vartheta_1$ and the cluster point is a regular point of inclusion $F(x)\in C$, and a similar convergence result was achieved under weaker conditions in \cite{Li02,Li07}. Another popular one, allowing $\vartheta_1$ to be extended real-valued and prox-regular, is the linearized proximal algorithm (LPA) proposed in \cite{Lewis16}. At each iterate, this method first performs a trial step by seeking a local optimal solution of a proximal linearized subproblem (that becomes strongly convex if $\vartheta_1$ is convex), and then derives a new iterate from the trial step by an efficient projection and/or other enhancements. Criticality of accumulation points under prox-regularity and identification under partial smoothness was studied in \cite{Lewis16}. Later, Hu et al. \cite{HuYang16} proposed a globalized LPA by using a backtracking line search, and obtained the global superlinear convergence of order ${2}/{p}$ with $p\in[1,2)$ for the iterate sequence by assuming that a cluster point $\overline{x}$ is a regular point of the inclusion $F(x)\in C$ with $C$ being the set of local weak sharp minima of order $p$ for $\vartheta_1$ at $F(\overline{x})$; and Pauwels \cite{Pauwels16} verified the convergence of the iterate sequence for the LPA with a backtracking search for the proximal parameters, under the definability of $F$ and $\vartheta_1$ in the same o-minimal structure of the real field and the twice continuous differentiability of $F$. In addition, for the standard NLP, i.e., model \eqref{prob} with $\vartheta_1=i_{\mathbb{R}_{-}^m}$ (the indicator of nonpositive orthant) and $\vartheta_2\!\equiv 0$, Botle and Pauwels \cite{Bolte16} proved that the iterate sequences for the moving balls method \cite{Auslender10}, the penalized SQP method and the extended SQP method converge to a KKT point, if all $F_i$ are semialgebraic and the (generalized) Mangasarian-Fromovitz constraint qualification (MFCQ) holds.

We note that almost all of the above methods require solving a convex or strongly convex program exactly in each step, which is impractical in computation. Although a practical inexact algorithm was proposed in \cite{Li02} (respectively, \cite{HuYang16}), its convergence analysis restricts a starting point in a neighborhood of a regular point (respectively, a quasi-regular point) of the inclusion $F(x)\!\in C$, which does not necessarily exist. This implies that, even for convex composite optimization problems, it is necessary to develop a globally convergent and practical algorithm.

Problem \eqref{prob} with $F\equiv\mathcal{I}\equiv G$ reduces to a standard DC program with nonsmoooth components. For this class of problems, a well-known method is the DC algorithm (DCA) of \cite{LeTai18,Pham97} that in each step linearizes the second DC component to yield a convex subproblem and uses its exact solution to define a new iterate; and another popular one is the proximal linearized method (PLM) of \cite{Nguyen17,Pang17,Souza16,Sun03} which can be regarded as a regularized variant of DCA because the convex subproblems are augmented with a proximal term to prevent tailing-off effect that makes calculations unstable as the iteration progresses. For the DCA, Le Thi et al. \cite{LeTai18Jota} justified the convergence of the iterate sequence by assuming that the objective function is subanalytic and continuous relative to its domain, and either of DC components is L-smooth around every critical point; and for the PLM, Nguyen et al. \cite{Nguyen17} achieved the same convergence result under the KL property of the objective function and the L-smoothness of the second DC component, or under the strong KL property of the objective function and the L-smoothness of the first DC component. To accelerate the DCA, Artacho et al. \cite{Artacho20} proposed a boosted DC algorithm (BDCA) with monotone line search by requiring the second DC component to be differentiable, and proved the convergence of the iterate sequence by assuming that the objective function has the strong KL property at critical points and the gradient of the second DC component is strictly continuous around the critical points. For problem \eqref{prob} with $\vartheta_1(t,y):=t+\theta(y),\, F\equiv(f;\mathcal{I})$ and $G\equiv\mathcal{I}$, where $\theta:\mathbb{X}\to\mathbb{R}$ is convex and $f:\mathbb{X}\to\mathbb{R}$ is an L-smooth function, Liu et al. \cite{LiuPong19} showed that the iterate sequence generated by the PLM with extrapolation is convergent under the KL property of a potential function, which removes the differentiability restriction on the second DC component in the convergence analysis of \cite{Nguyen17,Wen18}. In addition, for problem \eqref{prob} with $\vartheta_1(t,y)=t+i_{\mathbb{R}_{-}^m}(y)$, each $F_i\ (i=0,1,\ldots,m)$ being an L-smooth function and $G\equiv\mathcal{I}$, Yu et al. \cite{YuLu21} studied the monotone line search variant of the sequential convex programming (SCP) method in \cite{Lu12} by combining the idea of the moving balls method and that of the PLM, and proved that the iterate sequence converges to a stationary point of \eqref{prob} under the MFCQ and the KL property of a potential function if all $F_i\ (i=1,\ldots,m)$ are twice continuously differentiable and $\vartheta_2$ is Lipschitz continuously differentiable on an open set containing the stationary point set. Just recently, Le Thi et al. \cite{LeThi23} developed a DC composite algorithm for problem \eqref{prob} with $\vartheta_1$ and $\vartheta_2$ allowed to be extended-valued, which extends the LPA proposed in \cite{Lewis16} to a general DC composite problem. They achieved the convergence of the objective value sequence, and confirmed that every accumulation point $\overline{x}$ of the iterate sequence is a stationary point of \eqref{prob} by requiring that $\vartheta_1$ is continuous relative to its domain and $\vartheta_2$ is locally Lipschitz around $G(\overline{x})$.

It is worth pointing out that most of the above DC algorithms also require solving a (strongly) convex program exactly in each step, which is impractical unless the first DC component is simple. Although an inexact PLM was proposed in \cite{Oliveira19,Souza16},  the convergence of the iterate sequences was not obtained. For DC programs with the second DC component having a special structure, some enhanced proximal DC algorithms were proposed in  \cite{Dong21,Lu19,Pang17} to seek better d-stationary points, but they are inapplicable to large-scale DC programs such as model \eqref{SCAD-loss} because they need to compute at least one strongly convex program exactly in each step. An inexact enhanced proximal DC algorithm was also proposed in \cite{Lu19}, but the convergence of the iterate sequence was not established. Thus, even for DC programs with nonsmooth components, it is essential to develop an inexact PLM with global convergence.

This work aims to develop a globally convergent and practical algorithm for the DC composite problem \eqref{prob}. Since $F'$ and $G'$ are assumed to be strictly continuous, rather than globally Lipschitz continuous, on ${\rm dom}\,h$ in Assumption \ref{ass0} (ii), it cannot be reformulated as a DC program. Consequently, the DCAs, PLMs and SCP method mentioned above cannot be applied directly to \eqref{prob}. The term $\vartheta_2(G(x))$, catering to the outliers involved in the observation for low-rank matrix recovery, also hinders the direct applications of the above inexact LPAs and moving balls methods.
%-------------------------------------------------------------------------------
\subsection{Main contribution}\label{sec1.2}

The first contribution of this work is to propose an inexact LPA with a global convergence certificate for problem \eqref{prob}, which computes in each step an inexact minimizer of a strongly convex majorization constructed by the linearization of the inner $F$ and $G$ at the current iterate $x^k$ and the concave function $-\vartheta_2$ at $G(x^k)$. We justify that the generated iterate sequence converges to a stationary point in the sense of Definition  \ref{spoint-def} under Assumptions \ref{ass0}-\ref{ass1} and the KL property of a potential function $\Xi$ defined in \eqref{Xi-fun}. The stationary point in Definition \ref{spoint-def} is stronger than those yielded by the above DCAs, PLMs and SCP method. When $\vartheta_2\equiv 0$ and $h\equiv 0$, our iLPA is an inexact version of the composite Gauss-Newton method in \cite{Pauwels16}, so the obtained convergence results extends that of \cite{Pauwels16} via a different analysis technique. To the best of our knowledge, this is the first practical inexact LPA for DC composite problems to have the convergence certificate of iterate sequence. While the DC composite problem considered in \cite{LeThi23} is more general than model \eqref{prob}, their algorithm requires at each iterate solving exactly a (strongly) convex program and lacks the convergence guarantee of the iterate sequence even for model \eqref{prob}.


 The second contribution is to provide a verifiable condition for the KL property of the potential function $\Xi$ with exponent $p\in[1/2,1)$ at any critical point  $(\overline{x},\overline{x},\overline{z},\overline{\mathcal{Q}})$, by leveraging the KL property of exponent $p\in[1/2,1)$ for an almost separable nonsmooth function at $(F(\overline{x}),\overline{x},G(\overline{x}),\overline{z})$ and a condition on the subspace ${\rm Ker}([\nabla F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})])$ (see Proposition \ref{prop-KL}). When $\vartheta_2\equiv 0$ and $h\equiv 0$, the former is equivalent to requiring that $C$ is a set of local weak sharp minima of order $\frac{1}{1-p}$ for $\vartheta_1$ at $F(\overline{x})$ by \cite[Corollary 2.1]{BaiLi22}, while the latter subspace condition is weaker than the regularity used in \cite{Burke95,HuYang16}. Now there is no direct implication relation between this subspace condition and the quasi-regularity in \cite{HuYang16}; see the discussions in Remark \ref{remark-relation}. This result contributes to achieving the KL property of $\Xi$ with exponent $1/2$ for problem \eqref{prob} with piecewise linear quadratic (PLQ) convex $\vartheta_1$ and $\vartheta_2$.

 We apply the proposed iLPA with dPPASN (a solver developed in Section \ref{sec5} for subproblems) to the robust factorization model \eqref{SCAD-loss} from matrix completions with outliers and non-uniform sampling. Numerical comparisons with the Polyak subgradient method \cite{Charisopoulos21,LiZhu20} validate that our iLPA is superior to the subgradient method in terms of the computing time and the relative error for synthetic data or the normalized mean absolute error (NMAE) for the real jester, movie and netflix datasets. In particular, for $50000\times 17770$ netflix and $71567\times 10677$ movie-10M test examples, iLPA produces the favorable results in $3100$ seconds on a workstation running on 64-bit Windows Operating System with an Intel Xeon(R) W-2245 CPU 3.90GHz and 128 GB RAM.
%------------------------------------------------------------------------------------
\subsection{Notation}\label{sec1.3}

 We write $\Theta:=\Theta_1-\Theta_2$ with $\Theta_1:=\vartheta_1\circ F$ and $\Theta_2:=\vartheta_2\circ G$, and denote by $\mathcal{I}$ an identity mapping. For a linear mapping $\mathcal{B}:\mathbb{X}\to\mathbb{Y}$,
$\mathcal{B}^*:\mathbb{Y}\to\mathbb{X}$ denotes its adjoint. We say that a linear mapping $\mathcal{Q}:\mathbb{X}\to\mathbb{X}$ is positive semidefinite if it is self-adjoint and $\langle z,\mathcal{Q}z\rangle\ge 0$ for all $z\in\mathbb{X}$, and denote by $\mathbb{S}_{+}$ ($\mathbb{S}_{++}$) the set of all positive semidefinite (positive definite) linear mappings from $\mathbb{X}$ to $\mathbb{X}$. With a linear mapping $\mathcal{Q}\in\mathbb{S}_{+}$, let $\|z\|_{\mathcal{Q}}:=\sqrt{\langle z,\mathcal{Q}z\rangle}$ for $z\in\mathbb{X}$. For an integer $k\ge 1$, write $[k]:=\{1,\ldots,k\}$. If $F:\mathbb{X}\to\mathbb{Y}$ is strictly continuous at $x$, ${\rm lip}F(x)$ represents its Lipschitz modulus at $x$. For a set $\Omega\subset\mathbb{Y}$, ${\rm cl}(\Omega)$ denotes the closure of $\Omega$, ${\rm cone}(\Omega):=\{tx\,|\,x\in \Omega,t\ge 0\}$ means the cone generated by $\Omega$, and $i_{\Omega}$ denotes the indicator of $\Omega$. For given $x\in\mathbb{X}$ and $\varepsilon>0$, $\mathbb{B}(x,\varepsilon)$ denotes the closed ball centered at $x$ with radius $\varepsilon>0$. For a differentiable $H:\mathbb{X}\to\mathbb{Y}$, $\nabla H(x)$ denotes the adjoint of $H'(x)$, the differential mapping of $H$ at $x$, and if $H$ is twice differentiable at $x$, $D^2H(x)$ denotes the twice differential mapping of $H$ at $x$, and $D^2H(x)(u,\cdot)$ for $u\in\mathbb{X}$ is a linear mapping from $\mathbb{X}$ to $\mathbb{Y}$. For a proper function $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ and a point $x\in{\rm dom}f$, $\widehat{\partial}\!f(x)$ and $\partial\!f(x)$ respectively denote the regular and basic (limiting) subdifferential of $f$ at $x$. When $f=i_{\Omega}$ for a closed set $\Omega\subset\mathbb{X}$, $\partial\!f(x)=\mathcal{N}_{\Omega}(x)$, the normal cone to $\Omega$ at $x$. A vector $x$ is called a (limiting) critical point of $f$ if $0\in\partial\!f(x)$, and the set of all critical points of $f$ is denoted by ${\rm crit}f$.


%------------------------------------------------------------------------------------------
\section{Preliminaries}\label{sec2}

 Recall that a mapping $H:\mathcal{X}\subset\mathbb{X}\to\mathbb{Y}$ is strictly differentiable at $\overline{x}\in\mathcal{X}$ if for any $x,x'\to \overline{x}$,
 \[
   H(x')=H(x)+H'(\overline{x})(x'-x)+o(\|x'-x\|).
 \]
 By \cite[Theorem 9.18]{RW98}, when a function $f\!:\mathbb{X}\to(-\infty,\infty]$ is strictly differentiable at $\overline{x}\in{\rm dom}\,f$, it is (subdifferentially) regular at $\overline{x}$ with $\partial\!f(\overline{x})=\widehat{\partial}\!f(\overline{x})=\{\nabla\!f(\overline{x})\}$.
%------------------------------------------------------------------------------------
\subsection{Metric $q$-subregularity and KL property}\label{sec2.1}
 The metric $q\ (q>0)$-subregularity of a multifunction and the Kurdyka-{\L}ojasiewicz (KL) property of a nonsmooth function play a key role in the convergence analysis of algorithms. The former was used to analyze the convergence rate of proximal point algorithm for seeking a root to a maximal monotone operator \cite{LiMor12}, and the local superlinear convergence rates of proximal Newton-type methods for convex and nonsmooth composite optimization \cite{Mordu23,LiuPanWY22}. Its formal definition is stated as follows.
%----------------------------------------------------------------------------------------------
\begin{definition}\label{Def-subregular}
(see \cite[Definition 3.1]{LiMor12}) A multifunction $\mathcal{F}:\mathbb{X}\rightrightarrows\mathbb{X}$ is called (metrically) $q\ (q>0)$-subregular at a point $(\overline{x},\overline{y})\in{\rm gph}\mathcal{F}$, the graph of $\mathcal{F}$, if there exist $\varepsilon>0$ and $\kappa>0$ such that
\[
  {\rm dist}(x,\mathcal{F}^{-1}(\overline{y}))\le\!\kappa[{\rm dist}(\overline{y},\mathcal{F}(x))]^{q}\quad{\rm for\ all}\ x\in\mathbb{B}(\overline{x},\varepsilon).
 \]
 When $q=1$, this property is called the (metric) subregularity of $\mathcal{F}$ at $(\overline{x},\overline{y})$.
\end{definition}

 Obviously, if $\mathcal{F}$ is subregular at $(\overline{x},\overline{y})\in{\rm gph}\,\mathcal{F}$, it is $q\in(0,1]$-subregular at this point. When $\mathcal{F}$ is the subdifferential mapping of a class of nonconvex composite functions, its $q\in(0,1]$-subregularity at a point $(\overline{x},0)\!\in{\rm gph}\,\mathcal{F}$ is closely related to the KL property of exponent $1/(2q)$ of this class of composite functions (see \cite[Section 2.3]{LiuPanWY22}). To introduce the KL property of an extended real-valued function, for every $\eta\in(0,\infty]$, we denote by $\Upsilon_{\eta}$ the set consisting of all continuous concave $\varphi:[0,\eta)\to\mathbb{R}_{+}$ that is continuously differentiable on $(0,\eta)$ with $\varphi(0)=0$ and $\varphi'(s)>0$ for all $s\in(0,\eta)$.
%-------------------------------------------------------------------------------------------------
\begin{definition}\label{KL-def}
 A proper lsc function $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ is said to have the KL property at $\overline{x}\in{\rm dom}\,\partial\!f$ if there exist $\eta\in(0,\infty]$, $\varphi\in\Upsilon_{\!\eta}$ and a neighborhood $\mathcal{U}$ of $\overline{x}$ such that for all $x\in\mathcal{U}\cap\big[f(\overline{x})<f<f(\overline{x})+\eta\big]$,
 \[
  \varphi'(f(x)\!-\!f(\overline{x})){\rm dist}(0,\partial\!f(x))\ge 1.
 \]
 If $\varphi$ can be chosen as $\varphi(t)=ct^{1-p}$ with $p\in[0,1)$ for some $c>0$, then $f$ is said to have the KL property of exponent $p$ at $\overline{x}$. If $f$ has the KL property (of exponent $p$) at each point of ${\rm dom}\,\partial\!f$, it is called a KL function (of exponent $p$).
\end{definition}
%---------------------------------------------------------------------
\begin{remark}\label{KL-remark}
 By \cite[Lemma 2.1]{Attouch10}, a proper lsc function has the KL property of exponent $0$ at any non-critical point. Thus, to show that a proper lsc $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ is a KL function (of exponent $p$), it suffices to check its KL property (of exponent $p$) at critical points. The KL functions are extremely extensive by the discussions in \cite{Dries96} and \cite[Section 4]{Attouch09}, but it is not an easy task to verify the KL property of exponent $1/2$ except for some special class of nonsmooth functions; see \cite{LiPong18,YuLiPong21,WuPanBi21}.
\end{remark}
%------------------------------------------------------------------------------
\subsection{Stationary points of problem \eqref{prob}}\label{sec2.1}

 Before introducing the stationary point of problem \eqref{prob}, we provide a chain rule to characterize the subdifferential of composite functions, which improves the chain rule of \cite[Theorem 10.49]{RW98}.
%---------------------------------------------------------------------------------------------
\begin{lemma}\label{chain-rule}
 Let $f(x,z):=g(H(x,z))$ where $H:\mathbb{X}\times\mathbb{X}\to\mathbb{Y}$ is a mapping and $g:\mathbb{Y}\to\overline{\mathbb{R}}$ is a proper lsc function. Consider any $(\overline{x},\overline{z})\in{\rm dom}\,f$. Suppose that the mapping $H$ is strictly continuous at $(\overline{x},\overline{z})$, that $g$ is strictly continuous at $H(\overline{x},\overline{z})$, and that the multifunction $\mathcal{F}(x,z):=H(x,z)-{\rm dom}\,g$ is subregular at $((\overline{x},\overline{z}),0)$. Then, $\partial\! f(\overline{x},\overline{z})\subset D^*H(\overline{x},\overline{z})\partial\!g(H(\overline{x},\!\overline{z}))$ where $D^*H(\overline{x},\overline{z})$ denotes the coderivative of $H$ at $(\overline{x},\overline{z})$. If in addition $H$ is strictly differentiable at $(\overline{x},\overline{z})$ and $\partial g(H(\overline{x},\overline{z}))=\widehat{\partial}g(H(\overline{x},\overline{z}))$, it holds that
 \(
   \partial\! f(\overline{x},\overline{z})=\widehat{\partial}\!f(\overline{x},\overline{z})=\nabla H(\overline{x},\overline{z}){\partial}\!g(H(\overline{x},\overline{z})).
 \)
\end{lemma}
\begin{proof}
 Let $\widetilde{H}(x,z,\alpha):=(H(x,z);\alpha)$ for $(x,z,\alpha)\in\mathbb{X}\times\mathbb{X}\times\mathbb{R}$. Then, ${\rm epi}\,f=\widetilde{H}^{-1}({\rm epi}\,g)$. Since $\mathcal{F}$ is subregular at $((\overline{x},\overline{z}),0)$, and $H$ and $g$ are strictly continuous at $(\overline{x},\overline{z})$ and $\overline{w}=H(\overline{x},\overline{z})$, respectively, by \cite[Proposition 1]{LiuPan22} the multifunction $\widetilde{F}(x,z,\alpha)\!:=\widetilde{H}(x,z,\alpha)-{\rm epi}\,f$ is subregular at $((\overline{x},\overline{z},\overline{\alpha}),0)$ with $\overline{\alpha}=f(\overline{x},\overline{z})$. Then, by invoking \cite[Page 211]{Ioffe08} and the expression of $\widetilde{H}$, it follows that
 \[
  \mathcal{N}_{{\rm epi}f}(\overline{x},\overline{z},\overline{\alpha})\subset D^*\widetilde{H}(\overline{x},\overline{z},\overline{\alpha})\mathcal{N}_{{\rm epi}\,g}(\overline{w},\overline{\alpha})=\big\{(D^*H(\overline{x},\overline{z})\xi;\tau)\ |\ (\xi,\tau)\in\mathcal{N}_{{\rm epi}\,g}(\overline{w},\overline{\alpha})\big\}.
 \]	
 This, by \cite[Theorem 8.9]{RW98}, implies the desired inclusion.
 When $H$ is strictly differentiable at $(\overline{x},\overline{z})$, we have $D^*H(\overline{x},\overline{z})=\nabla H(\overline{x},\overline{z})$. By the definition of regular subdifferential (see \cite[Definition 8.3]{RW98}), $\widehat{\partial}\!f(\overline{x},\overline{z})\supset\nabla H(\overline{x},\overline{z})\widehat{\partial}g(H(\overline{x},\overline{z}))$. Along with the previous inclusion, we obtain the equality.
\end{proof}

Recall that ${\rm dom}\,\vartheta_1=\mathbb{Y}$ and ${\rm dom}\,\vartheta_2=\mathbb{Z}$. By invoking Lemma \ref{chain-rule} with $g=\vartheta_1$ and $H=F$, it follows that $\Theta_1$ is regular at any $x\in\mathbb{X}$ with $\partial\Theta_1(x)=\nabla\!F(x)\partial\vartheta_1(F(x))$, which by \cite[Exercise 10.10]{RW98} implies that $\partial(\Theta_1+h)(x)=\partial\Theta_1(x)+\partial h(x)$ for all $x\in{\rm dom}\,h$. Similarly, by invoking Lemma \ref{chain-rule} with $g=\vartheta_2$ and $H=G$, it follows that $\partial(-\Theta_2)(x)\subset\nabla\! G(x)\partial(-\vartheta_2)(G(x))$ for $x\in\mathbb{X}$. Thus, at any $x\in{\rm dom}\,\Phi$,
\[
 \partial\Phi(x)\subset \nabla\!F(x)\partial\vartheta_1(F(x))+\nabla\! G(x)\partial(-\vartheta_2)(G(x))+\partial h(x).
\]
This motivates us to introduce the following definition of stationary points for problem \eqref{prob}.
%----------------------------------------------------------------------------------------------
\begin{definition}\label{spoint-def}
 A vector $x\in\mathbb{X}$ is called a stationary point of problem \eqref{prob} if
 \[
   0\in\nabla\! F(x)\partial\vartheta_1(F(x))+\nabla\!G(x)\partial(-\vartheta_2)(G(x))+\partial h(x).
 \]
\end{definition}
\begin{remark}\label{remark-spoint}
 From \cite[Corollary 9.21]{RW98} and the strict continuity of $\vartheta_2$, it follows that
 \[
  -\partial(-\vartheta_2)(z)=\partial_{B}\vartheta_2(z):=\big\{\xi\in\mathbb{Z}\ |\ \exists z^k\to z\ {\rm with}\ \nabla\vartheta_2(z^k)\to\xi\big\}.
 \]
 Since the B-subdifferential $\partial_{B}\vartheta_2(z)$ is usually smaller than $\partial\vartheta_2(z)$, the stationary point in Definition \ref{spoint-def} is stronger than the one defined by replacing $\partial(-\vartheta_2)(G(x))$ with $-\partial\vartheta_2(G(x))$, while the latter is precisely the critical point used in the DC literature \cite{LeTai18Jota,LiuPong19,Nguyen17} for \eqref{prob} with $G=\mathcal{I}$ and $h\equiv 0$.
\end{remark}
	
To close this section, we state the relation between $\Theta_i$ for $i=1,2$ and its local linearization.
%----------------------------------------------------------------------------------------------
\begin{lemma}\label{vtheta1-lemma}
 Consider any $x\in {\rm dom}\,h$. For any $\varepsilon>0$, there exists $\delta>0$ such that for all $z\in\mathbb{B}(x,\delta)$,
 \begin{subnumcases}{}\label{Theta1-Lip}\!
  \big|\Theta_1(z)\!-\!\vartheta_1\big(F(x)\!+\!F'(x)(z\!-\!x)\big)\big|
   \!\le\!(1/2)({\rm lip}\,\vartheta_1(F(x))\!+\!\varepsilon)({\rm lip}\,F'(x)\!+\!\varepsilon)\|z\!-\!x\|^2,\\
  \label{Theta2-Lip}
 \!\big|\Theta_2(z)\!-\!\vartheta_2\big(G(x)\!+\!G'(x)(z\!-\!x)\big)\big|
 \!\le\!(1/2)({\rm lip}\,\vartheta_2(G(x))\!+\!\varepsilon)({\rm lip}\,G'(x)\!+\!\varepsilon)\|z\!-\!x\|^2.
\end{subnumcases}
\end{lemma}
\begin{proof}
 Fix any $\varepsilon>0$. By Assumption \ref{ass0} (ii), $\vartheta_1$ is strictly continuous at $F(x)$, so there is $\varepsilon_1>0$ such that
 \begin{equation}\label{ineq-vtheta1}
  |\vartheta_1(y)-\vartheta_1(y')|\le ({\rm lip}\,\vartheta_1(F(x))+\varepsilon)\|y-y'\|
  \quad\ \forall\, y,y'\in\mathbb{B}(F(x),\varepsilon_1).
 \end{equation}
 By Assumption \ref{ass0} (i), there exists $\delta>0$ such that $\mathbb{B}(x,\delta)\subset\mathcal{X}$ and for all $z\in\mathbb{B}(x,\delta)$,
 $\|F(z)-F(x)\|\le\varepsilon_1$ and $\|F'(x)(z-x)\|\le\varepsilon_1$, and furthermore, the strict continuity of $F'$ on $\mathcal{X}\supset{\rm dom}\,h$ implies that
 \begin{equation}\label{Lip-Fdiff}
  \|F'(z)-F'(z')\|\le({\rm lip}\,F'(x)+\varepsilon)\|z-z'\|\quad\ \forall\, z,z'\in\mathbb{B}(x,\delta).
 \end{equation}
 Now fix any $z\in\mathbb{B}(x,\delta)$. By invoking inequality \eqref{ineq-vtheta1} with $y=F(z)$ and $y'=F(x)+F'(x)(z-x)$,
 \begin{align*}
  |\Theta_1(z)\!-\!\vartheta_1(F(x)\!+\!F'(x)(z\!-\!x))|
  &\!\le \!({\rm lip}\,\vartheta_1(F(x))\!+\!\varepsilon)\|F(z)\!-\!F(x)-F'(x)(z\!-\!x)\|\\
  &\!=\!({\rm lip}\,\vartheta_1(F(x))\!+\!\varepsilon)\Big\|\int_{0}^{1}[F'(x\!+\!t(z\!-\!x))\!-\!F'(x)](z\!-\!x)dt\Big\|\\
  &\!\le\!\frac{1}{2}({\rm lip}\,\vartheta_1(F(x))+\varepsilon)({\rm lip}\,F'(x)+\varepsilon)\|z-x\|^2,
 \end{align*}	
 where the last inequality is due to \eqref{Lip-Fdiff} and $x+t(z-x)\in\mathbb{B}(x,\delta)$ for all $t\in[0,1]$. Inequality \eqref{Theta1-Lip} follows by the arbitrariness of $z\in\mathbb{B}(x,\delta)$. Using the same arguments results in inequality \eqref{Theta2-Lip}.
\end{proof}
%-------------------------------------------------------------------------------------------------
\section{Inexact linearized proximal algorithm}\label{sec3}

For any given $x\in\mathbb{X}$, let $\ell_{F}(\cdot,x)$ and $\ell_{G}(\cdot,x)$ be the linear approximation of $F$ and $G$ at $x$, respectively:
\begin{equation}\label{LinearFG}
 \ell_{F}(z,x):=F(x)+F'(x)(z-x)\ \ {\rm and}\ \ \ell_{G}(z,x):=G(x)+G'(x)(z-x)\quad\ \forall\,z\in\mathbb{X}.
\end{equation}
Let $x^k$ be the current iterate. Fix any $\varepsilon\!>\!0$. By invoking \eqref{Theta1-Lip} with $x\!=\!x^k$, for any $x$ close enough to $x^k$,
\begin{equation}\label{theta1-ineq0}
 \Theta_1(x)\le\vartheta_1(\ell_{F}(x,x^k))+\frac{1}{2}({\rm lip}\,\vartheta_1(F(x^k))+\varepsilon)({\rm lip}\,F'(x^k)+\varepsilon)\|x-x^k\|^2.
\end{equation}
Similarly, by invoking equation \eqref{Theta2-Lip}, for any $x$ sufficiently close to $x^k$,
\begin{equation}\label{theta2-ineq0}
 -\Theta_2(x)\le-\vartheta_2(\ell_{G}(x,x^k))+\frac{1}{2}({\rm lip}\,\vartheta_2(G(x^k))+\varepsilon)({\rm lip}\,G'(x^k)+\varepsilon)\|x-x^k\|^2.
 \end{equation}
Write ${\rm lip}\,\Theta(x^k):={\rm lip}\,\vartheta_1(F(x^k)){\rm lip}\,F'(x^k)+{\rm lip}\,\vartheta_2(G(x^k)){\rm lip}\,G'(x^k)$. Let $L_k$ be a constant close enough to ${\rm lip}\,\Theta(x^k)$ from above.  Pick any $\xi^k\in\partial(-\vartheta_2)(G(x^k))$. Then, from \eqref{theta1-ineq0}-\eqref{theta2-ineq0} and the expression of $\Theta$,
\begin{align}\label{Theta-ineq30}
  \Theta(x)&\le \vartheta_1(\ell_{F}(x,x^k))-\vartheta_2(\ell_{G}(x,x^k))+(L_k/2)\|x-x^k\|^2,\\
  &\le\vartheta_1(\ell_{F}(x,x^k))-\langle\nabla G(x^k)\xi^k,x-x^k\rangle+(L_k/2)\|x-x^k\|^2-\Theta_2(x^k)\nonumber
\end{align}
hold for all $x$ sufficiently close to $x^k$, where the second inequality is using the concavity of $-\vartheta_2$.
Thus, by choosing a positive definite (PD) linear operator $\mathcal{Q}_k\!:\mathbb{X}\to\mathbb{X}$ with $\mathcal{Q}_k\succeq L_{k}\mathcal{I}$, the following function
\[
  q_{k}(x):=\vartheta_1(\ell_{F}(x,x^k))+\langle\nabla G(x^k)\xi^k,x-x^k\rangle+h(x)+\frac{1}{2}\|x-x^k\|_{\mathcal{Q}_k}^2-\Theta_2(x^k)
  \quad\ \forall x\in\mathbb{X}
\]
is a locally strongly convex majorization of $\Phi=\Theta+h$ at $x^k$. Then, at the $k$th iterate, our inexact LPA seeks an inexact minimizer of $q_k$ as the next iterate. Since ${\rm lip}\,\Theta(x^k)$ is unknown, our method at each step captures its upper estimation $L_k$ via a simple backtrack, and its iterates are described as follows.
%--------------------------------------------------------------------------------------------------------

\begin{algorithm}[h]
\caption{\label{iLPA}{\bf\,(Inexact linearized proximal algorithm)}}
 {\bf{1.}} Initialization: Choose $\overline{\varrho}>1,\,0<\underline{\gamma}<\overline{\gamma},\,0<\mu<\underline{\gamma}/4$ and $x^0\in{\rm dom}\,h$.\\
 {\bf{2. For}} {$k=0,1,2,\ldots$} \\
 {\bf{3.   }}   \label{step3} \   Choose $\xi^{k}\in\partial(-\vartheta_2)(G(x^k))$ and $\gamma_{k,0}\in[\underline{\gamma},\overline{\gamma}]$.\\
 {\bf{4.    \ \    For}} {$j=0,1,2,\ldots$}	\\	
 {\bf{5.}} \label{step5} \ \ \     Choose a PD linear operator $\gamma_{k\!,\!j}\mathcal{I}\preceq\mathcal{Q}_{k\!,j}\!\!\preceq\!\overline{\varrho}\gamma_{k\!,\!j}\mathcal{I}$. Seek an inexact minimizer $x^{k,j}$ of
       \begin{equation}\label{subprobkj}
	\qquad\qquad\min_{x\in\mathbb{X}}q_{k,j}(x)\!:\!=\!\vartheta_1(\ell_{F}(x,x^k))\!+\!\langle\nabla G(x^k)\xi^k,x\!-\!x^k\rangle \!+\!h(x)\!+\!\frac{1}{2}\|x\!-\!x^k\|_{\mathcal{Q}_{k,j}}^2\!\!\!\!-\!\Theta_2(x^k)
       \end{equation}
     \qquad such that
      $q_{k,j}(x^{k,j})\!-\!q_{k,j}(\overline{x}^{k,j})\!\le\!\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2$, where $\overline{x}^{k,j}$ is the  solution of \eqref{subprobkj}.
			
 {\bf{6.\ \ \   If }} \label{step6} $\Theta(x^{k,j})\le\vartheta_1\big(\ell_{F}(x^{k,j},x^k)\big)-\vartheta_2\big(\ell_{G}(x^{k,j},x^k)\big)+\frac{1}{2}\|x^{k,j}-x^k\|_{\mathcal{Q}_{k,j}}^2$, go to step 9.

  \qquad Otherwise, let $\gamma_{k,j+1}=\overline{\varrho}\gamma_{k,j}$.\\	
   {\bf{7. \ \   EndFor}}\\		
  {\bf{8.}} Set $j_k=j$, $x^{k+1}=x^{k,j_k},\overline{x}^{k+1}=\overline{x}^{k,j_k}$ and $\mathcal{Q}_{k}=\mathcal{Q}_{k,j_k}$. If $x^{k+1}=x^k$, then stop.\\
{\bf{9. EndFor}}
 \end{algorithm}


\begin{remark}\label{remark-alg}
  {\bf(i)} The inner for-end loop aims at seeking a tight upper estimation for ${\rm lip}\,\Theta(x^k)$. As will be shown in Lemma \ref{ls-welldef1} below, the inner loop stops within a finite number of iterates and for each $k\in\mathbb{N}$ and $j\in\mathbb{N}$, the inexact minimizer $x^{k,j}$ exists, so Algorithm \ref{iLPA} is well defined. Clearly, $\{x^k\}_{k\in\mathbb{N}}\subset{\rm dom}\,h$.
 	
  \noindent
  {\bf(ii)} Algorithm \ref{iLPA} is an extension of the Gauss-Newton method \cite{Pauwels16} and the inexact LPA \cite[Algorithm 19]{HuYang16}, which are both proposed for problem \eqref{prob} with $\vartheta_2\equiv 0$ and $h\equiv 0$. Compared with the Gauss-Newton method \cite{Pauwels16}, Algorithm \ref{iLPA} is practical because its strongly convex subproblems are allowed to be solved inexactly and the inexact criterion  $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})\le\frac{\mu}{2}\|x^{k,j}-x^{k}\|^2$ is implementable by noting that the unknown $q_{k,j}(\overline{x}^{k,j})$ can be replaced by its lower estimation, say,  the dual objective value of \eqref{subprobkj}. Different from the inexact LPA of \cite{HuYang16}, our inexact criterion controls $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})$ by the quadratic term $\|x^{k,j}\!-\!x^{k}\|^2$ rather than the more restrictive $\|x^{k-1}\!-\!x^{k}\|^{\alpha}$ with $\alpha>2$. In addition, Algorithm \ref{iLPA} also extends the proximal DC algorithms proposed in \cite{LiuPong19} for problem \eqref{prob} with $F\equiv(f;\mathcal{I})$ and $G\equiv\mathcal{I}$ without requiring $\nabla\!f$ to be globally Lipschitz continuous.
	
  \noindent
  {\bf(iii)} For some $k\in\mathbb{N}$, if $x^{k,j_k}=x^{k+1}=x^{k}$, then from the strong convexity of $q_{k,j}$ with modulus not less than $\underline{\gamma}/2$, we have $\underline{\gamma}\|x^{k+1}-\overline{x}^k\|^2\le\mu\|x^{k+1}-x^k\|^2=0$ and then $x^{k+1}=\overline{x}^k$. Together with $0\in\partial q_{k,j_k}(\overline{x}^k)=\partial q_{k,j_k}(x^{k})$ and Definition \ref{spoint-def}, it follows that $x^{k}$ is a stationary point of problem \eqref{prob}. This interprets why Algorithm \ref{iLPA} stops when $x^{k+1}=x^k$ occurs in step 9.
\end{remark}
%-----------------------------------------------------------------------------------------
\begin{lemma}\label{ls-welldef1}
 Fix any $k$ such that $x^k$ is not a stationary point of \eqref{prob}. Then, under Assumption \ref{ass0},
 \begin{enumerate}
  \item[(i)] for each $j\in\mathbb{N}$, any $z\in{\rm dom}\,h$ close enough to $\overline{x}_{k,j}$ satisfies $q_{k,j}(z)-q_{k,j}(\overline{x}^{k,j})\le \frac{\mu}{2}\|x^{k,j}-x^k\|^2$;

  \item[(ii)] the inner loop of Algorithm \ref{iLPA} stops within a finite number of steps.
 \end{enumerate}
\end{lemma}
\begin{proof}
 {\bf(i)} Fix any $j\in\mathbb{N}$. Note that $x^k\ne\overline{x}^{k,j}$. If not, $0\in\partial q_{k,j}(x^k)$, which by the expression of $q_{k,j}$ and Definition \ref{spoint-def} implies that $x^k$ is a stationary point of problem \eqref{prob}. Consider the function $\psi(z):=q_{k,j}(z)-q_{k,j}(\overline{x}^{k,j})-(\mu/2)\|z-x^{k}\|^2$ for $z\in\mathbb{X}$. Clearly, $\psi(\overline{x}^{k,j})=-(\mu/2)\|\overline{x}^{k,j}-x^{k}\|^2<0$. Recall that $h$ is continuous relative to its domain. Hence, $\psi$ is continuous relative to ${\rm dom}\,h$. Thus, for any $z\in{\rm dom}\,h$ close enough to $\overline{x}^{k,j}$, it holds that $\psi(z)<0$, and the desired conclusion then follows.

 \noindent
 {\bf(ii)} Suppose on the contrary that the conclusion does not hold. Then, for all sufficiently large $j$,
  \begin{equation}\label{aim-ineq1}
   \Theta(x^{k,j})-\vartheta_1(\ell_{F}(x^{k,j},x^k))+\vartheta_2(\ell_{G}(x^{k,j},x^k))>(1/2)\|x^{k,j}-x^k\|_{\mathcal{Q}_{k,j}}^2.
  \end{equation}
  From the definition of $x^{k,j}$ in the inner loop and the expression of $q_{k,j}$, for each $j$ large enough,
  \begin{align}\label{temp-ineq31}
   q_{k,j}(\overline{x}^{k,j})+(\mu/2)\|x^{k,j}-x^{k}\|^2
  &\ge\vartheta_1(\ell_{F}(x^{k,j},x^{k}))\!+\!\langle\nabla G(x^k)\xi^k,x^{k,j}\!-\!x^k\rangle\nonumber\\
  &\quad+h(x^{k,j})+(1/2)\|x^{k,j}\!-\!x^k\|_{\mathcal{Q}_{k,j}}^2-\Theta_2(x^k).
  \end{align}
  For the proper and lsc convex function $h$, as ${\rm dom}\,h\ne\emptyset$, we have ${\rm ri}({\rm dom}\,h)\ne\emptyset$. Let $\widehat{x}\in{\rm ri}({\rm dom}\,h)$. From \cite[Theorem 23.4]{Roc70}, it follows that $\partial h(\widehat{x})\ne\emptyset$. Pick any $\widehat{v}\in\partial h(\widehat{x})$. Then,
 \begin{equation}\label{hineq}
  h(x)\ge h(\widehat{x})+\langle\widehat{v},x-\widehat{x}\rangle\quad\ \forall\,x\in{\rm dom}\,h.
 \end{equation}
  In addition, from the finite convexity of $\vartheta_1$, $\partial\vartheta_1(F(\widehat{x}))\ne\emptyset$. Pick any $\widehat{\zeta}\in\partial\vartheta_1(F(\widehat{x}))$. Then,
  \begin{equation}\label{vtheta1-ineq}
   \vartheta_1(y)\ge\vartheta_1(F(\widehat{x}))+\langle\widehat{\zeta},y-F(\widehat{x})\rangle\quad\ \forall y\in\mathbb{Y}.
  \end{equation}
  For each $j$ large enough, combining \eqref{vtheta1-ineq} with $y=\ell_{F}(x^{k,j},x^k)$, \eqref{hineq} with $x=x^{k,j}$, and \eqref{temp-ineq31} leads to
  \begin{align*}
  \Phi(x^k)\ge q_{k,j}(\overline{x}^{k,j})
  &\ge \vartheta_1(F(\widehat{x}))+\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle+\langle\nabla\!F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k, x^{k,j}-x^k\rangle\\
  &\quad+h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle+\frac{1}{2}\|x^{k,j}-x^k\|_{Q_{k,j}}^2-\frac{\mu}{2}\|x^{k,j}-x^{k}\|^2-\!\Theta_2(x^k),
 \end{align*} 	
 where the first inequality is due to $\Phi(x^k)=q_{k,j}(x^k)\ge q_{k,j}(\overline{x}^{k,j})$ for each $j\in\mathbb{N}$. Recall that $\mathcal{Q}_{k,j}\succeq\gamma_{k,0}\mathcal{I}\succeq\underline{\gamma}\mathcal{I}$ and $\underline{\gamma}>4\mu$. The last inequality implies that $x^{k,j}\to x^k$ as $j\to\infty$. Consequently, by invoking inequality \eqref{Theta-ineq30} with $x=x^{k,j}$, it follows that for all sufficiently large $j$,
 \begin{equation*}
  \Theta(x^{k,j})-\vartheta_1\big(\ell_{F}(x^{k,j},x^k)\big)+\vartheta_2\big(\ell_{G}(x^{k,j},x^k)\big)\le(L_k/2)\|x^{k,j}-x^k\|^2,
 \end{equation*}
 which clearly contradicts the above \eqref{aim-ineq1} because $L_k\mathcal{I}\prec\mathcal{Q}_{k,j}$ when $j$ is large enough.
\end{proof}
%--------------------------------------------------------------------------------------------
\section{Convergence analysis of Algorithm \ref{iLPA}}\label{sec4}
%---------------------------------------------------------------------------------------
 To analyze the convergence of Algorithm \ref{iLPA}, we need to construct an appropriate potential function. Let $\mathbb{W}:=\mathbb{X}\times\mathbb{X}\times\mathbb{Z}\times\mathbb{S}_{+}$. Inspired by the work \cite{LiuPong19}, for each $w=(x,s,z,\mathcal{Q})\in\mathbb{W}$, we define
\begin{equation}\label{Xi-fun}
 \Xi(w):=\vartheta_1(\ell_{F}(x,s))+\langle \ell_{G}(x,s),z\rangle+h(x)
	+\vartheta_2^*(-z)+\|x-s\|_{\mathcal{Q}}^2+i_{\mathbb{S}_{+}}(\mathcal{Q}),
\end{equation}
where $\vartheta_2^*$ denotes the conjugate function of $\vartheta_2$, i.e., $\vartheta_2^*(z^*)=\sup_{z\in\mathbb{Z}}\{\langle z^*,z\rangle-\vartheta_2(z)\}$.
The following lemma characterizes the subdifferential of the potential function $\Xi$ at any $w\in{\rm dom}\,\Xi$.
%------------------------------------------------------------------------------------
\begin{lemma}\label{subdiff-LemXi}
 Fix any $w=(x,s,z,\mathcal{Q})\in{\rm dom}\,h\times{\rm dom}\,h\times(-{\rm dom}\,\vartheta_2^*)\times\mathbb{S}_{+}$. Suppose that $F'$ is strictly differentiable at $x$ and $G'$ is strictly differentiable at $x$. Then  $\partial\Xi(w)=T_1(w)\times T_2(w)\times T_3(w)$ with
 \begin{align*}
 T_1(w)&=\left(\begin{matrix}
          \nabla\!F(s)\\
 	[D^2F(s)(x-s,\cdot)]^*\\	
      \end{matrix}\right)\partial\vartheta_1(\ell_{F}(x,s))
      +\left(\begin{matrix}
        \nabla G(s)z+\partial h(x)+2\mathcal{Q}(x-s)\\
	[D^2G(s)(x-s,\cdot)]^*z+2\mathcal{Q}(s-x)\\
      \end{matrix}\right),\\
 T_2(w)&=\ell_{G}(x,s)-\partial\vartheta_2^*(-z)\ \ {\rm and}\ \ T_3(w)=(s-x)(s-x)^{\top}+\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}).	 	
 \end{align*}
\end{lemma}
\begin{proof}
 Let $\psi(x',s'):=\vartheta_1(\ell_{F}(x',s'))$ for $(x',s')\in\mathbb{X}\times\mathbb{X}$. For any $w'=(x',s',z',\mathcal{Q}')\in\mathbb{W}$, define
 \[
  f(w'):=\psi(x',s')+h(x')+\vartheta_2^*(-z')+i_{\mathbb{S}_{+}}(\mathcal{Q}')\ \ {\rm and}\ \ g(w'):=\langle\ell_{G}(x',s'),z'\rangle+\|x'-s'\|_{\mathcal{Q}'}^2.
 \]
  Clearly, $\Xi=f+g$. Recall that $\vartheta_1$ is strictly continuous at $\ell_{F}(x,s)$ and ${\rm dom}\,\vartheta_1=\mathbb{Y}$. By Lemma \ref{chain-rule},  $\widehat{\partial}\psi(x,s)=\partial\psi(x,s)=\nabla\ell_{F}(x,s)\partial\vartheta_1(\ell_{F}(x,s))$. Due to the strict differentiability of $F'$ at $x$, the function $\psi$ is strictly continuous at $w$.
  By invoking \cite[Exercise 10.10 \& Corollary 10.5]{RW98}, it holds that
 \begin{equation}\label{temp-equa40}
   \partial\!f(w)=\widehat{\partial}\!f(w) =\partial\psi(x,s)\times\{0\}\times\{0\}+\partial h(x)\times\{0\}\times[-\partial\vartheta_2^*(-z)]\times\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}).
 \end{equation}
 Note that the strict differentiablity of $G'$ at $x$ implies that of $g$ at $w$, so $\partial\Xi(w)=\partial\!f(w)+\nabla g(w)$ and $\widehat{\partial}\Xi(w)=\widehat{\partial}\!f(w)+\nabla g(w)$.
 Along with $\partial\!f(w)=\widehat{\partial}\!f(w)$, the function $\Xi$ is regular at $w$. The second part follows by combining  \eqref{temp-equa40} with $\partial\psi(x,s)\!=\nabla\ell_{F}(x,s)\partial\vartheta_1(\ell_{F}(x,s))$ and the expression of $g$.
\end{proof}

 In the rest of this section, we let $\{(x^k,\overline{x}^k,\xi^k,\mathcal{Q}_{k})\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}, and assume that each $x^k$ is not a stationary point of \eqref{prob}. For each $k\in\mathbb{N}$, write $w^{k}:=(x^{k-1},\overline{x}^{k},\xi^{k-1},\mathcal{Q}_{k-1})$. From the iterate steps of Algorithm \ref{iLPA}, obviously, $\{w^k\}_{k\in\mathbb{N}}\subset{\rm dom}\,h\times{\rm dom}\,h\times(-{\rm dom}\,\vartheta_2^*)\times\mathbb{S}_{++}$.
%------------------------------------------------------------------------------------------
\subsection{Global convergence of Algorithm \ref{iLPA}}\label{sec4.1}
%--------------------------------------------------------------------------------------
From step \ref{step3}, for each $k\in\mathbb{N}$, $\xi^{k}\in\partial(-\vartheta_2)(G(x^k))\subset-\partial\vartheta_2(G(x^k))$, where the second inclusion is due to the finite convexity of $\vartheta_2$ and \cite[Corollary 9.21]{RW98}. Together with \cite[Theorem 23.5]{Roc70}, for each $k\in\mathbb{N}$,
\begin{subnumcases}{}\label{pre-equa0}	
 \vartheta_2(G(x^{k}))+\vartheta_2^*(-\xi^{k})=-\langle\xi^{k},G(x^{k})\rangle,\\
 \label{pre-equa1}  	
 -\vartheta_2(\ell_{G}(x^k,x^{k-1}))\le-\vartheta_2(G(x^{k-1}))+\langle\nabla G(x^{k-1})\xi^{k-1},x^{k}-x^{k-1}\rangle.
\end{subnumcases}
 In addition, from the definitions of $x^k$ and $\overline{x}^k$ and the strong convexity of $q_{k,j_k}$, for each $k\in\mathbb{N}$,
 \begin{equation}\label{pre-equa3}
  \Phi(x^k)=q_{k,j_k}(x^k)\ge q_{k,j_k}(\overline{x}^{k+1})+(1/2)\|\overline{x}^{k+1}-x^k\|_{\mathcal{Q}_k}^2.
 \end{equation}
 Next we will use equations \eqref{pre-equa0}-\eqref{pre-equa3} to establish the convergence of $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$.
%-----------------------------------------------------------------------------------------
\begin{proposition}\label{prop1-xk}
 For the sequence $\{w^k\}_{k\!\in\mathbb{N}}$, the following three statements hold under Assumption \ref{ass0}.
 \begin{enumerate}
 \item [(i)] For each $k\in\mathbb{N}$, $\Xi(w^{k+1})\le\Xi(w^{k})-(\underline{\gamma}/4-\mu)\|x^k-x^{k-1}\|^2$.
		
 \item[(ii)] For each $k\in\mathbb{N}$, $\Phi(x^k)+(\underline{\gamma}/4-\mu)\|x^k-x^{k-1}\|^2\le\Xi(w^k)\leq \Phi(x^{k-1})$.
		
 \item[(iii)] The sequences $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ and $\{\Xi(w^{k})\}_{k\in\mathbb{N}}$ are nonincreasing and convergent.
 \end{enumerate}
\end{proposition}
\begin{proof}
{\bf(i)} Fix any $k\in\mathbb{N}$. From step \ref{step6} of Algorithm \ref{iLPA} and inequality \eqref{pre-equa1}, it follows that
 \begin{align*}%\label{descent-eq1}
 \Theta(x^{k})
  &\le\vartheta_1(\ell_{F}(x^k,x^{k-1}))-\vartheta_2(\ell_{G}(x^k,x^{k-1}))+(1/2)\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
  &\le\vartheta_1(\ell_{F}(x^k,x^{k-1}))-\Theta_2(x^{k-1})+
  \langle\nabla G(x^{k-1})\xi^{k-1},x^k-x^{k-1}\rangle+(1/2)\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
 \end{align*}
 In addition, from inequality \eqref{pre-equa3} and the expression of $q_{k,j_k}$, we have that
 \[
  \Phi(x^{k})\ge\vartheta_1(\ell_{F}(\overline{x}^{k+1},x^k))+\langle\nabla G(x^k)\xi^k,\overline{x}^{k+1}-x^k\rangle+h(\overline{x}^{k+1})+\|\overline{x}^{k+1}-x^k\|_{\mathcal{Q}_k}^2-\vartheta_2(G(x^k)).
 \]
 Combining the last two inequalities with $\Phi(x^{k})=\Theta(x^{k})+h(x^k)$ yields that
 \begin{align*}
 &\vartheta_1(\ell_{F}(\overline{x}^{k+1},x^k))+\langle\nabla G(x^k)\xi^k,\overline{x}^{k+1}-x^k\rangle+h(\overline{x}^{k+1})+\|\overline{x}^{k+1}-x^k\|_{\mathcal{Q}_k}^2-\vartheta_2(G(x^k))\\
 &\le \vartheta_1\big(\ell_{F}(x^k,x^{k-1})\big)+\langle\nabla G(x^{k-1})\xi^{k-1},x^k-x^{k-1}\rangle+h(x^k)+\frac{1}{2}\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-\Theta_2(x^{k-1}).
 \end{align*}
 Together with the expression of $\Xi(w^{k+1})$ and equality \eqref{pre-equa0}, it then follows that
 \begin{align}
  \Xi(w^{k+1})&\le\vartheta_1\big(\ell_{F}(x^k,x^{k-1})\big)+\langle\nabla G(x^{k-1})\xi^{k-1},x^k\!-\!x^{k-1}\rangle+h(x^k)+\frac{1}{2}\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-\Theta_2(x^{k-1})\nonumber\\
 \label{final-ineq40}
 &=q_{k-1,j_{k-1}}(x^{k})\le q_{k-1,j_{k-1}}(\overline{x}^k)+(\mu/2)\|x^{k}-x^{k-1}\|^2\\
 &=\vartheta_1\big(\ell_{F}(\overline{x}^k,x^{k-1})\big)+\langle\nabla G(x^{k-1})\xi^{k-1},\overline{x}^k-x^{k-1}\rangle-\Theta_2(x^{k-1})+h(\overline{x}^k)\nonumber\\
 &\quad+(1/2)\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2+(\mu/2)\|x^{k}-x^{k-1}\|^2\nonumber\\
 \label{temp-ineq40}
 &=\Xi(w^{k})-(1/2)\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2+(\mu/2)\|x^{k}-x^{k-1}\|^2,
 \end{align}
 where the second inequality is using step \ref{step5} of Algorithm \ref{iLPA} and the expression of $q_{k-1,j_{k-1}}$, and the last equality is using \eqref{pre-equa0} and the expression of $\Xi(w^k)$. From Cauchy-Schwarz inequality, we have
 \begin{align*}
 \frac{1}{4}\|{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2
 &=\frac{1}{4}\|{x}^k-\overline{x}^k\|_{\mathcal{Q}_{k-1}}^2
  +\frac{1}{4}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2
  +\frac{1}{2}\langle {x}^k-\overline{x}^k,\mathcal{Q}_{k-1}(\overline{x}^k-x^{k-1})\rangle\\
 &\le \frac{1}{2}\|{x}^k-\overline{x}^k\|_{\mathcal{Q}_{k-1}}^2+\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
 \end{align*}
 In addition, from the strong convexity of $q_{k-1,j_{k-1}}$ and inequality \eqref{final-ineq40}, it follows that
 \begin{equation}\label{key-ineq40}
  \frac{1}{2}\|\overline{x}^k-x^{k}\|_{\mathcal{Q}_{k-1}}^2
 \le q_{k-1,j_{k-1}}(x^{k})-q_{k-1,j_{k-1}}(\overline{x}^{k})\le\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2.
 \end{equation}
 Then, from the last two equations, we immediately obtain that
 \begin{equation}\label{key-ineq41}
  \frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2  \ge-\frac{\mu}{2}\|{x}^{k}-x^{k-1}\|^2+\frac{1}{4}\|{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
 \end{equation}
 Substituting this inequality into \eqref{temp-ineq40} and noting that $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ leads to the desired inequality.
	
 \noindent
 {\bf(ii)} Fix any $k\in\mathbb{N}$. From the last two equalities in \eqref{temp-ineq40} and inequality \eqref{pre-equa3}, we have
 \[
  \Xi(w^{k})
   =q_{k-1,j_{k-1}}(\overline{x}^{k})+(1/2)\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\leq \Phi(x^{k-1}).
 \]
 In addition, from inequality \eqref{final-ineq40}, it follows that
 \begin{align*}
  q_{k-1,j_{k-1}}(\overline{x}^{k})
  &\ge\vartheta_1(\ell_{F}({x}^{k},x^{k-1}))-\vartheta_2(G(x^{k-1}))+\langle\nabla G(x^{k-1})\xi^{k-1},x^{k}-x^{k-1}\rangle\\
  &\quad+h(x^k)+\!(1/2)\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-(\mu/2)\|{x}^{k}-x^{k-1}\|^2\\
  &\ge\Phi(x^k)-(\mu/2)\|{x}^{k}\!-\!x^{k-1}\|^2,
  \end{align*}
  where the last inequality is using \eqref{pre-equa1} and step \ref{step3}. From the last two inequalities,
 \[
  \Phi(x^k)-({\mu}/{2})\|{x}^{k}-x^{k-1}\|^2+({1}/{2})\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\le\Xi(w^{k})\le\Phi(x^{k-1}),
 \]
 which together with \eqref{key-ineq41} and $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ implies the desired inequality.
	
 \noindent
 {\bf(iii)} The nonincreasing of the sequences $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ are due to $\mu<\underline{\gamma}/4$ and parts (i)-(ii). Recall that $\Phi$ is lower bounded by Assumption \ref{ass0} (iii). From part (ii) and $\mu<\underline{\gamma}/4$, the sequence $\{\Xi(w^{k})\}_{k\in\mathbb{N}}$ is lower bounded. Then, the sequences $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ are convergent.
\end{proof}

 To establish the convergence of the sequence $\{w^k\}_{k\in\mathbb{N}}$, we need the following assumption. Among others, Assumption \ref{ass1} (i) is rather weak because it requires $\Phi$ to have a bounded level set only associated to $\Phi(x^0)$ instead of any real number, and Assumption \ref{ass1} (ii) requires the strict differentiability of $F'$ and $G'$ on an open set $\mathcal{O}\supset\Gamma^*$ which is a little stronger than the $C^{1,1}$ assumption on $F$ in \cite{LeThi23}.
%-----------------------------------------------------------------------------------------
\begin{assumption}\label{ass1}
\begin{enumerate}
 \item[(i)] The level set $\mathcal{L}_{\Phi}(x^0):=\{x\in\mathbb{X}\ |\ \Phi(x)\le\Phi(x^0)\}$ is bounded.

 \item[(ii)] Let $\Gamma^*$ be the cluster point set of $\{w^k\}_{k\in\mathbb{N}}$. There exists an open set $\mathcal{O}\supset\Gamma^*$ such that $F'$ and $G'$ are both strictly differentiable on $\mathcal{O}_{x}$, where for a given $\Gamma\subset\mathbb{W}$ the sets $\Gamma_{\!x}$ and $\Gamma_{\!z}$ denote the projection of the set $\Gamma$ onto $\mathbb{X}$ and $\mathbb{Z}$, respectively.
\end{enumerate}
\end{assumption}
%------------------------------------------------------------------------------------------
\begin{proposition}\label{prop2-xk}
 For the sequence $\{w^k\}_{k\in\mathbb{N}}$, under Assumptions \ref{ass0} and \ref{ass1} (i) the following results hold.
 \begin{enumerate}
 \item[(i)] The sequence $\{\gamma_k\}_{k\in\mathbb{N}}$ with $\gamma_k:=\gamma_{k,j_k}$ is bounded, so is the sequence $\{\|\mathcal{Q}_k\|\}_{k\in\mathbb{N}}$.
				
 \item[(ii)] The sequence $\{w^{k}\}_{k\in\mathbb{N}}$ is bounded, and $\Gamma^*$ is a nonempty compact set.
		
 \item[(iii)] For each $\widehat{w}=(\widehat{x},\widehat{s},\widehat{\xi},\widehat{\mathcal{Q}})\in\Gamma^*$, it holds that $\widehat{x}=\widehat{s}$, $\widehat{x}$ is a stationary point of problem \eqref{prob}, and $\Xi(\widehat{w})=\lim_{k\to\infty}\Xi(w^k):=\overline{\Xi}$. If in addition Assumption \ref{ass1} (ii) holds, $\Gamma^*\subset{\rm crit}\,\Xi$.
\end{enumerate}
\end{proposition}
\begin{proof}
 We first argue that the sequence $\{(x^{k},\overline{x}^k,\xi^k)\}_{k\in\mathbb{N}}$ is bounded. Indeed, by Proposition \ref{prop1-xk} (iii), $\{x^k\}_{k\in\mathbb{N}}\subset\mathcal{L}_{\Phi}(x^0)$, so the boundedness of $\{x^k\}_{k\in\mathbb{N}}$ follows by Assumption \ref{ass1} (i). Recall that $\xi^k\in\partial(-\vartheta_2)(G(x^k))$ for each $k$ and $-\vartheta_2$ is strictly continuous. Combining the boundedness of $\{x^k\}_{k\in\mathbb{N}}$ and \cite[Theorem 9.13 \& Proposition 5.15]{RW98} (d), we achieve the boundedness of $\{\xi^k\}_{k\in\mathbb{N}}$. By combining \eqref{key-ineq40} with $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ and the boundedness of $\{x^k\}_{k\in\mathbb{N}}$, we deduce that $\{\overline{x}^k\}_{k\in\mathbb{N}}$ is bounded.

 \noindent
 {\bf(i)} Suppose on the contrary that the sequence $\{\gamma_k\}_{k\in\mathbb{N}}$ is unbounded. Then there exists an index set $K\subset\mathbb{N}$ such that $\lim_{K\ni k\to\infty}\gamma_k=\infty$. For each $k\in\mathbb{N}$, let $\widetilde{\gamma}_k=\overline{\varrho}^{-1}\gamma_k=\gamma_{k,j_k-1}$. From the inner for-end loop of Algorithm \ref{iLPA} and $\mathcal{Q}_{k,j_k-1}\succeq \widetilde{\gamma}_{k}\mathcal{I}$, it immediately follows that for each $k\in K$,
 \begin{equation}\label{aim-ineq40}
  \Theta(x^{k,j_k-1})
  >\vartheta_1\big(\ell_{F}(x^{k,j_k-1},x^k)\big)-\vartheta_2\big(\ell_{G}(x^{k,j_k-1},x^k)\big)+({\widetilde{\gamma}_k}/{2})\|x^{k,j_k-1}-x^k\|^2.
 \end{equation}
 For each $k\in K$, $x^{k,j_k-1}\ne x^{k}$. If not, by step \ref{step5} of Algorithm \ref{iLPA}, $x^k=x^{k,j_k-1}=\overline{x}^{k,j_k-1}$, which implies that $0\in\partial q_{k,j_k-1}(x^k)$ and then $x^k$ is a stationary point.
 Next we shall claim that $\lim_{K\ni k\rightarrow\infty}\|x^{k,j_k-1}-x^{k}\|=0$.  Fix any $k\in K$. From the optimality of $\overline{x}^{k,j_k-1}$ and the feasibility of $x^k$ to subproblem \eqref{subprobkj} with $j=j_{k}-1$,
 \[
  \Phi(x^k)=q_{k,j_k-1}({x}^{k})\ge q_{k,j_k-1}(\overline{x}^{k,j_k-1})\ge q_{k,j_k-1}(x^{k,j_k-1})-(\mu/2)\|x^{k,j_k-1}-x^{k}\|^2.
 \]
 where the second inequality is due to step \ref{step5} of Algorithm \ref{iLPA}. Along with the expression of $q_{k,j_k-1}$,
 \begin{align*}
  \Phi(x^k)&\ge \vartheta_1(\ell_{F}(x^{k,j_k-1},x^k))+\langle\nabla G(x^k)\xi^k,x^{k,j_k-1}-x^k\rangle+h(x^{k,j_k-1})-\Theta_2(x^k)\nonumber\\
  &\quad +(1/2)\|x^{k,j_k-1}-x^k\|_{\mathcal{Q}_{k,j_k-1}}^2-(\mu/2)\|x^{k,j_k-1}-x^{k}\|^2.
 \end{align*}
 Combining this inequality with \eqref{vtheta1-ineq} for $y=\ell_{F}(x^{k,j_k-1},x^k)$ and \eqref{hineq} for $x=x^{k,j_k-1}$ yields that
 \begin{align*}
  \Phi(x^k)&\ge \langle\nabla F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k,x^{k,j_k-1}-x^k\rangle+\langle\widehat{v},x^{k,j_k-1}-\widehat{x}\rangle+\Theta_1(\widehat{x})+h(\widehat{x})-\Theta_2(x^k)\\
  &\quad+\langle\widehat{\zeta},F(x^k)\!-\!F(\widehat{x})\rangle+(\widetilde{\gamma}_{k}/2)\|x^{k,j_k-1}-x^k\|^2-(\mu/2)\|x^{k,j_k-1}-x^{k}\|^2
 \end{align*}
 where the inequality is also using $\mathcal{Q}_{k,j_k-1}\succeq\gamma_{k,j_k-1}\mathcal{I}=\widetilde{\gamma}_{k}\mathcal{I}$. After a suitable rearrangement,
 \begin{align*}
  (1/2)(\widetilde{\gamma}_k-\mu)\|x^{k,j_k-1}-x^{k}\|^2
  &\le \Phi(x^k)+\Theta_2(x^k)-\Theta_1(\widehat{x})-\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle-h(\widehat{x})\\
  &\quad-\langle\nabla F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k,x^{k,j_k-1}-x^k\rangle-\langle\widehat{v},x^{k,j_k-1}-\widehat{x}\rangle\\
  &\le\Phi(x^0)+\Theta_2(x^k)-\Theta_1(\widehat{x})-\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle-h(\widehat{x})-\langle\widehat{v},x^k-\widehat{x}\rangle\\
  &\quad+\big[\|\nabla F(x^{k})\widehat{\zeta}\|
    +\|\nabla G(x^{k})\xi^{k}\|+\|\widehat{v}\|\big]\|{x}^{k,j_k-1}-x^{k}\|,
 \end{align*}
 where the last inequality is using $\Phi(x^k)\le\Phi(x^0)$ by Proposition \ref{prop1-xk} (ii). Note that $\lim_{K\ni k\to\infty}\widetilde{\gamma}_k=\infty$. Passing the limit $K\ni k\to\infty$, recalling that $x^{k,j_k-1}\ne x^{k}$ for each $k\in K$ and using the boundedness of $\{(x^k,\xi^k)\}_{k\in\mathbb{N}}$, we obtain $\lim_{K\ni k\to\infty}\|x^{k,j_k-1}\!\!-\!x^{k}\|=0$. Then, by invoking \eqref{Theta-ineq30} with $x=x^{k,j_k-1}$ for all sufficiently large $k\in K$,
 \begin{equation*}
  \Theta(x^{k,j_k-1})\le\vartheta_1\big(\ell_{F}(x^{k,j_k-1},x^k)\big)-\vartheta_2\big(\ell_{G}(x^{k,j_k-1},x^k)\big)+(L_k/2)\|x^{k,j_k-1}-x^k\|^2.
 \end{equation*}
 Recall that $L_k$ is a constant close enough to ${\rm lip}\,\Theta(x^k)$ from above.
 By the boundedness of $\{x^k\}_{k\in\mathbb{N}}$ and \cite[Theorem 9.2]{RW98}, the sequence $\{{\rm lip}\,\Theta(x^k)\}_{k\in\mathbb{N}}$ is bounded, so is the sequence $\{L_k\}_{k\in\mathbb{N}}$. Then, for all $k\in K$ large enough, the last inequality yields a contradiction to the above \eqref{aim-ineq40}, and the result follows.

 \noindent
 {\bf(ii)} The boundedness of $\{w^k\}_{k\in\mathbb{N}}$ follows by the boundedness of $\{(x^{k},\overline{x}^k,\xi^k)\}_{k\in\mathbb{N}}$ and part (i), which implies the nonemptyness and compactness of $\Gamma^*$.
	
 \noindent
 {\bf(iii)} Pick any $\widehat{w}=(\widehat{x},\widehat{s},\widehat{\xi},\widehat{\mathcal{Q}})\in\Gamma^*$. There exists an index set $K\subset\mathbb{N}$ such that $\lim_{K\ni k\to\infty}w^k=\widehat{w}$. From Proposition \ref{prop1-xk}  (ii)-(iii), $\lim_{K\ni k\to\infty}\|x^k-x^{k-1}\|=0$. Along with \eqref{key-ineq40} and $\mathcal{Q}_k\ge\underline{\gamma}I$, we have $\lim_{K\ni k\to\infty}\|\overline{x}^k-x^{k}\|=0$, so $\widehat{x}=\widehat{s}$. From the definition of $\overline{x}^k$ and \cite[Theorem 23.9]{Roc70}, for each $k\in\mathbb{N}$,
 \begin{equation*}
  0\in\nabla F(x^{k-1})\partial\vartheta_1(\ell_{F}(\overline{x}^k,x^{k-1}))+\nabla G(x^{k-1})
  \partial(-\vartheta_2)(G(x^{k-1}))+\mathcal{Q}_{k-1}(\overline{x}^k-x^{k-1})+\partial h(\overline{x}^k).
 \end{equation*}
 Passing the limit $K\ni k\to\infty$ to the last inclusion and using the outer semicontinuity of $\partial\vartheta_1,\partial(-\vartheta_2)$ and $\partial h$ yields that $0\in\nabla F(\widehat{x})\partial\vartheta_1(F(\widehat{x}))+\nabla G(\widehat{x})\partial(-\vartheta_2)(G(\widehat{x}))+\partial h(\widehat{x})$, so $\widehat{x}$ is a stationary point of problem \eqref{prob}. We next argue that $\Xi(\widehat{w})=\lim_{k\to\infty}\Xi(w^k)$. Note that $-\widehat{\xi}\in\partial\vartheta_2(G(\widehat{x}))$. Hence, $\langle G(\widehat{x}),\widehat{\xi}\rangle+\vartheta_2^*(-\widehat{\xi})=-\vartheta_2(G(\widehat{x}))$. By the expression of $\Xi$ and $\widehat{x}=\widehat{s}$, we have $\Xi(\widehat{w})=\Theta_1(\widehat{x})-\Theta_2(\widehat{x})+h(\widehat{x})$. From the feasibility of $\widehat{x}$ and the optimality of $\overline{x}^k$ to the $(k-1)$-th subproblem, it holds that
 \begin{align*}
 &\vartheta_1(\ell_{F}(\overline{x}^k,x^{k-1}))+\langle\nabla G(x^{k-1})\xi^{k-1},\overline{x}^{k}-x^{k-1}\rangle+h(\overline{x}^{k})+\frac{1}{2}\|\overline{x}^{k}-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
 &\le\vartheta_1(\ell_{F}(\widehat{x},x^{k-1}))+\langle\nabla G(x^{k-1})\xi^{k-1},\widehat{x}-x^{k-1}\rangle+h(\widehat{x})+\frac{1}{2}\|\widehat{x}-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2,
 \end{align*}
 which by the continuity of $\vartheta_1,\,F'$ and $G'$ implies that $\limsup_{K\ni k\to\infty}h(\overline{x}^{k})\le h(\widehat{x})$. Along with the lower semicontinuity of $h$, $\lim_{K\ni k\to\infty}h(\overline{x}^{k})=h(\widehat{x})$. By \eqref{pre-equa0}, $\lim_{K\ni k\to\infty}\vartheta_2^*(-\xi^k)=-\vartheta_2(G(\widehat{x}))-\langle G(\widehat{x}),\widehat{\xi}\rangle$. Thus, by the expression of $\Xi$,  $\lim_{K\ni k\to\infty}\Xi(w^k)=\vartheta_1(F(\widehat{x}))-\vartheta_2(G(\widehat{x}))+h(\widehat{x})=\Xi(\widehat{w})$. If in addition Assumption \ref{ass1} (ii) holds, by combining $\widehat{x}=\widehat{s}$,	 $0\in\nabla\!F(\widehat{x})\partial\!\vartheta_1\!(F(\widehat{x}))\!+\nabla G(\widehat{x})\partial(-\vartheta_2)(G(\widehat{x}))\!+\!\partial h(\widehat{x})$ and $-\widehat{\xi}\in\partial\vartheta_2(G(\widehat{x}))$ with Lemma \ref{subdiff-LemXi}, we conclude that $0\in\partial\Xi(\widehat{w})$, and then $\Gamma^*\subset{\rm crit}\,\Xi$.
\end{proof}

To achieve the global convergence of the sequence $\{w^k\}_{k\in\mathbb{N}}$, we also need the following proposition.
%---------------------------------------------------------------------------------------------
\begin{proposition}\label{prop3-xk}
 Under Assumptions \ref{ass0}-\ref{ass1}, there exist an index $\overline{k}\in\mathbb{N}$ and a constant $b>0$ such that for all $k\ge\overline{k}$, ${\rm dist}(0,\partial\Xi(w^{k}))\le b\|x^k-x^{k-1}\|$.
\end{proposition}
\begin{proof}
 Recall that $\Gamma^*$ is the cluster point set of the sequence $\{w^k\}_{k\in\mathbb{N}}$, so $\lim_{k\to\infty}{\rm dist}(w^k,\Gamma^*)=0$. Then, there exists $\overline{k}\in\mathbb{N}$ such that $w^k\in\mathcal{O}$ for all $k\ge\overline{k}$. For each $k\ge\overline{k}$, by the optimality of $\overline{x}^{k}$ to the $(k-\!1)$th subproblem, there exists $v^{k-1}\in\partial\vartheta_1(\ell_{F}(\overline{x}^{k},x^{k-1}))$ such that
 \begin{equation}\label{temp-inclusion1}
  0\in\nabla F(x^{k-1})v^{k-1}
    +\nabla G(x^{k-1})\xi^{k-1}+\partial h(\overline{x}^{k})+\mathcal{Q}_{k-1}(\overline{x}^{k}-x^{k-1}).
 \end{equation}
 Note that $0\in\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}_{k-1})$ because $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$. Recall that $G(x^{k-1})\in\partial\vartheta_2^*(-\xi^{k-1})$. For each $k\ge\overline{k}$, define
 \[
   \zeta^k:=\left(\begin{matrix}
   \mathcal{Q}_{k-1}(\overline{x}^{k}-x^{k-1})\\
   [D^2F(x^{k-1})(\overline{x}^{k}-x^{k-1},\cdot)]^*v^{k-1}+[D^2G(x^{k-1})(\overline{x}^{k}-x^{k-1},\cdot)]^*\xi^{k-1}+2\mathcal{Q}_{k-1}(x^{k-1}-\overline{x}^{k})\\
    \ell_{G}(\overline{x}^{k},x^{k-1})-G(x^{k-1})\\
   (x^{k-1}-\overline{x}^{k})(x^{k-1}-\overline{x}^{k})^{\top}
  \end{matrix}\right).
 \]
 By using \eqref{temp-inclusion1} and comparing with the expression of $\partial\Xi$ in Lemma \ref{subdiff-LemXi}, we obtain $\zeta^k\in\partial\Xi(w^k)$. Since $\vartheta_1$ and $\vartheta_2$ are strictly continuous, from \cite[Theorem 9.13]{RW98}, $\|v^{k-1}\|\le{\rm lip}\,\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))$ and $\|\xi^{k-1}\|\le{\rm lip}\,\vartheta_2(G(x^{k-1}))$. Recall that $\{(x^k,\overline{x}^k)\}_{k\in\mathbb{N}}$ is bounded, so are the sequences $\{\ell_{F}(\overline{x}^k,x^{k-1})\}_{k\in\mathbb{N}}$ and $\{G(x^{k-1})\}_{k\in\mathbb{N}}$. Together with \cite[Theorem 9.2]{RW98}, there necessarily exists a constant $b_1>0$ such that $\max(\|v^{k-1}\|,\|\xi^{k-1}\|)\le b_1$ for all $k\ge\overline{k}$. Along with the boundedness of $\{\|\mathcal{Q}_{k-1}\|\}_{k\in\mathbb{N}}$ and the expression of $\zeta^k$, there exists $b_2>0$ such that
 $|\!\|\zeta^k|\!\|\le b_2\|x^{k-1}-\overline{x}^{k}\|\le b_2[\|x^{k-1}-x^k\|+\|\overline{x}^{k}-x^k\|]$. By the choice of $\mathcal{Q}_{k-1}$ in Algorithm \ref{iLPA}, $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$. Along with \eqref{key-ineq40}, $\|\overline{x}^k-x^k\|\le\!\sqrt{\mu\underline{\gamma}^{-1}}\|x^k-x^{k-1}\|$ for all $k\ge\overline{k}$. The desired result then holds with  $b=b_2(1+\!\sqrt{\mu/\underline{\gamma}})$. The proof is completed.
\end{proof}

Now using Proposition \ref{prop1-xk} (i) and Propositions \ref{prop2-xk}-\ref{prop3-xk} and following the same arguments as those for \cite[Theorem 1]{Bolte14}, we have the following convergence result.
%----------------------------------------------------------------------------------------------
\begin{theorem}\label{global-converge}
 Suppose that Assumptions \ref{ass0}-\ref{ass1} hold, and that $\Xi$ is a KL function. Then, the sequence $\{x^k\}_{k\in\mathbb{N}}$ is convergent and its limit  $\overline{x}$ is a stationary point of \eqref{prob}.
\end{theorem}
\begin{remark}
 From \cite[Theorem 4.1]{Attouch10}, if $\Xi$ is definable in an o-minimal structure over the real field $(\mathbb{R},+,\cdot)$, then it has the KL property at each point of ${\rm dom}\,\partial\Xi$. By the expression of $\Xi$ and \cite[sections 2 $\&$\,3]{Ioffe09}, when $F,G$ and $\vartheta_1,\vartheta_2,h$ are definable in the same o-minimal structure over $(\mathbb{R},+,\cdot)$, $\Xi$ is definable in this o-minimal structure. As discussed in \cite[Section 4]{Attouch10}, definable functions in an o-minimal structure are very rich, which cover semialgebraic functions and globally subanalytic functions.
\end{remark}

\subsection{Local convergence rate of Algorithm \ref{iLPA}}\label{sec4.2}

By using Theorem \ref{global-converge} and Propositions \ref{prop1-xk}-\ref{prop3-xk} and following the same arguments as those for \cite[Theorem 2]{Attouch09}, it is not difficult to achieve the following local convergence rate result of Algorithm \ref{iLPA}.
%----------------------------------------------------------------------------------------
\begin{theorem}\label{convergece-rate}
 Suppose that Assumptions \ref{ass0}-\ref{ass1} hold, and that $\Xi$ has the KL property of exponent $p\in[1/2,1)$ on the set $\Gamma^*$. Then, the sequence $\{x^k\}_{k\in\mathbb{N}}$ is convergent and the following assertions hold:
 \begin{enumerate}
 \item[(i)]  when $p=1/2$, there exist $c_1>0$ and $\rho\in(0,1)$ such that $\|x^k-x^*\|\leq c_1\rho^k$;
		
 \item[(ii)] when $p\in({1}/{2},1)$, there exists $c_1>0$ such that $\|x^k-x^*\|\le c_1k^{-\frac{1-p}{2p-1}}$.
 \end{enumerate}
 \end{theorem}

 As is well known, to verify the KL property of exponent $1/2$ for a nonconvex and nonsmooth function is not an easy task because there are no convenient rules to identify it. Next we focus on the verifiable conditions for the KL property of $\Xi$ with exponent $p\in[1/2,1)$. To this end, we introduce
%---------------------------------------------------------------------------------------------
\begin{equation}\label{wXi-fun}
 \widetilde{\Xi}(x,s,z):=\vartheta_1(\ell_{F}(x,s))+\langle \ell_{G}(x,s),z\rangle+h(x)+\vartheta_2^*(-z)\quad\forall(x,s,z) \in\mathbb{X}\times\mathbb{X}\times\mathbb{Z}.
\end{equation}
Under the assumption of Lemma \ref{subdiff-LemXi}, at any $(x,s,z)\in{\rm dom}\,h\times{\rm dom}\,h\times(-{\rm dom}\,\vartheta_2^*)$,
%------------------------------------------------------------------------------------------------
\begin{equation}\label{subdiff-wXi}
 \partial\widetilde{\Xi}(x,s,z)=
  \left(\begin{matrix}
  \left(\begin{matrix}
   \nabla F(s)	\\ [D^2F(s)(x-s,\cdot)]^*\\
   \end{matrix}\right)\partial\vartheta_1(\ell_{F}(x,s))
    +\left(\begin{matrix}
   \nabla G(s)z+\partial h(x) \\ [D^2G(s)(x-s,\cdot)]^*z
     \end{matrix}\right)\\
   \ell_{G}(x,s)-\partial\vartheta_2^*(-z)
  \end{matrix}\right).
\end{equation}
By comparing \eqref{subdiff-wXi} with the expression of $\partial\Xi$ in Lemma \ref{subdiff-LemXi}, we see that $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$ if and only if $(\overline{x},\overline{x},\overline{z},\mathcal{Q})$ for a certain PD linear mapping $\mathcal{Q}\!:\mathbb{X}\to\mathbb{X}$ is a critical point of $\Xi$. By combining \eqref{subdiff-wXi} with Definition \ref{spoint-def}, if $\overline{x}$ is a stationary point of \eqref{prob}, there exists $\overline{z}\in\!-\partial\vartheta_2(G(\overline{x}))$ such that $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$; and if $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$, $\overline{x}$ is a stationary point of \eqref{prob}.

The following proposition provides a condition for the KL property of $\widetilde{\Xi}$ and $\Xi$ with exponent $p\in[1/2,1)$ by using that of an almost separable nonsmooth function and the structure of $F'$ and $G'$.
%------------------------------------------------------------------------------------------------
\begin{proposition}\label{prop-KL}
 Consider any $\overline{\chi}=(\overline{x},\overline{x},\overline{z})\in(\partial\widetilde{\Xi})^{-1}(0)$. Suppose that the following function
 \begin{equation}\label{ffun}
  f(y,s,u,z):=\vartheta_1(y)+h(s)+\langle u,z\rangle+\vartheta_2^*(-z)\ \ {\rm for}\  (y,s,u,z)\in\mathbb{Y}\times\mathbb{X}\times\mathbb{Z}\times\mathbb{Z}
 \end{equation}
 has the KL property of exponent $p\in[1/2,1)$ at $(\overline{y},\overline{x},\overline{u},\overline{z})$ with $\overline{y}=F(\overline{x})$ and $\overline{u}=G(\overline{x})$, and that
 \begin{equation}\label{key-cond}
 {\rm Ker}\big([\nabla F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})]\big)\cap{\rm cl}\,{\rm cone}\big[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{u}))\big]=\big\{0\big\}.
 \end{equation}
 Then, under Assumption \ref{ass1} (ii), the function $\widetilde{\Xi}$ has the KL property of exponent $p$ at $\overline{\chi}$, so does the function $\Xi$ at  $(\overline{\chi},\overline{\mathcal{Q}})$ for any PD linear mapping $\overline{\mathcal{Q}}$ from $\mathbb{X}$ to $\mathbb{X}$.
\end{proposition}
\begin{proof}
 Suppose on the contrary that $\widetilde{\Xi}$ does not have the KL property of exponent $p$ at $\overline{\chi}$. There exists a sequence $\{\chi^k\}_{k\in\mathbb{N}}$ with $\chi^k=(x^k,s^k,z^k)$ for each $k$, $\chi^k\to\overline{\chi}$ as $k\to\infty$, and  $\widetilde{\Xi}(\overline{\chi})<\widetilde{\Xi}(\chi^k)<\widetilde{\Xi}(\overline{\chi})+1/k$ for each $k$ such that the following inequality holds with $y^k:=\ell_{F}(x^k,s^k)$ and $u^k:=\ell_{G}(x^k,s^k)$:
 \[
  {\rm dist}\big(0,\partial\widetilde{\Xi}(x^k,s^k,z^k)\big)
  <(1/k)\big(f(y^k,x^k,u^k,z^k)-f(\overline{y},\overline{x},\overline{u},\overline{z})\big)^{p}.
 \]
 Obviously, it holds that $\{(x^k,s^k,z^k)\}_{k\in\mathbb{N}}\subset{\rm dom}\,\widetilde{\Xi}$. By combining this inequality with equation \eqref{subdiff-wXi}, for each $k\in\mathbb{N}$, there exist $v^k\in\partial\vartheta_1(y^k)$, $\xi^k\in\partial h(x^k)$ and $\zeta^k\in\partial\vartheta_2^*(-z^k)$ such that
 \begin{equation}\label{ineq1-KL}
  \left\|\left(\begin{matrix}
      \nabla F(s^k)v^k+\xi^k+\nabla G(s^k)z^k\\
   [D^2F(s^k)(x^k-s^k,\cdot)]^*v^k+[D^2G(s^k)(x^k-s^k,\cdot)]^*z^k\\	
			u^k-\zeta^k
  \end{matrix}\right)\right\|<\frac{1}{k}\Big(f(y^k,x^k,u^k,z^k)-\overline{f}\Big)^{p}
 \end{equation}
 with $\overline{f}=f(\overline{y},\overline{x},\overline{u},\overline{z})$.
 Since $f$ has the KL property of exponent $p$ at $(\overline{y},\overline{x},\overline{u},\overline{z})$, there exist $\delta'>0,\eta'>0$ and $c'>0$ such that for all $(y,x,u,z)\in\mathbb{B}((\overline{y},\overline{x},\overline{u},\overline{z}),\delta')\cap[\overline{f}<f<\overline{f}+\eta']$,
 \begin{equation}\label{vartheta-KL}
 {\rm dist}(0,\partial\!f(y,x,u,z))\ge c'\big(f(y,x,u,z)-\overline{f}\big)^{p}.
 \end{equation}
 From $\chi^k\to\overline{\chi}$ as $k\to\infty$, we have $x^k\in\mathcal{O}_{x}$ for all sufficiently large $k$, where $\mathcal{O}_x$ is the same as the one in Assumption \ref{ass1} (ii). Thus, by invoking Assumption \ref{ass0} (i) and recalling that  $\widetilde{\Xi}(\overline{w})<\widetilde{\Xi}(w^k)<\widetilde{\Xi}(\overline{w})+(1/k)$ for each $k\in\mathbb{N}$, it follows that  $(y^k,x^k,u^k,z^k)\in\mathbb{B}((\overline{y},\overline{x},\overline{u},\overline{z}),\delta')\cap[\overline{f}<f<\overline{f}+\eta']$ for all $k$ large enough. Together with the above inequalities \eqref{vartheta-KL} and \eqref{ineq1-KL}, it follows that for all sufficiently large $k$,
  \begin{align}\label{ineq2-KL}
   &\left\|\left(\begin{matrix}
    \nabla F(s^k)v^k+\xi^k+\nabla G(s^k)z^k\\
   [D^2F(s^k)(x^k-s^k,\cdot)]^*v^k+[D^2G(s^k)(x^k-s^k,\cdot)]^*z^k\\		
			u^k-\zeta^k
     \end{matrix}\right)\right\|\nonumber\\
   &<\frac{1}{kc'}{\rm dist}\big(0,\partial\!f(y^k,x^k,u^k,z^k)\big)
   \le\frac{1}{kc'}\|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|,
  \end{align}
  where the second inequality is using $(v^k,\xi^k,z^k,u^k-\zeta^k)\in\partial\!f(y^k,x^k,u^k,z^k)$, implied by  $v^k\in\partial\vartheta_1(y^k)$, $\xi^k\in\partial h(x^k)$ and $\zeta^k\in\partial\vartheta_2^*(-z^k)$ for each $k\in\mathbb{N}$.
  Combining $(v^k,\xi^k,z^k,u^k-\zeta^k)\in\partial\!f(y^k,x^k,u^k,z^k)$ with \eqref{vartheta-KL} yields that $\|(v^k,\xi^k,z^k,u^k-\zeta^k)\|\ge c'\big(f(y^k,x^k,u^k,z^k)-\overline{f}\big)^{p}>0$. For each $k$ large enough, write $(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k):=\frac{(v^k,\xi^k,z^k,u^k-\zeta^k)}{\|(v^k,\xi^k,z^k,u^k-\zeta^k)\|}$. Then, from \eqref{ineq2-KL} it follows that for all sufficiently large $k$,
  \begin{equation}\label{ineq3-KL}
   \left\|\left(\begin{matrix}
   \nabla F(s^k)\widetilde{v}^k+\widetilde{\xi}^k+\nabla G(s^k)\widetilde{z}^k\\
    [D^2F(s^k)(x^k-s^k,\cdot)]^*\widetilde{v}^k+[D^2G(s^k)(x^k-s^k,\cdot)]^*\widetilde{z}^k\\ \widetilde{\eta}^k
   \end{matrix}\right)\right\|\le\frac{1}{kc'}.
  \end{equation}
  From the boundedness of $\{(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k)\}_{k\in\mathbb{N}}$, there necessarily exists an index set $K\subset\mathbb{N}$ such that  $\{(\widetilde{v}^k;\widetilde{\xi}^k\!,\widetilde{z}^k\!,\widetilde{\eta}^k)\}_{k\!\in \!K}$ is convergent with the limit $(\widetilde{v},\widetilde{\xi},\widetilde{z},\widetilde{\eta})$ satisfying $\|(\widetilde{v},\widetilde{\xi},\widetilde{z},\widetilde{\eta})\|\!=\!1$.
	
  We next argue that the sequence $\{(\xi^k,u^k-\zeta^k)\}_{k\in\mathbb{N}}$ is necessarily bounded. If not, by noting that $\{(v^k,z^k)\}_{k\in\mathbb{N}}$ is bounded, from the unboundedness of $\{(\xi^k,u^k-\zeta^k)\}_{k\in\mathbb{N}}$ we deduce that $\widetilde{v}=0$, $\widetilde{z}=0$ and $\|(\widetilde{\xi},\widetilde{\eta})\|=1$. However, passing the limit $K\ni k\to\infty$ to inequality \eqref{ineq3-KL} and using $\widetilde{v}=0,\widetilde{z}=0$ leads to $\widetilde{\xi}=\widetilde{\eta}=0$, which is a contradiction to $\|(\widetilde{\xi},\widetilde{\eta})\|=1$. Thus, $\{(\xi^k,u^k-\zeta^k)\}_{k\in\mathbb{N}}$ is bounded, which by $u^k\to G(\overline{x})$ as $k\to\infty$ implies that $\{\zeta^k\}_{k\in\mathbb{N}}$ is bounded. If necessary by taking a subsequence, we assume $\lim_{K\ni k\to\infty}\zeta^k=\overline{\zeta}$. By the expression of $(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k)$ and $z^k\in-\partial\vartheta_2(\zeta^k)$, for each $k$ large enough,
  \[
   (\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k)\in{\rm cone}\big[\partial\vartheta_1(y^k)\times\partial h(x^k)\times(-\partial\vartheta_2(\zeta^k))].
  \]
  Together with the outer semicontinuity of $\partial\vartheta_1,\,\partial h$ and $\partial\vartheta_2$, it then follows that
 \[
  (\widetilde{v},\widetilde{\xi},\widetilde{z})\in{\rm cl}\,{\rm cone}[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{\zeta}))].
 \]
 In addition, passing the limit $K\ni k\to\infty$ to inequality \eqref{ineq3-KL} yields that $ \nabla\!F(\overline{x})\widetilde{v}+\widetilde{\xi}+\nabla\!G(\overline{x})\widetilde{z}=0$ and $\widetilde{\eta}=0$. By using $\widetilde{\eta}=0$ and the expression of $\widetilde{\eta}^k$ and recalling that $u^k\to G(\overline{x})$ as $k\to\infty$, it is not hard to deduce that $\overline{\zeta}=G(\overline{x})$. Together with the last equation, it follows that
 \[
  0\ne(\widetilde{v},\widetilde{\xi},\widetilde{z})\in{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})]\big)\cap{\rm cl}\,{\rm cone}\big[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{u})],
 \]
 which is a contradiction to the assumption in \eqref{key-cond}. Therefore, $\widetilde{\Xi}$ has the KL property of exponent $p$ at $\overline{\chi}$. Since $(\overline{\chi},\overline{\mathcal{Q}})$ is a critical point of $\Xi$,  using the same arguments as those for \cite[Lemma 1]{LiuPanWY22} can justify that $\Xi$ has the KL property of exponent $p$ at $(\overline{\chi},\overline{\mathcal{Q}})$. The proof is completed.
\end{proof}
\begin{remark}\label{remark42-KL}
 {\bf(a)} From the proof of Proposition \ref{prop-KL}, we see that its conclusions do not require the convexity of the functions $\vartheta_1,\,\vartheta_2$ and $h$, and still hold if $p\in[1/2,1)$ is replaced by $p\in(0,1)$. In addition, it is worth emphasizing that condition \eqref{key-cond} is point-based and verifiable.
	
 \noindent
 {\bf(b)} Let $\psi(u,z):=\langle u,z\rangle+\vartheta_2^*(-z)$ for $(u,z)\in\mathbb{Z}\times\mathbb{Z}$. By the expression of $f$ in \eqref{ffun}, when $\psi,\vartheta_1$ and $h$ are the KL functions of exponent $p\in[1/2,1)$, the function $f$ is a KL function of exponent $p\in[1/2,1)$. By \cite[Proposition 2.2 (i) $\&$ Remark 2.2]{LiuPanWY22}, when $\psi,\,\vartheta_1$ and $h$ are KL functions,
 their KL property of exponent $p\in[1/2,1)$ is implied by the $1/(2p)$-subregularity of their subdifferential mappings; for example, the KL property of $\psi$ with $p\in[1/2,1)$ at $(\overline{u},\overline{z})$ is implied by the $1/(2p)$-subregularity of $\partial\psi$ at $((\overline{u},\overline{z}),0)\in{\rm gph}\partial\psi$. Thus, by invoking \cite[Proposition 1]{Robinson81}, we conclude that $\vartheta_1$ and $h$ are KL functions of exponent $p\in[1/2,1)$ if they are piecewise linear-quadratic (PLQ) KL functions, and $\psi$ is a KL function of exponent $p\in[1/2,1)$ if $\vartheta_2$ is a PLQ function and $\psi$ is a KL function. When they are not PLQ functions, one can employ the criterions proposed in \cite{Geferee11} to examine the subregularity of their subdifferential mappings at the reference points.
\end{remark}
%------------------------------------------------------------------------------------------------
\begin{corollary}\label{corollary-KL}
 For problem \eqref{prob} with $\vartheta_2\equiv 0$, consider any point $(\overline{x},\overline{x})\in(\partial\widetilde{\Xi})^{-1}(0)$ with $\widetilde{\Xi}(x,s)=\vartheta_1(\ell_{F}(x,s))+h(x)$ for $(x,s)\in\mathbb{X}\times\mathbb{X}$. Suppose that the following function
 \begin{equation}\label{ffun1}
  f(y,s):=\vartheta_1(y)+h(s)\ \ {\rm for}\  (y,s)\in\mathbb{Y}\times\mathbb{X}
 \end{equation}
  has the KL property of exponent $p\in[1/2,1)$ at $(\overline{y},\overline{x})$ with $\overline{y}=F(\overline{x})$, and that
 \begin{equation}\label{key-cond1}
  {\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap{\rm cl}\,{\rm cone}\big(\partial\! f(\overline{y},\overline{x})\big)=\big\{0\big\}.
 \end{equation}
 Then, under Assumption \ref{ass1} (ii), the function $\widetilde{\Xi}$ has the KL property of exponent $p$ at $(\overline{x},\overline{x})$, and the corresponding $\Xi$ has the KL property of exponent $p$ at $(\overline{x},\overline{x},\overline{\mathcal{Q}})$ for any PD linear mapping $\overline{\mathcal{Q}}$.
\end{corollary}

Note that the function $f$ defined in \eqref{ffun1} has the KL property of exponent $1/2$ at $(F(\overline{x}),\overline{x})$ iff $\vartheta_1$ and $h$ have the KL property of exponent $1/2$ at $F(\overline{x})$ and $\overline{x}$, respectively. By combining \cite[Theorem 5 (ii)]{Bolte17} and \cite[Theorem 3.3]{Artacho08}, the KL property of $\vartheta_1$ with exponent $1/2$ at $\overline{y}=F(\overline{x})$ is equivalent to the subregularity of $\partial\vartheta_1$ at $(\overline{y},0)$, and that of $h$ is equivalent to the subregularity of $\partial h$ at $(\overline{x},0)$. Such a condition is equivalent to requiring that $\mathop{\arg\min}_{(y,s)\in\mathbb{Y}\times\mathbb{X}}f(y,s)$ is the set of local weak sharp minima of order 2 for $f$, which for $h\equiv 0$ is the one used in \cite[Theorem 20]{HuYang16} to achieve a global R-linear rate.

Next we take a closer look at condition \eqref{key-cond1} and discuss its relation with the regularity used in \cite{HuYang16} for problem \eqref{prob} with $\vartheta_2\equiv 0$. For this purpose, for a closed convex set $S\subset\mathbb{Y}$, we denote by $S^{\ominus}$ its negative polar, i.e., $S^{\ominus}:=\{y^*\in\mathbb{Y}\ |\ \langle y^*,y\rangle\le 0\ {\rm for\ all}\ y\in S\}$.
%-------------------------------------------------------------------------------------------------
\begin{remark}\label{remark-relation}
 {\bf(a)} Let $D:=\mathop{\arg\min}_{x\in\mathbb{X}}h(x)$ and recall that $C=\mathop{\arg\min}_{y\in\mathbb{Y}}\vartheta_1(y)$. Then, it holds that
 \[
  {\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x}))\subset[C\times D-(\overline{y},\overline{x})]^{\ominus}.
 \]
 Indeed, pick any $(u,v)\!\in\!{\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x}))$. Let $\{(u^k,v^k)\}_{k\in\mathbb{N}}\!\subset\!{\rm cone}(\partial\!f(\overline{y},\overline{x}))$ be such that $(u^k,v^k)\to (u,v)$. For each $k$, there exist $t_k\ge 0$ and $(\xi^k,\zeta^k)\in\partial\!f(\overline{y},\overline{x})$ such that $(u^k,v^k)\!=\!t_k(\xi^k,\zeta^k\!)$. Therefore,
 \[
  0\ge f(y,x)-f(\overline{y},\overline{x})\ge\langle(\xi^k,\zeta^k),(y,x)-(\overline{y},\overline{x})\rangle\quad\ \forall (y,x)\in C\times D,
 \]
 which implies that $(u^k,v^k)\in[C\times D-(\overline{y},\overline{x})]^{\ominus}$ and $(u,v)\in[C\times D-(\overline{y},\overline{x})]^{\ominus}$ by the closedness of the set $[C\times D-(\overline{y},\overline{x})]^{\ominus}$. The claimed inclusion then follows. In particular, this inclusion is generally strict; for example, consider $\vartheta_1(y)=\|y_1\|_1+y_2^2$ for $y\in\mathbb{R}^2$ and $F(x)\!=\!(x,-x)^{\top}$ for $x\in\mathbb{R}$. For $\overline{x}\!=\!1$, we have ${\rm cl}[{\rm cone}(\partial\vartheta_1(F(\overline{x})))]\!=\!\mathbb{R}_{+}\times\mathbb{R}_{-}$, but $[C\!-\!F(\overline{x})]^{\ominus}\!=\!\{y\in\mathbb{R}^2\ |\ y_2-y_1\le 0\}$. This shows that condition \eqref{key-cond1} is weaker than
 \(
  {\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap\big([C-F(\overline{x})]^{\ominus}\times[D-\overline{x}]^{\ominus}\big)=\big\{0\big\},
 \)
 which is precisely the regularity condition used in \cite[Theorem 18]{HuYang16} and \cite[Section 3]{Burke95}.
	
 \noindent
 {\bf(b)} Suppose that $h\equiv 0$. For any given $x\in\mathbb{X}$, let $D(x):=\{d\in\mathbb{X}\,|\,F(x)+F'(x)d=0\}$. From \cite[Definition 7 (c)]{HuYang16}, a vector $\overline{x}\in\mathbb{X}$ is called a quasi-regular point of the inclusion $F(x)\in C$ if there exist $r>0$ and $\beta_{r}>0$ such that for all $x\in\mathbb{B}(\overline{x},r)$, ${\rm dist}(0,D(x))\le\beta_{r}{\rm dist}(F(x),C)$. When $F$ is polyhedral but $C\ne\{0\}$, there are also examples for which this quasi-regularity condition does not hold; for example, $\overline{x}=1, F(x)=(x;0)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=(y_1\!-\!1)^4+|y_2|$ for $y\in\mathbb{R}^2$. Although condition \eqref{key-cond1} does not hold for this example, when $F$ is polyhedral and $\vartheta_1$ is a PLQ convex function, $\widetilde{\Xi}$ is necessarily a KL function of exponent $1/2$ by \cite[Theorem 5 (ii)]{Bolte17}, \cite[Theorem 3.3]{Artacho08} and \cite[Proposition 1]{Robinson81}, so is the function $\Xi$ by the second part of Corollary \ref{corollary-KL}. When $F$ is non-polyhedral, there are examples for which condition \eqref{key-cond1} holds but the quasi-regularity condition does not hold; for instance, $F(x)=(x^2;-x)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=\|y\|^2$ for $y\in\mathbb{R}^2$. It is easy to check that \eqref{key-cond1} holds at  $\overline{x}=0$, which along with the strong convexity of $\vartheta_1$ shows that $\widetilde{\Xi}$ and $\Theta_1$ are the KL functions of exponent $1/2$, but $\overline{x}$ is not a quasi-regular point of the inclusion $F(x)\in C$ because $D(x)=\{d\in\mathbb{R}\ |\ (x^2+2xd;-x-d)=0\}=\emptyset$ for all $x\ne 0$ sufficiently close to $\overline{x}$. Now it is unclear whether there is an example with nonlinear $F$ for which the quasi-regularity holds but condition \eqref{key-cond1} does not hold.
\end{remark}
%-------------------------------------------------------------------------------------------
\section{Dual PPA armed with SNCG to solve subproblems}\label{sec5}

 The efficiency of Algorithm \ref{iLPA} depends heavily on the computation of subproblem \eqref{subprobkj}. In this section, we develop an efficient method by applying the proximal point algorithm (PPA) armed with semismooth Newton to solve the dual of subproblem \eqref{subprobkj}, named dPPASN. For this purpose, for a proper closed convex $f:\mathbb{X}\to\overline{\mathbb{R}}$ and a constant $\tau>0$, $P_{\tau f}$ and $e_{\tau f}$ denotes the proximal mapping and the Moreau envelope of $f$ associated to $\tau$. For each $k\in\mathbb{N}$ and $j\in[j_k]$, we introduce the following notation:
 \begin{equation}\label{Akmap}
  \mathcal{A}_k:=F'(x^k),\,u^k:=\nabla G(x^k)\xi^k,\,c^k:=F(x^k),\,b^k:=\mathcal{A}_kx^k-c^k\ \ {\rm and}\ \   \mathcal{Q}_{k,j}:=\gamma_{k,j}\mathcal{I}+\alpha_k\mathcal{A}_k^*\mathcal{A}_k
 \end{equation}
 where $\alpha_k>0$ is specified in numerical experiments. Then subproblem \eqref{subprobkj} is equivalently written as
 \begin{align}\label{Esubprobkj}
 &\min_{x\in\mathbb{X},y\in\mathbb{Y}}\!\vartheta_1(y)+\langle u^k,x\rangle+h(x)+({\gamma_{k,j}}/{2})\|x-x^k\|^2+({\alpha_k}/{2})\|y-c^k\|^2-C_k \nonumber\\
 &\quad{\rm s.t.}\ \ \mathcal{A}_kx-b^k=y\quad\ {\rm with}\ \ C_k:=\langle u^k,x^k\rangle+\Theta_2(x^k).
\end{align}
 After an elementary calculation, the dual of problem \eqref{Esubprobkj} takes the following form
 \begin{equation}\label{dEsubprobkj}
 \min_{\zeta\in\mathbb{Y}}\ \Psi_k(\zeta):=\frac{\|\zeta\|^2}{2\alpha_k}-e_{\alpha_k^{-1}\vartheta_1}\big(\alpha_k^{-1}\zeta+c^k\big)+\frac{\|\mathcal{A}_k^*\zeta+u^k\|^2}{2\gamma_{k,j}} -e_{\gamma_{k,j}^{-1}h}\big(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)-C_k.
 \end{equation}
 Clearly, the strong duality holds for \eqref{Esubprobkj} and \eqref{dEsubprobkj}. Since $\Psi_k$ is a smooth convex function with Lipschitz continuous gradient, seeking an optimal solution of \eqref{dEsubprobkj} is equivalent to finding a root $\zeta^*$ of system
 \begin{equation}\label{system}
  0=\nabla\Psi_k(\zeta)=P_{\alpha_k^{-1}\vartheta_1}(\alpha_k^{-1}\zeta^{*}+c^k)+b^k-\gamma_{k,j}^{-1}\mathcal{A}_kP_{\gamma_{k,j}^{-1}h}(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{*}+u^k)).
 \end{equation}
 With such $\zeta^*$, one can recover the unique optimal solution $(\overline{x}^{k,j},\overline{y}^{k,j})$ of \eqref{Esubprobkj} or \eqref{subprobkj} via
 \[
  \overline{x}^{k,j}=P_{\gamma_{k,j}^{-1}h}(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{*}+u^k)) \ \ {\rm and}\ \ \overline{y}^{k,j}=P_{\alpha_k^{-1}\vartheta_1}(\alpha_k^{-1}\zeta^{*}+c^k).
 \]
 The nonsmooth system \eqref{system} is semismooth by combining \cite[Theorem 1]{Bolte09} and \cite[Proposition 3.1]{Ioffe09} when $\vartheta_1$ and $h$ are definable in the same o-minimal structure on the real field $(\mathbb{R},+,\cdot)$. However, a direct application of the semismooth Newton method to it faces the difficulty caused by the potential singularity of the generalized Hessian of $\Psi_k$. Inspired by this, we apply the inexact PPA armed with the semismooth Newton method to solving \eqref{dEsubprobkj}, whose iterate steps are described as follows.
%----------------------------------------------------------------------------------

 \begin{algorithm}[h]
 	\renewcommand{\thealgorithm}{A}
 	\caption{\label{PPASN}{\bf\ Inexact dPPA with semismooth Newton (dPPASN)}}
 	\textbf{1:} Initialization: Fix $k\in\mathbb{N}$ and $j\in[j_k]$. Choose $\tau_0>\underline{\tau}>0$ and an initial $\zeta^0\in\mathbb{Y}$.\\
 \textbf{2: For} {$l=0,1,2,\ldots$}  \textbf{do}\\
  \textbf{3:       }\textbf{    }\textbf{    } Seek an inexact minimizer $\zeta^{l+1}$ of the following problem with Algorithm \ref{SNCG} later:
    \begin{equation}\label{PPA-subprob}
 			\min_{\zeta\in\mathbb{Y}}\Psi_{k,l}(\zeta):=\Psi_{k}(\zeta)+\frac{\tau_{l}}{2}\|\zeta-\zeta^{l}\|^2.
 		\end{equation}

 \textbf{4:    } \textbf{    }\textbf{    }Update the parameter $\tau_{l+1}\in[\underline{\tau},\tau_l)$.\\
 \textbf{5: EndFor}
 \end{algorithm}

\begin{algorithm}[h]
 	\renewcommand{\thealgorithm}{A.1}
 	\caption{\label{SNCG}{\bf\ Semismooth Newton method}}
 	\textbf{Initialization:} Fix $k\in\mathbb{N},j\in[j_k]$ and $l\in\mathbb{N}$. Choose  $\underline{\eta}\!\in(0,1),\beta\in(0,1),\varsigma\in(0,1]$ and $
 	0<c_1<c_2<\frac{1}{2}$. Set $\nu:=0$ and let $\zeta^0=\zeta^{l}$.
 	
 	\textbf{while} the stopping conditions are not satisfied \textbf{do}
 	\begin{enumerate}
 		\item Choose  $\mathcal{U}^{\nu}\in\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta^{\nu}+c^k\big)$
 		and $\mathcal{V}^{\nu}\in\partial_{C}P_{\!\gamma_{k,j}^{-1}}h 		 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta^{\nu}+u^k)\big)$.
 		
 		\item Solve the following linear system via the conjugate gradient (CG) method
 		\[
 		\mathcal{W}^{\nu}d=-\nabla\Psi_{k,l}(\zeta^{\nu})\ \ {\rm with}\ \
 		\mathcal{W}^{\nu}=\tau_l\mathcal{I}+\alpha_{k}^{-1}\mathcal{U}^{\nu}+\gamma_{k,j}^{-1}\mathcal{A}_k\mathcal{V}^{\nu}\mathcal{A}_k^*,
 		\]
 		\ \ to find $d^{\nu}$ such that  $\|\mathcal{W}^{\nu}d^{\nu}+\nabla\Psi_{k,l}(\zeta^{\nu})\|\le\min(\underline{\eta},\|\nabla\Psi_{k,l}(\zeta^{\nu})\|^{1+\varsigma})$.
 		
 		\item Let $m_{\nu}$ be the smallest nonnegative integer $m$ such that
 		\begin{align*}
 			\Psi_{k,l}(\zeta^{\nu}\!+\!\beta^md^{\nu})\leq\Psi_{k,l}(\zeta^{\nu})+c_1\beta^m\langle\nabla\Psi_{k,l}(\zeta^{\nu}),d^{\nu}\rangle\\
 			\vert\langle\nabla\Psi_{k,l}(\zeta^{\nu}\!+\!\beta^md^{\nu}),d^{\nu}\rangle\vert\le c_2\vert\langle\nabla\Psi_{k,l}(\zeta^{\nu}),d^{\nu}\rangle\vert.
 		\end{align*}
 		
 		\item Set $\zeta^{\nu+1}=\zeta^{\nu}+\beta^{m_{\nu}}d^{\nu}$. Let $\nu\leftarrow \nu+1$, and then go to Step 1.
 	\end{enumerate}
 	\textbf{End(while)}
 \end{algorithm}
 Note that $-\Psi_{k}(\zeta^{l})\le {\rm dval}=q_{k,j}(\overline{x}^{k,j})$, where ``${\rm dval}$" is the optimal value of \eqref{dEsubprobkj}. When applying Algorithm \ref{PPASN} to solve the $k$th subproblem \eqref{subprobkj}, we terminate Algorithm \ref{PPASN} at the iterate $\zeta^{l}$ if
 \[
 q_{k,j}(x^{k,j})+\Psi_{k}(\zeta^{l})<({\underline{\gamma}}/2)\|x^{k,j}-x^k\|^2
 \ \ {\rm for}\ \ x^{k,j}=\!P_{\gamma_{k,j}^{-1}h}(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{l}+u^k)).
 \]
 This has a little difference from the inexactness condition in step \ref{step5} of Algorithm \ref{iLPA} by considering that $-\Psi_{k}(\zeta^{l})$ is only a lower estimation for $q_{k,j}(\overline{x}^{k,j})$. For the convergence of Algorithm \ref{PPASN}, see \cite{Roc76,Roc21}.

 Since $\Psi_{k,l}$ has a Lipschitz continuous gradient mapping, we define its generalized Hessian at $\zeta$ by the Clarke Jacobian of $\nabla\Psi_{k,l}$ at $\zeta$, i.e., $\partial^2\Psi_{k,l}(\zeta)\!:=\partial_{C}(\nabla\Psi_{k,l})(\zeta)$. From \cite[Page 75]{Clarke83}, it follows that $\partial^2\Psi_{k,l}(\zeta)d=\widehat{\partial}^2\Psi_{k,l}(\zeta)d$
for all $d\in\mathbb{Y}$ with
\[
\widehat{\partial}^2\Psi_{k,l}(\zeta)
\!=\!\tau_{l}\mathcal{I}+\alpha_k^{-1}\partial_{C}P_{\!\alpha_k^{-1}\vartheta_1}\big(\alpha_k^{-1}\zeta+c^k\big)
+\gamma_{k,j}^{-1}\mathcal{A}_k\partial_{C}P_{\!\gamma_{k,j}^{-1}h}
\big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)\mathcal{A}_k^{*}.
\]
By mimicking the proof in \cite[Section 3.3.4]{Ortega70}, every $\mathcal{U}\in\partial_{C}P_{\alpha_k^{-1}\vartheta_1}\big(\alpha_k^{-1}\zeta+c^k\big)$ and every $\mathcal{V}\in\partial_{C}P_{\!\gamma_{k,j}^{-1}h}
\big(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)$ are positive semidefinite. Along with $\tau_l>0$ for each $l\in\mathbb{N}$, the operator $\tau_lI+\alpha_{k}^{-1}\mathcal{U}+\gamma_{k,j}^{-1}\mathcal{A}_k\mathcal{V}\mathcal{A}_k^*$ is positive definite, so every element in $\widehat{\partial}^2\Psi_{k,l}(\zeta)$ is nonsingular. Thus, Algorithm \ref{SNCG} is well defined and its convergence analysis can be found in \cite{QiSun93,ZhaoST10}.
%----------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------
\section{Application to matrix completions with outliers}\label{sec6}

In this section, we apply Algorithm \ref{iLPA} armed with dPPASN to model \eqref{SCAD-loss} with $\vartheta\equiv\vartheta_1-\vartheta_2$, where  $\vartheta_1(y):=\|y\|_1$ and $\vartheta_2(y):=(1/\rho)\sum_{i=1}^{m}\theta_{a}(\rho|y_i|)\ (\rho>0)$ for $y\in\mathbb{R}^m$, and $\theta_{a}\!:\mathbb{R}\to\mathbb{R}$ is the function
\[
\theta_{a}(s):=\left\{\begin{array}{cl}
	0 & \textrm{if}\ s\leq \frac{2}{a+1},\\
	\frac{((a+1)s-2)^2}{4(a^2-1)} & \textrm{if}\ \frac{2}{a+1}<s\leq \frac{2a}{a+1},\\
	s-1 & \textrm{if}\ s>\frac{2a}{a+1}
\end{array}\right.\ {\rm with}\ \ a>1.
\]
Such $\vartheta$ with an appropriately large $\rho$ is shown to be an equivalent DC surrogate of the zero-norm in \cite{ZhangPan22}.
Model \eqref{SCAD-loss} with this $\vartheta$ takes the form of problem \eqref{prob} with $F(x)=\mathcal{A}(UV^{\top})-b=G(x)$ and $h(x)=\lambda(\|U\|_{2,1}+\|V\|_{2,1})$ for $x=(U,V)\in\mathbb{X}=\mathbb{R}^{n_1\times r}\times\mathbb{R}^{n_2\times r}$.
For such $F$ and $G$, at the iterate $x^k=(U^k,V^k)\in\mathbb{X}$, the linear mapping $\mathcal{A}_k$ and the matrix $u^k$ in \eqref{Akmap} are specified as
\[
  \mathcal{A}_k(G,H):=\mathcal{A}(U^kH^{\top}+G(V^k)^{\top})\ \ {\rm for}\ \ (G,H)\in\mathbb{X}\ \ {\rm and}\ \
  u^k=((\mathcal{A}^*\xi)V^k,(\mathcal{A}^*\xi)^{\top}U^k).
\]

To formulate the sampling operator $\mathcal{A}:\mathbb{R}^{n_1\times n_2}\to\mathbb{R}^m$, we assume that a random index set $\Omega=\big\{(i_t,j_t)\ |\ t=1,\ldots,m\big\}$ is available, and the samples of the indices are drawn independently from a general sampling distribution $\Pi\!=\{\pi_{kl}\}_{k\in[n_1],l\in[n_2]}$ on $[n_1]\times[n_2]$. We adopt the same non-uniform sampling scheme as used in \cite{Fang18}, i.e., $\pi_{kl}=p_kp_l$ for each $(k,l)$ with
\begin{equation}\label{sampling-scheme}
 (S1)\ \
  p_k=\left\{\begin{array}{ll}
      2p_0& {\rm if}\ k\le\frac{n_1}{10}, \\
      4p_0& {\rm if}\ \frac{n_1}{10}\le k\le \frac{n_1}{5},\\
      p_0& {\rm otherwise}\\
     \end{array}\right.\ \ {\rm or}\ \
 (S2)\ \ p_k=\left\{\begin{array}{ll}
		3p_0& {\rm if}\ k\le\frac{n_1}{10}, \\
		9p_0& {\rm if}\ \frac{n_1}{10}\le k\le \frac{n_1}{5},\\
		p_0& {\rm otherwise}\\
	\end{array}\right.
\end{equation}
where $p_0>0$ is a constant such that $\sum_{k=1}^{n_1}p_k=1$ or $\sum_{l=1}^{n_2}p_l=1$. Then, the mapping $\mathcal{A}$ is defined by
\[
\mathcal{A}(X):=(X_{i_1,j_1},\,X_{i_2,j_2},\ldots,X_{i_m,j_m})^{\top}\quad{\rm for}\ X\in\mathbb{R}^{n_1\times n_2}.
\]
Let $b=\mathcal{A}(M_{\Omega})$ where $M_{\Omega}$ is an $n_1\times n_2$ matrix with $[M_{\Omega}]_{i_tj_t}=0$ if $(i_t,j_t)\notin\Omega$ and
\begin{equation}\label{observe}
	[M_{\Omega}]_{i_t,j_t}=M_{i_t,j_t}^*+\varpi_t\ \ {\rm if}\ (i_t,j_t)\in\Omega\ \ {\rm for}\ \ t=1,2,\ldots,m,
\end{equation}
where $M^*\!\in\mathbb{R}^{n_1\times n_2}$ is the true matrix of rank $r^*$ for synthetic data and for real data it is a matrix drawn from the original incomplete data matrix, and $\varpi=(\varpi_1,\ldots,\varpi_m)^{\top}$ is a sparse noisy vector. The nonzero entries of $\varpi$ obey one of the distributions: {\bf(I)} $N(0,10^2)$; {\bf(II)} Student's t-distribution with $4$ degrees of freedom $2\times t_4$; {\bf(III)} Cauchy distribution with density $d(u)=\frac{1}{\pi(1+u^2)}$; {\bf(IV)} mixture normal distribution $N(0,\sigma^2)$ with $\sigma\sim {\rm Unif}(1,5)$; {\bf(V)} Laplace distribution with density $d(u)=0.5\exp(-|u|)$.
%-------------------------------------------------------------------------------
\subsection{Choice of parameters}\label{sec6.1}

We choose $a=4,\rho=0.01$ for $\vartheta_2$, and  $r=\min(100,\lfloor\frac{\min(n_1,n_2)}{2}\rfloor),\lambda=c_{\lambda}\|b\|$ for model \eqref{SCAD-loss}, where $c_\lambda>0$ is specified in the numerical experiments. The parameters of Algorithm \ref{iLPA} are chosen as follows:
\[
 \overline{\varrho} = 2,\,\underline{\gamma}=\max(10,n_1/100),\,\overline{\gamma}=10^6,\,\, \gamma_{k,0}=\underline{\gamma},
\]
 and the parameter $\alpha_k$ involved in $\mathcal{Q}_{k,j}=\gamma_{k,j}\mathcal{I}+\alpha_k\mathcal{A}_k^*\mathcal{A}_k$ is updated when $\textrm{mod}(k,3)=0$ by
 \[
  \alpha_{k}=\max(\alpha_{k-1}/1.2,10^{-3})\ \ {\rm with}\ \ \alpha_0=0.5.
 \]
 We terminate Algorithm \ref{iLPA} at $x^k$ when $k>2000$ or either of the following conditions is satisfied:
\begin{equation}\label{stop-cond}
 \frac{\|x^k-x^{k-1}\|_F}{1\!+\|b\|}\le \epsilon_1\ \ {\rm and}\ \
  \frac{\max_{j\in\{1,\ldots,9\}}|\Phi(x^k)-\Phi(x^{k-j})|}{\max\{1,\Phi(x^k)\}}\le\epsilon_2\ \ {\rm for}\ k\ge\overline{k}=10,
\end{equation}
where the first condition is by Remark \ref{remark-alg} (iii), and the second one is by the decrease of objective values.
We use $\epsilon_1=5\times 10^{-6},\epsilon_2=5\times 10^{-4}$ for synthetic data, and $\epsilon_1= 10^{-4},\epsilon_2=5\times 10^{-3}$ for real data.

We compare the performance of Algorithm \ref{iLPA} armed with dPPASN with that of the following Polyak subgradient method (see \cite{Charisopoulos21,LiZhu20}). Since $\min_{x\in\mathbb{X}}\Phi(x)$ in step \ref{step4} is unavailable in practice, we replace the step-size $\frac{\Phi(x^k)-\min_{x\in\mathbb{X}}\Phi(x)}{\|\zeta^k\|^2}$ with $\frac{0.05\Phi(x^k)}{\|\zeta^k\|^2}$. Note that there is no convergence certificate for Algorithm \ref{subGrad} with such an approximate one, and we here use it just for numerical comparison. For the fairness, we adopt the same starting point $x^0$ and the same stopping condition as for iLPA except that $\overline{k}=500$.
%-------------------------------------------------------------------------------------
\begin{algorithm}[H]
 \renewcommand{\thealgorithm}{2} 	
 \caption{\label{subGrad}{\bf(subGM for solving model \eqref{SCAD-loss})}}
 \textbf{1: } Initialization: Choose an initial point $x^0\in\mathbb{X}$.\\
 \textbf{2: For}   $k=0,1,2,\cdots$  \textbf{do} \\
  \textbf{3: } \textbf{  }\textbf{  }
    Choose a subgradient $\zeta^k\in\partial\Phi(x^k)$.\\	
  \textbf{4:  }\label{step4} \textbf{   }\textbf{   }\textbf{  }\ Set $x^{k+1}=x^k-\frac{\Phi(x^k)-\min_{x\in\mathbb{X}}\Phi(x)}{\|\zeta^k\|^2}\zeta^k$.\\
\textbf{5: EndFor}.
\end{algorithm}

The numerical tests in sections \ref{sec6.2}-\ref{sec6.3} are all performed in MATLAB on a laptop running on 64-bit Windows System with an Intel Xeon(R) i7-12700H CPU 2.30GHz and 32 GB RAM.
%--------------------------------------------------------------------------------------
\subsection{Numerical results for synthetic data}\label{sec6.2}

We generate randomly the true $M^*\!\in\mathbb{R}^{n_1\times n_2}$ by $M^*=M_{L}^*(M_{R}^*)^{\top}$, where the entries of $M_{L}^*\in\mathbb{R}^{n_1\times r^*}$ and $M_{R}^*\in\mathbb{R}^{n_2\times r^*}$ are sampled independently from the standard normal distribution $N(0,1)$, and set the number of nonzero entries of $\varpi$ as $\lfloor0.3m\rfloor$. We choose $x^0=(U_{1}\Sigma_{r}^{1/2},V_{1}\Sigma_{r}^{1/2})$ as the starting point of iLPA and subGM, where $U_{1}$ and $V_1$ are the matrix consisting of the first $r$ largest left and right singular vectors of $M_{\Omega}$, respectively, and $\Sigma_{r}$ is the diagonal matrix consisting of the first $r$ largest singular values of $M_{\Omega}$ arranged in an nonincreasing order. We evaluate the effect of matrix recovery by the relative error (RE), defined by $\frac{\|x^{\rm out}-M^*\|_F}{\|M^*\|_F}$, where $x^{\rm out}$ represents the output of a solver.

We first check how the relative errors vary with $c_{\lambda}$ or $\lambda$. Figure \ref{fig0} indicates that for iLPA and subGM, there exists an interval of $\lambda$ such that the stationary points yielded by them have satisfactory relative errors and their ranks are equal to $r^*$, and such an interval for iLPA is remarkably larger than the one for subGM, which means that iLPA has better robustness. All curves in Figures \ref{fig0} are the average results obtained by running $10$ examples generated randomly with noise of type V, $n_1=n_2=1000,r^*=5$ and sampling ratio $\textrm{SR}=0.25$.
%-----------------------------------------------------------------------------------------------


 \begin{figure}[H]
	\centering
	\subfigure{\includegraphics[height=6.5cm,width=6.5cm]{fig_lambda_iLPA.eps}}
	\subfigure{\includegraphics[height=6.5cm,width=6.5cm]{fig_lambda_subgrad.eps}}
	\caption{The curves of relative error and rank for two solvers as $c_{\lambda}$ increases}	\label{fig0}
\end{figure}

  Next we consider the recovery effect and running time (in seconds) of two solvers under different setting of $n_1\!=n_2, r^*$ and $\textrm{SR}$. Table \ref{tab1} reports the average results obtained by running $10$ examples generated randomly under each setting. We see that the average relative errors and ranks yielded by iLPA for five kinds of outliers are all lower than those yielded by subGM, and the average ranks returned by iLPA all coincide with the true rank, whereas some average ranks returned by subGM are higher than the true rank for some types of noise. Moreover, iLPA yields the desirable results with one $c_{\lambda}$ except for noise of type III, but subGM yields the results with well-selected $c_{\lambda}$ for different kinds of noises. The average CPU time required by subGM is at least 5 times more than that of iLPA.
%---------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \setlength{\abovecaptionskip}{-0.1cm}
 \begin{table}[h]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.0pt}
 	\renewcommand\arraystretch{1.2}
 	\centering
 	\scriptsize
 	\caption{\small Average RE and time for examples generated with the sampling scheme S1}\label{tab1}
 	\scalebox{1}{
 	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
 			%\hline
 	\Xhline{0.7pt}
 	& & \multicolumn{4}{l}{\  iLPA ($n_1\!=\!1000$,$r^*\!\!=\!10$)}&\multicolumn{4}{l||}{\  subGM ($n_1\!=\!1000$,$r^*\!\!=\!10$)}& \multicolumn{4}{l}{\ iLPA ($n_1\!=\!3000$,$r^*\!\!=\!15$)}&
 	\multicolumn{4}{l}{\ subGM ($n_1\!\!=\!3000$,$r^*\!\!=\!15$)}\\
 %%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
 	\hline		
 	$\varpi$ & SR&$c_{\lambda}$& RE & rank  &time&$c_{\lambda}$&  RE & rank & time&$c_{\lambda}$& RE & rank  &time&$c_{\lambda}$&  RE & rank & time\\
 	\hline			
 	&0.15 &0.06&\ {\color{red}\bf 3.78e-2} & 10 & 4.02 &0.06&\ 9.97e-2 & 10 & 51.5     &0.06& \ {\color{red}\bf 7.53e-4} & 15 & 22.1 &0.06&\ 4.00e-2 & 16 & 189.8\\
 I	&0.20 &0.06&\ {\color{red}\bf 3.95e-2} & 10 & 5.85 &0.06&\ 6.50e-2 & 10 & 54.6     &0.06&\ {\color{red}\bf 6.23e-4} & 15 & 24.1 &0.06&\ 3.82e-2 & 16 & 206.6 \\
 	&0.25 &0.06&\ {\color{red}\bf 1.04e-3} & 10 & 3.27 &0.06&\ 4.81e-2 & 10 & 55.8     &0.06&\ {\color{red}\bf 5.93e-4} & 15 & 26.0 &0.06&\ 3.09e-2 & 15 & 245.5\\
 	\hline
 	&0.15 &0.06&\ {\color{red}\bf 5.41e-3} & 10 & 4.94 &0.10&\ 2.78e-2  & 10 & 68.3    &0.06&\ {\color{red}\bf 7.23e-4} & 15 & 38.2 &0.15&\ 2.42e-2  & 16 & 207.4\\
 II	&0.20 &0.06&\ {\color{red}\bf 1.44e-3} & 10 & 5.30 &0.10&\ 2.40e-2  & 10 & 48.7    &0.06&\ {\color{red}\bf 5.87e-4} & 15 & 40.3 &0.15&\ 1.84e-2  & 15 & 208.2\\
 	&0.25 &0.06&\ {\color{red}\bf 1.07e-3} & 10 & 5.18 &0.10&\ 9.30e-3  & 10 & 50.2    &0.06&\ {\color{red}\bf 5.13e-4} & 15 & 42.4 &0.15&\ 1.13e-2  & 15 & 235.0\\
 	\hline
 	&0.15 &0.02&\ {\color{red}\bf 7.23e-2}& 10 & 16.2  &0.03 &\ 1.63e-1 & 11 & 48.9   &0.02&\ {\color{red}\bf 1.54e-3} & 15 & 91.8  &0.10&\ 8.00e-2 & 16 & 188.7 \\
III	&0.20 &0.02&\ {\color{red}\bf 5.36e-3}& 10 & 15.6  &0.03 &\ 7.51e-2 & 11 & 53.6   &0.02&\ {\color{red}\bf 1.69e-2} & 15 & 422.4 &0.10&\ 1.21e-1 & 16 & 208.1\\
 	&0.25 &0.02&\ {\color{red}\bf 2.27e-3}& 10 & 12.3  &0.03 &\ 4.78e-2 & 11 & 55.6   &0.02&\ {\color{red}\bf 9.53e-4} & 15 & 180.2 &0.10&\ 9.00e-3 & 16 & 231.7\\
 			
    \hline
 	&0.15 &0.06&\ {\color{red}\bf 7.62e-3}& 10 & 4.22  &0.08 &\ 2.99e-2 & 10 & 45.9   &0.06&\ {\color{red}\bf 6.50e-4} & 15 & 31.9 &0.15&\ 3.38e-2 & 16 & 182.7\\
IV	&0.20 &0.06&\ {\color{red}\bf 1.69e-3}& 10 & 4.64  &0.08 &\ 2.00e-2 & 10 & 48.6   &0.06&\ {\color{red}\bf 5.35e-4} & 15 & 34.1 &0.15&\ 2.58e-2 & 15 &200.5\\
	&0.25 &0.06&\ {\color{red}\bf 1.15e-3}& 10 & 4.49  &0.08 &\ 1.10e-2 & 10 & 53.6   &0.06&\ {\color{red}\bf 5.44e-4} & 15 & 39.3 &0.15&\ 1.77e-2 & 15 & 247.0\\
	\hline
 			
   &0.15 &0.06&\ {\color{red}\bf 3.43e-3}& 10 & 5.22  &0.15 &\ 4.86e-2 & 11 & 82.0   &0.06&\ {\color{red}\bf 7.37e-4} & 15 & 40.2 &0.15&\ 2.09e-2  & 15 & 202.7\\
V &0.20 &0.06&\ {\color{red}\bf 1.50e-3}& 10 & 5.64  &0.15 &\ 2.69e-2 & 10 & 64.9   &0.06&\ {\color{red}\bf 7.15e-4} & 15 & 45.3 &0.15&\ 1.50e-2  & 15 & 209.0\\
  &0.25 &0.06&\ {\color{red}\bf 1.18e-3}& 10 & 6.27  &0.15 &\ 1.35e-2 & 10 & 52.8   &0.06&\ {\color{red}\bf 6.20e-4} & 15 & 45.4 &0.15&\ 9.80e-3  &15 & 232.9 \\
 			
 \Xhline{0.7pt}
 \end{tabular}}
 \end{table}

%-------------------------------------------------------------------------------------------
\subsection{Numerical results for real data}\label{sec6.3}

 We test iLPA and subGM with matrix completion problems on some real data sets, including the jester joke dataset, the movieLens dataset, the netflix dataset, and the yahoo-music dataset. For each dataset, let $M^0$ denote the original incomplete data matrix such that the $i$th row of $M^0$ corresponds to the ratings given by the $i$th user. We first consider the jester joke dataset from \url{http://www.ieor.berkeley.edu/~goldberg/jester-data/}, and more detailed descriptions can be found in \cite{Chen12,Toh10}. Due to the large number of users, we randomly select $n_1$ users' ratings from $M^0$, and then randomly permute the ratings from the users to generate $M^*\!\in\mathbb{R}^{{n_1}\times 100}$. We generate a set of observed indices $\Omega$ with the sampling scheme S1, and then the observation matrix $M_{\Omega}$ via \eqref{observe}. Since we can only observe those entries $M_{jk}$ with $(j,k)\in\Omega$ and $M_{jk}$ available, the actual sampling ratio is less than the input SR. Since many entries are unknown, we cannot compute the relative error as we did for the simulated data. Instead, we take the metric of the normalized mean absolute error (NMAE) to measure the accuracy:
\[
 {\rm NMAE}=\frac{\sum_{(i,j)\in\Gamma\backslash\Omega}|X^{\rm out}_{i,j}-M_{i,j}|}
 {|\Gamma\backslash\Omega|(r_{\rm max}-r_{\rm min})}\ \ {\rm with}\ X^{\rm out}=U^{\rm out}(V^{\rm out})^{\top},
\]
where $\Gamma:=\{(i,j)\in[n_{1}]\times[100]:\ M_{ij}\ \textrm{is given}\}$ denotes the set of indices for which $M_{ij}$ is given, and $r_{\rm min}$ and $r_{\rm max}$ denote the lower and upper bounds for the ratings, respectively. For example, the range of the jester joke dataset is $[-10,10]$, so we have $r_{\rm max}-r_{\rm min}=20$.

For the jester joke dataset, we consider different settings of $n_1$ and SR, and report the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting in Table \ref{tabJester3}. Among others, the results with lower NMAEs as well as smaller ranks  are marked in red. We see that iLPA yields comparable even a little better results than subGM for all the jester-3 examples.
%----------------------------------------------------------------------------
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[h]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for jester-3 dataset}\label{tabJester3}
	\scalebox{1}{
	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad  iLPA ($n_{1}\!=\!1000$)}&\multicolumn{4}{l||}{\quad subGM ($n_1\!=\!1000$)}& \multicolumn{4}{l}{\quad iLPA ($n_{1}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{1}\!\!=\!5000$)}\\
		%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			\hline
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& 0.2285 & 3 & 0.19 &0.2& 0.2256 & 4 & 0.60      &0.2& {\color{red}\bf 0.2218} & 5 & 1.46 &0.2& 0.2233 & 5 & 9.42\\
			I	&0.20 &0.2& 0.2256 & 3 & 0.16 &0.2& 0.2214 & 4 & 0.98      &0.2&  0.2198 & 6 & 1.60 &0.2& 0.2214 & 5 & 10.5  \\
			&0.25 &0.2& 0.2208 & 3 & 0.17 &0.2& 0.2195 & 4 & 0.90      &0.2& {\color{red}\bf 0.2153} & 6 & 1.49 &0.2& 0.2171 & 6 & 10.9\\
			\hline
			&0.15 &0.2& {\color{red}\bf 0.2168} & 4 & 0.16 &0.2& 0.2170 & 5 & 1.17     &0.2&  0.2154 & 7 & 2.15 &0.2&  0.2155 & 6 & 10.0\\
			II	&0.20 &0.2& {\color{red}\bf 0.2119} & 5 & 0.15 &0.2 & 0.2121 & 5 & 1.13    &0.2& {\color{red}\bf 0.2127} & 7 & 2.50 &0.2& 0.2133 & 7 & 10.4 \\
			&0.25 &0.2& 0.2085 & 6 & 0.15 &0.2&0.2087 & 5 & 1.25       &0.2&  0.2067 & 9 & 2.49 &0.2& 0.2074  & 8& 10.6\\	
			\hline
			&0.15 &0.02& 0.2309 & 11 & 0.33 &0.02& 0.2255 & 13 & 1.19     &0.02& 0.2215 & 15 & 2.84 &0.02&\ 0.2209 & 16 & 10.0\\
			III	&0.20 &0.02& 0.2260 & 14 & 0.35 &0.02&0.2232 & 18 & 1.17     &0.02& {\color{red}\bf 0.2140}  &8 & 1.95 &0.02& 0.2144 & 8 & 10.3 \\
			&0.25 &0.02& 0.2235 & 13 & 0.22 &0.02& 0.2228 & 15 & 1.01     &0.02& 0.2104 & 15 & 2.49 &0.02& 0.2115  & 14& 10.7\\	
			\hline
			&0.15 &0.2& {\color{red}\bf 0.2180}& 3 & 0.16 &0.2& 0.2182 & 5 & 1.06                 &0.2& {\color{red}\bf 0.2164} & 6 & 2.31 &0.2& 0.2166 & 6 & 10.1\\
			IV	&0.20 &0.2& {\color{red}\bf 0.2131}& 5 & 0.16 &0.2 & 0.2134 & 5 & 1.10                &0.2& {\color{red}\bf0.2137} &7 & 2.35 &0.2& 0.2145 & 7 & 10.4 \\
			&0.25 &0.2&{\color{red}\bf 0.2104}& 5& 0.15 &0.2&{\color{red}\bf0.2104} & 5 & 1.27    &0.2& {\color{red}\bf0.2085} & 9 & 2.61 &0.2& 0.2092   & 9 & 10.6\\	
			\hline
			
			&0.15 &0.2& {\color{red}\bf 0.2165}  & 4 & 0.18 &0.2& 0.2167 & 5 & 1.12       &0.2& 0.2152 & 6 & 2.21 &0.2& {\color{red}\bf0.2151} & 6 & 9.97\\
			V	&0.20 &0.2& {\color{red}\bf 0.2116} & 5 & 0.17 &0.2 & 0.2119 & 5 & 1.17        &0.2& {\color{red}\bf 0.2122}  &7 & 2.44 &0.2& 0.2129 & 7 & 10.3 \\
			&0.25 &0.2& 0.2083 & 6 & 0.17 &0.2&0.2084 & 5 & 1.30           &0.2& {\color{red}\bf 0.2062} & 9 & 2.47 &0.2& 0.2069 & 9 & 10.5\\	
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}

 Next we consider the movieLens dataset from \url{http://www.grouplens.org/node/73}, for which the ratings range is from $r_{\rm min}=1$ to $r_{\rm max}=5$; see \cite{Chen12,Toh10} for more details. For the movie-100K dataset, we consider the data matrix $\widetilde{M}^0\!=M^0-3$ just like \cite{Toh10}, sample the observed entries in two schemes, and obtain  $M_{\Omega}$ via \eqref{observe} with $M^*\!=\widetilde{M}^0$. Table \ref{tabMovie-100K} reports the average NMAE, rank and running time (in seconds) after running $10$ times for $(n_1,n_2)=(943,1682)$. We see that iLPA yields better results than subGM for those examples generated by the scheme S1 for type III-V noise, and has a little better preformance than subGM for those examples generated with the scheme S2 for noise of types II-V.


%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for movie-100K dataset}\label{tabMovie-100K}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad \quad \ iLPA (S1)}&\multicolumn{4}{l||}{\quad \quad subGM (S1)}& \multicolumn{4}{l}{\quad \quad \  iLPA (S2)}&
			\multicolumn{4}{l}{\quad \quad subGM (S2)}\\
			%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			\hline
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			
			&0.15 &0.1&  0.2361 & 2 & 2.23 &0.1&  {\color{red}\bf 0.2311} & 2 & 6.34     &0.1&  0.2402 & 3 & 2.41 &0.1&  {\color{red}\bf 0.2342} & 2 & 6.52\\
			I	&0.20 &0.1&0.2246 & 2 & 2.12 &0.1& {\color{red}\bf 0.2229} & 2 & 6.48       &0.1&0.2259 & 2 & 2.47 &0.1& {\color{red}\bf 0.2240} & 2 & 6.70 \\
			&0.25 &0.1&  0.2186 & 2 & 2.37 &0.1& {\color{red}\bf 0.2176} & 2 & 6.57     &0.1&  0.2224 & 2 & 2.77 &0.1&  {\color{red}\bf \ 0.2201 }& 2 & 6.72 \\			
			\hline	
			
			&0.15 &0.3&  0.2249 & 3 & 2.42 &0.3&  {\color{red}\bf 0.2248} & 2 & 6.59    &0.3&  {\color{red}\bf0.2275} & 1 & 2.34 &0.3&   0.2284 & 2 & 6.44\\
			II  &0.20 &0.3&  0.2171 & 2 & 2.17 &0.3&  0.2174 & 1 & 6.71 &0.3&{\color{red}\bf0.2184} & 1 & 2.32 &0.3&  0.2185 & 2 & 6.72 \\
			&0.25 &0.3& {\color{red}\bf 0.2119} & 1 & 1.92 &0.3&  0.2123 & 1 & 6.74 &0.3& {\color{red}\bf 0.2142} & 1 & 1.95 &0.3&  0.2149 & 3 & 6.59\\
			\hline
			
			
			&0.15 &0.02& {\color{red}\bf  0.2369} & 2 & 1.65 &0.02& 0.2438 & 36 & 10.2  &0.02& {\color{red}\bf0.2397} & 3 & 1.91 &0.02&  0.2441 & 30 & 10.4\\
			III	&0.20 &0.02&{\color{red}\bf0.2312} & 1 & 1.69 &0.02& 0.2343 & 22 & 6.72     &0.02&{\color{red}\bf0.2314} & 2 & 1.77 &0.02&  0.2345& 15 & 6.58 \\
			&0.25 &0.02& {\color{red}\bf 0.2231} & 2 & 2.03 &0.02&  0.2263 & 15 & 6.73                   &0.02& {\color{red}\bf 0.2232} & 2 & 1.83 &0.02&  0.2271 & 9 & 6.59 \\
			\hline
			
			
			&0.15 &0.3& {\color{red}\bf 0.2277} & 1 & 1.69 &0.3&  0.2318 & 1& 6.58          &0.3&  {\color{red}\bf 0.2306} & 1 & 1.76 &0.3&   0.2335& 1 & 6.76\\
			IV	&0.20 &0.3&{\color{red}\bf0.2213} & 1 & 1.56 &0.3&  0.2238& 1 & 6.67            &0.3&{\color{red}\bf0.2226} & 1 & 1.56 &0.3&  0.2251 & 1 & 6.71\\
			&0.25 &0.3& {\color{red}\bf 0.2164} & 1 & 1.53 &0.3&  0.2184 & 1 & 6.76         &0.3& {\color{red}\bf 0.2190} & 1 & 1.49 &0.3&  0.2208 & 1 & 6.71\\
			\hline
			
			
			&0.15 &0.3&  {\color{red}\bf0.2218} & 2 & 2.41 &0.3&   0.2220& 3 & 6.45      &0.3& {\color{red}\bf 0.2268}& 4 & 2.92 &0.3&  0.2270 & 4 & 6.90\\
			V	&0.20 &0.3&{\color{red}\bf0.2152} & 2 & 2.10 &0.3&  0.2153 & 2 & 6.73        &0.3&{\color{red}\bf0.2161} & 3 & 2.07 &0.3&  0.2165 & 3 & 6.59 \\
			&0.25 &0.3&  0.2098& 3 & 2.32 &0.3&  0.2100 & 2 & 6.72      &0.3& {\color{red}\bf 0.2121}& 3 & 2.13 &0.3&  0.2127 & 3 & 6.63\\
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}


 For the movie-1M dataset, we first randomly select $n_1$ users and their $n_2$ column ratings from $M^0$ to formulate $M^*\in\mathbb{R}^{n_1\times n_2}$, sample the observed entries with the scheme S1, and then obtain the observation matrix $M_{\Omega}$ via \eqref{observe}. We consider the setting of $n_1=n_2=3000$ and the setting of $(n_1,n_2)=(6040,3706)$. Table \ref{tabMovie-1M} reports the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting. When $n_1=3000$, iLPA yields better results than subGM except for noise of type V, and when $n_1=6040$, it yields comparable results as subGM does.
%--------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.4pt}
  \renewcommand\arraystretch{1.2}
  \centering
  \scriptsize
  \caption{\small Average NMAE and running time of two solvers for movie-1M dataset}\label{tabMovie-1M}
   \scalebox{1}{
   	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
	\Xhline{0.7pt}
   & & \multicolumn{4}{l}{\  iLPA($n_{1}\!=\!3000$)}&\multicolumn{4}{l||}{\  subGM($n_1\!=\!3000$)}& \multicolumn{4}{l}{\ iLPA($6040\times 3706$)}&
	\multicolumn{4}{l}{\ subGM($6040\times 3706$)}\\
%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
	\hline		
	$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
	\hline	
			
	& 0.15 &0.1&  0.2173 & 1 & 7.96 &0.1& 0.2172 & 2 & 33.1  &0.1&  0.2108 & 1 & 24.3 &0.1&  0.2100 & 3 & 75.1\\
 I  & 0.20 &0.1&{\color{red}\bf0.2110} &1&7.65 &0.1&  0.2110&2 &34.0     &0.1&  0.2065 & 1 & 24.1 &0.1& 0.2060 & 2 & 77.5 \\
	&0.25 &0.1& {\color{red}\bf 0.2061} & 1 & 7.39 &0.1&  0.2062 & 3 & 34.4              &0.1& {\color{red}\bf 0.2010} & 1 & 23.7 &0.1&  0.2015 & 3 & 79.8\\
			\hline			
			
	& 0.15 &0.3&  {\color{red}\bf 0.2113} & 1 & 11.5 &0.3&  0.2123 & 1 & 32.5                 &0.3&{\color{red}\bf 0.2048} & 1 & 26.8 &0.3&  0.2049 & 1 & 75.4\\
II	&0.20 &0.3&  {\color{red}\bf 0.2057} & 1 & 9.41 &0.3&  0.2062 & 1 & 32.9 &0.3&  0.2013 & 1 & 26.1 &0.3& 0.2007 & 2 & 76.4\\
	& 0.25 &0.3& {\color{red}\bf 0.2012} & 1 & 9.28 &0.3&{\color{red}\bf0.2012}&1&41.5 &0.3&  0.1967 & 1 & 24.6 &0.3& 0.1959 & 2 & 79.6 \\
			
	\hline
			
	& 0.15 &0.02&  {\color{red}\bf0.2167} & 1 & 6.82 &0.02&  0.2207 & 1 & 32.0     &0.02&  0.2022 & 1 & 25.2 &0.02&  {\color{red}\bf0.2020} & 1 & 75.8\\
III	& 0.20 &0.02&  {\color{red}\bf0.2073} & 1 & 6.66 &0.02&  0.2099 & 1 & 32.6     &0.02&  0.1956 & 2 & 32.9 &0.02& {\color{red}\bf 0.1946} & 2 & 76.4\\
	& 0.25&0.02& {\color{red}\bf 0.2007} & 1 & 7.15 &0.02&  0.2022 & 1 & 33.3      &0.02& {\color{red}\bf 0.1880} & 3 &34.1 &0.02& 0.1881 & 3 & 79.2\\
			
	\hline
	&0.15 &0.3& {\color{red}\bf 0.2166} & 1 & 8.84 &0.3&  0.2187 & 1 & 31.6    &0.3& {\color{red}\bf \ 0.2095 }& 1 & 23.4 &0.3&  0.2104 & 1 & 75.1\\
IV  & 0.20 &0.3&  {\color{red}\bf0.2107} & 1 & 8.42 &0.3& 0.2119 & 1 & 32.1    &0.3& {\color{red}\bf 0.2052} & 1 & 23.6 &0.3&  0.2058 & 1 & 76.5\\
	& 0.25 &0.3& {\color{red}\bf 0.2060} & 1 & 8.48 &0.3&  0.2069 & 1 & 32.5   &0.3& {\color{red}\bf 0.2006} & 1 & 22.3 &0.3&  0.2011 & 1 & 79.4 \\			
			
	\hline
			
	&0.15 &0.3& 0.2086 & 3 & 13.7 &0.3&  0.2095 & 2 & 31.8 &0.3&  0.2028 & 2 & 31.5 &0.3& 0.2019 & 3 & 77.1\\
V	& 0.20 &0.3& 0.2022 & 3 & 13.2 &0.3& 0.2029 & 2 & 41.0      &0.3&  0.1980 & 2 & 30.8 &0.3& 0.1971 & 3 & 78.2\\
	& 0.25 &0.3&  0.1992 & 1 & 9.84 &0.3& 0.1971 & 2 & 32.6     &0.3&  0.1927 & 2 & 29.6 &0.3&  {\color{red}\bf0.1917} & 2 & 80.4\\
			
   \Xhline{0.7pt}
	\end{tabular}}
 \end{table}

 We consider the netflix dataset which can be downloaded from \url{https://www.kaggle.com/netflix-inc/netflix-prize-data\#qualifying.txt}. For this dataset, we first randomly select $n_1$ users with $n_1=6000$ and $10000$ and their $n_2=n_1$ column ratings from $M^0$, sample the observed entries with the sampling scheme S1, and then obtain $M_{\Omega}$ via \eqref{observe} with $M^*=M^0$. Table \ref{tabNetflix} reports the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting. We see that iLPA yields better results than subGM for $n_1=6000$, and for $n_1=10000$ they yield the comparable NMAEs but the ranks returned by iLPA are generally smaller than those did by subGM.
%---------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[h]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.4pt}
  \renewcommand\arraystretch{1.2}
  \centering
  \scriptsize
  \caption{\small Average NMAE and running time of two solvers for netflix dataset}\label{tabNetflix}
  \scalebox{1}{
   \begin{tabular}{cc|lccc|lccc||lccc|lccc}
  \Xhline{0.7pt}
	& & \multicolumn{4}{l}{\  iLPA($n_{1}\!=\!6000$)}&\multicolumn{4}{l||}{\  subGM($n_1\!=\!6000$)}& \multicolumn{4}{l}{\ iLPA($n_{1}\!=\!10000$)}&
	\multicolumn{4}{l}{\ subGM($n_{1}\!\!=\!10000$)}\\
	%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
	\hline		
	$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
	\hline	
	&0.15 &0.1& {\color{red}\bf 0.2259} & 2 & 48.2 &0.1&   0.2274 & 3 & 155.3& 0.1&  0.2186 & 2 & 130.2 &0.1&  0.2178 & 3 & 382.3\\
I	&0.20 &0.1& {\color{red}\bf 0.2200} & 2 & 40.7 &0.1&  0.2206 & 3 & 124.5 & 0.1& {\color{red}\bf 0.2130} & 3 & 110.1 &0.1& 0.2212 & 4 & 495.1 \\
	&0.25 &0.1&  0.2143 & 2 & 41.1 &0.1& 0.2139 & 3 & 119.2 & 0.1& 0.2080 & 2 & 112.8 &0.1&  0.2068 & 5 & 806.2\\
	\hline
			
    &0.15  &0.3& {\color{red}\bf\ 0.2177 }& 2 & 57.8 &0.3&  0.2181 & 3 & 135.9 &0.3&  0.2109 & 2 & 161.3 &0.3& 0.2102 & 3 & 317.3\\
II	&0.20 &0.3& {\color{red}\bf0.2125} & 2 & 57.2 &0.3&  0.2127 & 2 & 127.0 &0.3&  0.2056 & 3 & 143.9 &0.3& {\color{red}\bf 0.2046} & 3 & 319.1  \\
	&0.25 &0.3& {\color{red}\bf 0.2070} & 2 & 54.5 &0.3&  {\color{red}\bf0.2070} & 2 & 117.7  &0.3& 0.2006 & 2 & 140.3 &0.3&  {\color{red}\bf0.1999} & 2 & 320.8\\
	\hline
			
  	&0.15 &0.02&  {\color{red}\bf\ 0.2235 }& 1 & 32.2 &0.02&  0.2264 & 1 & 117.2  &0.02&  0.2071 & 3 & 162.7    &0.02&  {\color{red}\bf0.2060} & 3 & 297.4\\
III  &0.20 &0.02& {\color{red}\bf 0.2144} & 1 & 36.8 &0.02& 0.2159 & 1 & 115.4 &0.02&  0.2006 & 3 & 168.2 &0.02& 0.1988 & 4 & 299.9\\
	&0.25 &0.02& {\color{red}\bf 0.2065} & 2 & 38.5 &0.02&  0.2078 & 2 & 116.2 &0.02&  0.1942 & 5 & 177.2 &0.02&  0.1929 & 6 & 552.7 \\
			\hline
			
	&0.15 &0.3& {\color{red}\bf 0.2228} & 1 & 43.1 &0.3& 0.2245 & 1 & 109.4 &0.3&  0.2165 & 1 & 117.1 &0.3&  0.2161 & 2 & 314.7\\
IV	&0.20 &0.3& 0.2157 & 2 & 42.1 &0.3& 0.2188 & 1 & 111.3 &0.3&  0.2133 & 1 & 108.2 &0.3& 0.2105 & 2 & 316.6 \\
	&0.25 &0.3& 0.2124 & 2 & 39.8 &0.3& 0.2136 & 1 & 112.6 &0.3&  0.2073 & 1 & 112.0 &0.3&  0.2065 & 2 & 322.0\\
	\hline
			
	&0.15  &0.3&  {\color{red}\bf0.2152} & 3 & 68.1 &0.3& 0.2157 & 4 & 159.1 &0.3&  0.2086 & 3 & 181.9 &0.3&  0.2077 & 4 & 312.1\\
V  &0.20  &0.3&  {\color{red}\bf0.2100} & 3 & 62.0 &0.3& 0.2104 & 4 & 128.3 &0.3&  0.2035 & 3 & 163.5 &0.3& {\color{red}\bf 0.2020} & 3 & 313.3 \\
   &0.25 &0.3& {\color{red}\bf 0.2041} & 3 & 56.2 &0.3& 0.2044 & 4 & 146.8  &0.3&  0.1981 & 3 & 150.6 &0.3&  {\color{red}\bf0.1972} & 3 & 318.9 \\
			
  \Xhline{0.7pt}
 \end{tabular}}
\end{table}

The above numerical results demonstrate that iLPA yields comparable even a little better results than subGM within less running time. We also apply iLPA to some large-scale real dataset, and Table \ref{tabNetflix2} reports the NMAE, rank and running time (in seconds) obtained by running an example for each setting, on a workstation running on 64-bit Windows Operating System with an Intel Xeon(R) W-2245 CPU 3.90GHz and 128 GB RAM. We see that for $50000\times 17770$ netflix and $71567\times 10677$ movie-10M test examples, iLPA yields the favorable outputs in $3100$ seconds, while for $50000\times 10000$ yahoo-music example, it requires much more time when ${\rm SR}=0.25$, and their NMAEs are worse than other examples.
 %-------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
 	\setlength{\belowcaptionskip}{-0.01cm}
 	\setlength\tabcolsep{2.5pt}
 	\renewcommand\arraystretch{1.3}
 	\centering
 	\scriptsize
 	\caption{\small NMAE and running time of iLPA for large-scale examples from three datasets}\label{tabNetflix2}
 	\scalebox{1}{
 		\begin{tabular}{cc|lccc||lccc||lccc}
 			\hline
 			& & \multicolumn{4}{l||}{\  netflix($50000\times 17770$)}&\multicolumn{4}{l||}{\ movie-10M($71567\times 10677$)}&
 			\multicolumn{4}{l}{\ yahoo-music($50000\times 10000$)}\\
 			%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
 			\hline
 			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & \ rank  &time&$\ c_{\lambda}$&  NMAE &\ rank & time&$\ c_{\lambda}$& NMAE & \ rank  &time\\
 			\hline	
 			&0.15 &0.15&  0.2135 & 1 & 1726 & 0.15&  0.2061 & 3 & 1527 &0.08&  0.3556 & 8 & 1615\\
 			I   &0.20 &0.15&  0.2127 & 3 & 1238 & 0.15& 0.2015 & 3 & 1275 &0.08& 0.3550 & 6 & 1304 \\
 			&0.25 &0.15&  0.2035 & 2 & 2035 & 0.15& 0.1955 & 3 &1273 &0.08&  0.3343 & 7 & 3723\\
 			\hline
 			
 			&0.15  &0.35&  0.2072 & 3 & 2087 &0.40&  0.1981 & 6 & 1997 &0.38& 0.3572 & 2 & 1106\\
 			II	&0.20 &0.35&0.2030 & 4 & 1943  &0.40&  0.1936 & 4 & 1875 &0.38& 0.3576 &1 & 815  \\
 			&0.25 &0.35&  0.1941&4 & 1950  &0.40& 0.1855 & 5 & 2104 &0.38&  0.3271 &1 & 9150\\
 			\hline
 			
 			&0.15 &0.1&   0.2014 & 2 & 2666   &0.10&  0.1948 & 2 & 2203 &0.03& 0.3568 & 5 & 862\\
 			III     &0.20 &0.1&  0.1988 & 4 & 2121   &0.10&  0.1892 & 3 & 2044 &0.03& 0.3292 & 1 & 4560\\
 			&0.25 &0.1&  0.1901 & 4 & 2107   &0.10&  0.1800 & 4 & 2511 &0.03&  0.3038 & 3 & 8218 \\
 			\hline
 			
 			&0.15 &0.35&  0.2100 & 3 & 1855     &0.40&  0.2012 & 2 & 1872 &0.24&  0.3556 & 3 & 1728\\
 			IV	&0.20 &0.35& 0.2062 & 3 & 1690     &0.40&  0.1963 & 3 & 1665 &0.24&  0.3563 & 2 & 1099 \\
 			&0.25 &0.35& 0.1974 & 2 & 1882     &0.40&  0.1917 & 3 & 1474 &0.24&  0.3250 & 1 & 7967\\
 			\hline
 			
 			&0.15  &0.35&  0.2050& 5 & 2265  &0.40&  0.1956 & 7 & 2269 &0.40&  0.3570 & 2 & 1233\\
 			V  &0.20&0.35& 0.2004& 5 & 2187  &0.40&  0.1888 & 4 & 3077 &0.40& 0.3573 &1 & 843 \\
 			&0.25 &0.35& 0.1915 &5 & 1934   &0.40&  0.1827 & 8 & 2396 &0.40&0.3208 & 1 & 11581 \\
 			
 			\hline
 	\end{tabular}}
 \end{table}

%------------------------------------------------------------------------------------
\section{Conclusions}\label{sec6.0}

 We proposed an inexact LPA for solving the DC composite optimization problem \eqref{prob}, and established the convergence of its iterate sequence under Assumptions \ref{ass0}-\ref{ass1} and the KL property of the potential function $\Xi$. Furthermore, we also provided a verifiable condition for the KL property of $\Xi$ with exponent $p=1/2$, a guarantee for the local R-linear convergence rate of its iterate sequence, by leveraging such a property of $f$ in \eqref{ffun} and condition \eqref{key-cond}, and discussed its relation with the regularity or quasi-regularity conditions used in \cite{HuYang16} for the case $\vartheta_2\equiv 0$ and $h\equiv 0$. The proposed iLPA, armed with dPPASN for solving subproblems, is applied to matrix completions with outliers and non-uniform sampling, and extensive tests on synthetic and real data confirm its efficiency for computing the robust factorization model \eqref{SCAD-loss}.
\begin{thebibliography}{10}
\bibitem{Artacho08}
{\sc F.~J. Arag\'{o}n~Artacho and M.~H. Geoffroy}, {\em Characterization of
  metric regularity of subdifferential}, Journal of Convex Analysis, 15 (2008),
  pp.~365--380.


 % \bibitem{Ackooij19}
%{\sc W.~V. Ackooij and W.~D. Oliveira}, {\em Non-smooth DC-constrained optimization: %constraint qualification and minimizing methodologies}, Optimization Method \& Software, 34 %(2019),
 % pp.~1029--4937.

\bibitem{Artacho20}
{\sc F.~J.~A. Artacho and P.~T. Vuong}, {\em The boosted difference of convex
  functions algorithm for nonsmooth functions}, SIAM Journal on Optimization,
  30 (2020), pp.~980--1006.

\bibitem{Attouch09}
{\sc H.~Attouch and J.~Bolte}, {\em On the convergence of the proximal
  algorithm for nonsmooth functions involving analytic features}, Mathematical
  Programming, 116 (2009), pp.~5--16.

\bibitem{Attouch10}
{\sc H.~Attouch, J.~Bolte, P.~Redont, and A.~Soubeyran}, {\em Proximal
  alternating minimization and projection methods for nonconvex problems: an
  approach based on the kurdyka-{\l}ojasiewicz inequality}, Mathematics of
  Operations Research, 35 (2010), pp.~438--457.

\bibitem{Auslender10}
{\sc A.~Auslender, R.~Shefi, and M.~Teboulle}, {\em A moving balls
  approximation method for a class of smooth constrained minimization
  problems}, SIAM Journal on Optimization, 20 (2010), pp.~3232--3259.

\bibitem{BaiLi22}
{\sc S.~X. Bai, M.~H. Li, C.~W. Lu, D.~L. Zhu, and S.~E. Deng}, {\em The
  equivalence of three types of error bounds for weakly and approximately
  convex functions}, Journal of Optimization Theory and Application, 194
  (2022), pp.~220--245.

%\bibitem{Bolte07}
%{\sc J.~Bolte, A.~Daniilidis, A.~Lewis, and M.~Shiota}, {\em Clarke
%  subgradients of stratifiable functions}, SIAM Journal on Optimization, 18
%  (2007), pp.~556--572.

\bibitem{Bolte09}
{\sc J.~Bolte, A.~Daniilidis, and A.~Lewis}, {\em Tame functions are semismooth}, Mathematical Programming, 117 (2009), pp.~5--19.

\bibitem{Bolte17}
{\sc J.~Bolte, T.~P. Nguyen, J.~Peypouquet, and B.~W. Suter}, {\em From error
  bounds to the complexity of first-order descent methods for convex
  functions}, Mathematical Programming, 165 (2017), pp.~471--507.

\bibitem{Bolte16}
{\sc J.~Bolte and E.~Pauwels}, {\em Majorization-minimization procedures and
  convergence of sqp methods for semi-algebraic and tame programs}, Mathematics
  of Operations Research, 41 (2016), pp.~442--465.

\bibitem{Bolte14}
{\sc J.~Bolte, S.~Sabach, and M.~Teboulle}, {\em Proximal alternating
  linearized minimization for nonconvex and nonsmooth problems}, Mathematical
  Programming, 146 (2014), pp.~459--494.

\bibitem{BS00}
{\sc J.~F. Bonnans and A.~S. Sharpiro}, {\em Perturbation Analysis of
  Optimization}, Springer, New York, 2000.

\bibitem{Burke95}
{\sc J.~V. Burke and M.~C. Ferris}, {\em A gauss-newton method for convex
  composite optimization}, Mathematical Programming, 71 (1995), pp.~179--194.

  \bibitem{Clarke83}
{\sc F.~H. Clarke}, {\em Optimization and Nonsmooth Analysis}, New York, 1983.

\bibitem{Charisopoulos21}
{\sc V.~Charisopoulos, Y.~D. Chen, D.~Davis, M.~Diaz, L.~J. Ding, and
  D.~Drusvyatskiy}, {\em Low-rank matrix recovery with composite optimization:
  good conditioning and rapid convergence}, Foundations of Computational
  Mathematics, 21 (2021), pp.~1505--1593.

\bibitem{Chen12}
{\sc C.~H. Chen, B.~S. He, and X.~M. Yuan}, {\em Matrix completion via an
  alternating direction method}, IMA Journal of Numerical Analysis, 32 (2012),
  pp.~227--245.

\bibitem{Dong21}
{\sc H.~B. Dong and M.~Tao}, {\em On the linear convergence to weak/standard
  d-stationary points of dca-based algorithms for structured nonsmooth dc
  programming}, Journal of Optimization Theory and Applications, 189 (2021),
  pp.~190--220.


\bibitem{Dries96}
{\sc Lou Van den, Dries and C.~Miller}, {\em Geometric categories and o-minimal structures}, Duke Mathematical Journal, 84 (1996),
  pp.~497--540.


\bibitem{Fan01}
{\sc J.~Q. Fan and R.~Z. Li}, {\em Variable selection via nonconcave penalized
  likelihood and its oracle properties}, Journal of American Statistics
  Association, 96 (2001), pp.~1348--1360.

\bibitem{Fang18}
{\sc E.~X. Fang, H.~Liu, K.~C. Toh, and W.~X. Zhou}, {\em Max-norm optimization
  for robust matrix recovery}, Mathematical Programming, 167 (2018), pp.~5--35.

%\bibitem{Ferreria21}
%{\sc O.~P. Ferreria, E.~M. Santos, and J.~C.~O. Souza}, {\em A boosted dc
%  algorithm for non-differentiable dc components with non-monotone line
%  search}, 2021, \url{https://arxiv.org/abs/arXiv.2111.01290}.

\bibitem{Fletcher82}
{\sc R.~Fletcher}, {\em A model algorithm for composite nondifferentiable
  optimization problems}, Mathematical Programming Study, 17 (1982),
  pp.~67--76.


  \bibitem{Geferee11}
  {\sc H.~Gfrerer},
  {\em First order and second order characterizations of metric subregularity and calmness of constraint set mappings},
  SIAM Journal on Optimization, 21(2011): 1439-1474.

\bibitem{Gong13}
{\sc P.~H. Gong, C.~S. Zhang, Z.~S. Lu, J.~H. Huang, and J.~P. Ye}, {\em A
  general iterative shrinkage and thresholding algorithm for non-convex
  regularized optimization problems}, In: International Conference on Machine
  Learning,  (2013), pp.~37--45.

\bibitem{HuYang16}
{\sc Y.~H. Hu, C.~Li, and X.~Q. Yang}, {\em On convergence rates of linearized
  proximal algorithms for convex composite optimization with applications},
  SIAM Journal on Optimization, 26 (2016), pp.~1207--1235.

\bibitem{Ioffe09}
{\sc A.~D. Ioffe}, {\em An invitation to tame optimization}, SIAM Journal on
  Optimization, 19 (2009), pp.~1894--1917.

\bibitem{Ioffe08}
{\sc A.~D. Ioffe and J.~V. Outrata}, {\em On metric and calmness qualification
  conditions in subdifferential calculus}, Set-Valued Analysis, 16 (2008),
  pp.~199--227.

\bibitem{LeTai18Jota}
{\sc H.~A. Le~Thi, V.~N. Huynh, and T.~Pham~Dinh}, {\em Convergence analysis of
  difference-of-convex algorithm with subanalytic data}, Journal of
  Optimization Theory and Applications, 179 (2018), pp.~103--126.


\bibitem{LeThi23}
{\sc H. A. Le Thi, V. N. Huynh and T. Pham Dinh},
{\em Minimizing compositions of differences-of-convex functions with smooth mappings}, Mathematics of Operations Research, (2023), pp.~1--29. DOI: 10.1287/moor.2021.0258.

\bibitem{LeTai05}
{\sc H.~A. Le~Thi and T.~Pham~Dinh}, {\em The dc (difference of convex
  functions) programming and dca revisited with dc models of real world
  nonconvex optimization problems}, Annals of Operations Research, 133 (2005),
  pp.~23--46.

\bibitem{LeTai18}
{\sc H.~A. Le~Thi and T.~Pham~Dinh}, {\em Dc programming and dca: Thirty years
  of developments}, Mathematical Programming, 169 (2018), pp.~5--68.

\bibitem{Lewis16}
{\sc A.~S. Lewis and S.~J. Wright}, {\em A proximal method for composite
  minimization}, Mathematical Programming, 158 (2016), pp.~501--546.

\bibitem{Li07}
{\sc C.~Li and K.~F. Ng}, {\em Majorizing functions and convergence of the
  gauss-newton method for convex composite optimization}, SIAM Journal on
  Optimization, 18 (2007), pp.~613--642.

\bibitem{Li02}
{\sc C.~Li and X.~H. Wang}, {\em On convergence of the gauss-newton method for
  convex composite optimization}, Mathematical Programming, 91 (2002),
  pp.~349--356.

\bibitem{LiMor12}
{\sc G.~Y. Li and B.~S. Mordukhovich}, {\em H\"{o}lder metric subregularity with applications to proximal point method}, SIAM Journal on Optimization, 22 (2012),
  pp.~1655--1684.

\bibitem{LiPong18}
{\sc G.~Y. Li and T.~K. Pong}, {\em Calculus of the exponent of
  kurdyka-{\l}ojasiewicz inequality and its applications to linear convergence
  of first-order methods}, Foundations of Computational Mathematics, 18 (2018),
  pp.~1199--1232.

\bibitem{LiZhu20}
{\sc X.~Li, Z.~H. Zhu, A.~M.~C. So, and R.~Vidal}, {\em Nonconvex robust
  low-rank matrix recovery}, SIAM Journal on Optimization, 30 (2020),
  pp.~660--686.

\bibitem{LiuPanWY22}
{\sc R.~Y. Liu, S.~H. Pan, Y.~Q. Wu, and X.~Q. Yang}, {\em An inexact
  regularized proximal newton method for nonconvex and nonsmooth optimization},
  2022, \url{https://arxiv.org/abs/arXiv:2209.09119v4}.

\bibitem{LiuPong19}
{\sc T.~X. Liu, T.~K. Pong, and A.~Takeda}, {\em A refined convergence analysis
  of pdcae with applications to simultaneous sparse recovery and outlier
  detection}, Computational Optimization and Applications, 73 (2019),
  pp.~69--100.

\bibitem{LiuPan22}
{\sc Y.~L. Liu and S.~H. Pan}, {\em Twice epi-differentiability of a class of
  non-amenable composite functions}, 2022,
  \url{https://arxiv.org/abs/arXiv:2212.00303}.

\bibitem{Lu12}
{\sc Z.~S. Lu}, {\em Sequential convex programming methods for a class of
  structured nonlinear programming}, 2012,
  \url{https://arxiv.org/abs/arxiv:1210.3039}.

\bibitem{Lu19}
{\sc Z.~S. Lu, Z.~R. Zhou, and Z.~Sun}, {\em Enhanced proximal dc algorithms
  with extrapolation for a class of structured nonsmooth dc minimization},
  Mathematical Programming, 176 (2019), pp.~369--401.

%\bibitem{Mohammadi20}
%{\sc A.~Mohammadi and M.~E. Sarabi}, {\em Twice epi-differentiability of
%  extended-real-valued functions with applications in composite optimization},
%  SIAM Journal on Optimization, 30 (2020), pp.~2379--2409.

%\bibitem{Mordu94}
%{\sc B.~S. Morduhovich}, {\em Generalized differential calculus for nonsmooth
 % and set-valued mappings}, Journal of Mathematical Analysis and Applications,
  %183 (1994), pp.~250--288.

%\bibitem{Mordu15}
%{\sc B.~S. Morduhovich and O.~Y. Wei}, {\em Higher-order metric subregularity
 % and its applications}, Journal of Global Optimization, 63 (2015),
  %pp.~777--795.

%\bibitem{Meng05}
%{\sc F.~W. Meng and D.~F. Sun}, {\em Semismoothness of solutions to generalized equations %and the Moreau-Yosida regularization}, Mathematical Programming, 104 (2015),
 % pp.~561--581.


  \bibitem{Mordu23}
{\sc B.~S. Morduhovich, X.~M. Yuan, S.~Z. Zeng and J.~Zhang}, {\em A globally convergent proximal {N}ewton-type method in nonsmooth convex optimization}, Mathematical Programming, 198 (2023),
  pp.~899--936.

\bibitem{Nguyen17}
{\sc T.~A. Nguyen and M.~N. Nguyen}, {\em Convergence analysis of a proximal
  point algorithm for minimizing differences of functions}, Optimization, 66
  (2017), pp.~129--147.

\bibitem{Oliveira19}
{\sc D.~Oliveira, W and M.~P. Tcheou}, {\em An inertial algorithm for dc
  programming}, Set-Valued and Variational Analysis, 27 (2019), pp.~895--919.

 \bibitem{Ortega70}
{\sc J.~M. Ortega and W.~C Rheinboldt}, {\em Iterative Solution of Nonlinear Equations in Several Variables}, Academic Press, 1970.

\bibitem{Pang17}
{\sc J.~S. Pang, M.~Razaviyayn, and A.~Alvarado}, {\em Computing b-stationary
  points of nonsmooth dc programs}, Mathematics of Operations Research, 42
  (2017), pp.~95--118.

\bibitem{Pauwels16}
{\sc E.~Pauwels}, {\em The value function approach to convergence analysis in
  composite optimization}, Operations Research Letters, 44 (2016),
  pp.~790--795.

\bibitem{Pham97}
{\sc T.~Pham~Dinh and H.~A. Le~Thi}, {\em Convex analysis approach to dc
  programming: Theory, algorithms and applications}, Acta Mathematica
  Vietnamica, 22 (1997), pp.~289--355.

%\bibitem{Pham86}
%{\sc T.~Pham~Dinh and E.~B. Souad}, {\em Algorithms for solving a class of
%  nonconvex optimization problems: methods of subgradient}, Mathematics for
%  optimization. Fermat days. North Holland: Elsevier, 85 (1986), pp.~249--270.


\bibitem{QiSun93}
{\sc L.~Q. Qi and J.~Sun}, {\em A nonsmooth version of Newton's method}, Mathematical Programming Study, 58 (1993), pp.~353--367.


\bibitem{Robinson81}
{\sc S.~M. Robinson}, {\em Some continuity properties of polyhedral
  multifunctions}, Mathematical Programming Study, 14 (1981), pp.~206--214.

\bibitem{Roc70}
{\sc R.~T. Rockafellar}, {\em Convex Analysis}, Princeton University Press,
  1970.

  \bibitem{Roc76}
{\sc R.~T. Rockafellar}, {\em Augmented Lagrangians and applications of the proximal point algorithm in convex programming}, Mathematics of Operations Research,
  1(1976), PP.~97--116.

   \bibitem{Roc21}
{\sc R.~T. Rockafellar}, {\em Advances in convergence and scope of the proximal point algorithm}, Journal of Nonlinear and Convex Analysis,
  22(2021), PP.~2347--2374.

\bibitem{RW98}
{\sc R.~T. Rockafellar and R.~J.-B. Wets}, {\em Variational analysis},
  Springer, 1998.

\bibitem{Souza16}
{\sc J.~C.~O. Souza, P.~R. Oliveira, and A.~Soubeyran}, {\em Global convergence
  of a proximal linearized algorithm for difference of convex functions},
  Optimization Letters, 10 (2016), pp.~1529--1539.

%  \bibitem{Strekalovsky18}
%{\sc A.~S. Strekalovsky and I.~M Minarchenko}, {\em A local search method for optimization problem with d.c. inequality constraints},
%   Applied Mathematical Modelling, 59 (2018), pp.~229--244.

\bibitem{Sun03}
{\sc W.~Y. Sun, R.~J.~B. Sampaio, and M.~A.~B. Candido}, {\em Proximal point
  algorithm for minimization of dc functions}, Journal of Computational
  Mathematics, 21 (2003), pp.~451--462.

\bibitem{Toh10}
{\sc K.~C. Toh and S.~Yun}, {\em An accelerated proximal gradient algorithm for
  nuclear norm regularized linear least squares problems}, Pacific Journal of
  Optimization, 6 (2010), pp.~615--640.

\bibitem{Wen18}
{\sc B.~Wen, X.~J. Chen, and T.~K. Pong}, {\em Aproximal difference-of-convex
  algorithmwith extrapolation}, Computational Optimization and Applications, 69
  (2018), pp.~297--324.

\bibitem{WuPanBi21}
{\sc Y.~Q. Wu, S.~H. Pan, and S.~J. Bi}, {\em Kurdyka-{\l}ojasiewicz property
  of zero-norm composite functions}, Journal of Optimization Theory and
  Applications, 188 (2021), pp.~94--112.

\bibitem{YuLiPong21}
{\sc P.~R. Yu, G.~Y. Li, and T.~K. Pong}, {\em Kurdyka-{\l}ojasiewicz exponent
  via inf-projection}, Foundations of Computational Mathematics, 22 (2021),
  pp.~1171--1271.

\bibitem{YuLu21}
{\sc P.~R. Yu, T.~K. Pong, and Z.~S. LU}, {\em Convergence rate analysis of a
  sequential convex programming method with line search for a class of
  constrained difference-of-convex optimization problems}, SIAM Journal on
  Optimization, 31 (2021), pp.~2024--2054.

\bibitem{Zhang10}
{\sc C.~H. Zhang}, {\em Nearly unbiased variable selection under minimax
  concave penalty}, Annals of Statistics, 38 (2010), pp.~894--942.

\bibitem{ZhangPan22}
{\sc D.~D. Zhang, S.~H. Pan, S.~J. Bi, and D.~F. Sun}, {\em Zero-norm
  regularized problems: equivalent surrogates, proximal mm method and
  statistical error bound}, accepted by Computational Optimization and
  Applications.

  \bibitem{ZhaoST10}
{\sc X.~Y. Zhao, D.~F. Sun and K.~C Toh}, {\em A Newton-CG augmented Lagrangian method for semidefinite programming}, SIAM Journal on Optimization, 20(2010), pp.~1737--1765.

\end{thebibliography}

\end{document}


