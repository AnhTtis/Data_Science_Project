\documentclass[11pt,a4paper]{article}
\usepackage{}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{makecell,booktabs}
%\usepackage{graphicx}
\usepackage{subfigure}
 \usepackage{epsfig}
 \usepackage{epstopdf}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsbsy,amsmath,latexsym,amsfonts, epsfig, color, authblk, amssymb, graphics, bm, caption}
\usepackage{epsf,slidesec,epic,eepic}
\usepackage{fancybox}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{nccmath}
\usepackage{cases}
\usepackage{cite}
\usepackage[colorlinks, citecolor=blue]{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{malgorithm}{Algorithm}[section]
\newtheorem{aalgorithm}{Algorithm}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{condition}{Condition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{append}{Appendix}[section]
\newtheorem{alemma}{Lemma}
\newtheorem{atheorem}{Theorem}
\newtheorem{aproposition}{Proposition}
\newtheorem{aremark}{Remark}
\newtheorem{acorollary}{Corollary}
\newenvironment{aproof}{{\noindent}}{\hfill$\Box$\medskip}
\newenvironment{proof}{{\noindent \bf Proof:}}{\hfill$\Box$\medskip}

\definecolor{lred}{rgb}{1,0.8,0.8}
\definecolor{lblue}{rgb}{0.8,0.8,1}
\definecolor{dred}{rgb}{0.6,0,0}
\definecolor{dblue}{rgb}{0,0,0.5}
\definecolor{dgreen}{rgb}{0,0.5,0.5}

\title{An inexact linearized proximal algorithm for a class of DC composite optimization problems and applications}

\author{Ting Tao\footnote{(\href{mailto:taoting@fosu.edu.cn}{taoting@fosu.edu.cn}) School of Mathematics, Foshan University, Foshan },\ \	
 Ruyu Liu\footnote{(\href{mailto:maruyuliu@mail.scut.edu.cn}{maruyuliu@mail.scut.edu.cn}) School of Mathematics, South China University of Technology, Guangzhou},\ \
 Lianghai Xiao\footnote{(\href{mailto:lxiao@scut.edu.cn}{lxiao@scut.edu.cn})School of Mathematics, South China University of Technology, Guangzhou}\ \ {\rm and}\ \	 	
 Shaohua Pan\footnote{(\href{mailto:shhpan@scut.edu.cn}{shhpan@scut.edu.cn}) School of Mathematics, South China University of Technology}}
%--------------------------------------------------------------------------------------------------
 \begin{document}

 \maketitle

 \begin{abstract}
  This paper is concerned with a class of DC composite optimization problems which, as an extension of the convex composite optimization problem and the DC program with nonsmooth components, often arises from robust factorization models of low-rank matrix recovery. For this class of nonconvex and nonsmooth problems, we propose an inexact linearized proximal algorithm (iLPA) which in each step computes an inexact minimizer of a strongly convex majorization constructed by the partial linearization of their objective functions. The generated iterate sequence is shown to be convergent under the Kurdyka-{\L}ojasiewicz (KL) property of a potential function, and the convergence admits a local R-linear rate if the potential function has the KL property of exponent $1/2$ at the limit point. For the latter assumption, we provide a verifiable condition by leveraging the composite structure, and  clarify its relation with the regularity used for the convex composite optimization. Finally, the proposed iLPA is applied to a robust factorization model for matrix completions with outliers, DC programs with nonsmooth components, and $\ell_1$-norm exact penalty of DC constrained programs, and numerical comparison with the existing algorithms confirms the superiority of our iLPA in computing time and quality of solutions.
 \end{abstract}
%--------------------------------------------------------------------------------
 \section{Introduction}\label{sec1}

 Let $\mathbb{X},\mathbb{Y}$ and $\mathbb{Z}$ represent the finite dimensional real vector spaces endowed with the inner product $\langle\cdot,\cdot\rangle$ and its induced norm $\|\cdot\|$, and let $\overline{\mathbb{R}}\!:=(-\infty,\infty]$ denote the extended real number set.
 We are interested in the following DC composite optimization problem
 \begin{equation}\label{prob}
  \min_{x\in\mathbb{X}}\,\Phi(x)\!:=\vartheta_1(F(x))-\vartheta_2(G(x))+h(x),
 \end{equation}
 where the functions $h\!:\mathbb{X}\to\overline{\mathbb{R}},\vartheta_1\!:\mathbb{Y}\to\mathbb{R},\vartheta_2\!:\mathbb{Z}\to\mathbb{R}$ and the mappings $F\!:\mathbb{X}\to\!\mathbb{Y},G\!:\mathbb{X}\to\mathbb{Z}$ satisfy the basic assumption stated in Assumption \ref{ass0}:
 \begin{assumption}\label{ass0}
 \begin{description}
 \item[(i)] $h$ is closed and proper convex, and $\vartheta_1$ and $\vartheta_2$ are convex on ${\rm dom}h$;

 \item[(ii)] $F$ and $G$ are continuously differentiable on an open set containing ${\rm dom}h$, and their differential mappings $F'$ and $G'$ are strictly continuous on this open set;

 \item[(iii)] the objective function $\Phi$ is lower bounded on its domain ${\rm dom}\Phi={\rm dom}h$.
 \end{description}
 \end{assumption}
 Unless otherwise stated, we always write $\Theta:=\Theta_1-\Theta_2$ with $\Theta_1\!:=\vartheta_1\circ F$ and $\Theta_2\!:=\vartheta_2\circ G$.

 Problem \eqref{prob} is an extension of the convex composite optimization problem \cite{Fletcher82,Burke95} and the DC program with nonsmooth components \cite{Pham86,LeTai18}. The former provides a unified framework for studying the theory of many important classes of optimization problems such as amenable optimization problems, convex conic optimization and convex inclusions (see, e.g., \cite{BS00,RW98,Mohammadi20}), while the latter has extensive applications in many fields such as machine learning, statistics, financial optimization, supply chain management, and telecommunication (see \cite{Artacho20,LeTai18,LiuPong19} and the literature therein). Not only that, problem \eqref{prob} also arises from the following robust factorization model of low-rank matrix recovery:
 \begin{align}\label{SCAD-loss}
  &\min_{U\in\mathbb{R}^{n_1\times r},V\in\mathbb{R}^{n_2\times r}}\Phi(U,V):=
  \vartheta(\mathcal{A}(UV^{\top})-b)+\lambda(\|U\|_{2,1}+\|V\|_{2,1})
 \end{align}
 where $\mathcal{A}\!:\mathbb{R}^{n_1\times n_2}\to\mathbb{R}^{m}$ is a sampling operator, $b\in\mathbb{R}^{m}$ is an observation vector, $\lambda>0$ is a regularization parameter,  $\|\cdot\|_{2,1}$ denotes the column $\ell_{2,1}$-norm of a matrix, and $\vartheta\!:\mathbb{R}^m\to\mathbb{R}$ is a DC function to promote sparsity. Such $\vartheta$ includes the SCAD, the MCP and the capped $\ell_1$-norm (see \cite{Fan01,Zhang10,Gong13}), which are shown to be the equivalent DC surrogates of the zero-norm in \cite{ZhangPan22}. Due to the sparsity induced by $\vartheta$, the associated loss term $\vartheta(\mathcal{A}(\cdot)-b)$ copes well with outliers involved in the observation vector.

%--------------------------------------------------------------------------------
 \subsection{Related works}\label{sec1.1}

  When $\vartheta_2\equiv 0$ and $h\equiv 0$, model \eqref{prob} is precisely the convex composite optimization problem \cite{Fletcher82,Burke95}. For this class of nonconvex and nonsmooth problems, the Gauss-Newton method is a classical one for which the global quadratic convergence of the iterate sequence was obtained in \cite{Burke95} under a regularity of its accumulation point on the inclusion $F(x)\in C\!:=\mathop{\arg\min}_{y\in\mathbb{Y}}\vartheta_1(y)$ and $C$ is a set of weak sharp minima for the outer $\vartheta_1$, and a similar convergence result was achieved in \cite{Li02,Li07} under weaker conditions. Another popular method is the linearized proximal algorithm (LPA) proposed in \cite{Lewis16}, which allows the outer $\vartheta_1$ to be extended real-valued and prox-regular. In each iterate, it first finds a trial step by seeking a local optimal solution of a proximal linearized subproblem (which becomes strongly convex if $\vartheta_1$ is convex), and then derives a new iterate from the trial step by an efficient projection and/or other enhancements. Criticality of accumulation points under prox-regularity and identification under partial smoothness was studied in \cite{Lewis16}. Later, Hu et al. \cite{HuYang16} proposed a globalized LPA by using monotone line-search, and established the global superlinear convergence of order $\frac{2}{p}$ with $p\in[1,2)$ for the iterate sequence by assuming that its cluster point $\overline{x}$ is a regular point of the inclusion $F(x)\in C$ and $C$ is the set of local weak sharp minima of order $p$ for $\vartheta_1$ at $F(\overline{x})$; and for the LPA with a backtracking search for the proximal parameters, Pauwels \cite{Pauwels16} verified the convergence of the iterate sequence under the twice differentiability of $F$ and the definability of $F$ and $\vartheta_1$ in the same o-minimal structure of the real field by using the value function approach analysis technique in \cite{Bolte16}. In addition, for the standard NLP problem, which takes the form of \eqref{prob} with an extended real-valued $\vartheta_1$ and $\vartheta_2\equiv 0$, Bolte and Pauwels \cite{Bolte16} showed that the iterate sequence generated by the moving balls method \cite{Auslender10}, the penalized sequential quadratic programming (SQP) method, and the extended SQP method, respectively, converges to a critical point if all $F_i$ are semialgebraic and the (generalized)  Mangasarian-Fromovitz constraint qualification (MFCQ) holds.

  We observe that almost all of the above algorithms require solving a convex or strongly convex program exactly in each step, which is impractical in computation. Although a practical inexact algorithm was also proposed in \cite{Li02} (respectively, \cite{HuYang16}), its convergence analysis restricts a starting point in a neighborhood of a regular point (respectively, a quasi-regular point) of the inclusion $F(x)\!\in C$, and moreover, such regular or quasi-regular point does not necessarily exist. Thus, even for the convex composite optimization problem, it is necessary to develop a practical and globally convergent algorithm.


  When $F$ and $G$ are the identity mapping $\mathcal{I}$, problem \eqref{prob} becomes a DC program with nonsmoooth components. For this class of problems, a well-known method is the DC algorithm (DCA) of \cite{Pham86,Pham97,LeTai18} which in each step linearizes the second DC component to yield a convex subproblem and uses its exact solution to define a new iterate; and another popular one is the proximal linearized method (PLM) of \cite{Sun03,Souza16,Pang17,Nguyen17}, which can be seen as a regularized variant of DCA because the convex subproblems are augmented with a proximal term to prevent tailing-off effect that makes calculations unstable as the iteration process progresses. For the DCA, Le Thi et al. \cite{LeTai18Jota} proved the global convergence of the iterate sequence under the assumption that the objective function is subanalytic and continuous relative to its domain, and either of DC components is L-smooth around every critical point; and for the PLM, Nguyen et al. \cite{Nguyen17} achieved the same convergence result under the KL property of the objective function and the L-smoothness of the second DC component, or under the strong KL property of the objective function and the L-smoothness of the first DC component. For the PLM with extrapolation, Oliveira and Tcheou \cite{Oliveira19} proved that any cluster point of the generated sequence is a critical point, and for problem \eqref{prob} with  $\vartheta_1(t,y):=t+\theta(y), F\equiv(f;\mathcal{I})$ and $G\equiv\mathcal{I}$, where $f\!:\mathbb{X}\to\mathbb{R}$ is an L-smooth function, Liu et al. \cite{LiuPong19} showed that the iterate sequence is convergent under the KL property of a certain potential function, which removes the differentiability restriction on the second DC component in the convergence analysis of \cite{Nguyen17,Wen18}. To accelerate the DCA, Artacho et al. \cite{Artacho20} proposed a boosted DC algorithm (BDCA) with monotone line search, and Ferreira et al. \cite{Ferreria21} developed the BDCA with non-monotone line search so that the modified BDCA is applicable to DC programs with nonsmooth DC components. For the BDCAs in \cite{Artacho20,Ferreria21}, the convergence of their iterate sequences was achieved by assuming that the objective function has the strong KL property at critical points and the gradient of the second DC component is strictly continuous around the critical points. In addition, for problem \eqref{prob} with $\vartheta_1(t,y)=t+\delta_{\mathbb{R}_{-}^m}(y)$, each $F_i\ (i=0,1,\ldots,m)$ being an L-smooth function and $G\equiv\mathcal{I}$, Yu et al. \cite{YuLu21} studied the monotone line search variant of the sequential convex programming (SCP) method in \cite{Lu12}, by combining the idea of the moving balls method and that of the PLM.   They proved that the iterate sequence converges to a stationary point of \eqref{prob} under the MFCQ and the KL propoerty of a potential function if $F_i\ (i=1,\ldots,m)$ are also twice continuously differentiable and $\vartheta_2$ is Lipschitz continuously differentiable on an open set containing the stationary point set.

  It is worth to point out that most of the above DC algorithms require in each step  solving a convex or strongly convex program exactly, which is impractical in computation unless the first DC component is very simple. Though an inexact PLM was proposed in \cite{Souza16} and \cite{Oliveira19}, respectively, the convergence of the iterate sequences was not established. In addition, for DC programs with the second DC component having a maximum structure, some enhanced proximal DC algorithms were proposed in  \cite{Pang17,Lu19,Dong21} so as to seek better d-stationary points, but they are inapplicable to large-scale DC programs such as \eqref{SCAD-loss} because in each step they need to solve at least one strongly convex program exactly. An inexact enhanced proximal DC algorithm was also proposed in \cite{Lu19}, but the convergence of the iterate sequence was not obtained. Thus, even for DC programs with nonsmooth components, it is necessary to develop a globally convergent and practical PLM.

  This work aims to develop a globally convergent and practical algorithm for solving the DC composite problem \eqref{prob}. Since $F'$ and $G'$ are only assumed to be strictly continuous on ${\rm dom}h$ by Assumption \ref{ass0} (ii), it cannot be reformulated as a DC program, so the above DCAs, PLMs and SCP method cannot be directly applied to \eqref{prob}. The term  $\vartheta_2(G(\cdot))$, catering to the outerliers involved in the observation vectors for low-rank matrix recovery, hinders the direct applications of the above inexact LPAs and the moving balls method.
%-----------------------------------------------------------------------------------------------
  \subsection{Main contributions}\label{sec1.2}

  The main contribution of this work is to propose an inexact LPA with global convergence certificate for problem \eqref{prob}, which in each step computes an inexact minimizer of a strongly convex majorization constructed by the linearization of the inner $F$ and $G$ at the current iterate $x^k$ and the concave function $-\vartheta_2$ at $G(x^k)$. We justify that
  the generated iterate sequence converges to a stationary point in the sense of Definition  \ref{spoint-def} under Assumptions \ref{ass0}-\ref{ass1} and the KL property of a potential function $\Xi$ defined in \eqref{Xi-fun}, and if in addition the function $\Xi$ has the KL property of exponent $p\in[1/2,1)$ at the stationary point, the convergence has a local R-linear rate for $p=1/2$, and a sublinear rate for $p\in(1/2,1)$. The stationary point in Definition \ref{spoint-def} is stronger than those obtained from the above DCAs, PLMs and SCP method for $F\equiv\mathcal{I}$ and $G\equiv\mathcal{I}$.
  When $\vartheta_2\equiv 0$ and $h\equiv 0$, our iLPA is an inexact version of the composite Gauss-Newton method in \cite{Pauwels16}, so the obtained convergence results recover that of  \cite{Pauwels16} via a different analysis technique. 

  The KL property of $\Xi$ with exponent $p\in[1/2,1)$ is shown to hold for polyhedral $F,G$ and some special $\vartheta_1,\vartheta_2$ and $h$; see Proposition \ref{prop-KL0}. For general $F,G$ and $\vartheta_1,\vartheta_2,h$, we provide a verifiable condition for the KL property of the function $\Xi$ with exponent $p\in[1/2,1)$ at any critical point $(\overline{x},\overline{x},\overline{z},\overline{\mathcal{Q}})$ by using the KL property of exponent $p\in[1/2,1)$ for an almost separable nonsmooth function at $(F(\overline{x}),\overline{x},G(\overline{x}),\overline{z})$ and condition \eqref{key-cond} on the null space ${\rm Ker}([\nabla F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})])$; see Proposition \ref{prop-KL}. When $\vartheta_2\equiv 0$ and $h\equiv 0$, the former is equivalent to requiring that $C$ is a set of local weak sharp minima of order $\frac{1}{1-p}$ for $\vartheta_1$ at $F(\overline{x})$ by \cite[Corollary 2.1]{BaiLi22}, while condition \eqref{key-cond} is shown to be weaker than the regularity used in \cite{HuYang16,Burke95} and there is no direct implication relation between condition \eqref{key-cond} and quasi-regularity in \cite{HuYang16}; see the discussions in Remark \ref{remark-relation}.

  We apply the proposed iLPA armed with dPPASN (a proximal point algorithm to solve their dual problems with each proximal subproblem solved by a semismooth Newton method) to the robust factorization model \eqref{SCAD-loss} of low-rank matrix recovery, DC programs with nonsmooth components, and $\ell_1$-norm exact penalty of DC constrained programs. Numerical comparisons
  with the Polyak subgradient method \cite{Charisopoulos21,LiZhu20} for matrix completions with outliers and non-uniform sampling on synthetic and real data indicate that our iLPA is superior to the subgradient method in terms of the computing time and the relative error for synthetic examples or the normalized mean absolute error (NMAE) for Jester and Movie datasets.
  In addition, numerical comparisons with the non-monotone boosted DC algorithm (nmBDCA) proposed in \cite{Ferreria21} for the DC test examples from \cite{Artacho20} and the $\ell_1$-norm exact penalty of some DC constrained programs indicate that the solutions returned by our iLPA has a better quality than those yielded by nmBDCA.

 \medskip
 \noindent
 {\bf Notation.} Let $\mathbb{S}_{+}$ be the set of all self-adjoint positive semidefinite (PSD) linear mappings from $\mathbb{X}$ to $\mathbb{X}$, and let $\mathcal{I}$ denote an identity mapping. For an element $\mathcal{Q}\in\mathbb{S}_{+}$, write $\|z\|_{\mathcal{Q}}:=\sqrt{\langle z,\mathcal{Q}z\rangle}$ for $z\in\mathbb{X}$. For an integer $k\ge 0$, write $[k]=\{0,1,\ldots,k\}$.
 If $f\!:\mathbb{X}\to\mathbb{R}$ is strictly continuous at $x$, ${\rm lip}f(x)$ denotes its Lipschitz modulus at $x$. For a set $\Omega\subset\mathbb{Y}$, ${\rm cl}(\Omega)$ is the closure of $\Omega$, ${\rm cone}(\Omega):=\{tx\,|\,x\in \Omega,t\ge 0\}$ means the cone generated by $\Omega$, and $\delta_{\Omega}$ denotes the indicator of $\Omega$. For an $x\in\mathbb{X}$ and $\varepsilon>0$, $\mathbb{B}(x,\varepsilon)$ denotes the closed ball centered at $x$ with radius $\varepsilon>0$. For a linear mapping $\mathcal{A}\!:\mathbb{X}\to\mathbb{Y}$, let
 $\mathcal{A}^*\!:\mathbb{Y}\to\mathbb{X}$ denote its adjoint. For a differentiable  $g\!:\mathbb{X}\to\mathbb{Y}$, $\nabla g(x)$ denotes the adjoint of $g'(x)$, the differential mapping of $g$ at $x$, and if $g$ is twice differentiable at $x$, $D^2g(x)$ represents the twice differential mapping of $g$ at $x$, and $D^2g(x)(u,\cdot)$ for $u\in\mathbb{X}$ is a linear mapping from $\mathbb{X}$ to $\mathbb{Y}$.

%------------------------------------------------------------------------------------------
 \section{Preliminaries}\label{sec2}

 First, we recall from \cite{RW98} the basic subdifferential of an extended real-valued function.
 %------------------------------------------------------------------------------------------
 \begin{definition}\label{Gsubdiff-def}
  (see \cite[Definition 8.3]{RW98}) Consider a proper function $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ and a point $x\in{\rm dom}f$. The regular subdifferential of $f$ at $x$ is defined as
  \[
 	\widehat{\partial}\!f(x):=\bigg\{v\in\mathbb{X}\ \big|\
 	\liminf_{x\ne x'\to x}\frac{f(x')-f(x)-\langle v,x'-x\rangle}{\|x'-x\|}\ge 0\bigg\},
  \]
  and the basic (limiting or Morduhovich) subdifferential of $f$ at $x$ is defined as
  \[
 	\partial\!f(x):=\Big\{v\in\mathbb{X}\ |\  \exists\,x^k\to x\ {\rm with}\ f(x^k)\to f(x)\ {\rm and}\ v^k\in\widehat{\partial}f(x^k)\ {\rm such\ that}\ v^k\to v\Big\}.
  \]
 \end{definition}

 By Definition \ref{Gsubdiff-def}, for any $x\in{\rm dom}\,f$, the set $\widehat{\partial}\!f(x)$ is closed and convex, $\partial\!f(x)$ is closed but generally nonconvex, and $\widehat{\partial}\!f(x)\subset\partial\!f(x)$. The inclusion may be strict
 if $f$ is nonconvex. When $f=\delta_{\Omega}$ for a closed set $\Omega\subset\mathbb{X}$, at any $x\in\Omega$, $\partial \!f(x)=\mathcal{N}_{\Omega}(x)$, the normal cone to $\Omega$ at $x$. A vector $x$ is called a (limiting) critical point of $f$ if $0\in\partial\!f(x)$, and the set of all critical points of $f$ is denoted by ${\rm crit}f$.
%-----------------------------------------------------------------------------------------------
 \subsection{Subdifferentials of composite functions}\label{sec2.1}
 %-----------------------------------------------------------------------------------

 The following lemma provides a chain rule for the subdifferential of composite functions, which improves the conclusions of \cite[Theorem 10.6 \& 10.49]{RW98}.
%---------------------------------------------------------------------------------------------
 \begin{lemma}\label{chain-rule}
  Let $\Psi(x,z):=f(g(x,z))$ where $f\!:\mathbb{Y}\to\overline{\mathbb{R}}$ is a proper lsc function and $g\!:\mathbb{X}\times\mathbb{X}\to\mathbb{Y}$ is a mapping. Consider any $(\overline{x},\overline{z})\in{\rm dom}f$. Suppose that the multifunction $\mathcal{F}(x,z)\!:=g(x,z)-{\rm dom}f$ is metrically subregular at $((\overline{x},\overline{z}),0)$ and that $f$ is strictly continuous at $g(\overline{x},\overline{z})$ relative to ${\rm dom}f$.  Then, the following assertions hold:
  \begin{itemize}
  \item[(i)] $\partial\Psi(\overline{x},\overline{z})\subset D^*g(\overline{x},\overline{z})\partial\!f(g(\overline{x},\overline{z}))$ where $D^*g(\overline{x},\overline{z})$ is the coderivative of $g$ at $(\overline{x},\overline{z})$.

  \item[(ii)] If in addition $g$ is strictly differentiable at $(\overline{x},\overline{z})$ and $\partial\!f(g(\overline{x},\overline{z}))=\widehat{\partial}\!f(g(\overline{x},\overline{z}))$, it holds that
  \(
  \partial\Psi(\overline{x},\overline{z})=\widehat{\partial}\Psi(\overline{x},\overline{z})
  =\nabla\!g(\overline{x},\overline{z})\partial\!f(g(\overline{x},\overline{z})).
  \)
  \end{itemize}
 \end{lemma}
 \begin{proof}
  {\bf(i)} Let $\widetilde{g}(x,z,\alpha)\!:=(g(x,z);\alpha)$ for $(x,z,\alpha)\in\mathbb{X}\times\mathbb{X}\times\mathbb{R}$. Then,
  ${\rm epi}\Psi=\widetilde{g}^{-1}({\rm epi}\,f)$. Since $\mathcal{F}$ is metrically subregular at $((\overline{x},\overline{z}),0)$ and $f$ is strictly continuous at $\overline{\omega}:=g(\overline{x},\overline{z})$ relative to ${\rm dom}\,f$, from \cite[Proposition 2.1]{LiuPan22}  $\widetilde{F}(x,z,\alpha)\!:=\widetilde{g}(x,z,\alpha)-{\rm epi}\,f$ is metrically subregular at $((\overline{x},\overline{z},\overline{\alpha}),0)$ with $\overline{\alpha}\!:=f(\overline{\omega})$. By \cite[Page 211]{Ioffe08} and \cite[Proposition 2.4]{Mordu94},
  \[
   \mathcal{N}_{{\rm epi}\Psi}(\overline{x},\overline{z},\overline{\alpha})
   \subset D^*\widetilde{g}(\overline{x},\overline{z},\overline{\omega})\mathcal{N}_{{\rm epi}f}(\overline{\omega},\overline{\alpha})
   =\big\{(D^*g(\overline{x},\overline{z})\xi;\tau)\,|\,(\xi,\tau)\in\mathcal{N}_{{\rm epi}f}(\overline{\omega},\overline{\alpha})\big\}.
  \]	
  Together with \cite[Theorem 8.9]{RW98}, we obtain the desired inclusion.

  \noindent
  {\bf(ii)} Since $g$ is strictly differentiable at $(\overline{x},\overline{z})$, we have $D^*g(\overline{x},\overline{z})=\nabla g(\overline{x},\overline{z})$. In addition, by Definition \ref{Gsubdiff-def}, it is not hard to check that
  $\widehat{\partial}\Psi(\overline{x},\overline{z})\supseteq\nabla g(\overline{x},\overline{z})\widehat{\partial}\!f(g(\overline{x},\overline{z}))$.
  Together with part (i), we obtain the desired equalities.
 \end{proof}

 Recall that ${\rm dom}\vartheta_1=\mathbb{Y}$ and ${\rm dom}\vartheta_2=\mathbb{Z}$. By invoking Lemma \ref{chain-rule} with $f=\vartheta_1$ and $g=F$, $\Theta_1$ is regular with $\partial\Theta_1(x)=\nabla\!F(x)\partial\vartheta_1(F(x))$ at any $x\in\mathbb{X}$, which by \cite[Exercise 10.10]{RW98} implies that $\partial(\Theta_1+h)(x)=\partial\Theta_1(x)+\partial h(x)$ for all $x\in{\rm dom}\,h$,  while by invoking Lemma \ref{chain-rule} with $f=\vartheta_2$ and $g=G$,  $\partial(-\Theta_2)(x)\subset\nabla\! G(x)\partial(-\vartheta_2)(G(x))$ for $x\in\mathbb{X}$. Thus,
 \[
   \partial\Phi(x)\subset \nabla\!F(x)\partial\vartheta_1(F(x))+\nabla\! G(x)\partial(-\vartheta_2)(G(x))+\partial h(x)\quad\ \forall x\in{\rm dom}\Phi.
 \]
 This motivates us to introduce the following notion of stationary points for problem \eqref{prob}.
 %--------------------------------------------------------------------------------------------------
 \begin{definition}\label{spoint-def}
  A vector $x\in\mathbb{X}$ is called a stationary point of problem \eqref{prob} if
  \[
   0\in\nabla\! F(x)\partial\vartheta_1(F(x))+\nabla\!G(x)\partial(-\vartheta_2)(G(x))+\partial h(x).
  \]
 \end{definition}
 \begin{remark}\label{remark-spoint}
  From \cite[Corollary 9.21]{RW98} and the strict continuity of $\vartheta_2$, it follows that
  \[
   -\partial(-\vartheta_2)(z)=\partial_{B}\vartheta_2(z):=\big\{\xi\in\mathbb{Z}\,|\,\exists z^k\to z\ {\rm with}\ \nabla\vartheta_2(z^k)\to\xi\big\}.
  \]
  Since $\partial_{B}\vartheta_2(z)$ is usually smaller than $\partial\vartheta_2(z)$, the stationary point in terms of Definition \ref{spoint-def} is stronger than those defined by replacing $\partial(-\vartheta_2)(G(x))$ with $-\partial\vartheta_2(G(x))$. When $G=\mathcal{I}$ and $h\equiv 0$, the latter is precisely the critical point in the DC literature \cite{LeTai18Jota,LiuPong19,Nguyen17}.
 \end{remark}
%-----------------------------------------------------------------------------------------------
\subsection{KL property of nonsmooth functions}\label{sec2.2}
%---------------------------------------------------------------------------------------------------
 To introduce the KL property of an extended real-valued function, for any $\eta\in(0,\infty]$, we denote by $\Upsilon_{\!\eta}$ the set consisting of all continuous concave $\varphi\!:[0,\eta)\to\mathbb{R}_{+}$ that is continuously differentiable on $(0,\eta)$ with $\varphi(0)=0$ and $\varphi'(s)>0$ for all $s\in(0,\eta)$.
%-------------------------------------------------------------------------------------------------------
 \begin{definition}\label{KL-def}
 A proper lsc $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ is said to have the KL property at $\overline{x}\in{\rm dom}\,\partial\!f$ if there exist $\eta\in(0,\infty]$,
 a neighborhood $\mathcal{U}$ of $\overline{x}$, and a function $\varphi\in\Upsilon_{\!\eta}$ such that
 \[
  \varphi'(f(x)\!-\!f(\overline{x})){\rm dist}(0,\partial\!f(x))\ge 1
  \quad{\rm for\ all}\ x\in\mathcal{U}\cap\big[f(\overline{x})<f<f(\overline{x})+\eta\big].
 \]
 If $\varphi$ can be chosen as $\varphi(t)=ct^{1-p}$ with $p\in[0,1)$ for some $c>0$, then $f$ is said to have the KL property of exponent $p$ at $\overline{x}$. If $f$ has the KL property (of exponent $p$) at each point of ${\rm dom}\,\partial\!f$, it is called a KL function (of exponent $p$).
 \end{definition}
%---------------------------------------------------------------------
\begin{remark}\label{KL-remark}
 {\bf(a)} By \cite[Lemma 2.1]{Attouch10}, a proper lsc function has the KL property at any non-critical point. Thus, to show that a proper lsc $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ is a KL function, it suffices to check its KL property at critical points. From \cite{Bolte07}, definable functions in an o-minimal structure over the real number field are all KL functions, which are very broad including semialgebraic functions, global subanalytic functions, and etc. On the calculation of KL exponent for some special classes of KL functions, refer to the recent works \cite{LiPong18,YuLiPong21,WuPanBi21}.

 \noindent
 {\bf(b)} For a weakly convex $f$, its KL property of exponent $p\in(0,1)$ at  $\overline{x}\in{\rm crit}f$ is closely related to the metric $q$-subregularity of $\partial\!f$ at $(\overline{x},0)\!\in{\rm gph}\partial\!f$ (see \cite[section 2]{LiuPanWY22}). The latter means that there exist $\varepsilon\!>0$ and $\kappa\!>0$ such that ${\rm dist}(x,(\partial\!f)^{-1}(0))\le\!\kappa[{\rm dist}(0,\partial\!f(x))]^{q}$ for all $x\in\mathbb{B}(\overline{x},\varepsilon)$. Clearly, the metric subregularity of $\partial\!f$ at $(\overline{x},0)\!\in{\rm gph}\partial\!f$ implies the metric $q$-subregularity of $\partial\!f$ at $(\overline{x},0)\!\in{\rm gph}\partial\!f$ with $q\in[0,1)$.

  \noindent
 {\bf(c)} For a weakly convex $f$, its KL property of exponent $p\in(0,1)$ at  $\overline{x}\in{\rm crit}f$ has an intimate relation with the local weak sharpness of $X^*=\mathop{\arg\min}_{x\in\mathbb{X}}(f(x)-f(\overline{x}))_{+}$ at $\overline{x}$ with exponent $\gamma$ (see \cite[Corollary 2.1]{BaiLi22}). The latter means that there exist $\varepsilon>0$ and $\kappa>0$ such that for all $x\in\mathbb{B}(\overline{x},\varepsilon)$, $\kappa(f(x)-f(\overline{x}))_{+}\ge[{\rm dist}(x,X^*)]^{\gamma}$.
\end{remark} 	

 To close this section, we state the relation between the function $\Theta_1$ (respectively, $\Theta_2$) and its local linearization at a point.
%----------------------------------------------------------------------------------------------
 \begin{lemma}\label{vtheta1-lemma}
  Consider any $x\in\mathbb{X}$. There exists $\varepsilon>0$ such that for any $z\in\mathbb{B}(x,\varepsilon)$,
  \begin{subnumcases}{}\label{Theta1-Lip}
   \big|\Theta_1(z)-\vartheta_1\big(F(x)\!+\!F'(x)(z-x)\big)\big|
  \le\frac{1}{2}{\rm lip}\vartheta_1(F(x)){\rm lip}F'(x)\|z-x\|^2,\\
  \label{Theta2-Lip}
   \big|\Theta_2(z)-\vartheta_2\big(G(x)\!+\!G'(x)(z-x)\big)\big|
  \le\frac{1}{2}{\rm lip}\vartheta_2(G(x)){\rm lip}G'(x)\|z-x\|^2.
  \end{subnumcases}
 \end{lemma}
 \begin{proof}
 By Assumption \ref{ass0} (i), $\vartheta_1$ is strictly continuous at $F(x)$. There is $\delta>0$ such that
 \begin{equation}\label{ineq-vtheta1}
  |\vartheta_1(y)-\vartheta_1(y')|\le {\rm lip}\vartheta_1(F(x))\|y-y'\|
  \quad\ \forall y,y'\in\mathbb{B}(F(x),\delta).
 \end{equation}	
 By the continuous differentiability of $F$ at $x$, there is $\varepsilon>0$ such that for any $z\in\mathbb{B}(x,\varepsilon)$,
 \[
 \|F(z)-F(x)\|\le\delta\ \ {\rm and}\ \ \|F'(x)(z-x)\|\le\delta.
 \]
 In addition, since $F'$ is strictly continuous on ${\rm dom}\Theta$, if necessary by shrinking $\varepsilon$, we have
 \[
   \|F'(z)-F'(z')\|\le{\rm lip}F'(x)\|z-z'\|\quad\ \forall z,z'\in\mathbb{B}(x,\varepsilon).
 \]
 Now fix any $z\in\mathbb{B}(x,\varepsilon)$. By invoking \eqref{ineq-vtheta1} with $y=F(z)$ and $y'=F(x)+F'(x)(z-x)$,
 \begin{align*}
 |\Theta_1(z)\!-\!\vartheta_1(F(x)+F'(x)(z\!-\!x))|
 &\le {\rm lip}\vartheta_1(F(x))\|F(z)-F(x)-F'(x)(z\!-\!x)\|\\
 &={\rm lip}\vartheta_1(F(x))\Big\|\!\int_{0}^{1}[F'(x\!+\!t(z\!-\!x))\!-\!F'(x)](z\!-\!x)dt\Big\|\\
 &\le\frac{1}{2}{\rm lip}\vartheta_1(F(x)){\rm lip}F'(x)\|z-x\|^2,
\end{align*}	
 where the last inequality is due to Assumption \ref{ass0} (ii) and $x\!+\!t(z\!-\!x)\in\mathbb{B}(x,\varepsilon)$ for all $t\in[0,1]$.
 By the arbitrariness of $z\in\mathbb{B}(x,\varepsilon)$, we obtain inequality \eqref{Theta1-Lip}. Using the same arguments yields inequality \eqref{Theta2-Lip}. Here, we omit the details.
 \end{proof}
%--------------------------------------------------------------------------------------------------------
 \section{Inexact linearized proximal algorithm}\label{sec3}

  To state the basic idea of our inexact LPA, for any $x,z\in\mathbb{X}$ we define the mappings
  \[
  \ell_{F}(x,s):=F(x)+F'(x)(s-x)\ \ {\rm and}\ \ \ell_{G}(x,s):=G(x)+G'(x)(s-x).
  \]
  Clearly, $\ell_{F}(x,\cdot)\!:\mathbb{X}\to\mathbb{Y}$ and $\ell_{G}(x,\cdot)\!:\mathbb{X}\to\mathbb{Z}$ are the linear approximation of $F$ and $G$ at $x$. Let $x^k$ be the current iterate.
  By using \eqref{Theta1-Lip} with $x=x^k$, for any $x$ close to $x^k$,
  \begin{equation}\label{theta1-ineq0}
  	\Theta_1(x)\le\vartheta_1(F(x^k)\!+\!F'(x^k)(x-x^k))+\frac{1}{2}{\rm lip}\vartheta_1(F(x^k)){\rm lip}F'(x^k)\|x-x^k\|^2.
  \end{equation}
  Pick any $\xi^k\in\partial(-\vartheta_2)(G(x^k))$. By \eqref{Theta2-Lip} and the concavity of $-\vartheta_2$, for any $x$ close to $x^k$,
  \begin{align}\label{theta2-ineq0}
  -\Theta_2(x)&\le-\vartheta_2(G(x^k)+G'(x^k)(x-x^k))+\frac{1}{2}{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)\|x-x^k\|^2\\
  \label{theta2-ineq1}
  &\le -\vartheta_2(G(x^k))+\langle\xi^k,G'(x^k)(x\!-\!x^k)\rangle+\frac{1}{2}{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)\|x\!-\!x\|^2.
  \end{align}
  For each $k\in\mathbb{N}$, write ${\rm lip}\Theta(x^k)\!:=
  {\rm lip}\vartheta_1(F(x^k)){\rm lip}F'(x^k)+{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)$. By using inequalities \eqref{theta1-ineq0} and \eqref{theta2-ineq1} and the expression of $\Phi$, for any $x$ close to $x^k$,
 \[
  \Phi(x)\le \vartheta_1(\ell_{F}(x^k,x))+\langle\nabla G(x^k)\xi^k,x\!-\!x^k\rangle+h(x)\!+\!\frac{1}{2}{\rm lip}\Theta(x^k)\|x-x^k\|^2\!-\!\Theta_2(x^k).
  \]
  Thus, by choosing a self-adjoint positive definite (PD) linear operator $\mathcal{Q}_k\!:\mathbb{X}\to\mathbb{X}$ with $\mathcal{Q}_k\succeq {\rm lip}\Theta(x^k)\mathcal{I}$, we obtain the following local strong convex majorization of $\Phi$ at $x^k$:
 \[
   q_{k}(x):=\vartheta_1(\ell_{F}(x^k,x))+\langle\nabla G(x^k)\xi^k,x\!-\!x^k\rangle+h(x)\!+\!\frac{1}{2}\|x-x^k\|_{\mathcal{Q}_k}^2\!-\!\Theta_2(x^k).
 \]
 In each step, our inexact LPA constructs a local majorization $q_k$ of $\Phi$ and then seeks an approximate minimizer of $q_k$ as the next iterate. Its iterate steps are as follows.
 %--------------------------------------------------------------------------------------------------------
 \begin{algorithm}[h]
  \caption{\label{iLPA}{\bf\,(Inexact linearized proximal algorithm)}}
  \textbf{Initialization:} Choose $\varrho>1,\,0<\underline{\gamma}<\overline{\gamma},\,0<\mu<\underline{\gamma}/4$ and $x^0\in{\rm dom}\Theta$. Set $k:=0$.\\
%-----------------------------------------------------------------------------
  \textbf{while} some stopping criterion is not satisfied \textbf{do}
  \begin{enumerate}
  \item  Choose $\xi^{k}\in\partial(-\vartheta_2)(G(x^k))$ and $\gamma_{k,0}\in[\underline{\gamma},\overline{\gamma}]$.
 		
  \item {\bf For $j=0,1,2,\ldots$}
         \begin{enumerate}
 		 \item Choose a self-adjoint PD linear operator $\mathcal{Q}_{k,j}\!:\mathbb{X}\to\mathbb{X}$ with
 			$\gamma_{k,j}\mathcal{I}\preceq\mathcal{Q}_{k,j}\preceq\varrho\gamma_{k,j}\mathcal{I}$. Seek an inexact minimizer $x^{k,j}$ of the following subproblem
 			\begin{equation}\label{subprobkj}
 			\!\min_{x\in\mathbb{X}}q_{k,j}(x)\!:=\!\vartheta_1(\ell_{F}(x^k,x))+\langle\nabla G(x^k)\xi^k,x-x^k\rangle+h(x)+\frac{1}{2}\|x-x^k\|_{\mathcal{Q}_{k,j}}^2\!-\Theta_2(x^k)
 			\end{equation}
 			such that
 			$q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})\le\frac{1}{2}\mu\|x^{k,j}\!-x^{k}\|^2$, where $\overline{x}^{k,j}$ is the unique optimal solution of subproblem \eqref{subprobkj}.
 			
 	   \item If $\Theta(x^{k,j})\le\vartheta_1\big(\ell_{F}(x^k,x^{k,j})\big)-\vartheta_2\big(\ell_{G}(x^k,x^{k,j})\big)+\frac{1}{2}\|x^{k,j}\!-x^k\|_{\mathcal{Q}_{k,j}}^2$, then set $j_k:=j$ and go to step 4; else let $\gamma_{k,j+1}=\varrho^{j+1}\gamma_{k,0}$.
 	  \end{enumerate}
 		
 \item  {\bf End (for)}.
 		
 \item  Set $x^{k+1}=x^{k,j_k},\overline{x}^{k+1}=\overline{x}^{k,j_k}$ and $\mathcal{Q}_{k}=\mathcal{Q}_{k,j_k}$. Let $k\leftarrow k+1$ and return Step 1.
 \end{enumerate}
 \textbf{end while}
 \end{algorithm}
 \begin{remark}\label{remark-alg}
  {\bf(i)} The inner for-end loop in Algorithm \ref{iLPA} aims to seek a desired estimation for ${\rm lip}\Theta(x^k)$. As will be shown in Lemma \ref{ls-welldef1} below, the inner loop stops within a finite number of iterates, hence Algorithm \ref{iLPA} is well defined.
  Moreover, from $\overline{x}^{k,j}\in{\rm dom}h$ and $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})\le\!\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2$ for each $j$, we have
  $x^{k,j}\in{\rm dom}h$, so $\{x^k\}_{k\in\mathbb{N}}\subset{\rm dom}h$.

 \noindent
 {\bf(ii)} Algorithm \ref{iLPA} is an extension of the Gauss-Newton method \cite{Pauwels16} and the inexact LPA \cite[Algorithm 19]{HuYang16}, both proposed for problem \eqref{prob} with $\vartheta_2\equiv 0$ and $h\equiv 0$. Compared with the Gauss-Newton method \cite{Pauwels16}, Algorithm \ref{iLPA} is practical because its subproblems are allowed to be solved inexactly and the inexact criterion  $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})\le\frac{\mu}{2}\|x^{k,j}-x^{k}\|^2$ is implementable by noting that the unknown $q_{k,j}(\overline{x}^{k,j})$ can be replaced by its lower estimation, say, the dual objective value of \eqref{subprobkj}. Different from the inexact LPA of \cite{HuYang16}, our inexact criterion controls the difference $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})$ by the quadratic term $\|x^{k,j}\!-\!x^{k}\|^2$ rather than $\|x^{k-1}\!-\!x^{k}\|^{\alpha}$ with $\alpha>2$.
 In addition, Algorithm \ref{iLPA} also extends the proximal DC algorithms proposed in \cite{LiuPong19} for problem \eqref{prob} with $F\equiv(f;\mathcal{I})$ and $G\equiv\mathcal{I}$ without requiring the gradient of $f$ to be globally Lipschitz continuous.

 \noindent
 {\bf(iii)} When $x^{k}=x^{k-1}$, from Step (2a) and the strong convexity of subproblem \eqref{subprobkj}, we deduce that $x^k=\overline{x}^k$. Together with Remark \ref{remark-spoint} and Proposition \ref{prop3-xk} later, it follows that $x^{k-1}=\overline{x}^k$ is a stationary point of problem \eqref{prob}. This implies that $\|x^k-x^{k-1}\|\le\epsilon$ with a tiny $\epsilon>0$ is an appropriate termination condition for Algorithm \ref{iLPA}.
 \end{remark}
%-----------------------------------------------------------------------------------------
 \begin{lemma}\label{ls-welldef1}
  Under Assumption \ref{ass0}, for each $k\in\mathbb{N}$, the inner loop of Algorithm \ref{iLPA} stops within a finite number of steps.
 \end{lemma}
 \begin{proof}
  Fix any $k\in\mathbb{N}$. Suppose on the contradiction that the inner loop of Algorithm \ref{iLPA} does not stop within a finite number of steps, i.e., for each $j\in\mathbb{N}$,
 \begin{equation}\label{aim-ineq1}
  \Theta(x^{k,j})-\vartheta_1(\ell_{F}(x^k,x^{k,j}))+\vartheta_2(\ell_{G}(x^k,x^{k,j}))>\frac{1}{2}\|x^{k,j}-x^k\|_{\mathcal{Q}_{k,j}}^2.
  \end{equation}
  From the definition of $x^{k,j}$ in the inner loop and the expression of $q_{k,j}$, for each $j\in\mathbb{N}$,
  \begin{align*}
 	q_{k,j}(\overline{x}^{k,j})+\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2
 	&\ge\vartheta_1(\ell_{F}(x^k,x^{k,j}))\!+\!\langle\nabla G(x^k)\xi^k,x^{k,j}\!-\!x^k\rangle\\
 	&\quad+h(x^{k,j})+\frac{1}{2}\|x^{k,j}\!-\!x^k\|_{\mathcal{Q}_{k,j}}^2-\Theta_2(x^k).
  \end{align*}
  Note that ${\rm ri}({\rm dom}h)\ne\emptyset$ by the properness and convexity of $h$, so  there exists $\widehat{x}\in{\rm ri}({\rm dom}h)$ such that $\partial h(\widehat{x})\ne\emptyset$. Pick any $\widehat{v}\in\partial h(\widehat{x})$. Then, $h(x^{k,j})\ge h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle$ for each $j\in\mathbb{N}$. In addition, from the finite convexity of $\vartheta_1$, $\partial\vartheta_1(F(\widehat{x}))\ne\emptyset$. Pick any $\widehat{\zeta}\in\partial\vartheta_1(F(\widehat{x}))$. Then,
 $\vartheta_1(\ell_{F}(x^k,x^{k,j}))\ge\vartheta_1(F(\widehat{x}))+\langle\widehat{\zeta},\ell_{F}(x^k,x^{k,j})\rangle$ for each $j\in\mathbb{N}$. Together with the last inequality, it follows that for all $j\in\mathbb{N}$,
 \begin{align*}
  q_{k,j}(\overline{x}^{k,j})
  &\ge \vartheta_1(F(\widehat{x}))+\langle\nabla\!F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k, x^{k,j}-x^k\rangle
   +h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle\\
   &\quad+\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle+\frac{1}{2}\|x^{k,j}-x^k\|_{Q_{k,j}}^2
   -\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2-\!\Theta_2(x^k).
 \end{align*} 	
 Note that $\Phi(x^k)=q_{k,j}(x^k)\ge q_{k,j}(\overline{x}^{k,j})$ for each $j\in\mathbb{N}$. Then, it holds that
 \begin{align*}
 \Phi(x^k)
  &\ge \vartheta_1(F(\widehat{x}))+\langle\nabla\!F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k, x^{k,j}-x^k\rangle
 	+h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle\\
 &\quad+\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle+\frac{1}{2}\|x^{k,j}-x^k\|_{Q_{k,j}}^2
 	-\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2-\!\Theta_2(x^k).
 \end{align*}
  Recall that $\mathcal{Q}_{k,j}\succeq\gamma_{k,0}\mathcal{I}\succeq\underline{\gamma}\mathcal{I}$ and $\underline{\gamma}>4\mu$. The last inequality implies that $x^{k,j}\to x^k$ as $j\to\infty$. By invoking Lemma \ref{vtheta1-lemma} with $z=x^{k,j}$ and $x=x^k$, for all sufficiently large $j$,
  \begin{subnumcases}{}
  	 \vartheta_1(F(x^{k,j}))-\vartheta_1\big(\ell_{F}(x^k,x^{k,j})\big)
  	\le \frac{1}{2}{\rm lip}\vartheta_1(F(x^k)){\rm lip}F'(x^k)\|x^{k,j}\!-x^k\|^2,\nonumber\\
  	 -\vartheta_2(G(x^{k,j}))
  	+\vartheta_2(\ell_{G}(x^k,x^{k,j}))\le\frac{1}{2}{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)\|x^{k,j}\!-x^k\|^2.\nonumber
  \end{subnumcases}
 Adding the last two inequalities together leads to
 \begin{equation*}
  \Theta(x^{k,j})-\vartheta_1\big(\ell_{F}(x^k,x^{k,j})\big)+\vartheta_2\big(\ell_{G}(x^k,x^{k,j})\big)\le\frac{1}{2}{\rm lip}\Theta(x^k)\|x^{k,j}-x^k\|^2,
 \end{equation*}
 which clearly contradicts \eqref{aim-ineq1} because ${\rm lip}\Theta(x^k)\mathcal{I}\prec\mathcal{Q}_{k,j}$ when $j$ is large enough.
 \end{proof}
%------------------------------------------------------------------------------------
 \section{Convergence analysis of Algorithm \ref{iLPA}}\label{sec3}
 %------------------------------------------------------------------------------------------
 Write $\mathbb{W}:=\mathbb{X}\times\mathbb{X}\times\mathbb{Z}\times\mathbb{S}_{+}$. Inspired by the work \cite{LiuPong19}, we define the potential function
 \begin{equation}\label{Xi-fun}
  \Xi(w):=\vartheta_1(\ell_{F}(x,s))+\langle \ell_{G}(x,s),z\rangle+h(s)
  +\vartheta_2^*(-z)+\|s-x\|_{\mathcal{Q}}^2
 \end{equation}
 for any $w=(x,s,z,\mathcal{Q})\in\mathbb{W}$, which will play a key role in the subsequent convergence analysis. The following lemma characterizes the subdifferential of $\Xi$ at any $w\in{\rm dom}\Xi$.
 %-----------------------------------------------------------------------------------------------------
 \begin{lemma}\label{subdiff-LemXi}
  Consider any $w=(x,s,z,\mathcal{Q})\in{\rm dom}h\times{\rm dom}h\times(-{\rm dom}\vartheta_2^*)\times\mathbb{S}_{+}$. Suppose that $\vartheta_2^*$ is strictly continuous at $-z$ relative to ${\rm dom}\vartheta_2^*$, and that $F'$ is strictly differentiable at $x$ and $G'$ is differentiable at $x$. Then, $\partial\Xi(w)=T_1(w)\times T_2(w)\times T_3(w)$ with
  \begin{align*}
  T_1(w)&=\left(\begin{matrix}
 			[D^2F(x)(s\!-\!x,\cdot)]^*\\
 			\nabla\!F(x)
 		\end{matrix}\right)\partial\vartheta_1(\ell_{F}(x,s))
 		\!+\!\left(\begin{matrix}
 			[D^2G(x)(s\!-\!x,\cdot)]^*z+2\mathcal{Q}(x\!-\!s)\\
 			\nabla G(x)z+\partial h(s)+2\mathcal{Q}(s\!-\!x)
 		\end{matrix}\right),\\
 		T_2(w)&=\ell_{G}(x,s)-\partial\vartheta_2^*(-z)\ \ {\rm and}\ \ T_3(w)=(s-x)(s-x)^{\top}+	 \mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}).	 	
 \end{align*}
 \end{lemma}
  \begin{proof}
  Write $\psi(x',s')\!:=\vartheta_1(\ell_{F}(x',s'))$ for $(x',s')\in\mathbb{X}\times\mathbb{X}$. Define the functions
  \[
 	f(w')\!:=\psi(x',s')+h(s')+\vartheta_2^*(-z')+\delta_{\mathbb{S}_{+}}(\mathcal{Q}')\ {\rm and}\   \Upsilon(w'):=\langle\ell_{G}(x',s'),z'\rangle+\|s'\!-\!x'\|_{\mathcal{Q}'}^2
  \]
  for $w'=(x',s',z',\mathcal{Q}')\in\mathbb{W}$. Clearly, $\Xi=f+\Upsilon$. Recall that $\vartheta_1$ is strictly continuous at $\ell_{F}(x,s)$ and ${\rm dom}\vartheta_1=\mathbb{Y}$. By Lemma \ref{chain-rule},  $\widehat{\partial}\psi(x,s)=\partial\psi(x,s)=\nabla\ell_{F}(x,s)\partial\vartheta_1(\ell_{F}(x,s))$. Together with the expression of $f$ and \cite[Exercise 10.10]{RW98}, we obtain
  \[
 	\partial\!f(w)=\widehat{\partial}\!f(w)
 	=\big[\partial\psi(x,s)+\{0\}\times\partial h(s)\big]
 	\times[-\partial\vartheta_2^*(-z)]\times\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}).
  \]
  This, along with the differentiablility of $\Upsilon$ at $w$ and \cite[Exercise 10.10]{RW98}, implies that
  \[
 	\partial\Xi(w)=\partial\!f(w)+\nabla\Upsilon(w).
  \]
  By the expression of $\Upsilon$ and $\partial\psi(x,s)
  =\nabla\ell_{F}(x,s)\partial\vartheta_1(\ell_{F}(x,s))$, we obtain the result.
 \end{proof}

 In the rest of this section, for each $k\in\mathbb{N}$, we write $w^{k}\!:=(x^{k-1},\overline{x}^{k},\xi^{k-1},\mathcal{Q}_{k-1})$. For a set $\Gamma\subset\mathbb{W}$,
 we denote by $\Gamma_{\!x}$ and $\Gamma_{\!z}$ the projection of $\Gamma$ onto $\mathbb{X}$ and $\mathbb{Z}$, respectively, i.e., $x\in\Gamma_{\!x}$ if and only if there exists $(s,z,\mathcal{Q})\in\mathbb{X}\times\mathbb{Z}\times\mathbb{S}_{+}$ such that $(x,s,z,\mathcal{Q})\in\Gamma$. To investigate the convergence of the sequence $\{w^k\}_{k\in\mathbb{N}}$, we need the following assumption:
 %-----------------------------------------------------------------------------------------------------
 \begin{assumption}\label{ass1}
 \begin{description}
  \item[(i)] The level set $\mathcal{L}_{\Phi}(\Phi(x^0))\!:=\{x\in\mathbb{X}\,|\,\Phi(x)\le\Phi(x^0)\}$ is bounded.
 		
 \item[(ii)] There exists an open set $\mathcal{O}\supset\Gamma^*$ such that $\vartheta_2^*$ is strictly continuous on $\mathcal{O}_{z}$ relative to ${\rm dom}\vartheta_2^*$, and $F'$ is strictly differentiable on $\mathcal{O}_{x}$ and $G'$ is differentiable on $\mathcal{O}_{x}$.
 \end{description} 	
 \end{assumption}
%---------------------------------------------------------------------------------------------
 \subsection{Global convergence of Algorithm \ref{iLPA}}\label{sec4.1}
%-----------------------------------------------------------------------------------------------
 For each $k\in\mathbb{N}$, by the convexity of $\vartheta_2$ and $\xi^k\!\in\partial(-\vartheta_2)(G(x^k))\subset-\partial\vartheta_2(G(x^k))$, we have
 \begin{subequations}
 \begin{equation}\label{pre-equa0}
 \vartheta_2(G(x^{k-1}))+\vartheta_2^*(-\xi^{k-1})=-\langle\xi^{k-1},G(x^{k-1})\rangle
 \end{equation}
 \begin{equation}\label{pre-equa1}  	
 -\vartheta_2(\ell_{G}(x^{k-1},x^k))\le-\!\vartheta_2(G(x^{k-1}))\!+\!\langle\nabla G(x^{k-1})\xi^{k-1},x^{k}\!-\!x^{k-1}\rangle.
 \end{equation}
 \end{subequations}
 By the definitions of $x^k$ and $\overline{x}^k$ in step 4 and the strong convexity of $q_{k,j_k}$, for each $k\in\mathbb{N}$,
 \begin{equation}\label{pre-equa3}
  \Phi(x^k)=q_{k,j_k}(x^k)\ge q_{k,j_k}(\overline{x}^{k+1})+\frac{1}{2}\|\overline{x}^{k+1}-x^k\|_{\mathcal{Q}_k}^2.
 \end{equation}
 We first use \eqref{pre-equa0}-\eqref{pre-equa3} to establish the convergence of $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$.
%-----------------------------------------------------------------------------------------
 \begin{proposition}\label{prop1-xk}
  Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Then, under Assumption \ref{ass0}, the following assertions hold.
  \begin{itemize}
   \item [(i)] For each $k\in\mathbb{N}$, $\Xi(w^{k+1})\le\Xi(w^{k})-(\underline{\gamma}/4-\mu)\|x^k-x^{k-1}\|^2$.

  \item[(ii)] For each $k\in\mathbb{N}$, $\Phi(x^k)+(\underline{\gamma}/4-\mu)\|x^k-x^{k-1}\|^2\le\Xi(w^k)\leq \Phi(x^{k-1})$.

   \item[(iii)] The sequences $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ and $\{\Xi(w^{k})\}_{k\in\mathbb{N}}$ are decreasing and convergent.
  \end{itemize}
 \end{proposition}
 \begin{proof}
 {\bf(i)} Fix any $k\in\mathbb{N}$. By step 2(b) of Algorithm \ref{iLPA} and inequality \eqref{pre-equa1}, we have
 \begin{align*}%\label{descent-eq1}
   \Theta(x^{k})
   &\le\vartheta_1(\ell_{F}(x^{k-1},x^k))-\vartheta_2(\ell_{G}(x^{k-1},x^k))+\frac{1}{2}\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
   &\le\vartheta_1(\ell_{F}(x^{k-1},x^k))\!-\!\Theta_2(x^{k-1})\!+\!
   \langle\nabla G(x^{k-1})\xi^{k-1},x^k\!-\!x^{k-1}\rangle\!+\!\frac{1}{2}\|x^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
  \end{align*}
  In addition, from equation \eqref{pre-equa3} with the expression of $q_{k,j_k}$, it follows that
  \[
   \Phi(x^{k})
   \ge\vartheta_1(\ell_{F}(x^k,\overline{x}^{k+1}))+\langle\nabla G(x^k)\xi^k,\overline{x}^{k+1}\!-\!x^k\rangle+h(\overline{x}^{k+1})+\|\overline{x}^{k+1}\!-\!x^k\|_{\mathcal{Q}_k}^2-\Theta_2(x^k).
  \]
  Combining the last two inequalities with $\Phi(x^{k})=\Theta(x^{k})+h(x^k)$ yields that
  \begin{align*}
  &\vartheta_1(\ell_{F}(x^k,\overline{x}^{k+1}))+\langle\nabla G(x^k)\xi^k,\overline{x}^{k+1}\!-\!x^k\rangle+h(\overline{x}^{k+1})+\|\overline{x}^{k+1}\!-\!x^k\|_{\mathcal{Q}_k}^2\\
  &\le \vartheta_1\big(\ell_{F}(x^{k-1},x^k)\big)+\langle\nabla G(x^{k-1})\xi^{k-1},x^k\!-\!x^{k-1}\rangle+h(x^k)\\
  &\quad+\vartheta_2(G(x^k))-\!\vartheta_2(G(x^{k-1}))+\frac{1}{2}\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
 \end{align*}
  Together with the expression of $\Xi$ and equality \eqref{pre-equa0}, it then follows that
  \begin{align}
  \Xi(w^{k+1})&\le\vartheta_1\big(\ell_{F}(x^{k-1},x^k)\big)+\langle\nabla G(x^{k-1})\xi^{k-1},x^k\!-\!x^{k-1}\rangle+h(x^k)\nonumber\\
  &\quad-\!\vartheta_2(G(x^{k-1}))+\frac{1}{2}\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\nonumber\\
  \label{final-ineq400}
  &=q_{k-1,j_{k-1}}(x^{k})\le q_{k-1,j_{k-1}}(\overline{x}^k)+\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2\\
  &=\vartheta_1\big(\ell_{F}(x^{k-1},\overline{x}^k)\big)
  +\langle\nabla G(x^{k-1})\xi^{k-1},\overline{x}^k\!-\!x^{k-1}\rangle  -\!\vartheta_2(G(x^{k-1}))\nonumber\\
  &\quad+h(\overline{x}^k)+\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2+\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2\nonumber\\
  \label{temp-ineq40}
  &=\Xi(w^{k})-\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2+\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2
  \end{align}
  where the second inequlity is using step (2a) of Algorithm \ref{iLPA} and the expression of $q_{k-1,j_{k-1}}$, and the last equality is due to \eqref{pre-equa0} and the expression of function $\Xi$.
  For the term $\Delta_k\!:=\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2$ in \eqref{temp-ineq40}, using Cauchy-Schwarz inequality yields that
  \begin{align*}
   \Delta_k
   &\ge\frac{1}{2}\|\overline{x}^k\!-\!x^{k}\|_{\mathcal{Q}_{k-1}}^2+\frac{1}{2}\|x^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-\|\overline{x}^k\!-\!x^{k}\|_{\mathcal{Q}_{k-1}}^2-\frac{1}{4}\|{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
  &=-\frac{1}{2}\|\overline{x}^k\!-\!x^{k}\|_{\mathcal{Q}_{k-1}}^2+\frac{1}{4}\|{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
  \end{align*}
  In addition, by the strong convexity of $q_{k-1,j_{k-1}}$ and inequality \eqref{final-ineq400}, it holds that
  \begin{equation}\label{key-ineq40}
  \frac{1}{2}\|\overline{x}^k-x^{k}\|_{\mathcal{Q}_{k-1}}^2
  \le q_{k-1,j_{k-1}}(x^{k})-q_{k-1,j_{k-1}}(\overline{x}^{k})
  \le \frac{\mu}{2}\|x^{k}-x^{k-1}\|^2.
  \end{equation}
  Then, from the last two inequalities, we immediately obtain that
  \begin{equation}\label{key-ineq41}
   \Delta_k=\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2  \ge-\frac{\mu}{2}\|{x}^{k}-x^{k-1}\|^2+\frac{1}{4}\|{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
  \end{equation}
  Substituting \eqref{key-ineq41} into \eqref{temp-ineq40} and noting that $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ leads to the desired inequality.

  \noindent
  {\bf(ii)} Fix any $k\in\mathbb{N}$. From the last two equalities in \eqref{temp-ineq40} and inequality \eqref{pre-equa3}, it holds that
  \[
  \Xi(w^{k})
  = q_{k-1,j_{k-1}}(\overline{x}^{k})+\frac{1}{2}\|\overline{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\leq \Phi(x^{k-1}).
  \]
  In addition, from inequality \eqref{final-ineq400}, it follows that
  \begin{align*}
  	q_{k-1,j_{k-1}}(\overline{x}^{k})
   	&\ge\vartheta_1(\ell_{F}(x^{k-1},{x}^{k}))\!-\!\vartheta_2(G(x^{k-1}))\!+\!\langle\nabla G(x^{k-1})\xi^{k-1},x^{k}\!-\!x^{k-1}\rangle\\
  	 &\quad+h(x^k)+\!\frac{1}{2}\|x^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-\frac{\mu}{2}\|{x}^{k}\!-\!x^{k-1}\|^2\\
  	&\ge\Phi(x^k)\!-\!\frac{\mu}{2}\|{x}^{k}\!-\!x^{k-1}\|^2,
  \end{align*}
  where the last inequality is using \eqref{pre-equa1} and step (2b).
  From the last two inequalities,
  \[
    \Phi(x^k)-\frac{\mu}{2}\|{x}^{k}\!-\!x^{k-1}\|^2+\frac{1}{2}\|\overline{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\le\Xi(w^{k})\le\Phi(x^{k-1}),
  \]
  which together with \eqref{key-ineq41} and $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ implies the desired inequality.

 \noindent
 {\bf(iii)} The decreasing of the sequences $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ are due to parts (i)-(ii) and $\mu<\underline{\gamma}/4$.
  By Assumption \ref{ass0} (iii), the sequence $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ is lower bounded, so is $\{\Xi(w^{k})\}_{k\in\mathbb{N}}$ by part (ii). Consequently, they are both convergent.
 \end{proof}

 The following proposition shows the subsequence convergence of the sequence $\{w^k\}_{k\in\mathbb{N}}$.
%-----------------------------------------------------------------------------------------
\begin{proposition}\label{prop2-xk}
 Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}, and let $\Gamma^*$ denote its cluster point set. Suppose that Assumptions \ref{ass0} and \ref{ass1} (i) hold. Then,
 \begin{itemize}
 \item[(i)] the sequence $\{\gamma_k\}_{k\in\mathbb{N}}$ with $\gamma_k\!:=\!\gamma_{k,j_k}$ is bounded, so is the sequence $\{\|\mathcal{Q}_k\|\}_{k\in\mathbb{N}}$.

 	
 \item[(ii)] The sequence $\{w^{k}\}_{k\in\mathbb{N}}$ is bounded, and $\Gamma^*$ is nonempty and compact.
		
 \item[(iii)] For each $\widehat{w}=(\widehat{x},\widehat{z},\widehat{\xi},\widehat{\mathcal{Q}})\in\Gamma^*$, $\widehat{x}=\widehat{z}$ is a stationary point of problem \eqref{prob} and $\Xi(\widehat{w})=\Xi^*\!:=\lim_{k\to\infty}\Xi(w^k)$.
 If, in addition, Assumption \ref{ass1} (ii) holds, $\Gamma^*\subset{\rm crit}\,\Xi$.
	\end{itemize}
\end{proposition}
\begin{proof}
 {\bf(i)} Suppose on the contradiction that the sequence $\{\gamma_k\}_{k\in\mathbb{N}}$ is unbounded. Then there exists an index set $K\subset\mathbb{N}$ such that $\lim_{K\ni k\to\infty}\gamma_k=\infty$. For each $k\in K$, write $\widetilde{\gamma}_k:=\varrho^{-1}\gamma_k=\gamma_{k,j_k-1}$. To invoke inequalities
 \eqref{theta1-ineq0}-\eqref{theta2-ineq0} with $x=x^{k,j_k-1}$ for $k\in K$, we first argue that $\lim_{K\ni k\rightarrow\infty}\|x^{k,j_k-1}\!\!-\!x^{k}\|=0$. Fix any $k\in K$. From step 2(a), we have $q_{k,j_k-1}(\overline{x}^{k,j_k-1})-q_{k,j_k-1}(x^{k,j_k-1})\ge-\frac{\mu}{2}\|x^{k,j_k-1}-x^{k}\|^2$, which implies that
 \begin{align}\label{bound-M1}
 &q_{k,j_k-1}(\overline{x}^{k,j_k-1})\!-\!\vartheta_1(\ell_{F}(x^k,x^{k,j_k-1}))\!-\!\langle\nabla G(x^k)\xi^k,x^{k,j_k-1}\!-\!x^k\rangle\!-\!h(x^{k,j_k-1})\!+\Theta_2(x^k)\nonumber\\
 &\ge\frac{1}{2}\|x^{k,j_k-1}-x^k\|_{\mathcal{Q}_{k,j_k-1}}^2-\frac{\mu}{2}\|x^{k,j_k-1}-x^{k}\|^2
 	\ge\frac{\widetilde{\gamma}_k\!-\!\mu}{2}\|x^{k,j_k-1}\!\!-\!x^{k}\|^2,
 \end{align}
 where the last inequality is due to $\mathcal{Q}_{k,j_k-1}\succeq\gamma_{k,j_k-1}\mathcal{I}=\widetilde{\gamma}_{k}\mathcal{I}$.
 In addition, from the optimality of $\overline{x}^{k,j_k-1}$ and the feasibility of $x^k$ to subproblem \eqref{subprobkj} with $j=j_{k}-1$,
 \[
 q_{k,j_k-1}(\overline{x}^{k,j_k-1})\le q_{k,j_k-1}({x}^{k})=\Phi(x^k).
 \]
 Pick any $\widehat{v}\in\partial h(\widehat{x})$, which is nonempty due to ${\rm ri}({\rm dom}\,h)\ne\emptyset$. Then, $h(x^{k,j_k-1})\ge h(\widehat{x})+\langle\widehat{v},x^{k,j_k-1}-\widehat{x}\rangle$. Also, from the finite convexity of $\vartheta_1$, $\partial\vartheta_1(F(\widehat{x}))\ne\emptyset$. Pick any $\widehat{\zeta}\in\partial\vartheta_1(F(\widehat{x}))$. Then $\vartheta_1(\ell_{F}(x^k,x^{k,j_k-1}))\ge \vartheta_1(F(\widehat{x}))+\langle\widehat{\zeta},F(x^k)+F'(x^k)(x^{k,j_k-1}-x^k)-F(\widehat{x})\rangle$. Together with the last two inequalities, it then follows that
 \begin{align*}
 \frac{\widetilde{\gamma}_k\!-\!\mu}{2}\|x^{k,j_k-1}\!\!-\!x^{k}\|^2
  &\le \Phi(x^k)+\Theta_2(x^k)\!-\!\Theta_1(\widehat{x})\!-\!\langle\widehat{\zeta},F(x^k)\!-\!F(\widehat{x})\rangle\!-\!h(\widehat{x})\\
  &\quad-\langle\nabla F(x^k\!)\widehat{\zeta}+\!\!\nabla G(x^k)\xi^k,x^{k,j_k-1}\!\!-\!x^k\rangle-\langle\widehat{v},x^{k,j_k-1}-\widehat{x}\rangle\\
  &\le\Phi(x^0)+\Theta_2(x^k)\!-\!\Theta_1(\widehat{x})\!-\!\langle\widehat{\zeta},F(x^k)\!-\!F(\widehat{x})\rangle\!-\!h(\widehat{x})\\
 &\quad-\langle\widehat{v},x^k-\widehat{x}\rangle+\big[\|\nabla F(x^{k})\widehat{\zeta}\|
 	+\|\nabla G(x^{k})\xi^{k}\|+\|\widehat{v}\|\big]\|{x}^{k,j_k-1}\!-\!x^{k}\|,
 \end{align*}
 where the second inequality is due to Proposition \ref{prop1-xk} (ii).
 By Assumption \ref{ass1} (i), $\{x^k\}_{k\in\mathbb{N}}$ is bounded because $\{x^k\}_{k\in\mathbb{N}}\subset \mathcal{L}_{\Phi}(\Phi(x^0))$ by Proposition \ref{prop1-xk} (ii), so is the sequence $\{\xi^k\}_{k\in\mathbb{N}}$ by \cite[Theorem 9.13]{RW98} (d). Thus, from $\lim_{K\ni k\to\infty}\gamma_k=\infty$, the last inequality implies that $\lim_{K\ni k\to\infty}\|x^{k,j_k-1}\!\!-\!x^{k}\|=0$. Now by invoking \eqref{theta1-ineq0}-\eqref{theta2-ineq0} with $z=x^{k,j_k-1}$ for $k\in K$ and adding them together, for all sufficiently large $k\in K$, we have
 \begin{equation}\label{Theta-ineq0}
 \Theta(x^{k,j_k-1})\le\vartheta_1\big(\ell_{F}(x^k,x^{k,j_k-1})\big)-\vartheta_2\big(\ell_{G}(x^k,x^{k,j_k-1})\big)+\frac{1}{2}{\rm lip}\Theta(x^k)\|x^{k,j_k-1}-x^k\|^2.
 \end{equation}
 On the other hand, by the inner loop of Algorithm \ref{iLPA} and $\mathcal{Q}_{k,j_k-1}\succeq \widetilde{\gamma}_{k}\mathcal{I}$, for each $k\in K$,
 \begin{equation*}
 \Theta(x^{k,j_k-1})
 \ge\vartheta_1\big(\ell_{F}(x^k,x^{k,j_k-1})\big)-\vartheta_2\big(\ell_{G}(x^k,x^{k,j_k-1})\big)+\frac{\widetilde{\gamma}_k}{2}\|x^{k,j_k-1}\!-x^k\|^2,
 \end{equation*}
 which yields a contradiction to \eqref{Theta-ineq0} because ${\rm lip}\Theta(x^k)$ is bounded by the strict continuity of $\Theta$ and the boundedness of $\{x^k\}_{k\in\mathbb{N}}$, but $\lim_{K\ni k\to\infty}\widetilde{\gamma}_k=\infty$.	
	
 \noindent
 {\bf(ii)} The boundedness of $\{x^k\}_{k\in\mathbb{N}}$ and $\{\xi^k\}_{k\in\mathbb{N}}$ are justified in the proof of part (i). From \eqref{temp-ineq40}, $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$, Proposition \ref{prop1-xk} (iii) and the boundedness of $\{x^k\}_{k\in\mathbb{N}}$, the sequence $\{\overline{x}^k\}_{k\in\mathbb{N}}$ is bounded. Thus, along with part (i), we obtain the boundedness of $\{w^k\}_{k\in\mathbb{N}}$.

 \noindent
 {\bf(iii)} Pick any $\widehat{w}=(\widehat{x},\widehat{z},\widehat{\xi},\widehat{\mathcal{Q}})\in\Gamma^*$. There exists a subsequence $\{w^k\}_{k\in K}$ such that $\lim_{K\ni k\to\infty}w^k=\widehat{w}$, where $K\subset\mathbb{N}$ is an index set. From $\underline{\gamma}-4\mu>0$ and Proposition \ref{prop2-xk}  (ii)-(iii), $\lim_{K\ni k\to\infty}\|x^k-x^{k-1}\|=0$. Together with \eqref{key-ineq40} and $\mathcal{Q}_k\ge\underline{\gamma}I$, it follows that $\lim_{K\ni k\to\infty}\|\overline{x}^k-x^{k}\|=0$, so $\widehat{x}=\widehat{z}$. Since $\overline{x}^k$ is the optimal solution of the $(k\!-\!1)$-th subproblem, from \cite[Theorem 23.9]{Roc70}, ${\rm dom}\vartheta_1=\mathbb{Y}$ and ${\rm dom}\vartheta_2=\mathbb{Z}$,
 \begin{align*}
 	0&\in\nabla\!F(x^{k-1})\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))+\nabla G(x^{k-1})
 	\xi^{k-1}+\mathcal{Q}_{k-1}(\overline{x}^k\!-\!x^{k-1})+\partial h(\overline{x}^k)\\
 	&\subset\nabla\!F(x^{k-1})\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))+\nabla G(x^{k-1})
 	\partial(-\vartheta_2)(x^{k-1})+\mathcal{Q}_{k-1}(\overline{x}^k\!-\!x^{k-1})+\partial h(\overline{x}^k).
 \end{align*}
 Passing the limit $K\ni k\to\infty$ to the last inclusion and using the outer semicontinuity of $\partial\vartheta_1,\partial(-\vartheta_2)$ and $\partial h$ yields that $0\in\nabla\!F(\widehat{x})\partial\vartheta_1(F(\widehat{x}))+\nabla G(\widehat{x})\partial(-\vartheta_2)(G(\widehat{x}))+\partial h(\widehat{x})$, so $\widehat{x}$ is a stationary point of problem \eqref{prob}. We next argue that $\Xi(\widehat{w})=\lim_{k\to\infty}\Xi(w^k)$.
 Note that $-\widehat{\xi}\in\partial\vartheta_2(G(\widehat{x}))$. Hence, $\langle G(\widehat{x}),\widehat{\xi}\rangle+\vartheta_2^*(-\widehat{\xi})=-\vartheta_2(G(\widehat{x}))$.
 By the expression of $\Xi$ and $\widehat{x}=\widehat{z}$, we have $\Xi(\widehat{w})=\vartheta_1(F(\widehat{x}))-\vartheta_2(G(\widehat{x}))+h(\widehat{x})$.
 On the other hand, from the feasibility of $\widehat{x}$ and the optimality of $\overline{x}^k$ to the $(k\!-\!1)$-th subproblem, it follows that
 \begin{align*}
 	&\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))+\langle\nabla G(x^{k-1})\xi^{k-1},\overline{x}^{k}-x^{k-1}\rangle+h(\overline{x}^{k})+
 	\frac{1}{2}\|\overline{x}^{k}-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
 	&\le\vartheta_1(\ell_{F}(x^{k-1},\widehat{x})+\langle\nabla G(x^{k-1})\xi^{k-1},\widehat{x}-x^{k-1}\rangle+h(\widehat{x})+\frac{1}{2}\|\widehat{x}-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2,
 \end{align*}
 which by $\lim_{K\ni k\to\infty}\overline{x}^k=\lim_{K\ni k\to\infty}x^{k-1}$ and the continuity of  $\vartheta_1,F'$ and $G'$ implies that $\limsup_{K\ni k\to\infty}h(\overline{x}^{k})\le h(\widehat{x})$. Along with the lower semicontinuity of $h$,
 $\lim_{K\ni k\to\infty}h(\overline{x}^{k})=h(\widehat{x})$. From \eqref{pre-equa0},  $\lim_{K\ni k\to\infty}\vartheta_2^*(-\xi^k)=-\vartheta_2(G(\widehat{x}))-\langle G(\widehat{x}),\widehat{\xi}\rangle$.
 Thus, from the expression of $\Xi$, $\lim_{K\ni k\to\infty}\Xi(w^k)=\vartheta_1(F(\widehat{x}))-\vartheta_2(G(\widehat{x}))+h(\widehat{x})=\Xi(\widehat{w})$.

 If, in addition, Assumption \ref{ass1} (ii) holds, by combining $\widehat{x}=\widehat{z}$ and
 $0\in\nabla\!F(\widehat{x})\partial\vartheta_1(F(\widehat{x}))+\nabla G(\widehat{x})\partial(-\vartheta_2)(G(\widehat{x}))+\partial h(\widehat{x})$ with Lemma \ref{subdiff-LemXi},
 we have $0\in\partial\Xi(\widehat{w})$, i.e., $\Gamma^*\subset{\rm crit}\,\Xi$.
 \end{proof}

 To achieve the global convergence of $\{x^k\}_{k\in\mathbb{N}}$, we also need the following result.
%---------------------------------------------------------------------------------------------
 \begin{proposition}\label{prop3-xk}
  Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Then, under Assumptions \ref{ass0}-\ref{ass1}, there exist $\overline{k}\in\mathbb{N}$ and a constant $\alpha>0$ such that for each $k\ge\overline{k}$, there exists $\zeta^{k}\in\partial\Xi(w^{k})$ with $\|\zeta^{k}\|\le\alpha\|x^{k-1}-x^k\|$.
 \end{proposition}
 \begin{proof}
  By Proposition \ref{prop2-xk} (ii), $\lim_{k\to\infty}{\rm dist}(w^k,\Gamma^*)=0$, so there exists $\overline{k}\in\mathbb{N}$ such that $w^k\in\mathcal{O}$ for all $k\ge\overline{k}$. Fix any $k\ge\overline{k}$. By the optimality of $\overline{x}^{k}$ to the $(k\!-\!1)$th subproblem,
  \begin{equation}\label{temp-inclusion1}
  0\in\nabla F(x^{k-1})\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^{k}))
 	+\nabla G(x^{k-1})\xi^{k-1}+\partial h(\overline{x}^{k})+\mathcal{Q}_{k-1}(\overline{x}^{k}\!-\!x^{k-1}),
  \end{equation}
  which implies that $\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^{k}))\ne\emptyset$. Pick any $v^{k-1}\!\in\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^{k}))$.
  Recalling that $-\xi^{k-1}\!\in\partial\vartheta_2(G(x^{k-1}))$, we have $G(x^{k-1})\in\partial\vartheta_2^*(-\xi^{k-1})$. Since  $\mathcal{Q}_{k-1}\!\succeq\underline{\gamma}\mathcal{I}$, it holds that $0\in\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}_{k-1})$. Write
  \[
 	\zeta^k\!:=\!\left(\begin{matrix}\!
 	 [D^2F(x^{k-1})(\overline{x}^{k}\!-\!x^{k-1},\cdot\!)]^*v^{k-1}\!\!+\!\![D^2G(x^{k-1})(\overline{x}^{k}\!-\!x^{k-1},\cdot)]^*\xi^{k-1}\!+\!\!2\mathcal{Q}_{k-1}(x^{k-1}\!-\!\overline{x}^{k}\!)\\
 	\mathcal{Q}_{k-1}(\overline{x}^{k}-x^{k-1})\\
 	\ell_{G}(x^{k-1},\overline{x}^{k})-G(x^{k-1})\\
 	(\overline{x}^{k}-x^{k-1})(\overline{x}^{k}-x^{k-1})^{\top}
 	\end{matrix}\right).
  \]
 By comparing with the expression of $\partial\Xi$ in Lemma \ref{subdiff-LemXi} and using equation \eqref{temp-inclusion1}, it is not hard to obtain $\zeta^k\in\partial\Xi(w^k)$. Since $\vartheta_1$ and $\vartheta_2$ are strictly continuous at each $y\in\mathbb{Y}$ and $z\in\mathbb{Z}$, respectively, from \cite[Theorem 9.13]{RW98}, $\|v^{k-1}\|\le{\rm lip}\,\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))$ and $\|\xi^{k-1}\|\le{\rm lip}\,\vartheta_2(G(x^{k-1}))$. Together with the boundedness of $\{w^k\}_{k\in\mathbb{N}}$, there exists a constant $\alpha_1>0$ such that $\max(\|v^{k-1}\|,\|\xi^{k-1}\|)\le \alpha_1$ for all $k\ge\overline{k}$. Together with the boundedness of $\{\|\mathcal{Q}_{k-1}\|\}_{k\in\mathbb{N}}$ and the expression of $\zeta^k$, there exists $\alpha_2>0$ such that
 \[
  \|\zeta^k\|\le \alpha_2\|x^{k-1}-\overline{x}^{k}\|\le  \alpha_2\big[\|x^{k-1}-x^k\|+\|\overline{x}^{k}-x^k\|\big].
 \]
 In addition, by the choice of $\mathcal{Q}_{k-1}$ in Algorithm \ref{iLPA}, $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$. Along with \eqref{key-ineq40}, we get
 \begin{equation}\label{key-ineq42}
  \|\overline{x}^k-x^k\|\le\sqrt{\mu\underline{\gamma}^{-1}}\|x^k-x^{k-1}\|\quad\forall k\in\mathbb{N}.
 \end{equation}
 The desired result follows from the last two inequalities with  $\alpha=\alpha_2(1+\!\sqrt{\mu\underline{\gamma}^{-1}})$.
 \end{proof}

 Now using Proposition \ref{prop2-xk} (ii)-(iii) and Propositions \ref{prop1-xk} (i) and \ref{prop3-xk} and following the same arguments as those for \cite[Theorem 1]{Bolte14}, we have the following convergence result.
%----------------------------------------------------------------------------------------------
 \begin{theorem}\label{global-converge}
  Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Suppose that Assumptions \ref{ass0}-\ref{ass1} hold, and that $\Xi$ is a KL function. Then, $\{x^k\}_{k\in\mathbb{N}}$ and $\{\overline{x}^k\}_{k\in\mathbb{N}}$ converge to the same point, say, $x^*$, and $x^*$ is a stationary point of problem \eqref{prob}.
 \end{theorem}
 \begin{remark}
  From \cite[Theorem 4.1]{Attouch10}, if $\Xi$ is definable in an o-minimal structure over the real field $(\mathbb{R},+,\cdot)$, then it has the KL property at each point of ${\rm dom}\,\partial\Xi$. From the expression of $\Xi$ in \eqref{Xi-fun} and \cite[Sections 2 $\&$\,3]{Ioffe09}, when $F,G$ and $\vartheta_1,\vartheta_2,h$ are definable in the same o-minimal structure, $\Xi$ is definable in this o-minimal structure. As discussed in \cite[Section 4]{Attouch10} and \cite[Sections 2 $\&$\,3]{Ioffe09}, definable functions in an o-minimal structure are very rich, which cover semialgebraic functions, globally subanalytic functions, etc.
 \end{remark}
 %--------------------------------------------------------------------------------------------
 \subsection{Local convergence rate of Algorithm \ref{iLPA}}\label{sec4.2}

 For the local convergence rate of Algorithm \ref{iLPA}, we have the following result where, part (i) is implied by \cite[Remark 3.1 (a)]{QianPan22}, and parts (ii)-(iii) can be achieved by using Theorem \ref{global-converge}, Proposition \ref{prop1-xk} and the same arguements as those for \cite[Theorem 2]{Attouch09}.
 %-----------------------------------------------------------------------------------------------------
 \begin{theorem}\label{convergece-rate}
  Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Suppose that Assumptions \ref{ass0}-\ref{ass1} hold, and that $\Xi$ has the KL property of exponent $p\in(0,1)$ on the set $\Gamma^*$. Then, the sequence $\{w^k\}_{k\in\mathbb{N}}$ is convergent and the following assertions hold:
  \begin{itemize}
  \item[(i)] if $p\in(0,\frac{1}{2})$, then $\{x^k\}_{k\in\mathbb{N}}$ converges to $x^*$ with a  Q-superlinear rate of order $\frac{1}{2p}$;
  	
  \item[(ii)]  if $p=1/2$, there exist $c_1>0$ and $\rho\in(0,1)$ such that $\|x^k-x^*\|\leq c_1\rho^k$;
 		
  \item[(iii)] if $p\in(\frac{1}{2},1)$, then exist $c_1>0$ such that  $\|x^k-x^*\|\le c_1k^{-\frac{1-p}{2p-1}}$.
 \end{itemize}
 \end{theorem}

 As is well known, it is not an easy task to verify the KL property of exponent $p\in(0,1)$ for a nonconvex and nonsmooth function because there are no convenient sufficient and necessary rules to identify it. Next we focus on the verifiable conditions for the KL property of $\Xi$ with exponent $p\in(0,1)$. For this purpose, we introduce the function
%--------------------------------------------------------------------------------------------------
 \begin{equation}\label{wXi-fun}
 \widetilde{\Xi}(x,s,z):=\vartheta_1(\ell_{F}(x,s))+\langle \ell_{G}(x,s),z\rangle+h(s)+\vartheta_2^*(-z)\quad\forall(x,s,z) \in\mathbb{X}\times\mathbb{X}\times\mathbb{Z}.
 \end{equation}
  Under the assumption of Lemma \ref{subdiff-LemXi}, at any $(x,s,z)\in{\rm dom}h\times{\rm dom}h\times(-{\rm dom}\vartheta_2^*)$,
%---------------------------------------------------------------------------------------------------
 \begin{equation}\label{subdiff-wXi}
  \partial\widetilde{\Xi}(x,s,z)\!=\!
  \left(\begin{matrix}
 	\left(\begin{matrix}
 	[D^2F(x)(s-x,\cdot)]^*\\ \nabla\!F(x)
 	\end{matrix}\right)\partial\vartheta_1(\ell_{F}(x,s))
  + \left(\begin{matrix}
  	 [D^2G(x)(s-x,\cdot)]^*z\\ \nabla G(x)z+\partial h(s)
  	\end{matrix}\right)\\
  \ell_{G}(x,s)-\partial\vartheta_2^*(-z)
  \end{matrix}\right).
 \end{equation}
 By comparing \eqref{subdiff-wXi} with the expression of $\partial\Xi$ in Lemma \ref{subdiff-LemXi}, we conclude that  $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$ if and only if $(\overline{x},\overline{x},\overline{z},\mathcal{Q})$ for a certain PD linear mapping $\mathcal{Q}\!:\mathbb{X}\to\mathbb{X}$ is a critical point of $\Xi$. Moreover, by combining \eqref{subdiff-wXi} with Definition \ref{spoint-def}, it follows that if $\overline{x}$ is a stationary point of \eqref{prob},  there exists $\overline{z}\in-\partial\vartheta_2(G(\overline{x}))$ such that $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$; and if $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$, $\overline{x}$ is a stationary point of \eqref{prob}.
%--------------------------------------------------------------------------------------------
 \begin{proposition}\label{prop-KL0}
  If $F,G$ are polyhedral and $\vartheta_1$ is piecewise linear-quadratic convex, $\partial\vartheta_2^*$ and $\partial h$ are metrically p-subregular with $p\in(1/2,1]$ at any point of their graphs, and $\widetilde{\Xi}$ is a KL function, then $\widetilde{\Xi}$ and $\Xi$ are the KL function of exponent $\frac{1}{2p}$.
 \end{proposition}	
 \begin{proof}
  Let $f(x,s,z):=\langle \ell_{G}(x,s),z\rangle$ and $g(x,s,z)\!:=\vartheta_1(\ell_{F}(x,s))+h(s)+\vartheta_2^*(-z)$ for $(x,s,z)\in\mathbb{X}\times\mathbb{X}\times\mathbb{Z}$. Clearly, $\widetilde{\Xi}=f+g$. From the given assumption and \cite[Proposition 1]{Robinson81}, we deduce that $\partial\widetilde{\Xi}$ is metrically $p$-subregular at any $(x,s,z)\in{\rm dom}\partial\widetilde{\Xi}$ for the origin. By \cite[Proposition 2.2 (i) \& Remark 2.2]{LiuPanWY22}, $\widetilde{\Xi}$ is a KL function of exponent $\frac{1}{2p}$. By using the same arguments as those for \cite[Lemma 1]{LiuPanWY22}, so is the function $\Xi$.
 \end{proof} 	

 For general $F,G$ and convex $\vartheta_1,\vartheta_2$ and $h$, the following proposition provides a condition for the KL property of $\widetilde{\Xi}$ and $\Xi$ with exponent $p\in[\frac{1}{2},1)$ by leveraging that of an almost separable nonsmooth function and the structure of the mappings $F'$ and $G'$.
%------------------------------------------------------------------------------------------------

\begin{proposition}\label{prop-KL}
  Fix any $\overline{\chi}=(\overline{x},\overline{x},\overline{z})\in(\partial\widetilde{\Xi})^{-1}(0)$.
  Suppose that the following function
  \begin{equation}\label{ffun}
    f(y,s,u,z):=\vartheta_1(y)+h(s)+\langle u,z\rangle+\vartheta_2^*(-z)\ \ {\rm for}\  (y,s,u,z)\in\mathbb{Y}\times\mathbb{X}\times\mathbb{Z}\times\mathbb{Z}
  \end{equation}
  has the KL property of exponent $p\in[1/2,1)$ at $(\overline{y},\overline{x},\overline{u},\overline{z})$ with $\overline{y}=F(\overline{x}), \overline{u}=G(\overline{x})$, and
  \begin{equation}\label{key-cond}
  {\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})]\big)\cap{\rm cl}\,{\rm cone}\big[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{u}))\big]=\big\{0\big\}.
  \end{equation}
  Then, under Assumptions \ref{ass0}-\ref{ass1}, the function $\widetilde{\Xi}$ has the KL property of exponent $p$ at $\overline{\chi}$, and so does the function $\Xi$ at  $(\overline{\chi},\overline{\mathcal{Q}})$ for any PD linear mapping $\overline{\mathcal{Q}}\!:\mathbb{X}\to\mathbb{X}$.
 \end{proposition}

 \begin{proof}
  Suppose that $\widetilde{\Xi}$ does not have the KL property of exponent $p$ at $\overline{\chi}$. Then, there exists a sequence $\chi^k:=(x^k,s^k,z^k)\to\overline{\chi}$ with $\widetilde{\Xi}(\overline{\chi})<\widetilde{\Xi}(\chi^k)<\widetilde{\Xi}(\overline{\chi})+\frac{1}{k}$ such that
  \[
   {\rm dist}\big(0,\partial\widetilde{\Xi}(x^k,s^k,z^k)\big)
   <\frac{1}{k}\big(f(y^k,s^k,u^k,z^k)-f(\overline{y},\overline{s},\overline{u},\overline{z})\big)^{p}
   \quad{\rm for\ all}\ k\in\mathbb{N},
  \]
 where $y^k\!:=\ell_{F}(x^k,s^k)$ and $u^k\!:=\ell_{G}(x^k,s^k)$ for each $k\in\mathbb{N}$.
 Obviously, every $(x^k,s^k,z^k)\in{\rm dom}\widetilde{\Xi}$. By combining the last inequality with equation \eqref{subdiff-wXi}, for each $k\in\mathbb{N}$, there exist $v^k\in\partial\vartheta_1(y^k)$, $\xi^k\in\partial h(s^k)$ and $\zeta^k\in\partial\vartheta_2^*(-z^k)$ such that
 \begin{align}\label{ineq1-KL}
  &\bigg\|\left(\begin{matrix}
 [D^2F(x^k)(s^k-x^k,\cdot)]^*v^k+[D^2G(x^k)(s^k-x^k,\cdot)]^*z^k\\
 	\nabla\!F(x^k)v^k+\xi^k+\nabla G(x^k)z^k\\
 	u^k-\zeta^k
 \end{matrix}\right)\bigg\|\nonumber\\
 &<\frac{1}{k}\big(f(y^k,s^k,u^k,z^k)-f(\overline{y},\overline{x},\overline{u},\overline{z})\big)^{p}.
 \end{align}
 Since $f$ has the KL property of exponent $p$ at $(\overline{y},\overline{x},\overline{u},\overline{z})$, there exist $\delta'>0,\eta'>0$ and $c'>0$ such that for all $(y,s,u,z)\in\mathbb{B}((\overline{y},\overline{x},\overline{u},\overline{z}),\delta')\cap[f(\overline{y},\overline{x},\overline{u},\overline{z})<f<f(\overline{y},\overline{x},\overline{u},\overline{z})+\eta']$,
 \begin{equation}\label{vartheta-KL}
  {\rm dist}(0,\partial\!f(y,s,u,z))\ge c'\big(f(y,s,u,z)-f(\overline{y},\overline{x},\overline{u},\overline{z})\big)^{p}.
 \end{equation}
 From $\chi^k\to\overline{\chi}$, we have $x^k\in\mathcal{O}_{x}$ for all sufficiently large $k$, where $\mathcal{O}_x$ is the same as the one in Assumption \ref{ass1} (ii). Thus, by invoking Assumptions \ref{ass0}-\ref{ass1} and recalling that  $\widetilde{\Xi}(\overline{w})<\widetilde{\Xi}(w^k)<\widetilde{\Xi}(\overline{w})+\frac{1}{k}$ for each $k\in\mathbb{N}$, it follows that  $(y^k,s^k,u^k,z^k)\in\mathbb{B}((\overline{y},\overline{x},\overline{u},\overline{z}),\delta')\cap[f(\overline{y},\overline{x},\overline{u},\overline{z})<f<f(\overline{y},\overline{x},\overline{u},\overline{z})+\eta']$ for all sufficiently large $k$, which together with the above \eqref{vartheta-KL} and \eqref{ineq1-KL} implies that for all sufficiently large $k$,
 \begin{align}\label{ineq2-KL}
  &\bigg\|\left(\begin{matrix}
 	[D^2F(x^k)(s^k-x^k,\cdot)]^*v^k+[D^2G(x^k)(s^k-x^k,\cdot)]^*z^k\\
 	 \nabla F(x^k)v^k+\xi^k+\nabla G(x^k)z^k\\
 	 u^k-\zeta^k
 	\end{matrix}\right)\bigg\|\nonumber\\
  &<\frac{1}{kc'}{\rm dist}\big(0,\partial\!f(y^k,s^k,u^k,z^k)\big)
 	\le\frac{1}{kc'}\|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|,
  \end{align}
  where the last inequality is due to the expression of $f$ and the fact that $v^k\in\partial\vartheta_1(y^k)$, $\xi^k\in\partial h(s^k)$ and $\zeta^k\in\partial\vartheta_2^*(-z^k)$ for each $k\in\mathbb{N}$. Note that for all sufficiently large $k$, $(v^k,\xi^k,z^k,u^k\!-\zeta^k)\in\partial\!f(y^k,s^k,u^k,z^k)$, which along with \eqref{vartheta-KL} implies that
  \[
  \|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|\ge \!c'\big(f(y^k,s^k,u^k,z^k)-f(\overline{y},\overline{x},\overline{u},\overline{z})\big)^{p}>0.
  \]
  For each sufficiently large $k$, we write $(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k)
  \!:=\frac{(v^k,\xi^k,z^k,u^k\!-\zeta^k)}{\|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|}$. Then, from \eqref{ineq2-KL} it follows that for all sufficiently large $k$,
   \begin{equation}\label{ineq3-KL}
  \bigg\|\left(\begin{matrix}
  	[D^2F(x^k)(s^k-x^k,\cdot)]^*\widetilde{v}^k+[D^2G(x^k)(s^k-x^k,\cdot)]^*\widetilde{z}^k\\
  		\nabla\!F(x^k)\widetilde{v}^k+\widetilde{\xi}^k+\nabla G(x^k)\widetilde{z}^k\\
  		\widetilde{\eta}^k
  	\end{matrix}\right)\bigg\|\le\frac{1}{kc'}.
  \end{equation}
  By the boundedness of $\{(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k)\}_{k\in\mathbb{N}}$, there exists an index set $K\subset\mathbb{N}$ such that  $\{(\widetilde{v}^k;\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k)\}_{k\in K}$ is convergent and its limit, say $(\widetilde{v},\widetilde{\xi},\widetilde{z},\widetilde{\eta})$, satisfies $\|(\widetilde{v},\widetilde{\xi},\widetilde{z},\widetilde{\eta})\|=1$.

  We next argue that the sequence $\{(\xi^k,u^k\!-\!\zeta^k)\}_{k\in\mathbb{N}}$ is necessarily bounded. Suppose on the contradiction that  $\{(\xi^k,u^k\!-\zeta^k)\}_{k\in\mathbb{N}}$ is unbounded. Note that $\{(v^k,z^k)\}_{k\in\mathbb{N}}$ is bounded. Then, from the unboundedness of $\{(\xi^k,u^k-\zeta^k)\}_{k\in\mathbb{N}}$, we deduce that $\widetilde{v}=0$, $\widetilde{z}=0$ and
  $\|(\widetilde{\xi},\widetilde{\eta})\|=1$. On the other hand, passing the limit $k\to\infty$ to inequality \eqref{ineq3-KL} and using $\widetilde{v}=0,\widetilde{z}=0$ leads to $\widetilde{\xi}=\widetilde{\eta}=0$, which is a contradiction to $\|(\widetilde{\xi},\widetilde{\eta})\|=1$. Thus,  $\{(\xi^k,u^k-\!\zeta^k)\}_{k\in\mathbb{N}}$ is bounded, which by $u^k\to G(\overline{x})$ as $k\to\infty$ implies that $\{\zeta^k\}_{k\in\mathbb{N}}$ is bounded. If necessary by taking a subsequence, we assume that $\lim_{K\ni k\to\infty}\zeta^k=\overline{\zeta}$. From the expression of $(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k)$ and $z^k\in-\partial\vartheta_2(\zeta^k)$,  for each sufficiently large $k$,
  \[
  (\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k)\in{\rm cone}\big[\partial\vartheta_1(y^k)\times\partial h(s^k)\times(-\partial\vartheta_2(\zeta^k))].
  \]
  Together with the outer semicontinuity of $\partial\vartheta_1,\partial h$ and $\partial\vartheta_2$, it then follows that
  \[
  (\widetilde{v},\widetilde{\xi},\widetilde{z})\in{\rm cl}\,{\rm cone}[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{\zeta}))].
  \]
  On the other hand, passing the limit $K\ni k\to\infty$ to inequality \eqref{ineq3-KL}, we obtain that
  \[
   \nabla\!F(\overline{x})\widetilde{v}+\widetilde{\xi}+\nabla\!G(\overline{x})\widetilde{z}=0
   \ \ {\rm and}\ \ \widetilde{\eta}=0.
  \]
  By using $\widetilde{\eta}=0$ and the expression of $\widetilde{\eta}^k$ and recalling that $u^k\to G(\overline{x})$ as $k\to\infty$, it is not hard to deduce that $\overline{\zeta}=G(\overline{x})$. Then, from the last two equations, we have
  \[
   0\ne(\widetilde{v},\widetilde{\xi},\widetilde{z})\in{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})]\big)\cap{\rm cl}\,{\rm cone}\big[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{u})],
  \]
   which contradicts the assumption in \eqref{key-cond}. Therefore, $\widetilde{\Xi}$ has the KL property of exponent $p$ at $\overline{\chi}$. Since $(\overline{\chi},\overline{\mathcal{Q}})$ is a critical point of $\Xi$,  using the same arguments as those for \cite[Lemma 1]{LiuPanWY22} can justify that $\Xi$ has the KL property of exponent $p$ at $(\overline{\chi},\overline{\mathcal{Q}})$.
  \end{proof}
 \begin{remark}\label{remark42-KL}
  {\bf(a)} Let $\psi(u,z)\!:=\!\langle u,z\rangle+\vartheta_2^*(-z)$ for $(u,z)\in\mathbb{Z}\times\mathbb{Z}$. If $\psi,\vartheta_1$ and $h$ are KL functions of exponent $p\in(0,1)$, then the function $f$ defined in \eqref{ffun} is a KL function of exponent $p\in(0,1)$. By \cite[Proposition 2.2 (i) $\&$ Remark 2.2]{LiuPanWY22}, $\psi$ is a KL function of exponent $p\in(0,1)$ if $\vartheta_2^*$ is a KL function and $\partial\vartheta_2^*$ is metrically $\max(\frac{1}{2p},\frac{1}{p}\!-\!1)$-subregular. Since $\vartheta_1$ and $h$ are convex functions, their KL property of exponent $p\in(0,1)$ is implied by the metric $(\frac{1}{p}\!-\!1)$-subregularity of their subdifferential mappings by combining \cite[Theorem 3.4]{Mordu15} and \cite[Theorem 5 (ii)]{Bolte17}. Together with  \cite[Proposition 1]{Robinson81}, $\vartheta_1$ and $h$ are KL functions of exponent $p\in[1/2,1)$ if they are piecewise linear-quadratic convex.

  \noindent
  {\bf(b)} It is worth to point out that condition \eqref{key-cond} is point-based and easily verifiable, and the conclusions of Proposition \ref{prop-KL} do not require the convexity of $\vartheta_1$ and $h$.
  \end{remark}
%----------------------------------------------------------------------------------------------------
 \begin{corollary}\label{corollary-KL}
  Consider problem \eqref{prob} with $\vartheta_2\equiv 0$. Fix any $(\overline{x},\overline{x})\in(\partial\widetilde{\Xi})^{-1}(0)$ with $\widetilde{\Xi}(x,s)=\vartheta_1(\ell_{F}(x,s))+h(s)$ for $(x,s)\in\mathbb{X}\times\mathbb{X}$.
   Suppose that the following function
   \begin{equation}\label{ffun1}
   	f(y,s):=\vartheta_1(y)+h(s)\ \ {\rm for}\  (y,s)\in\mathbb{Y}\times\mathbb{X}
   \end{equation}
   has the KL property of exponent $p\in[1/2,1)$ at $(\overline{y},\overline{x})$ with $\overline{y}=F(\overline{x})$, and
   \begin{equation}\label{key-cond1}
   	{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap{\rm cl}\,{\rm cone}\big(\partial\! f(\overline{y},\overline{x})\big)=\big\{0\big\}.
   \end{equation}
   Then, under Assumptions \ref{ass0}-\ref{ass1}, the function $\widetilde{\Xi}$ has the KL property of exponent $p$ at $(\overline{x},\overline{x})$, and the corresponding function $\Xi$ 
    has the KL property of exponent $\max(p,\frac{1}{2})$ at $(\overline{x},\overline{x},\overline{\mathcal{Q}})$.
  \end{corollary}
 
 \begin{remark}\label{superlinear}
  Consider that $\vartheta_2\equiv 0$ and $h\!\equiv 0$. Fix any $\overline{x}\in(\partial\Theta_1)^{-1}(0)$. By combining \cite[Theorem 5 (ii)]{Bolte17} with \cite[Theorem 3.3]{Artacho08}, the KL property of $\vartheta_1$ with exponent $1/2$ at $\overline{y}$ is equivalent to the metric subregularity of $\partial\vartheta_1$ at $\overline{y}$ for the origin. By Corollary \ref{corollary-KL} and Theorem \ref{convergece-rate} (i), now $\{x^k\}_{k\in\mathbb{N}}$ converges to $x^*$ with a R-linear rate for $p=1/2$. Such a condition is equivalent to requiring $C$ to be the set of local weak sharp minima of order 2 for $\vartheta_1$, which was used in \cite[Theorem 20]{HuYang16} to achieve a global R-linear rate.
  \end{remark}
  
  Condition \eqref{key-cond1} is stronger than ${\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap\partial^{\infty}\!f(\overline{y},\overline{x})=\big\{0\big\}$, which is precisely the Robinson's CQ for constraint system
  $(F(x),s)\in{\rm dom}f$ at $(\overline{x},\overline{x})$, because ${\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x}))\supset\partial^{\infty}f(\overline{y},\overline{x})$ and this inclusion is generally strict. Indeed, by the convexity of $f$, when $(0,0)\in\partial\!f(\overline{y},\overline{x})$, ${\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x}))\supset[\partial\!f (\overline{y},\overline{x})]^{\infty}=\partial^{\infty}\!f(\overline{y},\overline{x})$, and when $(0,0)\notin\partial\!f(\overline{y},\overline{x})$, by \cite[Theorem 23.7]{Roc70}, ${\rm cl}\,{\rm cone}(\partial f(\overline{y},\overline{x}))=\mathcal{N}_{D}(\overline{y},\overline{x})$ with $D\!=\{(y,x)\in\mathbb{Y}\times\mathbb{X}\,|\,f(y,x)\le f(\overline{y},\overline{x})\}$, which together with $D\subset{\rm dom}f$ implies that ${\rm cl}\,{\rm cone}(\partial f(\overline{y},\overline{x}))\supset\mathcal{N}_{{\rm dom}f}(\overline{y},\overline{x})=\partial^{\infty}f(\overline{y},\overline{x})$. Thus, the stated implication holds.

  To close this section, we discuss the relation between condition \eqref{key-cond1} and the regularity used in \cite{HuYang16} for problem \eqref{prob} with $\vartheta_2\equiv 0$.
  For this purpose, for a closed convex set $S\subset\mathbb{Y}$, let $S^{\ominus}:=\{y^*\in\mathbb{Y}\,|\,\langle y^*,y\rangle\le 0\ {\rm for\ all}\ y\in S\}$ represent the negative polar of $S$.
%---------------------------------------------------------------------------------------------------
  \begin{remark}\label{remark-relation}
  {\bf(a)} Let $D\!:=\mathop{\arg\min}_{x\in\mathbb{X}}h(x)$ and recall that $C=\mathop{\arg\min}_{y\in\mathbb{Y}}\vartheta_1(y)$. Then, ${\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x})) \subset[C\times D-(\overline{y},\overline{x})]^{\ominus}$. Indeed, pick any $(u,v)\in{\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x}))$. Let $\{(u^k,v^k)\}_{k\in\mathbb{N}}\subset{\rm cone}(\partial\!f(\overline{y},\overline{x}))$ be such that $(u^k,v^k)\to (u,v)$. For each $k$, there exist $t_k\ge 0$ and $(\xi^k,\zeta^k)\in\partial\!f(\overline{y},\overline{x})$ such that $(u^k,v^k)=t_k(\xi^k,\zeta^k)$. Since $(\xi^k,\zeta^k)\in \partial\!f(\overline{y},\overline{x})$,
  \[
   0\ge f(y,x)-f(\overline{y},\overline{x})\ge\langle(\xi^k,\zeta^k),(y,x)-(\overline{y},\overline{x})\rangle
   \quad\ \forall (y,x)\in C\times D,
  \]
   which implies that $(u^k,v^k)\in[C\times D-(\overline{y},\overline{x})]^{\ominus}$ and $(u,v)\in[C\times D-(\overline{y},\overline{x})]^{\ominus}$ by the closedness of the set $[C\times D-(\overline{y},\overline{x})]^{\ominus}$.
  The stated inclusion then follows. In particular, this inclusion is generally strict; for example, consider $\vartheta_1(y)=\|y\|_1$ for $y\in\mathbb{R}^2$ and $F(x)=(x,-x)^{\top}$ for $x\in\mathbb{R}$. For $\overline{x}=1$, we have ${\rm cl}[{\rm cone}(\partial\vartheta_1(F(\overline{x})))]=\mathbb{R}_{+}\times\mathbb{R}_{-}$, but $[C-F(\overline{x})]^{\ominus}=\mathbb{R}^2$. This shows that condition \eqref{key-cond1} is weaker than the following one
  \[
   	{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap\big([C-F(\overline{x})]^{\ominus}\times[D-\overline{x}]^{\ominus}\big)=\big\{0\big\},
  \]
  which is precisely the regularity condition used in \cite[Theorem 20]{HuYang16} and \cite[Section 3]{Burke95}.

  \noindent
  {\bf(b)} Suppose in addition that $h\equiv 0$. From \cite[Definition 7 (c)]{HuYang16}, a vector $\overline{x}\in\mathbb{X}$ is called a quasi-regular point of the inclusion $F(x)\in C$ if there exist $r>0$ and $\beta_{r}>0$ such that
  \[
    {\rm dist}(0,D(x))\le\beta_{r}{\rm dist}(F(x),C)\quad{\rm for\ all}\ x\in\mathbb{B}(\overline{x},r),
  \]
  where $D(x)\!:=\{d\in\mathbb{X}\,|\,F(x)+F'(x)d=0\}$. When $F$ is polyhedral and $\vartheta_1$ is piecewise linear-quadratic, the function $\Theta_1$ is a KL function of exponent $1/2$ by Proposition \ref{prop-KL0}; while by Hoffman's lemma the above quasi-regularity condition also holds if $\vartheta_1$ is polyhedral. When $F$ is non-polyhedral or $\vartheta_1$ is not piecewise linear-quadratic,
  there are examples for which condition \eqref{key-cond1} holds but the quasi-regularity condition does not hold; for instance, $F(x)=(x^2;-x)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=\|y\|^2$ for $y\in\mathbb{R}^2$. It is easy to check that condition \eqref{key-cond1} holds at  $\overline{x}=0$, which along with the strong convexity of $\vartheta_1$ shows that $\widetilde{\Xi}$ and $\Theta_1$ are the KL functions of exponent $1/2$, but $\overline{x}$ is not a quasi-regular point of the inclusion $F(x)\in C$ because $D(x)=\{d\in\mathbb{R}\,|\,(x^2+2xd;-x-d)=0\}=\emptyset$ for all $x\ne 0$ sufficiently close to $\overline{x}$. In addition, there are also examples for which the quasi-regularity condition holds but condition \eqref{key-cond1} does not hold; for instance,
  $F(x)=(x;0)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=y_1^4+|y_2|$ for $y\in\mathbb{R}^2$.
  One can check that condition \eqref{key-cond1} does not hold at $\overline{x}=0$, while for any $x\in\mathbb{B}(\overline{x},1)$, ${\rm dist}(0,D(x))\le{\rm dist}(F(x),C)$ by noting that
  $D(x)=\{d\in\mathbb{R}\,|\,x+d=0\}$. However, $\Theta_1$ is still a KL function of exponent $1/2$ for this example. Now it is unclear whether there is an example for which the quasi-regularity holds but $\Theta_1$ is not a KL function of exponent $1/2$.
 \end{remark}
%------------------------------------------------------------------------------------------------
 \section{Numerical experiments}\label{sec6}

  Before testing the performance of Algorithm \ref{iLPA}, we first develop a solver to compute their subproblems. This solver, named dPPASN, is a proximal point algorithm to solve their dual problems with each proximal subproblem solved by a semismooth Newton method.
  To this end, for a proper closed convex $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ and a constant $\gamma>0$, $P_{\gamma}f$ and $e_{\gamma}f$ denotes the proximal mapping and the Moreau envelope of $f$ associated to $\gamma$. For each $k\in\mathbb{N}$ and $j\in[j_k]$, let
  $\mathcal{A}_k\!:=F'(x^k),u^k\!:=\nabla G(x^k)\xi^k,c^k\!:=F(x^k),b^k\!:=\mathcal{A}_kx^k-c^k$ and take $\mathcal{Q}_{k,j}\!:=\gamma_{k,j}\mathcal{I}+\alpha_k\mathcal{A}_k^*\mathcal{A}_k$, where  $\alpha_k>0$ is specified in the later experiments.
%----------------------------------------------------------------------------------------------------
 \subsection{Dual PPA armed with SNCG for subproblems}\label{sec5.1}

 With the above notation, the previous subproblem \eqref{subprobkj} can be equivalently written as
 \begin{align}\label{Esubprobkj}
 	&\min_{x\in\mathbb{X},y\in\mathbb{Y}}\!\vartheta_1(y)+\langle u^k,x\rangle+h(x)+\frac{\gamma_{k,j}}{2}\|x-x^k\|^2+\frac{\alpha_k}{2}\|y-c^k\|^2-\Theta_2(x^k) \nonumber\\
 	&\quad{\rm s.t.}\ \ \mathcal{A}_kx-b^k=y.
 \end{align}
 After an elementary calculation, the dual of problem \eqref{Esubprobkj} takes the following form
 \begin{equation}\label{dEsubprobkj}
 	\min_{\zeta\in\mathbb{Y}}\Psi_k(\zeta):= \frac{\|\zeta\|^2}{2\alpha_{k}}-e_{\alpha_k^{-1}}\vartheta_1\big(\alpha_{k}^{-1}\zeta+c^k\big)+\frac{\|\mathcal{A}_k^*\zeta\!+\!u^k\|^2}{2\gamma_{k,j}} -e_{\gamma_{k,j}^{-1}}h\big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+\!u^k)\big)-C_k
 \end{equation}
 with $C_k=\langle u^k,x^k\rangle+\Theta_2(x^k)$. Clearly, the strong duality holds for  \eqref{Esubprobkj} and \eqref{dEsubprobkj}.
 Since $\Psi_k$ is a smooth convex function with Lipschitz continuous gradient, seeking an optimal solution of \eqref{dEsubprobkj} is equivalent to finding a root $\zeta^*$ of system $\nabla\Psi_k(\zeta)=0$. With such $\zeta^*$, one can recover the unique optimal solution $(\overline{x}^{k,j},\overline{y}^{k,j})$ of \eqref{Esubprobkj} or subproblem \eqref{subprobkj} via
 \[
 \overline{x}^{k,j}\!=\!P_{\gamma_{k,j}^{-1}}h(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{*}\!+\!u^k))
 \ \ {\rm and}\ \ \overline{y}^{k,j}\!=\!P_{\alpha_k^{-1}}\vartheta_1(\alpha_k^{-1}\zeta^{*}\!+c^k).
 \]

 The system $\nabla\Psi_k(\zeta)=0$ is semismooth by \cite{Meng05}, but a direct application of the semismooth Newton method to it faces the difficulty caused by the potential singularity of the generalized Hessian of $\Psi_k$. Inspired by this, we apply the inexact PPA armed with the semismooth Newton method to solving \eqref{dEsubprobkj}, whose iterate steps are described as follows.
 %-------------------------------------------------------------------------------------
 \begin{algorithm}[h]
 	\renewcommand{\thealgorithm}{A}
 	\caption{\label{PPASN}{\bf\ Inexact PPA with semismooth Newton (dPPASN)}}
 	\textbf{Initialization:} Fix $k\in\mathbb{N}$ and $j\in[j_k]$.
 	Choose $\underline{\tau}>0,\tau_0>0$ and $\zeta^0\in\mathbb{Y}$. Set $l=0$.
 	\textbf{while} the stopping conditions are not satisfied \textbf{do}
 	\begin{enumerate}
 		\item Seek an inexact minimizer $\zeta^{l+1}$ of the following problem with Algorithm \ref{SNCG} later:
 		\begin{equation}\label{PPA-subprob}
 			\min_{\zeta\in\mathbb{Y}}\Psi_{k,l}(\zeta):=\Psi_{k}(\zeta)+\frac{\tau_{l}}{2}\|\zeta-\zeta^{l}\|^2.
 		\end{equation}
 		
 		\item Update $\tau_{l+1}\downarrow\underline{\tau}$. Let $l\leftarrow l+1$, and then go to Step 1.
 	\end{enumerate}
 	\textbf{End(while)}
 \end{algorithm}

 Inspired by the inexact criterion in Step (2a) of Algorithm \ref{iLPA}, in the implementation of Algorithm \ref{iLPA}, we terminate Algorithm \ref{PPASN} at $\zeta^{l}$ when $(\zeta^{l},x^{k,j})$ satisfies the relation
 \[
 q_{k,j}(x^{k,j})+\Psi_{k}(\zeta^{l})<({\underline{\gamma}}/2)\|x^{k,j}\!-\!x^k\|^2
 \ \ {\rm for}\ \ x^{k,j}\!=\!P_{\gamma_{k,j}^{-1}}h(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{l}\!+u^k)).
 \]
 This has a little difference from the one in Step (2a) of Algorithm \ref{iLPA} by considering that $-\Psi_{k}(\zeta^{l})$ is only a lower estimation for the optimal value $q_{k,j}(\overline{x}^{k,j})$ of subproblem \eqref{subprobkj}.
 For the convergence analysis of Algorithm \ref{PPASN}, the reader refers to \cite{Roc76,Roc21}.

 Note that $\Psi_{k,l}$ has a Lipschitz continuous gradient mapping. We define its generalized Hessian at $\zeta$ the Clarke Jacobian of $\nabla\Psi_{k,l}$ at $\zeta$, i.e., $\partial^2\Psi_{k,l}(\zeta)\!:=\partial_{C}(\nabla\Psi_{k,l})(\zeta)$. From \cite[Page 75]{Clarke83}, it follows that $\partial^2\Psi_{k,l}(\zeta)d=\widehat{\partial}^2\Psi_{k,l}(\zeta)d$
 for all $d\in\mathbb{Y}$ with
 \[
 \widehat{\partial}^2\Psi_{k,l}(\zeta)
 \!=\!\tau_{l}\mathcal{I}+\alpha_k^{-1}\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta+c^k\big)
 +\gamma_{k,j}^{-1}\mathcal{A}_k\partial_{C}P_{\!\gamma_{k,j}^{-1}}h
 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)\mathcal{A}_k^{*}.
 \]
 By mimicking the proof in \cite[Section 3.3.4]{Ortega70}, every $\mathcal{U}\!\in\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta+c^k\big)$ and every $\mathcal{V}\!\in\partial_{C}P_{\!\gamma_{k,j}^{-1}}h
 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)$ are symmetric and positive semidefinite. Along with $\tau_l>0$ for each $l\in\mathbb{N}$, the operator $\tau_lI+\alpha_{k}^{-1}\mathcal{U}+\gamma_{k,j}^{-1}\mathcal{A}_k\mathcal{V}\mathcal{A}_k^*$ is positive definite, so every element in $\widehat{\partial}^2\Psi_{k,l}(\zeta)$ is nonsingular. Thus, the following semismooth Newton method is well defined, whose convergence analysis can be found in \cite{QiSun93,ZhaoST10}.
 %-------------------------------------------------------------------------------------
 \begin{algorithm}[h]
 	\renewcommand{\thealgorithm}{A.1}
 	\caption{\label{SNCG}{\bf\ Semismooth Newton method}}
 	\textbf{Initialization:} Fix $k\in\mathbb{N},j\in[j_k]$ and $l\in\mathbb{N}$. Choose  $\underline{\eta}\!\in(0,1),\beta\in(0,1),\varsigma\in(0,1]$ and $
 	0<c_1<c_2<\frac{1}{2}$. Set $\nu:=0$ and let $\zeta^0=\zeta^{l}$.
 	
 	\textbf{while} the stopping conditions are not satisfied \textbf{do}
 	\begin{enumerate}
 		\item Choose  $\mathcal{U}^{\nu}\in\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta^{\nu}+c^k\big)$
 		and $\mathcal{V}^{\nu}\in\partial_{C}P_{\!\gamma_{k,j}^{-1}}h 		 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta^{\nu}+u^k)\big)$.
 		
 		\item Solve the following linear system via the conjugate gradient (CG) method
 		\[
 		\mathcal{W}^{\nu}d=-\nabla\Psi_{k,l}(\zeta^{\nu})\ \ {\rm with}\ \
 		\mathcal{W}^{\nu}=\tau_l\mathcal{I}+\alpha_{k}^{-1}\mathcal{U}^{\nu}+\gamma_{k,j}^{-1}\mathcal{A}_k\mathcal{V}^{\nu}\mathcal{A}_k^*,
 		\]
 		\ \ to find $d^{\nu}$ such that  $\|\mathcal{W}^{\nu}d^{\nu}+\nabla\Psi_{k,l}(\zeta^{\nu})\|\le\min(\underline{\eta},\|\nabla\Psi_{k,l}(\zeta^{\nu})\|^{1+\varsigma})$.
 		
 		\item Let $m_{\nu}$ be the smallest nonnegative integer $m$ such that
 		\begin{align*}
 			\Psi_{k,l}(\zeta^{\nu}\!+\!\beta^md^{\nu})\leq\Psi_{k,l}(\zeta^{\nu})+c_1\beta^m\langle\nabla\Psi_{k,l}(\zeta^{\nu}),d^{\nu}\rangle\\
 			\vert\langle\nabla\Psi_{k,l}(\zeta^{\nu}\!+\!\beta^md^{\nu}),d^{\nu}\rangle\vert\le c_2\vert\langle\nabla\Psi_{k,l}(\zeta^{\nu}),d^{\nu}\rangle\vert.
 		\end{align*}
 		
 		\item Set $\zeta^{\nu+1}=\zeta^{\nu}+\beta^{m_{\nu}}d^{\nu}$. Let $\nu\leftarrow \nu+1$, and then go to Step 1.
 	\end{enumerate}
 	\textbf{End(while)}
 \end{algorithm}

 In the subsequent sections, we apply Algorithm \ref{iLPA} armed with dPPASN (named iLPA) to model \eqref{SCAD-loss} from matrix completions with outliers and DC programs with nonsmooth components.  All tests are performed in MATLAB on a laptop running on 64-bit Windows System with an Intel Xeon(R) i7-12700H CPU 2.30GHz and 32 GB RAM.
%-----------------------------------------------------------------------------------------------
\subsection{Model \eqref{SCAD-loss} from matrix completions with outliers}\label{sec5.3}

 We consider model \eqref{SCAD-loss} with $\vartheta\equiv\vartheta_1-\vartheta_2$ where  $\vartheta_1(y)\!:=\|y\|_1$ and $\vartheta_2(z)\!:=\frac{1}{\rho}\sum_{i=1}^{m}\!\theta_{a}(\rho|z_i|)$ for $y,z\in\mathbb{R}^m$. Here, $\rho>0$ is a given parameter and $\theta_{a}\!:\mathbb{R}\to\mathbb{R}$ is the function given by
 \[
  \theta_{a}(s)=\left\{\begin{array}{cl}
	0 & \textrm{if}\ s\leq \frac{2}{a+1};\\
	\frac{((a+1)s-2)^2}{4(a^2-1)} & \textrm{if}\ \frac{2}{a+1}<s\leq \frac{2a}{a+1};\\
	s-1 & \textrm{if}\ s>\frac{2a}{a+1}.
 \end{array}\right.\quad{\rm with}\ \ a>1.
 \]
 From \cite[Example 3]{ZhangPan22}, $\lambda\theta_{a}$ with $\lambda>0, \rho=\frac{2}{(a+1)\lambda}$ is exactly the SCAD function $p_{\lambda}$ in \cite{Fan01}. Obviously, problem \eqref{SCAD-loss} takes the form of \eqref{prob} with $\mathbb{X}=\mathbb{R}^{n_1\times r}\times\mathbb{R}^{n_2\times r},\mathbb{Y}=\mathbb{Z}=\mathbb{R}^m$, $F(x)=G(x)=\mathcal{A}(UV^{\top})-b$ and $ h(x)=\lambda(\|U\|_{2,1}\!+\!\|V\|_{2,1})$ for $x=(U,V)\in\mathbb{X}$.
 By the smoothness of $\vartheta_2$, the vector $u^k$ in section \ref{sec5.1} becomes $-\mathcal{A}_k^*(\nabla\vartheta_2(F(W^k)))$ with
 \[
   \nabla\vartheta_2(F(x^k))=\min\Big[1,\max\Big(0,\!\frac{(a+1)\rho|F(x^k)|-2a}{2(a\!-\!1)}\Big)\Big].
 \]
 Inspired by the numerical tests in \cite{ZhangPan22}, we choose $a=4$ and $\rho=0.1$ for the experiments.

 To formulate the sampling operator $\mathcal{A}\!:\mathbb{R}^{n_1\times n_2}\to\mathbb{R}^m$, we assume that a random index set $\Omega=\!\big\{(i_t,j_t)\!:\ t=1,\ldots,m\big\}\subset([n_1]\times[n_2])^m$ is available, and the samples of the indices are drawn independently from a general sampling distribution $\Pi\!=\{\pi_{kl}\}_{k\in[n_1],l\in[n_2]}$ on $[n_1]\times[n_2]$. Note that we consider the sampling scheme with replacement. We adopt the same non-uniform sampling scheme as in \cite{Fang18}, i.e., take $\pi_{kl}=p_kp_l$ for each $(k,l)$ with
 \begin{equation}\label{sampling-scheme}
  (S1)\ \
  p_k=\!\left\{\begin{array}{ll}
 		2p_0& {\rm if}\ k\le\frac{n_1}{10} \\
 		4p_0& {\rm if}\ \frac{n_1}{10}\le k\le \frac{n_1}{5}\\
 		p_0& {\rm otherwise}\\
 	\end{array}\right.\ \ {\rm or}\ \
 	(S2)\ \ p_k=\!
 	\left\{\begin{array}{ll}
 		3p_0& {\rm if}\ k\le\frac{n_1}{10} \\
 		9p_0& {\rm if}\ \frac{n_1}{10}\le k\le \frac{n_1}{5}\\
 		p_0& {\rm otherwise}\\
 	\end{array}\right.
 \end{equation}
 where $p_0>0$ is a constant such that $\sum_{k=1}^{n_1}p_k=1$ or $\sum_{l=1}^{n_2}p_l=1$. Then, it holds that
 \[
 \mathcal{A}(X):=(X_{i_1,j_1},X_{i_2,j_2},\ldots,X_{i_m,j_m})^{\top}\quad{\rm for}\ X\in\mathbb{R}^{n_1\times n_2}.
 \]
 Let $b=\mathcal{A}(M_{\Omega})$ where $M_{\Omega}$ is an $n_1\times n_2$ matrix with $[M_{\Omega}]_{i_tj_t}=0$ if $(i_t,j_t)\notin\Omega$ and
 \begin{equation}\label{observe}
  [M_{\Omega}]_{i_t,j_t}=M_{i_t,j_t}^*+\varpi_t\ \ {\rm for}\ \ t=1,2,\ldots,m,
 \end{equation}
 where $M^*\!\in\mathbb{R}^{n_1\times n_2}$ represents the true matrix of rank $r^*$ for synthetic data and for real data a matrix drawn from the original incomplete data matrix, and $\varpi=(\varpi_1,\ldots,\varpi_m)^{\top}$ is a sparse noisy vector. The nonzero entries of $\varpi$ obey the following several kinds of distributions: {\bf(I)} $N(0,10^2)$; {\bf(II)} Student's t-distribution with $4$ degrees of freedom $2\times t_4$; {\bf(III)} Cauchy distribution with density $d(u)=\frac{1}{\pi(1+u^2)}$; {\bf(IV)} mixture normal distribution $N(0,\sigma^2)$ with $\sigma\sim {\rm Unif}(1,5)$; {\bf(V)} Laplace distribution with density $d(u)=0.5\exp(-|u|)$.

 For the subsequent tests, we always choose $r=\min(100,\lfloor\frac{\min(m,n)}{2}\rfloor)$ and
 $\lambda=c_{\lambda}\|b\|$ for model \eqref{SCAD-loss}, where the constant $c_\lambda>0$ is specified in the corresponding experiments. The parameters of Algorithm \ref{iLPA} are chosen as follows:
 \[
 \varrho = 2,\,\underline{\gamma}=\max(10,\lceil{\max(m,n)}/{100}\rceil),\,\overline{\gamma}=10^6,\,\, \gamma_{k,0}=\underline{\gamma},
 \]
 and the parameter $\alpha_k$ involved in $\mathcal{Q}_{k,j}=\gamma_{k,j}\mathcal{I}+\alpha_k\nabla\!F(x^k)F'(x^k)$ is updated by
 \[
 \alpha_{k}=\max(\alpha_{k-1}/1.2,10^{-3})\ \ {\rm with}\ \  \alpha_0=0.5
 \ \ {\rm when}\ \textrm{mod}(k,3)=0.
 \]
 By Remark \ref{remark-alg} (iii), we terminate Algorithm \ref{iLPA} at $x^k$ under one of the following conditions:
 \begin{equation}\label{stop-cond}
 \frac{\|x^k-x^{k-1}\|_F}{1+\|b\|}\le \epsilon_1\ \ {\rm or}\ \
 \frac{\max_{j\in[9]}|\Phi(x^k)-\Phi(x^{k-j})|}{\max(1,\Phi(x^k))}\le \epsilon_2\ {\rm for}\ k\ge\overline{k}\ \ {\rm or}\ \
 k>2000,
 \end{equation}
 where $\epsilon_1=5\times 10^{-6},\epsilon_2=5\times 10^{-4}$ for synthetic data, and $\epsilon_1= 10^{-4},\epsilon_1=5\times 10^{-3}$ for real data. We compare the performance of iLPA with that of the Polyak subgradient method (see \cite{Charisopoulos21,LiZhu20}) for solving \eqref{SCAD-loss}, whose iterate steps are described as follows:
  %-------------------------------------------------------------------------------------
 \begin{algorithm}[H]
 \renewcommand{\thealgorithm}{2} 	
 \caption{\label{subGrad}{\bf(subGM)\ Subgradient method for model \eqref{SCAD-loss}}}
 	\textbf{Initialization:} Choose an initial point $x^0$.
 	
 	\textbf{For} $k=0,1,2,\cdots$ \textbf{do}
 	\begin{enumerate}
 	\item Compute a subgradient $\zeta^k\in\partial\Phi(x^k)$.
 	
 	\item Set $x^{k+1}=x^k-\frac{\Phi(x^k)-\min_{x\in\mathbb{X}}\Phi(x)}{\|\zeta^k\|^2}\zeta^k$.
 	\end{enumerate}
 	\textbf{End(For)}
 \end{algorithm}
 Since $\min_{x\in\mathbb{X}}\Phi(x)$ is unavailable, we replace the step-size $\frac{\Phi(x^k)-\min_{x\in\mathbb{X}}\Phi(x)}{\|\zeta^k\|^2}$ with the approximate one $\frac{0.05\Phi(x^k)}{\|\zeta^k\|^2}$ in computation. Although there is no convergence certificate for Algorithm \ref{subGrad}, we here use it just for numerical comparisions, and as will be illustrated later, it has a desirable performance with such a step-size. For the fairness of comparisons, we adopt the same starting point $x^0$ as the one used for iLPA, and the same stopping condition as the one used for iLPA, i.e., one of the conditions in \eqref{stop-cond} except that $\overline{k}=10$ is used for iLPA, while $\overline{k}=500$ is used for subGM.
%--------------------------------------------------------------------------------------------------------
\subsubsection{Numerical results for synthetic data}\label{sec5.2.1}

 We generate the true $M^*\!\in\mathbb{R}^{n_1\times n_2}$ randomly by $M^*=M_{L}^*(M_{R}^*)^{\top}$, where the entries of $M_{L}^*\in\mathbb{R}^{n_1\times r^*}$ and $M_{R}^*\in\mathbb{R}^{n_2\times r^*}$ are sampled independently from the standard normal distribution $N(0,1)$, and set the sparsity of the noise vector $\varpi$ as $\lfloor0.3m\rfloor$. We choose $x^0=(U_{1}\Sigma_{r}^{1/2},V_{1}\Sigma_{r}^{1/2})$ as the starting point of iLPA and subGM, where $U_{1}$ and $V_1$ are the matrix consisting of the first $r$ largest left and
 right singular vectors of $M_{\Omega}$, respectively, and $\Sigma_{r}$ is the diagonal matrix consisting of the first $r$ largest singular values of $M_{\Omega}$ arranged in an nonincreasing order. We adopt the relative error (RE) to evaluate the effect of matrix recovery, defined by $\frac{\|x^{\rm out}-M^*\|_F}{\|M^*\|_F}$ where $x^{\rm out}$ means the output of a solver.

 We first check how the relative errors yielded by two solvers vary with $c_{\lambda}$ or $\lambda$.  Figure \ref{fig0} shows that for iLPA and subGM, there exists an interval of $\lambda$ such that the stationary points yielded by them have satisfactory relative errors and their ranks coincide with the true rank $r^*$, but such an interval of $\lambda$ for iLPA is clearly larger than the one for subGM. We next take a look at how the relative errors yielded by iLPA vary with the sparsity of $\varpi$ and the CPU time required by iLPA vary with the dimension $n_1=n_2$ under a fixed $\lambda=0.06\|b\|$. Figure \ref{fig1} (a) indicates that the relative error becomes higher as the sparsity of $\varpi$ decreases, and Figure \ref{fig1} (b) indicates that the CPU time required by iLPA increases mildly as the dimension $n_1=n_2$ becomes larger. All curves in Figures \ref{fig0}-\ref{fig1} are the average results obtained by running $10$ examples generated randomly with noise of type V, $n_1=n_2=1000,r^*=5$ and sampling ratio $\textrm{SR}=0.25$.
%-----------------------------------------------------------------------------------------------
 \begin{figure}[H]
	\centering
	\subfigure{\includegraphics[height=6.5cm,width=6.5cm]{fig_lambda_iLPA.eps}}
	\subfigure{\includegraphics[height=6.5cm,width=6.5cm]{fig_lambda_subgrad.eps}}
	\caption{The curves of relative error and rank for two solvers as $c_{\lambda}$ increases}	\label{fig0}
	\subfigure[]{\includegraphics[height=6.0cm,width=6.5cm]{fig_noise.eps}}
	\subfigure[]{\includegraphics[height=6.0cm,width=6.5cm]{fig_time.eps}}
	\caption{Relative error varies with sparsity of $\varpi$ and CPU time varies with $n_1$}
	\label{fig1}
\end{figure}

 Next we consider the recovery effect and running time (in seconds) of two solvers under different setting of $n_1=n_2, r^*$ and $\textrm{SR}$. Table \ref{tab1} reports the average results obtained by running $10$ examples generated randomly under each setting. We see that the average relative errors and ranks yielded by iLPA for five kinds of outliers are all lower than those yielded by subGM, although the former is applied to model \eqref{SCAD-loss} with the same $c_{\lambda}$ except for noise of type III, while the latter is applied to \eqref{SCAD-loss} with a carefully selected $c_{\lambda}$. The average CPU time required by subGM is at least 5 times more than that of iLPA.
%---------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \setlength{\abovecaptionskip}{-0.1cm}
 \begin{table}[h]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.0pt}
 	\renewcommand\arraystretch{1.2}
 	\centering
 	\scriptsize
 	\caption{\small Average RE and time for examples generated with the sampling scheme S1}\label{tab1}
 	\scalebox{1}{
 	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
 			%\hline
 	\Xhline{0.7pt}
 	& & \multicolumn{4}{l}{\  iLPA ($n_1\!=\!1000$,$r^*\!\!=\!10$)}&\multicolumn{4}{l}{\  subGM ($n_1\!=\!1000$,$r^*\!\!=\!10$)}& \multicolumn{4}{l}{\ iLPA ($n_1\!=\!3000$,$r^*\!\!=\!15$)}&
 	\multicolumn{4}{l}{\ subGM ($n_1\!\!=\!3000$,$r^*\!\!=\!15$)}\\
 	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
 			
 	$\varpi$ & SR&$c_{\lambda}$& RE & rank  &time&$c_{\lambda}$&  RE & rank & time&$c_{\lambda}$& RE & rank  &time&$c_{\lambda}$&  RE & rank & time\\
 	\hline			
 	&0.15 &0.06&\ {\color{red}\bf 3.78e-2} & 10 & 4.02 &0.06&\ 9.97e-2 & 10 & 51.5     &0.06& \ {\color{red}\bf 7.53e-4} & 15 & 22.1 &0.06&\ 4.00e-2 & 16 & 189.8\\
 I	&0.20 &0.06&\ {\color{red}\bf 3.95e-2} & 10 & 5.85 &0.06&\ 6.50e-2 & 10 & 54.6     &0.06&\ {\color{red}\bf 6.23e-4} & 15 & 24.1 &0.06&\ 3.82e-2 & 16 & 206.6 \\
 	&0.25 &0.06&\ {\color{red}\bf 1.04e-3} & 10 & 3.27 &0.06&\ 4.81e-2 & 10 & 55.8     &0.06&\ {\color{red}\bf 5.93e-4} & 15 & 26.0 &0.06&\ 3.09e-2 & 15 & 245.5\\
 	\hline
 	&0.15 &0.06&\ {\color{red}\bf 5.41e-3} & 10 & 4.94 &0.10&\ 2.78e-2  & 10 & 68.3    &0.06&\ {\color{red}\bf 7.23e-4} & 15 & 38.2 &0.15&\ 2.42e-2  & 16 & 207.4\\
 II	&0.20 &0.06&\ {\color{red}\bf 1.44e-3} & 10 & 5.30 &0.10&\ 2.40e-2  & 10 & 48.7    &0.06&\ {\color{red}\bf 5.87e-4} & 15 & 40.3 &0.15&\ 1.84e-2  & 15 & 208.2\\
 	&0.25 &0.06&\ {\color{red}\bf 1.07e-3} & 10 & 5.18 &0.10&\ 9.30e-3  & 10 & 50.2    &0.06&\ {\color{red}\bf 5.13e-4} & 15 & 42.4 &0.15&\ 1.13e-2  & 15 & 235.0\\
 	\hline
 	&0.15 &0.02&\ {\color{red}\bf 7.23e-2}& 10 & 16.2  &0.03 &\ 1.63e-1 & 11 & 48.9   &0.02&\ {\color{red}\bf 1.54e-3} & 15 & 91.8  &0.10&\ 8.00e-2 & 16 & 188.7 \\
III	&0.20 &0.02&\ {\color{red}\bf 5.36e-3}& 10 & 15.6  &0.03 &\ 7.51e-2 & 11 & 53.6   &0.02&\ {\color{red}\bf 1.69e-2} & 15 & 422.4 &0.10&\ 1.21e-1 & 16 & 208.1\\
 	&0.25 &0.02&\ {\color{red}\bf 2.27e-3}& 10 & 12.3  &0.03 &\ 4.78e-2 & 11 & 55.6   &0.02&\ {\color{red}\bf 9.53e-4} & 15 & 180.2 &0.10&\ 9.00e-3 & 16 & 231.7\\
 			
    \hline
 	&0.15 &0.06&\ {\color{red}\bf 7.62e-3}& 10 & 4.22  &0.08 &\ 2.99e-2 & 10 & 45.9   &0.06&\ {\color{red}\bf 6.50e-4} & 15 & 31.9 &0.15&\ 3.38e-2 & 16 & 182.7\\
IV	&0.20 &0.06&\ {\color{red}\bf 1.69e-3}& 10 & 4.64  &0.08 &\ 2.00e-2 & 10 & 48.6   &0.06&\ {\color{red}\bf 5.35e-4} & 15 & 34.1 &0.15&\ 2.58e-2 & 15 &200.5\\
	&0.25 &0.06&\ {\color{red}\bf 1.15e-3}& 10 & 4.49  &0.08 &\ 1.10e-2 & 10 & 53.6   &0.06&\ {\color{red}\bf 5.44e-4} & 15 & 39.3 &0.15&\ 1.77e-2 & 15 & 247.0\\
	\hline
 			
   &0.15 &0.06&\ {\color{red}\bf 3.43e-3}& 10 & 5.22  &0.15 &\ 4.86e-2 & 11 & 82.0   &0.06&\ {\color{red}\bf 7.37e-4} & 15 & 40.2 &0.15&\ 2.09e-2  & 15 & 202.7\\
V &0.20 &0.06&\ {\color{red}\bf 1.50e-3}& 10 & 5.64  &0.15 &\ 2.69e-2 & 10 & 64.9   &0.06&\ {\color{red}\bf 7.15e-4} & 15 & 45.3 &0.15&\ 1.50e-2  & 15 & 209.0\\
  &0.25 &0.06&\ {\color{red}\bf 1.18e-3}& 10 & 6.27  &0.15 &\ 1.35e-2 & 10 & 52.8   &0.06&\ {\color{red}\bf 6.20e-4} & 15 & 45.4 &0.15&\ 9.80e-3  &15 & 232.9 \\
 			
 \Xhline{0.7pt}
 \end{tabular}}
 \end{table}

%--------------------------------------------------------------------------------------------------------
\subsubsection{Numerical results for real data}\label{sec5.2.2}

 We test iLPA and subGM with matrix completion problems on some real data sets, including the Jester joke dataset, the MovieLens dataset, and the Netflix dataset. For each data set, let $M^0$ represent the original incomplete data matrix such that the $i$th row of $M^0$ corresponds to the ratings given by the $i$th user. We first consider the Jester joke dataset (see \url{http://www.ieor.berkeley.edu/~goldberg/jester-data/}), and more detailed descriptions can be found in \cite{Toh10,Chen12}. Due to the large number of users, as in \cite{Chen12}, we first randomly select $n_u$ users' ratings from $M^0$, and then randomly permute the ratings from the users to generate $M^*\in\mathbb{R}^{{n_u}\times 100}$. Next we generate a set of observed indices $\Omega$ with the sampling scheme S1, and then the observation matrix $M_{\Omega}$ via \eqref{observe}. Since we can only observe those entries $M_{jk}$ with $(j,k)\in\Omega$, and $M_{jk}$ is available, the actual sampling ratio is less than the input SR. Since many entries are unknown, we cannot compute the relative error as we did for the simulated data. Instead, we take the metric of the normalized mean absolute error (NMAE) to measure the accuracy of $X^{\rm out}$:
 \[
  {\rm NMAE}=\frac{\sum_{(i,j)\in\Gamma\backslash\Omega}|X^{\rm out}_{i,j}-M_{i,j}|}
  {|\Gamma\backslash\Omega|(r_{\rm max}-r_{\rm min})}\ \ {\rm with}\ X^{\rm out}=U^{\rm out}(V^{\rm out})^{\top},
 \]
 where $\Gamma:=\{(i,j)\in[n_{u}]\times[100]\!: M_{ij}\ \textrm{is given}\}$ denotes the set of indices for which $M_{ij}$ is given, $r_{\rm min}$ and $r_{\rm max}$ denote the lower and upper bounds for the ratings, respectively. In the Jester joke dataset, the range is $[-10,10]$. Thus, we have $r_{\rm max}-r_{\rm min}=20$.

 For the Jester joke dataset, we consider different settings of $n_u$ and SR, and report the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting in Tables \ref{tabJester1}-\ref{tabJester3}. We see that iLPA yields comparable even a little better results (lower NMAE and rank) than subGM does for all jester examples.
%---------------------------------------------------------------------------------
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for Jester-1 dataset}\label{tabJester1}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad \ iLPA ($n_{u}\!=\!1000$)}&\multicolumn{4}{l}{\quad  subGM ($n_u\!=\!1000$)}& \multicolumn{4}{l}{\quad \ iLPA ($n_{u}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{u}\!\!=\!5000$)}\\
			\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& {\color{red}\bf 0.2042} & 4 & 0.15 &0.2&0.2044 & 4 & 0.78     &0.2&  0.2015 & 9 & 1.56 &0.2& 0.2027 & 8 & 10.6\\
			I	&0.20 &0.2& {\color{red}\bf 0.1986} & 3 & 0.14 &0.2&0.1990& 3 & 0.40      &0.2&  0.1983 & 12 & 1.72 &0.2& 0.1989 & 11 & 11.4 \\
			&0.25 &0.2& {\color{red}\bf 0.1932} & 3 & 0.15 &0.2&0.1935 & 3 & 0.43     &0.2& 0.1914 & 14 & 1.61 &0.2& 0.1925 & 13 & 11.2\\
			\hline
			
			&0.15 &0.2& 0.1940 & 8 & 0.18 &0.2& 0.1942 & 7 & 1.51                    &0.2& 0.1912 & 13 & 1.96 &0.2& 0.1921 & 11 & 10.8\\
			II	&0.20 &0.2& 0.1884 & 9 & 0.20 &0.2 & 0.1886 & 7 & 1.52                   &0.2& 0.1887 & 16 & 1.83 &0.2& 0.1888 & 15 & 11.4\\
			&0.25 &0.2& {\color{red}\bf0.1829} & 8 & 0.21 &0.2&0.1830 & 8 & 1.40     &0.2& 0.1811& 17 & 1.62 &0.2& 0.1813 & 16 & 11.5\\
			\hline
			
			&0.15 &0.02& {\color{red}\bf 0.2007} & 11 & 0.21  &0.02& 0.2012 & 12 & 1.20  &0.02&0.1946 & 18 & 1.71 &0.02& 0.1953 & 15 & 10.7\\
			III	&0.20 &0.02& {\color{red}\bf 0.1972} & 16 & 0.21 &0.02 & 0.1984 & 16 & 1.12  &0.02& 0.1931 & 23 & 2.01 &0.02& 0.1941 & 21 & 11.3 \\
			&0.25 &0.02& {\color{red}\bf 0.1879} & 13 & 0.23 &0.02&0.1886 & 14 & 1.31    &0.02& 0.1829 & 25 & 2.38 &0.02& 0.1841 & 24 & 11.4\\
			\hline
			&0.15 &0.2&  0.1959 & 8 & 0.19 &0.2& 0.1961 & 7 & 1.47                                  &0.2& 0.1928 & 13 & 1.90 &0.2& 0.1936 & 11 & 10.8\\
			IV	&0.20 &0.2&  0.1901 & 9 & 0.19 &0.2 & 0.1902 & 7 & 1.38                                  &0.2& 0.1903 & 16 & 1.76 &0.2& 0.1905 & 15 & 11.4 \\
			&0.25 &0.2& {\color{red}\bf 0.1847} & 8 & 0.22 &0.2&{\color{red}\bf 0.1847} & 8 & 1.31  &0.2& 0.1827 & 17 & 1.74 &0.2& 0.1832 & 16 & 11.4\\
			\hline
			&0.15 &0.2& 0.1934 & 8 & 0.19  &0.2& 0.1936 & 7 & 1.45                  &0.2&  0.1907 & 13 & 2.01 &0.2& 0.1915 & 12 & 10.5\\
			V	&0.20 &0.2&\  0.1881 & 9 & 0.21 &0.2 & 0.1882 & 7 & 1.53                &0.2&  0.1883 & 16 & 1.73 &0.2&{\color{red}\bf 0.1883} & 15 & 10.6 \\
			&0.25 &0.2& 0.1825 & 9 & 0.21 &0.2&{\color{red}\bf0.1824} & 8 & 1.66                    &0.2& 0.1805 & 17 & 1.65 &0.2& 0.1808 & 16 & 10.4\\
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for Jester-2 dataset}\label{tabJester2}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad  iLPA ($n_{u}\!=\!1000$)}&\multicolumn{4}{l}{\quad  subGM ($n_u\!=\!1000$)}& \multicolumn{4}{l}{\quad iLPA ($n_{u}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{u}\!\!=\!5000$)}\\
			\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& {\color{red}\bf 0.2034} & 4 & 0.13 &0.2& 0.2036 & 4 & 0.83     &0.2& {\color{red}\bf 0.2016} & 9 & 1.50 &0.2& 0.2028 & 9 & 10.7\\
			I	&0.20 &0.2& {\color{red}\bf 0.1989} & 2 & 0.13 &0.2 & 0.1993 & 3 & 0.45    &0.2&  0.1964 & 12 & 1.68 &0.2& 0.1974 & 11 & 11.2 \\
			&0.25 &0.2& {\color{red}\bf 0.1936} & 2 & 0.15 &0.2&0.1939 & 3 & 0.38      &0.2&  0.1906 & 14 & 1.65 &0.2& 0.1917 & 13 & 11.3\\
			\hline
			&0.15 &0.2&0.1933 & 8 & 0.19 &0.2& 0.1938 & 7 & 1.37      &0.2& 0.1914 & 13 & 2.09 &0.2& 0.1921 & 11 & 10.8\\
			II	&0.20 &0.2&0.1892 & 9 & 0.19 &0.2& 0.1895 & 8 & 1.41      &0.2& 0.1872 & 16 & 1.95 &0.2& 0.1874 & 14 & 11.2 \\
			&0.25 &0.2& 0.1827 & 8 & 0.23 &0.2&{\color{red}\bf 0.1826} & 8 & 1.58          &0.2& 0.1800 & 17 & 1.49 &0.2& 0.1804 & 15 & 11.2\\		
			\hline
			
			&0.15 &0.02& {\color{red}\bf 0.2025} & 16 & 0.24 &0.02& 0.2032 & 17 & 1.38   &0.02& 0.1948& 18 & 1.75 &0.02& 0.1955 & 16 & 10.8\\
			III	&0.20 &0.02& {\color{red}\bf 0.1973} & 16 & 0.22 &0.02 & 0.1989 & 17 & 1.00  &0.02& 0.1914 & 22 & 1.86 &0.02& 0.1921 & 11 & 11.4\\
			&0.25 &0.02& 0.1889 & 14 & 0.27 &0.02&{\color{red}\bf 0.1888} & 15 & 1.33    &0.02& 0.1819 & 25 & 2.32 &0.02& 0.1830 & 24 & 11.3\\		
			\hline
			
			&0.15 &0.2& 0.1952 & 8 & 0.19 &0.2 & 0.1957 & 7 & 1.36                  &0.2&0.1931 & 13 & 1.97 &0.2& 0.1939 & 11 & 10.9\\
			IV	&0.20 &0.2& 0.1910 & 9 & 0.19 &0.2 & 0.1912 & 8 & 2.40                  &0.2& 0.1888 & 16 & 1.96 &0.2& 0.1891 & 14 & 11.3\\
			&0.25 &0.2& {\color{red}\bf0.1847} & 8 & 0.22 &0.2 &{\color{red}\bf 0.1847} & 8 & 1.31  &0.2& 0.1819 & 17 & 1.51 &0.2& 0.1823 & 15 & 11.2\\	
			\hline
			
			&0.15 &0.2& 0.1927 & 8 & 0.20 &0.2& 0.1929 & 7 & 1.40                  &0.2& 0.1907 & 13 & 2.04 &0.2& 0.1914 & 12 & 10.7\\
			V	&0.20 &0.2& 0.1882 & 9 & 0.20 &0.2& 0.1885 & 8 & 1.47                  &0.2& 0.1865 & 16 & 2.10 &0.2& 0.1868 & 14 & 11.3\\
			&0.25 &0.2&{\color{red}\bf 0.1823} & 8 & 0.23 &0.2&{\color{red}\bf 0.1823} & 8 & 1.56  &0.2& 0.1795 & 17 & 1.65 &0.2& 0.1799 & 15 & 11.2\\	
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[h]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for Jester-3 dataset}\label{tabJester3}
	\scalebox{1}{
	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad  iLPA ($n_{u}\!=\!1000$)}&\multicolumn{4}{l}{\quad subGM ($n_u\!=\!1000$)}& \multicolumn{4}{l}{\quad iLPA ($n_{u}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{u}\!\!=\!5000$)}\\
			\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& 0.2285 & 3 & 0.19 &0.2& 0.2256 & 4 & 0.60      &0.2& {\color{red}\bf 0.2218} & 5 & 1.46 &0.2& 0.2233 & 5 & 9.42\\
			I	&0.20 &0.2& 0.2256 & 3 & 0.16 &0.2& 0.2214 & 4 & 0.98      &0.2&  0.2198 & 6 & 1.60 &0.2& 0.2214 & 5 & 10.5  \\
			&0.25 &0.2& 0.2208 & 3 & 0.17 &0.2& 0.2195 & 4 & 0.90      &0.2& {\color{red}\bf 0.2153} & 6 & 1.49 &0.2& 0.2171 & 6 & 10.9\\
			\hline
			&0.15 &0.2& {\color{red}\bf 0.2168} & 4 & 0.16 &0.2& 0.2170 & 5 & 1.17     &0.2&  0.2154 & 7 & 2.15 &0.2&  0.2155 & 6 & 10.0\\
			II	&0.20 &0.2& {\color{red}\bf 0.2119} & 5 & 0.15 &0.2 & 0.2121 & 5 & 1.13    &0.2& {\color{red}\bf 0.2127} & 7 & 2.50 &0.2& 0.2133 & 7 & 10.4 \\
			&0.25 &0.2& 0.2085 & 6 & 0.15 &0.2&0.2087 & 5 & 1.25       &0.2&  0.2067 & 9 & 2.49 &0.2& 0.2074  & 8& 10.6\\	
			\hline
			&0.15 &0.02& 0.2309 & 11 & 0.33 &0.02& 0.2255 & 13 & 1.19     &0.02& 0.2215 & 15 & 2.84 &0.02&\ 0.2209 & 16 & 10.0\\
			III	&0.20 &0.02& 0.2260 & 14 & 0.35 &0.02&0.2232 & 18 & 1.17     &0.02& {\color{red}\bf 0.2140}  &8 & 1.95 &0.02& 0.2144 & 8 & 10.3 \\
			&0.25 &0.02& 0.2235 & 13 & 0.22 &0.02& 0.2228 & 15 & 1.01     &0.02& 0.2104 & 15 & 2.49 &0.02& 0.2115  & 14& 10.7\\	
			\hline
			&0.15 &0.2& {\color{red}\bf 0.2180}& 3 & 0.16 &0.2& 0.2182 & 5 & 1.06                 &0.2& {\color{red}\bf 0.2164} & 6 & 2.31 &0.2& 0.2166 & 6 & 10.1\\
			IV	&0.20 &0.2& {\color{red}\bf 0.2131}& 5 & 0.16 &0.2 & 0.2134 & 5 & 1.10                &0.2& {\color{red}\bf0.2137} &7 & 2.35 &0.2& 0.2145 & 7 & 10.4 \\
			&0.25 &0.2&{\color{red}\bf 0.2104}& 5& 0.15 &0.2&{\color{red}\bf0.2104} & 5 & 1.27    &0.2& {\color{red}\bf0.2085} & 9 & 2.61 &0.2& 0.2092   & 9 & 10.6\\	
			\hline
			
			&0.15 &0.2& {\color{red}\bf 0.2165}  & 4 & 0.18 &0.2& 0.2167 & 5 & 1.12       &0.2& 0.2152 & 6 & 2.21 &0.2& {\color{red}\bf0.2151} & 6 & 9.97\\
			V	&0.20 &0.2& {\color{red}\bf 0.2116} & 5 & 0.17 &0.2 & 0.2119 & 5 & 1.17        &0.2& {\color{red}\bf 0.2122}  &7 & 2.44 &0.2& 0.2129 & 7 & 10.3 \\
			&0.25 &0.2& 0.2083 & 6 & 0.17 &0.2&0.2084 & 5 & 1.30           &0.2& {\color{red}\bf 0.2062} & 9 & 2.47 &0.2& 0.2069 & 9 & 10.5\\	
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}

 Next we consider the MovieLens dataset (see \url{http://www.grouplens.org/node/73}), for which the ratings range is from $r_{\rm min}=1$ to $r_{\rm max}=5$; see \cite{Chen12,Toh10} for more description. For the Movie-100K dataset, like \cite{Toh10} we also consider the data matrix $\widetilde{M}^0=M^0-3$, sample the observed entries in two schemes, and obtain the observation matrix $M_{\Omega}$ via \eqref{observe} with $M^*\!=\widetilde{M}^0$. Table \ref{tabMovie-100K} reports the average NMAE, rank and running time (in seconds) after running $10$ times for the setting $(n_r,n_c)=(943,1682)$. We see that iLPA yields better results than subGM does for those examples generated with the sampling scheme S1 for  noise of types III-V, and has a little better preformance than subGM does for those examples generated with the sampling scheme S2 for noise of types II-V.

%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for Movie-100K dataset}\label{tabMovie-100K}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad \quad \ iLPA (S1)}&\multicolumn{4}{l}{\quad \quad subGM (S1)}& \multicolumn{4}{l}{\quad \quad \  iLPA (S2)}&
			\multicolumn{4}{l}{\quad \quad subGM (S2)}\\
			\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			
			&0.15 &0.1&  0.2361 & 2 & 2.23 &0.1&  {\color{red}\bf 0.2311} & 2 & 6.34     &0.1&  0.2402 & 3 & 2.41 &0.1&  {\color{red}\bf 0.2342} & 2 & 6.52\\
			I	&0.20 &0.1&0.2246 & 2 & 2.12 &0.1& {\color{red}\bf 0.2229} & 2 & 6.48       &0.1&0.2259 & 2 & 2.47 &0.1& {\color{red}\bf 0.2240} & 2 & 6.70 \\
			&0.25 &0.1&  0.2186 & 2 & 2.37 &0.1& {\color{red}\bf 0.2176} & 2 & 6.57     &0.1&  0.2224 & 2 & 2.77 &0.1&  {\color{red}\bf \ 0.2201 }& 2 & 6.72 \\			
			\hline	
			
			&0.15 &0.3&  0.2249 & 3 & 2.42 &0.3&  {\color{red}\bf 0.2248} & 2 & 6.59    &0.3&  {\color{red}\bf0.2275} & 1 & 2.34 &0.3&   0.2284 & 2 & 6.44\\
			II  &0.20 &0.3&  0.2171 & 2 & 2.17 &0.3&  0.2174 & 1 & 6.71 &0.3&{\color{red}\bf0.2184} & 1 & 2.32 &0.3&  0.2185 & 2 & 6.72 \\
			&0.25 &0.3& {\color{red}\bf 0.2119} & 1 & 1.92 &0.3&  0.2123 & 1 & 6.74 &0.3& {\color{red}\bf 0.2142} & 1 & 1.95 &0.3&  0.2149 & 3 & 6.59\\
			\hline
			
			
			&0.15 &0.02& {\color{red}\bf  0.2369} & 2 & 1.65 &0.02& 0.2438 & 36 & 10.2  &0.02& {\color{red}\bf0.2397} & 3 & 1.91 &0.02&  0.2441 & 30 & 10.4\\
			III	&0.20 &0.02&{\color{red}\bf0.2312} & 1 & 1.69 &0.02& 0.2343 & 22 & 6.72     &0.02&{\color{red}\bf0.2314} & 2 & 1.77 &0.02&  0.2345& 15 & 6.58 \\
			&0.25 &0.02& {\color{red}\bf 0.2231} & 2 & 2.03 &0.02&  0.2263 & 15 & 6.73                   &0.02& {\color{red}\bf 0.2232} & 2 & 1.83 &0.02&  0.2271 & 9 & 6.59 \\
			\hline
			
			
			&0.15 &0.3& {\color{red}\bf 0.2277} & 1 & 1.69 &0.3&  0.2318 & 1& 6.58          &0.3&  {\color{red}\bf 0.2306} & 1 & 1.76 &0.3&   0.2335& 1 & 6.76\\
			IV	&0.20 &0.3&{\color{red}\bf0.2213} & 1 & 1.56 &0.3&  0.2238& 1 & 6.67            &0.3&{\color{red}\bf0.2226} & 1 & 1.56 &0.3&  0.2251 & 1 & 6.71\\
			&0.25 &0.3& {\color{red}\bf 0.2164} & 1 & 1.53 &0.3&  0.2184 & 1 & 6.76         &0.3& {\color{red}\bf 0.2190} & 1 & 1.49 &0.3&  0.2208 & 1 & 6.71\\
			\hline
			
			
			&0.15 &0.3&  {\color{red}\bf0.2218} & 2 & 2.41 &0.3&   0.2220& 3 & 6.45      &0.3& {\color{red}\bf 0.2268}& 4 & 2.92 &0.3&  0.2270 & 4 & 6.90\\
			V	&0.20 &0.3&{\color{red}\bf0.2152} & 2 & 2.10 &0.3&  0.2153 & 2 & 6.73        &0.3&{\color{red}\bf0.2161} & 3 & 2.07 &0.3&  0.2165 & 3 & 6.59 \\
			&0.25 &0.3&  0.2098& 3 & 2.32 &0.3&  0.2100 & 2 & 6.72      &0.3& {\color{red}\bf 0.2121}& 3 & 2.13 &0.3&  0.2127 & 3 & 6.63\\
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}


 For the Movie-1M dataset, we first randomly select $n_r$ users and their $n_c$ column ratings from $M^0$ to formulate $M^*\in\mathbb{R}^{n_r\times n_c}$, sample the observed entries with scheme S1, and then obtain the observation matrix $M_{\Omega}$ via \eqref{observe}. We consider the setting of $n_r=n_c$ with $n_r=3000$ and
 the setting of $(n_r,n_c)=(6040,3706)$. Table \ref{tabMovie-1M} reports the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting. We see that iLPA and subGM yield comparable NMAEs and ranks.
%--------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.4pt}
  \renewcommand\arraystretch{1.2}
  \centering
  \scriptsize
  \caption{\small Average NMAE and running time of two solvers for Movie-1M dataset}\label{tabMovie-1M}
   \scalebox{1}{
   	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
	\Xhline{0.7pt}
   & & \multicolumn{4}{l}{\  iLPA($n_{u}\!=\!3000$)}&\multicolumn{4}{l}{\  subGM($n_u\!=\!3000$)}& \multicolumn{4}{l}{\ iLPA($6040\times 3706$)}&
	\multicolumn{4}{l}{\ subGM($6040\times 3706$)}\\
	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			
	$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
	\hline	
			
	& 0.15 &0.1&  0.2173 & 1 & 7.96 &0.1& 0.2172 & 2 & 33.1  &0.1&  0.2108 & 1 & 24.3 &0.1&  0.2100 & 3 & 75.1\\
 I  & 0.20 &0.1&{\color{red}\bf0.2110} &1&7.65 &0.1&  0.2110&2 &34.0     &0.1&  0.2065 & 1 & 24.1 &0.1& 0.2060 & 2 & 77.5 \\
	&0.25 &0.1& {\color{red}\bf 0.2061} & 1 & 7.39 &0.1&  0.2062 & 3 & 34.4              &0.1& {\color{red}\bf 0.2010} & 1 & 23.7 &0.1&  0.2015 & 3 & 79.8\\
			\hline			
			
	& 0.15 &0.3&  {\color{red}\bf 0.2113} & 1 & 11.5 &0.3&  0.2123 & 1 & 32.5                 &0.3&{\color{red}\bf 0.2048} & 1 & 26.8 &0.3&  0.2049 & 1 & 75.4\\
II	&0.20 &0.3&  {\color{red}\bf 0.2057} & 1 & 9.41 &0.3&  0.2062 & 1 & 32.9 &0.3&  0.2013 & 1 & 26.1 &0.3& 0.2007 & 2 & 76.4\\
	& 0.25 &0.3& {\color{red}\bf 0.2012} & 1 & 9.28 &0.3&{\color{red}\bf0.2012}&1&41.5 &0.3&  0.1967 & 1 & 24.6 &0.3& 0.1959 & 2 & 79.6 \\
			
	\hline
			
	& 0.15 &0.02&  {\color{red}\bf0.2167} & 1 & 6.82 &0.02&  0.2207 & 1 & 32.0     &0.02&  0.2022 & 1 & 25.2 &0.02&  {\color{red}\bf0.2020} & 1 & 75.8\\
III	& 0.20 &0.02&  {\color{red}\bf0.2073} & 1 & 6.66 &0.02&  0.2099 & 1 & 32.6     &0.02&  0.1956 & 2 & 32.9 &0.02& {\color{red}\bf 0.1946} & 2 & 76.4\\
	& 0.25&0.02& {\color{red}\bf 0.2007} & 1 & 7.15 &0.02&  0.2022 & 1 & 33.3      &0.02& {\color{red}\bf 0.1880} & 3 &34.1 &0.02& 0.1881 & 3 & 79.2\\
			
	\hline
	&0.15 &0.3& {\color{red}\bf 0.2166} & 1 & 8.84 &0.3&  0.2187 & 1 & 31.6    &0.3& {\color{red}\bf \ 0.2095 }& 1 & 23.4 &0.3&  0.2104 & 1 & 75.1\\
IV  & 0.20 &0.3&  {\color{red}\bf0.2107} & 1 & 8.42 &0.3& 0.2119 & 1 & 32.1    &0.3& {\color{red}\bf 0.2052} & 1 & 23.6 &0.3&  0.2058 & 1 & 76.5\\
	& 0.25 &0.3& {\color{red}\bf 0.2060} & 1 & 8.48 &0.3&  0.2069 & 1 & 32.5   &0.3& {\color{red}\bf 0.2006} & 1 & 22.3 &0.3&  0.2011 & 1 & 79.4 \\			
			
	\hline
			
	&0.15 &0.3& 0.2086 & 3 & 13.7 &0.3&  0.2095 & 2 & 31.8 &0.3&  0.2028 & 2 & 31.5 &0.3& 0.2019 & 3 & 77.1\\
V	& 0.20 &0.3& 0.2022 & 3 & 13.2 &0.3& 0.2029 & 2 & 41.0      &0.3&  0.1980 & 2 & 30.8 &0.3& 0.1971 & 3 & 78.2\\
	& 0.25 &0.3&  0.1992 & 1 & 9.84 &0.3& 0.1971 & 2 & 32.6     &0.3&  0.1927 & 2 & 29.6 &0.3&  {\color{red}\bf0.1917} & 2 & 80.4\\
			
   \Xhline{0.7pt}
	\end{tabular}}
 \end{table}

 Finally, we consider the Netflix dataset (see \url{https://www.kaggle.com/netflix-inc/netflix-prize-data\#qualifying.txt}). For this dataset, we first randomly select $n_r$ users with $n_r=6000$ and $10000$ and their $n_c=n_r$ column ratings from $M^0$, sample the observed entries with the sampling scheme S1, and then obtain the observation matrix $M_{\Omega}$ via \eqref{observe} with $M^*=M^0$. Table \ref{tabNetflix} reports the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting. We see that iLPA yields better results than subGM does for $n_r=6000$, and the two solvers have the comparable NMAEs for $n_r=10000$.
%---------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[h]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.4pt}
  \renewcommand\arraystretch{1.2}
  \centering
  \scriptsize
  \caption{\small Average NMAE and running time of two solvers for Netflix dataset}\label{tabNetflix}
  \scalebox{1}{
   \begin{tabular}{cc|lccc|lccc||lccc|lccc}
  \Xhline{0.7pt}
	& & \multicolumn{4}{l}{\  iLPA($n_{u}\!=\!6000$)}&\multicolumn{4}{l}{\  subGM($n_u\!=\!6000$)}& \multicolumn{4}{l}{\ iLPA($n_{u}\!=\!10000$)}&
	\multicolumn{4}{l}{\ subGM($n_{u}\!\!=\!10000$)}\\
	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			
	$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
	\hline	
	&0.15 &0.1& {\color{red}\bf 0.2259} & 2 & 48.2 &0.1&   0.2274 & 3 & 155.3& 0.1&  0.2186 & 2 & 130.2 &0.1&  0.2178 & 3 & 382.3\\
I	&0.20 &0.1& {\color{red}\bf 0.2200} & 2 & 40.7 &0.1&  0.2206 & 3 & 124.5 & 0.1& {\color{red}\bf 0.2130} & 3 & 110.1 &0.1& 0.2212 & 4 & 495.1 \\
	&0.25 &0.1&  0.2143 & 2 & 41.1 &0.1& 0.2139 & 3 & 119.2 & 0.1& 0.2080 & 2 & 112.8 &0.1&  0.2068 & 5 & 806.2\\
	\hline
			
    &0.15  &0.3& {\color{red}\bf\ 0.2177 }& 2 & 57.8 &0.3&  0.2181 & 3 & 135.9 &0.3&  0.2109 & 2 & 161.3 &0.3& 0.2102 & 3 & 317.3\\
II	&0.20 &0.3& {\color{red}\bf0.2125} & 2 & 57.2 &0.3&  0.2127 & 2 & 127.0 &0.3&  0.2056 & 3 & 143.9 &0.3& {\color{red}\bf 0.2046} & 3 & 319.1  \\
	&0.25 &0.3& {\color{red}\bf 0.2070} & 2 & 54.5 &0.3&  {\color{red}\bf0.2070} & 2 & 117.7  &0.3& 0.2006 & 2 & 140.3 &0.3&  {\color{red}\bf0.1999} & 2 & 320.8\\
	\hline
			
  	&0.15 &0.02&  {\color{red}\bf\ 0.2235 }& 1 & 32.2 &0.02&  0.2264 & 1 & 117.2  &0.02&  0.2071 & 3 & 162.7    &0.02&  {\color{red}\bf0.2060} & 3 & 297.4\\
III  &0.20 &0.02& {\color{red}\bf 0.2144} & 1 & 36.8 &0.02& 0.2159 & 1 & 115.4 &0.02&  0.2006 & 3 & 168.2 &0.02& 0.1988 & 4 & 299.9\\
	&0.25 &0.02& {\color{red}\bf 0.2065} & 2 & 38.5 &0.02&  0.2078 & 2 & 116.2 &0.02&  0.1942 & 5 & 177.2 &0.02&  0.1929 & 6 & 552.7 \\
			\hline
			
	&0.15 &0.3& {\color{red}\bf 0.2228} & 1 & 43.1 &0.3& 0.2245 & 1 & 109.4 &0.3&  0.2165 & 1 & 117.1 &0.3&  0.2161 & 2 & 314.7\\
IV	&0.20 &0.3& 0.2157 & 2 & 42.1 &0.3& 0.2188 & 1 & 111.3 &0.3&  0.2133 & 1 & 108.2 &0.3& 0.2105 & 2 & 316.6 \\
	&0.25 &0.3& 0.2124 & 2 & 39.8 &0.3& 0.2136 & 1 & 112.6 &0.3&  0.2073 & 1 & 112.0 &0.3&  0.2065 & 2 & 322.0\\
	\hline
			
	&0.15  &0.3&  {\color{red}\bf0.2152} & 3 & 68.1 &0.3& 0.2157 & 4 & 159.1 &0.3&  0.2086 & 3 & 181.9 &0.3&  0.2077 & 4 & 312.1\\
V  &0.20  &0.3&  {\color{red}\bf0.2100} & 3 & 62.0 &0.3& 0.2104 & 4 & 128.3 &0.3&  0.2035 & 3 & 163.5 &0.3& {\color{red}\bf 0.2020} & 3 & 313.3 \\
   &0.25 &0.3& {\color{red}\bf 0.2041} & 3 & 56.2 &0.3& 0.2044 & 4 & 146.8  &0.3&  0.1981 & 3 & 150.6 &0.3&  {\color{red}\bf0.1972} & 3 & 318.9 \\
			
  \Xhline{0.7pt}
 \end{tabular}}
\end{table}

 To sum up, iLPA yields comparable even a little better results than subGM does for all real datasets, and it requires less CPU time than subGM does. In particular, for $10000\times 10000$ Netflix test problems, iLPA yields the favorable outputs in $200$ seconds, and needs about half of the time required by subGM.

%------------------------------------------------------------------------------------
\subsection{DC programs with nonsmooth components}\label{sec5.3}
%-------------------------------------------------------------------------------------
 We confirm the efficiency of iLPA on DC programs with nonsmooth components, which take the form of \eqref{prob} with $h\equiv 0$. We apply iLPA to the test examples appearing in the DC literature \cite{Artacho20}, and compare its performance with that of the DC algorithm proposed in \cite{Ferreria21}, a non-monotone boosted DC algorithm (nmBDCA). Now let us describe these examples in terms of model \eqref{prob}, where $\phi(y):=\max_{1\le i\le m}y_i$ for $y\in\mathbb{R}^m$.
 \begin{example}\label{exam5.1}    $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}^{3}\times\mathbb{R},\mathbb{Z}=\mathbb{R}^{3}, \vartheta_1(y,t)=\phi(y)+t, \vartheta_2(z)=\phi(z)$ and
	\begin{align*}
		F(x)=(f_1^1(x);f_1^2(x);f_1^3(x);f_1^1(x)+f_2^2(x)+f_2^3(x)),\\
		G(x)=(f_2^1(x)\!+f_2^2(x);f_2^2(x)\!+\!f_2^3(x);f_2^1(x)\!+\!f_2^3(x))
	\end{align*}
	where $f_1^1(x)=x_1^4+x_2^2,f_1^2(x)=(2-x_1)^2+(2-x_2)^2,f_1^3(x)=2e^{-x_1+x_2},
	f_2^1(x)=x_1^2-2x_1+x_2^2-4x_2+4$, $f_2^2(x)=2x_1^2-5x_1+x_2^2-2x_2+4$ and $f_2^3(x)\!=x_1^2\!+2x_2^2\!-4x_2\!+1$.
 \end{example}
 \begin{example}\label{exam5.2}
  $\mathbb{X}=\mathbb{R}^4,\mathbb{Y}=\mathbb{R}^5\times \mathbb{R}^3\times \mathbb{R}^3,\mathbb{Z}=\mathbb{R}^{3}\times\mathbb{R}$, $\vartheta_1(y_1;y_2;y_3):=\|y_1\|_1+\phi(y_2)+\phi(y_3)$, $\vartheta_2(z,t):=\|z\|_1+t$, $F(x):=(F_1(x); F_2(x); F_3(x))$ with
  \begin{align*}
   F_1(x):=(x_1-1;x_3-1;10.1(x_2-1);10.1(x_4-1);4.95(x_2+x_4-2)),\\
   F_2(x):=200(0; x_1-x_2; -x_1-x_2),\,F_3(x):=180(0;x_3-x_4;-x_3-x_4),
  \end{align*}
  and $G(x):=(100x_1;90x_3;4.95(x_2-x_4);-100x_2-90x_4)$.
 \end{example}
 \begin{example}\label{exam5.3}
  $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}\times \mathbb{R}^3\times \mathbb{R},\mathbb{Z}=\mathbb{R},\vartheta_1(y_1;y_2;y_3)=|y_1|+\phi(y_2)+y_3,\vartheta_2(t)=|t|$, $F(x):=(F_1(x); F_2(x); F_3(x))$ with $F_1(x)=x_1-1,F_2(x)=200(1;x_1-x_2;-x_1-x_2)$
	and $F_3(x)=-x_1-x_2$, and $G(x):=100x_1$.
 \end{example}
 \begin{example}\label{exam5.4}
  $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}\times\mathbb{R}^3\times \mathbb{R}^9,\mathbb{Z}=\mathbb{R}^3,\vartheta_1(y_1;y_2;y_3)=|y_1|+\phi(y_2)+\phi(y_3),\vartheta_2(z,t)=\|z\|_1+|t|$, $F(x):=(F_1(x); F_2(x); F_3(x))$ with $F_1(x)=x_1-1,F_2(x)=200(0;x_1-x_2;-x_1-x_2)$
	and $F_3(x)=10(x_1^2+x_2^2+x_2; x_1^2+x_2^2-x_2; x_1+x_1^2+x_2^2+x_2-0.5;x_1+x_1^2+x_2^2-x_2-0.5;
	x_1\!-1;-\!x_1\!+2x_2\!-1;x_1\!-2x_2\!-1;-x_1\!-1;x_1\!+x_1^2\!+x_2^2)$, and $G(x):=(100x_1;10x_2;-100x_2+10(x_1^2+x_2^2))$.
 \end{example}
\begin{example}\label{exam5.5}
 $\mathbb{X}=\mathbb{R}^3,\mathbb{Y}=\mathbb{R}^3\times\mathbb{R}^4,\mathbb{Z}=\mathbb{R}^4,\vartheta_1(y_1;y_2)=\|y_1\|_1+\phi(y_2),\vartheta_2(z,t)=\|z\|_1+|t|$, $F(x):=(F_1(x); F_2(x))$ with $F_1(x)=2x,F_2(x)=10(0;x_1\!+x_2\!+2x_3\!-3;-x_1;-x_2,-x_3)$, and $G(x):=(x_1-x_2;x_1-x_2;10x_2;9\!-\!8x_1\!-\!6x_2\!-\!4x_3\!+\!4x_1^2\!+\!2x_2^2\!+2x_3^2)$.
 \end{example}
 \begin{example}\label{exam5.6}
  $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}^2,\mathbb{Z}=\mathbb{R},\vartheta_1(y)=\|y\|_1,\vartheta_2(t)=t$, $F(x):=x$ and $G(x):=-\frac{5}{2}x_1\!+\frac{3}{2}(x_1^2+x_2^2)$.	
 \end{example} 	

 For the above test examples, the parameters of Algorithm \ref{iLPA} are chosen as follows:
 \[
  \varrho = 2,\,\overline{\gamma}=10^6,\,\underline{\gamma}=0.01,\,  \gamma_{k,0}=\underline{\gamma},\, \alpha_k=\min\big\{10^{-4},\frac{10}{\max\{1,\|F'(x^0)\|\}}\big\}.
 \]
 The parameters of nmBDCA take the default setting in the code of nmBDCA. For the sake of fairness, we adopt the same stopping rule: $\|x^k-x^{k-1}\|\le  10^{-7}$ or $k\ge 1000$.

 Table \ref{DCtab1} below reprots the average results of $100$ times running for each example. Among others, $\min\Theta$ and $\max\Theta$ denote the minimum and maximum objective values among $100$ running, and \textbf{Nopt} records the number of solutions whose objective values have the absolute difference to the optimal value less than $10^{-5}$. In each running, the two solvers start from the same random initial point generated by MATLAB command $x^0=20*\textrm{rand(n,1)}-10$.  We see that iLPA finds more global optimal solutions than nmBDCA does except for Example \ref{exam5.3}. For Examples \ref{exam5.2} and \ref{exam5.4}, the number of optimal solutions returned by iLPA is close to twice that of optimal solutions given by nmBDCA. The average objective values yielded by iPLA is better than the one given by nmBDCA except for Example \ref{exam5.3}.
%------------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
 \setlength{\belowcaptionskip}{-0.01cm}
  \footnotesize
  \centering
  \caption{\small Numerical results of iLPA and nmBDCA for Examples \ref{exam5.1}-\ref{exam5.6}}\label{DCtab1}
  \scalebox{1}{
  \begin{tabular}{c|c|c|c|c|l|c|c|c|c|c}
   \hline
   \multicolumn{1}{c|}{}&\multicolumn{5}{c|}{iPMM}&\multicolumn{4}{c|}{nmBDCA}\\
	\hline
	Example  &$\min \Theta $  &$\max \Theta$ &$ {\rm ave}\ \Theta^{\rm out}$ & Nopt &time(s)&$\min \Theta $&$\max \Theta$ &$ {\rm ave}\ \Theta^{\rm out}$ & Nopt&time(s)\\
	\hline
	5.1    &2.0000     & 2.0000&2.0000 &100 &0.028 & 2.0000     & 2.0000  &  2.0000  &100 &  0.048 \\
	5.2    &1.78e-11  &2.0000 &0.8200 &59  &0.015 & 3.36e-08 & 13.1000 &  2.3150  & 28 &  0.047 \\
	5.3    &1.16e-7   &1.0000 &0.0300 &91  &0.012 & 1.82e-09 & 1.0000  &  0.0200  &98  &  0.024 \\
	5.4    &0.5000     &1.0000 &0.5150 &97  & 0.014& 0.5000     & 1.0000  &  0.7350  &53  &  0.252 \\
	5.5    &3.5000     &3.7500 &3.5225 &91  &0.054 & 3.5000     & 3.9405  &  3.5594  &77  &  0.030 \\
	5.6    &-1.1250   &-1.1250 &-1.1250 &100 &0.032 &-1.1250     &-1.1250  & -1.1250   &100 &  0.048 \\			
	\Xhline{1.5pt}
 \end{tabular}}
 \end{table}

%------------------------------------------------------------------------------------
 \subsection{$\ell_1$-norm penalty of DC constrained problems}\label{sec5.4}
%-------------------------------------------------------------------------------------
 We test the performance of iLPA on DC constrained problems and compare its performance with that of nmBDCA. Consider the following general DC constrained problem
 \begin{equation}\label{DC-constraint}
 	\min_{x\in \Omega}\big\{f(x)\ \ {\rm s.t.}\ \ c_i(x)-d_i(x)\leq0,\ i=1,\ldots,m\big\},
 \end{equation}
 where $\Omega\subset\mathbb{R}^n$ is a closed convex set, all $c_i\!:\mathbb{R}^n\to\mathbb{R}$ and  $d_i\!:\mathbb{R}^n\to\mathbb{R}$ are smooth convex functions, and $f\!:\mathbb{R}^n\to\mathbb{R}$ is continuously differentiable. We apply iLPA and nmBDCA to the $\ell_1$-norm penalized problem of \eqref{DC-constraint} with a fixed penalty factor $\beta$:
\begin{equation}\label{L1-penalty}
 \min_{x\in\mathbb{R}^n} f(x)+\beta\sum_{i=1}^m\max\big\{0,c_i(x)-d_i(x)\big\}+\delta_{\Omega}(x),
\end{equation}
 which takes the form of \eqref{prob} with $\mathbb{X}=\mathbb{R}^n,\mathbb{Y}=\mathbb{R}^m,\mathbb{Z}=\mathbb{R}, \vartheta_1(y):=\beta{\textstyle\sum_{i=1}^m}\max(y_i,0)$, $\vartheta_2(t):=t$, $h(x):=\delta_{\Omega}(x),F(x):=(c_1(x)-d_1(x),\ldots,c_q(x)-d_q(x))^{\top}$ and $G(x):=-f(x)$. The test examples include \textbf{mistake} and \textbf{hs108} from the COCONUT library, \textbf{hesse} from \cite{Ackooij19}, and the ore-processing problem from the Erdenet Mining Corporation (Mongolia) \cite{Strekalovsky18}. The first two examples do not contain the hard constraint $x\in\Omega$, but we impose a soft box set containing their feasible sets. Since nmBDCA is inapplicable to the extended real-valued convex functions, we apply it to the following equivalent form of \eqref{L1-penalty}:
\begin{equation}\label{EL1-penalty}
	\min_{x\in\mathbb{R}^n} \underbrace{\beta\sum_{i=1}^{q'}\max\big\{0,c_i(x)-d_i(x)\big\}+\frac{\mu}{2}\|x\|^2}_{g(x)}-\underbrace {f(x)+\frac{\mu}{2}\|x\|^2}_{h(x)},
\end{equation}
where $\mu>0$ is a constant such that $g$ and $h$ are convex, and $q'\ge q$ is the number of constraints (including the hard constraints for the last two examples).

For this group of examples, the parameters of Algorithm \ref{iLPA} are same as those used in section \ref{sec5.3} except $\gamma_{k,0}=\underline{\gamma}=\min\{\|F'(x^0)\|,100\}$.
We terminate the two solvers at $x^k$ if $\|x^k\!-x^{k-1}\|\le 10^{-6}$ and $\textbf{infea}^k\le 10^{-6}$, where $\textbf{infea}^k\!=\sum_{i=1}^{q'}\max\big\{0,c_i(x^k)-d_i(x^k)\big\}$ is the feasibility violation at the $k$th iterate, or the number of iterates is over $k_{\rm max}=10^3$.

Table \ref{DCtab2} reports the average results of $100$ times running for each example. In each running, the two solvers start from the same random initial point generated by MATLAB command $x^0=20*\textrm{rand(n,1)}-10$. The performance of nmBDCA depends on the choice of the parameter $\mu$ in \eqref{EL1-penalty}. We choose a good one for each problem as far as possible, although it does not necessarily make the corresponding $g$ and $h$ become convex. We see that except for \textbf{hesse}, the number of optimal solutions returned by iLPA is far more than that of optimal solutions given by nmBDCA.

\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\footnotesize
	\centering
	\caption{\small Numerical results of iLPA and nmBDCA for DC constrained problems}\label{DCtab2}
	\scalebox{1}{
	\begin{tabular}{|c|c|c|c|c|c|l|c|c|c|c|c|}
			\hline
			\multicolumn{2}{|c|}{}&\multicolumn{5}{c|}{iPMM}&\multicolumn{5}{c|}{nmBDCA}\\
			\hline
			Problem &$\beta$ & max & ave & Nopt & Infea&time(s) & max & ave & Nopt & Infea & time(s)\\
			\hline
			\textbf{mistake} &$10$     &-0.6572     & -0.9765   &68 &2.857e-4 &0.082 & 0.4593   & -0.9100  & 1  & 6.564e-9 &  0.073 \\
			\textbf{hs108}  &$10$     &-0.4995     &-0.7557    &67 &1.549e-5 &0.055 & -0.3810  & -0.7464  & 4  & 2.129e-6 &  0.092 \\
			\textbf{hesse}   &\ $10^4$ &-36.0000    &-204.975  &0  &5.177e-6 &0.008 &-21.9708  & -188.002 &0  & 5.052e-9 &  0.026 \\
			\textbf{ore} &\ $10^2$ & -0.9198    &-1.0633    &42 &7.549e-6 &0.037 & -0.9167  &-1.0808    &0  & 2.252-11 &  0.038 \\
			\Xhline{1.5pt}
	\end{tabular}}
\end{table}
%--------------------------------------------------------------------------------------------
 \section{Conclusions}\label{sec6}

 For the DC composite optimization problem \eqref{prob}, we have developed an inexact linearized proximal algorithm (iLPA) and established the whole convergence of the generated iterate sequence under Assumptions \ref{ass0}-\ref{ass1} and the KL property of the potential function $\Xi$. The latter holds when $F,G$ and $\vartheta_1,\vartheta_2,h$ are definable in the same o-minimal structure over the real field. If in addition $\Xi$ has the KL property of exponent $p\in(0,1)$, the convergence admits a local superlinear rate for $p\in(0,1/2)$ and a local linear rate for $p=1/2$. The KL property of $\Xi$ with exponent $p\in[1/2,1)$ is shown to hold for polyhedral $F,G$ and some special $\vartheta_1,\vartheta_2$ and $h$ (see Proposition \ref{prop-KL0}). In addition, we have provided a verifiable condition for the KL property of $\Xi$ with exponent $p\in[1/2,1)$ by leveraging such a property of $f$ in \eqref{ffun} and condition \eqref{key-cond}, and its relation with the regularity or quasi-regularity conditions used in \cite{HuYang16} is fully discussed for the case $\vartheta_2\equiv 0$ and $h\equiv 0$.
 Numerical comparions with the subgradient method on matrix completions with outliers and non-uniform sampling and comparions with nmBDCA on DC programs with nonsmooth components and $\ell_1$-norm exact penalty of DC constrained programs confirm the efficiency of the proposed iLPA armed with dPPASN. In our future work, we will focus on algorithms to yield better stationary points for some special $G$.

%--------------------------------------------------------------------------------------------
\bigskip
  \noindent
 {\large\bf Acknowledgements}\ \
  The authors are indebted to Professor Ferreira from Universidade Federal de Goi$\acute{a}$s, for sharing their code used to compute the DC programs with nonsmooth components.


 \begin{thebibliography}{1}
 
    \bibitem{Artacho08}
  {\sc F. J.\ Arag\'{o}n Artacho and M. H.\ Geoffroy},
  {\em Characterization of metric regularity of subdifferential},
  Journal of Convex Analysis, 15(2008): 365-380.
 	
 \bibitem{Ackooij19}
 {\sc W.\ V.\ Ackooij, W.\ d.\ Oliveira},
 {\em Non-smooth DC-constrained optimization: constraint qualification and minimizing methodologies},
 Optimization Method \& Software, 34 (2019): 1029-4937.


 \bibitem{Artacho20}
 {\sc F. J. A.\ Artacho and P. T.\ Vuong},
 {\em The boosted difference of convex functions algorithm for nonsmooth functions},
 SIAM Journal on Optimization, 30 (2020): 980-1006.



 \bibitem{Artacho18}
 {\sc F. J. A.\ Artacho, R. M. T.\ Fleming and P. T.\ Vuong},
 {\em  Accelerating the dc algorithm for smooth functions},
 Mathematical Programming, 169 (2018): 95-118.	


  \bibitem{Attouch09}
 {\sc H.\ Attouch and J.\ Bolte},
 {\em On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
 Mathematical Programming, 116 (2009): 5-16.


 \bibitem{Attouch10}
 {\sc H.\ Attouch, J.\ Bolte, P.\ Redont and A.\ Soubeyran},
 {\em Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the Kurdyka-{\L}ojasiewicz inequality}, Mathematics of Operations Research, 35 (2010): 438-457.

  \bibitem{Auslender10}
 {\sc A.\ Auslender, R.\ Shefi and M.\ Teboulle},
 {\em A moving balls approximation method for a class of smooth constrained minimization problems}, SIAM Journal on Optimization, 20 (2010): 3232-325.


 \bibitem{BaiLi22}
 {\sc S. X.\ Bai, M. H.\ Li, C. W.\ Lu, D. L.\ Zhu and S. E.\ Deng},
 {\em The equivalence of three types of error bounds for weakly and approximately convex functions},
 Journal of Optimization Theory and Application, 194 (2022): 220-245.


 \bibitem{Bolte07}
 {\sc J.\ Bolte, A.\ Daniilidis, A. Lewis and M.\ Shiota},
 {\em Clarke subgradients of stratifiable functions},
 SIAM Journal on Optimization, 18 (2007): 556-572.


  \bibitem{Bolte14}
  {\sc J.\ Bolte, S.\ Sabach and M.\ Teboulle},
  {\em Proximal alternating linearized minimization for nonconvex and nonsmooth problems},
  Mathematical Programming, 146 (2014): 459-494.


 \bibitem{Bolte16}
 {\sc J.\ Bolte and E.\ Pauwels},
 {\em Majorization-minimization procedures and convergence of SQP methods for semi-algebraic and tame programs}, Mathematics of Operations Research, 41 (2016): 442-465.


 \bibitem{Bolte17}
 {\sc J.\ Bolte, T. P.\ Nguyen, J.\ Peypouquet and B. W.\ Suter},
 {\em From error bounds to the complexity of first-order descent methods for convex functions}, Mathematical Programming, 165 (2017): 471-507.


  \bibitem{BS00}
 {\sc J. F.\ Bonnans and A. S.\ Sharpiro},
 {\em Perturbation Analysis of Optimization}, Springer, New York. 2000.

%
% \bibitem{Burke85}
% {\sc J. V.\ Burke},
% {\em Descent methods for composite nondifferentiable optimization problems},
% Mathematical Programming, 33(1985): 260279.

   \bibitem{Burke95}
 {\sc J. V.\ Burke and M. C.\ Ferris},
 {\em  A Gauss-Newton method for convex composite optimization},
 Mathematical Programming, 71 (1995): 179-194.


  \bibitem{Clarke83}
  {\sc F. H.\ Clarke},
  {\em Optimization and Nonsmooth Analysis}, New York, 1983.



  \bibitem{Charisopoulos21}
  {\sc V.\ Charisopoulos, Y. D.\ Chen, D.\ Davis, M.\ Diaz, L. J.\ Ding and D.\ Drusvyatskiy},
  {\em Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence},
  Foundations of Computational Mathematics, 21(2021): 1505-1593.


  \bibitem{Chen12}
  {\sc C. H.\ Chen, B. S.\ He and X. M.\ Yuan},
  {\em  Matrix completion via an alternating direction method},
  IMA Journal of Numerical Analysis, 32 (2012): 227-245.



%  \bibitem{Davis18}
%  {\sc D.\ Davis, D.\ Drusvyatskiy, K. J.\ MacPhee and C.\ Paquette},
%  {\em Subgradient methods for sharp weakly convex functions},
%  Journal of Optimization Theory and Applications, 179(2018): 962-982.


  \bibitem{Dong21}
  {\sc H. B.\ Dong and M.\ Tao},
  {\em On the linear convergence to weak/standard d-stationary
 points of DCA-based algorithms for structured nonsmooth DC programming},
 Journal of Optimization Theory and Applications, 189 (2021): 190-220.


 \bibitem{Fan01}
 {\sc J. Q.\ Fan and R. Z.\ Li},
 {\em Variable selection via nonconcave penalized likelihood and its oracle properties},
 Journal of American Statistics Association, 96 (2001): 1348-1360.

  \bibitem{Fang18}
  {\sc E. X.\ Fang, H.\ Liu, K. C.\ Toh and W. X.\ Zhou},
  {\em Max-norm optimization for robust matrix recovery},
  Mathematical Programming, 167 (2018): 5-35.


 \bibitem{Ferreria21}
 {\sc O. P.\ Ferreria, E. M.\ Santos and J. C. O.\ Souza},
 {\em A boosted DC algorithm for non-differentiable DC components with non-monotone line search}, arXiv.2111.01290, 2021.


 \bibitem{Fletcher82}
 {\sc R.\ Fletcher},
 {\em A model algorithm for composite nondifferentiable optimization problems},
 Mathematical Programming Study, 17 (1982): 67-76.


 \bibitem{Gong13}
 {\sc P. H.\ Gong, C. S.\ Zhang, Z. S.\ Lu, J. H.\ Huang and J. P.\ Ye},
 {\em A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems}, In: International Conference on Machine Learning, 2013: 37-45.


 \bibitem{HuYang16}
 {\sc Y. H.\ Hu, C.\ Li and X. Q.\ Yang},
 {\em On convergence rates of linearized proximal algorithms for convex composite optimization with Applications}, SIAM Journal on Optimization, 26 (2016): 1207-1235.


  \bibitem{Ioffe08}
 {\sc A. D.\ Ioffe and J. V.\ Outrata},
 {\em On metric and calmness qualification conditions in subdifferential calculus},
 Set-Valued Analysis, 16 (2008): 199-227.


 \bibitem{Ioffe09}
 {\sc A. D.\ Ioffe},
 {\em An invitation to tame optimization}, SIAM Journal on Optimization, 19 (2009): 1894-1917.

 \bibitem{Lewis16}
 {\sc A. S.\ Lewis and S. J.\ Wright},
 {\em A proximal method for composite minimization},
 Mathematical Programming, 158 (2016): 501-546.

  \bibitem{Li02}
 {\sc C.\ Li and X. H.\ Wang},
 {\em On convergence of the Gauss-Newton method for convex composite optimization},
 Mathematical Programming, 91 (2002): 349-356.

 \bibitem{Li07}
 {\sc C.\ Li and K. F.\ Ng},
 {\em Majorizing functions and convergence of the Gauss-Newton method for convex composite optimization}, SIAM Journal on Optimization, 18 (2007): 613-642.

 \bibitem{LiPong18}
 {\sc G. Y.\ Li and T. K.\ Pong},
 {\em Calculus of the exponent of Kurdyka-{\L}ojasiewicz inequality and its applications to linear convergence of first-order methods},
 Foundations of Computational Mathematics, 18 (2018): 1199-1232.


 \bibitem{LiZhu20}
 {\sc X.\ Li, Z. H.\ Zhu, A. M. C.\ So and R.\ Vidal},
 {\em Nonconvex robust low-rank matrix recovery},
 SIAM Journal on Optimization, 30 (2020): 660-686.

 \bibitem{LiuPong19}
 {\sc T. X.\ Liu, T. K.\ Pong and A.\ Takeda},
 {\em A refined convergence analysis of pDCAe with applications to simultaneous sparse recovery and outlier detection}, Computational Optimization and Applications, 73 (2019): 69-100.


 \bibitem{LiuPanWY22}
 {\sc R. Y.\ Liu, S. H.\ Pan, Y. Q.\ Wu and X. Q.\ Yang},
 {\em An inexact regularized proximal Newton method for nonconvex and nonsmooth optimization}, arXiv:2209.09119v3, 2022.


 \bibitem{LiuPan22}
 {\sc Y. L.\ Liu and S. H.\ Pan},
 {\em Twice epi-differentiability of a class of non-amenable composite functions}, arXiv:2212.00303, 2022.

 \bibitem{Lu12}
 {\sc Z.\ Lu},
 {\em Sequential convex programming methods for a class of structured nonlinear programming},
 arxiv:1210.3039, 2012.

 \bibitem{Lu19}
 {\sc Z. S.\ Lu, Z. R.\ Zhou and Z.\ Sun},
 {\em Enhanced proximal DC algorithms with extrapolation for a class of structured nonsmooth DC minimization}, Mathematical Programming, 176 (2019): 369-401.

  \bibitem{Meng05}
 {\sc F. W.\ Meng, D. F.\ Sun and G. Y.\ Zhao},
 {\em Semismoothness of solutions to generalized equations and the Moreau-Yosida regularization}, Mathematical Programming, 104 (2005): 561-581.


 \bibitem{Mohammadi20}
 {\sc A.\ Mohammadi and M. E.\ Sarabi},
 {\em Twice epi-differentiability of extended-real-valued functions with applications in composite optimization}, SIAM Journal on Optimization, 30 (2020): 2379-2409.

  % \bibitem{Mohammadi22}
 % {\sc A. Mohammadi},
 % {\em First-order variational analysis of non-amenable composite functions},
 %arXiv preprint arXiv:2204.01191 (2022).


 \bibitem{Mordu94}
 {\sc B. S.\ Morduhovich},
 {\em Generalized differential calculus for nonsmooth and set-valued mappings},
 Journal of Mathematical Analysis and Applications, 183 (1994): 250-288.

  \bibitem{Mordu15}
 {\sc B. S.\ Morduhovich and O. Y.\ Wei},
 {\em Higher-order metric subregularity and its applications},
 Journal of Global Optimization, 63 (2015): 777-795.


 \bibitem{Nguyen17}
 {\sc T. A.\ Nguyen and M. N.\ Nguyen},
 {\em Convergence analysis of a proximal point algorithm for minimizing differences of functions}, Optimization, 66 (2017): 129-147.


 \bibitem{Oliveira19}
 {\sc W. D.\ Oliveira and M. P.\ Tcheou},
 {\em An inertial algorithm for DC programming}, Set-Valued and Variational Analysis, 27 (2019): 895-919.


 \bibitem{Ortega70}
 {\sc J. M.\ Ortega and W. C.\ Rheinboldt},
 {\em Iterative Solution of Nonlinear
 Equations in Several Variables}, Academic Press, New York, 1970.


 \bibitem{Pang17}
 {\sc J. S.\ Pang, M.\ Razaviyayn and A.\ Alvarado},
 {\em Computing B-stationary points of nonsmooth DC programs},
 Mathematics of Operations Research, 42 (2017): 95-118.

 \bibitem{Pauwels16}
 {\sc E.\ Pauwels},
 {\em The value function approach to convergence analysis in composite optimization},
 Operations Research Letters, 44 (2016): 790-795.


  \bibitem{Pham86}
 {\sc D. T.\ Pham and E. B.\ Souad},
 {\em Algorithms for solving a class of nonconvex optimization problems: methods of subgradient}. Mathematics for optimization. Fermat days. North Holland: Elsevier; 85 (1986): 249-270.


 \bibitem{Pham97}
 {\sc D. T.\ Pham and H. A.\ Le Thi},
 {\em Convex analysis approach to DC programming: Theory, algorithms and applications},
 Acta Mathematica Vietnamica, 22 (1997): 289-355.


% \bibitem{Polyak69}
%  {\sc B. T.\ Polyak},
%  {\em Minimization of unsmooth functions},
%  USSR Computational Mathematics and Mathematical Physics, 9(1969): 14-29.

 \bibitem{QiSun93}
 {\sc L. Q.\ Qi and J.\ Sun},
 {\em A nonsmooth version of Newton's method}, Mathematical Programming, 58 (1993): 353-367.

 \bibitem{QianPan22}
 {\sc Y. T.\ Qian and S. H.\ Pan},
 {\em A superlinear convergence iterative framework for Kurdyka-{\L}ojasiewicz optimization and application},  arXiv:2210.12449v2, 2022.


  \bibitem{Roc70}
 {\sc R. T.\ Rockafellar},
 {\em Convex Analysis}, Princeton: Princeton University Press, 1970.

 \bibitem{Roc76}
 {\sc R. T.\ Rockafellar},
 {\em Augmented Lagrangians and applications of the proximal point algorithm in convex programming}, Mathematics of Operations Research, 1 (1976): 97-116.

 \bibitem{Roc21}
 {\sc R. T.\ Rockafellar},
 {\em Advances in convergence and scope of the proximal point algorithm}, Journal of Nonlinear and Convex Analysis, 22 (2021): 2347-2374.


 \bibitem{RW98}
 {\sc R. T.\ Rockafellar and R. J-B.\ Wets},
 {\em Variational analysis}, Springer, 1998.




  \bibitem{Robinson81}
 {\sc S. M.\ Robinson},
 {\em Some continuity properties of polyhedral multifunctions},
 Mathematical Programming Study, 14(1981): 206-214.



 \bibitem{Souza16}
 {\sc J. C. O.\ Souza, P. R.\ Oliveira and A.\ Soubeyran},
 {\em  Global convergence of a proximal linearized algorithm for difference of convex functions},
 Optimization Letters, 10 (2016): 1529-1539.

  \bibitem{Strekalovsky18}
 {\sc A.\ S.\ Strekalovsky and I.\ M.\ Minarchenko},
 {\em A local search method for optimization problem with d.c. inequality constraints},
 Applied Mathematical Modelling, 59 (2018): 229-244.

 \bibitem{Sun03}
{\sc W. Y.\ Sun, R. J. B.\ Sampaio and M. A. B.\ Candido},
{\em Proximal point algorithm for minimization of DC functions}, Journal of Computational Mathematics, 21 (2003): 451-462.

% \bibitem{LeThi14}
% {\sc H. A.\ Le Thi, V. N.\ Huynh and T. P.\ Dinh},
% {\em DC Programming and DCA for General DC Programs}, Advanced Computational Methods for Knowledge Engineering. Advances in Intelligent Systems and Computing, 282(2014): 15-35. 	
 	

 \bibitem{LeTai18}
 {\sc H. A.\ Le Thi and D. T.\ Pham},
 {\em  DC programming and DCA: Thirty years of developments},
 Mathematical Programming, 169 (2018): 5-68.
 	
 	
  \bibitem{LeTai18Jota}
 {\sc H. A.\ Le Thi, V. N.\ Huynh and T.\ Pham Dinh},
 {\em Convergence analysis of difference-of-convex algorithm with subanalytic data}, Journal of Optimization Theory and Applications,
 179 (2018): 103-126. 	

 \bibitem{Toh10}
 {\sc K. C.\ Toh and S.\ Yun},
 {\em An accelerated proximal gradient algorithm for nuclear norm regularized linear
 	least squares problems}, Pacific Journal of Optimization,
 6 (2010): 615-640. 	



  \bibitem{Wen18}
 {\sc B.\ Wen, X.\ Chen and T. K.\ Pong},
 {\em Aproximal difference-of-convex algorithmwith extrapolation},
 Computational Optimization and Applications, 69 (2018): 297-324.

 \bibitem{WuPanBi21}
 {\sc Y. Q.\ Wu, S. H.\ Pan and S. J.\ Bi},
 {\em Kurdyka-{\L}ojasiewicz property of zero-norm composite functions},
 Journal of Optimization Theory and Applications, 188 (2021): 94-112.


 \bibitem{YuLu21}
 {\sc P. R.\ Yu, T. K.\ Pong and Z.\ Lu},
 {\em Convergence rate analysis of a sequential convex programming method with line search for a class of constrained difference-of-convex optimization problems}, SIAM Journal on Optimization, 31 (2021): 2024-2054.

 \bibitem{YuLiPong21}
 {\sc P. R.\ Yu, G. Y.\ Li and T. K.\ Pong},
 {\em Kurdyka-{\L}ojasiewicz exponent via inf-projection},
 Foundations of Computational Mathematics, 22 (2021): 1171-1271.

  \bibitem{Zhang10}
 {\sc C. H.\ Zhang},
 {\em Nearly unbiased variable selection under minimax concave penalty},
 Annals of Statistics, 38(2010): 894-942.

 \bibitem{ZhangPan22}
 {\sc D. D.\ Zhang, S. H.\ Pan, S. J.\ Bi and D. F.\ Sun},
 {\em Zero-norm regularized problems: equivalent surrogates, proximal MM method and statistical error bound}, submited to Computational Optimization and Applications (under second review).

%  \bibitem{ZhangPan22}
% {\sc D. D.\ Zhang, S. H.\ Pan, S. J.\ Bi and D. F.\ Sun},
% {\em  A proximal dual semismooth Newton method for computing zero-norm penalized QR estimator}, arXiv:1907.03435v3, 2021.
%


 \bibitem{ZhaoST10}
{\sc X. Y.\ Zhao, D. F.\ Sun and K. C.\ Toh},
{\em A Newton-CG augmented Lagrangian method for semidefinite programming},
SIAM Journal on Optimization, 20 (2010): 1737-1765.






% \bibitem{Coste99}
% {\sc M.\ Coste},
% {\em An introduction to o-minimal geometry},
% Institut de Recherche Math$\acute{e}$matique de Rennes, 1999.
%
% \bibitem{Dries98}
% {\sc L.\ van den Dries and  C.\ Miller},
% {\em Tame topology and O-minimal structures},
%  Cambridge University Press, 1998.


%  \bibitem{Floudas99}
%  {\sc C.\ Floudas, P.\ Pardalos, C.\ Adjimanand, et al.},
%  {\em Handbook of Test Problems in Local and Global Optimization, 1st ed., Nonconvex Optimization and Its Applications},
%  Springer-Verlag US, Hingham, MA, 33(1999): 5-35.


% \bibitem{LeTai96}
% {\sc H. A.\ Le Thi, T.\ Pham Dinh and L. D.\ Muu},
% {\em Numerical solution for optimization over the efficient set by D.C. optimization algorithms},
% Operations Research Letters, 19(1996): 117-128.


% \bibitem{Facchinei03}
% {\sc F.\ Facchinei and J.\ S.\ Pang},
% {\em Finite-dimensional Variational Inequalities and Complementarity Problems},
%  Springer, New York, 2003.



% \bibitem{Hiriart84}
% {\sc J.\ B.\ Hiriart-Urruty, J.\ J.\ Strodiot and  V.\ H.\ Nguyen},
% {\em Generalized Hessian matrix and second-order optimality conditions for problems with $C^{1,1}$ data},
%  Applied Mathematics and Optimization, 11(1984): 43-56.


%  \bibitem{Nesterov83}
%  {\sc Y.\ Nesterov},
%  {\em A method of solving a convex programming problem with convergence rate $O(1/k^2)$},
%  Soviet Mathematics Doklady, 27(1983): 372-376.


 \end{thebibliography}

\end{document}


