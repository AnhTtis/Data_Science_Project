\documentclass[11pt,a4paper]{article}
\usepackage{}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{makecell,booktabs}
%\usepackage{graphicx}
\usepackage{subfigure}
 \usepackage{epsfig}
 \usepackage{epstopdf}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsbsy,amsmath,latexsym,amsfonts, epsfig, color, authblk, amssymb, graphics, bm, caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cases}
%\usepackage{cite}
\usepackage[colorlinks, citecolor=blue]{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{malgorithm}{Algorithm}[section]
\newtheorem{aalgorithm}{Algorithm}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{condition}{Condition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{append}{Appendix}[section]
\newtheorem{alemma}{Lemma}
\newtheorem{atheorem}{Theorem}
\newtheorem{aproposition}{Proposition}
\newtheorem{aremark}{Remark}
\newtheorem{acorollary}{Corollary}
\newenvironment{aproof}{{\noindent}}{\hfill$\Box$\medskip}
\newenvironment{proof}{{\noindent \bf Proof:}}{\hfill$\Box$\medskip}

\definecolor{lred}{rgb}{1,0.8,0.8}
\definecolor{lblue}{rgb}{0.8,0.8,1}
\definecolor{dred}{rgb}{0.6,0,0}
\definecolor{dblue}{rgb}{0,0,0.5}
\definecolor{dgreen}{rgb}{0,0.5,0.5}

\title{An inexact LPA for DC composite optimization and application to matrix completions with outliers}

\author{Ting Tao\footnote{(\href{mailto:taoting@fosu.edu.cn}{taoting@fosu.edu.cn}) School of Mathematics, Foshan University, Foshan },\ \	
 Ruyu Liu\footnote{(\href{mailto:maruyuliu@mail.scut.edu.cn}{maruyuliu@mail.scut.edu.cn}) School of Mathematics, South China University of Technology, Guangzhou},\ \
 Lianghai Xiao\footnote{(\href{mailto:lxiao@scut.edu.cn}{lxiao@scut.edu.cn})School of Mathematics, South China University of Technology, Guangzhou}\ \ {\rm and}\ \	 	
 Shaohua Pan\footnote{(\href{mailto:shhpan@scut.edu.cn}{shhpan@scut.edu.cn}) School of Mathematics, South China University of Technology}}
%--------------------------------------------------------------------------------------------------
 \begin{document}

 \maketitle

\begin{abstract}
	This paper is concerned with a class of DC composite optimization problems which, as an extension of convex composite optimization problems and DC programs with nonsmooth components, often arises in robust factorization models of low-rank matrix recovery. For this class of nonconvex and nonsmooth problems, we propose an inexact linearized proximal algorithm (iLPA) by computing in each step an inexact minimizer of a strongly convex majorization constructed with a partial linearization of their objective functions, and establish the global convergence of the generated iterate sequence under the Kurdyka-\L\"ojasiewicz (KL) property of a potential function. In particular, by leveraging the composite structure, we provide a verifiable condition for the potential function to have the KL property of exponent $1/2$ at the limit point, so for the iterate sequence to have a local R-linear convergence rate, and clarify its relationship with the regularity used in the convergence analysis of algorithms for convex composite optimization. Finally, our iLPA is applied to a robust factorization model for matrix completions with outliers, and numerical comparison with the Polyak subgradient method confirms its superiority in computing time and quality of solutions.
\end{abstract}

\noindent
{\bf Keywords:}\ DC composite optimization, inexact LPA, global convergence, KL property

%----------------------------------------------------------------------------------------
\section{Introduction}
Let $\mathbb{X},\mathbb{Y}$ and $\mathbb{Z}$ be the Euclidean vector spaces endowed with the inner product $\langle\cdot,\cdot\rangle$ and its induced norm $\|\cdot\|$, and let $\overline{\mathbb{R}}\!:=(-\infty,\infty]$ be the extended real number set. We are interested in the DC composite optimization problem
\begin{equation}\label{prob}
	\min_{x\in\mathbb{X}}\,\Phi(x)\!:=\vartheta_1(F(x))-\vartheta_2(G(x))+h(x),
\end{equation}
where the functions $h\!:\mathbb{X}\to\overline{\mathbb{R}},\vartheta_1\!:\mathbb{Y}\to\mathbb{R},\vartheta_2\!:\mathbb{Z}\to\mathbb{R},F\!:\mathbb{X}\to\!\mathbb{Y}$ and $G\!:\mathbb{X}\to\mathbb{Z}$ satisfy the following basic assumption:
\begin{assumption}\label{ass0}
	\begin{description}
		\item[(i)] $h$ is proper and closed convex, and $\vartheta_1,\vartheta_2$ are convex;
		
		\item[(ii)] $F$ and $G$ are continuously differentiable on an open set containing ${\rm dom}h$, and their differential mappings $F'$ and $G'$ are strictly continuous on this open set;
		
		\item[(iii)] the objective function $\Phi$ is lower bounded on its domain ${\rm dom}\Phi={\rm dom}h$.
	\end{description}
\end{assumption}
For convenience, we always write $\Theta\!:=\Theta_1-\Theta_2$ with $\Theta_1\!:=\vartheta_1\circ F$ and $\Theta_2\!:=\vartheta_2\circ G$.

Problem \eqref{prob}, as an extension of convex composite optimization problems \cite{Burke95,Fletcher82} and DC programs with nonsmooth components \cite{LeTai18,Pham86}, not only provides a unified framework for studying the theory of many important classes of optimization such as amenable optimization, convex conic optimization and convex inclusions (see \cite{BS00,RW98,Mohammadi20}), but also has extensive applications in many fields such as machine learning, statistics, financial optimization and telecommunication (see \cite{LeTai05,LeTai18}). Furthermore, it often appears in the robust factorization model of low-rank matrix recovery:
\begin{align}\label{SCAD-loss}
	\min_{U\in\mathbb{R}^{n_1\times r},V\in\mathbb{R}^{n_2\times r}}\Phi(U,V):=	\vartheta(\mathcal{A}(UV^{\top})-b)+\lambda(\|U\|_{2,1}+\|V\|_{2,1})
\end{align}
where $\lambda>0$ is a regularization parameter, $\|\cdot\|_{2,1}$ denotes the column $\ell_{2,1}$-norm of a matrix, $\mathcal{A}\!:\mathbb{R}^{n_1\times n_2}\to\mathbb{R}^{m}$ is a sampling operator, $b\in\mathbb{R}^{m}$ is an observation vector, and $\vartheta\!:\mathbb{R}^m\to\mathbb{R}$ is a DC function to promote sparsity. Such $\vartheta$ includes the SCAD, the MCP and the capped $\ell_1$-norm (see \cite{Fan01,Gong13,Zhang10}), which are shown to be the equivalent DC surrogates of the zero-norm \cite{ZhangPan22}. Due to the sparsity induced by $\vartheta$, the associated loss term $\vartheta(\mathcal{A}(\cdot)-b)$ can cope with outliers involved in the observation vector well.
%------------------------------------------------------------------------------------------
\subsection{Related works}\label{sec1.1}

Model \eqref{prob} with $\vartheta_2\!\equiv 0$ and $h\!\equiv 0$ is precisely the convex composite optimization problem \cite{Burke95,Fletcher82}. For this class of nonconvex and nonsmooth problems, the Gauss-Newton method is a classical one for which the global quadratic convergence of the iterate sequence was achieved in \cite{Burke95} under the weak sharpness of $C\!:=\mathop{\arg\min}_{y\in\mathbb{Y}}\vartheta_1(y)$ for $\vartheta_1$ and a regularity of its accumulation points on the inclusion $F(x)\in\!C$, and a similar convergence result was obtained under weaker conditions in \cite{Li02,Li07}. Another popular one, allowing $\vartheta_1$ to be extended real-valued and prox-regular, is the linearized proximal algorithm (LPA) proposed in \cite{Lewis16}. This methold in each iterate first performs a trial step by seeking a local optimal solution of a proximal linearized subproblem (which becomes strongly convex if $\vartheta_1$ is convex), and then derives a new iterate from the trial step by an efficient projection and/or other enhancements. Criticality of accumulation points under prox-regularity and identification under partial smoothness was studied in \cite{Lewis16}. Later Hu et al. proposed a globalized LPA by using the backtracking line search (see \cite[Algorithm 17]{HuYang16}), and established the global superlinear convergence of order $\frac{2}{p}$ with $p\in[1,2)$ for the iterate sequence by assuming that a cluster point $\overline{x}$ is a regular point of the inclusion $F(x)\!\in C$ and $C$ is the set of local weak sharp minima of order $p$ for $\vartheta_1$ at $F(\overline{x})$; and Pauwels \cite{Pauwels16} verified the convergence of the iterate sequence for the LPA with a backtracking search for the proximal parameters, under the definability of $F$ and $\vartheta_1$ in the same o-minimal structure of the real field and the twice differentiability of $F$. In addition, for the standard NLP problem, i.e., model \eqref{prob} with $\vartheta_1\!=\delta_{\mathbb{R}_{-}^m}$ (the indicator of nonpositive orthant) and $\vartheta_2\!\equiv 0$, Botle and Pauwels \cite{Bolte16} proved that the iterate sequences generated by the moving balls method \cite{Auslender10}, the penalized SQP method and the extended SQP method converge to a KKT point, if all $F_i$ are semialgebraic and the (generalized)  Mangasarian-Fromovitz constraint qualification (MFCQ) holds.

We find that almost all of the above methods require solving a convex or strongly convex program exactly in each step, which is impractical in computation. Although a practical inexact algorithm was proposed in \cite{Li02} (respectively, \cite{HuYang16}), its convergence analysis restricts a starting point in a neighborhood of a regular point (respectively, a quasi-regular point) of the inclusion $F(x)\!\in C$, and such a regular or quasi-regular point does not necessarily exist. Hence, even for the convex composite optimization problem, it is necessary to develop a globally convergent and practical algorithm.


Problem \eqref{prob} with $F\equiv\mathcal{I}$ and $G\equiv\mathcal{I}$ reduces to a standard DC program with nonsmoooth components. For this class of problems, a well-known method is the DC algorithm (DCA) of \cite{LeTai18,Pham97,Pham86} that in each step linearizes the second DC component to yield a convex subproblem and uses its exact solution to define a new iterate; and another popular one is the proximal linearized method (PLM) of \cite{Nguyen17,Pang17,Souza16,Sun03} which can be regarded as a regularized variant of DCA because the convex subproblems are augmented with a proximal term to prevent tailing-off effect that makes calculations unstable as the iteration progresses. For the DCA, Le Thi et al. \cite{LeTai18Jota} justified the global convergence of the iterate sequence under the assumption that the objective function is subanalytic and continuous relative to its domain, and either of DC components is L-smooth around every critical point; and for the PLM, Nguyen et al. \cite{Nguyen17} achieved the same convergence result under the KL property of the objective function and the L-smoothness of the second DC component, or under the strong KL property of the objective function and the L-smoothness of the first DC component. To accelerate the DCA, Artacho et al. \cite{Artacho20} proposed a boosted DC algorithm (BDCA) with monotone line search by requiring the second DC component to be differentiable, and proved the convergence of the iterate sequence by assuming that the objective function has the strong KL property at critical points and the gradient of the second DC component is strictly continuous around the critical points; and Ferreira et al. \cite{Ferreria21} developed the BDCA with non-monotone line search so that the modified BDCA is applicable to DC programs with nonsmooth DC components, but they did not achieve the convergence of the iterate sequence when DC components are both nonsmooth. For problem \eqref{prob} with $\vartheta_1(t,y)\!:=t+\!\theta(y), F\!\equiv(f;\mathcal{I})$ and $G\equiv\mathcal{I}$, where $\theta\!:\mathbb{X}\to\mathbb{R}$ is convex and $f\!:\mathbb{X}\to\mathbb{R}$ is an L-smooth function, Liu et al. \cite{LiuPong19} showed that the iterate sequence generated by  the PLM with extrapolation is convergent under the KL property of a potential function, which removes the differentiability restriction on the second DC component in the convergence analysis of \cite{Nguyen17,Wen18}. In addition, for problem \eqref{prob} with $\vartheta_1(t,y)\!=t\!+\delta_{\mathbb{R}_{-}^m}(y)$, each $F_i\ (i=0,1,\ldots,m)$ being an L-smooth function and $G\equiv\mathcal{I}$, Yu et al. \cite{YuLu21} studied the monotone line search variant of the sequential convex programming (SCP) method in \cite{Lu12} by combining the idea of the moving balls method and that of the PLM, and proved that the iterate sequence converges to a stationary point of \eqref{prob} under the MFCQ and the KL property of a potential function if all $F_i\ (i=1,\ldots,m)$ are twice continuously differentiable and $\vartheta_2$ is Lipschitz continuously differentiable on an open set containing the stationary point set.

It is worth to point out that most of the above DC algorithms also require solving a (strongly) convex program exactly in each step, which is impractical unless the first DC component is simple. Although an inexact PLM was proposed in \cite{Oliveira19,Souza16},  the convergence of the iterate sequences was not obtained. In addition, for DC programs with the second DC component having a special structure, some enhanced proximal DC algorithms were proposed in  \cite{Dong21,Lu19,Pang17} to seek better d-stationary points, but they are inapplicable to large-scale DC programs such as model \eqref{SCAD-loss} because they need to compute at least one strongly convex program exactly in each step. An inexact enhanced proximal DC algorithm was also proposed in \cite{Lu19}, but the convergence of the iterate sequence was not established. Thus, even for DC programs with nonsmooth components, it is essential to develop an inexact PLM with global convergence.

This work aims to develop a globally convergent and practical algorithm for the DC composite problem \eqref{prob}. Since $F'$ and $G'$ are assumed to be strictly continuous, rather than globally Lipschitz continuous, on ${\rm dom}h$ in Assumption \ref{ass0} (ii), it cannot be reformulated as a DC program. Consequently, the DCAs, PLMs and SCP method mentioned above cannot be directly applied to \eqref{prob}. The term $\theta_2(G(x))$, catering to the outliers involved in the observation for low-rank matrix recovery, also hinders the direct applications of the above inexact LPAs and moving balls methods.
%------------------------------------------------------------------------------------------
\subsection{Main contributions}\label{sec1.2}

The first contribution of this work is to propose an inexact LPA with a global convergence certificate for problem \eqref{prob}, which in each step computes an inexact minimizer of a strongly convex majorization constructed by the linearization of the inner $F$ and $G$ at the current iterate $x^k$ and the concave function $-\vartheta_2$ at $G(x^k)$. We justify that
the generated iterate sequence converges to a stationary point in the sense of Definition  \ref{spoint-def} under Assumptions \ref{ass0}-\ref{ass1} and the KL property of a potential function $\Xi$ defined in \eqref{Xi-fun}, and if in addition the function $\Xi$ has the KL property of exponent $p\in[1/2,1)$ at the stationary point, the convergence has a local R-linear rate for $p=1/2$ and a sublinear rate for $p\in(1/2,1)$. The stationary point in Definition \ref{spoint-def} is stronger than those obtained from the above DCAs, PLMs and SCP method but weaker than the d-stationary point. When $\vartheta_2\equiv 0$ and $h\equiv 0$, our iLPA is an inexact version of the composite Gauss-Newton method in \cite{Pauwels16}, so the obtained convergence results extends that of  \cite{Pauwels16} via a different analysis technique.

The second contribution is to provide a verifiable condition in Proposition \ref{prop-KL} for the KL property of the potential function $\Xi$ with exponent $p\in[1/2,1)$ at any critical point  $(\overline{x},\overline{x},\overline{z},\overline{\mathcal{Q}})$, by leveraging the KL property of an almost separable nonsmooth function with exponent $p\in[1/2,1)$ at $(F(\overline{x}),\overline{x},G(\overline{x}),\overline{z})$ and a condition on the subspace ${\rm Ker}([\nabla F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})])$. When $\vartheta_2\equiv 0$ and $h\equiv 0$, the former is equivalent to requiring that $C$ is a set of local weak sharp minima of order $\frac{1}{1-p}$ for $\vartheta_1$ at $F(\overline{x})$ by \cite[Corollary 2.1]{BaiLi22}, while the latter subspace condition is weaker than the regularity used in \cite{Burke95,HuYang16}. Now there is no direct implication relation between this subspace condition and the quasi-regularity in \cite{HuYang16}; see the discussions in Remark \ref{remark-relation}. 

We apply the iLPA armed with dPPASN (a solver for subproblems) to the robust factorization model \eqref{SCAD-loss} of low-rank matrix recovery. Numerical comparisons with the Polyak subgradient method \cite{Charisopoulos21,LiZhu20} for matrix completions with outliers and non-uniform sampling validate that our iLPA is superior to the subgradient method in terms of the computing time and the relative error for synthetic data or the normalized mean absolute error (NMAE) for the real jester, movie and netflix datasets. Also, for $50000\times 17770$ netflix and $71567\times 10677$ movie-10M test examples, iLPA yields the favorable results in $3100$ seconds on a workstation running on 64-bit Windows Operating System with an Intel Xeon(R) W-2245 CPU 3.90GHz and 128 GB RAM. 
%------------------------------------------------------------------------------------------
\section{Notation and preliminaries}\label{sec2}

Throughout this paper, $\mathbb{S}_{+}$ denotes the set of all self-adjoint positive semidefinite (PSD) linear mappings from $\mathbb{X}$ to $\mathbb{X}$, and $\mathcal{I}$ means an identity mapping. With $\mathcal{Q}\in\mathbb{S}_{+}$,  $\|z\|_{\mathcal{Q}}\!:=\!\sqrt{\langle z,\mathcal{Q}z\rangle}$ for $z\in\mathbb{X}$. For an integer $k\ge 0$, $[k]\!:=\{0,1,\ldots,k\}$.
If $f\!:\mathbb{X}\to\mathbb{R}$ is strictly continuous at $x$, ${\rm lip}f(x)$ means its Lipschitz modulus at $x$. For a set $\Omega\subset\mathbb{Y}$, ${\rm cl}(\Omega)$ is the closure of $\Omega$, ${\rm cone}(\Omega):=\{tx\,|\,x\in \Omega,t\ge 0\}$ means the cone generated by $\Omega$, and $\delta_{\Omega}$ denotes the indicator of $\Omega$. For  $x\in\mathbb{X}$ and $\varepsilon>0$, $\mathbb{B}(x,\varepsilon)$ denotes the closed ball centered at $x$ with radius $\varepsilon>0$. For a linear mapping $\mathcal{A}\!:\mathbb{X}\to\mathbb{Y}$, let
$\mathcal{A}^*\!:\mathbb{Y}\to\mathbb{X}$ denote its adjoint. For a differentiable  $g\!:\mathbb{X}\to\mathbb{Y}$, $\nabla g(x)$ denotes the adjoint of $g'(x)$, the differential mapping of $g$ at $x$, and if $g$ is twice differentiable at $x$, $D^2g(x)$ represents the twice differential mapping of $g$ at $x$, and $D^2g(x)(u,\cdot)$ for $u\in\mathbb{X}$ is a linear mapping from $\mathbb{X}$ to $\mathbb{Y}$. For a proper function $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ and a point $x\in{\rm dom}f$, $\widehat{\partial}\!f(x)$ and $\partial\!f(x)$ respectively denote the regular and basic (limiting) subdifferential of $f$ at $x$.  When $f=\delta_{\Omega}$ for a closed set $\Omega\subset\mathbb{X}$, $\partial \!f(x)=\mathcal{N}_{\Omega}(x)$, the normal cone to $\Omega$ at $x$. A vector $x$ is called a (limiting) critical point of $f$ if $0\in\partial\!f(x)$, and the set of all critical points of $f$ is denoted by ${\rm crit}f$.
%---------------------------------------------------------------------------------------------- 
\begin{definition}\label{Def-subregular}
	A multifunction $\mathcal{F}\!:\mathbb{X}\!\rightrightarrows\mathbb{X}$ is said to be (metrically) $q$-subregular at $(\overline{x},\overline{y})\in{\rm gph}\mathcal{F}$ if there exist $\varepsilon\!>0$ and $\kappa\!>0$ such that for all $x\in\mathbb{B}(\overline{x},\varepsilon)$, 
	\[
	{\rm dist}(x,\mathcal{F}^{-1}(\overline{y}))\le\!\kappa[{\rm dist}(\overline{y},\mathcal{F}(x))]^{q},
	\]
	and it is said to be (metrically) subregular at $(\overline{x},\overline{y})\in{\rm gph}\mathcal{F}$ if $q=1$.
\end{definition}
%-----------------------------------------------------------------------------------------------
\subsection{Stationary points of problem \eqref{prob}}\label{sec2.1}
%---------------------------------------------------------------------------------------------

We first provide a chain rule for the subdifferential of composite functions, improving the result of \cite[Theorem 10.49]{RW98}.
%---------------------------------------------------------------------------------------------
\begin{lemma}\label{chain-rule}
	Let $\Psi(x,z)\!:=f(g(x,z))$ where $f\!:\mathbb{Y}\to\overline{\mathbb{R}}$ is a proper lsc function and $g\!:\mathbb{X}\times\mathbb{X}\to\mathbb{Y}$ is a mapping. Consider any $(\overline{x},\overline{z})\in{\rm dom}f$. Suppose that the multifunction $\mathcal{F}(x,z)\!:=g(x,z)-{\rm dom}f$ is subregular at $((\overline{x},\overline{z}),0)$ and that $f$ is strictly continuous at $g(\overline{x},\overline{z})$ relative to ${\rm dom}f$. Then, $\partial\Psi(\overline{x},\!\overline{z})\!\!\subset\!\! D^*g(\overline{x},\overline{z})\partial\!f(g(\overline{x},\!\overline{z}))$ where $D^*g(\overline{x},\!\overline{z})$ is the coderivative of $g$ at $(\overline{x},\!\overline{z})$. If in addition $g$ is strictly differentiable at $(\overline{x},\overline{z})$ and $\partial\!f(g(\overline{x},\overline{z}))=\widehat{\partial}\!f(g(\overline{x},\overline{z}))$, it holds that
	\(
	\partial\Psi(\overline{x},\overline{z})=\widehat{\partial}\Psi(\overline{x},\overline{z})=\nabla g(\overline{x},\overline{z}){\partial}\!f(g(\overline{x},\overline{z})).
	\)
\end{lemma}
\begin{proof}
	Let $\widetilde{g}(x,z,\alpha)\!:=(g(x,z);\alpha)$ for $(x,z,\alpha)\in\mathbb{X}\times\mathbb{X}\times\mathbb{R}$ and $\overline{\omega}:=g(\overline{x},\overline{z})$. Then, ${\rm epi}\Psi=\widetilde{g}^{-1}({\rm epi}\,f)$. Since $\mathcal{F}$ is subregular at $((\overline{x},\overline{z}),0)$ and $f$ is strictly continuous at $\overline{\omega}$ relative to ${\rm dom}\,f$, by \cite[Proposition 2.1]{LiuPan22}  $\widetilde{F}(x,z,\alpha)\!:=\widetilde{g}(x,z,\alpha)-{\rm epi}\,f$ is subregular at $((\overline{x},\overline{z},\overline{\alpha}),0)$ with $\overline{\alpha}\!:=f(\overline{\omega})$. From \cite[Page 211]{Ioffe08} and \cite[Proposition 2.4]{Mordu94},
	\[
	\mathcal{N}_{{\rm epi}\Psi}(\overline{x},\overline{z},\overline{\alpha})
	\subset D^*\widetilde{g}(\overline{x},\overline{z},\overline{\omega})\mathcal{N}_{{\rm epi}f}(\overline{\omega},\overline{\alpha})
	=\big\{(D^*g(\overline{x},\overline{z})\xi;\tau)\,|\,(\xi,\tau)\in\mathcal{N}_{{\rm epi}f}(\overline{\omega},\overline{\alpha})\big\}.
	\]	
	Together with \cite[Theorem 8.9]{RW98}, we obtain the desired inclusion.
	Since $g$ is strictly differentiable at $(\overline{x},\overline{z})$, we have $D^*g(\overline{x},\overline{z})=\nabla g(\overline{x},\overline{z})$. In addition, by the definition of regular subdifferential (see \cite[Definition 8.3]{RW98}), $\widehat{\partial}\Psi(\overline{x},\overline{z})\supseteq\nabla g(\overline{x},\overline{z})\widehat{\partial}\!f(g(\overline{x},\overline{z}))$. Together with part (i), we obtain the desired equality.
\end{proof}

Recall that ${\rm dom}\vartheta_1=\mathbb{Y}$ and ${\rm dom}\vartheta_2=\mathbb{Z}$. By Lemma \ref{chain-rule} with $f=\vartheta_1$ and $g=F$, $\Theta_1$ is regular with $\partial\Theta_1(x)=\nabla\!F(x)\partial\vartheta_1(F(x))$ at any $x\in\mathbb{X}$, which by \cite[Exercise 10.10]{RW98} implies that $\partial(\Theta_1+h)(x)=\partial\Theta_1(x)+\partial h(x)$ for all $x\in{\rm dom}\,h$,  while by Lemma \ref{chain-rule} with $f=\vartheta_2$ and $g=G$,  $\partial(-\Theta_2)(x)\subset\nabla\! G(x)\partial(-\vartheta_2)(G(x))$ for $x\in\mathbb{X}$. Thus,
\[
\partial\Phi(x)\subset \nabla\!F(x)\partial\vartheta_1(F(x))+\nabla\! G(x)\partial(-\vartheta_2)(G(x))+\partial h(x)\quad\ \forall x\in{\rm dom}\Phi.
\]
This motivates us to introduce the following notion of stationary points for \eqref{prob}.
%-------------------------------------------------------------------------------------------------
\begin{definition}\label{spoint-def}
	A vector $x\in\mathbb{X}$ is called a stationary point of problem \eqref{prob} if
	\[
	0\in\nabla\! F(x)\partial\vartheta_1(F(x))+\nabla\!G(x)\partial(-\vartheta_2)(G(x))+\partial h(x).
	\]
\end{definition}
\begin{remark}\label{remark-spoint}
	By \cite[Corollary 9.21]{RW98} and the strict continuity of $\vartheta_2$, it follows that
	\[
	-\partial(-\vartheta_2)(z)=\partial_{B}\vartheta_2(z):=\big\{\xi\in\mathbb{Z}\,|\,\exists z^k\to z\ {\rm with}\ \nabla\vartheta_2(z^k)\to\xi\big\}.
	\]
	Since $\partial_{B}\vartheta_2(z)$ is usually smaller than $\partial\vartheta_2(z)$, the stationary point in Definition \ref{spoint-def} is stronger than those defined by replacing $\partial(-\vartheta_2)(G(x))$ with $-\partial\vartheta_2(G(x))$, which is precisely the critical point in the DC literature \cite{LeTai18Jota,LiuPong19,Nguyen17} when $G=\mathcal{I}$ and $h\equiv 0$.
\end{remark}
%----------------------------------------------------------------------------------------------
\subsection{KL property of nonsmooth functions}\label{sec2.2}
%----------------------------------------------------------------------------------------------
To introduce the KL property of an extended real-valued function, for any $\eta\in(0,\infty]$, we denote by $\Upsilon_{\!\eta}$ the set consisting of all continuous concave $\varphi\!:[0,\eta)\to\mathbb{R}_{+}$ that is continuously differentiable on $(0,\eta)$ with $\varphi(0)=0$ and $\varphi'(s)>0$ for all $s\in(0,\eta)$.
%-------------------------------------------------------------------------------------------------
\begin{definition}\label{KL-def}
	A proper lsc $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ is said to have the KL property at $\overline{x}\in{\rm dom}\,\partial\!f$ if there exist $\eta\in(0,\infty]$, $\varphi\in\Upsilon_{\!\eta}$ and a neighborhood $\mathcal{U}$ of $\overline{x}$ such that
	\[
	\varphi'(f(x)\!-\!f(\overline{x})){\rm dist}(0,\partial\!f(x))\ge 1
	\quad{\rm for\ all}\ x\in\mathcal{U}\cap\big[f(\overline{x})<f<f(\overline{x})+\eta\big].
	\]
	If $\varphi$ can be chosen as $\varphi(t)=ct^{1-p}$ with $p\in[0,1)$ for some $c>0$, then $f$ is said to have the KL property of exponent $p$ at $\overline{x}$. If $f$ has the KL property (of exponent $p$) at each point of ${\rm dom}\,\partial\!f$, it is called a KL function (of exponent $p$).
\end{definition}
%---------------------------------------------------------------------
\begin{remark}\label{KL-remark}
	{\bf(a)} By \cite[Lemma 2.1]{Attouch10}, a proper lsc function has the KL property at any non-critical point. Thus, to show that a proper lsc $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ is a KL function, it suffices to check its KL property at critical points. From \cite{Bolte07}, definable functions in an o-minimal structure over the real number field are all KL functions, which include semialgebraic functions, global subanalytic functions, and etc. On the calculation of KL exponent for some special classes of KL functions, see the recent works \cite{LiPong18,YuLiPong21,WuPanBi21}.
	
	\noindent
	{\bf(b)} For a weakly convex $f$, its KL property of exponent $p\in(0,1)$ at  $\overline{x}\in{\rm crit}f$ is closely related to the $q$-subregularity of $\partial\!f$ at $(\overline{x},0)\!\in{\rm gph}\partial\!f$ (see \cite[section 2]{LiuPanWY22}). 
\end{remark} 	

To close this section, we state the relation between the function $\Theta_i$ for $i=1,2$ and its local linearization at a point.
%----------------------------------------------------------------------------------------------
\begin{lemma}\label{vtheta1-lemma}
	Consider any $x\!\in\!\mathbb{X}$. There exists $\varepsilon\!>\!0$ such that for any $z\!\in\!\mathbb{B}(x,\!\varepsilon)$,
	\begin{subnumcases}{}\label{Theta1-Lip}
		\big|\Theta_1(z)-\vartheta_1\big(F(x)\!+\!F'(x)(z-x)\big)\big|
		\le\frac{1}{2}{\rm lip}\vartheta_1(F(x)){\rm lip}F'(x)\|z-x\|^2,\\
		\label{Theta2-Lip}
		\big|\Theta_2(z)-\vartheta_2\big(G(x)\!+\!G'(x)(z-x)\big)\big|
		\le\frac{1}{2}{\rm lip}\vartheta_2(G(x)){\rm lip}G'(x)\|z-x\|^2.
	\end{subnumcases}
\end{lemma}
\begin{proof}
	By Assumption \ref{ass0} (i), $\vartheta_1$ is strictly continuous at $F(x)$. There is $\delta>0$ such that
	\begin{equation}\label{ineq-vtheta1}
		|\vartheta_1(y)-\vartheta_1(y')|\le {\rm lip}\vartheta_1(F(x))\|y-y'\|
		\quad\ \forall y,y'\in\mathbb{B}(F(x),\delta).
	\end{equation}	
	By the continuous differentiability of $F$ at $x$, there is $\varepsilon\!>\!0$ such that for any $z\!\in\!\mathbb{B}(x,\!\varepsilon)$,
	\[
	\|F(z)-F(x)\|\le\delta\ \ {\rm and}\ \ \|F'(x)(z-x)\|\le\delta.
	\]
	In addition, since $F'$ is strictly continuous on ${\rm dom}\Theta$, if necessary by shrinking $\varepsilon$, 
	\[
	\|F'(z)-F'(z')\|\le{\rm lip}F'(x)\|z-z'\|\quad\ \forall z,z'\in\mathbb{B}(x,\varepsilon).
	\]
	Now fix any $z\in\mathbb{B}(x,\varepsilon)$. By invoking \eqref{ineq-vtheta1} with $y=F(z)$ and $y'=F(x)+F'(x)(z-x)$,
	\begin{align*}
		|\Theta_1(z)\!-\!\vartheta_1(F(x)+F'(x)(z\!-\!x))|
		&\le {\rm lip}\vartheta_1(F(x))\|F(z)-F(x)-F'(x)(z\!-\!x)\|\\
		&\!=\!{\rm lip}\vartheta_1(F(x))\Big\|\!\int_{0}^{1}[F'(x\!+\!t(z\!-\!x))\!-\!F'(x)](z\!-\!x)dt\Big\|\\
		&\!\le\!\frac{1}{2}{\rm lip}\vartheta_1(F(x)){\rm lip}F'(x)\|z-x\|^2,
	\end{align*}	
	where the last inequality is due to Assumption \ref{ass0} (ii) and $x\!+\!t(z\!-\!x)\in\mathbb{B}(x,\varepsilon)$ for all $t\in[0,1]$.
	By the arbitrariness of $z\in\mathbb{B}(x,\varepsilon)$, we obtain inequality \eqref{Theta1-Lip}. Using the same arguments yields inequality \eqref{Theta2-Lip}. Here, we omit the details.
\end{proof}
%-------------------------------------------------------------------------------------------------
\section{Inexact linearized proximal algorithm}\label{sec3}

For any $x,z\in\mathbb{X}$, define 
\begin{equation}\label{LinearFG}
	\ell_{F}(x,s):=F(x)+F'(x)(s-x)\ \ {\rm and}\ \ \ell_{G}(x,s):=G(x)+G'(x)(s-x).
\end{equation}
Clearly, $\ell_{F}(x,\cdot)\!:\mathbb{X}\to\mathbb{Y}$ and $\ell_{G}(x,\cdot)\!:\mathbb{X}\to\mathbb{Z}$ are the linear approximation of $F$ and $G$ at $x$. Let $x^k$ be the current iterate. By \eqref{Theta1-Lip} with $x=x^k$, for any $x$ close to $x^k$,
\begin{equation}\label{theta1-ineq0}
	\Theta_1(x)\le\vartheta_1(F(x^k)\!+\!F'(x^k)(x-x^k))+\frac{1}{2}{\rm lip}\vartheta_1(F(x^k)){\rm lip}F'(x^k)\|x-x^k\|^2.
\end{equation}
Pick any $\xi^k\!\!\in\!\partial(\!-\!\vartheta_2\!)(\!G(x^k\!))$. By \eqref{Theta2-Lip} and the concavity of $-\!\vartheta_2$, for any $x\!$ close to $x^k$,
\begin{align}\label{theta2-ineq0}
	-\Theta_2(x)
	&\le-\!\vartheta_2(G(x^k)\!+\!G'(x^k)(x\!-x^k))+\!\frac{1}{2}{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)\|x\!-\!x^k\|^2\\
	\label{theta2-ineq1}
	&\!\le\! -\vartheta_2(G(x^k))\!+\!\langle\xi^k,G'(x^k)(x\!-\!x^k)\rangle\!+\!\frac{1}{2}{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)\|x\!-\!x\|^2.
\end{align}
For each $k\in\mathbb{N}$, write ${\rm lip}\Theta(x^k)\!:=
{\rm lip}\vartheta_1(F(x^k)){\rm lip}F'(x^k)+{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)$. By using inequalities \eqref{theta1-ineq0} and \eqref{theta2-ineq1} and the expression of $\Phi$, for any $x$ close to $x^k$,
\[
\Phi(x)\le \vartheta_1(\ell_{F}(x^k,x))+\langle\nabla G(x^k)\xi^k,x\!-\!x^k\rangle+h(x)\!+\!\frac{1}{2}{\rm lip}\Theta(x^k)\|x-x^k\|^2\!-\!\Theta_2(x^k).
\]
Thus, by choosing a self-adjoint positive definite (PD) linear operator $\mathcal{Q}_k\!:\!\mathbb{X}\!\to\!\mathbb{X}\!$ with $\mathcal{Q}_k\succeq{\rm lip}\Theta(x^k)\mathcal{I}$, we get the following local strong convex majorization of $\Phi$ at $x^k$:
\[
q_{k}(x):=\vartheta_1(\ell_{F}(x^k,x))+\langle\nabla G(x^k)\xi^k,x\!-\!x^k\rangle+h(x)\!+\!\frac{1}{2}\|x-x^k\|_{\mathcal{Q}_k}^2\!-\!\Theta_2(x^k).
\]
In each step, our inexact LPA constructs a local majorization $q_k$ of $\Phi$ and then seeks an approximate minimizer of $q_k$ as the next iterate. Its iterate steps are as follows.
%--------------------------------------------------------------------------------------------------------
\begin{algorithm}[h]
	\caption{\label{iLPA}{\bf\,(Inexact linearized proximal algorithm)}}
	\textbf{Initialization:} Choose $\varrho>1,\,0<\underline{\gamma}<\overline{\gamma},\,0<\mu<\underline{\gamma}/4$ and $x^0\in{\rm dom}\Phi$. Set $k:=0$.\\
	%-----------------------------------------------------------------------------
	\textbf{while} some stopping criterion is not satisfied \textbf{do}
	\begin{enumerate}
		\item  Choose $\xi^{k}\in\partial(-\vartheta_2)(G(x^k))$ and $\gamma_{k,0}\in[\underline{\gamma},\overline{\gamma}]$.
		
		\item {\bf For $j=0,1,2,\ldots$}
		\begin{enumerate}
			\item Choose a self-adjoint PD linear operator $\mathcal{Q}_{k,j}\!:\mathbb{X}\to\mathbb{X}$ with
			$\gamma_{k,j}\mathcal{I}\preceq\mathcal{Q}_{k,j}\preceq\varrho\gamma_{k,j}\mathcal{I}$. 
			Seek an inexact minimizer $x^{k,j}$ the subproblem
			\begin{align}\label{subprobkj}
				\min_{x\in\mathbb{X}}q_{k,j}(x)&:=\vartheta_1(\ell_{F}(x^k,x))+\!\langle\nabla G(x^k)\xi^k,x-x^k\rangle \nonumber\\
				&\qquad +h(x)\!+\!(1/2)\|x-x^k\|_{\mathcal{Q}_{k,j}}^2\!-\Theta_2(x^k\!)
			\end{align}
			such that
			$q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})\le\frac{1}{2}\mu\|x^{k,j}\!-x^{k}\|^2$, where $\overline{x}^{k,j}$ is the unique optimal solution of subproblem \eqref{subprobkj}.
			
			\item If $\Theta(x^{k,j})\le\vartheta_1\big(\ell_{F}(x^k,x^{k,j})\big)-\vartheta_2\big(\ell_{G}(x^k,x^{k,j})\big)+\frac{1}{2}\|x^{k,j}\!-x^k\|_{\mathcal{Q}_{k,j}}^2$, then set $j_k:=j$ and go to step 4; else let $\gamma_{k,j+1}=\varrho^{j+1}\gamma_{k,0}$.
		\end{enumerate}
		
		\item  {\bf End (for)}.
		
		\item  Set $x^{k\!+\!1}\!=\!x^{k,j_k},\overline{x}^{k+1}\!=\!\overline{x}^{k,j_k}$ and $\mathcal{Q}_{k}\!=\!\mathcal{Q}_{k,j\!_k}$. Let $k\!\leftarrow\! k\!+\!1$ and return Step 1.
	\end{enumerate}
	\textbf{end while}
\end{algorithm}
\begin{remark}\label{remark-alg}
	{\bf(i)} The inner for-end loop in Algorithm \ref{iLPA} aims to seek a desired estimation for ${\rm lip}\Theta(x^k)$. As will be shown in Lemma \ref{ls-welldef1} below, the inner loop stops within a finite number of iterates, so Algorithm \ref{iLPA} is well defined. Moreover, from $\overline{x}^{k,j}\in{\rm dom}h$ and the inexact criterion in Step 2(a), 
	$x^{k,j}\in{\rm dom}h$, so $\{x^k\}_{k\in\mathbb{N}}\subset{\rm dom}h$.
	
	\noindent
	{\bf(ii)} Algorithm \ref{iLPA} is an extension of the Gauss-Newton method \cite{Pauwels16} and the inexact LPA \cite[Algorithm 19]{HuYang16}, both proposed for problem \eqref{prob} with $\vartheta_2\equiv 0$ and $h\equiv 0$. Compared with the Gauss-Newton method \cite{Pauwels16}, Algorithm \ref{iLPA} is practical because its subproblems are allowed to be solved inexactly and the inexact criterion  $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})\le\frac{\mu}{2}\|x^{k,j}-x^{k}\|^2$ is implementable by noting that the unknown $q_{k,j}(\overline{x}^{k,j})$ can be replaced by its lower estimation, say, the dual objective value of \eqref{subprobkj}. Different from the inexact LPA of \cite{HuYang16}, our inexact criterion controls $q_{k,j}(x^{k,j})-q_{k,j}(\overline{x}^{k,j})$ by the quadratic term $\|x^{k,j}\!-\!x^{k}\|^2$ rather than $\|x^{k-1}\!-\!x^{k}\|^{\alpha}$ with $\alpha>2$.
	In addition, Algorithm \ref{iLPA} also extends the proximal DC algorithms proposed in \cite{LiuPong19} for \eqref{prob} with $F\equiv(f;\mathcal{I})$ and $G\equiv\mathcal{I}$ without requiring $\nabla\!f$ to be globally Lipschitz continuous.
	
	\noindent
	{\bf(iii)} When $x^{k}\!=\!x^{k-1}$, from Step (2a) and the strong convexity of subproblem \eqref{subprobkj}, we deduce that $x^k\!=\!\overline{x}^k$. Together with Remark \ref{remark-spoint} and Proposition \ref{prop3-xk} later, it follows that $x^{k\!-\!1}\!=\!\overline{x}^k$ is a stationary point of problem \eqref{prob}. This implies that $\|x^k\!-\!x^{k\!-\!1}\|\le\epsilon$ with a tiny $\epsilon\!>0$ is an appropriate termination condition for Algorithm \ref{iLPA}.
\end{remark}
%-----------------------------------------------------------------------------------------
\begin{lemma}\label{ls-welldef1}
	Under Assumption \ref{ass0}, for each $k\in\mathbb{N}$, the inner loop of Algorithm \ref{iLPA} stops within a finite number of steps.
\end{lemma}
\begin{proof}
	Fix any $k\in\mathbb{N}$. Suppose on the contradiction that the inner loop of Algorithm \ref{iLPA} does not stop within a finite number of steps, i.e., for each $j\in\mathbb{N}$,
	\begin{equation}\label{aim-ineq1}
		\Theta(x^{k,j})-\vartheta_1(\ell_{F}(x^k,x^{k,j}))+\vartheta_2(\ell_{G}(x^k,x^{k,j}))>(1/2)\|x^{k,j}-x^k\|_{\mathcal{Q}_{k,j}}^2.
	\end{equation}
	From the definition of $x^{k,j}$ in the inner loop and the expression of $q_{k,j}$, for each $j\in\mathbb{N}$,
	\begin{align*}
		q_{k,j}(\overline{x}^{k,j})+(\mu/2)\|x^{k,j}\!-\!x^{k}\|^2
		&\ge\vartheta_1(\ell_{F}(x^k,x^{k,j}))\!+\!\langle\nabla G(x^k)\xi^k,x^{k,j}\!-\!x^k\rangle\\
		&\quad+h(x^{k,j})+(1/2)\|x^{k,j}\!-\!x^k\|_{\mathcal{Q}_{k,j}}^2-\Theta_2(x^k).
	\end{align*}
	Note that ${\rm ri}({\rm dom}h)\ne\emptyset$ by the properness and convexity of $h$, so  there exists $\widehat{x}\in{\rm ri}({\rm dom}h)$ such that $\partial h(\widehat{x})\ne\emptyset$. Pick any $\widehat{v}\in\partial h(\widehat{x})$. Then, $h(x^{k,j})\ge h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle$ for each $j\in\mathbb{N}$. In addition, from the finite convexity of $\vartheta_1$, $\partial\vartheta_1(F(\widehat{x}))\ne\emptyset$. Pick any $\widehat{\zeta}\in\partial\vartheta_1(F(\widehat{x}))$. Then,
	$\vartheta_1(\ell_{F}(x^k,x^{k,j}))\ge\vartheta_1(F(\widehat{x}))+\langle\widehat{\zeta},\ell_{F}(x^k,x^{k,j})-F(\widehat{x})\rangle$ for each $j\in\mathbb{N}$. Together with the last inequality, it follows that for all $j\in\mathbb{N}$,
	\begin{align*}
		q_{k,j}(\overline{x}^{k,j})
		&\ge \vartheta_1(F(\widehat{x}))+\langle\nabla\!F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k, x^{k,j}-x^k\rangle
		+h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle\\
		&\quad+\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle+\frac{1}{2}\|x^{k,j}-x^k\|_{Q_{k,j}}^2
		-\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2-\!\Theta_2(x^k).
	\end{align*} 	
	Note that $\Phi(x^k)=q_{k,j}(x^k)\ge q_{k,j}(\overline{x}^{k,j})$ for each $j\in\mathbb{N}$. Then, it holds that
	\begin{align*}
		\Phi(x^k)
		&\ge \vartheta_1(F(\widehat{x}))+\langle\nabla\!F(x^k)\widehat{\zeta}+\nabla G(x^k)\xi^k, x^{k,j}-x^k\rangle
		+h(\widehat{x})+\langle\widehat{v},x^{k,j}-\widehat{x}\rangle\\
		&\quad+\langle\widehat{\zeta},F(x^k)-F(\widehat{x})\rangle+\frac{1}{2}\|x^{k,j}-x^k\|_{Q_{k,j}}^2
		-\frac{\mu}{2}\|x^{k,j}\!-\!x^{k}\|^2-\!\Theta_2(x^k).
	\end{align*}
	Recall that $\mathcal{Q}_{k,j}\succeq\gamma_{k,0}\mathcal{I}\succeq\underline{\gamma}\mathcal{I}$ and $\underline{\gamma}>4\mu$. The last inequality implies that $x^{k,j}\!\to \!x^k$ as $j\!\to\!\infty$. By invoking Lemma \ref{vtheta1-lemma} with $z\!=\!x^{k,j}$ and $x\!=\!x^k$, for all sufficiently large $j$,
	\begin{subnumcases}{}
		\vartheta_1(F(x^{k,j}))-\vartheta_1\big(\ell_{F}(x^k,x^{k,j})\big)
		\le \frac{1}{2}{\rm lip}\vartheta_1(F(x^k)){\rm lip}F'(x^k)\|x^{k,j}\!-x^k\|^2,\nonumber\\
		-\vartheta_2(G(x^{k,j}))
		+\vartheta_2(\ell_{G}(x^k,x^{k,j}))\le\frac{1}{2}{\rm lip}\vartheta_2(G(x^k)){\rm lip}G'(x^k)\|x^{k,j}\!-x^k\|^2.\nonumber
	\end{subnumcases}
	Adding the last two inequalities together leads to
	\begin{equation*}
		\Theta(x^{k,j})-\vartheta_1\big(\ell_{F}(x^k,x^{k,j})\big)+\vartheta_2\big(\ell_{G}(x^k,x^{k,j})\big)\le\frac{1}{2}{\rm lip}\Theta(x^k)\|x^{k,j}-x^k\|^2,
	\end{equation*}
	which clearly contradicts \eqref{aim-ineq1} because ${\rm lip}\Theta(x^k)\mathcal{I}\prec\mathcal{Q}_{k,j}$ when $j$ is large enough.
\end{proof}
%--------------------------------------------------------------------------------------------
\section{Convergence analysis of Algorithm \ref{iLPA}}\label{sec4}
%------------------------------------------------------------------------------------------
Write $\mathbb{W}:=\mathbb{X}\times\mathbb{X}\times\mathbb{Z}\times\mathbb{S}_{+}$. Inspired by the work \cite{LiuPong19}, for each $w=(x,s,z,\mathcal{Q})\in\mathbb{W}$, we define the function 
\begin{equation}\label{Xi-fun}
	\Xi(w):=\vartheta_1(\ell_{F}(x,s))+\langle \ell_{G}(x,s),z\rangle+h(s)
	+\vartheta_2^*(-z)+\|s-x\|_{\mathcal{Q}}^2.
\end{equation}
This potential function will play a key role in the subsequent convergence analysis. The following lemma characterizes the subdifferential of $\Xi$ at any $w\in{\rm dom}\,\Xi$.
%--------------------------------------------------------------------------------------------
\begin{lemma}\label{subdiff-LemXi}
	Fix any $w=(x,s,z,\mathcal{Q})\in{\rm dom}h\times{\rm dom}h\times(-{\rm dom}\vartheta_2^*)\times\mathbb{S}_{+}$. Suppose that $\vartheta_2^*$ is strictly continuous at $-z$ relative to ${\rm dom}\vartheta_2^*$,  that $F'$ is strictly differentiable at $x$, and that $G'$ is differentiable at $x$. Then, $\partial\Xi(w)=T_1(w)\times T_2(w)\times T_3(w)$ with
	\begin{align*}
		T_1(w)&=\left(\begin{matrix}
			[D^2F(x)(s\!-\!x,\cdot)]^*\\
			\nabla\!F(x)
		\end{matrix}\right)\partial\vartheta_1(\ell_{F}(x,s))
		\!+\!\left(\begin{matrix}
			[D^2G(x)(s\!-\!x,\cdot)]^*z+2\mathcal{Q}(x\!-\!s)\\
			\nabla G(x)z+\partial h(s)+2\mathcal{Q}(s\!-\!x)
		\end{matrix}\right),\\
		T_2(w)&=\ell_{G}(x,s)-\partial\vartheta_2^*(-z)\ \ {\rm and}\ \ T_3(w)=(s-x)(s-x)^{\top}+	 \mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}).	 	
	\end{align*}
\end{lemma}
\begin{proof}
	Write $\psi(x',s')\!:=\vartheta_1(\ell_{F}(x',s'))$ for $(x',s')\in\mathbb{X}\times\mathbb{X}$. Define the functions
	\[
	f(w')\!:=\psi(x',s')+h(s')+\vartheta_2^*(-z')+\delta_{\mathbb{S}_{+}}(\mathcal{Q}')\ {\rm and}\   \Upsilon(w')\!:=\langle\ell_{G}(x',s'),z'\rangle+\|s'\!-\!x'\|_{\mathcal{Q}'}^2
	\]
	for $w'=(x',s',z',\mathcal{Q}')\in\mathbb{W}$. Clearly, $\Xi=f+\Upsilon$. Recall that $\vartheta_1$ is strictly continuous at $\ell_{F}(x,s)$ and ${\rm dom}\vartheta_1\!=\!\mathbb{Y}$. By Lemma \ref{chain-rule},  $\widehat{\partial}\psi(x,s)\!=\!\partial\psi(x,s)\!=\!\nabla\ell_{F}(x,s)\partial\vartheta_1(\ell_{F}(x,s))$. Together with the expression of $f$ and \cite[Exercise 10.10]{RW98}, it follows that
	\[
	\partial\!f(w)=\widehat{\partial}\!f(w)
	=\big[\partial\psi(x,s)+\{0\}\times\partial h(s)\big]
	\times[-\partial\vartheta_2^*(-z)]\times\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}).
	\]
	This, along with the differentiablility of $\Upsilon$ at $w$ and \cite[Exercise 10.10]{RW98}, implies that
	\[
	\partial\Xi(w)=\partial\!f(w)+\nabla\Upsilon(w).
	\]
	By the expression of $\Upsilon$ and $\partial\psi(x,s)=\nabla\ell_{F}(x,s)\partial\vartheta_1(\ell_{F}(x,s))$, we get the result.
\end{proof}

In the rest of this section, for each $k\in\mathbb{N}$, write $w^{k}\!:=(x^{k-1},\overline{x}^{k},\xi^{k-1},\mathcal{Q}_{k-1})$, and for a set $\Gamma\subset\mathbb{W}$, denote by $\Gamma_{\!x}$ and $\Gamma_{\!z}$ the projection of $\Gamma$ onto $\mathbb{X}$ and $\mathbb{Z}$, respectively, i.e., $x\in\Gamma_{\!x}$ if and only if there is $(s,z,\mathcal{Q})\in\mathbb{X}\times\mathbb{Z}\times\mathbb{S}_{+}$ such that $(x,s,z,\mathcal{Q})\in\Gamma$. 
%------------------------------------------------------------------------------------------
\subsection{Global convergence of Algorithm \ref{iLPA}}\label{sec4.1}
%-----------------------------------------------------------------------------------------------
For each $k\in\mathbb{N}$, from the convexity of $\vartheta_2$ and $\xi^k\!\in\partial(-\vartheta_2)(G(x^k))\subset-\partial\vartheta_2(G(x^k))$, it follows that
\begin{subnumcases}{}
	\label{pre-equa0}	
	\vartheta_2(G(x^{k-1}))+\vartheta_2^*(-\xi^{k-1})=-\langle\xi^{k-1},G(x^{k-1})\rangle,\\
	\label{pre-equa1}  	
	-\vartheta_2(\ell_{G}(x^{k-1},x^k))\le-\!\vartheta_2(G(x^{k-1}))\!+\!\langle\nabla G(x^{k-1})\xi^{k-1},x^{k}\!-\!x^{k-1}\rangle.
\end{subnumcases}
By the definitions of $x^k$ and $\overline{x}^k$ and the strong convexity of $q_{k,j_k}$, for each $k\in\mathbb{N}$,
\begin{equation}\label{pre-equa3}
	\Phi(x^k)=q_{k,j_k}(x^k)\ge q_{k,j_k}(\overline{x}^{k+1})+(1/2)\|\overline{x}^{k+1}-x^k\|_{\mathcal{Q}_k}^2.
\end{equation}
We first use \eqref{pre-equa0}-\eqref{pre-equa3} to establish the convergence of $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$.

%-----------------------------------------------------------------------------------------
\begin{proposition}\label{prop1-xk}
	Let $\{\!w^k\!\}_{k\!\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Then, under Assumption \ref{ass0}, the following assertions hold:
	\begin{itemize}
		\item [(i)] For each $k\in\mathbb{N}$, $\Xi(w^{k+1})\le\Xi(w^{k})-(\underline{\gamma}/4-\mu)\|x^k-x^{k-1}\|^2$.
		
		\item[(ii)] For each $k\in\mathbb{N}$, $\Phi(x^k)+(\underline{\gamma}/4-\mu)\|x^k-x^{k-1}\|^2\le\Xi(w^k)\leq \Phi(x^{k-1})$.
		
		\item[(iii)] The sequences $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ and $\{\Xi(w^{k})\}_{k\in\mathbb{N}}$ are decreasing and convergent.
	\end{itemize}
\end{proposition}
\begin{proof}
	{\bf(i)} Fix any $k\in\mathbb{N}$. By step 2(b) of Algorithm \ref{iLPA} and inequality \eqref{pre-equa1}, 
	\begin{align*}%\label{descent-eq1}
		\Theta(x^{k})
		&\le\vartheta_1(\ell_{F}(x^{k-1},x^k))-\vartheta_2(\ell_{G}(x^{k-1},x^k))+(1/2)\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
		&\le\!\vartheta_1(\ell_{F}(x^{k-1},x^k))\!-\!\Theta_2(x^{k-1})\!+\!
		\langle\nabla G(x^{k-1})\xi^{k\!-\!1},x^k\!-\!x^{k\!-1}\rangle\!+\!\frac{1}{2}\|x^k\!-\!x^{k\!-\!1}\|_{\mathcal{Q}_{k\!-1}}^2.
	\end{align*}
	In addition, from equation \eqref{pre-equa3} and the expression of $q_{k,j_k}$, it follows that
	\[
	\Phi(x^{k})
	\ge\vartheta_1(\ell_{F}(x^k,\overline{x}^{k+1}))+\langle\nabla G(x^k)\xi^k,\overline{x}^{k+1}\!-\!x^k\rangle+h(\overline{x}^{k+1})+\|\overline{x}^{k+1}\!-\!x^k\|_{\mathcal{Q}_k}^2-\Theta_2(x^k).
	\]
	Combining the last two inequalities with $\Phi(x^{k})=\Theta(x^{k})+h(x^k)$ yields that
	\begin{align*}
		&\vartheta_1(\ell_{F}(x^k,\overline{x}^{k+1}))+\langle\nabla G(x^k)\xi^k,\overline{x}^{k+1}\!-\!x^k\rangle+h(\overline{x}^{k+1})+\|\overline{x}^{k+1}\!-\!x^k\|_{\mathcal{Q}_k}^2\\
		&\le \vartheta_1\big(\ell_{F}(x^{k-1},x^k)\big)+\langle\nabla G(x^{k-1})\xi^{k-1},x^k\!-\!x^{k-1}\rangle+h(x^k)\\
		&\quad+\vartheta_2(G(x^k))-\!\vartheta_2(G(x^{k-1}))+(1/2)\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
	\end{align*}
	Together with the expression of $\Xi$ and equality \eqref{pre-equa0}, it then follows that
	\begin{align}
		\Xi(w^{k+1})&\le\vartheta_1\big(\ell_{F}(x^{k-1},x^k)\big)+\langle\nabla G(x^{k-1})\xi^{k-1},x^k\!-\!x^{k-1}\rangle+h(x^k)\nonumber\\
		&\quad-\!\vartheta_2(G(x^{k-1}))+\frac{1}{2}\|x^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\nonumber\\
		\label{final-ineq400}
		&=q_{k-1,j_{k-1}}(x^{k})\le q_{k-1,j_{k-1}}(\overline{x}^k)+\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2\\
		&=\vartheta_1\big(\ell_{F}(x^{k-1},\overline{x}^k)\big)
		+\langle\nabla G(x^{k-1})\xi^{k-1},\overline{x}^k\!-\!x^{k-1}\rangle  -\!\vartheta_2(G(x^{k-1}))\nonumber\\
		&\quad+h(\overline{x}^k)+\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2+\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2\nonumber\\
		\label{temp-ineq40}
		&=\Xi(w^{k})-\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2+\frac{\mu}{2}\|x^{k}-x^{k-1}\|^2,
	\end{align}
	where the second inequlity is using step (2a) of Algorithm \ref{iLPA} and the expression of $q_{k-1,j_{k-1}}$, and the last equality is due to \eqref{pre-equa0} and the expression of function $\Xi$.	For the term $\Delta_k\!:=\!\frac{1}{2}\|\overline{x}^k\!-\!x^{k\!-\!1}\|_{\mathcal{Q}_{k\!-\!1}}^2$ in \eqref{temp-ineq40}, using Cauchy-Schwarz inequality yields that
	\begin{align*}
		\Delta_k
		&\ge\frac{1}{2}\|\overline{x}^k\!-\!x^{k}\|_{\mathcal{Q}_{k-1}}^2+\frac{1}{2}\|x^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-\|\overline{x}^k\!-\!x^{k}\|_{\mathcal{Q}_{k-1}}^2-\frac{1}{4}\|{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
		&=-\frac{1}{2}\|\overline{x}^k\!-\!x^{k}\|_{\mathcal{Q}_{k-1}}^2+\frac{1}{4}\|{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
	\end{align*}
	In addition, by the strong convexity of $q_{k-1,j_{k-1}}$ and inequality \eqref{final-ineq400}, it holds that
	\begin{equation}\label{key-ineq40}
		\frac{1}{2}\|\overline{x}^k-x^{k}\|_{\mathcal{Q}_{k-1}}^2
		\le q_{k-1,j_{k-1}}(x^{k})-q_{k-1,j_{k-1}}(\overline{x}^{k})
		\le \frac{\mu}{2}\|x^{k}-x^{k-1}\|^2.
	\end{equation}
	Then, from the last two inequalities, we immediately obtain that
	\begin{equation}\label{key-ineq41}
		\Delta_k=\frac{1}{2}\|\overline{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2  \ge-\frac{\mu}{2}\|{x}^{k}-x^{k-1}\|^2+\frac{1}{4}\|{x}^k-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2.
	\end{equation}
	Substituting \eqref{key-ineq41} into \eqref{temp-ineq40} and noting that $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ leads to the desired inequality.
	
	\noindent
	{\bf(ii)} Fix any $k\in\mathbb{N}$. From the last two equalities in \eqref{temp-ineq40} and inequality \eqref{pre-equa3}, 
	\[
	\Xi(w^{k})
	=q_{k-1,j_{k-1}}(\overline{x}^{k})+(1/2)\|\overline{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\leq \Phi(x^{k-1}).
	\]
	In addition, from inequality \eqref{final-ineq400}, it follows that
	\begin{align*}
		q_{k-1,j_{k-1}}(\overline{x}^{k})
		&\ge\vartheta_1(\ell_{F}(x^{k-1},{x}^{k}))\!-\!\vartheta_2(G(x^{k-1}))\!+\!\langle\nabla G(x^{k-1})\xi^{k-1},x^{k}\!-\!x^{k-1}\rangle\\
		&\quad+h(x^k)+\!(1/2)\|x^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2-(\mu/2)\|{x}^{k}\!-\!x^{k-1}\|^2\\
		&\ge\Phi(x^k)-(\mu/2)\|{x}^{k}\!-\!x^{k-1}\|^2,
	\end{align*}
	where the last inequality is using \eqref{pre-equa1} and step (2b). From the last two inequalities,
	\[
	\Phi(x^k)-({\mu}/{2})\|{x}^{k}\!-\!x^{k-1}\|^2+({1}/{2})\|\overline{x}^k\!-\!x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\le\Xi(w^{k})\le\Phi(x^{k-1}),
	\]
	which together with \eqref{key-ineq41} and $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ implies the desired inequality.
	
	\noindent
	{\bf(iii)} The decreasing of the sequences $\{\Xi(w^k)\}_{k\in\mathbb{N}}$ and $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ are due to parts (i)-(ii) and $\mu<\underline{\gamma}/4$. By Assumption \ref{ass0} (iii), the sequence $\{\Phi(x^k)\}_{k\in\mathbb{N}}$ is lower bounded, so is $\{\Xi(w^{k})\}_{k\in\mathbb{N}}$ by part (ii). Consequently, they are both convergent.
\end{proof}

To establish the convergence of $\{w^k\}_{k\in\mathbb{N}}$, we need the following assumption: 
%-----------------------------------------------------------------------------------------
\begin{assumption}\label{ass1}
	{\bf(i)} The sequence $\{x^k\}_{k\in\mathbb{N}}$ is bounded; {\bf(ii)} there exists an open set $\mathcal{O}\!\supset\!\Gamma^*$, the cluster point set of $\{w^k\}_{k\in\mathbb{N}}$, such that $\vartheta_2^*$ is strictly continuous on $\mathcal{O}_{\!z}$ relative to ${\rm dom}\vartheta_2^*$, $F'$ is strictly differentiable on $\mathcal{O}_{\!x}$ and $G'$ is differentiable on $\mathcal{O}_{\!x}$.
\end{assumption}
%------------------------------------------------------------------------------------------
\begin{proposition}\label{prop2-xk}
	Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. If Assumption \ref{ass0} and Assumption \ref{ass1} (i) holds, then the following assertions hold:
	\begin{itemize}
		\item[(i)] the sequence $\{\gamma_k\}_{k\!\in\mathbb{N}}$ with $\gamma_k\!:=\!\gamma_{k,j_k}$ is bounded, so is the sequence $\{\|\!\mathcal{Q}_k\!\|\}_{k\in\mathbb{N}}$.
		
		
		\item[(ii)] $\{w^{k}\}_{k\in\mathbb{N}}$ is bounded, and $\Gamma^*$ is a nonempty, compact and connected set.
		
		\item[(iii)] For each $\widehat{w}=(\widehat{x},\widehat{z},\widehat{\xi},\widehat{\mathcal{Q}})\!\in\Gamma^*$, $\widehat{x}=\widehat{z}$ is a stationary point of \eqref{prob} and $\Xi(\widehat{w})\!=\Xi^*\!:=\lim_{k\to\infty}\Xi(w^k)$. If in addition Assumption \ref{ass1} (ii) holds, $\Gamma^*\subset{\rm crit}\,\Xi$.
	\end{itemize}
\end{proposition}
\begin{proof}
	{\bf(i)} Suppose on the contradiction that the sequence $\{\gamma_k\}_{k\in\mathbb{N}}$ is unbounded. Then there exists an index set $K\subset\mathbb{N}$ such that $\lim_{K\ni k\to\infty}\gamma_k=\infty$. For each $k\in K$, write $\widetilde{\gamma}_k:=\varrho^{-1}\gamma_k=\gamma_{k,j_k-1}$. We first argue that $\lim_{K\ni k\rightarrow\infty}\|x^{k,j_k-1}\!\!-\!x^{k}\|=0$. Fix any $k\in K$. By step (2a),  $q_{k,j_k\!-\!1}(\overline{x}^{k,j_k-1})\!-\!q_{k,j_k-1}(x^{k,j_k\!-\!1})\!\ge\!-\!\frac{\mu}{2}\|x^{k,j_k\!-\!1}-x^{k}\|^2$. Hence, 
	\begin{align*}%\label{bound-M1}
		&q_{k,j_k-1}(\overline{x}^{k,j_k-1}\!)\!-\!\vartheta_1(\ell_{F}(x^k,x^{k,j_k-1}\!))\!-\!\langle\nabla G(x^k)\xi^k,x^{k,j_k-1}\!-\!x^k\rangle\!-\!h(x^{k,j_k-1}\!)\!+\Theta_2(x^k\!)\nonumber\\
		&\ge\frac{1}{2}\|x^{k,j_k-1}\!-\!x^k\!\|_{\mathcal{Q}_{k,j_k-1}}^2\!-\!\frac{\mu}{2}\|x^{k,j_k-1}\!-\!x^{k}\|^2\ge\frac{\widetilde{\gamma}_k\!-\!\mu}{2}\|x^{k,j_k-1}\!\!-\!x^{k}\|^2,
	\end{align*}
	where the last inequality is due to $\mathcal{Q}_{k,j_k-1}\succeq\gamma_{k,j_k-1}\mathcal{I}=\widetilde{\gamma}_{k}\mathcal{I}$.
	In addition, from the optimality of $\overline{x}^{k,j_k-1}$ and the feasibility of $x^k$ to subproblem \eqref{subprobkj} with $j=j_{k}-1$,
	\[
	q_{k,j_k-1}(\overline{x}^{k,j_k-1})\le q_{k,j_k-1}({x}^{k})=\Phi(x^k).
	\]
	Pick any $\widehat{v}\in\partial h(\widehat{x})$, which is nonempty due to ${\rm ri}({\rm dom}\,h)\ne\emptyset$. Then, $h(x^{k,j_k-1})\ge h(\widehat{x})+\langle\widehat{v},x^{k,j_k-1}-\widehat{x}\rangle$. Also, from the finite convexity of $\vartheta_1$, $\partial\vartheta_1(F(\widehat{x}))\ne\emptyset$. Pick any $\widehat{\zeta}\in\partial\vartheta_1(F(\widehat{x}))$. Then $\vartheta_1(\ell_{F}(x^k,x^{k,j_k-1}))\ge \vartheta_1(F(\widehat{x}))+\langle\widehat{\zeta},\ell_{F}(x^k,x^{k,j_k-1})-F(\widehat{x})\rangle$. Together with the last two inequalities, it then follows that
	\begin{align*}
		\frac{\widetilde{\gamma}_k\!-\!\mu}{2}\|x^{k,j_k-1}\!\!-\!x^{k}\|^2
		&\le \Phi(x^k)+\Theta_2(x^k)\!-\!\Theta_1(\widehat{x})\!-\!\langle\widehat{\zeta},F(x^k)\!-\!F(\widehat{x})\rangle\!-\!h(\widehat{x})\\
		&\quad-\langle\nabla F(x^k)\widehat{\zeta}+\!\!\nabla G(x^k)\xi^k,x^{k,j_k-1}\!\!-\!x^k\rangle-\langle\widehat{v},x^{k,j_k-1}-\widehat{x}\rangle\\
		&\le\Phi(x^0)+\Theta_2(x^k)\!-\!\Theta_1(\widehat{x})\!-\!\langle\widehat{\zeta},F(x^k)\!-\!F(\widehat{x})\rangle\!-\!h(\widehat{x})\\
		&-\langle\widehat{v},x^k-\widehat{x}\rangle\!+\!\big[\|\nabla F(x^{k})\widehat{\zeta}\|
		\!+\!\|\nabla G(x^{k})\xi^{k}\|\!+\!\|\widehat{v}\|\big]\|{x}^{k,j_k-1}\!-\!\!x^{k}\|,
	\end{align*}
	where the last inequality is using $\Phi(x^k)\le\Phi(x^0)$ by Proposition \ref{prop1-xk} (ii). By Assumption \ref{ass1} (i) and by \cite[Theorem 9.13]{RW98} (d), the sequence $\{\xi^k\}_{k\in\mathbb{N}}$ is bounded. Combining the last inequality with $\lim_{K\ni k\to\infty}\gamma_k=\infty$, we have $\lim_{K\ni k\to\infty}\|x^{k,j_k-1}\!\!-\!x^{k}\|=0$. Now by adding \eqref{theta1-ineq0}-\eqref{theta2-ineq0} with $x=x^{k,j_k-1}$ for all sufficiently large $k\in K$ together, 
	\begin{equation}\label{Theta-ineq0}
		\Theta(x^{k,j_k\!-\!1}\!)\!\le\!\vartheta_1\!\big(\ell_{F}(x^k\!,x^{k,j_k\!-\!1})\big)\!\!-\!\vartheta_2\big(\ell_{G}(x^k\!,x^{k,j_k\!-\!1})\big)\!+\!\frac{{\rm lip}\Theta(x^k)}{2}\|x^{k,j_k\!-\!1}\!-\!x^k\|^2.
	\end{equation}
	However, by the inner loop of Algorithm \ref{iLPA} and $\mathcal{Q}_{k,j_k\!-\!1}\!\succeq\!\! \widetilde{\gamma}_{k}\mathcal{I}$, for each $k\!\in\! K$,
	\begin{equation*}
		\Theta(x^{k,j_k-1})
		\ge\vartheta_1\big(\ell_{F}(x^k,x^{k,j_k-1})\big)-\vartheta_2\big(\ell_{G}(x^k,x^{k,j_k-1})\big)+\frac{\widetilde{\gamma}_k}{2}\|x^{k,j_k-1}\!-x^k\|^2,
	\end{equation*}
	which yields a contradiction to \eqref{Theta-ineq0} because ${\rm lip}\Theta(x^k)$ is bounded by the strict continuity of $\Theta$ and the boundedness of $\{x^k\}_{k\in\mathbb{N}}$, but $\lim_{K\ni k\to\infty}\widetilde{\gamma}_k=\infty$.	
	
	\noindent
	{\bf(ii)} The boundedness of $\{\xi^k\}_{k\in\mathbb{N}}$ is justified in the proof of part (i). By combining \eqref{key-ineq40} with $\mathcal{Q}_{k-1}\succeq\underline{\gamma}\mathcal{I}$ and the boundedness of $\{x^k\}_{k\in\mathbb{N}}$, we deduce that $\{\overline{x}^k\}_{k\in\mathbb{N}}$ is bounded. Along with part (i), $\{w^k\}_{k\in\mathbb{N}}$ is bounded, so $\Gamma^*$ is nonempty and compact. Its connectness follows by using the same arguments as those for \cite[Lemma 5 (iii)]{Bolte14}.
	
	\noindent
	{\bf(iii)} Pick any $\widehat{w}=(\widehat{x},\widehat{z},\widehat{\xi},\widehat{\mathcal{Q}})\in\Gamma^*$. There exists an index set $K\subset\mathbb{N}$ such that $\lim_{K\ni k\to\infty}w^k=\widehat{w}$. From Proposition \ref{prop1-xk}  (ii)-(iii), $\lim_{K\ni k\to\infty}\|x^k-x^{k-1}\|=0$. Along with \eqref{key-ineq40} and $\mathcal{Q}_k\ge\underline{\gamma}I$, we have $\lim_{K\ni k\to\infty}\|\overline{x}^k-x^{k}\|=0$, so $\widehat{x}=\widehat{z}$. Since $\overline{x}^k$ is the optimal solution of the $(k\!-\!1)$-th subproblem, from \cite[Theorem 23.9]{Roc70},   \begin{equation*}
		0\in\nabla\!F(x^{k-1})\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))\!+\!\!\nabla G(x^{k-1})
		\partial(-\vartheta_2)(x^{k-1}\!)\!+\!\mathcal{Q}_{k-1}(\overline{x}^k\!-\!x^{k-1}\!)\!+\!\partial h(\overline{x}^k\!).
	\end{equation*}
	Passing the limit $K\ni k\to\infty$ to the last inclusion and using the outer semicontinuity of $\partial\vartheta_1,\partial(-\vartheta_2)$ and $\partial h$ yields that $0\in\!\nabla\!F(\widehat{x})\partial\vartheta_1(F(\widehat{x}))+\nabla G(\widehat{x})\partial(-\vartheta_2)(G(\widehat{x}))+\partial h(\widehat{x})$, so $\widehat{x}$ is a stationary point of \eqref{prob}. We next argue that $\Xi(\widehat{w})=\lim_{k\to\infty}\Xi(w^k)$. Note that $-\widehat{\xi}\in\partial\vartheta_2(G(\widehat{x}))$. Hence, $\langle G(\widehat{x}),\widehat{\xi}\rangle+\vartheta_2^*(-\widehat{\xi})=-\vartheta_2(G(\widehat{x}))$.
	By the expression of $\Xi$ and $\widehat{x}=\widehat{z}$, we have $\Xi(\widehat{w})=\vartheta_1(F(\widehat{x}))-\vartheta_2(G(\widehat{x}))+h(\widehat{x})$.
	On the other hand, from the feasibility of $\widehat{x}$ and the optimality of $\overline{x}^k$ to the $(k\!-\!1)$-th subproblem, 
	\begin{align*}
		&\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))+\langle\nabla G(x^{k-1})\xi^{k-1},\overline{x}^{k}-x^{k-1}\rangle+h(\overline{x}^{k})+
		\frac{1}{2}\|\overline{x}^{k}-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2\\
		&\le\vartheta_1(\ell_{F}(x^{k-1},\widehat{x})+\langle\nabla G(x^{k-1})\xi^{k-1},\widehat{x}-x^{k-1}\rangle+h(\widehat{x})+\frac{1}{2}\|\widehat{x}-x^{k-1}\|_{\mathcal{Q}_{k-1}}^2,
	\end{align*}
	which by $\lim_{K\ni k\to\infty}\|\overline{x}^k-x^{k-1}\|=0$ and the continuity of  $\vartheta_1,F'$ and $G'$ implies that $\limsup_{K\ni k\to\infty}h(\overline{x}^{k})\!\le\! h(\widehat{x})$. Together with the lower semicontinuity of $h$, $\lim_{K\ni k\to\infty}h(\overline{x}^{k})\!=\!h(\widehat{x})$. From \eqref{pre-equa0},  $\lim_{K\ni k\to\infty}\vartheta_2^*(-\xi^k)\!=\!-\vartheta_2(G(\widehat{x}))\!-\!\langle G(\widehat{x}),\widehat{\xi}\rangle$. Thus, by the expression of $\Xi$, $\lim_{K\ni k\to\infty}\Xi(w^k)\!=\!\vartheta_1(F(\widehat{x}))\!-\!\vartheta_2(G(\widehat{x}))+\!h(\widehat{x})\!=\Xi(\widehat{w})$. If in addition Assumption \ref{ass1} (ii) holds, by combining $\widehat{x}\!=\!\widehat{z}$ and	$0\in\nabla\!F(\widehat{x})\partial\!\vartheta_1\!(F(\widehat{x}))\!+\nabla G(\widehat{x})\partial(-\vartheta_2)(G(\widehat{x}))\!+\!\partial h(\widehat{x})$ with Lemma \ref{subdiff-LemXi}, we get $0\in\partial\Xi(\widehat{w})$, i.e., $\Gamma^*\!\subset{\rm crit}\,\Xi$.
\end{proof}

To achieve the global convergence of $\{x^k\}_{k\in\mathbb{N}}$, we also need the following result.
%---------------------------------------------------------------------------------------------
\begin{proposition}\label{prop3-xk}
	Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. If Assumptions \ref{ass0}-\ref{ass1} hold, then for each sufficiently large $k$ there exists $\zeta^{k}\in\partial\Xi(w^{k})$ with $\|\zeta^{k}\|\le\alpha\|x^k\!-\!x^{k-1}\|$ for a constant $\alpha>0$.
\end{proposition}
\begin{proof}
	By Proposition \ref{prop2-xk} (ii), $\lim_{k\to\infty}{\rm dist}(w^k,\Gamma^*)=0$, so there exists $\overline{k}\in\mathbb{N}$ such that $w^k\in\mathcal{O}$ for all $k\ge\overline{k}$. By the optimality of $\overline{x}^{k}$ to the $(k\!-\!1)$th subproblem,
	\begin{equation}\label{temp-inclusion1}
		0\in\nabla F(x^{k-1})\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^{k}))
		+\nabla G(x^{k-1})\xi^{k-1}+\partial h(\overline{x}^{k})+\mathcal{Q}_{k-1}(\overline{x}^{k}\!-\!x^{k-1}),
	\end{equation}
	which implies that $\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^{k}))\ne\emptyset$. Pick any $v^{k-1}\!\in\partial\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^{k}))$.
	Recall that $G(x^{k-1})\in\partial\vartheta_2^*(-\xi^{k-1})$. Since  $\mathcal{Q}_{k-1}\!\succeq\underline{\gamma}\mathcal{I}$,  $0\in\mathcal{N}_{\mathbb{S}_{+}}(\mathcal{Q}_{k-1})$. For each $k\ge\overline{k}$, let
	\[
	\zeta^k\!\!:=\!\left(\begin{matrix}\!
		[D^2F(x^{k\!-\!1})(\overline{x}^{k}\!\!-\!x^{k\!-\!1},\cdot\!)]^*v^{k\!-\!1}\!\!+\!\![D^2G(x^{k\!-\!1})(\overline{x}^{k}\!\!-\!x^{k\!-\!1},\!\cdot)]^*\xi^{k\!-\!1}\!\!+\!\!2\mathcal{Q}_{k\!-\!1}(x^{k\!-\!1}\!-\!\overline{x}^{k}\!)\\
		\mathcal{Q}_{k\!-\!1}(\overline{x}^{k}-x^{k\!-\!1})\\
		\ell_{G}(x^{k-1},\overline{x}^{k})-G(x^{k\!-\!1})\\
		(\overline{x}^{k}-x^{k\!-\!1})(\overline{x}^{k}-x^{k\!-\!1})^{\top}\!
	\end{matrix}\right).
	\]
	By comparing with the expression of $\partial\Xi$ in Lemma \ref{subdiff-LemXi} and using equation \eqref{temp-inclusion1}, it is not hard to obtain $\zeta^k\in\partial\Xi(w^k)$. Since $\vartheta_1$ and $\vartheta_2$ are strictly continuous at each $y\in\mathbb{Y}$ and $z\in\mathbb{Z}$, respectively, from \cite[Theorem 9.13]{RW98}, $\|v^{k-1}\|\le{\rm lip}\,\vartheta_1(\ell_{F}(x^{k-1},\overline{x}^k))$ and $\|\xi^{k-1}\|\le{\rm lip}\,\vartheta_2(G(x^{k-1}))$. Together with the boundedness of $\{w^k\}_{k\in\mathbb{N}}$, there exists a constant $\alpha_1>0$ such that $\max(\|v^{k-1}\|,\|\xi^{k-1}\|)\le \alpha_1$ for all $k\ge\overline{k}$. Together with the boundedness of $\{\|\mathcal{Q}_{k\!-\!1}\!\|\}_{k\in\mathbb{N}}$ and the expression of $\zeta^k$, there exists $\alpha_2\!>\!0$ such that
	\[
	\|\zeta^k\|\le \alpha_2\|x^{k-1}-\overline{x}^{k}\|\le  \alpha_2\big[\|x^{k-1}-x^k\|+\|\overline{x}^{k}-x^k\|\big].
	\]
	In addition, by the choice of $\mathcal{Q}_{k\!-\!1}$ in Algorithm \ref{iLPA}, $\mathcal{Q}\!_{k\!-\!1}\!\succeq\!\underline{\gamma}\mathcal{I}$. Together with \eqref{key-ineq40}, 
	\begin{equation*}%\label{key-ineq42}
		\|\overline{x}^k-x^k\|\le\sqrt{\mu\underline{\gamma}^{-1}}\|x^k-x^{k-1}\|\quad\forall k\in\mathbb{N}.
	\end{equation*}
	The desired result follows from the last two inequalities with  $\alpha=\alpha_2(1+\!\sqrt{\mu\underline{\gamma}^{-1}})$.
\end{proof}

Now using Proposition \ref{prop2-xk} (ii)-(iii), Propositions \ref{prop1-xk} (i) and \ref{prop3-xk}, and the same arguments as those for \cite[Theorem 1]{Bolte14}, we have the following convergence result.
%----------------------------------------------------------------------------------------------
\begin{theorem}\label{global-converge}
	Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Suppose that  Assumptions \ref{ass0}-\ref{ass1} hold, and that $\Xi$ is a KL function. Then, $\{x^k\}_{k\in\mathbb{N}}$ and $\{\overline{x}^k\}_{k\in\mathbb{N}}$ converge to the same point, say, $x^*$, and $x^*$ is a stationary point of problem \eqref{prob}.
\end{theorem}
\begin{remark}
	From \cite[Theorem 4.1]{Attouch10}, if $\Xi$ is definable in an o-minimal structure over the real field $(\mathbb{R},+,\cdot)$, then it has the KL property at each point of ${\rm dom}\,\partial\Xi$. By the expression of $\Xi$ and \cite[sections 2 $\&$\,3]{Ioffe09}, when $F,G$ and $\vartheta_1,\vartheta_2,h$ are definable in the same o-minimal structure, $\Xi$ is definable in this o-minimal structure. As discussed in \cite[Section 4]{Attouch10} and \cite[sections 2 $\&$\,3]{Ioffe09}, definable functions in an o-minimal structure are very rich, which cover semialgebraic functions, globally subanalytic functions, etc.
\end{remark}
%----------------------------------------------------------------------------------------
\subsection{Local convergence rate of Algorithm \ref{iLPA}}\label{sec4.2}

By using Theorem \ref{global-converge} and Proposition \ref{prop1-xk} and following the same arguements as those for \cite[Theorem 2]{Attouch09}, we have the following result for the local convergence rate of Algorithm \ref{iLPA}.
%-----------------------------------------------------------------------------------------------------
\begin{theorem}\label{convergece-rate}
	Let $\{w^k\}_{k\in\mathbb{N}}$ be the sequence generated by Algorithm \ref{iLPA}. Suppose that Assumptions \ref{ass0}-\ref{ass1} hold, and that $\Xi$ has the KL property of exponent $p\in[1/2,1)$ on $\Gamma^*$. Then, the sequence $\{w^k\}_{k\in\mathbb{N}}$ is convergent and the following assertions hold:
	\begin{itemize}
		\item[(i)]  if $p=1/2$, there exist $c_1>0$ and $\rho\in(0,1)$ such that $\|x^k-x^*\|\leq c_1\rho^k$;
		
		\item[(ii)] if $p\in(\frac{1}{2},1)$, there exists $c_1>0$ such that  $\|x^k-x^*\|\le c_1k^{-\frac{1-p}{2p-1}}$.
	\end{itemize}
\end{theorem}

As is well known, to verify the KL property of exponent $p=1/2$ for a nonconvex and nonsmooth function is not an easy task because there are no convenient rules to identify it. In what follows, we focus on the verifiable conditions for the KL property of $\Xi$ with exponent $p\in[1/2,1)$. For this purpose, we introduce the function
%--------------------------------------------------------------------------------------------------
\begin{equation}\label{wXi-fun}
	\widetilde{\Xi}(x,s,z):=\vartheta_1(\ell_{F}(x,s))+\langle \ell_{G}(x,s),z\rangle+h(s)+\vartheta_2^*(-z)\quad\forall(x,s,z) \in\mathbb{X}\times\mathbb{X}\times\mathbb{Z}.
\end{equation}
Under the assumption of Lemma \ref{subdiff-LemXi}, at any $(x,s,z)\in{\rm dom}h\times{\rm dom}h\times(-{\rm dom}\vartheta_2^*)$,
%---------------------------------------------------------------------------------------------------
\begin{equation}\label{subdiff-wXi}
	\partial\widetilde{\Xi}(x,s,z)\!\!=\!\!
	\left(\begin{matrix}\!
		\left(\begin{matrix}
			[D^2F(x)(s\!-\!x,\cdot)]^*\\ \nabla\!F(x)
		\end{matrix}\right)\!\partial\vartheta_1(\ell_{F}(x,s))
		\!+\!\left(\begin{matrix}
			[D^2G(x)(s\!-\!x,\cdot)]^*z\\ \nabla G(x)z\!+\!\partial h(s)
		\end{matrix}\right)\\
		\ell_{G}(x,s)-\partial\vartheta_2^*(-z)\!
	\end{matrix}\!\right).
\end{equation}
By comparing \eqref{subdiff-wXi} with the expression of $\partial\Xi$ in Lemma \ref{subdiff-LemXi}, we see that $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$ if and only if $(\overline{x},\overline{x},\overline{z},\mathcal{Q})$ for a certain PD linear mapping $\mathcal{Q}\!:\mathbb{X}\to\mathbb{X}$ is a critical point of $\Xi$. By combining \eqref{subdiff-wXi} with Definition \ref{spoint-def}, if $\overline{x}$ is a stationary point of \eqref{prob},  there exists $\overline{z}\in\!-\partial\vartheta_2(G(\overline{x}))$ such that $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$; and if $(\overline{x},\overline{x},\overline{z})$ is a critical point of $\widetilde{\Xi}$, $\overline{x}$ is a stationary point of \eqref{prob}.
%--------------------------------------------------------------------------------------------
\begin{proposition}\label{prop-KL0}	
	Suppose that $\widetilde{\Xi}$ is a KL function, that Assumption \ref{ass0} (i) holds, that $F,G$ are polyhedral and $\vartheta_1$ is piecewise linear-quadratic (PLQ), and that $\partial\vartheta_2^*$ and $\partial h$ are $p$-subregular with $p\in\!(\frac{1}{2},1]$ at any point of their graphs. Then $\widetilde{\Xi}$ and $\Xi$ are the KL function of exponent $\frac{1}{2p}$.
\end{proposition}	
\begin{proof}
	Let $f(x,s,z):=\langle \ell_{G}(x,s),z\rangle$ and $g(x,s,z)\!:=\vartheta_1(\ell_{F}(x,s))+h(s)+\vartheta_2^*(-z)$ for $(x,s,z)\in\mathbb{X}\times\mathbb{X}\times\mathbb{Z}$. Clearly, $\widetilde{\Xi}=f+g$. From the given assumption and \cite[Proposition 1]{Robinson81}, we deduce that $\partial\widetilde{\Xi}$ is  $p$-subregular at any $(x,s,z)\in{\rm dom}\partial\widetilde{\Xi}$ for the origin. By \cite[Proposition 2.2 (i) \& Remark 2.2]{LiuPanWY22}, $\widetilde{\Xi}$ is a KL function of exponent $\frac{1}{2p}$. By using the same arguments as those for \cite[Lemma 1]{LiuPanWY22}, so is the function $\Xi$.
\end{proof} 	

For general $F,G$ and convex $\vartheta_1,\vartheta_2$ and $h$, the following proposition provides a condition for the KL property of $\widetilde{\Xi}$ and $\Xi$ with exponent $p\in[\frac{1}{2},1)$ by using that of an almost separable nonsmooth function and the structure of $F'$ and $G'$.
%------------------------------------------------------------------------------------------------
\begin{proposition}\label{prop-KL}
	Fix any $\overline{\chi}=(\overline{x},\overline{x},\overline{z})\in(\partial\widetilde{\Xi})^{-1}(0)$.
	Suppose that the function
	\begin{equation}\label{ffun}
		f(y,s,u,z):=\!\vartheta_1(y)\!+\!h(s)\!+\!\langle u,z\rangle\!+\!\vartheta_2^*(-z)\ \ {\rm for}\  (y,s,u,z)\in\mathbb{Y}\!\times\mathbb{X}\!\times\mathbb{Z}\!\times\mathbb{Z}
	\end{equation}
	has the KL property of exponent $p\in[\frac{1}{2},1)$ at $(\overline{y},\overline{x},\overline{u},\overline{z})$ with $\overline{y}=F(\overline{x}), \overline{u}=G(\overline{x})$, and
	\begin{equation}\label{key-cond}
		{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})]\big)\cap{\rm cl}\,{\rm cone}\big[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{u}))\big]=\big\{0\big\}.
	\end{equation}
	Then, under Assumptions \ref{ass0} and \ref{ass1} (ii), the function $\widetilde{\Xi}$ has the KL property of exponent $p$ at $\overline{\chi}$, so does the function $\Xi$ at  $(\overline{\chi},\overline{\mathcal{Q}})$ for any PD linear mapping $\overline{\mathcal{Q}}\!:\mathbb{X}\to\mathbb{X}$.
\end{proposition}
\begin{proof}
	Suppose that $\widetilde{\Xi}$ does not have the KL property of exponent $p$ at $\overline{\chi}$. Then, there exists a sequence $\chi^k:=(x^k,s^k,z^k)\to\overline{\chi}$ with $\widetilde{\Xi}(\overline{\chi})\!<\!\widetilde{\Xi}(\chi^k)\!<\!\widetilde{\Xi}(\overline{\chi})+\frac{1}{k}$ such that
	\[
	{\rm dist}\big(0,\partial\widetilde{\Xi}(x^k,s^k,z^k)\big)
	<\frac{1}{k}\big(f(y^k,s^k,u^k,z^k)-f(\overline{y},\overline{s},\overline{u},\overline{z})\big)^{p}
	\quad{\rm for\ all}\ k\in\mathbb{N},
	\]
	where $y^k\!:=\ell_{F}(x^k,s^k)$ and $u^k\!:=\ell_{G}(x^k,s^k)$ for each $k\in\mathbb{N}$. Obviously, it holds that $\{(x^k,s^k,z^k)\}_{k\in\mathbb{N}}\subset{\rm dom}\,\widetilde{\Xi}$. By combining the last inequality with equation \eqref{subdiff-wXi}, for each $k\in\mathbb{N}$, there exist $v^k\in\partial\vartheta_1(y^k)$, $\xi^k\in\partial h(s^k)$ and $\zeta^k\in\partial\vartheta_2^*(-z^k)$ such that
	\begin{align}\label{ineq1-KL}
		&\left\|\left(\begin{matrix}
			[D^2F(x^k)(s^k-x^k,\cdot)]^*v^k+[D^2G(x^k)(s^k-x^k,\cdot)]^*z^k\\
			\nabla\!F(x^k)v^k+\xi^k+\nabla G(x^k)z^k\\
			u^k-\zeta^k
		\end{matrix}\right)\right\|\nonumber\\
		&<\frac{1}{k}\Big(f(y^k,s^k,u^k,z^k)-\overline{f}\Big)^{p}\ \ {\rm with}\ \overline{f}\!:=f(\overline{y},\overline{x},\overline{u},\overline{z}).
	\end{align}
	Since $f$ has the KL property of exponent $p$ at $(\overline{y},\overline{x},\overline{u},\overline{z})$, there exist $\delta'>0,\eta'>0$ and $c'>0$ such that for all $(y,s,u,z)\in\mathbb{B}((\overline{y},\overline{x},\overline{u},\overline{z}),\delta')\cap[\overline{f}<f<\overline{f}+\eta']$,
	\begin{equation}\label{vartheta-KL}
		{\rm dist}(0,\partial\!f(y,s,u,z))\ge c'\big(f(y,s,u,z)-\overline{f}\big)^{p}.
	\end{equation}
	From $\chi^k\to\overline{\chi}$, we have $x^k\in\mathcal{O}_{x}$ for all sufficiently large $k$, where $\mathcal{O}_x$ is the same as the one in Assumption \ref{ass1} (ii). Thus, by invoking Assumptions \ref{ass0}-\ref{ass1} and recalling that  $\widetilde{\Xi}(\overline{w})<\widetilde{\Xi}(w^k)<\widetilde{\Xi}(\overline{w})+\frac{1}{k}$ for each $k\in\mathbb{N}$, it follows that  $(y^k,s^k,u^k,z^k)\in\mathbb{B}((\overline{y},\overline{x},\overline{u},\overline{z}),\delta')\cap[\overline{f}<f<\overline{f}+\eta']$ for all sufficiently large $k$, which together with the above \eqref{vartheta-KL} and \eqref{ineq1-KL} implies that for all sufficiently large $k$,
	\begin{align}\label{ineq2-KL}
		&\left\|\left(\begin{matrix}
			[D^2F(x^k)(s^k-x^k,\cdot)]^*v^k+[D^2G(x^k)(s^k-x^k,\cdot)]^*z^k\\
			\nabla F(x^k)v^k+\xi^k+\nabla G(x^k)z^k\\
			u^k-\zeta^k
		\end{matrix}\right)\right\|\nonumber\\
		&<\frac{1}{kc'}{\rm dist}\big(0,\partial\!f(y^k,s^k,u^k,z^k)\big)
		\le\frac{1}{kc'}\|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|,
	\end{align}
	where the last inequality is due to the expression of $f$ and the fact that $v^k\in\partial\vartheta_1(y^k)$, $\xi^k\in\partial h(s^k)$ and $\zeta^k\in\partial\vartheta_2^*(-z^k)$ for each $k\in\mathbb{N}$. Note that for all sufficiently large $k$, $(v^k,\xi^k,z^k,u^k\!-\zeta^k)\in\partial\!f(y^k,s^k,u^k,z^k)$, which along with \eqref{vartheta-KL} implies that
	\[
	\|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|\ge \!c'\big(f(y^k,s^k,u^k,z^k)-\overline{f}\big)^{p}>0.
	\]
	For each sufficiently large $k$, we write $(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k)
	\!:=\frac{(v^k,\xi^k,z^k,u^k\!-\zeta^k)}{\|(v^k,\xi^k,z^k,u^k\!-\zeta^k)\|}$. Then, from \eqref{ineq2-KL} it follows that for all sufficiently large $k$,
	\begin{equation}\label{ineq3-KL}
		\left\|\left(\begin{matrix}
			[D^2F(x^k)(s^k-x^k,\cdot)]^*\widetilde{v}^k+[D^2G(x^k)(s^k-x^k,\cdot)]^*\widetilde{z}^k\\
			\nabla\!F(x^k)\widetilde{v}^k+\widetilde{\xi}^k+\nabla G(x^k)\widetilde{z}^k\\
			\widetilde{\eta}^k
		\end{matrix}\right)\right\|\le\frac{1}{kc'}.
	\end{equation}
	By the boundedness of $\{(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k,\widetilde{\eta}^k)\}_{k\in\mathbb{N}}$, there exists an index set $K\subset\mathbb{N}$ such that  $\{(\widetilde{v}^k;\widetilde{\xi}^k\!,\widetilde{z}^k\!,\widetilde{\eta}^k\!)\}_{k\in K}$ is convergent and its limit, say $(\widetilde{v},\widetilde{\xi},\widetilde{z},\widetilde{\eta})$, satisfies $\|(\widetilde{v},\widetilde{\xi},\widetilde{z},\widetilde{\eta})\|\!=\!1$.
	
	We next argue that the sequence $\{(\xi^k,u^k\!-\!\zeta^k)\}_{k\in\mathbb{N}}$ is necessarily bounded. Suppose on the contradiction that  $\{(\xi^k\!,u^k\!-\zeta^k\!)\}_{k\in\mathbb{N}}$ is unbounded. Note that $\{(v^k,z^k)\}_{k\in\mathbb{N}}$ is bounded. Then, from the unboundedness of $\{(\xi^k,u^k-\zeta^k)\}_{k\in\mathbb{N}}$, we deduce that $\widetilde{v}=0$, $\widetilde{z}=0$ and
	$\|(\widetilde{\xi},\widetilde{\eta})\|=1$. On the other hand, passing the limit $k\to\infty$ to inequality \eqref{ineq3-KL} and using $\widetilde{v}=0,\widetilde{z}=0$ leads to $\widetilde{\xi}=\widetilde{\eta}=0$, which is a contradiction to $\|(\widetilde{\xi},\widetilde{\eta})\|=1$. Thus,  $\{(\xi^k,u^k-\!\zeta^k)\}_{k\in\mathbb{N}}$ is bounded, which by $u^k\to G(\overline{x})$ as $k\to\infty$ implies that $\{\zeta^k\}_{k\in\mathbb{N}}$ is bounded. If necessary by taking a subsequence, we assume that $\lim_{K\ni k\to\infty}\zeta^k=\overline{\zeta}$. From the expression of $(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k)$ and $z^k\in-\partial\vartheta_2(\zeta^k)$,  for each sufficiently large $k$,
	\[
	(\widetilde{v}^k,\widetilde{\xi}^k,\widetilde{z}^k)\in{\rm cone}\big[\partial\vartheta_1(y^k)\times\partial h(s^k)\times(-\partial\vartheta_2(\zeta^k))].
	\]
	Together with the outer semicontinuity of $\partial\vartheta_1,\partial h$ and $\partial\vartheta_2$, it then follows that
	\[
	(\widetilde{v},\widetilde{\xi},\widetilde{z})\in{\rm cl}\,{\rm cone}[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{\zeta}))].
	\]
	On the other hand, passing the limit $K\ni k\to\infty$ to inequality \eqref{ineq3-KL}, we obtain that
	\[
	\nabla\!F(\overline{x})\widetilde{v}+\widetilde{\xi}+\nabla\!G(\overline{x})\widetilde{z}=0
	\ \ {\rm and}\ \ \widetilde{\eta}=0.
	\]
	By using $\widetilde{\eta}=0$ and the expression of $\widetilde{\eta}^k$ and recalling that $u^k\to G(\overline{x})$ as $k\to\infty$, it is not hard to deduce that $\overline{\zeta}=G(\overline{x})$. Then, from the last two equations, we have
	\[
	0\ne(\widetilde{v},\widetilde{\xi},\widetilde{z})\in{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}\ \ \nabla G(\overline{x})]\big)\cap{\rm cl}\,{\rm cone}\big[\partial\vartheta_1(\overline{y})\times\partial h(\overline{x})\times(-\partial\vartheta_2(\overline{u})],
	\]
	which contradicts the assumption in \eqref{key-cond}. Hence, $\widetilde{\Xi}$ has the KL property of exponent $p$ at $\overline{\chi}$. Since $(\overline{\chi},\overline{\mathcal{Q}})$ is a critical point of $\Xi$,  using the same arguments as those for \cite[Lemma 1]{LiuPanWY22} can justify that $\Xi$ has the KL property of exponent $p$ at $(\overline{\chi},\overline{\mathcal{Q}})$.
\end{proof}
\begin{remark}\label{remark42-KL}
	{\bf(a)} From the proof of Proposition \ref{prop-KL}, we see that its conclusions still hold if $p\in[1/2,1)$ is replaced by $p\in(0,1)$, and do not require the convexity of $\vartheta_1$ and $h$. It is worth to point out that condition \eqref{key-cond} is point-based and verifiable.
	
	\noindent
	{\bf(b)} Let $\psi(u,z)\!:=\!\langle u,z\rangle+\vartheta_2^*(-z)$ for $(u,z)\in\mathbb{Z}\times\mathbb{Z}$. If $\psi,\vartheta_1$ and $h$ are KL functions of exponent $p\in[1/2,1)$, then the function $f$ defined in \eqref{ffun} is a KL function of exponent $p\in[1/2,1)$. By \cite[Proposition 2.2 (i) $\&$ Remark 2.2]{LiuPanWY22}, $\psi$ is a KL function of exponent $p\in[1/2,1)$ if $\vartheta_2^*$ is a KL function and $\partial\vartheta_2^*$ is $\frac{1}{2p}$-subregular at any point of its graph. Since $\vartheta_1$ and $h$ are convex functions, their KL property of exponent $p\in[1/2,1)$ is implied by the $(\frac{1}{p}\!-\!1)$-subregularity of their subdifferential mappings by combining \cite[Theorem 3.4]{Mordu15} and \cite[Theorem 5 (ii)]{Bolte17}. Along with  \cite[Proposition 1]{Robinson81}, $\vartheta_1$ and $h$ are KL functions of exponent $p\in[1/2,1)$ if they are convex and PLQ. 
\end{remark}
%------------------------------------------------------------------------------------------------
\begin{corollary}\label{corollary-KL}
	Consider problem \eqref{prob} with $\vartheta_2\!\equiv 0$. Fix any $(\overline{x},\overline{x})\!\in\!(\partial\widetilde{\Xi})^{-1}(0)$ with $\widetilde{\Xi}(x,s)=\vartheta_1(\ell_{F}(x,s))+h(s)$ for $(x,s)\in\mathbb{X}\times\mathbb{X}$.
	Suppose that the function
	\begin{equation}\label{ffun1}
		f(y,s):=\vartheta_1(y)+h(s)\ \ {\rm for}\  (y,s)\in\mathbb{Y}\times\mathbb{X}
	\end{equation}
	has the KL property of exponent $p\in[1/2,1)$ at $(\overline{y},\overline{x})$ with $\overline{y}=F(\overline{x})$, and that
	\begin{equation}\label{key-cond1}
		{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap{\rm cl}\,{\rm cone}\big(\partial\! f(\overline{y},\overline{x})\big)=\big\{0\big\}.
	\end{equation}
	Then, under Assumptions \ref{ass0} and \ref{ass1} (ii), $\widetilde{\Xi}$ has the KL property of exponent $p$ at $(\overline{x},\overline{x})$, and $\Xi$
	has the KL property of exponent $p$ at  $(\overline{x},\overline{x},\overline{\mathcal{Q}})$ for any PD linear mapping $\overline{\mathcal{Q}}$.
\end{corollary}

Obviously, the function $f$ defined in \eqref{ffun1} has the KL property of exponent $1/2$ at $(F(\overline{x}),\overline{x})$ if and only if $\vartheta_1$ and $h$ have the KL property of exponent $1/2$ at $F(\overline{x})$ and $\overline{x}$, respectively. By combining \cite[Theorem 5 (ii)]{Bolte17} and \cite[Theorem 3.3]{Artacho08}, the KL property of $\vartheta_1$ with exponent $1/2$ at $\overline{y}=F(\overline{x})$ is equivalent to the subregularity of $\partial\vartheta_1$ at $\overline{y}$ for the origin, and that of $h$ is equivalent to the subregularity of $\partial h$ at $\overline{x}$ for the origin. Such a condition is equivalent to requiring  $\mathop{\arg\min}_{(y,s)\in\mathbb{Y}\times\mathbb{X}}f(y,s)$ to be the set of local weak sharp minima of order 2 for $f$, which for $h\equiv 0$ precisely becomes the one used in \cite[Theorem 20]{HuYang16} to achieve a global R-linear rate.

Next we take a closer look at condition \eqref{key-cond1} and discuss its relation with the regularity adopt in \cite{HuYang16} for problem \eqref{prob} with $\vartheta_2\equiv 0$. To this end, for a closed convex set $S\subset\mathbb{Y}$, let $S^{\ominus}:=\{y^*\in\mathbb{Y}\,|\,\langle y^*,y\rangle\le 0\ {\rm for\ all}\ y\in S\}$ denote its negative polar.
%-------------------------------------------------------------------------------------------------
\begin{remark}\label{remark-relation}
	{\bf(a)} Let $D\!:=\!\mathop{\arg\min}_{x\in\mathbb{X}}h(x)$ and recall that $C\!=\!\mathop{\arg\min}_{y\in\mathbb{Y}}\vartheta_1(y)$. Then, ${\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x})) \!\subset\![C\times D-(\overline{y},\overline{x})]^{\ominus}$. Indeed, pick any $(u,v)\!\in\!{\rm cl}\,{\rm cone}(\partial\!f(\overline{y},\overline{x}))$. Let $\{(u^k,v^k)\}_{k\in\mathbb{N}}\!\subset\!{\rm cone}(\partial\!f(\overline{y},\overline{x}))$ be such that $(u^k,v^k)\to (u,v)$. For each $k$, there exist $t_k\ge 0$ and $(\xi^k,\zeta^k)\in\partial\!f(\overline{y},\overline{x})$ such that $(u^k,v^k)\!=\!t_k(\xi^k,\zeta^k\!)$. Therefore,
	\[
	0\ge f(y,x)-f(\overline{y},\overline{x})\ge\langle(\xi^k,\zeta^k),(y,x)-(\overline{y},\overline{x})\rangle
	\quad\ \forall (y,x)\in C\times D,
	\]
	which implies that $(u^k,v^k)\in[C\times D-(\overline{y},\overline{x})]^{\ominus}$ and $(u,v)\in[C\times D-(\overline{y},\overline{x})]^{\ominus}$ by the closedness of the set $[C\times D-(\overline{y},\overline{x})]^{\ominus}$. The stated inclusion then follows. In particular, this inclusion is generally strict; for example, consider $\vartheta_1(y)=\|y_1\|_1+y_2^2$ for $y\in\mathbb{R}^2$ and $F(x)\!=\!(x,-x)^{\top}$ for $x\in\mathbb{R}$. For $\overline{x}\!=\!1$, we have ${\rm cl}[{\rm cone}(\partial\vartheta_1(F(\overline{x})))]\!=\!\mathbb{R}\times\{0\}$, but $[C\!-\!F(\overline{x})]^{\ominus}\!=\!\mathbb{R}^2$. This shows that condition \eqref{key-cond1} is weaker than the following one
	\[
	{\rm Ker}\big([\nabla\!F(\overline{x})\ \ \mathcal{I}]\big)\cap\big([C-F(\overline{x})]^{\ominus}\times[D-\overline{x}]^{\ominus}\big)=\big\{0\big\},
	\]
	which is exactly the regularity condition used in \cite[Theorem 18]{HuYang16} and \cite[Section 3]{Burke95}.
	
	\noindent
	{\bf(b)} Suppose that $h\equiv 0$. Write $D(x)\!:=\{d\in\mathbb{X}\,|\,F(x)+F'(x)d=0\}$. From \cite[Definition 7 (c)]{HuYang16}, a vector $\overline{x}\in\mathbb{X}$ is called a quasi-regular point of the inclusion $F(x)\in C$ if there exist $r>0$ and $\beta_{r}>0$ such that for all $x\in\mathbb{B}(\overline{x},r)$, 
	\[
	{\rm dist}(0,D(x))\le\beta_{r}{\rm dist}(F(x),C).
	\]
	When $F$ is polyhedral and $C=\{0\}$, by Hoffman's lemma, the above quasi-regularity condition  holds. In this case, there are examples for which condition \eqref{key-cond1} does not hold; for instance, $\overline{x}=0, F(x)=(x;0)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=y_1^4+|y_2|$ for $y\in\mathbb{R}^2$. When $F$ is polyhedral but $C\ne\{0\}$, there are also examples for which the above quasi-regularity condition does not hold; for example, $\overline{x}=1, F(x)=(x;0)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=(y_1\!-\!1)^4+|y_2|$ for $y\in\mathbb{R}^2$. Of course, condition \eqref{key-cond1} does not hold for this example. However, when $F$ is polyhedral, if $\theta_1$ is a PLQ convex function, then $\widetilde{\Xi}$ is a KL function of exponent $1/2$ by the proof of Proposition \ref{prop-KL0}, so is the function $\Xi$ by the second part of Corollary \ref{corollary-KL}, and consequently, from Theorem \ref{convergece-rate} (i), $\{x^k\}_{k\in\mathbb{N}}$ converges to $x^*$ with a R-linear rate. When $F$ is non-polyhedral, there are examples for which condition \eqref{key-cond1} holds but the quasi-regularity condition does not hold; for instance, $F(x)=(x^2;-x)^{\top}$ for $x\in\mathbb{R}$ and $\vartheta_1(y)=\|y\|^2$ for $y\in\mathbb{R}^2$. It is easy to check that \eqref{key-cond1} holds at  $\overline{x}=0$, which along with the strong convexity of $\vartheta_1$ shows that $\widetilde{\Xi}$ and $\Theta_1$ are the KL functions of exponent $1/2$, but $\overline{x}$ is not a quasi-regular point of the inclusion $F(x)\in C$ because $D(x)=\{d\in\mathbb{R}\,|\,(x^2+2xd;-x-d)=0\}=\emptyset$ for all $x\ne 0$ sufficiently close to $\overline{x}$. Now it is unclear whether there is an example with nonlinear $F$ for which the quasi-regularity holds but condition \eqref{key-cond1} does not hold.
\end{remark}
%-------------------------------------------------------------------------------------------
 \section{Numerical experiments}\label{sec6}

  Before testing the performance of Algorithm \ref{iLPA}, we first develop a solver to compute their subproblems. This solver, named dPPASN, is a proximal point algorithm to solve their dual problems with each proximal subproblem solved by a semismooth Newton method.
  To this end, for a proper closed convex $f\!:\mathbb{X}\to\overline{\mathbb{R}}$ and a constant $\gamma>0$, $P_{\gamma}f$ and $e_{\gamma}f$ denotes the proximal mapping and the Moreau envelope of $f$ associated to $\gamma$. For each $k\in\mathbb{N}$ and $j\in[j_k]$, let
  $\mathcal{A}_k\!:=F'(x^k),u^k\!:=\nabla G(x^k)\xi^k,c^k\!:=F(x^k),b^k\!:=\mathcal{A}_kx^k-c^k$ and take $\mathcal{Q}_{k,j}\!:=\gamma_{k,j}\mathcal{I}+\alpha_k\mathcal{A}_k^*\mathcal{A}_k$, where  $\alpha_k>0$ is specified in the later experiments.
%----------------------------------------------------------------------------------------------------
 \subsection{Dual PPA armed with SNCG for subproblems}\label{sec5.1}

 With the above notation, the previous subproblem \eqref{subprobkj} can be equivalently written as
 \begin{align}\label{Esubprobkj}
 	&\min_{x\in\mathbb{X},y\in\mathbb{Y}}\!\vartheta_1(y)+\langle u^k,x\rangle+h(x)+\frac{\gamma_{k,j}}{2}\|x-x^k\|^2+\frac{\alpha_k}{2}\|y-c^k\|^2-\Theta_2(x^k) \nonumber\\
 	&\quad{\rm s.t.}\ \ \mathcal{A}_kx-b^k=y.
 \end{align}
 After an elementary calculation, the dual of problem \eqref{Esubprobkj} takes the following form
 \begin{equation}\label{dEsubprobkj}
 	\min_{\zeta\in\mathbb{Y}}\Psi_k(\zeta):= \frac{\|\zeta\|^2}{2\alpha_{k}}-e_{\alpha_k^{-1}}\vartheta_1\big(\alpha_{k}^{-1}\zeta+c^k\big)+\frac{\|\mathcal{A}_k^*\zeta\!+\!u^k\|^2}{2\gamma_{k,j}} -e_{\gamma_{k,j}^{-1}}h\big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+\!u^k)\big)-C_k
 \end{equation}
 with $C_k=\langle u^k,x^k\rangle+\Theta_2(x^k)$. Clearly, the strong duality holds for  \eqref{Esubprobkj} and \eqref{dEsubprobkj}.
 Since $\Psi_k$ is a smooth convex function with Lipschitz continuous gradient, seeking an optimal solution of \eqref{dEsubprobkj} is equivalent to finding a root $\zeta^*$ of system $\nabla\Psi_k(\zeta)=0$. With such $\zeta^*$, one can recover the unique optimal solution $(\overline{x}^{k,j},\overline{y}^{k,j})$ of \eqref{Esubprobkj} or subproblem \eqref{subprobkj} via
 \[
 \overline{x}^{k,j}\!=\!P_{\gamma_{k,j}^{-1}}h(x^k-\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{*}\!+\!u^k))
 \ \ {\rm and}\ \ \overline{y}^{k,j}\!=\!P_{\alpha_k^{-1}}\vartheta_1(\alpha_k^{-1}\zeta^{*}\!+c^k).
 \]

 The system $\nabla\Psi_k(\zeta)=0$ is semismooth by \cite{Meng05}, but a direct application of the semismooth Newton method to it faces the difficulty caused by the potential singularity of the generalized Hessian of $\Psi_k$. Inspired by this, we apply the inexact PPA armed with the semismooth Newton method to solving \eqref{dEsubprobkj}, whose iterate steps are described as follows.
 %-------------------------------------------------------------------------------------
 \begin{algorithm}[h]
 	\renewcommand{\thealgorithm}{A}
 	\caption{\label{PPASN}{\bf\ Inexact PPA with semismooth Newton (dPPASN)}}
 	\textbf{Initialization:} Fix $k\in\mathbb{N}$ and $j\in[j_k]$.
 	Choose $\underline{\tau}>0,\tau_0>0$ and $\zeta^0\in\mathbb{Y}$. Set $l=0$.
 	\textbf{while} the stopping conditions are not satisfied \textbf{do}
 	\begin{enumerate}
 		\item Seek an inexact minimizer $\zeta^{l+1}$ of the following problem with Algorithm \ref{SNCG} later:
 		\begin{equation}\label{PPA-subprob}
 			\min_{\zeta\in\mathbb{Y}}\Psi_{k,l}(\zeta):=\Psi_{k}(\zeta)+\frac{\tau_{l}}{2}\|\zeta-\zeta^{l}\|^2.
 		\end{equation}
 		
 		\item Update $\tau_{l+1}\downarrow\underline{\tau}$. Let $l\leftarrow l+1$, and then go to Step 1.
 	\end{enumerate}
 	\textbf{End(while)}
 \end{algorithm}

 Inspired by the inexact criterion in Step (2a) of Algorithm \ref{iLPA}, in the implementation of Algorithm \ref{iLPA}, we terminate Algorithm \ref{PPASN} at $\zeta^{l}$ when $(\zeta^{l},x^{k,j})$ satisfies the relation
 \[
 q_{k,j}(x^{k,j})+\Psi_{k}(\zeta^{l})<({\underline{\gamma}}/2)\|x^{k,j}\!-\!x^k\|^2
 \ \ {\rm for}\ \ x^{k,j}\!=\!P_{\gamma_{k,j}^{-1}}h(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_{k}\zeta^{l}\!+u^k)).
 \]
 This has a little difference from the one in Step (2a) of Algorithm \ref{iLPA} by considering that $-\Psi_{k}(\zeta^{l})$ is only a lower estimation for the optimal value $q_{k,j}(\overline{x}^{k,j})$ of subproblem \eqref{subprobkj}.
 For the convergence analysis of Algorithm \ref{PPASN}, the reader refers to \cite{Roc76,Roc21}.

 Note that $\Psi_{k,l}$ has a Lipschitz continuous gradient mapping. We define its generalized Hessian at $\zeta$ the Clarke Jacobian of $\nabla\Psi_{k,l}$ at $\zeta$, i.e., $\partial^2\Psi_{k,l}(\zeta)\!:=\partial_{C}(\nabla\Psi_{k,l})(\zeta)$. From \cite[Page 75]{Clarke83}, it follows that $\partial^2\Psi_{k,l}(\zeta)d=\widehat{\partial}^2\Psi_{k,l}(\zeta)d$
 for all $d\in\mathbb{Y}$ with
 \[
 \widehat{\partial}^2\Psi_{k,l}(\zeta)
 \!=\!\tau_{l}\mathcal{I}+\alpha_k^{-1}\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta+c^k\big)
 +\gamma_{k,j}^{-1}\mathcal{A}_k\partial_{C}P_{\!\gamma_{k,j}^{-1}}h
 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)\mathcal{A}_k^{*}.
 \]
 By mimicking the proof in \cite[Section 3.3.4]{Ortega70}, every $\mathcal{U}\!\in\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta+c^k\big)$ and every $\mathcal{V}\!\in\partial_{C}P_{\!\gamma_{k,j}^{-1}}h
 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta+u^k)\big)$ are symmetric and positive semidefinite. Along with $\tau_l>0$ for each $l\in\mathbb{N}$, the operator $\tau_lI+\alpha_{k}^{-1}\mathcal{U}+\gamma_{k,j}^{-1}\mathcal{A}_k\mathcal{V}\mathcal{A}_k^*$ is positive definite, so every element in $\widehat{\partial}^2\Psi_{k,l}(\zeta)$ is nonsingular. Thus, the following semismooth Newton method is well defined, whose convergence analysis can be found in \cite{QiSun93,ZhaoST10}.
 %-------------------------------------------------------------------------------------
 \begin{algorithm}[h]
 	\renewcommand{\thealgorithm}{A.1}
 	\caption{\label{SNCG}{\bf\ Semismooth Newton method}}
 	\textbf{Initialization:} Fix $k\in\mathbb{N},j\in[j_k]$ and $l\in\mathbb{N}$. Choose  $\underline{\eta}\!\in(0,1),\beta\in(0,1),\varsigma\in(0,1]$ and $
 	0<c_1<c_2<\frac{1}{2}$. Set $\nu:=0$ and let $\zeta^0=\zeta^{l}$.
 	
 	\textbf{while} the stopping conditions are not satisfied \textbf{do}
 	\begin{enumerate}
 		\item Choose  $\mathcal{U}^{\nu}\in\partial_{C}P_{\!\alpha_k^{-1}}\vartheta_1\big(\alpha_k^{-1}\zeta^{\nu}+c^k\big)$
 		and $\mathcal{V}^{\nu}\in\partial_{C}P_{\!\gamma_{k,j}^{-1}}h 		 \big(x^k\!-\!\gamma_{k,j}^{-1}(\mathcal{A}_k^*\zeta^{\nu}+u^k)\big)$.
 		
 		\item Solve the following linear system via the conjugate gradient (CG) method
 		\[
 		\mathcal{W}^{\nu}d=-\nabla\Psi_{k,l}(\zeta^{\nu})\ \ {\rm with}\ \
 		\mathcal{W}^{\nu}=\tau_l\mathcal{I}+\alpha_{k}^{-1}\mathcal{U}^{\nu}+\gamma_{k,j}^{-1}\mathcal{A}_k\mathcal{V}^{\nu}\mathcal{A}_k^*,
 		\]
 		\ \ to find $d^{\nu}$ such that  $\|\mathcal{W}^{\nu}d^{\nu}+\nabla\Psi_{k,l}(\zeta^{\nu})\|\le\min(\underline{\eta},\|\nabla\Psi_{k,l}(\zeta^{\nu})\|^{1+\varsigma})$.
 		
 		\item Let $m_{\nu}$ be the smallest nonnegative integer $m$ such that
 		\begin{align*}
 			\Psi_{k,l}(\zeta^{\nu}\!+\!\beta^md^{\nu})\leq\Psi_{k,l}(\zeta^{\nu})+c_1\beta^m\langle\nabla\Psi_{k,l}(\zeta^{\nu}),d^{\nu}\rangle\\
 			\vert\langle\nabla\Psi_{k,l}(\zeta^{\nu}\!+\!\beta^md^{\nu}),d^{\nu}\rangle\vert\le c_2\vert\langle\nabla\Psi_{k,l}(\zeta^{\nu}),d^{\nu}\rangle\vert.
 		\end{align*}
 		
 		\item Set $\zeta^{\nu+1}=\zeta^{\nu}+\beta^{m_{\nu}}d^{\nu}$. Let $\nu\leftarrow \nu+1$, and then go to Step 1.
 	\end{enumerate}
 	\textbf{End(while)}
 \end{algorithm}

 In the subsequent sections, we apply Algorithm \ref{iLPA} armed with dPPASN (named iLPA) to model \eqref{SCAD-loss} from matrix completions with outliers and DC programs with nonsmooth components.  All tests are performed in MATLAB on a laptop running on 64-bit Windows System with an Intel Xeon(R) i7-12700H CPU 2.30GHz and 32 GB RAM.
%-----------------------------------------------------------------------------------------------
\subsection{Model \eqref{SCAD-loss} from matrix completions with outliers}\label{sec5.3}

 We consider model \eqref{SCAD-loss} with $\vartheta\equiv\vartheta_1-\vartheta_2$ where  $\vartheta_1(y)\!:=\|y\|_1$ and $\vartheta_2(z)\!:=\frac{1}{\rho}\sum_{i=1}^{m}\!\theta_{a}(\rho|z_i|)$ for $y,z\in\mathbb{R}^m$. Here, $\rho>0$ is a given parameter and $\theta_{a}\!:\mathbb{R}\to\mathbb{R}$ is the function given by
 \[
  \theta_{a}(s)=\left\{\begin{array}{cl}
	0 & \textrm{if}\ s\leq \frac{2}{a+1};\\
	\frac{((a+1)s-2)^2}{4(a^2-1)} & \textrm{if}\ \frac{2}{a+1}<s\leq \frac{2a}{a+1};\\
	s-1 & \textrm{if}\ s>\frac{2a}{a+1}.
 \end{array}\right.\quad{\rm with}\ \ a>1.
 \]
 From \cite[Example 3]{ZhangPan22}, $\lambda\theta_{a}$ with $\lambda>0, \rho=\frac{2}{(a+1)\lambda}$ is exactly the SCAD function $p_{\lambda}$ in \cite{Fan01}. Obviously, problem \eqref{SCAD-loss} takes the form of \eqref{prob} with $\mathbb{X}=\mathbb{R}^{n_1\times r}\times\mathbb{R}^{n_2\times r},\mathbb{Y}=\mathbb{Z}=\mathbb{R}^m$, $F(x)=G(x)=\mathcal{A}(UV^{\top})-b$ and $ h(x)=\lambda(\|U\|_{2,1}\!+\!\|V\|_{2,1})$ for $x=(U,V)\in\mathbb{X}$.
 By the smoothness of $\vartheta_2$, the vector $u^k$ in section \ref{sec5.1} becomes $-\mathcal{A}_k^*(\nabla\vartheta_2(F(W^k)))$ with
 \[
   \nabla\vartheta_2(F(x^k))=\min\Big[1,\max\Big(0,\!\frac{(a+1)\rho|F(x^k)|-2a}{2(a\!-\!1)}\Big)\Big].
 \]
 Inspired by the numerical tests in \cite{ZhangPan22}, we choose $a=4$ and $\rho=0.01$ for the experiments.

 To formulate the sampling operator $\mathcal{A}\!:\mathbb{R}^{n_1\times n_2}\to\mathbb{R}^m$, we assume that a random index set $\Omega=\!\big\{(i_t,j_t)\!:\ t=1,\ldots,m\big\}\subset([n_1]\times[n_2])^m$ is available, and the samples of the indices are drawn independently from a general sampling distribution $\Pi\!=\{\pi_{kl}\}_{k\in[n_1],l\in[n_2]}$ on $[n_1]\times[n_2]$. Note that we consider the sampling scheme with replacement. We adopt the same non-uniform sampling scheme as in \cite{Fang18}, i.e., take $\pi_{kl}=p_kp_l$ for each $(k,l)$ with
 \begin{equation}\label{sampling-scheme}
  (S1)\ \
  p_k=\!\left\{\begin{array}{ll}
 		2p_0& {\rm if}\ k\le\frac{n_1}{10} \\
 		4p_0& {\rm if}\ \frac{n_1}{10}\le k\le \frac{n_1}{5}\\
 		p_0& {\rm otherwise}\\
 	\end{array}\right.\ \ {\rm or}\ \
 	(S2)\ \ p_k=\!
 	\left\{\begin{array}{ll}
 		3p_0& {\rm if}\ k\le\frac{n_1}{10} \\
 		9p_0& {\rm if}\ \frac{n_1}{10}\le k\le \frac{n_1}{5}\\
 		p_0& {\rm otherwise}\\
 	\end{array}\right.
 \end{equation}
 where $p_0>0$ is a constant such that $\sum_{k=1}^{n_1}p_k=1$ or $\sum_{l=1}^{n_2}p_l=1$. Then, it holds that
 \[
 \mathcal{A}(X):=(X_{i_1,j_1},X_{i_2,j_2},\ldots,X_{i_m,j_m})^{\top}\quad{\rm for}\ X\in\mathbb{R}^{n_1\times n_2}.
 \]
 Let $b=\mathcal{A}(M_{\Omega})$ where $M_{\Omega}$ is an $n_1\times n_2$ matrix with $[M_{\Omega}]_{i_tj_t}=0$ if $(i_t,j_t)\notin\Omega$ and
 \begin{equation}\label{observe}
  [M_{\Omega}]_{i_t,j_t}=M_{i_t,j_t}^*+\varpi_t\ \ {\rm for}\ \ t=1,2,\ldots,m,
 \end{equation}
 where $M^*\!\in\mathbb{R}^{n_1\times n_2}$ represents the true matrix of rank $r^*$ for synthetic data and for real data a matrix drawn from the original incomplete data matrix, and $\varpi=(\varpi_1,\ldots,\varpi_m)^{\top}$ is a sparse noisy vector. The nonzero entries of $\varpi$ obey the following several kinds of distributions: {\bf(I)} $N(0,10^2)$; {\bf(II)} Student's t-distribution with $4$ degrees of freedom $2\times t_4$; {\bf(III)} Cauchy distribution with density $d(u)=\frac{1}{\pi(1+u^2)}$; {\bf(IV)} mixture normal distribution $N(0,\sigma^2)$ with $\sigma\sim {\rm Unif}(1,5)$; {\bf(V)} Laplace distribution with density $d(u)=0.5\exp(-|u|)$.

 For the subsequent tests, we always choose $r=\min(100,\lfloor\frac{\min(m,n)}{2}\rfloor)$ and
 $\lambda=c_{\lambda}\|b\|$ for model \eqref{SCAD-loss}, where the constant $c_\lambda>0$ is specified in the corresponding experiments. The parameters of Algorithm \ref{iLPA} are chosen as follows:
 \[
 \varrho = 2,\,\underline{\gamma}=\max(10,{\max(m,n)}/{100}),\,\overline{\gamma}=10^6,\,\, \gamma_{k,0}=\underline{\gamma},
 \]
 and the parameter $\alpha_k$ involved in $\mathcal{Q}_{k,j}=\gamma_{k,j}\mathcal{I}+\alpha_k\nabla\!F(x^k)F'(x^k)$ is updated by
 \[
 \alpha_{k}=\max(\alpha_{k-1}/1.2,10^{-3})\ \ {\rm with}\ \  \alpha_0=0.5
 \ \ {\rm when}\ \textrm{mod}(k,3)=0.
 \]
 By Remark \ref{remark-alg} (iii), we terminate Algorithm \ref{iLPA} at $x^k$ under one of the following conditions:
 \begin{equation}\label{stop-cond}
 \frac{\|x^k-x^{k-1}\|_F}{1+\|b\|}\le \epsilon_1\ \ {\rm or}\ \
 \frac{\max_{j\in[9]}|\Phi(x^k)-\Phi(x^{k-j})|}{\max(1,\Phi(x^k))}\le \epsilon_2\ {\rm for}\ k\ge\overline{k}\ \ {\rm or}\ \
 k>2000,
 \end{equation}
 where $\epsilon_1=5\times 10^{-6},\epsilon_2=5\times 10^{-4}$ for synthetic data, and $\epsilon_1= 10^{-4},\epsilon_2=5\times 10^{-3}$ for real data. We compare the performance of iLPA with that of the Polyak subgradient method (see \cite{Charisopoulos21,LiZhu20}) for solving \eqref{SCAD-loss}, whose iterate steps are described as follows:
  %-------------------------------------------------------------------------------------
 \begin{algorithm}[H]
 \renewcommand{\thealgorithm}{2} 	
 \caption{\label{subGrad}{\bf(subGM)\ Subgradient method for model \eqref{SCAD-loss}}}
 	\textbf{Initialization:} Choose an initial point $x^0$.
 	
 	\textbf{For} $k=0,1,2,\cdots$ \textbf{do}
 	\begin{enumerate}
 	\item Compute a subgradient $\zeta^k\in\partial\Phi(x^k)$.
 	
 	\item Set $x^{k+1}=x^k-\frac{\Phi(x^k)-\min_{x\in\mathbb{X}}\Phi(x)}{\|\zeta^k\|^2}\zeta^k$.
 	\end{enumerate}
 	\textbf{End(For)}
 \end{algorithm}
 Since $\min_{x\in\mathbb{X}}\Phi(x)$ is unavailable, we replace the step-size $\frac{\Phi(x^k)-\min_{x\in\mathbb{X}}\Phi(x)}{\|\zeta^k\|^2}$ with the approximate one $\frac{0.05\Phi(x^k)}{\|\zeta^k\|^2}$ in computation. Although there is no convergence certificate for Algorithm \ref{subGrad}, we here use it just for numerical comparisions, and as will be illustrated later, it has a desirable performance with such a step-size. For the fairness of comparisons, we adopt the same starting point $x^0$ as the one used for iLPA, and the same stopping condition as the one used for iLPA, i.e., one of the conditions in \eqref{stop-cond} except that $\overline{k}=10$ is used for iLPA, while $\overline{k}=500$ is used for subGM.
%--------------------------------------------------------------------------------------------------------
\subsubsection{Numerical results for synthetic data}\label{sec5.2.1}

 We generate the true $M^*\!\in\mathbb{R}^{n_1\times n_2}$ randomly by $M^*=M_{L}^*(M_{R}^*)^{\top}$, where the entries of $M_{L}^*\in\mathbb{R}^{n_1\times r^*}$ and $M_{R}^*\in\mathbb{R}^{n_2\times r^*}$ are sampled independently from the standard normal distribution $N(0,1)$, and set the sparsity of the noise vector $\varpi$ as $\lfloor0.3m\rfloor$. We choose $x^0=(U_{1}\Sigma_{r}^{1/2},V_{1}\Sigma_{r}^{1/2})$ as the starting point of iLPA and subGM, where $U_{1}$ and $V_1$ are the matrix consisting of the first $r$ largest left and
 right singular vectors of $M_{\Omega}$, respectively, and $\Sigma_{r}$ is the diagonal matrix consisting of the first $r$ largest singular values of $M_{\Omega}$ arranged in an nonincreasing order. We adopt the relative error (RE) to evaluate the effect of matrix recovery, defined by $\frac{\|x^{\rm out}-M^*\|_F}{\|M^*\|_F}$ where $x^{\rm out}$ means the output of a solver.

 We first check how the relative errors yielded by two solvers vary with $c_{\lambda}$ or $\lambda$.  Figure \ref{fig0} shows that for iLPA and subGM, there exists an interval of $\lambda$ such that the stationary points yielded by them have satisfactory relative errors and their ranks coincide with the true rank $r^*$, but such an interval of $\lambda$ for iLPA is clearly larger than the one for subGM. We next take a look at how the relative errors yielded by iLPA vary with the sparsity of $\varpi$ and the CPU time required by iLPA vary with the dimension $n_1=n_2$ under a fixed $\lambda=0.06\|b\|$. Figure \ref{fig1} (a) shows that the relative
  error becomes higher as the sparsity of $\varpi$ becomes worse, but when the number of nonzero entries of $\varpi$ is $\lfloor 0.8m\rfloor$, its relative error is still lower than 0.065, and Figure \ref{fig1} (b) indicates that the CPU time required by iLPA increases mildly as the dimension $n_1=n_2$ becomes larger. All curves in Figures \ref{fig0}-\ref{fig1} are the average results obtained by running $10$ examples generated randomly with noise of type V, $n_1=n_2=1000,r^*=5$ and sampling ratio $\textrm{SR}=0.25$.
%-----------------------------------------------------------------------------------------------
 \begin{figure}[H]
	\centering
	\subfigure{\includegraphics[height=6.5cm,width=6.5cm]{fig_lambda_iLPA.eps}}
	\subfigure{\includegraphics[height=6.5cm,width=6.5cm]{fig_lambda_subgrad.eps}}
	\caption{The curves of relative error and rank for two solvers as $c_{\lambda}$ increases}	\label{fig0}
	\subfigure[]{\includegraphics[height=6.0cm,width=6.5cm]{fig_noise.eps}}
	\subfigure[]{\includegraphics[height=6.0cm,width=6.5cm]{fig_time.eps}}
	\caption{Relative error varies with sparsity of $\varpi$ and CPU time varies with $n_1$}
	\label{fig1}
\end{figure}

 Next we consider the recovery effect and running time (in seconds) of two solvers under different setting of $n_1=n_2, r^*$ and $\textrm{SR}$. Table \ref{tab1} reports the average results obtained by running $10$ examples generated randomly under each setting. We see that the average relative errors and ranks yielded by iLPA for five kinds of outliers are all lower than those yielded by subGM, although the former is applied to model \eqref{SCAD-loss} with the same $c_{\lambda}$ except for noise of type III, while the latter is applied to \eqref{SCAD-loss} with a carefully selected $c_{\lambda}$. The average CPU time required by subGM is at least 5 times more than that of iLPA.
%---------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \setlength{\abovecaptionskip}{-0.1cm}
 \begin{table}[h]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.0pt}
 	\renewcommand\arraystretch{1.2}
 	\centering
 	\scriptsize
 	\caption{\small Average RE and time for examples generated with the sampling scheme S1}\label{tab1}
 	\scalebox{1}{
 	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
 			%\hline
 	\Xhline{0.7pt}
 	& & \multicolumn{4}{l}{\  iLPA ($n_1\!=\!1000$,$r^*\!\!=\!10$)}&\multicolumn{4}{l||}{\  subGM ($n_1\!=\!1000$,$r^*\!\!=\!10$)}& \multicolumn{4}{l}{\ iLPA ($n_1\!=\!3000$,$r^*\!\!=\!15$)}&
 	\multicolumn{4}{l}{\ subGM ($n_1\!\!=\!3000$,$r^*\!\!=\!15$)}\\
 %%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
 	\hline		
 	$\varpi$ & SR&$c_{\lambda}$& RE & rank  &time&$c_{\lambda}$&  RE & rank & time&$c_{\lambda}$& RE & rank  &time&$c_{\lambda}$&  RE & rank & time\\
 	\hline			
 	&0.15 &0.06&\ {\color{red}\bf 3.78e-2} & 10 & 4.02 &0.06&\ 9.97e-2 & 10 & 51.5     &0.06& \ {\color{red}\bf 7.53e-4} & 15 & 22.1 &0.06&\ 4.00e-2 & 16 & 189.8\\
 I	&0.20 &0.06&\ {\color{red}\bf 3.95e-2} & 10 & 5.85 &0.06&\ 6.50e-2 & 10 & 54.6     &0.06&\ {\color{red}\bf 6.23e-4} & 15 & 24.1 &0.06&\ 3.82e-2 & 16 & 206.6 \\
 	&0.25 &0.06&\ {\color{red}\bf 1.04e-3} & 10 & 3.27 &0.06&\ 4.81e-2 & 10 & 55.8     &0.06&\ {\color{red}\bf 5.93e-4} & 15 & 26.0 &0.06&\ 3.09e-2 & 15 & 245.5\\
 	\hline
 	&0.15 &0.06&\ {\color{red}\bf 5.41e-3} & 10 & 4.94 &0.10&\ 2.78e-2  & 10 & 68.3    &0.06&\ {\color{red}\bf 7.23e-4} & 15 & 38.2 &0.15&\ 2.42e-2  & 16 & 207.4\\
 II	&0.20 &0.06&\ {\color{red}\bf 1.44e-3} & 10 & 5.30 &0.10&\ 2.40e-2  & 10 & 48.7    &0.06&\ {\color{red}\bf 5.87e-4} & 15 & 40.3 &0.15&\ 1.84e-2  & 15 & 208.2\\
 	&0.25 &0.06&\ {\color{red}\bf 1.07e-3} & 10 & 5.18 &0.10&\ 9.30e-3  & 10 & 50.2    &0.06&\ {\color{red}\bf 5.13e-4} & 15 & 42.4 &0.15&\ 1.13e-2  & 15 & 235.0\\
 	\hline
 	&0.15 &0.02&\ {\color{red}\bf 7.23e-2}& 10 & 16.2  &0.03 &\ 1.63e-1 & 11 & 48.9   &0.02&\ {\color{red}\bf 1.54e-3} & 15 & 91.8  &0.10&\ 8.00e-2 & 16 & 188.7 \\
III	&0.20 &0.02&\ {\color{red}\bf 5.36e-3}& 10 & 15.6  &0.03 &\ 7.51e-2 & 11 & 53.6   &0.02&\ {\color{red}\bf 1.69e-2} & 15 & 422.4 &0.10&\ 1.21e-1 & 16 & 208.1\\
 	&0.25 &0.02&\ {\color{red}\bf 2.27e-3}& 10 & 12.3  &0.03 &\ 4.78e-2 & 11 & 55.6   &0.02&\ {\color{red}\bf 9.53e-4} & 15 & 180.2 &0.10&\ 9.00e-3 & 16 & 231.7\\
 			
    \hline
 	&0.15 &0.06&\ {\color{red}\bf 7.62e-3}& 10 & 4.22  &0.08 &\ 2.99e-2 & 10 & 45.9   &0.06&\ {\color{red}\bf 6.50e-4} & 15 & 31.9 &0.15&\ 3.38e-2 & 16 & 182.7\\
IV	&0.20 &0.06&\ {\color{red}\bf 1.69e-3}& 10 & 4.64  &0.08 &\ 2.00e-2 & 10 & 48.6   &0.06&\ {\color{red}\bf 5.35e-4} & 15 & 34.1 &0.15&\ 2.58e-2 & 15 &200.5\\
	&0.25 &0.06&\ {\color{red}\bf 1.15e-3}& 10 & 4.49  &0.08 &\ 1.10e-2 & 10 & 53.6   &0.06&\ {\color{red}\bf 5.44e-4} & 15 & 39.3 &0.15&\ 1.77e-2 & 15 & 247.0\\
	\hline
 			
   &0.15 &0.06&\ {\color{red}\bf 3.43e-3}& 10 & 5.22  &0.15 &\ 4.86e-2 & 11 & 82.0   &0.06&\ {\color{red}\bf 7.37e-4} & 15 & 40.2 &0.15&\ 2.09e-2  & 15 & 202.7\\
V &0.20 &0.06&\ {\color{red}\bf 1.50e-3}& 10 & 5.64  &0.15 &\ 2.69e-2 & 10 & 64.9   &0.06&\ {\color{red}\bf 7.15e-4} & 15 & 45.3 &0.15&\ 1.50e-2  & 15 & 209.0\\
  &0.25 &0.06&\ {\color{red}\bf 1.18e-3}& 10 & 6.27  &0.15 &\ 1.35e-2 & 10 & 52.8   &0.06&\ {\color{red}\bf 6.20e-4} & 15 & 45.4 &0.15&\ 9.80e-3  &15 & 232.9 \\
 			
 \Xhline{0.7pt}
 \end{tabular}}
 \end{table}

%--------------------------------------------------------------------------------------------------------
\subsubsection{Numerical results for real data}\label{sec5.2.2}

 We test iLPA and subGM with matrix completion problems on some real data sets, including the jester joke dataset, the movieLens dataset, and the netflix dataset. For each data set, let $M^0$ represent the original incomplete data matrix such that the $i$th row of $M^0$ corresponds to the ratings given by the $i$th user. We first consider the jester joke dataset (see \url{http://www.ieor.berkeley.edu/~goldberg/jester-data/}), and more detailed descriptions can be found in \cite{Toh10,Chen12}. Due to the large number of users, as in \cite{Chen12}, we first randomly select $n_1$ users' ratings from $M^0$, and then randomly permute the ratings from the users to generate $M^*\in\mathbb{R}^{{n_1}\times 100}$. Next we generate a set of observed indices $\Omega$ with the sampling scheme S1, and then the observation matrix $M_{\Omega}$ via \eqref{observe}. Since we can only observe those entries $M_{jk}$ with $(j,k)\in\Omega$, and $M_{jk}$ is available, the actual sampling ratio is less than the input SR. Since many entries are unknown, we cannot compute the relative error as we did for the simulated data. Instead, we take the metric of the normalized mean absolute error (NMAE) to measure the accuracy of $X^{\rm out}$:
 \[
  {\rm NMAE}=\frac{\sum_{(i,j)\in\Gamma\backslash\Omega}|X^{\rm out}_{i,j}-M_{i,j}|}
  {|\Gamma\backslash\Omega|(r_{\rm max}-r_{\rm min})}\ \ {\rm with}\ X^{\rm out}=U^{\rm out}(V^{\rm out})^{\top},
 \]
 where $\Gamma:=\{(i,j)\in[n_{1}]\times[100]\!: M_{ij}\ \textrm{is given}\}$ denotes the set of indices for which $M_{ij}$ is given, $r_{\rm min}$ and $r_{\rm max}$ denote the lower and upper bounds for the ratings, respectively. In the jester joke dataset, the range is $[-10,10]$. Thus, we have $r_{\rm max}-r_{\rm min}=20$.

 For the jester joke dataset, we consider different settings of $n_1$ and SR, and report the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting in Tables \ref{tabJester1}-\ref{tabJester3}. We see that iLPA yields comparable even a little better results (lower NMAE and rank) than subGM does for all jester examples.
%---------------------------------------------------------------------------------
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for jester-1 dataset}\label{tabJester1}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad \ iLPA ($n_{1}\!=\!1000$)}&\multicolumn{4}{l||}{\quad  subGM ($n_1\!=\!1000$)}& \multicolumn{4}{l}{\quad \ iLPA ($n_{1}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{1}\!\!=\!5000$)}\\
			%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			\hline
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& {\color{red}\bf 0.2042} & 4 & 0.15 &0.2&0.2044 & 4 & 0.78     &0.2&  0.2015 & 9 & 1.56 &0.2& 0.2027 & 8 & 10.6\\
			I	&0.20 &0.2& {\color{red}\bf 0.1986} & 3 & 0.14 &0.2&0.1990& 3 & 0.40      &0.2&  0.1983 & 12 & 1.72 &0.2& 0.1989 & 11 & 11.4 \\
			&0.25 &0.2& {\color{red}\bf 0.1932} & 3 & 0.15 &0.2&0.1935 & 3 & 0.43     &0.2& 0.1914 & 14 & 1.61 &0.2& 0.1925 & 13 & 11.2\\
			\hline
			
			&0.15 &0.2& 0.1940 & 8 & 0.18 &0.2& 0.1942 & 7 & 1.51                    &0.2& 0.1912 & 13 & 1.96 &0.2& 0.1921 & 11 & 10.8\\
			II	&0.20 &0.2& 0.1884 & 9 & 0.20 &0.2 & 0.1886 & 7 & 1.52                   &0.2& 0.1887 & 16 & 1.83 &0.2& 0.1888 & 15 & 11.4\\
			&0.25 &0.2& {\color{red}\bf0.1829} & 8 & 0.21 &0.2&0.1830 & 8 & 1.40     &0.2& 0.1811& 17 & 1.62 &0.2& 0.1813 & 16 & 11.5\\
			\hline
			
			&0.15 &0.02& {\color{red}\bf 0.2007} & 11 & 0.21  &0.02& 0.2012 & 12 & 1.20  &0.02&0.1946 & 18 & 1.71 &0.02& 0.1953 & 15 & 10.7\\
			III	&0.20 &0.02& {\color{red}\bf 0.1972} & 16 & 0.21 &0.02 & 0.1984 & 16 & 1.12  &0.02& 0.1931 & 23 & 2.01 &0.02& 0.1941 & 21 & 11.3 \\
			&0.25 &0.02& {\color{red}\bf 0.1879} & 13 & 0.23 &0.02&0.1886 & 14 & 1.31    &0.02& 0.1829 & 25 & 2.38 &0.02& 0.1841 & 24 & 11.4\\
			\hline
			&0.15 &0.2&  0.1959 & 8 & 0.19 &0.2& 0.1961 & 7 & 1.47                                  &0.2& 0.1928 & 13 & 1.90 &0.2& 0.1936 & 11 & 10.8\\
			IV	&0.20 &0.2&  0.1901 & 9 & 0.19 &0.2 & 0.1902 & 7 & 1.38                                  &0.2& 0.1903 & 16 & 1.76 &0.2& 0.1905 & 15 & 11.4 \\
			&0.25 &0.2& {\color{red}\bf 0.1847} & 8 & 0.22 &0.2&{\color{red}\bf 0.1847} & 8 & 1.31  &0.2& 0.1827 & 17 & 1.74 &0.2& 0.1832 & 16 & 11.4\\
			\hline
			&0.15 &0.2& 0.1934 & 8 & 0.19  &0.2& 0.1936 & 7 & 1.45                  &0.2&  0.1907 & 13 & 2.01 &0.2& 0.1915 & 12 & 10.5\\
			V	&0.20 &0.2&\  0.1881 & 9 & 0.21 &0.2 & 0.1882 & 7 & 1.53                &0.2&  0.1883 & 16 & 1.73 &0.2&{\color{red}\bf 0.1883} & 15 & 10.6 \\
			&0.25 &0.2& 0.1825 & 9 & 0.21 &0.2&{\color{red}\bf0.1824} & 8 & 1.66                    &0.2& 0.1805 & 17 & 1.65 &0.2& 0.1808 & 16 & 10.4\\
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for jester-2 dataset}\label{tabJester2}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad  iLPA ($n_{1}\!=\!1000$)}&\multicolumn{4}{l||}{\quad  subGM ($n_1\!=\!1000$)}& \multicolumn{4}{l}{\quad iLPA ($n_{1}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{1}\!\!=\!5000$)}\\
		%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			\hline
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& {\color{red}\bf 0.2034} & 4 & 0.13 &0.2& 0.2036 & 4 & 0.83     &0.2& {\color{red}\bf 0.2016} & 9 & 1.50 &0.2& 0.2028 & 9 & 10.7\\
			I	&0.20 &0.2& {\color{red}\bf 0.1989} & 2 & 0.13 &0.2 & 0.1993 & 3 & 0.45    &0.2&  0.1964 & 12 & 1.68 &0.2& 0.1974 & 11 & 11.2 \\
			&0.25 &0.2& {\color{red}\bf 0.1936} & 2 & 0.15 &0.2&0.1939 & 3 & 0.38      &0.2&  0.1906 & 14 & 1.65 &0.2& 0.1917 & 13 & 11.3\\
			\hline
			&0.15 &0.2&0.1933 & 8 & 0.19 &0.2& 0.1938 & 7 & 1.37      &0.2& 0.1914 & 13 & 2.09 &0.2& 0.1921 & 11 & 10.8\\
			II	&0.20 &0.2&0.1892 & 9 & 0.19 &0.2& 0.1895 & 8 & 1.41      &0.2& 0.1872 & 16 & 1.95 &0.2& 0.1874 & 14 & 11.2 \\
			&0.25 &0.2& 0.1827 & 8 & 0.23 &0.2&{\color{red}\bf 0.1826} & 8 & 1.58          &0.2& 0.1800 & 17 & 1.49 &0.2& 0.1804 & 15 & 11.2\\		
			\hline
			
			&0.15 &0.02& {\color{red}\bf 0.2025} & 16 & 0.24 &0.02& 0.2032 & 17 & 1.38   &0.02& 0.1948& 18 & 1.75 &0.02& 0.1955 & 16 & 10.8\\
			III	&0.20 &0.02& {\color{red}\bf 0.1973} & 16 & 0.22 &0.02 & 0.1989 & 17 & 1.00  &0.02& 0.1914 & 22 & 1.86 &0.02& 0.1921 & 11 & 11.4\\
			&0.25 &0.02& 0.1889 & 14 & 0.27 &0.02&{\color{red}\bf 0.1888} & 15 & 1.33    &0.02& 0.1819 & 25 & 2.32 &0.02& 0.1830 & 24 & 11.3\\		
			\hline
			
			&0.15 &0.2& 0.1952 & 8 & 0.19 &0.2 & 0.1957 & 7 & 1.36                  &0.2&0.1931 & 13 & 1.97 &0.2& 0.1939 & 11 & 10.9\\
			IV	&0.20 &0.2& 0.1910 & 9 & 0.19 &0.2 & 0.1912 & 8 & 2.40                  &0.2& 0.1888 & 16 & 1.96 &0.2& 0.1891 & 14 & 11.3\\
			&0.25 &0.2& {\color{red}\bf0.1847} & 8 & 0.22 &0.2 &{\color{red}\bf 0.1847} & 8 & 1.31  &0.2& 0.1819 & 17 & 1.51 &0.2& 0.1823 & 15 & 11.2\\	
			\hline
			
			&0.15 &0.2& 0.1927 & 8 & 0.20 &0.2& 0.1929 & 7 & 1.40                  &0.2& 0.1907 & 13 & 2.04 &0.2& 0.1914 & 12 & 10.7\\
			V	&0.20 &0.2& 0.1882 & 9 & 0.20 &0.2& 0.1885 & 8 & 1.47                  &0.2& 0.1865 & 16 & 2.10 &0.2& 0.1868 & 14 & 11.3\\
			&0.25 &0.2&{\color{red}\bf 0.1823} & 8 & 0.23 &0.2&{\color{red}\bf 0.1823} & 8 & 1.56  &0.2& 0.1795 & 17 & 1.65 &0.2& 0.1799 & 15 & 11.2\\	
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}
%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[h]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for jester-3 dataset}\label{tabJester3}
	\scalebox{1}{
	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad  iLPA ($n_{1}\!=\!1000$)}&\multicolumn{4}{l||}{\quad subGM ($n_1\!=\!1000$)}& \multicolumn{4}{l}{\quad iLPA ($n_{1}\!=\!5000$)}&\multicolumn{4}{l}{\quad subGM ($n_{1}\!\!=\!5000$)}\\
		%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			\hline
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			&0.15 &0.2& 0.2285 & 3 & 0.19 &0.2& 0.2256 & 4 & 0.60      &0.2& {\color{red}\bf 0.2218} & 5 & 1.46 &0.2& 0.2233 & 5 & 9.42\\
			I	&0.20 &0.2& 0.2256 & 3 & 0.16 &0.2& 0.2214 & 4 & 0.98      &0.2&  0.2198 & 6 & 1.60 &0.2& 0.2214 & 5 & 10.5  \\
			&0.25 &0.2& 0.2208 & 3 & 0.17 &0.2& 0.2195 & 4 & 0.90      &0.2& {\color{red}\bf 0.2153} & 6 & 1.49 &0.2& 0.2171 & 6 & 10.9\\
			\hline
			&0.15 &0.2& {\color{red}\bf 0.2168} & 4 & 0.16 &0.2& 0.2170 & 5 & 1.17     &0.2&  0.2154 & 7 & 2.15 &0.2&  0.2155 & 6 & 10.0\\
			II	&0.20 &0.2& {\color{red}\bf 0.2119} & 5 & 0.15 &0.2 & 0.2121 & 5 & 1.13    &0.2& {\color{red}\bf 0.2127} & 7 & 2.50 &0.2& 0.2133 & 7 & 10.4 \\
			&0.25 &0.2& 0.2085 & 6 & 0.15 &0.2&0.2087 & 5 & 1.25       &0.2&  0.2067 & 9 & 2.49 &0.2& 0.2074  & 8& 10.6\\	
			\hline
			&0.15 &0.02& 0.2309 & 11 & 0.33 &0.02& 0.2255 & 13 & 1.19     &0.02& 0.2215 & 15 & 2.84 &0.02&\ 0.2209 & 16 & 10.0\\
			III	&0.20 &0.02& 0.2260 & 14 & 0.35 &0.02&0.2232 & 18 & 1.17     &0.02& {\color{red}\bf 0.2140}  &8 & 1.95 &0.02& 0.2144 & 8 & 10.3 \\
			&0.25 &0.02& 0.2235 & 13 & 0.22 &0.02& 0.2228 & 15 & 1.01     &0.02& 0.2104 & 15 & 2.49 &0.02& 0.2115  & 14& 10.7\\	
			\hline
			&0.15 &0.2& {\color{red}\bf 0.2180}& 3 & 0.16 &0.2& 0.2182 & 5 & 1.06                 &0.2& {\color{red}\bf 0.2164} & 6 & 2.31 &0.2& 0.2166 & 6 & 10.1\\
			IV	&0.20 &0.2& {\color{red}\bf 0.2131}& 5 & 0.16 &0.2 & 0.2134 & 5 & 1.10                &0.2& {\color{red}\bf0.2137} &7 & 2.35 &0.2& 0.2145 & 7 & 10.4 \\
			&0.25 &0.2&{\color{red}\bf 0.2104}& 5& 0.15 &0.2&{\color{red}\bf0.2104} & 5 & 1.27    &0.2& {\color{red}\bf0.2085} & 9 & 2.61 &0.2& 0.2092   & 9 & 10.6\\	
			\hline
			
			&0.15 &0.2& {\color{red}\bf 0.2165}  & 4 & 0.18 &0.2& 0.2167 & 5 & 1.12       &0.2& 0.2152 & 6 & 2.21 &0.2& {\color{red}\bf0.2151} & 6 & 9.97\\
			V	&0.20 &0.2& {\color{red}\bf 0.2116} & 5 & 0.17 &0.2 & 0.2119 & 5 & 1.17        &0.2& {\color{red}\bf 0.2122}  &7 & 2.44 &0.2& 0.2129 & 7 & 10.3 \\
			&0.25 &0.2& 0.2083 & 6 & 0.17 &0.2&0.2084 & 5 & 1.30           &0.2& {\color{red}\bf 0.2062} & 9 & 2.47 &0.2& 0.2069 & 9 & 10.5\\	
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}

 Next we consider the movieLens dataset (see \url{http://www.grouplens.org/node/73}), for which the ratings range is from $r_{\rm min}=1$ to $r_{\rm max}=5$; see \cite{Chen12,Toh10} for more description. For the movie-100K dataset, like \cite{Toh10} we also consider the data matrix $\widetilde{M}^0=M^0-3$, sample the observed entries in two schemes, and obtain the observation matrix $M_{\Omega}$ via \eqref{observe} with $M^*\!=\widetilde{M}^0$. Table \ref{tabMovie-100K} reports the average NMAE, rank and running time (in seconds) after running $10$ times for the setting $(n_1,n_2)=(943,1682)$. We see that iLPA yields better results than subGM does for those examples generated with the sampling scheme S1 for  noise of types III-V, and has a little better preformance than subGM does for those examples generated with the sampling scheme S2 for noise of types II-V.

%---------------------------------------------------------------------------------
\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\setlength\tabcolsep{1.4pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\scriptsize
	\caption{\small Average NMAE and running time of two solvers for movie-100K dataset}\label{tabMovie-100K}
	\scalebox{1}{
		\begin{tabular}{cc|lccc|lccc||lccc|lccc}
			\Xhline{0.7pt}
			& & \multicolumn{4}{l}{\quad \quad \ iLPA (S1)}&\multicolumn{4}{l||}{\quad \quad subGM (S1)}& \multicolumn{4}{l}{\quad \quad \  iLPA (S2)}&
			\multicolumn{4}{l}{\quad \quad subGM (S2)}\\
			%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
			\hline
			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
			\hline	
			
			&0.15 &0.1&  0.2361 & 2 & 2.23 &0.1&  {\color{red}\bf 0.2311} & 2 & 6.34     &0.1&  0.2402 & 3 & 2.41 &0.1&  {\color{red}\bf 0.2342} & 2 & 6.52\\
			I	&0.20 &0.1&0.2246 & 2 & 2.12 &0.1& {\color{red}\bf 0.2229} & 2 & 6.48       &0.1&0.2259 & 2 & 2.47 &0.1& {\color{red}\bf 0.2240} & 2 & 6.70 \\
			&0.25 &0.1&  0.2186 & 2 & 2.37 &0.1& {\color{red}\bf 0.2176} & 2 & 6.57     &0.1&  0.2224 & 2 & 2.77 &0.1&  {\color{red}\bf \ 0.2201 }& 2 & 6.72 \\			
			\hline	
			
			&0.15 &0.3&  0.2249 & 3 & 2.42 &0.3&  {\color{red}\bf 0.2248} & 2 & 6.59    &0.3&  {\color{red}\bf0.2275} & 1 & 2.34 &0.3&   0.2284 & 2 & 6.44\\
			II  &0.20 &0.3&  0.2171 & 2 & 2.17 &0.3&  0.2174 & 1 & 6.71 &0.3&{\color{red}\bf0.2184} & 1 & 2.32 &0.3&  0.2185 & 2 & 6.72 \\
			&0.25 &0.3& {\color{red}\bf 0.2119} & 1 & 1.92 &0.3&  0.2123 & 1 & 6.74 &0.3& {\color{red}\bf 0.2142} & 1 & 1.95 &0.3&  0.2149 & 3 & 6.59\\
			\hline
			
			
			&0.15 &0.02& {\color{red}\bf  0.2369} & 2 & 1.65 &0.02& 0.2438 & 36 & 10.2  &0.02& {\color{red}\bf0.2397} & 3 & 1.91 &0.02&  0.2441 & 30 & 10.4\\
			III	&0.20 &0.02&{\color{red}\bf0.2312} & 1 & 1.69 &0.02& 0.2343 & 22 & 6.72     &0.02&{\color{red}\bf0.2314} & 2 & 1.77 &0.02&  0.2345& 15 & 6.58 \\
			&0.25 &0.02& {\color{red}\bf 0.2231} & 2 & 2.03 &0.02&  0.2263 & 15 & 6.73                   &0.02& {\color{red}\bf 0.2232} & 2 & 1.83 &0.02&  0.2271 & 9 & 6.59 \\
			\hline
			
			
			&0.15 &0.3& {\color{red}\bf 0.2277} & 1 & 1.69 &0.3&  0.2318 & 1& 6.58          &0.3&  {\color{red}\bf 0.2306} & 1 & 1.76 &0.3&   0.2335& 1 & 6.76\\
			IV	&0.20 &0.3&{\color{red}\bf0.2213} & 1 & 1.56 &0.3&  0.2238& 1 & 6.67            &0.3&{\color{red}\bf0.2226} & 1 & 1.56 &0.3&  0.2251 & 1 & 6.71\\
			&0.25 &0.3& {\color{red}\bf 0.2164} & 1 & 1.53 &0.3&  0.2184 & 1 & 6.76         &0.3& {\color{red}\bf 0.2190} & 1 & 1.49 &0.3&  0.2208 & 1 & 6.71\\
			\hline
			
			
			&0.15 &0.3&  {\color{red}\bf0.2218} & 2 & 2.41 &0.3&   0.2220& 3 & 6.45      &0.3& {\color{red}\bf 0.2268}& 4 & 2.92 &0.3&  0.2270 & 4 & 6.90\\
			V	&0.20 &0.3&{\color{red}\bf0.2152} & 2 & 2.10 &0.3&  0.2153 & 2 & 6.73        &0.3&{\color{red}\bf0.2161} & 3 & 2.07 &0.3&  0.2165 & 3 & 6.59 \\
			&0.25 &0.3&  0.2098& 3 & 2.32 &0.3&  0.2100 & 2 & 6.72      &0.3& {\color{red}\bf 0.2121}& 3 & 2.13 &0.3&  0.2127 & 3 & 6.63\\
			
			\Xhline{0.7pt}
	\end{tabular}}
\end{table}


 For the movie-1M dataset, we first randomly select $n_1$ users and their $n_2$ column ratings from $M^0$ to formulate $M^*\in\mathbb{R}^{n_1\times n_2}$, sample the observed entries with scheme S1, and then obtain the observation matrix $M_{\Omega}$ via \eqref{observe}. We consider the setting of $n_1=n_2$ with $n_1=3000$ and
 the setting of $(n_1,n_2)=(6040,3706)$. Table \ref{tabMovie-1M} reports the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting. We see that iLPA and subGM yield comparable NMAEs and ranks.
%--------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.4pt}
  \renewcommand\arraystretch{1.2}
  \centering
  \scriptsize
  \caption{\small Average NMAE and running time of two solvers for movie-1M dataset}\label{tabMovie-1M}
   \scalebox{1}{
   	\begin{tabular}{cc|lccc|lccc||lccc|lccc}
	\Xhline{0.7pt}
   & & \multicolumn{4}{l}{\  iLPA($n_{1}\!=\!3000$)}&\multicolumn{4}{l||}{\  subGM($n_1\!=\!3000$)}& \multicolumn{4}{l}{\ iLPA($6040\times 3706$)}&
	\multicolumn{4}{l}{\ subGM($6040\times 3706$)}\\
%	\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
	\hline		
	$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
	\hline	
			
	& 0.15 &0.1&  0.2173 & 1 & 7.96 &0.1& 0.2172 & 2 & 33.1  &0.1&  0.2108 & 1 & 24.3 &0.1&  0.2100 & 3 & 75.1\\
 I  & 0.20 &0.1&{\color{red}\bf0.2110} &1&7.65 &0.1&  0.2110&2 &34.0     &0.1&  0.2065 & 1 & 24.1 &0.1& 0.2060 & 2 & 77.5 \\
	&0.25 &0.1& {\color{red}\bf 0.2061} & 1 & 7.39 &0.1&  0.2062 & 3 & 34.4              &0.1& {\color{red}\bf 0.2010} & 1 & 23.7 &0.1&  0.2015 & 3 & 79.8\\
			\hline			
			
	& 0.15 &0.3&  {\color{red}\bf 0.2113} & 1 & 11.5 &0.3&  0.2123 & 1 & 32.5                 &0.3&{\color{red}\bf 0.2048} & 1 & 26.8 &0.3&  0.2049 & 1 & 75.4\\
II	&0.20 &0.3&  {\color{red}\bf 0.2057} & 1 & 9.41 &0.3&  0.2062 & 1 & 32.9 &0.3&  0.2013 & 1 & 26.1 &0.3& 0.2007 & 2 & 76.4\\
	& 0.25 &0.3& {\color{red}\bf 0.2012} & 1 & 9.28 &0.3&{\color{red}\bf0.2012}&1&41.5 &0.3&  0.1967 & 1 & 24.6 &0.3& 0.1959 & 2 & 79.6 \\
			
	\hline
			
	& 0.15 &0.02&  {\color{red}\bf0.2167} & 1 & 6.82 &0.02&  0.2207 & 1 & 32.0     &0.02&  0.2022 & 1 & 25.2 &0.02&  {\color{red}\bf0.2020} & 1 & 75.8\\
III	& 0.20 &0.02&  {\color{red}\bf0.2073} & 1 & 6.66 &0.02&  0.2099 & 1 & 32.6     &0.02&  0.1956 & 2 & 32.9 &0.02& {\color{red}\bf 0.1946} & 2 & 76.4\\
	& 0.25&0.02& {\color{red}\bf 0.2007} & 1 & 7.15 &0.02&  0.2022 & 1 & 33.3      &0.02& {\color{red}\bf 0.1880} & 3 &34.1 &0.02& 0.1881 & 3 & 79.2\\
			
	\hline
	&0.15 &0.3& {\color{red}\bf 0.2166} & 1 & 8.84 &0.3&  0.2187 & 1 & 31.6    &0.3& {\color{red}\bf \ 0.2095 }& 1 & 23.4 &0.3&  0.2104 & 1 & 75.1\\
IV  & 0.20 &0.3&  {\color{red}\bf0.2107} & 1 & 8.42 &0.3& 0.2119 & 1 & 32.1    &0.3& {\color{red}\bf 0.2052} & 1 & 23.6 &0.3&  0.2058 & 1 & 76.5\\
	& 0.25 &0.3& {\color{red}\bf 0.2060} & 1 & 8.48 &0.3&  0.2069 & 1 & 32.5   &0.3& {\color{red}\bf 0.2006} & 1 & 22.3 &0.3&  0.2011 & 1 & 79.4 \\			
			
	\hline
			
	&0.15 &0.3& 0.2086 & 3 & 13.7 &0.3&  0.2095 & 2 & 31.8 &0.3&  0.2028 & 2 & 31.5 &0.3& 0.2019 & 3 & 77.1\\
V	& 0.20 &0.3& 0.2022 & 3 & 13.2 &0.3& 0.2029 & 2 & 41.0      &0.3&  0.1980 & 2 & 30.8 &0.3& 0.1971 & 3 & 78.2\\
	& 0.25 &0.3&  0.1992 & 1 & 9.84 &0.3& 0.1971 & 2 & 32.6     &0.3&  0.1927 & 2 & 29.6 &0.3&  {\color{red}\bf0.1917} & 2 & 80.4\\
			
   \Xhline{0.7pt}
	\end{tabular}}
 \end{table}

 Finally, we consider the netflix dataset \footnote{\url{https://www.kaggle.com/netflix-inc/netflix-prize-data\#qualifying.txt}}. For this dataset, we first randomly select $n_1$ users with $n_1=6000$ and $10000$ and their $n_1=n_2$ column ratings from $M^0$, sample the observed entries with the sampling scheme S1, and then obtain the observation matrix $M_{\Omega}$ via \eqref{observe} with $M^*=M^0$. Table \ref{tabNetflix} reports the average NMAE, rank and running time (in seconds) obtained by running $10$ times for each setting. We see that iLPA yields better results than subGM does for $n_1=6000$, and the two solvers have the comparable NMAEs for $n_1=10000$.
%---------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[h]
  \setlength{\belowcaptionskip}{-0.01cm}
  \setlength\tabcolsep{1.4pt}
  \renewcommand\arraystretch{1.2}
  \centering
  \scriptsize
  \caption{\small Average NMAE and running time of two solvers for netflix dataset}\label{tabNetflix}
  \scalebox{1}{
   \begin{tabular}{cc|lccc|lccc||lccc|lccc}
  \Xhline{0.7pt}
	& & \multicolumn{4}{l}{\  iLPA($n_{1}\!=\!6000$)}&\multicolumn{4}{l||}{\  subGM($n_1\!=\!6000$)}& \multicolumn{4}{l}{\ iLPA($n_{1}\!=\!10000$)}&
	\multicolumn{4}{l}{\ subGM($n_{1}\!\!=\!10000$)}\\
	%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
	\hline		
	$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$&  NMAE & rank & time&$c_{\lambda}$& NMAE & rank  &time&$c_{\lambda}$& NMAE & rank & time\\
	\hline	
	&0.15 &0.1& {\color{red}\bf 0.2259} & 2 & 48.2 &0.1&   0.2274 & 3 & 155.3& 0.1&  0.2186 & 2 & 130.2 &0.1&  0.2178 & 3 & 382.3\\
I	&0.20 &0.1& {\color{red}\bf 0.2200} & 2 & 40.7 &0.1&  0.2206 & 3 & 124.5 & 0.1& {\color{red}\bf 0.2130} & 3 & 110.1 &0.1& 0.2212 & 4 & 495.1 \\
	&0.25 &0.1&  0.2143 & 2 & 41.1 &0.1& 0.2139 & 3 & 119.2 & 0.1& 0.2080 & 2 & 112.8 &0.1&  0.2068 & 5 & 806.2\\
	\hline
			
    &0.15  &0.3& {\color{red}\bf\ 0.2177 }& 2 & 57.8 &0.3&  0.2181 & 3 & 135.9 &0.3&  0.2109 & 2 & 161.3 &0.3& 0.2102 & 3 & 317.3\\
II	&0.20 &0.3& {\color{red}\bf0.2125} & 2 & 57.2 &0.3&  0.2127 & 2 & 127.0 &0.3&  0.2056 & 3 & 143.9 &0.3& {\color{red}\bf 0.2046} & 3 & 319.1  \\
	&0.25 &0.3& {\color{red}\bf 0.2070} & 2 & 54.5 &0.3&  {\color{red}\bf0.2070} & 2 & 117.7  &0.3& 0.2006 & 2 & 140.3 &0.3&  {\color{red}\bf0.1999} & 2 & 320.8\\
	\hline
			
  	&0.15 &0.02&  {\color{red}\bf\ 0.2235 }& 1 & 32.2 &0.02&  0.2264 & 1 & 117.2  &0.02&  0.2071 & 3 & 162.7    &0.02&  {\color{red}\bf0.2060} & 3 & 297.4\\
III  &0.20 &0.02& {\color{red}\bf 0.2144} & 1 & 36.8 &0.02& 0.2159 & 1 & 115.4 &0.02&  0.2006 & 3 & 168.2 &0.02& 0.1988 & 4 & 299.9\\
	&0.25 &0.02& {\color{red}\bf 0.2065} & 2 & 38.5 &0.02&  0.2078 & 2 & 116.2 &0.02&  0.1942 & 5 & 177.2 &0.02&  0.1929 & 6 & 552.7 \\
			\hline
			
	&0.15 &0.3& {\color{red}\bf 0.2228} & 1 & 43.1 &0.3& 0.2245 & 1 & 109.4 &0.3&  0.2165 & 1 & 117.1 &0.3&  0.2161 & 2 & 314.7\\
IV	&0.20 &0.3& 0.2157 & 2 & 42.1 &0.3& 0.2188 & 1 & 111.3 &0.3&  0.2133 & 1 & 108.2 &0.3& 0.2105 & 2 & 316.6 \\
	&0.25 &0.3& 0.2124 & 2 & 39.8 &0.3& 0.2136 & 1 & 112.6 &0.3&  0.2073 & 1 & 112.0 &0.3&  0.2065 & 2 & 322.0\\
	\hline
			
	&0.15  &0.3&  {\color{red}\bf0.2152} & 3 & 68.1 &0.3& 0.2157 & 4 & 159.1 &0.3&  0.2086 & 3 & 181.9 &0.3&  0.2077 & 4 & 312.1\\
V  &0.20  &0.3&  {\color{red}\bf0.2100} & 3 & 62.0 &0.3& 0.2104 & 4 & 128.3 &0.3&  0.2035 & 3 & 163.5 &0.3& {\color{red}\bf 0.2020} & 3 & 313.3 \\
   &0.25 &0.3& {\color{red}\bf 0.2041} & 3 & 56.2 &0.3& 0.2044 & 4 & 146.8  &0.3&  0.1981 & 3 & 150.6 &0.3&  {\color{red}\bf0.1972} & 3 & 318.9 \\
			
  \Xhline{0.7pt}
 \end{tabular}}
\end{table}
 
 The above comparison results show that iLPA yields comparable even a little better results than subGM does, and requires less CPU time than subGM does. We also apply iLPA to some large-scale real dataset, and Table \ref{tabNetflix2} reports the NMAE, rank and running time (in seconds) obtained by running an example for each setting, on a workstation running on 64-bit Windows Operating System with an Intel Xeon(R) W-2245 CPU 3.90GHz and 128 GB RAM. We see that for $50000\times 17770$ netflix and $71567\times 10677$ movie-10M test examples, iLPA yields the favorable outputs in $3100$ seconds, while for $50000\times 10000$ yahoo-music test examples, it requires much more time when ${\rm SR}=0.25$, and their NMAEs from are worse than other examples.
 %-------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
 	\setlength{\belowcaptionskip}{-0.01cm}
 	\setlength\tabcolsep{2.5pt}
 	\renewcommand\arraystretch{1.3}
 	\centering
 	\scriptsize
 	\caption{\small NMAE and running time of iLPA for large-scale examples from three datasets}\label{tabNetflix2}
 	\scalebox{1}{
 		\begin{tabular}{cc|lccc||lccc||lccc}
 			\hline
 			& & \multicolumn{4}{l||}{\  netflix($50000\times 17770$)}&\multicolumn{4}{l||}{\ movie-10M($71567\times 10677$)}&
 			\multicolumn{4}{l}{\ yahoo-music($50000\times 10000$)}\\
 			%\cmidrule(lr){3-6} \cmidrule(lr){7-10}\cmidrule(lr){11-14} \cmidrule(lr){15-18}
 			\hline
 			$\varpi$ & SR\ &\ $c_{\lambda}$& NMAE & \ rank  &time&$\ c_{\lambda}$&  NMAE &\ rank & time&$\ c_{\lambda}$& NMAE & \ rank  &time\\
 			\hline	
 			&0.15 &0.15&  0.2135 & 1 & 1726 & 0.15&  0.2061 & 3 & 1527 &0.08&  0.3556 & 8 & 1615\\
 			I   &0.20 &0.15&  0.2127 & 3 & 1238 & 0.15& 0.2015 & 3 & 1275 &0.08& 0.3550 & 6 & 1304 \\
 			&0.25 &0.15&  0.2035 & 2 & 2035 & 0.15& 0.1955 & 3 &1273 &0.08&  0.3343 & 7 & 3723\\
 			\hline
 			
 			&0.15  &0.35&  0.2072 & 3 & 2087 &0.40&  0.1981 & 6 & 1997 &0.38& 0.3572 & 2 & 1106\\
 			II	&0.20 &0.35&0.2030 & 4 & 1943  &0.40&  0.1936 & 4 & 1875 &0.38& 0.3576 &1 & 815  \\
 			&0.25 &0.35&  0.1941&4 & 1950  &0.40& 0.1855 & 5 & 2104 &0.38&  0.3271 &1 & 9150\\
 			\hline
 			
 			&0.15 &0.1&   0.2014 & 2 & 2666   &0.10&  0.1948 & 2 & 2203 &0.03& 0.3568 & 5 & 862\\
 			III     &0.20 &0.1&  0.1988 & 4 & 2121   &0.10&  0.1892 & 3 & 2044 &0.03& 0.3292 & 1 & 4560\\
 			&0.25 &0.1&  0.1901 & 4 & 2107   &0.10&  0.1800 & 4 & 2511 &0.03&  0.3038 & 3 & 8218 \\
 			\hline
 			
 			&0.15 &0.35&  0.2100 & 3 & 1855     &0.40&  0.2012 & 2 & 1872 &0.24&  0.3556 & 3 & 1728\\
 			IV	&0.20 &0.35& 0.2062 & 3 & 1690     &0.40&  0.1963 & 3 & 1665 &0.24&  0.3563 & 2 & 1099 \\
 			&0.25 &0.35& 0.1974 & 2 & 1882     &0.40&  0.1917 & 3 & 1474 &0.24&  0.3250 & 1 & 7967\\
 			\hline
 			
 			&0.15  &0.35&  0.2050& 5 & 2265  &0.40&  0.1956 & 7 & 2269 &0.40&  0.3570 & 2 & 1233\\
 			V  &0.20&0.35& 0.2004& 5 & 2187  &0.40&  0.1888 & 4 & 3077 &0.40& 0.3573 &1 & 843 \\
 			&0.25 &0.35& 0.1915 &5 & 1934   &0.40&  0.1827 & 8 & 2396 &0.40&0.3208 & 1 & 11581 \\
 			
 			\hline
 	\end{tabular}}
 \end{table}

%------------------------------------------------------------------------------------
\subsection{DC programs with nonsmooth components}\label{sec5.3}
%-------------------------------------------------------------------------------------
 We confirm the efficiency of iLPA on DC programs with nonsmooth components, which take the form of \eqref{prob} with $h\equiv 0$. We apply iLPA to the test examples appearing in the DC literature \cite{Artacho20}, and compare its performance with that of the DC algorithm proposed in \cite{Ferreria21}, a non-monotone boosted DC algorithm (nmBDCA). Now let us describe these examples in terms of model \eqref{prob}, where $\phi(y):=\max_{1\le i\le m}y_i$ for $y\in\mathbb{R}^m$.
 \begin{example}\label{exam5.1}    $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}^{3}\times\mathbb{R},\mathbb{Z}=\mathbb{R}^{3}, \vartheta_1(y,t)=\phi(y)+t, \vartheta_2(z)=\phi(z)$ and
	\begin{align*}
		F(x)=(f_1^1(x);f_1^2(x);f_1^3(x);f_1^1(x)+f_2^2(x)+f_2^3(x)),\\
		G(x)=(f_2^1(x)\!+f_2^2(x);f_2^2(x)\!+\!f_2^3(x);f_2^1(x)\!+\!f_2^3(x))
	\end{align*}
	where $f_1^1(x)=x_1^4+x_2^2,f_1^2(x)=(2-x_1)^2+(2-x_2)^2,f_1^3(x)=2e^{-x_1+x_2},
	f_2^1(x)=x_1^2-2x_1+x_2^2-4x_2+4$, $f_2^2(x)=2x_1^2-5x_1+x_2^2-2x_2+4$ and $f_2^3(x)\!=x_1^2\!+2x_2^2\!-4x_2\!+1$.
 \end{example}
 \begin{example}\label{exam5.2}
  $\mathbb{X}=\mathbb{R}^4,\mathbb{Y}=\mathbb{R}^5\times \mathbb{R}^3\times \mathbb{R}^3,\mathbb{Z}=\mathbb{R}^{3}\times\mathbb{R}$, $\vartheta_1(y_1;y_2;y_3):=\|y_1\|_1+\phi(y_2)+\phi(y_3)$, $\vartheta_2(z,t):=\|z\|_1+t$, $F(x):=(F_1(x); F_2(x); F_3(x))$ with
  \begin{align*}
   F_1(x):=(x_1-1;x_3-1;10.1(x_2-1);10.1(x_4-1);4.95(x_2+x_4-2)),\\
   F_2(x):=200(0; x_1-x_2; -x_1-x_2),\,F_3(x):=180(0;x_3-x_4;-x_3-x_4),
  \end{align*}
  and $G(x):=(100x_1;90x_3;4.95(x_2-x_4);-100x_2-90x_4)$.
 \end{example}
 \begin{example}\label{exam5.3}
  $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}\times \mathbb{R}^3\times \mathbb{R},\mathbb{Z}=\mathbb{R},\vartheta_1(y_1;y_2;y_3)=|y_1|+\phi(y_2)+y_3,\vartheta_2(t)=|t|$, $F(x):=(F_1(x); F_2(x); F_3(x))$ with $F_1(x)=x_1-1,F_2(x)=200(1;x_1-x_2;-x_1-x_2)$
	and $F_3(x)=-x_1-x_2$, and $G(x):=100x_1$.
 \end{example}
 \begin{example}\label{exam5.4}
  $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}\times\mathbb{R}^3\times \mathbb{R}^9,\mathbb{Z}=\mathbb{R}^3,\vartheta_1(y_1;y_2;y_3)=|y_1|+\phi(y_2)+\phi(y_3),\vartheta_2(z,t)=\|z\|_1+|t|$, $F(x):=(F_1(x); F_2(x); F_3(x))$ with $F_1(x)=x_1-1,F_2(x)=200(0;x_1-x_2;-x_1-x_2)$
	and $F_3(x)=10(x_1^2+x_2^2+x_2; x_1^2+x_2^2-x_2; x_1+x_1^2+x_2^2+x_2-0.5;x_1+x_1^2+x_2^2-x_2-0.5;
	x_1\!-1;-\!x_1\!+2x_2\!-1;x_1\!-2x_2\!-1;-x_1\!-1;x_1\!+x_1^2\!+x_2^2)$, and $G(x):=(100x_1;10x_2;-100x_2+10(x_1^2+x_2^2))$.
 \end{example}
\begin{example}\label{exam5.5}
 $\mathbb{X}=\mathbb{R}^3,\mathbb{Y}=\mathbb{R}^3\times\mathbb{R}^4,\mathbb{Z}=\mathbb{R}^4,\vartheta_1(y_1;y_2)=\|y_1\|_1+\phi(y_2),\vartheta_2(z,t)=\|z\|_1+|t|$, $F(x):=(F_1(x); F_2(x))$ with $F_1(x)=2x,F_2(x)=10(0;x_1\!+x_2\!+2x_3\!-3;-x_1;-x_2,-x_3)$, and $G(x):=(x_1-x_2;x_1-x_2;10x_2;9\!-\!8x_1\!-\!6x_2\!-\!4x_3\!+\!4x_1^2\!+\!2x_2^2\!+2x_3^2)$.
 \end{example}
 \begin{example}\label{exam5.6}
  $\mathbb{X}=\mathbb{R}^2,\mathbb{Y}=\mathbb{R}^2,\mathbb{Z}=\mathbb{R},\vartheta_1(y)=\|y\|_1,\vartheta_2(t)=t$, $F(x):=x$ and $G(x):=-\frac{5}{2}x_1\!+\frac{3}{2}(x_1^2+x_2^2)$.	
 \end{example} 	

 For the above test examples, the parameters of Algorithm \ref{iLPA} are chosen as follows:
 \[
  \varrho = 2,\,\overline{\gamma}=10^6,\,\underline{\gamma}=0.01,\,  \gamma_{k,0}=\underline{\gamma},\, \alpha_k=\min\big\{10^{-4},\frac{10}{\max\{1,\|F'(x^0)\|\}}\big\}.
 \]
 The parameters of nmBDCA take the default setting in the code of nmBDCA. For the sake of fairness, we adopt the same stopping rule: $\|x^k-x^{k-1}\|\le  10^{-7}$ or $k\ge 1000$.

 Table \ref{DCtab1} below reprots the average results of $100$ times running for each example. Among others, $\min\Theta$ and $\max\Theta$ denote the minimum and maximum objective values among $100$ running, and \textbf{Nopt} records the number of solutions whose objective values have the absolute difference to the optimal value less than $10^{-5}$. In each running, the two solvers start from the same random initial point generated by MATLAB command $x^0=20*\textrm{rand(n,1)}-10$.  We see that iLPA finds more global optimal solutions than nmBDCA does except for Example \ref{exam5.3}. For Examples \ref{exam5.2} and \ref{exam5.4}, the number of optimal solutions returned by iLPA is close to twice that of optimal solutions given by nmBDCA. The average objective values yielded by iPLA is better than the one given by nmBDCA except for Example \ref{exam5.3}.
%------------------------------------------------------------------------------------------------------------------
 \setlength{\tabcolsep}{1mm}
 \begin{table}[H]
 \setlength{\belowcaptionskip}{-0.01cm}
  \footnotesize
  \centering
  \caption{\small Numerical results of iLPA and nmBDCA for Examples \ref{exam5.1}-\ref{exam5.6}}\label{DCtab1}
  \scalebox{1}{
  \begin{tabular}{c|c|c|c|c|l|c|c|c|c|c}
   \hline
   \multicolumn{1}{c|}{}&\multicolumn{5}{c|}{iPMM}&\multicolumn{4}{c|}{nmBDCA}\\
	\hline
	Example  &$\min \Theta $  &$\max \Theta$ &$ {\rm ave}\ \Theta^{\rm out}$ & Nopt &time(s)&$\min \Theta $&$\max \Theta$ &$ {\rm ave}\ \Theta^{\rm out}$ & Nopt&time(s)\\
	\hline
	5.1    &2.0000     & 2.0000&2.0000 &100 &0.028 & 2.0000     & 2.0000  &  2.0000  &100 &  0.048 \\
	5.2    &1.78e-11  &2.0000 &0.8200 &59  &0.015 & 3.36e-08 & 13.1000 &  2.3150  & 28 &  0.047 \\
	5.3    &1.16e-7   &1.0000 &0.0300 &91  &0.012 & 1.82e-09 & 1.0000  &  0.0200  &98  &  0.024 \\
	5.4    &0.5000     &1.0000 &0.5150 &97  & 0.014& 0.5000     & 1.0000  &  0.7350  &53  &  0.252 \\
	5.5    &3.5000     &3.7500 &3.5225 &91  &0.054 & 3.5000     & 3.9405  &  3.5594  &77  &  0.030 \\
	5.6    &-1.1250   &-1.1250 &-1.1250 &100 &0.032 &-1.1250     &-1.1250  & -1.1250   &100 &  0.048 \\			
	\Xhline{1.5pt}
 \end{tabular}}
 \end{table}

%------------------------------------------------------------------------------------
 \subsection{$\ell_1$-norm penalty of DC constrained problems}\label{sec5.4}
%-------------------------------------------------------------------------------------
 We test the performance of iLPA on DC constrained problems and compare its performance with that of nmBDCA. Consider the following general DC constrained problem
 \begin{equation}\label{DC-constraint}
 	\min_{x\in \Omega}\big\{f(x)\ \ {\rm s.t.}\ \ c_i(x)-d_i(x)\leq0,\ i=1,\ldots,m\big\},
 \end{equation}
 where $\Omega\subset\mathbb{R}^n$ is a closed convex set, all $c_i\!:\mathbb{R}^n\to\mathbb{R}$ and  $d_i\!:\mathbb{R}^n\to\mathbb{R}$ are smooth convex functions, and $f\!:\mathbb{R}^n\to\mathbb{R}$ is continuously differentiable. We apply iLPA and nmBDCA to the $\ell_1$-norm penalized problem of \eqref{DC-constraint} with a fixed penalty factor $\beta$:
\begin{equation}\label{L1-penalty}
 \min_{x\in\mathbb{R}^n} f(x)+\beta\sum_{i=1}^m\max\big\{0,c_i(x)-d_i(x)\big\}+\delta_{\Omega}(x),
\end{equation}
 which takes the form of \eqref{prob} with $\mathbb{X}=\mathbb{R}^n,\mathbb{Y}=\mathbb{R}^m,\mathbb{Z}=\mathbb{R}, \vartheta_1(y):=\beta{\textstyle\sum_{i=1}^m}\max(y_i,0)$, $\vartheta_2(t):=t$, $h(x):=\delta_{\Omega}(x),F(x):=(c_1(x)-d_1(x),\ldots,c_q(x)-d_q(x))^{\top}$ and $G(x):=-f(x)$. The test examples include \textbf{mistake} and \textbf{hs108} from the COCONUT library, \textbf{hesse} from \cite{Ackooij19}, and the ore-processing problem from the Erdenet Mining Corporation (Mongolia) \cite{Strekalovsky18}. The first two examples do not contain the hard constraint $x\in\Omega$, but we impose a soft box set containing their feasible sets. Since nmBDCA is inapplicable to the extended real-valued convex functions, we apply it to the following equivalent form of \eqref{L1-penalty}:
\begin{equation}\label{EL1-penalty}
	\min_{x\in\mathbb{R}^n} \underbrace{\beta\sum_{i=1}^{q'}\max\big\{0,c_i(x)-d_i(x)\big\}+\frac{\mu}{2}\|x\|^2}_{g(x)}-\underbrace {f(x)+\frac{\mu}{2}\|x\|^2}_{h(x)},
\end{equation}
where $\mu>0$ is a constant such that $g$ and $h$ are convex, and $q'\ge q$ is the number of constraints (including the hard constraints for the last two examples).

For this group of examples, the parameters of Algorithm \ref{iLPA} are same as those used in section \ref{sec5.3} except $\gamma_{k,0}=\underline{\gamma}=\min\{\|F'(x^0)\|,100\}$.
We terminate the two solvers at $x^k$ if $\|x^k\!-x^{k-1}\|\le 10^{-6}$ and $\textbf{infea}^k\le 10^{-6}$, where $\textbf{infea}^k\!=\sum_{i=1}^{q'}\max\big\{0,c_i(x^k)-d_i(x^k)\big\}$ is the feasibility violation at the $k$th iterate, or the number of iterates is over $k_{\rm max}=10^3$.

Table \ref{DCtab2} reports the average results of $100$ times running for each example. In each running, the two solvers start from the same random initial point generated by MATLAB command $x^0=20*\textrm{rand(n,1)}-10$. The performance of nmBDCA depends on the choice of the parameter $\mu$ in \eqref{EL1-penalty}. We choose a good one for each problem as far as possible, although it does not necessarily make the corresponding $g$ and $h$ become convex. We see that except for \textbf{hesse}, the number of optimal solutions returned by iLPA is far more than that of optimal solutions given by nmBDCA.

\setlength{\tabcolsep}{1mm}
\begin{table}[H]
	\setlength{\belowcaptionskip}{-0.01cm}
	\footnotesize
	\centering
	\caption{\small Numerical results of iLPA and nmBDCA for DC constrained problems}\label{DCtab2}
	\scalebox{1}{
	\begin{tabular}{|c|c|c|c|c|c|l|c|c|c|c|c|}
			\hline
			\multicolumn{2}{|c|}{}&\multicolumn{5}{c|}{iPMM}&\multicolumn{5}{c|}{nmBDCA}\\
			\hline
			Problem &$\beta$ & max & ave & Nopt & Infea&time(s) & max & ave & Nopt & Infea & time(s)\\
			\hline
			\textbf{mistake} &$10$     &-0.6572     & -0.9765   &68 &2.857e-4 &0.082 & 0.4593   & -0.9100  & 1  & 6.564e-9 &  0.073 \\
			\textbf{hs108}  &$10$     &-0.4995     &-0.7557    &67 &1.549e-5 &0.055 & -0.3810  & -0.7464  & 4  & 2.129e-6 &  0.092 \\
			\textbf{hesse}   &\ $10^4$ &-36.0000    &-204.975  &0  &5.177e-6 &0.008 &-21.9708  & -188.002 &0  & 5.052e-9 &  0.026 \\
			\textbf{ore} &\ $10^2$ & -0.9198    &-1.0633    &42 &7.549e-6 &0.037 & -0.9167  &-1.0808    &0  & 2.252-11 &  0.038 \\
			\Xhline{1.5pt}
	\end{tabular}}
\end{table}
%--------------------------------------------------------------------------------------------
 \section{Conclusions}\label{sec6}

 For the DC composite optimization problem \eqref{prob}, we have developed an inexact linearized proximal algorithm (iLPA) and established the whole convergence of the generated iterate sequence under Assumptions \ref{ass0}-\ref{ass1} and the KL property of the potential function $\Xi$. The latter holds when $F,G$ and $\vartheta_1,\vartheta_2,h$ are definable in the same o-minimal structure over the real field. If in addition $\Xi$ has the KL property of exponent $p\in(0,1)$, the convergence admits a local superlinear rate for $p\in(0,1/2)$ and a local linear rate for $p=1/2$. The KL property of $\Xi$ with exponent $p\in[1/2,1)$ is shown to hold for polyhedral $F,G$ and some special $\vartheta_1,\vartheta_2$ and $h$ (see Proposition \ref{prop-KL0}). In addition, we have provided a verifiable condition for the KL property of $\Xi$ with exponent $p\in[1/2,1)$ by leveraging such a property of $f$ in \eqref{ffun} and condition \eqref{key-cond}, and its relation with the regularity or quasi-regularity conditions used in \cite{HuYang16} is fully discussed for the case $\vartheta_2\equiv 0$ and $h\equiv 0$.
 Numerical comparions with the subgradient method on matrix completions with outliers and non-uniform sampling and comparions with nmBDCA on DC programs with nonsmooth components and $\ell_1$-norm exact penalty of DC constrained programs confirm the efficiency of the proposed iLPA armed with dPPASN. In our future work, we will focus on algorithms to yield better stationary points for some special $G$.

%--------------------------------------------------------------------------------------------
\bigskip
  \noindent
 {\large\bf Acknowledgements}\ \
  The authors are indebted to Professor Orizon Pereira Ferreira and Jo\~{a}o Carlos O. Souza from Universidade Federal de Goi\'{a}s, for sharing their code used to compute the DC programs with nonsmooth components.


 \begin{thebibliography}{1}

    \bibitem{Artacho08}
  {\sc F. J.\ Arag\'{o}n Artacho and M. H.\ Geoffroy},
  {\em Characterization of metric regularity of subdifferential},
  Journal of Convex Analysis, 15(2008): 365-380.
 	
 \bibitem{Ackooij19}
 {\sc W.\ V.\ Ackooij, W.\ d.\ Oliveira},
 {\em Non-smooth DC-constrained optimization: constraint qualification and minimizing methodologies},
 Optimization Method \& Software, 34 (2019): 1029-4937.


 \bibitem{Artacho20}
 {\sc F. J. A.\ Artacho and P. T.\ Vuong},
 {\em The boosted difference of convex functions algorithm for nonsmooth functions},
 SIAM Journal on Optimization, 30 (2020): 980-1006.



% \bibitem{Artacho18}
%% {\sc F. J. A.\ Artacho, R. M. T.\ Fleming and P. T.\ Vuong},
% {\em  Accelerating the dc algorithm for smooth functions},
% Mathematical Programming, 169 (2018): 95-118.	


  \bibitem{Attouch09}
 {\sc H.\ Attouch and J.\ Bolte},
 {\em On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
 Mathematical Programming, 116 (2009): 5-16.


 \bibitem{Attouch10}
 {\sc H.\ Attouch, J.\ Bolte, P.\ Redont and A.\ Soubeyran},
 {\em Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the Kurdyka-{\L}ojasiewicz inequality}, Mathematics of Operations Research, 35 (2010): 438-457.

  \bibitem{Auslender10}
 {\sc A.\ Auslender, R.\ Shefi and M.\ Teboulle},
 {\em A moving balls approximation method for a class of smooth constrained minimization problems}, SIAM Journal on Optimization, 20 (2010): 3232-325.


 \bibitem{BaiLi22}
 {\sc S. X.\ Bai, M. H.\ Li, C. W.\ Lu, D. L.\ Zhu and S. E.\ Deng},
 {\em The equivalence of three types of error bounds for weakly and approximately convex functions},
 Journal of Optimization Theory and Application, 194 (2022): 220-245.


 \bibitem{Bolte07}
 {\sc J.\ Bolte, A.\ Daniilidis, A. Lewis and M.\ Shiota},
 {\em Clarke subgradients of stratifiable functions},
 SIAM Journal on Optimization, 18 (2007): 556-572.


  \bibitem{Bolte14}
  {\sc J.\ Bolte, S.\ Sabach and M.\ Teboulle},
  {\em Proximal alternating linearized minimization for nonconvex and nonsmooth problems},
  Mathematical Programming, 146 (2014): 459-494.


 \bibitem{Bolte16}
 {\sc J.\ Bolte and E.\ Pauwels},
 {\em Majorization-minimization procedures and convergence of SQP methods for semi-algebraic and tame programs}, Mathematics of Operations Research, 41 (2016): 442-465.


 \bibitem{Bolte17}
 {\sc J.\ Bolte, T. P.\ Nguyen, J.\ Peypouquet and B. W.\ Suter},
 {\em From error bounds to the complexity of first-order descent methods for convex functions}, Mathematical Programming, 165 (2017): 471-507.


  \bibitem{BS00}
 {\sc J. F.\ Bonnans and A. S.\ Sharpiro},
 {\em Perturbation Analysis of Optimization}, Springer, New York. 2000.

%
% \bibitem{Burke85}
% {\sc J. V.\ Burke},
% {\em Descent methods for composite nondifferentiable optimization problems},
% Mathematical Programming, 33(1985): 260279.

   \bibitem{Burke95}
 {\sc J. V.\ Burke and M. C.\ Ferris},
 {\em  A Gauss-Newton method for convex composite optimization},
 Mathematical Programming, 71 (1995): 179-194.


  \bibitem{Clarke83}
  {\sc F. H.\ Clarke},
  {\em Optimization and Nonsmooth Analysis}, New York, 1983.



  \bibitem{Charisopoulos21}
  {\sc V.\ Charisopoulos, Y. D.\ Chen, D.\ Davis, M.\ Diaz, L. J.\ Ding and D.\ Drusvyatskiy},
  {\em Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence},
  Foundations of Computational Mathematics, 21(2021): 1505-1593.


  \bibitem{Chen12}
  {\sc C. H.\ Chen, B. S.\ He and X. M.\ Yuan},
  {\em  Matrix completion via an alternating direction method},
  IMA Journal of Numerical Analysis, 32 (2012): 227-245.



%  \bibitem{Davis18}
%  {\sc D.\ Davis, D.\ Drusvyatskiy, K. J.\ MacPhee and C.\ Paquette},
%  {\em Subgradient methods for sharp weakly convex functions},
%  Journal of Optimization Theory and Applications, 179(2018): 962-982.


  \bibitem{Dong21}
  {\sc H. B.\ Dong and M.\ Tao},
  {\em On the linear convergence to weak/standard d-stationary
 points of DCA-based algorithms for structured nonsmooth DC programming},
 Journal of Optimization Theory and Applications, 189 (2021): 190-220.


 \bibitem{Fan01}
 {\sc J. Q.\ Fan and R. Z.\ Li},
 {\em Variable selection via nonconcave penalized likelihood and its oracle properties},
 Journal of American Statistics Association, 96 (2001): 1348-1360.

  \bibitem{Fang18}
  {\sc E. X.\ Fang, H.\ Liu, K. C.\ Toh and W. X.\ Zhou},
  {\em Max-norm optimization for robust matrix recovery},
  Mathematical Programming, 167 (2018): 5-35.


 \bibitem{Ferreria21}
 {\sc O. P.\ Ferreria, E. M.\ Santos and J. C. O.\ Souza},
 {\em A boosted DC algorithm for non-differentiable DC components with non-monotone line search}, arXiv.2111.01290, 2021.


 \bibitem{Fletcher82}
 {\sc R.\ Fletcher},
 {\em A model algorithm for composite nondifferentiable optimization problems},
 Mathematical Programming Study, 17 (1982): 67-76.


 \bibitem{Gong13}
 {\sc P. H.\ Gong, C. S.\ Zhang, Z. S.\ Lu, J. H.\ Huang and J. P.\ Ye},
 {\em A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems}, In: International Conference on Machine Learning, 2013: 37-45.


 \bibitem{HuYang16}
 {\sc Y. H.\ Hu, C.\ Li and X. Q.\ Yang},
 {\em On convergence rates of linearized proximal algorithms for convex composite optimization with Applications}, SIAM Journal on Optimization, 26 (2016): 1207-1235.


  \bibitem{Ioffe08}
 {\sc A. D.\ Ioffe and J. V.\ Outrata},
 {\em On metric and calmness qualification conditions in subdifferential calculus},
 Set-Valued Analysis, 16 (2008): 199-227.


 \bibitem{Ioffe09}
 {\sc A. D.\ Ioffe},
 {\em An invitation to tame optimization}, SIAM Journal on Optimization, 19 (2009): 1894-1917.

 \bibitem{Lewis16}
 {\sc A. S.\ Lewis and S. J.\ Wright},
 {\em A proximal method for composite minimization},
 Mathematical Programming, 158 (2016): 501-546.

  \bibitem{Li02}
 {\sc C.\ Li and X. H.\ Wang},
 {\em On convergence of the Gauss-Newton method for convex composite optimization},
 Mathematical Programming, 91 (2002): 349-356.

 \bibitem{Li07}
 {\sc C.\ Li and K. F.\ Ng},
 {\em Majorizing functions and convergence of the Gauss-Newton method for convex composite optimization}, SIAM Journal on Optimization, 18 (2007): 613-642.
 




\bibitem{LeTai05}
{\sc H. A.\ Le Thi and T.\ Pham Dinh},
{\em The DC (difference of convex functions) programming and DCA revisited with DC models of real world nonconvex optimization problems},
Annals of Operations Research, 133 (2005): 23-46. 	

 \bibitem{LeTai18}
{\sc H. A.\ Le Thi and D. T.\ Pham},
{\em  DC programming and DCA: Thirty years of developments},
Mathematical Programming, 169 (2018): 5-68.


\bibitem{LeTai18Jota}
{\sc H. A.\ Le Thi, V. N.\ Huynh and T.\ Pham Dinh},
{\em Convergence analysis of difference-of-convex algorithm with subanalytic data}, Journal of Optimization Theory and Applications,
179 (2018): 103-126. 	
 

 \bibitem{LiPong18}
 {\sc G. Y.\ Li and T. K.\ Pong},
 {\em Calculus of the exponent of Kurdyka-{\L}ojasiewicz inequality and its applications to linear convergence of first-order methods},
 Foundations of Computational Mathematics, 18 (2018): 1199-1232.


 \bibitem{LiZhu20}
 {\sc X.\ Li, Z. H.\ Zhu, A. M. C.\ So and R.\ Vidal},
 {\em Nonconvex robust low-rank matrix recovery},
 SIAM Journal on Optimization, 30 (2020): 660-686.

 \bibitem{LiuPong19}
 {\sc T. X.\ Liu, T. K.\ Pong and A.\ Takeda},
 {\em A refined convergence analysis of pDCAe with applications to simultaneous sparse recovery and outlier detection}, Computational Optimization and Applications, 73 (2019): 69-100.


 \bibitem{LiuPanWY22}
 {\sc R. Y.\ Liu, S. H.\ Pan, Y. Q.\ Wu and X. Q.\ Yang},
 {\em An inexact regularized proximal Newton method for nonconvex and nonsmooth optimization}, arXiv:2209.09119v3, 2022.


 \bibitem{LiuPan22}
 {\sc Y. L.\ Liu and S. H.\ Pan},
 {\em Twice epi-differentiability of a class of non-amenable composite functions}, arXiv:2212.00303, 2022.

 \bibitem{Lu12}
 {\sc Z.\ Lu},
 {\em Sequential convex programming methods for a class of structured nonlinear programming},
 arxiv:1210.3039, 2012.

 \bibitem{Lu19}
 {\sc Z. S.\ Lu, Z. R.\ Zhou and Z.\ Sun},
 {\em Enhanced proximal DC algorithms with extrapolation for a class of structured nonsmooth DC minimization}, Mathematical Programming, 176 (2019): 369-401.

  \bibitem{Meng05}
 {\sc F. W.\ Meng, D. F.\ Sun and G. Y.\ Zhao},
 {\em Semismoothness of solutions to generalized equations and the Moreau-Yosida regularization}, Mathematical Programming, 104 (2005): 561-581.


 \bibitem{Mohammadi20}
 {\sc A.\ Mohammadi and M. E.\ Sarabi},
 {\em Twice epi-differentiability of extended-real-valued functions with applications in composite optimization}, SIAM Journal on Optimization, 30 (2020): 2379-2409.

  % \bibitem{Mohammadi22}
 % {\sc A. Mohammadi},
 % {\em First-order variational analysis of non-amenable composite functions},
 %arXiv preprint arXiv:2204.01191 (2022).


 \bibitem{Mordu94}
 {\sc B. S.\ Morduhovich},
 {\em Generalized differential calculus for nonsmooth and set-valued mappings},
 Journal of Mathematical Analysis and Applications, 183 (1994): 250-288.

  \bibitem{Mordu15}
 {\sc B. S.\ Morduhovich and O. Y.\ Wei},
 {\em Higher-order metric subregularity and its applications},
 Journal of Global Optimization, 63 (2015): 777-795.


 \bibitem{Nguyen17}
 {\sc T. A.\ Nguyen and M. N.\ Nguyen},
 {\em Convergence analysis of a proximal point algorithm for minimizing differences of functions}, Optimization, 66 (2017): 129-147.


 \bibitem{Oliveira19}
 {\sc W. D.\ Oliveira and M. P.\ Tcheou},
 {\em An inertial algorithm for DC programming}, Set-Valued and Variational Analysis, 27 (2019): 895-919.


 \bibitem{Ortega70}
 {\sc J. M.\ Ortega and W. C.\ Rheinboldt},
 {\em Iterative Solution of Nonlinear
 Equations in Several Variables}, Academic Press, New York, 1970.


 \bibitem{Pang17}
 {\sc J. S.\ Pang, M.\ Razaviyayn and A.\ Alvarado},
 {\em Computing B-stationary points of nonsmooth DC programs},
 Mathematics of Operations Research, 42 (2017): 95-118.

 \bibitem{Pauwels16}
 {\sc E.\ Pauwels},
 {\em The value function approach to convergence analysis in composite optimization},
 Operations Research Letters, 44 (2016): 790-795.


  \bibitem{Pham86}
 {\sc D. T.\ Pham and E. B.\ Souad},
 {\em Algorithms for solving a class of nonconvex optimization problems: methods of subgradient}. Mathematics for optimization. Fermat days. North Holland: Elsevier; 85 (1986): 249-270.


 \bibitem{Pham97}
 {\sc D. T.\ Pham and H. A.\ Le Thi},
 {\em Convex analysis approach to DC programming: Theory, algorithms and applications},
 Acta Mathematica Vietnamica, 22 (1997): 289-355.


% \bibitem{Polyak69}
%  {\sc B. T.\ Polyak},
%  {\em Minimization of unsmooth functions},
%  USSR Computational Mathematics and Mathematical Physics, 9(1969): 14-29.

 \bibitem{QiSun93}
 {\sc L. Q.\ Qi and J.\ Sun},
 {\em A nonsmooth version of Newton's method}, Mathematical Programming, 58 (1993): 353-367.

% \bibitem{QianPan22}
% {\sc Y. T.\ Qian and S. H.\ Pan},
% {\em A superlinear convergence iterative framework for Kurdyka-{\L}ojasiewicz optimization and application},  arXiv:2210.12449v2, 2022.


  \bibitem{Roc70}
 {\sc R. T.\ Rockafellar},
 {\em Convex Analysis}, Princeton: Princeton University Press, 1970.

 \bibitem{Roc76}
 {\sc R. T.\ Rockafellar},
 {\em Augmented Lagrangians and applications of the proximal point algorithm in convex programming}, Mathematics of Operations Research, 1 (1976): 97-116.

 \bibitem{Roc21}
 {\sc R. T.\ Rockafellar},
 {\em Advances in convergence and scope of the proximal point algorithm}, Journal of Nonlinear and Convex Analysis, 22 (2021): 2347-2374.


 \bibitem{RW98}
 {\sc R. T.\ Rockafellar and R. J-B.\ Wets},
 {\em Variational analysis}, Springer, 1998.




  \bibitem{Robinson81}
 {\sc S. M.\ Robinson},
 {\em Some continuity properties of polyhedral multifunctions},
 Mathematical Programming Study, 14(1981): 206-214.



 \bibitem{Souza16}
 {\sc J. C. O.\ Souza, P. R.\ Oliveira and A.\ Soubeyran},
 {\em  Global convergence of a proximal linearized algorithm for difference of convex functions},
 Optimization Letters, 10 (2016): 1529-1539.

  \bibitem{Strekalovsky18}
 {\sc A.\ S.\ Strekalovsky and I.\ M.\ Minarchenko},
 {\em A local search method for optimization problem with d.c. inequality constraints},
 Applied Mathematical Modelling, 59 (2018): 229-244.

 \bibitem{Sun03}
{\sc W. Y.\ Sun, R. J. B.\ Sampaio and M. A. B.\ Candido},
{\em Proximal point algorithm for minimization of DC functions}, Journal of Computational Mathematics, 21 (2003): 451-462.

% \bibitem{LeThi14}
% {\sc H. A.\ Le Thi, V. N.\ Huynh and T. P.\ Dinh},
% {\em DC Programming and DCA for General DC Programs}, Advanced Computational Methods for Knowledge Engineering. Advances in Intelligent Systems and Computing, 282(2014): 15-35. 	
 	



 \bibitem{Toh10}
 {\sc K. C.\ Toh and S.\ Yun},
 {\em An accelerated proximal gradient algorithm for nuclear norm regularized linear
 	least squares problems}, Pacific Journal of Optimization,
 6 (2010): 615-640. 	



  \bibitem{Wen18}
 {\sc B.\ Wen, X.\ Chen and T. K.\ Pong},
 {\em Aproximal difference-of-convex algorithmwith extrapolation},
 Computational Optimization and Applications, 69 (2018): 297-324.

 \bibitem{WuPanBi21}
 {\sc Y. Q.\ Wu, S. H.\ Pan and S. J.\ Bi},
 {\em Kurdyka-{\L}ojasiewicz property of zero-norm composite functions},
 Journal of Optimization Theory and Applications, 188 (2021): 94-112.


 \bibitem{YuLu21}
 {\sc P. R.\ Yu, T. K.\ Pong and Z.\ Lu},
 {\em Convergence rate analysis of a sequential convex programming method with line search for a class of constrained difference-of-convex optimization problems}, SIAM Journal on Optimization, 31 (2021): 2024-2054.

 \bibitem{YuLiPong21}
 {\sc P. R.\ Yu, G. Y.\ Li and T. K.\ Pong},
 {\em Kurdyka-{\L}ojasiewicz exponent via inf-projection},
 Foundations of Computational Mathematics, 22 (2021): 1171-1271.

  \bibitem{Zhang10}
 {\sc C. H.\ Zhang},
 {\em Nearly unbiased variable selection under minimax concave penalty},
 Annals of Statistics, 38(2010): 894-942.

 \bibitem{ZhangPan22}
 {\sc D. D.\ Zhang, S. H.\ Pan, S. J.\ Bi and D. F.\ Sun},
 {\em Zero-norm regularized problems: equivalent surrogates, proximal MM method and statistical error bound}, submited to Computational Optimization and Applications (under second review).

%  \bibitem{ZhangPan22}
% {\sc D. D.\ Zhang, S. H.\ Pan, S. J.\ Bi and D. F.\ Sun},
% {\em  A proximal dual semismooth Newton method for computing zero-norm penalized QR estimator}, arXiv:1907.03435v3, 2021.
%


 \bibitem{ZhaoST10}
{\sc X. Y.\ Zhao, D. F.\ Sun and K. C.\ Toh},
{\em A Newton-CG augmented Lagrangian method for semidefinite programming},
SIAM Journal on Optimization, 20 (2010): 1737-1765.






% \bibitem{Coste99}
% {\sc M.\ Coste},
% {\em An introduction to o-minimal geometry},
% Institut de Recherche Math$\acute{e}$matique de Rennes, 1999.
%
% \bibitem{Dries98}
% {\sc L.\ van den Dries and  C.\ Miller},
% {\em Tame topology and O-minimal structures},
%  Cambridge University Press, 1998.


%  \bibitem{Floudas99}
%  {\sc C.\ Floudas, P.\ Pardalos, C.\ Adjimanand, et al.},
%  {\em Handbook of Test Problems in Local and Global Optimization, 1st ed., Nonconvex Optimization and Its Applications},
%  Springer-Verlag US, Hingham, MA, 33(1999): 5-35.


% \bibitem{LeTai96}
% {\sc H. A.\ Le Thi, T.\ Pham Dinh and L. D.\ Muu},
% {\em Numerical solution for optimization over the efficient set by D.C. optimization algorithms},
% Operations Research Letters, 19(1996): 117-128.


% \bibitem{Facchinei03}
% {\sc F.\ Facchinei and J.\ S.\ Pang},
% {\em Finite-dimensional Variational Inequalities and Complementarity Problems},
%  Springer, New York, 2003.



% \bibitem{Hiriart84}
% {\sc J.\ B.\ Hiriart-Urruty, J.\ J.\ Strodiot and  V.\ H.\ Nguyen},
% {\em Generalized Hessian matrix and second-order optimality conditions for problems with $C^{1,1}$ data},
%  Applied Mathematics and Optimization, 11(1984): 43-56.


%  \bibitem{Nesterov83}
%  {\sc Y.\ Nesterov},
%  {\em A method of solving a convex programming problem with convergence rate $O(1/k^2)$},
%  Soviet Mathematics Doklady, 27(1983): 372-376.


 \end{thebibliography}

\end{document}


