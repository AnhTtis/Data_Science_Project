




\section{Experimental Results}

We conduct simulated experiments in two different environments---our own \textit{guided maze} and \textit{office floorplan} environments---in which the robot navigates from a randomly generated start location to a randomly generated point goal in unseen space. We evaluate the following approaches:
\begin{LaTeXdescription}
    \item[Multi-Robot Learning over Subgoals (MR-LSP)] Our approach, in which the expected cost of a collective action is computed via Eq.~\eqref{eq:MR-LSP}. When only one robot is used, this approach corresponds to LSP planning via Eq.~\eqref{eq:LSP}.
    \item[Non-Learned Optimistic Planner] We assume that all unseen space is unoccupied and compute paths through each subgoal to reach the goal. The optimistic plan cost is computed for each subgoal and linear sum assignment~\cite{Kuhn2010TheHM} is used to ensure that robots pursue different subgoals (a topological constraint on their plans similar to~\cite{bhattacharyaTopologicalConstraints}) while minimizing net optimistic plan cost to reach the goal. This planning strategy serves as a \emph{non-learned baseline}.
    \item[Linear Sum Assignment using LSP (LSA-LSP)] This planning strategy also uses linear sum assignment to enforce that robots select different subgoals to pursue, yet the expected cost associated with each subgoal is computed via the single-robot LSP approach Eq.~\eqref{eq:LSP}.
    This planning strategy serves as a \emph{learning-informed baseline}. When one robot is used, this approach corresponds to LSP planning via Eq.~\eqref{eq:LSP}.
    \item[Known-Space Planner] Planning in the fully-known map (no uncertainty), providing a lower bound on possible cost. Since the shortest path is known, this planner has the same performance for any number of robots.
\end{LaTeXdescription}

\begin{figure}[t]
    \vspace{1em}
    \centering
    \footnotesize
    \begin{tabular}{cccc}
    \toprule
     & \multicolumn{3}{c}{\textbf{Guided Maze Environment}}           \\
    \cmidrule(lr){2-4}
             Planner                & 1 robot         & 2 robots        & 3 robots       \\
    \midrule
    Non Learned Optimistic   & 207.50          & 144.88          & 144.44         \\
    LSP-LSA (Learned)        & \textbf{162.46}          & \textbf{133.12}          & \underline{132.66}         \\
    MR-LSP (Ours)            & \textbf{162.46} & \underline{134.22} & \textbf{132.3} \\
    \midrule
    Known-Map Planner            & 130.55          & 130.55          & 130.55        \\
    \bottomrule
    \\[-6pt]
    \end{tabular}
    \includegraphics[width=8.45cm]{Sections/images/maze_scatter.png}
    \caption{\textbf{Navigation in maze environment results}\quad The table shows the average cost (in meters) accrued in 100 experiments for each planner. The scatter plot each shows the performance of our approach versus the baselines for two robots. The result shows planners which use learning (MR-LSP and LSP-LSA) outperform the non-learned planner. The performance of learned baseline (LSP-LSA) is similar to MR-LSP (slightly better for two robots) in a relatively simpler environment.}   
    \vspace{-1.5em}\label{fig:scatter_result_maze}
\end{figure}


\subsection{Guided Maze Environment Results}
We first perform experiments in our ``guided'' maze environment, in which a green path on the ground connects the goal to another point in the maze. Initially, the robot team is placed at the center of this path so that two green routes extend from its location, only one leading to the goal.
The maze is simply connected---i.e., there exists only a single path to the goal---and so a single robot, even one that understands the significance of the green path, must get lucky to reach the goal quickly by choosing the correct path. However, a two robot team can divide-and-conquer, following both green paths simultaneously and reaching the goal quickly with high reliability.



We evaluate \replaced{our planners}{each of our planner} in 100 guided maze environments, and show that our \replaced{MR-LSP}{Multi-Robot Learning over Subgoals Planner (MR-LSP)} and the LSA-LSP learned baseline perform near-optimally in this environment for 2- and 3-robot experiments, outperforming both the 1-robot trials and the non-learned optimistic baseline. Fig.~\ref{fig:scatter_result_maze} shows the average cost for each planner in this environment.
\replaced{Both MR-LSP and LSA-LSP planners use learning to evaluate goodness of paths and understand the importance of splitting the team to follow both green paths for reaching the goal. In contrast, the non-learned optimistic planner explores many unlikely paths due to a lack of understanding of the green path's significance. These behaviors are evident in Fig.~\ref{fig:MazeResult}(b) and (c), highlighting the significance of using learning to prioritize the green path and follow multiple promising routes simultaneously.}{Both the MR-LSP and LSA-LSP planners rely on learning to evaluate the goodness of paths to the goal and correctly understand that the team should split up and follow both green paths simultaneously to reach the goal.
By contrast, the non-learned optimistic planner does not understand the significance of the green path and explores many paths unlikely to lead to the goal. These behaviors can be seen in Fig.~\ref{fig:MazeResult}(b) and (c), demonstrating both the importance of using learning to prefer the green path and of splitting up to follow multiple promising routes simultaneously.}


\begin{figure}[t]
        \vspace{1em}
        \centering
        \includegraphics[width=8.45cm]{Sections/images/maze_navigation.png}
        \caption{\textbf{Navigation under uncertainty in maze} \quad (a) Images taken on board robot used as inputs to learning. (b) Generally learned planner knows to follow green path to reach goal quickly. (c) Occasionally, MR-LSP with single robot (LSP) explores alternative route, whereas with multiple robots MR-LSP guides coordinated exploration.}
        \label{fig:MazeResult}
        \vspace{-1.5em}
\end{figure}


\subsection{Office Floorplan Environment Results}
\replaced{We conduct experiments in our simulated \emph{office floorplan} environments. These environments are designed to mimic a typical office building, with multiple hallways intersecting and connecting many offices, including some corner rooms. Clutter in each office simulates furniture and obstructs view from the hallway. We evaluate our planners in 100 procedurally generated office environments---distinct from those seen during training---with varying numbers of robots and report the effectiveness of our MR-LSP planning approach in Fig.~\ref{fig:scatter_result_office}.}{Â We additionally conduct experiments in our simulated \emph{office floorplan} environments.
These environments are designed so as to mimic a typical office building, with multiple intersecting hallways each with many offices connected to them, including some corner rooms connected to two hallways.
Clutter in each office simulates furniture and obstructs view of the entire office from the hallway.
%%% Figure was here$
We evaluate in 100 office environments for each of our planners and number of robots and report results in Fig.~\ref{fig:scatter_result_office} that demonstrate the effectiveness of our MR-LSP planning approach.}


%Our MR-LSP approach understands both the utility of following hallways until the goal can be reached.
Our MR-LSP approach seems to both understand the utility of following hallways until the goal can be reached and how to effectively allocate exploratory actions to different team members, improving performance over both LSA-LSP and Non-Learned Optimistic planners.
\replaced{The LSA-LSP learned baseline, which uses the same subgoal estimator as the MR-LSP planner, tends to follow hallways and avoid exploring rooms. However, due to a lack of coordination among the team, it only moderately improves over the non-learned baseline. Fig.~\ref{fig:office1}b shows an example of this behavior. The LSA-LSP planner can only coordinate two robots myopically, resulting in poorer behavior compared to the MR-LSP planner, which quickly explores promising routes to the goal. }{The LSA-LSP learning-informed baseline, which uses the same subgoal property estimator as does the MR-LSP planner, also tends to follow hallways, yet the lack of non-myopic coordination amongst the team, leading to only moderate improvements over the non-learned baseline.


We show a qualitative example of this behavior in Fig.~\ref{fig:office1}b. The LSA-LSP (learned baseline) planner knows to follow hallways and avoid exploring rooms, unlike the non-learned planner. However, the LSA-LSP planner can only myopically coordinate two robots, resulting in poor behavior when compared to the MR-LSP planner, which more quickly explores the promising routes to the goal.}
%yet with only a myopic view of the coordination between the robots, does not 
%switches back to explore the room to minimize overall `global cost' for two robots. The MR-LSP planner explores different areas to reach the goal.}

\begin{figure}[t]
    \vspace{1em}
    \centering
    \footnotesize
    \begin{tabular}{cccc}
    \toprule
     & \multicolumn{3}{c}{\textbf{Office Floorplan Environment}}    \\
    \cmidrule(lr){2-4}
                   Planner          & 1 robot         & 2 robots      & 3 robots       \\
    \midrule
    Non Learned Optimistic   & 206.90          & 167.56        & \underline{156.25}         \\
    LSP-LSA (Learned)        & \textbf{159.84}          & \underline{161.96}        & 158.23         \\
    MR-LSP (Ours)            & \textbf{159.84} & \textbf{145.28} & \textbf{149.10} \\
    \midrule
    Known-Map Planner            & 129.48          & 129.48        & 129.48         \\
    \bottomrule
    \\[-6pt]
    \end{tabular}
    \includegraphics[width=8.45cm]{Sections/images/office_scatter.png}
    \caption{\textbf{Navigation in office floorplan results} \quad The table shows the average cost (in meters) accrued in 100 experiments for each planners. The scatter plot each shows performance of our approach versus the baselines for two robots. In a simulated office floorplan, MR-LSP outperforms both non-learned and learned baselines.   }
    \vspace{-15pt}
    \label{fig:scatter_result_office}
\end{figure}


\replaced{In the office floorplan environments, 3-robot MR-LSP experiments show \emph{increased} average cost compared to 2-robot MR-LSP experiments in the same space. Fig.~\ref{fig:3robots} shows a scenario where 2-robot MR-LSP outperforms 3-robot MR-LSP planning. Additional experiments increasing the number of samples for PO-UCT from 15k to 100k showed improved performance, indicating that the slight degradation is owed to a limited computational budget and should not be seen as a limitation of the approach itself.}{We point out that average cost \emph{increases} for the 3-robot MR-LSP experiments in the office floorplan environments compared to 2-robot MR-LSP experiments in the same space. Fig.~\ref{fig:3robots} shows an example of a scenario in which 2-robot MR-LSP outperforms the 3-robot MR-LSP planning. We conducted additional experiments on this and other trials in which we increased the number of samples for PO-UCT from 15k to 100k and found that the performance improved suggesting that the slight performance degradation is owed to a limited computational budget and should not be seen as a limitation of the approach itself.}
In future work, we will explore this relationship in more depth and reimplement our planner in a faster, compiled language.
