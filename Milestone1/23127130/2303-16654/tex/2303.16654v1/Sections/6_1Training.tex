

\section{Training Data Generation and Learning Subgoal Properties} \label{sec:Training}
To compute the expected cost during MR-LSP planning, we require the subgoal properties $P_S$, $R_S$, and $R_E$ for all subgoals. We train a convolutional neural network, similar to that of \cite{bradley2021learning}, to estimate these properties from images collected by the robot.

The convolutional neural network (CNN) takes a $128\times 512$ RGB panoramic image aligned towards subgoal, egocentric position of the subgoal and goal as inputs. The image is passed through 4 convolutional layers, after which the relative-distance features are concatenated, passed through an additional 9 convolutional layers, and finally 5 fully connected layers, which output the subgoal properties. Our CNN estimates the subgoal properties for all subgoals and, with distances $D$ computed from the occupancy grid, are used to compute cost of a collective-action via Eq.~\eqref{eq:MR-LSP}.
% This is done for all the subgoal to estimate the properties for all the subgoals. The subgoal properties is used with the current occupancy grid for the cost computation.}

To generate training data, we use a non-learned optimistic planner to navigate previously unseen environments. Observations (images) and  are collected from every step and labels of for each subgoal for $P_S$, $R_S$, and $R_E$ are computed from the underlying known map.
$P_S=1$ if the subgoal leads to the goal, for which $R_S$ is the distance to reach the goal, and $P_S=0$ if it does not, where $R_E$ is the distance the robot would travel before reaching a dead end and turning around. 
