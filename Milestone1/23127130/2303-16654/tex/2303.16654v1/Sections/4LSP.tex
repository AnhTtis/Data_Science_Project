\section{Preliminaries: Single-Robot Planning in a Partial Map via Learning over Subgoals} \label{sec:LSP}
% First: start with *what/why*
Even for single robot planning, planning via the POMDP model in Eq.~\eqref{eq:POMDP} requires both enormous computational effort and also access to a distribution over possible environments, difficult to obtain in general.
So as to mitigate the complexities of \emph{single-robot} navigation in a partial map, the \emph{Learning over Subgoals Planner} (LSP) \cite{stein2018learning} introduces an abstraction in which actions correspond to navigation through \emph{subgoals} placed at boundaries between free and unknown space, simplifying the process of imagining the impact of actions that enter unseen space.
Under the LSP abstraction, model-based planning is augmented via predictions from learning, allowing for both reliability and good performance.
% Then talk about *how*

For the \replaced{LSP}{Learning over Subgoals planning} abstraction, temporally-extended actions correspond to \emph{subgoals}, each associated with a contiguous boundary between free and unseen space.
A \emph{high-level action} $a_t$ consists of (1) navigating to the subgoal and then (2) exploring the unknown space beyond in an effort to reach the unseen goal.
Planning is done over an abstract belief state: a tuple $b_t = \left<m_t, \mathcal{S}_u, q_t\right>$, where $m_t$ is the (partial) map of the environment, $\mathcal{S}_u$ is the set of unexplored subgoals, and $q_t$ is the robot pose. Each high-level action $a_t \in \mathcal{S}_u$ has a binary outcome: with probability $P_S(a_t)$, the robot \emph{succeeds} in reaching the goal or (with probability $1 - P_S(a_t)$) \emph{fails} to reach the goal.

Upon selecting an action $a_t$, the robot must first move through known space to the boundary, accumulating a cost $D(m_t, q_t, a_t)$.
If the robot succeeds in reaching the goal, it accumulates a \emph{success cost} $R_S(a_t)$, the expected cost for robot to reach the goal, and navigation is complete.
If it fails to reach the goal, the robot accumulates a cost associated with exploring the region and needing to turn back $R_E(a_t)$ and the state is updated to reflect that the robot has moved (to position $q(a_t)$) and that the subgoal associated with $a_t$ is explored: $b_{t+1} = \left<m_t, \mathcal{S}_u\backslash \{a_t\}, q(a_t)\right>$.
Upon failing to reach the goal, the robot must subsequently select another action from the set of unexplored subgoals: $a_{t+1}\in\mathcal{S}_u\backslash \{a_t\}$.

The expected cost of a high-level action $a_t$ can be written as a Bellman Equation:\footnote{The statistics of belief ($P_S$, $R_S$, and $R_E$) are used as input to the planner for planning, instead of the belief $b_t$, in order to transform the challenging POMDP in Eq.~\eqref{eq:POMDP} into a simpler stochastic MDP in Eq.~\eqref{eq:LSP}.}
% The expected cost of a high-level action $a_t$ can be written as a Bellman Equation:\footnote{We don't explicitly use the belief ($b_t$) during planning. We estimate $P_S$, $R_S$, and $R_E$ from the belief $(b_t)$ which is used for plannning. The notations are shortened here for clarity.}
\begin{multline}\label{eq:LSP}
    Q(b_t,a_t \in \mathcal{S}_u) = D(m_t, q_t, a_t) + P_S(a_t)  R_S(a_t) + \\\quad{}
    (1- P_S(a_t))\bigg[ R_E(a_t) + \!\! \min_{a_{t+1}\in\mathcal{S}_u\backslash \{a_t\}} Q(b_{t+1},a_{t+1})) \bigg]
\end{multline}
% \textcolor{blue}{ These terms ---too difficult to compute exactly---are estimated from images collected on board the robot via learning.}

The terms $P_S$, $R_S$, and $R_E$---too difficult to compute exactly---are estimated from images collected on board the robot via learning.

% concluding *remarks*
% For single-robot planning, LSP, reliable by design, has demonstrated state of the art performance under uncertainty, despite its reliance on learning.
