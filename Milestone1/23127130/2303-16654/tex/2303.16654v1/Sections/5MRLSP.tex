\section{Multi-Robot Learning over Subgoals Planning (MR-LSP)} \label{sec:MR-LSP}
% First talk about why multi-robot planning using LSP is non-trivial
For single robot planning, the Learning over Subgoals planning paradigm (LSP) has demonstrated state-of-the-art performance under uncertainty and reliability-by-design, despite its reliance on learning.
% The use of high-level action set, and predicted properties of unseen space has allowed LSP to plan ahead into the future.
Here, we extend the LSP state and action abstraction to support multi-robot planning.

While the LSP action abstraction is designed for single robot planning, generalizing the space of high-level actions to incorporate multi-robot planning is straightforward---the \emph{collective} high-level action assigns each robot a subgoal for it to navigate towards and explore beyond.
As such the collective action for an $N$-robot team can be written as a list of subgoals: $a_t = [\sigma_1, \sigma_2, \cdots, \sigma_N]$.

However, generalizing the LSP model to support multiple robots is made complicated by the fact that different robots are executing their respective subgoal-actions \emph{concurrently} and therefore may finish exploration at different times; when one robot has finished exploration beyond one subgoal, the others may not yet be done and may not even have reached the unseen space they seek to explore.
Moreover, whenever one robot completes a subgoal-action, the planner has the capacity to reassign the actions of each robot and thus where they should travel next.
If planning is to take into account the impact of concurrent action execution, we must augment the LSP state abstraction and state transition model to incorporate these effects.

In this section, we introduce our Multi-Robot Learning over Subgoals Planner (MR-LSP), which introduces a new state and state transition abstraction to model concurrent action execution, an essential component of multi-robot coordination.
Key to our approach is the recognition that estimates of the expected time to complete an action (the costs of success $R_S$ and failure $R_E$) tell us which robot is expected to complete its exploration first, knowledge we can use to simplify the process of imagining the future during planning.
We introduce a Bellman Equation for our new multi-robot planning abstraction (Sec.~\ref{sec:mrlsp:detail}), which we use in combination with a Monte-Carlo Tree Search-based approach to compute the expected cost of multi-robot collective action (Sec.~\ref{sec:pouct}).

% Define high level action and state
\subsection{Expected Cost of Multi-Robot High-Level Actions}\label{sec:mrlsp:detail}
Under the MR-LSP model, the abstract state of the environment is a tuple $b_t = \left<m_t, \mathcal{S}_u, \mathcal{S}_g, q_t\right>$,  where $m_t$ is the map, $\mathcal{S}_u$ is the set of unexplored subgoals, $\mathcal{S}_g$ is the set of subgoals that are known to lead to the goal and $q_t$ is a list of the robot poses.
A high-level \emph{collective action} $a_t$ assigns each robot a subgoal to explore.
For an abstract state $b_t$, the set of high-level actions $\mathcal{A}(b_t)$ specifies all collective-actions for the robot team: $\mathcal{A}(b_t) =\bigotimes_{i\in I}(\mathcal{S}_u \cup \mathcal{S}_g)$,  constrained such that no two robots can explore same subgoal whenever possible.
A collective-action $a_t$ is thus a list of subgoals, that specifies to which subgoal each robot will next navigate to and explore beyond in an effort to reveal the goal.

Key to defining our state transition model is the idea that the multi-robot team can select a new collective-action whenever a \emph{single} robot reveals whether or not a subgoal will lead to the goal; the high-level state is updated when the \emph{first} of the robot's subgoal-actions is completed.
As in the LSP model, under our multi-robot abstraction we estimate how long it will take a single robot to either reach the goal ($D + R_S$) or to explore ($D + R_E$) beyond a particular subgoal $s$.
Given estimates of the costs of success $R_S$ and exploration $R_E$, we can determine which of the robots is expected to complete its high-level action first, yielding both (i) the subgoal $\sigma'$ that the team will first discover either does or does not lead to the goal and (ii) how long this discovery will take $D'$. Mathematically,
\begin{equation}\label{eq:frontiers-and-cost}
\begin{split}
D'(b_t, a_t) &= \min_{\forall \sigma \in a_t}\!\left(D(b_t, \sigma) + \min(R_S(\sigma), R_E(\sigma))\right) \\
\sigma'(b_t, a_t) &= \argmin_{\forall \sigma \in a_t}\!\left(D(b_t, \sigma) + \min(R_S(\sigma), R_E(\sigma))\right) 
\end{split}
\end{equation}

Under our model, the outcome of a collective action $a_t$ reveals only whether $\sigma'$ leads to the goal or not, and so the outcome of that high-level collective action is binary.
After the execution of collective action ($a_t$),
with probability $P_S(\sigma')$ the state transitions to an abstract \emph{success state} ($b_{S}$) where the robots can reach goal from subgoal $\sigma'$ or with probability $1-P_S(\sigma')$ the state transitions to an abstract \emph{failure} state ($b_{F}$) where the robots cannot reach the goal from subgoal $\sigma'$. 
Under both success and failure, the subgoal $\sigma'$ becomes explored, and so is removed from $\mathcal{S}_u$, and the robot have traveled a distance $D'$ towards completing their respective high-level subgoal-actions, ending up at new poses $q_t(a_t, D')$.
The tuple representation of abstract states associated with success ($b_{S}$) and failure ($b_{F}$) to find a path to the goal is:
\begin{align}\label{eq:updated-beliefs}
    b_{S} &= \left<m_t, \mathcal{S}_u' =  \mathcal{S}_u \backslash \{\sigma'\}, \mathcal{S}_g' = \mathcal{S}_g \cup \{\sigma'\}, q_t(a_t, D')\right> \nonumber \\
    b_{F} &= \left<m_t, \mathcal{S}_u' =  \mathcal{S}_u \backslash \{\sigma'\}, \mathcal{S}_g' = \mathcal{S}_g, q_t(a_t, D')\right>
\end{align}

Given our state and action abstraction and state transition model, we can write a Bellman Equation that defines the expected cost $Q$ for a collective action $a_t$ (see also Fig.~\ref{fig:mrlsp-schematic}):
\begin{multline} \label{eq:MR-LSP}
    Q(b_t, a_t \in \mathcal{A}(b_t)) = D' + P_S(\sigma')  \min_{a_{t+1} \in \mathcal{A}(b_{S})} Q(b_{S},a_{t+1}) \\
    +\big[1 - P_S(\sigma')\big] \min_{a_{t+1} \in \mathcal{A}(b_{F})}Q(b_{F},a_{t+1})
\end{multline}
Difficult to compute exactly, the terms $P_S$, $R_S$, and $R_E$ are estimated via learning from images collected on the robot; see Sec.~\ref{sec:Training} for a discussion of our neural network structure and training process.
Owing to the combinatorial explosion of possible actions, we cannot tractably plan while including all subgoals in the set of candidate actions. Following the example of LSP~\cite{stein2018learning}, we limit the number of subgoals under consideration to seven for all MR-LSP experiments.

We note that when a single robot is used, our MR-LSP equation reduces to the single-robot LSP model of Stein et al.~\cite{stein2018learning}, and so our approach extends the original LSP model to support multi-robot planning.\footnote{Owing to a change in how exploration is treated during concurrent action execution in our MR-LSP model---see Eq.~\eqref{eq:frontiers-and-cost}---the single-robot exploration cost $R_E$ of Eq.~\eqref{eq:LSP} becomes $R_E \leftarrow \text{min}(R_S, R_E)$, a slight deviation from the original LSP definition. With this (small) change, one-robot MR-LSP via Eq.~\eqref{eq:MR-LSP} is equivalent to single-robot LSP planning via Eq.~\eqref{eq:LSP}.}


\subsection{Navigation via MR-LSP}\label{sec:mrlsp:navigation}
The high-level collective action defines the long-horizon robot behavior.
Upon selecting a collective action $a_t$ via Eq.~\eqref{eq:MR-LSP}, each robot makes progress towards each subgoal.
We use an A$^{\!*}$ plan cost computed over the observed grid to select motion primitives for each robot that make progress towards their assigned subgoal.
The robots (i) move according to these primitive actions, (ii) observe their surroundings, (iii) update the partial map and the set of subgoals, and (iv) compute a new collective action based on this newly-updated map.
This process repeats until the goal is reached.
