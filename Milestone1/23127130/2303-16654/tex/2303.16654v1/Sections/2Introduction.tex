\section{Introduction}

% \textbf{1st par}: Write what we're trying to do. \par
We aim to navigate through an unknown environment using multiple robots to find an unseen point goal in minimum expected distance: e.g., for package retrieval. To perform well, the multi-robot team needs to collectively navigate the unexplored region, seeking out promising routes to the goal while avoiding regions that typically lead to dead-ends.
% or region that are already explored by other robots.

% \textbf{2nd par}: \textbf{Why is it hard?} \\
Planning well in an unknown environment requires making inferences about unseen parts of the environment; in theory, robots must envision all possible configurations of the unknown space---including regions unlikely to lead to the goal---to determine how to navigate so that they can most quickly reach the unseen goal.
Multi-robot planning under uncertainty can be modeled as a Decentralized Partially Observable Markov Decision Processes (Dec-POMDP) \cite{DECPOMDP_MAIN1,DECPOMDPMain2}. However, Dec-POMDP planning is computationally intractable in general~\cite{DecPOMDPintractable} and it requires access to a distribution over possible environments, difficult to obtain in general.

% Learning based techniques for trying to predict unseen space.
Learning is often used to help make inferences about unseen space needed to inform good behavior and is an increasingly powerful tool for planning under uncertainty~\cite{gupta,richter1} and Dec-POMDP planning \cite{DecPOMDPusingLearning}.
However, despite impressive progress in this domain, particularly for model-free approaches trained via deep reinforcement learning~\cite{marl, rl1, rl2}, many such strategies struggle to learn effectively in large-scale environments and can be brittle in practice~\cite{RLsurvey}.

\begin{figure}[t]
        \centering
        \includegraphics[width=8.45cm]{Sections/images/intro.png}
        \caption{Our \textbf{Multi-Robot Learning over Subgoals Planner (MR-LSP)} uses learning to guide robots toward coordinated exploration of regions more likely to reach the unseen goal.}
        \vspace{-2em}\label{fig:intro}
\end{figure}
So as to avoid the computational expense of planning over the space of short-time-horizon primitive actions, many approaches for multi-robot planning introduce a topological action abstraction that simplify planning~\cite{frontier-multirobot,topological-mr,frontier-mr-1}. Such approaches typically constrain robot motion so that each robot intends to leave known space through different \emph{frontiers}, each a boundary between free and unseen space, or via paths constrained to belong to different relative homology \cite{bhattacharyaTopologicalConstraints, BhattacharyaHomologyPathPlanning}.
Though these action abstractions simplify planning and coordination between robots, action selection typically relies on simple, greedy heuristics to decide where each robot should navigate next and do not alleviate the challenges of predicting the impact of an action.

The recent Learning over Subgoals Planning abstraction (LSP)~\cite{stein2018learning} overcomes this limitation for single-robot planning.
In LSP, learning is used to estimate the goodness of high-level (frontier-associated) actions that enter unseen space, including both the likelihood that such an action will reach the goal and its expected cost.
Learning augments a model-based planning abstraction, affording performant and reliable navigation under uncertainty.
However, despite LSP's improved performance in single-robot planning under uncertainty, the LSP state transition model is not straightforwardly extended to support multiple robots, which must have the capacity to concurrently explore different unseen regions.

%Abstraction in action introduced by these work not only makes planning process simpler but also supports reasoning about how humans decide to move in a particular direction. However, they typically relies on simple, greedy heuristics to decide where each robot should navigate next and do not alleviate the challenges of predicting the impact of an action. \par

% We want to use topological action abstraction, and use learning to make inference about the unseen parts of the environment. 

% \textbf{4th Par}:What are you gonna do about it, How are you combining all of it?
Leveraging insights from multi-robot topologically constrained planning and the recent Learning over Subgoals Planning (LSP) abstraction, we introduce a multi-robot generalization of the LSP model: Multi-Robot Learning over Subgoal Planning (MR-LSP). Our approach supports multi-robot planning, is reliable, and leverages learning to inform where robots should navigate next.
Each robot's high-level (topologically-constrained) actions correspond to navigation to a \emph{subgoal}---associated with a frontier---and then navigation beyond in an effort to reach the goal.
% [MAYBE INCLUDE: approximate Dec-POMDP as a stochastic MDP]
% [TODO: say 1 sentence about the model / transition model that makes it clear that/why/how we do what LSP could not. We introduce a new state abstraction and transition model that captures [something something multi-robot].]
We introduce a new state abstraction and transition model that allows our high-level planner to envision how the robot team will redistribute effort once each robot finishes their respective exploratory action, a key feature of coordinated multi-robot planning.
%robots concurrently exploring different regions of space and how the robot team can redistribute effort when an action is completed, a key feature of coordinated multi-robot planning.
%that robots can concurrently explore different regions of space and replan once that exploration is done, a key feature of multi-robot planning not modeled by the single-robot LSP abstraction.
%the progress of a robot's action concurrently and capability for robots to reallocate actions in the response of robot finishing its action. 
Our abstraction lets us approximate the challenging Dec-POMDP as a stochastic MDP and solve it via sample-based tree search.
Learning is used to estimate the goodness of each action and informs planning via a Bellman Equation for our model-based planning abstraction.
%With only a single robot, our approach reduces to previous work \cite{stein2018learning}. 

We demonstrate the effectiveness of our approach in a simulated office floorplan environment, showing that our approach reduces cost by 13.29\% (two robots) and 4.6\% (three robots) versus standard non-learned optimistic planning and a competitive learning-informed baseline of our own design.

% and 4.6\% lower cost for a two robots and three robots as compared against both a 
% with cost accrued by standard optimistic planners in simulated office floor environment.
% Our approach shows similar performance with other learned baselines in simpler environment but performs significantly better in relatively complex environment like office floor environment which is a better representation of real world. 
