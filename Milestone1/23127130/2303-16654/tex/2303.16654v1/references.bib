% Frontier based exploration for single robot
@article{frontier-singlerobot,
  title={A frontier-based approach for autonomous exploration},
  author={Brian Yamauchi},
  journal={Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'},
  year={1997},
  pages = {}
}

% Frontier based exploration for multiple robots
@inproceedings{frontier-multirobot,
  title={Frontier-based exploration using multiple robots},
  author={Yamauchi, Brian},
  booktitle={Proceedings of the second international conference on Autonomous agents},
  pages = {},
  year={1998}
}

% Topological planning for multirobot
@INPROCEEDINGS{frontier-mr-1,

  author={Wang, Yiheng and Liang, Alei and Guan, Haibing},

  booktitle={2011 IEEE Symposium on Swarm Intelligence}, 

  title={Frontier-based multi-robot map exploration using Particle Swarm Optimization}, 

  year={2011},

  volume={},

  number={},

  pages = {},

  doi={10.1109/SIS.2011.5952584}}

% topological planning mr
@InProceedings{topological-mr,
  author = {Piyush Khandelwal and Peter Stone},
  title = {Multi-robot Human Guidance using Topological Graphs},
  booktitle = {AAAI Spring 2014 Symposium on Qualitative Representations for Robots (AAAI-SSS)},
  location = {Stanford, California, USA},
  month = {},
  year = {2014},
  abstract = {
      Prior approaches to human guidance using robots inside a building have
      typically been limited to a single robot guide that navigates a human
      from start to goal. However, due to their limited mobility, the robot is
      often unable to keep up with the human's natural speed. In contrast, this
      paper addresses this difference in mobility between robots and people by
      presenting an approach that uses multiple robots to guide a human. Our
      approach uses a compact topological graph representation of the
      environment, and we first present the procedure for generating this
      representation. Next, we formulate the multi-robot guidance problem as a
      Markov Decision Process (MDP). Using a model of human motion in the
      presence of guiding robots, we define the transition function for this
      MDP. Finally, we solve the MDP using Value Iteration to obtain an
      optimal policy for placing robots and evaluate this policy's
      effectiveness.  
  },
}


@inproceedings{stein2018learning,
  title={Learning over subgoals for efficient navigation of structured, unknown environments},
  author={Stein, Gregory J and Bradley, Christopher and Roy, Nicholas},
  booktitle={Conference on Robot Learning},
  pages = {},
  year={2018},
  organization={}
}

@INPROCEEDINGS{richter1,

  author={Richter, Charles and Ware, John and Roy, Nicholas},

  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, 

  title={High-speed autonomous navigation of unknown environments using learned probabilities of collision}, 

  year={2014},

  volume={},

  number={},

  pages = {},

  doi={10.1109/ICRA.2014.6907760}}
  
 
@misc{gupta,
  doi = {10.48550/ARXIV.1702.03920},
  
  author = {Gupta, Saurabh and Tolani, Varun and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cognitive Mapping and Planning for Visual Navigation},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={},
  number={},
  pages = {},
  year={1998},
  publisher={}
}


@article{article,
author = {Madani, Omid},
year = {1999},
month = {12},
pages = {},
title = {On the Computability of Infinite-Horizon Partially Observable Markov Decision Processes}
}

@inproceedings{Kuhn2010TheHM,
  title={The Hungarian Method for the Assignment Problem},
  author={Harold W. Kuhn},
  booktitle={50 Years of Integer Programming},
  year={2010}
}


@article{doi:10.1177/0278364913496484,
author = {G. Ayorkor Korsah and Anthony Stentz and M. Bernardine Dias},
title ={A comprehensive taxonomy for multi-robot task allocation},
journal = {The International Journal of Robotics Research},
volume = {32},
number = {12},
pages = {},
year = {2013},
doi = {10.1177/0278364913496484},

URL = { 
        https://doi.org/10.1177/0278364913496484
    
},
eprint = { 
        https://doi.org/10.1177/0278364913496484
    
}
,
    abstract = { Task allocation is an important aspect of many multi-robot systems. The features and complexity of multi-robot task allocation (MRTA) problems are dictated by the requirements of the particular domain under consideration. These problems can range from those involving instantaneous distribution of simple, independent tasks among members of a homogenous team, to those requiring the time-extended scheduling of complex interrelated multi-step tasks for members of a heterogenous team related by several constraints. The existing widely used taxonomy for task allocation in multi-robot systems was designed for problems with independent tasks and does not deal with problems with interrelated utilities and constraints. While that taxonomy was a ground-breaking contribution to the MRTA literature, a survey of recent work in MRTA reveals that it is no longer a sufficient taxonomy, due to the increasing importance of interrelated utilities and constraints in realistic MRTA problems under consideration. Thus, in this paper, we present a new, comprehensive taxonomy, iTax, that explicitly takes into consideration the issues of interrelated utilities and constraints. Our taxonomy maps categories of MRTA problems to existing mathematical models from combinatorial optimization and operations research, and hence draws important parallels between robotics and these fields. }
}

@techreport{Pineau-2002-8519,
author = {Joelle Pineau and Sebastian Thrun},
title = {An integrated approach to hierarchy and abstraction for POMDPs},
year = {2002},
month = {},
institution = {Carnegie Mellon University},
address = {},
number = {},
keywords = {Markov decision process, POMDP, reinforcement learning, hierarchical planning, abstraction, robot control, dialogue systems},
}
  
  
  %Decpomdps
  @inproceedings{DecPOMDPintractable,
author = {Bernstein, Daniel S. and Hansen, Eric A. and Zilberstein, Shlomo},
title = {Bounded Policy Iteration for Decentralized POMDPs},
year = {2005},
publisher = {},
address = {},
abstract = {We present a bounded policy iteration algorithm for infinite-horizon decentralized POMDPs. Policies are represented as joint stochastic finite-state controllers, which consist of a local controller for each agent. We also let a joint controller include a correlation device that allows the agents to correlate their behavior without exchanging information during execution, and show that this leads to improved performance. The algorithm uses a fixed amount of memory, and each iteration is guaranteed to produce a controller with value at least as high as the previous one for all possible initial state distributions. For the case of a single agent, the algorithm reduces to Poupart and Boutilier's bounded policy iteration for POMDPs.},
booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
pages = {},
numpages = {6},
location = {Edinburgh, Scotland},
series = {}
}
  @INPROCEEDINGS{DECPOMDP_MAIN1,

  author={Amato, Christopher and Chowdhary, Girish and Geramifard, Alborz and Ãœre, N. Kemal and Kochenderfer, Mykel J.},

  booktitle={52nd IEEE Conference on Decision and Control}, 

  title={Decentralized control of partially observable Markov decision processes}, 

  year={2013},

  volume={},

  number={},

  pages = {},

  doi={10.1109/CDC.2013.6760239}}

  @inproceedings{DecPOMDPusingLearning,
author = {Amato, Christopher},
title = {Decision-Making under Uncertainty in Multi-Agent and Multi-Robot Systems: Planning and Learning},
year = {2018},
isbn = {9780999241127},
publisher = {},
abstract = {Multi-agent planning and learning methods are becoming increasingly important in today's interconnected world. Methods for real-world domains, such as robotics, must consider uncertainty and limited communication in order to generate high-quality, robust solutions. This paper discusses our work on developing principled models to represent these problems and planning and learning methods that can scale to realistic multi-agent and multi-robot tasks.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {},
numpages = {5},
location = {Stockholm, Sweden},
series = {}
}

@inproceedings{MACROACTION1,
author = {Amato, Christopher and Konidaris, George D. and Kaelbling, Leslie P.},
title = {Planning with Macro-Actions in Decentralized POMDPs},
year = {2014},
isbn = {9781450327381},
publisher = {},
address = {},
abstract = {Decentralized partially observable Markov decision processes (Dec-POMDPs) are general models for decentralized decision making under uncertainty. However, they typically model a problem at a low level of granularity, where each agent's actions are primitive operations lasting exactly one time step. We address the case where each agent has macro-actions: temporally extended actions which may require different amounts of time to execute. We model macro-actions as 'options' in a factored Dec-POMDP model, focusing on options which depend only on information available to an individual agent while executing. This enables us to model systems where coordination decisions only occur at the level of deciding which macro-actions to execute, and the macro-actions themselves can then be executed to completion. The core technical difficulty when using options in a Dec-POMDP is that the options chosen by the agents no longer terminate at the same time. We present extensions of two leading Dec-POMDP algorithms for generating a policy with options and discuss the resulting form of optimality. Our results show that these algorithms retain agent coordination while allowing near-optimal solutions to be generated for significantly longer horizons and larger state-spaces than previous Dec-POMDP methods.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {},
numpages = {},
keywords = {hierarchy, decentralized pomdps, planning under uncertainty},
location = {},
series = {}
}

@INPROCEEDINGS{MACROACTION2,  author={Amato, Christopher and Konidaris, George and Cruz, Gabriel and Maynor, Christopher A. and How, Jonathan P. and Kaelbling, Leslie P.},  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},   title={Planning for decentralized control of multiple robots under uncertainty},   year={2015},  volume={},  number={},  pages={},  doi={10.1109/ICRA.2015.7139350}}


@article{MACROACTION3,
author = {Christopher Amato and George Konidaris and Ariel Anders and Gabriel Cruz and Jonathan P How and Leslie P Kaelbling},
title ={Policy search for multi-robot coordination under uncertainty},
journal = {The International Journal of Robotics Research},
volume = {},
number = {},
pages = {},
year = {2016},
doi = {10.1177/0278364916679611},

eprint = { 
        https://doi.org/10.1177/0278364916679611
    
}
,
    abstract = { We introduce a principled method for multi-robot coordination based on a general model (termed a MacDec-POMDP) of multi-robot cooperative planning in the presence of stochasticity, uncertain sensing, and communication limitations. A new MacDec-POMDP planning algorithm is presented that searches over policies represented as finite-state controllers, rather than the previous policy tree representation. Finite-state controllers can be much more concise than trees, are much easier to interpret, and can operate over an infinite horizon. The resulting policy search algorithm requires a substantially simpler simulator that models only the outcomes of executing a given set of motor controllers, not the details of the executions themselves and can solve significantly larger problems than existing MacDec-POMDP planners. We demonstrate significant performance improvements over previous methods and show that our method can be used for actual multi-robot systems through experiments on a cooperative multi-robot bartending domain. }
}


@misc{DECPOMDPMain2,
  doi = {10.48550/ARXIV.1502.06030},
  
  author = {Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar and Amato, Christopher and How, Jonathan P.},
  
  keywords = {Multiagent Systems (cs.MA), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Decentralized Control of Partially Observable Markov Decision Processes using Belief Space Macro-actions},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% REINFORCEMENT LEARNING
@inproceedings{rl1,
  title={Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  author={Jayesh K. Gupta and Maxim Egorov and Mykel J. Kochenderfer},
  booktitle={AAMAS Workshops},
  year={2017}
}

@INPROCEEDINGS{marl,

  author={Jin, Yue and Zhang, Yaodong and Yuan, Jian and Zhang, Xudong},

  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 

  title={Efficient Multi-agent Cooperative Navigation in Unknown Environments with Interlaced Deep Reinforcement Learning}, 

  year={2019},

  volume={},

  number={},

  pages = {},


  doi={10.1109/ICASSP.2019.8682555}}

@misc{rl2,
  doi = {10.48550/ARXIV.1605.06676},
  
  author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{RLsurvey,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={},
  number={},
  pages = {},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}


@article{bhattacharyaTopologicalConstraints,
author = {Bhattacharya, S. and Likhachev, M. and Kumar, Vijay},
year = {2012},
month = {},
pages = {},
title = {Topological constraints in search-based robot path planning},
volume = {},
journal = {Autonomous Robots},
doi = {10.1007/s10514-012-9304-1}
}

@ARTICLE{BhattacharyaHomologyPathPlanning,

  author={Bhattacharya, Subhrajit and Ghrist, Robert and Kumar, Vijay},

  journal={IEEE Transactions on Robotics}, 

  title={Persistent Homology for Path Planning in Uncertain Environments}, 

  year={2015},

  volume={},

  number={},

  pages = {},

  doi={10.1109/TRO.2015.2412051}}

@inproceedings{merlin2020locally,
  title={Locally observable markov decision processes},
  author={Merlin, Max and Parikh, Neev and Rosen, Eric and Konidaris, George},
  booktitle={ICRA 2020 Workshop on Perception, Action, Learning},
  year={2020}
}

@article{silver2010monte,
  title={Monte-Carlo planning in large POMDPs},
  author={Silver, David and Veness, Joel},
  journal={Advances in neural information processing systems},
  volume={},
  year={2010}
}

@inproceedings{hansen2004dynamic,
  title={Dynamic programming for partially observable stochastic games},
  author={Hansen, Eric A and Bernstein, Daniel S and Zilberstein, Shlomo},
  booktitle={AAAI},
  volume={},
  pages = {},
  year={2004}
}

@article{szer2012maa,
  title={{MAA*}: A heuristic search algorithm for solving decentralized POMDPs},
  author={Szer, Daniel and Charpillet, Fran{\c{c}}ois and Zilberstein, Shlomo},
  journal={arXiv preprint arXiv:1207.1359},
  year={2012}
}

@incollection{littman1995learning,
  title={Learning policies for partially observable environments: Scaling up},
  author={Littman, Michael L and Cassandra, Anthony R and Kaelbling, Leslie Pack},
  booktitle={Machine Learning Proceedings},
  pages = {},
  year={1995},
  publisher={}
}

@inproceedings{hoerger2019multilevel,
  title={Multilevel monte-carlo for solving pomdps online},
  author={Hoerger, Marcus and Kurniawati, Hanna and Elfes, Alberto},
  booktitle={The International Symposium of Robotics Research},
  pages = {},
  year={2019},
  organization={}
}

@inproceedings{amato2015scalable,
  title={Scalable planning and learning for multiagent POMDPs},
  author={Amato, Christopher and Oliehoek, Frans},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={},
  number={},
  year={2015}
}

@inproceedings{bradley2021learning,
  title={Learning and planning for temporally extended tasks in unknown environments},
  author={Bradley, Christopher and Pacheck, Adam and Stein, Gregory J and Castro, Sebastian and Kress-Gazit, Hadas and Roy, Nicholas},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {},
  year={2021},
  organization={}
}

@inproceedings{lauer2000algorithm,
  title={An algorithm for distributed reinforcement learning in cooperative multi-agent systems},
  author={Lauer, Martin and Riedmiller, Martin},
  booktitle={In Proceedings of the Seventeenth International Conference on Machine Learning},
  year={2000},
  organization={}
}
@article{matignon2012independent,
  title={Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems},
  author={Matignon, Laetitia and Laurent, Guillaume J and Le Fort-Piat, Nadine},
  journal={The Knowledge Engineering Review},
  volume={},
  number={},
  pages = {},
  year={2012},
  publisher={}
}

@inproceedings{henderson2018rlmatters,
  title =	 {Deep Reinforcement Learning that Matters},
  author =	 {Peter Henderson and Riashat Islam and Philip Bachman and
                  Joelle Pineau and Doina Precup and David Meger},
  booktitle =	 {AAAI Conference on Artificial Intelligence},
  year =	 2018
}
