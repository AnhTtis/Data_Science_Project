% Encoding: UTF-8
@String(AAAI = {AAAI})
@String(ACCV  = {ACCV})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(CGF  = {Comput. Graph. Forum})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})
@String(CVM = {Computational Visual Media})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(ICASSP=	{ICASSP})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(IJCAI = {IJCAI})
@String(IJCV = {Int. J. Comput. Vis.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(JOV	 = {J. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(PR   = {Pattern Recognition})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(TCSVT = {IEEE TCSVT})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(TOG= {ACM Trans. Graph.})
@String(TVC  = {The Vis. Comput.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(VR   = {Vis. Res.})

@InProceedings{chen2022think-GL,
  author    = {Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  booktitle = {CVPR},
  title     = {Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation},
  year      = {2022},
  pages     = {16537--16547},
}

@InProceedings{qiao2022HOP,
  author    = {Qiao, Yanyuan and Qi, Yuankai and Hong, Yicong and Yu, Zheng and Wang, Peng and Wu, Qi},
  booktitle = {CVPR},
  title     = {{HOP}: History-and-Order Aware Pre-training for Vision-and-Language Navigation},
  year      = {2022},
  pages     = {15418--15427},
}

@InProceedings{chen2021history,
  author    = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},
  booktitle = {NeurIPS},
  title     = {History aware multimodal transformer for vision-and-language navigation},
  year      = {2021},
  pages     = {5834--5847},
}

@InProceedings{hong2020l-e-graph,
  author    = {Hong, Yicong and Rodriguez, Cristian and Qi, Yuankai and Wu, Qi and Gould, Stephen},
  booktitle = {NeurIPS},
  title     = {Language and visual entity relationship graph for agent navigation},
  year      = {2020},
  pages     = {7685-7696},
  journal_  = {NeurIPS},
}

@InProceedings{Chen_2022_HM3D_AutoVLN,
  author    = {Chen, Shizhe and Guhur, Pierre-Louis and Tapaswi, Makarand and Schmid, Cordelia and Laptev, Ivan},
  booktitle = {ECCV},
  title     = {Learning from Unlabeled 3D Environments for Vision-and-Language Navigation},
  year      = {2022},
  pages     = {638--655},
}

@InProceedings{moudgil2021soat,
  author    = {Moudgil, Abhinav and Majumdar, Arjun and Agrawal, Harsh and Lee, Stefan and Batra, Dhruv},
  booktitle = {NeurIPS},
  title     = {{SOAT}: A scene-and object-aware transformer for vision-and-language navigation},
  year      = {2021},
  pages     = {7357--7367},
  journal_  = {Advances in Neural Information Processing Systems},
}

@InProceedings{2021airbert,
  author    = {Guhur, Pierre-Louis and Tapaswi, Makarand and Chen, Shizhe and Laptev, Ivan and Schmid, Cordelia},
  booktitle = {CVPR},
  title     = {Airbert: In-domain pretraining for vision-and-language navigation},
  year      = {2021},
  pages     = {1634--1643},
}

@InProceedings{VLN-2018vision,
  author    = {Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle = {CVPR},
  title     = {Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  year      = {2018},
  pages     = {3674--3683},
}

@InProceedings{hong2021vln-bert,
  author    = {Hong, Yicong and Wu, Qi and Qi, Yuankai and Rodriguez-Opazo, Cristian and Gould, Stephen},
  booktitle = {CVPR},
  title     = {Vln bert: A recurrent vision-and-language bert for navigation},
  year      = {2021},
  pages     = {1643--1653},
}

@InProceedings{zhu2021soon,
  author    = {Zhu, Fengda and Liang, Xiwen and Zhu, Yi and Yu, Qizhi and Chang, Xiaojun and Liang, Xiaodan},
  booktitle = {CVPR},
  title     = {Soon: Scenario oriented object navigation with graph-based exploration},
  year      = {2021},
  pages     = {12689--12699},
}

@InProceedings{2020reverie,
  author    = {Qi, Yuankai and Wu, Qi and Anderson, Peter and Wang, Xin and Wang, William Yang and Shen, Chunhua and Hengel, Anton van den},
  booktitle = {CVPR},
  title     = {Reverie: Remote embodied visual referring expression in real indoor environments},
  year      = {2020},
  pages     = {9982--9991},
}

@InProceedings{anderson2018bottom,
  author    = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle = {CVPR},
  title     = {Bottom-up and top-down attention for image captioning and visual question answering},
  year      = {2018},
  pages     = {6077--6086},
}

@InProceedings{hao2020towards,
  author    = {Hao, Weituo and Li, Chunyuan and Li, Xiujun and Carin, Lawrence and Gao, Jianfeng},
  booktitle = {CVPR},
  title     = {Towards learning a generic agent for vision-and-language navigation via pre-training},
  year      = {2020},
  pages     = {13137--13146},
}

@InProceedings{dialog-navigation-2019,
  author        = {Thomason, Jesse and Murray, Michael and Cakmak, Maya and Zettlemoyer, Luke},
  booktitle     = {CoRL},
  title         = {Vision-and-dialog navigation},
  year          = {2019},
  pages         = {394--406},
  organization_ = {PMLR},
}

@InProceedings{2018-speaker,
  author    = {Fried, Daniel and Hu, Ronghang and Cirik, Volkan and Rohrbach, Anna and Andreas, Jacob and Morency, Louis-Philippe and Berg-Kirkpatrick, Taylor and Saenko, Kate and Klein, Dan and Darrell, Trevor},
  booktitle = {NeurIPS},
  title     = {Speaker-follower models for vision-and-language navigation},
  year      = {2018},
  journal   = {NeurIPS},
}

@InProceedings{tan2019learning,
  author    = {Tan, Hao and Yu, Licheng and Bansal, Mohit},
  booktitle = {NAACL},
  title     = {Learning to navigate unseen environments: Back translation with environmental dropout},
  year      = {2019},
  pages     = {2610--2621},
  journal_  = {arXiv preprint arXiv:1904.04195},
}

@InProceedings{2019reinforced,
  author    = {Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Wang, William Yang and Zhang, Lei},
  booktitle = {CVPR},
  title     = {Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation},
  year      = {2019},
  pages     = {6629--6638},
}

@InProceedings{qi2021-road-to-know,
  author    = {Qi, Yuankai and Pan, Zizheng and Hong, Yicong and Yang, Ming-Hsuan and van den Hengel, Anton and Wu, Qi},
  booktitle = {ICCV},
  title     = {The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation},
  year      = {2021},
  pages     = {1655--1664},
}

@InProceedings{2021structured-scene,
  author    = {Wang, Hanqing and Wang, Wenguan and Liang, Wei and Xiong, Caiming and Shen, Jianbing},
  booktitle = {CVPR},
  title     = {Structured scene memory for vision-language navigation},
  year      = {2021},
  pages     = {8455--8464},
}

@InProceedings{2020-RXR,
  author    = {Ku, Alexander and Anderson, Peter and Patel, Roma and Ie, Eugene and Baldridge, Jason},
  booktitle = {EMNLP},
  title     = {Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding},
  year      = {2020},
  pages     = {4392-4412},
  journal_  = {arXiv preprint arXiv:2010.07954},
}

@InProceedings{zhu2020vision,
  author    = {Zhu, Fengda and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan},
  booktitle = {CVPR},
  title     = {Vision-language navigation with self-supervised auxiliary reasoning tasks},
  year      = {2020},
  pages     = {10012--10022},
}

@InProceedings{gao-2021room,
  author    = {Gao, Chen and Chen, Jinyu and Liu, Si and Wang, Luting and Zhang, Qiong and Wu, Qi},
  booktitle = {CVPR},
  title     = {Room-and-object aware knowledge reasoning for remote embodied referring expression},
  year      = {2021},
  pages     = {3064--3073},
}

@InProceedings{2019-lxmert,
  author    = {Tan, Hao and Bansal, Mohit},
  booktitle = {EMNLP},
  title     = {Lxmert: Learning cross-modality encoder representations from transformers},
  year      = {2019},
  pages     = {5103--5114},
  journal_  = {arXiv preprint arXiv:1908.07490},
}

@InProceedings{radford2021learning,
  author        = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle     = {ICML},
  title         = {Learning transferable visual models from natural language supervision},
  year          = {2021},
  pages         = {8748--8763},
  organization_ = {PMLR},
}

@InProceedings{kuo2022beyond,
  author    = {Kuo, Chia-Wen and Kira, Zsolt},
  booktitle = {CVPR},
  title     = {Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning},
  year      = {2022},
  pages     = {17969--17979},
}

@InProceedings{shen2022k,
  author    = {Shen, Sheng and Li, Chunyuan and Hu, Xiaowei and Xie, Yujia and Yang, Jianwei and Zhang, Pengchuan and Rohrbach, Anna and Gan, Zhe and Wang, Lijuan and Yuan, Lu and others},
  booktitle = {NeurIPS},
  title     = {K-lite: Learning transferable visual models with external knowledge},
  year      = {2022},
  pages     = {15558--15573},
  journal_  = {arXiv preprint arXiv:2204.09222},
}

@InProceedings{2017conceptnet,
  author    = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle = {AAAI},
  title     = {Conceptnet 5.5: An open multilingual graph of general knowledge},
  year      = {2017},
  pages     = {4444--4451},
}

@InProceedings{auer2007dbpedia,
  author    = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle = {The Semantic Web},
  title     = {Dbpedia: A nucleus for a web of open data},
  year      = {2007},
  pages     = {722--735},
}

@InProceedings{qi2019ke,
  author    = {Qi, Mengshi and Wang, Yunhong and Qin, Jie and Li, Annan},
  booktitle = {CVPR},
  title     = {KE-GAN: Knowledge embedded generative adversarial networks for semi-supervised scene parsing},
  year      = {2019},
  pages     = {5237--5246},
}

@Book{meyer2012wiktionary,
  author    = {Meyer, Christian M and Gurevych, Iryna},
  publisher = {na},
  title     = {Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography},
  year      = {2012},
}

@Article{2017visual-genome,
  author    = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal   = {IJCV},
  title     = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  year      = {2017},
  number    = {1},
  pages     = {32--73},
  volume    = {123},
  publisher = {Springer},
}

@InProceedings{2020image-vit,
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle = {ICLR},
  title     = {An image is worth 16x16 words: Transformers for image recognition at scale},
  year      = {2020},
  journal_  = {arXiv preprint arXiv:2010.11929},
}

@InProceedings{ross2011reduction,
  author        = {Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle     = {AISTATS},
  title         = {A reduction of imitation learning and structured prediction to no-regret online learning},
  year          = {2011},
  pages         = {627--635},
  organization_ = {JMLR Workshop and Conference Proceedings},
}

@InProceedings{zhang2021vinvl,
  author    = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle = {CVPR},
  title     = {Vinvl: Revisiting visual representations in vision-language models},
  year      = {2021},
  pages     = {5579--5588},
}

@Article{gigerenzer1996reasoning,
  author    = {Gigerenzer, Gerd and Goldstein, Daniel G},
  journal   = {Psychological review},
  title     = {Reasoning the fast and frugal way: models of bounded rationality.},
  year      = {1996},
  number    = {4},
  pages     = {650},
  volume    = {103},
  publisher = {American Psychological Association},
}

@InProceedings{an2021neighbor,
  author    = {An, Dong and Qi, Yuankai and Huang, Yan and Wu, Qi and Wang, Liang and Tan, Tieniu},
  booktitle = {ACM MM},
  title     = {Neighbor-view enhanced model for vision and language navigation},
  year      = {2021},
  pages     = {5101-5109},
}

@InProceedings{zhu-etal-2022-diagnosing,
  author    = {Zhu, Wanrong and Qi, Yuankai and Narayana, Pradyumna and Sone, Kazoo and Basu, Sugato and Wang, Xin and Wu, Qi and Eckstein, Miguel and Wang, William Yang},
  booktitle = {NAACL},
  title     = {Diagnosing Vision-and-Language Navigation: What Really Matters},
  year      = {2022},
  pages     = {5981-5993},
}

@InProceedings{qi2020object,
  author    = {Qi, Yuankai and Pan, Zizheng and Zhang, Shengping and van den Hengel, Anton and Wu, Qi},
  booktitle = {ECCV},
  title     = {Object-and-action aware model for visual language navigation},
  year      = {2020},
  pages     = {303-317},
}

@Comment{jabref-meta: databaseType:bibtex;}
