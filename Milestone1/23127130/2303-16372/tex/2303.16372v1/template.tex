\documentclass{article}


\usepackage{mymacros}
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm}

\title{Non-Asymptotic Lower Bounds For Training Data Reconstruction}


\author{
 Prateeti Mukherjee \\
  Microsoft Research\\
  India\\
  \texttt{t-pmukherjee@microsoft.com} \\
  %% examples of more authors
   \And
 Satya Lokam \\
  Microsoft Research\\
  India\\
  \texttt{satya.lokam@microsoft.com} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive \emph{non-asymptotic} minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD \cite{Abadi2016} and Projected Noisy SGD\cite{Feldman2018} to cover the broader notion of metric differential privacy. 
\end{abstract}


% keywords can be removed
\keywords{Information Security \and Differential Privacy  \and Minimax Lower Bounds}

\section{Introduction}
\label{sec:introduction}

Machine Learning has become increasingly pervasive in real world, risk-sensitive systems. The rise in capabilities of machine learning is perhaps most apparent in recent times with the fascinating potential of large-scale deep learning systems such as Large Language Models \cite{gpt-3} and Neural Diffusion Models \cite{dall-e-model}. These powerful algorithms consume enormous amounts of data to derive meaningful patterns for improved empirical performance. Theoretical analysis, however, reveals that Neural Networks are capable of memorizing the underlying training data \cite{Bresler2020, eldan-memorization, ohadshamir-memorization}, rendering such algorithms vulnerable to adversarial attacks that can infer sensitive attributes of the training set, or worse, reconstruct one or more training data points entirely. Differential privacy was introduced in an attempt to formalize privacy protection while deriving meaningful conclusions from statistical information, and has since evolved into the gold-standard of privacy in machine learning. This has inspired a plethora of of research in the development of  differentially private counterparts of popular machine learning \cite{Kamalika2008, Jain2011, Kamalika2009, Alabi2020} and deep learning \cite{Abadi2016,Jie2022,Song2013} algorithms. However, the promise of differential privacy is often difficult to interpret, and a rigorous quantification of \emph{``how much data privacy does an $\epsilon$-DP guarantee actually confer?"} is important to develop sufficiently private algorithms. 


%In an attempt to formalize privacy in learning while deriving meaningful statistical information from data, differential privacy 

%has evolved into the \emph{de-facto} notion of privacy in machine learning, particularly due to the composability of privacy guarantees across computations and the inherent resilience to post-processing; the latter enables retention of meaningful privacy, even in the presence of arbitrary side information. This has inspired a plethora of research in the development of differentially private counterparts of popular machine learning algorithms\cite{Kamalika2008, Jain2011, Kamalika2009, Alabi2020}. However, the promise of differential privacy is often difficult to interpret, and a rigorous quantification of 
%\emph{``how much data privacy does an $\epsilon$-DP guarantee actually confer?"} is important to develop sufficiently private algorithms. 
%\emph{``what kinds of privacy leakage attacks, and to what extent, does an $\epsilon$-DP guarantee actually protect against?''} 


The objective here is to understand the \emph{semantics} of differential privacy, i.e., measuring the level of protection offered by differentially private learners against information leakage, often specified by the attack class. For Membership Inference Attacks (MIAs), wherein, the adversary attempts to infer the membership status of a target individual in the training dataset, prior work  \cite{Yeom2018} has demonstrated that DP algorithms ensure that the adversary cannot perform much better than random guessing for values of \(\epsilon\leq 0.4 \). Results of this form, that specify an explicit range of permissible values for the privacy parameter are crucial in reconciling the gap between theoretical understanding of privacy guarantees and an effective validation of privacy claims made in practice. For instance, citing privacy-utility tradeoffs, most practical deployments of $\epsilon$-DP algorithms consider $\epsilon \in [2, 4]$ to be sufficiently private \cite{smartnoise-pdf,apple-privacy}. In contrast, prior work demonstrates that such algorithmic setups are highly susceptible to MIAs. In particular, for \(\epsilon \geq 2\), one can design adversaries that can detect the presence of a sensitive record in the training dataset of any \(\epsilon\)-DP algorithm, with a success rate $\geq 88\%$, thereby rendering the promise of data protection effectively meaningless \cite{Humphries2020}. Naturally, such theoretical analyses that translate privacy guarantees of an algorithm into lower bounds on the effectiveness of a certain class of attacks find practical value in empirical auditing of privacy preserving algorithms \cite{gen-framework-auditin, ullman-how-private-is-sgd}. In particular, equipped with such a lower bound, the task of auditing whether an algorithm satisfies a claimed DP guarantee reduces to the problem of designing an adversarial attack with a success rate that nearly matches the lower bound implied by the purported privacy guarantee. 

%Results of this form, that translate privacy guarantees of an algorithm into lower bounds on the effectiveness of a certain class of attacks are crucial in reconciling the gap between theoretical understanding of privacy guarantees and an effective validation of privacy claims made in practice.

%Results of this form that provide an explicit range of permissible values for the privacy parameter $\epsilon$ are very useful, since they pro
%This line of work attempts to understand the \emph{semantics} of differential privacy, i.e., quantifying the level of protection offered by differentially private learners against a specific class of attacks. For Membership Inference Attacks (MIAs), wherein, the adversary attempts to infer the membership status of a target individual in the training dataset, prior work  \cite{Yeom2018} has demonstrated that DP algorithms ensure that the adversary cannot perform much better than random guessing for values of \(\epsilon\leq 0.4 \). Results of this form that obtain upper bounds on the privacy parameter as a function of the success probability of an attack are very useful, since they provide concrete evidence of the protection offered by an algorithm. In addition, such results aid in reconciling the gap between theoretical understanding of privacy guarantees and an effective validation of privacy claims made in practice. For instance, citing privacy-utility tradeoffs, most practical deployments of $\epsilon$-DP algorithms consider $\epsilon \in [2, 4]$ to be sufficiently private \cite{smartnoise-pdf,apple-privacy}. In contrast, prior work shows that such algorithmic setups are highly susceptible to MIAs. In particular, for \(\epsilon \geq 2\), one can design adversaries that can detect the presence of a sensitive record in the training dataset of any \(\epsilon\)-DP algorithm, with a success rate $\geq88\%$, thereby rendering the promise of data protection effectively meaningless \cite{Guo2022}. Results that translate DP-guarantees of an algorithm into lower bounds on the effectiveness of certain classes of attacks also find practical value in empirical auditing of DP \cite{gen-framework-auditin, ullman-how-private-is-sgd}. In particular, equipped with such a lower bound, the task of auditing whether an algorithm satisfies a claimed DP guarantee reduces to the problem of designing an adversarial attack with a success rate that nearly matches the lower bound implied by the purported privacy guarantee. 
While substantial research exists on the protection offered by private algorithms against membership inference, it may be deemed as a minimal, but not sufficient requirement for privacy, especially in settings where the membership status itself is not sensitive. For example, an individual's presence in a public network is not sensitive information. However, if an adversary is able to determine the exact location of the individual, their privacy is at risk. In machine learning, this would be an instance of a Data Reconstruction Attack (DRA), wherein, the adversary attempts to reconstruct an instance of the training dataset through the output of a learner. Notions such as differential privacy should, in theory, protect against such adversaries. Empirically, prior work \cite{Balle2022} has established that this hypothesis is indeed true. In this paper, we study the theoretical underpinnings of a private algorithm's resilience to reconstruction adversaries.

\subsection{Contributions}\label{subsec:contributions}
Our work aims to understand the data protection offered by private learners against Data Reconstruction Attacks (DRAs) by means of an information-theoretic analysis. Our contributions are summarized as follows.

\paragraph{Non-asymptotic lower bounds for general privacy accountants}

We present the first non-asymptotic analysis of the effectiveness of data reconstruction attacks against private algorithms. We do so by deriving minimax lower bounds on the reconstruction accuracy of \emph{any adversary} as a function of the privacy parameter of the private algorithm and the query complexity of the adversary. The non-asymptotic setting is of particular importance to randomization-based notions of privacy such as differential privacy, wherein, querying the algorithm on a fixed dataset yields varied outcomes. While the analyses of DRAs  by Guo et~al\cite{Guo2022} considers only the asymptotic setting and is restricted to relatively weaker notions of privacy (such as $(2, \epsilon)-$ R\'enyi DP) for large $\epsilon$, our non-asymptotic analysis is highly general and covers stronger notions of privacy such as pure DP and $(\alpha, \epsilon)-$ R\'enyi DP for any $\alpha \geq 1, \epsilon \geq 0$. In fact, our analysis goes beyond pure differential privacy and considers the notion of \emph{Metric Differential Privacy} or \emph{Lipschitz Privacy} \cite{Chatzikokolakis2013, Koufogiannis2015, Koufogiannis2016, Imola2022} -- a formalism that generalizes differential privacy by accounting for the underlying metric structure of the data beyond the Hamming metric.


%We present the first known analysis of privacy preservation against data reconstruction attacks in the non-asymptotic setting. This is of particular importance to randomization-based notions of privacy, such as differential privacy, where, querying the learner on a fixed dataset yields varied outcomes. Furthermore, while prior work has derived asymptotic lower bounds under restrictive assumptions on privacy\cite{Guo2022}, our analysis holds for more general privacy accountants such as $\epsilon$-

\paragraph{Improved dependence on privacy parameter and extension to high-dimensional regime}

Most modern applications of machine learning involve high dimensional datasets, where the data dimension is of the same order as (and sometimes larger than) the available number of samples. In such settings, most statistical algorithms exhibit a significant loss in performance, a behavior generally attributed to the \emph{curse of dimensionality} \cite{Bishop2006}. Thus, from the perspective of the adversary, we intuitively expect the task of training data reconstruction to be harder in the high dimensional regime, particularly when the query budget of the adversary is finite. While this intuition is confirmed by thorough empirical evaluation in Balle et~al\cite{Balle2022}, existing works on the theoretical analysis of training data reconstruction do not capture the inherent hardness of the problem in high dimensions. To this end, we focus on metric DP and extend our lower bound analysis to the high dimensional regime, obtaining lower bounds of the form $\frac{d}{n \epsilon^2}$ for a query budget of $n$. For metric DP, we observe that our lower bounds exhibit a quadratic drop in the reconstruction error with respect to the privacy parameter $\epsilon$, in both the classical regime as well as high dimensional regime, thereby obtaining a substantial improvement over the exponential drop observed in the lower bound analysis of (pure) differentially private learners. We accompany these theoretical findings with an extensive set of experiments to empirically evaluate the tightness of our bounds.

\paragraph{Metric DP accountant for privacy-preserving SGD} We complement our findings regarding the theoretical improvements conferred by metric differential privacy by demonstrating that most commonly used privacy-inducing mechanisms in the differential privacy literature can be easily extended to accommodate for mDP. To this end, we show that the privacy analysis of the Gaussian Mechanism, Differentially Private Stochastic Gradient Descent (DP-SGD) \cite{Abadi2016} and Projected Noisy Stochastic Gradient Descent (PN-SGD) \cite{Feldman2018} can be extended to metric DP. Furthermore, for PN-SGD, we demonstrate that the requisite noise level for metric DP is less than that of approximate DP by a factor of $diam(\mathcal{X})$, where $\mathcal{X}$ is the input data domain. 

%To the best of our knowledge, our work is the first to: 1) analyse minimax risk of a reconstruction adversary from a non-asymptotic viewpoint, 2) against pure differentially private learners and (3) generalized differentially private learners in (4) the classical, as well as, high-dimensional regime. Furthermore, we prove that the lower bounds obtained in prior work \cite{Guo2022} is invalid for any $\epsilon < $

%Furthermore, we extend the analysis for Lipschitz private learners to the high-dimensional regime and   

%Generalized Differential Privacy in Practice

%\paragraph{2. Improved Dependence on Privacy Parameter and Extension to High-dimensional Regime}
%We observe a quadratic drop in reconstruction error as a function of the privacy parameter \(\epsilon\). This is a substantial improvement over the inverse exponential relation \(\Theta(1/(e^{\epsilon}-1))\) obtained in prior work. Furthermore, while it is empirically observed that reconstruction is harder when the training dataset is high dimensional, a theoretical understanding of this phenomenon is lacking in literature. To fill this gap, we extend our analysis to the high-dimensional regime.

%\paragraph{3. Extending Privacy Analysis of Differentially Private Noisy SGD to Lipschitz privacy}
%Lipschitz privacy is a generalization of differential privacy to metric spaces
%Data Reconstruction Attacks, wherein, the adversary attempts to reconstruct an instance of the training dataset, is an instance of blatant non-privacy \cite{Dwork2017}, where the adversary is able to reconstruct a candidate dataset that agrees with the real training dataset on all but \(on\) entries. Notions such as differential privacy should, in theory, protect against blatant non-privacy. Empirically, prior work \cite{Balle2022} has established that this hypothesis is indeed true. In this paper, we study the theoretical underpinnings of a private algorithm's resilience to reconstruction adversaries.
\section{Technical Preliminaries}\label{sec:prelims}
%In this section, we present the background pertinent to privacy notions, the threat model, and error metrics to assess the adversary's computations.
In this section, we present the relevant background on various notions of privacy and some useful properties of statistical divergence measures. 
\subsection{Differential Privacy}
\begin{definition}[(Pure) Differential Privacy \cite{Dwork2017}]
\label{defn:pure-dp}
    For any \(\epsilon \geq 0\), a randomized learner \(\learner:\dataspace^{n}\rightarrow\labelspace\) is \(\epsilon\)-differentially private(DP) if, for every pair of neighboring datasets \(\dataset, \dataset' \in \dataspace\) that differ in a single training sample, the following is satisfied:
    \begin{equation}
        \forall T \subseteq \labelspace, \; \prob[\learner(\dataset)\in T]\leq e^{\epsilon}\prob[\learner(\dataset')\in T].
    \end{equation}
\end{definition}
%If the above holds with probability at least \(1-\delta\) for some \(\delta\in[0,1]\), the learner is \((\epsilon, \delta)-\) DP.
The above definition imposes a strict restriction on the probability mass between the output distributions \(\learner(\dataset), \learner(\dataset')\). Relaxing this constraint, if we allow \(\delta\) fraction of the mass (for some $\delta \geq 0$) to be located outside of the curve between these output distributions, we obtain the following definition of approximate differential privacy. 
\begin{definition}[(Approximate) Differential Privacy]
%\prateeti{ADP is different from the probabilistic approximate DP (that the above holds with prob at least 1-\(\delta\)). We can cite this paper and show one of their curves to highlight that \(delta\) has no effect on the ratio of probabilities: https://eprint.iacr.org/2018/277.pdf for ADP}
\label{defn:adp}
For any \(\epsilon\geq0, \delta\geq0\), a randomized learner \(\learner:\dataspace^{n}\rightarrow\labelspace\) is \((\epsilon, \delta)\)-(approximate) differentially private (ADP) if, for every pair of neighboring datasets \(\dataset, \dataset' \in \dataspace\) that differ in a single training sample, the following inequality holds:
    \begin{equation}
        \forall T \subseteq \labelspace, \; \prob[\learner(\dataset)\in T]\leq e^{\epsilon}\prob[\learner(\dataset')\in T] + \delta.\footnote{Setting \(\epsilon=0\), this implies an upper bound on the total variation between the output distributions \(\|P-Q\|_{TV}\leq \delta\), where \(P\) and \(Q\) are defined as in \ref{defn:rdp}}
    \end{equation}
\end{definition}
In essence, for any pair of input datasets that differ in a single data point, the output distributions of the learner (as a function of the input dataset) must be sufficiently \emph{statistically indistinguishable}. To this end, there exist several relaxations of differential privacy in the literature that consider varying notions of statistical indistinguishability, a popular relaxation being R\'enyi Differential Privacy.
\begin{definition}[R\'enyi Differential Privacy \cite{Mironov2017}]
\label{defn:rdp}
    For any \(\epsilon \geq 0\) and \(\alpha \in [1,\infty]\), a randomized learner \(\learner:\dataspace^{n}\rightarrow\labelspace\) is \((\alpha, \epsilon)\)-R\'enyi differentially private(RDP) if, for every pair of neighboring datasets \(\dataset, \dataset' \in \dataspace\) that differ in a single training sample, the R\'enyi Divergence is bounded by \(\epsilon\):
    \begin{equation}
    \label{eqn:rdp-def}
        D_{\alpha}(P||Q) = \frac{1}{\alpha - 1}\log\expctn_{x\sim Q}\left[\left(\frac{P(x)}{Q(x)}\right)^{\alpha}\right]\leq\epsilon
    \end{equation}
    %\satya{Corrected by the big parenthesis before powering to $\alpha$.}
    where \(P\) and \(Q\) denote the output distributions \(\learner(\dataset)\) and \(\learner(\dataset')\), respectively.  The case of $\alpha = 1$ and $\alpha = \infty$ are defined by taking the appropriate limits on the LHS of \eqref{eqn:rdp-def}
\end{definition} 
\begin{comment}
        \begin{equation}
        \expctn_{o\sim\learner(\dataset')}\left[\frac{\prob[\learner(\dataset) = o]}{\prob[\learner(\dataset') = o]}\right] \leq e^{(\alpha-1)\epsilon}
    \end{equation}
\end{comment}
%In essence, if \(\dataset'\) is defined as \(\dataset':=\dataset\setminus\{z\}\), where \(z = (x,y)\) is a single training sample defined by a \(d\)-dimensional feature vector \(x\in\reals^{d}\) and a label \(y\in\reals\), one should not be able to infer (almost) anything about this individual sample from the output of the learner \(\learner\). Notice that the output distribution of \(\learner\) is parameterized by its input dataset \dataset\  or \dataset', whichever is relevant.
A common paradigm in releasing statistical information with differential privacy is to generate a noisy estimate of the true statistic. Namely, if $f: \dataspace\to\reals^{d}$ is a real-valued function\footnote{The restriction to real-valued functions is not essential.}, $\learner(\dataset) = f(\dataset) + \eta$ is differentially private for an appropriate noise level $\eta$. The magnitude of perturbation is generally calibrated according to the sensitivity of the function $f$, defined as follows:
%We also note that mechanisms in differential privacy generally require the following sensitivity constraint:
\begin{definition}[Sensitivity]
\label{defn:global-sensitivity}
    Given any function \(f:\dataspace\rightarrow\reals^{d}\),
 \begin{equation}
     \Delta_{f} = \underset{\dataset,\dataset'\in\dataspace ; \|\dataset-\dataset'\|\leq 1}{\max}\|f(\dataset)-f(\dataset')\|
 \end{equation}
\end{definition}
%Note that the sensitivity constraint is over all neighboring datasets \(\dataset, \dataset'\in\dataspace\).
The Gaussian mechanism, for instance, requires that $f(\dataset)$ be perturbed with noise drawn from the Gaussian distribution, as follows:
\begin{definition}[Gaussian Mechanism]
\label{defn:dp-gaussian-mech}
    Given any function \(f:\dataspace\rightarrow\reals^{d}\), the Gaussian mechanism is defined by
 \begin{equation}
     \learner(\dataset) = f(\dataset) + \mathcal{N}(0, \Delta_{f}^{2}\sigma^{2})
 \end{equation}
 For $\epsilon < 1$ and $c^{2} > 2\ln(1.25/\delta)$, the Gaussian mechanism with parameter $\sigma \geq c\Delta_{f}/\epsilon$ satisfies $(\epsilon, \delta)$ approximate DP \cite{Dwork2017}.
\end{definition}










It is apparent from the above definitions that differential privacy does not account for the underlying metric structure in the data domain (beyond the discrete / Hamming metric), and is entirely dependent on the \emph{adjacency} relation between datasets. This results in several restrictions, such as requiring every row to represent an individual or a single unit of protection. As such, this notion is quite inflexible, and is unable to handle cases where each row belongs to some arbitrary domain of secrets, where there is no natural definition of \emph{neighbors}, but there exists a notion of \emph{distinguishability} of secrets. For any meaningful notion of privacy, if two secrets are statistically indistinguishable, they must remain so in the output of the randomized learner. Furthermore, privacy claims in most learning algorithms are achieved by distorting non-private computations via the addition of noise scaled in accordance to the nature of the computation. Differential privacy requires that the same level of noise be added to every feature corresponding to an individual in a statistical database, regardless of the security risk associated with the feature. Often, however, certain features are more sensitive than the rest. For example, a person's residential address is significantly more sensitive than their date of joining a social media platform, and should therefore be treated differently for fine-grained privacy. These shortcomings are addressed in a generalization of differential privacy, termed Metric Differential Privacy (mDP). 
%\prateeti{TODO: check correctness of the feature-based privacy claim}

\subsubsection{Metric Differential Privacy}\label{sec:mDP}
Consider an arbitrary collection of secrets \(\dataset\), with samples drawn from a metric space \((\dataspace, \rho)\), wherein the metric  \(\rho: \dataspace\times\dataspace\rightarrow [0,\infty]\) measures the dissimilarity between any two points in \(\dataspace\). In practical applications, \(\dataset\) is modelled as an aggregation of \(N\) samples \(\{z_{1}, z_{2}, ..., z_{N}\}\), where each sample \(z_{i}\in \reals^{d+1}\) is a high-dimensional vector, comprising of \((x,y)\) pairs, where \(x\in \mathcal{S}_x \subset \reals^{d}\) is a \(d-\)dimensional feature vector and \(y \in \mathcal{S}_y \subset \reals\) is the corresponding label. The measure of dissimilarity for any two such collections \(\dataset, \dataset'\in\dataspace\) is summed over all samples, i.e., \(\rho(\dataset, \dataset') = \sum_{i=1}^{N}\rho(z_{i},z_{i}')\). % \satya{Above, $\labelspace$ is used to refer to \emph{models} produced by the learning algorithm.} 

%In practice, the domain \(\featurespace = \reals^{n}\) is equipped with the \(l_{2}\) norm
%We operate in the setting where the data is an arbitrary collection of secrets from a finite vector space \(\featurespace\), equipped with a norm \(\|.\|\)

%with samples \(\{x_{i}\}_{i=1}^{n}\in\dataspace\), sampled according to some distribution \(P_{\dataspace}\). The data domain \(\dataspace\) is equipped with a metric \(\rho:\dataspace \times \dataspace \rightarrow [0,\infty]\) that measures the dissimilarity between any two points in \(\dataspace\). 
%Consider a dataset \(\dataset\) with n samples \(\{z_{i}\}_{i=1}^{n}\in\dataspace\), sampled according to some distribution \(P_{\dataspace}\). We assume that the samples come from a metric space \((\dataspace, d_{\dataspace})\), where the metric \(d_{\dataspace}:\dataspace \times \dataspace \rightarrow \reals\) is a measure of the dissimilarity between any two points in the dataspace \(\dataspace\). 

\begin{definition}[Metric Differential Privacy \cite{Chatzikokolakis2013, Koufogiannis2016}]\label{defn:mDP}
 Given a metric space \((\dataspace, \rho)\), for any \(\epsilon_{L} \geq 0\), a randomized learner \(\learner: \dataspace \rightarrow\labelspace\) is \(\epsilon_{L}-\)metric differentially private (mDP) if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\) and \(\forall T\subseteq\labelspace\),
    \begin{equation}
    \prob[\learner(\dataset)\in T]\leq e^{\epsilon_{L}\rho(\dataset, \dataset')}\prob[\learner(\dataset')\in T]
        %\footnote{Note that DP is a special case of lipschitz-DP in the Hamming metric, for \(\rho_{H}(x,x')=1\).}
    \end{equation}
    Or, equivalently, if the log-probability function of the corresponding output distributions is \(\epsilon_{L}-\)Lipschitz
    \begin{equation}
        |\ln{\prob[\learner(\dataset)\in T]} - \ln{\prob[\learner(\dataset')\in T]}|\leq\epsilon_{L} \rho(\dataset,\dataset')
    \end{equation}
    
\end{definition}
\begin{remark}
Note that any \(\epsilon-\)mDP learner is \(t\epsilon-\)DP whenever $\rho(\dataset, \dataset') \leq t$. Setting $\rho$ to be the Hamming metric and $t = 1$ naturally recovers Definition \ref{defn:pure-dp}. Thus, metric differential privacy is a strict generalization of differential privacy. Similar to traditional differential privacy, metric differential privacy is preserved under post-processing and composition\cite{Koufogiannis2016}.
\end{remark}

\subsection{Upper Bounds on Statistical Divergence}

The privacy definitions listed above impose a constraint on the extent to which a model trained on \(\dataset\) can be dissimilar from a model trained on \(\dataset'\), given that \(\dataset,\dataset'\) belong to the same domain. Naturally, information theoretic interpretations of these definitions would involve measuring the statistical divergence between the output distributions of the randomized learner \(\learner\) when trained on two different datasets $\dataset$ and $\dataset'$. To this end, we quantify the implications of \(\epsilon-\)DP and \(\epsilon_{L}-\)mDP on two \(f-\)divergence measures that are extensively used in DP literature, namely, the KL divergence and the R\'enyi divergence. 
\begin{lemma}[KL Upper Bounds for DP and mDP\cite{Bun2016}]
\label{lemma:kl}
    For any \(\epsilon\)-DP learner \(\learner\), trained on neighboring datasets \(\dataset, \dataset'\), the Kullback-Leibler(KL) divergence between the output distributions satisfies the following inequality:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset')) &\leq\epsilon\DPhyperbolic\\
        & \approx \min\left\{\epsilon, \frac{\epsilon^{2}}{2}\right\}
    \end{split}
    \end{equation}
    If the learner is \(\epsilon_{L}\)-mDP, the KL-divergence is bounded above as follows:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset'))& \leq\epsilon_{L}\rho(\dataset, \dataset')\MDPhyperbolic\\
        &\approx \min\left\{\epsilon_{L}\rho(\dataset, \dataset'), \frac{\epsilon_{L}^{2}\rho(\dataset, \dataset')^{2}}{2}\right\}
    \end{split} 
    \end{equation}
\end{lemma}
\begin{proof}
    See Appendix \ref{appendix-l1}
\end{proof}
\begin{lemma}[R\'enyi Upper Bounds for DP and mDP]
\label{lemma:rd}
For any \(\epsilon\)-DP learner \learner, trained on neighboring datasets \(\dataset,\dataset'\), the R\'enyi divergence between the output distributions satisfies the following inequality for $\alpha \in (1, \infty)$:
    \begin{equation}
        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon, \frac{3\alpha\epsilon^{2}}{2}\right\}
    \end{equation}
    If the learner is \(\epsilon_{L}-\)mDP, the Re\'nyi-divergence is bounded above as follows:
    \begin{equation}
        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon_{L}\rho(\dataset,\dataset'), \frac{3\alpha\epsilon_{L}^{2}\rho(\dataset,\dataset')^{2}}{2}\right\}
    \end{equation}
    \begin{proof}
        See Appendix \ref{appendix-l2}
    \end{proof}
\end{lemma}

\section{Problem Formulation}
\label{sec:problem-formulation}
Consider a dataset \(\advData = \{z_{1}, ..., z_{N-1}\}\), where each \(z_{i} = (x_{i}, y_{i})\) is a random variable, representing the \(i\)-th row in \(\advData\), drawn according to some distribution on the alphabet \(\dataspace\). % Without loss of generality, we will assume that the domain \(\dataspace = \reals^{(n-1)\times(d+1)}\), and is equipped with the \(l_{2}\) norm.
 Let \(\challenge \in \dataspace\) be an arbitrary sample chosen by the learner, which we will refer to as the challenge. The training dataset \(\datatrain = \advData\cup\{\challenge\}\) is used by the randomized learner \(\learner\) to train the parameters of the model \(\model\leftarrow\learner(\datatrain)\). 
%It is generally assumed that both \(\learner\) and \(\model\) are released to the public \cite{Balle2022}.
%For the case of unbiased adversaries, the attack algorithm \(\mathcal{A}\) has access to the trained model \(h\) and \(\mathcal{D}= \mathcal{D}_{train} \setminus \{z\}\), and outputs a reconstruction \(\hat{z}\leftarrow\mathcal{A}(h, \mathcal{D})\) of the target sample \(z\).

An adversary \(\adv\) then attempts to reconstruct the challenge data point \(\challenge\) with access to the following:
%\paragraph{Fixed dataset}\(\advData\), i.e., all samples in the training set, excluding \(\challenge\)
%\paragraph{Released model}\(\model\), trained on \(\advData\cup\challenge\)
%\paragraph{Training algorithm}\(\learner\)
%\paragraph{Query budget}\(m\), that specifies the number of queries the adversary can make to the learner (and corresponding model) 
%\begin{comment}
\begin{itemize}
    \item The fixed dataset \(\advData\), i.e., $N-1$ samples in the training set, excluding \(\challenge\)
    \item Samples from the output distribution $\learner(\datatrain)$
    %\item The training algorithm \(\learner\)
    %\item A fixed query budget \(m\) that specifies the number of queries the adversary can make to the learner
\end{itemize}
%\end{comment}

In addition to the above, an \emph{informed} adversary has access to some side knowledge \texttt{aux} about \(\challenge\), possibly acquired from public datasets, via web-scraping, or deduced from the nature of \(\learner\) and \(\advData\). As suggested in Balle et~al.\cite{Balle2022}, this side knowledge could be modeled as a collection of \(k\) \textit{shadow} targets \texttt{aux}\(:=\{\Tilde{z_{i}}|i\in[k], \Tilde{z_{i}}\in\dataspace\}\), not contained within the fixed dataset \(\advData\). We assume that the adversary has a finite query budget $m$ that specifies the number of queries the adversary can make to the learner. 

Given the above threat model, it is clear that the only piece of information obscured from the adversary is the challenge \(\challenge\). Thus, the adversary's goal can be reduced to a \emph{parameter estimation problem}, wherein, the distribution \(\learner(\datatrain)\) is parameterized by \(\challenge\). Let \(\modelDist\) denote the class of distributions induced by the output of the randomized learner \(\learner\), i.e., \(\modelDist = \{\learner(\advData\cup\{z\})|z\in\dataspace\}\), supported on some space of models \(\mathcal{H}\). For each \(i\in [k]\), the adversary trains the model \(\Tilde{h_{i}}=\learner(\advData\cup\{\Tilde{z_{i}}\})\) on the fixed dataset \(\advData\) with the \(i\)-th shadow target in their collection of \(k\) shadow targets. We refer to this shadow model-target pair as \(\shadow := \{(\Tilde{h_{i}}, \Tilde{z_{i}})\}_{i=1}^{k}\), which serves as the attack training dataset for the attack model \(\phi\leftarrow\adv(\shadow)\). In doing so, the adversary exhausts a fraction of the initial query budget \(m\). With the remaining \(n = m-k\) query budget, she draws $\{h_{1}, ..., h_{n}\}$ i.i.d. samples from the distribution \(P_{z}\in\modelDist\), which serve as the validation set used to generate the estimate \(\zhat\) in \eqref{eq:zhat}. 
%\satya{We should use $\mathcal{H}$ for range of learning algos in Defns 3.2, 3.3.,3.4, etc.,}
\begin{equation}\label{eq:zhat}
    \zhat = \frac{1}{n}\sum_{i=1}^{n}\phi(h_{i}),
\end{equation}
%\satya{where $h_i$ is the model produced by the $i$-th run of ? on ? . Each $\phi(h_{i})$ produces a candidate $\hat{z} \in \mathbb{R}^{d+1}$. Is the average above coordinate-wise average? Even for DP? }
The above formulation may be viewed as a privacy game, wherein, the learner (or the defender) and the adversary play against each other.

\section{Reconstruction Metric}\label{sec:metric}

Throughout this work, we measure the quality of the adversary's reconstruction of the challenge sample in terms of the minimax risk. 
%Informally, the minimax risk is a measure of the smallest risk attainable by any reconstruction adversary.

%\prateeti{TODO: Eucledian metric, just mention MSE and say it can be extended to other metrics}
Without loss of generality, we will assume that the data domain \(\dataspace^{N} = \reals^{N\times(d+1)}\) is equipped with the \(\ell_{2}\) norm and define the risk as the standard Mean Squared Error(MSE)\footnote{While this is a simplification for consistency with prior work, our techniques are general enough for extension to arbitrary metric spaces.}. Although the challenge is a fixed sample, \(\zhat\) is a random variable due to the randomness of \(\model\), which in turn implies that \(\|\challenge - \zhat\|_{2}^{2}\) is also a random variable. To obtain a deterministic quantity as a measure of the risk of an estimator, we take the expectation over the randomness of \(\distY\in\modelDist\).
%We first define a metric \(\metric\) on the dataspace \(\dataspace\) to measure how far \(\zhat\) deviates from \(\challenge\), and a non-decreasing function \(\func\) applied on this metric, such that \(\phi(0)=0\). For simplicity (and to remain consistent with prior work \cite{Guo2022}), we set \(\rho(\challenge, \zhat) = |\challenge-\zhat|\), and \(\phi(t) = t^{2}\), i.e., the risk is the mean-squared error. 
\begin{definition}[Maximum Risk]
    The maximum risk of the estimator \(\zhat\) in \eqref{eq:zhat} is obtained by choosing a challenge \(\challenge\) that results in the worst candidate reconstruction, as follows:
    %The maximum risk of an estimator \(\zhat\), attempting to reconstruct \(\challenge\), with \(m-k\) samples drawn \(i.i.d.\) from \(\distY\), is defined by
    \begin{equation}
        \mathcal{R}_{max}(\zhat) = \sup_{\distY \in \modelDist}\error
    \end{equation}
\end{definition}
%\satya{The expectation in $\error$ is over randomness used by training algorithm to produce $h$, namely $h_i$'s?. The parametrized distribution $P$ on models is really $P_z$?}
\begin{definition}[Minimax Risk]
\label{def:minimax-risk}
%The minimax risk of an estimator \(\hat{z}\), drawing k samples from the distribution \(P\) parameterized by \(z\) is defined by
An optimal estimator would minimize the maximum risk defined as follows:
\begin{equation}
    \minimax = \inf_{\hat{z}}\sup_{\distY\in \modelDist}\error
\end{equation}
where, the \(\sup\) is taken over distributions \(\distY\in\modelDist\) parameterized by \(\challenge\), and the \(\inf\) is over all estimators with \((m-k)\) samples drawn according to \(\distY\). 
%\satya{Shouldn't the $\inf$ be over all adversaries? Each adversary defines a $\phi$ and a $\hat{z}$, right?}
\end{definition}

This choice of metric captures the privacy game outlined in section \ref{sec:problem-formulation}. In essence, for each adversary, the learner finds a challenge point \(\challenge\) where the candidate reconstruction \(\zhat\) is farthest from \(\challenge\), as observed in $\sup_{P_{z}\in\modelDist}$. The estimators $\hat{z}$ are ranked in order of their worst-case performance and the adversary picks the optimal estimator that minimizes the worst-case risk, as suggested by the $\inf_{\hat{z}}$ term. Lower bounds on this quantity imply that no reconstruction adversary can achieve a risk lower than the minimax risk on every distribution in \(\modelDist\), parameterized by \(\challenge\).

%The adversaries are ranked in order of their worst-case performance, 
%This process is repeated for all adversaries, ranking them in order of their worst-case performance, and pick the optimal adversary that minimizes this risk. Lower bounds on this quantity imply that no reconstruction adversary with a query budget of \(m\) and prior knowledge of \(k\) shadow targets can achieve a risk lower than the minimax risk on every distribution in \(\modelDist\), parameterized by \(\challenge\).
\section{Main Results}
\label{sec:main-results}
In this section, we establish lower bounds on the resilience of any \(\epsilon-\)DP learner and \(\epsilon_{L}-\)mDP learner against adversaries of the form described in Section \ref{sec:problem-formulation}.
\subsection{Error bounds for DP learners}
\begin{theorem}[\(\epsilon-\)DP]
\label{thm:le-cam}
    If the learner \(\learner\) is \(\epsilon-\)differentially private, the minimax rate of error of a reconstruction adversary with $n$ samples from the output distribution is bounded as follows:
    \begin{equation}
        \minimax \geq C diam(\dataspace)^{2}e^{-n\epsilon \tanh\left(\frac{\epsilon}{2}\right)}
    \end{equation}
    where, $C$ is a universal constant and \(diam(\dataspace) = \sup_{z,z'\in\dataspace}\|z-z'\|_{2}\)
\end{theorem}
\begin{proof}
    We defer the full proof to Appendix \ref{appendix-t1}. 
\end{proof}
We note the following key implications of this result:
\paragraph{Query complexity} We discuss the dependence on the query complexity through the lens of an adversary. According to the threat model described in Section \ref{sec:problem-formulation}, the adversary's query budget is split between the creation of the attack training dataset ($k$) and the sampling process for estimation ($n$). One might argue that a large shadow training dataset implies better reconstruction, and that the natural choice would be to exhaust the entirety of the query budget in preparing the shadow training dataset, i.e., $n=1$. However, we note that preparing the shadow training dataset is expensive, since the adversary trains \(h_{i}\leftarrow\learner(\dataset\cup\{z_{i}\})\) for every shadow target in \texttt{aux}. Furthermore, in practice, the adversary obtains \texttt{aux} through arbitrary sources; it might simply not be possible for the adversary to obtain a large number of shadow targets. We observe that the reconstruction error drops quickly as the number of samples drawn from the output distribution $P_{z}$ is increased. This is expected, owing to the reduction to parameter estimation. Therefore, even in the absence of a large attack training set, it is possible to reduce reconstruction error by drawing multiple samples from \(P_{z}\).
%\satya{Assuming $m \geq k$? How does such an optimal separation look like? Why not choose $m=k$ so the exponential dependency disappears?} 

\paragraph{Extension to R\'enyi DP} A key step in our proof of Theorem \ref{thm:le-cam} is to bound the KL-divergence betwen the two output distributions of the $\epsilon$-DP algorithm by applying Lemma \ref{lemma:kl}. To this end, we highlight that our analysis can be straightforwardly extended to $(\alpha, \epsilon)$ R\'enyi DP for any $\alpha \geq 1$ by using the comparison inequality $KL(P||Q) \leq D_{\alpha}(P||Q)$ However, as is evident by comparing Lemma \ref{lemma:kl} and \ref{lemma:rd}, the obtained result would be the same as Theorem \ref{thm:le-cam} modulo some constant factors. In particular, for \(\alpha = 2\), the upper bound on \(D_{KL}(\learner(\dataset)||\learner(\dataset'))\) coincides with the upper bound on \(D_{2}(\learner(\dataset)||\learner(\dataset'))\) for \(\epsilon \geq 1\), and varies by a constant multiplicative factor for \(\epsilon < 1\).  
%whereas, for \(\alpha < 1\), we are able to obtain tighter lower bounds, directly in terms of the R\'enyi divergence. 


\paragraph{Comparison with prior work} To the best of our knowledge, the result closest to Theorem \ref{thm:le-cam} that has appeared in prior work is Theorem 1 of Guo et~al.\cite{Guo2022}, which considers the special case of $(2, \epsilon)$ R\'enyi DP. For the sake of completeness, we restate their result and highlight its differences from Theorem \ref{thm:le-cam} as follows:
\begin{theorem}[Theorem 1 \cite{Guo2022}]\label{thm:guo-2022}
    If the learner \(\learner\) is \((2,\epsilon)-\)RDP, and the reconstruction attack outputs \(\hat{z}(h)\) upon observing \(h\leftarrow\learner(\dataset)\), then, for an unbiased estimator,  
    \begin{equation}
        \expctn[\|\hat{z(h)}-z\|^{2}_{2}] \geq C\frac{\sum_{i=1}^{d}diam_{i}(\dataspace)^{2}}{(e^{\epsilon}-1)}
    \end{equation}
    where \(diam_{i}(\dataspace) = \underset{z,z'\in\dataspace, z_{j} = z'_{j}\forall j\neq i}\sup|z_{i}-z_{i}'|\) is the i-th coordinate-wise diameter, and $C$ is a universal constant.
\end{theorem}
The differences between the above result of Guo et~al.\cite{Guo2022} and Theorem \ref{thm:le-cam} of this paper are listed below.
%While our bound exhibits a dependence on \(diam(\dataspace)^{2}\), the lower bound in \cite{Guo2022} requires a sum over the squared coordinate-wise diameter. This results in arbitrarily loose bounds, even with a bounded diameter assumption.
\begin{enumerate}
\item Upon investigating the validity of the lower bound admitted by Theorem 1 of Guo et~al.\cite{Guo2022}, we find that the lower bound is invalid for any \(\epsilon < \ln{(1 + \frac{d}{4})}\). Therefore, even for the low value of $d = 100$, Theorem \ref{thm:guo-2022} only holds for $\epsilon \geq 3.26$. On the contrary, prior work \cite{Humphries2020, Yeom2018} suggest that, even under $\epsilon$DP, if $\epsilon = 3.26$, an adversary can correctly predict the presence of an individual record through the output of the learner with a success probability of over 98\%, effectively reducing to non-privacy. Our result for $\epsilon$DP leads to non-vacuous bounds for small values of \(\epsilon\) and is tight (up to constant factors) in the high privacy regime (\(\epsilon = 0\)). The analysis is discussed in detail in Appendix \ref{appendix-compare}. 
%For small \(\epsilon\) 
\item Our result is non-asymptotic in the sense that it exhibits an explicit dependence on the query complexity of the adversary, which is absent in \ref{thm:guo-2022}.
%\item We illustrate the improvement conferred by our result in Figure \ref{fig:compare}.

\end{enumerate}

%Intuitively, their analysis assumes that the adversary's task of estimating a d\footnote{we replace \(d+1\) with \(d\) for consistency with prior work}-dimensional vector\(\challenge\) 
%This results in arbitrarily loose bounds, even in the case when \(diam(\dataspace)\) is bounded. For example, let \(\dataspace = \{z\in\reals^{d}|\|z\|\leq 1\}\) be the unit ball in \(\reals^{d}\). Here, Theorem \ref{thm:le-cam} admits a lower bound of \(\frac{1}{e^{(m-k)\epsilon \tanh\left(\frac{\epsilon}{2}\right)}}\), while the result from \cite{Guo2022} admits a lower bound of \(\frac{d}{e^{\epsilon}-1}\). 
%\paragraph{Dimension dependence} 
%\paragraph{Tightness} As we shall demonstrate in the appendix \ref{appendix-compare}, for the \(\epsilon=0\) regime, this result is tight up to constant factors. 
%\paragraph{Data dimensionality}
\paragraph{Bounded Domain Assumption} In line with prior work \cite{Yeom2018, Tramer2020, Guo2022}, our lower bounds for the protection offered by \(\epsilon-\)DP algorithms rely on the assumption that the domain \(\dataspace\) is bounded, i.e., \(diam(\dataspace)\) is finite. We observe that this assumption cannot be avoided in the analysis of traditional DP algorithms since the standard notion of differential privacy is oblivious to the underlying metric structure of the input space, yielding worst-case behavior. In the results that follow, we remove this boundedness assumption and derive minimax lower bounds for reconstruction attacks against \(\epsilon_{L}-\)mDP learners.

%As a consequence, $D_{KL}(\dataset_{-}\cup\{z\}||\dataset_{-}\cup\{z'\})\leq\epsilon$, regardless of the separation between $z,z'$, resulting in worst-case behaviour.
\subsection{Error bounds for mDP learners}
We first obtain lower bounds for reconstruction MSE in the classical regime, where the implicit assumption \(d \ll n\) holds.
\begin{theorem}[Low-dimensional Regime] 
\label{thm:mdp-lecam}
For any challenge \(\challenge\) in the training set of an \(\epsilon_{L}\)-mDP learner, the minimax rate of error for a reconstruction adversary with $n$ samples from the output distribution is bounded below as follows:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
        \minimax \geq \frac{1}{2ne\epsilon_{L}^{2}}. 
    \end{equation}
\end{theorem}
\begin{proof}
    See Appendix \ref{appendix-t2}.    
\end{proof}
A few key implications of this result are listed below:
\paragraph{Dependence on privacy parameter} Note that the lower bound in Theorem \ref{thm:mdp-lecam} exhibits an inverse quadratic dependence on the privacy parameter \(\epsilon_{L}\) as opposed to the inverse exponential drop observed in Theorem \ref{thm:le-cam}. We also observe that, in the limit \(\epsilon_{L}\rightarrow 0\), the lower bound in Theorem \ref{thm:mdp-lecam} grows rapidly. This, however, is not unusual, since there is no assumption on the boundedness of \(\dataspace\), and the MSE can be arbitrarily large. 

\paragraph{Data dimensionality} While Theorem \ref{thm:mdp-lecam} admits an improved dependence on \(\epsilon_{L}\), the bounds may be sub-optimal if the challenge \(\challenge\) is high dimensional. We notice that the data dimensionality is not captured in this result since our analysis holds for the classical case, where the implicit assumption is that \(d \ll
 n\). In practice, datasets often have a high-dimensional flavor and the influence of dimension is apparent in a variety of estimation problems. A very dramatic consequence of dimension is observed in regression problems, where the difficulty in minimizing the MSE is extremely difficult in higher dimensions as a result of the \emph{curse of dimensionality} \cite{Bishop2006, Wainwright2019}. Since training data reconstruction is, in essence, a form of parameter estimation, it is important to investigate non-asymptotic lower bounds on the MSE of reconstruction in the high-dimensional regime. Note that, in this analysis, the adversary is the entity solving the parameter estimation problem, operating under a restricted query budget \(m\). From a practical point of view, it is quite likely that the number of samples drawn from the output distribution $n$ ($\leq m$) is less than the input dimension $d$. To this end, we present the following result for training data reconstruction when the data dimension is in the order of (or larger than) the number of samples. 
 
 %in the attack training set \(k\) is less than the number of dimensions \(d\), since it is computationally expensive to prepare the attack training set \(\shadow\). To this end, we present the following result for training data reconstruction when the data dimension is in the order of (or larger than) the number of samples.   
%Furthermore, for an adversary with a restricted query budget \(m\), it is possible that the number of attack training samples \(k\) is  
%Motivated by this, we present the following result for training data reconstruction when the data dimension is in the order of (or larger than) the number of samples.   
\begin{theorem}[High Dimensional Regime]
\label{thm:mdp-fanos}
For any challenge \(\challenge\) in the training set of an \(\epsilon_{L}\)-mDP learner, the minimax risk for a reconstruction adversary with $n$ samples is bounded below as follows:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
    \begin{split}
     %\minimax &\geq \Omega\left(\max\left\{\frac{d}{(m-k)\epsilon^{2}}, \frac{d^{2}}{(m-k)^{2}\epsilon^{2}}\right\}\right)
     \minimax \geq \Omega\left(\frac{d}{n\epsilon_{L}^{2}}\right),
    \end{split}
    \end{equation}
where the $\Omega$ hides an \emph{absolute constant} independent of $d, n,$ and $ \epsilon_{L}$.
\end{theorem}
\begin{proof}
    Refer to appendix \ref{appendix-t3}.    
\end{proof}
%\satya{These theorems look like the bounds are a function of $z$, both because it appears on the l.h.s. and because the theorems start with "Given a fixed $z$ ...". But they don't, right? The dependence of $z$ is taken care of by the sup over $P_z$ in the minimax definition.}
We interpret the result as follows:
\paragraph{Dimension dependence} The explicit dimension dependence exhibited in the lower bound of Theorem \ref{thm:mdp-fanos} complements the empirical observations of Balle et~al.\cite{Balle2022}, where, from the lens of an adversary, the problem of data reconstruction on a private learner is much harder in high dimensional settings, given that each challenge sample \(\challenge\) has \(d-\) degrees of freedom. Furthermore, we note that the coordinate-wise diameter $\sum_{i}^{d}diam_{i}(\dataspace)^{2}$ is typically $O(d)$. Therefore, for $\epsilon = \epsilon_{L}, n = 1$ this result is quantitatively better than that of prior work \cite{Guo2022} since $\frac{d}{t^{2}}\geq \frac{d}{e^{t}-1}$ for $t>0$. 
%In particular, for a fixed value of \(\epsilon\), we investigate the effect of the ratio \(d/n\) as it converges to a strictly positive value \(\alpha > 0\), and compare the theoretical lower bounds in Theorem \ref{thm:lp-fanos} and \ref{thm:lp-lecam} against empirical observations in prior work \cite{Balle2022} for a fixed privacy parameter \(\epsilon\). \satya{Where are these investigations?} 
\paragraph{Reduction to Theorem \ref{thm:mdp-lecam}} Theorem \ref{thm:mdp-fanos} can be interpreted as a generalization of Theorem \ref{thm:mdp-lecam} for the high-dimensional regime since it recovers Theorem \ref{thm:mdp-lecam} for $d = O(1)$.
%\satya{ $O(1)$? But we only assumed $d < n$ before the theorem.} 
%\paragraph{High-dimensional asymptotics}
%\paragraph{Extension to \(\epsilon-\)DP learners}
%Recall that every \(\epsilon-\)Lipschitz private learner is \(t\epsilon-\)differentially private, for the adjacency relation \(\|\dataset - \dataset'\|_{1}\leq t\). 
%Note that for \(\epsilon<0.22\), the curve for the lower bound in Guo et~al. for \((2,\epsilon)\)-R\'enyi DP is higher than our result for pure DP in Theorem \ref{thm:le-cam}. This is because the lower bound in Guo et~al. is invalid for values of \(\epsilon\leq0.22\), as proved in Appendix \ref{appendix-compare}. For this illustration, we set \(diam(\dataspace) = diam_{i}(\dataspace) = 1\), \(d=4\), and \(m-k = 1\).}


It is apparent from these results that lower bounds on the minimax rate of reconstruction error for mDP learners are quantitatively superior to their DP counterparts. This is expected, since minimax lower bounds often require the construction of packing sets contained in the parameter space, and careful selection of the separation between the elements is crucial for the tightness of the bound. While metric differential privacy is, in theory, a more natural choice for resilience against reconstruction adversaries, it is important to understand the applicability of this notion in practice, particularly in the context of deep learning. In the following section, we complement our lower bound analysis by designing mDP accountants for Noisy Stochastic Gradient Descent(SGD) algorithms.
%It is apparent from the above results that an information-theoretic analysis of minimax risk of reconstruction is more intuitive for mDP learners. 
\section{Metric DP accountant for Noisy SGD}
We first establish the requisite framework for analyzing mDP in practice. We emphasize that, since mDP is a generalization of DP, practical applications would require a relaxation similar to \ref{defn:adp} and \ref{defn:rdp} in order to establish privacy guarantees. To this end, we consider the following relaxed versions of metric differential privacy. 
\begin{definition}[Approximate mDP \cite{Imola2022}]
\label{def:approximate-lipschitz-privacy}
Given a metric space \((\dataspace, \rho)\), for any \(\epsilon_{L}\geq 0, \delta\geq 0\), a randomized learner \(\learner: \dataspace \rightarrow\labelspace\) is \((\epsilon_{L}, \delta)-\)mDP if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\) and \(\forall T\subseteq\labelspace\),
\begin{equation}
    \prob[\learner(\dataset)\in T]\leq e^{\epsilon_{L}\rho(\dataset, \dataset')}\prob[\learner(\dataset')\in T] + \delta
\end{equation}
\end{definition}
\begin{definition}[R\'enyi mDP]
\label{def:renyi-lipschitz}
Given a metric space \((\dataspace, \rho)\), for any \(\epsilon_{L}\geq 0, \alpha\in[1,\infty]\), a randomized learner \(\learner: \dataspace^n \rightarrow\labelspace\) is \((\alpha, \epsilon_{L})-\)R\'enyi mDP if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\) the R\'enyi Divergence is bounded as follows
    \begin{equation}
        D_{\alpha}(P||Q) = \frac{1}{\alpha - 1}\log\expctn_{x\sim Q}\left[\left(\frac{P(x)}{Q(x)}\right)^{\alpha} \right]\leq\epsilon_{L}\rho(\dataset,\dataset')
    \end{equation}
    where \(P\) and \(Q\) denote the output distributions \(\learner(\dataset)\) and \(\learner(\dataset')\), respectively. 
\end{definition}
It is easy to see that Definition \ref{def:approximate-lipschitz-privacy} and Definition \ref{def:renyi-lipschitz} recover Approximate DP and Renyi DP respectively, when restricted to the Hamming metric, in a manner similar to how mDP generalizes pure DP. 

Recall that mechanisms in differential privacy generally require the sensitivity constraint in Definition \ref{defn:global-sensitivity} over all neighboring datasets \(\dataset, \dataset'\in\dataspace\). For metric differential privacy, we define a relaxed requirement which we call input lipschitzness. 
\begin{definition}[Input Lipschitzness]
 A function \(f:\dataspace\rightarrow\reals^{d}\) is $L_{input}$ input lipschitz if the following holds
    \begin{equation}
        \|f(\dataset)-f(\dataset')\|\leq L_{input}\rho(\dataset,\dataset')
    \end{equation}
\end{definition}
%Having set up the framework for analyzing mDP in practice, we now investigate the feasibility of extending popular mechanisms in differential privacy to accommodate for Lipschitz privacy. 
Equipped with this notion of sensitivity, we extend the Gaussian mechanism for DP \ref{defn:dp-gaussian-mech} to mDP, as follows:
\begin{proposition}[Gaussian Mechanism for mDP]
\label{prop:gaussian-lp}
    Consider any two datasets \(\dataset, \dataset' \in \dataspace\), at a distance \(\rho(\dataset,\dataset')\) and an input lipschitz function \(f:\dataspace\rightarrow\reals^{d}\). For any \(\delta \in (0,1)\) and \(c^{2}>\ln(1.25/\delta)\), the Gaussian Mechanism with parameter \(\sigma \geq cL_{input}/\epsilon_{L}\) is \((\epsilon_L,\delta)-\)mDP, where  \(L_{input}\) is the input Lipchitz constant of \(f\).
\end{proposition}
\begin{proof}
From the definition of input Lipschitzness and sensitivity, it follows that 
\begin{equation}
    \Delta_{f} = \|f(\dataset)-f(\dataset')\|\leq L_{input}\rho(\dataset,\dataset')
\end{equation}
 Setting \(\epsilon = \epsilon_{L}\rho(\dataset, \dataset')\), we are required to bound the following quantity for \(f\) applied on datasets \(\dataset, \dataset'\):

 \begin{equation}
\left|\ln\frac{e^{\frac{-1}{2\sigma^{2}}\|x\|^{2}}}{e^{\frac{-1}{2\sigma^{2}}\|x + \Delta_{f}\|^{2}}}\right|\leq \epsilon = \epsilon_{L}\rho(\dataset, \dataset')
 \end{equation}
 where the numerator is the probability of observing a specific output \(f(\dataset)+x\) and the denominator is the probability of observing the same output when the input dataset is replaced by \(\dataset'\). The proof of our claim then follows from the proof of Gaussian Mechanism for \((\epsilon, \delta)\) differential privacy \cite{Dwork2017}. 
\end{proof}

%Given these relaxations, we are now equipped to extend the Gaussian noise mechanism in differential privacy to metric DP.
We are now ready to develop mDP accountants for privacy-preserving algorithms. A popular approach to differentially private deep learning involves an extension of Stochastic Gradient Descent (SGD)\cite{Abadi2016} to incorporate gradient clipping and noisy gradient updates. The resulting algorithm is called Differentially Private SGD (DP-SGD), wherein, $(\epsilon, \delta)$ differential privacy is guaranteed via an application of the Gaussian mechanism at each iteration. Since mDP is a generalization of traditional DP, we expect that iterative DP algorithms that rely on additive noise at each intermediate solution step are often inherently mDP, owing to the application of composition theorems that admit natural extensions to arbitrary metric spaces. For algorithms that rely on contractive noisy iterations without releasing the intermediate solutions, such as Projected Noisy Stochastic Gradient Descent \cite{Feldman2018}, we propose a privacy accounting strategy for metric differential privacy. 
%propose an mDP accountant that requires less noise that the original RDP accountant in Algorithm 1 of Feldman et.~al\cite{Feldman2018}. 
%privacy amplifications by withholding the result at intermediate steps
%additive-noise mechanisms at each iteration
%additive-noise mechanisms for privacy are often inherently mDP. We find that, for DP-SGD, this hypothesis is indeed true. For algorithms that rely on 

\paragraph{Metric Differential Privacy in DP-SGD}
In the analysis of DP-SGD \cite{Abadi2016}, privacy is attained via the addition of Gaussian distributed noise. Standard arguments on the composition of privacy mechanisms renders the differentially private variant of SGD \((q\epsilon, q\delta)-\)differentially private at each step, where \(q = L/N\) is the lot size. We note that, by replacing the global sensitivity assumption with input Lipschitzness, the Gaussian mechanism ensures \((\epsilon_{L}, \delta)\)-Lipschitz privacy for appropriately scaled noise, as in Proposition \ref{prop:gaussian-lp}. Thus, DP-SGD inherently incorporates Lipschitz privacy, with privacy accounting based on standard composition theorems \cite{Kasiviswanathan2008}.
\paragraph{Metric Differential Privacy in PN-SGD}
When analysing the privacy of learning algorithms that require iterative updates on an intermediate solution, it is common practice to ensure privacy at each iteration and argue about the cumulative loss of privacy via composition theorems. Another popular direction is the theoretical analysis of Noisy Stochastic Gradient Descent to formalize privacy amplifications under certain assumptions and obtain bounds on the degradation of privacy across iterations. We extend the analysis of one such algorithm, the Projected Noisy Stochastic Gradient Descent (PNSGD) \cite{Feldman2018}, restated in Algorithm \ref{alg:pnsgd}, and establish that the algorithm satisfies $(\alpha, \epsilon)$-R\'enyi mDP with a lower noise magnitude than that required for $(\alpha, \epsilon)$-R\'enyi DP.

\begin{algorithm}[tb]
   \caption{Projected Noisy Stochastic Gradient Descent (Algorithm 1 of Feldman et.~al.\cite{Feldman2018})}
   \label{alg:pnsgd}
\begin{algorithmic}
   \State {\bfseries Input:} Dataset $S = \{x_{1}, ..., x_{n}\},  f: \mathcal{K}\times\mathcal{X}\rightarrow \reals$, which is a convex function in the first parameter, learning rate $\eta$, starting point $w_{0}\in\mathcal{K}$, noise parameter $\sigma$
   \For{$t\in\{0, ..., n-1\}$}
   \State  $v_{t+1}\leftarrow w_{t} - \eta(\nabla_{w}f(w_{t}, x_{t+1})+Z)$, where $Z\sim\mathcal{N}(0,\sigma^{2}\mathbb{I}_{d})$.
   \State  $w_{t+1}\leftarrow\prod_{\mathcal{K}}(v_{t+1})$, where $\prod_{\mathcal{K}}(w)=\arg\min_{\theta\in\mathcal{K}}\|\theta-w\|_{2}$ is the $l_{2}$ projection on $\mathcal{K}$
   %\FOR{$i=1$ {\bfseries to} $m-1$}
   %\IF{$x_i > x_{i+1}$}
   %\STATE Swap $x_i$ and $x_{i+1}$
   \EndFor
   \State return the final iterate $w_{n}$ 
   %\ENDIF
   %\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{proposition}
\label{prop:pnsgd-mdp}
    Let $\cK \subset \mathbb{R}^d$ be a convex set and let $\{ f(., x) \}_{x \in cX}$ be a family of convex, $\beta$-smooth functions over $\cK$, where the gradients are $L_{input}$-input Lipschitz. Furthermore, assume $\cX$ is a bounded set. Then, for any $\eta \leq 2/\beta$ and $\alpha > 1$, initializing $w_0 \in \cK$ and dataset $S \in \cX^{n}$, PNSGD run with $\sigma^{2} \geq \frac{2 \alpha L^{2}_{input} diam(\cX)}{\epsilon_L (n- t + 1)}$ satisfies $(\epsilon_L, \alpha)$ metric differential privacy. 
\end{proposition}
\begin{proof}
    See Appendix \ref{appendix-pnsgd}
\end{proof}

\begin{claim}
Proposition \ref{prop:pnsgd-mdp} requires less perturbation for $(\epsilon_{L}, \alpha)$-R\'enyi mDP than that required for $(\epsilon, \alpha)$-R\'enyi DP. 
%in Feldman et.~al.\cite{Feldman2018}.
\end{claim}
\begin{proof}
    Proposition \ref{prop:pnsgd-mdp} establishes that, for convex and $\beta$-smooth functions with $L_{input}$-input Lipschitz gradients, PN-SGD run with a noise level $\sigma^2\geq\frac{2\alpha L^{2}_{input}diam(X)}{\epsilon_{L}(n-t+1)}$ satisfies $(\alpha, \epsilon_{L})$ R\'enyi mDP. On the contrary, Theorem 23 of Feldman et.~al.\cite{Feldman2018} suggests that for convex, $\beta$-smooth and $G$-Lipschitz functions, PN-SGD run with a noise level $\sigma^{2}\geq \frac{2\alpha G^{2}}{\epsilon (n-t+1)}$ satisfies $(\alpha, \epsilon)$ Renyi DP. We highlight that, for a large class of problems, the Lipschitz constant $G$ (which is essentially an upper bound on $\| \nabla_{w} f(w, x) \|_{2}$) is itself diameter dependent. For instance, consider the family of functions $f(w, x) = \frac{1}{2}\|w - x \|^{2}_{2}$ where $w, x \in X \subset R^d$ with $diam(X) < \infty$. This setup is practically relevant for statistical problems such as mean estimation \cite{Tsai2022}. Since $\nabla_{w} f(w, x) = w - x$, this class of functions is (a) $G$-Lipschitz with $G = diam(X)$, (b) $\beta$-smooth, and has $L_{input}$ input Lipschitz gradients with $\beta = L_{input} = 1$. Consequently, prior work \cite{Feldman2018} requires a noise level $\sigma^{2}\geq\frac{2\alpha diam(X)^{2}}{\epsilon (n-t+1)}$ for $(\alpha, \epsilon)$ R\'enyi DP whereas Proposition \ref{prop:pnsgd-mdp} requires a lower noise level of $\sigma^{2}\geq\frac{2\alpha diam(X)}{\epsilon (n-t+1)}$ for $(\alpha, \epsilon)$ R\'enyi mDP.
\end{proof}

\section{Empirical Analyses}

\begin{figure}\label{fig:lr}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/recons_mse_dp_final.pdf}
         \caption{DP (includes lower bound from prior work \cite{Guo2022})}
         \label{fig:lr-dp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/recons_mse_mdp_final.pdf}
         \caption{Metric DP}
         \label{fig:lr-mdp}
     \end{subfigure}
     \caption{Plot of empirical reconstruction MSE for $\epsilon\in[0.1,5)$ compared against theoretical lower bounds for (a) $\epsilon$-DP and (b) $\epsilon$-mDP learners. The solid lines represent the average over 50 runs and the shaded regions are 95\% confidence intervals. The y-axes are on the logarithmic scale.}
\end{figure}

We empirically evaluate the tightness of our bounds for differentially private and metric differentially private algorithms against the informed reconstruction adversary of Balle et.~al.\cite{Balle2022}. 

\paragraph{Training}
We reproduce the experiments of Guo et.~al.\cite{Guo2022} for private linear logistic regression. The regressor is tasked with binary MNIST \cite{Lecun1998} classification of digits 0 v/s 1. The training dataset comprises of $N = 12,665$ samples of dimension $d = 784$, in addition to a discrete label $y\in\{0,1\}$. Consistent with prior work \cite{Guo2022}, we treat the label as public information and only seek the reconstruct the image $x\in[0,1]^{784}$. 

\paragraph{Privacy Accounting}
We treat the privacy accounting strategy for differentially private and metric differentially private learners separately. For $\epsilon$-DP, the linear logistic regressor is trained privately via output perturbation \cite{Chaudhuri2008}, wherein, noise is drawn from the Laplace distribution with mean $0$ and $\sigma = 2/N\epsilon\lambda$. Since privacy is attained using the Laplace mechanism, the output perturbation technique can be adapted to arbitrary metrics, with appropriate scaling \cite{Chatzikokolakis2013}. Therefore, we use the scaling factor proposed by Chatzikokolakis et.~al.\cite{Chatzikokolakis2013} for the Euclidean metric to train the linear logistic regressor with $\epsilon$-mDP. We set $\lambda = 5 \times 10^{-4}$ and learning rate = $0.3$ for both models.

\paragraph{Reconstruction Adversary}
We implement the reconstruction attack proposed by Balle et.~al.\cite{Balle2022} for Generalized Linear Models (GLMs). In line with the experiments of Guo et.~al. \cite{Guo2022}, we further strengthen the attack by supplying the label $y$ of the challenge $z=(x,y)$ to the adversary, in addition to $D_{-}$. For ease of comparison, we set $n = 1$. 

\paragraph{Observations}
Under differential privacy, we plot the empirical reconstruction MSE against the lower bounds of Guo et.~al.\cite{Guo2022}(Theorem 1) and Theorem \ref{thm:le-cam} of this paper. We note that the input dimensionality of the MNIST dataset is $784$. Therefore, given that the lower bound of prior work is vacuous for any value of $\epsilon < \ln\left(1 + \frac{d}{4}\right)$, for $d = 784$, their lower bound violates trivial upper bounds for $\epsilon < 5.28$. This is apparent in Figure \ref{fig:lr-dp}, where the empirical upper bound falls bellow the lower bound of Guo et.~al. for $\epsilon < 5$. On the contrary, the lower bound in Theorem \ref{thm:le-cam} of this paper is tight in the high privacy regime (as evident in Figure \ref{fig:lr-dp}), but decays exponentially with the privacy parameter $\epsilon$. Under metric differential privacy, we observe from Figure \ref{fig:lr-mdp} that the lower bound of Theorem \ref{thm:mdp-fanos} lies strictly above that of Theorem \ref{thm:mdp-lecam}, and is highly indicative of the empirical MSE. This is expected, since the input data is high-dimensional. 

\section{Discussion and Future Directions}
In this work, we performed an information-theoretic analysis on the effectiveness of data reconstruction attacks on private learners, and derived non-asymptotic lower bounds on the reconstruction accuracy of an adversary as a function of their query budget and the privacy parameter of the model. Our analysis encompassed various notions of privacy, such as pure DP, R\'enyi DP, and metric DP. Our framework also captured the increased difficulty of the problem (from the perspective of an adversary) in the high dimensional setting, a phenomenon which was previously only noted empirically. Finally, we also established that (approximate) metric differential privacy guarantees can be easily ensured in noisy SGD.  Promising avenues of future work include examining the tightness of the obtained lower bounds for specific instances, and investigating, from the lens of an adversary, whether the problem of data reconstruction from a private model admits any fundamental statistical-computational tradeoffs.

\newpage
\section{Appendix}
\subsection{Preliminaries}
\begin{definition}[Packing set]
    A \(\delta\)-packing of the set \(\dataspace\) with respect to a metric \(\rho\) is a collection of elements \(\{z_{1}, ..., z_{M}\}\) such that, for all distinct \(i,j\in[M]\), we have \(\rho(z_{i}, z_{j}) \geq \delta\)
\end{definition}
\begin{lemma}[Varshamov-Gilbert]
\label{lemma:gilbert-v}
For d \(\geq\) 1, there exists a subset \(\mathcal{V}\) of the d-dimensional Hypercube \(\mathcal{H}_{d} = \{-1,1\}^{d}\) of cardinality \(|\mathcal{V}|\geq e^{d/8}\) such that the \(l_{1}\) distance 
\begin{equation}
    |v-v'|_{1} = 2\sum_{j=1}^{d}\mathbbm{1}\{v_{j}\neq v_{j}'\} \geq \frac{d}{2}
\end{equation}
    for all distinct \(v,v'\in\mathcal{V}\)
\end{lemma}
\begin{lemma}[Le Cam's Inequality]
\label{lemma:le-cam}
For a binary hypothesis testing problem with test \(\Psi:\mathcal{X}\rightarrow \{1,2\}\) that takes a sample \(X\), drawn according to either \(P_{0}\) or \(P_{1}\) defined on an arbitrary set \(\mathcal{X}\), 
    \begin{equation}
        \inf_{\Psi}{P_{0}(\Psi(X)\neq 0) + P_{1}(\Psi(X)\neq 1)} = 1 - \|P_{0}-P_{1}\|_{TV}
    \end{equation}
\end{lemma}
\begin{lemma}[Fano's Inequality]
\label{lemma:fanos}
%For an M-ary hypothesis testing problem with test \(\Psi: \mathcal{X}\rightarrow\{1,2,.., M\}\) that attempts to distinguish between \(M\) distributions \(\{P_{1}, P_{2}, ..., P_{M}\}\) on an arbitrary set \(\mathcal{X}\),
%Let \(\{z_{1}, ..., z_{M}\}\) be a \(2\delta\) packing in the \(\rho\) semi-metric. Then
Let \(V\) be a random variable drawn uniformly at random from the finite set \(\mathcal{V}\), where \(|\mathcal{V}|\geq 2\). For any Markov Chain \(V\rightarrow X \rightarrow \hat{V}\), 
\begin{equation}
    \prob[\hat{V}\neq V] \geq 1 - \frac{I(V; \hat{V} + \ln 2)}{\ln|\mathcal{V}|}
\end{equation}
In essence, for any hypothesis test \(\Psi: X \rightarrow V\), 
\begin{equation}
    \inf_{\Psi}[\Psi(X)\neq V] \geq 1 - \frac{I(V; \hat{V} + \ln 2)}{\ln|\mathcal{V}|}
\end{equation}
    
\end{lemma}
\subsection{Proof of Lemma \ref{lemma:kl}}\label{appendix-l1}
\begin{lemma}[Kullback-Leibler]
    For any \(\epsilon\)-DP learner \(\learner\), trained on neighboring datasets \(\dataset, \dataset'\), the Kullback-Leibler(KL) divergence between the output distributions satisfies the following inequality:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset')) &\leq\epsilon\DPhyperbolic\\
        & \approx \min\left\{\epsilon, \frac{\epsilon^{2}}{2}\right\}
    \end{split}
    \end{equation}
    If the learner is \(\epsilon_{L}\)-mDP, the KL-divergence is bounded above as follows:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset'))& \leq\epsilon_{L}\rho(\dataset, \dataset')\MDPhyperbolic\\
        &\approx \min\left\{\epsilon_{L}\rho(\dataset, \dataset'), \frac{\epsilon_{L}^{2}\rho(\dataset, \dataset')^{2}}{2}\right\}
    \end{split} 
    \end{equation}
\end{lemma}
\begin{proof}
    For an \(\epsilon\)-differentially private learner, the distributions \(\learner(\dataset)\) and \(\learner(\dataset')\) must be \(\epsilon\) close, which naturally introduces a bound on the Radon-Nikodym derivative of the output distributions. Let \(P,Q\) denote the output distributions \(\learner(\dataset), \learner(\dataset')\), respectively. \(\epsilon\)-DP suggests \(\left|ln\frac{dP}{dQ}(z)\right|\leq\epsilon\), i.e., \(\frac{dP}{dQ}(z)\) takes values in the interval \([e^{-\epsilon}, e^{\epsilon}], \forall z\in\dataspace\), w.p. \(1\), which in turn implies that \(\expctn\left[\frac{dP}{dQ}(z)\right] = 1\). 
    By definition of KL-Divergence,
    \begin{equation}
    \begin{split}
        %D_{KL}(P||Q) & = \sum_{z}p(z)ln\left(\frac{p(z)}{q(z)}\right)\\
        %             & = \sum_{z}q(z)\left(\frac{p(z)}{q(z)}\right)ln\left(\frac{p(z)}{q(z)}\right)\\
        %             & = \expctn_{z\sim Q}\left[\left(\frac{p(z)}{q(z)}\right)ln\left(\frac{p(z)}{q(z)}\right)\right]\\
        D_{KL}(P||Q) & := \expctn_{P}\left[log\frac{dP}{dQ}\right]\\
                     & = \expctn_{Q}\left[\frac{dP}{dQ}log\frac{dP}{dQ}\right]
    \end{split}
    \end{equation}
    Notice that the expectation is over a function of the form \(xln(x)\), which is convex for \(x>0\). To upper bound \(D_{KL}(P||Q)\), we must find a p.d.f. that maximizes the expectation on the right, achieved by placing all mass at the extremes of the interval \([e^{-\epsilon}, e^{\epsilon}]\). Given that \(\expctn\left[\frac{dP}{dQ}\right] = 1\), we have the following equation:
    \begin{equation}
        p e^{-\epsilon} + (1-p) e^{\epsilon}=1
    \end{equation}
    Solving the above gives us the p.d.f. of the quantity \(\frac{dP}{dQ}(z)\)
    \begin{equation}
        \frac{dP}{dQ}(z) =
        \begin{cases}
            e^{\epsilon} & \text{w.p. \(\frac{1-e^{-\epsilon}}{e^{\epsilon}-e^{-\epsilon}}\)} \\
            e^{-\epsilon} & \text{w.p. \(\frac{e^{\epsilon}-1}{e^{\epsilon}-e^{-\epsilon}}\)}
        \end{cases}
    \end{equation}
    Thus, the KL between the output distributions of an \(\epsilon\)-DP learner is bounded above as follows:
    \begin{equation}
        D_{KL}(\learner(\dataset)||\learner(\dataset')) \leq \epsilon \tanh\left(\frac{\epsilon}{2}\right) \approx \min\left\{\epsilon, \frac{\epsilon^{2}}{2}\right\}
    \end{equation}
    where the approximation is by a first order Taylor expansion of \(\tanh(.)\)\\
    
    Note that, to adhere to the definition of differential privacy (\ref{defn:pure-dp}), \(\dataset, \dataset'\) must be neighboring datasets, i.e., the Hamming distance \(d_{H}(\dataset, \dataset')\leq 1\). Naturally, the set of all datasets, together with the adjacency relation, form a Hamming graph, where \(\rho_{H}(\dataset,\dataset')\) between any two datasets \(\dataset, \dataset'\) is the number of rows where \(\dataset\) differs from \(\dataset'\). Therefore, by transitivity, we can set the privacy budget as \(\epsilon \rho_{H}(\dataset, \dataset')\). If we replace the Hamming metric with any metric \(\rho(\dataset,\dataset'): \dataspace \times \dataspace \rightarrow [0,\infty]\) that defines the metric space \((\dataspace, \rho)\) (where \(\dataspace\) is the data domain), then for \textit{any} two datasets \(\dataset, \dataset'\), we have the following result for an \(\epsilon_{L}\)-mDP learner:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset')) &\leq \epsilon_{L}  d(\dataset, \dataset')  \tanh\left(\frac{\epsilon_{L} \rho(\dataset, \dataset')}{2}\right)\\
        & \approx \min\left\{\epsilon_{L} \rho(\dataset, \dataset'), \frac{\epsilon_{L}^{2} \rho(\dataset, \dataset')^{2}}{2}\right\}
    \end{split}
    \end{equation}
\end{proof}
\subsection{Proof of Lemma \ref{lemma:rd}}\label{appendix-l2}

\begin{lemma}[R\'enyi]
For any \(\epsilon\)-DP learner \learner, trained on neighboring datasets \(\dataset,\dataset'\), the R\'enyi divergence between the output distributions satisfies the following inequality for $\alpha \in (1, \infty)$:
    \begin{equation}
        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon, \frac{3\alpha\epsilon^{2}}{2}\right\}
    \end{equation}
    If the learner is \(\epsilon_{L}-\)mDP, the Re\'nyi-divergence is bounded above as follows:
    \begin{equation}
        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon_{L}\rho(\dataset,\dataset'), \frac{3\alpha\epsilon_{L}^{2}\rho(\dataset,\dataset')^{2}}{2}\right\}
    \end{equation}
    \begin{proof}
        See Appendix \ref{appendix-l2}
    \end{proof}
\end{lemma}

\begin{proof}
The proof of this Lemma uses arguments similar to that of Lemma \ref{lemma:kl}. Let $P$ and $Q$ denote the output distributions of $\learner(D)$ and $\learner(D')$ respectively. We first note that the definition of $\epsilon$-DP implies, $| \frac{d P}{d Q} | \leq \epsilon$. From the definition of $D_\alpha$, it follows that,

\begin{align*}
   D_{\alpha}(P || Q) = \frac{1}{\alpha - 1} \expctn_{Q}\left[ \left(\frac{d P}{d Q} \right)^\alpha \right] \leq \frac{\alpha \epsilon}{\alpha - 1} \leq \epsilon 
\end{align*}

We now establish that $D_{\alpha}(P || Q) \leq \frac{3 \alpha \epsilon^2}{2}$. We first observe that this inequality is trivially satisfied if $\epsilon > 1$ since $\epsilon < \frac{3 \alpha \epsilon^2}{2}$ for $\epsilon > 1$ which imples $D_{\alpha}(P || Q) \leq \epsilon < \frac{3\alpha \epsilon^2}{2}$. Similarly, this inequality is also trivially satisfied if $\alpha > 1 + 1/\epsilon$ since $\epsilon \leq \frac{3\epsilon}{2} < \frac{3 \alpha \epsilon^2}{2}$. Hence, we consider the case when $\epsilon \leq 1$ and $\alpha \leq 1 + \frac{1}{\epsilon}$. We note that the $\epsilon$-DP condition implies $e^{-\epsilon} \leq \frac{d P}{d Q} \leq e^{\epsilon}$. From Taylor's Theorem, it follows that, for any $t \in [0, c - 1]$, $(1 + t)^{\alpha} \leq 1 + \alpha t + \frac{\alpha(\alpha - 1)}{2} \max\{ 1, c^{\alpha - 2} \} t^2$. We apply this identity to the computation of the Renyi divergence, using the fact that $e^{-\epsilon} \leq | \frac{d P}{d Q} | \leq e^{\epsilon} $, 
\begin{align*}
    \exp\left( (\alpha - 1) D_{\alpha}(P || Q) \right) &= \int \left( \frac{d P}{dQ} \right)^{\alpha} dQ = \int \left( 1 + \frac{d P}{dQ} - 1 \right)^{\alpha} dQ \\
    &\leq 1 + \alpha \int \left( \frac{d P}{dQ}  - 1\right) dQ + \frac{\alpha (\alpha - 1)}{2} \max \{ 1, e^{\alpha(\epsilon - 2)} \} \left( \frac{d P}{dQ}  - 1\right)^2 dQ \\
    &\leq 1 + \frac{\alpha (\alpha - 1)}{2} e^{\max\{0,\epsilon(\alpha - 2)\}} (e^{\epsilon} - 1)^2 
\end{align*}
Now, using the fact that $\alpha - 2 \leq 1/\epsilon - 1$ and $\ln(1 + x) \leq x$, it follows that,
\begin{align*}
    D_{\alpha}(P || Q) \leq \frac{\alpha (e^{\epsilon} - 1)^2}{2} e^{\max \{ 0, 
1 - \epsilon \} } \leq \frac{3 \alpha \epsilon^2}{2}
\end{align*}
where the last inequality follows from numerical calculation using the fact that $\epsilon \leq 1$. 

The proof for mDP follows by repeating the same argument with $\epsilon = \epsilon_L \rho(\dataset, \dataset')$.
\end{proof}

\subsection{Proof of Theorem \ref{thm:le-cam}}\label{appendix-t1}
\begin{theorem}[\(\epsilon-\)DP]
\label{thm:le-cam}
    If the learner \(\learner\) is \(\epsilon-\)differentially private, the minimax rate of error of a reconstruction adversary with $n$ samples from the output distribution is bounded as follows:
    \begin{equation}
        \minimax \geq C diam(\dataspace)^{2}e^{-n\epsilon \tanh\left(\frac{\epsilon}{2}\right)}
    \end{equation}
    where, $C$ is a universal constant and \(diam(\dataspace) = \sup_{z,z'\in\dataspace}\|z-z'\|_{2}\)
\end{theorem}
\begin{proof}
We begin by constructing a \(2\Delta\) packing set \(Z_{\Delta}\) of the dataspace \(\dataspace\), of size \(|Z_{\Delta}| = M\)and assume that the challenge \(\challenge\) is drawn uniformly at random from this set. \\
From Markov's inequality, we have:
    \begin{equation}
    \begin{split}
        \inf_{\hat{z}}\sup_{\distY\in\modelDist}\error &\geq \Delta^{2}\inf_{\hat{z}}\sup_{\distY\in\modelDist}\prob[\|\challenge - \zhat\|_{2}\geq\Delta] \\
        &\geq \Delta^{2}\inf_{\hat{z}}\sum_{i\in[M]}\prob[\|\challenge_{i} - \zhat\|_{2}\geq\Delta]
    \end{split}
    \end{equation}
For any estimator \(\zhat\), we define a minimum distance test \(\Psi\) as
    \begin{equation}
        \Psi(\zhat) = \underset{1\leq j\leq M}{\operatorname{argmin}}\|z_{j}-\zhat\|_{2}
    \end{equation}
    For some \(j\in[M]\), let \(\Psi(\zhat)\neq j\). Then, there exists some \(k\in[M]\) where \(\|z_{k} - \zhat\|_{2}\leq \|z_{j} - \zhat\|_{2}\). From triangle inequality, we have,\\
    \begin{equation}
    \begin{split}
        \|z_{j}-\zhat\|_{2} &\geq \|z_{j}-z_{k}\|_{2} - \|z_{k}-\zhat\|_{2} \geq \|z_{j}-z_{k}\|_{2} - \|z_{j} - \zhat\|_{2}\\
        &\geq \frac{1}{2}\|z_{j}-z_{k}\|_{2}\\
        &\geq \Delta
    \end{split}
    \end{equation}
    This observation allows us to obtain a lower bound on the minimax risk in terms of the error probability of the test \(\Psi\), resulting in a relaxation for the \(\inf\) over all estimators to an \(\inf\) over hypothesis tests \(\Psi\):
    \begin{equation}
    \begin{split}\label{eq:hyp-reduction}
        \minimax &\geq \Delta^{2}\inf_{\hat{z}}\sum_{i\in[M]}\prob[|\challenge_{i} - \zhat|_{2}\geq\Delta]\\
        &\geq \Delta^{2}\inf_{\Psi}\sum_{i\in[M]}\prob_{z_{i}}[\Psi\neq i]
    \end{split}
    \end{equation}
For \(M=2\), by Lemma \ref{lemma:le-cam}, we have
\begin{equation}
\label{eq:reduction-lecam}
    \minimax \geq \Delta^{2}(1 - \|P_{1}^{m-k}-P_{2}^{m-k}\|_{TV})
\end{equation}
where, \(P_{1}, P_{2}\) denote the output distributions from the learner trained on \(\dataset\cup\{z_{1}\}\) and \(\dataset\cup\{z_{2}\}\), respectively. We make the following observations on equation \eqref{eq:reduction-lecam}:\\
1. The choice of \(\Delta\) is instrumental in determining the tightness of the lower bound. As \(\Delta\) grows, \(\Delta^{2}\) is larger, but it becomes increasingly easier to distinguish between the distributions \(P_{1}\) and \(P_{2}\), thereby reducing the probability of error for the test \(\Psi\)\\

2. Total variation distance is not very well attuned to the i.i.d. sampling of \((m-k)\) samples from the product distributions \(P^{m-k}_{1}, P^{m-k}_{2}\). The KL Divergence is more suited for this analysis, due to the tensorization property \(D_{KL}(P^{n}||Q^{n}) = nD_{KL}(P||Q)\). 
To this end, we first represent the total variation distance between the distributions \(P_{1}\) and \(P_{2}\) as a function of the probability density functions \(p_{1}(x)\) and \(p_{2}(x)\) for \(x\in\mathcal{X}\) 
    \begin{equation}
        \begin{split}
            1 - \|P_{1}-P_{2}\|_{TV} & =  \int min\{p_{1}(x), p_{2}(x)\}dx\\
                            & \geq \frac{1}{2}\int min\{dP_{1}, dP_{2}\}dx \int max \{dP_{1}, dP_{2}\}dx\\
                            & \geq \frac{1}{2} \left( \int \sqrt{min\{dP_{1}, dP_{2}\}, max\{dP_{1}, dP_{2}\}}dx \right)^{2}\\
                            & = \frac{1}{2}\left(\int\sqrt{dP_{1}. dP_{2}}dx\right)^{2}\\
                            & = \frac{1}{2}\left(\int dP_{1}\sqrt{\frac{dP_{2}}{dP_{1}}}dx\right)^{2}\\
                            & = \frac{1}{2}\left(\mathbb{E}\left[\sqrt{\frac{dP_{2}}{dP_{1}}}\right]\right)^{2}\\
                            & = \frac{1}{2}\exp\left(2 \log\left( \mathbb{E}_{P_{1}}\left[\sqrt{\frac{dP_{2}}{dP_{1}}}\right]\right)\right)
        \end{split}
    \end{equation}
    where the second inequality follows from Cauchy-Schwatrz. We then apply Jensen's inequality to express the above in terms of the log-likelihood ratio to eventually derive an expression for the minimax error lower bound in terms of KL divergence. 
    \begin{equation}
        \begin{split}
            \minimax & \geq \frac{\Delta^{2}}{4}\exp\left(2 log \left(\mathbb{E}_{P_{1}}\left[\sqrt{\frac{dP_{2}}{dP_{1}}}\right]\right)\right)\\
            & \geq \frac{\Delta^{2}}{4}\exp\left(2\mathbb{E}_{P_{1}}\left[log\left(\sqrt{\frac{dP_{2}}{dP_{1}}}\right)\right]\right)\\
            & = \frac{\Delta^{2}}{4}\exp\left(-
            \mathbb{E}_{P_{1}}\left[log\left(\frac{dP_{1}}{dP_{2}}\right)\right]\right)\\
            & = \frac{\Delta^{2}}{4}\exp(-D_{KL}(P_{1}||P_{2}))\\\label{le-cam-exponential}
        \end{split}
    \end{equation}
    We argue that the optimal choice of \(\Delta\) for an \(\epsilon-\)DP learner is the separation between two farthest points in the dataspace \(\dataspace\), i.e., \(\Delta = diam(\dataspace)/2\). This is due to the statistical indistinguishability constraint imposed on the output distributions \(P_{1}, P_{2}\) by \(\epsilon-\)DP. Lemma \ref{lemma:kl} suggests that, for any two neighboring datasets \(\dataset\cup\{z_{1}\}, \dataset\cup\{z_{2}\}\), the KL divergence between the output distributions for an \(\epsilon-\)DP learner is at most \(\epsilon \tanh(\epsilon/2)\). This, together with the tensorization property of KL divergence, yields the result in \ref{thm:le-cam}
\end{proof}
\subsection{Comparison with prior work}\label{appendix-compare}
In this section we provide a detailed comparison between the lower bounds on the reconstruction rate of error obtained for \(\epsilon-\)DP learners in Theorem \ref{thm:le-cam} above against the rates obtained for \((2,\epsilon)-\)RDP learners in prior work (Theorem 1 of Guo et.~al. \cite{Guo2022}, restated below).

\begin{theorem}
\label{thm:Guo-et-al}[Theorem 1 \cite{Guo2022}]
    If the learner \(\learner\) is \((2,\epsilon)-\)RDP, and the reconstruction attack outputs an unbiased estimate \(\hat{z(h)}\) upon observing \(h\leftarrow\learner(\dataset)\), then
    \begin{equation}
    \label{eq:Guo-et-al}
        \expctn[\|\hat{z(h)}-z\|^{2}_{2}] \geq \frac{\sum_{i=1}^{d}diam_{i}(\dataspace)^{2}}{4 (e^{\epsilon}-1)}
    \end{equation}
    where \(diam_{i}(\dataspace)\) denotes the i-th coordinate-wise diameter defined as \(\underset{z,z'\in\dataspace, z_{j} = z'_{j}\forall j\neq i}\sup|z_{i}-z_{i}'|\)
\end{theorem}

We first note that both lower bounds require a bounded domain assumption, due to the dependence on \(diam(\dataspace)\) in Theorem \ref{thm:le-cam} and on \(diam_{i}(\dataspace)\) in Theorem \ref{thm:guo-2022}. However, even if this assumption holds, Theorem \ref{thm:guo-2022} yields invalid lower bounds for any \(\epsilon < \ln(1 + d/4)\). In contrast, our bound in Theorem \ref{thm:le-cam} is valid for all values of \(\epsilon\geq 0\), and tight up to constant factors for \(\epsilon=0\). We formally state and prove these claims as follows:

\textbf{Claim 1. Theorem~\ref{thm:Guo-et-al} gives vacuous Lower Bounds for \(\epsilon < \left(1 + \frac{d}{4}\right)\).}
%\begin{remark}
%    Prior work derives vacuous Lower Bounds for small \(\epsilon\)
%\end{remark}
\begin{proof}
\begin{comment}
    Note that for any \(z\in\dataspace\), the candidate reconstruction \(\hat{z}\) belongs to the same domain. For any pair of \(z,z'\in\dataspace\), where \(z\neq z'\),  we have the following string of inequalities:
    \begin{equation}\label{eq:expctn-upper}
        \expctn[\|z-\hat{z}\|_{2}^{2}] \stackrel{(1)}\leq \underset{z,z'\in\dataspace}\sup\|z-z'\|_{2}^{2} \stackrel{(2)}= diam(\dataspace)^{2} \stackrel{(3)}\leq \sum_{i=1}^{d}diam_{i}(\dataspace)^{2},  
    \end{equation}
    where (2) follows from the definition of diameter, and (3) follows from the definition of the \(l_{2}\) norm.
    %and (3) follows from a hybrid argument; namely, define $z = z_0, z_1, \ldots, z_{d-1}, z_d = z'$, where $z_i$ has the first $i$ coordinates from $z'$ and the last $d-i$ from $z$ and note that $z-z' = \sum_{i=1}^d z_{i-1} - z_i$. 
    %\satya{Maybe not so trivially. We need $z_1, \ldots, z_d$ such that first one is $z$ and last one is $z'$ and successive ones differ in $i$-th coordinate.}

    Combining \eqref{eq:expctn-upper} and \eqref{eq:Guo-et-al}, we note that we will have meaningful bounds only if 
    $4(e^{\epsilon}-1) \geq 1$ or 
    $\epsilon \geq \ln{(5/4)} \approx 0.22.$
\end{comment}
Consider the data reconstruction problem where the data domain $\mathcal{Z}$ is the unit ball in $\reals^{d}$ (equipped with the $\ell_2$ norm). Then, $\sum_{i = 1}^{d} diam_i(\mathcal{Z})^2 = d $ whereas $diam(\mathcal{Z}) = 1$. Let $z \in \mathcal{Z}$ be arbitrary. As per our problem setup in Section \ref{sec:problem-formulation}, $P_z = \mathcal{M}(\mathcal{D}_{-}\cup\{ z\})$ denotes the distribution induced by the randomized learner, parameterized by $z$. By definition of the diameter, any data reconstruction attack $\hat{z}$ satisfies the trivial upper bound 

\begin{equation}\label{eq:expctn-upper}
    \expctn_{h \sim P_z}\left[\| \hat{z}(h) - z \|^2_{2}\right] \leq diam(\mathcal{Z})^2 = 1
\end{equation}

However, the lower bound in Theorem 1 of Guo et.~al.\cite{Guo2022} states that for any data reconstruction attack $\hat{z}$ on a $(2, \epsilon)$ Renyi Differentially Private model, the reconstruction MSE is lower bounded as

$$\expctn_{h \sim P_z}\left[\| \hat{z}(h) - z \|^2_{2}\right] \geq \frac{\sum_{i = 1}^{d} diam_{i}(\mathcal{Z})^2}{4(e^{\epsilon} - 1)} \geq \frac{d}{4(e^\epsilon - 1)}$$
which is strictly greater than $1$ for any $\epsilon < \ln(1+d/4)$, thereby contradicting the trivial upper bound established above.

Note that, although we chose $\mathcal{Z}$ to be the unit ball in the above sketch, this is purely for the sake of clarity. In fact, the above construction generalizes to almost all possible choices of the data domain $\mathcal{Z}$ since $\sum_{i=1}^{d} diam_{i}(\mathcal{Z})^2$ is typically larger than $diam(\mathcal{Z})^2$ by a multiplicative factor of $d$ in most cases. 
    
    
    %Let \(\sum_{i=1}^{d}diam(\dataspace) = R^{2}\) where \(R\) is some constant, arising from the bounded domain assumption.
    %Consider any \(\eta > R^{2}\). We then choose the privacy parameter \(\epsilon\) as 
    %\begin{equation}
     %   \epsilon \leq \ln{(1+\frac{R^{2}}{4\eta})} < \ln{(5/4)} \approx 0.22
    %\end{equation}
    %For \(\epsilon = 0.22\), Theorem \ref{thm:guo-2022} admits the following lower bound:
    %\begin{equation}
        %\expctn[\|z-\hat{z}\|_{2}^{2}] \geq \frac{R^{2}}{4(e^{\epsilon} - 1)} \geq \eta > R^{2}
    %\end{equation}
    %However, we know from \eqref{eq:expctn-upper} that \(\expctn[\|z-\hat{z}\|_{2}^{2}] \leq R^{2}\), arriving at a contradiction. Therefore, for any \(\epsilon < 0.22\), the lower bound presented in Theorem \ref{thm:guo-2022}\cite{Guo2022} is invalid. 
\end{proof}

\textbf{Claim 2. The lower bound presented in Theorem \ref{thm:le-cam} in this work is valid for any \(\epsilon \geq 0\) and tight (up to constant factors) for \(\epsilon = 0\).}
\begin{proof}
    We know from \eqref{eq:expctn-upper} that \(\expctn_{h\sim P_{z}}[\|\zhat-z\|_{2}^{2}] \leq diam(\dataspace)^{2}\), implying the following upper bound on the minimax risk:
    \begin{equation}
        \inf_{\hat{z}}\sup_{\distY\in\modelDist}\expctn_{h\sim P_{z}}[\|\zhat-z\|_{2}^{2}] \leq \sup_{\distY\in\modelDist}\expctn_{h\sim P_{z}}[\|\zhat-z\|_{2}^{2}] \leq diam(\dataspace)^{2}
    \end{equation}
    As \(\epsilon\rightarrow 0\), the last factor in Theorem \ref{thm:le-cam} approaches 1. Thus, we have 
    \begin{equation}
        \inf_{\hat{z}}\sup_{\distY\in\modelDist}\expctn_{h\sim P_{z}}[\|\zhat-z\|_{2}^{2}] \geq \frac{diam(\dataspace)^{2}}{4}
    \end{equation}
    Therefore, the lower bound in Theorem \ref{thm:le-cam} is tight (up to constant factors) for \(\epsilon = 0\).    
    %\prateeti{(\(0,\delta\)-DP interesting)}
\end{proof}

\subsection{Proof of Theorem \ref{thm:mdp-lecam}}\label{appendix-t2}
\begin{theorem}[Low-dimensional Regime] 
For any challenge \(\challenge\) in the training set of an \(\epsilon_{L}\)-mDP learner, the minimax rate of error for a reconstruction adversary with $n$ samples from the output distribution is bounded below as follows:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
        \minimax \geq \frac{1}{2ne\epsilon_{L}^{2}}. 
    \end{equation}
\end{theorem}
\begin{proof}
    For the low-dimensional regime, we repeat the process of reduction from estimation to testing in Appendix \ref{appendix-t1} and apply Lemma \ref{lemma:le-cam} for \(M=2\). In terms of the separation \(\Delta\), the minimax risk is bounded below as follows:
    \begin{equation}
        \minimax \geq \frac{\Delta^{2}}{2}e^{-nD_{KL}(P_{0}||P_{1})}
    \end{equation}
    where, \(P_{0}, P_{1}\) represent the output distributions from the learner trained on \(\dataset\cup\{z_{0}\}, \dataset\cup\{z_{1}\}\), respectively.\\
    Unlike differential privacy, where the statistical divergence between the output distributions do not account for the separation of inputs, the separation \(\Delta\) shows up explicitly in the KL bound for mDP learners, as observed in Lemma \ref{lemma:kl}.
    \begin{equation}
        D_{KL}(P_{0}||P_{1})\leq \min\left\{\epsilon_{L}\Delta, \frac{\epsilon_{L}^{2}\Delta^{2}}{2}\right\}
    \end{equation}
    \paragraph{Case 1: \(D_{KL}(P_{0}||P_{1})\leq \epsilon_{L}\Delta\).}
    %This gives us
    \begin{equation}
        \minimax \geq \frac{\Delta^{2}}{4}e^{-n\epsilon_{L}\Delta}
    \end{equation}
    Maximizing over \(\Delta\), we have
    \begin{equation}
        \minimax \geq \frac{1}{n^{2}e^{2}\epsilon_{L}^{2}} \text{   , for } \Delta = \frac{2}{n\epsilon_{L}}
    \end{equation}
    \paragraph{Case 2: \(D_{KL}(P_{0}||P_{1})\leq \frac{\epsilon_{L}^{2}\Delta^{2}}{2}\).}
    %This gives us 
    \begin{equation}
        \minimax \geq \frac{\Delta^{2}}{4}e^{-n\frac{\epsilon_{L}^{2}\Delta^{2}}{2}}
    \end{equation}
    Maximizing over \(\Delta\), we have 
    \begin{equation}
        \minimax \geq \frac{1}{4en\epsilon_{L}^{2}}\text{   , for } \Delta = \frac{1}{\epsilon_{L}}\sqrt{\frac{2}{n}}
    \end{equation}
    The tightest lower bound of this form is given by
    \begin{equation}
        \minimax \geq \max\left\{\frac{1}{n^{2}e^{2}\epsilon_{L}^{2}}, \frac{1}{2en\epsilon_{L}^{2}}\right\} = \frac{1}{2en\epsilon_{L}^{2}}
        %\approx \Omega\left(\frac{1}{n\epsilon_{L}^{2}}\right)
    \end{equation}
    
%\prateeti{Mention why this works in low dimensions}    
\end{proof}
%\prateeti{TODO: change all \(Ms\) to \(|M|s\)}
\subsection{Proof of Theorem \ref{thm:mdp-fanos}}\label{appendix-t3}
\begin{theorem}[High Dimensional Regime]
    For any challenge \(\challenge\) in the training set of an \(\epsilon_{L}\)-mDP learner, the minimax risk for a reconstruction adversary with $n$ samples is bounded below as follows:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
    \begin{split}
     %\minimax &\geq \Omega\left(\max\left\{\frac{d}{(m-k)\epsilon^{2}}, \frac{d^{2}}{(m-k)^{2}\epsilon^{2}}\right\}\right)
     \minimax \geq \Omega\left(\frac{d}{n\epsilon_{L}^{2}}\right),
    \end{split}
    \end{equation}
where the $\Omega$ hides an \emph{absolute constant} independent of $d, n,$ and $ \epsilon_{L}$.
\end{theorem}
\begin{proof}

We extend the results in Theorem \ref{thm:mdp-lecam} to the high-dimensional setting. When constructing the packing set in low dimensions, \(|Z_{\Delta}|= M = 2\) is a reasonable assumption, given that the adversary's task of reconstructing a challenge sample in lower dimensions is relatively easier. However, as \(d\) grows, the reconstruction problem becomes harder, and the analysis in Theorem \ref{thm:mdp-lecam} yields loose lower bounds since the estimation problem is substantially harder than testing between two choices of the challenge sample \(\challenge\). Intuitively, in the high dimensional regime, we require \(M\geq o(d)\). For analysis in the high-dimensional regime, we assume \(d>2\)\\
We note that the reconstruction process detailed in Section \ref{sec:problem-formulation} can be interpreted as a Markov chain as follows:\\

1. Learner (defender) draws a challenge \(\challenge\sim Z_{\Delta}\), uniformly at random, appends it to the fixed dataset \(\dataset\) and computes \(\learner(\dataset\cup\{\challenge\})\)

2. The adversary draws \(n\) samples \(\{h_{1}, ..., h_{n}\}\) from the output distribution \(P_{z}\) induced by \(\learner(\dataset\cup\{\challenge\})\) 

3. With these samples at hand, the adversary outputs a reconstruction \(\zhat\)

The resulting Markov chain is: \(\challenge\rightarrow\{h_{1}, ..., h_{n}\}\rightarrow\zhat\). We have from Lemma \ref{lemma:fanos}
\begin{equation}
\label{eq:minimax}
\minimax \geq \Delta^{2}\left[1 - \frac{I(z;\zhat) + \ln 2}{\ln M}\right]
\end{equation}
For a tight lower bound, we upper bound the mutual information \(I(z;\zhat)\) and lower bound the metric entropy \(\ln{M}\).
\begin{equation}
    I(z;\zhat)\stackrel{(1)}\leq I(z; h_{1}, ..., h_{n}) \stackrel{(2)}\leq nD_{KL}(P_{z}||P_{z'}))
\end{equation}
where \((1)\) follows from Data Processing Inequality and (2) follows as a result of the convexity of \(\log\), and tensorization property of KL divergence. Note that the last inequality is an upper bound for all pairs \(z,z'\in Z_{\Delta}\). We construct a local packing of \(\dataspace\) with the following constraints:\\
1. \(|z-z'|\geq 2\Delta\)\\
2. \(D_{KL}(P_{z}||P_{z'})\leq \kappa^{2}\Delta^{2}\), for all distinct pairs \(z,z'\in Z_{\Delta}\), and \(\kappa>0\) is a problem-specific constant. 

Let \(Z_{\Delta}\) be a \(1/2-\)packing of the \(l_{2}\) unit ball. Constraint (1) is satisfied by the definition of \(1/2\) packing and constraint (2) is satisfied by the statistical indistinguishability guarantee of mDP learners. We have \(\kappa = \epsilon_{L}^{2}/2\) from Lemma \ref{lemma:kl} and \(M = 2^{d}\) from Lemma \ref{lemma:gilbert-v}. This gives us

\begin{equation}
    \minimax \geq \Delta^{2}\left[1-\frac{n\epsilon_{L}^{2}\Delta^{2} + \ln 2}{d\ln 2}\right]
\end{equation}

Maximizing over \(\Delta\), we have
\begin{equation}
    \minimax \geq \frac{\ln{2}(d-2)}{2n\epsilon_{L}^{2}} \approx \Omega\left\{\frac{d}{n\epsilon_{L}^{2}}\right\}
\end{equation}
for \(\Delta = \frac{1}{\epsilon_{L}}\sqrt{\ln{2}(d-2)/n}\)

\end{proof}

\subsection{Proof of Proposition \ref{prop:pnsgd-mdp}}\label{appendix-pnsgd}
\begin{proposition}
    Let $\cK \subset \mathbb{R}^d$ be a convex set and let $\{ f(., x) \}_{x \in cX}$ be a family of convex, $\beta$-smooth functions over $\cK$, where the gradients are $L_{input}$-input Lipschitz. Furthermore, assume $\cX$ is a bounded set. Then, for any $\eta \leq 2/\beta$ and $\alpha > 1$, initializing $w_0 \in \cK$ and dataset $S \in \cX^{n}$, PNSGD run with $\sigma^{2} \geq \frac{2 \alpha L^{2}_{input} diam(\cX)}{\epsilon_L (n- t + 1)}$ satisfies $(\epsilon_L, \alpha)$ metric differential privacy.
\end{proposition}
\begin{proof}
The proof of this theorem is an extension of Theorem 23 of Feldman et~al.\cite{Feldman2018} to Renyi mDP. Let $S \coloneqq (x_1, \dots, x_n)$ and $S' \coloneqq (x_1, \dots, x_{t-1}, x^{'}_{t}, \dots, x_n)$ be two arbitrary datasets that differ in index $t$. Since $\eta \leq 2/\beta$, the projected SGD updates are contractive noisy iterations as per Definition 19 of Feldman et~al.\cite{Feldman2018}. 

We define the projected SGD operators for the dataset $S$ as $g_i(w) = \Pi_{\cK}(w) - \eta \nabla f(\Pi_{\cK}(w), x_i) , \ i \in [n]$. The projected SGD operators $g^{'}_i$ for the dataset $S'$ are defined in a similar fashion. Since the datsets differ only in the $t^{\textrm{th}}$ index, we note that $g_i = g^{'}_i \ \forall i \in [n] \setminus \{ t \}$ and,
\begin{align*}
    \sup_{w} \| g_{t}(w) - g^{'}_{t}(w) \|_{2} = \eta \sup_{w} \| \nabla f(\Pi_{\cK}(w), x_t) - \nabla f(\Pi_{\cK}(w), x^{'}_t) \|_{2} \leq \eta L_{input} \| x_t - x^{'}_t \|_{2}
\end{align*}
Now, applying Theorem 22 of Feldman et~al.\cite{Feldman2018} with $a_1, \dots, a_{t-1} = 0$, $a_t, \dots, a_n = \frac{\eta L_{input} \| x_t - x^{'}_t \|_{2}}{n-t+1}$, $s_t = \eta L_{input} \| x_t - x^{'}_t \|_{2}$ and $s_i = 0, \ \forall i \in [n] \setminus \{ t \}$, we conclude,
\begin{align*}
    D_{\alpha}(w_n || w^{'}_n) \leq \frac{\alpha}{2 \eta^2 \sigma^2} \sum_{i=1}^{n} a^2_i \leq \frac{2 \alpha L^{2}_{input} \| x_t - x^{'}_t \|^{2}_{2}}{\sigma^2 (n-t+1)}
\end{align*} 
From the above inequality, we conclude that setting $\sigma^2 \geq \frac{2 \alpha L^{2}_{in} diam(\cX)}{\epsilon_L (n- t + 1)}$ suffices to ensure $D_{\alpha}(w_n || w^{'}_n) \leq \epsilon_L \| x_t - x^{'}_t \|^{2}$.
\end{proof}

%\subsection{Projected Noisy Stochastic Gradient Descent}

%\paragraph{Privacy Accounting}

%\paragraph{Observation}







%We observe that, for DP-SGD, this is indeed true. 



%Having set the framework for analyzing metric differential privacy in practice, we now investigate the feasibility of metric differential privacy in deep learning. We first discuss 

\begin{comment}
By a simple application of Markov's Inequality, we note that the quantity \(\error\) is bounded below by the error probability of a candidate reconstruction.
\begin{equation}\label{eq:markov-lb}
    \error\geq \delta^{2}\prob[|\zhat - z^{*}|\geq\delta]
\end{equation}
where \(\delta\) is the error threshold. This reduction is useful since it achieves a lower bound on the minimax rate of reconstruction error in terms of the error probability of candidate reconstruction for every \(\challenge\). Note that, since we operate in the minimax setting, we are essentially obtaining a lower bound on the error of the best possible adversary on the most difficult challenge sample. 

%While this reduction is useful and directly achieves a lower bound on the minimax rate of reconstruction error, it is still difficult to iterate over all estimators \(\hat{z}\). Furthermore, the choice of \(\delta\) is of great importance to ensure good approximations that lead to tight lower bounds. In what follows, we replace the \(\inf\) over all estimators to an \(\inf\) over all hypothesis tests and model training data reconstruction in the hypothesis testing framework.

\subsection{Reduction to Hypothesis Testing}
Recall that the reconstruction attack comprises of two stages - training the attack model \(\phi\leftarrow\adv(\shadow)\), followed by sampling \(n\) i.i.d. samples to generate candidate reconstruction \(\zhat\) of \(\challenge\). It is already apparent from \ref{def:minimax-risk} that, in the minimax regime, we are required to carefully choose the challenge \(\challenge\) for meaningful bounds on the target learner's resilience to training data reconstruction attacks. In this work, we assume that the challenge is contained in a \(2\delta-\)packing set of the dataspace \(\dataspace\), i.e., \(\challenge\in\{z\in\dataspace| |z-z'|\geq \delta \forall z\in\dataspace, \text{where} z\neq z'\}\) of cardinality \(k\).  



%We first focus on the second stage in the attack procedure, and in doing so, emphasize the importance of modeling the adversary's prior in the reconstruction process.

We assume the the 
\end{comment}



 
\begin{comment}
A relaxation to approximate Lipschitz privacy follows:
\prateeti{Add}
Furthermore, we are able to extend this notion to include \((\alpha,\epsilon')-\)RDP as follows:
\begin{definition}
    The learner \(\learner\) is \((\alpha, \epsilon')-\)Lipschitz RDP if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\), the R\'enyi divergence of the output distributions is \(\epsilon'-\)Lipschitz:
    \begin{equation}
        D_{\alpha}(P||Q)\leq\epsilon'\rho(\dataset,\dataset')
    \end{equation}
    where \(P\) and \(Q\) denote the output distributions \(\learner(\dataset)\) and \(\learner(\dataset')\), respectively.
\end{definition}
\end{comment}

\begin{comment}

 Note that, while this definition is closest to that of standard differential privacy (for the Hamming metric, with \(\rho(\dataset,\dataset')\leq1\)), it could be extended to incorporate metrics in the output space \(\labelspace\) as follows:

\begin{definition}
    A randomized learner \(\learner: \dataspace \rightarrow\labelspace\), where the input space is equipped with a metric \(\rho_{\dataspace}\) and the output space is equipped with a metric \(\rho_{\labelspace}\), satisfies \(\epsilon-\)Lipschitz privacy if for every pair of input datasets\(\dataset, \dataset'\in\dataspace\), the corresponding output distributions are \(\epsilon-\)Lipschitz 
    %, and \(\forall T\subseteq\labelspace\),
    \begin{equation}
        %\rho_{\dataspace}(\prob[\learner(\dataset)\in T], \prob[\learner(\dataset')\in T])\leq\epsilon\rho_{\dataspace}(\dataset,\dataset')
        \rho_{\labelspace}(\learner(\dataset), \learner(\dataset'))\leq\epsilon\rho_{\dataspace}(\dataset,\dataset')
    \end{equation}
\end{definition}
This 
%For simplicity, let \(P=\prob[\learner(\dataset)\in T]\) and \(Q=\prob[\learner(\dataset')\in T]\). This definition reduces to \ref{defn:pure-dp} for \(\rho_{\labelspace}(P,Q) = \max_{T\subseteq\labelspace}\left|\ln\frac{P(T)}{Q(T)}\right|\) and to \ref{defn:renyi-dp} for \(\rho_{\labelspace}(P,Q) =\frac{1}{\alpha-1}\sum_{T\subseteq\labelspace}\ln{P(T)^{\alpha}Q(T)^{1-\alpha}}\)
\end{comment}


\begin{comment}
\section{Introduction}
Our project is a competition on Kaggle (Predict Future Sales). We are provided with daily historical sales data (including each products sale date, block ,shop price and amount). And we will use it to forecast the total amount of each product sold next month. Because of the list of shops and products slightly changes every month. We need to create a robust model that can handle such situations.
 
\section{Task description and data construction}
\label{sec:headings}
We are provided with five datasets from Kaggle: Sales train, Sale test, items, item categories and shops. In the Sales train dataset, it provides the information about the sales number of an item in a shop within a day. In the Sales test dataset, it provides the shop id and item id which are the items and shops we need to predict. In the other three datasets, we can get the information about items name and its category, and the shops name.
\paragraph{Task modeling.}
We approach this task as a regression problem. For every item and shop pair, we need to predict its next month sales(a number).
\paragraph{Construct train and test data.}
In the Sales train dataset, it only provides the sale within one day, but we need to predict the sale of next month. So we sum the day's sale into month's sale group by item, shop, date(within a month).
In the Sales train dataset, it only contains two columns(item id and shop id). Because we need to provide the sales of next month, we add a date column for it, which stand for the date information of next month.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\begin{figure} % picture
    \centering
    \includegraphics{test.png}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}
\end{comment}




%===============================================
%===============================================


\bibliographystyle{unsrt}  
\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{comment}
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}
\end{comment}

\end{document}
