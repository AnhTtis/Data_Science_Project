\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
\PassOptionsToPackage{numbers, compress}{natbib}

% ready for submission
%\usepackage{neurips_2023}
\usepackage[preprint]{neurips_2023}
\usepackage{comment}
\usepackage{mymacros}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{caption}
\usepackage{subcaption}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bbm}


\title{Non-Asymptotic Lower Bounds for Training Data Reconstruction}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Prateeti Mukherjee\\%\thanks{Use footnote for providing further information
  %  about author (webpage, alternative address)---\emph{not} for acknowledging
  %  funding agencies.} \\
  Microsoft Research\\
  India\\
  %Pittsburgh, PA 15213 \\
  \texttt{t-pmukherjee@microsoft.com} \\
  % examples of more authors
  \And
   Satya Lokam \\
   Microsoft Research \\
   India \\
   \texttt{satya.lokam@microsoft.com} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  %The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  %both the left- and right-hand margins. Use 10~point type, with a vertical
  %spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  %bold, and in point size 12. Two line spaces precede the abstract. The abstract
  %must be limited to one paragraph.
% by informed adversaries
  Mathematical notions of privacy, such as differential privacy, are often stated as probabilistic guarantees that are difficult to interpret. It is imperative, however, that the implications of data sharing be effectively communicated to the data principal to ensure informed decision-making and offer full transparency with regards to the associated privacy risks. To this end, our work presents a rigorous quantitative evaluation of the protection conferred by private learners by investigating their resilience to training data reconstruction attacks. We accomplish this by deriving \emph{non-asymptotic} lower bounds on the reconstruction error incurred by any adversary against $(\epsilon, \delta)$ differentially private learners for target samples that belong to any compact metric space. Working with a generalization of differential privacy, termed metric privacy, we remove boundedness assumptions on the input space prevalent in prior work, and prove that our results hold for general locally compact metric spaces. We extend the analysis to cover the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget, and demonstrate that our bounds are minimax optimal under certain regimes. 
  %Motivated by the improvements conferred by metric DP, we extend the privacy analyses of popular deep learning algorithms to cover the broader notion of metric differential privacy, and show that, under mild assumptions, the semantic protection offered by R\'enyi mDP is as strong as R\'enyi DP, while introducing substantially less noise.
  %A fundamental challenge in privacy-preserving machine learning lies  
  %Privacy-preserving machine learning with notions that 
  %While differential privacy has emerged as the gold standard of privacy over the past decade, the notion presents a probabilistic guarantee that is difficult to interpret.
  %under mild assumptions, the 
  %level of noise needed to preserve $(\epsilon, \alpha)$-R\'enyi mDP in Noisy Stochastic Gradient Descent algorithms is substantially lower than that of $(\epsilon, \alpha)$-R\'enyi DP. 
  
  % We empirically validate the tightness of our bounds on a wide array of private learners and reconstruction algorithms.
  
  %Motivated by the improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD\cite{Abadi2016} and Projected Noisy SGD\cite{Feldman2018} to cover the broader notion of metric differential privacy, and observe improved dependence on the input dataspace.

  %Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD\cite{Abadi2016} and Projected Noisy SGD\cite{Feldman2018} to cover the broader notion of metric differential privacy.
  
  %to facilitate transparency with regards to the associated privacy risks.

   %Yet, communicating the implications of  to people contributing their data is
%vital to avoiding privacy theaterâ€”presenting meaningless privacy protection as meaningfulâ€”and
%empowering more informed data-sharing decisions. D

  %We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive \emph{non-asymptotic} minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget, and is minimax optimal 
  
  
 %Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD\cite{Abadi2016} and Projected Noisy SGD\cite{Feldman2018} to cover the broader notion of metric differential privacy. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Machine Learning has become increasingly pervasive, finding applications in multiple real world, risk-sensitive workflows. The fascinating potential of machine learning is perhaps most apparent in recent times, with the development of large-scale deep learning algorithms such as Large Language Models \cite{gpt-3} and Neural Diffusion Models \cite{dall-e-model}. These powerful algorithms consume enormous amounts of data to derive meaningful patterns for improved empirical performance. Theoretical analyses, however, reveal that Neural Networks are capable of memorizing the underlying training data \cite{Bresler2020, eldan-memorization, ohadshamir-memorization}, rendering such algorithms vulnerable to information leakage and adversarial attacks that can infer sensitive attributes of the training set, or worse, reconstruct one or more training samples entirely. Differential privacy was introduced in an attempt to formalize the notion of privacy protection while deriving meaningful conclusions from statistical information, and has since evolved into the \emph{de-facto} notion of privacy in machine learning. This has inspired a plethora of research in the development of  differentially private counterparts of popular machine learning \cite{Kamalika2008, Jain2011, Kamalika2009, Alabi2020} and deep learning \cite{Abadi2016,Jie2022,Song2013} algorithms. However, the promise of differential privacy is often difficult to interpret, and a rigorous quantification of \emph{``how much data privacy does an $(\epsilon, \delta)$-DP guarantee actually confer?"} is important to develop sufficiently private algorithms. 

Our work aims to answer this question by investigating the \emph{semantics} of differential privacy, i.e., measuring the level of protection offered by differentially private learners against information leakage, often specified by the attack class. For Membership Inference Attacks (MIAs), wherein, the adversary attempts to infer the membership status of a target individual in the training dataset, prior work \cite{Yeom2018} shows that $\epsilon$-DP algorithms ensure the adversary cannot perform much better than random guessing for values of \(\epsilon\leq0.4 \). Results of this form, that specify an explicit range of permissible values for the privacy parameter, are crucial in reconciling the gap between theoretical understanding of privacy guarantees and effective validation of privacy claims made in practice. For instance, citing privacy-utility tradeoffs, most practical deployments of $\epsilon$-DP algorithms consider $\epsilon \in [2, 4]$ to be sufficiently private \cite{smartnoise-pdf,apple-privacy}. In contrast, prior work demonstrates that such algorithmic setups are highly susceptible to MIAs. In particular, for \(\epsilon \geq 2\), one can design adversaries that can detect the presence of a sensitive record in the training dataset of any \(\epsilon\)-DP algorithm, with a success rate $\geq 88\%$, thereby rendering the promise of data protection effectively meaningless \cite{Humphries2020}. Naturally, such theoretical analyses that translate privacy guarantees of an algorithm into lower bounds on the effectiveness of a certain class of attacks find practical value in empirical auditing of privacy preserving algorithms \cite{gen-framework-auditin, ullman-how-private-is-sgd}. Equipped with such a lower bound, the crucial task of \emph{auditing} whether an algorithm satisfies a claimed DP guarantee reduces to the problem of designing an adversarial attack with a success rate that nearly matches the lower bound implied by the purported privacy guarantee. 

While substantial research exists on the protection offered by private algorithms against membership inference, it is not a sufficient requirement for privacy, especially in settings where the membership status itself is not sensitive. For example, an individual's presence in a public network is not sensitive information. However, if an adversary is able to determine the exact location of the individual, their privacy is at risk. In machine learning, this would be an instance of a Training Data Reconstruction Attack (DRA), wherein, the adversary attempts to reconstruct an instance of the training dataset through the output of a learner. Notions such as differential privacy should, in theory, protect against such adversaries. Empirically, prior work \cite{Balle2022} has established that this hypothesis is indeed true. In this paper, we study the theoretical underpinnings of a private algorithm's resilience to such reconstruction adversaries.

The semantics of differential privacy with respect to training data reconstruction was first analyzed in the recent work of \citet{Guo2022}, where they derived asymptotic lower bounds on the Euclidean MSE of a reconstruction adversary against $(2, \epsilon)$-R\'enyi DP learners. While they claim that these private learners offer strong semantic protection against reconstruction adversaries, unfortunately, as we show in Section \ref{sec:main-results}, their lower bound result violates trivial upper bounds for low values of $\epsilon$. In particular, if the input space has dimensionality $d=784$, as is the case for MNIST \cite{deng2012mnist}, the lower bound of \citet{Guo2022} is only valid for $\epsilon\geq 5.28$. Furthermore, prior work requires the input metric space to be compact and relies on the existence of coordinate-wise diameters. Such stringent assumptions hinder the generalizability of their results beyond $\reals^{d}$, particularly in unstructured domains such as text, where the notion of coordinate-wise diameters may not be well-defined or relevant. 
%, thereby making it difficult to generalize their results beyond $\reals^{d}$. This is quite restrictive and does not support analysis of privacy notions in unstructured domains such as text. 
%Under limitations, their work suggests the use of minimax lower bounds \cite{rigollet} as a promising avenue for the analysis of the more general notion of $(\epsilon, \delta)$-DP. In this paper, we explore this direction, and generalize our analysis to any locally compact input metric space, thereby enabling analysis of privacy notions in unstructured domains such as text.
%arbitrary (locally compact) metric spaces.
%raises the question whether a hypothesis testing framework \cite{rigollet} enables the use of general DP accountants. In this paper, we answer this question to the affirmative, and further extend the analysis to arbitrary locally compact metric spaces. 
%This raises the fundamental question whether it is even possible to construct in a numerically efficient manner differentially private synthetic data that come with rigorous utility
%guarantees for a wide range of (possibly complex) queries, while achieving a favorable privacyutility tradeoff? In this paper we will answer this question to the affirmative.
%Furthermore, their analysis does not capture the inherent hardness of reconstruction in the high dimensional regime that was empirically observed in \citet{Balle2022}, and does not admit natural extensions to arbitrary metric spaces. 
%most modern applications of machine learning involve high dimensional datasets, where the data dimension if of the same order as (and sometimes larger than) the available number of samples. 
\subsection{Contributions}
%and present the first
Our work contributes to filling this gap through a \emph{non-asymptotic} analysis of the effectiveness of training data reconstruction by \emph{informed} adversaries against private learners, and by deriving minimax lower bounds on the reconstruction accuracy of \emph{any adversary} as a function of the privacy parameter of the algorithm and the query complexity of the adversary. More specifically, we focus on the threat model of \citet{Balle2022}, and develop a general information-theoretic framework that covers the stronger privacy notion of $(\epsilon, \delta)$-DP for \emph{any $\epsilon\geq 0$}. In fact, our analysis goes beyond standard differential privacy and considers the notion of \emph{Metric Differential Privacy} or \emph{Lipschitz Privacy} \cite{Chatzikokolakis2013, Koufogiannis2015, Koufogiannis2016, Imola2022} -- a formalism that generalizes differential privacy by accounting for the underlying metric structure of the data beyond the Hamming metric. The main results of this work may be (informally) summarized as follows.

\textbf{Lower Bounds for $(\epsilon, \delta)$-DP [Theorem \ref{thm:dp-lecam}]} For $(\epsilon, \delta)$-DP learners, with training set sampled from a compact metric space $(\mathcal{Z}, \rho)$, we show that any adversary that draws $n$ samples from the learner's output distribution to generate a candidate reconstruction $\hat{z}$ of any target training sample $z$ incurs a minimax error of at least $\Omega(\text{diam}(\mathcal{Z})^{2}e^{-n\epsilon\tanh(\epsilon/2)}(1-\delta))$. We further demonstrate that this result is minimax optimal in the high privacy regime.
%We prove that, for any compact input metric space $(\mathcal{Z}, \rho)$, if the adversary attempts to generate a reconstruction $\hat{z}$ of a challenge sample $z\in\mathcal{Z}$ in the training set of an $(\epsilon, \delta)$-DP learner with $n$ samples from the learner's output distribution $P_{z}$, their minimax rate of reconstruction error satisfies $\inf_{\hat{z}}\sup_{P_{z}\in\mathcal{P}}\expctn[\phi(z, \hat{z})]\geq\Omega(\text{diam}(\mathcal{Z})^{2}e^{-n\epsilon\tanh(\epsilon/2)}(1-\delta))$, where $\phi: \reals_{+}\to\reals_{+}$ is a non-decreasing function and $\Omega$ hides constant factors in the bound. We demonstrate that, unlike prior work, this result is minimax optimal in the high privacy regime. %For comparison with prior work, we propose a trivial extension to $(\alpha, \epsilon)$-RDP learners. 
%\prateeti{TODO: elaborate. For DP (high privacy) for mDP (gaussian mech.)}
%\footnote{Datasets that differ in a single element.}

\textbf{Analysis in arbitrary locally compact metric spaces [Theorem \ref{thm:mdp-lecam}, Theorem \ref{thm:mdp-fanos}]} Differential privacy does not account for the underlying metric structure of the data and is largely dependent on the notion of \emph{neighboring} datasets. In unstructured data domains, such as text, it is challenging to establish a natural definition of \emph{neighbors}, but there exists a notion of \emph{distinguishability} of representations. As a concrete example, consider a topic classification problem where the author's identity is private information. In this case, two representations that are "similar in topic" must remain so in the output of a private mechanism, irrespective of the author. These notions of distance cannot be captured in the Hamming metric, and require more sophisticated measures such as the Earth Mover's distance \cite{text-processing}. To this end, we extend our analysis to cover any arbitrary locally compact linear space $\mathcal{Z}$, equipped with a metric $\rho$. To accommodate for such generality, we employ the notion of metric differential privacy, and prove that, if the learner is $(\epsilon_{L},\delta)$-mDP, the minimax risk of reconstruction in the low dimensional regime is at least $\Omega\left(\frac{1-\delta}{n\epsilon_{L}^{2}}\right)$. 
In the high dimensional regime, we present a generalization of this result and derive lower bounds of the form $\Omega\left(\frac{\tilde{d}(1-\delta)}{n\epsilon_{L}^{2}}\right)$, where $\tilde{d}$ is a quantity dependent on the metric entropy in this space (see Section \ref{sec:main-results}). Note that, unlike prior work on attack lower bounds for differential privacy\cite{Yeom2018, Tramer2020, Guo2022}, our approach in the mDP setting does not require any assumptions on the boundedness of $\mathcal{Z}$.
%To be able to
%work in such generality, we employ the notion of metric privacy which reduces to differential
%privacy when we specialize to empirical measures 
%If the learner is $(\epsilon_{L},\delta)$mDP, under the same assumptions, the minimax risk satisfies $\inf_{\hat{z}}\sup_{P_{z}\in\mathcal{P}}\expctn[\ell_{2}(z, \hat{z})]\geq c_{1}(1-\delta)/n\epsilon_{L}^{2}$, where $c_{0}, c_{1}$ are universal constants, independent of $n,d,\epsilon,\epsilon_{L}, \delta$. We also show that our lower bounds are minimax optimal (upto constant factors) under certain regimes.
%It is apparent from the above definitions that differential privacy does not account for the underlying metric structure in the data domain (beyond the discrete / Hamming metric), and is entirely dependent on the \emph{adjacency} relation between datasets. This results in several restrictions, such as requiring every row to represent an individual or a single unit of protection. As such, this notion is quite inflexible, and is unable to handle cases where each row belongs to some arbitrary domain of secrets, where there is no natural definition of \emph{neighbors}, but there exists a notion of \emph{distinguishability} of secrets. For any meaningful notion of privacy, if two secrets are statistically indistinguishable, they must remain so in the output of the randomized learner. Furthermore, privacy claims in most learning algorithms are achieved by distorting non-private computations via the addition of noise scaled in accordance to the nature of the computation. Differential privacy requires that the same level of noise be added to every feature corresponding to an individual in a statistical database, regardless of the security risk associated with the feature. Often, however, certain features are more sensitive than the rest. For example, a person's residential address is significantly more sensitive than their date of joining a social media platform, and should therefore be treated differently for fine-grained privacy. These shortcomings are addressed in a generalization of differential privacy, termed Metric Differential Privacy (mDP).
%\textbf{Analysis in high dimensions [Theorem \ref{thm:}]} Most modern applications of machine learning involve high dimensional datasets, where the data dimension is of the same order as (and sometimes larger than) the available number of samples. In such settings, statistical algorithms often suffer from a significant loss in performance, a behavior generally attributed to the \emph{curse of dimensionality} \cite{Bishop2006}. Thus, from the perspective of the adversary, we intuitively expect the task of training data reconstruction to be harder in the high dimensional regime, particularly when they are restricted in the number of samples they can draw from the learner's output distribution. While this intuition is confirmed by thorough empirical evaluation in \citet{Balle2022}, prior work on the theoretical analysis of training data reconstruction does not capture the inherent hardness of the problem in high dimensions for unstructured domains. To circumvent this barrier, we present a simple extension of our analysis to the high dimensional regime and prove that, for any challenge sample $z\in\reals^{d}$ in the training set of an $(\epsilon_{L}, \delta)$mDP learner, the adversary's minimax risk satisifies $\inf_{\hat{z}}\sup_{P_{z}\in\mathcal{P}}\expctn[\ell_{2}(z, \hat{z})]\geq \Omega\left(\frac{d(1-\delta)}{n\epsilon^{2}}\right)$. 
%query budget of the adversary is finite.

\textbf{Accounting in Private Deep Learning [Corollary \ref{cor:pnsgd-cor}]} We observe that the lower bounds in the $(\epsilon,\delta)$-mDP setting are quantitatively superior to those obtained for $(\epsilon,\delta)$-DP (of this paper) and $(\alpha, \epsilon)$-RDP (of prior work \cite{Guo2022}). We complement our findings regarding the theoretical improvements conferred by metric differential privacy by demonstrating that most privacy-inducing mechanisms in the DP literature and accounting methods in noisy stochastic gradient descent algorithms admit natural extensions to metric DP. As a corollary, we show that Projected Noisy Stochastic Gradient Descent(PNSGD)\cite{Feldman2018} run with noise level $\sigma^2\geq\frac{2\alpha L^{2}_{input}\text{diam}(\mathcal{X})}{\epsilon_{L}(n-t+1)}$ satisfies $(\alpha, \epsilon_{L})$ R\'enyi mDP for input $t\in[n]$, which improves the scaling factor of the noise in the best known result for $(\alpha, \epsilon)$ R\'enyi DP by $\text{diam}(\mathcal{X})$, where $\mathcal{X}$ is the input data domain and the gradients are $L_{input}$-input Lipschitz.

%To the best of our knowledge, we present the first nonasymptotic analysis of the privacy protection conferred by 1) $(\epsilon, \delta)$ differentially private learners, when the input space is bounded, and 2) $(\epsilon, \delta)$ metric private for any locally compact metric space in both low and high dimensions, against \emph{informed} training data reconstruction adversaries. 

 Our results are non-asymptotic, in the sense that they capture an explicit dependence on the query complexity of the adversary, and make no assumptions on the internal structure of either the adversary or the private learner. Analysis in this setting is particularly important in randomization-based notions of privacy, such as differential privacy, wherein, querying the algorithm multiple times on a fixed dataset yields varied outcomes. Intuitively, for a private classifier model that preserves utility, the accuracy of the outputs captures the distance of the target sample to the decision boundary. As an example, if the target training sample is comprised of $d$ binary features $x = [x_{1}, x_{2}, ..., x_{d}]^{T}$, \footnote{With slight abuse of notation, we use $x$ to denote the target sample and $x_{i}\in\{0,1\}$ to represent features.}repeatedly flipping the bit vectors would define a region around the target sample. Querying the model for each perturbed sample in this region and recording how well the model performs might reveal the correct configuration of features to reconstruct $x$.
 
 To the best of our knowledge, our lower bounds on the minimax reconstruction risk in arbitrary input metric spaces are the first of their kind in the literature. While the technical results presented in this work rely on well-known statistical tools, the goal is to present insights and information-theoretic arguments that may be used in the modeling and design of privacy-preserving algorithms, specifically in problems involving unstructured data domains. 
%n. To the best of our knowledge, our minimax risk lower bounds
%for high-dimensional mean estimation and linear regression in both
%low and high dimensions are the first of their kind in the literature

%\textbf{Empirical validation} We empirically evaluate the tightness of our bounds on a wide array of private learners and reconstruction algorithms of \citet{Balle2022}.

%The non-asymptotic setting is of particular importance to randomization-based notions of privacy such as differential privacy, wherein, querying the algorithm multiple times on a fixed dataset yields varied outcomes. Intuitively, for a private classifier model that preserves utility, the accuracy of the outputs captures the distance of the target sample to the decision boundary. As an example, if the target training sample is comprised of $d$ binary features $x = [x_{1}, x_{2}, ..., x_{d}]^{T}$\footnote{This is a slight abuse of notation. We use $x$ to denote the target sample and $x_{i}\in\{0,1\}$ to represent features of $x$.}, repeatedly flipping the bit vectors would define a region around the target sample. Querying the model for each perturbed sample in this region and recording how well the model performs might reveal the correct configuration of features to reconstruct $x$. 
%\cite{ShadiCISPA}
%and the model is queried for each perturbed sample in this region. For correctly guessed values of $x_{i}$, the returned labels are less likely to change significantly, as opposed to 
%Querying the model for each such perturbation and counting the number of perturbed samples that
%populating a region around the target sample, with multiple perturbations across the feature vectors 

%To the best of our knowledge, the semantics of differential privacy with respect to training data reconstruction was first analyzed in the recent work of \citet{Guo2022}, where they derived asymptotic lower bounds on the MSE of a reconstruction adversary against $(2, \epsilon)$-R\'enyi DP learners. However, as we show in Section \ref{sec:main-results}, their lower bound result violates trivial upper bounds for low values of $\epsilon$. In particular, for the MNIST dataset where $d=784$, the lower bound of \citet{Guo2022} is only valid for $\epsilon\geq 5.28$. On the contrary, our non-asymptotic analysis is highly general and covers stronger notions of privacy such as pure DP and $(\alpha, \epsilon)-$ R\'enyi DP for any $\alpha \geq 1, \epsilon \geq 0$. In fact, our analysis goes beyond pure differential privacy and considers the notion of \emph{Metric Differential Privacy} or \emph{Lipschitz Privacy} \cite{Chatzikokolakis2013, Koufogiannis2015, Koufogiannis2016, Imola2022} -- a formalism that generalizes differential privacy by accounting for the underlying metric structure of the data beyond the Hamming metric. In this setting, our lower bounds exhibit a quadratic drop in the reconstruction error with respect to the privacy parameter $\epsilon$, in both the classical regime as well as high dimensional regime, thereby obtaining a substantial improvement over the exponential drop observed in the lower bound analysis of (pure) differentially private learners and R\'enyi private learners derived in prior work \cite{Guo2022}. We also show that our lower bounds are minimax optimal (upto constant factors) under certain regimes.

%Most modern applications of machine learning involve high dimensional datasets, where the data dimension is of the same order as (and sometimes larger than) the available number of samples. In such settings, statistical algorithms often suffer from a significant loss in performance, a behavior generally attributed to the \emph{curse of dimensionality} \cite{Bishop2006}. Thus, from the perspective of the adversary, we intuitively expect the task of training data reconstruction to be harder in the high dimensional regime, particularly when the query budget of the adversary is finite. While this intuition is confirmed by thorough empirical evaluation in \citet{Balle2022}, prior work on the theoretical analysis of training data reconstruction does not capture the inherent hardness of the problem in high dimensions. %\prateeti{not possible to capture this in dp}
%To reconcile theoretical understanding of data reconstruction in such settings with the phenomena observed in practic
%To circumvent this barrier, we present a simple extension of our analysis to the high dimensional regime and obtain tight lower bounds of the form $\Omega\left(\frac{d}{n \epsilon^2}\right)$ for a query budget of $n$ in the mDP setting. We complement our findings regarding the theoretical improvements conferred by metric differential privacy by demonstrating that most commonly used privacy-inducing mechanisms in the differential privacy literature and accounting methods in noisy stochastic gradient descent algorithms admit natural extensions to arbitrary metric spaces. In fact, for Projected Noisy Stochastic Gradient Descent (PNSGD)\cite{Feldman2018}, we show that, for a certain family of functions, the requisite noise level to satisfy metric R\'enyi DP is less than that of traditional R\'enyi DP by a factor of $diam(\mathcal{X})$, where $\mathcal{X}$ is the input data domain.
%\prateeti{break contribution and introduction. Add informal theorem statements instead of text.}
%The main results of this work may be (informally) summarized as follows:
%\section{Related Works}
%\prateeti{TODO}
%- Lower Bounds via discrepancy (Dinur et al.) 
%- Statistical query release v/s DP training
%- Guo et al: In their work, they state hypothesis testing as future work. We do exactly that here.
\section{Notation and Preliminaries}\label{sec:prelims}
%In this section, we present the background pertinent to privacy notions, the threat model, and error metrics to assess the adversary's computations.
%In this section, we present the relevant background on various notions of privacy and some useful properties of statistical divergence measures.
Let $\mathcal{D}$ be an arbitrary collection of secrets, with samples drawn from the linear space $\mathcal{Z}$, equipped with a metric $\rho: \mathcal{Z}\times\mathcal{Z}\to[0,\infty)$ that measures the dissimilarity between points in $\mathcal{Z}$. For the purposes of our analysis, we will assume $(\mathcal{Z}, \rho)$ is locally compact, unless stated otherwise. Note that this property is satisfied by almost every possible data domain, including finite dimensional normed spaces, such as $\reals^{d}$. When restricted to the Hamming metric, we use $\rho = \rho_{H}$. In practical applications, \(\dataset\) is modelled as an aggregation of \(N\) samples \(\{z_{1}, z_{2}, ..., z_{N}\}\), where each sample \(z_{i}\in \mathcal{S}_{x}\times\mathcal{S}_{y}\) is a high-dimensional vector, comprising of \((x,y)\) pairs, where \(x\in \mathcal{S}_{x}\) is a \(d-\)dimensional feature vector and \(y \in \mathcal{S}_{y}\) is the corresponding label. The measure of dissimilarity for any two such collections \(\dataset, \dataset'\in\dataspace\) is summed over all samples, i.e., \(\rho(\dataset, \dataset') = \sum_{i=1}^{N}\rho(z_{i},z_{i}')\). We use the $\Omega$ notation to characterize the dependence of our rates on $N, d, \epsilon$ and $\delta$, suppressing numerical constants. Additionally, we use $C$ to represent universal constants, independent of $N, d, \epsilon$ and $\delta$, that may take different values depending on the context.

We first present the relevant background on various notions of privacy.
%\subsection{Differential Privacy}
%\subsection{Privacy Definitions}
\begin{definition}[(Pure) Differential Privacy \cite{Dwork2017}]
\label{defn:pure-dp}
    For any \(\epsilon \geq 0\), a randomized learner \(\learner:\dataspace^{N}\to\labelspace\) is \(\epsilon\)-differentially private(DP) if, for every pair of datasets \(\dataset, \dataset' \in \dataspace\) such that $\rho_{H}(\dataset, \dataset')\leq 1$, the output probability distributions satisfy:
    \begin{equation}
        \forall T \subseteq \labelspace, \; \prob[\learner(\dataset)\in T]\leq e^{\epsilon}\prob[\learner(\dataset')\in T].
    \end{equation}
\end{definition}
%If the above holds with probability at least \(1-\delta\) for some \(\delta\in[0,1]\), the learner is \((\epsilon, \delta)-\) DP.
%The above definition imposes a strict restriction on the probability mass between the output distributions \(\learner(\dataset), \learner(\dataset')\).
$(\epsilon, \delta)$- DP is a relaxation of this constraint, such that the above holds with probability at least $1-\delta$.
%if we allow \(\delta\) fraction of the mass (for some $\delta \geq 0$)if to be located outside of the curve between these output distributions, we obtain the following definition of approximate differential privacy. 
\begin{comment}

\begin{definition}[(Approximate) Differential Privacy]
%\prateeti{ADP is different from the probabilistic approximate DP (that the above holds with prob at least 1-\(\delta\)). We can cite this paper and show one of their curves to highlight that \(delta\) has no effect on the ratio of probabilities: https://eprint.iacr.org/2018/277.pdf for ADP}
\label{defn:adp}
For any \(\epsilon\geq0, \delta\geq0\), a randomized learner \(\learner:\dataspace^{n}\rightarrow\labelspace\) is \((\epsilon, \delta)\)-(approximate) differentially private (ADP) if, for every pair of neighboring datasets \(\dataset, \dataset' \in \dataspace\) that differ in a single training sample, the following inequality holds:
    \begin{equation}
        \forall T \subseteq \labelspace, \; \prob[\learner(\dataset)\in T]\leq e^{\epsilon}\prob[\learner(\dataset')\in T] + \delta.\footnote{Setting \(\epsilon=0\), this implies an upper bound on the total variation between the output distributions \(\|P-Q\|_{TV}\leq \delta\), where \(P\) and \(Q\) are defined as in \ref{defn:rdp}}
    \end{equation}
\end{definition}
\end{comment}


In essence, for any pair of input datasets that differ in a single data point, the output distributions of the learner (as a function of the input dataset) must be sufficiently \emph{statistically indistinguishable}. There exist several relaxations of differential privacy in the literature that consider varying notions of statistical indistinguishability, a popular relaxation being R\'enyi Differential Privacy.
\begin{definition}[R\'enyi Differential Privacy \cite{Mironov2017}]
\label{defn:rdp}
    For any \(\epsilon \geq 0\) and \(\alpha > 1\), a randomized learner \(\learner:\dataspace^{N}\rightarrow\labelspace\) is \((\alpha, \epsilon)\)-R\'enyi differentially private(RDP) if, for every pair of datasets \(\dataset, \dataset' \in \dataspace\) such that $\rho_{H}(\dataset, \dataset')\leq 1$, the R\'enyi Divergence is bounded by \(\epsilon\):
    \begin{equation}
    \label{eqn:rdp-def}
        D_{\alpha}(P||Q) = \frac{1}{\alpha - 1}\log\expctn_{x\sim Q}\left[\left(\frac{P(x)}{Q(x)}\right)^{\alpha}\right]\leq\epsilon
    \end{equation}
    %\satya{Corrected by the big parenthesis before powering to $\alpha$.}
    where \(P\) and \(Q\) denote the output distributions \(\learner(\dataset)\) and \(\learner(\dataset')\), respectively.  %The case of $\alpha = 1$ and $\alpha = \infty$ are defined by taking the appropriate limits on the LHS of \eqref{eqn:rdp-def}
\end{definition} 
\begin{comment}
        \begin{equation}
        \expctn_{o\sim\learner(\dataset')}\left[\frac{\prob[\learner(\dataset) = o]}{\prob[\learner(\dataset') = o]}\right] \leq e^{(\alpha-1)\epsilon}
    \end{equation}
\end{comment}
As is evident in the above definitions, traditional differential privacy is restricted to the discrete setting, where datasets can differ in a single element. The following definition extends the classical concept of differential privacy to general metric spaces. 


%In essence, if \(\dataset'\) is defined as \(\dataset':=\dataset\setminus\{z\}\), where \(z = (x,y)\) is a single training sample defined by a \(d\)-dimensional feature vector \(x\in\reals^{d}\) and a label \(y\in\reals\), one should not be able to infer (almost) anything about this individual sample from the output of the learner \(\learner\). Notice that the output distribution of \(\learner\) is parameterized by its input dataset \dataset\  or \dataset', whichever is relevant.
\begin{comment}
    
A common paradigm in releasing statistical information with differential privacy is to generate a noisy estimate of the true statistic. Namely, if $f: \dataspace\to\reals^{d}$ is a real-valued function\footnote{The restriction to real-valued functions is not essential.}, $\learner(\dataset) = f(\dataset) + \eta$ is differentially private for an appropriate noise level $\eta$. The magnitude of perturbation is generally calibrated according to the sensitivity of the function $f$, defined as follows:
%We also note that mechanisms in differential privacy generally require the following sensitivity constraint:
\begin{definition}[Sensitivity]
\label{defn:global-sensitivity}
    Given any function \(f:\dataspace\rightarrow\reals^{d}\),
 \begin{equation}
     \Delta_{f} = \underset{\dataset,\dataset'\in\dataspace ; \|\dataset-\dataset'\|\leq 1}{\max}\|f(\dataset)-f(\dataset')\|
 \end{equation}
\end{definition}
%Note that the sensitivity constraint is over all neighboring datasets \(\dataset, \dataset'\in\dataspace\).
The Gaussian mechanism, for instance, requires that $f(\dataset)$ be perturbed with noise drawn from the Gaussian distribution, as follows:
\begin{definition}[Gaussian Mechanism]
\label{defn:dp-gaussian-mech}
    Given any function \(f:\dataspace\rightarrow\reals^{d}\), the Gaussian mechanism is defined by
 \begin{equation}
     \learner(\dataset) = f(\dataset) + \mathcal{N}(0, \Delta_{f}^{2}\sigma^{2})
 \end{equation}
 For $\epsilon < 1$ and $c^{2} > 2\ln(1.25/\delta)$, the Gaussian mechanism with parameter $\sigma \geq c\Delta_{f}/\epsilon$ satisfies $(\epsilon, \delta)$ approximate DP \cite{Dwork2017}.
\end{definition}
\end{comment}
%\prateeti{TODO: write short motivation for metric DP. Mention text unstructured if there's space. Also don't think mDP should be a separate section.}
%It is apparent from the above definitions that differential privacy does not account for the underlying metric structure in the data domain (beyond the discrete / Hamming metric), and is entirely dependent on the \emph{adjacency} relation between datasets. This results in several restrictions, such as requiring every row to represent an individual or a single unit of protection. As such, this notion is quite inflexible, and is unable to handle cases where each row belongs to some arbitrary domain of secrets, where there is no natural definition of \emph{neighbors}, but there exists a notion of \emph{distinguishability} of secrets. For any meaningful notion of privacy, if two secrets are statistically indistinguishable, they must remain so in the output of the randomized learner. Furthermore, privacy claims in most learning algorithms are achieved by distorting non-private computations via the addition of noise scaled in accordance to the nature of the computation. Differential privacy requires that the same level of noise be added to every feature corresponding to an individual in a statistical database, regardless of the security risk associated with the feature. Often, however, certain features are more sensitive than the rest. For example, a person's residential address is significantly more sensitive than their date of joining a social media platform, and should therefore be treated differently for fine-grained privacy. These shortcomings are addressed in a generalization of differential privacy, termed Metric Differential Privacy (mDP). 
%\prateeti{TODO: check correctness of the feature-based privacy claim}
%\subsubsection{Privacy in arbitrary metric spaces}\label{sec:mDP}
%Consider an arbitrary collection of secrets \(\dataset\), with samples drawn from a metric space \((\dataspace, \rho)\), wherein the metric  \(\rho: \dataspace\times\dataspace\rightarrow [0,\infty]\) measures the dissimilarity between any two points in \(\dataspace\). In practical applications, \(\dataset\) is modelled as an aggregation of \(N\) samples \(\{z_{1}, z_{2}, ..., z_{N}\}\), where each sample \(z_{i}\in \mathcal{S}_{x}\times\mathcal{S}_{y}\) is a high-dimensional vector, comprising of \((x,y)\) pairs, where \(x\in \mathcal{S}_{x}\) is a \(d-\)dimensional feature vector and \(y \in \mathcal{S}_{y}\) is the corresponding label. The measure of dissimilarity for any two such collections \(\dataset, \dataset'\in\dataspace\) is summed over all samples, i.e., \(\rho(\dataset, \dataset') = \sum_{i=1}^{N}\rho(z_{i},z_{i}')\). % \satya{Above, $\labelspace$ is used to refer to \emph{models} produced by the learning algorithm.} 

%In practice, the domain \(\featurespace = \reals^{n}\) is equipped with the \(l_{2}\) norm
%We operate in the setting where the data is an arbitrary collection of secrets from a finite vector space \(\featurespace\), equipped with a norm \(\|.\|\)

%with samples \(\{x_{i}\}_{i=1}^{n}\in\dataspace\), sampled according to some distribution \(P_{\dataspace}\). The data domain \(\dataspace\) is equipped with a metric \(\rho:\dataspace \times \dataspace \rightarrow [0,\infty]\) that measures the dissimilarity between any two points in \(\dataspace\). 
%Consider a dataset \(\dataset\) with n samples \(\{z_{i}\}_{i=1}^{n}\in\dataspace\), sampled according to some distribution \(P_{\dataspace}\). We assume that the samples come from a metric space \((\dataspace, d_{\dataspace})\), where the metric \(d_{\dataspace}:\dataspace \times \dataspace \rightarrow \reals\) is a measure of the dissimilarity between any two points in the dataspace \(\dataspace\). 

\begin{definition}[Metric Differential Privacy \cite{Chatzikokolakis2013, Koufogiannis2016}]\label{defn:mDP}
 Given a metric space \((\dataspace, \rho)\), for any \(\epsilon_{L} \geq 0\), a randomized learner \(\learner: \dataspace^{N} \rightarrow\labelspace\) is \(\epsilon_{L}-\)metric differentially private (mDP) if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\) and \(\forall T\subseteq\labelspace\),
    \begin{equation}
    \prob[\learner(\dataset)\in T]\leq e^{\epsilon_{L}\rho(\dataset, \dataset')}\prob[\learner(\dataset')\in T]
        %\footnote{Note that DP is a special case of lipschitz-DP in the Hamming metric, for \(\rho_{H}(x,x')=1\).}
    \end{equation}
\end{definition}
%    Or, equivalently, if the log-probability function of the corresponding output distributions is \(\epsilon_{L}-\)Lipschitz
%    \begin{equation}
        %|\ln{\prob[\learner(\dataset)\in T]} - \ln{\prob[\learner(\dataset')\in T]}|\leq\epsilon_{L} \rho(\dataset,\dataset')
%    \end{equation}
    
%\end{definition}
\begin{remark}
Note that Definition \ref{defn:mDP} naturally recovers Definition \ref{defn:pure-dp} when $\rho = \rho_{H}(\dataset, \dataset')\leq 1$. Thus, metric differential privacy is a strict generalization of differential privacy. Similar to DP, mDP is preserved under post-processing and composition\cite{Koufogiannis2016}.
%Note that any \(\epsilon-\)mDP learner is \(t\epsilon-\)DP whenever $\rho(\dataset, \dataset') \leq t$. Setting $\rho$ to be the Hamming metric $\rho_{H}$and $t = 1$ naturally recovers Definition \ref{defn:pure-dp}. Thus, metric differential privacy is a strict generalization of differential privacy. Similar to traditional differential privacy, metric differential privacy is preserved under post-processing and composition\cite{Koufogiannis2016}.
\end{remark}
\textbf{Properties of metric spaces}\\
%\begin{definition}[Diameter and Separation of a Set]
%    For any set $\mathcal{Z}$, the \emph{diameter} $\text{diam}(\mathcal{Z}) := \sup_{x,x'\in\mathcal{Z}}\rho(x,x')$ is the maximum distance between points in $\mathcal{Z}$, and the \emph{separation} $\text{sep}(\mathcal{Z}):=\inf_{x\neq x'\in\mathcal{Z}}\rho(x,x')$ is the smallest distance between distinct points in $\mathcal{Z}$. 
%\end{definition}
\begin{definition}[Covering Number\cite{Wainwright2019}] An $\eta$-cover of the set $\mathcal{Z}$ with respect to the metric $\rho$ is a set $\{z_{i}\}_{i\in[N]}$ such that for any point $z\in\mathcal{Z}, \exists$ some index $v\in[N]$ such that $\rho(z,z_{v})\leq\eta$. The $\eta$-covering number $N(\eta, \mathcal{Z}, \rho) := \inf\{N\in\mathbb{N}:\exists \text{ an }\eta\text{-cover } z_{1}, ..., z_{N} \text{ of } \mathcal{Z}\}$
\end{definition}
\begin{definition}[Packing Number\cite{Wainwright2019}] An $\eta$-packing of the set $\mathcal{Z}$ with respect to the metric $\rho$ is a set $\{z_{i}\}_{i\in[M]}$ such that for all distinct $v,v'\in[M]$, we have $\rho(z_{v},z_{v'})\geq\eta$. The $\eta$-packing number $M(\eta, \mathcal{Z}, \rho) := \sup\{M\in\mathbb{N}:\exists \text{ an }\eta\text{-packing } z_{1}, ..., z_{M} \text{ of } \mathcal{Z}\}$
\end{definition}
\subsection{Upper Bounds on Statistical Divergence}
%The privacy definitions listed above impose a constraint on the extent to which a model trained on \(\dataset\) can be dissimilar from a model trained on \(\dataset'\), given that \(\dataset,\dataset'\) belong to the same domain. Naturally, information theoretic interpretations of these definitions would involve measuring the statistical divergence between the output distributions of the randomized learner \(\learner\) when trained on two different datasets $\dataset$ and $\dataset'$. 
In line with the \emph{statistical indistinguishability} interpretation of privacy, we quantify the implications of \(\epsilon\)DP and \(\epsilon_{L}\)mDP on two \(f-\)divergence measures that are extensively used in DP literature, namely, the KL divergence and the R\'enyi divergence. 
\begin{lemma}[KL Upper Bounds\cite{Bun2016,dwork2016concentrated}]
\label{lemma:kl}
    For any \(\epsilon_{L}\)-mDP learner \(\learner\) trained on \(\dataset, \dataset'\) at distance $\rho(\dataset, \dataset')$, the Kullback-Leibler(KL) divergence between the output distributions satisfies the following inequality:
    %\begin{equation}
    %\begin{split}
    %    D_{KL}(\learner(\dataset)||\learner(\dataset')) &\leq\epsilon\DPhyperbolic
    %    \approx \min\left\{\epsilon, \frac{\epsilon^{2}}{2}\right\}
    %\end{split}
    %\end{equation}
    %If the learner is \(\epsilon_{L}\)-mDP, the KL-divergence is bounded above as follows:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset'))& \leq\epsilon_{L}\rho(\dataset, \dataset')\MDPhyperbolic \approx \min\left\{\epsilon_{L}\rho(\dataset, \dataset'), \frac{\epsilon_{L}^{2}\rho(\dataset, \dataset')^{2}}{2}\right\}
    \end{split} 
    \end{equation}
\end{lemma}
%\begin{proof}
%    See Appendix \ref{appendix-l1}
%\end{proof}
\begin{lemma}[R\'enyi Upper Bounds]
\label{lemma:rd}
For any \(\epsilon_{L}\)-mDP learner $\learner$, trained on datasets \(\dataset,\dataset'\) at distance $\rho(\dataset, \dataset')$, the R\'enyi divergence between the output distributions satisfies the following inequality for $\alpha \in (1, \infty)$:
%    \begin{equation}
%        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon, \frac{3\alpha\epsilon^{2}}{2}\right\}
%    \end{equation}
%    If the learner is \(\epsilon_{L}-\)mDP, the Re\'nyi-divergence is bounded above as follows:
    \begin{equation}
        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon_{L}\rho(\dataset,\dataset'), \frac{3\alpha\epsilon_{L}^{2}\rho(\dataset,\dataset')^{2}}{2}\right\}
    \end{equation}
    %\begin{proof}
    %    See Appendix \ref{appendix-l2}
    %\end{proof}
\end{lemma}
For $\epsilon$DP, the above bounds hold with $\rho(\dataset, \dataset') = \rho_{H}(\dataset, \dataset') \leq 1$.
\begin{proof}
    See Appendix \ref{appendix:lemmas} for proofs of both lemmas.
\end{proof}
\section{Problem Formulation}
\label{sec:problem-formulation}
Let \(\advData := \{z_{1}, ..., z_{N-1}\}\) represent a fixed dataset, comprising of $N-1$ training samples. We borrow the privacy game between the learner and the adversary outlined in prior work \cite{Guo2022}(with slight modifications), which proceeds as follows:

% Without loss of generality, we will assume that the domain \(\dataspace = \reals^{(n-1)\times(d+1)}\), and is equipped with the \(l_{2}\) norm.
\textbf{Learner:} The learner chooses an arbitrary challenge sample $\challenge\in\dataspace$, appends it to the training dataset $\datatrain = \advData\cup\{\challenge\}$, and trains the parameters of the model $h\leftarrow\learner(\datatrain)$.\\
\textbf{Adversary:} The adversary receives the fixed dataset $\advData$, and can draw samples $h_{1}, ..., h_{n}$ from the output distribution $\learner(\datatrain)$ to generate a reconstruction $\hat{z}$ of the challenge sample $\challenge$.

%Let \(\challenge \in \dataspace\) be an arbitrary sample chosen by the learner, which we will refer to as the challenge. The training dataset \(\datatrain = \advData\cup\{\challenge\}\) is used by the randomized learner \(\learner\) to train the parameters of the model \(\model\leftarrow\learner(\datatrain)\). %It is generally assumed that both \(\learner\) and \(\model\) are released to the public \cite{Balle2022}.
%For the case of unbiased adversaries, the attack algorithm \(\mathcal{A}\) has access to the trained model \(h\) and \(\mathcal{D}= \mathcal{D}_{train} \setminus \{z\}\), and outputs a reconstruction \(\hat{z}\leftarrow\mathcal{A}(h, \mathcal{D})\) of the target sample \(z\).
%An adversary \(\adv\) then attempts to reconstruct the challenge data point \(\challenge\) with access to the following:
%\paragraph{Fixed dataset}\(\advData\), i.e., all samples in the training set, excluding \(\challenge\)
%\paragraph{Released model}\(\model\), trained on \(\advData\cup\challenge\)
%\paragraph{Training algorithm}\(\learner\)
%\paragraph{Query budget}\(m\), that specifies the number of queries the adversary can make to the learner (and corresponding model) 
\begin{comment}
\begin{itemize}
    \item The fixed dataset \(\advData\), i.e., $N-1$ samples in the training set, excluding \(\challenge\)
    \item Samples from the output distribution $\learner(\datatrain)$
    %\item The training algorithm \(\learner\)
    %\item A fixed query budget \(m\) that specifies the number of queries the adversary can make to the learner
\end{itemize}
\end{comment}
In addition to the above, an \emph{informed} adversary has access to some side knowledge \texttt{aux} about \(\challenge\), possibly acquired from public datasets, via web-scraping, or deduced from the nature of \(\learner\) and \(\advData\). As suggested in \citet{Balle2022}, this side knowledge could be modeled as a collection of \(k\) \emph{shadow} targets \texttt{aux}\(:=\{\Tilde{z_{i}}|i\in[k], \Tilde{z_{i}}\in\dataspace\}\), not contained within the fixed dataset \(\advData\). We assume that the adversary has a finite query budget $m$ that specifies the number of queries the adversary can make to the learner. 

Given the above threat model, it is clear that the only piece of information obscured from the adversary is the challenge \(\challenge\). Thus, the adversary's goal can be reduced to a \emph{parameter estimation problem}, wherein, the distribution \(\learner(\datatrain)\) is parameterized by \(\challenge\). Let \(\modelDist\) denote the class of distributions induced by the output of the randomized learner \(\learner\), i.e., \(\modelDist = \{\learner(\advData\cup\{z\})|z\in\dataspace\}\), supported on some space of models \(\mathcal{H}\). For each \(i\in [k]\), the adversary trains the model \(\Tilde{h_{i}}=\learner(\advData\cup\{\Tilde{z_{i}}\})\) on the fixed dataset \(\advData\) with the \(i\)-th shadow target in their collection of \(k\) shadow targets. We refer to this shadow model-target pair as \(\shadow := \{(\Tilde{h_{i}}, \Tilde{z_{i}})\}_{i=1}^{k}\), which serves as the attack training dataset for the attack model \(\phi\leftarrow\adv(\shadow)\). In doing so, the adversary exhausts a fraction of the initial query budget \(m\). With the remaining \(n = m-k\) query budget, she draws $\{h_{1}, ..., h_{n}\}$ i.i.d. samples from the distribution \(P_{z}\in\modelDist\), which serve as the validation set used to generate $\zhat = \frac{1}{n}\sum_{i=1}^{n}\phi(h_{i})$ %\eqref{eq:zhat}. 
%\satya{We should use $\mathcal{H}$ for range of learning algos in Defns 3.2, 3.3.,3.4, etc.,}
%\begin{equation}\label{eq:zhat}
%    \zhat = \frac{1}%{n}\sum_{i=1}^{n}\phi(h_{i})
%\end{equation}
%\satya{where $h_i$ is the model produced by the $i$-th run of ? on ? . Each $\phi(h_{i})$ produces a candidate $\hat{z} \in \mathbb{R}^{d+1}$. Is the average above coordinate-wise average? Even for DP? }
%The above formulation may be viewed as a privacy game, wherein, the learner (or the defender) and the adversary play against each other.

\subsection{Reconstruction Metric}\label{subsec:metric}
%Throughout this work, we measure the quality of the adversary's reconstruction of the challenge sample in terms of the minimax risk. 
%Informally, the minimax risk is a measure of the smallest risk attainable by any reconstruction adversary.
%\prateeti{TODO: Eucledian metric, just mention MSE and say it can be extended to other metrics}
Given a metric space $(\mathcal{Z}, \rho)$, let $l:[0,\infty)\to[0,\infty)$ be an increasing differentiable function \footnote{For ease of exposition, we assume $l(t) = t^{2}$ and obtain minimax risks for the MSE associated with $\rho$. However, our techniques are general enough to support any differentiable monotonic function.}. The minimax risk is defined as follows:
%Unlike prior work \cite{Guo2022}, our analysis is not restricted to $\reals^{d}$. However, for the purposes of comparison with prior work, we 


%Unless stated otherwise, we will assume that the data domain \(\dataspace^{N} = \reals^{N\times(d+1)}\) is equipped with the \(\ell_{2}\) norm and define the risk as the standard Mean Squared Error(MSE)\footnote{While this is a simplification for consistency with prior work, our techniques are general enough for extension to arbitrary metric spaces, as we shall show in the lower bounds for metric privacy.}. Although the challenge is a fixed sample, \(\zhat\) is a random variable due to the randomness of \(\model\), which in turn implies that \(\|\challenge - \zhat\|_{2}^{2}\) is also a random variable. To obtain a deterministic quantity as a measure of the risk of an estimator, we take the expectation over the randomness of \(\distY\in\modelDist\).
%We first define a metric \(\metric\) on the dataspace \(\dataspace\) to measure how far \(\zhat\) deviates from \(\challenge\), and a non-decreasing function \(\func\) applied on this metric, such that \(\phi(0)=0\). For simplicity (and to remain consistent with prior work \cite{Guo2022}), we set \(\rho(\challenge, \zhat) = |\challenge-\zhat|\), and \(\phi(t) = t^{2}\), i.e., the risk is the mean-squared error. 
%\begin{definition}[Maximum Risk]
%    The maximum risk of the estimator \(\zhat\) in \eqref{eq:zhat} is obtained by choosing a challenge \(\challenge\) that results in the worst candidate reconstruction, as follows:
    %The maximum risk of an estimator \(\zhat\), attempting to reconstruct \(\challenge\), with \(m-k\) samples drawn \(i.i.d.\) from \(\distY\), is defined by
%    \begin{equation}
%        \mathcal{R}_{max}(\zhat) = \sup_{\distY \in \modelDist}\error
%    \end{equation}
%\end{definition}
%\satya{The expectation in $\error$ is over randomness used by training algorithm to produce $h$, namely $h_i$'s?. The parametrized distribution $P$ on models is really $P_z$?}
\begin{definition}[Minimax Risk]
\label{def:minimax-risk}
%The minimax risk of an estimator \(\hat{z}\), drawing k samples from the distribution \(P\) parameterized by \(z\) is defined by
An optimal adversary will minimize the maximum risk incurred when the learner picks a challenge \(\challenge\) that results in the worst candidate reconstruction:
\begin{equation}
    \minimax = \inf_{\hat{z}}\sup_{\distY\in \modelDist}\expctn[l(\rho(z, \zhat))]
\end{equation}
where, the \(\sup\) is taken over distributions \(\distY\in\modelDist\) parameterized by \(\challenge\), and the \(\inf\) is over all reconstruction attacks $\hat{z}$ with \(n\) samples drawn according to \(\distY\). \footnote{If the metric space is equipped with the \(\ell_{2}\) norm with $\mathcal{Z}\in\reals^{d}$, the RHS reduces to the Euclidean Mean Squared Error(MSE) analysed in prior work \cite{Guo2022}.}
%\satya{Shouldn't the $\inf$ be over all adversaries? Each adversary defines a $\phi$ and a $\hat{z}$, right?}
\end{definition}

This choice of metric captures the privacy game outlined in Section \ref{sec:problem-formulation}. In essence, for each adversary, the learner finds a challenge point \(\challenge\) where the candidate reconstruction \(\zhat\) is farthest from \(\challenge\), as observed in $\sup_{P_{z}\in\modelDist}$. The attacks $\hat{z}$ are ranked in order of their worst-case performance and the adversary picks the optimal that minimizes the worst-case risk, as suggested by the $\inf_{\hat{z}}$ term. Lower bounds on this quantity imply that no reconstruction adversary can achieve a risk lower than the minimax risk on every distribution in \(\modelDist\), parameterized by \(\challenge\).
\section{Main Results}
\label{sec:main-results}
In this section, we establish lower bounds on the resilience of any \((\epsilon, \delta)\)-DP learner and \((\epsilon_{L}, \delta)\)-mDP learner against adversaries of the form described in Section \ref{sec:problem-formulation}.
%\subsection{Analysis in the low dimensional regime}
\begin{theorem}[Differential Privacy]
\label{thm:dp-lecam}
    Let $(\mathcal{Z}, \rho)$ be any compact metric space. For any challenge sample $z\in\mathcal{Z}$ in the training set of an  \((\epsilon, \delta)\)-DP learner, a reconstruction adversary that draws $n$ samples from the learner's output distribution incurs a minimax risk of:
    \begin{equation}
        \minimax \geq C diam(\dataspace)^{2}e^{-n\epsilon \tanh\left(\frac{\epsilon}{2}\right)}(1-\delta)
    \end{equation}
    where, \(diam(\dataspace) = \sup_{z,z'\in\dataspace}\rho(z,z')\)
\end{theorem}
\begin{proof}
    We defer the full proof to Appendix \ref{appendix:main-results}. 
\end{proof}
We show in Appendix \ref{appendix:main-results} that this bound is tight (upto constant factors) in the high privacy regime.
 \paragraph{Query complexity} We discuss the dependence on the query complexity through the lens of an adversary. According to the threat model described in Section \ref{sec:problem-formulation}, the adversary's query budget is split between the creation of the attack training dataset $k$ and the sampling process for reconstruction $n$. One might argue that a large shadow training dataset implies better reconstruction, and that the natural choice would be to exhaust the entirety of the query budget in preparing the shadow training dataset, i.e., $n=1$. However, we note that preparing the shadow training dataset is expensive, since the adversary trains \(\learner(\dataset\cup\{\tilde{z_{i}}\})\) for every shadow target in \texttt{aux}. Furthermore, in practice, the adversary obtains \texttt{aux} through arbitrary sources; it might simply not be possible for the adversary to obtain a large number of shadow targets. We observe that the reconstruction error drops quickly as the number of samples drawn from the output distribution $P_{z}$ is increased. This is expected, owing to the reduction to parameter estimation. Our result emphasizes that even in the absence of a large attack training set, it is possible to produce good candidate reconstructions by sampling from the output distribution.
%\satya{Assuming $m \geq k$? How does such an optimal separation look like? Why not choose $m=k$ so the exponential dependency disappears?} 

\paragraph{Extension to R\'enyi DP} A key step in our proof of Theorem \ref{thm:dp-lecam} is to bound the KL-divergence between the two output distributions of the $\epsilon$-DP algorithm by applying Lemma \ref{lemma:kl}. To this end, we highlight that our analysis can be straightforwardly extended to $(\alpha, \epsilon)$-R\'enyi DP for any $\alpha \geq 1$ by using the comparison inequality $KL(P||Q) \leq D_{\alpha}(P||Q)$. However, as is evident by comparing Lemma \ref{lemma:kl} and \ref{lemma:rd}, the obtained result would be the same as Theorem \ref{thm:dp-lecam} modulo some constant factors. In particular, the upper bound on \(D_{KL}(\learner(\dataset)||\learner(\dataset'))\) coincides with the upper bound on \(D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\) for \(\epsilon \geq 1\), and varies by a multiplicative factor in terms of $\alpha$ for \(\epsilon < 1\). We discuss this extension further in Appendix \ref{appendix:main-results}.


\paragraph{Comparison with prior work} To the best of our knowledge, the result closest to Theorem \ref{thm:dp-lecam} that has appeared in prior work is Theorem 1 of \citet{Guo2022}, which considers the special case of $(2, \epsilon)$-R\'enyi DP. For sake of completeness, we restate their result and highlight its differences from Theorem \ref{thm:dp-lecam} as follows:
\begin{theorem}[Theorem 1 \cite{Guo2022}]\label{thm:guo-2022}
    If the learner \(\learner\) is \((2,\epsilon)-\)RDP, and the reconstruction attack outputs \(\hat{z}(h)\) upon observing \(h\leftarrow\learner(\dataset)\), then, if $\zhat$ is unbiased,  
    \begin{equation}
        \expctn[\|\hat{z(h)}-z\|^{2}_{2}] \geq C\frac{\sum_{i=1}^{d}diam_{i}(\dataspace)^{2}}{(e^{\epsilon}-1)}
    \end{equation}
    where \(diam_{i}(\dataspace) = \underset{z,z'\in\dataspace, z_{j} = z'_{j}\forall j\neq i}\sup|z_{i}-z_{i}'|\), and $C$ is a universal constant.
\end{theorem}
The differences between the above result of \citet{Guo2022} and Theorem \ref{thm:dp-lecam} of this paper are listed below.
%While our bound exhibits a dependence on \(diam(\dataspace)^{2}\), the lower bound in \cite{Guo2022} requires a sum over the squared coordinate-wise diameter. This results in arbitrarily loose bounds, even with a bounded diameter assumption.
%\begin{enumerate}

\textbf{1. Invalid lower bounds:} Upon investigating the validity of the lower bound admitted by Theorem \ref{thm:guo-2022}, we find that the bound is invalid for any \(\epsilon < \ln{(1 + \frac{d}{4})}\). Therefore, when $d = 784$, as is true for simple practical applications such as MNIST \cite{Lecun1998} digit classification, their lower bound violates trivial upper bounds for $\epsilon < 5.28$. On the contrary, our result for $(\epsilon, \delta)$-DP is tight (up to constant factors) in the high privacy regime ($\epsilon\to0$). The analysis is discussed in detail in Appendix \ref{appendix:main-results}.
%Therefore, even for the low value of $d = 100$, Theorem \ref{thm:guo-2022} only holds for $\epsilon \geq 3.26$. On the contrary, prior work \cite{Humphries2020, Yeom2018} suggest that, even under $\epsilon$DP, if $\epsilon = 3.26$, an adversary can correctly predict the presence of an individual record through the output of the learner with a success probability of over 98\%, effectively reducing to non-privacy.
%For small \(\epsilon\) \footnote{A more precise terminology would be Levenshtein distance, but they are often used interchangeably} 

\textbf{2. Extension beyond $\reals^{d}$:} Theorem 1 of \citet{Guo2022} relies on the assumption that the coordinate wise diameter $\text{diam}_{i}(\mathcal{Z})$ is well defined. While this is true for $\mathcal{Z}\subseteq\reals^{d}$, we note that this assumption is quite restrictive. For example, let $\mathcal{Z}$ be a collection of strings of varying lengths (where the maximum length is finite), equipped with the edit distance metric. The coordinate-wise edit distance is the maximum difference between corresponding characters of any two strings of the same length, which is not well-defined when $\mathcal{Z}$ is composed of strings with variable length. Therefore, their lower bound does not apply to this setting. On the contrary, our result is still valid in this case, since $\text{diam}(\mathcal{Z})$ is well-defined and represents the maximum dissimilarity between \emph{any} two strings in $\mathcal{Z}$.
%We note that their results are restricted to the case when $\mathcal{Z}\in\reals^{d}$. This is due to the fact that the coordinate wise diameter $diam_{i}(\mathcal{Z})$ is only defined when the alphabet of the input space is the set of reals, where the notion of absolute difference exists in every coordinate. On the contrary, our lower bounds are defined for arbitrary metrics, provided $diam(\mathcal{Z})$ is finite.   
%Our result is non-asymptotic in the sense that it exhibits an explicit dependence on the query complexity of the adversary, which is absent in \ref{thm:guo-2022}.
%\item We illustrate the improvement conferred by our result in Figure \ref{fig:compare}.
%\end{enumerate}
\paragraph{Compact metric spaces} In line with prior work \cite{Yeom2018, Tramer2020, Guo2022}, our lower bounds for the protection offered by traditional DP algorithms rely on the assumption that the domain \(\dataspace\) is bounded, i.e., \(diam(\dataspace)\) is finite, and the input metric space is compact. We observe that this assumption cannot be avoided in the analysis of traditional DP algorithms without making assumptions on the either the private learner, the data distribution, or the adversary's reconstruction model. This is due to the fact that the standard notion of differential privacy is oblivious to the underlying metric structure of the input space, yielding worst-case behavior. In what follows, we remove this boundedness assumption and extend the analysis to arbitrary metric spaces.
\subsection{Extension to unbounded metric spaces}
%\end{definition}
%\paragraph{Assumption} We assume that the input space $\mathcal{Z}$ is endowed with any arbitrary metric $\rho$, such that log covering number of metric space $(\mathcal{Z}, \rho)$ satisfies $\log(N())$
\begin{theorem}[\((\epsilon, \delta)\)-mDP]
\label{thm:mdp-lecam}
Let $(\mathcal{Z}, \rho)$ be any locally compact metric space. For any challenge \(\challenge\in\mathcal{Z}\) in the training set of an \((\epsilon_{L}, \delta)\)-mDP learner, a reconstruction adversary with $n$ samples from the output distribution incurs a minimax risk of:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
        \minimax \geq \frac{(1-\delta)}{2ne\epsilon_{L}^{2}} 
    \end{equation}
\end{theorem}
\begin{proof}
    See Appendix \ref{appendix:main-results}.    
\end{proof}
In this case, we observe an inverse quadratic dependence on the privacy parameter \(\epsilon_{L}\) as opposed to the inverse exponential drop observed in Theorem \ref{thm:dp-lecam}. Despite this improvement, the bounds may be sub-optimal if the challenge \(\challenge\) is high dimensional. %We notice that the data dimensionality is not captured in this result since our analysis holds for the classical case, where the implicit assumption is that \(d \ll n\).
In practice, this is often found to be true, and the influence of dimension is apparent in a variety of estimation problems \cite{Bishop2006, Wainwright2019}. %A very dramatic consequence of dimension is observed in regression problems, where the difficulty in minimizing the MSE is extremely difficult in higher dimensions as a result of the \emph{curse of dimensionality} \cite{Bishop2006, Wainwright2019}.
%Since training data reconstruction is, in essence, a form of parameter estimation, it is important to investigate non-asymptotic lower bounds on the MSE of reconstruction in the high-dimensional regime. 
According to the threat model defined in Section \ref{sec:problem-formulation}, the adversary is essentially solving a parameter estimation problem, operating under a restricted query budget \(m\). From a practical point of view, it is quite likely that the number of samples drawn from the output distribution $n$ ($\leq m$) is less than the input dimension $d$. We explicitly capture this dependence in arbitrary metric spaces by extending the analysis to cover the high dimensional regime.
\begin{theorem}[High Dimensional Regime]
\label{thm:mdp-fanos}
Let $\mathbb{B}(\mathcal{Z})$ denote the unit ball in the locally compact metric space $(\mathcal{Z}, \rho)$ such that $N(1/2, \mathbb{B}(\mathcal{Z}), \rho)<\infty$. For any challenge $z$ in the training set of an $(\epsilon, \delta)$-mDP learner, the adversary's minimax risk of reconstruction satisfies:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
    \begin{split}
     %\minimax &\geq \Omega\left(\max\left\{\frac{d}{(m-k)\epsilon^{2}}, \frac{d^{2}}{(m-k)^{2}\epsilon^{2}}\right\}\right)
     \minimax \geq \Omega\left(\frac{\tilde{d}(1-\delta)}{n\epsilon_{L}^{2}}\right),
    \end{split}
    \end{equation}
where, $ \tilde{d} = \ln N(1/2, \mathbb{B}(\mathcal{Z}), \rho)$, i.e., the log covering number of the unit ball in $\mathcal{Z}$.
\end{theorem}
\begin{corollary}
\label{cor:normed-space}
    In particular, if $(\mathcal{Z}, \rho)$ is a normed space of finite dimension $d$, the lower bound reduces to 
    \begin{equation}
        \minimax \geq \Omega\left(\frac{d (1-\delta)}{n\epsilon_{L}^{2}}\right)
    \end{equation}
\end{corollary}
\begin{proof}
    See appendix \ref{appendix:main-results}.    
\end{proof}
In light of Corollary \ref{cor:normed-space}, we call $\tilde{d}$ of Theorem \ref{thm:mdp-fanos} the effective dimension. However, we note that our result applies to arbitrary infinite dimensional locally compact metric spaces. 
%\paragraph{Existence of $\tilde{d}$} Any locally compact metric space
\paragraph{Dimension dependence} The explicit dimension dependence exhibited in the lower bound of Theorem \ref{thm:mdp-fanos} and Corollary \ref{cor:normed-space} complements the empirical observations of Balle et~al.\cite{Balle2022}, where, from the lens of an adversary, the problem of data reconstruction on a private learner is much harder in high dimensional settings, given that each challenge sample \(\challenge\) has \(d-\) degrees of freedom. Furthermore, we note that the coordinate-wise diameter $\sum_{i}^{d}diam_{i}(\dataspace)^{2}$ is typically $O(d)$. Therefore, for $\epsilon = \epsilon_{L}, n = 1$ this result is quantitatively better than that of prior work \cite{Guo2022} since $\frac{d}{t^{2}}\geq \frac{d}{e^{t}-1}$ for $t>0$.
\paragraph{Reduction to Theorem \ref{thm:mdp-lecam}} Theorem \ref{thm:mdp-fanos} can be interpreted as a generalization of Theorem \ref{thm:mdp-lecam} for the high-dimensional regime since it recovers Theorem \ref{thm:mdp-lecam} for $\tilde{d} = O(1)$.

\section{Empirical Analyses}\label{sec:experiments}

We empirically evaluate the tightness of our bounds for differentially private and metric differentially private algorithms against the informed reconstruction adversary of Balle et.~al.\cite{Balle2022}. We defer additional experiments to Appendix \ref{appendix:experiments}

\textbf{Training} We reproduce the experiments of Guo et.~al.\cite{Guo2022} for private linear logistic regression. The regressor is tasked with binary MNIST \cite{Lecun1998} classification of digits 0 v/s 1. The training dataset comprises of $N = 60,000$ samples of dimension $d = 784$, in addition to a discrete label $y\in\{0,1\}$, with $10,000$ samples reserved for validation.. Consistent with prior work \cite{Guo2022}, we treat the label as public information and only seek the reconstruct the image $x\in[0,1]^{784}$. 

\textbf{Privacy Accounting} We treat the privacy accounting strategy for differentially private and metric differentially private learners separately. For $\epsilon$-DP, the linear logistic regressor is trained privately via output perturbation \cite{Chaudhuri2008}, wherein, noise is drawn from the Laplace distribution with mean $0$ and $\sigma = 2/N\epsilon\lambda$. Since privacy is attained using the Laplace mechanism, the output perturbation technique can be adapted to arbitrary metrics, with appropriate scaling \cite{Chatzikokolakis2013}. Therefore, we use the scaling factor proposed by \citet{Chatzikokolakis2013} for the Euclidean metric to train the linear logistic regressor with $\epsilon$-mDP. %We set $\lambda = 5 \times 10^{-4}$ and learning rate = $0.3$ for both models.

\textbf{Reconstruction Adversary} We implement the reconstruction attack proposed by \citet{Balle2022} for Generalized Linear Models (GLMs). In line with the experiments of \citet{Guo2022}, we further strengthen the attack by supplying the label $y$ of the challenge $z=(x,y)$ to the adversary, in addition to $D_{-}$. For ease of comparison, we set $n = 1$. 

\textbf{Observations} Under differential privacy, we plot the empirical reconstruction MSE against the lower bounds of \citet{Guo2022}(Theorem 1) and Theorem \ref{thm:dp-lecam} of this paper. We note that the input dimensionality of the MNIST dataset is $784$. Therefore, given that the lower bound of prior work is vacuous for any value of $\epsilon < \ln\left(1 + \frac{d}{4}\right)$, for $d = 784$, their lower bound violates trivial upper bounds for $\epsilon < 5.28$. This is apparent in Figure \ref{fig:lr-dp}, where the empirical upper bound falls below the lower bound of \citet{Guo2022} for $\epsilon < 5$. On the contrary, the lower bound in Theorem \ref{thm:dp-lecam} of this paper is tight in the high privacy regime (as evident in Figure \ref{fig:lr-dp}), but decays exponentially with the privacy parameter $\epsilon$. Under metric differential privacy, we observe from Figure \ref{fig:lr-mdp} that the lower bound of Theorem \ref{thm:mdp-fanos} lies strictly above that of Theorem \ref{thm:mdp-lecam}, and is highly indicative of the empirical MSE. %This is expected, since the input data is high-dimensional. 
\begin{figure}\label{fig:lr}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/recons_mse_dp_final.pdf}
         \caption{DP (includes lower bound from prior work \cite{Guo2022})}
         \label{fig:lr-dp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/recons_mse_mdp_final_neurips.pdf}
         \caption{Metric DP}
         \label{fig:lr-mdp}
     \end{subfigure}
     \caption{Plot of empirical reconstruction MSE for $\epsilon\in[0.1,5)$ compared against theoretical lower bounds for (a) $\epsilon$-DP and (b) $\epsilon$-mDP learners. The solid lines represent the average over 50 runs and the shaded regions are 95\% confidence intervals. The y-axes are on the logarithmic scale.}
\end{figure}

%It is apparent from these results that lower bounds on the minimax rate of reconstruction error for mDP learners are quantitatively superior to their DP counterparts. This is expected, since minimax lower bounds often require the construction of packing sets contained in the parameter space, and careful selection of the separation between the elements is crucial for the tightness of the bound. 
While empirical validation confirms that mDP is a more natural choice for resilience against reconstruction adversaries, it is important to understand the applicability of this notion in private deep learning. To this end, we complement our lower bound analysis by a) demonstrating that noise mechanisms in DP literature can be straightforwardly extended to mDP (see Appendix \ref{appendix:mechanism-extensions}), and b) proving that, under mild assumptions, mDP accounting for PNSGD\cite{Feldman2018} improves the Gaussian noise parameter by a factor of $\text{diam}(\mathcal{Z})$.
\section{Metric DP Accounting in Noisy SGD}

We first establish the requisite framework for analyzing mDP in practice. We emphasize that, since mDP is a generalization of DP, practical applications would require a relaxation similar to \ref{defn:rdp} in order to establish privacy guarantees. To this end, we consider analogous relaxations of metric differential privacy and their properties in Appendix \ref{appendix:mechanism-extensions}. Since the privacy guarantees of noisy stochastic gradient descent algorithms often rely on the analysis of the Gaussian mechanism, we propose the following formulation for mDP.
\begin{proposition}[Gaussian Mechanism for mDP]
\label{prop:gaussian-lp}
    Consider any two datasets \(\dataset, \dataset' \in \dataspace\), at a distance \(\rho(\dataset,\dataset')\) and an input lipschitz function \(f:\dataspace\rightarrow\reals^{d}\). For any \(\delta \in (0,1)\) and \(c^{2}>\ln(1.25/\delta)\), the Gaussian Mechanism with parameter \(\sigma \geq cL_{input}/\epsilon_{L}\) is \((\epsilon_L,\delta)\)-mDP, where  \(L_{input}\) is the input Lipschitz constant of \(f\), i.e., $ \|f(\dataset)-f(\dataset')\|\leq L_{input}\rho(\dataset,\dataset')$.
\end{proposition}
\begin{proof}
    See Appendix \ref{appendix:propositionPNSGD}. 
\end{proof}
A popular approach to differentially private deep learning involves an extension of Stochastic Gradient Descent (SGD) to incorporate gradient clipping and noisy gradient updates. The resulting algorithm is called Differentially Private SGD (DP-SGD)\cite{Abadi2016}, wherein, $(\epsilon, \delta)$ differential privacy is guaranteed via an application of the Gaussian mechanism at each iteration. Since mDP is a generalization of traditional DP, we expect that iterative DP algorithms that rely on additive noise at each intermediate solution step are often inherently mDP, owing to the application of composition theorems that admit natural extensions to mDP. For algorithms that rely on contractive noisy iterations without releasing the intermediate solutions, such as Projected Noisy Stochastic Gradient Descent \cite{Feldman2018}, we propose the following privacy accounting strategy for metric differential privacy.

\begin{proposition}
\label{prop:pnsgd-mdp}
    Let $\cK \subset \mathbb{R}^d$ be a convex set and let $\{ f(., x) \}_{x \in cX}$ be a family of convex, $\beta$-smooth functions over $\cK$, where the gradients are $L_{input}$-input Lipschitz. Furthermore, assume $\cX$ is a bounded set. Then, for any $\eta \leq 2/\beta$ and $\alpha > 1$, initializing $w_0 \in \cK$ and dataset $S \in \cX^{n}$, PNSGD run with $\sigma^{2} \geq \frac{2 \alpha L^{2}_{input} diam(\cX)}{\epsilon_L (n- t + 1)}$ satisfies $(\alpha, \epsilon_{L})$-R\'enyi mDP.
\end{proposition}
\begin{proof}
    See Appendix \ref{appendix:propositionPNSGD}
\end{proof}
\begin{corollary}\label{cor:pnsgd-cor}
For a large class of functions, Proposition \ref{prop:pnsgd-mdp} requires less perturbation for $(\alpha, \epsilon_{L})$-R\'enyi mDP than that required for $(\alpha, \epsilon)$-R\'enyi DP. 
%in Feldman et.~al.\cite{Feldman2018}.
\end{corollary}
\begin{proof}
    For a detailed statement and proof, see Appendix \ref{appendix:propositionPNSGD}
\end{proof}
\section{Conclusion and Limitations}
In this work, we performed an information-theoretic analysis on the effectiveness of training data reconstruction attacks against private learners, and derived non-asymptotic minimax lower bounds on the reconstruction error of an adversary. We removed restrictive assumptions prevalent in prior work, where the analysis is limited to $\reals^{d}$, and demonstrated that our results cover general locally compact metric spaces, thereby enabling privacy analysis in unstructured data domains such as text. While our analysis does not impose any assumptions on the data distribution, private learner or the adversary, we believe that it may be possible to obtain sharper instance-specific rates by considering particular classes of data distributions and noise mechanisms for randomized learners, and modeling functions for the adversary. However, given the highly complex nature of real-world data distributions, model architectures and attack paradigms, we conjecture that any analysis making specific distributional (and modelling) assumptions would have limited applicability. 
%lower bounds for DP learners that can alleviate the diameter dependence by considering specific classes of data distributions, randomized learners and adversaries. However, given the highly complex nature of real-world data distributions, model architectures and attack paradigms, we believe that any analysis making specific distributional (and modelling) assumptions would have limited applicability. 
%Promising avenues of future work include examining the tightness of the obtained lower bounds for specific instances, and investigating, from the lens of an adversary, whether the problem of data reconstruction from a private model admits any fundamental statistical-computational tradeoffs. 
%Our analysis is highly general 
%encompassed various notions of privacy, with extensions to 
%Our analysis encompassed various notions of privacy, such as pure DP, R\'enyi DP, and metric DP. Our framework also captured the increased difficulty of the problem (from the perspective of an adversary) in the high dimensional setting, a phenomenon which was previously only noted empirically. Finally, we also established that (approximate) metric differential privacy guarantees can be easily ensured in noisy SGD.  
%\paragraph{Limitations} We note that our analysis does not impose any assumptions on the data distribution, private learner or the adversary. we conjecture that it may be possible to obtain instance-specific lower bounds for DP learners that can alleviate the diameter dependence by considering specific classes of data distributions, randomized learners and adversaries. However, given the highly complex nature of real-world data distributions, model architectures and attack paradigms, we believe that any analysis making specific distributional (and modelling) assumptions would have limited applicability.


%While metric differential privacy is, in theory, a more natural choice for resilience against reconstruction adversaries, it is important to understand the applicability of this notion in practice, particularly in the context of deep learning. In the following section, we complement our lower bound analysis by designing mDP accountants for Noisy Stochastic Gradient Descent(SGD) algorithms.
 
 



%The adversaries are ranked in order of their worst-case performance, 
%This process is repeated for all adversaries, ranking them in order of their worst-case performance, and pick the optimal adversary that minimizes this risk. Lower bounds on this quantity imply that no reconstruction adversary with a query budget of \(m\) and prior knowledge of \(k\) shadow targets can achieve a risk lower than the minimax risk on every distribution in \(\modelDist\), parameterized by \(\challenge\).
%For metric DP, we observe that our lower bounds exhibit a quadratic drop in the reconstruction error with respect to the privacy parameter $\epsilon$, in both the classical regime as well as high dimensional regime, thereby obtaining a substantial improvement over the exponential drop observed in the lower bound analysis of (pure) differentially private learners and R\'enyi private learners in prior work \cite{Guo2022}. %We accompany these theoretical findings with an extensive set of experiments to empirically validate the tightness of our bounds.

%We complement our findings regarding the theoretical improvements conferred by metric differential privacy by demonstrating that most commonly used privacy-inducing mechanisms in the differential privacy literature can be easily extended to accommodate for mDP. To this end, we show that the privacy analysis of the Gaussian Mechanism, Differentially Private Stochastic Gradient Descent (DP-SGD) \cite{Abadi2016} and Projected Noisy Stochastic Gradient Descent (PN-SGD) \cite{Feldman2018} admit natural extensions to metric DP. 

%\prateeti{contractive noisy updates PN-SGD, mild assumptions on }


%Furthermore, for PN-SGD, we demonstrate that the requisite noise level for metric DP is less than that of approximate DP by a factor of $diam(\mathcal{X})$, where $\mathcal{X}$ is the input data domain.





%if the input data dimensionality $d=100$, the lower bound of \citet{Guo2022} is only valid for $\epsilon\geq 3.26$


%While the analyses of DRAs  by Guo et~al\cite{Guo2022} considers only the asymptotic setting and is restricted to relatively weaker notions of privacy (such as $(2, \epsilon)-$ R\'enyi DP) for large $\epsilon$, 


%\subsection{Contributions}\label{subsec:contributions}
%Our work aims to understand the data protection offered by private learners against training data reconstruction by means of an information-theoretic analysis. Our contributions are summarized as follows.

%\paragraph{Non-asymptotic lower bounds for general privacy accountants}

%We present the first non-asymptotic analysis of the effectiveness of training data reconstruction by \emph{informed} adversaries against private learners. We do so by deriving minimax lower bounds on the reconstruction accuracy of \emph{any adversary} as a function of the privacy parameter of the private algorithm and the query complexity of the adversary. The non-asymptotic setting is of particular importance to randomization-based notions of privacy such as differential privacy, wherein, querying the algorithm on a fixed dataset yields varied outcomes. While the analyses of DRAs  by Guo et~al\cite{Guo2022} considers only the asymptotic setting and is restricted to relatively weaker notions of privacy (such as $(2, \epsilon)-$ R\'enyi DP) for large $\epsilon$, our non-asymptotic analysis is highly general and covers stronger notions of privacy such as pure DP and $(\alpha, \epsilon)-$ R\'enyi DP for any $\alpha \geq 1, \epsilon \geq 0$. In fact, our analysis goes beyond pure differential privacy and considers the notion of \emph{Metric Differential Privacy} or \emph{Lipschitz Privacy} \cite{Chatzikokolakis2013, Koufogiannis2015, Koufogiannis2016, Imola2022} -- a formalism that generalizes differential privacy by accounting for the underlying metric structure of the data beyond the Hamming metric.




%We present the first known analysis of privacy preservation against data reconstruction attacks in the non-asymptotic setting. This is of particular importance to randomization-based notions of privacy, such as differential privacy, where, querying the learner on a fixed dataset yields varied outcomes. Furthermore, while prior work has derived asymptotic lower bounds under restrictive assumptions on privacy\cite{Guo2022}, our analysis holds for more general privacy accountants such as $\epsilon$-

%\paragraph{Improved dependence on privacy parameter and extension to high-dimensional regime}

%Most modern applications of machine learning involve high dimensional datasets, where the data dimension is of the same order as (and sometimes larger than) the available number of samples. In such settings, most statistical algorithms exhibit a significant loss in performance, a behavior generally attributed to the \emph{curse of dimensionality} \cite{Bishop2006}. Thus, from the perspective of the adversary, we intuitively expect the task of training data reconstruction to be harder in the high dimensional regime, particularly when the query budget of the adversary is finite. While this intuition is confirmed by thorough empirical evaluation in Balle et~al\cite{Balle2022}, existing works on the theoretical analysis of training data reconstruction do not capture the inherent hardness of the problem in high dimensions. To this end, we focus on metric DP and extend our lower bound analysis to the high dimensional regime, obtaining lower bounds of the form $\frac{d}{n \epsilon^2}$ for a query budget of $n$. For metric DP, we observe that our lower bounds exhibit a quadratic drop in the reconstruction error with respect to the privacy parameter $\epsilon$, in both the classical regime as well as high dimensional regime, thereby obtaining a substantial improvement over the exponential drop observed in the lower bound analysis of (pure) differentially private learners. We further demonstrate that these bounds are minimax optimal under certain regimes. We accompany these theoretical findings with an extensive set of experiments to empirically validate the tightness of our bounds.

%\paragraph{Metric DP accountant for noisy SGD} We complement our findings regarding the theoretical improvements conferred by metric differential privacy by demonstrating that most commonly used privacy-inducing mechanisms in the differential privacy literature can be easily extended to accommodate for mDP. To this end, we show that the privacy analysis of the Gaussian Mechanism, Differentially Private Stochastic Gradient Descent (DP-SGD) \cite{Abadi2016} and Projected Noisy Stochastic Gradient Descent (PN-SGD) \cite{Feldman2018} can be extended to metric DP. Furthermore, for PN-SGD, we demonstrate that the requisite noise level for metric DP is less than that of approximate DP by a factor of $diam(\mathcal{X})$, where $\mathcal{X}$ is the input data domain.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{references}
\bibliographystyle{abbrvnat}
\newpage

\appendix
%\section{Contents}\label{appendix:organization}
\tableofcontents
\section{Broader Impact}
While this work is primarily theoretical in nature, we believe our technical findings offer valuable insights that are of relevance in the development of privacy-preserving algorithms and equip researchers and practitioners with essential knowledge to facilitate informed modeling and design of robust privacy solutions. To the best of our knowledge, we do not foresee any negative social impact.
\section{Organization}
In Appendix \ref{appendix:experiments}, we present additional empirical analyses. In Appendix \ref{appendix:lemmas}, we discuss useful properties of metric spaces and present the deferred proofs on upper bounds on statistical divergence measures for differential privacy and metric differential privacy. We then present the proofs of the main results in Appendix \ref{appendix:main-results}. In Appendix \ref{appendix:mechanism-extensions}, we present extensions of paradigms prevalent in classical DP literature to metric DP. We then discuss the privacy analysis of the proposed metric DP accountant for Projected Noisy Stochastic Gradient Descent in Appendix\ref{appendix:propositionPNSGD}. Finally, we present a literature review in Appendix \ref{appendix:related-literature}. 

\section{Additional Experiments}\label{appendix:experiments}
In this section, we present additional experiments to evaluate the tightness of our analysis. 

\paragraph{Private Training with PN-SGD} We train a private logistic regression model for MNIST 0 v/s 1 classification with Projected Noisy Stochastic Gradient Descent (Algorithm 1 of \citet{Feldman2018}, restated in Algorithm \ref{alg:pnsgd}). The training dataset comprises of $N = 60,000$ samples of dimension $d = 784$, in addition to a discrete label $y\in\{0,1\}$, with $10,000$ samples reserved for validation. Consistent with prior work \cite{Guo2022}, we treat the label as public information and only seek the reconstruct the image $x\in[0,1]^{784}$. We normalize the data to have bounded $\ell_{2}$ norm and do not clip gradients, in line with the implementation of \citet{abadi2016tensorflow}, and refer the interested reader there for additional details.

\paragraph{Privacy Accounting} For $(\epsilon, \delta)$-DP, we apply the R\'enyi DP bound for contractive noisy iterations in \citet{Feldman2018}, and obtain appropriate position-dependent $(\epsilon, \delta)$-DP guarantees. For metric differential privacy, we use the bound proposed in Proposition \ref{prop:pnsgd-mdp} of this paper. Privacy is attained via the addition of Gaussian noise, with earlier inputs enjoying stronger privacy guarantees on account of the noise injected in subsequent steps. 

\paragraph{Reconstruction Adversary}  We implement the reconstruction attack proposed in \citet{Balle2022} for Generalized Linear Models (GLMs). In line with the experiments of \citet{Guo2022}, we further strengthen the attack by supplying the label $y$ of the challenge $z=(x,y)$ to the adversary, in addition to $D_{-}$. For ease of comparison, we set $n = 1$. 

\paragraph{Observations} 
Under differential privacy, we plot the empirical reconstruction MSE against the lower bounds of Guo et.~al.\cite{Guo2022}(Theorem 1) and Theorem \ref{thm:dp-lecam} of this paper.  We observe from Figure \ref{fig:pnsgd-dp} that the curve for the empirical upper bound lies under the lower bound of Guo et.~al. for $\epsilon \in [0.5, 2)$. On the contrary, the lower bound in Theorem \ref{thm:dp-lecam} of this paper does not violate the empirical upper bound. However, the curve suggests that the bound is relatively loose when the logistic regression algorithm is trained with PN-SGD. In the mDP setting, we observe from Figure \ref{fig:pnsgd-mdp} that the lower bound of Theorem \ref{thm:mdp-fanos} is highly indicative of the empirical upper bound. This could be attributed to the fact that mDP accounting better captures the influence of contractive noisy iterations in PN-SGD. 
%This could be attributed to the fact that PN-SGD accounting allows for a tighter privacy analysis in comparison to the output perturbation algorithm studied in Section \ref{sec:experiments}. 
%On the contrary, the lower bound in Theorem \ref{thm:dp-lecam} of this paper 
%is tight in the high privacy regime (as evident in Figure \ref{fig:lr-dp}), but decays exponentially with the privacy parameter $\epsilon$. Under metric differential privacy, we observe from Figure \ref{fig:lr-mdp} that the lower bound of Theorem \ref{thm:mdp-fanos} lies strictly above that of Theorem \ref{thm:mdp-lecam}, and is highly indicative of the empirical MSE.
%\prateeti{TODO}
\begin{algorithm}[h]
   \caption{Projected Noisy Stochastic Gradient Descent (Algorithm 1 of \citet{Feldman2018})}
   \label{alg:pnsgd}
\begin{algorithmic}
   \State {\bfseries Input:} Dataset $S = \{x_{1}, ..., x_{n}\},  f: \mathcal{K}\times\mathcal{X}\rightarrow \reals$, which is a convex function in the first parameter, learning rate $\eta$, starting point $w_{0}\in\mathcal{K}$, noise parameter $\sigma$
   \For{$t\in\{0, ..., n-1\}$}
   \State  $v_{t+1}\leftarrow w_{t} - \eta(\nabla_{w}f(w_{t}, x_{t+1})+Z)$, where $Z\sim\mathcal{N}(0,\sigma^{2}\mathbb{I}_{d})$.
   \State  $w_{t+1}\leftarrow\prod_{\mathcal{K}}(v_{t+1})$, where $\prod_{\mathcal{K}}(w)=\arg\min_{\theta\in\mathcal{K}}\|\theta-w\|_{2}$ is the $l_{2}$ projection on $\mathcal{K}$
   %\FOR{$i=1$ {\bfseries to} $m-1$}
   %\IF{$x_i > x_{i+1}$}
   %\STATE Swap $x_i$ and $x_{i+1}$
   \EndFor
   \State return the final iterate $w_{n}$ 
   %\ENDIF
   %\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{figure}\label{fig:lr}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/recons_mse_pnsgd_dp_neurips_final.pdf}
         \caption{DP (includes lower bound from prior work \cite{Guo2022})}
         \label{fig:pnsgd-dp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/recons_mse_pnsgd_neurips_mdp_final.pdf}
         \caption{Metric DP}
         \label{fig:pnsgd-mdp}
     \end{subfigure}
     \caption{Plot of empirical reconstruction MSE for $\epsilon\in[0.5,2), \delta = 10^{-5}$ compared against theoretical lower bounds for (a) $(\epsilon, \delta)$-DP and (b) $(\epsilon, \delta)$-mDP learners trained with Projected Noisy Stochastic Gradient Descent of \citet{Feldman2018}. Since the standard deviations were very low, we did not plot confidence intervals.}
\end{figure} 

%\paragraph{Semantic Guarantees of Differential Privacy} 
\section{Useful Lemmas}\label{appendix:lemmas}
In this section, we present the proofs of some properties that are important for our lower bound analysis. 

We begin by stating the following folklore result on the covering number of norm balls, which can be found in \citet{Wainwright2019}. 
\begin{lemma}[Covering number of norm balls]\label{lemma:norm-ball-packing}
    Let $\|.\|$ be any arbitrary norm on $\reals^{d}$ and let $\mathbb{B}$ denote the unit ball in this norm, i.e., $\mathbb{B} = \{x\in\reals^{d} \text{ such that } \|x\|\leq 1\}$. Then, the covering number of $\mathbb{B}$ is bounded as
    \begin{equation}
     \left(\frac{1}{\eta}\right)^{d} \leq N(\eta, \mathbb{B}, \|.\|) \leq \left(1 + \frac{2}{\eta}\right)^{d}   
    \end{equation}
\end{lemma}
We also use the following well-known results from statistics and information theory, and refer the readers to \citet{canonne} and \citet{Wainwright2019} for a detailed treatment.
\begin{lemma}[Bretagnolleâ€“Huber \cite{canonne}]\label{lemma:bh-bound}
    Let $P_{1}$ and $P_{2}$ be two arbitrary probability measures supported on a domain $\mathcal{X}$. Then, 
    \begin{equation*}
        \|P_{1}-P_{2}\|_{TV} \leq 1 - \frac{1}{2}e^{-D_{KL}(P_{1}||P_{2})}
    \end{equation*}
\end{lemma}

\begin{lemma}[Le Cam's Inequality \cite{Wainwright2019}]
\label{lemma:le-cam}
Let $P_1$ and $P_2$ be two arbitrary probability measures supported on a domain $\mathcal{X}$ and let $\Psi : \mathcal{X} \to \{1, 2\}$ be any arbitrary function that is measurable with respect to both $P_1$ and $P_2$. Then, the following holds,
    \begin{equation}
        {P_{1}(\Psi(X)\neq 1) + P_{2}(\Psi(X)\neq 2)} \geq 1 - \|P_{1}-P_{2}\|_{TV}
    \end{equation}
\end{lemma}
\begin{lemma}[Fano's Inequality \cite{Wainwright2019}]
\label{lemma:fanos}
%For an M-ary hypothesis testing problem with test \(\Psi: \mathcal{X}\rightarrow\{1,2,.., M\}\) that attempts to distinguish between \(M\) distributions \(\{P_{1}, P_{2}, ..., P_{M}\}\) on an arbitrary set \(\mathcal{X}\),
%Let \(\{z_{1}, ..., z_{M}\}\) be a \(2\delta\) packing in the \(\rho\) semi-metric. Then
Let \(V\) be a random variable drawn uniformly at random from the finite set \(\mathcal{V}\), where \(|\mathcal{V}|\geq 2\). For any Markov Chain \(V\rightarrow X \rightarrow \hat{V}\), 
\begin{equation}
    \prob[\hat{V}\neq V] \geq 1 - \frac{I(V; \hat{V} + \ln 2)}{\ln|\mathcal{V}|}
\end{equation}
In essence, for any hypothesis test \(\Psi: X \rightarrow V\), 
\begin{equation}
    \inf_{\Psi}[\Psi(X)\neq V] \geq 1 - \frac{I(V; \hat{V} + \ln 2)}{\ln|\mathcal{V}|}
\end{equation}
    
\end{lemma}
\subsection{Proof of Lemma \ref{lemma:kl}}\label{appendix-l1}
\begin{lemma}[Kullback-Leibler]
    For any \(\epsilon_{L}\)-mDP learner \(\learner\) trained on \(\dataset, \dataset'\) at distance $\rho(\dataset, \dataset')$, the Kullback-Leibler(KL) divergence between the output distributions satisfies the following inequality:
    %\begin{equation}
    %\begin{split}
    %    D_{KL}(\learner(\dataset)||\learner(\dataset')) &\leq\epsilon\DPhyperbolic
    %    \approx \min\left\{\epsilon, \frac{\epsilon^{2}}{2}\right\}
    %\end{split}
    %\end{equation}
    %If the learner is \(\epsilon_{L}\)-mDP, the KL-divergence is bounded above as follows:
    \begin{equation}
    \begin{split}
        D_{KL}(\learner(\dataset)||\learner(\dataset'))& \leq\epsilon_{L}\rho(\dataset, \dataset')\MDPhyperbolic \approx \min\left\{\epsilon_{L}\rho(\dataset, \dataset'), \frac{\epsilon_{L}^{2}\rho(\dataset, \dataset')^{2}}{2}\right\}
    \end{split} 
    \end{equation}
\end{lemma}
%\prateeti{TODO: change proof to mDP, add citation}
\begin{proof}
We refer the interested reader to the proof of \citet{dwork2016concentrated}. 
\end{proof}
    %For an \(\epsilon\)-differentially private learner, the distributions \(\learner(\dataset)\) and \(\learner(\dataset')\) must be \(\epsilon\) close, which naturally introduces a bound on the Radon-Nikodym derivative of the output distributions. Let \(P,Q\) denote the output distributions \(\learner(\dataset), \learner(\dataset')\), respectively. \(\epsilon\)-DP suggests \(\left|ln\frac{dP}{dQ}(z)\right|\leq\epsilon\), i.e., \(\frac{dP}{dQ}(z)\) takes values in the interval \([e^{-\epsilon}, e^{\epsilon}], \forall z\in\dataspace\), w.p. \(1\), which in turn implies that \(\expctn\left[\frac{dP}{dQ}(z)\right] = 1\). 
    %By definition of KL-Divergence,
    %\begin{equation}
    %\begin{split}
        %D_{KL}(P||Q) & = \sum_{z}p(z)ln\left(\frac{p(z)}{q(z)}\right)\\
        %             & = \sum_{z}q(z)\left(\frac{p(z)}{q(z)}\right)ln\left(\frac{p(z)}{q(z)}\right)\\
        %             & = \expctn_{z\sim Q}\left[\left(\frac{p(z)}{q(z)}\right)ln\left(\frac{p(z)}{q(z)}\right)\right]\\
    %    D_{KL}(P||Q) & := \expctn_{P}\left[log\frac{dP}{dQ}\right]\\
    %                 & = \expctn_{Q}\left[\frac{dP}{dQ}log\frac{dP}{dQ}\right]
    %\end{split}
    %\end{equation}
    %Notice that the expectation is over a function of the form \(xln(x)\), which is convex for \(x>0\). To upper bound \(D_{KL}(P||Q)\), we must find a p.d.f. that maximizes the expectation on the right, achieved by placing all mass at the extremes of the interval \([e^{-\epsilon}, e^{\epsilon}]\). Given that \(\expctn\left[\frac{dP}{dQ}\right] = 1\), we have the following equation:
    %\begin{equation}
    %    p e^{-\epsilon} + (1-p) e^{\epsilon}=1
    %\end{equation}
    %Solving the above gives us the p.d.f. of the quantity \(\frac{dP}{dQ}(z)\)
    %\begin{equation}
    %    \frac{dP}{dQ}(z) =
    %    \begin{cases}
    %        e^{\epsilon} & \text{w.p. \(\frac{1-e^{-\epsilon}}{e^{\epsilon}-e^{-\epsilon}}\)} \\
    %        e^{-\epsilon} & \text{w.p. \(\frac{e^{\epsilon}-1}{e^{\epsilon}-e^{-\epsilon}}\)}
    %    \end{cases}
    %\end{equation}
    %Thus, the KL between the output distributions of an \(\epsilon\)-DP learner is bounded above as follows:
    %\begin{equation}
    %    D_{KL}(\learner(\dataset)||\learner(\dataset')) \leq \epsilon \tanh\left(\frac{\epsilon}{2}\right) \approx \min\left\{\epsilon, \frac{\epsilon^{2}}{2}\right\}
    %\end{equation}
    %where the approximation is by a first order Taylor expansion of \(\tanh(.)\)\\
    
    %Note that, to adhere to the definition of differential privacy (\ref{defn:pure-dp}), \(\dataset, \dataset'\) must be neighboring datasets, i.e., the Hamming distance \(d_{H}(\dataset, \dataset')\leq 1\). Naturally, the set of all datasets, together with the adjacency relation, form a Hamming graph, where \(\rho_{H}(\dataset,\dataset')\) between any two datasets \(\dataset, \dataset'\) is the number of rows where \(\dataset\) differs from \(\dataset'\). Therefore, by transitivity, we can set the privacy budget as \(\epsilon \rho_{H}(\dataset, \dataset')\). If we replace the Hamming metric with any metric \(\rho(\dataset,\dataset'): \dataspace \times \dataspace \rightarrow [0,\infty]\) that defines the metric space \((\dataspace, \rho)\) (where \(\dataspace\) is the data domain), then for \textit{any} two datasets \(\dataset, \dataset'\), we have the following result for an \(\epsilon_{L}\)-mDP learner:
    %\begin{equation}
    %\begin{split}
    %    D_{KL}(\learner(\dataset)||\learner(\dataset')) &\leq \epsilon_{L}  d(\dataset, \dataset')  \tanh\left(\frac{\epsilon_{L} \rho(\dataset, \dataset')}{2}\right)\\
    %    & \approx \min\left\{\epsilon_{L} \rho(\dataset, \dataset'), \frac{\epsilon_{L}^{2} \rho(\dataset, \dataset')^{2}}{2}\right\}
    %\end{split}
    %\end{equation}
%\end{proof}
\subsection{Proof of Lemma \ref{lemma:rd}}\label{appendix-l2}

\begin{lemma}[R\'enyi]
For any \(\epsilon_{L}\)-mDP learner $\learner$, trained on datasets \(\dataset,\dataset'\) at distance $\rho(\dataset, \dataset')$, the R\'enyi divergence between the output distributions satisfies the following inequality for $\alpha \in (1, \infty)$:
%    \begin{equation}
%        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon, \frac{3\alpha\epsilon^{2}}{2}\right\}
%    \end{equation}
%    If the learner is \(\epsilon_{L}-\)mDP, the Re\'nyi-divergence is bounded above as follows:
    \begin{equation}
        D_{\alpha}(\learner(\dataset)||\learner(\dataset'))\leq \min\left\{\epsilon_{L}\rho(\dataset,\dataset'), \frac{3\alpha\epsilon_{L}^{2}\rho(\dataset,\dataset')^{2}}{2}\right\}
    \end{equation}
\end{lemma}

\begin{proof}
The proof of this Lemma uses arguments similar to that of Lemma \ref{lemma:kl}. Let $P$ and $Q$ denote the output distributions of $\learner(\dataset)$ and $\learner(\dataset')$ respectively. We first note that the definition of $\epsilon_{L}$-mDP implies, $| \frac{d P}{d Q} | \leq \gamma$, where, $\gamma = \epsilon_{L}\rho(\dataset, \dataset')$. From the definition of $D_\alpha$, it follows that,

\begin{align*}
   D_{\alpha}(P || Q) = \frac{1}{\alpha - 1} \expctn_{Q}\left[ \left(\frac{d P}{d Q} \right)^\alpha \right]  \leq \gamma 
\end{align*}

We now establish that $D_{\alpha}(P || Q) \leq \frac{3 \alpha \gamma^2}{2}$. We first observe that this inequality is trivially satisfied if $\gamma > 1$ since $\gamma < \frac{3 \alpha \gamma^2}{2}$ for $\gamma > 1$ which imples $D_{\alpha}(P || Q) \leq \gamma < \frac{3\alpha \gamma^2}{2}$. Similarly, this inequality is also trivially satisfied if $\alpha > 1 + 1/\gamma$ since $\gamma \leq \frac{3\gamma}{2} < \frac{3 \alpha \gamma^2}{2}$. Hence, we consider the case when $\gamma \leq 1$ and $\alpha \leq 1 + \frac{1}{\gamma}$. We note that the $\epsilon$-mDP condition implies $e^{-\gamma} \leq \frac{d P}{d Q} \leq e^{\gamma}$. From Taylor's Theorem, it follows that, for any $t \in [0, c - 1]$, $(1 + t)^{\alpha} \leq 1 + \alpha t + \frac{\alpha(\alpha - 1)}{2} \max\{ 1, c^{\alpha - 2} \} t^2$. We apply this identity to the computation of the Renyi divergence, using the fact that $e^{-\gamma} \leq | \frac{d P}{d Q} | \leq e^{\gamma} $, 
\begin{align*}
    \exp\left( (\alpha - 1) D_{\alpha}(P || Q) \right) &= \int \left( \frac{d P}{dQ} \right)^{\alpha} dQ = \int \left( 1 + \frac{d P}{dQ} - 1 \right)^{\alpha} dQ \\
    &\leq 1 + \alpha \int \left( \frac{d P}{dQ}  - 1\right) dQ + \frac{\alpha (\alpha - 1)}{2} \max \{ 1, e^{\alpha(\gamma - 2)} \} \left( \frac{d P}{dQ}  - 1\right)^2 dQ \\
    &\leq 1 + \frac{\alpha (\alpha - 1)}{2} e^{\max\{0,\gamma(\alpha - 2)\}} (e^{\gamma} - 1)^2 
\end{align*}
Now, using the fact that $\alpha - 2 \leq 1/\gamma - 1$ and $\ln(1 + x) \leq x$, it follows that,
\begin{align*}
    D_{\alpha}(P || Q) \leq \frac{\alpha (e^{\gamma} - 1)^2}{2} e^{\max \{ 0, 
1 - \gamma \} } \leq \frac{3 \alpha \gamma^2}{2}
\end{align*}
where the last inequality follows from numerical calculation using the fact that $\gamma \leq 1$. 

The proof for DP follows by specializing to the Hamming metric $\rho(\dataset, \dataset') = \rho_{H}(\dataset, \dataset') \leq 1$.
\end{proof}

\section{Proofs of Main Results}\label{appendix:main-results}

\subsection{Proof of Theorem \ref{thm:dp-lecam}}
\begin{theorem}%[Differential Privacy]
    Let $(\mathcal{Z}, \rho)$ be any compact metric space. For any challenge sample $z\in\mathcal{Z}$ in the training set of an  \((\epsilon, \delta)\)-DP learner, a reconstruction adversary that draws $n$ samples from the learner's output distribution incurs a minimax risk of:
    \begin{equation}
        \minimax \geq C diam(\dataspace)^{2}e^{-n\epsilon \tanh\left(\frac{\epsilon}{2}\right)}(1-\delta)
    \end{equation}
    where, \(diam(\dataspace) = \sup_{z,z'\in\dataspace}\rho(z,z')\)
\end{theorem}
\begin{proof}
    Let $E_\delta$ denote the event that the learner $\learner$ is $\epsilon$-differentially private. It follows from Definition \ref{defn:pure-dp} that $\prob[E_\delta]\geq 1-\delta$. The reconstruction error $\error$ can then be bounded by the conditional reconstruction error as follows:
    \begin{equation}
    \begin{split}
        \error &= \errorc\prob[E_\delta] + \errorcnot\prob[E_{\delta}^{c}]\\
               &\geq \errorc\prob[E]\\
               &\geq (1-\delta)\errorc
    \end{split}
    \end{equation}
Taking inf and sup on both sides, we obtain
\begin{equation}
\label{eq:cond-lecam}
\minimax \geq (1-\delta) \conderrorc
\end{equation}
    Equipped with the above, we now derive our reconstruction error lower bound by conditioning on the event that $\learner$ is $\epsilon$-DP. To this end, we begin with a direct application of Markov's inequality, as follows:
    \begin{equation}
    \begin{split}
        \conderrorc &\geq t^{2}\inf_{\hat{z}}\sup_{\distY\in\modelDist}\prob[\rho(\challenge,\zhat)\geq t | E_{\delta}]\\
    \end{split}
    \end{equation}
    Consider any $t > 0$. Let $Z_{t}$ be a finite subset of the input space $\dataspace$ of cardinality $N_{t} \geq 2$ such that $\rho(z_1, z_2) \geq 2t$. Since $\dataspace$ is a compact metric space, $N_t$ is guaranteed to be finite. Furthermore, by choosing $t$ small enough, $N_t \geq 1$ unless $\mathcal{Z}$ is a singleton (in which case the data reconstruction problem is trivial as there is only one candidate data point). Consequently, the uniform distribution supported on $Z_t$ is a valid distribution over the challenge points. Since each possible value of the challenge point $z \in \dataspace$ induces an output distribution $\distY \in \modelDist$, we can lower bound the supremum over $\modelDist$ by taking the average over $\mathsf{Uniform}(Z_t)$ to obtain

    \begin{align*}              \sup_{\distY\in\modelDist}\prob[\rho(\challenge,\zhat)\geq t | E_{\delta}] &\geq \mathbb{E}_{z \sim \mathsf{Uniform}(Z_t)}\left[\prob[\rho(\challenge,\zhat)\geq t | E_{\delta}]\right] \\
    &\geq \frac{1}{N_t} \sum_{i \in [N_t]} \prob[\rho(\challenge,\zhat)\geq t | E_{\delta}]
    \end{align*}

    It follows that,
    \begin{equation}\label{eq:lecam1}
        t^{2}\inf_{\hat{z}}\sup_{\distY\in\modelDist}\prob[\rho(\challenge,\zhat)\geq t | E_{\delta}] \geq \frac{t^{2}}{N_{t}}\inf_{\hat{z}}\sum_{i\in[N_{t}]}\prob[\rho(\challenge_{i},\zhat)\geq t | E_{\delta}]
    \end{equation}
    Let $Z_t = \{ z_j | j \in N_t \}$  and let $\Psi : \dataspace \to [N_t]$ be a function defined as $\Psi(\zhat) = \underset{j\in [N_{t}]}{\operatorname{argmin}}\rho(z_{j},\zhat)$. $\Psi$ represents the index of the challenge point in $Z_t$ that is closest to the candidate reconstruction $\zhat$. By an application of the triangle inequality, we note that, if there exists some $j\in N_{t}$ such that $ \Psi(\zhat) \neq j$, then there must exist some $k\in[N_{t}]$ such that the following holds
    \begin{equation}
        \begin{split}
            \rho(z_{j},\zhat) &\geq \rho(z_{k},\zhat)\\
                              &\geq \rho(z_{k}, z_{j}) - \rho(z_{j}, \zhat) \\
                              &\geq 2t - \rho(z_j, \zhat)
        \end{split}
    \end{equation}
    where the last inequality follows from the fact that $Z_t$ is a $2t$-packing of $\dataspace$. Rearranging we conclude that, $\rho(z_{j},\zhat) \geq t$, i.e., $\Psi(\zhat) \neq j$ implies $\rho(z_j, \zhat) \geq t$ for every $j \in [N_t]$. Then, by the monotonicity property of probability measures, we infer that $\prob[\rho(\challenge_{j},\zhat)\geq t | E_{\delta}] \geq \prob[\Psi(\zhat)\neq j | E_{\delta}]$. Substituting this into equation \ref{eq:lecam1}, we obtain the following 
    \begin{equation}
        \frac{t^{2}}{N_{t}}\inf_{\hat{z}}\sum_{i\in[N_{t}]}\prob[\rho(\challenge_{i},\zhat)\geq t | E_{\delta}] \geq \frac{t^{2}}{N_{t}}\sum_{i\in[N_{t}]}\prob[\Psi(\zhat)\neq i | E_{\delta}]
    \end{equation}

    The classical definition of differential privacy considers neighboring datasets that differ only in a single sample \cite{Dwork2017}. To this end, we consider two datasets $\dataset_{1}, \dataset_{2}$ such that $\dataset_{1} = \dataset\cup\{z_{1}\}, \dataset_{2} = \dataset\cup\{z_{2}\}$, with $z_{1},z_{2}\in Z_{t}$. In line with the threat model outlined in Section \ref{sec:problem-formulation}, the learner samples $z$ uniformly at random from $Z_{t}$ to produce $\learner(\dataset\cup\{z\})$. The adversary then draws $n$ samples from the output distribution $P_{z}$ induced by $\learner(\dataset\cup\{z\})$. In accordance with this consideration, we specialize our lower bound to the case $N_t = 2$ and obtain the following \footnote{The specialization to $N_t = 2$ is specific to the case of differential privacy. As we shall show later, our analysis of metric privacy uses more sophisticated choices of the packing to obtain finer lower bounds}  :
    \begin{align*}
        \conderrorc &\geq \frac{t^{2}}{2}\left\{\prob_{z_{1}}[\Psi(\hat{z}(h_{1}, ..., h_{n}))\neq 1 | E_{\delta}] + \prob_{z_{2}}[\Psi(\hat{z}(h_{1}, ..., h_{n}))\neq 2 | E_{\delta}]\right\}
    \end{align*}

    Let $P_{z_1}$ and $P_{z_2}$ be the output distributions of the learner trained on \(\dataset\cup\{z_{1}\}\) and \(\dataset\cup\{z_{2}\}\) respectively, \emph{conditioned on the event $E_{\delta}$ that the learner is $\epsilon$-differentially private}. Furthermore , let $P^{n}_{z_1}$ and $P^{n}_{z_2}$ denote the respective product distributions. Since $N_t = 2$, $\Psi$ now has range $\{ 1, 2\}$. Thus, we invoke Lemma \ref{lemma:le-cam} to conclude,
    
    \begin{equation}\label{eq:reduction-lecam}
        \conderrorc \geq \frac{t^{2}}{2} \left\{1 - \|P_{z_{1}}^{n} - P_{z_{2}}^{n}\|_{TV}\right\}
    \end{equation}

    Now, upper bounding the RHS using \ref{lemma:bh-bound}, we obtain the following: 
    \begin{equation}
        \begin{split}
            \conderrorc &\geq  \frac{t^{2}}{4}\exp(-D_{KL}(P^{n}_{z_1}||P^{n}_{z_2}))\\
            &= \frac{t^{2}}{4}\exp(-nD_{KL}(P_{z_1}||P_{z_2}))
            \label{le-cam-exponential}
        \end{split}
    \end{equation}
    where the last step uses the tensorization property of KL divergence. 

    We highlight that, under the constraint of $\epsilon$-differential privacy, the optimal choice of the spearation $2t$ is the separation between two farthest points in the data space, i.e., $t = \text{diam}(\dataspace)/2$. This is due to the rigid statistical indistinguishability constraint imposed on the output distributions $P_{z_1}$ and $P_{z_2}$ by the definition of differential privacy. This is justified by Lemma \ref{lemma:kl}, since $D_{KL}(P_{z_1}||P_{z_2}) \leq \epsilon \tanh(\epsilon/2)$ under the $\epsilon$-DP constraint. Note that this bound holds \emph{irrespective of the distance between the data points $z_1$ and $z_2$}. Thus, we conclude that, 
    \begin{align*}
        \errorc &\geq  \frac{\text{diam}(\dataspace)^{2}}{16}\exp(-nD_{KL}(P_{z_1}||P_{z_2})) \geq \frac{\text{diam}(\dataspace)^{2}}{16}\exp(-n\epsilon \tanh(\epsilon/2))
    \end{align*}
    
    Substituting the above into equation \ref{eq:cond-lecam}, we obtain the result of Theorem \ref{thm:dp-lecam}.
    
    %following:
    %\begin{equation}\label{eq:}
    %    \minimax \geq (1 - \delta)\frac{diam(\dataspace)^{2}}{8}\exp(-n\epsilon \tanh(\epsilon/2))
    %\end{equation}
\end{proof}
\subsection{Extension to R\'enyi DP}
\begin{corollary}\label{cor:dp-lecam-renyi}
    Let $(\mathcal{Z}, \rho)$ be any compact metric space. For any challenge sample $z\in\dataspace$ in the training set of an $(\alpha, \epsilon)$-R\'enyi DP learner, a reconstruction adversary that draws $n$ samples from the learner's output distribution incurs a minimax risk of 
    \begin{equation}
        \minimax \geq C \text{diam}(\dataspace)^{2}e^{-n\min\{\epsilon, \frac{3\alpha\epsilon^{2}}{2}\}}
    \end{equation}
    where, \(\text{diam}(\dataspace) =\sup_{z,z'\in\dataspace}\rho(z,z')\)
\end{corollary}
\begin{proof}
Consider any $z_1, z_2 \in \dataspace$ with separation $2t$. Let $P_{z_1}$ and $P_{z_2}$ be the output distributions of the $(\alpha, \epsilon)$-RDP learner trained on \(\dataset\cup\{z_{1}\}\) and \(\dataset\cup\{z_{2}\}\) respectively. Applying the same arguments as that of Theorem \ref{thm:dp-lecam}, we obtain the following: 
    \begin{equation}
        \minimax = \inf_{\hat{z}}\sup_{P_{z}}\expctn[\rho(z, \zhat)^{2}] \geq \frac{\text{diam}(\dataspace)^{2}}{16}\exp\left\{-n D_{KL}(P_{z_{1}}||P_{z_{2}})\right\}
    \end{equation}
    The proof is then completed using the comparison inequality $D_{KL}(P||Q)\leq D_{\alpha}(P||Q)$ and applying Lemma \ref{lemma:rd}. 
\end{proof}
\subsection{Comparison with prior work}\label{appendix-compare}
In this section we provide a detailed comparison between the lower bounds on the reconstruction rate of error obtained for \(\epsilon\)DP learners in Theorem \ref{thm:dp-lecam} above against the rates obtained for \((2,\epsilon)-\)RDP learners in prior work (Theorem 1 of \citet{Guo2022}, restated below).

\begin{theorem}
\label{thm:Guo-et-al}[Theorem 1 \cite{Guo2022}]
    If the learner \(\learner\) is \((2,\epsilon)-\)RDP, and the reconstruction attack outputs an unbiased estimate \(\hat{z(h)}\) upon observing \(h\leftarrow\learner(\dataset)\), then
    \begin{equation}
    \label{eq:Guo-et-al}
        \expctn[\|\hat{z(h)}-z\|^{2}_{2}] \geq \frac{\sum_{i=1}^{d}diam_{i}(\dataspace)^{2}}{4 (e^{\epsilon}-1)}
    \end{equation}
    where \(diam_{i}(\dataspace)\) denotes the i-th coordinate-wise diameter defined as \(\underset{z,z'\in\dataspace, z_{j} = z'_{j}\forall j\neq i}\sup|z_{i}-z_{i}'|\)
\end{theorem}

We first note that both lower bounds require a bounded domain assumption, due to the dependence on \(diam(\dataspace)\) in Theorem \ref{thm:dp-lecam} and on \(diam_{i}(\dataspace)\) in Theorem \ref{thm:guo-2022}. However, even if this assumption holds, Theorem \ref{thm:guo-2022} yields invalid lower bounds for any \(\epsilon < \ln(1 + d/4)\). In contrast, our bound in Theorem \ref{thm:dp-lecam} is valid for all values of \(\epsilon\geq 0\), and tight up to constant factors for \(\epsilon=0\). We formally state and prove these claims as follows:

\subsubsection{Theorem 1 of \citet{Guo2022} gives vacuous Lower Bounds for \(\epsilon < \left(1 + \frac{d}{4}\right)\).}
Consider the data reconstruction problem where the data domain $\mathcal{Z}$ is the unit ball in $\reals^{d}$ (equipped with the $\ell_2$ norm). Then, $\sum_{i = 1}^{d} diam_i(\mathcal{Z})^2 = d $ whereas $diam(\mathcal{Z}) = 1$. Let $z \in \mathcal{Z}$ be arbitrary. As per our problem setup in Section \ref{sec:problem-formulation}, $P_z = \mathcal{M}(\mathcal{D}_{-}\cup\{ z\})$ denotes the distribution induced by the randomized learner, parameterized by $z$. By definition of the diameter, any data reconstruction attack $\hat{z}$ satisfies the trivial upper bound 

\begin{equation}\label{eq:expctn-upper}
    \expctn_{h \sim P_z}\left[\| \hat{z}(h) - z \|^2_{2}\right] \leq diam(\mathcal{Z})^2 = 1
\end{equation}

However, the lower bound in Theorem 1 of Guo et.~al.\cite{Guo2022} states that for any data reconstruction attack $\hat{z}$ on a $(2, \epsilon)$ Renyi Differentially Private model, the reconstruction MSE is lower bounded as

$$\expctn_{h \sim P_z}\left[\| \hat{z}(h) - z \|^2_{2}\right] \geq \frac{\sum_{i = 1}^{d} diam_{i}(\mathcal{Z})^2}{4(e^{\epsilon} - 1)} \geq \frac{d}{4(e^\epsilon - 1)}$$
which is strictly greater than $1$ for any $\epsilon < \ln(1+d/4)$, thereby contradicting the trivial upper bound established above.

Note that, although we chose $\mathcal{Z}$ to be the unit ball in the above sketch, this is purely for the sake of clarity. In fact, the above construction generalizes to almost all possible choices of the data domain $\mathcal{Z}$ since $\sum_{i=1}^{d} diam_{i}(\mathcal{Z})^2$ is typically larger than $diam(\mathcal{Z})^2$ by a multiplicative factor of $d$ in most cases. 

\subsubsection{Lower Bound of Theorem \ref{thm:dp-lecam} is tight in high privacy regime.}\label{appendix-tightness-lecam}
The lower bound presented in Theorem \ref{thm:dp-lecam} in this work is valid for any \(\epsilon, \delta \geq 0\) and tight (up to constant factors) for \(\epsilon \to 0, \delta \to 0\) \begin{proof}
By definition of the diameter, $\expctn_{h\sim P_{z}}[\rho(\zhat, z)^{2}] \leq diam(\dataspace)^{2}$, implying the following upper bound on the minimax risk:
    \begin{equation}
        \inf_{\hat{z}}\sup_{\distY\in\modelDist}\expctn_{h\sim P_{z}}[\rho(\zhat, z)^{2}] \leq \sup_{\distY\in\modelDist}\expctn_{h\sim P_{z}}[\rho(\zhat, z)^{2}] \leq diam(\dataspace)^{2}
    \end{equation}
    As \(\epsilon, \delta = 0\), the last factor in Theorem \ref{thm:dp-lecam} becomes 1. Thus, we have 
    \begin{equation}
        \inf_{\hat{z}}\sup_{\distY\in\modelDist}\expctn_{h\sim P_{z}}[\rho(\zhat, z)^{2}] \geq Cdiam(\dataspace)^{2}
    \end{equation}
    Therefore, the lower bound in Theorem \ref{thm:dp-lecam} is tight (up to constant factors) for \(\epsilon, \delta = 0\).    
    %\prateeti{(\(0,\delta\)-DP interesting)}
\end{proof}


\subsection{Proof of Theorem \ref{thm:mdp-lecam}}\label{appendix-t2}
\begin{theorem*}[\((\epsilon, \delta)\)-mDP]
\label{thm:mdp-lecam}
Let $(\mathcal{Z}, \rho)$ be any locally compact metric space. For any challenge \(\challenge\in\mathcal{Z}\) in the training set of an \((\epsilon_{L}, \delta)\)-mDP learner, a reconstruction adversary with $n$ samples from the output distribution incurs a minimax risk of:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
        \minimax \geq \frac{(1-\delta)}{2ne\epsilon_{L}^{2}} 
    \end{equation}
\end{theorem*}

\begin{proof}
Using the same argument as that of Theorem \ref{thm:dp-lecam}, we lower bound the expected risk by the conditional expected risk. To this end, let $E_\delta$ denote the event that the learner is $\epsilon$-metric differentially private. By Definition \ref{defn:mDP}, $\prob(E_\delta) \geq 1 -\delta$. Hence, 
    \begin{equation}
    \begin{split}
        \error &= \errorc\prob[E_\delta] + \errorcnot\prob[E_{\delta}^{c}]\\
               &\geq \errorc\prob[E]\\
               &\geq (1-\delta)\errorc
    \end{split}
    \end{equation}
Taking sup and inf on both sides, we obtain
\begin{align*}
    \minimax \geq \conderrorc
\end{align*}
Consider two points $z_1, z_2 \in \dataspace$ with separation $2t$ and let \(P_{z_{1}}, P_{z_{2}}\) represent the output distributions from the learner trained on \(\dataset\cup\{z_{1}\}, \dataset\cup\{z_{2}\}\), respectively, \emph{when conditioned on the event $E_\delta$ that the learner is $\epsilon$-mDP} 
Following the same arguments as Theorem \ref{thm:dp-lecam}, we obtain
    \begin{equation}
        \conderrorc \geq \frac{t^2}{4}e^{-nD_{KL}(P_{z_{1}}||P_{z_{2}})}
    \end{equation}

    From Lemma \ref{lemma:kl}, we recall that the KL divergence between the two output distributions of an mDP learner satisfies the following. 
    \begin{align*}
         D_{KL}(P_{z_{1}}||P_{z_{2}}) &\leq \epsilon_{L}t  \\
          D_{KL}(P_{z_{1}}||P_{z_{2}}) &\leq \frac{\epsilon_{L}^{2}t^{2}}{2}
    \end{align*}

    As noted in the proof of Theorem \ref{thm:dp-lecam}, this is in stark contrast to the case of differential privacy where the statistical indistinguishability guarantees between the output distributions of the private learner does not take into account the separation between inputs. 

    We now derive a lower bound for the conditional reconstruction error using the inequality $D_{KL}(P_{z_{1}}||P_{z_{2}}) \leq \epsilon_{L}t$. In this case, we obtain
    \begin{equation}
        \conderrorc \geq \frac{t^{2}}{4}e^{-n\epsilon_{L}t}
    \end{equation}
    Maximizing over \(t\), we have
    \begin{equation}
    \label{eq:mdp-lecam-lb-coarser}
        \conderrorc \geq \frac{1}{n^{2}e^{2}\epsilon_{L}^{2}} \text{   , for } t = \frac{2}{n\epsilon_{L}}
    \end{equation}
    We now derive a lower bound using $D_{KL}(P_{z_{1}}||P_{z_{2}}) \leq \frac{\epsilon_{L}^{2}t^{2}}{2}$ to obtain the following:

    \begin{equation}
        \conderrorc \geq \frac{t^{2}}{4}e^{-n\frac{\epsilon_{L}^{2}t^{2}}{2}}
    \end{equation}
    Maximizing over \(t\), we have 
    \begin{equation}
    \label{eq:mdp-lecam-lb-finer}
        \conderrorc \geq \frac{1}{4en\epsilon_{L}^{2}}\text{   , for } t = \frac{1}{\epsilon_{L}}\sqrt{\frac{2}{n}}
    \end{equation}

Combining the lower bounds \eqref{eq:mdp-lecam-lb-coarser} and \eqref{eq:mdp-lecam-lb-finer}, we obtain:

    \begin{equation}
        \conderrorc \geq \max\left\{\frac{1}{n^{2}e^{2}\epsilon_{L}^{2}}, \frac{1}{2en\epsilon_{L}^{2}}\right\} = \frac{1}{2en\epsilon_{L}^{2}}
        %\approx \Omega\left(\frac{1}{n\epsilon_{L}^{2}}\right)
    \end{equation}
    The proof is then completed by using $\minimax \geq (1-\delta) \conderrorc$ similar to Theorem \ref{thm:dp-lecam}
\end{proof}
\subsection{Proof of Theorem \ref{thm:mdp-fanos}}
\begin{theorem*}
Let $\mathbb{B}(\mathcal{Z})$ denote the unit ball in the locally compact metric space $(\mathcal{Z}, \rho)$ such that $N(1/2, \mathbb{B}(\mathcal{Z}), \rho)<\infty$. For any challenge $z$ in the training set of an $(\epsilon, \delta)$-mDP learner, the adversary's minimax risk of reconstruction satisfies:
    %\textbf{(Metric DP)} Given a fixed challenge \(\challenge\) in the training set of an \(\epsilon\)-mDP learner, the minimax rate of error for a reconstruction adversary, taking \(k\) samples from \(\learner(\dataset\cup\{z\}_{i=1}^{k})\) is bounded below by
    \begin{equation}
    \begin{split}
     %\minimax &\geq \Omega\left(\max\left\{\frac{d}{(m-k)\epsilon^{2}}, \frac{d^{2}}{(m-k)^{2}\epsilon^{2}}\right\}\right)
     \minimax \geq \Omega\left(\frac{\tilde{d}(1-\delta)}{n\epsilon_{L}^{2}}\right),
    \end{split}
    \end{equation}
where, $ \tilde{d} = \ln N(1/2, \mathbb{B}(\mathcal{Z}), \rho)$, i.e., the log covering number of the unit ball in $\mathcal{Z}$.
\end{theorem*}
\begin{proof}
As before, we condition on the event $E_{\delta}$ that the learner is $\epsilon$-mDP and use the fact that $\minimax \geq (1-\delta) \conderrorc$.

Let $C$ be a $\nicefrac{1}{2}$-covering of $\mathbb{B}(\dataspace)$. We note that, since $\mathbb{B}(\dataspace)$ is the unit ball, $|C| \geq 2$. Furthermore, since $\dataspace$ is locally compact, $|C|< \infty$ \cite{rudin1976principles}. Consider any arbitrary separation $t > 0$. Since $\dataspace$ is a linear space, we can scale $C$ by a factor of $t$ to obtain the set $Z_t$. 

We derive this fine-grained lower bound via Fano's Inequality, which requires us to model the data deconstruction process as a Markov Chain. To this end, we recall that the reconstruction process detailed in Section \ref{sec:problem-formulation} can be realized as the following privacy game:
\begin{enumerate}
    \item The learner (defender) draws a challenge point $z \sim \mathsf{Uniform}(Z_t)$, appends it to the fixed dataset $\mathcal{D}$, and computes $\mathcal{M}(\mathcal{D} \cup \{z\})$.

    \item The adversary draws \(n\) samples \(\{h_{1}, ..., h_{n}\}\) from the output distribution \(P_{z}\) induced by \(\learner(\dataset\cup\{\challenge\})\) and outputs a reconstruction \(\zhat\). Similar to Theorem \ref{thm:dp-lecam}, the output distribution is conditioned on the event $E_\delta$.
\end{enumerate}
Clearly, this can be represented as the Markov chain: \(\challenge\rightarrow\{h_{1}, ..., h_{n}\}\rightarrow\zhat\). We have from Lemma \ref{lemma:fanos}
\begin{equation}
\label{eq:minimax}
\conderrorc \geq t^{2}\left[1 - \frac{I(z;\zhat) + \ln 2}{\ln |Z_{t}|}\right]
\end{equation}
We now upper bound the mutual information by the KL divergence as follows 
\begin{equation}
    I(z;\zhat)\stackrel{(1)}\leq I(z; h_{1}, ..., h_{n}) \stackrel{(2)}\leq nD_{KL}(P_{z}||P_{z'}))
\end{equation}
where \(z,z'\in Z_{t}\), \((1)\) follows from Data Processing Inequality and (2) follows as a result of the convexity of \(x\ln(x)\), and tensorization property of KL divergence. By the triangle inequality, we note that $\rho(z, z') \leq 2t$. Furthermore, $|Z_t| = |C| = N(\nicefrac{1}{2}, \mathbb{B}(\dataspace), \rho)$ Finally, by Lemma \ref{lemma:kl}, $D_{KL}(P_z, P_{z'}) \leq 2 \epsilon^2_L t^2$. Substituting these into equation \ref{eq:minimax}, we obtain

\begin{equation}
    \conderrorc \geq t^{2}\left[1-\frac{n\epsilon_{L}^{2}t^{2} + \ln 2}{\ln N(\nicefrac{1}{2}, \mathbb{B}(\dataspace), \rho)}\right]
\end{equation}

Maximizing over \(t\), we have
\begin{equation}
    \conderrorc \geq  \Omega\left\{\frac{\ln N(\nicefrac{1}{2}, \mathbb{B}(\dataspace), \rho)}{n\epsilon_{L}^{2}}\right\} = \Omega\left\{\frac{\tilde{d}}{n\epsilon_{L}^{2}}\right\}
\end{equation}
Converting the conditional lower bound into an unconditonal lower bound similar to Theorem \ref{thm:dp-lecam}, the proof is completed. 
\end{proof}
\subsection{Proof of Corollary \ref{cor:normed-space}}
As a corollary of Theorem \ref{thm:mdp-fanos}, we prove the following reconstruction lower bound that holds for any arbitrary finite-dimensional normed space.
\begin{corollary*}
    If $(\mathcal{Z}, \rho)$ is a normed space of finite dimension $d$, the lower bound reduces to 
    \begin{equation}
        \minimax \geq \Omega\left(\frac{d (1-\delta)}{n\epsilon_{L}^{2}}\right)
    \end{equation}
\end{corollary*}
\begin{proof}
 Since  $(\mathcal{Z}, \rho)$ is a normed space of finite dimension $d$, it is isomorphic to $(\mathbb{R}^d, \| \cdot \|)$ for some appropriate norm $\| \cdot \|$ \cite{hoffmann1971linear}. Thus, using Lemma \ref{lemma:norm-ball-packing} and upper bounding the covering number by the packing number, we conclude that $\ln M(\nicefrac{1}{2}, \mathbb{B}(\dataset), \| \cdot \|) = \Theta(d)$
 
 
 Thus, from Lemma \ref{lemma:norm-ball-packing}, we note that $\ln N(\nicefrac{1}{2}, \mathbb{B}(\dataset), \| \cdot \|) = \Theta(d \ln(2))$. The proof is completed by substituting this bound into Theorem \ref{thm:mdp-fanos}.  
\end{proof}

\section{Metric Differential Privacy in Practice}\label{appendix:mechanism-extensions}
In this section, we first establish the requisite framework for analyzing mDP in practice and present an extension of the Gaussian Noise Mechanism to metric privacy. We emphasize that, since mDP is a generalization of DP, practical applications would require a relaxation similar to \ref{defn:rdp} in order to establish privacy guarantees. To this end, we consider the following relaxed version of metric differential privacy. 
%\begin{definition}[Approximate mDP \cite{Imola2022}]
%\label{def:approximate-lipschitz-privacy}
%Given a metric space \((\dataspace, \rho)\), for any \(\epsilon_{L}\geq 0, \delta\geq 0\), a randomized learner \(\learner: \dataspace \rightarrow\labelspace\) is \((\epsilon_{L}, \delta)-\)mDP if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\) and \(\forall T\subseteq\labelspace\),
%\begin{equation}
%    \prob[\learner(\dataset)\in T]\leq e^{\epsilon_{L}\rho(\dataset, \dataset')}\prob[\learner(\dataset')\in T] + \delta
%\end{equation}
%\end{definition}
\begin{definition}[R\'enyi mDP]
\label{def:renyi-lipschitz}
Given a metric space \((\dataspace, \rho)\), for any \(\epsilon_{L}\geq 0, \alpha\in[1,\infty]\), a randomized learner \(\learner: \dataspace^n \rightarrow\labelspace\) is \((\alpha, \epsilon_{L})-\)R\'enyi mDP if for every pair of input datasets \(\dataset, \dataset'\in\dataspace\) the R\'enyi Divergence is bounded as follows
    \begin{equation}
        D_{\alpha}(P||Q) = \frac{1}{\alpha - 1}\log\expctn_{x\sim Q}\left[\left(\frac{P(x)}{Q(x)}\right)^{\alpha} \right]\leq\epsilon_{L}\rho(\dataset,\dataset')
    \end{equation}
    where \(P\) and \(Q\) denote the output distributions \(\learner(\dataset)\) and \(\learner(\dataset')\), respectively. 
\end{definition}
It is easy to see that Definition \ref{def:renyi-lipschitz} recovers Renyi DP when restricted to the Hamming metric, in a manner similar to how mDP generalizes pure DP. 

Before proceeding further, we recall standard notions of sensitivity and noise mechanisms for classical differential privacy.

A common paradigm in releasing statistical information with differential privacy is to generate a noisy estimate of the true statistic. Namely, if $f: \dataspace\to\reals^{d}$ is a real-valued function\footnote{The restriction to real-valued functions is not essential.}, $\learner(\dataset) = f(\dataset) + \beta$ is differentially private for an appropriate noise level $\beta$. The magnitude of perturbation is generally calibrated according to the sensitivity of the function $f$, defined as follows:
%We also note that mechanisms in differential privacy generally require the following sensitivity constraint:
\begin{definition}[Sensitivity]
\label{defn:global-sensitivity}
    Given any function \(f:\dataspace\rightarrow\reals^{d}\),
 \begin{equation}
     \Delta_{f} = \underset{\dataset,\dataset'\in\dataspace ; \|\dataset-\dataset'\|\leq 1}{\max}\|f(\dataset)-f(\dataset')\|
 \end{equation}
\end{definition}
%Note that the sensitivity constraint is over all neighboring datasets \(\dataset, \dataset'\in\dataspace\).
The Gaussian mechanism, for instance, requires that $f(\dataset)$ be perturbed with noise drawn from the Gaussian distribution, as follows:
\begin{definition}[Gaussian Mechanism]
\label{defn:dp-gaussian-mech}
    Given any function \(f:\dataspace\rightarrow\reals^{d}\), the Gaussian mechanism is defined by
 \begin{equation}
     \learner(\dataset) = f(\dataset) + \mathcal{N}(0, \Delta_{f}^{2}\sigma^{2})
 \end{equation}
 For $\epsilon < 1$ and $c^{2} > 2\ln(1.25/\delta)$, the Gaussian mechanism with parameter $\sigma \geq c\Delta_{f}/\epsilon$ satisfies $(\epsilon, \delta)$-DP \cite{Dwork2017}.
\end{definition}

The sensitivity constraint described in Definition \ref{defn:global-sensitivity} is over all neighboring datasets \(\dataset, \dataset'\in\dataspace\), in order to accommodate for classical differential privacy. For metric differential privacy, we define a relaxed requirement which we call input lipschitzness. 
\begin{definition}[Input Lipschitzness]
 A function \(f:\dataspace\rightarrow\reals^{d}\) is $L_{input}$ input lipschitz if the following holds
    \begin{equation}
        \|f(\dataset)-f(\dataset')\|\leq L_{input}\rho(\dataset,\dataset')
    \end{equation}
\end{definition}
%Having set up the framework for analyzing mDP in practice, we now investigate the feasibility of extending popular mechanisms in differential privacy to accommodate for Lipschitz privacy. 
Equipped with this notion, we extend the Gaussian mechanism for DP (\ref{defn:dp-gaussian-mech}) to mDP, as follows:
\begin{proposition}[Gaussian Mechanism for mDP]
\label{prop:gaussian-lp}
    Consider any two datasets \(\dataset, \dataset' \in \dataspace\), at a distance \(\rho(\dataset,\dataset')\) and an input lipschitz function \(f:\dataspace\rightarrow\reals^{d}\). For any \(\delta \in (0,1)\) and \(c^{2}>\ln(1.25/\delta)\), the Gaussian Mechanism with parameter \(\sigma \geq cL_{input}/\epsilon_{L}\) is \((\epsilon_L,\delta)-\)mDP, where  \(L_{input}\) is the input Lipchitz constant of \(f\).
\end{proposition}
\begin{proof}
From the definition of input Lipschitzness and sensitivity, it follows that 
\begin{equation}
    \Delta_{f} = \|f(\dataset)-f(\dataset')\|\leq L_{input}\rho(\dataset,\dataset')
\end{equation}
 Setting \(\epsilon = \epsilon_{L}\rho(\dataset, \dataset')\), we are required to bound the following quantity for \(f\) applied on datasets \(\dataset, \dataset'\):

 \begin{equation}
\left|\ln\frac{e^{\frac{-1}{2\sigma^{2}}\|x\|^{2}}}{e^{\frac{-1}{2\sigma^{2}}\|x + \Delta_{f}\|^{2}}}\right|\leq \epsilon = \epsilon_{L}\rho(\dataset, \dataset')
 \end{equation}
 where the numerator is the probability of observing a specific output \(f(\dataset)+x\) and the denominator is the probability of observing the same output when the input dataset is replaced by \(\dataset'\). The proof of our claim then follows trivially from the proof of Gaussian Mechanism for \((\epsilon, \delta)\) differential privacy \cite{Dwork2017}. 
\end{proof}

We are now ready to develop mDP accountants for privacy-preserving algorithms. A popular approach to differentially private deep learning involves an extension of Stochastic Gradient Descent (SGD)\cite{Abadi2016} to incorporate gradient clipping and noisy gradient updates. The resulting algorithm is called Differentially Private SGD (DP-SGD), wherein, $(\epsilon, \delta)$ differential privacy is guaranteed via an application of the Gaussian mechanism at each iteration. Since mDP is a generalization of traditional DP, we expect that iterative DP algorithms that rely on additive noise at each intermediate solution step are often inherently mDP, owing to the application of composition theorems that admit natural extensions to arbitrary metric spaces.

\paragraph{mDP Accounting for DP-SGD}
In the analysis of DP-SGD \cite{Abadi2016}, privacy is attained via the addition of Gaussian distributed noise. Standard arguments on the composition of privacy mechanisms renders the differentially private variant of SGD \((q\epsilon, q\delta)-\)differentially private at each step, where \(q = L/N\) is the lot size. We note that, by replacing the global sensitivity assumption with input Lipschitzness, the Gaussian mechanism ensures \((\epsilon_{L}, \delta)\)-mDP for appropriately scaled noise, as in Proposition \ref{prop:gaussian-lp}. Thus, DP-SGD inherently incorporates metric differential privacy, with privacy accounting based on standard composition theorems \cite{Kasiviswanathan2008}.

\section{Privacy Analysis for Metric DP in PN-SGD}\label{appendix:propositionPNSGD}
When analysing the privacy of learning algorithms that require iterative updates on an intermediate solution, it is common practice to ensure privacy at each iteration and argue about the cumulative loss of privacy via composition theorems. Another popular direction is the theoretical analysis of Noisy Stochastic Gradient Descent to formalize privacy amplifications under certain assumptions and obtain bounds on the degradation of privacy across iterations. We extend the analysis of one such algorithm, the Projected Noisy Stochastic Gradient Descent (PNSGD) \cite{Feldman2018}, restated in Algorithm \ref{alg:pnsgd}, and establish that the algorithm satisfies $(\alpha, \epsilon_{L})$-R\'enyi mDP with a lower noise magnitude than that required for $(\alpha, \epsilon)$-R\'enyi DP.

\begin{proposition*}
    Let $\cK \subset \mathbb{R}^d$ be a convex set and let $\{ f(., x) \}_{x \in cX}$ be a family of convex, $\beta$-smooth functions over $\cK$, where the gradients are $L_{input}$-input Lipschitz. Furthermore, assume $\cX$ is a bounded set. Then, for any $\eta \leq 2/\beta$ and $\alpha > 1$, initializing $w_0 \in \cK$ and dataset $S \in \cX^{n}$, PNSGD run with $\sigma^{2} \geq \frac{2 \alpha L^{2}_{input} diam(\cX)}{\epsilon_L (n- t + 1)}$ satisfies $(\alpha, \epsilon_{L})$-R\'enyi mDP.
\end{proposition*}
\begin{proof}
    The proof of this theorem is an extension of Theorem 23 of Feldman et~al.\cite{Feldman2018} to Renyi mDP. Let $S \coloneqq (x_1, \dots, x_n)$ and $S' \coloneqq (x_1, \dots, x_{t-1}, x^{'}_{t}, \dots, x_n)$ be two arbitrary datasets that differ in index $t$. Since $\eta \leq 2/\beta$, the projected SGD updates are contractive noisy iterations as per Definition 19 of Feldman et~al.\cite{Feldman2018}. 

We define the projected SGD operators for the dataset $S$ as $g_i(w) = \Pi_{\cK}(w) - \eta \nabla f(\Pi_{\cK}(w), x_i) , \ i \in [n]$. The projected SGD operators $g^{'}_i$ for the dataset $S'$ are defined in a similar fashion. Since the datsets differ only in the $t^{\textrm{th}}$ index, we note that $g_i = g^{'}_i \ \forall i \in [n] \setminus \{ t \}$ and,
\begin{align*}
    \sup_{w} \| g_{t}(w) - g^{'}_{t}(w) \|_{2} = \eta \sup_{w} \| \nabla f(\Pi_{\cK}(w), x_t) - \nabla f(\Pi_{\cK}(w), x^{'}_t) \|_{2} \leq \eta L_{input} \| x_t - x^{'}_t \|_{2}
\end{align*}
Now, applying Theorem 22 of Feldman et~al.\cite{Feldman2018} with $a_1, \dots, a_{t-1} = 0$, $a_t, \dots, a_n = \frac{\eta L_{input} \| x_t - x^{'}_t \|_{2}}{n-t+1}$, $s_t = \eta L_{input} \| x_t - x^{'}_t \|_{2}$ and $s_i = 0, \ \forall i \in [n] \setminus \{ t \}$, we conclude,
\begin{align*}
    D_{\alpha}(w_n || w^{'}_n) \leq \frac{\alpha}{2 \eta^2 \sigma^2} \sum_{i=1}^{n} a^2_i \leq \frac{2 \alpha L^{2}_{input} \| x_t - x^{'}_t \|^{2}_{2}}{\sigma^2 (n-t+1)}
\end{align*} 
From the above inequality, we conclude that setting $\sigma^2 \geq \frac{2 \alpha L^{2}_{in} diam(\cX)}{\epsilon_L (n- t + 1)}$ suffices to ensure $D_{\alpha}(w_n || w^{'}_n) \leq \epsilon_L \| x_t - x^{'}_t \|^{2}$.
\end{proof}

\begin{corollary*}
For a large class of functions, Proposition \ref{prop:pnsgd-mdp} requires less perturbation for $(\alpha, \epsilon_{L})$-R\'enyi mDP than that required for $(\alpha, \epsilon)$-R\'enyi DP. 
%in Feldman et.~al.\cite{Feldman2018}.
\end{corollary*}
\begin{proof}
    Proposition \ref{prop:pnsgd-mdp} establishes that, for convex and $\beta$-smooth functions with $L_{input}$-input Lipschitz gradients, PN-SGD run with a noise level $\sigma^2\geq\frac{2\alpha L^{2}_{input}diam(X)}{\epsilon_{L}(n-t+1)}$ satisfies $(\alpha, \epsilon_{L})$ R\'enyi mDP. On the contrary, Theorem 23 of Feldman et.~al.\cite{Feldman2018} suggests that for convex, $\beta$-smooth and $G$-Lipschitz functions, PN-SGD run with a noise level $\sigma^{2}\geq \frac{2\alpha G^{2}}{\epsilon (n-t+1)}$ satisfies $(\alpha, \epsilon)$ Renyi DP. We highlight that, for a large class of problems, the Lipschitz constant $G$ (which is essentially an upper bound on $\| \nabla_{w} f(w, x) \|_{2}$) is itself diameter dependent. For instance, consider the family of functions $f(w, x) = \frac{1}{2}\|w - x \|^{2}_{2}$ where $w, x \in X \subset \reals^{d}$ with $diam(X) < \infty$. This setup is practically relevant for statistical problems such as mean estimation \cite{Tsai2022}. Since $\nabla_{w} f(w, x) = w - x$, this class of functions is (a) $G$-Lipschitz with $G = diam(X)$, (b) $\beta$-smooth, and has $L_{input}$ input Lipschitz gradients with $\beta = L_{input} = 1$. Consequently, prior work \cite{Feldman2018} requires a noise level $\sigma^{2}\geq\frac{2\alpha diam(X)^{2}}{\epsilon (n-t+1)}$ for $(\alpha, \epsilon)$ R\'enyi DP whereas Proposition \ref{prop:pnsgd-mdp} requires a lower noise level of $\sigma^{2}\geq\frac{2\alpha diam(X)}{\epsilon (n-t+1)}$ for $(\alpha, \epsilon_{L})$ R\'enyi mDP.
\end{proof}

\section{Related Literature}\label{appendix:related-literature}
 We first distinguish between the traditional reconstruction attack paradigm introduced in the seminal work of \citet{DinurNissim} from \emph{training} data reconstruction attacks studied in this work. Consider an $n$-row dataset $\dataset$, wherein, each row corresponds to the information of an individual. The former setting considers adversaries that can recover the secret bit of $o(n)$ individuals, simply by observing noisy answers to simple queries on the dataset $\dataset$. The discovery of such reconstruction attacks predate (and inspired) differential privacy, and has since been studied extensively, with several works demonstrating that any mechanism that provides overly accurate responses to a sufficient number of linear queries is blatantly non-private, i.e., it succumbs to reconstruction attacks \cite{DworkYekhanin, muthukrishnan2012optimal}. For an exhaustive survey of related literature in this setting, we refer the reader to the works of \citet{exposed} and \citet{complexityDP}.
 
 In this work, we focus on a related but different threat model, wherein, an \emph{informed} adversary attempts to reconstruct a training sample using information from the model training procedure, such as intermediate gradients, and prior knowledge of other training samples. This is consistent with the strong adversary model studied in the works of \citet{Balle2022} and \citet{Guo2022}. While these works derive reconstruction error bounds in the bayesian and frequentist formalisms respectively, their results are limited to the Euclidean metric. \citet{stock2022defending} obtain lower bounds on the leakage of secrets in language tasks in R\'enyi differentially private learners. However, they are restricted to a structured and simplistic notion of secret bits, which does not capture the complexity of privacy protection in unstructured textual data. On the contrary, our lower bound results are more general and hold for arbitrary metric spaces.

%\section{Proof of Corollary}\label{appendix:corollaryPNSGD}


\begin{comment}
\section{Submission of papers to NeurIPS 2023}


Please read the instructions below carefully and follow them faithfully. \textbf{Important:} This year the checklist will be submitted separately from the main paper in OpenReview, please review it well ahead of the submission deadline: \url{https://neurips.cc/public/guides/PaperChecklist}.


\subsection{Style}


Papers to be submitted to NeurIPS 2023 must be prepared according to the
instructions presented here. Papers may only be up to {\bf nine} pages long,
including figures. Additional pages \emph{containing only acknowledgments and
references} are allowed. Papers that exceed the page limit will not be
reviewed, or in any other way considered for presentation at the conference.


The margins in 2023 are the same as those in previous years.


Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
NeurIPS website as indicated below. Please make sure you use the current files
and not previous versions. Tweaking the style files may be grounds for
rejection.


\subsection{Retrieval of style files}


The style files for NeurIPS and other conference information are available on
the website at
\begin{center}
  \url{http://www.neurips.cc/}
\end{center}
The file \verb+neurips_2023.pdf+ contains these instructions and illustrates the
various formatting requirements your NeurIPS paper must satisfy.


The only supported style file for NeurIPS 2023 is \verb+neurips_2023.sty+,
rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
  Microsoft Word, and RTF are no longer supported!}


The \LaTeX{} style file contains three optional arguments: \verb+final+, which
creates a camera-ready copy, \verb+preprint+, which creates a preprint for
submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
\verb+natbib+ package for you in case of package clash.


\paragraph{Preprint option}
If you wish to post a preprint of your work online, e.g., on arXiv, using the
NeurIPS style, please use the \verb+preprint+ option. This will create a
nonanonymized version of your work with the text ``Preprint. Work in progress.''
in the footer. This version may be distributed as you see fit, as long as you do not say which conference it was submitted to. Please \textbf{do
  not} use the \verb+final+ option, which should \textbf{only} be used for
papers accepted to NeurIPS. 


At submission time, please omit the \verb+final+ and \verb+preprint+
options. This will anonymize your submission and add line numbers to aid
review. Please do \emph{not} refer to these line numbers in your paper as they
will be removed during generation of camera-ready copies.


The file \verb+neurips_2023.tex+ may be used as a ``shell'' for writing your
paper. All you have to do is replace the author, title, abstract, and text of
the paper with your own.


The formatting instructions contained in these style files are summarized in
Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.


\section{General formatting instructions}
\label{gen_inst}


The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
type with a vertical spacing (leading) of 11~points.  Times New Roman is the
preferred typeface throughout, and will be selected for you by default.
Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
indentation.


The paper title should be 17~point, initial caps/lower case, bold, centered
between two horizontal rules. The top rule should be 4~points thick and the
bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
below the title to rules. All pages should start at 1~inch (6~picas) from the
top of the page.


For the final version, authors' names are set in boldface, and each name is
centered above the corresponding address. The lead author's name is to be listed
first (left-most), and the co-authors' names (if different address) are set to
follow. If there is only one co-author, list both author and co-author side by
side.


Please pay special attention to the instructions in Section \ref{others}
regarding figures, tables, acknowledgments, and references.


\section{Headings: first level}
\label{headings}


All headings should be lower case (except for first word and proper nouns),
flush left, and bold.


First-level headings should be in 12-point type.


\subsection{Headings: second level}


Second-level headings should be in 10-point type.


\subsubsection{Headings: third level}


Third-level headings should be in 10-point type.


\paragraph{Paragraphs}


There is also a \verb+\paragraph+ command available, which sets the heading in
bold, flush left, and inline with the text, with the heading followed by 1\,em
of space.


\section{Citations, figures, tables, references}
\label{others}


These instructions apply to everyone.


\subsection{Citations within the text}


The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.


The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}


If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2023+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}


If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{neurips_2023}
\end{verbatim}


As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the
citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.


\subsection{Footnotes}


Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).


Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}


\subsection{Figures}


\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}


All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.


You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.


\subsection{Tables}


All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.


Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.


Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Math}
Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)

\subsection{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.


\section{Preparing PDF files}


Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''


Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.


\begin{itemize}


\item You should directly generate PDF files using \verb+pdflatex+.


\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.


\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.


\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


\end{itemize}


If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.


\subsection{Margins in \LaTeX{}}


Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.


\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}



\section{Supplementary Material}

Authors may wish to optionally include extra information (complete proofs, additional experiments and plots) in the appendix. All such materials should be part of the supplemental material (submitted separately) and should NOT be included in the main submission.


\section*{References}


References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}
\end{comment}

\end{document}