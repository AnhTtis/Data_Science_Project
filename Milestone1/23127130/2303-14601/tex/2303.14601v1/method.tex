
\section{Our PORE}
\label{method}
\CR{We first give an overview of our PORE, then define our ensemble recommender system and show it is $(e,r)$-provably robust, and finally describe our algorithm to compute the certified intersection size $r$ for each user. }

\subsection{\CR{Overview}}
Our key intuition is that, when the number of fake users is bounded, a random subset of a small number of users is likely to only include genuine users. Therefore, a recommender system built using the rating scores of such a random subset of users is not affected by fake users. 
Based on the intuition,  PORE builds multiple recommender systems using random subsets of users and takes majority vote among them to recommend items for users. 


Specifically, we create multiple submatrices from a  rating-score matrix, where each submatrix contains the rating scores of $s$ different randomly selected users, i.e., $s$ rows randomly selected from the   rating-score matrix. Then, we use an arbitrary base algorithm  to build a recommender system (called \emph{base recommender system}) on each submatrix and use it to recommend items for users. Finally, we build an \emph{ensemble recommender system}, which takes a majority vote among the base recommender systems as the final recommended items for each user. Figure~\ref{fig:overrview} shows our ensemble recommender system and  its robustness against data poisoning attacks. 


Next, we first formally define our ensemble recommender system in PORE. Then, we show   PORE is provably robust against data poisoning attacks. In particular, given an arbitrary set of items $\mathcal{I}_u$ for a user $u$, we prove that at least $r$ of the top-$N$ items recommended to $u$ by PORE are guaranteed to be among $\mathcal{I}_u$ under any data poisoning attacks.  Moreover, we derive the certified intersection size $r$. Finally, we design an algorithm to compute the certified intersection sizes $r$ for all users simultaneously. 


\subsection{Our Ensemble Recommender System}
\vspace{-2mm}
\myparatight{Item probability $p_i$} Given a  rating-score matrix $\mathbf{M}$, 
we randomly sample a submatrix with $s$ rows of $\mathbf{M}$, i.e., the submatrix consists of the rating scores of $s$ different randomly selected users. 
For convenience, we denote by $\mathbf{X}$ the sampled submatrix. 
Then, we use an arbitrary base algorithm $\mathcal{A}$ to build a base recommender system on the sampled submatrix $\mathbf{X}$. 
We use the base recommender system to recommend top-$N'$ items (denoted as $\mathcal{A}(\mathbf{X}, u)$) to each user $u$ in the sampled submatrix. Since the submatrix is randomly sampled, the recommended top-$N'$ items are also random. To consider such randomness, we denote by $p_{i}$ the probability that item $i$ is recommended to a user $u$. Formally, we define $p_{i}$ as follows: $p_{i} = \text{Pr}(i \in \mathcal{A}(\mathbf{X},u))$. We call $p_i$ \emph{item probability}. 

Note that we consider $\mathcal{A}(\mathbf{X},u)$ is an empty set when $u$ is not in the sampled submatrix $\mathbf{X}$, as many recommender systems make recommendations for users in the rating-score matrix that was used to build the recommender systems.  
Since a base recommender system is built using $s$ rows of the rating-score matrix $\mathbf{M}$, we can build ${n \choose s}$ base recommender systems in total, where $n$ is the total number of users/rows in $\mathbf{M}$. Essentially, our item probability $p_i$ is the fraction of the ${n \choose s}$ base recommender systems that recommend item $i$ to the user $u$. 


\myparatight{Poisoned item probability $p_{i}'$} Under data poisoning attacks, the rating-score matrix $\mathbf{M}$ becomes a poisoned version $\mathbf{M}^{\prime}$. 
We denote by $\mathbf{Y}$ a random submatrix with $s$ rows sampled from $\mathbf{M}^{\prime}$. Moreover, we define \emph{poisoned item probability} $p_{i}' =  \text{Pr}(i \in \mathcal{A}(\mathbf{Y},u))$, i.e., $p_{i}'$ is the probability that the item $i$ is in the top-$N'$ items recommended to $u$ when the base recommender system is built based on  $\mathbf{Y}$. 



\myparatight{Our ensemble recommender system $\mathcal{T}$} Our ensemble recommender system (denoted as $\mathcal{T}$) recommends the top-$N$ items with the largest item probabilities $p_{i}$'s for a user $u$. Essentially, our ensemble recommender system takes a majority vote among the ${n \choose s}$ base recommender systems. In particular, our ensemble recommender system essentially recommends the top-$N$ items that are the most frequently recommended by the ${n \choose s}$ base recommender systems to a user. 
For simplicity, we use $\mathcal{T}(\mathbf{M},u)$ to denote the set of top-$N$ items recommended to the user $u$ by our ensemble recommender system $\mathcal{T}$ when the rating-score matrix is $\mathbf{M}$. Note that $N'$ and $N$ are different parameters, i.e., $N'$ is the number of items recommended to a user by a base recommender system while $N$ is the number of items recommended to a user by our ensemble recommender system. We will explore their impact on  our ensemble recommender system in experiments. 

 Under data poisoning attacks, our ensemble recommender system $\mathcal{T}$ uses the poisoned item probabilities to make recommendations. Specifically, $\mathcal{T} (\mathbf{M}', u)$ is the set of top-$N$ items with the largest poisoned item probabilities $p_{i}'$'s that are recommended to $u$ by  $\mathcal{T}$ under attacks. 

\subsection{Deriving the Certified Intersection Size}
\label{sec:derive_certified_performance}

We show that our ensemble recommender system algorithm $\mathcal{T}$ is $(e,r)$-provably robust. In particular, for any given number of fake users $e$,  we can derive the certified intersection size $r$ of  $\mathcal{T}$ for any user $u$. Specifically, given an arbitrary set of items $\mathcal{I}_u$, 
   we show that at least $r$ of the recommended top-$N$ items $\mathcal{T} (\mathbf{M}', u)$ are in $\mathcal{I}_u$ when there are  at most $e$ fake users, no matter what rating scores they use. \neil{Later, we can replace $\mathcal{I}_u$ as our desired sets of items.} Formally, we aim to show the following: $\min_{\mathbf{M}' \in \mathcal{L}(\mathbf{M},e)} |\mathcal{I}_{u} \cap \mathcal{T} (\mathbf{M}',u)| \geq r$, 
where $\mathcal{L}(\mathbf{M},e)$ is the set of all possible poisoned-rating-score matrices  and denotes all data poisoning attacks with at most $e$ fake users. 
Next, we first overview our main idea  and then show our theorem. 


\myparatight{Overview of our derivation} Our proof is based on the \emph{law of contraposition}. Suppose we have a statement: $U \longrightarrow V$, whose contraposition is $\neg V \longrightarrow \neg U$. The law of contraposition means that a statement is true if and only if its contraposition is true. We define a predicate $V$ as $V: |\mathcal{I}_{u} \cap \mathcal{T} (\mathbf{M}',u)| \geq r$. The predicate $V$ is true if at least $r$ of the top-$N$ items recommended by $\mathcal{T}$ for $u$ are   in $\mathcal{I}_{u}$ when the poisoned-rating-score matrix is a given $\mathbf{M}'$. Then, we derive a necessary condition (denoted as $\neg U$)  for $\neg V$ to be true, i.e., we have $\neg V \longrightarrow \neg U$.   By the law of contraposition, we know that $V$ is true if  $U$ is true. 
Roughly speaking, \neil{$U$} means that the $r$th largest poisoned item probability for items in $\mathcal{I}_u$ is \neil{larger} than the $(N - r +1)$th largest poisoned item probability for items in $\mathcal{I}\setminus\mathcal{I}_u$, when the poisoned-rating-score matrix is $\mathbf{M}'$. 

The challenge in deriving the condition \neil{for $U$ to be true} is that it is hard to compute the poisoned item probabilities $p_{i}^{\prime}$'s due to the complexity of recommender system. To address the challenge, we resort to derive a lower bound of  $p_{i}^{\prime}$ for each $i\in \mathcal{I}_u$ and an upper bound of $p_j^{\prime}$ for each $j\in \mathcal{I} \setminus \mathcal{I}_u$. In particular, we derive the lower/upper bounds of poisoned item probabilities using  lower/upper bounds of item probabilities. We consider lower/upper bounds of item probabilities instead of their exact values, because it is challenging to compute them exactly. Suppose we have a lower bound $\underline{p_i}$ of $p_i$ for each $i \in \mathcal{I}_u$ and an upper bound $\overline{p}_j$ of $p_j$ for each $j \in \mathcal{I} \setminus \mathcal{I}_u$, i.e., we have the following: 
\begin{align}
\label{probability_upper_low_bound_theorem_1}
 p_i \geq  \underline{p_i}  \text{ and } p_j \leq \overline{p}_j, 
\end{align}
where $i \in \mathcal{I}_u$ and $j \in \mathcal{I} \setminus \mathcal{I}_u$. 
In the next section, we design an algorithm to estimate such lower/upper bounds of item probabilities. 
Given the lower/upper bounds $\underline{p_i}$ and $\overline{p}_j$, we derive a lower bound of $p_i^{\prime}$ for each $i\in \mathcal{I}_u$ and an upper bound of $p_j^{\prime}$ for each $j\in \mathcal{I} \setminus \mathcal{I}_u$ via  a variant Neyman-Pearson Lemma~\cite{neyman1933ix} that we develop. Our variant  is applicable to multiple functions, while the standard Neyman-Pearson Lemma is only applicable to one function. 


Next, we show our intuition to derive the upper and lower bounds (please refer to the proof of Theorem~\ref{theorem_certified_size} for formal analysis) of the poisoned item probabilities. 
We denote by $\Phi$ the union of the domain spaces of $\mathbf{X}$ and $\mathbf{Y}$, i.e., each element in $\Phi$ is a submatrix with $s$ rows sampled from $\mathbf{M}$ or $\mathbf{M}^{\prime}$. Our idea is to find subsets in  $\Phi$ such that we can apply our variant of the Neyman-Pearson Lemma to derive the upper/lower bounds of the poisoned item probabilities. Moreover, the upper/lower bounds are related to the probabilities that the random submatrices $\mathbf{X}$ and $\mathbf{Y}$ are in the subsets, which can be easily computed. 
We denote by $P$ (or $A$)  the set of submatrices sampled from $\mathbf{M}$ (or $\mathbf{M}^{\prime}$) that include the user $u$. 

\myparatight{Deriving a lower bound of $p_i^{\prime}$, $ i \in \mathcal{I}_u$} We can find a subset $C_i \subseteq P$ such that we have $\text{Pr}(\mathbf{X} \in C_i) = p_i^* \triangleq \frac{\lfloor \underline{p_i} \cdot {n \choose s}\rfloor}{{n \choose s}}$. 
Note that we can find such a subset because $p_i^*$ is an integer multiple of $1/{n \choose s}$. Then, via our variant of the Neyman-Pearson Lemma, we can derive a lower bound of $p_i^{\prime}$ using the probability that the random submatrix $\mathbf{Y}$ is in the subset $C_i$, i.e., we have:  $p_i^{\prime} \geq \text{Pr}(\mathbf{Y} \in C_i), \ \forall i\in \mathcal{I}_u$. 

\myparatight{Deriving an upper bound of $p_j^{\prime}$, $j \in \mathcal{I} \setminus \mathcal{I}_u$} We first find a subset $C_j^{\prime} \subseteq P$ such that we have the following: $\text{Pr}(\mathbf{X} \in C_j^{\prime}) = \overline{p}^{*}_j = \frac{\lceil \overline{p}_j \cdot {n \choose s}\rceil}{{n \choose s}}$. 
Given the subset $C_j^{\prime}$, we further define a subset $C_j = C_j^{\prime} \cup (A \setminus P)$. Then, based on our variant of the Neyman-Pearson Lemma,  we  derive an upper bound of $p_j^{\prime}$ using the probability that the random submatrix $\mathbf{Y}$ is in the subset $C_j$, i.e., we have:
\begin{align}
\label{upper_bound_single_mainbody}
    p_j^{\prime} \leq \text{Pr}(\mathbf{Y} \in C_j).
\end{align} 
In our derivation, we further improve the upper bound via jointly considering multiple items in $\mathcal{I} \setminus \mathcal{I}_u$. Suppose $\mathcal{H}_{c} \subseteq \mathcal{I}\setminus\mathcal{I}_u$ is a set of $c$ items.  We denote
$\overline{p}_{\mathcal{H}_{c}}=\sum_{j \in \mathcal{H}_{c}}\overline{p}_{j}$.
Then, we can find a subset $C_{\mathcal{H}_{c}}^{\prime}$ such that we have the following:  $\text{Pr}(\mathbf{X}\in C^{\prime}_{\mathcal{H}_{c}}) = \overline{p}^{*}_{\mathcal{H}_{c}} \triangleq \frac{\lceil (\overline{p}_{\mathcal{H}_{c}}/N')\cdot {n \choose s} \rceil}{ {n \choose s}  }$. 
Given the subset $C_{\mathcal{H}_{c}}^{\prime}$, we further define a subset $C_{\mathcal{H}_{c}} = C_{\mathcal{H}_{c}}^{\prime} \cup (A \setminus P)$.
Then,  we have the following upper bound for the smallest poisoned item probability in the set $\{p_j^{\prime} | j \in \mathcal{H}_{c}\}$:
\begin{align}
\label{upper_bound_combined_mainbody}
    \min_{j \in \mathcal{H}_{c}} p_{j}^{\prime} \leq \frac{N' \cdot \text{Pr}(\mathbf{Y}\in C_{\mathcal{H}_{c}})}{c}.
\end{align}
Finally, we can combine the upper bounds in Equation~(\ref{upper_bound_single_mainbody}) and~(\ref{upper_bound_combined_mainbody}) to derive an upper bound of the $(N-r+1)$th largest poisoned item probability in $\mathcal{I} \setminus \mathcal{I}_u$. Note that we don't jointly consider multiple items in $\mathcal{I}_u$ when deriving the lower bounds for poisoned item probabilities in $\mathcal{I}_u$ because it does not improve the lower bounds. 


Formally, we have the following theorem: 
\begin{theorem}
\label{theorem_certified_size}
Suppose we have a rating-score matrix $\mathbf{M}$, a user $u$, 
 and an arbitrary set of $k$ items $\mathcal{I}_u=\{\mu_1,\mu_2,\cdots,\mu_k\}$.  
Furthermore, we have a lower bound  $\underline{p_{i}}$ for each $i \in \mathcal{I}_u$ and an upper bound $\overline{p}_{j}$ for each $j \in \mathcal{I}\setminus \mathcal{I}_u$ that satisfy  Equation~(\ref{probability_upper_low_bound_theorem_1}). 
Without loss of generality, we assume $\underline{p_{\mu_1}}\geq \underline{p_{\mu_2}} \geq \cdots \geq \underline{p_{\mu_k}}$.
Under any data poisoning attacks with at most  $e$ fake users, we have the following guarantee: $\min_{\mathbf{M}' \in \mathcal{L}(\mathbf{M},e)} |\mathcal{I}_{u} \cap \mathcal{T} (\mathbf{M}', u)| \geq r$,  
where $r$ is the solution to the following optimization problem or $0$ if it does not have a solution:
\begin{align}
\label{optimization_problem_theorem_1}
     r & =\argmax_{r'\in\{1,2,\cdots,\min(k, N)\}} r' \nonumber \\
     \text{s.t. } & 
           \underline{p^{*}_{\mu_{r'}}} >   \min ( \min_{c=1}^{N-r' +1}\frac{ N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} + \sigma)}{c}, 
     \overline{p}^{*}_{v_1}+ \sigma),  
\end{align}
where $n' = n + e$, $\sigma = \frac{s}{n'} \cdot \frac{{n' \choose s}}{{n \choose s}} -\frac{s}{n}$ ,  $ \underline{p^{*}_{\mu_{r'}}} =\frac{\lfloor \underline{p_{\mu_{r'}}} \cdot {n \choose s}\rfloor}{{n \choose s}}$, $\mathcal{H}_{c}=\{v_1, v_2, \cdots, v_{c}\}$ is the set of $c$ items that have the smallest item-probability upper bounds among the $N-r'+1$ items with the largest item-probability upper bounds in $\mathcal{I}\setminus \mathcal{I}_u $, $v_1$ is the item in $\mathcal{H}_1$, $\overline{p}_{\mathcal{H}_{c}}=\sum_{j \in \mathcal{H}_{c}}\overline{p}_{j}$, $ \overline{p}^{*}_{\mathcal{H}_{c}} = \frac{\lceil (\overline{p}_{\mathcal{H}_{c}}/N')\cdot {n \choose s} \rceil}{ {n \choose s}  }$, and $\overline{p}^{*}_{v_1} = \frac{\lceil \overline{p}_{v_1} \cdot {n \choose s}\rceil}{{n \choose s}} $. 
\end{theorem}
\begin{proof}
See Appendix~\ref{proof_of_certified_theorem}. 
\end{proof}



\subsection{Computing the Certified Intersection Size}
Given a base algorithm $\mathcal{A}$, a  rating-score matrix $\mathbf{M}$, a set of genuine users $\mathcal{U} = \{u_1, u_2, \cdots, u_n\}$, a set of items $\mathcal{I}_{u}$ for each genuine user $u$, the maximum number of fake users $e$, and a sampling size $s$, we aim to compute the certified intersection size of our ensemble recommender system for each user in $\mathcal{U}$. The key to compute the certified intersection size is to solve $r$ in the optimization problem in Equation~(\ref{optimization_problem_theorem_1}). Specifically, given a user $u$, the key challenge to solve the optimization problem in Equation~(\ref{optimization_problem_theorem_1}) is how to estimate the 
item-probability lower bounds $\underline{p_i}$ for $\forall i \in \mathcal{I}_u$ and upper bounds $\overline{p}_j$ for $\forall j \in \mathcal{I}\setminus \mathcal{I}_{u}$. One naive way is to build ${n \choose s}$ base recommender systems and compute the exact item probabilities. However, such approach is computationally infeasible as ${n \choose s}$ is huge. To address the challenge, we design an algorithm to estimate lower/upper bounds of item probabilities via building $T << {n \choose s}$ base recommender systems. Next, we introduce  estimating the lower/upper bounds of the item probabilities,  solving $r$ using the estimated item-probability bounds, and our complete algorithm. 


\myparatight{Estimating the item-probability lower/upper bounds} We randomly sample $T$ submatrices from $\mathbf{M}$, where each submatrix contains  $s$ rows of $\mathbf{M}$. For simplicity, we denote them as $\Gamma_1, \Gamma_2, \cdots, \Gamma_T$. Then, we build a base recommender system for each submatrix $\Gamma_{t}$ using the base  algorithm $\mathcal{A}$, where $t=1, 2, \cdots, T$. Given a user $u$, we use each base recommender system to recommend top-$N'$ items for the user. We denote by $\mathcal{A}(\Gamma_t, u)$  the set of top-$N'$ items recommended to the user $u$ by the base recommender system built on the submatrix $\Gamma_t$. Note that  $\mathcal{A}(\Gamma_t, u)$ is empty if the user $u$ is not in the submatrix $\Gamma_t$. We denote by $T_i$ the frequency of an item $i$  among the recommended top-$N'$ items of the $T$ base recommender systems, i.e., $T_i$ is the number of base recommender systems whose top-$N'$ recommended items for $u$ include $i$. Based on the definition of  item probability $p_i$, the frequency $T_i$ follows a binomial distribution with parameters $T$ and $p_i$,  i.e., we have the following: $\text{Pr}(T_i = t) = {T \choose t} \cdot p_i^t \cdot (1 - p_i)^{T - t}$, 
where $t=0, 1,  \cdots, T$. 
 Our goal is to estimate a lower or upper bound of $p_i$ given $T_i$ and $T$. This is essentially a \emph{binomial proportion confidence interval} estimation problem. Therefore, we can leverage the standard Clopper-Pearson method~\cite{clopper1934use} to estimate a lower or upper bound of $p_i$ from a given $T_i$ and $T$. Formally, we have the following:
\begin{align}
\label{cp_lower_bound}
    \underline{p_i} & = \text{Beta}(\frac{\alpha_u}{m}; T_i, T-T_i+1), i\in \mathcal{I}_u, \\
    \label{cp_upper_bound}
    \overline{p}_j  &= \text{Beta}(1-\frac{\alpha_u}{m}; T_j, T-T_j+1), j \in \mathcal{I}\setminus \mathcal{I}_{u},  
\end{align}
where  $1 - \alpha_u/m$ is the confidence level for estimating the lower/upper bound of one item probability, $m$ is the total number of items, and $\text{Beta}(\beta;\varsigma, \vartheta)$ is the $\beta$th quantile of the Beta distribution with shape parameters $\varsigma$ and $\vartheta$. Based on  \emph{Bonferroni correction} in statistics, the \emph{simultaneous confidence level} of estimating the lower/upper bounds of the $m$ item probabilities is at least $1 - \alpha_u$. Given an item set $\mathcal{H}_{c}$ defined in Theorem~\ref{theorem_certified_size}, we can estimate $\overline{p}_{\mathcal{H}_{c}}$ as  $\overline{p}_{\mathcal{H}_{c}} = \min(\sum_{j \in \mathcal{H}_{c}} \overline{p}_{j}, N' - \sum_{i \in \mathcal{I}_u} \underline{p_i} )$,  where both $\sum_{j \in \mathcal{H}_{c}} \overline{p}_{j}$ and $N' - \sum_{i \in \mathcal{I}_u} \underline{p_i}$ are upper bounds of ${p}_{\mathcal{H}_{c}}$, and we use the smaller one. 
 



\begin{algorithm}[tb]
   \caption{\textsc{BinarySearch}}
   \label{alg:binary_search}
\begin{algorithmic}
   \STATE {\bfseries Input:} $e$, $s$, $N'$, $N$, $ \mathcal{I}_{u},\{\underline{p_i}|i\in \mathcal{I}_{u}\}$, and $ \{\overline{p}_j|j\in \mathcal{I}\setminus\mathcal{I}_{u}\}$
   \STATE {\bfseries Output:}  $r_u$ \\
   $low, high \gets 1, \min(|\mathcal{I}_u|,N)$ \\
   \WHILE{$low < high $}
   \STATE $r' =\lceil (low+high)/2 \rceil$ \\
   \IF{\textsc{VerifyConstraint($r',e, s, N', N,\mathcal{I}_{u},\{\underline{p_i}|i\in \mathcal{I}_{u}\},\{\overline{p}_j|j\in \mathcal{I}\setminus\mathcal{I}_{u}\}$)} $== 1$}
   \STATE $low \gets r' $ \\
   \ELSE 
   \STATE $high \gets r' - 1$ \\
   \ENDIF
   \ENDWHILE
    \IF{\textsc{VerifyConstraint($r',e, s, N', N,\mathcal{I}_{u},\{\underline{p_i}|i\in \mathcal{I}_{u}\},\{\overline{p}_j|j\in \mathcal{I}\setminus\mathcal{I}_{u}\}$)} $== 1$}
   \STATE return $ r' $ \\
   \ELSE 
   \STATE return $ 0$ \\
   \ENDIF
\end{algorithmic}
\end{algorithm}







\myparatight{Solving the optimization problem}  We note that  Equation~(\ref{optimization_problem_theorem_1}) has the following property: its left-hand side and right-hand side respectively decreases and increases as $r'$ increases. Thus, given the estimated item-probability bounds, we can efficiently solve the optimization problem in Equation~(\ref{optimization_problem_theorem_1}) via binary search to obtain $r_u$ for each user $u$. Algorithm~\ref{alg:binary_search} shows our \textsc{BinarySearch} algorithm. The function \textsc{VerifyConstraint}  verifies whether the constraint in Equation~(\ref{optimization_problem_theorem_1}) is satisfied for a given $r'$ and returns 1 if so. 





\begin{algorithm}[tb]
   \caption{\textsc{Compute $r$}}
   \label{alg:certify}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\mathbf{M}$, $s$, $T$, $\mathcal{A}$, $N'$, $\alpha$, $e$, $N$, $\mathcal{U}$, and $\{\mathcal{I}_u| u\in \mathcal{U}\}$
   \STATE {\bfseries Output:}  $r_u$ for each user $u\in \mathcal{U}$ \\
   $\Gamma_1, \Gamma_2,\cdots,\Gamma_T \gets  \textsc{RandomSample}(\mathbf{M},s)$ \\
   \FOR{$u$ {\bfseries in} $\mathcal{U}$}
   \STATE counts$[i] \gets \sum_{t=1}^{T}\mathbb{I}(i \in \mathcal{A}(\Gamma_t, u)), i\in \{1,2,\cdots,m\} $ \\
   \STATE $\underline{p_{i}}, \overline{p}_{j} \gets \textsc{BoundEst}(\text{counts},\frac{\alpha}{n}), i \in \mathcal{I}_{u}, j \in \mathcal{I}\setminus \mathcal{I}_{u}$ \\
   \STATE $r_{u} = \textsc{BinarySearch}(e, s, N', N,\mathcal{I}_{u},\{\underline{p_i}|i\in \mathcal{I}_{u}\},\{\overline{p}_j|j\in \mathcal{I}\setminus\mathcal{I}_{u}\})$
   \ENDFOR
   \STATE \textbf{return}
 $\{r_u|u\in \mathcal{U}\}$
\end{algorithmic}
\end{algorithm}








\myparatight{Complete algorithm} Algorithm~\ref{alg:certify} shows our complete algorithm to compute the certified intersection size $r_u$ for each user $u\in \mathcal{U}=\{u_1, u_2, \cdots, u_n\}$.
The function \textsc{RandomSample} randomly samples $T$ submatrices,   each of which contains $s$ rows sampled from $\mathbf{M}$ uniformly at random. 
\textsc{BoundEst} estimates the item-probability bounds with a confidence level $1-\frac{\alpha}{n}$, i.e., $\alpha_u = \frac{\alpha}{n}$, for each user $u \in \mathcal{U}$,  based on Equation~(\ref{cp_lower_bound})~-~(\ref{cp_upper_bound}). 
 \textsc{BinarySearch} solves the optimization problem in Equation~(\ref{optimization_problem_theorem_1}) via binary search to obtain $r_u$ for $u$ based on the estimated item-probability bounds. \neil{Note that Algorithm~\ref{alg:certify} requires a clean rating-score matrix $\mathbf{M}$, which may be sampled from the clean data distribution.} 


Due to randomness, the estimated item-probability bounds may be incorrect, e.g., an estimated item-probability lower bound is larger than the true item probability for some item and some user or an estimated item-probability upper bound is smaller than the true item probability for some item and some user. When such estimation error happens for a user $u$, the solved certified intersection size $r_u$ is incorrect for $u$. Since the simultaneous confidence level of estimating the item-probability bounds in our algorithm is at least $1-\frac{\alpha}{n}$ for any user $u \in \mathcal{U}$, the probability of having at least one incorrectly estimated item-probability bound  for any user $u \in \mathcal{U}$ is at most $\frac{\alpha}{n}$.  
Moreover, our following theorem shows that the probability of having an incorrect certified intersection size $r_u$ for at least one user in $\mathcal{U}$ is bounded by $\alpha$: 
\begin{theorem}
\label{proposition_1}
The probability that our Algorithm~\ref{alg:certify} returns an incorrect $r_u$
for at least one user in $\mathcal{U}$ is at most $\alpha$.
\end{theorem}
\begin{proof}
See Appendix~\ref{proof_of_proposition}. 
\end{proof}












