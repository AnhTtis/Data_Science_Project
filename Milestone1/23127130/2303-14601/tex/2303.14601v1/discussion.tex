\section{\CR{Discussion and Limitations}}
\label{discussion_limitation}

\vspace{-2mm}
\myparatight{\neil{Theoretical guarantees}} \neil{For any given number of fake users, our method can derive a certified intersection size $r$ for each user. Note that our theoretical guarantee still holds even if fake users rate new items.  However, when the fraction of fake users is large (e.g., 49\%), the derived $r$ and the corresponding certified Precision@$N$/Recall@$N$/F1-Score@$N$ may reduce to 0. As the first step on provably robust recommender systems, our method can derive a non-zero $r$ against a moderate number of fake users. It is still an open challenge to derive a non-trivial $r$ for a large fraction of fake users. Essentially, there is a trade-off between performance without attack and robustness, which is controlled by  $s$ (number of users in each submatrix). When the fraction of fake users is large, using a small $s$ makes the ensemble recommender system more robust but less accurate. 


There are several directions to further improve the theoretical guarantees, i.e., derive a larger $r$ for a given fraction of fake users. First,  we considered a very strong threat model, where each fake user can arbitrarily rate all items. For instance, in MovieLens-100k, each fake user can rate up to 1,682 items, which accounts for 1.7\% of the rating scores from all genuine users. Fake users that rate a large number of items can be easily detected, as genuine users often rate a small number of items. Therefore, one way to further improve theoretical guarantee is to consider fake users that rate a bounded number of items. Second,  PORE is applicable to any base recommender system algorithm without considering the knowledge of the base algorithm. Therefore, the second possible way to derive better theoretical guarantees is to consider the domain knowledge of a specific base algorithm. }


\myparatight{\neil{Base algorithms and voting mechanisms}} \neil{We focus on using the same base algorithm to train each base recommender system in this work. We note that  the base recommender systems can be trained using different base algorithms. In particular, our theoretical guarantee holds for any (randomized) base algorithm. Therefore, given a set of base algorithms, we can randomly pick one to train a base recommender system. Moreover, we can view each base recommender system is trained using a randomized base algorithm sampled from the set of base algorithms. PORE uses hard voting when aggregating the items recommended by the base recommender systems. Hard voting has to be used to derive the theoretical guarantee of the ensemble recommender system.}
 
\myparatight{\neil{Targeted data poisoning attacks}} \neil{In this work, we focus on untargeted data poisoning attacks, which aim to reduce the overall performance of a recommender system. Our method can guarantee a lower bound of recommendation performance against any untargeted data poisoning attacks. Targeted data poisoning attacks aim to promote specific attacker-chosen items (called \emph{target items})~\cite{yang2017fake}. It is an interesting future work to derive provable robustness guarantees against such attacks. Specifically, given a fraction of fake users, we aim to derive an upper bound of the number of genuine users, to which the target items are recommended. }

