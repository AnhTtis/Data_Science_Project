\section{Background}
\label{problem}


\begin{figure*}[!t]
	 \centering
{\includegraphics[width=0.9\textwidth]{figs/poisoningattacks_07212021.pdf}} 
\caption{\emph{Left}: a recommender system without data poisoning attacks.  \emph{Right}: an attacker manipulates the recommended items for  users $u_1$ and $u_3$ by injecting a fake user (i.e., $u_5$) into the system.}
\label{fig:attacks}
\end{figure*}

\subsection{Recommender Systems}
\myparatight{Rating-score matrix} Suppose we have $n$ users and $m$ items which are denoted as $\mathcal{U}=\{u_1, u_2, \cdots, u_n\}$ and $\mathcal{I}=\{i_1, i_2, \cdots, i_m\}$, respectively. We use $\mathbf{M}$ to denote the  rating-score matrix which has $n$ rows and $m$ columns, where a row corresponds to a user and a column corresponds to an item. Essentially, the rating-score matrix $\mathbf{M}$ captures the users' interests towards different items.  In particular, an entry $\mathbf{M}_{ui}$  represents the rating score that the user $u$ gave to the item $i$. For instance, the rating score could be an integer in the range $[1, 5]$, where $1$ is the lowest rating score and denotes that the item does not attract the user's interest, and $5$ is the largest rating score and denotes that the item attracts the user's interest substantially. We note that our method is applicable to any type of rating scores, e.g., binary, integer-valued, and continuous. $\mathbf{M}_{ui} = 0$ means that the user $u$ has not rated the item $i$ yet. For convenience, we denote by $\mathcal{R}$ the domain of a rating score including 0, i.e., $\mathbf{M}_{ui} \in \mathcal{R}$.  

\myparatight{Top-$N$ recommended items} A recommender system aims to help users discover new items that may arouse his/her interests. 
A recommender system algorithm takes the rating-score matrix $\mathbf{M}$ as input and recommends top-$N$ items to each user that he/she has not rated yet but is potentially interested in. 
For simplicity, we use $\mathcal{A}$ to denote a recommender system algorithm. Moreover, we use $\mathcal{A}(\mathbf{M},u)$ to denote the set of top-$N$ items recommended to  user $u$ when the recommender system  is built by $\mathcal{A}$ on $\mathbf{M}$. 




\myparatight{Recommender system algorithms} Many algorithms have been proposed to build recommender systems such as Item-based Recommendation (IR)~\cite{sarwar2001item,linden2003amazon,argyriou2020microsoft}, Bayesian Personalized Ranking (BPR)~\cite{rendle2012bpr}, Matrix Factorization~\cite{koren2009matrix}, Neural Collaborative Filtering (NCF)~\cite{he2017neural}, and LightGCN~\cite{he2020lightgcn}. For instance, {IR} 
calculates the similarities between different items based on their rating scores, predicts users' missing rating scores using  such similarities, and recommends a user the $N$ items that the user has not rated yet but have the largest predicted rating scores. Due to its scalability, IR has been widely deployed in industry, e.g., Amazon~\cite{linden2003amazon}. 
According to recent benchmarks released by Microsoft~\cite{Recommenders_url}, BPR achieves state-of-the-art  performance, e.g., BPR even outperforms more complex algorithms such as NCF~\cite{he2017neural} and LightGCN~\cite{he2020lightgcn}. 



\subsection{Data Poisoning Attacks}
Many studies~\cite{lam2004shilling,mobasher2007toward,li2016data,yang2017fake,fang2018poisoning,fang2020influence,huangdata2021} 
showed that recommender systems are not robust to data poisoning attacks (Section~\ref{related} discusses more details).  
In a data poisoning attack, an attacker creates fake users in a recommender system and assigns carefully crafted rating scores to them, such that the recommender system, which is built based on the rating scores of  genuine and fake users,  makes attacker-desired, arbitrary recommendations. 
For instance, a data poisoning attack could substantially degrade the performance of a recommender system; and a data poisoning attack could promote certain items (e.g., videos on YouTube and products on Amazon) via spoofing a recommender system  to recommend  them to many genuine users. \CR{Figure~\ref{fig:attacks} illustrates data poisoning attacks.}

We denote by $\mathbf{M}'$ the \emph{poisoned-rating-score matrix}.  A data poisoning attack  aims to reduce the intersection between  $\mathcal{A}(\mathbf{M},u)$ and $\mathcal{A}(\mathbf{M}',u)$ via carefully designing the rating scores of the fake users. Different attacks essentially assign different rating scores to the fake users. 






\section{Problem Formulation}
\myparatight{Threat model}
We assume an attacker can inject fake users into a recommender system via registering and maintaining fake accounts~\cite{thomas2013trafficking}. 
We consider an attacker can inject at most $e$ fake users into a recommender system, e.g., because of limited resources to register and maintain fake accounts. However, we assume each fake user can arbitrarily rate as many items as the attacker desires. Moreover, we assume the attacker has whitebox access to the recommender system, e.g., the attacker has access to the rating scores of all genuine users as well as the recommender system algorithm and its parameters. In other words, we consider strong attackers, who can perform any data poisoning attacks. 

A poisoned-rating-score matrix $\mathbf{M}'$ extends the rating-score matrix  $\mathbf{M}$ by at most $e$ rows, which correspond to the rating scores of the at most $e$ fake users. Different data poisoning attacks essentially select different rating scores for the fake users and  result in different poisoned-rating-score matrix $\mathbf{M}'$. We use $\mathcal{L}(\mathbf{M},e)$ to denote the set of all possible poisoned-rating-score matrices when the clean rating-score matrix is $\mathbf{M}$ and the number of fake users is at most $e$. $\mathcal{L}(\mathbf{M},e)$ essentially denotes all possible data poisoning attacks with at most $e$ fake users.  Formally, we define $\mathcal{L}(\mathbf{M},e)$ as follows: 
\begin{align}
   \mathcal{L}(\mathbf{M},e) = \{\mathbf{M}' | \mathbf{M}'_{ui}=\mathbf{M}_{ui} \text{ and } \mathbf{M}'_{vi} \in \mathcal{R}, \nonumber\\ \forall u\in \mathcal{U}, v\in \mathcal{V}, i\in \mathcal{I} \}, 
\end{align}
where $\mathcal{R}$ is the domain of a rating score, $\mathcal{U}$ is the set of genuine users, $\mathcal{V}$ is the set of at most $e$ fake users (i.e., $|\mathcal{V}|\leq e$), and $\mathcal{I}$ is the set of items. 


\begin{figure*}[!t]
	 \centering
{\includegraphics[width=0.9\textwidth]{figs/ensemblerecom_07212021.pdf}} 
\caption{Robustness of our ensemble recommender system against data poisoning attacks.}
\label{fig:overrview}
\end{figure*}


\myparatight{Provably robust recommender system algorithm}
We say a recommender system algorithm is provably robust against data poisoning attacks if a certain number of its recommended top-$N$ items for a user are provably unaffected by any data poisoning attacks. 
Specifically, given a set of items $\mathcal{I}_u$, we say a recommender system algorithm $\mathcal{A}$ is provably robust for a user $u$ if it satisfies the following property: 
\begin{align}
\label{intersectionsizeproperty}
     \min_{\mathbf{M}' \in \mathcal{L}(\mathbf{M},e)} |\mathcal{I}_{u} \cap \mathcal{A}(\mathbf{M}',u)|\geq r,
\end{align}
where $\mathcal{L}(\mathbf{M},e)$ is the set of all possible poisoned-rating-score matrices (i.e., all possible data poisoning attacks with at most $e$ fake users),  $|\mathcal{I}_{u} \cap \mathcal{A}(\mathbf{M}',u)|$ is the size of  the intersection between $\mathcal{I}_{u}$ and the top-$N$ items recommended to $u$ by $\mathcal{A}$ under attacks, and $r$ is called \emph{certified intersection size}. Note that $r$ may depend on the user $u$ and the number of fake users $e$, but we omit its explicit dependency on $u$ and $e$ for simplicity.  

When $\mathcal{I}_u$ is the set of top-$N$ items recommended to user $u$ by $\mathcal{A}$ under no attacks, i.e., $\mathcal{I}_u=\mathcal{A}(\mathbf{M},u)$, our provable robustness means that at least $r$ of the $N$ items in $\mathcal{A}(\mathbf{M},u)$ are still recommended to $u$ under any attacks with at most $e$ fake users. When $\mathcal{I}_u$ is the set of ground truth test items for $u$ (i.e., the set of items that $u$ is indeed interested in), our provable robustness means that at least $r$ of the ground truth test items are recommended to $u$ under attacks. As we will discuss more details in experiments,   $r$ in the latter case can be used to derive a lower bound of recommendation performance such as Precision@$N$,  Recall@$N$, and F1-Score@$N$ under any data poisoning attacks. We formally define a provably robust recommender system algorithm as follows: 
\begin{definition}[$(e,r)$-Provably Robust Recommender System]
Suppose the number of fake users is at most $e$ and a data poisoning attack can arbitrarily craft the rating scores for the fake users. We say a recommender system algorithm $\mathcal{A}$ is $(e,r)$-provably robust for a user $u$ if its certified intersection size for $u$ is at least $r$, i.e., if Equation~(\ref{intersectionsizeproperty}) is satisfied.
\end{definition}

