



\section{Related Work}
\label{related}


\myparatight{Data poisoning attacks to recommender systems} Many data poisoning attacks to recommender systems~\cite{lam2004shilling,burke2005segment,mobasher2007toward,seminario2014attacking,li2016data,yang2017fake,fang2018poisoning,christakopoulou2019adversarial,zhang2020practical,fang2020influence,song2020poisonrec,wadhwa2020data,tang2020revisiting,huangdata2021,zhang2021data,wu2021triple} have been proposed. Early attacks are algorithm-agnostic, i.e., the crafted rating scores of fake users do not depend on the recommender system algorithm~\cite{lam2004shilling,burke2005segment,mobasher2007toward}. 
Recently,  more advanced data poisoning attacks~\cite{li2016data,yang2017fake,fang2018poisoning,fang2020influence,song2020poisonrec,huangdata2021,zhang2021data,wu2021triple} have been optimized for  specific recommender system algorithms. For instance, Yang et al.~\cite{yang2017fake} proposed to inject fake co-visitations to poison association rule based recommender systems, Fang et al. proposed optimized data poisoning attacks to graph based recommender systems~\cite{fang2018poisoning} and matrix factorization based recommender systems~\cite{fang2020influence}, and Huang et al.~\cite{huangdata2021} proposed data poisoning attacks optimized to deep learning based recommender systems. 

\myparatight{Empirical defenses against data poisoning attacks to recommender systems} 
One family of defenses~\cite{burke2006classification,zhang2006attack,wu2012hysad,zhang2014hht,fang2018poisoning} aim to detect  fake users via analyzing their abnormal rating score patterns. The key assumption is  that the rating scores of fake users and genuine users have different patterns. 
Fo instance, Burke~\cite{burke2006classification} extracted features from each user's rating scores and trained a classifier to predict whether a user is fake or not. 
\neil{Another family of defenses~\cite{sandvig2007robustness,mehta2007robust,tang2019adversarial,chen2019adversarial,yuan2019adversarial,liu2020certifiable,hidano2020recommender} try to train more robust recommender systems. For instance, adversarial training~\cite{goodfellow2015explaining}, which was developed to train robust machine learning classifiers, has been extended to train robust recommender systems by multiple work~\cite{tang2019adversarial,yuan2019adversarial}. However, none of the above defenses provides provable robustness guarantees. In particular, they cannot derive certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$. As a result, they can still be attacked by adaptive attacks.} 

\myparatight{Ensemble recommender systems}
Ensemble methods have been explored to improve the empirical performance of  recommender systems~\cite{bell2008bellkor,toscher2009bigchaos,wu2007collaborative}. For instance, around a decade ago, the winning teams~\cite{bell2008bellkor,toscher2009bigchaos} in the well-known Netflix competition on 
predicting user rating scores for movies  blended multiple base recommender systems built by different base algorithms. 
However,  these studies are different from ours. The key difference is that they didn't derive the provable robustness guarantees for their ensemble recommender systems. 
Moreover,  they use different ways to aggregate the base recommender systems, e.g., they aggregated the rating scores predicted by the base recommender systems.  

\myparatight{Provably robust classifiers against data poisoning attacks} Several works~\cite{jia2020intrinsic,jia2022certified} proposed  certified defenses against data poisoning attacks for machine learning classifiers. The key difference between classifiers and recommender systems is that an input has a single ground-truth label in a classifier while a user has multiple ground-truth items in a recommender system. As a result, these methods achieve sub-optimal certified robustness guarantees when generalized to recommender systems as shown in our experimental results.
