










\section{Evaluation}
\label{evaluation}
\subsection{Experimental Setup}
\vspace{-2mm}
\myparatight{Datasets} We mainly evaluate  PORE on MovieLens-100k and MovieLens-1M benchmark datasets~\cite{movielens,harper2015movielens}, which consist of around $100,000$ and $1,000,000$ rating scores, respectively.  Specifically, MovieLens-100k  contains $943$ users and $1,682$ items, where each user rated $106$ items on average. MovieLens-1M  contains $6,040$ users and $3,952$ items, where each user on average rated $166$ items. Following~\cite{argyriou2020microsoft}, for each user, we sample $75\%$ of its rating scores as training data and treat its remaining rated items as test items. The users' training data form the rating-score matrix and are used to build recommender systems, while their test items are used to evaluate the performance of the recommended top-$N$ items.  

 \myparatight{Base algorithms} PORE is applicable to any base algorithm. To show such generality, we evaluate  two  base  algorithms, i.e.,  Item-based  Recommendation  (IR) ~\cite{argyriou2020microsoft} and Bayesian Personalized Ranking (BPR)~\cite{rendle2012bpr}. We adopt their public implementations~\cite{argyriou2020microsoft}. 
{We adopt IR because it  has been widely deployed in industry~\cite{linden2003amazon}. We adopt BPR because it achieves state-of-the-art  performance according to recent benchmarks released by Microsoft~\cite{Recommenders_url}.} 


\begin{figure*}[!t]
	 \centering
	 \vspace{-2mm}
{\includegraphics[width=0.16\textwidth]{figs/compare_with_bagging/100k_sar_200_100000_10_Certified-Precision-10.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/compare_with_bagging/100k_sar_200_100000_10_Certified-Recall-10.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/compare_with_bagging/100k_sar_200_100000_10_Certified-F1-Score-10.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/compare_with_bagging/1m_sar_200_100000_10_Certified-Precision-10.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/compare_with_bagging/1m_sar_200_100000_10_Certified-Recall-10.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/compare_with_bagging/1m_sar_200_100000_10_Certified-F1-Score-10.pdf}}
\vspace{-3mm}
\caption{Our PORE outperforms bagging when extended from classifiers to recommender systems  on MovieLens-100k (left three) and MovieLens-1M (right three), where $N=10$ and base algorithm is IR.}
\label{compare_with_existing_defense}
\end{figure*}



\begin{figure*}[!t]
	 \centering
	 \vspace{-2mm}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/k/100k/100k_change_k_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/k/100k/100k_change_k_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/k/100k/100k_change_k_f.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/k/1m/1m_change_k_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/k/1m/1m_change_k_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/k/1m/1m_change_k_f.pdf}}
\vspace{-3mm}
\caption{Impact of $N'$ on the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR for MovieLens-100k (left three) and MovieLens-1M (right three), where $N=10$.}
\label{impact_of_N_prime}
\end{figure*}




\myparatight{Evaluation metrics} When there are no data poisoning attacks,  \emph{Precision@$N$},  \emph{Recall@$N$}, and  \emph{F1-Score@$N$} are standard metrics to evaluate the performance of a recommender system. We denote by $\mathcal{E}_u$ the  set of test items for a user $u$. Precision@$N$ for a user $u$ is the fraction of the top-$N$ items recommended for $u$ that are  in $\mathcal{E}_u$,   Recall@$N$ for $u$ is the fraction of the items in $\mathcal{E}_u$ that are  in the top-$N$  items recommended for $u$, while F1-Score@$N$ for a user $u$ is the harmonic mean of the user's Precision@$N$ and Recall@$N$. The Precision@$N$ (or Recall@$N$ or F1-Score@$N$) of a recommender system algorithm is the users' average  Precision@$N$ (or Recall@$N$ or F1-Score@$N$). 

Under data poisoning attacks,  Precision@$N$, Recall@$N$, and F1-Score@$N$ are insufficient to evaluate a recommender system algorithm. This is because they may be different under different data poisoning attacks, and it is infeasible to enumerate all possible attacks.  To address the challenge, 
we propose to evaluate a recommender system algorithm using \emph{certified Precision@$N$},  \emph{certified Recall@$N$}, and  \emph{certified F1-Score@$N$} under attacks. Like the standard Precision@$N$ (or Recall@$N$ or F1-Score@$N$), calculating our {certified Precision@$N$},  {certified Recall@$N$}, and  {certified F1-Score@$N$} also only requires a clean rating-score matrix and thus does not depend on any specific data poisoning attack.  Certified Precision@$N$ (or certified Recall@$N$ or certified F1-Score@$N$) is a \emph{lower bound} of Precision@$N$ (or Recall@$N$ or F1-Score@$N$) under any data poisoning attacks with at most $e$ fake users. 
For instance, a certified Precision@$N$ of 0.3 means that a recommender system achieves at least Precision@$N$ of 0.3 when the number of fake users is at most $e$, no matter what rating scores they use.  Specifically, our certified Precision@$N$ for a user $u$ is the least fraction of the top-$N$ recommended items  for $u$ that are guaranteed to be in $\mathcal{E}_u$ when there are at most $e$ fake users;  our certified Recall@$N$ for $u$ is the least fraction of $u$'s test items $\mathcal{E}_u$ that are guaranteed to be in the top-$N$ recommended items; while certified F1-Score@$N$ for a user is the harmonic mean of the user's certified Precision@$N$ and certified Recall@$N$. Formally, we have the following for a user $u$: 
\begin{align}
\label{definition_of_cpn}
 &   \text{Certified Precision@}N = \min_{\mathbf{M}' \in \mathcal{L}(\mathbf{M},e)} \frac{|\mathcal{E}_{u} \cap \mathcal{T} (\mathbf{M}', u)|}{N}, \\
 \label{definition_of_crn}
&    \text{Certified Recall@}N = \min_{\mathbf{M}' \in \mathcal{L}(\mathbf{M},e)} \frac{|\mathcal{E}_{u} \cap \mathcal{T} (\mathbf{M}', u)|}{|\mathcal{E}_{u}|}, \\
\label{definition_of_cfn}
&    \text{Certified F1-Score@}N = \min_{\mathbf{M}' \in \mathcal{L}(\mathbf{M},e)} \frac{2 \cdot |\mathcal{E}_{u} \cap \mathcal{T} (\mathbf{M}', u)|}{|\mathcal{E}_{u}|+N}, 
\end{align}
where $|\cdot|$ is the size of a set. We can compute the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ for each user by Algorithm~\ref{alg:certify}. In particular, we can compute certified intersection size $r_u$ for each user $u$ using Algorithm~\ref{alg:certify} by letting $\mathcal{I}_u = \mathcal{E}_u$. Given $r_u$, the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ for a user $u$ are at least $\frac{r_u}{N}$, $\frac{r_u}{|\mathcal{E}_{u}|}$, and $\frac{2r_u}{|\mathcal{E}_{u}|+N}$, respectively.
A recommender system algorithm's certified Precision@$N$ (or  Recall@$N$ or  F1-Score@$N$) is the average of the \emph{genuine users'} certified Precision@$N$ (or  Recall@$N$ or  F1-Score@$N$). 


\myparatight{Compared methods} We note that PORE is the first provably robust recommender system algorithm against data poisoning attacks. Therefore, there are no prior recommender system algorithms we can compare with in terms of \emph{certified} Precision@$N$, Recall@$N$, and F1-Score@$N$. However, Jia et al.~\cite{jia2020intrinsic} showed that bagging can be used to build provably robust defense against data poisoning attacks for \emph{machine learning classifiers}, which we extend to recommender systems and compare with our PORE.  
Roughly speaking, given a training dataset, bagging trains multiple base classifiers,  each of which is trained on a random subset of training examples in the training dataset. Given a testing input, bagging uses each base classifier to predict its label and takes a majority vote among the predicted labels as the final predicted label for the testing input. Jia et al. showed that bagging can guarantee that the predicted label for an input is provably unaffected by a bounded number of fake training examples injected into the training dataset. 

We generalize their provable guarantee to derive certified intersection size for each genuine user in recommender systems, which can be then used to compute certified Precision@$N$, Recall@$N$, and F1-Score@$N$ for bagging. Specifically, we treat a user as a testing input, an item as a label, and a base recommender system as a base classifier in the terminology of bagging. Like our PORE, the generalized bagging builds $T$ base recommender systems and takes majority vote among them to recommend top-$N$ items to each user.  Note that since a base classifier predicts one label for a testing input, we set $N'=1$, i.e., a base recommender system (i.e., a base classifier) recommends top-1 item (i.e., predicts one label) for a user (i.e., a testing input). Finally, for each user, bagging recommends him/her the $N$ items with the largest (poisoned) item probabilities. Given a set of items $\mathcal{I}_u$ for a user $u$, we denote by $\underline{p_i}$  a lower bound of item probability $p_i$ of item $i \in \mathcal{I}_u$. We use $\overline{p}_l$ to denote the largest upper bound of item probabilities of items in $\mathcal{I} \setminus \mathcal{I}_u$, i.e., $\overline{p}_l = \max_{j \in \mathcal{I} \setminus \mathcal{I}_u } \overline{p}_j$. We can estimate these item-probability bounds using our method in Equation~\ref{cp_lower_bound} and~\ref{cp_upper_bound}.  Given  $\underline{p_i}$ and $\overline{p}_l$, we can compute an integer $Z_i$ based on Theorem 1 in bagging~\cite{jia2020intrinsic}. Roughly speaking, bagging can guarantee the poisoned item probability $p_i'$ is larger than $p_l'$ under any data poisoning attacks with at most $Z_i$ fake users. Therefore, given at most  $e$ fake users, the certified intersection size of bagging for a user $u$ can be computed as $\min\{\sum_{i \in \mathcal{I}_u }\mathbb{I}(Z_i \geq e), N\}$, where $\mathbb{I}$ is an indicator function. 

\neil{We note that when bagging is extended to recommender systems, both $N'$ and $N$ can only be 1 when using the techniques in~\cite{jia2020intrinsic} to derive its provable robustness guarantees.  PORE can be viewed as an extension of bagging to recommender systems, but $N'$ and $N$ can be arbitrary positive integers. Due to such differences, we propose new techniques to derive the robustness guarantee of PORE. Our major technical contribution is to derive a better guarantee for bagging applied to recommender systems.}





\begin{figure*}[!t]
	 \centering
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/N/100k/100k_change_N_precision.pdf}} 
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/N/100k/100k_change_N_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/N/100k/100k_change_N_f.pdf}} 
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/N/1m/1m_change_N_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/N/1m/1m_change_N_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/N/1m/1m_change_N_f.pdf}}
\vspace{-3mm}
\caption{Impact of $N$ on the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR for MovieLens-100k (left three) and MovieLens-1M (right three). }
\label{impact_of_N}
\end{figure*}





\begin{figure*}[!t]
	 \centering
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/s/100k/100k_change_s_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/s/100k/100k_change_s_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/s/100k/100k_change_s_f.pdf} } 
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/s/1m/1m_change_s_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/s/1m/1m_change_s_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/s/1m/1m_change_s_f.pdf}}
\vspace{-3mm}
\caption{Impact of $s$ on the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR for MovieLens-100k (left three) and MovieLens-1M (right three), where $N=10$.}
\label{impact_of_s}
\end{figure*}





\myparatight{Parameter setting} {PORE has the following parameters: $N'$ is the number of items recommended by a base recommender system for a user, $N$ is the number of items recommended by our ensemble recommender system for a user, $T$ is the number of base recommender systems, $1-\alpha$ is the confidence score, and $s$ is the number of rows sampled from the  rating-score matrix in each submatrix}. Unless otherwise mentioned, we adopt the following default parameter settings: $N'=1$, $N=10$, $T=100,000$, $\alpha =0.001$, $s=200$ for MovieLens-100k, and  $s=500$ for MovieLens-1M.  We will study the impact of each parameter on the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of PORE while fixing the remaining parameters to their default settings.  
We call our ensemble recommender system \emph{ensemble IR} (or \emph{ensemble BPR}) when the base  algorithm is IR (or BPR). By default, we use IR as the base  algorithm because of its scalability.  





\subsection{Experimental Results} 

 We report Precision@$N$/Recall@$N$/F1-Score@$N$ under no attacks (i.e., $e=0$), while we report certified Precision@$N$ /Recall@$N$/F1-Score@$N$ under attacks (i.e., $e\geq 1$).


\myparatight{Our PORE outperforms bagging~\cite{jia2020intrinsic}} \CR{Figure~\ref{compare_with_existing_defense} compares our PORE with bagging on the two datasets. We find that our PORE substantially outperforms bagging when extended from classifiers to recommender systems. The reason is that PORE  jointly considers multiple items when deriving the certified intersection size. In contrast, bagging can only consider each item independently when extended to recommender systems, and thus achieve a suboptimal certified intersection size.} 

\myparatight{Impact of $N'$} Figure~\ref{impact_of_N_prime} shows the impact of $N'$. We have two observations. First, our method has similar Precision@$N$/Recall@$N$/F1-Score@$N$ for different $N'$ when there are no attacks (i.e., $e=0$).  Second, a smaller $N'$ achieves a lower certified Precision@$N$, certified Recall@$N$, or certified F1-Score@$N$ when $e$ is small (e.g., $e=1$), but the curve has a longer tail. In other words, a smaller $N'$ is more robust against data poisoning attacks as the number of fake users $e$ increases. The reason is that an attack has a smaller manipulation space when $N'$ is smaller. This observation is also consistent with our theoretical result in Equation~(\ref{optimization_problem_theorem_1}). Specifically, given the same item-probability lower/upper bounds, a smaller $N'$ may lead to a larger certified intersection size. 
Therefore, we set $N'$ to be $1$ by default in our experiments. 

\begin{table}[tp]\renewcommand{\arraystretch}{1.3}

  \centering
  \fontsize{7.5}{8}\selectfont
  \caption{Precision@$10$, Recall@$10$, and F1-Score@$10$ of IR, Ensemble IR, BPR, and Ensemble BPR under no attacks. } 
  \vspace{-3mm}
 \subfloat[MovieLens-100k]{ \begin{tabular}{|c|c|c|c|}
    \hline
    {Algorithm}   & Precision@10 & Recall@10 & F1-Score@10 \\
    \hline
    IR & 0.330753 & 0.176385 &0.193783  \\
    \hline
    Ensemble IR & 0.332556 & 0.178293 & 0.195624  \\
    \hline
    BPR & 0.349841 & 0.181807 & 0.199426  \\
    \hline
    Ensemble BPR & 0.352280 & 0.173296 & 0.193362 \\
    \hline
  \end{tabular}}
  
   \subfloat[MovieLens-1M]{ \begin{tabular}{|c|c|c|c|}
    \hline
    {Algorithm}    & Precision@10 & Recall@10 & F1-Score@10 \\
    \hline
    IR & 0.270116 & 0.104350 & 0.127704 \\
    \hline
    Ensemble IR & 0.262616 & 0.103638 & 0.126191 \\
    \hline
    BPR & 0.324449 & 0.118385 & 0.144765 \\
    \hline
    Ensemble BPR & 0.362945 & 0.119441 & 0.151509 \\
    \hline
  \end{tabular}}

  \label{asr-train-test}
  \end{table}





\begin{figure*}[!t]
	 \centering
	 \vspace{-2mm}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/T/100k/100k_change_T_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/T/100k/100k_change_T_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/T/100k/100k_change_T_f.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/T/1m/1m_change_T_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/T/1m/1m_change_T_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/T/1m/1m_change_T_f.pdf}}
\vspace{-3mm}
\caption{Impact of $T$ on the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR for MovieLens-100k (left three) and MovieLens-1M (right three), where $N=10$.}
\label{impact_of_T}
\end{figure*}









\begin{figure*}[!t]
	 \centering
   \vspace{-3mm}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/alpha/100k/100k_change_alpha_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/alpha/100k/100k_change_alpha_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/alpha/100k/100k_change_alpha_f.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/alpha/1m/1m_change_alpha_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/alpha/1m/1m_change_alpha_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/sar/alpha/1m/1m_change_alpha_f.pdf}}
\vspace{-3mm}
\caption{Impact of $\alpha$ on the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR for MovieLens-100k (left three) and MovieLens-1M (right three), where $N=10$.}
\label{impact_of_alpha}
\end{figure*}

\begin{figure*}[!t]
	 \centering
  \vspace{-3mm}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/bpr/100k/100k_bpr_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/bpr/100k/100k_bpr_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/bpr/100k/100k_bpr_f.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/bpr/1m/1m_bpr_precision.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/bpr/1m/1m_bpr_recall.pdf}}
{\includegraphics[width=0.16\textwidth]{figs/replace_e=0_square/bpr/1m/1m_bpr_f.pdf}}
\vspace{-3mm}
\caption{Comparing the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR and ensemble BPR for MovieLens-100k (left three) and MovieLens-1M (right three), where $N=10$.}
\label{compare_sar_bpr}
\end{figure*}


\vspace{-1mm}
\myparatight{Impact of $N$} Figure~\ref{impact_of_N} shows the impact of $N$. The results show that  $N$ achieves a tradeoff between Precision@$N$ under no attacks (i.e., $e=0$) and robustness under attacks. Specifically, a smaller $N$ achieves a higher Precision@$N$ under no attacks but the certified Precision@$N$ decreases more quickly as $e$ increases. The certified Recall@$N$ increases as $N$ increases. The reason is that more items are recommended to each user as $N$ increases. 
The certified F1-Score@$N$ drops more quickly as $e$ increases when $N$ is smaller, because the certified Recall@$N$ drops more quickly. 






\myparatight{Impact of $s$} Figure~\ref{impact_of_s} shows the impact of $s$. We have two observations. First, our method achieves similar Precision@$N$, Recall@$N$, and F1-Score@$N$ for different $s$ when there are no attacks (i.e., $e=0$). Second, a larger $s$ achieves a larger certified Precision@$N$, certified Recall@$N$, or certified F1-Score@$N$ when $e$ is small, but they decrease more quickly as $e$ increases. This is because it's more likely to sample fake users in a submatrix when $s$ is larger.   



\myparatight{Impact of $T$ and $\alpha$}  Figure~\ref{impact_of_T} and~\ref{impact_of_alpha} show the impact of $T$ and $\alpha$, respectively. We have the following observations. First, Precision@$N$, Recall@$N$, or F1-Score@$N$ is similar for different $T$ when there are no attacks. In other words, a small $T$ is enough for our ensemble recommender system to achieve good recommendation performance when there are no attacks. Second,  certified Precision@$N$, certified Recall@$N$, or certified F1-Score@$N$ increases as $T$ or $\alpha$ increases. The reason is that a larger $T$ or $\alpha$ can produce tighter estimated item-probability lower/upper bounds, based on which we may compute larger certified intersection sizes $r$ in our Algorithm~\ref{alg:certify}. Therefore, we use a larger $T$ by default in our experiments to better show the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of PORE. We also observe that the certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ are insensitive to $\alpha$ once it is small enough. 

\myparatight{Ensemble IR vs. ensemble BPR}
Figure~\ref{compare_sar_bpr} compares ensemble IR and ensemble BPR. The results show that they achieve similar certified Precision@$N$/Recall@$N$/F1-Score@$N$. One exception is that ensemble BPR achieves higher certified Precision@$N$ on MovieLens-1M dataset. 

\myparatight{Standard recommender system vs. ensemble recommender system under no attacks} Table~\ref{asr-train-test} compares  standard recommender systems and our ensemble recommender systems with respect to the standard Precision@$N$, Recall@$N$, and F1-Score@$N$ when there are no attacks, where  $s=300$ for MovieLens-100k and $s=1,000$ for MovieLens-1M. A standard recommender system leverages IR (or BPR) to train a single recommender system on the entire rating-score matrix.  The results show that our ensemble recommender system achieves comparable performance with a standard recommender system when there are no attacks. \neil{Ensemble BPR achieves higher Precision@N than ensemble IR on both datasets. The reason is that  BPR achieves higher precision than  IR at training the base recommender systems.}



\begin{figure}[!t]
	 \centering
{\includegraphics[width=0.23\textwidth]{figs/revision/overhead_100k_s.pdf}}
{\includegraphics[width=0.23\textwidth]{figs/revision/overhead_1m_s.pdf}}
\vspace{-1mm}
\caption{\neil{Training time of PORE as a function of $s$ on MovieLens-100k (left) and MovieLens-1M (right).}}
\label{computation_complexity_s}
\end{figure}

\begin{figure}[!t]
	 \centering
{\includegraphics[width=0.23\textwidth]{figs/revision/overhead_100k.pdf}}
{\includegraphics[width=0.23\textwidth]{figs/revision/overhead_1m.pdf}}
\vspace{-1mm}
\caption{\neil{Training time of PORE as a function of $T$ on MovieLens-100k (left) and MovieLens-1M (right).}}
\label{computation_complexity_T}
\end{figure}

\myparatight{\neil{Training time of ensemble IR and BPR}} \neil{PORE trains $T$ base recommender systems, each of which is trained using rating scores of $s$ users. 
Figure~\ref{computation_complexity_s} shows the training time of ensemble IR/BPR as a function of $s$, while Figure~\ref{computation_complexity_T} shows the impact of $T$ on the  training time of ensemble IR/BPR. As expected, the training time of ensemble IR/BPR increases linearly as $s$ or $T$ increases. This is because a larger $s$ means each base recommender system is trained using more data, while a larger $T$ means more base recommender systems are trained. Ensemble IR is more efficient than ensemble BPR because IR is more efficient than BPR. }

\begin{figure}[!t]
	 \centering
{\includegraphics[width=0.33\textwidth]{figs/revision/100k_with_f.pdf}}
\vspace{-1mm}
\caption{Comparing  certified F1-Score@$N$ of sampling with and without replacement for ensemble IR on MovieLens-100k, where $N=10$.}
\label{samplingcomparison}
\end{figure}

\begin{figure}[!t]
	 \centering
{\includegraphics[width=0.33\textwidth]{figs/revision/10m_orig_sar_precision.pdf}}
\vspace{-1mm}
\caption{\neil{Certified Precision@$N$, certified Recall@$N$, and certified F1-Score@$N$ of ensemble IR for MovieLens-10M, where $N=10$.}}
\label{10m}
\end{figure}


\myparatight{\neil{Sampling with vs. without replacement}}  \neil{ PORE randomly samples a submatrix without replacement, i.e., the sampled $s$ users are different in a submatrix. Sampling with replacement means that the $s$ users may have duplicates, i.e., a submatrix may include rating scores of less than $s$ unique users. We can extend our theoretical guarantees to sampling with replacement. The theoretical analysis is similar to  sampling without replacement, so we omit it for simplicity. Figure~\ref{samplingcomparison} compares the certified F1-Score@$N$ of sampling with and without replacement for ensemble IR on MovieLens-100k dataset. Our results show that sampling with and without replacement achieves comparable certified F1-Score@$N$. They also achieve comparable certified Precision@$N$ and certified Recall@$N$, which we omit for simplicity. }




\myparatight{\neil{Evaluation on a large dataset}} \neil{We also evaluate  PORE on MovieLens-10M~\cite{movielens}, which consists of around $10,000,000$ rating scores. We set $s=5,000$, $T=1,000$, and IR as the base recommender system algorithm, and we adopt the default settings for other parameters.  We set a smaller $T$ than our previous experiments because it is more expensive to train each base recommender system on MovieLens-10M.  Figure~\ref{10m} shows the results. Our results indicate that our method is applicable to a large dataset and can derive certified performance guarantees against data poisoning attacks. We note that the certified Precision@$N$/Recall@$N$/F1-Score@$N$ of bagging also reduces to 0 with just 1 fake user.}