\appendix

\section{Proof of Theorem 3.1}
\label{proof_of_certified_theorem}
Recall that, given a rating-score matrix $\mathbf{M}$ and its poisoned version $\mathbf{M}^{\prime}$, $\mathbf{X}$ and $\mathbf{Y}$ are respectively two submatrices with $s$ rows randomly sampled from $\mathbf{M}$ and $\mathbf{M}^{\prime}$ without replacement. \neil{We use $\Phi$ to denote the domain space of $\mathbf{Y}$, i.e., each element in $\Phi$ is a submatrix with $s$ rows sampled from $\mathbf{M}'$. Note that since $\mathbf{M}$ is a submatrix of $\mathbf{M}'$, the domain space of $\mathbf{X}$ is a subset of $\Phi$.}  For simplicity, we define the following notations. Suppose we have $\mathbf{Z} \in \Phi$ and another rating-score matrix $\mathbf{W}$, we say $\mathbf{Z}  \prec \mathbf{W}$ (or $\mathbf{Z}  \nprec \mathbf{W}$) if $\mathbf{Z}$ is (or is not) in the domain space created by sampling $s$ rows from $\mathbf{W}$.  
We have $\mathbf{X} \prec \mathbf{M}$ and $\mathbf{Y} \prec \mathbf{M}'$ based on our defined notations. Similarly, give a user $u$ and $\mathbf{Z}\in \Phi$, we say $u \vdash \mathbf{Z}$ if $\mathbf{Z}$ contains rating scores of user $u$. We say  $u \nvdash \mathbf{Z}$ if $\mathbf{Z}$ does not contain user $u$'s rating scores.

The following lemma generalizes the Neyman-Pearson Lemma~\cite{neyman1933ix} to multiple functions:
\begin{lemma}
\label{lemma_np_general}
Let $\mathbf{X}$, $\mathbf{Y}$ be two random variables with probability densities $\text{Pr}(\mathbf{X}=\mathbf{Z})$ and $\text{Pr}(\mathbf{Y}=\mathbf{Z})$, where $\mathbf{Z} \in \Phi$. Let $g_{1}, g_{2}, \cdots, g_{\gamma}:\Phi \xrightarrow{} \{0,1\}$ be $\gamma$ random or deterministic functions. Let $\eta$ be an integer such that: $\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})\leq \eta, \forall \mathbf{Z}\in \Phi,$
where $g_{l}(1|\mathbf{Z})$ denotes the probability that $g_{l}(\mathbf{Z})=1$. Similarly, we use $g_{l}(0|\mathbf{Z})$ to denote the probability that $g_{l}(\mathbf{Z})=0$. Then, we have the following:  

(1) If $\Phi^{\prime} \subseteq \{\mathbf{Z}\in \Phi:g_{1}(1|\mathbf{Z})=g_{2}(1|\mathbf{Z})=\cdots=g_{\gamma}(1|\mathbf{Z})=0 \}$, $O_1=\{\mathbf{Z}\in \Phi \setminus \Phi^{\prime}: \text{Pr}(\mathbf{Y}=\mathbf{Z})<\rho \cdot \text{Pr}(\mathbf{X}=\mathbf{Z})  \}$ and $O_2=\{\mathbf{Z}\in \Phi\setminus \Phi^{\prime}: \text{Pr}(\mathbf{Y}=\mathbf{Z})=\rho \cdot \text{Pr}(\mathbf{X}=\mathbf{Z})   \}$ for some $\rho > 0$. Assuming we have $O_3 \subseteq O_2$, and $O=O_1 \cup O_3$. Then, if we have $\frac{\sum_{l=1}^{\gamma}\text{Pr}(g_{l}(\mathbf{X})=1)}{\eta}\geq \text{Pr}(\mathbf{X}\in O)$, then $\frac{\sum_{l=1}^{\gamma}\text{Pr}(g_{l}(\mathbf{Y})=1)}{\eta}\geq \text{Pr}(\mathbf{Y}\in O)$. 

(2) If $\Phi^{\prime} \subseteq \{\mathbf{Z}\in \Phi:g_{1}(1|\mathbf{Z})=g_{2}(1|\mathbf{Z})=\cdots=g_{\gamma}(1|\mathbf{Z})=0 \}$,  $O_1=\{\mathbf{Z}\in \Phi: \text{Pr}(\mathbf{Y}=\mathbf{Z})> \rho \cdot \text{Pr}(\mathbf{X}=\mathbf{Z})  \}$ and $O_2=\{\mathbf{Z}\in \Phi: \text{Pr}(\mathbf{Y}=\mathbf{Z})=\rho \cdot \text{Pr}(\mathbf{X}=\mathbf{Z}) \}$ for some $\rho > 0$. Assuming we have $O_3 \subseteq O_2$, and $O=O_1 \cup O_3 $. Then, if we have $\frac{\sum_{l=1}^{\gamma}\text{Pr}(g_{l}(\mathbf{X})=1)}{\eta}\leq \text{Pr}(\mathbf{X}\in O)$, then $\frac{\sum_{l=1}^{\gamma}\text{Pr}(g_{l}(\mathbf{Y})=1)}{\eta}\leq \text{Pr}(\mathbf{Y}\in O)$. 
\end{lemma}
\begin{proof}
We first prove part (1). For convenience, we denote the complement of $O$ as $O^{c}$. Then, we have the following: 
\allowdisplaybreaks
{\small 
\begin{align}
  & \frac{\sum_{l=1}^{\gamma}\text{Pr}(g_{l}(\mathbf{Y})=1)}{\eta}- \text{Pr}(\mathbf{Y}\in O) \\
=&\int_{\Phi}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} - \int_{O}\text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} \\
=&\int_{O^{c}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} \nonumber \\
& + \int_{O}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} - \int_{O}\text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} \\
=&\int_{O^{c}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} \nonumber\\
&- \int_{O}(1-\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta})\cdot\text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} \\
=&\int_{O^{c}\setminus \Phi^{\prime}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} + \int_{ \Phi^{\prime}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta} \nonumber \\
\label{lemma_np_general_e1}
& \cdot \text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} 
- \int_{O}(1-\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta})\cdot\text{Pr}(\mathbf{Y}=\mathbf{Z})d\mathbf{Z} \\
\geq&\rho \cdot[\int_{O^{c}\setminus  \Phi^{\prime}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z} + \int_{ \Phi^{\prime}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\nonumber \\
\label{lemma_np_general_e2}
&\cdot \text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z}
- \int_{O}(1-\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta})\cdot\text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z}] \\
=&\rho \cdot[\int_{O^{c}}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z} \nonumber \\
& + \int_{O}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z} - \int_{O}\text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z}] \\
=&\rho \cdot[\int_{\Phi}\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta}\cdot \text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z} - \int_{O}\text{Pr}(\mathbf{X}=\mathbf{Z})d\mathbf{Z}] \\
=&\rho \cdot [\frac{\sum_{l=1}^{\gamma}\text{Pr}(g_{l}(\mathbf{X})=1)}{\eta}- \text{Pr}(\mathbf{X}\in O)] \\
\geq & 0.
\end{align}
}
\begin{figure}[!t]
\centering
{\includegraphics[width=0.28\textwidth]{figs/illustration/ppt.pdf}}
\caption{Illustration of subset $P$, $Q$, $A$, and $B$.}
\vspace{-4mm}
\label{illustration_figure}
\end{figure}
We have Equation~(\ref{lemma_np_general_e2}) from~(\ref{lemma_np_general_e1}) due to the fact that $\forall \mathbf{Z}\in O^{c}\setminus \Phi^{\prime}, \text{Pr}(\mathbf{Y}=\mathbf{Z} \geq \rho \cdot \text{Pr}(\mathbf{X}=\mathbf{Z})$, $\forall \mathbf{Z} \in \Phi^{\prime}, g_{1}(1|\mathbf{Z})=g_{2}(1|\mathbf{Z})=\cdots=g_{\gamma}(1|\mathbf{Z})=0$,  $1-\frac{\sum_{l=1}^{\gamma}g_{l}(1|\mathbf{Z})}{\eta} \geq 0$, and $\text{Pr}(\mathbf{Y}=\mathbf{Z}) \leq \rho \cdot \text{Pr}(\mathbf{X}=\mathbf{Z}), \forall \mathbf{Z}\in O$.
Similarly, we can prove the part (2). We omit the details for simplicity. 
\end{proof}
Given a user $u$ and $\Phi$, we define the following subsets: 
{\small 
\begin{align}
\label{definition_of_q_app}
&  P = \{\mathbf{Z}\in \Phi|\mathbf{Z}\prec \mathbf{M},u \vdash \mathbf{Z}\} ,   Q = \{\mathbf{Z}\in \Phi|\mathbf{Z}\prec \mathbf{M},u \nvdash \mathbf{Z}\} \\
\label{definition_of_b_app}
 &     A = \{\mathbf{Z}\in \Phi|\mathbf{Z}\prec \mathbf{M}',u \vdash \mathbf{Z}\},    B = \{\mathbf{Z}\in \Phi|\mathbf{Z}\prec \mathbf{M}',u \nvdash \mathbf{Z}\}
 \end{align}
 }
It is easy to verify that $P \cap Q = \emptyset$ and $A \cap B = \emptyset$.  Moreover, since $M'$ contains rating scores for all the user in $\mathbf{M}$, we have $P \subseteq A$ and $Q \subseteq B$. Figure~\ref{illustration_figure} shows an illustration of these four subsets.
Based on our sampling without replacement, we have the following probability mass functions for $\mathbf{X}$ and $\mathbf{Y}$ in $\Phi$:
{\small 
\begin{align}
\label{probability_density_start_app}
&\text{Pr}(\mathbf{X} = \mathbf{Z}) = 
\begin{cases}
 \frac{1}{{n \choose s}}, &\text{ if } \mathbf{Z} \in P  \cup Q, \\
 0, &\text{ otherwise}.
\end{cases} \\
\label{probability_density_end_app}
&\text{Pr}(\mathbf{Y} = \mathbf{Z} ) = 
\begin{cases}
 \frac{1}{{n' \choose s}}, &\text{ if } \mathbf{Z} \in A  \cup B, \\
 0, &\text{ otherwise}.
\end{cases} 
\end{align}
}
Given the probability mass functions, we have the following probabilities: 
\allowdisplaybreaks
{\small 
\begin{align}
\label{probability_of_x_in_region_start_app}
 &   \text{Pr}(\mathbf{X}\in P)= 1 - {n-1 \choose s}\cdot  \frac{1}{{n \choose s}} =  \frac{s}{n}, 
 & \text{Pr}(\mathbf{X}\in A) =  \frac{s}{n}, \\
  &   \text{Pr}(\mathbf{Y}\in P)  =  \frac{s}{n} \cdot \frac{{n \choose s}}{{n' \choose s}}, 
   \label{probability_of_x_in_region_end_app}
 & \text{Pr}(\mathbf{Y}\in A) = \frac{s}{n'}.
\end{align}
}
We have $\text{Pr}(\mathbf{X}\notin P)={n-1 \choose s}\cdot  \frac{1}{{n \choose s}}$ because there are overall ${n-1 \choose s}$ possible submatrices if the user $u$ is not sampled from $\mathbf{M}$. Then, we can compute $ \text{Pr}(\mathbf{X}\in P)$ based on the fact that $ \text{Pr}(\mathbf{X}\in P)+\text{Pr}(\mathbf{X}\notin P)=1$. We obtain $\text{Pr}(\mathbf{X}\in A)$ based on the fact that $P \subseteq A$ and $A \cap B = \emptyset$. Similarly, we can compute the probability of $\mathbf{Y}$ in these subsets. 

We will leverage the law of contraposition to prove our theorem. Suppose we have a statement: $U \longrightarrow V$, then, its contraposition is $\neg V \longrightarrow \neg U$. The law of contraposition tells us that a statement is true if and only if its contraposition is true. We define the following two predicates: 
{\small 
\begin{align}
 U: &  ~ \underline{p^{*}_{\mu_{r}}} >   \min ( \min_{c=1}^{N-r +1}\frac{ N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} + \sigma)}{c},  \overline{p}^{*}_{v_1}+ \sigma)\\
&\text{ and } 1 \leq r \leq \min(k, N), \nonumber \\
V:&~  |\mathcal{I}_{u} \cap \mathcal{T}(\mathbf{M}',u)| \geq r ,
\end{align}
}
where $\sigma = \frac{s}{n'} \cdot \frac{{n' \choose s}}{{n \choose s}} -\frac{s}{n}$ . 

\myparatight{Deriving the necessary condition} We assume $\neg V$ is true, i.e.,  $|\mathcal{I}_{u} \cap \mathcal{T}(\mathbf{M}',u)| < r$. If $r=0 $ or $ r > \min(k, N)$, then, $\neg U$ is true. Next, we consider $1 \leq r \leq \min(k, N)$ and we will show $\underline{p^{*}_{\mu_{r}}} \leq   \min ( \min_{c=1}^{N-r +1}\frac{ N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} + \sigma)}{c},  \overline{p}^{*}_{v_1}+ \sigma)$ is true. If $|\mathcal{I}_{u} \cap \mathcal{T}(\mathbf{M}',u)| < r$, then, there exist at least $k-r +1$ items in $\mathcal{I}_{u}$ that are not recommended to the user $u$ by our ensemble recommender system when taking $\mathbf{M}'$ as input. In other words, there exist at least $N - r +1$ items in $\mathcal{I} \setminus \mathcal{I}_u$ appears in the recommended item set $\mathcal{T}(\mathbf{M}',u)$. For simplicity, we use $\mathcal{D}_{r}$ and $\mathcal{V}_{r}$ to denote subsets of $k - r +1$ items in $\mathcal{I}_{u}$ and $N - r + 1$ items in $\mathcal{I}\setminus\mathcal{I}_{u}$, respectively. Formally, we have the following: 
\begin{align}
    \exists \mathcal{D}_{r}, \mathcal{V}_{r}, \text{ s.t. } \mathcal{D}_{r}\cap \mathcal{T}(\mathbf{M}',u) =\emptyset, \mathcal{V}_{r} \subseteq \mathcal{T}(\mathbf{M}',u), 
\end{align}
The above equation means that the poisoned item probabilities of the items in $\mathcal{V}_{r}$ are no smaller than the poisoned item probabilities of the items in $\mathcal{D}_{r}$. In other words, we have the following for $\mathcal{D}_{r}$ and $\mathcal{V}_{r}$:
{\small 
\begin{align}
    \max_{i \in \mathcal{D}_{r}}\text{Pr}(i \in \mathcal{A}(\mathbf{Y},u)) \leq \min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)).
\end{align}
}
Moreover, since there exist $\mathcal{D}_{r}$ and $\mathcal{V}_{r}$, we have the following necessary condition if $|\mathcal{I}_{u} \cap \mathcal{T}(\mathbf{M}',u)| < r$ and $1 \leq r \leq \min(k, N)$: 
{\small 
\begin{align}
\label{necessary_condition_of_nega_u_is_true_app}
    \min_{\mathcal{D}_{r}}\max_{i \in \mathcal{D}_{r}}\text{Pr}(i \in \mathcal{A}(\mathbf{Y},u)) \leq \max_{\mathcal{V}_{r}}\min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)).
\end{align}
}
Next, we will derive the lower bound of left-hand side and the upper bound of the right-hand side for the above equation. 

\myparatight{Deriving a lower bound of $ \min_{\mathcal{D}_{r}}\max_{i \in \mathcal{D}_{r}}\text{Pr}(i \in \mathcal{A}(\mathbf{Y},u))$} For $\forall i \in \mathcal{D}_{r}$, based on Equation~(\ref{probability_upper_low_bound_theorem_1}), we have the following: 
{\small 
\begin{align}
\label{probability_upper_low_bound_theorem_1_app}
&\underline{p^{*}_i} \triangleq \frac{\lfloor \underline{p_i} \cdot {n \choose s}\rfloor}{{n \choose s}} \leq \underline{p_i} \leq \text{Pr}(i \in \mathcal{A}(\mathbf{X},u))
\end{align}
}
Based on the definition of $\mathbf{X}$, we have: 
{\small 
\begin{align}
\label{probability_upper_bound_proof}
    \text{Pr}(i \in \mathcal{A}(\mathbf{X},u)) \geq \underline{p^{*}_i}.
\end{align}
}
For $\forall i \in \mathcal{D}_{r}$, we define the function $g_{i}(\mathbf{Z})= \mathbb{I}(i \in \mathcal{A}(\mathbf{Z},u))$, where $\mathbb{I}$ is an indicator function. Specifically, we have $g_{i}(\mathbf{Z})=0$ for $\forall \mathbf{Z}\in B$ based on the definition of $B$ in Equation~(\ref{definition_of_b_app}), which will be used when we leverage Lemma~\ref{lemma_np_general} to derive the lower bound of $\text{Pr}(g_{i}(\mathbf{Y})=1)$. We have $\text{Pr}(g_{i}(\mathbf{X})=1) \geq \underline{p^{*}_i}$ based on Equation~(\ref{probability_upper_bound_proof}) and the definition of $g_i$.  For $\forall i \in \mathcal{D}_{r}$, we can find $C_{i} \subseteq P$ such that we have the following:
{\small 
\begin{align}
\label{construct_of_region_A_i}
    \text{Pr}(\mathbf{X} \in C_{i}) = \underline{p^{*}_i}. 
\end{align}
}
Note that we can find such a subset because $\underline{p^{*}_i}$ is an integer multiple of $\frac{1}{{n \choose s}}$. Then, we have the following:
{\small 
\begin{align}
\label{condition_of_np_lemma_i_u_of_x_app}
 \text{Pr}(g_{i}(\mathbf{X})=1) \geq   \text{Pr}(\mathbf{X} \in C_{i}).
\end{align}
}
For simplicity, we define the following quantity: 
{\small 
\begin{align}
    \tau = {n \choose s}/{n' \choose s}. 
\end{align}
}
Next, we will apply Lemma~\ref{lemma_np_general} to obtain the lower bound of $\text{Pr}(g_{i}(\mathbf{Y})=1)$. In particular, we let $\Phi^{\prime} =  B$ since we have $g_{i}(\mathbf{Z})=0$ for $\forall \mathbf{Z} \in B$. Note that we omit $Q$ since we have $Q \subseteq B$. Then, we have $A = \Phi \setminus \Phi^{\prime}$.  Based on Equation~(\ref{probability_density_start_app}) -~(\ref{probability_density_end_app}), we have $\text{Pr}(\mathbf{Y}=\mathbf{Z})= \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$ if $\mathbf{Z} \in P$ and $\text{Pr}(\mathbf{Y}=\mathbf{Z}) > \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$ if $\mathbf{Z} \in A \setminus P$.  We let $O_1 = \emptyset$ since there is no subset that satisfies $\text{Pr}(\mathbf{Y}=\mathbf{Z}) < \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$. Furthermore, we let $O_2 = P $, $O_3 = C_{i} \subseteq O_2$, $\eta = 1$, and $\gamma=1$. Finally, we let $O = O_1 \cup O_3 = C_i$. We can apply Lemma~\ref{lemma_np_general} based on the condition in  Equation~(\ref{condition_of_np_lemma_i_u_of_x_app}) and we have the following: 
\begin{align}
    \text{Pr}(g_{i}(\mathbf{Y})=1) \geq \text{Pr}(\mathbf{Y} \in C_i).
\end{align}
Based on the definition of $g_{i}$, we have the following: 
{\small 
\begin{align}
& \text{Pr}(i \in \mathcal{A}(\mathbf{Y},u)) \\
=& \text{Pr}(g_{i}(\mathbf{Y})=1) \\
\geq & \text{Pr}(\mathbf{Y} \in C_i) \\
= &   \text{Pr}(\mathbf{X} \in C_i)\cdot \tau \\
=& \underline{p^{*}_i} \cdot \tau.
\end{align}
}
For simplicity, we denote $\mathcal{D}_{r}=\{d^{\prime}_{1},d^{\prime}_{2}, \cdots, d^{\prime}_{z}\}$, where $z = k-r +1$. Without loss of generality, we assume $\underline{p_{d^{\prime}_{1}}} \geq \cdots \geq \underline{p_{d^{\prime}_{z}}}$. We have the following: 
\begin{align}
 \max_{i \in \mathcal{D}_{r}}\text{Pr}(i \in \mathcal{A}(\mathbf{Y},u)) 
\geq \max_{i \in \mathcal{D}_{r}} \underline{p^{*}_i} \cdot \tau
= \underline{p^{*}_{d^{\prime}_{1}}} \cdot \tau. 
\end{align}
Then, we have the following: 
\begin{align}
\min_{\mathcal{D}_{r}}\max_{i \in \mathcal{D}_{r}}\text{Pr}(i \in \mathcal{A}(\mathbf{Y},u)) 
=  \min_{\mathcal{D}_{r}} \underline{p^{*}_{d^{\prime}_{1}}} \cdot \tau
\end{align}
Therefore, when $\mathcal{D}_{r}=\{\mu_{r},\mu_{r+1},\cdots, \mu_{k}\}$, $\underline{p^{*}_{d^{\prime}_{1}}} \cdot \tau$ reaches the minimal value which is $\underline{p^{*}_{\mu_{r}}}\cdot \tau$. In other words, we have the following: 
\begin{align}
    \min_{\mathcal{D}_{r}}\max_{i \in \mathcal{D}_{r}}\text{Pr}(i \in \mathcal{A}(\mathbf{Y},u))  \geq \underline{p^{*}_{\mu_{r}}}\cdot \tau. 
\end{align}

\myparatight{Deriving an upper bound of $\max_{\mathcal{V}_{r}}\min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u))$} For $\forall j \in \mathcal{V}_r$, given Equation~(\ref{probability_upper_low_bound_theorem_1}), we have the following: 
{\small 
\begin{align}
\label{probability_upper_low_bound_theorem_1_app_low}
&\overline{p}^{*}_j = \frac{\lceil \overline{p}_j \cdot {n \choose s}\rceil}{{n \choose s}}  \geq \overline{p}_j \geq \text{Pr}(j \in \mathcal{A}(\mathbf{X},u)).
\end{align}
}
Suppose we have  $\mathcal{V}_{r}=\{v^{\prime}_1, v^{\prime}_2, \cdots, v^{\prime}_{w}\}$, where $w= N - r +1$. Without loss of generality, we assume the following:
\begin{align}
\label{without_loss_condition_2_app}
\overline{p}_{v^{\prime}_1} \leq \overline{p}_{v^{\prime}_2}\leq \cdots \leq \overline{p}_{v^{\prime}_w}.  
\end{align}
We first derive an upper bound of $\min_{j \in \mathcal{V}_{r}}$ $\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u))$. Given an arbitrary item $j \in \mathcal{V}_{r}$, we have the following inequality based on Equation~(\ref{probability_upper_low_bound_theorem_1_app_low}) and our definition of $\mathbf{X}$:
\begin{align}
\label{probability_lower_bound_proof}
    \text{Pr}(j \in \mathcal{A}(\mathbf{X},u)) \leq \overline{p}^{*}_j.
\end{align}
Given an item $j \in \mathcal{V}_r$, we define the function $g_{j}(\mathbf{Z}) = \mathbb{I}(j \in \mathcal{A}(\mathbf{Z},u))$. We have $\text{Pr}(g_{j}(\mathbf{X}) = 1) \leq \overline{p}^{*}_j$ based on Equation~(\ref{probability_lower_bound_proof}) and the definition of $g_j$. Then, we can leverage Lemma~\ref{lemma_np_general} to derive an upper bound for $\text{Pr}(g_{j}(\mathbf{X}) = 1)$. In particular, we can find $C^{\prime}_j \subseteq P$ such that we have the following:
\begin{align}
    \text{Pr}(\mathbf{X} \in C^{\prime}_j) = \overline{p}^{*}_j.
\end{align}
We let $C_{j} = C^{\prime}_j \cup (A \setminus P)$. Since we have $\text{Pr}(\mathbf{X} \in A\setminus P)= \text{Pr}(\mathbf{X} \in A)-\text{Pr}(\mathbf{X} \in P)=0$, we have the following:
\begin{align}
    \text{Pr}(\mathbf{X} \in C_j) = \text{Pr}(\mathbf{X} \in C^{\prime}_j) + \text{Pr}(\mathbf{X} \in A\setminus P) = \overline{p}^{*}_j.
\end{align}
Based on $\text{Pr}(g_{j}(\mathbf{X}) = 1) \leq \overline{p}^{*}_j$, we have the following:
\begin{align}
\label{condition_of_np_lemma_j_u_of_x_app}
    \text{Pr}(g_{j}(\mathbf{X})=1) \leq \text{Pr}(\mathbf{X}\in C_j).
\end{align}
Next, we will apply Lemma~\ref{lemma_np_general} to obtain the upper bound of $\text{Pr}(g_{j}(Y)=1)$. In particular, we let $\Phi^{\prime} =  B$ since we have $g_{j}(\mathbf{Z})=0$ for $\forall \mathbf{Z} \in B$. Then, we have $A = \Phi \setminus \Phi^{\prime}$.  Based on Equation~(\ref{probability_density_start_app}) -~(\ref{probability_density_end_app}), we have $\text{Pr}(\mathbf{Y}=\mathbf{Z})= \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$ if $\mathbf{Z} \in P$ and $\text{Pr}(\mathbf{Y}=\mathbf{Z}) > \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$ if $\mathbf{Z} \in A \setminus P$.  We let $O_1 = A \setminus P$, $O_2 = P $, $O_3 = C^{\prime}_{j} \subseteq O_2$, $\eta=1$, and $\gamma=1$. Finally, we let $O = O_1 \cup O_3 = C_j$. We can apply Lemma~\ref{lemma_np_general} based on the condition in  Equation~(\ref{condition_of_np_lemma_j_u_of_x_app}) and we have the following: 
{\small 
\begin{align}
&\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \\
    =&\text{Pr}(g_{j}(\mathbf{Y})=1) \\
    \leq & \text{Pr}(\mathbf{Y} \in C_j) \\
    =& \text{Pr}(\mathbf{Y} \in C^{\prime}_j) + \text{Pr}(\mathbf{Y} \in A\setminus P) \\
    =& \text{Pr}(\mathbf{X} \in C^{\prime}_j)\cdot \tau + \text{Pr}(\mathbf{Y} \in A) - \text{Pr}(\mathbf{Y} \in P) \\
    =& \overline{p}^{*}_j \cdot \frac{{n \choose s}}{{n' \choose s}}+ \frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}.
\end{align}
}
Given Equation~(\ref{without_loss_condition_2_app}), we have the following:
{\small 
\begin{align}
\min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \leq \overline{p}^{*}_{v^{\prime}_{1}} \cdot \frac{{n \choose s}}{{n' \choose s}}+ \frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}.
\end{align}
}
Therefore, when $\mathcal{V}_{r}$ contains the set of items in $\mathcal{I} \setminus \mathcal{I}_{u}$ that have the largest probability bounds, which we denote as $\mathcal{V}_{r}=\{v_1, v_2, \cdots, v_w\}$ where $\overline{p}_{v_1} \leq \overline{p}_{v_2}\leq \cdots \leq \overline{p}_{v}$, the upper bound of $\max_{\mathcal{V}_{r}}\min_{j \in \mathcal{V}_{r}}$ $ \text{Pr}(j \in \mathcal{A}(\mathbf{Y},u))$ reaches the maximum value. Formally, we have the following: 
{\small 
\begin{align}
\max_{\mathcal{V}_{r}}\min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u))  \leq \overline{p}^{*}_{v_{1}} \cdot \frac{{n \choose s}}{{n' \choose s}}+ \frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}. 
\end{align}
}

Next, we will derive another upper bound for $\text{Pr}(g_{j}(Y)=1)$ via jointly considering multiple items. In particular, we use $\mathcal{H}_{c}$ to denote an arbitrary subset of $\mathcal{V}_{r}$ that contains $c$ items, i.e., $\mathcal{H}_{c} \subseteq \mathcal{V}_{r}$. We denote $\overline{p}_{\mathcal{H}_{c}} = \sum_{j \in \mathcal{H}_{c}} \overline{p}_j$, i.e., the summation of probability upper bounds for the items in $\mathcal{H}_{c}$. Then, for each item $j \in \mathcal{H}_{c}$, we define $g_{j}(\mathbf{Z}) = \mathbb{I}(j \in \mathcal{A}(\mathbf{Z},u))$. Given a positive integer $\eta$, we define the following quantity: 
{\small 
\begin{align}
    \overline{p}^{*}_{\mathcal{H}_{c}} = \frac{\lceil (\overline{p}_{\mathcal{H}_{c}}/\eta)\cdot {n \choose s} \rceil}{ {n \choose s}  }.
\end{align}
}
We can find $C^{\prime}_{\mathcal{H}_{c}} \subseteq P$ such that we have the following: 
{\small 
\begin{align}
    \text{Pr}(\mathbf{X}\in C^{\prime}_{\mathcal{H}_{c}}) = \overline{p}^{*}_{\mathcal{H}_{c}}. 
\end{align}
}
Then, we define $C_{\mathcal{H}_{c}} =C^{\prime}_{\mathcal{H}_{c}} \cup (A \setminus P)$ and we have the following: 
{\small 
\begin{align}
    \text{Pr}(\mathbf{X} \in C_{\mathcal{H}_{c}}) = \text{Pr}(\mathbf{X} \in C^{\prime}_{\mathcal{H}_{c}}) + \text{Pr}(\mathbf{X} \in A \setminus P) = \overline{p}^{*}_{\mathcal{H}_{c}}. 
\end{align}
}
Based on the definition of $g_{j}(\mathbf{Z})$, we have the following: 
{\small 
\begin{align}
\label{condition_of_np_lemma_multiple_j_u_of_x_app}
    \frac{\sum_{j \in \mathcal{H}_{c}} \text{Pr}(g_{j}(X)=1)}{\eta}\leq \frac{\sum_{j \in \mathcal{H}_{c}} \overline{p}_j}{\eta} \leq  \overline{p}^{*}_{\mathcal{H}_{c}} =  \text{Pr}(\mathbf{X} \in C_{\mathcal{H}_{c}}). 
\end{align}
}
Next, we will leverage Lemma~\ref{lemma_np_general} to derive an upper bound for $\sum_{j \in \mathcal{H}_{c}} \text{Pr}(g_{j}(\mathbf{Y})=1)$. Given a rating-score matrix $\mathbf{Z}$ as input, the recommender system algorithm $\mathcal{A}$ recommends $N'$ items to a user. Therefore, we have  $\sum_{j \in \mathcal{H}_{c}} \mathbb{I}(j \in \mathcal{A}(\mathbf{Z},u)) \leq N'$, i.e., $\sum_{j \in \mathcal{H}_{c}} g_{j}(\mathbf{Z}) \leq N'$. Based on this, we let $\eta = N'$. Since there are $c$ items in $\mathcal{H}_{c}$, we let $\gamma =  c$. We let $\Phi^{\prime} =  B$ since we have $g_{j}(\mathbf{Z})=0$ for $\forall \mathbf{Z} \in B$. Then, we have $A = \Phi \setminus \Phi^{\prime}$.  Based on Equation~(\ref{probability_density_start_app}) -~(\ref{probability_density_end_app}), we have $\text{Pr}(\mathbf{Y}=\mathbf{Z})= \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$ if $\mathbf{Z} \in P$ and $\text{Pr}(\mathbf{Y}=\mathbf{Z}) > \tau \cdot \text{Pr}(\mathbf{X}=\mathbf{\mathbf{Z}})$ if $\mathbf{Z} \in A \setminus P$.  We let $O_1 = A \setminus P$, $O_2 = P $, and $O_3 = C^{\prime}_{\mathcal{H}_{c}} \subseteq O_2$. Finally, we let $O = O_1 \cup O_3 = C_{\mathcal{H}_{c}}$. We can apply Lemma~\ref{lemma_np_general} based on the condition in  Equation~(\ref{condition_of_np_lemma_multiple_j_u_of_x_app}) and we have the following: 
{\small 
\begin{align}
&\sum_{j \in \mathcal{H}_{c}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \\
    =&\sum_{j \in \mathcal{H}_{c}}\text{Pr}(g_{j}(\mathbf{Y})=1) \\
    \leq & N' \cdot \text{Pr}(\mathbf{Y} \in C_{\mathcal{H}_{c}}) \\
    =& N' \cdot( \text{Pr}(\mathbf{Y} \in C^{\prime}_{\mathcal{H}_{c}}) + \text{Pr}(\mathbf{Y} \in A\setminus P)) \\
    =& N' \cdot(\text{Pr}(\mathbf{X} \in C^{\prime}_{\mathcal{H}_{c}})\cdot \tau + \text{Pr}(\mathbf{Y} \in A) - \text{Pr}(\mathbf{Y} \in P)) \\
    =& N' \cdot (\overline{p}^{*}_{\mathcal{H}_{c}} \cdot \frac{{n \choose s}}{{n' \choose s}}+  (\frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}})).
\end{align}
}
Then, we have the following: 
{\small 
\begin{align}
\label{2_derive_min_app_0}
& \min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \\
\label{2_derive_min_app_1}
\leq & \min_{j \in \mathcal{H}_{c}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \\
\label{2_derive_min_app_2}
\leq & \frac{\sum_{j \in \mathcal{H}_{c}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u))}{c} \\
\label{2_derive_min_app_3}
\leq & N' \cdot (\overline{p}^{*}_{\mathcal{H}_{c}} \cdot \frac{{n \choose s}}{{n' \choose s}}+  (\frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}))/c.
\end{align}
}
We have Equation~(\ref{2_derive_min_app_1}) from~(\ref{2_derive_min_app_0}) because $\mathcal{H}_{c} \subseteq \mathcal{V}_{r}$ and Equation~(\ref{2_derive_min_app_2}) from~(\ref{2_derive_min_app_1}) because the smallest value is no larger than the average value in a set. We note that the upper bound of $ \min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) $ is non-decreasing as $\overline{p}_{\mathcal{H}_{c}}$ increases. Based on Equation~(\ref{without_loss_condition_2_app}), the upper bounds reaches the minimal value when $\mathcal{H}_{c} = \{v^{\prime}_1, v^{\prime}_2,$ $  \cdots, v^{\prime}_{c}\}$. Taking all possible $c$ into consideration, we have the following: 
{\small 
\begin{align}
   & \min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \\ 
    \leq & \min_{c=1}^{N -r +1} N' \cdot (\overline{p}^{*}_{\mathcal{H}_{c}} \cdot \frac{{n \choose s}}{{n' \choose s}}+  (\frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}))/c, \nonumber
\end{align}
}
where  $\mathcal{H}_{c}=\{v^{\prime}_1, v^{\prime}_2, \cdots, v^{\prime}_{c}\}$. 
Similarly, the upper bound of $\max_{\mathcal{V}_{r}}$ $\min_{j \in \mathcal{V}_{r}}$ $\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u))$ reaches the maximum value when $\mathcal{V}_{r}$ contains the $N-r + 1$ items among all items in $\mathcal{I}\setminus \mathcal{I}_u$ that have the largest probability upper bounds, which we denote as $\mathcal{V}_{r}=\{v_1, v_2, \cdots, v_w\}$, where $\overline{p}_{v_1} \leq \overline{p}_{v_2}\leq \cdots \leq \overline{p}_{v_w}$ and $w=N -r +1$. Formally, we have the following: 
{\small 
\begin{align}
& \max_{\mathcal{V}_{r}}\min_{j \in \mathcal{V}_{r}}\text{Pr}(j \in \mathcal{A}(\mathbf{Y},u)) \\
\leq & \min_{c =1}^{N-r +1} N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} \cdot \frac{{n \choose s}}{{n' \choose s}}+  (\frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}))/c, 
\end{align}
}
where $\mathcal{H}_{c}=\{v_1, v_2, \cdots, v_{c}\}$.
Since we have Equation~(\ref{necessary_condition_of_nega_u_is_true_app}) when $\neg U$ is true and $1 \leq r \leq \min(k, N)$, we have the following: 
{\small 
\begin{align}
\label{equation_of_the_middle}
    \underline{p^{*}_{\mu_{r}}} \cdot  \frac{{n \choose s}}{{n' \choose s}}  \leq  & \min ( \min_{c=1}^{N-r +1}\frac{ N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} \cdot \frac{{n \choose s}}{{n' \choose s}}+  (\frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}))}{c}, \nonumber\\
    & \overline{p}^{*}_{v_1}\cdot \frac{{n \choose s}}{{n' \choose s}}+ \frac{s}{n'}-\frac{s}{n}\cdot \frac{{n \choose s}}{{n' \choose s}}).
\end{align}
}
where $\mathcal{V}_{r}=\{v_1, v_2, \cdots, v_{N - r +1}\}$  and $\mathcal{H}_{c}=\{v_1, v_2, \cdots, v_{c}\}$. 
The Equation~(\ref{equation_of_the_middle}) is equivalent to the following: 
\begin{align}
   \underline{p^{*}_{\mu_{r}}}  \leq   \min ( \min_{c=1}^{N-r +1}\frac{ N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} + \sigma)}{c}, 
     \overline{p}^{*}_{v_1}+ \sigma),  
\end{align}
where $\sigma = \frac{s}{n'} \cdot \frac{{n' \choose s}}{{n \choose s}} -\frac{s}{n}$ . 

\myparatight{Applying the law of contraposition} We leverage the law of contraposition and we have the following: if we have $1 \leq r \leq \min(k, N)$ and the following: 
{\small 
\begin{align}
\label{optimization_prob_app}
   \underline{p^{*}_{\mu_{r}}} >   \min ( \min_{c=1}^{N-r +1}\frac{ N' \cdot(\overline{p}^{*}_{\mathcal{H}_{c}} + \sigma)}{c}, 
     \overline{p}^{*}_{v_1}+ \sigma),  
\end{align}
}
where $\sigma = \frac{s}{n'} \cdot \frac{{n' \choose s}}{{n \choose s}} -\frac{s}{n}$. Then, we have $|\mathcal{I}_{u} \cap \mathcal{T}(\mathbf{M}',u)| \geq r $.  The Equation~(\ref{optimization_prob_app}) is satisfied for $\forall \mathbf{M}' \in \mathcal{L}(\mathbf{M},e)$. We can find the maximum value of $r$, where $1 \leq r \leq \min(k, N)$, that satisfies the Equation~(\ref{optimization_prob_app}), which is essentially the optimization problem in the Equation~(\ref{optimization_problem_theorem_1}). We reach the conclusion. 




\section{Proof of Theorem 3.2}
\label{proof_of_proposition}
Based on Equation~(\ref{cp_lower_bound})~-~(\ref{cp_upper_bound}) and Boole's inequality in probability theory, we have the following probability: 
\begin{align}
    \text{Pr}((p_i \geq \underline{p_i},\forall i \in \mathcal{I}_u) \wedge (p_j \leq \overline{p}_j,\forall j \in \mathcal{I}\setminus\mathcal{I}_u)) \geq 1 - \alpha_u, 
\end{align}
where $R \wedge S$ is true if and only if $R$ is true and $S$ is true. Note that there is no randomness in our optimization problem in Equation~(\ref{optimization_problem_theorem_1}). Therefore, the probability that our Algorithm~\ref{alg:certify} computes an incorrect $r_u$ 
for the user $u$ is at most $\alpha_u$. 
Recall that we set $\alpha_u = \frac{\alpha}{n}$ in our Algorithm~\ref{alg:certify}. 
Based on the Boole's inequality, we know the probability that our Algorithm~\ref{alg:certify} computes an incorrect $r_u$
for at least one user among  all users in $\mathcal{U}$ is at most $\alpha$.








