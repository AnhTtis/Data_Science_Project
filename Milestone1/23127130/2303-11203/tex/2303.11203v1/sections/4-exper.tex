%%%%%%%%% Experiments
\vspace{-0.3cm}   
\section{Evaluation}
\label{sec:experiments}
\noindent
We evaluate our proposed \textit{Less is More 3D} ({\ourmodel}) approach against
state-of-the-art 3D point cloud semantic segmentation approaches using the SemanticKITTI~\cite{behley2019semantickittia} and ScribbleKITTI~\cite{Unal_2022_CVPR} benchmark datasets.

\input{tables/benchmark_2.tab}

\subsection{Experimental Setup}

\bdtitle{SemanticKITTI}~\cite{behley2019semantickittia} is a large-scale 3D point cloud dataset for semantic scene understanding with 20 semantic classes consisting of 22 sequences - [\texttt{00} to \texttt{10} as \textit{training}-split (of which \texttt{08} as \textit{validation}-split) + \texttt{11} to \texttt{21} as \textit{test}-split].

\bdtitle{ScribbleKITTI}~\cite{Unal_2022_CVPR} is the first scribble (\ie sparsely) annotated dataset for LiDAR semantic segmentation providing sparse annotations for the \textit{training} split of SemanticKITTI for 19 classes, with only 8.06\% of points from the full SemanticKITTI dataset annotated.

\bdtitle{Evaluation Protocol:} Following previous work~\cite{zhu2021cylindrical,jiang2021guided,Unal_2022_CVPR,kong2022lasermix}, we report performance on the SemanticKITTI and ScribbleKITTI {\trainset} for intermediate training steps, as this metric provides an indication of the pseudo-labeling quality, and on the {\validset} to assess the performance benefits of each individual component. Performance is reported using the mean Intersection over Union (mIoU, as \%) metric. For semi-supervised training, we report over both the benchmarks using the SemanticKITTI and ScribbleKITTI {\validset} under 5\%, 10\%, 20\%, and 40\% partitioning. We further report the relative performance of semi-supervised or scribble-supervised for ScribbleKITTI (SS) training to the fully supervised upper-bound (FS) in percentages (SS/FS) to further analyze semi-supervised performance and report the results for the fully-supervised training on both {\validset}s for reference. The trainable parameter count and number of multiply-adds (multi-adds) are additionally provided as a metric of computational cost.

\bdtitle{Implementation Details:} Training is performed using 4$\times$ NVIDIA A100 80GB GPU without pre-trained weights with a DDP shared training strategy~\cite{FairScale2021} to maintain GPU scaling efficiency, whilst reducing memory overhead significantly. Specific hyper-parameters are set as follows - Mean Teacher: $\kappa=0.99$; unreliable pseudo-labeling: $\lambda_C=0.3$, $\tau=0.5$; {\samplshort}: $\beta = \{7.45$, $5.72$, $4.00$, $2.28$, $0$\} for sampling \{5\%, 10\%, 20\%, 40\%, 100\%\} labeled training frames, assuming the remainder as unlabeled; Reflec-TTA: $N_b=10$, $s=3$ various Reflec-TTA bin sizes, following~\cite{Unal_2022_CVPR}, we set each bin $b_i = \left(\rho, \phi\right) \in \{ (20, 40), (40, 80), (80, 120)$\}.


\begin{figure}[thp]
    \centering
    \includegraphics[width=0.478\textwidth]{figures/visual_results_big.pdf}
    \caption{Comparing the 10\% sampling split of SemanticKITTI (SeK, first row) and ScribbleKITTI (ScK, second row) {\validset} with ground-truth (left), our approach (middle) and Unal \etal~\cite{Unal_2022_CVPR} (right) with areas of improvement highlighted.}
    \label{fig:visual_results}
    \vspace{-0.4cm}
\end{figure}
\input{tables/ablation.tab}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.2cm}
\subsection{Experimental Results}
\vspace{-0.2cm}
\noindent
In~\cref{tab:benchmark_2}, we present the performance of our \textit{Less is More} 3D ({\ourmodel}) point cloud semantic segmentation approach both with ({\ourmodelsdsc}) and without ({\ourmodel}) SDSC in a side-by-side comparison with leading contemporary state-of-the-art approaches on the SemanticKITTI and ScribbleKITTI benchmark {\validset}s to illustrate our approach offers superior or comparable (within 1\% mIoU) performance across all sampling ratios. Furthermore, we present supporting qualitative results in~\cref{fig:visual_results}.

On SemanticKITTI, with a lack of available supervision, {\ourmodel} shows a relative performance (SS/FS) from $85.6\%$ ($5\%$-fully-supervised) to $91.1\%$ ($40\%$-fully-supervised), and {\ourmodelsdsc} from $85.3\%$ to $92.0\%$, compared to their respective fully supervised upper-bound. {\ourmodel}/{\ourmodelsdsc} performance is also less sensitive to reduced labeled data sampling compared with other methods.

Our model significantly outperforms on small ratio sampling splits, \eg, $5\%$ and $10\%$. {\ourmodel} shows up to $19.8\%$ and $18.9\%$ mIoU improvements whilst, with a smaller model size {\ourmodelsdsc} again shows significant mIoU improvements by up to $16.4\%$ and $15.5\%$ when compared with other range and voxel-based methods respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.15cm}
\subsection{Ablation Studies}
\vspace{-0.15cm}
\bdtitle{Effectiveness of Components.} In~\cref{tab:ablation} we ablate each component of {\ourmodel} step by step and report the performance on the SemanticKITTI {\trainset} at the end of training as an overall indicator of pseudo-labeling quality in addition to the corresponding {\validset}.

As shown in~\cref{tab:ablation}, adding unreliable pseudo-labeling (UP) in the distillation stage, we can increase the $valid$ mIoU by $+0.7\%$ on average in {\validset}. Appending reflectivity features (RF) in the training stage, we further improve the mIoU on the {\trainset} by $+0.7\%$ on average. Due to the improvements in training, the model generates a higher quality of pseudo-labels, which results to a $+0.5\%$ increase in mIoU in the {\validset}. If we disable reflectivity features in the training stage, applying Reflec-TTA in the distillation stage alone, we then get an average improvement of $+1.3\%$ compared with pseudo-labeling only. On the whole, enabling all reflectivity-based components (RF+RT) shows great improvements of up to $+2.8\%$ in $validation$ mIoU.
\input{tables/computation_cost.tab}
\input{tables/sampl.tab} 
\input{tables/pseudo.tab}
\input{tables/ref_vs_inten.tab}

Substituting the uniform sampling with our {\samplshort} strategy, we observe further average improvements of $+1.0\%$ and $+0.8\%$ on $training$ and $validation$ respectively (\cref{tab:ablation}).

Our SDSC module reduces the trainable parameters of our model by $57\%$, with a performance cost of $-0.7\%$ and $-1.4\%$ mIoU on $training$ and $validation$ respectively (\cref{tab:ablation}). Finally, we provide two models, one without SDSC ({\ourmodel}) and one with ({\ourmodelsdsc}), corresponding to the bottom two rows of~\cref{tab:ablation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bdtitle{Effectiveness of SDSC module.} In ~\cref{tab:computation_cost}, we compare our {\ourmodel} and {\ourmodelsdsc} with recent state-of-the-art methods under 5\%-labeled semi-supervised training on the SemanticKITTI and ScribbleKITTI {\validset}s. {\ourmodelsdsc} outperforms the voxel-based methods~\cite{zhu2021cylindrical,Unal_2022_CVPR} with at least a \textbf{2.3}$\times$ reduction in model size. Similarly, with comparable model size~\cite{choy20194d,tang2020searching,yan20222dpass}, {\ourmodelsdsc} has higher mIoU in both datasets and up to \textbf{641}$\times$ fewer multiply-add operations.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bdtitle{Effectiveness of {\samplshort} strategy.} In~\cref{tab:sampl}, we illustrate the effectiveness of our {\samplshort} strategy by comparing  {\ourmodel} with two widely-used strategies in semi-supervised training, \ie, random sampling and uniform sampling on SemanticKITTI~\cite{behley2019semantickittia} and ScribbleKITTI~\cite{Unal_2022_CVPR} {\validset}. Whilst uniform and random sampling have comparable results on both {\validset}s, simply applying our {\samplshort} strategy improves the baseline by $+0.90\%$, $+0.75\%$, $+0.60\%$ and $+0.55\%$ on SemanticKITTI under $5\%$, $10\%$, $20\%$ and $40\%$ sampling protocol respectively.
Furthermore, using corresponding range images of point cloud, rather than RGB images to compute the spatio-temporal redundancy within {\samplshort} (see {\samplshort}-R in~\cref{tab:sampl}), has no significant difference on the performance.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-5pt}
\bdtitle{Effectiveness of Unreliable Pseudo-Labeling.} In~\cref{tab:pseudo}, we evaluate selecting negative candidates with different reliability to illustrate the improvements of using unreliable pseudo-labels in semi-supervised semantic segmentation. The \textit{“Unreliable”} selecting of negative candidates outperforms other alternative methodologies, showing the positive performance impact of unreliable pseudo-labels.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bdtitle{Effectiveness of Reflec-TTA.} In~\cref{tab:ablation}, we compare {\ourmodel} performance with and without Reflec-TTA and further experiment on the SemanticKITTI and ScribbleKITTI {\validset} in~\cref{tab:ref_vs_inten}. This demonstrates that the LiDAR point-wise intensity feature $I^\circledast$, in place of the distance-normalized reflectivity feature $R^\circledast$, offers inferior on-task performance.