%%%%%%%%% INTRODUCTION
\vspace{-0.3cm}   
\section{Introduction}
\vspace{-0.1cm}   

\noindent
3D semantic segmentation of LiDAR point clouds has played a key role in scene understanding, facilitating applications such as autonomous driving~\cite{hu2020randlanet,hou2022pointtovoxel,jaritz2021xmuda,Unal_2022_CVPR,yi2021complete,zhu2021cylindrical, abarghouei19depth} and robotics~\cite{milioto2019rangenet, wu2018squeezesega, wu2019squeezesegv2a,alonso20203dmininet}. However, many contemporary methods require relatively large backbone architectures with millions of trainable parameters requiring  many hundred gigabytes of annotated data for training at a significant computational cost. Considering the time-consuming and costly nature of 3D LiDAR annotation, such methods have become less feasible for practical deployment.  

\begin{figure}[tp]
    \hspace*{-15pt}
    \centering
    \includegraphics[width=0.53\textwidth]{figures/computaion_cost_big.pdf}
    \caption{mIoU performance (\%) against parameters and multiply-add operations on SemanticKITTI (fully annotated) and ScribbleKITTI (weakly annotated) under the 5\% sampling protocol.
    \vspace{-0.2cm}
    }
    \label{fig:computaion_cost}
\end{figure}

Existing supervised 3D semantic segmentation methods~\cite{wu2018squeezesega,wu2019squeezesegv2a,choy20194d,milioto2019rangenet,tang2020searching,xu2020squeezesegv3,kochanov2020kprnet,zhu2021cylindrical,yan20222dpass} primarily focus on designing network architectures for densely annotated data. To reduce the need for large-scale data annotation, and inspired by similar work in 2D~\cite{chen2021semisupervised, oord2019representation, wang2022semisupervised}, recent 3D work proposes efficient ways to learn from weak supervision~\cite{Unal_2022_CVPR}. However, such methods still suffer from high training costs and inferior on-task performance. To reduce computational costs, a 2D projection-based point cloud representation is often considered~\cite{alonso20203dmininet, cortinhal2020salsanext, kochanov2020kprnet, milioto2019rangenet, wu2018squeezesega, wu2019squeezesegv2a, xu2020squeezesegv3, zhang2020polarneta}, but again at the expense of significantly reduced on-task performance. As such, we observe a gap in the research literature for the design of semi or weakly supervised methodologies that employ a smaller-scale architectural backbone, hence facilitating improved training efficiency whilst also reducing their associated data annotation requirements.

In this paper, we propose a semi-supervised methodology for 3D LiDAR point cloud semantic segmentation. Facilitated by three novel design aspects, our \textit{Less is More} (LiM) based methodologies require \textit{less} training data and \textit{less} training computation whilst offering (\textit{more}) improved accuracy over contemporary state-of-the-art approaches (see~\cref{fig:computaion_cost}).

Firstly, from an architectural perspective, we propose a novel \textbf{Sparse Depthwise Separable Convolution (SDSC)} module, which substitutes traditional sparse 3D convolution into existing 3D semantic segmentation architectures, resulting in a significant reduction in trainable parameters and numerical computation whilst maintaining on-task performance (see ~\cref{fig:computaion_cost}). Depthwise Separable Convolution has shown to be very effective within image classification tasks~\cite{chollet2017xception}. Here, we tailor a sparse variant of 3D Depthwise Separable Convolution for 3D sparse data by first applying a single submanifold sparse convolutional filter~\cite{graham2017submanifold,graham20183d} to each input channel with a subsequent pointwise convolution to create a linear combination of the sparse depthwise convolution outputs. This work is the first to attempt to introduce depthwise convolution into the 3D point cloud segmentation field as a conduit to reduce model size. Our SDSC module facilitates a 50\% reduction in trainable network parameters without any loss in segmentation performance.

Secondly, from a training data perspective, we propose a novel \textbf{{\samplfull} ({\samplshort})} strategy that more effectively sub-samples a set of diverse frames from a continuously captured LiDAR sequence in order to maximize diversity within a minimal training set size. We observe that continuously captured LiDAR sequences often contain significant temporal redundancy, similar to that found in video ~\cite{akramullah2014digital}, whereby temporally adjacent frames provide poor data variation. On this basis, we propose to compute the temporal correlation between adjacent frame pairs, and use this to select the most informative sub-set of LiDAR frames from a given sequence. Unlike passive sampling (\eg, uniform or random sampling), our active sampling approach samples frames from each sequence such that redundancy is minimized and hence training set diversity is maximal. When compared to commonplace passive random sampling approaches~\cite{jiang2021guided, kong2022lasermix, Unal_2022_CVPR}, {\samplshort} explicitly focuses on extracting a diverse set of training frames that will hence maximize model generalization.

Finally, in order to employ semi-supervised learning, we propose a soft pseudo-label method informed by the LiDAR reflectivity response, thus maximizing the use of any annotated data samples. Whilst directly using unreliable soft pseudo-labels generally results in performance deterioration~\cite{arazo2020pseudolabeling}, the voxels corresponding to the unreliable predictions can instead be effectively leveraged as negative samples of unlikely categories.  Therefore, we use cross-entropy to separate all voxels into two groups, \ie, a reliable and an unreliable group with low and high-entropy voxels respectively. We utilize predictions from the reliable group to derive positive pseudo-labels, while the remaining voxels from the unreliable group are pushed into a FIFO category-wise memory bank of negative samples~\cite{alonso2021semisuperviseda}. To further assist semantic segmentation of varying materials in the situation where we have weak/unreliable/no labels, we append the reflectivity response features onto the point cloud features, which again improve segmentation results.

We evaluate our method on the SemanticKITTI~\cite{behley2019semantickittia} and ScribbleKITTI~\cite{Unal_2022_CVPR} {\validset}. Our method outperforms contemporary state-of-the-art semi-~\cite{kong2022lasermix,jiang2021guided} and weakly-~\cite{Unal_2022_CVPR} supervised methods and offers \textit{more} in terms of performance on limited training data, whilst using \textit{less} trainable parameters and \textit{less} numerical operations (\textit{Less is More}). 


Overall, our contributions can be summarized as follows:
\begin{itemize}
    \item A novel methodology for semi-supervised 3D LiDAR semantic segmentation that uses significantly \textit{less} parameters and offers (\textit{more}) superior accuracy.\footnote{{Full source code: \url{https://github.com/l1997i/lim3d/}.}}

    \item A novel Sparse Depthwise Separable Convolution (SDSC) module, to reduce trainable network parameters, and to both reduce the likelihood of over-fitting and facilitate a deeper network architecture.

    \item A novel {\samplfull} ({\samplshort}) strategy, to extract a maximally diverse data subset for training by removing temporal redundancy and hence future annotation requirements.
    
    \item A novel soft pseudo-labeling method informed by LiDAR reflectivity as a proxy to in-scene object material properties,  facilitating effective use of limited data annotation. 
\end{itemize}
