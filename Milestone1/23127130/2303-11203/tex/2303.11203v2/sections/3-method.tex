\vspace{-0.2cm}   
\section{Methodology}
\vspace{-0.2cm}




\noindent
We first present an overview of the mean teacher framework we employ (\cref{sec:mean-teacher}) and then explain our use of unreliable pseudo-labels informed by LiDAR reflectivity for semi-supervised learning (\cref{sec:learning-from-unreliable-pseudo-labels}). Subsequently, we detail our {\samplshort} strategy for dataset diversity (\cref{sec:MRFD}) and finally our parameter-reducing SDSC module (\cref{sec:sdsc}).

Formally, given a LiDAR point cloud $P=\{\textbf{p} \mid \textbf{p}=(x,y,z,I,R) \in \mathbb{R}^5\}$ where $(x,y,z)$ is a 3D coordinate, $I$ is intensity and $R$ is reflectivity, our goal is to train a semantic segmentation model by leveraging both a large amount of unlabeled $U = \{\textbf{p}_i^u\}_{i=1}^{N_u} \varsubsetneq P$ and a smaller set of labeled data $V = \{(\textbf{p}_i^v, \textbf{y}_i^v)\}_{i=1}^{N_v} \varsubsetneq P$. 


Our overall architecture involves three stages (\cref{fig:model}): (1) \textbf{Training:} we utilize reflectivity-prior descriptors and adapt the Mean Teacher framework to generate high-quality pseudo-labels; (2) \textbf{Pseudo-labeling:} we fix the trained teacher model prediction in a class-range-balanced~\cite{Unal_2022_CVPR} manner, expanding dataset with Reflectivity-based Test Time Augmentation (Reflec-TTA) during test time; (3) \textbf{Distillation with unreliable predictions:} we train on the generated pseudo-labels, and utilize unreliable pseudo-labels in a category-wise memory bank for improved discrimination.


\subsection{Mean Teacher Framework}
\label{sec:mean-teacher}

We introduce weak supervision using the Mean Teacher framework~\cite{tarvainen2017mean}, which %
avoids the prominent slow training issues associated with Temporal Ensembling~\cite{laine2017temporal}. This framework consists of two models of the same architecture known as the student and teacher respectively, for which we utilize a Cylinder3D~\cite{zhu2021cylindrical}-based segmentation head $f$. The weights of the student model $\theta$ are updated via standard backpropagation, while the weights of the teacher model $\theta^*$ are updated by the student model through Exponential Moving Averaging (EMA):
\begin{equation}\label{equ:ema}
    \theta_{t+1}^* = \kappa  \theta_t^* + (1-\kappa ) \theta_{t+1}, \quad t \in \{0,1,\cdots T-1\},
\end{equation}
where $\kappa$ denotes a smoothing coefficient to determine update speed, and $T$ is the maximum time step.

During training, we train a set of weakly-labeled point cloud frames with voxel-wise inputs generated via asymmetrical 3D convolution networks~\cite{zhu2021cylindrical}. For every point cloud, our optimization target is to minimize the overall loss:
\begin{equation}\label{equ:overall_loss}
    \mathcal{L} = \mathcal{L}_S + \lambda_U \mathcal{L}_U + g \lambda_C \mathcal{L}_C,
\end{equation}
where $\mathcal{L}_S$ and $\mathcal{L}_U$ denote the losses applied to the supervised and unsupervised set of points respectively, $\mathcal{L}_C$ denotes the contrastive loss to make full use of unreliable pseudo-labels, $\lambda_U$ is the weight coefficient of $\mathcal{L}_U$ to balance the losses, and $g$ is the gated coefficient of $\mathcal{L}_C$. $g$ equals $\lambda_C$ if and only if it is in the distillation stage.
We use the consistency loss (implemented as a Kullback-Leibler divergence loss~\cite{hou2022pointtovoxel}), lovasz softmax loss~\cite{berman2018lovaszsoftmax}, and the voxel-level InfoNCE~\cite{oord2019representation} as $\mathcal{L}_U$, $\mathcal{L}_S$ and $\mathcal{L}_C$ respectively.

We first generate our pseudo-labels for the unlabeled points via the teacher model. Subsequently, we generate reliable pseudo-labels in a class-range-balanced (CRB)~\cite{Unal_2022_CVPR} manner, and utilize the qualified unreliable pseudo-labels as negative samples in the distillation stage. Finally, we train the model with both reliable and qualified unreliable pseudo-labels to maximize the quality of the pseudo-labels.


\subsection{Learning from Unreliable Pseudo-Labels}
\label{sec:learning-from-unreliable-pseudo-labels}


\begin{figure}[thp]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/unreliable.pdf}
    \caption{Illustration on unreliable pseudo-labels. Left: entropy predicted from an unlabeled point cloud, with lower entropy corresponding to greener color. Right: Category-wise probability of an unreliable prediction \colorbox[RGB]{115,0,231}{\textcolor{fg3_yellow}{\textsf{X}}}, only \textcolor{fg3_blue}{top-4} and \textcolor{fg3_orange}{last-4} probabilities shown.}
    \label{fig:unreliable}
    \vspace{-8pt}
\end{figure}

\noindent
Unreliable pseudo-labels are frequently eliminated from semi-supervised learning tasks or have their weights decreased to minimize performance loss~\cite{sajjadi2016regularization,xie2020selftraining,zou2021pseudoseg,jiang2021guided,yang2022stb,Unal_2022_CVPR}. In line with this idea, we utilize CRB method~\cite{Unal_2022_CVPR} to first mask off unreliable pseudo-labels and then subsequently generate high-quality reliable pseudo-labels. 

However, such a simplistic discarding of unreliable pseudo-labels may lead to valuable information loss as it is clear that unreliable pseudo-labels (\ie, the corresponding voxels with high entropy) can offer information that helps in discrimination. Voxels that correlate to unreliable predictions can alternatively be thought as negative samples for improbable categories~\cite{wang2022semisupervised}, although performance would suffer if such unreliable predictions are used as pseudo-labels directly~\cite{arazo2020pseudolabeling}. 
As shown in~\cref{fig:unreliable}, the unreliable pseudo predictions show a similar level of confidence on \texttt{car} and \texttt{truck} classes, whilst being sure the voxel cannot be \texttt{pole} or \texttt{road}. 
Thus, together with the use of CRB for high-quality reliable pseudo-labels, we also ideally want to make full use of these remaining unreliable pseudo-labels rather than simply discarding them. Following~\cite{wang2022semisupervised}, we propose a method to leverage such unreliable pseudo-labels for 3D voxels as negative samples. However, to maintain a stable amount of negative samples, we utilize a category-wise memory bank $\mathcal{Q}_c$ (FIFO queue, \cite{wu2018unsupervised}) to store all the negative samples for a given class $c$. As negative candidates in some specific categories are severely limited in a mini-batch due to the long-tailed class distribution of many tasks (\eg autonomous driving), without such an approach in place we may instead see the gradual dominance of large and simple-to-learn classes within our generated pseudo-labels. 


Following ~\cite{oord2019representation,he2020momentum}, our method has three prerequisites, \ie, anchor voxels, positive candidates, and negative candidates. They are obtained by sampling from a particular subset, constructed via~\cref{eq:anchor_voxels_sampling_l} and ~\cref{equ:postive_samples}, in order to reduce overall computation. In particular, the set of features of all candidate anchor voxels for class $c$ is denoted as:
\begin{equation} \label{eq:anchor_voxels_sampling_l}
    \mathcal{A}_c=\left\{\mathbf{E}_{a,b} \mid y^{*}_{a,b}=c, p_{a,b}(c)>\delta_p\right\},
\end{equation}
where $\mathbf{E}_{a,b}$ is the feature embedding for the $a$-th point cloud frame at voxel $b$, $\delta_p$ is the positive threshold of all classes, $p_{a,b}(c)$ is the softmax probability by the segmentation head at $c$-th dimension. 
$y^{*}_{a,b}$ is set to the ground truth label $y^{*}_{a,b}$ if the ground truth is available, otherwise, $y^{*}_{a,b}$ is set to the pseudo label $\hat{y}_{a,b}$, due to the absence of ground truth.


The positive sample is the common embedding center of all possible anchors, which is the same for all anchors from the same category, shown in~\cref{equ:postive_samples}.
\begin{equation}\label{equ:postive_samples}
    \mathbf{E_c^+} = \frac{1}{\left\lvert \mathcal{A}_c \right\rvert} \sum_{\textbf{E}_c \in \mathcal{A}_c}{\textbf{E}_c}.
\end{equation}
Following~\cite{wang2022semisupervised}, we similarly construct multiple negative samples $\textbf{E}_c^-$ for each anchor voxel.




Finally, for each anchor voxel containing one positive sample and $N-1$ negative samples, we propose the voxel-level InfoNCE loss~\cite{oord2019representation} (a variant of contrastive loss) $\mathcal{L}_C$ in~\cref{eq:contrastive_loss} to encourage maximal similarity between the anchor voxel and the positive sample, and the minimal similarity between the anchor voxel and multiple negative samples.
\begin{equation}
    \label{eq:contrastive_loss}
    \footnotesize
    \begin{split}
        \mathcal{L}_C &= -\frac{1}{C} \sum_{c=0}^{C-1} \underset{\mathbf{E}_c}{\mathbb{E}}\left[\log \frac{f\left(\textbf{e}_c, \textbf{e}_c^{+}, \tau\right)}{\sum_{\textbf{e}{_{c,j}^{-}} \in \textbf{E}{_c^{-}}} f\left(\textbf{e}_c, \textbf{e}_{c,j}^{-}, \tau\right)}\right] \\
        &= -\frac{1}{C} \sum_{c=0}^{C-1} \underset{\mathbf{E}_c}{\mathbb{E}} \left[ \log \frac{\exp\left(\left\langle \textbf{e}_c, \textbf{e}_c^+ \right\rangle / \tau \right)}{\exp\left(\left\langle \textbf{e}_c, \textbf{e}_c^+ \right\rangle / \tau \right) + \sum\limits_{j=1}^{N-1}{\exp \left(\left\langle \textbf{e}_{c}, \textbf{e}_{c,j}^- \right\rangle  / \tau \right)}} \right]
    \end{split}
\end{equation}
where $\left\langle \cdot, \cdot \right\rangle$ denotes cosine similarity. $\mathbf{e_c}$, $\mathbf{e_c^+}$ and $\textbf{e}_{c, j}^{-}$ denote the embedding, positive sample of the current anchor voxel, and embedding of the $j$-th negative sample of class $c$. 










\subsection{Reflectivity-Based Test Time Augmentation}

\noindent To obtain minimal accuracy degradation despite very few weak labels, \eg, 1\% weakly-labeled ScribbleKITTI~\cite{Unal_2022_CVPR} dataset, we propose a Test Time Augmentation (TTA) that does not depend on any label, but only relies on a feature of original LiDAR points themselves. Also included in almost every LiDAR benchmark dataset for autonomous driving~\cite{Geiger2012,behley2019semantickittia,nuscenes2019,lyft2019,li2021durlara,Unal_2022_CVPR}, is the intensity of light reflected from the surface of an object at each point. In the presence of limited data labels in the semi-supervised learning case, this property of the material surface, normalized by distance to obtain the surface reflectivity in~\cref{equ:def_reflec}, could readily act as auxiliary information to identify different semantic classes. 

Our intuition is that reflectivity $R$, as a point-wise distance-normalized intensity feature, offers consistency across lighting conditions and range as:
\begin{equation}\label{equ:def_reflec}
    R = I r^{2} = \frac{S}{4 \pi r^2} \cdot r^{2} \propto S,
\end{equation}
where $S$ is the return strength of the LiDAR laser pulse, $I$ is the intensity and $r$ is the point distance from the source on the basis that scene objects with similar surface material, coating, and color characteristics will share similar $S$ returns.

\begin{figure}[thp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/reflec_tta.pdf}
    \caption{Coarse histograms of Reflec-TTA bins (not to scale). }
    \label{fig:reflec_tta}
    \vspace{-5pt}
\end{figure}



On this basis, we define our novel reflectivity-based Test Time Augmentation (Reflec-TTA) technique, as a substitute for label-dependent Pyramid Local Semantic-context (PLS) augmentation~\cite{Unal_2022_CVPR} during test-time as ground truth is not available. We append our point-wise reflectivity to the existing point features in order to enhance performance in presence of false or non-existent pseudo-labels at the distillation stage. As shown in~\cref{fig:reflec_tta},  and following~\cite{Unal_2022_CVPR}, we apply various sizes $s$ of bins in cylindrical coordinates to analyze the intrinsic point distribution of the LiDAR sensor at varying resolutions (shown in red, green and blue in ~\cref{fig:reflec_tta}). For each bin $b_i$, we compute a coarse histogram, $\mathbf{h}_i$:
\begin{equation}\label{equ:bins_histo}
    \begin{aligned}
        &\mathbf{h}_i& &\hspace{-10pt} = \left\{ h_i^{(k)} \mid k \in \left[1, N_{b}\right]  \right\} \in \mathbb{R}^{N_{b}}, \quad i \in \left[1, s\right], \\
        &h_i^{(k)}& &\hspace{-10pt} =\#\left\{R_j \in r_k, \; \forall j \mid p_j \in b_i\right\}, \\
        &r_k& &\hspace{-10pt} = [\; (k-1)/N_{b}, \; k/N_{b}\;), \quad k \in \left[\; 1, N_{b} \;\right].
    \end{aligned}
\end{equation}
The Reflec-TTA features $R^\circledast$ of all points $p_j \in b_j$ is further computed as the concatenation of the coarse histogram $\mathbf{h}_i$ of the normalized histogram:
\begin{equation}\label{equ:reflectta}
    R^\circledast=\left\{\mathbf{h}_i / \max\left(\mathbf{h}_i\right)  \mid i \in \left[1, s\right] \right\} \in \mathbb{R}^{s N_{b}}
\end{equation}
In the distillation stage, we append $R^\circledast$ to the input features and redefine the input LiDAR point cloud as the augmented set of points $P^\circledast = \left\{p \mid \left(x,y,z,I,R^\circledast\right) \in \mathbb{R}^{s N_{b}+4} \right\} $.




\subsection{\textbf{\textls[-2]{{\samplfull}}}}
\label{sec:MRFD}

\noindent
Due to the spatio-temporal correlation of LiDAR point cloud sequences often captured from vehicles in %
metropolitan locales, many large-scale point cloud datasets demonstrate significant redundancy. Common datasets employ a frame rate of 10Hz~\cite{Geiger2012,nuscenes2019,behley2019semantickittia,lyft2019,Bijelic2020,sun2020scalability,li2021durlara}, and a number of concurrent laser channels (beams) of 32~\cite{nuscenes2019}, 64~\cite{Geiger2012,behley2019semantickittia,lyft2019,Bijelic2020,sun2020scalability} or 128~\cite{li2021durlara}. Faced with such large-scale, massively redundant training datasets, the popular practice of semi-supervised semantic segmentation approaches~\cite{tarvainen2017mean,french2020semisupervised,zou2018unsupervised,chen2021semisupervised,kong2022lasermix} is to uniformly sample 1\%, 10\%, 20\%, or 50\% of the available annotated training frames, without considering any redundancy attributable to temporary periods of stationary capture (\eg due to traffic, \cref{fig:mrfd_real}) or multi-pass repetition (\eg due to loop closure). 


\begin{figure}[htp]
    \hspace*{-12pt}
    \centering
    \includegraphics[width=0.52\textwidth]{figures/mrfd_real.pdf}
    \caption{Illustration of LiDAR frame temporal correlation as \textit{[\#~frame~ID]~redundancy} with 5\% sampling on SemanticKITTI~\cite{behley2019semantickittia} ( sequence \texttt{00}) using uniform sampling (selected frames in \textcolor{fg5_blue}{\FilledSmallCircle}) and {\samplshort} strategy (\textcolor{fg5_red}{\FilledSmallCircle}).}
    \label{fig:mrfd_real}
    \vspace{-10pt}
\end{figure}

To extract a diverse set of frames, we propose a novel algorithm called {\samplfull} ({\samplshort}, \cref{alg:MRFD}) that determines spatio-temporal redundancy by analyzing the spatial-overlap within time-continuous LiDAR frame sequences. %
The key idea is that if spatial-overlap among some continuous frame sequence is high due to spatio-temporal redundancy, %
multiple representative frames can be sub-sampled for training, significantly reducing both training dataset size and redundant training computation. %

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1\textwidth]{figures/mrfd_ex.pdf}
    \caption{Overview of our proposed {\samplfull} approach.}
    \label{fig:mrfd_ex}
    \vspace{-8pt}
\end{figure*}

\cref{fig:mrfd_ex} shows an overview of {\samplshort}. It is conducted inside each temporal continuous LiDAR sequence $e$. First, we evenly divide $p$ point cloud frames in each sequence into $\left\lceil p/q \right\rceil$ subsets (containing $q$ frames). For each frame at time $t$ inside the subset, we find its corresponding RGB camera image in the dataset at time $t$ and $t+1$. To detect the spatio-temporal redundancy at time $t$, the similarity $\psi(t, t+1)$ between temporally adjacent frames are then computed via the Structural Similarity Index Measure (SSIM,~\cite{wang2004image}). We utilize the mean value of similarity scores between all adjacent frames in the current subset as a proxy to estimate the spatio-temporal redundancy present. A sampling rate is then determined according to this mean similarity for frame selection within this subset. This is repeated for all subsets in every sequence to construct our final set of sub-sampled LiDAR frames for training.

Concretely, as shown in~\cref{alg:MRFD}, we implement a {\samplshort} supervisor that determines the most informative assignments (\ie, the key point cloud frames) that the teacher and student networks should train on respectively. The {\samplshort} supervisor has an empirical supervisor function $\upsilon$, which decides the amount of assignments, \ie, the sampling rate corresponding to the extent of spatio-temporal redundancy.  
Using SSIM~\cite{wang2004image} as the redundancy function $\psi$ to measure the similarity between the RGB images associated with two adjacent point clouds, we define the empirical supervisor function $\upsilon$ with decay property $\upsilon (x) = \exp(-\beta x)$, where $\beta \in (0, +\infty)$ is the decay coefficient, and $x$ is the redundancy calculated from $\psi$. In this way, the higher the degree of spatio-temporal redundancy (as $\psi \rightarrow 1$), the lower the sampling rate our {\samplshort} supervisor will allocate, hence reducing the training set requirements for teacher and student alike.

\begin{algorithm}[htp]
    \footnotesize
    \SetKwInput{KwInput}{Input}
    \SetKwInput{KwOutput}{Return}
    \DontPrintSemicolon
    \SetAlgoLined
    \SetNoFillComment
    \caption{{\samplfull}.}
    \label{alg:MRFD}
    \SetAlgoVlined
    \KwInput{Point cloud frames pool $P$ (size of $p$), subset size $q$, redundancy function $\psi \in [0, 1]$ and empirical supervisor function~$\upsilon$.}
    Divide $P$ evenly into $\left\lceil p/q \right\rceil$ subsets $Q$. \\
    $D \gets$ empty dictionary. \\
    \ForAll{$e \gets 0:n_e-1$}{
        \tcp{loop for all sequences} 
        $C_e \gets \varnothing$ \tcp*{chosen point cloud frames} 
        \ForAll{$i \gets 0:\left\lceil p/q \right\rceil-1$}{
            \tcp{loop for subsets $Q$}
            $Q_{i, j} \gets$ $j$-th frame in subset $Q_i$. \\
            $\overline{M_i} \gets \frac{1}{q} \sum_{j=0}^{q-1}{\psi(Q_{i,j})}$. \tcp*{redundancy} 
            $k_i \gets \left\lceil \upsilon(\overline{M_i}) \cdot q\right\rceil $. \\
            $T_i \gets$ select $k_i$ frames in $Q_i$ with the smallest $M_i$.  \\
            $C_e \gets C_e \cup T_i$. 
        }
        Append key-value pair $(e, C_e)$ into $D$. 
    }
    \KwOutput{Dictionary $D$.}
\end{algorithm}

\begin{figure}[thp]
    \hspace{-0.48cm}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sdsc_module.pdf}
    \caption{Illustration of the SDSC convolution module.}
    \label{fig:sdsc_module}
    \vspace{-0.5cm}
\end{figure}





\subsection{Sparse Depthwise Separable Convolution}
\label{sec:sdsc}


\noindent
Existing LiDAR point cloud semantic segmentation methods generally rely on a large-scale backbone architecture with tens of millions of trainable parameters ~\cite{hu2020randlanet,hou2022pointtovoxel,jaritz2021xmuda,Unal_2022_CVPR,yi2021complete,zhu2021cylindrical} due to the requirement for 3D (voxel-based) convolution operations, to operate on the voxelized topology of the otherwise unstructured LiDAR point cloud representation, which suffer from both high computational training demands and the risk of overfitting. Based on the observation that depthwise separable convolution has shown results comparable with regular convolution in tasks such as image classification but with significantly fewer trainable parameters~\cite{chollet2017xception,howard2017mobilenets,sandler2018mobilenetv2,tan2019efficientnet,masters2021making,howard2019searching}, here we pursue the use of such an approach within 3D point cloud semantic segmentation.



As such we propose the first formulation of sparse variant depthwise separable convolution~\cite{howard2017mobilenets} applied to 3D point clouds, namely Sparse Depthwise Separable Convolution (SDSC). SDSC combines the established computational advantages of sparse convolution for point cloud segmentation~\cite{graham20183d}, with the significant trainable parameter reduction offered by depthwise separable convolution~\cite{chollet2017xception}. 


Our SDSC module, as outlined in~\cref{fig:sdsc_module}, initially takes a tensor $F \in \mathbb{R} ^{H_F \times W_F \times L_F \times M}$ as input, where $H_F$, $W_F$, $L_F$ and $M$ denote radius, azimuth, height in the cylinder coordinate~\cite{zhu2021cylindrical} and channels respectively. Firstly, a sparse depthwise convolution $\text{SDC}(M,M,D_k,s=1)$ is applied, with $M$ input and output feature planes, a kernel size of $D_k$ and stride $s$ in order to output a tensor $T \in \mathbb{R} ^{H_F \times W_F \times L_F \times M}$. Inside our sparse depthwise convolution, $M$ sparse spatial convolutions are performed independently over each input channel using submanifold sparse convolution~\cite{graham20183d} due to its tensor shape preserving property at no computational or memory overhead. Secondly, the sparse pointwise convolution $\text{SPC}(M,N,1,s=1)$ projects the channels output $T$ by the sparse depthwise convolution onto a new channel space, to mix the information across different channels. As a result, the sparse depthwise separable convolution $\text{SDSC}(M,N,D_k,s=1)$ is the compound of the sparse depthwise convolution and the sparse pointwise convolution, namely $\text{SDSC}(M,N,D_k,s=1)=\text{SDC}\circ \text{SPC}$.

Using a sparse voxelized input representation similar to~\cite{graham2015sparse}, and a series of such SDSC sub-modules we construct the popular Cylinder3D~\cite{zhu2021cylindrical} sub-architectures within our overall Mean Teacher architectural design (\cref{fig:model}). 