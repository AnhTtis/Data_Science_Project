\section{Introduction}

Freely available vision-language (VL) data has shown great promise in advancing vision tasks \cite{Radford2021LearningTV,wslimageseccv2018,Jia2021ScalingUV}. 
Unlike smaller, curated vision-language datasets like COCO \cite{Lin2014MicrosoftCC}, 
captions on the web \cite{Ordonez2011Im2TextDI, Desai2021RedCapsWI, Changpinyo2021Conceptual1P} only \textit{partially} describe the corresponding image, and often describe the \textit{context}, which could include
objects that do not appear in the image. We hypothesize this 
poses a greater challenge for weakly-supervised object detection (WSOD) than learning cross-modal representations for image recognition (e.g. as in CLIP). 
WSOD involves learning to localize objects, i.e. predict bounding box coordinates along with the corresponding semantic label, from image-level labels only (i.e. using weaker supervision than the outputs expected at test time). So, noise could compound the challenge of implicitly learning localization.
WSOD has primarily been applied \cite{ye_2019_cap2det, Fang2022DataDD} to smaller, relatively cleaner, paid-for crowdsourced vision-language datasets like COCO \cite{Lin2014MicrosoftCC} and Flickr30K \cite{Young2014Flicker30K}.

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{iccv2023AuthorKit/figures/images/concept_fig_w_labels_full.png}
    % \caption{Extracted labels from captions raise challenges such as missing objects or defects, annotated in our dataset, \textbf{C}aption \textbf{La}bel \textbf{N}oise. \textbf{None of the underlined objects are clearly visible.} We propose a method to detect such noise and compare it to alternatives.}
    \caption{Examples of noisy extracted labels (underlined%in captions
    ) from our \textbf{C}aption \textbf{La}bel \textbf{N}oise dataset. 
    % Each caption contains an underlined object that is not visible in the corresponding image. 
    We categorize types of similar context present instead of the underlined object, as well as types of visual defects and linguistic indicators that are useful for detecting noise.}
    \label{fig:concept}
\end{figure}

We argue that extending WSOD from paid-for captions to large-scale, in-the-wild captions is not trivial. 
% Unlike captions written by 
Annotators write captions that %for the purpose of
faithfully describe an image, however, web captions %on the web 
go beyond a 
% redundant, 
descriptive relationship with their corresponding image.
%in many ways. 
For example, a word can be used literally or metaphorically (``that was a piece of \underline{cake}'') or have multiple senses, of which only one sense 
is relevant to the object detection vocabulary. A caption could also share a story and include context that goes beyond the visual contents of the image; this context could mention an object name within location names or describe occluded or unpictured interactions with objects as shown in Figure \ref{fig:concept}. \textbf{This richness of language is relevant as narration for the image but not as supervision for the precise localization of objects.} On the visual side, user-uploaded content frequently features diverse object presentations, including intriguing atypical objects, hand-drawn objects, or photos taken from within vehicles (``in my \underline{car}'').

We refer to image-level labels extracted from captions, that are incorrect (object not present in the corresponding image), as \textbf{v}isually \textbf{a}bsent \textbf{e}xtracted \textbf{l}abels (VAELs). We show VAELs pose a challenge for weakly-supervised object detection.
 
To cope with this challenge, we propose \textbf{VEIL}, short for \textbf{V}etting \textbf{E}xtracted \textbf{I}mage \textbf{L}abels, to directly learn whether a label is clean or not from \textit{caption context}. 
We first extract potential labels from each caption using substring matching or exact match \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019, Fang2022DataDD}. 
We then use a transformer to 
predict whether each extracted label is visually present. We refer to this prediction \emph{task} as extracted label \underline{vetting}.
We bootstrap %image-level 
pseudo-ground-truth visual presence labels for each extracted label or object mention using an ensemble of two pretrained object recognition models \cite{yolov5, zhang2021vinvl}, for a variety of large-scale, noisy datasets: Conceptual Captions \cite{Sharma2018ConceptualCA}, RedCaps \cite{Desai2021RedCapsWI}, and SBUCaps \cite{Ordonez2011Im2TextDI}. While 
these models are trained on COCO and similar datasets, they generalize well to estimating extracted label visual presence on in-the-wild VL datasets; however, their predictions are better used as targets for VEIL, rather than directly for vetting. 
Once we vet the extracted labels, we use them to train a weakly-supervised object detector. 

% We investigate sources of noise across three in-the-wild datasets from diverse sources: a photo-sharing platform, a social media platform, and images with alt-text (typically used for VL pertaining). 
We collect and release the \textbf{C}aption \textbf{La}bel \textbf{N}oise (CLaN) dataset with annotations on object visibility (label noise) and object appearance defects (visual noise such as atypical appearance) over three in-the-wild datasets. 
To support using language context to filter object labels, we annotate linguistic indicators of noise that explain \textit{why} an object is absent from the image but mentioned in the caption. 
%These indicators include describing context outside the image, non-literal use, different word sense, etc. 
% We compare 
Our label vetting method outperforms nine diverse baselines, including standard cross-modal alignment prediction methods (CLIP), adaptive noise reduction methods, pseudo-label prediction, simple rule-based methods, and no vetting. 
% Our method improves upon the baselines both in terms of predicting extracted label visual presence (measured with F1) and producing cleaner training data for object detection. 
This means VEIL produces cleaner WSOD training data which leads to an improvement of +10 mAP over data cleaned using Large Loss Matters \cite{Kim2022LargeLM} and +3 mAP improvement over using CLIP \cite{Radford2021LearningTV} for filtering.
%, when we train on a held-out SBUCaps subset. 
Our findings reveal that naively combining noisy SBUCaps supervision with clean labels from Pascal VOC-07 degrades performance (42.06 mAP) versus using only clean labels (43.48 mAP); however, vetting with VEIL improves performance to 51.31 mAP.
% We show % +9.25 mAP 
% a significant improvement when training WSOD with both clean (annotated in Pascal VOC 07) and noisy, but vetted labels from SBUCaps (51.31 mAP) compared to naively combining clean with noisy labels without vetting (42.06 mAP) or only using clean labels (43.48 mAP). 
Lastly, VEIL's 
% generalizes and 
% its 
gains persist across datasets, object vocabulary, and scale.
%WSOD performance improves with vetting over no vetting as the train dataset is scaled.

To summarize, our contributions are:
\begin{enumerate}[nolistsep,noitemsep]
    \item VEIL, a transformer-based extracted label, visual presence classifier, and
    \item constructing 
    the  \textbf{C}aption \textbf{La}bel \textbf{N}oise dataset.
\end{enumerate}

We find that: \begin{enumerate}[nolistsep,noitemsep]
    \item VEIL outperforms language-conditioned, visual-conditioned, and language-agnostic label noise correction approaches
    in vetting labels from a wide set of in-the-wild datasets for weakly-supervised object detection. 
    \item VEIL enables effective combination of extracted noisy and clean labels.
    \item Even when VEIL is trained on one dataset/category, but applied to another, it shows advantages over baselines. 
\end{enumerate}