\section*{Limitations}

We identify the following limitations of our work. 
First, we assume that captions from SBUCaps, RedCaps, CC cover most in-the-wild caption types.
Second, while VEIL shows promise in generalizing across datasets, there is a performance drop due to label noise distribution differences between datasets. For example, Table 1 shows differences in linguistic indicator distributions across datasets. Since VEIL relies on caption context, it will be sensitive to such changes as shown by our generalization analysis in Sec. \ref{sec:expts}.
Third, VEIL also shows that it can filter unseen object categories (Table 5), however, its performance is noticeably below VEIL-ID which was trained on those object categories. This would be an interesting future direction for research.
Fourth, we noticed that MIST (WSOD method) was highly sensitive to learning rate and that Large Loss Matters was highly sensitive to hyperparameters. We have included these results in A.4.
Fifth, VEIL is sensitive to the gold labels used for training. % VEIL. 
We found that using weaker models (VinVL) to produce labels for VEIL will lead to suboptimal vetting and WSOD results compared to using a stronger model (YOLOv5). 

Lastly, generative vision-language models such as GPT4-V \cite{Achiam2023GPT4TR} open an opportunity to reject noisy labels as well. We think our work would be useful in aiding GPT4-V; a prompt defining noisy samples could use criteria from CLaN (e.g. types of object visibility, visual defects, and linguistic indicators categories). We believe VEIL still serves as a \textbf{lightweight} method to vet labels and could be trained using pseudo-visual presence labels from any source, including generative vision-language models.

