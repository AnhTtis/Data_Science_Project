\section{Conclusion}
We released the Caption Label Noise (CLaN) dataset where we annotated types of visually absent extracted labels 
and linguistic indicators of noise in 300 image-caption pairs from three in-the-wild datasets.
Using CLaN, we find that caption context can be used to vet (filter) extracted labels from caption context. We proposed VEIL, a lightweight text model which is trained to predict visual presence using pseudo labels sourced from two pretrained models for recognition. VEIL outperformed nine baselines representative of current noise filtering techniques that could be adapted for captions.

% Our simple technique for filtering (VEIL) avoids some of the biases and limitations of prior methods that could be adapted for use in filtering, e.g. CLIP and Large Loss. 
We demonstrate three key findings specific to vetting for WSOD: (1) there is a distinct advantage in learning to filter 
as opposed to filtering using pseudo-ground truth visual presence labels; 
% that were used to train VEIL
(2) vetting noisy labels is necessary to improve performance  
% not only when training exclusively on noisy data but also
when combined with a clean data source (existing image recognition and detection datasets); (3) structured noise such as noun modifiers and prepositional phrases (e.g. ``car window'', ``on a boat'') has a disproportionate impact on localization and was difficult to detect using visual-based methods like CLIP and Large Loss Matters.
% through the higher performance of VEIL-vetted WSOD compared to CLIP-vetted WSOD. 
This last finding 
implies that %(1) popular methods to filter image-caption pairs like CLIP can miss certain types of noise and (2)
not all noise is equal in impact. CLaN is a starting point for this type of analysis and further research is needed to expand noise categories and measure the impact of the different types of noise.% and measure their impact. 

% Considering that there are existing clean human-annotated datasets, we show that adding extracted labels from in-the-wild captions (noisy) is ineffective without vetting. We leave exploring further analysis on why this happens to future research.

% for many vision tasks. 
% Our work can enhance value in (1) the categorization of label noise especially because those categories can be used as guidance for a VLM, and in (2) establishing diverse baselines for the vetting task.

% Old conclusion
% We showed visually absent extracted labels are common in the wild, VEIL which uses language context to infer if mentioned objects are visually present, and the benefits of its vetting.
%generalizes across datasets and categories, and its benefits persist when adding noisy to clean data. 