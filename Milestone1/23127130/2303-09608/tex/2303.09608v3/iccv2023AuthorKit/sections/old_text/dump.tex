%First, we confirm that the precision of extracted labels from captions from three in-the-wild datasets is far lower than curated, descriptive captions from COCO \cite{Lin2014MicrosoftCC} and VIST-DII \cite{huang2016visual}.

%Building on this, we show that the narrative element found in in-the-wild corpora can impact the ability to successfully train an object detection model, and the contribution of this noise can be reduced using VEIL.
%Multimodal datasets commonly include vision and language data, but differ in the form each modality is presented; for example, the vision modality can be presented via static images \cite{Schuhmann2021LAION400MOD,Radford2021LearningTV,wslimageseccv2018}, images in a sequence \cite{huang2016visual,Yagcioglu2018RecipeQAAC}, or long \cite{Miech2019HowTo100MLA,Chen_2017_CVPR} or short videos \cite{Tang2021ComprehensiveIV}. Similarly, language in multimodal datasets can also be represented in diverse forms such as procedural (instructional) language \cite{Miech2019HowTo100MLA,Yagcioglu2018RecipeQAAC,Hessel2019UnsupervisedDO}, narrative-like language
%\cite{huang2016visual,Chen_2017_CVPR,Biten2019GoodNE,Huang2020MovieNetAH,Thomas2019PredictingTP, Ordonez2011Im2TextDI, Desai2021RedCapsWI}, user-written alternative text \cite{Sharma2018ConceptualCA,Radford2021LearningTV,Lin2014MicrosoftCC,Young2014FromID}.
%Typically, instructional videos have been used for learning visual-textual grounding \cite{Miech2019HowTo100MLA,Miech2020EndtoEndLO}; Hessel \etal \cite{Hessel2020BeyondIV} studied the extension to using \textit{diverse non-instructional videos} for visual-textual grounding. Similarly part of our work studies the effect of \textit{extracting labels} from descriptive or narrative captions for weakly supervised object detection, rather than a multimodal retrieval-based notion of grounding. We also go beyond studying differences in object detection performance when extracting labels from descriptive or narrative captions and generally screen for false positive extracted label noise.

%, prior to finetuning detection using full supervision from a constrained set of classes. 
%As described in detail in the previous section, vision-language pretraining for open vocabulary object detection utilizes captions during pretraining while detection can be learnt from annotated bounding boxes \cite{zareian2021open}.  or from pseudo-labeled bounding boxes \cite{regionclip}. \cite{Gao2021OpenVO} or a combination of both \cite{Li2021GroundedLP_GLIP} uses both ground truth and pseudo-labeled bounding boxes .
%Additionally, we argue that false positives are far more harmful than false negatives, as there are few positive examples than there are negative examples. 
% Another approach used a vision-language dataset during pretraining, and bounding boxes for only some categories, to generalize detection abilities to novel classes \cite{zareian2021open}. 

%Zero-shot detection \cite{ZSOD_1_Bansal_2018_ECCV} is a task that tests the ability of an object detection model trained on a set of base classes to generalize to a set of unseen target classes. 

%Lastly, Detic \cite{Zhou2022DetectingTC} performs weakly supervised object detection with a CLIP-based open vocabulary classifier head on ImageNet \cite{Deng2009ImageNetAL} and Conceptual Captions \cite{Sharma2018ConceptualCA} as pretraining prior to supervised learning from base class examples. 

% some papers:
% Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model
% RegionCLIP
% ProposalCLIP
% ECCV 2022 https://arxiv.org/abs/2111.09452 
% Detic
%GLIP

% We evaluate only the state-of-the-art small-loss-based approach \cite{Kim2022LargeLM} on weakly supervised object detection because it's computationally efficient to integrate with WSOD. %Our work is complementary to the progress in this problem because 
%This highlights another area for adaptive noise reduction methods to focus on.

%One could also perform label extraction that considers synonyms, synsets, and word-similarity based matching against a closed vocabulary set; this can significantly increase the number of examples \footnote{For our purposes this is not needed since we are only trying to show that visual presence can be detected directly from captions even while learning from noisy pseudo-visual presence labels and that VEIL's filtering significantly improves WSOD performance compared to no filtering and language agnostic filtering and/or correction.}. 

%We begin with a case study of the VIST dataset \cite{huang2016visual} which contains both descriptive and narrative captions for the same image (presumably cleaner/noisier, respectively). We then examine broader noise patterns extending beyond narrativeness, in three additional datasets. 

% FROM METHOD 

% These embeddings are then passed to BERT, which performs multi-head self-attention over the tokens and outputs token-level output embeddings $v\in \mathbb{R}^{C\times d}$

%This is represented as an indicator function $\mathbbm{1}_{EL}(i)$ in our description below, but in practice is simply a vector mask. 
%(See Table \ref{}). This 250 image-caption subset was annotated with visual presence/absence of certain objects (elephant, truck, cake, bus, cow) for each of the in-the-wild datasets.
%The image and caption's visual presence pseudo labels are represented as binary vector $y$ where 1 denotes visual presence. 
% An extracted label mask with the same length as the number of tokens and all the tokens corresponding to an extracted label is marked as 1. This mask is set up such that the binary cross entropy loss averaged only over the tokens associated within the extracted label token set $EL$.

%with one hidden layer followed by the tanh activation function:

%special token works better empirically in generalizing to new categories in addition to generalizing to other datasets. Overall, we show that these techniques can help improve its ability to generalize to new categories.

% We test out additional variants that reduce the model's reliance on category specific predictions: inserting a special token right before each extracted label and randomly masking the tokens associated with the extracted labels with probability $p_m$. We hypothesize that a special token can learn a representation specific to vetting, rather than   There are other existing special tokens like {\tt [CLS], [SEP], [MASK] } in BERT \cite{BERT}; so we add {\tt [EM\_LABEL] } to the vocabulary and its respective fixed input embedding is randomly initialized. When preprocessing the caption, this special token is injected before the object mention within the caption. The model should be able to implicitly learn that this token is related to the adjacent tokens through positional embeddings concatenated with each input embedding, an existing feature of transformers \cite{Attn_Vaswani}. 

% We ignore predictions from tokens not associated with an extracted label, but these tokens are useful in establishing caption context. 
% MOVING PART ABOUT GOLD/PSEUDO GROUND TRUTH LABELS TO ANALYSIS SECTION

% We follow prior literature \cite{bilen2016weakly,Ye_Zhang_Kovashka_Li_Qin_Berent_2019} 
% to train a network to predict object categories.
% A convolutional neural network base encoder $h(x_i)$ is used to extract a feature map for an image, $x_i$. We use VGG-16 \cite{Liu2015VeryDC} with the fully connected layers removed as our base encoder, initialized with pre-trained ImageNet \cite{Deng2009ImageNetAL} weights. Region of interest (ROI) pooling \cite{Girshick2015FastR} is then applied to the feature map and the regions of interest $R_i \in \mathbb{R}^{4\times M}$ to generate a feature embedding $\phi(x_i)_m$ for each region: 
% $\phi(x_i) = ROIPool(h(x_i), R_i)$.
% We initialize two parallel fully-connected layers $f_c$ and $f_d$ whose outputs will be normalized to give a classification score, the probability that an object $c$ is present in that region, and detection score, the probability that $R_{i,m}$ contributes to the image-level class prediction, respectively. 
% \begin{equation}
%     p^{cls}_{m, c} = \sigma{(f_c(\phi(x_i)_m))}, ~~~ p^{det}_{m, c }=\frac{\exp\Big(f_d(\phi(x_i)_m)\Big)}{\sum_{j=1}^{M} \exp\Big(f_d(\phi(x_i)_j)\Big)}
% \end{equation}

% The class and detection predictions for each region are multiplied and summed over $M$ proposals to produce one image-level class prediction vector.
% \begin{equation}
%     \hat{p_c}=\sigma\Bigg(\sum_{m=1}^{M} p_{m,c}^{det} o_{m,c}^{cls}\Bigg)    
% \end{equation}  

% To train, we apply the multiple instance detection loss to the extracted image-level label and the predicted image-level label \cite{bilen2016weakly}:
% \begin{equation}
%     L_{mid}=\frac{1}{C}\sum_{c=1}^{C}\Big[y_c \log \hat{p_c} + (1 - y_c) \log(1-\hat{p_c})\Big]
% \end{equation}

% FROM EXPERIMENTS

%We will also describe the implementation details of other methods we compare against.
%\textbf{Implementation Details for Vetting Methods}

%We evaluate the ability of VEIL and other intuitive approaches to vet extracted labels for visual presence on each data source's test split. First, we establish a simple baseline by examining performance without vetting. Then, we'll compare the vetting methods, which are arranged into three categories: language-conditioned, image-conditioned, and image and language conditioned. Note that VEIL-Same Dataset is the only approach trained on the same source's train split and Large Loss Matters \cite{Kim2022LargeLM} is simulated on the test split. 

% Figure 4 in that paper shows the improvement 
%However, the precision may suggest that LocalCLIP-E sometimes hallucinates an object or the ground truth may be missing an object detection (inaccurate visual presence labels). 
% Since image-based approaches for label vetting work quite well, we also tested how much pseudo-labeled data it would take for VEIL-SBUCaps to do better than the image-based methods. Our results shown in Figure \ref{fig:train_subset} (left) indicate only a small amount (25K) is needed to do better in terms of F1 compared to LLM, but more data (75K) is needed to improve F1 over Local CLIP-E. However, all this data is pseudo-labeled so it does not require any additional cost. 

% \textbf{Localization vs. Recognition.}
% We can see that even though Reject Large Loss and Local CLIP-E have strong vetting results (Table \ref{tab:direct_eval}) they perform poorly in weakly supervised object detection (Table \ref{tab:det_v_recognition}). We hypothesize there might be differences in detection and recognition results, as recognition may benefit from some types of noisy data, such as a specific type of structured noise where context is shared between images with the VAEL and examples of actually containing that category.
% When evaluating the precision of image-level predictions on Pascal VOC 2007 \cite{Everingham2010ThePV}, we observe that Local CLIP-E does do better when it comes to recognition ability (+19\% relative to VEIL-SBUCaps), but significantly worse in detection (-62\% relative to VEIL-SBUCaps). Similarly, we observe only an 8\% drop in recognition performance for Reject Large Loss compared to VEIL-SBUCaps but a 38\% drop in detection. VEIL$_{\text{ST}}$-Redcaps,CC is weaker than VEIL-SBUCaps in vetting SBUCaps data, but has a similar performance drop in both detection and recognition. This indicates VEIL vets out noisy labels harmful to localization, even when trained on different sources, and its accepted labels are better for learning localization.

% \input{iccv2023AuthorKit/figures/structured_noise_qual}

% We show examples in Fig.~\ref{fig:structured_qual} containing correct image-level predictions from the LLM-trained WSOD model and the VEIL-SBUCaps-vetted WSOD model. We show recurring examples across categories where parts of the object (car engine) or background context (water or buildings) are detected as the object by LLM. This appears to be the effect of structured noise in training where labels are applied to images that do not fully contain the object. Related noun phrases from the captions (``\textbf{car} engine", ``\textbf{bus} stop") and prepositional phrases (``on \textbf{boat}", not shown due to space) are specific structured noise types that would share visual contents across images. These parts and contextual image regions are exactly what the LLM WSOD model detects as the object while our method captures the actual object with high confidence.
\input{iccv2023AuthorKit/figures/scale}

%Differences in performance might come from higher or lower prevalence of certain VAEL noise types; for example, RedCaps contains more non-literal narrative artifacts than SBUCaps. 
%However, the main takeaway is that across all datasets precision improves significantly compared to no vetting and F1 performance is competitive and better on noisier datasets (improvement in terms of precision overcomes drop in recall). 
%Even though the labels are cleaner, we may still prefer to have better recall so that we can utilize as much data as possible from these datasets. 

%VEIL$_{\text{ST}}$ and random masking the extracted label tokens in the caption. 

% \textbf{Connection to WSOD.}
% To see if precision plays a larger role and can make up for a lower F1 (compared to no vetting) we train MIST \cite{ren2020instance} on the ID test split and compare between no vetting, VEIL-ID and VEIL$_{\text{ST}}$-OOD. In Figure \ref{fig:train_subset} (right), we show that applying VEIL to unseen categories improves performance by . This shows the promise of applying VEIL across category boundaries.
% \textbf{NEED TO CHECK AFTER RESULTS FROM NO VETTING on ID ARE IN - We also used the data from this experiment to compute a correlation between ELV precision/recall, and WSOD mAP results. The only statistically significant correlation we found was for ELV precision and mAP in the OOD setting (but not for recall and mAP), confirming in yet another way the importance of studying precision of extracted labels and VAEL noise.}











% ---------- EVEN OLDER TEXT

%, as well as \cite{tang2017multiple,yang2019towards,zeng2019wsod2,ren2020instance,shen2020uwsod} which iteratively update pseudo-labels based on high-scoring proposals.

% Empirically we find that the language-agnostic noisy label selection and/or correction approaches perform better than unfiltered baselines, they still have limited improvement in object localization, as opposed to object recognition, due to the structured noise. 
%In this paper we want to understand the different types of noise or misalignments that may occur in diverse multimodal datasets on the object or caption level. We develop a taxonomy of noisy labels extracted or not extracted from captions. We find that while some of these seasons may have simple linguistic structural heuristics, others rely more on the surrounding semantic context.

%The only exception is \cite{Chen_2017_CVPR} which uses wildlife documentaries but relies on track information, which is available in video but not image datasets. 

% \textbf{Coherence analysis in language and across modalities.} Discourse structure relays syntactic information about how text is organized. In language, each span is connected in a meaningful and coherent manner to the next; \textit{how} it is connected is known as the discourse or coherence relation. Discourse relations and structure are a well studied topic
% %with competing theories 
% \cite{Mann1988RhetoricalST,webberJoshiDiscourse,wolfGibsonDiscourse}. We use parsers that follow the taxonomy of discourse relations from rhetorical structure theory \cite{Mann1988RhetoricalST}; each span serves a function in rhetoric. This concept was extended to multimodal discourse, specifically images and captions in instructional \cite{Alikhani2019CITEAC} and caption generation contexts \cite{Alikhani2020ClueCC}, but has not been used for object detection before.

%\subsection{Extracted Label Vetting Method}

%There are two variants to the visual presence detection methods. The first produces a single visual presence prediction for the entire caption and the other produces a visual presence prediction \textit{per label}. For the former, the visual presence detection method takes in some text input $t_i$ gathered from the caption $c_i$ for image $x_i$, and produces a score $r_i$. To get a visual presence classification $\hat{y_i}$, the score $r_i$ must be above some threshold. If $\hat{y_i}=1$ then the extracted labels from the caption are accepted, else rejected.

% \textbf{Descriptiveness Classifier.}
% The descriptiveness classifier first computes a frequency-based vector representation of part of speech from $c_i$. Then, this vector $v_i$ is fed as input to a logistic regression model:
% $$\text{descriptiveness\_score} = \sigma(W^Tv_i)$$
% $$\hat{y_i} = \text{descriptiveness\_score} > thresh$$
% The narrativeness score is complementary to the descriptiveness score: $$\text{narrativeness\_score}=1-\text{descriptiveness\_score}$$
% Additionally, for the descriptiveness or narrativeness classifier we set $thresh = 0.5$.
