\section{Method}
\label{sec:veil}

% MOVED EXTRACTING LABELS TO ANALYSIS SECTION

\input{iccv2023AuthorKit/figures/elavet_fig}

\textbf{Vetting labels (VEIL).} The extracted label vetting task aims to predict binary visual presence targets (present/absent)
for \textit{each} extracted label in the caption using \textbf{only} the caption context, not the corresponding image. 
We hypothesize there is enough signal in the caption to vet the most harmful label noise. This reduces the model complexity and prevents distractions from the visual modality (similar context). %justification for language only
The method is overviewed in Fig.~\ref{fig:elavet_arch}. Given a caption, WordPiece \cite{wu2016google} produces a sequence of subword tokens $C$; each token is mapped to corresponding embeddings, resulting in $e \in \mathbb{R}^{d\times C}$.
%Our model takes in a sequence of $C$ word token-level caption embeddings. 
These embeddings are passed through a pretrained language model (BERT \cite{BERT}), $h$, which includes multiple layers of multi-head self-attention over tokens in the caption to compute token-level output embeddings $v\in \mathbb{R}^{d\times C}$.
An MLP is applied to these embeddings and the output is a sequence of visual presence predictions per token, $r\in [0,1]^{C}$. 
\begin{gather}
    v = h(e) \\
    r = \sigma(W_2(\tanh(W_1v))
\end{gather}
where $W_1\in \mathbb{R}^{d\times d}$ and $W_2\in \mathbb{R}^{1\times d}$. 

Not all predictions in $r$ correspond to an extracted label, so we use a mask, $M\in [0,1]^{C}$, such that binary cross entropy loss is only applied to predictions/targets associated with the extracted labels. %are used in binary cross entropy loss.  
To train this network, the pseudo-label targets are present, $y_i = 1$, if a pretrained image-level object recognition model also predicts the %same category as the 
extracted label. 
\begin{gather}
    L_{i} = M_i\Big[y_i \log r_i + (1 - y_i) \log(1-r_i)\Big]\\
    L=\frac{1}{M^{T}M}\sum_{i=1}^{C} L_{i}
\end{gather}
While using pretrained object recognition models may appear unfair, %we believe that 
bootstrapping this knowledge to train a language model to predict token-level binary visual presence 
% prediction using only caption input 
has efficiency benefits (no image input required), can generalize to extracted labels outside of the recognition model's vocabulary 
% since visual presence is predicted on a per token level 
(see Sec. \ref{sec:expts} for generalization experiments), and is realistic for WSOD, since detection labels are more limited, whereas many recognition labels exist. 

During \emph{inference}, if an extracted label 
%has been tokenized into 
was mapped to multiple tokens (e.g. ``teddy bear''), the predicted scores are averaged to a single prediction.

%\textbf{Special token.} We test VEIL$_{\text{ST}}$ which inserts a special token {\tt [EM\_LABEL]} before each extracted label in the caption to reduce the model's reliance on category-specific cues and improve generalization to other datasets. We find that it helps only the latter.




\textbf{Weakly-supervised object detection.}
To test the ability of extracted label filtering or correction methods for weakly-supervised object detection, we train MIST \cite{ren2020instance}. MIST extends WSDDN \cite{bilen2016weakly} and OICR \cite{Tang2017MultipleID} which combine class scores for a large number of regions in the image to compute an image-level prediction (used for training).  
%to improve iterative refinement such that multiple instances are not grouped as one.
VEIL uses image-level pseudo-visual presence labels 
% training data 
from the in-the-wild datasets to train the vetting model, and we want to see how its ability to vet labels for WSOD generalizes to unseen data. Thus, we use the test splits of the in-the-wild datasets to train MIST, as they are unseen by all vetting methods. We do not evaluate the WSOD model on these in-the-wild datasets, but on disjoint datasets which have bounding boxes (PASCAL VOC and COCO). 

%\textbf{Weighted sampling.}
% \begin{gather}    
% \end{gather}

%\textbf{Implementation.}
%VEIL is implemented in PyTorch \cite{pytorch} and 
%We use a pretrained BERT encoder \cite{BERT} prior to the per-token visual presence classification layer. 
%We use MIST \cite{ren2020instance} to learn an object detection model using weak supervision from image-level labels extracted from captions. 
%We simulate a batch size of 8.
%for all experiments unless specified otherwise. 
%To address class imbalance during WSOD, 
%caused by in-the-wild datasets' long-tail distributions
%we use the complement of the sub-sampling probability from \cite{Mikolov2013DistributedRO} as weights. 
%We train under different GPU settings due to resource constraints, and use gradient accumulation for some experiments. 
%We used 4 RTX A5000 GPUs and trained for 50k iterations with a batch size of 8, or 100k iterations on 4 Quadro RTX 5000 GPUs with a batch size of 4 and gradient accumulation (parameters updated every two iterations to simulate a batch size of 8).
%We use batch size 8. %See supp for more.