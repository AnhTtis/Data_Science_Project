\section{Label Noise Analysis and Dataset}%Caption Label Noise (CLaN) Dataset}
\label{sec:analysis}

We analyze what makes large in-the-wild datasets a challenging source of labels for object detection. 

\textbf{Datasets analyzed.} 
\textbf{RedCaps} \cite{Desai2021RedCapsWI} 
consists of 12M Reddit image-text pairs collected from a curated set of subreddits with heavy visual content.
\textbf{SBUCaps} \cite{Ordonez2011Im2TextDI} consists of 1 million Flickr photos with text descriptions written by their owners.
Captions were selected if at least one prepositional phrase and 2 matches with a predefined vocabulary were found.
Conceptual Captions (\textbf{CC}) \cite{Sharma2018ConceptualCA} contains 3M image-alt-text pairs after heavy post-processing: named entities in captions were hypernymized and image-text pairs were accepted if there was an overlap between Google Cloud Vision API class predictions and the caption. 
%This motivates our exploration of VAEL noise.

\textbf{Extracted object labels.} Given a vocabulary of object classes, we extract a label for an image if there is an exact match between the object name and the corresponding caption ignoring punctuation. 
While this strategy will result in some noisy labels, it represents how labels are extracted in prior work \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019,Fang2022DataDD} due to the absence of clean annotations. Using gold standard labels (defined next), we calculate the precision of the extracted labels. %Even with CC's heavy post-processing% causes cleaner labels than the other in-the-wild datasets, 
% All of the i
% In-the-wild datasets exhibit very low extracted label precision,
% % compared to COCO. This precision 
% ranging from 0.463 for SBUCaps, 0.596 for RedCaps, to 0.737 for CC, compared to
% % all much lower than 
% 0.948 for COCO (see no vetting precision in Tab. \ref{tab:direct_eval_all}). 
In-the-wild datasets exhibit much lower extracted label precision, with SBUCaps at 0.463, RedCaps at 0.596, and CC at 0.737, in stark contrast to COCO's 0.948 (refer to Tab. \ref{tab:direct_eval_all} for no-vetting precision).






\textbf{Gold standard object labels.} We use \emph{image-level} predictions from a pretrained image recognition model to \emph{estimate} visual presence \textit{gold standard} labels (pseudo-ground-truth) because in-the-wild datasets do not have object annotations. 
We use an object recognition ensemble with the X152-C4 object-attribute model \cite{zhang2021vinvl} 
and Ultralytic  YOLOv5-XL \cite{yolov5}.
This ensemble achieves strong accuracy, 82.2\% on SBUCaps, 85.6\% on RedCaps, and 86.8\% on CC (see Appendix Sec.~\ref{appx:quality_of_pretrained_image_recognition_ensemble}: we annotate a subset to estimate  accuracy).
%Our cross-category experiments show we do not require labels for all classes.
For our analysis of visually absent extracted labels (VAEL), we sample image-caption pairs where the extracted label and gold standard label disagree.
Note we never use bounding-box pseudo labels, only image-level ones. 

\textbf{Caption Label Noise (CLaN) dataset annotations collected.} % \textbf{Noise annotations collected.}
To understand the label noise distribution, we select 100 VAEL examples per dataset (RedCaps, SBUCaps, CC) and annotate four types of information (abbreviations are underlined): 
\begin{itemize}[nolistsep,noitemsep]
    \item (Q1: Label Noise) How much of the VAEL object is present (\underline{vis}ible, \underline{part}ially visible, completely \underline{abs}ent); 
    \item (Q2: Similar Context) If the VAEL object is completely absent, is there traditionally \underline{co-occ}urring context (``boat'' and ``water'') or a semantically \underline{sim}ilar object (e.g. ``cake'' and ``bread'', ``car'' and ``truck'') is present instead; %in the image; 
    \item (Q3: Visual Defects) If the VAEL object is visible/partially visible, is the object  \underline{occl}uded, have key \underline{parts} missing, or have an \underline{atyp}ical appearance (e.g. knitted animal); and
    \item (Q4: Linguistic Indicators) What linguistic cues explain why the VAEL object is mentioned but absent, e.g. the caption discusses events or information \underline{beyond} what the image shows (see %bottom-right ex. in 
    Fig. \ref{fig:concept}), 
    % (``beyond'' in Tab.~\ref{tab:stats})
    describes the \underline{past} (``earlier that day, my \textbf{dog} peed
on a flower'') or the VAEL is: within 
% extracted label is part of 
a \underline{prep}ositional phrase and likely to describe the setting not objects (e.g. ``on a \textbf{train}''),  used in a \underline{non-lit}eral way (``\textbf{elephant} in the room''), a noun \underline{mod}ifying another noun (``car park''), a different word \underline{sense} (e.g. ``bed'' vs ``river bed''), or part of a \underline{named} entity (see 
% top-left example in 
Fig. \ref{fig:concept}). Note multiple linguistic indicators could be used to detect the absent object.
\end{itemize}

Two authors provide the annotations, with Cohen's Kappa agreements of 0.76 for Q1, 0.33 for Q2, 0.45 for Q3, and 0.58 for Q4. We calculate Cohen's Kappa for each option and 
% aggregate agreement through 
compute a weighted average for each question, with weights derived from average option counts
% between the two annotators 
across annotators and the three datasets. We compute the average disagreement as the number of disagreements divided by the number of samples annotated for each question per dataset, averaged over all datasets. The average disagreement is 25.1\% for Q2, 25.3\% for Q3, 14.6\% for Q4. When comparing similar context (``co-occ'' or ``sim'') vs ``no similar context'' 
% merging similar/co-occurring objects (vs none of them) 
for Q2 and any defects (``occl'', ``parts'', ``atyp'') vs ``no defects'' for Q3, disagreement is 28.7\% for Q2, 17.0\% for Q3. The disagreements are fairly low.
% We label the dataset Caption Label Noise, or CLaN.

In Table \ref{tab:stats}, we show what fraction of samples fall into each annotated category, excluding ``Other'', ``Unclear'' and uncommon categories. We average the distribution between the two annotators.

\textbf{Statistics: Label noise.}
We first characterize the visibility of objects flagged as VAELs by the recognition ensemble. SBUCaps has the highest rate of completely absent images (58.5\%), followed closely by RedCaps. 
SBUCaps also has the highest rate of partially visible objects (20\%). 
CC has the highest full visibility (32.8\%),
%followed by RedCaps (29.2\%) and then SBUCaps (21.5\%) where full visibility is 
defined as the object having 75\% or more visibility from a given viewpoint. 
Samples with 
%the high rate of 
absent and partially-visible objects %justifies using pseudo-ground-truth labels from the recognition ensemble; these both 
constitute poor training data for WSOD, and their high rate motivates our VEIL approach. 

\textbf{Statistics: Similar context.}
Certain images with absent objects may be more harmful than others. Prior work shows that models exploit co-occurrences between an object and its context to do recognition, but when this context is absent, performance drops \cite{Singh_Mahajan_Grauman_Lee_Feiszli_Ghadiyaram_2020}. We hypothesize that including images without the actual object %present
and with this contextual bias could hurt localization when supervising detection \textit{implicitly}. Additionally, semantically similar objects may blur decision boundaries.
Different annotators may have different references for similarity or co-occurrence frequency, but our annotators achieve fair agreement ($\kappa=0.33$). In Table \ref{tab:stats}, we find high rates of co-occurring contexts in samples with completely absent VAELs for SBUCaps (42.5\%) and CC (30.9\%).
SBUCaps and CC also have a 12-14\% rate of %semantically 
similar objects present instead of the VAEL. 
% For example, if the image contains co-occurring context (e.g. "bus stop with buildings") while the object (e.g. "bus") is absent, then the inclusion of this example could harm localization.

\textbf{Statistics: Visual defects.}
We hypothesize there may be visual defects that caused the recognition ensemble to miss fully visible objects. Here, we compute the percent of at least \textit{one} visual defect in fully or partially visible samples: 79\% for CC, 
% In CC, 79\% of fully or partially visible objects have a visual defect (not shown in Tab. \ref{tab:stats}),
87\% for SBUCaps, and 69\% for RedCaps. % over the fully or partially visible subset. 
Tab. \ref{tab:stats} has the distribution by visual defect type; this shows that atypical appearance is the most common defect for RedCaps and CC (49\% and 57.3\%). We argue atypical examples constitute poor training data for WSOD, especially when learning from scratch.
The caption context (e.g. ``acrylic illustration of the funny mouse'') may indicate the possibility of a visual defect, further motivating the VEIL design. 

\textbf{Statistics: Linguistic indicators.} Noun modifiers are frequently occurring indicators over all datasets. Prepositional phrases are significant in SBUCaps (40.5\%) and CC (31.3\%).
Need for caption context in vetting is motivated %All datasets contain many VAELs mentioned in contexts going 
by many VAELs being mentioned in contexts going beyond the image, e.g.:
``just got back from the river. friend \textbf{sank his truck pulling his \underline{boat} out}. long story short, rip this beast'' (RedCaps). 
We find prevalent structured noise (pattern to the images associated with a particular noisy label) for indicators like ``noun modifier'' and ``prepositional phrase'' 
due to high levels of occlusion and similar contexts.
