\section{Related Work}

\textbf{Vision-language datasets} 
include crowdsourced captions \cite{Young2014Flicker30K, Lin2014MicrosoftCC, huang2016visual,Krishna2016VisualGC} and alt-text written by users to aid visually impaired readers \cite{Sharma2018ConceptualCA,Changpinyo2021Conceptual1P,Radford2021LearningTV,Schuhmann2021LAION400MOD} are widely used for vision-language grounding due to abundance and high visual-text alignment.
There are also large in-the-wild datasets sourced from social media like Reddit \cite{Desai2021RedCapsWI} and user-uploaded captions for photos shared on Flickr \cite{Ordonez2011Im2TextDI}. 
We show the narrative element found in these in-the-wild datasets, captured by the linguistic cues we investigate, impact the ability to successfully train an object detection model.

\textbf{Weakly-supervised object detection} (WSOD) is a multiple-instance learning problem to train a model to localize and classify objects from image-level labels \cite{bilen2016weakly,tang2017multiple,wan2019cmil,gao2019cmidn,ren2020instance,Shao2022DeepLF}.
Cap2Det was the first work to leverage unstructured text accompanying an image for WSOD by predicting pseudo image-level labels from captions \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019, Unal2022LearningTO}.
However, Cap2Det cannot operate across novel categories as it 
directly predicts image-level labels and aims to correct  
% The text classifier in Cap2Det was only used when exact string matching produced no labels, which means it mainly corrects 
% Further, Cap2Det targets 
false negatives, %(visually present, not extracted labels)
 not visually absent extracted labels. Detic \cite{Zhou2022DetectingTC} uses weak supervision from 
ImageNet \cite{Deng2009ImageNetAL} and extracts labels from Conceptual Captions (CC) %\cite{Sharma2018ConceptualCA} 
to pretrain an open vocabulary object detection model with a CLIP classifier head. While these approaches succeed in leveraging relatively clean, crowdsourced datasets like COCO, 
Flickr30K and ImageNet, both see lower performance in training with CC \cite{Unal2022LearningTO,Zhou2022DetectingTC}. Other prior work \cite{Gao2021OpenVO} uses  
a pretrained vision-language model 
%(on 14M data) ALBEF \cite{Li2021AlignBF} 
to generate pseudo-bounding box annotations,
% COCO, Visual Genome, and SBUCaps, 
but always requires clean data (COCO), and does not explicitly study the contribution of in-the-wild datasets.

\input{iccv2023AuthorKit/sections/analysis_table}

\textbf{Vision-language pre-training for object detection.} 
Image-text grounding has been leveraged as a pretraining task for open vocabulary object detection \cite{ZSOD_3_Rahman2020ImprovedVA, ZSL_Rec_Diff_ZSOD_Rahman2020ZeroShotOD, zareian2021open, ZSOD_2_OVOD_1_gu2022openvocabulary, regionclip, Du2022LearningTP,wu2023aligning}, followed by bounding box supervision from base classes. 
Some methods distill knowledge from existing pretrained vision-language grounding models like CLIP and ALIGN \cite{Jia2021ScalingUV} to get proposals \cite{Shi2022ProposalCLIPUO} and supervision for object detection \cite{Du2022LearningTP, regionclip}; however, these do not 
% compare clean vs 
study the effect of noisy supervision in a setting without bounding box supervision. 
In contrast, we perform weakly-supervised object detection (WSOD) using noisy image-level labels from captions only. WSOD is a \textbf{distinct task} from open-vocabulary detection and has the \textbf{advantage} of not requiring expensive 
bounding boxes. % on base classes.
We focus on \textbf{rejecting labels} harmful for localization. 

\textbf{Adaptive label noise reduction in classification.}
Adaptive methods reject or correct noisy labels ad-hoc during training. These methods exploit a network's ability to learn representations of clean labels earlier in training. This assumes there are no clear visual patterns in the noisy samples corresponding to a particular corrupted label, leading to their memorization later in training \cite{Zhang2016UnderstandingDL}.
We instead show diverse real-world datasets contain naturally occurring \emph{structured} noise, where in many cases there are visual patterns to the corrupted label.
Large Loss Matters \cite{Kim2022LargeLM} is representative of such adaptive noise reduction methods and we find that it struggles with noisy labels extracted from in-the-wild captions.
