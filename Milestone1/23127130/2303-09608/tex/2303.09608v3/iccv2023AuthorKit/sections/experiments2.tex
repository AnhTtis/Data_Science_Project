\section{Experiments}
\label{sec:expts}

\input{iccv2023AuthorKit/tables/direct_eval_by_dataset.tex}
% \input{iccv2023AuthorKit/tables/indirect_evaluation.tex}
%\input{iccv2023AuthorKit/figures/cross_category}

We show the ability of VEIL to vet noisy extracted labels, remove structured noise, and outperform language-agnostic filtering and image-based filtering methods. 
We test generalization ability in VEIL through cross-dataset and cross-category experiments. Lastly, we evaluate weakly-supervised object detection settings using only noisy supervision and a combination of noisy and clean supervision.

\subsection{Experiment Details}

We use three in-the-wild image-caption datasets: SBUCaps \cite{Ordonez2011Im2TextDI}, RedCaps \cite{Desai2021RedCapsWI}, Conceptual Captions \cite{Sharma2018ConceptualCA}; and three crowdsourced datasets that fall into descriptive: COCO \cite{Lin2014MicrosoftCC}, VIST-DII \cite{huang2016visual}) and narrative: VIST-SIS \cite{huang2016visual}. 
In-the-wild 
and VIST captions are filtered using substring matching against COCO categories; this creates a subset of image-caption pairs where there is at least one match. This subset is split into 80\%-20\% train-test; see Appendix Sec. \ref{appx:vetting_dataset_details} for image-caption counts. See Sec. \ref{sec:analysis} for details on how pseudo-ground truth visual presence is produced for all datasets except COCO which has object annotations. % datasets where image-level object labels don't exist (e.g. all our datasets except COCO).
The WSOD models are trained on SBUCaps with labels vetted by different methods, and evaluated on PASCAL VOC 2007 test \cite{Everingham2010ThePV} and COCO val 2014 \cite{Lin2014MicrosoftCC}.
%The mAP metric uses $IOU=0.5$. However, when contrasting the image-level classification performance and detection performance we refer to them as Recognition maP and Detection mAP, respectively.

\subsection{Methods Compared}
Since we train and test VEIL on various datasets, we use the convention VEIL-X to signify that VEIL is trained on the \textit{train-split} of X where X is the dataset name. 
%To test the source generalization capacity of VEIL given a data source (SBUCaps), we report VEIL-Cross Dataset results which refers to the best performing VEIL using data sourced from the remaining datasets.
We group the methods we compare against into language-based, visual-based, and visual-language methods. They are category-agnostic, except for Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019} and Large Loss Matters (LLM) \cite{Kim2022LargeLM}, both of which must be applied on closed vocabulary. 
\\\textbf{No Vetting} accepts all extracted labels (\emph{recall}=1).
\\\textbf{Global CLIP and CLIP-E} use the ViT-B/32 pretrained CLIP \cite{Radford2021LearningTV} model. To enhance alignment \cite{Hessel2021CLIPScoreAR}, we add the prompt ``A photo depicts'' to the caption and calculate the cosine similarity between the image and text embeddings generated by CLIP. We train a Gaussian Mixture Model with two components on dataset-specific cosine similarity distributions. During inference, we accept image-text pairs with predicted components aligned with higher visual-caption cosine similarity.
For the ensemble variant (CLIP-E), we prepend multiple prompts to the caption
and use maximum cosine similarity.
\\\textbf{Local CLIP and CLIP-E} 
use cosine similarity between the image and the prompt ``this is a photo of a'' followed by the \textbf{extracted label}. This method directly vets the extracted label compared to GlobalCLIP which filters the entire caption. Since the caption context is ignored, this is image-conditioned.
% Only extracted labels are filtered rather than entire captions, making this image-conditioned, not image-language conditioned vetting like Global CLIP. 
Local CLIP-E ensembles prompts.
\\\textbf{Reject Large Loss.} LLM \cite{Kim2022LargeLM} is a language-agnostic adaptive noise rejection and correction method. To test its vetting ability, we simulate five epochs of WSOD training \cite{bilen2016weakly} and consider label targets with a loss exceeding the large loss threshold as ``predicted to be visually absent'' after the first epoch. LLM controls the strength of the rejection rate using the relative delta hyperparameter
% hyperparameter called the relative delta modulates the strength of the rejection rate, and is 
(0.002 in \cite{Kim2022LargeLM}); we use 0.01 and show our ablations in Appendix Sec. \ref{appx:WSOD_Implementation_Details}.
\\\textbf{Accept Descriptive.} We use a descriptiveness classifier \cite{Rai2023ImprovingLO} 
\textit{trained} to predict whether a VIST \cite{huang2016visual} caption comes from the DII (descriptive) or SIS (narrative) split. The input is a multi-label binary vector representing part of speech tags (e.g. proper noun, adjective, verb - past tense, etc) present. % in the caption. 
We accept extracted labels from captions with descriptiveness over 0.5.
\\\textbf{Reject Noun Mod.} Since an extracted label could be modifying another noun (``\underline{car} park''), a simple baseline is to reject 
an extracted label if the POS label is an adjective or is followed by a noun. 
\\\textbf{Cap2Det.} We reject a label if it is not predicted by the Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019} classifier.


\input{iccv2023AuthorKit/sections/vetting_analysis}


\subsection{Extracted Label Vetting Evaluation}

\textbf{VEIL selects cleaner labels 
compared to no vetting and other methods, even when evaluated on datasets differing from the training dataset (e.g. trained on Redcaps-Train and evaluated on SBUCaps-Test).}
Tab.~\ref{tab:direct_eval_f1} shows the F1 score which is the harmonic mean of the vetting precision and recall (shown separately in Appendix Sec. \ref{appx:vetting_prec_rec}).  
Most language-based methods, except Accept Descriptive, improve or maintain the F1 score of No Vetting, even though it has perfect recall. Rule-based methods and Cap2Det perform strongly but are outperformed by both VEIL-Same Dataset (trained and tested on the same dataset) and VEIL-Cross Dataset (trained on a different dataset than that shown in the column; we show the best cross-dataset result in this table; see Appendix Sec. \ref{appx:cross_dataset_ablations} for all cross-dataset results). VEIL-Cross Dataset outperforms other language-based approaches, showing VEIL's generalization potential, except on COCO where Cap2Det does slightly better.
Image-and-language-conditioned approaches (Global CLIP/CLIP-E) make label decisions based on the overall caption, so if part of the caption is visually absent, the alignment could be low.
%certain language can affect the alignment even if the object is actually visually present. 
%Table \ref{tab:direct_eval} shows these methods obtain low F1 scores.
Among image-based approaches for label vetting, Local CLIP benefits significantly from using an ensemble of prompts compared to Global CLIP; ensembling prompts improves zero-shot image recognition in prior work \cite{Radford2021LearningTV}. 
Reject Large Loss has the strongest F1 score among the image-based methods, but is worse than VEIL.



\textbf{Using CLaN, we find that VEIL is stronger than CLIP-based vetting at rejecting different forms of label noise.
Captions alone contain cues about noise.}
We hypothesize that LocalCLIP-E would do well at vetting VAELs explained by linguistic cues like ``non-literal'' and ``beyond the image'' as they are likely to have low image-caption cosine similarity.
We also hypothesize that VEIL would do better than LocalCLIP-E at vetting VAELs that are noun modifiers or in prepositional phrases, which can be easily picked up from the caption.
Further, visual noise in the form of similar context but absent/partially visible object (Q2 in CLaN), could be detected by VEIL from linguistic cues like noun modifiers, prepositional phrases, or caption context implying different word sense. However, LocalCLIP-E may be oblivious to the context differing from the VAEL category. 
We evaluate these hypotheses on the CLaN dataset in Tab.~\ref{tab:vetting_analysis}. We omit ``visible'' VAEL samples as these may be pseudo-label errors and the ``past'' linguistic indicator due to too few samples. 
We find VEIL vets truly absent objects for SBUCaps much better than LocalCLIP-E, and comparably for RedCaps or CC. It vets partially visible objects better than LocalCLIP-E by a significant margin; these can be harmful in WSOD which is already prone to part domination \cite{ren2020instance}.
VEIL also recognizes that similar context rather than the actual VAEL category, are present.
VEIL performs better at vetting visible objects that have visual defects
which can be mentioned in caption context 
(``acryllic illustration of \underline{dog}'').
As expected, we find that for all datasets, VEIL vets VAELs from prepositional phrases better than LocalCLIP-E, and noun modifiers for SBUCaps and RedCaps. LocalCLIP-E does better on ``beyond the image'' and non-literal VAELs except on SBUCaps where VEIL excels.

\input{iccv2023AuthorKit/tables/cross_dataset_CLIP.tex}

\textbf{VEIL generalizes across training sources and is complementary to CLIP-based vetting.}
We train VEIL on one dataset (or multiple) and evaluate on an unseen target. We find that combining multiple sources improves precision (Tab.~\ref{tab:cross_dataset_direct_eval}). 
%To better utilize caption context, we test VEIL$_{\text{ST}}$ which predicts visual presence using a special token {\tt [EM\_LABEL]}. We find that this improves F1 performance. Lastly, 
We also try ensembling by averaging predictions between LocalCLIP-E and VEIL-Cross Dataset and find that both are complementary; that is, the ensemble has better precision and recall compared to VEIL-Cross Dataset or LocalCLIP-E alone. 
There is still a significant gap between VEIL-Same Dataset and even the ensembled model in terms of precision and F1. We leave improving source generalizability to future research.

\input{iccv2023AuthorKit/tables/cross_category.tex}

\textbf{VEIL produces cleaner labels even on unseen object categories.}
We define an in-domain category set (ID) of 20 randomly picked categories from COCO \cite{Lin2014MicrosoftCC}, and an out-of-domain category set (OOD) consisting of the 60 remaining categories. We restrict the labels using these limited category sets and create two train subsets, ID and OOD from SBUCaps \textit{train} and one ID test subset from SBUCaps \textit{test}. 
We find that transferring VEIL-OOD to unseen categories improves F1 score compared to no vetting as shown in Table \ref{tab:cross_category}. Additionally, VEIL-OOD has higher precision (0.59) compared to LocalCLIP-E (0.53) which was trained on millions of image-captions. This indicates an ability to reject false positive labels from unseen classes.
We hypothesize training on more categories could improve category generalization, but leave further experiments to future research.

\textbf{Why can VEIL generalize? }%Generalization.} These two generalization experiments raise questions on how VEIL can generalize across training sources or out-of-distribution scenarios. 
 We hypothesize that linguistic indicators explaining the visually absent label can be found in captions across datasets and \textit{can} be independent of the object category: past tense, prepositional phrase, noun modifier, and named entities are all represented within BERT \cite{BERT}, which we finetune in VEIL. To evaluate the effect of linguistic indicators in generalization, we compute the \emph{distance} between the linguistic indicator distributions for each dataset pair in CLaN. We compute the correlation between the \emph{distance} and cross-dataset performance. %(all results shown in Appendix Sec. \ref{appx:cross_dataset_ablations})
 We observe a moderately strong negative Pearson correlation ($\rho = -0.62$). % between linguistic indicator distribution distance and cross-dataset performance.
% ; this means that similar (low distance) linguistic indicator distributions correlate with higher cross-dataset performance. 
This indicates that VEIL implicitly learns associations between linguistic indicators 
 and VAELs which can help in generalizing.
%, without being instructed to learn them.

\input{iccv2023AuthorKit/tables/det_vs_recognition}

\subsection{Impact on Weakly-Sup. Object Detection}

%\textbf{Comparison between High Performing Methods in Extracted Label Vetting}
We select the most promising vetting methods from the previous section and use them to vet labels from an in-the-wild dataset's, SBUCaps, unseen (\textit{test}) split and then train WSOD models using the vetted labels. Then, these WSOD models are evaluated on detection benchmarks like VOC-07 and COCO-14.
We evaluate two different VEIL methods, VEIL-SBUCaps and VEIL% $_{\text{ST}}$
-RedCaps,CC  
% Both vet labels from SBUCaps and use them to train WSOD, but the vetting method is trained on either (1) SBUCaps or (2) RedCaps and CC captions.
to demonstrate the generalizability of VEIL on WSOD.
Note that we relax Large Loss Matters \cite{Kim2022LargeLM} to \textit{correct} visually absent extracted labels, in addition to unmentioned but present objects (false negatives).
After vetting, we remove any images without labels and since category distribution follows a long-tail distribution, we apply weighted sampling \cite{Mikolov2013DistributedRO}.
We train MIST \cite{ren2020instance} for 50K iter. with batch size 8. %for each method. 

\textbf{VEIL vetting leads to better detection and recognition capabilities than vetting through CLIP, or an adaptive label noise correction method (Large Loss Matters).} We find that VEIL-SBUCaps performs the best as shown in Tab.~\ref{tab:det_v_recognition}.
In particular, it boosts the detection performance of No Vetting by 9.3\% absolute and 29.8\% relative gain (40.5/31.2\% mAP) on VOC-07 and by 35\% relative gain (10.4/7.7\% mAP) on COCO.
Interestingly, VEIL-SBUCaps and VEIL-Redcaps,CC have a similar performance improvement, despite VEIL-Redcaps,CC (best VEIL cross-dataset result on SBUCaps) having poorer performance than Local CLIP-E in Tab.~\ref{tab:cross_dataset_direct_eval}. 

\textbf{VEIL generalizes from its bootstrapped data}. Directly using predictions from the pretrained object recognition model (used to produce visual presence targets for VEIL at the image level) to vet (GT* method in Tab.~\ref{tab:det_v_recognition}) 
performs worse than VEIL % in both detection and recognition \
as shown by 40.5 mAP vs 40.0 mAP on VOC 
% for detection and 74.3 vs 69.0 on VOC for recognition. This also extends to COCO, where we observe 
and 10.4 mAP vs 9.2 mAP on COCO. 
% VEIL generalizes from its bootstrapped data: w
We speculate that learning to identify label noise is an easier task than categorizing different objects; furthermore, image recognition models could still select samples that might be harmful for learning localization (similar contexts, occlusion, etc). % so this could generalize better to unseen data
% as it is learning essential features akin to knowledge distillation from a teacher model 
 The image recognition model may also wrongly reject clean labels. %These differences in predictions would be due to the difference in input (caption rather than image), where possibly, VEIL needs less data to understand visual presence because captions will have less variation than natural images.  
 We leave further exploration to future research.

\textbf{Structured noise negatively impacts localization.} Using the CLaN dataset, we observe
%a number of examples with structured noise. 
one type of structured noise found from extracting labels from prepositional phrases, specifically where images were taken inside vehicles. We hypothesize such structured noise would have significant impact on localization for the vehicle objects. We use CorLoc to estimate the localization ability on vehicles in VOC-07 (``aeroplane'', ``bicycle'', ``boat'', ``car'', ``bus'', ``motorbike'', ``train''). We observe a CorLoc of 60.2\% and 54.1\% for VEIL-SBUCaps and LocalCLIP-E, respectively. 
%For another supercategory, animal, we observe a smaller improvement (57.9 vs 56.8) from LocalCLIP-E and VEIL-SBUCaps. 
This shows structured noise can have a strong impact on localization.

%\input{iccv2023AuthorKit/figures/scale}



\input{iccv2023AuthorKit/tables/semi_supervised}

\textbf{Naively mixing clean and noisy samples without vetting for WSOD leads to worse performance than only using clean samples. Vetting
% and weighted sampling 
in-the-wild samples (noisy) with VEIL is essential to improving performance.} We study how vetting impacts a setting where labels are drawn from both annotated image-level labels from 5K VOC-07 train-val \cite{Everingham2010ThePV} (clean) and 50K in-the-wild SBUCaps \cite{Ordonez2011Im2TextDI} captions (noisy). In Tab.~\ref{tab:mixed_supervision} we observe that naively adding noisy supervision to clean supervision actually hurts performance  %(-3.2\%) 
compared to only using clean supervision. After vetting the labels extracted from SBUCaps  \cite{Ordonez2011Im2TextDI} using VEIL-SBUCaps, we observe that the model sees a 17.9\% relative improvement (51.31/43.48\% mAP) compared to using only clean supervision from VOC-07. We see further improvements when applying weighted sampling (WS) to the added, class-imbalanced data (54.76/51.31\% mAP).

\textbf{VEIL improves WSOD performance even at scale.} 
We sampled the held-out RedCaps dataset in increments of 50K samples up to a total of 200K samples. For each scale, we train two WSOD models with weighted sampling using the unfiltered samples and those vetted with VEIL-SBUCaps,CC. 
The mAP at 50K, 100K, 150K, and 200K samples is 4.2, 10.7, 12.0, 12.9 with vetting and 1.9, 8.2, 10.6, 10.4 without vetting.
The non-vetted model's performance declines after 150K samples. 
% Even when VEIL is trained on other datasets, this
% This indicates vetting can adapt to scale better even when VEIL is trained on other datasets. The 
This trend suggests that vetting will continue outperforming no-vetting when dataset sizes increase.

% \input{iccv2023AuthorKit/figures/train_subset_figures.tex}




