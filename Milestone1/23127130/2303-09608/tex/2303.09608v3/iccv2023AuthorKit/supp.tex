%We provide supplemental materials to our main text. 

In Section \ref{appx:quality_of_pretrained_image_recognition_ensemble}, we evaluate the quality of the pretrained image recognition ensemble and in Section \ref{appx:vetting_dataset_details}, we present additional dataset details such as counts. In Section \ref{sec:expts} from the main text, we provided only vetting F1 scores in Table \ref{tab:direct_eval_f1} over multiple methods, so in Section \ref{appx:vetting_prec_rec} we provide a detailed table of the vetting precision and recall for the same methods. Furthermore in Section \ref{appx:cross_dataset_ablations}, we show more comprehensive cross-dataset ablations, such as adding more training datasets and training with a special token. 

We discuss our hyperparameter selection for WSOD in further detail in Section \ref{appx:WSOD_Implementation_Details} and show additional metrics of the WSOD models on the COCO-14 benchmark presented in the main text in Section \ref{appx:wsod_benchmarking_on_additional_coco_metrics}.

Finally in Section \ref{appx:qual_examples}, we showcase the vetting ability of VEIL in comparison to other approaches through qualitative results, along with additional examples from the WSOD models trained using vetted training data.
% \input{iccv2023AuthorKit/sections/data_appendix}


\subsection{Quality of Pretrained Image Recognition Ensemble}
\label{appx:quality_of_pretrained_image_recognition_ensemble}

\input{iccv2023AuthorKit/tables/supp/ensemble_detector_performance}

Since we used vision-language datasets without any object annotations, we have no way of knowing whether an object mentioned in the caption is present in the image. To keep our method scalable and datasets large, we used object predictions from pretrained image recognition models to produce visual presence pseudo labels for extracted labels. We test the VinVL detector \cite{zhang2021vinvl} and YOLOv5 detector \cite{yolov5}, and their ensemble (aggregating predictions) on COCO-14 Image Recognition in Table \ref{tab:coco_ensemble} and a visual presence annotated subset in Table \ref{in_the_wild_ens}. For the latter, per dataset we annotated the visual presence of 50 extracted labels from unique images for each category. We used the following randomly selected VOC \cite{Everingham2010ThePV} categories: elephant, truck, cake, bus, and cow. We found that while the ensemble variant and the VinVL detector are worse than YOLOv5 in image recognition on a common benchmark, COCO-14, the ensemble performs better than the single models on visual presence. Since this is the task we aim to do, we select the ensemble model to generate visual presence targets. Additionally, these results indicate there is still significant noise in using these models to generate pseudo labels, so using these pretrained image recognition models is not the same quality as human annotations. Despite this, VEIL still successfully harnesses these noisy targets to reason about visual presence from captions.

\subsection{Vetting Dataset Details}
\label{appx:vetting_dataset_details}
\input{iccv2023AuthorKit/tables/data_counts}
While the overall image-text pairs are 12M pairs for RedCaps, 3M pairs for CC, 1M for SBUCaps, 500K pairs for COCO, 40K and 60K pairs for VIST-DII and VIST-SIS, respectively, after extracting labels using exact match with COCO categories, there are a number of captions which don't have any matches. We filter out those captions. In Table \ref{tab:datacount} we provide counts after filtering for both vetting train and test splits of each dataset.


\begin{table}[t]
    \centering
    \begin{tabular}{c|c}
        Relative Delta & Pascal VOC-07 $\text{mAP}_{50}$  \\\hline
         0.002 & 28.25 \\
         0.01 & 30.93\\
         0.05 & 28.11 \\
    \end{tabular}
    \caption{Relative delta hyperparameter ablation}
    \label{tab:rel_delta}
\end{table}

\input{iccv2023AuthorKit/figures/supp/vetting_quals}



\subsection{Vetting Precision/Recall}
\label{appx:vetting_prec_rec}

Table \ref{tab:direct_eval_f1} in the main text showed the F1 on the extracted label vetting task, from twelve methods. In Table \ref{tab:direct_eval_all} here, we separately show Precision and Recall on the same task.

\input{iccv2023AuthorKit/old_files/direct_eval_table_backup}

\subsection{Cross-Dataset Ablations}
\label{appx:cross_dataset_ablations}

\input{iccv2023AuthorKit/tables/supp/cross_dataset_prec_rec}
\input{iccv2023AuthorKit/tables/supp/cross_dataset_f1}


We show results over all the cross-dataset settings we evaluated in Table \ref{tab:cross_dataset_prec_rec}. Notably, this shows that precision in the cross-dataset setting is always better than no vetting except on COCO which already has high precision and differs in composition (more descriptive) compared to the other datasets.


\textbf{Combining multiple datasets.} We find that VEIL is able to leverage additional datasets to an extent. For example, combining SBUCaps and CC leads to significant improvements (7-16\% relative) in F1 as shown in Table \ref{tab:cross_dataset_f1} and, combining SBUCaps and Redcaps in training improves performance on both validation sets. When combining all datasets, only the non-in-the-wild datasets see an improved performance. 

\textbf{Using special token.} We test VEIL$_{\text{ST}}$ which inserts a special token {\tt [EM\_LABEL]} before each extracted label in the caption to reduce the model's reliance on category-specific cues and improve generalization to other datasets. We find that using VEIL w/ ST on average improves F1 by 1 pt compared to just VEIL when transferring to other datasets. This comes at a tradeoff to the performance on the same dataset; however, CC w/ ST improves performance on all datasets.

\begin{table*}[]
    \centering
\begin{tabular}{c|ccc|ccc} \hline
& \multicolumn{3}{c|}{ mAP, IoU }& \multicolumn{3}{c}{mAP, Area} \\\hline
& {0.5:0.95} & 0.5 & 0.75 & S & M & L\\ \hline
GT* & 4.19  & 9.17 & 3.40 & 1.10 & 4.34 & 6.76 \\ \hline
No Vetting & 3.24 & 7.70 & 2.37 & \underline{1.06} & 4.00 & 5.08 \\
Large Loss \cite{Kim2022LargeLM} & 3.11 & 7.54 & 2.15 & 0.92 & 3.80 & 4.88 \\
LocalCLIP-E \cite{Radford2021LearningTV}& 3.66 & 7.77 & 3.08 & 0.79 & 3.96 & 5.96 \\
VEIL$_{\text{ST}}$-R,CC & \underline{3.90} & \underline{8.60} & \underline{3.14} & 0.93 & \underline{4.25} & \underline{6.28} \\
VEIL-SBUCaps & \textbf{4.89} & \textbf{10.37} & \textbf{4.20} & \textbf{1.26} & \textbf{5.24} & \textbf{7.53} \\\hline
\end{tabular}
    \caption{COCO-14 benchmark for WSOD models trained with various vetting methods. (GT*) directly vets labels using the pretrained object detectors which were used to train VEIL. Bold indicates best performance in each column and underline indicates second best result in the column.}
    \label{tab:coco_all_metrics}
\end{table*}

\subsection{WSOD Implementation Details}
\label{appx:WSOD_Implementation_Details}

We used 4 RTX A5000 GPUs and trained for 50k iterations with a batch size of 8, or 100k iterations on 4 Quadro RTX 5000 GPUs with a batch size of 4 and gradient accumulation (parameters updated every two iterations to simulate a batch size of 8).

\textbf{Learning Rates.} We trained four models without vetting on SBUCaps with learning rates from `1e-5' till `1e-2', for each order of magnitude, and observed that the model trained with a learning rate of `1e-2' had substantially better Pascal VOC-07 detection performance. We used this learning rate for all the WSOD models trained on SBUCaps. We applied a similar learning rate selection method for WSOD models trained on RedCaps, except we tested over every half order of magnitude and found that `5e-5' was optimal when training on RedCaps. 

% \subsection{Large Loss Matters Hyperparameter}


\textbf{Relative Delta.} In Large Loss Matters (LLM) \cite{Kim2022LargeLM}, relative delta controls how fast the rejection rate will increase over training. To find the best relative delta, we tested over three initializations, with $rel\_delta=0.002$ as the setting recommended in \cite{Kim2022LargeLM}. We used the best result in Table \ref{tab:rel_delta} when reporting results in the main paper.
\subsection{WSOD Benchmarking on Additional COCO Metrics}
\label{appx:wsod_benchmarking_on_additional_coco_metrics}

In our main text, we compared the average precision of the model across all the classes and all the IoU (Intersection over Union) thresholds from 0.5 to 0.95. We show mAP at specific thresholds 0.5 and 0.75 in Table \ref{tab:coco_all_metrics}. We see that cross-dataset VEIL vetting performs relatively 32\% better than no vetting in a stricter IoU (0.75). The mAP metric can be further broken down by area sizes of ground truth bounding boxes, which is denoted by S, M, and L. VEIL-based vetting outperforms the rest in Medium (6\% better than best non-VEIL vetting) and Large objects (5\% better than best non-VEIL vetting); while VEIL-Same Dataset still performs best on small objects, VEIL-Cross Dataset performs slightly worse than no vetting.

% \subsection{Weighted Sampling Hyperparameter}
% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%         Weighted Sampling & Pascal VOC-07 mAP_{50}  \\
%          0.002 & 28.25 \\
%          0.01 & 30.93\\
%          0.05 & 28.11 \\
%     \end{tabular}
%     \caption{Relative delta hyperparameter ablation}
%     \label{tab:rel_delta}
% \end{table}
% \subsection{Large Loss Matters Hyperparameter}
% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%         Relative Delta & Pascal VOC-07 mAP_{50}  \\
%          0.002 & 28.25 \\
%          0.01 & 30.93\\
%          0.05 & 28.11 \\
%     \end{tabular}
%     \caption{Relative delta hyperparameter ablation}
%     \label{tab:rel_delta}
% \end{table}
\subsection{Additional Qualitative Results}
\label{appx:qual_examples}

\textbf{Vetting Qualitative Examples.} Using annotations from CLaN, we provide qualitative examples comparing the vetting capability of methods on VAELs with common linguistic indicators (prepositional phrase, different word sense, non-literal) found in RedCaps in Figure \ref{fig:vetting_quals}.

\textbf{WSOD Qualitative Examples.} In Figure \ref{fig:structured_qual}, we present further qualitative evidence on the impact of different vetting methods on weakly supervised object detection. There are varying degrees of part and contextual bias from all methods; however, No Vetting has the most pronounced part domination and context bias as shown by its detection of bicycle wheels and car doors (top two rows), and misidentifying a child as a chair (bottom row) and detections covering both boat and water. Both VEIL methods outperform the rest of the models in detecting smaller objects (see first two rows). LocalCLIP-E misses smaller objects in the background (first two rows) and also has part domination (bicycle).

\input{iccv2023AuthorKit/figures/supp/wsod_quals}


