\section{Experiments}
We evaluate ELaVet under the visual presence binary classification task. To evaluate label quality in this task across multiple vision-language datasets without ground truth image-level labels we compute pseudo-image labels for each image from an ensemble of VINVL and YoloV5 pretrained object detectors. We also test ELaVet as a label noise filtering method for weakly supervised object detection. We mainly benchmark the resulting object detector on PASCAL VOC 2007.

\textbf{ELaVet Training.} For each dataset mentioned in section \ref{}, we filter out any caption without a COCO \cite{} object mention, and then save  80-20 train-test splits. For each train split, we train a separate ELaVet model to predict visual presence/absence for each object mention. Since we don't have ground truth object category available for images from SBUCaps \cite{Ordonez2011Im2TextDI}, RedCaps \cite{Desai2021RedCapsWI}, Conceptual Captions \cite{Sharma2018ConceptualCA}, and VIST \cite{huang2016visual}, we use pseudo-labels from an ensemble of pretrained object detectors. We use YOLOv5 and VinVL's object detection model as we observed higher precision on visual presence classification. We evaluated this on a small 250 subset of hand-annotated visual presence/absence of certain objects (elephant, truck, cake, bus, cow) for each of the following datasets: SBUCaps, RedCaps, and Conceptual Captions. We found that an ensemble between YOLOv5 and VinVL had better performance across these three datasets, so we used these object detectors as ground truth for visual presence (if an object mention is also predicted to be in the image by the object detector).
 
\textbf{Weakly Supervised Object Detection (WSOD) and Evaluation.} We use the test split of SBUCaps as the train data for weakly supervised object detection, because the in-domain ELaVET is trained on the train split of SBUCaps. All noise filtering approaches are integrated as a preprocessing filtering step and the noise correction approaches are integrated to training. We benchmark on Pascal VOC 2007 \cite{everingham_2010} and COCO \cite{Lin2014MicrosoftCC} and use the mAP metric to evaluate both classification and localization ability of the WSOD models. 

We simulate a batch size of 8 for all experiments unless specified otherwise. We trained under different GPU settings (4 TK GPUs, with a batch size of 8 or 4 TK GPUs with a batch size of 4 and gradient accumulation which means that we updated parameters every two iterations simulating a batch size of 8) due to resource availability, and used gradient accumulation for some experiments . %We find that gradient accumulation makes virtually no difference to model performance.

%\subsection{Descriptiveness Score Predictor}



\subsection{Direct evaluation of cleanliness}
\input{iccv2023AuthorKit/tables/direct_eval_by_dataset.tex}
We have the following baselines:\\
\textbf{No filtering.} We apply no filtering technique to the image-caption pairs.\\
\textbf{CLIP-based methods.} We use the ViT-B/32 pretrained CLIP \cite{Radford2021LearningTV} image and text model for these methods. We prepend the entire caption with the following prompt, "A photo depicts" as recommended by CLIPScore \cite{Hessel2021CLIPScoreAR} and then compute the cosine similarity between the image and text embeddings outputted by CLIP. On the train datasets, we train a Gaussian Mixture model with two components. The cluster containing higher scoring image-text pairs is assumed to correspond to higher visual-text alignment. This method is what we call GlobalCLIP, and image-text pairs that are predicted to be more visually aligned are selected. For LocalCLIP, we follow a similar process except instead of computing the cosine similarity between an image and caption, we take the cosine similarity between an image and the prompt ``this is a photo of a" followed by the extracted label. LocalCLIP filters extracted labels whereas GlobalCLIP filters based on the caption, even if there are extracted labels that are visually present. For the ensemble variant, we use a variety of prompts, and prepend each to the caption or extracted label. Then for each image and text (the prompt and caption/extracted label), we calculate the cosine similarity between the image-text CLIP embeddings and use the highest scoring caption.
\\\textbf{Descriptiveness and Narrativeness.} If the text has a descriptiveness score higher than 0.5, the descriptiveness based method selects the caption. However, the narrativeness method does the opposite.
\\\textbf{Rule Based Methods.} These rule based methods are motivated by the fact that a category could be mentioned in a noun phrase (``\textbf{car} park"). The first noun modifier based filters an extracted label if the part-of-speech label is an adjective or is followed directly after by a noun. The second noun modifier rule uses the first rule and also filters if the extracted label is not associated with a noun POS tag.
\\\textbf{LLM Simulation.} LLM \cite{Kim2022LargeLM} is language-agnostic dynamic noisy label selection and correction method. For the purposes of measuring its ability to detect VPELs, we simulate five epochs of WSOD training \cite{bilen2016weakly} and assume any label exceeding the large loss threshold specified in \cite{Kim2022LargeLM} as ``predicted to be visually absent" after the first epoch. 

For most methods there is a significant improvement in precision (except for COCO which is a fairly clean dataset), except selecting based on Descriptiveness or Narrativeness (only one can do better). The performance of the in-domain alignment model is much stronger in terms of precision and recall than the other noise filtering methods on most of the datasets. Additionally, AM Cross-Dataset has better F1 scores compared to CLIP despite not using additional information - image and trained on less than 0.5\% of the data. 

In terms of text only approaches (excluding LLM and CLIP-based methods), both alignment models do better than no filtering and the descriptiveness-based and rule based methods in terms of precision and recall. Cap2Det still achieves better results than no filtering despite being trained on COCO Captions which follow a different input distribution (mainly descriptive) and noise distribution (not as noisy). However, the precision not as strong as AM Cross-Dataset. 

Language conditioned VPEL selection approaches do a better job at improving the quality of samples than an large loss matters. However, LLM has fairly strong recall and at times better precision than non-AM language condiitoned methods. LLM seems to have the least improvement in precision on SBUCaps. Local CLIP-E does better than LLM in terms of precision.

The significant drop in recall between the first noun modifier rule and the second (filtering based on whether the object is a noun) shows that there are enough instances where an object mention might be labeled as not a noun but is still present in the image. This might be due to mislabeling in the POS tag prediction due to domain shifts in these captions. However, on more descriptive texts such as CC, COCO, and VIST-DII there's still a drop in recall, so perhaps its not just due to domain shifts. 

\subsubsection{Knowledge Sharing capability of the Alignment Model}
\textbf{What is the cross-dataset generalization ability of the alignment model?}
From the table there are improvements in terms of precision in 4/5 datasets, and improvements in terms of F1 score in 3/5 compared to no filtering. There may be a few reasons why the model might not generalize to a different dataset, for example difference in data distribution, sentence structure, noise types (prevalence of noun-phrase type errors in dataset - more linguistic structure context needed over non-literal errors - more semantic context needed), or different category distributions. 
\input{iccv2023AuthorKit/tables/cross_dataset_CLIP.tex}
We first investigate applying different levels of masking and predicting based on special token rather than the output embedding associated with the object mention directly to learn more general noise patterns applicable to different objects. We also include CLIP and try to reach the cross-domain performance of CLIP. In Table \ref{}, the AM performance slightly improves when applying special tokens however not as much as CLIP. Interestingly, ensembling CLIP and AM Cross-Dataset produces better precision and recall. This is because TK. CLIP seems to do poorly in cases where ``", however the alignment model predicts a lower score ``". My hypothesis is related to object sense.

\textbf{What is the cross-label generalization ability of the alignment model?}
\input{iccv2023AuthorKit/tables/cross_category.tex}
\begin{figure}
    \centering
    \includegraphics[scale=0.40]{iccv2023AuthorKit/figures/images/cross_category_results.png}
    \caption{The star in red represents the mean.}
    \label{fig:cross_cat}
\end{figure}
We can see that in Table 7, while the precision improves over unfiltered, the recall is fairly low. The special token variant of our approach improves recall significantly, however a gap remains between unfiltered and ID in terms of F1. It appears that the OOD can identify when a label is noisy, however is too strict and object dependent, at least when trained on only 20 object classes.

\subsection{Effect on WSOD}

\textbf{How do all the language conditioned filtering compare against one another?}


\textbf{How do all the filtering approaches compare against adaptive noise correction approaches?}


\input{iccv2023AuthorKit/tables/indirect_evaluation.tex}

% First we establish a baseline by looking at performance when \textbf{No Vetting} is applied. Language-conditioned approaches take the caption as input, and produce a score that is used for selection: descriptiveness score, narrativeness score, rule based variants, Cap2Det, and VEIL. Table \ref{tab:table2_label} shows that precision improves across most of these methods. The exceptions being descriptiveness score predictor (and its complement, narrativeness) which seem very weakly or unrelated to the visual presence of labels, however, descriptiveness improves precision in all the descriptive leaning  datasets (CC, COCO, VIST, VIST-DII). This can be explained by recall significantly dropping on predominantly narrative style datasets (SBUCaps, RedCaps), which means that even though captions may be in a narrative-like style it doesn't mean that we should reject its contents. Similarly, even though the rule-based methods significantly improve precision, they suffer in recall due to sometimes the object mention in the noun phrase might be implied to be present (show example: TK). Seeing the drop in recall and F1, it appears that the second rule is too strict, as there may be situations where an object mention might not be tagged as a noun due to odd sentence structure or ?? (TK find examples). Still, given that the precision is still low, a significant number of visually absent extracted labels fall under other categories that are not easily captured by rules (non-literal, narrative artifact). Cap2Det despite being trained on COCO, still transfers and slightly boosts the precision of labels in out-of-domain settings like SBUCaps, RedCaps, and Conceptual Captions. Unsurprisingly, VEIL Same Dataset beats all methods in the web-sourced datasets when it comes to precision and F1, indicating that despite differences in recall, overall there's a much cleaner pool of labels without rejecting too many. Furthermore, a perfect fit between the ground truth and VEIL's predictions would be undesirable as it would be no better than using an object detector predictions as vetting. It's better to learn general properties of noisy labels because even if they may appear in the data, maybe the example would be difficult as it might not be the main focus. We can see that VEIL Cross Dataset is stronger than any of the language-conditioned approaches in terms of precision and F1, indicating potential for generalization. We will explore this more concretely in the next section.
