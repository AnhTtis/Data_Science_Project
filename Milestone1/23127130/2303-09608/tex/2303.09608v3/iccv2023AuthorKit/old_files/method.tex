\section{Methods}
\subsection{Extracting Labels}
\label{sec:extracting}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.30]{iccv2023AuthorKit/figures/images/wsod_model_diagram.drawio.png}
    \caption{Diagram explaining global and local view considerations while extracting labels from captions. The local view classifier takes \textbf{a} feature among: gerund, verb base form, past tense, past participle, non-3rd person singular present, 3rd person singular present, nucleus, satellite, subject, object.}
    \label{fig:global_local}
\end{figure}

\textbf{DII/SIS Classifier}
In our previous analysis, we observed that linguistic features like part of speech and discourse relations like RST \cite{Mann1988RhetoricalST} 
and CLUE \cite{Alikhani_Stone_2019} 
are distinctive between descriptive and narrative captions. We train a logistic regression classifier to predict the %source of captions (i.e. DII or SIS), or specifically the 
descriptive-ness of the caption. 
%Details of how each caption is tagged with linguistic features is mentioned in Section 3.
%We extract Part-of-Speech tags using SpaCy, a natural language processing library \cite{spacy2}. We extract RST discourse relations in two stage approach. First, we segment the caption into elementary discourse units using the method described in Li et al. \cite{li_segbot} and then use an off-the-shelf model \cite{wang-etal-2017-two} to produce an RST diagram. Discourse relations corresponding to each caption are then extracted from the RST diagram.

\noindent \textbf{Rule Based Classifier}
The global view method, described above, filters out entire captions. The local view method, on the other hand, chooses to extract labels based a lexical match between the selected categories and a five-word window of a particular linguistic feature in a caption. We illustrate this in Fig.~\ref{fig:global_local}. For nucleus/satellite local view classifiers, lexical match is directly applied to the span associated with nucleus or satellite, rather than a 5-word window.

\subsection{Weakly Supervised Object Detection}
\label{sec:wsod}

We follow prior literature \cite{bilen2016weakly,Ye_Zhang_Kovashka_Li_Qin_Berent_2019} to train a network to predict the labels identified in Sec.~\ref{sec:extracting}.
A convolutional neural network base encoder $h(x_i)$ is used to extract a feature map for an image, $x_i$. We use VGG-16 \cite{Liu2015VeryDC} with the fully connected layers removed as our base encoder, initialized with pre-trained ImageNet \cite{Deng2009ImageNetAL} weights. Region of interest (ROI) pooling \cite{Girshick2015FastR} is then applied to the feature map and the regions of interest $R_i \in \mathbb{R}^{4\times M}$ to generate a feature embedding $\phi(x_i)_m$ for each region: 
$\phi(x_i) = ROIPool(h(x_i), R_i)$.
We initialize two parallel fully-connected layers $f_c$ and $f_d$ whose outputs will be normalized to give a classification score, the probability that an object $c$ is present in that region, and detection score, the probability that $R_{i,m}$ contributes to the image-level class prediction, respectively. 
\begin{equation}
    p^{cls}_{m, c} = \sigma{(f_c(\phi(x_i)_m))}, ~~~ p^{det}_{m, c }=\frac{\exp\Big(f_d(\phi(x_i)_m)\Big)}{\sum_{j=1}^{M} \exp\Big(f_d(\phi(x_i)_j)\Big)}
\end{equation}

The class and detection predictions for each region are multiplied and summed over $M$ proposals to produce one image-level class prediction vector.
\begin{equation}
    \hat{p_c}=\sigma\Bigg(\sum_{m=1}^{M} p_{m,c}^{det} o_{m,c}^{cls}\Bigg)    
\end{equation}  

To train, we apply the multiple instance detection loss to the extracted image-level label and the predicted image-level label \cite{bilen2016weakly}:
\begin{equation}
    L_{mid}=\frac{1}{C}\sum_{c=1}^{C}\Big[y_c \log \hat{p_c} + (1 - y_c) \log(1-\hat{p_c})\Big]
\end{equation}

