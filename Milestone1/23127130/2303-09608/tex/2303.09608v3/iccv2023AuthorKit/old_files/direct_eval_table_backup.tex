\begin{table*}[h]
\begin{center}
\small 
\begin{tabular}{>{\centering\arraybackslash}c|l|c|c|c|c|c|c}
\hline
\multicolumn{1}{c}{} & \multicolumn{1}{|c}{} & \multicolumn{2}{|c}{\textbf{SBUCaps}} & \multicolumn{2}{|c}{\textbf{RedCaps}} & \multicolumn{2}{|c}{\textbf{Conceptual Captions}} \\\cline{2-8}
 & \textbf{Method} &  \textbf{PREC / REC} &  \textbf{F1} &  \textbf{PREC / REC} &  \textbf{F1} &  \textbf{PREC / REC} &  \textbf{F1} \\
\cline{2-8}
& No Vetting & 0.463 / 1.000 & 0.633 & 0.596 / 1.000 &  0.747 & 0.737 / 1.000  & \underline{0.849} \\
\hline
&Global CLIP %\cite{Radford2021LearningTV} 
& 0.531 / 0.700 & 0.604 & 0.618 / 0.551 & 0.583 & 0.753 / 0.458 & 0.569 \\
\multirow{-2}{*}{\scriptsize{VL}}& Global CLIP - E %\cite{Radford2021LearningTV} 
& 0.526 / 0.683 & 0.594 & 0.625 / 0.522 & 0.569 & 0.745 / 0.417 & 0.534 \\
 \hline
&   Local CLIP %\cite{Radford2021LearningTV} 
& 0.588 / 0.246 &  0.347 & 0.723 / 0.591 & 0.651 & 0.750 / 0.240 & 0.363 \\
 & Local CLIP - E %\cite{Radford2021LearningTV} 
& \underline{0.708} / 0.820 & \underline{0.760} & \underline{0.770} / 0.924 & \underline{0.840} & \underline{0.842} / 0.462 & 0.597 \\
\multirow{-3}{*}{\scriptsize{V}}& Reject Large Loss % \cite{Kim2022LargeLM}
 & 0.530 / \textbf{0.898} & 0.667 & 0.700 / 0.908 & 0.790 & 0.806 / 0.858 & 0.831 \\
  \hline
& Accept Descriptive & 0.449 / 0.542 & 0.491 & 0.561 / 0.326 & 0.413 & 0.739 / 0.741 & 0.740 \\
&Reject Noun Mod. & 0.517 / 0.769 & 0.618 & 0.644 / 0.776 & 0.703 & 0.765 / 0.870 & 0.814 \\
 &  Cap2Det %\cite{ye_2019_cap2det}
 & 0.500 / \underline{0.884} & 0.639 & 0.633 / \textbf{0.945} & 0.758 & 0.758 / \textbf{0.956} & 0.846 \\\cline{2-8}
& VEIL-Same Dataset & \textbf{0.828} / 0.791 & \textbf{0.809} & \textbf{0.855} / \underline{0.929} & \textbf{0.890} & \textbf{0.884} / \underline{0.935}& \textbf{0.909}\\



\multirow{-7}{*}{\scriptsize{L}}& VEIL-Cross Dataset
 & 0.636 / 0.811 & 0.713 & 0.747 / 0.847 & 0.793 & 0.834 / 0.866 &	\underline{0.850} \\
\hline\hline
\multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{|c}{\textbf{}} & \multicolumn{2}{|c}{\textbf{VIST}} & \multicolumn{2}{|c}{\textbf{VIST-DII}} & \multicolumn{2}{|c}{\textbf{VIST-SIS}} \\
\cline{2-8}
 & \textbf{Method} &  \textbf{PREC / REC} &  \textbf{F1}  &  \textbf{PREC / REC} &  \textbf{F1}  &  \textbf{PREC / REC} &  \textbf{F1} \\
\cline{2-8}
 & No Vetting & 0.744 / 1.000 & 0.853 & 0.779 / 1.000 & 0.876 & 0.695 / 1.000 & \underline{0.820}\\ \hline
& Global CLIP %\cite{Radford2021LearningTV} 
& 0.772 / 0.589 & 0.668  & 0.788 / 0.518 & 0.625 & 0.754 / 0.624 & 0.683 \\
\multirow{-2}{*}{\scriptsize{VL}} & Global CLIP - E %\cite{Radford2021LearningTV} 
& 0.769 / 0.569 & 0.654 & 0.785 / 0.504 &  0.613 & 0.741 / 0.595 & 0.660 \\\hline
& Local CLIP %\cite{Radford2021LearningTV} 
& 0.752 / 0.298 & 0.427 & 0.787 / 0.341 & 0.476 & 0.738 / 0.292 & 0.418 \\
& Local CLIP - E %\cite{Radford2021LearningTV}  
& \textbf{0.874} / 0.671 &  0.759  & \textbf{0.886} / 0.572 & 0.695 & \textbf{0.833 }/ 0.793 & 0.812\\ 
\multirow{-3}{*}{ \scriptsize{V}}& Reject Large Loss  %\cite{Kim2022LargeLM} 
& 0.755 / 0.811 &	0.782 &	0.792 /	0.796 &	0.794 &	0.700 / 0.791&	0.743 \\ \hline
& Accept Descriptive  & 0.755 / 0.631 & 0.687 & 0.784 / 0.913 & \underline{0.844 }& 0.686 / 0.163 & 0.264\\
& Reject Noun Mod. & 0.775 / 0.879 & 0.823 & 0.813 / 0.883 & 0.847 & 0.716 / 0.875 & 0.788 \\
& Cap2Det %\cite{ye_2019_cap2det} 
& 0.781 / 0.877 & 0.826 & 0.823 / 0.887 & 0.854 &  0.704 / 0.859 &0.774\\ \cline{2-8}
& VEIL-Same Dataset & 0.789 / \textbf{0.971} & \underline{0.871} & 0.819 / \textbf{0.992} & \textbf{0.892 }& 0.690 / \textbf{0.998} & 0.816 \\
\multirow{-7}{*}{\scriptsize{L}}& VEIL-Cross Dataset & \underline{0.835 }/ \underline{0.920} & \textbf{0.875} & \underline{0.870} / \underline{0.915}
& \textbf{0.892} & \underline{0.765 }/\underline{ 0.920} & \textbf{0.830} \\\hline \hline 
\multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{|c|}{\textbf{}} & \multicolumn{2}{c|}{\textbf{COCO}} & \multicolumn{4}{c}{} \\
\cline{2-4}
 & \textbf{Method} &  \textbf{PREC / REC} &  \textbf{F1} \\\cline{2-4}
& No Vetting & 0.948 / 1.000 & \textbf{0.973 }\\ \cline{1-4}
& Global CLIP %\cite{Radford2021LearningTV} 
& 0.945 / 0.509 & 0.662  \\
\multirow{-2}{*}{\scriptsize{VL}} & Global CLIP - E %\cite{Radford2021LearningTV}  
& 0.931 / 0.487 &  0.640 \\\cline{1-4}
& Local CLIP% \cite{Radford2021LearningTV} 
& 0.951 / 0.307 & 0.464  \\
& Local CLIP - E % \cite{Radford2021LearningTV} 
& 0.972 / 0.663 & 0.788 \\ 
\multirow{-3}{*}{\scriptsize{V} } 
 & Reject Large Loss  %\cite{Kim2022LargeLM} 
 & 0.963 / 0.837	& 0.896 \\ \cline{1-4}
& Accept Descriptive  &  0.948 / 0.923 & 0.935 \\
& Accept Narrative  & 0.942 / 0.077 & 0.143 \\
& Reject Noun Mod. & 0.958 / 0.859 & 0.906 \\
& Cap2Det %\cite{ye_2019_cap2det} 
&\textbf{ 0.978} / \underline{0.950} & \underline{0.964} \\ \cline{2-4}
& VEIL-Same Dataset & 0.948 / \textbf{1.000} & \textbf{0.973} \\
\multirow{-7}{*}{\scriptsize{L}}& VEIL-Cross Dataset & \underline{0.975} / 0.942 & 0.958 \\\cline{1-4}
\end{tabular}
\end{center}
\caption{Extracted label vetting evaluation metrics. Bold indicates best result in column, and in the recall columns No Vetting is excluded as it always has perfect recall.}
\label{tab:direct_eval_all}
\end{table*}
