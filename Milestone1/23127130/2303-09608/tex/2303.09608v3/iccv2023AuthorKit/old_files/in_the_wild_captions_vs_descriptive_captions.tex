\section{Analysis: Descriptive vs In-the-wild captions}
\label{sec:analysis}

We analyze what makes three large-scale in-the-wild datasets a challenging source of labels for object detection methods.
We use the VIST dataset to demonstrate linguistic differences of narrative and descriptive captions, then show the ratio of narrative captions, and a taxonomy of sources of extracted label noise, in the other four datasets. 
% also describe preprocessing

\textbf{COCO} \cite{Lin2014MicrosoftCC} contains cleanly crowdsourced captions, and stands in for the status quo in weakly supervised object detection since the cost of collecting COCO does not scale well.
Visual Storytelling (\textbf{VIST}) \cite{huang2016visual} contains three styles of captions, of which we use two: \textbf{D}escriptions of \textbf{I}mages in \textbf{I}solation (DII), which are similar to captions commonly used for pretraining for object detection \cite{Lin2014MicrosoftCC,Young2014Flicker30K}, and 
\textbf{S}tory for \textbf{I}mages in \textbf{S}equence (SIS), similar to in-the-wild captions. 
Each image contains both DII and SIS captions, which allows us to observe the impact of caption style while keeping the image constant.
Conceptual Captions (\textbf{CC}) \cite{Sharma2018ConceptualCA}, and the remaining two datasets, are collected from in-the-wild data sources. CC contains 3 million image-alt-text pairs after heavy post-processing; samples were accepted if there was an overlap between Google Cloud Vision API class predictions and the caption and named entities were hypernymized.
\textbf{RedCaps} \cite{Desai2021RedCapsWI} is the largest dataset used in our paper, consisting of 12M image-text pairs collected from Reddit by crawling a manually curated list of subreddits with heavy visual content.
SBU Captioned Photo Dataset (\textbf{SBUCaps}) \cite{Ordonez2011Im2TextDI} consists of 1 million Flickr photos with associated text descriptions written by their owners.
%on the photo-sharing platform. 
%To construct this dataset, 
Only captions with at least one prepositional phrase and at least 2 matches with a predefined vocabulary were accepted.

% Prior work \cite{Alikhani2019CaptionAA} demonstrates linguistic differences between captions produced from crowdsourcing (VIST, COCO, Flickr) and online in-the-wild captions (CC, Recipe).
% %and other written text corpora (ANC, FS). This paper
% It notes that natural captions from VIST-SIS and CC contain ``play-by-play descriptions [...]
% %in the narrative present (or sometimes for VIST, past) 
% rather than the progressive descriptions provided by crowd-workers who just
% describe what they see.'' We hypothesize that this narrative element found in in-the-wild corpora, can impact the visibility of objects mentioned in the caption, and this noise can diminish the ability to successfully train an object detection model. Positive signals are sparse and false positive noise can have a greater impact than false negative noise  \ref{}. This motivates us to more effectively use this widely available form of weak supervision from in-the-wild captions, using VEIL.

\subsection{Descriptive/Narrative Linguistic Differences}
\label{sec:ling_diffs}

%The linguistic structure of a caption may correlate with visual-textual alignment. For example, a caption with a low count of nouns might not mention the salient objects. Prepositions such as ``next to'' may convey spatial information that is likely to be visible. Other prepositions such as ``like'' may indicate the use of non-literal language (figure of speech) and any object that follows will likely \emph{not} to be visible. Discourse relations and tagging content as nucleus (core) or satellite (helper) could inform us that non-visual context is provided, e.g. ``We used the gondolas one night [nucleus; gondolas visible] to go to a ball [satellite; ball not visible]." 

\begin{table}[t] %[!htb]
      \centering
        \small
        \begin{tabular}{r c c}\hline
            Part of Speech \vline & Descriptive (DII) & Narrative (SIS) \\ \hline
            noun \vline & \textbf{3.85} & 2.73 \\
            preposition \vline& \textbf{1.75} & 1.01 \\
            adjective \vline & \textbf{0.98} & 0.85 \\
            personal pronoun \vline & 0.18 & \textbf{0.80} \\%\hline
            %\textbf{verb} \vline &1.55 & \textbf{2.00}\\\hline
            verb base \vline & 0.07 & \textbf{0.37}\\
            verb gerund \vline& \textbf{0.61} & 0.23\\
            past tense \vline& 0.08 & \textbf{0.89}\\
            %past participle \vline&  \textbf{0.21} & 0.17\\
            %non-3rd person sing pres \vline&\textbf{ 0.22}& 0.16\\
            %3rd person sing pres \vline& \textbf{0.36}& 0.19\\
        \end{tabular}
    \caption{Part of speech differences in descriptive/narrative caps. }
    \label{tab:pos_dii_sis}
\end{table}

\noindent\textbf{Part of Speech (POS).}
We extract POS tags over all VIST captions using SpaCy \cite{spacy2} and observe differences in pronoun usage and verb tense between descriptive and narrative captions (Table \ref{tab:pos_dii_sis}). The increased use of pronouns in SIS suggests a deviation from an impersonal, objective tone. For the same image, one SIS caption says ``\textbf{we} finally arrive at the island" while a DII caption provides count and detailed information: ``\textbf{a group of four men} sitting together".
In the supplementary materials, we show verb tense and aspect also differ between descriptive/narrative captions. Present tense is much more common in descriptive captions (81\% in DII vs 47\% in SIS) while past tense is more common in narrative captions (19\% in DII and 53\% in SIS). 
%Further, aspect distribution is also distinct across DII and SIS. Progressive aspect refers to verbs that describe an ongoing activity such as ``The event is starting, there are even some dancers forming outside" (SIS) while perfective describes a completed activity, e.g. ``The old part has been \textbf{removed} and now there many loose wires now" (SIS). 
DII contains mostly progressive captions (95\%) meaning that it describes ongoing activities more likely to be present in the accompanying visual scene, and thus more aligned captions. 

\textbf{Discourse relations.} Rhetorical structure theory studies how text is structured for cohesion \cite{Mann1988RhetoricalST}. 
%There is an argument or a claim (nucleus) which is furthered by supporting spans (satellite) through a particular type of relation. 
In the supplementary materials, we show the biggest difference in coherence relations occurs for ``Elaboration" (10.5\% more in DII than SIS) and ``Attribution" (3.5\% more in SIS).
%, ``Joint" (1.9\% more in SIS) and ``Enablement" (1.7\% more in SIS) relations. Labels extracted from Temporal-SIS captions could contain either a future or past (``before'', not currently visible) reference in nucleus/satellite; while Temporal-DII contains references to the present (``while''). 

\textbf{Visual-text alignment.} The average cosine similarity over DII caption-image embeddings extracted using CLIP \cite{Radford2021LearningTV} is $0.3032\pm0.0345$, significantly larger than for SIS, $0.2633\pm0.0450$. 
In our supplementary materials, we show SIS exhibits more diverse image-text relations (CLUE discourse types \cite{Alikhani2020ClueCC}) than DII.

\subsection{Descriptiveness features and predictor}

%\input{iccv2023AuthorKit/tables/dii_sis_classifier_table.tex}
\input{iccv2023AuthorKit/tables/dii_sis_clsf.tex}

Based on the above analysis, we examine \textbf{which linguistic features are most discriminative} between descriptive and narrative captions. We assign a positive label to captions drawn from the DII split and a negative label to SIS captions. The input vector to this classifier contains binary variables corresponding to the presence of a linguistic feature (e.g. proper noun, adjective, verb - past tense, etc) in the caption. 
We compare different combinations of the input in table \ref{tab:dii_sis_classifier}.
We find that part-of-speech is the strongest linguistic feature, and performs nearly as well as BERT \cite{BERT}, despite being much smaller and having less overhead (no forward pass required). 


\subsection{Descriptiveness ratio in in-the-wild datasets}
%\textbf{How well does this classifier transfer to other in-the-wild datasets?}

%\input{iccv2023AuthorKit/tables/descriptiveness_performance.tex}

We label 100 captions in each of SBUCaps, RedCaps, Conceptual Captions and COCO, as having a descriptive or narrative style. We define ``descriptive'' style as the caption describing important parts of the image and using objective language. We define ``narrative'' as having subjective elements, first-person perspective ("I", "my", etc), using non-literal expressions ("it's a piece of cake"), or assigning intent ("owner shows off her dog"). While our criteria are based on VIST-SIS, they are inclusive of captions beyond those narrative examples.
%, as real-world captions are not constrained by creating a caption drawing from visible events in previous or future frames. 
%We find that in-the-wild datasets contain far more narrative-like examples than COCO, a heavily descriptive dataset as shown in Table \ref{tab:descriptiveness_in_wild_annot}. 
We found RedCaps contains the most narrative-like captions (62\%), SBUCaps contains 37\%, and ConceptualCaptions 33\% (lowest amongst in-the-wild datasets), and COCO had 5\%. 
This is understandable because RedCaps are from a social media source, where people frequently write in a story-like fashion to communicate and connect with others; contrastively, Conceptual Captions is written with the intent to convey the image, or sometimes the meaning behind the image to visually impaired users using assistive technology. 
%we show the ability of our descriptiveness classifier to transfer to these datasets. 
%In fact, it's surprising that there's still a considerable amount of narrative-like captions in a heavily preprocessed image-alt text dataset like Conceptual Captions.
%Next, we test the ability of the part-of-speech based descriptiveness classifier trained on VIST descriptive and narrative captions to predict whether an in-the-wild caption is descriptive. We use the small subset mentioned earlier for evaluation.
In our supplementary materials, we show that while our part-of-speech descriptiveness classifier is able to correctly identify descriptive captions in most datasets, it struggles with correctly identifying narrative captions. 
%We suspect that this might be due to the domain shift from VIST-SIS to the narratives found in the real world. Furthermore, the features that indicate narrativeness in the natural captions are not represented by part-of-speech features. 
For example, "interesting" (an adjective) indicates subjectivity, but "red" (also adjective) is concrete. 
%first-person pronouns like "I" and "my" fall under personal pronouns. 
%But, beyond the part of speech tags, there are implicit first-person perspectives found in the grammatically incorrect short hand writing found in natural captions: "watching a doggie show in the bar while waiting for the pizza". The underspecification of the features used for the descriptiveness classifier explains why it fails to generalize to narrative captions in-the-wild.


This motivates our VEIL approach which extracts broader patterns than those studied in this section.


