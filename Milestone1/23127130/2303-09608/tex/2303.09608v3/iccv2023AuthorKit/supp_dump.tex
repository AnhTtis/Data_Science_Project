\section{Case Study: Additional Analysis on Descriptiveness/Narrativeness}

\subsection{Linguistic Differences}
Table \ref{tab:pos_dii_sis} presents the distribution of part-of-speech tags in DII and SIS captions, revealing a significant disparity in frequency across various parts of speech. We show that verb tense is more evenly distributed in narrative than descriptive captions which favor present tense. Additionally, Table \ref{tab:aspect} shows aspect, inferred using part-of-speech tags. Aspect relates to visual-text alignment. Progressive aspect refers to verbs that describe an ongoing, hence likely to be shown, activity such as ``The event is starting, there are even some dancers forming outside" while perfective describes a completed or close to completion activity, e.g. ``The old part has been \textbf{removed} and now there many loose wires now". Table \ref{tab:aspect} shows that progressive aspect is more common in both, but very frequent in DII captions, which aligns with our expectations. 
\begin{table}[!htb]
      \centering
        \begin{tabular}{|r|c|c|}\hline
            Part of Speech   & DII & SIS \\ \hline
            noun   & \textbf{3.85} & 2.73 \\
            preposition  & \textbf{1.75} & 1.01 \\
            adjective   & \textbf{0.98} & 0.85 \\
            personal pronoun   & 0.18 & \textbf{0.80} \\\hline
            \textbf{verb}   &1.55 & \textbf{2.00}\\\hline
            verb base   & 0.07 & \textbf{0.37}\\
            verb gerund  & \textbf{0.61} & 0.23\\
            past tense  & 0.08 & \textbf{0.89}\\
            past participle  &  \textbf{0.21} & 0.17\\
            non-3rd person sing pres  &\textbf{ 0.22}& 0.16\\
            3rd person sing pres  & \textbf{0.36}& 0.19\\
            \hline
        \end{tabular}
    \caption{The difference in part of speech between descriptive and narrative captions. }
    \label{tab:pos_dii_sis}
\end{table}

\begin{table}
  \centering
    %\caption{Using a random sample of 10,000 captions, we observe verb tense and grammatical aspect. The SIS corpora consists of a slightly imbalanced mix of verbs in the past tense form and present tense form, whereas DII is mainly written in the present form. In terms of aspect, both DII and SIS favor progressive aspect where verbs describe ongoing events rather than completed events.}
    \label{tab:verb_tense_distribution}
    \begin{tabular}{|r|c|r|c|c|}\hline
         & past & present  & progressive & perfective \\ \hline
        DII & 18.5\% & \textbf{81.4\%} &  \textbf{94.5\%} & 5.4\%\\
        SIS & \textbf{52.8\%} & 47.2\% &  \textbf{75.1\%} & 24.9\%\\
        \hline
    \end{tabular}
    \caption{Distribution over tense and aspect. }
    \label{tab:aspect}
\end{table}


% \noindent\textbf{Part of Speech (POS).}
% As mentioned in the main text, we extract POS tags over all VIST captions using SpaCy \cite{spacy2} and observe differences in pronoun usage and verb tense between descriptive and narrative captions (Table \ref{tab:pos_dii_sis}). The increased use of pronouns in SIS suggests a deviation from an impersonal, objective tone. Verb tense and aspect also differ between descriptive/narrative captions (Table \ref{tab:verb_tense_distribution}). Past and present tense are both frequent in SIS captions, with more verbs referring to the past than present. Past  and present tense in SIS can occur in the same sentence: ``Afterwards, we take a couple photographs because we paid the photographer to do so." An aligned image would show either show the couple posing for a photo or the transaction. This interaction between past and present occurs over 33\% of SIS captions vs 16\% in DII. 

% Aspect is more evenly distributed in narrative than descriptive captions, but progressive aspect is more common in bother. Progressive aspect refers to verbs that describe an ongoing activity such as ``The event is starting, there are even some dancers forming outside" (SIS) while perfective describes a completed %or close to completion 
% activity, e.g. ``The old part has been \textbf{removed} and now there many loose wires now" (SIS). 

% We hypothesize part of speech tags can be a strong feature to discriminate between DII and SIS, and DII provides more aligned captions since it describes ongoing activities more likely to be present in the accompanying visual scene.

\begin{table}[t]
    \centering
    \scalebox{0.95}{
    \begin{tabular}{|r|c|c|}\hline
        Linguistic features & PREC & REC \\\hline
        POS & \textbf{0.8823} & \textbf{0.8782}\\
        CLIP \cite{Radford2021LearningTV} & 0.7215 & 0.7170 \\\hline
        CLIP+POS & 0.8925 & 0.8893\\
        BERT & \textbf{0.9570} & \textbf{0.9560}
        \\\hline % prec dropped from 0.968 -> 0.957 (previously evaluated on a subset)
    \end{tabular}}
    \caption{We evaluate precision/recall of DII/SIS classifiers on a VIST holdout.}
    %taking in combinations of linguistic features 
    %We see that part of speech tags are quite discriminative, however using all features: RST, CLIP*, and CLUE increase the precision and recall of the classifier. 
    %\cite{Radford2021LearningTV}. 
    \label{tab:dii_sis_classifier}
\end{table}

\input{iccv2023AuthorKit/tables/evaluating_on_pascal_dii_sis}

\subsection{Descriptiveness Classifier}
\textbf{Training details.} We examine using part-of-speech tags to discriminate between descriptive and narrative captions, and compare against methods trained with much more data, BERT \cite{BERT} and CLIP \cite{Radford2021LearningTV}. To set up this task, we use captions from the VIST dataset \cite{huang2016visual} which contains multiple captions written in three distinct styles for each image. We use the descriptive-in-isolation (DII) and story-in-sequence (SIS) captions from VIST \cite{huang2016visual} and assign a positive label (descriptive) to captions drawn from the DII split and a negative label (narrative) to captions drawn from the SIS split. We then train a logistic regression model, which we refer to as the descriptiveness classifier. This classifier takes a binary vector corresponding to the presence of a linguistic feature (e.g. proper noun, adjective, verb - past tense, etc) in the caption. The CLIP-based descriptiveness classifier uses only the cosine similarity between the image embeddings and caption embeddings to distinguish between descriptiveness and narrativeness; training a logistic regression model on a single feature has the effect of finding an appropriate threshold for this task. For the BERT upper bound comparison, we finetune BERT without freezing weights.
For this experiment, we sample a balanced subset of VIST with captions containing an exact match with a category from the following constrained label set: dog, boat, person, car. 
We compare different combinations of the input in Table \ref{tab:dii_sis_classifier}.
We find that simple linguistic features such as part-of-speech do nearly as well as BERT \cite{BERT}, despite being much smaller and having less overhead (no forward pass required). Further, POS is stronger than CLIP, and combining them achieves a better results than either one in combination, indicating these features carry complementary information for this task.

\textbf{WSOD applicability.} For this experiment, we sample a balanced subset of VIST with captions containing an exact match with a category from the following constrained label set: dog, boat, person, car. 
Table \ref{tab:eval_dii_sis_predictions} shows that these classifiers can be used to select quality data for training weakly-supervised object detection models. Each classifier assigns descriptiveness scores; we show that when the highest-scoring caption is selected per image (resulting in a pool of descriptive captions), the results are \textbf{4-6 times better} than when selecting the lowest-scoring captions (resulting in narrative captions). Specifically, we extract labels from the selected captions as pseudo image-level labels using exact string matching, to train the multiple instance detection network using WSDDN \cite{bilen2016weakly}. We measure mAP at $IOU=0.5$ over the selected categories (car, boat, dog, person). 

\textbf{Evaluating the descriptiveness classifier on in-the-wild datasets.}
Seeing the low performance of the descriptiveness classifier in vetting of extracted labels (Table 2 in the main text), we hypothesize that this is due to inability of this classifier to distinguish between descriptive/narrative captions in in-the-wild datasets. We thus test the ability of the part-of-speech based descriptiveness classifier trained on VIST descriptive and narrative captions to predict whether an in-the-wild caption is descriptive. As described in the main text, we label 100 captions as having a descriptive or narrative style from SBUCaps, RedCaps and Conceptual Captions. We use the part-of-speech based classifier trained on VIST to predict whether the annotated captions are descriptive; predictions over the threshold 0.5 are classified as descriptive. We use metrics like accuracy, descriptive misclassification rate ($\frac{\text{\# of incorrect descriptive predictions}}{\text{\# of descriptive captions}}$), and narrative misclassification rate ($\frac{\text{\# of incorrect narrative predictions}}{\text{\# of narrative captions}}$) to evaluate performance.

%We define a narrative style as having subjective elements, first-person perspective ("I", "my", etc), using non-literal expressions ("it's a piece of cake"), assigning intent ("owner shows off her dog"). Note that while VIST-SIS contains examples of narrative-like captions, our definition is inclusive of captions beyond those narrative examples, as real-world captions are not constrained by creating a caption drawing from visible events in previous or future frames. 

% We find that in-the-wild datasets contain far more narrative-like examples than COCO, a heavily descriptive dataset as shown in Table \ref{tab:descriptiveness_in_wild_annot}. RedCaps contains the most narrative-like captions (62\%) in the in-the-wild datasets, whereas ConceptualCaptions contains the least (33\%) we show the ability of our descriptiveness classifier to transfer to these datasets. This is understandable because RedCaps are from a social media source, where people frequently write in a story-like fashion to communicate and connect with others; Conceptual Captions is written with the intent to convey the image, or sometimes the meaning behind the image to visually impaired users using assistive technology. In fact, it's surprising that there's still a considerable amount of narrative-like captions in a heavily preprocessed image-alt text dataset like Conceptual Captions.


\input{iccv2023AuthorKit/tables/descriptiveness_performance.tex} 



 We show the results in Table \ref{tab:desc_classifier_score_in_wild_annot}. We find that while the descriptiveness classifier is able to correctly identify descriptive captions in most datasets, it struggles with correctly identifying narrative captions. We suspect that this might be due to the domain shift from VIST-SIS to the narratives found in the real world. Furthermore, the features that indicate narrativeness in the natural captions are not represented by part-of-speech features. For example, specific words indicate subjectivety, like ``interesting" and others are concrete ``red", yet both are considered adjectives in terms of part-of-speech. Similarly, first-person pronouns like ``I" and ``my" fall under personal pronouns. But, beyond the part of speech tags, there are implicit first-person perspectives found in the grammatically incorrect short hand writing found in natural captions: ``watching a doggie show in the bar while waiting for the pizza". The underspecification of the features used for the descriptiveness classifier explains why it fails to generalize to narrative captions in-the-wild.

