\section{Introduction}

Freely available vision-language (VL) data has shown great promise to advance vision tasks \cite{Radford2021LearningTV,wslimageseccv2018,Jia2021ScalingUV}. However, the same advances have not been observed in weakly-supervised object detection. 
%A picture is worth a thousand words, but most captions have far fewer words, thus 
Unlike smaller, curated vision-language datasets like MS COCO \cite{Lin2014MicrosoftCC}, 
captions on the web \cite{Ordonez2011Im2TextDI, Desai2021RedCapsWI, Changpinyo2021Conceptual1P} only \textit{partially} describe the corresponding image, and often describe the \textit{context} behind the image. Thus, not only do these datasets contain user-uploaded images with  significant visual diversity, but captions contain %repeated 
mentions of objects that do not actually appear in the image. We hypothesize this poses a greater challenge for implicitly learning localization through weakly-supervised object detection (WSOD), than learning cross-modal representations and image recognition. Thus, WSOD has primarily been applied \cite{ye_2019_cap2det, Fang2022DataDD} to smaller-scale paid-for crowdsourced vision-language datasets like COCO \cite{Lin2014MicrosoftCC} and Flickr30K \cite{Young2014Flicker30K} until recently \cite{Zhou2022DetectingTC, Gao2021OpenVO}.  

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{iccv2023AuthorKit/figures/images/concept_fig_underlined.png}
    \caption{Extracted labels from captions can raise various challenges. Linguistic indicators explaining missing objects or defects are annotated in our dataset, \textbf{C}aption \textbf{La}bel \textbf{N}oise.}% (CLaN).}
    \label{fig:concept}
\end{figure}

%With naturally occurring, in-the-wild captions there should be greater scrutiny for what is mentioned. 
Unlike captions written by annotators for the purpose of faithfully describing an image, captions on the web go beyond a redundant, descriptive relationship with their corresponding image. Language is colorful, a word can be used in literal or metaphorical ways (``that was a piece of \underline{cake}"). A word can have multiple senses, of which only one corresponds to the sense intended by the object detection vocabulary. Frequently, a caption could share a story by including context that goes beyond the visual contents of the image but mention the object, like providing location names and unpictured interactions with objects as shown in Figure \ref{fig:concept}. 
User-uploaded content feature diverse object presentations, including photos taken from within vehicles, and intriguing atypical presentations of deformed objects and hand-drawn objects; all of this is relevant for narration for the image but not as supervision for precise localization.  We refer to image-level labels extracted from captions, that are incorrect (object not present in corresponding image), as \textbf{v}isually \textbf{a}bsent \textbf{e}xtracted \textbf{l}abels (VAELs). We show that VAELs pose a challenge for weakly-supervised detection.
 
To cope with this challenge and mitigate this noise, we propose \textbf{VEIL}, short for \textbf{V}etting \textbf{E}xtracted \textbf{I}mage \textbf{L}abels, to directly learn whether a label is clean or not from \textit{caption context}. 
%, without requiring additional annotation for this task. 
We first extract potential labels from each caption using substring matching or exact match \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019, Fang2022DataDD}. 
%, which we refer to as ``extracted labels''
We then use a transformer to 
% represent each caption, and train it to
predict whether each extracted label is visually present or absent. We refer to this prediction \emph{task} as extracted label vetting (ELV or vetting for short). 
We bootstrap labels from an ensemble of two pretrained object recognition models \cite{yolov5, zhang2021vinvl}, to predict pseudo-ground-truth visual presence labels on a variety of large-scale, noisy datasets: Conceptual Captions \cite{Sharma2018ConceptualCA}, RedCaps \cite{Desai2021RedCapsWI}, and SBUCaps \cite{Ordonez2011Im2TextDI}. While 
%the YOLOv5 \cite{yolov5} and X152-C4 \cite{zhang2021vinvl} 
these detectors are trained on COCO \cite{Lin2014MicrosoftCC}, 
%the latter is trained additionally on
VisualGenome \cite{Krishna2016VisualGC}, OpenImages \cite{Kuznetsova2018DatasetV4}, and Objects365 \cite{Shao2019Objects365AL}, they generalize well to estimating extracted label visual presence on the VL datasets we used. 
% Our trained vetting model also generalizes well across dataset boundaries. Importantly, it shows promise in generalizing across category boundaries. This suggests VEIL can be applied to class-agnostic vetting, which would be trained on a set of base categories but evaluated on held-out novel categories. 
Once we vet the extracted labels, we use them to train a weakly-supervised object detector. 

We investigate sources of noise across three in-the-wild datasets from diverse sources: photo-sharing platform, social media platform, and images with alt-text (typically used for VL pretraining). We collect and will release a small dataset with annotations on object visibility (label noise) and object appearance defects (visual noise such as occlusion, missing key parts, and atypical appearance). %As expected, we find that significant label and visual noise, in addition to class imbalance in these three datasets. 
%We find a significant number of samples where objects may be missing, however there is contextual overlap or a semantically similar object is present in the image (``car" instead of ``truck"). 
To further show support for using language context to filter object labels, we annotate linguistic indicators of noise which 
%contribute to the visual absence of the object and as an 
explain why a VAEL is absent from the image but included in the caption, such as describing context outside the image, non-literal use% of object word
, different word sense, etc. We find prevalent structured noise (there is a pattern to the images associated with a particular noisy label) from linguistic indicators like ``noun modifier" and ``prepositional phrase".
%We propose a method to filter label noise (vetting) and visual noise indirectly based on the caption, and apply weighted sampling and demonstrate significant performance gains.

We compare our label vetting method to eleven baselines, including standard cross-modal alignment prediction methods (CLIP), adaptive noise reduction methods, pseudo-label prediction, simple rule-based methods, and no vetting. Our method improves upon the baselines both in terms of predicting extracted label visual presence (measured with F1) and producing cleaner training data for object detection leading to an improvement of +10 mAP  over Large Loss Matters \cite{Kim2022LargeLM} and +3 mAP improvement over using CLIP \cite{Radford2021LearningTV} for filtering.
%, when we train on a held-out SBUCaps subset. 
Additionally, we show that VEIL generalizes across dataset and vocabulary.
%, and serves as a better vetting method prior to WSOD than CLIP. 
We show % +9.25 mAP 
a significant improvement when training WSOD with both clean labels from Pascal VOC 07 (annotated) and noisy, but vetted labels from SBUCaps (51.31 mAP) compared to combining clean labels with the noisy labels without vetting (42.06 mAP) or only using clean labels (43.48 mAP). Lastly, we show that WSOD performance improves with vetting over no vetting as the train dataset is scaled.

Our contributions are as follows:
\begin{enumerate}[nolistsep,noitemsep]
    \item We propose VEIL, a transformer-based extracted label, visual presence classifier.
    \item We apply VEIL and compare both across constructed-for-pay image caption datasets \cite{huang2016visual, Lin2014MicrosoftCC} and a wide set of in-the-wild datasets whose sources span social media \cite{Desai2021RedCapsWI}, photo-sharing platforms \cite{Ordonez2011Im2TextDI}, and image-alt-text pairs \cite{Sharma2018ConceptualCA}.
    \item VEIL performs better than %all other label noise reduction techniques 
    language-conditioned and language-agnostic label noise detection/correction approaches
    in recognizing visually present extracted labels and when vetting labels for weakly-supervised object detection. 
    %The resulting detection model shows substantial gains on common object detection benchmarks \cite{Everingham2010ThePV, Lin2014MicrosoftCC}. 
    \item Even when VEIL is trained on one dataset, but applied to another dataset, there are similar gains in WSOD. 
    \item VEIL shows potential to generalize across categories.
    \item Vetting with VEIL improves WSOD performance when scaling training data and effectively combines extracted labels (noisy) with clean labels.
    \item We annotate 300 samples from three in-the-wild datasets and release the  \textbf{C}aption \textbf{La}bel \textbf{N}oise (CLaN) dataset.
    %on three dataset subsets. 
    %\item We show adaptive label noise correction methods show shortcomings in WSOD.
\end{enumerate}