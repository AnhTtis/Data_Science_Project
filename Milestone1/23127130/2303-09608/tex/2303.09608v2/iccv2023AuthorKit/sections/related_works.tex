\section{Related Work}

\textbf{Vision-language datasets.} 
%The visual data in these datasets are typically either images or videos. For text accompanying images, 
Common datasets \cite{Young2014Flicker30K, Lin2014MicrosoftCC, huang2016visual} contain high-level multiple crowdsourced captions per image, including dense captions \cite{Krishna2016VisualGC}. 
Alt-text written by users to aid visually impaired readers \cite{Sharma2018ConceptualCA,Changpinyo2021Conceptual1P,Radford2021LearningTV,Schuhmann2021LAION400MOD} is widely used for vision-language grounding due to its abundance and assumed visual-text alignment %\cite{Schuhmann2021LAION400MOD,Radford2021LearningTV, 
\cite{zareian2021open, regionclip}. There are also large in-the-wild datasets sourced from social media like Reddit posts \cite{Desai2021RedCapsWI} and user-uploaded captions for publicly shared photos on Flickr \cite{Ordonez2011Im2TextDI}. 
%Even though both alt-text and crowdsourced captions are intuitively descriptive compared to social media sourced captions, 
%Prior work \cite{Alikhani2019CaptionAA} shows that there are still linguistic differences between captions produced from crowdsourcing (COCO, Flickr30k, VIST) and online in-the-wild captions (Conceptual Captions, RecipeQA). Building on this, 
We show the narrative element found in these, captured by several linguistic cues we investigate, can impact the ability to successfully train an object detection model.
%, and our method, VEIL, reduces the contribution of this noise.

\textbf{Weakly-supervised object detection} (WSOD) is a multiple instance learning problem to train a model to localize and classify objects from image-level labels \cite{bilen2016weakly,tang2017multiple,wan2019cmil,gao2019cmidn,ren2020instance}.
The first work to leverage unstructured text accompanying an image for WSOD, Cap2Det, predicted pseudo image-level labels from captions \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019, Unal2022LearningTO}.
%Cap2Det trains a multi-label text classifier to predict a pseudo label at the image level for WSOD, and uses these labels to train a WSOD model. 
However, Cap2Det cannot operate across categories as it requires observing training data to predict any particular category. 
%is trained on COCO captions which follows a descriptive style and contains far less VAEL noise than in-the-wild captions. While it can be used alone, 
The text classifier in Cap2Det was only used when exact string matching produced no labels, which means it mainly corrects false negatives (visually present, not extracted labels), not visually absent extracted labels. Detic \cite{Zhou2022DetectingTC} uses weak supervision from 
%image-level annotations from 
ImageNet \cite{Deng2009ImageNetAL} and extracted labels from Conceptual Captions (CC) %\cite{Sharma2018ConceptualCA} 
to pretrain an open vocabulary object detection model with a CLIP classifier head. While these approaches succeed in leveraging relatively clean, crowdsourced datasets like COCO. %\cite{Lin2014MicrosoftCC}
Flickr30K and ImageNet, both approaches see lower performance in training with CC. Other prior work \cite{Gao2021OpenVO} uses a pretrained vision-language multimodal model (on 14M data) \cite{Li2021AlignBF} to generate pseudo-bounding box annotations using cross-attention between caption (including the mentioned object of interest) and image regions. While it is trained on COCO, Visual Genome, and SBUCaps, it does not explicitly study the contribution of SBUCaps. 
%to object detection performance. 
%Related to this, we study the extension to using in-the-wild captions for weakly-supervised object detection. 
% explain diff with using those for pretraining
% Also include some semi supervised works


\input{iccv2023AuthorKit/sections/analysis_table}




\textbf{Vision-language pre-training for object detection.} 
Image-text grounding has been leveraged as a pretraining task for open vocabulary object detection \cite{zareian2021open, ZSOD_2_OVOD_1_gu2022openvocabulary, ZSOD_3_Rahman2020ImprovedVA, ZSL_Rec_Diff_ZSOD_Rahman2020ZeroShotOD, regionclip, Du2022LearningTP}, followed by supervision from bounding boxes from base classes. 
%GLIP \cite{Li2021GroundedLP_GLIP} also leverages image-text grounding during pretraining, however, image and text representations are fused at earlier layers in each modality-specific encoder which is more applicable to phrase grounding and computationally expensive for object detection. 
%Instead of training a vision-language grounding model from scratch and then training an object detection model, 
Some methods distill knowledge from existing pretrained vision-language grounding models like CLIP \cite{Radford2021LearningTV} and ALIGN \cite{Jia2021ScalingUV} to get proposals \cite{Shi2022ProposalCLIPUO} and supervision for object detection \cite{Du2022LearningTP, regionclip}; however the latter do not compare clean vs noisy supervision in a setting without bounding boxes. 
Our work differs in that we 
%do not attempt open vocabulary detection, rather 
focus on rejecting samples that are harmful for localization, and perform WSOD using noisy samples from captions only, without any bounding box annotations.
%Additionally, VEIL shows some promise in vetting novel object mentions in captions which means it could be used to augment or reject captions in vision-language pretraining for detection.






\textbf{Adaptive label noise reduction in classification.}
%Label noise in classification refers to when target labels for data are misaligned with the true labels, either through incorrect positive or missing labels. 
Adaptive methods reject or correct noisy labels ad-hoc during training. 
%Small-loss-based approaches select a sample if the overall loss or part of the loss %  like a predictive consistency loss, 
%C. Avoids cross-entropy based training all together which is less robust to noise--> not including for now
These methods exploit a network's ability to learn representations of clean labels earlier in training, thus assuming noisy samples have little visual pattern with the corrupted label and are learnt later in training \cite{Zhang2016UnderstandingDL}.
%rely on lack of easy visual pattern between images with the same corrupted label so that the network would need to memorize \cite{Zhang2016UnderstandingDL} the noisy label. 
We instead show diverse real-world datasets contain naturally occurring \emph{structured} noise, where in many cases there are visual patterns to the corrupted label.
Large Loss Matters \cite{Kim2022LargeLM} is representative of such adaptive noise reduction methods and we find that it struggles with noisy labels extracted from in-the-wild captions.
%than in the commonly benchmarked recognition, and so small-loss approaches struggle. 