\section{Method}
\label{sec:veil}

% MOVED EXTRACTING LABELS TO ANALYSIS SECTION

\input{iccv2023AuthorKit/figures/elavet_fig}

\textbf{Vetting labels (VEIL).} The extracted label vetting (ELV) task uses visual presence targets that are assigned based on predictions from a pretrained object recognition model for \textit{each} extracted label from the caption.
The method is overviewed in Fig.~\ref{fig:elavet_arch}.
Our model takes in a sequence of $C$ word token-level caption embeddings. WordPiece \cite{wu2016google} tokenization breaks captions into subwords, and each subword is mapped to a corresponding embedding, resulting in $e \in \mathbb{R}^{d\times C}$. These embeddings are passed through a language model, $h$, which include multiple layers of multi-head self-attention over tokens in the caption to compute token-level output embeddings $v\in \mathbb{R}^{d\times C}$.
These embeddings are then passed to an MLP and the model outputs a sequence of visual presence predictions per token, $r\in [0,1]^{C}$. 
\begin{gather}
    v = h(e) \\
    r = \sigma(W_2(\tanh(W_1v))
\end{gather}
where $W_1\in \mathbb{R}^{d\times d}$ and $W_2\in \mathbb{R}^{1\times d}$. 

Not all predictions in $r$ correspond to an extracted label, so we use a mask, $M\in [0,1]^{C}$, such that only the predictions associated with the extracted labels are used in binary cross entropy loss.  
To train this network, the pseudo-label targets are present, $y_i = 1$, if a pretrained object detector also predicts the same category as the extracted label. 
\begin{gather}
    L = \frac{1}{M^{T}M}\sum_{i=1}^{C}M_i\Big[y_i \log r_i + (1 - y_i) \log(1-r_i)\Big]
\end{gather}
During \emph{inference}, if an extracted label 
%has been tokenized into 
was mapped to multiple tokens (e.g. ``teddy bear"), the predictions are averaged.

\textbf{Special token.} We test VEIL$_{\text{ST}}$ which inserts a special token {\tt [EM\_LABEL]} before each extracted label in the caption to reduce the model's reliance on category-specific cues and improve generalization to other datasets. We find that it helps only the latter.
%\textbf{VEIL variants.} We test out two methods that can reduce the model's reliance on category specific predictions and improve generalization to other datasets: (1) VEIL$_{\text{ST}}$ which inserts a special token {\tt [EM\_LABEL] }right before each extracted label, and (2) randomly masking the tokens associated with the extracted labels with probability $p_m$.
%By inserting a special token right before each extracted label in the caption, we are explicitly indicating to the model which tokens correspond to the extracted labels. This can help the model to better learn the relationship between the surrounding context and the extracted labels, and to focus more on the relevant parts of the input. We have some success with randomly masking the input extracted label tokens to encourage the model to extract information from the surrounding context instead of specific categories. However, we only apply this variant in the category generalization experiments. 



\textbf{Weakly-supervised object detection.}
To test the ability of extracted label filtering or correction methods for weakly-supervised object detection, we train MIST \cite{ren2020instance}. MIST extends WSDDN \cite{bilen2016weakly} and OICR \cite{Tang2017MultipleID} to mine pseudo-ground truth boxes prior to iterative refinement such that multiple instances are not grouped as one.
VEIL uses training data from the in-the-wild datasets to train the vetting model, and we want to see how its ability to vet labels generalizes to unseen data. Thus, we use the test splits of the in-the-wild datasets to train MIST, as they are unseen by all vetting methods. We do not evaluate the WSOD model on these in-the-wild datasets, but rather on disjoint datasets, VOC-07 \cite{Everingham2010ThePV} and COCO 2014 \cite{Lin2014MicrosoftCC}. 

%\textbf{Weighted sampling.}
% \begin{gather}    
% \end{gather}

\textbf{Implementation details.}
VEIL is implemented in PyTorch \cite{pytorch} and uses a pretrained BERT encoder \cite{BERT} prior to the per-token visual presence classification layer. 
%We use MIST \cite{ren2020instance} to learn an object detection model using weak supervision from image-level labels extracted from captions. 
We simulate a batch size of 8 for all experiments unless specified otherwise. To address class imbalance during WSOD, 
%caused by in-the-wild datasets' long-tail distributions
we use the complement of the sub-sampling probability introduced in Word2Vec \cite{Mikolov2013DistributedRO} as weights. 
%We train under different GPU settings due to resource constraints, and use gradient accumulation for some experiments. 
We used 4 RTX A5000 GPUs and trained for 50k iterations with a batch size of 8, or 100k iterations on 4 Quadro RTX 5000 GPUs with a batch size of 4 and gradient accumulation (parameters updated every two iterations to simulate a batch size of 8).