\section{Experiments}
\label{sec:expts}

\input{iccv2023AuthorKit/tables/direct_eval_by_dataset.tex}
% \input{iccv2023AuthorKit/tables/indirect_evaluation.tex}
%\input{iccv2023AuthorKit/figures/cross_category}

We show the ability of VEIL %and other language conditioned vetting methods 
to match and exceed language-agnostic filtering and image-based filtering methods in extracted label vetting (ELV). Next, we highlight the promising ability of VEIL to vet noisy extracted labels prior to weakly-supervised object detection training and %a critical ability to 
remove structured noise.
%; this is important because structured noise decreases localization ability in WSOD. 
Lastly, we benchmark the generalization ability of VEIL in cross-dataset and cross-category settings.

\subsection{Experiment Details}

%\textbf{Datasets.}  
%As described in Sec.~\ref{sec:analysis}, 
We use three in-the-wild image-caption datasets (SBUCaps \cite{Ordonez2011Im2TextDI}, RedCaps \cite{Desai2021RedCapsWI}, Conceptual Captions \cite{Sharma2018ConceptualCA}) and three human annotated datasets that fall into descriptive (COCO \cite{Lin2014MicrosoftCC}, VIST-DII \cite{huang2016visual}) and narrative (VIST-SIS \cite{huang2016visual}). 
Each in-the-wild dataset and VIST are first reduced to a subset of image-caption pairs where a COCO category is explicitly mentioned in the caption. This subset is split into 80-20 train-test split. 
The WSOD models are trained on SBUCaps with labels vetted by different methods, and evaluated on PASCAL VOC 2007 test \cite{Everingham2010ThePV} and COCO val 2014 \cite{Lin2014MicrosoftCC}. The mAP metric uses $IOU=0.5$.
%evaluates both classification and localization ability of the WSOD models at 
%unless specified otherwise. 
However, when contrasting the image-level classification performance and detection performance we refer to them as Recognition maP and Detection mAP, respectively.

\subsection{Methods Compared}
For VEIL, we use the convention VEIL-DatasetX to signify that VEIL is trained on the train-split of DatasetX. Next, we describe the methods we compare against. 
We group these into language-based, image-based, and image-language methods. They are category-agnostic, except for Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019} and LLM \cite{Kim2022LargeLM} which must be applied on closed vocabularies. 
\\\textbf{No Vetting.} Accept all extracted labels (has \emph{perfect recall}).
\\\textbf{Global CLIP and CLIP-E.} We use the ViT-B/32 pretrained CLIP \cite{Radford2021LearningTV} model. To enhance alignment \cite{Hessel2021CLIPScoreAR}, we add the prompt ``A photo depicts" to the caption and calculate the cosine similarity between the image and text embeddings generated by CLIP. Since the cosine similarity distribution varies per dataset, we train a Gaussian Mixture Model with two components on the train datasets and select image-text pairs predicted to the component with higher visual-caption alignment. 
% Global CLIP filters based on the whole image and caption, even if there are visually present extracted labels. 
For the ensemble variant (CLIP-E), we prepend multiple prompts to the caption,
% or extracted label, %and calculate the cosine similarity between image-text CLIP embeddings, and lastly 
and use the score from the highest-scoring prompt.
\\\textbf{Local CLIP and CLIP-E} follow a similar process but use cosine similarity between the image and the prompt ``this is a photo of a" followed by the extracted label. Extracted labels are filtered by Local CLIP, not entire captions, making this image-conditioned, not image-language conditioned vetting like Global CLIP. Local CLIP-E ensembles prompts.
\\\textbf{Reject Large Loss.} LLM \cite{Kim2022LargeLM} is language-agnostic adaptive noise rejection and correction method. To test its ELV ability, we simulate five epochs of WSOD training \cite{bilen2016weakly} and consider label targets with a loss exceeding the large loss threshold as ``predicted to be visually absent" after the first epoch. The large loss threshold uses a relative delta hyperparameter controlling the rejection rate (set as 0.002 in \cite{Kim2022LargeLM}).
\\\textbf{Accept Descriptive / Narrative.} We train a logistic regression model to predict whether a VIST \cite{huang2016visual} caption comes from the DII (descriptive) or SIS (narrative) split. The input vector to this logistic regression model contains binary variables corresponding to the presence of a part of speech vector (e.g. proper noun, adjective, verb - past tense, etc) in the caption. We accept extracted labels from a caption if it yields a score higher than 0.5 for descriptiveness or narrativeness, respectively. 
%We use the part-of-speech (POS) based classifier trained on VIST-DII (descriptive proxy) and VIST-SIS (narrative proxy).
\\\textbf{Reject Noun Mod. (Adj/Any).} Since an extracted label could be modifying another noun (``\underline{car} park"), a very simple baseline would reject such labels. We try two variants, the first noun modifier rule rejects an extracted label if the POS label is an adjective or is followed by a noun. The second rule rejects if the extracted label is not a noun.
\\\textbf{Cap2Det.} We reject a label if it is not predicted by the Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019} classifier.


\input{iccv2023AuthorKit/sections/vetting_analysis}


\subsection{Extracted Label Vetting Evaluation}

Table \ref{tab:direct_eval} shows the F1 score which combines the precision and recall of their vetting (shown separately in supp).  
Most language-based methods improve or maintain the F1 score of No Vetting, even though it has perfect recall, except for Accept Descriptive/Narrative.

Rule-based methods and Cap2Det perform  strongly, but are outperformed by both VEIL-Same Dataset (trained and tested on the same dataset, without a special token) and VEIL-Cross Dataset (trained on a different dataset than that shown in the column; we show the best cross-dataset result). 

VEIL-Cross Dataset outperforms language-based approaches,
%in precision (in supp) and F1, 
showing VEIL's generalization potential, except COCO where Cap2Det does slightly better.

Image-and-language-conditioned approaches (Global CLIP/CLIP-E) make label decisions based on the overall caption, so if it contains certain language, it can affect the alignment even if the object is actually visually present. Table \ref{tab:direct_eval} shows these methods obtain low F1 scores.
%; while they improve precision compared to No Vetting (see supp), recall suffers terribly.

Among image-based approaches for label vetting, we observe that Local CLIP benefits significantly from using an ensemble of prompts compared to Global CLIP; ensembling is well documented in improving zero-shot image recognition in prior work \cite{Radford2021LearningTV}. 
Reject Large Loss has the strongest F1 score among the image-based methods, and in supp we show it has strong recall but limited precision improvement over No Vetting, indicating the presence of false positives that do not lead to a large loss. 



\textbf{Vetting performance on CLaN.} We hypothesize that LocalCLIP-E would do well at vetting VAELs explained by linguistic cues like non-literal and beyond the image as they are likely to have little to no visual similarity with the extracted category representation.
%, which would be hard to ground. 
We also hypothesize that VEIL would do better than LocalCLIP-E at vetting VAELs that are noun modifiers or in prepositional phrases.
%, and by extension share similar objects/contexts with the extracted but absent category. These
Further, similar context can sometimes be explained by linguistic cues like noun modifiers and prepositional phrases which can be easily picked up from the caption, but LocalCLIP-E may be oblivious to them differing from the true VAEL category. 
We evaluate these hypotheses on the CLaN dataset in Table \ref{tab:vetting_analysis}. We omit ``visible'' VAEL samples as there are pseudo-label errors, and the ``past'' linguistic indicator due to too few samples. 
We find that VEIL vets truly absent objects for SBUCaps much better than LocalCLIP-E, and comparably for RedCaps or CC. It vets partially visible objects better than LocalCLIP-E by a significant margin; these can be harmful in WSOD which is already prone to part domination \cite{ren2020instance}.
VEIL also recognizes that similar context to, rather than the actual VAEL category, are present.
%Since VEIL performs better at vetting VAELs from partially visible objects, it 
VEIL performs better at vetting visible objects that have visual defects
%occluded or have missing key parts, 
which can be mentioned in caption context 
%. VEIL's ability to vet atypical objects can be explained from (1) caption context indicating an atypical object 
(``acryllic illustration of \underline{dog}"), and
CLIP being trained on 400M diverse images with atypical objects and other defects. 
%where objects would have high visual appearance diversity, including atypical instances.
As expected, we find that for all datasets, VEIL vets VAELs from prepositional phrases better than LocalCLIP-E, and noun modifiers for SBUCaps and RedCaps. LocalCLIP-E does better on ``beyond the image" and non-literal VAELs, except on SBUCaps, where VEIL still excels.

\input{iccv2023AuthorKit/tables/cross_dataset_CLIP.tex}
\input{iccv2023AuthorKit/tables/cross_category.tex}

\input{iccv2023AuthorKit/tables/det_vs_recognition}


%\subsection{VEIL Generalizability}
%In addition to bootstrapping pseudo-labeled cleanliness data on the same source, we also test VEIL's ability to generalize to unseen sources and categories. Table \ref{tab:direct_eval} shows it can generalize between datasets. Across the language-conditioned approaches, VEIL-Cross Dataset exceeds F1 on most datasets except for CC and COCO, where it is still competitive. In supp, we show increased precision over all approaches. 

\textbf{Cross dataset source generalization.}
We train VEIL on one dataset (or multiple) and evaluate on an unseen target. We find that combining multiple sources improves precision in Table \ref{tab:cross_dataset_direct_eval}. To better utilize caption context, we test VEIL$_{\text{ST}}$ which predicts visual presence using a special token {\tt [EM\_LABEL]}. We find that this improves F1 performance. Lastly, we try ensembling by averaging predictions between LocalCLIP-E and VEIL-Cross Dataset, and find that its precision and recall is highest among the VEIL variants and LocalCLIP-E. This means that VEIL and LocalCLIP-E can be used together. There is still a significant gap between VEIL-Same Dataset and even the ensembled model in terms of precision and F1. We leave improving source generalizability to future research.

\textbf{Cross-category generalization.} We define an in-domain category set (ID) of 20 randomly picked categories from COCO \cite{Lin2014MicrosoftCC}, and an out-of-domain category set (OOD) consisting of the 60 remaining categories. We restrict the labels using these limited category sets and create two train subsets, ID and OOD from SBUCaps \textit{train} and one ID test subset from SBUCaps \textit{test}. %We use the VEIL variants %as described in Section \ref{sec:veil}, 
We find that transferring VEIL-OOD to unseen categories improves F1 score compared to no vetting as shown in Table \ref{tab:cross_category}. %When we trained with the special token variant, however we found little improvement so we left it out.
We hypothesize training on more categories could improve category generalization, but leave further experiments to future research.


\subsection{Impact on Weakly Supervised Object Detection}

%\textbf{Comparison between High Performing Methods in Extracted Label Vetting}
We select the most promising vetting methods from the previous section and use them to vet labels from the SBUCaps \textit{test} split since CLIP-based and VEIL-based methods use the train set for threshold and model training respectively. 
We show two different VEIL methods, VEIL-SBUCaps and VEIL$_{\text{ST}}$-RedCaps,CC. Both vet labels from SBUCaps and use them to train WSOD, but the vetting method is trained on either (1) SBUCaps or (2) RedCaps and CC, using the special token. We show both methods to demonstrate the generalizability of the vetting model.
Note that Large Loss Matters \cite{Kim2022LargeLM} has been relaxed to also \textit{correct} visually absent extracted labels, instead of only unmentioned but present objects (false negatives).
After vetting, we remove any images without labels and since category distribution follows a long-tail distribution, we apply weighted sampling.
We train MIST \cite{ren2020instance} for 50K iterations with a batch size of 8 for each method. 

We find that VEIL-SBUCaps performs the best as shown in Table \ref{tab:det_v_recognition}.
In particular, it boosts the detection performance of No Vetting by 9.3\% absolute and 29.8\% relative gain (40.5/31.2\% mAP) on VOC-07 and by 35\% relative gain (10.4/7.7\% mAP) on COCO.
Interestingly, VEIL-SBUCaps and VEIL$_{\text{ST}}$-Redcaps,CC both seem to have a similar performance improvement, despite VEIL$_{\text{ST}}$-Redcaps,CC (best VEIL cross-dataset result on SBUCaps) having poorer performance than Local CLIP-E in Table \ref{tab:cross_dataset_direct_eval}. Additionally, directly using the pretrained object recognition model (used to produce visual presence targets for VEIL) predictions to vet (GT* method in the table) performs worse than VEIL in both detection and recognition. This suggests VEIL \textbf{generalizes from its bootstrapped data}. % to perform significantly better on weakly supervised object detection. 

Using the CLaN dataset, we observe
%a number of examples with structured noise. 
one type of structured noise found from extracting labels from prepositional phrases, specifically where images were taken inside vehicles. We hypothesize such structured noise would have significant impact on localization for the vehicle objects. We use CorLoc to estimate the localization ability for vehicles in VOC-07 (``aeroplane", `bicycle", ``boat", ``car", ``bus", ``motorbike", ``train"). We observe a CorLoc of 60.2\% and 54.1\% for VEIL-SBUCaps and LocalCLIP-E, respectively. 
%For another supercategory, animal, we observe a smaller improvement (57.9 vs 56.8) from LocalCLIP-E and VEIL-SBUCaps. 
This shows structured noise can have strong impact on localization.
\input{iccv2023AuthorKit/figures/scale}

\textbf{Impact of scale on WSOD.} To assess the impact of scaling the noisy training set, we progressively sampled the held-out RedCaps dataset in increments of 50K samples up to a total of 200K samples. For each scale, we train two WSOD models with weighted sampling using the unfiltered samples and those vetted with VEIL-SBUCaps,CC. The 200K-sample model trained for 120K iterations, and subsets trained for iterations proportional to their sample size. Figure \ref{fig:scale} shows consistent improvement with vetting as the sample size scales. The non-vetted model's performance declines after 150K samples; more unvetted samples has diminishing or worse, negative returns. This indicates that vetting can adapt to scale better even when VEIL is trained on other datasets. The trend suggests that vetting will continue outperforming no-vetting even when dataset sizes increase.
% anything about why the performance is lower? Perhaps our label and visual noise dataset might show that the VAELs in Redcaps are worse. However, that says nothing about the samples that are included...

\input{iccv2023AuthorKit/tables/semi_supervised}

\textbf{Effect of mixing clean and noisy samples for WSOD.} We study how vetting impacts a setting where labels are drawn from both annotated image-level labels from 5K VOC-07 train-val \cite{Everingham2010ThePV} (clean) and 50K in-the-wild SBUCaps \cite{Ordonez2011Im2TextDI} captions (noisy). In Table \ref{tab:mixed_supervision} we observe that adding noisy supervision to clean supervision actually hurts performance  %(-3.2\%) 
compared to only using clean supervision. After vetting the labels extracted from SBUCaps  \cite{Ordonez2011Im2TextDI} using VEIL-SBUCaps, we observe that the model sees a 17.9\% relative improvement (51.31/43.48\% mAP) to using only clean supervision from VOC-07. We see further improvements when applying weighted sampling to the added, class imbalanced data (54.76/51.31\% mAP).

% \input{iccv2023AuthorKit/figures/train_subset_figures.tex}

\textbf{Conclusion.} We showed visually absent extracted labels are common in in-the-wild datasets.
%than datasets commonly used for detection pretraining. 
We proposed VEIL which uses language context to infer whether mentioned objects are visually present. It outperforms other vetting strategies, generalizes across datasets and categories, and its benefits persist when adding noisy to clean data. 
%We posit that VEIL can be used to effectively sample localization critical samples and help WSOD use noisier in-the-wild image-caption data.
% \textbf{Ethics note.}



