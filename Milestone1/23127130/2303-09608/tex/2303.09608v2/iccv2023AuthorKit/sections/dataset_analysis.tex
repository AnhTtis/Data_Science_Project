


\section{Label Noise Analysis and Dataset}%Caption Label Noise (CLaN) Dataset}
%\section{Analysis: Label and Visual Noise Dataset}
\label{sec:analysis}

We analyze what makes large-scale in-the-wild datasets a challenging source of labels for object detection methods. 

% ANALYSIS TABLE MOVED UP TO RELATED WORK SO IT CAN BE AT THE TOP OF THE PAGE

\textbf{Datasets analysed.}
%\label{sec:datasets}
Conceptual Captions (CC) \cite{Sharma2018ConceptualCA}, RedCaps  \cite{Desai2021RedCapsWI}, and SBUCaps \cite{Ordonez2011Im2TextDI}, are collected from in-the-wild data sources. \textbf{CC} contains 3 million image-alt-text pairs after heavy post-processing; named entities in captions were hypernymized and image-text pairs were accepted if there was an overlap between Google Cloud Vision API class predictions and the caption.
\textbf{RedCaps} %is the largest dataset used in our paper, 
consists of 12M image-text pairs collected from Reddit by crawling a manually curated list of subreddits with heavy visual content.
\textbf{SBUCaps} consists of 1 million Flickr photos with text descriptions written by their owners.
Only captions with at least one prepositional phrase and at least 2 matches with a predefined vocabulary were accepted.
These in-the-wild datasets exhibit very low precision of the extracted labels, ranging from 0.463 for SBUCaps, 0.596 for RedCaps, to 0.737 for CC, all much lower than the 0.948 for COCO (shown in supp). 
%This motivates our exploration of VAEL noise.

\textbf{Extracted object labels.} Given a vocabulary of object classes, we extract a label for an image if there is exact match between the object name and the corresponding caption ignoring punctuation, as in \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019,Fang2022DataDD}.

\textbf{Gold standard object labels.} We use pseudo-ground-truth predictions from a pretrained image recognition model to estimate visual presence \textit{gold standard} labels because these in-the-wild datasets do not have image-level object annotations. 
We use an object recognition ensemble with the X152-C4 object-attribute model \cite{zhang2021vinvl} 
% (trained on four public object detection datasets) 
and the Ultralytic  YOLOv5-XL \cite{yolov5}.
This ensemble achieves strong accuracy, %on visual presence detection, e.g.
82.2\% on SBUCaps, 85.6\% on RedCaps, and 86.8\% on CC.
%We observed higher visual presence accuracy on a small annotated set compared to each model alone (see supp). 
We extract VAELs by selecting images where extracted and gold-standard labels disagree. 
%Note in some cases a VAEL will correspond to a present object which is however missed by the recognition ensemble.

\textbf{Noise annotations collected.}
We select 100 VAEL examples per dataset (RedCaps, SBUCaps, CC).
We annotate four types of information for these examples: 
\begin{itemize}[nolistsep,noitemsep]
    \item (Q1: Label Noise) How much of the VAEL object is present (\underline{vis}ible, \underline{part}ially visible, completely \underline{abs}ent); 
    \item (Q2: Similar Context) If the VAEL object is completely missing, whether a traditionally co-occurring context (``boat" and ``water"), or semantically similar object (e.g. ``cake" and ``bread", ``car" and ``truck") is present; 
    \item (Q3: Visual Defects) If visible/partially visible, whether the VAEL object is occluded, has key parts missing, or atypical appearance (e.g. knitted animal); and
    \item (Q4: Linguistic Indicators) What linguistic cues, if any, explain why the VAEL object is mentioned but absent, e.g. the caption discusses events or information beyond what the image shows (``beyond'' in Table \ref{tab:stats}), describes the past (``past''), the extracted label is part of a prepositional phrase and likely to describe the setting and not objects (``on a train''), is a noun modifying another noun, is used in a non-literal way, has a different word sense (e.g. ``bed'' vs ``river bed''), or is part of a named entity.
\end{itemize}

Two annotators (authors) provide the annotations, with high agreement: 0.76 for Q1, 0.33 for Q2, 0.45 for Q3, and 0.58 for Q4. We calculate Cohen's Kappa for each option and aggregate agreement through a weighted average for each question, with weights derived from average option counts between the two annotators across the three datasets. We label the dataset Caption Label Noise, or CLaN.

In Table \ref{tab:stats}, we show what fraction of samples fall into each annotated category, excluding ``Other'', ``Unclear'' and uncommon categories. We average the distribution between the two annotators.

\textbf{Statistics: Label noise.}
We first characterize the visibility of objects flagged as VAELs by the recognition ensemble. We find that SBUCaps has the highest rate of completely absent images (58.5\%), followed closely by RedCaps. 
CC has the highest full visibility (32.8\%),
%followed by RedCaps (29.2\%) and then SBUCaps (21.5\%) where full visibility is 
defined as the object from a given viewpoint having 75\% or more visibility. 
SBUCaps also has the highest rate of partially visible objects (20\%). 
The high rate of absent and partially-visible objects justifies the use of pseudo-ground-truth labels from the recognition ensemble; these both constitute poor training data for WSOD. 
We investigate the fully-visible objects flagged as VAELs shortly, through our visual defect annotations. 
%These significant rates of visibility motivates investigating any visual defects in partially or fully visible objects in Q3 which may explain why the recognition ensemble flagged these as absent and could be considered a difficult example \cite{Bengio2009CurriculumL}. Secondly, it also motivates marking linguistic indicators in Q4 which can be used to predict both visual defects and completely absent objects in Q4.

\textbf{Statistics: Similar context.}
%One issue with solely focusing on object visibility is that 
Certain images with absent objects may be more harmful than others. Prior work has shown that models exploit co-occurrences between an object and its context which helps overall recognition accuracy, but can hurt performance when that context is absent \cite{Singh_Mahajan_Grauman_Lee_Feiszli_Ghadiyaram_2020}. We hypothesize the inclusion of images with this context bias without the actual object present could affect localization especially when supervising detection \textit{implicitly}, and semantically similar context may blur decision boundaries.
%hurting classification. 
%This motivates annotating if the image contains co-occurring context or if semantically similar objects are present instead of the VAEL. The question is subjective as 
Different annotators may have different references for similarity or co-occurrence frequency, but our annotators achieve fair agreement ($\kappa=0.33$). In Table \ref{tab:stats}, we find high rates of co-occurring contexts in samples with completely absent VAELs for SBUCaps (42.5\%) and CC (30.9\%).
%, while this is rarer in RedCaps (4\%). 
Across all datasets, we see a similar rate, 12\%-15\%, of similar context being present instead of the VAEL. 
% For example, if the image contains co-occurring context (e.g. "bus stop with buildings") while the object (e.g. "bus") is absent, then the inclusion of this example could harm localization.

\textbf{Statistics: Visual defects.}
%VAELs include a number of fully visible samples.
%or partially  
We hypothesize there may be visual defects which caused the recognition ensemble to miss fully-visible objects. 
%Summing over the columns in Visual defects in Table \ref{tab:stats}, we observe that indeed most visible samples have defects.
Over the fully or partially visible subset, in CC 79\% of fully or partially visible objects have a visual defect, 87\% for SBUCaps, and 69\% for RedCaps. 
The most common defect for RedCaps and CC is atypical (49\% and 57.3\%); we argue these atypical examples constitute poor training data for WSOD.
%, resp.) and occlusion for SBUCaps (26.5\%) in Table \ref{tab:stats}. This also shows that using the recognition ensemble to estimate visual presence is also a decent proxy for visual defects, so the recognition ensemble's missed predictions are useful. Upon further examination of captions, 
We find that the caption context (e.g. ``acrylic illustration of the funny mouse") may indicate the possibility of a visual defect, which further motivates the VEIL design. 

\textbf{Statistics: Linguistic indicators.} Noun modifier is one of the most frequently occurring linguistic indicator. Prepositional phrase is also significant in SBUCaps (40.5\%) and CC (31.3\%).
%, compared to RedCaps (5.7\%)
%and non-literal use is common in SBUCaps and RedCaps.
%have almost double the amount of non-literal linguistic indicator compared to CC. 
All datasets contain many VAELs which are mentioned in contexts going beyond the image; for example:
%the VAEL, ``boat” is mentioned in text that goes beyond the image: 
``just got back from the river. friend \textbf{sank his truck pulling his \underline{boat} out}. long story short, rip this beast” (RedCaps). %These linguistic indicators motivate rule-based methods as our baselines, but 
%Given that VAELs can be explained by a number of these indicators, rule-based method will miss out on VAELs. 
%We believe this relationship between linguistic indicators and VAELs can be learned implicitly if a transformer-based model is provided with a loose proxy for visual presence/absence labels, motivating VEIL.