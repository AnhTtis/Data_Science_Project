% \begin{figure}
%     \centering   
%     %\includegraphics[scale=0.5]{iccv2023AuthorKit/figures/images/prec_train_subset.png}  
%     \includegraphics[scale=0.3]{iccv2023AuthorKit/figures/images/f1_train_subset.png}
%     \includegraphics[scale=0.3]{iccv2023AuthorKit/figures/images/cross_cat_black.png}
%     \caption{This figure shows the impact of pseudo-labeled data size for VEIL-SBUCaps on its performance on SBUCaps-Test VPEL Detection. VEIL only needs 10\% of the data to beat No Vetting,
%     %(XX vs 0.633 F1), 
%     and 50\% of the labeled data (80K samples) to beat CLIP.} %This indicates that we only need a small amount of pseodo-labeled data to beat CLIP in terms of precision and slightly more to improve recall and therefore F1.}
%     \label{fig:train_subset}
% \end{figure}

\begin{figure}
  \centering
  %\subfigure[This figure shows the impact of pseudo-labeled data size for VEIL-SBUCaps on its performance on SBUCaps-Test VPEL Detection. VEIL only needs 10\% of the data to beat No Vetting,
    %(XX vs 0.633 F1),     and 50\% of the labeled data (80K samples) to beat CLIP.]{
    \includegraphics[width=0.2\textwidth]{iccv2023AuthorKit/figures/images/limited_train_data_bigger_2.png}
    %} \subfigure[Caption for subfigure 2]{
    \includegraphics[width=0.2\textwidth]{}
    %}
  \caption{(Left) The impact of pseudo-labeled data size for VEIL-SBUCaps on its performance on SBUCaps-Test vetting. 
  %VEIL only needs 10\% of the data to beat No Vetting, and 50\% of the labeled data (80K samples) to beat CLIP.}
  (Right) Cross-category generalization, comparing of WSOD performance on VOC-07 mAP (over 7 overlapping ID categories) when vetting train data, SBUCaps-Test ID, using VEIL-ID and VEIL-OOD. Dashed line indicates mean AP and solid line is median AP.}
\label{fig:train_subset}
\end{figure}