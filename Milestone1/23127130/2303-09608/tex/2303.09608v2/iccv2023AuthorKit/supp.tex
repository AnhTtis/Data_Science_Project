% \documentclass[10pt,letterpaper]{article}
% \usepackage{iccv}
% \usepackage{times}
% \usepackage{subfigure}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{xcolor}
% \usepackage{multirow}
% \usepackage{array}
% \usepackage{makecell}
% \usepackage{tabu, booktabs}
% \usepackage{xcolor,colortbl}
% \usepackage{newtxtext,newtxmath}
% \usepackage{enumitem}
% \usepackage{bbm}

% \definecolor{lightgreen}{rgb}{0.60, 0.78, 0.75}
% \definecolor{lightorange}{rgb}{1.0, 0.949, 0.80}
% \definecolor{verylightorange}{rgb}{1.0, 0.875, 0.502}
% \definecolor{verylightgreen}{rgb}{0.796, 0.886, 0.871}
	
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission
% % Pages are numbered in submission mode, and unnumbered in camera-ready

% \begin{document}
% {\Large \bf  }
% \author{Arushi Rai\\
% University of Pittsburgh\\
% %Institution1 address\\
% {\tt\small arr159@pitt.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until thess closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Adriana Kovashka\\
% University of Pittsburgh\\
% %First line of institution2 address\\
% {\tt\small kovashka@cs.pitt.edu}
% }
% % \date{\vspace{-5ex}}
% \maketitle

\section{Supplementary Materials}
We provide supplemental materials to our main text. 

We present a detailed table of the vetting precision and recall of all methods described in the main text, for which we show F1 performance in Table 2 of the main text. Furthermore, we show more comprehensive cross-dataset ablations, such as adding more training datasets and training with a special token. 

We discuss our hyperparameter selection for WSOD in further detail and show additional metrics of the WSOD models on the COCO-14 benchmark presented in the main text.

Finally, we showcase the vetting ability of VEIL in comparison to other approaches through qualitative results, along with additional examples from the WSOD models trained using vetted training data.
% \input{iccv2023AuthorKit/sections/data_appendix}
\section{Vetting Precision/Recall}

Table 2 in the main text showed the F1 on the extracted label vetting task, from twelve methods. In Table \ref{tab:direct_eval} here, we separately show Precision and Recall on the same task.

\input{iccv2023AuthorKit/old_files/direct_eval_table_backup}

\section{Cross-Dataset Ablations}

\input{iccv2023AuthorKit/tables/supp/cross_dataset_prec_rec}
\input{iccv2023AuthorKit/tables/supp/cross_dataset_f1}

Table \ref{tab:cross_dataset_prec_rec} is included as reference which shows that precision in the cross dataset setting is always better than no vetting with the exception of COCO.
\input{iccv2023AuthorKit/figures/supp/vetting_quals}

\textbf{Combining multiple datasets.} We find that VEIL is able to leverage additional datasets to an extent. For example, combining SBUCaps and CC leads to significant improvements (7-16\% relative) in F1 as shown in Table \ref{tab:cross_dataset_f1} and, combining SBUCaps and Redcaps in training improves performance on both validation sets. When combining all datasets, only the non-in the wild datasets see an improved performance. 

\textbf{Using special token.} We find that using VEIL w/ ST on average improves F1 by 1 pt compared to just VEIL when transferring to other datasets. This comes at a tradeoff with respect to the performance on the same dataset; however CC w/ ST improves performance on all datasets.
\begin{table*}[]
    \centering
\begin{tabular}{c|ccc|ccc} \hline
& \multicolumn{3}{c|}{ mAP, IoU }& \multicolumn{3}{c}{mAP, Area} \\\hline
& {0.5:0.95} & 0.5 & 0.75 & S & M & L\\ \hline
GT* & 4.19  & 9.17 & 3.40 & 1.10 & 4.34 & 6.76 \\ \hline
No Vetting & 3.24 & 7.70 & 2.37 & \underline{1.06} & 4.00 & 5.08 \\
Large Loss \cite{Kim2022LargeLM} & 3.11 & 7.54 & 2.15 & 0.92 & 3.80 & 4.88 \\
LocalCLIP-E \cite{Radford2021LearningTV}& 3.66 & 7.77 & 3.08 & 0.79 & 3.96 & 5.96 \\
VEIL$_{\text{ST}}$-R,CC & \underline{3.90} & \underline{8.60} & \underline{3.14} & 0.93 & \underline{4.25} & \underline{6.28} \\
VEIL-SBUCaps & \textbf{4.89} & \textbf{10.37} & \textbf{4.20} & \textbf{1.26} & \textbf{5.24} & \textbf{7.53} \\\hline
\end{tabular}
    \caption{COCO-14 benchmark for WSOD models trained with various vetting methods. (GT*) directly vets labels using the pretrained object detectors which were used to train VEIL. Bold indicates best performance in each column and underline indicates second best result in the column.}
    \label{tab:coco_all_metrics}
\end{table*}
\section{WSOD Hyperparameters}
\textbf{Learning Rates.} We trained four models without vetting on SBUCaps with learning rates from `1e-5' till `1e-2', for each order of magnitude, and observed that the model trained with a learning rate of `1e-2' had substantially better Pascal VOC-07 detection performance and used this learning rate for all the WSOD models trained on SBUCaps. We applied a similar learning rate selection method for WSOD models trained on RedCaps, except we tested over every half order of magnitude and found that `5e-5' was optimal when training on RedCaps. 

% \section{Large Loss Matters Hyperparameter}
\begin{table}[]
    \centering
    \begin{tabular}{c|c}
        Relative Delta & Pascal VOC-07 $\text{mAP}_{50}$  \\\hline
         0.002 & 28.25 \\
         0.01 & 30.93\\
         0.05 & 28.11 \\
    \end{tabular}
    \caption{Relative delta hyperparameter ablation}
    \label{tab:rel_delta}
\end{table}
\textbf{Relative Delta.} In Large Loss Matters (LLM) \cite{Kim2022LargeLM}, relative delta controls how fast the rejection rate will increase over training. To find the best relative delta, we tested over three initializations, with $rel\_delta=0.002$ as the setting recommended in \cite{Kim2022LargeLM}. We used the best result in Table \ref{tab:rel_delta} when reporting results in the main paper.
\section{WSOD Benchmarking on Additional COCO Metrics}
In our main text we compared the average precision of the model across all the classes and all the IoU (Intersection over Union) thresholds from 0.5 to 0.95. We show mAP at specific thresholds 0.5 and 0.75 in Table \ref{tab:coco_all_metrics}. We see that cross dataset VEIL vetting performs relatively 32\% better than no vetting in a stricter IoU (0.75). The mAP metric can be further broken down by area sizes of ground truth bounding boxes, which is denoted by S, M, and L. VEIL-based vetting outperforms the rest in Medium (6\% better than best non-VEIL vetting) and Large objects (5\% better than best non-VEIL vetting); while VEIL-Same Dataset still performs best on small objects, VEIL-Cross Dataset performs slightly worse than no vetting.

% \section{Weighted Sampling Hyperparameter}
% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%         Weighted Sampling & Pascal VOC-07 mAP_{50}  \\
%          0.002 & 28.25 \\
%          0.01 & 30.93\\
%          0.05 & 28.11 \\
%     \end{tabular}
%     \caption{Relative delta hyperparameter ablation}
%     \label{tab:rel_delta}
% \end{table}
% \section{Large Loss Matters Hyperparameter}
% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%         Relative Delta & Pascal VOC-07 mAP_{50}  \\
%          0.002 & 28.25 \\
%          0.01 & 30.93\\
%          0.05 & 28.11 \\
%     \end{tabular}
%     \caption{Relative delta hyperparameter ablation}
%     \label{tab:rel_delta}
% \end{table}
\section{Additional Qualitative Results}
\input{iccv2023AuthorKit/figures/supp/wsod_quals}
\textbf{Vetting Qualitative Examples.} Using annotations from CLaN, we provide qualitative examples comparing the vetting capability of methods on VAELs with common linguistic indicators (prepositional phrase, different word sense, non-literal) found in RedCaps in Figure \ref{fig:vetting_quals}.

\textbf{WSOD Qualitative Examples.} In Figure \ref{fig:structured_qual}, we present further qualitative evidence on the impact of different vetting methods on weakly supervised object detection. There are varying degrees of part and contextual bias from all methods; however, No Vetting has the most pronounced part domination and context bias as shown by its detection of bicycle wheels and car doors (top two rows), and misidentifying a child as a chair (bottom row) and detections covering both boat and water. Both VEIL methods outperform the rest of the models in detecting smaller objects (see first two rows). LocalCLIP-E misses smaller objects in the background (first two rows) and also has part domination (bicycle).
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }
% \end{document}
