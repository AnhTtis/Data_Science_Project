\begin{table}[]
    \centering
    \small
    \begin{tabular}{p{60pt}|p{52pt}|c|c}
    \hline
    Method & Train Dataset & Prec/Rec & F1 \\
    \hline
    No Vetting & - & 0.463 / 1.000 & 0.633 \\ 
    VEIL & SBUCaps & 0.828 / 0.791 & 0.809 \\ \hline
    VEIL & RedCaps (R) & 0.668 / 0.759 & 0.710 \\
    VEIL & CC & 0.585 / 0.846 & 0.692 \\
    VEIL & R, CC & 0.689 / 0.722 & 0.705 \\
    VEIL$_{\text{ST}}$ & R, CC & 0.649 / 0.797 & 0.716 \\ \hline
    % \hline
    % Alignment Model + CLIP Ensemble (max) & RedCaps, WIT \cite{Radford2021LearningTV} & 0.617 / 0.928 & 0.741 \\
    % \hline
    %  Alignment Model + CLIP Ensemble (min) & RedCaps, WIT \cite{Radford2021LearningTV} & 0.814 / 0.676 & 0.738 \\
    % \hline
    LCLIP-E & WIT  & 0.708 / 0.820 & 0.760 \\
    VEIL+LCLIP-E & R,CC,WIT& 0.733 / 0.848 & 0.786 \\ \hline
    \end{tabular}

    \caption{Source generalization of VEIL; vetting on SBUCaps. LCLIP-E is LocalCLIP-E. CLIP is trained on WIT.} %\cite{Radford2021LearningTV}.}
    % We can improve performance of alignment model by using a special token for visual presence prediction. Furthermore, ensembling with CLIP improves both precision and recall leading to a 2.6 pt increase in the F1 score; the ensemble averages the GMM alignment probability and visual presence probability from the VEIL model. compared to CLIP. Incorporating more datasets improves precision at the cost of recall, however training on multiple datasets with a special token improves recall (*) data used for pretrained CLIP model.}
    \label{tab:cross_dataset_direct_eval}
\end{table}

