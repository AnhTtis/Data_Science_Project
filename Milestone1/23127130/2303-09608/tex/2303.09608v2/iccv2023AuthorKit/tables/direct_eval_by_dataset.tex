\begin{table*}[h]
\begin{center}
\small
\begin{tabular}{>{\centering\arraybackslash}m{0.7in}|p{1.7in}|p{0.5in}|p{0.5in}|p{0.35in}|p{0.35in}|p{0.35in}|p{0.35in}|p{0.35in}|p{0.30in}}
\hline
 &  \textbf{Method} & \textbf{SBUCaps} & \textbf{RedCaps} & \textbf{CC} &  \textbf{VIST} & \textbf{VIST-DII} & \textbf{VIST-SIS}& \textbf{COCO} & \textbf{AVG}\\\cline{2-10}

& No Vetting & 0.633 & 0.747 &  0.849 & 0.853 & \underline{0.876} & \underline{0.820} & \textbf{0.973} & 0.822 \\
\hline
&Global CLIP {\scriptsize \cite{Radford2021LearningTV}} &  0.604&	0.583	&0.569	&0.668	&0.625&	0.683&	0.662& 0.628\\
\multirow{-2}{*}{Img + Lang}& Global CLIP - E {\scriptsize\cite{Radford2021LearningTV}} &  0.594 &	0.569&	0.534&	0.654&	0.613&	0.660&	0.640 & 0.609\\
 \hline
&   Local CLIP {\scriptsize\cite{Radford2021LearningTV}} &   0.347 & 0.651 & 0.363 & 0.427	&0.476	&0.418	&0.464 &0.449 \\
& Local CLIP - E {\scriptsize\cite{Radford2021LearningTV}} &\underline{0.760} & \underline{0.840} & 0.597 & 0.759	&0.695&	0.812&	0.788 &0.750 \\
\multirow{-3}{*}{ Image}& Reject Large Loss  {\scriptsize\cite{Kim2022LargeLM}}
 & 0.667 & 0.790 & 0.831&0.782	&0.794	&0.743&	0.896 &0.786 \\
  \hline
& Accept Descriptive & 0.491 & 0.413 & 0.740 & 0.687&	0.844&	0.264&	0.935 & 0.625\\
& Accept Narrative & 0.470 & 0.645 & 0.383 & 0.487 &	0.154&	0.757	&0.143 & 0.434 \\
&Reject Noun Mod. (Adj) & 0.618 & 0.703 & 0.814& 0.823	&0.847	&0.788 &	0.906 & 0.786 \\
& Reject Noun Mod. (Any)
 & 0.616 & 0.689 & 0.812 & 0.821	&0.842	&0.782 &	0.900&  0.780\\
 &  Cap2Det {\scriptsize\cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019}}
 & 0.639 & 0.758 & 0.846& 0.826&	0.854	&0.774	& \underline{0.964}& 0.809\\\cline{2-10}
& VEIL-Same Dataset & \textbf{0.809} & \textbf{0.890} & \textbf{0.909} & \underline{0.871}	&\textbf{0.892}	& 0.816 &	\textbf{0.973} & \textbf{0.884}\\
\multirow{-7}{*}{Lang}& VEIL-Cross Dataset
 & 0.716 & 0.793 & \underline{0.850} & \textbf{0.875} &	\textbf{0.892}	&\textbf{0.830} &	0.958 & \underline{0.842}\\\hline
\end{tabular}
\end{center}
\caption{Extracted Label Vetting F1 Performance. %Visual presence ground truth is estimated by an object detection ensemble, X152-C4 \cite{zhang2021vinvl} and YOLOv5-XL \cite{yolov5}, on all datasets except for COCO, where we use existing annotations. 
\textbf{Bold} indicates best performance in each column, and \underline{underlined} second-best.}
%Precision/recall values are shown in our supplementary materials.}
\label{tab:direct_eval}
\end{table*}
