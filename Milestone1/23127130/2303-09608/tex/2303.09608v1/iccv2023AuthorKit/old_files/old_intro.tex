Recent progress in self-supervised and weakly-supervised computer vision tasks has been fueled by data freely available on the web \cite{Radford2021LearningTV,wslimageseccv2018,Jia2021ScalingUV}.  Generally, weak semantic information about visual content is extracted from co-occurring text for a range of tasks from image recognition to weakly supervised object detection to vision-language grounding. Such metadata could come from user-generated content (e.g. tagging, captions) which are constructed by users to serve their own purposes, rather than constructed for pay. This marks a divide between captions written by human annotators following objective guidelines, and how people gravitate towards narratives when creating content, whether it is a caption for a photo posted on Instagram or a voice-over accompanying a documentary. 
Yet weakly supervised object detection (WSOD) methods have been supported \emph{not} by such narrative data, but by crowdsourced, paid-for datasets such as COCO and Flickr30K. Not only are these datasets of a limited scale, but the captions in these datasets often take on only descriptive styles \cite{huang2016visual}. Furthermore, they do not represent the full diversity of written language due to annotation instructions \cite{Alikhani_Stone_2019}. 

Language supervised approaches are only a proof of concept when applied to small datasets. Its potential to match and/or exceed fully-supervised methods appears when it can harness large amounts of data \cite{Jia2021ScalingUV, Radford2021LearningTV, wslimageseccv2018}. Not only has weakly supervised object detection not been applied to large-scale internet data, but there are various challenges to using such data for WSOD; training resources (WSOD has a higher memory usage than weakly supervised image recognition), extracted label noise (objects in the image missing from caption, or objects mentioned in the caption but not in the image), and diverse images (could contain a lot of atypical object instances). We attempt to take a deeper look at applying WSOD to in-the-wild data and applying existing label noise detection and correction.

Motivated by the difference between in-the-wild captions and constructed-for-pay captions, we first verify the linguistic differences between descriptive and subjective narrative styles on an existing image-caption dataset \cite{huang2016visual} with explicitly labeled descriptive and narrative captions. We hypothesize that descriptive captions are cleaner than narrative captions. We use discriminative linguistic features to construct a descriptiveness score predictor which is applied to other in-the-wild image-caption datasets, such as RedCaps \cite{Desai2021RedCapsWI}, SBUCaps \cite{Ordonez2011Im2TextDI}, and ConceptualCaptions \cite{Sharma2018ConceptualCA}, to filter out narrative captions.
\begin{figure}
    \centering
    \includegraphics[scale=0.25]{iccv2023AuthorKit/figures/images/vael_examples.png}
    \caption{\textbf{Examples of types of visually absent extracted labels (VAELs).} In-the-wild captions, depending on the source have varying types of visually absent mentioned objects in their captions. Each of these type of VAELs, signifies the importance of semantic context implied by the rest of the caption when choosing to extract the label.}
    \label{fig:vael_examples}
\end{figure}

Label noise falls into two categories: objects that appear in the image but are not mentioned in the caption and objects mentioned in the caption but don't appear in the image. The first type of noise is intuitive to understand; there are many objects that appear in the image, but not all are worth writing about (background or objects not relevant to the plot). However, the second type of noise is a little strange; after all, why would an object be mentioned if it doesn't appear in the image? This can be explained by how image-level labels are obtained from captions. Since labels are extracted based on exact match (ignoring any punctuation) with a predefined label set, there may be different senses that are used or the extracted label is related to another word (class label bolded; ``hot \textbf{dog}", ``\textbf{car} park") which may or may not be related to the class label's word sense. Additionally, the mentioned object may be a narrative artifact, and it may be implied in the caption that the object is not present as shown in Figure \ref{fig:vael_examples}. These different types of VAEL motivates considering the semantics before deciding to extract a label.

Label noise from object mentions in captions, can follow a visual pattern. The visually absent object mention (bus in ``We were waiting for the bus at the bus stop") can appear in similar visual contexts (bus stop) or the use of two similar (but distinct) objects classes interchange-ably  like mentioning “car” instead of “truck”. Additionally, polysemic object names would have visual neighbors with the same label, but not the same sense as class labels defined by object detection datasets. For example, “bed” could be a bed for a dog or bed for a human or an ``explicitly" noisy sense ``bed” w.r.t. a garden. This type of structured false positive noise is harmful for a precise localization task, like object detection as it may increase dependence on contextual cues. Empirically, we show that language-agnostic data-dependent noise correction approaches that appear to be robust to structured noise in image recognition (benefits from contextual cues), show limited improvements in localization when compared to language-conditioned noise filtering approaches.

Ultimately, we conduct an exploratory study of types of noises across diverse datasets and compare language-conditioned methods and language-agnostic noisy label detection to identify noisy labels. Specifically, our contributions are as follows:
\begin{itemize}
    \item Develop a taxonomy of image-level label noise found in in-the-wild captions.
    \item Illustrate narrative-like tendencies and other differences in linguistic structure of in-the-wild captions compared to descriptive captions.
    \item Apply and compare both new language-conditioned and existing language-agnostic label noise detection/correction approaches across constructed-for-pay image caption datasets \cite{huang2016visual, Lin2014MicrosoftCC} and a wide set of in-the-wild datasets that span sources the following: social media \cite{Desai2021RedCapsWI}, photo-sharing platform \cite{Ordonez2011Im2TextDI}, and image-alt-text pairs \cite{Sharma2018ConceptualCA}.
    \item Evaluate label noise correction techniques in their ability to correctly detect label noise and their performance on benchmarks.
    \item Our transformer-based extracted label, visual presence classifier (object-alignment model) performs better than all other label noise correction techniques in recognizing visually present extracted labels and when used to filter labels for weakly supervised object detection. The resulting object detection model shows substantial improvements on common object detection benchmarks \cite{Everingham2010ThePV, Lin2014MicrosoftCC}.
\end{itemize}





% ----- OLD TEXT

%Specifically, for the task of weakly supervised object detection, we want to determine if a mentioned object in the caption should be extracted as a label. Language provides additional context that may indicate that the mentioned object in the caption might not be visible in the corresponding image. We compare against using existing methods such as CLIP, or a simple baseline to identify this misalignment or can other language-structure conditioned heuristics perform better? We compare these methods across a wide set of in-the-wild datasets that span sources the following: social media \cite{Desai2021RedCapsWI}, photo-sharing platform \cite{Ordonez2011Im2TextDI}, and image-alt-text pairs \cite{Sharma2018ConceptualCA}. In addition to the language-conditioned approaches mentioned earlier, we compare against existing language-agnostic noisy multi-label classification techniques. These methods typically rely on the assumption that clean labels have simpler patterns that are learnt earlier in training and this knowledge can be exploited later in training. However, this either assumes that noise is weak or unstructured. Label noise from object mentions in captions, have a pattern. The visually absent object mention (bus in "We were waiting for the bus at the bus stop") can appear in similar visual contexts (bus stop) or the use of two similar (but distinct) objects classes interchange-ably  like mentioning “car” instead of “truck”. Additionally, polysemic object names would have visual neighbors with the same label, but not the same sense as object detection datasets. For example, “bed” could be a bed for a dog or bed for a human, but object detection datasets have a higher representation of the latter or an explicitly noisy sense “bed” w.r.t. a garden.  Empirically we find that the language-agnostic noisy label selection and/or correction approaches perform better than unfiltered baselines, they still have limited improvement in object localization, as opposed to object recognition, due to the structured noise. 

%The use of in-the-wild multimodal data (Conceptual Captions) has been mainly limited to tasks like caption generation and image-text or video-text grounding. Larger, diverse data have been sourced from social media like Instagram for weakly supervised image recognition \cite{wslimageseccv2018}; however, image-level labels  are inferred from Instagram hashtags – not user-provided captions. Weakly supervised object detection (also semantic segmentation) approaches have demonstrated the use of extracting labels from descriptive human-annotated captions \cite{ye_2019_cap2det}. 

%Even in vision-language grounding where diverse, noisy multimodal datasets have been used, there appear to be diminishing returns when naively increasing the dataset (by including noisy examples). For example, ALIGN \cite{Jia2021ScalingUV}, trained on 1 billion samples, shows little-to-no improvement over CLIP, trained on 400 million image-text pairs, on finetuned or zero-shot ImageNet, an image recognition benchmark. Image-text grounding from larger image-text datasets such as LAION-5B \cite{Schuhmann2022LAION5BAO} show improvements on less than half of zero-shot image recognition benchmarks despite using 10x more data than CLIP. While there may be other reasons why performance degrades or shows no improvements when using so much more data, one reason might be that there are semantic misalignments within the image and the caption. 

