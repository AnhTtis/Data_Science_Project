\begin{table*}[h]
\begin{center}
\small
\begin{tabular}{>{\centering\arraybackslash}m{0.5in}|p{1.3in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}|p{0.4in}}
\hline
\cellcolor{verylightorange} & \cellcolor{lightgreen} \textbf{Method} & \cellcolor{lightgreen}\textbf{SBUCaps} & \cellcolor{lightgreen}\textbf{RedCaps} & \cellcolor{lightgreen}\textbf{CC} & \cellcolor{lightgreen} \textbf{VIST} & \cellcolor{lightgreen}\textbf{VIST-DII} & \cellcolor{lightgreen}\textbf{VIST-SIS}& \cellcolor{lightgreen}\textbf{COCO} & \cellcolor{lightgreen}\textbf{AVG}\\\cline{2-10}

\cellcolor{verylightorange}&\cellcolor{lightorange} No Vetting & 0.633 & 0.747 &  \underline{0.849} & 0.853 & \underline{0.876} & \underline{0.820} & \textbf{0.973} & 0.822 \\
\hline
\cellcolor{verylightorange}&\cellcolor{lightorange}Global CLIP \cite{Radford2021LearningTV} &  0.604&	0.583	&0.569	&0.668	&0.625&	0.683&	0.662& 0.628\\
\multirow{-2}{*}{\makecell{Image \\+ Lang.}}\cellcolor{verylightorange}&\cellcolor{lightorange} Global CLIP - E \cite{Radford2021LearningTV} &  0.594 &	0.569&	0.534&	0.654&	0.613&	0.660&	0.640 & 0.609\\
 \hline
\cellcolor{verylightorange}&   \cellcolor{lightorange}Local CLIP \cite{Radford2021LearningTV} &   0.347 & 0.651 & 0.363 & 0.427	&0.476	&0.418	&0.464 &0.449 \\
\cellcolor{verylightorange}&\cellcolor{lightorange} Local CLIP - E \cite{Radford2021LearningTV} &\underline{0.760} & \underline{0.840} & 0.597 & 0.759	&0.695&	0.812&	0.788 &0.750 \\
\multirow{-3}{*}{\makecell{ Image}}\cellcolor{verylightorange}&\cellcolor{lightorange} Reject Large Loss  \cite{Kim2022LargeLM}
 & 0.667 & 0.790 & 0.831&0.782	&0.794	&0.743&	0.896 &0.786 \\
  \hline
\cellcolor{verylightorange}& \cellcolor{lightorange}Accept Descriptive & 0.491 & 0.413 & 0.740 & 0.687&	0.844&	0.264&	0.935 & 0.625\\
\cellcolor{verylightorange}& \cellcolor{lightorange}Accept Narrative & 0.470 & 0.645 & 0.383 & 0.487 &	0.154&	0.757	&0.143 & 0.434 \\
\cellcolor{verylightorange}&\cellcolor{lightorange}Reject Noun Mod. (Adj) & 0.618 & 0.703 & 0.814& 0.823	&0.847	&0.788 &	0.906 & 0.786 \\
\cellcolor{verylightorange}&\cellcolor{lightorange} Reject Noun Mod. (Any)
 & 0.616 & 0.689 & 0.812 & 0.821	&0.842	&0.782 &	0.900&  0.780\\
\cellcolor{verylightorange} &\cellcolor{lightorange}  Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019}
 & 0.639 & 0.758 & 0.846& 0.826&	0.854	&0.774	& \underline{0.964}& 0.809\\\cline{2-10}
\cellcolor{verylightorange}&\cellcolor{lightorange} VEIL-Same Dataset & \textbf{0.809} & \textbf{0.890} & \textbf{0.909} & \underline{0.871}	&\textbf{0.892}	& 0.816 &	\textbf{0.973} & \textbf{0.884}\\
\multirow{-7}{*}{\makecell{Lang.}}\cellcolor{verylightorange}&\cellcolor{lightorange} VEIL-Cross Dataset
 & 0.716 & 0.793 & 0.828 & \textbf{0.875} &	\textbf{0.892}	&\textbf{0.830} &	0.958 & \underline{0.842}\\\hline
\end{tabular}
\end{center}
\caption{Extracted Label Vetting F1 Performance. Visual presence ground truth is estimated by an object detection ensemble, X152-C4 \cite{zhang2021vinvl} and YOLOv5-XL \cite{yolov5}, on all datasets except for COCO, where we use existing annotations. \textbf{Bold} indicates the best performance in each column, and \underline{underlined} denotes the second-best performance.}
%Precision/recall values are shown in our supplementary materials.}
\label{tab:direct_eval}
\end{table*}
