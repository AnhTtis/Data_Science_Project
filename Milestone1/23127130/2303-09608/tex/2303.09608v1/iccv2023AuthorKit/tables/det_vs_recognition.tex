\begin{table}[t] %[h]
\begin{center}
\small
\begin{tabular}{p{1.15in}|p{0.60in}|p{0.60in}|p{0.3in}}
\hline
\textbf{Method (Train Data Size in Thousands)} &\textbf{VOC Det. mAP} ($\Delta$) &\textbf{VOC Rec. mP} ($\Delta$) & \textbf{COCO 0.5:0.95 mAP} \\
\hline
No Vetting  (19) & 16.0 \textcolor{red}{(-45\%)} & 58.0 \textcolor{red}{(-15\%)} & 1.9 \\
GT* (17) & 8.2 \textcolor{red}{(-72\%)} & \underline{77.8} \textcolor{green}{(13\%)} & 1.2\\
Large Loss \cite{Kim2022LargeLM} (19) & 18.1 \textcolor{red}{(-38\%)} &  63.2 \textcolor{red}{(-8\%)} & 1.7 \\
LocalCLIP-E \cite{Radford2021LearningTV} (18) & 11.0 \textcolor{red}{(-62\%)} & \textbf{81.9} \textcolor{green}{(19\%)} & 1.4\\
VEIL$_{\text{ST}}$-R,CC (18) & \underline{26.8} \textcolor{red}{(-8\%)} & 61.2 \textcolor{red}{(-11\%)} & \underline{2.9} \\
VEIL-SBUCaps (16) & \textbf{29.1} (--) & 68.3 (--) & \textbf{3.1} \\
\hline
\end{tabular}
\end{center}
\caption{Impact of vetting on WSOD performance on VOC-07 and COCO-14 datasets. There is a significant difference in detection and recognition on VOC-07 illustrated by $\Delta$, relative performance change w.r.t. VEIL-SBUCaps on the same column. This highlights that VEIL variants filter out labels harmful to localization. (GT*) directly vets labels using the pretrained object detectors which were used to train VEIL.} 
\label{tab:det_v_recognition}
\end{table}