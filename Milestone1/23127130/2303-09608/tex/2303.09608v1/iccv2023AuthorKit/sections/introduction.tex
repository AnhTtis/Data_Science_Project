\section{Introduction}

Freely available vision-language (VL) data has shown great promise to advance vision tasks \cite{Radford2021LearningTV,wslimageseccv2018,Jia2021ScalingUV}. It provides a scalable way to learn image recognition and cross-modal representations but it is not directly fitting for localization tasks like object detection due to large amounts of noise found in pseudo-labels extracted from captions. Thus, weakly supervised object detection has primarily been applied to smaller-scale paid-for crowdsourced vision-language datasets like COCO \cite{Lin2014MicrosoftCC} and Flickr30K \cite{Young2014Flicker30K}.  

A critical challenge with larger-scale VL datasets is that their captions contain sparse positive signal which is further diluted by objects mentioned in the caption that do not appear in the corresponding image, as shown in Figure \ref{fig:vael_examples}. We define this as a visually absent extracted label (VAEL) and find there are \emph{diverse} reasons for the presence of VAELs are found in the in-the-wild VL datasets.
%(label is a subset of a noun phrase: ``\textbf{oven} mitts"), 
%but each dataset has different distributions of noise types due to differences in user behavior on those dynamic platforms. 
For example, RedCaps \cite{Desai2021RedCapsWI} and SBUCaps \cite{Ordonez2011Im2TextDI} contain VAEL noise due to additional narrative context provided by the user, such as non-visualized context from when the picture was taken or previous activities done. RedCaps \cite{Desai2021RedCapsWI}, specifically, contains difficult to detect non-literal object mentions (``diaper \textbf{cake}", a decoration made for a baby shower) that may be partially associated with the label, but not necessarily the visual aspect. This is because users are motivated by recognition (upvotes) and engagement within their niche communities rather than sharing literal descriptions of their photos. 

Adaptive methods applied during training to either reject or correct label noise assume that noisy labels will be memorized  \cite{Zhang2016UnderstandingDL} and in later epochs, invoke the largest losses \cite{Kim2022LargeLM, Sun2022PNPRL, Sun2020CRSSCSR, Yao2021JoSRCAC, Wei2020CombatingNL}.
%or exploit label consistency across visual neighbors \cite{Iscen2022LearningWN}. 
This is not fitting for the above-mentioned datasets as the noise is \emph{structured}: it has a visual pattern. For example, visually absent labels extracted from noun phrases like "\textbf{car} engine" or prepositional phrases "under the \textbf{car"} will likely be visually similar to other examples described by the same phrase. 

\input{iccv2023AuthorKit/figures/fig1}

Instead, we propose \textbf{VEIL}, short for \textbf{V}etting \textbf{E}xtracted \textbf{I}mage \textbf{L}abels to directly learn whether a label is clean or not from caption context, without requiring additional annotation for this task. We first extract potential labels from each caption using substring matching or exact match \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019, Fang2022DataDD}, which we refer to as ``extracted labels''. We then use a transformer to 
% represent each caption, and train it to
predict whether each extracted label is visually present or absent. We refer to this prediction task as extracted label vetting (ELV or vetting for short). 

We bootstrap labels from an ensemble of two pretrained object detectors \cite{yolov5, zhang2021vinvl}, to predict pseudo ground truth visual presence labels on a variety of large-scale, noisy datasets: Conceptual Captions \cite{Sharma2018ConceptualCA}, RedCaps \cite{Desai2021RedCapsWI}, and SBUCaps \cite{Ordonez2011Im2TextDI}. While 
%the YOLOv5 \cite{yolov5} and X152-C4 \cite{zhang2021vinvl} 
these detectors are trained on COCO \cite{Lin2014MicrosoftCC}, 
%the latter is trained additionally on
VisualGenome \cite{Krishna2016VisualGC}, OpenImages \cite{Kuznetsova2018DatasetV4}, and Objects365 \cite{Shao2019Objects365AL}, they generalize well to estimating extracted label visual presence on the VL datasets we used. Our trained vetting model also generalizes well across dataset boundaries. Importantly, it shows promise in generalizing across category boundaries. This suggests VEIL can be applied to class-agnostic vetting, which would be trained on a set of base categories but evaluated on held-out novel categories. Once we vet the extracted labels, we use them to train a weakly-supervised object detection model. 

We compare our method to eleven baselines, including standard cross-modal alignment prediction methods (CLIP), adaptive noise reduction methods, pseudo-label prediction, simple rule-based methods, and no vetting. We show that our method improves upon the baselines both in terms of predicting extracted label visual presence (measured with F1) and producing cleaner training data for object detection leading to improvement of +11 mAP  over Large Loss Matters \cite{Kim2022LargeLM} and +18 mAP improvement over using CLIP \cite{Radford2021LearningTV} for filtering \cite{Radford2021LearningTV}, when we train on a subset of SBUCaps.
 
Our contributions are as follows:
\begin{itemize}[nolistsep,noitemsep]
    \item Our transformer-based extracted label, visual presence classifier (\textbf{VEIL}) performs better than all other label noise reduction techniques in recognizing visually present extracted labels and when used to vet labels for weakly supervised object detection. The resulting detection model shows substantial gains on common object detection benchmarks \cite{Everingham2010ThePV, Lin2014MicrosoftCC}. Furthermore, even when VEIL is trained on one dataset, but applied to another dataset there are similar gains in WSOD. It also shows potential to generalize across labels.
    \item We analyze the types of noise, and apply and compare both new language-conditioned and existing language-agnostic label noise detection/correction approaches across constructed-for-pay image caption datasets \cite{huang2016visual, Lin2014MicrosoftCC} and a wide set of in-the-wild datasets whose sources span social media \cite{Desai2021RedCapsWI}, photo-sharing platforms \cite{Ordonez2011Im2TextDI}, and image-alt-text pairs \cite{Sharma2018ConceptualCA}.
    %\item We analyze the types of noise, e.g. narrative-like tendencies and other differences in linguistic structure, of in-the-wild captions compared to descriptive captions.
    \item We show adaptive label noise correction methods %in their ability to correctly detect label noise and their performance on \textit{object detection} benchmarks which 
    show shortcomings in localization as opposed to recognition.
\end{itemize}