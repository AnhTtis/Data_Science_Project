\section{Experiments}
\label{sec:expts}

\input{iccv2023AuthorKit/tables/direct_eval_by_dataset.tex}
% \input{iccv2023AuthorKit/tables/indirect_evaluation.tex}
%\input{iccv2023AuthorKit/figures/cross_category}

%This section describes general experiment setup details followed by experiments that 
We show the ability of VEIL %and other language conditioned vetting methods 
to match and exceed language-agnostic filtering and image-based filtering methods in the task of extracted label vetting (ELV). Next, we highlight experiments showing the promising ability of VEIL to vet noisy extracted labels prior to weakly supervised object detection training as well as a critical ability to remove structured noise; this is important because structured noise decreases localization ability in WSOD. Lastly, we include experiments benchmarking the generalization ability of VEIL in cross-dataset and cross-label settings.

\subsection{Experiment Details and Baselines}

\textbf{Datasets.}  As described in Sec.~\ref{sec:analysis}, we use three in-the-wild image-caption datasets (SBUCaps \cite{Ordonez2011Im2TextDI}, RedCaps \cite{Desai2021RedCapsWI}, Conceptual Captions \cite{Sharma2018ConceptualCA}) and three human annotation datasets that fall into descriptive datasets (COCO \cite{Lin2014MicrosoftCC}, VIST-DII \cite{huang2016visual}) and a specific narrative dataset (VIST-SIS \cite{huang2016visual}) for our experiments. 
Each of the in-the-wild datasets and VIST are first reduced to a subset of image-caption pairs where a COCO category is explicitly mentioned in the caption. This reduced subset is split into train and test following a 80-20 split. 
The WSOD models are trained on SBUCaps with labels vetted by different methods; these are evaluated on PASCAL VOC 2007 - test \cite{Everingham2010ThePV} and COCO val 2014 \cite{Lin2014MicrosoftCC}. The mAP metric evaluates both classification and localization ability of the WSOD models at $IOU=0.5$ unless specified otherwise. However, when contrasting the image-level classification performance and detection performance we refer to them as Recognition mP (mean precision) and Detection mAP, respectively.

%We will also describe the implementation details of other methods we compare against.
%\textbf{Implementation Details for Vetting Methods}

\textbf{Methods compared.}
Next, we describe the methods we compare against. We group these into language-based, image-based, and image-language methods. They are category-agnostic (except for Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019} and LLM \cite{Kim2022LargeLM} which must be applied on closed vocabularies). 
\\\textbf{No Vetting.} Accept all extracted labels (\emph{perfect recall}).
\\\textbf{Global CLIP and CLIP-E.} We use the ViT-B/32 pretrained CLIP \cite{Radford2021LearningTV} model. To enhance alignment \cite{Hessel2021CLIPScoreAR}, we add the prompt ``A photo depicts" to the caption and calculate the cosine similarity between the image and text embeddings generated by CLIP. Since the cosine similarity distribution varies per dataset, we train a Gaussian Mixture model with two components on the train datasets and select image-text pairs predicted to the component with higher visual-caption alignment. 
% Global CLIP filters based on the whole image and caption, even if there are visually present extracted labels. 
For the ensemble variant (CLIP-E), we curate multiple prompts, prepend them to the caption,
% or extracted label, %and calculate the cosine similarity between image-text CLIP embeddings, and lastly 
and use the score from the highest-scoring prompt.
\\\textbf{Local CLIP and CLIP-E} follow a similar process but use cosine similarity between the image and the prompt ``this is a photo of a" followed by the \textbf{extracted label}. Extracted labels are filtered by Local CLIP, not entire captions, making this image-conditioned, not image-language conditioned vetting like Global CLIP. Local CLIP-E ensembles prompts.
\\\textbf{Reject Large Loss.} LLM \cite{Kim2022LargeLM} is language-agnostic adaptive noise rejection and correction method. To test its ELV ability, we simulate five epochs of WSOD training \cite{bilen2016weakly} and consider label targets with a loss exceeding the large loss threshold as ``predicted to be visually absent" after the first epoch. The large loss threshold uses a relative delta hyperparameter controlling the rejection rate (set as 0.002 in \cite{Kim2022LargeLM}).
\\\textbf{Accept Descriptive / Narrative.} We train a logistic regression model to predict whether a VIST \cite{huang2016visual} caption comes from the DII (descriptive) or SIS (narrative) split. The input vector to this logistic regression model contains binary variables corresponding to the presence of a part of speech vector (e.g. proper noun, adjective, verb - past tense, etc) in the caption. We accept extracted labels from a caption if it yields a score higher than 0.5 for descriptiveness or narrativeness, respectively. 
%We use the part-of-speech (POS) based classifier trained on VIST-DII (descriptive proxy) and VIST-SIS (narrative proxy).
\\\textbf{Reject Noun Mod. (Adj/Any).} Since a category could be mentioned in a noun phrase (``\textbf{car} park"), a very simple baseline would reject labels in noun modifier. We try two variants, the first noun modifier rule rejects an extracted label if the POS label is an adjective or is followed by a noun. The second rule rejects if the extracted label is not a noun.
\\\textbf{Cap2Det.} We reject a label if it is not predicted by the Cap2Det \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019} classifier.

\subsection{Extracted Label Vetting Evaluation}

%We evaluate the ability of VEIL and other intuitive approaches to vet extracted labels for visual presence on each data source's test split. First, we establish a simple baseline by examining performance without vetting. Then, we'll compare the vetting methods, which are arranged into three categories: language-conditioned, image-conditioned, and image and language conditioned. Note that VEIL-Same Dataset is the only approach trained on the same source's train split and Large Loss Matters \cite{Kim2022LargeLM} is simulated on the test split. 

Table \ref{tab:direct_eval} shows the F1 score which combines the quality (precision) and recall of their vetting; precision and recall are shown separately in supp. %Language-based approaches use caption input and accept labels based on some scoring mechanism, such as descriptiveness, narrativeness, Cap2Det \cite{ye_2019_cap2det}, and VEIL or a discrete output like the noun modifier-based rules. 
Most language-based methods improve or maintain the F1 score of No Vetting, even though it has perfect recall, except for Accept Descriptive/Narrative.
% In supp, we show that Accept Descriptive improves precision in descriptive-leaning datasets (CC, COCO, VIST, VIST-DII), but its F1 significantly drops in narrative-leaning datasets (SBUCaps, RedCaps, VIST-SIS) mainly due to recall since narrative captions still contain VPEL. 
Rule-based methods and Cap2Det perform quite strongly.
However, they are outperformed by both VEIL-Same Dataset (trained and tested on the same dataset, without using a special token) and VEIL-Cross Dataset (trained on a different dataset than that shown in the column; we show the best cross-dataset result). 
%, however the second rule is too strict. There is still room for improvement as shown by the improvement in performance when compared with VEIL-Same Dataset and VEIL-Cross Dataset. This may be because visually absent labels that fall under narrative artifact categories (see Figure \ref{fig:label_noise_taxonomy}) which cannot be detected by this simple rule. Cap2Det transfers and boosts precision in out-of-domain settings while preserving recall. Overall, VEIL-Same Dataset has the highest precision and F1 in all web-sourced datasets. 
VEIL-Cross Dataset outperforms language-based approaches,
%in precision (in supp) and F1, 
indicating potential for generalization across datasets, except on CC and COCO where Cap2Det does slightly better.

Image-and-language-conditioned approaches (Global CLIP/CLIP-E) make label decisions based on the global (overall) caption, so if the caption contains certain language, it can affect the alignment between the image and caption, even if the object is actually visually present. Table \ref{tab:direct_eval} shows these methods obtain low F1 scores; while they improve precision compared to No Vetting (shown in supp), recall suffers terribly.

Among image-based approaches for label vetting, we observe that Local CLIP benefits significantly from using an ensemble of prompts compared to Global CLIP; ensembling is well documented in improving zero-shot image recognition in prior work \cite{Radford2021LearningTV}. % Figure 4 in that paper shows the improvement 
%However, the precision may suggest that LocalCLIP-E sometimes hallucinates an object or the ground truth may be missing an object detection (inaccurate visual presence labels). 
Reject Large Loss has the strongest F1 score among the image-based methods, and in supp we show it has strong recall but limited precision improvement over No Vetting, indicating the presence of false positives that do not lead to a large loss. 
%While Large Loss Matters was designed for correcting false negatives, it is part of a family of noisy label learning methods that use small-loss to select clean samples, so we are still motivated to use this framework as it is a general method.

Since image-based approaches for label vetting work quite well, we also tested how much pseudo-labeled data it would take for VEIL-SBUCaps to do better than the image-based methods. Our results shown in Figure \ref{fig:train_subset} (left) indicate only a small amount (25K) is needed to do better in terms of F1 compared to LLM, but more data (75K) is needed to improve F1 over Local CLIP-E. However, all this data is pseudo-labeled so it does not require any additional cost. 

\input{iccv2023AuthorKit/tables/det_vs_recognition}
\subsection{Impact on Weakly Supervised Object Detection}

%\textbf{Comparison between High Performing Methods in Extracted Label Vetting}
We select the most promising vetting methods from the previous section and use them to vet labels from the SBUCaps \textit{test} split since CLIP-based and VEIL-based methods use the train set. Note that Large Loss Matters (LLM) \cite{Kim2022LargeLM} has been relaxed to also \textit{correct} visually absent extracted labels, instead of only unmentioned objects, that are visually present (false negative noise).
%given each caption. 
We remove any images without labels. The category distribution follows a long-tail distribution, so we balance each vetted subset such that each category is equally represented (250 examples minimum) through over- and undersampling. We train MIST \cite{ren2020instance} for 50K iterations with a batch size of 8 for each method. 
We find that VEIL-SBUCaps performs the best despite using the least amount of data as shown in Table \ref{tab:det_v_recognition}.
In particular, it boosts the detection performance of No Vetting by 13.1\% absolute and 80\% relative gain (29.1/16.0\% mAP) on VOC-07 and by 63\% relative gain (3.1/1.9\% mAP) on COCO.
Interestingly, VEIL-SBUCaps and VEIL$_{\text{ST}}$-Redcaps,CC both seem to have a similar performance improvement, despite VEIL$_{\text{ST}}$-Redcaps,CC (best VEIL cross-dataset result on SBUCaps) having poorer performance than Local CLIP-E in Table \ref{tab:cross_dataset_direct_eval}. Additionally, there is a significant difference in detection performance between the VEIL and image-conditioned vetting methods \cite{Radford2021LearningTV, Kim2022LargeLM} and even directly using the pretrained object detector ensemble (used to produce visual presence targets for VEIL) predictions to vet (GT* method in the table). This suggests VEIL \textbf{generalizes from its bootstrapped data} to perform significantly better on weakly supervised object detection. 
% From the large detection performance gap between VEIL and the image-conditioned vetting methods, not all noise has the same impact on weakly supervised object detection.

\textbf{Localization vs. Recognition.}
We can see that even though Reject Large Loss and Local CLIP-E have strong vetting results (Table \ref{tab:direct_eval}) they perform poorly in weakly supervised object detection (Table \ref{tab:det_v_recognition}). We hypothesize there might be differences in detection and recognition results, as recognition may benefit from some types of noisy data, such as a specific type of structured noise where context is shared between images with the VAEL and examples of actually containing that category.
%since it can still benefit from shortcutting. 
When evaluating the precision of image-level predictions on Pascal VOC 2007 \cite{Everingham2010ThePV}, we observe that Local CLIP-E does do better when it comes to recognition ability (+19\% relative to VEIL-SBUCaps), but significantly worse in detection (-62\% relative to VEIL-SBUCaps). Similarly, we observe only an 8\% drop in recognition performance for Reject Large Loss compared to VEIL-SBUCaps but a 38\% drop in detection. VEIL$_{\text{ST}}$-Redcaps,CC is weaker than VEIL-SBUCaps in vetting SBUCaps data, but has a similar performance drop in both detection and recognition. This indicates VEIL vets out noisy labels harmful to localization, even when trained on different sources, and its accepted labels are better for learning localization.

\input{iccv2023AuthorKit/figures/structured_noise_qual}

We show examples in Fig.~\ref{fig:structured_qual} containing correct image-level predictions from the LLM-trained WSOD model and the VEIL-SBUCaps-vetted WSOD model. We show recurring examples across categories where parts of the object (car engine) or background context (water or buildings) are detected as the object by LLM. This appears to be the effect of structured noise in training where labels are applied to images that do not fully contain the object. Related noun phrases from the captions (``\textbf{car} engine", ``\textbf{bus} stop") and prepositional phrases (``on \textbf{boat}", not shown due to space) are specific structured noise types that would share visual contents across images. These parts and contextual image regions are exactly what the LLM WSOD model detects as the object while our method captures the actual object with high confidence.

\input{iccv2023AuthorKit/figures/train_subset_figures.tex}

\input{iccv2023AuthorKit/tables/cross_dataset_CLIP.tex}
\input{iccv2023AuthorKit/tables/cross_category.tex}


\subsection{VEIL Generalizability}
In addition to bootstrapping pseudo-labeled cleanliness data on the same source, we also test VEIL's ability to generalize to unseen sources and categories. Table \ref{tab:direct_eval} shows that VEIL has an ability to generalize between datasets. Across the language-conditioned approaches, VEIL-Cross Dataset exceeds F1 on most datasets except for CC and COCO, where it is still competitive. In supp, we show increased precision over all approaches. 
%Differences in performance might come from higher or lower prevalence of certain VAEL noise types; for example, RedCaps contains more non-literal narrative artifacts than SBUCaps. 
%However, the main takeaway is that across all datasets precision improves significantly compared to no vetting and F1 performance is competitive and better on noisier datasets (improvement in terms of precision overcomes drop in recall). 
%Even though the labels are cleaner, we may still prefer to have better recall so that we can utilize as much data as possible from these datasets. 

\textbf{Cross Dataset Source Generalization.}
We train VEIL on one dataset (or multiple) and evaluate on an unseen target. We find that combining multiple sources improves precision in Table \ref{tab:cross_dataset_direct_eval}. To better utilize caption context, we test VEIL$_{\text{ST}}$ which predicts visual presence using a special token {\tt [EM\_LABEL]} as described in Section \ref{sec:veil}. We find that this improves F1 performance. Lastly, we try ensembling by averaging predictions between LocalCLIP-E and VEIL-Cross Dataset, and find that its precision and recall is highest among the VEIL variants and LocalCLIP-E. This means that VEIL and LocalCLIP-E can be used together. 
There is still a significant gap between VEIL-Same Dataset and even the ensembled model in terms of precision and F1. We leave improving source generalizability to future research.

\textbf{Cross-Category Generalization.} We set this up by having an in-domain category set (ID) consisting of 20 randomly picked categories from COCO \cite{Lin2014MicrosoftCC}, and an out-of-domain category set of 20 randomly picked categories (OOD) from the remaining unpicked categories. We restrict the labels using these limited category sets and create two train subsets, ID and OOD from SBUCaps \textit{train}  and one ID test subset from SBUCaps \textit{test}. We use the VEIL variants as described in Section \ref{sec:veil}, VEIL$_{\text{ST}}$ and random masking the extracted label tokens in the caption. 
We find that naively transferring VEIL-OOD to unseen categories vets extremely strictly (high precision, very low recall) as shown in Table \ref{tab:cross_category}.
%, compared to no vetting and even VEIL-ID. 
Randomly masking with $p_m=$ 0.15, 0.25 or 0.50 leads to higher F1 performance. The special token variant improves recall significantly (compare $p_m = 0.0$ with/without ST), however a gap remains between no vetting and ID in terms of F1. %Note that this is testing category generalization when the model has only been trained on 20 object classes, so 
We hypothesize training on more categories could improve category generalization.

\textbf{Connection to WSOD.}
To see if precision plays a larger role and can make up for a lower F1 (compared to no vetting) we train MIST \cite{ren2020instance} on the ID test split and compare between no vetting, VEIL-ID and VEIL$_{\text{ST}}$-OOD. In Figure \ref{fig:train_subset} (right), we show that in terms of the median performance (solid lines), VEIL-OOD does better than No Vetting. In terms of mean performance (dashed lines), VEIL-OOD is slightly lower than VEIL-ID. This shows the promise of applying VEIL across category boundaries.
We also used the data from this experiment to compute a correlation between ELV precision/recall, and WSOD mAP results. The only statistically significant correlation we found was for ELV precision and mAP in the OOD setting (but not for recall and mAP), confirming in yet another way the importance of studying precision of extracted labels and VAEL noise.

\textbf{Conclusion.}
We established that visually absent extracted labels are more prevalent in in-the-wild datasets compared to datasets commonly used for detection pretraining such as COCO. We proposed VEIL which uses only language context to infer whether mentioned objects are visually present in the image. We show that this type of model significantly outperforms other vetting strategies in detection performance. We posit that VEIL can be used to effectively sample localization critical samples and help WSOD use noisier in-the-wild image-caption data.
% \textbf{Ethics note.}



