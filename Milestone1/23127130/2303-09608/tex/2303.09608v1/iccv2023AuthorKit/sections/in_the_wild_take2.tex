\section{Analysis: Descriptive vs In-the-wild captions}
\label{sec:analysis}

We analyze what makes three large-scale in-the-wild datasets a challenging source of labels for object detection methods.
We begin with a case study of the VIST dataset \cite{huang2016visual} which contains both descriptive and narrative captions for the same image (presumably cleaner/noisier, respectively). We then examine broader noise patterns extending beyond narrativeness, in three additional datasets. 

\subsection{Datasets Analysed}
\label{sec:datasets}

\textbf{COCO} \cite{Lin2014MicrosoftCC} contains cleanly crowdsourced captions, and stands in for the status quo in weakly supervised object detection since the cost of collecting COCO does not scale well.
Visual Storytelling (\textbf{VIST}) \cite{huang2016visual} contains three styles of captions, of which we use two: \textbf{D}escriptions of \textbf{I}mages in \textbf{I}solation (DII), which are similar to COCO captions, 
%commonly used for pretraining for object detection \cite{Lin2014MicrosoftCC,Young2014Flicker30K}, 
and \textbf{S}tory for \textbf{I}mages in \textbf{S}equence (SIS), similar to in-the-wild captions. 
Each image contains both DII and SIS captions, which allows us to observe the impact of caption style while keeping the image constant.
Conceptual Captions (\textbf{CC}) \cite{Sharma2018ConceptualCA}, and the remaining two datasets, are collected from in-the-wild data sources. CC contains 3 million image-alt-text pairs after heavy post-processing; named entities in captions were hypernymized and image-text pairs were accepted if there was an overlap between Google Cloud Vision API class predictions and the caption.
\textbf{RedCaps} \cite{Desai2021RedCapsWI} is the largest dataset used in our paper, consisting of 12M image-text pairs collected from Reddit by crawling a manually curated list of subreddits with heavy visual content.
SBU Captioned Photo Dataset (\textbf{SBUCaps}) \cite{Ordonez2011Im2TextDI} consists of 1 million Flickr photos with associated text descriptions written by their owners.
%on the photo-sharing platform. 
%To construct this dataset, 
Only captions with at least one prepositional phrase and at least 2 matches with a predefined vocabulary were accepted.

The in-the-wild datasets exhibit very low precision of the extracted labels. The precision ranges from 0.463 for SBUCaps, 0.596 for RedCaps, to 0.737 for Conceptual Captions, all much lower than the 0.948 for COCO. This motivates our exploration of VAEL noise.

\subsection{Case Study: Descriptiveness/Narrativeness}
\label{sec:ling_diffs}

% To begin our analysis, 
We use VIST, which contains caption annotations that could be considered a proxy for the quality of labels that could be extracted from corresponding captions. We consider descriptive captions to contain higher-quality labels than narrative captions. We examine some linguistic differences between those captions and then examine the ratio of descriptiveness vs narrativeness in the in-the-wild datasets. We also compare how the descriptive-vs-narrative distinction relates to visual-text alignment. We show that while useful for distinguishing aligned and misaligned images and text on VIST, this distinction is insufficient on other datasets. In Sec.~\ref{sec:expts}, we also show that a descriptiveness classifier does not perform competitively to other techniques for vetting.  This observation means that there are many different types of label noise beyond narrativeness, and we categorize them in a taxonomy, in Sec.~\ref{sec:taxonomy}. It also motivates our development of a technique that can capture broader types of label noise, namely VEIL (Sec.~\ref{sec:veil}).

\textbf{Part of Speech (POS) differences.}
We extract POS tags over all VIST captions using SpaCy \cite{spacy2} and observe differences in pronoun usage and verb tense between descriptive and narrative captions. The increased use of pronouns in SIS suggests a deviation from an impersonal, objective tone. For the same image, one SIS caption says ``\textbf{we} finally arrive at the island" while a DII caption provides count and detailed information: ``\textbf{a group of four men} sitting together".
We also find that present verb tense is much more common in descriptive captions (81\% in DII vs 47\% in SIS) while past tense is more common in narrative captions (19\% in DII and 53\% in SIS). 
DII contains mostly progressive captions (95\%) meaning that it describes ongoing activities more likely to be present in the accompanying visual scene, and thus captions are more visually aligned. Nouns and prepositions are also much more common in DII than SIS. 
In the supplementary file, we show further details about the linguistic distinctions in descriptive and narrative captions.

\input{iccv2023AuthorKit/tables/clip_in_the_wild}

\textbf{Descriptiveness ratio in in-the-wild datasets.}
We label 100 captions in each of SBUCaps, RedCaps, Conceptual Captions and COCO, as having a descriptive or narrative style. We define ``descriptive'' style as the caption describing important parts of the image and using objective language. We define ``narrative'' as having subjective elements, first-person perspective (``I", ``my", etc), using non-literal expressions (``it's a piece of cake"), or assigning intent (``owner shows off her dog"). While our criteria are based on VIST-SIS, they are inclusive of captions beyond those narrative examples.
We found RedCaps contains the most narrative-like captions (62\%), SBUCaps contains 37\%, and CC has 33\% (lowest amongst in-the-wild datasets), and COCO has 5\%. 
This is understandable because RedCaps is from a social media source, where people write in a story-like fashion to communicate and connect with others. In contrast, Conceptual Captions aim to convey the image via a description to visually impaired users. 

While narrativeness is certainly present in these in-the-wild datasets, clean/noisy data need not perfectly align to descriptiveness/narrativeness. We thus assess if the descriptiveness/narrativeness proxy is useful on these in-the-wild datasets by measuring statistics on visual-text alignment. We observe in Table \ref{tab:clip_in_the_wild} that descriptive and narrative captions in VIST are well-separated in terms of alignment. Descriptive captions in SBUCaps, RedCaps, and Conceptual Captions have slightly higher visual-text alignment than narrative ones, but the gap is reduced and is only significant 
on RedCaps (p=0.05). This implies that factors beyond descriptive or narrative drive lower visual-text alignment in these datasets.
%We then test our part-of-speech descriptiveness classifier and show that this classifier has higher precision on descriptive captions in most datasets except RedCaps in our supplementary materials; however, it struggles with correctly identifying narrative captions. 
%Part-of-speech may not fully capture the narrative criteria in our annotation protocol. For example, ``interesting" (an adjective) indicates subjectivity, but ``red" (also adjective) is concrete. 
%Despite these limitations, we still employ the descriptiveness classifier as a vetting baseline. 
This issue has motivated us to investigate label noise in both descriptive and narrative captions more directly, and propose VEIL, which extracts more comprehensive patterns of noise.
%, as described in the following section.
