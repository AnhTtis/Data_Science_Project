\section{Related Work}

\textbf{Vision-language datasets.} 
%The visual data in these datasets are typically either images or videos. For text accompanying images, 
Common datasets \cite{Young2014Flicker30K, Lin2014MicrosoftCC, huang2016visual} contain high-level multiple crowdsourced captions per image, including dense captions \cite{Krishna2016VisualGC}. 
Alt-text written by users to aid visually impaired readers \cite{Sharma2018ConceptualCA,Changpinyo2021Conceptual1P,Radford2021LearningTV,Schuhmann2021LAION400MOD} is widely used for vision-language grounding due to its abundance and assumed visual alignment \cite{Schuhmann2021LAION400MOD,Radford2021LearningTV, zareian2021open, regionclip}. There are also large vision-language datasets sourced from social media like Reddit posts \cite{Desai2021RedCapsWI} and user-uploaded captions for publicly shared photos on Flickr \cite{Ordonez2011Im2TextDI}. 
%Even though both alt-text and crowdsourced captions are intuitively descriptive compared to social media sourced captions, 
Prior work \cite{Alikhani2019CaptionAA} shows that there are still linguistic differences between captions produced from crowdsourcing (COCO, Flickr30k, VIST) and online in-the-wild captions (Conceptual Captions, RecipeQA). Building on this, we show that the narrative element found in in-the-wild corpora can impact the ability to successfully train an object detection model, and our method, VEIL, reduces the contribution of this noise.
%Building on this, we show that the narrative element found in in-the-wild corpora can impact the ability to successfully train an object detection model, and the contribution of this noise can be reduced using VEIL.
%Multimodal datasets commonly include vision and language data, but differ in the form each modality is presented; for example, the vision modality can be presented via static images \cite{Schuhmann2021LAION400MOD,Radford2021LearningTV,wslimageseccv2018}, images in a sequence \cite{huang2016visual,Yagcioglu2018RecipeQAAC}, or long \cite{Miech2019HowTo100MLA,Chen_2017_CVPR} or short videos \cite{Tang2021ComprehensiveIV}. Similarly, language in multimodal datasets can also be represented in diverse forms such as procedural (instructional) language \cite{Miech2019HowTo100MLA,Yagcioglu2018RecipeQAAC,Hessel2019UnsupervisedDO}, narrative-like language
%\cite{huang2016visual,Chen_2017_CVPR,Biten2019GoodNE,Huang2020MovieNetAH,Thomas2019PredictingTP, Ordonez2011Im2TextDI, Desai2021RedCapsWI}, user-written alternative text \cite{Sharma2018ConceptualCA,Radford2021LearningTV,Lin2014MicrosoftCC,Young2014FromID}.
%Typically, instructional videos have been used for learning visual-textual grounding \cite{Miech2019HowTo100MLA,Miech2020EndtoEndLO}; Hessel \etal \cite{Hessel2020BeyondIV} studied the extension to using \textit{diverse non-instructional videos} for visual-textual grounding. Similarly part of our work studies the effect of \textit{extracting labels} from descriptive or narrative captions for weakly supervised object detection, rather than a multimodal retrieval-based notion of grounding. We also go beyond studying differences in object detection performance when extracting labels from descriptive or narrative captions and generally screen for false positive extracted label noise.

\textbf{Weakly-supervised object detection} (WSOD) is a multiple instance learning problem to train a model to localize and classify objects from image-level labels \cite{bilen2016weakly,tang2017multiple,wan2019cmil,gao2019cmidn,tang2017multiple,ren2020instance}.
The first work to leverage unstructured text accompanying an image for WSOD, Cap2Det, predicted pseudo image-level labels from captions \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019, Unal2022LearningTO}.
%Cap2Det trains a multi-label text classifier to predict a pseudo label at the image level for WSOD, and uses these labels to train a WSOD model. 
However, Cap2Det cannot operate across categories as it requires observing training data to predict any particular category. 
%is trained on COCO captions which follows a descriptive style and contains far less VAEL noise than in-the-wild captions. While it can be used alone, 
The text classifier in Cap2Det was only used when exact string matching produced no labels, which means it mainly corrects false negatives (visually present, not extracted labels), not visually absent extracted labels. Detic \cite{Zhou2022DetectingTC} uses weak supervision from image-level annotations from ImageNet \cite{Deng2009ImageNetAL} and extracted labels from Conceptual Captions (CC) \cite{Sharma2018ConceptualCA} to pretrain an open vocabulary object detection model with a CLIP classifier head. While these approaches succeed in leveraging multimodal datasets like COCO \cite{Lin2014MicrosoftCC} or Flickr30K \cite{Young2014Flicker30K} (descriptive, crowdsourced) and annotated image-level labels from ImageNet \cite{Deng2009ImageNetAL}, both approaches see lower performance in training with CC \cite{Sharma2018ConceptualCA}. Other prior work \cite{Gao2021OpenVO} uses a pretrained vision-language multimodal model (on 14M data) \cite{Li2021AlignBF} to generate pseudo-bounding box annotations using cross-attention between caption (including the mentioned object of interest) and image regions. While this is trained on COCO, Visual Genome, and SBUCaps, this work does not explicitly study the contribution of SBUCaps \cite{Ordonez2011Im2TextDI} to object detection performance. %, prior to finetuning detection using full supervision from a constrained set of classes. 
%As described in detail in the previous section, vision-language pretraining for open vocabulary object detection utilizes captions during pretraining while detection can be learnt from annotated bounding boxes \cite{zareian2021open}.  or from pseudo-labeled bounding boxes \cite{regionclip}. \cite{Gao2021OpenVO} or a combination of both \cite{Li2021GroundedLP_GLIP} uses both ground truth and pseudo-labeled bounding boxes .
%Additionally, we argue that false positives are far more harmful than false negatives, as there are few positive examples than there are negative examples. 
% Another approach used a vision-language dataset during pretraining, and bounding boxes for only some categories, to generalize detection abilities to novel classes \cite{zareian2021open}. 
Related to this, we study the extension to using in-the-wild captions for weakly-supervised object detection. 
% explain diff with using those for pretraining

% Also include some semi supervised works

\textbf{Vision-language pre-training for object detection.} 
%Zero-shot detection \cite{ZSOD_1_Bansal_2018_ECCV} is a task that tests the ability of an object detection model trained on a set of base classes to generalize to a set of unseen target classes. 
Image-text grounding has been leveraged as a pretraining task for open vocabulary object detection \cite{zareian2021open, ZSOD_2_OVOD_1_gu2022openvocabulary, ZSOD_3_Rahman2020ImprovedVA, ZSL_Rec_Diff_ZSOD_Rahman2020ZeroShotOD, regionclip, Du2022LearningTP}, followed by fully-supervised object detection training on examples from base classes. GLIP \cite{Li2021GroundedLP_GLIP} also leverages image-text grounding during pretraining, however, image and text representations are fused at earlier layers in each modality-specific encoder which is more applicable to phrase grounding and computationally expensive for object detection. Instead of training a vision-language grounding model from scratch and then training an object detection model, some methods distill knowledge from existing pretrained vision-language grounding models like CLIP \cite{Radford2021LearningTV} and ALIGN \cite{Jia2021ScalingUV} for getting proposals \cite{Shi2022ProposalCLIPUO} and supervision for object detection \cite{Du2022LearningTP, regionclip, Du2022LearningTP}. 
%Lastly, Detic \cite{Zhou2022DetectingTC} performs weakly supervised object detection with a CLIP-based open vocabulary classifier head on ImageNet \cite{Deng2009ImageNetAL} and Conceptual Captions \cite{Sharma2018ConceptualCA} as pretraining prior to supervised learning from base class examples. 

Our work differs in that we do not attempt open vocabulary detection, rather focus on rejecting samples that are harmful for localization. We perform WSOD using noisy samples from captions only, without any bounding box annotations.
Additionally, VEIL shows some promise in vetting novel object mentions in captions which means it could be used to augment or reject captions in vision-language pretraining for detection.
% some papers:
% Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model
% RegionCLIP
% ProposalCLIP
% ECCV 2022 https://arxiv.org/abs/2111.09452 
% Detic
%GLIP

\textbf{Adaptive Label Noise Reduction in Classification.}
Label noise in classification refers to when target labels for data are misaligned with the true labels, either through incorrect positive %labels by annotators 
or missing labels. Adaptive methods seek to reject or correct noisy labels ad-hoc during training.  Small-loss-based approaches select a sample if the overall loss or part of the loss %  like a predictive consistency loss, 
%C. Avoids cross-entropy based training all together which is less robust to noise--> not including for now
These methods rely on no (easy) visual pattern between images with the same corrupted label so that the network would need to memorize \cite{Zhang2016UnderstandingDL} the noisy label. We instead show that diverse real-world datasets contain naturally occurring \emph{structured} noise. 
% We evaluate only the state-of-the-art small-loss-based approach \cite{Kim2022LargeLM} on weakly supervised object detection because it's computationally efficient to integrate with WSOD. %Our work is complementary to the progress in this problem because 
We show in the detection setting, structured noise is far more harmful than in the commonly benchmarked recognition task, and small-loss-based approaches currently struggle. 
%This highlights another area for adaptive noise reduction methods to focus on.


% ---------- OLD TEXT

%, as well as \cite{tang2017multiple,yang2019towards,zeng2019wsod2,ren2020instance,shen2020uwsod} which iteratively update pseudo-labels based on high-scoring proposals.

% Empirically we find that the language-agnostic noisy label selection and/or correction approaches perform better than unfiltered baselines, they still have limited improvement in object localization, as opposed to object recognition, due to the structured noise. 
%In this paper we want to understand the different types of noise or misalignments that may occur in diverse multimodal datasets on the object or caption level. We develop a taxonomy of noisy labels extracted or not extracted from captions. We find that while some of these seasons may have simple linguistic structural heuristics, others rely more on the surrounding semantic context.

%The only exception is \cite{Chen_2017_CVPR} which uses wildlife documentaries but relies on track information, which is available in video but not image datasets. 

% \textbf{Coherence analysis in language and across modalities.} Discourse structure relays syntactic information about how text is organized. In language, each span is connected in a meaningful and coherent manner to the next; \textit{how} it is connected is known as the discourse or coherence relation. Discourse relations and structure are a well studied topic
% %with competing theories 
% \cite{Mann1988RhetoricalST,webberJoshiDiscourse,wolfGibsonDiscourse}. We use parsers that follow the taxonomy of discourse relations from rhetorical structure theory \cite{Mann1988RhetoricalST}; each span serves a function in rhetoric. This concept was extended to multimodal discourse, specifically images and captions in instructional \cite{Alikhani2019CITEAC} and caption generation contexts \cite{Alikhani2020ClueCC}, but has not been used for object detection before.