\section{Method}
\label{sec:veil}
\textbf{Extracting labels.} Given a vocabulary of object categories, we extract a label from an image if there is an exact %lexical
match between a class name and the corresponding caption ignoring punctuation, following \cite{Ye_Zhang_Kovashka_Li_Qin_Berent_2019,Fang2022DataDD}.
%One could also perform label extraction that considers synonyms, synsets, and word-similarity based matching against a closed vocabulary set; this can significantly increase the number of examples \footnote{For our purposes this is not needed since we are only trying to show that visual presence can be detected directly from captions even while learning from noisy pseudo-visual presence labels and that VEIL's filtering significantly improves WSOD performance compared to no filtering and language agnostic filtering and/or correction.}. 

\textbf{Vetting labels (VEIL).} The extracted label vetting (ELV) task uses visual presence targets that are assigned based on a pretrained (noisy) object detector for \textit{each} extracted label from the caption.  
Our model takes in a sequence of $C$ word token-level caption embeddings and outputs a sequence of visual presence predictions, $r\in [0,1]^{C}$. Predictions not associated with an extracted label are ignored (masked). 
WordPiece \cite{wu2016google} tokenization breaks captions into subwords, and each subword is mapped to a corresponding embedding, resulting in $e \in \mathbb{R}^{C\times d}$. These embeddings are passed through a language model, $h$, which includes multiple layers of multi-head self-attention over tokens in the caption to compute token-level output embeddings $v\in \mathbb{R}^{d\times C}$.
% These embeddings are then passed to BERT, which performs multi-head self-attention over the tokens and outputs token-level output embeddings $v\in \mathbb{R}^{C\times d}$
This is then passed to an MLP (Per-token Visual Presence Vetting):
%with one hidden layer followed by the tanh activation function:
\begin{gather}
    v = h(e) \\
    r = \sigma(W_2(\tanh(W_1v))
\end{gather}
where $W_1\in \mathbb{R}^{d\times d}$ and $W_2\in \mathbb{R}^{1\times d}$. The method is overviewed in Fig.~\ref{fig:elavet_arch}.

\input{iccv2023AuthorKit/figures/elavet_fig}


To train this network, the pseudo-label visual presence targets are present, $y_i = 1$, if a pretrained object detector also predicts the same category as the extracted label. Not all predictions in $r$ correspond to an extracted label, so we use a mask, $M\in [0,1]^{C}$, such that only the predictions associated with the extracted labels are used in binary cross entropy loss.  
%This is represented as an indicator function $\mathbbm{1}_{EL}(i)$ in our description below, but in practice is simply a vector mask. 
%(See Table \ref{}). This 250 image-caption subset was annotated with visual presence/absence of certain objects (elephant, truck, cake, bus, cow) for each of the in-the-wild datasets.
%The image and caption's visual presence pseudo labels are represented as binary vector $y$ where 1 denotes visual presence. 
% An extracted label mask with the same length as the number of tokens and all the tokens corresponding to an extracted label is marked as 1. This mask is set up such that the binary cross entropy loss averaged only over the tokens associated within the extracted label token set $EL$.
\begin{gather}
    L = \frac{1}{M^{T}M}\sum_{i=1}^{C}M_i\Big[y_i \log r_i + (1 - y_i) \log(1-r_i)\Big]
\end{gather}

\textbf{VEIL variants.} We test out two methods that can reduce the model's reliance on category specific predictions and improve generalization to other datasets: (1) VEIL$_{\text{ST}}$ which inserts a special token {\tt [EM\_LABEL] }right before each extracted label, and (2) randomly masking the tokens associated with the extracted labels with probability $p_m$.
By inserting a special token right before each extracted label in the caption, we are explicitly indicating to the model which tokens correspond to the extracted labels. This can help the model to better learn the relationship between the surrounding context and the extracted labels, and to focus more on the relevant parts of the input. We have some success with randomly masking the input extracted label tokens to encourage the model to extract information from the surrounding context instead of specific categories. However, we only apply this variant in the category generalization experiments. 



%special token works better empirically in generalizing to new categories in addition to generalizing to other datasets. Overall, we show that these techniques can help improve its ability to generalize to new categories.

% We test out additional variants that reduce the model's reliance on category specific predictions: inserting a special token right before each extracted label and randomly masking the tokens associated with the extracted labels with probability $p_m$. We hypothesize that a special token can learn a representation specific to vetting, rather than   There are other existing special tokens like {\tt [CLS], [SEP], [MASK] } in BERT \cite{BERT}; so we add {\tt [EM\_LABEL] } to the vocabulary and its respective fixed input embedding is randomly initialized. When preprocessing the caption, this special token is injected before the object mention within the caption. The model should be able to implicitly learn that this token is related to the adjacent tokens through positional embeddings concatenated with each input embedding, an existing feature of transformers \cite{Attn_Vaswani}. 

\textbf{Weakly-supervised object detection.}
To test the ability of extracted label filtering or correction methods for weakly supervised object detection, we train MIST \cite{ren2020instance} on specified \textit{test} splits of the in-the-wild datasets as they are unseen by all vetting methods. We evaluate the WSOD model on disjoint datasets, VOC-07 \cite{Everingham2010ThePV} and COCO 2014 \cite{Lin2014MicrosoftCC}. MIST extends WSDDN \cite{bilen2016weakly} and OICR \cite{Tang2017MultipleID} to mine pseudo-ground truth boxes prior to iterative refinement such that multiple instances are not grouped as one.



\textbf{Implementation details.}
VEIL is implemented in PyTorch \cite{pytorch} and uses a pretrained BERT encoder \cite{BERT} prior to the per-token visual presence classification layer. 
% We ignore predictions from tokens not associated with an extracted label, but these tokens are useful in establishing caption context. 
To extract pseudo-ground truth we use an ensemble of the X152-C4 object-attribute detection model \cite{zhang2021vinvl} 
% (trained on four public object detection datasets) % we mention this in the introduction
and the Ultralytic  YOLOv5-XL \cite{yolov5} object detectors.
This ensemble achieves strong accuracy on visual presence detection, e.g. 82.2\% on SBUCaps, 85.6\% on RedCaps, and 86.8\% on CC.
We observed higher visual presence accuracy on a small annotated set compared to each model alone (see supp). Lastly, we use the convention VEIL-DatasetX to signify that VEIL is trained on the train-split of DatasetX.

We use MIST \cite{ren2020instance} to learn an object detection model using weak supervision from image-level labels extracted from captions. We simulate a batch size of 8 for all experiments unless specified otherwise. We trained under different GPU settings due to resource constraints, and used gradient accumulation for some experiments. We used 4 RTX A5000 GPUs, and trained for 50k iterations with a batch size of 8 or 100k iterations on 4 Quadro RTX 5000 GPUs with a batch size of 4 and gradient accumulation (we updated parameters every two iterations to simulate a batch size of 8).

% We follow prior literature \cite{bilen2016weakly,Ye_Zhang_Kovashka_Li_Qin_Berent_2019} 
% to train a network to predict object categories.
% A convolutional neural network base encoder $h(x_i)$ is used to extract a feature map for an image, $x_i$. We use VGG-16 \cite{Liu2015VeryDC} with the fully connected layers removed as our base encoder, initialized with pre-trained ImageNet \cite{Deng2009ImageNetAL} weights. Region of interest (ROI) pooling \cite{Girshick2015FastR} is then applied to the feature map and the regions of interest $R_i \in \mathbb{R}^{4\times M}$ to generate a feature embedding $\phi(x_i)_m$ for each region: 
% $\phi(x_i) = ROIPool(h(x_i), R_i)$.
% We initialize two parallel fully-connected layers $f_c$ and $f_d$ whose outputs will be normalized to give a classification score, the probability that an object $c$ is present in that region, and detection score, the probability that $R_{i,m}$ contributes to the image-level class prediction, respectively. 
% \begin{equation}
%     p^{cls}_{m, c} = \sigma{(f_c(\phi(x_i)_m))}, ~~~ p^{det}_{m, c }=\frac{\exp\Big(f_d(\phi(x_i)_m)\Big)}{\sum_{j=1}^{M} \exp\Big(f_d(\phi(x_i)_j)\Big)}
% \end{equation}

% The class and detection predictions for each region are multiplied and summed over $M$ proposals to produce one image-level class prediction vector.
% \begin{equation}
%     \hat{p_c}=\sigma\Bigg(\sum_{m=1}^{M} p_{m,c}^{det} o_{m,c}^{cls}\Bigg)    
% \end{equation}  

% To train, we apply the multiple instance detection loss to the extracted image-level label and the predicted image-level label \cite{bilen2016weakly}:
% \begin{equation}
%     L_{mid}=\frac{1}{C}\sum_{c=1}^{C}\Big[y_c \log \hat{p_c} + (1 - y_c) \log(1-\hat{p_c})\Big]
% \end{equation}




% ------ OLD TEXT

%\subsection{Extracted Label Vetting Method}

%There are two variants to the visual presence detection methods. The first produces a single visual presence prediction for the entire caption and the other produces a visual presence prediction \textit{per label}. For the former, the visual presence detection method takes in some text input $t_i$ gathered from the caption $c_i$ for image $x_i$, and produces a score $r_i$. To get a visual presence classification $\hat{y_i}$, the score $r_i$ must be above some threshold. If $\hat{y_i}=1$ then the extracted labels from the caption are accepted, else rejected.

% \textbf{Descriptiveness Classifier.}
% The descriptiveness classifier first computes a frequency-based vector representation of part of speech from $c_i$. Then, this vector $v_i$ is fed as input to a logistic regression model:
% $$\text{descriptiveness\_score} = \sigma(W^Tv_i)$$
% $$\hat{y_i} = \text{descriptiveness\_score} > thresh$$
% The narrativeness score is complementary to the descriptiveness score: $$\text{narrativeness\_score}=1-\text{descriptiveness\_score}$$
% Additionally, for the descriptiveness or narrativeness classifier we set $thresh = 0.5$.
