% \documentclass[10pt,letterpaper]{article}
% \usepackage{iccv}
% \usepackage{times}
% \usepackage{subfigure}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{xcolor}
% \usepackage{multirow}
% \usepackage{array}
% \usepackage{makecell}
% \usepackage{tabu, booktabs}
% \usepackage{xcolor,colortbl}
% \usepackage{newtxtext,newtxmath}
% \usepackage{enumitem}
% \usepackage{bbm}

% \definecolor{lightgreen}{rgb}{0.60, 0.78, 0.75}
% \definecolor{lightorange}{rgb}{1.0, 0.949, 0.80}
% \definecolor{verylightorange}{rgb}{1.0, 0.875, 0.502}
% \definecolor{verylightgreen}{rgb}{0.796, 0.886, 0.871}
	
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission
% % Pages are numbered in submission mode, and unnumbered in camera-ready

% \begin{document}
{\Large \bf VEIL: Vetting Extracted Image Labels from In-the-Wild Captions for Weakly-Supervised Object Detection (Supplemental Materials) }
% \author{Arushi Rai\\
% University of Pittsburgh\\
% %Institution1 address\\
% {\tt\small arr159@pitt.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until thess closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Adriana Kovashka\\
% University of Pittsburgh\\
% %First line of institution2 address\\
% {\tt\small kovashka@cs.pitt.edu}
% }
% % \date{\vspace{-5ex}}
% \maketitle
\section{Introduction}
We provide supplemental materials to our main text. First, we provide additional analysis tables relating to part-of-speech distribution between DII and SIS captions in VIST \cite{huang2016visual}. Then we explore variants of a descriptiveness classifier, comparing CLIP and BERT as features. We show that selecting VIST captions based on descriptiveness during WSOD training leads 4-6x higher performance than selecting based on narrativeness across three descriptiveness estimation methods. Additionally, the POS-based descriptiveness classifier outperforms selecting captions based on ground-truth descriptiveness (DII caption) by 55\%. We assess the transferability of the descriptiveness classifier on in-the-wild captions, and find that it has a low descriptivness misclassification rate for most datasets.

Next, we provide a benchmark of the object detectors and object detector ensemble used to generate visual presence pseudo-labels. 

We then present a detailed table of the vetting precision and recall of all methods described in Section 5.1 of the main text, for which we show F1 performance in Section 5.2 (Table 2) of the main text. Furthermore, we show more comprehensive cross-dataset ablations, such as adding more training datasets and training with a special token. 

We also show additional metrics of the WSOD models on the COCO-14 benchmark presented in Section 5.3 of the main text.

Finally, we showcase the vetting ability of VEIL in comparison to other approaches through qualitative results, along with additional examples from the WSOD models trained using vetted training data.

\section{Case Study: Additional Analysis on Descriptiveness/Narrativeness}

\subsection{Linguistic Differences}
Table \ref{tab:pos_dii_sis} presents the distribution of part-of-speech tags in DII and SIS captions, revealing a significant disparity in frequency across various parts of speech. We show that verb tense is more evenly distributed in narrative than descriptive captions which favor present tense. Additionally, Table \ref{tab:aspect} shows aspect, inferred using part-of-speech tags. Aspect relates to visual-text alignment. Progressive aspect refers to verbs that describe an ongoing, hence likely to be shown, activity such as ``The event is starting, there are even some dancers forming outside" while perfective describes a completed or close to completion activity, e.g. ``The old part has been \textbf{removed} and now there many loose wires now". Table \ref{tab:aspect} shows that progressive aspect is more common in both, but very frequent in DII captions, which aligns with our expectations. 
\begin{table}[!htb]
      \centering
        \begin{tabular}{|r|c|c|}\hline
            Part of Speech   & DII & SIS \\ \hline
            noun   & \textbf{3.85} & 2.73 \\
            preposition  & \textbf{1.75} & 1.01 \\
            adjective   & \textbf{0.98} & 0.85 \\
            personal pronoun   & 0.18 & \textbf{0.80} \\\hline
            \textbf{verb}   &1.55 & \textbf{2.00}\\\hline
            verb base   & 0.07 & \textbf{0.37}\\
            verb gerund  & \textbf{0.61} & 0.23\\
            past tense  & 0.08 & \textbf{0.89}\\
            past participle  &  \textbf{0.21} & 0.17\\
            non-3rd person sing pres  &\textbf{ 0.22}& 0.16\\
            3rd person sing pres  & \textbf{0.36}& 0.19\\
            \hline
        \end{tabular}
    \caption{The difference in part of speech between descriptive and narrative captions. }
    \label{tab:pos_dii_sis}
\end{table}

\begin{table}
  \centering
    %\caption{Using a random sample of 10,000 captions, we observe verb tense and grammatical aspect. The SIS corpora consists of a slightly imbalanced mix of verbs in the past tense form and present tense form, whereas DII is mainly written in the present form. In terms of aspect, both DII and SIS favor progressive aspect where verbs describe ongoing events rather than completed events.}
    \label{tab:verb_tense_distribution}
    \begin{tabular}{|r|c|r|c|c|}\hline
         & past & present  & progressive & perfective \\ \hline
        DII & 18.5\% & \textbf{81.4\%} &  \textbf{94.5\%} & 5.4\%\\
        SIS & \textbf{52.8\%} & 47.2\% &  \textbf{75.1\%} & 24.9\%\\
        \hline
    \end{tabular}
    \caption{Distribution over tense and aspect. }
    \label{tab:aspect}
\end{table}


% \noindent\textbf{Part of Speech (POS).}
% As mentioned in the main text, we extract POS tags over all VIST captions using SpaCy \cite{spacy2} and observe differences in pronoun usage and verb tense between descriptive and narrative captions (Table \ref{tab:pos_dii_sis}). The increased use of pronouns in SIS suggests a deviation from an impersonal, objective tone. Verb tense and aspect also differ between descriptive/narrative captions (Table \ref{tab:verb_tense_distribution}). Past and present tense are both frequent in SIS captions, with more verbs referring to the past than present. Past  and present tense in SIS can occur in the same sentence: ``Afterwards, we take a couple photographs because we paid the photographer to do so." An aligned image would show either show the couple posing for a photo or the transaction. This interaction between past and present occurs over 33\% of SIS captions vs 16\% in DII. 

% Aspect is more evenly distributed in narrative than descriptive captions, but progressive aspect is more common in bother. Progressive aspect refers to verbs that describe an ongoing activity such as ``The event is starting, there are even some dancers forming outside" (SIS) while perfective describes a completed %or close to completion 
% activity, e.g. ``The old part has been \textbf{removed} and now there many loose wires now" (SIS). 

% We hypothesize part of speech tags can be a strong feature to discriminate between DII and SIS, and DII provides more aligned captions since it describes ongoing activities more likely to be present in the accompanying visual scene.

\begin{table}[t]
    \centering
    \scalebox{0.95}{
    \begin{tabular}{|r|c|c|}\hline
        Linguistic features & PREC & REC \\\hline
        POS & \textbf{0.8823} & \textbf{0.8782}\\
        CLIP \cite{Radford2021LearningTV} & 0.7215 & 0.7170 \\\hline
        CLIP+POS & 0.8925 & 0.8893\\
        BERT & \textbf{0.9570} & \textbf{0.9560}
        \\\hline % prec dropped from 0.968 -> 0.957 (previously evaluated on a subset)
    \end{tabular}}
    \caption{We evaluate precision/recall of DII/SIS classifiers on a VIST holdout.}
    %taking in combinations of linguistic features 
    %We see that part of speech tags are quite discriminative, however using all features: RST, CLIP*, and CLUE increase the precision and recall of the classifier. 
    %\cite{Radford2021LearningTV}. 
    \label{tab:dii_sis_classifier}
\end{table}

\input{iccv2023AuthorKit/tables/evaluating_on_pascal_dii_sis}

\subsection{Descriptiveness Classifier}
\textbf{Training details.} We examine using part-of-speech tags to discriminate between descriptive and narrative captions, and compare against methods trained with much more data, BERT \cite{BERT} and CLIP \cite{Radford2021LearningTV}. To set up this task, we use captions from the VIST dataset \cite{huang2016visual} which contains multiple captions written in three distinct styles for each image. We use the descriptive-in-isolation (DII) and story-in-sequence (SIS) captions from VIST \cite{huang2016visual} and assign a positive label (descriptive) to captions drawn from the DII split and a negative label (narrative) to captions drawn from the SIS split. We then train a logistic regression model, which we refer to as the descriptiveness classifier. This classifier takes a binary vector corresponding to the presence of a linguistic feature (e.g. proper noun, adjective, verb - past tense, etc) in the caption. The CLIP-based descriptiveness classifier uses only the cosine similarity between the image embeddings and caption embeddings to distinguish between descriptiveness and narrativeness; training a logistic regression model on a single feature has the effect of finding an appropriate threshold for this task. For the BERT upper bound comparison, we finetune BERT without freezing weights.
For this experiment, we sample a balanced subset of VIST with captions containing an exact match with a category from the following constrained label set: dog, boat, person, car. 
We compare different combinations of the input in Table \ref{tab:dii_sis_classifier}.
We find that simple linguistic features such as part-of-speech do nearly as well as BERT \cite{BERT}, despite being much smaller and having less overhead (no forward pass required). Further, POS is stronger than CLIP, and combining them achieves a better results than either one in combination, indicating these features carry complementary information for this task.

\textbf{WSOD applicability.} For this experiment, we sample a balanced subset of VIST with captions containing an exact match with a category from the following constrained label set: dog, boat, person, car. 
Table \ref{tab:eval_dii_sis_predictions} shows that these classifiers can be used to select quality data for training weakly-supervised object detection models. Each classifier assigns descriptiveness scores; we show that when the highest-scoring caption is selected per image (resulting in a pool of descriptive captions), the results are \textbf{4-6 times better} than when selecting the lowest-scoring captions (resulting in narrative captions). Specifically, we extract labels from the selected captions as pseudo image-level labels using exact string matching, to train the multiple instance detection network using WSDDN \cite{bilen2016weakly}. We measure mAP at $IOU=0.5$ over the selected categories (car, boat, dog, person). 

\textbf{Evaluating the descriptiveness classifier on in-the-wild datasets.}
Seeing the low performance of the descriptiveness classifier in vetting of extracted labels (Table 2 in the main text), we hypothesize that this is due to inability of this classifier to distinguish between descriptive/narrative captions in in-the-wild datasets. We thus test the ability of the part-of-speech based descriptiveness classifier trained on VIST descriptive and narrative captions to predict whether an in-the-wild caption is descriptive. As described in the main text, we label 100 captions as having a descriptive or narrative style from SBUCaps, RedCaps and Conceptual Captions. We use the part-of-speech based classifier trained on VIST to predict whether the annotated captions are descriptive; predictions over the threshold 0.5 are classified as descriptive. We use metrics like accuracy, descriptive misclassification rate ($\frac{\text{\# of incorrect descriptive predictions}}{\text{\# of descriptive captions}}$), and narrative misclassification rate ($\frac{\text{\# of incorrect narrative predictions}}{\text{\# of narrative captions}}$) to evaluate performance.

%We define a narrative style as having subjective elements, first-person perspective ("I", "my", etc), using non-literal expressions ("it's a piece of cake"), assigning intent ("owner shows off her dog"). Note that while VIST-SIS contains examples of narrative-like captions, our definition is inclusive of captions beyond those narrative examples, as real-world captions are not constrained by creating a caption drawing from visible events in previous or future frames. 

% We find that in-the-wild datasets contain far more narrative-like examples than COCO, a heavily descriptive dataset as shown in Table \ref{tab:descriptiveness_in_wild_annot}. RedCaps contains the most narrative-like captions (62\%) in the in-the-wild datasets, whereas ConceptualCaptions contains the least (33\%) we show the ability of our descriptiveness classifier to transfer to these datasets. This is understandable because RedCaps are from a social media source, where people frequently write in a story-like fashion to communicate and connect with others; Conceptual Captions is written with the intent to convey the image, or sometimes the meaning behind the image to visually impaired users using assistive technology. In fact, it's surprising that there's still a considerable amount of narrative-like captions in a heavily preprocessed image-alt text dataset like Conceptual Captions.


\input{iccv2023AuthorKit/tables/descriptiveness_performance.tex} 



 We show the results in Table \ref{tab:desc_classifier_score_in_wild_annot}. We find that while the descriptiveness classifier is able to correctly identify descriptive captions in most datasets, it struggles with correctly identifying narrative captions. We suspect that this might be due to the domain shift from VIST-SIS to the narratives found in the real world. Furthermore, the features that indicate narrativeness in the natural captions are not represented by part-of-speech features. For example, specific words indicate subjectivety, like ``interesting" and others are concrete ``red", yet both are considered adjectives in terms of part-of-speech. Similarly, first-person pronouns like ``I" and ``my" fall under personal pronouns. But, beyond the part of speech tags, there are implicit first-person perspectives found in the grammatically incorrect short hand writing found in natural captions: ``watching a doggie show in the bar while waiting for the pizza". The underspecification of the features used for the descriptiveness classifier explains why it fails to generalize to narrative captions in-the-wild.

\section{Quality of Pretrained Object Detector Ensemble}

\input{iccv2023AuthorKit/tables/supp/ensemble_detector_performance}

Since we used vision-language datasets without any object annotations, we have no way of knowing whether an object mentioned in the caption is present in the image. To keep our method scalable and datasets large, we used object predictions from a pretrained object detector to produce visual presence pseudo labels for extracted labels. We test the VinVL detector \cite{zhang2021vinvl} and YOLOv5 detector \cite{yolov5}, and their ensemble (aggregating predictions) on COCO-14 Image Recognition in Table \ref{tab:coco_ensemble} and a visual presence annotated subset in Table \ref{in_the_wild_ens}. For the latter, per dataset we annotated the visual presence of 50 extracted labels from unique images for each category. We used the following randomly selected VOC \cite{Everingham2010ThePV} categories: elephant, truck, cake, bus, and cow. We found that while the ensemble variant and the VinVL detector is worse than YOLOv5 in image recognition on a common benchmark, COCO-14, it performs better than the single models on visual presence. Since this is the task we aim to do, we select the ensemble model to generate visual presence targets. Additionally, these results indicate there is still significant noise in using these models to generate pseudo labels, so using these pretrained object detectors is not the same quality as human annotations. Despite this, VEIL still successfully harnesses these noisy targets to reason about visual presence from captions.

\section{Vetting Precision/Recall}

Table 2 in the main text showed the F1 on the extracted label vetting task, from twelve methods. In Table \ref{tab:direct_eval} here, we separately show Precision and Recall on the same task.

\input{iccv2023AuthorKit/old_files/direct_eval_table_backup}

\section{Cross-Dataset Ablations}

\input{iccv2023AuthorKit/tables/supp/cross_dataset_prec_rec}
\input{iccv2023AuthorKit/tables/supp/cross_dataset_f1}

Table \ref{tab:cross_dataset_prec_rec} is included as reference which shows that precision in the cross dataset setting is always better than no vetting with the exception of COCO.

\textbf{Combining multiple datasets.} We find that VEIL is able to leverage additional datasets to an extent. For example, combining SBUCaps and CC leads to significant improvements (7-16\% relative) in F1 as shown in Table \ref{tab:cross_dataset_f1} and, combining SBUCaps and Redcaps in training improves performance on both validation sets. When combining all datasets, only the non-in the wild datasets see an improved performance. 

\textbf{Using special token.} We find that using VEIL w/ ST on average improves F1 by 1 pt compared to just VEIL when transferring to other datasets. This comes at a tradeoff with respect to the performance on the same dataset; however CC w/ ST improves performance on all datasets.
\begin{table*}[]
    \centering
\begin{tabular}{c|ccc|ccc} \toprule
& \multicolumn{3}{c|}{ mAP, IoU }& \multicolumn{3}{c}{mAP, Area} \\
& {0.5:0.95} & 0.5 & 0.75 & S & M & L\\ \midrule
No Vetting & 1.8 & 4.3 & 1.4 & \underline{0.7} & 2.0 & 2.9 \\
GT* & 1.2 & 3.6 & 0.7 & 0.4 & 2.0 & 1.6\\ 
Large Loss \cite{Kim2022LargeLM} & 1.7 & 4.1 & 1.2 & \underline{0.7 }& 1.9 & 2.4\\
LocalCLIP-E \cite{Radford2021LearningTV}& 1.4 & 3.7 & 0.9 & 0.1 & 2.4 & 0.2 \\
VEIL$_{\text{ST}}$-R,CC & \underline{2.9}& \underline{6.6}& \underline{2.2}& \textbf{0.8}& \underline{3.0}& \underline{4.7 }\\
VEIL-SBUCaps & \textbf{3.1} & \textbf{7.0} & \textbf{2.5} & \textbf{0.8} & \textbf{3.2} & \textbf{5.4} \\\toprule
\end{tabular}
    \caption{COCO-14 benchmark for WSOD models trained with various vetting methods. (GT*) directly vets labels using the pretrained object detectors which were used to train VEIL. Bold indicates best performance in each column and underline indicates second best result in the column..}
    \label{tab:coco_all_metrics}
\end{table*}
\section{WSOD Benchmarking on Additional COCO Metrics}
In our main text we compared the average precision of the model across all the classes and all the IoU (Intersection over Union) thresholds from 0.5 to 0.95. We show mAP at specific thresholds 0.5 and 0.75 in Table \ref{tab:coco_all_metrics}. We see that VEIL-based vetting perform nearly \textbf{twice} as well as no vetting in a stricter IoU (0.75); non-VEIL vetting fail to outperform no vetting under both less precise IoU (0.5) and stricter IoU (0.75). The mAP metric can be further broken down by area sizes of ground truth bounding boxes, which is denoted by S, M, and L. VEIL-based vetting outperforms the rest in all area size settings, but sees significant gains in Medium (20\% better than best non-VEIL vetting) and Large objects (41\% better than best non-VEIL vetting).
\section{Additional Qualitative Results}
\input{iccv2023AuthorKit/figures/supp/vetting_quals}
\input{iccv2023AuthorKit/figures/supp/wsod_quals}
\textbf{Vetting Qualitative Examples.} We provide qualitative examples comparing the vetting capability of various methods on captions falling into three VAEL noise types (prepositional phrase, related noun phrase, non-literal) found in RedCaps, and more broadly all in-the-wild datasets in Figure \ref{fig:vetting_quals}.

\textbf{WSOD Qualitative Examples.} In Figure \ref{fig:structured_qual}, we present further qualitative evidence on the impact of different vetting methods on weakly supervised object detection. While the main text showed qualitative examples comparing Large Loss and VEIL-SBUCaps, we now expand this to include other vetting methods. Our findings reveal that non-VEIL methods exhibit significant part bias, where only a portion of the object is consistently detected instead of the entire object. Moreover, all of the models under consideration struggle with instance disambiguation. VEIL-based methods cover the object better with the exception of the third row, where the CLIP-based vetting method has tighter bounding boxes.
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }
% \end{document}
