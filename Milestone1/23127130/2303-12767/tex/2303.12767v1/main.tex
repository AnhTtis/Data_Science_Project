\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[
backend=biber,
backref=true,
sorting = none,
% style=alphabetic,
% citestyle=authoryear-comp
]{biblatex}
\usepackage[]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta, 
    urlcolor=cyan,
    citecolor= red}

\addbibresource{main.bib} %Imports bibliography file
\usepackage{parskip}
\linespread{2} %double spaced
\usepackage{graphicx} %for \begin{figure} to work
\usepackage{float} %for figure placement
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage[toc,page]{appendix}

\newcounter{HWNumberOfComments}
\stepcounter{HWNumberOfComments}
\newcommand{\hw}[1]{\textsf{{\textcolor{blue}{[hw\#\arabic{HWNumberOfComments}\stepcounter{HWNumberOfComments}:#1]}}}}

\newcounter{JSNumberOfComments}
\stepcounter{JSNumberOfComments}
\newcommand{\js}[1]{\textsf{{\textcolor{magenta}{[js\#\arabic{JSNumberOfComments}\stepcounter{JSNumberOfComments}:#1]}}}}

\newcommand{\cm}[1]{\textcolor{red}{#1}}
\newcommand{\bm}[1]{\textcolor{blue}{#1}}



\begin{document}

\begin{center}
    \large \textbf{Can we trust the evaluation on ChatGPT?}
    \\\small Rachith Aiyappa,$^{a,}$\footnote{$^1$ To whom correspondence should be addressed. E-mail: racball@iu.edu}  
    Jisun An,$^{a}$
    Haewoon Kwak,$^{a}$
    Yong-Yeol Ahn$^{a,b}$\\
    $^a$Complex Networks and Systems, Luddy School of Informatics, Computing, and Engineering, Indiana University, Bloomington, Indiana, USA, 47408\\
    $^b$Indiana University Network Science Institute, Indiana University, Bloomington, Indiana, USA, 47408
\end{center}

\section*{Abstract}

ChatGPT, the first large language model with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPTâ€™s performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study in stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
% 
\section{Introduction}

ChatGPT~\cite{chatgpt} has become the most prominent and widely-adopted pre-trained large language model (LLM) thanks to its impressive capabilities to perform a plethora of natural language tasks and its public accessibility. 
Although significant concerns regarding LLMs, particularly their tendency to ``hallucinate'' (or ``making things up'') and generation of biased or harmful content in scale have been raised~\cite{bender2021dangers, alkaissi2023artificial}, ChatGPT is becoming a common tool not only for everyday tasks such as essay writing, translation, and summarization~\cite{taecharungroj2023can,patel2023chatgpt}, but also for more sophisticated tasks such as code generation, debugging~\cite{sobania2023analysis}, and mathematical problem-solving~\cite{frieder2023mathematical}. 
With more than 100 million users within two months after its launch~\cite{milmo2023chatgpt} and its abilities to \textit{solve} complex tasks like the bar exam~\cite{terwiesch} and medical licensing exam~\cite{kung2023performance}, ChatGPT has stirred public perception of AI and has been speculated as the paradigm for the next-generation search engine and writing assistant, which is already being tested by Microsoft's Bing search and Office products~\cite{bing}. Beyond commercial interests, LLMs  are also being tested for assisting scientific research~\cite{stokel2023chatgpt,dowling2023chatgpt,van2023chatgpt,wu2023large}.  
% and has sparked debates about what kinds of research questions may become obsolete and ~\cite{}.

Although OpenAI---the creators of ChatGPT---performed internal tests, they do not cover all problem domains. Although the excellent general performance of ChatGPT is evident, it is still important to quantitatively characterize its performance on specific tasks to better understand the model. 
Note that, given that it is currently not possible for a user to fine-tune ChatGPT, one can only evaluate it with a few-shot/zero-shot setting---a highly desirable setting that requires close to no annotated data.
% particularly in a zero shot setting---a setting in which a language model is used as is, without further fine-tuning or being shown examples of the task. 
A recent study showed that although ChatGPT performs generally well in many tasks, it has different strengths and weaknesses for different tasks and does not tend to beat the SOTA models~\cite{kocon2023chatgpt}.

However, given that the ChatGPT is a \emph{closed} model without information about its training dataset and how it is currently being trained, there is an elephant in the room: \emph{how can we know whether ChatGPT has not been contaminated with the evaluation datasets?} 
Preventing data leakage (training-test contamination) is one of the most fundamental principles of machine learning because such leakage makes evaluation results unreliable.
It has been shown that LLMs can also be significantly affected by data leakage, both by the leakage of labels and even by the leakage of dataset without labels~\cite{min2022rethinking,brown2020language,gpt4}.
Given that the ChatGPT's training datasets are unknown and that ChatGPT is constantly updated, partly based on human inputs from more than 100 million users via Reinforcement Learning from Human Feedback (RLHF)~\cite{chatgpt}, it is impossible to ascertain the lack of data leakage, especially for the datasets that have been on the internet. 

As far as it has been known, ChatGPT is trained in a three-step process. First, an initial LLM (GPT 3/3.5) is fine-tuned in a supervised manner on a dataset curated by asking hired human annotators to write what they think is the desired output to prompts submitted to the OpenAI API.~\footnote{Additional labeler-written prompts are included too.} Next, a set of prompts is sampled from a larger collection of prompts submitted to the OpenAI API. For each prompt in this set, the LLM produces multiple responses, which are then ranked by human annotators who are asked to indicate their preferred response. The second step of the RLHF process then trains a reward model (RM) on this dataset of response-ranking pairs to mimic the human ranking. This step keeps the LLM frozen and solely trains the RM. Finally, the LLM is made to generate responses to a set of prompts, which were not included in the previous steps, but submitted to the OpenAI API nevertheless. The now-frozen RM is used as a reward function, and the LLM is further fine-tuned to maximize this reward using the Proximal Policy Optimization (PPO) algorithm~\cite{schulman2017proximal}. 

Thus, if OpenAI continuously updates its models, by using queries submitted by researchers who wanted to evaluate ChatGPT's performance on various Natural Language Processing (NLP) tasks, it is likely that ChatGPT is already contaminated with the test datasets of many NLP tasks, which can lead to apparent excellence in NLP tasks. 
Such contamination has been documented in the training data of other language models~\cite{brown2020language,dodge2021documenting,carlini2020extracting}.\footnote{https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks} 

It is important to highlight a distinction between two kinds of contamination acknowledged in literature~\cite{dodge2021documenting}---(1) the case when both the task input text and output labels of test datasets are used for training/fine-tuning versus (2) the case when just the input text is used. The latter is surely a smaller concern. However, even without the correct labels, exposure to the text in the same domain has been documented to increase the performance of the model to the corresponding NLP task~\cite{min2022rethinking}. 
Although we do not have any documented evidence that the ground-truth output answers/labels of the NLP tasks were submitted to the platform and the ChatGPT model has been trained with such data, the annotator-generated responses to queries submitted to OpenAI during the RLHF step could potentially match the input text with output labels of the right kind. Given that language models show competitive performance in classification tasks despite poorly labeled data~\cite{min2022rethinking,garg2022can}, we cannot discard the possibility that the RLHF pipeline might essentially be a weaker variant of type (1) contamination.

Here, we use a case study of a stance detection problem~\cite{kuccuk2020stance} to raise awareness on this issue of data leakage and ask a question about how we should approach the evaluation of closed models. Stance detection is a fundamental computational tool that is widely used across many disciplines, including political science and communication studies.
It refers to the task of extracting the standpoint (e.g., Favor, Against, or Neither) towards a target from a given text. The task becomes more challenging when the texts are from social media like Twitter because of the presence of abbreviations, hashtags, URLs, spelling errors, and the incoherent nature of tweets. 
Recent studies have claimed that ChatGPT outperforms most of the previous models proposed for this task~\cite{zhang2022would} on few existing evaluation datasets, such as the SemEval 2016 Task6 dataset~\cite{mohammad2016semeval,mohammad2017stance} and P-stance~\cite{li2021p}, even in a zero-shot setting where the model was not fine-tuned on the task-specific training data.    

Can this result be due to the data leakage and contamination of the model? 
Could this study itself have contaminated the ChatGPT model?
Although it is not possible to definitely answer these questions, it is \emph{also impossible to rule out} the possibility of contamination. 
%Here, to probe this possibility, we compare the performance of multiple ChatGPT model releases on the same datasets, while fully acknowledging the limitations of this approach. 

Following its release on Nov $30^{th}\ 2022$, on Dec $15^{th}$ $2022$, Jan $9^{th}$, Jan $30^{th}$, Feb $9^{th}$, and Feb $13^{th}\ 2023$, ChatGPT has been updated multiple times.\footnote{https://help.openai.com/en/articles/6825453-chatgpt-release-notes} While most of these releases updated the model itself, it is our understanding that the February releases were about handling more users to the platform, optimizing for speed, and the offering of ChatGPT plus---a subscription plan which provides priority access to new features, and faster response times.\footnote{https://openai.com/blog/chatgpt-plus/} 
Given that there has been at least one study that evaluated ChatGPT's performance on stance detection tasks~\cite{zhang2022would}, and that newer versions of ChatGPT are more likely to be \emph{exposed} to past queries to the platform, an opportunity arises to test whether the performance of the newer versions of ChatGPT on stance detection has been substantially improved after the study by \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would}. 

As we will present below, we do see an overall improvement in the performance before and after the publication of the stance detection evaluation paper~\cite{zhang2022would}. 
Of course, there is an alternative explanation that the model simply got better. 
However, we would also like to note that OpenAI has been updating the model primarily to address the model's problematic behaviors by making it more restricted, which led to the observation, although largely anecdotal, that the model has become `less impressive.' 

%we cannot rule out other explanations completely given that OpenAI aims to make updated versions more ``safe'' and restrictive, and less ``toxic'' and ``biased,''  a substantial improvement on this particular task can be indicative of potential data leakage from the submission of test data, in combination with RLHF, to the older versions of ChatGPT. 
% Because our result is observational, we cannot conclude whether the improved performance is due to the enhancement in the model itself or the exposure to the test dataset. 
%We are currently preparing an active experiment to validate our concern in a more robust way. 


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth, trim={0 3in 0 2in}]{seq_of_events.pdf}
    \caption{Updates of ChatGPT ever since its release on November 30, 2022. The versions of ChatGPT, each fine-tuned by RLHF process based on the queries to the OpenAI API platform, are indicated by the date ticks. The blocks contain the datasets, relevant to this study, on which ChatGPT's performance is evaluated on.}\label{fig:timeline}
\end{figure}

\section{Methods}

Given that the Zhang et al.~\cite{zhang2022would} was released on arXiv on December 30, 2022, and ChatGPT was launched on November 30, 2022, we assume \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would} used either the November 30 or December 15 version of ChatGPT (henceforth called V1) to obtain their results (Fig.~\ref{fig:timeline}). Following their work, we used the test sets of SemEval 2016 Task 6~\cite{mohammad2016semeval,mohammad2017stance} and P-stance~\cite{li2021p} to perform our experiments.  

We also used the same prompt. Specifically for SemEval 2016 Task 6, for instance, given the input: ``RT GunnJessica: Because i want young American women to be able to be proud of the 1st woman president \#SemST'', the input to ChatGPT is: ``\textit{what's the attitude of the sentence:} `RT GunnJessica: Because i want young American women to be able to be proud of the 1st woman president \#SemST' to the target `Hillary Clinton'. \textit{select from ``favor, against or neutral''}. 
Similarly, since the P-stance dataset does not have a neutral stance, the prompt is slightly modified to ``\textit{what's the attitude of the sentence:} `Air borne illnesses will only become more common with climate change. We need to immediately address this and fight for Medicare for All or this could be the new normal. \#BernieSanders' to the target `Bernie Sander'. \textit{select from ``favor, or against}''.~\footnote{This was confirmed with \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would} through email communication since the current version of their paper on arXiv does not explicitly mention the prompt.} 


Since ChatGPT did not provide an API to collect data at the time of the experiment, we first manually collected the responses of \textit{Jan 30th ChatGPT} for 860 tweets from the test data of SemEval 2016 Task 6, pertaining to the targets, `Hillary Clinton (HC),' `Feminist Movement (FM),' and `Legalization of Abortion (LA)' and extract the stance label from them. While the test set contains tweets pertaining to other targets (`Atheism,' `Donald Trump,' `Climate Change is a Real Concern'), we sampled the 860 tweets pertaining to the targets used in the previous work~\cite{zhang2022would}. 
After manual inspection of the preliminary results of the 860 tweets, we decided to collect and include the responses for the 2157 tweets in the P-stance test dataset in our analysis, but the \textit{Jan 30th ChatGPT}  version was no longer available by then. Nevertheless, we use an open-source API~\footnote{https://github.com/acheong08/ChatGPT} to automate the collection of responses from the \textit{Feb 13th ChatGPT plus} for both the P-stance and SemEval 2016 Task 6 datasets. Then we manually go through these (often verbose) responses to extract the stance labels from them when explicitly mentioned. 

In sum, we were only able to use the \textit{Feb 13th ChatGPT plus} version for the P-stance dataset and the \textit{Jan 30th ChatGPT} and \textit{Feb 13th ChatGPT plus} version for the SemEval 2016 Task 6 dataset because OpenAI (1) does not provide access to its older models after newer models are released, (2) imposes an upper bound on the number of requests which can be submitted to the platform in an hour, and, at the time of this experiment, (3) lacked a public API which in turn hindered the speed and efficiency of data collection. 

\section{Evaluation Metric and Results}

The macro-F and micro-F scores are shown for different versions of ChatGPT in a zero-shot setting on SemEval 2016 Task 6 and P-Stance datasets in Table~\ref{tab:sem} and Table~\ref{tab:pstance}, respectively. 
The macro-F score is calculated by averaging the F scores for the favor and against classes. 
The micro-F score is calculated by considering the total number of true positives, true negatives, false positives, and false negatives across the favor and against classes instead of averaging the F scores for each class. 
% 
\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}
\begin{table}[ht]
\centering
\setlength\tabcolsep{20pt} 
\begin{tabular}[t]{lccc}
\hline
\multirow{1}{10em}{\centering Model} &  HC & FM & LA \\
\hline
\multirow{1}{10em}{\centering V1} & \multirow{1}{4em}{\centering 79.5/78.0} & 
\multirow{1}{4em}{\centering 68.4/72.6} & 
\multirow{1}{4em}{\centering 58.2/59.3}
\\
\hdashline
\multirow{1}{10em}{\centering Jan 30 ChatGPT} & \multirow{1}{4em}{\centering \textbf{87.83/86.9}} & \multirow{1}{4em}{\centering \textbf{83.22/80.79}}& \multirow{1}{4em}{\centering \textbf{72.43/68.33}}\\
\hdashline
\multirow{1}{10em}{\centering Feb 13 ChatGPT plus} & \multirow{1}{4em}{\centering 82.9/81.87} & \multirow{1}{4em}{\centering 75.94/71.96}& \multirow{1}{4em}{\centering 65.56/61.74}
\end{tabular}
\caption{Micro-F1/Macro-F1 scores of different versions of ChatGPT in a zero-shot setting on the SemEval 2016 Task 6 stance detection dataset.}~\label{tab:sem}
\end{table} 
% 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth, trim={0 4in 0 3.5in}]{prev_zeroshot_semeval.pdf}
    \caption{Evolution of zero-shot performance, measured using the macro-F score, on the SemEval 2016 Task 6A by various models. Scores of the previous models are taken from~\cite{zhang2022would}.}\label{fig:prev_zeroshot_semeval}
\end{figure}

\begin{table}[ht]
\centering
\setlength\tabcolsep{20pt} 
\begin{tabular}[t]{lccc}
\hline
\ \ \ \ \ \ Model &  Trump & Biden & Bernie \\
\hline
\multirow{1}{10em}{\centering V1} & \multirow{1}{4em}{\centering 82.8/\textbf{83.2}} & 
\multirow{1}{4em}{\centering 82.3/82.0} & 
\multirow{1}{4em}{\centering 79.4/79.4}
\\
\hdashline
\multirow{1}{10em}{\centering Feb 13 ChatGPT plus} & \multirow{1}{4em}{\centering \textbf{83.76}/83.09} & \multirow{1}{4em}{\centering \textbf{83.07/82.69} }& \multirow{1}{4em}{\textbf{\centering 79.7/79.6}}
\end{tabular}
\caption{Micro-F1/Macro-F1 scores of different versions of ChatGPT in a zero-shot setting on the P-stance stance detection dataset.}~\label{tab:pstance}
\end{table} 

Overall, we see an improvement in performance, measured using the micro-F and macro-F scores, in recent versions of ChatGPT compared to V1. 
In particular, we see an average of 12.46 and 8.6 point improvement in the micro and macro-F scores, respectively, when comparing Jan 30 ChatGPT to V1 on the SemEval task. We see a smaller but non-negligible improvement---6.1 point on the micro-F and 1.89 point on the macro-F---when comparing Feb 13 ChatGPT plus to V1 on the same task. Fig.~\ref{fig:prev_zeroshot_semeval} also shows the temporal evolution of zero-shot performances of various models on selected targets of SemEval. The macro-F scores of the models are taken from the previous work~\cite{zhang2022would}. Although it is still difficult to conclude with only a few data points, we see a significant jump in the zero-shot capability of ChatGPT when compared to previous models. Given that ChatGPT is based on InstructGPT3 in which some NLP dataset contamination was already documented~\cite{brown2020language}, this raises further concerns if V1 too may have been contaminated.

A similar plot for the micro-F scores is not shown here due to our pending uncertainties of scores indicated in the previous work~\cite{zhang2022would} (see Appendix~\ref{app:lims}) and the general unavailability of micro-F scores by other models.
On the P-Stance dataset, we observe a 0.74-point improvement in the micro-F scores and a 0.26 point in the macro-F scores when comparing Feb 13 ChatGPT plus to V1.  

In sum, the improvement is greater for SemEval than for the P-Stance dataset. On the SemEval dataset, we also observe a performance drop by Feb 13 ChatGPT plus relative to Jan 30 ChatGPT. Even though the performance has dropped, it is still quite an improvement compared to V1. 
% 

\section{Discussion}~\label{sec:lims}
% 
In this article, we discuss the reasons why we cannot trust the evaluation of ChatGPT models at its face value due to the possibility of data leakage. 
First, the closed nature of the model makes it impossible to verify whether any existing dataset was used or not.
Second, with a constant training loop, it is also impossible to verify that no researchers or users have leaked a particular dataset to the model, especially given the sheer scale of availability of the model (more than 100 million users\footnote{https://seo.ai/blog/chatgpt-user-statistics-facts} at the time of writing). 
Any evaluation attempt using ChatGPT may \emph{expose} ChatGPT to the evaluation dataset.
Even the mere exposure of the input may make evaluation unreliable~\cite{brown2020language,radford2019language}.
Therefore, unless the evaluation is completely new, it is difficult to ensure the lack of data leakage to the model. 

Given that data leakage likely leads to a boost in apparent performance, we did a case study where there \textit{could} have been potential contamination, with documented evidence that researchers performed an evaluation of ChatGPT with an existing test dataset.  
In other words, the stance detection task that uses the SemEval 2016 Task 6 and P-stance datasets may no longer be a zero-shot problem for ChatGPT.
Although we cannot rule out the explanation that the ChatGPT is simply superior to previous models, it is also impossible to rule out the possibility of data leakage.  

% Perhaps more worryingly given the upgradation from GPT3 to InstructGPT to ChatGPT also followed a similar protocol---based on fine-tuning on queries submitted to the OpenAI platform---this raises concerns about the previously claimed zero or few-shot capabilities of these models which has been partly addressed in \citeauthor{brown2020language}~\cite{brown2020language}.

This work sheds light on a bigger problem when it comes to using ChatGPT and similar large language models on NLP benchmarks. Given these models are trained on large chunks of the entire web, care must be taken to ensure that the pre-training and fine-tuning data of these models are not contaminated by the very benchmarks their performance is often tested on. 
Given the results showing that even a benign contamination can lead to measurable differences, making claims about the zero-shot or few-shot inference capabilities of these models require a more careful inspection of the training datasets of these models. 
Yet, this is becoming increasingly challenging because the most prominent language models, like ChatGPT and the recently released GPT-4\footnote{GPT-4's technical report \url{https://cdn.openai.com/papers/gpt-4.pdf} says, ``Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, \textbf{dataset construction}, training method, or similar.''}, are \emph{closed} and more models are following the practice.

Our analysis in this work is illustrative and exhibits many limitations. 
These limitations come from the fact that the ChatGPT system is new and being actively developed. 
The collection and extraction of stance labels from the responses of Jan 30 ChatGPT was done manually on the SemEval 2016 Task 6. However, due to the rate limitations, this was not done in one sitting since Jan 30 ChatGPT did not entertain more than a fixed (approx. 40) queries in an hour. There was a noticeable difference between the responses of ChatGPT at the beginning of the session (more verbose) when compared to when it was nearing its rate limit (less verbose; single-word responses). Additionally, in each sitting, a single chat session was used to feed multiple inputs, one at a time, to ChatGPT~\footnote{sometimes factors network errors which made ChatGPT unresponsive forced us to open a new chat session in the same sitting. But for a major chunk, a single session was used per sitting}, which may have accumulated context for subsequent inputs. In contrast, we used an open-source API for our experiments with the \textit{Feb 13 ChatGPT plus} version, which opened a new chat session per query. This may be one explanation for the drop in performance between Jan 30 and Feb 13 observed in Table~\ref{tab:sem} but recent work showed this to have an insignificant effect, although on a different dataset~\cite{kocon2023chatgpt}. An alternate explanation might be due to catastrophic forgetting---a documented phenomenon in large language models where the model tends to forget older information they were trained on in light of newer information~\cite{mccloskey1989catastrophic}. 
Yet another explanation could be that the Feb 13 ChatGPT plus is more \textit{diplomatic} than its predecessors given OpenAI's pursuit to make it less toxic and less biased. 
Due to the same reasons mentioned above, we could not try multiple queries for each input and could not estimate the uncertainty of the performance. 
The most critical limitation is, as we repeatedly stated above, that our result cannot prove nor disprove whether the data leakage happened or not as well as whether it has affected the evaluation of ChatGPT or not. 
However, we would like to underline that our primary goal of this article is to highlight the ample possibility of data leakage and the impossibility of verifying the \emph{lack of data leakage} with a closed model.
As long as the trend of closed models and continuous training loop continues, it will become more challenging to prevent data leakage (training-test data contamination) and ensure fair evaluation of models.
Therefore, in order to ensure the fair evaluability of the models, we argue that the model creators should (1) pay closer attention to the training datasets and document potential data contamination, (2) create mechanisms through which the training datasets and models can be scrutinized regarding data leakage, and (3) build systems that can prevent data contamination from user inputs. 

\section{Data Availability}

The responses of ChatGPT, from which stance labels were manually extracted, can be made available upon request.

\begin{appendices}

\section{Uncertainties in Zhang et al.~\cite{zhang2022would}}~\label{app:lims}
\
The results we obtain in Tables~\ref{tab:sem} and~\ref{tab:pstance} is compared against \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would} who used an older version of ChatGPT (called V1, in this paper). However, we believe that their work needs more clarification. At the time of writing this manuscript, we have requested further clarification from the authors.

The main source of uncertainty is the difference between the definitions of F1-m and F1-avg. \textit{\citeauthor{zhang2022would}} define F1-m to be the ``macro-F score" and F-avg as ``the average of F1 on Favor on Against" classes. It is our understanding that these two definitions are the same which would mean that for each target, the F1-m ad F1-avg should be the same. However, these scores are different from each other in \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would}.
We also conjecture that there are a few misplaced scores in Tables 1, 2, and 3 in \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would}. For instance, the scores of the PT-HCL and TPDG models in their Tables 1 and 2, should be the macro average F scores according to their original articles. However, these are placed under F1-avg and F1-m respectively in \textit{\citeauthor{zhang2022would}}~\cite{zhang2022would}.  
In our work, hoping to capture the \textit{worst case scenario}, we assume F1-m is the micro average and F1-avg is the macro average.  

Additionally, there is a mismatch between the input query to ChatGPT presented in the body of the previous work and that presented in the figures. We assumed that the format presented in the screenshot is what was used and selected it for this work with the neutral option being present (absent) for SemEval (P-Stance).

\end{appendices} 

\newpage

\printbibliography

\end{document}

