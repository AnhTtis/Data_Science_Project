{
    "arxiv_id": "2303.12767",
    "paper_title": "Can we trust the evaluation on ChatGPT?",
    "authors": [
        "Rachith Aiyappa",
        "Jisun An",
        "Haewoon Kwak",
        "Yong-Yeol Ahn"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2024-03-04"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12767v1"
    ],
    "publication_venue": null,
    "doi": "10.18653/v1/2023.trustnlp-1.5"
}