\section{Related Work}
\label{sec:related}

 \begin{figure*}
  \includegraphics[width=\textwidth]{figures/pipeline.pdf}
  \vspace{-0.5cm}
  \caption{\textbf{An Overview of \OURS{}.} From left to right: \textbf{(0).} \OURS{} takes as input a pair of triplets $\mathcal{P} = (\mathbf{P}, \mathbf{N}, \mathbf{X})$ and $\mathcal{Q} = (\mathbf{Q}, \mathbf{M}, \mathbf{Y})$, both with three dimensions referring to the point cloud, the estimated normals, and the initial features. \textbf{(1)}[$\S{}$.~\ref{sec:local_geometry}]. A stack of encoder blocks hierarchically downsamples the points to coarser superpoints and encodes the local geometry, yielding superpoint triplets $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$. Each encoder block consists of an Attentional Abstraction Layer~(AAL) for downsampling and abstraction, followed by $e\times$ PPF Attention Layers~(PALs) for local geometry encoding and context aggregation. Both of them are based on our proposed PPF Attention Module~(PAM), which enables the pose-agnostic encoding of pure geometry.~(See Fig.~\ref{fig:local_attention}). \textbf{(2)}[$\S{}$.~\ref{sec:global_context}]. Global information is fused to enhance the superpoint features $\mathbf{X}^\prime$ and $\mathbf{Y}^\prime$. Specially, the geometric cues are globally aggregated as a rotation-invariant position representation, which introduces spatial awareness in the coupled cross-frame context aggregation. After a stack of $g\times$ global transformers, the globally-enhanced triplets $\widetilde{\mathcal{P}}^\prime$ and  $\widetilde{\mathcal{Q}}^\prime$ are produced. \textbf{(3).}[$\S{}$.~\ref{sec:local_geometry}] Superpoint triplets $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$ are decoded to point triplets $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$ by a stack of decoder blocks. Each block consists of a Transition Up Layer~(TUL) for upsampling and context aggregation, followed by $d\times$ PALs built upon PAM.  \textbf{(4).}[$\S{}$.~\ref{sec:matching}] Correspondence set $\mathcal{C}$ is generated in a coarse to fine manner~\cite{yu2021cofinet} by matching $\widetilde{\mathcal{P}}^\prime$ and $\widetilde{\mathcal{Q}}^\prime$ first and matching $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$ consecutively. See the context for more details. \textbf{(5).} $\mathcal{C}$ is finally established between $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$.}
  \label{fig:pipeline}
  \vspace{-0.5cm}
\end{figure*}


\noindent\textbf{Methods with Extrinsic Rotation Invariance.} The mainstream of deep learning-based point cloud matching approaches is intrinsically rotation-sensitive. Pioneers \cite{zeng20173dmatch,deng2018ppfnet} learn to describe local patches from a rotation-variant input. FCGF~\cite{choy2019fully} leverages fully-convolutional networks to accelerate geometry description. D3Feat~\cite{bai2020d3feat} jointly detects and describes sparse keypoints for matching. Predator~\cite{huang2021predator} incorporates the global context to enhance the local descriptors and predicts the overlap regions for point sampling. CoFiNet~\cite{yu2021cofinet} extracts coarse-to-fine correspondences to alleviate the repeatability issue of keypoints. GeoTrans~\cite{qin2022geometric} considers the geometric information in fusing the intra-frame information globally. However, the awareness of spatial positions is missing in their cross-frame fusion. Lepard~\cite{li2022lepard} extends the non-rigid shape matching~\cite{saleh2022bending,trappolini2021shape} to point clouds and proposes a re-positioning module to alleviate the pose variations. REGTR~\cite{yew2022regtr} directly regresses the corresponding coordinates and registers point clouds in an end-to-end fashion. However, all of these methods suffer from instability with additional rotations. 


\noindent\textbf{Models with Intrinsic Rotation Invariance.} A branch of handcrafted descriptors~\cite{chua1997point, tombari2010unique, guo2013rotational} align the input to a canonical representation according to an estimated local reference frame~(LRF), while the others~\cite{rusu2008aligning,rusu2009fast,drost2010model} mine the rotation-invariant components and encode them as the representation of the local geometry. Inspired by that, some deep learning-based methods~\cite{deng2018ppf,gojcic2019perfect,barroso2020hdd,saleh2020graphite,ao2021spinnet,yu2022riga,saleh2022bending} are designed to be intrinsically rotation-invariant to make the neural models focus on the pose-agnostic pure geometry. As a pioneer, PPF-FoldNet~\cite{deng2018ppf} consumes PPF-based patches and learns the descriptors using a FoldingNet~\cite{yang2018foldingnet}-based architecture without supervision. LRF-based works~\cite{gojcic2019perfect,saleh2020graphite,ao2021spinnet,saleh2022bending} achieve rotation invariance by aligning their input to the defined canonical representation. YOHO~\cite{wang2022you} adopts a group of rotations to learn a rotation-equivariant feature group and further obtain the invariance via group pooling. A common problem of the rotation-invariant methods is the less distinctive features. To address the issue, RIGA~\cite{yu2022riga} incorporates the global context into local descriptors, which significantly enhances the feature distinctiveness. However, the ineffective local geometry encoding and global position description restrict its performance. 


