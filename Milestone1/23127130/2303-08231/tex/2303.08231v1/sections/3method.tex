\vspace{-0.2cm}
\section{Method}
\label{sec:method}
An overview of \OURS{} can be found in Fig.~\ref{fig:pipeline}. \OURS{} consists of an encoder-decoder architecture named Point Pair Feature Transformer~(PPFTrans) for geometry encoding and a stack of $g\times$global transformers for global context aggregation. The correspondences $\mathcal{C}$ are extracted by matching features in a coarse-to-fine manner~\cite{yu2021cofinet}.
\subsection{Problem Statement}
We tackle the problem of matching a pair of partially-overlapping point clouds $\mathbf{P}\in \mathbb{R}^{n \times 3}$ and $\mathbf{Q}\in \mathbb{R}^{m \times 3}$. We aim at extracting a correspondence set $\mathcal{C}=\{(\mathbf{p}_{i}, \mathbf{q}_{j})\big| \mathbf{p}_i\in \mathbf{P}, \mathbf{q}_j\in \mathbf{Q}\}$ minimizing:
\vspace{-0.1cm}
\begin{equation}
\frac{1}{|C|} \sum_{(\mathbf{p}_{i}, \mathbf{q}_{j})\in \mathcal{C}} \lVert \mathcal{M}^*(\mathbf{p}_{i}) - \mathbf{q}_{j} \rVert_2,
\label{eq:definition}
\vspace{-0.2cm}
\end{equation}
where $\lVert\cdot\rVert_2$ denotes the Euclidean norm and $|\cdot|$ is the set cardinality. $\mathcal{M}^*(\cdot)$ stands for the ground-truth mapping function that maps $\mathbf{p}_i$ to its corresponding position in $\mathbf{Q}$. In rigid scenarios, it is defined by a transformation $\mathbf{T}\in SE(3)$. For the non-rigid cases it can be denoted as a per-point flow $\mathbf{f}_i\in\mathbb{R}^{3}$ known as the deformation field.

\subsection{PPFTrans for Geometry Description}
\label{sec:local_geometry}
\noindent\textbf{Overview.} Taking point cloud $\mathbf{P}$ as an example, PPFTrans consumes a triplet $\mathcal{P} = (\mathbf{P}, \mathbf{N}, \mathbf{X})$, with $\mathbf{P}\in \mathbb{R}^{n\times3}$ the points cloud, $\mathbf{N}\in \mathbb{R}^{n \times 3}$ the normals estimated from $\mathbf{P}$, and $\mathbf{X} = \vec{\mathbf{1}}\in\mathbb{R}^{n\times 1}$ the initial point features. The encoder produces the superpoint triplet  $\mathcal{P}^{\prime}=(\mathbf{P}^\prime, \mathbf{N}^\prime, \mathbf{X}^\prime)$, with $n^\prime$ superpoints and $\mathbf{X}^\prime\in\mathbb{R}^{n^\prime\times c^\prime}$.  With the consecutive decoder, $\mathcal{P}^\prime$ is decoded to a triplet $\hat{\mathcal{P}} = (\hat{\mathbf{P}}, \hat{\mathbf{N}}, \hat{\mathbf{X}})$ including $\hat{n}$ points with features $\hat{\mathbf{X}}\in \mathbb{R}^{\hat{n}\times\hat{c}}$. Notably, as we adopt a Farthest Point Sampling~(FPS) strategy~\cite{qi2017pointnet++}, it always satisfies that $\mathbf{P}^\prime \subseteq \hat{\mathbf{P}} \subseteq \mathbf{P}$ and $\mathbf{N}^\prime \subseteq \hat{\mathbf{N}} \subseteq \mathbf{N}$.  The same goes for a second point cloud $\mathbf{Q}$ with an input triplet $\mathcal{Q}=(\mathbf{Q}\in \mathbb{R}^{m \times 3}, \mathbf{M}\in \mathbb{R}^{m \times 3}, \mathbf{Y} = \vec{\mathbf{1}} \in \mathbb{R}^{m \times 1})$ by the shared architecture. In the rest of this paper, we only demonstrate for $\mathcal{P}$ unless the model processes $\mathcal{Q}$ differently. We further define the \textit{Anchor} triplet as $\mathcal{P}^{A}=(\mathbf{P}^{A}, \mathbf{N}^{A}, \mathbf{X}^{A})$ with $n^{A}$ points and $\mathbf{X}^A\in\mathbb{R}^{n^A \times c^A}$, and the \textit{Supporter} triplet $\mathcal{P}^{S}=(\mathbf{P}^{S}, \mathbf{N}^{S}, \mathbf{X}^{S})$ with $n^{S}$ points and $\mathbf{X}^S\in\mathbb{R}^{n^S \times c^S}$ for the following demonstration. It always holds that $\mathbf{P}^{A}\subseteq\mathbf{P}^{S}\subseteq \mathbf{P}$ and $\mathbf{N}^{A}\subseteq\mathbf{N}^{S}\subseteq \mathbf{N}$.

\vspace{-0.5cm}

\subsubsection{Pose-Agnostic Local Coordinates} 
The basis of PPFTrans is the pose-agnostic local coordinate representation that we construct based on PPFs. We detail the construction on \textit{Supporter} triplet $\mathcal{P}^{S}$ w.r.t. the \textit{Anchor} triplet $\mathcal{P}^{A}$ hereafter. Let $\mathcal{P}^{A}_i := (\mathbf{p}^{A}_i\in \mathbf{P}^{A}, \mathbf{n}^{A}_i\in \mathbf{N}^{A}, \mathbf{x}^{A}_i\in \mathbf{X}^{A})\in \mathcal{P}^{A}$ denote the triplet constructed by picking the corresponding item on each dimension. For each $\mathcal{P}^{A}_i$, its $k$-nearest neighbors in $\mathcal{P}^{S}$ are first retrieved according to the Euclidean distance between $\mathbf{p}^{A}_i$ and $\mathbf{P}^{S}$, denoted as $\mathcal{P}^{S}_{\mathcal{N}(i)} := (\mathbf{P}^{S}_{\mathcal{N}(i)}, \mathbf{N}^{S}_{\mathcal{N}(i)},\mathbf{X}^{S}_{\mathcal{N}(i)})\subseteq \mathcal{P}^{S}$, with $\mathcal{N}(i)$ the $k$-nearest neighbors. We then adopt PPFs to construct a local coordinate system around each $\mathbf{p}^{A}_i$ to represent the pose-agnostic position of $\mathbf{P}^{S}_\mathcal{N}(i)$ w.r.t. it. The coordinate of point $\mathbf{p}_j^{S} \in \mathbf{P}^{S}_\mathcal{N}(i)$ is transferred to:
\begin{equation}
 \mathbf{e}^{S}_j = (\lVert\mathbf{d}\rVert_2, \angle(\mathbf{n}^{A}_i, \mathbf{d}), \angle(\mathbf{n}_j^{S}, \mathbf{d}), \angle(\mathbf{n}_j^{S}, \mathbf{n}^{A}_i)),
\label{eq:ppfs}
\end{equation}
with $\mathbf{d} = \mathbf{p}_j^{S} - \mathbf{p}_i^{A}$, and $\mathbf{n}_i^{A}$ and $\mathbf{n}_j^{S}$ the estimated normals of $\mathbf{p}_i^{A}$ and $\mathbf{p}_j^{S}$, respectively. $\angle(\mathbf{v}_1,\mathbf{v}_2)$ computes the angles between the two vectors~\cite{deng2018ppf,birdal2015point}. The transferred coordinates of $\mathbf{P}^{S}_{\mathcal{N}(i)}$ are denoted as $\mathbf{E}^{S}_{\mathcal{N}(i)}$.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{figures/local_attention.pdf}
  \caption{\textbf{Left}: The workflow of the PPF Attention Module~(PAM). \textbf{Right}: The computation of the local attention.}
  \label{fig:local_attention}
  \vspace{-0.5cm}
\end{figure}

\subsubsection{PPF Attention Module} 
The core component of PPFTrans is the PPF Attention Module~(PAM) that takes as input a \textit{Supporter} triplet $\mathcal{P}^{S}$. With the \textit{Anchor} point cloud $\mathbf{P}^{A}$ and associated normals $\mathbf{N}^{A}$ known, PAM generates the \textit{Anchor} features $\mathbf{X}^{A}$ by aggregating the pose-agnostic local geometry and highly-representative learned context from $\mathcal{P}^{S}$. PAM can be defined as:
\begin{equation}
\mathcal{P}^{A} = \delta(\mathbf{P}^{A}; \mathbf{N}^{A} \big| \mathcal{P}^{S}).
\label{eq:pam}
\end{equation}

 As shown in Fig.~\ref{fig:local_attention}~(a), for each $\mathbf{p}^A_i\in \mathbf{P}^A$ with normal $\mathbf{n}^A_i$, we find its corresponding point $\mathbf{p}^S_j\in \mathbf{P}^S$ whose associated feature $\mathbf{x}^S_j$ is assigned to $\mathbf{p}^A_i$ as the initial description. Then, $k$-nearest neighbors from $\mathbf{P}^{S}$ are retrieved according to the Euclidean distance in 3D space, yielding $\mathbf{P}^{S}_{\mathcal{N}(i)}\subseteq\mathbf{P}^{S}$ and $\mathbf{X}^{S}_{\mathcal{N}(i)}\subseteq\mathbf{X}^{S}$. Following Eq.~\ref{eq:ppfs}, $\mathbf{P}^{S}_{\mathcal{N}(i)}$ is transferred to the pose-agnostic position representation $\mathbf{E}^{S}_{\mathcal{N}(i)}$ which is consecutively projected to the coordinate embedding $\mathbf{E}_S$ via a linear layer. ${\mathbf{x}}^{S}_j$ and $\mathbf{X}^{S}_{\mathcal{N}(i)}$ are projected to the contextual features $\mathbf{x}_S$ and $\mathbf{X}_S$ by a second shared linear layer, respectively. In Fig.~\ref{fig:local_attention}~(b), the designed local attention mechanism uses five learnable matrices $\mathbf{W}_G$, $\mathbf{W}_E$, $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$ to further project the input features. Specifically, $\mathbf{W}_G$ and $\mathbf{W}_E$ project the input coordinate embedding to the geometric representation and positional encoding by:

\begin{equation}
    \mathbf{G} = \mathbf{E}_S\mathbf{W}_G, \quad \mathbf{E} = \mathbf{E}_S\mathbf{W}_E,
\label{eq:project1}
\end{equation}
respectively. Similarly, $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$ project the learned context to \textit{Query}, \textit{Key}, and \textit{Value} as:

\vspace{-0.5cm}
\begin{equation}
    \mathbf{q} = \mathbf{x}_S\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X}_S\mathbf{W}_K, \quad \mathbf{V} = \mathbf{X}_S\mathbf{W}_V,
\label{eq:project2}
\vspace{-0.2cm}
\end{equation}
respectively, where $\mathbf{q}$ and $\mathbf{K}$ are used for retrieving similar context and $\mathbf{V}$ encodes the contextual messages flowing from \textit{Supporter} to \textit{Anchor}. The attention vector $\mathbf{a}$ that measures the contextual similarity between \textit{Anchor} and \textit{Supporter}, and the message $\mathbf{m}$ that encodes both the pose-agnostic geometry and the representative context read as:
\begin{equation}
    \mathbf{a} = \text{Softmax}(\frac{\mathbf{q}\mathbf{E}^{T} + \mathbf{q}\mathbf{K}^{T}}{\sqrt{c_0}}) \quad \text{and} \quad \mathbf{m} = \mathbf{a}\mathbf{G} + \mathbf{a}\mathbf{V},
\label{eq:local_attention}
\end{equation}
respectively. The message $\mathbf{m}$ is further projected and aggregated to $\mathbf{x}^S_j$ via an element-wise addition followed by a normalization through LayerNorm~\cite{ba2016layer}. A final linear layer projects the obtained feature to $\mathbf{x}^{A}_i\in \mathbb{R}^{c_A}$, from which $\mathbf{X}^{A}$ is finally obtained to formulate the output $\mathcal{P}^A$ with the known $\mathbf{P}^A$ and $\mathbf{N}^A$.

\subsubsection{Encoder and Decoder Blocks}
\label{seq: encoder}
\noindent\textbf{Encoder.} 
We construct our encoder by stacking several encoder blocks, each consisting of an Attentional Abstraction Layer~(AAL) followed by $e\times$PPF Attention Layers~(PALs). The encoder block takes as input the \textit{Supporter} triplet $\mathcal{P}^{S}$ which consecutively flows to AAL, where $\mathbf{P}^{S}$ is downsampled to obtain the \textit{Anchor} points $\mathbf{P}^{A}$ with associated normals $\mathbf{N}^{A}$ via FPS~\cite{qi2017pointnet++}. The \textit{Anchor} triplet $\mathcal{P}^{A}$ is consecutively generated in AAL via a PAM as $\mathcal{P}^{A} = \delta(\mathbf{P}^{A}; \mathbf{N}^{A} \big| \mathcal{P}^{S})$ according to Eq.~\ref{eq:pam}. A sequence of $e\times$PALs is applied for further enhancing the \textit{Anchor} features $\mathbf{X}^{A}$, with each PAL updating the features as:
\begin{equation}
    \mathcal{P}^{A} \leftarrow \theta(\mathcal{P}^{A})=\text{ReLU}(\mathbf{X}^{A} + \phi(\delta(\mathbf{P}^{A};\mathbf{N}^{A} \big| \mathcal{P}^{A}))),
\label{eq:PAL}
\end{equation}

\noindent with $\phi$ the LayerNorm~\cite{ba2016layer} and $\delta$ the PAM of PAL. $\mathcal{P}^{A}\leftarrow\theta(\mathcal{P}^{A})$ depicts updating the features of $\mathcal{P}^{A}$ with the output of PAL. The output of the encoder is defined as $\mathcal{P}^\prime:=\mathcal{P}^{A}$, with $\mathcal{P}^{A}$ the output of the last encoder block.

\noindent\textbf{Decoder.} 
We construct the decoder part by stacking a series of decoder blocks, including a Transition Up Layer~(TUL) followed by $d\times$ PAL. The encoder block takes as input both the \textit{Anchor} triplet $\mathcal{P}^{A}$ and the \textit{Supporter} triplet $\mathcal{P}^{S}$, where $\mathcal{P}^{S}$ is from the encoder via skip connections. The input is forwarded to the Transition Up Layer~(TUL), where each $\widetilde{\mathbf{x}}^{S}_j \in \widetilde{\mathbf{X}}^{S}$ is interpolated by:
\begin{equation}
\widetilde{\mathbf{x}}^{S}_j = \frac{\sum_{i\in \mathcal{N}(j)} w_i^j \mathbf{x}^{A}_i}{\sum_{i\in \mathcal{N}(j)} w_i^j} , \ \text{with} \ w_i^j = \frac{1}{\lVert \mathbf{p}^{S}_j - \mathbf{p}^{A}_i\rVert_2},
\label{eq:interpolation}
\end{equation}
\noindent with $\mathcal{N}(j)$ the $k$-nearest neighbors of $\mathbf{p}^S_j$. \textit{Supporter} features are updated by two linear layers as $\mathcal{P}^{S}\leftarrow \zeta_1(\mathbf{X}^{S}) + \zeta_2(\widetilde{\mathbf{X}}^{S})$. A sequence of $d\times$ PALs are adopted after the TUL, with each enhancing the features as $\mathcal{P}^{S} \leftarrow \theta(\mathcal{P}^{S})$ according to Eq.~\ref{eq:PAL}. The output of decoder is denoted as $\hat{\mathcal{P}}:=\mathcal{P}^{S}$, with $\mathcal{P}^{S}$ the output of the last decoder block.

%\subsubsection{Comparisons to Existing Local Attention.}
%\label{sec: comparison}
%Our designed PPFAttention layer differs to the attention mechanism used in~\cite{zhao2021point} mainly in two aspects: 1). We leverage the standard multi-head scalar attention proposed in~\cite{vaswani2017attention} to cut off the heavy computation and memory burden of the vector attention used in~\cite{zhao2021point}. A comparison between these two different mechanisms in terms of efficiency can be found in Fig.~\ref{fig:comparison}. 2). As we observed in our experiments, the relative coordinate used in~\cite{zhao2021point} with the vector attention mechanism is hard to handle the variance of poses, as the coordinate itself is sensitive to rotations. What is even worse is that a combination of the relative coordinates and the standard multi-head attention mechanism cannot converge in the point cloud matching task. Based on this observation, we adopt the PPFs, which guarantees the rotation invariance by design, as a local coordinate system to provide the positional encoding in our local attention. Such a design avoids the redundant learning of pose variance through the network and facilitates the pure geometry encoding, which helps \OURS{} surpass the state-of-the-arts while keeping its efficiency.


\begin{figure}
  \includegraphics[width=0.48\textwidth]{figures/global_self.pdf}
  \vspace{-0.8cm}
  \caption{The computation graph of our global transformer consisting of the coupled Geometry-Aware Self-Attention Module~(GSM) and Position-Aware Cross-Attention Module~(PCM).}
  \label{fig:global_self}
  \vspace{-0.5cm}
\end{figure}

\subsection{Global Transformer for Context Aggregation}
\label{sec:global_context}
\noindent\textbf{Overview.} Different from previous works~\cite{huang2021predator,yu2021cofinet,qin2022geometric,yew2022regtr,yu2022riga} that disentangle the self- and cross-attention as individual modules, we couple them together as a global transformer, with the cross-frame spatial awareness learned by the self-attention in a rotation-invariant manner. The global transformer takes as input a pair of triplets $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$, and enhances the features with the global context, yielding $\widetilde{\mathbf{X}}^{\prime}\in \mathbb{R}^{n^\prime \times {c}^\prime}$ and $\widetilde{\mathbf{Y}}^{\prime}\in \mathbb{R}^{m^\prime \times {c}^\prime}$, respectively. We stack $g\times$global transformers, with each including a Geometry-Aware Self-Attention Module~(GSM) and a Position-Aware Cross-Attention Module~(PCM)~(See Fig.~\ref{fig:pipeline} and Fig.~\ref{fig:global_self}).

%\begin{figure}
%  \includegraphics[width=0.48\textwidth]{figures/global_cross.pdf}
%  \caption{\textbf{Illustration of the Position-Aware Cross-Attention.}}
%  \label{fig:global_cross}
%\end{figure}

\noindent\textbf{Geometry-Aware Self-Attention Module.} We detail GSM at the bottom of Fig.~\ref{fig:global_self}. GSM has two branches, where the geometry branch mines the geometric cues from the geometric embedding proposed in~\cite{qin2022geometric}, and the context branch aggregates the global context across the frame. We refer the readers to the Appendix for the detailed construction of the pairwise geometric embedding~$\mathbf{G}^\prime_P\in \mathbb{R}^{n^\prime \times n^\prime \times c^\prime}$. Similar to Eq.~\ref{eq:project1}, the geometric context $\mathbf{G}^\prime$ and the positional encoding $\mathbf{E}^\prime$ are linearly projected from $\mathbf{G}^\prime_P$ by two learnable matrices in the geometry branch, respectively. $\mathbf{E}^\prime$ is further processed in the geometry branch and finally leveraged as the rotation-invariant cross-frame position awareness. In the context branch, $\mathbf{Q}^\prime$, $\mathbf{K}^\prime$, and $\mathbf{V}^\prime$ are obtained by linearly mapping the input features $\mathbf{X}^\prime$ with the corresponding matrices in a similar way as Eq.~\ref{eq:project2}. Then the hybrid score matrix $\mathbf{S}^\prime\in \mathbb{R}^{n^\prime \times n^\prime}$ is computed as the similarity from both the geometric and contextual information:

\begin{equation}
    \mathbf{S}^\prime(i, j) = \frac{(\mathbf{q}^\prime_i)(\mathbf{e}^\prime_{i, j} + \mathbf{k}^\prime_j)^T}{\sqrt{c^\prime}},
\label{eq:hybrid_score}
\end{equation}
with $\mathbf{e}^\prime_{i, j} := \mathbf{E}^\prime(i, j, :)$, $\mathbf{q}^\prime_{i} := \mathbf{Q}^\prime(i, :)$, and $\mathbf{k}^\prime_{j} := \mathbf{K}^\prime(j, :)$ the $c^\prime$-dimension vectors.
The hybrid attention $\mathbf{A}^\prime$ is obtained via a Softmax function over each row of $\mathbf{S}^\prime$, and the geometric messages $\mathbf{M}^\prime_G \in \mathbb{R}^{n^\prime \times c^\prime}$ are computed as:

\begin{equation}
\mathbf{M}_G^\prime(i, :) = \sum_{1\leq j \leq n^\prime} a^\prime_{i, j}\mathbf{g}^\prime_{i, j},
\label{eq:geo_mes}
\end{equation}

\noindent with $a^\prime_{i, j} := \mathbf{A}^\prime(i, j)$ and $\mathbf{g}^\prime_{i, j} := \mathbf{G}^\prime(i, j, :)$. The contextual messages $\mathbf{M}_V^\prime \in \mathbb{R}^{n^\prime\times c^\prime}$ are computed by $\mathbf{A}^\prime\mathbf{V}^\prime$. After a feed-forward network~\cite{vaswani2017attention}, global positional encoding $\mathbf{E}^\prime_P$ and globally-enhanced context $\mathbf{C}^\prime_P$ are generated.

\noindent\textbf{Position-Aware Cross-Attention Module.} The PCM consumes a pair of doublets $(\mathbf{E}^\prime_P, \mathbf{C}^\prime_P)$ and $(\mathbf{E}^\prime_Q, \mathbf{C}^\prime_Q)$ that are generated from $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$ by a shared GSM, respectively. As the cross-attention is directional, we apply the same PCM twice, with the first aggregation from $\mathcal{Q}^\prime$ to $\mathcal{P}^\prime$~(See the top part of Fig.~\ref{fig:global_self}), and the second reversed. As the first step, the rotation-invariant spatial representation is incorporated to make the consecutive cross-attention position-aware, yielding position-aware features $\mathbf{F}^\prime_P=\mathbf{E}^\prime_P + \mathbf{C}^\prime_P \in \mathbb{R}^{n^\prime \times c^\prime}$ and $\mathbf{F}^\prime_Q=\mathbf{E}^\prime_Q + \mathbf{C}^\prime_Q \in \mathbb{R}^{m^\prime \times c^\prime}$. Similar to Eq.~\ref{eq:project2}, $\widetilde{\mathbf{Q}}^\prime$, $\widetilde{\mathbf{K}}^\prime$, and $\widetilde{\mathbf{V}}^\prime$ are defined as the linear projection of $\mathbf{F}^\prime_P$, $\mathbf{F}^\prime_Q$, and $\mathbf{F}^\prime_Q$, respectively. The attention matrix $\widetilde{\mathbf{A}}\in \mathbb{R}^{n^\prime \times m^\prime}$ is computed via a row-wise softmax function applied on $\widetilde{\mathbf{Q}}^\prime\widetilde{\mathbf{K}}^{\prime T}$. The fused messages are presented as $\widetilde{\mathbf{A}}\widetilde{\mathbf{V}}^\prime$, which are finally mapped to the output features $\widetilde{\mathbf{X}}^\prime$ through a feed-forward network. As we introduce spatial awareness at the beginning of PCM, both the attention computation and message fusion are aware of the cross-frame positions. After the twice application of PCM, the input features are enhanced as $\mathcal{P}^\prime\leftarrow\widetilde{\mathbf{X}}^\prime$ and $\mathcal{Q}^\prime\leftarrow\widetilde{\mathbf{Y}}^\prime$, respectively. The global aggregation stage finally generates a pair of triplets $\widetilde{\mathcal{P}}^\prime:=(\mathbf{P}^\prime, \mathbf{N}^\prime, \widetilde{\mathbf{X}}^\prime)$ and $\widetilde{\mathcal{Q}}^\prime:=(\mathbf{Q}^\prime, \mathbf{M}^\prime, \widetilde{\mathbf{Y}}^\prime)$, with the enhanced features from the last global transformer.


\subsection{Point Matching and Loss Funcion}

\label{sec:matching}
\noindent\textbf{Superpoint Matching.} As shown in Fig.~\ref{fig:pipeline}, the point matching stage takes as input a pair of superpoint triplets $\widetilde{\mathcal{P}}^\prime$ and $\widetilde{\mathcal{Q}}^\prime$ output by the global aggregation stage, as well as a pair of point triplets $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$ produced by the decoder. 
We adopt the coarse-to-fine matching strategy proposed in~\cite{yu2021cofinet} for feature-based point matching. Following~\cite{qin2022geometric}, we first normalize the superpoint features $\widetilde{\mathbf{X}}^\prime$ and $\widetilde{\mathbf{Y}}^{\prime}$ on to a unit hypersphere, and measure the pairwise similarity using a Gaussian correlation matrix $\widetilde{\mathbf{S}}$ with $\widetilde{\mathbf{S}}(i, j)=-\text{exp}(-\lVert\widetilde{\mathbf{x}}_i^\prime - \widetilde{\mathbf{y}}_j^\prime\rVert_2^2)$. After a dual-normalization~\cite{rocco2018neighbourhood,sun2021loftr,qin2022geometric} on $\widetilde{\mathbf{S}}$ for global feature correlation, superpoints associated to the top-$k$ entries are selected as the coarse correspondence set $\mathcal{C}^\prime = \{(\mathbf{p}_i^\prime, \mathbf{q}_j^\prime)\big| \mathbf{p}_i^\prime \in \mathbf{P}^\prime, \mathbf{q}_j^\prime \in \mathbf{Q}^\prime\}$.

\noindent\textbf{Point Matching.} For extracting point correspondences from $\mathcal{C}^\prime$, denser points $\hat{\mathbf{P}}$ and $\hat{\mathbf{Q}}$ are first assigned to superpoints. To this end, the point-to-node strategy~\cite{yu2021cofinet} is leveraged, where each point is assigned to its closest superpoint in 3D space. Given a superpoint $\mathbf{p}_i^\prime \in \mathbf{P}^\prime$, the group of points assigned to it is denoted as $\hat{\mathbf{G}}^P_i$ with $\hat{\mathbf{G}}^P_i \subseteq \hat{\mathbf{P}}$. The group of features associated to $\hat{\mathbf{G}}^P_i$ is further defined as $\hat{\mathbf{G}}^X_i$ with $\hat{\mathbf{G}}^X_i \subseteq \hat{X}$. For each superpoint correspondence $\mathcal{C}^\prime_l = (\mathbf{p}^\prime_i, \mathbf{q}^\prime_j)$, the similarity between the corresponding feature groups $\hat{\mathbf{G}}^X_i$ and $\hat{\mathbf{G}}^Y_j$ is calculated as
$\hat{\mathbf{S}}_l = \hat{\mathbf{G}}^X_i (\hat{\mathbf{G}}_j^{Y})^T /\sqrt{\hat{c}}$, with $\hat{c}$ the feature dimension. We then follow~\cite{sarlin2020superglue} to append a stack row and column to $\hat{\mathbf{S}}_l$ filled with a learnable parameter $\alpha$, and iteratively run the Sinkhorn Algorithm~\cite{sinkhorn1967concerning} on it, yielding a normalized similarity matrix $\overline{\mathbf{C}}_l$. By removing the slack row and entry of $\overline{\mathbf{C}}_l$ we obtain the confidence matrix $\hat{\mathbf{C}}_l$, from which the mutual top-k entries, \ie., entries with top-$k$ confidence on both the row and the column, are selected to formulate a point correspondence set $\mathcal{C}_l$. The final correspondence set $\mathcal{C}$ is collected by $\mathcal{C} = \cup^{|\mathcal{C}|}_{l=1} \mathcal{C}_l$. 

\noindent\textbf{Loss Function.} Our loss function reads as $\mathcal{L} = \mathcal{L}_s + \lambda\mathcal{L}_p$, with a superpoint matching loss $\mathcal{L}_s$ and a point matching loss $\mathcal{L}_p$ balanced by a hyper-parameter $\lambda$~($\lambda=1$ by default). The detailed definition is introduced in the Appendix.

\iffalse
\subsection{Loss Functions}
\label{sec:loss}


\noindent\textbf{Superpoint Matching Loss.} We use the Circle Loss~\cite{sun2020circle} for superpoint matching. Following~\cite{qin2022geometric}, we use the overlapping ratio between the vicinity of superpoints to weigh different ground truth superpoint correspondences. More specifically, for each superpoint $\mathbf{p}^\prime_i\in \mathbf{P}^\prime$, we sample a positive set of superpoints from $\mathbf{Q}^\prime$, denoting as $\mathcal{E}^P_i = \{\mathbf{q}^\prime_j \in \mathbf{Q}^\prime\| \mathcal{R}(\mathbf{p}^\prime_i, \mathbf{q}^\prime_j) > \tau_r\}$, where $\mathcal{R}$ is the function that calculates the overlapping ratio between the vicinity of two superpoints, and $\tau_r$ is the threshold to select the positive samples~($\tau_r$=0.1 by default). The detailed definition of $\mathcal{R}$ is given in the Appendix. We further sample a negative set of superpoints $\mathcal{F}^{P}_i$ from $\mathbf{q}^\prime_j\notin \mathcal{E}^P_i$. Then for $\mathbf{P}^\prime$, the superpoint matching loss is computed as:

\begin{equation}
\mathcal{L}^P_c = \frac{1}{n^\prime}\sum_{i=1}^{n^\prime}log[1 + \sum_{\mathbf{q}^\prime_j \in \mathcal{E}^P_i}{e^{r^j_i\beta^j_e(d^j_i-\Delta_e)}}\cdot \sum_{\mathbf{q}^\prime_k\in \mathcal{F}^P_i}{e^{\beta^k_f(\Delta_f-d^k_i)}}],
\label{eq:coarse}
\end{equation}

\noindent where $r^j_i := \mathcal{R}(\mathbf{p}^\prime_i, \mathbf{q}^\prime_j)$ and $d^j_i = \lVert\mathbf{x}^\prime_i - \mathbf{y}^\prime_j \rVert_2$, with $\mathbf{x}^\prime_i \leftrightarrow \mathbf{p}^\prime_i$ and $\mathbf{y}^\prime_j \leftrightarrow \mathbf{q}^\prime_j$. Moreover, $\Delta_e$ and $\Delta_f$ are the positive and negative margins~($\Delta_e$=0.1 and $\Delta_f$=1.4 by default). $\beta^j_e = \gamma(d^J_i - \Delta_e)$ and $\beta^k_f=\gamma(\Delta_f - d^k_i)$ are the weights individually determined for different samples, with a hyper-parameter $\gamma$. The same loss for $\mathbf{Q}^\prime$ is defined in a similar way, and we have the overall superpoint matching loss as $\mathcal{L}_s = (\mathcal{L}^P_s + \mathcal{L}^Q_s)/2$.

\noindent\textbf{Point Matching Loss.} For each superpoint correspondence $\mathcal{C}^\prime_l = (\mathbf{p}^\prime_i, \mathbf{q}^\prime_j) \in \mathcal{C}^\prime$, we adopt a negative log-likelihood loss~\cite{sarlin2020superglue} on its corresponding normalized similarity matrix $\overline{\mathbf{C}}_l\in \mathbb{R}^{(\overline{n} + 1)\times (\overline{m} + 1)}$, with $\overline{n}_l = |\hat{\mathbf{G}}^{X}_i|$ and $\overline{m}_l = |\hat{\mathbf{G}}^{Y}_j|$. We further define $\mathcal{M}_l = \{(u, v)| \hat{\mathbf{p}}_u \in \hat{\mathbf{G}}^P_i \Leftrightarrow \hat{\mathbf{q}}_v \in \hat{\mathbf{G}}^Q_j\}$, with $\Leftrightarrow$ denoting corresponding points. We further define two sets $\mathcal{I}_l = \{u|\hat{\mathbf{p}}_u \nLeftrightarrow \hat{\mathbf{q}}_v, \forall\hat{\mathbf{q}}_v \in \hat{\mathbf{G}}^Q_j\}$ and $\mathcal{J}_l = \{v|\hat{\mathbf{q}}_v \nLeftrightarrow \hat{\mathbf{p}}_u, \forall\hat{\mathbf{p}}_u \in \hat{\mathbf{G}}^P_i\}$, with $\nLeftrightarrow$ depicting non-corresponding relationship. Then the point matching loss on $\mathcal{C}^\prime_l$ can be defined as:


\begin{equation}
\begin{aligned}
\mathcal{L}_f^l = -\sum_{(u, v)\in \mathcal{M}_l} \log \overline{c}^l_{u, v} - \sum_{u\in \mathcal{I}_l} \log \overline{c}^l_{u, \overline{m}_l + 1} \\ - \sum_{v \in \mathcal{J}_l} \log \overline{c}^l_{\overline{n}_l + 1, v} ,
\end{aligned}
\label{eq:fine}
\end{equation}
where $\overline{c}^{l}_{u, v}:= \overline{\mathbf{C}}_l(u, v)$ stands for the entry on the $u^{th}$ row  and $v^{th}$ column of $\overline{\mathbf{C}}_l$. The overall point matching loss reads as $\mathcal{L}_p = \frac{1}{|\mathcal{C}^\prime|}\sum_{l=1}^{|\mathcal{C}^\prime|} \mathcal{L}^l_p$.
\fi