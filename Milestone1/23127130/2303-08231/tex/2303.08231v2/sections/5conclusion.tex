\vspace{-0.2cm}
\section{Conclusion}
\label{sec:conclusion}
We introduced \OURS{} - an intrinsically rotation-invariant model for point cloud matching. We proposed PAM~(PPF Attention Mechanism) that embeds PPF-based local coordinates to encode rotation-invariant geometry. This design lies at the core of AAL~(Attention Abstraction Layer), PAL~(PPF Attention Layer), and TUL~(Transition Up Layer) which are consecutively stacked to compose PPFTrans~(PPF Transformer) for representative and pose-agnostic geometry description. We further enhanced features by introducing a novel global transformer architecture, which ensures the rotation-invariant cross-frame spatial awareness.
%The global context is then aggregated for feature enhancement via the global transformer structure with the rotation-invariant cross-frame spatial awareness. 
Extensive experiments are conducted on both rigid and non-rigid benchmarks to demonstrate the superiority of our approach, especially the remarkable robustness against arbitrary rotations. %However, as \OURS{} does not explicitly handle the occlusion, it may fail in cases with extremely limited overlap. 
Limitations are discussed in the Appendix.

\noindent\textbf{Acknowledgment.} This paper is supported by the National Natural Science Foundation of China under Grant No. 62025208. We appreciate the help from Lennart Bastian, Mert Karaoglu, Ning Liu, and Zhiying Leng.