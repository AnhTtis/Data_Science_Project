\section{Related Work}
\label{sec:related}

\noindent\textbf{Models with Extrinsic Rotation Invariance.} The mainstream of deep learning-based point cloud matching approaches is intrinsically rotation-sensitive. Pioneers \cite{zeng20173dmatch,deng2018ppfnet} learn to describe local patches from a rotation-variant input. FCGF~\cite{choy2019fully} leverages fully-convolutional networks to accelerate the geometry description. D3Feat~\cite{bai2020d3feat} jointly detects and describes sparse keypoints for matching. Predator~\cite{huang2021predator} incorporates the global context to enhance the local descriptors and predicts the overlap regions for keypoint sampling. CoFiNet~\cite{yu2021cofinet} extracts coarse-to-fine correspondences to alleviate the repeatability issue of keypoints. GeoTrans~\cite{qin2022geometric} considers the geometric information in fusing the intra-frame context globally. However, the awareness of spatial positions is missing in the cross-frame aggregation. Lepard~\cite{li2022lepard} extends the non-rigid shape matching~\cite{trappolini2021shape,saleh2022bending,tang2022neural} to point clouds~\cite{qin2023deep} and proposes a re-positioning module to alleviate the pose variations. REGTR~\cite{yew2022regtr} directly regresses the corresponding coordinates and registers point clouds in an end-to-end fashion. Nonetheless, all of these methods suffer from instability with additional rotations. 


\noindent\textbf{Methods with Intrinsic Rotation Invariance.} A branch of handcrafted descriptors~\cite{chua1997point, tombari2010unique, guo2013rotational} aligns the input to a canonical representation according to an estimated local reference frame~(LRF), while the others~\cite{rusu2008aligning,rusu2009fast,drost2010model} mine the rotation-invariant components and encode them as the representation of the local geometry. Inspired by that, some deep learning-based methods~\cite{deng2018ppf,gojcic2019perfect,barroso2020hdd,saleh2020graphite,ao2021spinnet,yu2022riga,saleh2022bending} are designed to be intrinsically rotation-invariant to make the neural models focus on the pose-agnostic pure geometry. As a pioneer, PPF-FoldNet~\cite{deng2018ppf} consumes PPF-based patches and learns the descriptors using a FoldingNet~\cite{yang2018foldingnet}-based architecture without supervision. LRF-based works~\cite{gojcic2019perfect,saleh2020graphite,ao2021spinnet,saleh2022bending} achieve rotation invariance by aligning their input to the defined canonical representation. YOHO~\cite{wang2022you} adopts a group of rotations to learn a rotation-equivariant feature group and further obtain the invariance via group pooling. A common problem of the rotation-invariant methods is the less distinctive features. Although RIGA~\cite{yu2022riga} incorporates the global context into local descriptors to enhance the feature distinctiveness, its ineffective local geometry encoding and global position description learned by PointNet~\cite{qi2017pointnet} still constraint the representation ability of its descriptors. 


 \begin{figure*}
  \includegraphics[width=\textwidth]{figures/pipeline.pdf}
  \vspace{-0.5cm}
  \caption{\textbf{An Overview of \OURS{}.} From left to right: \textit{\textbf{(0).}} \OURS{} takes as input a pair of triplets $\mathcal{P} = (\mathbf{P}, \mathbf{N}, \mathbf{X})$ and $\mathcal{Q} = (\mathbf{Q}, \mathbf{M}, \mathbf{Y})$, each with three dimensions referring to the point cloud, the estimated normals, and the initial features. \textit{\textbf{(1).}}[$\S{}$.~\ref{sec:local_geometry}] A stack of encoder blocks hierarchically downsamples the points to coarser superpoints and encodes the local geometry, yielding superpoint triplets $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$. Each encoder block consists of an Attentional Abstraction Layer~(AAL) for downsampling and abstraction, followed by $e\times$ PPF Attention Layers~(PALs) for local geometry encoding and context aggregation. Both of them are based on our proposed PPF Attention Mechanism~(PAM), which enables the pose-agnostic encoding of pure geometry.~(See Fig.~\ref{fig:differences} and Fig.~\ref{fig:local_attention}). \textit{\textbf{(2).}}[$\S{}$.~\ref{sec:global_context}] Global information is fused to enhance the superpoint features of $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$. The geometric cues are globally aggregated as a rotation-invariant position representation, which introduces spatial awareness in the consecutive cross-frame context aggregation. After a stack of $g\times$ global transformers, the globally-enhanced triplets $\widetilde{\mathcal{P}}^\prime$ and  $\widetilde{\mathcal{Q}}^\prime$ are produced. \textit{\textbf{(3).}}[$\S{}$.~\ref{sec:local_geometry}] Superpoint triplets $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$ are decoded to point triplets $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$ by a stack of decoder blocks. Each block consists of a Transition Up Layer~(TUL) for upsampling and context aggregation, followed by $d\times$ PALs.  \textit{\textbf{(4).}}[$\S{}$.~\ref{sec:matching}] By adopting the coarse-to-fine matching~\cite{yu2021cofinet}, $\widetilde{\mathcal{P}}^\prime$ and $\widetilde{\mathcal{Q}}^\prime$ are matched to generate superpoint correspondences, which are consecutively refined to point correspondences between $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$. \textit{\textbf{(5).}} $\hat{\mathcal{C}}$ is established between $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$.}
  \label{fig:pipeline}
  \vspace{-0.5cm}
\end{figure*}
