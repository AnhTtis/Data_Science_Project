\vspace{-0.2cm}
\section{Method}
\label{sec:method}
\noindent\textbf{Problem Statement.} We tackle the problem of matching a pair of partially-overlapping point clouds $\mathbf{P}\in \mathbb{R}^{n \times 3}$ and $\mathbf{Q}\in \mathbb{R}^{m \times 3}$, and extract a correspondence set $\hat{\mathcal{C}}=\{(\hat{\mathbf{p}}_{i}, \hat{\mathbf{q}}_{j})\big| \hat{\mathbf{p}}_i\in \hat{\mathbf{P}} \subseteq \mathbf{P}, \hat{\mathbf{q}}_j\in \hat{\mathbf{Q}} \subseteq \mathbf{Q}\}$ that minimizes:
\vspace{-0.1cm}
\begin{equation}
\frac{1}{|\hat{\mathcal{C}}|} \sum_{(\hat{\mathbf{p}}_{i}, \hat{\mathbf{q}}_{j})\in \hat{\mathcal{C}}} \lVert \mathcal{M}^*(\hat{\mathbf{p}}_{i}) - \hat{\mathbf{q}}_{j} \rVert_2,
\label{eq:definition}
\vspace{-0.2cm}
\end{equation}
where $\lVert\cdot\rVert_2$ denotes the Euclidean norm and $|\cdot|$ is the set cardinality. $\mathcal{M}^*(\cdot)$ stands for the ground-truth mapping function that maps $\hat{\mathbf{p}}_i$ to its corresponding position in $\hat{\mathbf{Q}}$. In rigid scenarios, it is defined by a transformation $\mathbf{T}^*\in SE(3)$. For the non-rigid cases it can be denoted as a per-point flow $\mathbf{f}^*_i\in\mathbb{R}^{3}$ known as the deformation field.

\noindent\textbf{Method Overview.} An overview of \OURS{} is shown in Fig.~\ref{fig:pipeline}. \OURS{} consists of an encoder-decoder architecture named Point Pair Feature Transformer~(PPFTrans) for local geometry encoding and a stack of $g\times$ global transformers for global context aggregation. Correspondence set $\hat{\mathcal{C}}$ is extracted by the coarse-to-fine matching~\cite{yu2021cofinet}.

\subsection{PPF Attention Mechanism}

\noindent\textbf{Overview.} Fig.~\ref{fig:differences} compares three different self-attention mechanisms. The standard attention~\cite{vaswani2017attention} only leverages the input context to obtain the \textit{Query} $\mathbf{Q}$ and \textit{Key} $\mathbf{K}$ to compute the contextual attention $\mathbf{A}_C$, as well as the \textit{Value} $\mathbf{V}$ that encodes information for the contextual message $\mathbf{M}_C$. GeoTrans~\cite{qin2022geometric} proposes to learn the positional encoding $\mathbf{E}$ from the geometry and calculates a second attention $\mathbf{A}_G$ to reweigh $\mathbf{A}_C$. However, the cues contained in the raw geometry are totally neglected. To this end, we propose to learn the pose-agnostic geometric cues $\mathbf{G}$ and further generate the geometric message $\mathbf{M}_G$ in the PPF Attention Mechanism~(PAM). On the local level, $\mathbf{M}_G$ is combined with $\mathbf{M}_C$ for feature enhancement, while on the global level, it is used to learn the rotation-invariant position representation for the cross-frame context aggregation.
More specifically, we define PAM on an \textit{Anchor} triplet $\mathcal{P}^{A}=(\mathbf{P}^{A}, \mathbf{N}^{A}, \mathbf{X}^{A})$ and a \textit{Support} triplet $\mathcal{P}^{S}=(\mathbf{P}^{S}, \mathbf{N}^{S}, \mathbf{X}^{S})$, both with three dimensions referring to the point cloud, the estimated normals, and the associated features, respectively. PAM aggregates the learned context and geometric cues from $\mathcal{P}^{S}$ and flows the messages to $\mathcal{P}^{A}$.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{figures/rebuttal.pdf}
  \caption{Illustration of different self-attention computation in the standard attention~\cite{vaswani2017attention}, GeoTrans~\cite{qin2022geometric}, and PAM.}
  \label{fig:differences}
  \vspace{-0.5cm}
\end{figure}





\noindent\textbf{Pose-Agnostic Coordinate Representation.} The basis of PAM is the pose-agnostic local coordinate representation that we construct based on PPFs~\cite{drost2010model}. Let $\mathcal{P}^{A}_i := (\mathbf{p}^{A}_i\in \mathbf{P}^{A}, \mathbf{n}^{A}_i\in \mathbf{N}^{A}, \mathbf{x}^{A}_i\in \mathbf{X}^{A})\in \mathcal{P}^{A}$ denote the triplet constructed by picking the $i^{th}$ item on each dimension. For each $\mathbf{p}^{A}_i$, a subset of $\mathcal{P}^{S}$ is first retrieved according to the Euclidean distance w.r.t. $\mathbf{P}^S$, denoted as $\mathcal{P}^{S}_{\mathcal{N}(i)} := (\mathbf{P}^{S}_{\mathcal{N}(i)}, \mathbf{N}^{S}_{\mathcal{N}(i)},\mathbf{X}^{S}_{\mathcal{N}(i)})\subseteq \mathcal{P}^{S}$, with $\mathcal{N}(i)$ the indices of $k$-nearest neighbors. We then adopt PPFs~\cite{drost2010model} to construct a local coordinate system around each $\mathbf{p}^{A}_i$ to represent the pose-agnostic position of $\mathbf{P}^{S}_{\mathcal{N}(i)}$ w.r.t. it. The coordinate of point $\mathbf{p}_j^{S} \in \mathbf{P}^{S}_{\mathcal{N}(i)}$ is transferred to:
\begin{equation}
 \mathbf{e}^{S}_j = (\lVert\mathbf{d}\rVert_2, \angle(\mathbf{n}^{A}_i, \mathbf{d}), \angle(\mathbf{n}_j^{S}, \mathbf{d}), \angle(\mathbf{n}_j^{S}, \mathbf{n}^{A}_i)),
\label{eq:ppfs}
\end{equation}
with $\mathbf{d} = \mathbf{p}_j^{S} - \mathbf{p}_i^{A}$, and $\mathbf{n}_i^{A}$ and $\mathbf{n}_j^{S}$ the estimated normals of $\mathbf{p}_i^{A}$ and $\mathbf{p}_j^{S}$, respectively. $\angle(\mathbf{v}_1,\mathbf{v}_2)$ computes the angles between the two vectors~\cite{deng2018ppf,birdal2015point}. The transferred coordinates of $\mathbf{P}^{S}_{\mathcal{N}(i)}$ are denoted as $\mathbf{E}^{S}_{\mathcal{N}(i)}$.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{figures/local_attention.pdf}
  \caption{\textbf{Left}: The workflow of the PPF Attention Mechanism~(PAM). \textbf{Right}: Detailed calculation of the attention.}
  \label{fig:local_attention}
  \vspace{-0.5cm}
\end{figure}



\noindent\textbf{PPF Attention Mechanism.} 
PPF Attention Mechanism~(PAM) takes as input the \textit{Support} triplet $\mathcal{P}^{S}$ and the \textit{Anchor} point cloud $\mathbf{P}^{A}$ with estimated normals $\mathbf{N}^{A}$. PAM generates the \textit{Anchor} features $\mathbf{X}^{A}$ by aggregating the pose-agnostic local geometry and highly-representative learned context from $\mathcal{P}^{S}$, which is defined as:

\begin{equation}
\mathcal{P}^{A} = \delta(\mathbf{P}^{A}, \mathbf{N}^{A} \big| \mathcal{P}^{S}),
\label{eq:pam}
\end{equation}

\noindent with $\delta(\cdot)$ representing PAM. As shown in Fig.~\ref{fig:local_attention}~(a), for each $\mathbf{p}^A_i\in \mathbf{P}^A$ with normal $\mathbf{n}^A_i$, we find its nearest point $\mathbf{p}^S_j\in \mathbf{P}^S$ whose associated feature $\mathbf{x}^S_j$ is assigned to $\mathbf{p}^A_i$ as the initial description. Then, $k$-nearest neighbors from $\mathbf{P}^{S}$ are retrieved according to the Euclidean distance in 3D space, yielding $\mathbf{P}^{S}_{\mathcal{N}(i)}\subseteq\mathbf{P}^{S}$ and $\mathbf{X}^{S}_{\mathcal{N}(i)}\subseteq\mathbf{X}^{S}$. Following Eq.~\ref{eq:ppfs}, $\mathbf{P}^{S}_{\mathcal{N}(i)}$ is transferred to the pose-agnostic position representation $\mathbf{E}^{S}_{\mathcal{N}(i)}$, which is consecutively projected to the coordinate embedding $\mathbf{E}_S$ via a linear layer. ${\mathbf{x}}^{S}_j$ and $\mathbf{X}^{S}_{\mathcal{N}(i)}$ are projected to the contextual features $\mathbf{x}_S$ and $\mathbf{X}_S$ by a second shared linear layer, respectively. In Fig.~\ref{fig:local_attention}~(b), the attention mechanism uses five learnable matrices $\mathbf{W}_G$, $\mathbf{W}_E$, $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$ to project the input. Specifically, $\mathbf{W}_G$ and $\mathbf{W}_E$ project the input coordinate representation to the geometric cues and positional encoding by:

\begin{equation}
    \mathbf{G} = \mathbf{E}_S\mathbf{W}_G \quad \text{and} \quad \mathbf{E} = \mathbf{E}_S\mathbf{W}_E,
\label{eq:project1}
\end{equation}

\noindent respectively. Similarly, $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$ project the learned context to \textit{Query}, \textit{Key}, and \textit{Value} as:

\vspace{-0.5cm}
\begin{equation}
    \mathbf{q} = \mathbf{x}_S\mathbf{W}_Q, \; \mathbf{K} = \mathbf{X}_S\mathbf{W}_K, \; \text{and} \; \mathbf{V} = \mathbf{X}_S\mathbf{W}_V,
\label{eq:project2}
\vspace{-0.2cm}
\end{equation}
respectively. The attention $\mathbf{a}$ that measures the feature similarity, and the message $\mathbf{m}$ that encodes both the pose-agnostic geometry and the representative context read as:
\begin{equation}
    \mathbf{a} = \text{Softmax}(\frac{\mathbf{q}\mathbf{E}^{T} + \mathbf{q}\mathbf{K}^{T}}{\sqrt{c_0}}) \quad \text{and} \quad \mathbf{m} = \mathbf{a}\mathbf{G} + \mathbf{a}\mathbf{V},
\label{eq:local_attention}
\end{equation}
respectively. The message $\mathbf{m}$ is projected and aggregated to $\mathbf{x}^S_j$ via an element-wise addition followed by a normalization through LayerNorm~\cite{ba2016layer}. The final linear layer projects the obtained feature to $\mathbf{x}^{A}_i$, from which $\mathbf{X}^{A}$ is obtained to formulate the output $\mathcal{P}^A$ with the known $\mathbf{P}^A$ and $\mathbf{N}^A$.
%\subsubsection{Comparisons to Existing Attention Mechanisms}

\subsection{PPFTrans for Local Geometry Description}
\label{sec:local_geometry}
\noindent\textbf{Overview.} As illustrated in Fig.~\ref{fig:pipeline}, PPFTrans consumes triplets $\mathcal{P}$ and $\mathcal{Q}$. Taking $\mathcal{P} = (\mathbf{P}, \mathbf{N}, \mathbf{X})$ as an example, it consists of $\mathbf{P}\in \mathbb{R}^{n\times3}$ the points cloud, $\mathbf{N}\in \mathbb{R}^{n \times 3}$ the normals estimated from $\mathbf{P}$, and $\mathbf{X} = \vec{\mathbf{1}}\in\mathbb{R}^{n\times 1}$ the initial point features. The encoder produces the superpoint triplet  $\mathcal{P}^{\prime}=(\mathbf{P}^\prime, \mathbf{N}^\prime, \mathbf{X}^\prime)$ with $\mathbf{P}^\prime\in \mathbb{R}^{n^\prime\times 3}$ and $\mathbf{X}^\prime\in\mathbb{R}^{n^\prime\times c^\prime}$.  With the consecutive decoder, $\mathcal{P}^\prime$ is decoded to a triplet $\hat{\mathcal{P}} = (\hat{\mathbf{P}}, \hat{\mathbf{N}}, \hat{\mathbf{X}})$ including $\hat{n}$ points with features $\hat{\mathbf{X}}\in \mathbb{R}^{\hat{n}\times\hat{c}}$. Notably, as we adopt a Farthest Point Sampling~(FPS) strategy~\cite{qi2017pointnet++}, it always satisfies that $\mathbf{P}^\prime \subseteq \hat{\mathbf{P}} \subseteq \mathbf{P}$.  The same goes for a second point cloud $\mathbf{Q}$ with an input triplet $\mathcal{Q}=(\mathbf{Q}\in \mathbb{R}^{m \times 3}, \mathbf{M}\in \mathbb{R}^{m \times 3}, \mathbf{Y} = \vec{\mathbf{1}} \in \mathbb{R}^{m \times 1})$ by the shared architecture. In the rest of this paper, we only demonstrate for $\mathcal{P}$ unless the model processes $\mathcal{Q}$ differently.

\noindent\textbf{Encoder.} 
The encoder is constructed by stacking several encoder blocks, each including an Attentional Abstraction Layer~(AAL) followed by $e\times$PPF Attention Layers~(PALs). Each block consumes the output of the previous block as the \textit{Support} triplet $\mathcal{P}^{S}$~($\mathcal{P}^{S} = \mathcal{P}$ for the first block). $\mathcal{P}^{S}$ first flows to AAL, where \textit{Anchor} points $\mathbf{P}^{A}$ with associated normals $\mathbf{N}^{A}$ are obtained via FPS~\cite{qi2017pointnet++}. The \textit{Anchor} triplet $\mathcal{P}^{A}$ is then generated in AAL via a PAM following Eq.~\ref{eq:pam}. A sequence of PALs is applied for enhancing the \textit{Anchor} features $\mathbf{X}^{A}$, each updating the features as:
\begin{equation}
    \mathcal{P}^{A} \leftarrow \theta(\mathcal{P}^{A})=\text{ReLU}(\mathbf{X}^{A} + \phi(\delta(\mathbf{P}^{A},\mathbf{N}^{A} \big| \mathcal{P}^{A}))),
\label{eq:PAL}
\end{equation}

\noindent with $\phi$ the LayerNorm~\cite{ba2016layer}, $\delta$ the PAM, and $\theta$ the PAL. $\leftarrow$ depicts feature updating. The encoder block outputs the updated $\mathcal{P}^A$, and the output of the whole encoder is defined as $\mathcal{P}^\prime$, which is the output of the final encoder block.

\noindent\textbf{Decoder.} 
We build the decoder by stacking a series of decoder blocks, each consisting of a Transition Up Layer~(TUL) followed by $d\times$ PAL. Each block takes the output of the previous block as the \textit{Anchor} triplet $\mathcal{P}^{A}$~($\mathcal{P}^{A} = \mathcal{P}^\prime$ for the first block), and takes the \textit{Support} triplet $\mathcal{P}^{S}$ from the encoder via skip connections. The input flows to TUL, where each feature $\widetilde{\mathbf{x}}^{S}_j \in \widetilde{\mathbf{X}}^{S}$ assigned to $\mathbf{p}^S_j\in \mathbf{P}^S$ is interpolated by:
\vspace{-0.3cm}
\begin{equation}
\widetilde{\mathbf{x}}^{S}_j = \frac{\sum_{i\in \mathcal{N}(j)} w_i^j \mathbf{x}^{A}_i}{\sum_{i\in \mathcal{N}(j)} w_i^j} , \ \text{with} \ w_i^j = \frac{1}{\lVert \mathbf{p}^{S}_j - \mathbf{p}^{A}_i\rVert_2},
\label{eq:interpolation}
\end{equation}
\noindent with $\mathcal{N}(j)$ the $k$-nearest neighbors of $\mathbf{p}^S_j$ in $\mathbf{P}^A$. Features are updated by two linear layers as $\mathcal{P}^{S}\leftarrow \zeta_1(\mathbf{X}^{S}) + \zeta_2(\widetilde{\mathbf{X}}^{S})$. A sequence of PALs is adopted after TUL, each enhancing the features as $\mathcal{P}^{S} \leftarrow \theta(\mathcal{P}^{S})$ according to Eq.~\ref{eq:PAL}. The decoder block outputs the updated $\mathcal{P}^S$, and the output of the whole decoder is denoted as $\hat{\mathcal{P}}$, which is the output of the final decoder block.

%\subsubsection{Comparisons to Existing Local Attention.}
%\label{sec: comparison}
%Our designed PPFAttention layer differs to the attention mechanism used in~\cite{zhao2021point} mainly in two aspects: 1). We leverage the standard multi-head scalar attention proposed in~\cite{vaswani2017attention} to cut off the heavy computation and memory burden of the vector attention used in~\cite{zhao2021point}. A comparison between these two different mechanisms in terms of efficiency can be found in Fig.~\ref{fig:comparison}. 2). As we observed in our experiments, the relative coordinate used in~\cite{zhao2021point} with the vector attention mechanism is hard to handle the variance of poses, as the coordinate itself is sensitive to rotations. What is even worse is that a combination of the relative coordinates and the standard multi-head attention mechanism cannot converge in the point cloud matching task. Based on this observation, we adopt the PPFs, which guarantees the rotation invariance by design, as a local coordinate system to provide the positional encoding in our local attention. Such a design avoids the redundant learning of pose variance through the network and facilitates the pure geometry encoding, which helps \OURS{} surpass the state-of-the-arts while keeping its efficiency.


\begin{figure}
  \includegraphics[width=0.48\textwidth]{figures/global_self.pdf}
  \vspace{-0.8cm}
  \caption{The computation graph of our global transformer consisting of the Geometry-Aware Self-Attention Module~(GSM) and Position-Aware Cross-Attention Module~(PCM).}
  \label{fig:global_self}
  \vspace{-0.5cm}
\end{figure}

\subsection{Global Transformer for Context Aggregation}
\label{sec:global_context}
\noindent\textbf{Overview.} Our designed global transformer takes as input a pair of triplets $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$, and enhances the features with the global context, yielding $\widetilde{\mathbf{X}}^{\prime}\in \mathbb{R}^{n^\prime \times {c}^\prime}$ and $\widetilde{\mathbf{Y}}^{\prime}\in \mathbb{R}^{m^\prime \times {c}^\prime}$, respectively. We stack $g\times$global transformers, with each including a Geometry-Aware Self-Attention Module~(GSM) and a Position-Aware Cross-Attention Module~(PCM)~(See Fig.~\ref{fig:pipeline} and Fig.~\ref{fig:global_self}).
Different from previous works~\cite{huang2021predator,yu2021cofinet,qin2022geometric,yew2022regtr,yu2022riga} that totally neglect the cross-frame spatial relationships, we propose to learn a rotation-invariant position representation for each superpoint to enable the position-aware cross-frame context aggregation.
%\begin{figure}
%  \includegraphics[width=0.48\textwidth]{figures/global_cross.pdf}
%  \caption{\textbf{Illustration of the Position-Aware Cross-Attention.}}
%  \label{fig:global_cross}
%\end{figure}

\noindent\textbf{Geometry-Aware Self-Attention Module.} 
On the global level, we modify PAM to learn the rotation-invariant position representation and to aggregate the learned context across the whole frame simultaneously. The design of GSM is detailed in Fig.~\ref{fig:global_self}~(a). GSM has two branches, where the geometry branch mines the geometric cues from the pairwise rotation-invariant geometry representation proposed in~\cite{qin2022geometric}, and the context branch aggregates the global context across the frame. We refer the readers to the Appendix for the detailed construction of $\mathbf{R}^\prime\in \mathbb{R}^{n^\prime \times n^\prime \times c^\prime}$ and the ablation study on it. Similar to Eq.~\ref{eq:project1}, the geometric cues $\mathbf{G}^\prime$ and the positional encoding $\mathbf{E}^\prime$ are linearly projected from $\mathbf{R}^\prime$. $\mathbf{E}^\prime$ is further processed in the geometry branch and finally leveraged as the rotation-invariant position representation. In the context branch, $\mathbf{Q}^\prime$, $\mathbf{K}^\prime$, and $\mathbf{V}^\prime$ are obtained by linearly mapping the input features $\mathbf{X}^\prime$ similar to Eq.~\ref{eq:project2}. The hybrid score matrix $\mathbf{S}^\prime\in \mathbb{R}^{n^\prime \times n^\prime}$ is computed as:

\begin{equation}
    \mathbf{S}^\prime(i, j) = \frac{(\mathbf{q}^\prime_i)(\mathbf{e}^\prime_{i, j} + \mathbf{k}^\prime_j)^T}{\sqrt{c^\prime}},
\label{eq:hybrid_score}
\end{equation}
with $\mathbf{e}^\prime_{i, j} := \mathbf{E}^\prime(i, j, :)$, $\mathbf{q}^\prime_{i} := \mathbf{Q}^\prime(i, :)$, and $\mathbf{k}^\prime_{j} := \mathbf{K}^\prime(j, :)$ the $c^\prime$-dimension vectors.
The hybrid attention $\mathbf{A}^\prime$ is obtained via a Softmax function over each row of $\mathbf{S}^\prime$, and the geometric messages $\mathbf{M}^\prime_G \in \mathbb{R}^{n^\prime \times c^\prime}$ are computed as:

\begin{equation}
\mathbf{M}_G^\prime(i, :) = \sum_{1\leq j \leq n^\prime} a^\prime_{i, j}\mathbf{g}^\prime_{i, j},
\label{eq:geo_mes}
\end{equation}

\noindent with $a^\prime_{i, j} := \mathbf{A}^\prime(i, j)$ and $\mathbf{g}^\prime_{i, j} := \mathbf{G}^\prime(i, j, :)$. The contextual messages $\mathbf{M}_V^\prime \in \mathbb{R}^{n^\prime\times c^\prime}$ are computed by $\mathbf{A}^\prime\mathbf{V}^\prime$. After a feed-forward network~\cite{vaswani2017attention}, the position representation $\mathbf{E}^\prime_P$ and globally-enhanced context $\mathbf{C}^\prime_P$ are generated.

\noindent\textbf{Position-Aware Cross-Attention Module.} PCM consumes a pair of doublets $(\mathbf{E}^\prime_P, \mathbf{C}^\prime_P)$ and $(\mathbf{E}^\prime_Q, \mathbf{C}^\prime_Q)$ that are generated from $\mathcal{P}^\prime$ and $\mathcal{Q}^\prime$ by a shared GSM, respectively. As the cross-attention is directional, we apply the same PCM twice, with the first aggregation from $\mathcal{Q}^\prime$ to $\mathcal{P}^\prime$~(See Fig.~\ref{fig:global_self} (b)), and the second reversed. As the first step, the rotation-invariant position representation is incorporated to make the consecutive cross-attention position-aware, yielding position-aware features $\mathbf{F}^\prime_P=\mathbf{E}^\prime_P + \mathbf{C}^\prime_P$ and $\mathbf{F}^\prime_Q=\mathbf{E}^\prime_Q + \mathbf{C}^\prime_Q$. Similar to Eq.~\ref{eq:project2}, $\widetilde{\mathbf{Q}}^\prime$, $\widetilde{\mathbf{K}}^\prime$, and $\widetilde{\mathbf{V}}^\prime$ are computed as the linear projection of $\mathbf{F}^\prime_P$, $\mathbf{F}^\prime_Q$, and $\mathbf{F}^\prime_Q$, respectively. The attention matrix $\widetilde{\mathbf{A}}\in \mathbb{R}^{n^\prime \times m^\prime}$ is computed via a row-wise softmax function applied on $\widetilde{\mathbf{Q}}^\prime\widetilde{\mathbf{K}}^{\prime T}$. The fused messages are presented as $\widetilde{\mathbf{A}}\widetilde{\mathbf{V}}^\prime$, which are finally mapped to the output features $\widetilde{\mathbf{X}}^\prime$ through a feed-forward network. As we introduce spatial awareness at the beginning of PCM, both the attention computation and message fusion are aware of the cross-frame positions. After the twice application of PCM, the input features are enhanced as $\mathcal{P}^\prime\leftarrow\widetilde{\mathbf{X}}^\prime$ and $\mathcal{Q}^\prime\leftarrow\widetilde{\mathbf{Y}}^\prime$, respectively. The global aggregation stage finally generates a pair of triplets $\widetilde{\mathcal{P}}^\prime:=(\mathbf{P}^\prime, \mathbf{N}^\prime, \widetilde{\mathbf{X}}^\prime)$ and $\widetilde{\mathcal{Q}}^\prime:=(\mathbf{Q}^\prime, \mathbf{M}^\prime, \widetilde{\mathbf{Y}}^\prime)$, with the enhanced features from the last global transformer.


\subsection{Point Matching and Loss Funcion}

\label{sec:matching}
\noindent\textbf{Superpoint Matching.} As shown in Fig.~\ref{fig:pipeline}, the point matching stage consumes a pair of superpoint triplets $\widetilde{\mathcal{P}}^\prime$ and $\widetilde{\mathcal{Q}}^\prime$ obtained from the global transformer, as well as a pair of point triplets $\hat{\mathcal{P}}$ and $\hat{\mathcal{Q}}$ produced by the decoder. 
We adopt the coarse-to-fine matching proposed in~\cite{yu2021cofinet}. Following~\cite{qin2022geometric}, we first normalize the superpoint features $\widetilde{\mathbf{X}}^\prime$ and $\widetilde{\mathbf{Y}}^{\prime}$ onto a unit hypersphere, and measure the pairwise similarity using a Gaussian correlation matrix $\widetilde{\mathbf{S}}$ with $\widetilde{\mathbf{S}}(i, j)=-\text{exp}(-\lVert\widetilde{\mathbf{x}}_i^\prime - \widetilde{\mathbf{y}}_j^\prime\rVert_2^2)$. After a dual-normalization~\cite{rocco2018neighbourhood,sun2021loftr,qin2022geometric} on $\widetilde{\mathbf{S}}$ for global feature correlation, superpoints associated to the top-$k$ entries are selected as the coarse correspondence set $\mathcal{C}^\prime = \{(\mathbf{p}_i^\prime, \mathbf{q}_j^\prime)\big| \mathbf{p}_i^\prime \in \mathbf{P}^\prime, \mathbf{q}_j^\prime \in \mathbf{Q}^\prime\}$.

\noindent\textbf{Point Matching.} For extracting point correspondences, denser points $\hat{\mathbf{P}}$ and $\hat{\mathbf{Q}}$ are first assigned to superpoints. To this end, the point-to-node strategy~\cite{yu2021cofinet} is leveraged, where each point is assigned to its closest superpoint in 3D space. Given a superpoint $\mathbf{p}_i^\prime \in \mathbf{P}^\prime$, the group of points assigned to it is denoted as $\hat{\mathbf{G}}^P_i \subseteq \hat{\mathbf{P}}$. The group of features associated to $\hat{\mathbf{G}}^P_i$ is further defined as $\hat{\mathbf{G}}^X_i$ with $\hat{\mathbf{G}}^X_i \subseteq \hat{X}$. For each superpoint correspondence $\mathcal{C}^\prime_l = (\mathbf{p}^\prime_i, \mathbf{q}^\prime_j)$, the similarity between the corresponding feature groups $\hat{\mathbf{G}}^X_i$ and $\hat{\mathbf{G}}^Y_j$ is calculated as
$\hat{\mathbf{S}}_l = \hat{\mathbf{G}}^X_i (\hat{\mathbf{G}}_j^{Y})^T /\sqrt{\hat{c}}$, with $\hat{c}$ the feature dimension. We then follow~\cite{sarlin2020superglue} to append a stack row and column to $\hat{\mathbf{S}}_l$ filled with a learnable parameter $\alpha$, and iteratively run the Sinkhorn Algorithm~\cite{sinkhorn1967concerning}. After removing the slack row and column of $\hat{\mathbf{S}}_l$, the mutual top-k entries, \ie., entries with top-$k$ confidence on both the row and the column, are selected to formulate a point correspondence set $\hat{\mathcal{C}}_l$. The final correspondence set $\hat{\mathcal{C}}$ is collected by $\hat{\mathcal{C}} = \cup^{|\mathcal{C}^\prime|}_{l=1} \hat{\mathcal{C}}_l$. 

\noindent\textbf{Loss Function.} Our loss function reads as $\mathcal{L} = \mathcal{L}_s + \lambda\mathcal{L}_p$, with a superpoint matching loss $\mathcal{L}_s$ and a point matching loss $\mathcal{L}_p$ balanced by a hyper-parameter $\lambda$~($\lambda=1$ by default). The detailed definition is introduced in the Appendix.

\iffalse
\subsection{Loss Functions}
\label{sec:loss}


\noindent\textbf{Superpoint Matching Loss.} We use the Circle Loss~\cite{sun2020circle} for superpoint matching. Following~\cite{qin2022geometric}, we use the overlapping ratio between the vicinity of superpoints to weigh different ground truth superpoint correspondences. More specifically, for each superpoint $\mathbf{p}^\prime_i\in \mathbf{P}^\prime$, we sample a positive set of superpoints from $\mathbf{Q}^\prime$, denoting as $\mathcal{E}^P_i = \{\mathbf{q}^\prime_j \in \mathbf{Q}^\prime\| \mathcal{R}(\mathbf{p}^\prime_i, \mathbf{q}^\prime_j) > \tau_r\}$, where $\mathcal{R}$ is the function that calculates the overlapping ratio between the vicinity of two superpoints, and $\tau_r$ is the threshold to select the positive samples~($\tau_r$=0.1 by default). The detailed definition of $\mathcal{R}$ is given in the Appendix. We further sample a negative set of superpoints $\mathcal{F}^{P}_i$ from $\mathbf{q}^\prime_j\notin \mathcal{E}^P_i$. Then for $\mathbf{P}^\prime$, the superpoint matching loss is computed as:

\begin{equation}
\mathcal{L}^P_c = \frac{1}{n^\prime}\sum_{i=1}^{n^\prime}log[1 + \sum_{\mathbf{q}^\prime_j \in \mathcal{E}^P_i}{e^{r^j_i\beta^j_e(d^j_i-\Delta_e)}}\cdot \sum_{\mathbf{q}^\prime_k\in \mathcal{F}^P_i}{e^{\beta^k_f(\Delta_f-d^k_i)}}],
\label{eq:coarse}
\end{equation}

\noindent where $r^j_i := \mathcal{R}(\mathbf{p}^\prime_i, \mathbf{q}^\prime_j)$ and $d^j_i = \lVert\mathbf{x}^\prime_i - \mathbf{y}^\prime_j \rVert_2$, with $\mathbf{x}^\prime_i \leftrightarrow \mathbf{p}^\prime_i$ and $\mathbf{y}^\prime_j \leftrightarrow \mathbf{q}^\prime_j$. Moreover, $\Delta_e$ and $\Delta_f$ are the positive and negative margins~($\Delta_e$=0.1 and $\Delta_f$=1.4 by default). $\beta^j_e = \gamma(d^J_i - \Delta_e)$ and $\beta^k_f=\gamma(\Delta_f - d^k_i)$ are the weights individually determined for different samples, with a hyper-parameter $\gamma$. The same loss for $\mathbf{Q}^\prime$ is defined in a similar way, and we have the overall superpoint matching loss as $\mathcal{L}_s = (\mathcal{L}^P_s + \mathcal{L}^Q_s)/2$.

\noindent\textbf{Point Matching Loss.} For each superpoint correspondence $\mathcal{C}^\prime_l = (\mathbf{p}^\prime_i, \mathbf{q}^\prime_j) \in \mathcal{C}^\prime$, we adopt a negative log-likelihood loss~\cite{sarlin2020superglue} on its corresponding normalized similarity matrix $\overline{\mathbf{C}}_l\in \mathbb{R}^{(\overline{n} + 1)\times (\overline{m} + 1)}$, with $\overline{n}_l = |\hat{\mathbf{G}}^{X}_i|$ and $\overline{m}_l = |\hat{\mathbf{G}}^{Y}_j|$. We further define $\mathcal{M}_l = \{(u, v)| \hat{\mathbf{p}}_u \in \hat{\mathbf{G}}^P_i \Leftrightarrow \hat{\mathbf{q}}_v \in \hat{\mathbf{G}}^Q_j\}$, with $\Leftrightarrow$ denoting corresponding points. We further define two sets $\mathcal{I}_l = \{u|\hat{\mathbf{p}}_u \nLeftrightarrow \hat{\mathbf{q}}_v, \forall\hat{\mathbf{q}}_v \in \hat{\mathbf{G}}^Q_j\}$ and $\mathcal{J}_l = \{v|\hat{\mathbf{q}}_v \nLeftrightarrow \hat{\mathbf{p}}_u, \forall\hat{\mathbf{p}}_u \in \hat{\mathbf{G}}^P_i\}$, with $\nLeftrightarrow$ depicting non-corresponding relationship. Then the point matching loss on $\mathcal{C}^\prime_l$ can be defined as:


\begin{equation}
\begin{aligned}
\mathcal{L}_f^l = -\sum_{(u, v)\in \mathcal{M}_l} \log \overline{c}^l_{u, v} - \sum_{u\in \mathcal{I}_l} \log \overline{c}^l_{u, \overline{m}_l + 1} \\ - \sum_{v \in \mathcal{J}_l} \log \overline{c}^l_{\overline{n}_l + 1, v} ,
\end{aligned}
\label{eq:fine}
\end{equation}
where $\overline{c}^{l}_{u, v}:= \overline{\mathbf{C}}_l(u, v)$ stands for the entry on the $u^{th}$ row  and $v^{th}$ column of $\overline{\mathbf{C}}_l$. The overall point matching loss reads as $\mathcal{L}_p = \frac{1}{|\mathcal{C}^\prime|}\sum_{l=1}^{|\mathcal{C}^\prime|} \mathcal{L}^l_p$.
\fi