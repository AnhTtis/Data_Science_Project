{
    "arxiv_id": "2303.08360",
    "paper_title": "Knowledge Distillation from Single to Multi Labels: an Empirical Study",
    "authors": [
        "Youcai Zhang",
        "Yuzhuo Qin",
        "Hengwei Liu",
        "Yanhao Zhang",
        "Yaqian Li",
        "Xiaodong Gu"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Knowledge distillation (KD) has been extensively studied in single-label image classification. However, its efficacy for multi-label classification remains relatively unexplored. In this study, we firstly investigate the effectiveness of classical KD techniques, including logit-based and feature-based methods, for multi-label classification. Our findings indicate that the logit-based method is not well-suited for multi-label classification, as the teacher fails to provide inter-category similarity information or regularization effect on student model's training. Moreover, we observe that feature-based methods struggle to convey compact information of multiple labels simultaneously. Given these limitations, we propose that a suitable dark knowledge should incorporate class-wise information and be highly correlated with the final classification results. To address these issues, we introduce a novel distillation method based on Class Activation Maps (CAMs), which is both effective and straightforward to implement. Across a wide range of settings, CAMs-based distillation consistently outperforms other methods.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08360v1"
    ],
    "publication_venue": null
}