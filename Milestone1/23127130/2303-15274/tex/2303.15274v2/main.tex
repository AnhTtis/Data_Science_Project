% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{bm}
\usepackage[accsupp]{axessibility}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage{xcolor}
% \newcommand{\yg}[1]{\textcolor{blue}{#1}}

\newcommand{\yg}[1]{\color{blue}{#1}\color{black}}



% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}
\input{definitions}
%%%%%%%%% TITLE - PLEASE UPDATE
\title{Gazeformer: Scalable, Effective and Fast Prediction of \\Goal-Directed Human Attention}

\author{Sounak Mondal$^{1}$, Zhibo Yang$^{1,2}$, Seoyoung Ahn$^{1}$, Dimitris Samaras$^{1}$, Gregory Zelinsky$^{1}$, Minh Hoai$^{1,3}$\\
$^{1}$Stony Brook University, \quad $^{2}$Waymo LLC, \quad  $^{3}$VinAI Research
%Stony Brook, NY\\
%{\tt\small \{somondal@cs.stonybrook.edu}
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Zhibo Yang\\
% Stony Brook University\\
% Stony Brook, NY\\
% %{\tt\small secondauthor@i2.org}
% \and
% Seoyoung Ahn\\
% Stony Brook University\\
% Stony Brook, NY\\
% %{\tt\small secondauthor@i2.org}
% \and
% Dimitris Samaras\\
% Stony Brook University\\
% Stony Brook, NY\\
% %{\tt\small secondauthor@i2.org}
% \and
% Gregory Zelinsky\\
% Stony Brook University\\
% Stony Brook, NY\\
% %{\tt\small secondauthor@i2.org}
% \and
% Minh Hoai\\
% Stony Brook University\\
% Stony Brook, NY\\
% %{\tt\small secondauthor@i2.org}
% }
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Predicting human gaze is important in Human-Computer Interaction (HCI). %Recent models in human gaze prediction focus on goal-directed attention. %Existing fixation-prediction methods rely on representations of the target learned by deep object detectors, and therefore cannot easily generalize to new target categories. 
However, to practically serve HCI applications, gaze prediction models must be scalable, fast, and accurate in their spatial and temporal gaze predictions. Recent scanpath prediction models focus on goal-directed attention (search). Such models are limited in their application due to a common approach relying on trained target detectors for all possible objects, and the availability of human gaze data for their training (both not scalable). In response, we pose a new task called \textit{ZeroGaze}, a new variant of zero-shot learning where gaze is predicted for never-before-searched objects, and we develop a novel model, \textit{Gazeformer}, to solve the ZeroGaze problem. %gaze prediction during search as a new variant of the zero-shot learning problem and call this new task \textit{ZeroGaze}. We also develop a novel model called \textit{Gazeformer} to solve it. %In the ZeroGaze paradigm, we leverage the contextual similarity between categories for which there exists gaze data to predict gaze trajectories for categories without gaze data. 
 In contrast to existing methods using object detector modules, Gazeformer encodes the target using a natural language model, thus leveraging semantic similarities in scanpath prediction. We use a transformer-based encoder-decoder architecture because transformers are particularly useful for generating contextual representations. Gazeformer surpasses other models by a large margin~(19\%--70\%) on the ZeroGaze setting. It also outperforms existing target-detection models on standard gaze prediction for both target-present and target-absent search tasks. In addition to its improved performance, Gazeformer is more than five times faster than the state-of-the-art target-present visual search model. Code can be found at \href{https://github.com/cvlab-stonybrook/Gazeformer/}{\nolinkurl{https://github.com/cvlab-stonybrook/Gazeformer/}}%Code can be found at \href{https://github.com/cvlab-stonybrook/Gazeformer/}{\nolinkurl{https://github.com/cvlab-stonybrook/Gazeformer/}}
\end{abstract}

%%%%%%%%% BODY TEXT

\begin{figure}[ht!]
\centering
% \includegraphics[width=\linewidth]{./latex/assets/Setting_intro_new.pdf}\\
% (a)\\
% \includegraphics[width=\linewidth]{./latex/assets/Teaser_stats.pdf}
% (b)
\includegraphics[width=\linewidth]{./assets/teaser_camera.pdf}
%\vskip -0.1in
%\caption{(a) {\bf GazeTrain versus ZeroGaze settings.} An example scenario where human scanpaths for only ``fork'', ``cup'' and ``laptop'' are available during training. In the traditional \textit{GazeTrain} setting, the model is asked to predict scanpaths for only those target categories it has seen during training, such as ``fork''. However, in the \textit{ZeroGaze} setting, the model is expected to predict scanpaths for target categories for which training scanpaths are \textit{not available}, such as ``knife''. (b) {\bf Gazeformer is holistically superior.} Gazeformer is more scalable, fast and effective than previous methods.}
\caption{An example scenario where human scanpaths for only $N$ categories such as  ``fork'', ``cup'' and ``laptop'' are available during training.  In the traditional \textit{GazeTrain} setting, the model is asked to predict scanpaths for only those $N$ target categories it has seen during training, such as ``fork''. However, in the \textit{ZeroGaze} setting, the model is expected to predict scanpaths for target categories for which training scanpaths are \textit{not available}, such as ``knife''. Gazeformer is overall superior since it is more scalable (better ZeroGaze performance), more effective (better GazeTrain performance) and faster (higher inference speed) than previous methods.}
%\vskip -0.1in
\label{fig:setting}
\end{figure}
\section{Introduction}
\label{sec:intro}

The prediction of human gaze behavior is a computer vision problem with clear application to the design of more interactive systems that can anticipate a user's attention. In addition to addressing basic %Besides addressing
cognitive science questions, gaze modeling has applications in human-computer interaction (HCI) and Augmented/Virtual Reality (AR/VR) systems~\cite{kapp2021arett, chung2022static, bennett2021assessing}, non-invasive healthcare~\cite{vidal2012wearable, novak2013enhancing}, visual display design~\cite{halverson2007minimal, takahashi2022gaze}, robotics~\cite{kim2021gaze, fujii2018gaze}, education~\cite{chettaoui2023student, beuget2019eye, m_Robello-PERC18},  etc. %The area of HCI that has seen the most rapid recent interest in gaze modeling is the field of AR/VR systems~\cite{kapp2021arett, chung2022static, bennett2021assessing}, non-invasive healthcare~\cite{vidal2012wearable, novak2013enhancing}, visual display design~\cite{halverson2007minimal, takahashi2022gaze} and robotics ~\cite{kim2021gaze, fujii2018gaze}. 
It is likely that gaze modeling will be ubiquitous in all HCI interfaces in the near future. Hence, there is a need for gaze prediction models that are reliable and effective yet efficient and capable of fast inference for use with edge devices (e.g., smartglasses,  AR/VR headsets).

Our focus is on goal-directed behavior (not free viewing), and specifically the prediction of gaze fixations as a person searches for a given target-object category (e.g., a clock or fork). Visual search is engaged in innumerable task-oriented human activities (e.g., driving) in both real and AR/VR environments. The gaze prediction problem for visual search takes an image input and outputs a sequence of eye movements conditioned on the target. %This kind of \textit{visual search} is important for various task-oriented activities where humans connect with real or virtual environments through a machine interface such as driving and interactions in AR/VR environments. 
Existing architectures for search fixation prediction use either panoptic segmentation maps \cite{yang2020predicting} or object detection modules \cite{chen2021predicting, yang2022target} to encode specific search targets. However, the applicability of these methods is essentially limited to the relatively small number of target objects for which panoptic segmentation or object detector models can be realistically trained. For example, to predict the fixations in search for ``pizza cutter'', which is an object category not included in the dataset used to train the backbone detector in \cite{chen2021predicting}, a new ``pizza cutter'' detector would need to be trained, meaning this and related methods do not scale beyond their training data. Relatedly, existing approaches~\cite{yang2020predicting, chen2021predicting, yang2022target} have required the laborious collection of large-scale datasets of search behavior for each target category ($\sim3000$ scanpaths per category) included in the dataset, an approach that is unscalable to the innumerable potential targets that humans search for in the wild.
%More fundamentally, all existing models require large-scale eyetracking datasets that are collected over every target category that model needs to search for (e.g.,$\sim$3000 scanpaths per each category;~\cite{chen2020coco, m_Yang-etal-CVPR20}), which make the approaches ultimately unscalable to numerous target categories that will appear in the wild. 
 
We therefore introduce a new problem that we refer to as \textit{ZeroGaze}, which is an extension of zero-shot learning to the gaze prediction problem. Under ZeroGaze, a model must predict the fixations that a person makes while searching for a target (e.g., a fork) despite the unavailability of search fixations (e.g., a person looking for a fork) for model training. A more challenging version of the problem is when there are no trained detectors for the target (e.g., a fork detector) either.  %To our knowledge, this is the first formulation of the ZeroGaze problem. 
This is in contrast to the traditional ``GazeTrain'' setting (where the model has been trained on gaze behavior for all the categories that it encounters during inference). %The differences between GazeTrain and ZeroGaze setting are highlighted in \Fref{fig:setting}. 
To address the ZeroGaze problem, we propose \textit{Gazeformer}, a novel multimodal model for scanpath prediction. Gazeformer is scalable because it does not require training a backbone detector on the fixation behavior of people searching for specific object categories. 
%Gazeformer shows a desirable property for a scalable model for search fixation prediction because it is free of the limitation of needing a backbone detector or training datasets of fixation behavior from people searching for specific object categories. 
We use a linguistic embedding of the target object from a language model (RoBERTa~\cite{liu2019roberta}) and a transformer encoder-decoder architecture \cite{NIPS2017_3f5ee243} to model the joint image-target representations and the spatio-temporal context embedded within it. Given that recent language models can encode any object using any text string, they provide a powerful and scalable solution to the representation of target categories for which no gaze data is available for training. Additionally, Gazeformer decodes the fixation time-steps in \textit{parallel} using a transformer decoder, providing significantly faster inference than any other method, which is necessary for gaze tracking applications in HCI products (especially in wearable edge devices). We show the advantages of our model in Fig. \ref{fig:setting}. The specific contributions of this study are:

%However, the vast majority of computer vision models on this topic have attempted to predict fixation behavior in a free-viewing task, where the assumption is that a person's gaze behavior reflects no particular goal. Here our focus is on goal-directed behavior, and specifically the prediction of gaze fixations as a person searches for a given target-object category, such as a clock or a spoon. %Only recently there are computational models for predicting goal-directed behavior in complex scenes, where the goal is to search for a visual target that belongs to a specified object category such as a microwave or a clock. Increased interest in human attention has also created a huge need for attention annotation \cite{papadopoulos2014training, gilani2015pet, zelinsky2019benchmarking, chen2020coco}. 
%Far fewer computer vision models have addressed this basic human behavior, an effort hampered by a lack of human fixation data for model training. Whereas numerous datasets\cite{lin2014microsoft, krizhevsky2012imagenet} provide image encoders such as ResNet\cite{he2015deep} and ViT\cite{dosovitskiy2020image} with abundant data for training, even a recent and (relatively) large-scale dataset of visual search fixations collected to fill this gap \cite{chen2020coco} falls short. This is because the human eye movements needed for annotation are time-consuming and cumbersome to collect, making this approach ultimately impossible to scale. Moreover, the pandemic revealed weaknesses in reliance on behavioral annotations for model training, where almost all fixation data collection was paused due to public health concerns. Here we introduce a new problem that we refer to as \textit{ZeroGaze}, which we apply to the fixations made during search tasks. In \textit{ZeroGaze}, a model must predict the scanpath of fixations made to a target-object category without having been trained on the search behavior for that target. For example, a model would predict the fixations that a person makes while searching for a fork despite having never been trained on fork search fixations. To the best of our knowledge, this is the first formulation of the \textit{ZeroGaze} problem. 
% The prediction of human gaze behavior is a computer vision problem with clear application to the design of more interactive systems that can anticipate a user's attention. The area of human-computer interaction (HCI) that has seen the most rapid recent interest in gaze tracking is the field of 
% %he prediction of human gaze behavior is a topic that bridges basic research questions in the behavioral vision literature and applications in computer vision that might use predictions of user attention to build more interactive systems that anticipate this behavior. In recent years, gaze tracking has seen much traction in the area of human-computer interaction - specifically in 
% Augmented Reality/Virtual Reality systems~\cite{kapp2021arett, chung2022static, bennett2021assessing}, non-invasive healthcare~\cite{vidal2012wearable, novak2013enhancing}, visual display design~\cite{halverson2007minimal, takahashi2022gaze} and robotics ~\cite{kim2021gaze, fujii2018gaze}. It is estimated that gaze tracking will be ubiquitous in all HCI interfaces in the near future. Hence, there is a need for gaze prediction models that are reliable and effective yet efficient and capable of fast inference for use with edge devices (such as smartglasses and AR/VR headsets). Critically, HCI-aimed models must also be scalable and not reliant on training datasets of fixation behavior (or its mouse-based proxy, as in ~\cite{jiang2015salicon}) from people searching images of scenes for specific object categories \cite{yang2020predicting, chen2021predicting, yang2022target}. %, which is the approach taken by previous models of human fixation prediction . Previous studies have predicted human gaze fixations %during scene viewing or searching, previous studies collected eye-movement data \todo{citation} or their proxies (e.g, mouse cursor movement \todo{citation}) to train a model. 
% %by training models on the fixation behavior\cite{yang2020predicting, chen2021predicting, yang2022target}, or its proxies (e.g., mouse cursor movement~\cite{jiang2015salicon}),
% %of people searching or freely viewing images of scenes. 
% Despite advances in web-based data collection~\cite{semmelmann2018online, papoutsaki2015scalable}, the accurate collection of human eye movements is cumbersome and makes any approach using fixation labels for training ultimately unlikely to scale. Our focus is on goal-directed behavior (not free viewing), and specifically the prediction of gaze fixations as a person searches for a given target-object category (e.g., a clock or fork). Goal-directed behavior datasets are collected over a limited number of categories, thereby creating a generalization problem to new target categories that will appear in the wild. %Moreover, the pandemic revealed weaknesses in reliance on behavioral annotations for model training, where almost all fixation data collection was paused due to public health concerns. 
% We therefore introduce a new problem that we refer to as \textit{ZeroGaze}, which is an extension of zero-shot learning to the gaze prediction problem.  Under ZeroGaze, a model must predict the fixations that a person makes while searching for a target (e.g., a fork) despite the unavailability of search fixations (e.g., a person looking for a fork) for model training. To our knowledge, this is the first formulation of the ZeroGaze problem. This is in contrast to the traditional ``GazeTrain'' setting (where the model has been trained on gaze behavior for all categories that it encounters during inference). The differences between GazeTrain and ZeroGaze setting are highlighted in Fig. \ref{fig:setting}. 

% Existing architectures for search fixation prediction use either one-hot encoding and panoptic maps \cite{yang2020predicting} or object detection modules \cite{chen2021predicting, yang2022target} to encode specific search targets. However, the applicability of these methods is necessarily limited to the relatively small number of target objects for which one-hot encoder or object detector models can be realistically trained. For example, if an application is to predict the fixations in search for ``pizza cutter'', which is an object category not included in the dataset used to train the backbone detector in \cite{chen2021predicting}, a new ``pizza cutter'' detector would need to be trained, meaning this and related methods do not scale beyond their training data. %Because it is not feasible to train models for all the object categories that humans might engage with using their goal-directed attention, 
% Hence, a desirable property for a scalable model is to be free of the limitation of needing a backbone detector trained on all potential search targets. Additionally, since we envisage gaze prediction to be an integral part of HCI products (especially in wearable edge devices), we also want models to provide fast inference for better user experience. However, recent models\cite{yang2020predicting, chen2021predicting, yang2022target} are sequential in nature and take considerable time for inference. 

% %Most existing architectures for search fixation prediction used either one-hot encoding \cite{yang2020predicting} or detector masks \cite{chen2021predicting} to encode the search target, but these methods are limited in their applicability because they require a new model for each new target category. %The issue with these methods is that they are difficult to extend to new categories since both one-hot encoding and detectors are confined to a predetermined number of categories. This severely limits the applicability of these models because 
% %It is not feasible to collect sufficient gaze data to train models for all the object categories that humans might engage with their goal-directed behavior. Additionally, if gaze data is collected for an object category that was not included in the training set for the backbone detector, that backbone detector would require retraining to include that object before it could be used for prediction. %would be works like that of \cite{chen2021predicting} will have to retrain their backbone detector model.  \mhoai{This sentence is hard for me to parse; please rewrite}. 
% %For example, the CenterNet detector\cite{duan2019centernet} used by \cite{chen2021predicting} is pretrained on the COCO dataset. Hence, if gaze data is collected for the ``chopsticks'' category (which is not included in COCO), a new ``chopsticks'' detector would need to be trained. This makes it difficult for methods such as \cite{chen2021predicting} that rely on trained object detectors to scale beyond their training set, in this case the 171 categories included in COCO. %models to scale beyond 171 categories included in COCO dataset. \mhoai{The last sentence needs rewritten as well.}\todo{Not sure about this rationale - opinion? Open to deletion.}



% % Our approach seeks to overcome these limitations by encoding target information in a way that can generalize to \textit{most object categories}. We do this by using linguistic embeddings. %To overcome the limitations in these models, we strive to encode the target information in such a way that we can extend to \textit{any possible category}. We turn to linguistic embeddings as a possible solution. 
% % Recent advances in Natural Language Understanding and large scale language models enable us to encode real-world information related to the entities. Most importantly, linguistic embeddings enable scanpath prediction models to scale because language models can encode any string that is used to describe the target object. Additionally, language models are useful in extracting semantic relationships between objects (e.g. the similarity between a ``baseball cap'' and a ``fedora''). 

% %In this paper, we present a novel multimodal  model called  \textit{Gazeformer}. Gazeformer uses a transformer \cite{NIPS2017_3f5ee243} encoder-decoder architecture to predict the scanpath of fixations as they search for a target for which there exists a linguistic description. Gazeformer is inspired by recent advances in transformer-based architectures in Computer Vision (e.g., DETR \cite{carion2020end}, MDETR\cite{kamath2021mdetr}) and integrates for task encoding a convolutional ResNet-50 \cite{he2015deep} backbone for image feature extraction with a transformer-based language model, RoBERTa \cite{liu2019roberta}. In contrast to previous methods that used Reinforcement Learning and Inverse Reinforcement Learning methods to predict the sequence of fixations comprising a scanpath, Gazeformer treats scanpath prediction as a sequential modeling problem, thereby exploiting the known success of transformers to contextually model sequential text data. Likewise, we leverage the self-attention mechanism in transformers to learn contextualized image representations, where each token patch influences the representation of the other tokens. Human vision is well known to use context to guide attention (e.g., knowing that forks can often be found with plates, that microwaves usually are found on horizontal surfaces, etc.), thereby motivating our application of transformers to human fixation prediction in hopes of capturing this contextual guidance of attention. %embedded in transformers to contextualize image representations since context plays an important role in human visual search. \todo{find pertinent papers} The self attention mechanism and positional encodings also enable us to predict all fixations in parallel. 
% To address these issues, we propose \textit{Gazeformer}, a novel multimodal model for scanpath prediction. % that leverages recent methods from Natural Language Understanding. 
% We use a linguistic embedding of the target object using a language model (RoBERTa~\cite{liu2019roberta}) and a transformer encoder-decoder architecture \cite{NIPS2017_3f5ee243} to model the joint image-target representations and the spatio-temporal context embedded within it.  %Recent advances in large-scale natural language modeling \cite{NIPS2017_3f5ee243, devlin2018bert, liu2019roberta, brown2020language} enable the learning of vector representations of objects and their semantic relationships.  
% %us to learn the vector representation of real-world objects and their semantic relationship 
% %(e.g., the similarity between a ``baseball cap'' and a ``fedora'' or contextual similarity between ``fork'' and ``spoon''). 
% Given that recent language models can encode any object using any text string, they provide a powerful and scalable solution to the representation of target categories whose human gaze data is not available. %Moreover, Gazeformer uses a powerful transformer-based encoder-decoder architecture to effectively model spatio-temporal context. Gazeformer is multiple times faster than existing methods and achieves state-of-the-art performance on various visual tasks.%, which was originally developed for machine translation \cite{NIPS2017_3f5ee243} and is now widely used in computer vision (e.g., DETR \cite{carion2020end}, MDETR\cite{kamath2021mdetr}). %In contrast to previous methods that learn a search scanpath as a unidirectional sequence of fixations (e.g., the IRL framework\cite{yang2020predicting}), Gazeformer's use of a transformer enables it to learn the bidirectional context between fixations and to predict the entire fixation sequence ``all-at-once'', which significantly improves inference speed.  %thereby modeling the influence of both global and local image context that may be contributing to fixations in the decoded scanpath sequence. We demonstrate that by using a transformer-based mechanism along with a semantic embedding of target objects a model can predict the contextual guidance of attention observed in human vision (e.g., knowing that forks can often be found with plates, that microwaves usually are found on horizontal surfaces, etc.).

%  %For a predefined set of categories, we are able to cache target encodings which makes our scanpath prediction pipeline more compact and efficient.  

% %We evaluated our model using the target-present (TP) portion of COCO-Search18 dataset \cite{chen2020coco}, which is currently the largest dataset of search fixations available. We show that \textit{Gazeformer} not only outperforms previous visual search architectures in the \textit{ZeroGaze} setting where humans are searching for a target that the model has never seen, but also achieves better performance than baselines in the traditional setting where the model is trained on all the target categories in the evaluation set (henceforth, \textit{GazeTrain} setting). Gazeformer's performance is also superior to previous methods on target-absent (TA) task as shown empirically on target-absent scenarios of COCO-Search18 dataset. We also observe that in addition to providing superior performance consistently over several tasks, Gazeformer is also upto 5.7X faster than the existing architectures. Compared to other scanpath prediction architectures, Gazeformer is effective, efficient, easy to understand and code, scales well without additional data, and is promising in its potential to serve as a suitable gaze tracking architecture for gaze tracking applications such as AR/VR systems and other wearable devices. %Moreover, it can potentially be extended to more complex visual search tasks. 

% The specific contributions of our paper are as follows:
\begin{enumerate}\denselist
    \item We introduce the \textit{ZeroGaze} task, where a model must predict the search fixation scanpath for a new target category without training on prior knowledge of the gaze behavior to that target. %This task alleviates the cumbersome efforts required to collect gaze data for new categories.
    
    \item We devise a novel multimodal transformer-based architecture called \textit{Gazeformer}, which learns the interaction between the image input and the language-based semantic features of the target. %Gazeformer learns contextual representations of scene objects and combines them with semantically meaningful target category embeddings from a language model (RoBERTa~\cite{liu2019roberta}). %Our model combines transformers' ability to learn rich contextual relationships between scene objects and a language model's ability to encode semantically meaningful embeddings for target categories.
      \item We propose a novel and effective scheme of fixation modeling that uses a set of Gaussian distributions in continuous image space rather than learning multinomial probability distributions over patches, resulting in an intuitive distance-based objective function. 

      \item We achieve a new state-of-the-art in search scanpath prediction. It outperforms baselines, not only in our new ZeroGaze setting, but also in the traditional setting where models are trained on the gaze behavior (GazeTrain) and in target-absent search. \textit{Gazeformer} also generalizes to unknown categories while being up to more than 5 times faster than previous methods.
      
    % \item We achieve a new state-of-the-art in search scanpath prediction, outperforming baselines in the ZeroGaze setting on a target-present search task (COCO-Search18 benchmark). We further achieve new SOTA in the traditional setting where models are trained on the gaze behavior (GazeTrain). \textit{Gazeformer} also generalizes to non-canonical target names and to categories that are not annotated in the COCO dataset. 
    
    % \item \textit{Gazeformer} generalizes to target-absent search, again achieving SOTA performance in what has proved to be a challenging problem.% requiring the prediction of search termination.%, which has proved a challenging task for existing methods.
    % %achieves significantly better performance on the COCO-Search18 benchmark compared to state-of-the-art models on a target-present (TP) search task, both in the ZeroGaze setting and in the traditional setting where models are trained on gaze behavior (GazeTrain).

  
    % \item Our novel scanpath prediction method is not only very effective, it is also very efficient, up to more than 5 times faster than previous methods. %This improved efficiency is due to our modelling of fixation parameters using a set of Gaussian distributions, which creates a more intuitive objective function and enables Gazeformer to predict scanpath fixations and task termination (when to stop searching) \textit{in one go}.%The parallel decoding enables Gazeformer to registers upto \textit{5.7X speedup} over existing scanpath prediction models.

    
    %\item Gazeformer also registers upto 5.7X speedup over existing scanpath prediction models. %Our model leverages transformers' ability to learn rich contextual representation of the scene objects and a language model's ability to encode semantically meaningful embeddings for target categories to support competitive gaze prediction in both ZeroGaze and GazeTrain settings. 
    %\item This improved efficiency is due to our modelling of fixation parameters using a set of Gaussian distributions, which creates a more intuitive objective function and enables Gazeformer to predict scanpath fixations and task termination (when to stop searching) \textit{in one go}. Gazeformer can therefore generalize to a target-absent (TA) search, which has proved a challenging task for existing methods. %when trained on just target-present or just target-absent data. 
    \end{enumerate}


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

%------------------------------------------------------------------------

\section{Related Work}

\myheading{Gaze Prediction for Search Task}.
%The aim of gaze prediction is to predict the patterns of fixations and saccades that humans make during image viewing. 
Beginning with~\cite{IttiPAMI98}, most efforts for gaze prediction have been focused on using saliency maps \cite{borji2013state, kummerer2017understanding, kruthiventi2017deepfix, huang2015salicon, kummerer2014deep} to predict free-viewing behavior.
%However, saliency models that perform well on predicting free-viewing behaviors~\cite{borji2015salient, masciocchi2009everyone, berg2009free} do not generalize to the prediction of other human gaze behaviors, such as searching for target objects~\cite{henderson2007visual, koehler2014saliency}.
In free-viewing, attention is driven entirely by features extracted from the visual input, whereas in a search task, a top-down goal is used as well to guide attention. %Target guidance during search was first quantified using very simple targets having simple features that were known to the searcher~\cite{wolfe1994guided}. This work was followed by computational models that used images as inputs~\cite{zelinsky2008theory, zelinsky2013modeling} and incorporated other top-down factors affecting search guidance  \cite{aydemir2011search, malcolm2010combining}, most notably global scene context~\cite{torralba2006contextual}. 
%With deep learning models fixation prediction during search reached new levels of success. %None of these models, however, could fully account for the efficiency of human attentional control, rendering the need for more comprehensive understanding of search mechanisms that underlie human gaze behavior. 
An early study predicted search fixations for two target categories, microwave ovens and clocks, in the identical scene contexts\cite{zelinsky2019benchmarking,zelinsky2021predicting}. Inverse Reinforcement Learning was used to predict search fixations in \cite{yang2020predicting}, exploiting a recent large-scale dataset of search fixations encompassing 18 targets categories (COCO-Search18 \cite{chen2021coco}). \cite{chen2021predicting} proposed a model that predicts scanpaths using task-specific attention maps and performs well in several visual tasks including visual search. \cite{chen2022characterizing, yang2022target} addressed the target-absent problem. %They introduce a novel state representation calledFoveated Feature Maps (FFM) which incorporate the simulated fovea into a feature pyramid of a pretrained CNN. 
%The \cite{yang2020predicting}, \cite{chen2021predicting} and \cite{yang2022target} models therefore constitute state-of-the-art in predicting search fixations, but they are confined to a predefined set of categories. %neither can be used to predict the gaze behavior for a new target category for which no fixation data are available. 


\myheading{Zero Shot Learning}.
In machine learning, Zero Shot Learning refers to a problem whereby samples encountered during inference time are from classes that have not been observed during training. Zero Shot Learning has been explored in the context of image classification~\cite{elhoseiny2013write}, object detection~\cite{rahman2018zero} and many more computer vision tasks, and Zhang \etal~\cite{zhang2018finding} extended it to zero-shot visual search by proposing a network that predicts scanpaths for unseen novel objects. However, their model requires seeing a \textit{visual image of an exemplar of the target class} as input. %which complicates model comparison and violates a strict zero-shot definition. 
In contrast, we focus on the problem of ``ZeroGaze'' prediction, where the model must infer scanpaths for targets without relying on template matching or fixation data specific to the target category. %without appearance knowledge and without training on fixation data for that category.

 \begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{assets/CVPR_architecture.pdf}
%
\caption{{\bf Overall architecture of the Gazeformer model}. A transformer encoder block is used to contextualize CNN-extracted visual features. After being jointly embedded with target semantic features from a language model (LM), the resultant features interact with $L$ fixation queries in a  transformer decoder. The $L$ output encodings are passed to 7 MLP layers to obtain coordinates, duration and validity for all possible fixations in parallel. We use ResNet-50~\cite{he2016deep} as the CNN and RoBERTa~\cite{liu2019roberta} as the LM for our experiments. }
%\vskip -0.1in
\label{fig:arch}
\end{figure*}
\myheading{Transformers}.
%Transformers~\cite{NIPS2017_3f5ee243} were introduced as a new architecture for natural language understanding tasks such as machine translation. 
Transformers~\cite{NIPS2017_3f5ee243} use an attention mechanism to provide context for any position in the input sequence. %Hence, it removes any assumption of locality and takes the entire input sequence into consideration while encoding representations. 
%In contrast to RNNs, transformer encoders process the entire sequence in parallel with the aid of positional encodings to embed positional information of the tokens. Currently, transformers have supplanted RNNs as the architecture of choice for natural language understanding.
Recently, transformers have made considerable impact in Computer Vision\cite{dosovitskiy2021an, touvron2021training, liu2021swin}. %Vision Transformers (ViT) were introduced by \cite{dosovitskiy2020image} %, and is a standard Transformer model applied on the image as a sequence of ``patches''. and this inspired follow-up work . %A limitation of Vision Transformers is that they require a tokenization of input images into a fixed length sequence, which makes the model fail to learn some important local structure in the image (e.g., edges, lines). 
%A limitation of many Vision Transformers is that they support constant number of the tokens as input. Hence, details that are visible only at finer or coarser scales is lost. 
A particular line of transformer-based models (DETR~\cite{carion2020end}, MDETR~\cite{kamath2021mdetr}) use a convolutional back-end to extract local visual features, which are then used in a transformer encoder-decoder architecture to perform contextual reasoning. %An influential example of such a model is DETR~\cite{carion2020end}, which performs object detection and panoptic segmentation on images. MDETR~\cite{kamath2021mdetr} extended DETR to multimodal scenarios like phrase grounding, referring expression comprehension and segmentation. %This multi-modal transformer model combines CNN backbone image features and textual features from RoBERTa, and passes these to a DETR-like architecture to perform downstream tasks. 
%Gazeformer is largely inspired by these models in that it uses a CNN backbone to encode raw visual features of patches and then uses a transformer encoder-decoder architecture to perform contextual reasoning. 
%In contrast to MDETR, Gazeformer does not fuse the raw visual features and textual embedding; rather it combines the textual task embedding from RoBERTa with the output of the transformer encoder. 
A recent work~\cite{chao2021transformer} predicts free-viewing viewport scanpath for 360$^{\circ}$ videos with a transformer encoder based architecture.
To our best knowledge, ours is the first study that applies a \textit{transformer encoder-decoder} architecture with \textit{multimodal understanding} to predict human scanpaths for \textit{visual search}.

\section{Gazeformer}
Two factors enable Gazeformer to generalize to the ZeroGaze scanpath prediction problem. 
%Here we introduce a novel model, Gazeformer that addresses this ZeroGaze scanpath prediction problem. There are four key features of Gazeformer that help address this problem. 
First, Gazeformer jointly embeds visual and semantic features of a search target, with the latter extracted from a language model (RoBERTa\cite{liu2019roberta}). This means that the target is encoded as an interaction between image features and real-world semantic relationships.  
%The interaction of the image input with the target is therefore encoded in a meaningful space reflecting real-world semantic relationships. Because the image input is now encoded in the meaningful semantic space of the real-world (not constrained to a fixed set of categories), the representation learned by Gazeformer can be generalized to hypothetically any categories that exist in human's dictionary. 
Second, the self-attention mechanism of the encoder allows it to learn the global and local context of the scene that previous convolution-based methods missed. Gazeformer learns where targets are usually located, meaning that it learns object co-occurrence and other object-scene relationships that convey information about target location (e.g., first finding a kitchen counter top when searching for a microwave), and this enables it to generalize well to target-absent search, where scene context is crucial. 
Two additional factors improve Gazeformer's scanpath prediction capabilities. First, our model uses a transformer-based decoder to predict an entire sequence of search fixations in parallel (including when to terminate). This parallel decoding not only offers a considerable speed up in inference time for scanpath prediction, but also allows the model to learn long-range dependencies between fixations in a bidirectional manner, differing from the standard unidirectional sequential scanpath prediction approach\cite{yang2020predicting, chen2021predicting, yang2022target}. 
Second, instead of performing a patch-wise prediction for generating a sequence of fixations, as in prior approaches~\cite{yang2020predicting, chen2021predicting, yang2022target}, we directly regress the fixation locations parameterized by Gaussian distributions. This enables Gazeformer to learn an effective distance-based objective function for predicting the entire sequence of scanpath fixations at once. 
%for scanpath prediction. Along with predicting the entire sequence of fixations at once, our model also predicts fixation duration which means applications using our model will be able to pinpoint \textit{when} human attention will change. This feature was missing in a few previous works~\cite{yang2020predicting, yang2022target}.


%Here we introduce a novel model, Gazeformer that addresses this ZeroGaze problem. Gazeformer jointly embeds visual features with semantic features of a search target, which is extracted from a state-of-the-art language model (RoBERTa\cite{liu2019roberta}). The interaction of the image input with the target is therefore encoded in a meaningful space reflecting real-world semantic relationships.The proposed method uses a transformer-based encoder-decoder architecture. The self-attention mechanism of the encoder allows it to learn the global and local context of the scene that simpler convolution-based methods miss. Our model also uses a transformer-based decoder to predict an entire sequence of search fixations. This parallel decoding 
%not only offers a considerable speed up in training and inference time for scanpath prediction, but also allows the model to learn long-range dependencies between fixations and the regions fixated in a bidirectional manner which differs from the previous unidirectional sequential scanpath prediction methods\cite{yang2020predicting, chen2021predicting}. 
 

% Gazeformer that jointly embeds visual features with semantic features extracted from a state-of-the-art language model (RoBERTa\cite{liu2019roberta}) corresponding to a search target and parameterizes fixations as a set of Gaussian parameters. %The interaction of the image input with the target is therefore encoded in a meaningful space reflecting real-world semantic relationships. %, making the representation learned by Gazeformer potentially generalizable to any object category, and not the fixed few for which there are trained object detectors. %Because the image input is now encoded in the meaningful semantic space of the real-world (not constrained to a fixed set of categories), the representation learned by Gazeformer can be generalized to hypothetically any categories that exist in human's dictionary. . %The self-attention mechanism of the encoder allows it to learn the global and local context of the scene that simpler convolution-based methods miss. Our model also uses a transformer-based decoder to predict an entire sequence of search fixations. This parallel decoding not only offers a considerable speed up in training and inference time for scanpath prediction, but also allows the model to learn long-range dependencies between fixations and the regions fixated in a bidirectional manner which differs from the previous unidirectional sequential scanpath prediction methods\cite{yang2020predicting, chen2021predicting}. 
% The details of each model component and loss function are provided below.



\subsection{Architecture}
The overview of Gazeformer architecture is provided in \Fref{fig:arch}. First, image features are extracted from a ResNet-50 backbone and further processed through a transformer encoder to obtain contextual image features $\F_{image}$. The semantic embedding $\F_{target}$ of a search target is extracted from RoBERTa. Consequently, we compute $\F_{joint}$, a joint visual-semantic embedding of image and target features. Given the features $\F_{joint}$ and learnable fixation queries (which represent the time step information of each fixation), a transformer decoder produces a sequence of fixation embeddings \textit{in parallel} for each time-step. The fixation embeddings are then processed through seven independent Multi-Layer Perceptron (MLP) layers, which yield (1) fixation coordinates, (2) fixation duration, and (3) scanpath termination information as final model output. Below are the details of each model component.

% Gazeformer employs a ResNet-50 backbone for extracting raw image patch features and a transformer-based RoBERTa language model for extracting raw task text features. The image features are passed through a transformer encoder with $N$ layers to obtain a contextual representation of all image patches. Thereafter, the contextual patch features and task text features are independently transformed and appended in the channel dimension, and this concatenated tensor undergoes a linear transformation and activation to yield task-aware contextual image patch features. Now, these features serve as inputs to a transformer decoder with $N$ layers along with learnable fixation embeddings. The fixation embeddings essentially represent the positional encoding of the fixation at each time step. The final output passes through five independent Multi-Layer Perceptron (MLP) layers whose outputs are used to deduce: (1) the fixation coordinates, and (2) fixation sequence termination. The overall architecture is portrayed in \Fref{fig:arch}. %The final output is then fed to five separate MLP layers - two MLP layers \textit{regress} the mean and log variance of x-coordinate of fixation, two MLP layers \textit{regress} the mean and log variance of y-coordinate of fixation and one MLP layer \textit{classifies} whether a time step is a valid fixation or a padding value.

\myheading{Image Feature Encoding.} We extract features for an image by resizing it to $1024{\times}640$ resolution and passing it through a ResNet-50~\cite{he2016deep} network to extract a feature map of dimensions ${C{\times} h {\times} w}$, where $C=2048, h=20, w=32$.  We intentionally choose the resize resolution to be  $1024{\times}640$ so that $h=20$ and $w=32$, in order to have the same input granularity as baseline representations \cite{yang2020predicting, yang2022target}. We flatten  the spatial dimensions of this feature map to obtain a 2D feature tensor of size $C{\times} hw$. %The ResNet-50 network used here is the backbone network of a Mask-RCNN model~\cite{he2017mask}, which has been pretrained on Microsoft COCO~\cite{lin2014microsoft} training set. 
Since $C=2048$ is prohibitively large for further computation, we use a linear layer to reduce the number of feature dimensions from $2048$ to a smaller value $d$. This forms the input to a transformer encoder block that consists of a cascade of $N_{enc}$ standard transformer encoder~\cite{NIPS2017_3f5ee243} layers. Each layer uses multi-head self-attention, feed-forward networks, and layer normalization to find contextual embeddings of the input. To indicate the location of each patch, we use a fixed sinusoidal $2D$ positional encoding as in \cite{carion2020end}. This encoder block creates a task-agnostic contextualized representation of the image in the form of feature tensor $\F_{image}\in \mathbb{R}^{d{\times}hw}$. 

% \myheading{Image encoding.} We extract features for an image by resizing it to $1024{\times}640$ resolution and passing it through a ResNet-50 network to extract a feature map of dimensions ${C{\times} h {\times} w}$ where $C=2048, h=20, w=32$.  We flatten  the spatial dimensions of this feature map to obtain a 2D feature tensor of size $C{\times} hw$. We intentionally choose the resize resolution to be  $1024{\times}640$ so that $h=20$ and $w=32$, being consistent with the dimensions of the DCB representation in~\cite{yang2020predicting}. The ResNet-50 network used here is the backbone network of a Mask-RCNN model~\cite{he2017mask}, which has been pretrained on Microsoft COCO~\cite{lin2014microsoft} training set. Since $C=2048$ is prohibitively large for further computations, we use a linear layer to reduce the number of feature dimensions from $2048$ to a smaller value $d$.  This forms the input to a transformer encoder block that consists of a cascade of standard $N$ transformer encoder layers. Each layer uses multi-head self attention, feed forward networks, and layer-normalization layers to find contextual embeddings of the input. To indicate the location of each patch, we use a fixed sinusoidal $2D$ positional encoding as used in \cite{carion2020end}. This encoder block creates a task-agnostic contextualized representation of the image patches. At the end of the transformer block, we obtain feature tensor $\F_{img}\in \mathbb{R}^{d{\times} hw}$. 

\myheading{Target Feature Semantic Encoding.} To extract target features, we use the language model RoBERTa~\cite{liu2019roberta} (we use the RoBERTa-base variant) to encode the text string signifying the target category (such as ``potted plant'') as a tensor $\mathbf{F}_{target}\in\mathbb{R}^{d_{text}}$, where $d_{text}=768$. Unlike word embedding frameworks like Word2vec~\cite{Mikolov2013EfficientEO} and GloVe~\cite{pennington2014glove}, RoBERTa can encode both single-word category names such as ``car'' and multi-word target category names such as ``stop sign'' to a fixed length  vector of dimension ${d_{text}}$. 

% \myheading{Task encoding.} To extract task features, we use a RoBERTa-based language model to encode the text string signifying the target category (such as ``cup'' or ``potted plant'') as a tensor $\mathbf{F}_{task}\in\mathbb{R}^{d_{text}}$, where $d_{text}=768$. Unlike other word embedding frameworks like word2vec~\cite{mikolov2013efficient} and GloVe~\cite{pennington2014glove}, RoBERTa can encode both single-word category names such as ``car'' and multi-word target category names such as ``stop sign'' and ``potted plant'' to a fixed length  vector of ${d_{text}}$ dimensions. 




%<<<<<<< HEAD
%At this stage, we want the contextual image features to be aware of the target category. In order to do that, we first independently map $\mathbf{F}_{enc}$ and $\mathbf{F}_{text}$ to a shared multimodal latent space using modality-specific linear transformation. The obtained feature tensors corresponding to $\mathbf{F}_{enc}$ and $\mathbf{F}_{text}$ are $\mathcal{F}_{img}\in\mathbb{R}^{d{\times} hw}$ and $\mathcal{F}_{text}\in\mathbb{R}^{d}$ respectively. We spatially repeat and concatenate $\mathcal{F}_{text}$ along channel dimension with $\mathcal{F}_{img}$ to obtain tensor $\mathcal{F}\in \mathbb{R}^{2d{\times} hw}$. We thereafter reduce the dimension to $d$ by applying $1{\times} 1$ convolution on $\mathcal{F}$ to obtain $\mathcal{F}_{MM}\in \mathbb{R}^{d{\times} hw}$. This is the task-aware contextual feature tensor for the image. As the $1{\times} 1$ convolution finds correlations between the patch features in $\mathcal{F}_{img}$ and the task encoding $\mathcal{F}_{text}$ (which share a multimodal latent space), we hypothesize that the fusion serves as a zero-shot object detector for patches containing the target and semantic context extractor for other patches which share context with the target.
%=======

\myheading{Joint Embedding of Image and Target Feature.}
To create a joint visual-semantic embedding space of image and target features, we first independently map $\mathbf{F}_{image}$ and $\mathbf{F}_{target}$ to a shared multimodal $d$-dimensional latent space using modality-specific linear transformations. Then we tile the modality-specific transformation of $\mathbf{F}_{target}$ spatially $hw$ times. We concatenate the transformed image features and the transformed and tiled target features (both now sized ${d{\times}hw}$) along the channel dimension and obtain a 2D tensor of size  ${2d{\times}hw}$, which is again linearly projected (with ReLU activation) to the dimension size of~$d$. The final jointly embedded visual-semantic features are $\mathbf{F}_{joint}\in \mathbb{R}^{d\times h w}$. This joint embedding step differs from previous multimodal methods \cite{kamath2021mdetr, yang2022tubedetr} in that we append semantic features of a search target only after a target-agnostic contextual image representation is obtained from the encoder. 

% \myheading{Multimodal Fusion.}
% At this stage, we want the contextual image features to be aware of the target category. To do that, we first independently map $\mathbf{F}_{enc}$ and $\mathbf{F}_{task}$ to a shared multimodal latent space using modality-specific linear transformation. The obtained feature tensors corresponding to $\mathbf{F}_{img}$ and $\mathbf{F}_{task}$ are $\mathcal{F}_{img}\in\mathbb{R}^{d\times hw}$ and $\mathcal{F}_{task}\in\mathbb{R}^{d}$ respectively. \mhoai{Is $d$ of the shared embedding the same with $d$ of the image features mentioned above? } We spatially repeat and concatenate $\mathcal{F}_{text}$ along channel dimension with $\mathcal{F}_{img}$ to obtain a 2D tensor of size  ${2d\times hw}$. Subsequently, we reduce the dimension to $d$ by applying a linear projection and ReLU activation to obtain $\mathcal{F}_{mm}\in \mathbb{R}^{d\times h w}$. This is the task-aware contextual feature tensor for the image. As the linear projection finds correlations between the patch features in $\mathcal{F}_{img}$ and the task encoding $\mathcal{F}_{text}$ (which share a multimodal latent space), we hypothesize that the fusion serves as a object detector for patches containing the target and semantic context extractor for other patches which share context with the target. Note that this fusion step deters significantly from fusion in previous multimodal learning papers\cite{kamath2021mdetr} since we do not fuse the task features with the image features before encoding. Rather, we follow human behavior and create a task-agnostic scene representation with the encoder, and then modify it with task features for the decoder to predict scanpaths.
%>>>>>>> 6296af9ba7631ee0a1ca4331e97466ef4d12dbd5


\myheading{Fixation Decoding.} We decode an entire sequence of fixations in \textit{one go} using a transformer decoder. Like most transformer architectures, we have a maximum output scanpath sequence length $L$ which requires fixation sequences to be \textit{padded} if their lengths are smaller than $L$. The decoder contains $N_{dec}$ standard transformer decoder layers~\cite{NIPS2017_3f5ee243} (stacked self-attention and cross-attention layers with minor modifications) and processes $\mathbf{F}_{joint}$ along with a set of~$L$ \textit{fixation queries} $Q_i\in \mathbb{R}^d, i\in\{0,1,...,L-1\}$. %\mhoai{I still don't understand this part. Where does the fixation queries come from??? Is it the input or the output? }. 
The fixation queries are randomly initialized learnable embeddings that provide fixation time step information. At each decoder layer, the latent fixation embeddings interact with each other through self-attention, and also interact with $\mathbf{F}_{joint}$ through encoder-decoder cross-attention. Note that before each cross-attention step, a fixed 2D positional encoding is added to $\mathbf{F}_{joint}$ in order to provide positional information about the patches. We encode initial fixation location $(x_0, y_0)$ (which affects the generated scanpath) using another fixed 2D positional encoding and add it to $Q_{0}$. %The initial fixation is fixed to the center of the image in the dataset we used. 
%Given the information learned from the encoder (along with a fixed 2D positional encoding for positional information about the input patches), the decoder predicts fixation information for each step utilizing its self-attention and encoder-decoder cross-attention mechanisms.
The output of the transformer decoder block is $\mathbf{F}_{dec}\in \mathbb{R}^{d{\times} L}$.
% \myheading{Transformer Decoder.} We decode the fixation time-steps in \textit{parallel} with  a non-autoregressive decoder. Like most transformer architectures, we have a maximum output sequence length $L$ and hence this requires fixation sequences to be \textit{padded} if their lengths are smaller than $L$. The transformer decoder block contains $N$ standard transformer decoder layers (with minor modifications) and takes as inputs $\mathcal{F}_{mm}$ and a set of $L$ \textit{fixation} \textit{queries} $Q_i\in \mathbb{R}^d, i\in\{0,1,...,L-1\}$. \mhoai{Is $d$ here the same as $d$ for the dimension of $F_{mm}$? This part about fixation queries are not clear. }  The fixation queries are essentially learnable embeddings and provide positional info about the time steps to the transformer layers. Because the generated scanpath also depends on the initial fixation $(x_0, y_0)$, we encode this using a distinct fixed 2D positional encoding and add it to $Q_{0}$. We additionally use the fixed 2D positional encoding for positional information about the patches. Essentially, the decoder block forms as a site for multi-headed \textit{self-attention} and \textit{encoder-decoder} \textit{attention}. Because of the non-autoregressive decoding, each time-step embedding is generated by the decoder block \textit{in} \textit{parallel} which allows all time steps from all $N$ layers to attend to each other freely. So, the decoder layers are able to \textit{bidirectionally} attend to the L time-steps, which is in contrast to previous scanpath prediction models which adopt a more sequential (left-to-right) mode of fixation sequence prediction. The output of the transformer decoder block is thus $\mathcal{F}_{dec}\in \mathbb{R}^{d{\times} L}$.


\myheading{Scanpath Prediction.} Previous scanpath prediction models  \cite{yang2020predicting, chen2021predicting, yang2022target} predict fixations on a patch-level granularity by reducing the image space into a set of discrete locations and generating a multinomial probability distribution over them. However, one limitation of this approach is that all patches are at the same distance from each other---patches closer to each other are treated the same way as patches that are further apart. To remedy this, we propose to directly \textit{regress} the raw fixation co-ordinates for each $L$ possible time steps from $\mathbf{F}_{dec}$. To incorporate the inter-subject variability %uncertainty
in human fixations, we model fixation locations and durations using Gaussian distributions, and consequently regress the mean and log-variance of 2D co-ordinates and duration of a fixation  using six separate MLP layers. For the $i^{th}$ fixation, let $\mu_{x_i}, \mu_{y_i}, \mu_{t_i}, \lambda_{x_i}, \lambda_{y_i},  \lambda_{t_i}$ denote the mean and log-variance for the $x$ and $y$ positions and duration $t$. Using the reparameterization trick~\cite{kingma2013auto}, the fixation co-ordinates $x_i$ and $y_i$ and duration $t_i$ are estimated as follows:
\begin{align}
 &   x_i = \mu_{x_{i}} + \epsilon_{x_{i}} {\cdot} \exp({0.5 \lambda_{x_{i}}}), \quad 
    y_i = \mu_{y_{i}} + \epsilon_{y_{i}} {\cdot} \exp({0.5 \lambda_{y_{i}}}), \nonumber  \\
&    t_i = \mu_{t_{i}} + \epsilon_{t_{i}} {\cdot} \exp({0.5 \lambda_{t_{i}}}), \quad \epsilon_{x_{i}}, \epsilon_{y_{i}}, \epsilon_{t_{i}} \in \mathcal{N}(0,1).
\end{align}
This allows our network to be fully differentiable despite having a probabilistic component. Finally, since we use padding, we make a prediction for each output if the current step belongs to a valid fixation or a padding token. This is done by an MLP classifier with softmax activation which classifies each of the $L$ slices of $\mathbf{F}_{dec}$ to be a valid fixation or padding token. During inference, we collect the sequence of $(x_i, y_i, t_i, v_i)$ ordered quads for $i \in \{0,1,...,L-1\}$, where $x_i$, $y_i$ are the coordinates of fixation $i$, $t_i$ is the duration of fixation $i$, and $v_i$ the probability of being a valid fixation. We traverse the sequence from $i=0$ to $i=L-1$ and terminate the sequence at the first padding token ($v_i< 0.5$). We experimentally confirmed that our model performs better with regression than with classification. Architecture details are in the supplemental.

% \myheading{Scanpath Prediction.} Previous scanpath prediction models like \cite{yang2020predicting, chen2021predicting} associate fixations on a patch-level granularity and reduce the image space into a set of discrete locations and output a multinomial probability distribution over these locations. However, one limitation of this approach is that all locations are equally separated from each other---patches closer to each other are treated the same way as patches that are further apart. To remedy this, we propose to directly \textit{regress} the raw fixation co-ordinates. We also see experimentally that our model performs better with regression than with classification (details in supplementary). To incorporate the uncertainty in human fixations, we regress the mean  and log-variance of the 2D co-ordinate of a fixation  using four MLP layers. For the $i^{th}$ fixation, let $\mu_{x_i}, \mu_{y_i}, \lambda_{x_i}, \lambda_{y_i}$ denote the mean and log-variance for the $x$ and $y$ positions. Using the reparameterization trick used in Variational Autoencoders \cite{kingma2013auto} the fixation co-ordinates $x_i$ and $y_i$ are estimated as follows:
% \begin{equation}
%     x_i = \mu_{x_{i}} + \epsilon {\cdot} \exp({0.5 \lambda_{x_{i}}}), \quad 
%     y_i = \mu_{y_{i}} + \epsilon {\cdot} \exp({0.5 \lambda_{y_{i}}}), \quad \epsilon \in \mathcal{N}(0,1).
% \end{equation}
% This allows our network to be fully differentiable despite having a probabilistic component. Additionally, since we might have padded tokens in our sequence, we also need to predict if a time step corresponds to a valid fixation or a padding token. This is done by an MLP classifier with softmax activation that classifies a time-step into two classes: valid or padding token. During inference, we collect the sequence of $(x_i, y_i, v_i)$ ordered triads for $i \in \{0,1,...,L-1\}$, where $x_i$, $y_i$ are the coordinates of fixation $i$, and $v_i$ the probability of being a padding token. Finally, we traverse the sequence from left to right and terminate the sequence when we encounter the first padding token. 

% This algorithm is simple enough to be explained by the above sentence already. There is no need to have an explicit algorithm here.
%A detailed pseudocode can be found in Algorithm \ref{alg:inference}.
%\begin{algorithm}[ht!]
%\caption{Scanpath Inference}
%\textbf{Input} : $\{(x_i, y_i, v_i)$, $i\in \{0,1,...,L-1\}\}$
% 
%\textbf{Output} : $\{(x_j, y_j)$, $j\in \{0,1,...,l-1\}\}$, $l$ 
%\begin{algorithmic} 
%\STATE $i \leftarrow 0$
%\STATE scanpath = \{\}
%\WHILE{$i < L$}
%\IF{$v_i < 0.5$}
%\STATE \textbf{break}
%\ELSE
%\STATE scanpath.insert(($x_i, y_i$))
%\ENDIF
%\STATE $i \leftarrow i + 1$
%\ENDWHILE
%\STATE $l \leftarrow i $
%\end{algorithmic}
%\label{alg:inference}
%\end{algorithm}
\subsection{Training}

%\myheading{Training losses.} 
We view scanpath prediction as a sequence modeling task. The total multitask loss $\mathcal{L}$ to train our network with backpropagation for a minibatch of $M$ samples is:

\begin{align}
    &\mathcal{L} = \frac{1}{M} \sum_{j=1}^{M} \left( \mathcal{L}_{xyt}^{j} + \mathcal{L}_{val}^{j} \right), \\
    \textrm{where } &\mathcal{L}_{xyt}^{j} = \frac{1}{l^{j}}  \sum_{i=0}^{l^{j}-1} \left( |x_i^{j}  - \hat{x}_i^{j}| + |y_i^{j}  - \hat{y}_i^{j}| + |t_i^{j}  - \hat{t}_i^{j}| \right), \nonumber \\ %\quad  x_i^{j} = \mu_{x_{i}^{j}} + \epsilon {\cdot} \exp({0.5 \lambda_{x_{i}^{j}}}), \quad \epsilon \in \mathcal{N}(0,1).
   % &\mathcal{L}_{t}^{j} = \frac{1}{l^{j}}  \sum_{i=0}^{l^{j}-1} \left( |t_i^{j}  - \hat{t}_i^{j}| \right),\\
    &\mathcal{L}_{val}^{j} = - \frac{1}{L}  \sum_{i=0}^{L-1} \left(v_i^{j} \log \hat{v}_i^{j} + (1-v_i^{j})\log (1 - \hat{v}_i^{j})\right). \nonumber 
\end{align}
Here $s^j = \{(x_{i}^{j}, y_{i}^{j}, t_{i}^{j})\}_{i=0}^{L-1}$ is the  predicted scanpath and $L$ is the maximum scanpath length. $l^{j}$ is the length of the ground truth scanpath $\hat{s}^j = \{(\hat{x}_{i}^{j}, \hat{y}_{i}^{j}, \hat{t}_{i}^{j})\}_{i=0}^{l^{j}-1}$. $\hat{v}_i^{j}$ is a binary scalar  representing ground truth of $i^{th}$ token in $s^j$ being a valid fixation or padding and $v_i^{j}$ is the probability of that token being a valid fixation as estimated by our model. %$\mathcal{L}$ is a multi-task loss to train our network using backpropagation. 
The losses included in $\mathcal{L}_{xyt}$ are the $L_1$ regression losses for $x$ and $y$ co-ordinates and duration $t$ of the fixations. $\mathcal{L}_{val}$ is the negative log likelihood loss for validity prediction for each token. We find the $L_1$ loss to be more appropriate for scanpath prediction because of the intrinsic variability in human fixation locations in a multi-subject setting.  Note that we mask out ground truth padded tokens while calculating $\mathcal{L}_{xyt}$ to only account for valid fixations in ground truth. Optimization and training details are in the supplemental.
% is calculated only for those tokens which are designated as valid fixations in ground truth.
% \mhoai{Can you explicitly write the loss in terms of $\mu_{x_i}, \lambda_{x_i}$, based on your description, I see the loss would involve $\mu_{x_i}$, but I am not sure where $\lambda_{x_i}$ would be used. The lost can be written more explicitly for the valid and padding tokens; can you write it as the summation over index $i$?}

%
% \begin{figure}[ht!]
% \centering
% \includegraphics[width=\linewidth]{assets/Settings.pdf}
% \caption{Comparison between GazeTrain and ZeroGaze inference setting. Here, we show an example scenario where human scanpaths for only "fork", "cup" and "stop sign" are available during training. In the GazeTrain setting, the model will be asked to predict scanpaths for only those target categories it has seen during training. However, in the ZeroGaze setting, the model will be expected to predict scanpaths for target categories for which training scanpaths were not available, such as "knife", "bottle" and "car".}
% \label{fig:setting}
% \end{figure}

\section{Experiments}
We evaluate and compare the model performance under two prediction scenarios: ZeroGaze and GazeTrain. In the ZeroGaze setting, a model must predict search fixations for a category not encountered during training. GazeTrain is the traditional scanpath prediction setting where a model is trained and tested to predict search fixations using the same set of target categories. All experiments pertaining to ZeroGaze and GazeTrain settings were conducted using the target-present data of the COCO-Search18~\cite{chen2021coco} dataset following the evaluation scheme in~\cite{yang2020predicting}. COCO-Search18 consists of 3101 target-present (TP) images, and 100,232 scanpaths collected for 18 categories from 10 subjects. We use the original train-validation-test split provided by \cite{yang2020predicting} and report all results on the test set. We also demonstrate the superior generalizability of our method by comparing it with other baselines on the target-absent data of COCO-Search18 after training the model on either (1) the target-present data or (2) the target-absent data. The IRL model from Yang \etal~\cite{yang2020predicting}, the model from Chen \etal~\cite{chen2021predicting} and a recent FFM model from Yang \etal~\cite{yang2022target} are used as baseline methods. Since only Chen \etal~\cite{chen2021predicting}'s model predict duration, we also provide a variation of Gazeformer called \textit{Gazeformer-noDur} which does not train on or predict fixation duration for fair comparison with the IRL and FFM baselines. We remap the predicted fixations from Chen \etal~\cite{chen2021predicting}'s model to a $20{\times}32$ grid following \cite{yang2022target}. All of the models except IRL predict search stopping behavior, and we use the baselines' stopping prediction modules whenever available to obtain a fairer comparison. We try to maintain the same hyperparameters across settings for all models to evaluate generalizability and scalability.

\subsection{Implementation Details}

% We use three AdamW~\cite{loshchilov2017decoupled} optimizers (FastOpt, MidOpt and SlowOpt) as mentioned in the previous section. We assign learning rate of 1e-6 to SlowOpt, 2e-6 to MidOpt and 1e-4 to FastOpt. %All optimizers share the same weight decay of 1e-4. 
Unless explicitly specified otherwise, the results reported in this paper are for the Gazeformer model with 6 encoder layers, 6 decoder layers, 8 attention heads per multihead attention block, and hidden size ($d$) of 512.  Following \cite{yang2020predicting}, we set maximum scanpath length to 7 (including the initial fixation) for our experiments. % and is trained for 200 epochs with batch size 32. We use a dropout rate of 0.1 for the encoder block and modality-specific transformations, 0.2 for the  multimodal fusion and the decoder block, and 0.4 for the seven MLP output layers for fixation prediction. Please note that we maintain the same hyperparameters for our model in order to emphasize the generalizability of our model. 
Due to the limited availability of training data and computational resources for additionally finetuning the backbone networks, we %refrain from training the backbone CNN model (ResNet-50) and language model (RoBERTa) and 
use frozen ResNet-50 and RoBERTa encodings. Models generate 10 scanpaths per test image by sampling at each fixation, and the reported metrics are averages. %Gazeformer code will be made available upon publication. 



% \setlength{\tabcolsep}{5pt}
% \begin{table*}[ht!]
% \begin{center}
% \caption{Model performance comparison under the traditional GazeTrain setting. Best performance is highlighted in bold. Performances that exceed human consistency are underlined.}
% \label{table:gazetrain_results}
% \begin{tabular}{l|cc|cc|cc|cc|c|c|c}
% \toprule 
%  & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c|}{SemSS$\bm{\uparrow}$} & \multicolumn{2}{c|}{FED$\bm{\downarrow}$} & \multicolumn{2}{c|}{SemFED $\bm{\downarrow}$} & MM & NSS & CC\\  
%  & w/o Dur & w/ Dur & w/o Dur & w/ Dur & w/o Dur & w/ Dur & w/o Dur & w/ Dur & $\bm{\uparrow}$ & $\bm{\uparrow}$ & $\bm{\uparrow}$
%  \\
%  \Xhline{0.75pt}
% Human & 0.490 & 0.409 & 0.548&0.456 & 2.531&11.526 & 1.637&8.086& 0.857 & 8.129 & 0.472 \\ \hline
% IRL~\cite{yang2020predicting} & 0.418&- & 0.499&- & 2.722&- & 2.182&- & 0.833 & 6.895 & 0.434 \\
% Chen \etal~\cite{chen2021predicting} & 0.451&0.403 & 0.504&0.446 & \underline{2.187} &\underline{10.795} & 1.788&8.782 & 0.820 & 6.901 & \underline{0.547} \\
% FFM~\cite{yang2022target} & 0.392&- & 0.443&- & 2.693&- & 2.284&- & 0.808 & 5.576 & 0.370 \\ \hline
% Gazeformer-noDur & \underline{\textbf{0.504}}&- & \textbf{0.534}&- & \underline{\textbf{2.061}}&- & \textbf{1.742}&- & 0.849 & \underline{8.356} & \underline{0.559} \\
% Gazeformer & \underline{\textbf{0.504}}&\underline{\textbf{0.451}} & 0.525&\underline{\textbf{0.485}} & \underline{2.072}& \underline{\textbf{9.708}} & 1.810 &\underline{\textbf{7.688}} & \textbf{0.852} & \underline{\textbf{8.375}} & \underline{\textbf{0.561}}\\
% \bottomrule 
% \end{tabular}

% \end{center}
% \end{table*}

% \setlength{\tabcolsep}{5pt}
% \begin{table}[ht!]
% \begin{center}
% \caption{Performance comparison for models tested on target-absent data after training on (a) target-present data and (b) target-absent data. Best performance is highlighted in bold. Performances that exceed human consistency are underlined.}
% \label{table:ta_results}
% \begin{tabular}{l|cc|cc}
% \toprule 
%  & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c}{SemSS$\bm{\uparrow}$}\\
%  & w/o Dur & w/ Dur & w/o Dur & w/ Dur
%  \\
%  \Xhline{0.75pt}
% Human & 0.398 & 0.369 & 0.436 & 0.404\\ \hline
% IRL~\cite{yang2020predicting} & 0.304 & - & 0.349 & -  \\
% Chen \etal~\cite{chen2021predicting} & 0.350 & 0.330 & 0.395 & 0.380  \\
% FFM~\cite{yang2022target} & 0.360 & - & 0.413 & - \\ \hline
% Gazeformer-noDur & 0.366 & - & \textbf{0.419} & -  \\
% Gazeformer & \textbf{0.368} & \textbf{0.356} & \textbf{0.419} & \textbf{0.399}\\
% \bottomrule 
% \end{tabular}
% (a)\\

% \begin{tabular}{l|cc|cc}
% \toprule 
%  & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c}{SemSS$\bm{\uparrow}$}\\
%  & w/o Dur & w/ Dur & w/o Dur & w/ Dur
%  \\
%  \Xhline{0.75pt}
% %Human & 0.398 & 0.369 & 0.436 & 0.404\\ \hline
% IRL~\cite{yang2020predicting} & 0.323 & - & 0.378 & -\\
% Chen \etal~\cite{chen2021predicting} & 0.345 & 0.323 & 0.347 & 0.335  \\
% FFM~\cite{yang2022target} & 0.362 & - & 0.413 & - \\ \hline
% Gazeformer-noDur & 0.369 & - & 0.422 & -  \\
% Gazeformer & \textbf{0.375} & \textbf{0.361} & \underline{\textbf{0.438}} & \underline{\textbf{0.417}} \\
% \bottomrule 
% \end{tabular}
% (b)
% \end{center}
% \end{table}


\subsection{Metrics}
We report model performance in search scanpath prediction using a diverse set of metrics. \textit{Sequence Score (SS)} \cite{yang2020predicting} converts scanpaths into strings of fixation cluster IDs and a string matching algorithm \cite{needleman1970general} measures similarity between two strings. %Contrary to the ScanMatch metric used in \cite{chen2021predicting}, this metric does not perform binning over scanpath data using a predefined grid size, which is vulnerable to scale and perspective variations. 
\textit{Semantic Sequence Score (SemSS)}~\cite{yang2022target} modifies SS by converting scanpaths to strings of fixated objects in the scene instead of cluster IDs, increasing interpretability. Unlike the implementation of SemSS by \cite{yang2022target} that disregarded the COCO stuff classes, we include both thing and stuff classes for object assignment. We also include \textit{Fixation Edit Distance (FED)} and \textit{Semantic Fixation Edit Distance (SemFED)}, which convert scanpaths to strings like the SS and SemSS metrics, respectively, but use the Levenshtein algorithm\cite{Levenshtein1965BinaryCC} to measure scanpath dissimilarity. \textit{Multimatch (MM)} \cite{anderson2015comparison, dewhurst2012depends} is a popular metric that measures the scanpath similarity at the pixel level. %It measures five aspects of scanpath similarity: shape, direction, length, position, and duration. 
Here, MM indicates the average of the shape, direction, length, and position scores (individual metrics in supplemental). %Additionally, we report saliency metrics \textit{NSS} and \textit{CC}.  
\textit{Correlation Coefficient (CC)~\cite{jost2005assessing}} measures the correlation between the normalized model and the human fixation map (i.e., the 2D distribution map of all fixations convolved with a Gaussian). \textit{Normalized Scanpath Saliency (NSS)~\cite{peters2005components}} is a discrete approximation of CC, which averages the values of a model's fixation map from the human fixated locations~\cite{bylinskii2018different}. Higher values for SS, SemSS, MM, NSS and CC indicate greater similarity between model-generated and human search scanpaths. The direction is opposite for FED and SemFED. We include SS, FED, SemSS and SemFED both with duration (as in~\cite{cristino2010scanmatch}) and without duration. 




% \begin{enumerate}
%    \item \textbf{Sequence Score (SS)} \cite{yang2020predicting}: This metric converts scanpaths into strings of fixation cluster IDs and a string matching algorithm \cite{needleman1970general} measures similarity between two strings. %Contrary to the ScanMatch metric used in \cite{chen2021predicting}, this metric does not perform binning over scanpath data using a predefined grid size, which is vulnerable to scale and perspective variations. 
%    %We also include Sequence Score with duration (SS-Dur) using the scheme presented in~\cite{cristino2010scanmatch}. 
%     % \item \textbf{Probability Mismatch}: This metric is the sum of absolute differences between the human and model cumulative probability of fixating on the target. 
%     \item{\textbf{Semantic Sequence Score (SemSS)}}~\cite{yang2022target}: This metric converts scanpaths into strings of objects in the scene that fixations land on and a string matching algorithm \cite{needleman1970general} measures similarity between two strings. %We also include Sequence Score with duration (SemSS-Dur) using the scheme presented in~\cite{cristino2010scanmatch}. 
%     Please note that in contrast to \cite{yang2022target}'s implementation which disregards the stuff classes of COCO, we include both thing and stuff classes for object assignment of fixations.
%     \item \textbf{Fixation Edit Distance (FED)}~\cite{chen2021predicting}: This metric converts scanpaths into strings of fixation cluster IDs (similar to Sequence Score) but uses Levenshtein algorithm\cite{levenshtein1966binary} to measure scanpath dissimilarity. %We also include Fixation Edit Distance (FED-Dur) with duration using the scheme presented in~\cite{cristino2010scanmatch}. 
%     \item \textbf{Semantic Fixation Edit Distance (SemFED)}: This metric converts scanpaths into strings of object categories in the scene(similar to Semantic Sequence Score) but uses Levenshtein algorithm\cite{levenshtein1966binary} to measure scanpath dissimilarity. %We also include Semantic Fixation Edit Distance with duration (SemFED-Dur) using the scheme presented in~\cite{cristino2010scanmatch}.
%     \item \textbf{Multimatch} \cite{anderson2015comparison, dewhurst2012depends}: This metric measures the scanpath similarity at the pixel level. It measures five aspects of scanpath similarity: shape, direction, length, position, and duration. We report the average of multimatch scores for shape, direction, length, position as $MM$. Multimatch scores for duration is omitted for models which do not predict duration and will be included with the individual multimatch scores in the supplementary material.
%     \item \textbf{Normalized Scanpath Saliency (NSS)~\cite{peters2005components}}: This metric measures the average of the response values at human eye positions in a models saliency map that has been normalized to have zero mean and unit standard deviation.

%     \item  \textbf{Correlation Coefficient (CC)~\cite{jost2005assessing}} This metric measures the correlation between
% model saliency map and ground truth saliency map after normalizing both saliency
% maps to have zero mean and unit variance.
% \end{enumerate}

\setlength{\tabcolsep}{4pt}
\begin{table*}[ht!]
\centering
\begin{tabular}{l|cc|cc|cc|cc|c|c|c}
\toprule 
 & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c|}{SemSS$\bm{\uparrow}$} & \multicolumn{2}{c|}{FED$\bm{\downarrow}$} & \multicolumn{2}{c|}{SemFED $\bm{\downarrow}$} & MM  & CC & NSS\\  
 & w/o Dur & w/ Dur & w/o Dur & w/ Dur & w/o Dur & w/ Dur & w/o Dur & w/ Dur & $\bm{\uparrow}$ & $\bm{\uparrow}$ & $\bm{\uparrow}$
 \\
 \Xhline{0.75pt}
IRL~\cite{yang2020predicting} & 0.290 & - & 0.314 & - & 4.606 & - & 4.377 & - & 0.774 & 0.241 & 4.018 \\
Chen \etal~\cite{chen2021predicting} & 0.210 & 0.041 & 0.211 & 0.034 & 5.720 & 210.498 & 5.608 & 211.636 &0.717 & 0.002 & 0.001 \\
FFM~\cite{yang2022target} & 0.300 & - & 0.334 & - & 3.271 & - & 2.918 & -& 0.731 & 0.271 & \textbf{5.247}\\ \hline
Gazeformer-noDur & \textbf{0.359} & - & \textbf{0.391} & - & 2.788 & - & 2.474 & - &\textbf{0.822} & 0.316 & 4.671 \\
Gazeformer & 0.358 & \textbf{0.312} & \textbf{0.391} & \textbf{0.348} & \textbf{2.766} & \textbf{12.505} & \textbf{2.438} & \textbf{10.391} & 0.812 & \textbf{0.324} & 4.929\\
\bottomrule 
\end{tabular}\\
(a)\\%ZeroGaze Setting\\

\begin{tabular}{l|cc|cc|cc|cc|c|c|c}
\toprule 
 & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c|}{SemSS$\bm{\uparrow}$} & \multicolumn{2}{c|}{FED$\bm{\downarrow}$} & \multicolumn{2}{c|}{SemFED $\bm{\downarrow}$} & MM & CC & NSS\\  
 & w/o Dur & w/ Dur & w/o Dur & w/ Dur & w/o Dur & w/ Dur & w/o Dur & w/ Dur & $\bm{\uparrow}$ & $\bm{\uparrow}$ & $\bm{\uparrow}$
 \\
 \Xhline{0.75pt}
Human & 0.490 & 0.409 & 0.548&0.456 & 2.531&11.526 & 1.637&8.086& 0.857 & 0.472 & 8.129\\ \hline
IRL~\cite{yang2020predicting} & 0.418&- & 0.499&- & 2.722&- & 2.182&- & 0.833 & 0.434 & 6.895\\
Chen \etal~\cite{chen2021predicting} & 0.451&0.403 & 0.504&0.446 & \underline{2.187} &\underline{10.795} & 1.788&8.782 & 0.820 & \underline{0.547} & 6.901 \\
FFM~\cite{yang2022target} & 0.392&- & 0.443&- & 2.693&- & 2.284&- & 0.808 & 0.370 & 5.576 \\ \hline
Gazeformer-noDur & \underline{\textbf{0.504}}&- & \textbf{0.534}&- & \underline{\textbf{2.061}}&- & \textbf{1.742}&- & 0.849  & \underline{0.559} & \underline{8.356}\\
Gazeformer & \underline{\textbf{0.504}}&\underline{\textbf{0.451}} & 0.525&\underline{\textbf{0.485}} & \underline{2.072}& \underline{\textbf{9.708}} & 1.810 &\underline{\textbf{7.688}} & \textbf{0.852} & \underline{\textbf{0.561}}& \underline{\textbf{8.375}} \\
\bottomrule 
\end{tabular}\\
(b)%GazeTrain Setting
%\vskip -0.1in
\caption{Model performance comparison under (a) ZeroGaze setting, (b) traditional GazeTrain setting.  Best performance is highlighted in bold. Performances that exceed human consistency are underlined.}
\label{table:all_results}
%\end{center}
\end{table*}



\subsection{ZeroGaze Setting}
\begin{figure*}[ht!]
\centering
\includegraphics[width=\linewidth]{assets/ZeroGaze.pdf}
\caption{Visualization of scanpath prediction under the ZeroGaze setting. %Each column corresponds to the scanpath generated from human, IRL~\cite{yang2020predicting}, Chen et al's model~\cite{chen2021predicting}, FFM~\cite{yang2022target} and Gazeformer (Our). The targets are ``sink'' and ``fork''. 
The number and radius indicate the fixation
order and duration (if predicted), respectively. Our Gazeformer model predicts efficient, human-like scanpaths.}
\label{fig:zerogaze}
\end{figure*}
Models were trained on 17 of the COCO-Search18 categories and tested on the one left-out category in a cross-validation manner. For example, to evaluate ZeroGaze performance on category $\mC$, we remove the category $\mC$ scanpaths from the training data and predict gaze behavior for this category during testing. Thus, we ensure that a model does not see any behavioral data related to category $\mC$ for ZeroGaze prediction. Performance was averaged over all categories and weighted by the number of test cases for each category. As shown in \Tref{table:all_results}(a), Gazeformer achieves state-of-the-art ZeroGaze performance, outperforming the baselines by considerable margins across almost all metrics. See Fig.~\ref{fig:zerogaze} for some qualitative results. More qualitative results can be found in the supplemental.
% As it can be seen, our model generates very efficient, human-like search scanpath without being trained on the behavioral data on these categories. %We make a trivial modification to IRL model by increasing size of one-hot encoding from 18 to 134 (for 134 COCO categories) to reflect realistic zero-shot setting.

%Please note that in case of IRL and FFM model, the model only accomodates 18 COCO-Search18 categories. In any zero-shot setting, the model must expect a wider variety of unseen classes. Hence, for fair comparison, we expand the dimension of one-hot vector used for target embedding in IRL model to 134 COCO categories, not 18 as in the original IRL model. It was non-trivial to do the same for FFM model - so we refrained from making this modification to FFM model. Please note that all previous methods are confined to a set number of pre-defined categories while our model can be applied to a \textit{true} ZeroGaze setting where the target can be \textit{any perceivable object} since we are guided by language. %instead of object detectors and one-hot embeddings.

\subsection{Traditional GazeTrain Setting}

In the traditional GazeTrain setting, models were trained with target-present search fixations on 18 different target categories (from the COCO-Search18 dataset) and tested on the same set of categories but with unseen test cases. We compared the predictive performance of Gazeformer and previous models of search scanpath prediction. Table \ref{table:all_results}(b) shows that Gazeformer generally outperforms the baselines (by significant margins for some metrics) and is at (or slightly exceeds) a noise ceiling imposed by human consistency rates (i.e., predictions cannot be much better). Hence, Gazeformer is the new state-of-the-art in target-present visual search scanpath prediction. We include qualitative results of Gazeformer and other baseline models in the supplemental.



% \setlength{\tabcolsep}{5pt}
% \begin{table}[ht!]
% \begin{center}
% \caption{Performance comparison for models trained with target-absent data and tested on target-absent data. The best performance for each metric is highlighted in bold. The performance that exceeds human consistency are underlined.}
% \label{table:taonta_results}
% \begin{tabular}{l|cc|cc}
% \toprule 
%  & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c}{SemSS$\bm{\uparrow}$}\\
%  & w/o Dur & w/ Dur & w/o Dur & w/ Dur
%  \\
%  \Xhline{0.75pt}
% Human & 0.398 & 0.369 & 0.436 & 0.404\\ \hline
% IRL~\cite{yang2020predicting} & 0.323 & - & 0.378 & -\\
% Chen \etal~\cite{chen2021predicting} & 0.345 & 0.323 & 0.347 & 0.335  \\
% FFM~\cite{yang2022target} & 0.362 & - & 0.413 & - \\ \hline
% Gazeformer-noDur & 0.369 & - & 0.422 & -  \\
% Gazeformer & \textbf{0.375} & \textbf{0.361} & \underline{\textbf{0.438}} & \underline{\textbf{0.417}} \\
% \bottomrule 
% \end{tabular}

% \end{center}
% \end{table}

\setlength{\tabcolsep}{5pt}
\begin{table*}[ht!]
\centering

\begin{tabular}{l|cc|cc|c|cc|cc|c}
\toprule 
& \multicolumn{5}{|c|}{Trained on Target-Present} & \multicolumn{5}{c}{Trained on Target-Absent}
\\
 \Xhline{0.75pt}
 & \multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c|}{SemSS$\bm{\uparrow}$} & MM &\multicolumn{2}{c|}{SS$\bm{\uparrow}$} & \multicolumn{2}{c|}{SemSS$\bm{\uparrow}$} & {MM}\\
 & w/o Dur & w/ Dur & w/o Dur & w/ Dur & $\bm{\uparrow}$ & w/o Dur & w/ Dur & w/o Dur & w/ Dur & $\bm{\uparrow}$
 \\
 \Xhline{0.75pt}
Human & 0.398 & 0.369 & 0.436 & 0.404 & 0.838 & 0.398 & 0.369 & 0.436 & 0.404 & 0.838\\ \hline
IRL~\cite{yang2020predicting} & 0.304 & - & 0.349 & - & 0.808 & 0.323 & - & 0.378 & - & 0.805 \\
Chen \etal~\cite{chen2021predicting} & 0.350 & 0.330 & 0.395 & 0.380  & 0.813 & 0.345 & 0.323 & 0.347 & 0.335 & 0.799\\
FFM~\cite{yang2022target} & 0.360 & - & 0.413 & -  & 0.814 & 0.362 & - & 0.413 & - & 0.814\\ \hline
Gazeformer-noDur & 0.366 & - & \textbf{0.419} & -  & \textbf{0.833} & 0.369 & - & 0.422 & - & 0.830 \\
Gazeformer & \textbf{0.368} & \textbf{0.356} & \textbf{0.419} & \textbf{0.399}
 & 0.825 & \textbf{0.375}&\textbf{0.361} & \underline{\textbf{0.438}} & \underline{\textbf{0.417}}& \underline{\textbf{0.844}}\\
\bottomrule 
\end{tabular}
\caption{Performance comparison for models tested on target-absent data after training on target-present data and target-absent data. Best performance is highlighted in bold. Performances that exceed human consistency are underlined.}
\label{table:ta_results}
%\end{center}
\end{table*}

\subsection{Generalizability to Target-Absent Search}



To demonstrate generalizability, we evaluated model performance on COCO-Search18 target-absent trials under two settings - (1) where the model is trained on only target-present data, and (2) where the model is trained on only target-absent data. %To stress generalizability, we do not tune Gazeformer's hyperparameters for this specific task. 
The first setting should be direct evidence for the capability of the model to learn the important contextual relationships between scene objects that humans use to guide their search for a target when it is absent from the scene. %are crucial for search even when it is trained on target-present trials. 
The second setting tests the model's capability to learn search behavior for the target-absent task by training on target-absent human trials. %even without the presence of a target. 
We report the results for both settings in Table~\ref{table:ta_results}.  Similar to the target-present results, Gazeformer again outperforms all baselines in multiple metrics, establishing new SOTA in target-absent scanpath prediction as well (although the human noise ceiling was only achieved with training on the target-absent data). Gazeformer appears to be learning scene context and object relationships well and generalizes to target-absent search without any change in architecture or hyperparameters. We include a comprehensive version of Table \ref{table:ta_results} and qualitative evidence for Gazeformer's use of context through attention maps in the supplemental.



\subsection{Inference Speed}

\begin{table}[ht!]
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{lccc}
\toprule 
 & Time (in ms)$\bm{\downarrow}$  & Inferences/s $\bm{\uparrow}$ & Speedup $\bm{\uparrow}$\\  
 \Xhline{0.75pt}
 %\midrule 
 Chen \etal~\cite{chen2021predicting} & 386 & 2.59 & 1X  \\

 FFM~\cite{yang2022target} & 133 & 7.52 & 2.9X\\
 IRL~\cite{yang2020predicting} & 85 & 11.77 & 4.5X\\
 Gazeformer & \textbf{68} &\textbf{14.71} & \textbf{5.7X}\\
 \bottomrule 
 \end{tabular}
 \caption{Comparison of inference speeds. Gazeformer achieves 5.7X speedup over highly competent Chen \etal's~\cite{chen2021predicting} model.}
\label{table:speed_results}
%\end{center}
\end{table}

 Despite clearly outperforming baselines in search scanpath prediction, Gazeformer is many times faster. We attribute this to the parallel decoding mechanism in the transformer decoder that predicts the entire scanpath in one go while other baselines are sequential in nature. We report the average time measured on COCO-Search18's test split of target-present trials in Table 
\ref{table:speed_results}. 
%Note that the reported results are the average time measured on COCO-Search18's test split of target-present trials. 
We measure time in the real-world setting where the model receives and operates on one search task case (image-target pair) at a time. %As it is evident from Table \ref{table:speed_results}, Chen \etal's model is the slowest while Gazeformer model is the fastest despite being superior in performance across the traditional target-present and target-absent tasks.





\subsection{Extension to Unknown Categories}


\def\subFigSz{0.32\linewidth}
\begin{figure}[ht!]
\centering
  \includegraphics[width=\subFigSz]{assets/extend/hatchback.jpg} 
  \includegraphics[width=\subFigSz]{assets/extend/sedan.jpg}  
  \includegraphics[width=\subFigSz]{assets/extend/mug.jpg}\\
  \makebox[\subFigSz]{\small{find ``hatchback''}}
\makebox[\subFigSz]{\small{find ``sedan''}}
\makebox[\subFigSz]{\small{find ``mug''}}\\
%(a)Beyond COCO-Search18 canonical category names\\

\includegraphics[width=\subFigSz]{assets/extend/trash-can.jpg} 
  \includegraphics[width=\subFigSz]{assets/extend/pizza-cutter.jpg}  
  \includegraphics[width=\subFigSz]{assets/extend/soda-can.jpg}\\
  \makebox[\subFigSz]{\small{find ``trash can''}}
\makebox[\subFigSz]{\small{find ``pizza cutter''}}
\makebox[\subFigSz]{\small{find ``soda can''}}
%(b) Beyond COCO categories
\caption{Generalization of Gazeformer to unknown categories. Top row shows extensions to non-canonical names of COCO-Search18's categories; bottom row shows extensions beyond COCO-annotated categories.} %(a) beyond COCO-Search18's category's canonical names, (b) beyond COCO-annotated categories }
\label{fig:lm_extends}
\end{figure}

Since the proposed model uses the RoBERTa language model to encode target objects, hypothetically it can generalize to searching for any object that can be described in words or text.  First, we generate and visualize new scanpaths using the synonyms or hyponyms of the COCO-Search 18 objects to define targets (e.g., replacing ``cup'' with ``mug'' and ``car'' with ``hatchback'' or ``sedan''). We also investigate if our model can extend beyond the MS-COCO dataset used to train backbone models, to completely unseen targets like ``trash can'', ``pizza cutter'' or ``soda can''%, showing zero-shot detection capabilities
. Quantitative comparison is not possible because there are no human search fixation data, so we only generate and visualize scanpaths for these cases. As shown in Fig.~\ref{fig:lm_extends}, Gazeformer generates plausible natural-looking scanpaths that successfully find the unknown target. %thereby highlighting its scalability and robustness. 
Previous methods are confined to a predefined set of categories due to their reliance on detector/panoptic maps and fail to handle these scenarios.% and categories for which annotation is available in the training data.%This is important for use in real-life scenarios where a human user is not confined to only the canonical names of targets.

%\subsection{Ablation Studies}

\subsection{Impact of Language Embeddings}

Gazeformer uses RoBERTa\cite{liu2019roberta} linguistic embeddings as target representations, and to explore their role in model performance, we replaced the 18 target category embeddings from RoBERTa with fixed random embeddings. %In order to fairly compare this model without meaningful target embeddings with the models with best performance in ZeroGaze (i.e. FFM and IRL model), we trained the Gazeformer-noDur model with these \textit{fixed random} embeddings under default hyperparameter settings on ZeroGaze task. 
%The results are provided in Table \ref{table:zgrandom_results}.

\setlength{\tabcolsep}{8pt}
\begin{table}[ht!]
\centering

\begin{tabular}{lccc}
\toprule 
Target Embedding & SS$\bm{\uparrow}$ & FED$\bm{\downarrow}$& NSS$\bm{\uparrow}$\\  
% \Xhline{0.75pt}
\midrule 
RoBERTa & \textbf{0.359} & \textbf{2.788} & \textbf{4.671}\\
Fixed Random & 0.336 & 2.873 & 4.524\\
\bottomrule 
\end{tabular}\\
(a)\\[0.2cm]

\setlength{\tabcolsep}{5pt}
\begin{tabular}{llccc}
\toprule 
Category & Embedding & SS$\bm{\uparrow}$ & FED$\bm{\downarrow}$& NSS$\bm{\uparrow}$\\  
% \Xhline{0.75pt}
\midrule 
\multirow{2}{1.4cm}{stop sign} & RoBERTa & 0.430 & 2.256 & 4.559\\
& Random &  0.358 & 2.176 & 4.258\\\hline
\multirow{2}{1.4cm}{clock} & RoBERTa & 0.399 & 2.600 & 2.110\\
& Random &  0.310 & 3.274 & 1.168\\\hline
\multirow{2}{1.4cm}{cup} & RoBERTa & 0.354 & 2.942 & 1.483\\
& Random &  0.357 & 2.724 & 1.547\\
\bottomrule 
\end{tabular}\\
(b)
\caption{ (a) ZeroGaze results for RoBERTa and  fixed random embeddings. (b) Comparison of ZeroGaze results within categories.}
\label{table:zgrandom_results}
%\end{center}
\end{table}
\def\subFigSz{0.32\linewidth}
\begin{figure}[ht!]
\centering
\includegraphics[width=\subFigSz]{assets/lm_ablate/000000194724.jpg} 
   \includegraphics[width=\subFigSz]{assets/lm_ablate/000000463527.jpg}
  \includegraphics[width=\subFigSz]{assets/lm_ablate/000000302806.jpg}
%   \makebox[\subFigSz]{\small{find ``trash can''}}
% \makebox[\subFigSz]{\small{find ``pizza cutter''}}
% \makebox[\subFigSz]{\small{find ``soda can''}}
%(b) Beyond COCO categories
\caption{Gazeformer often confuses a target with semantically similar objects under the ZeroGaze setting, e.g., being distracted by a bottle or bowl when the new target category is a cup.}
%searching for a bottle or container when the new target category is a cup.} %(a) beyond COCO-Search18's category's canonical names, (b) beyond COCO-annotated categories }
\label{fig:distractor}
\end{figure}

Table \ref{table:zgrandom_results}(a) shows that RoBERTa embeddings improve %a considerable effect on 
Gazeformer's performance under the ZeroGaze setting. Interestingly, Table \ref{table:zgrandom_results}(b) shows that language embeddings for different targets produce different benefits, with ``stop sign'' benefiting the most and ``cup'' the least. We speculate that this is due to how target words used in natural language differ in their semantic consistency. For example, the word ``cup'' is used in various contexts, often interchangeably with multiple different synonyms, e.g., drink, bowl, container. Indeed, we find that the model is often confused with other semantically similar objects when searching for a target; this is especially true when the target is a cup (Fig.~\ref{fig:distractor}). Note that even with fixed random embeddings, Gazeformer achieves slightly better performance with respect to the FFM and IRL models. We posit that this is due to the ability of the Gazeformer transformer encoder block to explicitly capture scene context which enables scene exploration - a considerable part of the search process. Nevertheless, semantic cues from the language model embeddings boost the performance of our model even further. 


\subsection{Pixel Regression vs Grid Classification}

In \Tref{table:cls_results}, we demonstrate the impact of regressing the fixation co-ordinate parameters as compared to the more standard method of classifying amongst patches. We replace the regression step of Gazeformer with a classifier MLP %with $hw$ output neurons followed by a softmax layer 
that learns a probability distribution over the $20\times32$ patches (henceforth, called  \textit{Gazeformer-noReg}).

\setlength{\tabcolsep}{8pt}
\begin{table}[ht!]
\centering
\begin{tabular}{lccc}
\toprule 
& SS$\bm{\uparrow}$ & FED$\bm{\downarrow}$& NSS$\bm{\uparrow}$\\  
% \Xhline{0.75pt}
\midrule 
Gazeformer & \textbf{0.504} & \textbf{2.072} & \textbf{8.375}\\
Gazeformer-noReg & 0.477 & 2.158 & 7.545\\
\bottomrule 
\end{tabular}
\caption{Gazeformer performance compared to a model variant where pixel regression is replaced by patch classification (\textit{Gazeformer-noReg}). Results are from GazeTrain setting.}
\label{table:cls_results}
%\end{center}
\end{table}

Table \ref{table:cls_results} shows that directly regressing the fixation locations provides significantly better performance, as hypothesized. %This can be attributed to the fact that regression penalizes closer predicted fixations less than it penalizes further ones. Additionally, the model is also free to fine-tune the location within the patch. 
Also note that even when the model estimates a grid probability distribution (similar to previous methods), the performance is still superior to baselines. This highlights the strength of our core architecture.

\section{Conclusion}

We introduced \textit{ZeroGaze}, a novel attention task aimed at scalability whereby a model must predict the fixation scanpaths for search targets despite having no prior search fixations available for training. We proposed a new multimodal, transformer-based model called \textit{Gazeformer} coupled with a novel scanpath prediction method, which not only showed impressive ZeroGaze results but also scaled well to uncommon and unknown %with impressive results and scales well even when a very specific unpopular 
target categories. Our model also achieved new state-of-the-art %performance in the traditional scanpath prediction problem and target-absent search
scanpath prediction performance in traditional target-present and target-absent search, while having faster inference speeds.
%Gazeformer also predicted fixation duration, a problem that has been relatively neglected in the fixation prediction literature (but see, Chen \etal~\cite{chen2021predicting}, 2021). 
Because Gazeformer can generate fixation locations and durations for an entire scanpath in negligible time, it enables anticipation of \textit{where} and \textit{when} a persons attention will shift, thus making it ideal for time-critical HCI applications. 
We expect that Gazeformer's scalability, effectiveness, and speed will fuel the use of gaze prediction models in HCI applications and products. In future work, %we plan to explore the model's ability to search for targets specified by complete language expressions (object referral~\cite{yu2016modeling}). 
we look forward to extensions of Gazeformer to other visual tasks like free-viewing and VQA. %more gaze prediction models geared towards the ubiquitous use in HCI products. %Our hope is that this new task and model will fuel additional research into the ZeroGaze problem and lessen the need for expensive and limited human data collection for predicting human attention. 
%%%%%%%%% REFERENCES


\myheading{Acknowledgement}. {\small  This project was supported by US National Science Foundation Awards IIS-1763981, IIS-2123920, NSDF DUE-2055406, and the SUNY2020 Infrastructure Transportation Security Center, and a gift from Adobe.}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
