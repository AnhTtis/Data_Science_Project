% !TEX root = main.tex
% \vspace{-6pt}
\section{Methodology}\label{sec:methodology}
%\vspace{-3pt}


\begin{figure}[t]
    \begin{algorithm}[H]
        \small
        \caption{Adaptive Iterative Structured Pruning}
        \begin{algorithmic}[1]
            \STATE \textbf{Input:} An uncompressed network, and the pruning target
            \STATE \textbf{Output:} A pruned network that meets the target
            \STATE {[}Initialize{]} Initialize a network $f(x; M^0 \odot W_{0}^{0})$ with the initial mask $M^0=\left \{ 0,1 \right \}^{\left | W_{0}^{0} \right |}$ %\kaiqi{and initial Threshold $T[0]=0.0$}
            \STATE {[}Save weights{]} Train the network for $k$ epochs, yielding network $f(x; M^0 \odot W_{k}^{0})$, and save weights $W_{k}^{0}$
            \STATE {[}Train to converge{]} Train the network for \kaiqi{$E-k$} epochs to converge,  producing network \kaiqi{$f(x; M^0 \odot W_{E}^{0})$}
            \FOR {pruning round $r$ ($r \geq 1$)}
                \STATE {[}Calculate attention{]} Calculate the attention value of each filter using the attention mapping function $F(\cdot)$
                \STATE {[}Prune{]} From \kaiqi{$W_{E}^{r-1}$}, prune filters  with an attention value less than $T[r]$, producing a mask $M^r$ and a network \kaiqi{$f(x; M^r \odot W_{E}^{r-1})$}
                \STATE {[}Rewind Weights{]} Reset the remaining filters to $W_{k}^{0}$ at epoch $k$, producing network $f(x; M^r \odot W_{k}^{r-1})$
                \STATE {[}Rewind Learning Rate{]} Reset the learning rate schedule to its state from epoch $k$
                \STATE {[}Retrain{]} Retrain the unpruned filters for \kaiqi{$E-k$} epoch to converge, yielding network \kaiqi{$f(x; M^r \odot W_{E}^{r})$}
                \STATE {[}Evaluate{]} Evaluate the retrained network $f(x; M^r \odot W_{E}^{r})$ according to the target
                \STATE {[}Reset Weights{]} If the target is not met, reset the weights to an earlier round
                \STATE {[}Adapt Threshold{]} Calculate the next threshold $T[r+1]$ %and the local Threshold of each convolution layer for the next pruning round
            \ENDFOR
        \end{algorithmic}
        \label{alg:overall_pruning}
    \end{algorithm}
\end{figure}




\begin{figure}[t]
	\begin{subfigure}{0.9\columnwidth}
		\centering
		\includegraphics[width=6cm]{figs/visualization_of_activations.pdf}
		\caption{Filter activation outputs.}
		\label{fig:visualization_of_activations}
	\end{subfigure}
	\begin{subfigure}{0.9\columnwidth}
		\centering
        \includegraphics[width=6cm]{figs/conv.eps}
		\caption{Convolution layer tensors.}
		\label{fig:conv2d}
	\end{subfigure}
	\caption{\kaiqi{(a) Activation outputs of 16 filters, and (b) input and output tensors of a convolution layer}.}
 	% \caption{(a) Activation outputs of 16 filters of a \kaiqi{convolution layer}. (b) Input and output tensors of a \kaiqi{convolution layer (conv2d)}.}
% 	\caption{(a) Activation outputs of 16 filters of a conv2d layer. The first two columns show the original image and the image after data augmentation; and the other columns show the activation outputs of each filter. (b) Input and output tensors of a convolution layer.}
\end{figure}






Algorithm~\ref{alg:overall_pruning} lists the proposed adaptive structured pruning for AAP. We improve LTH-based iterative pruning by proposing activation-based attention pruning (Lines 7 and 8) and adaptive pruning policies (Lines 11--14), to automatically and efficiently generate a pruned model that meets the user's different objectives. 
To represent pruning of weights, \kaiqi{we use a mask $M^r \epsilon \left \{0, 1\right \} ^ {|W^r|}$ for each weight tensor $W_t^r$, where $r$ is the pruning round number and $t$ is the training epoch}. Therefore, the pruned network at the end of training epoch \kaiqi{$E$} is represented by the element-wise product \kaiqi{$M^r \odot W_E^r$}. Lines 3--5 are to train the original model to completion while saving the weights at epoch $k$. Lines 6--15 represent a pruning round. Lines 7 and 8 prune the model (discussed in Section~\ref{sec:filterPruning}). Lines 9 (optional) and 10 perform rewinding. Line 11 retrains the pruned model for the remaining \kaiqi{$E-k$} epochs. Line 12 evaluates the pruned model according to the pruning target. If the target is not met, Line 13 resets the weights to an earlier round. Line 14 calculates the threshold for the next pruning round following the adaptive pruning policy (discussed in Section~\ref{sec:adaptive_pruning})
% \kaiqi{Sec. 3: The first part of this methodology section seems very redundant with Algorithm 1, and it would be more helpful if the information was not repeated but rather complemented.}
%, and calculates the local pruning threshold for each convolution layer (discussed in Section~\ref{sec:layer-aware}).





\subsection{Attention-based Filter Pruning}
\label{sec:filterPruning}


%In comparison, \m{explain why activation values are more effective in finding unimportant filters, and explain its connection to attention}
First, we propose that, compared with the \textit{weight values} of a filter, its \textit{activation values} are more effective indicators of finding unimportant filters to prune.
Activations like ReLu enable non-linear operations, and enable convolutional layers to act as feature detectors.
If an activation value is small, then its corresponding feature detector is not important for prediction tasks. 
On the other hand, some filters, even though their weight values are small, can still produce useful non-zero activation values that are important for learning features during backpropagation.
%
We present a visual motivation in Figure~\ref{fig:visualization_of_activations}. 
The figure shows the activation outputs of 16 filters of a convolution layer on one input image. 
The first image on the left is the original image, and the second image is the input features after data augmentation. 
We observe that some filters extract image features with high activation patterns, e.g., the $6$th and $12$th filters. 
In comparison, the activation outputs of some filters are close to zero, such as the $2$nd, $14$th, and $16$th filters. 
Therefore, from visual inspection, removing filters with weak activation patterns is likely to have a low impact on the final accuracy of the pruned model.




% \kaiqi{Thus the key problem is to design a proper function that can reflect the useful information of the activation feature maps of each filter. NN Slimming~\cite{liu2017learning} measures the number of zero values in the activations from extensive input images and removes the filters with a large portion of zeros in activations. However, this data-driven approach heavily relies on the distribution of the input images. The larger number of images used to calculate Average Percentage of Zeros (APoZ), the more accurate the measurement of NN Slimming. It requires a large number of images (e.g. 50,000 of ImageNet classification task) to maintain a reasonable prediction. Another previous work -- HRank~\cite{lin2020hrank} partially solves this problem by conducting a Singular Value Decomposition (SVD) for activations. It uses 500 images to get reasonable results in ImageNet. Instead, the proposed attention mapping function (will be discussed next) is robust to the input images. No matter what the input image is, the filter that can extract useful image features (e.g., the $6$th and $12$th filters in Figure~\ref{fig:visualization_of_activations}) always has a higher attention value. For example, we empirically observe that only one batch of images, e.g. 128 of ImageNet classification task, are needed to evaluate the importance of filters.}

Thus the key problem is to design a proper function that can reflect the useful information of the activation feature maps of each filter. 
%
% Previous activation-based filter pruning methods, such as NN Slimming~\cite{liu2017learning} and HRank~\cite{lin2020hrank}, address this problem by measuring Average Percentage of Zeros (APoZ) of the activations or conducting a Singular Value Decomposition (SVD) for activations, respectively. However, these data-driven approaches require a large amount of inputs (e.g., 50,000 of ImageNet images for NN Slimming) to achieve a reasonably accurate prediction.
% AP+Coreset~\cite{dubey2018coreset} computes the mean value of activations over all training samples
% Provable Filter Pruning (PFP)~\cite{liebenwein2019provable} computes the sensitivity of activations using a subset from the validation set
%
\kaiqi{Previous activation-based filter pruning methods address this problem using different forms of activations: NN Slimming \cite{liu2017learning} measures Average Percentage of Zeros (APoZ) of the activations; HRank \cite{lin2020hrank} conductes a Singular Value Decomposition (SVD) for activations; AP+Coreset \cite{dubey2018coreset} computes the mean value of activations; Provable Filter Pruning (PFP) \cite{liebenwein2019provable} computes the sensitivity of activations. 
However, these data-driven approaches require a large amount of inputs (e.g., 50,000 of ImageNet images for NN Slimming and all training samples for AP+Coreset) to achieve a reasonably accurate prediction and leads to data-dependent compressed models.
%
Instead, we aim to design a function that is data-independent and robust to inputs: no matter what the input image is, the filters that can extract useful image features should always be maintained.}









Activation-based attention is a good indicator of neurons regarding their ability to capture features \cite{zagoruyko2016paying}. 
%
Attention has been proven to be useful in various tasks, including neural machine translation \cite{bahdanau2014neural}, object localization \cite{oquab2015object}, and knowledge transfer for image classification~\cite{zagoruyko2016paying}. 
\kaiqi{Also, motivated by human attention mechanism theories, attention maps can be obtained by computing a Jacobian of network outputs regarding the inputs \cite{simonyan2013deep}, guided backpropagation~\cite{springenberg2014striving}, or converting the linear classification layer into a convolutional layer \cite{zhou2016learning}.}
% Attention Transfer (AT)~\cite{zagoruyko2016paying} proposed attention as a mechanism of transferring knowledge from one network to another.}
%
However, the effectiveness of using attention as a mechanism for model compression is currently unexplored, and it imposes new challenges, e.g., the attention mapping function defined in Attention Transfer~\cite{zagoruyko2016paying} outputs a flattened 2D matrix representing the ability to capture features of the whole convolution layer, not individual filters in that layer, and cannot be used to prune individual filters.








%\m{explain how we choose inputs for computing activation values, contrast it to the above related works, and provide a pointer to the ablation study}
We address the challenge of effectively identifying insignificant filters in a network with novel designs for the attention mapping function.
%
We start with some notations shown in Figure~\ref{fig:conv2d}.
For the $i$th 2D convolution (conv2d) layer, let $X^i \epsilon R^{n^{i-1}\times h^{i-1}\times w^{i-1}}$ denote the input features, and $F^i_j  \epsilon R^{n^{i-1}\times k^i \times k^i}$ be the $j$th filter, where $h^{i-1}$ and $w^{i-1}$ are the height and width of the input features, respectively, $n^{i-1}$ is the number of input channels, $n^{i}$ is the number of output channels, and $k^i$ is the kernel size of the filter. The activation of the $j$th filter $F^i_j$ after ReLu mapping is therefore denoted by $A^i_j \epsilon R^{h^i \times w^i}$. 
%, where $h_{i}$ and $w_{i}$ are respectively the height and width of the output features. 
%
The proposed attention mapping function takes a 2D activation $A^i_j \epsilon R^{h^i \times w^i}$ of filter $F^i_j$ as input, and outputs a 1D value which will be used as an indicator of the importance of filters. We consider three forms of activation-based attention mapping functions, where $p \geq 1$ and $a_{k,l}^i$ denotes every element of $A^i_j$:
\begin{enumerate} %\vspace{-9pt}
  \item \textbf{Attention Mean} (mean of the \kaiqi{activation} values) %raised to the power of $p$: 
  %\vspace{-3pt}
  
  $F_{mean}(A^i_j)= \frac{1}{h^{i}\times w^{i}}\sum_{k=1}^{h^i}\sum_{l=1}^{w^i} {\left | a_{k,l}^i  \right |}^{p}$; %\vspace{-9pt}
  \item \textbf{Attention Max} (max of the \kaiqi{activation} values) %raised to the power of $p$: 
  %\vspace{-3pt}
  
  $F_{max}(A^i_j)= max_{l=1, h^{i}\times w^{i}} \left | a_{k,l}^i \right |^{p}$; %\vspace{-9pt}
  \item \textbf{Attention Sum} (sum of the \kaiqi{activation} values) %raised to the power of $p$: 
  %\vspace{-3pt}
  
  $F_{sum}(A^i_j)= \sum_{k=1}^{h^i}\sum_{l=1}^{w^i} {\left | a_{k,l}^i \right |}^{p}$. %\vspace{-9pt}
\end{enumerate}
%
%We choose $F_{mean}(A^i_j)$ with $p$ equals to $1$ as the indicator to identify and prune unimportant filters, and our method removes the filters whose attention value is lower than the pruning threshold. See Ablation Study for the choices of attention maps and values of $p$. 
From these three, we choose Attention Mean, $F_{mean}(A^i_j)$ with $p$ equal to $1$ as the indicator to identify and prune unimportant filters. 
%(See Section \ref{sec:ablation} for the choices of attention maps and values of $p$.)
%
Also, in contrast to the related works, our proposed attention mapping function is robust to the inputs, \kaiqi{including real data or arbitrary random vectors}; the attention values are calculated by only one batch of randomly chosen training data. 
%
See Section \ref{sec:ablation} for ablation studies on the choices of attention functions and the effect of data for evaluating attention values.
%
%Our method removes the filters whose attention value is lower than the pruning threshold.



To the best of our knowledge, we are the first to study the effectiveness of using attention theories for model compression tasks such as structured pruning and solve the challenges by designing novel attention mapping functions that are effective for filter pruning.



