% !TEX root = main.tex

%\vspace{-18pt}
\begin{abstract}
%Deploying complex deep learning models on resource-constrained edge devices is challenging.
%because they have substantial compute and memory resource requirements, whereas edge devices' resource budget is limited. 
%To solve this problem, extensive pruning techniques have been proposed for compressing networks. 
Pruning is a promising approach to compress deep learning models in order to deploy them on resource-constrained edge devices.
%
However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware; and they often require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. 
%
To address these limitations, this paper presents Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning approach to automatically generate small, accurate, and hardware-efficient models that meet user objectives. 
%
First, it proposes iterative structured pruning using activation-based attention maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that AAP substantially outperforms the state-of-the-art structured pruning works for a variety of model architectures. 
Our code is at: \url{https://github.com/kaiqi123/Automatic-Attention-Pruning.git}.
%on CIFAR-10 and ImageNet datasets. 
%For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method achieves the largest parameter reduction (79.11\%), outperforming the related works by 22.81\% to 66.07\%, and the largest FLOPs reduction (70.13\%), outperforming the related works by 14.13\% to 26.53\%.
\end{abstract}



% Pruning is a promising approach to compress deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware; and they often require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning approach to automatically generate small, accurate, and hardware-efficient models that meet user objectives. First, it proposes iterative structured pruning using activation-based attention maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that AAP substantially outperforms the state-of-the-art structured pruning works for a variety of model architectures. Our code is at: https://github.com/kaiqi123/Automatic-Attention-Pruning.git.