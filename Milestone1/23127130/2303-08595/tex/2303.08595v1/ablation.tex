%\vspace{-6pt}
\subsection{Ablation Study}
\label{sec:ablation}


% ================================================> important
% \begin{figure}[t]
% 	\centering
% % 	\vspace{-8pt} %may remove
% 	\subfigure[\footnotesize{Effect of Attention Pruning}]{
%         \label{fig:ablation_IAP}
% 		\centering
% 		\includegraphics[width=0.22\textwidth]{figs/parameters_resnet50.eps}
% 	}%
% 	\vspace{-5pt}
% 	\subfigure[\footnotesize{Effect of Adaptive Pruning}]{
%         \label{fig:ablation_adaptive_pruning}
% 		\centering
% 		\includegraphics[width=0.22\textwidth]{figs/adaptive_pruning_new.eps}
% 	}%
% 	\vspace{-7pt}
%     \caption{\footnotesize{(a) Top-1 accuracy of ResNet-50 iteratively pruned by the proposed attention pruning vs. L1-Norm pruning on ImageNet. (b) Accuracy loss and parameter reduction over the pruning rounds as the pruning threshold is adapted following Algorithm 2.}}
%     % \vspace{-12pt}
% \end{figure}

% \begin{figure}[t]
% 	\centering
%  	\vspace{-12pt} %may remove
% 	\subfigure[\footnotesize{Conv1}]{
% 		\centering
% 		\includegraphics[width=0.45\linewidth]{figs/conv1_distribution.eps}
% 	}%		
% 	\vspace{-12pt}
% 	\subfigure[\footnotesize{Group1.Block2.Conv3}]{
% 		\centering
% 		\includegraphics[width=0.45\linewidth]{figs/group1_distribution.eps}
% 	}
% 	\subfigure[\footnotesize{Group2.Block3.Conv3}]{
% 		\centering
% 		\includegraphics[width=0.45\linewidth]{figs/group2_distribution.eps}
% 	}%
% 	\subfigure[\footnotesize{Group3.Block5.Conv3}]{
% 		\centering
% 		\includegraphics[width=0.45\linewidth]{figs/group3_distribution.eps}
% 	}%
% 	\vspace{-12pt}
% 	\caption{\footnotesize{Attention values of filters from 4 convolution layers of ResNet-50 on ImageNet, for 10 batches of randomly chosen inputs.}}
% 	\label{fig:attention_distribution}
% 	\vspace{-12pt}
% \end{figure}
% ================================================> important

% \begin{figure}[t]
% 	\centering
% % 	\vspace{-8pt} %may remove
% 	\begin{subfigure}{0.47\columnwidth}
% 		\centering
% 		% \includegraphics[width=3.5cm]{figs/parameters_resnet50.eps}
%   		\includegraphics[width=3.5cm]{figs/ablation_ILP.eps}
% 		\caption{Effect of Attention Pruning}
% 		\label{fig:ablation_IAP}
% 	\end{subfigure}
% 	\begin{subfigure}{0.47\columnwidth}
% 		\centering
% 		\includegraphics[width=3.5cm]{figs/adaptive_pruning_new.eps}
% 		\caption{Effect of Adaptive Pruning}
% 		\label{fig:ablation_adaptive_pruning}
% 	\end{subfigure}
% 	\vspace{-7pt}
%     \caption{\footnotesize{(a) \kaiqi{Top-1 accuracy of VGG-16 iteratively pruned by proposed attention pruning vs. L1-Norm based pruning on CIFAR-10.} (b) Accuracy loss and parameter reduction over the pruning rounds as the pruning threshold is adapted following Algorithm 2.}}
%     % \vspace{-12pt}
% \end{figure}


\begin{figure*}[t]
	\centering
	\begin{subfigure}{0.65\columnwidth}
		\centering
		% \includegraphics[width=3.5cm]{figs/parameters_resnet50.eps}
  		\includegraphics[width=4.5cm]{figs/ablation_ILP.pdf}
		\caption{Effect of Attention Pruning}
		\label{fig:ablation_IAP}
	\end{subfigure}
	\begin{subfigure}{0.65\columnwidth}
		\centering
		\includegraphics[width=4.5cm]{figs/adaptive_pruning_new.pdf}
		\caption{Effect of Adaptive Pruning}
		\label{fig:ablation_adaptive_pruning}
	\end{subfigure}
 	\begin{subfigure}{0.65\columnwidth}
		\centering
		\includegraphics[width=4.5cm]{figs/ablation_layerwise_sparsity.pdf}
		\caption{Layer-wise Sparsity}
		\label{fig:ablation_layerwise_sparsity}
	\end{subfigure}
    \caption{\footnotesize{(a) \kaiqi{Top-1 accuracy of VGG-16 iteratively pruned by the proposed attention pruning vs. L1-Norm based pruning on CIFAR-10. (b) Accuracy loss and parameter reduction over the pruning rounds as the pruning threshold is adapted following Algorithm~\ref{alg:pruning_policy}. (c) Layer-wise sparsity of a pruned VGG-19 on CIFAR-10.}}}
\end{figure*}


\begin{figure}[t]
	\centering
 	% \vspace{-12pt} %may remove
	\begin{subfigure}{0.47\columnwidth}
		\centering
		\includegraphics[width=4cm]{figs/conv1_distribution.pdf}
		\caption{Conv1}
	\end{subfigure}
	% \vspace{-3pt}
	\begin{subfigure}{0.47\columnwidth}
		\centering
		\includegraphics[width=4cm]{figs/group1_distribution.pdf}
		\caption{Group1.Block2.Conv3}
	\end{subfigure}
	\begin{subfigure}{0.47\columnwidth}
		\centering
		\includegraphics[width=4cm]{figs/group2_distribution.pdf}
		\caption{Group2.Block3.Conv3}
	\end{subfigure}
	\begin{subfigure}{0.47\columnwidth}
		\centering
		\includegraphics[width=4cm]{figs/group3_distribution.pdf}
		\caption{Group3.Block5.Conv3}
	\end{subfigure}
	% \vspace{-6pt}
	\caption{{Attention values of filters from 4 convolution layers of ResNet-50 on ImageNet, given 8 different batches of inputs, including randomly chosen real images and arbitrary random vectors.}}
	\label{fig:attention_distribution}
% 	\vspace{-12pt}
\end{figure}








% \begin{figure}[t]
% % 	\vspace{-20pt}
% 	\centering
% 	\subfigure[Top-1 Test Accuracy]{
% 		\centering
% 		\includegraphics[width=0.45\linewidth]{figs/stability_acc.eps}
% 		\label{fig:ablation_accuracy}
% 	}%
% 	\subfigure[Stability to Pruning]{
% 		\centering
% 		\includegraphics[width=0.45\linewidth]{figs/stability_dis.eps}
% 		\label{fig:ablation_stability}
% 	}%
% 	\vspace{-12pt}
% 	\caption{\footnotesize{\kaiqi{The effect of the rewinding epoch (x-axis) on  (a) Top-1 test accuracy, and (b) pruning stability, for pruned ResNet-50 with a reduction of parameters of 16.26\% on ImageNet.}}}
% \end{figure}

% \begin{figure}[t]
%     % \vspace{-30pt}
% 	\centering
% 	\includegraphics[width=0.45\linewidth]{figs/distribution.pdf}
% % 	\vspace{-30pt}
% 	\caption{The the distribution of the attention values of each convolutional layer of the original ResNet-50 and pruned ResNet-50 with a parameter reduction of 96.31\% on ImageNet.}
% 	\label{fig:distribution}
% \end{figure}



% \begin{table}[t]
% \centering
% % \vspace{-40pt}
% \scriptsize
% \caption{Results of one-shot pruning with different types of attention mapping functions and L1 Norm for VGG-16 on CIFAR-10 dataset, with a sparsity of 42.27\%.} %The Top-1 accuracy of the original model is 93.63\% (5 runs).
% \label{tab:ablation_attention}
% \centering
% \begin{tabular}{cc}
%     \toprule
%     Method               & Acc. ↓ (\%) \\
%     \hline
%     \textbf{Attention Mean (p=1)} & \textbf{0.38}        \\
%     Attention Mean (p=2) & 0.42        \\
%     Attention Mean (p=4) & 0.47        \\
%     Attention Sum (p=1)  & 0.63        \\
%     Attention Max (p=1)  & 0.51        \\
%     L1 Norm              & 0.48        \\
%     \bottomrule
% \end{tabular}
% % \vspace{-15pt}
% \end{table}







\noindent \textbf{The effect of attention-based iterative pruning.}
%
\kaiqi{We fix the percentage of filters that the pruning process removes in each round, which is non-adaptive iterative pruning, and compare using our proposed attention values (termed IAP) with 1) using the conventional weight values, e.g. the L1-Norm of filters (as in Iterative L1-Norm Pruning (ILP)~\cite{renda2020comparing}), and 2) using the L1-Norm of filters divided by the filter size (termed ILP-Mean) to choose which filters to remove.
%
Figure~\ref{fig:ablation_IAP} shows the Top-1 accuracy of VGG-16 with the parameters reduction of 9.92\%, 57.73\% and 69.56\% pruned by IAP, ILP, and ILP-Mean on CIFAR-10.
%
% As shown in Figure~\ref{fig:IAP_ILP}, 
IAP leads to a higher accuracy than the others by 0.1\% to 0.58\%.} %on VGG-16 (CIFAR-10).

% [93.98, 93.26, 93.17], [93.75, 93.16, 92.88], [93.56, 92.98, 92.59]


% We fix the percentage of filters that the pruning process removes in each round, and compare using our proposed attention values vs. using the conventional weight values (as in Iterative L1-Norm Pruning (ILP)) to choose which filters to remove.
% %
% Figure~\ref{fig:ablation_IAP} shows the Top-1 accuracy of ResNet-50 on ImageNet over the pruning rounds. Our attention-based method shows a graceful accuracy degradation as the model is pruned iteratively. In comparison, ILP shows steep accuracy losses when the parameter reduction is above 14.58\%. For example, with 54.73\% of parameter reduction, AAP achieves an accuracy loss of only 5.21\%, whereas ILP’s accuracy loss is 34.37\%.
% acc loss:
% ours: 75.54 - 70.33 = 5.21
% ILP: 75.54 - 41.17 = 34.37



\smallskip
\noindent \textbf{The effect of attention mapping functions.}
%Table~\ref{tab:ablation_attention} shows the Top-1 accuracy loss of the VGG-16 pruned by one-shot pruning using different types of attention mapping functions on CIFAR-10, with a sparsity of 42.27\%.
Using VGG-16 on CIFAR-10 as an example, first, we analyze different types of attention mapping functions (discussed in Section~\ref{sec:filterPruning}), i.e., Attention Mean ($p=1$), Attention Sum ($p=1$), and Attention Max ($p=1$), when used in one-shot attention-based pruning. With a parameter reduction of 57.73\%, Attention Mean leads to the lowest top-1 accuracy loss, lower than Attention Sum and Attention Max by 0.25\% and 0.13\%, respectively. Then, we analyze Attention Mean with different values of $p$. When $p$ is set to $1$, it leads to the lowest accuracy loss: 0.38\% when $p=1$ vs 0.42\% when $p=2$ and 0.47\% when $p=4$. This confirms our choice of Attention Mean with $p=1$ in our evaluation.
% Also, all the forms of Attention Mean outperforms L1-Norm, which validates that the proposed activation-based attention mapping function is more efficient than weight magnitude based methods to evaluate the importance of filters.




\smallskip
\noindent \textbf{The effect of adaptive pruning.}
%
Using ResNet-56 on CIFAR-10 as an example, we show in Figure~\ref{fig:ablation_adaptive_pruning} how the adaptation of the pruning threshold (following Algorithm~\ref{alg:pruning_policy}) affects accuracy loss and parameters reduction over the pruning rounds. 
%We set the pruning target of 1\% accuracy loss when pruning ResNet-56 on CIFAR-10. 
From Round 1 to Round 24, the accuracy loss of the pruned model is lower than the target accuracy loss (1\%), so the algorithm increases the pruning aggressiveness gradually by increasing the threshold. At Round 25, the accuracy loss exceeds the target, so the algorithm rolls back the model weights and the pruning threshold back to Round 24, and restarts the pruning from there more conservatively. The above process repeats until after Round 39, the model size converges, and the algorithm terminates at reducing 88.23\% of parameters.




\smallskip
\noindent \textbf{The effect of Layer-aware Threshold Adjustment.}
%
\kaiqi{AAP considers the importance of each layer by adjusting differentiated layer-specific thresholds. % (see Section 3.1). 
% A layer that contributes more to the model’s size or FLOPs is more likely to have redundant filters to prune without affecting the model’s accuracy. 
% Different from Siu et al., we only keep the last full-connected layer dense. 
Figure~\ref{fig:ablation_layerwise_sparsity} shows the layer-wise sparsity of a pruned VGG-19 with a total parameters reduction of 85.99\% on CIFAR-10. 
More filters are pruned from higher layers than lower layers.} % with 14.01\% parameters remaining
% with a total parameters reduction of 85.99\%
 % and an accuracy improvement of 0.26\% on CIFAR-10


 
\smallskip
% \noindent \textbf{The effect of randomly choosing training data for evaluating attention values}.
\noindent \textbf{The effect of inputs for evaluating attention values}.
%
The importance of each filter is evaluated by its attention value, which is calculated by one batch of randomly chosen training data after each pruning round (Line 7 in Algorithm~\ref{alg:overall_pruning}). The attention values are \textit{not sensitive} to the inputs because the model is converged from the training phase.
%
\kaiqi{As an example, Figure~\ref{fig:attention_distribution} shows the attention value of each filter of 4 convolution layers of ResNet-50 on ImageNet, given eight different batches of inputs, including randomly chosen real images and arbitrary random vectors.} 
The attention value of each filter is consistent across different batches. 

% As an example, Figure~\ref{fig:attention_distribution} shows the attention value of each filter of 4 convolution layers of ResNet-50 on ImageNet, given 10 different batches of randomly chosen inputs. The attention value of each filter is consistent across different batches. 



% \kaiqi{
% \smallskip
% \noindent \textbf{The Effect of Rewinding Epoch}\label{sec:appendix_rewinding}
% %
% We define \emph{stability to pruning} as the L2 distance between the weights of the pruned and the original model. 
% %
% As shown in Figure~\ref{fig:ablation_accuracy} and \ref{fig:ablation_stability}, we validate the observations that for deep networks, rewinding to very early stages is sub-optimal as the network has not learned considerably and they are not stable to pruning (L2 distance is small); and rewinding to very late training stages is also sub-optimal because there is not enough time to retrain. 
% %
% Rewinding to 75\%-90\% of training time leads to good accuracy.}


% In order to understand how the rewinding impacts accuracy of the pruned models, we analyze \emph{stability to pruning}, which is defined as the L2 distance between the masked weights of the pruned network and the original network at the end of training.
% %
% We validate the observations that for deep networks, rewinding to very early stages is sub-optimal as the network has not learned considerably by then; and rewinding to very late training stages is also sub-optimal because there is not enough time to retrain. Specifically,
% %
% Figure~\ref{fig:ablation_accuracy} shows the Top-1 test accuracy of the pruned ResNet-50 with 83.74\% remaining parameters when learning rate is rewound to different epochs, and Figure~\ref{fig:ablation_stability} shows the stability values at the corresponding rewinding epochs. We observe that there is a region, 65 to 80 epochs, where the resulting accuracy is high. We find that L2 distance closely follows this pattern, showing large distance for early training epochs and small distance for later training epochs. 
% %
% Our findings show that rewinding to 75\%-90\% of training time leads to good accuracy.