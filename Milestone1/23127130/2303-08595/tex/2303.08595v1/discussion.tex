% !TEX root = main.tex
%\vspace{-6pt}
\subsection{Discussions}\label{sec:discussion}








\noindent \textbf{Comparison to related works.}
%
The above results validate that 1) the proposed \textit{activation-based attention pruning} is more effective than weight-magnitude-based pruning (e.g.,  EigenDamage \cite{wang2019eigendamage}, ILP \cite{renda2020comparing}, NSP \cite{zhuang2020neuron}) in finding unimportant filters;
% , as discussed in Section~\ref{sec:filterPruning};
%
% For example, on ResNet-56 with CIFAR-10,  with 1\% of accuracy loss, the amount of parameter reduction achieved by our method is higher than that of ILP by 34.51\%; and with 70\% FLOPs reduction, the accuracy achieved by our method is higher than PPR by 0.87\%. On VGG-19 with Tiny-ImageNet, our method achieves 10.71\% higher parameter reduction, and 12.76\% higher FLOPs reduction than EigenDamage.
%
% The results also validate that 
%
2) the proposed \textit{activation-based attention pruning} is better than other activation-based pruning techniques that are based on different forms of activation feature maps, such as NN Slimming~\cite{liu2017learning}, HRank~\cite{lin2020hrank}, \kaiqi{AP+Coreset~\cite{dubey2018coreset} and PFP~\cite{liebenwein2019provable}};
%
% For example, for the same level of parameter reduction (about 70\%), comparied with HRank, our method achieves a lower accuracy loss, 2.98\%, 0.69\%, and 0.08\%  lower  on ResNet-56 (CIFAR-10), VGG-16 (CIFAR-10), and ResNet-50 (ImageNet), respectively; And compared with NN Slimming, our method achieves significantly lower accuracy loss by 11\% on VGG-19  with Tiny-ImageNet.
%
% Finally, the results confirm that 
%
and 3) the proposed \textit{adaptive pruning} can achieve better results than other automatic pruning methods (e.g., AMC~\cite{he2018amc} and GAL~\cite{lin2019towards}). 
% , DCP~\cite{zhuang2018discrimination}, , DeepHoyer (DH)~\cite{yang2019learning}

%
% For example, when targeting at about 55\% of FLOPs reduction on ResNet-56, our method achieves 1.53\% higher accuracy than AMC; and when targeting at about 60\% of parameters reduction on ResNet-50, our method achieves 0.84\% higher accuracy than AMC.
%
% Our method directly targets unimportant filters in the model, whereas AMC has to use reinforcement learning to explore the whole network.




% The baseline Top-1 accuracy of ResNet-50 on ImageNet is 74.97\% (5 runs). 
% &                                  &                       & \textbf{Ours} & \textbf{0.48}  & \textbf{29.42} & \textbf{-}     \\ \cline{2-7} 
% &                                  &                       & \textbf{Ours} & \textbf{1.09}  & \textbf{37.17} & \textbf{-}     \\ \cline{3-7} %1.09
% &                                  &                       & \textbf{Ours} & \textbf{0.57}  & \textbf{-}     & \textbf{31.88} \\ \cline{1-7} %31.88



\smallskip
\noindent \textbf{Inference speedup.}
%
The reduction in model complexity in FLOPs that AAP achieves does translate to real speedup in model inference. We conducted inference experiments using PyTorch framework on Raspberry Pi 3B+, a widely used \kaiqi{Internet-of-Things (IoT)} platform. 
%The OS is Raspberry Pi OS (64 bit), CPU is 1.4 GHz quad-core Cortex A53, and the memory is 1 GB.
%
%As shown in Table~\ref{tab:inference}, we measure the latency and throughput of pruned models, including ResNet-56 and VGG-16 with CIFAR-10, and ResNet-50 with TinyImageNet.  
Table~\ref{tab:inference} shows our method significantly improves the inference speed. 
%On CIFAR-10, without accuracy drop, the pruned ResNet-56 and VGG-16 achieves the speedup of 1.49$\times$ and 2.13$\times$, respectively; And with 0.59\% accuracy drop,  the pruned VGG-16 achieves the throughput speedup of 3.6$\times$. On Tiny-ImageNet, the pruned ResNet-50 achieves the speedup of 1.01$\times$ and 1.49$\times$, respectively, with a higher Top-1 accuracy than the original ResNet-50, 3.92\% and 1.62\% higher, respectively.


\smallskip
\noindent \textbf{Extension to multi-objective optimization.}
%
Our method can be readily extended to support \textit{multiple objectives} and \textit{multiple constraints} for optimizing the pruned model in terms of accuracy, size, and/or speed simultaneously.
%multi-objective optimization by re-defining the ``acceptable'' scenario (Line 8 of Algorithm 2, 4, and 5) and changing the input (Line 1). 
%
% For example, considering both accuracy and speed, the objective can be to guarantee the accuracy loss \textit{and} FLOPs reduction while minimizing the model size. The input of the algorithm should be Target Accuracy Loss and Target FLOPs Reduction. After each round of pruning, if the model accuracy loss is below than the target accuracy loss \textit{and} reduced percentage of FLOPs is below than Target FLOPs Reduction, it is considered ``acceptable''. 
%
Table~\ref{tab:multi_optimization} shows two examples from pruning VGG-19 with CIFAR-10. In the first example, the objective is to minimize the accuracy loss under the constraints of 80\% reduction in \textit{both} FLOPs and parameters. In the second example, the objective is to maximize \textit{both} the FLOPs reduction and parameters reduction given no more than 1\% accuracy loss. 

%In this case, our method reduces 89.11\% of the parameters and 62.51\% of FLOPs while losing 0.15\% accuracy.





% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
% \vspace{-10pt}
\centering
\scriptsize
\caption{\footnotesize{Inference speedup on Raspberry Pi. \kaiqi{For each type of model, the two rows are two pruned models from the same uncompressed model with different levels of parameters reduction.}}}
% \caption{Inference speedup on Raspberry Pi. \kaiqi{For each type of model, the two rows are two pruned models from the same uncompressed model with different levels of parameter reduction.}}
% 10 runs are done for each number.
%The throughput of the original ResNet-56 (CIFAR-10), VGG-16 (CIFAR-10), and ResNet-50 (Tiny-ImageNet) are 17.63fps, 1.77fps, and 1.74fps, respectively.}} %Top-1 accuracy is reported.
\vspace{-10pt}
\label{tab:inference}
\begin{tabular}{lllll}
\toprule
Model and Dataset                                                                              & Target & Acc. ↓ (\%) & FLOPs. ↓ (\%) & Speedup \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}ResNet-56\\ (CIFAR-10)\end{tabular}}      & \multirow{2}{*}{0\%}                                      & -0.10        & 33.77         & 1.20$\times$                                                          \\
                                                                                     &                                                         & -0.09       & 56.33         & 1.49$\times$                                                         \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}VGG-16 \\ (CIFAR-10)\end{tabular}}        & \multirow{2}{*}{1\%}                                      & -0.17       & 59.51         & 2.13$\times$                                                         \\
                                                                                     &                                                         & 0.59        & 73.89         & 3.60$\times$                                                          \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}ResNet-50\\ (Tiny-ImageNet)\end{tabular}} & \multirow{2}{*}{5\%}                                      & -3.92       & 35.06         & 1.01$\times$                                                         \\
                                                                                     &                                                         & -1.62       & 58.16         & 1.49$\times$                                                         \\ 
\bottomrule
\end{tabular}
\end{table}




% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
% \vspace{-5pt}
\centering
\scriptsize
\caption{\footnotesize{Multi-objective optimization on VGG-19 (CIFAR-10).}}
% \caption{Multi-objective optimization on VGG-19 with CIFAR-10.}
\vspace{-10pt}
\label{tab:multi_optimization}
% \begin{tabular}{cp{1.7cm}p{0.4cm}p{0.5cm}p{0.5cm}}
\begin{tabular}{lp{2.05cm}p{0.3cm}p{0.5cm}p{0.5cm}}
% \begin{tabular}{ccccc}
\toprule
Optimization Objectives                 & Constraints                            & \begin{tabular}[c]{@{}l@{}}Acc. \\ ↓ (\%)\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Params. \\ ↓ (\%)\end{tabular}         & \begin{tabular}[c]{@{}l@{}}FLOPs \\ ↓ (\%)\end{tabular}          \\ \hline
\multirow{2}{*}{Minimize Accuracy Loss} & Params.  ↓  \textgreater 80\%     & \multirow{2}{*}{1.20}  & \multirow{2}{*}{83.11} & \multirow{2}{*}{80.60}  \\
                                        & and FLOPs  ↓  \textgreater 80\%       &                       &                        &                        \\ \hline
Maximize FLOP Reduction            & \multirow{2}{*}{Acc. ↓ \textless 1\%} & \multirow{2}{*}{0.15} & \multirow{2}{*}{89.11} & \multirow{2}{*}{62.51} \\
and Params. Reduction           &                                       &                       &                        &                        \\ 
\bottomrule
\end{tabular}
% \vspace{-10pt}
\end{table}


