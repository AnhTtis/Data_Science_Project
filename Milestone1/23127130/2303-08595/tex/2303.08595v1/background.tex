\section{Background and Related Works}
\label{sec:background}





\noindent{\textbf{Unstructured Pruning vs. Structured Pruning.}} 
%
Unstructured pruning (e.g.,~\cite{lecun1990optimal,han2015deep,molchanov2017variational}) prunes individual elements in the weight tensors of a model. It has less impact on model accuracy, compared to structured pruning, because it is finer-grained, but unstructured pruned models are hard to accelerate on commodity hardware.
%
Structured pruning is a coarser-grained approach that prunes entire regular regions of the weight tensors of a model. 
%
It is more difficult to prune a model without causing accuracy loss using structured pruning, because by removing entire regions, it might remove weight elements that are important to the final accuracy. However, structured pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries~\cite{he2018amc}.







 
\smallskip
\boldhdr{One Shot Pruning vs. Iterative Pruning} 
%
One-shot pruning prunes a pre-trained model and then retrains it once, whereas iterative pruning prunes and retrains the model in multiple rounds. 
%
Iterative pruning generally achieves much better performance than one-shot pruning because multiple retraining phases help recover the accuracy lost during pruning.
In particular, recent works based on the Lottery Ticket Hypothesis (LTH) have achieved great results in creating smaller and more accurate models through iterative pruning with rewinding~\cite{frankle2018lottery}. LTH posits that a dense network has a sub-network, termed as a ``winning ticket'', which can achieve an accuracy comparable to the original network. 
%
However, existing LTH-based works consider only unstructured pruning, e.g., Iterative Magnitude Pruning (IMP)~\cite{frankle2018lottery, frankle2019stabilizing} and \kaiqi{Synflow~\cite{tanaka2020pruning}}, which, as discussed above, are hardware-inefficient. 
% \kaiqi{Will add add a comprehensive discussion of the LTH related works.}







\smallskip
\boldhdr{Automatic Pruning} 
For pruning to be useful in practice, it is important to automatically meet the pruning objectives for diverse machine learning applications and devices. Most pruning methods require users to explore and tune multiple hyper-parameters, e.g., with LTH-based pruning, users need to determine how many parameters to prune in each round. Manual tuning is time-consuming and often leads to sub-optimal results. 
%
Some works use learning-based methods to find smaller models in a Neural Architecture Search (NAS) approach: AMC \cite{he2018amc}, NAS \cite{zoph2018learning}, NT \cite{caireinforcement}, and N2N \cite{ashok2017n2n} use reinforcement learning, and GAL \cite{lin2019towards} uses adversarial learning.
%
But these methods have to explore a large search space of all available layer-wise sparsity, which is time consuming when neural networks are large and datasets are complex.




Therefore, there is a great need for an automatic, iterative, structured pruning solution that can automatically and efficiently generate small, accurate, and hardware-efficient models.
The challenges are three-fold. 




First, \textit{how to effectively identify the insignificant parameters in a model to prune?} Existing works have explored different mechanisms, e.g., L2-Norm in \kaiqi{Soft Filter Pruning (SFP) \cite{he2018soft}, Soft Channel Pruning (SCP) \cite{kang2020operation} and EagleEye \cite{li2020eagleeye}}, geometric median in FPGM \cite{he2019filter}, Hessian in \kaiqi{EigenDamage \cite{wang2019eigendamage}}, Empirical Sensitivity in \kaiqi{Provable Filter Pruning (PFP)} \cite{liebenwein2019provable}, adversarial knockoff features in SCOP \cite{tang2020scop}, polarization regularizer in \kaiqi{Neuron-level Structured Pruning (NSP) \cite{zhuang2020neuron}}, LASSO regression in \kaiqi{Channel Pruning (CP) \cite{he2017channel}}, and other information considering the relationship between neighboring layers (\kaiqi{Gate Batch Normalization (GBN) \cite{you2019gate}}, Sparse Structure Selection (SSS) \cite{huang2018data}, Hinge \cite{li2020group}, \kaiqi{Pruning From Scratch (PFS) \cite{wang2020pruning} and Stripe-Wise Pruning (SWP) \cite{meng2020pruning}}). 
%
In comparison, \textit{activation-based attention}, proposed in this paper, can more effectively capture the importance of filters, and pruning based on attention values can produce much better models, as quantitatively shown in our evaluation (Section~\ref{sec:evaluation}).
% \kaiqi{We will add a comprehensive discussion in the recent works about activation-based pruning.  - In Sec.2 Pg.2 col.2, some of the related papers are simply listed, with acronyms. This is not very useful for the reader. I would suggest having at least of brief description of each of these methods and how they are related to the proposed method. I would also suggest avoiding the use of acronyms without explanation unless the acronym is well known.}
%values not only capture the features of inputs but also contain the information of convolution layers that act as feature detectors for prediction tasks.




Second, \textit{how to design an effective iterative pruning process to recover the accuracy loss caused by structured pruning?} 
LTH-based iterative pruning is a promising approach, but it has only been shown to work with unstructured pruning such as IMP. Its counterpart in structured pruning---Iterative L1-norm-based pruning (ILP)~\cite{renda2020comparing}, which removes filters based on their L1-norm values, cannot effectively prune a model while maintaining its accuracy. For example, ILP can prune ResNet-50 by at most 11.5\% of parameters when the maximum accuracy loss is limited to 1\% on ImageNet. So directly applying iterative pruning with existing weight-magnitude-based structured pruning methods does not produce accurate pruned models. 
%
This paper proposes a novel \textit{LTH-based iterative, structured pruning} solution using attentions, and it significantly outperforms ILP and other related structured pruning works that involve an iterative process (GDP~\cite{guo2021gdp}, \kaiqi{ACTD~\cite{wang2021accelerate}, Quantization and Pruning (QP)~\cite{paupamah2020quantisation}, IMP-Refill and IMP-Regroup~\cite{chen2022coarsening})}.




The third challenge is \textit{how to automate the pruning process so it does not require any human intervention?} 
%
The existing structured pruning works all require difficult hand-tuning of many hyper-parameters,
e.g., DCP \cite{zhuang2018discrimination} and MDP \cite{guo2020multi} need multiple hyper-parameters to balance the original task-specific loss and the additional pruning loss; VCNNP \cite{zhao2019variational} requires careful settings of $\tau$ and $\theta$ to decide which filters to prune; DMC \cite{gao2020discrete} and \kaiqi{DeepHoyer \cite{yang2019learning}} require parameters to decide the regularization strength with different settings for different datasets and models.
%
To address this challenge, this paper proposes a fully automated pruning solution that can automatically generate pruned models that meet users' diverse model accuracy, size, and speed requirements.



% =========================> Summary of related works
% GDP~\cite{guo2021gdp}: GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization
% SCOP~\cite{tang2020scop}: knockoff feature, Redundant filters can be discovered in the adversarial process of different features. Through
% PPR~\cite{zhuang2020neuron}: Neuron-level Structured Pruning using Polarization Regularizer
% RFR~\cite{he2017channel}: remove, same with CP
% CP~\cite{he2017channel}: LASSO regression
% GD~\cite{you2019gate}: If we prune the filters of each layer independently, it may result in the misalignment of feature maps in the shortcut connection. To solve the misalignment problem, we propose the Group Pruning: we assign the GBNs connected by the pure shortcut connections to the same group.
% SFP~\cite{he2018soft}: We use the `p-norm to evaluate the importance of each filter as Eq. (2)
% FPGM~\cite{he2019filter}: Filter pruning via geometric median for deep convolutional neural networks acceleration
% SCP~\cite{kang2020operation}: Similar to a normal filter pruning method, the filters are firstly ranked according to their L1-norm and the rl of the least important filters are trimmed off permanently. 
% Hinge~\cite{li2020group}: Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression: By simply changing the way the sparsity regularization is enforced, filter pruning and lowrank decomposition can be derived accordingly. This provides another flexible choice for network compression because the techniques complement each other. For example, in popular network architectures with shortcut connections (e.g. ResNet), filter pruning cannot deal with the last convolutional layer in a ResBlock while the low-rank decomposition methods can
% EigenDamage (ED)~\cite{wang2019eigendamage}: We use the Fisher matrix to approximate the Hessian. In the following, we briefly discuss the relationship between these matrices, 4.1. Approximating the Hessian with the Fisher Matrix
% DMC~\cite{gao2020discrete}: Discrete Model Compression With Resource Constraint for Deep Neural Networks. Although directly optimizing discrete variables is a complex non-smooth, non-convex and NP-hard problem, our optimization method can circumvent these difficulties by using the straight-through estimator. λ decides the regularization strength in our method. We choose λ = 4 in all CIFAR-10 experiments and λ = 8 for all ImageNet experiments.
% PFP~\cite{liebenwein2019provable}: Provable filter pruning for efficient neural networks. Our algorithm only requires two inputs in practice: the desired pruning ratio (PR) and failure probability δ ∈ (0, 1), since the number of samples in each layer is automatically assigned by our allocation procedure described in Sec. 3 
% PFS~\cite{wang2020pruning}: Pruning from scratch. Pruned Structure Similarity First, we compare the structure similarity between different pruned models. For each pruned model, we calculate the pruning ratio of each layer, i.e., the number of remaining channels divided by the number of original channels
% MDP~\cite{guo2020multi}: Multi-dimensional pruning: A unified framework for model compression. Lc is the standard cross-entropy loss. Lst is the penalty to control the computational complexity, with which we expect the importance scores of most branches are close to zero so that most branches will be removed in the pruning process. Lgate is the l1-norm based regularizer that enforces the gate values g (l) i,k from most branches and channels to be close to zero so that these channels can be safely removed in the pruning process
% ATD~\cite{wang2021accelerate}: Accelerate CNNs from three dimensions: a comprehensive pruning framework, we propose two approaches to lower the cost: 1) specializing the polynomial to ensure an accurate regression even with less training data; 2) employing iterative pruning and fine-tuning to collect the data faster
% DeepHoyer (DH)~\cite{yang2019learning}: DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures. Here, α and β are pre-selected weight decay parameters for the regularizer
% *************** Maybe useful 
%Pruning techniques can be broadly categorized into two classes, namely, unstructured pruning and structured pruning. 
% PFS: Therefore, we propose a novel network pruning pipeline which allows pruning from scratch.
% GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization
% MDP: Multi-dimensional pruning: A unified framework for model compression
% ATD: Accelerate CNNs from three dimensions: a comprehensive pruning framework
% L1-norm~\cite{li2016pruning}, average percentage of zero~\cite{molchanov2016pruning}, and other information considering the relationship between neighboring layers~\cite{theis2018faster, lee2018snip}. 
%Most of the existing structured pruning methods use certain rule-based heuristics to , such as 
% =========================> end of summary




