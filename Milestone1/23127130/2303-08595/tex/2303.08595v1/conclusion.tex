% !TEX root = main.tex
%\vspace{-6pt}
\section{Conclusions}\label{conclusion}

%\m{Try to provide some insights, not just a summary. What have we learned? What do you want the readers to remember?}
%\m{this is still just a summary, not insights.}

% This paper proposes an activation-based, adaptive structured pruning approach to automatically and efficiently generate a pruned small model. In order to meet diverse pruning objectives for accuracy-critical, memory-constrained, and latency-critical tasks, we propose three adaptive pruning policies to automatically provide the most resource-efficient model with the given acceptable accuracy loss, and the most accurate model within a given memory footprint or computational intensity. Our results outperforms existing structured pruning approaches with a large margin in all cases. The models pruned by the proposed method produce a new baseline for the future work on structurally pruned models. 


This paper proposes Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning solution to automatically and efficiently generate small, accurate, and hardware-efficient models that meet diverse user requirements. We show that activation-based attention is a more precise indicator for identifying unimportant filters to prune than the commonly used weight magnitude value. We also offer an effective way to perform structured pruning in an adaptive process and find small and accurate sub-networks that are at the same time hardware efficient. Finally, we argue that automatic pruning is essential for pruning to be useful in practice, and propose an adaptive method that can automatically meet diverse user objectives in terms of model accuracy, size, and inference speed but without user intervention.
%
Our results confirm that our solution outperforms existing structured pruning approaches by a large margin.

%Also, the proposed activation-based attention pruning can be combined with various pruning frameworks, such as one-shot pruning, iterative pruning or the proposed adaptive pruning, to further improve the pruning effectiveness. Furthermore, the proposed adaptive iterative pruning has potential positive impact on the real-world applications since it automatically optimizes the pruning process to meet different user objectives, for accuracy-critical, memory-constrained, and latency-critical tasks. Our results show that the proposed method outperforms existing structured pruning approaches with a large margin. The models pruned by the proposed method produce a new baseline for the future work on structurally pruned models.

% For example, on ResNet-56 with CIFAR-10, without accuracy drop, our method achieves the largest parameter reduction (63.11\%), outperforming the related works by 6.81\% to 50.07\%, and the largest FLOPs reduction (63.61\%), outperforming the related works by 7.61\% to 20.01\%. In addition, our method enables a pruned ResNet-56 reaching 0.16\% higher accuracy than the original model with only 30\% of the original's parameters. On ResNet-50 on ImageNet, for the same level of accuracy loss, our method reduces significantly more parameters (7.48\% to 28.48\% higher than the related works) and more FLOPs (about 17\% higher than the related works). 

% In this paper, we investigate iterative structured pruning with rewinding. Structural pruning removes entire filters from  weight tensors, resulting in pruned models that can be easily accelerated using off-the-shelf libraries. We introduce two activation-based iteration structured pruning techniques---Iterative Activation-based Pruning (IAP) and Adaptive Iterative Activation-based Pruning (AIAP) to aggressively prune the models. We observe that IAP and AIAP can find winning lottery tickets, i.e., models that are smaller and more accurate than the original model. Our methods also substantially outperform weight-based Iterative L1-norm Pruning (ILP) on a variety of architectures, e.g., LeNet-300-100 (MNIST), LeNet-5 (CIFAR-10), and ResNet-50 (ImageNet). For the same memory footprint, IAP and AIAP achieve much higher accuracy (e.g., 30\% higher Top-1 accuracy for pruned ResNet-50 models with 45.27\% remaining parameters); for the same accuracy loss, they achieve much higher compression and faster inference speed (e.g., 1.95$\times$ higher compression and 1.95$\times$ speedup for pruned LeNet-300-100 models with no accuracy loss).

%For example, "for the same memory footprint (X), IAP and AIAP can achieve X higher accuracy; and for the same accuracy loss (1\%), they can achieve Z higher compression ratio."


%for 1\% accuracy drop, IAP and AIAP achieve 7.75$\times$ and 15.88$\times$ compression on Lenet-5, and 1.25$\times$ and 1.71$\times$ compression on ResNet-50, respectively, while ILP achieves 4.77$\times$ and 1.13$\times$, respectively. Additionally, IAP and AIAP reduce the compute flops by a factor of 1.05$\times$ and 1.33 compared to ILP, respectively, on ResNet-50 with just 1\% accuracy drop.
