% !TEX root = main.tex
%\vspace{-12pt}
\section{Introduction}\label{sec:introduction}
%\vspace{-6pt}

Deep neural networks (DNNs) have substantial computational and memory requirements. As the use of deep learning grows rapidly on a wide variety of Internet of Things and devices, the mismatch between resource-hungry DNNs and resource-constrained devices also becomes increasingly severe.
%
Pruning is a promising approach to identify and remove the parameters that do not contribute significantly to the accuracy of a DNN.
%
Recent works based on the Lottery Ticket Hypothesis (LTH) have achieved great results in creating smaller and more accurate models (dubbed as ``winning tickets'') through iterative pruning with rewinding~\cite{frankle2018lottery}. 
%
However, LTH has only been shown to work with unstructured pruning which, unfortunately, leads to models with low sparsity and difficult to accelerate on commodity hardware; e.g., directly applying NVIDIA cuSPARSE on unstructured pruned models can lead to a 60$\times$ slowdown compared to dense kernels on GPUs~\cite{deftnn}.
%
Moreover, most pruning methods require users to explore and adjust multiple hyper-parameters, which is time-consuming and often leads to sub-optimal results; e.g., with LTH-based iterative pruning, users need to determine how many parameters to prune in each pruning round. 
%Tuning the pruning process is time consuming and often leads to sub-optimal results.
%LTH posits that a dense randomly initialized network has a sub-network, termed as a \textit{winning ticket}, which can achieve an accuracy comparable to the original network. At the beginning of each pruning round, it rewinds the weights and/or learning rate of the sub-network to some early epoch of the training phase of the original model to reduce the distance between the sub-network and original model and increase the change of finding the winning ticket.



% We propose an \textit{activation-based}, \textit{adaptive}, \textit{iterative structured pruning} framework to find the optimal models that are at the same time hardware efficient and to automatically meet the users' model accuracy, size, and speed requirements. 
We propose Automatic Attention Pruning (AAP), an \textit{adaptive}, \textit{attention-based}, \textit{structured pruning} solution to automatically generate small, accurate, and hardware-efficient models that meet users' accuracy, size, and speed requirements.
%
We improve the LTH-based iterative pruning framework by proposing two methods.
%
First, we propose a novel attention pruning method to identify and remove unimportant filters. Specifically, we properly define an attention mapping function that takes the 2D activation feature map of a filter as input and outputs a 1D value used to indicate the importance of the filter. This approach is more effective than weight-value-based filter pruning~\cite{renda2020comparing, zhuang2020neuron, wang2019eigendamage} because activation-based attention values not only capture the features of inputs but also contain the information of convolution layers that act as feature detectors for prediction tasks. Also, it is better than previous activation-based filter pruning methods~\cite{lin2020hrank, liu2017learning} since the accuracy of its measurement does not depend on the amount of inputs. 
%\m{maybe expand on the overall strategy a bit} \kaiqi{The proposed method is automatic, i.e., users do not have to figure out how to configure the pruning process, and efficient, i.e., the pruning process should produce the user-desired model as quickly as possible.}
% Our approach to pruning is to automatically and efficiently generate a pruned model that meets the users' different objectives. Automatic pruning means that users do not have to figure out how to configure the pruning process. Efficient pruning means that the pruning process should produce the user-desired model as quickly as possible. 
% First, we properly define the attention based on activation values, i.e., the intermediate output tensors after the non-linear activation, to identify unimportant filters. \m{shouldn't the first contribution be "we propose activation-based filter pruning"? also mention we propose a proper way to do iterative structured pruning.} This approach tends to be more effective than weight-value based filter pruning because activations not only capture features of training dataset, but also contain the information of convolution layers that act as feature detectors for prediction tasks.
% in an LTH-based iterative pruning (with rewinding) process
%We then integrate this attention-based method into the proposed adaptive pruning framework to prune the filters in each round and find the ``winning ticket'' that is small, accurate, and hardware-efficient.
%the LTH-based iterative pruning framework to prune the filters in each round and find the winning ticket that is small, accurate, and hardware-efficient.
%Then, we propose a proper way to do iterative structured pruning by iteratively removing the filters where its attention output is below than an adaptive threshold. 
%\kaiqi{Also, the activation-based attention pruning prunes different number of filters from each layer in a parameters/FLOPs-aware mechanism. It prunes the filter where its attention output is below than the threshold of that layer. The threshold of each layer is determined by the adaptive global threshold and the redundant factor, which is defined as the number of remaining parameters/FLOPs in this layer divided by the remaining parameters/FLOPs in the entire model. This approach tends to be better than pruning a fixed percentage of filters in each layer or setting a uniform threshold for all layers, since the layer that retains more filters than the other layers, it is more likely to have more redundant filters to prune.}  





Second, we propose an adaptive pruning method that automatically optimizes the pruning process according to different user objectives. For latency-sensitive scenarios like interactive virtual assistants, we propose FLOPs-guaranteed pruning to achieve the best accuracy with the acceptable inference speed; for memory-limited environments like embedded systems, we propose model-size-guaranteed pruning to achieve the best accuracy and fit the memory constraint; for accuracy-critical applications such as those on self-driving cars, we propose accuracy-guaranteed pruning to create the most resource-efficient model with the acceptable accuracy loss. Given the target, our method adaptively controls the pruning aggressiveness by adjusting the global threshold used to prune filters.
%
Moreover, it recognizes the difference in each layer's contribution to the model's size and computational complexity and uses a layer-wise threshold, calculated by dividing each layer's remaining parameters or FLOPs by the entire model's remaining parameters or FLOPs, to prune each layer with a differentiated level of aggressiveness.




The proposed AAP outperforms the related works significantly in all cases targeting accuracy loss, parameters reduction, and FLOPs reduction for a variety of model architectures. 
For example, on ResNet-56 with CIFAR-10, without accuracy drop, AAP achieves the largest parameters reduction (79.11\%), outperforming the related works by 22.81\% to 66.07\%, and the largest FLOPs reduction (70.13\%), outperforming the related works by 14.13\% to 26.53\%. 
On ResNet-50 with ImageNet, for the same level of parameters and FLOPs reduction, AAP achieves the smallest accuracy loss, lower than the related works by 0.08\% to 2.61\%; 
and for the same level of accuracy loss, AAP reduces significantly more parameters (6.45\% to 29.61\% higher than the related works) and more FLOPs (0.82\% to 17.2\% higher than the related works).
%In addition, our method enables a pruned model to reach 0.6\% or 1.06\% higher accuracy than the original model but with only 28.43\% or 46.11\% of the original's parameters. 



In summary, our main contributions are: 1) a novel iterative, structured pruning approach for finding the ``winning ticket'' models that are hardware efficient; 2) a new attention-based mechanism for accurately identifying unimportant filters for pruning, which is much more effective than existing methods; and 3) an adaptive pruning method that can automatically optimize the pruning process according to diverse real-world scenarios.