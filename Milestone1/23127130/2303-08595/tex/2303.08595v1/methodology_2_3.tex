\subsection{Adaptive Iterative Pruning}\label{sec:adaptive_pruning}



\begin{figure}[t]
% \vspace{-12pt}
% \begin{algorithm}[t]
    \begin{algorithm}[H]
        \centering
        \small
        \caption{Accuracy-guaranteed Adaptive Pruning}
        \begin{algorithmic}[1]
            % \STATE \textbf{Input:} An converged uncompressed network $f(x; M^0 \odot W_{N}^{0})$ and its weights $W_{k}^{0}$ at epoch $k$, and the target accuracy loss $AccLossTarget$
            \STATE \textbf{Input:} A converged uncompressed network and the target accuracy loss $AccLossTarget$
            \STATE \textbf{Output:} The smallest model meeting the accuracy target
            \STATE Initialize: $T=0.0, \lambda=0.01$.
            \FOR {pruning round $r$ ($r \geq 1$)}
                \STATE Prune the model using $T[r]$ (Refer to Lines 7 and 8 in Algorithm~\ref{alg:overall_pruning})
                % \STATE Rewind weights to $W_{k}^{0}$ 
                % \STATE Rewind Learning Rate to its state from epoch $k$
                \STATE Rewind weights and learning rate (Refer to Lines 9 and 10 in Algorithm~\ref{alg:overall_pruning})
                \STATE Train the pruned model, and evaluate its accuracy $Acc[r]$ (Refer to Lines 11 and 12 in Algorithm~\ref{alg:overall_pruning})
                \STATE Calculate the accuracy loss $AccLoss[r]$: \\$AccLoss[r] = Acc[0] - Acc[r]$
                \IF {$AccLoss[r] < AccLossTarget$}
                    \IF {the changes of model size are within 0.1\% for several rounds}
                        \STATE Terminate
                    \ELSE
                        \STATE $\lambda[r+1] = \lambda[r]$
                        \STATE $T[r+1] = T[r] + \lambda[r+1]$
                    \ENDIF
                \ELSE
                    \STATE Find the last acceptable round $k$
                    \IF {$k$ has been used to roll back for several times}
                        \STATE Mark $k$ as unacceptable
                        \STATE \kaiqi{Go to Step 17}
                    \ELSE
                        \STATE Roll back model weights to round $k$
                        \STATE $\lambda[r+1] = \lambda[r]/2.0^{(C+1)}$ (\kaiqi{$C$} is the number of times for rolling back to round $k$)
                        \STATE $T[r+1] = T[k] + \lambda[r+1]$
                    \ENDIF
                \ENDIF
            \ENDFOR
        \end{algorithmic}
    \label{alg:pruning_policy}
    \end{algorithm}
% \vspace{-25pt}
\end{figure}






\noindent Our approach to pruning is to automatically and efficiently generate a pruned model that meets the users' different objectives. Automatic pruning means that users do not have to figure out how to configure the pruning process. Efficient pruning means that the pruning process should produce the user-desired model as quickly as possible. 
%
Users' pruning objectives can vary depending on the usage scenarios: 
%Meeting the users' different objectives means different pruning policies are used for different applications. The application scenarios can be classified into three categories: 
1) Accuracy-critical tasks, like those used by self-driving cars, have stringent accuracy requirements, which are critical for safety, but do not have strict limits on their computing and storage usages; 2) Memory-constrained tasks, like those deployed on microcontrollers, have very limited available memory to store the models but do not have strict accuracy requirements; and 3) Latency-sensitive tasks, like those employed by virtual assistants where timely responses are desirable but accuracy is not a hard constraint.






In order to achieve automatic and efficient pruning, we propose three adaptive pruning policies to provide 1) Accuracy-guaranteed pruning which produces the most resource-efficient model with the acceptable accuracy loss; 2) Memory-constrained pruning which generates the most accurate model within a given memory footprint; and 3) FLOPs-constrained pruning which creates the most accurate model within a given computational intensity.
%
Specifically, our adaptive pruning method automatically adjusts the global threshold ($T$) used in our iterative structured pruning algorithm (Algorithm~\ref{alg:overall_pruning}) to quickly find the model that meets the pruning objective.
%
Other objectives (e.g., limiting a model's energy consumption) as well as multi-objective optimization can also be readily supported (See Section~\ref{sec:discussion}).
%
We take Accuracy-guaranteed Adaptive Pruning, described in Algorithm~\ref{alg:pruning_policy}, as an example to show the procedure of adaptive pruning. 
Algorithm~\ref{alg:pruning_policy} is a specific version of Algorithm~\ref{alg:overall_pruning} with the acceptable accuracy loss set as the pruning target.
Other versions with memory and FLOPs targets are included in Appendix~\ref{sec:appendix_adaptive_pruning}.
%~\ref{sec:appendix_memory_pruning} and \ref{sec:appendix_flops_pruning}, respectively. 
%Given a target accuracy loss, we use a search and resetting strategy to determine the hyper-parameters $T$ and $\lambda$. \m{Don't use ``we''. It suggests we have set the parameters manually.}






%In Algorithm~\ref{alg:pruning_policy}, the objective is to guarantee the accuracy loss while minimizing the model size (the algorithm for minimizing the model FLOPs is similar).
%
In the algorithm, $T$ controls the aggressiveness of pruning, and $\lambda$ determines the increment of $T$ at each pruning round. Pruning starts conservatively, with $T$ initialized to $0$, so that only completely useless filters that cannot capture any features are pruned. After each round, if the model accuracy loss is below the target accuracy loss, it is considered ``acceptable'', and the algorithm increases the aggressiveness of pruning by incrementing $T$ by $\lambda$, with $\lambda$ initialized to 0.01. 
%
%($\lambda$ is initialized to not be too small so $T$ does not grow too slowly, and it can also be adapted, as explained next, if it turns out to be too large.)
%\m{need to explain how $\lambda$ adapts}\kaiqi{$\lambda$ does not change when the accuracy loss is acceptable.}
%
% At the same time, all the round numbers where the accuracy loss is below than the target accuracy loss is added to a list, noted as $TrackedRound$. \m{No need to introduce this variable. Just say the last acceptable round.}
%
%If the current accuracy loss is below than the target accuracy loss, that means, redundant parameters still exists given the target, so $T$ increases by $\lambda$. %
As pruning becomes increasingly aggressive, the accuracy eventually drops below the target in a certain round which is considered ``unacceptable''. When this happens, our algorithm rolls back the model weights and pruning threshold to the last acceptable round where the accuracy loss is within the target, and restarts the pruning from there but more conservatively---it increases the threshold more slowly by cutting the $\lambda$ value by half.
%
If this still does not lead to an acceptable round, the algorithm cuts $\lambda$ by half again and restarts again.
%
If after several trials, the accuracy loss is still not acceptable, the algorithm rolls back even further and restarts from an earlier round. 
%\kaiqi{It does not always roll back to the first round, since $\lambda$ will reduce to a small value in later rounds, thus leading to the changes of $T$ being in a small range. As $T$ determines the aggressiveness of pruning, the model size will converge finally (If the changes of the number of parameters is within 0.1\% for several times, we consider it as convergence).}\m{explain why it doesn't always roll back to the first round.}
%
The rationale behind this adaptive algorithm is that the aggressiveness of pruning should accelerate when the model is far from the pruning target and decelerate when it is close to the target. 






Note that the value of $\lambda$ continuously decreases as the algorithm gets close to the target, which guarantees the convergence of the pruned model: 
when $\lambda$ becomes sufficiently small, $T$ does not grow much anymore, which means no more filters are pruned and the model converges. See Section~\ref{sec:ablation} for an example. The algorithm terminates when the changes of model size is within 0.1\% for several rounds. 
%
% \kaiqi{Although $T$ and $\lambda$ are two hyper-parameters needed initialization, which violates ``automatic'' to some degree, the proposed algorithm works well for various models and datasets when $T$ and $\lambda$ are simply initialized to 0.0 and 0.1, respectively, thanks to the process of rolling back.}






The proposed algorithms can automatically---the only two parameters $T$ and $\lambda$ are automatically tuned---generate pruned models that meet diverse user requirements in model accuracy, size, and speed. Moreover, by making LTH-based iterative pruning adaptive, the algorithms can automatically find the best models that meet user requirements. Compared to related works which require time-consuming manual tuning of pruning parameters, AAP is the first to achieve these goals which are crucial to the practical use of model pruning for diverse real-world scenarios.
%\kaiqi{Although the idea of the proposed adaptive pruning policy is uncomplicated, its easy implementation enables convenient generalization to various models. Also, it is crucial for real scenarios since the pruning process can meet the usersâ€™ specific objectives automatically and effectively.}
% None of the related works achieves that.
%Eventually, the changes of model size converges (when the changes of the number of parameters is within 0.1\% for several rounds) and the algorithm terminates.
%The overall rationale behind this adaptive pruning algorithm is that, \kaiqi{at the beginning, the model is large and has many redundant filters, so the algorithm increases the aggressiveness of pruning as long as the accuracy is acceptable. Then, as most of useless filters are pruned, it is likely to prune useful filters and lead to a large accuracy drop that exceeds the target. So the algorithm uses rollback and carries out the pruning process conservatively to enable the model close to the target as much as possible while remaining a small model size.}
%The index is the number of times of resetting to the same round. 
%If the accuracy loss is below than the target accuracy continuously after resetting the model to that round for several times, so that $\lambda$ is close to zero, it stops rolling back to that around and removes that round from the list $TrackedRound$. Then it finds another latest round from the list $TrackedRound$ and repeats the above resetting steps. The algorithm terminates when the list $TrackedRound$ is empty.
% At the same time, in order to make sure accuracy loss is close to the target accuracy loss while model size is as small as possible, our pruning policy carries out a fine search on $\lambda$ based on its value of the resetting round. 








\subsection{Layer-aware Threshold Adjustment}
\label{sec:layer-aware}

While adapting the global pruning threshold using the above discussed policies, our pruning method further considers the difference in each layer's contribution to model size and complexity and uses differentiated layer-specific thresholds to prune the layers. 
%The filter size is different if they are in different layers, i.e., $k^i$ (Figure~\ref{fig:conv2d}). Also, each convolution layer contributes differently to the pruning objective.
As shown in Figure~\ref{fig:conv2d}, in terms of the contribution to model size, the number of parameters of layer $i$ can be estimated as $N^i = n^{(i-1)} \times k^i \times k^i \times n^i$; in terms of the contribution to computational complexity, the number of FLOPs of layer $i$ can be estimated as $F^i = 2 \times h^i \times w^i \times N^i$.
%
A layer that contributes more to the model's size or FLOPs is more likely to have redundant filters to prune without affecting the model's accuracy.
% A layer that contains more remaining parameters or FLOPs is more likely to have redundant filters that can be pruned without affecting the model's accuracy.




Therefore, to effectively prune a model while maintaining its accuracy, we need to treat each layer differently at each round of the iterative pruning process based on its current contributions to the model size and complexity.
%
Specifically, our adaptive pruning method calculates a weight for each layer based on its contribution and then uses this weight to adjust the current global threshold and derive a local threshold for the layer. 
%
If the goal is to reduce model size, the weight is calculated as each layer's number of remaining parameters $N^i[r]$ divided by the model's total number of remaining parameters $N^{Total}[r]$: $w^i[r] = \frac{N^i[r]}{N^{Total}[r]}$, where $r$ is the pruning round.
If the goal is to reduce model computational complexity, the weight is calculated as each layer's remaining FLOPs $F^i[r]$ divided by the model's total remaining FLOPs $F^{Total}[r]$: $w^i[r] = \frac{F^i[r]}{F^{Total}[r]}$. Then the threshold of layer $i$ is calculated as: $T^i[r] = T[r] \times w^i[r]$.
%
These layer-specific thresholds are then used to prune the layers in the current pruning round; they replace the global threshold $T[r]$ used to prune filters in Line 8 of Algorithm~\ref{alg:overall_pruning}.
%Assuming the goal is to reduce the model size, the procedure in a pruning round $r$ for the $i$th convolutional layer is shown in Algorithm~\ref{alg:layer_threshold}.





Compared to the related works which often use a single threshold to prune parameters for the entire network \cite{han2015learning, zhao2019variational}, AAP's layer-specific thresholds allow it to generate better pruned models, and these thresholds are also fully automatically tuned.



% =======> algorithm 3
% \begin{figure}[t]
% \vspace{-12pt}
%     % \begin{algorithm}[t]
%     \begin{algorithm}[H]
%         \centering
%         \small
%         \caption{Layer-aware Threshold Adjustment}
%         \begin{algorithmic}[1]
%             \STATE \textbf{Input:} \kaiqi{The global threshold $T[r]$ in a pruning round $r$}
%             \STATE \textbf{Output:} \kaiqi{The local threshold $T^i[r]$ for each convolution layer $i$ in the pruning round $r$}
%             \FOR {the layer $i$ in a pruning round $r$}
%                 \STATE Calculate the number of remaining parameters of unpruned filters of the current layer and the whole model, respectively: $N^i[r]=n^{(i-1)}[r] \times k^i[r] \times k^i[r] \times n^i[r]$, \\$N^{Total}[r] = \sum_{i=1}^{i=M} n^{(i-1)}[r] \times k^i[r] \times k^i[r] \times n^i[r]$, where $M$ is the number of convolution layers, and for other notations refer to Activation-based Filter Pruning.
%                 \STATE Calculate the threshold $T^i[r]$ of the current layer: \\ $T^i[r] = T[r] \times \frac{N^i[r]}{N^{Total}[r]}$.
%                 % \STATE For each filter $F^i_j$ in $F^i$, calculate its attention: $F_{mean}(A^i_j)= \frac{1}{h^{i}\times w^{i}}\sum_{k=1}^{h^i}\sum_{l=1}^{w^i} {\left | a_{k,l}^i  \right |}^{p}$.
%                 % \STATE Prune the filter if its attention is not larger than threshold $T^i[r]$, i.e., set $M^i_j=0$ if $F_{mean}(A^i_j) \leq T^i[r]$; Otherwise, set $M^i_j=1$
%             \ENDFOR
%         \end{algorithmic}
%     \label{alg:layer_threshold}
%     \end{algorithm}
% \vspace{-20pt}
% \end{figure}








