% !TEX root = main.tex
\section{Appendix}\label{sec:appendix}


In the supplementary materials, we first discuss the adaptive pruning policy with memory and FLOPs targets in Section~\ref{sec:appendix_adaptive_pruning}. 
% Then we introduce the implementation details in Section~\ref{sec:implementation_details}. 
Then we introduce the results of the additional experiments in Section~\ref{sec:appendix_experiments}.




\subsection{Adaptive Pruning Policy}\label{sec:appendix_adaptive_pruning}


\subsubsection{Memory-constrained Adaptive Pruning}\label{sec:appendix_memory_pruning}

The Memory-constrained Adaptive Pruning Algorithm is shown in Algorithm~\ref{alg:memory_pruning_policy}. 

\begin{algorithm}[H]
    \centering
    \small
    % \setstretch{0.5}
    \caption{Memory-constrained Adaptive Pruning}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Target Parameters Reduction $ParamTarget$
        \STATE \textbf{Output:} A small pruned model with an acceptable model size
        \STATE Initialize: $T=0.0, \lambda=0.01$.
        \FOR {pruning round $r$ ($r \geq 1$)}
            % \STATE Prune the model using $T[r]$ (Refer to Algorithm 3)
            % \STATE Train the pruned model, calculate its remaining number of parameters $Param[r]$
            \STATE Prune the model using $T[r]$ %(Refer to Lines 7 and 8 in Algorithm 1)
            \STATE Rewind weights and the learning rate %(Refer to Lines 9 and 10 in Algorithm 1)
            \STATE Train the pruned model, and calculate its remaining number of parameters $Param[r]$ %(Refer to Lines 11 and 12 in Algorithm 1)
            
            \STATE Calculate the parameter reduction: $ParamRed[r]$: $ParamRed[r] = Param[0] - Param[r]$
            \IF {$ParamRed[r] < ParamTarget$}
                \IF {the changes of model size are within 0.1\% for several rounds} 
                    \STATE Terminate
                \ELSE
                    \STATE $\lambda[r+1] = \lambda[r]$
                    \STATE $T[r+1] = T[r] + \lambda[r+1]$
                \ENDIF
            \ELSE
                \STATE Find the last acceptable round $k$
                \IF {$k$ has been used to roll back for several times}
                    \STATE Mark $k$ as unacceptable
                    \STATE Go to Step 17
                \ELSE
                    \STATE Roll back model weights to round $k$
                    \STATE $\lambda[r+1] = \lambda[r]/2.0^{(C+1)}$ ($C$ is the number of times for rolling back to round $k$)
                    \STATE $T[r+1] = T[k] + \lambda[r+1]$
                \ENDIF
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\label{alg:memory_pruning_policy}
\end{algorithm}
% \newpage

\vfill

\subsubsection{FLOPs-constrained Adaptive Pruning}\label{sec:appendix_flops_pruning}


The FLOPs-constrained Adaptive Pruning Algorithm is shown in Algorithm~\ref{alg:flops_pruning_policy}.
\begin{algorithm}[H]
    \centering
    \small
    % \setstretch{0.5}
    \caption{FLOPs-constrained Adaptive Pruning}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Target FLOPs Reduction $FLOPsTarget$
        \STATE \textbf{Output:} A small pruned model with an acceptable FLOPs
        \STATE Initialize: $T=0.0, \lambda=0.01$.
        \FOR {pruning round $r$ ($r \geq 1$)}
            % \STATE Prune the model using $T[r]$ (Refer to Algorithm 3)
            % \STATE Train the pruned model, calculate its remaining FLOPs $FLOPs[r]$
            \STATE Prune the model using $T[r]$ %(Refer to Lines 7 and 8 in Algorithm 1)
            \STATE Rewind weights and the learning rate %(Refer to Lines 9 and 10 in Algorithm 1)
            \STATE Train the pruned model, and calculate its remaining FLOPs $FLOPs[r]$ %(Refer to Lines 11 and 12 in Algorithm 1)
            
            \STATE Calculate the parameters reduction: $FLOPsRed[r]$: $FLOPsRed[r] = FLOPs[0] - FLOPs[r]$
            \IF {$FLOPsRed[r] < FLOPsTarget$}
                \IF {the changes of FLOPs are within 0.1\% for several rounds} 
                    \STATE Terminate
                \ELSE
                    \STATE $\lambda[r+1] = \lambda[r]$
                    \STATE $T[r+1] = T[r] + \lambda[r+1]$
                \ENDIF
            \ELSE
                \STATE Find the last acceptable round $k$
                \IF {$k$ has been used to roll back for several times}
                    \STATE Mark $k$ as unacceptable
                    \STATE Go to Step 17
                \ELSE
                    \STATE Roll back model weights to round $k$
                    \STATE $\lambda[r+1] = \lambda[r]/2.0^{(C+1)}$ ($C$ is the number of times for rolling back to round $k$)
                    \STATE $T[r+1] = T[k] + \lambda[r+1]$
                \ENDIF
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\label{alg:flops_pruning_policy}
\end{algorithm}

\vfill

% \subsection{Implementation Details}\label{sec:implementation_details}


% % Our code is uploaded in the Supplementary Material. 
% We implemented our proposed method on PyTorch version 1.6.0.
% %
% % For ResNet models On CIFAR-10, the learning rate is set to 0.1 initially and decays with a rate of 0.1 at epochs 91 and 136. Weight decay is set to 0.0002. For ResNet-50 on ImageNet, the learning rate increases to 0.256 in a warmup mechanism during the first five epochs and decays with a factor of 0.1 at epochs 30, 60, 80 (\cite{renda2020comparing, frankle2019stabilizing}).
% %
% The implementation details are present in Table~\ref{tab:implementation_details}. 
% %
% % We use 2 GPUS for training models on CIFAR-10 and Tiny-ImageNet, and 4 GPUS on ImageNet. 
% %
% % For LeNet-5 on CIFAR-10, the Adam optimizer is used, and the learning rate is set to 0.0002 without decay.
% %
% % Nesterov SGD optimizer is used for other models with a momentum of 0.9. 
% The learning rate decays with a factor of 0.1 at decay epochs shown in Table~\ref{tab:implementation_details}. 
% %
% Simple data augmentation (random crop and random horizontal flip) is used for all training images. 



% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% \begin{table}[htp]
% \centering
% \caption{Implementation details}
% \vspace{-10pt}
% \label{tab:implementation_details}
% \begin{tabular}{cc|cc|ccc}
% \toprule
% \multirow{2}{*}{Dataset}       & \multirow{2}{*}{Model} & \multicolumn{2}{c|}{Learning Rate Schdeluder} & \multirow{2}{*}{Training Epochs} & \multirow{2}{*}{Weight decay} & \multirow{2}{*}{Batch Size} \\
%                               &                        & Initial Learning Rate  & Decay Epochs         &                                  &                               &                             \\ \hline
% \multirow{2}{*}{MNIST}      & LeNet-5              & 0.1                    & N/A        & 100                              & 0.0                      & 256                         \\
%                                & LeNet-300-100              & 0.0012                    &  N/A        & 100                              & 0.0                      & 256                         \\ \hline
% \multirow{7}{*}{CIFAR-10}      & ResNet-56              & 0.1                    & {[}91, 136{]}        & 182                              & 2.00E-04                      & 128                         \\
%                               & ResNet-50              & 0.1                    & {[}91, 136{]}        & 182                              & 2.00E-04                      & 128                         \\
%                               & ShuffleNet             & 0.1                    & {[}60, 120, 160{]}   & 200                              & 4.00E-05                      & 128                         \\
%                               & MobileNet-V2           & 0.1                    & {[}150, 225{]}       & 300                              & 4.00E-05                      & 128                         \\
%                               & VGG-16                 & 0.05                   & {[}150, 180, 210{]}  & 240                              & 5.00E-04                      & 128                         \\
%                               & VGG-19                 & 0.05                   & {[}150, 180, 210{]}  & 240                              & 5.00E-04                      & 128                         \\
%                               & LeNet-5                & 0.0002                 & N/A                  & 24                               & 1.00E-04                      & 128                         \\ \hline
% \multirow{2}{*}{Tiny-ImageNet} & ResNet-101             & 0.1                    & {[}150, 225{]}       & 300                              & 2.00E-04                      & 256                         \\
%                               & VGG-19                 & 0.1                    & {[}150, 225{]}       & 300                              & 2.00E-04                      & 256                         \\ \hline
% ImageNet                       & ResNet-50              & 0.256                  & {[}30, 60, 80{]}     & 90                               & 1.00E-04                      & 64                          \\ 
% \bottomrule
% \end{tabular}
% \end{table}





\newpage

\subsection{Additional Experiments}\label{sec:appendix_experiments}

\subsubsection{The Effect of Rewinding Epoch}\label{sec:appendix_rewinding}


To understand how the rewinding impacts the accuracy of the pruned models, we analyze \emph{stability to pruning}, which is defined as the L2 distance between the masked weights of the pruned network and the original network at the end of training.
%
We validate the observations that for deep networks, rewinding to very early stages is sub-optimal as the network has not learned considerably by then; and rewinding to very late training stages is also sub-optimal because there is not enough time to retrain. Specifically,
%
Figure~\ref{fig:ablation_accuracy} shows the Top-1 test accuracy of the pruned ResNet-50 with a parameter reduction of 16.26\% on ImageNet when the learning rate is rewound to different epochs, and Figure~\ref{fig:ablation_stability} shows the stability values at the corresponding rewinding epochs. We observe that there is a region, 65 to 80 epochs, where the resulting accuracy is high. We find that the L2 distance closely follows this pattern, showing a large distance for early training epochs and a small distance for later training epochs. 
%
Our findings show that rewinding to 75\%-90\% of training time leads to good accuracy.
% \m{this study is not so important. can be removed to save space.}

% \begin{figure*}[htp]
% 	\centering
% 	\subfigure[Top-1 Test Accuracy]{
% 		\centering
% 		\includegraphics[width=0.35\textwidth]{figs/stability_acc.eps}
% 		\label{fig:ablation_accuracy}
% 	}%
% 	\subfigure[Stability to Pruning]{
% 		\centering
% 		\includegraphics[width=0.35\textwidth]{figs/stability_dis.eps}
% 		\label{fig:ablation_stability}
% 	}%
% 	\caption{The effect of the rewinding epoch (x-axis) on  (a) Top-1 test accuracy, and (b) pruning stability, for pruned ResNet-50 with a parameter reduction of 16.26\% on ImageNet.}
% \end{figure*}

\begin{figure*}[htp]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\centering
  		\includegraphics[width=4.5cm]{figs/stability_acc.pdf}
		\caption{Top-1 Test Accuracy}
		\label{fig:ablation_accuracy}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
  		\includegraphics[width=4.5cm]{figs/stability_dis.pdf}
		\caption{Stability to Pruning}
		\label{fig:ablation_stability}
	\end{subfigure}
	\caption{The effect of the rewinding epoch (x-axis) on  (a) Top-1 test accuracy, and (b) pruning stability, for pruned ResNet-50 with a parameter reduction of 16.26\% on ImageNet.}
\end{figure*}



\subsubsection{The Effect of Attentions}\label{sec:appendix_attention}
% \section{The Effect of Attentions}\label{sec:appendix_attention}

Figure~\ref{fig:attention} shows the attention of each filter of the first convolution layer of ResNet-50 on ImageNet with different values of $p$ (p=1, 2, 4). The setting where $p$ is equal to 1 tends to be best since it promotes the effectiveness of the pruning by enabling the gap between the mean values of the useful and useless filters to be large.

% Figure~\ref{} shows the attention of each filter of the first convolution layer of ResNet-50 on ImageNet with different values of $p$ (p=1, 2, 4), which is defined in equation~\ref{}. Figure~\ref{} can valiadate that the pruned model has the best performance when p is set to 1. The 2nd, 3rd, 6th, 10th, and 12th filter can capture more information than other filters, so we aim to retain those filters by pruning filters with a larger mean value of attention. The setting where p is equal to 1 enables the gap between the mean values of the useful filters and useless filters to be larger, so it promotes the effectiveness of the pruning. \m{Unlikely to have space for this. Don't work on it for now.}



% \begin{figure*}[htp]
% 	\centering
% 	\subfigure[p=1]{
% 		\begin{minipage}[t]{0.33\linewidth}
% 			\centering
% 			\includegraphics[width=2.0in]{figs/p_1.eps}
% 		\end{minipage}%
% 	}%
% 	\subfigure[p=2]{
% 		\begin{minipage}[t]{0.33\linewidth}
% 			\centering
% 			\includegraphics[width=2.0in]{figs/p_2.eps}
% 		\end{minipage}%
% 	}%
% 	\subfigure[p=4]{
% 		\begin{minipage}[t]{0.33\linewidth}
% 			\centering
% 			\includegraphics[width=2.0in]{figs/p_4.eps}
% 		\end{minipage}%
% 	}%
% 	\centering
% 	\caption{Attentions of each filter of the first convolution layer of ResNet-50 on ImageNet with different values of $p$ ($p = 1, 2, 4$).} 
% 	\label{fig:attention}
% \end{figure*}



\begin{figure*}[htp]
	\centering
	\begin{subfigure}{0.33\linewidth}
		\centering
  		\includegraphics[width=2.0in]{figs/p_1.pdf}
		\caption{p=1}
	\end{subfigure}
	\begin{subfigure}{0.33\linewidth}
		\centering
  		\includegraphics[width=2.0in]{figs/p_2.pdf}
		\caption{p=2}
	\end{subfigure}
 	\begin{subfigure}{0.33\linewidth}
		\centering
  		\includegraphics[width=2.0in]{figs/p_4.pdf}
		\caption{p=4}
	\end{subfigure}
	\caption{Attentions of each filter of the first convolution layer of ResNet-50 on ImageNet with different values of $p$ ($p = 1, 2, 4$).} 
 	\label{fig:attention}
\end{figure*}




\newpage
\subsubsection{Inference Speedup on CPU}
Figure~\ref{fig:speedup_cpu} shows the speedup of ResNet-50 with a parameter reduction of 34.54\% and 57.07\%, respectively, on one Intel(R) Xeon(R) Silver 4215R CPU. We run it for 10 trails. The input image size is $224 \times 224$. The average throughput of the original ResNet-50 for 10 trails is 3.67fps.

% \begin{figure*}[htp]
% 	\centering
% 	\vspace{-15pt}
% 	\subfigure[Speedup]{
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{figs/rn50_speedup_cpu.eps}
% 	}%
% 	\subfigure[Throughput]{
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{figs/rn50_throughput_cpu.eps}
% 	}%
% 	\vspace{-10pt}
% 	\caption{The illustration of (a) the speedup and (b) the throughput of ResNet-50 with a parameters reduction of 34.54\% and 57.07\%, respectively, on one Intel(R) Xeon(R) Silver 4215R CPU. We run it for 10 trails. The input image size is $224 \times 224$. The average throughput of the original ResNet-50 is 3.67fps.}
% 	\label{fig:speedup_cpu}
% \end{figure*}

\begin{figure*}[htp]
	\centering
	\begin{subfigure}{0.4\textwidth}
		\centering
  		\includegraphics[width=6.0cm]{figs/rn50_speedup_cpu.pdf}
        % \vspace{-18pt}
        \caption{Speedup}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
  		\includegraphics[width=6.0cm]{figs/rn50_throughput_cpu.pdf}
        % \vspace{-18pt}
        \caption{Throughput}
	\end{subfigure}
	\caption{The illustration of (a) the speedup and (b) the throughput of ResNet-50 with a parameter reduction of 34.54\% and 57.07\%, respectively, on one Intel(R) Xeon(R) Silver 4215R CPU. We run it for 10 trails. The input image size is $224 \times 224$. The average throughput of the original ResNet-50 is 3.67fps.}
 	\label{fig:speedup_cpu}
\end{figure*}



\subsubsection{Inference Speedup on Raspberry Pi}
% \subsubsection{ResNet-56 on CIFAR-10}
\boldhdr{ResNet-56 on CIFAR-10}
%
Figure~\ref{fig:speedup_pi_cifar10_rn56} shows the speedup of ResNet-56 with a FLOP reduction of 33.77\% and 56.33\%, respectively, on Raspberry Pi 3B+. We run it for 10 trails. The input image size is $32 \times 32$. The average throughput of the original ResNet-56 for 10 trails is 17.63fps.

% \begin{figure*}[htp]
% 	\centering
% 	\vspace{-15pt}
% 	\subfigure[Speedup]{
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{figs/cifar10_rn56_speedup.eps}
% 	}%
% 	\subfigure[Throughput]{
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{figs/cifar10_rn56_throughput.eps}
% 	}%
% 	\vspace{-10pt}
% 	\caption{The illustration of (a) the speedup and (b) the throughput of ResNet-56 with a FLOPs reduction of 33.77\% and 56.33\%, respectively, on Raspberry Pi 3B+. We run it for 10 trails. The input image size is $32 \times 32$. The average throughput of the original ResNet-56 is 17.63fps.}
% 	\label{fig:speedup_pi_cifar10_rn56}
% \end{figure*}


\begin{figure*}[htp]
	\centering
	% \vspace{-15pt}
	\begin{subfigure}{0.4\textwidth}
		\centering
  		\includegraphics[width=6.0cm]{figs/cifar10_rn56_speedup.pdf}
    % \vspace{-18pt}
		\caption{Speedup}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
  		\includegraphics[width=6.0cm]{figs/cifar10_rn56_throughput.pdf}
    % \vspace{-18pt}
		\caption{Throughput}
	\end{subfigure}
	% \vspace{-10pt}
	\caption{The illustration of (a) the speedup and (b) the throughput of ResNet-56 with a FLOP reduction of 33.77\% and 56.33\%, respectively, on Raspberry Pi 3B+. We run it for 10 trails. The input image size is $32 \times 32$. The average throughput of the original ResNet-56 is 17.63fps.}
	\label{fig:speedup_pi_cifar10_rn56}
\end{figure*}



\boldhdr{VGG-16 on CIFAR-10}
%
Figure~\ref{fig:speedup_pi_cifar10_vgg16} shows the speedup of VGG-16 with a FLOPs reduction of 59.51\% and 73.89\%, respectively, on Raspberry Pi 3B+. We run it for 10 trails. The input image size is $32 \times 32$. The average throughput of the original VGG-16 for 10 trails is 1.77fps.

% \begin{figure*}[htp]
% 	\centering
% 	\vspace{-15pt}
% 	\subfigure[Speedup]{
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{figs/cifar10_vgg16_speedup.eps}
% 	}%
% 	\subfigure[Throughput]{
% 		\centering
% 		\includegraphics[width=0.4\textwidth]{figs/cifar10_vgg16_throughput.eps}
% 	}%
% 	\vspace{-10pt}
% 	\caption{The illustration of (a) the speedup and (b) the throughput of VGG-16 with a FLOPs reduction of 59.51\% and 73.89\%, respectively, on Raspberry Pi 3B+. We run it for 10 trails. The input image size is $32 \times 32$. The average throughput of the original VGG-16 is 1.77fps.}
% 	\label{fig:speedup_pi_cifar10_vgg16}
% \end{figure*}

\begin{figure*}[htp]
	\centering
	% \vspace{-15pt}
	\begin{subfigure}{0.4\textwidth}
		\centering
  		\includegraphics[width=6.0cm]{figs/cifar10_vgg16_speedup.pdf}
    % \vspace{-18pt}
		\caption{Speedup}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
		\centering
  		\includegraphics[width=6.0cm]{figs/cifar10_vgg16_throughput.pdf}
    % \vspace{-18pt}
		\caption{Throughput}
	\end{subfigure}
	% \vspace{-10pt}
	\caption{The illustration of (a) the speedup and (b) the throughput of VGG-16 with a FLOP reduction of 59.51\% and 73.89\%, respectively, on Raspberry Pi 3B+. We run it for 10 trails. The input image size is $32 \times 32$. The average throughput of the original VGG-16 is 1.77fps.}
	\label{fig:speedup_pi_cifar10_vgg16}
\end{figure*}


\newpage
% \vspace{-10pt}
\subsubsection{Attention Distributions}
Figure~\ref{fig:distribution} shows the distribution of the attention values of each convolutional layer of the original ResNet-50 and pruned ResNet-50 with a parameter reduction of 96.31\% on ImageNet. Specifically, given one batch of images, the models do the inference once. We first measure the attention value ($p=1$) of each filter for each image, and then we calculate its average attention values for one batch of images.

\begin{figure*}[h]
    % \vspace{-30pt}
	\centering
	\includegraphics[width=\textwidth]{figs/distribution.pdf}
 	% \vspace{-40pt}
	\caption{The the distribution of the attention values of each convolutional layer of the original ResNet-50 and the pruned ResNet-50 with a parameter reduction of 96.31\% on ImageNet.}
	\label{fig:distribution}
\end{figure*}