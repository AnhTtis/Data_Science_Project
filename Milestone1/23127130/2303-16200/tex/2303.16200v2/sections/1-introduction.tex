\newpage
\section{Introduction}


We are living through a period of unprecedented progress in AI development. In the last decade, the cutting edge of AI went from distinguishing cat pictures from dog pictures to generating photorealistic images \cite{ramesh2022hierarchical}, writing professional news articles, playing complex games such as Go at superhuman levels \cite{silver2018general}, writing human-level code \cite{chen2021evaluating}, and solving protein folding \cite{jumper2021highly}. It is possible that this momentum will continue, and the coming decades may see just as much progress.

This paper will discuss the AIs of today, but it is primarily concerned with the AIs of the future. If current trends continue, we should expect AI agents to become just as capable as humans at a growing range of economically relevant tasks. This change could have huge upsides---AI could help solve many of the problems humanity faces. But as with any new and powerful technology, we must proceed with caution. Even today, corporations and governments use AI for more and more complex tasks that used to be done by humans. As AIs become increasingly capable of operating without direct human oversight, AIs could one day be pulling high-level strategic levers. If this happens, the direction of our future will be highly dependent on the nature of these AI agents.


So what will that nature be? When AIs become more autonomous, what will their basic drives, goals, and values be? How will they interact with humans and other AI agents? Will their intent be aligned with the desires of their creators? Opinions on how human-level AI will behave span a broad spectrum between optimism and pessimism. On one side of the spectrum, we can hope for benevolent AI agents, that avoid harming humans and apply their intelligence to goals that benefit society. Such an outcome is not guaranteed. On the other side of the spectrum, we could see a future controlled by artificial agents indifferent to human flourishing.

Due to the potential scale of the effects of AI in the coming decades, we should think carefully about the worst-case scenarios to ensure they do not happen, even if these scenarios are not certain. Preparing for disaster is not overly pessimistic; rather it is prudent. As the COVID-19 pandemic demonstrated, it is important for institutions and governments to plan for possible catastrophes well in advance, not only to react once they are happening: many lives could have been saved by better pandemic prevention measures, but people are often not inclined to think about risks from uncommon situations. In the same way, we should develop plans for a variety of possible situations involving risks from AI, even though some of those situations will never happen. At its worst, a future controlled by AI agents indifferent to humans could spell large risks for humanity, so we should seriously consider our future plans now, and not wait to react when it may be too late.

A common rebuttal to any predictions about the effects of advanced AIs is that we don't yet know how they will be implemented. Perhaps AIs will simply be better versions of current chatbots, or better versions of the agents that can beat humans at Go. They could be cobbled together with a variety of machine learning methods, or belong to a totally new paradigm. In the face of such uncertainty about the implementation details, can we predict anything about their nature?

We believe the answer is yes. In the past, people successfully made predictions about lunar eclipses and planetary motions without a full understanding of gravity. They projected dynamics of chemical reactions, even without the correct theory of quantum physics. They formed the theory of evolution long before they knew about DNA. %
In the same way, we can predict whether natural selection will apply to a given situation, and predict what traits natural selection would favor. We will discuss the criteria that enable natural selection and show that natural selection is likely to influence AI development. If we know how natural selection will apply to AIs, we can predict some basic traits of future AI agents.



In this work, we take a bird's-eye view of the environment that will shape the development of AI in the coming decades. We consider the pressures that drive those who develop and deploy AI agents, and the ways that humans and AI will interact. These details will have strong effects on AI designs, so from such considerations we can infer what AI agents will probably look like. We argue that natural selection creates incentives for AI agents to act against human interests. Our argument relies on two observations. Firstly, \textbf{natural selection may be a dominant force in AI development}. Competition and power-seeking may dampen the effects of safety measures, leaving more ``natural'' forces to select the surviving AI agents. Secondly, \textbf{evolution by natural selection tends to give rise to selfish behavior}. While evolution can result in cooperative behavior in some situations (for example in ants), we will argue that AI development is not such a situation. From these two premises, it seems likely that \textbf{the most influential AI agents will be selfish}. In other words, they will have no motivation to cooperate with humans, leading to a future driven by AIs with little interest in human values. While some AI researchers may think that undesirable selfish behaviors would have to be intentionally designed or engineered, this is simply not so when natural selection selects for selfish agents.
Notably, this view implies that even if we can make some AIs safe, there is still the risk of bad outcomes. In short, even if \textit{some} developers successfully build altruistic AIs, others will build less altruistic agents who will outcompete the altruistic ones.

We present our core argument in more detail in \Cref{sec:argument}. Then in \Cref{sec:altruism}, we examine how the mechanisms that foster altruism among humans might fail with AI and cause AI to act selfishly against humans. We then move onto \Cref{sec:counteracting}, where we discuss some mechanisms to oppose these Darwinian forces and increase the odds of a desirable future.
