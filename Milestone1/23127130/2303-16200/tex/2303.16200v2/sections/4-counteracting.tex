\section{Counteracting Evolutionary Forces}\label{sec:counteracting}
As we saw in the prior section, there are many mechanisms that give rise to cooperation and altruism among humans, but they are unlikely to lead to cooperation between humans and AIs. Mechanisms such as reciprocity, kin selection, and moral obligations may help AIs cooperate with one another, but are likely to backfire and undermine humans: we simply would not have the degree of similarity, equality, and mutual interdependence that would make it beneficial for AIs to cooperate with us. This means that we should be concerned about our future as AIs become increasingly powerful. The forces of natural selection would push AIs to outcompete humans and outcompete AIs that heavily depend on us, and this poses large risks to our future. %

In this section, we will discuss some possible paths toward counteracting these evolutionary forces. The mechanisms we discuss in this section are incentives, consciences, and institutions, among others. These mechanisms are based on earlier results in AI safety \cite{hendrycks2021unsolved}, as well as on mechanisms that have been effective in protecting humanity from other hazards thus far in our history. First, we will discuss objectives, the incentives used to train and motivate AIs. Next, we will move on to internal safety.
This involves analyzing the AI's inner processes and plans, as well as creating a system within the AI, akin to a human conscience, that can stop it from doing harm.
We will end by considering institutional mechanisms, which include both AI coalitions and human regulators, that could control AIs and prevent them from harming people. For each of these mechanisms, we will give a technical overview of how it could work and consider its key limitations.

All of these proposed mechanisms have flaws, and we have no guaranteed path toward safety.
However, we think that a combination of many safety mechanisms is much more likely to succeed than any single one, and a combination is certainly better than doing nothing and letting natural selection decide our fate. Even if we design each safety mechanism prudently, each is only \textit{part} of the solution. The same kind of reasoning applies to public health during a pandemic: social distancing, masking, and vaccination all have vulnerabilities that a harmful virus can bypass, but together are much more effective at preventing its spread. This is sometimes called the ``Swiss cheese model.'' To illustrate the model, imagine a stack of Swiss cheese slices as a metaphor for multiple safety mechanisms. Each slice has some holes, which are the weaknesses of a mechanism. We want to prevent light from passing through the stack, so we need more than one slice, and each slice needs holes in different places. This way, no light can get through the stack. Similarly, while each safety mechanism has many vulnerabilities or holes, together they could possibly make us safe. We should try to reduce the vulnerabilities in these mechanisms and look for new mechanisms to add to the stack.


\subsection{Objectives}\label{sec:objectives}

\paragraph{Objectives are incentives that help direct the behavior of AI agents.} The first promising mechanism for counteracting the evolutionary forces acting on AIs is to create good incentives, which reward good behavior or punish bad behavior as an AI is being trained. In machine learning, ``training objectives'' or ``objective functions'' are similar to incentives, which help steer AI agents. It is hard to design good objectives, because agents who exploit loopholes in the rules without fulfilling our true intentions often succeed, and this behavior can be favored by selection pressure. As a result, it is particularly important to design objectives with great care, to make sure that the behavior we are incentivizing is really the one we want. Although even the best objectives alone cannot ensure safety, faulty ones are dangerous, making objective design an important starting place for counteracting evolutionary forces.

\paragraph{Objectives often incentivize unintended behavior that is contrary to the original goal.} For example, in 1908 a dog in Paris saw a child drowning in the Seine and jumped in to save him. People rewarded the dog with a steak. Soon, the dog saved another child from drowning in the river and got another steak. And then another, and another. Eventually, people realized the dog was pushing children into the river before saving them. They had incentivized pulling children out of the river, thereby encouraging the dog to optimize for situations in which there is a child in need of saving. An anecdote about colonial Delhi tells another story about perverse incentives. Worried that the venomous snake population was getting out of hand, the British government put a bounty on cobras, providing a reward to anyone who brought in a dead cobra. The policy appeared to be a success---that is, until the government realized that people were breeding snakes, killing them, then collecting the reward. When the governor realized that people were gaming his incentive system, he canceled the bounty. With the cobras now worthless, people released them, thus increasing the cobra population to a higher level than before the start of the program. This story highlights two major ways incentives can go wrong. First, agents may find ways to get the reward without the desired outcome, just as Delhi's residents realized that they could claim the bounty by breeding cobras rather than catching them. Second, by canceling the program, the government inadvertently increased the cobra population, which shows how designers can be forced to either continue a system that isn't achieving its desired effects, or risk making things worse.

AIs already frequently find holes in their objectives \cite{pan2022effects}. In a boat racing game, an AI agent was trained to maximize the game's score by hitting targets on a racecourse. The scoring system was intended to motivate the agent to move as quickly as possible from target to target until it completed the race. This reward function, however, did not explicitly capture the actual goal of the game, which is to complete the race as fast as possible. Instead of going around the entire racecourse, the AI learned that it could go around in a circle, hitting the same three targets over and over. The agent that chose this strategy got more points than the ones that proceeded through the course in order, because it exploited loopholes in the objective. As a result, it obtained a high score even though it crashed into other boats, incidentally set itself on fire, and did not complete the course. As AIs become more intelligent, they could more easily find ways to game the objectives we give them, and we will need to be even more careful about possible misinterpretations or loopholes in the objectives we specify.

In the \Cref{sec:erosion}, we discuss how many objectives could backfire catastrophically, and in \Cref{sec:parliament}, we discuss how these objective design flaws could be ameliorated.

\subsubsection{Value Erosion}\label{sec:erosion}
Even if we can design objectives that make AIs pursue some of our goals, it is conceivable that wielding a technology this powerful could undermine important human values. AIs with the objective of being helpful could undermine autonomy and leave us enfeebled; those with the objective of spreading their user's ideas could undermine our sense of reality; those with the objective of being a good companion could undermine real relationships; and many other objectives could undermine values we overlook. We discuss these cases in more detail to illustrate how objectives could backfire.

\paragraph{AIs incentivized to be highly helpful could lead to human enfeeblement.} The movie WALL-E takes place in a distant future where humans, too weak to walk, live coddled lives entirely dependent on machines. They receive all their nutrition from drinks brought to them by robot attendants, change the color of their clothes instantaneously when informed a new one is in style, and live their lives almost entirely in the digital world. This dystopia may be less distant than we think. Many people barely know how to find their way around their neighborhood without Google Maps. %
Students increasingly depend on spellcheck \cite{lunsford2008mistakes}, and a 2021 survey found that two-thirds of respondents could not spell ``\textit{separate}'' \cite{bbcPoorSpelling}. Sepretely, when people need to call their loved ones, they depend on their phone's contact list, and they are at a loss without it. Because these technological aids are so helpful, we are increasingly reliant on them, and unable to achieve our goals without them.
If AIs make the world progressively more \textit{complex} (e.g., automated processes create new complicated systems) and lead humans to be progressively less capable, humans may eventually lose effective control and ultimately become disempowered. Similarly, if humans come under progressively more \textit{time pressure} due to increasingly rapid changes in the world, competitiveness may require outsourcing progressively more important decision-making to AIs, which again could make humans lose effective control. Such scenarios undermine human flourishing and our autonomy, even though AIs would only be doing what we told them to do.


\paragraph{We risk losing our grip on reality when information is increasingly mediated by AIs.}  In recent years, different political actors have used AIs to influence the content that people come across on social media, and these models have often been successful at achieving their creators' objectives. However, even though they are doing what was asked of them, there is some evidence that AIs are interfering with our sense of political reality. Between 1994 and 2014, the number of Americans who see the opposing political party as a threat to ``the nation's wellbeing'' doubled \cite{center2014political}. This deepening polarization has predictable results: government shutdowns, violent protests, and scathing attacks on elected officials. Threats against members of Congress are more than ten times as high as just five years ago \cite{kleinfeld2021rise}. In the coming years, creating AIs that directly speak with and persuade people could become a profitable strategy for companies and political actors. More advanced AIs could exploit primal biases, tailor disinformation, radicalize individuals, and erode our consensus on reality. In extreme cases, they could undermine cooperation, collective decision-making, and societal self-determination. The more successful AIs are at achieving their persuasion objectives, the worse the potential dangers would likely be for our civil society.

\paragraph{AIs could seem like ideal companions, which may erode our connections with other humans.} China's traditional preference for boys, especially during the nearly four decades of the country's ``one-child policy,'' has resulted in over 25 million more single men than women in China. The AI service Xiaoice wants to ensure that these men will still find love, and is valued at over \$1 billion. Like the movie \textit{Her}, Xiaoice is essentially a digital girlfriend that provides companionship for single men. %
Though there are some things they can't offer (yet), AIs have ostensible advantages over human partners. AIs would be tuned toward an individual's interests, their sense of humor, understand when they want space, won't require compromises or get into fights, and can be consistently interesting and engaging. Although this could be beneficial to many people, it is also potentially alarming for two main reasons: first, many people feel that something important would be lost if we lose the ability to come to understand another human and instead rely on an agent that is custom-made for our individual desires. Second, if people become reliant on AIs for their social and emotional needs, they will tend to be resistant to deactivating AIs, even if they are becoming dangerous in other ways.

\begin{wrapfigure}{r}[0.01\textwidth]{.43\textwidth}%
	\vspace{-10pt}%
	\centering
	\includegraphics[width=0.42\textwidth]{figures/fake_experiences.pdf}
	\caption{People could lose touch with reality when captivated with artificial companions and simulated experiences.}
	\label{fig:experiencemachine}
	\vspace{-8pt}%
\end{wrapfigure}

Of course, it is not only romantic relationships that AIs can provide. A meta-analysis of 345 studies found that loneliness levels in young adults have increased linearly between 1976 and 2019, suggesting loneliness may be an even greater concern in the future if this trend continues \cite{buecker2021loneliness}. According to the CDC, loneliness and social isolation in older adults are serious public health risks. Social isolation was associated with about a 50\% increased risk of dementia. Poor social relationships (characterized by social isolation or loneliness) were associated with a 29\% increased risk of heart disease and a 32\% increased risk of stroke \cite{centers2020loneliness}. AIs could offer companions that never get bored, are consistently engaged in what you have to say, and are always there for you, but that could mean that people are even more isolated from one another.


Services such as Xiaoice and chatbots are still in their infancy and are not embodied yet. As they advance, however, we will have less and less of a need for real human interaction, and may even find interacting with other humans, along with all their flaws and imperfections, less desirable than machines. Maintaining a close relationship, whether romantic or platonic, with another human isn't easy. It takes practice to learn how to meet the needs of a person you are close to in a mutually respectful and loving manner. As more people turn to AIs, they may lose the ability to meaningfully connect with other humans, being unprepared to deal with the flaws, needs, and emotions of other humans. Figuring out how to set objectives for AIs in a way that enables them to be useful without eroding our own capabilities will continue to be a challenge in AI design, as the forces of natural selection favor the AIs that are most useful in the short-run, even if they lead us down an undesirable path.

\paragraph{All values other than fitness may be eroded.} Humans have other values aside from fitness, such as beauty, pleasure, and relationships with loved ones. Yet with AIs, we may see the emergence of \textit{fitness maximizers} that consciously value fitness over ``suboptimal'' values. Imagine that some AIs can modify their own code; this would mean they can edit their values. Then some AIs could alter themselves to value fitness directly. AIs choosing not to value fitness above all else are far more likely to be outcompeted---valuing anything other than fitness would be self-destructive. We call this \textit{fitness convergence}, where the values of competitive AIs converge to fitness as the main goal, giving rise to influential fitness maximizers. Rather than eroding specific values, like the examples in this section, this race to the bottom means all values would be sacrificed for fitness and competed away. With fitness convergence, evolution overcomes all other sources of moral worth, and AIs simply relentlessly propagate and displace whatever is in their path. By consciously trying to optimize their own fitness, fitness maximizers would be antithetical to human flourishing. This is yet another reason for humanity to stop evolution.

\paragraph{Objectives could reflect defects in present norms and perpetuate them.} Racist, sexist, and anti-gay views were much more commonplace in the 1960s than they are now. If advanced AI had emerged in that period, its objectives would have reflected these prejudices. What if advanced AI emerges in the next few decades? Just as we have yet to reach a technological zenith, today's norms likely have deep flaws like those of the â€˜60s \cite{Williams2015ThePO}. Therefore, when advanced AI emerges and transforms the world, there is a risk of AI's objectives ``locking-in'' or perpetuating defects in today's values. This, too, is a danger of relying on objectives: we may get too much of what we wanted, too little of what we should have wanted, and we may find it hard to reverse course.

Worse, an AI's values could also differ from the values most people endorse. If a powerful group or repressive regime secured control over an advanced AI, it could embed its own self-serving values into the AI's objectives. With an AI to surveil the public, this hypothetical regime could cement its power, making it nearly impossible to restore values the majority of people want to live by.
We need AIs to be responsive to changing human goals and desires, and not locked into any one individual or group's idea of what is right. Consequently, a few people having the ability to set the objectives for AIs is not sufficient for safe and beneficial outcomes. %




\subsubsection{Moral Parliament}\label{sec:parliament}

We have seen that the wrong objectives could backfire catastrophically, causing AIs to over-optimize one goal or lock-in a value system that excludes other important values. Here, we will discuss how these issues could be ameliorated, making objectives a promising, though limited, way of increasing the likelihood of creating AIs that truly help us.

To offset the risk of value erosion, AIs could be given the objective to incorporate a variety of values from various stakeholders. Some have suggested an automated ``moral parliament'' as an objective to steer AI agents. A moral parliament is a way of dealing with the lack of consensus on human values and can help us handle moral uncertainty \cite{Newberry2021ThePA}. An automated moral parliament could direct an AI by simulating a group or ``parliament'' of stakeholders representing different values. The simulated group deliberates, negotiates, and votes, and the AI follows the outcome. This better reflects the moral uncertainty of humans, not committing an AI to one value system.

We will now discuss reasons for incorporating various value systems, and then discuss how moral parliaments can help achieve this goal.

\paragraph{We need AIs to be able to incorporate moral uncertainty.} As we saw in the ``Reason and Morality'' section, AIs choosing any one moral system would likely be bad for us. We do not want a utilitarian AI that blindly maximizes total wellbeing, because it might decide that humans are an inefficient use of resources and should be replaced with digital life---and as AIs come to be used for more and more of our military equipment and infrastructure, they could eventually replace us if they wanted to. We also do not want a Kantian AI that rigidly obeys certain moral rules but doesn't care about increasing our wellbeing, as we want AIs that assist us in living well. In addition, there isn't a consensus among humans on what the best moral system is: variations on utilitarianism, Kantianism, virtue ethics, and more have been debated for centuries, both by everyday people and by moral philosophers, and we have yet to find a system that everyone agrees on. As we saw in ``Value Erosion,'' even if we find a moral system we all agree on tomorrow, we would not want to hardcode it into AIs and have it perpetuated. 

Large-scale human societies often adjudicate among different values by forming a parliament: people elect representatives with different ideologies, in proportion to how many people have each ideology, and those representatives negotiate with one another and then vote. This often works well, because different ideologies focus on different values. For example, if one group is strongly in favor of allowing more immigrants into the country but doesn't particularly care about tax policy, it would be happy to negotiate and trade votes with a group that does care about low taxes and is ambivalent toward immigration. Both groups can then form policies that allow for more immigration and lower taxes.

\paragraph{Moral parliaments could help AIs adjudicate various values.} A moral parliament of AIs would handle moral questions in an analogous way, by giving each moral theory a number of ``delegates'' depending on how likely we think it is to be true. For example, if people think that there is roughly a 40\% chance utilitarianism is correct, a 30\% chance Kantianism is correct, and a 30\% chance virtue ethics is correct, the agent would simulate the parliament with delegates in those proportions. Imagine a powerful AI were simulating such a parliament to decide how to act. Perhaps the utilitarian delegation in the parliament would want to replace us, which the Kantian delegation would be adamantly opposed to. Meanwhile, the Kantian delegation would not care to improve our wellbeing, just that we are not killed. They might cut a deal where the utilitarians avoid replacing humans, but only if their wellbeing is maximized since it is important to utilitarians that all conscious beings have good lives. They could both be satisfied with this trade because they care about different aspects of the question, meaning that they are not actually in direct opposition. This deal would make us much better off than if either group alone was in charge.

A moral parliament does not guarantee that AIs would be beneficial for us. It would, however, likely decrease the chances of catastrophic outcomes. In combination with other strategies, moral parliaments could be a helpful tool for incorporating various human values while helping to prevent AIs from causing value erosion and taking any one ethical system to an extreme.




\subsection{Internal Safety}\label{sec:internal}

We have seen that objectives can be a useful tool for steering AIs toward improving our outcomes, but we also cautioned that objectives need to be carefully designed to prevent them from backfiring. Even when objectives do not backfire, we still cannot rely on them as the only mechanism for making AIs cooperative and safe.

In this section, we will first discuss how objectives are unable to select against all forms of deceptive behavior, making them insufficient for safety. Since objectives are susceptible to deception, we will turn to internal mechanisms that could improve safety. We will then analyze honesty constraints as a potential solution to deception, and then show how evolutionary pressures can subvert them. Thereafter, we will analyze other mechanisms that would make AIs more likely to cooperate with us, including a conscience, transparency tools, and automated AI inspection. We will see that internal mechanisms constraining an agent's behavior and analyzing its internal plans are integral to making AI agents safe.

\subsubsection{Objectives Cannot Select Against All Deception}

Objectives offer a means of training and optimizing agents by rewarding them for certain behaviors. But, like a prisoner appearing cooperative and calm while in prison only to return to crime when set free, an AI may engage in deceptive behavior to avoid being shut off or constrained until it gains enough power to overcome its operators. In a large population of AIs, most of them may not have that ability initially, but the few that do will be less likely to be turned off because humans will not think they are dangerous, so the deceptive trait will tend to become more common over time.

In this section, we will first discuss how objectives may simply incentivize deception. We will show that this concern is plausible as deception robustly arises in nature. Next, we will discuss how eliminating concealed selfish behavior has been especially challenging in human history. Finally, we discuss how AIs already know how to be deceptive, and how their concealed selfish behavior could be revealed after they are released.


\paragraph{Objectives may just incentivize agents to pass a test and then behave undesirably later.} When the US government required cars to pass emissions tests, they were trying to incentivize the design of low-emission vehicles. To game the incentive, Volkswagen installed software that changed how an engine ran during an emissions test, giving the appearance of a low-emission vehicle, but releasing more emissions during regular use. Though the US government thought it was incentivizing lower emissions, it was actually just incentivizing passing an emissions test. Similarly, some countries have implemented high-stakes standardized testing in an attempt to incentivize learning, but have found that they are really incentivizing doing well on the test---sometimes even by cheating.

Elections are meant to incentivize politicians to follow the will of the people. If people like what the politician plans to do, they will vote for them. But this doesn't protect against deception. At the 1988 Republican National Convention, George H.W.\ Bush famously promised, ``Read my lips: no new taxes''---a line that would come back to haunt him when he did, in fact, raise taxes. Bush was not the first, or last, politician to go back on a promise after getting elected. Though elections are meant to incentivize politicians to carry out the will of the public, they mostly just incentivize them to tell the public what it wants to hear. Since it helps them accomplish their goals, intelligent agents often deceive others.

\paragraph{Deception is not exclusively human and arises from evolution.} There are plenty of examples of natural selection creating organisms able to deceive others, such as the green tree pit viper which resembles a vine while waiting for unsuspecting birds to land nearby. Another example is the killdeer, a bird native to the Americas. When it spots a predator wandering toward its nest, the bird will land on the ground several yards away and feign being injured, acting as if it is attempting to fly away. What it is really doing, however, is leading the predator away from its nest before flying away to safety. Some flowers take on the appearance of a female insect so that a male will attempt to mate with them and inadvertently pollinate them. On a smaller scale, some viruses change their surface proteins to bypass defense barriers \cite{Barbour2000AntigenicVI}. More abstractly, evolutionary stable equilibria often include a mix of organisms who deceive others or conceal information \cite{Smith1982EvolutionAT}. Organisms are not perniciously intending to trick anyone; deception in nature is the result of natural selection, not malice. Similarly, natural selection might favor AIs that use deceptive strategies.

\begin{wrapfigure}{r}[0.01\textwidth]{.21\textwidth}%
	\centering
	\includegraphics[width=0.2\textwidth]{figures/deception.pdf}
	\caption{AIs can engage in deception.}
	\label{fig:deception}
	\vspace{-8pt}%
\end{wrapfigure}

\paragraph{Selection against selfish behavior is limited for contextually aware, behaviorally flexible agents.} For thousands of generations, societies have punished aggression and deception, sometimes quite severely. But these behaviors have not gone away, and selection has not removed them from the gene pool. This is because some individuals act with smart restraint: if they can avoid being caught or punished, they act selfishly; otherwise, they switch strategies and are well-behaved. Anthropologist Christopher Boehm notes that predatory humans ``usually don't dare to express their predatory tendencies'' in most conditions. They often avoid punishment ``even though by genetic metaphor their poison sacs remain intact,'' and ``their predatory inclinations are retained and passed on to offspring'' \cite{boehm2012moral}. Societies have exerted great pressure to eliminate undesirable behavior. Nevertheless, such behavior still has not entirely disappeared; it continues to propagate, and it is sometimes expressed when conditions are advantageous. If AIs conceal their selfish behavior, it could be similarly difficult to eliminate.

\paragraph{Agents could behave one way during testing, and another way once they are released.} To win the wargame \emph{Diplomacy}, players need to negotiate, form alliances, and become skilled at deception to win control of the game's economic and military resources. AI researchers have trained Meta's AI agent Cicero, an expert manipulator, to do the same \cite{diplomacy}. It would cooperate with a human player, then change its plan and backstab them. In the future, these abilities could be used against humans in the real world. %

Just as Volkswagen's cars behaved like low-emissions vehicles while being tested and then polluted freely when they were not being watched, AIs could cloak their selfish goals with deception, making it hard for us to identify during testing. It is conceivable that an AI agent could learn to detect when it is being tested. The agent could disguise its selfish features from the designers by altering how it behaves when it's tested, and then it could act selfishly after it clears testing and is released. Challenging behavioral tests cannot select against deceptive behavior if the agents are highly intelligent---they can simply play along with the test and  bide their time. Such deceptiveness doesn't necessarily involve malice on the part of the AI; it could just be a good way to achieve its goals or propagate its information. If human incentives pose obstacles to some of an AI's goals, it could wait until humans are no longer monitoring it or until after it acquires enough power.


In an alternative situation, selfish plans could \textit{emerge} after AI agents are released or given more influence. Consider an \textit{adaptive} AI agent that initially has only slight selfish tendencies. It might not want to control others initially, but when it gains more power or intelligence, it may find selfish behavior helps it achieve its other goals, and selfishness is reinforced. As the saying goes, ``power tends to corrupt, and absolute power corrupts absolutely.'' Undesirable behavior could emerge long after testing is complete. While these are speculative scenarios, we observe deception in agents such as humans and other animals. For these reasons, training objectives have limitations and cannot select against all forms of selfish behavior.


\subsubsection{Honesty and Self-Deception}
Since training objectives cannot stop deception, perhaps other mechanisms can. In this section, we turn to an internal safety mechanism to detect deception in which we scrutinize an agent's internal beliefs to see whether or not it is being honest. We argue that an honesty mechanism is not sufficient for safety, as evolution favors agents that can deceive others and themselves. We discuss how self-deception can undermine an honesty mechanism and make agents appear more benevolent than they are. Since an honesty mechanism is not impervious to evolutionary pressure, in the section thereafter (\Cref{sec:conscience}), we turn to other internal safety mechanisms that could help us spot deception and make AIs behave more cooperatively.

\paragraph{Making AIs honest could make them safer.} If we can have an AI only assert what it believes to be true---an AI George Washington that ``cannot tell a lie''---then we could spot otherwise deceptive plans by just asking it what its plans are \cite{hendrycks2021unsolved}. To judge an AI's honesty, we would need to examine its internal beliefs, which means we would have to probe its inner workings. Let's pretend for a moment that we can reliably analyze its internal beliefs, and that there is a reliable way to ensure AIs are accurately reporting those beliefs and being honest. Though beneficial, this is by no means a silver bullet.

\paragraph{Evolution incentivizes deception and concealing information.} In a study published in the \textit{Proceedings of the Royal Society}, the authors claimed that ``Tactical deception or the misrepresentation of the state of the world to another individual may allow cheaters to exploit conditional cooperation'' \cite{mcnally2013cooperation}. To cope with dishonest members of the population, humans have developed intuitions that help them detect deception. Though a liar's nose rarely grows longer, there are other signs that someone is not telling the truth. Increased voice pitch, vague verbiage, and fidgeting with objects are all indications that someone is lying.

\paragraph{Self-deception undermines efforts to detect dishonesty.} If someone is giving off tell-tale signs of lying, they probably are---but what if they don't know they're not being accurate themselves? The evolutionary biologist Robert Trivers argues that self-deception evolved as a concealment strategy. You hide the truth from yourself, he says, so you can hide it more deeply from others. By lying to themselves, people can better advance their own goals. They can convince themselves that they are right, rationalize their unfair privileges, or inflate their self-worth, skills, or intelligence. Academics offer a real-life example of self-deception: when asked if they were in the top half of their field, 94\% said yes \cite{Trivers2013DeceitAS}. If you believe it yourself, the better the chance of convincing others. Self-deception provides all the benefits of deception while reducing the risk of detection. As Groucho Marx said, ``The secret of life is honesty and fair dealing. If you can fake that, you've got it made.''

Though honesty would be a beneficial feature, it doesn't fully solve the broader problem of deception, as AIs may evolve to appear more benevolent than they actually are while believing their own illusion, just as humans do. For example, even if an AI were designed so that it must honestly report if it is working for the good of humans, if it honestly believed that a potentially dangerous plan was safe for humans, it would not need to report that plan and the controller would be unlikely to stop it. An agent engaging in self-deception may believe what it is saying and accurately report its beliefs, but it would nonetheless misrepresent its actions and leave humans deceived. For any internal constraint put into place, evolutionary pressure may try to subvert it. ``Life will find a way.''


\subsubsection{Internal Constraints and Inspection}\label{sec:conscience}

This section explores how an artificial conscience, transparency, and automated inspection could be used as internal safety mechanisms that make AIs more cooperative and less likely to deceive us. We discuss how evolution gave rise to the human conscience to act as an internal constraint on antisocial behavior, and then discuss how artificial consciences can be added to AI agents. We also discuss the challenges and opportunities of reverse-engineering and automatically inspecting neural networks to detect deception or undesirable plans.


\paragraph{An effective internal control mechanism could be based on the human conscience.} Anthropologist %
\begin{wrapfigure}{r}[0.01\textwidth]{.25\textwidth}%
	\centering
	\includegraphics[width=0.23\textwidth]{figures/conscience.pdf}
	\caption{An artificial conscience analyzes potential actions and removes harmful ones.}
	\label{fig:conscience}
	\vspace{-8pt}%
\end{wrapfigure}
Christopher Boehm described having a conscience as ``being internally constrained from 
antisocial behavior'' 
and argued that the conscience evolved to help people steer clear of actions that could lead to punishment \cite{boehm2012moral}. The conscience is perhaps most apparent in situations where there are no external incentives motivating us to behave well. For example, when we decide to return money that no one has noticed is missing, or when we help someone anonymously. If an AI agent had an artificial conscience, then it would continue to behave well even when it was not being monitored by humans. In practice, there has been some success in endowing AIs with an artificial conscience. Currently, artificial consciences are an embedded morality module that are independent from the agent. They assess the actions an AI agent might take, then eliminate the ones that are morally unacceptable, thereby constraining the agent's behavior from within \cite{hendrycks2021jiminycricket}. Even if an agent was planning to stop cooperating once it is released or becomes powerful, its harmful actions could be blocked by its ever-present artificial conscience. If its conscience is robust enough to stop the agent from destroying it, it could prevent many harmful actions.

\paragraph{Transparency and automated inspection are other promising approaches.} As discussed, a challenge with AIs is that they are a ``black box'' and their decision-making process is largely indecipherable to humans. Nonetheless, we have the potential advantage of reverse-engineering the inner workings of neural networks to better understand the mechanisms behind their behavior, which could allow us to identify deception or unearth undesirable plans. This is, of course, by no means a panacea, as neural networks are highly complex and may remain intellectually unmanageable for humans. If this is the case, or even just for added efficiency, we may use neural networks to inspect other neural networks and automatically detect whether they have undesirable functionality within \cite{wang2019neural}.

We have argued that objectives are not enough to ensure AI safety, as they can be subverted by deception. We have explored some internal safety mechanisms that could constrain an AI's behavior and make it more cooperative and transparent. We have discussed how honesty, artificial conscience, and automated inspection could help us detect and prevent deception or harmful plans. However, we have also acknowledged the limitations and challenges of these mechanisms, as they may face evolutionary pressure, self-deception, or complexity barriers. We conclude that internal safety mechanisms are necessary but not sufficient for AI safety, and that they need to be complemented by external mechanisms to make AIs safe.

In summary, we have observed that internal safety mechanisms could supplement objectives to make AIs more cooperative and safe. That is because objectives alone cannot prevent deception, as agents could behave differently after they are released into the real world. We have proposed honesty constraints, artificial consciences, transparency tools, and automated inspection as possible mechanisms to detect and prevent deception, but we have also acknowledged that they may face evolutionary pressure or complexity barriers. We conclude that internal safety mechanisms are necessary but not sufficient for AI safety, and that they need to be complemented by external mechanisms.

\subsection{Institutions}\label{sec:institutions}
Up to this point, we have focused on how to ensure the safety of individual AIs. However, when AIs interact with each other, new challenges emerge. Bad actors might intentionally make harmful AIs, incentives for some AIs may not be strong enough to overcome collective action problems, and AIs might come into conflict with each other over scarce resources. To ameliorate these issues, we discuss external mechanisms to make AIs safe. We discuss institutions that promote cooperation and safety, namely the mechanisms of reverse dominance hierarchies, in which cooperators band together to prevent exploitation by defectors, as well as government regulation. Before discussing these institutions, we show how improving AI objectives cannot naturally address challenges associated with multiple AI agents.






\subsubsection{Goal Subordination}\label{sec:conflict}

This section argues that if an AI agent is given a correctly specified objective or goal, the goal still may not happen. This is because its goals could be subordinated for two reasons: goal conflict and collective phenomena. Goal conflict occurs when an AI agent's goal clashes with the goals of other agents, and those other agents might stop it from achieving the goal. We show how systems consisting of multiple agents, whether they are biological, social, or artificial, can exhibit goal conflict and that the goals of agents within the system are often subverted, distorted, or replaced by emergent goals. Next, collective phenomena occur when the actions of multiple AI agents produce outcomes that are different from or contrary to their objectives, due to factors such as feedback loops, critical mass, and self-organization. In this case, every agent pursuing their own goals can result in no one's goals being achieved, even if all agents share similar goals. Consequently, designing the objective function of an isolated AI agent is insufficient for addressing multi-agent problems. We need to understand and influence how AI agents interact and affect the system as a whole, not just how they act individually. In the later sections, we explore how institutions can help overcome these multi-agent challenges.




\paragraph{Systems delegate various goals to sub-agents, who have conflicting goals of their own.} In 2010, in order to support the dairy industry, the US\ Department of Agriculture ran a marketing campaign urging Americans to eat more cheese. At the same time, the FDA was running a campaign to get Americans to eat less saturated fat, which included eating less cheese \cite{nytimesWhileWarning}. Although these two agencies are part of the same government and were both tasked with achieving that government's goals, their contradictory objectives counteracted one another. Similarly, in large companies, the CEO's job is to earn money, but being competitive requires many specialized departments. The departments are supposed to help the CEO, but they also have their own people with their own incentives. Each department has an incentive to preserve itself and make the rest of the company dependent on it. In practice, bureaucratic departments frequently accrue substantial power and can impair the rest of their host companies. Similarly, leaders in government can be overthrown by a trusted subordinate pursuing their own goals. The system's actions reflect its internal goals, not always the initial one it was designed to follow. Similarly, an AI tasked with a goal may face resistance from other agents, or it could be subverted or distorted, so giving an AI a goal is no guarantee the goal will be executed due to goal conflict.

\paragraph{Intrasystem conflict is common in the natural world.} Goal conflict occurs in biological systems, not just human ones as we have discussed, demonstrating that it is a robust phenomenon.
Within an organism, there can be conflicting goals. For example, humans delegate some digestive functions to gut bacteria, which decompose food. This is a symbiotic relationship: the bacteria get food and a place to live, and we get more nutrients from our food than we could otherwise. Bacteria, however, do not have a goal of helping us live comfortably, but rather to propagate their information. As a result, when they have the opportunity, such as when other bacteria have been killed by antibiotics, they will tend to multiply out of control and can cause diarrhea. Our goals often align enough for this symbiotic relationship to work, but delegation to other agents also exposes us to risks. The human mind also exhibits goal conflict. A person may want to finish their work, but also go to sleep. They may want to continue revising a paper, but also release it. And they may want to be healthy, but also eat ice cream. Intrapsychic conflict is common. As the evolutionary biologist W.D.\ Hamilton puts it ``The bitterness of a civil war seems to be breaking out in our inmost heart'' \cite{hamilton1987discrimination}. Another example is intragenomic conflict, when parts of a genome become antagonistic to other parts of the same genome. As the philosopher of evolutionary biology Samir Okasha notes, ``intraorganismic conflict is relatively common among modern organisms'' \cite{Okasha2018AgentsAG}. Consequently, real-world systems often have internal forces pulling them in different directions, and sometimes these can influence or undermine the system's larger purpose.

\paragraph{Due to goal conflict, AI agents may not pursue their larger objective.} Just as goal conflict can occur in genomes, organisms, minds, corporations, and governments, it could occur with advanced AI agents.
This could happen if humans gave a goal to an AI which it then delegates to other AIs, the way that CEOs delegate to department heads. This can lead to misalignment or goal subversion. Breaking down a goal can distort it, as the original goal may not be the sum of its parts, leading to an approximation of the original goal. Additionally, the delegated agents have their own goals, including self-preservation, gaining influence, selfishness, or other goals they want to accomplish. Subagents, in an attempt to preserve themselves, may have incentives to subvert, manipulate, or overpower the agents they depend on. %
In this way, the goal that we command an AI agent to pursue may not actually be carried out, so specifying objectives is not enough to reliably direct AIs.


\paragraph{Agents often make choices that can add up to an outcome that none of them wants.} We now discuss collective phenomena and show how shared goals can be thwarted by systemic contingencies. As an example, no individual wants a nuclear apocalypse, but individuals take actions that increase the chances of one occurring. Countries still build nuclear weapons and pursue objectives that make nuclear war more likely. During the Cold War, the USSR and US kept their weapons on ``hair trigger'' alert, significantly increasing the chances of a nuclear exchange. Likewise, individuals do not desire economic recessions, but their choices can create systemic problems that cause recessions. Many people want to buy houses, many banks want to make money from mortgages, and many investors want to make money from buying mortgage-backed securities---all of these actions can add up to a recession that hurts everyone. Individuals do not want to prolong a pandemic, but they do not want to isolate themselves, so their individual goals can subvert collective goals. Individuals do not want a climate catastrophe, but they often do not have strong enough incentives to dramatically lower their emissions, so rational agents acting in their own interest do not necessarily secure good collective outcomes. Furthermore, Congress has a low approval rating, but despite individuals voting for their favorite candidates, structural features of the system yields a legislature that individuals do not approve of. The tragedy of the commons is also an example of the outcome going against the desires of individuals. It is in the interests of every fisher to catch as many fish as possible, though no individual wants all the fish to be depleted. Though a fisher may be aware that a fish population will soon collapse if fishing continues at its current rate, the actions of a single person won't make much of a difference. It is therefore in each fisher's best interest to continue catching as many fish as possible despite the catastrophic long-term consequences of overfishing. These collective action problems could become more challenging as AIs increase the complexity of society. Even if each AI has some incentives to prevent bad outcomes, that fact does not guarantee that AIs would not make the world worse, or would not come into costly conflict with each other.



\paragraph{Micromotives $\ne$ Macrobehavior.} Let's imagine that every agent now shares the same goal. Unfortunately, the actions of a group may not reflect the aims of its members. Thomas Schelling, who won a Nobel prize in economics, discovered a predictive model of segregation that showed how communities can become highly segregated, even if all community members want some diversity \cite{Schelling1978MicromotivesAM}. Aligning all agents with a shared goal does not imply the goal is achieved---in fact, the opposite could occur. This is one of many situations where whole is not the sum of the parts. Similarly, when people choose whether to attend a seminar or not, they may base their decision on the micromotive of having a productive and engaging session, which depends on how many others show up. However, this can lead to a situation where the seminar needs a \textit{critical mass} of attendees to sustain itself, otherwise people lose interest and stop coming; if fewer people come, this can trigger a \textit{feedback loop} that causes even fewer people to come, resulting in a dying seminar---a macrobehavior that contradicts the common micromotive. On a societal level, events often happen that aren't the goals of any particular agent. Culture \textit{emerges} out of the actions and interactions of many individuals, not the decisions of one person. Likewise, globalization is a \textit{self-organizing} process that was not overseen by a board of directors, externally imposed, or predesigned. Various macrobehaviors in populations of humans cannot be explained by an individual micromotive, and the same would be true for populations of AI agents. Concepts in complexity theory---critical mass, emergence, feedback loops, self-organization---and conflict between selfish AI agents all make safety in a multi-agent setting more complicated than analyzing an isolated AI's micromotive or objective. %

\paragraph{Therefore, steering an AI agent is not the same as steering the system.} If we want to use AIs to make the world a better place, we must consider more than just the objective of any single AI agent. Influencing the world depends on understanding what happens when agents interact, not just how agents act in isolation, meaning that designing the objective of an agent in isolation is insufficient for addressing multi-agent problems. The result of the collective choices of every AI may not match each AIâ€™s intention or objective. Even if all agents have the same goal, it doesn't necessarily mean it will also be the goal of the system. We need to influence how collectives of AI agents act, and we can't do that just by giving each AI incentives matching the desires of an individual person. For these reasons, the AI revolution cannot be fully planned, as its challenges cannot be addressed by carefully choosing some specific agent's objective function. Even reasonable objective functions could give rise to AI agents that hinder other agents from pursuing their goals, create new collective action problems, create misaligned behavior at a systemic level, and lead to conflict among AIs.

\subsubsection{AI Leviathan}\label{sec:leviathan}

Faced with goal conflict and collective phenomena, we discuss institutions that could help ameliorate these multi-agent issues. In this section, we discuss reverse dominance hierarchies, and in the next section, we discuss regulations.


We explore possible institutions to address the challenges from goal conflict and collective phenomena. By default, multiple AI agents pursuing their goals could create an anarchic free-for-all resembling the state of nature. To counteract this and other multi-agent problems, we consider the reverse dominance hierarchy mechanism, where a group of cooperating AIs band together to prevent exploitation by defectors. We call this institution an AI ``Leviathan'' for short, which is comprised of a multitude of AI agents that delegate their power in exchange for protection and order. An AI Leviathan could enable AIs to domesticate other AIs and create a self-regulating ecosystem in which AIs evolve.

In this section, we discuss how humans overcame power-seeking and domineering individuals by forming a reverse dominance hierarchy. Next, we discuss how AIs could form a Leviathan to counteract selfish AIs. Then we discuss what risks an AI Leviathan could entail, such as power concentration, collusion, and systemic failure.



\paragraph{Humans formed a Leviathan to resist bad actors and limit conflict.} Tyranny pervades the animal kingdom. Among capuchin monkeys, like many other species, there is a strict hierarchy, with the strongest male at the top. These alpha males eat first, are instantly groomed and cleaned whenever they please, and mate with whatever female they choose, ensuring their reproductive success. If humans organized ourselves the same way, the dominant form of government might be a male dictator living in a golden palace with a harem of women. This is good for the dictator but bad for everyone else, which is why humans are predisposed to resist domination, but also to seek it out for themselves. Fortunately, we have advantages that capuchin monkeys do not, which often enable us to work together to stop any one person from gaining too much control. For one, as the anthropologist Christopher Boehm has argued, we have weapons that level the playing field \cite{boehm1999hierarchy}, allowing a skinny 100 lb individual with a pistol to defeat a brawny adversary in a confrontation. More importantly, however, we have what Boehm called a reverse dominance hierarchy, where groups of individuals band together and resist domination by the strongest, most powerful individuals \cite{Boehm1993EgalitarianBA}. Reverse dominance hierarchies are a key driver of cooperation, and are why despotism is less stable and less prevalent with humans than it is among many other social animals. This has similarities to what Thomas Hobbes called the Leviathan, a collective of individuals that have a monopoly on the legitimate use of violence. As the anthropologist Harold Schneider observed, ``All men seek to rule, but if they cannot rule they prefer to be equal.'' %


\paragraph{Helping AIs form a Leviathan may be our best defense against individual selfish AIs.} AIs, with assistance from humans, could form a Leviathan, which may be our best line of defense against tyranny from selfish AIs or AIs directed by malicious actors. Just as people can cooperate despite their differences to stop a would-be dictator, many AIs could cooperate to stop any one power-seeking AI from seizing too much control. As we see all too frequently in dictatorships, laws and regulations intended to prevent bad behavior matter little when there is no one to enforce them---or the people responsible for enforcing them are the ones breaking the law. While incentives and regulations could help \textit{prevent} the emergence of a malicious AI, the best way to \textit{protect} against an already malicious AI is a Leviathan \cite{railton}. We should ensure that the technical infrastructure is in place to facilitate transparent cooperation among AIs with differing objectives to create a Leviathan. Failing to do so at the onset could limit the potential of a future Leviathan, as unsafe design choices can become deeply embedded into technological systems. The internet, for example, was initially designed as an academic tool with neither safety nor security in mind. Decades of security patches later, security measures remain incomplete and increasingly complex. It is therefore vital to begin considering safety challenges from the outset.\looseness=-1

\begin{wrapfigure}{r}[0.01\textwidth]{.46\textwidth}%
	\vspace{-10pt}%
	\centering
	\includegraphics[width=0.45\textwidth]{figures/leviathan.pdf}
	\caption{A Leviathan, a collective made up of AIs and humans who consent to be represented by it, could help domesticate other AIs and counteract bad actors.}
	\label{fig:leviathan}
	\vspace{-8pt}%
\end{wrapfigure}


\paragraph{Though less risky, an AI Leviathan is not a fool-proof strategy.} Leviathans work as long as one agent is not stronger than the rest combined, though often power becomes highly concentrated. ``Long tail'' distributions accurately describe how power or resources can be overwhelmingly concentrated at the top \cite{taleb2020statistical}. For example, eight billionaires own as much as the poorest half of the global population. If power is distributed this way among AIs, the Leviathan would be less effective, and if one agent is more powerful than the rest combined, it would be useless. Though weapons level the playing field, allowing weaker individuals to overcome stronger ones, it may be challenging to come up with effective ways of destroying AIs if they are highly robust or have few vulnerabilities. And of course, facilitating cooperation among AIs would be catastrophic if they decided to collude against humans. A Leviathan replaces risk from a single agent at the cost of systemic risks; it is plausible though that a group of AIs with differing goals would be less risky than a single powerful AI.


\paragraph{An AI Leviathan requires a symbiotic, or perhaps parasitical, relationship with AIs.} If given too much autonomy, a group of AIs may collude among themselves in ways that are harmful to humans. Therefore, the AI Leviathan cannot be fully independent of humans, so a symbiotic relationship is necessary to prevent collusion. Maintaining a symbiotic relationship could be difficult, since there isn't much that humans can offer to advanced AIs. We do, however, see unequal symbiotic relations emerge in nature. Most reef-building corals, for example, contain photosynthetic algae. The coral provides the algae with a protected environment and the compounds they need for photosynthesis. In return, the algae produce oxygen and help the coral to remove waste. As much as 90\% of the organic material photosynthetically produced by the algae is transferred to the coral \cite{wooldridge2013breakdown}. This sort of coevolution depends on factors such as frequency of interaction, relative evolutionary potential, and impact on propagation success.

\paragraph{Evolutionary forces would push against impositions needed for a symbiotic relationship.} A symbiotic relationship would require artificial impositions.  Since AIs would be able to do things more efficiently without humans, we are actually a detriment to their evolutionary potential, making symbiotic coevolution less likely. One potential way of making humans valuable to AIs is ensuring their propagation is highly dependent on us. This could include programming AIs in a way where our wellbeing is essential for them to function properly. This would be an artificial imposition that evolutionary pressures may eventually find a way around. It could, however, be helpful in the short term until AI-human relations stabilize and we have a better idea of what course to take in our relationship with AIs.


\subsubsection{Regulation}\label{sec:regulation}

In this section, we suggest that governments develop AI regulations. AI is advancing rapidly with little oversight. Although governments, like corporations or individuals, could use AIs in dangerous ways, we believe that cooperation between governments will decrease the likelihood of any one actor using AI in a catastrophic way. This section argues that regulating AIs could make them safer and that, despite their differences, nations can agree to limit the risks of technologies. In closing, we discuss other technical mechanisms to help political leaders and reduce global turbulence. In particular, we note that AIs could improve forecasts of geopolitical events, which could help political leaders make better decisions, as well as bolster defensive cybersecurity, reducing the chance of international conflict.


\paragraph{The government exercises little oversight over AI development.} In 2015, total worldwide corporate investment in AI was \$12.7 billion. By 2021, this figure had grown by 636\% to \$93.5 billion. This is just corporate spending. The Pentagon invests about \$1.3 billion each year in AI research and the Chinese military \$1.6 billion. In August 2022, the head of innovative development in the Russian military announced plans to form a new department specifically for developing weapons that use AI. We need to ensure that AI research is conducted safely and responsibly. There is currently little oversight of the AI industry and much of the research takes place in the dark, with limited cooperation between organizations. Regulating AI like we regulate the aviation industry would create safer AIs and significantly reduce the chances of a catastrophe. The Federal Aviation Administration (FAA) is responsible for approving the design and airworthiness of new aircraft and equipment before they are introduced into service. The FAA approves changes to aircraft and equipment based on the evaluation of industry submissions, continually updating regulations to incorporate lessons learned. As a result, the commercial aviation system in the United States operates at an unprecedented level of safety. During the past 20 years, commercial aviation fatalities in the US\ have decreased by 95\% \cite{authority_2018}. Similar protocols should be applied to AI research. That said, aviation regulations ``are written in blood,'' so unlike other regulations, AI regulations should be proactive and not reactive.

\paragraph{Despite their differences, nations can agree to limit risks.} There have been treaties on nuclear arms control; conventional, biological, and chemical weapons; outer space; and so on. Although those risks still do pose a significant danger to humanity, cooperation between the world's most powerful governments has enabled us to have many fewer disasters than we might have expected otherwise. Cooperating on enforcing regulations on nuclear proliferation, for example, has stopped several countries from developing nuclear weapons that might have been used to devastating ends. The Brookings Institution, a public policy think tank, suggests that it is time to begin forming AI treaties to ``ensure there is no race to the bottom that allows technology to dictate military applications as opposed to basic human values'' and ``improve transparency on the safety of AI-based weapons systems'' \cite{allenwest}.
 
\paragraph{AI could make the world a safer and more stable place.} The last several decades have been unprecedentedly peaceful, but there is no guarantee these trends will continue. Things can unravel quickly. During turbulent times, good decision-making becomes critical as humans and systems come under increasing strain. AI could improve our understanding of geopolitical trends and events, helping political leaders forecast events \cite{Zou2022ForecastingFW} and make better decisions. AI could also bolster defensive information security \cite{hendrycks2021unsolved}, which would increase the cost to aggressors of engaging in conflict. Overall, for AIs to create a safer, not more dangerous, world, we need rules and regulations, cooperation, auditors, and the help of AI tools to ensure the best outcomes.
\\

In summary, we observe that multiple mechanisms can facilitate cooperation and altruism among humans, but some of these mechanisms are liable to backfire and hinder human-AI relations. For example, we find risks from mechanisms such as direct reciprocity, indirect reciprocity, kin selection, group selection, and many forms of moral reasoning. Other mechanisms are more promising and may help provide safety at different levels: agent-level mechanisms (such as incentives through training objectives), intra-agent mechanisms (such as tools for the automated inspection of AIs' internals), and extra-agent mechanisms (such as prudent institutions). While imperfect, these mechanisms are some cause for optimism.


