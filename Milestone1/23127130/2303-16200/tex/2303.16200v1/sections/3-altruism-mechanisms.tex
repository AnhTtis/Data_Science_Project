\section{Natural Selection Favors Selfish AIs}\label{sec:altruism}

The previous section concludes the main argument of this paper. Readers could skip to the conclusion (\Cref{sec:conclusion}), or read the following two sections for an examination of counterarguments and remedies. In this section, we will examine some possible arguments for the claim that altruistic AIs will naturally be more fit than selfish ones, and we argue that mechanisms pushing toward altruism are unlikely to help and may even backfire.


\subsection{Biological Altruism and Cooperation}
In nature, organisms often compete to the death, eating one another or being eaten. But there are also many examples of altruism in nature, where one organism benefits another by reducing its own prospects for passing on its genes. On its face, this might seem like an argument that AIs developed by natural selection may be altruistic, cooperative, and not a threat to humans.


\paragraph{A variety of natural organisms can be altruistic, in particular circumstances.} 
% For example, when a predator appears, some monkeys alert the others with loud cries, but this puts them at greater risk of being devoured as the rest flee to safety.
Vampire bats, for example, regularly regurgitate blood and donate it to other members of their group who have failed to feed that night, ensuring they do not starve \cite{carter2020development}. % For example, some monkeys give alarm cries when they see a predator, which makes that monkey more likely to get eaten while alerting the others to escape.
Among eusocial insects such as ants and wasps, sterile workers dedicate their lives to foraging for food, protecting the queen, and tending to the larvae, while being physically unable to ever have their own offspring. They only serve the group, not themselves, an arrangement that Darwin found quite puzzling \cite{darwin1859}. We even see altruism at the cellular level. Cells found in filamentous bacteria, so named because they form chains, regularly kill themselves to provide much needed nitrogen for the communal thread of bacterial life, with every tenth cell or so ``committing suicide'' \cite{nowak2011supercooperators}. %Even in some species of bacteria, bacteria form chains in which some otherwise healthy bacteria die, providing nutrients to those around them.
Insects and bacteria are not altruistic out of love or care for another; their self-sacrifice for the good of others is instinctual. On its face, this may seem like a compelling argument that evolution favors altruism, which might ameliorate concerns about AIs developing selfish traits.

\paragraph{Cooperation and altruism improve human evolutionary fitness too.} Humans are not particularly impressive physically. Pound for pound, chimps are about twice as strong and would stand a much better chance of escaping from a lion. Strategizing and working together, however, turns a group of humans into an apex predator. As a result, humans are naturally cooperative. From childhood through old age, in societies around the world, people often choose to help strangers, even at their own expense. % According to the evolutionary psychologist Steven Pinker, ``Human nature is complex. Even if we do have inclinations toward violence, we also have inclination to empathy, to cooperation, to self-control.'' 

\paragraph{However, we should not expect AIs to be altruistic or cooperative naturally.} Since organisms can be altruistic, AIs could too; the nature of nature is not nasty, brutish, and short but cooperative, harmonious, and nurturing---or so the argument goes.
To evaluate this argument, we must understand how altruism and cooperation emerge. In the following sections, we decompose cooperation and altruism into various mechanisms. We discuss the most prominent mechanisms \cite{Nowak2006FiveRF,boehm2012moral,pinker2012better,page2018model,Stearns2007AREWS}, so we will examine direct reciprocity (cooperate with an expectation of repeated interaction); indirect reciprocity (cooperate to improve reputation); kin selection (cooperate with genetic relatives); group selection (groups of cooperators out-compete other groups); morality and reason (cooperate since defection is immoral and unreasonable); incentives (carrots and sticks); consciences (the internalization of norms); and institutions such as reverse-dominance hierarchies (cooperators band together to prevent exploitation by defectors).
While these mechanisms may lead humans to be more altruistic and cooperative, we argue many of these mechanisms will not improve relations between humans and AIs and they may, in fact, backfire. However, the last three mechanisms---incentives, consciences, and reverse-dominance hierarchies---are more promising, and we analyze them in \Cref{sec:counteracting}.

% Nearly all of these mechanisms backfire
% As we will see, benevolent behavior is due to specific mechanisms that won't apply to relations between AIs and humans. They may, in fact, backfire and make things worse for us, if they enhance cooperation between AIs against us. There are a few possible exceptions, which we'll talk about in the final section of this paper. But overall, even if the most optimistic views on nature are correct, it will matter little when dealing with machines.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/mechanisms.pdf}
    \caption{An overview of four mechanisms that facilitate cooperation.}
    \label{fig:mechanisms}
\end{figure}


\subsection{Direct and Indirect Reciprocity}
\paragraph{Direct and indirect reciprocity are two mechanisms that enable cooperation in nature.}  With direct reciprocity, one individual helps another based on the expectation that they will repay the favor. Direct reciprocity requires repeated encounters between two individuals---otherwise, there is no way to reciprocate. Indirect reciprocity is based on reputation: if someone is known as a helpful person, people will be more likely to help them. Reciprocity enables even selfish individuals to cooperate; if an individual helps others, the individual may be directly repaid, or the individual may gain a good reputation and be helped by others.
%If a person helps others, they are more likely to be helped. 
%Helping others makes them more likely to help, % Helping friends often makes people more likely to help, because other people think the favor may be repaid in the future.

\paragraph{Reciprocity only makes sense for the period of time when humans can benefit AIs.} Reciprocity is based on a cost-benefit ratio. A choice to help someone else rather than pursue one's own goals has a cost, and a rational agent would only help someone because of reciprocity if they think it will be worth it in the future \cite{Nowak2006FiveRF}. This is not to say all examples of reciprocity are the result of an explicit cost-benefit calculation. An explicit cost-benefit calculation is not needed if selectively helping others becomes a cultural expectation, a genetic disposition, or a learned intuitive habit. 
% ---helping everyone indiscriminately could be less fit than helping others when it's likely to pay off, so the more fit trait of selectively helping others would become more commonplace. An explicit cost-benefit calculation is not needed if selectively helping others becomes a cultural expectation, a genetic disposition, or a learned intuitive habit. %As we saw with lions and lancet liver flukes, complex behaviors can become widely practiced without the individuals consciously intending to propagate their information. Individuals who help others when it pays off can be more successful than those who help everyone indiscriminately, which means that the trait of helping selectively would propagate more in a population. 
% complex behaviors can become widely practiced without the individuals consciously intending to propagate their information; individuals who help others when it pays off can be more successful than those who help everyone indiscriminately, which means that the trait of helping selectively would propagate more in a population. %means those who are more selectively help others will be more likely to propagate their information. 
%This is not to say all examples of reciprocity are the result of an explicit cost-benefit calculation. As we saw with lions and lancet liver flukes, complex behaviors can become commonplace and practiced without intent; if the behavior's benefits exceeds the cost and helps make their information more likely to propagate, then the trait can become more widespread.  
% As described before with lions and lancet liver flukes, complex behavior does not require conscious calculation or intent; individuals that help others when the benefits surpass the costs will be more successful and propagate their information.
% As described above with selfishness in lions or lancet liver flukes, this does not require an explicit choice or knowledge that this calculation will happen; it simply means that individuals that help others in situations where the benefits exceed the costs will be more successful than those that help others in all situations, which means that those traits will proliferate over time. 
To think about whether AIs would reciprocate with humans, we can consider what it would gain and what it would give up. 
%An AI could ask itself what it gains from cooperating with humans and what it is giving up.
Reciprocity might make sense with AIs that are about as capable as a human, but once AIs are far more capable than any human, they would likely find little benefit from collaborating with us. Humans often choose to be cooperative toward other humans, but are rarely cooperative toward ravens, because we don't have strong reciprocal relationships with them. Cooperating with one another could be beneficial to AIs, so it is reasonable to expect that reciprocity could emerge within a community of AIs. Humans, meanwhile, would not have much to offer in return, ending up left out in the cold.

\subsection{Kin and Group Selection}
\paragraph{Kin and group selection are mechanisms that promote altruism.} Many of the examples of biological altruism described above happen between closely related individuals. Consider a gazelle that spots a stalking lion in the tall grass. It could slink away unnoticed but instead lets out a shriek, alerting others in its herd of the danger while singling itself out as a target. Proponents of kin selection would argue that the gazelle alerted its herd because of the genetic similarities it shares with the other members \cite{hamilton1964genetical}. A gene that causes an individual to behave altruistically toward its relatives will often be favored by natural selection---since these relatives have a better chance of also carrying the gene. Conversely, group selection refers to the idea that natural selection sometimes acts on groups of organisms as a whole. This results in the evolution of traits that may be disadvantageous to individuals, but advantageous to the group. As Darwin put it, ``A tribe including many members who...\ were always ready...\ to sacrifice themselves for the common good, would be victorious over most other tribes; and this would be natural selection'' \cite{darwin1871descent}. If it is generally true that a group of uncooperative agents will do worse, then it is tempting to argue that powerful AIs will tend to be cooperative with humans. 

\paragraph{Humans might suffer if AIs develop natural tendencies to favor their own kind or group.} Firstly, we do not have a close kin relationship with AIs. Preserving humans would not help AIs propagate their own information: we are too different. Mathematically, kin selection only happens when the cost-benefit ratio is greater than relatedness, and that will not be true between humans and AIs. We are far more closely related to cows than to AIs \cite{Elsik2009TheGS}, and we would not like AIs treating us the way that we treat cows. AIs will have far more kinship with one another than with us, and kin selection will tend to make them nepotistic toward one another, not toward us.  If kin selection did play a role in the evolution of AIs, it would likely create bias against us, rather than benevolence toward us.
 
\paragraph{Group selection promotes in-group benevolence, but inter-group viciousness.} Group selection only makes sense when a group is more effective than some subset of the group breaking off. Unless humans add value to a group of AIs, AI-human groups would fail to outcompete groups composed purely of AIs. AIs would likely do better by forming their own groups. In addition, group selection only makes inter-group competition stronger. Chimpanzees regularly display cooperative and altruistic tendencies toward members of their own troop but interact with other troops viciously and without mercy.  AIs are more likely to see one another as part of their group, so they will tend to be cooperative with one another and competitive with us.



\subsection{Morality and Reason}

\paragraph{It is conceivable that smarter and wiser AI agents will be more moral.} As we have advanced as a species, we have discovered truths within fields such as science and mathematics, and we may have also advanced morally. As the philosopher Peter Singer argues in \textit{The Expanding Circle} \cite{singer1981expanding}, over the course of human history, people have steadily expanded the circle of those who deserve compassion and dignity. When we first started out, it included oneself, one's family, and one's tribe. Eventually, people decided that perhaps others deserved the same. Later the circle of altruism expanded to people of different nations, races, genders, and so on. Many believe there is moral progress, akin to progress in science and mathematics, and that universal moral truths can be discovered through reflection and reasoning. %As AIs will be significantly more advanced than humans cognitively, some have speculated that they will be more advanced morally as well.
Just as humans have become more altruistic as they have become more advanced, some think AIs may naturally become more altruistic too. If this is true, as AIs become more powerful, they could also become more moral, so by the time they have the potential to threaten us, they might also have the decency to refrain.

%If this is true, then perhaps by the time AIs are powerful enough to pose any substantive threat, they will also be moral enough not to want to.

\paragraph{However, AIs automatically becoming more moral rests on many assumptions.} AIs developing morality on their own as they gain the ability to reason is certainly possible, and an interesting idea. But it alone isn't enough to guarantee our safety. Believing that any highly intelligent agent would also be moral only makes sense if one has confidence in all of the following three premises:

\begin{enumerate}
\item Moral claims can be true or false and their correctness can be discovered through reason.
\item The moral claims that are really true are good for humans if AIs apply them.
\item AIs that know about morality will choose to make their decisions based on morality and not based on other considerations.
\end{enumerate}

\noindent Although any or all of those premises could be true, betting the future of humanity on the claim that all of them are true would be folly.

\paragraph{Whether some moral claims are objectively true is not completely certain.} Even though some moral philosophers believe that moral claims reflect real truths about the world, the arguments for this view are not decisiveâ€”certainly not enough to stake the future of humanity on. The remainder of this section, however, will argue that even if it is true, this is still not sufficient in guaranteeing the safety of humanity.

\paragraph{The end result of moral progress is unclear.} If moral claims refer to real truths about the world, then there are some moral claims that are true and others that are false. There are universal moral concepts that can be found across all cultures, such as fairness or the understanding that hurting others for no reason is wrong. But there are also areas where various cultures disagree. In the West, for instance, arranged marriages are seen as unethical. In India, where they are perfectly acceptable, people are shocked that Western culture condones putting parents in retirement homes. Among both ordinary people and moral philosophers, there is no consensus about what moral code is best. This means that, if AIs use their superior intelligence to deduce the correct moral ideas, we still do not know what they will believe or how they will behave.

\paragraph{Existing best guesses at morality are often not human-compatible.} It is possible, however, to examine the different moral systems humans have come up with and use them to speculate what moral system AIs might adopt and how it would influence their actions. We can imagine, for example, that AIs use reason to deduce that utilitarianism is correct, meaning that agents ought to maximize the total pleasure of all sentient beings \cite{sidgwick2019methods}. At first glance, this might seem good for humans: a utilitarian AI would want us to be happy. But an extremely powerful utilitarian AI---say, one that controls some of the US military's weapons technology---could also conclude that humans consume too much space and energy, and therefore replacing humans with AIs would be the most efficient way to increase the amount of pleasure in the world.

Alternatively, AIs could have a moral code similar to Kantianism. In this case, they would treat any being that has the capacity to reason always as an end and never as a means \cite{gregor2014immanuel}. While such AIs would be morally obligated to avoid lying or killing humans, they would not necessarily care for our wellbeing or flourishing. Since Kantianism places only a few restrictions on its adherents, we still might not have good lives if the world is increasingly designed by and for AIs.

It is certainly possible that AIs could develop moral principles that prevent them from harming humans. We can imagine an AI basing its morals on a thought experiment such as the ``veil of ignorance.'' Participants are asked to imagine what society they would create, assuming that they are behind a veil of ignorance and do not know what economic class, race, or social standing they will have in society. Philosopher John Rawls argues that since participants do not know where in society they will be placed, they would construct a society in which the worst-off members are still well off \cite{Rawls1971ATO}. Such a Rawlsian social contract could work out well for humanity. But it is far from assured and much could go wrong. AIs might see us similarly to how most humans see cows, excluding us from the social contract and not prioritizing our wellbeing. Humans asked to imagine themselves behind the Rawlsian veil of ignorance rarely consider the possibility that they could become a cow. Moreover, according to Nobel laureate John Harsanyi, the people who design society from behind the veil of ignorance would not aim to benefit the most disadvantaged member, but rather to raise the average wellbeing across all members \cite{Harsanyi1955CardinalWI}. The chances of being the most miserable member of society are low, and one could claim that the overall quality of society is not determined by the most upset or least satisfied member. If this is so, the veil of ignorance results in maximizing the average utility of society's members---a utilitarian outcome---but we earlier established that a utilitarian AI might aim to replace all biological life with digital life. Whether AIs adopt a utilitarian, Kantian, or Rawlsian moral code, AIs aiming to implement an existing moral system could prove disastrous for humanity.

\paragraph{If AIs think a human-compatible moral code is true, they still may not follow it.} Finally, even if AIs did discover a moral code that stipulated it is wrong to harm humans and good to help them, it still might not help. For humans, selfish motivations are often in tension with and outweigh moral motivations, and the same might be true of AIs. Even if an agent is aware of what's right, that does not mean it will do what's right. Ultimately, AIs being more moral than us does not guarantee security.

% In summary, when selfish agents interact with other agents, they can enhance their fitness either by \emph{competing} with other agents, or by \emph{cooperating} with them. Humans who are selfish have many reasons to cooperate with each other rather than always be in conflict and compete. However, with AIs, there simply are not strong reasons for cooperating over competing: the opportunity cost of interacting with us over AIs is high, they are not genetically related to us, and even various forms of moral reasoning does not suggest that they should work toward good outcomes for us. There is an absence of reasons for AIs to cooperate, so it is reason to expect pressure toward conflict and competition.

