\section{Conclusion}\label{sec:conclusion}

% In this work we move from the realm of speculation by establishing that a source of catastrophic risks is highly likely, since evolutionary pressure is certain given the three factors for evolution by natural selection. This leaves only a question of evolutionary pressure's intensity---which could be substantial if variation and competition are---rather than whether catastrophic risk factors will emerge at all. This makes it a question of degree. Since there are many rounds of adaptation, evolution could be intense.

At some point, AIs will be more fit than humans, which could prove catastrophic for us since a survival-of-the-fittest dynamic could occur in the long run. AIs very well could outcompete humans, and be what survives. Perhaps altruistic AIs will be the fittest, or humans will forever control which AIs are fittest. Unfortunately, these possibilities are, by default, unlikely. As we have argued, AIs will likely be selfish. There will also be substantial challenges in controlling fitness with safety mechanisms, which have evident flaws and will come under intense pressure from competition and selfish AIs.

The scenario where AIs pose risks is not mere speculation. Since evolution by natural selection is assured given basic conditions, this leaves only a question of evolutionary pressure's intensity, rather than whether catastrophic risk factors will emerge at all. The intensity of evolutionary pressure will be high if AIs adapt rapidly---these rapidly accumulating changes can make evolution happen more quickly and increase evolutionary pressure. Similarly, the intensity of evolutionary pressure will be high if there will be many varied AIs or if there will be intense economic or international competition. Since high Darwinian pressure is plausible, AIs would plausibly be less influenced by human control, more `wild' and influenced by the behavior of other AIs, and more selfish.

The outcome of human-AI coevolution may not match hopeful visions of the future. Granted, humans have experienced co-evolving with other structures that are challenging to influence, such as cultures, governments, and technologies. However, these have never been able to seize control of the broader world's evolution before. Worse, unlike technology and government, the evolutionary process can go on without us; as humans become less and less needed to perform tasks, eventually nothing will really depend on us. There is even pressure to make the process free from our involvement and control. The outcome: natural selection gives rise to AIs that act as an invasive species. This would mean that the AI ecosystem stops evolving on human terms, and we would become a displaced, second-class species.

Natural selection is a formidable force to contend with. Now that we are aware of this larger evolutionary process, however, it is possible to escape and thwart Darwinian logic. To meet this challenge, we offer three practical suggestions. First, we suggest supporting research on AI safety. While no safety technique is a silver bullet, together they can help shape the composition of the evolving population of AI agents and cull unsafe AI agents. Second, looking to the farther future, we advocate avoiding giving AIs rights for the next several decades and avoid building AIs with the capacity to suffer or making them worthy of rights. It is possible that someday we could share society with AIs equitably, yet by prematurely circumscribing limitations on our ability to influence their fitness, we will likely enter a no-win situation. Finally, biology reminds us that the threat of external dangers can provide the impetus for cooperation and lead individuals to set aside their differences. We therefore strongly urge corporations and nations developing AIs to recognize that AIs could pose a catastrophic threat and engage in unprecedented multi-lateral cooperation to extinguish competitive pressures. If they do not, economic and international competition would be the crucible that gives rise to selfish AIs, and humanity would act on the behalf of evolutionary forces and potentially play into its hands.





% By default, human-AI coevolution may not turn out well. This is because AIs will at some point be more fit than humans, and since there will be a survival of the fittest dynamic at place in the long-run, AIs may outcompete humans and be what survives. We responded to the objection that the fittest AIs would be altruistic toward humans. We also raised doubt that what is fittest will be decisively determined by humans, as there are many challenges in making the fitness of AI agents controlled by safety measures.
% We argued Darwinian pressures will emerge, become intense, and may become dominant.

% Humans could become a second species. Historically, humans have collectively been able to influence the evolution of cultures, governments, and technologies. But as humans are automated and become less and less needed, eventually nothing will really depend on them. As AIs become more and more powerful, the decisive influence humans have always enjoyed may be eroded. The ecosystem of AIs and humans may stop evolving on human terms. In this way, AIs may displace us like an invasive species.



\subsubsection*{Acknowledgements}
I would like to thank Avital Morris, David Lambert, Euan McLean, Thomas Woodside, Ivo Andrews, Jack Ryan, Kyle Gracey, and Justis Mills for their help and feedback.
