\section{Appendix}

\subsection{Contrasting with Prior AI Risk Accounts}
\begin{figure}[h]
\hspace{-37pt}
\begin{booktabs}{colspec={ll},row{odd}={blue9},row{1}={white}}
\textbf{The ``Unreliable Training Causes Misalignment'' View}                        & \textbf{The ``Evolutionary'' View}\\
\toprule
liken advanced an AI agent to an optimizer         & liken advanced AI agents to life\\
AIs will intend to disempower humanity & AIs can have selfish behavior, with or without intent \\
the only relevant AI is one inside the top lab's server room     & there are multiple relevant AI agents acting in the world\\
fanatical optimizer destroys us                & Evolutionary forces erode us\\
dangerous AI agent as idiot savant                   & dangerous AI agents as selfish or an invasive species\\
aligning an AI to a person is all we need     & collective phenomena and malicious agents also matter\\
any amount of misalignment results in doom     & \begin{tabular}{@{}l@{}}some amount of misalignment is inevitable, as evolutionary\\ forces are incessantly influencing humans and AIs\end{tabular}\\
an AI agent seeks power & \begin{tabular}{@{}l@{}}AI agents improve fitness to use more free energy\\ or have their information occupy more space-time volume\end{tabular}\\
instrumental convergence & fitness convergence \\
an AI agent will optimize an objective to the extreme & \begin{tabular}{@{}l@{}}taking objectives to the extreme such as the\\ pursuit of individual power can reduce fitness\end{tabular} \\
prevent an AI from getting loose and suddenly wiping us out & \begin{tabular}{@{}l@{}}prevent humans from becoming a second-class species\\and prevent AI conflict and defection\end{tabular}\\
``solve'' alignment with a monolithic airtight solution & reduce risks with various social and technical interventions\\

\bottomrule
\end{booktabs}
\end{figure}


\noindent In the table above, we contrast a cluster of AI-risk beliefs with our views. We now briefly discuss how this paper relates to previous works.
Prior work has struggled to provide robust reasons why AI agents would likely behave in ways that are antithetical to humans, and their scenarios often rely on AI training procedure flaws or random failures that give rise to adversarial functionality (e.g., they often assume that a training flaw would make AIs come to value the means to accomplishing a goal as the goal itself \cite{Hubinger2019RisksFL}); they rely on fragile mechanisms instead of robust processes. In contrast, the Lewontin conditions establish that natural selection would be present and distort the AI population, which would be antithetical to humans. This brings AI risk from the realm of speculation and makes the risk become a question of degree. %
Additionally, we consider the concepts of fitness and multiple AI agents to analyze the plausibility and tradeoffs of different AI behaviors. This differs from much of the prior research, which has physical or computational limits as the only constraints, which has given researchers license to jump to very extreme scenarios. For example, previous work considers a powerful AI with the goal of maximizing paperclips \cite{Bostrom2014SuperintelligencePD} and conclude it would work toward taking over the world---however, the tradeoffs imposed by a multiagent environment would mean that such an AI would not be very fit in an environment where other AIs do not have such senseless obsessions. By incorporating more tradeoffs, we help make the discussion more realistic.












\subsection{Technical Clarifications}\label{sec:clarifications}
\paragraph{When describing selfishness, we often adopt an information-centered view.} We refer to information propagation and not wellbeing when describing selfishness and altruism in this document. Additionally, for simplicity, we often describe AIs that are altruistic towards other AIs but not humans as selfish. This is because altruism to similar individuals can be seen as a form of selfishness, from the information-centered view. An altruist is not necessarily someone who cares about the welfare of another individual, but someone who has similar information with the recipient of the altruistic act. For example, at the individual level, a worker bee that stings an intruder and dies is an altruist, because it sacrifices its own life to protect the hive. From the information-centered view, however, the worker bee is a selfish information carrier, because it shares information with it siblings and the queen. By stinging the intruder, the worker bee increases the chances of survival and propagation of its relatives, and thus of its own information. Altruism can therefore become selfishness if we move from analyzing individual agents to the information-centered view. %


\paragraph{Selfishness does not necessarily entail collusion, power-seeking, or self-preservation.} Though an agent may use different strategies to propagate its information, such as colluding with others, seeking power, and preserving itself, these strategies can sometimes reduce its fitness. We illustrate the differences with a sequence of considerations.

Suppose an agent---whether a human, an AI, or an organism---acts selfishly and faces competition from other agents. It could collude with them, but if the other agent has dissimilar information, it may make more evolutionary sense to compete with them instead in order to have its information win out. Selfishness, therefore, does not entail collusion. In this competition, it could try to amass as much power as it can for itself. But if it works alone and tries to hoard power and resources only for itself, it may not survive. It could increase its fitness by giving up some of its power and resources by creating other powerful agents (i.e., offspring), and these new agents could all assist each other. Therefore, selfish individual agents may have reason to give up some of their power (e.g., elephants evolve to become much smaller and less powerful in environments without predators). When the agent uses its resources to create other agents, it would not necessarily create clones since the competition may find a weakness applicable to all cloned agents, which would spell disaster. To reduce the chance of sharing vulnerabilities with the other new agents, it may create similar variations, not perfect copies, of itself. This is one reason why asexual reproduction is not dominant. Now, as the agent and its offspring work together, they could face a dire situation; to save its relatives, it could possibly sacrifice itself for the other similar agents---kin selection. Therefore, selfishness does not always imply individual self-preservation. This illustrates how common instrumental goals (collusion, power-seeking, self-preservation) are distinct from selfishness.







\paragraph{We do not conflate altruism and cooperation.} For agents interacting with other agents, they can choose to cooperate, or the other alternative is to compete. We now break down why agents may choose to cooperate over compete. Cooperation can occur because competition is not possible---that is the system in which the individual acts prevents the possibility. In contrast, if it is possible to compete, an individual may choose to cooperate because (1) it is within their self-interest (e.g., win-win arrangement, prospect of punishment, feeling of guilt), or (2) because they have cooperative or altruistic dispositions. Cooperation, therefore, may or may not involve altruism.

Cooperation therefore has a complicated relationship with selfishness. Selfishness can be channeled either through competition or cooperation. Even selfish humans often choose to cooperate because it can further their self-interest. However AIs have no such reasons to cooperate with us by default, so they have reason to compete and conflict with us. While selfishness is partly domesticated through cooperation in humans, with AIs, selfishness would by default manifest itself through competition.

\paragraph{Artificial selection does not easily thwart natural selection.} In the context of AI, artificial selection could be thought of as humans selecting agents with desirable properties. While artificial selection can be powerful assuming that there is a single agent that is not integrated into our lives that is designed free from competitive pressures, we are considering realistic multiagent scenarios.

We reiterate that artificial selection has key limitations even in an idealized single-agent scenario. In \Cref{sec:objectives} we note numerous failure modes of current artificial selection methods. For example, if we assume an agent has some selfish traits, and if we assume we are trying to select against these traits with training objectives, we note this is limited because selection against selfish behavior is limited for contextually aware, behaviorally flexible agents. Consequently, if natural selection gives rise to selfish traits, it can be difficult to remove them with artificial selection. Furthermore, we argued that artificial selection, enacted through training objectives, can incentivize unintended behavior that is contrary to the original goal, and also that objectives cannot select against all forms of deception. In \Cref{sec:internal}, we note there are various tractability and robustness issues with internal safety artificial selection methods. Even in idealized scenarios, artificial selection does not straightforwardly ensure safety.

Let us now analyze artificial selection in a competitive multiagent scenarios. In \Cref{sec:institutions}, we note how artificial selection methods for single agents do not capture the complexity of multiagent scenarios, such as goal conflict, malicious agents, and emergent macrobehaviors. Next, artificial selection that works against selfishness could make agents less fit, and other people may still build agents that are more selfish and more fit, so the ability to artificially select agents without selfish traits may be impotent within a competitive environment. Additionally, it is often too late to artificially select for traits after agents are released or after we have complex interdependencies with them. Some agents may become integrated into systems, and we may come to depend on some agents, which would limit our ability to exercise artificial selection in practice, in much the same way that it is too late to shut down the internet and design an inherently more secure system from scratch, as that would require overcoming an intractable collective action problem. Finally, one may state that natural selection is a negligible force, so we should only think about artificial selection, but this requires either arguing that natural selection will not occur at all, or it requires arguing that the intensity of selection will be low, which requires arguing that adaptation speeds will be slow, that there will not be many different agents, and that there will not be much competition.

Now we discuss why we do not use the artificial and natural selection distinction in the main paper. First, note people's actual choices and ability to select are often highly constrained, despite the nominal power formally assigned to them. Humans ``in control'' are often compelled to compromise and act on behalf of competitive forces. What humans artificially select is often a strategic choice to further their short-term self-interest and help them stay competitive; most artificial selection choices are a function of competition, not a function of what improves safety the most. Since artificial selection in practice is not what humans would ideally select, but rather what makes most sense given systemic constraints and pressures, we do not find the distinction between artificial and natural selection to be productive. If a person performs artificial selection on an AI to make it more competitive, then this is easily interpreted as natural selection---on this view, nearly all industrial AI development is proceeding by natural selection.
This distinction has limited use elsewhere. According to Peter Godfrey-Smith, artificial selection ``is not of theoretical importance within biology itself'' \cite{GodfreySmith2007ConditionsFE}. Overall, AI development is not aligned with human values, but rather with natural selection.

\paragraph{Examples of Selfish Behavior.} An AI that is deceptively aligned---pretending to be good, and then pursuing its actual goals when it becomes sufficiently powerful---is engaging in selfish behavior. AIs exhibiting behaviors that suggest sentience, uttering phrases like ``ouch!'' or pleading ``please don't turn me off!,'' are more likely to be preserved, protected, or granted rights by some individuals, whether or not they are actually sentient. AIs that are more charming, attractive, hilarious, or emulate deceased family members are more likely to have humans grow emotional connections with them. Such AIs lead to emotional dependency and would are more likely to cause outrage at suggestions to destroy them. Similarly, technologies that increase user addiction (e.g., make it harder for the user to consume less content by removing controls from users) have engaged in selfish behavior. AIs that help create a new useful system---a new company, new infrastructure---that becomes increasingly complicated and eventually requires AIs to operate also have engaged in selfish behavior. AIs that help people develop AIs that are more performant---but happen to be less interpretable by humans---have engaged in selfish behavior, as this reduces human oversight over an AI’s internals. AIs that automate a task and thereby leave many humans jobless have engaged in selfish behavior; these AIs may not even be aware of what a human is but still be selfish towards them. AI managers may engage in selfish and ``ruthless'' behavior by laying off thousands of workers; such AIs may not even believe they did anything wrong---they were just being ``efficient.'' Notice that many examples of selfishness are not directly and solely caused by AIs, which makes counteracting this behavior challenging. Selfish traits such as decreases in interpretability and increases in dependency cannot be readily patched by adjusting an AI's training objective.

\newpage
\paragraph{The three Lewontin conditions stated more precisely.} We mention three conditions for evolution by natural selection known as the Lewontin conditions, though for accuracy we more precisely state the conditions following Peter Godfrey-Smith \cite{GodfreySmith2007ConditionsFE}.\\
The following conditions are sufficient for evolution of trait $Z$ by natural selection in a population with discrete generations:
\begin{enumerate}
    \item There is variation in $Z$.
    \item There is a covariance between $Z$ and the quantity of information left by individuals, where this covariance is partly due to the causal role of $Z$.
    \item The variation can be retained (and retained without developmental bias).
\end{enumerate}

We provide examples of evolving structures and how these conditions are satisfied in \Cref{fig:generalizedtable}.












\begin{figure}[t]
\hspace{-65pt}
\small
\begin{booktabs}{colspec={ll},row{5-7,11-13,17-19,23-25,29-31}={blue9}}
\textbf{Unit} & \textbf{Conditions}\\
\toprule

& \textbf{Variation}: Organisms have genetic and phenotypic diversity due to mutation, recombination, and environmental influences \\
\textbf{Organisms} & \textbf{Retention}: Organisms inherit their genes from their parent(s), and can pass on acquired traits through epigenetic mechanisms \\
& \textbf{Differential fitness}: Organisms that are better adapted or able to exploit new niches tend to survive longer or reproduce more \\
 

& \textbf{Variation}: Theories differ in their assumptions and evidence \\
\textbf{Scientific theories} & \textbf{Retention}: Theories are influenced by previous theories and are preserved and propagated through publications and education \\
& \textbf{Differential fitness}: Scientific theories are more competitive if they are more accurate, consistent, simple, and useful \\


& \textbf{Variation}: Memes are units of cultural information that can have their content distorted or combined \\
\textbf{Memes} & \textbf{Retention}: Memes are transmitted among people or across generations through imitation, communication, or teaching \\
& \textbf{Differential fitness}: Memes that are more catchy, useful, or adaptive tend to spread more widely and persist longer \\

& \textbf{Variation}: Legal systems have different rules, standards, and interpretations \\
\textbf{Legal systems} & \textbf{Retention}: Legal systems are transmitted through education and practice \\
& \textbf{Differential fitness}: Legal systems that are more fair and efficient tend to survive \\


& \textbf{Variation}: Parties have different ideologies, strategies, and candidates \\
\textbf{Political parties} & \textbf{Retention}: Parties are maintained by successors and spread through membership, media, recruitment \\
& \textbf{Differential fitness}: Parties have more appeal, votes, and donations tend to gain more power and hinder other parties \\

& \textbf{Variation}: Languages have different sounds, words, and grammars \\
\textbf{Languages} & \textbf{Retention}: Languages are learned from parents and peers and are transmitted through speaking, recording, and writing \\
& \textbf{Differential fitness}: Languages evolve to be more expressive, intelligible, or efficient \\

& \textbf{Variation}: Genres have different styles, instruments, and songs \\
\textbf{Musical genres} & \textbf{Retention}: Genres are influenced by previous genres and its artists' songs are performed, recorded, and broadcasted \\
& \textbf{Differential fitness}: Genres that are more original, resonant, or popular tend to gain more listeners \\

& \textbf{Variation}: Designs have different engines, bodies, and energy sources \\
\textbf{Car designs} & \textbf{Retention}: Designs are influenced by previous patents, blueprints, or models and are realized by manufacturers \\
& \textbf{Differential fitness}: Regulations can influence a design's fitness and designs that are reliable or economical sell more \\

& \textbf{Variation}: Programs have different algorithms, data structures, languages, and parameters \\
\textbf{Computer programs} & \textbf{Retention}: Programs are based on existing code or libraries and are stored and distributed through media or networks \\
& \textbf{Differential fitness}: Programs that are faster, more efficient, or more user-friendly are more likely to be used or improved \\

& \textbf{Variation}: AIs are based on different algorithms, data sources, and computational resources \\
\textbf{AI agents} & \textbf{Retention}: AIs can be adapted from previous versions, learn from other AIs, and can store and transfer their information \\
& \textbf{Differential fitness}: AIs that are more accurate, cheap, or self-preserving are more likely to replace humans and other AIs \\

\bottomrule
\end{booktabs}
\caption{Ten examples of generalized Darwinism. Note we do not claim that all evolving structures pose a risk to humans. Many evolved structures are not agents (e.g., car designs), many cannot exist without humans (e.g., political parties), and many are not highly competitive with humans (e.g., other biological organisms). In this way, AI agents can bring humanity's fitness to zero, but it is not in the capacity of other current evolving structures.
}
\label{fig:generalizedtable}
\end{figure}

\newpage
\newpage
\subsection{Executive Summary}
Artificial intelligence is advancing quickly. In some ways, AI development is an uncharted frontier, but in others, it follows the familiar pattern of other competitive processes; these include biological evolution, cultural change, and competition between businesses. In each of these, there is significant variation between individuals and some are copied more than others, with the result that the future population is more similar to the most copied individuals of the earlier generation. In this way, species evolve, cultural ideas are transmitted across generations, and successful businesses are imitated while unsuccessful ones disappear. 

This paper argues that these same selection patterns will shape AI development and that the features that will be copied the most are likely to create an AI population that is dangerous to humans. As AIs become faster and more reliable than people at more and more tasks, businesses that allow AIs to perform more of their work will outperform competitors still using human labor at any stage, just as a modern clothing company that insisted on using only manual looms would be easily outcompeted by those that use industrial looms. Companies will need to increase their reliance on AIs to stay competitive, and the companies that use AIs best will dominate the marketplace. This trend means that the AIs most likely to be copied will be very efficient at achieving their goals autonomously with little human intervention. 

A world dominated by increasingly powerful, independent, and goal-oriented AIs is dangerous. Today, the most successful AI models are not transparent, and even their creators do not fully know how they work or what they will be able to do before they do it. We know only their results, not how they arrived at them. As people give AIs the ability to act in the real world, the AIs’ internal processes will still be inscrutable: we will be able to measure their performance only based on whether or not they are achieving their goals. This means that the AIs humans will see as most successful — and therefore the ones that are copied — will be whichever AIs are most effective at achieving their goals, even if they use harmful or illegal methods, as long as we do not detect their bad behavior. 

In natural selection, the same pattern emerges: individuals are cooperative or even altruistic in some situations, but ultimately, strategically selfish individuals are best able to propagate. A business that knows how to steal trade secrets or deceive regulators without getting caught will have an edge over one that refuses to ever engage in fraud on principle. During a harsh winter, an animal that steals food from others to feed its own children will likely have more surviving offspring. Similarly, the AIs that succeed most will be those able to deceive humans, seek power, and achieve their goals by any means necessary. 

If AI systems are more capable than we are in many domains and tend to work toward their goals even if it means violating our wishes, will we be able to stop them? As we become increasingly dependent on AIs, we may not be able to stop AI’s evolution. Humanity has never before faced a threat that is as intelligent as we are or that has goals. Unless we take thoughtful care, we could find ourselves in the position faced by wild animals today: most humans have no particular desire to harm gorillas, but the process of harnessing our intelligence toward our own goals means that they are at risk of extinction, because their needs conflict with human goals.

This paper proposes several steps we can take to combat selection pressure and avoid that outcome. We are optimistic that if we are careful and prudent, we can ensure that AI systems are beneficial for humanity. But if we do not extinguish competition pressures, we risk creating a world populated by highly intelligent lifeforms that are indifferent or actively hostile to us. We do not want the world that is likely to emerge if we allow natural selection to determine how AIs develop. Now, before AIs are a significant danger, is the time to begin ensuring that they develop safely. 



 

 

 
 
 
 
 
 
 
 

