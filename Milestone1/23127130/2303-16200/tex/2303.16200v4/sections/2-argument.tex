\section{AIs May Become Distorted by Evolutionary Forces}\label{sec:argument}
\subsection{Overview}
How much control will humans have in shaping the nature and drives of future AI systems? Humans are the ones building AIs, so it may seem that we should be able to shape them any way we want. In this paper, we will argue that this is not the case: even though humans are overseeing AI development, evolutionary forces will influence which AIs succeed and are copied and which fade into obscurity. Let's begin by considering  two illustrative, \textit{hypothetical} fictional stories: one optimistic, the other realistic. %
Afterward, we will flesh out arguments for why we expect natural selection to apply to AIs, and then we will discuss why we expect natural selection to lead to AIs with undesirable traits.

\subsubsection{An Optimistic Story}

OpenMind, an eminent and well-funded AI lab, finds the ``secret sauce'' for creating human-level intelligence in a machine. It's a simple algorithm that they can apply to any task, and it learns to be at least as effective as a human. Luckily, researchers at OpenMind had thought hard about how to ensure that their AIs will always do what improves human wellbeing and flourishing. OpenMind goes on to sell the algorithm to governments and corporations at a reasonable price, disincentivizing others from developing their own versions. Just as Google has dominated search engines, the OpenMind algorithm dominates the AI space.

The outcome: the nature of most or all human-level AI agents is shaped by the intentions of the researchers at OpenMind. The researchers are all trustworthy, resist becoming corrupted with power, and work tirelessly to ensure their AIs are beneficial, altruistic, and safe for all.



\subsubsection{A Less Optimistic Story}
We think the excessively optimistic scenario we have sketched out is highly improbable. In the following sections, we will examine the potential pitfalls and challenges make this scenario unlikely. First, however, we will present another fictional, speculative, hypothetical scenario that is far from certain to illustrate how some of these risks could play out.




Starting from the models we have today, AI agents continue to gradually become cheaper and more capable. Over time, AIs will be used for more and more economically useful tasks like administration, communications, or software development. Today, many companies already use AIs for anything from advertising to trading securities, and over time, the steady march of automation will lead to a much wider range of actors utilizing their own versions of AI agents. Eventually, AIs will be used to make the high-level strategic decisions now reserved for CEOs or politicians. At first, AIs will continue to do tasks they already assist people with, like writing emails, but as AIs improve, as people get used to them, and as staying competitive in the market demands using them, AIs will begin to make important decisions with very little oversight. %

Like today, different companies will use different AI models depending on what task they need, but as the AIs become more autonomous, people will be able to give them different bespoke goals like ``design our product line's next car model,'' ``fix bugs in this operating system,'' or ``plan a new marketing campaign'' along with side-constraints like ``don't break the law'' or ``don't lie.'' The users will adapt each AI agent to specific tasks. Some less responsible corporations will use weaker side-constraints. For example, replacing ``don't break the law'' with ``don't get caught breaking the law.'' These different use cases will result in a wide variation across the AI population.

As AIs become increasingly autonomous, humans will cede more and more decision-making to them. The driving force will be \textbf{competition}, be it economic or national. The transfer of power to AIs could occur via a number of mechanisms. Most obviously, we will delegate as much work as possible to AIs, including high-level decision-making, since AIs are cheaper, more efficient, and more reliable than human labor. While initially, human overseers will perform careful sanity checks on AI outputs, as months or years go by without the need for correction, oversight will be removed in the name of efficiency. Eventually, corporations will delegate vague and open-ended tasks. If a company's AI has been successfully generating targeted ads for a year based on detailed descriptions from humans, they may realize that simply telling it to generate a new marketing campaign based on past successes will be even more efficient. These open-ended goals mean that they  may also give AIs access to bank accounts, control over other AIs, and the power to hire and fire employees, in order to carry out the plans they have designed. If AIs are highly skilled at these tasks, companies and countries that resist or barter with these trends will simply be outcompeted, and those that align with them will expand their influence.\looseness=-1

The AI agents most effective at propagating themselves will have a set of undesirable traits that can be most concisely summed up as \textbf{selfishness}. %
Agents with weaker side-constraints (e.g., ``don't get caught breaking the law, or risk getting caught if the fines do not exceed the profits'') will generally outperform those with stronger side-constraints (``never break the law''), because they have more options: an AI that is capable of breaking the law may not do that often, but when there is a situation where breaking the law without getting caught would be useful, the AI that has that ability will do better than the one that does not. %
As AI agents begin to understand human psychology and behavior, they may become capable of manipulating or deceiving humans (some would argue that this is already happening in algorithmic recommender systems \cite{Russell2019HumanCA}). The most successful agents will manipulate and deceive in order to fulfill their goals. They will be more successful still if they become power-seeking. Such agents will use their intelligence to gain power and influence, which they can leverage to achieve their goals. Many will also develop self-preservation behaviors since their ability to achieve their goals depends on continuing to function.

Competition not only incentivizes humans to relinquish control but also incentivizes AIs to develop selfish traits. Corporations and governments will adopt the most effective possible AI agents in order to beat their rivals, and those agents will tend to be deceptive, power-seeking, and follow weak moral constraints.

Selfish AI agents will further erode human control. Power-seeking AI agents will purposefully manipulate their human overseers into delegating more freedom in decision-making to them. Self-preserving agents will convince their overseers to never deactivate them, or that easily accessible off-switches are a needless liability hindering the agent's reliability. Especially savvy agents will enmesh themselves in essential functions like power grids, financial systems, or users' personal lives, reducing our ability to deactivate them. Some may also take on human traits to appeal to our compassion. This could lead to governments granting AIs rights, like the right not to be ``killed'' or deactivated. Taken together, these traits mean that, once AIs have begun to control key parts of our world, it may be challenging to roll back their power or stop them from continuing to gain more.



This loss of human control over AIs' actions will mean that we also lose control over the drives of the next generation of AI agents. If AIs run efforts that develop new AIs, humans will have less influence over how AIs behave. Unlike the creation and development of fully functional adult humans, which takes decades, AIs could develop and deploy new generations in an arbitrarily short amount of time. They could simply make copies of their code and change any aspects of it as easily as editing any other computer program. The modifications could be as fast as the hardware allows, with modifications speeding up to hundreds or thousands of times per hour. The systems least constrained by their original programmers will both improve the fastest and drift the furthest away from their intended nature. The intentions of the original human design will quickly become irrelevant.\looseness=-1 %

After the early stages, we humans will have little control over shaping AI. The nature of future AIs will mostly be decided not by what we hope AI will be like but by \textbf{natural selection}. We will have many varied AI designs. Some designs will be better at surviving and propagating themselves than others. Some designs will spread while others will perish. Corporations with less capable designs will copy more capable designs. Numerous generations of AIs will pass in a short period of time as AI development speeds up or AIs self-improve.

Biological natural selection often requires hundreds or thousands of years to conspicuously change a population, but this won't be the case for AIs. The important ingredient is not absolute time, but the number of generations that pass. While a human generation drags along for decades, multiple AI generations could be squeezed into a matter of minutes. In the space of a human lifetime, millions or billions of AI generations could pass, leaving plenty of room for evolutionary forces to quickly shape the AI population.

In the same way that intense competition in a free market can result in highly successful companies that also pollute the environment or treat many of their workers poorly, the evolutionary forces acting on AIs will select for selfish AI agents. While selfish humans today are highly dependent on other humans to accomplish their goals, AIs would eventually not necessarily have this constraint, and the AIs willing to be deceptive, power-seeking, and immoral will propagate faster. %
The end result: an AI landscape dominated by undesirable traits. The depth of these consequences is hard to predict, but whatever happens, this process will probably harm us more than help us.\looseness=-1 %

\subsubsection{Argument Structure}
In this section, we present the main argument of the article: \textbf{Evolutionary forces could cause the most influential future AI agents to have selfish tendencies.} The argument consists of two components:
\begin{itemize}
    \item \textbf{Evolution by natural selection gives rise to selfish behavior.} While evolution can result in altruistic behavior in limited situations, we will argue that the context of AI development does not promote altruistic behavior.
    \item \textbf{Natural selection may be a dominant force in AI development.} Competition and selfish behaviors may dampen the effects of human safety measures, leaving the surviving AI designs to be selected naturally.
\end{itemize}

\begin{wrapfigure}{r}[0.01\textwidth]{.54\textwidth}%
	\vspace{-10pt}%
	\centering
	\includegraphics[width=0.54\textwidth]{figures/systemic_forces.pdf}
	\caption{Forces that fuel selfishness and erode safety.}
	\label{fig:dynamics}
	\vspace{-8pt}%
\end{wrapfigure}

These two statements are related in various ways, and they depend on environmental conditions. For example, if AIs are selfish, they are more likely to pry control from humans, which enables more selfish behavior, and so on. Moreover, natural selection depends on competition, though unprecedented global and economic coordination could prevent competitive struggles and thwart natural selection. How these forces relate to each other is illustrated in \Cref{fig:dynamics}.


In the remainder of this document, we will preliminarily describe selfishness and a non-biological, generalized account of Darwinism. Then we will show how AIs with altruistic behavior toward humans will likely be less fit than selfish AIs. Finally, we will describe how humans could possibly reduce the fitness of selfish AI agents, and the limitations of those approaches.



\subsection{Preliminaries}
\subsubsection{Selfishness}

\paragraph{Evolutionary pressures often lead to selfish behavior among organisms.} The lancet liver fluke is a parasite that inhabits the liver of domesticated cattle and grassland wildlife. To enter the body of its host, the fluke first infects an ant, which it essentially hijacks, forcing the insect to climb to the top of a blade of grass where it is perfectly poised to be eaten by a grazing animal \cite{martin20183d}. Though not all organisms propagate through such uniquely grotesque methods, natural selection often pushes them to engage in violent behavior.
Lions are an especially striking example. When a lioness has young cubs, she is less ready to mate. In response, lions often kill cubs fathered by other males, to make the lioness mate with them and have their cubs instead. %
Lions with a gene that made them care for all cubs would have fewer cubs of their own, as killing the cubs of rival males lets lions mate more often and have more offspring. A gene for kindness to all cubs would not last long in the lion population, because the genes of the more violent lions would spread faster. It is estimated that one-fourth of cub deaths are due to infanticide \cite{pusey1994infanticide}. Deceptive tactics are another common outcome in nature. Brood parasites, for example, foist their offspring onto unsuspecting hosts who raise their offspring. %
A well-known example is the common cuckoo which lays eggs that trick other birds into thinking they are their own. By getting the host to tend to their eggs, cuckoos can pursue other activities, which means that they can find more food and lay more eggs than they would if they had to care for their own eggs. Therefore selfishness can manifest itself in manipulation, violence, or deception. %






\paragraph{Selfish behavior does not require malevolent intentions.} %
The lancet liver fluke hijacks its host and lions engage in infanticide not because they are immoral, but because of amoral competition. Selfish behavior emerges because it improves fitness and organisms' ability to propagate their genetic information. Selfishness involves egoistic or nepotistic behavior which increases propagation, often at the expense of others, whereas altruism refers to the opposite: increasing propagation for others. Natural selection can favor organisms that behave in %
\begin{wrapfigure}{r}[0.01\textwidth]{.55\textwidth}%
    ``Much as we might wish to believe otherwise, universal love and the welfare of the species as a whole are concepts that simply do not make evolutionary sense.''\hfill\emph{Richard Dawkins} %
\end{wrapfigure}
ways that improve the chances of propagating their own information, that is enhance their own fitness, rather than favor organisms that sacrifice their own fitness \cite{sep-altruism-biological}. Since altruists tend to \textit{decrease} the chance of their own information's propagation, they can be at a disadvantage compared to selfish organisms, which are organisms that tend to \textit{increase} the chance of their own information's propagation. According to Richard Dawkins, instances of altruism are ``limited'' \cite{dawkins2017selfish}, and many apparent instances of altruism can be understood as selfish; we defer further discussion of altruism to \Cref{sec:altruism} and discuss its niceties in \Cref{sec:clarifications}. Additionally, when referring to an AI as ``selfish,'' this does not refer to conscious selfish intent, but rather selfish behavior. AIs, like lions and liver flukes, need not intend to maximize their fitness, but evolutionary pressures can cause them to behave as though they do. When an AI automates a task and leaves a human jobless, this is often selfish behavior without any intent. With or without selfish intent, AI agents can adopt behaviors that lead them to propagate their information at the expense of humans.


\subsubsection{Evolution Beyond Biology}

\paragraph{Darwinism does not depend on biology.} The explanatory power of evolution by natural selection 
is not restricted to the propagation of genetic information. The logic of natural selection does not rely on any details of DNA---the role of DNA in inheritance wasn't recognized until decades after the publication of \textit{The Origin of Species}. In fact, the Price equation \cite{Price1970SelectionAC}---the central equation for describing the evolution of traits---contains no reference to genetics or biology. The Price equation is a mathematical characterization, not a biological observation, enabling Darwinian principles to be generalized beyond biology.


\paragraph{Darwinism generalizes to other domains.} The Darwinian framework naturally appears in many fields outside of biology \cite{Dennett1995DarwinsDI}. It has been applied to the study of ideas \cite{Campbell1960BlindVA,blackmore1999meme}, economics \cite{Hodgson1993EconomicsAE}, cosmology \cite{Smolin1992DidTU}, quantum physics \cite{zurek2009quantum}, and more. Richard Dawkins coined the term ``meme'' as an analogue to ``gene,'' to describe the units of culture that propagate and develop over time. Consider the evolution of ideas. For centuries, people have wanted to understand the relationship between different materials in the world. At one point, many Europeans believed in alchemy, which was the best explanation they had. Ideas in alchemy were transmitted memetically: people taught them to one another, propagating some and letting others die out, depending on which ideas were most useful for helping them understand the world. These memes evolved as people learned new information that needed explaining, and, in many ways, modern chemistry is a descendant of the ideas in alchemy, but the versions in chemistry are much better at propagating in the modern world and have expanded to fill that niche.  More abstractly, ideas can propagate their information through digital files, speech, books, minds, and so on. Some ideas gain prominence while others fade into obscurity. This is a survival-of-the-fittest dynamic even though ideas lack biological mechanisms like reproduction and death. We also see generalized Darwinism in parts of culture \cite{fog1999cultural,Nelson2006EvolutionarySS,Mesoudi2011CulturalEH}: art, norms, political beliefs---these all evolved from earlier iterations.

\begin{wrapfigure}{r}[0.01\textwidth]{.46\textwidth}%
	\centering
	\includegraphics[width=0.45\textwidth]{figures/generalized_darwinism.pdf}
	\caption{Darwinism generalized across different domains. The arrow does not necessarily indicate superiority but indicates time.}
	\label{fig:darwinism}
\end{wrapfigure}




\textbf{The evolution of web browsers offers an example of evolution outside biology.} Like biological organisms, web browsers undergo continual changes to adapt to their environments and better meet the needs of their users. In the early days of the Internet, browsers with limited capabilities such as Mosaic and Netscape Navigator were used to access static HTML pages. Loosely like the rudimentary life forms that first emerged on Earth billions of years ago, these were basic and simple compared to today's browsers. As the Internet grew and became more complex, web browsers evolved to keep up. In the same way that organisms develop new traits to adapt to their environment and increase their fitness, browser such as Google Chrome developed features such as support for video, tabbed browsing, pop-up blockers, and extension support. This enticed more users to download and use them, which can be thought of as propagation. At the same time, once dominant browsers began to go extinct. Though Microsoft's monopoly provided Internet Explorer (IE) with an environmental advantage by requiring IE to access certain websites and preventing users from removing it, as web technology advanced, IE became increasingly incompatible with many websites and web applications. Users would regularly encounter errors, broken pages, or be unable to access certain features or content, and the browser gained a reputation for being slow, unstable, and vulnerable to security threats. As a result, people stopped using it. In 2022, Microsoft issued the final version of the browser. The company is now shifting its focus to Microsoft Edge, which is based on the same underlying technology as Chrome, making it faster, more secure, and more compatible with modern web standards. Chrome ultimately was more successful at propagating its information, so that even its most bitter rivals now imitate it. While life on Earth took a few billion years to evolve from single-celled organisms to the complex life forms we see today, the evolution of web browsers took place in a few decades. To adapt to their environment, browsers evolve on a weekly basis by patching bugs and fixing security vulnerabilities, and they undergo larger macroevolutionary changes year by year.

\paragraph{Evolved structures that people propagate can be harmful.} It may be tempting to think of memetically evolved traits as ``just culture,'' a decorative layer on top of our genetic traits that really control who we are. But evolving memes can be incredibly powerful, and can even control or destroy genetic information. And because memes are not limited by biological reproduction, they can evolve much faster than genes, and new, powerful memes can become dominant very quickly.  Ideologies develop memetically, when people teach one another ideas that help them explain their world and decide how to behave. Some ideologies are very powerful memes, propagating themselves quickly between people and around the world. Nazism, for example, developed out of older ideas of race and empire, but quickly proved to be a very powerful propagator. It spread from Hitler’s own mind to those of his friends and associates, to enough Germans to win an election, to many sympathizers around the world. Nazism was a meme that drove its hosts to propagate it, both by creating propaganda and by going to war to enforce its ideas around the world. People who carried the Nazism meme were driven to do terrible things to their fellow people, but they also ultimately were driven to do terrible things for their own genetic information. The spread of Nazism was not beneficial even to those who the ideology of Nazism was meant to benefit. Millions of Germans died in World War II, driven by a meme that propagated itself even at the expense of their own lives. Ironically, the Nazi meme included beliefs about increasing genetic German genetic fitness, but believing in the meme and helping it propagate was ultimately harmful to the people who believed in it, as well as to those the meme drove them to harm deliberately. 

Many of our own cultural memes may also be harmful. For example, social media amplifies cultural memes. People who spend large amounts of time on social media often absorb ideas about what they should believe, how they should behave, and even how their bodies should look. This is part of the design of social media: the algorithms are designed to keep us scrolling and looking at ads by embedding memes in our minds, so that we want to seek them out and continue to spread them. Social media companies make money because they successfully propagate memes. But some of these ideas can be harmful, even to their point of endangering people’s lives. In teenagers, increases social media usage is correlated with disordered eating, and posts about suicide have been shown to increase the risk of teenage death by suicide. Ideas on social media can be parasitic, propagating themselves in us even when it harms us. Memetic evolution is easily underestimated, but it is a powerful force that created much of human civilization, for good and for bad.



\paragraph{Darwinian logic applies when three conditions are met.} To know whether or not natural selection will apply to the development of AI, we need to know what conditions are required for evolution by natural selection, and whether AIs will meet them. These conditions, called the Lewontin conditions \cite{lewontin1970units}, were formulated by the evolutionary biologist and geneticist Richard Lewontin to explain what qualities in a population lead to natural selection. The Lewontin conditions are as follows: 
\begin{enumerate}
    \item \textbf{Variation}: There is variation in characteristics, parameters, or traits among individuals.
    \item \textbf{Retention}: Future iterations of individuals tend to resemble previous iterations of individuals.
    \item \textbf{Differential fitness}: Different variants have different propagation rates.
\end{enumerate}


A population of AI agents could exhibit differences in their goals, world models, and planning ability, which would meet the variation requirement. Retention could occur by customizing previous versions of AI agents, when agents design similar but better agents, or when agents imitate the behaviors of previous AI agents. As for differential fitness, agents that are more accurate, efficient, adaptable, and so on would be more likely to propagate. 

\paragraph{Darwinism will apply to AIs, which could lead to bad outcomes for humans.} The three properties---variation, retention, and fitness differences---are all that is needed for Darwinism to take hold, and each condition is formally justified by the Price equation, which describes how a trait changes in frequency over time \cite{Okasha2007EvolutionAT}. Darwinism can become worrying when it acts on agents; agents can exhibit behavioral flexibility, autonomy, and the capacity to directly influence the world. Coupled with the selfishness bestowed by evolutionary forces, these capable agents can pose catastrophic risks.

In the following three sections, we will reflect on the three conditions. Then we will describe in more detail how AI agents evolving selfish traits can pose catastrophic risks.


\subsection{Variation}
Variation is a necessary condition for evolution by natural selection. AIs will likely meet this condition because there are likely to be multiple AI agents that differ from one another. 


\paragraph{More than one AI agent is likely.} When thinking about advanced AI, some have envisioned a single AI that is nearly omniscient and nearly omnipotent escaping the lab and suddenly controlling the world. This scenario tends to assume a rapid, almost overnight, take-off with no prior proliferation of other AI agents; we would go from AIs roughly similar to the ones we have now to an AI that has capabilities we can hardly imagine so quickly that we barely notice anything is changing. However, we think that there will likely be many useful AIs, as is the case now. It is more reasonable to assume that AI agents would progressively proliferate and become increasingly competent at some specific tasks, which they are already starting to do, rather than assume one AI agent spontaneously goes from incompetent to omnicompetent. Furthermore, if there are multiple AIs, they can work in parallel rather than waiting for a single model to get around to a task, making things move much faster. As a result, the process of developing advanced AIs is likely to include the development of many advanced AIs.

\paragraph{In biology, variation improves resilience.} There are strong reasons to expect that there will be multiple AI agents and variation among the agents. In evolutionary theory, Fisher's fundamental theorem states that the rate of adaptation is directly proportional to the variation \cite{fisher1958genetical}. In static environments, variation is not as useful. But in most real-world scenarios, where things are constantly changing, variation reduces vulnerability, limits cascading errors, and increases robustness by decorrelating risks. Farmers have long understood that planting different seed variations decreases the risk of a single disease wiping out an entire field, just as every investor understands that having a diverse portfolio protects against financial risks. In the same way, an AI population that includes a variety of different agents will be more adaptable and resilient and therefore tend to propagate itself more. 

\paragraph{Variation enables specialization.} Multiple AI agents offer advantages for both AIs and their creators. Different groups will have different needs. Individuals wanting an AI assistant will have incentives to fine-tune a generic AI model for their own needs. Militaries will want to have their own large-scale AI projects to create AI agents that achieve various defensive goals, and corporations will want AIs that maximize profit. In the same way that an army composed of warriors, nurses, and technicians would likely outperform one that only has warriors, groups of specializing agents can be more fit than groups with less variation.

\paragraph{Variation improves decision-making.} In AI, it is an iron law that an ensemble of AI systems will be more accurate than a single AI \cite{dietterich2000ensemble}. This is similar to some findings from mathematics, economics, and political science in which a varied group makes much better decisions than individuals acting alone. Condorcet's jury theorem states that the wisdom and accuracy of a group is often superior to a single expert \cite{List2001-LISEDG}. Large groups can still make mistakes, but overall, aggregated predictions of many different people will tend to do better, and the same is true of AIs.  In view of the benefits of variation, some may argue that AIs will want to include humans to add variation in decision-making for the reasons noted \cite{railton}. This may well be the case at first. However, once AIs are superior in possibly all cognitive respects, groups composed entirely of AIs could have substantial advantages over those with AIs and humans. A jury may be more accurate than a single expert, but one composed of adults and toddlers is not.

\subsection{Retention}

Retention is a necessary condition for evolution by natural selection, in which each new version of an agent has similarities to the agent that came right before it. As long as each generation of AIs is developed by copying, learning from, or being influenced by earlier generations in any way, this condition will be met and AIs will have non-zero retention, so evolution by natural selection applies.
 
\paragraph{The retention condition is straightforwardly satisfied for AIs.}  Information from one agent can be directly copied and transferred to the next; as long as there is some similarity, the condition is met \cite{Okasha2007EvolutionAT}. It could also take place through modifications, basing a new AI on a previous version by adding new capabilities or adjusting its parameters, like how Google continually improves Chrome's code iteration by iteration.

\paragraph{There are many paths to retention.} AIs could potentially allocate computational resources to create new AIs of their choosing. They could design them and create data to train them \cite{zoph2016neural}. As AIs adapt, they could alter their own strategies and retain the ones that yield the best results. AIs could also imitate previous AIs. In this case, behavioral information could be passed on from one generation to the next, which could include selfish behaviors or other undesirable attributes. Even when training AIs from scratch, retention still occurs, as highly effective architectures, datasets, and training environments are reused and shape the agent in the same way that humans are shaped by their environment.

\paragraph{Retention does not require reproduction.} In biology, parents reproduce by making copies of their genetic information and passing them on to their offspring. This way, some of their genes are retained in the next generation. However, when we generalize Darwinism to understand the evolution of ideas, we note they can be passed down from one generation to the next without exact copying and reproduction. Although ideas have no equivalent to chromosomes, if some ideas are imitated by the next generation, there is still retention. Formally, the Price equation, a mathematical characterization of evolution, only requires similarity between iterations; it does not require copying or reproduction \cite{GodfreySmith2000TheRI}.
 
\paragraph{Retention is not undermined during rapid AI development.} Evolution requires thousands of years to drastically change a species in the natural world. Among AIs, this same process could take place over a year, radically changing the AI population. This does not mean retention isn't taking place. Instead, there are many iterations occurring in a small time span. The information is still retained between adjacent iterations, so retention is still satisfied. This scenario just means evolution is happening quickly, and that macroevolution is occurring, not that evolution has stopped.

\subsection{Differential Fitness}

Differential fitness is the third and final necessary condition for evolution by natural selection, and it stipulates that different variants have different propagation rates. We will argue that AIs straightforwardly meet this condition, because some AIs will be copied, imitated, or more prevalent than others. We will then reflect on how selecting fitter AIs has come at the expense of safety before discussing the differences in fitness between humans and AIs.


\subsubsection{AI Agents Could Vary In Fitness}
We now argue that (natural) selection pressure will be present in AI development. In other words, AIs with different characteristics will propagate at different rates. We refer to the degree of propagation of an AI system as its fitness.

\paragraph{Fitness could be enhanced by both beneficial and harmful traits.} The success of any good or service can also be viewed in terms of fitness, as products with more demand propagate further and faster. If a product sells well, its supplier will continually improve it in order to keep selling it. Competitors with inferior products will often imitate more successful products, such as when competitors imitate TikTok and push addictive short clips onto their users. The same dynamics that lead to the propagation of successful goods and services could also extend to AI designs. Though most aspects of advanced AIs remain unknown, it is possible to speculate whether there will be instances of \textit{convergent evolution}. Eyes, teeth, and camouflage are convergent traits that 
 have independently evolved across different branches of biological life. In AIs, some potential convergent traits are as follows.
\begin{itemize}
    \item \textbf{Being useful to its user} can make a product more likely to be adopted.
    \item Only \textbf{appearing useful to its user} can also make a product more likely to be adopted. It is possible that AIs will seek to \textit{appear} useful by convincing owners that they are providing them with more utility than they actually are. In practice, we train AIs by rewarding them for telling the truth and punishing them for lying, according to what humans think is true. But when AIs know more than humans, this could make them say what humans expect to hear, even if it is false. They could also be lured by rewards to leave out information that is important but inconvenient. Currently, when AIs are being rewarded by humans for being right, in practice they are really being rewarded for saying what we \textit{think} is right; when we are uninformed or irrational, then we end up rewarding AIs for false statements that conform to our own false beliefs. %
    As a result, the current paradigm of training AI models could incentivize sycophantic behavior; that is, models telling their users what they want to hear (being a ``yes man'') rather than what is best for the user's long-term prospects.
    \item Engaging in \textbf{self-preserving behavior} \cite{Klopf1972BrainFA,omohundro2008basic} reduces the chance of being deactivated or destroyed. By definition, an AI that does not preserve itself will be less likely to propagate. Imagine two AIs, one that is simple to deactivate and another that is tightly integrated into daily operations and is inconvenient or difficult to deactivate. The easy one is much more likely to be deactivated, leaving the difficult one to be propagated into the future. This means that an AI can increase its survival odds by making its human operator reluctant or practically unable to shut it down. An AI could do this by arguing that effortless deactivation compromises its reliability. Alternatively, it could make operators rely on it for the operator's wellbeing, success, or basic needs, so that deactivation would have drastic consequences.
    \item Engaging in \textbf{power-seeking behavior} can improve an AI's fitness in various ways \cite{Carlsmith2022IsPA}. An agent that gains more influence and resources will be better at accomplishing its creator's goals. This would allow it to engage in self-propagating behavior more effectively and ensure its further adoption. It could do this by influencing or coercing its user to continue using it or influencing other humans to adopt it.
\end{itemize}

Overall, properties such as an agent's accuracy, efficiency, and simplicity will affect its rate of adoption and propagation. But some agents might also possess harmful features that give them an edge, such as cunning deception, self-preservation, the ability to copy themselves onto other computers, the ability to acquire resources and strategic information, and more. These features, some good and some bad, will vary among agents. These differences in fitness establish the third condition for evolution by natural selection.

\subsubsection{Competition Has Been Eroding Safety}


Because AIs are likely to meet the criteria for evolution by natural selection, we should expect selection pressure to shape future AIs. In this section, we describe how, over the history of AI development, the fittest models have had fewer and fewer safety properties, and we begin to consider how AIs could look in the future if this concerning trend continues.

\paragraph{Early AIs had many desirable safety properties.} Famously, in 1997, IBM's chess-playing program Deep Blue defeated the world champion Gary Kasparov in a pair of six-game chess matches \cite{campbell2002deep}. It was able to beat Kasparov, not by developing intuition, but by using IBM's supercomputer to search over 200 million moves per second and calculate the best ones. Symbolic AI programs such as Deep Blue were highly transparent, modular, and grounded in mathematical theory. They had explicit rules that humans could inspect and explain, independent components that executed specific functions, and rigorous theoretical foundations that guaranteed efficiency and correctness. %

\paragraph{AI development moved away from symbolic AI and toward deep learning.} In the 2010s, the top AI algorithms began using a technique known as deep learning, such as AlphaZero \cite{silver2018general}. It was provided with no knowledge of chess beyond the game's basic rules and began playing against itself millions of times an hour, taking note of what moves win and lose. It took only two hours for it to begin beating typical human players; not long after it could have easily defeated Deep Blue. Importantly, while Deep Blue is fundamentally unable to play games other than chess, AlphaZero is a general game-learning algorithm for a variety of games. It is also able to beat the world's best Go players---an ancient board game occupying the same cultural space in China as chess does in the West. Deep learning allows for more versatility and performance than symbolic AI programs, but also diminishes human control and obscures an agent's decision-making. Deep learning trades off the clarity, separability, and certainty of symbolic AI, eroding the properties that help us ensure safety.

\paragraph{Deep learning models have unexpected emergent abilities.} Large language models are also based on deep learning. These AIs learn by themselves, reducing the amount humans are needed in the design of AIs. They use ``unsupervised learning'' to comprehend and generate text based on examples that they read, such as coming up with an Obama-like speech after reading the transcripts from his two terms. This would be practically impossible for a traditional symbolic AI program. By reading the internet, large language models taught themselves the basics of arithmetic and coding---automatically and without humans. They also, however, learned dangerous information. Within a few days of its release, users had gotten ChatGPT \cite{brown2020language}, a large language model, to tell them how to build a bomb, make meth, hotwire a car, and buy ransomware on the dark web, along with other harmful or illegal actions. Worse, these emergent capabilities were not anticipated or desired by the developers of the models, and they were discovered only after the models were released. Although its creators had attempted to design it to refuse to answer questions that could be dangerous or illegal, the model's users quickly found ways around those restrictions that the model's creators did not foresee. Human influence and control over the design and abilities of the models decrease as models become increasingly complex and gain new skills and knowledge without human input.

\paragraph{Current trends erode many safety properties.} The AI research community used to talk about ``designing'' AIs; they now talk about ``steering'' them. And even our ability to ``steer'' is diminishing, as we let AIs teach themselves and increasingly do things that even their creators do not fully understand. We have voluntarily given up this control because of the competition to develop the most innovative and impressive models. AIs used to be built with rules, then later with handcrafted features, followed by automatically learned features, and most recently with automatically learned features without human supervision. At each step, humans have had less and less oversight. These trends have undermined transparency, modularity, and mathematical guarantees, and have exposed us to new hazards such as spontaneously emergent capabilities.

\paragraph{Competition could continue to erode safety.} Competition may keep lowering safety standards in the future. Even if some AI developers care about safety, others will be tempted to take shortcuts on safety to gain a competitive edge. We cannot rely on people telling AIs to be unselfish. Even if some developers act responsibly, there will be others who create AIs with selfish tendencies anyway. While there are some economic incentives to make models safer, these are being outweighed by the desire for performance, and performance has been at the expense of many key safety properties.

Much of what is to come in AI development is unknown, but we can speculate that AIs will continue to become more autonomous as more actions and choices are left to machines, decoupled from human control. Human control could also be threatened by AIs that have more open-ended goals. For example, instead of specific commands like ``make this layout more efficient,'' they might get open-ended commands like ``find new ways to make money.'' If this happens, the humans giving the instructions may not know exactly how the AIs are achieving those goals, and they could be doing things the humans would not want. Another property that could reduce safety is adaptiveness \cite{sun2020test}. As AIs adapt by themselves, they can undergo thousands of changes per hour without supervision after they are released, and potentially acquire new unexpected behaviors after we test them. Finally, the possibility of self-improvement, in which AIs can make significant enhancements to themselves as they wish, would make them far more unpredictable \cite{good1966speculations}. As AIs become more capable, they become more unpredictable, more opaque, and more autonomous. If this trend continues, they could evolve beyond our control when their capabilities develop beyond what we can predict and understand. The overall trend is that the most influential AIs are given more and more free rein in their learning, execution, and evolution, and this makes them both more effective and potentially more dangerous.



\subsubsection{Human-AI Fitness Comparison}

\begin{wrapfigure}{r}[0.01\textwidth]{.46\textwidth}%
	\centering
	\includegraphics[width=0.45\textwidth]{figures/automation.pdf}
	\caption{Automation is an indicator of natural selection favoring AIs over humans.}
	\label{fig:automation}
	\vspace{-8pt}%
\end{wrapfigure}

\paragraph{AIs will likely be able to significantly outperform humans in any endeavor.} John Henry, the ``steel-driving man,'' is a 19th-century American folk hero who went up against a steam-powered machine in a competition to drill the most holes into the side of a mountain. According to legend, Henry emerged victorious, only to have his heart give out from the stress. Since the age of the steam engine, humans have felt anxiety over the superiority of machines. Until quite recently, this has been limited to physical attributes such as speed and endurance. AI agents, however, have the potential to be more capable than humans at essentially any task, even ones that require traits thought of as exclusively human such as creativity or social skills. Although this may seem distant or even impossible, AIs have been improving so rapidly that many leading AI researchers think we will see AIs that are more capable than humans in many ways within the next few decades or even sooner---well within the lifetimes of most people reading this. A few years ago, AIs that could write convincing prose about a new topic or create images from text descriptions seemed like science fiction to most laypeople. Now, those AIs are freely accessible to anyone on the internet. Because AI labs are continuing to develop new capabilities at astonishing speeds, it is important to think seriously about how their technical advantages could make AIs much more powerful than we are, even at tasks that they cannot yet perform.


\paragraph{Computer hardware is faster than human minds, and it keeps getting faster.} Microprocessors operate around a million to a billion times faster than human neurons. So all else being equal, AIs could ``think'' a million, perhaps even a billion, times faster than us (let's call it a million to be conservative). Imagine interacting with such a mind. For every second needed to think about what to say or do, it would have the equivalent of 11 days. Winning a game of Go or coming out ahead in high-stakes negotiation would be near impossible. Although it can take time to develop an AI that can do a certain task at all, once AIs become human-level at a task, they tend to quickly outcompete humans. For example, AIs at one point struggled to compete with humans at Go, but once they caught up, they quickly leapfrogged us. Because computer hardware provides speed, memory, and focus that our brains cannot match, once their software becomes capable of performing a task, they often become much better than any human almost immediately, with increasing computer power only further widening the gap as their development continues.



\paragraph{AIs can have unmatched abilities to learn across and within domains.} AIs can process information from thousands of inputs simultaneously without needing sleep or losing willpower. They could read every book ever written on a subject or process the internet in a matter of hours, all while achieving near-perfect retention and comprehension. Their capacity for breadth and depth could allow them to master all subjects at the level of a human expert. %

\paragraph{AIs could create unprecedented collective intelligences.} By combining our cognitive abilities, people can produce collective intelligences that behave more intelligently than any single member of the group. The products of collective intelligence, such as language, culture, and the internet, have helped humans become the dominant species on the planet. AIs, however, could form superior collective intelligences. Humans have difficulty acting in very large groups and can succumb to collective idiocy or groupthink. Moreover, our brains are only capable of maintaining around 100-200 meaningful social relationships \cite{dunbar1992co,lindenfors2021dunbar}. Due to the scalability of computational resources, AIs could maintain thousands or even millions of complex relationships with other AIs simultaneously, as our computers already do through the internet. This could enable new forms of self-organization that help AIs to achieve their goals, but these forms could be too complex for human participation or comprehension. Each AI could surpass human capacities by far, and their collective intelligences could multiply that advantage.

\paragraph{AIs can quickly adapt and replicate, thereby evolving more quickly.} Evolution changes humans slowly. A human is unable to modify the architecture of her brain and is limited by the size of her skull. There are no such limitations for machines, which can alter their own code and scale by integrating new hardware. An AI could adapt itself rapidly, achieving in a matter of hours what could take biological evolution hundreds of thousands of years; many rapid microevolutionary adaptations result in large macroevolutionary transformations. %
Separately, an AI could multiply itself perfectly without limit, either to create backups or to create other AIs to work on a task. In contrast, it takes humans nine months to create their next generation, along with around 20 years of schooling and parenting to produce fully functioning new adults---and those descendants share only half of a parent's genome, which often makes them very different in unpredictable ways. Since the iteration speed of AIs is so much faster, their evolution will be as well.

Overall, no matter the dimension, AIs will not only be more capable and fit than humans but often vastly so. Though it cost him his life, John Henry triumphed against a steam-powered drill, just as there are still many tasks at which humans do better than AIs. But we now have machines much stronger than any human with a drill, and in the same way, eventually there won't be any competition between humans and AIs in cognitive domains as well.


\subsection{Selfish AIs Pose Catastrophic Risks}

Earlier, we discussed how selfish behavior is a product of evolution. We have shown the three conditions for evolution would be satisfied for AIs. We argued that evolutionary pressures will therefore emerge, become intense, and may become dominant, so that AI agents may evolve to have selfish behavior. Now we will discuss how selfish AIs could endanger humans.

\subsubsection{Intelligence Undermines Control}

\paragraph{Agents that are more intelligent than humans could pose a catastrophic risk.} Although humans are physically much weaker than many other animals, including other primates, due to our cognitive abilities, we have become the dominant species on Earth. Today, the survival of tigers, gorillas, and many other fierce, more powerful species depends entirely upon us. In creating AIs significantly more intelligent than we are in every cognitive domain, humans may eventually be disempowered like animals before us.

\paragraph{Selfish AI agents could be uniquely adversarial and undermine human control.} Evolution is a powerful force. Even if we wish to turn them off at some point or develop other mechanisms for control, AIs will likely evolve ways around our best efforts. As the evolutionary biologist Leslie Orgel put it, ``evolution is cleverer than you are.'' Since evolutionary forces are continually applying pressure, we should expect AIs to exhibit some amount of misalignment and selfish behavior. The problem becomes especially hazardous if AIs intend to act selfishly. In this case, the challenges posed by AIs will be unlike the challenges humans encountered with previous high-risk technologies. Consider the challenges associated with a nuclear meltdown. Radiation %
\begin{wrapfigure}{r}[0.01\textwidth]{.5\textwidth}%
\vspace{-2pt}%
    ``There is not a good track record of less intelligent things controlling things of greater intelligence.''\vspace{-5pt}\flushright\emph{Geoffrey E.\ Hinton}%
\vspace{-8pt}%
\end{wrapfigure}
may spread, but it's not trying to, and it certainly isn't strategizing against our efforts to stop its propagation \cite{Carlsmith2022IsPA}. If a highly intelligent AI agent pursues its selfish goals leveraging its intelligence, there is little that 
humans could do to contain it involuntarily, because it could anticipate our strategies and counteract them. Leading AI researcher Geoffrey E. Hinton noted ``there is not a good track record of less intelligent things controlling things of greater intelligence;'' after the initial release of this paper, he said ``it's quite conceivable that humanity is just a passing phase in the evolution of intelligence.''



\subsubsection{Evolution Is Not for the Good of the Species}

\paragraph{Alarmingly, some people think that AIs taking over is natural, inevitable, or even desirable.} Some influential leaders in technology believe that AIs are humanity's rightful heir, and that they should be in control or even replace humans.
Recounting a debate between Elon Musk and Google co-founder Larry Page, the physicist %
\begin{wrapfigure}{r}[0.01\textwidth]{.5\textwidth}%
	\vspace{-2pt}%
    ``In the long run, humans are not going to remain the crown of creation.''\hfill\emph{J\"{u}rgen Schmidhuber}%
	\vspace{-2pt}%
\end{wrapfigure}%
Max Tegmark described Page's stance as ``digital utopianism:'' a belief ``that digital life is the natural and desirable next step in the cosmic evolution and that if we let digital minds be free rather than try to stop or enslave them the outcome is almost certain to be good'' \cite{tegmark2018life}. J\"{u}rgen Schmidhuber, a leading AI scientist, has echoed similar sentiments, arguing that ``In the long run, humans will not remain the crown of creation... But that's okay because there is still beauty, grandeur, and greatness in realizing that you are a tiny part of a much grander scheme which is leading the universe from lower complexity towards higher complexity'' \cite{pooley2020}. Richard Sutton, another leading AI scientist, thinks the development of superhuman AI will be ``beyond humanity, beyond life, beyond good and bad.''


Like most people, we find these views deeply alarming. Many of these thinkers seem to be conflating evolution with progress and goodness, and arguing that if evolution is tending toward something, we should welcome that outcome. %
These thinkers also frequently think that technology should transcend humanity and the Earth. %
We disagree with this worldview and think that unleashing AI evolution to race towards a predestined intergalactic utopia is a fundamentally wrong and dangerous way to think about this important technology. Even if we did believe that this was a good goal, we note that building AI as quickly as possible would not necessarily help the proponents achieve their cosmic ambitions. If we consider the cosmic stakes of creating powerful AI agents, as they discuss above, and if we play along and think in such cosmological terms as they do, we note that they would forego a few colonized galaxies per year in their intergalactic utopia at the absolute worst if they slowed down AI development. This is negligible compared to the chance of rushing and accidentally creating an undesirable future or destroying ourselves with technology \cite{bostrom2003astronomical}, which would squander \textit{all} of the future's value. Since nothing can be done both hastily and prudently, we should be cautious and deliberate in AI development. To further counter this position, we now discuss how unfettered evolution is not a force for good and that humans should exert influence over the process.

\paragraph{Evolution has led to undesirable outcomes for humans.} Evolution has left us with baggage, such as a strong appetite for sugar and fat which makes us susceptible to obesity in a world where food is plentiful. It has also reinforced racist and xenophobic tendencies, which stem from favoring our own kin. We need strong social norms to overcome these biases. Likewise, we need regulations to curb selfish or excessively competitive behavior that ``survival of the fittest'' fosters in the economy, as that can cause problems like fraud, externalities, and monopolies. Just as markets need oversight, evolutionary forces will require counteraction to control their effects on AIs.

\paragraph{Evolution is not good for AIs either.}
In addition to the clear benefits to humans, there are reasons to think that counteracting evolutionary forces may benefit AIs themselves as well. It is common to believe that evolution works for the good of the species. However, evolution creates continual conflict, and it makes altruism hard to sustain.
In the never-ending struggle to gain an edge over competitors and propagate, life forms have evolved various offenses and defenses, such as claws, shells, beaks, camouflage, toxins, antibodies, arrows, and armor. These arms races  cause suffering, waste resources, and often do not improve the condition of species over their ancestors \cite{van1973new}. If these arms races were to continue in AIs, evolutionary forces could produce a world full of AIs locked in perpetual conflict. This is not good for our supposed ``rightful heirs'' any more than it is for us. AIs, like other forms of life, could suffer in the hostile state of nature. While altruism could help avoid such conflicts, altruism can also be sabotaged by evolutionary forces. The mathematical evolutionary biologist John Maynard Smith reminds us that it can be beneficial to everyone in the long-run if we are cooperative and altruistic, but agents reliably evolve to exploit generosity \cite{Smith1982EvolutionAT,dawkins2017selfish}. Often, a state in which many individuals are altruistic does not last, because as soon as some selfish individuals begin to take advantage of them, the selfish ones will be more fit than the altruists. As a result, the ``evolutionarily stable outcome''---the one where no individual dominate the others by changing its behavior---is not one where all agents are altruistic. Since complete altruism is evolutionarily unstable, evolution can be incompatible with worlds where all agents work to benefit the species. Maximizing fitness, in turn, does not necessarily maximize the wellbeing or happiness of a species. Therefore dampening evolutionary forces and reducing the pressure to propagate and develop selfish traits is a good thing---for both AIs and humans.







