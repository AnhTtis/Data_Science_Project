\section{Conclusion}\label{sec:conclusion}


At some point, AIs will be more fit than humans, which could prove catastrophic for us since a survival-of-the-fittest dynamic could occur in the long run. AIs very well could outcompete humans, and be what survives. Perhaps altruistic AIs will be the fittest, or humans will forever control which AIs are fittest. Unfortunately, these possibilities are, by default, unlikely. As we have argued, AIs will likely be selfish. There will also be substantial challenges in controlling fitness with safety mechanisms, which have evident flaws and will come under intense pressure from competition and selfish AIs.

The scenario where AIs pose risks is not mere speculation. Since evolution by natural selection is assured given basic conditions, this leaves only a question of evolutionary pressure's intensity, rather than whether catastrophic risk factors will emerge at all. The intensity of evolutionary pressure will be high if AIs adapt rapidly---these rapidly accumulating changes can make evolution happen more quickly and increase evolutionary pressure. Similarly, the intensity of evolutionary pressure will be high if there will be many varied AIs or if there will be intense economic or international competition. Since high evolutionary pressure is plausible, AIs would plausibly be less influenced by human control, more `wild' and influenced by the behavior of other AIs, and more selfish.

The outcome of human-AI coevolution may not match hopeful visions of the future. Granted, humans have experienced co-evolving with other structures that are challenging to influence, such as cultures, governments, and technologies. However, these have never been able to seize control of the broader world's evolution before. Worse, unlike technology and government, the evolutionary process can go on without us; as humans become less and less needed to perform tasks, eventually nothing will really depend on us. There is even pressure to make the process free from our involvement and control. The outcome: natural selection gives rise to AIs that act as an invasive species. This would mean that the AI ecosystem stops evolving on human terms, and we would become a displaced, second-class species.

Natural selection is a formidable force to contend with. Now that we are aware of this larger evolutionary process, however, it is possible to escape and thwart Darwinian logic. To meet this challenge, we offer three practical suggestions. First, we suggest supporting research on AI safety. While no safety technique is a silver bullet, together they can help shape the composition of the evolving population of AI agents and cull unsafe AI agents. Second, looking to the farther future, we advocate avoiding giving AIs rights for the next several decades and avoid building AIs with the capacity to suffer or making them worthy of rights. It is possible that someday we could share society with AIs equitably, yet by prematurely circumscribing limitations on our ability to influence their fitness, we will likely enter a no-win situation. Finally, biology reminds us that the threat of external dangers can provide the impetus for cooperation and lead individuals to set aside their differences. We therefore strongly urge corporations and nations developing AIs to recognize that AIs could pose a catastrophic threat and engage in unprecedented multi-lateral cooperation to extinguish competitive pressures. If they do not, economic and international competition would be the crucible that gives rise to selfish AIs, and humanity would act on the behalf of evolutionary forces and potentially play into its hands.









\subsubsection*{Acknowledgements}
I would like to thank Avital Morris, David Lambert, Euan McLean, Thomas Woodside, Ivo Andrews, Jack Ryan, Kyle Gracey, and Justis Mills for their help and feedback.
