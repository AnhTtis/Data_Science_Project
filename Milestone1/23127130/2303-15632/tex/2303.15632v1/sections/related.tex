

Concept-based explanation methods are a popular class of model explanation methods that explain model behaviour and outputs with human-understandable semantic concepts. 
They can be categorized along several axes.
First, there are methods that explain some aspect of a model post-hoc~\cite{bau2017netdissect,fong2018net2vec,kim2018tcav,ramaswamy2022elude,zhou2018ibd}.
NetDissect~\cite{bau2017netdissect} and Net2Vec~\cite{fong2018net2vec} give insights about what the model has learned by identifying which neuron or which set of neurons encode a specific concept.
On the other hand, TCAV (Testing with Concept Activation Vectors)~\cite{kim2018tcav} learns vectors in the model activation space that corresponds to concepts and use them to quantify how sensitive the model's predictions are to a specific concept. Other methods focus on explaining how a model predicts a certain target class~\cite{ramaswamy2022elude,zhou2018ibd}.
IBD (Interpretable Basis Decomposition)~\cite{zhou2018ibd} first learns concept vectors in the model activation space, then decomposes the model's prediction in terms of these vectors. 
ELUDE (Explanation via Labelled and Unlabelled DEcomposition)~\cite{ramaswamy2022elude} also decomposes the model's prediction, but focuses on characterizing what portion of the prediction can and cannot be explained with available concepts.

More recently, concept-based ``interpretable-by-design'' models have been proposed~\cite{koh2020conceptbottleneck,marcos_accv_2020,radenovic2022neural,dubey2022scalable}.
They all take the concept bottleneck approach where the full model consists of a bottleneck that recognizes concepts in an input image and a discriminative model that classifies the image based on the bottleneck outputs (concept scores).
The discriminative model is parameterized as an interpretable model (e.g., linear model, generalized additive model), which makes the full model interpretable, i.e., an interpretable discriminative model reasoning with interpretable features (concepts recognized by the bottleneck).
The differences between each work lie in their choice of parameterization and training algorithm for the bottleneck and the discriminative model.

Although all these methods are called ``concept-based,''
the relationship between them are largely unclear.
This poses a huge challenge to researchers and practitioners who want to compare different methods' capabilities, constraints, and (implicit) assumptions.
In this work, we address this challenge by presenting a unified method that encapsulates and characterizes all aforementioned methods with respect to two axes: faithfulness and understandability.

Our work is similar in spirit to a growing body of works that introduce analyses and frameworks to better understand, evaluate, and compare model explanation methods~\cite{adebayo2018sanity,adebayo2022iclr,adebayo2020neurips,han2022LFA,kim2022hive,ramaswamy2023overlookedfactors,sokol2020explainability}.
In particular, Han et al.~\cite{han2022LFA} also argue the need for a unified framework and propose one for attribution heatmap explanations (specifically perturbation and gradient-based methods), which aim to explain which input features are relevant to a model's output decision.
Ramaswamy et al.~\cite{ramaswamy2023overlookedfactors} examine concept-based methods as we do, but they focus on analyzing commonly overlooked factors: probe dataset choice, concept learnability, and number of concepts used in an explanation.
On the other hand, we introduce a unified method (UFO) that deepens our understanding of the behavior of concept-based methods and their relation to one another.

UFO also enables researchers and practitioners to generate customized explanations by turning the knobs of faithfulness and understandability.




