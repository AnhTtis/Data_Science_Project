We now propose our unified method (Understandability and Faithfulness Operationalization) for concept-based explanations, which explain some aspect of the model's output (either its final prediction or intermediate scores) in terms of a set of pre-defined semantic concepts. We develop this as a post-hoc explanation method: generating an explanation when given access to the trained model as well as a \emph{probe dataset}, a dataset similar to the training dataset that is labelled with a set of concepts. 

\smallsec{Given} Concretely, consider an image classification CNN model $F\colon\mathcal{X} \rightarrow \mathbb{R}^D$, which outputs a vector of dimension $D$ when given an image $x\in\mathcal{X}$ as an input, and let $y(x) = \argmax F(x)$, which outputs a model's prediction. 
Note that $F$ here may be the final or an intermediate layer output of the model, corresponding to the layer we aim to explain.\footnote{If $F$ is the final layer of the model, we will demonstrate in~\cref{ssec:operationalization} that our framework can be adapted to explain the model's predicted class $y(x) = \argmax F(x)$ instead of the model's full output.}

We want to explain this output in terms of $C$ semantic concepts. To do so, we use a probe dataset $X\subset \mathcal{X}$, where each $x \in X$ is annotated $A(x) \in \{0,1\}^C$ with the presence or absence of each of these concepts. By correlating the presence or absence of these concepts with the model's output, we can identify what human-understandable concepts are contributing to the model's predictions. 

\smallsec{Explanation framework} We consider $F=g \circ f$ as a combination of an intermediate function $f \colon \mathcal{X} \rightarrow \mathbb{R}^n$, which produces a set of $n$ image features (that we will attempt to explain using the $C$ concepts), and a function $g \colon \mathbb{R}^n \rightarrow \mathbb{R}^D$, which combines the features into the $D$-dimensional output.

We explain $F$ by learning two functions $h_{\text{conc}}\colon \mathbb{R}^n \rightarrow \mathbb{R}^C$, which maps the features $f$ to the concepts, and a function $h_\text{pred}\colon\mathbb{R}^C \rightarrow \mathbb{R}^D$, which maps the concepts to the model output $F$. Then, we have two different objectives: 
\begin{enumerate}
    \item To maximize the \textbf{faithfulness} of the explanation. Simply put, this objective requires the explanation to mimic the model's output as far as possible, i.e., $F(x) \approx h_\text{pred} \circ h_{\text{conc}} \circ f(x)$. %for images $x \in X$.
    \item To maximize the \textbf{understandability} of the explanation. An explanation that perfectly mimics the model but is not human-understandable doesn't make the model more interpretable: humans need to be able to parse the explanation resulting from $h_\text{pred}$ and $h_{\text{conc}}$. 
\end{enumerate}

Currently, there is no clear consensus on what these two different terms could mean. In the following subsections, we explore the definitions that each of these objectives could take and highlight how our proposed definitions can describe existing concept-based explanations. 

\subsection{Faithfulness} 
We can vary the faithfulness of an explanation by changing the definition for how an explanation should mimic a model's output (i.e., how an explanation of the form $h_{\text{pred}} \circ h_{\text{conc}}$ approximates $g$, the latter part of a model). We describe three definitions below:
\begin{enumerate}
\itemsep0em
\item \textbf{Most faithful (\MF).} 
First, we can match the full $D$-dimensional output of our explanation with that of the model
for all possible images, not just those in the probe dataset (i.e., $\mathcal{X}$ instead of $X$).
One way to achieve this would be to learn $h_\text{conc}$ and $h_\text{pred}$ such that $h_\text{pred} \circ h_\text{conc} = g$. 

\item \textbf{\textcolor{black}{Somewhat faithful} (\SF).}
Next, we can relax our first definition and require that the full outputs of the explanation and model match only for images in the probe dataset.
Then, rather than requiring $h_\text{pred} \circ h_\text{conc} = g$, we would minimize the following mean-squared error (MSE) loss for all $x \in X$:
\begin{equation}
    \| g \circ f(x) - h_\text{pred} \circ h_\text{conc} \circ f(x)\|.
\end{equation}
This would be potentially more useful to developers who wish to debug or improve a model: explanations that model the full score distribution allow for better diagnoses of the model. 

\item \textbf{\textcolor{black}{Least faithful} (\LF).} 
Finally, instead of mimicking the model's full output, our explanation can mimic just the model's prediction.
Then, we would minimize the following cross-entropy (CE) loss for all $x \in X$: 
\begin{equation}
    CE(y(x),  h_\text{pred} \circ h_\text{conc}).
\end{equation}
This would be more useful for end-users of the model, who just want to understand how a specific prediction is being made. 
\end{enumerate} 

\subsection{Understandability}
\label{subsec:understandability}
The understandability of an explanation can vary along the following three axes:

\smallsec{Complexity of functions} The choice of the two learned explanation functions $h_\text{conc}$ and $h_\text{pred}$ can vary the understandability of the explanation.
These can be general functions; however, they are typically chosen as linear functions in most concept-based explanation methods.
A linear $h_\text{conc}$ makes it easy to learn $h_\text{conc}$, while a linear $h_\text{pred}$ makes it easy for a human to understand a model's prediction as a linear combination of concepts.

\smallsec{Number of concepts} 
    Prior works have shown that humans can realistically only reason with a small number of $K$ concepts (typically with $K=16$ or $32$)~\cite{ramaswamy2023overlookedfactors}. 
    Thus, we allow the user to select $K$ concepts using a selection matrix $S$ that picks $K$ out of $C$ concepts and let $\mathbbm{1}_S$ be the indicator vector of these $K$ chosen concepts. Then, we can describe our explanation as $h_\text{pred}  \circ \mathbbm{1}_S \circ h_\text{conc}\circ f(x)$.

\smallsec{Concept encoding} 
    Humans can more easily reason with binary values for concepts (e.g., is a bed present or absent?) than they can with continuous values for concepts (e.g., the probability of bed being present is 0.74). 
    Thus, we can describe how concepts are encoded as binary values (\MU), probabilities (\SU), or continuous values (\LU).
    When given a concept encoding $u \in \mathcal{C}$, we define $\mathcal{C}$ as follows:
    \begin{enumerate}[noitemsep,topsep=0pt]
    \item \textbf{\textcolor{black}{Most understandable} (\MU).} $\mathcal{C} = \{0,1\}^C$.% using $A_i(\cdot)$
    \item \textbf{\textcolor{black}{Somewhat understandable} (\SU).} $\mathcal{C} = [0,1]^C$.% using $\sigma(h_\text{pred}(\cdot))$
    \item \textbf{\textcolor{black}{Least understandable} (\LU).} $\mathcal{C} = \mathbb{R}^C$.% using $h_\text{pred}(\cdot)$.
    \end{enumerate}
    
In our main experiments, we use linear functions for $h_\text{conc}$ and $h_\text{pred}$, set $K = 16$ concepts, and vary understandability based on how concepts are encoded.


\subsection{Operationalization of explanations}
\label{ssec:operationalization}
Now, we instantiate specific objective functions using our different definitions of faithfulness and understandability.
Our objective functions are all of the following form, where we find the optimal selection matrix $S$ that chooses $K$ semantic concepts, and the optimal functions $h_\text{conc}$ mapping the features $f$ to the $C$ semantic concepts and $h_\text{pred}$ mapping these concepts to the model predictions:
\begin{align}
    \argmin_{h_\text{conc}, h_\text{pred}, S} \lambda_1 L_{\text{mimic}} + \lambda_2 L_{\text{align}}
\end{align}
Here, $L_\text{mimic}$ varies based on the specific definitions used and describes how an explanation should mimic a model, while $L_\text{align}$ is fixed as follows and describes how $h_\text{conc}$ aligns features to concepts for images in the probe dataset:
\begin{align}
    L_\text{align} = \sum_{x \in X} \sum_i CE(\mathbbm{1}_S\circ h_\text{conc}\circ f(x)_i, \mathbbm{1}_S \circ A_i(x))
\end{align}
Then, hyperparameters $\lambda_1$ and $\lambda_2$ allow us to prioritize the mapping from the features to the concepts over the mapping from the concepts to the final output, and vice-versa.

We start with the simplest definition of $L_\text{mimic}$.

\noindent\textbf{Least understandable}, \textbf{\textcolor{black}{most faithful} (\LU, \MF)}:
\begin{equation}
\label{eq:mf}
L_\text{mimic}^\text{\LU, \MF} = 
   \sum_{x \in \mathcal{X}}\|g \circ f(x) - h_\text{pred} \circ \mathbbm{1}_S \circ h_\text{conc}\circ f(x)\|
\end{equation}  
However, decomposing a general $g$ into $h_\text{conc}$ and $h_\text{pred}$ is not tractable, as this would require us to minimize the above for all images $x \in \mathcal{X}$ (not just those in the probe dataset $X$).

Instead, we consider only losses that are tractable by using less strict definitions of faithfulness below.\footnote{Changes to the previous equation are denoted in \textbf{\textcolor{red}{bolded red}}.}  

\noindent {\bf \textcolor{black}{Least understandable}, \textcolor{black}{somewhat faithful}  (\LU, \SF):} 
\begin{equation}
\label{eq:sf_lu}
   L_\text{mimic}^\text{\LU, \SF} = \sum_{\textcolor{red}{\boldsymbol{x \in X}}} \|g\circ f(x) - h_\text{pred}  \circ \mathbbm{1}_S \circ h_\text{conc}\circ f(x)\| 
\end{equation}

\noindent Here, we are only concerned mimicking full outputs on the probe dataset and thus have a tractable objective.

\cref{eq:sf_lu} can also be modified to be more understandable.
Rather than using the continuous output of $h_\text{conc} \circ f(x)$, we could also use a probabilistic version of it.

\noindent {\bf \textcolor{black}{Somewhat understandable}, \textcolor{black}{somewhat faithful} \ (\SU, \SF):} 
\begin{equation}
\label{eq:sf_su}
   L_\text{mimic}^\text{\SU, \SF} = 
   \sum_{x \in X} \|g \circ f(x) - h_\text{pred}  \circ \mathbbm{1}_S \circ 
   \textcolor{red}{\boldsymbol{p}} \circ h_\text{conc}\circ f(x)
   \|
\end{equation}
where $p \colon \mathbb{R}^n \to [0, 1]^C$ is a function that maps features to probabilities (e.g. the sigmoid function).

We can make the explanation even more understandable by replacing $p \circ h_\text{conc}\circ f(x)$ entirely with the binary, ground-truth attributes encoded by $A(\cdot)$, i.e., explaining the modelâ€™s output with perfect knowledge of the concepts.

\noindent {\bf \textcolor{black}{Most understandable}, \textcolor{black}{somewhat faithful}, (\MU, \SF):} 
\begin{equation}
\label{eq:sf_mu}
   L_\text{mimic}^\text{\MU, \SF} = \sum_{x \in X} \|g \circ f(x) - h_\text{pred}  \circ \mathbbm{1}_S \circ \textcolor{red}{\boldsymbol{A(x)}}\|
\end{equation}

Finally, we could use the least strict definition of faithfulness, where we only care about mimicking the single model prediction $y(x)$ rather than its full output $g \circ f(x)$. This can be paired with all three understandability definitions.

\noindent {\bf \textcolor{black}{Least understandable}, \textcolor{black}{least faithful}  (\LU, \LF):}\footnote{Changes to $L_\text{mimic}^\text{\LU, \SF}$ given by \cref{eq:sf_lu} are denoted in \textcolor{NavyBlue}{\textbf{bolded blue}}.}
\begin{equation}
\label{eq:lf_lu}
   L_\text{mimic}^\text{\LU, \LF} = \sum_{x \in X} \textcolor{NavyBlue}{\boldsymbol{CE}}(y(x),h_\text{pred}  \circ \mathbbm{1}_S \circ h_\text{conc}\circ f(x))
\end{equation}

\noindent {\bf Somewhat understandable, \textcolor{black}{Least faithful} (\SU, \LF):} 
\begin{equation}
\label{eq:lf_su}
   L_\text{mimic}^\text{\SU, \LF} = \sum_{x \in X} CE(y(x) , h_\text{pred}  \circ \mathbbm{1}_S \circ 
   \textcolor{red}{\boldsymbol{p}} \circ h_\text{conc}\circ f(x) )
\end{equation}


\noindent {\bf Most understandable, \textcolor{black}{least faithful}(\MU, \LF):} 
\begin{equation}
\label{eq:lf_mu}
   L_\text{mimic}^\text{\MU, \LF} = \sum_{x \in X} CE (y(x) , h_\text{pred} \circ \mathbbm{1}_S \circ \textcolor{red}{\boldsymbol{A(x)}})
\end{equation}

\subsection{Optimization}
The trickiest part of the optimization is the selection of $K$ concepts out of $C$ concepts in total, since this is non-differentiable. Based on prior work, we assume $h_\text{pred}$ and $h_\text{conc}$ to be linear functions, and use a group Lasso regularization~\cite{yuan2006model} that forces the squared sum of the coefficients of a concept to 0. That is, $h_\text{pred}$ is learned as a coefficient matrix $W_\text{pred} \in \mathbb{R}^{D \times C}$. Assuming that the columns of $W_\text{pred}$ are sorted in increasing order of their squared $\ell_2$ norm ($\sum_{j =1}^D (W_\text{pred})_{j, i}^2$) (i.e. 1st column has smallest $\ell_2$ norm, followed by 2nd column, etc.), during training, we add the following regularization loss: 
\begin{equation}
\label{eq:regularization}
L_\text{reg} = \sum_{i=1}^{C-K} \Big|\sum_{j =1}^D (W_\text{pred})_{j, i}^2 \Big|
\end{equation}

We gradually increase the weight of this loss during training, initially allowing the $W_\text{pred}$ to use all concepts to identify the most relevant ones, and then forcing the $C - K$ smallest columns of $W_\text{pred}$ to 0. 

