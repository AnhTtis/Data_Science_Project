\input{tables/explanation_accuracy.tex}

In~\cref{ssec:experiments_tradeoff}, we analyze the faithfulness-vs-understandability tradeoff by optimizing \cref{eq:sf_lu,eq:sf_su,eq:sf_mu,eq:lf_lu,eq:lf_su,eq:lf_mu} 
for the same model and examine which concepts are highlighted in each case. We find that an explanation's accuracy decreases as the explanation is made more understandable and that the concepts highlighted are highly dependent on the equation optimized. 
In~\cref{ssec:experiment_prior_work}, we compare our results to that of prior works~\cite{bau2017netdissect,zhou2018ibd,ramaswamy2022elude}. 

\subsection{Faithfulness-vs-understandability tradeoff}
\label{ssec:experiments_tradeoff}

\smallsec{Experimental setup}
\label{ssec:implementation_details}
We use a ResNet18~\cite{he2016resnet} model trained on Places365~\cite{zhou2017places} as our blackbox model to explain. This model takes as input an image and outputs a vector with the predicted probabilities of the image belonging to each of the 365 classes. Similar to ELUDE~\cite{ramaswamy2022elude}, we consider these predictions at 3 different granularities (using annotations provided in the dataset):

\begin{enumerate}[noitemsep,topsep=0pt]
    \item \textbf{Binary} (2 classes): ``indoor'' vs. ``outdoor'' scenes
    \item \textbf{Grouped} (16 classes): coarse-grain scene categories (e.g., ``home/hotel'' or ``forest/field/jungle'')
    \item \textbf{Fine-grained} (365 classes): scene labels (e.g., ``bedroom'' or ``bamboo forest'')
\end{enumerate}

For binary and grouped scene predictions, we replace and retrain the final layer of the model. For fine-grained predictions, we analyze the 365-class model but focus only on explaining the top 20 classes that are most represented within the probe dataset, to simplify computation.

We use the ADE20k~\cite{zhou2017ade20k,zhou2019ade20k_ijcv} dataset (license: BSD 3-Clause) as the probe dataset with which to generate explanations, splitting its images randomly into train (60\%, 11839 images), val (20\%, 3947 images), and test (20\%, 3947 images).
We use train to optimize~\cref{eq:sf_lu,eq:sf_su,eq:sf_mu,eq:lf_lu,eq:lf_su,eq:lf_mu},
val to pick hyperparameters (e.g., $\lambda_1, \lambda_2$), and test to measure the accuracy of the explanation (i.e., the number of images for which the discrete explanation output matches that of the model).
In~\cite{bau2017netdissect}, this dataset was further densely labelled with 1197 concepts comprising of objects, object parts, scenes, colors and textures; we use a subset of these concepts. First, we remove concepts that occur in fewer than 20 images within the training dataset. This gives us a set of 309 concepts. We further prune these concepts to remove concepts that are correlated with each other, following the findings of~\cite{ramaswamy2023overlookedfactors}. 
Correlations between concepts can be computed using the ground truth scores or the learned score of the concept. Surprisingly, we find that these result in very different correlations. Thus, we select the set of concepts separately for each setting of understandability (\MU, \SU, \LU). More details are given in the appendix.

\input{tables/finegrained_attr_whole_table.tex}

We use linear models for $h_\text{conc}$ and $h_\text{pred}$, with $f(x)$ as the output of the penultimate layer of the model. These are trained for 5000 epochs with batch size = 1024 using either cross entropy or MSE loss (F vs. FF) and the Adam~\cite{KB14Adam} optimizer with a learning rate = 1e-3. We set $\lambda_1$ and $\lambda_2$ as hyperparameters by picking the ratio $\lambda_1 \colon \lambda_2$ that has the highest validation accuracy.
%(see the appendix for details). 
Finally, we set the number of concepts used to $K = 16$ (for the binary classifier) or $K=32$ (for the grouped and fine-grained classifiers) concepts, based on prior work~\cite{ramaswamy2023overlookedfactors}. See the supp. mat. for details.

\subsubsection{How well do explanations emulate the model?}
\label{sssec:faithfulness}
We first analyze how well different explanations emulate the model as we vary the levels of faithfulness and understandability.
We do so by measuring and comparing the average L2 distance between the distributions output by the explanation and the model, which capture how well the explanation emulate the model. See Tab.~\ref{tab:faithful} for the full results.
We find that, as expected, the L2 distance is lower for the somewhat faithful (\SF) explanation, as compared to the least faithful (\LF) explanation. Moreover, we find that as the understandability increases, the faithfulness decreases:
the least understandable (\LU) explanation emulates the model better than the somewhat understandable (\SU) and the most understandable (\MU) explanations. This suggests a trade-off between faithfulness and interpretability: explanations that emulate the model well can be less understandable. Applications that require precise mimicking of the model's predictions (for example, if being used to debug a model) would be better explained using the less understandable, more faithful (\LU, \SF) version of UFO, whereas explanations being given to a lay-person might require higher understandability and thus, be better explained using a more understandable, less faithful (\MU, \LF) version of UFO. 

\subsubsection{How do ``important'' concepts change?}
\label{sssec:understandability}
Next we analyze how ``important'' concepts, i.e., concepts selected by the explanation, change as we vary the faithfulness and understandability objectives.
We find that the important concepts are very different under different objectives: 
~\Cref{tab:finegrained_attr} shows the three most important concepts (i.e., the three concepts with the highest absolute coefficients in $h_{pred}$) for a subset of coarse-grained scene groups.

We notice there are significant differences between the selected concepts under different settings of faithfulness and understandability. For example, consider the most understandable explanation setting. When using the somewhat faithful (\SF) setting, the top 3 concepts for the scene ``bedroom'' include ``bed,'' ``building'' and ``pillow'' (with a negative coefficient for ``building''). For the least faithful (\LF) setting, the top 3 concepts are instead ``cap'', ``buffet'', and ``saucepan.'' 
In order to quantify this better, we consider the overlap among the top 10 concepts for each class. We see that the median overlap is just 1 for fine-grained scenes and grouped scenes, and 2 for binary (Fig.~\ref{fig:conc_hist}).

\begin{figure}[t]
    \centering
   \includegraphics[width=\linewidth]{images/all_hist.pdf}
    \caption{
    \textbf{Concept overlap (\cref{sssec:understandability})}.
    For three Places365 models (binary, grouped, and fine-grained), we report the number of shared concepts selected by the six different types of explanations (\{\MU, \SU, \LU\} $\times$ \{\SF, \LF\}). Concretely, for each scene class, we identify the 10 most important concepts (based on the absolute coefficient value) for each type of explanation, and measure the overlap in concepts between each pair of explanations.
    We see that the average overlap is extremely low, between 1 and 2 concepts for most classes, indicating that the selection of the right understandability and faithfulness objectives is critical.}
    \label{fig:conc_hist}
\end{figure}

Given that the concept overlap is so low, we analyze what concepts are chosen by different types of explanations. We consider different aspects of the concepts chosen:  the frequency (fraction of images that contain the concept), the average size (fraction of the image occupied by the segmentation mask of the concept), and the learnability (measured using the normalized average precision and the ROC AUC). In general, we find that concepts highlighted by the most understandable (\MU) explanation tend to occur more often, be larger in size,  and be more learnable, as shown in Fig.~\ref{fig:understand}. For somewhat faithful explanations (\SF), we see that for all metrics, somewhat understandable (\SU) explanations contain more frequently occurring, larger and easier to learn concepts than less understandable (\LU) explanations. However, for less faithful (\LF) explanations, 
this trend does not always hold. We interpret this as follows.

For the \MU{} formulation, the \textit{frequency} of the concept occurrence is directly related to the amount of information encoded within the ground-truth concept annotations. Thus, concepts with higher base rates are more likely to be used within the explanation.
For \SU{} and \LU{} formulations, this is not the case, since we use either the learned scores for the concepts (either $h_\text{conc} \circ f(x)$ or a probabilistic version of it), and these continuous vectors contain information regardless of the base rates of the concepts. 
Next, the \textit{size} of the concept can influence its \textit{learnability}: larger concepts can be easier for a model to learn, thus, these concepts are more likely to be used to learn a scene class.
However, for the \SU{} and \LU{} formulations, we note that the concept encodings could include other information within it (potentially even additional unlabelled concepts). Thus, the selected concepts might be the ones that are themselves less learnable. 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/new_comp.png}    
    \caption{\textbf{Comparison of concepts chosen}. We compare the concepts based on their \textit{frequency}, the average \textit{size} of the concept within an image (fraction size) and the \textit{learnability} of the concept from the feature space (normalized AP), across the 6 ($\{$\MU, \SU, \LU $\} \times \{$\SF, \LF$\}$) settings. In general, we see that concepts chosen by the \MU{} setting are typically larger, occur more often and are easier to learn, compared to the \SU{} and \LU{} settings. }
    \label{fig:understand}
\end{figure}

\subsection{Comparison to prior work}
\label{ssec:experiment_prior_work}
Finally, we consider prior works that generate concept-based explanations and analyze them within our framework. We find that we are able to express all concept-based explanations~\cite{bau2017netdissect,fong2018net2vec,kim2018tcav,zhou2018ibd,koh2020conceptbottleneck,ramaswamy2022elude} in our framework (see supp. mat. for more details). For methods whose optimizations are slightly from ours~\cite{bau2017netdissect,zhou2018ibd,ramaswamy2022elude}, we also run the closest form of our optimization along with their method, and compare the results produced. 

\smallsec{NetDissect~\cite{bau2017netdissect}} 
Rather than explaining the full output of the network $F(x) \in \mathbb{R}^D$, NetDissect explains $F(x) \in \mathbb{R}$ for each neuron. Furthermore, NetDissect correlates the neuron's output with the pixel-level \emph{segmentation} of each concept so the formulation changes slightly to accommodate predicting concepts at an image-level to a pixel-level. Generally, this can be achieved with $\lambda_1 = 0$ and changing $L_{\text{align}}$ to denote how well aligned an individual neuron's output is to concept segmentations.
Within our optimization, we set $\lambda_1 = 0$ and optimize $h_\text{conc}$ to find the best semantic concept for each neuron. 
%Note that 
While this is different to the segmentation intersection over union (IOU) score that NetDissect uses,  we still identify similar explanations (Tab.~\ref{tab:netdissect_comp}).

\input{tables/netdissect_comparison.tex}


Concretely, as in NetDissect, we set $f$ to the output of the final convolutional layer in the ResNet18~\cite{he2016resnet}, which outputs a $7 \times 7$ feature map for each neuron. ADE20k~\cite{zhou2017ade20k,zhou2019ade20k_ijcv} is labelled with segmentation masks. Thus, for each of the 49 regions, we are able to identify the most common object (or object part) and use this as a coarse segmentation map. We compute 
alignment using normalized AP~\cite{hoiem2012error} between the coarse segmentation map and the output of the neuron.
Other implementation details remain the same as before.

\smallsec{Net2Vec~\cite{fong2018net2vec} and TCAV~\cite{kim2018tcav}} Similar to NetDissect~\cite{bau2017netdissect}, these methods aim to explain the output of an intermediate layer. 
Here, $h_\text{conc}$ is a linear function that finds the best mapping from the feature space $f$ for each concept, and our optimization is exactly the same as that of these methods. 

\smallsec{IBD~\cite{zhou2018ibd}}
Here, the method attempts to explain the logits of the model. It assumes that both $h_\text{conc}$ and $h_\text{pred}$ are linear functions, and that the feature space $f$ is the output of the final convolutional layer. IBD optimizes \cref{eq:sf_su}, and thus is a (\SU, \SF) method. They optimize this equation by first computing $h_\text{conc}$ as a sequence of orthogonal vectors (called an ``interpretable basis''). They then express the model output as a linear combination of these basis vectors, with each weight being positive. They also limit the number of concepts per target class. This can be modelled by optimizing $S$ and $h_\text{pred}$ per target class.
Here, the main difference is that IBD adds a non-negative constraint to the coefficients in $h_\text{pred}$. This constraint appears to significantly affect concept selection, such that the set of concepts selected using the constraint vs. without it do not overlap in general. (results in the supp. mat.) 

\smallsec{Concept Bottleneck models~\cite{koh2020conceptbottleneck,marcos_accv_2020,radenovic2022neural,dubey2022scalable}} 
Unlike the other methods, Concept Bottleneck (CB) models are ``interpretable-by-design''. These models are learned as a composition of two functions, one that maps images to concepts and another that maps concepts to the final output. We focus on one, the concept bottleneck model~\cite{koh2020conceptbottleneck}, but others are similar. In our framework, this is an (\LU, \SF) method, i.e, it optimizes~\cref{eq:sf_lu}\footnote{Koh et al.~\cite{koh2020conceptbottleneck} also try adding a sigmoid layer after predicting concepts (i.e, optimizing Eq.~\ref{eq:sf_su}), but found a significant drop in model accuracy.}, where $h_\text{conc}$ is the identity function and $h_\text{pred}$ is $g$ itself. All concepts are allowed to be used within the explanation. However, the explanations generated can be hard to understand given the large number of concepts used, and the continuous encoding of the concepts. Removing the constraint of the number of concepts selected make the optimizations between Concept Bottleneck and the (\LU, \SF) identical, and hence, we do not run experiments. 

\smallsec{ELUDE~\cite{ramaswamy2022elude}}
In this work, the authors ignore the mapping from the features to the concepts entirely, with $\lambda_2 = 0$. They use the least faithful (\LF) and most understandable (\MU) definition and optimize \cref{eq:lf_mu}. Rather than pre-deciding $K$, they add a regularization constraint when optimizing to use fewer concepts. 
The main difference between the optimization (\MU, \LF) and ELUDE is in its L1 penalty. 
ELUDE enforces that the number of concepts used per target class is minimized using an L1 penalty (the total number of concepts used might be large), whereas we use a grouped L1 penalty to minimize the total number of concepts used.

Thus, we optimize~\cref{eq:lf_mu} per target class. For each coarse-grain scene group, we compare the concepts selected using this modified formulation (using $K = 8$ concepts) to the ones selected by ELUDE and report the number of common concepts between these two explanations  in~\cref{fig:elude_comp} 
For a given class, we find that the number of common concepts increases with its the base rate of the class (i.e, if sufficient positive examples of the class exist, the concepts used by ELUDE and our formulation align well (\cref{fig:elude_comp}). 

\begin{figure}[t]
    \centering
    \vspace{-0.2in}
    \includegraphics[width=0.8\linewidth]{images/elude_comp.pdf}
    \caption{\textbf{Comparison with ELUDE.} The least faithful, most understandable (\MU, \LF) formulation of UFO produces explanations similar to ELUDE~\cite{ramaswamy2022elude}, particularly for scene classes that are frequent in the probe dataset. For each coarse-grained scene group, we plot the base rate of the scene class within ADE20k and the number of concepts shared between the explanations. 
    }
    \label{fig:elude_comp}
\end{figure}


