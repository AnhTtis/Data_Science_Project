
We present a unified method (UFO) that formalizes faithfulness and understandability as mathematical objectives and encapsulates existing concept-based explanation methods.
We show how tuning the knobs of these two objectives affects how much of the model's behavior is explained as well as how and why concept selection varies based on these objectives (e.g., more understandable explanations select concepts that are larger and more learnable ).
We also compare outputs of UFO to existing methods, and show that it is very similar to NetDissect and similar to ELUDE for well represented within the probe dataset.
Our work clearly synthesizes how existing works compare to one another for the first time and provides a useful paradigm through which future methods can be developed and described.

\smallsec{Acknowledgements} 
We thank Angelina Wang, Byron Zhang, Rohan Jinturkar and rest of the Princeton Visual AI Lab members (especially Nicole Meister) who provided helpful feedback on our work.
This material is based upon work partially supported by the National Science Foundation under Grants No. 1763642, 2145198 and 2112562. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We also acknowledge support from the Princeton SEAS Howard B. Wentz, Jr. Junior Faculty Award to OR, Princeton SEAS Project X Fund to RF and OR, Open Philanthropy Grant to RF, and NSF Graduate Research Fellowship to SK. 