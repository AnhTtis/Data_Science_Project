
As convolutional neural networks (CNNs) start to be used to make consequential decisions, such as those in medical diagnosis and treatment, there is a need for these models to be interpretable. Over the past decade, many model explanation methods have been proposed, ranging from local methods that explain a single prediction, e.g., by highlighting relevant pixels in an input image~\cite{chattopadhay2018gradcamplusplus,fong19understanding,Petsiuk2018rise,selvaraju2017gradcam,simonyan2013saliency,zeiler2014visualizing,zhang2016excitation,zhou2016cam}, to global methods that provide a higher-level understanding of what the model has learned and how it predicts a certain target class~\cite{bau2017netdissect,fong2018net2vec,ramaswamy2022elude,zhou2018ibd}. However, explanations often \emph{disagree} with each other~\cite{krishna2022disagreement},
making it difficult for users to choose which explanation method to use.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/unifying.png}
    \caption{Consider a CNN that outputs probabilities for the target classes (e.g., bedroom, studio). 
    % When needing an explanation, 
    For explanations, different users might have different objectives of (1) ``faithfulness'': matching probabilities of target classes (user 1, \textcolor{Maroon}{``somewhat faithful''}) vs. just matching the output class (user 2, \textcolor{ForestGreen}{``less faithful''}) and (2) ``understandability'': encoding concepts as probabilities (user 1, \textcolor{Purple}{``somewhat understandable''}) or binary values (user 2, \textcolor{NavyBlue}{``more understandable''}).
    Our proposed method unifies existing concept-based explanation methods for CNNs and enables users to control ``faithfulness'' and ``understandability.''}
    \label{fig:pull}
\end{figure}

One reason for this disagreement is the lack of consensus among researchers about the goals of these methods. While most methods attempt to optimize objectives like ``faithfulness'' and ``understandability,'' these terms are not well defined and can be translated into different mathematical objectives. For example, ``faithfulness'' requires that the explanation accurately describes the model's behaviour, but this can be thought of at different levels: Do we care about the distribution of all class scores from the model (as in \cite{zhou2018ibd,koh2020conceptbottleneck})? Or just the score for the target class (as in \cite{ramaswamy2022elude})? 
Similarly, ``understandability'' can mean different things to different explanation users: 
AI experts (e.g., model developers) may find an explanation that encodes concepts as probabilities understandable, while non-experts (e.g., lay end-users) may not and thus, prefer an explanation that encodes concepts as binary values~\cite{kim2023helpmehelptheai}.
In \cref{fig:pull}, we show how different users might want different levels of faithfulness and understandability in model explanations. 

In this work, we propose UFO, a novel concept-based CNN explanation method that formalizes (U)nderstandability and (F)aithfulness as mathematical (O)bjectives. 
UFO unifies existing methods~\cite{bau2017netdissect,fong2018net2vec,kim2018tcav,koh2020conceptbottleneck,zhou2018ibd} that provide an explanation for an output of a CNN model layer in terms of pre-defined semantic concepts. However, different from these works, UFO enables users to explicitly set the desired levels of faithfulness and understandability, and seamlessly obtain an explanation that is suitable for their needs. 

Our main contributions are as follows:
\begin{itemize}
    \item We operationalize the notions of ``faithfulness'' and ``understandability'' and propose a set of definitions for each of these terms. We then integrate them into a unified concept-based explanation method which is flexible enough to accommodate the different notions depending on the downstream application.
    \item We demonstrate how the method can be used to generate explanations of varying levels of faithfulness and understandability, and analyze where these explanations differ. This reveals a number of insights into how concept are selected by concept-based explanations, and which concepts are more likely to be used to explain a model. We demonstrate how frequency, size, and learnability of the selected concepts differ across the different explanation objectives.
    \item We discuss how our method generalizes and unifies most prior concept-based explanation approaches. We empirically validate this assertion by comparing our produced explanations to those from 3 prior works (NetDissect~\cite{bau2017netdissect}, IBD~\cite{zhou2018ibd} and ELUDE~\cite{ramaswamy2022elude}).  
\end{itemize}

UFO enables researchers and practitioners to reason concretely about choices for understandability and faithfulness, and compare to methods with similar incentives.  

