\section{Introduction}
\label{sec:intro}

\begin{figure}[ht!]

   \includegraphics[width=1.22\linewidth]{Figure1_new_ver7.png}
  
   \caption{A comparison of PSNR results between our training method (SDAT, \blue{blue} circles) and a standard training on the entire dataset (EDT, \red{red} circles) applied on in-house and various popular architectures \cite{ xing2022residual, li2023efficient} over Kodak \cite{li2008image} dataset. By using our training method we achieved better results compared to standard training across all architectures. In addition we achieved state-of-the-art (SOTA) results using \textbf{GRL} architecture.
   } 

   % (\textcolor{mygreen}{green} circles shows additional popular demosaic works \cite{mei2023pyramid,gharbi2016deep, ramakrishnan2019deep, zhang2021plug}).
\label{fig:intro}
\end{figure}


\begin{figure*}[!ht]
\noindent
    \begin{minipage}{0.24\textwidth}
    \captionsetup{justification=centering}
    \centering
    
     \begin{subfigure}[b]{\textwidth}
       \includegraphics[width=1\linewidth]{sec/urban_26_results/gt.png}
       \caption{GT image 26 from Urban100}
       \label{fig:urban_orig} 
    \end{subfigure}
     
    \vspace{5mm}
    
     \begin{subfigure}[b]{\textwidth}
       \includegraphics[width=1\linewidth]{sec/urban_26_results/gt_urban84.png}
       \caption{Crop from GT}
       \label{fig:urban_gt} 
    \end{subfigure}
    
    \end{minipage}%
    \hspace{2mm}
    \begin{minipage}{0.74\textwidth}
    \vspace{2mm}
        \begin{minipage}{1\textwidth}
            \captionsetup{justification=centering}
                            \label{fig:urban_gt_crop}
                \subfloat[Original RSTCANet-B]{
                    \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/originalB_urban84.png}%{exact_orig_B_medium_small_2.png}
                }
                \label{fig:urban_B}
                % \subfloat[Original RSTCANet-S]{
                %     \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/originalS_urban84.png}%{exact_orig_S_medium_small_2.png}
                %     %\vphantom{\includegraphics[width=0.2\textwidth,valign=c]{u_tgv2d_result}}
                % }
                %\captionsetup{justification=justified}
                \subfloat[RNAN]{
                    \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/rnan_urban84.png}%{exact_RNAN_medium_small_2.png}
                }
                \label{fig:tgv_ef_ipm_first_inst}
                \subfloat[Original RSTCANet-L]{
                \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/rstcanetL_urban84.png}%{exact_Rcst_L_medium_small_2.png}
            }
            \label{fig:RSTC_L}
        \end{minipage}%
        \vspace{5mm}
        \begin{minipage}{1\textwidth}
            
            \captionsetup{justification=centering}
            \subfloat[RSTCANet-B-SDAT]{
                \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/oursb_urban84.png}%{exact_OURS_b_medium_small_2.png}
            }
                            \label{fig:tgv_ef_ipm_final_inst_2}
            \captionsetup{justification=centering}
            \subfloat[GRL]{
                \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/grl_original_urban84.png}%{exact_OURS_b_medium_small_2.png}
            }
                \label{fig:tgv_ef_ipm_final_inst_2}
            \captionsetup{justification=centering}
            \subfloat[GRL-SDAT]{
                \includegraphics[width=0.3\textwidth,valign=b]{sec/urban_26_results/grl_ours_urban84.png}%{exact_OURS_b_medium_small_2.png}
            }
        \end{minipage}%
        \end{minipage}
			
			\captionsetup{justification=justified}
			\caption{Qualitative comparison of our method compared to other top methods: RNAN ,RSTCANet and GRL. The RNAN model has 9M parameters, RSTCANet presents 3 different model sizes: B, S, and L, having 0.9M, 3.1M, and 7.1M parameters respectively and GRL consists of 3.1M parameters. We demonstrate the results of our suggested training scheme to train the RSTCANet-B and GRL models. It is easy to see that  RSTCANet-B-SDAT model produces superior qualitative results compared to all original RSTCANet variants while having the least amount of parameters.}
			\label{fig:qual_res}
\end{figure*}


The task of image demosaicing is a crucial step in digital photography and refers to the process of reconstructing a full-resolution RGB color image from incomplete data obtained from the use of a color filter array (CFA), such as the Bayer pattern of GRBG. In digital cameras, CFA samples only a fraction of the image information, which makes the task of demosaicing complex and challenging. 
This task is part of a larger group of image restoration problems, such as denoising, deblurring, single image resolution, in-painting, removing JPEG compression, \etc. These tasks aim at recovering a high-quality image from degraded input data.  
Image restoration problems are usually considered ill-posed, in the sense that it is usually not possible to determine a unique solution that can accurately reconstruct the original image. This is particularly true for image demosaicing due to the fact that the red, green, and blue color channels are sampled at different locations and rates, which can cause severe aliasing issues. In recent years, the use of convolutional neural networks (CNNs) and transformers has shown significant success in addressing image restoration problems \cite{liang2021swinir, zhang2017learning, zhang2019residual,mei2023pyramid,xing2022residual, li2023efficient} in general, and for image demosaicing in particular \cite{gharbi2016deep, kokkinos2018deep, liu2020joint, niu2018low, qian2022rethinking, tan2018deepdemosaicking, xing2022residual}. \\ It is well known that inductive bias, which refers to the assumptions and prior knowledge that the model brings to the learning process, is necessary for the model convergence \cite{mitchell1980need}. But, in some cases, it can have the opposite effect and negatively impact the model's ability to generalize. This is true for deep neural networks as well and has triggered an extensive line of study, \eg \cite{geirhos2018imagenet, ritter2017cognitive}, just to mention a few. The bias can arise from various factors such as the network architecture, the training dataset, and others.
Our focus in this study is on the inductive bias of a given model caused by the trained dataset.
In the case of image demosaicing, biased solutions can cause artifacts, such as zippers, Moiré, color distortion, and more.
The most common cause of inductive bias is the well observed phenomenon that natural images are mostly smooth, in the sense that nearby pixels tend to have similar values \cite{hyvarinen2009natural, zontak2011internal}, however, there are more elements in the data that can cause a given model to induce bias which are less obvious and are more difficult to characterize.
% \sout{   induced in image demosiaicng task, where a common inductive bias is the well observed phenomenon that natural images are mostly smooth, in the sense that nearby pixels tend to have similar values \cite{hyvarinen2009natural, zontak2011internal}. This is also known as the notion that for natural images, patches typically exhibit a long-tailed distribution, where only a small portion of sample patches are considered as extremely difficult inputs (Zhang \etal \cite{zhang2021deep} provide a survey regarding handling the long tail problem for various deep learning tasks). In the case of image demosaicing, this can lead the output of a model in regions that do not demonstrate this behavior to be incorrect in the formation of common artifacts, such as zippers and Moiré.} To address this challenge, previous research has proposed various solutions, such as mining examples of structures that are prone to artifacts \cite{gharbi2016deep}, using guided custom losses   \cite{liu2020joint}, or employing custom architectures tailored specifically for the task \cite{liu2020joint, ma2022searching}. \red{lets talk about the SOUT in   this section, it might be too confusing and misleading as that we are trying to solve just smooth patches... }\\
In this paper we propose \textbf{SDAT}, a novel training approach designed to guide convergence towards less biased solutions. Our method extracts sub-datasets from the entire training data, each inducing a unique bias over the trained model. Our training utilizes these sub-datasets to steer the learning process in a non-traditional manner, and prevents converging towards highly likely biased solutions. This approach involves alternation between training on the entire dataset and training on the extracted sub-datasets.



%In this paper, we tackle this issue by the following: First, our method detects sub-datasets categories within the initial training data that exhibit a high likelihood of causing the model to converge towards a biased solution. Subsequently, we employ a novel training approach, which leverages these sub-datasets to steer the learning process, in a non-traditional manner, and prevent converging towards high likely biased solutions. This approach involves alternation between training on the entire dataset and training on the generated sub-datasets.  

%The proposed method is different from previous methods in that it addresses the challenge of inductive bias by explicitly taking into account the difficult samples in the training dataset.

We demonstrate how SDAT is able to improve the performance of different architectures across all evaluated benchmarks in comparison to the originally implemented training, as can be seen in \Cref{fig:intro}. One type of evaluation is following a recent trend of low-capacity models (less than 200k parameters) for edge devices performing image demosaicing \cite{ma2022searching, ramakrishnan2019deep, wang2021compact}. We show that our method is able to effectively utilize the model's capacity and surpass recent relevant works across all benchmarks using fewer number of parameters, showcasing a more efficient solution for low-capacity devices. Another is demonstrating improved performance over high capacity models (in an order of 1M parameters and higher). We evaluate our method both on CNN based architectures and transformers. Thus we exemplify the effectiveness of the propsed method regradless of the architecture. Using SDAT we achieved state-of-the-art on three popular benchmarks by training a variant of the GRL architecture \cite{li2023efficient}.
 \\
% To further highlight our training method we show that we are able to achieve our SOTA results while training on a significant lower amount of data compared to other methods. Thus we demonstrate the effectiveness of our proposal in cases where data is scarce.  
We summarize our main contributions as follows: 
\begin{enumerate} 
    \item We introduce a novel training method for image demosaicing that is able to explore the parameter space more effectively than standard training methods, thus decreasing bias induced by the data. 
    \item We evaluate our training scheme over different model sizes and different types of architectures, showing great improvement all around and achieving SOTA results over three highly popular benchmarks for image demosacing. 
\end{enumerate}

A comparison of visual results can be found in \Cref{fig:qual_res}. More qualitative results can be found in the supplementary material.  

 