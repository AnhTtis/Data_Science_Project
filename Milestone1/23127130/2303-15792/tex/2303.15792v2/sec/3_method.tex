\section{Method} 
\label{sec:method} 

% \red{Here also it makes the reader think we are only focus on smooth patches bias... Need to make it less the subject but more as an example... 
% We can start from " After performining a training over the ... (That way we dont give the focus to the smooth patches, Moreover, I feel uncomfortable to give a reasoning using statistics as "tail of patch distribution" because we somewhat imply that the bias is from imbalanced distribution, and we dont really support this, as then we will need to face more theoretically why balanced sampling is not usefull ... I mean we dont want to clain we define a connection between biases and statistics ( even though its probably true)  ) }
% As natural images consist mainly of smooth patches \cite{hyvarinen2009natural, levin2012patch, zontak2011internal}, after performing a training over the entire dataset, most of the models are able to predict good results on most areas, however struggle with more complex ones, \eg edges, repetitive patterns, etc. These challenging patches are part of the tail of the patch-distribution in natural images. This difficulty arises from the data-induced bias, which directs the trained model towards a biased local minimum solution. \\
In the following we present the required stages to perform SDAT.  
% To address this bias issue while still using the entire dataset for training, we propose an innovative training approach.  
We will show that by applying our training regime we are able to better utilize the given dataset, \ie achieve better results for a given network architecture without modification to it or to the loss function. 
% The method involves cycling through various sub-datasets extracted from the entire dataset, each of which induces a distinct bias over the trained model.
The method involves training alternatively between the entire dataset and the various sub-datasets in a cyclic fashion. Each of the sub-datasets induces a distinct bias over the trained model. SDAT consists of several traversals, where each traversal means, a complete training cycle over all sub-datasets and the entire dataset.\\  
We first outline the details of our suggested training method and explain how it aids in steering the convergence towards a less biased solution by exploiting the specific characteristics of the collected sub-datasets. Then we provide a detailed explanation of our process for gathering these sub-datasets and how we generated them to possess their distinct characteristics (\ie distinct bias) as mentioned above. 

%comprised of two main steps: (1) constructing and identifying sub-categories from the dataset each induces a different bias over the trained model (2) Applying a cyclic training approach that incorporates these sub-categories along with the entire dataset. This method helps guide the training process towards achieving solutions with reduced bias.        

 

% -------------------------------------------------------------------

\subsection{Training method}
\label{sub:cyclic_train}
%Once the sub-categories identification process is finished we can proceed to the step that we call cyclic training.
%In the following we will describe our training method, 
Our training method as depicted in \Cref{fig:optimization} incorporates two primary mechanisms: 
\begin{enumerate}
    \item {\bf Dataset alterantion:} The training process consists of alternating between different training phases. We alternate between a collected sub-dataset and the entire dataset. This alternation takes place every set number of iterations. At each cycle a different sub-dataset is used.
    \item {\bf Solution selection process:} During each training phase we monitor every iteration the average of all sub-datasets' validation loss along with the corresponding model's weights:
                   \begin{equation}
        \label{eq:Vt_calc} 
    \ \Bar{V}_t = \frac{1}{N}\sum_{i=1}^{N} \text{$V_{w_t,c_i}$} 
    \end{equation} 
            where $t$ is the iteration number (training step), $V_{w_t,c_i}$ is the average validation value of sub-dataset $c_i$, given model's weights $w_t$, and $N$ is the number of sub-datasets.
\begin{figure*}[ht!]
%		\captionsetup{justification=centering}
		\centering
        \begin{tabular}{ ccccc }
		%\subfloat[$u$ - output eigenfunction]{
        %\subfloat{
			\includegraphics[width=0.14\textwidth]{sec/sub_categories_patches/grids_gt.png} &
            \includegraphics[width=0.14\textwidth]{sec/sub_categories_patches/gt_zippers.png} &
            \includegraphics[width=0.152\textwidth]{l1_gt.png} &
            \includegraphics[width=0.14\textwidth]{sec/sub_categories_patches/edge_gt.png} & 
			\includegraphics[width=0.141\textwidth]{sec/sub_categories_patches/precept_gt.png} \\
            \includegraphics[width=0.14\textwidth]{sec/sub_categories_patches/grids_pred.png}&
            \includegraphics[width=0.14\textwidth]{sec/sub_categories_patches/pred_zippers.png}&
            \includegraphics[width=0.152\textwidth]{l1_pred.png} &
            \includegraphics[width=0.14\textwidth]{sec/sub_categories_patches/edge_pred.png}&
            \includegraphics[width=0.141\textwidth]{sec/sub_categories_patches/precept_pred.png} \\
            (a) Grid &
            (b) Zipper  &
            (c) $L_1$ &
            (d) Edge &  
            (e) Perceptual \\
		%\captionsetup{justification=justified}
\end{tabular}
		\caption{Examples of sub-datasets patches that are found via the different metrics. Each metric focuses on different aspects in image demosaicing. The top row depicts the ground truth patches, and the bottom the prediction of the network after vanilla training. (a) A patch with grid artifacts, (b) a patch with zipper artifact, (c) a patch with high $L1$ distance, (d)  a patch with high edge distance, and (e) a patch with high perceptual distance.}
		\label{fig:mined_metrics}
   \vspace{0.3cm}

\end{figure*}
    The chosen model's weights for the next alteration are the $w_t$ obtained the lowest $\Bar{V}_t$ during the training phase. See \cref{fig:sub_cat_train} for a more detailed example. 
     We observed that a model consistently achieving low validation loss across all sub-datasets is more likely 
    to converge towards a non-biased solution.

\iffalse
               \begin{equation}
        \label{eq:HZ} 
    \ \Bar{V}_t = \frac{1}{N}\sum_{i=1}^{N} \text{$V_{w_t,c_i}$} 
    \end{equation} 
        where $\Bar{V}_t$ is the average of the validation values at step $t$.  $V_{w_t,c_i}$ is the validation value of sub-dataset $c_i$, given the model's weights $w_t$ at step $t$, and $N$ is the number of categories.
    \begin{equation}
    \label{eq:Argmin}
    \begin{split}
      & \hat{t}_{ci} = \argmin_t \Bar{V}_t  \\ & w_{next} = w_{\hat{t}_{ci}}
    \end{split}
    \end{equation}
        Where $\hat{t}_{ci}$ denotes the time index of the model that achieved the lowest average validation loss during the training session over category $c_i$, and $w_{next}$ denotes the model's weights we use for the next alternation. We observed that a model that consistently performs well in validation across all these sub-datasets is more likely to converge towards a generalized solution.
         \fi 
\end{enumerate}
\noindent These two key factors combined help in guiding the convergence towards a less biased solution. Training over the entire dataset limits the exploration of the solution space due to the inherent bias of the dataset. By training over the sub-datasets we are able to strongly steer the convergence towards a wider range of solutions, as each sub-dataset induces a different bias on the convergence.\\
Moreover, we maintain the usage of the entire dataset, as it contains the overall statistics of the task and every second alternation we train over the entire dataset. We are able to achieve a solution that does not converge to a biased local minimum due to the selection process, as it only propagates to the following training phase the least biased solution achieved during the current training phase.
 This way we are able to enjoy both worlds - we can still use the entire dataset for training, without being limited  only to the solutions governed by the inherent dataset bias.\\ 
{\bf Sub-datasets convergence rate:} 
When training on the entire dataset, simple functions are converged first and complex functions are converged later as demonstrated in prior research \cite{rahaman2019spectral,basri2020frequency}. Our models also exhibited a similar trend. The initial convergence was governed by the entire dataset bias (in our case due to smooth patches in natural images), hindering further convergence toward more complex elements (high frequency, edges and more). Every training phase over a specific sub-dataset, the accumulation  of gradients is till the last non-biased update as in the following:  
    \begin{equation}
    \begin{aligned}
        \Gamma_{c_i} &= \sum_{t = t_{ci}}^{\hat{t}_{ci}} \frac{d}{dw_t} \mathcal{L}_{w_t, c_i} \\
        \hat{t}_{ci} &= \argmin_t \Bar{V}_t
    \end{aligned}
    \label{eq:GradsCalc}
\end{equation}
        Where $\Gamma_{c_i}$ denotes the accumulated gradients obtained during a training session of a specific sub-dataset, $t_{ci}$ and $\hat{t}_{ci}$ denotes the training starting and ending time index respectively, using sub-dataset $c_i$ and  $\mathcal{L}_{w_t, c_i}$ denotes the training loss function given the model's weights $w_t$ and training over sub-dataset $c_i$.\\  After each traversal over all sub-datasets, the model's weights are composed based on the accumulated custom gradients from each sub-dataset:
        \begin{equation} 
        \label{eq:sub_cat}  
    w_{traversal} = w_{0} +\sum_{i=1}^{i=N} \Gamma_{c_i} +  \Gamma_{c_0}
    \end{equation}
        where $w_{0}$ is the weights of the model at the beginning of the traversal, $\Gamma_{c_0}$ is the accumulated gradients obtained along the entire dataset training phases. 

\noindent As demonstrated in \cite{rahaman2019spectral,basri2020frequency} and observed empirically in our own experiments, controlling the convergence rate over different elements in the dataset leads to different solutions. Our training mechanism adapts the convergence rate of each sub-dataset as in \cref{eq:sub_cat}, in order to obtain the least biased solution.
% Do we want to add the term that the sub-categories validation metrics regularize the solution to a less inductive bias solutions ? I've seen when papers use the term regularize the reviewers arguing less ... 
% -----------------------------------------------------------------
      
  
{\bf Learning rate scheduler:}
To ensure stability during training, every dataset alternation we use a learning rate (LR) scheduler that starts with a low value and gradually increases to a higher value of LR.
Shifting between datasets significantly impacts the gradients at the current weight space position, altering the structure of the loss weight landscape and requiring readjustment of the LR. Only the weights that achieved the lowest validation loss across all sub-datasets in each training session are carried over to the next alternation as in \cref{eq:Vt_calc,eq:GradsCalc}, filtering out unstable solutions that might arise from unsuitable LR.



\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=8.2cm, width=0.92\linewidth]{correlation_training_2.png}
    \end{center}         
        \caption{ Our two step elimination process for selecting the sub-datasets. (a) shows the first step, where each graph represents a training session over a sub-dataset (the horizontal axis shows the number of epochs, the vertical axis shows the validation error). The green line represents the validation error of the entire dataset and the red line represents the validation error of the trained sub-dataset. Validation errors of sub-datasets with strong negative correlation convergence compared to the entire dataset validation error, are selected and highlighted in light blue. (b) depicts an example of the second step. Each sub-dataset's (from those chosen in (a)) validation error is compared with the validation error of the other sub-datasets. The objective is to merge the sub-datasets that demonstrate positive correlation convergence, resulting in a similar bias being induced by the trained model. In the example we demonstrate the comparison of sub-dataset 1's validation (red line) with that of other chosen sub-datasets (green line). The sub-datasets chosen to be merged with other sub-datasets are highlighted in light blue.}
	\label{fig:UnCorr}   

\end{figure*}






\subsection{Sub-datasets identification and creation}  
%In the first step, we search for challenging sub-categories from  he overall dataset, \ie patches that are poorly reconstructed by the model that was trained over the entire dataset. 
We now describe the collection of sub-datasets from the entire dataset, each inducing a different bias on the trained model. This phase consists of the following:    
\begin{enumerate}
\label{sub_sec:idn_sub_cat2}
    \item {\bf Generation of sub-datasets}: To establish a starting point for data categorization, we performed a training using the entire dataset to obtain a base model. We used a combination of existing metrics, such as $L_1$ and perceptual \cite{ledig2017photo}, and custom metrics to detect a wide variety of artifact types created by the base model's outputs. For brevity we do not state them here and more details can be found in the supplementary. 
    % These metrics detect specific patch artifacts characteristics, such as color, structure, frequency and more (please refer to the supplementary material for detailed metric information). 
    This approach allows us to create a diverse pool of sub-datasets within the data. Examples of some of the identified patch categories are shown in \Cref{fig:mined_metrics}.
      
    \item {\bf Selection and elimination of sub-datasets}: From the collected pool of sub-datasets we keep the sub-datasets, that training on them induces a bias that hinders the generalization of the model on the entire dataset. These sub-datasets induce a different bias than the entire dataset and eventually help converging to a better solution. This procedure is illustrated in \Cref{fig:UnCorr} and is composed from the following two steps:
\begin{enumerate} 
\label{sub_sec:idn_sub_cat}
    \item {\bf Searching for inverse-correlation:} We conduct brief training sessions over all sub-datasets generated in the previous stage. Each session trains over a single sub-dataset and starts from the model that was trained over the entire dataset, as described in the previous stage. Throughout each session, we evaluate the validation loss over each of the different sub-datasets, including the entire dataset. We search for a negative correlation, in the sense of Spearman's rank correlation coefficient, between the validation loss on the trained sub-dataset and the validation loss on the entire dataset. If indeed there is a negative correlation, we identify a sub-dataset that the model struggles to generalize using the bias induced by the entire dataset. This means the model must discard its data-induced bias to achieve convergence for that specific sub-dataset.
    
    \item {\bf Merging sub-datasets}: When we detect sub-datasets with a positive correlation response between their validation losses (in the same sense as in step (a)), it means that training
    over one of the sub-datasets does not harm the modelâ€™s generalization over the other sub-dataset. Therefore, these sub-datasets are merged into a single sub-dataset.
          
\end{enumerate}        
\end{enumerate}      

In summary, we identify certain sub-datasets that are susceptible to bias, where each of the final generated sub-datasets is likely to induce a unique bias over the trained model. Since a model that has converged toward a biased solution is likely to produce a high validation loss over these sub-datasets, it is used as a metric to estimate a biased solution, shown in \cref{eq:Vt_calc}.
Pseudo-code of the process can be found in the supplementary material. \\
  
% \begin{algorithm}
% \small 
% \caption{Identifying Desired Data Categories}
% \label{alg:IdentifyingDesiredDataCategories}
% \SetAlgoLined
% \DontPrintSemicolon 


% \KwIn{ \textit{generalModel} as pre-trained model trained on general dataset \\ \textit{allSubCategories} as the mined sub-categories}
% \KwOut{Non correlated sub-categories}

% \medskip
 
% \textbf{Step 1. Inverse-correlation:} \\ 
% ValidationCategories = $\{ \}$  \\
% BankCategories = $\{ \}$ \\

% \For{subCategory \in allSubCategories}
%         {        
%         \begin{enumerate}
%             % \item Load baseline Pre-trained model weights into  model.
%             \item \textrm{Train model} on subCategory.
%             \item Calculate the validation loss curve for all mined categories, including general dataset validation curve.
%             \item \If{  \textbf{corr}(general validation, sub-category validation) $< Th $   }
%             {  
%                 \tcp*{Negative correlation response}
%                 \hspace{3mm} BankCategories $\gets$ subCategory \\
%                 \hspace{3mm} ValidationCategories[subCategory] $\gets$ all sub-categories validation errors
%             }
%         \end{enumerate} 
%     } 
%     \\
    
%    ValidationCategories output:
%     \begin{tabular}{l l}
%     Sub-category1: & all validation curves \footnotesize (trained on sub-category1)\\ 
%         ... & ... \\
%     Sub-categoryN: & all validation curves \footnotesize (trained on sub-categoryN) \\ 
%     \end{tabular} \\ ~ \\ 

% \textbf{Step2. Inter-correlation between sub-categories:}

% \For{\textbf{SubCategory} in  \textbf{BankCategories}}{
%    \begin{enumerate}
%         \item  ValiCurvSubCategory $\longleftarrow$ ValidationCategories[SubCategory][SubCategory]  
%         % \item  ValSubCategory - validation curve for sub-category trained over sub-category.  
%         % \item Get all validation curves over all categories during\\the training over sub-category. 'ValidationCategories[\textbf{SubCategory}]'
        
%        \item \For{\textbf{ValiOtherCategory} in ValidationCategories[\textbf{SubCategory}]}{
%           \hspace{3mm } \If{  \textbf{corr}(ValiCurvOtherCategory, \\ ValiCurvSubCategory ) 
%            $> Th $  }{\# Positive correlation response \;  
%           SubCategory \textbf{$U$} OtherCategory }
%           }
%         \end{enumerate} 
%      } 
   
     

%   \textbf{Return}  data sub-categories.
% \end{algorithm}
 
        % \begin{algorithm}
        % \small 
        % \caption{Identifying Desired Data Categories}
        % \label{alg:IdentifyingDesiredDataCategories}
        % \SetAlgoLined
        % \DontPrintSemicolon 
        
        % % \textbf{Input}: Pre-trained model trained on all dataset, mined data categories.
        
        % % \textbf{Output}: Non correlate data categories.
        
        % \KwIn{ \textit{generalModel} as pre-trained model trained on general dataset \\ \textit{allSubCategories} as the mined sub-categories}
        % \KwOut{Non correlated sub-categories}
        
        % \medskip
         
        % \textbf{Step 1. Inverse-correlation:} \\ 
        %      allValiCurves = $\{ \}$ \\ 
        %      bankCategories = $\{ \}$ \\
        
        %     \For{ subCategory  {\normalfont in} allSubCategories}
        %         {   
        %         \begin{enumerate}
        %             \item Load \textit{generalModel} into  \textit{model}
        %             \item Train \textit{model} on \textit{subCategory}
        %             \item Calculate the validation loss curve for all mined categories, including general dataset validation curve.
        %             \item \textit{valiCorr} $\gets$ \textbf{corr}(\textit{genlVali}, \textit{SubCatVali})
        %             \item     \If{  valiCorr $< Th $   }
        %             { 
        %                 \tcp*{Negative correlation response}
        %                 \hspace{3mm} bankCategories $\gets$ sub category; \\
        %                 \hspace{3mm} allValiCurves[subCategory]  $ \gets $ all categories validation errors
        %             }
        %         \end{enumerate} }
                
        %    allValiCurves output:
        %     \begin{tabular}{l l}
        %     Sub-category1: & all validation curves \footnotesize (trained on sub-category1)\\ 
        %         ... & ... \\
        %     Sub-categoryN: & all validation curves \footnotesize (trained on sub-categoryN) \\ 
        %     \end{tabular} \\ ~ \\ 
        
        % \textbf{Step2. Inter-correlation between sub-categories:}
        
        % \For{subCategory  {\normalfont in}  bankCategories}
        %     {
        %     \begin{enumerate}
        %         \item  \textit{valiCurvesSubCategory} $\gets$  \textit{allValiCurves}[\textit{subCategory}] 
        %         % \item  ValSubCategory - validation curve for sub-category trained over sub-category.  
        %         % \item Get all validation curves over all categories during\\the training over sub-category. 'ValidationCategories[\textbf{SubCategory}]'
        
        %         \item  \textit{selfValiCurv} $\gets$  \textit{ValiCurvesSubCategory}[\textit{subCatagorey}]
                
        %        \item \For{otherValiCurv {\normalfont in} valiCurvesSubCategory}
        %         {
        %             \vspace{1mm}  \textit{valiCorr} $\gets$ \textbf{corr}( \textit{selfValiCurv},  \textit{otherValiCurv})
                    
        %             \If{  valiCorr $> Th_2 $ \\ }
        %             {
        %                \hspace{-5mm} \tcp*{Positive correlation response}
                        
        %                 \textbf{merge}  \textit{otherCategory} with  \textit{subCategory} in \textit{bankCategories}
        %             }
        %         }
        %     \end{enumerate} 
        %     } 
           
             
        
        %   \textbf{Return}  bankCategories.
        % \end{algorithm}
        

% --------------------------------------------------------------------
% The sub-categories identification process is finished 
% best sub-categories are identified

% ----------------------------------------------------------------

