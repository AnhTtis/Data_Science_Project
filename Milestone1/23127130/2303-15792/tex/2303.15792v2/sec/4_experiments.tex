
\section{Experiments}
\label{sec:exp}
In order to evaluate our suggested training method we divided our experiments into two architecture groups that might be affected differently: (1) Low-capacity models and (2) High-capacity models.
Over each architecture we conducted two training types: the first is a training over the entire dataset, which we refer to as EDT (Entire Dataset Training) and the second is our proposed SDAT method.\\
For both types of training settings we used crops of size 64 $\times$ 64 as the input and a batch size of 16. We also used data augmentations, such as a random flip and transpose over both image axes during training. All models were trained using the $L_{1}$ norm between the ground-truth and estimated RGB images as the loss function. \\
\textbf{EDT method settings:} we used the DIV2K \cite{agustsson2017ntire} dataset for training. 
We used the Adam optimizer \cite{kingma2014adam} with learning rate $5 \times 10^{-4}$,  $\beta_{1} = 0.9 $, $ \beta_{2} = 0.999 $, and $ \epsilon = 1 \times 10^{-8}$. The models were trained for a total of 2M iterations. \\
\textbf{SDAT method settings:} In addition to using the DIV2K dataset as a whole, we employed the process described in \Cref{sub_sec:idn_sub_cat2}, and chose 5 sub-datasets obtained from the DIV2K dataset. These sub-datasets demonstrated the strongest negative correlation with the entire dataset. We used the same optimizer parameters as in the standard training, except for the suggested modification to the LR scheduler, as explained in \Cref{sub:cyclic_train}. We trained for 20 traversals (each traversal means, a complete training cycle over all sub-datasets and the entire dataset). Each traversal consists of 100K iterations. For more detailed information please refer to the supplementary material.

\subsection{Low-capacity-models}
\label{sub:low_capacity}
We trained three CNN architectures with 9.5K, 16K and 84K parameters. Our CNN has a similar structure to the overall structure of DemosaicNet suggested by Gharbi \etal in \cite{gharbi2016deep}, with slight differences. We replaced each of the two convolution layers with 3 Inverted Linear bottlenecks (ILB) \cite{sandler2018mobilenetv2}. We obtain different architecture sizes by adjusting the expansion size parameters of the ILB. In addition to comparing the results between EDT and our suggested method, we also compared our results (in terms of PSNR) with recent studies that focus on achieving high performance for image demosaicing using low capacity models \cite{ramakrishnan2019deep,wang2021compact}. As indicated in \Cref{tb:low_capacity}, our training method presents superior results across all benchmarks compared to EDT method. Additionally, while our trained CNNs have the lowest number of parameters in each sub-range of parameters, the networks outperform all other methods across all benchmarks.

\begin{table}[!t]\footnotesize
  \hspace*{-0.5cm}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        Parameter & \multirow{2}{*}{Method} & \multirow{2}{*}{Params.} & \multirow{2}{*}{Kodak } & \multirow{2}{*}{MCM} & \multirow{2}{*}{Urban100} \\ 
        range & ~ & ~ &  &  &  \\ \hline \hline
        \multirow{3}{*}{9K-12K} & Ark Lab \cite{ramakrishnan2019deep} & 10K & 40.4 & 36.9 & - \\   
        ~ & Wang \cite{wang2021compact} & 11.7K & 40.8 & 36.75 & 36.4 \\ 
        ~ & CNN EDT & 9.5K & 38.3 &35.4 & 35.44 \\ 
        ~ & \bf{CNN SDAT} & 9.5K & \textbf{41.39} & \textbf{37.02} & \textbf{36.79} \\
        \hline 
        \multirow{2}{*}{16K-20K} & Ark Lab & 20K & 40.9 & 37.6 & - \\    
         ~ & CNN EDT & 16K & 39.31 & 35.81 & 36.2 \\
        ~ & \bf{CNN SDAT} & 16K & \textbf{42.05} & \textbf{37.80} & \textbf{37.52} \\ \hline
        \multirow{3}{*}{80K-200K} & Ark Lab & 200K & 41.2 & 38.2 & - \\ 
        ~ & Wang & 183K & 41.72 & 37.91 & 37.7 \\  
         ~ & CNN EDT & 84K & 40.25 & 37.44 & 37.04 \\ 
        ~ & \bf{CNN SDAT} & 84K & \textbf{42.38} & \textbf{38.52} & \textbf{38.01} \\ \hline
    \end{tabular}
    \caption{PSNR results for image demosaicing for compact networks. Best results are highlighted in bold.}
\label{tb:low_capacity}
\end{table}



\subsection{ High-capacity-models}
\label{sub:transformers_arc}
We evaluated our SDAT method in comparison to the EDT method over three leading transformers architectures. 
First, we implemented RSTCANet \cite{xing2022residual}, which is based on the SwinIR \cite{liang2021swinir} architecture. We applied both methods over two RSTCANet variants, RSTCANet-B and RSTCANet-S with 0.9M and 3.1M parameters respectively. Additionally we trained a variant of the GRL-S \cite{li2023efficient} architecture with 3.1M parameters. More information on the different architectures can be found in the supplementary material. 

As can be seen in \Cref{table:our_vs_standard}, we performed the evaluation over four popular benchmarks that represent natural images, with additional two benchmarks HDR-VDP and Moir\'e initially presented in \cite{gharbi2016deep}, and are known as a collection of hard patches. Our training method over all architectures produced superior results across all benchmarks in comparison to the EDT method described above. 
It is worth mentioning, that while the EDT method yielded comparable or even superior results to the published results across all RSTCANet variants, it could not reconstruct the reported results for the GRL. However, as can be seen in \Cref{tb:SOTA} by using the SDAT method to train the GRL architecture, we achieved SOTA over three popular benchmarks and second best in the fourth.



\begin{table*}[!ht]\normalsize  
\centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
        Method &MCM &  Kodak &CBSD68  &  Urban100& HDR-VDP& Moir\'e\\  \hline
        RSTCANet-B-EDT & 38.83 & 42.22 &  41.83 & 38.45 & 33.6& 36 \\ \hline
        RSTCANet-B-SDAT &  \textbf{39.72} &\textbf{43.22} & \textbf{42.58} & \textbf{39.88} & \textbf{34.52}& \textbf{37.81} \\\hline
        \hline
        RSTCANet-S-EDT & 39.52 & 42.71 & 42.44 &39.72  & 34.34& 37.53 \\ \hline
        RSTCANet-S-SDAT & \textbf{39.75} & \textbf{43.31} & \textbf{42.82} & \textbf{40.14} & \textbf{34.72}&  \textbf{37.92}\\\hline
        \hline  
        GRL-EDT & 39.7 & 43.14& 42.95 &40.05 & 34.7 & 38.1\\ \hline
       GRL-SDAT &  \textbf{40.11} &\textbf{43.65} & \textbf{43.31} & \textbf{40.67}& \textbf{34.95} & \textbf{38.3} \\\hline
    \end{tabular} 
    % \vspace{-3pt} % Adjust the value as needed
    \caption{PSNR results for image demosaicing for leading transformer architectures. We compare standard training over the entire dataset (EDT) with our suggested training approach (SDAT). Best results for each architecture are in bold.}  
         % \vspace{-10pt} % Adjust the value as needed
\vspace{0.35cm}

\label{table:our_vs_standard}
\end{table*}
 
 \begin{table*}[!h]    
\center    
\begin{center}
%\vspace{-3mm}
%\begin{tabular*}{75.8mm}{@{\extracolsep{-0.99mm}}cccccccccc|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method} &  \multirow{2}{*}{Params.} & \multicolumn{2}{c|}{MCM} &  \multicolumn{2}{c|}{Kodak} &  \multicolumn{2}{c|}{CBSD68} &  \multicolumn{2}{c|}{Urban100}   
\\
%\hline
\cline{3-10}
 & & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM 
\\
\hline
\hline

IRCNN \cite{zhang2017learning}
& 1.5M & 37.84 & 0.988 & 40.65 & 0.991 & 40.31 & 0.992 & 37.03 & 0.986
\\

DRUNet\cite{zhang2021plug}
& 11M & 39.40 & \underline{0.991} & 42.30 & 0.994 & 42.33 & 0.995 & 39.22 & 0.990
\\

RNAN \cite{zhang2019residual}
& 9M & 39.71 & 0.972 & 43.09 & 0.990 & 42.50 & 0.992 & 39.75 & 0.984
\\

JDD \cite{xing2021end}
& 3.5M & 38.85 & 0.990 & 42.23 & 0.994 & - & - & 38.34 & 0.989
\\
PANet \cite{mei2023pyramid}
& 6M & 40.00 & 0.973 & 43.29 & 0.990 & 42.86 & 0.989 & 40.50 & 0.985
\\

RSTCANet-B \cite{xing2022residual} 
& 0.9M & 38.89 & 0.990 & 42.11 & 0.994 & 41.74 & 0.9954 & 38.52 & 0.990
\\

RSTCANet-S \cite{xing2022residual} 
& 3.1M & 39.58 & 0.991 & 42.61 & 0.995 & 42.36 & 0.995 & 39.69 & 0.992
\\

RSTCANet-L \cite{xing2022residual} 
& 7.1M & 39.91 & \textbf{0.991} & 42.74 & 0.995 & 42.47 & 0.996 & 40.07 & \underline{0.993}  
\\
GRL \cite{li2023efficient} 
& 3.1M & \underline{40.06} & 0.988 & \underline{43.34} & \underline{0.995} & \textbf{43.89} & \textbf{0.997} & \underline{40.52} & 0.992  
\\
\hline   
RSTCANet-B \bf{SDAT}
& 0.9M & 39.72 & 0.98 & 43.22 & 0.995 & 42.58 & 0.996 & 39.88 & 0.991
\\

RSTCANet-S \bf{SDAT} 
& 3.1M & 39.72 & 0.990 & 43.31 & 0.995 & 42.82 & 0.996 & 40.14 & \textbf{0.993 } 
\\ 
GRL \bf{SDAT}
& 3.1M & \textbf{40.11} & 0.988 & \textbf{43.65} & \textbf{0.996} & \underline{43.31} & \underline{0.996} & \textbf{40.67} & 0.992 
\\ 
\hline  
\end{tabular}
\end{center}
\caption{ Benchmark results (PSNR and SSIM) for image demosaicing. Best results are in bold, and second best are underlined. Except from results obtained by our SDAT  method, the rest of the results in this table were obtained by the official code provided by the authors or reported by them in the original papers.}
\label{tb:SOTA}
\vspace{0.2cm}
\end{table*}




%------------------------------------------------------------------

\begin{table*}[!th]
\center  
\begin{tabular}{|c|cc|cc|cc|cc|cc|}
\hline
\multirow{3}{*}{Params.} & \multicolumn{2}{c|}{\multirow{2}{*}{EDT}} & \multicolumn{2}{c|}{\multirow{2}{*}{Mined training}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Uniform \\ distribution training\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}SDAT\\  without EDT phase\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{SDAT}} \\
 & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \cline{2-11} 
 & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} &MCM \\ \hline
16K & \multicolumn{1}{c|}{39.20} & 35.90 & \multicolumn{1}{c|}{ \underline{40.20} } & 36.40 & \multicolumn{1}{c|}{40.10} & \underline{37.50} & \multicolumn{1}{c|}{39.70} & 37.50 & \multicolumn{1}{c|}{ \textbf{42.05} } & \textbf{37.80} \\ \hline
176K & \multicolumn{1}{c|}{40.55} & 37.20 & \multicolumn{1}{c|}{40.90} & 37.50 & \multicolumn{1}{c|}{ \underline{41.10} } & \underline{38.20} & \multicolumn{1}{c|}{41.00} & 38.00 & \multicolumn{1}{c|}{ \textbf{42.41} } & \textbf{38.67} \\ \hline
\end{tabular}
\caption{ PSNR results of various training methods compared to ours over the Kodak and McMaster datasets. We evaluated each training method using two architectures consisting 16K and 176K parameters. Best results are in bold, and second best are underlined}
\label{tb:mined_data}
\vspace{0.1cm}
\end{table*}

% \begin{table*}[!th]
% \begin{tabular}{|c|cc|cc|cc|cc|cc|}
% \hline
% \multirow{3}{*}{Params.} & \multicolumn{2}{c|}{\multirow{2}{*}{Standard training}} & \multicolumn{2}{c|}{\multirow{2}{*}{Mined training}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Standard training\\ equal training\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{Uniform distribution training}} & \multicolumn{2}{c|}{\multirow{2}{*}{Ours}} \\
%  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \cline{2-11} 
%  & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM \\ \hline
% 16K & \multicolumn{1}{c|}{39.20} & 35.90 & \multicolumn{1}{c|}{40.20} & 36.40 & \multicolumn{1}{c|}{40.10} & 37.50 & \multicolumn{1}{c|}{39.70} & 37.50 & \multicolumn{1}{c|}{42.05} & 37.60 \\ \hline
% 180K & \multicolumn{1}{c|}{40.55} & 37.20 & \multicolumn{1}{c|}{40.90} & 37.50 & \multicolumn{1}{c|}{41.10} & 38.20 & \multicolumn{1}{c|}{41.00} & 38.00 & \multicolumn{1}{c|}{42.50} & 38.60 \\ \hline
% \end{tabular}
% \end{table*}


 
 
\subsection{Ablation Study}
{\bf Comparing different training methods:} 
To verify the contribution of our suggested training method we conducted the following ablation study to compare our method effectiveness versus other training methods: (1) standard training over the entire dataset, (2) standard training over hard mined examples only (similar to \cite{gharbi2016deep}), (3) standard training over a uniform distribution, i.e. we compose a new dataset where the probability to sample a training example from the entire dataset and from the sub-datasets is equal, and (4) performing our method, but discarding the alternation to train on the entire dataset. The training procedure for the standard training methods (1)-(3) is the same as described for EDT settings above. As indicated in \Cref{tb:mined_data}, our method surpasses all other training regimes, by a considerable margin. 
Comparing to standard training, we conducted a wider experiment covering a wide range of model sizes using the same training procedure described above.
The results are shown in \Cref{fig:intro}. It is easy to see how we are able to improve the networks' performance between 1-3.5dB depending on the initial model size (the smaller the network, the higher the performance boost) and the evaluation dataset. \\
{\bf Comparing different numbers of sub-datasets:}
As outlined in \Cref{sub:cyclic_train}, our approach involves switching between a collection of sub-datasets and the entire dataset. In this experiment, we assess the effectiveness of our training method by randomly selecting one and two sub-datasets. The results, as shown in \Cref{tb:num_categories}, demonstrate that the number of sub-datasets included in the alternation process has a significant impact on the resulting performance. This demonstrates that being able to identify additional types of bias in the original dataset can further improve the network's overall generalization.

\begin{table}[!t] \footnotesize
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
    \hline
        \multirow{2}{*}{Params.}   & \multicolumn{2}{c|}{1 sub-dataset} & \multicolumn{2}{c|}{2 sub-datasets} & \multicolumn{2}{c|}{5 sub-datasets} \\ 
        \cline{2-7}
        ~   & Kodak  & MCM  & Kodak  & MCM  & Kodak  & MCM  \\ \hline
        16K & 39.22  & 36.21 & 40.50 & 36.54 & \bf{42.05} & \bf{37.80} \\ \hline
        176K & 40.50  & 37.25 & 41.25 & 37.77 & \bf{42.41} & \bf{38.67} \\ \hline
    \end{tabular}
            \caption{ PSNR results over Kodak and MCM datasets using a different number of sub-datasets for training. Best results are in bold.}
\label{tb:num_categories}      
\end{table}
