\section{Related Work}
\label{sec:RW}

 \begin{figure*}[!ht]
    \begin{center}
        \includegraphics[width=1.0\linewidth]{Cyclic_training.png}
    \end{center}    
        \caption{ An illustration of the SDAT method. Each cycle consists of two training phases. The first consists training over a specific sub-dataset obtained from the pool of collected sub-datasets as explained in \cref{sub_sec:idn_sub_cat2}, while the second, consists of training over the entire dataset.
        Each phase is initialized by the model's weights that achieved the lowest validation loss across all sub-datasets in the previous phase.
        Every cycle a different sub-dataset is selected. The number of cycles depends on the number of the categories and architecture. }
	\label{fig:optimization}   
\end{figure*}

Image demosacing aims to restore missing information from the under-sampled CFA of the camera sensor, as for in most cases each pixel only captures a single color. This is a well studied problem with numerous methods suggested for solving it. The majority of methods focus on the well known Bayer pattern, which captures only one of the following colors: red, green, or blue at each pixel.\\
% \textbf{Classic solutions:} initially most methods were model-based methods, depending on various image priors to help handle challenging areas (\ie areas in the image that are not flat). Such priors could be searching for edges in the image and trying to avoid interpolating across them \cite{li2008image}. Another example could be leveraging the correlations between the different color channels. In some cases methods based on this prior initially interpolate the green channel, as it is sampled twice the frequency than the red and blue channels, and the green interpolated channel is used thereafter as a guidance map for the rest \cite{chang2004effective, zhang2009robust}. An additional common prior is relying on the self similarity properties (\eg \cite{shechtman2007matching}) of natural images, thus completing the lost data from other similar patches in the image \cite{buades2009self, zhang2011color}.\\
 Deep learning methods have become more and more popular, and there has been an active line of study dedicated for performing image demosacing alone, \eg \cite{syu2018learning, tan2017color, xing2022residual, kerepecky2023nerd, lee2023efficient}.     
 In other cases it is joint with other tasks (such as image denoisng), \eg \cite{liu2020joint, qian2022rethinking, xing2021end}. Most of the joint methods still train a network and analyze its performance for the image demosaicing task alone. In \cite{gharbi2016deep} the authors designed a joint demosaicing and denoising network using CNNs and constructed a dataset containing solely challenging patches they mined from online data, based on demosaic artifacts. They then went on to train and test the network on cases without the presence of noise. Most works did not follow this practice, and trained the demosaicing network on a general dataset (\ie without applying any mining).\\
 \textbf{Dedicated task architecture:} A prominent line of work focused on architectural modification according to the prior knowledge of the task.
Tan \etal \cite{tan2017color} proposed a CNN model that first uses bi-linear interpolation for generating the initial image and then throws away the input mosaic image. Given the initial image as the input, the model has two stages for demosaicing. The first stage estimates green and red/blue channels separately while the second stage estimates three channels jointly. The idea to reconstruct the green channel separately or reconstructing it first to use later on as a guidance map is also used by several others, such as  Cui \etal \cite{cui2018color}, and Liu \etal \cite{liu2020joint}. Other works compared the effects of convolution kernels of different sizes on the reconstruction, \eg the work by Syu \etal \cite{syu2018learning}, which concluded the larger the size of the convolution kernel, the higher the reconstruction accuracy. Inspired by \cite{syu2018learning}, Gou \etal \cite{guo2020residual}, suggested to adapt the inception block of \cite{szegedy2017inception} to reduce computation while still having a large receptive field.\\
\textbf{General architectural modification:}
 Another recent popular line of study is utilizing a general neural network architecture and training it separately on various image restoration tasks (\ie each task requires a separate model with different weights), such as \cite{mei2023pyramid,liang2021swinir, mou2021dynamic, zhang2021plug, zhang2017learning, zhang2019residual}. Zhang \etal suggested \cite{zhang2019residual} the RNAN architecture, which is a CNN network. Xing and Egiazarian \cite{xing2022residual} suggested slight modifications to the SwinIR architecture of \cite{liang2021swinir} for the specific task of demosaicing. PANet \cite{mei2023pyramid} performed multi-scale pyramid attention mechanism. 
 Li \etal \cite{li2023efficient} proposed the GRL transformer-based  
 for various image-to-image restoration tasks. Specifically for the demosaicing task, their suggested method achieved SOTA accuracy over KODAK \cite{li2008image}, McMaster \cite{zhang2011color}, Urban \cite{huang2015single} and CBSD68 \cite{937655} datasets. Their solution is focused on artifacts removal, as their network receives an initial demosaic solution.\\
 \begin{figure*}[!ht]
  % \hspace*{-1cm} 
    \begin{center}
        \includegraphics[width=0.92\linewidth]{sec/alternation_Example_ver_3.png}
    \end{center}
    \vspace{-2mm}
        \caption{ Depiction of the weight propagation process of SDAT. An illustration of the alternation process between a sub-dataset and the entire dataset. Each graph is a training phase over a single dataset, where the horizontal axis is the training steps and the vertical axis is the average validation loss across all sub-datasets marked with $\Bar{V}_t$.
        The star on each of the graphs marks the iteration index each training phase stopped accumulate gradients for a specific dataset, as it propagates model weights achieved at that index to the following training phase.
                Furthermore, there can be training phases that do not aggregate any gradients. As can be seen on the rightmost graph the model could not achieve a convergence that lowers $\Bar{V}_t$, therefore, it accumulated zero training steps.}
	\label{fig:sub_cat_train}   
\end{figure*}
 \textbf{Low-capacity demosaic solutions:} 
It is noteworthy to mention another line of work that focus on the implementation of low-capacity networks for image demosaicing. In \cite{ramakrishnan2019deep} Ramakrishnan \etal employed Neural Architecture Search (NAS) and optimization techniques, while in \cite{wang2021compact} Wang \etal proposed a custom UNet architecture. Both approaches evaluate their solutions across different model capacities (\ie number of model parameters).\\
Our method (see \cref{fig:optimization} for an overview) exhibits several significant distinctions from prior research, as most of the works focused on custom losses and 
architectural modifications \cite{zhang2019residual,xing2022residual,lee2023efficient,liang2021swinir}. This work focuses on convergence and training perspectives. Unlike mining-based methods \cite{gharbi2016deep} that rely on error criteria to identify only hard examples, our approach employs different metrics and uses a novel identification mechanism to identify multiple sub-datasets, that exhibit different characteristics, each inducing a different bias into the trained model. Furthermore, our training method utilizes the sub-datasets to steer and guide convergence while training on the entire dataset. 


 
%-------------------------------------------------------------------------

% -------------------------------------------------------------------------

