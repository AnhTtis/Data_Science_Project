\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
%\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage[ruled,vlined]{algorithm2e}

%%% added packages %%%%
\usepackage{color}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{array}
\usepackage[ruled,vlined]{algorithm2e}

\SetKwInOut{KwIn}{Input}
\SetKwInOut{KwOut}{Output}


\newcommand{\TD}[1]{{\color{red} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\pink}[1]{{\color{magenta} #1}}

\setlength{\belowcaptionskip}{-8pt}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{cleveref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi  
\begin{document}

%%%%%%%%% TITLE
\title{Make the Most Out of Your Net: Alternating Between Canonical and Hard Datasets for Improved Image Demosaicing}

\author{Yuval Becker\\
{\tt\small yuval.becker@samsung.com}
\and
Raz Z. Nossek\\
{\tt\small raz.nossek@samsung.com}
\and
Tomer Peleg\\
{\tt\small tomer.peleg@samsung.com}
\and 
Samsung\\
Samsung Israel R\&D Center, Tel Aviv, Israel\\
}


\maketitle
% Remove page # from the first page of camera-ready.


%%%%%%%%% ABSTRACT
\begin{abstract}
Image demosaicing is an important step in the image processing pipeline for digital cameras, and it is one of the many tasks within the field of image restoration. 
A well-known characteristic of natural images is that most patches are smooth, while high-content patches like textures or repetitive patterns are much rarer, which results in a long-tailed distribution. 
This distribution can create an inductive bias when training machine learning algorithms for image restoration tasks and for image demosaicing in particular. There have been many different approaches to address this challenge, such as utilizing specific losses or designing special network architectures. What makes our work is unique in that it tackles the problem from a training protocol perspective.
Our proposed training regime consists of two key steps. The first step is a data-mining stage where sub-categories are created and then refined through an elimination process to only retain the most helpful sub-categories. The second step is a cyclic training process where the neural network is trained on both the mined sub-categories and the original dataset.
We have conducted various experiments to demonstrate the effectiveness of our training method for the image demosaicing task. Our results show that this method outperforms standard training across a range of architecture sizes and types, including CNNs and Transformers. Moreover, we are able to achieve state-of-the-art results with a significantly smaller neural network, compared to previous state-of-the-art methods. 
%This not only demonstrates the efficiency of our training regime but also its potential to be applied to other image restoration tasks.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

\begin{figure}[ht!]
\begin{center}
   \includegraphics[width=1\linewidth]{Figure1_second.png}
\end{center}
   \caption{PSNR results of our approach (\pink{pink} stars) applied on various architectures and compared to leading previous works \cite{gharbi2016deep, ramakrishnan2019deep, xing2022residual,zhang2017learning, zhang2019residual, zhang2021plug} over Kodak \cite{li2008image} dataset. Using the suggested training method we achieve better results than all other networks of the same amount of parameters. In addition we achieve state-of-the-art results while having almost an order of magnitude less parameters than the previous.}
\label{fig:intro}
\end{figure}


\begin{figure*}[!ht]
	\captionsetup{justification=centering}
	\centering
	\subfloat[Image 26 from Urban100]{
			\includegraphics[width=0.22\textwidth,valign=c]{gt_full_image_comp.png}
			%\vphantom{\includegraphics[width=0.2\textwidth,valign=c]{u_point_first}}%		
			}
		\label{fig:urban_gt}
		\begin{minipage}{0.75\textwidth}
			\captionsetup{justification=centering}
			\subfloat[Crop from GT]{
				\includegraphics[width=0.26\textwidth,valign=b]{exact_gt_medium_small_recatangle.png}
				}
				\label{fig:urban_gt_crop}
				\subfloat[Original RSTCANet-B]{
					\includegraphics[width=0.26\textwidth,valign=b]{exact_orig_B_medium_small_2.png}
				}
				\label{fig:urban_B}
				\subfloat[Original RSTCANet-S]{
					\includegraphics[width=0.26\textwidth,valign=b]{exact_orig_S_medium_small_2.png}
					%\vphantom{\includegraphics[width=0.2\textwidth,valign=c]{u_tgv2d_result}}
				}
				\\
				%\captionsetup{justification=justified}
				\subfloat[RNAN]{
					\includegraphics[width=0.26\textwidth,valign=b]{exact_RNAN_medium_small_2.png}
				}
				\label{fig:tgv_ef_ipm_first_inst}
				\subfloat[Original RSTCANet-L]{
					\includegraphics[width=0.26\textwidth,valign=b]{exact_Rcst_L_medium_small_2.png}
				}
				\label{fig:tgv_ef_ipm_final_inst}
				\captionsetup{justification=centering}
				\subfloat[Our RSTCANet-B]{
					\includegraphics[width=0.26\textwidth,valign=b]{exact_OURS_b_medium_small_2.png}
				}
				
			\end{minipage}
			
			\captionsetup{justification=justified}
			\caption{Qualitative comparison of our method compared to other top methods RNAN and RSTCANet. The RNAN model has 9M parameters, while RSTCANet presents 3 different model sizes: B, S, and L, having 0.9M, 3.1M, and 7.1M parameters respectively. We use our suggested training scheme to train the B model. It is easy to see that our model is able to achieve the best restoration result while having the least amount of parameters.}
			\label{fig:qual_res}
\end{figure*}

The task of image demosaicing is a crucial step in digital photography and refers to the process of reconstructing a full-resolution RGB color image from incomplete data obtained from the use of a color filter array (CFA), such as the Bayer pattern of GRBG. In digital cameras, CFA samples only a fraction of the image information, which makes the task of demosaicing complex and challenging. This task is part of a larger group of image restoration problems, such as denoising and deblurring, which all aim to recover a high-quality image from degraded input data.
Image restoration problems are usually considered ill-posed, in the sense that it is usually not possible to determine a unique solution that can accurately reconstruct the original image. This is particularly true for image demosaicing due to the fact that the red, green, and blue color channels are sampled at different locations and rates, which can cause severe aliasing issues.\\
In recent years, the use of convolutional neural networks (CNNs) has shown significant success in addressing image restoration problems \cite{liang2021swinir, zhang2017learning, zhang2019residual} in general, and for image demosaicing in particular \cite{gharbi2016deep, kokkinos2018deep, liu2020joint, niu2018low, qian2022rethinking, tan2018deepdemosaicking}. However, it is well known that inductive bias, which refers to the assumptions and prior knowledge that the model brings to the learning process, is necessary for the model to generalize \cite{mitchell1980need}. But, in some cases, inductive bias can have the opposite effect and negatively impact the model's ability to generalize. This is true for deep neural networks as well and has been an extensive line of study, \eg \cite{geirhos2018imagenet, ritter2017cognitive}, just to mention a few. The bias can arise from various factors such as the network architecture, the training dataset, and others. 
Our focus in this study is on image restoration tasks, where a common inductive bias is the well observed phenomenon that natural images are mostly smooth, in the sense that nearby pixels tend to have similar values \cite{hyvarinen2009natural, zontak2011internal}. This is also known as the notion that for natural images, patches typically exhibit a long-tailed distribution, where only a small portion of sample patches are considered as extremely difficult inputs (Zhang \etal \cite{zhang2021deep} provide a survey regarding handling the long tail problem for various deep learning tasks). In the case of image demosaicing, this can lead the output of a model in regions that do not demonstrate this behavior to be incorrect in the formation of common artifacts, such as zippers and moir√©. To address this challenge, previous research has proposed various solutions, such as mining examples of structures that are prone to artifacts \cite{gharbi2016deep}, using guided custom losses  \cite{liu2020joint}, or employing custom architectures tailored specifically for the task \cite{liu2020joint, ma2022searching}. \\
In this paper, we approach this problem from a different angle and propose a novel training method that aims to overcome the limitations of previous methods. Initially, the method identifies difficult patch samples in the original training dataset and divides them into sub-categories.Then, the model is trained in an approach we call cyclic training, using the sub-categories to guide the learning process, in a non-standard scheme, alternating between the entire dataset and the generated sub-categories. 
%The proposed method is different from previous methods in that it addresses the challenge of inductive bias by explicitly taking into account the difficult samples in the training dataset.
In addition, there is a recent trend of developing low-capacity models (less than 50k parameters) for edge devices to perform image demosaicing \cite{ma2022searching, ramakrishnan2019deep, wang2021compact}.
We show that our method is able to effectively utilize the model's capacity and surpass recent relevant works across all benchmarks using fewer number of parameters, showcasing a more efficient solution for low-capacity devices.
In addition, we do not limit ourselves to low capacity models or to CNN architectures as we apply our method and demonstrate improved performance over standard capacity models (approx. 1M parameters and higher). We test our method on an atchitecture proposed by Xing and Egiazarian \cite{xing2022residual}, which is based on the Swin transformer of Liu \etal \cite{liu2021swin}.
We achieve state-of-the-art (SOTA) results on several benchmarks while using a 10x smaller model from the latest SOTA as can be seen in \Cref{fig:intro}.
To further highlight our training method we show that we are able to achieve our SOTA results while training on a significant lower amount of data compared to other methods. Thus we demonstrate the effectiveness of our proposal in cases where data is scarce. \\
  
We summarize our main contributions as follows: 
\begin{enumerate}
    \item We introduce a novel training method that is able to explore the parameter space more effectively than standard training methods, thus decreasing inductive bias induced by the data.
    \item We evaluate our training scheme over different model sizes and different types of architectures, showing great improvement all around and achieving SOTA results over several benchmarks. 
\end{enumerate}

A comparison of visual results can be found in \Cref{fig:qual_res}. More qualitative results can be found in the supplementary material.  

  \begin{figure*}[ht!]
%		\captionsetup{justification=centering}
		\centering
        \begin{tabular}{ ccccc }
		%\subfloat[$u$ - output eigenfunction]{
        %\subfloat{
			\includegraphics[width=0.17\textwidth]{grids_gt1.png} &
            \includegraphics[width=0.15\textwidth]{zipper_gt.png} &
            \includegraphics[width=0.15\textwidth]{l1_gt.png} &
            \includegraphics[width=0.15\textwidth]{edge_gt.png} &
			\includegraphics[width=0.15\textwidth]{lpips_gt.png} \\
            \includegraphics[width=0.17\textwidth]{grids_pred1.png}&
            \includegraphics[width=0.15\textwidth]{zipper_pred.png}&
            \includegraphics[width=0.15\textwidth]{l1_pred.png} &
            \includegraphics[width=0.15\textwidth]{edge_pred.png}&
            \includegraphics[width=0.15\textwidth]{lpips_pred.png} \\
            (a) Grid &
            (b) Zipper  &
            (c) $L_1$ &
            (d) Edge &
            (e) Perceptual \\
		%\captionsetup{justification=justified}
\end{tabular}
		\caption{Examples of patches that are found via the different metrics. The top row depicts the ground truth patches, and the bottom the prediction of the network after vanilla training. (a) A grid patch, (b) a patch with zipper artifact, (c) a patch with high $L1$ distance, (d)  A patch with high edge distance, and (e) a patch with high perceptual distance.}
		\label{fig:mined_metrics}
\end{figure*}

\section{Related Work}
\label{sec:RW}
Image demosacing aims to restore missing information from the under-sampled CFA of the camera sensor, as for in most cases each pixel only captures a single color. This is a well studied problem with numerous methods suggested for solving it. The majority of methods focus on the well known Bayer pattern, which captures only one of the following colors: red, green, or blue; at each pixel.
Initially most methods were model-based methods, depending on various image priors to help handle challenging areas (\ie areas in the image that are not flat). Such priors could be searching for edges in the image and trying to avoid interpolating across them \cite{li2008image}. Another example could be leveraging the correlations between the different color channels. In some cases methods based on this prior initially interpolate the green channel, as it is sampled twice the frequency than the red and blue channels, and the green interpolated channel is used thereafter as a guidance map for the rest \cite{chang2004effective, zhang2009robust}. An additional common prior is relying on the self similarity properties (\eg \cite{shechtman2007matching}) of natural images, thus completing the lost data from other similar patches in the image \cite{buades2009self, zhang2011color}.\\
Recently as deep learning methods have become more and more popular, there has been an active line of study dedicated for performing image demosacing alone, \eg \cite{syu2018learning, tan2017color, xing2022residual}, while in other cases it is joint with other tasks (such as image denoisng), \eg \cite{liu2020joint, qian2022rethinking, xing2021end}. Most of the joint methods still train a network and analyze its performance for the image demosaicing task alone. In \cite{gharbi2016deep} the authors designed a joint demosaicing and denoising network using CNNs and constructed a dataset containing solely challenging patches they mined from online data, based on demosaic artifacts. They then went on to train and test the network on cases without the presence of noise. Tan \etal \cite{tan2017color} proposed a CNN model that first uses bi-linear interpolation for generating the initial image and then throws away the input mosaic image. Given the initial image as the input, the model has two stages for demosaicing. The first stage estimates green and red/blue channels separately while the second stage estimates three channels jointly. The idea to reconstruct the green channel separately or reconstructing it first to use later on as a guidance map is also used by several others, such as  Cui \etal \cite{cui2018color}, and Liu \etal \cite{liu2020joint}. Other works compared the effects of convolution kernels of different sizes on the reconstruction, \eg the work by Syu \etal \cite{syu2018learning}, which concluded the larger the size of the convolution kernel, the higher the reconstruction accuracy. Inspired by \cite{syu2018learning}, Gou \etal \cite{guo2020residual}, propose to adapt the inception block of \cite{szegedy2017inception} to reduce computation while still having a large receptive field.
Another recent popular line of study is suggesting a general neural network architecture and training it separately on various image restoration tasks (\ie each tasks requires a separate model with different weights), such as \cite{liang2021swinir, zhang2021plug, zhang2017learning, zhang2019residual}. Zhang \etal suggest in \cite{zhang2019residual} the RNAN architecture, which is a CNN network, has achieved SOTA results on the popular Kodak benchmark \cite{li2008image}. Xing and Egiazarian \cite{xing2022residual} suggest slight modifications to the SwinIR architecture of \cite{liang2021swinir} and demonstrate their performance for the demosacing tasks achieving SOTA results on other benchmarks \cite{huang2015single, MartinFTM01, zhang2011color}. 
%-------------------------------------------------------------------------

% -------------------------------------------------------------------------

\section{Method} 
\label{sec:method}

As natural images consist mainly of smooth patches \cite{hyvarinen2009natural, levin2012patch, zontak2011internal}, after performing initial training, the model is able to predict good results on most areas, however it struggles with more complex areas, \eg edges, repetitive patterns, etc. These challenging patches are part of the tail of the patch distribution in natural images. This challenge arises because the model converges to a local minimum with a strong inductive bias induced by the data. In order to alleviate this bias while still training on the entire dataset, we introduce the following method. Our method consists of two main steps: (1) constructing datasets of sub-categories and (2) utilizing a novel training scheme we denote as cyclic training.
In the first step, we search for challenging sub-categories from the overall dataset, \ie patches that are poorly reconstructed by the model that was trained over the entire dataset. 
We then perform an elimination process to keep only the sub-categories that will have a positive impact on the network results after our novel optimization scheme used in the second step. 
In the case of demosaicing we found these are areas that suffer from artifacts such as moir\'e, zipper, etc. 
In the second step, we introduce an optimization scheme that alternates between the selected sub-categories as well as the original dataset to improve generalization and achieve better results.
% --------------------------------------------------------------------
\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=8.3cm, width=0.85\linewidth]{correlation_graph2.png}
    \end{center}  
        \caption{ Our sub categories elimination process. (a) shows the first step, where each graph represents a training session over a mined category (x axis represent the number of epochs, y axis represents the validation error). The green line represents the validation error of the original dataset, and the red line represents the validation error of a sub-category. Categories with strong negative correlation convergence compared to the original dataset are selected and highlighted in light blue. Part (b) depicts an example of the second step. Each sub-category's validation error is compared with the validation error of the other sub-categories. The objective is to merge the sub-categories that demonstrate positive correlation convergence, resulting in a similar inductive bias being induced in the model. These sub-categories are highlighted in light blue.}
	\label{fig:UnCorr}   
\end{figure*}


\subsection{Dataset Creation}  
{\bf Generation of Sub-Categories}
It is difficult to predict a priori, \ie before the model is trained, which areas in the image will be challenging for the model to predict correctly. Therefore, as an initial step we train our model on the entire dataset. We will denote the initial predicted patch $\hat{P}_{init}$ and the patch in the ground-truth RGB image as $P$, both of size $H_p \times W_p \times 3$. We then perform a mining stage. In this stage, we identify different sub-categories where the network struggles and create a pool of sub-categories to train on. The following custom metrics are used for this stage:
\begin{enumerate}
    \item \textit{Zipper metric} - this metric $d_{zip}$ is used to find zipper artifacts near edges in the image. First we calculate a mask $M_{NE}$ to identify non edge areas as follows
    \begin{equation}
        \label{eq:MNE}
        M_{NE} := | C( \hat{P}_{init} ) - C\left( P \right) | < \varepsilon_1,
    \end{equation}
    where $C$ is some kind of edge detector, \eg Canny, and $\varepsilon_1$ is a predefined threshold.
    Then we are able to calculate
    \begin{equation}
        \label{eq:HZ}
        d_{zip} := \| | \, | \nabla \hat{P}_{init} | - \left| \nabla P \right| |  \odot M_{NE} \|_1,
    \end{equation}
    where $ \left| \nabla \right| = \left|\frac{\partial}{\partial x} \right| + \left|\frac{\partial}{\partial y} \right|$, and $\odot$ is the element-wise product.
    
    \item \textit{Grid metric} - this metric $d_{grid}$ is used to find grid-like artifacts in flat areas. 
    \begin{multline}
        \label{eq:HG}
        d_{grid} := \| 2 \cdot | \nabla \hat{P}_{init} | \, \odot \\
        ( \sigma ( \frac{\alpha}{\left| \nabla P \right|}  ) -0.5 ) \odot ( | \nabla \hat{P}_{init} | > \varepsilon_2 ) \|_1,
    \end{multline}
    where $\sigma$ is the Sigmoid function, $\alpha > 0$ is a parameter that scales the values of the Sigmoid, and $\varepsilon_2$ is another predefined threshold.

    \item $L_1$ \textit{distance} - a simple photometric distance.
    \begin{equation}
    \label{eq:L1}
        d_{L_1}: = \| \hat{P}_{init} -  P  \|_1     
    \end{equation}
    

    \item \textit{Perceptual metric} \cite{ledig2017photo} - this metric $ d_{VGG/i.j} $ is used for capturing the perceptual distance between the source and predicted patches.
    \begin{equation}
	\label{eq:vgg}
	    d_{VGG/i.j} := \| \phi_{i,j}(\hat{P}_{init}) - \phi_{i,j}(P) \|^2_2,
	\end{equation}
    where $\phi_{i,j}$ indicates the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the pre-trained VGG19 network \cite{simonyan2014very}.
    
    \item \textit{Edge distance} - This metric $d_{edge}$ is used to find distortions around edges
    \begin{equation}
	\label{eq:edge}
	    d_{edge} := \| C( \hat{P}_{init} ) - C\left( P \right) \|_1.
	\end{equation}
    
\end{enumerate}


Examples of the patches identified by the custom metrics presented above can be seen in \Cref{fig:mined_metrics}. Note, some sub-categories are generated by changing the threshold of the metric such as in \Cref{eq:HG}. \\
{\bf Selecting Sub-Categories} Following the collection of sub-categories, we perform an additional step where we 
select sub-categories, that training on them, hinder the generalization of the model on the entire dataset.
%identify on which of the sub-categories induce strong bias in the model. 
This procedure is illustrated in \Cref{fig:UnCorr} and is composed from the following two steps:
\begin{enumerate}
\label{sub_sec:idn_sub_cat}
    \item \textit{Searching for inverse-correlation}: We conduct a brief training procedure over each sub-category, starting from the pre-trained model that was trained over the entire dataset. During each training session, we evaluate the validation loss for each of the different sub-categories, including the original dataset, throughout the training process. By observing a sub-category validation loss that is negatively correlated with the original dataset validation loss, in the sense of Spearman's rank correlation coefficient, we identify a sub-category that the model struggles to generalize using the bias induced by the original dataset. The model must discard its inductive bias to achieve convergence for that specific sub-category.
    
    \item \textit{Merging sub-catagories}: Sub-categories with a positive correlation response between their validation losses (in the same sense as in step 1) are combined into a single sub-category. That is, training over one of the sub-categories does not harm the model's generalization over the other sub-category.
      
\end{enumerate}
Pseudo code of the process can be found in the supplementary material. 

% \begin{algorithm}
% \small 
% \caption{Identifying Desired Data Categories}
% \label{alg:IdentifyingDesiredDataCategories}
% \SetAlgoLined
% \DontPrintSemicolon 


% \KwIn{ \textit{generalModel} as pre-trained model trained on general dataset \\ \textit{allSubCategories} as the mined sub-categories}
% \KwOut{Non correlated sub-categories}

% \medskip
 
% \textbf{Step 1. Inverse-correlation:} \\ 
% ValidationCategories = $\{ \}$  \\
% BankCategories = $\{ \}$ \\

% \For{subCategory \in allSubCategories}
%         {        
%         \begin{enumerate}
%             % \item Load baseline Pre-trained model weights into  model.
%             \item \textrm{Train model} on subCategory.
%             \item Calculate the validation loss curve for all mined categories, including general dataset validation curve.
%             \item \If{  \textbf{corr}(general validation, sub-category validation) $< Th $   }
%             {  
%                 \tcp*{Negative correlation response}
%                 \hspace{3mm} BankCategories $\gets$ subCategory \\
%                 \hspace{3mm} ValidationCategories[subCategory] $\gets$ all sub-categories validation errors
%             }
%         \end{enumerate} 
%     }
%     \\
    
%    ValidationCategories output:
%     \begin{tabular}{l l}
%     Sub-category1: & all validation curves \footnotesize (trained on sub-category1)\\ 
%         ... & ... \\
%     Sub-categoryN: & all validation curves \footnotesize (trained on sub-categoryN) \\ 
%     \end{tabular} \\ ~ \\ 

% \textbf{Step2. Inter-correlation between sub-categories:}

% \For{\textbf{SubCategory} in  \textbf{BankCategories}}{
%    \begin{enumerate}
%         \item  ValiCurvSubCategory $\longleftarrow$ ValidationCategories[SubCategory][SubCategory]  
%         % \item  ValSubCategory - validation curve for sub-category trained over sub-category.  
%         % \item Get all validation curves over all categories during\\the training over sub-category. 'ValidationCategories[\textbf{SubCategory}]'
        
%        \item \For{\textbf{ValiOtherCategory} in ValidationCategories[\textbf{SubCategory}]}{
%           \hspace{3mm } \If{  \textbf{corr}(ValiCurvOtherCategory, \\ ValiCurvSubCategory ) 
%            $> Th $  }{\# Positive correlation response \;  
%           SubCategory \textbf{$U$} OtherCategory }
%           }
%         \end{enumerate} 
%      } 
   
     

%   \textbf{Return}  data sub-categories.
% \end{algorithm}
 
        % \begin{algorithm}
        % \small 
        % \caption{Identifying Desired Data Categories}
        % \label{alg:IdentifyingDesiredDataCategories}
        % \SetAlgoLined
        % \DontPrintSemicolon 
        
        % % \textbf{Input}: Pre-trained model trained on all dataset, mined data categories.
        
        % % \textbf{Output}: Non correlate data categories.
        
        % \KwIn{ \textit{generalModel} as pre-trained model trained on general dataset \\ \textit{allSubCategories} as the mined sub-categories}
        % \KwOut{Non correlated sub-categories}
        
        % \medskip
         
        % \textbf{Step 1. Inverse-correlation:} \\ 
        %      allValiCurves = $\{ \}$ \\ 
        %      bankCategories = $\{ \}$ \\
        
        %     \For{ subCategory  {\normalfont in} allSubCategories}
        %         {   
        %         \begin{enumerate}
        %             \item Load \textit{generalModel} into  \textit{model}
        %             \item Train \textit{model} on \textit{subCategory}
        %             \item Calculate the validation loss curve for all mined categories, including general dataset validation curve.
        %             \item \textit{valiCorr} $\gets$ \textbf{corr}(\textit{genlVali}, \textit{SubCatVali})
        %             \item     \If{  valiCorr $< Th $   }
        %             { 
        %                 \tcp*{Negative correlation response}
        %                 \hspace{3mm} bankCategories $\gets$ sub category; \\
        %                 \hspace{3mm} allValiCurves[subCategory]  $ \gets $ all categories validation errors
        %             }
        %         \end{enumerate} }
                
        %    allValiCurves output:
        %     \begin{tabular}{l l}
        %     Sub-category1: & all validation curves \footnotesize (trained on sub-category1)\\ 
        %         ... & ... \\
        %     Sub-categoryN: & all validation curves \footnotesize (trained on sub-categoryN) \\ 
        %     \end{tabular} \\ ~ \\ 
        
        % \textbf{Step2. Inter-correlation between sub-categories:}
        
        % \For{subCategory  {\normalfont in}  bankCategories}
        %     {
        %     \begin{enumerate}
        %         \item  \textit{valiCurvesSubCategory} $\gets$  \textit{allValiCurves}[\textit{subCategory}] 
        %         % \item  ValSubCategory - validation curve for sub-category trained over sub-category.  
        %         % \item Get all validation curves over all categories during\\the training over sub-category. 'ValidationCategories[\textbf{SubCategory}]'
        
        %         \item  \textit{selfValiCurv} $\gets$  \textit{ValiCurvesSubCategory}[\textit{subCatagorey}]
                
        %        \item \For{otherValiCurv {\normalfont in} valiCurvesSubCategory}
        %         {
        %             \vspace{1mm}  \textit{valiCorr} $\gets$ \textbf{corr}( \textit{selfValiCurv},  \textit{otherValiCurv})
                    
        %             \If{  valiCorr $> Th_2 $ \\ }
        %             {
        %                \hspace{-5mm} \tcp*{Positive correlation response}
                        
        %                 \textbf{merge}  \textit{otherCategory} with  \textit{subCategory} in \textit{bankCategories}
        %             }
        %         }
        %     \end{enumerate} 
        %     } 
           
               
             
        %   \textbf{Return}  bankCategories.
        % \end{algorithm}
        
\begin{figure*}[!ht]
    \begin{center}
        \includegraphics[width=1\linewidth]{Training_graph_update4.png}
    \end{center}    
        \caption{ An illustration of our cyclic training procedure. Each cycle consists of two training phases. The first  consists  training over a specific sub-category for several epochs, while the second, consists of training over the entire dataset.
        Each phase is initialized by the model's weights that achieved the lowest validation loss across all sub-categories in the previous phase.
        Every cycle a different sub-category is selected. The number of cycles depends on the number of the categories and architecture. }
	\label{fig:optimization}   
\end{figure*}
% --------------------------------------------------------------------
% The sub-categories identification process is finished 
% best sub-categories are identified
\subsection{Cyclic Training}
\label{sub:cyclic_train}
Once the sub-categories identification process is finished we can proceed to the step that we call cyclic training. This training method, as depicted in \Cref{fig:optimization}, involves alternating between a sub-category, that was mined from the previous stage, and the entire dataset during the training process. This alternation takes place every set number of epochs.
To ensure stability during the training, every dataset alternation we use a learning rate (LR) scheduler that starts with a low value and gradually increases to a higher value of LR. % This helps prevent the training from becoming unstable as it alternates between different datasets.
When performing the training with each LR we also look at the validation loss for each of the different sub-categories, and then choose the model that had the lowest average loss across the validations to be the model we use for the next alternation.
%By the way we constructed our data categories, a model that aims to generalize over all the selected sub-categories is strongly punished by following a convergence path that contains an inductive bias.   

{\bf Dataset Alternation} 
The dataset alternation stage in our training process involves training on only one specific sub-category of the dataset per iteration, as the mined categories are typically harder to converge, as supported by \cite{rahaman2019spectral}, and as we also observed empirically. \\
The probability to find a local minimum, given a weights space position for complex functions, in our case mined sub-categories, compared to the original dataset is much lower. Therefore, we first train over a sub-category till convergence, then alternate to train over the original dataset given the previous obtained weight space position of the previous iteration.
Additionally, when training on the original dataset, simple functions are converged first and complex functions are converged later as demonstrated in prior research \cite{rahaman2019spectral,basri2020frequency}. Our models also exhibited a similar trend, initially generalizing over simple elements but having difficulty in converging over more complex ones. The initial convergence resulted in a strong inductive bias (in our case smooth patches), hindering further convergence over complex functions. To address this issue, we alternate between a mined sub-category dataset and the original dataset during training. That way, we encourage the model to converge at the same rate over both datasets, resulting in a solution with reduced inductive bias. Even though every second alternation our model trains over the entire dataset, the mined sub-categories were constructed in a way that their average validation loss penalizes solutions with strong inductive biases, as shown in \Cref{fig:UnCorr}. By doing so, we are able to effectively train the model using the entire dataset.
% Do we want to add the term that the sub-categories validation metrics regularize the solution to a less inductive bias solutions ? I've seen when papers use the term regularize the reviewers arguing less ... 
% -----------------------------------------------------------------
      
  
{\bf Learning Rate Scheduler}
Our method involves a non-standard learning rate protocol. Shifting between datasets significantly impacts the gradients at the current weight space position, altering the structure of the loss weight landscape and requiring readjustment of the learning rate. We begin with a low learning rate and gradually increase it. As mentioned above, and as depict in \Cref{fig:optimization}, only model weights that achieved the lowest validation loss across all categories are carried over to the next alternation, filtering out unstable solutions that might arise from unsuitable lr.
% -----------------------------------------------------------------


\begin{table*}[!h]    
\center    
\begin{center}
\label{tab:results_psnr_demosaic_rgb}
%\vspace{-3mm}
%\begin{tabular*}{75.8mm}{@{\extracolsep{-0.99mm}}cccccccccc|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method} &  \multirow{2}{*}{Params.} & \multicolumn{2}{c|}{McMaster18} &  \multicolumn{2}{c|}{Kodak24} &  \multicolumn{2}{c|}{CBSD68} &  \multicolumn{2}{c|}{Urban100}   
\\
%\hline
\cline{3-10}
 & & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM 
\\
\hline
\hline

IRCNN \cite{zhang2017learning}
& 1.5M & 37.84 & 0.9885 & 40.65 & 0.9915 & 40.31 & 0.9924 & 37.03 & 0.9864
\\

DRUNet\cite{zhang2021plug}
& 11M & 39.40 & \underline{0.9914} & 42.30 & 0.9944 & 42.33 & 0.9955 & 39.22 & 0.9906
\\

RNAN \cite{zhang2019residual}
& 9M & 39.71 & 0.9725 & 43.09 & 0.9900 & 42.50 & 0.9929 & 39.75 & 0.9848
\\

xing \cite{xing2021end}
& 3.5M & 38.85 & 0.9904 & 42.23 & 0.9947 & - & - & 38.34 & 0.9895
\\

RSTCANet-B
& 0.9M & 38.89 & 0.9902 & 42.11 & 0.9948 & 41.74 & 0.9954 & 38.52 & 0.9906
\\

RSTCANet-S
& 3.1M & 39.58 & 0.9910 & 42.61 & 0.9951 & 42.36 & 0.9958 & 39.69 & 0.9924
\\

RSTCANet-L \cite{xing2022residual} 
& 7.1M & \textbf{39.91} & \textbf{0.9916} & 42.74 & 0.9952 & 42.47 & 0.9960 & \underline{40.07} & \underline{0.9931}
\\
\hline 
RSTCANet-B \bf{Ours}
& 0.9M & 39.65 & 0.98 & \underline{43.10} & \underline{0.9957} & \underline{42.54} & \underline{0.9961} & 39.88 & 0.9915
\\

RSTCANet-S \bf{Ours}
& 3.1M & \underline{39.72} & 0.990 & \textbf{43.13} & \textbf{0.9958} & \textbf{42.82} & \textbf{0.9963} & \textbf{40.14} & \textbf{0.993}      
\\ 
\hline  
\end{tabular}
\end{center}
\vspace{2mm}
\caption{Quantitative results for image demosaicing. We compare using  PSNR and SSIM. Best results are in bold, and second best marked with underline.}
\label{tb:SOTA}
\end{table*}

\section{Experiments}
\label{sec:exp}
In order to evaluate the effectiveness of our proposed method, we conducted experiments on the image demosaicing task using a CNN with a varying number of parameters. We compared our method with a standard training approach across different model sizes. Furthermore, we compared our results with existing works that focus on the use of low capacity models for the image demosaicing task. In addition to a standard CNN architecture, we evaluated our method using a transformer-based architecture. 
Results were compared using four well established demosaicing benchmarks: Kodak\cite{li2008image}, McMaster (MCM) \cite{zhang2011color}, Urban100 \cite{huang2015single}, and CBSD68 \cite{MartinFTM01}. 

{\bf Training dataset and settings} For training we used the DIV2K \cite{agustsson2017ntire} dataset. To further show the strength of our proposed training method instead of training on the entire dataset, as others, we randomly chose only 70 (out of the 800 available) images , to train all our models. We used the Adam optimizer \cite{kingma2014adam} with $\beta_{1} = 0.9 $, $ \beta_{2} = 0.999 $, and $ \epsilon = 1 \times 10^{-8}$ combined with our custom scheduler as explained in \Cref{sub:cyclic_train}.  We used crops of 64 $\times$ 64 for the input and a batch size of 32. 
We used a random flip and transpose over both image axes during training. All models were trained using the $L_{1}$ norm as the loss function.

\begin{table}[!t]\footnotesize
        
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        Parameter & \multirow{2}{*}{Method} & \multirow{2}{*}{Params.} & \multirow{2}{*}{KODAK} & \multirow{2}{*}{MCM} & \multirow{2}{*}{URBAN} \\ 
        range & ~ & ~ &  &  &  \\ \hline \hline
        \multirow{3}{*}{9K-12K} & ark lab \cite{ramakrishnan2019deep} & 10K & 40.4 & 36.9 & - \\ 
        ~ & wang \cite{wang2021compact} & 11.7K & 40.8 & 36.75 & 36.4 \\ 
        ~ & \bf{Ours} & 9.5K & \textbf{41.39} & \textbf{37.02} & \textbf{36.79} \\ \hline
        \multirow{2}{*}{16K-20K} & ark lab & 20K & 40.9 & 37.6 & - \\ 
        ~ & \bf{Ours} & 16K & \textbf{42.05} & \textbf{37.80} & \textbf{37.52} \\ \hline
        \multirow{3}{*}{80K-200K} & ark lab & 200K & 41.2 & 38.2 & - \\ 
        ~ & wang & 183K & 41.72 & 37.91 & 37.7 \\
        ~ & \bf{Ours} & 84K & \textbf{42.38} & \textbf{38.52} & \textbf{38.01} \\ \hline
    \end{tabular}
    \caption{PSNR results for image demosaicing for compact networks. Best results are in bold.}
\label{tb:low_capacity}
\end{table}

{\bf Model Architecture} 
 Our experiments were conducted using two different architectures. One based on CNNs and the other transformer-based. 
 The architecture for the CNN models has a similar structure to the overall structure of DemosaicNet suggested by Gharbi \etal in \cite{gharbi2016deep}, with slight differences. We replaced each of the two convolution layers with 3 Inverted Linear bottlenecks (ILB) \cite{sandler2018mobilenetv2}. We obtain different architecture sizes by adjusting the expansion size parameters of the ILB.
 For the transformer architecture we implemented RSTCANet \cite{xing2022residual}, which is based on SwinIR \cite{liang2021swinir} architecture. We applied our method over two variants from \cite{xing2022residual}, RSTCANet-B and RSTCANet-S with 0.9M and   3.1M parameters respectively.
 
%--------------------------------------------------------------------
 \subsection{Comparison of Solutions}
 
{\bf Comparison between leading solutions for demosaicing task}.
As can be seen in \Cref{tb:SOTA}, our suggested method achieves state-of-the-art results on three key benchmarks with roughly two to three times fewer parameters than current state-of-the-art models. Additionally, our 0.9M parameter model outperforms previous state-of-the-art models on two benchmarks, despite being approximately x10 smaller in size. 

{\bf Low capacity models for demosaicing task}.
We compare our method with recent studies that focus on achieving high performance for image demosaicing using low capacity models. In \cite{ramakrishnan2019deep} Ramakrishnan \etal employs Neural Architecture Search (NAS) and optimization techniques, while in \cite{wang2021compact} Wang \etal propose a custom UNet-based architecture. Both approaches evaluate their solutions across different model capacities (\ie number of model parameters) using established benchmarks. As indicated in \Cref{tb:low_capacity}, our training method consistently achieves the best results across all benchmarks while having the lowest number of parameters in each sub-range of parameters.

%------------------------------------------------------------------

\begin{table*}[!th]
\center  
\begin{tabular}{|c|cc|cc|cc|cc|cc|}
\hline
\multirow{3}{*}{Params.} & \multicolumn{2}{c|}{\multirow{2}{*}{Standard training}} & \multicolumn{2}{c|}{\multirow{2}{*}{Mined training}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Uniform \\ distribution training\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Cyclic training\\ over sub-categories\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{Ours}} \\
 & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \cline{2-11} 
 & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM \\ \hline
16K & \multicolumn{1}{c|}{39.20} & 35.90 & \multicolumn{1}{c|}{ \underline{40.20} } & 36.40 & \multicolumn{1}{c|}{40.10} & \underline{37.50} & \multicolumn{1}{c|}{39.70} & 37.50 & \multicolumn{1}{c|}{ \textbf{42.05} } & \textbf{37.80} \\ \hline
176K & \multicolumn{1}{c|}{40.55} & 37.20 & \multicolumn{1}{c|}{40.90} & 37.50 & \multicolumn{1}{c|}{ \underline{41.10} } & \underline{38.20} & \multicolumn{1}{c|}{41.00} & 38.00 & \multicolumn{1}{c|}{ \textbf{42.41} } & \textbf{38.67} \\ \hline
\end{tabular}
\caption{ PSNR results of various training methods compared to ours over the Kodak and MCM datasets. We evaluated each training method using two architectures consisting 16K and 176K parameters.}
\label{tb:mined_data}
\end{table*}

% \begin{table*}[!th]
% \begin{tabular}{|c|cc|cc|cc|cc|cc|}
% \hline
% \multirow{3}{*}{Params.} & \multicolumn{2}{c|}{\multirow{2}{*}{Standard training}} & \multicolumn{2}{c|}{\multirow{2}{*}{Mined training}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Standard training\\ equal training\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{Uniform distribution training}} & \multicolumn{2}{c|}{\multirow{2}{*}{Ours}} \\
%  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \cline{2-11} 
%  & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM & \multicolumn{1}{c|}{Kodak} & MCM \\ \hline
% 16K & \multicolumn{1}{c|}{39.20} & 35.90 & \multicolumn{1}{c|}{40.20} & 36.40 & \multicolumn{1}{c|}{40.10} & 37.50 & \multicolumn{1}{c|}{39.70} & 37.50 & \multicolumn{1}{c|}{42.05} & 37.60 \\ \hline
% 180K & \multicolumn{1}{c|}{40.55} & 37.20 & \multicolumn{1}{c|}{40.90} & 37.50 & \multicolumn{1}{c|}{41.10} & 38.20 & \multicolumn{1}{c|}{41.00} & 38.00 & \multicolumn{1}{c|}{42.50} & 38.60 \\ \hline
% \end{tabular}
% \end{table*}



\subsection{Ablations}
{\bf Comparing different training methods.} 
To verify the contribution of our suggested training method the following ablation study compares our method effectiveness versus other training methods: (1) standard training over the entire dataset, (2) standard training over hard mined examples only (similar to \cite{gharbi2016deep}), (3) standard training over a uniform distribution, i.e. we compose a new dataset where the probability to sample a training example from the general dataset and from the sub-categories is equal, and (4) performing our methods, but discarding the general dataset alternation phase. In all training types the sub-category metrics were utilized to select 5\% of the crops with the highest error, with a crop size of 64 $\times$ 64. The training procedure for the standard training methods (1)-(3) was similar, with an initial learning rate of $5 \times 10^{-4}$, that was reduced by half after every 100K iterations. The models were trained for a total of 2M iterations. As indicated in \Cref{tb:mined_data}, our method surpasses all other training regimes, by a considerable margin. 
For standard training we conducted a wider experiment covering a wide range of model sizes using the same training procedure described above. The results are shown in \Cref{fig:kodak_mcm_ablation}. It is easy to see how we are able to improve the networks' performance between 1-3.5[dB] depending on the initial model size (the smaller the network, the higher the performance boost) and the evaluation dataset.

\begin{figure}[!t] 
    %\captionsetup{justification=centering}
    \centering
    \subfloat[]{\label{fig:kdk_abl}
        \includegraphics[width=0.4\textwidth]{kodak_ablation_standart_training.png}
    } \\
    \subfloat[]{\label{fig:mcm_abl}
        \includegraphics[width=0.4\textwidth]{mcm_ablation_standart_training.png}
     }
    \caption{our training method compared to a standard training over the (a) KODAK and (b) MCM datasets. The results of our training method are in pink stars and the results of regular training are in blue. 
    %As can be seen the performance of our training method surpasses the standard training by a large margin across all ranges of parameters. 
    }
    \label{fig:kodak_mcm_ablation}
\end{figure} 



{\bf Comparison with a different number of categories}
As outlined in \Cref{sub:cyclic_train} , our approach involves switching between sub-categories tailored for each architecture, as detailed in the \Cref{sub_sec:idn_sub_cat}. In this experiment, we assess the effectiveness of our training method by randomly selecting one and two categories. The results, as shown in \Cref{tb:num_categories}, demonstrate that the number of categories included in the alternation process has a significant impact on the performance outcomes.



\begin{table}[!t]\footnotesize  
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
    \hline
        \multirow{2}{*}{Params.}   & \multicolumn{2}{c|}{1 category} & \multicolumn{2}{c|}{2 categories} & \multicolumn{2}{c|}{All categories} \\ 
        \cline{2-7}
        ~   & Kodak  & MCM  & Kodak  & MCM  & Kodak  & MCM  \\ \hline
        16K & 39.22  & 36.21 & 40.50 & 36.54 & 42.05 & 37.80 \\ \hline
        176K & 40.50  & 37.25 & 41.25 & 37.77 & 42.41 & 38.67 \\ \hline
    \end{tabular}
            \caption{ PSNR results over Kodak and MCM datasets using a different number of sub-categories for training. The number of categories chosen by the algorithm for the 16K model and the 176K model are 6 and 5 respectively. }
\label{tb:num_categories}      
\end{table}



 
%----------------------------------------------------------------------------------------------------------------
\section{Conclusion} 
In this paper, we addressed the challenge of inductive bias in image demosaic task and proposed a novel training method to improve the generalization of a model. Our method involves two main steps: defining and identifying sub-categories beneficial to model convergence and an alternating training scheme. 
It demonstrated an improved performance over standard training methods and achieved state-of-the-art results on several benchmarks using smaller models. Our suggestion also effectively utilized the model's capacity, surpassing recent relevant works across all benchmarks using fewer parameters. This result is crucial for edge devices, where the trend is towards low-capacity models. The method's success demonstrates the importance of considering the dataset structure in optimizing a model's training process. Our findings suggest that our proposed method can be applied to other image restoration tasks and can be used as a trigger for further research in this field.
  

%  In this paper, we addressed the challenge of inductive bias in image demosaic task and proposed a novel training method to improve the generalization of a model. Our method involves two main steps: defining and identifying sub-categories beneficial to model convergence and a custom training scheme. 
% Our method demonstrated an improved performance over standard training methods and achieved state-of-the-art results on several benchmarks using smaller models. Our method also effectively utilized the model's capacity, surpassing recent relevant works across all benchmarks using fewer parameters. This result is crucial for edge devices, where the trend is towards low-capacity models.

% \section{Conclusion}  
% In conclusion, our proposed method effectively addresses the challenge of inductive bias in image demosaicing task. Our method's success demonstrates the importance of considering the dataset structure in optimizing a model's training process. Our findings suggest that our proposed method can be applied to other image restoration tasks and can be used as a trigger for further research in this field.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}  
}


  
\end{document} 

 


% --------------------------------------------------------------------
\subsection{Final}

You must include your signed IEEE copyright release form when you submit
your finished paper. We MUST have this form before your paper can be
published in the proceedings.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}

of mined samples from ch