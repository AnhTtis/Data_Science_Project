\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
%\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{float}

%%% added packages %%%%
\usepackage{color}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{array}
\usepackage[ruled,vlined]{algorithm2e}
% \usepackage{subcaption}
\usepackage{setspace}

\SetKwInOut{KwIn}{Input}
\SetKwInOut{KwOut}{Output}

\newcommand{\TD}[1]{{\color{red} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\pink}[1]{{\color{magenta} #1}}

\setlength{\belowcaptionskip}{-8pt}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{cleveref}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi  
\iccvfinalcopy % *** Uncomment this line for the final submission

\begin{document}
%%%%%%%%% TITLE
\title{Make the Most Out of Your Net: Alternating Between Canonical and Hard Datasets for Improved Image Demosaicing - Supplementary material} 



\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
\label{sec:supplementary}

\renewcommand{\thepage}{S\arabic{page}}  
\renewcommand{\thesection}{S\arabic{section}}    
\renewcommand{\thetable}{S\arabic{table}}   
\renewcommand{\thefigure}{S\arabic{figure}}
% \newfloat{algorithm}{htbp}{loa}[figure]
% \floatname{algorithm}{Algorithm}
% \renewcommand{\thealgorithm}{S\arabic{algorithm}}

\section{Method}
\subsection{Selecting sub-categories}
Our method for selecting sub-categories is presented in the following pseudo code \Cref{alg:IdentifyingDesiredDataCategories}. We outline the method for selecting specific sub-categories among the mined sub-categories. The method involves two steps: In step 1 we re-train a model that was previously trained on the entire dataset. The re-training is performed on one of the mined sub-categories we calculate the validation loss curves on all the sub-categories, as well as the validation loss curve of the original dataset. We then check if the validation loss curve on the trained sub-category is negatively correlated with the validation loss curve on the original dataset. If so, the sub-category is added to a dictionary called "bank categories". In step 2, the algorithm checks the correlation between the validation loss curves for each pair of sub-categories in the bank. If the correlation is positive, the algorithm combines the two sub-categories into a single sub-category. We use Spearman's rank correlation coefficient to measure the correlation between the validation loss curves. The algorithm also includes a threshold parameter (Th) to determine when a correlation is considered significant and another threshold parameter (Th2) to determine when two sub-categories should be merged.


% Pseudo code  \Cref{alg:IdentifyingDesiredDataCategories}. This pseudo code outlines the method for selecting specific sub-categories among the mined sub-categories. The method involves two steps: In step 1, we use a pre-trained model to train each mined sub-category and calculate its validation loss curve, as well as that of the original dataset. We then check if the validation loss curve for a given sub-category is negatively correlated with the validation loss curve for the original dataset. If so, the sub-category is added to a dictionary called "bank categories". In step 2, the algorithm checks the correlation between the validation loss curves for each pair of sub-categories in the bank. If the correlation is positive, the algorithm combines the two sub-categories into a single sub-category.


\begin{algorithm}
\small 
\setstretch{0.77}  
\caption{Identifying Sub-Categories}
\label{alg:IdentifyingDesiredDataCategories}
\SetAlgoLined
\DontPrintSemicolon 

% \textbf{Input}: Pre-trained model trained on all dataset, mined data categories.

% \textbf{Output}: Non correlate data categories.

\KwIn{ \textbf{generalModel} as pre-trained model trained on general dataset \\ \textbf{allSubCategories} as the mined sub-categories}
\KwOut{Non correlated sub-categories}

\medskip
 
\textbf{Step 1. Inverse-correlation:} \\ 
     allValiCurves = $\{ \}$, bankCategories = $\{ \}$ \\ 

    \For{ subCategory  {\normalfont in} allSubCategories}
        {   
        \begin{enumerate}
            \item Load \textit{generalModel} into  \textit{model}
            \item Train \textit{model} on \textit{subCategory}
            \item Calculate the validation loss curve for all mined categories, including general dataset validation curve.
            \item \textit{valiCorr} $\gets$ \textbf{corr}(\textit{genlVali}, \textit{SubCatVali})
            \item     \If{  valiCorr $< Th $   }
            { 
                \tcp*{Negative correlation response}
                \hspace{3mm} bankCategories $\gets$ sub category; \\
                \hspace{3mm} allValiCurves[subCategory]  $ \gets $ all categories validation errors
            }
        \end{enumerate} }
        
   allValiCurves output:
    \begin{tabular}{l l}
    Sub-category1: & all validation curves \footnotesize (trained on \\ sub-category1)\\ 
        ... & ... \\
    Sub-categoryN: & all validation curves \footnotesize (trained on \\ sub-categoryN) \\ 
    \end{tabular} \\ ~ \\ 

\textbf{Step2. Inter-correlation between sub-categories:}
 
\For{subCategory  {\normalfont in}  bankCategories}
    {
    \begin{enumerate}
        \item  \textit{valiCurvesSubCategory} $\gets$  \textit{allValiCurves}[\textit{subCategory}] 
        % \item  ValSubCategory - validation curve for sub-category trained over sub-category.  
        % \item Get all validation curves over all categories during\\the training over sub-category. 'ValidationCategories[\textbf{SubCategory}]'

        \item  \textit{selfValiCurv} $\gets$  \textit{ValiCurvesSubCategory}[\textit{subCatagorey}]
        
       \item \For{otherValiCurv {\normalfont in} valiCurvesSubCategory}
        {
            \vspace{1mm}  \textit{valiCorr} $\gets$ \textbf{corr}( \textit{selfValiCurv},  \textit{otherValiCurv})
            
            \If{  valiCorr $> Th_2 $ \\ }
            {
               \hspace{-5mm} \tcp*{Positive correlation response}
                
                \textbf{merge}  \textit{otherCategory} with  \textit{subCategory} in \textit{bankCategories}
            }
        }
    \end{enumerate} 
    } 
  \textbf{Return}  bankCategories.
\end{algorithm}  

\section{Qualtitative results}
This section contains supplementary qualitative examples that illustrate the effectiveness of our approach. We compare the outcomes of our method against a standard training method and other leading demosaicing methods.\label{sub:Visual results}  
\subsection{Vanilla training in compare to Our method}
We demonstrate a qualitative evaluation  using a 16K parameters model over chosen sub-categories. We compare between the obtained results using a vanilla training and our method. See in Figures \ref{fig:edges} to \ref{fig:zipper}.

\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=10cm, width=0.85\linewidth]{Edges_sup_title.png }
    \end{center}  
        \caption{Edge metric data category   }
	\label{fig:edges}   
\end{figure*}          

\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=10cm, width=0.85\linewidth]{L1_sup_title.png }
    \end{center}  
        \caption{L1 metric data category }
	\label{fig:l1}   
\end{figure*} 

\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=10cm, width=0.85\linewidth]{grids_sup_title.png }
    \end{center}  
        \caption{Grids metric data category  }
	\label{fig:grids}   
\end{figure*} 

\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=10cm, width=0.85\linewidth]{Perceptual_sup_title.png }
    \end{center}  
        \caption{Perceptual metric data category   }
	\label{fig:perceptual}     
\end{figure*} 

\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=10cm, width=0.85\linewidth]{Zipper_sup_title.png}
    \end{center}  
        \caption{Zipper metric data category   } 
	\label{fig:zipper}   
\end{figure*}  

\begin{figure*}[h!]
    \begin{center}
        \includegraphics[height=9cm, width=1\linewidth]{kodak_1.png}
    \end{center}    
        \caption{Qualitative result using three models with increasing number of parameters: 16K, 84k and 900k over image 1 in Kodak dataset  \cite{li2008image} .}
	\label{fig:kodak_1}   
\end{figure*}    

 
\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[height=9.5cm, width=1.05\linewidth]{Kodak19.png}
    \end{center}  
        \caption{Qualitative result using three models with increasing number of parameters: 16K, 84k and 900k  over image 19 in Kodak dataset \cite{li2008image} .  }
	\label{fig:kodak_19}   
\end{figure*}   


\begin{figure*}[ht!]
  \centering
  \subfloat[] {\label{fig:subfig1}
    \includegraphics[height=10cm, width=1.05\linewidth]{URBAN_86_SUP_3.png}
    }
    \hspace{0.4\linewidth}
   \subfloat[] {\label{fig:subfig2}
    \includegraphics[height=10cm, width=1.05\linewidth]{urban_28_sup_4.png}
  }
  \caption{Qualitative comparison of our method compared to other top methods RNAN and RSTCANet. The RNAN model has 9M parameters, while RSTCANetB , RSTCANetS and RSTCANETL have 0.9M , 3.1M and  7.1M respectively.}
  \label{fig:merged}
\end{figure*}



\subsection{Qualitative results over different models}
In this section, we present the outcomes of our training method utilizing three models with varying parameter counts, namely 16k, 84k, and 900k parameters. To construct the 16k and 84k parameter models, we utilize an architecture similar to Gharbi's \cite{gharbi2016deep} , while for the 900k parameter model, we implement the RSTCANetB architecture \cite{xing2022residual}. We demonstarted over chosen examples from Kodak dataset \cite{li2008image}. See in Figures \ref{fig:kodak_1} and \ref{fig:kodak_19}.

\subsection{Our method compare to top demosaicing models}
Qualitative comparison of our method to other leading demosaicing methods, RNAN \cite{zhang2019residual} and RSTCANet over chosen images from Urban100 \cite{huang2015single}. RNAN comprises 9M parameters,  RSTCANet consists of three different model sizes  B, S, and L  with 0.9M, 3.1M, and 7.1M parameters, respectively. We use our suggested training scheme to train the B model.  Our results, as shown in \Cref{fig:merged}, demonstrate that our method outperforms the RSTCANetB method, which has the same architecture, and also outperforms RSTCANetS and RNAN, despite having significantly fewer parameters. When compared to RSTCANetL, our RSTCANetB produces similar results, with a slight advantage towards RSTCANetL in some cases.

\bibliographystyle{ieee_fullname}
\bibliography{egbib}
\end{document} 