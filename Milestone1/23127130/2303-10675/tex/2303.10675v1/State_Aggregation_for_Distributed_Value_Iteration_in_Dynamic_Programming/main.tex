\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

\pdfminorversion=4

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{breqn}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{graphics}
\usepackage{bbold}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}[thm]{Claim}
\newtheorem{axiom}[thm]{Axiom}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{fact}[thm]{Fact}
\newtheorem{hypo}[thm]{Hypothesis}
\newtheorem{assum}[thm]{Assumption}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{crit}[thm]{Criterion}

\newtheorem{defn}[thm]{Definition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{rem}[thm]{Remark}
\newtheorem{prob}[thm]{Problem}
\newtheorem{prin}[thm]{Principle}
\newtheorem{alg}{Algorithm}

%\usepackage{todonotes}
%\newcommand{\revnote}[2][]{%
%    \todo[inline, color={red!100!green!50},size=\small, #1]{%
%    \textbf{Kostas:}~#2}%
%}
%
%\newcommand{\note}[2][]{%
%    \todo[inline, color={green!50},size=\small, #1]{~#2}%
%}

\begin{document}

\title{State Aggregation for Distributed Value Iteration in Dynamic Programming}

\author{Nikolaus Vertovec
\and
Kostas Margellos \thanks{For the purpose of Open Access the authors have applied a CC BY public copyright licence to any Author Accepted Manuscript (AAM) version arising from this submission. The authors are with the Department of Engineering Science, University of Oxford. Email:\{nikolaus.vertovec, kostas.margellos\}@eng.ox.ac.uk}}

% make the title area
\maketitle

\begin{abstract}
    We propose a distributed algorithm to solve a dynamic programming problem with multiple agents, where each agent has only partial knowledge of the state transition probabilities and costs. We provide consensus proofs for the presented algorithm and derive error bounds of the obtained value function with respect to what is considered as the "true solution" obtained from conventional value iteration. To minimize communication overhead between agents, state costs are aggregated and shared between agents only when the updated costs are expected to influence the solution of other agents significantly. We demonstrate the efficacy of the proposed distributed aggregation method to a large-scale urban traffic routing problem. Individual agents aim at reaching a common access point and share local congestion information with other agents allowing for fully distributed routing with minimal communication between them.
\end{abstract}
\begin{keywords}
    Dynamic programming; Value iteration; Consensus; Distributed algorithms.
\end{keywords}

\IEEEpeerreviewmaketitle
\section{Introduction}
Value iteration is a well-established method for solving dynamic programming problems, yet it exhibits scalability issues for applications with a large state space. To this end, state aggregation can be used to reduce the number of states that need to be considered. The aggregation approach has a long history in scientific computing with applications ranging from the improvement of Galerkin methods \cite{Chatelin1982}, to solving large-scale optimization models \cite{Rogers1991}, and dynamic programming \cite{Bean1987}. In \cite{Tsitsiklis1996} it is shown how the aggregation approach can be used in conjunction with value iteration, however, for problems with large decision spaces and potentially cost and transition probabilities that evolve over time, conventional aggregation will still be hampered by scalability issues. To this end, we propose a multi-agent value iteration algorithm that utilizes aggregation to minimize communication overhead and allows for solving dynamic programming problems where each agent has only partial knowledge of the state transition costs and probability.

In the past, value iteration has been expanded to a multi-agent framework for the class of problems that contain a joint decision space \cite{BERTSEKAS2020}. Similarly to the multi-agent rollout algorithm presented in \cite{Bertsekas2019MultiagentRA}, no restrictions are imposed as far as the agents' knowledge of the underlying transition probabilities and costs is concerned. Such restrictions have been already considered for problems of small-scale as in \cite{Paul2022MultiAgentNR}. These centralized-learning, distributed-execution schemes \cite{Zhang2021}, however, require a centralized critic for estimating the expected team benefit in a non-cooperative setting. We instead opt for a distributed approach considering a cooperative setting, however, still minimizing communication overhead.

This paper performs the following main contributions: (i) We propose a distributed value iteration algorithm for multi-agent dynamic programming, preventing transition probabilities and state costs from constituting global information among agents; (ii) We provide a rigorous mathematical analysis on the convergence and optimality properties of the proposed algorithm, merging tools from multi-agent consensus and dynamic programming principles; and, (iii) We demonstrate our scheme on a non-trivial traffic routing problem. 

The rest of the paper is organized as follows. Section \ref{sec:Problem Setup} provides some mathematical preliminaries and introduces a distributed multi-agent value iteration algorithm. Section \ref{sec:proofs} provides the main statements and associated mathematical analysis supporting the proposed algorithm. In Section \ref{sec:Application} we introduce a traffic routing problem and show how our proposed approach is able to solve the problem in a distributed manner. Finally, Section \ref{sec:Conclusion} provides concluding remarks and directions for future work.

\section{Problem Setup} \label{sec:Problem Setup}
\subsection{Multi-agent Markov Decision Processes}
We first introduce a standard Markov Decision Process (MDP) that will be expanded to a multi-agent setting in view of the proposed distributed algorithm presented in the sequel. We consider a finite set of $n$ states denoted by $\mathcal{X}$ and let $g(i,u,j)$ be the cost-to-go from state $i \in \mathcal{X}$ to state $j \in \mathcal{X}$, using the input $u \in \mathcal{U}(i)$, where $\mathcal{U}(i)$ is a finite set of actions/decisions available at state $i$. Furthermore, let $p(i,u,j)$ be the transition probability to transition from state $i$ to state $j$ using the input $u$, with $\sum_{j = 1}^n p(i,u,j) = 1$. The objective is to solve a global discounted infinite horizon dynamic programming problem. The associated Bellman equation is given by
\begin{equation}
    J^*(i) = \min_{u \in U(i)} \sum_{j=1}^n p(i,u,j)[g(i,u,j) + \alpha J^*(j)],
    \label{eqn:bellman}
\end{equation}
where $\alpha \in [0,1)$ is a discount factor, and $J^*$ denotes the optimal value function that satisfies the Bellman identity. Solving \eqref{eqn:bellman} using conventional value iteration or policy iteration requires knowledge of all transition probabilities as well as associated costs. 
%\revnote{Expand to explain that we have finite actions/states etc.}

To this end, we consider a setting where a set of $q$ agents collaborate to solve the aforementioned infinite horizon dynamic programming problem in a distributed manner, with only partial knowledge of the state transition probabilities and costs while minimizing communication between agents. As such, we partition the state-space $\mathcal{X}$ into $q$ subsets, $I_l \subset \mathcal{X}$, such that for all $m, l \in \{1, \ldots, q \}$ with $m \neq l$, $I_l \cap I_m = \emptyset$. Each agent knows only the state transition probabilities and cost for transitions originating within its state subset $I_l$, i.e., for each agent $l$, $g_l : I_l \times \mathcal{U} \times \mathcal{X} \rightarrow \mathbb{R}$, $p_l : I_l \times \mathcal{U} \times \mathcal{X} \rightarrow [0,1]$. Using conventional value iteration would require each agent to share its knowledge of the transition probabilities and costs, resulting in significant communication overhead. In the subsequent section, we instead introduce an alternative method relying on state aggregation that allows solving the discounted infinite horizon dynamic programming problem under consideration in a distributed manner where some tentative information is exchanged only with other agents.

\subsection{Distributed Value Iteration}
We start by aggregating the value function (expected optimal cost-to-go) to construct for each agent $l=1,\ldots,q$, the aggregate value, $r_l$, defined as 
\begin{equation}
    r_l = \sum_{i \in I_l} d_{li} J(i),
\end{equation}
where $J$ constitutes an approximation of the optimal value function of \eqref{eqn:bellman}, and $d_{li}$ is the so called disaggregation probability (encoding the contribution of each agent's value function to the aggregate one), satisfying for all $l=1,\ldots,q$,
\begin{align}
    \sum_{i \in I_l} d_{li} = &{} 1, \\
    d_{li} = & 0, \: \text{ for all } i \notin I_l.
\end{align}
The aggregate values for each agent, $r_l$, can be combined into a common vector denoted by $r$; tentative values for this vector will be communicated across all agents. Thus we define $r$ as
\begin{equation}
    r = \begin{bmatrix} r_1, \ldots, r_l, \ldots, r_q \end{bmatrix}^T.
\end{equation}
Since $r$ will be updated by means of an iterative scheme to be introduced in the sequel, by $r^k$ we denote its value at iteration $k$.


\begin{figure}
 	\centering
	\includegraphics[width=0.45\textwidth]{Figures/DMDP.png}
	\caption{An example of how the transition probabilities are manipulated using disaggregation probabilities, $d_{li}$. Each transition from a state within an aggregation set $I_l$ to a state lying in another aggregation set, e.g., $I_m$, is rerouted to an auxiliary state. The cost-to-go value at this auxiliary state is $r_m$, the aggregated cost over the relevant aggregation set $I_m$. From the auxiliary state, it is possible to reach states within the aggregation set with disaggregation probability $d_{mj}$.}
	\label{fig:DMDP}
\end{figure}
Next, we introduce the aggregation probability satisfying for all $l=1,\ldots,q$,
\begin{equation}
    \phi_{jl} = \begin{cases}
    1, & \text{if } j \in I_l, \\
    0, & \text{otherwise}.
    \end{cases}
\end{equation}
Such a formulation of the  aggregation probability is known as hard aggregation in the literature \cite[pg. 311]{bertsekas2019}.

To solve the discounted infinite horizon dynamic programming problem, we propose a distributed algorithm for which the pseudocode is given in Algorithm \ref{alg:AgentComms} and \ref{alg:AgentUpdate}. Initially, each agent $l$, $l = 1, \ldots, q$, starts with some tentative value of the aggregate vector $r^{0}$, and local cost function $V_l^{0}$. Any choice of $r^{0}$ and $V_l^{0}$ is acceptable thus without loss of generality, we choose $r_l^{0} = 0, \forall l \in 1, \ldots, q$ and $V_l^{0}(i) = 0, \forall i \in I_l$. Utilizing the aggregate vector $r^{k}$ at iteration $k \geq 0$, each agent $l$ locally and in parallel executes Algorithm \ref{alg:AgentUpdate} so as to construct the updated local value function $V_l^{k+1}: I_l \rightarrow \mathbb{R}$ which will, in turn, yield an updated aggregated cost $r_l^{k+1} \in \mathbb{R}$ (Algorithm \ref{alg:AgentComms}, Steps \ref{alg2:line:func}).

To minimize communication overhead, an updated aggregated cost $r_l^{k+1}$ is only shared with other agents when the aggregated cost has changed significantly, i.e., $\|r_l^{k+1} - r_l^{\mathrm{prev}}\| > C_{\mathrm{threshold}}$, where $r_l^{\mathrm{prev}}$ is the aggregated cost previously broadcast to other agents and $C_{\mathrm{threshold}}$ is a communication threshold (Algorithm \ref{alg:AgentComms}, Steps \ref{alg2:line:broadcast_start} - \ref{alg2:line:broadcast_end}). For a further discussion of using a communication threshold to limit the transmission of insignificant data, we refer to \cite{Gatsis22}.

Until convergence is reached, each agent will repeat the execution of Algorithm \ref{alg:AgentUpdate} and the subsequent communication step. Note that at iteration $k$, the vector $r^{k}$ may contain aggregate values from other agents received at prior time steps. It will be shown in sequel that convergence will nevertheless be reached.

We now turn to the update of $V_l^{k}$ and $r_l^{k}$, performed when calling Algorithm \ref{alg:AgentUpdate}. Each agent uses the local value function, $V_l^{k}$ to represent the cost-to-go function at each state $i \in I_l$, and the aggregated cost, $r^{k}$, to approximate the cost for states $j \in \mathcal{X} \backslash I_l$. The local value function is iteratively updated for each state $i \in I_l$ (Algorithm \ref{alg:AgentUpdate}, Steps \ref{alg1:line:V_update_start} - \ref{alg1:line:V_update_end}). This update follows from standard value iteration; choosing the control action at state $i$ that minimizes the expected cost-to-go (Algorithm \ref{alg:AgentUpdate}, Step \ref{alg2:line:bellmanoperator}) and using a Gauss-Seidel update on the local value function to improve convergence and minimize memory usage (Algorithm \ref{alg:AgentUpdate}, Step \ref{alg2:line:gauss-seidel}). We perform an update only to the local value function since we restrict ourselves to states for which sufficient knowledge of the transition probabilities and costs is available. After the local value function has been updated, the updated local aggregated cost, $r_l^{k+1}$, is computed (Algorithm \ref{alg:AgentUpdate}, Step \ref{alg1:line:r_l}).

Notice that Algorithms \ref{alg:AgentComms} and \ref{alg:AgentUpdate} prevent disclosing the transition probabilities to all agents; in contrast, each agent $l$, has access only to the probabilities associated to transitions originating from their partition $I_l$, $l=1,\ldots,q$.

\begin{algorithm}
\caption{Distributed value iteration}
\begin{algorithmic}[1]
    \State \textbf{Initialization}
    \State Set $r_l^{0} \leftarrow 0$ for all $l = {1, \ldots, q}$
    \State Set $r_l^{\mathrm{prev}} \leftarrow 0$ for all $l = {1, \ldots, q}$
    \State Set $V_l^{0}(i) \leftarrow 0$ for all $i \in I_l$ and $l = {1, \ldots, q}$
    \State $k = 0$
\Repeat \label{alg2:line:repeat}
\For{each Agent $l \in {1, \ldots, q}$} 
    \State $V_l^{k+1}, r_l^{k+1} \leftarrow $Agent-Update($V_l^{k}, r^{k}$) \label{alg2:line:func}
    \If{$\|r_l^{k+1} - r_l^{\mathrm{prev}}\| > C_{\mathrm{threshold}}$} \label{alg2:line:broadcast_start}
        \State Send $r_l^{k+1}$ to all agents in the network
        \State $r_l^{\mathrm{prev}} \leftarrow r_l^{k+1}$  
    \EndIf \label{alg2:line:broadcast_end}
\EndFor 
\Until{$\|r_l^{k+1} - r_l^{k}\|\leq \mathrm{tolerence}$ for all $l = {1, \ldots, q}$}
\end{algorithmic}
\label{alg:AgentComms}
\end{algorithm}

\begin{algorithm}
\caption{Agent-Update}
\begin{algorithmic}[1]
\Function{Agent-Update}{$V_l^{k}, r^{k}$}
    \For{$i \in I_l$} \label{alg1:line:V_update_start}
        \State $V_l^{k+1}(i) \leftarrow \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)[g_l(i,u,j) + \alpha (\phi_{jl} V_l^{k}(j) + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} r_m^{k})]$ \label{alg2:line:bellmanoperator}
        \State $V_l^{k}(i) \leftarrow V_l^{k+1}(i)$ \label{alg2:line:gauss-seidel}
    \EndFor \label{alg1:line:V_update_end}
    \State $r_l^{k+1} \leftarrow \sum_{j \in I_l} d_{lj} V_l^{k+1}(j)$ \label{alg1:line:r_l}
    \State \textbf{return} $V_l^{k+1}, r_l^{k+1}$
\EndFunction
\end{algorithmic}
\label{alg:AgentUpdate}
\end{algorithm}

The proposed algorithm involves an agent-to-agent communication protocol. At each algorithm iteration $k \geq 0$ we consider the directed communication graph $(V,E(k))$, where the node set $V = \{1,\ldots,q\}$ includes the agents and the set $E(k)$ directed edges $(m,l)$ indicating that at iteration $k$ agent $l$ receives information from agent $m$. Let $E_B(k) =  \bigcup_{i=kB}^{(k+1)B-1} E(i)$ for some integer $B$. 
\begin{assum} \label{ass:Network}
    [Connectivity and Communication] There exists a positive integer $B$ such that $(V,E_B(k))$ is fully connected for each iteration $k$, i.e., for any two agents, there exists an edge such that, within finite time, they can exchange information.
\end{assum}

Assumption \ref{ass:Network} can be relaxed to allow $(V,E_B(k))$ to be $B$-strongly connected \cite{Saadatniaki2020}, however, this requires agents to relay information. To ease the algorithm analysis, we only consider the setting of Assumption \ref{ass:Network}.

\section{Algorithm analysis} \label{sec:proofs}
The following theorem summarizes the main result of this section.
\begin{thm} Consider Assumption \ref{ass:Network}.
Algorithm \ref{alg:AgentComms} converges asymptotically to a common value for $r$ among agents, i.e., there exists $\bar{r}$ such that $\lim_{k \to \infty} \|r^k - \bar{r}\| = 0$. 
Moreover, for all $l=1,\ldots,q$, for all $i \in \mathcal{X}$, $V_l^k(i)$ converges to some $\overline{V}_l^*(i)$.
%\revnote{or "to $r^*$ such that $\lim_{k \rightarrow \infty} \|r_l^k - r^*\| =0$ for all $l=1, \ldots$"}
\label{thm:consensus}
\end{thm}

%Note that an agent $l$ may have computed an update to its aggregated cost, $r_l$, which deviates less than $C_{\mathrm{threshold}}$ from its previously transmitted cost. Since an agent's own $r_l$ is not used for computing an update to its own cost function, $V_l$, the updated $r_l$ when not transmitted can be discarded and thus consensus is still considered to have been reached.

\begin{proof}
We show convergence when agents transmit $r_l^k$, $l=1,\ldots,q$, irrespective of whether this deviates more than $C_{\mathrm{threshold}}$ from their previously transmitted cost.
This is without loss of generality as in case some updates are discarded from Step \ref{alg2:line:func} in Algorithm \ref{alg:AgentComms}, then the resulting sequence of transmitted costs would form a subsequence of $\{r_l^k\}_{k=0}^\infty$, hence it will be convergent as well if $\{r_l^k\}_{k=0}^\infty$ is shown to converge.


Define the incremental update of the shared variable and its worst-case value (in norm) among all agents as 
\begin{align*}
    \epsilon_l^{k} & = r_l^{k+1} - r_l^{k}  \\
    \epsilon_\infty^{k} & = \max_{l = 1, \ldots, q} |r_l^{k+1} - r_l^{k}|,
\end{align*}

%The transmission threshold is a tunable parameter that influences the frequency at which agents communicate. While it might affect the speed of convergence, it does not influence the algorithm's ability to converge, as anytime the threshold for transmission is not reached, an agent $l$ will not send out an updated $r_l$ which subsequently is equivalent to setting $\epsilon_l^k = 0$ for all other agents, thus satisfying the condition for reaching convergence. We thus, neglect the transmission threshold for the remainder of the proof and it suffices to show that the largest element change to $r$ at iteration $k$, i.e. $\epsilon_\infty^{k}$ will diminish with $k$. 
  
  Consider the update to the value $V_l^{k}(i)$, for an arbitrary iteration $k$ and agent $l=1,\ldots,q$, which we define as 
\begin{align*}
    \delta_l^{k}(i) & = V_l^{k+1}(i) - V_l^{k}(i) \\
    & = \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl} V_l^{k}(j) \\ & \qquad  + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} r_m^{k}]\Big) - V_l^{k}(i)\\
    %
    %& = \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl} (V_l^{k-1}(j) \\ & \qquad +\delta_l^{k-1}(j)) + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} (r_m^{k-1} + \epsilon_m^{k-1})]\Big) - V_l^{k}(i) \\
    %
    & = \min_{u \in U(i)}\Big( \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl} V_l^{k-1}(j) \\ & \qquad + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} r_m^{k-1}]\Big) + \alpha\sum_{j=1}^n p_l(i,u,j)\Big(\phi_{jl}\delta_l^{k-1}(j) \\ & \qquad  +\sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \epsilon_m^{k-1} \Big)\Big) - V_l^{k}(i) \\
    %
    & \leq V_l^{k}(i) + \max_{u \in U(i)} \alpha\sum_{j=1}^n p_l(i,u,j)\Big(\phi_{jl}\delta_l^{k-1}(j) \\ & \quad +\sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \epsilon_m^{k-1} \Big) - V_l^{k}(i)\\
    %
    & = \max_{u \in U(i)} \alpha\sum_{j=1}^n p_l(i,u,j)\Big(\phi_{jl}\delta_l^{k-1}(j) +\sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \epsilon_m^{k-1} \Big),
\end{align*}
where the first equality follows from the definition of $V_l^k$, and the second one from a rearrangement after substituting $V_l^{k}(j) = V_l^{k-1}(j) + \delta_l^{k-1}(j)$ and $r_m^{k} = r_m^{k-1} + \epsilon_m^{k-1}$. The inequality follows from the definition of $V_l^k$ and via considering the maximum rather than the minimum over $u\in U(i)$.

For each $l=1, \ldots, q$, we now define the maximum update $\delta_l^{k}$, over all states $i \in I_l$, as
\begin{equation}
    \overline{\delta}_l^{k} = \max_{i \in I_l} \delta_l^{k}(i). \label{eq:max_delta}
\end{equation}
Then it follows that for all $i\in I_l$,
\begin{align}
    \delta_l^{k}(i) & \leq \max_{u \in U(i)} \alpha\sum_{j=1}^n p_l(i,u,j)\Big(\phi_{jl}\delta_l^{k-1}(j) +\sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \epsilon_m^{k-1} \Big) \nonumber\\
    %
    & \leq \max_{u \in U(i)} \alpha\sum_{j=1}^n p_l(i,u,j)\Big(\phi_{jl}\overline{\delta}_l^{k-1} +\sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \epsilon_\infty^{k-1} \Big) \nonumber \\
    %
    & \leq \max_{u \in U(i)} \alpha \sum_{j=1}^n p_l(i,u,j)\Big(\sum_{m=1}^q \phi_{jm} \max\{\overline{\delta}_l^{k-1},\epsilon_\infty^{k-1}\} \Big) \nonumber \\
    %
    & = \alpha\max\{\overline{\delta}_l^{k-1},\epsilon_\infty^{k-1}\},\label{eq:delta_bound}
\end{align}  
where the first inequality follows from \eqref{eq:max_delta}, and the last one from the fact that $\sum_{j=1}^n p(i,u,j) = 1$, and $\sum_{m=1}^q \phi_{jm} = 1$.

For $\max_{l \in 1, \ldots, q} \overline{\delta}_l^{k}$ it then immediately follows that
\begin{align}
    \overline{\delta}_\infty^{k} \coloneqq \max_{l \in 1, \ldots, q} \overline{\delta}_l^{k} & \leq  \alpha \max\{\max_{l \in 1, \ldots, q} \overline{\delta}_l^{k-1},\epsilon_\infty^{k-1}\}. \label{eq:delta_inf}
\end{align}
We now consider the update $\epsilon_\infty^{k}$, i.e.,
\begin{align}
    \epsilon_\infty^k &= \max_{l \in 1, \ldots, q} |r_l^{k+1} - r_l^{k}| \nonumber \\
    %
    %& = \max_{l \in 1, \ldots, q} |\sum_{i \in I_l} d_{li} V_l^{k+1}(i) - r_l^{k}| \\
    %
    & =  \max_{l \in 1, \ldots, q} |\sum_{i \in I_l} d_{li} (V_l^{k}(i) + \delta_l^{k}(i)) - r_l^{k}| \nonumber\\
    % 
    & = \max_{l \in 1, \ldots, q} |r_l^{k} + \sum_{i \in I_l} d_{li}\delta_l^{k}(i) - r_l^{k}| \nonumber \\
    % 
    & = \max_{l \in 1, \ldots, q} |\sum_{i \in I_l} d_{li}\delta_l^{k}(i)| \nonumber \\
    % 
    & \leq \alpha\max_{l \in 1, \ldots, q} |\sum_{i \in I_l} d_{li} \max\{\overline{\delta}_l^{k-1},\epsilon_\infty^{k-1}\}|\nonumber \\
    % 
    & = \alpha\max_{l \in 1, \ldots, q} | \max\{\overline{\delta}_l^{k-1},\epsilon_\infty^{k-1}\}| \nonumber \\
    % 
    & = \alpha | \max\{\max_{l \in 1, \ldots, q} \overline{\delta}_l^{k-1},\epsilon_\infty^{k-1}\}|, \label{eq:eps_inf}
\end{align}
where the second equality follows from the fact that $r_l^{k+1} = \sum_{i \in I_l} d_{li} V_l^{k+1}(i)$ and $V_l^{k+1}(i) = V_l^{k}(i) + \delta_l^{k}(i)$, while the third one is due to Step \ref{alg1:line:r_l} of Algorithm \ref{alg:AgentUpdate}. The inequality is due to \eqref{eq:delta_bound}, the second last equality follows from the fact that $\sum_{i \in I_l} d_{li} = 1$, while the last one is due to a change in the order of the two max operators.

By \eqref{eq:delta_inf} and \eqref{eq:eps_inf} we then have that $\max\{\overline{\delta}_\infty^{k}, \epsilon_\infty^{k}\} \leq \alpha \max\{\overline{\delta}_\infty^{k-1}, \epsilon_\infty^{k-1}\}$, which implies that $\max\{\overline{\delta}_\infty^{k}, \epsilon_\infty^{k}\}$ is contractive. As a result, $\{\max\{\overline{\delta}_\infty^{k}, \epsilon_\infty^{k}\}\}_{k\geq 0}$, and hence also $\{\epsilon_\infty^{k}\}_{k\geq0}$ and $\{\overline{\delta}_{\infty}^k\}_{k\geq 0}$ will be converging to zero. The former implies that for all $l=1,\ldots,q$ there exists $\bar{r}$ such that $\lim_{k \to \infty} \|r^k - \bar{r}\| = 0$, while the latter (due to \eqref{eq:max_delta}, \eqref{eq:delta_inf}, and the definition of $\delta_l^k(i)$) implies that for all $l=1,\ldots,q$, for all $i\in \mathcal{X}$, $\{V_l^k(i)\}_{k\geq 0}$ would be convergent, thus concluding the proof.
\end{proof}
Theorem \ref{thm:consensus} implies consensus among agents to a common $\bar{r}$, and also establishes convergence of $\{V_l^k(i)\}_{k \geq 0}$ to some $V_l^*(i)$.
Next, we consider error bounds between the limiting $V_l^*(i)$ and the optimal $J^*(i)$, satisfying \eqref{eqn:bellman}.
%Moreover, since $r$ will converge, it follows from standard value iteration that all agents will eventually converge to a cost $V_l(i)$ for all states $i \in I_l$. 
%\revnote{expand that convergence/consensus on r implies convergence in $V_l$} for all states $i \in I_l$. 



\begin{thm}
Consider Assumption \ref{ass:Network} and Algorithm \ref{alg:AgentComms}. For all $l=1,\ldots,q$, $i \in \mathcal{X}$, we have that the limit point $V_l^*(i)$ of $\{V_l^k(i)\}_{k \geq 0}$ satisfies
\begin{equation}
    |V_l^*(i) - J^*(i)| \leq \alpha \frac{\delta}{1-\alpha}, \label{eq:errorBounds}
\end{equation}
where $\delta  \coloneqq \max_{l \in \{1, \ldots, q\}}\max_{i,j \in I_l} |J^*(i) - J^*(j)|$ and $J^*$ is the solution of \eqref{eqn:bellman}.
\label{thm:errorBounds}
\end{thm}
\begin{proof}
The proof is inspired by \cite{Tsitsiklis1996}. Fix $l=1,\ldots,q$ and $i\in \mathcal{X}$. We only show that $V_l^*(i) \leq J^*(i) + \alpha \frac{\delta}{1-\alpha}$, as the other side in \eqref{eq:errorBounds} follows symmetric arguments.
To this end, for each state $i \in I_l$, define
$\overline{V}_l(i) \coloneqq J^*(i) + \alpha \frac{\delta}{1-\alpha}$.
It follows then that the aggregate of $\overline{V}_l(i)$ can be constructed as
\begin{align}
    \overline{r}_l & \coloneqq \sum_{j \in I_l} d_{lj}\overline{V}_l(j) = \sum_{j \in I_l} d_{lj} (J^*(j) + \alpha \frac{\delta}{1-\alpha} ) \nonumber \\
    %
    & \leq
    \sum_{j \in I_l} d_{lj}(\min_{i \in I_l} J^*(i) + \delta + \alpha \frac{\delta}{1-\alpha} ) \nonumber \\
    %
    & = \min_{i \in I_l} J^*(i) + \delta + \alpha \frac{\delta}{1-\alpha} = \min_{i \in I_l} J^*(i) + \frac{\delta}{1-\alpha}, \label{eq:r_bar}
\end{align}
where the inequality follows from the fact that by the definition of $\delta$ resulting in $J^*(j) \leq \min_{i \in I_l} J^*(i) + \delta$, while the second last equality utilizes $\sum_{j \in I_l} d_{lj} = 1$.

Denote the Bellman operator induced by \eqref{eqn:bellman} as $T$, such that we can compactly write \eqref{eqn:bellman} as $J^*(i) = (TJ^*)(i)$, where effectively by $(TJ^*)(i)$ we imply the right-hand side of \eqref{eqn:bellman} which depends on $i$ and on $J^*(j)$ for all $j=1,\ldots,n$.
For all $i=1,\ldots,n$, we now consider the application of the Bellman operator to $\overline{V}_l$, i.e.,
\begin{align}
    (T \overline{V}_l)(i) & = \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) +\alpha [\phi_{jl} \overline{V}_l(j) \nonumber \\ & \qquad 
    + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \overline{r}_m^{k}]\Big) \nonumber \\
    %
    & \leq \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl}  (J^*(j) \nonumber \\ & \qquad 
    + \alpha\frac{\delta}{1-\alpha}) + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} (\min_{j \in I_m} J^*(j) + \frac{\delta}{1-\alpha})]\Big) \nonumber \\
    %
    %& \leq \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl} (J^*(j) \\ & \qquad 
    %+ \underbrace{\alpha}_{< 1}\frac{\delta}{1-\alpha}) + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} (J^*(j) + \frac{\delta}{1-\alpha})]\Big) \\
    %
    & \leq \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) \nonumber \\
    & \qquad + \alpha \sum_{m=1}^q \phi_{jm}(J^*(j) + \frac{\delta}{1-\alpha})\Big) \nonumber \\
    %
    & \leq \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha J^*(j)\Big) \nonumber \\
    & \qquad + \alpha \frac{\delta}{1-\alpha} \nonumber \\
    %
    & = J^*(i) + \alpha \frac{\delta}{1-\alpha}
      = \overline{V}_l(i), \label{eq:V_mon}
\end{align}
where the first inequality is due to the definition of $\overline{V}_l$. The second inequality follows, since $\min_{j \in I_m} J^*(j) \leq J^*(j)$ for all $j \in \mathcal{X}$, and $\alpha\frac{\delta}{1-\alpha} \leq \frac{\delta}{1-\alpha}$, that in turn allows us to combine the terms multiplied with $\phi_{jl}$ into the last summation. The last inequality follows from $\sum_{m=1}^q \phi_{jm} =1$, while the second last equality is due to the Bellman equation in \eqref{eqn:bellman}.

By \eqref{eq:V_mon} we have that $(T \overline{V}_l)(i) \leq \overline{V}_l(i)$, for all $i=1,\ldots,n$, which implies that $\{\overline{V}_l^k(i)\}_{k \geq 0}$ is a non-increasing sequence. Moreover, $T$ is contractive and as such it will converge to its (unique) fixed point; a direct consequence of Theorem \ref{thm:consensus} is that this fixed point is $V_l^*(i)$ (as the latter was constructed by successive applications of the Bellman operator). Therefore, for all $i=1,\ldots,n$, $V_l^*(i) = \lim_{k \to \infty} (T^k \overline{V}_l)(i) \leq \overline{V}_l(i)$, thus establishing $V_l(i) \leq J^*(i) + \alpha \frac{\delta}{1-\alpha}$, concluding the proof.
\end{proof}

\begin{comment}
\subsection{temporary sanity check}
Let us begin by defining the value function $\underline{V}_l(i)$ for each state $i \in I_l$ as 
\begin{align*}
    \underline{V}_l(i) & \coloneqq J^*(i) - \alpha \frac{\delta}{1-\alpha}.
\end{align*}
Then it follows that the vector $\underline{r}$ can be defined component-wise as
\begin{align*}
    \underline{r}_l & \coloneqq \sum_{j \in I_l} d_{lj}\underline{V}_l(i) = \sum_{j \in I_l} d_{lj} (J^*(i) - \alpha \frac{\delta}{1-\alpha} ) \\
    & \geq
    \max_{j \in I_l} J^*(j) - \delta - \alpha \frac{\delta}{1-\alpha} = \max_{j \in I_l} J^*(j) - \frac{\delta}{1-\alpha}.
\end{align*}
We now consider the update to the value function at each iteration $k$ as
\begin{align*}
    \underline{V}_l^{k+1}(i) & = \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl} \underline{V}_l^{k}(j) \\ & \qquad 
    + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} \underline{r}_m^{k}]\Big) \\
    %
    & = \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl}  (J^*(j) \\ & \qquad 
    - \alpha\frac{\delta}{1-\alpha}) + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} (\max_{j \in I_m} J^*(j) - \frac{\delta}{1-\alpha})]\Big) \\
    %
    & \geq \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [\phi_{jl} (J^*(j) \\ & \qquad 
    - \alpha\frac{\delta}{1-\alpha}) + \sum_{\substack{m=1 \\ m\neq l}}^q \phi_{jm} (J^*(j) - \frac{\delta}{1-\alpha})]\Big) \\
    %
    & \geq \min_{u \in U(i)} \sum_{j=1}^n p_l(i,u,j)\Big(g_l(i,u,j) + \alpha [J^*(j) - \frac{\delta}{1-\alpha}]\Big) \\
    %
    & = J^*(i) - \alpha \frac{\delta}{1-\alpha}
      = \underline{V}_l^{k}(i).
\end{align*}
\end{comment}

It follows from Theorem \ref{thm:errorBounds}, that if the aggregation sets $I_{1}, \ldots, I_{q}$ are chosen such that the cost function $J^*$ is expected to vary moderately between states within an aggregation set, then the maximum error compared to $V_{1}, \ldots, V_{q}$ will be moderate as well. One method to aggregate states with similar costs is to use feature-based aggregation, whereby the aggregation is performed on a set of representative features instead of representative states. This form of aggregation has been thoroughly investigated; we refer to \cite[pg. 322]{bertsekas2019}, \cite[pg. 68]{bertsekas1996} and references therein. For applications with a high discount factor, i.e., $\alpha << 1$, the maximum error will also be moderate.


\section{Application to traffic routing} \label{sec:Application}
\subsection{Simulation Set-up}
We demonstrate the proposed algorithm in a traffic routing case study. To this end, we begin by modeling the traffic network as a graph, with vertices representing junctions and edges representing roads connecting junctions. An illustration of such a graph representation for the Oxford road network is shown in Figure \ref{fig:oxford_graph}.

\begin{figure}
 	\centering
	\includegraphics[width=0.45\textwidth]{Figures/oxford_graph.png}
	\caption{Graph representation of the Oxford road network. Junctions are represented by vertices and edges represent roads.}
	\label{fig:oxford_graph}
\end{figure}

As is common in transit node routing, all agents are trying to reach a common access node, such as a freeway used for long-distance routing. Each agent's state is the last visited vertex in the local traffic network. The goal is to find the fastest path to the access point. In our example related to the Oxford traffic network, the access point will be London Road, which leads long-distance travelers toward London.

The cost at each vertex is the expected travel time to the access point. This information can be gathered by a vehicle sharing its current speed and location with other cars close by, independent of the destination, as this information is needed solely to determine the cost of an edge. The cost-to-go information is collected locally by an agent and is used to determine $g_l$, $l=1,\ldots,q$, as
\begin{equation}    
    g_l = \frac{\text{length of edge}}{\text{average speed of cars on edge}}. 
    \label{eqn:cost-to-go}
\end{equation}
%A schematic illustration of such a distributed traffic routing set-up is illustrated in Figure \ref{fig:TrafficApplication}.

%\begin{figure}
% 	\centering
%	\includegraphics[width=0.5\textwidth]{Figures/TrafficApplication.png}
%	\caption{An illustration of the adopted set-up: Each agent gathers local traffic data to compute a local cost for each %vertex. Aggregate costs are used for all vertices lying outside the aggregation set.}
%	\label{fig:TrafficApplication}
%\end{figure}

The advantage of applying the proposed algorithm is its distributed nature, i.e., there is no central authority that is collecting traffic data. Each agent exchanges anonymous speed/congestion data with other vehicles in its local vicinity to build an estimate of the congestion and required travel time within a given region. Using a long-range low bandwidth network, an agent is able to communicate its aggregated cost with other agents trying to reach a given access point. The disaggregation and aggregation sets can be defined in advance based on geographical features and thus do not need to be broadcasted.

\subsection{Simulation results}
To demonstrate the efficacy of the proposed distributed algorithm, the Oxford road network is partitioned into 5 subgraphs using K-means clustering of the vertices based on their euclidean distance, as shown in Figure \ref{fig:clustering}.

\begin{figure}
 	\centering
	\includegraphics[width=0.45\textwidth]{Figures/clustering.png}
	\caption{The Oxford road network is subdivided into 5 subgraphs representing the aggregation sets.}
	\label{fig:clustering}
\end{figure}

The cost-to-go is calculated as in \eqref{eqn:cost-to-go}, where the average car speed is randomly generated from a uniform distribution to lie between $25\%$ and $100\%$ of the speed limit of the relevant road. For each vertex, the outgoing edges are enumerated and the transition probability between two vertices is set to either 1 or 0, depending on whether an edge directly connects the vertices and the input selects that edge.

The disaggregation probabilities are zero for all nodes that have no edge connected to a vertex in another aggregation set $I_l$. The remaining vertices are given a normalized non-zero disaggregation probability. The discount factor,  $\alpha = 0.9$, is chosen close enough to 1 so as to reflect the desire to reach the final node and avoid loops, yet smaller than 1, so as to weight costs further off as less important due to the constantly evolving traffic situation.

Solving the aggregate problem and comparing it to what is considered to be the "true solution" $J^*$ obtained via conventional value iteration, we notice that the average error of the expected cost-to-go, i.e., $\frac{1}{n} \sum_{i \in \bigcup_{l = 1, \ldots, q} I_l}(|J_{\mathrm{agg}}(i) - J^*(i)|)$, is 0.0086 minutes, and the maximum error i.e., $\mathrm{max}_{i \in \bigcup_{l \in 1, \ldots, q} I_l}(|J_{\mathrm{agg}}(i) - J^*(i)|)$, is 0.5960 minutes, where $J_{\mathrm{agg}}$ denotes the combined cost-to-go-function of the aggregated problem, i.e., $J_{\mathrm{agg}}(i) = \sum_{l = 1, \ldots, q} \phi_{il} V_l(i)$, for all $i \in \mathcal{X}$. The value function is shown in Figure \ref{fig:cost}. The evolution over time of the aggregated costs is shown in Figure \ref{fig:communication}. The colored dots represent when the change to an agent's local value function compared to the last broadcast is greater than the chosen communication threshold ($0.1$ in this example). At this point, the agent will broadcast its updated aggregated cost. If since the last broadcast, no agents value function has changed significantly, the agents signal each other that convergence is reached and the algorithm terminates.
\begin{figure}
 	\centering
	\includegraphics[width=0.45\textwidth]{Figures/cost.png}
	\caption{The node cost sorted by size. The blue line indicates the "true cost" computed using conventional value iteration, the red markers represent the cost obtained by the proposed distributed algorithm.}
	\label{fig:cost}
\end{figure}
\begin{figure}
 	\centering
	\includegraphics[width=0.45\textwidth]{Figures/communication.png}
	\caption{The aggregated cost $r$ evolving over time with minimal communication between agents. The distributed algorithm is run in a multi-threaded environment with each agent sending its $r$ values between threads.}
	\label{fig:communication}
\end{figure}

To reproduce the numerical results the associated code has been made available in \cite{VertovecCode2022}, with the ability to upload any open street map file, which is then converted to a graph and subsequently a discounted Markov decision process.

\section{Conclusion}\label{sec:Conclusion}
We presented a multi-agent extension of aggregated value iteration which was shown to be able to solve large-scale dynamic programming problems in a fully distributed manner. The presented methodology finds application in problems where each agent has only partial knowledge of the transition probabilities and costs. To this end, we demonstrated its efficacy in a distributed traffic routing problem, for which the code has been made available in \cite{VertovecCode2022}. 

Future work will consider expanding the methodology and proofs beyond the case of hard aggregation (where each state is clearly assigned to no more than one agent). From an application point of view, we aim to further investigate the merits of the proposed scheme over conventional traffic routing methodologies with respect to user privacy, tracking, and reliance on external centralized servers. 
%\revnote{add necessary funding sentence}

\bibliographystyle{./bibtex/IEEEtran}
\bibliography{./bibtex/IEEEabrv,./bibtex/IEEEexample,./bibtex/bibliography}


\end{document}