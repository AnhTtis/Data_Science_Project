\section{Introduction}
% Motivation
% background
Point cloud uses a great number of unconstrained 3D points to render 3D objects and scenes realistically, in which each point is formed using the coordinate $(x,y,z)$ and associated attributes such as the color (RGB or YCoCg), normal, and reflectance. Nowadays, point clouds have been massively used in networked applications including Augmented and Virtual Reality, Autonomous Machinery, etc., making the desire for efficient Point Cloud Compression (PCC) more and more indispensable. In addition to those rules-based PCC solutions, such as Geometry-based PCC (G-PCC) or Video-based PCC (V-PCC) standardized under the ISO/IEC MPEG committee~\cite{cao2021compression}, learning-based PCC approaches have attracted worldwide attention and demonstrated noticeable compression gains~\cite{quach2022survey} in Point Cloud Geometry Compression (PCGC). Among them, our earlier multiscale sparse representation-based PCGC has reported state-of-the-art performance~\cite{Wang2021SparseTM,Wang2021MultiscalePC} on a variety of point clouds (e.g., dense object and sparse LiDAR data). This work, furthermore, extends the multiscale approach for lossless Point Cloud Attribute Compression (PCAC). Following the convention~\cite{MPEG_GPCC_CTC}, losslessly compressed geometry is assumed to study the PCAC.


Similar to the geometry occupancy compression, the attribute compression performance also depends on the accuracy of probability estimation~\cite{Wang2021SparseTM}. However, the dynamic range of the Point Cloud Attribute (PCA) is generally much wider than the binary occupancy status (e.g., 1 for occupied and 0 for non-occupied), e.g., as for the color attributes, the intensity of an 8-bit RGB sample is typically ranging from 0 to 255. Thus, a better solution is highly desired to exploit redundancy across attributes for compact representation. With this aim, we leverage the cross-scale, cross-group, and cross-color\footnote{Cross-color prediction is feasible for multi-channel attributes like three-channel RGB/YCoCg colors.} predictions, which thoroughly examine the point correlation to improve the attribute probability approximation in compression.


As shown in Fig.~\ref{fig:multiscale}, {for the encoding process, the original point cloud tensor is downscaled progressively to form multiscale tensors, where the geometry is scaled dyadically and the associated attributes are averagely pooled; while the decoding mirrors the encoding steps to upscale the geometric tensor and unpool the attributes accordingly.}
Upon such multiscale representation, cross-scale prediction can be applied, where decoded lower-scale attributes are leveraged to estimate the probability of current-scale attributes for encoding/decoding. Currently, we enforce the integer attribute at each scale through a simple rounding-based quantization. And random quantization residual is compressed assuming the uniform distribution.

Instead of estimating the attribute probability of all valid points at the current scale in one shot using lower-scale information only, we can optionally approximate the probability from one group to another following a predefined grouping pattern shown in Fig.~\ref{fig:multistage}, by which we best utilize the lower-scale and inter-group correlations (same-scale) jointly.  Parallel processing is applied for intra-group computation.

Furthermore, as there exist inter-color correlations even after the color space conversion~\cite{CCP_HEVC}, we can further reduce the compression bitrate by enforcing the cross-color prediction. Here, we use the YCoCg color space as the standardized G-PCC~\cite{GPCC}.  The compression at a given scale follows the processing order of Y, Co, and Cg, where the processing of Co (Cg) can use the previously-processed Y (Y and Co), as in Fig.~\ref{fig:multichannel}.  Note that for the attribute with a single-channel component, e.g., the reflectance in sparse LiDAR point clouds, we only use cross-scale and cross-group prediction.

The above attribute probability approximation using cross-scale, cross-group, or cross-color prediction is facilitated using Sparse Convolutional Neural Networks (SparseCNNs), which is referred to as the SAPA in Fig.~\ref{fig:network}. The SAPA model inputs valid neighbors to produce associated Laplacian distribution parameters for the probability derivation. The advantages of sparse convolutions can be found in~\cite{choy20194d}.


{\bf Contributions.}
1)  To the best of our knowledge, this work is probably {\it the first} lightweight and generalized lossless point cloud attribute compression approach that outperforms the latest G-PCC, e.g., about 15\%, 12\%, 37\%, 10\%, and 5\% bitrate reduction for respective 8iVFB, Owlii, MVUB, ScanNet and Ford datasets;  2) The outstanding compression performance comes with the elegant utilization of neighborhood correlations through the cross-scale, cross-group, and cross-color prediction for attribute probability estimation; 3) The lightweight computation is due to the use of sparse convolution (to reflect the sparsity of point clouds) and parallel processing inherently supported by our design, demonstrating similar encoding/decoding runtime as the G-PCC codec. 

