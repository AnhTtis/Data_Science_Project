

\section{Experimental Results}\label{sec:exp}


\begin{figure*}[t]
\centering
\includegraphics[width=5.5in]{figs/dataset.pdf}
\caption{{Datasets Used In This Study.}}
\label{fig:dataset}
\end{figure*}

\input{figs/table.tex}


\subsection{Performance Evaluation}

\textbf{Loss Function.}
We measure the total bits of losslessly compressed attributes as the loss metric in learning, e.g., 
%\begin{equation}
    $Loss = \sum_{s=1}^{S}\sum_{c=1}^{3}\sum_{g=1}^{8}\sum_{i=1}^{N^{(s,g,c)}}-\log_2(p(\hat{x}_{i}^{(s,g,c)}))$
%\label{eq:loss}
%\end{equation}
where $s, c$, and $g$ are the scale, color channel, and group index; $N^{(s,g,c)}$ is the number of POVs for color component $c$ in the group $g$ at the scale $s$; and $p$ is calculated using \eqref{eq:entropy}.
\vspace{0.1cm}

{\noindent\bf Datasets.} Extensive experiments are conducted using different datasets shown in Fig.~\ref{fig:dataset} to evaluate the compression efficiency of our method. These datasets include object and scene point clouds from sources with very different geometry and attribute characteristics.

{\it HumanBodies.} 
To measure the compression efficiency on real-life object point clouds, we consider point clouds widely used in standardization committees, including 4 sequences from 8i Voxelized Full Bodies (8iVFB)~\cite{8iVFB}, 4  from Owlii dynamic human mesh (Owlii)~\cite{xu2017owlii}, and 5  from Microsoft Voxelized Upper Bodies (MVUB)~\cite{microsoft2019microsoft}. %As shown . these datasets have diverse geometry precisions and texture characteristics. 
% For \edit{cross-validation}, 
We select 7 of them for training and others for testing, as suggested in~\cite{sheng2021deep,fang20223dac}. The training dataset includes \textit{longdress} \&  \textit{soldier} from 8iVFB, \textit{model} \&  \textit{exercises} from Owlii, and \textit{Andrew}, \textit{David} \&  \textit{Sarah} from MVUB, which are partitioned into 6000 patches for training. Limited by the GPU memory, each patch contains less than 100,000 points. 
The testing dataset includes \textit{loot} \& \textit{redandblack} from 8iVFB, \textit{basketball\_player} \&  \textit{dancer} from Owlii, and \textit{Phil} \&  \textit{Ricardo} from MVUB. All testing samples are prohibited in training.

{\it ScanNet}~\cite{Dai2017ScanNetR3}.
This is a large-scale indoor point cloud dataset totally containing more than 1600 scans, which is widely used in 3D scene understanding tasks. Following common preprocessing methods, raw point clouds are quantized to 2cm and 5cm, respectively. We use 1503 scans for training and 100 scans for the test as suggested.

{\it Ford}~\cite{MPEG_GPCC_CTC}.
Ford is the official outdoor LiDAR dataset used in MPEG, which contains 3 1500-frame sequences. Following the MPEG common test condition, we use the first sequence for training and the other two for testing. Experiments are examined using both original 1mm-precision and quantized 2cm-precision samples.
\vspace{0.1cm}

{\noindent\bf Quantitative Evaluation.} Our test results are shown in Table~\ref{table:main}.
% anchor
The latest G-PCC version TMC13v14, which provides state-of-the-art performance of lossless PCAC through well-engineered attribute transform~\cite{GPCC}, serves as the anchor. We strictly follow the common test conditions~\cite{MPEG_GPCC_CTC} to generate the anchor results.
%Please note that on Ford\_q1mm, we disable the angular mode which requires additional LiDAR device information for a fair comparison.

As seen, the proposed method offers significant gains against the G-PCC anchor across a variety of datasets:  For object point clouds, we provide 15.2\%, 12.5\%, and 36.7\% bitrate reduction to the G-PCC for 8iVFB, Owlii, and MVUB, respectively; while for indoor/outdoor scene point clouds, we achieve 13.2\% and 9.7\% gains on ScanNet with 5cm and 2cm precision, and 6.0\% and 4.7\% gains on Ford with 2cm and 1mm precision, respectively.

Extensive results suggest that our proposed method generalizes well from dense object point clouds to very sparse LiDAR data, which mainly owes to the joint utilization of cross-scale, cross-group, and cross-color prediction to exploit neighborhood correlations exhaustively. Similar to~\cite{Wang2021SparseTM}, we train the model using two consecutive scales and enforce the model sharing across all scales.

As for the computational complexity measured by the encoding/decoding runtime (seconds per frame or s/frame), our method and the G-PCC share the same order of magnitude when our model is tested on an RTX 3090 GPU, which is attractive for practical applications. Note that these numbers are served as the reference for intuitive understanding because G-PCC and our method run on different platforms (CPU vs. GPU and C++ vs. Python). 

Compared with the well-developed G-PCC, our method only provides a basic learning-based framework that needs further improvement and optimization, especially on sparse point clouds like LiDAR data. 
For example, some advanced technologies in G-PCC~\cite{GPCC} like direct coding mode and angular mode can be borrowed to improve the learned method. 


\subsection{Discussion}
\textbf{Modular Impact.} 
We exemplify the performance-complexity tradeoff by examining the impact of  cross-scale, cross-group, and cross-color prediction modules.
As shown in Table~\ref{table:main}, on 8iVFB, the coding gain over the G-PCC increases greatly from the baseline using only cross-scale (CS) prediction with 4.8\% loss, to the case using both cross-scale and cross-group (CG) predictions with 14.7\% compression gain. When the cross-color (CC) prediction is further included, the bitrate reduction  is improved to 15.2\%.  The complexity also increases when involving more prediction modules. In practice, we can carefully choose modules according to the complexity requirement.
\vspace{0.1cm}


{\noindent\bf  Model Generalization.} 
To evidence the generalization of the proposed method, 
we generate a large number of synthetic samples for training~\cite{Wang2022SparsePCAC}. We first densely sample points from raw ShapeNet~\cite{chang2015shapenet} meshes and quantized the coordinates to 8-bit integers. Next, we randomly select images from COCO dataset~\cite{microsoft2014coco} and map them to the points as color attributes through projection, as shown in Fig.~\ref{fig:dataset}. This synthetic dataset provides 6000 object point clouds with different shapes and textures for the training.  
Then we use the models trained on synthesized colorized ShapeNet to test the real-captured object point clouds.   Although the bitrate reduction declines because of very diverse characteristics between training and testing data when comparing with other results in Table~\ref{table:main},  we still achieve averaged 5.2\%, 4.0\%, and  8.8\% positive gain over the G-PCC on 8iVFB, Owlii, and MVUB, respectively,  confidently demonstrating the effectiveness and generalization of our method.