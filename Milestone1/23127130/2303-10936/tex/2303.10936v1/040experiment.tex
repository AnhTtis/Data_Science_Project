\subsection{Implementation details}

We use the Matterport3D \cite{chang2017matterport3d} dataset with Habitat simulator \cite{savva2019habitat} in our main experiments. The scenes in the Matterport3D dataset are 3D reconstructions of real-world environments, split into a training set (54 scenes) and a test set (10 scenes). We assume that the perfect agent pose and depth image can be obtained in our setup. 

The exploration policy consists of convolutional layers followed by fully connected layers. 
%In addition to the semantic map, we also input the agent orientation to the policy.
The pre-trained Mask RCNN is frozen while training the exploration policy. We use the PPO with a time horizon of 20 steps, 8 mini-batches, and 4 epochs in each PPO update to train the policy. The reward, entropy, and value loss coefficients are set to 0.02, 0.001, and 0.5, respectively. We use Adam optimizer with a learning rate of $2.5 \times 10^{-5}$. The maximum number of steps in each episode is 500. The $\lambda$ and $\delta$ are experimentally set to 0.3 and 0.1, respectively. To fairly compare with previous methods, we set the number of training steps to 500k in all experiments. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{table*}[h]
%\footnotesize
%\large
\caption{Comparison with the state-of-the-art methods for object detection (Bbox) and instance segmentation (Segm) using AP50 as the metric. n means the exploration policy is progressively trained for n times.}
\label{tab:sota}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|l|cccccc|c}
\hline
{Task}&{Method}&{Chair}&{Couch}&{Potted Plant}&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{}&{\color{red}Pre-trained}&{21.05}&{25.23}&{22.58}&{24.24}&{22.62}&{29.67}&{24.23}\\
{}&{\color{red}Re-trained}&{23.77}&{27.36}&{26.20}&{25.21}&{24.82}&{34.48}&{26.97}\\
{}&{Random}&{29.98}&{31.65}&{23.91}&{28.66}&{31.78}&{40.44}&{31.07}\\
{Bbox}&{Active Neural SLAM \cite{chaplot2020learning}}&{32.02}&{32.74}&{31.94}&{30.31}&{26.30}&{38.68}&{32.00}\\
{}&{Semantic Curiosity \cite{chaplot2020semantic}}&{33.51}&{33.11}&{32.91}&{29.57}&{25.76}&{39.97}&{32.46}\\
{}&{Ours (n=1)}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{\bf{34.07}}\\
{}&{\color{red}Ours (n=3)}&{33.34}&{34.48}&{35.28}&{32.12}&{31.87}&{43.11}&{\color{red}\bf{35.03}}\\
\hline
{}&{\color{red}Pre-trained}&{12.72}&{22.98}&{16.71}&{23.82}&{23.85}&{29.75}&{21.64}\\
{}&{\color{red}Re-trained}&{14.99}&{24.68}&{18.36}&{24.32}&{25.15}&{34.23}&{23.62}\\
{}&{Random}&{18.22}&{27.25}&{8.82}&{28.19}&{29.08}&{39.39}&{25.16}\\
{Segm}&{Active Neural SLAM \cite{chaplot2020learning}}&{17.89}&{29.24}&{15.22}&{29.66}&{27.29}&{38.61}&{26.32}\\
{}&{Semantic Curiosity \cite{chaplot2020semantic}}&{18.18}&{30.06}&{18.39 }&{29.03}&{26.70}&{40.01}&{27.06}\\
{}&{Ours (n=1)}&{19.18}&{30.14}&{15.56}&{31.03}&{28.19}&{43.43}&{\bf{27.92}}\\
{}&{\color{red}Ours (n=3)}&{19.28}&{30.13}&{16.22}&{31.27}&{28.92}&{44.76}&{\bf{\color{red}28.42}}\\
\hline
\end{tabular}
}
\end{table*}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We pre-train a Mask-RCNN model with FPN \cite{lin2017feature} using ResNet-50 as the backbone on the COCO \cite{lin2014microsoft} dataset labeled with 6 overlapping categories with the Matterport3D, i.e., ‘chair’, ‘couch’, ‘potted plant’, ‘bed’, ‘toilet’ and ‘tv’. Then we fine-tune this model on the gathered samples with a fixed learning rate of 0.001. All other hyper-parameters are set to default settings in Detectron2 \cite{wu2019detectron2}. We randomly collect the samples in test scenes of different episodes to evaluate the final perception model. The AP50 score is adopted as the evaluation metric, which is the average precision with at least $50\%$ IOU. 

We further deploy our method to a real robot. Our robot is equipped with a Kinect V2 camera, a 2D LiDAR, and an onboard computer (with an Intel i5- 7500T CPU and an NVIDIA GeForce GTX 1060 GPU). Note that the LiDAR is only used with wheel odometers to perform localization. We test our method in a built 60$m^{2}$ house with a dining room, a living room, and a bedroom. 
%In real robots, we use the sensor fusion (i.e., 2D LiDAR and wheel odometer) to reduce the self-positioning errors.

%We also show that our method could also guide the real robot to find informative trajectories and samples. 

% 两类无法区分：RPN：0.2, 0.3, 0.4, 0.5 thres: 0.1, 0.2, 0.3 
\newcolumntype{Z}{p{1.1cm}<{\centering}}
\newcolumntype{W}{p{1.5cm}<{\centering}}
\begin{table*}[t]
\centering
\caption{Effects of setting different thresholds in hard sample selection on object detection task. \label{tab:sam}}
\begin{tabularx}{\linewidth}{X<{\centering}|X<{\centering}|ZZWZZZ|W}
\toprule
{Method}&{Training Image}&{Chair}&{Couch}&{Potted Plant}&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{$\delta$ = 0.1}&{20k}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{34.07}\\
{$\delta$ = 0.2}&{13k}&{33.38}&{34.61}&{31.34}&{31.84}&{26.24}&{41.64}&{33.18}\\
{$\delta$ = 0.3}&{9k}&{32.79}&{35.03}&{31.10}&{31.81}&{22.83}&{41.11}&{32.44}\\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{Main Results}
\subsubsection{Simulation Environment}
To demonstrate the effectiveness of our method, we compare our fine-tuned object detection and instance segmentation results with the state-of-the-art methods as shown in Tab.~\ref{tab:sota}. Note that these methods all use around 20k training images. Pre-trained means the perception model was pre-trained on the raw COCO dataset. Re-trained means we re-train the pre-trained model utilizing COCO dataset labeled with 6 overlapping categories with the Matterport3D. Random is a baseline exploration policy that samples actions randomly. It can be seen that our model achieves the best performance and can further improve the performance when progressively training the exploration strategy three times based on the latest fine-tuned perception model. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{table}[h]
\caption{\color{red}Ablation studies on the object detection task. SDD and CDU means the semantic distribution disagreement reward and the class distribution uncertainty reward in trajectory exploration, respectively. HSM means the hard sample mining. SC means the Semantic Curiosity \cite{chaplot2020semantic}.}
\label{tab:abl}
%\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2.8mm}{
%\centering
\begin{tabular}{l|cccccc|c}
\hline
{Method}&{Chair}&{Couch}&{Potted Plant}&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{\color{red}Ours w/o SDD}&{32.08}&{34.51}&{32.05}&{29.91}&{27.95}&{42.76}&{33.21} \\
{Ours w/o CDU}&{33.49}&{34.39}&{32.95}&{31.19}&{27.31}&{42.18}&{33.59}\\
{Ours w/o HSM}&{32.44}&{33.69}&{32.22}&{30.85}&{27.67}&{41.78}&{33.11}\\
{SC + HSM}&{33.87}&{33.52}&{32.69}&{31.08}&{27.93}&{41.08}&{33.36}\\
{Ours}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{\bf{34.07}}\\
\hline
\end{tabular}
}
\end{table}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcolumntype{Z}{p{0.4cm}<{\centering}}
\newcolumntype{W}{p{0.7cm}<{\centering}}
\newcolumntype{M}{p{0.5cm}<{\centering}}
\begin{table}[t]
\centering
\caption{Ablation studies on the object detection task. SDD and SDU means the semantic distribution disagreement reward and the semantic distribution uncertainty reward in trajectory exploration, respectively. HSS means the hard sample selection. SC means the Semantic Curiosity \cite{chaplot2020semantic}. \label{tab:abl}}
\begin{tabularx}{\linewidth}{X<{\centering}|ZZZZZM|W}
\toprule
{Method}&{Chair}&{Couch}&{Potted Plant}
&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{Ours w/o SDD}&{32.08}&{34.51}&{32.05}&{29.91}&{27.95}&{42.76}&{33.21} \\
{Ours w/o SDU}&{33.49}&{34.39}&{32.95}&{31.19}&{27.31}&{42.18}&{33.59}\\
{Ours w/o HSS}&{32.44}&{33.69}&{32.22}&{30.85}&{27.67}&{41.78}&{33.11}\\
{SC + HSS}&{33.87}&{33.52}&{32.69}&{31.08}&{27.93}&{41.08}&{33.36}\\
{Ours}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{\bf{34.07}}\\
\bottomrule
\end{tabularx}
\end{table}

Specifically, compared with the pre-trained model, our fine-tuned model gives 10.80$\%$ AP50 gains on the box detection metric.
Compared with the previous best competitor Semantic Curiosity \cite{chaplot2020semantic} which rewards trajectories with inconsistent labeling behavior and encourages the embodied agent to explore such areas, our model significantly outperforms it by 2.57$\%$ absolute AP50 point on object box detection and 1.36$\%$ on instance segmentation. The improved performances over the best competitor indicate that our proposed informative trajectory exploration and hard sample selection method is very effective for this task. 
%Compared with the Active Neural SLAM \cite{chaplot2020learning} which aims to maximize the total explored areas, our model achieves much better performances by 2.07$\%$, demonstrating the effectiveness of our data gathering policy. 
In addition, we can see that our method is more friendly to instances with simple shapes, e.g., Bed and Tv. These instance's shapes are easier to be reconstructed through 3D mapping. Objects with much more complicated shapes, e.g., Potted Plant, are more likely to involve mapping errors, which in turn decreases the performance of instance segmentation.

\subsubsection{Real Robot}
We also deploy our learned exploration 
policy on a real robot to explore informative trajectories and hard samples in an unseen environment. In practice, we gather 170 hard samples for fine-tuning the pre-trained model and an additional 50 randomly collected samples for validation, with an average of 4 objects in each image. Benefiting from the gathered informative images, the fine-tuned perception 
model can improve the detection and segmentation performances from 79.1\% AP50 and 76.7\% AP50 to 97.3\% AP50 and 96.1\% AP50, respectively. 


\subsection{Ablation Analysis}

Our method comprises two modules: informative trajectory exploration and hard sample selection. To investigate these two components, we perform a set of ablation studies with $n$ = 1 for simplicity, as shown in Tab.~\ref{tab:abl}. 


We first investigate the importance of rewarding semantic distribution disagreement across viewpoints and semantic distribution uncertainty to explore the trajectory. 
It can be seen that the AP50 accuracy on object detection drops 0.71$\%$ (SC+HSS vs. Ours) by replacing our exploration policy as SC.
The exploration module proves the effectiveness of learning informative trajectories for subsequent sample selection. Then we investigate the importance of semantic distribution uncertainty based hard sample selection by removing it (Ours w/o HSS).
The AP50 accuracy on object detection drops 0.96$\%$, demonstrating that selecting hard samples enhances the perception results. In addition, by comparing the results between Ours and Ours w/o SDD, Ours and Ours w/o SDU (semantic distribution disagreement and uncertainty in informative trajectory exploration), we can find that utilizing SDD and SDU can generate more effective trajectories. 

%Experimental results show that 'Ours w/o CPU' model selects more Couch and Potted Plant categories to be labeled (16470 VS 15519; 11246 VS 10743), indicating that these two categories of objects have prediction uncertainty at the same time when having prediction disagreement. 


\newcolumntype{Z}{p{0.5cm}<{\centering}}
\newcolumntype{W}{p{0.8cm}<{\centering}}
\begin{table}[t]
\centering
\caption{Effects of progressively training the exploration policy for $n$ times on the object detection task. \label{tab:multi}}
\begin{tabularx}{\linewidth}{X<{\centering}|ZZZZZZ|W}
\toprule
{Method}&{Chair}&{Couch}&{Potted Plant}&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{n = 1}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{34.07}\\
{n = 2}&{32.18}&{33.32}&{36.06}&{31.38}&{30.81}&{44.53}&{34.71}\\
{n = 3}&{33.34}&{34.48}&{35.28}&{32.12}&{31.87}&{43.11}&{\bf{35.03}}\\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/traj_size.pdf}
\caption{Qualitative examples of learned trajectories and sampled images from the Matterport3D environment and the real robot. The first row shows the explored informative trajectories trained by semantic distribution disagreement and uncertainty rewards. The second row shows the gathered hard images by semantic distribution uncertainty estimation.}
\label{fig:trajectory}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/seg.pdf}
\caption{Qualitative examples of instance segmentation by different models.}
\label{fig:seg}
\end{figure}

We compare the effectiveness of setting different thresholds $\delta$ in hard sample selection as shown in Tab.~\ref{tab:sam}. In this experiment, we sample the images from explored trajectories with 6 episodes and fixed steps in each training scene, resulting in different numbers of sampled training images at different thresholds. We can find that decent performances can be achieved by training very few hard samples, which demonstrates the effectiveness of selecting hard samples. Tab.~\ref{tab:multi} shows the experimental results when progressively training the exploration policy multiple times based on the latest fine-tuned perception model. Note that they all use 20k training images.
%Considering the simplicity, we adopt $n=1$ in this paper. 

To exploit measures of uncertainty in semantic distributions, we utilize the entropy of categorical distribution (ECS) in place of the heuristic in Eq.~\ref{4} as shown in Tab.~\ref{tab:uncetain}. We experimentally set the threshold of entropy to 0.4. The improved performance indicates that the uncertainty between all distributions is more effective than between the two categories. 


\newcolumntype{Z}{p{0.5cm}<{\centering}}
\newcolumntype{W}{p{0.8cm}<{\centering}}
\begin{table}[t]
\caption{Effects of utilizing the entropy of categorical distribution (ECS) in place of the heuristic in Eq.~\ref{4} to measure semantic distribution uncertainty on the object detection task. \label{tab:uncetain}}
\begin{tabularx}{\linewidth}{X<{\centering}|ZZZZZZ|W}
\toprule
{Method}&{Chair}&{Couch}&{Potted Plant}&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{Ours}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{34.07}\\
{ECS}&{31.40}&{32.70}&{34.23}&{30.90}&{30.75}&{46.06}&{\bf{34.34}}\\
\bottomrule
\end{tabularx}
\end{table}



\subsection{Qualitative Results}

To verify whether the proposed exploration policy and hard sample selection method can obtain the observations with inconsistent or uncertain semantic distributions, we visualize the explored trajectories and sampled images from the Matterport3D dataset and real-world environment, as shown in Fig.~\ref{fig:trajectory}. We can see that our model is able to gather inconsistent and uncertain detections via semantic distribution disagreement and uncertainty estimation. 
For example, the couch is detected as different objects (chair/couch) or distributions from different viewpoints on the first row. Besides, the couch is detected as couch and chair with almost close scores on the second row. By collecting these observations that are poorly identified by the pre-trained perception model for labeling, the model can be fine-tuned better. 

Fig.~\ref{fig:seg} shows the segmentation masks obtained by three different models, i.e., pre-trained, Semantic Curiosity \cite{chaplot2020semantic}, and our \method, demonstrating our proposed method's benefits. As the figure shows, our generated segmentation masks have more obvious object shapes and finer outlines in the first column. Besides, our model, fine-tuned exclusively on hard samples, can detect the missed objects by the pre-trained and Semantic Curiosity \cite{chaplot2020semantic} models, as shown in the third and fifth columns. 