Pre-training on large-scale datasets to build reusable models has drawn great attention in recent years, e.g., the deep visual models \cite{he2016deep} pre-trained on ImageNet \cite{krizhevsky2012imagenet} can be reused for detection \cite{he2017mask,wang2020solo}, and pre-trained language model like BERT \cite{devlin2018bert} can be used for image-text retrieval \cite{chen2020uniter}. To better adapt to downstream tasks, many researchers focus on fine-tuning models on small-scale task-related datasets \cite{sun2019fine,wang2021list}. 
However, generalizing the perception model pre-trained on large-scale internet images to embodied tasks is insufficiently studied, which will help various relevant applications (e.g., home robots). In order to use as few annotations as possible, efficiently collecting training data in embodied scenes becomes the main challenge.

%Since the training images are collected from the Internet, these pre-trained models can successfully classify and segment objects in Internet images. 
%To generalize these pre-trained models to human-like robotic agent more efficiently instead of training from scratch, we need some domain knowledge about the new environment. Advances in active visual learning \cite{misra2018learning,sener2017active} have focused on efficient data collection techniques that can be used to well address this problem. 

Different from visual learning based on \textit{static} data (e.g., images), the embodied agent can \textit{move} around and interact with 3D environment. Therefore, efficiently collecting training samples means learning an exploration policy to encourage the agent to explore the areas where the pre-trained model performs poorly. Since the ground-truth labels in scenes are unavailable, the underlying spatial-temporal continuity in the 3D world can be used self-supervised. To use the consistency in semantic predictions, the previous method \cite{chaplot2020semantic} proposes a semantic curiosity policy, which explores inconsistent labeling of the same object by the perception model. When an exploration trajectory is learned, all observations on this trajectory are collected for labeling to fine-tune the pre-trained perception model. 
Despite the advance, this method utilizes a fuzzy inconsistency estimation (i.e., projecting multiple objects at different heights to the same location in a 2D map). 
%the uncertainty of the predicted semantic distribution that reflects the recognition performance of the pre-trained perception model in the new environment 
In addition, the uncertainty of the predicted semantic distribution that reflects what the pre-trained perception model does not know in the new environment and the hard sample selection on the trajectory are ignored in \cite{chaplot2020semantic}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/illu.pdf}
\caption{The illustration of semantic distribution disagreement, e.g., the bed is recognized to different objects/distributions across three viewpoints ($v1, v2, v3$), and semantic distribution uncertainty, e.g., the probability of couch in observation $o1$  being predicted as bed and couch is relatively close. bg means background.}
\label{fig:illu}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.92\linewidth]{fig/main_model.pdf}
\caption{The architecture of our proposed informative trajectory and sample exploration method. It contains two steps: the exploration policy aims to encourage the agent to explore the objects with semantic distribution disagreement or uncertainty, then the training stage aims at gathering hard samples on trajectories based on semantic distribution uncertainty to fine-tune the pre-trained model.}
\label{fig:main_model}
\end{figure*}

To solve these problems, we propose learning to Explore Informative Trajectories and Samples (\method) for embodied perception, as shown in Fig.~\ref{fig:main_model}. It consists of two steps: learning exploration policy and collecting hard samples to fine-tune the perception model.
During the exploration, our agent moves around and collects multi-view observations fused by Exponential Moving Average to generate a 3D semantic map, which weakens the impact of misidentification from an unfamiliar viewpoint.
The generated 3D map can be regarded as the pseudo ground truth due to the fusion of predicted results from different viewpoints. Unlike previous work \cite{chaplot2020semantic} that adopts predicted labels to build the semantic map, our work builds a predicted probabilistic distribution map, i.e., a 3D semantic distribution map. 
It can be used to constrain not only the predicted labels but also the predicted distributions, 
%to reduce the uncertainty of predictions, 
as shown in Fig.~\ref{fig:illu} (left).
Then one curiosity reward is measured by the semantic distribution disagreement between the semantic prediction of the current perspective and the generated 3D semantic distribution map. We also measure the uncertainty of the predicted semantic distribution as shown in Fig.~\ref{fig:illu} (right) as another curiosity reward. These two rewards are used together to learn the exploration policy by maximizing the disagreement and uncertainty of semantic predictions. Therefore, our agent can move to the areas where the 
semantic predictions are different from pseudo ground truth or the
probabilities being predicted as two categories are relatively close. 
%The probabilities of being predicted as two classes are relatively close.
%confidences belonging to two categories are relatively close.. 
After obtaining the exploration strategy, we gather indistinguishable hard samples on each trajectory for the subsequent training. The selection method is the same as the uncertainty of semantic distribution. 
The gathered data is labeled and utilized to fine-tune the pre-trained perception model, making it generalized well to new environments. We also show that the perception model could be sustainably improved by our explore-finetune process back and forth.
The method is evaluated on a challenging Matterport dataset \cite{chang2017matterport3d} and our real-robot environment. Experimental results show that our \method approach outperforms the state-of-the-art methods.

The contributions of this paper can be summarized as threefold. (1) We propose a novel informative trajectory exploration method for embodied perception by measuring the semantic distribution disagreement across viewpoints and the uncertainty of each observation. In addition, we are probably the first to exploit the uncertainty over semantic predictions to handle this task. 
% measured by 3D semantic distribution map
(2) The hard samples on the trajectory selected by the semantic distribution uncertainty further sieves the observations recognized well by the pre-trained model, which can enhance the performance better when fine-tuning the pre-trained model. (3) The proposed method achieves the best result on the challenging Matterport dataset. 
%Extensive ablation studies verify the effectiveness of each component in the \method. 
