%In this section, we briefly introduce the related work about prior studies on active perception and learning, semantic mapping, as well as some embodied tasks. 

\subsection{Robot Perception Learning}
Visual perception is a crucial function of a robot. Some works \cite{chaplot2020object,zhu2022navigating} directly utilize the perception model pre-trained on COCO \cite{lin2014microsoft} images to perform object goal navigation. To improve the performance in embodied tasks, some researchers \cite{jayaraman2016look,yang2019embodied,yeung2016end} focus on learning a policy to directly improve metrics of interest end-to-end at test time by obtaining information about the environment. Unlike them, we aim to explore informative samples self-supervised to better fine-tune the pre-trained perception model. 
%Active learning \cite{yang2018visual,pathak2018learning,jayaraman2018end} also aims to actively select a subset of unlabeled images for labeling to improve its model, but it is based on pre-collected datasets. 
%It has wide applications, e.g., object detection \cite{jayaraman2018end,vijayanarasimhan2014large,yang2018visual}, instance segmentation \cite{pathak2018learning} and medical image analysis \cite{kuo2018cost}
The exploration in reinforcement learning \cite{eysenbach2018diversity,pathak2017curiosity,pathak2019self} also aims to maximize an intrinsic reward function to encourage the agent to seek previously unseen or poorly understood parts of the environment. 
%In addition, Pathak et al. \cite{pathak2019self} measure the consistency among multiple models. 
Different from them, we compute the reward function by multi-view consistency in semantics. 
The active interactive imitation learning \cite{hoque2021thriftydagger, menda2019ensembledagger, hoque2021lazydagger} are also related to our work in disagreement and uncertainty measuring to decide whether to request a label from the human. However, our agent does not require human intervention when learning the exploration policy, and the exploration purpose is to improve the perception model. 
Recently, Chaplot et al. \cite{chaplot2020semantic} measure the semantic curiosity to learn the exploration policy for embodied perception. But they ignore the uncertainty over semantic predictions and hard sample selection on the learned trajectory. Besides, some works \cite{chaplot2021seal, fang2020move} attempt to learn both exploration and perception utilizing pseudo labels in a completely self-supervised manner without requiring any extra labels. In this paper, we propose effectively generalizing the pre-trained perception model to embodied tasks, where informative trajectories and samples are gathered by utilizing a 3D semantic distribution map to measure the semantic distribution disagreement and the semantic distribution uncertainty. Then the gathered data is labeled to fine-tune the perception model.

\subsection{Semantic Mapping}
3D mapping aiming to reconstruct a dense map of the environment has achieved great advances in recent years. Fuentes-Pacheco et al. \cite{fuentes2015visual} do a very detailed survey. Researchers also consider adding semantic information to the 3D map \cite{chaplot2021seal}. Similar to them, we adopt the same setting and learn 3D semantic mapping by differentiable projection operations. In this paper, we propose a 3D semantic distribution map, which is used to learn the exploration policy.
% constrain the semantic distributions to learn exploration policy. 

\subsection{Embodied Task}
Embodied agents can move around and interact with the surrounding environment. Many environments are photo-realistic reconstructions of indoor \cite{chang2017matterport3d,xia2018gibson} and outdoor \cite{dosovitskiy2017carla,geiger2013vision} scenes, where the ground-truth labels for objects are also provided. Recently, many researchers have used these simulated environments in visual navigation \cite{chaplot2020object,gupta2017cognitive}, visual question answering \cite{das2018embodied} and visual exploration \cite{chaplot2020learning}. Visual navigation usually involves point/object goal navigation \cite{chaplot2020object} and vision-and-language navigation \cite{anderson2018vision} where the path to the goal is described in natural language. 
Visual question answering \cite{das2018embodied} should intelligently navigate to explore the environment, gather necessary visual information, and then answer the question. 
% Recent works explicitly train an end-to-end RL policy to maximize the explored area in visual exploration \cite{chaplot2020learning}. 
Unlike them, our agent aims to gather data for labeling to generalize the pre-trained perception model to unseen environments efficiently. 