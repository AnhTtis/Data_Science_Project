

We aim to train an embodied agent with a perception model pre-trained on internet images to explore informative trajectories and samples effectively. Then the perception model fine-tuned on the gathered data can generalize well to a new environment. As shown in Fig.~\ref{fig:main_model}, our proposed method consists of two main parts. The exploration part aims to learn the active movement of an agent to obtain informative trajectories via semantic distribution disagreement and semantic distribution uncertainty self-supervised. %utilizing a 3D semantic distribution map. 
Then we take advantage of the semantic distribution uncertainty to collect hard samples on the learned trajectory. After images are collected and semantically labeled, we fine-tune the perception model on these images.
%at the perception learning stage. 

\subsection{3D Semantic Distribution Mapping}

Note that for each time step $t$, our agent's observation space consists of an RGB observation $I_{t} \in \mathbb R^{3 \times W_{I} \times H_{I}}$, a depth observation $D_{t} \in \mathbb R^{W_{I} \times H_{I}}$, and a 3-DOF pose sensor $x_{t} \in \mathbb R^{3}$ which denotes the $x$-$y$ coordinates and the orientation of the agent. The agent has three discrete actions: \texttt{move forward}, \texttt{turn left} and \texttt{turn right}. 

The easiest way to associate semantic predictions across frames on a trajectory is to project the predictions on the top-down view to build a 2D semantic map as \cite{chaplot2020semantic}. 
However, due to the embodied agent moving in a 3D environment, the height information is lost when projecting the predictions onto a 2D map. These will result in projecting multiple objects at different heights to the same location, e.g., if a potted plant is on the table, the potted plant and table will be projected to the same location. Therefore, the noise will be generated when calculating the disagreement across different viewpoints. 
In this paper, we utilize the 3D semantic distribution map to measure the semantic distribution disagreement.
%, which is a voxel-based representation. 
The semantic map $M$ is a 4D tensor of size $K \times L_{M} \times W_{M} \times H_{M}$, where $L_{M}$, $W_{M}$, $H_{M}$, denote the 3 spatial dimensions, and $K = C + 2$, where $C$ is the total number of semantic object categories. The first two channels in $K$ represent whether the corresponding voxel (x-y-z location) contains obstacles and is the explored area, respectively. The other channels denote the predicted semantic probability distribution among $C$ categories from the pre-trained perception model.
%The $P$ denotes the semantic probability distribution among $C$ categories from pre-trained perception model. 
The map is initialized with all zeros at the beginning of an episode, $M_0 = [0]^{K \times L_{M} \times W_{M} \times H_{M}}$. The agent always starts at the center of the map facing east at the beginning of the episode, $x_0 = (L_{M}/2, W_{M}/2, 0.0)$ same as \cite{chaplot2020object}. 

Fig.~\ref{fig:main_model} shows the 3D semantic mapping procedure at a time step. The agent takes action and then sees a new observation $I_{t}$. The pre-trained perception model (e.g., Mask RCNN \cite{he2017mask}) is adopted to predict the semantic categories of the objects seen in $I_{t}$, where the semantic prediction is a probability distribution among $C$ categories for each pixel. The depth observation $D_{t}$ is used to compute the point cloud. Each point in the point cloud is associated with the corresponding semantic prediction, which is then converted into 3D space using differentiable geometric transformations based on the agent pose to get the voxel representation. This voxel representation in the same location is aggregated over time using Exponential Moving Average to get the 3D semantic distribution map:
%\begin{equation}
   %M_{t} = M_{t-1}, t=1
%\end{equation}
%\begin{equation}
   %M_{t} = \lambda * M_{t-1} + (1-\lambda) * m_{t}, t>1
%\end{equation}
\begin{equation}
\begin{split}
   M_{t} = \left \{
\begin{array}{ll}
   M_{t-1}, & t=1 \\
   \lambda * M_{t-1} + (1-\lambda) * m_{t}, & t>1 \\
\end{array}
\right.
\end{split}
\end{equation}
where $m_{t}$ means the voxel representation at time step $t$ and $\lambda$ aims to control the relative importance of $M_{t-1}$ and $m_{t}$. The map can integrate the predicted semantics of the same object from different viewpoints to alleviate the misrecognition caused by the unfamiliar viewpoint. Therefore, the map representation can be used as pseudo ground truth labels of objects in the scene.  


\subsection{Exploring Informative Trajectory}

The goal of exploration policy $a_{t} = \pi (I_{t}, \theta)$ is exploring objects that are poorly identified by the current perception model based on the observation $I_{t}$, where $a_{t}$ means the action and $\theta$ represents the parameters of the policy model. 
Hence, we can collect valuable observations in the explored areas to fine-tune the perception model. We propose two novel distribution-based rewards to train the exploration policy by maximizing the disagreement and uncertainty during moving.

The semantic distribution disagreement reward is defined as the Kullback-Leibler divergence between the current prediction and the 3D semantic distribution map, which encourages the agent to explore the objects with different semantic distributions across viewpoints:
%not only the new objects but also the objects with different predictions across viewpoints:
\begin{equation}
   r_d = KL(m_{t}, M_{t-1}).
\end{equation}
Unlike semantic curiosity \cite{chaplot2020semantic} which maximizes the label inconsistency based on the 2D semantic map, our semantic distribution disagreement aims to explore the objects with different distributions from the 3D semantic distribution map. 

In addition, we propose a semantic distribution uncertainty reward $r_u$ to explore the objects whose predicted probabilities belonging to two categories are relatively close, as Eq.~\ref{4} explains. 
\begin{equation}
\begin{split}
   r_u = \left \{
\begin{array}{ll}
   1, & u>\delta \\
   0, & u<\delta \\
\end{array}
\right.
\end{split}
\end{equation}
To train the policy, we first input the semantic map to a global exploration policy to select a long-term goal (i.e., an x-y coordinate of the map). Then a deterministic Fast Marching Method \cite{sethian1996fast} is used for path planning, which uses low-level navigation actions to achieve the goal. We sample the long-term goal every 25 local steps, same as \cite{chaplot2020object} to reduce the time horizon for exploration in reinforcement learning. The Proximal Policy Optimization (PPO) %\cite{schulman2017proximal} 
is used to train the policy. 

\subsection{Efficient Sample Selection and Continue Training}

After obtaining the trajectory, the easiest way is to label all observations on the trajectory. Although the trained exploration policy can find more objects with inconsistent and uncertain predictions, there are still many observations that the pre-trained model can accurately identify. To efficiently fine-tune the perception model, we propose a sample selection method by measuring the uncertainty $u$ of the semantic distribution:
%The uncertainty estimation $u$ is computed as:
\begin{equation}
\label{4}
   u = Second_{max}(P_{i}),
\end{equation}
where $P_{i} \in {\mathbb R} ^{C}$ is the predicted class probability of $i$th object in a single image, the $Second_{max}$ means the second largest score in $\{ p_{i}^{0}, p_{i}^{1}, ..., p_{i}^{C-1} \}$. If $u$ is larger than threshold $\delta$, we select the corresponding image. Considering that the semantic distribution disagreement relies heavily on multi-view observations in the trajectory will reduce the efficiency of selection, and thus it is not utilized to select hard samples. 

We label the selected images and use them to fine-tune the perception model. 

\newcolumntype{Z}{p{1.2cm}<{\centering}}
\newcolumntype{W}{p{1.5cm}<{\centering}}
\begin{table*}[t]
\centering
\caption{Comparison with the state-of-the-art methods for object detection (Bbox) and instance segmentation (Segm) using AP50 as the metric. n means the exploration policy is progressively trained for n times. \label{tab:sota}}
\begin{tabularx}{\linewidth}{Z|X|ZZWZZZ|Z}
\toprule
{Task}&{Method}&{Chair}&{Couch}&{Potted Plant}&{Bed}&{Toilet}&{Tv}&{Average} \\
\hline
{}&{Pre-trained}&{21.05}&{25.23}&{22.58}&{24.24}&{22.62}&{29.67}&{24.23}\\
{}&{Re-trained}&{23.77}&{27.36}&{26.20}&{25.21}&{24.82}&{34.48}&{26.97}\\
{}&{Random}&{29.98}&{31.65}&{23.91}&{28.66}&{31.78}&{40.44}&{31.07}\\
{Bbox}&{Active Neural SLAM \cite{chaplot2020learning}}&{32.02}&{32.74}&{31.94}&{30.31}&{26.30}&{38.68}&{32.00}\\
{}&{Semantic Curiosity \cite{chaplot2020semantic}}&{33.51}&{33.11}&{32.91}&{29.57}&{25.76}&{39.97}&{32.46}\\
{}&{Ours (n=1)}&{33.57}&{34.36}&{32.79}&{31.54}&{28.38}&{43.81}&{34.07}\\
{}&{Ours (n=3)}&{33.34}&{34.48}&{35.28}&{32.12}&{31.87}&{43.11}&{\bf{35.03}}\\
\hline
\hline
{}&{Pre-trained}&{12.72}&{22.98}&{16.71}&{23.82}&{23.85}&{29.75}&{21.64}\\
{}&{Re-trained}&{14.99}&{24.68}&{18.36}&{24.32}&{25.15}&{34.23}&{23.62}\\
{}&{Random}&{18.22}&{27.25}&{8.82}&{28.19}&{29.08}&{39.39}&{25.16}\\
{Segm}&{Active Neural SLAM \cite{chaplot2020learning}}&{17.89}&{29.24}&{15.22}&{29.66}&{27.29}&{38.61}&{26.32}\\
{}&{Semantic Curiosity \cite{chaplot2020semantic}}&{18.18}&{30.06}&{18.39 }&{29.03}&{26.70}&{40.01}&{27.06}\\
{}&{Ours (n=1)}&{19.18}&{30.14}&{15.56}&{31.03}&{28.19}&{43.43}&{27.92}\\
{}&{Ours (n=3)}&{19.28}&{30.13}&{16.22}&{31.27}&{28.92}&{44.76}&{\bf{28.42}}\\
\bottomrule
\end{tabularx}
\end{table*}