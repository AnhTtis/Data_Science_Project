\subsection{Accuracy on High Impact Bugs} \label{subsec:per_impact_accuracy}

In this section, we show the top-\emph{k} accuracy across the implemented bugs
with an average IPC impact $>$1\% in our test architectures. Figure~\ref{fig:per_ipc_1} shows
the top-\emph{k} accuracy obtained by our proposed methodologies as the value of \emph{k}
increases. These results are separated using the bug partitioning scheme explained in Section~\ref{subsec:impl_bugs}.
Figure~\ref{fig:greater_1_var} shows the behavior across the 
``unseen variations of seen bug types'',
Figure~\ref{fig:greater_1_type} shows the results on the 
``unseen bug types'' and Figure~\ref{fig:greater_1_trained} shows the same, but
for the ``seen bug variations'' exclusively.  The figures show values of
\emph{k}$<$5, as longer lists of possible locations do not
provide significant help to designers to conduct their debug.

In the figures, ``CBC (GBDT)'' refers to the case of CBC with
per-time-step classification and gradient boosted tree models 
(100 trees per model, other parameters left at default).
The ``CBC (CNN)'' performs a per-trace classification using
convolutional neural networks, as described in
Section~\ref{subsec:one_stage_methodology}
(2 1D-CONV layers with 100 filters each, followed by
3 FC layers with 300, 100, 50 neurons with ReLU activation functions 
and a final output layer of a single neuron with sigmoid activation).
The ``P2BC'' results
correspond to an implementation with CNNs for the classifiers on the
second stage (the first stage uses GBDT with 250 trees, while the second
stage uses the same architecture as ``CBC (CNN)''). 
The results labeled as ``Random'' represent what would
be obtained with a random guess. The baseline for comparison should be the 
state of the art, which is manual debug. However, quantifying manual debug is difficult, as it varies significantly from person to person. We consider a random result a practically quantifiable baseline for comparison. Even if a manual debug is more accurate than random, given that our methodologies are automated, we are able to speed up the process significantly vs manual debug. 


In this case, CBC (GBDT) is the best performing technique, even better
than the ensemble. The accuracy obtained by the ensemble is
impacted due to P2BC having inferior accuracy for higher impact bugs
when compared to CBC (GBDT). 

Considering all the bugs (weighted average across the three bug partitions), our CBC (GBDT) methodology is able to achieve 76.8\% top-1 accuracy and over 90\% top-3 accuracy. As shown in  
Figure~\ref{fig:per_ipc_1}, results for ``unseen variations of seen bug types''
and ``seen bug variations'' are above that, both achieving a top-3 accuracy of about 95\%. As expected, the
``unseen bug types'' are the hardest to predict accurately, however, even
for these difficult cases of bugs that do not look similar to the ones
used for training, our CBC (GBDT) methodology achieves 76.9\%
top-3 accuracy.

The performance impact produced by a 
performance bug is not necessarily observed on the unit producing the bug. 
Our methodologies make no assumptions regarding this, in fact, the bug which incorrectly marks instructions as “serialized” is an example of how our methodologies are able to handle this. Our methods identify this type of bugs as occurring in the “Rename” stage, however serializing instructions will have an impact on the performance starting at the “issue” stage.

In our evaluation we found the units ``Branch'' and ``Memory'' 
to be the only ones with a localization rate much lower than the average. However, this is also
correlated with the fact that the implemented bugs for those two units had a much lower IPC
impact than the bugs on the other units. 
