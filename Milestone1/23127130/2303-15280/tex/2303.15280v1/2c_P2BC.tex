\subsection{Methodology \#2: Performance Prediction error-Based Classification (P2BC)} \label{subsec:two_stage_methodology}

\begin{figure*}[htb!]
  \centering
   \includegraphics[width = 0.9\textwidth]{figures/P2BC_Micro2022.pdf}
  \caption{Overview of the P2BC methodology.}
  \label{fig:two_stage_methodology_overview}
\end{figure*}

The Performance Prediction error-Based Classification (P2BC) method is
composed of two different ML stages. An overview of this
is shown in Figure~\ref{fig:two_stage_methodology_overview}. Its first
stage is a set of performance (IPC) prediction models trained
exclusively with bug-free design data, so as to capture the
relationship between the counters and the performance under healthy
conditions.  When such models are applied on buggy designs,
significant prediction errors show up as the healthy conditions no
longer hold.  The second stage uses those prediction errors as
features for its ML classifiers. The intuition behind this approach is
that the specific workloads where the model and the ground truth diverge,
as well as the characteristics of that divergence, can provide
key information on which particular functional unit is the source of
the bug. Hence, P2BC localizes bugs according to symptoms of
performance model failures.

The first stage of the P2BC the methodology is heavily inspired by
prior work on performance bug detection~\cite{carvajal2021detection}, although the second stage of
P2BC differs significantly, as it leverages the full error trace to feed the ML models for
performance bug localization, while prior work used a single error
metric as input to a heuristic rule-based classifier merely for
determining bug existence.


\subsubsection{P2BC Stage 1: Performance Modeling}

Just as described in Section~\ref{subsec:one_stage_methodology}, we extract data from a set of performance counters, either via simulation
or from the actual silicon chip.
The goal of this stage is to model the bug-free processor
performance. To do this, we developed ML models 
that use the performance counter data as input features in order to infer
bug-free behavior of an specific target metric (IPC in this case).
There is one ML regression model for each workload, with counters selected according to
Section~\ref{subsec:counter_selection}. 

Unlike CBC where, in order to increase visibility, the super-set of
the counters selected across all the workloads are used as features
for the ML models, in P2BC each workload uses only
the counters selected specifically for it, as those are sufficient to infer the 
performance obtained by the system. Increasing the set of counters might provide 
undesired redundancy that will reduce the model sensitivity to performance bugs,
degrading the methodology's capacity to localize the bugs.

We use a per-workload model strategy since, due to their
specialization to the behavior of
each microbenchmark, it
achieves lower error (7.7\% RRMSE, in average for our setup) than workload agnostic models (27\%, in average~\cite{ardalani2015cross}). Further, our methodology does not support the usage of static analysis models~\cite{ardalani2019static}, as they are unlikely to trigger bugs due to not being executed on the actual design.

In order to train the bug-free IPC models, we use performance counter data coming exclusively
from bug-free legacy designs in order to establish a bug-free performance baseline. 
In discussions with industry partners, we
find that their availability can be safely assumed given the extensive post-silicon
debug that these designs have gone through by their end-of-life. However, even if it is not
possible to have designs that are completely bug-free, with P2BC we would be able to localize
new bugs that are not part of the baselines learned by the ML models.
That said, prior work~\cite{carvajal2021detection} demonstrated that even
when small performance bugs exist, performance prediction can perform
well.  Using different legacy designs ensures the model learns to
differentiate abnormal behavior due to performance bugs vs. due to
different microarchitectures.

Here, we use GBDT~\cite{friedman2001greedy} models
for performance prediction, which were shown to be the best 
technique for this task~\cite{carvajal2021detection}. These
performance models are not meant to be golden references, in fact,
they perform poorly for buggy designs. However, the prediction errors
contain useful information and serve as features for the 2nd stage
classifiers. Since the performance counters are processed in a time-series
manner, these models produce an inferred (or ML estimated) IPC time trace.

Once the inferred IPC trace is obtained, an error
trace is calculated by computing the difference between
the IPC obtained via ML performance models and the one obtained from
the simulations (or in-silicon execution). These error traces are used
as inputs for the second stage of this methodology.

\subsubsection{P2BC Stage 2: Error-based Bug Localization}

Since the error traces from stage one come from different workloads,
which might have different execution times,
the number of time-steps on each trace might differ between workloads. Therefore,
after error traces are obtained for all the
workloads, they go through a resampling procedure.
The goal of this procedure is to transform all the traces so that they all 
have a uniform number of time-steps $T_R$,
so that ML strategies like CNNs
can be used. This is only required due to the mixing of data from different
workloads, since that is not needed by CBC, resampling is not required in that 
method.

The resampling we use is the Fourier methodology implemented on the SciPy library~\cite{scipy}. This procedure is based on Nyquist-Shannon sampling theorem
\cite{shannon1949communication}.  The basic idea is that, by modifying
the frequency domain spectrum of a signal, the number of samples
needed to capture all the trace information can be changed as well.
Depending on the number of samples in the trace, there can be two
possible resampling mechanisms:

\begin{compactitem}
    \itemsep0em
  \item \textbf{Down-sampling}: This is when the number of 
    time-steps in $w_i$, denoted $T_i$, 
    is greater than $T_R$. In this case, after an FFT is
    applied, the frequency spectrum is truncated at the required
    maximum frequency. With this, the signal can be rebuilt with less
    samples via FFT\textsuperscript{-1}.
  \item \textbf{Up-sampling}: This is when $T_i < T_R$. Here, after an FFT is applied,
    the frequency spectrum is zero-padded.  Then, the signal can be
    rebuilt with more samples via  FFT\textsuperscript{-1}.
\end{compactitem}

Empirical results demonstrated that the best results can be achieved
when the target number of samples is equal to the average number of
time-steps across all the workloads. 

The resampled traces are used as inputs to a multi-class 
classifier which is trained to identify the
microarchitectural unit where a performance bug is located. A
``one-vs-all'' methodology is followed, same as in
Section~\ref{subsec:one_stage_methodology}.

The IPC inference errors obtained by using stage one in legacy architectures
are used to train this stage classifier. As opposed to CBC, where one classifier
per workload is needed, here we use a single OvA classifier. Each base classifier $m_{j}$
is trained with data from one or more legacy architectures, samples from legacy
architectures with bugs occurring at unit $u_j$ are considered
``positive'' cases for the model, while bugs in any other unit
are considered ``negative''. Note that the training of this stage, bug-free architectures
are not mandatory, as they are merely used as ``negative'' samples.

The classifiers are implemented using a 1D-CNN.  The convolution operations are performed exclusively
along different time-steps of the same workload, and every workload is
used as a different channel~\cite{lecun1995convolutional}. The reason
to do this is because there is no relevant information to be learned
across same time-steps of different workload given that the order in which
they appear is
completely arbitrary.

When an inference is performed, the confidence score of every model 
is proportional to the probability assigned by the model for a bug being present at each 
unit. Higher confidence scores are obtained for the most likely locations of the bug.