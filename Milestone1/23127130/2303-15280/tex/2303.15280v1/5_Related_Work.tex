\section{Related Work} \label{sec:related_work}

Although the importance and difficulty of microprocessor performance
debugging has been recognized for over a quarter-century, few works
have attempted to tackle the issue.  Most of these prior works have
focused on detecting whether a performance bug is present in the
design, while only a few have attempted to localize the bugs.

\subsection{Performance Bug Detection}

Bose~\cite{Bose1994} attempted to detect microprocessor performance bugs by building
a performance fault model and simulating application traces.  
Creating this model is a very manual effort, and some of the
decisions taken to simplify the model are not realistic for modern
designs.  Other works used processor timing
models~\cite{surya1994architectural} and developed ad-hoc
microbenchmarks that enforce certain design invariants with repetitive
execution of hand-crafted loops.  Here, a different microbenchmark
must be developed for every test, constraining the coverage
space and, although a failure provides symptoms of a
bug, it does not assert the presence of the bug to any
specific location.

Singhal \emph{et al.}~\cite{Singhal2004} described the procedures and
tools used for performance verification in the Intel Pentium 4
processor on a 90nm technology.  This work shows that detection of
performance bugs relies on the execution of benchmarks and checking
whether the design achieved the expected performance, otherwise, a
manual debug process would be needed.  The recent work by Carvajal
\emph{et al.}~\cite{carvajal2021detection} automated detection
mechanisms with machine learning strategies relying on the knowledge
of performance behavior observed in legacy designs. Although this
strategy shows promise, it is a first step towards automated
performance debugging, as it only shows whether a bug is present in
the entire system, lacking a way to provide details regarding the
location of the detected bug.

\subsection{Performance Bug Localization}

Very few works have attempted microprocessor performance bug localization.  The few
partially related are restricted to test generation, and
none provide an automatic overall methodology.

Utamaphetai \emph{et al.}~\cite{Utamaphethai1999} model the entire
design as buffers and finite state machines in order to use strategies
developed for test generation.  These simplifications make the
methodology unable to localize bugs that are not necessarily related
to state transitions or dependencies.  Adir \emph{et
  al.}~\cite{adir2005generic} perform a microarchitectural-level
verification based on a test plan created by coverage models. These
coverage models however, require ``creativity'' and significant domain
knowledge.  Since the analysis of test program results is still
manual, their evaluation required almost 10 days (without considering
the time to create the coverage models) to validate a single
microarchitectural unit. Scalability concerns can arise when other
units and the interactions between are considered.  Both of these
works focus on test generation techniques while the analysis of test
results is still a conventional manual approach.


\iffalse
A multitude of works have developed
analytical performance models for superscalar out-of-order processors~\cite{noonburg1994theoretical,
karkhanis2004first,eyerman2009mechanistic,clement1993analytical,black1998calibration}
that could be used for performance debugging.
However, in order to achieve high accuracy, a large amount of parameters and combination of 
events that could affect the performance need to be accommodated, which makes analytical models
very complex, hence, although useful, these techniques are not widely used for performance verification. As an
attempt to automate the performance model generation Ould \emph{et al.}~\cite{Ould2007}
developed a tree-based strategy that uses performance counters in order to estimate the
design performance. However, the construction of the model is performed by executing and monitoring
the results of multiple traces on the design for which the model is being built,
which makes this strategy unfit for performance debugging, as the models will reflect
whatever bug present in the design.
\fi

%Others have also evaluated the usage of performance counters as inputs to a model to estimate a target metric has in different areas such as software performance regressions~\cite{shang2015perf} power analysis~\cite{Joseph2001, Contreras2005, Bircher2007,rodrigues2013,tsafack2014exploiting,Yang2016} or functional verification~\cite{BugMD, Poulos2018, Efendioglu2019}.

\subsection{Performance Bugs in Other Domains}

Even though the area of performance debugging in microprocessors has
not received a lot of attention in recent years, several strategies
for performance debugging in other areas have been proposed. The most
widely studied are performance debugging in cloud computing and
software engineering.

In the cloud computing environment, performance issues lie on the
detection of anomalies that affect the quality of the
service~\cite{Ibidunmoye2015}.  Gan \emph{et al.}~\cite{gan2019seer}
developed a deep learning-based online QoS violation prediction
technique for cloud computing by using runtime traces. This work
detects an upcoming violation, and identifies the microservice
responsible for it.  This is similar to the localization task tackled
in this work, but in a different environment where the source
corresponds to a microservice being executed by a system, as opposed
to a microarchitectural unit that is part of the system itself.

Recent works using ML approaches for performance
evaluation in software have been proposed.  Alam \emph{et
  al.}~\cite{alam2019autoperf} detect performance regressions due to
code changes using autoencoders.  In other works, a common strategy
for performance bug localization, is to use a language processing
models based on bug reports from software project
databases~\cite{rao2011retrieval,zhou2012bl, saha2013improving,
  akbar2019scor}.  The models learn to identify the correlations
between specific keywords in bug reports and the code changes
implemented to fix them. Although software performance debugging is
complex, performance debugging in microprocessor is a more complicated
task due to the much larger level of concurrency. 

Other works have attempted automation of functional bug localization~\cite{park2010blog}, 
while interesting, we find that functional bug approaches do not work well for performance 
debugging due to its lack of a known correct baseline. 