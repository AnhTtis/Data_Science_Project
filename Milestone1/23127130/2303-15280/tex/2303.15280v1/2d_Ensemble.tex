\subsection{Ensemble of both methods} \label{subsec:ensemble}

Although both methodologies provide satisfactory results, as shown in
Section~\ref{sec:evaluation}, they don't excel in the same cases.  In
order to take advantage of the strengths of both methods, a simple
ensemble scheme is presented in this section. The procedure is as
follows.

\begin{compactenum}
\itemsep0em 
\item The final confidence scores of each methodology are normalized,
  so that for every design, the sum of the confidences across all the
  units $u_j$ equals one.  Since a ``one-vs-all'' methodology is used,
  this cannot be guaranteed beforehand without this step.
\item For each unit, the average score obtained across both
  methodologies represents its final score.
\item This newly calculated scores are ranked, and their order is used
  to determine the unit with the highest probability to have a
  performance bug.
\end{compactenum}

Overall, the overhead of the proposed methodologies is very low, as ML model training takes about 30-60 minutes per model, and inference is in the order of seconds (this could be further accelerated with GPUs). Further, the simulation of each SimPoint takes 10-20 minutes to complete. Since all our models and simulations are independent, with enough computational resources, they can all be launched in parallel, producing a negligible overhead on the overall debugging process. 