\section{Introduction} \label{sec:intro}

Large amounts of time and effort are devoted to
verification and validation of every microprocessor design project.
Broadly, design verification can be broken into two large categories:
(1) functional and (2) performance verification, which is to identify design bugs that degrade performance without affecting functionality. Performance bugs are different from performance bottleneck as the former is due to design mistakes while the later is caused by tight resource constraints. Performance loss due to performance bugs  can 
be very significant, with recent reported cases shown to be
$>10\%$~\cite{mccalpin2018hpl}. This demonstrates a critical 
need for automated mechanisms for performance debugging.  As 
recent designs from Intel~\cite{corei7-11}, AMD~\cite{ryzen-9},
ARM~\cite{cortex-a}, and others place an even greater emphasis
on core performance, design complexity has scaled
dramatically, likewise scaling the difficulty in all forms of
verification.


%Functional verification has received extensive attention from researchers and, although complex, it benefits from the availability of known correct outputs that can be used to compare against.

Performance verification at microarchitecture level ensures that a
design correctly achieves expected performance in terms of execution
time or cycle count.  The main challenge in this task is that, unlike
functional verification, there is no exact golden reference to compare
against.  This is because of the high difficulty of modeling all the
interactions between the different units in complex microprocessor
designs, and accurately represent how they affect the overall system
performance.  %This task also suffers from 
%the lack of a good debugging infrastructure, as well as from 
%limited visibility into intermediate points in the design, which are mostly exposed through performance counters. Although useful for estimating the performance of the system, these counters are very difficult to use for manual debugging because of their complex relationship with processor performance and due to the large amounts of data they generate.  
Traditionally, performance
verification is conducted mostly through manual techniques which rely
on rough estimations of performance gain expected by
microarchitectural changes~\cite{Singhal2004}. Such manual processes
are not only very lengthy but also error-prone.



The process of performance verification and debugging roughly consists of two steps: (1)~detection, which determines whether a
design achieves expected performance or not, and (2)~localization,
which identifies the microarchitectural units causing the performance
issues and is the focus of this work.

There are few previous studies on automating detection of
microprocessor performance bugs~\cite{Bose1994,
  surya1994architectural,carvajal2021detection}. 
The majority of those~\cite{Bose1994, surya1994architectural} relies on capturing
design intentions using a bespoke performance model as a golden
reference, this  entails long development time and may contain
errors by itself. Recently, a data driven and machine learning
(ML)-based approach~\cite{carvajal2021detection} was developed for
automatic performance bug detection with high accuracy. Although
significant, these works do not solve the 
problem of performance bug localization.
%pressing problem of identifying where the performance bug is.

Works in automating microprocessor performance bug localization
are even scarcer.  Adir \emph{et al.}~\cite{adir2005generic}
propose perhaps the only partially related work of which we are
aware.  Their work focuses on formal planning of test program
generation for individual units, such as issue queues. This strategy
follows conventional functional verification, involving heavy
manual effort, costing significant engineer-time to develop a test
plan, and as much as ten days of computer runtime per functional
unit. To the best of our knowledge, there has been no systematic study
on automatic performance bug localization for microarchitecture
designs.

Performance bug localization is a complicated task, which is currently
solved using mostly manual techniques.
Even
in the more widely studied area of functional validation, the industry
lacks a standardized mechanism to automate bug localization, it has
been only recently that academic efforts have attempted to automate
this task~\cite{BugMD}. Considering this, it is important to note that
any type of design automation which successfully reduces the 
time and effort required by engineers to debug their designs is highly
valuable. Since automatic performance debug for microprocessors is
a huge yet under-studied challenge, it is very difficult, if not impossible, to find a perfect solution in a single work. Although our work is not perfect, it serves a key stepping stone 
 toward solving the problem.

This work tackles the performance bug localization problem by
using ML to generate a ranked list of most likely mi\-cro\-ar\-chi\-tec\-tur\-al units that 
might contain the bug.  This list may be used
to prioritize the debugging order, as well as to identify
teams with the right expertise to perform further debug. Two different methodologies are
proposed, evaluated, and contrasted. These data-driven
techniques achieve high
accuracy, while being fully automated. Further, they
consider intra- and inter-unit interactions, as opposed to other
techniques proposed in the partially related previous work~\cite{adir2005generic} which
considered only intra-unit behavior.

%Our methods are based on ML, wherein our models are
%trained using data from legacy designs.
%To take the full advantage of
%these approaches, we assume that architectural changes in a new design
%are incremental when compared to its previous
%generations. Examining recent processors from major vendors including
%Intel, ARM, and AMD, we find this assumption holds true, since the generational change in microarchitectures
%is largely incremental. Thus, the methodologies proposed here provide
%alue for a multitude of upcoming designs.  However, even when
%disruptive changes occur, the methodologies can still be beneficial for bug localization on structures that conform to previous microarchitectures, using workloads that
%do not exercise new functionalities. Further, as general purpose microarchitectures become ever more mature, and the inter-generational performance gains decrease, 
%it is even more important to retain as much performance as possible, making performance debugging ever more important.

The major contributions of this work include the following:
\begin{compactitem}
\itemsep0em 
\item This is the first systematic study on fully automatic
  performance bug localization for microarchitecture designs, to the
  best of our knowledge.

\item Two ML-based approaches to tackle performance bug
  localization, as well as a hybrid of both, are evaluated
  and contrasted.

\item For bugs with an average IPC impact greater than 1\%, our best
  performing methodology identifies the correct bug location as the
  most likely unit in $\sim77\%$ of the cases, and achieves over 90\%
  accuracy when the three most likely options (out of 11 possible) are
  considered.  

\item One of the proposed methodologies is not only very accurate localizing
performance bugs, but it can also be applied to confirm the results
of performance bug detection with high accuracy.

\item Although the focus of this work is on microprocessor core,
we evaluated our methodologies on the processor memory hierarchy. This evaluation
uses a different experimental setup, showing the robustness of the proposed techniques.

\end{compactitem}

As an early work on performance bug localization, the design of this study is subject to potential limitations, however, we feel it still represents a good first step towards solving the problem. The scope of our work and its limitations are as follows:
\begin{compactitem}

\item Legacy
designs with identified performance bugs are required, so that the ML
models can be trained. Bug-free legacy designs are required only 
in one of the methodologies, yet, if available, the other can take advantage of the additional data.
However, thanks to the thorough pre- and post-silicon
debug to which the designs are submitted, these legacy designs are
generally available.

\item We assume that only one bug is present at a time, 
in parallel to the single-fault model which is common practice
in VLSI testing works. As explained in Section~\ref{subsec:impl_bugs}, we still expect 
our methodologies to work well in the presence of multiple bugs in a single design.

\item Our methodologies do not provide a quantitative coverage guarantee.  
In general, performance bug
coverage is extremely difficult to define and is a potential
research topic on its own. We know of no prior work which presents a
definition of such coverage. Nonetheless, the evaluated bugs are based on published errata, cover a large amount of microarchitectural units and affect the system in a variety of ways. Thus, we feel these bugs represent a reasonable start for early work in this area.

\item We assume that there are no dramatic structural
microarchitectural changes between the legacy designs and the
designs under debug. Examining recent processors from major vendors, including Intel, ARM, and AMD, we find this assumption holds true, since the generational change in microarchitectures
is largely incremental. That said, even when larger shifts occur, the
methodologies can be partially reused. For example, consider the
introduction of the AVX instructions with Intel's Sandy Bridge
architecture in 2011.  Initially there would be no available data to
test these instructions using our methodologies, however the rest of
the Sandy Bridge design could be debugged with our methodology,
leveraging workloads that do not exercise the new instructions.  In
future implementations, data from Sandy Bridge can be used to build
the models required to use our methods for debugging AVX. 
%Further, as general purpose microarchitectures become even more mature, and the inter-generational performance gains decrease, it is even more important to retain as much performance as possible, making performance debugging even more important.}

\item We limit our evaluation to a pre-silicon setup, because
it is infeasible for us to inject known design bugs in silicon to
evaluate the methodologies.  Further, should our methodologies be
applied to a commercially available design, and an actual bug be
found and localized, we would not be able to verify that such
localization is correct without prior knowledge of its existence so
as to verify our findings. However, our methodologies can be applied in both pre- and post-silicon scenarios. During pre-silicon stages fixing performance 
bugs is easier and cheaper, 
the availability of performance counters is greater (due to the usage of a
simulator) and the counters can be sampled at a much faster rate. 
By using only counters available in-silicon, and adjusting the sampling frequency, we could use the proposed
methodologies during post-silicon stages. In post-silicon analysis the methodology
could be applied to longer workloads, providing a way to exercise complicated bugs that
are not possible to trigger with short pre-silicon traces.
%Further, we can follow hybrid approaches where the ML model training is performed using simulations, and the techniques are applied to data obtained from microprocessors during post-silicon debug. }

\end{compactitem}

Despite the aforementioned, we present a first, useful, yet attainable,
step towards the goal of performance bug localization, and we hope this work can draw the attention of the research community
to the broader performance validation domain.

\iffalse{
In Section~\ref{sec:scope} we describe the problem
formulation and outline the scope of this work.  We note that, to
date, very little work exists in automating performance bug
localization. 

Section~\ref{sec:methodology} 
describes the approaches developed to tackle the performance bug
localization task.  Section~\ref{sec:experimental_setup} provides
details of the architectures, and performance bugs used for
evaluation. Section~\ref{sec:evaluation} presents results obtained in
several experiments developed to evaluate the methodologies. A brief
review of previous work related to performance debugging is presented
in Section~\ref{sec:related_work}.  And finally,
Section~\ref{sec:conclusion} concludes the paper.
}
\fi