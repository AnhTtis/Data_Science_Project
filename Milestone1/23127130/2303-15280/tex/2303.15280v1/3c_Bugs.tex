\subsection{Implemented Bugs} \label{subsec:impl_bugs}

Given its wide acceptance on the computer architecture community, the
gem5~\cite{gem5} simulator is treated as a performance bug-free
simulator. Therefore, in order to evaluate the methodologies,
performance bugs are artificially injected into it. 
Although we use gem5 as a ``bug-free'' baseline, just as any big
software (or hardware) project, it is likely to have bugs,
we believe this should not deter us from implementing debugging 
mechanisms. 

To obtain a list
of bugs that cover most microarchitectural units and are considered
realistic, multiple errata of commercial microprocessors were
reviewed~\cite{intel_xeon_errata,nxp_7448_errata,TI_am3517_errate,intel_386_errata}
and industry experts were consulted. Ultimately, a list of 22
different bug types were implemented in gem5 and are used to evaluate
our scheme. To implement these bugs, we manually edit gem5 code
such that the simulator would behave as the bug describes,
rather than follow its normal behavior.

Although relatively few in the scope of all possible bugs,
these bugs represent a reasonable start for early work in the area.
We examine our localization technique by localizing the
bug to one of 11 possible units. The assignment of bug locations
to each unit, and the description of each performance bug type are
found in Table~\ref{tab:bugs_gem5}. Since the evaluation of the
techniques is based on simulations, no further breakdown is
possible in most units, given the abstraction of unit implementation
in gem5.

For each of these bug types, multiple variations were implemented by
modifying the values of \textit{X}, \textit{Y}, \textit{N}, \textit{R}, and \textit{T}.  The
average IPC impact of these bugs is measured across the used SPEC
CPU2006 applications, and its distribution is shown in
Figure~\ref{fig:bug_hist}.  Bugs with large IPC impact ($>5\%$) are
usually easier to debug, therefore, we include a smaller fraction of
these bugs.  Bugs that produce a performance degradation between
1\%$-$5\%, can be considered to produce a moderate impact, while
degradation $<$1\% is considered small.  Only bugs with IPC degradation $>0.1\%$ were considered for our
evaluations.

\begin{figure}[htb!]
  \centering
  \includegraphics[width = 0.32\textwidth]{figures/bug_distr_ISCA.pdf}
  \caption{Average IPC impact distribution for evaluated bugs.}
  \label{fig:bug_hist}
\end{figure}

For some bug types, none of their variations are used to train the ML models,
so the methodology can be evaluated on bugs that it has
never encountered before, we refer to these as the ``unseen bug
types''.  For the other bug types, some  variations are used to
train the ML models, while others are left exclusively for testing. This partitioning
evaluates how the model performs on bugs that are similar, but not equal, to
those that it has encountered before
(with the variations left exclusively for testing), as well as in cases where
the same bug seen in legacy designs is encountered (with the variations used for training). We refer to these
as ``unseen variations of seen bug types'' and ''seen bug
variations'', respectively. 
For example, consider a particular bug type X, for which we have three variations: X.A, X.B and X.C. If we use data from legacy architectures
with X.A and X.B to train our models, a sample from a test architecture with X.C would
be an ``unseen bug variation of a seen bug type'' while samples from test architectures
with X.A and X.B would be from a ``seen bug variation''. On the other hand, if none of the training samples
have bug X, the samples from test architectures with any variation of this bug would be considered to be from an
``unseen bug type''.

We collected data for three different variations of each of the 22 implemented bug types. For six of those,
neither variation was used during training (``unseen bug types''). For the remaining 16 bug types,
two variations are used for training (``seen bug variations''), while the other one is left exclusively 
for testing (``unseen bug variation of a seen bug type''). In summary, 27.3\% of the samples correspond to
``unseen bug types'', 24.2\% correspond to ``unseen bug variation of a seen bug type'' and 48.5\% correspond to ``seen bug types''.

The data organization is as follows:

\begin{compactitem}
  \itemsep0em 
\item Training data:
  \begin{compactenum}
    \itemsep0em 
  \item Data with positive labels: For each model trained to identify
    performance bugs in unit $u_j$, the samples with positive labels
    are those from bugs in unit $u_j$ which are placed in
    the ``seen bug variations'' category. These are
    exclusively from the ``train'' microarchitectures.
  \item Data with negative labels: For each model corresponding to
    unit $u_j$, the samples with negative labels are those from
    bugs that do not occur in unit $u_j$ 
    (or bug-free cases, if available) and 
    which are
    placed in the ``seen bug variations'' category. These include data
    exclusively from the ``train'' microarchitectures.
  \end{compactenum}
\item Testing data:
  \begin{compactenum}
    \itemsep0em 
  \item Designs with bugs: Samples from ``seen'' and the two
    ``unseen'' categories of bugs coming exclusively from ``test''
    architectures are evaluated.  In any given sample, only one bug is
    inserted.
  \item Bug-free designs: As mentioned earlier, it is assumed that a
    performance bug has already been detected on the design under
    test. However, an analysis of bug-free
    architectures using the methodologies is shown in
    Section~\ref{subsec:no_bug_handling} to evaluate
    false-positives at detection.
  \end{compactenum}
\end{compactitem}

Note that all the samples used for training are
taken from ``seen bug variations'' in ``train'' architectures. On the
other hand, every sample used for testing comes from ``test''
architectures but can either be a ``seen bug variation'', an
``unseen variation of a seen bug type''  or a
completely ``unseen bug type''.

For our evaluation, at most one bug is injected in every design. Although this is a gap to be addressed in future work, the assumption of a single dominant bug is a first step towards solving the problem. We expect that our methodology would work with multiple bugs because, given that the classifiers for each unit are independent (due to OvA), our methodology should produce high confidence of bugs in all (or most) of the units where a bug is detected. 