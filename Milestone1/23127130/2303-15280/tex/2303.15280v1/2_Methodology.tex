
\section{Problem Definition and Scope}
\label{sec:scope}

%FORMULATION, OBJECTIVE, INPUT, ASSUMPTIONS

The \emph{objective} of this work is to identify the
microarchitectural units where a detected performance bug is located.
The \emph{scope} of this work, as presented here, is limited to
microarchitectural-level performance bugs in the processor core that
affect the IPC of the system\footnote{We note that, while we do not
test on components other than the processor core, there is no
reason to think that this methodology could not work there.}.
Bugs that might affect circuit-level timing (\emph{i.e.} clock
period) are not considered. For testing and insertion convenience, we cover cases where the hardware does not achieve the expected performance due to a microarchitecture bug, that is, due to waste or under-utilization of resources by implementation faults. The methodologies, however, can be used for performance bottlenecks arising from suboptimal hardware, algorithms, or other settings.

%Both of our proposed approaches require legacy
%designs with identified performance bugs in order to train the ML
%models.  Bug-free legacy designs are required only in one of the
%proposed methods, yet, if available, the other can take advantage of
%the additional data.  Due to the thorough pre- and post-silicon
%debug to which the designs are submitted, these legacy designs are
%usually available.
%We have limited the scope of our evaluation to
%buggy designs with a single bug at a time, in parallel to the
%single-fault model which is common practice in VLSI testing
%works. As early work on performance bug localization, we feel this
%represents a good first step towards solving this problem.
  
%The techniques presented here are evaluated using synthetic bugs,
%many of which are based on published errata, as such, they do not
%provide quantitative coverage guarantees. In general performance bug
%coverage is extremely difficult to define and is a potential
%research topic on its own. We know of no prior work which presents a
%definition of such coverage. Nonetheless, evaluated bugs cover
%a large amount of microarchitectural units and affect the system in
%a variety of ways. Thus, we feel these bugs represent a reasonable
%start for early work in this area.

%We also assume that there are no dramatic, structural
%microarchitectural changes between the legacy designs and the
%designs under debug. That said, even when larger shifts occur, the
%methodologies can be partially reused.  For example, consider the
%introduction of the AVX instructions with Intel's Sandy Bridge
%architecture in 2011.  Initially there would be no available data to
%test these instructions using our methodologies, however the rest of
%the Sandy Bridge design could be debugged with our methodology,
%leveraging workloads that do not exercise the new instructions.  In
%future implementations, data from Sandy Bridge can be used to build
%the models required to use our methods for debugging AVX.



\section{Methodology} \label{sec:methodology}

\subsection{Overview}

In this work, we propose two machine learning-based methodologies aiming to
identify the microarchitectural unit where a detected performance
bug exists. Here, we use the term microarchitectural
unit to refer us to a segment of a microprocessor that performs a
specific task. Examples of microarchitectural units in this context
would be Fetch, Decode, Branch Predictors, Load/Store Queue, etc.
  
Both of the proposed approaches leverage performance counter data as
inputs, since they are  available in almost any
microprocessor design.  This prevents the overhead of adding dedicated
data acquisition infrastructure.  In addition, microarchitecture
designs usually have hundreds or thousands of Performance
Counters~\cite{perfmon-intel,perf_counters_amd}, which are generally
sufficient to extract necessary information for performance
estimation. 
%Where prior work has only attempted to detect the existence of such bugs~\cite{carvajal2021detection}, this work aims to determine which functional unit contains that bug.

Here, we use ML due to its capability of knowledge extraction from
data, and its strength to handle complicated nonlinear behaviors.
Existing bug localization, in practice, is by and large manual, while
ML is perhaps the best approach to mimic a human manual process among
mathematical or algorithmic options. However, our goal is not to
remove the human from the debugging task, but merely speedup the
process by providing useful guidance extracted from the data.
Performance counters are used as input features to our ML-based
methodologies.  They are extracted from the results of simulating
(or executing) a diverse set of workloads.
%, which are used as stimuli applied to the microprocessor, in order to measure how it behaves under different circumstances.

The first proposed methodology uses multiple ML models,
each of which classifies if a given unit contains the bug. Then,
results of these models are aggregated across different workloads to
obtain an ordered list of units according to their confidence levels
for bug existence. This methodology is referred to as
\textbf{``Counter-Based Classification''} or \textbf{``CBC''}.

The second methodology is a two stage approach.  Its first stage
includes machine learning models for predicting bug-free performance
in terms of IPC. In the second stage, prediction errors of these
models are utilized for estimating the likelihood of bug existence for
each unit.  The name \textbf{``Performance Prediction error-Based
  Classification''} or \textbf{``P2BC''} is used to refer to this
method.

The output from either of the methodologies provides a priority list
of the most likely microarchitecture units that might contain the
performance bug, this list can be used for further analysis by
validation or design engineers.

\input{2a_Probe_Design}
\input{2b_CBC}
\input{2c_P2BC}

\subsection{Trade-offs} \label{subsec:tradeoffs}

These two methodologies have their advantages and drawbacks, these
are discussed as follows:
\begin{compactenum}
\itemsep0em 
\item \textbf{Accuracy vs data storage and runtime}: 
  As we show in Section~\ref{sec:evaluation}, CBC generally performs better than P2BC.  However, CBC
  requires a higher number of models than  P2BC. For the former, one model per unit per workload is needed
  ($|W| \cdot |U|$ in total), while the latter requires one model per
  workload for IPC estimation (Stage 1) and one model per unit for bug
  localization (Stage 2), for a total of $|W| + |U|$.  The increased number of
  models in CBC creates a larger data storage requirement
  ($100\times$ in our evaluation), as well as a longer runtime for
  training and inference ($6-10\times$).
  Nevertheless, although CBC is slower than P2BC, for both
  methodologies, full training can be achieved within
  a day, while the inference time is in the order of a just few seconds,
  without paralellization.

\item \textbf{Incremental workload addition}: CBC is more friendly to
  incrementally adding new workloads.  To add a new workload, $|U|$
  new models must be trained for this method. Although for P2BC the
  number of trained models is not significantly larger, only $|U|+1$ (one
  new IPC model, and $|U|$ re-trained models in stage two), adding a new workload
  requires to re-train the entire stage two, which could impact the
  quality of results and requires a longer training period.
    
\item \textbf{Incremental bug unit location addition}: Both approaches
  allow for incremental addition of microarchitectural units where
  bugs might be located.  This is useful for cases when a bug location
  that was not part of the initial list of considered classes needs to be included, or when a new structure is added
  to the system.  In this case, only one new model would need to be
  trained for P2BC, while CBC would need $|W|$. Although
  re-training the models for other units using designs with bugs in
  the new one is not required, doing so might improve the accuracy of
  the overall debugging approach.

\end{compactenum}

\input{2d_Ensemble}
