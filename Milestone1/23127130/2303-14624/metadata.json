{
    "arxiv_id": "2303.14624",
    "paper_title": "Guiding AI-Generated Digital Content with Wireless Perception",
    "authors": [
        "Jiacheng Wang",
        "Hongyang Du",
        "Dusit Niyato",
        "Zehui Xiong",
        "Jiawen Kang",
        "Shiwen Mao",
        "Xuemin",
        "Shen"
    ],
    "submission_date": "2023-03-26",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.AI",
        "cs.HC",
        "cs.NI"
    ],
    "abstract": "Recent advances in artificial intelligence (AI), coupled with a surge in training data, have led to the widespread use of AI for digital content generation, with ChatGPT serving as a representative example. Despite the increased efficiency and diversity, the inherent instability of AI models poses a persistent challenge in guiding these models to produce the desired content for users. In this paper, we introduce an integration of wireless perception (WP) with AI-generated content (AIGC) and propose a unified WP-AIGC framework to improve the quality of digital content production. The framework employs a novel multi-scale perception technology to read user's posture, which is difficult to describe accurately in words, and transmits it to the AIGC model as skeleton images. Based on these images and user's service requirements, the AIGC model generates corresponding digital content. Since the production process imposes the user's posture as a constraint on the AIGC model, it makes the generated content more aligned with the user's requirements. Additionally, WP-AIGC can also accept user's feedback, allowing adjustment of computing resources at edge server to improve service quality. Experiments results verify the effectiveness of the WP-AIGC framework, highlighting its potential as a novel approach for guiding AI models in the accurate generation of digital content.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.14624v1"
    ],
    "publication_venue": null
}