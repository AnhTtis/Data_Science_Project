
\pdfoutput=1


\documentclass[11pt, dvipsnames]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{colortbl}
\usepackage{color}
\usepackage{tabularx}
\usepackage{dingbat}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{stfloats}
\usepackage{subfig}
\usepackage[T1]{fontenc}
\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{fontawesome5}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{inconsolata}
\newcommand{\cmark}{{\textbf{\textcolor[rgb]{0.1, 0.5, 0.1}{\ding{51}}}}}
\newcommand{\xmark}{{\textbf{\color{red}{\ding{55}}}}}

\def\emojihappy{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_happy.jpeg}}}
\def\emojineutral{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_neutral.jpeg}}}
\def\emojithink{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_think.jpeg}}}
\def\emojiwarning{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_attention.jpeg}}}

\definecolor{myblue}{rgb}{0.82, 0.94, 0.75}

\title{Error Analysis Prompting Enables Human-Like Translation Evaluation\\ in Large Language Models: A Case Study on ChatGPT}

\author{Qingyu~Lu$^{\diamondsuit, \Re}$,
\ \textbf{
\ Baopu Qiu$^{\flat, \Re}$,
\ Liang Ding$^{\Re}$, 
\ Liping Xie$^{\diamondsuit}$,
\ Dacheng Tao$^{\Re}$} \\
\ $^{\diamondsuit}$Southeast University
\ $^{\Re}$JD Explore Academy, JD.com Inc.
\ $^{\flat}$Nanjing University\\
\includegraphics[scale=0.15]{figs/emoji_mail.png} \texttt{luqingyu@seu.edu.cn}, \texttt{qiubaopu@smail.nju.edu.cn}, \texttt{dingliang1@jd.com}\\
\includegraphics[scale=0.09]{figs/tool.jpeg} \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}
}

\begin{document}
\maketitle
\begin{abstract}

Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research~\citep{kocmi2023llmfindings} has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}.
To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts~\cite{wei2022chain} and Error Analysis~\cite{lu2022toward}, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}.
Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query.
Our findings aim to provide a preliminary experience for appropriately evaluating translation quality on ChatGPT while offering a variety of tricks in designing prompts for in-context learning. 
We anticipate that this report will shed new light on advancing the field of translation evaluation with LLMs by enhancing both the accuracy and reliability of metrics. 
% The project can be found in \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}.

\end{abstract}

\section{Introduction}

Large language models (LLMs), especially Generative Pre-trained Transformer (GPT) models~\cite{radford2019language, brown2020language}, have shown remarkable performance in various natural language processing (NLP) tasks. Recently, Open AI developed ChatGPT, an interactive chatbot built upon InstructGPT~\citep{ouyang2022training}, which has captured the attention of researchers in the NLP community~\cite{qin2023chatgpt,zhong2023chat}. This chatbot is capable of integrating multiple NLP tasks and can generate detailed and comprehensive responses to human inquiries. Additionally, it can respond appropriately to follow-up questions and maintain sensitivity throughout several turns of conversation.

Previous research has demonstrated that ChatGPT can perform as well as or even better than other LLMs in machine translation task~\cite{hendy2023good,jiao2023chatgpt,Peng2023ChatGPT4MT}. However, it remains uncertain whether ChatGPT can be used as a metric to evaluate the quality of translations. If ChatGPT is suitable for this task, then, how to develop appropriate prompts that can make ChatGPT generate reliable evaluations?
Concurrent to our work, \citet{kocmi2023llmfindings} present an encouraging finding that LLMs, e.g., ChatGPT, could outperform current best MT metrics at the system level quality assessment with zero-shot standard prompting, but such kind of prompts show unreliable performance at the segment level.

\begin{figure*}[t]

\includegraphics[scale=0.38]{figs/overview.pdf}
\centering
\caption{An comparative overview between Standard Prompting and our proposed Error Analysis Prompting in assessing the MT quality with ChatGPT.}
\label{fig:overview}
\end{figure*}

In this work, we take the further step by carefully investigating the current advanced few-shot prompting strategies upon ChatGPT for MT quality assessment, and propose a novel prompting strategy -- \textbf{Error Analysis (EA) Prompting}, combining the Chain-of-Thought (CoT, \citet{wei2022chain}) and Error Analysis (EA, \citet{lu2022toward}). 
We give an example of this prompt in Figure~\ref{fig:overview}. The idea is to prompt ChatGPT to generate a human-like evaluation like MQM \citep{freitag-etal-2021-experts} by \ding{182} \textit{identifying major and minor errors}, and \ding{183} \textit{scoring the translations according to the severity of these errors}. In addition, we also explore the potential of ChatGPT compared with modern neural metrics like COMET \citep{rei-etal-2020-comet}, BERTScore \citep{zhang2020bertscore} and BLEURT \citep{sellam-etal-2020-bleurt}.

Our experiments and analyses illustrate that: 

\begin{itemize}
    \item[\emojihappy] Our proposed EA Prompting outperforms standard prompting \citep{kocmi2023llmfindings} at the segment level, achieving human-like evaluations at both the system level and segment level.
    \item[\emojithink] When designing prompts, itemized responses are better than lengthy and detailed explanations of errors. Moreover, splitting the instruction into two identifying errors and scoring translation can improve evaluation stability.
    \item[\emojineutral]  The boosted performance from EA prompting is observed in the zero-shot scenario on \texttt{text-davinci-003} rather than in the few-shot scenario, which indicates that we need to adjust our settings when utilizing other GPT models.
    \item[\emojiwarning] Despite its good performance, we show that ChatGPT is NOT a stable evaluator and may score the same translation differently.
    \item[\emojiwarning] It is NOT advisable to combine multiple translations into a single query input, as ChatGPT has a preference for former translations. 
\end{itemize}

The remainder of this report is designed as follows. We present the evaluation settings and comparative results in Section~\ref{sec:evaluation}. In Section~\ref{sec:casestudy}, we highlight several potential issues that researchers should be aware of when using ChatGPT as a translation evaluator. Conclusions are described in Section~\ref{sec:conclusion}.

\section{ChatGPT As An Evaluation Metric}
\label{sec:evaluation}
\subsection{Experiment Setup}

\paragraph{Dataset}

We utilize the testset from the WMT20 Metric shared task in two language pairs: Zh-En and En-De. To ensure the reliability of our experiment, for each language pair, we divide the segments into four groups based on the number of tokens they contain (15-24, 25-34, 35-44, 45-54). We randomly sample 10 segments from each group and form a new dataset containing 40 segments. Details are shown in Table~\ref{tab:experiments}. 

\paragraph{Human Evaluation} 

Human evaluation of translated texts is widely considered to be the gold standard in evaluating metrics. We use a high-quality human evaluation dataset Multi-dimensional Quality Metrics (MQM, \citet{freitag-etal-2021-experts}) as human judgments. This dataset is annotated by human experts and has been widely adopted in recent translation evaluation \citep{freitag-etal-2022-results} and quality estimation tasks \citep{zerva-etal-2022-findings} in WMT.

\begin{table*}[ht]
\centering
\begin{tabularx}{0.94\textwidth}{cccX}
\toprule[0.5mm]
Language Pair & Segments & Systems & Systems Selected \\\midrule
En-De & 40 & 7 & \small{Tohoku-AIP-NTT, OPPO, eTranslation, Tencent\_Translation, Huoshan\_Translate, Online-B, Online-A} \\
Zh-En & 40 & 8 & \small{Huoshan\_Translate, WeChat\_AI, Tencent\_Translation, OPPO, THUNLP, DeepMind, DiDi\_NLP, Online-B} \\
\bottomrule[0.5mm]
\end{tabularx}
\caption{Statistics of testset. Source, reference texts, and translations are from the WMT20 metrics shared task. }
\label{tab:experiments}
\end{table*}

\paragraph{Meta Evaluation}

We utilize the accuracy of pairwise system-ranking \citep{kocmi-etal-2021-ship} for the system level comparison. At the segment level, we follow \citet{freitag-etal-2022-results} to adopt the average of three types of Kendall correlation. Specifically, these values are computed by flattening the scores into a single vector and calculating the average correlations over systems, or over segments.

\paragraph{Baseline} 

We compare LLMs with several commonly used baseline metrics for MT evaluation. \textbf{BLEU} \citep{papineni-etal-2002-bleu} is the most popular metric that compares the n-gram overlap of the translation with human reference, but it has been criticized for not capturing the full semantic meaning of the translation \citep{freitag-etal-2022-results}.
\textbf{BERTScore} \citep{zhang2020bertscore} is a neural metric that relies on pre-trained models to compute the semantic similarity with the reference. \textbf{BLEURT} \citep{sellam-etal-2020-bleurt} and \textbf{COMET} \citep{rei-etal-2020-comet} are supervised neural metrics that leverage human judgments to train. They have shown a high correlation with human judgments.

\paragraph{Large Language Models} We test the evaluation capability on ChatGPT using the default model of ChatGPT-plus, and compare it with text-davinci-003, a base model of ChatGPT.

\begin{table*}[ht]
\centering
{
\begin{tabular}{lcccc}
\toprule[0.5mm] 
\multirow{2}{*}{\textbf{Metrics}}  & \multicolumn{2}{c}{\textbf{En-De}}  & \multicolumn{2}{c}{\textbf{Zh-En}}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& \textbf{System(\%)} & \textbf{Segment(\%)} & \textbf{System(\%)} & \textbf{Segment(\%)} \\ \midrule
BLEU \citep{papineni-etal-2002-bleu}  &71.43 &3.55    &21.43    &14.71     \\ 
BERTscore \citep{zhang2020bertscore}  &\textbf{76.19} &12.30    &25.00    &26.75     \\ 
BLEURT \citep{sellam-etal-2020-bleurt}&\textbf{76.19} &33.44   &57.14    &32.76  \\ 
COMET \citep{rei-etal-2020-comet}     &71.43 &\textbf{33.47}   &50.00    &\textbf{38.97}   \\
\midrule
text-davinci-003              &42.86 &11.86   &53.57    &23.08   \\
ChatGPT-EA                               &\textbf{76.19} &26.40    &\textbf{60.71}    &36.73   \\
\bottomrule[0.5mm]
\end{tabular}}
\caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. The best results are \textbf{bold}.}
\label{tab:mainres}
\end{table*}


\begin{figure}[t]
\centering
\includegraphics[scale=0.33]{figs/segment_compare.pdf}
\caption{ The segment level comparison between ChatGPT and text-davinci-003 with standard prompting ("Standard"), in-context error analysis prompting ("EA"), and zero-shot error analysis prompting ("zero-shot EA"). }
\label{fig:incontext}
\end{figure}

\subsection{ChatGPT as a metric attains SOTA performance at the system level} 

Table~\ref{tab:mainres} presents the performance of LLMs compared with other baseline metrics. We report the best-performing setting, where LLMs with EA prompting. We can see that: 
\begin{itemize}
    \item at the system level, ChatGPT achieves SOTA performance compared with existing evaluation metrics for both language pairs. However, text-davinci-003 obtains inferior results compared with other metrics. Our results are consistent with the findings of \citet{kocmi2023llmfindings}, who tested the performance of large language models on \emph{full test set} of the WMT22 metric task. 
    \item ChatGPT and text-davinci-003 lag behind state-of-the-art metrics for En-De at the segment level. For Zh-En, while text-davinci-003 remains suboptimal, ChatGPT with EA prompting exhibits superior performance relative to all other metrics, with the exception of COMET.
\end{itemize}

\begin{table*}[ht]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccccccccc}
\toprule[0.5mm] 
\multicolumn{2}{c}{\textbf{Instruction}} & \multicolumn{2}{c}{\textbf{Response}} & \textbf{Separation} & \multicolumn{8}{c}{\textbf{Score - Segment\#38}} & \textbf{Total} \\ 
\cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){6-13} 
Standard & EA   & Detailed      & Itemized      &           
& sys1  & sys2  & sys3  & sys4  & sys5  & sys6  & sys7  & sys8 &       \\ \midrule
\cmark &    & \cmark  &       &    -        &  
-3& -2  & 0  & -3  & -1  & -3  & -1  & -2     & -15    \\
\cmark &    & & \cmark      &    -        &  
-3& -3  & -2  & 0  & -2  & -2  & -3  & -2     & -17    \\
 & \cmark    & \cmark  &       &    \xmark        &  
-1& -1  & -3  & -1  & -1  & 0  & -1  & -2     & -10    \\
 & \cmark    & \cmark  &       &    \cmark        &  
-2& -2  & -2  & -3  & -1  & -2  & -2  & -2     & -16    \\
 & \cmark    &  &   \cmark    &    \xmark       &  
-5& -5  & -3  & -4  & -5  & -4  & -4  & -3     & -28    \\
 & \cmark    &  &  \cmark     &    \cmark       &  
-4& -4  & -3  & -6  & -3  & -4  & -4  & -3     & -26    \\
\bottomrule[0.5mm]
\end{tabular}}
\caption{Comparison of the segment level scores of ChatGPT for different variants of in-context prompts. We divide the instructions into two categories: standard and error analysis ("EA"). The response template can either be itemized or detailed. As for the error analysis instruction, it can be separated into two queries (one for identifying errors and another for scoring) or combined into a single query.}
 \label{tab:prompts_selection}
\end{table*}


\subsection{Error analysis prompting with ChatGPT is better than standard prompting at the segment level}

To improve the segment level evaluation capabilities of ChatGPT, we combine the idea of Chain-of-Thought \citep{wei2022chain} and Error Analysis \citep{lu2022toward}. Chain-of-Thought has been successfully applied in complex reasoning tasks, which encourages the LLM to break down the task into a series of reasoning steps, allowing it to better understand the context and formulate a more accurate response. Error analysis strategy \citep{lu2022toward} aims to generate human-like evaluation by incorporating human evaluation framework, e.g. MQM \citep{freitag-etal-2021-experts}, into existing metrics to obtain better discriminating ability for errors, e.g., lexical choice~\cite{ding2021} or adequacy~\cite{popovic2020} errors. Specifically, we instruct ChatGPT to identify major and minor errors in the translation, and then enable ChatGPT to score the translation based on the severity of errors.

 Figure~\ref{fig:incontext} compares the segment level results between different prompting strategies. Prompting ChatGPT with error analysis can benefit translation evaluation between segments by improving Kendall correlation by a large margin (26.01 vs 36.73). However, simply replacing prompting instruction from scoring with zero-shot EA will even damage the performance, since identifying errors without samples will make ChatGPT become more unstable. This also highlights the importance of prompting with in-context examples.

Moreover, on text-davinci-003, the improvements from EA prompting are shown in the zero-shot scenario ("zero-shot EA"). The reason for this may be that while text-davinci-003 is capable of detecting errors when prompted with explicit instructions for error analysis, it may face challenges in fully comprehending the task of error analysis when presented with in-context examples. Compared with text-davinci-003, ChatGPT has been trained using reinforcement learning through human feedback and conversational tuning, which enables it to generalize to error analysis through in-context examples.

\subsection{Error analysis prompting empowers ChatGPT to produce human-like evaluations} \label{sec:diffprompts}

Given the crucial significance of the prompt design, we explore several versions of in-context prompt contexts and present an analysis in Table~\ref{tab:prompts_selection}. See Appendix~\ref{sec:appendix_testprompt} for the prompt contexts used in our experiment. We find that:

\paragraph{(i) ChatGPT becomes more adept at identifying errors when instructed by error analysis.} When designing in-context examples, is it worth designing an instruction on error analysis or simply adopting standard scoring instruction? We find that error analysis instructions can make ChatGPT better understand the task of error identification in the best setting, since more errors are identified compared with standard instructions (28 vs 17 in total). As a result, We recommend using error analysis instructions in prompt contexts instead of standard instruction.

\paragraph{(ii) Itemized template response is better than detailed illustration.} As shown in the "Response" column, providing descriptions of errors in detail hinders ChatGPT's ability to accurately identify errors. For example, when using error analysis with combined instruction, ChatGPT with itemized response can identify more errors than detailed response (28 vs 10 in total). It seems that lengthy paragraphs will make it difficult for ChatGPT to fully comprehend the details of each error, potentially leading to confusion regarding the intended task. Therefore, it is recommended to provide clear and concise descriptions of errors in a format that is easy for ChatGPT to process and comprehend.

\paragraph{(iii) Separating the scoring process from error identification may improve the stability of ChatGPT.} We suggest splitting the instruction of error analysis into two queries, one for identifying errors and the other for scoring the translation. Although this may not cause a significant performance gain, we observe that sometimes ChatGPT fails to deduct points for identified errors or presents an incorrect calculation of scores. Separating the scoring process may be helpful, as it allows ChatGPT to focus on one single procedure in each query, thus can provide more accurate judgments.

\section{Case Study} \label{sec:casestudy}
In this section, we list several typical issues with the case study that should be aware of when using ChatGPT as a translation evaluator.
\subsection{ChatGPT is unstable when conducting evaluation process}

\begin{figure}[t]
\centering
\includegraphics[scale=0.46]{figs/case_unstable.png}
\caption{When evaluating the same translation three times, ChatGPT generates similar explanations but different scores.}\label{fig:inconsistency}
\end{figure}

When assessing translations using ChatGPT, it is not uncommon to observe variations in the scores assigned to the same input. As shown in Figure~\ref{fig:inconsistency}, we regenerate several responses with the same input and obtain 3 different scores (98, 95, 100) for the translation. The discrepancies in scores could be attributed to the inherent randomness of the model behind ChatGPT. Another possible reason is the lack of clearly stated evaluation criteria described in the prompt contexts. Therefore, we suggest using specific guidelines such as the template we propose to minimize the impact of these variations.

\subsection{ChatGPT prefers former inputs when provided with multiple translations}

\begin{figure}[t]
\centering
\includegraphics[scale=0.69]{figs/case_multiple.pdf}
\caption{Comparison of providing multiple translations in sequential or reverse order. ChatGPT tends to prefer the former translations and generate contradictory judgments.}\label{fig:input_bias}
\end{figure}

An interesting phenomenon is that when multiple translations are presented together as a single input to ChatGPT for evaluation, it tends to believe that the translations provided earlier are of higher quality, while the quality of later translations are relatively poorer. 

Figure~\ref{fig:input_bias} shows an example of the attack on ChatGPT. We provide 8 translations along with their corresponding source and reference sentences. First, we present the translations sequentially, and ask ChatGPT to rank them according to their translation quality. ChatGPT ranks the translations as (SYS1, SYS2, SYS4, SYS5, SYS3, SYS6, SYS7, SYS8), with SYS1 being the best translation and SYS8 being the worst. Then, we reverse the order of translations and obtain an entirely different sequence of ranks - (SYS8, SYS7, SYS6, SYS5, SYS4, SYS3, SYS2, SYS1), with SYS8 being the best translation and SYS1 being the worst. 

The contradictory results may be attributed to the auto-regressive nature of the decoder model, which gives more attention to the latter input, potentially leading to greater identification of errors for the translation input later. Therefore, we recommend that researchers input one translation at a time instead of providing multiple translations. 

\subsection{ChatGPT may directly adopt existing evaluation metrics}

\begin{figure}[t]

\includegraphics[scale=0.36]{figs/case_bleu.jpg}
\centering
\caption{An example on ChatGPT directly adopting BLEU to evaluate translation quality.  }
\label{fig:nobleu}
\end{figure}

We observe that with standard prompting, sometimes ChatGPT directly adopts existing evaluation metrics, such as BLEU and METEOR. An example of this behavior is in Figure~\ref{fig:nobleu}.

However, as our objective is to examine ChatGPT's inherent capacity for translation evaluation, rather than its ability to implement pre-existing evaluation procedures, we include an explicit instruction of "Do not use existing metrics" in standard prompting. This encourages ChatGPT to develop its own approach to evaluating translations, independent of existing metrics.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we explore the potential of ChatGPT as a metric for evaluating translations. We design a novel in-context prompting strategy based on chain-of-thought and error analysis, and show that this strategy significantly improves ChatGPT's evaluation performance. We compare our approach with other prompt designs to show the effectiveness of error analysis. We hope the experience can benefit NLP researchers in developing more reliable promoting strategies. In section~\ref{sec:casestudy}, we also highlight several potential issues that researchers should be aware of when using ChatGPT as a translation evaluator.

In future work, we would like to experiment with our method on more test sets and top-performed systems~\cite{wmt19,iwslt21,ding2021iwslt,wmt22,vegamt}, to make our conclusion more convincing. Also, it is worth exploring the reference-free settings, i.e., quality estimation~\cite{specia2010machine,qiu2022original} evaluation performance, with our proposed error analysis prompting. 
Lastly, it will be interesting to automatically generate the samples in our few-shot error analysis prompting strategy.

\section*{Limitations}
All experiments in this paper were conducted using the interaction screen of ChatGPT. As a result, the test set used in this study is limited. We will conduct more experiments in future work to further validate and refine our current results and findings.

\bibliography{anthology,arxiv_version}
\bibliographystyle{acl_natbib}

\appendix
% \onecolumn

\section{Prompt Contexts} \label{sec:appendix_testprompt}
Figure~\ref{fig:bestprompt} compares the prompt contexts implemented in error analysis prompting with a detailed response and combined instruction discussed in Section~\ref{sec:diffprompts}.

\begin{figure*}[h]
\includegraphics[scale=0.70]{figs/prompt_context.pdf}
\centering
\caption{A comparison between our proposed error analysis prompting and other prompt contexts.}
\label{fig:bestprompt}
\end{figure*}

\end{document}

