
\pdfoutput=1


\documentclass[11pt, dvipsnames]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{colortbl}
\usepackage{color}

\usepackage{tabularx}
\usepackage{dingbat}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{stfloats}
\usepackage{subfig}
\usepackage[T1]{fontenc}
\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{fontawesome5}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{inconsolata}
\usepackage{arydshln}
\usepackage{CJKutf8}
% \usepackage{subcaption}

\newcommand{\cmark}{{\textbf{\textcolor[rgb]{0.1, 0.5, 0.1}{\ding{51}}}}}
\newcommand{\xmark}{{\textbf{\color{red}{\ding{55}}}}}

\def\emojihappy{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_happy.jpeg}}}
\def\emojineutral{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_neutral.jpeg}}}
\def\emojithink{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_think.jpeg}}}
\def\emojiwarning{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_attention.jpeg}}}

\definecolor{myblue}{rgb}{0.82, 0.94, 0.75}

\title{Error Analysis Prompting Enables Human-Like Translation Evaluation\\ in Large Language Models: A Case Study on ChatGPT}

\author{Qingyu~Lu$^{\diamondsuit, \Re}$,
\ \textbf{
\ Baopu Qiu$^{\flat, \Re}$,
\ Liang Ding$^{\Re}$, 
\ Kanjian Zhang$^{\diamondsuit}$,
\ Tom Kocmi$^{\heartsuit}$,
\ Dacheng Tao$^{\Re}$} \\
\ $^{\diamondsuit}$Southeast University
\ $^{\Re}$JD Explore Academy, JD.com Inc.
\ $^{\flat}$Nanjing University
\ $^{\heartsuit}$Microsoft\\
\includegraphics[scale=0.15]{figs/emoji_mail.png} \texttt{luqingyu@seu.edu.cn}, \texttt{qiubaopu@smail.nju.edu.cn}, \\ \texttt{liangding.liam@gmail.com},
\texttt{tomkocmi@microsoft.com}\\
\includegraphics[scale=0.09]{figs/tool.jpeg} \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}
}

\begin{document}
\maketitle
\begin{abstract}

Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research~\citep{kocmi2023llmfindings} has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}.
To further improve the performance of LLMs on MT quality assessment, we conduct an investigation into several prompting methods, and propose a new prompting method called \textbf{\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts~\cite{wei2022chain} and Error Analysis~\cite{lu2022toward}. Our results on \texttt{WMT22} indicate that prompting LLMs like ChatGPT with error analysis can \textit{generate human-like MT evaluations at both the system and segment level}.
Additionally, we first discover some limitations of ChatGPT as an MT evaluator, such as changing the order of input may significantly influence the judgment when providing multiple translations in a single query. This work provides a preliminary experience of prompting LLMs as an evaluator to improve the reliability of translation evaluation metrics under the error analysis paradigm.
% The project can be found in \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}.

\end{abstract}

\section{Introduction}

Large language models (LLMs), especially Generative Pre-trained Transformer (GPT) models~\cite{radford2019language, brown2020language}, have shown remarkable performance in various natural language processing (NLP) tasks. Recently, Open AI developed ChatGPT, an interactive chatbot built upon InstructGPT~\citep{ouyang2022training}, which has captured the attention of researchers in the NLP community~\cite{qin2023chatgpt,zhong2023chat}. This chatbot is capable of integrating multiple NLP tasks and can generate detailed and comprehensive responses to human inquiries. Additionally, it can respond appropriately to follow-up questions and maintain sensitivity throughout several turns of conversation.

Previous research has demonstrated that ChatGPT can perform as well as or even better than other LLMs in machine translation task~\cite{hendy2023good,jiao2023chatgpt,Peng2023ChatGPT4MT}. However, it remains uncertain whether ChatGPT can be used as a metric to evaluate the quality of translations. If ChatGPT is suitable for this task, then, how to develop appropriate prompts that can make ChatGPT generate reliable evaluations?
Concurrent to our work, GEMBA \citep{kocmi2023llmfindings} present an encouraging finding that LLMs, e.g., ChatGPT, could outperform current best MT metrics at the system level quality assessment with zero-shot standard prompting, but such kind of prompts show unreliable performance at the segment level.

\begin{figure*}[t]

\includegraphics[scale=0.38]{figs/overview.pdf}
\centering
\caption{An comparative overview between Standard Prompting and our proposed Error Analysis Prompting in assessing the MT quality with ChatGPT.}
\label{fig:overview}
\end{figure*}

In this work, we take the further step by investigating the advanced few-shot prompting strategies upon ChatGPT for MT quality assessment, and propose a novel prompting strategy -- \textbf{Error Analysis Prompting} (EAPrompt), combining the Chain-of-Thought (CoT, \citet{wei2022chain}) and Error Analysis (EA, \citet{lu2022toward}). 
We give an example of EAPrompt in Figure~\ref{fig:overview}. The idea is to prompt ChatGPT to generate a human-like evaluation like MQM \citep{freitag-etal-2021-experts} by \ding{182} \textit{identifying major and minor errors}, and \ding{183} \textit{scoring the translations according to the severity of these errors}.

We conduct experiments on 122,823 segments on 64 MT systems across various domains to verify the effectiveness of our approach, and find that:

% In addition, we also explore the potential of LLMs compared with modern neural metrics like COMET \citep{rei-etal-2020-comet}, BERTScore \citep{zhang2020bertscore} and BLEURT \citep{sellam-etal-2020-bleurt}.

% Our experiments and analyses illustrate that: 

\begin{itemize}
    \item[\emojihappy] Our proposed EAPrompt outperforms standard prompting \citep{kocmi2023llmfindings} at both the system level and the segment level, achieving human-like evaluations on LLMs such as \texttt{gpt-3.5-turbo} (Turbo). Notably, EAPrompt is also reliable when evaluating top-performing MT systems.
    \item[\emojihappy] EAPrompt on Turbo achieves nearly the performance of GEMBA on GPT-4. We believe if EAPrompt were applied to GPT-4, it could potentially outperform the GEMBA variants.
    % outperforms standard prompting \citep{kocmi2023llmfindings} at both the system level and the segment level, achieving human-like evaluations on LLMs such as gpt-3.5-turbo.
    \item[\emojithink] When designing prompts, itemized responses are better than lengthy and detailed explanations of errors. Moreover, splitting the instruction into two identifying errors and scoring translation can improve evaluation stability.
    % \item[\emojineutral]  The boosted performance from EA prompting is observed in the zero-shot scenario on \texttt{text-davinci-003} rather than in the few-shot scenario, which indicates that we need to adjust our settings when utilizing other GPT models.
    \item[\emojineutral] EAPrompt may have a detrimental effect on \texttt{text-davinci-003} (Dav3) as an evaluator. This discrepancy may be attributed to Dav3's relatively inferior CoT capability in contrast to Turbo.
    \item[\emojiwarning] Despite its good performance, we show that ChatGPT may be unreliable at evaluating high-quality MT systems, and may score the same translation differently.
    \item[\emojiwarning] It is NOT advisable to combine multiple translations into a single query input, as ChatGPT has a preference for former translations. 
\end{itemize}

% This work provides a preliminary experience of prompting LLMs as an evaluator with error analysis. We hope this approach can improve the reliability of translation evaluation metrics, and also can be extended to other evaluation scenarios of natural language generation, such as text summarization, data-to-text tasks.

This study provides an initial exploration of utilizing error analysis to prompt LLMs as evaluators. The proposed approach holds the promise of enhancing the reliability of translation evaluation metrics. Furthermore, this methodology can also be extended to benefit other evaluation scenarios within natural language generation, including text summarization and data-to-text tasks.

% The remainder of this report is designed as follows. We introduce the details on prompting ChatGPT with Error Analysis in Section~\ref{sec:evaluation}.
% present the evaluation settings and comparative results in Section~\ref{sec:evaluation}. In Section~\ref{sec:casestudy}, we highlight several potential issues that researchers should be aware of when using ChatGPT as a translation evaluator. Conclusions are described in Section~\ref{sec:conclusion}.

\section{Prompt ChatGPT with Error Analysis}
\label{sec:evaluation}

\subsection{Translation Evaluation Metric}

Translation evaluation metrics are used to assess the performance of machine translation systems on specific test sets \citep{freitag-etal-2022-results, mathur-etal-2020-results}. Modern evaluation metrics often leverage pre-trained language models to enhance reliability \citep{kocmi-etal-2021-ship}. These metrics typically take inputs from three sources: the sentence from source language ("Source"), the reference translation provided by human translators ("Reference"), and the hypothesis being evaluated ("Translation"). In scenarios where reference signals are not provided, this "reference-less" metric can also be utilized for quality estimation purposes \citep{zerva-etal-2022-findings, specia2010machine, qiu2022original}. The output of the metric is a score or rank indicating the translation quality of each hypothesis.

To test the reliability of metrics, we use human evaluation as the golden standard. A common approach for collecting human evaluation is Direct Assessment (DA, \citet{graham2017can}), which evaluate the sentence score ranging from 0~100. Multi-dimensional Quality Metric (MQM) is adopted recently in WMT as a high-quality human evaluation strategy \citep{freitag-etal-2021-experts}. It asks human experts to annotate the errors in the hypothesis and categorize them into "Major" and "Minor" indicating their severity.

\subsection{Prompt LLMs as Evaluation Metrics}

When prompting an LLM as an evaluation metric, it is crucial to design appropriate instructions that describe the evaluation task and the scoring range. In this paper, we mainly adopt two prompting strategies: "Standard Prompting" and "Error Analysis Prompting".

The standard prompting approach directly asks LLMs to generate a score that reflects the quality of the translation. In a recent study, GEMBA \citep{kocmi2023llmfindings} adopted four different standard prompting techniques, demonstrating its state-of-the-art at the system level when compared to other model-based metrics. However, they also observe that the performance at the segment level is relatively poorer. This highlights the importance of combining Chain-of-Thought with the Error Analysis Strategy to prompt LLMs in a manner that more closely resembles human evaluation.

\subsection{Error Analysis Prompting}

Motivated by the MQM framework in human evaluation, the idea of the error Analysis (EA) paradigm, as introduced by \citet{lu2022toward}, is to enhance the automatic scoring process by explicitly incorporating error identification, thus providing a more human-like evaluation.

The Chain-of-Thought (CoT) prompting strategy was first proposed by \citet{wei2022chain}. Instead of directly generating the answer, CoT prompts LLMs to think step-by-step. This approach has shown significant performance improvements on reasoning tasks, such as GSM8K \citep{cobbe2021training}. CoT is an emergent ability of LLMs and has been incorporated in instruction fine-tuning of LLMs \citep{chung2022scaling} as well as in benchmarks designed to evaluate LLM capabilities \citep{suzgun2022challenging}.

In this work, we combine the CoT and EA paradigms, introducing a novel prompting strategy called Error Analysis Prompting (EAPrompt). As shown in Figure~\ref{fig:overview}, EAPrompt divides the scoring process into two stages: First, the LLM is instructed to identify major and minor errors in the translation ("Instruction: Identify Errors"). Subsequently, the number of these two types of errors is counted, and the final score is computed ("Instruction: Score Translation"). Distinguished from standard prompting, EAPrompt provides a more detailed and human-like evaluation approach.

After exploring several prompt contexts in initial experiments, we made the following modifications to EAPrompt as follows:

\begin{itemize}
  \item we adopt the one-shot learning format \citep{brown2020language} to enhance the LLMs' understanding of the task; different in-context examples are used for different language pairs;
  \item we employ itemized template response, enabling clearer identification and quantification of errors;
  \item we divide the scoring process into two stages, enhancing the stability of LLMs during each query and avoiding potential inconsistencies.
\end{itemize}

We present a thorough analysis of the prompt variants' impact in \S\ref{sec:diffprompts}. The specific prompt contexts we utilize can be found in Appendix \ref{sec:appendix_testprompt}.

\begin{table*}[ht]
\centering
\begin{tabularx}{0.94\textwidth}{ccccX}
\toprule[0.5mm]
\textbf{Dataset} & \textbf{Language Pair} & \textbf{Segments} & \textbf{Systems} & \textbf{Domains} \\\midrule
\multirow{3}{*}{WMT22} & En-De & 2037 & 15 & conversational, e-commerce, news, social \\
% \small{Online-B, bleurt\_bestmbr, M2M100\_1.2B-B4, Online-G, PROMT, Online-W, OpenNMT, Lan-Bridge, Online-A, QUARTZ\_TuneReranking, Online-Y, bleu\_bestmbr, comet\_bestmbr, JDExploreAcademy, chrf\_bestmbr} \\
 & En-Ru & 2037 & 16 & conversational, e-commerce, news, social
\\
%\small{HuaweiTSC,JDExploreAcademy,Lan-Bridge,M2M100\_1.2B-B4,Online-A,Online-B,Online-G,Online-W,Online-Y,PROMT,QUARTZ\_TuneReranking,SRPOL,bleu\_bestmbr,comet\_bestmbr,eTranslation,chrf\_bestmbr} \\
 & Zh-En & 1875 & 18 & conversational, e-commerce, news, social \\\cmidrule(lr){1-5}
%\small{Lan-Bridge,JDExploreAcademy,M2M100\_1.2B-B4,Online-Y,AISP-SJTU,comet\_bestmbr,Online-W,LanguageX,HuaweiTSC,Online-B,bleu\_bestmbr,Online-A,bleurt\_bestmbr,Online-G,DLUT,NiuTrans,QUARTZ\_TuneReranking,chrf\_bestmbr} \\
\multirow{2}{*}{WMT20} & En-De & 1418 & 7 & news \\
%\small{ eTranslation.737,Huoshan\_Translate.832,Online-A.1574,Online-B.1590,OPPO.1535,Tencent\_Translation.1520,Tohoku-AIP-NTT.890} \\
 & Zh-En & 2000 & 8 & news \\ %\small{DeepMind.381,DiDi\_NLP.401,Huoshan\_Translate.919,Online-B.1605,OPPO.1422,Tencent\_Translation.1249,THUNLP.1498,WeChat\_AI.1525} \\

\bottomrule[0.5mm]
\end{tabularx}
\caption{Statistics of testset. Source, reference texts, and translations are from the WMT20 and WMT22 metrics shared task. }
\label{tab:experiments}
\end{table*}

\section{Experimental Results}\label{sec:results}

\subsection{Experiment Setup}

\paragraph{Dataset}

We utilize the test set from the WMT22 shared tasks \citep{freitag-etal-2022-results} in three language pairs: English-German (\texttt{En-De}), English-Russian (\texttt{En-Ru}), and Chinese-English (\texttt{Zh-En}). In addition, we present results from WMT20 shared tasks \citep{mathur-etal-2020-results} in Appendix~\ref{sec:appendix_wmt20result}\footnote{Since there might be potential contamination of the WMT20 test set in GPT training, we exclude these results from the main findings.}. Table~\ref{tab:experiments} provides detailed information about our test set. Compared to the WMT20 test set (news domain only), the WMT22 testset consists of samples from 4 domains - conversational, e-commerce, news, and social. Since the training data of LLMs from OpenAI we use were up to Sep 2021, this significantly reduces the chance of testset leakage, making our analyses more convincible.

\paragraph{Human Evaluation} 

Human evaluation of translated texts is widely considered to be the gold standard in evaluating metrics. We use a high-quality human evaluation dataset Multi-dimensional Quality Metrics (MQM, \citet{freitag-etal-2021-experts}) as human judgments. This dataset is annotated by human experts and has been widely adopted in recent translation evaluation \citep{freitag-etal-2022-results} and quality estimation tasks \citep{zerva-etal-2022-findings} in WMT.

\paragraph{Meta Evaluation} 

We utilize the system-level pairwise accuracy of system-ranking \citep{kocmi-etal-2021-ship}. At the segment level, we follow \citet{freitag-etal-2022-results} to adopt the average of three types of Kendall correlation. Specifically, these values are computed by flattening the scores into a single vector and calculating the average correlations over systems, or over segments. To avoid skewing the results, we compute the pairwise accuracy for MT systems across all three language pairs as the final performance at the system level. We compute the average Kendall correlation for the segment-level results across all language pairs. In order to maintain consistency and comparability with other metrics, all the meta-evaluation are calculated with \texttt{MTME}\footnote{ \url{https://github.com/google-research/mt-metrics-eval}}, a metric evaluation tool recommended by WMT22 \citep{freitag-etal-2022-results}.

\subsection{Baselines and Large Language Models} 

\paragraph{Baseline Metrics}

Given the reported unreliability of \textbf{BLEU} \citep{papineni-etal-2002-bleu} in WMT22, we compare LLMs with several commonly used model-based metrics for MT evaluation. \textbf{BLEURT} \citep{sellam-etal-2020-bleurt} and \textbf{COMET} \citep{rei-etal-2020-comet} are supervised neural metrics that leverage human judgments to train. \textbf{UniTE} \citep{wan-etal-2022-unite} is a learnt metric that evaluates MT outputs combining three different evaluation scenarios. \textbf{MetricX XXL} \citep{freitag-etal-2022-results} is a large-scale multi-task metric that fine-tunes LLM checkpoints using diverse human feedback data. For reference-less metrics, we reproduce \textbf{COMET-QE} \citep{rei-etal-2021-references}, which was one of the top-performing metrics in WMT21. These metrics have shown a strong correlation with human judgments. 

\paragraph{Large Language Models} We assess the evaluation capability of LLMs via OpenAI API\footnote{\url{https://platform.openai.com/}}. We mainly involve two models from the GPT3.5 family: gpt-3.5-turbo ("\textbf{Turbo}") and text-davinci-003 ("\textbf{Dav3}"). Both models are selected due to their proximity in capabilities to ChatGPT, since the internal model behind ChatGPT is unknown. We compare our proposed approach with the GPT-4 model ("\textbf{GPT-4}") on GEMBA, to see if the performance on \textbf{Turbo} with EAPrompt could approach this powerful LLM.

\subsection{Experimental Results}

\begin{table*}[ht]
\centering
\begin{tabular}{llllllllll}
\toprule[0.5mm]
 &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{En-Ru}} & \multicolumn{2}{c}{\textbf{Zh-En}} & \multicolumn{2}{c}{\textbf{Overall}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
          \textbf{Models} &    \textbf{Metrics / Prompts} &        \textbf{SYS} &         \textbf{SEG} &         \textbf{SYS} &         \textbf{SEG} &         \textbf{SYS} &         \textbf{SEG} &   \textbf{SYS} &         \textbf{SEG} \\\midrule
       \textbf{Baselines} &          MetricX XXL &       \textbf{76.9} &         36.0 &       \textbf{91.4} &         \underline{\textbf{42.0}} &       84.6 &       42.7 &  \textbf{85.0} & \underline{\textbf{40.2}} \\
              &             BLEURT20 &       \textbf{76.9} &       34.4 &       90.5 &       35.9 &       84.6 &       36.1 &  84.7 & 35.5        \\
              &              COMET22 &       \textbf{76.9} &       36.8 &       86.7 &         40.0 &       \textbf{86.8} &       \underline{\textbf{42.8}} &  83.9 & 39.9         \\
              &                UniTE &       74.4 &       \underline{\textbf{36.9}} &       87.6 &       37.8 &       84.6 &       35.7 &       82.8 & 36.8        \\
            &      COMET-QE[noref] &       71.8 &       28.1 &         80.0 &       34.1 &       81.3 &       36.5 &       78.1 & 32.9        \\\midrule
    \textbf{Dav3} &                   GEMBA-DA &       92.3 &       \textbf{30.6} &       85.7 &       \textbf{33.2} &       \textbf{86.8} &       \textbf{37.1} &  \textbf{88.0} & \textbf{33.6}          \\\rowcolor{orange!20}
              &        \textbf{EAPrompt} &       67.9 &         20.0 &       87.6 &       25.5 &       79.1 &       22.9 &      79.6 & 22.8        \\
              &            GEMBA-DA[noref] &       87.2 &         18.0 &       \textbf{88.6} &       25.8 &       82.4 &       28.9 &  86.1 & 24.2        \\\rowcolor{orange!20}
              & \textbf{EAPrompt[noref]} &       39.7 &       12.7 &         80.0 &       23.1 &       74.7 &       17.6 &       67.9 & 17.8        \\\midrule
   \textbf{Turbo}  &               GEMBA-Stars &       \textbf{89.7} &       25.9 &       90.5 &       22.3 &       87.9 &       26.5 &       89.4 & 24.9        \\\rowcolor{orange!20}
              &        \textbf{EAPrompt} &       \textbf{89.7} &       \textbf{32.4} &       \textbf{92.4} &       \textbf{32.3} &       \underline{\textbf{90.1}} &       \textbf{36.2} &  \textbf{90.9} & \textbf{33.6}        \\
              &           GEMBA-SQM[noref] &       \textbf{89.7} &       25.9 &       91.4 &       30.9 &       81.3 &       29.1 & 87.6 & 28.6        \\\rowcolor{orange!20}
              & \textbf{EAPrompt[noref]} &       87.2 &       28.4 &       90.5 &       30.1 &       \underline{\textbf{90.1}} &       32.5 &  89.4 & 30.3        \\\midrule
           \textbf{GPT-4} &               GEMBA-Stars &       89.7 &       \textbf{32.6} &       \underline{\textbf{94.3}} &       35.1 &         \textbf{89.0} &       \textbf{38.2} &     \textbf{\textbf{91.2}} & \textbf{35.3}        \\
              &       GEMBA-Classes[noref] &       \underline{\textbf{92.3}} &       30.4 &       92.4 &         \textbf{39.0} &         \textbf{89.0} &       31.3 &  \textbf{\textbf{91.2}} & 33.6        \\

\bottomrule[0.5mm]
\end{tabular}
\caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores, respectively. The best results among the same model are highlighted in \textbf{bold}. The best results among all metrics are \underline{underlined}. Our proposed EAPrompt results are highlighted in \colorbox{orange!20}{orange}.}
\label{tab:mainres}
\end{table*}


% \bottomrule
% \multicolumn{7}{c}{\textbf{WMT22}}                                                                                                    \\
%                  &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{En-Ru}} & \multicolumn{2}{c}{\textbf{Zh-En}} & \multicolumn{2}{c}{\textbf{Overall}} \\
% \midrule
% Models           & Metrics / Prompts & SYS & SEG & SYS & SEG & SYS & SEG & SYS & SEG(avg.) \\
% \midrule
% BaseLine        & MetricX XXL & 76.9 & 36.0 & 91.4 & 42.0 & 84.6 & 42.7 & 40.2 & 85.0             \\
%                  & BLEURT20   & 76.9 & 34.4 & 90.5 & 35.9 & 84.6 & 36.1 & 35.5 & 84.7             \\
%                  & COMET22    & 36.8            & 40             & 42.8           & 39.9                & 83.9           \\
%                  & COMET-QE{[}noref{]}      & 28.1           & 34.1           & 36.5           & 32.9                & 78.1           \\
%                  & UniTE                    & 36.9           & 37.8           & 35.7           & 36.8                & 82.8           \\
% \midrule
% Turbo    & DA                       & 25             & 23.4           & 25.5           & 24.6                & 86.5           \\
%                  & Stars                    & 25.9           & 22.3           & 26.5           & 24.9                & 89.4           \\
%                  & SQM                      & 29.8           & 27.7           & 31.3           & 29.6                & 87.2           \\
%                  & Classes                  & 17             & 16.7           & 17.8           & 17.2                & 82.5           \\
%                  & Ours-EAPrompt            & 32.4           & 32.3           & 36.2           & 33.6                & 90.9           \\
%                  & DA{[}noref{]}            & 25.5           & 29.4           & 26.4           & 27.1                & 86.9           \\
%                  & Stars{[}noref{]}         & 25.5           & 27.9           & 26.1           & 26.5                & 84.3           \\
%                  & SQM{[}noref{]}           & 25.9           & 30.9           & 29.1           & 28.6                & 87.6           \\
%                  & Classes{[}noref{]}       & -1.0           & 2.7            & 2.9            & 1.5                 & 62.0           \\
%                  & Ours-EAPrompt{[}noref{]} & 28.4           & 30.1           & 32.5           & 30.3                & 89.4           \\


% \begin{table*}[ht]
% \centering
% {
% \begin{tabular}{lcccc}
% \toprule[0.5mm] 
% \multirow{2}{*}{\textbf{Metrics}}  & \multicolumn{2}{c}{\textbf{En-De}}  & \multicolumn{2}{c}{\textbf{Zh-En}}\\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5}
% & \textbf{System(\%)} & \textbf{Segment(\%)} & \textbf{System(\%)} & \textbf{Segment(\%)} \\ \midrule
% BLEU \citep{papineni-etal-2002-bleu}  &71.43 &3.55    &21.43    &14.71     \\ 
% BERTscore \citep{zhang2020bertscore}  &\textbf{76.19} &12.30    &25.00    &26.75     \\ 
% BLEURT \citep{sellam-etal-2020-bleurt}&\textbf{76.19} &33.44   &57.14    &32.76  \\ 
% COMET \citep{rei-etal-2020-comet}     &71.43 &\textbf{33.47}   &50.00    &\textbf{38.97}   \\
% \midrule
% text-davinci-003              &42.86 &11.86   &53.57    &23.08   \\
% ChatGPT-EA                               &\textbf{76.19} &26.40    &\textbf{60.71}    &36.73   \\
% \bottomrule[0.5mm]
% \end{tabular}}
% \caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. The best results are \textbf{bold}.}
% \label{tab:mainres}
% \end{table*}


% \begin{figure}[t]
%   \centering
%   \includegraphics[scale=0.33]{figs/segment_compare.pdf}
%   \caption{ The segment level comparison between ChatGPT and text-davinci-003 with standard prompting ("Standard"), in-context error analysis prompting ("EA"), and zero-shot error analysis prompting ("zero-shot EA"). }
%   \label{fig:incontext}
% \end{figure}

We compute the system and segment level performance of EAPrompt with LLMs in Table~\ref{tab:mainres}. Full results of WMT20 and WMT22 are presented in Appendix~\ref{sec:appendix_wmt20result}. We can see that: 

\paragraph{(i) EAPrompt achieves state-of-the-art performance for Turbo at both the system level and segment level.} 

Consistent with the findings of \citet{kocmi2023llmfindings}, LLMs  achieve SOTA performance across all three language pairs at the system level. Notably, EAPrompt further enhances \textbf{Turbo}'s performance compared to other prompting strategies, achieving an overall score of 90.9\% as opposed to GEMBA-DA's 89.4\% in terms of pairwise accuracy.

At the segment level, despite previous findings by \citet{kocmi2023llmfindings} regarding the poor correlation between LLMs as evaluators and human judgments, EAPrompt surpasses GEMBA-DA's performance on Turbo by a significant margin, averaging 9.7\% improvement. This result verifies the effectiveness of EAPrompt when used with LLMs such as ChatGPT.

\paragraph{(ii) EAPrompt on Turbo approaches the performance of GPT-4.}

We also compare EAPrompt on \textbf{Turbo} with prompting \textbf{GPT-4} with GEMBA. An interesting finding is that the difference between EAPrompt on \textbf{Turbo} (90.9 - system level, 33.6 - segment level) and GEMBA-Stars on \textbf{GPT-4} (91.2 - system level, 35.3 - segment level) is negligible. This finding suggests that if EAPrompt were applied to \textbf{GPT-4}, it could potentially outperform the GEMBA variants. This belief stems from the fact that GEMBA mainly leverages the DA scoring technique, aiming to avoid using test sets as development sets in \textbf{GPT-4}. Consequently, EAPrompt appears to be better suited for ChatGPT models as the translation evaluator.

\paragraph{(iii) EAPrompt also boosts Turbo's performance in reference-less scenarios.}

Our findings remain consistent in reference-less settings ("[noref]"), where EAPrompt[noref] on \textbf{Turbo} shows improved performance compared to GEMBA, with an increase of 1.8\% at the system level and 1.7\% at the segment level.
Notably, EAPrompt[noref] on \textbf{Turbo} surpasses existing reference-less metrics and even reference-based metrics at the system level. These results indicate that LLMs possess impressive cross-lingual capabilities, and that are well-suited for quality estimation under EAPrompt, where the absence of reference translations poses a significant challenge.

\paragraph{(iv) EAPrompt may have a detrimental effect on Dav3 as an evaluator.}

EAPrompt with \textbf{Dav3} does not exhibit the same level of effectiveness as observed with \textbf{Turbo}. This discrepancy may be attributed to \textbf{Dav3}'s relatively inferior CoT capability in contrast to \textbf{Turbo}. A standard prompting strategy like GEMBA-DA may be more suitable, while the \textbf{Turbo} model may be better suited for dialog completion, likely excels in CoT and aligns well with our two-turn EAPrompt approach. Another suspicion is that a stricter profanity filter was applied in \textbf{Dav3}, leading to instances where the model returns "None" outputs.

\paragraph{} It is important to note that the results for WMT20 are also provided in Appendix~\ref{sec:appendix_wmt20result}. These results largely mirror the findings from WMT22, reinforcing the consistency of our observations across different evaluation settings.

\section{Analysis}

\begin{table*}[ht]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccccccccccccccc}
\toprule[0.5mm] 
\multicolumn{2}{c}{\textbf{Instruction}} & \multicolumn{2}{c}{\textbf{Response}} & \textbf{Separation} & \multicolumn{3}{c}{\textbf{Human}} & \multicolumn{3}{c}{\textbf{Turbo}} & \multicolumn{3}{c}{\textbf{Dav3}} & \multicolumn{3}{c}{\textbf{GPT-3.5}(Interface)}  & \multicolumn{3}{c}{\textbf{GPT-4}(Interface)}  \\ 
\cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
Standard & EA   & Detailed      & Itemized      &           
& Maj & Min & All & Maj & Min & All & Maj & Min & All & Maj & Min & All & Maj & Min & All   \\ \midrule

\cmark &    &  - & -      &    -        &  
\multirow{4}{*}{1} & \multirow{4}{*}{1} & \multirow{4}{*}{-6} & - & - & 70 & - & - & 85 & - & - & 90 & - & - & 70      \\
 & \cmark    &  \cmark &      &    \xmark       &  &  &  &
0 & 2 & -2 & 1 & 1 & -6 & 1 & 2 & -7 & 1 & 1 & -6      \\
 & \cmark    &  &  \cmark      &    \xmark        & & & &
1 & 0 & -5 & 1 & 1 & -6 & 0 & 2 & -2 & 1 & 0 & -5      \\
 & \cmark    &  &  \cmark     &    \cmark        &  & & &
2 & 0 & -10 & 1 & 0 & -5 & 2 & 0 & -10 & 1 & 1 & -6     \\


\bottomrule[0.5mm]
\end{tabular}}
\caption{Comparison of the segment level scores of ChatGPT for different variants of prompts. Instructions are divided into standard prompting and EA. The response can either be itemized or detailed. The instruction could be separated into two queries (one for identifying errors and another for scoring) or combined into a single query. "Maj", "Min", and "All" represent the number of major errors, minor errors and final score, respectively.} 
 \label{tab:prompts_selection}
\end{table*}

\subsection{The reliability of ChatGPT when evaluating top-performance systems}

% \begin{figure*}[t]
%   \centering
  
%   \begin{minipage}{0.3\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/topk/SEG-en-de.png}
%     \caption{1}
%   \end{minipage}
%   \begin{minipage}{0.3\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/topk/SEG-en-ru.png}
%     \caption{2}
%   \end{minipage}
%   \begin{minipage}{0.3\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/topk/SEG-zh-en.png}
%     \caption{2}
%   \end{minipage}
% \centering
% \caption{Top-k}
% \label{fig:topk}
% \end{figure*}


\begin{figure*}[t]
  \centering
  
  \subfloat[System level\label{subfig:a}]{%
    \includegraphics[width=\textwidth]{figs/topk/combined_fig-sys.pdf} % Replace with the actual file name and path of subfigure (a)
  }
  \vfill
  \subfloat[Segment level\label{subfig:b}]{%
    \includegraphics[width=\textwidth]{figs/topk/combined_fig-seg.pdf} % Replace with the actual file name and path of subfigure (b)
  }
  
  \caption{System and Segment level Performance of top-k MT systems on WMT22 dataset.}
  \label{fig:topk}
\end{figure*}

Previous research has shown that the unreliability of automatic metrics in evaluating top-performing systems, as they exhibit a significant decline in correlation with human evaluation \citep{mathur-etal-2020-tangled}. To further testify the reliability of EAPrompt on LLMs, we compute the performance of LLMs on top-k MT systems at the system and segment level, presented in Figure~\ref{fig:topk}. We can observe that, at the segment level, the ranking of different prompts remains relatively consistent, indicating the stability in performance of LLMs. However, at the system level, there is a noticeable decline in correlations when restricting the evaluation to the top k <= 6 prompts in most settings. This finding warns us that LLMs as an evaluator may not exhibit the same level of accuracy especially when the quality of Machine Translation (MT) systems is relatively high.

\subsection{EAPrompt brings acceptable invalid answers compared with standard prompting}

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule[0.5mm] 
\textbf{Prompt} & \textbf{Dav3} & \textbf{Turbo} & \textbf{GPT-4} \\ \midrule
GEMBA - DA & 0 & 0 & 0 \\
   \quad - Stars & 58 & 0 & - \\
   \quad - SQM & 1279 & 0 & - \\
   \quad - Classes & 0 & 0 & - \\\rowcolor{orange!20}
\textbf{EAPrompt} & \textbf{40} & \textbf{3} & - \\\midrule
GEMBA - DA[noref] & 53 & 0 & 0 \\
   \quad - Stars[noref] & 1 & 0 & 0 \\
   \quad - SQM[noref] & 1 & 0 & 0 \\
   \quad - Classes[noref] & 0 & 0 & - \\\rowcolor{orange!20}
\textbf{EAPrompt[noref]} & \textbf{58} & \textbf{6} & - \\
\bottomrule[0.5mm]
\end{tabular}
\caption{Number of invalid answers using different prompts in WMT22 (testset size: 106,758).}
 \label{tab:invalid_ans}
\end{table}

As GEMBA \citep{kocmi2023llmfindings} highlights in their study that LLMs may provide invalid answers by explaining their decision-making process instead of providing a definitive score. Given that our approach involves more complex prompt contexts, necessitating two separate queries for each test sample, a possible concern is that there is a higher probability that EAPrompt may generate invalid answers compared to standard prompting. 

To this end, we report the number of invalid answers in Table~\ref{tab:invalid_ans}. We observe that when using EAPrompt on \textbf{Turbo} for WMT22, it generates only 9 invalid answers (6 in the reference-based setting, 3 in the reference-less setting). However, on \textbf{Dav3}, this number increases to 98, which may be attributed to that \textbf{Dav3} model utilizes a stricter profanity filter, causing some samples to not receive the expected response. Overall, the number of invalid answers is comparable to that of GEMBA, confirming the effectiveness of our approach and its suitability for real-world evaluation.

\subsection{EAPrompt empowers ChatGPT to produce human-like evaluations} \label{sec:diffprompts}

Given the crucial significance of the prompt design, we explore several versions of in-context prompt contexts and present an analysis in Table~\ref{tab:prompts_selection}. See Appendix~\ref{sec:appendix_testprompt} for the prompt contexts used in our experiment. We find that:

\paragraph{(i) ChatGPT becomes more adept at identifying errors when instructed by error analysis.} For standard prompting, it becomes challenging to interpret the overall quality of a test sample due to the differing evaluation criteria among different models. For instance, a translation score of 70 on \textbf{Turbo} might be considered worse, whereas on the \textbf{GPT-3.5} interface, a score of 90 could be perceived as significantly better. In contrast, EAPrompt's judgment provides more specific instructions, resulting in greater interpretability. Therefore, we recommend incorporating error analysis instructions in prompt contexts rather than relying solely on standard instructions.

\paragraph{(ii) Itemized template response is better than detailed illustration.} As shown in the "Response" column, providing descriptions of errors in detail may hinder ChatGPT's capability to accurately identify errors. In our initial experiments, we also observe that generating excessively detailed responses can lead to incorrect error counting or misclassification of error severity. Therefore, it is recommended to provide clear and concise descriptions of errors in a format that is easy for ChatGPT to process and comprehend.

\paragraph{(iii) Separating the scoring process from error identification may improve the stability of ChatGPT.} We suggest splitting the instruction of error analysis into two queries, one for identifying errors and the other for scoring the translation. Although this may not cause a significant performance gain, we observe that sometimes ChatGPT fails to deduct points for identified errors or presents an incorrect calculation of scores. Separating the scoring process may be helpful, as it allows ChatGPT to focus on one single procedure in each query, thus can provide more accurate judgments.

\section{Case Study} \label{sec:casestudy}

\begin{figure*}[t]

\includegraphics[scale=0.54]{figs/case_combined.pdf}
\centering
\caption{Case study of potential issues in ChatGPT. \textbf{Top}: ChatGPT exhibits variations in its responses upon multiple regenerations; \textbf{Medium}: different input order of samples may affect the judgment of ChatGPT; \textbf{Bottom}: ChatGPT sometimes relies on existing metrics during translation evaluation.}
\label{fig:casestudy}
\end{figure*}

In Figure~\ref{fig:casestudy}, we list several typical issues with the case study that should be aware of when using ChatGPT as a translation evaluator. We also present detailed responses in Appendix~\ref{appendix:examples}.

\subsection{ChatGPT is unstable when conducting evaluation process} \label{sec:unstable}

When assessing translations using ChatGPT, it is not uncommon to observe variations in the scores assigned to the same input. As shown in As depicted in \textbf{Case 1}, we regenerate several responses with the same input and obtain 3 different scores (98, 95, 100) for the translation. The discrepancies in scores could be attributed to the inherent randomness of the model behind ChatGPT. Another possible reason is the lack of clearly stated evaluation criteria described in the prompt contexts. Therefore, we suggest using specific guidelines we propose to minimize the impact of these variations.

\subsection{ChatGPT prefers former inputs when provided with multiple translations} \label{sec:inputbias}

An interesting phenomenon is that when multiple translations are presented together as a single input to ChatGPT for evaluation, it tends to believe that the translations provided earlier are of higher quality, while the quality of later translations are relatively poorer. 

\textbf{Case 2} shows an example of the attack on ChatGPT. We provide 8 translations along with their corresponding source and reference sentences. First, we present the translations sequentially, and ask ChatGPT to rank them according to their translation quality. ChatGPT ranks the translations as (\textit{SYS1, SYS2, SYS4, SYS5, SYS3, SYS6, SYS7, SYS8}), with \textit{SYS1} being the best translation and \textit{SYS8} being the worst. Then, we reverse the order of translations and obtain an entirely different sequence of ranks - (\textit{SYS8, SYS7, SYS6, SYS5, SYS4, SYS3, SYS2, SYS1}), with \textit{SYS8} being the best translation and \textit{SYS1} being the worst. 

The contradictory results may be attributed to the auto-regressive nature of the decoder model, which gives more attention to the latter input, potentially leading to greater identification of errors for the translation input later. Therefore, we recommend that researchers input one translation at a time instead of providing multiple translations. 

\subsection{ChatGPT may directly adopt existing evaluation metrics} \label{sec:existmetric}

We observe that in certain cases, when prompted conventionally, ChatGPT occasionally relies on established evaluation metrics like BLEU and METEOR. An illustration of this behavior can be seen in \textbf{Case 3}, where ChatGPT tends to prioritize the BLEU score instead of offering judgments based on its inherent capabilities.

As our objective is to examine ChatGPT's perfomance for translation evaluation, rather than its capability to implement pre-existing evaluation procedures, we include an explicit instruction of "Do not use existing metrics" in standard prompting. This encourages ChatGPT to develop its own approach to evaluating translations.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we explore the potential of ChatGPT as a metric for evaluating translations. We design a novel in-context prompting strategy based on chain-of-thought and error analysis, and show that this strategy significantly improves ChatGPT's evaluation performance. We further verify the capabilities of LLMs for evaluating top-performing MT systems, and also compare the number of invalid answers in our approach. We compare our approach with other prompt designs to show the effectiveness of error analysis. We hope the experience can benefit NLP researchers in developing more reliable promoting strategies. In section~\ref{sec:casestudy}, we also highlight several potential issues that researchers should be aware of when using ChatGPT as a translation evaluator.

In future work, we would like to experiment with a broader range of LLMs~\cite{wmt19,iwslt21,wmt22,vegamt}, to make our conclusion more convincing. 
Lastly, it will be interesting to test the capabilities of LLMs for other MT-related tasks, such as grammatical error correction and automatic post-editing \cite{wu2023chatgpt, vidal-etal-2022-automatic}.

\section*{Limitations}

The limitations of this work are three-fold:

\begin{itemize}
    \item Potential Test Data Contamination: Although we utilized WMT22 to minimize the risk of test set leakage in the training data of LLMs, it is still possible that some contamination from the test data remains. Therefore, future researchers utilizing these datasets should be cautious and carefully address this issue, as it may affect the availability of the test set for comparison purposes.
    \item Budget Constraints: Due to limited resources, we were unable to explore more prompt choices comprehensively in our research. The findings presented in this study only reflect our initial experiments. The impact of different prompt choices, as well as the influence of unstable problem and input order issues in ChatGPT, remain for further investigation.
    \item Limited Range of LLMs Tested: In this study, we focused on evaluating a limited number of LLMs that we believed possessed potential and capability as translation evaluators. However, it is important to note that not all existing LLMs can necessarily serve as reliable evaluators under the EAPrompt approach. Future research could explore and experiment with a broader range of LLMs, examining their effectiveness and assessing their suitability as evaluators.
\end{itemize}

\section*{Ethics Statement}

We take ethical considerations very seriously, and strictly adhere to the Code of Ethics. All procedures performed in this study are in accordance with the ethical standards. This paper focuses on evaluating the capabilities of LLM as a translation evaluator. Our proposed approach, EAPrompt, does not include statements that induce the model to generate harmful information. Additionally, this method solely extracts and processes the numerical scores from the model's response, thereby further mitigating the potential risks. Both the datasets and models used in this paper are publicly available and have been widely adopted by researchers. Our model will not learn from user inputs or cause potential risks to the NLP community. We ensure that the findings and conclusions of this paper are reported accurately and objectively. Informed consent was obtained from all individual participants included in this study.

% All experiments in this paper were conducted using the interaction screen of ChatGPT. As a result, the test set used in this study is limited. We will conduct more experiments in future work to further validate and refine our current results and findings.

\bibliography{anthology,arxiv_version}
\bibliographystyle{acl_natbib}

\appendix
% \onecolumn

\section{Prompt Contexts} \label{sec:appendix_testprompt}
Figure~\ref{fig:bestprompt} compares the prompt contexts implemented in error analysis prompting with a detailed response and combined instruction discussed in Section~\ref{sec:diffprompts}.

\begin{figure*}[h]
\includegraphics[scale=0.67]{figs/prompt_context.pdf}
\centering
\caption{The prompt contexts utilized in EAPrompt. We also present other prompt contexts we use when comparing different prompt selections.}
\label{fig:bestprompt}
\end{figure*}

\section{Additional Results} \label{sec:appendix_wmt20result}
To further validate the findings of our experiments, we provide a comprehensive presentation of the WMT22 results obtained under various prompt settings, as shown in Table~\ref{tab:mainres_all}. 

In addition, we present a comparative analysis of the results obtained by EAPrompt on WMT20, in contrast to the baseline metrics, as illustrated in Table~\ref{tab:wmt20res}.

\section{Performance on other LLMs}

In order to further verify the effect of EAPrompt strategy on other types of LLMs, we use an opensource model, LLama2-70b-chat \cite{touvron2023llama} as our base model, with an in-house LLama2-70b model, which has been fine-tuned on mathematical reasoning datasets to enhance CoT capabilities. we also test our approach on GPT-4, with a subset of 600 samples, due to the budget constraints. Results are shown in Table 5. 

\begin{table}[ht]
\centering
\small
\begin{tabular}{lccc}
\toprule[0.5mm] 
\textbf{Model} & \textbf{DA} & \textbf{EAPrompt} & \textbf{$\Delta$} \\ \midrule
Llama2-70b-chat & 21.9 & 22.6 & +0.7 \\
Llama2-70b +SFT  & 22.1 & 37.4 & +15.3 \\
GPT-4* & 44.92 & 45.97 & +1.05 \\
\bottomrule[0.5mm]
\end{tabular}
\caption{Segment Level Kendall Correlation on WMT22 zh-en using different models. *: Due to budget limit, the experiments of GPT-4 was conducted on a subset of 600 samples.}
 \label{tab:rebuttal_res}
\end{table}

We can see that, EAPrompt exhibits a slight advantage over GEMBA-DA by 0.7 on the base model. However, once we inject additional reasoning capabilities into the model (referred to as "+SFT"), the performance gap widens significantly, with EAPrompt outperforming GEMBA-DA by a large margin (+15.3). This further verifies that EAPrompt is better suited for LLMs with stronger CoT reasoning capabilities. 

On GPT-4, EAPrompt shows a slight performance advantage over GEMBA-DA, outperforming it by 1.05 points. This could be attributed to the intrinsic evaluation capabilities of GPT-4, which may already be adept at translation evaluation. EAPrompt introduces interpretability to the translation evaluation process while simultaneously upholding evaluation accuracy.

\begin{table*}[ht]
\centering
\begin{tabular}{llllllllll}
\toprule[0.5mm]
 &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{En-Ru}} & \multicolumn{2}{c}{\textbf{Zh-En}} & \multicolumn{2}{c}{\textbf{Overall}} \\
          \textbf{Models} &    \textbf{Metrics / Prompts} &        \textbf{SYS} &        \textbf{SEG} &        \textbf{SYS} &        \textbf{SEG} &        \textbf{SYS} &        \textbf{SEG} &  \textbf{SYS} &        \textbf{SEG} \\\midrule
       \textbf{Baselines} &          MetricX XXL &       \textbf{76.9} &         36.0 &       \textbf{91.4} &         \underline{\textbf{42.0}} &       84.6 &       42.7 &  \textbf{85.0} & \underline{\textbf{40.2}} \\
              &             BLEURT20 &       \textbf{76.9} &       34.4 &       90.5 &       35.9 &       84.6 &       36.1 &  84.7 & 35.5        \\
              &              COMET22 &       \textbf{76.9} &       36.8 &       86.7 &         40.0 &       \textbf{86.8} &       \underline{\textbf{42.8}} &  83.9 & 39.9         \\
              &                UniTE &       74.4 &       \underline{\textbf{36.9}} &       87.6 &       37.8 &       84.6 &       35.7 &       82.8 & 36.8        \\\cdashline{2-10}[3pt/3pt]
            &      COMET-QE[noref] &       71.8 &       28.1 &         80.0 &       34.1 &       81.3 &       36.5 &       78.1 & 32.9        \\\midrule
    \textbf{Dav3} &                   DA &       92.3 &       \textbf{30.6} &       85.7 &       \textbf{33.2} &       86.8 &       \textbf{37.1} &  \textbf{88.0} & \textbf{33.6}          \\
              &                Stars &       88.5 &       29.4 &       81.9 &       29.4 &       \textbf{87.9} &       29.7 &     85.8 & 29.5        \\
              &                  SQM &       \underline{\textbf{93.6}} &       28.3 &       83.8 &       30.8 &       80.2 &       34.6 &  85.4 & 31.2        \\
              &              Classes &       83.3 &       23.5 &       87.6 &       28.9 &       84.6 &       25.1 &  85.4 & 25.8        \\\rowcolor{orange!20}
              &        \textbf{EAPrompt} &       67.9 &         20.0 &       87.6 &       25.5 &       79.1 &       22.9 &      79.6 & 22.8        \\\cdashline{2-10}[3pt/3pt]
              &            DA[noref] &       87.2 &         18.0 &       \textbf{88.6} &       25.8 &       82.4 &       28.9 &  86.1 & 24.2        \\
              &         Stars[noref] &       80.8 &       19.8 &       83.8 &         31.0 &       84.6 &       23.5 & 83.2 & 24.8        \\
              &           SQM[noref] &       82.1 &       21.8 &       84.8 &       32.8 &       80.2 &       26.8 &  82.5 & 27.1         \\
              &       Classes[noref] &       76.9 &       17.6 &       81.9 &       27.1 &       76.9 &       17.2 &  78.8 & 20.6        \\\rowcolor{orange!20}
              & \textbf{EAPrompt[noref]} &       39.7 &       12.7 &         80.0 &       23.1 &       74.7 &       17.6 &       67.9 & 17.8        \\\midrule
   \textbf{Turbo} &                   DA &       85.9 &         25.0 &       90.5 &       23.4 &       82.4 &       25.5 &  86.5 & 24.6        \\
              &                Stars &       \textbf{89.7} &       25.9 &       90.5 &       22.3 &       87.9 &       26.5 &       89.4 & 24.9        \\
              &                  SQM &       87.2 &       29.8 &       91.4 &       27.7 &       82.4 &       31.3 &       87.2 & 29.6         \\
              &              Classes &       82.1 &         17.0 &       87.6 &       16.7 &       76.9 &       17.8 &  82.5 & 17.2        \\\rowcolor{orange!20}
              &        \textbf{EAPrompt} &       \textbf{89.7} &       \textbf{32.4} &       \textbf{92.4} &       \textbf{32.3} &       \underline{\textbf{90.1}} &       \textbf{36.2} &  \underline{\textbf{90.9}} & \textbf{33.6}        \\\cdashline{2-10}[3pt/3pt]
              &            DA[noref] &       83.3 &       25.5 &       90.5 &       29.4 &       85.7 &       26.4 &      86.9 & 27.1        \\
              &         Stars[noref] &       88.5 &       25.5 &       88.6 &       27.9 &       75.8 &       26.1 &      84.3 & 26.5        \\
              &           SQM[noref] &       \textbf{89.7} &       25.9 &       91.4 &       30.9 &       81.3 &       29.1 & 87.6 & 28.6        \\
              &       Classes[noref] &       62.8 &         -1.0 &       61.9 &        2.7 &       61.5 &        2.9 &   62.0 & 1.5          \\\rowcolor{orange!20}
              & \textbf{EAPrompt[noref]} &       87.2 &       28.4 &       90.5 &       30.1 &       \underline{\textbf{90.1}} &       32.5 &  89.4 & 30.3        \\\midrule
           \textbf{GPT-4} &                   DA &       89.7 &       35.7 &       92.4 &       35.8 &       86.8 &       38.2 &  89.8 & 36.6       \\
              &                Stars &       89.7 &       32.6 &       94.3 &       35.1 &         \textbf{89.0} &       38.2 &     \textbf{91.2} & 35.3        \\
              &                  SQM &       88.5 &         \textbf{38.0} &       92.4 &       38.8 &       84.6 &       39.8 &  88.7 & 38.9        \\
              &              Classes &       89.7 &       22.2 &       90.5 &       26.7 &       86.8 &       27.3 &      89.1 & 25.4        \\\cdashline{2-10}[3pt/3pt]
              &            DA[noref] &       84.6 &       31.1 &       91.4 &       \textbf{40.5} &       85.7 &       \textbf{40.7} &  87.6 & 37.4       \\
              &         Stars[noref] &       84.6 &       30.8 &       93.3 &       36.6 &       87.9 &       40.4 &  89.1 & 35.9        \\
              &           SQM[noref] &       84.6 &       35.9 &       \underline{\textbf{95.2}} &       43.2 &       85.7 &       41.6 &  89.1 & \underline{\textbf{40.2}}        \\
              &       Classes[noref] &       \textbf{92.3} &       30.4 &       92.4 &         39.0 &         \textbf{89.0} &       31.3 &  \textbf{91.2} & 33.6        \\

\bottomrule[0.5mm]
\end{tabular}
\caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. The best results among the same model are highlighted in \textbf{bold}. The best results among all metrics are \underline{underlined}. Our proposed EAPrompt results are highlighted in \colorbox{orange!20}{orange}.}
\label{tab:mainres_all}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{llllllllll}
\toprule[0.5mm]
 &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{Zh-En}} \\\cmidrule(lr){3-4} \cmidrule(lr){5-6}
          \textbf{Models} &    \textbf{Metrics / Prompts} &        \textbf{SYS} &        \textbf{SEG} &        \textbf{SYS} &        \textbf{SEG} \\\midrule
       \textbf{Baselines} &         BLEU \cite{papineni-etal-2002-bleu} & 85.7	&12.2	&67.9	&17.4\\
              &             BLEURT \cite{sellam-etal-2020-bleurt} &     76.2	 &\textbf{43.6}	 &\textbf{82.1}	 &43.1     \\
              &             COMET \cite{rei-etal-2020-comet} &      85.7	 &34.8	 &78.6	 &37.1 \\
              &             PRISM \cite{thompson-post-2020-automatic} &     \textbf{90.5}	 &28.2	 &\textbf{82.1}	 &32.3  \\\midrule
   \textbf{Turbo}  &   \textbf{EAPrompt} &   \textbf{90.5}	&35.5	&\textbf{82.1}	&\textbf{46.1}
       \\\cdashline{2-10}[3pt/3pt]

              & \textbf{EAPrompt[noref]} &       81	&34.9	 &75	 &37.1   \\


\bottomrule[0.5mm]
\end{tabular}
\caption{The system and segment level results of metrics on WMT20 En-De and Zh-En datasets, using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. Best results are highlighted in \textbf{bold}.}
\label{tab:wmt20res}
\end{table*}



\section{Examples for Prompt Selection and Case Study}\label{appendix:examples}

\begin{table*}[ht]
\centering
\begin{tabular}{ll}
\toprule[0.5mm] 
\textbf{System }&  Online-A.en\\
\textbf{Domain}      &  conversational\\
\textbf{Doc\_id }     & 1 \\
\textbf{Seg\_id}  & 6 \\
\textbf{Text }     &\begin{tabular}[l]{@{}l@{}}\textbf{Source:} \begin{CJK}{UTF8}{gbsn}\end{CJK} \\ \textbf{Reference:} May I ask what the status of the order is now?\\
\textbf{Translation:} Please ask, what is the order situation now?\end{tabular}\\
\begin{tabular}[l]{@{}l@{}}\textbf{Human} \\\textbf{Evaluation} \end{tabular}&
\begin{tabular}[l]{@{}l@{}}
\textbf{Major Error:} "Please ask" - Accuracy/Mistranslation\\
\textbf{Minor Error:} "situation" - Style/Awkward
\end{tabular}\\

\bottomrule[0.5mm]
\end{tabular}
\caption{Case information.}
\label{tab:case_info}
\end{table*}

Table~\ref{tab:case_info} illustrates the detailed information of example utilzed in Section~\ref{sec:diffprompts}.

The prompt contexts of \textbf{Case 1} regarding the issue of instability are depicted in Figure~\ref{fig:inconsistency}, as discussed in Section \ref{sec:unstable}. The prompt contexts of \textbf{Case 2} and the response from ChatGPT concerning the input order issue are shown in Figure~\ref{fig:input_bias}, as illustrated in Section \ref{sec:inputbias}. Furthermore, Figure~\ref{fig:nobleu} displays the prompt contexts of \textbf{Case 3} and the response from ChatGPT addressing the utilization of existing metrics, as explained in Section \ref{sec:existmetric}.

\begin{figure}[t]
\centering
\includegraphics[scale=0.46]{figs/case_unstable.png}
\caption{When evaluating the same translation three times, ChatGPT generates similar explanations but different scores.}\label{fig:inconsistency}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.69]{figs/case_multiple.pdf}
\caption{Comparison of providing multiple translations in sequential or reverse order. ChatGPT tends to prefer the former translations and generate contradictory judgments.}\label{fig:input_bias}
\end{figure}

\begin{figure}[t]

\includegraphics[scale=0.36]{figs/case_bleu.jpg}
\centering
\caption{An example on ChatGPT directly adopting BLEU to evaluate translation quality.  }
\label{fig:nobleu}
\end{figure}

\end{document}

