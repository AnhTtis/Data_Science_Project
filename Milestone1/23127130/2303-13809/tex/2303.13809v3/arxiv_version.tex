
\pdfoutput=1


\documentclass[11pt, dvipsnames]{article}

% \usepackage[review]{acl}
\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{float}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{colortbl}
\usepackage{color}

\usepackage{tabularx}
\usepackage{dingbat}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{stfloats}
\usepackage{subfig}
\usepackage[T1]{fontenc}
% \usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
% \usepackage[english, russian]{babel}

%--------------------------------------
\usepackage{CJKutf8}
\usepackage{multirow}
\usepackage{array}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{fontawesome5}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{inconsolata}
\usepackage{arydshln}
\usepackage{CJKutf8}
\usepackage{fancyvrb}
\usepackage{listings}
% \usepackage{subcaption}

\newcommand{\cmark}{{\textbf{\textcolor[rgb]{0.1, 0.5, 0.1}{\ding{51}}}}}
\newcommand{\xmark}{{\textbf{\color{red}{\ding{55}}}}}

\def\emojihappy{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_happy.jpeg}}}
\def\emojineutral{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_neutral.jpeg}}}
\def\emojithink{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_think.jpeg}}}
\def\emojiwarning{\raisebox{-0.55ex}{\includegraphics[width=1.3em]{figs/emoji_attention.jpeg}}}

\definecolor{myblue}{rgb}{0.82, 0.94, 0.75}

\title{Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models}

\author{Qingyu~Lu$^{\diamondsuit, \Re}$,
\ \textbf{
\ Baopu Qiu$^{\flat, \Re}$,
\ Liang Ding$^{\Re}$, 
\ Kanjian Zhang$^{\diamondsuit}$,
\ Tom Kocmi$^{\heartsuit}$,
\ Dacheng Tao$^{\Re}$} \\
\ $^{\diamondsuit}$Southeast University
\ $^{\Re}$JD Explore Academy, JD.com Inc.
\ $^{\flat}$Nanjing University
\ $^{\heartsuit}$Microsoft\\
\includegraphics[scale=0.15]{figs/emoji_mail.png} \texttt{luqingyu@seu.edu.cn}, \texttt{qiubaopu@smail.nju.edu.cn}, \\ \texttt{liangding.liam@gmail.com},
\texttt{tomkocmi@microsoft.com}\\
\includegraphics[scale=0.09]{figs/tool.jpeg} \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}
}

\begin{document}
\maketitle
\begin{abstract}

Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research \citep{kocmi-federmann-2023-large} has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \textit{performs poorly at the segment level}.
To further improve the performance of LLMs on MT quality assessment, we conduct an investigation into several prompting designs, and propose a new prompting method called \textbf{\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts \cite{wei2022chain} and Error Analysis~\cite{lu-etal-2023-toward}. This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, \citet{freitag-etal-2021-experts}) and \textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation. 
% We will release our code and scripts to facilitate the community.


% Additionally, we first discover some limitations of ChatGPT as an MT evaluator, such as changing the order of input may significantly influence the judgment when providing multiple translations in a single query. This work provides a preliminary experience of prompting LLMs as an evaluator to improve the reliability of translation evaluation metrics under the error analysis paradigm.
% The project can be found in \url{https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt}.

\end{abstract}

\section{Introduction}

Large language models (LLMs), especially Generative Pre-trained Transformer (GPT) models~\cite{radford2019language, brown2020language} such as ChatGPT~\citep{ouyang2022training, achiam2023gpt}, have shown remarkable performance in various natural language processing (NLP) tasks~\cite{qin2023chatgpt,zhong2023chat}. LLMs are capable of integrating multiple NLP tasks and can generate detailed and comprehensive responses to human inquiries. Additionally, they can respond appropriately to follow-up questions and maintain sensitivity throughout several turns of conversation.

Previous research has demonstrated that LLMs can perform as well as or even better than other LLMs in machine translation task~\cite{hendy2023good,jiao2023chatgpt,Peng2023ChatGPT4MT}. Given the high cost and time-intensive nature of human evaluation, there is a growing demand for MT metrics that offer both explainability and reliability. Therefore, LLMs hold promise in serving as ideal evaluators, capable of generating both judgments and explanations for the translations. 

Concurrent to our research, GEMBA \citep{kocmi-federmann-2023-large} presents an encouraging finding that GPT models can surpass current best MT metrics at the system level quality assessment using straightforward zero-shot standard prompting, confirming the reliability and potential of this technique. However, such prompts exhibit unrealistic performance at the segment level, and cannot offer additional interpretable information regarding translation errors, thus detracting from the goal of achieving a "human-like" evaluation.

\begin{figure*}[t]

\includegraphics[scale=0.35]{figs/0216_main_icon.pdf}
\centering
\caption{\textbf{A comparative overview between GEMBA Prompting and our proposed Error Analysis Prompting} in assessing the MT quality with LLMs.}
\label{fig:overview}
\end{figure*}

To this end, we take the further step by carefully investigating advanced prompting strategies upon various LLMs for MT quality assessment and propose a novel prompting strategy - \textbf{Error Analysis Prompting} (EAPrompt), combining the Chain-of-Thought (CoT, \citet{wei2022chain}) and Error Analysis (EA, \citet{lu-etal-2023-toward}). 
We give an example of EAPrompt in Figure~\ref{fig:overview}. The idea is to prompt LLMs to emulate the human evaluation framework - MQM \citep{freitag-etal-2021-experts} by \ding{182} \textit{identifying major\&minor errors}, and \ding{183} \textit{scoring the translations according to the severity of these errors}.

We conduct experiments using the test set from the WMT22 metrics shared task, comprising 106,758 segments on 54 MT systems across diverse domains to verify the effectiveness of our approach. Our findings reveal that:

% In addition, we also explore the potential of LLMs compared with modern neural metrics like COMET \citep{rei-etal-2020-comet}, BERTScore \citep{zhang2020bertscore} and BLEURT \citep{sellam-etal-2020-bleurt}.

% Our experiments and analyses illustrate that: 

\begin{itemize}
    \item EAPrompt significantly enhances the performance of LLMs at the system level. Notably, prompting \textit{GPT-3.5-Turbo} with EAPrompt outperforms all other metrics and prompting strategies, establishing a new state-of-the-art.
    \item EAPrompt surpasses GEMBA in 8 out of 9 test scenarios across various language models and language pairs, demonstrating superior performance at the segment level.
    \item The findings regarding EAPrompt's strong performance remain consistent even in reference-less settings, highlighting its suitability for quality estimation tasks.
    \item When designing prompts, we recommend the EAPrompt variant featuring a 2-step separated prompting approach and itemized error demonstrations.
    \item Further analysis confirms that EAPrompt adeptly distinguishes major errors from minor ones, closely aligning its error distribution with MQM.
    \item Optimizing the inference costs of EAPrompt can be achieved by leveraging Regular Expressions instead of counting queries.
    % \item[\emojineutral]  The boosted performance from EA prompting is observed in the zero-shot scenario on \texttt{text-davinci-003} rather than in the few-shot scenario, which indicates that we need to adjust our settings when utilizing other GPT models.
    % \item[\emojineutral] EAPrompt may have a detrimental effect on \texttt{text-davinci-003} (Dav3) as an evaluator. This discrepancy may be attributed to Dav3's relatively inferior CoT capability in contrast to Turbo.
    % \item[\emojiwarning] Despite its good performance, we show that ChatGPT may be unreliable at evaluating high-quality MT systems, and may score the same translation differently.
    % \item[\emojiwarning] It is NOT advisable to combine multiple translations into a single query input, as ChatGPT has a preference for former translations. 
\end{itemize}

% This work provides a preliminary experience of prompting LLMs as an evaluator with error analysis. We hope this approach can improve the reliability of translation evaluation metrics, and also can be extended to other evaluation scenarios of natural language generation, such as text summarization, data-to-text tasks.

This study provides an initial exploration of utilizing error analysis to prompt LLMs as evaluators. EAPrompt can also be extended to benefit other evaluation scenarios within language generation, including summarization and data-to-text tasks.

% The remainder of this report is designed as follows. We introduce the details on prompting ChatGPT with Error Analysis in Section~\ref{sec:evaluation}.
% present the evaluation settings and comparative results in Section~\ref{sec:evaluation}. In Section~\ref{sec:casestudy}, we highlight several potential issues that researchers should be aware of when using ChatGPT as a translation evaluator. Conclusions are described in Section~\ref{sec:conclusion}.

\section{Prompt LLMs with Error Analysis}
\label{sec:evaluation}

\subsection{Translation Evaluation Metric}

Translation evaluation metrics are used to assess the performance of machine translation systems on specific test sets \citep{freitag-etal-2022-results, mathur-etal-2020-results}. These metrics typically take inputs from three sources: the sentence from source language ("Source"), the reference translation provided by human translators ("Reference"), and the hypothesis being evaluated ("Translation"). In scenarios where reference signals are not provided, this "reference-less" metric can also be utilized for quality estimation purposes \citep{zerva-etal-2022-findings, specia2010machine, qiu2022original}. The output of the metric is a score or rank indicating the translation quality of each hypothesis.

To verify the reliability of MT metrics, Multi-dimensional Quality Metric (MQM) has been adopted recently in WMT as a high-quality human evaluation strategy \citep{freitag-etal-2021-experts}. It asks human experts to annotate the errors in the hypothesis and categorize them into "Major" and "Minor" indicating their severity. A detailed example of MQM annotation is presented in Appendix~\ref{appendix:MQM}.

\subsection{Prompt LLMs as Evaluation Metrics}

When prompting LLMs as evaluation metrics, it is crucial to design appropriate instructions that describe the evaluation task. In this paper, we mainly adopt two prompting strategies: "GEMBA Prompting" and "Error Analysis Prompting".

GEMBA \citep{kocmi-federmann-2023-large} is a zero-shot prompting approach that directly asks LLMs to generate a score that reflects the quality of the translation, which shows state-of-the-art performance on GPT models when compared to other model-based metrics. However, they also observe that the performance at the segment level is relatively poorer. This highlights the importance of combining Chain-of-Thought with the Error Analysis Strategy to prompt LLMs in a manner that more closely resembles human evaluation.

\subsection{Error Analysis Prompting}

Motivated by the MQM framework in human evaluation, the idea of the Error Analysis (EA) paradigm, as introduced by \citet{lu-etal-2023-toward}, is to enhance the automatic scoring process by explicitly incorporating error identification, thus providing a more human-like evaluation.

The Chain-of-Thought (CoT) prompting strategy was first proposed by \citet{wei2022chain}. Instead of directly generating the answer, CoT prompts LLMs to think step-by-step. This approach has shown significant performance improvements on reasoning tasks, such as GSM8K \citep{cobbe2021training}. CoT is an emergent ability of LLMs and has been incorporated in instruction fine-tuning of LLMs \citep{chung2022scaling} as well as in benchmarks designed to evaluate LLM capabilities \citep{suzgun2022challenging}.

In this work, we combine the CoT and EA paradigms, introducing a novel prompting strategy called Error Analysis Prompting (EAPrompt). As shown in Figure~\ref{fig:overview}, EAPrompt divides the scoring process into two stages: First, the LLM is instructed to identify major and minor errors in the translation ("Instruction: Identify Errors"). Subsequently, the number of these two types of errors is counted ("Instruction: Count Errors"). Distinguished from GEMBA prompting, EAPrompt emulates the evaluation process of MQM and produces more explanable and reliable automatic evaluations.

After exploring several prompt contexts in initial experiments, we made the following modifications to EAPrompt as follows:

\begin{itemize}
  \item we adopt the one-shot learning format \citep{brown2020language} to enhance the LLMs' understanding of the task (\S\ref{sec:mainres}); different in-context examples are used for different language pairs;
  \item we employ itemized error demonstration in the template response, enabling clearer identification and quantification of errors (\S\ref{sec:ablation});
  \item we partition the evaluation process into two stages to enhance the reliability of metric performance. Additionally, we present a simplified alternative to optimize inference costs by counting errors automatically (\S\ref{sec:repr_exp}).
\end{itemize}

\subsection{Post-processing of LLM responses} \label{sec:postprocess}

After obtaining the number of major and minor errors, we compute the final score of the translation using the following equation:
\begin{equation}
\text{score} = - w_{\text{major}}n_{\text{major}} - w_{\text{minor}}n_{\text{minor}},
\end{equation}
where $n_{\text{major}}$ and $n_{\text{minor}}$ denotes the number of major and minor errors respectively, while $w_{\text{major}}$ and $w_{\text{minor}}$ represent the severity weight assigned to major and minor errors. Since different LLMs may apply distinct criteria for major and minor errors, we follow \citet{lu-etal-2023-toward} to adopt a flexible scoring approach by fixing the $w_{\text{minor}} = 1$ while treating $w_{\text{major}}$ as a latent variable within EAPrompt. We present an analysis on the influence of this variable in \S\ref{sec:major_weight_adjust} and the detailed implementation in experiments is described in Appendix~\ref{sec:appendix_post}.

\begin{table*}[ht]
\centering
\begin{tabularx}{0.94\textwidth}{ccccX}
\toprule[0.5mm]
\textbf{Dataset} & \textbf{Language Pair} & \textbf{Segments} & \textbf{Systems} & \textbf{Domains} \\\midrule
\multirow{3}{*}{WMT22} & En-De & 2037 & 17 & conversational, e-commerce, news, social \\
% \small{Online-B, bleurt\_bestmbr, M2M100\_1.2B-B4, Online-G, PROMT, Online-W, OpenNMT, Lan-Bridge, Online-A, QUARTZ\_TuneReranking, Online-Y, bleu\_bestmbr, comet\_bestmbr, JDExploreAcademy, chrf\_bestmbr} \\
 & En-Ru & 2037 & 17 & conversational, e-commerce, news, social
\\
%\small{HuaweiTSC,JDExploreAcademy,Lan-Bridge,M2M100\_1.2B-B4,Online-A,Online-B,Online-G,Online-W,Online-Y,PROMT,QUARTZ\_TuneReranking,SRPOL,bleu\_bestmbr,comet\_bestmbr,eTranslation,chrf\_bestmbr} \\
 & Zh-En & 1875 & 20 & conversational, e-commerce, news, social \\
%\small{Lan-Bridge,JDExploreAcademy,M2M100\_1.2B-B4,Online-Y,AISP-SJTU,comet\_bestmbr,Online-W,LanguageX,HuaweiTSC,Online-B,bleu\_bestmbr,Online-A,bleurt\_bestmbr,Online-G,DLUT,NiuTrans,QUARTZ\_TuneReranking,chrf\_bestmbr} \\
% \multirow{2}{*}{WMT20} & En-De & 1418 & 7 & news \\
%\small{ eTranslation.737,Huoshan\_Translate.832,Online-A.1574,Online-B.1590,OPPO.1535,Tencent\_Translation.1520,Tohoku-AIP-NTT.890} \\
 % & Zh-En & 2000 & 8 & news \\ %\small{DeepMind.381,DiDi\_NLP.401,Huoshan\_Translate.919,Online-B.1605,OPPO.1422,Tencent\_Translation.1249,THUNLP.1498,WeChat\_AI.1525} \\

\bottomrule[0.5mm]
\end{tabularx}
\caption{\textbf{Statistics of testset}. Source, reference texts, and translations are from the WMT22 metrics shared task. }
\label{tab:experiments}
\end{table*}

\section{Experimental Results}\label{sec:results}

\subsection{Experiment Setup}

\paragraph{Dataset}

We utilize the test set from the WMT22 shared tasks \citep{freitag-etal-2022-results} in English-German (En-De), English-Russian (En-Ru), and Chinese-English (Zh-En) across 4 different domains - conversational, e-commerce, news, and social. Table~\ref{tab:experiments} provides statistics about our test set.

% Since the training data of LLMs from OpenAI we use were up to Sep 2021, this significantly reduces the chance of testset leakage, making our analyses more convincible.
% In addition, we present results from WMT20 shared tasks \citep{mathur-etal-2020-results} in Appendix~\ref{sec:appendix_wmt20result}\footnote{Since there might be potential contamination of the WMT20 test set in GPT training, we exclude these results from the main findings.}. Compared to the WMT20 test set (news domain only),

\paragraph{Human Evaluation} 

 We utilize MQM \citep{freitag-etal-2021-experts} as human judgments, which is annotated by human experts and has been widely adopted in recent WMT metrics shared tasks \citep{freitag-etal-2022-results} and quality estimation tasks \citep{zerva-etal-2022-findings}.

\paragraph{Meta Evaluation}

We follow the standard meta-evaluation approach to measure the performance of MT evaluation metrics \citep{freitag-etal-2023-results}. At the system level, we use pairwise accuracy across all three language pairs, which calculates the proportion of all possible pairs of MT systems that are ranked the same by the metric and human scores \citep{kocmi-etal-2021-ship}. At the segment level, we adopt the group-by-item pairwise accuracy with tie calibration as described by \citet{deutsch-etal-2023-ties}. We use the $\text{acc}_{eq}^*$ variant to compare vectors of metric and gold scores for each segment, then average the results over segments. All the meta-evaluation are calculated with MTME\footnote{ \url{https://github.com/google-research/mt-metrics-eval}}, a metric evaluation tool recommended by WMT \citep{freitag-etal-2022-results} to maintain comparability with other metrics.

\subsection{Baselines and Large Language Models} 

\paragraph{Baseline Metrics}

Given the reported unreliability of BLEU \citep{papineni-etal-2002-bleu}, we compare our method with several model-based metrics for MT evaluation. BLEURT \citep{sellam-etal-2020-bleurt} and COMET \citep{rei-etal-2020-comet} are supervised neural metrics fine-tuned on human evaluation. We employ the \textbf{BLEURT20} and \textbf{COMET-22} for reference-based metrics, and \textbf{COMET-QE} for the reference-less metric.
\textbf{UniTE} \citep{wan-etal-2022-unite} is also a learnt metric that evaluates MT outputs combining three different evaluation scenarios. We also adopt \textbf{UniTE-src} for comparing reference-less metrics. \textbf{MetricX-XXL} \citep{juraska-etal-2023-metricx} is a large-scale multi-task metric that fine-tunes LLM checkpoints using diverse human feedback data. For reference-less metrics, we also reproduce \textbf{MaTESe-QE} \citep{perrella-etal-2022-matese}, a metric leveraging transformer-based multilingual encoders to identify error spans in translations. 

\paragraph{Large Language Models} For prioprietary models, we use the OpenAI API to experiment with \textbf{GPT-3.5-Turbo} \footnote{We use the 0613 OpenAI model versions.}. We also experiment with a human-aligned Llama2-70B series model \cite{touvron2023llama} fine-tuned with multilingual translation data, noted as "\textbf{Llama2-70b-Chat}" in experimental results. We also use a high-quality sparse mixture-of-experts model, Mixtral-8x7b \citep{jiang2024mixtral}. We use a state-of-the-art checkpoint \textbf{Mixtral-8x7b-Instruct} which has been optimised through supervised fine-tuning and direct preference optimisation to follow instructions.

%\textcolor{red}{1.1 T tokens with 70\% English corpus and the other 30\% for multilingual and code data}

\subsection{Prompts for LLM evaluators}

For GEMBA Prompting, we adopt the GEMBA-DA variant as suggested by \citep{kocmi-federmann-2023-large}, given its widespread usage and superior performance across three language pairs \citep{kocmi-federmann-2023-gemba}. 

For Error Analysis Prompting (EAPrompt), we conduct a comparison of various prompting strategies of EAPrompt in \S\ref{sec:ablation}, and use the best-performing variant for other experiments. We show the detailed prompt contexts in Appendix~\ref{sec:appendix_testprompt}. 

%We assess the evaluation capability of LLMs via OpenAI API\footnote{\url{https://platform.openai.com/}}. We mainly involve two models from the GPT3.5 family: gpt-3.5-turbo ("\textbf{Turbo}") and text-davinci-003 ("\textbf{Dav3}"). Both models are selected due to their proximity in capabilities to ChatGPT, since the internal model behind ChatGPT is unknown. We compare our proposed approach with the GPT-4 model ("\textbf{GPT-4}") on GEMBA, to see if the performance on \textbf{Turbo} with EAPrompt could approach this powerful LLM.

\subsection{Experimental Results} \label{sec:mainres}

% \begin{table*}[ht]
% \centering
% \begin{tabular}{llllllllll}
% \toprule[0.5mm]
%  &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{En-Ru}} & \multicolumn{2}{c}{\textbf{Zh-En}} & \multicolumn{2}{c}{\textbf{Overall}} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
%           \textbf{Models} &    \textbf{Metrics / Prompts} &        \textbf{SYS} &         \textbf{SEG} &         \textbf{SYS} &         \textbf{SEG} &         \textbf{SYS} &         \textbf{SEG} &   \textbf{SYS} &         \textbf{SEG} \\\midrule
%        \textbf{Baselines} &          MetricX XXL &       \textbf{76.9} &         36.0 &       \textbf{91.4} &         \underline{\textbf{42.0}} &       84.6 &       42.7 &  \textbf{85.0} & \underline{\textbf{40.2}} \\
%               &             BLEURT20 &       \textbf{76.9} &       34.4 &       90.5 &       35.9 &       84.6 &       36.1 &  84.7 & 35.5        \\
%               &              COMET22 &       \textbf{76.9} &       36.8 &       86.7 &         40.0 &       \textbf{86.8} &       \underline{\textbf{42.8}} &  83.9 & 39.9         \\
%               &                UniTE &       74.4 &       \underline{\textbf{36.9}} &       87.6 &       37.8 &       84.6 &       35.7 &       82.8 & 36.8        \\
%             &      COMET-QE[noref] &       71.8 &       28.1 &         80.0 &       34.1 &       81.3 &       36.5 &       78.1 & 32.9        \\\midrule
%     \textbf{Dav3} &                   GEMBA-DA &       92.3 &       \textbf{30.6} &       85.7 &       \textbf{33.2} &       \textbf{86.8} &       \textbf{37.1} &  \textbf{88.0} & \textbf{33.6}          \\\rowcolor{orange!20}
%               &        \textbf{EAPrompt} &       67.9 &         20.0 &       87.6 &       25.5 &       79.1 &       22.9 &      79.6 & 22.8        \\
%               &            GEMBA-DA[noref] &       87.2 &         18.0 &       \textbf{88.6} &       25.8 &       82.4 &       28.9 &  86.1 & 24.2        \\\rowcolor{orange!20}
%               & \textbf{EAPrompt[noref]} &       39.7 &       12.7 &         80.0 &       23.1 &       74.7 &       17.6 &       67.9 & 17.8        \\\midrule
%    \textbf{Turbo}  &               GEMBA-Stars &       \textbf{89.7} &       25.9 &       90.5 &       22.3 &       87.9 &       26.5 &       89.4 & 24.9        \\\rowcolor{orange!20}
%               &        \textbf{EAPrompt} &       \textbf{89.7} &       \textbf{32.4} &       \textbf{92.4} &       \textbf{32.3} &       \underline{\textbf{90.1}} &       \textbf{36.2} &  \textbf{90.9} & \textbf{33.6}        \\
%               &           GEMBA-SQM[noref] &       \textbf{89.7} &       25.9 &       91.4 &       30.9 &       81.3 &       29.1 & 87.6 & 28.6        \\\rowcolor{orange!20}
%               & \textbf{EAPrompt[noref]} &       87.2 &       28.4 &       90.5 &       30.1 &       \underline{\textbf{90.1}} &       32.5 &  89.4 & 30.3        \\\midrule
%            \textbf{GPT-4} &               GEMBA-Stars &       89.7 &       \textbf{32.6} &       \underline{\textbf{94.3}} &       35.1 &         \textbf{89.0} &       \textbf{38.2} &     \textbf{\textbf{91.2}} & \textbf{35.3}        \\
%               &       GEMBA-Classes[noref] &       \underline{\textbf{92.3}} &       30.4 &       92.4 &         \textbf{39.0} &         \textbf{89.0} &       31.3 &  \textbf{\textbf{91.2}} & 33.6        \\

% \bottomrule[0.5mm]
% \end{tabular}
% \caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores, respectively. The best results among the same model are highlighted in \textbf{bold}. The best results among all metrics are \underline{underlined}. Our proposed EAPrompt results are highlighted in \colorbox{orange!20}{orange}.}
% \label{tab:mainres}
% \end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{llccccc}
\toprule[0.5mm]
 & & & \textbf{System-Level Acc.} & \multicolumn{3}{c}{\textbf{Segment-Level Acc*}} \\
 \cmidrule(lr){4-4} \cmidrule(lr){5-7}
 \textbf{Models} & \textbf{Metrics} / \textbf{Prompts} & \textbf{Ref?} & \textbf{All (3 LPs)} & \textbf{En-De} & \textbf{En-Ru} & \textbf{Zh-En} \\\midrule
\multirow{7}{*}{\textbf{Baselines}}
 & MetricsX-XXL & \cmark & 85.0 & \underline{60.4} & \underline{60.6} & \underline{54.4} \\
 & BLEURT20 & \cmark & 84.7 & 56.8 & 54.0 & 48.9 \\
 & COMET22 & \cmark & 83.9 & 59.4 & 57.7 & 53.6 \\
 & UniTE & \cmark & 82.8 & 59.8 & 57.7 & 51.7 \\
\rowcolor{gray!16} \cellcolor{white} & COMET-QE & \xmark & 78.1 & 55.5 & 53.4 & 48.3\\
\rowcolor{gray!16} \cellcolor{white} & UniTE-src & \xmark & 75.9 & 58.2 & 55.4 & 50.8 \\
\rowcolor{gray!16} \cellcolor{white} & MaTESe-QE & \xmark & 74.8 & 57.2 & 49.9 & 49.4 \\\midrule
\multirow{4}{*}{\textbf{Llama2-70b-Chat}}
 & GEMBA & \cmark & 74.1 & 53.7 & 48.8 & 45.4 \\
 & EAPrompt & \cmark & 85.4 \textcolor{red}{(+11.3)} & 55.2\textcolor{red} {(+1.5)} & 51.4 \textcolor{red}{(+2.6)} & \textbf{50.2 \textcolor{red}{(+4.8)}} \\
 & \cellcolor{gray!16}GEMBA & \cellcolor{gray!16}\xmark & \cellcolor{gray!16}72.6 & \cellcolor{gray!16}54.1 & \cellcolor{gray!16}47.8 & \cellcolor{gray!16}45.0 \\
\rowcolor{gray!16} \cellcolor{white} & EAPrompt & \xmark & 85.8 \textcolor{red}{(+13.2)} & 55.0 \textcolor{red}{(+0.9)} & 51.6 \textcolor{red}{(+3.8)} & 49.3 \textcolor{red}{(+4.3)} \\\midrule
\multirow{4}{*}{\textbf{Mixtral-8x7b-Instruct}}
 & GEMBA & \cmark & 69.7 & 54.8 & 48.3 & 46.7 \\
 & EAPrompt & \cmark & 84.0 \textcolor{red}{(+14.3)} & 53.8 \textcolor{blue}{(-1.0)} & 50.6 \textcolor{red}{(+2.3)} & 48.2 \textcolor{red}{(+1.5)} \\
 & \cellcolor{gray!16}GEMBA & \cellcolor{gray!16}\xmark & \cellcolor{gray!16}74.1 & \cellcolor{gray!16}54.8 & \cellcolor{gray!16}47.5 & \cellcolor{gray!16}46.2 \\
\rowcolor{gray!16} \cellcolor{white} & EAPrompt & \xmark & 82.5 \textcolor{red}{(+8.4)} & 54.1 \textcolor{blue}{(-0.7)} & 49.9 \textcolor{red}{(+2.4)} & 48.3 \textcolor{red}{(+1.1)} \\\midrule
\multirow{4}{*}{\textbf{GPT-3.5-Turbo}}
 & GEMBA & \cmark & 86.5 & 55.2 & 49.5 & 48.2 \\
 & EAPrompt & \cmark & \textbf{\underline{91.2} \textcolor{red}{(+4.7)}} & \textbf{56.7 \textcolor{red}{(+1.5)}} & 53.3 \textcolor{red}{(+3.8)} & 50.0 \textcolor{red}{(+1.8)} \\
 & \cellcolor{gray!16}GEMBA & \cellcolor{gray!16}\xmark & \cellcolor{gray!16}86.9 & \cellcolor{gray!16}54.7 & \cellcolor{gray!16}50.0 & \cellcolor{gray!16}47.6 \\
 \rowcolor{gray!16} \cellcolor{white} & EAPrompt & \xmark & 89.4 \textcolor{red}{(+2.5)} & 55.7 \textcolor{red}{(+1.0)} & \textbf{53.4} \textcolor{red}{(+3.4)} & 48.8 \textcolor{red}{(+1.2)} \\

\bottomrule[0.5mm]
\end{tabular}
\caption{\textbf{The performance of metrics using pairwise accuracy} (\%) \textbf{at the system level and pairwise accuracy with tie calibration} (\%) \textbf{at the segment level}. All results are compared with human-annotated MQM scores. The best results among the same model are highlighted in \textbf{bold}. The best results among all metrics are \underline{underlined}. }
\label{tab:mainres_new}
\end{table*}



% \bottomrule
% \multicolumn{7}{c}{\textbf{WMT22}}                                                                                                    \\
%                  &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{En-Ru}} & \multicolumn{2}{c}{\textbf{Zh-En}} & \multicolumn{2}{c}{\textbf{Overall}} \\
% \midrule
% Models           & Metrics / Prompts & SYS & SEG & SYS & SEG & SYS & SEG & SYS & SEG(avg.) \\
% \midrule
% BaseLine        & MetricX XXL & 76.9 & 36.0 & 91.4 & 42.0 & 84.6 & 42.7 & 40.2 & 85.0             \\
%                  & BLEURT20   & 76.9 & 34.4 & 90.5 & 35.9 & 84.6 & 36.1 & 35.5 & 84.7             \\
%                  & COMET22    & 36.8            & 40             & 42.8           & 39.9                & 83.9           \\
%                  & COMET-QE{[}noref{]}      & 28.1           & 34.1           & 36.5           & 32.9                & 78.1           \\
%                  & UniTE                    & 36.9           & 37.8           & 35.7           & 36.8                & 82.8           \\
% \midrule
% Turbo    & DA                       & 25             & 23.4           & 25.5           & 24.6                & 86.5           \\
%                  & Stars                    & 25.9           & 22.3           & 26.5           & 24.9                & 89.4           \\
%                  & SQM                      & 29.8           & 27.7           & 31.3           & 29.6                & 87.2           \\
%                  & Classes                  & 17             & 16.7           & 17.8           & 17.2                & 82.5           \\
%                  & Ours-EAPrompt            & 32.4           & 32.3           & 36.2           & 33.6                & 90.9           \\
%                  & DA{[}noref{]}            & 25.5           & 29.4           & 26.4           & 27.1                & 86.9           \\
%                  & Stars{[}noref{]}         & 25.5           & 27.9           & 26.1           & 26.5                & 84.3           \\
%                  & SQM{[}noref{]}           & 25.9           & 30.9           & 29.1           & 28.6                & 87.6           \\
%                  & Classes{[}noref{]}       & -1.0           & 2.7            & 2.9            & 1.5                 & 62.0           \\
%                  & Ours-EAPrompt{[}noref{]} & 28.4           & 30.1           & 32.5           & 30.3                & 89.4           \\


% \begin{table*}[ht]
% \centering
% {
% \begin{tabular}{lcccc}
% \toprule[0.5mm] 
% \multirow{2}{*}{\textbf{Metrics}}  & \multicolumn{2}{c}{\textbf{En-De}}  & \multicolumn{2}{c}{\textbf{Zh-En}}\\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5}
% & \textbf{System(\%)} & \textbf{Segment(\%)} & \textbf{System(\%)} & \textbf{Segment(\%)} \\ \midrule
% BLEU \citep{papineni-etal-2002-bleu}  &71.43 &3.55    &21.43    &14.71     \\ 
% BERTscore \citep{zhang2020bertscore}  &\textbf{76.19} &12.30    &25.00    &26.75     \\ 
% BLEURT \citep{sellam-etal-2020-bleurt}&\textbf{76.19} &33.44   &57.14    &32.76  \\ 
% COMET \citep{rei-etal-2020-comet}     &71.43 &\textbf{33.47}   &50.00    &\textbf{38.97}   \\
% \midrule
% text-davinci-003              &42.86 &11.86   &53.57    &23.08   \\
% ChatGPT-EA                               &\textbf{76.19} &26.40    &\textbf{60.71}    &36.73   \\
% \bottomrule[0.5mm]
% \end{tabular}}
% \caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. The best results are \textbf{bold}.}
% \label{tab:mainres}
% \end{table*}


% \begin{figure}[t]
%   \centering
%   \includegraphics[scale=0.33]{figs/segment_compare.pdf}
%   \caption{ The segment level comparison between ChatGPT and text-davinci-003 with standard prompting ("Standard"), in-context error analysis prompting ("EA"), and zero-shot error analysis prompting ("zero-shot EA"). }
%   \label{fig:incontext}
% \end{figure}

We compute system\&segment level performance of EAPrompt with LLMs in Table~\ref{tab:mainres_new}. We see that: 

\paragraph{(i) At the system level, EAPrompt empowers GPT-3.5-Turbo to surpass all other metrics and achieves state-of-the-art performance.} 

Consistent with the findings of \citet{kocmi-federmann-2023-large}, LLMs achieve state-of-the-art performance across all three language pairs at the system level, significantly outperforming traditional metrics ("\textbf{Baselines}") by a large margin. 

Remarkably, when prompting all LLMs with EAPrompt, the performance notably surpasses GEMBA at the system level, achieving the highest pairwise accuracy of 91.2\% on \textbf{GPT-3.5-Turbo}, thus establishing a new SOTA.

% At the segment level, despite previous findings by \citet{kocmi2023llmfindings} regarding the poor correlation between LLMs as evaluators and human judgments, EAPrompt surpasses GEMBA-DA's performance on Turbo by a significant margin, averaging 9.7\% improvement. This result verifies the effectiveness of EAPrompt when used with LLMs such as ChatGPT.

\paragraph{(ii) At the segment level, EAPrompt outperforms GEMBA in 8 out of 9 tested scenarios.}

At the segment level, despite previous findings by \citet{kocmi-federmann-2023-large} regarding the weak correlation between LLMs as evaluators and human judgments, prompting with EAPrompt addresses this drawback of LLM evaluators, outperforming GEMBA's performance on nearly all tested LLMs and language pairs by a significant margin. The best segment-level results are achieved by \textbf{GPT-3.5-Turbo} for En-De (56.7) and En-Ru (53.4), and by \textbf{Llama2-70b-chat} for Zh-En (50.2). This validates the effectiveness of our EAPrompt.

The only exception of the result is observed for En-De \textbf{Mixtral-8x7b-Instruct}, where the segment-level accuracy is lower than GEMBA by 1.0. This discrepancy might be attributed to the limited capability of identifying translation errors in En-De language pair. Another notable finding is that prompting with LLMs, both with GEMBA and EAPrompt, fails to surpass current best metrics ("\textbf{Baselines}") at the segment level. This could be because these baseline metrics have been fine-tuned using extensive translation and human evaluation datasets, while the LLMs employed in our experiment are versatile models guided by few-shot prompts. 


\paragraph{(iii) EAPrompt enhances the performance of LLMs as translation evaluators in reference-less scenarios.}

Our main findings remain consistent with both reference-based and reference-less settings (indicated by "\cmark" and "\xmark" in \textbf{Ref?}, respectively), where EAPrompt continues to outperform GEMBA across all three tested LLMs at the system level, and in 8 out of 9 scenarios at the segment level. The improvement is slightly lower compared to scenarios with referenced signals.

These results underscore the impressive cross-lingual capabilities of LLMs and their suitability for quality estimation under EAPrompt, even in the absence of reference translations, which poses a significant challenge on MT evaluation.

\begin{table*}[ht]
\centering
\Large
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule[0.5mm] 
\textbf{Prompt} & \multicolumn{2}{c}{\textbf{Demo of Errors}} & \multicolumn{2}{c}{\textbf{Type of Queries}} & \multicolumn{4}{c}{\textbf{Mixtral-8x7b-Instruct}} & \multicolumn{4}{c}{\textbf{Llama2-70b-Chat}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
  & Detailed & Itemized & 1-step & 2-step & All (3 LPs) & En-De & En-Ru & Zh-En & All (3 LPs) & En-De & En-Ru & Zh-En \\ \midrule
\textbf{GEMBA} & - & - & - & - & 69.7 & \textbf{54.8} & 48.3 & 46.7 & 74.1 & 53.7 & 48.8 & 45.4 \\ \cmidrule(lr){1-13}
\multirow{4}{*}{\textbf{EAPrompt}} & \cmark & & \cmark & & 75.2 & 53.4 & 50.0 & 45.0 & 62.0 & 53.7 & 47.0 & 47.8 \\
 & \cmark & & & \cmark & 75.5 & 53.4 & 47.9 & 45.5 & 84.7 & 53.5 & 46.9 & 47.5 \\
 & & \cmark & \cmark & & 60.2 & 53.4 & 45.1 & 45.6 & 56.9 & 53.7 & 48.4 & \textbf{50.2} \\
 & & \cmark & & \cmark & \textbf{84.0} & 53.7 & \textbf{50.6} & \textbf{48.2} & \textbf{85.4} & \textbf{55.2} & \textbf{51.4} & \textbf{50.2} \\

\bottomrule[0.5mm]
\end{tabular}}
\caption{\textbf{Comparison of the system level} ("All (3 LPs)") \textbf{and segment level} ("En-De", "En-Ru", "Zh-En") \textbf{performance of LLMs with different variants of prompts for EAPrompt}. We compare itemized or detailed responses to demonstrate identified errors. We also compare the instructions, whether separated into two queries (marked as "2-step", one for identifying errors and another for scoring) or combined into a single query (marked as "1-step"). The
best results among all prompt variants are highlighted in \textbf{bold}.} 
 \label{tab:prompts_selection_new}
\end{table*}

% \paragraph{(iv) EAPrompt may have a detrimental effect on Dav3 as an evaluator.}

% EAPrompt with \textbf{Dav3} does not exhibit the same level of effectiveness as observed with \textbf{Turbo}. This discrepancy may be attributed to \textbf{Dav3}'s relatively inferior CoT capability in contrast to \textbf{Turbo}. A standard prompting strategy like GEMBA-DA may be more suitable, while the \textbf{Turbo} model may be better suited for dialog completion, likely excels in CoT and aligns well with our two-turn EAPrompt approach. Another suspicion is that a stricter profanity filter was applied in \textbf{Dav3}, leading to instances where the model returns "None" outputs.

% \paragraph{} It is important to note that the results for WMT20 are also provided in Appendix~\ref{sec:appendix_wmt20result}. These results largely mirror the findings from WMT22, reinforcing the consistency of our observations across different evaluation settings.



% % \begin{table*}[ht]
% % \centering
% % \small
% % \resizebox{\linewidth}{!}{
% % \begin{tabular}{cccccccccccccccccccc}
% % \toprule[0.5mm] 
% % \multicolumn{2}{c}{\textbf{Instruction}} & \multicolumn{2}{c}{\textbf{Response}} & \textbf{Separation} & \multicolumn{3}{c}{\textbf{Human}} & \multicolumn{3}{c}{\textbf{Turbo}} & \multicolumn{3}{c}{\textbf{Dav3}} & \multicolumn{3}{c}{\textbf{GPT-3.5}(Interface)}  & \multicolumn{3}{c}{\textbf{GPT-4}(Interface)}  \\ 
% % \cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
% % Standard & EA   & Detailed      & Itemized      &           
% % & Maj & Min & All & Maj & Min & All & Maj & Min & All & Maj & Min & All & Maj & Min & All   \\ \midrule

% % \cmark &    &  - & -      &    -        &  
% % \multirow{4}{*}{1} & \multirow{4}{*}{1} & \multirow{4}{*}{-6} & - & - & 70 & - & - & 85 & - & - & 90 & - & - & 70      \\
% %  & \cmark    &  \cmark &      &    \xmark       &  &  &  &
% % 0 & 2 & -2 & 1 & 1 & -6 & 1 & 2 & -7 & 1 & 1 & -6      \\
% %  & \cmark    &  &  \cmark      &    \xmark        & & & &
% % 1 & 0 & -5 & 1 & 1 & -6 & 0 & 2 & -2 & 1 & 0 & -5      \\
% %  & \cmark    &  &  \cmark     &    \cmark        &  & & &
% % 2 & 0 & -10 & 1 & 0 & -5 & 2 & 0 & -10 & 1 & 1 & -6     \\


% \bottomrule[0.5mm]
% \end{tabular}}
% \caption{Comparison of the segment level scores of ChatGPT for different variants of prompts. Instructions are divided into standard prompting and EA. The response can either be itemized or detailed. The instruction could be separated into two queries (one for identifying errors and another for scoring) or combined into a single query. "Maj", "Min", and "All" represent the number of major errors, minor errors and final score, respectively.} 
%  \label{tab:prompts_selection}
% \end{table*}



% \subsection{The reliability of ChatGPT when evaluating top-performance systems}

% \begin{figure*}[t]
%   \centering
  
%   \begin{minipage}{0.3\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/topk/SEG-en-de.png}
%     \caption{1}
%   \end{minipage}
%   \begin{minipage}{0.3\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/topk/SEG-en-ru.png}
%     \caption{2}
%   \end{minipage}
%   \begin{minipage}{0.3\linewidth}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/topk/SEG-zh-en.png}
%     \caption{2}
%   \end{minipage}
% \centering
% \caption{Top-k}
% \label{fig:topk}
% \end{figure*}


% \begin{figure*}[t]
%   \centering
  
%   \subfloat[System level\label{subfig:a}]{%
%     \includegraphics[width=\textwidth]{figs/topk/combined_fig-sys.pdf} % Replace with the actual file name and path of subfigure (a)
%   }
%   \vfill
%   \subfloat[Segment level\label{subfig:b}]{%
%     \includegraphics[width=\textwidth]{figs/topk/combined_fig-seg.pdf} % Replace with the actual file name and path of subfigure (b)
%   }
  
%   \caption{System and Segment level Performance of top-k MT systems on WMT22 dataset.}
%   \label{fig:topk}
% \end{figure*}

% Previous research has shown that the unreliability of automatic metrics in evaluating top-performing systems, as they exhibit a significant decline in correlation with human evaluation \citep{mathur-etal-2020-tangled}. To further testify the reliability of EAPrompt on LLMs, we compute the performance of LLMs on top-k MT systems at the system and segment level, presented in Figure~\ref{fig:topk}. We can observe that, at the segment level, the ranking of different prompts remains relatively consistent, indicating the stability in performance of LLMs. However, at the system level, there is a noticeable decline in correlations when restricting the evaluation to the top k <= 6 prompts in most settings. This finding warns us that LLMs as an evaluator may not exhibit the same level of accuracy especially when the quality of Machine Translation (MT) systems is relatively high.

\subsection{Ablation Study of Prompt Variants}  \label{sec:ablation}
% \subsection{EAPrompt empowers ChatGPT to produce human-like evaluations}

Given the crucial significance of the prompt design, we investigate several versions of in-context prompt contexts and present an analysis in Table~\ref{tab:prompts_selection_new}. The prompt contexts used in our experiment are detailed in Appendix~\ref{sec:appendix_testprompt}. Due to budget constraints, we utilize two LLMs, \textbf{Mixtral-8x7b-Instruct} and \textbf{Llama2-70b-Chat}, as the test bed for this ablation study. Our findings indicate that:

% \paragraph{(i) ChatGPT becomes more adept at identifying errors when instructed by error analysis.} For standard prompting, it becomes challenging to interpret the overall quality of a test sample due to the differing evaluation criteria among different models. For instance, a translation score of 70 on \textbf{Turbo} might be considered worse, whereas on the \textbf{GPT-3.5} interface, a score of 90 could be perceived as significantly better. In contrast, EAPrompt's judgment provides more specific instructions, resulting in greater interpretability. Therefore, we recommend incorporating error analysis instructions in prompt contexts rather than relying solely on standard instructions.

\paragraph{(i) Itemized error demonstration is superior to detailed illustration.} 
We assume that when identifying translation errors, providing detailed descriptions may impede the LLM's capability to accurately identify errors and count the number of them. As illustrated in the "\textbf{Demo of Errors}" column, employing itemized error demonstrations instead of detailed paragraphs yields improved performance at both the system and segment levels for both tested LLMs.

In our initial study, we observed that generating excessively detailed responses could lead to incorrect error counting or misclassification of error severity. Therefore, it is recommended to employ clear and concise error descriptions in a format that is easily processed and comprehended by LLMs.

\paragraph{(ii) Separating the scoring process from error identification with two queries will enhance the performance of LLMs as translation evaluators.} Another consideration in prompt design is the division of the evaluation process into error identification and error counting. As depicted in the "\textbf{Type of Queries}" column, it is evident that the performance of using a single prompting step is considerably lower than that of employing a  2-step prompting approach. This may be because separating the scoring process allows LLMs to concentrate on a single task in each query, thereby facilitating more accurate judgments and reducing the likelihood of incorrectly counting the number of errors.

\paragraph{(iii) Among the prompting strategies, EAPrompt appears to be more suitable for the LLMs as translation evaluators.} When compared with GEMBA prompting strategies, the EAPrompt variant featuring a 2-step separated prompting approach and itemized response achieves superior performance in enhancing LLMs' effectiveness as translation evaluators. Consequently, we recommend employing this particular variant for LLMs as translation evaluators.

\section{Analysis}

\subsection{EAPrompt aligns with human judgment through similar distribution of major and minor errors across most LLMs} \label{sec:dist_shift}

To investigate can LLMs align with gold human judgement MQM through similar distributions of major and minor errors, we present the error distribution across various test scenarios in Figure~\ref{fig:dist_shift}. 

We can see that, for major errors, all tested LLMs exhibit distributions that closely resemble MQM. Regarding minor errors, Mixtral-8x7b-Instruct appears to produce a slightly higher frequency of such errors compared to other LLMs, while the distribution of other LLMs remains consistent with MQM. This observation further validates the efficacy of EAPrompt. 

This finding provides valuable insights into enhancing the reliability of LLMs as translation evaluators. It suggests a potential focus on guiding LLMs to more accurately identify minor errors, such as clarifying the specific categories and severity of minor errors.

\subsection{EAPrompt empowers LLMs to distinguish major errors from minor ones} \label{sec:major_weight_adjust}

A potential concern on EAPrompt is whether this technique can prompt LLMs to distinguish major errors from minor ones. To address this concern, we adjust 
the weight assigned to major errors ($w_{\text{major}}$) in the score computation process outlined in \S\ref{sec:postprocess}. We visualize the impact of this adjustment on both the system and segment-level performance in Figure~\ref{fig:weight_influence}. If the metric effectively distinguishes major errors from minor ones, we anticipate a noticeable performance decrease when the weight of major errors $w_{\text{major}}$ approaches that of minor errors ($w_{\text{minor}}$ = 1 in this study). 

Our findings reveal that for all three LLMs tested, adjusting $w_{\text{major}} < 3$ results in a substantial performance decline, indicating that prompting error analysis with all tested LLMs possesses the ability to discriminate major errors from minor ones.

Another noteworthy observation from this analysis is that when $w_{\text{major}} \geq 5$, both the system-level and segment level-accuracies exhibit minimal fluctuation, suggesting that the performance of EAPrompt remains nearly unaffected by this latent variable during score computation. 

\begin{figure*}[t]

\includegraphics[scale=0.36]{figs/error_dist.pdf}
\centering
\caption{\textbf{Distribution of identified error counts} across various LLMs and human evaluation (MQM), for the language pairs En-De, En-Ru and Zh-En, repectively.}
\label{fig:dist_shift}
\end{figure*}

\begin{figure}[t]

\includegraphics[scale=0.5]{figs/error_weight_short.pdf}
\centering
\caption{\textbf{Effect of varying major error weight} ($w_{\text{major}}$) on EAPrompt across different LLMs at both system and segment levels.}
\label{fig:weight_influence}
\end{figure}

\subsection{EAPrompt optimizes inference costs by utilizing regular expressions instead of counting queries} \label{sec:repr_exp}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{lccccc}
\toprule[0.5mm]
 & & \textbf{System-Level Acc.} & \multicolumn{3}{c}{\textbf{Segment-Level Acc*}} \\
 \cmidrule(lr){3-3} \cmidrule(lr){4-6}
 \textbf{Models} & \textbf{Repr?} & \textbf{All (3 LPs)} & \textbf{En-De} & \textbf{En-Ru} & \textbf{Zh-En}\\\midrule
\multirow{2}{*}{\textbf{Llama2-70b-Chat}}
 & \cmark & 85.0 & 55.6 & 51.5 & 50.4 \\
 & \cellcolor{gray!16}\xmark & \cellcolor{gray!16}85.4 & \cellcolor{gray!16}55.2 & \cellcolor{gray!16}51.4 & \cellcolor{gray!16}50.2 \\\midrule
\multirow{2}{*}{\textbf{Mixtral-8x7b-Instruct}}
 & \cmark & 82.8 & 53.7 & 50.9 & 47.6 \\
 & \cellcolor{gray!16}\xmark & \cellcolor{gray!16}84.0 & \cellcolor{gray!16}53.7 & \cellcolor{gray!16}50.6 & \cellcolor{gray!16}48.2 \\\midrule
\multirow{2}{*}{\textbf{GPT-3.5-Turbo}}
 & \cmark & 90.1 & 56.8 & 53.9 & 50.0 \\
 & \cellcolor{gray!16}\xmark & \cellcolor{gray!16}91.2 & \cellcolor{gray!16}56.7 & \cellcolor{gray!16}53.3 & \cellcolor{gray!16}50.0  \\
\bottomrule[0.5mm]
\end{tabular}
\caption{\textbf{Performance comparison of EAPrompt between utilizing the Regular Expression Matching strategy} ("\cmark" in \textbf{Repr?}) and the counting query strategy ("\xmark" in \textbf{Repr?}) across various LLMs.}
\label{tab:regular_expr}
\end{table*}

Since EAPrompt adopts a two-step prompting strategy, one related question is: can we simplify the query process to reduce inference costs? One potential approach involves substituting the scoring query step with an algorithm that identifies major and minor errors using regular expressions (\textbf{Repr}) to detect bullet points or initial numbers. A detailed description of the \textbf{Repr} matching strategy is provided in the Appendix. The analysis, as depicted in Table~\ref{tab:regular_expr}, indicates that employing Repr matching strategy, as opposed to the original query for counting errors (indicated by "\xmark" in \textbf{Repr?}), yields minimal performance variation at both system and segment levels. Thus, if inference costs are a concern for this metric, substituting the second query step of EAPrompt with regular expressions could be a viable option. Note that for different LLMs, a tailored regular expression pattern may be necessary to encompass various response structures.

\subsection{Case Study}

We discuss potential issues encountered by LLMs and their corresponding solutions in Appendix~\ref{sec:casestudy}, including invalid responses, input order bias, etc. We aim to provide insights that should be considered when utilizing LLMs as translation evaluators.

\section{Related Work}

\paragraph{Translation Evaluation Metrics} MT Evaluation metrics are of crucial importance to the development of MT systems \citep{freitag-etal-2022-results}. Studies have shown that traditional surface-based metrics such as BLEU \citep{papineni-etal-2002-bleu} are no longer suitable for evaluating high-quality MT systems \citep{mathur-etal-2020-tangled}. Modern metrics like COMET \citep{rei-etal-2020-comet}, MetricsX-XXL \citep{juraska-etal-2023-metricx}, BLEURT \citep{sellam-etal-2020-bleurt}, and UniTE \citep{wan-etal-2022-unite} leverage human evaluations and high-quality translations for training. While these metrics achieve strong correlation with human judgements such as MQM \citep{freitag-etal-2021-experts}, there is a growing demand for explainability in their evaluation. Despite progress, recent research struggles to strike a balance between the reliability and explainability of these metrics \citep{lu-etal-2023-toward, xu-etal-2022-errors, perrella-etal-2022-matese}. In this work, we delve into the potential of LLMs for "human-like" translation evaluation, as they possess the capability to explicitly identify translation errors without further fine-tuning, which resembles the evaluation process of human.

\paragraph{LLMs as Evaluators} LLMs refers to language models with hundreds of billion of parameters which are trained on massive textual data \citep{evaluation-survey-2024, zhao2023survey}. Since the emergence of ChatGPT, LLMs have shown its remarkable proficiency across various NLP tasks \citep{achiam2023gpt, touvron2023llama}. A prevalent application of LLMs is harnessing them as evaluators for assessing the performance of Chatbots \citep{zheng2023judging}. Recent studies also show LLM's efficacy in evaluating NLG tasks like summarization and dialog generation through multi-step prompting \citep{liu-etal-2023-g}. GEMBA \citep{kocmi-federmann-2023-large} is the pioneering effort in utilizing LLMs as translation evaluators via a zero-shot prompting approach with GPT models. In this work, EAPrompt innovatively combines error analysis \citep{lu-etal-2023-toward} and chain-of-thought \citep{wei2022chain} to prompt LLMs for achieving human-like translation evaluation.

Subsequent work follows our work to further explore the potential of LLMs as translation evaluators. AutoMQM \citep{fernandes-etal-2023-devil} parallels our approach, utilizing PaLM-2 model \citep{anil2023palm} as the testbed. GEMBA-MQM \citep{kocmi-federmann-2023-gemba} further improves EAPrompt by employing a  few-shot prompting technique using GPT-4, making this approach universally applicable across languages. Another line of research focuses on fine-tuning LLMs to accurately predict error spans in translations. For instance, InstructScore \citep{xu-etal-2023-instructscore} fine-tunes a Llama model \citep{touvron2023llama1}, while XCOMET \citep{guerreiro2023xcomet} scales from COMETKiwi \citep{rei-etal-2023-scaling} to achieve this goal.




\section{Conclusion}
\label{sec:conclusion}
In this paper, we explore the potential of LLMs as a metric for evaluating translations. We design a novel one-shot prompting strategy EAPrompt based on chain-of-thought and error analysis, and show that this strategy significantly improves the evaluation performance on both the system and segment levels. We compare different EAPrompt variants and ultimately opt for a 2-step prompting approach with itemized error demonstrations. Further analysis confirms EAPrompt's proficiency in error identification and its alignment with the commonly accepted human evaluations MQM. 
% In section~\ref{sec:casestudy}, we also highlight several potential issues that researchers should be aware of when using ChatGPT as a translation evaluator.

In future work, we would like to experiment with a broader range of LLMs~\cite{wmt19,iwslt21,wmt22,vegamt}, to make our conclusion more convincing. 
Lastly, it will be interesting to test the capabilities of LLMs for other MT-related tasks, such as grammatical error correction and automatic post-editing \cite{wu2023chatgpt, vidal-etal-2022-automatic}.

\section*{Limitations}

The limitations of this work are three-fold:

\begin{itemize}
    \item Potential Test Data Contamination: Although we utilized WMT22 to minimize the risk of test set leakage in the training data of LLMs, it is still possible that some contamination from the test data remains. Therefore, future researchers utilizing these datasets should be cautious and carefully address this issue, as it may affect the availability of the test set for comparison purposes.
    \item Budget Constraints: Due to limited resources, we were unable to explore more prompt choices comprehensively in our research. The findings presented in this study only reflect our initial experiments. We leave the impact of different prompt choices for further investigation.
    \item Limited Range of LLMs Tested: In this study, we focused on evaluating a limited number of LLMs that we believed possessed potential and capability as translation evaluators. However, it is important to note that not all existing LLMs can necessarily serve as reliable evaluators under the EAPrompt approach. Future research could explore and experiment with a broader range of LLMs, examining their effectiveness and assessing their suitability as evaluators.
\end{itemize}

\section*{Ethics Statement}

We take ethical considerations very seriously, and strictly adhere to the Code of Ethics. All procedures performed in this study are in accordance with the ethical standards. This paper focuses on evaluating the capabilities of LLM as a translation evaluator. Our proposed approach, EAPrompt, does not include statements that induce the model to generate harmful information. Additionally, this method solely extracts and processes the numerical scores from the model's response, thereby further mitigating the potential risks. Both the datasets and models used in this paper are publicly available and have been widely adopted by researchers. Our model will not learn from user inputs or cause potential risks to the NLP community. We ensure that the findings and conclusions of this paper are reported accurately and objectively. Informed consent was obtained from all individual participants included in this study.

% All experiments in this paper were conducted using the interaction screen of ChatGPT. As a result, the test set used in this study is limited. We will conduct more experiments in future work to further validate and refine our current results and findings.

\bibliography{anthology,arxiv_version}
% \bibliographystyle{acl_natbib}

\appendix
% \onecolumn

\section{Description of MQM} \label{appendix:MQM}
Multidimensional Quality Metric (MQM) is a human evaluation framework commonly used in WMT metrics shared tasks as the golden standard \citep{freitag-etal-2021-experts, freitag-etal-2023-results}. In this paper, EAPrompt emulates MQM to identify major and minor errors, providing insightful explanations for the translation. Table~\ref{tab:case_info} illustrates an example in detail annotated through MQM framework.

\section{Post-processing of EAPrompt} \label{sec:appendix_post}

As described in \S\ref{sec:postprocess}, we treat $w_{\text{major}}$ as a latent variable within EAPrompt.
In our experiments, we select this latent variable with the best averaging performance for each LLMs denoted as $w_{\text{major}}^*$. The value was reported in Table~\ref{tab:error_weight}.

\begin{table}[ht]
\centering
\small
\begin{tabular}{lc}
\toprule[0.5mm] 
\textbf{Model} & \textbf{$w_{\text{major}}^*$} \\ \midrule
GPT-3.5-Turbo & 6 \\
Llama2-70b-Chat & 10 \\
Mixtral-8x7b-Instruct & 10 \\
\bottomrule[0.5mm]
\end{tabular}
\caption{\textbf{Optimal values of $w_{\text{major}}^*$ for each LLM}. To ensure fair comparison, we maintain this variable constant across all tested scenarios for every LLM.}
 \label{tab:error_weight}
\end{table}

\section{Prompt Contexts of EAPrompt} \label{sec:appendix_testprompt}
Figure~\ref{fig:bestprompt} provides the prompt contexts implemented in EAPrompt, along with the detailed error demonstration and combined query instruction discussed in \S\ref{sec:ablation} for reproduction of our experiments.

\section{Counting Errors using Regular Expressions Matching}

In Figure~\ref{fig:expr}, we present an overview of our error-matching strategy utilized in \S\ref{sec:repr_exp} to automatically identify the number of major and minor errors. The procedure can be listed as follows:

\begin{itemize}
    \item[1.] Locate "major error" and "minor error" within the response, then segment the response accordingly.
    \item[2.] Utilize Regular Expression matching to identify the initial numbers of major and minor errors. For implementation, we include three different initial number formats: "1.", "1)" and "(1)" (using "1" as an example);
    \item[3.] Record the number of major and minor errors.
\end{itemize}


\begin{figure*}[h]
\includegraphics[scale=0.67]{figs/prompt_context2.pdf}
\centering
\caption{\textbf{The prompt contexts employed in EAPrompt}. We present itemized/detailed responses for error demonstrations and separated/combined instructions for different types of queries.}
\label{fig:bestprompt}
\end{figure*}

\begin{figure*}[h]
\includegraphics[scale=0.47]{figs/expr.pdf}
\centering
\caption{\textbf{The regular expression matching strategy} utilized in \S\ref{sec:repr_exp} to automatically count the number of major and minor errors in the LLM response.}
\label{fig:expr}
\end{figure*}

% \section{Additional Results of EAPrompt on GPT-4}

% In order to further verify the effect of EAPrompt strategy on high-quality LLMs, we test our approach on GPT-4. Due to the budget constraints, we randomly sample 30 translations from the WMT22 Zh-En test set and collect the score of 20 systems, 
% only test on 600 samples...

% with a subset of 600 samples, due to the budget constraints. Results are shown in Table 5. 

% \begin{table}[ht]
% \centering
% \small
% \begin{tabular}{lccc}
% \toprule[0.5mm] 
% \textbf{Model} & \textbf{DA} & \textbf{EAPrompt} & \textbf{$\Delta$} \\ \midrule
% Llama2-70b-chat & 21.9 & 22.6 & +0.7 \\
% Llama2-70b +SFT  & 22.1 & 37.4 & +15.3 \\
% GPT-4* & 44.92 & 45.97 & +1.05 \\
% \bottomrule[0.5mm]
% \end{tabular}
% \caption{Segment Level Kendall Correlation on WMT22 zh-en using different models. *: Due to budget limit, the experiments of GPT-4 was conducted on a subset of 600 samples.}
%  \label{tab:rebuttal_res}
% \end{table}

% We can see that, EAPrompt exhibits a slight advantage over GEMBA-DA by 0.7 on the base model. However, once we inject additional reasoning capabilities into the model (referred to as "+SFT"), the performance gap widens significantly, with EAPrompt outperforming GEMBA-DA by a large margin (+15.3). This further verifies that EAPrompt is better suited for LLMs with stronger CoT reasoning capabilities. 

% On GPT-4, EAPrompt shows a slight performance advantage over GEMBA-DA, outperforming it by 1.05 points. This could be attributed to the intrinsic evaluation capabilities of GPT-4, which may already be adept at translation evaluation. EAPrompt introduces interpretability to the translation evaluation process while simultaneously upholding evaluation accuracy.

% \section{Results of EAPrompt on GPT-4} \label{sec:gpt4_result}
% To further validate the findings of our experiments, we provide a comprehensive presentation of the WMT22 results obtained under various prompt settings, as shown in Table~\ref{tab:mainres_all}. 

% In addition, we present a comparative analysis of the results obtained by EAPrompt on WMT20, in contrast to the baseline metrics, as illustrated in Table~\ref{tab:wmt20res}.


\section{Case Study} \label{sec:casestudy}

\begin{figure*}[t]

\includegraphics[scale=0.54]{figs/case_combined.pdf}
\centering
\caption{\textbf{Case study of potential issues in LLMs}. All three cases are from GPT-3.5-Turbo model ("ChatGPT").  \textbf{Top}: LLM exhibits variations in its responses upon multiple regenerations; \textbf{Medium}: different input order of samples may affect the judgment of LLM; \textbf{Bottom}: LLM sometimes relies on existing metrics during translation evaluation.}
\label{fig:casestudy}
\end{figure*}

In Figure~\ref{fig:casestudy}, we list several typical issues with the case study that should be aware of when using LLMs such as ChatGPT as translation evaluators.

\subsection{Potential instability in the responses without temperature control}

\hl{\textbf{Issue:}}
When evaluating translations using LLMs, the generated responses may vary significantly. See in \textbf{Case 1}, we regenerate several responses with the same input and obtain 3 different scores (98, 95, 100) for the translation.

\noindent\hl{\textbf{Solution:}}
We control the temperature parameter to mitigate the variability in LLM judgments. Accordingly, for all experiments detailed in this paper, we set the temperature to 0 for \textbf{GPT-3.5-Turbo}. For the other two models, namely \textbf{Llama2-70b-Chat} and \textbf{Mixtral-8x7b-Instruct}, we opted for a temperature setting of 0.05 since the inference parameter from these two models should be above zero.

% . As shown in As depicted in \textbf{Case 1}, we regenerate several responses with the same input and obtain 3 different scores (98, 95, 100) for the translation. The discrepancies in scores could be attributed to the inherent randomness of the model behind ChatGPT. Another possible reason is the lack of clearly stated evaluation criteria described in the prompt contexts. Therefore, we suggest using specific guidelines we propose to minimize the impact of these variations.

\subsection{Input order bias when evaluating multiple translations simultaneously} \label{sec:inputbias}

\hl{\textbf{Issue:}}
An alternative prompting strategy is to present multiple translations together as a single input to LLMs for evaluation, reducing the number of queries and potentially saving budget. However, we observe a bias where
translations presented earlier tend to get higher scores compared to those presented later. As shown in \textbf{Case 2}, we provide 8 translations along with their corresponding source and reference sentences. At the first time, we present the translations sequentially and ask LLM to rank them according to their translation quality. Then, we reverse the order of translations and obtain an entirely different sequence of ranks. 

\noindent\hl{\textbf{Solution:}}
The contradictory results may be attributed to the auto-regressive nature of the decoder model, which gives more attention to the latter input, potentially leading to greater identification of errors for the translation input later. Therefore, we recommend that researchers input one translation at a time instead of providing multiple translations. 

\subsection{LLMs may generate invalid answers for all prompting strategies} \label{sec:existmetric}


\hl{\textbf{Issue:}}
We observe that in certain cases, LLMs may not function as translation evaluators that may producing invalid answers with textual explanations. A typical case is illustrated in \textbf{Case 3}, where ChatGPT tends to prioritize the BLEU score instead of offering judgments based on its inherent capabilities.

\noindent\hl{\textbf{Solution:}}
We follow the method mentioned in \citet{kocmi-federmann-2023-large} for handling invalid answers, where we introduce randomness to LLMs by iteratively increasing the temperature. Subsequently, we take the first response that falls within the expected score range.

% \clearpage
% \onecolumn
% \section{Appendix: Prompt Templates}
% Below we provide our prompt templates which we use for the experiments described in this paper. Template \textbf{portions in bold face} are used only when a human reference translation is available.



% \subsection{EAPrompt for En-De}

% We implement EAPrompt for En-De using the following prompt template:

%  {\footnotesize
%     \begin{Verbatim}[commandchars=+\[\]]
%         Source: They were addressed to her son, who has autism and lives in a private care 
%         facility, she said. But instead of her son's name inside when you opened them, the
%         letters said Dear Maine's Department of Health and Human Services -- in Cincinnati,
%         she told local media.
%         +textbf[Reference: Sie seien an ihren Sohn adressiert, der an Autismus leidet und in einer]
%         +textbf[privaten Pflegeeinrichtung lebt, sagte sie. Aber als Sie die Briefe ffnete, stand]
%         +textbf[darin nicht der Name ihres Sohnes, sondern sie waren an das Gesundheitsministerium]
%         +textbf[von Maine gerichtet, in Cincinnati, wie sie den lokalen Medien sagte.]
%         Translation: Sie wurden an ihren Sohn gerichtet, der Autismus hat und in einer pri-
%         vaten Pflegeeinrichtung lebt, sagte sie. Aber anstelle des Namens ihres Sohnes im 
%         Inneren, als Sie sie ffneten, sagten die Briefe Dear Maine 's Department of Health
%         and Human Services -- in Cincinnati, sagte sie den lokalen Medien.
%         Based on the given source +textbf{and reference}, identify the major and minor errors in
%         this translation. Note that Major errors refer to actual translation or grammatical 
%         errors, and Minor errors refer to smaller imperfections, and purely subjective 
%         opinions about the translation.

%         Major errors:
%         (1) Sie  Mistranslation
%         (2) Dear Maine 's Department of Health and Human Services  Untranslated text
%         Minor errors:
%         (1) sagten  Mistranslation
%         (2) im Inneren  Mistranslation
%         (3) Briefe ,,  Omission
%         (4) wurden  Grammar
%         (5) im Inneren, als Sie sie ffneten, sagten die Briefe  Awkward Style

%         Source: {source_seg}
%         +textbf[Reference: {reference_seg}]
%         Translation: {target_seg}
%         Based on the given source +textbf{and reference}, identify the major and minor errors in
%         this translation. Note that Major errors refer to actual translation or grammatical 
%         errors, and Minor errors refer to smaller imperfections, and purely subjective 
%         opinions about the translation.

%         {LLM Response 1}

%         Based on the above error information, Output 2 numbers ONLY with the format: "x, x", 
%         indicating the number of major and minor errors. DO NOT ADD other information!

%         {LLM Response 2}
%     \end{Verbatim}
% }

% \subsection{EAPrompt for En-Ru}


% We implement EAPrompt for En-Ru using the following prompt template:

%  {\footnotesize
%     \begin{Verbatim}[commandchars=+\[\]]
%         Source: Experience fast-paced, action-packed combat, hunt monsters and huge bosses, fight with friends in a guild to siege nodes and castles, and train in a variety of life skills such as fishing, trading, crafting, cooking, sailing, and much more!<REF>
%         Reference:     ,      ,     ,     ,      ,   , ,  , ,    !</REF>
%         Translation:   ,  ,      ,     ,     ,      ,   , , ,  ,     !

%         Major errors:
%         (1) ""  Inconsistency
%         (2) ""  Inconsistency
%         Minor errors:
%         (1) " "  Agreement
%         (2) "   Agreement
%         (3) ""  Mistranslation
%         (4) " "  Mistranslation

%         Source: {source_seg}
%         +textbf[Reference: {reference_seg}]
%         Translation: {target_seg}
%         Based on the given source +textbf{and reference}, identify the major and minor errors in
%         this translation. Note that Major errors refer to actual translation or grammatical 
%         errors, and Minor errors refer to smaller imperfections, and purely subjective 
%         opinions about the translation.

%         {LLM Response 1}

%         Based on the above error information, Output 2 numbers ONLY with the format: "x, x", 
%         indicating the number of major and minor errors. DO NOT ADD other information!

%         {LLM Response 2}
%     \end{Verbatim}
% }


% \subsection{GEMBA}

% We implement GEMBA \citep{kocmi-federmann-2023-large} using the following prompt template:

%  {\footnotesize
%     \begin{Verbatim}[commandchars=+\[\]]
%         Score the following translation from {source_lang} to {target_lang} +textbf[with respect to]
%         +textbf[the human reference] on a continuous scale from 0 to 100, where a score of zero means
%         "no meaning preserved" and score of one hundred means "perfect meaning and grammar".
    
%         {source_lang} source: "{source_seg}"
%         +textbf[{target_lang} human reference: {reference_seg}]
%         {target_lang} translation: "{target_seg}"
%         Score:
%     \end{Verbatim}
% }

%As our objective is to examine ChatGPT's perfomance for translation evaluation, rather than its capability to implement pre-existing evaluation procedures, we include an explicit instruction of "Do not use existing metrics" in standard prompting. This encourages ChatGPT to develop its own approach to evaluating translations.



% \subsection{EAPrompt brings acceptable invalid answers compared with standard prompting} \label{sec:invalid_res}

% \begin{table}[ht]
% \centering
% \small
% \begin{tabular}{lcccc}
% \toprule[0.5mm] 
% & & \multicolumn{2}{c}{\textbf{Prompt}} \\\cmidrule(lr){3-4}
% \textbf{Model} & \textbf{Ref?} & GEMBA & EAPrompt \\\midrule
% \multirow{2}{*}{\textbf{Llama2-70b-Chat}} 
% & \cmark & & \\
% & \rowcolor{gray!16} \xmark & & \\\midrule
% \multirow{2}{*}{\textbf{Mixtral-8x7b-Instruct}} 
% & \cmark & & \\
% & \rowcolor{gray!16} \xmark & & \\\midrule
% \multirow{2}{*}{\textbf{GPT-3.5-Turbo}} 
% & \cmark & & \\
% & \rowcolor{gray!16} \xmark & & \\
% \bottomrule[0.5mm]
% \end{tabular}
% \caption{Tab for \S\ref{sec:invalid_res} Invalid Responses.}
%  \label{tab:invalid_ans}
% \end{table}

% As GEMBA \citep{kocmi-federmann-2023-large} highlights in their study that LLMs may provide invalid answers by explaining their decision-making process instead of providing a definitive score. Given that our approach involves more complex prompt contexts, necessitating two separate queries for each test sample, a possible concern is that there is a higher probability that EAPrompt may generate invalid answers compared to standard prompting. 

% To this end, we report the number of invalid answers in Table~\ref{tab:invalid_ans}. We observe that when using EAPrompt on \textbf{Turbo} for WMT22, it generates only 9 invalid answers (6 in the reference-based setting, 3 in the reference-less setting). However, on \textbf{Dav3}, this number increases to 98, which may be attributed to that \textbf{Dav3} model utilizes a stricter profanity filter, causing some samples to not receive the expected response. Overall, the number of invalid answers is comparable to that of GEMBA, confirming the effectiveness of our approach and its suitability for real-world evaluation.



% \begin{table*}[ht]
% \centering
% \begin{tabular}{llllllllll}
% \toprule[0.5mm]
%  &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{En-Ru}} & \multicolumn{2}{c}{\textbf{Zh-En}} & \multicolumn{2}{c}{\textbf{Overall}} \\
%           \textbf{Models} &    \textbf{Metrics / Prompts} &        \textbf{SYS} &        \textbf{SEG} &        \textbf{SYS} &        \textbf{SEG} &        \textbf{SYS} &        \textbf{SEG} &  \textbf{SYS} &        \textbf{SEG} \\\midrule
%        \textbf{Baselines} &          MetricX XXL &       \textbf{76.9} &         36.0 &       \textbf{91.4} &         \underline{\textbf{42.0}} &       84.6 &       42.7 &  \textbf{85.0} & \underline{\textbf{40.2}} \\
%               &             BLEURT20 &       \textbf{76.9} &       34.4 &       90.5 &       35.9 &       84.6 &       36.1 &  84.7 & 35.5        \\
%               &              COMET22 &       \textbf{76.9} &       36.8 &       86.7 &         40.0 &       \textbf{86.8} &       \underline{\textbf{42.8}} &  83.9 & 39.9         \\
%               &                UniTE &       74.4 &       \underline{\textbf{36.9}} &       87.6 &       37.8 &       84.6 &       35.7 &       82.8 & 36.8        \\\cdashline{2-10}[3pt/3pt]
%             &      COMET-QE[noref] &       71.8 &       28.1 &         80.0 &       34.1 &       81.3 &       36.5 &       78.1 & 32.9        \\\midrule
%     \textbf{Dav3} &                   DA &       92.3 &       \textbf{30.6} &       85.7 &       \textbf{33.2} &       86.8 &       \textbf{37.1} &  \textbf{88.0} & \textbf{33.6}          \\
%               &                Stars &       88.5 &       29.4 &       81.9 &       29.4 &       \textbf{87.9} &       29.7 &     85.8 & 29.5        \\
%               &                  SQM &       \underline{\textbf{93.6}} &       28.3 &       83.8 &       30.8 &       80.2 &       34.6 &  85.4 & 31.2        \\
%               &              Classes &       83.3 &       23.5 &       87.6 &       28.9 &       84.6 &       25.1 &  85.4 & 25.8        \\\rowcolor{orange!20}
%               &        \textbf{EAPrompt} &       67.9 &         20.0 &       87.6 &       25.5 &       79.1 &       22.9 &      79.6 & 22.8        \\\cdashline{2-10}[3pt/3pt]
%               &            DA[noref] &       87.2 &         18.0 &       \textbf{88.6} &       25.8 &       82.4 &       28.9 &  86.1 & 24.2        \\
%               &         Stars[noref] &       80.8 &       19.8 &       83.8 &         31.0 &       84.6 &       23.5 & 83.2 & 24.8        \\
%               &           SQM[noref] &       82.1 &       21.8 &       84.8 &       32.8 &       80.2 &       26.8 &  82.5 & 27.1         \\
%               &       Classes[noref] &       76.9 &       17.6 &       81.9 &       27.1 &       76.9 &       17.2 &  78.8 & 20.6        \\\rowcolor{orange!20}
%               & \textbf{EAPrompt[noref]} &       39.7 &       12.7 &         80.0 &       23.1 &       74.7 &       17.6 &       67.9 & 17.8        \\\midrule
%    \textbf{Turbo} &                   DA &       85.9 &         25.0 &       90.5 &       23.4 &       82.4 &       25.5 &  86.5 & 24.6        \\
%               &                Stars &       \textbf{89.7} &       25.9 &       90.5 &       22.3 &       87.9 &       26.5 &       89.4 & 24.9        \\
%               &                  SQM &       87.2 &       29.8 &       91.4 &       27.7 &       82.4 &       31.3 &       87.2 & 29.6         \\
%               &              Classes &       82.1 &         17.0 &       87.6 &       16.7 &       76.9 &       17.8 &  82.5 & 17.2        \\\rowcolor{orange!20}
%               &        \textbf{EAPrompt} &       \textbf{89.7} &       \textbf{32.4} &       \textbf{92.4} &       \textbf{32.3} &       \underline{\textbf{90.1}} &       \textbf{36.2} &  \underline{\textbf{90.9}} & \textbf{33.6}        \\\cdashline{2-10}[3pt/3pt]
%               &            DA[noref] &       83.3 &       25.5 &       90.5 &       29.4 &       85.7 &       26.4 &      86.9 & 27.1        \\
%               &         Stars[noref] &       88.5 &       25.5 &       88.6 &       27.9 &       75.8 &       26.1 &      84.3 & 26.5        \\
%               &           SQM[noref] &       \textbf{89.7} &       25.9 &       91.4 &       30.9 &       81.3 &       29.1 & 87.6 & 28.6        \\
%               &       Classes[noref] &       62.8 &         -1.0 &       61.9 &        2.7 &       61.5 &        2.9 &   62.0 & 1.5          \\\rowcolor{orange!20}
%               & \textbf{EAPrompt[noref]} &       87.2 &       28.4 &       90.5 &       30.1 &       \underline{\textbf{90.1}} &       32.5 &  89.4 & 30.3        \\\midrule
%            \textbf{GPT-4} &                   DA &       89.7 &       35.7 &       92.4 &       35.8 &       86.8 &       38.2 &  89.8 & 36.6       \\
%               &                Stars &       89.7 &       32.6 &       94.3 &       35.1 &         \textbf{89.0} &       38.2 &     \textbf{91.2} & 35.3        \\
%               &                  SQM &       88.5 &         \textbf{38.0} &       92.4 &       38.8 &       84.6 &       39.8 &  88.7 & 38.9        \\
%               &              Classes &       89.7 &       22.2 &       90.5 &       26.7 &       86.8 &       27.3 &      89.1 & 25.4        \\\cdashline{2-10}[3pt/3pt]
%               &            DA[noref] &       84.6 &       31.1 &       91.4 &       \textbf{40.5} &       85.7 &       \textbf{40.7} &  87.6 & 37.4       \\
%               &         Stars[noref] &       84.6 &       30.8 &       93.3 &       36.6 &       87.9 &       40.4 &  89.1 & 35.9        \\
%               &           SQM[noref] &       84.6 &       35.9 &       \underline{\textbf{95.2}} &       43.2 &       85.7 &       41.6 &  89.1 & \underline{\textbf{40.2}}        \\
%               &       Classes[noref] &       \textbf{92.3} &       30.4 &       92.4 &         39.0 &         \textbf{89.0} &       31.3 &  \textbf{91.2} & 33.6        \\

% \bottomrule[0.5mm]
% \end{tabular}
% \caption{The system and segment level results of metrics using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. The best results among the same model are highlighted in \textbf{bold}. The best results among all metrics are \underline{underlined}. Our proposed EAPrompt results are highlighted in \colorbox{orange!20}{orange}.}
% \label{tab:mainres_all}
% \end{table*}

% \begin{table*}[h]
% \centering
% \begin{tabular}{llllllllll}
% \toprule[0.5mm]
%  &                          & \multicolumn{2}{c}{\textbf{En-De}} & \multicolumn{2}{c}{\textbf{Zh-En}} \\\cmidrule(lr){3-4} \cmidrule(lr){5-6}
%           \textbf{Models} &    \textbf{Metrics / Prompts} &        \textbf{SYS} &        \textbf{SEG} &        \textbf{SYS} &        \textbf{SEG} \\\midrule
%        \textbf{Baselines} &         BLEU \cite{papineni-etal-2002-bleu} & 85.7	&12.2	&67.9	&17.4\\
%               &             BLEURT \cite{sellam-etal-2020-bleurt} &     76.2	 &\textbf{43.6}	 &\textbf{82.1}	 &43.1     \\
%               &             COMET \cite{rei-etal-2020-comet} &      85.7	 &34.8	 &78.6	 &37.1 \\
%               &             PRISM \cite{thompson-post-2020-automatic} &     \textbf{90.5}	 &28.2	 &\textbf{82.1}	 &32.3  \\\midrule
%    \textbf{Turbo}  &   \textbf{EAPrompt} &   \textbf{90.5}	&35.5	&\textbf{82.1}	&\textbf{46.1}
%        \\\cdashline{2-10}[3pt/3pt]

%               & \textbf{EAPrompt[noref]} &       81	&34.9	 &75	 &37.1   \\


% \bottomrule[0.5mm]
% \end{tabular}
% \caption{The system and segment level results of metrics on WMT20 En-De and Zh-En datasets, using pairwise accuracy (\%) and Kendall correlation (\%) with human-annotated MQM scores. Best results are highlighted in \textbf{bold}.}
% \label{tab:wmt20res}
% \end{table*}



% \section{Examples for Prompt Selection and Case Study}\label{appendix:examples}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{ll}
\toprule[0.5mm] 
\textbf{System }&  Online-A.en\\
\textbf{Domain}      &  conversational\\
\textbf{Doc\_id }     & 1 \\
\textbf{Seg\_id}  & 6 \\
\textbf{Text }     &\begin{tabular}[l]{@{}l@{}}\textbf{Source:} \begin{CJK}{UTF8}{gbsn}\end{CJK} \\ \textbf{Reference:} May I ask what the status of the order is now?\\
\textbf{Translation:} Please ask, what is the order situation now?\end{tabular}\\
\begin{tabular}[l]{@{}l@{}}\textbf{Human} \\\textbf{Evaluation} \end{tabular}&
\begin{tabular}[l]{@{}l@{}}
\textbf{Major Error:} "Please ask" - Accuracy/Mistranslation\\
\textbf{Minor Error:} "situation" - Style/Awkward
\end{tabular}\\

\bottomrule[0.5mm]
\end{tabular}
\caption{\textbf{An example of MQM}, comprising information of the test sample along with human-annotated major and minor errors.}
\label{tab:case_info}
\end{table*}

% Table~\ref{tab:case_info} illustrates the detailed information of example utilzed in Section~\ref{sec:diffprompts}.

% The prompt contexts of \textbf{Case 1} regarding the issue of instability are depicted in Figure~\ref{fig:inconsistency}, as discussed in Section \ref{sec:unstable}. The prompt contexts of \textbf{Case 2} and the response from ChatGPT concerning the input order issue are shown in Figure~\ref{fig:input_bias}, as illustrated in Section \ref{sec:inputbias}. Furthermore, Figure~\ref{fig:nobleu} displays the prompt contexts of \textbf{Case 3} and the response from ChatGPT addressing the utilization of existing metrics, as explained in Section \ref{sec:existmetric}.

% \begin{figure}[t]
% \centering
% \includegraphics[scale=0.46]{figs/case_unstable.png}
% \caption{When evaluating the same translation three times, ChatGPT generates similar explanations but different scores.}\label{fig:inconsistency}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[scale=0.69]{figs/case_multiple.pdf}
% \caption{Comparison of providing multiple translations in sequential or reverse order. ChatGPT tends to prefer the former translations and generate contradictory judgments.}\label{fig:input_bias}
% \end{figure}

% \begin{figure}[t]

% \includegraphics[scale=0.36]{figs/case_bleu.jpg}
% \centering
% \caption{An example on ChatGPT directly adopting BLEU to evaluate translation quality.  }
% \label{fig:nobleu}
% \end{figure}

\end{document}

