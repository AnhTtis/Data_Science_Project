%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{widetext}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Neural stochastic models for collective movement}

\begin{document}

\twocolumn[
\icmltitle{Discovering mesoscopic descriptions of collective movement \\
with neural stochastic modelling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Utkarsh Pratiush}{sire,ece}
\icmlauthor{Arshed Nabeel}{ces}
\icmlauthor{Vishwesha Guttal}{ces}
\icmlauthor{Prathosh AP}{ece}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sire}{School of Interdisciplinary Research, Indian Institute of Technology, Delhi }
\icmlaffiliation{ces}{Center for Ecological Sciences, Indian Institute of Science, Bengaluru, India.}
\icmlaffiliation{ece}{Deparment of Electrical Communication Engineering, Indian Institute of Science, Bengaluru}
% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Vishwesha Guttal}{guttal@iisc.ac.in}
\icmlcorrespondingauthor{Prathosh AP}{prathosh@iisc.ac.in}
% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Collective motion is an ubiquitous phenomenon in nature, inspiring engineers, physicists and mathematicians to develop mathematical models and bio-inspired designs. Collective motion at small to medium group sizes ($\sim$10-1000 individuals, also called the \emph{`mesoscale'}), can show nontrivial features due to stochasticity. Therefore, characterizing both the deterministic and stochastic aspects of the dynamics is crucial in the study of mesoscale collective phenomena. Here, we use a physics-inspired, neural-network based approach to characterize the stochastic group dynamics of interacting individuals, through a \emph{stochastic differential equation (SDE)} that governs the collective dynamics of the group. We apply this technique on both synthetic and real-world datasets, and identify the deterministic and stochastic aspects of the dynamics using \emph{drift} and \emph{diffusion} fields, enabling us to make novel inferences about the nature of order in these systems. 

% Discovering SDE's is an important step towards studying collective behaviour in ecological systems. Methods such as equation learning helps in it but the polynomial fitting aspect to it imposes certain biases in modelling. In this work we propose a neural network methods which are equation free thus abstains from assuming any biases. Through this work we are able to show asymmetry which can be seen in data to be seen in learnt drift and diffusion functions which is not caught by equation learning methods. We also extend the work where we make the drift and diffusion of SDE an extra function of position which we argue can be achieved only using our method for discovering SDE's.
\end{abstract}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bof}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\boldeta}{\boldsymbol{\eta}}
\newcommand{\pfg}{p_{\bof, \bg}}
\newcommand{\modm}{|\bm|}
\newcommand{\trel}{T_{\text{rel}}}


\section{Introduction}

Collective motion is a phenomenon that observed in natural and synthetic systems, and has fascinated physicists and biologists alike~\cite{vicsek2012collective,sumpter2010collective,camazine2020selforg}. Many systems across scales---such as microscopic organisms~\cite{dinet2021linking,be2019statistical}, cells~\cite{rorth2009cellmigration, alert2020physical}, human crowds~\cite{chen2018social}, and synthetic active matter~\cite{ramaswamy2010mechanics,ramaswamy2017active}---exhibit self-organized collective movement. How the seemingly simple behaviour and interactions of the individuals give rise to the complex self-organised emergent dynamics of the group, is one of the central questions in the study of collective dynamics.

At the level of individual organisms, animal behaviour is complex, and modelling every aspect of individual {\it stochastic} animal behaviour seems an unattainable goal. While the stochastic effects typically average out in the limit of infinite (or sufficiently large) group sizes, real-world animal groups are finite, and often small to medium sized (10 to 1000 individuals). At these \emph{`meoscopic'} scales, the individual-level stochasticity can affect on the group dynamics in non-trivial ways. Therefore, a correct description of the dynamics at the mesoscale should incorporate stochasticity~\cite{mckane2004stochastic, yates2009locust, biancalani2014prl, bruckner2019stochastic, jhawar2020fish}. 

Owing to the complexity at the individual behavioural level, one often uses a coarse-grained dynamical description of the collective system. A physicist's way of approaching this this is to quantify the state of the group, the so called \emph{order parameter}, whose dynamics is then modelled to obtain a parsimonious description of the system. In the context of collective motion, order parameter can be the \emph{group polarisation}---a quantity analogous to the magnetisation of spin systems~\cite{vicsek1995novel}. To study the dynamics of order parameter at the mesoscale, the inherent stochasticity becomes crucial; and the order parameter dynamics must be modelled as a stochastic process. A commonly used framework for continuous stochastic processes such as this is the \emph{diffusion process}, which can be described using a \emph{stochastic differential equation (SDE)}. An SDE describes a stochastic process by disentangling it into a deterministic term, called the \emph{drift}, and a stochastic term, called the \emph{diffusion}~\cite{van1992stochastic, gardiner2009}.

Analytically deriving coarse-grained SDE descriptions starting from individual-level interactions is an arduous task, even for highly simplified toy models of collective behaviour~\cite{mckane2004stochastic,biancalani2014prl,van1992stochastic,jhawar2019bookchapter}. Indeed, for real-world collective systems, one seldom has a detailed understanding of the individual behaviour; therefore, obtaining a group-level description seems hopeless. 

We address this challenge by tackling the inverse problem: we use a data-driven approach to estimate mesoscopic SDEs directly from observed group trajectory data. From the individual movement trajectories, we compute the \emph{polarization order parameter}, quantifying the level of alignment in the group. We then use a physics-inspired, neural-network based approach~\cite{dietrich2021esde,evangelou2022esde} to fit an SDE model to describe the temporal dynamics of polarization. To facilitate the interpretability of the discovered neural SDE---something which is notoriously hard in neural-network-based approaches---we propose a method to visualize the discovered drift and diffusion as fields. The drift and diffusion fields enable us to readily identify the deterministic equilibria, structure of stochasticity, etc.

We use this approach to study both simulated as well as real-world fish (species \emph{Etroplus suratensis}) schools. For a well-studied simulation model of collective behaviour~\cite{buhl2006disorder,biancalani2014prl,dyson2015onset,jhawar2020fish,jhawar2020inferring}, the neural SDE models can accurately recover the mean field SDEs---and hence identify underlying interactions. We also discover SDEs from a real world dataset of schooling fish, where we observe the signatures of \emph{noise-induced order}, a counter-intuitive phenomenon where intrinsic noise serves to increase group-level order rather than destroy it~\cite{biancalani2014prl,jhawar2020inferring}. This was first observed by~\cite{jhawar2020fish}, who used the conventional SDE discovery methods~\cite{jhawar2020inferring,tabar2019book}. Our neural-network-based approach recapitulates the same result; we emphasize that this result is non-obvious, since neural SDEs capture a much wider hypothesis class than the conventional SDEs.

The main contributions of this study are as follows:
\begin{itemize}
    \item We demonstrate the efficacy neural-network-based approaches~\cite{dietrich2021esde,evangelou2022esde} for discovering mesoscale stochastic dynamics for collective behaviour data, using both simulated datasets where the mesoscale SDEs are analytically established.
    \item To make the results of the \emph{grey-box} neural network techniques more interpretable, we propose novel visualization techniques to visualize the drift and diffusion fields in an easily interpretable form.
    \item We apply these techniques to study the mesoscale dynamics in real-world experimental datasets of fish schools~\cite{jhawar2020fish}, thereby demonstrating the significance of these techniques in animal behaviour and ecology. To the best of our knowledge, ours is the first study that applies these techniques to a real world dataset of animal collective behaviour, or even the broader field of ecology.
\end{itemize}

% Since stochasticity is important at the mesoscale, one needs to use a \emph{stochastic differential equation (SDE)} to model the order parameter. However, even for simple toy models with highly simplified individual behaviour, mean-field SDEs are often analytically intractable. Indeed, for real-world collective systems, on seldom has a detailed understanding of the individual behaviour; obtaining a group-level description seems hopeless.

% We address this problem using a data-driven approach, where we estimate mesoscale SDEs directly from the group trajectories. This \emph{neural stochastic model (NSMM)} approach use neural networks to approximate effective SDEs~\cite{dietrich2021esde,evangelou2022esde} that describe the group-level dynamics of the data. Here, neural networks are employed to obtain smooth approximations for the deterministic and stochastic terms of the SDE, while imposing no additional constraints. We also propose a novel way to visualize the deterministic and stochastic terms in the SDE in terms of a vector field and a tensor field respectively: something which is essential for interpreting the discovered models.

% We validate our approach by discovering mesoscopic SDEs for a class of toy-models, and verify that the SDEs match the analytical derivations. We then move on to studying a real-world dataset of real-word fish (\emph{Etroplus suratensis}) trajectories. The mean field SDEs show evidence for \emph{noise-induced schooling}---a counter-intuitive phenomena where intrinsic stochasticity results in creation of order in an otherwise disordered system. This independently reproduces a previous study of the same fish species (CITE), also demonstrated noise-induced schooling. Using a modification of of the NSMM which can account for the effect of a driver variable, we study the effect of boundary interactions on the schooling. We observe that \textcolor{red}{[put detailed results here once confirmed.]}

\paragraph{Related work:} A class of techniques for estimating SDEs from data involve computing the \emph{jump moments} from the observed time-series, and estimating the drift and diffusion coefficients of the SDE using certain conditional averages of the jump-moments~\cite{tabar2019book, gardiner2009}. However, these conditional averages are often noisy and inaccurate, especially when the available data is limited. More recently, approaches have been developed to use jump moments to obtain smooth estimates of the drift and diffusion coefficients, using kernel density estimation, sparse regression  etc.~\cite{gorjao2019kramersmoyal,nabeel2022pydaddy,boninsegna2018sparse,wang2022data}. Another approach is to use parametric estimation techniques, by assuming an appropriate parametric form for the drift and diffusion functions~\cite{nielsen2000parameter}---however, these techniques have the drawback of having to know the parametric form of the SDE \emph{a priori}. 

The neural-network-based approach used in this study was first introduced by~\cite{dietrich2021esde}, and was subsequently applied to derive effective SDE descriptions from simulated Brownian dynamics~\cite{evangelou2022esde}. This is a \emph{`physics-inspired, grey-box'} approach, where partial knowledge and assumptions about the underlying physics is used to make the neural network model at least partly interpretable. 

Alternative approaches for data-driven neural SDE modelling also exist in literature, such as~\cite{song2020score, li2020scalable}, which build on previous work on neural ODEs~\cite{chen2018neural, liu2020rode}. Alternative approaches work by matching moments~\cite{o2021stochastic} or ensemble distributions~\cite{o2021stochastic}, or use VAEs to learn an SDE in a latent space~\cite{hasan2021identifying}. We chose the approach of~\cite{dietrich2021esde} for its simplicity and flexibility because of the way the drift and diffusion are explicitly modelled and incorporated into a straightforward loss-function.


% Other approaches also exist, such as
% \textcolor{green}{[Utkarsh: ADD REFERENCES FOR OTHER NEURAL SDE APPROACHES here-- done see below paragraph augment here as required]}


% \textcolor{blue}{Neural sde/ode related part}
% Use of SDE's in machine learning community is primarily focused toward's generative modeling. Score based and Diffusion based generative models~\cite{song2020score, ho2020denoising} relies on the idea of SDE's and has gained a lot of popularity. There have been work around learning deterministic(ODE) dynamics using neural networks~\cite{chen2018neural, liu2020rode}. There is also work around learning SDE's from data~\cite{song2020score, li2020scalable}. Note, In all of the above cases the interest is towards the generated distribution. In collective behavior study, physicists are interested in discovering the form of the SDE. SPINN, as suggested by~\cite{o2021stochastic}  brings the neural network based diffusion and drift function closer to Kramer-Moyal's estimate's. In~\cite{yang2020generative} authors suggests a physics informed approach that generates fictious paths from the densities, but excludes step wise correspondence. These methods struggle in the case when data is available as successive snapshots. A similar approach~\cite{hasan2021identifying} to ours whose cost function is also based on Euler-Mauriyama scheme uses a VAE framework.

% Stochasticity is inherent in all biological systems, and models of biological systems have to appropriately model the stochasticity. When the system evolution can be modelled as a continuous-state, continuous-time process, a \emph{stochastic differential equation} is a good model for the behaviour of the system dynamics.

% While modelling the collective behaviour of organisms, one is often interested in how overall group behaviour emerges from the individual interactions. To characterize the group behaviour, one usually uses an appropriate \emph{order parameter}, such as the \emph{group polarization}, which characterizes the level of order in the group. The evolution of the group polarization is a stochastic process due to the idiosyncratic nature of the behaviour of individual organisms.

\section{Background \& Methods}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline.pdf}
    \caption{\textbf{Schematic of the estimation procedure.} (a) Individual trajectories are recorded for a real or simulated collective motion. (b) From the individual trajectories, the polarization order parameter ($\bm$) is computed, quantifying the degree of alignment in the school. (c) Neural networks are trained, using a likelihood framework, to estimate the drift ($\bof$) and diffusion ($\bg$) functions. (d) For interpretation, the drift and diffusion functions are visualized as vector and tensor fields respectively. }
    \label{fig:schematics}
\end{figure*}

% This can go to the appendix.
% \subsection{Experimental setup}
% We use a dataset of schooling fish in this study. Groups of \textit{Etroplus suratensis} fish were placed in a shallow circular tank (so that their movement is restricted to two dimensions), and their movement was recorded using a high-resolution camera. From the recorded videos, we tracked the positions $\bx_i(t)$ and velocities $\bv_i(t)$ of each fish $i$ using a tracking software \emph{trex} (CITE). From the individual fish trajectories, the \emph{group polarisation order parameter} was computed as the average of the normalized velocity vectors, i.e. $\bm = \frac{1}{N} \sum_i \hat \bv_i(t)$. The goal is to model the stochastic dynamics of $\bm$ using an SDE.

\subsection{\label{sec:datasets} Datasets}

We use both simulated as well as real-world datasets of collective movement in this study. The simulated data comes from an idealized model of collective movement, for which the mesoscopic dynamics are well-studied~\cite{jhawar2020fish,jhawar2019bookchapter}. The real-world dataset is an open-access dataset (\cite{jiteshjhawar_2020_3632470}), consisting of trajectories of fish schools of different group sizes.

\paragraph{Agent based models for collective movement.} We use simulations from an idealized agent based model of collective movement, with $N = 30$ individuals. Each individual is described solely by its velocity vector $\bv_i$. The magnitude of $\bv_i$ is assumed to be constant across all agents and across time, only the direction changes. Direction changes can happen asynchronously at random time-points, in one of two ways:

\begin{enumerate}
    \item An agent $i$ can spontaneously turn and choose a new direction, i.e. $\theta_i(t) \leftarrow \eta$ for $\eta \sim \mathsf{Unif}[-\pi, +\pi)$.
    \item An agent $i$ can copy the direction of one or more other agents, chosen randomly from the entire group.
\end{enumerate}

The turn and copy events themselves happen as a Poisson process. The models can be classified as \emph{pairwise} or \emph{ternary} interaction models based on how many neighbours are copied. The mesoscopic SDEs for these models are already known in literature~\cite{jhawar2020fish,jhawar2019bookchapter}---also see Appendix~\ref{sec:analytical}. Therefore, the model simulations act as a test-bed to verify the efficacy of the data-driven estimation procedure.

\paragraph{Real-world datasets of schooling fish.} 
We used an openly available dataset of fish schooling~\cite{jhawar2020fish,jiteshjhawar_2020_3632470}. This dataset consists of the position $\bx_i(t)$ and velocity $\bv_i(t)$ trajectories of fish (species \emph{Etroplus suratensis}), swimming in a circular arena. From the individual fish trajectories, the polarization order parameter is computed as detailed in the following section. We used datasets with $N = 15, 30$ and $60$ fish. The data was available at a sampling interval of $0.12s$. 
% Removing detailed description of dataset for anonymity.
% The fish were placed in a shallow, circular tank (radius 30cm) so that their motion was restricted to two dimensions. The movement of the fish was recorded using a high-resolution video camera, for a duration of ~1 hr. From the videos, the trajectories of the individual fish were extracted. From the individual fish trajectories, the polarization order parameter is computed as detailed in the following section. 

\subsection{\label{sec:ordparam} Mesoscopic descriptions of collective dynamics}

While modelling collective movement, the emergent dynamics of the group can often be characterized in terms of an \emph{order parameter}, which characterizes the degree of order in the group. An oft-used order parameter is the group polarization, which captures the level of alignment among the individuals of the group.

For a group of $N$ individuals, each with positions $\bx_i$ and velocities $\bv_i$, the polarization can be computed as the mean of the normalized velocity vectors, i.e.,
\begin{align}
    \bm = \frac{1}{N} \sum_{i=1}^{N} \frac{\bv_i}{|\bv_i|}
\end{align}

Modelling the time-evolution of $\bm$ allows us to gain an understanding of the overall emergent dynamics of the group. For relatively small group sizes, idiosyncrasies in the individual $\bv_i$'s can have a significant effect on the group dynamics. These result in stochastic fluctuations which cannot be ignored while modelling the dynamics of $\bm$. Therefore, we use the framework of \emph{stochastic differential equations (SDE)} to model the $\bm$, which models both deterministic as well as stochastic aspects of the time-evolution of $\bm$.

\subsection{Data-driven discovery of stochastic differential equations}

Given the time series of the 2-dimensional polarization vector $\bm(t)$ sampled with a finite time-interval $\Delta t$, 
our goal is to discover an It\^{o} stochastic differential equation (SDE) model that explains the time-series, of the following form:

\begin{align}
    \frac{d \bm}{dt} = \bof(\bm) + \bg(\bm) \cdot \boldeta(t)  \label{eq:sde}
\end{align}

where $\boldeta$ is 2-dimensional white noise process. Here, $\bof$ is called the \emph{drift} function and $\bg$ is called the \emph{diffusion} function. The goal in the SDE discovery problem is to discover representations for $\bof$ and $\bg$.
% , either in the form of an analytical expression~\cite{brunton2016sindy,boninsegna2018sparse,nabeel2022pydaddy}, or based so-called \emph{equation-free} approaches.
In this work, 
% we take the latter approach, where the drift and diffusion functions are approximated using neural networks. We 
we use a likelihood-based framework, first introduced by~\cite{dietrich2021esde}. Here, $\bof$ and $\bg$ are represented using neural networks, trained to maximize a likelihood function based on the finite-time transition probability. 

Specifically, let $p(\cdot, t_1 | \bm_0, t_0)$ be the probability density function of $\bm(t_1)$ conditional to $\bm(t_0) = \bm_0$. Then, for a small time-step $\Delta t$, $p(\cdot, t + \Delta t | \bm_0, t)$ can be approximated based on the stochastic Euler approximation as:

\begin{align}
    p(\cdot, t + \Delta t | \bm_0, t) \sim \mathcal{N}\left( \bm_0 + \bof(\bm_0) \cdot \Delta t, \bg(\bm_0) \cdot \Delta t \right)
\end{align}

Parameterizing $p$ in terms of the drift and diffusion functions as $\pfg$, this gives rise to the following log-likelihood loss function which can be maximized to fit $\bof$ and $\bg$:

% \begin{widetext}
\begin{align}
    &\mathcal{L}\left(\bof, \bg \middle| \bm_0, \bm_1 \right) 
        = \log \pfg \left(\bm_1, t + \Delta t \middle | \bm_0, t \right) && \nonumber \\
    &    = \frac{(\bm_1 - \bm_0 - \bof(\bm) \Delta t)^2}{\bg(\bm_0)^2 \Delta t} +
           \log \left | \bg(\bm_0)^2 \Delta t\right | + \log 2 \pi \label{eq:likelihood}
\end{align}
% \end{widetext}
where $\bm_0$ and $\bm_1$ are successive points in the time series dataset, sampled $\Delta t$ apart. The loss function $\mathcal L$ can be minimized to fit an appropriate representation of $\bof$ and $\bg$.

We represent $\bf$ and $\bg$ using neural networks, which were trained to minimize the loss function in Equation~\ref{eq:likelihood}--see Section~\ref{sec:implementation} for further details about the implementation. Figure~\ref{fig:schematics} illustrates the full data-driven model discovery procedure, starting from the individual trajectories, computing the polarization order parameter, estimating drift and diffusion using neural networks, and visualizing them as fields.

\subsection{\label{sec:implementation} Implementation details}

\paragraph{Dataset preparation:} The agent-based schooling model was implemented and simulated in MATLAB, using the stochastic simulation algorithm (SSA)~\cite{gillespie2007stochastic}. From the trajectories of individual velocity vectors produced by the simulation, the group polarization $\bm(t)$ was computed (Section~\ref{sec:ordparam}), which was used for training the model. Similarly, for the fish schooling dataset, the tracked individual trajectories were tracked (see Section~\ref{sec:datasets}) were used to compute the group polarization $\bm(t)$.

In both the agent-based models as well as real-world datasets, there is rotational and mirror symmetry in the dynamics (the simulations has no angular bias, while the fish schools were swimming in a circular arena). Therefore, we expect these symmetries to carry over to the mean-field models as well. To encourage the neural network to learn these symmetries, we augment the time-series of $\bm(t)$ with rotated (16 rotations equally spaced between $-\pi$ and $\pi$) and flipped (horizontal and vertical flips) versions of itself. 

\paragraph{Architecture and training details:} The drift and diffusion functions $\bof$ and $\bg$ were represented using fully-connected neural networks with 5 layers and 150 neurons per layer, with ELU activation function~\cite{clevert2015elu}. The network was trained with a batch size 512 and test-train split of 90:10, with the Adamax optimizer. To validate the discovered models, we generate simulated trajectories using the model, and compare the probability density (histogram) and autocorrelation function of the simulated trajectory to that of the input trajectory. The similarity between the histograms was quantified using the Wasserstein metric, $W_1$~\cite{ramdas2017wasserstein}. The similarity of the autocorrelation was quantified using the relative timescale discrepancy, $\trel = | \tau - \hat \tau | / \tau$, where $\tau$ and $\hat \tau$ are the autocorrelation times of the real and simulated time series respectively. A good fit according to these metrics ensures that the discovered dynamical model matches the actual dynamics at least in the weak (distributional) sense~\cite{kloeden1992stochastic}.
% We carried out all our model training in Tensorflow[cite]. A simple feed-forward neural network architecture with 5 layers was used, each layer having 100 perceptrons. We used elu activation function[cite], which was decided based on trial and error. Training was carried on with batch size of 128. Ten percent of data was set aside for testing. Adamax optimizer module[cite] was used for optimising the network.

% Two networks were initialized, one predicting the mean and other predicting the variance. Both the neural networks were optimised simultaneously using the combined loss function.

% \paragraph{Modelling non-autonomous dynamics with driver variables.} Consider a situation where the dynamics is also influenced by additional driver variable(s) $\br$, which are not part of the dynamical variables. The dynamics of $\bm$, the dynamical variable, now obeys the following equation:

% \begin{align}
%     \frac{d \bm}{dt} = \bof(\bm, \br) + \bg(\bm, \br) \cdot \boldeta(t)  \label{eq:sde-2}
% \end{align}

% With the likelihood framework, extending the estimation procedure to this scenario becomes straightforward---simply replacing $\bof$ and $\bg$ in Eq.~\ref{eq:likelihood} with the modified functions allows one to estimate a stochastic dynamical model with driver variables. Here, we use this modification to  study how the polarisation dynamics of the fish school depends on the proximity the tank boundary, by introducing $\br$, the distance from from the tank center, as a driver variable.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/fitmetrics-abm.pdf}
    \caption{\textbf{Goodness-of-fit metrics for mesoscale models for agent-based simulations.} (a, c) Comparison of histograms of $|\bm|$ from the actual time series as well as a time series simulated with the discovered neural stochastic model; for the pairwise (a) and ternary (c) interaction models. (b, d) A similar comparison between the autocorrelation functions of the actual and simulated $|\bm|$ time series from the two models.}
    \label{fig:metrics-sim}
\end{figure*}

\subsection{Visualizing the drift and diffusion fields}

Since the drift and diffusion functions are represented as neural networks (and not equations that are immediately interpretable), a proper visualization is crucial to enable easy interpretation of the discovered dynamical model. To do this, we visualize the drift and diffusion functions as fields.

Visualizing $\bof$ is relatively straightforward. Since $\bof : \mathbb R^2 \to \mathbb R^2$ is a vector function, it can be visualized as a vector field---see Figure~\ref{fig:fields-sim} (a) for an example. The arrows indicate the strength and direction of the drift field at each value of $\bm$. The vector field representation makes it easy to identify features equilibrium points (the vectors will have zero length at the equilibrium point) and their stability (near an equilibrium point, the vectors will point towards it if it is stable, and away from it if it is unstable).

Visualizing the diffusion field is more intricate: since $\bg: \mathbb R^2 \to \mathbb R^{2 \times 2}$ is a matrix function, the diffusion field is a tensor field. First, notice that for a given $\bm$, the covariance matrix of $\bm$ is given by $G(\bm) = \bg(\bm) \bg(\bm)^T$. For any value of $\bm$, $G(\bm)$ can be represented by an ellipse centered at $\bm$, with its axes parallel to the eigenvectors of $G(\bm)$ and axis lengths proportional to the eigenvalues of $G(\bm)$. This is a representation of the diffusion tensor field, and gives an overview of the strength and directionality of the diffusive force for each value of $\bm$. For readers familiar with tensor field visualizations, this is a visualization of the $G$-tensor with elliptical glyphs.
   
% \section{Training and architecture details:}

% We carried out all our model training in Tensorflow[cite]. A simple feed-forward neural network architecture with 5 layers was used, each layer having 100 perceptrons. We used elu activation function[cite], which was decided based on trial and error. Training was carried on with batch size of 128. Ten percent of data was set aside for testing. Adamax optimizer module[cite] was used for optimising the network.

% Two networks were initialized, one predicting the mean and other predicting the variance. Both the neural networks were optimised simultaneously using the combined loss function.

% We see great value around developing and using neural netowrk architecture's that can better explain the collective motion phenomenon, looking into equivariant network's can be promising exploring direction's.

\section{Results}
\subsection{Mesoscopic equations for simulation models of collective behaviour}


We first used our approach to discover the mesoscopic dynamics from simulated trajectories, for a class of models where the mesoscopic SDEs are already known---see Section~\ref{sec:datasets}. This serves as a test-bed for the efficacy of the approach. With pairwise as well as ternary interactions, the simulated flocks show a high degree of polarization. However, the mechanism by which order is created is fundamentally different in these two models. 

Figure~\ref{fig:metrics-sim} compares the histogram and autocorrelation of a simulated time series from the discovered models, to that of the input time series used to train the model (also see Table~\ref{comparison}). There is a close match between the input and the generated data (\emph{pairwise:} $W_1 = 0.0686, \trel = 0.2770$, \emph{ternary:} $W_1 = 0.0270, \trel = 0.0437$). We emphasize that this is despite the model not being directly trained to match the histogram or the autocorrelation---suggesting that the discovered models are good approximations for the actual mesoscopic dynamics.

% The training performance of NSMM on the simulated datasets is shown in (FIG). The model was trained for [XXX] epochs, and validation loss falls below [XXX] within the training duration. At the end of the training process, we generated simulated time-series from the trained models. The histogram of the simulated time-series closely matches the histogram of the original time-series (KL-divergence = [XXX]), suggesting that the discovered model is a good approximation is the actual (unknown) generative model.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/simulated.pdf}
    \caption{\textbf{Mesoscopic dynamics of simulated fish schools.} The drift and diffusion fields estimated by the neural model agrees with the theoretically predicted drift and diffusion fields. (a) Drift field for a simulated school with only pairwise interactions, showing a single attractor at $\bm = 0$. (b) Diffusion field for the pairwise model simulation, showing that the strength of diffusion is maximum at $\bm = 0$ and decreases outwards. (c) Drift field for a simulated school with higher-order (ternary) interactions, showing a ring-shaped attractor for a high value of $|\bm|$. (d) Diffusion field for the ternary model simulation, showing the same pattern of diffusion as in the pairwise model.}
    \label{fig:fields-sim}
\end{figure*}

The estimated drift and diffusion fields are shown in Figure~\ref{fig:fields-sim}. For a model with pairwise interactions, the drift field shows a single attractor at the origin, denoting that the deterministic dynamics has the effect of depolarising the group. However, the strength of diffusion is maximum at the origin and decreases as $\modm$: this means that the stochastic forces push the system away from the $\bm = 0$ stable state, creating order in the system. This counter-intuitive phenomenon of high polarisation arising as a consequence of stochastic effects is called \emph{noise induced order} in literature.

Contrast this with the case of the ternary interaction model, where the the drift field shows ring-shaped attractor, at a high value of $\modm$. The diffusion field is similar to the pairwise model, with diffusion being strongest near the origin and decreasing outwards. This means that the order in the ternary interaction model is primarily deterministic.

This demonstrates that our approach is able to distinguish between the fundamentally different dynamics in the two models considered, despite the time series and the histogram of the datasets looking qualitatively similar. Indeed, the estimated models are also in qualitative agreement with theoretical predictions~(Appendix~\ref{sec:analytical}).

\subsection{\label{sec:real-fish} Mesoscopic equations for real-world fish schools}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/real.pdf}
    \caption{\textbf{Mesoscopic dynamics of a real-world school of 30 \emph{Etroplus suratensis} fish.} The observed drift and diffusion fields show evidence for noise-induced order. (a) Drift field for the \emph{Etroplus school}, showing a single attractor at $\bm = 0$. (b) Diffusion field for school, showing that the strength of diffusion is maximum at $\bm = 0$ and decreases outwards.}
    \label{fig:fields-real}
\end{figure}

We now move on to modelling the stochastic mesoscopic dynamics of a real-world data set of collective motion, based on a recently published data on schools of fish~\cite{jhawar2020fish}. Similar to the simulation models, we seek to discover mesoscopic SDEs for the dynamics of the polarization order-parameter $\bm$ of real schools of fish.

% This deviation is interesting, as it suggests deviations in the datasets from the assumptions of the drift-diffusion SDE framework, especially Markovianity and stationarity.
% We hypothesize that the deviation is because of a spurious periodicity in the time series of $|\bm|$---in the experimental trajectory, there are intervals of time where the fish actively follow the boundary of the circular tank, causing a non-stationary oscillation in the time series of $\bm$. This can lead to residual temporal correlations in $|\bm|$. This is a spurious effect arising due to the oscillatory non-stationarity in the data; so we do not expect it to affect the results of the subsequent paragraphs. Indeed, the autocorrelation functions of individual components $m_x$ and $m_y$ of the actual and simulated time series show a good correspondence in the overall decay, barring the oscillations~\textcolor{red}{(APPENDIX)}. 

% The training performance of NSMM on the simulated datasets is shown in (FIG). The model was trained for [XXX] epochs, and validation loss falls below [XXX] within the training duration. The histograms show the comparison between the distributions of the actual and simulated (using the model discovered by NSMM) time-series, and show a close match (KL-divergence = [XXX]).

We observe that the deterministic mean-field dynamics of the school, characterized by the drift field, drives the system towards a disordered state ($\bm = 0$). However, the diffusion is maximum at $\bm = 0$, driving the system away from the stable state and resulting in noise induced order, similar to the model system with pairwise interactions. This surprising phenomenon was first observed in real-world fish schools in~\cite{jhawar2020fish}; our neural approach is able to reproduce this result. This is a non-trivial result: despite the neural networks being able to represent a much larger hypothesis class, the drift and diffusion fields converged to the simple forms shown in Figure~\ref{fig:fields-real}.

We also derived mesoscopic models for fish schools of group sizes 15 and 60. The results are described in Appendix~\ref{sec:groupsizes} and Figure~\ref{fig:groupsizes}, and are qualitatively similar to the results for the 30-fish school. The net strength of diffusion goes down as the school size increases, an observation that matches theoretical predictions (Appendix~\ref{sec:analytical}).

The fit metrics for the fish schooling dataset (30 fish) is shown in Figure~\ref{fig:metrics-real} (also see Table~\ref{comparison}). Like the agent-based models, there is a good agreement between the histograms of the actual and the simulated $|\bm|$ time series ($W_1 = 0.0823$). However, autocorrelation function shows a deviation from the actual time-series ($\trel = 0.6652$). This deviation is present in other group sizes also (15 and 60 fish). This deviation is because of a spurious periodicity in the time series of $|\bm|$---caused by the fish swimming along the boundary of the tank---which is not captured by the mesoscopic SDE model. The deviation is also present with other SDE estimation techniques (see Table~\ref{comparison}), and is a constraint of the model itself, and not the estimation procedure.

Table~\ref{comparison} summarizes the quantitative performance of the neural effective SDE estimation technique, on the five different datasets considered in this study (2 simulated agent-based models, 3 real-world fish schools of different sizes). For comparison, we use a package PyDaDDy~\cite{nabeel2022pydaddy}, which estimates SDEs from data using classical techniques, \textit{viz.} jump-moment estimation and equation-learning with sparse-regression. While we reemphasize that the focus of a study like this is to obtain interpretable models; overall, the neural SDE estimation approach performs quantitatively better than PyDaddy, in both the Wasserstein metric $W_1$ and relative timescale discrepancy $\trel$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/fitmetrics-real.pdf}
    \caption{\textbf{Goodness-of-fit metrics for mesoscale models for a real-world fish school.} (a) Comparison of histograms of $|\bm|$ from the actual time series as well as a time series simulated with the discovered neural stochastic model; for the schooling dynamics of a school of 30 fish. (b) Comparison of the autocorrelation functions between the real and simulated time series for the same dataset.}
    \label{fig:metrics-real}
\end{figure}

% \paragraph{Effect of confinement on schooling behavior:} In laboratory studies of collective animal movement, the organisms are usually confined in a bounded arena. It is reasonable to ask if the arena causes boundary effects that meaningfully alters the collective dynamics. To study boundary effects, we study how the mean-field equations change as a function of $r$, the distance from the center of the (circular) tank.

% \textcolor{red}{The following paragraph may change based on the result.} We discovered SDE models of the form in \ref{eq:sde-2}, where $\bf(\bm, r)$ and $\bg(\bm, r)$ are now functions of both $\bm$ and $r$. (FIG) shows the drift and diffusion fields for 3 different values of $r$. We observe that the dynamics remain qualitatively similar across $r$, i.e. the boundary effects do not qualitatively change the stochastic dynamics of schooling.

% \textcolor{red}{[Optional paragraph or non-linear $r$-dependence of "net drift strength", "net diffusion strength etc. Need come up with ways to compute and quantify these.]}

\section{Discussion}

Compared to conventional techniques for SDE identification, neural networks have the advantage of being less restrictive in the hypothesis space they encompass, and hence are more general. It is worth noting that, even with the added flexibility, the neural approach still discovers qualitatively the same model for fish schools as \cite{jhawar2020fish}, lending further credence to the observation of noise-induced order in the original study.

%The specific datasets demonstrated in this study do not fully utilize this flexibility---the inherent rotational and mirror symmetries in the data enable the use of simpler models. 

Indeed, we expect the neural-network based approach to be more powerful---even necessary---in scenarios with more complex movement, which lack these symmetries. For example, organisms could be moving under the influence of an inhomegenous light or chemical field~\cite{tian2021chemotaxis,puckett2018phototaxis}. In such cases, enforcing user-defined parametric models, or relying on simple hypothesis classes (such as polynomials), may impose strong and often undesirable inductive biases on the SDE discovery process, which the neural-net-based approach avoids.

Further, the likelihood-based estimation procedure is readily generalizable to more complex stochastic dynamics. While such extensions are out of the scope of the current work, it is worth highlighting that the framework is quite general, and can be extended to accommodate jump discontinuities, extrinsic driver variables, non-stationarity, etc., all of which are commonly observed in real-world datasets~\cite{carpenter2020stochastic,salmaso2000factors}.

Finally, we emphasize the need for proper visualization tools to facilitate interpretability of the discovered models. When the goal of model discovery is scientific enquiry (and not merely to obtain a predictive model), tools to interpret and analyze the discovered models become crucial. Our proposed way of visualizing the drift and diffusion fields achieves exactly this, and enables one to easily understand the deterministic phase portrait and the stochasticity landscape of the discovered model.

\begin{table}
\caption{Comparison of model performance with a classical technique for data-driven SDE discovery}
\label{comparison}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccr}
\toprule
% \begin{sc}
\textbf{Dataset} & 
\multicolumn{2}{c}{\textbf{Neural SDE}} & 
\multicolumn{2}{c}{\textbf{PyDaddy}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
                    & $W_1$ & $\trel$ & $W_1$ & $\trel$ \\
% \end{sc}
\midrule
ABM (Pairwise)      & \textbf{0.0686} & \textbf{0.2770} & 0.767 & 0.3020 \\
ABM (Ternary)       & \textbf{0.0270} & \textbf{0.0437} & 0.0283 & 0.1583 \\
Etroplus (15 fish)  & 0.0823 & \textbf{0.6652} & \textbf{0.0818} & 0.7787\\
Etroplus (30 fish)  & 0.0357 & \textbf{0.5895} & \textbf{0.0263} & 0.7144 \\
Etroplus (60 fish)  & \textbf{0.0155} & \textbf{0.2217} & 0.0302 & 0.4409 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\section{Conclusion}

In this study, we utilize a physics-informed, grey-box neural network model to discover mesoscale SDEs for collective movement~\cite{dietrich2021esde,evangelou2022esde}. Using this technique, we discover SDEs for both simulated and real-world datasets of collective movement. With simulated datasets, this approach is able to identify the difference between different models, even when the time series and histograms look qualitatively similar. Furthermore, the SDEs discovered by this approach in each case are are qualitatively similar to the ones predicted by theory. For a real-world dataset of schooling fish, the neural-net approach discovers an SDE that predicts \emph{noise-induced order}, reproducing the result of a recent study~\cite{jhawar2020fish} that used conventional SDE estimation techniques. 

To conclude, we demonstrate the applicability and versatility of neural network models for studying stochastic dynamics of collective animal movement, and of biological systems in general. The combination of a physics-informed, grey-box estimation process along with proper visualization techniques enabled us to discover models that are quantitatively more accurate than traditional approaches (Table~\ref{comparison}), while still remaining readily interpretable. As high-quality datasets of complex phenomena become commonplace in biology and related fields, these techniques---and clear ways to interpret them---become indispensable. 

\section*{Code \& Data Availability}
The fish schooling dataset used in study is an openly available dataset, available at~\cite{jiteshjhawar_2020_3632470}.



The neural SDE identification code was a modified version of ~\cite{Dietrich2021}. Our modified version of the code is available at \href{https://github.com/utkarshp1161/Neu_sde}{https://github.com/utkarshp1161/Neu\_sde}.

\section*{Acknowledgements}
We thank Vivek Jadhav for his help with the agent-based model simulations, and Somaditya Santra for discussions.



% \paragraph{Data:}Will be made available on request.
% Compared to commonly used non-parametric techniques of SDE estimation~\cite{tabar2019book,friedrich2011approaching}, NSMM also has the advantage of producing a generative model for the dynamics. This makes the approach appealing in 



% We have a time series data of an observable x, we wish to find the underlying $\mathrm{f}$ and  $\sigma$ functions satisfying the below equation.
% \begin{align}
% \mathrm{d} x_t=f\left(x_t\right) \mathrm{d} t+\sigma\left(x_t\right) \mathrm{d} W_t
% \label{eqn-1}
% \end{align}

% \subsection{Method 1: Equation learning method:}
% As discussed by authors in \cite{nabeel2022pydaddy}, this method involves two steps:
% \begin{itemize}
%     \item{Given a time series data extract drift and diffusion components using jump moments or the Kramer-Moyal coefficients [cite]}
%     \item{Equation learning technique based on sparse regression is used to determine the analytical form of extracted drift and diffusion functions.[cite] }
% \end{itemize}

% \subsubsection{Part A: Using jump moments to estimate drift and diffusion - Kramers-Moyal expansion}

% Given a stochastic process \bold{x}(t), gradient of probability density P(\bold{x},t) with respect to time is given by:

% \begin{align}
% \frac{\partial P(\boldsymbol{x}, t)}{\partial t}=\sum_{n=1}^{\infty}\left(-\frac{\partial}{\partial \boldsymbol{x}}\right)^n \mathbf{D}^{(n)}(\boldsymbol{x}, t) P(\boldsymbol{x}, t)
% \label{km-1}
% \end{align}

% eqn \ref{km-1} is also referred to as Kramer's-Moyal expansion. The jumping of positions from x(t) to x(t+h) is referred to as moment of transition probability and determined by:

% \begin{align}
% D^{(n)}(\boldsymbol{x}, t)=\frac{1}{n !} \lim _{h \rightarrow 0} \frac{\left\langle[\boldsymbol{x}(t+h)-\boldsymbol{x}(t)]^n\right\rangle}{h}
% \label{km-2}
% \end{align}

% where $<.>$ refers to expectation. Only the first two moments of the equation \ref{km-1} are non zero when the process is Gauss-Markov. Kramers-Moyal expansion reduces to forward Fokker-Planck equation. For N variables, the Fokker-Planck equation is given by:

% \begin{align}
% \frac{\partial \boldsymbol{P}}{\partial t}=-\sum_{i=1}^N \frac{\partial}{\partial x_i}\left(D_i^{(1)}(\boldsymbol{x}) \boldsymbol{P}\right)+\sum_{i, j=1}^N \frac{\partial^2}{\partial x_i \partial x_j}\left(D_{i j}^{(2)}(\boldsymbol{x}) \boldsymbol{P}\right)
% \label{km-3}
% \end{align}

% Connection of above equation to equation \ref{eqn-1} is given by, $F(x) = \boldsymbol{D\textsuperscript{(1)}}$ and  $\sigma^2 = 2\boldsymbol{D\textsuperscript{(2)}}$. The drift and diffusivity is estimated by below equation using the experimental data.

% \begin{align}
% \boldsymbol{F}\left(\boldsymbol{x}^i(t)\right) & \approx \frac{\left\langle\boldsymbol{x}^i(t+h)-\boldsymbol{x}^i(t)\right\rangle}{h} \\
% \boldsymbol{\sigma^2}\left(\boldsymbol{x}^i(t)\right) & \approx \frac{\left\langle\left(\boldsymbol{x}^i(t+h)-\boldsymbol{x}^i(t)\right)^2\right\rangle}{h} 
% \end{align}

% \subsubsection{Part B: Determine analytical drift and diffusion using equation learning}


% \subsection{Method 2: Learning SDE with neural network using Euler Mauriyama Scheme:}

% $\mathrm{f}$ and  $\sigma$ in eqn 1 is estimated using two neural networks which we train using maximizing a likelihood function \cite{dietrich2021learning}. In order to integrate equation \ref{eqn-1} the Euler-Mauriyama scheme suggests an approximate solution:

% \begin{equation}
% x_1=x_0+h f\left(x_0\right)+\sigma\left(x_0\right) \delta W_0
% \label{eqn-2}
% \end{equation}

% Where $h>0$ is small and $\delta W_0$ is a vector of n iid random variables normally distributed with mean zero and variance h. Let's restrict to 1 dimension for simplicity. We can think of $x_1$ as a point sampled from a normal (multivariate) distribution which is conditioned on $x_0$ and h.


% We have a time series data of an observable x, we wish to find the underlying $\mathrm{f}$ and  $\sigma$ functions satisfying the below equation.
% \begin{align}
% \mathrm{d} x_t=f\left(x_t\right) \mathrm{d} t+\sigma\left(x_t\right) \mathrm{d} W_t
% \label{eqn-1}
% \end{align}

% \subsection{Method 1: Equation learning method:}
% As discussed by authors in \cite{nabeel2022pydaddy}, this method involves two steps:
% \begin{itemize}
%     \item{Given a time series data extract drift and diffusion components using jump moments or the Kramer-Moyal coefficients [cite]}
%     \item{Equation learning technique based on sparse regression is used to determine the analytical form of extracted drift and diffusion functions.[cite] }
% \end{itemize}

% \subsubsection{Part A: Using jump moments to estimate drift and diffusion - Kramers-Moyal expansion}

% Given a stochastic process \bold{x}(t), gradient of probability density P(\bold{x},t) with respect to time is given by:

% \begin{align}
% \frac{\partial P(\boldsymbol{x}, t)}{\partial t}=\sum_{n=1}^{\infty}\left(-\frac{\partial}{\partial \boldsymbol{x}}\right)^n \mathbf{D}^{(n)}(\boldsymbol{x}, t) P(\boldsymbol{x}, t)
% \label{km-1}
% \end{align}

% eqn \ref{km-1} is also referred to as Kramer's-Moyal expansion. The jumping of positions from x(t) to x(t+h) is referred to as moment of transition probability and determined by:

% \begin{align}
% D^{(n)}(\boldsymbol{x}, t)=\frac{1}{n !} \lim _{h \rightarrow 0} \frac{\left\langle[\boldsymbol{x}(t+h)-\boldsymbol{x}(t)]^n\right\rangle}{h}
% \label{km-2}
% \end{align}

% where $<.>$ refers to expectation. Only the first two moments of the equation \ref{km-1} are non zero when the process is Gauss-Markov. Kramers-Moyal expansion reduces to forward Fokker-Planck equation. For N variables, the Fokker-Planck equation is given by:

% \begin{align}
% \frac{\partial \boldsymbol{P}}{\partial t}=-\sum_{i=1}^N \frac{\partial}{\partial x_i}\left(D_i^{(1)}(\boldsymbol{x}) \boldsymbol{P}\right)+\sum_{i, j=1}^N \frac{\partial^2}{\partial x_i \partial x_j}\left(D_{i j}^{(2)}(\boldsymbol{x}) \boldsymbol{P}\right)
% \label{km-3}
% \end{align}

% Connection of above equation to equation \ref{eqn-1} is given by, $F(x) = \boldsymbol{D\textsuperscript{(1)}}$ and  $\sigma^2 = 2\boldsymbol{D\textsuperscript{(2)}}$. The drift and diffusivity is estimated by below equation using the experimental data.

% \begin{align}
% \boldsymbol{F}\left(\boldsymbol{x}^i(t)\right) & \approx \frac{\left\langle\boldsymbol{x}^i(t+h)-\boldsymbol{x}^i(t)\right\rangle}{h} \\
% \boldsymbol{\sigma^2}\left(\boldsymbol{x}^i(t)\right) & \approx \frac{\left\langle\left(\boldsymbol{x}^i(t+h)-\boldsymbol{x}^i(t)\right)^2\right\rangle}{h} 
% \end{align}

% \subsubsection{Part B: Determine analytical drift and diffusion using equation learning}


% \subsection{Method 2: Learning SDE with neural network using Euler Mauriyama Scheme:}

% $\mathrm{f}$ and  $\sigma$ in eqn 1 is estimated using two neural networks which we train using maximizing a likelihood function \cite{dietrich2021learning}. In order to integrate equation \ref{eqn-1} the Euler-Mauriyama scheme suggests an approximate solution:

% \begin{equation}
% x_1=x_0+h f\left(x_0\right)+\sigma\left(x_0\right) \delta W_0
% \label{eqn-2}
% \end{equation}

% Where $h>0$ is small and $\delta W_0$ is a vector of n iid random variables normally distributed with mean zero and variance h. Let's restrict to 1 dimension for simplicity. We can think of $x_1$ as a point sampled from a normal (multivariate) distribution which is conditioned on $x_0$ and h.


% We have a time series data of an observable x, we wish to find the underlying $\mathrm{f}$ and  $\sigma$ functions satisfying the below equation.
% \begin{align}
% \mathrm{d} x_t=f\left(x_t\right) \mathrm{d} t+\sigma\left(x_t\right) \mathrm{d} W_t
% \label{eqn-1}
% \end{align}

% \subsection{Method 1: Equation learning method:}
% As discussed by authors in \cite{nabeel2022pydaddy}, this method involves two steps:
% \begin{itemize}
%     \item{Given a time series data extract drift and diffusion components using jump moments or the Kramer-Moyal coefficients [cite]}
%     \item{Equation learning technique based on sparse regression is used to determine the analytical form of extracted drift and diffusion functions.[cite] }
% \end{itemize}

% \subsubsection{Part A: Using jump moments to estimate drift and diffusion - Kramers-Moyal expansion}

% Given a stochastic process \bold{x}(t), gradient of probability density P(\bold{x},t) with respect to time is given by:

% \begin{align}
% \frac{\partial P(\boldsymbol{x}, t)}{\partial t}=\sum_{n=1}^{\infty}\left(-\frac{\partial}{\partial \boldsymbol{x}}\right)^n \mathbf{D}^{(n)}(\boldsymbol{x}, t) P(\boldsymbol{x}, t)
% \label{km-1}
% \end{align}

% eqn \ref{km-1} is also referred to as Kramer's-Moyal expansion. The jumping of positions from x(t) to x(t+h) is referred to as moment of transition probability and determined by:

% \begin{align}
% D^{(n)}(\boldsymbol{x}, t)=\frac{1}{n !} \lim _{h \rightarrow 0} \frac{\left\langle[\boldsymbol{x}(t+h)-\boldsymbol{x}(t)]^n\right\rangle}{h}
% \label{km-2}
% \end{align}

% where $<.>$ refers to expectation. Only the first two moments of the equation \ref{km-1} are non zero when the process is Gauss-Markov. Kramers-Moyal expansion reduces to forward Fokker-Planck equation. For N variables, the Fokker-Planck equation is given by:

% \begin{align}
% \frac{\partial \boldsymbol{P}}{\partial t}=-\sum_{i=1}^N \frac{\partial}{\partial x_i}\left(D_i^{(1)}(\boldsymbol{x}) \boldsymbol{P}\right)+\sum_{i, j=1}^N \frac{\partial^2}{\partial x_i \partial x_j}\left(D_{i j}^{(2)}(\boldsymbol{x}) \boldsymbol{P}\right)
% \label{km-3}
% \end{align}

% Connection of above equation to equation \ref{eqn-1} is given by, $F(x) = \boldsymbol{D\textsuperscript{(1)}}$ and  $\sigma^2 = 2\boldsymbol{D\textsuperscript{(2)}}$. The drift and diffusivity is estimated by below equation using the experimental data.

% \begin{align}
% \boldsymbol{F}\left(\boldsymbol{x}^i(t)\right) & \approx \frac{\left\langle\boldsymbol{x}^i(t+h)-\boldsymbol{x}^i(t)\right\rangle}{h} \\
% \boldsymbol{\sigma^2}\left(\boldsymbol{x}^i(t)\right) & \approx \frac{\left\langle\left(\boldsymbol{x}^i(t+h)-\boldsymbol{x}^i(t)\right)^2\right\rangle}{h} 
% \end{align}

% \subsubsection{Part B: Determine analytical drift and diffusion using equation learning}


% \subsection{Method 2: Learning SDE with neural network using Euler Mauriyama Scheme:}

% $\mathrm{f}$ and  $\sigma$ in eqn 1 is estimated using two neural networks which we train using maximizing a likelihood function \cite{dietrich2021learning}. In order to integrate equation \ref{eqn-1} the Euler-Mauriyama scheme suggests an approximate solution:

% \begin{equation}
% x_1=x_0+h f\left(x_0\right)+\sigma\left(x_0\right) \delta W_0
% \label{eqn-2}
% \end{equation}

% Where $h>0$ is small and $\delta W_0$ is a vector of n iid random variables normally distributed with mean zero and variance h. Let's restrict to 1 dimension for simplicity. We can think of $x_1$ as a point sampled from a normal (multivariate) distribution which is conditioned on $x_0$ and h.

\bibliographystyle{icml2023}
\bibliography{references}


\newpage
\appendix
\onecolumn

\section{\label{sec:analytical} Analytically derived mesoscopic SDEs for agent-based models}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/analytical.pdf}
    \caption{\textbf{Analytically derived mesoscale dynamics of agent based models.} (a) Analytically derived drift field for a model with only pairwise interactions, showing a single attractor at $\bm = 0$. (b) Analytically derived diffusion field for the pairwise model, showing that the strength of diffusion is maximum at $\bm = 0$ and decreases outwards. (c) Analytically derived drift field for a model with higher-order (ternary) interactions, showing a ring-shaped attractor for a high value of $|\bm|$. (d) Analytically derived diffusion field for the ternary model simulation, showing the same pattern of diffusion as in the pairwise model.}
    \label{fig:fields-analytical}
\end{figure*}

The agent-based models of collective behaviour used in this study have been well-studied, and mesoscopic SDEs for these models have been analytically derived~\cite{jhawar2019bookchapter, jhawar2020fish}.

Recap the model description:
\begin{itemize}
    \item An agent $i$ can spontaneously turn and choose a new direction, i.e. $\theta_i(t) \rightarrow \eta$ for $\eta \sim \mathsf{Unif}[-\pi, \pi)$. The spontaneous turns happen as a Poisson process with a rate $r_1$.
    \item An agent $i$ can copy the direction of one (pairwise interaction) or the average direction of two (ternary interaction) other agents, chosen randomly from the group. The copy interactions can happen at a rates $r_2$ and $r_3$ respectively.
\end{itemize}

For the pairwise interaction model, the mesoscale SDE can be analytically derived to be of the following form:

\begin{align}
    \frac{d \bm}{dt} &=
    -r_1 \bm +
    \sqrt{ \frac{r_1 + r_2\left(1 - \modm^2\right)}{N}} I \cdot \boldeta(t) \label{eq:analytical-pairwise}
\end{align}

where $N$ is the total number of agents in the group.

Similarly, for the ternary interaction model, the derived mesoscale SDE has the form:

\begin{align}
    \frac{d \bm}{dt} &=
    -r_1 \bm + r_3 \left(1 - \modm^2 \right) \bm +
    \sqrt{ \frac{r_1 + (r_2 + r_3)\left(1 - \modm^2\right)}{N}} I \cdot \boldeta(t) \label{eq:analytical-ternary}
\end{align}

The diffusion term is a diagonal matrix, and has a $1 - \modm^2$ term. This means that the noise is maximum at $\modm = 0$ and decreases with increasing $\modm$. The drift term is linear in $\bm$ for the pairwise interaction model, and the deterministic stable equilibrium for this system is at $\modm = 0$. Therefore, the order in the pairwise interaction model is noise-induced, and arises solely due to the high strength of diffusion near $\modm = 0$. On the other hand, for the ternary interaction model, drift term is cubic in $\bm$, and has a $\left(1 - \modm^2\right)\bm$ term. This contributes to a ring-shaped attractor near $\modm = 1$, i.e. the order in the ternary interaction model originates from a combination of both deterministic and noise-induced effects.

The drift and diffusion fields generated from equations \ref{eq:analytical-pairwise} and \ref{eq:analytical-ternary} are shown in Figure~\ref{fig:fields-analytical}. There is a close correspondence between these plots and the ones in Figure~\ref{fig:fields-sim}, suggesting that the neural effective SDE approach is able to correctly infer the underlying mesoscale SDEs.

\section{\label{sec:groupsizes} Mesoscopic models for fish schools of different group sizes}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fields-groupsizes.pdf}
    \caption{\textbf{Estimated drift and diffusion fields for polarization dynamics of fish schools of different group sizes.} (a, b) Drift and diffusion fields for $\bm$ for a 15-fish school (c, d) Drift and diffusion fields for $\bm$ for a 15-fish school, same as in Figure~\ref{fig:fields-real}. (e, f) Drift and diffusion fields for $\bm$ for a 60-fish school.}
    \label{fig:groupsizes}
\end{figure}

We repeated the analysis in Section~\ref{sec:real-fish} on two other group sizes, \emph{viz.} $N=15$ and $N=60$. The fit metrics and drift/diffusion fields are shown in Figure.~\ref{fig:groupsizes}. Overall, the fields look qualitatively similar across different group sizes. However, the net strength of diffusion decreases with increasing group size (notice the range of the color axis). This is in accordance with theoretical predictions: in theory, we expect the diffusion term $\bg$ to scale as $1/\sqrt{N}$ (see Equation~\ref{eq:analytical-pairwise}). 

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
