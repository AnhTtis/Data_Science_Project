\section{Evaluation}

\begin{table}[]
\small
\centering
\singlespacing
\begin{tabular}{lcccc}
\toprule
name & NeRF & Thin & Semi-T &  Avg \\
\midrule
Plen ($\sigma=10$)    &          1.212 &    0.595 &            \second{}1.337 &    1.048 \\
Plen ($\sigma=50$)    &          0.944 &    0.609 &            1.344 &    \second{}0.966 \\
Plen ($\sigma=100$)   &          0.977 &    \second{}0.548 &            1.990 &    1.172 \\
Mip360 ($\sigma=10$)  &          1.699 &    1.796 &            4.900 &    2.798 \\
Mip360 ($\sigma=50$)  &          1.805 &    1.091 &            5.170 &    2.689 \\
Mip360 ($\sigma=100$) &          1.928 &    1.277 &            6.098 &    3.101 \\
NeuS                  &          1.798 &    0.834 &            2.313 &    1.648 \\
HFS                   &          \second{}0.550 &    1.086 &            2.090 &    1.242 \\
Ours                  &         \first{}0.534 &   \first{}0.366 &           \first{}0.843 &   \first{}0.581 \\
\midrule
Plen (depth 0.1)      &          0.590 &    0.441 &            0.974 &    0.668 \\
Mip360 (depth 0.9)    &          \firstalt{}0.396 &    0.320 &            1.414 &    0.710 \\
Mip360 (depth 0.1)    &          2.047 &    0.588 &            2.346 &    1.660 \\
Ours (trimmed)        &          \firstalt{}0.396 &    \firstalt{}0.304 &            \firstalt{}0.680 &    \firstalt{}0.460 \\
\bottomrule
\end{tabular}
    \caption{\textbf{Chamfer distance $\downarrow \times 10^{-2}$ on synthetic datasets.} We color the \colorbox{first}{best}, \colorbox{second}{second best} watertight surfaces and the \colorbox{first}{best} non-watertight surfaces.
    % \GR{I think it is still confusing to have two different colors for best results.}\WW{I changed to use the same color, but IMHO it's actually more confusing. We probably want to change back}\GR{Maybe just split it into two separate tables. Makes it also easier to reference from the text}
    % \WW{@fangcheng, hanxue what do you think of the table colors here? Should we make the watertight best a different color?}
    % Our method is capable of producing surfaces of both versions with overall better quality compared to the baselines. 
    Our method produces higher quality surfaces of both types than the baselines.
    % Note that our method does not rely on depth extraction and therefore is not affected by view selection. 
    % \CO{we are also better than mipnerf360, let's remove "on par with".}\WW{My concern is that if you look at individual results in the supp, you'll see that we are not always better -- they achieve lower cf on quite a few scenes than us. Should we just remove "on par" or "comparable"?}\GR{better/on par $\rightarrow$ overall better}
    } 
    \label{tab:chamfer}
\end{table}

\begin{figure*}
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/main_syn_short.png}
\end{center}
   \caption{\textbf{Qualitative evaluation on synthetic datasets.} Red color indicates the L1 error in the reconstruction. Our method can reconstruct the semi-transparent and thin surfaces missed in NeuS and HFS, while recovering more complete and noise-free surfaces compared to NeRF-based methods. Results on additional scenes and non-watertight surfaces can be found in the supplementary. 
   % \GR{last row is now a bit too close to the third.}\WW{The spacing is actually exactly the same... let me change the zoom size}
   }
\label{fig:main_syn}
\end{figure*}

We quantitatively and qualitatively evaluate our method on an extended version of the NeRF synthetic dataset~\cite{nerf}, with 8 additional objects with delicate and thin structures, and 8 objects with semi-transparent materials. We also show qualitative comparison on challenging real-world scenes from the LLFF dataset~\cite{llff} and captured by our own. 
% We do not evaluate on datasets such as DTU~\cite{dtu}, as they do not contain any thin structures or semi-transparent surfaces. 
% In fact, those types of geometry are challenging to be captured accurately with a depth scanner in the real world. 
We compare with recent SDF optimization methods including NeuS~\cite{neus} and HFS~\cite{HFS}, and both density level set and depth extracted geometry from Plenoxels~\cite{plenoxels}
% \GR{We actually compare to Plenoxels and never to vanilla NeRF}\WW{Nice catch, thx!} 
and MipNeRF360~\cite{mipnerf360}.

\subsection{Datasets}

\paragraph{NeRF Synthetic} NeRF synthetic dataset contains 8 objects with various challenging properties such as high specularity and thin structures. The synthetic objects are rendered with Blender~\cite{blender} to obtain RGB images. 
% However, we found that the training views in the original dataset are sampled from the upper hemisphere of the scene, making the bottom part of objects unobservable.
% \FZ{we probably do not need to justify everything. consider removing this explanation to save space}\WW{Simplified a bit, but I think the rest of the explanation is quite necessary for readers. I'll keep a note and see how much space we have in the end.} 
We sampled 100 different training views from a full sphere and re-rendered the original scenes to make the bottom part visible, except for ``ship" scene we also removed the bottom tank which blocks too many views. For each scene, we also rendered the depth, which we then converted to dense point clouds for quantitative geometry evaluation. This removes any invisible inner structures in the 3D assets.
% This depth-scanned geometry extraction simulates the process of obtaining ground truth geometry for real datasets~\cite{dtu} and removes any redundant inner structures in the 3D models.
\vspace{-10pt}
\paragraph{Thin \& Semi-Transparent Blender}
We build two additional synthetic datasets featuring objects with thin structures and semi-transparent materials respectively. The renderings and ground truth geometry of those additional scenes are obtained in the same way as the above NeRF synthetic dataset. We will release all the datasets with reference geometry upon publication. 

\vspace{-10pt}
\paragraph{LLFF~\cite{llff}} 
We qualitatively evaluate our method on scenes from the LLFF dataset, which contains forward-facing captures of challenging real-world scenes. 
% We selected scenes that contain thin structures such as leaves and branches to best demonstrate our method
% \GR{why just a few, sounds like cherry picking}. \WW{I just removed the sentence, is it ok now?}\GR{yes}
As we are not focusing on the novel view synthesis task, we use all the available images for training. We additionally captured a scene
%\GR{can we show more in the supp?}\WW{we have a few more scenes, but they don't look that nicely/or are a bit repetitive. I'll try to get some more, but for now I'll first focus on DTU}
featuring semi-transparent materials and processed it using a similar scheme to LLFF.




\subsection{Implementation Details}

% \WW{Need to mention L2 image loss}

Following Plenoxels~\cite{plenoxels}, we use a sparse voxel grid of size $512^3$ where each vertex stores the surface scalar $\delta$, raw opacity $\sigma_\alpha$ and 9 SH coefficients. We directly initialize all the grid values from Plenoxels pre-trained with provided hyperparameters and prune voxels with densities $\sigma$ lower than $5$.
% \GR{is density defined in the method?}\WW{I wanted to mean we discard grids with density lower than 5 during initialization, any better suggestion on the wording?}\GR{What I meant is, is there a symbol defined for the density refered here in the method section. If yes, use it here. If no, why not?}\WW{Ah I see, added the symbol}
% . Our implementation can also optionally initialize from a set of voxelated density or SH coefficients trained with other NeRF-based methods through SH fitting~\cite{plenoctrees}\GR{Do we show this somewhere (supp?). If not, remove this sentence}. 
We use $s_\sigma=0.05$ to downscale the density values during initialization.
For experiments on synthetic datasets, we initialize $5$ level sets at $\bm{\tau}_\sigma = \{10, 30, 50, 70, 90\}$ and linearly decay the truncated alpha compositing parameter $a$ from $5$ to $2$ in $10k$ iterations. 
% (only the first 5 intersections are taken into account at the beginning, and the first 2 in the end).
% For Semi-Trasparent Blender dataset, we use $\bm{\tau}_\sigma = \{10, 30\}$ and keep $a=2$.
For experiments on real-world scenes, we initialize a single level set $\bm{\tau}_\sigma = \{10\}$.
% and use the same truncated alpha compositing setting. 
A detailed list of hyperparameters can be found in the supplementary.
% Our code is based on PyTorch and CUDA, whose parallelism makes efficient training time possible. 
We train for $50k$ iterations with a batch size of $5k$ rays, which takes around 17 minutes for synthetic scenes and 22 minutes for real-world scenes on an NVIDIA A100-SXM-80GB GPU (excluding Plenoxels training). 
% More efficient implementation with sophisticated data structure and sparser regularization is certainly possible.

% Further optimization of the training time is certainly possible, such as applying surface normal and TV regularization on a portion of the grid. A detailed list of hyperparameters can be found in the supplementary.

% For the evaluation of our method, we directly sample points on our surfaces by sending 75 evenly-spaced rays along the x,y, and z-axis and solving the cubic polynomials. 
% % 
% We provide two different evaluations of our method, one where only the first level set $\tau_\sigma=10$ is extracted, hence the surface is guaranteed to be watertight\FZ{let's check math correctness}\WW{Should be, due to numerical precision and grid, we can never have surfaces represented with a thin layer of 0s -- that would require two vertices to have exactly 0s on them which is not possible. @cengiz tell me if I'm wrong}, and a ``trimmed" setting where we sample points from all level sets, and trim out ones with $\alpha < 0.1$ to remove any invisible surfaces.

\begin{figure*}[]
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/llff2.png}
\end{center}
   \caption{\textbf{Qualitative evaluation on real-world scenes.} We show the reconstructed surfaces on LLFF~\cite{llff} scenes and a scene captured by us. NeuS fails to reconstruct sensible surfaces, while ours can reconstruct more complete surfaces with fewer artifacts. 
   % \GR{Can we also hightlight an area on the third row, and also add the same highlights to col 2 and 3.}\WW{On it!, will make row 3 the same, but for col 2 and 3 I probably don't do cos there isn't much point and the zooms can be distracting.}\GR{I would prefer consistency, even if there is not much to zoom in}\WW{Thx, I'll keep a note on that and come back later}
   }
\label{fig:llff}
\end{figure*}

\subsection{Baselines}

We compared our method with the state-of-the-art SDF-based reconstruction methods, as well as NeRF-based methods with level set and mode depth-extracted geometry.

\vspace{-10pt}
\paragraph{SDF Methods} We compare with NeuS~\cite{neus} and HFS~\cite{HFS}. HFS is a follow-up work on NeuS and achieves more detailed geometry reconstruction. We did not include IDR \cite{idr}, UNISURF \cite{UNISURF} or VolSDF \cite{volsdf} as NeuS and HFS have shown superior performance. 
% We use marching cubes with a grid resolution of $512^3$ to extract explicit surface from the learned SDF representation, and then use the same point sampling function as in the official DTU evaluation script~\cite{dtu} to sample points for Chamfer distance calculation.
% \FZ{ealier we said we extract point clouds and here we extract mesh?}\WW{evaluation of SDF is as follows: SDF->mesh->points. You always use points to evaluate chamfer} \WW{call?}
% \WW{Should we mention HFS failed two scenes here?}\GR{Would mention it in detail in the experiment subsection where it fails.}\WW{Added}
% \ww{We noticed that the SDF-based methods tend to perform badly on scenes with semi-transparent materials due to their solidity assumption. They usually learn a large mesh surrounding the actual object and use view-dependent radiance to simulate a semi-transparent visual effect. For a fair comparison, we remove any mesh that is more than 0.5 away from the actual geometry during evaluation.} 
% \WW{Note that currently HFS fails on some scenes (give empty mesh after 500 iters). I plan to just state that and remove those scenes from HFS's average calculation}
% \GR{Is this still true with the bbox fixes?} \WW{Yes, that's still the case. I also tried tuning the learning rates a bit but still no success}
\vspace{-10pt}
\paragraph{NeRF Methods} We compared with Plenoxels~\cite{plenoxels} (referred to as ``Plen" in the table) and MipNeRF~360~\cite{mipnerf360} (referred to as ``Mip360"), an improved version of NeRF with conical frustum sampling and weight-concentration regularization. 
% For evaluating the surface from the thresholded density field, we use the same point-sampling algorithm as in our approach, where the density field is voxelated into a grid of size $512^3$, and 75 rays are sent for each voxel to sample points on surfaces. 
In addition to level set surfaces, we also compared with mode depth-extracted surfaces from NeRF methods. We send rays from 100 spiral views from elevation $-180 \degree$ to $180 \degree$ to cover the whole object, and extract samples with the highest weight along each ray. We found this approach gives the best performance compared to mean or median depth extraction. We also mask out samples extracted from rays with accumulation weight
%\GR{Is there a symbol defined for this weight in the method section?}\WW{@gernot, unfortunately we removed the technical background set and no longer have a symbol defined. Let's just use pure text here?}
less than 0.1 from Plenoxels, and less than either $0.1$ or $0.9$ from MipNeRF 360.
% To ensure the evaluation is done on well-defined surfaces and deal with the potential imbalance in the distribution of depth-extracted points, we first downsample the points with density 0.001, then run \ww{alpha shapes with $\alpha=0.003$ to reconstruct meshes for Chamfer distance evaluation}. We did not use TSDF~\cite{tsdf} or Poisson surface reconstruction~\cite{poisson_recon}, as the former tends to remove delicate structures and the latter can produce many redundant surfaces and requires per-scene pruning. Overall, we found out that alpha shape\cite{alpha_shape} gives the best result when reconstructing surfaces from very densely sampled points\GR{From this sentence it is not clear if alpha shapes is also used in the evaluation, or if it is just some observation.} \WW{See above highlighted sentence. I hope this should make things clear?}.

% Although comparison with this type of geometry is often missing in the past literature, we found that it provides very good accuracy and completeness, surpassing both of the SDF-based methods. 





\subsection{Evaluation on Synthetic dataset}




We quantitatively evaluate our method on all three synthetic datasets and report the Chamfer-L1 distance as used in DTU~\cite{dtu} in Table \ref{tab:chamfer}. We followed the evaluation protocol of DTU~\cite{dtu}, where we extracted dense point clouds from reconstructions and downsampled both predicted and ground truth points with 0.001 density before computing the Chamfer distance.
%\GR{Is the Chamfer evaluation metric defined somewhere?}\WW{Following NeuS, seems that it's not needed to define Chamfer. Ofc if you know a suitable citation, we can also cite it.}\GR{NeuS cites Unisurf and IDR and IDR writes it uses the Chamfer-L1 distance. Are we using that one? If yes, we could write: Chamfer-L1 distance as used in IDR}\WW{Ah I see, added that. Thx!}
HFS failed and learned empty surfaces on two scenes in the Thin Blender dataset, hence we removed them when calculating its average Chamfer. 
Our non-trimmed single surface provides better watertight surfaces than the watertight baselines. Our trimmed representation further improves the Chamfer distance by removing redundant surfaces trapped at low opacity region, also giving better or comparable surfaces than the depth-extracted surfaces from NeRF-based methods. 
% \WW{Can remove below?}
% The depth-extracted surfaces from MipNeRF360 with accumulation less than $0.9$ trimmed give a comparable Chamfer to ours on NeRF synthetic dataset, but it assumes solid objects and fails on the Semi-Transparent Blender dataset. 
% In addition, note that the both training and depth extraction of MipNeRF360 is much more inefficient than ours.

% In addition, the training of our method takes no more than 30 minutes (including training of Plenoxels) and surface extraction of both versions takes less than 5 seconds, whereas training of MipNeRF360 takes around 24 hours on the same GPU and depth extraction takes around 30 minutes. \WW{Remove this sentence?}

We show qualitative results of watertight surfaces in Figure~\ref{fig:main_syn}. While NeuS and HFS are capable of reconstructing very smooth surfaces, they miss the thin structures and do not perform well on semi-transparent materials. Level set surfaces from Plenoxels and MipNeRF360 contain holes, floaters, and inner surfaces; see Figure~\ref{fig:teaser}b for an inside view of the surfaces. 
% Depth-extracted surfaces from them are dependent on the views chosen and can be unstable.
Our approach is capable of reconstructing those surfaces with minimal artifacts.


\subsection{Evaluation on Real World Dataset}

We show the qualitative evaluation of watertight surfaces on real-world scenes in Figure~\ref{fig:llff}. NeuS fails to reconstruct proper surfaces and models everything as nearly flat surfaces due to the limited views. Plenoxels can recover surfaces with high-quality details through a low density level value, but tends to miss some surfaces and produce noisy density floaters. By initializing from Plenoxels and further optimizing with surface regularizations, our approach reconstructs surfaces with fewer artifacts. 
Note that although Plenoxels with level value $\sigma=10$ gives views with decent quality, it tends to leave many redundant surfaces inside the object. This is not well reflected due to the forward-facing nature of the dataset, but can be seen from our evaluation on synthetic datasets.
% as well as Figure~\ref{fig:teaser}. 



\subsection{Ablation}

We show the ablation study on the watertight surfaces from our method in Table~\ref{tab:abla}. 
% Specifically, we crop part of the outer surface in ``ficus" scene to show the inside view. 
The truncated alpha compositing deals with the inner volume artifacts inherited from initialization, while the surface regularization $\mathcal{L}_\mathbf{n}, \mathcal{L}_\delta$ and convergence loss $\mathcal{L}_c$ encourage smooth and complete surfaces. Using a single level set $\bm{\tau}_\sigma=\{10\}$ to initialize the surface may fail to capture all information in the pre-trained Plenoxels and causes artifacts in optimization. Ours with all techniques enabled achieves the best overall performance. More qualitative results can be found in the supplementary.

% \begin{figure*}
% \begin{center}
%   \includegraphics[width=0.95\textwidth]{figures/abla.png}
% \end{center}
% %    \caption{\textbf{Ablation Study.} }
% % \label{fig:abla}
% % \begin{table}[]
% % \centering

%    \caption{\textbf{Ablation Study.} We show the non-trimmed surfaces from different ablations of our method. 
%    %\GR{Why no qualitative results?}\WW{added}
%    \GR{Can be moved to supp if needed.}
%    % ``no truncate" refers to no truncated alpha compositing, $\mathbf{\tau}_\sigma$
%    % For ``ficus‚Äù scene, we crop part of the surface on the vase to show the inner structures.
%    }
% \label{fig:abla}
% % \end{table}
% \end{figure*}

\begin{table}[]
\small
\centering
\singlespacing
\tabcolsep=0.06cm
\begin{tabular}{lcccccc}
\toprule
{} & w/o $\mathcal{L}_\delta$ & w/o $\mathcal{L}_\delta$, $\mathcal{L}_\mathbf{n}$ & w/o trunc & w/o $\mathcal{L}_c$ & $\bm{\tau}_\sigma=\{10\}$ & Ours \\
\midrule
ficus &  0.828 &     1.121 &     0.922 &    0.467 &      0.637 &  \textbf{0.389} \\
lyre  &  0.586 &     0.702 &     0.583 &    0.242 &      0.404 &  \textbf{0.182} \\
\bottomrule
\end{tabular}
    \caption{\textbf{Quantitative results of ablation study.} We report the Chamfer distance $ \times 10^{-2}$ of our watertight surfaces. 
    % The full method achieves the best Chamfer in both cases.
    }
    \label{tab:abla}
\end{table}



% Although our approach can model semi-transparent surfaces to a better extent compared to existing methods, it still fails to reconstruct surfaces with very low opacity, where additional priors might be required to achieve reliable reconstruction. 
% Our surface is also less smooth compared to MLP-based SDF methods due to the lack of spatial smoothness encoded in the MLP. Using stronger surface regularization can enhance the smoothness, but also destroys thin structures in the reconstruction. Adapting to a hybrid representation similar to InstantNGP~\cite{instantNGP} where the surface scalars are obtained via latent codes and shallow MLP might help with this issue. Our representation also requires a background model to be applied on 360 \degree real-world scenes, but this can be implemented very easily using a similar approach to~\cite{nerf++,plenoxels}.