\begin{figure}
\begin{center}
  \includegraphics[width=0.45\textwidth]{figures/teaser.png}
\end{center}
    \vspace{-0.7em}
   \caption{\textbf{Comparison with SDF and NeRF methods.} a) Due to the assumption of solid surfaces, SDF methods such as NeuS~\cite{neus} fail to learn semi-transparent surfaces or thin structures with a strong blending effect. 
   b) We compare the inside views of the reconstructed surfaces with NeRF-based methods~\cite{mipnerf360, plenoxels} by cropping in the middle. Surfaces from NeRF-based methods contain artifacts such as floaters or inner surfaces. In comparison, our approach can accurately reconstruct surfaces exhibiting semi-transparency while removing noisy surface artifacts.
   The red color in geometry indicates the error of the reconstruction in L1 distance.}
    \vspace{-0.5em}
\label{fig:teaser}
\end{figure}


\section{Introduction}



Recovering surfaces from RGB images is a complex and challenging task in computer vision, with numerous practical applications such as photogrammetry, 3D asset creation, and custom 3D fabrication.
Traditional approaches rely on Structure-from-Motion~\cite{schoenberger2016sfm} and Multi-View Stereo~\cite{schoenberger2016mvs} pipelines to first reconstruct a 3D point set of the scene to which surfaces can be fitted~\cite{poisson_recon, delaunay}.
% \GR{cite poisson, delaunay} \WW{Added citation for delaunay, but can you check if this is the correct one?}.
% These techniques work by minimizing the reprojection error of detected key points across images during the reconstruction process\GR{that is only true for SfM (first step), but not for MVS (seconds step) and meshing}.\WW{@fangcheng can you help with it?}
% A crucial step in these pipelines is to minimize the reprojection error of geometrical entities such as feature points.
Differentiable rendering techniques have emerged as a more versatile reconstruction procedure. 
With properly derived gradients, these techniques can simultaneously learn all  scene parameters by minimizing the error of RGB renderings.
% significantly loosening the restrictions on the choice of geometric representations to be learned.
This significantly loosens the restrictions on the choice of geometric representations to be learned.
In particular, implicit surface representations~\cite{sdfdiff, diffsdf, idr, iron}, 
% \WW{I'd remove NeuS and VolSDF from this as they rely on volume rendering}
\ie level sets of a scalar field such as a signed distance function (SDF), show promising results in surface reconstruction due to their robustness to complex geometry and topology.



% \FZ{TODO: more citations need to be properly inserted for this new intro}

% One open challenge in this domain that previous research has not adequately addressed is reconstructing surfaces that exhibit semi-transparency effects. 
One open challenge that has not been adequately addressed in this domain is reconstructing implicit surfaces that exhibit semi-transparency effects. 
% \ww{This includes both surfaces made of semi-transparent materials and thin structures, which appear to be semi-transparent due to the blending effect; see Figure~\ref{fig:blending}.} 
The prevailing assumption in earlier works is that the surface is solid throughout, and differentiable rendering techniques for implicit surfaces only examine the intersection of rays and the nearest surface \cite{Niemeyer2020CVPR, sdfdiff, liu2020dist}. As a result, the forward rendering process in those studies cannot simulate scenes with non-solid effects, leading to an inability to reconstruct their surfaces; see Figure~\ref{fig:teaser}a. 
% \WW{Citation?}
%\GR{I think this puts now too much emphasize on the modelling of (semi)transparent surfaces. Is not really aligned with the teaser, experiments, etc.?}\WW{Later on we argue that thin structure is also a case of semi-transparency due to the blending effect. I think it's suitable to mention the issue with semi-transparency first, but we could try making sentence shorter.}

Modeling opaqueness is not solely crucial for reconstructing semi-transparent surfaces, but is also necessary for the recovery of extremely \emph{thin surfaces}. As illustrated in Figure~\ref{fig:blending}, when rendering a thin surface that only partially occupies a pixel with an insufficient number of sampled rays, solid foreground objects must be treated as semi-transparent in order to achieve an accurate pixel color blending the foreground and background. 
% \ww{Therefore, incorporating semi-transparency is also essential for accurately reconstructing thin surfaces.} \WW{Remove this sentence?}
Methods that neglect this feature may fail to capture the correct reconstruction of thin structures; see Figure~\ref{fig:teaser}a.



Differentiable volume rendering techniques ~\cite{nerf} have demonstrated considerable success in rendering semi-transparent and thin objects by integrating radiance along a ray through an entire scene.
Their geometric representation is a density field where a surface can be implicitly defined by applying a density threshold.
However, density represents occupancy (geometry) and transparency (material) in a tightly coupled manner, posing a non-trivial challenge in choosing the threshold for surface extraction. 
% using density thresholding.
% This coupling of geometry and materials within the density field presents a non-trivial challenge for extracting surfaces using straightforward density thresholding
For example, a high threshold value may exclude transparent objects from the reconstruction, whereas a low threshold results in the erroneous reconstruction of density floaters that are not part of the desired surface; see Figure~\ref{fig:teaser}b. 
% \FZ{TODO: one or two sentences about NeuS here.}\WW{On it!} 
NeuS~\cite{neus} attempts to alleviate this issue by combining SDF with volume rendering, but it still assumes a solid surface at the end of optimization and hence fails to reconstruct surfaces with semi-transparency; see Figure \ref{fig:teaser}a.
% More recently, NeuS~\cite{neus} tries to alleviate this issue by learning an SDF as the backbone and mapping the signed distance values to unbiased weights\FZ{what are the weights for? p.s. I removed the marching cube part as it is only needed for meshing while our interest is to just get implicit surface}\WW{Weights are the rendering weights in volume rendering}\GR{Talking about ... by maximizing the volume rendering weights ... might be too specific in the introduction. Not really understandable without more context.}\WW{I guess we have to state the solid assumption of NeuS somehow, hopefully this isn't too confusing} in volume rendering.
% Nevertheless, the optimization of NeuS still assumes solid surfaces and encourages surfaces to fully occlude the ray by maximizing the volume rendering weights. 
% Hence, it fails to accurately reconstruct semi-transparent objects; see Figure \ref{fig:neus_quick_view}. 
% Furthermore, NeuS starts with a centered sphere and deforms it to match the target shape. This process tends to completely miss any thin structures such as branches and strings during reconstruction; see Figure \ref{fig:nerf_crop}. Yet, NeuS cannot easily utilize per-scene shape prior \GR{what about MVS depth that can be used during optimization; needs to more specific what priors}\WW{I changed "and" to "by" in the later sentence to make the things more clear. Is it better now?} \ww{by} initializing with different shapes due to its use of neural representation\GR{Is it actually true? Can a pre-trained nerf not be reparametrized (density) to be roughly a SDF init?} \WW{Would be less convenient as you will need to fit your MLP. One possible trick is to take the original density MLP with modified last layer activation, but that fixes the network architecture}. 



\begin{figure}
\begin{center}
  \includegraphics[width=0.45\textwidth]{figures/blending.png}
\end{center}
   \caption{\textbf{Blending effect.} a) Real-world capture takes incoming light from multiple rays per pixel. Pixels that are partially occupied by a solid object are therefore rendered as a mixture of the object and background color. b) With one-sample rendering in reconstruction, it becomes necessary for the thin objects to be modeled as semi-transparent for the pixel color to match the ground truth. c) d) Our representation is fully capable of mimicking this phenomenon, leading to the accurate surface reconstruction of thin structures.
   %\GR{put a and b into one row, currently too big, too much white space}\WW{Done, much better now}
   }
\label{fig:blending}
\end{figure}


In order to faithfully reconstruct surfaces of semi-transparent and thin objects, two key requirements must be fulfilled: 1) a representation that explicitly decouples geometry and materials; and 
2) a differentiable rendering process that considers more than just the nearest intersection between the ray and the scene.
% \WW{I modified this due to our use of truncated alpha compositing, which encourages us to not consider all intersections.}\FZ{looks good}
% 2) a differentiable rendering process that considers all the intersections between a ray and the scene, instead of only the nearest one.
To this end, we introduce \name{}, a novel surface representation based on a grid structure without neural networks. 
% The grid values provide complete separation among geometry, opacity, and appearance. 
We use separate values on the grid to represent geometry, opacity, and appearance.
We define the surface as multiple level sets of a continuous scalar field, where the field itself is given by a trilinear interpolation of the grid values. Unlike previous methods, our representation does not require the scalar field to be an SDF subject to the Eikonal constraint.
An important advantage of our approach is that the \emph{exact} intersection points between a ray and all the surfaces, regardless of whether they are solid or transparent, can be determined by analytically solving a cubic polynomial. The \emph{closed-form} solution allows for full \emph{differentiability} in our forward rendering process, which simulates the semi-transparency effects via alpha compositing of intersection points.

% \WW{combine this paragraph?}
We further propose a series of initialization and optimization strategies that are designed to facilitate efficient and accurate reconstruction. 
% \FZ{Perhaps report the average training time and time needed for pre-training (initialization) here?}
The total training time of our method is around 30 minutes, including a short pre-training of Plenoxels~\cite{plenoxels} as initialization of our surface.
% time spent to pre-train Plenoxels~\cite{plenoxels} as the initialization.
% \GR{I think there is no need to highlight/mention the 10 min pre-training of Plenoxels in the intro.}\WW{I agree, just mentioning 30 minutes should be fine?}\FZ{I also removed Plenoxels here. Readers will find out how init is exactly done in method section.}\WW{I think we should put at least NeRF or plenoxels, reader would be confused what pre-training means.}
% including pre-training and initializing from plenoxels~\cite{plenoxels} which takes around 10 minutes.
We evaluate our method on an extended version of the NeRF synthetic dataset~\cite{nerf}, which contains 8 original scenes and 16 additional objects with challenging thin structures or semi-transparent materials. We show that our method is capable of reconstructing surfaces with better overall quality than the existing SDF and NeRF based methods. We also qualitatively evaluate our method on real-world scenes from the LLFF dataset~\cite{llff} and a scene featuring semi-transparent materials captured by us.

In summary, our contributions are: 
1) A novel grid-based scene representation for implicit surface reconstruction, specifically tailored for semi-transparent and thin objects. It incorporates a closed-form and differentiable evaluation of all surface intersections along the ray, and properly decouples geometry and opacity.
% 2) A series of initialization and optimization strategies that have proven to be effective in achieving efficient training and accurate reconstruction.
2) A series of initialization and optimization strategies for efficient training and accurate reconstruction.
3) We show overall superior reconstruction quality compared to state-of-the-art methods on scenes featuring thin and semi-transparent objects.

% 1) We propose a novel grid-based scene representation for implicit surface reconstruction, specifically tailored for semi-transparent and thin objects. It incorporates a closed-form and differentiable evaluation of all surface intersections along the ray, and properly decouples geometry and opacity.
% 2) We develop a series of initialization and optimization strategies that have proven to be effective in achieving efficient training and accurate reconstruction.
% 3) Our method demonstrates overall superior reconstruction quality compared to state-of-the-art methods on scenes featuring thin and semi-transparent objects.

% \begin{itemize}
%     \item We propose a novel grid-based scene representation for implicit surface reconstruction, specifically tailored for semi-transparent and thin objects, with two key features: 1) a closed-form (and thus fully differentiable) evaluation of exact intersection points between a ray and all the surfaces; and 2) proper decoupling of geometry and material properties.
%     \item We develop a series of initialization and optimization strategies that have proven to be effective in achieving efficient training and accurate reconstruction.
%     \item Our method demonstrates \ww{overall} superior reconstruction quality compared to state-of-the-art methods on scenes featuring thin and semi-transparent objects.
% \end{itemize}
% \GR{If space is needed (likely) the summary can be removed and the points hightlighted/emphasized in the text.}

% \WW{======= Old Version =======}

% % Sphere-tracing SDF
% % \FZ{Are we certain our closed-form ray-surface intersection is novel? If yes, it's best for us to evaluate the benefits of closed-form ray-surface intersection separately, as this is a contribution that is independent of decoupling occupancy and opacity.} \WW{I won't say it's super novel, as several past works have identified this approach, but didn't apply them}\FZ{Okay, in that case, I would introduce the issue with semi-transparency first and try motivating our closed-form ray-surface intersection as a way to find all the intersections (as opposed to the closest surface). I'll draft it.}\WW{Thx!}
% To render an implicit surface in a differentiable manner,
% early methods incorporate gradient tricks to render approximated \WW{are they all approximated?} ray-surface intersections with differentiable sphere-tracing~\cite{idr, sdfdiff, diffsdf}, 
% % Limitation of Sphere-tracing SDF
% but they only have gradients at first intersections and therefore perform unstably at sharp depth discontinuities.
% % NeuS
% NeuS~\cite{neus} alleviates this issue by incorporating differentiable volume rendering into SDF optimization. 
% % Limitation of NeuS
% Nevertheless, the optimization of NeuS still assumes solid surfaces and hence fails to accurately reconstruct semi-transparent objects or thin structures with blending effects; see Figure \ref{fig:nerf_crop} and \ref{fig:neus_quick_view}. 
% Furthermore, NeuS starts with a centered sphere and deforms it to match the target shape. This process tends to completely miss any thin structures such as branches and strings during reconstruction; see Figure \ref{fig:nerf_crop}. Yet, NeuS cannot easily utilize per-scene shape prior \GR{what about MVS depth that can be used during optimization; needs to more specific what priors}\WW{I changed "and" to "by" in the later sentence to make the things more clear. Is it better now?} \ww{by} initializing with different shapes due to its use of neural representation\GR{Is it actually true? Can a pre-trained nerf not be reparametrized (density) to be roughly a SDF init?} \WW{Would be less convenient as you will need to fit your MLP. One possible trick is to take the original density MLP with modified last layer activation, but that fixes the network architecture}.  
% % \WW{A bit difficult, as thin structure also has semi-transparency. I won't worry too much about this}

% % NeRF 
% Meanwhile, recent works on NeRF~\cite{nerf} present impressive outcomes in synthesizing novel views of scenes with thin structures and semi-transparent materials due to their ability to model volumetric density.
% % Limitation of NeRF 
% However, such volumetric geometry is entangled with material opacity, as the density in NeRF is used to represent both geometry location and the amount of ray occluded. \GR{a bit more specific what is meant here}\WW{added one sentence, better now?}\GR{Not yet, what does amount of ray occlued mean}. This makes it difficult to be converted to surface representation and utilized in existing mesh-based pipelines without introducing artifacts such as noisy inner volume or density floaters.
% Although there have been many attempts to regularize NeRF to produce a more concentrated volumetric field~\cite{mipnerf360, infonerf, refnerf}, those methods focus on preventing visual artifacts in novel view synthesis and it is still unknown how proper surfaces can be recovered from such a volumetric representation. 
% Existing approaches to extract explicit geometry from NeRF include density thresholding and depth extraction combined with surface reconstruction methods~\cite{tsdf, poisson_recon, alpha_shape}, but choosing suitable parameters that prevent artifacts is almost impossible; see Figure \ref{fig:nerf_crop}, whereas depth-extracted geometry does not guarantee watertight surfaces and its quality can be dependent on the views chosen. 

% % \WW{TODO: highlight the benefits of not being SDF and states we are more generalized version of it. Concern -- do we need to abla with Eikonal constraint?}
% % Motivation and aim
% To address the shortcomings of SDF-based approaches when dealing with thin and semi-transparent surfaces as well as the geometry ambiguity in NeRF, we present a novel grid-based surface optimization approach with disentangled opacity and appearance.
% % Introduce our method
% % Voxel-based surface
% Our surface is defined as the level sets of a continuous scalar field, which is represented through trilinear interpolations of discretized scalars stored in a voxel grid. It can be seen as a more generalized version of SDF, where the implicit scalar field is not required to be the distance to the closest surface.
% % Closed-form intersection
% A key insight is that the exact surface-ray intersection on our representation can be found in \textit{closed-form} through the analytical solution of a cubic polynomial, and is therefore \textit{differentiable by construction}. 
% % NeRF initialization
% Moreover, our explicit grid-based representation utilizes the efficient and robust convergence of volumetric training by coarsely initializing from a pre-trained NeRF. 
% The decoupled surface, opacity, and appearance can then be further optimized through alpha compositing to refine the inherited artifacts and reconstruct clean surfaces for semi-transparent or thin structures. 

% % \WW{======= Below is Common Part =======}

% We evaluate our method on an extended version of the NeRF synthetic dataset~\cite{nerf}, which contains 16 additional objects with challenging thin structures and semi-transparent materials. By initializing from trained Plenoxels~\cite{plenoxels}, a grid-accelerated NeRF, and further optimizing for 17 minutes, we show that our method is capable of reconstructing surfaces with better quality than the existing SDF and NeRF methods. We also qualitatively demonstrate our method on a few real-world scenes from the LLFF dataset~\cite{llff} and a scene featuring semi-transparent materials captured by us. \WW{Double check the claim} Our contributions include:

% \begin{itemize}
%     \item We introduce a novel grid-based surface representation that employs closed-form ray-surface intersections and differentiable alpha compositing for surface optimization.
%     \item  Our surface representation is disentangled from opacity and can therefore accurately model semi-transparent surfaces and thin structures with strong blending effects.
%     \item Our grid-based representation efficiently initializes from pre-trained NeRFs (Plenoxels), and is further optimized to refine the surfaces and achieve better overall reconstruction quality than the state-of-the-art methods on scenes with thin and semi-transparent objects.
%     % \item Our representation can be initialized from pre-trained NeRF (Plenoxels) and further optimized to refine the surfaces, achieving better or comparable performance to the state-of-the-art methods when reconstructing scenes with thin structures or semi-transparency. 
%         \WW{Check claim!}
% \end{itemize}






