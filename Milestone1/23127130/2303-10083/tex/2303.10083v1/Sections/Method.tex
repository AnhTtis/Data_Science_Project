\section{Method}

\begin{figure*}
\begin{center}
  \includegraphics[width=0.975\textwidth]{figures/method.png}
\end{center}
   \caption{\textbf{Method.} 
   a) Our surface representation is based on a voxel grid storing explicit values, without neural networks; see Section~\ref{sec:representation}. 
   b) We utilize a closed-form and differentiable method to compute ray-surface intersection. This is achieved by solving a cubic polynomial of depth $t$ with known parameters $f_0,...,f_3$ and $\tau_i$; see Section~\ref{sec:rendering}. 
   c) We incorporate surface-specific regularization such as truncated alpha compositing to obtain clean and accurate surfaces; see Section~\ref{sec:optimization}.
   d) We utilize a coarse initialization via Plenoxels~\cite{plenoxels} to start with roughly correct yet noisy surfaces. 
   e) The optimization results in clean and complete surfaces in the end.}
\label{fig:method}
\end{figure*}


% \begin{figure}
% \begin{center}
%   \includegraphics[width=0.45\textwidth]{figures/representation.png}
% \end{center}
%    \caption{\textbf{Representation.} We directly store and optimize surface scalars $\hat{\delta}$, surface opacity $\alpha$ \WW{Note this isn't exactly correct, as we store $\sigma_\alpha$, but I don't want to get into the details of that}, and spherical harmonics coefficients at each voxel vertex. Through trilinear interpolation of 8 nearby vertices, we obtain a continuous scalar field, where we take multiple level sets $\bm{\tau}$ as surface. \WW{A bit ugly, probably just don't use this figure}\GR{yes, the colors do not work at all here. Also the right arrows have two intersections, but only one is marked?}}
% \label{fig:representation}
% \end{figure}

Given a set of multi-view posed RGB images, our goal is to recover an implicit surface of the scene objects, particularly in cases involving thin structures or semi-transparency.
% containing thin structures or semi-transparent materials, our goal is to obtain an accurate surface reconstruction of it.
% \fz{To achieve this, we propose a grid-based representation that decouples geometry and material properties and employs a differentiable alpha compositing process to detect and render all the ray-surface intersections.}
% By decoupling surface geometry and opacity, our representation allows for the modeling of semi-transparent materials and thin structures.
% 
% ==== new version =====
% To achieve this, we design a grid-based representation where the grid values contain information pertaining to the surface's geometry and material properties (Figure~\ref{fig:method}a). 
Towards this, we design \name{}, a grid-based representation where the grid contains values pertaining 
% \WW{I changed the ``information ..." to this, as I feel that misleads ppl to think we are using latent code and have information implicitly encoded in it} 
to the surface's geometry and material properties (Figure~\ref{fig:method}a). 
This enables a closed-form evaluation of all the ray-surface intersections (Figure~\ref{fig:method}b), and thus a fully differentiable alpha composition (Figure~\ref{fig:method}c) to render the intersection points.
% \ww{with surface specific constraints via alpha truncation} 
Our method utilizes Plenoxels~\cite{plenoxels} to efficiently initialize a coarse surface (Figure~\ref{fig:method}d), and through the optimization of a photometric loss and additional surface regularization, we are able to reconstruct clean and accurate surfaces.

% Through coarse surface initialization from  we use photometric loss and additional surface regularizations to achieve more accurate reconstructions while alleviating the redundant and missing surfaces inherited during initialization. 
% \FZ{We need to refer to the main diagram here. Let me think about it}\WW{cool, thx!}
% \FZ{perhaps move this high-level intro of optimization to the start of \ref{sec:optimization}? Flows better immediately with the next sentence.}
% With coarse surfaces initialized from pre-trained NeRF models, we introduce truncated alpha compositing to achieve more accurate reconstruction while alleviating the redundant and missing surfaces inherited from NeRF. 

% \GR{Paragraph below can be removed}
% In the following sections, we explain the details of our representation (Section \ref{sec:representation}), its closed-form rendering (Section \ref{sec:rendering}), and the optimization scheme (Section \ref{sec:optimization}).

% \subsection{SDF and NeRF}\label{sec:sdf_nerf}
% \WW{We might completely remove this section due to page limit}

% % \CO{We need examples of where vol. density fails, where SDF fails, how we combine these to solve the problems.} \WW{see figure \ref{fig:neu}}

% An SDF is a function $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ that maps any spatial coordinate to a signed distance to the closest surface, where the sign indicates whether the sample is inside or outside of the object. The geometry is therefore represented as the surface $\mathcal{S}$ formed by the zero-level set of the SDF:

% \begin{align}
%     \mathcal{S} = \{\mathbf{x} \in \mathbb{R}^3 | f(\mathbf{x}) = 0\} \;\text{with}\, ||\nabla f || = 1 
%     \label{eq:eikonal}
% \end{align}

% \noindent
% where $||\nabla f || = 1$ is the Eikonal constraint that must be satisfied for a valid SDF. Note that SDF is a pure surface representation without any material properties such as appearance or opacity. Existing SDF optimization methods~\cite{neus, neus2, volsdf, HFS} assume solid surfaces and deform a centered sphere to match the target shape.
% % due to the lack of additional prior on the reconstruction target.
% % Optimization of SDF must start with an initial shape \GR{ref?} \WW{I'm not sure if there is a good ref to this because this is quite standard?}. Without additional prior information on the reconstruction target, the SDF is usually initialized as a centered sphere. 
% This causes issues when reconstructing semi-transparent surfaces or thin structures with blending effects, where those parts are often completely missed (see results from NeuS in Figure \ref{fig:nerf_crop} and \ref{fig:neus_quick_view} .)
% %in the optimized SDF; see Figure \ref{fig:neus_quick_view}.

% NeRF is a continuous 5D plenoptic function that models geometry as density $\sigma: \mathbb{R}^3 \rightarrow \mathbb{R}$ and non-Lambertian appearance as view-dependent radiance $\mathbf{c}: \mathbb{R}^3 \times \mathbb{S}^2 \rightarrow \mathbb{R}^3$. Given a camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ with origin $\mathbf{o}$ and direction $\mathbf{d}$, it can be rendered by integrating the radiance within a depth range~$[t_n, t_f]$ according to volumetric rendering equation:

% % $\sigma(\mathbf{x}) \in \mathbb{R}$
% % $\mathbf{c}(\mathbf{x}, \mathbf{d}) \in \mathbb{R}^3$

% \begin{align}
% \hat{C}(\mathbf{r}) &= \int_{t_n}^{t_f} T(t) \sigma(t) \mathbf{c}(t) \, dt 
% \label{eq:compositnerf}
% \\
% T(t) &= \exp\left(-\int_{t_n}^{t} \sigma(s) \, ds\right)
% \label{eq:transmittance}
% \end{align}

% \noindent
% where the notation is simplified as $\sigma(t)\equiv \sigma(\mathbf{r}(t))$ and $\mathbf{c}(t)\equiv \mathbf{c}(\mathbf{r}(t), \mathbf{d})$.

% \WW{Note that we need to find a better place for the below paragraph}
% The optimization of NeRF usually starts by initializing the density field to have small and non-zero values across the whole space. Therefore, most samples on the training ray have a non-trivial contribution to the final rendering and, hence, have rich gradients on them. Based on the photo-metric loss computed on multi-view input images, density values are optimized to form a volumetric geometry that roughly matches the target shape.
% %Based on the multi-view input images, density values at different spatial locations are trained to either increase or decrease to quickly become a volumetric geometry that roughly matches the target shape. 
% This process is substantially different from training
% an SDF as it does not involve deforming and moving an existing geometry, but instead optimizing a volumetric field with greater flexibility
% %converging a sparse volumetric field. (not sparse?)
% \CO{We need a better argument than deform surface vs. optimize density. What makes a vol. rep. + vol. rendering better than sdf + vol. rendering?} \WW{The main reason is that NeRF assumes no initial shape at all, while NeuS still does so and starts with a sphere. Moreover, there's no constrain in optimizing densities, but optimization of SDF is constrained by Eikonal constraint}, As a result, %NeRF performs extremely well when 
% NeRF exhibits exceptional performance in reconstructing thin structures with strong blending effects.
% However, it is non-trivial to convert its volumetric geometry to accurate surfaces, as native density thresholding or depth extraction tends to suffer with the geometry artifacts learned with NeRF.

\subsection{Representation}\label{sec:representation}

% \CO{A main contribution is: we have a representation that suports multiple surface patches. These don't have to represent closed surfaces.} \WW{I'll need to think about how we claim this contribution... as I feel it doesn't have very meaningful applications -- no matter how thin an object is in real life, it is technically still a closed surface.}\FZ{We need a paragraph of a high-level overview of our method. Decoupling geometry from materials and its benefit should be one of the key messages here.} \WW{I agree! Wrote one below, what do you think of it?}

% In order to reconstruct surfaces of semi-transparent objects and thin structures with strong blending effects, we propose a grid-based implicit surface representation that disentangles geometry from opacity and hence can model non-solid surfaces.
% \hx{where each voxel grid stores a scale value $\delta \in \mathbb{R}$ for surface representation, an alpha value $\alpha \in \mathbb{R}$ responsible for opacity and SH coefficients for radiance.} 

% Our representation utilizes explicit values stored on a voxel grid to represent surface, opacity, and appearance of the scene objects.
% The main idea resembles the spirit of multi-plane images (MPI) approaches~\cite{nex, mpi} where each image plane has varying opacity. The difference is that our surface is not constrained as planes, but can be deformed freely to reconstruct the target geometry. \FZ{We are only similar to MPI in rendering. let's move this to the rendering section, or remove it.} \WW{I was unable to find a good place to insert it in the rendering section. I think keeping it here is a good choice. Let's chat}

% Another key insight is that our surfaces can be rendered through closed-form alpha compositing and is fully differentiable, allowing the geometry, opacity, and appearance to be optimized end-to-end via photometric loss. 
% \FZ{feels that many points are repeated here. anything we should particularly mention that was not highlighted earlier?}\WW{I agree, maybe just remove this paragraph? (Maybe keep the argument about MPI)}\WW{How about now?}


\paragraph{Surface}\label{surface}

% To overcome the limitation of SDF learning on thin and semi-transparent objects while mitigating the geometry artifacts and ambiguity in NeRF, we propose a novel surface representation that can be directly initialized from an existing NeRF and can be optimized further via alpha compositing to achieve better quality. 
% \FZ{I would leave initialization to the implementation details and only focus on the representation formulation here. } \WW{I think this is an important part of our methodology, we have a more detailed section about this in \ref{sec:optimization}} \FZ{Yes, it is an important optimization strategy, which will be highlighted in \ref{sec:optimization}. However, in terms of presentation, the formulation of the representation should be separate from its reconstruction/optimisation. Only this way you can sell the representation formulation and its optimization strategy as two standalone contributions.} \WW{Ah I agree, I removed this and wrote a high-level overview.}

% The surface
% \hx{Our geometry(not our surface is represented using scalar field)} 
% is represented using a continuous scalar field $\delta: \mathbb{R}^3 \rightarrow \mathbb{R}$, 
We represent the surface as the level sets of a continuous scalar field $\delta: \mathbb{R}^3 \rightarrow \mathbb{R}$.
For each spatial coordinate $\mathbf{x}$ within a voxel $v_\mathbf{x}$, the value of the scalar field $\delta(\mathbf{x})$ is obtained by 
the trilinear interpolation of scalars stored at the voxel vertices:
\begin{equation}
    \delta(\mathbf{x}) = \trilearp(\mathbf{x}, \{\hat{\delta}_i\}_{i=1}^{8})
    \label{eq:trilearp_surf}
\end{equation}
% \begin{align}
% \delta(\mathbf{x}) = &\trilearp(\mathbf{x}, \{\hat{\delta}_i\}_{i=1}^{8})
% \label{eq:trilearp_surf}
    % \\
    % = &(1-z')( (1-y')((1-x')\hat\delta_{1}+x'\hat\delta_{5}) \nonumber 
    % \\
    % &+ y'((1-x') \hat\delta_{3} + x' \hat\delta_{7})  ) \nonumber
    % \\
    % &+ z'( (1-y')((1-x') \hat\delta_{2}+x' \hat\delta_{6})  \nonumber
    % \\
    % &+ y'((1-x') \hat\delta_{4} + x' \hat\delta_{8})  )
    % \label{eq:trilearp_surf}
%    \\
%\mathcal{S} =& \{\mathbf{x} \in \mathbb{R}^3 | \delta(\mathbf{x}) \in \bm{\tau}\}
% \end{align}
% \GR{Eq 2 can also be removed, or moved to the supp mat if space is needed.}
\noindent
where 
% $x', y', z' \in [0,1]$ are the relative coordinates with respect to the local coordinate system of the voxel $v_\mathbf{x}$, 
$\{\hat{\delta}_i\}_{i=1}^{8}$ are the surface scalars stored at its eight adjacent vertices.
% \FZ{you may state the straightforward continuous version first. Explain the issue to motivate the current formulation.} \WW{What do you mean by the straightforward continuous version?} \FZ{e.g. use an MLP to store a $ \mathbb{R}^3 \rightarrow \mathbb{R}$ mapping.} \WW{The issue in that case would be that it's no longer possible to find intersection via cubic polynomials, but explaining that would take a long paragraph. Do you think that's really necessary? I don't think MLP is the more natural and straightforward way tho}\FZ{You can still do ray marching to find the intersection if the implicit field is represented by an MLP. But you may argue that evaluating an MLP would be more expensive than solving a polynomial.} \WW{Then you will need some technique to approximate the gradient, as sphere tracing is not differentiable. Eitherway I think what we have is the most straightforward approach -- it already exists in graphics before MLP took over the field.} 
% \FZ{I think this is an excellent point to motivate our design --- it gives well-defined gradients. It is worth being highlighted somewhere.}
% The surface is then implicitly defined as level sets $\bm{\tau} = \{\tau_i\}_{i=1}^n$ on the scalar field which defines the surface $\mathcal{S}$ as:
The surface is then implicitly defined as level sets on the scalar field. Specifically, given a set of constants $\bm{\tau} = \{\tau_i\}_{i=1}^n$ which we refer to as \emph{level values}, the surface $\mathcal{S}$ is defined as:
% \begin{align}
% \mathcal{S} =& \{\mathbf{x} \in \mathbb{R}^3 | \delta(\mathbf{x}) \in \bm{\tau}\}
% \end{align}
\begin{equation}
%\mathcal{S} =& \{\mathbf{x} \in \mathbb{R}^3,\tau_i\ \in \bm{\tau}  | \delta(\mathbf{x}) = \tau_i\}
\mathcal{S} = \{\mathbf{x} \in \mathbb{R}^3 | \exists \tau \in \bm{\tau}: \delta(\mathbf{x}) = \tau\} \,.
\end{equation}
% The value of $\tau_i$ and the number of level sets $n$ are selected through hyperparameters tuning.
The cardinality and values of $\bm{\tau}$ are determined through hyperparameter tuning.
% \FZ{if we only extract a single level set at the end. I would consider stating that our representation only stores one level set in this sentence, and explain that it is sufficient to represent multiple surface patches with a single level set. Initialization with multiple level sets would be explained in the optimization section.}\WW{Our non-watertight setting (trimmed surface) extracts from all level sets. That's also a technique that shows to lower cf by a lot.} 
Note that this implicit surface field does not model distance as in SDF, therefore, it is not subject to the Eikonal constraint.

% When we shoot a ray $\mathbf{r}(t)$ from posed camera views, we obtain the interaction points by xxx 
% \hx{@Walter, I think you miss a very important paragraph here explaining how you get close form solution of interaction, }

\vspace{-5pt}
\paragraph{Opacity and Appearance}
Within the same voxel grid, we also model the surface opacity denoted as $\alpha(\mathbf{x})$ and view-dependent appearance denoted as $\mathbf{c}(\mathbf{x}, \mathbf{d})$ 
% \hx{explicitly}
in the same explicit style as in Plenoxels~\cite{plenoxels}. $\mathbf{c}(\mathbf{x}, \mathbf{d})$ is represented via coefficients of the spherical harmonic (SH) function which maps view direction $\mathbf{d}$ to radiance. Trilinear interpolation is applied to obtain opacity and SH coefficients at surface locations. 
% \hx{We calculate the opacity and SH value at the ray-surface intersection points from equation xxx via trilinear interpolating corresponding values stored in adjacent voxel grids.}
Note that although $\alpha(\mathbf{x})$ is essentially defined across all valid voxels in the 3D space, it is only utilized where the surface exists.

% and is used to represent the opacity of surfaces only. 
% 
% 
% 
% \vspace{-10pt}
% \paragraph{Closed-Form Alpha Compositing}
\subsection{Differentiable rendering}
\label{sec:rendering}
% \FZ{made it a new subsection as it is seperate from representation} \WW{Nice, I agree}

A key feature in the rendering process of our representation is that it does not involve any Monte-Carlo sampling as in NeRF or sphere tracing as in SDF-based methods. Instead, it relies on ray-voxel traversal and directly takes samples at the ray-surface intersections found through a \textit{closed-form} and fully \textit{differentiable} function. Specifically, for each camera ray with origin $\mathbf{o}$ and direction $\mathbf{d}$, we first determine the set of voxels it traverses through and substitute the ray equation $\mathbf{r}(t) = \mathbf{x} = \mathbf{o} + t \mathbf{d}$ into Eq\onedot~\ref{eq:trilearp_surf}.
% \GR{o and d are only defined below in the substitution, would mention them here. Also c in the paragraph above is dependent on d, c(d)}
for each voxel $v_\mathbf{x}$. 
By setting $\delta(\mathbf{x})$ to each level set value $\tau_i$, we obtain a cubic polynomial $\tau = f_3 t^3 + f_2 t^2 + f_1 t + f_0$ with a single unknown $t$.
% 
%Specifically, for each camera ray, we first determine the voxels group it intersects with. For each voxel $v_i$ in the group, we substitute the ray equation $\mathbf{x} = \mathbf{o} + t \mathbf{d}$ into Eq \ref{eq:trilearp_surf} and set $\delta(\mathbf{x})$ to each level set value $\tau_i$. In this way, we obtain a cubic polynomial equation with a single unknown $t$:
% 
% \begin{align}
%     \tau = f_3 t^3 + f_2 t^2 + f_1 t + f_0
% \end{align}
% \GR{Not that interesting equation, can be written inline to save space.}
% 
% \noindent 
$f_0, \ldots, f_3$ are known coefficients obtained from camera origin $\mathbf{o}$, ray direction $\mathbf{d}$ and voxel surface scalars $\{\hat{\delta}\}_{i=1}^{8}$. 
% \FZ{how do you determine which cell of $\{\hat{\delta}\}$ the ray intersects with? do you exclude cells that do not contain a surface? }\WW{through ray-voxel traversal (don't think it's super relevant to our method so I didn't mention it). Cells with no surfaces would not give valid intersections, so it doesn't matter we include them or explicitly remove them}\FZ{if you have to check every cell for ray-voxel traversal w/o any data structure or excluding empty cells, I would imagine that wouldn't be so cheap?} \WW{We are not claiming efficiency as one of the contributions, but actually with sparse voxel grid and CUDA this traversal is very fast (can be faster tho)}\WW{TODO: mention surface checking}
We refer to the supplementary materials and \cite{ray_trace_sdf} for a detailed derivation. The real roots of this cubic polynomial can be found in closed-form via Vieta's approach~\cite{numerical_recipes}, which minimizes the error caused by the numerical precision issues. Roots that give intersections outside of each corresponding voxel $v_\mathbf{x}$ are deemed invalid and removed. The remaining intersections are ordered from near to far and taken as samples for rendering. For all intersection points $\{t_i\}$ along the ray, we obtain their surface opacity $\alpha$ and view-dependent radiance $\mathbf{c}$ through trilinear interpolation and evaluating the SH function, and then perform alpha compositing to render the pixel color:
% \CO{let's write this is a special case of the general vol. rendering eq.s in sec. 3.1.}\WW{Noted! As we might completely remove 3.1, let's check it at a later time}\WW{3.1 removed}
%
\begin{align}
\hat{C}(\mathbf{r}) &= \sum_{i=1}^N T_\alpha(t_i) \alpha(t_i) \mathbf{c}(t_i)
\label{eq:alpha_compositing}
\\
T_\alpha(t_i) &= \prod_{j=1}^{i-1} (1-\alpha(t_j))
\label{eq:alpha_transmittance}
\end{align}
% \FZ{we never defined $\mathbf{r}$, just $\hat{C}(\mathbf{o}, \mathbf{d})$ would be clear. } \WW{I added $\mathbf{r}$ above. It is widely used later so we should have it.}
% where we simplify the notation and omit $\mathbf{d}$.
where we simplify our notation as $\alpha(t_i)\equiv \alpha(\mathbf{r}(t_i))$ and $\mathbf{c}(t_i)\equiv \mathbf{c}(\mathbf{r}(t_i), \mathbf{d})$.
As the samples are taken analytically through cubic polynomial root solving, which is a fully differentiable function, we have gradients from the photometric loss back to the implicit surface field, without requiring any approximation or re-parameterization trick. 
% It is then possible to use the photometric loss $\mathcal{L}(\mathbf{r}) = (C(\mathbf{r}) - \hat{C}(\mathbf{r}))^2$ to further optimize the surface, opacity, and appearance altogether.



\subsection{Optimization}\label{sec:optimization}

\paragraph{NeRF Initialization} 
One advantage of our representation is that we can easily initialize coarse surfaces from a pre-trained NeRF
% which shows great robustness against thin structures and significant efficiency\FZ{not sure if I grasp the main message. Left a marker here.}\WW{training of NeRF is more robust and efficient as it does not need to concern with geometry constraints such as Eikonal constraint.}\FZ{I see there is an entire paragraph later explaining why this initialization is good. Probably better keep all the arguments there and only explain what's done here.}\WW{I think this single sentence is good here, to motivate a little bit why we decided to do this. Let's leave it here for now and remove it if we don't have space.} when incorporating grid-based acceleration techniques~\cite{instantNGP, plenoxels, directvoxgo}
. In practice, we use Plenoxels~\cite{plenoxels}, a grid-based NeRF method that can be trained efficiently within 10 minutes. 
% We initialize the implicit surface field by simply taking the density values $\sigma(\mathbf{x})$ at the locations of voxel vertices. \ww{We then choose a set of values $\bm{\tau}_\sigma$ to be used as the level sets on the discretized voxel density to obtain initial coarse surfaces.}
% \FZ{Let's also be careful calling $\bm{\tau}_\sigma$ level sets. $\bm{\tau}_\sigma$ is just a set of constants. The surface is the actual level set. I'll rewrite as I see.}\WW{Thx! You are right, I'll also keep an eye.}\FZ{To avoid confusion, let's refer to $\tau$ as level values (I think this is the term used in math). I've established the use of this term in \ref{sec:representation} }\WW{Thx! that's a lot more clear!}
% \ww{For ease of optimization, we transform both the surface scalars and level set values to have a relatively constant scale and a zero level set:}
% \FZ{Is $\bm{\tau}$ a trainable parameter or fixed?} \WW{it is a hyperparameter and is fixed} 
% \CO{we need to state that we only initialize these at the voxel vertices.} \WW{I agree! Tried to highlight it here, but the sentence could still be confusing. @fangcheng what do you think? Any ideas?}
After fitting a NeRF, we obtain a density field $\sigma(\mathbf{x})$ from which we select a set of raw level values $\bm{\tau}_\sigma$ to define the initial surfaces. We then normalize the density $\sigma(\mathbf{x})$ to be used as our initial surface scalars $\delta(\mathbf{x})$, and normalize the raw level values to be used as our level values:
% 
\begin{align}
    \delta(\mathbf{x}) &= \frac{\sigma(\mathbf{x}) - \tilde{\tau}_\sigma}{\overline{||\nabla \sigma||}}
    \\
    \bm{\tau} &= \left\{\frac{\tau_\sigma - \tilde{\tau}_\sigma}{\overline{||\nabla \sigma||}} \mid \tau_\sigma \in \bm{\tau}_\sigma \right\}
\end{align}
% \FZ{There seem to be some notation issues here. $\bm{\tau}$ was used earlier in \ref{sec:representation} to indicate a set rather than a number.}\WW{Yeah it's meant to be a set, how about now?} \CO{let's use non-bold for scalars}\WW{Should be fixed by now, any new issue you spotted?}
\noindent
where $\tilde{\tau}_\sigma$ is the median of the chosen raw level values,
% chosen density level sets, 
$\overline{||\nabla \sigma||}$ is the average norm of the finite difference gradient of the voxelated density field and is used to keep the initialized surface field within a constant range to make optimization easier. 
Note that $\bm{\tau}_\sigma$ is a hyperparameter and is defined only to make the selection of initial surface level values more convenient, whereas $\bm{\tau}$ is the actual level set values of the implicit surface field used throughout the optimization.

As Plenoxels also uses a grid-based implementation, this initialization can be done very efficiently by directly taking and transforming the density values stored on the grids.
% Initialization advantage for non-distance field
Our initialization scheme allows the geometry information to be maximally preserved and inherited to the surface field. For example, some surfaces could have been modeled using density values slightly lower than the chosen level sets $\bm{\tau}_\sigma$ due to the ambiguity in NeRF training. After initialization, those spatial locations would have surface scalars very close to $\min(\bm{\tau})$, regardless of their actual distance to the initialized surfaces. This is a strong indication of potentially missed surfaces and the later optimization can easily update the surface scalars to allow them to emerge back again. % Both advantages are because our implicit surface field is not required to be a valid SDF.



We also initialize the opacity and SH field from the pre-trained Plenoxels to facilitate optimization. After taking the raw density values $\sigma$ in the voxel grid, they are rescaled with a constant $s_\sigma$ to be used as raw surface opacity $\sigma_\alpha$, then mapped to opacity through a combined exponential-ReLU activation:
% 
% \WW{I moved the confusing $\sigma_\alpha$ part here, can someone have a read?}
% \FZ{looks good to me}
% \FZ{Better add spatial coordinates just to be consistent: e.g. $\sigma_\alpha(\mathbf{x})$}
% \FZ{Can we just merge the following two Eqs? is $\sigma_\alpha$ needed anywhere?}
% 
% \hx{more clear to just use $\sigma$ instead of $\sigma_\alpha$}
% \CO{What's the diff. between $\sigma$ and $\sigma_\alpha$? What is $\alpha$? We never define it.} \WW{Modified the wording a little bit. Can you have a look again and see if it makes more sense? Thx!} \CO{let's put this eq. into the nerf initialization section, it is very confusing that we say we interpolate alpha, and then say alpha depends on sth. else we did not define.  also, this relation is not essential for the representation, it is an extension.} \WW{I agree! On it}
% 
\begin{align}
\sigma_\alpha &= s_\sigma \ \sigma
\\
\alpha &= 1 - \exp\left(-\relu(\sigma_\alpha)\right) \,.
\end{align}
% 
Note that this activation resembles the mapping from discretized sample density to opacity in volume rendering without the step size term~\cite{nerf}. Unlike sigmoid which has a vanishing gradient towards 0, this activation can more easily
% does not have saturating gradient and 
encourage sparsity in the surface opacity to remove redundant surfaces. The rescaling $s_\sigma$ is to ensure that initialized raw opacities do not map to $\alpha$ with too high values after removing the step size term, causing the gradients to be saturated. 
Note that, during training, we update $\sigma_\alpha$ as the training parameters rather than $\alpha$.
% \WW{I'd prefer the old way of saying it. It sounds like $\alpha$ is a parameter that's trainable, however it isn't.}\FZ{better?}\WW{I removed "directly updating" after "rather than", see above. Do you think it's ok?}\FZ{looks great}
After initialization, $\sigma$ from NeRF is discarded and our subsequent optimization is carried out on $\sigma_\alpha$. 
The SH coefficients are also initialized from the pre-trained Plenoxels and further optimized.
% The SH field can be directly obtained from SH-based NeRF models~\cite{plenoxels, plenoctrees}, or through an SH fitting process on other NeRFs~\cite{plenoctrees}. All of the surface, opacity, and SH fields are further optimized together during training.

In comparison to our approach, SDF methods such as NeuS~\cite{neus} cannot easily take the advantages of initializing from Plenoxels or other NeRF-based methods 
%\WW{@gernot modified it into this, sounds good?} 
due to two key aspects: 1) initializing the weights of the network to become an SDF that matches the coarse shape learned by NeRF is difficult, as it requires additional optimization to fit the shape while satisfying the Eikonal constraint;
%\GR{In what respect is it difficult}\WW{Added, looks good?}
 and 2) even for explicit grid-based SDF methods~\cite{sdfdiff, diffsdf}, algorithms such as fast sweeping~\cite{fast_sweep2, fast_sweep} are required to explicitly assign the grid values as distance to closest surface~\cite{sdf_redistance, sdf_redistance2, sdf_redistance3}. 
 % to satisfy the Eikonal constraint
 % \GR{now Eikonal constraint is mentioned twice}\WW{I'd just remove the second eik}
This process can be done efficiently, but it no longer preserves the information in the initialized density field, making the removal of redundant surfaces and recovery of missing surfaces more difficult. 
% \GR{(last paragraph) Not sure such a detailed comparison to SDF based methods (NeuS) is needed in the method section.}

% In the actual training, we also scale down $\delta_\alpha$ by $0.01$ \GR{how is this exact value justified?} \WW{In fact it's not really decided yet, it's a hyperparameter that still undergoes fine-tuning. But the idea is that we would like to start with lower alpha values so we have gradients for all intersections initially.} 


\paragraph{Truncated Alpha Compositing} 
% \WW{rewrote this to mention backface culling first, please have a check}
As shown in~\cite{refnerf}, a critical artifact in vanilla NeRF-like methods, including Plenoxels, is that they tend to learn low-density surfaces and inner volumes that are only visible from certain angles to represent high-frequency view-dependent appearance. This leads to very noisy inner surfaces when initialized from Plenoxels; see Figure~\ref{fig:teaser}b and \ref{fig:method}. 
To regularize those artifacts, we first remove ray intersections on backward-facing surfaces, and define the remaining set $\mathcal{\setLetter}$ of intersection points to be considered for rendering and regularization: 
\begin{equation}
    t_i \in \mathcal{\setLetter}\,\,\text{if}\, \underbrace{\mathbf{n}(t_i) \cdot \mathbf{d} \leq 0}_{\text{back-face culling}}
\end{equation}
% \begin{align}
%     F = \{t_i, \underbrace{-\mathbf{n}(t_i) \cdot \mathbf{d} \geq 0}_{\text{back-face culling}} \}_{i=1}^N
% \end{align}
\noindent
where $t_i$ is an original intersection, $\mathbf{n}(t_i) = \frac{-\nabla \delta(t_i)}{||\nabla \delta(t_i)||}$ is the surface normal, and 
$\mathbf{n}(t_i) \cdot \mathbf{d}$ checks whether the surface is facing backwards. This is similar to back-face culling in rendering. We then apply additional constraints by incorporating a truncated version of alpha compositing, where the later intersections along are ray are down-weighted to give less contribution to the rendering. Specifically, we obtain the re-weighted sample opacity as:
% 
\begin{align}
    % \alpha^*(t_i) &= \max\left(w_a(i), \beta \right) \ \alpha(t_i) \  \mathbb{I}[\nabla \delta(t_i) \cdot \mathbf{d} \geq 0]
    \alpha^*(t_i) &= \gamma(i-1) \ \alpha(t_i) 
    \label{eq:truncated_alpha}
    \\
    \gamma(x) &= \left(1-\cos(\pi \clamp(a-x, 0, 1))\right) / 2
\end{align}
% 
\noindent
where $i$ is the index of the intersection starting from 1. Note that this is the index with the back-face intersections excluded. 
During training, we linearly reduce $a$ so that $\gamma(i-1)$ is only greater than zero for a smaller range of $i$.
% \FZ{what about a more concrete argument --- e.g. this acts as a prior that not all the surfaces are transparent in the scene. Also, we need a pointer to the ablation for this loss.}\WW{The reason why we have this truncation is explained in the beginning of paragraph, I also don't want to point to abla as it's getting too long.}
% \ww{This acts as a prior that not all the surfaces are transparent in the scene}
I.e., only the first $\ceil(a)$ intersections are kept and the rest are truncated. 
% This provides us with a more customized approach for regularizing the surface while supporting multi-surface rendering.
% Moreover, as most rays would intersect with a level set at least twice, once when entering the surface and once when leaving it, we compare the gradient of the surface field against the ray direction through dot product and remove samples taken on the inward-facing surfaces via the indicator function $\mathbb{I}$. Note that such intersections with inward-facing surfaces do not increment the sample count $i$.\WW{mention backface cull}
We now re-define our alpha compositing using this truncated version of opacity values: $\hat{C}^*(\mathbf{r}) = \sum_{t_i \in \mathcal{\setLetter}} T_\alpha^*(t_i) \alpha^*(t_i) \mathbf{c}(t_i)$, where $ T_\alpha^*(t_i) = \prod_{j=1}^{i-1} (1-\alpha^*(t_j))$.
% , and the $t_i$ here and in all subsequent equations are taken from the set $F$ instead.


% \paragraph{Truncated Alpha Compositing} As shown in~\cite{refnerf}, a critical artifact in vanilla NeRF-like methods including Plenoxels is that they tend to learn low-density surfaces with inner volumes that are only visible from certain angles to represent high-frequency view-dependent appearance. This leads to very noisy inner surfaces when initialized from Plenoxels; see Figure~\ref{fig:nerf_crop} and \ref{fig:method}. To regularize the inherited inner surface artifacts, we \fz{remove backward-facing surfaces and} apply additional constraints by incorporating a truncated version of alpha compositing, where the total number of ray-surface intersections is recorded along each ray, and the later intersections are down-weighted to give less contribution to the rendering. Specifically, we obtain the re-weighted sample opacity as:


% \begin{align}
%     % \alpha^*(t_i) &= \max\left(w_a(i), \beta \right) \ \alpha(t_i) \  \mathbb{I}[\nabla \delta(t_i) \cdot \mathbf{d} \geq 0]
%     \alpha^*(t_i) &= \gamma(i) \ \alpha(t_i) \  \underbrace{\mathbb{I}[\nabla \delta(t_i) \cdot \mathbf{d} \geq 0]}_{\text{back-face culling}}
%     \label{eq:truncated_alpha}
%     \\
%     \gamma(i) &= \left(1-\cos(\pi \clamp(a-i, 0, 1))\right) / 2
% \end{align}

% \noindent
% \WW{explain $i$ better}
% where $i$ is the index of the intersection starting from 0, $\mathbb{I}[\nabla \delta(t_i) \cdot \mathbf{d} \geq 0]$ is an indicator function that checks the direction of the ray against surface normal, and removes the intersection on backward-facing surfaces. This approach is similar to back-face culling in rendering. Note that the back-face intersections are actually excluded and therefore do not increment the index $i$ for later intersections. 
% During training, we linearly reduce $a$ so that $\gamma(i)$ is only greater than zero for a smaller range of $i$.
% % \FZ{what about a more concrete argument --- e.g. this acts as a prior that not all the surfaces are transparent in the scene. Also, we need a pointer to the ablation for this loss.}\WW{The reason why we have this truncation is explained in the beginning of paragraph, I also don't want to point to abla as it's getting too long.}
% % \ww{This acts as a prior that not all the surfaces are transparent in the scene}
% Thus, only the first $\ceil(a)$ intersections are kept and the rest are truncated. 
% % This provides us with a more customized approach for regularizing the surface while supporting multi-surface rendering.
% % Moreover, as most rays would intersect with a level set at least twice, once when entering the surface and once when leaving it, we compare the gradient of the surface field against the ray direction through dot product and remove samples taken on the inward-facing surfaces via the indicator function $\mathbb{I}$. Note that such intersections with inward-facing surfaces do not increment the sample count $i$.\WW{mention backface cull}
% We then re-define our alpha compositing using this truncated version of opacity values: $\hat{C}^*(\mathbf{r}) = \sum_{i=1}^N T_\alpha^*(t_i) \alpha^*(t_i) \mathbf{c}(t_i)$, where $ T_\alpha^*(t_i) = \prod_{j=1}^{i-1} (1-\alpha^*(t_j))$.



% \WW{Faking sampling removed as it contributes very little to the result. Possibly because the background blending issue is partially resolved via surface opacity, and geometry error at pixel level is really small.}

% \paragraph{Fake Sampling} 

% \WW{Warning: latest ablation result seems to suggest this fake sampling is not needed (or give very little contribution). I'll look into it and potential remove this section}
% Although the rendering of our surface can be done deterministically, the sampling and training would be limited to only where the ray-surface intersects, causing optimization at depth discontinuities to be irregular. In addition, pixels containing the object edge would appear to have a mixed appearance of both the object and the background. Such a phenomenon is not simulated in a native surface intersection rendering. Hence, We take additional samples where the camera ray misses a surface in the voxel and refer to them as fake samples. More exactly, for a ray $\mathbf{r}$ and a voxel $\mathbf{v}$ which contains a valid surface (i.e., surface scalars $\delta$ at its 8 vertices do not have the same sign across any level set), we compute the intersections between ray and the voxel boundaries $t_{\mathbf{v}n}, t_{\mathbf{v}f}$, and take a fake sample at the middle: $t_\mathbf{v}' = (t_{\mathbf{v}n} + t_{\mathbf{v}f}) / 2$.

% Ideally, we would like to find out the distance from the fake sample to the closest surface and apply a penalty on its opacity based on the distance, so a fake sample far away from the actual surface contributes less to the rendering. However, due to the complexity of determining the exact distance to the implicit surface, we approximate it via trilinear interpolation of a locally normalized surface scalar grid.

% \begin{align}
% d &= \trilearp(t_\mathbf{v}', \delta_{000},...,\delta_{111}) / std(\delta_{000} ,..., \delta_{111})
% % \\
% % \delta_{000}',...,\delta_{111}' &= \frac{\delta_{000} ,..., \delta_{111}}{std(\delta_{000} ,..., \delta_{111})}
% \end{align}
% \GR{Notation using $\sim$ is not clear to me.} \WW{I wanted to express $\delta_{000}$, $\delta_{001}$, $\delta_{010}$, $\delta_{011}$,..., all the way to $\delta_{111}$, do you know any better notation?} \GR{maybe define the set $\mathfrac{D}$ that contains all $\delta$ and use the set.} \WW{I changed it to $...$ notation and explained in the 3.2 section, do you think it's better?}

% \noindent
% That is, we normalize the trilinear interpolation by the standard deviation of surface scalars at 8 vertices. This prevents the estimated distance $d$ to vary arbitrarily with a global scale on surface scalars while the actual surface remains unchanged. We then apply an un-normalized Gaussian on the distance to re-weight the opacity of the fake sample: $\alpha_f^* = exp(- \frac{d^2}{2s^2} ) \alpha^*$, where $s$ is the standard deviation of Gaussian and is gradually decreased during the training to limit the contribution of fake samples. Note that fake samples are re-weighted with both truncated alpha compositing and the surface distance, but they are not considered as real intersections and therefore do not increment the sample count $i$ in Eq. \ref{eq:truncated_alpha}. 

% \GR{Why only one fake sample, and why exactly this one? Would it help to generate more fake samples, even outside of the given voxel and use an approximation for the distance?} \WW{Good point. I have implemented unlimited fake samples as a hyperparameter and tested it, and it doesn't seem to improve the result so I disabled it. I think it is probably related to the fake that our surface representation is very local, taking fake sample and optimizing surface values at voxel far away from an existing surface would hardly help any with the optimization of that surface.}

\vspace{-5pt}
\paragraph{Regularization} 
We additionally apply regularizations to enforce smooth surfaces and mitigate the artifacts inherited from initialization. First, a surface convergence loss is applied to each 
% \FZ{what is non-trivial ray-surface intersection?}\WW{intersections with non-trivial opacity. Any better word?} 
ray-surface intersection with non-trivial truncated opacity to encourage different level sets to converge together, as a single level set is sufficient to represent multiple disconnected surface patches:
% First, a surface convergence loss is applied on each ray-surface intersection that is non-trivial to encourage the initialized multi-level set to converge together, \ww{leading to a single surface in the end.} \FZ{explain why they should converge together first?}\WW{Hard to say. They should just be the same surface anyway?}:
% \FZ{write the threshold as a variable and report the actual number being used in implementation?} \WW{I feel that's over complicating things. Note that this is not a hyperparameter and is not tuned}
% 
% \begin{align}
%     \mathcal{L}_c(t_i) &= |\tilde{t} - t_i| \ \mathbb{I}[\alpha^*(t_i) > 1\mathrm{e}{-8}]
%     \\
%     \text{where } w(\tilde{t}) &= \max(\{w(t_i)\}_i)
%     \\
%     w(t_i) &= T_{\alpha^*(t_i)} \alpha^*(t_i)
% \end{align}
% 
% \WW{Can someone check if the notation $t \in \mathbf{r}$ in particular is ok?}\FZ{$t$ is the intersection points exactly the same as $t_i$ in eq~\ref{eq:alpha_compositing} right? we should keep the notation consistent here. I left some comments in eq~\ref{eq:alpha_compositing}. }
\begin{equation}
\begin{split}
    \mathcal{L}_c(\mathbf{r}) &= \sum_{t_i \in \mathcal{\setLetter}} |\tilde{t} - t_i| \ \mathbb{I}[\alpha^*(t_i) > 1\mathrm{e}{-8}]
    % \\
    % \text{where } w(\tilde{t}) &= \max(\{w(t_i), t \in \mathbf{r}\})
    % \\
    % w(t) &= T_{\alpha^*(t)} \alpha^*(t)
\end{split}
\end{equation}
% 
% \FZ{Seems this loss may not only regularize the geometry but the alpha value as well?}
\noindent
where $\tilde{t}$ is the depth of the sample with the highest rendering weight $w(t_i) = T^*_{\alpha}(t_i) \alpha^*(t_i)$ on the ray, and $\mathbb{I}$ is the indicator function. This loss remedies the out-growing surfaces initialized from the sparse density floaters by encouraging them to move towards the actual surface location. The surface is also smoothed via a combination of an L1 normal smoothness regularization and a total variation (TV) loss applied on the surface field:
% 
\begin{equation}
\begin{split}
\mathcal{L}_\mathbf{n} &= \frac{1}{|\mathcal{V}|}  \sum_{\mathbf{x} \in \mathcal{V}} | \nabla \mathbf{n}(\mathbf{x}) |
\label{eq:l1_norm_reg}
\\
\mathcal{L}_\delta &= \frac{1}{|\mathcal{V}|}  \sum_{\mathbf{x} \in \mathcal{V}} ||\nabla \hat{\delta}(\mathbf{x})||
\end{split}
\end{equation}
% 
\noindent
% \FZ{is $\bm{v}$ a spatial coordinate? perhaps re-write it as $\bm{x}$ to be consistent with earlier eqs}\WW{Fixed}
where $\mathcal{V}$ contains the spatial coordinates of all voxels,
% all the voxels initialized from Plenoxels, 
$\nabla \hat{\delta}(\mathbf{x})$ is the surface gradient at vertices obtained using finite differences on the adjacent grid values. Note that normal regularization is applied regardless of whether the voxel contains a valid surface or not, as it can help to smooth the overall surface field instead of just the actual surface. While the normal regularization $\mathcal{L}_\mathbf{n}$ encourages smooth surfaces in a local scope, it alone struggles to remove redundant inner surfaces. $\mathcal{L}_\delta$ is more effective for regularizing redundant surfaces with large errors; see Figure \ref{fig:abla_crop}. 
% The surface TV loss penalizes the gradient magnitude to encourage a simple scalar field, while the thin structures can still be represented using surface scalars with a smaller scale, thanks to the non-distance representation of our implicit surface.
Note that the use of $\mathcal{L}_\delta$ is only possible because our implicit surface field is not an SDF and does not need to satisfy the Eikonal constraint.
% \FZ{this sentence is too long. is it needed?}  \WW{Modified, better?}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/abla_crop.png}
    \caption{Reconstruction without regularization tends to have redundant inner surfaces. Encouraging consistent normals in the local neighborhood via $\mathcal{L}_\mathbf{n}$ enforces smoother surfaces, but the redundant surfaces still remain. With both normal regularization and TV loss applied on the surface scalar field, we can obtain clean and smooth reconstruction.}
    \label{fig:abla_crop}
\end{figure}

Finally, we regularize the opacity field with a weight-based entropy loss adapted from~\cite{infonerf} on each ray and an L1 sparsity loss:
% 
\begin{align}
\mathcal{L}_\mathcal{H}(\mathbf{r}) &= - \sum_{t_i \in \mathcal{\setLetter}} \bar{w}(t_i) \log(\bar{w}(t_i))
\label{eq:w_entropy}
\\
\text{where } \bar{w}(t_i) &= \frac{T^*_{\alpha}(t_i) \alpha^*(t_i)}{\sum_{t_j \in \mathcal{\setLetter}}T^*_{\alpha}(t_j) \alpha^*(t_j)}
\\
\mathcal{L}_\alpha &= \frac{1}{|\mathcal{V}'|} \sum_{\mathbf{x} \in \mathcal{V}'} |\relu(\sigma_\alpha(\mathbf{x}))|
\label{eq:alpha_sparse} 
\end{align}
% 
\noindent
where 
% (excluding fake samples \GR{fake samples not used anymore?} \WW{will be updated})
$\mathcal{V}'$ is 10\% of all existing voxels sampled uniformly at each iteration. They together encourage surfaces to have more concentrated and minimal opacity. Note that $\mathcal{L}_\mathcal{H}(\mathbf{r})$ does not always give beneficial regularization for scenes with semi-transparent effects, but we empirically found that having this term with a small weight can help remove the noise in the surface opacity.

Overall, the optimization target is:
% 
\begin{align}
\mathcal{L} &= \frac{1}{|B|} \sum_{\mathbf{r} \in B} \Big( || C(\mathbf{r}) - \hat{C}^*(\mathbf{r})||^2 
+ \lambda_c \mathcal{L}_c(\mathbf{r}) 
\nonumber \\
&+ \lambda_\mathbf{n} \mathcal{L}_\mathbf{n} 
+ \lambda_\delta \mathcal{L}_\delta 
+ \lambda_\mathcal{H} \mathcal{L}_\mathcal{H}(\mathbf{r})
+ \lambda_\alpha \mathcal{L}_\alpha \Big) 
\end{align}
% 
\noindent
where $\lambda_c, \lambda_\mathbf{n}, \lambda_\delta, \lambda_\mathcal{H}, \lambda_\alpha$ are hyperparameters. 
% \GR{Are the lambda values defined somewhere? How have they been derived?}\WW{Thx, fixed}

% \FZ{We need to state the total loss function somewhere?}\WW{How important would that be? As we don't have space to mention the regularization weights, I was planning to put everything in supp, is it ok?}\WW{Added one for now}
% \mathcal{L}_\alpha(\mathbf{x}) &= [1 - \bar{w}(\mathbf{x})] log(\sigma_\alpha(\mathbf{x})) \text{, if } \sigma_\alpha(\mathbf{x}) > 0
% \\




% Eq \ref{eq:l1_norm_reg} is an L1 regularization on the gradient of surface normals. We apply it in the same style as the sparse Toal-Variation loss~\cite{neural_volume}, where we sample 10\% of grids during each training iteration and apply it to them.  Eq \ref{eq:w_entropy} is the weight entropy loss adapted from~\cite{infonerf} and is applied to all samples along each training ray 
% % \GR{Why is the weight normalized, not really needed, as the values are already between 0 and 1?} \WW{This is because entropy loss without regularization would encourage the sum of weights to be either 0 or 1, and I thought the former case could cause an issue, but let me actually try without reg}
% . It encourages the weight distribution along the ray to concentrate around a single sample and removes sparse floater artifacts. Eq \ref{eq:alpha_sparse} is a normalized weight-dependent sparsity loss applied to all samples, where $[]$ is the gradient-stopping operator. The aim of this loss is to remove the sparse surface floaters and inner surfaces inherited from NeRF. \GR{Wouldn't it be better to avoid those inner surfaces from the NeRF init in the first place?} \WW{This is unfortunately very difficult to avoid, assuming an original version of NeRF, see slack.}




