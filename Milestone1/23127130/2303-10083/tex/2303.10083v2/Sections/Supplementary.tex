\clearpage

\appendix
{
    \centering
    \Large
    \textbf{\name{}: Implicit Surface Reconstruction for Semi-Transparent and Thin Objects with Decoupled Geometry and Opacity} \\
    \vspace{0.5em}Supplementary Material \\
    \vspace{1.0em}
}


\section{Overview}


In the supplementary material, we include additional experiment details and evaluation results. 
% We also present extra experiments on the DTU~\cite{dtu} dataset to demonstrate our performance on general and smooth surfaces and show that although our approach does not achieve state-of-the-art performance due to the lack of smoothness constraint, we can still effectively regularize and improve the surfaces initialized from Plenoxels \cite{plenoxels}\FZ{Probably remove this sentence --- too detailed as an overview of the supplementary. Also there is no need to highlight our limitations lol} \WW{lol I agree}. 
We also encourage the reader to watch the video results contained in the supplementary files.


% \section{Code and Data}

% We attach the source code, the example training script, as well as an example scene in the zip file. Due to the size limit, we are unable to attach the ground truth geometry. Full data will be released upon publication.

\section{Implementation Details}

\subsection{Closed-Form Intersection}



As briefly mentioned in Section~\ref{sec:rendering} of our paper, we determine the ray-surface intersections through the analytical solution of cubic polynomials. Note that a similar technique has been identified in previous works \cite{ray_trace_sdf, sdfdiff}, but they applied it on SDF only. We identify that the same approach can be applied to a more generalized implicit surface field without the Eikonal constraint. We now present the detailed derivation of it. 

% Given a voxel $v_\mathbf{x}$, the value of the implicit surface field within the voxel can be determined through trilinear interpolation of eight surface scalars stored on the vertices of $v_\mathbf{x}$:


Given a camera ray $\mathbf{r}(t) = \mathbf{o} + t \mathbf{d}$ with origin $\mathbf{o}$ and direction $\mathbf{d}$, our aim is to find the intersections between the ray and a level set surface with value $\tau_i$ within a voxel $v_\mathbf{x}$, which $\mathbf{r}(t)$ is guaranteed to hit. The value of the implicit surface field within $v_\mathbf{x}$ can be determined through the trilinear interpolation of eight surface scalars stored on the vertices of $v_\mathbf{x}$:
% 
% \begin{align}
% \delta(\mathbf{x}) = &\trilearp(\mathbf{x}, \{\hat{\delta}_i\}_{i=1}^{8})
%     \\
%     = &(1-z')( (1-y')((1-x')\hat\delta_{1}+x'\hat\delta_{5}) \nonumber 
%     \\
%     &+ y'((1-x') \hat\delta_{3} + x' \hat\delta_{7})  ) \nonumber
%     \\
%     &+ z'( (1-y')((1-x') \hat\delta_{2}+x' \hat\delta_{6})  \nonumber
%     \\
%     &+ y'((1-x') \hat\delta_{4} + x' \hat\delta_{8})  )
%     \label{eq:trilearp_surf}
% \end{align}
\begin{align}
\delta(\mathbf{x}) = &\trilearp(\mathbf{x}, \{\hat{\delta}_i\}_{i=1}^{8})
    \\
    = &(1-z)( (1-y)((1-x)\hat\delta_{1}+x\hat\delta_{5}) \nonumber 
    \\
    &+ y((1-x) \hat\delta_{3} + x \hat\delta_{7})  ) \nonumber
    \\
    &+ z( (1-y)((1-x) \hat\delta_{2}+x \hat\delta_{6})  \nonumber
    \\
    &+ y((1-x) \hat\delta_{4} + x \hat\delta_{8})  ) 
    \label{eq:trilearp_surf}
\end{align}

\noindent
where $[x,y,z] = \mathbf{x} - \mathbf{l}$ are the relative coordinates within the voxel, and $\mathbf{l} = \floor(\mathbf{x})$.
% \GR{that should be floor instead of ceil?}\WW{you are right! Thx!}
% $\{\mathbf{l}, \mathbf{l} + [0,0,1], \mathbf{l} + [0,1,0],...\mathbf{l} + [1,1,1]\}$ are the eight vertices of voxel $v_\mathbf{x}$. I.e., $\mathbf{l}$ is the lower-left-front vertex of voxel $v_\mathbf{x}$ \GR{What is $\mathbf{l}$}.
Note that $x,y,z \in [0,1]$. 
We first determine the near and far intersections $t_n, t_f$ between the ray and voxel $v_\mathbf{x}$ through the ray-box AABB algorithm, and then redefine a new camera origin $\mathbf{o}' = \mathbf{o} + t_n \mathbf{d} - \mathbf{l}$. 
We hence directly have $[x,y,z] = \mathbf{o}' + t' \mathbf{d} \in [0,1]$, where $t' = t - t_n$ without the need for calculating relative coordinates again. By denoting $\mathbf{o}' = [o'_x,o'_y,o'_z]$, $\mathbf{d} = [d_x, d_y, d_z]$, we substitute the above as well as $\delta(\mathbf{x}) = \tau_i$ into Equation~\ref{eq:trilearp_surf}:



\begin{align}
    \tau_i = &(1-(o'_z + t' d_z))( (1-(o'_y + t' d_y))
    \nonumber  \\
    &((1-(o'_x + t' d_x))\hat\delta_{1}+(o'_x + t' d_x)\hat\delta_{5}) \nonumber \\
        &+ (o'_y + t' d_y)((1-(o'_x + t' d_x)) \hat\delta_{3} + (o'_x + t' d_x) \hat\delta_{7})  ) 
        \nonumber \\
        &+ (o'_z + t' d_z)( (1-(o'_y + t' d_y))
        \nonumber \\
        &((1-(o'_x + t' d_x)) \hat\delta_{2}+(o'_x + t' d_x) \hat\delta_{6})  
        \nonumber \\
        &+ (o'_y + t' d_y)((1-(o'_x + t' d_x)) \hat\delta_{4} + (o'_x + t' d_x) \hat\delta_{8})  ) \,.
\end{align}

By re-arranging the equation, we obtain:

\begin{align}
    \tau_i = f_3 t'^3 + f_2 t'^2 + f_1 t' + f_0
\end{align}
 
 \noindent
 where 

\begin{equation}
\begin{split}
 f_0 = &(m_{00} (1-o'_y) + m_{01} (o'_y)) (1-o'_x) 
 \\ &+ (m_{10} (1-o'_y) + m_{11} (o'_y)) (o'_x)
\\   
 f_1 = & (m_{10} (1-o'_y) + m_{11} (o'_y)) d_x + k_{1} (o'_x)
 \\ &-(m_{00} (1-o'_y) + m_{01} (o'_y)) d_x + k_{0} (1-o'_x) 
 \\
 f_2 = &k_{1} d_x+h_{1} (o'_x)-k_{0} d_x+h_{0} (1-o'_x)
 \\
 f_3 = &h_{1} d_x-h_{0} d_x
\end{split}
\end{equation}

\noindent
and
\begin{equation}
\begin{split}
m_{00} =& \hat{\delta}_1   (1-o'_z) + \hat{\delta}_2   (o'_z)
\\
m_{01} =& \hat{\delta}_3   (1-o'_z) + \hat{\delta}_4   (o'_z)
\\
m_{10} =& \hat{\delta}_5   (1-o'_z) + \hat{\delta}_6   (o'_z)
\\
m_{11} =& \hat{\delta}_7   (1-o'_z) + \hat{\delta}_8   (o'_z)
\\
k_{0} =& (m_{01} d_y + d_z (\hat{\delta}_4 - \hat{\delta}_3) (o'_y)) 
\\ &- (m_{00} d_y - d_z (\hat{\delta}_2 - \hat{\delta}_1) (1-o'_y))
\\
k_{1} =& (m_{11} d_y + d_z (\hat{\delta}_8 - \hat{\delta}_7) (o'_y)) 
\\ &-(m_{10} d_y - d_z (\hat{\delta}_6 - \hat{\delta}_5) (1-o'_y))
\\
h_{0} =& d_y d_z (\hat{\delta}_4 - \hat{\delta}_3) -d_y d_z (\hat{\delta}_2 - \hat{\delta}_1)
\\
h_{1} =& d_y d_z (\hat{\delta}_8 - \hat{\delta}_7) -d_y d_z (\hat{\delta}_6 - \hat{\delta}_5) \,.
\end{split}
\end{equation}

Therefore, we obtain a cubic polynomial with a single unknown $t'$. Note that here we only sketch the main idea. For the actual implementation, we refer to~\cite{ ray_trace_sdf} which provides a more concise implementation that formulates the cubic polynomials with fewer operations through the use of fused-multiply-add. 

We then incorporate Vieta's approach~\cite{numerical_recipes} to solve the real roots for $t'$ in an analytic way. Namely, we first re-write the cubic polynomial as follows:

\begin{align}
\tau_i &= t'^3 + a t'^2 + b t' + c
\\
a &= \frac{f_2}{f_3}, b = \frac{f_1}{f_3}, c = \frac{f_0}{f_3} \,.
% \\
% b &= f_1 /f_3
% \\
% c &= f_0 /f_3
\end{align}

Then, compute:

\begin{align}
Q &= \frac{a^2 - 3b}{9}
\\
R &= \frac{2a^3 -9ab+27c}{54}  \,.
\end{align}

If $R^2 < Q^3$, we have three real roots given by:

\begin{align}
\theta &= \arccos(\frac{R}{\sqrt{Q^3}})
\\
t'_1 &= -2 \sqrt{Q} \cos (\frac{\theta}{3}) - \frac{a}{3}
\\
t'_2 &= -2 \sqrt{Q} \cos (\frac{\theta -2\pi}{3}) - \frac{a}{3}
\\
t'_3 &= -2 \sqrt{Q} \cos (\frac{\theta +2\pi}{3}) - \frac{a}{3}
\end{align}

where $t'_1 \leq t'_2 \leq t'_3$. This can be trivially seen from $0 \leq \theta \leq \pi$, $\sqrt{Q} \geq 0$ and $\cos (\frac{\theta}{3}) \geq \cos (\frac{\theta -2\pi}{3}) \geq \cos (\frac{\theta +2\pi}{3})$.

If $R^2 \geq Q^3$, we only have a single real root. First compute:

\begin{align}
A &= -\sign(R) \left( |R| + \sqrt{R^2 - Q^3} \right)^{1/3}
\\
B &=
\begin{cases}
  Q/A, & \text{if}\ A \neq 0 \\
  0, & \text{otherwise}
\end{cases} \,.
\end{align}

Then, the only real root can be obtained as:

\begin{align}
    t'_1 = (A+b) - \frac{a}{3} \,.
\end{align}

The intersection coordinate can therefore be determined as $\mathbf{r}(t_n + t')$. We then check the intersections against the bounding box of each voxel $v_\mathbf{x}$ to remove any samples outside of the voxels.
Besides, the cubic polynomial might return multiple valid real roots within the voxel if $R^2 < Q^3$. If the roots are unique, that means the ray intersects with the same surface multiple times within the voxel, all the intersections are taken for rendering. However, if the roots are identical, we remove the redundant ones to prevent using the same intersection multiple times. 
% Besides, as the cubic polynomial might return multiple real but non-distinct roots, to prevent using the same intersection multiple times, we also check the roots and remove the redundant ones with differences less than $10^{-12}$. 
% \ww{Apart from cases where the roots are identical, we allow multiple intersections with multiple surfaces within each voxel.}
% \GR{Within a voxel you can have multiple intersections. Did I miss it, or how is this handled?}. \WW{Yes you can. It is not handled in any special way, just taking as samples and render through alpha compositing}

As both the formulation of cubic polynomials and Vieta's approach are fully differentiable, we hence directly have gradients defined on our surface representation $\hat{\delta}$ from the photometric loss. 

% \ww{The flow of gradients is: photometric loss $\rightarrow$ opacity $\alpha$ and SH coefficients $\rightarrow$ intersection location $\rightarrow$ surface $\hat{\delta}$.} \GR{If you really want to show the gradient flow, I would just write down the differential using the chain rule.} \WW{I guess let's just don't write about it, a bit redundant anyway}

% \subsection{Surface Finite Difference}



\subsection{Hyperparameters}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.45\textwidth]{figures/truncate_func.png}
\end{center}
    \caption{\textbf{The truncated alpha compositing reweight function $\gamma(i-1)$.} The x-axis is the index of the intersection starting from 1 (excluding intersections on back-facing surfaces). By reducing $a$, we effectively slide the curve to the left.}
    \label{fig:truncate_func}
\end{figure}


\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/abla.pdf}
\end{center}
   \caption{\textbf{Ablation Study.} We show the qualitative results of different ablations. Our full approach achieves the best quality overall.}
\label{fig:abla}
\end{figure*}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.45\textwidth]{figures/abla_ek.png}
\end{center}
   \caption{\textbf{Comparison with the Eikonal constraint regularization.} We visualize the out view (first column) and the inside view (second column) by cropping the surfaces along the y-axis. It can be clearly seen that the Eikonal constraint does not regularize the surface to be clean and smooth, but rather creates additional noises in optimization.}
\label{fig:abla_ek}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/abla_crop.pdf}
    \caption{Reconstruction without regularization tends to give an enormous amount of redundant inner surfaces. Encouraging consistent normals in the local neighborhood via $\mathcal{L}_{\mathbf{n}_1}, \mathcal{L}_{\mathbf{n}_2}$ enforces smoother surfaces, but redundant surfaces still remain. With both normal regularization and TV loss applied on the surface scalar field, we can obtain clean and smooth reconstruction.}
    \label{fig:abla_crop}
\vspace{-15pt}
\end{figure}


Our code is based on Plenoxels~\cite{plenoxels}. 
We similarly use a sparse voxel grid of size $512^3$ where each vertex stores the surface scalar $\delta$, raw opacity $\sigma_\alpha$ and 9 SH coefficients for each color channel. We directly initialize all the grid values from Plenoxels pre-trained with original hyperparameters and prune voxels with densities $\sigma$ lower than $5$.
We use $s_\sigma=0.05$ to downscale the density values during initialization.
We train for $50k$ iterations with a batch size of $5k$ rays, which takes around 17 minutes for synthetic scenes and 22 minutes for real-world scenes on an NVIDIA A100-SXM-80GB GPU (excluding Plenoxels training). We used grid search to determine the optimal hyperparameters. We use the same delayed exponential learning rate schedule, where the learning rate is delayed with a scale of $0.01$ during first $25k$ iterations. 
As previously mentioned, the interval of level values is selected by first determining a valid range of Plenoxels density field. We then select a suitable number of level values, i.e., the carnality $n$ of our multi level sets, by trying $1, 3, 5, 10$ evenly-spaced level values on the ``ship" scene from NeRF Synthetic dataset. We found $n=5$ to give the best performance.
At the end of the training, we also remove invisible surfaces with opacity $\alpha$ less than $0.1$.

\paragraph{Synthetic}
For experiments on synthetic datasets, we initialize $5$ level sets at $\bm{\tau}_\sigma = \{10, 30, 50, 70, 90\}$, and linearly decay the truncated alpha compositing parameter $a$ from $5$ to $2$ in first $10k$ iterations. 
For surface scalars $\hat{\delta}$, we use $10^{-5}$ as both starting and end learning rate. For raw opacity values $\sigma_\alpha$, we start with $10^{-2}$ and end with $10^{-3}$. For SH we keep the learning rate at $10^{-3}$ without exponential decay or initial delay. We use the RMSProp~\cite{rmsprop} optimizer for training. 
For the regularization weights, we set $\lambda_c = 10^{-6}$ for the first $10k$ iterations and $0$ for the rest of training. \ww{We use 
$\lambda_\delta=10^{-3}$, 
$\lambda_\mathcal{H} = 10^{-4}$, 
$\lambda_\alpha = 10^{-9}$,
$\lambda_{\mathbf{n}_1} = 10^{-6}$ and $\lambda_{\mathbf{n}_2} = 0$ for the Thin dataset. $\lambda_{\mathbf{n}_2}$ was disabled as it tends to destroy the thin structures with rapid normal variations. For the Translucent dataset, we use $\lambda_\delta=10^{-5}$, $\lambda_\alpha = 10^{-11}$,  
$\lambda_\mathcal{H} = 10^{-4}$,
$\lambda_{\mathbf{n}_2} = 10^{-4}$, and linearly decay $\lambda_{\mathbf{n}_1}$ from $10^{-2}$ to $10^{-4}$.}
% \GR{How did you come up with those values?} \WW{Added arguments regarding grid search.}

\paragraph{Real-World}
% \WW{update this}
\ww{For experiments on real-world scenes, we initialize with less level sets $\bm{\tau}_\sigma = \{10, 30, 50\}$, as we found level values above 50 give almost empty surfaces due to higher density regularization in Plenoxels initialization. 
We use the hyperparameters used by the original authors to run LLFF experiments to train Plenoxels. For our method, we use level sets $\tau_\sigma = \{10, 30, 50\}$ as level value above $50$ gives almost empty space. We change the surface scalar learning rate to start and end both at $10^{-4}$ with a delay ratio of $10^{-2}$ and delay steps of $25k$. The learning rates of opacity and SH are the same as in the synthetic experiments.
For regularizations, we use the same $\lambda_c$ and $\lambda_\mathcal{H}$ as synthetic experiments, and set 
$\lambda_\delta=5 \times 10^{-3}$, $\lambda_\alpha = 10^{-9}$,  
$\lambda_{\mathbf{n}_2} = 10^{-3}$ and linearly decay $\lambda_{\mathbf{n}_1}$ from $10^{-2}$ to $10^{-3}$.
%
% For the scene ``yellow cup", as the strong regularization tends to break some of the delicate translucent surfaces, we lower the normal regularization to $\lambda_{\mathbf{n}_2} = 10^{-4}$ and linearly decay $\lambda_{\mathbf{n}_1}$ from $10^{-2}$ to $10^{-5}$ instead.
}

% \ww{For real-world scene ``mug", we used the hyperparameters used by the original authors to run LLFF experiments to train Plenoxels. For our method, we use level sets $\tau_\sigma = \{10, 30, 50\}$ as level value above $50$ gives almost empty space. We change the surface scalar learning rate to start and end both at $10^{-3}$ with a delay ratio of $10^{-4}$ and delay steps of $45k$. The learning rates of opacity and SH are the same as in the synthetic experiments.
% For regularizations, we use the same $\lambda_c$ and $\lambda_\mathcal{H}$ as synthetic experiments, and set 
% $\lambda_\delta=10^{-3}$, $\lambda_\alpha = 10^{-9}$,  
% $\lambda_{\mathbf{n}_2} = 10^{-4}$ and linearly decay $\lambda_{\mathbf{n}_1}$ from $10^{-2}$ to $10^{-4}$.
% %
% For scenes ``dark cup" and ``yellow cup", as a significant amount of far background is included in the capture, we incorporated the background model implemented by Plenoxels in both Plenoxels pre-training and our training. The background model is a set of additional grids wrapped into spherical coordinates to surround the foreground voxels. We used a background model with 32 layers and 512 resolutions. For those two scenes, We change the surface learning rate to start and end both at $10^{-4}$ with a delay ratio of $10^{-2}$ and delay steps of $25k$.
% For regularizations, we changed to use
% $\lambda_\delta=5 \times 10^{- 3}$, $\lambda_\alpha = 0$,  
% and linearly decay $\lambda_{\mathbf{n}_1}$ from $10^{-2}$ to $10^{-5}$. The other the hyperparameters are kept the same.
% }

 For the implementation of surface TV loss $\mathcal{L}_\delta$, we calculate the gradient via forward finite difference in the same way as Plenoxels~\cite{plenoxels}:

\begin{align}
\nabla_x \hat{\delta}(i,j,k) = \frac{| \hat{\delta}(i+1,j,k) - \hat{\delta}(i,j,k)|D_x}{256}
\end{align}

\noindent 
where $i,j,k$ are the vertex coordinate, $D_x$ is the grid resolution in $x$ dimension and is $512$ for all experiments in our case.  $\nabla_y \hat{\delta}(i,j,k)$ and $\nabla_z \hat{\delta}(i,j,k)$ are calculated accordingly. We simply ignore the edge vertices when computing the surface TV loss by using the Neumann boundary conditions.
% \GR{Neumann boundary conditions}.

The truncated alpha compositing reweight function can be seen as a truncated Hann window \cite{nerfies}, as shown in Figure~\ref{fig:truncate_func}. By reducing $a$ during the training, we slide the curve to the left and hence gradually anneal the influence of later intersections.





% \begin{figure*}[t]
% \begin{center}
%   \includegraphics[width=0.95\textwidth]{figures/llff_supp.png}
% \end{center}
%     \caption{\textbf{Additional results on real-world scenes.} Note that the geometry renderings do not exactly match the RGB due to the use of normalized device coordinates.}
%     \label{fig:llff_sup}
% \end{figure*}

\section{Additional Experiments}

\subsection{Synthetic Dataset}

\paragraph{Experiment Details}

For quantitative evaluation, we adapt the Python version of DTU~\cite{dtu} evaluation script~\cite{python_dtu}, where we extracted dense point clouds from all level surfaces and downsampled both predicted and ground truth points with 0.001 density before computing the Chamfer distance.
For evaluation of NeuS~\cite{neus} and HFS~\cite{HFS}, we first extracted the mesh using marching cubes with resolution $512^3$, then used the script to sample points on the mesh to compute the Chamfer distance. For evaluation of Plenoxels~\cite{plenoxels}, MipNeRF360~\cite{mipnerf360} and our method, we directly sample points on the implicit surfaces by sending dense virtual rays within each grid of a $512^3$ voxel grid through our closed-form intersection finding. This makes the computation of sample opacity and trimming of the surface easier. 

For training of NeuS \cite{neus}, HFS \cite{HFS} and MipNeRF360 \cite{mipnerf360}, we used the provided hyperparameters. We used the hyperparameters for real-world thin structure reconstruction experiments for NeuS, as we found it gives better performance on the NeRF Synthetic dataset. For training on the Translucent Blender dataset, we set the background to white for all methods as the semi-transparent objects are rendered with a white background in Blender.

\ww{To select a level set value on the density field of Pleboxels~\cite{plenoxels} and MipNeRF 360~\cite{mipnerf360} for surface extraction, we use the same methods as in~\cite{neus, UNISURF}, where we extracted and evaluated surfaces on levels $\tau_\sigma = \{10, 30, 50, 70, 90, 100\}$, which fully covers the surfaces we used to initialize from Plenoxels. We computed the average norm on Synthetic, Thin, and Translucent datasets and selected the level set value with the best Chamfer distance on each of the datasets. For Plenoxels, the level sets are $90, 30$ and for MipNeRF 360, the level sets are $50, 50$ for the two datasets respectively. We report the quantitative results for each level set in Tab~\ref{tab:nerf_lvs} and show a few qualitative examples in Figure~\ref{fig:nerf_lvs}.}

\begin{table}[]
\small
\centering
\singlespacing
\tabcolsep=0.06cm
\begin{tabular}{lccc}
\toprule
{} &   Thin & Translucent &  average \\
\midrule
Plen ($\sigma=10$)    &  0.759 &       0.813 &    0.786 \\
Plen ($\sigma=30$)    &  0.886 &       \first{0.761} &    0.824 \\
Plen ($\sigma=50$)    &  0.687 &       0.812 &    \first{0.750} \\
Plen ($\sigma=70$)    &  0.563 &       1.062 &    0.812 \\
Plen ($\sigma=90$)    &  \first{0.526} &       1.597 &    1.062 \\
Plen ($\sigma=100$)   &  0.541 &       1.832 &    1.186 \\
Mip360 ($\sigma=10$)  &  1.882 &        3.76 &    2.821 \\
Mip360 ($\sigma=30$)  &  1.468 &       3.081 &    2.274 \\
Mip360 ($\sigma=50$)  &  \first{1.445} &       \first{3.063} &    \first{2.254} \\
Mip360 ($\sigma=70$)  &  1.526 &        3.07 &    2.298 \\
Mip360 ($\sigma=90$)  &  1.635 &       3.116 &    2.376 \\
Mip360 ($\sigma=100$) &  1.693 &       3.203 &    2.448 \\
\bottomrule
\end{tabular}
    \caption{\textbf{Chamfer distance $\downarrow \times 10^{-2}$ on synthetic datasets.}
    We color the \colorbox{first}{best} level sets for Plenoxels~\cite{plenoxels} (Plen in table) and MipNeRF360~\cite{mipnerf360} (Mip360 in table) respectively.
    }
    \label{tab:nerf_lvs}
\end{table}

\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/plen_lvs.pdf}
  \includegraphics[width=0.95\textwidth]{figures/mip_lvs.pdf}
\end{center}
    \caption{\textbf{Surfaces extracted using different level sets from Plenoxels~\cite{plenoxels} and MipNeRF360~\cite{mipnerf360}.} We remove part of the exterior surface in each scene to visualize the interior reconstructions. Due to the ambiguity of density representation, a low density level set gives more complete surfaces but could contain a significant amount of noise, whereas a high density level set can miss a lot of surfaces.}
    \label{fig:nerf_lvs}
\end{figure*}



% For depth-extracted surfaces from Plenoxels~\cite{plenoxels} and MipNeRF 360~\cite{mipnerf360}, to ensure the evaluation is done on proper surfaces and deal with the potential imbalance in the distribution of depth-extracted points, we first downsampled the points with density $0.001$, then ran alpha shapes~\cite{alpha_shape} with $\alpha=0.003$ to reconstruct meshes for Chamfer distance evaluation. We did not use TSDF~\cite{tsdf} or Poisson surface reconstruction~\cite{poisson_recon}, as the former tends to remove delicate structures and the latter can produce many redundant surfaces and requires careful pruning. Overall, we found out that alpha shape gives the best result when reconstructing surfaces from very densely sampled points.

\paragraph{Additional Results}

We show all the qualitative results in 
% Figure~\ref{fig:synthetic_all}, 
\ref{fig:delicate_all}, \ref{fig:transparent_all}, as well as the individual Chamfer distance for each scene in 
% Table~\ref{tab:nerf_syn}, 
\ref{tab:delicate} and \ref{tab:semi_t}.
The qualitative comparisons shown in both main paper and the supplementary are done by first evaluating the L1 error on each sampled point, then rendering the point cloud with Eye-Dome Lighting (EDL) using PyVista~\cite{pyvista}.
We also show additional novel view RGB renderings of our method in Figure~\ref{fig:rgb_renderings}. But please note that we do not claim state-of-the-art performance in novel view synthesis.

% \ww{We include an additional setting of our method as ''Ours (Conv Lv)", where we enforce the multiple surface level values to converge during the training and keep only a single level value in the end. We simply re-scale the level values as:

% \begin{equation}
% \bm{\tau}' = \left\{  (\tau - \tau_0) \times s_\tau + \tau_0, \tau \in \bm{\tau} \right\}
% \end{equation}

% \noindent
% where $\tau_0 = min(\bm{\tau})$ is the lowest level value. We linearly decay $s_\tau$ from $1$ to $0$ during first $45k$ iterations, and leave only one level set $\tau_0$ afterward. This setting guarantees no redundant ``layered" surfaces would appear in the reconstruction and might be more desirable in certain applications. However, due to its hard constraints and its use of only a single level set in the end, it is less robust to the outliers and the reconstructions are slightly worse compared to our main setting. Note that the two settings look similar qualitatively but different in the metric, mainly because the reconstructions with converging level sets tend to give a small overall bias that is not obvious in the visualization. Regardless, both settings achieve better results than }

% We note that in some scenes, although reconstructions from NeuS and HFS missed a significant part of the thin structures, they turned out to give comparable or lower Chamfer distances than our reconstruction. This is particularly obvious in scenes such as ``slide" (bars on the ladder). The reason behind this is that our method might have learned slightly incorrect locations for some smooth surfaces in the scene, but as the thin structures occupy a relatively small portion of the whole geometry, the error caused by missing them is significantly down-weighted. 




\subsection{Real-World Dataset}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.45\textwidth]{figures/llff_neuralangelo.pdf}
\end{center}
    \caption{\textbf{Additional real-world comparisons with neuralangelo~\cite{neuralangelo}} Note that neuralangelo uses a different coordinate system and camera processing pipeline for COLMAP scenes, therefore the reconstructions are not perfectly aligned, but it can still be clearly seen that our method achieves better reconstruction quality on thin structures and translucent surfaces.}
    \label{fig:llff_neuralangelo}
\end{figure}


\ww{We show additional comparisons with neuralangelo~\cite{neuralangelo} in Fig~\ref{fig:llff_neuralangelo}. Note that as neuralangelo uses a different camera normalization for COLMAP scenes instead of Normalized Device Coordinate (NDC), which we use for our method and all other baselines, the reconstruction of neuralangelo is therefore not exactly aligned. We use an interactive viewer with Eye Dome Lighting~\cite{CloudCompare} and manually selected camera positions with close views for comparison. Regardless, it can be clearly seen that although neuralangelo excels at reconstructing smooth surfaces, it fails to faithfully reconstruct thin or translucent surfaces. Our method achieves a significant improvement over it in terms of thin and translucent surface reconstruction.}

% \paragraph{Experiment Details} For experiments on forward-facing real-world scenes from LLFF \cite{llff} or captured by our own, we used the provided LLFF hyperparameters to train Plenoxels, whereas for the training of our method, we used a single-level set initialization $\bm{\tau}_\sigma = \{10\}$. We found that this setting can produce cleaner and more smooth surfaces, as the training of Plenoxels was applied with a very strong TV regularization on the density field $\sigma$. As a result, the initialized surface field is much smoother and spread out. This increases the difficulty in converging the surfaces initialized with multiple level sets. For the additional experiments on semi-transparent objects (``mug" and ``rough"), as surfaces initialized from Plenoxels are very incomplete, we increased the surface learning rate to $10^{-3}$ with the same delay schedule applied.

% % \WW{More experiments are still running, might need to change this}
% % For these experiments, we focus on the reconstruction of foreground objects and hence se

% \paragraph{Additional Results} We show qualitative results on additional scenes in Figure~\ref{fig:llff_sup}. It can be seen that both ours and Plenoxels can recover thin structures accurately, whereas NeuS reconstructed over smooth surfaces. Besides, Plenoxels and NeuS missed a more significant chunk of semi-transparent surfaces. In comparison, our method managed to recover more complete geometry. However, a part of the semi-transparent surfaces is still missing due to the strong ambiguity in the scene. More sophisticated regularization and additional priors might be required to fully recover those surfaces with minimal artifacts.



\subsection{Ablation}



We should additional qualitative ablation of our method in Figure~\ref{fig:abla}. 
% \ww{We include the qualitative results for an additional setting ``no reg", whereas no regularization and truncated alpha composition is applied. The quantitative results of this setting is $1.578 \times 10^{-2}$, $1.000 \times 10^{-2}$ and $0.412 \times 10^{-2}$ for each scene respectively, much worse than our full setting.} 
In addition, we show a comparison between the results after applying our TV surface regularization $\mathcal{L}_\delta$ and after applying the Eikonal constraint regularization used in most SDF optimization methods in Figure~\ref{fig:abla_ek}. Namely, in replace of TV surface regularization, we encourage the norm of the gradient of the surface field at every vertex to get close to $1$ via mean squared error:
% \GR{This is not an L2 penalitye it is a squared loss, MSE}\WW{yeah you are right, my mistake. Thx!}


\begin{align}
    \mathcal{L}_{ek} = \frac{1}{|\mathcal{V}|}  \sum_{\mathbf{x} \in \mathcal{V}} (||\nabla \hat{\delta}(\mathbf{x})||_2 - 1)^2 \,.
\end{align}


From Figure~\ref{fig:abla_ek}, it can be clearly seen that the Eikonal constraint is not sufficient to regularize and remove the noisy inner surfaces inherited from initialization. Moreover, it turns out to even harm the optimization by introducing additional surface floaters while trying to constrain the surface field into an SDF. This also shows that converting the surfaces extracted from a density field into proper SDF is a non-trivial task. 

In Figure~\ref{fig:abla_crop}, we show that normal regularization $\mathcal{L}_{\mathbf{n}_1}, \mathcal{L}_{\mathbf{n}_2}$ are insufficient for removing heavily biased surfaces initialized from Plenoxels, whereas $\mathcal{L}_\delta$ is more effective in this case.





\subsection{DTU Dataset}

\begin{table}
    \centering
\begin{tabular}{lrrrrr}
\toprule
                    {} &  37 &  40 &  63 &  69 &  110 \\
\midrule
Plen ($\sigma=10$)  &        1.90 &        1.86 &        1.86 &        2.04 &         1.96 \\
Plen ($\sigma=50$)  &        1.46 &        1.43 &        1.66 &        \second{}1.60 &         1.75 \\
Plen ($\sigma=100$) &        \second{}1.34 &        1.57 &        2.99 &        2.22 &         2.43 \\
NeuS                &        \first{}0.98 &        \first{}0.56 &        \second{}1.13 &        \first{}1.45 &         \second{}1.43 \\
Ours                &        \second{}1.34 &        \second{}1.36 &        \first{}0.99 &        1.91 &         \first{}1.37 \\
\bottomrule
\end{tabular}
\caption{\textbf{Chamfer distance $\downarrow$ on DTU scenes.} We color the \colorbox{first}{best} and \colorbox{second}{second best} surfaces.}
    \label{tab:dtu}
\end{table}


\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.45\textwidth]{figures/dtu_crop.png}
\end{center}
   \caption{\textbf{Inside views of reconstructions on DTU dataset.} 
   % \GR{How does it look for NeuS?}
   % \WW{make new figure}.
   Red color indicates the L1 error in reconstruction, and blue indicates the reconstruction masked out by the DTU official masks. Surfaces extracted from Plenoxels contain many noisy inner surfaces that had to be masked out during evaluation to achieve low Chamfer distance.
   }
\label{fig:dtu_crop}
\end{figure}

We additionally show reconstruction results on some DTU \cite{dtu} scenes in Figure~\ref{fig:dtu} and Table~\ref{tab:dtu}. We note that as DTU does not contain many thin structures or semi-transparent materials, but mostly smooth surfaces only, our method is therefore not expected to achieve state-of-the-art performance in this scenario. In fact, our method reconstructs reasonable surfaces, but performs worse than NeuS overall. This is mainly due to a lack of natural spatial smoothness constraint present in the MLP architecture of NeuS, which allows it to perform well on datasets like DTU that contain many smooth surfaces, but worse on our synthetic dataset with a focus on thin structures. 
% \GR{Maybe stupid and too late, but could we initialize from NeuS + Plenoxels?} \WW{It is certainly possible, we just need to take SDF values at vertices and initialize surface from 0 level set. However, I don't think it's very meaningful as our method would not be able to improve anything (there isn't really anything to improve anyway)}
% Deforming from a sphere shape also gives a strong inductive bias for NeuS to reconstruct smooth surfaces, but causes it to fail to capture the complicated thin structures, as shown in our main experiments.

We also note that although the qualitative comparison in Figure~\ref{fig:dtu} shows that our method can refine the level set surfaces extracted from Plenoxels by correcting the out-growing surfaces while preventing holes, the Chamfer distance does not always show an improvement. This is because the official DTU evaluation provides carefully created masks to remove reconstruction on parts that do not have proper reference geometry scanned by the depth scanner. This also excludes the majority of the inner surfaces from level set surfaces of Plenoxels, making their Chamfer distances much better; see Figure~\ref{fig:dtu_crop}.

\paragraph{Experiment Details} We compared with level set surfaces from Plenoxels \cite{plenoxels} and NeuS \cite{neus} trained with masks. We used the image masks provided by IDR \cite{idr} to set the background to white before training both Plenoxels and ours. For Plenoxels, we used the same hyperparameters for training on NeRF Synthetic dataset. We used slightly different hyperparameters from the ones we used for training NeRF Synthetic and Thin datasets. Namely, we modified the surface scalar learning rate to start with $10 ^ {-4}$ and end with $10^{-6}$. We increased $\lambda_\delta$ to $0.05$, $\lambda_\mathcal{H}$ to $10^{-3}$ and $\lambda_\alpha$ to $10^{-8}$. We also kept the truncated alpha compositing parameter $a$ at $5$ throughout training.










\include{Sections/sup_syn_figs_tabs}

\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/dtu.png}
\end{center}
   \caption{\textbf{Qualitative results on DTU \cite{dtu} dataset.} As the DTU scenes mainly contain smooth surfaces without any semi-transparent materials, our method does achieve state-of-the-art performance on this dataset. However, note that our method can still accurately capture the thin structure that is missed by NeuS in Scan 63. Moreover, our method can effectively correct the out-growing surface artifacts in Plenoxels. Red color indicates the L1 error in reconstruction, blue indicates the reconstruction masked out by the DTU official masks, and green indicates reconstructions that are too far away from reference and hence clipped during evaluation.
   % \GR{Can we also show the inside as in the teaser?}\WW{Added!}
   }
\label{fig:dtu}
\end{figure*}






