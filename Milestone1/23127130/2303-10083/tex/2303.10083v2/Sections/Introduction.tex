


\section{Introduction}



Recovering object geometry as surfaces from RGB images is a long-standing problem in computer vision, with numerous practical applications such as photogrammetry, 3D asset creation, and custom 3D fabrication.
Traditional approaches rely on Structure-from-Motion~\cite{schoenberger2016sfm} and Multi-View Stereo~\cite{schoenberger2016mvs} pipelines to first reconstruct a 3D point set of the scene to which surfaces can be fitted~\cite{poisson_recon, delaunay}.
% \GR{cite poisson, delaunay} \WW{Added citation for delaunay, but can you check if this is the correct one?}.
% These techniques work by minimizing the reprojection error of detected key points across images during the reconstruction process\GR{that is only true for SfM (first step), but not for MVS (seconds step) and meshing}.\WW{@fangcheng can you help with it?}
% A crucial step in these pipelines is to minimize the reprojection error of geometrical entities such as feature points.
Differentiable rendering techniques have emerged as a more versatile reconstruction procedure. 
With properly derived gradients, these techniques can simultaneously learn both geometry and appearance by minimizing the error of RGB renderings.
% significantly loosening the restrictions on the choice of geometric representations to be learned.
This significantly loosens the restrictions on the choice of geometric representations to be learned.
In particular, implicit surface representations~\cite{sdfdiff, diffsdf, idr, iron}, 
% \WW{I'd remove NeuS and VolSDF from this as they rely on volume rendering}
\ie level sets of a scalar field such as a signed distance function (SDF), show promising results in surface reconstruction due to their robustness to complex geometry and topology.



% \FZ{TODO: more citations need to be properly inserted for this new intro}

% One open challenge in this domain that previous research has not adequately addressed is reconstructing surfaces that exhibit semi-transparency effects. 
One open challenge that has not been adequately addressed in this domain is reconstructing implicit surfaces that exhibit translucent effects. 
% \ww{This includes both surfaces made of semi-transparent materials and thin structures, which appear to be semi-transparent due to the blending effect; see Figure~\ref{fig:blending}.} 
The prevailing assumption in earlier works is that the surface is opaque throughout, and differentiable rendering techniques for implicit surfaces only examine the intersection of rays and the nearest surface \cite{Niemeyer2020CVPR, sdfdiff, liu2020dist}. As a result, the forward rendering process in those studies cannot simulate scenes with non-opaque effects, leading to an inability to reconstruct their surfaces.
% ; \ww{see the third row of Figure~\ref{fig:teaser}}. 
% \WW{I removed reference to Figure~\ref{fig:teaser}, because technically speaking NeuS isn't one of the methods we discuss here. Those are even older methods like IDR which doesn't use volume rendering at all.}

Modeling opaqueness is not solely crucial for reconstructing translucent surfaces, but is also necessary for the recovery of extremely thin surfaces with sub-pixel silhouette. As illustrated in Figure~\ref{fig:blending}, when rendering a thin structure that only partially occupies a pixel with an insufficient number of sampled rays, opaque foreground objects must be treated as semi-transparent in order to achieve an accurate pixel color blending the foreground and background. 
% \ww{Therefore, incorporating semi-transparency is also essential for accurately reconstructing thin surfaces.} \WW{Remove this sentence?}
Methods that neglect this feature may fail to capture the correct reconstruction of thin structures; see Figure~\ref{fig:teaser}.

\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/teaser.png}
\end{center}
    \vspace{-0.7em}
   \caption{
   \textbf{Illustration.}
   We illustrate the representations of NeuS, NeRF, and our method, as well as reconstructed surfaces. 
   NeuS~\cite{neus} uses SDF to optimize for opaque surfaces and hence misses translucent or thin surfaces with blending effects in the reconstruction.
   NeRF methods such as Plenoxels~\cite{plenoxels} can represent semi-transparency with density field, but as density couples both occupancy and opacity, surfaces extracted from it would contain holes or redundant surface floater.
   In contrast, our approach models decoupled surface and opacity fields. We use a surface field without Eikonal constraint and multiple level sets $\tau_0,\tau_1,...$ to model geometry with different levels of confidence and opacity, and utilize a closed-form intersection formula to enable differentiable rendering, and hence can accurately reconstruct surfaces exhibiting semi-transparency. 
   }
\label{fig:teaser}
\vspace{-10pt}
\end{figure*}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.4\textwidth]{figures/blending.png}
\end{center}
   \caption{\textbf{Blending effect.} a) Real-world capture takes incoming light from multiple rays per pixel. Pixels that are partially occupied by an opaque object are therefore rendered as a mixture of the object and background color. b) With one-sample rendering in reconstruction, it becomes necessary for extremely tiny objects to be modeled as semi-transparent for the pixel color to match the ground truth. c), 
   % \GR{desc of c missing} \WW{oh actually desc is for both c) and d)}
   d) Our representation is fully capable of representing this phenomenon, leading to the accurate surface reconstruction of thin structures.
   %\GR{put a and b into one row, currently too big, too much white space}\WW{Done, much better now}
   }
\label{fig:blending}
\vspace{-15pt}
\end{figure}






Differentiable volume rendering techniques ~\cite{nerf} have demonstrated considerable success in rendering translucent and thin objects by integrating radiance along a ray through an entire scene.
Their geometric representation is a density field where a surface can be implicitly defined by applying a density threshold.
However, density represents both occupancy (geometry) and transparency (material) in a tightly coupled manner, posing a non-trivial challenge in choosing the threshold for surface extraction. 
% using density thresholding.
% This coupling of geometry and materials within the density field presents a non-trivial challenge for extracting surfaces using straightforward density thresholding
A high threshold may exclude translucent surfaces from the reconstruction, whereas a low threshold results in the erroneous reconstruction of density floaters and redundant surfaces that are not part of the desired reconstruction.
% ; see Figure~\ref{fig:teaser}. 
% \FZ{TODO: one or two sentences about NeuS here.}\WW{On it!} 
NeuS~\cite{neus} attempts to alleviate this issue by combining SDF with volume rendering, but it still assumes an opaque surface at the end of optimization and hence fails to reconstruct surfaces with semi-transparency.
% ; see Figure \ref{fig:teaser}.
Figure~\ref{fig:teaser} illustrates failure cases of both types of methods on translucent or thin surfaces. 
\ww{Recent Gaussian Splatting methods~\cite{gaussian_splatting, sugar} uses anisotropic 3D Gaussians to achieve efficient rendering, but they similarly ignore the occupancy-transparency ambiguity and only retain opaque Gaussians for surface extraction, and hence cannot reconstruct thin or translucent surfaces.}
% More recently, NeuS~\cite{neus} tries to alleviate this issue by learning an SDF as the backbone and mapping the signed distance values to unbiased weights\FZ{what are the weights for? p.s. I removed the marching cube part as it is only needed for meshing while our interest is to just get implicit surface}\WW{Weights are the rendering weights in volume rendering}\GR{Talking about ... by maximizing the volume rendering weights ... might be too specific in the introduction. Not really understandable without more context.}\WW{I guess we have to state the opaque assumption of NeuS somehow, hopefully this isn't too confusing} in volume rendering.
% Nevertheless, the optimization of NeuS still assumes opaque surfaces and encourages surfaces to fully occlude the ray by maximizing the volume rendering weights. 
% Hence, it fails to accurately reconstruct semi-transparent objects; see Figure \ref{fig:neus_quick_view}. 
% Furthermore, NeuS starts with a centered sphere and deforms it to match the target shape. This process tends to completely miss any thin structures such as branches and strings during reconstruction; see Figure \ref{fig:nerf_crop}. Yet, NeuS cannot easily utilize per-scene shape prior \GR{what about MVS depth that can be used during optimization; needs to more specific what priors}\WW{I changed "and" to "by" in the later sentence to make the things more clear. Is it better now?} \ww{by} initializing with different shapes due to its use of neural representation\GR{Is it actually true? Can a pre-trained nerf not be reparametrized (density) to be roughly a SDF init?} \WW{Would be less convenient as you will need to fit your MLP. One possible trick is to take the original density MLP with modified last layer activation, but that fixes the network architecture}. 






Two key requirements must be fulfilled for faithfully reconstructing surfaces of translucent and thin objects: 1) a representation that explicitly decouples geometry and materials; and 
% 2) a differentiable rendering process that considers more than just the nearest intersection between the ray and the scene.
2) a differentiable rendering pipeline that considers more than the nearest ray-surface intersection, while also enabling gradient flows to both surface and opacity.
% \WW{I modified this due to our use of truncated alpha compositing, which encourages us to not consider all intersections.}\FZ{looks good}
% 2) a differentiable rendering process that considers all the intersections between a ray and the scene, instead of only the nearest one.
To this end, we introduce \name{}, a novel surface representation based on a grid structure without neural networks. 
% The grid values provide complete separation among geometry, opacity, and appearance. 
We use separate values on the grid to represent geometry, opacity, and appearance.
We define the surface as multiple level sets of a continuous scalar field, where the field itself is given by a trilinear interpolation of the grid values. Unlike previous methods, our representation does not require the scalar field to be an SDF subject to the Eikonal constraint.
An important property 
% \hx{change advantage to 'property'?} \WW{sounds good} 
of our approach is that the \emph{exact} intersection points between a ray and all the surfaces, regardless of whether they are opaque or transparent, can be determined by analytically solving a cubic polynomial. The \emph{closed-form} solution allows for full \emph{differentiability} to both geometry and material in our forward rendering process, which simulates the semi-transparency effects via alpha compositing of intersection points. 
% \hx{do you need to mention differentiable wrt. both geometry and material? e.g. These intersection points participate in our forward rendering process, which simulates the semi-transparent effects via alpha compositing of these points. In this way, The \emph{closed-form} solution allows for full \emph{differentiability} of photometric loss with respect to both material and geometry}. \WW{Thx, I think that's a good idea. I added it in a more concise style}

% \WW{combine this paragraph?}
We further propose a series of initialization and optimization strategies that are designed to facilitate efficient and accurate reconstruction. 
% \FZ{Perhaps report the average training time and time needed for pre-training (initialization) here?}
The total training time of our method is around 30 minutes.
% , including a short pre-training of Plenoxels~\cite{plenoxels} \GR{do you need to highlight the pre-training via Pleoxels here?}. \WW{You are right, removed}
% time spent to pre-train Plenoxels~\cite{plenoxels} as the initialization.
% \GR{I think there is no need to highlight/mention the 10 min pre-training of Plenoxels in the intro.}\WW{I agree, just mentioning 30 minutes should be fine?}\FZ{I also removed Plenoxels here. Readers will find out how init is exactly done in method section.}\WW{I think we should put at least NeRF or plenoxels, reader would be confused what pre-training means.}
% including pre-training and initializing from plenoxels~\cite{plenoxels} which takes around 10 minutes.
We evaluate our method on an extended version of the NeRF synthetic dataset~\cite{nerf}, which contains 8 original scenes and 16 additional objects with challenging thin structures or translucent materials. We show that our method is capable of reconstructing surfaces with better quality than the existing SDF and NeRF based methods.
% \hx{if provide some surprising improvement number will be better.} \WW{I'm actually not sure. If we were to write this, it would be like our method is 78\% better than NeuS. It sounds fancy but comparison is actually not that meaningful once you realized that NeuS simply doesn't work on our dataset}
% \ww{We also show that our method achieves better qualitative results on real-world captures featuring translucent materials.}
% \hx{How about the performance? should emphasis our advantage}

In summary, our contributions are: 
1) A novel grid-based scene representation for implicit surface reconstruction, specialized for translucent and thin objects. It incorporates a closed-form and differentiable evaluation of all surface intersections along the ray, and properly decouples surface geometry and opacity.
% 2) A series of initialization and optimization strategies that have proven to be effective in achieving efficient training and accurate reconstruction.
% 2) A series \GR{better to be specific instead of a series} of initialization and optimization strategies for efficient training and accurate reconstruction.
2) Initialization scheme utilizing fast Plenoxels \cite{plenoxels} training and optimization with truncated volume rendering and surface smoothness constraint for efficient training and accurate reconstruction.
3) We show superior reconstruction quality compared to state-of-the-art methods on synthetic and real-world scenes featuring thin and translucent objects.

% 1) We propose a novel grid-based scene representation for implicit surface reconstruction, specifically tailored for semi-transparent and thin objects. It incorporates a closed-form and differentiable evaluation of all surface intersections along the ray, and properly decouples geometry and opacity.
% 2) We develop a series of initialization and optimization strategies that have proven to be effective in achieving efficient training and accurate reconstruction.
% 3) Our method demonstrates overall superior reconstruction quality compared to state-of-the-art methods on scenes featuring thin and semi-transparent objects.

% \begin{itemize}
%     \item We propose a novel grid-based scene representation for implicit surface reconstruction, specifically tailored for semi-transparent and thin objects, with two key features: 1) a closed-form (and thus fully differentiable) evaluation of exact intersection points between a ray and all the surfaces; and 2) proper decoupling of geometry and material properties.
%     \item We develop a series of initialization and optimization strategies that have proven to be effective in achieving efficient training and accurate reconstruction.
%     \item Our method demonstrates \ww{overall} superior reconstruction quality compared to state-of-the-art methods on scenes featuring thin and semi-transparent objects.
% \end{itemize}
% \GR{If space is needed (likely) the summary can be removed and the points hightlighted/emphasized in the text.}

% \WW{======= Old Version =======}

% % Sphere-tracing SDF
% % \FZ{Are we certain our closed-form ray-surface intersection is novel? If yes, it's best for us to evaluate the benefits of closed-form ray-surface intersection separately, as this is a contribution that is independent of decoupling occupancy and opacity.} \WW{I won't say it's super novel, as several past works have identified this approach, but didn't apply them}\FZ{Okay, in that case, I would introduce the issue with semi-transparency first and try motivating our closed-form ray-surface intersection as a way to find all the intersections (as opposed to the closest surface). I'll draft it.}\WW{Thx!}
% To render an implicit surface in a differentiable manner,
% early methods incorporate gradient tricks to render approximated \WW{are they all approximated?} ray-surface intersections with differentiable sphere-tracing~\cite{idr, sdfdiff, diffsdf}, 
% % Limitation of Sphere-tracing SDF
% but they only have gradients at first intersections and therefore perform unstably at sharp depth discontinuities.
% % NeuS
% NeuS~\cite{neus} alleviates this issue by incorporating differentiable volume rendering into SDF optimization. 
% % Limitation of NeuS
% Nevertheless, the optimization of NeuS still assumes opaque surfaces and hence fails to accurately reconstruct semi-transparent objects or thin structures with blending effects; see Figure \ref{fig:nerf_crop} and \ref{fig:neus_quick_view}. 
% Furthermore, NeuS starts with a centered sphere and deforms it to match the target shape. This process tends to completely miss any thin structures such as branches and strings during reconstruction; see Figure \ref{fig:nerf_crop}. Yet, NeuS cannot easily utilize per-scene shape prior \GR{what about MVS depth that can be used during optimization; needs to more specific what priors}\WW{I changed "and" to "by" in the later sentence to make the things more clear. Is it better now?} \ww{by} initializing with different shapes due to its use of neural representation\GR{Is it actually true? Can a pre-trained nerf not be reparametrized (density) to be roughly a SDF init?} \WW{Would be less convenient as you will need to fit your MLP. One possible trick is to take the original density MLP with modified last layer activation, but that fixes the network architecture}.  
% % \WW{A bit difficult, as thin structure also has semi-transparency. I won't worry too much about this}

% % NeRF 
% Meanwhile, recent works on NeRF~\cite{nerf} present impressive outcomes in synthesizing novel views of scenes with thin structures and semi-transparent materials due to their ability to model volumetric density.
% % Limitation of NeRF 
% However, such volumetric geometry is entangled with material opacity, as the density in NeRF is used to represent both geometry location and the amount of ray occluded. \GR{a bit more specific what is meant here}\WW{added one sentence, better now?}\GR{Not yet, what does amount of ray occlued mean}. This makes it difficult to be converted to surface representation and utilized in existing mesh-based pipelines without introducing artifacts such as noisy inner volume or density floaters.
% Although there have been many attempts to regularize NeRF to produce a more concentrated volumetric field~\cite{mipnerf360, infonerf, refnerf}, those methods focus on preventing visual artifacts in novel view synthesis and it is still unknown how proper surfaces can be recovered from such a volumetric representation. 
% Existing approaches to extract explicit geometry from NeRF include density thresholding and depth extraction combined with surface reconstruction methods~\cite{tsdf, poisson_recon, alpha_shape}, but choosing suitable parameters that prevent artifacts is almost impossible; see Figure \ref{fig:nerf_crop}, whereas depth-extracted geometry does not guarantee watertight surfaces and its quality can be dependent on the views chosen. 

% % \WW{TODO: highlight the benefits of not being SDF and states we are more generalized version of it. Concern -- do we need to abla with Eikonal constraint?}
% % Motivation and aim
% To address the shortcomings of SDF-based approaches when dealing with thin and semi-transparent surfaces as well as the geometry ambiguity in NeRF, we present a novel grid-based surface optimization approach with disentangled opacity and appearance.
% % Introduce our method
% % Voxel-based surface
% Our surface is defined as the level sets of a continuous scalar field, which is represented through trilinear interpolations of discretized scalars stored in a voxel grid. It can be seen as a more generalized version of SDF, where the implicit scalar field is not required to be the distance to the closest surface.
% % Closed-form intersection
% A key insight is that the exact surface-ray intersection on our representation can be found in \textit{closed-form} through the analytical solution of a cubic polynomial, and is therefore \textit{differentiable by construction}. 
% % NeRF initialization
% Moreover, our explicit grid-based representation utilizes the efficient and robust convergence of volumetric training by coarsely initializing from a pre-trained NeRF. 
% The decoupled surface, opacity, and appearance can then be further optimized through alpha compositing to refine the inherited artifacts and reconstruct clean surfaces for semi-transparent or thin structures. 

% % \WW{======= Below is Common Part =======}

% We evaluate our method on an extended version of the NeRF synthetic dataset~\cite{nerf}, which contains 16 additional objects with challenging thin structures and semi-transparent materials. By initializing from trained Plenoxels~\cite{plenoxels}, a grid-accelerated NeRF, and further optimizing for 17 minutes, we show that our method is capable of reconstructing surfaces with better quality than the existing SDF and NeRF methods. We also qualitatively demonstrate our method on a few real-world scenes from the LLFF dataset~\cite{llff} and a scene featuring semi-transparent materials captured by us. \WW{Double check the claim} Our contributions include:

% \begin{itemize}
%     \item We introduce a novel grid-based surface representation that employs closed-form ray-surface intersections and differentiable alpha compositing for surface optimization.
%     \item  Our surface representation is disentangled from opacity and can therefore accurately model semi-transparent surfaces and thin structures with strong blending effects.
%     \item Our grid-based representation efficiently initializes from pre-trained NeRFs (Plenoxels), and is further optimized to refine the surfaces and achieve better overall reconstruction quality than the state-of-the-art methods on scenes with thin and semi-transparent objects.
%     % \item Our representation can be initialized from pre-trained NeRF (Plenoxels) and further optimized to refine the surfaces, achieving better or comparable performance to the state-of-the-art methods when reconstructing scenes with thin structures or semi-transparency. 
%         \WW{Check claim!}
% \end{itemize}




