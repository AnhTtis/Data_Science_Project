\section{Evaluation}


\begin{figure}[t]
  \centering
\hfill
  \begin{minipage}[]{0.45\textwidth}
\small
\centering
\singlespacing
\begin{tabular}{lccc}
\toprule
{} &   Thin & Translucent &  Avg \\
\midrule
Plen   &          0.526 &          0.761 &          0.644 \\
Mip360 &          1.445 &          3.063 &          2.254 \\
NeuS          &          1.048 &          2.344 &          1.696 \\
HFS           &          0.925 &          3.698 &          2.312 \\
neuralangelo  &          0.424 &          1.125 &          0.774 \\
NeRRF         &          2.349 &          2.086 &          2.218 \\
Ours          &  \first{0.284} &  \first{0.624} &  \first{0.454} \\
\bottomrule
\end{tabular}
    \captionof{table}{\textbf{Chamfer distance $\downarrow \times 10^{-2}$ on synthetic datasets.} We highlight the \colorbox{first}{best} methods. 
    % Please refer to the supplementary for results on individual scenes. 
    We justify the choice of density level set values for Plenoxels and MipNeRF360 in the supplementary. 
    } 
    \label{tab:chamfer}
  \end{minipage}
  \hfill
  \begin{minipage}[]{0.47\textwidth}
  \includegraphics[width=\textwidth]{figures/low_res.pdf}
   \caption{\textbf{Surface reconstruction with lower resolutions.} Our method can reconstruct thin surfaces under a low resolution, where thin structures such as ropes heavily blend with the background.
   }
\label{fig:low_res}
  \end{minipage}
% \vspace{-15pt}
\end{figure}


\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.95\textwidth]{figures/main_syn.pdf}
\end{center}
   \caption{\textbf{Qualitative evaluation on synthetic datasets.} The red color indicates the L1 error in the reconstruction.
   % , and part of the exterior surface in the ``monkey" scene was removed to illustrate the interior
   % HFS fails to reconstruct the ``kitchen table" scene. 
   Our method can reconstruct the translucent and thin surfaces missed in NeuS and HFS, while recovering more accurate and noise-free surfaces compared to neuralangelo and NeRF-based methods. Additional scenes can be found in the supplementary. 
   % \WW{TODO: explain the red cross}
   % \GR{method names are too close to the images}\WW{TODO} 
   % \GR{Likely reviewers will ask for RGB/rendering results also?}\WW{I might pick a few scenes and show RGB in supplementary, but probably not going to show comparison, as our RGBs are not as good as the others and would be a distraction. I assume that's fine, as NeuS/other surface recon papers don't show RGB comparison either.}
   }
\label{fig:main_syn}
% \vspace{-10pt}
\end{figure*}

% \begin{figure}
% \begin{center}
%   \includegraphics[width=0.48\textwidth]{figures/low_res.pdf}
% \end{center}
%    \caption{\textbf{Surface reconstruction with lower resolutions.} \ww{Our method can reconstruct thin surfaces under a low resolution, where thin structures such as ropes heavily blend with the background.}
%    }
% \label{fig:low_res}
% \vspace{-10pt}
% \end{figure}


We quantitatively and qualitatively evaluate our method on an extended version of the NeRF synthetic dataset~\cite{nerf}, with 8 additional objects with delicate and thin structures, and 8 objects with translucent materials. We also show a qualitative comparison on challenging real-world scenes containing translucent surfaces. 
% We do not evaluate on datasets such as DTU~\cite{dtu}, as they do not contain any thin structures or translucent surfaces. 
% In fact, those types of geometry are challenging to be captured accurately with a depth scanner in the real world. 
We compare with recent SDF optimization methods including NeuS~\cite{neus} and HFS~\cite{HFS},
% and both density level set and depth extracted geometry from Plenoxels~\cite{plenoxels}
and NeRF-based methods including Plenoxels~\cite{plenoxels}
and MipNeRF360~\cite{mipnerf360}.

\subsection{Datasets}

\noindent
\textbf{Synthetic Thin \& Translucent}: We render 8 scenes with thin structures and 8 with translucent surfaces with Blender~\cite{blender}. 
We sampled 100 different training views from a full sphere. For Thin dataset, we included the ``ficus" and ``ship" scenes from the original NeRF Synthetic dataset~\cite{nerf}. For each scene, we also rendered the depth and converted them to dense point clouds for quantitative geometry evaluation. This removes any invisible inner structures in the 3D assets. Except for the translucent scene ``monkey" and ``vase" where part of the object is completely surrounded by translucent surfaces, we hence directly extracted the scene meshes as the ground truth geometry. We will release all the datasets with reference geometry upon publication. 

% \noindent
% \textbf{Thin \& Translucent Blender}:
% We build two additional synthetic datasets featuring objects with thin structures and translucent materials respectively. The renderings and ground truth geometry of those additional scenes are obtained in the same way as the above NeRF synthetic dataset, \ww{except for the translucent scene ``monkey" and ``vase" where part of the object is completely surrounded by translucent surfaces, we hence directly took the scene meshes as the ground truth geometry}. We will release all the datasets with reference geometry upon publication. 

% \noindent
% \textbf{NeRF Synthetic}: NeRF synthetic dataset contains 8 objects with various challenging properties such as high specularity and thin structures. The synthetic objects are rendered with Blender~\cite{blender} to obtain RGB images. 
% We sampled 100 different training views from a full sphere and re-rendered the original scenes to make the bottom part visible, except for ``ship" scene where we also removed the bottom tank which blocks too many views. For each scene, we also rendered the depth, which we then converted to dense point clouds for quantitative geometry evaluation. This removes any invisible inner structures in the 3D assets.

% \noindent
% \textbf{Thin \& Translucent Blender}:
% We build two additional synthetic datasets featuring objects with thin structures and translucent materials respectively. The renderings and ground truth geometry of those additional scenes are obtained in the same way as the above NeRF synthetic dataset, \ww{except for the translucent scene ``monkey" and ``vase" where part of the object is completely surrounded by translucent surfaces, we hence directly took the scene meshes as the ground truth geometry}. We will release all the datasets with reference geometry upon publication. 

\noindent
\textbf{Real-World}: 
We additionally captured a real-world scene with thin structures and two with translucent surfaces to qualitatively evaluate our performance. The real-world captures are processed with Colmap~\cite{colmap1, colmap2} to obtain camera parameters. \ww{Note that since our work aims to resolve the geometry-material ambiguity in image-based neural reconstruction instead of handling complicated specular reflection or light transport, we hence focus on real-world thin translucent surfaces with less obvious view-dependent effects, such as cups.}


% We qualitatively evaluate our method on scenes from the LLFF dataset, which contains forward-facing captures of challenging real-world scenes. 
% As we are not focusing on the novel view synthesis task, we use all the available images for training. We additionally captured a scene
% featuring semi-transparent materials and processed it using a similar scheme to LLFF.




% \subsection{Implementation Details}

% % \WW{Need to mention L2 image loss}

% Following Plenoxels~\cite{plenoxels}, we use a sparse voxel grid of size $512^3$ where each vertex stores the surface scalar $\delta$, raw opacity $\sigma_\alpha$ and 9 SH coefficients for each color channel. We directly initialize all the grid values from Plenoxels pre-trained with original hyperparameters and prune voxels with densities $\sigma$ lower than $5$.
% % \GR{is density defined in the method?}\WW{I wanted to mean we discard grids with density lower than 5 during initialization, any better suggestion on the wording?}\GR{What I meant is, is there a symbol defined for the density refered here in the method section. If yes, use it here. If no, why not?}\WW{Ah I see, added the symbol}
% % . Our implementation can also optionally initialize from a set of voxelated density or SH coefficients trained with other NeRF-based methods through SH fitting~\cite{plenoctrees}\GR{Do we show this somewhere (supp?). If not, remove this sentence}. 
% We use $s_\sigma=0.05$ to downscale the density values during initialization.
% For experiments on synthetic datasets, we initialize $5$ level sets at $\bm{\tau}_\sigma = \{10, 30, 50, 70, 90\}$,
% % , \ww{which is chosen by first examining the valid range of initialized density field, then evenly dividing into 5 level values} \WW{This repeats section 3.3, but reviewers were quite curious how the level sets were chosen so maybe it's fine to repeat}
% and linearly decay the truncated alpha compositing parameter $a$ from $5$ to $2$ in first $10k$ iterations. 
% % (only the first 5 intersections are taken into account at the beginning, and the first 2 in the end).
% % For Semi-Trasparent Blender dataset, we use $\bm{\tau}_\sigma = \{10, 30\}$ and keep $a=2$.
% For experiments on real-world scenes, we initialize with less level sets $\bm{\tau}_\sigma = \{10, 30, 50\}$, as we found level values above 50 give almost empty surfaces due to higher density regularization in Plenoxels initialization.
% At the end of the training, we also remove invisible surfaces with opacity $\alpha$ less than $0.1$.
% % and use the same truncated alpha compositing setting. 
% % \ww{We fix all level values during the training. We provide an alternative setting where level values are converged to a single value in the supplementary.}
% % \WW{Added a reference to sup about the converging levels setting.}
% Please refer to the supplementary for detailed hyperparameters.


% \begin{figure}[]
% \begin{center}
%   \includegraphics[width=0.8\textwidth]{figures/llff.pdf}
% \end{center}
%    \caption{\textbf{Qualitative evaluation on translucent real-world scenes.} We show the reconstructed surfaces of real thin or translucent objects. 
%    % NeuS and GeoNeuS fail to capture the thin structure and a significant part of the translucent surface. 
%    % Plenoxels may give incomplete or unsmooth surfaces. Our method can reconstruct more complete surfaces with less noise. 
%    The images are not aligned perfectly due to the use of NDC during optimization. \ww{We would like to highlight that, due to strong view-dependent appearance changes caused by global brightness shifts in an uncontrolled capture environment, it is nearly impossible to reconstruct the translucent surfaces perfectly. Our method achieves significant improvement upon baselines, validating the feasibility of our approach.}
%    }
% \label{fig:llff}
% \vspace{-20pt}
% \end{figure}





\begin{figure*}[t]
  \centering
  \begin{minipage}[]{0.6\textwidth}
  \includegraphics[width=\textwidth]{figures/llff.pdf}
   \caption{\textbf{Qualitative evaluation on translucent real-world scenes.} 
   % We show the reconstructed surfaces in real-world scenes. 
    The images are not aligned perfectly due to the use of NDC during optimization.
    % \ww{We would like to highlight that, due to strong view-dependent appearance changes caused by global brightness shifts in an uncontrolled capture environment, it is nearly impossible to reconstruct the translucent surfaces perfectly. Our method achieves significant improvement upon baselines, validating the feasibility of our approach.}
   }
\label{fig:llff}
  \end{minipage}
\hfill
  \begin{minipage}[]{0.35\textwidth}
\small
\centering
\singlespacing
\tabcolsep=0.06cm
% 
% \begin{tabular}{lrr}
% \toprule
%      name &  ficus  &  table \\
% \midrule
% w/o $\mathcal{L}_\delta$     &  0.279  &          \first{0.373} \\
% w/o $\mathcal{L}_\delta$, $\mathcal{L}_{\mathbf{n}_{1,2}}$  &  0.333  &          0.404 \\
% w/o trunc  &  0.837  &          0.431 \\
% w/o $\mathcal{L}_c$   &  0.266  &          0.412 \\
% $\bm{\tau}_\sigma=\{10\}$ &  0.583  &          0.470 \\
% Ours & \first{0.240}  &          \first{0.373} \\
% \bottomrule
% \end{tabular}
% 
\begin{tabular}{lrrrr}
\toprule
     name &  ship &  ficus & table &  monkey \\
\midrule
    no tv &    .317 &  .279 & .373 &   .777 \\
 notvnorm &    .373 &  .333 & .404 &   .970 \\
 no trunc &    .522 &  .837 & .431 &   1.03 \\
  no conv &    .287 &  .266 & .412 &   .857 \\
single lv &    .624 &  .583 & .470 &   .946 \\
     Ours &    \first{.277} &  \first{.240} & \first{.373} &   \first{.776} \\
\bottomrule
\end{tabular}
% 
% 
    \captionof{table}{\textbf{Quantitative results of ablation study.} We report the Chamfer distance $ \times 10^{-2}$ on two scenes from each synthetic dataset. More results can be found in the supplementary.
    % The full method achieves the best Chamfer in both cases.
    }
    \label{tab:abla}
  \end{minipage}
% \vspace{-10pt}
\end{figure*}




\subsection{Baselines}

We compared our method with the state-of-the-art SDF-based reconstruction methods, as well as NeRF-based methods with level set geometry.
% 
% \noindent
We compare with NeuS~\cite{neus} HFS~\cite{HFS}, GeoNueS~\cite{GeoNeus} and neuralangelo~\cite{neuralangelo}. 
Since GeoNueS~\cite{GeoNeus} requires SfM points and visibility masks as input, we only compare with it qualitatively in real-world scenes.
% \noindent
% 
% \textbf{NeRF Methods}:
We also compare with Plenoxels~\cite{plenoxels} and MipNeRF~360~\cite{mipnerf360}, a follow-up of NeRF with conical frustum sampling and weight regularization. We also compare with NeRRF~\cite{chen2023nerrf}, which reconstructs fully transparent objects with additional mask supervision.
% For evaluating the surface from the thresholded density field, we use the same point-sampling algorithm as in our approach, where the density field is voxelated into a grid of size $512^3$, and 75 rays are sent for each voxel to sample points on surfaces. 
% In addition to level set surfaces, we also compared with mode depth-extracted surfaces from NeRF methods. We send rays from 100 spiral views from elevation $-180 \degree$ to $180 \degree$ to cover the whole object, and extract samples with the highest weight along each ray. We found this approach gives the best performance compared to mean or median depth extraction. We also mask out samples extracted from rays with accumulation weight
%\GR{Is there a symbol defined for this weight in the method section?}\WW{@gernot, unfortunately we removed the technical background set and no longer have a symbol defined. Let's just use pure text here?}
% less than 0.1 from Plenoxels, and less than either $0.1$ or $0.9$ from MipNeRF 360.
% To ensure the evaluation is done on well-defined surfaces and deal with the potential imbalance in the distribution of depth-extracted points, we first downsample the points with density 0.001, then run \ww{alpha shapes with $\alpha=0.003$ to reconstruct meshes for Chamfer distance evaluation}. We did not use TSDF~\cite{tsdf} or Poisson surface reconstruction~\cite{poisson_recon}, as the former tends to remove delicate structures and the latter can produce many redundant surfaces and requires per-scene pruning. Overall, we found out that alpha shape\cite{alpha_shape} gives the best result when reconstructing surfaces from very densely sampled points\GR{From this sentence it is not clear if alpha shapes is also used in the evaluation, or if it is just some observation.} \WW{See above highlighted sentence. I hope this should make things clear?}.

% Although comparison with this type of geometry is often missing in the past literature, we found that it provides very good accuracy and completeness, surpassing both of the SDF-based methods. 

% \ww{To the best of our knowledge, $\alpha$Surf is the first method that tries to reconstruct translucent surfaces purely from RGB images. Existing methods that target transparent object reconstruction~\cite{neto, through_glass, laser_transparent_recon} rely on additional supervision such as environment matting or mapping, and are hence not directly comparable to ours.}



\subsection{Evaluation on Synthetic dataset}




We quantitatively evaluate our method on the synthetic datasets and report the Chamfer-L1 distance as evaluated in~\cite{dtu} in Table \ref{tab:chamfer}. 
% We followed the evaluation protocol of~\cite{dtu}, where we extracted dense point clouds from all level surfaces and downsampled both predicted and ground truth points with 0.001 density before computing the Chamfer distance.
%\GR{Is the Chamfer evaluation metric defined somewhere?}\WW{Following NeuS, seems that it's not needed to define Chamfer. Ofc if you know a suitable citation, we can also cite it.}\GR{NeuS cites Unisurf and IDR and IDR writes it uses the Chamfer-L1 distance. Are we using that one? If yes, we could write: Chamfer-L1 distance as used in IDR}\WW{Ah I see, added that. Thx!}
HFS failed and learned empty surfaces on two Translucent scenes, while neuralangelo failed on one scene in Thin Blender.
% , hence we removed them when calculating its average. 
% Our non-trimmed single surface provides better watertight surfaces than the watertight baselines. Our trimmed representation further improves the Chamfer distance by removing redundant surfaces trapped at low opacity region, also giving better or comparable surfaces than the depth-extracted surfaces from NeRF-based methods. 
In comparison, our method signifcantly outperforms the baselines, and the difference is particularly noticeable when compared to SDF-based methods on the translucent dataset, as the baselines cannot properly represent translucent surfaces. NeRF-based methods can potentially recover the translucent surfaces with a low density level set, but their ambiguity in representation leads to significant noise in the reconstruction.

% \WW{Can remove below?}
% The depth-extracted surfaces from MipNeRF360 with accumulation less than $0.9$ trimmed give a comparable Chamfer to ours on NeRF synthetic dataset, but it assumes solid objects and fails on the Semi-Transparent Blender dataset. 
% In addition, note that the both training and depth extraction of MipNeRF360 is much more inefficient than ours.

% In addition, the training of our method takes no more than 30 minutes (including training of Plenoxels) and surface extraction of both versions takes less than 5 seconds, whereas training of MipNeRF360 takes around 24 hours on the same GPU and depth extraction takes around 30 minutes. \WW{Remove this sentence?}

We show qualitative comparison in Figure~\ref{fig:main_syn}. \ww{While NeuS, HFS and Neuralangelo are capable of reconstructing highly smooth surfaces, they cannot properly capture translucent or thin surfaces. 
% Neuralangelo reconstructs redundant and unsmooth surfaces upon translucent materials. 
Surfaces from Plenoxels and MipNeRF360 contain holes and floaters and are unsmooth.} NeRRF is capable of reconstructing smooth translucent surfaces, but fails to model intricate structures.
Our approach is capable of reconstructing surfaces with minimal artifacts. In Figure~\ref{fig:low_res}, we show surfaces reconstructed from 800, 400, and 200 resolution images on a scene with thin structures. Our method can reconstruct the thin surfaces despite the heavy blending effects in the low-resolution images.

\subsection{Evaluation on Real World Dataset}

We show the qualitative comparison on real-world scenes with thin and translucent surfaces in Figure~\ref{fig:llff}. 
% \ww{We focus on the quality of reconstructed object surfaces instead of background here, as Plenoxels and our method do not incorporate a background module.}
\ww{
We show surfaces extracted Plenoxels using $\sigma=10$ level sets, as it has the best qualitative results compared to $30, 50$ levels.
Neus and GeoNueS fail to reconstruct the majority of the thin or translucent surface, while surfaces from Plenoxels are noisy and unsmooth.
\ww{Our method mitigates the artifacts inherited from NeRF initialization and reconstructs surfaces with much higher quality.} 
\ww{We would like to highlight that, due to the highly ill-posed nature of the problem and view-dependent appearance changes caused by global brightness shifts in an uncontrolled capture environment, it is nearly impossible to reconstruct the translucent surfaces perfectly. Our method achieves significant improvement upon baselines, validating the feasibility of our approach.}
\ww{Additional comparison with neuralangelo~\cite{neuralangelo} can be found in the supplementary.}
% whereas our approach can capture more complete and accurate translucent surfaces.
% Regardless, due to the highly ill-posed nature of the reconstruction problem, some translucent surfaces that always occlude a non-textured object, such as the lower part of the translucent cup in the scene ``cup", still cannot be reconstructed accurately. Additional semantic priors would be required to reconstruct the complete surface in this case.
}
% NeuS fails to reconstruct proper surfaces and models everything as nearly flat surfaces due to the limited views. Plenoxels can recover surfaces with high-quality details through a low density level value, but tends to miss some surfaces and produce noisy density floaters. By initializing from Plenoxels and further optimizing with surface regularizations, our approach reconstructs surfaces with fewer artifacts.  \WW{TODO: to be updated}
% Note that although Plenoxels with level value $\sigma=10$ gives views with decent quality, it tends to leave many redundant surfaces inside the object. This is not well reflected due to the forward-facing nature of the dataset, but can be seen from our evaluation on synthetic datasets.
% as well as Figure~\ref{fig:teaser}. 


\subsection{Ablation}
\label{sec:abla}

% We show ablation study in Table~\ref{tab:abla}. 
% Specifically, we crop part of the outer surface in ``ficus" scene to show the inside view. 
As shown in Table~\ref{tab:abla},
the truncated alpha compositing deals with the inner volume artifacts inherited from initialization, while the surface regularization $\mathcal{L}_{\mathbf{n}_1}, \mathcal{L}_{\mathbf{n}_2}, \mathcal{L}_\delta$ and convergence loss $\mathcal{L}_c$ encourage smooth and accurate surfaces. Using a single level set $\bm{\tau}_\sigma=\{10\}$ to initialize the surface fails to capture all information in the pre-trained NeRF and causes artifacts in optimization. Ours with all techniques enabled achieves the best performance. More results can be found in the supplementary. 

% \begin{figure*}
% \begin{center}
%   \includegraphics[width=0.95\textwidth]{figures/abla.png}
% \end{center}
% %    \caption{\textbf{Ablation Study.} }
% % \label{fig:abla}
% % \begin{table}[]
% % \centering

%    \caption{\textbf{Ablation Study.} We show the non-trimmed surfaces from different ablations of our method. 
%    %\GR{Why no qualitative results?}\WW{added}
%    \GR{Can be moved to supp if needed.}
%    % ``no truncate" refers to no truncated alpha compositing, $\mathbf{\tau}_\sigma$
%    % For ``ficus‚Äù scene, we crop part of the surface on the vase to show the inner structures.
%    }
% \label{fig:abla}
% % \end{table}
% \end{figure*}

% \begin{table}[]
% \small
% \centering
% \singlespacing
% \tabcolsep=0.06cm
% \begin{tabular}{lcccccc}
% \toprule
% {} & w/o $\mathcal{L}_\delta$ & w/o $\mathcal{L}_\delta$, $\mathcal{L}_{\mathbf{n}_{1,2}}$ & w/o trunc & w/o $\mathcal{L}_c$ & $\bm{\tau}_\sigma=\{10\}$ & Ours \\
% \midrule
% ficus &  0.279 &     0.333 &     0.837 &    0.266 &      0.583 &   \first{0.240} \\
% % lyre  &  0.235 &      0.290 &     0.599 &    0.231 &       0.380 &  \first{0.188} \\
% % case &  0.837 &     1.148 &     1.051 &    0.872 &      0.903 &  \first{0.822} \\
% table &  \first{0.373} &     0.404 &     0.431 &    0.412 &       0.470 &                              \first{0.373} \\
% \bottomrule
% \end{tabular}
%     \caption{\textbf{Quantitative results of ablation study.} We report the Chamfer distance $ \times 10^{-2}$ on one scene from each synthetic dataset.
%     % The full method achieves the best Chamfer in both cases.
%     }
%     \label{tab:abla}
% \vspace{-10pt}
% \end{table}


% Although our approach can model semi-transparent surfaces to a better extent compared to existing methods, it still fails to reconstruct surfaces with very low opacity, where additional priors might be required to achieve reliable reconstruction. 
% Our surface is also less smooth compared to MLP-based SDF methods due to the lack of spatial smoothness encoded in the MLP. Using stronger surface regularization can enhance the smoothness, but also destroys thin structures in the reconstruction. Adapting to a hybrid representation similar to InstantNGP~\cite{instantNGP} where the surface scalars are obtained via latent codes and shallow MLP might help with this issue. Our representation also requires a background model to be applied on 360 \degree real-world scenes, but this can be implemented very easily using a similar approach to~\cite{nerf++,plenoxels}.