\documentclass[10pt,journal,compsoc]{IEEEtran}
%\documentclass[lettersize,journal]{IEEEtran}

%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
 
\fi






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{txfonts}
\usepackage{media9} 
\usepackage{rotating}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=green,      
    urlcolor=purple,
    }

\usepackage{nomencl}
\makenomenclature

%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Affective Workload Allocation \\ for Multi-human Multi-robot Teams}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Wonse~Jo,~%~\IEEEmembership{Member,~IEEE,}      
        Ruiqi~Wang,~\IEEEmembership{Student Member,~IEEE,}
        Baijian~Yang,~\IEEEmembership{Member,~IEEE,}
        Dan~Foti,~%~\IEEEmembership{Member,~IEEE,}
        Mo~Rastgaar,~\IEEEmembership{Senior Member,~IEEE,}
        and~Byung-Cheol~Min,~\IEEEmembership{Member,~IEEE}% <-this % stops a space

\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem W.S. Jo, R.Q. Wang, B. Yang, B.C. Min was with the Department of Computer and Information Technology, Purdue University, West Lafayette,
IN, 47906.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: \{jow, wang5357, byang, minb\}@purdue.edu
\IEEEcompsocthanksitem D. Foti was with the Department of Psychological Sciences, Purdue University, West Lafayette,
IN, 47906.\protect\\
E-mail: foti@purdue.edu
\IEEEcompsocthanksitem M. Rastgaar was with the School of Engineering Technology, Purdue University, West Lafayette,
IN, 47906.\protect\\
E-mail: rastgaar@purdue.edu

}% <-this % stops an unwanted space
\thanks{Manuscript received March XX, 2023; revised XX xx, 202X.}}



% The paper headers
\markboth{IEEE TRANSACTIONS ON Affective Computing, ~Vol.~X, No.~X, XXX~2023}%
{Jo \MakeLowercase{\textit{et al.}}: Affective Workload Allocation for Multi-human Multi-robot Teams}


\IEEEtitleabstractindextext{
    \begin{abstract}
    The interaction and collaboration between humans and multiple robots represent a novel field of research known as human multi-robot systems. 
    Adequately designed systems within this field allow teams composed of both humans and robots to work together effectively on tasks such as monitoring, exploration, and search and rescue operations. 
    This paper presents a deep reinforcement learning-based affective workload allocation controller specifically for multi-human multi-robot teams. 
    The proposed controller can dynamically reallocate workloads based on the performance of the operators during collaborative missions with multi-robot systems. 
    The operators' performances are evaluated through the scores of a self-reported questionnaire (i.e., subjective measurement) and the results of a deep learning-based cognitive workload prediction algorithm that uses physiological and behavioral data (i.e., objective measurement). 
    To evaluate the effectiveness of the proposed controller, we use a multi-human multi-robot CCTV monitoring task as an example and carry out comprehensive real-world experiments with 32 human subjects for both quantitative measurement and qualitative analysis. 
    Our results demonstrate the performance and effectiveness of the proposed controller and highlight the importance of incorporating both subjective and objective measurements of the operators' cognitive workload as well as seeking consent for workload transitions, to enhance the performance of multi-human multi-robot teams.
    
    %The interaction and collaboration between humans and multiple robots constitutes a relatively new area of research known as human multi-robot systems. Well-designed systems in this field can enable a team of humans and robots to effectively work together on complex and sophisticated tasks such as monitoring, exploration, and search and rescue operations. This paper introduces an affective workload allocation controller that can adaptively allocate workload in real-time while taking into consideration the conditions and work performance of human operators in multi-human multi-robot teams. The proposed controller is primarily composed of three parts: an affective measurement tool, an affective prediction model, and a deep reinforcement learning-based affective workload allocation controller. The affective measurement tool and affective prediction model are used to perceive user and contextual data, and predict human CWL in real-time, respectively. The workload allocation controller takes each operator’s CWL using both objective and subjective measurements as inputs, considers the operator’s task performance model, and allocates optimal workloads to human operators. To evaluate our proposed system, we took a CCTV monitoring task by a multi-human multi-robot team as an example and conducted extensive team-based user experiments involving eight teams (32 human subjects) for quantitative measurement and qualitative analysis. Through these experiments, we demonstrated the effectiveness and performance of the proposed system. Furthermore, we investigated optimal methods for affective workload allocations by comparing other allocation strategies used in the user experiments. As a result, we found that the inclusion of the subjective and objective measurement of an operator's CWL, as well as seeking consent for the workload transitions, must be incorporated in the workload allocation system to improve the team performance of the multi-human multi-robot teams.
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%Short Version%%%%%%%%%%%%%%%%%%%%%%%%
    %This paper introduces an affective workload allocation system for multi-human multi-robot teams that can adaptively allocate workload in real-time while taking into consideration the conditions and work performance of human operators. The proposed system is composed of three parts: an affective measurement tool, an affective prediction model, and a deep reinforcement learning-based affective workload allocation controller. The affective measurement tool and affective prediction model are used to perceive user and contextual data, and predict human cognitive load in real-time, respectively. The workload allocation controller takes an operator’s cognitive load using both objective and subjective measurements as inputs, considers the operator’s task performance model, and allocates optimal workloads to human operators. The proposed system was evaluated using a CCTV monitoring task by a MH-MR team as an example in extensive real-world team-based user experiments involving 32 human subjects. The results of the experiments demonstrated the effectiveness and performance of the proposed system and found that the inclusion of subjective and objective measurement of an operator’s cognitive load, as well as seeking consent for workload transitions, improves the team performance of multi-human multi-robot teams.
    
    \end{abstract}
    
    % Note that keywords are not normally used for peerreview papers.
    \begin{IEEEkeywords}
    Workload allocation, Cognitive load, Multi-human multi-robot teams, Affective computing, Human-robot interaction, and Multi-robot systems.
    \end{IEEEkeywords}
    
}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
%\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\iffalse
\section*{Nomenclature}
    %\vspace{10pt}
    \addcontentsline{toc}{section}{Nomenclature}
    \begin{IEEEdescription}[\IEEEusemathlabelsep\IEEEsetlabelwidth{$V_1,V_2,V_3,V_4$}]
    \item[MRS] Multi-robot system.
    \item[SA] Situational awareness.
    \item[MH-MR] Multi-human multi-robot.
    \item[DRL] Deep reinforcement learning.
    \item[AWAC] Affective workload allocation controller.
    \item[DL] Deep learning.
    \item[CCTV] Closed-circuit television.
    \item[HRI] Human-robot interaction.
    \item[UAV] Unmanned aerial vehicles.
    \item[RL] Reinforcement learning.
    \item[APM] Affective prediction model.
    \item[ISA] Instantaneous self-assessment.
    \item[POMDP] Partially observable Markov decision process.
    \item[HPM] Human performance model.
    \item[GUI] Graphic user interface.
    \item[SAM] Self-assessment manikin.
    \item[NASA-TLX] NASA task load index.
    \item[IS] ISA session.
    \item[AS] Approval session.
    \item[PS] Prediction session.
    \item[ODS] Object detection server.
    \item[MSS] Mission score server.
    \item[ROS2] Robot operating system 2.\\
    
   % \vspace{10pt}
\end{IEEEdescription}
\fi

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{A}{s} artificial intelligence continues to advance, multi-robot systems (MRS) are demonstrating consistent performance and precision that surpasses human ability in various large-scale operations, such as mass surveillance \cite{kolling2008multi}, search and rescue \cite{luo2011multi}, manufacturing, and assembly \cite{sun2002adaptive}. However, compared to humans, MRS still has deficiencies in situational awareness (SA) when it comes to effectively handling complex task dynamics in the real world \cite{hoffman2004collaboration}. For example, adjusting the operation of an MRS in a timely manner in response to new missions and environmental changes can be challenging when operating the system for an extended period of time. This issue is currently mitigated by having a human operator participate in task execution, which improves efficiency. However, systems with many robots can produce excessively high cognitive workloads for a single human operator, making it difficult for them to track each robot's work. This can be addressed by having multiple operators in the loop to provide some level of supervision, resulting a multi-human multi-robot (MH-MR) team \cite{chen2014human}. 

While incorporating human operators and considering them as the core of the decision-making process can significantly improve the system's SA and flexibility, it can also introduce more uncertainty and complexity. Human conditions such as cognitive workload and emotion, as well as performance, are inconsistent and susceptible to internal or external factors \cite{kolb2022leveraging,malvankar2015optimal}. Thus, optimizing the performance of the entire MH-MR team, including optimal workload and workload allocation among multiple humans, is a crucial challenge. The system must monitor human states and reallocate workload, such as the number of robots to be supervised, to maintain each human in optimal interaction conditions with robots \cite{dahiya2022survey,ijtsma2019computational}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{ch5_figures/framework.pdf}
    \caption{Conceptual illustration of the deep reinforcement learning (DRL)-based affective workload allocation controller (AWAC) for multi-human multi-robot (MH-MR) teams.}
    \label{fig:AWA_concept}
\end{figure*}

While the allocation of workload in MRS \cite{khamis2015multi,zhang2012centralized,gini2017multi} and the scheduling of tasks in human-autonomy collaboration \cite{johannsmeier2016hierarchical,karami2020task,creech2021resource} have been well studied, there has been limited research on the objective of optimally coordinating multiple humans in a MH-MR team. In the limited literature available, existing works on workload allocation in MH-MR teams that take human factors into account are still in a preliminary stage and have several drawbacks. Most methods only consider human performance metrics or task difficulty as indicators reflecting human decision-making ability without considering individual human cognitive workload \cite{malvankar2015optimal,ijtsma2019computational,talebpour2019adaptive}. 

However, cognitive workload, which measures the level of mental resources required to complete certain tasks \cite{debie2019multimodal}, serves as a vital factor that influences human ability to process environmental information and make decisions \cite{biondi2021overloaded,son2010age,roy2020can}. Therefore, neglecting this fundamental benchmark makes it difficult to effectively imitate and encode unstructured human decision-making processes. For example, a decrease in human performance could be the result of both cognitive overload and underload, making it challenging to determine the optimal task reallocation strategy (e.g., increasing or decreasing the current workload) without considering cognitive workload. 

Moreover, most existing approaches tend to build a model that encodes the relationship between system attributes, including contextual information and human factors, and the system performance, to serve as one-step rules for determining the optimal workload distribution \cite{malvankar2015optimal,talebpour2019adaptive,fusaro2021integrated,mina2020adaptive}. However, due to the complexity of a MH-MR team and the individual differences of humans and task scenarios, building a valid and generalizable model is a challenging task. Additionally, current workload allocation models are difficult to deploy in realistic MH-MR task scenarios as they mostly lack monitoring and assessment of contextual information and human states, and have barely been validated by real-world experiments.

To address these limitations, we propose a deep reinforcement learning (DRL)-based affective workload allocation controller (AWAC) for MH-MR teams, as illustrated in Fig. \ref{fig:AWA_concept}. The proposed controller is capable of adaptively reallocating workloads based on the operators' performance during collaborative missions with the MRS. Operator performance is estimated by self-reporting questionnaire scores (i.e., subjective measurement) and the results of a deep learning (DL)-based cognitive workload prediction algorithm using physiological and behavioral data. To evaluate our proposed system, we use a closed-circuit television (CCTV) monitoring task by a MH-MR team as an example and conduct extensive real-world team-based user experiments for quantitative measurement and qualitative analysis.

The main contributions of this paper are as follows:
\begin{itemize}
    \item We propose a data-driven human performance model to estimate human operator's mission performance from cognitive workload measurements. It can be adapted to various applications by tuning parameters based on empirical experiments.
    \item We design a DRL-based affective workload allocation controller taking into consideration team performances.   
    \item We validate the productivity and effectiveness of the proposed workload allocation controller through extensive user experiments.
    \item We investigate various workload allocation strategies for MH-MR teams.    
\end{itemize}

%of three major modules: 1) an affective measurement tool (AMT) capable of monitoring various physiological and behavioral signals of multiple operators that reveal objective CWL, subjective CWL, and other system contextual information; 2) an affective prediction model (APM) capable of assessing objective CWL of multiple humans from multi-modal signals collected by the AMT; 3) an affective workload allocation controller (AWAC) capable of adaptively allocating workload among humans based on the human mission performance which is calculated by the subjective and objective CWLs and other system information from AMT and APM using deep reinforcement learning. 

%To evaluate our proposed system, we take a CCTV monitoring task by a MH-MR team as an example and conduct extensive real-world team-based user experiments for quantitative measurement and qualitative analysis.

%how to adaptively allocate workload among multiple operators 



% Need to add contribution and goal


% Need to add the structure of this paper.
The paper is organized as follows. Section \ref{sec:background} presents the background and related works. Section \ref{sec:proposed_system} provides details of the proposed affective workload allocation controller in this paper. Section \ref{sec:design_user_exp} describes the design of a team-based monitoring task that was conducted to validate the proposed controller. In Section \ref{sec:result_analysis}, we present the results of the extensive user experiments and analyze the team performance of the proposed affective workload allocation system. Lastly, Section \ref{sec:discussion} discusses the findings of this study in depth, and Section \ref{sec:conclusion} concludes the proposed the affective workload allocation system.

\section{Background} % Need to add more papers.
\label{sec:background}
In this section, we introduce the background of MH-MR teams and an overview of existing research on workload allocation in these systems.

\subsection{Multi-human Multi-robot (MH-MR) Teams}
%Defintion:::
An MH-MR team refers to a team that consists of multiple human operators and multiple robots, working together to achieve assigned missions \cite{mina2020adaptive}.
% Advantages:
These teams allow for coordinate of actions and shared responsibilities between operators and robots, leading to numerous benefits such as improved efficiency, flexibility, safety, decision-making, collaboration, and resource utilization. The combination of multiple humans and robots enables effective and efficient task completion, with the team capable of adapting quickly to changing conditions and requirements. Multiple robots reduce the risk of a single point of failure, resulting in improved system safety. With multiple humans on the team, more information can be gathered, processed, and analyzed, leading to improved decision-making and problem-solving. Teamwork between humans and robots improves communication and collaboration, ultimately leading to better outcomes. Finally, the use of multiple robots can result in more efficient resource utilization, reducing waste and increasing overall system productivity.

%Applications:
MH-MR teams have been applied in various robotic applications, including but not limited to surveillance coverage problems \cite{li2019distributed}, patrolling \cite{kantaros2017distributed}, search and rescue \cite{wang2017multi}, inspection or assembly of items in an industrial conveyor belt by robotic manipulators \cite{zhang2015multi}, and other multi-agent scenarios \cite{karamouzas2014stigmergy}.


\subsection{Workload Allocation in MH-MR teams}
%2. workload allocation in MH-MR teams: why important; 
Workload allocation in MH-MR teams is a critical issue for practical human-robot interaction (HRI) applications. It is essential for mission completion and success, as it facilitates a clear division of workloads, increased efficiency and productivity, reduced workload and stress on individual operators, and improved adaptability to changing circumstances in the work environment. Assigning tasks to the most-suitable human operator helps avoid conflicts and enhances collaboration and communication between human and robotic agents. With a well-defined workload allocation strategy, the MH-MR team can work cohesively toward achieving the assigned missions.


%3. Limitations of current work (following the intro): a) limited research on the objective of optimally coordinating multiple humans in a MH-MR team; b) not considering human conditions (cognitive workload); c) model-based; d) not validated in real-world scenarios; can not be employed into real-world
Previous research has examined human-in-the-loop systems, such as team organization, task scheduling \cite{chien2012scheduling}, and studies on SA in human-robot systems \cite{drury2003awareness}. Workload allocation strategies based on human cognitive workload \cite{kaber2006situation} have also been researched to control unmanned aerial vehicles (UAV) \cite{prinzel2000closed,wilson2003operator}, and task performance and difficulty of human operators have been studied to reallocate workloads during missions \cite{parasuraman2007adaptive}.
%4. Difference of our work
However, these allocation strategies have typically prioritized system output while overlooking the needs of human operators. As a result, the majority of workload allocation research has been limited to applications in MH-MR teams in real-world settings.

%Why we consider both subjective and objective cognitive load; 
According to \cite{roy2020can},the utilization of human cognitive workload is crucial in HRI applications for maximizing the entire system's performance, including productivity and effectiveness. This is because robotic/autonomous systems can adjust their workloads and control parameters (e.g., speed, control method) based on the human operator's condition, which can be affected by internal or external factors such as personal reasons, stress, negative emotions, and unusual environments. Two primary methods are used to measure human cognitive workload: subjective measurement and objective measurement. 

Subjective measurement, such as self-reported ratings or questionnaires, provides insight into the user's perceived level of cognitive workload, which can be influenced by factors such as motivation and prior experience. However, it is difficult to measure the operator's cognitive workload in real-time, and there is a high possibility of bias or fake cognitive workload, which is intentionally generated by human operators to reduce their workload. 

On the other hand, objective measurement, such as eye-tracking or physiological measures, provides a more direct and quantifiable assessment of cognitive workload. They can reveal specific aspects of the interaction that are causing increased mental effort, such as complex visual displays or cognitively demanding tasks. Some studies in the affective computing and HRI fields have used physiological sensors to reflect the operator's cognitive workload or emotional states \cite{9223531}. Other studies have utilized behavioral data, such as head pose, eye blinking and gazing, as well as dynamic movement of the input interfaces, to estimate affective states \cite{narayanan2020proxemo}. 
However, systems using objective measurement are vulnerable to malfunctioning physiological sensors or behavioral monitoring devices, which can result in inaccurate measurements of human cognitive workload and impair the overall system's performance. 


By incorporating both subjective and objective measurements, the limitations of each method can be mitigated, resulting in a more comprehensive understanding of the human's cognitive workload in HRI applications. This, in turn, enables better-informed design decisions and an enhanced overall user experience. 

% why we use RL, 
% other applications of workload allocation in MRS or other similar areas using MDP and RL

Subjective and objective cognitive workload measurements, when used in conjunction with reinforcement learning (RL), can serve as a workload allocation controller for MH-MR teams. RL optimizes workloads based on both measurements, allowing for real-time allocation of tasks among multiple humans and robots to achieve desired outcomes. This results in a more effective and efficient system that balances the needs of both humans and robots. Furthermore, RL allows for adaptation to changing conditions, which is crucial in dynamic and uncertain human-robot interaction scenarios. Despite its potential benefits, there is limited understanding and fewer studies that have explored the use of both measurements and the RL approach for MH-MR applications in  real world settings. 

Previous research has predominantly focused on the use of RL for workload allocation in human-robot interaction scenarios \cite{zhang2022reinforcement}, \cite{lim2021adaptive}, \cite{ghadirzadeh2016sensorimotor}. These studies aimed to balance the workload between humans and robots in collaborative tasks while considering factors such as user satisfaction, task efficiency, and cognitive load. For instance, Zhang \textit{et. al} \cite{zhang2022reinforcement} proposed an algorithm to optimize task allocation in complex assembly operations. The algorithm was evaluated through a virtual assembly of an alternator and showed great potential in reducing human workload and improving efficiency in human-robot collaboration tasks.
Lim \textit{et. al} \cite{lim2021adaptive} presented a human-machine interface and interaction system to support adaptive automation in unmanned aircraft systems. This system uses a network of physiological sensors and machine learning models to infer the user's cognitive workload in single human operator and MRS scenarios, where the human operator is responsible for coordinating the tasks of multiple UAVs.  %not considered MH-MR team 
Ghadirzadeh \textit{et. al} \cite{ghadirzadeh2016sensorimotor} developed a data-efficient RL framework for modeling physical human-robot collaborations that enables the robot to learn how to collaborate with a human operator. The framework reduces uncertainty in the interaction using Gaussian processes, and optimal action selection is ensured through Bayesian optimization. 

However, these approaches have primarily focused on optimizing task allocation in single human-robot interaction scenarios, which limits the applicability of workload allocation research to real-world collaborations involving multiple robots and human operators. Additionally, these approaches have not comprehensively considered the various conditions of the human operator and have not taken into account the human operator's cognitive workload in collaborative missions with multiple robots. Furthermore, the narrow focus on specific learning models limits their ability to adapt to unexpected situations.

Therefore, we propose a deep reinforcement learning-based affective workload allocation controller for multi-human multi-robot teams that enables human operators to collaborate more effectively with multi-robot systems and teammates, and has the adaptive ability to handle unexpected situations. The controller assigns appropriate workloads based on the operator's and teammate's cognitive workload measured through both subjective and objective measurements using deep reinforcement learning. If one operator has a negative cognitive workload, for example, more work is assigned to the other operator so that the workload is balanced. We validate the proposed affective workload allocation controller by comparing it with various workload allocation methods through generalized team-based user experiments.



\section{Affective Workload Allocation Controller (AWAC)}
\label{sec:proposed_system}
In this section, we introduce our AWAC for MH-MR teams. The AWAC aims to enable human operators to collaborate more effectively with multi-robot systems and teammates by intelligently assigning appropriate workloads based on both subjective and objective measurements of cognitive workload of the operator and their teammates. 
%The affective workload allocation consists of three major interfaces for conducting collaborative missions with multi-human multi-robot (MH-MR) teams: the Affective Monitoring Tool (AMT), the Affective Prediction Model (APM), and the Affective Workload Allocation Controller (AWAC). 
For example, if one operator has a high cognitive workload, the proposed AWAC will assign more work to other operators to balance the workload. The AWAC is designed to improve the efficiency and effectiveness of MH-MR teams by mitigating the impact of cognitive workload on task performance.

To measure the operator's objective cognitive workload, we utilzed physiological and behavioral data collected from two commercially available wearable biosensors, the Empactica E4 \cite{empatica} and Emotiv Insight \cite{emotiv}, as well as a webcam such as the Intel RealSense. The data collected is then used as input for the affective prediction model (APM), which predicts the operator's affective states in real-time \cite{wang2022husformer}. By utilizing the high temporal resolution of these measurements, the APM provides a detailed understanding of the operator's affective states and supports the dynamic allocation of workloads in real-time.

To measure the operator's subjective level of workload, we employed the instantaneous self-assessment (ISA) \cite{jordan1992instantaneous}, a self-reported tool that promptly evaluates an individual's current stress level. The ISA consists of questions regarding physical, emotional, and cognitive stress symptoms and provides a subjective evaluation based on the responses. The aim is to offer a convenient way of monitoring stress levels and supporting stress management. 

%\subsection{Affective Measurement Tool}\label{monitoring_framework}
% The Affective Measurement Tool (AMT) is the tool used to measure physiological and behavioral data from wearable biosensors and behavior-monitoring devices, respectively. We utilize a ROS2-based wearable biosensors package \cite{jo2020ros} to record human data over wearable biosensors for communicating with multi-robot systems supporting ROS2. This package allows for communication between wearable biosensors and multi-robot systems supporting ROS2. To provide real-time computing, support for multiple robots, and reliable streaming data from various nodes, each node in the biosensor package follows a standardized node and topic structure \cite{jo2021toward}.

%In the AMT, raw physiological signals from wearable biosensors are collected at a 100 Hz sampling rate, while behavioral features from the facial view are extracted at a 30 Hz sampling rate. This allows for efficient and accurate measurement of affective states. For the physiological signals, we used two off-the-shelf wearable biosensors, the Empactica E4 \cite{empatica} and Emotiv Insight \cite{emotiv}, to collect physiological data, and a webcam (e.g., Intel RealSense) to record behavioral data. The Emotiv Insight provides readings of 5-channel EEGs, power spectrum (theta, alpha, beta, and gamma), performance metrics, and motion data, while the Empatica E4 provides readings of blood volume pulse (BVP), galvanic skin response (GSR), heart rate (HR), inter-beat interval (IBI), skin temperature (ST), and motion data. These measurements allow us to accurately and reliably capture affective states during collaborative tasks with multi-robot systems. For the behavioral data, we extracted various features from the facial camera views, such as eye aspect ratio (EAR) \cite{mehta2019real}, facial action units, and facial expressions \cite{tian2001recognizing}. These behavioral measurements provide insight into operator affective states and can be used to complement the physiological data collected by the wearable biosensors.

%The data collected from wearable biosensors and behavioral features from the facial camera views in the AMT are downsampled to a sampling rate of 100 Hz. This processed data is then input into the Affective Prediction Model (APM), which uses it to predict operator affective states in real-time. This process allows the team to effectively collaborate and perform tasks efficiently, despite variations in operator affective states. By leveraging the high temporal resolution of these measurements, the AMT and APM can provide a detailed understanding of operator affective states and support the dynamic allocation of workloads in real-time.


%\subsection{Affective Prediction Model}\label{dl_prediction}
%The Affective Prediction Model (APM) is used to predict CWLs from objective measurement of physiological and behavioral signals, which can range from low to medium to high. To make these predictions, the APM uses the downsampled dataset collected through AMT. This dataset, called the MOCAS dataset \cite{wonse_jo_2022_7023242}, is a multimodal affective dataset that includes data from wearable biosensors and behavioral monitoring devices. The stimulus used in the dataset is a generalized CCTV monitoring task, which is designed to elicit various levels of cognitive load by changing the assigned workload (e.g., the number of camera views and robot speed). The MOCAS dataset was built from data collected from 21 participants and was validated using statistical analysis of subjective measurements and machine learning-based classification analysis of objective measurements.

%For predicting the human cognitive loads referred to as the human's affective state, we applied \textit{Husformer} on APM, which is an end-to-end multimodal transformer framework for the recognition of multimodal human cognitive load \cite{wang2022husformer}. \textit{Husformer} fuses modalities with adaptive cross-modal interactions, inspiring one modality to directly attend to features of other modalities where strong cross-modal relevance exists. In addition, \textit{Husformer} can adaptively highlight important contextual information in the fusion representation. Experimental results on four public benchmark multimodal dataset of human affective states (e.g., raw DEAP \cite{koelstra2011deap}, WESAD \cite{schmidt2018introducing}, CogLoad \cite{gjoreski2020datasets}, and MOCAS \cite{wonse_jo_2022_7023242} datasets) demonstrated the effectiveness of the proposed \textit{Husformer} on the general human affective state recognition; specifically, it outperformed other state-of-the-art multi-modal-based baselines and demonstrated enhanced performance over using a single modality. The best accuracy is 96.45$\pm$12.11\% to predict three classes of cognitive load (e.g., low, medium, and high) from the downsampled MOCAS dataset. The outputs of \textit{Husformer} is then normalized with a range of from -2 to 2 to use the equivalent scale of ISA scores. 

\subsection{Problem Formulation}
Although having knowledge of contextual information and human conditions (e.g., cognitive workload) can serve as a decision-making foundation for the workload allocation problem, it is still a challenging task to determine the ideal workload distribution, given the complex nature of MH-MR teams. Unlike existing model-based approaches that rely on a single-step allocation rule model, we address the workload allocation in MH-MR teams as a partially observable Markov decision process (POMDP) and apply DRL to find the optimal solution. The partial observability arises from the limited ability of the APM to accurately and completely predict continuous human conditions during a task. The proposed POMDP for the affective workload allocation in MH-MR teams is defined as a tuple $(\mathcal{H}, \mathcal{W}, \mathcal{S}, \mathcal{O},\mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma)$, where:

% Replace all parameters based on this tuple;;;

\begin{itemize}
    \item $\mathcal{H}$ := \{${h_1}$, \dots, ${h_n}$\} is a finite set of $n$ human agents.
    \item $\mathcal{W}$ := \{${w_1}$, \dots, ${w_i}$\} is a finite set of $i$ tasks, e.g., the number of robots to be supervised, to be assigned to $n$ human agents.
    \item $\mathcal{S}$ := \{${S^s}$ $\times$ ${S^o}$\} is the joint human state observed, including subjective cognitive workload obtained by the self-reporting questionnaire, ${S^s}$ := \{${s^s_1}$ $\times$ \dots $\times$ ${s^s_n}$\}, and objective cognitive workload assessed by the APM, ${S^o}$ := \{${s^o_1}$ $\times$ \dots $\times$ ${s^o_n}$\}, of the $n$ human agents.
    \item $\mathcal{O}$ := \{${o_1}$ $\times$ \dots $\times$ ${o_q}$\} is the joint observation of the contextual information, e.g., current workload distribution, performance metrics, and other important system characteristics.
    \item $\mathcal{A}$ := \{${a_1}$ $\times$ \dots $\times$ ${a_n}$\} is the joint allocation decision by assigning $w_i$ to $h_n$.
    \item $\mathcal{T}$ := ${P}\left(s^{\prime} \mid s, a\right)$ is the state transition function.
    \item $\mathcal{R}$ := $f_R\left(s \times o, s^{\prime} \times o^{\prime}\right)$ is the reward function, which gives immediate reward after the transition from $s \times o$ to $s^{\prime} \times o^{\prime}$ by taking action $a$.
    \item $\gamma$ is the discount factor.
\end{itemize}

This formulation allows fundamental modeling of environment dynamics of the affective workload allocation problem in a MH-MR team, and explicitly defines various attributes of the team, such as the human state, including subjective and objective cognitive workloads, and contextual information, including performance metrics, current workload distribution and other system characteristics. The goal of this POMDP is to find the optimal policy $\pi^{*}: (s \times o) \mapsto {a}$ that obtains maximum system performance, i.e., the expected total discounted reward $\mathbb{E}[\sum_{k=0}^{\infty} \gamma^{k} r_{t+k}]$. Through this optimization process, the AWAC algorithm learns to adaptively adjust workload distribution for $n$ human agents based on contextual information and human conditions, including subjective and objective measurements of cognitive workloads, to maximize the operational performance.

%The Affective Workload Allocation Controller (AWAC) dynamically adjusts the workload of human operators based on their predicted performance, which is determined by both subjective and objective measurements of CWL. The subjective measurement is a self-report of cognitive load using Instantaneous Self-Assessment (ISA) \cite{jordan1992instantaneous}. The objective measurement is the prediction result of the APM, which ranges from -2 to 2. 
%The AWAC allows the MH-MR team to effectively collaborate and perform tasks efficiently, despite variations in operator affective states.

\subsection{Data-driven Human Performance Model (HPM)}
\label{human_performance}
While the introduction of the POMDP and DRL can provide a better chance to find the optimal workload allocation strategy, it requires a high volume of interaction rounds to reach good performance. Therefore, a DRL model is typically trained in a simulation environment to achieve results in a cost-effective and timely manner. To build a sound simulation environment for an MH-MR team, the key challenge is to build a human performance model (HPM) that can simulate human performance based on the human state and serve as the transition function of the human state. 
To address this challenge, we propose a data-driven HPM that estimates the human operator's current mission performance from the subjective and objective measurements of the cognitive workloads. This HPM can be easily adapted to various applications by tuning the parameters of the equations derived from the empirical experiments. 


%1. Describe the human model in a generalizable way so that readers can easily build one for their task scenarios;\\
%2. Describe why our data-driven human model is reasonable (validation);\\
%3. Describe how the parameters can be tailored to enable the human model to be generalized to certain humans or task scenarios 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{ch5_figures/human_performance_model.pdf}
    \caption{Generalized correlation between performance and levels of cognitive workload based on the Yerkes-Dodson law \cite{yerkes1907dancing}.}
    \label{fig:human_yerkes_dodson}
\end{figure}

%%%% Jo,, from here (02/13/2023)
For the generalized HPM, we applied the Yerkes-Dodson law \cite{yerkes1907dancing}, which is a psychological principle that describes the relationship between cognitive workloads and mission performance. It states that performance generally improves with increased cognitive workloads, but only up to a certain point. Beyond this point, further increases in cognitive workloads lead to a decline in performance. The law is represented by an inverted-U shape as illustrated in Fig. \ref{fig:human_yerkes_dodson}, and can be mathematically described by the Gaussian distribution as Eq. \ref{eq:ch05_performance_isa_pred}, with the optimal level of cognitive workloads for maximum performance at the peak of the curve. The law helps explain why too much stress can have a negative impact on performance and why finding the right level of stress is important for optimal performance. The Yerkes-Dodson law has been applied to various fields to estimate human's performance in HRI research \cite{wang2014human,sam2021investigating,wampler2017towards}.
\begin{equation} %% e fitting
    \label{eq:ch05_performance_isa_pred}
    \begin{split}
        P(\mathcal{S}) = \frac{A}{{\sigma \sqrt {2\pi } }}e^{{{ - \left({\mathcal{S} - \mu } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {\mathcal{S} - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}} \\
        %A = \displaystyle\sum\limits_{i=0}^n-1 \frac{(x_{i+1} - x_{i})(y_{i+1} - y_{i})}{2}
    \end{split}
\end{equation}


\noindent
where area $A$ is calculated using the trapezoidal rule: $A = \sum_{i=0}^{n-1} (x_{i+1} - x_{i})(y_{i+1} - y_{i})/2$. The maximum and minimum values of ${S^s}$, ${S^o}$, and $P$ are used in the calculation. The values of $\sigma$ and $\mu$ are determined by the type of mission and the measurement methods used to evaluate human performance, such as subjective measurement, $P_{S^s}$ and objective measurement, $P_{S^o}$. $\mathcal{S}$ is the measured cognitive workload and serves as an input variable to convert into $P$. 


\iffalse
Additionally, we analyzed the human's mission performance with the NASA-TLX scores of MOCAS dataset, which is a famous questionnaire in the psychology field for subjective measurement of the cognitive workload \cite{hart1988development}, and validated that the trend of the correlation between the performance and the NASA-TLX scores is similar to the results of using the ISA score. 
In order to generalize the correlations, we normalized the distribution of the ISA and applied a polynomial curve fitting, then generated below Eq. \ref{eq:ch05_performance_isa_pred} for building the HPM.

\begin{equation} %% e fitting
    \label{eq:ch05_performance_isa_pred}
    \begin{split}
        P_{o} = -0.1294c_{o}^{2} - 0.1981c_{o} + 0.9149
        \\
        P_{s} = -0.2055c_{s}^{2} + 0.0891c_{s} + 1
    \end{split}
\end{equation}

\fi

Then, the integrated human operator's mission performance model using $P_{S^s}$ and $P_{S^o}$ is estimated by 
\begin{equation}
    \label{eq:ch05_all_performance}
    \begin{split}
    P_{h_n} = \alpha_{p}P_{S^o}  + \beta_{p}P_{S^s}.
    \end{split}
\end{equation}
Here, we added two weights ($\alpha_{p} = 0.5$, $\beta_{p} = 0.5$) for the sensitivity of the proposed controller and type of the mission. We utilized both $P_{S^s}$ and $P_{S^o}$, which are calculated by subjective questionnaires and objective measurements of the cognitive workload, respectively, in order to protect against unexpected malfunctions of the physiological sensors as the objective measurement and fake answers of the subjective questionnaires made by human operators intentionally. 

In order to allocate the workloads based on the estimated operator's mission performance $P$, we developed the DRL-based AWAC to find optimal changes for multi-human operators $\mathcal{H}, H \in \{h_1, \dots, h_n\}$, by comparing the current team mission performance with the predicted next team mission performance that reflects changes in the two variables of $\mathcal{W}$, which are the changed workloads of human agents:
%According to Pearson's correlation coefficient ($\gamma$), which measures the relationship between two variables \cite{freedman2007statistics}, we found the correlation between the number of camera views and the means of operator's cognitive load of the subjective measurement reported on our MOCAS dataset \cite{smartlab2022mocas}. $\gamma$ indicates the relationship between two variables with ranges from -1 to 1. The positive high $\gamma$ means a proportional relationship between both variables, whereas the negative high $\gamma$ means an inverse relationship. 
%Given the results of Pearson's correlation, we observed the positive relationship ($\gamma > 0.3$) between the number of the camera view and the cognitive load measured from subjective CWL (e.g., ISA and NASA-TLX). Additionally, the result of the Repeated Measures ANOVA (rmANOVA) test showed that there are significant differences between the number of camera views and the ISA scores ($p < .01$), and results of the weighted NASA-TLX scores ($p < .01$). Thus, we can assume that the change in the number of camera views directly influences the human cognitive load. After that, AWAC predicts the next cognitive load of the human operator from the changes of the number of camera views (e.g., -1 or +1 camera view) as below Eq. \ref{eq:ch05_predicted_ISA_preidction},
\begin{equation}
    \label{eq:ch05_predicted_ISA_preidction}
    \begin{split}
    {S^{o}}_{t+1} = S^{o}_{t} + \Delta w
    \\
   {S^{s}}_{t+1} = S^{s}_{t} + \Delta w
    \end{split}
\end{equation}
where $\Delta w$ is the variance of the assigned workloads. The next performance is estimated using Eqs. \ref{eq:ch05_performance_isa_pred} and \ref{eq:ch05_all_performance}, as
\begin{equation}
    \label{eq:ch05_predicted_performance}
    \begin{split}
       P(S_{t+1}) = \alpha_{p}P({S^{o}}_{t+1}) + \beta_{p}P({S^{s}}_{t+1}).
    \end{split}
\end{equation}
These values are utilized in the AWAC algorithm to assign the optimal workloads based on each human operator's performance.



\subsection{Proximal Policy Optimization for Workload Allocation}
% assumption
Based on the HPM, we built a DRL model to allocate optimal workloads for enhancing team performance. To train our DRL model, we assume that the objective measurement of the cognitive workload predicted by APM indicates the human operator's cognitive workload, and there are no fake answers on the subjective measurement of the cognitive workload. Fig. \ref{fig:drl_diagram_model} depicts the learning diagram used in the proposed DRL model to find optimal transitions of the workloads based human operators' cognitive workloads.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{ch5_figures/RLF.pdf}
    \caption{A learning diagram of the proposed DRL model.}
    \label{fig:drl_diagram_model}
\end{figure}

%For estimating the team mission performance in the CCTV monitoring task, we consider the human operator's cognitive load. Human operators can report their cognitive load through subjective and objective methods. The subjective method is the ISA self-assessment questionnaire and the objective method is the outputs of the APM from behavioral and physiological data.
% state space
The state space ($\mathcal{S}$) in the DRL model is designed to consider individual cognitive workloads measured by the self-reporting questionnaire ($S^{s}_{h_n}$) and by the predicted cognitive workload by APM ($S^{o}_{h_n}$), where $n$ is the number of the human operators and $n \in \{1, \dots , n\}$. 
$S^{o}$ represents the objective cognitive workload, and $S^{s}$ represents the subjective cognitive workload.
% action space
The action space ($\mathcal{A}$) represents the assigned workloads based on operators' performance, $ A \in (a_{1}, \dots, a_{n})$, which are estimated based on operators' $S^{o}$ and $S^{s}$.
% reward
We designed the team mission performance-based reward ($\mathcal{R}$) in order for the DRL model to achieve high team mission performance by comparing predicted performance after taking the next state ($\mathcal{S}^{\prime}$) and action ($\mathcal{A}^{\prime}$) with the current state and action. 

We built an environment to train our DRL model, utilizing the predefined state space $\mathcal{S}$, action space $\mathcal{A}$, and reward $\mathcal{R}$, through the use of OpenAI gym \cite{brockman2016openai} (Algorithm \ref{alg:drl_env_gym}). This was done in conjunction with our surveillance environment, as depicted in Fig. \ref{fig:drl_diagram_model}. Using PPO, we trained the DRL model on the environment to obtain the optimal policy $\pi$, which can be expressed as \cite{schulman2017proximal},

\begin{equation}
    \label{eq:ch05_ppo}
    L^{CLIP}(\theta) = \mathbb{E}{t}\left[\min\left(r_t(\theta)\frac{\pi\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)},clip(r_t(\theta),1-\epsilon,1+\epsilon)\right)\right]
\end{equation}

\noindent
where $\theta$ is the policy parameters, $\pi_\theta$ is the current policy, $\pi_{\theta_{old}}$ is the old policy, $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio, $clip(r_t(\theta),1-\epsilon,1+\epsilon)$ is the clipped function, and $\epsilon$ is the clipping parameter.

\begin{algorithm}[t!]
    \caption{The DRL learning environment.}
    \label{alg:drl_env_gym}
    
    \begin{algorithmic}
        \State Initialize state observation $ \mathcal{S} \gets {(s^{o}_{1}, \dots, s^{o}_{n}), (s^{s}_{1}, \dots, s^{s}_{n})}$;
        \State Initialize actions space $\mathcal{A} \gets (a_{1}, \dots, a_{n})$
        \State Initialize weights $w \gets (\alpha_{p}, \beta_{p})$;
        \While{not terminated}
            \State Receive initial state observation $(S^{o}, S^{s})$ with random;
            \State Receive initial actions space $\mathcal{A}$;
            \ForAll{$episode$ = 1, $\dots$, i}
                \State Initial actions space $\mathcal{A}$ with random;
                \State Calculate reward $r(s, a)$;
                \State Calculate transition in actions space $\mathcal{A}^{\prime}$;
                \State Calculate $P_{team}^{\prime}$ according to $ \mathcal{S}$, $ \mathcal{A}^{\prime}$, and $w$;
                \State Calculate reward $r(s^{\prime}, a^{\prime})$;
                \If{Check termination criteria}:
                    \State Stop this episode;
                \Else
                    \State Update current state observation $s$;
                    \State $episode$ = $episode + 1$
                \EndIf
            \EndFor
            \State Updated policy $\pi$
        \EndWhile
    \end{algorithmic}
\end{algorithm}

This process is terminated if $P_{team}^{\prime}$ is less than $P_{team}$, or if the sum of the assigned $\mathcal{A}$ falls outside the range of $\mathcal{W}$. A total of 1,000,000 samples were used to train the DRL model using three episodes, covering various workload allocation methods. Fig. \ref{fig:ep_rew_mean} depicts the mean episode rewards for each episode.

\iffalse
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{ch5_figures/random_vs_drl_edited.pdf}
    \caption{Comparison performance of the workload allocation methods with random workload allocation and our DRL-based workload allocation.}
    \label{fig:drl_validation}
\end{figure}
\fi

To validate the performance of our AWAC model, we compared it with a random workload allocation method. Both models were provided with equivalent inputs ($\mathcal{S}$) and subjected to equivalent restrictions as used in the HPM, along with the sum of workloads. We then measured the performance of each model based on the inputs, repeated the experiment 10,000 times, and performed rmANOVA tests. Our results indicate that the proposed model outperforms the random workload allocation method ($p < .01$).
% as illustrated in Fig. \ref{fig:drl_validation}.

\section{Case Study in CCTV Surveillance Scenarios}
\label{sec:design_user_exp}
In this section, we describe how the proposed AWAC was applied to MH-MR CCTV surveillance scenario, and its performance was evaluated through extensive team-based user experiments.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth,height=0.2\textheight]{ch5_figures/rollout_ep_rew_mean_re.pdf}
    \caption{Average of the accumulated rewards for the trained model of our DRL-based AWAC.}
    \label{fig:ep_rew_mean}
\end{figure}

\subsection{Team-based CCTV Monitoring Task}
To validate the proposed AWAC method, we designed a team-based user experiment that involved the MH-MR CCTV monitoring task, which required multiple human operators to monitor and control multiple agents/robots simultaneously. Such surveillance missions are widely required in diverse multi-human-multi-agents system task scenarios, including security monitoring \cite{wang2008affective}, air traffic management \cite{pacaux2002common} and performance checking \cite{stowers2017framework}. The surveillance scenario is typical in real-world human-agent systems, as illustrated at the bottom of Fig. \ref{fig:AWA_concept}, in which human operators undertake a simultaneous CCTV monitoring task while multiple sensors track them.

Based on the surveillance scenario, we built a generalized surveillance environment that supported multi-human operators in conducting CCTV monitoring tasks with multi-robot systems as MH-MR teams. Two human subjects participated together as a team to validate the proposed AWAC for MH-MR teams and investigate the optimal take allocation strategies. As illustrated in Fig. \ref{fig:experiment_protocal}, the participants conducted the eight tasks (from Task A to H). This study was designed as a 2 x 2 x 2 within-subject study, with each task having three sets of the CCTV monitoring task, and each set taking 120 seconds. This user experiment was reviewed and approved by the Purdue University Institutional Review Board (IRB) (\#IRB-2021-1813). 


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.65\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/phase2_experiment_protocol.pdf}
        \caption{}
        \label{fig:experiment_protocal}
    \end{subfigure}    
    \hspace{3pt}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{ch5_figures/user_study_details.pdf}
        \caption{}
        \label{fig:detail_user_experiment}
    \end{subfigure}  
    \caption{Team-based user experiment. (a) Experiment protocol for the entire user experiment, and (b) Details of the user experiment procedures for a single task consisting of three sets.}
    \label{fig:user_experiment}
\end{figure*}


\subsection{Participants} 
For the team-based user experiment, we recruited 32 participants through flyers, social networking services, and email. All participants met the following requirements:

\begin{itemize} 
    \item Over 18 years old, 
    \item No medical history of brain disorders (e.g., stroke, brain tumor, surgery), no mental illness (e.g., depression, bipolar), 
    \item No heart diseases (e.g., high/low blood pressure, myocardial infarction), no vision or muscle impairment, 
    \item No skin irritation or allergic reaction to glycerin and saline fluids.
\end{itemize}

Each participant was compensated \$15 for their time and efforts. To increase the subjects' engagement in the study, we provided additional compensation based on the overall team's scores on the given tasks. The 1st place award was a \$30 Amazon gift card, the 2nd place was a \$20 Amazon gift card, and the 3rd place was a \$10 Amazon gift card. 


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.35\linewidth}
        \centering 
        %\includegraphics[width=0.98\linewidth]{ch5_figures/phase2_robot_setting.pdf}
        \includegraphics[width=0.98\linewidth]{ch5_figures/phase2_environment.pdf}
        \caption{}
        \label{fig:mrs_testbed}
    \end{subfigure}    
    \begin{subfigure}[b]{0.34\linewidth}
        \centering 
        %\includegraphics[width=1\linewidth]{ch5_figures/user_exp_operator.pdf}
        \includegraphics[width=1\linewidth]{ch5_figures/phase2_human_environment.pdf}
        \caption{}
        \label{fig:participant_desk}
    \end{subfigure}
    \begin{subfigure}[b]{0.28\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/experiment_setting-3.pdf}
        %\caption{Multiple sensors collecting multi-modal data from participants during tasks}
        \caption{}
        \label{fig:phase_2_environment}
    \end{subfigure}
    
    \caption{Illustration of the designed surveillance mission scenario. (a) MRS testbed for conducting surveillance missions, (b) Multi-human operators' desk for surveillance missions, and (c) Wearable biosensors and behavioral-monitoring devices used to collect physiological features (red line) and behavioral data (blue line) from each participant.}
    \label{fig:experiment_setting}
\end{figure*}

\subsection{User Study Procedure}
Prior to starting the main experiment, we explained the entire experimental procedure to participants and asked them to fill out an informed consent and a demographic questionnaire. The participants were then instructed to wear two wearable biosensors as shown in Fig. \ref{fig:phase_2_environment}  and perform a surveillance mission using the CCTV monitoring graphic user interface (GUI) program depicted in Fig. \ref{fig:surveillance_GUIs}. The GUI displays live videos from the MRS, which consists of six SMARTmBOT platforms and shows the team's current mission scores. To complete the surveillance mission, participants were required to click on the streaming video windows on the GUI program that contained specific objects, such as skeleton Among US figures, which were considered abnormal objects in this study, as shown in Fig. \ref{fig:mrs_testbed}.

\begin{figure*}[t]
    \centering
    %% For ready and main views gui
    \begin{subfigure}[b]{0.33\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/main_gui.png}
        \caption{}
        \label{img:gui_start}
    \end{subfigure}    
    %% For popup screen
    \begin{subfigure}[b]{0.33\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/ISA_popup.png}
        \caption{}
        \label{img:gui_isa}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/approval_session.png}
        \caption{}
        \label{img:gui_approval}
    \end{subfigure}
    \caption{Graphic user interface (GUI) programs for human operators in the generalized surveillance scenario. (a) Main GUI for performing closed-circuit television (CCTV) monitoring tasks, (b) ISA session (IS) for participants to report current cognitive workload by rating from very low (-2) to very high (2), and (c) Approval session (AS) for admitting transmitted additional workload from a teammate.}
    \label{fig:surveillance_GUIs} %subjective_questionaires
\end{figure*}

After completing the survey and sensor calibration process, we provided instructions for each of the nine tasks (one training and eight main experiment tasks) that the participants needed to complete. Each task consisted of three sets and three break times, as illustrated in Fig. \ref{fig:detail_user_experiment}. Additionally, participants were given time to familiarize themselves with the experiment hardware and to understand the CCTV monitoring tasks used in this user experiment by conducting one trial experiment. The experimental process was outlined to the participants before the start of the main experiment.  

To validate the DRL-based-AWAC, we utilized predicted cognitive workloads and self-reported ISA scores as inputs to dynamically allocate appropriate workloads, taking into account the performance of the other team member (i.e., both subjective and objective cognitive workloads were considered). Two participants formed a team and conducted a simulated CCTV monitoring task together in an environment shown in Fig. \ref{fig:participant_desk}. The CCTV monitoring task was repeated eight times, with each task consisting of three sets of the CCTV task for 100 seconds, as illustrated in Fig. \ref{fig:detail_user_experiment}. During the task, the number of camera views was automatically adjusted based on the participants' task performance, which was calculated using the participants' ISA scores and/or the cognitive workload predicted by the APM. 

Participants were instructed to perform a simulated CCTV monitoring task, in which they had to click on the window displaying a streaming camera view containing an abnormal object from each robot platform. They study considered two types of objects, as illustrated in Fig. \ref{fig:mrs_testbed}: (a) abnormal objects (e.g., skeleton objects) and (b)  normal objects. The objects were randomly placed in  separated rooms, as shown in Fig. \ref{fig:mrs_testbed}, and differed in color and quantity. The participants' mission was to click on the abnormal objects in the streaming camera views within the mission time (e.g., 6 minutes). Clicking on a normal object resulted in a penalty of 3 points, while clicking on an abnormal object in the streaming camera views earned them 1 point. 

The robots moved through the corridor (shown in red in Fig. \ref{fig:mrs_testbed}) at a speed of approximately 300 $mm$/s and streamed the room conditions to the participants' screens to simulate a CCTV monitoring task, as illustrated in Fig. \ref{fig:surveillance_GUIs}. The participants then conducted the simulated CCTV monitoring task using the screen. This procedure was repeated eight times under different experimental conditions. After completing each task, participants were asked to evaluate their emotional and cognitive states using questionnaires such as self-assessment manikin (SAM), ISA, and NASA task load index (NASA-TLX) via the GUI programs.  
At the end of the user experiment, participants were interviewed for approximately 5 minutes to gain insight into their overall experience with our MH-MR AWAC system.  We conducted a semi-structured interview, which began with a lead-off question (e.g., \textit{``Did you notice any differences between the eight sessions? If so, what were they?"}) and was followed by several other questions (e.g.,\textit{ ``What are your thoughts on the ISA and Approval sessions during the mission?"}, and \textit{``Which method do you prefer for conducting this CCTV monitoring task: changing or fixed workloads?"}). 

A supplementary video that demonstrates the user study experiment can be found at \url{https://youtu.be/qrmAQqfdLZk}.

\subsection{Details of Tasks in the User Experiment}
In Task A, two participants view a fixed number of camera views (e.g., three camera views). In Task B, two participants discuss and decide on the allocation of workload (e.g., the number of camera views) before starting the main task, known as a consensus step, and view the fixed number of camera views based on the outcome of the discussion. In Task C and Task D, AWAC adjusts the workloads based on the subjective cognitive workload reported by ISA, called ISA Session (IS), with an additional workload transition method known as Approval Session (AS) used to seek consent from the other participant before changing the workload. Task C has AS, while Task D does not. 
In Task E and Task F, AWAC automatically adjusts the number of camera views based on the objective cognitive workload predicted by APM, called Prediction Session (PS). Task E has AS, while Task F does not. In Task G and Task H, AWAC automatically adjusts the number of camera views based on both subjective and objective measurements of the cognitive workload (e.g., IS and PS). Task G has AS, while Task H does not. The tasks are summarized in Table \ref{tab:ch5_task_list}.

% \usepackage{graphicx}
\begin{table}[h!]
    \centering
    \caption{Summary of tasks used in the user experiment.}
    \label{tab:ch5_task_list}
    \resizebox{0.8\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \rowcolor[HTML]{C0C0C0} 
        \textbf{Task} & \textbf{\begin{tabular}[c]{@{}c@{}}Fixed \\ workload\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}ISA \\ session \\ (IS) \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Prediction \\ session \\ (PS) \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Approval \\ session \\ (AS) \end{tabular}} \\ \hline\hline
        \textbf{A} & O & X & X & X \\ \hline
        \textbf{B} & O & X & X & O \\ \hline
        \textbf{C} & X & O & X & O \\ \hline
        \textbf{D} & X & O & X & X \\ \hline
        \textbf{E} & X & X & O & O \\ \hline
        \textbf{F} & X & X & O & X \\ \hline
        \textbf{G} & X & O & O & O \\ \hline
        \textbf{H} & X & O & O & X \\ \hline 
    \end{tabular}
    }
\end{table}

\begin{figure}[t]
    \centering 
    \begin{subfigure}[b]{0.85\linewidth}
        \centering 
         \includegraphics[width=1\linewidth]{ch5_figures/ISA_prediction_performance_v2.pdf}
        \caption{}
        \label{fig:ISA_prediction_performance}
    \end{subfigure}    
    \begin{subfigure}[b]{0.85\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]
{ch5_figures/both_predicted_isa_map_v2.pdf}
        \caption{}
        \label{fig:performance_map}
    \end{subfigure}  
    \caption{Human performance model. (a) The normalized human performance model (HPM) with instantaneous self-assessment (ISA) scores and prediction results, and (b) An illustration of the HPM, calculated using the ISA score and predicted cognitive workload.}
    \label{fig:human_performance_model}
\end{figure}

\subsection{Tuning HPM parameters}
% As with the Yerkes-Dodson law \cite{yerkes1907dancing} commonly utilized to estimate human's performance on HRI research \cite{wang2014human,sam2021investigating,wampler2017towards}, we found that human's mission performance differs depending on the amount of cognitive load and the difficulty of the mission or task (e.g., the number of cameras) from 30 participants data of MOCAS dataset \cite{smartlab2022mocas}.
% We analyzed the ISA scores and the predicted cognitive load by the APM from the MOCAS dataset, and then found the correlation between the ISA scores and the human's mission performance (or mission scores) and between the predicted cognitive load and the human's mission performance. 
% Fig. \ref{fig:ISA_prediction_performance} shows that the mission scores differ depending on the ISA scores and the predicted cognitive load. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{ch5_figures/Phase2_study_Diagram.pdf}
    \caption{System architecture for the MH-MR surveillance task.}
    \label{fig:surveillance_system_mhmr}
\end{figure*}


%% Update.......need.....
Using the MOCAS dataset \cite{smartlab2022mocas}, we analyzed the results of the subjective and objective measurements of cognitive workload, obtained from ISA answers and APM prediction results as depicted in Fig. \ref{fig:ISA_prediction_performance}. Then, we tuned the parameters of Eq. \ref{eq:ch05_performance_isa_pred} based on the empirical results of the extensive MOCAS dataset. The tuned HPM can estimate the current CCTV mission performance of the human operator from the subjective and objective measurements of cognitive workload. The correlation between performance and subjective and objective cognitive workload is illustrated in Fig. \ref{fig:performance_map}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For our surveillance mission with MH-MR teams, each mission consists of three sets. To incentivize better performance in each episode, we define the reward function as $r = 0.33$ if the next team mission performance is greater than the current team performance, and 0 otherwise: 
\begin{equation} \label{action_space}
    r(s, a) =   \left\{
                \begin{array}{@{}ll@{}}
                    0.33, & \text{if}\ P_{team_{t+1}} \geq P_{team_{t}} 
                    \\
                    0, & \text{otherwise}
                \end{array}\right.
\end{equation}
\noindent where $P_{team}$ is the mean of all human operator's mission performance, and $P_{team^{\prime}}$ is the predicted team mission performance on the next step, calculated by reflecting transitions in the number of camera views. We assume that as the number of camera views ($w_{i}$) increases, the operator's cognitive workload ($h_{n}$) and other state variables ($S^{o}, S^{s}$) will also increase in the next step. The correlation between the number of camera views and state variables was found through empirical results from the MOCAS dataset ($p < .01$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overall System Configuration}

We developed a team-based surveillance system involving two human operators and six multi-robot platforms, as shown in Fig. \ref{fig:surveillance_system_mhmr}. The hardware configuration used for the operators in this user experiment is identical and is mainly comprised of the APM, which serves as the main interfaces for reading physiological signals and behavioral features, and measuring the objective measurement of the cognitive workload. The subjective and objective measurements of cognitive workload are then utilized as inputs for AWAC to reallocate workload. In addition to the main interfaces, there are additional sub-interfaces that were used for conducting the team-based surveillance mission during the user experiment as outlined below.



\subsubsection{CCTV Monitoring GUI Program}
The CCTV GUI Program is the direct interface between the participant and the multi-robot system for conducting the surveillance task, as shown in Fig. \ref{fig:surveillance_GUIs}. The CCTV GUI program displays multiple windows of the camera views and team scores while performing the task. The GUI starts with the setup screen to test communication among the wearable biosensors, behavioral monitoring devices, and multi-robot systems through ROS2. If there is no issue, the GUI enters the preparation step, showing a black cross for 5 seconds, followed by a 5-second countdown timer. Then, the GUI enters the main experiment step where the participant monitors multiple camera views simultaneously, called the CCTV monitoring task \cite{smartlab2022mocas}, to find abnormal objects on the screens. Additionally, only the team scores are displayed on the GUI to reduce peer and time pressures. 

Each task consists of three sets, and each set takes 100 seconds, with a break time of about 20 seconds between each set. The break time is mainly used for reporting participants' subjective cognitive workload through the IS for 10 seconds, and the other 10 seconds are for AS. However, participants have break time if the current task does not require collecting both values (such as Task A and B). After each task, participants are redirected to three surveys, including SAM, ISA, and NASA-TLX, where they respond based on their feelings and thoughts about the entire task. We then repeated eight tasks for 90 minutes.

\subsubsection{Object Detection Server (ODS)}
The object detection server (ODS) performs the CCTV monitoring task of detecting abnormal objects on the streaming video from the multi-robots displayed on the GUI program. The ODS checks whether the human operator detects abnormal or normal objects and provides audio feedback to the human operator based on the results of the ODS. If the human operator detects an abnormal object, they are awarded 1 point. However, if they detect a normal object, they lose 3 points. For detecting abnormal objects, we used the object detection algorithm \cite{redmon2016you}, which has an accuracy of 99.96\% to detect abnormal objects among two classes (i.e., normal and abnormal objects). 


\subsubsection{Mission Score Server (MSS)}
The mission score server (MSS) manages the rewards (+1) and consequences (-3) based on the participant's performance in the CCTV monitoring task. The MSS works in conjunction with the ODS and GUI program during task execution. The ODS updates the MSS by determining if the human operator has completed the task correctly, while the GUI program serves as the user's direct interface and displays the updated team score as the task is being carried out.


\subsubsection{Multi-Robot System (MRS)}
The MRS consists of six ROS2-based multi-robot platforms, known as SMARTmBOT \cite{jo2022smartmbot}. 
Each SMARTmBOT is an open-source mobile robot platform based on robot operating system 2 (ROS2), with a diameter and height of 15 $cm$ and 10 $cm$, respectively, and a weight of 900 $grams$. The robot's orientation and direction are tracked by the Vicon motion capture system with 100 Hz sampling rate, which uses reflective markers attached to the top of the robots. To perform the surveillance mission, a pure-pursuit control algorithm was employed, allowing the robot to repetitively travel between the starting position and the goal position at a velocity of approximately 300 $mm/s$ \cite{paden2016survey}.


%% showing order 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{ch5_figures/experiement_subject.png}
    \caption{Experiment snapshots for our user experiments.}
    \label{fig:all_teams_study} 
\end{figure}

\section{Results and Analysis}
\label{sec:result_analysis}  

In this section, we present the results and findings of our extensive team-based user experiments. 
We recruited 32 participants (female: 9 and male: 23) for 16 teams and conducted the experiments to validate the proposed AWAC, as shown in Fig. \ref{fig:all_teams_study}. The participants had an age range of 18 to 34 (\textit{Mean}=23.81, \textit{S.D.}=4.17) and had no prior experience performing the designed CCTV monitoring task and satisfied the requirements for our user experiment. They performed one dummy task in the beginning to become familiar with our CCTV monitoring task and then performed the eight tasks. The overall experiment tasks took about 90 minutes to complete, from Task A to Task H. 

Table \ref{tab:standized_results} shows the normalized team performance (or obtained mission scores) of all the teams. We normalized the raw team performance ($P$) by dividing it by the mean of the team performance ($\mu$); $P_{norm} = P/\mu$, in order to create standards and transform data taken from different teams into a consistent format.

\begin{table}[h!] % Updated (8 teams)
    \centering
    \caption{Normalized team performance of all 16 teams.}
    \label{tab:standized_results}
    \resizebox{1\columnwidth}{!}{%
        \begin{tabular}{|c||c|c|c|c|c|c|c|c|}
        \hline
        \rowcolor[HTML]{C0C0C0} 
        \textbf{} & \textbf{Task~A} & \textbf{Task~D} & \textbf{Task~F} & \textbf{Task~H} & \textbf{Task~B} & \textbf{Task~C} & \textbf{Task~E} & \textbf{Task~G} \\ \hline \hline
        \textbf{T1} & 0.8447 & 1.0014 & 1.4295 & 0.9059 & 1.0855 & 0.8524 & 0.9556 & 0.9250 \\ \hline
        \textbf{T2} & 0.9703 & 0.7378 & 0.9110 & 0.9514 & 1.0605 & 0.9561 & 1.2503 & 1.1625 \\ \hline
        \textbf{T3} & 0.6945 & 1.0103 & 0.7546 & 0.9201 & 1.1658 & 1.0981 & 1.1633 & 1.1934 \\ \hline
        \textbf{T4} & 1.0060 & 1.0060 & 1.1039 & 0.9530 & 0.9265 & 0.9080 & 1.0245 & 1.0721 \\ \hline
        \textbf{T5} & 0.6750 & 1.0083 & 0.9356 & 1.0724 & 1.1877 & 0.7883 & 1.0531 & 1.2796 \\ \hline
        \textbf{T6} & 0.7458 & 0.7002 & 1.1509 & 0.9148 & 1.0355 & 1.1885 & 1.1348 & 1.1294 \\ \hline
        \textbf{T7} & 1.0700 & 0.8767 & 0.9778 & 0.9531 & 1.0992 & 0.9531 & 0.9621 & 1.1082 \\ \hline
        \textbf{T8} & 0.9347 & 1.0304 & 0.9375 & 0.8883 & 1.0878 & 1.0796 & 0.9101 & 1.1315 \\ \hline
        \textbf{T9} & 0.8380 & 1.1639 & 1.0658 & 1.1957 & 0.8012 & 0.7032 & 1.0977 & 1.1345 \\ \hline
        \textbf{T10} & 1.1335 & 1.0059 & 1.1573 & 1.2166 & 0.9911 & 0.5994 & 0.9614 & 0.9347 \\ \hline
        \textbf{T11} & 1.1040 & 1.1040 & 0.9189 & 1.0316 & 0.9976 & 0.9380 & 0.8870 & 1.0189 \\ \hline
        \textbf{T12} & 1.0772 & 0.9299 & 1.2296 & 1.0875 & 0.7724 & 0.9868 & 0.9816 & 0.9351 \\ \hline
        \textbf{T13} & 1.0257 & 0.9441 & 0.9996 & 1.1074 & 0.8559 & 0.9767 & 0.9996 & 1.0911 \\ \hline
        \textbf{T14} & 1.0449 & 0.8652 & 0.9012 & 0.9758 & 1.1776 & 1.0919 & 0.9261 & 1.0173 \\ \hline
        \textbf{T15} & 1.0793 & 0.9580 & 1.1122 & 1.1273 & 1.1299 & 0.9302 & 0.8291 & 0.8341 \\ \hline
        \textbf{T16} & 1.0584 & 1.0894 & 1.1868 & 1.1846 & 1.0650 & 0.8547 & 0.9366 & 0.6244 \\ \hline \hline
        \textbf{Mean} & 0.9564	& 0.9645	& 1.0483	& 1.0303	& 1.0275 & 	0.9316 & 1.0046	& 1.0370 \\ \hline
        \textbf{S.D.} & 0.1507	& 0.1234	& 0.1639	& 0.1121	& 0.1293	& 0.1513	& 0.1110	& 0.1593 \\ \hline
        \end{tabular}%
}
\end{table}

\subsection{AWAC Validation and Results} 

We performed the rmANOVA test to investigate the effects of the proposed AWAC on team performance by comparing it to Task A, D, F, and H. We used a Bonferroni correction for post-hoc analysis \cite{kirk1974experimental}. Fig. \ref{fig:drl_validation_exp} illustrates the team performance of Task A, D, F, and H. 

The rmANOVA analysis revealed a significant difference in the dependent variable among the groups, $F(3, 45)$ = 2.21, $p < .1$, with mean scores of 0.9564 for Task A, 0.9645 for Task D, 1.0483 for Task F, and 1.0303 for Task H (Table \ref{tab:all_anova_restuls}). Multiple comparison corrections using Bonferroni showed that the means of the following pairs were significantly different ($p < .1$): Task A \textit{vs.} F, Task A \textit{vs.} H, and Task D \textit{vs.} H. However, we did not find any significant difference between Task A and D ($p = 0.8663$), Task D and F ($p = 0.1138$), and Task F and H ($p = 0.6909$). 

\begin{table}[h!] 
    \centering
    \caption{Results of the rmANOVA test for the team performance scores obtained in Task A, D, F, and H.}
    \label{tab:all_anova_restuls}
    \resizebox{1\columnwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \rowcolor[HTML]{C0C0C0} 
        \textbf{Source} & \textbf{DF} & \textbf{Sum of Square} & \textbf{Mean Square} & \textbf{$F$ Statistic} & \textbf{$p$-value} \\ \hline \hline
        \textbf{Between groups} & 3 & 0.103	  & 0.034	 &  2.214 & 0.0995 \\ \hline
        \textbf{Within groups} & 60 & 1.161  & 0.019	 &  &  \\ \hline
        \textbf{Error} & 45 & 0.6955 & 0.016	 &  &  \\ \hline
        \end{tabular}%
    }
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}{1\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{ch5_figures/workload_validation_plot_ADFH.pdf}
        %\caption{The distribution of the team performance on the four tasks used on the validation of AWAC performance; Task A, D, F, and H.}
        \caption{}
        \label{fig:drl_validation_exp}
    \end{subfigure}
    \begin{subfigure}{1\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{ch5_figures/phase_score_ADFH.pdf}
        %\caption{A mean of the obtained team performance in each set (e.g., 1st set, 2nd set, and 3rd set) of Task A, D, F, and H.}
        \caption{}
        \label{fig:phase_score} 
    \end{subfigure}
    \caption{Results of the comparison experiment to validate the AWAC on four tasks (Task A, D, F, and H). (a) Distribution of team performance on the four tasks used to validate the AWAC performance, and (b) Mean team performance obtained in each set (1st set, 2nd set, and 3rd set) of the four tasks.}
    \label{fig:drl_validation}
\end{figure}

%We also found that the means of the pairs (Task A \textit{vs.} F, Task A \textit{vs.} H, and Task D \textit{vs.} H) are significantly different using multiple comparison correction ($p < .1$), while we observed that the there is no significant difference between Task A and D ($p = 0.8663$), between Task D and F ($p = 0.1138$), and between Task F and H ($p = 0.6909$). 

Based on the rmANOVA result, we conclude that Task F and H, which utilized the proposed AWAC, produced higher team performance scores than the baseline task (Task A) ($p < .1$). The mean mission score in Task A was 372.25, but the other three tasks using AWAC were 376.375, 403.875, and 402.125, respectively. 
Therefore, we confirmed the effectiveness of the proposed AWAC in improving team performance by adaptively allocating the operator's workload based on their affective state.
Given the similar team performance of Task A and Task D, we can assume that IS does not significantly affect the team performance compared to PS used in Task F and H.  

In order to more deeply investigate the performance of our AWAC, we analyzed the team performance obtained in each set of the task; each task in our user experiment has three sets as illustrated in Fig. \ref{fig:detail_user_experiment}. We calculated the team performance obtained in the 1st, 2nd, and 3rd sets, as illustrated in Fig. \ref{fig:phase_score}. We found that the team performance obtained in each set of tasks with our AWAC (Task D, F, and H) was higher than that of Task A without AWAC in the 2nd set of the missions ($p < .1$). However, the team performance of Task D, F, and H decreased in the next 3rd set compared to Task A, but there was no statistically significant difference, so we could not say whether the team performance increased or decreased compared to the previous set.

Thus, we can conclude that our AWAC plays a significant role in improving team performance. Additionally, we observed that the team performance of Task D suddenly dropped, which may imply that the subjective cognitive workload responses were inaccurate, resulting in our AWAC reallocating the wrong workloads to each operator. We also noticed that the performance of Task A increased in the transition from the 2nd set to the 3rd set, but the team performance remained lower than tasks with AWAC. This observation suggests that our AWAC can effectively maximize operator performance in a shorter time than other allocation methods. 


The subjective analysis of the effectiveness of the AWAC also supports the results of the objective analysis. We found that 64.93\% of participants agreed with the workload reallocation proposed by the AWAC. This agreement ratio was collected by asking participants to allow the transition of workloads in the tasks with AWAC (Task C, E, and G) during the experiment. Therefore, we can conclude that the performance of the AWAC is subjectively acceptable and effective for human operators compared to the baseline task without AWAC.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{ch5_figures/SAM_rmANOVA_CDEFGH.pdf}
    \caption{Valence ratings on the self-assessment manikin (SAM) for tasks applied to our AWAC.}
    \label{fig:sam_score} 
\end{figure}

As illustrated in Fig. \ref{fig:sam_score}, we also observed that the valence scores of the SAM questionnaire in the task with our AWAC are positive ($p < .1$). This means that our AWAC can positively influence human operator's emotional states by inducing positive valence (e.g., happy), thereby improving the productivity and effectiveness of the missions. This is consistent with the Yerke-Dodson law \cite{yerkes1908relation} and other research findings \cite{hsieh2019dissociable,du2020examining}. 


Fig. \ref{fig:p_test_ps} illustrates an example of workload reallocation recorded during the user experiment while participants were performing the CCTV monitoring task. The figure demonstrates that our AWAC successfully reallocated the workload for each participant based on their performance estimated from ISA scores and predicted cognitive workload. 

\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{1\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/exp_data_p_a.pdf}
        \caption{}
        \label{fig:p_test_ps_a}
    \end{subfigure}
    
    \begin{subfigure}[b]{1\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/exp_data_p_b.pdf}
        \caption{}
        %\label{fig:p_test_ps_b}
    \end{subfigure}
    
    \begin{subfigure}[b]{1\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/exp_data_p_c.pdf}
        \caption{}
        %\label{fig:p_test_ps_b}
    \end{subfigure}
    \caption{Examples of workload allocation transitions generated by AWAC during user experiments. (a) Operator A's data and (b) Operator B's data. Red indicates ISA scores, blue indicates team performance, green indicates the allocated camera number, and black indicates predicted cognitive workload, and (c) Team performance data, where the black line represents the team performance, red represents Operator A's performance, and blue represents Operator B's performance.}
    \label{fig:p_test_ps} % updated P8
\end{figure}



\subsection{Comparison of Workload Allocation Methods} 
Beyond validating the performance and effectiveness of the AWAC, we investigated the effects of various workload allocation methods on team performance for MH-MR teams. We designed three sessions as mentioned in Table \ref{tab:ch5_task_list}; IS, PS, and AS. The IS session measured the subjective cognitive workload of human operators using a five-point rating scale during the missions. The PS session predicted the cognitive workload of human operators using DL-based APM. The AS session asked the human operator to accept or reject the proposed workload transition. If the human operator rejected the workload change, the workload would not be altered.

To investigate the impact of the AWAC on team performance, we conducted an rmANOVA test and compared it to Task B, C, E, and G. We performed a post-hoc analysis using the Bonferroni correction. Fig. \ref{fig:workload_method_exp} presents the team performance of Task B, C, E, and G. The rmANOVA test showed a significant difference in the dependent variable among the groups, $F(3, 45)$ = 2.36, $p < .1$, with Task B having a mean of 1.0275, Task C with 0.9316, Task E with 1.0046, and Task G with 1.037 (Table \ref{tab:workload_anova_results}). 

\begin{figure}[t]
    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{ch5_figures/workload_validation_plot_BCEG.pdf}
        \caption{}
        \label{fig:workload_method_exp}
    \end{subfigure}

    \begin{subfigure}[b]{1\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{ch5_figures/workload_validation_plot_AB.pdf}
        \caption{}
        \label{fig:comparison_a_b}
    \end{subfigure}
    \caption{Distribution of team performance on four tasks used to compare workload allocation methods. (a) Tasks B, C, E, and G, and (b) Tasks A and B.}
    \label{fig:comparison_results}
\end{figure}



\begin{table}[h!] 
    \centering
    \caption{Results of the rmANOVA test for the team performance scores obtained in Task B, C, E, and G.}
    \label{tab:workload_anova_results}
    \resizebox{1\columnwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \rowcolor[HTML]{C0C0C0} 
        \textbf{Source} & \textbf{DF} & \textbf{Sum of Square} & \textbf{Mean Square} & \textbf{$F$ Statistic} & \textbf{$p$-value} \\ \hline \hline
        \textbf{Between groups} & 3 & 0.1092	  & 0.0364	 &  2.3588 & 0.0842 \\ \hline
        \textbf{Within groups} & 60 & 1.1596  & 0.0193	 &  &  \\ \hline
        \textbf{Error}          & 45 & 0.6946 & 0.0154	 &  &  \\ \hline
        \end{tabular}%
    }
\end{table}

Based on the results of the team performance analysis, it can be concluded that Task G, which utilized both IS and PS, had the highest team performance compared to other tasks (Task B, C, and E). It was also observed that the capability-based workload allocation (Task B) had better team performance than tasks that only used IS or only used PS. 
Therefore, it can be suggested that workload allocation methods that include AS should consider both subjective and objective cognitive workload (IS and PS) to optimize team performance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the tasks without AS (Task A, D, F, and H), we performed the rmANOVA test to compare the effects of the experimental conditions of the tasks on the team performance. Fig. \ref{fig:drl_validation_exp} shows the distribution of team performance on Task A, D, F, and H. The rmANOVA results showed that there was a statistically significant difference, which means that the difference between the averages of all four tasks is statistically significant ($F(3, 60)$=2.2139, $p < .1$). The mean of Task A is 0.9564, the mean of Task D is 0.9645, Task F is 1.048, and Task H is 1.03. Thus, we can conclude that workload allocation methods without AS should consider applying the objective operator's cognitive workload (e.g., PS) to maximize the team performance.


We performed an rmANOVA test to compare the effects of the experimental conditions of Task A and B on the team performance. Fig. \ref{fig:comparison_a_b} shows the distribution of team performance on both tasks.
The rmANOVA revealed no statistically significant difference between the two tasks ($F(1, 30)$=1.596, $p=0.2258$), indicating that the difference between their averages is not enough to be statistically significant. However, we observed that Task B resulted in higher team performance than Than A. 


\iffalse
\begin{figure*}[h!]
    \centering
     \begin{subfigure}[b]{0.33\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/interaction_AS_PS.pdf}
        \caption{}
        \label{fig:interaction_AS_PS}
    \end{subfigure}    
    \begin{subfigure}[b]{0.33\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/interaction_IS_PS.pdf}
        \caption{}
        \label{fig:interaction_IS_PS}
    \end{subfigure}    
    \begin{subfigure}[b]{0.33\linewidth}
        \centering 
        \includegraphics[width=1\linewidth]{ch5_figures/interaction_PS_AS.pdf}
        \caption{}
        \label{fig:interaction_PS_AS}
    \end{subfigure}
    
    \caption{Comparing team performance within two within-subject factors: (a) AS \textit{vs.} PS, (b) IS \textit{vs.} PS, and (c) PS \textit{vs.} AS.}
    \label{fig:interaction}
\end{figure*}
\fi
   

%We performed the two-way ANOVA test to investigate the interaction effects of IS, PS, and AS on team performance. Fig. \ref{fig:interaction} shows the team performance based on within-subject factors. The results of the ANOVA showed that there was no statistically significant difference. However, we observed that there are slight interaction effects. From Fig. \ref{fig:interaction_AS_PS}, the presence of AS does not affect the team performance. From Fig. \ref{fig:interaction_IS_PS}, the presence of IS is slightly related to the team performance. From Fig. \ref{fig:interaction_PS_AS}, the presence of PS is slightly related to the team performance.

%Therefore, we can conclude that AS is not related to the system performance, but IS and/or PS slightly influence the team performance. 

\subsection{Analysis of Subjective Questionnaires} 
After completing each task in the user experiment, participants were asked to rate their emotions, feelings, and cognitive workload using SAM, ISA, and NASA-TLX. We conducted rmANOVA tests on the results of the subjective questionnaires and found no statistically significant difference between tasks in all questionnaires at $p < .1$ ($F(7, 255)$ = 1.7411, $p = 0.3474$). However, we observed that Task G with PS and IS (\textit{Mean}=0.91, \textit{S.D.}=1.99) and Task E with PS  (\textit{Mean}=0.91, \textit{S.D.}=2.07) had a positive effects on the participant's emotions, especially valence, compared to the other tasks. The distribution of the valence values of the SAM questionnaire is illustrated in Fig. \ref{fig:sam_score}.


\subsection{Analysis of Post-Interview} 
%(e.g., ``Did you feel any difference between the eight sessions? Why?'') 
%(e.g., "Can you give us your opinions about the ISA SEssion and Approval session during the mission? '', 
%''Which is better to conduct this CCTV monitoring task with changing or fixed workloads?''). 
After completing all tasks, we conducted interviewed with the participants to gather feedback on our AWAC system. 72.43\% of the participants reported that IS and AS were helpful in conducting missions (``\textit{Personally, IS and AS were very helpful because I felt like I could lead the system} [P\_A of T4].'').
On the other hand, 62.5\% of participants (20 of 32 participants) preferred changing workload for the CCTV monitoring task rather than a fixed workload  (``\textit{It was so cool that this system automatically recommended a new workload based on our performance! Honestly, I am not sure how accurate the system is, but I could definitely feel that the recommended workload was better than sticking to a fixed workload. I felt that it improved my working ability} [P\_B of T10].''). 

During the post-interview, we found that some participants felt pressure from their teammates due to the mission scoreboard that was displayed on the CCTV monitoring program at the end of each task.
We observed some participants quickly closed the scoreboard program to avoid seeing their results (``\textit{Wow! You (teammate) did a good job. How to get the scores? Do you have any strategies to get more scores? Next time, I will get more score than you (teammate)} [P\_B of T3].''). 
However, the original objective of showing overall scores was to obtain confirmation from participants for the additional compensation. 
We observed that some participants felt sorry for getting lower mission scores compared to their teammates, especially if they were friends (``\textit{I am very sorry to my teammate. My personal score is lower than theirs (teammate)} [P\_A of T1].'')

Furthermore, some participants requested to pause the missions as they experienced headaches or discomfort caused by the EEG headset. This is a known issue when using wearable biosensors, as evidenced by one participant who said, (``\textit{Can you stop the experiment for a minute? I have a headache} [P\_A of T15].''). The participants were given a 5-minute break during which they removed their EEG headsets. After the break, their headaches disappeared, and they were able to continue with the remaining tasks in the experiment. 


\subsection{Summary of Findings}
We validated the performance of the AWAC and found statistically significant results. The tasks with AWAC achieved better team performance scores compared to tasks without AWAC, suggesting that reallocating the workload based on the operator's cognitive workload for the team mission has positive effects on team performance.  Among the four tasks (Task A, D, F, and H), the task with the PS achieved the best team performance, indicating that predicting the operator's cognitive workload may play an important role in workload allocation for MH-MR teams.

When AS was provided, having both the IS (i.e., subjective measurement) and the PS (i.e., objective cognitive workload measurement) achieved the best team performance. In addition, having the PS performed better than having the IS, suggesting that the objective measurement may be a better option for achieving better team performance. Furthermore, when the subjective and objective sessions were not provided, and only AS was given, the performance was better than when the subjective and objective measurements were provided separately. This suggests that workload allocation through consultation among team members can be more effective than workload allocation through subjective and objective measurements. However, as mentioned above, this consultation-based approach was not better than when all three sessions (AS, PS, and IS) were provided. In other words, the best performance in the CCTV monitoring task introduced in this study can be achieved by allocating the same workload in the beginning and soon after implementing the proposed AWAC with all three sessions. 


\section{Discussion}
\label{sec:discussion}
% AWAC
We presented an affective workload allocation controller (AWAC) that allocates optimal workloads to enhance team performance in controlling multi-robot systems using DRL. The AWAC estimated the operator's performance gased on the measured cognitive workloads of the operator and teammates and the HPM. It assigned suitable workloads accordingly, making the collaboration more efficient. We validated the AWAC in a generalized real-world team-based surveillance scenario and investigated the effectiveness of the workload allocation methods for MH-MR teams. 

% human performance model
However, it is possible to have generalization errors in the proposed HPM and the DL-based APM. The HPM aims to estimate human operators' mission performance using self-reported and predicted cognitive workload via the ISA and our DL prediction model, respectively.  
To develop the HPM, we utilized the ISA scores and predicted cognitive workloads of human subjects from the dataset we built in a previous study \cite{smartlab2022mocas}. From the dataset, we observed that the cognitive workloads differ depending on the number of camera views while performing the surveillance task, and there are positive correlations between NASA-TLX and ISA scores ($\gamma > 0.5$). Therefore, we assumed that most participants had similar cognitive workloads depending on the number of camera views. For instance, when performing our surveillance mission with one camera view, their cognitive workload is low, and if they use more camera views, the cognitive workload increases. We applied this knowledge to develop the HPM.

% DRL-based workload allocations
The DL-based prediction algorithm was also developed using the same dataset collected from the previous experiment \cite{smartlab2022mocas}. However, due to participants' facial masks, our facial feature extraction programs failed to extract some of the facial features (such as eye aspect ratio and action units) from the subjects. Therefore, we only used 70\% of the MOCAS dataset for training the deep-learning-based prediction algorithm. To compensate for the small size of the training dataset, we applied \textit{K}-fold cross-validation ($K$=5), which is a re-sampling technique that generates more data from a limited data sample. Thus, we assumed that the prediction results could represent the current human operator's cognitive workload while performing the CCTV monitoring task. 

% Sensibility
To ensure the sensibility of the proposed DRL-based APM, we added weights ($\alpha$ and $\beta$) to Eq. \ref{eq:ch05_all_performance}, which estimates human performance using both subjective and objective measurements of the cognitive workload. During our team-based user experiment, we defined $\alpha_{p}$ and $\beta_{p}$ as 0.5 each, which were decided based on our pilot test. However, we may need to adjust there weights depending on the accuracy of the DL-based prediction algorithm. If the prediction results of the algorithm are reliable and high accuracy, we may need to increase the weight of $\beta_{p}$ to allocate the workload appropriately based on the human operator's performance. 


We iteratively designed the MH-MR surveillance mission through pilot tests to rigorously validate the proposed workload allocation algorithm. During these tests, we observed that participants could potentially become more skilled in the CCTV monitoring tasks as they became accustomed to them. To mitigate this situation, we carefully designed the task order and altered the sequence of the camera views for each task to prevent participants from memorizing and predicting the location of abnormal objects in the camera views. 

% adding more variable (robot speed) 
In this research, we have only considered the number of the camera view to determine the operator's workload allocation for operators. This decision was made to focus on validating the effectiveness of monitoring human affective states on MH-MR team missions and finding the optimal workload allocation methods for MH-MR teams. However, we have identified another factor through previous experiments; which is that robot speed can also impact mission scores. In a previous study \cite{smartlab2022mocas}, we observed that the mission scores of participants were influenced by the robot speed ($p < .01$).  

Moreover, the proposed AWAC was only validated through a user experiment involving two human operators and six multi-robot platforms due to limitations in experiment space and equipment, raising concerns about its scalability. However, this issue can be addressed by expanding our DRL framework to include more human operator actions ($\mathcal{A}$) and states ($\mathcal{s}$) or multi-robot platforms related to the total workloads. It is important to note that the number of human operators should be less than the number of robot agents to effectively allocate workloads based on operators' performance for the mission. 
%Additionally, we only validated the AWAC on homogeneous missions and robot platforms, which limits its generalizability. This limitation can be addressed by adding a new weight to the AWAC based on the mission type and functions of the robot platforms, which can be determined through empirical experiments. 

% Comparing with traditional methods. 
Based on the results of our experiment, our proposed workload allocation method that considers both subjective and objective cognitive workload measurement (i.e., IS and PS) and incorporates the operator's opinion on workload transition (i.e., AS) led to better team performance compared to traditional workload allocation methods (Task A) ($p < .1$). 
The most common workload allocation method in our society is the capability-based workload allocation under the agreement, which allocates workloads based on the operator's preference, ability, skills, experience, and so on. Moacdieh \textit{et. al} \cite{moacdieh2020effects} found that the preference-based workload allocation method has benefits on workload allocation, while Wigboldus \textit{et. al} \cite{wigboldus2004capacity} proposed that capability-based workload has greater productivity and effectiveness. However, capability-based workload allocation methods are difficult to handle sudden changes in the operator's capability
during the mission, so \cite{shriram2013eeg, cabrera2014novel, mina2020adaptive} proposed utilizing objective cognitive workload measurement methods using physiological sensors (i.e., PS) to overcome this drawback. They found that cognitive workload measurement-based workload allocation methods outperformed traditional allocation methods, which was also observed in our study. Specifically, objective-measurement-based workload allocation (Task E) outperformed capability-based workload allocation methods (Task C). Furthermore, we found that our proposed allocation methods using both IS and PS (Task G) outperformed traditional workload allocation methods (Task B and C) and objective cognitive workload measurement-based workload allocation methods (Task E).

% b. Describe limited experiments. We validated the proposed system by taking only CCTV monitoring as an example, and describe why this experiment is sufficient and, if not, what additional experiments should be performed. Additionally, other possible scenarios are also described.
To validate our AWAC, we adapted a surveillance scenario, which is typical in real-world human-agent systems. In this scenario a human subject undertakes a simultaneous CCTV monitoring task while multiple sensors track the environment. 
Therefore, our system can be widely utilized in various multi-human-multi-agents system task scenarios, such as security monitoring \cite{muller2008machine, wang2008affective}, air traffic management \cite{hoc1998cognitive, pacaux2002common, giraudet2015neuroergonomic}, and performance checking \cite{liu2006queueing, stowers2017framework}. 
%Although the currently proposed AWAC mainly considers a homogeneous team and task scenario, it can handle heterogeneous team and different tasks, by modifying the workload amount based on robot hardware specifications and functions. 
Additionally, we only validated the AWAC on homogeneous missions and robot platforms, which limits its generalizability. This limitation can be addressed by adding a new weight to the AWAC based on the mission type and functions of the robot platforms, which can be determined through empirical experiments. 



  



\section{Conclusion}
\label{sec:conclusion}
We introduced the deep reinforcement learning-based affective workload allocation controller (AWAC) that enables human operators to perform better with teammates and multi-robot systems. The AWAC intelligently assigns appropriate workloads based on individual and team performance metrics, which are calculated using a combination of subjective cognitive workload reported through the instantaneous self-assessment method and objective cognitive workload predicted by our DL-based affective physiological model. We evaluated the effectiveness of our AWAC and human performance model through team-based user experiments on a CCTV surveillance scenario involving multi-human and multi-robot teams. We used rmANOVA to analyze the mission scores obtained during the experiments, which showed that our proposed DRL-based AWAC resulted in better team performance ($p < .1$). This indicates that workload allocation based on human operators' cognitive workload is critical to improving team performance. Furthermore, we compared different workload allocation strategies for MH-MR teams in the experiment, and found that allocating workload by consulting with team members is a more efficient method than relying solely on our AWAC that reflects subjective and objective cognitive workloads. 


\section*{Acknowledgment}
This research was supported by the National Science Foundation under Grant No. IIS-1846221. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

We specially thank Revanth Krishna Senthilkumaran and Go-Eum Cha for their invaluable assistance in conducting the user experiments for this research. 


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

\bibliography{all-biblatex}
\bibliographystyle{IEEEtran}

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% biography section

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bio_Figures/Wonse_Jo.png}}]{Wonse Jo} received the B.S. in robotics engineering from Hoseo University, South Korea in 2013 and M.S. degrees in electronic engineering from the Kyung-Hee University, South Korea, in 2015. He is currently pursuing the Ph.D. degree in computer and information technology at Purdue University, West Lafayette, IN, USA. His research interests include affective robotics/computing, human multi-robot interaction, environmental robotics, and field robotics.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bio_Figures/Ruiqi_Wang.jpg}}]{Ruiqi Wang} (Student Member, IEEE) received a B.E. degree in robotics engineering from Beijing University of Chemical Technology, Beijing, China, in 2020. He is currently working towards a Ph.D. degree in the Department of Computer and Information Technology at Purdue University, West Lafayette, IN, USA. His research interests include human-robot interaction, affective robotics, and human-in-the-loop robot learning.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bio_Figures/prof_Baijian.jpg}}]{Baijian Yang} (Member, IEEE) received his Ph.D. in Computer Science from Michigan State University, and his MS and BS in Automation (EECS) from Tsinghua University. He is currently a Professor at the Department of Computer and Information Technology, Purdue University. He served as a steering committee member of IEEE Cybersecurity Initiative between 2015 and 2017, and served as a board director of ATMAE from 2014-2016. His research interests include cybersecurity, data-driven security analytics, and applied machine learning. 

In addition to his rich academic background, he holds several industry certifications, such as CISSP, MCSE, and Six Sigma Black Belt and has published two books on Windows Phone Programming. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bio_Figures/MoRastgaar-1.jpg}}]{Mo Rastgaar} (Senior Member, IEEE) received the B.S. degree from the Sharif University of Technology, Tehran, Iran, in 1995; the M.S. degree from Tehran Polytechnic, Iran, in 1998; and the Ph.D. degree from Virginia Polytechnic Institute and State University, Blacksburg, VA, USA, in 2008, all in mechanical engineering. 
He was a Post-Doctoral Associate in the Newman Laboratory for Biomechanics and Human Rehabilitation, Massachusetts Institute of Technology, Cambridge, MA, USA from 2008- to 2010. From 2011 to 2018, he was an assistant professor and associate professor at Michigan Tech, Houghton, MI, USA. In 2019, he joined Purdue University, West Lafayette, IN, USA, where he is currently an Associate Professor at Polytechnic Institute and the Director of the Human-Interactive Robotics Lab. 

He is a recipient of the 2014 National Science Foundation CAREER Award. His research focuses on advancing maneuverability in lower extremity robotic prostheses and assistive robots by characterizing the agility in the human gait. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bio_Figures/CURRENT-Dan-Foti.jpg}}]{Daniel Foti} received a B.A. degree in Biomedical Engineering from Harvard University in 2006. He completed his graduate studies at Stony Brook University, receiving his M.A. in Psychology in 2008 and his Ph.D. in Clinical Psychology in 2013. He completed his predoctoral clinical internship at McLean Hospital in Belmont, MA. He joined the faculty in the Department of Psychological Sciences at Purdue University as an Assistant Professor in 2013. He was promoted to Associate Professor with tenure in 2018. His research interests include using psychophysiological measures to study cognition, emotion, and reward, as well as abnormalities in these processes that are associated with vulnerability to psychiatric illnesses.

He was a recipient of the James C. Naylor Award for Teaching Excellence in Psychology in 2019. He was named Director of the T32 Predoctoral Training Program through the Indiana Clinical and Translational Sciences Institute in 2021. He was elected to the Board of Directors for the Society for Psychophysiological Research in 2021, and he was elected Treasurer in 2022.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bio_Figures/Byung-Cheol_Min.jpg}}]{Byung-Cheol Min} (Member, IEEE) received a B.S. degree in electronics engineering and a M.S. degree in electronics and radio engineering from Kyung Hee University, Yongin, South Korea, in 2008 and 2010, respectively, and a Ph.D. degree in technology with a specialization in robotics from Purdue University, West Lafayette, IN, USA, in 2014. He is currently an Associate Professor and University Faculty Scholar with the Department of Computer and Information Technology and the Director of the SMART Laboratory, Purdue University. Prior to this position, he was a Postdoctoral Fellow with the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA. His research interests include multi-robot systems, human–robot interaction, robot design and control, with applications in field robotics, and assistive technology and robotics.

He was a recipient of the NSF CAREER Award, in 2019; the Purdue PPI Outstanding Faculty in Discovery Award, in 2019; the Purdue CIT Outstanding Graduate Mentor Award, in 2019; the Purdue Focus Award, in 2020; the Purdue PPI Interdisciplinary Research Collaboration Award, in 2021; the Purdue Corps of Engagement Award, in 2022. He was named a Purdue University Faculty Scholar, in 2021.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

% that's all folks
\end{document}


