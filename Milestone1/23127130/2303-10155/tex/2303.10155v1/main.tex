\documentclass[11pt,reqno]{amsart}

\usepackage{geometry}
\geometry{margin=1.5in}

\usepackage{graphicx} % Required for inserting images

\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{bm}
\usepackage{bbm}
\usepackage{cite}
\usepackage{psfrag}
\usepackage{cite}
\usepackage{color,soul}
%\usepackage{dsfont}
%\usepackage{ulem}
\usepackage{breakcites}     % Break long citations over multiple lines
\usepackage{bigints}

\usepackage{enumitem}
\setlist{leftmargin=1.6em}

\usepackage[hidelinks]{hyperref} % add hyperlinks to cross-referenced elements
\hypersetup{colorlinks=true,linkcolor=red,citecolor=blue}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma} \def\lemref#1{Lemma~\ref{#1}}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\theoremstyle{remark}
\newtheorem{remark}{Remark}


\usepackage{graphicx}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\newcommand{\algrule}[1][.2pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}


\makeatletter
\def\subsubsection{\@startsection{subsubsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{-.5em}%
  {\normalfont\bfseries}}
\makeatother

\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\EOT}{\mathsf{S}_c^{\mspace{1mu}\varepsilon}}
\newcommand{\EOTq}{\mathsf{S}_{\|\cdot\|^2}^{\mspace{1mu}1}}


\newcommand{\vc}[0]{\bm{c}}
\newcommand{\vh}[0]{\bm {h}}
\newcommand{\vn}[0]{\bm {n}}
\newcommand{\vp}[0]{\bm {p}}
\newcommand{\vv}[0]{\bm {v}}
\newcommand{\vx}[0]{\bm {x}}
\newcommand{\vy}[0]{\bm {y}}
\newcommand{\vz}[0]{\bm {z}}
\newcommand{\vV}[0]{\bm {V}}
\newcommand{\vW}[0]{\bm {W}}
\newcommand{\vX}[0]{\bm {X}}
\newcommand{\vY}[0]{\bm {Y}}
\newcommand{\vZ}[0]{\bm {Z}}
\newcommand{\vT}[0]{\bm{T}}
\newcommand{\vxbar}[0]{\overline{\bm {x}}}
\newcommand{\vzero}[0]{\bm {0}}
\newcommand{\vone}[0]{\bm {1}}
\newcommand{\mbf}[1]{\mbox{\boldmath$#1$}}
\newcommand{\Cov}[0]{\mathrm{Cov}}
\newcommand{\Var}[0]{\mathrm{Var}}
\newcommand{\med}[0]{\mathrm{med}}
\newcommand{\iid}[0]{\overset{iid}{\sim}}
\newcommand{\pen}[0]{\mathrm{pen}}
\newcommand{\diag}[0]{\mathrm{diag}}
\newcommand{\sign}[0]{\mathrm{sign}}
\newcommand{\Tr}[0]{\mathrm{tr}}
\newcommand{\trinorm}{\ensuremath{| \! | \! |}}
\newcommand{\ind}[0]{\mathbbm{1}}
\newcommand{\Span}{\text{Span}}
\newcommand{\calA}[0]{\mathcal{A}}
\newcommand{\calB}[0]{\mathcal{B}}
\newcommand{\calC}[0]{\mathcal{C}}
\newcommand{\calF}[0]{\mathcal{F}}
\newcommand{\calG}[0]{\mathcal{G}}
\newcommand{\calH}[0]{\mathcal{H}}
\newcommand{\calK}[0]{\mathcal{K}}
\newcommand{\calM}[0]{\mathcal{M}}
\newcommand{\calP}[0]{\mathcal{P}}
\newcommand{\calS}[0]{\mathcal{S}}
\newcommand{\calT}[0]{\mathcal{T}}
\newcommand{\calX}[0]{\mathcal{X}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calY}[0]{\mathcal{Y}}


\newcommand{\supp}{\mathrm{spt}}
%KK I prefer using spt for support. int(supp) looks a bit long
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\G}[0]{\mathbb{G}}
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\U}[0]{\mathbb{U}}
\newcommand{\V}[0]{\mathbb{V}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\normal}[0]{N}

\newcommand{\Prob}[0]{\mathbb{P}}
\newcommand{\I}[0]{\mathbb{I}}
\newcommand{\Cross}{\mathbin{\tikz [x=1.6ex,y=1.6ex,line width=.15ex] \draw (0,0) -- (1,1) (0,1) -- (1,0);}}%

\newcommand{\Id}[0]{\text{Id}}
\newcommand{\vxi}[0]{\mbf \xi}
\newcommand{\hiota}[0]{h_{(\iota)}}
\newcommand{\fiota}[0]{f_{(\iota)}}
\newcommand{\vech}[0]{\text{vech}}
\newcommand{\MAX}[0]{\text{MAX}}
\newcommand{\mn}[0]{\text{multinomial}}
\newcommand{\ub}[0]{\underline{b}}
\renewcommand{\vec}[0]{\text{vec}}

\newcommand{\Hmt}[1]{\dot H^{-1,2}\left( #1 \right)}
\newcommand{\Ht}[1]{\dot H^{1,2}\left( #1 \right)}
\newcommand{\dx}[1]{{d}#1}

%\renewcommand{\tilde}{\widetilde}
%\renewcommand{\hat}{\widehat}
%\newcommand{\qed}{\hfill \ensuremath{\blacksquare}}
%\renewcommand{\qedsymbol}{$\blacksquare$}





\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Special Letters
\newcommand{\sF}{\mathsf{F}}
\newcommand{\sS}{\mathsf{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\FSG}{\mathcal{F}^{(\mathsf{SG})}_{d,K}}
\newcommand{\BB}{\mathbb{B}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\DD}{\D}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\MM}{\mathbb{M}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\OO}{\mathbb{O}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\UU}{\mathbb{U}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\WW}{\mathbb{W}}
\newcommand{\XX}{\cmatbb{X}}
\newcommand{\YY}{\mathbb{Y}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand*{\dd}{\, \mathsf{d}}
\newcommand{\vasti}{\bBigg@{3.5 }}
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{5}}
\newcommand{\Vastt}{\bBigg@{7}}

\makeatother

% Math mode shorthands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{align}}
\newcommand{\ea}{\end{align}}
\newcommand{\baa}{\begin{align*}}
\newcommand{\eaa}{\end{align*}}

% Math mode commands
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\newcommand{\essinf}[0]{\text{essinf}}

% Gaussian Notation
\newcommand*{\Noise}{\mathcal{G}_\sigma}
\newcommand*{\noise}{g_\sigma}
\newcommand*{\noised}{\tilde{g}_\sigma}
\newcommand*{\gauss}{\varphi_\sigma}
\newcommand*{\gaussd}{\tilde{\varphi}_\sigma}
\newcommand*{\gausss}{\varphi_{\frac{\sigma}{\sqrt{2}}}}
\newcommand*{\gaussI}{\varphi}
\newcommand*{\gausssI}{\varphi_{\frac{1}{\sqrt{2}}}}
\newcommand*{\Gaussk}{\mathcal{N}_{\sigma_k}}
\newcommand*{\Gauss}{\mathcal{N}_\sigma}
\newcommand*{\Gaussone}{\mathcal{N}_{\sigma_1}}
\newcommand*{\Gausstwo}{\mathcal{N}_{\sigma_2}}
\newcommand*{\Gaussmix}{\mathcal{N}_{\sqrt{\sigma_2^2-\sigma_1^2}}}
\newcommand*{\Gausss}{\mathcal{N}_{\frac{\sigma}{\sqrt{2}}}}
\newcommand{\p}[1]{\left(#1\right)}
\newcommand{\s}[1]{\left[#1\right]}
\newcommand{\cp}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\nmc}{n_{\mathsf{MC}}}
\newcommand{\Chi}{\rchi^2}
\newcommand{\wass}{\mathsf{W}_1}
\newcommand{\Wp}{\mathsf{W}_p}
\newcommand{\GWp}{\mathsf{W}_p^{(\gamma^\sigma)}}
\newcommand{\GWo}{\mathsf{W}_1^{(\gamma^\sigma)}}
\newcommand{\entwass}{\mathsf{S}_\varepsilon}
\newcommand{\kr}[1]{\left\|#1\right\|_{\mathsf{KR}}}
\newcommand{\krs}[1]{\left\|#1\right\|_{\mathsf{KR}-\sigma}}
\newcommand{\lip}[1]{\big\|#1\big\|_{\mathsf{Lip}_{1,0}}}
\newcommand{\Lip}{\mathsf{Lip}_{1,0}}
\newcommand{\ip}[1]{\left\langle#1\right\rangle}
\newcommand{\erf}{\mathsf{erf}}

% Sliced distance Notation
\newcommand{\SWp}{\underline{\mathsf{W}}_p}
\newcommand{\SWone}{\underline{\mathsf{W}}_1}
\newcommand{\SWtwo}{\underline{\mathsf{SW}}_2}
\newcommand{\MSWp}{\overline{\mathsf{W}}_p}
\newcommand{\MSWone}{\overline{\mathsf{W}}_1}
\newcommand{\MSWtwo}{\overline{\mathsf{W}}_2}
\newcommand{\unitsph}{\mathbb{S}^{d-1}}
\newcommand{\proj}{\mathfrak{p}}
\newcommand{\ptheta}{\mathfrak{p}^\theta}
\newcommand{\WMC}{\widehat{\underline{\mathsf{W}}}_\mathsf{MC}^p}

% Probability Notation
\newcommand{\Pas}{P^{\otimes n}}
\newcommand{\empmu}{\hat{\mu}_n}
\newcommand{\empnu}{\hat{\nu}_n}
\newcommand{\emppi}{\hat{\pi}_n}
\newcommand{\prodmu}{\mu^{\otimes n}}
\newcommand{\prodnu}{\nu^{\otimes n}}
\newcommand{\dconv}{\stackrel{d}{\to}}
\newcommand{\pconv}{\overset{\PP}{\to}}
\newcommand{\asconv}{\overset{a.s.}{\to}}
\newcommand{\law}{\cL}

%Kernel notation
\newcommand{\Kni}{K \left ( \frac{X_i - x}{h_n} \right )}

\newcommand{\zg}[1] {{\color{red}ZG: [#1]}}
\newcommand{\kk}[1] {{\color{black!30!green}KK: [#1]}}
\newcommand{\rs}[1] {{\color{magenta}RS: [#1]}}
\newcommand{\gr}[1] {{\color{orange}GR: [#1]}}


% Additional special commands
\newcommand{\eest}{\hat{T}_{n,\epsilon}^x{}}

\DeclareMathOperator{\Card}{Card}
\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\pre}{pre}
\DeclareMathOperator{\esssup}{esssup}
\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\dist}{dist}

\begin{document}

\title[Limit Theorems for semidiscrete OT maps]{Limit Theorems for semidiscrete\\
optimal transport maps}

\thanks{
Z. Goldfeld is partially supported by NSF grants  CCF-2046018, and DMS-2210368, and the IBM Academic Award.
K. Kato is partially supported by NSF grants DMS-1952306, DMS-2014636, and DMS-2210368.}


\author[R. Sadhu]{Ritwik Sadhu}
\address[R. Sadhu]{
Department of Statistics and Data Science, Cornell University.
}
\email{rs2526@cornell.edu}

\author[Z. Goldfeld]{Ziv Goldfeld}
\address[Z. Goldfeld]{
School of Electrical and Computer Engineering, Cornell University.
}
\email{goldfeld@cornell.edu}

\author[K. Kato]{Kengo Kato}
\address[K. Kato]{
Department of Statistics and Data Science, Cornell University.
}
\email{kk976@cornell.edu}






\begin{abstract}
We study statistical inference for the optimal transport (OT) map (also known as the Brenier map) from a known absolutely continuous reference distribution onto an unknown finitely discrete target distribution. We derive limit distributions for the $L^p$-estimation error with arbitrary $p \in [1,\infty)$ and for linear functionals of the empirical OT map. The former has a non-Gaussian limit, while the latter attains asymptotic normality. %has a centered Gaussian limit. 
For both cases, we also establish consistency of the nonparametric bootstrap. The derivation of our limit theorems relies on new stability estimates of functionals of the OT map with respect to the dual potential vector, which could be of independent interest.  
\end{abstract}

\keywords{Bootstrap, functional delta method, Hadamard directional derivative, limit distribution, optimal transport map, semidiscrete optimal transport}

\date{First version: March 15, 2023. This version: \today.}

\maketitle

%\kk{To Ritwik and Ziv: I added comments on computational aspects of semidiscrete OT at the bottom of Sec 2. There are a few review articles on that and I borrowed some wording from them. Also I made another pass on the main text and proofs, and it seems that the manuscript is ready. Let me know what you think. 3/16.}

\section{Introduction}

\subsection{Overview}

Optimal transport (OT) provides a versatile framework to compare probability measures and has seen a surge of applications in statistics, machine learning, and applied mathematics; see, e.g., \cite{panaretos2020invitation,peyre2019computational,santambrogio2017euclidean}, among many others. We refer to \cite{ambrosio2005,villani2008optimal,santambrogio15} as standard references on OT. For Borel probability measures $R$ and $P$ on $\R^d$ with finite second moments, the Kantorovich OT problem with quadratic cost reads as
\begin{equation}
\min_{\pi \in \Pi (R,P)}  \int \frac{1}{2} \| \vy-\vx \|^2 \, d \pi(\vy,\vx),
\label{eq: Kantorovich}
\end{equation}
where $\Pi (R,P)$ denotes the collection of couplings of $R$ and $P$ (i.e., each $\pi \in \Pi(R,P)$ is a probability measure on the product space with marginals $R$ and $P$). 
A central object of interest in OT theory is the OT map, or the Brenier map \cite{brenier1991polar}. 
For absolutely continuous $R$, the unique optimal solution $\pi^*$ to \eqref{eq: Kantorovich} exists and  concentrates on the graph of a deterministic map $T^*$, called the OT map. The OT map has been applied to transfer learning and domain adaptation, among many others. Also, the OT map can be seen as a multivariate extension of the quantile function \cite{carlier2016vector,chernozhukov2017monge, ghosal2019multivariate,
hallin2021distribution} and was recently applied to causal inference \cite{torous2021optimal}. %The recent work \cite{torous2021optimal} discusses an intriguing application of the OT map to causal inference. 

Motivated by these applications, the statistical analysis of the OT map has seen increased interest, mostly focusing on %extremely active research, especially on 
estimation error rates \cite{chernozhukov2017monge, hutter2021minimax, deb2021rates, manole2021plugin, pooladian2021entropic, pooladian2022debiaser, divol2022optimal, pooladian2023minimax}; see a literature review below for a further discussion on these references. Still, there is much to be desired on statistical \textit{inference} for the OT map, such as valid testing and the construction of confidence sets, both of which hinge on a limit distribution theory. Alas, to the best of our knowledge, there have been no limit distribution results for the OT map, except for the $d=1$ case where the OT map agrees with the composition of the quantile and distribution functions (cf. \cite[Chapter 2]{santambrogio15}). 

The present paper tackles this challenge and derives several limit distribution results for the empirical OT map under the \textit{semidiscrete} setting. Semidiscrete OT refers to the case where the input distribution $R$ is absolutely continuous while the target $P$ is (finitely) discrete. For known reference measure $R$, which is natural when viewing the OT map as a vector quantile function, we derive limit distributions for the $L^p$-estimation errors $\| \hat{\vT}_n-\vT^* \|_{L^p(R)}^p$ with arbitrary $p \in [1,\infty)$ and for linear functionals of the form $\langle \bm{\varphi},\hat{\vT}_n \rangle_{L^2(R)} = \int \langle \bm{\varphi}(\vy),\hat{\vT}_n(\vy) \rangle \, dR(\vy)$. Here $\bm{\varphi}$ is a bounded Borel vector field, and $\hat{\vT}_n$ is the empirical OT map transporting $R$ onto the empirical distribution. 
The limit distribution for the $L^p$-estimation error is non-Gaussian, while that for the linear functional is a centered Gaussian. For both cases, we also establish the consistency of the nonparametric bootstrap. 

These statistical results enable us to perform various inference tasks for the OT map, such as the construction of $L^p$-confidence sets for $\vT^*$ and confidence intervals for $\langle \bm{\varphi},\vT^*\rangle_{L^2(R)}$.
One drawback of $L^p$-confidence sets is that they are difficult to visualize compared to $L^\infty$-confidence bands. To address this, we discuss a method to construct a confidence band derived from an $L^p$-confidence set that satisfies a certain relaxed coverage guarantee (cf. \cite[Section 5.8]{wasserman2006all}). 

It is important to note that the OT map in the semidiscrete case is a discrete mapping that is piecewise constant over the partition of $\R^d$ defined by the \textit{Laguerre cells} \cite{aurenhammer1987power,aurenhammer1998minkowski,kitagawa2019convergence}. This means that no nondegenerate \textit{pointwise} limit distribution exists for the empirical OT map. Nevertheless, this observation does not contradict our limit theorems since both functionals aggregate the contributions of the empirical OT map near the boundaries of the Laguerre cells, which add up to nontrivial weak limits. 

The proof of our limit theorems relies on new stability estimates of the OT map with respect to the dual potential vector--this result could be of independent interest. Specifically, we show that the $L^p$-error functional is Hadamard \textit{directionally} differentiable with a nonlinear derivative as a mapping of the dual potential vector. For the linear functional, we establish (Frech\'{e}t) differentiability and characterize the derivative. The derivation of these stability estimates, which relies on the careful analysis of the facial structures of the Laguerre cells, is the main technical contribution of the present paper. Given the differentiability results, the limit distributions follow by combining the extended delta method \cite{romisch2004} and a central limit theorem for the empirical dual potential vector recently derived in \cite{del2022central}.
The bootstrap consistency readily follows for the linear functional, as it is Frech\'{e}t differentiable with respect to the dual potential vector. For the $L^p$-error functional, while the derivative is nonlinear, the bootstrap is still shown to be consistent, thanks to the specific structure of the derivative; see the discussion after Proposition \ref{prop: bootstrap consistency}.

\subsection{Literature review}

The literature on statistical OT has rapidly expanded in recent years, so we confine ourselves to discussing only references directly related to OT map estimation and semidiscrete OT.
The literature on OT map estimation has mostly focused on continuous targets. \cite{chernozhukov2017monge} propose viewing the OT map as a multivariate extension of the quantile function and establish local uniform consistency of the empirical OT map; see also \cite{carlier2016vector,ghosal2019multivariate,hallin2021distribution}. \cite{hutter2021minimax} derive minimax rates for estimating the OT map and analyze wavelet-based estimators. For continuous targets, the results of \cite{hutter2021minimax} suggest that the minimax rate (for the $L^2$-loss) would be $n^{-1/d}$ when no further smoothness assumptions are imposed on the dual potentials (although the $n^{-1/d}$-minimax rate is formally a conjecture). Thus, estimation of the OT map suffers from the `curse of dimensionality', similarly to the OT cost itself \cite{dudley1969speed,fournier2015rate,weed2019sharp,niles2019estimation}. 
See also \cite{deb2021rates,manole2021plugin,pooladian2021entropic,divol2022optimal} for other contributions to the empirical estimation of the OT map. None of the above references contain limit distribution results for the OT map. 

An object related to the OT map is an entropic OT (EOT) map \cite{pooladian2021entropic}. It is defined by the barycentric projection of the optimal coupling for the entropically regularized OT problem \cite{cuturi2013lightspeed,peyre2019computational}. The EOT map approximates the standard OT map as the regularization parameter approaches zero \cite{mikami2004monge,carlier2022convergence}. For a \textit{fixed} regularization parameter, \cite{gonzalez2022weak,rigollet2022sample,gonzalez2022weak,goldfeld2022EOT} derive the parametric convergence rate for the empirical EOT map. Furthermore,  \cite{goldfeld2022EOT}  establish a limiting Gaussian distribution, bootstrap consistency, and asymptotic efficiency for the empirical EOT map; see also \cite{gonzalez2022weak}.  However, the results of \cite{goldfeld2022EOT} and \cite{gonzalez2022weak} do not extend to vanishing regularization parameters and hence do not cover standard OT map estimation.  

For semidiscrete OT, \cite{kitagawa2019convergence} and \cite{bansil2022quantitative} derive several important structural results, including regularity of the dual objective function.  In the statistics literature, there are two recent papers related to ours. The first is \cite{del2022central}, which derives limit distributions for the OT cost and dual potential vector in the semidiscrete setting. The derivation of our limit theorems builds on their work, but as noted before, the bulk of our effort is devoted to establishing (directional) derivatives of the $L^p$-error and linear functionals of the OT map with respect to the dual potential vector; these are not covered by \cite{del2022central} and require substantial work.\footnote{The most recent update of \cite{del2022central} available at \texttt{https://hal.science/hal-03232450v3} added a remark (Remark 4.9) showing that $\sqrt{n}\| \hat{\vT}_n - \vT^* \|_{L^2(R)}^2$ is stochastically bounded, but leaves the problem of finding weak limits of $\hat{\vT}_n-\vT^*$ open.  }
The second related work is \cite{pooladian2023minimax}, which shows that empirical EOT maps with vanishing regularization parameters achieve the parametric convergence rate toward the standard OT map (under the squared $L^2$-loss) in the semidiscrete setting, showing that OT map estimation in the semidiscrete case is free of the curse of dimensionality. No limit distribution results are derived in that paper. %for the However, \cite{pooladian2023minimax} does not contain limit distribution results for their estimators. 



\subsection{Organization}
The rest of this paper is organized as follows. 
In Section \ref{sec: preliminaries}, we present background material of semidiscrete OT. 
Section \ref{sec: main} collects the main results.
We first discuss assumptions, derive key stability estimates for the functionals of the OT maps, and then present the limit theorems. All the proofs are gathered in Section \ref{sec: proofs}. Section \ref{sec: conclusion} leaves some concluding remarks. Appendix \ref{sec: functional delta} contains a brief review of Hadamard differentiability and the extended delta method. 


\subsection{Notation}
For $a,b \in \R$, we use the notation $a \vee b = \max \{a,b \}$ and $a \wedge b = \min \{ a,b \}$.
We use $\| \cdot \|$ and $\langle \cdot, \cdot \rangle$ to denote the Euclidean norm and inner product, respectively, while $\| \cdot \|_{\infty}$ designates the sup norm for functions. Vectors are written in boldface letters. Let $\vzero$ and $\vone$ denote the vectors of all zeros and ones, respectively; their dimensions should be understood from the context. Let $\stackrel{d}{\to}$ denote convergence in distribution. For a subset $A$ of Euclidean space, the boundary and interior are denoted by $\partial A$ and $\inte (A)$, respectively. Also, define $\dist (\vy,A) := \inf \{ \| \vy-\vy' \| : \vy' \in A \}$. For two sets $A$ and $B$, $A \Delta B = (A \setminus B) \cup (B \setminus A)$ denotes their symmetric difference.
For $d \in \NN$ and $0 \le r \le d$, $\cH^{r}$ denotes the $r$-dimensional Hausdorff measure on $\R^d$; see \cite{evans1991measure} for a textbook treatment of Hausdorff measures.



\section{Preliminaries}
\label{sec: preliminaries}
We consider a semidiscrete OT problem under quadratic cost. Let $R$ be a Borel probability measure on $\R^d$ with compact support and $P$ be a finitely discrete distribution on $\R^d$ with support $\cX = \{ \vx_1,\dots,\vx_N \}$, where $\vx_1,\dots,\vx_N$ are all distinct. Consider the Kantorovich problem (\ref{eq: Kantorovich}). 
 Assuming that $R$ is absolutely continuous with respect to the Lebesgue measure, Brenier's theorem \cite{brenier1991polar} yields that the Kantorovich problem (\ref{eq: Kantorovich}) admits a unique optimal solution (coupling) $\pi^*$. Furthermore, the coupling $\pi^*$ is induced by an $R$-a.s. unique map $\vT^*: \R^d \to \cX$, i.e.,  $\pi^*$ agrees with the joint law of $(\vY,\vT^*(\vY))$ for $\vY \sim R$. We shall refer to $\vT^*$ as the \textit{OT map} transporting $R$ onto $P$. 

By duality theory for OT problems (cf. \cite{ambrosio2005, villani2008optimal,santambrogio15}), the OT map $\vT^*$ can be characterized via the dual problem
\begin{equation}
\label{eq: semi_dual}
\max_{\vz\in\R^N} \, \langle \vz,\vp \rangle + \int \min_{1 \leq i\leq N} \left ( \frac{1}{2} \| \vy-\vx_i \|^2 - z_i\right )\,dR(\vy),
\end{equation}
where $\vp = (p_1,\dots,p_N)^{\intercal} := (P(\{ \vx_1 \}),\dots,P(\{\vx_N\}))^{\intercal}$ is the simplex (frequency) vector corresponding to $P$. 
The dual problem (\ref{eq: semi_dual}) admits an optimal solution $\vz^*$, which we call a \textit{dual potential vector}. Then, the OT map $\vT^*$ can be expressed as 
\[
\begin{split}
\vT^*(\vy) &= \vy - \nabla_{\vy}\left [\min_{1 \le i \le N} \left ( \frac{1}{2} \|\vy-\vx_i\|^2 - z_i^* \right ) \right ]\\
&=\argmin_{\vx_i: 1 \le i \le N} \left ( \frac{1}{2} \|\vy-\vx_i\|^2 - z_i^* \right )
\end{split}
\]
for $Q$-a.e. $\vy$. (For $Q$-a.e. $\vy$, the argmin on the right-hand side is a singleton.)

For $\vz \in \R^N$, define the \textit{Laguerre cells} $\{ C_i(\vz) \}_{i=1}^N$ as 
\[
\begin{split}
  C_i(\vz) &:= \bigcap_{\substack{j \ne i \\ 1 \le j \le N}} \left \{\vy \in \R^d : \frac{1}{2}\|\vx_i - \vy\|^2 
 - z_i \leq \frac{1}{2} \|\vx_j - \vy\|^2 - z_j\right \} \\
 &= \bigcap_{\substack{j \ne i \\ 1 \le j \le N}}\big\{\vy \in \R^d: \langle \vx_i-\vx_j,\vy \rangle \geq b_{ij} (\vz) \big \},
\end{split}
\]
with $b_{ij}(\vz) :=  b_{ij}(z_i,z_j) := (\|\vx_i\|^2 - \|\vx_j\|^2)/2 -z_i + z_j$. Each Laguerre cell $C_i(\vy)$ is a polyhedral set defined by the intersection of $N-1$ half-spaces. Then, we see that
\begin{equation}
\vT^*(\vy) = \vx_i \quad \text{for} \quad \vy \in \inte (C_i(\vz^*)) \quad \text{for some} \ i \in \{1,\dots,N \}.
\label{eq: OT map}
\end{equation}
Since the Laguerre cells form a partition of $\R^d$ up to Lebesgue negligible sets, the expression (\ref{eq: OT map}) defines a $Q$-a.e. defined map with values in $\cX$. Furthermore, since $\vT^*$ is a transport map, i.e., $\vT^*(\vY) \sim P$ for $\vY \sim R$, we have 
\[
\begin{split}
R\big (C_i(z^*)\big) &=R\big ( \inte (C_i(\vz^*)) \big ) = \Prob \big(\vT^*(\vY) = \vx_i \big) \\
&= P(\{ \vx_i \}) = p_i > 0
\end{split}
\] 
for every $i \in \{ 1,\dots, N \}$.

In the present paper, we assume that $R$ is a \textit{known} reference measure and are interested in making statistical inference on $\vT^*$ for unknown $P$. Given an i.i.d. sample $\vX_1,\dots,\vX_n$ from $P$, a natural estimator for $\vT^*$ is the empirical OT map $\hat{\vT}_n$ transporting $R$ onto the empirical distribution $P_n = n^{-1}\sum_{i=1}^n \delta_{\vX_i}$.  
Set 
\[
\hat{\vp}_n = (\hat{p}_{n,1},\dots,\hat{p}_{n,N})^{\intercal} = \big(P_n(\{\vx_1\}),\dots,P_n(\{\vx_N\})\big)^\intercal
\]
as the sample version of the frequency vector $\vp$. Then, the empirical OT map $\hat{\vT}_n$ admits the expression
\[
\hat{\vT}_n (\vy) = \argmin_{\vx_i: 1 \le i \le N} \left ( \frac{1}{2} \|\vy-\vx_i\|^2 - \hat{z}_{n,i} \right ),
\]
where $\hat{\vz}_n =(\hat{z}_{n,1},\dots,\hat{z}_{n,N})^{\intercal}$ is an optimal solution to the dual problem (\ref{eq: semi_dual}) with $\vp$ replaced by $\hat{\vp}_n$.

We shall study limit theorems for the empirical OT map. The problem has a certain subtlety which we shall discuss here. 
In our semidiscrete setup, OT maps are piecewise constant functions with values in the discrete set $\cX$. Hence, it is not hard to see that, for every $\vy \in \inte(C_i(\vz^*))$, the empirical OT map $\hat{\vT}_n(\vy)$ \textit{exactly} coincides with $\vT^*(\vy)$ as $n \to \infty$. This effectively means that finding pointwise limit distributions is vacuous.  See Proposition \ref{prop: pointwise} ahead for the precise statement. However, this `super consistency' result has limited statistical values since (i) the population Laguerre cells are unknown;  (ii) there is no guarantee that a chosen reference point $\vy$ lies in the interior of one of the population Laguerre cells; (iii) the sample size needed to guarantee $\hat{\vT}_n(\vy) = \vT^*(\vy)$ relies on how close $\vy$ is to the boundary of $C_i(\vz^*)$; and (iv)  the super consistency result cannot capture the behavior of the OT map near the boundaries of the Laguerre cells. 

Because of these reasons, we shall analyze functionals of the empirical OT map other than pointwise ones. 
The preceding  observation does not preclude the possibility of finding nontrivial weak limits for certain functionals of the empirical OT map, because the contributions from the behaviors of the empirical OT map near the boundaries of the (population) Laguerre cells may pile up and lead to a nondegenerate limit.  
Specifically, we shall focus on the following functionals\footnote{We use $L^s$ instead of $L^p$ as $p$ might be confused with the probability simplex vector or its elements.}:
\begin{itemize}
\item $L^s$-estimation error with arbitrary $s \in [1,\infty)$: 
\[
\| \hat{\vT}_n - \vT^* \|_{L^s(R)}^s= \int \| \hat{\vT}_n(\vy)-\vT^*(\vy) \|^s dR(\vy); \quad \text{and}
\]
\item  Linear functional: $\langle \bm{\varphi}, \hat{\vT}_n \rangle_{L^2(R)} = \int \langle \bm{\varphi}, \hat{\vT}_n \rangle dR$ for a bounded Borel vector field $\bm{\varphi}:\R^d \to \R^d$.
\end{itemize}
We will establish (nondegenerate) weak limits for those functionals. (For the linear functional case, nondegeneracy of the weak limit relies on the choice of $\bm{\varphi}$.) For both functionals, we will also establish consistency of the nonparametric bootstrap. These statistical results enable statistical inference for local and global behaviors of the OT map. See Example \ref{ex: application} ahead for more details.



\begin{remark}[On assumption of known $R$]
Our assumption of known reference measure $R$ is natural when we view the OT map as a multivariate extension of the quantile function \cite{chernozhukov2017monge}.
 Indeed, when $d=1$ and $R= \text{Unif}[0,1]$, the OT map $T^*$ agrees with the quantile function of $P$. 
In general, the OT map shares two important properties of the quantile function. (i) For $\vY \sim R$, $\vT^*(\vY)$ recovers the target distribution $P$, $\vT^*(\vY) \sim P$; and (ii) $\vT^*$ is a \textit{monotone}, i.e., $\langle \vT^*(\vy) - \vT^*(\vy'), \vy-\vy' \rangle \ge 0$, which follows by the fact that $\vT^*$ agrees with the gradient of a convex function. A common choice of the reference measure $R$ is the uniform distribution over the unit cube $[0,1]^d$ or the unit ball $\{ \vy : \| \vy \| \le 1 \}$.
\end{remark}


\begin{remark}[Computational aspects]
The decomposition into Laguerre cells is known as a `power diagram' in computational geometry \cite{aurenhammer1987power}, for which efficient algorithms are available \cite{bowyer1981computing,watson1981computing}. They can be implemented in several programming libraries, such as CGAL \cite{cgal1996cgal} and GEOGRAM \cite{levy2015geogram}. Efficient algorithms for computing dual potential vectors were proposed by \cite{merigot2011multiscale,kitagawa2019convergence}.  Linear convergence of a dumped Newton algorithm for solving the dual problem (\ref{eq: semi_dual}) was proved by \cite{kitagawa2019convergence}. We refer to \cite{levy2018notions} and \cite[Section 5]{peyre2019computational} for a review of computational aspects of semidiscrete OT. 
\end{remark}




\section{Main results}
\label{sec: main}

Our approach to finding limit distributions for the preceding functionals relies on establishing (directional) differentiability with respect to the dual potential vector. Indeed, the bulk of our effort is devoted to proving those stability estimates. 
The desired limit distributions follow by combining a limit distribution result for the empirical dual potential vector (cf. \cite{del2022central}) and the delta method. After discussing regularity conditions on the reference measure $R$ and the Borel vector field $\bm{\varphi}$, we present key differentiability results, which would be of independent interest,  and then move on to discussing the limit theorems. 

\subsection{Assumptions}

Throughout, we maintain the following assumption on the reference measure $R$. 
\begin{assumption}[Regularity of $R$]
\label{asp: cont}
The reference measure $R$ is compactly supported and absolutely continuous with Lebesgue density $\rho$ continuous on a compact and convex set $\cY$ containing the support of $R$. The set $\cY$ is either polyhedral or satisfying $\cH^{d-1}(\partial \cY \cap H) = 0$ for every hyperplane $H$ in $\R^d$.
\end{assumption}

The density $\rho$ need not be globally continuous. In particular, Assumption \ref{asp: cont} allows for the uniform distributions over the unit cube and ball. Without loss of generality, we set $\rho = 0$ on $\cY^c$. 

For the limit theorems, we need an additional assumption on  the regularity of $R$, stated as follows.

\begin{assumption}[$L^1$-Poincar\'{e} inequality]
\label{asp: poincare}
The reference measure $R$ satisfies an $L^1$-Poincar\'{e} inequality, i.e., there exists a finite constant $\mathsf{C}_{\mathrm{P}}$ such that for $\vY \sim R$,
\[
\E\big [ |f(\vY) - \E[f(\vY)]| \big] \le \mathsf{C}_{\mathrm{P}} \E\big[\| \nabla f (\vY) \|\big],
\]
for every smooth function $f$ on $\R^d$ with compact support. 
\end{assumption}

Assumption \ref{asp: poincare} is not needed for the differentiability results in the next section, but needed to establish a limit distribution for $\hat{\vz}_n$, whose derivation relies on the results of \cite{kitagawa2019convergence}. In \cite{kitagawa2019convergence}, the $L^1$-Poincar\'{e} inequality is used to guarantee strict concavity of the dual objective function in nontrivial directions, which ensures the uniqueness of the dual potential vector subject to proper normalization. 
The $L^1$-Poincar\'{e} inequality is known to be equivalent to Cheeger's isoperimetric inequality \cite[Lemma 2.2]{milman2007role} and satisfied by any \textit{log-concave distribution}, i.e., a distribution $Q$ of the form $dQ= e^{-\psi} \,d\vy$ for some convex function $\psi: \R^d \to (-\infty,\infty]$; see \cite{bobkov1999isoperimetric}.
For example, the uniform distributions over the unit cube and ball both satisfy Assumption \ref{asp: poincare}.



When we consider inference for linear functionals of the form $\langle \bm{\varphi}, \vT^* \rangle_{L^2(R)}$, we make the following assumption on the Borel vector field $\bm{\varphi}$. 

\begin{assumption}[Regularity of $\bm{\varphi}$]
\label{asp: text function}
The Borel vector field $\bm{\varphi}: \R^d \to \R^d$ is bounded and its set of discontinuities  has Hausdorff dimension strictly less than $d-1$. 
\end{assumption}

The role of Assumption \ref{asp: text function} will be discussed after Theorem \ref{thm: Hadamard_diff} below. 


\subsection{Differentiability results}
For $\vz \in \R^N$, define a map $\vT_{\vz}$ with values in $\cX$ by
\[
\vT_{\vz}(\vy) = \argmin_{\vx_i: 1 \le i \le N} \left ( \frac{1}{2} \|\vy-\vx_i\|^2 - z_{i} \right ),
\]
which is well-defined $R$-a.e.
In this section, we establish (directional) differentiability of the following functions:
\[
\begin{split}
\delta_s(\vz_1,\vz_2) &= \|\vT_{\vz_1} - \vT_{\vz_2}\|_{L^s(R)}^s, \vz_1,\vz_2 \in \R^N, \ \text{and} \\
\gamma_{\bm{\varphi}} (\vz) &= \langle \bm{\varphi}, \vT_{\vz} \rangle_{L^2(R)} = \int \langle \bm{\varphi},\vT_{\vz} \rangle \, dR, \ \vz \in \R^N.
\end{split}
\]
Observe that $\| \hat{\vT}_n - \vT^* \|_{L^s(R)}^s = \delta_s(\hat{\vz}_n,\vz^*)$ and $\langle \bm{\varphi},\hat{\vT}_n - \vT^* \rangle_{L^2(R)}= \gamma_{\bm{\varphi}}(\hat{\vz}_n) - \gamma_{\bm{\varphi}}(\vz^*)$.
Combined with a limit distribution result for $\hat{\vz}_n$ (which will be discussed in the next section), the limit distributions for these functionals follow via the extended delta method.
It turns out that the $\delta_s$ functional is not (Frech\'{e}t) differentiable at $(\vz^*,\vz^*)$, but \textit{Hadamard directionally differentiable}, which is enough to invoke the extended delta method; see Appendix \ref{sec: functional delta} for a brief review of directional Hadamard differentiability and the extended delta method.
Note that to find a limit distribution for $\| \hat{\vT}_n-\vT^*\|_{L^s(R)}^s$, we only need to derive a Hadamard directional derivative of a simpler function $\vz \mapsto \| \vT_{\vz} - \vT_{\vz^*} \|_{L^s(R)}^s$ at $\vz^*$.  However, to study the bootstrap  for the $L^s$-functional, we need to analyze the two-variable mapping $(\vz_1,\vz_2) \mapsto \| \vT_{\vz_1} - \vT_{\vz_2} \|_{L^s(R)}^s$.

To state our differentiability results, we need additional notations. 
For a subset $D$ of a hyperplane in $\R^d$, the $R$-surface measure of $D$ is defined by
\[
R^{+}(D) = \int_D \rho \,d\cH^{d-1}.
\]
Recall the Laguerre cells $\{ C_i(\vz) \}_{i=1}^N$ defined in the previous section and $b_{ij}(\vz) = (\|\vx_i\|^2-\|\vx_j\|^2)/2-z_i+z_j$. When evaluated at $\vz = \vz^*$  (an optimal solution to the dual problem in (\ref{eq: semi_dual}) for $(R,P)$), we often omit the dependence on $\vz^*$, i.e., $b_{ij} = b_{ij}(\vz^*)$ and $C_i = C_i(\vz^*)$. (Here $\vz^*$ can be any optimal solution to the dual problem (\ref{eq: semi_dual}).)
Set
\[
\begin{split}
D_{ij} &:= C_i \cap C_j = C_i  \cap \big\{\vy: \langle \vx_i-\vx_j,\vy \rangle = b_{ij}\big\} \\
&= C_j  \cap \big\{\vy: \langle \vx_i-\vx_j,\vy \rangle = b_{ij}\big\}.
\end{split}
\]
Observe that $b_{ij}$ is anti-symmetric in $(i,j)$, $b_{ij}  = -b_{ji}$, and satisfying $b_{ij} + b_{jk} = b_{ik}$, while $D_{ij}$ is symmetric in $(i,j)$, $D_{ij} = D_{ji}$. The collection $\{ D_{ij} \}_{j \ne i}$ forms the boundary of $C_i$.

We are now ready to state the main result of this section. 
\begin{theorem}[Differentiability results]
\label{thm: Hadamard_diff}
Suppose Assumption \ref{asp: cont} holds. The following hold. 
\begin{enumerate}
    \item[(i)] For arbitrary $s \in [1,\infty)$, the map $\R^{2N} \ni (\vz_1,\vz_2) \mapsto \delta_s(\vz_1, \vz_2) = \|\vT_{\vz_1} - \vT_{\vz_2}\|_{L^s(R)}^s$ is Hadamard directionally differentiable at $(\vz^*, \vz^*)$  with derivative 
\begin{equation}
\label{eq: h_deriv}
[ \delta_s ]'_{(\vz^*, \vz^*)}(\vh_1, \vh_2) = \sum_{1 \leq i < j \leq N} \|\vx_i - \vx_j\|^{s-1} \, R^{+}(D_{ij})\, |h_{2,j} - h_{2,i} - h_{1,j} + h_{1,i}| 
\end{equation}
for $\vh_{\ell} = (h_{\ell,1},\dots,h_{\ell,N})^{\intercal} \in \R^N (\ell=1,2)$. 
\item[(ii)] Suppose in addition that Assumption \ref{asp: text function} holds. Then, the map $\R^N \ni \vz \mapsto \gamma_{\bm{\varphi}}(\vz) = \langle \bm{\varphi},\vT_{\vz} \rangle_{L^2(R)}$ is (Frech\'{e}t) differentiable at $\vz^*$ with derivative
\begin{equation}
[\gamma_{\bm{\varphi}}]_{\vz^*}'(\vh) = \sum_{1\leq i < j \leq N} \frac{h_i - h_j}{\|\vx_i - \vx_j\|}\int_{D_{ij}}  \langle \vx_i - \vx_j,  \bm{\varphi}(\vy) \rangle  \rho (\vy) \, d\cH^{d-1}(\vy)
\label{eq: h-deriv2}
\end{equation}
for $\vh = (h_1,\dots,h_N)^{\intercal}\in \R^N$.
\end{enumerate}
\end{theorem}

The derivative of $\delta_s$ in (\ref{eq: h_deriv}) is nonlinear in $(\vh_1,\vh_2)$, so $\delta_s$ is only directionally differentiable. On the other hand, $\gamma_{\bm{\varphi}}$ is Frech\'{e}t differentiable and the derivative in (\ref{eq: h-deriv2}) is linear in $\vh$. The derivative $[\gamma_{\bm{\varphi}}]_{\vz^*}'$ may vanish depending on the choice of $\bm{\varphi}$. For example, if $\bm{\varphi}$ vanishes on the boundaries of the Laguerre cells $\{ C_i \}_{i=1}^N$, then $[\gamma_{\bm{\varphi}}]_{\vz^*}' \equiv 0$. However, the derivative may be nonvanishing if the support of $\bm{\varphi}$ intersects the boundaries of the Laguerre cells. 

The proof of Theorem \ref{thm: Hadamard_diff} is lengthy and constitutes the majority of Section \ref{sec: proofs}. For Part (i), we observe that 
\[
\delta_s(\vz^* + t\vh_1,\vz^*+t\vh_2) = \sum_{1 \le i \ne j \le N} \|\vx_i-\vx_j\|^s R\big( C_i(\vz^*+t\vh_1) \cap C_j(\vz^*+t\vh_2) \big).
\]
The proof proceeds by carefully analyzing the facial structures of polyhedral sets of the form $C_i(\vz^*+t\vh_1) \cap C_j(\vz^*+t\vh_2)$, which gives rise to a Gateaux directional derivative. To lift the Gateaux differentiability to the Hadamard one, we establish local Lipshitz continuity of $\delta_s$ (cf. \cite{shapiro1990concepts}). The proof of Part (ii) is essentially similar upon observing that
\[
\gamma_{\bm{\varphi}}(\vz^* + t\vh) - \gamma_{\bm{\varphi}}(\vz^*) = \sum_{1 \le i \neq j \le N} \int_{C_i(\vz^*) \cap C_j(\vz^* + t\vh)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \ dR(\vy).
\]
Assumption \ref{asp: text function} is essential to guarantee that the discontinuities of $\bm{\varphi}$ do not affect the local behavior of $\bm{\varphi}$ around each $D_{ij} = C_i \cap C_j$ with respect to $\cH^{d-1}$. 





\subsection{Limit theorems}
\label{sec: limit}
Recall the setting: let $\vX_1,\dots,\vX_n$ be an i.i.d. sample from $P$ with empirical distribution $P_n = n^{-1}\sum_{i=1}^n \delta_{\vX_i}$. The frequency vector for $P_n$ is denoted by $\hat{\vp}_n = (\hat{p}_{n,1},\dots,\hat{p}_{n,N})^{\intercal} = (P_n(\{\vx_1\}),\dots,P_n(\{\vx_N \}))^{\intercal}$. 
We first state a limit distribution result for $\hat{\vz}_n$ following \cite{del2022central}, which in turn builds on \cite{kitagawa2019convergence}. 
To this end, we need to guarantee the uniqueness (identification) of an optimal solution to the dual problem (\ref{eq: semi_dual}) subject to proper normalization. Obviously, if $\vz$ is optimal for (\ref{eq: semi_dual}), then $\vz + c\vone$ for any $c \in \R$ is optimal as well, so we make the normalization that $\langle \vz,\vone \rangle = 0$.

Let 
\[
\cQ= \Big\{ \bm{q} = (q_1,\dots,q_{N-1})^{\intercal} : q_i \ge 0 \ (\forall i \in \{ 1,\dots, N-1 \}), \langle \bm{q},\vone \rangle \le 1 \Big \}.
\]
Each element $\bm{q} \in \cQ$ corresponds to the probability simplex vector on $\R^N$ via $\bm{q} \mapsto \big(\bm{q}^\intercal, 1-\langle \bm{q},\vone \rangle \big)^{\intercal}$. 
 Also, define 
\[
\cQ_{+} = \big \{ \bm{q} = (q_1,\dots,q_{N-1})^{\intercal}: q_i > 0 \ (\forall i \in \{1,\dots,N-1 \}), \langle \bm{q}, \vone \rangle < 1 \big \}. 
\]
Set the objective function in the dual problem as
\[
\Phi(\vz,\bm{q}) = \sum_{i=1}^{N-1} z_i q_i + z_N \left (1-\sum_{i=1}^{N-1}q_i \right )+ \int \min_{1 \leq i\leq N} \left ( \frac{1}{2} \| \vy-\vx_i \|^2 - z_i\right )\,dR(\vy)
\]
for $\vz \in \R^N$ and $\bm{q} \in \cQ$.

The next lemma follows from the results in \cite{kitagawa2019convergence}. See also \cite{bansil2022quantitative}.
\begin{lemma}
\label{lem: kitagawa}
Under Assumptions \ref{asp: cont} and \ref{asp: poincare}, 
for every $\bm{q} \in \cQ_+$, the optimization problem
\begin{equation}
\max_{\vz \in \langle \vone \rangle^{\bot}} \Phi(\vz,\bm{q})
\label{eq: dual}
\end{equation}
admits the unique optimal solution $\vz^*(\bm{q})$, where $\langle \vone \rangle^{\bot} = \{ \vz \in  \R^N : \langle \vz,\vone \rangle = 0 \}$. Furthermore, the mapping $\cQ_+ \ni \bm{q} \mapsto \vz^*(\bm{q})$ is continuously differentiable.  
\end{lemma}


Denote $\vp^{-N} := (p_1,\dots,p_{N-1})^{\intercal} \in \cQ_{+}$. To avoid ambiguity, set
\[
\vz^* = \vz^*(\vp^{-N}) \quad \text{and} \quad \hat{\vz}_n =
\begin{cases}
 \vz^* (\hat{\vp}_n^{-N}) &\text{if} \ \quad \hat{\vp}_n^{-N} \in \cQ_{+}, \\
 \vz_0 &\text{otherwise},
 \end{cases}
\]
where $\vz_0 \in \langle \vone \rangle^{\bot}$ is an arbitrary fixed vector. Since $\hat{\vp}_n^{-N} \in \cQ_{+}$  with probability approaching one, the limit theorems below continue to hold for any other modification to $\hat{\vz}_n$ outside the event $\hat{\vp}_n^{-N} \in \cQ_{+}$.



Since $\sqrt{n}(\hat{\vp}_n^{-N}-\vp^{-N}) \stackrel{d}{\to} \cN(\vzero,A_P)$ with
\[
\underbrace{A_{P}}_{(N-1) \times (N-1)} = \begin{pmatrix}
p_1 (1-p_1)  & \cdots & -p_1 p_{N-1} \\
\vdots & \ddots & \vdots \\
-p_{N-1}p_1  & \cdots & p_{N-1}(1-p_{N-1}) 
\end{pmatrix}
\]
by the multivariate central limit theorem, an application of the delta method yields 
\begin{equation}
\sqrt{n}(\hat{\vz}_n-\vz^*) \stackrel{d}{\to} \cN(0,B_P^\intercal A_P B_P), \label{eq: CLT}
\end{equation}
where $B_P= \nabla \vz^*(\bm{q})|_{\bm{q}=\vp^{-N}}$; cf. \cite{del2022central}. Since $\langle \hat{\vz}_n-\vz^*, \vone \rangle = 0$, the Gaussian distribution $\cN(0,B_P^\intercal A_P B_P)$ is singular. 

Now,
combining (\ref{eq: CLT}) and the differentiability results in Theorem \ref{thm: Hadamard_diff}, together with the extended delta method (cf. Lemma \ref{lem: functional delta method}), we obtain the following proposition.
Recall that $\vT^* = \vT_{\vz^*}$ and $\hat{\vT}_n = \vT_{\hat{\vz}_n}$. 


\begin{proposition}[Limit distributions]
\label{prop: limit}
Suppose Assumptions \ref{asp: cont} and \ref{asp: poincare} hold. Let $\vW = (W_1,\dots,W_N)^{\intercal} \sim \cN(0,B_P^\intercal A_P B_P)$. Then the following hold.
\begin{enumerate}
    \item[(i)] For arbitrary $s \in [1,\infty)$, we have 
    \begin{equation}
    \sqrt{n} \| \hat{\vT}_n-\vT^* \|_{L^s(R)}^s \stackrel{d}{\to}  \sum_{1 \leq i < j \leq N} \|\vx_i - \vx_j\|^{s-1} \, R^{+}(D_{ij})\, |W_i-W_j|.
    \label{eq: limit law}
    \end{equation}
    \item[(ii)] Suppose in addition that Assumption \ref{asp: text function} holds. Then
    \[
    \sqrt{n} \langle \bm{\varphi},\hat{\vT}_n - \vT^* \rangle_{L^2(R)} \stackrel{d}{\to} \cN(0,\sigma_{P,\bm{\varphi}}^2),
    \]
    where $\sigma_{P,\bm{\varphi}}^2$ is the variance of the following random variable
    \[
    \sum_{1\leq i < j \leq N} \frac{W_i - W_j}{\|\vx_i - \vx_j\|}\int_{D_{ij}}  \langle \vx_i - \vx_j,  \bm{\varphi}(\vy) \rangle  \rho (\vy) \, d\cH^{d-1}(\vy).
    \]
\end{enumerate}
\end{proposition}


\begin{remark}[Asymptotic efficiency of $\langle \bm{\varphi},\hat{\vT}_n \rangle_{L^2(R)}$]
Consider Part (ii) and assume that $\sigma_{P,\bm{\varphi}}^2 > 0$. 
Observe that $\langle \bm{\varphi},\vT^* \rangle_{L^2(R)} = \gamma_{\bm{\varphi}} (\vz^*(\vp))$ and the function $\bm{q} \mapsto \gamma_{\bm{\varphi}} (\vz^*(\bm{q}))$ is differentiable at $\vp$.
Since $\hat{\vp}_n^{-N}$ is the maximum likelihood estimator for $\vp^{-N}$, the empirical estimator $\langle \bm{\varphi},\hat{\vT}_n \rangle_{L^2(R)} = \gamma_{\bm{\varphi}} (\vz^*(\hat{\vp}_n^{-N}))$ is asymptotically efficient in the Haj\'{e}k-Le Cam sense. See Chapter 8 in \cite{vanderVaart1998asymptotic} for details. 
\end{remark}



Next, we discuss an application of the nonparametric bootstrap  for estimating the limit distributions in Proposition \ref{prop: limit}. The limit distributions in Proposition \ref{prop: limit} depend on the population distribution $P$ in a rather complicated way, and the analytical estimation is nontrivial.  Hence, using the bootstrap is an appealing alternative for statistical inference. 

Let $\vX_1^B,\dots,\vX_n^B$ be an i.i.d. sample from $P_n$ conditional on $\vX_1,\dots,\vX_n$ and  $P_n^B=n^{-1}\sum_{i=1}^n \delta_{\vX_i^B}$ denote the bootstrap empirical distribution. Let \[
\hat{\vp}_n^B = (\hat{p}_{n,1}^B,\dots,\hat{p}_{n,N}^B)^{\intercal} = \big(P_n^B(\{\vx_1\}),\dots,P_n^B(\{\vx_N \})\big)^{\intercal}
\]
denote the corresponding frequency vector with $\hat{\vp}_n^{B,-N} = (\hat{p}_{n,1}^B,\dots,\hat{p}_{n,N-1}^B)^{\intercal}$. Set
\[
\hat{\vz}_n^B =
\begin{cases}
 \vz^* (\hat{\vp}_n^{B,-N}) &\text{if} \ \quad \hat{\vp}_n^{B,-N} \in \cQ_{+}, \\
 \vz_0 &\text{otherwise},
 \end{cases}
\]
and $\hat{\vT}_n^B = \vT_{\hat{\vz}_n^B}$. 

For a sequence of (univariate) bootstrap statistics $S_n^B$ (i.e., functions of $\vX_1,\dots,\vX_n$ and $\vX_1^B,\dots,\vX_n^B$) and a (nonrandom) distribution $\nu$ on $\R$, we say that \textit{the conditional law of $S_n^B$ given the sample converges weakly to $\nu$  in probability}  if  
\[
\sup_{g \in BL_1(\R)} \Big |\E\left [ g(S_n^B) \mid \vX_1,\dots, \vX_n \right] - \E_{S \sim \nu}[g(S)] \Big | \to 0
\]
in probability, where $BL_1(\R)$ is the set of $1$-Lipschitz functions $g: \R \to [-1,1]$; cf. Chapter 3.6 in \cite{van1996weak} and Chapter  23 in \cite{vanderVaart1998asymptotic}.

We are now ready to state the bootstrap consistency results. 

\begin{proposition}[Bootstrap consistency]
\label{prop: bootstrap consistency}
Suppose Assumptions \ref{asp: cont} and \ref{asp: poincare} hold. Then the following hold.
\begin{enumerate}
    \item[(i)] For arbitrary $s \in [1,\infty)$, the conditional law of $\sqrt{n}\| \hat{\vT}_n^B - \hat{\vT}_n\|_{L^s(R)}^s$ given the sample converges weakly to the limit law in (\ref{eq: limit law}) in probability. 
    \item[(ii)] Suppose in addition that Assumption \ref{asp: text function} holds. Then the conditional law of $\sqrt{n} \langle \bm{\varphi},\hat{\vT}_n^B - \hat{\vT}_n \rangle_{L^2(R)}$ given the sample converges weakly to $\cN(0,\sigma_{P,\bm{\varphi}}^2)$, where $\sigma_{P,\bm{\varphi}}^2$ is given in Proposition \ref{prop: limit}. 
\end{enumerate} 
\end{proposition}

The proof first establishes a conditional central limit theorem for $\sqrt{n}(\hat{\vz}_n^B-\hat{\vz}_n)$, which follows from the delta method for the bootstrap. Given this, Part (ii) follows from another application of the delta method for the bootstrap, since the mapping $\vz \mapsto \gamma_{\bm{\varphi}}(\vz)$ is (Frech\'{e}t) differentiable at $\vz^*$. 

Part (i) might seem surprising as the corresponding mapping $(\vz_1,\vz_2) \mapsto \delta_s(\vz_1,\vz_2)$ is only directionally differentiable with a nonlinear derivative. 
In fact, \cite{dumbgen1993nondifferentiable} and \cite{fang2019} show that the bootstrap fails to be consistent for functionals with nonlinear Hadamard derivatives. However, their results do not collide with Part (i). The results of \cite{dumbgen1993nondifferentiable} and \cite{fang2019} applied to our setting show that the conditional law of $\sqrt{n}(\| \hat{\vT}_n^B - \vT^* \|_{L^s(R)}^s - \| \hat{\vT}_n - \vT^* \|_{L^s(R)}^s) = \sqrt{n}(\delta_s(\hat{\vz}_n^B,\vz^*) - \delta_s(\hat{\vz}_n,\vz^*))$ fails to be consistent for estimating the limit law in (\ref{eq: limit law}), which is indeed the case, but our application of the bootstrap is different and uses $\sqrt{n}\| \hat{\vT}_n^B - \hat{\vT}_n \|_{L^s(R)}^s = \sqrt{n}\delta_s(\hat{\vz}_n^B,\hat{\vz}_n)$ instead. See Proposition 3.8 in \cite{goldfeld2022limit} for a related discussion. 

The proof of Part (i) goes as follows.
By the differentiability result from Theorem \ref{thm: Hadamard_diff} (i), we can approximate $\sqrt{n}\delta_s(\hat{\vz}_n^B,\hat{\vz}_n)$ by
\[
\sqrt{n}[\delta_s]_{(\vz^*,\vz^*)}'(\hat{\vz}_n^B-\vz^*,\hat{\vz}_n-\vz^*),
\]
which, from the explicit expression of the derivative in (\ref{eq: h_deriv}), agrees with 
\[
[\delta_s]_{(\vz^*,\vz^*)}'(\sqrt{n}(\hat{\vz}_n^B-\hat{\vz}_n),\vzero). 
\]
The conditional law of the above converges weakly to the law of $[\delta_{s}]_{(\vz^*,\vz^*)}'(\vW,\vzero)$ with $\vW \sim \cN(\vzero,B_P^\intercal A_P B_P)$, which agrees with the limit law in (\ref{eq: limit law}).


\begin{example}[Statistical applications]
\label{ex: application}
The results of Propositions \ref{prop: limit} and \ref{prop: bootstrap consistency} enable us to construct $L^s$-confidence sets for $\vT^*$ and confidence intervals for $\langle \bm{\varphi},\vT^* \rangle_{L^2(R)}$. 
For example, consider constructing an $L^1$-confidence set for $\vT^*$. Let $S$ be a random variable following the limit law in (\ref{eq: limit law}). Given $\alpha \in (0,1)$, set
\[
\hat{\tau}_n(1-\alpha) = \text{conditional $(1-\alpha)$-quantile of $\sqrt{n}\| \hat{\vT}_n^B - \hat{\vT}_n\|_{L^1(R)}$},
\]
which can be computed via simulations. Then, provided that the distribution function of $S$ is continuous at its $(1-\alpha)$-quantile, the confidence set of the form
\begin{equation}
\big \{ \vT : \sqrt{n}\| \hat{\vT}_n-\vT \|_{L^1(R)} \le \hat{\tau}_{n}(1-\alpha) \big \}
\label{eq: L1 confidence set}
\end{equation}
contains $\vT^*$ with probability approaching $1-\alpha$.

One drawback of $L^1$-confidence sets is that they are difficult to visualize compared to $L^\infty$-confidence bands. Section 5.8 in \cite{wasserman2006all} discusses a method to construct a confidence band from an $L^2$-confidence set, which builds on an idea in \cite{juditsky2003nonparametric}. Such a confidence band does not satisfy the uniform coverage guarantee but instead satisfies the \textit{average coverage}. We adapt the method in \cite[Section 5.8]{wasserman2006all} to $L^1$-confidence sets. Consider the confidence band of the form
\[
\cC_{n}^{1-\alpha}(\vy) = \left \{  \vx : \| \hat{\vT}_n (\vy) - \vx \| \le\frac{\hat{\tau}_n(1-\alpha/2)}{\sqrt{n}} \cdot \frac{2}{\alpha} \right \}, \ \vy \in \cY.
\]
Then, the argument in \cite[p.~95]{wasserman2006all} yields that the band $\cC_{n}^{1-\alpha}$ has average coverage at least $1-\alpha + o(1)$, i.e., 
\begin{equation}
\int \Prob \big (\vT^*(\vy) \in \cC_{n}^{1-\alpha}(\vy) \big) \, dR(\vy) \ge 1-\alpha + o(1),
\label{eq: average coverage}
\end{equation}
provided that the $L^1$-confidence set (\ref{eq: L1 confidence set})  has coverage probability $1-\alpha+o(1)$ for every $\alpha \in (0,1)$. For the reader's convenience, we include the derivation of (\ref{eq: average coverage}) in Section \ref{sec: average}. 
Since $\vT^*$ only take values in $\cX$, we may intersect $\cC_n^{1-\alpha}(\vy)$ with $\cX$ to construct a tighter confidence band (cf. \cite{chernozhukov2020generic}), 
\[
\tilde{\cC}_{n}^{1-\alpha}(\vy) = \cC_{n}^{1-\alpha}(\vy) \cap \cX.
\]
The average coverage property continues to hold for the latter confidence band. 
\end{example}


Finally, we state a super consistency result for the empirical OT map mentioned in Section \ref{sec: preliminaries}.

\begin{proposition}[Super consistency]
\label{prop: pointwise}
Suppose Assumptions \ref{asp: cont} and \ref{asp: poincare} hold. Then, for every $i \in \{ 1,\dots, N \}$ and compact set $K \subset \inte (C_i(\vz^*))$, we have 
\[
\hat{\vT}_n(\vy) = \vT^*(\vy) \quad \text{for all} \ \vy \in K
\]
with probability approaching one as $n \to \infty$. 
\end{proposition}


\section{Proofs}
\label{sec: proofs}



\subsection{Proof of Theorem \ref{thm: Hadamard_diff}}

Before proving the theorem, we first prove the following technical lemma, which will be repeatedly used in the sequel.

\begin{lemma}
\label{lem: DCT}
Suppose Assumption \ref{asp: cont} holds. Then, for every affine subspace $H$ of $\R^d$ with dimension $r$ and every fixed $t_0 > 0$, we have
\[
\int_{H} \sup_{\substack{\vv \in (H-\vy_0)^{\bot} \\ \| \vv \| \le t_0}} \rho (\vy + \vv) \, d\cH^{r}(\vy) < \infty,
\]
where $\vy_0$ is an arbitrary fixed point in $H$ and $(H-\vy_0)^{\bot} = \{ \vv : \langle \vv, \vy-\vy_0 \rangle = 0, \forall \vy \in H \}$.  
\end{lemma}

\begin{proof}
For every $\vy \in H$ and $\vv \in (H-\vy_0)^{\bot}$ with $\| \vv \| \le t_0$, we have 
\[
\rho(\vy+\vv) \le \| \rho \|_{\infty} \mathbbm{1}_{\cY^{t_0}}(\vy),
\]
where $\cY^{t_0}$ denotes the $t_0$-expansion of $\cY$, i.e., $\cY^{t_0} = \{ \vy : \dist (\vy,\cY) \le t_0 \}$. 
Since $\cY$ is compact, $\cH^{r}(H \cap \cY^{t_0}) < \infty$, completing the proof. 
\end{proof}
\subsubsection{Proof of Theorem \ref{thm: Hadamard_diff} (i)}

We first note that
\[
\delta_s(\vz_1, \vz_2) = \sum_{1\le i \neq j \le N} \|\vx_i - \vx_j\|^s R\big(C_i(\vz_1) \cap C_j(\vz_2)\big). 
\]
For $\vh_1,\vh_2 \in \R^N$ and $t > 0$, we have 
\[
C_j(\vz^* + t\vh_2) = \bigcap_{k\neq j} \big\{\vy: \langle \vx_j-\vx_k,\vy \rangle - b_{jk} \ge t (h_{2,k} -  h_{2,j})\big\},
\]
so  $C_i(\vz^* + t\vh_1) \cap C_j(\vz^* + t\vh_2) \subset D_{ij}(\vh_1,\vh_2,t) $, where
\[
\begin{split}
D_{ij}(\vh_1,\vh_2,t) &:= C_i(\vz^*+t\vh_1) \cap \big\{\vy: \langle \vx_i-\vx_j,\vy \rangle - b_{ij} \le t (h_{2,j} - t h_{2,i})\big\} \\
&=\Big (\bigcap_{k \ne i,j} \big\{ \vy : \langle \vx_i-\vx_k,\vy \rangle - b_{ik} \ge t(h_{1,k} - h_{1,i}) \big\} \Big) \\
&\qquad \cap \big \{\vy : t(h_{1,j} - h_{1,i}) \leq  \langle \vx_i-\vx_j,\vy \rangle - b_{ij} \leq t(h_{2,j} - h_{2,i}) \big\}.
\end{split}
\]
Here we used the fact that $b_{ji} = -b_{ij}$. 
We divide the rest of the proof into three steps. 

\medskip

\underline{Step 1}. First, we shall show that for every $i,j \in \{ 1,\dots, N \}
$ with $i \ne j$  and every $\vh_1,\vh_2 \in \R^N$, 
\[
R\big(D_{ij}(\vh_1,\vh_2,t) \setminus C_j(\vz^* + t\vh_2)  \big) =O(t^2), \ t \downarrow 0. 
\]
Observe that, for every $\vy \in  D_{ij}(\vh_1,\vh_2,t) \setminus C_j(\vz^* + t\vh_2)$, there exists $k \in \{ 1,\dots, N \} \setminus \{ i,j \}$ such that $\langle \vx_j - \vx_k, \vy \rangle - b_{jk} < t(h_{2,k} - h_{2,j})$. For such $k$, we have
\[
\begin{split}
\langle \vx_i - \vx_k,\vy \rangle -b_{ik} &= \langle \vx_i - \vx_j,\vy \rangle - b_{ij} + \langle \vx_j - \vx_k,\vy \rangle - b_{jk} \\
&< t(h_{2,j}-h_{2,i} + h_{2,k} - h_{2,j}) = t(h_{2,k}-h_{2,i}),
\end{split}
\]
where we used the fact that $b_{ik}=b_{ij} + b_{jk}$. 
Since $\langle \vx_i - \vx_k,\vy \rangle -b_{ik} \ge t(h_{1,k}-h_{1,i})$ by $\vy \in D_{ij}(\vh_1,\vh_2,t)$, we have
\[
t(h_{1,k}-h_{1,i}) \le \langle \vx_i - \vx_k,\vy \rangle -b_{ik} <t(h_{2,k}-h_{2,i}). 
\]
Conclude that
\[
    D_{ij}(\vh_1,\vh_2,t) \setminus \big ( C_i(\vz^* + t\vh_1) \cap C_j(\vz^* + t\vh_2) \big ) 
    \subset \bigcup_{k \neq i,j} A_{ijk}(t), 
\]
where
\[
\begin{split}
    A_{ijk}(t) := &\big\{\vy: t(h_{1,j} - h_{1,i}) \leq  \langle \vx_i-\vx_j,\vy \rangle - b_{ij} \leq t(h_{2,j} - h_{2,i}) \big \}  \\&\quad \cap \big\{\vy: t(h_{1,k} - h_{1,i}) \leq  \langle \vx_i-\vx_k,\vy \rangle - b_{ik} \leq t(h_{2,k} - h_{2,i}) \big \}.
\end{split}
\]

Pick any $k \ne i,j$. 
If $\vx_i - \vx_j$ and $\vx_i - \vx_k$ are linearly independent, then we have
$R(A_{ijk}(t)) = O(t^2)$ by Lemma \ref{lem: DCT}. 


Conversely, suppose that $\vx_i - \vx_j$ and $\vx_i - \vx_k$ are linearly dependent, i.e., $\vx_i - \vx_k = c (\vx_i-\vx_j)$ for some $c\neq 0$. Set $L_1 = \{\vy : \langle \vx_i - \vx_j, \vy \rangle = b_{ij} \}$ and $L_2 = \{\vy: \langle \vx_i - \vx_k, \vy \rangle =b_{ik}\} = \{ \vy : \langle \vx_i-\vx_j,\vy \rangle = c^{-1}b_{ik}  \}$. For every $\vy \in A_{ijk}(t)$,
\[
\dist(\vy,L_1) \vee \dist(\vy,L_2) \leq \frac{t (| h_{1,j} - h_{1,i}| \vee |h_{2,j} - h_{2,i} |\vee| h_{1,k} - h_{1,i}| \vee |h_{2,k} - h_{2,i} |)}{\|\vx_i - \vx_j\|}.
\]
Furthermore,  since $L_1$ and $L_2$ are parallel, we have
\[
\dist(L_1, L_2) := \max \Big \{ \sup_{\vy' \in L_2} \dist (\vy',L_1), \sup_{\vy' \in L_1} \dist (\vy',L_2) \Big \} = \frac{|b_{ij} - c^{-1}b_{ik}|}{\|\vx_i - \vx_j\|}.
\]
Pick any $t_\ell \downarrow 0$ ($\ell \to \infty$).
Suppose that
\[
A_{ijk}(t_\ell) \ne \varnothing \quad \text{for infinitely many $\ell$}. 
\]
Then, we  have $\dist(L_1, L_2) = 0$, i.e., $b_{ik} = cb_{ij}$. In what follows, we separately consider the cases where $c < 0$, $c \in (0,1)$, or $c > 1$. 

\begin{itemize}
    \item Whan $c < 0$: Observe that 
\[
\begin{split}
C_i (\vz^*) &\subset \big\{\vy: \langle \vx_i - \vx_j, \vy \rangle \ge b_{ij} \big\} \cap \big\{ \vy: \langle \vx_i - \vx_k, \vy \rangle \ge b_{ik} \big\} \\
&=\big\{\vy: \langle \vx_i - \vx_j, \vy \rangle \ge b_{ij} \big\} \cap \big\{ \vy: c \langle \vx_i - \vx_j, \vy \rangle \ge c b_{ij} \big\} \\
&=\big\{\vy: \langle \vx_i - \vx_j, \vy \rangle \ge b_{ij} \big\} \cap \big\{ \vy: \langle \vx_i - \vx_j, \vy \rangle \le b_{ij} \big\} \quad (\text{because $c<0$}) \\
&=\big \{\vy: \langle \vx_i - \vx_j, \vy \rangle = b_{ij}\big\},
\end{split}
\]
which entails $R\big(C_i(\vz^*)\big) = 0$. But this contradicts the fact that $R\big(C_i(\vz^*)\big) = p_i > 0$.
  \item When $c \in (0,1)$: Since $b_{ki} = -cb_{ij}$, $b_{kj} = b_{ki} + b_{ij}  = (1-c) b_{ij}$, and $\vx_k-\vx_j = (1-c)(\vx_i-\vx_j)$, we have 
\[
\begin{split}
C_k (\vz^*) &\subset \big\{\vy: \langle \vx_k - \vx_i, \vy \rangle \ge b_{ki} \big\} \cap \big\{ \vy: \langle \vx_k - \vx_j, \vy \rangle \ge b_{kj} \big\} \\
&=\big\{\vy: -c\langle \vx_i - \vx_j, \vy \rangle \ge -cb_{ij} \big\} \cap \big\{ \vy: (1-c) \langle \vx_i - \vx_j, \vy \rangle \ge (1-c) b_{ij} \big\} \\
&=\big\{\vy: \langle \vx_i - \vx_j, \vy \rangle \le b_{ij} \big\} \cap \big\{ \vy: \langle \vx_i - \vx_j, \vy \rangle \ge b_{ij} \big\} \quad (\text{because $c \in (0,1)$}) \\
&=\big \{\vy: \langle \vx_i - \vx_j, \vy \rangle = b_{ij}\big\},
\end{split}
\]
which entails $R\big(C_k(\vz^*)\big) = 0$, a contradiction.
\item When $c > 1$: Since $b_{jk} = (c-1) b_{ij}$ and $\vx_j-\vx_k = (c-1)(\vx_i-\vx_j)$, we have
\[
\begin{split}
C_j (\vz^*) &\subset \big\{\vy: \langle \vx_j - \vx_i, \vy \rangle \ge b_{ji} \big\} \cap \big\{ \vy: \langle \vx_j - \vx_k, \vy \rangle \ge b_{jk} \big\} \\
&=\big\{\vy: \langle \vx_i - \vx_j, \vy \rangle \le b_{ij} \big\} \cap \big\{ \vy: (c-1) \langle \vx_i - \vx_j, \vy \rangle \ge (c-1) b_{ij} \big\} \\
&=\big\{\vy: \langle \vx_i - \vx_j, \vy \rangle \le b_{ij} \big\} \cap \big\{ \vy: \langle \vx_i - \vx_j, \vy \rangle \ge b_{ij} \big\} \quad (\text{because $c > 1$}) \\
&=\big \{\vy: \langle \vx_i - \vx_j, \vy \rangle = b_{ij}\big\},
\end{split}
\]
which entails $R\big(C_j(\vz^*)\big) = 0$, a contradiction.
\end{itemize}
Therefore, in all cases, we have $A_{ijk}(t) = \varnothing$ for sufficiently small $t > 0$. 




Finally, summing $R\big(A_{ijk}(t)\big)$ over $k \neq i, j$, we have
\[
R\big(D_{ij}(\vh_1,\vh_2,t) \setminus C_j(\vz^* + t\vh_2)\big) = O(t^2),
\]
as desired.

\medskip

\underline{Step 2}. 
Next, we shall evaluate the probability $R(D_{ij}(\vh_1,\vh_2,t))$ as $t \downarrow 0$, which is given by the following lemma. 

\begin{lemma}
\label{lem: thin mass}
Under Assumption \ref{asp: cont}, for every $i,j \in \{ 1,\dots,N \}$ with $i \ne j$ and every $\vh_1,\vh_2 \in \R^N$, we have 
\begin{equation}
\begin{split}
&R \big ( D_{ij}(\vh_1,\vh_2,t) \cup D_{ji}(\vh_1,\vh_2,t) \big) \\
&= R^{+}(D_{ij})  \frac{t | h_{2,j}-h_{2,i} - h_{1,j} + h_{1,i} |}{\|\vx_i - \vx_j\|} + o(t), \ t \downarrow 0.
\end{split}
\label{eq: surface measure}
\end{equation}
\end{lemma}
The proof of Lemma \ref{lem: thin mass} is lengthy and  deferred after the proof of Theorem \ref{thm: Hadamard_diff} (i). 




\medskip


\underline{Step 3}. By Steps 1 and 2, we have
\begin{align*}
    \delta_s(\vz^* + t\vh_1,\vz^* + t\vh_2) &= \sum_{1\leq i \neq j \leq N} \|\vx_i - \vx_j\|^s R \big ( C_i(\vz^* + t\vh_1) \cap C_j(\vz^* + t\vh_2) \big )\\
    &= \sum_{1\leq i \ne j \leq N} \|\vx_i - \vx_j\|^s R \big ( D_{ij}(\vh_1,\vh_2, t) \big) + o(t) \\
    &=t \underbrace{\sum_{1\leq i < j \leq N} \|\vx_i - \vx_j\|^{s-1} R^{+}(D_{ij}) | h_{2,j}-h_{2,i} - h_{1,j} + h_{1,i} |}_{=: [\delta_s]'_{(\vz^*,\vz^*)}(\vh_1,\vh_2)} + o(t).
\end{align*}
Thus, we have shown the directional  Gateaux differentiability for $\delta$,
\[
\lim_{t \downarrow 0}\frac{\delta_s(\vz^* + t\vh_1,\vz^* + t\vh_2)}{t} = [\delta_s]'_{(\vz^*,\vz^*)}(\vh_1,\vh_2).
\]
 To lift the Gateaux differentiability to the Hadamard one, it suffices to verify that $\delta_s$ is locally Lipschitz \cite{shapiro1990concepts}. To this end, observe that for $\vY \sim R$,
\begin{equation}
\begin{split}
    &|\delta_s(\vz_1,\vz_2) - \delta_s(\vz'_1,\vz'_2)| \\
    &= \left| \EE \left [ \|\vT_{\vz_1}(\vY) - \vT_{\vz_2}(\vY)\|^s - \|\vT_{\vz'_1}(\vY) - \vT_{\vz'_2}(\vY)\|^s \right] \right |\\
    &\leq s \max_{1 \leq i < j \leq N}\|\vx_i -\vx_j\|^{s-1} \EE\left [ \|\vT_{\vz_1}(\vY) - \vT_{\vz'_1}(\vY)\| + \|\vT_{\vz'_2}(\vY) - \vT_{\vz_2}(\vY)\| \right ],
    \end{split}
    \label{eq: delta Lip}
\end{equation}
where we used the inequality $|a^s-b^s| \le s(a \vee b)^{s-1}|a-b|$ for $a,b \ge 0$ combined with the fact that $\| \vT_{\vz} (\vy) - \vT_{\vz'}(\vy) \| \le \max_{1 \leq i < j \leq N}\|\vx_i -\vx_j\|$.
For $(\vz,\vz') = (\vz_1,\vz_1')$ or $(\vz_2, \vz_2')$, we have
\begin{equation}
\begin{split}
    \EE\big[\|\vT_{\vz}(\vY) - \vT_{\vz'}(\vY)\|\big] &= \sum_{1 \le i \ne j \le N} \|\vx_i - \vx_j\| R\big(C_i(\vz) \cap C_j(\vz')\big)\\
    &\leq \max_{1 \leq i < j \leq N} \|\vx_i - \vx_j\| \sum_{i=1}^N  R\big(C_i(\vz) \setminus C_i(\vz')\big),
    \label{eq: delta Lip2}
    \end{split}
\end{equation}
where the second inequality follows because $\{C_i(\vz')\}_{i=1}^N$ forms a partition of $\R^d$ up to Lebesgue negligible sets. 

Combining (\ref{eq: delta Lip}) and (\ref{eq: delta Lip2}), we have 
\begin{equation}
\begin{split}
 &|\delta_s(\vz_1,\vz_2) - \delta_s(\vz'_1,\vz'_2)| \\
 &\leq  \max_{1 \leq i < j \leq N} \|\vx_i - \vx_j\|^s \sum_{i=1}^N \big \{ R\big(C_i({\vz_1}) \setminus C_i({\vz_1}')\big) + R\big(C_i({\vz_2}) \setminus C_i({\vz_2}')\big) \big \}.
 \end{split}
 \label{eq: delta_difference_bound}
\end{equation}
Hence, local Lipschitz continuity of $\delta_s$ follows from the next lemma, whose proof is postponed after the proof of Theorem \ref{thm: Hadamard_diff} (i). 
\begin{lemma}
\label{lem: quant_stability}
Pick any $i \in \{ 1,\dots, N \}$. 
Under Assumption \ref{asp: cont}, for every finite $U > 0$, there exists a constant $\ell_U$ such that $R\big(C_i(\vz) \setminus C_i(\vz')\big) \le \ell_U \| \vz-\vz' \|$ whenever $\|\vz\| \vee \| \vz'\| \le U$.
\end{lemma}

 


Now, for every $\vh_1^t \to \vh_1 \in \R^N$ and $\vh_2^t \to \vh_2 \in \R^N$ as $t \downarrow 0$, we have
\begin{align*}
&\lim_{t \downarrow 0}  \left\{\frac{\delta_s(\vz^* + t\vh_1^t,\vz^* + t\vh_2^t) - \delta_s(\vz^*,\vz^*)}{t} - [\delta_s]'_{(\vz^*,\vz^*)}(\vh_1,\vh_2) \right \} \\
&= \lim_{t \downarrow 0} \left \{\frac{\delta_s(\vz^* + t\vh_1,\vz^* + t\vh_2) - \delta_s(\vz^*,\vz^*)}{t} - [\delta_s]'_{(\vz^*,\vz^*)}(\vh_1,\vh_2) \right \} \\
&\quad + \lim_{t \downarrow 0}\frac{\delta_s(\vz^* + t\vh_1^t,\vz^* + t\vh_2^t) - \delta_s(\vz^*+t\vh_1,\vz^*+t\vh_2)}{t}\\
& = 0+\lim_{t \downarrow 0} O(\|\vh_1^t - \vh_1\| + \|\vh_2^t - \vh_2\|) = 0.
\end{align*}
This completes the proof.
\qed

\begin{proof}[Proof of Lemma \ref{lem: thin mass}]
The proof is partially inspired by that of  Lemma 2 in \cite{chernozhukov2017detailed}. We first note that if $h_{1,j} - h_{1,i} = h_{2,j} - h_{2,i}$, then
\[
R\big(D_{ij}(\vh_1,\vh_2,t)\big) = R\big(D_{ji}(\vh_1,\vh_2,t)\big) = 0,
\]
so there is nothing to prove. Consider the case where $h_{1,j} - h_{1,i} \ne h_{2,j} - h_{2,i}$. Assume without loss of generality that $h_{2,j}-h_{2,i} > h_{1,j} - h_{1,i}$, for which $D_{ji}(\vh_1,\vh_2,t) = \varnothing$.  We shall show that 
\[
R\big(D_{ij}(\vh_1,\vh_2,t)\big) = \frac{t (h_{2,j}-h_{2,i} - h_{1,j}+h_{i,1})}{\|\vx_i - \vx_j\|} R^{+}(D_{ij}) + o(t).
\]
We divide the remaining proof into two steps. 

\medskip


\underline{Step 1}. 
We first show that 
\[
R(D_{ij}\big(\vh_1,\vh_2,t)\big) = R\big(\tilde D_{ij}(\vh_1,\vh_2,t)\big) + O(t^2),
\]
where 
\[
\tilde D_{ij}(\vh_1, \vh_2, t) :=\left \{\vy_0 + \tau \vv : \vy_0 \in D_{ij},\, \frac{t(h_{1,j} - h_{1,i})}{\|\vx_i - \vx_j\|}\leq \tau \leq \frac{t(h_{2,j}- h_{2,i})}{\|\vx_i - \vx_j\|} \right\}
\]
and $\vv = (\vx_i - \vx_j)/\|\vx_i - \vx_j\|$, the unit normal vector to the hyperplane $H_{ij} := \{ \vy : \langle \vx_i-\vx_j,\vy \rangle = b_{ij} \}$ containing $D_{ij}$. Recall that 
\[
D_{ij} = \Big (\bigcap_{k \ne i,j}\big \{ \vy : \langle \vx_i-\vx_k,\vy \rangle \ge b_{ik}  \big\} \Big) \cap \big\{ \vy: \langle \vx_i-\vx_j,\vy \rangle = b_{ij}  \big\}.
\]

Consider $\vy = \vy_0 + \tau\vv \in  \tilde{D}_{ij}(\vh_1,\vh_2,t) \setminus D_{ij}(\vh_1,\vh_2,t)$. As $\vy \notin D_{ij}(\vh_1,\vh_2,t)$, we have either $\vy \notin C_i(\vz^*+t\vh_1)$ or $\langle \vx_i-\vx_j,\vy \rangle -b_{ij} > t(h_{2,j}-h_{2,i})$, but the latter cannot hold because $\vy \in \tilde D_{ij}(\vh_1, \vh_2, t)$. So we must have $\vy \notin C_i(\vz^*+t\vh_1)$. Furthermore, since
\[
\begin{split}
\langle \vx_i - \vx_j, \vy \rangle - b_{ij} &= \underbrace{\langle \vx_i - \vx_j, \vy_0 \rangle}_{=b_{ij}} +\tau \underbrace{\langle \vx_i - \vx_j, \vv \rangle}_{=\| \vx_i-\vx_j \|} - b_{ij} \\
&=  \tau \|\vx_i - \vx_j\| \geq t(h_{1,j} - h_{1,i}),
\end{split}
\]
there exists $k \neq i, j$ such that
\[
t(h_{1,k} - h_{1,i}) > \langle \vx_i - \vx_k, \vy \rangle - b_{ik} \ge -t\underbrace{|\langle \vx_i-\vx_k,\vx_i-\vx_j \rangle| \big (|h_{1,j}-h_{1,i}| \vee |h_{2,j}-h_{2,i}|\big)}_{=:\ell_{ijk}}.
\]
Conclude that 
\[
\tilde D_{ij}(\vh_1, \vh_2, t) \setminus D_{ij}(\vh_1,\vh_2,t) \subset \bigcup_{k \ne i,j} \tilde{A}_{ijk}(t),
\]
where 
\[
\begin{split}
\tilde{A}_{ijk}(t) &= \big\{\vy: t(h_{1,j} - h_{1,i}) \leq  \langle \vx_i-\vx_j,\vy \rangle - b_{ij} \leq t(h_{2,j} - h_{2,i}) \big \} \\
&\quad \cap \big \{ \vy : -t \ell_{ijk} \le \langle \vx_i - \vx_k, \vy \rangle - b_{ik} \le t(h_{1,k} - h_{1,i}) \big \}.
\end{split}
\]
Arguing as in Step 1 in the proof of Theorem \ref{thm: Hadamard_diff}, we see that $R\big(\tilde{A}_{ijk}(t)\big) = O(t^2)$, so  $R\big(\tilde D_{ij}(\vh_1, \vh_2, t) \setminus D_{ij}(\vh_1,\vh_2,t)\big) = O(t^2)$. 

Next, we consider $D_{ij}(\vh_1,\vh_2,t) \setminus  \tilde D_{ij}(\vh_1,\vh_2,t)$. Recall the hyperplane $H_{ij}=:\{ \vy : \langle \vx_i-\vx_j,\vy \rangle = b_{ij} \}$. Since 
\[
D_{ij}(\vh_1,\vh_2,t) \subset \left \{\vy_0 + \tau \vv : \vy_0 \in H_{ij},\, \frac{t(h_{1,j} - h_{1,i})}{\|\vx_i - \vx_j\|}\leq \tau \leq \frac{t(h_{2,j}- h_{2,i})}{\|\vx_i - \vx_j\|} \right\}, 
\]
the projection of every $\vy \in D_{ij}(\vh_1,\vh_2,t) \setminus  \tilde D_{ij}(\vh_1,\vh_2,t)$ onto $H_{ij}$ falls outside of $D_{ij}$, i.e., $\vy$ can be decomposed as $\vy = \vy_0 + \tau \vv$ for some $\vy_0 \in H_{ij} \setminus D_{ij}$ and $\frac{t(h_{1,j} - h_{1,i})}{\|\vx_i - \vx_j\|}\leq \tau \leq \frac{t(h_{2,j}- h_{2,i})}{\|\vx_i - \vx_j\|}$. By definition, there exists $k \neq i,j$ such that
$\langle \vx_i - \vx_k, \vy_0 \rangle < b_{ik}$, which implies that 
$
 t(h_{1,k}-h_{1,j}) \le \langle \vx_i - \vx_k, \vy \rangle - b_{ik} < t\ell_{ijk}.
$
Hence,
\[
D_{ij}(\vh_1,\vh_2,t) \setminus  \tilde D_{ij}(\vh_1,\vh_2,t) \subset \bigcup_{k \ne i,j} \tilde{\tilde{A}}_{ijk}(t),
\]
where 
\[
\begin{split}
\tilde{\tilde{A}}_{ijk}(t) &= \big\{\vy: t(h_{1,j} - h_{1,i}) \leq  \langle \vx_i-\vx_j,\vy \rangle - b_{ij} \leq t(h_{2,j} - h_{2,i}) \big \} \\
&\quad \cap \big \{ \vy :  t(h_{1,k}-h_{1,j}) \le \langle \vx_i - \vx_k, \vy \rangle - b_{ik} \le t\ell_{ijk} \big \}.
\end{split}
\]
Again, arguing as in Step 1 in the proof of Theorem \ref{thm: Hadamard_diff}, we see that $R\big(\tilde{\tilde{A}}_{ijk}(t)\big) = O(t^2)$, so  $R\big( D_{ij}(\vh_1, \vh_2, t) \setminus \tilde D_{ij}(\vh_1,\vh_2,t)\big) = O(t^2)$. 
Conclude that 
\[
R\big( D_{ij}(\vh_1,\vh_2,t) \Delta \tilde D_{ij}(\vh_1,\vh_2,t) \big ) =  O(t^2).
\]


\medskip

\underline{Step 2}. In view of Step 1, it suffices to prove the desired conclusion with $D_{ij}(\vh_1,\vh_2,t)$ replaced by $\tilde{D}_{ij}(\vh_1,\vh_2,t)$. Observe that
\begin{align*}
    R\big(\tilde D_{ij}(\vh_1,\vh_2,t)\big) &= \int_{\tilde D_{ij}(\vh_1,\vh_2,t)} \rho(\vx) \, d\vx \\&=  \int^{\frac{t(h_{2,j} - h_{2,i})}{\|\vx_i - \vx_j\|}}_{\frac{t(h_{1,j} - h_{1,i})}{\|\vx_i - \vx_j\|}} \left ( \int_{D_{ij}} \rho(\vy + \tau\vv) \,d\cH^{d-1}(\vy)\right )\,d\tau\\
    &= t \int^{\frac{h_{2,j} - h_{2,i}}{\|\vx_i - \vx_j\|}}_{\frac{h_{1,j} - h_{1,i}}{\|\vx_i - \vx_j\|}} \left ( \int_{D_{ij}} \rho(\vy+ t\tau\vv) \,d\cH^{d-1}(\vy)\right )\,d\tau.
\end{align*}
Recall that $R^{+}(D_{ij}) = \int_{D_{ij}} \rho\,d\cH^{d-1}$.

We first consider the case where $\cH^{d-1}(\partial \cY \cap D_{ij}) = 0$. Since $\rho$ is continuous on $\cY$ by Assumption \ref{asp: cont} and vanishes on $\cY^c$, for every $\vy \in (\partial \cY)^c$ and $\tau \in \R$, we have $\rho(\vy + t\tau\vv) \to \rho(\vy)$ as $t \downarrow 0$.  
Combined with Lemma \ref{lem: DCT}, we may apply the dominated convergence theorem to conclude that
\[
R\big(\tilde D_{ij}(\vh_1,\vh_2,t)\big) = R^{+}(D_{ij})  \frac{t (h_{2,j}-h_{2,i}-h_{1,j}+h_{1,i})}{\|\vx_i - \vx_j\|} + o(t).
\]


Next, suppose that $\cH^{d-1}(\partial \cY \cap D_{ij}) > 0$, which entails $\cH^{d-1}(\partial \cY \cap H_{ij}) > 0$. By Assumption \ref{asp: cont}, this happens only when $\cY$ is polyhedral, in which case $H_{ij}$ is a supporting hyperplane to $\cY$ and hence $\inte (\cY) \cap H_{ij} = \varnothing$. 
So, we have either $\langle \vx_i-\vx_j,\vy \rangle - b_{ij}> 0$ for all $\vy \in \interior(\cY)$, or $\langle \vx_i-\vx_j,\vy \rangle - b_{ij}< 0$ for all $\vy \in \interior(\cY)$. This implies $p_i = R\big(C_i(\vz^*)\big) = 0$ or $p_j = R\big(C_j(\vz^*)\big) = 0$, both of which are contradictions. This completes the proof of Lemma \ref{lem: thin mass}.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem: quant_stability}]

The lemma follows from Lemma 5.5 in \cite{bansil2022quantitative}. For the sake of completeness, we provide a direct proof.  Set $\varepsilon_0 = \min_{1 \le i < j \le N} \| \vx_i-\vx_j \| > 0$. If $\vy \in C_i(\vz) \setminus C_i(\vz')$, then there exists $j \ne i$ such that
\[
z_j-z_i \le \langle \vx_i-\vx_j,\vy \rangle - b_{ij}(\vzero) < z_j'-z_i'.
\]
Hence,
\[
C_i(\vz) \setminus C_i(\vz') \subset \bigcup_{j \ne i} \big \{ \vy : z_j-z_i \le \langle \vx_i-\vx_j,\vy \rangle - b_{ij}(\vzero) < z_j'-z_i' \big \}.
\]
Pick and fix any $j \ne i$.
Consider the hyperplane $H = \{ \vy : \langle \vx_i-\vx_j,\vy \rangle = b_{ij}(\vzero) \}$ and the associated unit normal vector $\vv = (\vx_i-\vx_j)/\|\vx_i-\vx_j\|$. Then,
\begin{equation}
\begin{split}
&R\Big ( \big \{ \vy : z_j-z_i \le \langle \vx_i-\vx_j,\vy \rangle - b_{ij}(\vzero) < z_j'-z_i' \big \} \Big ) \\
&= \int_{\frac{z_j-z_i}{\|\vx_i-\vx_j\|}}^{\frac{z_j'-z_i'}{\|\vx_i-\vx_j\|}} \left ( \int_{H} \rho(\vy + t\vv) \, d\cH^{d-1}(\vy) \right )\, dt.
\end{split}
\label{eq: stability}
\end{equation}
From the proof of Lemma \ref{lem: DCT}, the inner integral is bounded by 
\[
\| \rho \|_{\infty} \underbrace{\cH^{d-1}(H \cap \cY^{t_0})}_{<\infty}
\]
with 
\[
t_0 := \frac{2U}{\varepsilon_0} \ge \frac{|z_j-z_i| \vee | z_j'-z_i' |}{\|\vx_i-\vx_j\|}.
\]
Hence, the right-hand side on (\ref{eq: stability}) is bounded by
\[
\frac{|z_j'-z_j| + |z_i'-z_i|}{\varepsilon_0} \, \| \rho \|_{\infty}\cH^{d-1}(H\cap \cY^{t_0}).
\]
This leads to the desired result. 
\end{proof}


\subsubsection{Proof of Theorem \ref{thm: Hadamard_diff} (ii)}
Frech\'{e}t differentiability is equivalent to Hadamard differentiability for a function defined on an open subset of a Euclidean space, so we shall prove Hadamard differentibility of $\gamma_{\bm{\varphi}}$. 
We continue using the same notation as in Part (i). 
Observe that
\[
\gamma_{\bm{\varphi}}(\vz^* + t\vh) - \gamma_{\bm{\varphi}}(\vz^*) = \sum_{1 \le i \neq j \le N} \int_{C_i(\vz^*) \cap C_j(\vz^* + t\vh)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \ dR(\vy).
\]

Let $\vh \in \R^N$. 
Pick any fix any $i,j \in \{1,\dots,N \}$ with $i \ne j$. 
We have $C_i(\vz^*) \cap C_j(\vz^* + t\vh) \subset D_{ij}(\vzero,\vh,t)$, and only one of $D_{ij}(\vzero,\vh,t)$ and $D_{ji}(\vzero,\vh,t)$ is nonempty. Assume without loss of generality that $D_{ij}(\vzero,\vh,t)$ is nonempty. From the proof of Part (i), we know that $R\big(D_{ij}(\vzero,\vh,t) \setminus  C_j(\vz^* + t\vh)\big) = O(t^2)$, so that using the fact that $\bm{\varphi}$ is bounded, we have
\begin{align*}
    \int_{C_i(\vz^*) \cap C_j(\vz^* + t\vh)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \, dR(\vy) =\int_{D_{ij}(\vzero,\vh,t)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \, dR(\vy) + O(t^2).
\end{align*}


Next, set
\[
\tilde D_{ij}(\vh, t) :=\left \{\vy_0 + \tau \vv : \vy_0 \in D_{ij}, 0 \leq \tau \leq \frac{t(h_{j}- h_{i})}{\|\vx_i - \vx_j\|} \right\}
\]
with $\vv = (\vx_i - \vx_j)/\|\vx_i - \vx_j\|$. Arguing as in Step 1 in the proof of Lemma \ref{lem: thin mass} above, we see that $R(D_{ij}(\vzero,\vh,t) \Delta \tilde D_{ij}(\vh,t)) = O(t^2)$, so that 
\begin{align*}
&\int_{D_{ij}(\vzero,\vh,t)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \, dR(\vy) \\
&= \int_{\tilde D_{ij}(\vh,t)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \, dR(\vy) + O(t^2)\\
&= t \int_0^{\frac{h_j - h_i}{\|\vx_i - \vx_j\|}} \left ( \int_{D_{ij}} \langle \vx_j - \vx_i,\bm{\varphi}(\vy + t\tau \vv) \rangle  \rho(\vy + t\tau \vv) \,d\cH^{d-1}(\vy)\right ) \,d\tau + O(t^2). 
\end{align*}
Since the set of discontinuities of $\bm{\varphi}$ has Hausdorff dimension strictly less than $d-1$, we have
\[
\begin{split}
&\int_0^{\frac{h_j - h_i}{\|\vx_i - \vx_j\|}} \left ( \int_{D_{ij}} \langle \vx_j - \vx_i,\bm{\varphi}(\vy + t\tau \vv) \rangle  \rho(\vy + t\tau \vv) \,d\cH^{d-1}(\vy)\right ) \,d\tau \\
&= \frac{h_j - h_i}{\|\vx_i - \vx_j\|} \int_{D_{ij}}  \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \rho(\vy) \, d\cH^{d-1}(\vy) + o(1).
\end{split}
\]
Conclude that 
\[
\begin{split}
&\int_{C_i(\vz^*) \cap C_j(\vz^* + t\vh)} \langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle \, dR(\vy)\\ 
&\quad = \frac{t(h_i - h_j)}{\|\vx_i - \vx_j\|} \int_{D_{ij}}  \langle \vx_i - \vx_j, \bm{\varphi}(\vy) \rangle \rho(\vy) \, d\cH^{d-1}(\vy) + o(t). 
\end{split}
\]


We have shown the Gateaux differentiability for $\gamma_{\bm{\varphi}}$, 
\[
\lim_{t \downarrow 0} \frac{\gamma_{\bm{\varphi}}(\vz^* + t\vh) - \gamma_{\bm{\varphi}}(\vz^*)}{t} = \underbrace{\sum_{1 \le i < j \le N}\frac{h_i - h_j}{\|\vx_i - \vx_j\|} \int_{D_{ij}}  \langle \vx_i - \vx_j, \bm{\varphi}(\vy) \rangle \rho(\vy) \, d\cH^{d-1}(\vy)}_{=[\gamma_{\bm{\varphi}}]'_{\vz^*}(\vh)}.
\]
It remains to show that $\gamma_{\bm{\varphi}}$ is locally Lipschitz. Observe that
\begin{align*}
    |\gamma_{\bm{\varphi}}(\vz) - \gamma_{\bm{\varphi}}(\vz')| &\leq \sum_{1 \le i \neq j \le N} \int_{C_i(\vz) \cap C_j(\vz')} |\langle \vx_j - \vx_i, \bm{\varphi}(\vy) \rangle|\ dR(\vy)\\
    &\leq \big \| \| \bm{\varphi} \| \big \|_{\infty} \max_{1 \le i<j \le N} \|\vx_j - \vx_i\| \sum_{1 \le i \ne j \le N} R\big(C_i(\vz) \cap C_j(\vz')\big),
\end{align*}
and 
\[
\sum_{1 \le i \ne j \le N} R\big(C_i(\vz) \cap C_j(\vz')\big) = \sum_{i=1}^N R\big(C_i(\vz) \setminus C_i(\vz')\big).
\]
Lemma \ref{lem: quant_stability} now guarantees local Lipschitz continuity of $\gamma_{\bm{\varphi}}$, completing the proof.
\qed

\subsection{Proofs for Section \ref{sec: limit}}
\begin{proof}[Proof of Lemma \ref{lem: kitagawa}]
We will use several results in \cite{kitagawa2019convergence}. Conditions (Reg), (Twist), and (QC) in \cite{kitagawa2019convergence} hold, and and $c$-convexity in \cite{kitagawa2019convergence} reduces to standard convexity in our setting; cf. the discussion after Corollary 4.3 in \cite{del2022central}. Condition (PW) in \cite{kitagawa2019convergence} is guaranteed by Assumption \ref{asp: poincare}.

By  duality theory for OT problems, for every $\bm{q} \in \cQ$, the set of optimal solutions $\cZ^*(\bm{q})$ to the dual problem (\ref{eq: dual}) is nonempty.
Theorem 1.4 in \cite{kitagawa2019convergence} shows that the function $\Phi (\cdot,\bm{q})$ is strictly concave on $\langle \vone \rangle \times \cZ_+$ with $\cZ_+ = \big\{ \vz : R\big(C_i(\vz)\big) > 0 \ (\forall i \in \{ 1,\dots, N \} \big\}$. If $\bm{q} \in \cQ_+$, then for every $\vz \in \cZ^*(\bm{q})$, we have $R\big(C_i(\vz)\big) = q_i > 0$ for every $i \in \{ 1,\dots, N-1 \}$ and $R\big(C_{N}(\vz)\big) = 1-\langle \bm{q},\vone \rangle > 0$, so that $\cZ^*(\bm{q}) \subset \langle \vone \rangle \times \cZ_+$. Hence, for every $\bm{q} \in \cQ_+$, $\cZ^*(\bm{q})$ is a singleton, $\cZ^*(\bm{q}) = \{ \vz^*(\bm{q}) \}$. 

Next, since the last coordinate of $\vz \in \langle \vone \rangle^{\bot}$ is determined by the first $N-1$ coordinates, $z_N = -\langle \vz^{-N},\vone \rangle$ with $\vz^{-N}=(z_1,\dots,z_{N-1})^{\intercal}$, the restriction of the map $\vz \mapsto \Phi(\vz,\bm{q})$ on $\langle \vone \rangle^{\bot}$ can be thought of as a function of $\vz^{-N}$. Set $\tilde{\Phi} (\vz^{-N},\bm{q}) = \Phi (\vz^{-N}, -\langle \vz^{-N},\vone \rangle,\bm{q})$. 
By Theorems 1.1 and 5.1 in \cite{kitagawa2019convergence}, for every $\bm{q} \in \cQ_{+}$, the map $\vz^{-N} \mapsto \tilde{\Phi}(\vz^{-N},\bm{q})$ is concave and twice continuously differentiable, and its Hessian is invertible in a neighborhood of $\vz^{*,-N}(\bm{q}) = (z_1^*(\bm{q}),\dots,z_{N-1}^*(\bm{q}))^{\intercal}$ (invertibility of the Hessian follows because $\cZ_+$
is open by Proposition 2.3 in \cite{kitagawa2019convergence} and $\vz^*(\bm{q}) \in \cZ_+$ when $\bm{q} \in \cQ_+$). Since $\vz^{*,-N}(\bm{q})$ is the unique root of $\nabla_{\vz^{-N}}\tilde{\Phi}(\vz^{-N},\bm{q}) = \vzero$ for every $\bm{q} \in \cQ_+$, the implicit function theorem guarantees continuous differentiability of the map $\bm{q} \mapsto \vz^{*,-N}(\bm{q})$ on $\cQ_+$. Finally, since $z^*_N(\bm{q}) = -\langle \vz^{*,-N}(\bm{q}), \vone \rangle$, the map $\bm{q} \mapsto \vz^{*}(\bm{q})$ is continuously differentiable on $\cQ_+$. 
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop: limit}]
The proposition follows from combining the central limit theorem for $\hat{\vz}_n$ in (\ref{eq: CLT}), the differentiability results in Theorem \ref{thm: Hadamard_diff}, and the extended delta method (Lemma \ref{lem: functional delta method}). 
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop: bootstrap consistency}]
By the conditional central limit theorem for the bootstrap (cf. Theorem 23.4 in \cite{vanderVaart1998asymptotic}), the conditional law of $\sqrt{n}(\hat{\vp}_n^{B,-N}-\hat{\vp}_n^{-N})$ given the sample converges weakly to $\cN(\vzero,A_P)$ in probability.
The delta method for the bootstrap (cf. Theorem 23.5 in \cite{vanderVaart1998asymptotic}) yields that the conditional law of $\sqrt{n}(\hat{\vz}_n^B-\hat{\vz}_n)$ given the sample converges weakly to $\cN(\vzero,\Sigma_{P})$ in probability, where $\Sigma_P = B_P^{\intercal}A_PB_P$.
Given this, Part (ii) follows from another application of the delta method for the bootstrap. So we focus on proving Part (i). 

We first observe that
\[
\begin{pmatrix}
\sqrt{n}(\hat{\vz}_n^B-\hat{\vz}_n) \\
\sqrt{n}(\hat{\vz}_n-\vz^*)
\end{pmatrix}
\stackrel{d}{\to}
\cN \left ( \vzero, \begin{pmatrix} \Sigma_P & \vzero \\ \vzero & \Sigma_P 
\end{pmatrix}
\right)
\]
unconditionally (cf. Problem 23.8 in \cite{vanderVaart1998asymptotic}), which implies that
\[
\begin{pmatrix}
\sqrt{n}(\hat{\vz}_n^B-\vz^*) \\
\sqrt{n}(\hat{\vz}_n-\vz^*)
\end{pmatrix}
\stackrel{d}{\to}
\cN \left ( \vzero, \begin{pmatrix} 2\Sigma_P & \Sigma_P \\ \Sigma_P & \Sigma_P 
\end{pmatrix}
\right)
.
\]
From this, the extended delta method (Lemma \ref{lem: functional delta method}) yields 
\[
\sqrt{n}\|\hat{\vT}_n^B - \hat{\vT}_n \|_{L^s(R)}^s = \sqrt{n}\delta_s(\hat{\vz}_n^B,\hat{\vz}_n) = [\delta_s]_{(\vz^*,\vz^*)}'\big( \sqrt{n}(\hat{\vz}_n^B-\vz^*),\sqrt{n}(\hat{\vz}_n-\vz^*) \big) + \varepsilon_n
\]
with $\varepsilon_n \to 0$ in probability. From the expression (\ref{eq: h_deriv}) of the derivative, we see that
\[
[\delta_s]_{(\vz^*,\vz^*)}'\big( \sqrt{n}(\hat{\vz}_n^B-\vz^*),\sqrt{n}(\hat{\vz}_n-\vz^*) \big) = \underbrace{[\delta_s]_{(\vz^*,\vz^*)}'\big( \sqrt{n}(\hat{\vz}_n^B-\hat{\vz}_n),\vzero \big)}_{=: S_n^B}.
\]
By the continuous mapping theorem, the conditional law of $S_n^B$ given the sample converges weakly to the limit law in (\ref{eq: limit law}) in probability. Let $S$ be a random variable following the limit law in (\ref{eq: limit law}). We will show that
\[
\sup_{g \in BL_1(\R)} \Big |\E\big[ g(S_n^B + \varepsilon_n) \mid \vX_1,\dots,\vX_n \big] - \E[g(S)] \Big | \to 0
\]
in probability, which leads to the conclusion of Part (i). For every $g \in BL_1(\R)$, 
\[
|g(S_n^B + \varepsilon_n) - g(S_n^B)| \le 2 \wedge |\varepsilon_n|.
\]
By Markov's inequality, we have 
\[
\E\big[ 2 \wedge |\varepsilon_n| \mid \vX_1,\dots,\vX_n \big] \to 0
\] 
in probability. Now, we have
\[
\begin{split}
&\sup_{g \in BL_1(\R)}
|\E\big[ g(S_n^B + \varepsilon_n) \mid \vX_1,\dots,\vX_n \big] - \E[g(S)] \Big | \\
&\le \sup_{g \in BL_1(\R)}
|\E\big[ g(S_n^B) \mid \vX_1,\dots,\vX_n \big] - \E[g(S)] \Big | \\
&\quad + \E\big[ 2 \wedge |\varepsilon_n| \mid \vX_1,\dots,\vX_n \big].
\end{split}
\]
Both terms on the right-hand side converge to zero in probability. This completes the proof. 
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop: pointwise}]
 Since $K \subset \interior (C_i(\vz^*))$ is compact, we have
    \[
    \varepsilon_0 := \min_{j \ne i} \inf_{\vy \in K} \big \{ \langle \vx_i - \vx_j, \vy \rangle - d_{ij} \big\} > 0.
    \]
    Now, for every $j \ne i$,
    \[
    \begin{split}
    \langle \vx_i - \vx_j, \vy \rangle - d_{ij} (\hat{\vz}_n) &= \langle \vx_i - \vx_j, \vy \rangle -d_{ij} - (\hat{z}_{n,i}-z_i^*) + (\hat{z}_{n,j}-z_j^*) \\
    &\ge \varepsilon_0 - 2\max_{1 \le k \le N} |\hat{z}_{n,k} - z_{k}^*|. 
    \end{split}
    \]
    Hence, combining (\ref{eq: CLT}), we have
    \[
    \begin{split}
    \PP\big( \hat{\vT}_n(\vy) = \vT^*(\vy),
  \forall \vy \in K\big) &\ge \Prob \big ( K \subset \inte(C_i(\hat{\vz}_n)) \big) \\
  &\ge \Prob \left ( \max_{1 \le k \le N} |\hat{z}_{n,k} - z_{k}^*| <  \varepsilon_0/2 \right )  \to 1,
  \end{split}
    \]
    as desired.
\end{proof}

\subsection{Derivation of (\ref{eq: average coverage})}
\label{sec: average}

Assume that the $L^1$-confidence set (\ref{eq: L1 confidence set}) has coverage probability $1-\alpha+o(1)$ for every $\alpha \in (0,1)$. Let $\vY \sim R$ be independent of the sample. Define the event $A = \big \{ \sqrt{n}\| \hat{\vT}_n - \vT^* \|_{L^1(R)}  \le \hat{\tau}_n(1-\alpha/2) \big \}$. Then, we have 
\[
\begin{split}
&1-\int \Prob \big ( \vT^*(\vy) \in \cC_n^{1-\alpha}(\vy) \big) \, dR(\vy) \\ &= \Prob \big ( \vT^*(\vY) \notin \cC_n^{1-\alpha}(\vY) \big) \\
&=\Prob \left ( \sqrt{n}\| \hat{\vT}_n (\vY) - \vT^*(\vY) \| > \hat{\tau}_n(1-\alpha/2) \cdot \frac{2}{\alpha} \right ) \\
&\le \Prob \left ( \left \{ \sqrt{n}\| \hat{\vT}_n (\vY) - \vT^*(\vY) \| > \hat{\tau}_n(1-\alpha/2) \cdot \frac{2}{\alpha} \right \} \cap A \right ) + \Prob (A^c).
\end{split}
\]
By construction, $\Prob(A^c) = \alpha/2 + o(1)$. Furthermore, on the event $A$,  the Markov inequality yields 
\[
\begin{split}
&\Prob \left ( \left \{ \sqrt{n}\| \hat{\vT}_n (\vY) - \vT^*(\vY) \| > \hat{\tau}_n(1-\alpha/2) \cdot \frac{2}{\alpha} \right \} \mid \vX_1,\dots,\vX_n \right ) \\
&\le \frac{\sqrt{n}\| \hat{\vT}_n - \vT^* \|_{L^1(R)}}{\hat{\tau}_n(1-\alpha/2)} \cdot \frac{\alpha}{2} \le \frac{\alpha}{2}.
\end{split}
\]
Conclude that 
\[
1-\int \Prob \big ( \vT^*(\vy) \in \cC_n^{1-\alpha}(\vy) \big) \, dR(\vy) \le \alpha+o(1). 
\]

\section{Concluding remarks}
\label{sec: conclusion}

In this paper, we have established limit theorems for the $L^s$-estimation errors and linear functionals of the empirical OT map in the semidiscrete setting. The main ingredient of the proof is new stability estimates of these functionals with respect to the dual potential vector, whose derivation requires the careful analysis of the facial structures of the Laguerre cells. For both functionals, we have established the consistency of the nonparametric bootstrap. These results enable constructing confidence sets and bands for the OT map as well as confidence intervals for the linear functionals. 

\appendix 



\section{Hadamard differentiability and extended delta method}
\label{sec: functional delta}
 
In this appendix, we briefly review Hadamard differentiability and the extended delta method. Basic references include \cite{van1996weak,vanderVaart1998asymptotic, romisch2004}.

Let $\mathfrak{D}$ be a normed space and $\phi$ be a real function defined on an open subset $\Theta$ of $\mathfrak{D}$. We say that $\phi$ is 
\textit{Hadamard directionally differentiable}  at $\theta \in \Theta$  if there exists a function $\phi'_{\theta}: \mathfrak{D} \to \R$ such that 
\begin{equation}
\lim_{n \to \infty} \frac{\phi(\theta_{n}) - \phi(\theta)}{t_n} = \phi_{\theta}'(h)
\label{eq: H derivative}
\end{equation}
for every sequence of positive reals $t_n \downarrow 0$ and every sequence $\theta_n \in \Theta$ with $t_n^{-1}(\theta_{n} -\theta) \to h$ as $n \to \infty$.
The derivative $\phi_{\theta}'$ is automatically continuous (cf. Proposition 3.1 in \cite{shapiro1990concepts}) and positively homogeneous but need not be linear. 
If the derivative $\phi_{\theta}'$ is linear, then we say that $\phi$ is \textit{Hadamard differentiable}  at $\theta$. When $\mathfrak{D}$ is finite-dimensional, Hadamard differentiability is equivalent to Frech\'{e}t differentiability; cf. Example 3.9.2 in \cite{van1996weak}. 



\begin{lemma}[extended delta method; \cite{romisch2004}]
\label{lem: functional delta method}
Consider the above setting. Let $T_n: \Omega \to \Theta$ be maps such that $r_n (T_n - \theta) \stackrel{d}{\to} T$ for some norning sequence $r_n \to \infty$ and Borel measurable map $T: \Omega \to \mathfrak{D}$ with values in a separable subset of $\mathfrak{D}$. Then, we have  $r_n \big(\phi(T_n) - \phi(\theta)\big) - \phi_{\theta}'(r_n(T_n-\theta)) \to 0$ in outer probability. In particular, $r_n \big(\phi(T_n) - \phi(\theta)\big) \stackrel{d}{\to} \phi_{\theta}'(T)$. 
\end{lemma}






\bibliographystyle{alpha}
\bibliography{ref.bib}

\end{document}
