\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsthm,amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbold}
%\usepackage{algpseudocode}
\usepackage{array}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage[normalem]{ulem}

\usepackage{todonotes}
\usepackage{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\begin{document}

\title{Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios\\
{}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}


\author{
    \IEEEauthorblockN{Yi Tian Xu\IEEEauthorrefmark{1},  Jimmy Li\IEEEauthorrefmark{1}, Di Wu\IEEEauthorrefmark{1}, Michael Jenkin\IEEEauthorrefmark{1}, Seowoo Jang\IEEEauthorrefmark{2}, Xue Liu\IEEEauthorrefmark{1}, and Gregory Dudek\IEEEauthorrefmark{1}}
    \IEEEauthorblockA{\IEEEauthorrefmark{1}Samsung AI Center Montreal, Canada
    \\\{yitian.xu, jimmy.li, di.wu1, m.jenkin, steve.liu, greg.dudek\}@samsung.com}
    \IEEEauthorblockA{\IEEEauthorrefmark{2}Samsung Electronics, Korea (South), seowoo.jang@samsung.com}
    }

%Montreal, Canada \\




\maketitle

\begin{abstract}
%With the continuous growth in communication network complexity and traffic volume, automated data-driven solutions for optimizing network performance are receiving increasing attention in recent years. In particular, reinforcement learning (RL) has been widely studied for load balancing and has demonstrated impressive performance gain over traditional rule-based methods. However, 
%Pure reinforcement learning (RL) methods generally require an enormous amount of data to train, and generalize poorly to scenarios that are not encountered during training. 
% RESPOND TO REVIEWER 1: In abstract, the authors mention that “In this paper we study the use of pre-trained reinforcement learning policies for wireless network optimization.” However, in the manuscript I don’t see sufficient discussion on this.
% In this paper we study the use of pre-trained reinforcement learning policies for wireless network optimization. 
With the continuous growth in communication network complexity and traffic volume, communication load balancing solutions are receiving increasing attention. Specifically, reinforcement learning (RL)-based methods have shown impressive performance compared with traditional rule-based methods. However, standard RL methods generally require an enormous amount of data to train, and generalize poorly to scenarios that are not encountered during training. We propose a policy reuse framework in which a policy selector chooses the most suitable pre-trained RL policy to execute based on the current traffic condition. Our method hinges on a policy bank composed of policies trained on a diverse set of traffic scenarios. When deploying to an unknown traffic scenario, we select a policy from the policy bank based on the similarity between the previous-day traffic of the current scenario and the traffic observed during training. Experiments demonstrate that this framework can outperform classical and adaptive rule-based methods by a large margin.

% \textcolor{blue}{Tianyu: How novel are these traffic scenarios? I feel the word "novel" here is not very appropriate.} 

% The mobile communication traffic has been growing very quickly in recent years. Both the number of mobile device and traffic demand per device are growing very rapidly. Besides the fast increase of the total amount, the traffic load is also distributed very evenly. With such challenges, the load balancing of communication systems has became more challenging. In recent years, reinforcement learning (RL) based methods have been studied and investigated to implement efficient load balancing. However, most of these ignore the changes of for the traffic patterns of real-world scenarios. In this work, we proposed a policy identifier based load balancing framework for load balancing for devices in both active mode and idle mode. Specifically, we first train a set of policies on existing traffic scenarios and use this trained RL policies to form a policy bank for future use. Then, we present to use the state action pairs to identify which policy we should use when a new (unseen) traffic scenario is encountered. Experiment results based on the results traffics have shown that the proposed methods have shown impressive performance and can outperform other load balancing methods by a large margin.


\end{abstract}

\begin{IEEEkeywords}
    load balancing, reinforcement learning, policy reuse
\end{IEEEkeywords}

\input{introduction.tex}
%\input{related-works.tex}
\input{background.tex}
\input{methodology.tex}
\input{experiment.tex}

\input{conclusion.tex}





%\section*{Acknowledgment}
%\textcolor{red}{acknowledgment}

{
\bibliographystyle{IEEEtran}
\bibliography{main.bib}
}
\end{document}










