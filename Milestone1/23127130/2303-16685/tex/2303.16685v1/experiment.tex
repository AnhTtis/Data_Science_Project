\section{Experimental Results}\label{exp}

\input{data-analysis.tex}

\subsection{Policy selector training}
% \subsection{Policy selector experiment setup}
\label{sec:policy-sector-training}
We now describe the training process of our policy selector. After constructing the policy bank $\Pi$ as in Section~\ref{sec:policy-bank-analysis}, we run BasicLB and each policy $\pi\in \Pi$ on each of the training scenarios in $\mathcal{X}$ to collect the data used to train the policy selector.
% We now describe implementation details of our policy selection method. 
%On the very first day in the simulation scenarios, our policy selector has no data that can be used to select a policy. Therefore, we use a rule-based method to balance the load on the first day. 
% The data that we have collected to train our policy selector is generated by running BasicLB and each policy in the policy bank $\Pi$ on each of the 9 traffic scenarios in $\mathcal{X}$. 
Specifically, we run each policy $\pi\in \Pi$ on each scenario $X\in\mathcal{X}$ for one week and we collect the traffic condition data at each hour. In addition, we repeat this process by running BasicLB on each each scenario $X\in\mathcal{X}$ for one week. We use the data generated by BasicLB as part of the training set since we need to rely on the rule-based method to perform load balancing on the first day, as there is no data that can be used to select a policy. 
% \textcolor{gray}{tianyu: I think here we need to make it clear on how we used both rule-based lb and the policy in the policy bank to collect data, i.e. when do we use what policy} 
In total, we have gathered 15.12K samples corresponding to the hourly traffic condition. These samples are reformatted using a sliding window algorithm to create $T=24$ hour data samples. By randomly selecting 30\% of the samples as our validation set, we use cross-validation to choose
% \textcolor{blue}{tianyu:no the here} 
hyperparameters of the 
% \textcolor{blue}{tianyu:, and need to add an "of the" here} 
policy selector, as discussed in~\ref{sec:policy-selector}.

During evaluation, we bring our policy selector online. For each evaluation scenario,
% in our test set $\mathcal{X}'$
we first run BasicLB to obtain one day of data to initiate the policy selection process. Then, at the beginning of each new day, we feed the data from the previous day to the policy selector to obtain a selected policy to run on that new day. 

\subsection{Performance evaluation}
\label{sec:policy-sector-result}
We evaluate our proposed policy reuse framework and the policy selector on fixed and transient traffic scenarios.
%This section shows the performance evaluation results of our proposed policy reuse framework and the policy selector in fixed and transient traffic scenarios. 
% The last part explores the possibility of improving the performance by fine-tuning the select policy with a small amount of experience interaction under the new scenario. 

\subsubsection{Fixed traffic scenario}
\label{sec:fix-traffic-scenario}

\begin{table}[t]
\centering
\caption{Average performance over 6 days and all training scenarios.}
\label{tab:train-res-kpis}
\begin{tabular}{l|c c c c c}
\toprule
& Reward & {\bf $G_{avg}$} & {\bf $G_{min}$} & {\bf$ G_{sd}$} & {\bf $G_{cong}$}\\
\midrule
{\bf BEST/NEW-$\pi$} & 0.479 & 3.600 & 2.246 & 1.487 & 0.889\\
\midrule
{\bf BasicLB} & 0.401 & 3.033 & 1.680 & 2.190 & 0.837\\
{\bf AdaptLB} & 0.438 & 3.228 & 1.990 & 1.851 & 0.847\\
{\bf RAND-$\pi$} & 0.447 & 3.425 & 2.013 & 1.724 & 0.862\\
{\bf Policy selector} & {\bf 0.479} & {\bf 3.600} & {\bf 2.246} & {\bf 1.487} & {\bf 0.889}\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Average performance over 6 days and all testing scenarios.}
\label{tab:test-res-kpis}
\begin{tabular}{l|c c c c c}
\toprule
& Reward & {\bf $G_{avg}$} & {\bf $G_{min}$} & {\bf $G_{sd}$} & {\bf $G_{cong}$}\\
\midrule
{\bf BEST-$\pi$} & 0.452 & 3.399 & 2.016 & 1.680 & 0.887\\
{\bf NEW-$\pi$} & 0.456 & 3.365 & 2.057 & 1.631 &	0.889\\
\midrule
{\bf BasicLB} & 0.403 & 3.036 & 1.646 & 2.204 & 0.854\\
{\bf AdaptLB} & 0.422 & 3.144 & 1.834 & 1.936 & 0.847\\
{\bf RAND-$\pi$} & 0.426 & 3.245 & 1.847 & 1.822 & 0.855\\
{\bf Policy selector} & {\bf 0.446} & {\bf 3.355} & {\bf 2.010} & {\bf 1.692} & {\bf 0.867}\\
\bottomrule
\end{tabular}
\end{table}

This experiment tests each scenario in $\mathcal{X}\bigcup\mathcal{X}'$ independently for a simulation period of one week.
% This experiment is conducted on all training and testing traffic scenarios for one week. 
For all methods, including the baselines, BasicLB is applied on the first day. Tables~\ref{tab:train-res-kpis} and~\ref{tab:test-res-kpis} shows the comparison of the average performance over the remaining 6 days. Overall, our policy selector outperforms BasicLB or AdaptLB by 20.33\% and 9.84\%, respectively, on the training scenarios ($\mathcal{X}$), and by 10.26\% and 5.24\%, respectively, on the test scenarios ($\mathcal{X}'$). Furthermore, it achieves on average the closest performance to BEST-$\pi$ and NEW-$\pi$ upper bounds compared to the other baselines. 

Recall that BEST-$\pi$ is not a feasible solution to be deployed in a real network as it requires all policies in $\Pi$ to be applied to the scenario. It can be considered as a performance upper bound for the policy reuse framework. Similarly, NEW-$\pi$, which trains a new RL policy on the unseen traffic scenario, can also be considered as another performance upper bound. For the training scenarios, NEW-$\pi$ and BEST-$\pi$ are equivalent since the policy with the best performance in $\Pi$ for any scenario $X\in \mathcal{X}$ is also the policy trained on $X$. For the testing scenarios, as expected, NEW-$\pi$ is better than BEST-$\pi$, but only by 0.94\% in terms of reward as shown in Table~\ref{tab:test-res-kpis}. Compared to our policy selector, our policy selector achieves an accuracy of 100\%, reaching the two upper bound performance for all training scenarios in $\mathcal{X}$. For the testing scenarios, BEST-$\pi$ and NEW-$\pi$ are on average only 1.21\% and 2.16\% higher than our proposed method, respectively. This demonstrates that our policy reuse framework can efficiently be used to avoid training on unseen scenarios without significant loss in performance.

Figure~\ref{fig:reward_results} shows the detailed performance comparison of the average reward for each scenario.
For certain test scenarios in $\mathcal{X}'$, especially in Group 2, BasicLB or AdaptLB achieves the best performance. 
%As follows from the discussion of Figure~\ref{fig:rl_policy_21_sec_test}, 
Group 2 includes some scenarios that are relatively more difficult to optimize. However, our policy selector can outperform RAND-$\pi$ for all scenarios in Group 2, demonstrating the effectiveness of choosing the policy based on the similarity of the traffic condition. 

% the comparison of the average reward over the remaining 6 days. 
%We observe in Figure~\ref{fig:train-reward-result} that with our policy selector,
% We achieve the upper bound performance, same as Best-$\pi$,
% We achieve the same performance as Best-$\pi$ 
% in the scenarios in the training set $\mathcal{X}$. In fact, our policy selector has near 100\% accuracy on the training and validation dataset mentioned in Section~\ref{sec:policy-sector-training}. In this online experiment, it achieves an accuracy of 100\% for the scenarios in $\mathcal{X}$. 
% However, for the scenarios in $\mathcal{X}'$, 
%as we observe in Figure~\ref{fig:test-reward-result}, 
% and hence obtained a slightly lower average reward compared to BEST-$\pi$.



%\begin{figure}[ht]
%\centering
%    \hspace{0.01\linewidth}
%    \subfigure[Average reward on the training scenarios]{\includegraphics[width=\linewidth]{Figures/training-result-reward.png}\label{fig:train-reward-result}}
%    \hspace{0.01\linewidth}
%    \subfigure[Average reward on the testing scenarios]{\includegraphics[width=\linewidth]{Figures/testing-result-reward.png}\label{fig:test-reward-result}}
%    \caption{Comparison of the average reward over 6 days. Our policy reuse framework with the policy selector (blue) achieves the closest performance to our upper bound BEST-$\pi$ (red) on average. For the training scenarios, it is exactly the same as BEST-$\pi$.}\label{fig:reward_results}
%\end{figure}



 
% Furthermore, our policy selector outperforms RAND-$\pi$ by 7.38\% and 4.36\% on $\mathcal{X}$ and $\mathcal{X}'$, respectively, demonstrating the effectiveness of choosing the policy based on the similarity of the traffic condition. 

\subsubsection{Transient traffic scenario}

This experiment evaluates how our policy reuse framework adapts to a changing traffic condition. We construct a transient traffic scenario $\tilde{X}$ by consecutively running a sequence of random scenarios picked from $\mathcal{X}\bigcup\mathcal{X}'$. Each scenario $\mathcal{X}\bigcup\mathcal{X}'$ is run for 3 consecutive days. We compare our proposed framework, which selects a policy on each day, against its variation which selecting a policy on the first day only. Both use the policy selector to select the policy. Again, BasicLB is applied on the first day. 

Figure~\ref{fig:transient-scenario-result} plots the average reward on each day for 24 days. The vertical grid shows the day on which the scenario changes. As shown in this figure, our framework can chose a suitable policy after it has experienced a new traffic for a day, and its performance compared to BasicLB and AdaptLB is consistent with the result in Section~\ref{sec:fix-traffic-scenario}.
% Depending on the scenario, our framework may not outperform BasicLB or AdaptLB, like at the beginning where scenario 8 was chosen. This is consistent with the result in Section~\ref{sec:fix-traffic-scenario}. 
Although compared to selecting a policy on the first day only, our proposed framework occasionally gets a lower reward on the days when the scenario changes, like on day 7 and 13, it can quickly recover on the next day and achieves a higher performance overall. This demonstrates the merit of our framework, in particular for real traffic scenarios where changes in daily traffic patterns may occur, but not as frequent as in this synthetic scenario $\tilde{X}$. 


\begin{figure}[t]
    \centering
        \vspace{1mm}
    \includegraphics[width=0.8\linewidth]{Figures/online-policy-change.png}
    \caption{The average reward for each day with transient traffic scenario. We change the traffic scenario every 3 days. Our method (blue) may not perform optimally on the first day when the scenario changes, but it can recover quickly on the next day and it outperforms the other baselines overall.}
    \label{fig:transient-scenario-result}
\end{figure}


% \subsubsection{Fast adaptation}

% In this experiment, we allow the selected policy on the first day to be fine-tuned to the new traffic scenario. Specifically, for each test traffic scenario in $\mathcal{X}'$, we run BasicLB on the first day and use the data on the first day to select a policy through our policy selector. We then train a new policy on the first day of the new scenario by initializing the weights \textcolor{blue}{tianyu: of the policy networks} with the weights from the selected policy. 

% Figure~\ref{fig:fast-adapt} shows the average and standard deviation of the learning curves for training a new policy on the new scenario with the policy weights initialized by the weights from the selected policy (selected) and by the worst policy (worst) according to our experiment described in Figure~\ref{fig:rl_policy_21_sec_test}. We also compare this result with training from scratch, which initializes the weights by a random orthogonal matrix. We observe that, in general, the curves for the selected policy start at a much higher reward value, and they converge much faster compared to the other two. Although the curves form the worst policy start also at a higher reward value compared to the curves from learning from scratch, it improves much slower at the first 10K interactions and eventually becomes the one with the lowest reward. As expected, a policy trained on a similar traffic scenario will have more relevant learning that can be transferred to the new scenario, resulting in a faster adaptation to the new scenario. This can be very helpful considering the 

% \begin{figure}[ht]
%     \centering
%         \vspace{1mm}
%     \includegraphics[width=\linewidth]{Figures/fast_adapt_smooth.pdf}
%     \caption{Learning curves for fine-tuning on the test scenarios. We experiment with using the policy selector to choose a trained model for initialization (selected) and using the model with the worst performance on the target test scenario for initialization (worst). The learning curve for training from scratch (scratch) is also shown. For each case, we show the mean curve across all test scenarios as the percentage increase with respect to the initial reward achieved by training from scratch. The channel denotes $\pm0.25$ standard deviation from the mean.}
%     \label{fig:fast-adapt}
% \end{figure}

% Figure~\ref{fig:fast-adapt-avg-reward-test} shows the improvement made by fine-tuning the selected policy after 4k, 8k, 50k, 100k and 200k interaction experiences. We observe that after 4k, the fine-tuned policy can already achieves close if not better reward on all traffic scenarios in $\mathcal{X}'$. 

% \begin{figure}[ht]
%     \centering
%         \vspace{1mm}
%     \includegraphics[width=\linewidth]{Figures/fast-adapt-sectors.png}
%     \caption{Average reward over the first day on each testing scenarios.}
%     \label{fig:fast-adapt-avg-reward-test}
% \end{figure}

% Table~\ref{tab:fast-adapt-avg-kpis-test} shows that, on average, FastAdapt-4k can already outperform BasicLB and AdaptLB by a large margin. In particular, for the reward, it achieves 13.57\% and 11.24\% percent improvement over BasicLB and AdaptLB, respectively.

% \begin{table}[htb]
%     \centering
%     \caption{Average performance over over \textcolor{blue}{tianyu: extra over} the first day and all testing scenarios.\textcolor{blue}{tianyu: Can we make this consistant with table 2 and 3, namely average over 6 days instead of only first day? This way we can compare with the results in table 3}}
%     \label{tab:fast-adapt-avg-kpis-test}
%     \begin{tabular}{l|c c c c c}
% \toprule
% & Reward & {\bf $G_{avg}$} & {\bf $G_{min}$} & {\bf $G_{sd}$} & {\bf $G_{cong}$}\\
% \midrule
% BasicLB & 0.402 & 3.027 & 1.655 & 2.282 & 0.850\\
% AdaptLB & 0.411 & 3.080 & 1.771 & 2.132 & 0.832\\
% FastAdapt-4k & 0.457 & 3.409 & 2.076 & 1.764 & 0.887\\
% % FastAdapt-8k & 0.459 & 3.412 & 2.093 & 1.744 & 0.891\\
% % FastAdapt-50k & 0.466 & 3.443 & 2.166 & 1.707 & 0.890\\
% % FastAdapt-100k & 0.469 & 3.460 & 2.180 & 1.701 & 0.895\\
% % FastAdapt-200k & 0.472 & 3.471 & 2.198 & 1.687 & 0.899\\
% \bottomrule
%     \end{tabular}
% \end{table}