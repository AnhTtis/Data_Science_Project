
\begin{figure*}[thbp]
    \centering
        \vspace{1mm}
    \includegraphics[width=0.7\textwidth]{Figures/sector-identifer-framework.png}
    \caption{The proposed policy reuse framework. At the start of each day, the policy selector selects a policy from the policy bank based on the previous day's network traffic. Then, at each hour, the chosen load balancing policy will be used. }
    \label{fig:framework-illus}
\end{figure*}
\section{Background}\label{background}

\subsection{Load Balancing}
\label{sec:lb-background}

% \textcolor{gray}{tianyu: maybe "rearranging" is better?} 
% \textcolor{blue}{tianyu: Does UEs means user equipmentS? if so, then it should be user equipmentS in the front} 
% \textcolor{blue}{tianyu: I could be wrong, but if UEs is plural, then there should be no "the"} 
Load balancing in a wireless network involves redistributing user equipment (UEs) between network cells where a {\bf cell} is a combination of carrier frequency and spatial region relative to the physical base station. A base station can host multiple cells serving different regions (or {\bf sectors}). Load balancing can be triggered between cells in the same (or different) sector(s) and base station(s).
% \textcolor{gray}{Di: should we explain what are the idle and active UEs here?}

UEs exist in one of two states: active and idle. A UE is active when it is actively consuming network resources. When a UE is not in such a state, it is idle. Active mode UEs are served by a single cell. Idle mode UEs are said to camp on a given cell and this is the cell that will serve this UE when it becomes active. As discussed in~\cite{kang2021hrl}, there are two types of load balancing methods: (1) active UE load balancing (AULB) which is done through handover, and (2) idle UE load balancing (IULB) which is done through cell-reselection. AULB results in instantaneous changes in the load distribution. IULB  affects the anticipated load distribution when the idle UE becomes active. 
%The rest of this section will discuss the two categories in detail and present the metrics we use to evaluate the performance of a load balancing solution.

% \textcolor{blue}{tianyu: should there be a reference?} 
{\em Active UE load balancing (AULB)}:
AULB, such as mobility load balancing (MLB)~\cite{DBLP:conf/vtc/KwanAPTK10}, transfers active UEs from their serving cells to neighboring cells if better signal quality can be reached there.  Handover occurs when $F_j > F_i + a_{i,j} + H$, where $F_i$ and $F_j$ are the signal quality measurements from the source and neighboring cells, respectively. $F_i$ and $F_j$ are generally quantified by the Reference Signal Received Power (RSRP)~\cite{kang2021hrl}. $H$ is the handover hysteresis and $a_{i,j}$ is a control parameter, such as the Cell Individual Offset (CIO). By decreasing $a_{i,j}$, we can more easily hand over UEs from cell $i$ to cell $j$, thereby offloading network load from cell $i$ to cell $j$, and vice-versa. Finding the best $a_{i,j}$ value for different combinations of traffic status at cells $i$ and $j$ allows us to optimize AULB. 

{\em Idle UE load balancing (IULB)}:
IULB moves idle UEs from their camped cell to a neighboring cell based on cell-reselection~\cite{kang2021hrl}. From the cell it is camped on, an idle UE receives receive minimal service. Once it turns into active mode, it stays at the cell it camped on, and later can be moved to another cell through AULB. Generally, cell-reselection is triggered when $F_i < \beta_{i,j} \mbox{ and } F_j > \gamma_{i,j}$, where $\beta_{i,j}$ and $\gamma_{i,j}$ are control parameters. By increasing $\beta_{i,j}$ and decreasing $\gamma_{i,j}$, we can more easily move idle UEs from cell $i$ to cell $j$, and vice-versa. Hence, optimally controlling these parameters will allow us to balance the anticipated load and reduce congestion when idle UEs become active. 

\subsection{Performance metrics}
\label{sec:performance-metrics}
Let $C$ be the group of cells on which we want to balance the load. We evaluate network performance using four throughput-based system metrics, where (a)-(c) are variations to those described in~\cite{kang2021hrl}. 

\paragraph{$G_{avg}$} describes the average throughput over all cells in $C$, defined as
\[
    G_{avg} = \frac{1}{|C|}\sum_{c\in C}\frac{A_{c}}{\Delta t},
\]
where $\Delta t$ is the time interval length and $A_{c}$ is the total throughput of cell $c$ during that time interval. Maximizing $G_{avg}$ means increasing the overall performance of the cells in $C$. 

\paragraph{$G_{min}$} is the minimum throughput among all cells in $C$, defined as
\[
    G_{min} = \min_{c\in C}{\frac{A_{c}}{\Delta t}}.
\]
Maximizing $G_{min}$ improves the worst-case cell performance. 

\paragraph{$G_{sd}$} is the standard deviation of the throughput, defined as 
\[
    G_{sd} = \sqrt{\frac{1}{|C|} \sum_{c\in C}\left(\frac{A_{c}}{\Delta t} - G_{avg}\right)^2}.
\]
Minimizing $G_{sd}$ reduces the performance gap between the cells, allowing them to provide fairer services. 

\paragraph{$G_{cong}$} quantifies the ratio of uncongested cells, defined as
\[
    G_{cong} = \frac{1}{|C|}\sum_{c\in C}\mathbb{1}\left(\frac{A_{c}}{\Delta t} > \epsilon\right),
\]
where $\mathbb{1}(\cdot)$ is the indicator function and $\epsilon$ is a small value. Maximizing $G_{cong}$ discourages cells getting into congested state. In our experiments, we use $\epsilon = 1$Mbps.

%\subsection{Reinforcement Learning (RL)}
%In reinforcement learning, an agent aims to learn a control policy $\pi$ that maximizes its long-term accumulated reward through interactions with the environment~\cite{sutton2018reinforcement}. The problem to be solved is modeled as a Markov Decision Process (MDP) which is characterized by a state space $\mathcal{S}$, an action space $\mathcal{A}$, a reward function $r$, and a state transition probability function $P$. At each state $s_t \in \mathcal{S}$, the RL agent selects an action $a_t \in A$ by following the current policy $\pi$ and the agent then receives a reward $r(s_t,a_t)$ and the state $s_t$ moves to $s_{t+1}$.  

%Classic RL methods (e.g., \cite{ng2006autonomous,riedmiller2009reinforcement}) are limited to low-dimensional problems as they suffer from computational, memory, and sample complexities. In order to tackle problems approaching real-world complexity, deep reinforcement learning (DRL)-based approaches have been found to be more effective. DRL has also convincingly demonstrated its ability to find optimal solutions without the aid of hand-crafted rules for high-dimensional problems. Examples here include  AlphaZero~\cite{silver2018general}, which has defeated human world champions in the games of chess, Shogi and Go. Other areas of application for DRL include robotics~\cite{arulkumaran2017deep}, internet of things~\cite{lei2020deep} and 5G communication networks~\cite{xiong2019deep}.

% \textcolor{blue}{Di: This sentence seems not 100\% correct} 
%Sequential decision-making is an important and well-studied problem in the field of machine learning. It covers a wide range of applications such as telecommunication, finance, self-driving cars etc. In short, sequential decision-making describes the task where given some past experience, an intelligent agent is expected to make a decision in an uncertain environment in order to achieve the given objective. 

% \textcolor{blue}{Di: This sentence seems not 100\% correct} 
%Sutton et.\  al.~\cite{sutton1984temporal} proposed a formal framework known today as reinforcement learning (RL) for sequential decision-making. The core idea of RL is that by mimicking a biological agent, the artificial agent can learn from its past experience by optimizing some objectives given in the form of cumulative rewards. Formally speaking, a general RL problem is a discrete-time stochastic control process, i.e. a tuple $\langle \mathcal{S}, \mathcal{A}, f, r, \mu\rangle$, where $\mathcal{S}$ is a set of states, $\mathcal{A}$ is a set of actions, $f: \mathcal{S}^* \rightarrow \mathcal{S}$ is the state transition function \footnote{we use $\mathcal{S}^*$ to denote all possible trajectories generated by the set $\mathcal{S}$}, $r: (\mathcal{S}\bigotimes \mathcal{A})^*\rightarrow \mathbb{R}$ is the reward function\footnote{$\bigotimes$ denotes Cartesian product between two sets.} and $\mu$ is the initial distribution over all states of the system. In this framework, a common assumption is that the control process follows Markov property, that is, the future of the process only depends on the current state. Under the Markov assumption, the transition and reward functions are simplified as $f: \mathcal{S} \rightarrow \mathcal{S}$ and $r: \mathcal{S}\bigotimes \mathcal{A} \rightarrow \mathbb{R}$. 

%The solution to an RL problem (policy) is a function: $\pi: \mathcal{S} \rightarrow \mathcal{A}$. To obtain this solution, the agent needs to achieve the maximum expected cumulative rewards, i.e. $\pi  = {\mathrm{argmax}_{\pi^'}} {\mathbb{E}^{\pi^'}} (R_1 + R_2 + \cdots R_n)$, where $R_i$ denotes the reward obtained at time step $i$. 
%There are two main types of approaches, one is the value-based method, the other is the policy gradient-based method. The value-based method focuses on building a value function or an action-value function, i.e. an estimation of the accumulated rewards and then generate a policy based on the estimated value function by taking the argmax over the action space. Some significant work includes Q learning~\cite{watkins1989learning}, Q learning with function approximator~\cite{gordon1996stable} and recently, deep Q networks (DQN)~\cite{mnih2015human}. The policy gradient method, originally developed in~\cite{sutton2000policy}, leverages a function approximator (e.g. neural networks) to model the policy and directly optimizes the policy with respect to a performance objective (typically the expected cumulative reward). 

%One heuristic 
% people 
%found in training policy networks (i.e. neural networks parameterized policy) is that if the parameter updates change the policy too much at one step, it is often detrimental to the training process, which is known as policy collapse. By enforcing a KL divergence constraint between each update, trust region policy optimization (TRPO)~\cite{schulman2015trust} successfully adopts this idea and guarantees a monotonic improvement over policy iteration.  However, TRPO is often criticized for its complex structure and its incompatibility with some common deep learning structures. To alleviate this problem, a clipped surrogate objective is introduced in the proximal policy optimization (PPO) method~\cite{schulman2017proximal}. The proposed method only requires first-order optimization and can still retain similar performance compared to TRPO. The method is much simpler to implement, and more importantly, the authors show empirically that it has better sample complexity compared to TRPO, which is of great importance when it comes to real-world application. Closer examination of some engineering details of the PPO implementation has been conducted in~\cite{hsu2020revisiting}. It revealed some drawbacks of the PPO framework such as stability issues over the continuous action domain and proposed some simple workarounds to resolve these problems. In our experiment, we choose PPO~\cite{schulman2017proximal} as our RL algorithm as it reduces the risk of policy collapse and provides a more stable learning.
% \textcolor{gray}{Di: Can we add more details for PPO.} 

%The clipped surrogate objective from PPO~\cite{schulman2017proximal} comes from an observation from TRPO~\cite{schulman2015trust} that by denoting $r_t(\theta) = \frac{\pi_\theta (a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$, TRPO essentially is maximizing a "surrogate" objective:
%\begin{equation}
%    L(\theta)  = \hat{\mathbb{E}}_t[r_t(\theta)\hat{A}_t]
%\end{equation}
%where $\hat{A}_t$ is an estimator of the advantage function at timestep $t$.  Note the expression does not have the KL constrain proposed in TRPO and as one might deduce, maximizing $L$ will lead to an excessively large policy update. A trick proposed by PPO is to clip the probability ration $r_t(\theta)$ to a small interval around 1, that is:
%\begin{equation}
%    L^\mathrm{CLIP}(\theta) = \hat{\mathbb{E}}_t[\mathrm{min}(r_t(\theta)\hat{A}_t, \mathrm{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
%\end{equation}
%Therefore the change in the parameter $\theta$ is regularized based on the choice of $\epsilon$. 