\section{Related Work}\label{related-works}
% In this section, we discuss the relevant prior arts in load balancing. 
%- Early work: benefits of load balancing identified in \cite{tolli2002performance, tolli2002adaptive}
Load balancing has long been identified as a crucial aspect of radio resource management \cite{tolli2002performance, tolli2002adaptive}. Typically, load balancing aims to improve throughput, ensure fairness, reduce latency, while also minimizing the number of handovers \cite{hu2010self}. Load balancing has often been studied under the larger umbrella of self-organizing networks (SON), which aims to provide a holistic framework for self-configuration, self-optimization, and self-healing, with load balancing being a key topic within self-optimization \cite{hu2010self}.  The inclusion of SON as part of the standard 3GPP Long Term Evolution (LTE) specifications has further accelerated the research effort in load balancing in recent years \cite{nec2009son}.

%- self-organizing network (SON) and LTE \cite{nec2009son} \cite{hu2010self}. Self optimization is a key aspect of SON. 
%MLB is an important topic in self-optimization, which uses handovers to balance the network load among cells and base stations. 

Rule-based load balancing has been the dominant approach over the last few decades. Popular approaches include the adjustment of the coverage area of various cells \cite{ali2007directional, li2005umts,viering2009math}, as well as the adjustment of handover parameters that affect the cell selection criteria of user equipment (UEs) \cite{jansen2010handover,kwan2010on,yang2012high,nasri2013handover}. Along this latter direction, Kwan \emph{et al.} has introduced a method for adjusting cell-specific offsets between neighboring cells at constant step sizes based on the cell load \cite{kwan2010on}. Yang \emph{et al.} further proposes a technique for computing the step size adaptively, so as to balance cell loads more efficiently. Other load balancing approaches that have been studied include the minimization of the number of handovers \cite{hu2010self}, and the tuning of hysteresis and time-to-trigger parameters \cite{jansen2010handover}

%Handover parameter tuning \cite{jansen2010handover}
%-  Adjustment of coverage area \cite{ali2007directional}, \cite{li2005umts}, \cite{viering2009math}
%- A body of work on adjustment of the handover region \cite{nasri2013handover}. 
%Adjust cell-specific offset between neighboring cells. Adjust offset at constant step-sizes based on cell load\cite{kwan2010on}. To improve efficiency, propose adaptive step size to more efficiently balance the load \cite{yang2012high}.

%- Other approaches minimize number of handovers \cite{hu2010self}, and tune the hysteresis and time-to-trigger parameters \cite{jansen2010handover}

The application of reinforcement learning (RL) is an active area of research, especially with the recent development of deep RL \cite{mnih2015human, silver2018general}. Q-learning \cite{watkins1989learning} is a popular choice among the various reinforcement learning methods, since it does not require the explicit modelling system dynamics, and the expected utility of each control action can be easily extracted. Munoz \emph{et al.} use Q-learning to find the optimal parameters for a fuzzy logic controller that adjusts load balancing parameters \cite{munoz2011optimization}.
Mwanje \emph{et al.} use Q-learning to adaptively tune the step size of cell-specific offsets based on the load condition \cite{mwanje2013qlearn}.  While these lines of work achieve load balancing by controlling the serving cells, the optimization of UE behavior has also been studied. Kudo \emph{et al.} apply Q-learning from the perspective of UEs to determine to which cells to send service requests in order to minimize outage \cite{kudo2014qlearn}. In contrast to these prior work, this paper focuses on addressing challenges related to variations in traffic patterns across different cells. Our method selects the most suitable RL policy from a policy bank based on network measurements, which allows our system to achieve significant improvement over rule-based methods in unseen traffic scenarios.