
\section{Methodology}\label{method}
% In this paper, we propose a load balancing framework based on policy reuse that selects suitable pre-trained RL policies to execute in unseen traffic scenarios.
% We develop a policy reuse-based load balancing framework that uses a policy bank containing a set of RL policies pre-trained on a diverse set of traffic scenarios and a policy selector that selects a suitable policy in the policy bank based on the recent traffic condition. 
We develop a policy reuse-based framework for load balancing. It employs a policy bank that stores a set of RL policies pre-trained on a diverse set of traffic scenarios and a policy selector that selects a suitable policy in the policy bank based on the recent traffic condition.
% leverage a neural-network-based classifier to select suitable pre-trained RL policies for unseen traffic scenarios.
We model the policy selector as a deep neural network classifier that estimates the similarity between the current traffic pattern and those used to train the 
% pre-trained 
RL policies in the policy bank. See Figure~\ref{fig:framework-illus}.
%The selection of a suitable policy can
% From a policy bank containing a diverse set of pre-trained policies, we can select a policy that 
%result in 
% \textcolor{blue}{tianyu: allows-> shows? or maybe something else, I feel allows here is not appropriate, it sounds very passive somehow. } 
%significant improvement over classical and adaptive rule-based methods without the need to train a new RL policy or to fine-tune an existing one on the unseen scenario.
%Furthermore, our proposed policy reuse framework allows the system to continuously adapt to the most recent traffic pattern by periodically selecting a new policy (e.g., every 24 hours). An illustration of the framework is shown in Figure~\ref{fig:framework-illus}. 

%In the rest of this section, we first present the problem formulation in Section~\ref{sec:rl-formulation}. Then, we present our construction of the policy bank and of the policy selector in Sections~\ref{sec:policy_bank_method} and~\ref{sec:policy-selector}, respectively.

\subsection{Problem formulation}
\label{sec:rl-formulation}

Let $\mathcal{X}=\{X_1, \dots, X_M\}$ be the set of $M$ traffic scenarios and $\Pi=\{\pi_1, \dots, \pi_M\}$, the corresponding set of pre-trained RL policies. 
%Each policy $\pi_i$ contains a set of weights $\theta_i$ learned through an RL algorithm on the traffic scenario $X_i \in \mathcal{X}$. 
These learned policies form our policy bank and will later be used to perform load balancing on unseen traffic scenarios $\mathcal{X}'=\{X'_1, \dots, X'_N\}$

% ANSWER TO RESPONSE TO REVIEWER 1: What is meant by unseen traffic? A traffic in any network application is always unseen. Are the authors using some specific attributes of the unseen traffic?
which is disjoint from $\mathcal{X}$.
The policy selector selects a suitable policy in $\Pi$ for each of the unseen traffic scenarios in $\mathcal{X}'$ based on which traffic scenario in $\mathcal{X}$ is the most similar to them.

At the level of the RL policy, a standard Markov Decision Process (MDP) formulation is used for the load balancing problem and Proximal Policy Optimization (PPO)\cite{schulman2017proximal} is used for training. 
% \textcolor{gray}{tianyu: I think maybe move the description of the MDP (state action, etc) here would be better, so that you don't need to go in to too much details in the next couple sentences. Alternatively, maybe for the next couple sentences you can be a bit more abstract, like we select and action a, and transit from state $s_t$ to $s_{t+1}$, omitting the details on what is the action etc.} 
At each time step $t$, an action $a_t$, containing new load balancing parameter values, is chosen according to the network state $s_t$. 
% We design the latter to describe the recent network traffic pattern, hence we formulate it as the concatenation of the network status information from the last $k$ time steps (i.e.: from time step $t-k$ to $t$). 
% \textcolor{blue}{tianyu: I don't quite understand the last sentence.} 
After applying $a_t$, the network transitions from $s_t$ to $s_{t+1}$ according to the dynamics of the network captured by the transition probability function $P(s_{t+1}|s_{t}, a_{t})$. The MDP is defined as a tuple $\langle\mathcal{S}, \mathcal{A}, R, P, \mu\rangle$ where: $\mathcal{S}$ is the state space, where each state is a continuous high-dimensional vector of network status information in the last $k$ time steps, describing the recent traffic pattern. The network status information contains the number of active UEs, the bandwidth utilization, and the average throughput of every cell. These features are averaged over the time interval between each application of a new action. These are the same features used in~\cite{kang2021hrl}. In our experiments, each time step is one hour and we use $k=4$. $\mathcal{A}$ is the action space, where each action is the concatenation of the load balancing parameters $\alpha_{i,j}, \beta{i,j}$ and $\gamma_{i,j}$ for all $i,j\in C$. $R$ is the reward, which is a weighted average of the performance metrics defined in Section~\ref{sec:performance-metrics}. In our formulation, the reward can be directly computed with the state. $P$ is the transition probability function, $P(s_{t+1}|s_{t}, a_{t})$. Finally, $\mu$ is the initial distribution over all states in $\mathcal{S}$, $\mu = P(s_0)$.
% \textcolor{gray}{tianyu: up to $t-k$ gives people an impression that you are using data from timestep 0 to $t-k$, where I believe here it should be "the most recent k steps" right? maybe just use "last k timesteps" } 
While $\mathcal{S}$, $\mathcal{A}$ and $R$ are the same for all traffic scenarios, $P$ and $\mu$ can be different for different scenarios. As a RL policy is trained to maximize the long-term reward, it will inevitably be biased by $P$ and $\mu$, therefore a policy trained on one scenario may not be optimal on another.

\begin{figure}[t]
    \centering
        \vspace{1mm}
    \includegraphics[width=\columnwidth]{Figures/21sec-traffic-analysis.png}
    \caption{Average traffic over one week for each of 21 traffic scenarios. Scenarios are clustered into 3 groups using K-means based on active UEs, throughput, and percentage of PRB utilization. Group 1 has high traffic on cell 1; group 2 has high traffic on cell 4; group 3 has low traffic in general.}
    \label{fig:traffic_21_sec}
\end{figure}

\subsection{Policy bank}
\label{sec:policy_bank_method}
%RL allows the system to autonomously approximate the optimal control under different traffic conditions. Therefore, we use an RL algorithm to train policies to populate our policy bank $\Pi$. 

% One of the main challenges for load balancing with a data-driven solution is the generalization of the learning to diverse communication traffic patterns. 
In order to ensure that our policy bank covers a wide range of traffic conditions, we first cluster the traffic scenarios based on their daily traffic patterns to identify different traffic types. We describe the daily traffic pattern as a sequence of states over 24 hours, and we use K-Means to perform the clustering. For simplicity, we randomly pick a subset of scenarios from each type to form $\mathcal{X}$. Then PPO is applied using the MDP formulation from Section~\ref{sec:rl-formulation} on each $X_i\in \mathcal{X}$ to obtain the policy $\pi_i\in \Pi$.
The policies are learned by maximizing the expected sum of discounted future rewards:
\[
    \pi_i = \mbox{argmax}_{\pi}\mathbb{E}_{\pi}\left(\sum_{t=1}^n \lambda^{t-1}R_t\right),
\]
where $n$ is the length of an interaction experience trajectory and $\lambda$ is the discount factor. %In our experiments, we use $n=24$. 

%\textcolor{gray}{Jimmy: The mention of K-means is too non-committal. I think we can start with a generic sentence about using clustering to find the training sectors, but I think we need to follow that up with a sentence like: ``In our work, we use K-means to perform the clustering". Also, we should add a sentence saying what is the feature vector used to perform clustering.}  



\subsection{Policy selector}
\label{sec:policy-selector}

 The policy selector aims to find the traffic scenario $X_i \in \mathcal{X}$ that is most similar to the target scenario $X'\in \mathcal{X}'$. We then select $\pi_i$ that was trained on $X_i$ to execute on $X'$. We model the policy selector as a non-linear function using a neural network that takes
% When testing on an unseen traffic scenario $X'$ we take 
as input the states 
% description as discussed in Section~\ref{sec:rl-formulation}
% \textcolor{blue}{tianyu: do you think here we should use a different word than "network"? I think you are saying to feed the traffic information into the neural nets right? but when you use network status, it feels like you are talking about neural nets somehow. Maybe use "traffic status"? or, in more details, just "state description as discussed in section III A"} 
from the last $T$ time steps
% to the traffic identifier 
to select the best policy index.
% in our policy bank $\Pi$
In our experiments, we use $T=24$ hours, allowing us to capture the peaks and valleys in the regular daily traffic pattern observed in our traffic scenario data as we will discuss in Section~\ref{sec:traffic-analysis}. 
% ANSWER TO REVIEWER 3: At the start of each day, the policy selector selects a policy from the policy bank based on the previous dayâ€™s network traffic. However, weekday and weekend have different traffic patterns. How about adopting the same day of the week in the last week?
In general, the choice for $T$ can be arbitrary and the input of the policy selector can be easily expanded to capture correlations from, for example, historical trends from the same day of the week, from the same month of the year, etc. 

% \textcolor{gray}{tianyu: General comment: I think maybe a flow chart where you incorporate every component of the methodology session would be beneficial. Right now all the subsections are bit disjoint, you might want something to tie them together to give the reader a better experience in understanding the method. In addition, not sure if we should add another subsection for fast adaptation as well.}