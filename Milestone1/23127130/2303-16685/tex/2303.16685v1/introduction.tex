\section{Introduction}
Load balancing has long been identified as a crucial aspect of radio resource management \cite{tolli2002performance, tolli2002adaptive}. Typically, load balancing aims to improve throughput, ensure fairness, reduce latency, while also minimizing the number of handovers \cite{hu2010self}. Load balancing has often been studied under the larger umbrella of self-organizing networks (SON), which aims to provide a holistic framework for self-configuration, self-optimization, and self-healing, with load balancing being a key topic within self-optimization \cite{hu2010self}.  The inclusion of SON as part of the standard 3GPP Long Term Evolution (LTE) specifications has further accelerated the research effort in load balancing in recent years \cite{nec2009son}.


% With the wide deployment of the fifth generation (5G) system, the communication traffic has been increasing rapidly in recent years~\cite{samdanis2020road}. As reported in, the total mobile traffic has increased ten times in the last four years and it is expected to increase 70\% annually by 2023. This ongoing increase of communication traffic is due to the increased number of connected devices as well as the traffic consumed per device. With the increasing adoption of online streaming, augmented reality (AR)-based applications, and virtual reality (VR)-based applications, the traffic consumption per device has increased significantly. Besides the fast increase for the total volume of communication traffic, its distribution is also very uneven. As presented in~\cite{hto}, almost 50\% of the total of communication traffic is served by 15\% of all the cells. Without efficient load scheduling, the quality of service (QoS) will significantly affected and bring in customer unsatisfaction. Thus, the load balancing, which aims to improve the traffic distribution and alleviate the traffic fluctuation is of significant importance to improve the overall system efficiency and customer satisfaction.
%generated by video streaming and real-time applications such as augmented and virtual reality (AR/VR), the fifth generation (5G) system is anticipated to meet the requirements for supporting this massive increase of data demand~\cite{samdanis2020road}. Load balancing, 
%which aims to improve communication traffic distribution to alleviate fluctuation, 
%which aims to 

%With the explosive growth of devices and wireless data traffic evenly distributing communication traffic among the available radio resources, has become an important and challenging problem for modern communication networks. Traditionally, load balancing is approached using expert knowledge and hand-crafted rules (see \cite{tolli2002adaptive} and \cite{zheng2020reliable} for examples). However, the continuous growth of networks in size and cell density is posing a greater challenge to these methods. The modelling of the correlation between the load balancing control parameters and various traffic-related factors becomes extremely difficult with more irregular network typologies, diverse user behaviours and complex signal interference.
%\cite{tolli2002adaptive,li2005umts,ali2007directional,viering2009math,jansen2010handover,kwan2010on,yang2012high,nasri2013handover,zheng2020reliable}. 
%As presented in~\cite{hto}, almost 50\% of the total communication traffic is served by 15\% of all the cells. Efficient load balancing techniques are expected to significantly improve the quality of service (QoS) and hence boost customer satisfaction.  
% \textcolor{blue}{This sentence is too long.} 
% \textcolor{blue}{tianyu: is it 'traffic-related'?} 
% \textcolor{blue}{tianyu: I would change this to "and various traffic-relate factors", the clause in the subjective makes the sentence unnecessarily long and complicated} 
% \textcolor{blue}{tianyu: I would use extremely here}  
% \textcolor{blue}{tianyu: is "signal interference relationships" a terminology? if not maybe "signal interference" is enough? cause I'm not sure what relationship this is supposed to be.}.   

%\textcolor{blue}{recognition} 
%With the standardization and rapid adoption  of self-organizing networks (SONs)~\cite{hu2010self}, 
% \textcolor{blue}{tianyu: load balancing methods?}
%~\cite{munoz2011optimization,munoz2012fuzzy,mwanje2013qlearn,kudo2014qlearn,mwanje2016cognitive,xu2019load,kan g2021hrl}.
% \textcolor{blue}{tianyu: } 
%Learning- and data-driven solutions to  balancing in wireless networks are becoming increasingly attractive. In particular, reinforcement learning (RL) has been adopted by a number of researchers (e.g., \cite{munoz2011optimization,,xu2019load,kang2021hrl} to learn a load balancing policy by interacting with the communication system. An RL-based approach enables the system to adapt to unknown environments solely based on the environment's feedback, and hence it can be self-adapted and self-optimized to
% \textcolor{blue}{tianyu:no the here} 
%specific conditions of the environment. Furthermore, deep RL, which uses a deep neural network to model its acquired knowledge, exhibits  strong expressivity~\cite{lu2017expressive},
% \textcolor{blue}{tianyu: strong expressivity instead of high expressive power}
%allowing us to capture 
% \textcolor{blue}{tianyu:no the here} 
%complex non-linear correlations between 
% \textcolor{blue}{tianyu:no the here} 
%control actions and the traffic condition. 

% Reinforcement learning aims to learn an efficient control policy by interacting with the environment. In recent years, reinforcement learning based approaches have been paid more attention. In a RL-based approach, a control agent learns a load balancing policy by interacting with the communication system~\cite{}. Control actions are chosen by the RL agent depending on the observation of system state. RL-based approaches offer the possibility of being more adaptive and providing a solution that is closer to the optimal solution than a static rule-based approach. As shown in~\cite{}, the rule-based approaches are typically based on experts experiences and a set of pre-selected rules. As shown in~\cite{}, the rule-based approaches are typically based on experts experiences and a set of pre-selected rules. As shown in~\cite{}, the rule-based approaches are typically based on experts experiences and a set of pre-selected rules.

Rule-based load balancing has been the dominant approach over the last few decades. Popular approaches include the adjustment of the coverage area of various cells \cite{ali2007directional, li2005umts,viering2009math}, as well as the adjustment of handover parameters that affect the cell selection criteria of user equipment (UEs) \cite{jansen2010handover,kwan2010on,yang2012high,nasri2013handover}. Reinforcement learning aims to learn a control policy via interacting with the environment and it has also recently shown some promising results via directly learn from a given data set~\cite{fucloser}. Reinforcement learning has been applied to several related applications and shown some impressive results~\cite{wu2018optimizing, fu2022reinforcement,zhang2022metaems,wu2018machine,huang2021modellight}. Reinforcement learning has also been applied with some success to load balancing\cite{li2022traffic, wu2021load,feriani2022multiobjective} and 
although reinforcement learning (RL)-based methods have achieved impressive performance on communication load balancing problems, the resulting policies are highly dependent on the training data and  may take a large number of interactions with the environment to learn a reliable control policy. For example, in~\cite{xu2019load}, it requires around ten thousand interactions with the environment for a learned policy to converge. This poses critical challenges when we try to train new models on new traffic scenarios. %deploying trained RL policies in the real world, since traffic conditions can vary greatly across deployment sites, and over time. 
%As such, the RL-based solution must be able to quickly and continuously adapt to the current environment.
%\textcolor{blue}{This raises concerns for deploying a pre-trained model in a network with a novel traffic condition that has not been seen during training.} 
%In addition, due to the structure of the RL routine, it is cumbersome to generalize an agent learned from one particular environment to a slightly different one without the proper tool. This drawback often leads to constraints on real-world applications, especially for our load balancing application, where the agent needs to adapt to unseen traffic patterns. 

% Reinforcement learning based solutions have achieved some impressive performance on communication load balancing problems. However, as reported in several previous works, it will take a large amount of interaction experience to learn a reliable control policy. For example, it may require more than one million interactions with the environments to learn a reliable control policies to play the Atari games. Furthermore, the learned machine model will be highly affected by the accuracy of simulators. to enable when Different from the previous works that mainly. These challenges hinder the efficient usage of reinforcement learning based approaches. 

% \textcolor{blue}{tianyu: based on recent traffic patterns. by using analyze, it sounds more human-involved, which is not the case for us.} 
%the traffic condition of the previous day. 
% This selector module is modelled by a classifier trained on the states visited by the policies in the policy bank in their respective training environments. 

To address these issues, inpired by some previous works on knowledge reuse~\cite{wu2022efficient,wu2022short,DBLP:conf/ijcai/LinW21,wu2019multiple,wu2017boosting}, in this work, we develop a load balancing framework based on policy reuse, which selects
a suitable policy from a collection of pre-trained policies using real-time traffic data. Prior to deployment we train a set of RL-based control policies on a diverse set of traffic scenarios, forming a policy bank. Operating as a classifier during deployment, the policy selector  chooses an RL-based control policy from the policy bank based on the recent traffic pattern. 
%The policy selector module operates in two steps: first we collect trajectories from each traffic scenario using the policy from the policy bank, respectively; then we train a classifier with the data obtained from this step. This classifier is then used to select the appropriate RL-based control policy during operation. 
% first work that studies how to use pre-trained RL policies for unseen traffic scenarios \textcolor{blue}{tianyu: first work that generalize pre-trained RL policies to unseen traffic scenarios} 
To the best of our knowledge, this is the first work that generalizes pre-trained RL policies to unseen traffic scenarios in the context of communication load balancing. The main contributions of this paper are:
(i) we propose a policy reuse framework for load balancing that efficiently adapts to network traffic conditions, and 
(ii) we show that a policy trained on a similar traffic scenario can outperform rule-based and adaptive rule-based load balancing methods. Based on this observation, we present a policy selector using a deep neural network classifier.
    % \item We show that the policy selected by our policy selector can be effectively fine-tuned on the target scenario, achieving much faster convergence compared to training a new policy from scratch. This provides further validation for our policy selector, and presents additional optimization opportunities in situations where fine-tuning on the target scenario is allowed. \textcolor{blue}{tianyu: perhaps this point needs to blend in more in the introduction.}


% To tackle the aforementioned concerns, in this work, we propose to use transfer learning and reinforcement learning for communication traffic load balancing. Specifically, we first train a set of reinforcement learning based control policies on a diverse set of traffic scenarios. Second, we propose a policy selector why can efficiently and accurately chose a control policy from the policy bank. To the best of our knowledge, this is the first work that studies how to train efficient reinforcement policies and utilize the learned policies for unseen traffic scenarios. 

The remainder of this paper is organized as follows. In Section~\ref{background}, we introduce the technical background. The proposed framework is presented in Section~\ref{method}. Section~\ref{exp} presents experimental results comparing our proposed framework against several baselines.
% , and shows the advantage of our solution in the settings where fine-tuning of the pre-trained models is allowed
Finally, 
%related work is presented in Section~\ref{related-works} and 
we conclude in Section~\ref{conclusion}.


% The remainder of this paper is organized as follows. In Section~\ref{background}, we introduce the technical background of this paper including Markov decision processes, reinforcement learning, and transfer learning. The proposed transfer deep reinforcement learning-based methodology is presented in Section~\ref{method}. Section~\ref{exp} presents experimental results comparing the proposed method against several baselines. Finally, the paper concludes in Section~\ref{conclusion}.


