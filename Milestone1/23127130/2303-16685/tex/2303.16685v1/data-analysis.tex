% \textcolor{gray}{Di: I feel we jumped into these details too quickly. Can we add one sentence to introduce the main focus of experiment part.} 
%This section presents the evaluation of our policy reuse framework. 
% In order to create various traffic scenarios, we collected a proprietary dataset of hourly communication traffic from an existing communication network over one week. In this dataset, there are 21 sectors and each sector has 4 cells with different frequencies and capacities. This dataset was used to tune a proprietary system-level network simulator so that it mimicks the real-world traffic conditions. 


We collected a proprietary dataset of hourly communication traffic from an existing communication network over one week. In this dataset, there are 21 sectors and each sector has 4 cells with different frequencies and capacities. This dataset was used to tune a proprietary system-level network simulator so that it mimics real-world traffic conditions. Details about the dataset and simulator are presented in Section~\ref{sec:traffic-analysis} and~\ref{sec:simulator}, respectively. Section~\ref{sec:baselines} lists the baselines that we use to compare our proposed method. Section~\ref{sec:policy-bank-analysis} constructs and analyses the policy bank that we obtained using simulated scenarios. Finally, Section~\ref{sec:policy-sector-training} and~\ref{sec:policy-sector-result} present our experiment with the policy selector and its performance evaluation.

%Our experiment studies load balancing on these traffic scenarios. Section~\ref{sec:traffic-analysis} presents these traffic scenarios and their clustering in detail. 
%
%To enable the RL algorithm to interact with a simulated environment, we recreate the real-world scenarios within a proprietary system-level network simulator by tuning the simulator's configurations so that it mimics real-world traffic conditions. 
%For each scenario, the configuration that results in a traffic condition matching the closest to the real-world data is kept. 
% More details about the simulator are presented in Section~\ref{sec:simulator}.

%Section~\ref{sec:baselines} presents the baseline methods we use to compare the effectiveness of our proposed framework. In Section~\ref{sec:policy-bank-analysis}, we investigate the performance of the policies in our policy bank on each traffic scenario. The high performance of policies on novel traffic scenarios similar to the traffic scenario seen during training further supports the need for a policy selector. Finally, we explain in detail our experiments using the policy selector in Section~\ref{sec:policy-sector-training} and present its results in Section~\ref{sec:policy-sector-result}.

\begin{figure}[t]
\centering
\begin{tabular}{c}
\includegraphics[width=0.7\columnwidth]{Figures/21sec-rl-analysis_train_group2.png} \\
(a) Average reward on the training scenarios \\
~\\
\includegraphics[width=0.7\columnwidth]{Figures/21sec-rl-analysis_test_group2.png} \\
(b)  Average reward on the testing scenarios \\
\end{tabular}
    \caption{Comparison of the average reward over one week between policies in the policy bank $\Pi$ (orange), the policy $\tilde{\pi}$ trained on all scenarios in $\mathcal{X}$ (blue), and BasicLB (green) across training and testing scenarios. For the policy bank evaluation (orange), we show the mean and indicate the minimum and maximum with error bars.}
    \label{fig:rl_policy_21_sec}
\end{figure}

\subsection{Traffic clustering and analysis}
\label{sec:traffic-analysis}
%\textcolor{gray}{Jimmy: I think we need to add 1 sentence here stating the overall purpose of this subsection. For example, something like "In this section we analyze the three predominant groups of network conditions...blah blah blah, by analyzing the distribution of traffic among various cells....". I think you can write something more accurate than me. } 
% Load balancing aims to move UEs from high traffic cells to low traffic cells to improve the overall efficiency of the network. %Various factors related to the environment, time and the system hardware can describe the traffic condition of a cell. 
%In this section, we conduct preparatory analysis to construct a diverse set of training scenarios with which we use to build our policy bank. 
To identify different types of traffic, we applied the K-Means clustering algorithm to the daily traffic condition described by three traffic-related factors: the number of active UEs, network throughput, and 
% resource utilization in terms of 
the percentage of physical resource block (PRB) used. Three interpretable groups emerge from the clustering process: %After 100 iterations, we divide our dataset into three interpretable groups: 
% The communication traffic can be affected by many factors such as the UE distribution, time of day, and system hardware. For this analysis, we consider the same factors used in state description of our MDP: (1) the number of active UEs, (2) network throughput, and (3) the bandwidth utilization in terms of the percentage of physical resource block (PRB) used. Using K-Means clustering on the daily traffic condition, we can divide our dataset into three interpretable groups:
% \textcolor{blue}{tianyu: I would just remove the "relatively" in all the items. In addition, I think figure 2 should be moved to this page.}
(Group 1) High traffic on the first cell; 
(Group 2) High traffic on the fourth cell;  and 
(Group 3) Low traffic in general.
Figure~\ref{fig:traffic_21_sec} shows how traffic varies across the 21 different traffic scenarios across these three groups. A cell has a high volume of traffic when it has a large number of active UEs, low throughput, and high utilization. 

%We conducted further analysis on the temporal aspect of the traffic condition for each group. %Figure~\ref{fig:avg_traffic_group} shows the traffic condition during low and high utilization hours. 
%We defined low utilization hours to be between midnight and 8 AM, which matches approximately to the time when most human consumers are inactive, and high utilization hours to be the other hours, when they are more likely to be actively accessing the network. From the figure, we observe that the magnitude of each traffic-related factor shifts significantly between the two time periods.

%\begin{figure}[ht]
%\centering
    % \subfigure[Average traffic.]{\includegraphics[width=\linewidth]{Figures/traffic_all_time_21sec.png}\label{fig:avg_traffic_group_all_time}}
%    \hspace{0.01\linewidth}
%    \subfigure[Average traffic during low utilization hours (0:00-8:00)]{\includegraphics[width=\linewidth]{Figures/traffic_non_peak_time_21sec.png}\label{fig:avg_traffic_group_non_peak}}
%    \hspace{0.01\linewidth}
 %   \subfigure[Average traffic during high utilization hours (8:00-24:00)]{\includegraphics[width=\linewidth]{Figures/traffic_peak_time_21sectors.png}\label{fig:avg_traffic_group_peak}}
 %   \caption{Average traffic over one week for each group at low and high utilization hours, illustrating the general daily traffic pattern shared across each group.}\label{fig:avg_traffic_group}
%\end{figure}

\begin{figure}[t]
\centering
\begin{tabular}{c}
\includegraphics[width=0.8\linewidth]{Figures/training-result-reward.png} \\
(a) Average reward on the training scenarios \\
~\\
\includegraphics[width=0.8\linewidth]{Figures/testing-result-reward.png} \\
(b) Average reward on the testing scenarios
\end{tabular}
\caption{Comparison of the average reward over 6 days. Our policy reuse framework with the policy selector (blue) achieves the closest performance to our upper bound BEST-$\pi$ (red) on average. For the training scenarios, it is exactly the same as BEST-$\pi$.}
\label{fig:reward_results}
\end{figure}

\subsection{Simulator}
\label{sec:simulator}
%\textcolor{gray}{Jimmy: Again, adding a high level sentence here would be good. Something like "Key to our experimental framework is a system-level simulator that allows our RL agent to collect interaction experiences for training...."} 
%A key to our experiments is the use of a communication network simulator to mimic real-world data, allowing our RL algorithm to interact with our collected traffic scenarios.
We use a proprietary system-level network simulator, as in~\cite{kang2021hrl}. 
%An example of a simulation scenario is shown in Figure~\ref{fig:sls}. 
This simulator emulates 4G/5G communication network behaviours, and supports various configurations that allow us to customize the traffic condition. In our experiment, we fix the number of base stations to 7, with one base station in the center of the layout. Each base station has 3 sectors and each sector has 4 cells with different carrier frequencies that are identical across all sectors and base stations. We vary the number and distribution of UEs, the packet size and request interval such that the simulation traffic condition at the north-east sector of the center base station matches  the real-world data presented. In our experiments, we aim to balance the load in this particular sector. Our RL policies are only aware of the control parameters and the traffic condition in this sector. 
%Our evaluation is also based on the performance of this sector.

To mimic real-world data, a fraction of the UEs are
% \textcolor{blue}{tianyu:it's what we do right? so just use "a portion of UEs IS ...", also remove the "the" before UEs.} 
uniformly concentrated at specified regions while the remaining are uniformly distributed across the environment. These dense traffic locations change at each hour. All UEs follow a random walk process with an average speed of 3~m/s. The packet arrival follows a Poisson process with variable size between 50~Kb to 2~Mb and inter-arrival time between 10 to 320~ms. Both are specified at each hour to create the desired traffic condition.
%\begin{figure}[ht]
%    \centering
%        \vspace{1mm}
%    \includegraphics[width=0.8\linewidth]{Figures/sls_simulator.png}
%    \caption{An example of a simulation scenario with 7 base stations, each with 3 sectors and 4 cells on each sector. Active and idle UEs connected to different cells are indicated by different shapes and colors.}
%    \label{fig:sls}
%\end{figure}

\subsection{Baselines}
\label{sec:baselines}
To showcase the effectiveness of the proposed method, we compare our solution with the following baselines: 
% \textcolor{gray}{tianyu: Maybe we should include the performance on learning from scratch to further showcase that we perform reasonably well, i.e. not too far away from the learning from scratch method.}
\textbf{Rule-based load balancing (BasicLB)} uses a fixed set of LB parameter values for all traffic scenarios. \textbf{Adaptive rule-based load balancing} (AdaptLB)~\cite{yang2012high} changes the LB parameter values based on the load status of the cells. \textbf{Random policy selection (RAND-$\pi$)} randomly selects a policy in the policy bank $\Pi$ at the beginning of every day. \textbf{Best policy selection (BEST-$\pi$)} selects the best policy based on the performance of all policies in $\Pi$ on the unseen scenarios in ${X}'$ for the whole week. \textbf{New policy trained on the unseen scenario (NEW-$\pi$)} directly trains a new RL policy on the unseen traffic scenario from scratch. BEST-$\pi$ is the best possible performance obtainable form one policy in the policy bank
%, as shown show in Figure~\ref{fig:rl_policy_21_sec_test} 
and it is not a feasible solution to deploy on a real network due to the use of exhaustive search. Similarly, NEW-$\pi$ is another upper bound on performance, and it is also not feasible if the RL agent is not allowed to learn from scratch on an unseen traffic scenario.


\subsection{Policy bank construction and analysis}
% \subsection{Policy bank analysis}
\label{sec:policy-bank-analysis}

%In this section, we conduct preliminary investigations on the performance of the RL policies in the policy bank
% \textcolor{blue}{tianyu: a "what" RL policy? maybe change it to "... performance of the constructed policy bank"?} 
%on unseen traffic scenarios. 
To ensure that our policy bank contains a diverse selection of policies trained from all types of traffic, we randomly select 3 traffic scenarios from each group introduced in Section~\ref{sec:traffic-analysis} to form our set of 9 training scenarios $\mathcal{X}$ and use the remaining 12 scenarios $\mathcal{X}'$ for testing. Following the formulation in Section~\ref{sec:rl-formulation}, we train one PPO policy for each $X\in \mathcal{X}$, creating a policy bank $\Pi$. The reward $R_t$ is the weighted average of the performance metrics defined in Section~\ref{sec:performance-metrics}.
% :
% \[
%     R_t = \frac{0.25}{10.7} G_{avg} + \frac{4}{4.9} G_{min} + \frac{0.5}{2.4(1+G_{sd})} + 0.2 G_{cong}.
% \]
% Because these metrics have values at different scales, we obtain these weights by searching exhaustively through a finite set of weight combinations and these weights were chosen due to their empirical performance for training an RL policy. 
% Note that we use the reciprocal of the shifted $G_{sd}$ so that maximizing the reward minimizes $G_{sd}$.
The weights are selected according to the empirical performance and they are correlated with the magnitude of the metrics. 
Note that we use the reciprocal of $G_{sd}$ so that maximizing the reward minimizes $G_{sd}$.
We also construct another RL policy $\tilde{\pi}$ trained on all scenarios in $\mathcal{X}$ for comparison. This is done by collecting interaction experience on each of the scenarios in parallel at each iteration in the learning process. All policies are trained for 200K interactions.
% with a decay $\lambda=0.97$. 
We use the PPO implementation in the Stable-Baseline 3~\cite{stable-baselines3} Python package. 

% The following was moved from the Method section:
We model the policy selector by a feed-forward neural network classifier with 3 hidden layers (with 128, 64, and 32 neurons, respectively), each preceded by a batch normalization and followed by a rectified linear unit activation. The output layer uses a softmax activation. The architecture hyperparameters were chosen using cross-validation.
% and 1 output layer.
% \textcolor{blue}{tianyu:is it 4 hidden layers or 3 layers with 1 output layer? I think maybe just be a bis specific here by saying "3 hidden layers and 1 output layer" to avoid confusion.)} 
% Each of the first three layers follows a batch normalization. The number of neurons for these layers are 128, 64 and 32, respectively.  
% \textcolor{gray}{tianyu: did we use cross validation to choose the network structure? if so maybe add it.} 
% We use rectified linear unit activation for the first three layers and softmax for the last layer. The number of layers, the number of neurons for each layer and the activation function were chosen using cross-validation.

%\textcolor{gray}{Jimmy: I feel the wording ``picking any policy" is not very clear, because it sounds like you are choosing a policy in the policy bank. I think it would be better to say something along the lines of ``the average performance of executing each policy in the policy bank on the target scenario"} 
Figure~\ref{fig:rl_policy_21_sec} illustrates the performance of executing each policy in the policy bank $\Pi$ in comparison with $\tilde{\pi}$ and the BasicLB method. We observe that, for some scenarios, the minimum possible average reward resulted from an RL policy in $\Pi$ lies
% \textcolor{blue}{tianyu: lies} 
much lower than the average reward resulted from the BasicLB. This supports the assumption that an RL policy trained on one scenario may not generalize well to another, and implies that randomly choosing a policy from the policy bank can significantly degrade the performance for some scenarios. On the other hand, the maximum possible average reward from an RL policy in $\Pi$ is always higher than the average reward resulted by $\tilde{\pi}$ and BasicLB even for the test scenarios. Furthermore, $\tilde{\pi}$ under-performs BasicLB for some test scenarios such as 12 and 14. This indicates that with careful selection of a policy trained from an individual scenario, we can achieve significant improvement over a policy trained on multiple scenarios and BasicLB. The next sections will present our results with the policy selector.



%\begin{figure}[ht]
%\centering
%    \hspace{0.01\linewidth}
%    \subfigure[Average reward on the training scenarios]{\includegraphics[width=\linewidth]{Figures/21sec-rl-analysis_train_group2.png}\label{fig:rl_policy_21_sec_train}}
%    \hspace{0.01\linewidth}
%    \subfigure[Average reward on the testing scenarios]{\includegraphics[width=\linewidth]{Figures/21sec-rl-analysis_test_group2.png}\label{fig:rl_policy_21_sec_test}}
    %\caption{Comparison of the average reward over one week for choosing any policy in the policy bank $\Pi$ against a policy $\tilde{\pi}$ trained on all scenarios in $\mathcal{X}$ and BasicLB. The blank line shows the minimum and maximum average reward.}
%    \caption{Comparison of the average reward over one week between policies in the policy bank $\Pi$ (orange), the policy $\tilde{\pi}$ trained on all scenarios in $\mathcal{X}$ (blue), and BasicLB (green) across training and testing scenarios. For the policy bank evaluation (orange), we show the mean, and indicate the minimum and maximum with error bars.}
%    \label{fig:rl_policy_21_sec}
%\end{figure}

%To further understand the correlation between the RL policy and the traffic condition, we conduct an  experiment that verifies whether policies trained on similar traffic conditions result in higher performance. The similarity between individual sectors' traffic patterns is non-trivial to define due to their high volatility. Hence, for simplicity, we use the L2 distance on the average traffic condition during the low and high utilization hours. Table~\ref{tab:similarity-performance} show the smallest number $k$ such that the policy trained on the $k$th most similar traffic scenario is within the top $k$ best performance for the unseen scenarios in $\mathcal{X}'$. Interestingly, we observe that group 2 has the smallest $k$ on average and group 1 has the largest $k$ on average. Referring back to Figure~\ref{fig:rl_policy_21_sec_test}, we see that any policy in $\Pi$ can easily outperform the rule-based method in a group 1 scenario, while it is much harder in a group 2 scenario. This demonstrates the benefit of selecting a policy based on the similarity of the traffic condition between the new scenario and the seen scenarios. 



%\begin{table}[!htbp]
%\centering
%\caption{The smallest number $k$ such that the policy trained on the $k$th most similar traffic scenario is within the top $k$ best performance for the unseen scenarios.}
%\label{tab:similarity-performance}
%\begin{tabular}{|c | c | c|  c | c | c |}
%\hline
%\multicolumn{2}{|c|}{\bf Group 1} & %\multicolumn{2}{c|}{\bf Group 2} &
%\multicolumn{2}{c|}{\bf Group 3}\\
%\hline
%Scenario  & $k$  & Scenario  & $k$  & Scenario &  $k$ %\\
%\hline
%3 & 3 & 8 & 1 & 16 & 1\\
%4 & 4 & 12 & 2 & 17 & 2 \\
%5 & 3 & 13 & 1 & 18 & 2 \\
%7 & 3 & 14 & 1 & 19 & 3\\
%\hline
%\end{tabular}
%\end{table}