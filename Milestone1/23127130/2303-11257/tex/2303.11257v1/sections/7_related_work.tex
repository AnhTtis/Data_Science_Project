\paragraph{Variance scaling analysis}
\citet{Klambauer17} and \citet{Peiwen22} propose activation functions that encourage unit-variance activations and gradients, which are complementary to unit scaling. \citet{He16} introduce residual networks, using skip connections and explicit normalisation to stabilise forward and backward passes. Variants on normalisation~\citep{Ioffe15,Ba16,Labatie21,Salimans16}  are complementary to unit scaling, which considers the norm of the gradients as well as activations and does not constrain activation norms after initialisation. Alternative residual schemes~\citep{Zhang19,Brock21} can be incorporated into unit-scaled models, although the residual layer output variance should not be allowed to grow with depth.

The reparameterisation implied by unit scaling is also used by \citet{Jacot18}, later broadened by \citet{Yang20} and exploited by \citet{Yang22} in their work analysing the training behaviour of deep networks. Motivated by low-precision computation rather than training dynamics, unit scaling applies scaling factors locally throughout the compute graph, but the effect on training hyperparameter scaling is similar.

\paragraph{FP8 inference}
Although there has been little hardware support for FP8 training, accelerated 8-bit inference is increasingly common via the use of integer quantisation \citep{Jacob18} to the INT8 format. This process typically results in degraded accuracy, requiring additional techniques such as quantisation-aware training (see \citet{Nagel21} for a thorough discussion on this topic). Though recent efforts have been made to improve efficient INT8 quantisation \citep{Yao22, Park22, Dettmers22, Xiao22}, the use of FP8 enables accelerated inference in the same format as training, promising a substantial improvement in the simplicity and accuracy of 8-bit inference \citep{Kuzmin22}.
