\subsection{Character language modelling}

\paragraph{Experimental Setup} To evaluate unit scaling for multiple model architectures and optimisers, we perform small-scale experiments on WikiText-103 raw character language modelling \citep{Merity17}. We train causal language models, using cross entropy loss during training and evaluate on bits per character (BPC). All models follow the pattern of a Transformer decoder layer \citep{Vaswani17}, with the following variants:

\textit{Sequence layer type}: Attention, RNN and Convolution.

\textit{Norm placement}: PreNorm, PostNorm and NoNorm.

\textit{Residual scaling}: default, fixed and running-mean (as defined in Section~\ref{sec:scaling_strategy}).

Over the product of these settings, we compare the performance of regular (baseline) and unit scaling in both FP32 and FP16. For this, we also evaluate the regular model in FP16 with loss scaling. For full hyperparameters and details, see Appendix~\ref{app:char}.

\paragraph{Results} The above configurations amount to a 2092-run sweep, the results of which are shown in Figure~\ref{fig:char_sweep}. First, these demonstrate the need for scaling when using FP16. This is due to gradient underflow, since loss scaling with a factor of 2048 resolves the issue. Second, they demonstrate that unit scaling, despite changing the training behaviour of the model beyond just numerics, matches or even slightly improves upon baseline performance in almost all cases. Finally, they show that no tuning is necessary when switching unit scaling to FP16.

We also explore the effect of using different residual scaling schemes, with results shown in Figure~\ref{fig:char_residual}. We find that performance is not sensitive to the choice of scheme, and suggest that running-mean or fixed are reasonable choices when using unit scaling.

\subsection{Masked language modelling}
\label{sec:mlm}


\begin{table*}[h!]
\caption{Downstream performance of regular and unit-scaled BERT models. We pretrain 3 models for every \textit{model-method-format} combination, then fine-tune 5 SQuAD v1.1 and 5 v2.0 runs for each (i.e. 15 runs per downstream task). The values shown represent the mean across the 15 runs, with ± indicating the standard deviation across the mean scores of the 3 sub-groups.
† published result from \citet{Devlin19}.
‡ published result from \citet{Noune22}; this model also adds an activation scale alongside the loss scale.}
\label{tab:bert_squad_results}
\centering
\vspace{0.6em}

\begin{tabular}{lllllll}
\toprule
\multirow{2}{*}{Model} &
  \multirow{2}{*}{Method} &
  \multirow{2}{*}{Precision} &
  \multicolumn{2}{c}{SQuAD v1.1} &
  \multicolumn{2}{c}{SQuAD v2.0} \\
 &              &      & \multicolumn{1}{c}{EM}            & \multicolumn{1}{c}{F1}            & \multicolumn{1}{c}{EM}            & \multicolumn{1}{c}{F1}            \\ \hline
\multirow{4}{*}{Base} &
  No Scaling † &
  FP32 &
  80.8 &
  88.5 &
  — &
  — \\ &
  Loss Scaling &
  FP16 &
  80.55 (±0.16) &
  88.19 (±0.16) &
  73.36 (±0.27) &
  76.47 (±0.23) \\
 & Unit Scaling & FP16 & 79.96 (±0.31) & 87.86 (±0.44) & 72.31 (±0.60) & 75.70 (±0.53) \\
 & Unit Scaling & FP8  & 80.15 (±0.18) & 88.04 (±0.12) & 72.28 (±0.02) & 75.67 (±0.01) \\ \hline
\multirow{5}{*}{Large} &
  No Scaling † & FP32 & 84.1 & 90.9 & 78.7 & 81.9 \\
 & Loss Scaling & FP16 & 84.23 (±0.20) & 90.93 (±0.14) & 77.52 (±0.63) & 80.54 (±0.61) \\
 & Loss Scaling ‡ & FP8 & 83.40 (±0.23) & 90.69 (±0.16) & — & — \\
 & Unit Scaling & FP16 & 85.67 (±0.10) & 92.14 (±0.08) & 79.94 (±0.10) & 82.97 (±0.09) \\
 & Unit Scaling & FP8  & 85.22 (±0.03) & 91.77 (±0.10) & 79.29 (±0.31) & 82.29 (±0.29) \\ \bottomrule
\end{tabular}

\end{table*}

\paragraph{Experimental setup}
To evaluate unit scaling against a standard baseline known for challenging numerics, where loss scaling is conventionally required \citep{Lin2020}, we train unit-scaled BERT\textsubscript{BASE} and BERT\textsubscript{LARGE} models.

We use the standard BERT masked language model pretraining objective over English Wikipedia articles, and demonstrate downstream performance on SQuAD v1.1 and SQuAD v2.0 \citep{Rajpurkar2016, Rajpurkar2018}. We follow the unit scaling recipe, along with our guide on aligning a unit scaled model with a regular model (Appendix~\ref{app:aligning}).

Full hyperparameters and details are covered in Appendix~\ref{app:mask}. Note that we do not sweep any additional hyperparameters for our unit-scaled BERT (or character language models) relative to the baselines.

\paragraph{Results}

We report our results in Table~\ref{tab:bert_squad_results}. For unit scaling in FP16, we are able to attain the same performance as the baseline model, and whereas the baseline requires sweeping a loss scale, unit scaling works in all cases out-of-the-box. Due to differences in the effective optimiser step size across parameters (Section~\ref{sec:recipe}), our regular and unit-scaled models aren't exactly equivalent, but deviations in their downstream performance are minor (BERT\textsubscript{BASE} is slightly below the baseline, and BERT\textsubscript{LARGE} is slightly above).

For FP8, we build on the results of \citet{Noune22} who demonstrate the training of loss-scaled BERT in FP8 with no degradation relative to FP16.
We show that the same can also be achieved with unit scaling, with no additional techniques required to make FP8 work over FP16---we simply quantise our matmul inputs into FP8 and are able to train accurately. These results represent the first time BERT\textsubscript{BASE} or BERT\textsubscript{LARGE} have been trained in either FP16 or FP8 without requiring a form of loss scaling.

To highlight the precise effects of unit scaling, we show histograms for activations, weights and gradients for unit-scaled FP16 BERT. These can be found in Figures~\ref{fig:bert_scaling_us_init},~\ref{fig:bert_scaling_us_end}, alongside equivalent plots for a regular FP16 BERT.

We will open-source our code upon publication, to enable inspection of our methods and reproduction of our results.
