\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{figures/numerics_snr.pdf}
\vspace{-0.9em}
\caption{The signal to noise ratio (SNR) of samples from a normal distribution, quantised in FP16 and FP8, as a function of the distribution's scale.}
\label{fig:signal_to_noise}
\vspace{-0.9em}
\end{figure}

\begin{table*}
    \caption{A comparison of techniques for low precision training. `$\sim$' indicates that this method ideally requires no tuning, but in practice may introduce hyperparameters that need to be swept.}
    \label{tab:method_comparison}
    \centering
    \vspace{0.6em}
    \begin{tabular}{lP{3.0cm}P{2.8cm}P{3.3cm}P{1.6cm}}\toprule
        Method & Fine-grained scaling & No tuning required & Adapts during training \\
        \midrule
        Loss scaling & × & × & ×\\
        Automatic loss scaling & × & \checkmark & \checkmark \\
        Automatic per-tensor scaling & \checkmark & $\sim$ & \checkmark\\
        Unit scaling & \checkmark & \checkmark & ×\\
        \bottomrule
    \end{tabular}
\end{table*}

\subsection{Floating-point formats for deep learning}
\label{sec:formats}

\paragraph{Definition}

The conventional representation used for floating point numbers is defined by the IEEE 754 standard \citep{IEEE2019}. In this standard, a binary floating point format can be defined by specifying the number of exponent bits, $E$, and the number of mantissa bits, $M$. A value within such a format is defined by a sign bit, exponent and mantissa value. Each is represented using a bit-string of the requisite length (with values $b_{\textrm{sign}}, b_{\textrm{exp}}, b_{\textrm{mant}}$ respectively), which are interpreted as follows:
\[\begin{aligned}
\textrm{exponent} &= b_{\textrm{exp}} - \textrm{bias},\quad \; (\textrm{bias}=2^{E-1}-1)
\\
\textrm{mantissa} &= 1 + \frac{b_{\textrm{mant}}}{2^M},
\\
\textrm{value}&=(-1)^{b_{\textrm{sign}}} \times 2^{\textrm{exponent}}\times \textrm{mantissa}
\end{aligned}\]
There are also a small number of `special values' which denote bit-strings to which the above interpretation does not apply. These represent infinities, NaN (not-a-number) and a range of `subnormal numbers' which allow for the representation of even smaller (absolute) values.

Common floating point formats used in machine learning that implement the IEEE 754 standard are shown in Table~\ref{tab:fp_formats}. The term \textit{low precision} typically refers to all formats requiring fewer than 32 bits. More recently, two kinds of FP8 format have been proposed, which we term E4 and E5, i.e. $(E,M)=(4,3) \textrm{ or } (5,2)$. These are similar to the IEEE 754 standard, but contain differences, especially for the representation of special values. These formats are covered in detail in Appendix~\ref{app:fp8_formats}.

\paragraph{Quantisation error}

Formats with more exponent bits are able to represent a wider range of values, whereas those with more mantissa bits have smaller gaps between represented values.
This trade-off between range and precision can be framed in terms of \textit{quantisation error}. This consists of two terms: the loss of accuracy due to values lying outside the absolute range of a format (overflow or underflow) is termed the \textit{clipping error} (or \textit{saturation error}), whereas the loss of accuracy due to values lying between representable numbers is termed the \textit{rounding error}.

We demonstrate the effect quantisation error has for different formats in Figure~\ref{fig:signal_to_noise}. This shows the signal to noise ratio (SNR) of normally distributed values $X \sim \mathcal{N}(0, \sigma^2)$ quantised in FP16 and FP8 as $\sigma$ varies. SNR measures the faithful reproduction of an input (signal) versus the error (noise) introduced, defined as $\mathop{\mathbb{E}}[X^2] / \mathop{\mathbb{E}}[(q(X) - X)^2]$, where $q(\cdot)$ is the quantisation function mapping an input to the nearest representable value.

The heights of the SNR curves reflect the level of rounding error incurred by each format, and the widths reflect the range in which they are free of clipping error. With the exception of subnormal numbers (which slope away on the left-hand-side), the height of each format’s SNR curve is roughly constant. This reflects the fact that exponents are evenly distributed, giving a relative rounding error that is approximately uniform.


\subsection{Trade-offs of low-precision training}
\label{sec:lpt-tradeoffs}

\paragraph{Drawbacks}
The two common 16-bit formats, FP16 and BFLOAT16, offer different trade-offs: FP16 has more precision, but BFLOAT16 has more range. As a result FP16 is more prone to clipping error, requiring careful scaling, and BFLOAT suffers more from rounding error, which in some cases can degrade model accuracy \citep[e.g.][]{Rae2021}.

For FP8 there is a reduction in both range and precision. For range, the same techniques used to train in FP16 are required, and for precision,
the use of FP8 has thus far been restricted to only the inputs of matmul (matrix multiply) operations \citep{Sun19, Noune22, Micikevicius22}, with 3 mantissa bits typically required for weights and activations, and 2 mantissa bits for gradients.

\paragraph{Benefits}
The potential efficiency gains when using low-precision formats are substantial. These include memory usage (often a limiting factor for large models), bandwidth usage (the main overhead for low-arithmetic-intensity ops), compute (the main overhead for high-arithmetic-intensity ops) and cross-device communication (a substantial overhead for distributed training).

\subsection{Low-precision training techniques}
\label{sec:lpt-techniques}

Here we analyse existing techniques for addressing the challenges of low precision training. Table~\ref{tab:method_comparison} provides a summary of their trade-offs and a comparison with unit scaling.

\paragraph{Mixed precision}

Mixed precision is the use of multiple number formats with different bit-widths. This differs from the traditional approach of placing all values in FP32, with \citet{Micikevicius18} showing that most activations, weights and gradients (collectively, \textit{tensors}) can be put in FP16 with no loss in accuracy, with the exception of master weights that are often kept in FP32. Mixed precision training is also possible in BFLOAT16 \citep{Kalamkar19}.

By `training in FP8' we mean that matmuls are performed in FP8 (inputs are cast down to FP8, with outputs in higher precision) with wider formats typically used elsewhere, following the lead of \citet{Sun19, Noune22} and \citet{Micikevicius22}. FP8 reduces both precision and range, and has not generally been used for other operations as matmuls benefit most from using low-precision formats.

Mixed precision training is complementary to unit scaling---all of our experiments use some form of mixed precision.

\paragraph{Loss scaling}

Reduced range in FP16 and FP8 is particularly challenging for the backward pass, where standard model-design practices lead to gradients that risk underflow.

To combat this, \citet{Micikevicius18} have observed that the loss can be multiplied by a scalar to increase the scale of gradients, where weight gradients are then divided by the same scalar in the optimiser. This is valid due to the linearity of the backward pass implicit in the chain rule. Loss scaling is often essential to accurate mixed precision training in FP16 and FP8.

However, there is no theoretical motivation for the choice of loss scale, which instead must be found empirically. This comes with a number of downsides.
Firstly, a hyperparameter sweep must be conducted to find the loss scale value. This can require multiple full runs, as insufficient loss scales may only become apparent later in training. Secondly, it's not clear ahead-of-time what changes require the loss scale to be re-swept. Thirdly, as loss scaling only applies a single, global scaling factor, it has no mechanism to combat differences in scale between gradient tensors. For some models this difference may be too large for effective training.

\paragraph{Automatic loss scaling}

The dynamic adjustment of the loss scale during training is termed \textit{automatic loss scaling} \citep{Kuchaiev2018}. This can remove the need to sweep the initial loss scale, and combats shifts in tensor distributions during training.

The combination of automatic loss scaling and automatic selection of number formats, is termed \textit{automatic mixed precision} \citep{Pytorch23}. Unit scaling doesn't specify tensors' formats, so can be used in systems that automate it.

\paragraph{Per-tensor scaling}

To address the inherent scaling difficulties of FP8 training, \citet{Micikevicius22} propose a per-tensor scaling system, re-scaling locally based on runtime statistics.

Like unit scaling, at the beginning of training this technique may be able to achieve well-scaled tensors throughout the model. However, additional compute, memory, bandwidth and cross-device communication costs may be incurred by the recording of statistics (see Section \ref{sec:discussion} for a more detailed discussion of the potential compute overheads incurred by each of these schemes).
