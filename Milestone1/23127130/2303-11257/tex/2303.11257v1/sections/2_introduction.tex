The development of algorithms that efficiently leverage available hardware has been key to the substantial advances seen in deep learning over the last decade \citep{Sutton19, Hooker21}.

With the increase in size of state-of-the-art models, hardware-efficiency is also motivated by the need to lower the costs of training. These have grown to become substantial---in terms of money, time, and environmental impact \citep{Strubell19, Chowdhery22, Luccioni22}.

However, with the end of Moore's law and Dennard scaling \citep{Esmaeilzadeh11, Theis17}, increased transistor density can no longer be relied upon to provide a simple path towards greater efficiency, and other techniques must be leveraged. One such technique is the use of low-precision number formats. The gains to be had here are considerable: compute, memory and bandwidth usage all depend on the bit-width of a format.

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.5\textwidth, trim = 0.1cm 0.2cm 0.4cm 0.2cm, clip]{figures/fig_1a_v2.pdf}
    \includegraphics[width=0.5\textwidth, trim = 2.5cm 2.5cm 2.5cm 2.5cm, clip]{figures/fig_1b_v2.pdf}
    \end{center}
    \vspace{-0.3cm}
    \caption{
    \textit{Above:} Unit scaling of an FFN layer. We multiply each tensor by a fixed scalar to achieve consistent scale, no longer requiring a loss scale to control the scale of $\nabla_{x_4}$. Hyperparameters here are the same as those in our BERT\textsubscript{LARGE} experiments (Table~\ref{tab:mask_hyperparameters}).
    \vspace{0.5em}\\\hspace{\textwidth}
    \textit{Below}: A histogram of exponent values at initialisation for the above FFN, with shade indicating bin density.
    The $y$-axis reflects exponent values available in FP16, while dashed  lines show the max/min exponents of the FP8~E4 format of \citet{Noune22}. 
    }
    \label{fig:illustration}
    \vspace{-0.2cm}
\end{figure}

Unlike inference, where integer quantisation is possible \citep{Jacob18}, for training, floating point formats are required~\citep{Noune22, Micikevicius22, Kuzmin22}. The traditional approach of using 32-bit floats is being superseded by mixed precision strategies, which place many values into 16-bit formats~\citep{Micikevicius18}. Furthermore, 8-bit floating-point hardware is becoming available \citep{Graphcore22, Nvidia22}, with the potential for accurate 8-bit training already demonstrated~\citep{Wang18, Sun19, Noune22, Micikevicius22}.

However, the use of low-precision formats introduces new difficulties, reducing the absolute range of representable values and increasing quantisation noise. Existing techniques to address these issues either introduce additional overhead or require manual tuning. An approach is needed which is both accurate and places minimal burden on the user.

To this end, we present {\em unit scaling}: a technique for model design that operates on the principle of ideal scaling at initialisation (unit variance for activations, weights and gradients). This is achieved by considering how each operation in the model affects the variance of different tensors, and introducing fixed scaling factors to counteract changes.

Empirically, we show that unit scaling aligns values much closer to the centre of the representable range than conventional loss scaling \citep{Micikevicius18}, and removes the need for a scaling hyperparameter to be swept. None of our experiments require dynamic re-scaling of values, indicating robustness to shifting distributions during training.

\subsection{Contributions}
In this paper we make the following contributions:
\begin{enumerate}[itemsep=-0.2em]
\vspace{-0.7em}
\item We provide an analysis of how scale changes as a result of operations within a typical model, and the challenges this introduces for low-precision training.
\item We present unit scaling: a method for combating changes in scale, along with an implementation recipe and code examples.
\item We validate unit scaling empirically across a range of models and optimisers.
\item For the first time, we show training of BERT\textsubscript{BASE} and BERT\textsubscript{LARGE} in FP16 without loss scaling. We then go a step further, training successfully in FP8, still without degradation.
\vspace{-0.7em}
\end{enumerate}

We emphasise that our method works out-of-the-box, with no extra sweeps or hyperparameters, demonstrating the effectiveness of unit scaling for simplifying the use of low-precision formats.
