% Updating the bib file:
% - Try to keep to the format FirstauthornameYY
% - Sort alphabetically by Firstauthorname
% - You can also use this, if wanted https://colab.research.google.com/drive/1asPsVmjbIlJJ6GvOWd0BIjvSId5nGNdC


@article{Abdolrashidi21,
    Title = {Pareto-optimal quantized {ResNet} is mostly 4-bit},
    Author = {Abdolrashidi, AmirAli and Wang, Lisa and Agrawal, Shivani and Malmaud, Jonathan and Rybakov, Oleg and Leichner, Chas and Lew, Lukasz},
    Journal = {IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2021},
    Year = {2021},
}

@article{Ba16,
    Title = {Layer normalization},
    Author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
    Journal = {arXiv preprint arXiv:1607.06450},
    Year = {2016},
}

@article{Bai22,
    Title={Constitutional AI: Harmlessness from AI Feedback},
    Author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
    Journal={arXiv preprint arXiv:2212.08073},
    Year={2022}
}

@article{Banner18,
	Title = {Scalable Methods for 8-bit Training of Neural Networks},
	Author = {Banner, Ron and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
	Journal = {arXiv preprint arXiv:1805.11046},
	Year = {2018},
}

@article{Bondarenko2021,
  Title = {Understanding and overcoming the challenges of efficient transformer quantization},
  Author = {Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  Journal = {arXiv preprint arXiv:2109.12948},
  Year = {2021},
}

@article{Brock21,
    Title = {High-performance large-scale image recognition without normalization},
    Author = {Brock, Andy and De, Soham and Smith, Samuel L. and Simonyan, Karen},
    Journal = {38th International Conference on Machine Learning, ICML 2021},
    Year = {2021},
}

@article{Carlini21,
    Title={Extracting training data from large language models},
    Author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
    Journal={30th USENIX Security Symposium (USENIX Security 21)},
    Pages={2633--2650},
    Year={2021}
}

@article{Chowdhery22,
    Title = {Palm: Scaling language modeling with pathways},
    Author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian et al.},
    Journal = {arXiv preprint arXiv:2204.02311},
    Year = {2022},
}

@article{Dai19,
    Title = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
    Author = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc Viet Le and Ruslan Salakhutdinov},
    Journal = {Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL}},
    Year = {2019}
}

@article{Dettmers22,
    Title = {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
    Author  = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	Journal = {arXiv preprint arXiv:2208.07339},
    Year = {2022},
}

@article{Devlin19,
    Title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    Author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    Journal = {2019 Conference Of The North American Chapter Of The Association For Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    Year = {2019},
}

@article{Esmaeilzadeh11,
    Title = {Dark silicon and the end of multicore scaling},
    Author = {Esmaeilzadeh, Hadi and Blem, Emily and St. Amant, Renee and Sankaralingam, Karthikeyan and Burger, Doug},
    Journal = {Proceedings of the 38th annual international symposium on Computer architecture},
    Pages = {365--376},
    Year = {2011}
}

@article{Glorot10,
    Title = {Understanding the difficulty of training deep feedforward neural networks},
    Author = {Glorot, Xavier and Bengio, Yoshua},
    Journal = {13th International Conference on Artificial Intelligence and Statistics, AISTATS 2010},
    Year = {2010},
}

@misc{Graphcore22,
	Title = {Graphcore Launches {C}600 {PCI}e Card For {AI} Compute },
	Author = {Graphcore},
	Year = {2022},
	Howpublished = {\url{https://www.graphcore.ai/posts/graphcore-launches-c600-pcie-card-for}
	\\
	\url{-ai-compute}},
	Note = {(Online: accessed 25 January 2023)}
}

@article{He15,
    Title = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
    Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    Journal = {IEEE International Conference on Computer Vision, ICCV 2015},
    Year = {2015},
}

@article{He16,
    Title = {Deep Residual Learning for Image Recognition},
    Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    Journal = {IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2016},
    Year = {2016},
}

@article{Hooker21,
  Title = {The hardware lottery},
  Author = {Sara Hooker},
  Journal   = {Communications of the Association for Computing Machinery},
  Year      = {2021},
}

@article{Huang20,
  Title = {Improving Transformer Optimization Through Better Initialization},
  Author = {Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  Journal = {Proceedings of the 37th International Conference on Machine Learning},
  Year = {2020},
}

@article{IEEE2019,
  Title = {{IEEE} Standard for Floating-Point Arithmetic},
  Author = {{IEEE}, Computer Society},
  Journal = {{IEEE} Std 754-2019},
  Year = {2019},
  Pages = {1--84},
}

@article{Ioffe15,
    Title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    Author = {Ioffe, Sergey and Szegedy, Christian},
    Journal = {32nd International Conference on Machine Learning, ICML 2015},
    Year = {2015},
}

@article{Jacob18,
    Title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
    Author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew G. and Adam, Hartwig and Kalenichenko, Dmitry},
    Journal = {IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2018},
    Year = {2018},
}

@article{Jacot18,
    Title = {Neural Tangent Kernel: Convergence and generalization in neural networks},
    Author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
    Journal = {Advances in Neural Information Processing Systems 31, NeurIPS 2018},
    Year = {2018},
}

@article{Jia2019,
    Title = {Dissecting the graphcore ipu architecture via microbenchmarking},
    Author = {Jia, Zhe and Tillman, Blake and Maggioni, Marco and Scarpazza, Daniele Paolo},
    Journal = {arXiv preprint arXiv:1912.03413},
    Year = {2019}
}

@article{Kalamkar19,
	Title = {A Study of {BFLOAT}16 for Deep Learning Training},
	Author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
	Journal = {arXiv preprint arXiv:1905.12322},
	Year = {2019},
}

@article{Kingma15,
    Title = {Adam: A method for stochastic optimization},
    Author = {Kingma, Diederik P. and Ba, Jimmy},
    Journal = {3rd International Conference on Learning Representations, ICLR 2015},
    Year = {2015},
}

@article{Klambauer17,
    Title = {Self-Normalizing Neural Networks},
    Author = {Klambauer, G\"{u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
    Journal = {Advances in Neural Information Processing Systems 30, NeurIPS 2017},
    Year = {2017},
}

@article{Kuchaiev2018,
    Title = {Mixed-precision training for nlp and speech recognition with openseq2seq},
    Author = {Kuchaiev, Oleksii and Ginsburg, Boris and Gitman, Igor and Lavrukhin, Vitaly and Li, Jason and Nguyen, Huyen and Case, Carl and Micikevicius, Paulius},
    Journal = {arXiv preprint arXiv:1805.10387},
    Year = {2018}
}

@article{Kuzmin22,
    Title = {FP8 Quantization: The Power of the Exponent},
    Author = {Kuzmin, Andrey and Van Baalen, Mart and Ren, Yuwei and Nagel, Markus and Peters, Jorn and Blankevoort, Tijmen},
    Journal = {arXiv preprint arXiv:2208.09225},
    Year = {2022},
}

@article{Labatie21,
    Title = {Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence},
    Author = {Labatie, Antoine and Masters, Dominic and Eaton-Rosen, Zach and Luschi, Carlo},
    Journal = {Advances in Neural Information Processing Systems 34, NeurIPS 2021},
    Year = {2021},
}

@article{Lin2021,
    Title={Truthful{QA}: Measuring how models mimic human falsehoods},
    Author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
    Journal={arXiv preprint arXiv:2109.07958},
    Year={2021}
}

@article{Lin2020,
    title={Multi-node {BERT}-pretraining: Cost-efficient approach},
    Author={Lin, Jiahuang and Li, Xin and Pekhimenko, Gennady},
    Journal={arXiv preprint arXiv:2008.00177},
    Year={2020}
}

@article{Liu2021,
    Title={DExperts: Decoding-time controlled text generation with experts and anti-experts},
    Author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
    Journal={arXiv preprint arXiv:2105.03023},
    Year={2021}
}

@article{Luccioni22,
  Title = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  Author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  Journal = {arXiv preprint arXiv:2211.02001},
  Year = {2022},
}

@article{Mellempudi19,
	Title = {Mixed Precision Training With 8-bit Floating Point},
	Author = {Mellempudi, Naveen and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat},
	Journal = {arXiv preprint arXiv:1905.12334},
	Year = {2019},
}

@article{Merity17,
    Title = {Pointer Sentinel Mixture Models},
    Author = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
    Journal = {5th International Conference on Learning Representations, ICLR 2017},
    Year = {2017},
}

@article{Micikevicius18,
    Title = {Mixed Precision Training},
    Author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
    Journal = {6th International Conference on Learning Representations, ICLR 2018},
    Year = {2018},
}

@article{Micikevicius22,
    Title = {{FP8} Formats for Deep Learning},
    Author = {Micikevicius, Paulius and Stosic, Dusan and Judd, Patrick and Kamalu, John and Oberman, Stuart and Shoeybi, Mohammad and Siu, Michael and Wu, Hao},
    Journal = {arXiv preprint arXiv:2209.05433},
    Year = {2022},
}

@article{Mishkin15,
    Title = {All you need is a good init},
    Author = {Mishkin, Dmytro and Matas, Jiri},
    Journal = {arXiv preprint arXiv:1511.06422},
    Year = {2015},
}

@article{Nadeem20,
    Title={Stereo{S}et: Measuring stereotypical bias in pretrained language models},
    Author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
    Journal={arXiv preprint arXiv:2004.09456},
    Year={2020}
}

@article{Nagel21,
    Title = {A White Paper on Neural Network Quantization},
    Author = {Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
    Journal = {arXiv preprint arXiv:2106.08295},
    Year = {2021},
}

@article{Noune22,
    Title = {8-bit Numerical Formats for Deep Neural Networks},
    Author = {Noune, Badreddine and Jones, Philip and Justus, Daniel and Masters, Dominic and Luschi, Carlo},
    Journal = {arXiv preprint arXiv:2206.02915},
    Year = {2022},
}

@misc{Nvidia22,
	Title = {NVIDIA {H}100 {T}ensor {C}ore {GPU} {A}rchitecture},
	Author = {Nvidia},
	Year = {2022},
	Howpublished = {\url{https://resources.nvidia.com/en-us-}
	\\
	\url{tensor-core}},
	Note = {(Online: accessed 25 January 2023)}
}

@article{Ouyang22,
    Title={Training language models to follow instructions with human feedback},
    Author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
    Journal={arXiv preprint arXiv:2203.02155},
    Year={2022}
}

@article{Park22,
    Title = {nu{Q}mm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models},
    Author = {Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
    Journal = {arXiv preprint arXiv:2206.09557},
    Year = {2022},
}

@article{Peiwen22,
    Title = {Normalized Activation Function: Toward Better Convergence},
    Author = {Peiwen, Yuan and Changsheng, Zhu},
    Journal = {arXiv preprint arXiv:2208.13315},
    Year = {2022},
}

@misc{Pytorch23,
	Title = {AUTOMATIC MIXED PRECISION PACKAGE - TORCH.AMP},
	Author = {{P}y{T}orch},
	Year = {2023},
	Howpublished = {\url{https://pytorch.org/docs/stable/amp.html}},
	Note = {(Online: accessed 25 January 2023)}
}

@article{Rae2021,
    Title = {Scaling language models: Methods, analysis \& insights from training {G}opher},
    Author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
    Journal = {arXiv preprint arXiv:2112.11446},
    Year = {2021},
}

@article{Rajpurkar2016,
    Title = {{SQuAD}: 100,000+ questions for machine comprehension of text},
    Author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
    Journal = {arXiv preprint arXiv:1606.05250},
    Year = {2016}
}

@article{Rajpurkar2018,
    Title = {Know what you don't know: Unanswerable questions for {SQuAD}},
    Author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
    Journal = {arXiv preprint arXiv:1806.03822},
    Year = {2018}
}

@article{Sakr2022,
  Title = {Optimal clipping and magnitude-aware differentiation for improved quantization-aware training},
  Author = {Sakr, Charbel and Dai, Steve and Venkatesan, Rangha and Zimmer, Brian and Dally, William and Khailany, Brucek},
  Journal = {39th International Conference on Machine Learning, ICML 2022},
  Year = {2022},
}

@article{Salimans16,
    Title = {Weight normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
    Author = {Salimans, Tim and Kingma, Durk P},
    Journal = {Advances in Neural Information Processing Systems 29, NeurIPS 2016},
    Year = {2016},
}

@article{Strubell19,
    Title = {Energy and policy considerations for deep learning in NLP},
    Author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
    Journal = {arXiv preprint arXiv:1906.02243},
    Year = {2019}
}

@article{Sun19,
	Title = {Hybrid 8-bit Floating Point ({HFP8}) Training and Inference for Deep Neural Networks},
	Author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
	Journal = {Advances in Neural Information Processing Systems 32, NeurIPS 2019},
	Year = {2019}
}

@misc{Sutton19,
	Title = {The Bitter Lesson},
	Author = {Sutton, Richard S.},
	Year = {2019},
    Howpublished = {\url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}},
    Note = {(Online: accessed 25 January 2023)}
}

@misc{Tesla21,
	Title = {A Guide to Teslaâ€™s Configurable Floating Point Formats \& Arithmetic },
	Author = {Tesla},
	Year = {2021},
	Howpublished = {\url{https://tesla-cdn.thron.com/static/MXMU3S_tesla-dojo}
	\\
	\url{-technology_1WDVZN.pdf}},
	Note = {(Online: accessed 25 January 2023)}
}

@article{Theis17,
    Title = {The end of {M}oore's law: A new beginning for information technology},
    Author = {Theis, Thomas N. and Wong, H.-S. Philip},
    Journal = {Computing in Science \& Engineering},
    Volume={19},
    Number={2},
    Pages={41--50},
    Year={2017},
    Publisher={IEEE}
}

@article{Vaswani17,
    Title = {Attention is All you Need},
    Author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    Journal = {Advances in Neural Information Processing Systems 30, NeurIPS 2017},
    Year = {2017},
}

@article{Wang18,
	Title = {Training Deep Neural Networks with 8-bit Floating Point Numbers},
	Author = {Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash},
	Journal = {arXiv preprint arXiv:1812.08011},
	Year = {2018},
}

@article{Wei22,
    Title={Emergent abilities of large language models},
    Author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
    Journal={arXiv preprint arXiv:2206.07682},
    Year={2022}
}

@article{Weidinger21,
    Title={Ethical and social risks of harm from language models},
    Author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
    Journal={arXiv preprint arXiv:2112.04359},
    Year={2021}
}

@article{Wu16,
    Title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    Author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Lukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
    Journal = {arXiv preprint arXiv:1609.08144},
    Year = {2016}
}

@article{Xiao22,
    Author = {Xiao, Guangxuan and Lin, Ji and Seznec, Micka{\"{e}}l and Demouth, Julien and Han, Song},
    Title = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
	Journal = {arXiv preprint arXiv:2211.10438},
	Year = {2022},
}

@misc{Xla17,
	Title = {{XLA} -- {TensorFlow}, compiled},
	Author = {{XLA and TensorFlow teams}},
	Year = {2017},
	Howpublished = {\url{https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html}},
	Note = {(Online: accessed 26 January 2023)}
}

@article{Xu20,
    Title={Recipes for safety in open-domain chatbots},
    Author={Xu, Jing and Ju, Da and Li, Margaret and Boureau, Y-Lan and Weston, Jason and Dinan, Emily},
    Journal={arXiv preprint arXiv:2010.07079},
    Year={2020}
}
	
@article{Yang20,
    Title = {Feature learning in infinite-width neural networks},
    Author = {Yang, Greg and Hu, Edward J.},
    Journal = {arXiv preprint arXiv:2011.14522},
    Year = {2020},
}

@article{Yang22,
    Title = {Tensor Programs {V}: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
    Author = {Yang, Greg and Hu, Edward J. and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
    Journal = {arXiv preprint arXiv:2203.03466},
    Year = {2022},
}

@article{Yao22,
    Title = {ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
    Author = {Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
    Journal = {arXiv preprint arXiv:2206.01861},
    Year = {2022},
}

@article{You2019,
    Title={Large batch optimization for deep learning: Training {BERT} in 76 minutes},
    Author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
    Journal={arXiv preprint arXiv:1904.00962},
    Year={2019}
}

@article{Zamirai20,
  Title = {Revisiting BFloat16 Training},
  Author = {Zamirai, Pedram and Zhang, Jian and Aberger, Christopher R. and De Sa, Christopher},
  Journal = {arXiv preprint arXiv:2010.06192},
  Year = {2020},
}

@article{Zhang19,
    Title = {Residual Learning Without Normalization via Better Initialization},
    Author = {Hongyi Zhang and Yann N. Dauphin and Tengyu Ma},
    Journal = {7th International Conference on Learning Representations, ICLR 2019},
    Year = {2019},
}

@article{Zilly17,
    Title = {Recurrent Highway Networks},
    Author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\i}k, Jan and Schmidhuber, J{\"u}rgen},
    Journal = {34th International Conference on Machine Learning, ICML 2017},
    Year = {2017},
}
