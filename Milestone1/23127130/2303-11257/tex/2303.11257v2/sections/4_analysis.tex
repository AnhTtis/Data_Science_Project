\label{sec:analysis}
For normally distributed tensors we use the term \textit{scale} to refer to standard deviation. We observe minimal change (relative to the range of our formats) of the mean. Scale therefore characterises the probability of clipping error given a format, as too large or small a scale will lead to values that lie outside of the representable range.

\paragraph{Ideal scaling}
Given we are able to influence the scale of tensors at the start of training, the questions arises---what scale should we aim for? As suggested by Figure~\ref{fig:signal_to_noise}, we argue that unit scale, $\sigma=1$ is a `sweet spot' representing a sensible compromise between several competing factors. We address this question further in Appendix~\ref{app:unit_criterion}.

\paragraph{Is scale predictable?}
The ability to predict the scales of tensors in a deep learning model would give us a powerful tool to address clipping error. This is hard in general, but the problem is simpler at initialisation. Before any training steps, parameters are drawn from known initialisation distributions, so if the input distribution is known, analysis or simulation can derive the scale of each tensor.

A further simplification is to make local distributional assumptions for a single layer in the model and consider the propagation of scale through the model. This permits a methodical analysis: first, characterise the scaling effect of each operation independently; second, propagate scales through the computational graph, forwards and backwards. We provide an example of such analysis in Appendix~\ref{app:scaling_example}.

\paragraph{Scaling at initialisation}
Since the initial distribution of parameters is directly controlled by the model designer, the dominant approach to scaling is to select initial parameter variance to trade off forward and backward pass variance scaling \citep{Glorot10,He15}.

Such schemes were developed to avoid exploding/vanishing gradients in deep multilayer perceptrons. As such, they do not seek to constrain the scale of parameters and parameter gradients. They are also limited to computations where scale factors can be moved into trainable parameters.

\paragraph{Example: BERT \citep{Devlin19}}
BERT's initialisation scheme does not use the rules of \citet{Glorot10}, instead initialising all non-bias parameters from $N(0, (0.02)^2)$. It also adopts a scaling factor from the Transformer \citep{Vaswani17}, which scales the product of activation matrices $Q K^\top$, $Q, K \in \mathbb{R}^{s \times d}$ by $1/\sqrt{d}$.

We instrument the model to record histograms of all tensors at the start and end of training, and plot the results in Figures~\ref{fig:bert_scaling_reg_init} and \ref{fig:bert_scaling_reg_end}. In light of this analysis, we can understand loss scaling as simply enacting a shift of the \textit{gradx} and \textit{gradw} histograms by $\log_2(\textrm{loss scale})$ bits to the right, trading off underflow and overflow globally across gradient tensors.

BERT with loss scaling illustrates the drawbacks of having just three scales: weight initialisation scale, loss scale, and $Q K^\top$ scale. These are not sufficient to centre most tensors' distributions in the representable range.
