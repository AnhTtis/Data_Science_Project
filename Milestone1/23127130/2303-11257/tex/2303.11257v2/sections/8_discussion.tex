\label{sec:discussion}
\paragraph{Compute overhead} Unit scaling relies solely on the addition of scaling operations of the form $\gamma \cdot X$, where $\gamma$ is a fixed scalar and $X$ is a tensor. These scaling factors can be fused into the preceding ops (e.g. via \texttt{torch.jit}, \texttt{torch.compile}  or \texttt{jax.jit}). By doing this we observe that the increase in memory-access cost is negligible. For models with reasonably large hidden sizes, the compute overhead is also minimal. For example, the FLOPs required to train our unit-scaled BERT\textsubscript{LARGE} are only 0.2\% greater than the baseline (explained further in Appendix~\ref{app:overhead}). Basic loss scaling operates on a similar principle, and only introduces a single scaling factor. From this we conclude that both techniques have low overall overhead, assuming a fused implementation.

Automatic loss scaling has an additional feature which increases overhead: its requirement to occasionally discard batches. This assumes that re-scaling is determined by tracking gradient overflows (the standard approach, as used in \citet{Pytorch23}). When overflows occur, batches must not be used to update parameters. The overhead of dropping batches is tolerable for FP16 but may not be for FP8 \citep{Micikevicius22}.

Proposed automatic per-tensor scaling schemes take a different approach, and have potential to add overhead in other areas (how much depends largely on software and hardware characteristics). \citet{Micikevicius22} reject scaling based on gradient overflows, instead opting for heuristics based on properties of the tensors being scaled. Their preferred training heuristic is not specified, but for inference they choose between max, percentile, and minimum MSE methods. These approaches trade-off overhead for accuracy. At one extreme, max is likely easy to fuse but may be distorted by outliers; at the other extreme minimum MSE may be more robust but is challenging to implement efficiently (e.g. \citet{Sakr2022}). Distributed training adds further challenges, potentially requiring the communication of statistics across devices to keep scales synchronised.

It remains to be seen whether effective automatic scaling methods can be implemented efficiently given these complexities. This will likely be an important future research objective. In contrast unit scaling, with fixed precomputed scaling factors, offers a simpler alternative.

\paragraph{Broader impact}
The potential for unit scaling to simplify the use of 8-bit number formats may lead to increased adoption, and in turn facilitate training larger models. At scale, new capabilities emerge \citep{Wei22}, potentially exacerbating known harms \citep{Weidinger21} such as toxicity \citep{Nadeem20}, misinformation \citep{Lin2021}, privacy concerns \citep{Carlini21} and environmental damage \citep{Strubell19}. To mitigate these outcomes, a variety of methods have been proposed, including reinforcement learning from human \citep{Ouyang22} or AI \citep{Bai22} feedback, anti-experts \citep{Liu2021} and baked-in safety models \citep{Xu20}, all of which are applicable to unit-scaled models.

\paragraph{Conclusion}
We have demonstrated that unit scaling addresses the complexities of low-precision training, providing a simpler and more granular solution. This is demonstrated by our training of BERT\textsubscript{LARGE} for the first time without loss scaling, in FP16 and even FP8. The community's transition to FP8 training will see new capabilities emerge as a result of improved efficiency, and this transition can be accelerated by unit scaling.
