\section{\ourDS Dataset}
\label{sec:lasco}
In this section we present \ourDS ({\it \ourDSfull}), a new \coir dataset of open-domain natural images, scaling up existing datasets.
We take advantage of the popular VQA task being well studied, with carefully labeled datasets \cite{justAsk_VQA_2021, balanced_vqa_v2}. 
%on its variations \cite{VQA,balanced_vqa_v2,vilbert,plotqa, levy2022classification, okvqa, DocVQA, X_VLM}. VQA requires answering an open language question about a given image. 
We utilize the {\it VQA2.0}\cite{balanced_vqa_v2} dataset to create \ourDS with minimal human effort. %{\it VQA2.0} was labelled on COCO~\cite{coco} images
{\it VQA2.0} has two important features: 1) A balanced answer set for VQA; 2) Introduced ``complementary'' samples, with counter examples.
%used to ensure that the model is not lingual biased and visual input is attended during answer prediction (called 'Making the V in VQA Matter' \cite{balanced_vqa_v2}). 
A complementary image is defined as an image that is similar to the original image in the VQA, but if asked the same question, will result a different answer.
%The authors addressed a major textual bias in the previous VQA dataset \cite{VQA}, showing that models exploit language biases in order to answer questions. To mitigate this shortcoming, they collected ``complementary'' samples introducing {\it VQA2.0}, namely a balanced version of the VQA dataset with counter examples, as described below.
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\linewidth]{figures/lasco/VQA2_Modifications2.drawio.pdf}
   \caption{Generating transition-text from \emph{VQA2.0} labels to \ourDS. Given two paired triplets $(I, Q, A), (I_c, Q, A_c)$ that share the same question and image visual information, a transition text between $I\rightarrow I_c$ is phrased by $G:=${\it GPT-3}\cite{GPT_3}, based on $(Q, A_c)$.}
   \label{fig:our_vqa2}
\end{figure}
\input{tables/datasets}
\subsection{Data Roaming}
\label{sec:Data Collection}
We generate \coir triplets from {\it VQA2.0} samples and their ``complementary'' counterparts. For each sample, we use the original image $I$ and its complementary counterpart $I_c$ as query and target images, respectively, and we convert the question $Q$ and the answer $A_c$ (for the complementary image) into a transition-text. \Cref{fig:our_vqa2} shows an overview of this process. For brevity, let us denote {\it VQA2.0} by set $\mathcal{D}$.

{\bf Image pairing:} 
We now make use of each triplet $(I, Q, A)\in \mathcal{D}$, and its complementary triplet $(I_c, Q, A_c)\in \mathcal{D}$, that satisfies $I\neq I_c, A\neq A_c$, to create a \coir triplet. By construction, each $I_c$ image was manually selected from the 24 (visually) nearest neighbors of $I$ such that: 1) any premise assumed in $Q$ must hold in $I_c$; 2) it makes sense to ask $Q$ about $I_c$; and 3) the answer to $Q$ about $I_c$ differs from $A$. The actual answer $A_c$ was provided by a different annotator, in a second round. 
%Nearest neighbors were calculated by $L^2$ distance of penultimate VGG~\cite{vgg_net} features.
These properties in construction of $\mathcal{D}$ allow building our new dataset. Next, we present our scheme for constructing the transition-text $S_c$ from $I$ to $I_c$ from the existing $(Q,A_c)$ pair.

{\bf Transition text:} 
The text conversion task is defined by $G:(Q,A_c) \mapsto S_c$. To this end, we make use of a strong language model, \eg, $G:=${\it GPT-3} due to its few-shot capabilities~\cite{GPT_3}, to perform the conversion. To create an example set for the task, we recruit $20$ annotators to manually rephrase $\sim\!300$ randomly sampled $(Q,A_c)$ pairs to valid transition texts $S_c$. We then provide GPT-3 a short description of the task, with three annotated examples of $(Q,A_c,S_c)$, and ask the model to perform the task on a new given pair. We further exploit the symmetry in the transition, $I\rightarrow I_c$ and $I\leftarrow I_c$ to generate more triplets. Finally, we organize the triplets as $(I,S_{c},I_c)$, $(I_c,S,I)$, with $S_{c}$ and $S$ indicating the appropriate transition text. For an extensive list of examples we refer the reader to our suppl. material.
% We construct a transition text from $I$ to $I_c$ by rephrasing $Q$ and $A_c$, since the images $I, I_c$ differ in $A, A_c$ w.r.t the question $Q$. Let us denote a function $G$ for phrasing a valid transition text, given the information in $Q$ and $A_c$, namely: $G(Q, A_c)$. For example, the pair $(Q,A_c)$ (``What color is the building? White'') is rephrased to $G(Q, A_c)=$ ``Change the color of the building to white'' (see \cref{fig:our_vqa2}). To automate this task with minimal manual labeling, we choose a strong language model $G:=${\it GPT-3} due to its few-shot capabilities \cite{GPT_3}. 
% %This language model is based on the idea of in-context learning, namely predicting the answer given only a natural language description of the task (without any gradient updates over the model).
% Following \cite{GPT_3}, we show the model few samples of our task, \ie converting a question and an answer to a transition-text. To this end, we recruit $~20$ annotators to manually rephrase $\sim300$ random $(Q,A_c)$ pairs to valid transition texts. To create variability, for each transition task we create new transition texts, with three random examples and $(Q,A_c)$ from our annotated dataset.

{\bf Quality control:} We further conduct a data curation process and preliminary evaluation for the quality of our generated \coir triplets. To this end, we first remove triplets that share the same query/target image. For text, we apply automatic rule-based scripts to filter out potentially wrong conversions (\eg, too short, unexpected characters such as `\textbackslash{}n', etc.). In our manual review of $1000$ random text conversions we judged $91.7\%$ of them to produce well-phrased and reasonable transition texts. Finally, we conduct a short user study to compare the quality of our annotations to fully human-labelled annotations. We sample $\sim300$ random triplets $(Q_i, Q_t, T_i)$, of query-image, query-text, and target image (respectively). Triplets were shown to a total of 30 users, who were asked whether $Q_t$ ``adequately describes the transition/modification'', from $Q_i$ to $T_i$, or not. This experiment was conducted on three different CoIR datasets. The results show $82.02\%$ positive rate for \ourDS samples, compared to $81.15\%$ for \fashioniq, and $82.65\%$ for \cirr.

\Cref{tab:datasets} compares statistics of \ourDS to previous \coir datasets. \ourDS contains over 389K queries, $\times$10 larger than previous datasets, with an image set containing 121.5K different images, compared to previous 19K--41K. The size of the test image corpus, determining the target search space, is almost 40K, compared to 2.3K in \cirr and 15.4K in \fashioniq. In terms of natural language, \ourDS is richer with 13.5K different language tokens, compared to 4.4K in \fashioniq and 6.8K in \cirr. Moreover, \ourDS and \emph{VQA2.0} are both labeled on COCO's image set; thus, image captions are publicly available for each of \ourDS's images. Utilizing captions as an additional cue (see \Cref{sec:evaluation}) allows creating a rich dataset for training \coir to reach high performance in both Text-to-Image and \coir tasks.


