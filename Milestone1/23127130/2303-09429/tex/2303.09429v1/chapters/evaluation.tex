\section{Evaluation}
\label{sec:evaluation}
We evaluate our model on \fashioniq, \cirr, and {\it \ourDS} benchmarks, one domain-specific and the others more general and broad, based on natural images. We start our evaluation with the impact of our method on the retrieval performance, measured by Recall@K. Next, we examine the effect of using our new \ourDS dataset for training and pre-training (train/val.~split of 92\% \& 8\%). We also present results with pre-training with a mixture of COCO captions, that are very descriptive to better handle samples where the transition text is highly detailed, making the the query image often redundant (\ie text-to-Image retrieval). To this end,  we conduct also an experiment where we train \our on \ourDS, replacing 50\% of transition-texts $Q_t$, with captions, corresponding to the target image. Namely, we change the train distribution to combine both \coir and text-to-image samples, as discussed in \cref{sec:Data Collection}. We also show our model's capabilities when one of the modalities is missing (either image or text).
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\linewidth]{figures/retreivals_cirr_val_ex.drawio.pdf}
   \caption{\our top-4 retrievals (from left to right) for queries in \cirr (top two) and \fashioniq (bottom two). The query (image and text) is shown in the left column. The single ground truth target is framed in green. Arguably, additional images could be marked acceptable (referred as false-negatives).}
   \label{fig:fiq_cirr_ex}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}
\fashioniq \cite{fashioniq} contains crowdsourced descriptions of differences between images of fashion products. Images are collected from the web and divided to three categories of {\it Shirts}, {\it Dresses} and {\it Tops\&Tees}. The query and target images were automatically paired based on title similarities (crawled from the web). This dataset consists of 30K queries (see \Cref{tab:datasets}), annotated on 40.5K different fashion images. There are 4.4K different tokens in the transition-texts (according to {\it BERT} tokenizer). The validation corpus contains 15.4K different images (from which target should be retrieved). The bottom two rows of \Cref{fig:fiq_cirr_ex} show \fashioniq retrieval examples.

\cirr contains open domain natural images, taken from NLVR2\cite{nlvr2}. It contains a total of 36.5K queries annotated on 19K different images, with 6.8K unique tokens in the transition texts. Examples may be seen in the top two rows of \cref{fig:fiq_cirr_ex}. Its validation corpus is relatively small, with a size of 2.3K. The authors further suggest two benchmarks, one \textit{general}, with the target search space as the entire validation corpus, and a \textit{subset}, where the search space is a subgroup of six images similar to the query image (based on pre-trained ResNet15 feature distance), demonstrating a fine-grained retrieval task. 
%------ Original version
% \fashioniq \cite{fashioniq} contains crowdsourced descriptions of differences between images of fashion products. Images are collected from the web and divided to three categories of {\it Shirts}, {\it Dresses} and {\it Tops\&Tees}. The query and target images were paired based on title similarities (crawled from the web). This dataset consists of 30K queries (\cf \Cref{tab:datasets}), annotated on 40.5K different fashion images. There are 4.4K different tokens in the transition texts (according to {\it BERT} tokenizer). The bottom two rows of \Cref{fig:fiq_cirr_ex} show \fashioniq retrieval examples.

% \cirr contains open domain natural images, taken from NLVR2\cite{nlvr2}. Images were divided to small subgroups of six similar images, based on pre-trained ResNet152\cite{resnet} feature distance. Annotators provided with such subgroup $A=\{I_1,...,I_6\}$ and $I_i,I_j \in A$, and were asked to write a ``transition text'' (originally called ``modification text'') from $I_i$ to $I_j$ that is valid solely to $I_j\in A$. The authors further suggest two benchmarks, one \textit{general}, where the target image is retrieved from the entire validation set, and a \textit{subset} benchmark, where the target is searched among its subgroup only, representing a fine-grained retrieval task. 
% Compared with existing datasets, \cirr places more emphasis on distinguishing between visually similar images, which provides a greater challenge, as well as a chance for studying fine-grained vision-and-language (V\&L) reasoning in \coir with natural images. The authors claim that their benchmark reduces the number of false-negative samples, being fully labeled \cite{cirr}. \cirr contains a total of 36.5K queries annotated on 19K different images, with 6.8K unique tokens in the transition texts.
% \dnote{the above is a very detailed description of \cirr. Do we really need to go over all of the above details? In fact, this whole subsection of evaluation seems just describes previous datasets and doesn't present any results.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}
\label{sec:results}
We start by showing the performance of our new CASE architecture on {\it FashionIQ}, in \Cref{tab:fiq_val}. The results are broken down to different clothing categories and Recall@K values. As baselines, we also show the results of our model when trained only on query-image (Image-only) or query-text (Text-only).
%\rnote{\sout{I believe the results for Text-only are based on a different trained model.}}\mnote{It's ours}
\our achieves SoTA results, surpassing the previous top performing method (LF-CLIP \cite{cclip}) by a large margin (13.4\% and 11.6\% absolute points at Recall@10,50 respectively). %Comparing to previous SoTA, \our improves R@10 by $13.4\%$ and R@50 by $11.65\%$.
The poor results for \our (Image-only) baseline, show that visual information is not sufficient for {\it FashionIQ}, as often the transition text asks for a certain change in the image (see \Cref{sec:modality_redundancy}). However, the \our (Text-only) baseline results are close to the previous method of LF-CLIP~\cite{cclip}, indicating the high capability of our shift encoder. Yet, the exploitation of multi-modal compositionality is evident, when both image and transition text are used as input to our model (see CASE results). 
%The LF-B baseline shows poor results, implying the \drep{efficiency}{effectiveness} of \dtext{using} cross-attention \sout{use}. \dtext{It also performs much worse than LF-C, which only differs by its encoders (CLIP instead of BLIP)}. \dnote{but why the BLIP version is so much worse?}
\input{tables/fashioniq}

\Cref{tab:cirr_test} shows results on \cirr. At the top we present the results from previous methods. The right columns $R_{\textit{subset}}@K$, correspond to the fine-grained retrieval task (introduced originally in \cite{cirr}). Here, we show five different variants of our model. As in {\it FashionIQ}, the poor results for \our (Image-Only) imply that the query-image alone is not sufficient for retrieval also on \cirr.
Interestingly, \our(Text-only) version is top performing in the {\it subset} benchmark. We believe this is caused by the existing image similarity in the subset, making the query image redundant for the task (see visual examples in suppl.~material). 
%We show a visual example in \cref{fig:cirr_problematic_ex}, with the corresponding ground truth annotation. ***($R_{\textit{subset}}@K$), comparing to \our with rank \#5. However, \our ranks the ground truth image at rank \#16 among the entire corpus, comparing to \our(Only-Text) with rank \#20. In addition, results of \our respond better to the query than \our(Only-Text), preserving the same dog breed/color (see suppl. material).
\input{tables/CIRR}
Next, we observe the effectiveness of \our, which consistently outperforms previous methods, for all K values, on both \textit{general} and \textit{subset} benchmarks with a significant gap, \eg, by absolute 14.4\% at Recall@1 (33.59\% to 48\%). This is an evidence of a stronger image-text composition layers suggested in our model, containing the early fusion cross-attention module. Some qualitative examples are shown in \cref{fig:fiq_cirr_ex}, showing a high level of perception in our model. We further show visual and textual explainability maps in the supplementary material.

Next we examine the impact of {\it pre-training} \our with our new dataset, \ourDS. The results are shown under \our Pre-\ourDS, gaining $\sim$0.5-1\% improvement. We found that this relatively small improvement is due to the modality redundancy in the \cirr dataset. To justify this assumption we show that mixing the train set with samples from COCO captions further boosts \our performance, a fact that we regarded to improvement of the Text-to-Image (T2I) search capability (see \Cref{sec:modality_redundancy}). In order to evaluate the generalization that can be gained from \ourDS, we train CASE on \ourDS and our mixed \ourDS + COCO (\ourDS.Ca) dataset, and test in {\it zero-shot} on \cirr test set. As a reference point, we show the results for CASE initialized with BLIP weights (\our Init.), without any further training. Not surprisingly, the results for \our Init.~are poor, as the BLIP model trained on text-image joint embeddings cannot handle the triplet nature of \coir, where the transition text and query image have a different meanings. 
However, we get a strong boost nearly reaching prior SoTA, when fine-tuning on the suggested \ourDS dataset.

We are further able to outperform it, even in {\it zero-shot} test, when trained on our combined external dataset (\ourDS.Ca). 
Finally, in \Cref{tab:lasco_val}, we benchmark on \ourDS. To this end, we apply two \our variants (Text-Only, Image-Only) that result in poor performance, implying the necessity of both modalities in this dataset. We further test LF-CLIP \cite{cclip} trained on \ourDS, and observe its significant drop in performance (compared to CIRR), implying that \ourDS dataset introduces a higher true \coir challenge. Finally, \our performs best also here, raising \eg, Recall@1 from 4.01\% (by prior LF-CLIP\cite{cclip}) to 7.08\%, and Recall@50 from 32.08\% to 50.25\%. 

{\bf Impact of early fusion:} We examine the impact of BLIP's pre-trained weights by comparing it with CLIP, under a late fusion architecture (due to lack of cross-attention layers in CLIP). To this end, we choose previous SoTA (Baldrati \etal \mbox{\cite{cclip}}, referred to as LF-CLIP) that was initialized with CLIP weights. Using the same architecture but changing the CLIP components with BLIP, with the corresponding pretrained weights, we obtain another model, LF-BLIP. It separately encodes each modality to a global feature vector (but with BLIP encoders), then fuses both via a learnable MLP, similar to \cite{cclip}. Comparing to LF-CLIP, the performance of LF-BLIP is slightly higher on \ourDS (\Cref{tab:lasco_val}), but is significantly lower on FashionIQ and CIRR (\Cref{tab:cirr_test,tab:fiq_val}), implying the advantage of CLIP's pre-trained features. However, \our performs significantly better. This experiment implies that the impact of early fusion is more significant than simply replacing CLIP with BLIP.

% , and its BLIP version LF-B. To this end, we train LF-C and LF-B on \ourDS. We observe a significant drop in performance for all methods, \dnote{compared to CIRR?}, implying that \ourDS sets a higher \coir challenge. Although \dtext{the use of BLIP in} LF-B boosts the results, \our performs best, by a large margin. Note that the architecture of CLIP's encoders is not capable of supporting cross-attention, thus its weights are not usable for this purpose. \dnote{last part about the weights isn't clear to me, for what purpose? for cross-attention? rephrase...}
\input{tables/lasco}

%The performance of our "Text-only'' baseline, where \our receives query-texts only, are higher than all previous methods. That supports our claims in \cref{sec:intro} regarding \cirr's distribution, as a mix of \coir and T2I samples. However, we show increase in performance by a large margin, when training \our with image information, implying the usage of \our in the query-image. We further examine the influence of pre-trained weights on \our results. First, we show our model capability for VLP understanding, by showing basic results with \our initialized with pretrained BLIP components (Init.), without further training. 
%We than pre-train \our on \ourDS for 8 epochs, and examine results of both zero-shot and training on \cirr. We observe that our model is able to generalize from \ourDS to \cirr by its competitive zero-shot results (Pre. \ourDS) w.r.t previous SoTA. We further finetune weights on \cirr to receive better performance, showing \ourDS effectiveness.
%Since \ourDS and VQA2 \cite{balanced_vqa_v2} share the same image set as COCO \cite{coco}, captions are public available for each image in the datasets. In an attempt to mimic \cirr's nature, where some target images are solely described by the query-text, we pre-train \our on \ourDS by replacing $50\%$ of query texts by the target's captions. In this manner, the model (Pre. \ourDS w/cap.) is trained for both kind of queries we discussed in \cref{sec:intro}: \coir and T2I. Our model zero-shot results are higher than all previous methods that was trained on \cirr, without "seeing`` a single sample from the \cirr dataset. Further finetuning on \cirr result in SoTA performance.

% \subsection{Image Independence}
% There are two major problems in \coir datasets that are different, yet related to each other: 1) False-negatives 2) Image-independence. The first is about the possibility of multiple proper target images existence, for a single query (while only one was labeled as positive). In practice, training in a contrastive manner with a limited batch size (\eg 1024) may mitigate this shortcoming with the assumption that there are "not many'' false-negative images in the image corpus, so the probability of false-negative to be selected to the same batch are low. However, it is not solving the problem for testing.
% \rnote{Should we describe this as a new approach/tool and as a novelty or just an idea for analysis. I have some doubts} A major concern on \coir datasets labeling is about image-independent queries. Some queries may contain a descriptive text that makes the query-image information redundant. For example, a caption of the target-image as a query-text that ignores the query-image completely, may include enough information for the target-image retrieval. We observe a similar phenomenon in \cirr \cite{cirr}. Our conjecture is that annotators described target-images without paying enough attention to the query-images. In order to measure or demonstrate how many samples are image-independent, we propose a new metric {\it "CT2I''} ({\it CLIP text-to-image}). The key idea is to measure the performance of text-to-image baseline that tries to retrieve images based on text-only queries. We choose CLIP as off-the-shelf method, to preform in a zero-shot manner, without further training. \rnote{You can't show results before the Evaluation and definition of the measure. So I am moving the results and the Table to Analysis Section.}
 
\section{Modality Redundancy}
\label{sec:modality_redundancy}

In this section, we first propose a simple analysis of existing \coir datasets to examine the degree to which their queries require each of the two modalities for successful retrieval. Next, a similar analysis is proposed for assessing the bias of a method towards modality redundancies.

Ideally, both modalities of a composed query should be necessary for the retrieval task. For example, a transition-text such ``Change the color to be more cream colored'' in the top row of \Cref{fig:fiq_cirr_ex}, will only succeed in finding the proper target in conjuction with the query image, since the type of object cannot be inferred from the text alone.
However, in practice, one of the modalities may be redundant, with the degree of redundancy depending on the information conveyed by the other modality.
In the extreme case, the query-image might be completely redundant, reducing the task to Text-to-Image retrieval; alternatively, the query-text might be redundant, with the task becoming Image-to-Image retrieval.
To quantitatively assess the degree of redundancy in \coir datasets, we measure the Recall@K performance of naive Text-to-Image and Image-to-Image retrieval using the embeddings produced by an independent off-the-shelf CLIP model \cite{clip}.
%if one of the modalities is redundant (\eg, query-image), the query might be associated to another task (\eg Text-to-Image). However, such redundancies are not necessarily a binary characteristic, and depend on the relative information contributed by each modality. In order to examine these, we use an independent off-the-shelf CLIP \cite{clip} model for analyzing \coir queries. We evaluate its retrieval performance on each dataset, in two different tasks: Text-to-Image (T2I) and Image-to-Image, by encoding the proper modality for the shared embeddings space search.
% In composed image retrieval, the expectation is that the composed queries should require both modalities (image and text) in order to properly indicate the desired target.
%for a dataset (triplets) that require both image and text queries to reach the designated target.
% Datasets or samples where only one modality suffices for retrieving the target introduce a modality redundancy, and might drive the model to solve a different task of content-based-image retrieval (CBIR) or text-to-image (T2I).
%In a a general \coir dataset, one would expect both modalities to be necessary in order to retrieve the desired target, and that omitting one of them would result in a wrong retrieval.
%For example, a detailed description of the target contained in the query-text, may make the query-image redundant.
% We can categorize samples in a certain dataset as (1) Text-to-image, (2) Image-to-image, or (3) Composed, where either single modality or composition of the two is required for target retrieval. One would associate a higher redundancy level for a sample/dataset if it tends to belong to category (1) or (2) and lower redundancy level (equivalent to higher compositionality) if it tends to belong to category (3). In order to examine the level of modality redundancy in certain \coir datasets, we use an independent off-the-shelf CLIP \cite{clip} model and evaluate the retrieval performance in each dataset using Text-to-Image (T2I) and Image-to-Image by CLIP. Here, the search is conducted per-modality according to CLIP embeddings.
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.85\linewidth]{figures/clip_on_datasets.pdf}
   \caption{Modality-redundancy level in several datasets (lower is better). Recall@K values for {\it Text-to-Image} and {\it Image-to-Image} retrieval using off-the-shelf CLIP \cite{clip}. Lower values indicate higher requirement for image and text compositionality. The results for  COCO\cite{coco} are shown as a reference for a purely text-to-image (single modality) retrieval task, presenting an upper bound.
	}
   \label{fig:tti_clip}
\end{figure}
To create a continuous measure we compute the Recall@K for varying $K$ values. These measurements computed on several datasets are plotted in \Cref{fig:tti_clip}.
%\Cref{fig:tti_clip} indicates the level modality redundancy in several datasets.
A lower curve indicates that the corresponding dataset is more challenging for a uni-modal query. Note that the \ourDS and \fashioniq curves are much lower than \cirr, implying that the samples in \cirr contain more modality-redundant queries. 
For reference, we also plot the performance of the CLIP-based Text-to-Image retriever using COCO captions as query text (a commonly used benchmark for Text-to-Image retrieval \cite{BLIP, Align_before_use, li2020oscar, clip}). While COCO may be viewed as an ``upper bound'' for this task, note that the \cirr curve is quite close to it.
%Note that COCO serves as an ``upper bound'' since its texts are the target-captions, which assumed to be sufficient for retrieval.

Next, we employ a similar analysis for studying the degree to which \coir methods (trained on a certain dataset) are affected by the presence of modality redundancies in the dataset.
Starting from the full \cirr validation set, denoted as $V$, we generate a sequence of progressively ``purified" subsets $V_n \subset V$, with each subset containing fewer modality redundancies. Specifically, subset $V_n$ is generated by removing from $V$ all of the queries for which the naive CLIP-based Text-to-Image retriever, retrieves the correct target image among it's top-$n$ results. In \Cref{fig:k-filter} we plot the average of Recall$@\{1,5,10,50\}$ as a function of $n$.


%We further take advantage of this approach for studying \ourDS influence on our model, by examining its \coir abilities, by the following experiment. We evaluate \our versions on different subsets of \cirr's validation set, denoted as $V$. In order to evaluate the \coir abilities (separated from T2I abilities), we ``purify'' $V$ by filtering out T2I queries, according to CLIP performance (T2I) on \cirr. Hence, all queries in $V$ which CLIP was successfully retrieved in top-$n$ were removed from $V$, defining $V_n\subseteq V$. Intuitively speaking, the higher $n$, the more difficult it gets to CLIP to retrieve the query's target, which based on transition-text only. We measure the performance on $V_n$ with an average of Recall$@\{1,5,10,50\}$ and present the results in \cref{fig:k-filter}.


% as the level of dependence on  the query-image. \rnote{Higher Recall@K values indicate that targets are likely to be retrieved given only the transition-text, without the need for the query-image.}
% \dnote{Fig. 4 is impossible to understand at this point, since the x-axis is not explained at all!}
% Lower result are therefore show higher bi-modality dependency requirement in the retrieval, at the specific dataset. Note that \ourDS and \fashioniq show a lower measures than \cirr, implying that the \cirr's distribution is tended to be a mix of Text-to-Image and composed samples. We further show results on COCO captions (a commonly used benchmark for Text-to-Image retrieval \cite{BLIP, Align_before_use, li2020oscar, clip}) as reference. The high modality redundancy values for COCO are expected as full textual description (captions), associated with the images, are sufficient for successful retrieval. 
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.85\linewidth]{figures/k-filtered.pdf}
       \caption{Average retrieval performance on subsets of \cirr determined by image-redundancy levels. Higher values of Average Recall, imply {\it higher level of compositionality} between text and image (lower modality redundancy). See \Cref{sec:modality_redundancy}.} 
   \label{fig:k-filter}
\end{figure}

Note that the performance of the CLIP-based retriever (blue line) vanishes at $V_{50}$, since by construction, $V_{50}$ contains only queries for which CLIP failed to retrieve the target within its top 50 results.
A similar trend is observed with \our-(Text-Only) (green line), showing performance degradation with increased $n$, as it also relies solely on query-text. While \our that was trained on \cirr (orange line) shows better performance, it still suffers some degradation as $n$ grows, implying that some bias towards modality redundancies might still exist. However, when \our is trained or pre-trained on \ourDS dataset (pink and brown lines, respectively), it achieves the best performance, which is roughly constant regardless of $n$.
Thus, \ourDS appears to be effective at removing bias towards redundancies, and \our pre-trained on it is better suited for datasets with high compositionality.


%with approximately constant values across all $n$'s. This result shows the benefit from (pre-) training on \ourDS, that most likely require compositionality, and therefore are less biased towards transition-text. We believe that our suggested method can be used in future for evaluation of biases in both \coir models and datasets.


% \mnotec{For each $n\in\{0,...,100\}$,
% we construct $V_n\subseteq V$ according to CLIP Text-to-Image retrieval  performance (query-text to target-image). At each $n$ we create a new subset $V_n:=V\setminus C_n$, by removing the query samples where CLIP ranked the ground truth target image, in top-n candidates, denoted as $C_n$. Note that $\forall n\in \{0,..99\}~C_{n}\subseteq C_{n+1} \Rightarrow V_{n+1}\subseteq V_{n}$. Namely, $V_n$ includes all the queries that CLIP failed to retrieve their ground truth target within top-n candidates. Higher $n$, therefore indicate lower image redundancy, \ie the transition text is not sufficient and the target is more dependent on both of the query modalities.} 

%Denote $C_k\subseteq V$ as the queries on which CLIP rank the ground truth target image, in top-K candidates. We define $V_k:=V\setminus C_k$. 

%%%%%%%%
%filter out samples by excluding T2I samples while keeping \coir samples. We detect T2I samples by using off-the-shelf CLIP baseline. For each $k\in[100]$, we exclude samples that made it to the baseline's top-k rank. Namely, we consider $100$ different difficulty levels of retrieving samples based on text only, assuming T2I samples will get small values of $k$'s, while \coir samples will get high values since they are depend, by definition, on their query-image as well. \Cref{tab:tti_clip} demonstrates datasets difficulties for text-to-image (T2I) retrieval, by showing results of CLIP off-the-shelf baseline, for T2I task, without further finetuning. Note that COCO \cite{coco} is a commonly used benchmark for T2I task \cite{BLIP, Align_before_use, li2020oscar, clip}. \Cref{fig:k-filter} shows comparison between different models, on various k-filtered subsets of \cirr validation set.

%We observe that \cirr's domain includes but is not limited to samples of the \coir tasks. In fact, this dataset contains a mix of T2I and CIOR samples. \ie part of the samples could be solved by solely text queries (T2I) while the other require the query-image information (\coir). We assume such phenomenon may be caused by annotation mistakes, where annotators describe the target image while ignoring the query image. In order to mitigate this shortcoming, and to show its existence, we filter out samples by excluding T2I samples while keeping \coir samples. We detect T2I samples by using off-the-shelf CLIP baseline. For each $k\in[100]$, we exclude samples that made it to the baseline's top-k rank. Namely, we consider $100$ different difficulty levels of retrieving samples based on text only, assuming T2I samples will get small values of $k$'s, while \coir samples will get high values since they are depend, by definition, on their query-image as well. \Cref{tab:tti_clip} demonstrates datasets difficulties for text-to-image (T2I) retrieval, by showing results of CLIP off-the-shelf baseline, for T2I task, without further finetuning. Note that COCO \cite{coco} is a commonly used benchmark for T2I task \cite{BLIP, Align_before_use, li2020oscar, clip}. \Cref{fig:k-filter} shows comparison between different models, on various k-filtered subsets of \cirr validation set.


% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/fiq/fiq_test_ex_1.drawio.pdf}
%   \caption{Test examples from \fashioniq dataset. The query (image and text) is most left. \our top 5 ranking is presented. Ground truth answer is marked in green (rank 1).}
%   \label{fig:fiq_ex_1}
% \end{figure}

% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/cirr/cirr_val_ex_1.drawio.pdf}
%   \caption{Test examples from \cirr dataset. The query (image and text) is most left. \our top 4 ranking is presented. Ground truth answer is marked in green.}
%   \label{fig:cirr_ex_1}
% \end{figure}
