\section{Ablation Study}
\label{sec:ablations}
In this section we ablate various key-components of our approach to examine their impact on performance. 
\Cref{tab:fiq_ablations} reports ablation results on the frequently used \fashioniq dataset. We observed similar trends also on CIRR (see suppl. material). 
We train \our without the reverse queries objective described in \Cref{sec:method_reverse_queries} (No-RQ). We observe that reverse queries improve performance by $0.5$--$1.2$\% absolute points ($\sim\!\!5$\% relative performance boost at R@1). Using surrogate Recall@K loss instead of common contrastive loss, further improves results by roughly $0.5\%$ absolute points. Finally, we examine the influence of fine-tuning our ViT parameters. We observe improvement at higher K values (R@10, R@50) that trades-off with lower R@K metrics (R@1, R@5). However, this experiment is dataset-dependent.
%: training on \fashioniq with 25.1K images (see \cref{tab:datasets}) result a R@K trade-off between different $K$ (as described above).
We observed that when training on \ourDS, with 81.6K images, and testing on the \ourDS test set, all metrics were improved by absolute $0.4-2\%$.
% ----- CVPR ver ------
% In this section we ablate various key-components of our approach to examine their impact on performance. \Cref{tab:fiq_ablations} reports the ablation results on the frequently used \fashioniq dataset.
% First, we train \our without the reverse queries objective described in \cref{sec:method_reverse_queries} (No-RQ). We observe that reverse queries improve performance by $0.5$--$1.2$\% absolute points ($\sim\!\!5$\% relative performance boost at R@1). Using surrogate Recall@K loss instead of common contrastive loss, further improves results by roughly $0.5\%$ absolute points. Finally, we examine the influence of fine-tuning our ViT parameters. We observe improvement at higher K values (R@10, R@50) that trades-off with lower R@K metrics (R@1, R@5). However, this experiment is dataset-dependent.
% %: training on \fashioniq with 25.1K images (see \cref{tab:datasets}) result a R@K trade-off between different $K$ (as described above).
% We observed that when training on \ourDS, with 81.6K images, and testing on the \ourDS test set, all metrics were improved by absolute $0.4-2\%$.

% Next, we examine the impact of our early fusion approach by building an alternative baseline with late fusion also based on BLIP (dubbed LF-B) and show the results in \Cref{tab:fiq_ablations}. Following the LF-C model presented by Baldrati \etal \mbox{\cite{cclip}}, LF-B separately encodes each modality to a global feature vector (but with BLIP encoders), then fuses both via a learnable MLP.
% The LF-B baseline shows poor results, implying the effectiveness of using our early fusion approach with cross-attention. 
% -------------------
\input{tables/fiq_ablations}
% \input{tables/cirr_ablations}
% \input{tables/TTI_clip}
% ======================