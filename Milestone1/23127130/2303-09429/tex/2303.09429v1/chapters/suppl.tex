In this supplementary material we start in \cref{sec:case_fusion} with an elaboration of the cross-attention module in our approach called CASE (Cross-Attention driven Shift Encoder). Then we describe our Text and Image only variants, \ie, CASE (Image-only) and CASE(Text-only), shown in the Evaluation section (Sec.~5) in the paper. \Cref{sec:retrieval_exaples} includes additional retrieval examples from our test bed namely {\it FashionIQ}~\cite{fashioniq}, \cirr~\cite{cirr}, as well as our newly suggested dataset LaSCo. 
Next, in \cref{sec:lasco_creation} we show more examples on creation of our transition texts for the triples and our user-study quality assessment. \cref{sec:modality_redundancy_supp} points out more examples on the {\it Modality Redundancy}. Additional ablation analysis of the suggested {\it CASE} architecture are shown in \cref{sec:ablation_study} We conclude the supplementary material in \cref{sec:explainability}, by an explainability analysis showing visualizations over text and image.   
\section{\our Modality Fusion}
\label{sec:case_fusion}
In this section we elaborate on the cross-attention module introduced in Sec.~3 of the paper and \our single modality baselines.

Let us denote $attn_d$ as the original self-attention layer \cite{vaswani2017attention}, applied on vectors in dimension $d\in \mathbb{N}$. Denote the Query, Key and Value of the text modality (inserted into our {\it Shift Encoder} module), obtained from a self-attention layer at certain block, as $Q_t, K_t, V_t \in \mathbb{R}^{n_t\times d}$ where $n_t$ denotes the number of the text tokens and $d$ the representation dimension. We denote by $Q_v, K_v, V_v \in \mathbb{R}^{n_v\times d}$ the Query, Key and Value obtained from the Visual Transformer (ViT) output, with $n_v$ as the number of visual tokens. Bi-modal fusion is then obtained by inter-changing the self-attention matrices between the modalities, as the following:
\begin{eqnarray}
    S = attn_d(Q_t, K_v, V_v) := \mathit{softmax}(\frac{Q_t {K_v}^T}{\sqrt{d}}) V_v \in \mathbb{R}^{n_t\times d}
\end{eqnarray}
Note that $Q_t, Q_v$ are exchanged, to allow information flow or fusion between the modalities. The output $S$ is then fed to a Feed-Forward layer (see Fig.~1 in the paper). 

{\bf Single Modality CASE:} In order to evaluate the impact of each modality separately on CASE, we present in the paper two baselines trained on a single modality: Text-Only and Image-Only. For the first, we set the input image to a tensor of zeros, resulting in a fixed ``image'' input. For the latter, we mask the text by inserting a constant token of [CLS], excluding any other textual information.

{\bf Late Fusion (LF) architecture:}
The paper mentions and describes the LF baselines, referred to as LF-CLIP or LF-BLIP. \Cref{fig:lf_arc} shows an overview of the LF architecture. The weights of the image/text encoders are initialized alternatively with pre-trained weights of CLIP \cite{clip} or BLIP \cite{BLIP}, for LF-CLIP or LF-BLIP, respectively.

\begin{figure*}[t]
	\centering
	%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.9\linewidth]{figures/LF.pdf}
	\caption{An overview of Late Fusion (LF) architecture. First, each modality is separately encoded with its proper expert (encoder) to a single global feature vector. Next, these two global features are concatenated and being fed to a learnable MLP, with a final output of a single vector. When using CLIP components (with associated pre-trained weights) this architecture introduces the last CoIR SoTA method of \cite{cclip}.}
	\label{fig:lf_arc}
\end{figure*}

\section{Retrieval Examples}
\label{sec:retrieval_exaples}
Here we show retrieval examples of \our, for different validation queries on several datasets.
Note that in evaluation, every query is labeled with only one target image, ignoring answers that might be argued as acceptable (false-negatives). Inevitably, this phenomenon appears in larger image corpora.


{\bf CIRR:} \Cref{fig:cirr_retrievals} shows the performance of \our on \cirr examples. There are arguably more than a single acceptable result in several cases. Sometimes, the reason behind tagging the specific target image is unclear due to existence of several similar images. This can be an outcome of the CIRR dataset creation protocol, where the annotator is naturally not exposed to the whole dataset, a fact that further implies the challenge associated with creating a ``clean" CoIR large corpus. This leads to an increase in the count of ``false-negatives" (due declination of the labeled target in the rank). See for instance, rows 6, 8, and 9. Another shortcoming observable in these examples (\Cref{fig:cirr_retrievals}) is the {\it Modality-Redundancy} (see \Cref{sec:modality_redundancy_supp}).
\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth,height=0.95\textheight]{figures/suppl/CIRR_retrievals.drawio.pdf}
   \caption{\our retrievals on \cirr validation set. For convenience, images were resized to the same shape. Labeled target image (ground truth) is framed in green. Rows 1-6 demonstrate {\it Modality Redundancy} as they were also successfully retrieved based only on the given transition text (without using the query image).}
   \label{fig:cirr_retrievals}
\end{figure*}

{\bf FashionIQ:} Several retrieval examples are shown in \Cref{fig:fiq_retrievals}. In numerous instances, the top 5-10 retrievals are very similar and can be arguably considered as legitimate answers.

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth,height=0.91\textheight]{figures/suppl/FIQ_retrievals.drawio.pdf}
   \caption{\our retrievals on \fashioniq validation set. For convenience, images were resized to the same shape. Labeled target image (ground truth) is framed in green. The example in the last row, indicates a case where the corpus contains many images similar to the target and can be acceptable retrievals. The true target was ranked 529 here.}
   \label{fig:fiq_retrievals}
\end{figure*}

{\bf LaSCo:} \Cref{fig:lasco_retrievals_1,fig:lasco_retrievals_2} show several retrieval examples on \ourDS validation set. The results show that our \our model can handle broad concepts from an open domain, having a rich vocabulary. An interesting example is further shown in the last row in \Cref{fig:lasco_retrievals_1}, implying that the model attempts to take into consideration text that appears in an image, 
a consequence of learning from the rich \ourDS dataset. Note that \ourDS validation set includes $39.8$K images, a few order of magnitudes larger than previous datasets, which causes it to inevitably contain false-negatives (See \Cref{fig:lasco_retrievals_2}).

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth]{figures/suppl/LASCO_retrievals_1.drawio.pdf}
   \caption{\our retrievals on \ourDS validation set. For convenience, images were resized to the same shape. Labeled target image (ground truth) is framed in green.}
   \label{fig:lasco_retrievals_1}
\end{figure*}
\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth]{figures/suppl/LASCO_retrievals_2.drawio.pdf}
   \caption{\our retrievals on \ourDS validation set. Examples where the target was not ranked at top-5. The right column shows the ground-truth target, with the associated CASE rank underneath. Note however, although the target was missed, the top-5 images are often acceptable answers. For convenience, images were resized to the same shape. Labeled target image (ground truth) is framed in green.}
   \label{fig:lasco_retrievals_2}
\end{figure*}

\section{LaSCo Dataset Creation}
\label{sec:lasco_creation}
In this section we show more examples of \ourDS samples.
As described in Sec.~4 in the main paper, query/target images are paired based on samples and their ``complementary'' images in VQA2.0. To create the transition-texts we exploit the GPT-3 few-shot \cite{GPT_3} inference capabilities, in the following way.

{\bf Few-shot Prompt:} Given the a triplet and its complementary, $(I,Q,A),(I_c,Q,A_c)\in \mathcal{D}$ where $\mathcal{D}:=$VQA2.0, we generate transition text from $I$ to $I_c$ based on $(Q, A_c)$. To use the few-shot ability of GPT-3, we provide it with a short {\color{Green}{description of the task}}, three manually {\color{Orange}{annotated examples}} and {\color{blue}a pair {$(Q, A_c)$} to rephrase}. An example for such input is presented below (used to generate the second example in \Cref{fig:vqa_to_lasco_1}):

{\it { \color{Green}{Rephrase the following texts:}}\textbackslash{}n
{\color{orange}{``Are there any humans visible in the photo? yes'' = ``Add some people to the photo''}}\textbackslash{}n
{\color{orange}{``Is there a square on the doors? yes'' = ``Find a door with a square on top of it''}}\textbackslash{}n
{\color{orange}{``What color is the man's tie? blue'' = ``Change the color of the tie of the man to be blue''}}\textbackslash{}n
{\color{blue}{``Is the sky cloudy? yes'' =}}}\\
This input results in the transition-text ``Make the sky cloudy''.

Several examples with the associated images, determining \ourDS triplets are shown in  \Cref{fig:vqa_to_lasco_1,fig:vqa_to_lasco_2,fig:vqa_to_lasco_3}. Using the symmetry we show transition text in both directions (See Sec.~6 in the paper). We observe that generally the generated text correctly describes the change between the images. It is natural, coherent and grammatically correct (due to the capabilities of GPT-3). We further see in many examples that the rephrasing of GPT-3 is not limited to the vocabulary in the question and the answer. In some cases \eg, {\it Is the sky clear? No,} the model shows high level of understanding by using new words in the created transition text, as in here {\it ``Make the sky cloudy''} (second example in \cref{fig:vqa_to_lasco_1}).
However, there are also cases where GPT-3 fails to produce an appropriate transition-text. Two such examples are shown in \Cref{fig:vqa_to_lasco_fail}.
%We also show two failure cases in \Cref{fig:vqa_to_lasco_fail}, where GPT-3 failed to generate a proper transition text.

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth,height=0.95\textheight]{figures/suppl/VQA2_to_lasco_2.drawio.pdf}
   \caption{Creating \ourDS triplets from VQA2.0, using our Data Roaming approach. The question, associated answers together with the original and complimentary images are shown. Note the transition-texts automatically created by our approach.}
   \label{fig:vqa_to_lasco_1}
\end{figure*}

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth,height=0.95\textheight]{figures/suppl/VQA2_to_lasco_1.drawio.pdf}
   \caption{Creating \ourDS triplets from VQA2.0, using our Data Roaming approach. The question, associated answers together with the original and complimentary images are shown. Note the transition-texts automatically created by our approach.}
   \label{fig:vqa_to_lasco_2}
\end{figure*}

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth,height=0.95\textheight]{figures/suppl/VQA2_to_lasco_3.drawio.pdf}
   \caption{Creating \ourDS triplets from VQA2.0, using our Data Roaming approach. The question, associated answers together with the original and complimentary images are shown. Note the transition-texts automatically created by our approach.}
   \label{fig:vqa_to_lasco_3}
\end{figure*}

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\textwidth,height=0.7\textheight]{figures/suppl/VQA2_to_lasco_failure.drawio.pdf}
   \caption{Arguably failure converting cases, while creating \ourDS triplets from VQA2.0, using our Data Roaming approach. The duplicated image at the top is from the source. First row shows a logic failure, since the fence is behind the man, and not the other way. Second row shows a questionable rephrasing.}
   \label{fig:vqa_to_lasco_fail}
\end{figure*}

% \subsection{$\text{Recall}@K$ vs $\text{Recall}_{\text{subset}}@K$}
\subsection{CoIR User Study}
Here we provide more details regarding our user study that was separately conducted on \ourDS, FashionIQ and CIRR datasets. We sample $\sim300$ random CoIR triplets from the dataset, each one of them is composed of query-image, target-image and a transition-text. We present each triplet by showing the transition-text and the two images, query and target, side by side as left and right, respectively. We ask the users to rate each triplet just by positive/negative (``Good''/``Bad'') labels.
We crowd-sourced 30 students in the ages of 20-35, from The Hebrew University of Jerusalem, for the evaluation.
All individuals were asked to review the dataset annotations with the following instructions:

\begin{quotation}
``We want you to evaluate the following annotations for the 'Image retrieval' task:\\
{\bf The input}: image (left) + text\\
{\bf The Output}: image (right)\\

The target is to find the (right) image within a large database of images.\\
The query for search is the left image, serves as context, and the text is some modification/request.

% Please review our following annotations:\\
% If you find a sample that the text \sout{is} does not adequately describe\sout{s}
% the transition Left $\rightarrow$ Right, please mark it as {\color{red}{Bad}}.
% ''
How well can the target image on the {\bf right} be derived from the source image on the {\bf left}? If you find that the target image is adequately described by the source image and the required change in the text, please label it as `\textcolor{green}{Good}', otherwise as `\textcolor{red}{Bad}'. ''
\end{quotation}

{\bf User study validation:} In order to further validate the results of the user study described above, we performed a similar study on a larger scale using the Amazon Mechanical Turk (AMT) platform. Specifically, 1000 random samples from each dataset were rated by 3 different AMT workers, using a 1--5 rating scale (worst--best, respectively). A mean opinion score (MOS) was computed for each sample as the average of the three ratings. 
% \sout{Similarly to the results reported in the main paper, we observe a significant similarity between the distribution of these scores for the three datasets.}
%
% By extending the options of feedback from binary (``Bad''/``Good'') to a 1--5 rating scale (worst-best, respectively), we conduct the same experiment \sout{with an higher feedback resolution} \rnote{in larger scale}, on extra 1,000 samples per dataset. Each sample was rated by 3 different users \rnote{to compute a mean opinion score (MOS)} \sout{and get scored by their average rating}. \sout{We observe a similarity between the rating distributions of all three datasets} \rnote{As we agreed, it's better not to discuss distributions. Provide the MOS as you computed it.}. 
Binarization of the ratings (considering 1,2 as `Bad', otherwise as `Good') yields a positive (Good) rate of 90.9\%, 93.8\% and 97.1\% for \ourDS, FashionIQ and CIRR, respectively. The corresponding MOS scores were 3.58, 3.77 and 3.92. The Overall (relative) gap between \ourDS and the other datasets is up to $7\%$. This further confirms that although \ourDS was generated using data roaming, the positive rate of the samples is nearly as high as for datasets that were manually annotated.

Raters were chosen from Amazon Mechanical Turk (AMT) with an HIT approval rate of at least 98\%, from the United States and United Kingdom. 
% \include{tables/amt}

% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/datasets_amt.pdf}
%    \caption{Users rating of CoIR annotations, conducted on 1000 random samples per dataset.}
%    \label{fig:datasets_amt}
% \end{figure}

\section{Modality Redundancy}
\label{sec:modality_redundancy_supp}
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\columnwidth]{figures/cirr/cirr_problematic_ex_a.pdf}
   \caption{A CIRR query-target pair within its collected image sub-group. The transition-text is descriptive enough to reach the target-image (framed in green), within this sub-group.}
   \label{fig:cirr_problematic_ex}
\end{figure}
 Here we present more examples on the modality redundancy effect that we address in the paper (Sec.~6). 
 \Cref{fig:cirr_retrievals} shows examples from CIRR validation set that were retrieved by our \our.
 Note that in rows 1-6, the query texts are quite descriptive and likely adequate to retrieve the target, solely based on textual information. We put it to test by also applying \our(Text-Only) on these examples. Based on transition texts only, it successfully retrieved the target images at rank \#1, implying the redundancy of the ignored query images.
 % To this end we show several results from CIRR validation set that were successfully retrieved as rank \#1 by our CASE(Text-Only) baseline using only the ``transition text". These cases are shown in rows 1-6 in \Cref{fig:cirr_retrievals}. 
 % The retrieval results are depicted as ranked images from left to right. 
 A different example of modality redundancy is reflected in the ``Sub-group" experiments in CIRR. In these experiments a manual transition-text was created to map the query image (on the left) to the target (framed in green) while it was shuffled, by construction, in a sub-group of visually similar images. 
Despite the fact that query-text is not descriptive enough to retrieve the target image from the entire corpus (since it might refer to any kind of dog), it is sufficient to do so within the particular sub-group, due to the high visual similarity. The success \our(Text-Only) on this particular subtask reaching the highest $\text{Recall}_{\text{subset}}@K$ (cf. Table 3, in the main paper) can be related to this fact. 
% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/cirr/cirr_exp_txt_2.drawio.pdf}
%   \caption{Changes in retrieval ranking, w.r.t different text prompts.}
%   \label{fig:cirr_exp_txt}
% \end{figure}



% \begin{figure}[t]
% \begin{subfigure}[b]{1\columnwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/cirr/cirr_problematic_ex_a.pdf}
%          \caption{A CIRR query-target pair within its collected image group.}
%          \label{fig:cirr_problematic_ex_a}
%      \end{subfigure}
%      \vfill
% \begin{subfigure}[b]{1\columnwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/cirr/cirr_problematic_ex_b.pdf}
%          \caption{Top-4 retrievals comparison of \our (top) vs \our-Blind (bottom).}
%          \label{fig:cirr_problematic_ex_b}
%      \end{subfigure}
%      \vfill
%   \caption{A CIRR sample of which a text-to-image model performs better on a tiny group of 6 images (\ref{fig:cirr_problematic_ex_a}) that preserve the same context. However, retrieving from the entire corpus (\ref{fig:cirr_problematic_ex_b}) is impossible without the proper context.}
%   \label{fig:cirr_problematic_ex}
   
% \end{figure}

% Top-5 retrievals of two models, \our and \our-(Text-Only) are shown in \cref{fig:cirr_problematic_ex_b}. The \our-(Text-Only)  performance (bottom row) depend on the query-text only, thus fail to preserve the visual context by retrieving dogs of another breed and color, as \our does (top row). However, in this specific example, \our-(Text-Only) perform better than \our on the six-members subgroup ($\text{Recall}_{\text{subset}}@K$ metric), by retrievals at places \#1 vs \#4. \our perform better on the entire corpus ($\text{Recall}@K$ metric), by retrievals of \#16 vs \#20. 

\section{Ablation Study}
\label{sec:ablation_study}
In this section we show our ablation analysis also on other datasets, in addition to FashionIQ presented in the main paper.
\Cref{tab:cirr_ablations} shows further ablation results on the CIRR dataset. As we expected, our Reverse-Query objective is less crucial to the CIRR dataset, since the transition-texts are ``over-informative" for selecting the target images (as discussed in the main paper).
\input{tables/cirr_ablations}

\subsection{Batch Size Ablation}
Here, we show ablation results on the batch size in CASE training. \Cref{tab:fiq_ablations_bs} presents the results with different batch sizes conducted on \fashioniq dataset. The optimal batch size is around the range 512-1024.
\input{tables/fiq_ablations_bs}


\section{Explainability}
\label{sec:explainability}
We also present an explainability tool for our model for analysis and better understanding its reasoning pattern. To this end, we use two existing methods to generate a visual and textual ``reasoning", during the retrieval.  
%In \cref{fig:cirr_exp} we present such visualization heatmaps over the query inputs.

{\bf Visual modality:} Following \cite{vasu2021explainableCBIR}, we slide a black window (mask) over the query-image and feed it to our model. For each masked query-image, we calculate the cosine distance of the model's output to the ground truth target, in the embedding space. We overlay the result as a heat-map over the query image. ``Hotter'' (tending to red) regions indicate increased importance in retrieval (since masking them results in a larger distance).
%The distance between the final output and the desired target is than calculated and visualized on the query-images. 

\begin{figure}[t]
\begin{subfigure}[b]{1\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cirr/cirr_exp_1.drawio.pdf}
         \caption{Preserving human in target image}
         \label{fig:cirr_exp_1}
     \end{subfigure}
     \vfill
\begin{subfigure}[b]{1\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cirr/cirr_exp_2.drawio.pdf}
         \caption{ }
         \label{fig:cirr_exp_2}
     \end{subfigure}
     \vfill
% \begin{subfigure}[b]{1\columnwidth}
%      \centering
%      \includegraphics[width=\textwidth]{figures/cirr/cirr_exp_3.drawio.pdf}
%      \caption{ }
%      \label{fig:cirr_exp_3}
%  \end{subfigure}
%  \vfill
\begin{subfigure}[b]{1\columnwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/cirr/cirr_exp_4.drawio.pdf}
     \caption{ }
     \label{fig:cirr_exp_4}
 \end{subfigure}
 \vfill
\begin{subfigure}[b]{1\columnwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/cirr/cirr_exp_5.drawio.pdf}
     \caption{Preserving same looking house}
     \label{fig:cirr_exp_5}
 \end{subfigure}
 \vfill
\begin{subfigure}[b]{1\columnwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/cirr/cirr_exp_6.drawio.pdf}
     \caption{ }
     \label{fig:cirr_exp_6}
 \end{subfigure}
 \vfill
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/cirr/cirr_exp.pdf}
   \caption{Explainability heatmaps for \coir task, tested on our model (CASE). Model's attention is indicated by 'jet' heatmap over the image and background highlight over the transition-text. CASE combines information from perceptually reasonable parts of the image and text to reach the target.}
   \label{fig:cirr_exp}
 
\end{figure}

{\bf Text modality:}
For visualizing the relevance of text tokens, we calculate the gradients over attention maps in the shift-encoder, w.r.t the desired target, using the multi-modal visualization method as suggested in \cite{Chefer_2021_ICCV}). 

These two maps create a visualization on the attended regions and textual tokens in the query. Note that, we expect the model to focus on {\it complementary} information between the query-image and transition text. It needs to ``see" the gist or the object in the image and ``understatnd" the change or a specification from the transition-text. 

We demonstrate this explainability method on a few examples in \Cref{fig:cirr_exp}. In \Cref{fig:cirr_exp_1} a diver is shown. The transition text asks for change in the background. As observed, the model attends the diver and the sea in the image (not specified in text), as well as the words such as `Add' and `background'. \Cref{fig:cirr_exp_6} shows another example with a dog, with the transition-text asking to fold its ears forward. Our model attends the head of the dog in the image, probably identifying the dog's breed and ears and corresponding to important cues in the text, \eg `ears' and `forward', to retrieve the correct target image.
%focus on both preserved and need to be changed, the dog identity and its ears. The model also focus on the critical text tokens that defines the subject and its modification (``Fold'', ``ears'', ``dog'', ``forward'').
