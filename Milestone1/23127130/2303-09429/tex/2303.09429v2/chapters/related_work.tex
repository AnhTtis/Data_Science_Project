\section{Related Work}
\label{sec:related_work}
% ============ COIR ==========
%Recent progress in the field of Vision and Language Pre-training (VLP) \cite{clip, BLIP, Align_before_use, li2020oscar, UNITER, ALIGN, DBLP:conf/mm/WuWSH19, vilbert, OFA, large_scale_pretrain_visdial, pretrain_for_vqa_and_img_cap, X_VLM} has shown to be effective for various downstream tasks, \eg, VQA \cite{VQA, balanced_vqa_v2, OFA, vilbert, X_VLM}, Visual Dialog \cite{visdial, large_scale_pretrain_visdial}, Image captioning \cite{li2020oscar, BLIP, OFA, X_VLM}, Image Retrieval (in its variations) \cite{cclip, clip, BLIP, Align_before_use, X_VLM}, \coir \cite{tirg, Kim_Yu_Kim_Kim_2021_dcnet, VAL_IR, FashionVLP, cclip}, etc.
% I have put the above under commnet due to space.
%*****Radford \etal~\cite{clip} introduced {\it CLIP}: a VLP model of two encoders, image and text, that are jointly optimized in a ``late fusion'' manner for image/text alignment. 
%The encoders are stand-alone components: each modality is fed to a separate encoder with the consecutive representations ``interacting'' and trained with a contrastive loss. 

\paragraph{Data Roaming:}
A major challenge in many multi-modal tasks, such as text-video and text-audio retrieval, is the lack of large-scale training data. Due to the complexity involved in creating multi-modal datasets such as text to image \cite{flickr, coco} or video retrieval \cite{MSR_VTT, MSVD}, several studies suggest using raw narrated video footage \cite{howto100M} for video retrieval or altering the narration to create a dataset for Visual Question Answering (VQA) \cite{justAsk_VQA_2021}. Other works try to enhance existing datasets, \eg, COCO captioning \cite{nocaps2019} to more diversity and object categories. In this line of work, \citet{nagrani2022learning} propose a new video mining pipeline which involves automatically transferring captions from image captioning datasets to video clips, to create a new large-scale, weakly labelled audio-video captioning dataset. Nevertheless, for \coir models to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less annotated datasets. In this paper (\Cref{sec:lasco}) we propose a methodology for leveraging VQA2.0 \cite{balanced_vqa_v2}, a large existing and labeled dataset for the VQA task.

% Modality redundancy
\coir datasets consist of triplets of query image, transition text and a target image. In order to differentiate these datasets from text-to-image and image-to-image retrieval, these triplets should ideally satisfy a condition where reaching the target image in the corpus will necessarily require both modalities. 
In this paper we further suggest an analysis tool for the ``quality'' of certain dataset, measured by ``modality redundancy''. We show that our newly generated large scale dataset exhibits higher quality, compared to CIRR on natural images, and is on-par with the domain-specific manually annotated FashionIQ dataset.

\paragraph{Composed Image Retrieval:} 
CoIR methods commonly learn a shared embedding space between the text and visual modalities. These methods often differ by encoding models, \eg, \cite{tirg} that uses ResNet and LSTM and learns a shift encoder. Other methods suggest different attention mechanisms, \eg, \citet{VAL_IR,HosseinzadehW20}. \citet{FashionVLP} focuses on specific domain characteristics such as the fashion domain (with FashionIQ dataset). Different fusion strategies between visual and textual modalities has gained high attention suggesting early \cite{HosseinzadehW20} and late \cite{ARTEMIS} fusion methods. 

Recent works leverage VLM foundation models, \eg \citet{cclip, Arithmetic_multimodal_IR} use CLIP features reaching top performance. 
Encouraged by \cite{cclip} we suggest a strong baseline built from pretrained BLIP components, finetuned on CoIR task.

Lastly, \citet{Kim_Yu_Kim_Kim_2021_dcnet} suggested enforcing cyclic consistency from query/target images back to the transition-text. To this end, they jointly optimized two separate networks, one devoted to the \coir task; another predicting the query text, given the query and target images. The latter is used for re-ranking the target candidates. In this work, we suggest a different auxiliary task, dubbed \emph{reverse objective}, inspired by the CoIR task.
Our reverse objective maps the target image, conditioned on the query text, back to the query image.


% ****\citet{VAL_IR} proposed VAL ({\it Visiolinguistic Attention Learning}) that uses LSTM for query text encoding and CNN for image encoding. They further conduct hierarchical fusion at different CNN layers with objective to match feature maps of query image, conditioned on query text, to the target image. 

% \citet{FashionVLP} focus on the image encoding part, targeting the specific fashion domain. They extract image features at multiple contextual levels to focus on different fashion-related aspects of images.
% %Their model called {\it FashionVLP} used various of pretrained networks, such as BERT\cite{devlin2019bert}, Faster-RCNN \cite{fater_rcnn} and ResNet\cite{resnet}, two first were fixed while the latter was fine-tuned. 
% Although outperforming previous method on {\it FashionIQ}, this method is tailored to the specific fashion domain characteristics.

% \citet{HosseinzadehW20} proposed a dot-product attention mechanism for fusing textual features with image region features, based on a region proposal network. Regional and textual features are separately extracted and merged by early fusion. However, \citet{ARTEMIS} reported a better performance also using two different experts, but in a late-fusion manner, fusing global features of the image and text.

% \citet{cclip} introduced a method that leverages CLIP text and image representations. Using CLIP features, they train a ``combiner'' model: an MLP that performs {\it late fusion} of V\&L features, projecting them into a new shared space where the target images are embedded. We further refer to this method as Late-Fusion CLIP ({\it LF-CLIP}), in our comparisons (\Cref{sec:results}). This rather simple architecture significantly outperforms previous methods (\cf  \Cref{tab:cirr_test,tab:fiq_val}), demonstrating the power of recent foundation models. Our study continues this line of work.

% \citet{cirr}, in addition to introducing the new \cirr dataset, suggest a new scheme dubbed CIRPLANT. Their method leverages a rich pretrained V\&L model for modifying visual features conditioned on natural language, so as to match those of the target image. Their architecture consists of a ResNet \cite{resnet} feature extractor and a projection FC-layer. A transformer is further used for text encoding and V\&L fusion. 

% Lastly, \citet{Kim_Yu_Kim_Kim_2021_dcnet} suggested enforcing cyclic consistency from query/target images back to the transition-text. To this end, they jointly optimized two separate networks, one devoted to the \coir task; another predicting the query text, given the query and target images. The latter is used for re-ranking the target candidates. In this work, we suggest a different auxiliary task, dubbed \textit{reverse objective}, inspired by the CoIR task.
% Our reverse objective learns to map the target image, conditioned on the query text, back to the query image.

% While utilization of foundation V\&L encoders such as CLIP showed a performance boost in \coir, they are still used in a ``late fusion'' mode, as previous works, where image and text are separately encoded (each as a single global vector) before they are able to interact. Other works take a middle way by letting text tokens to interact with a single global visual feature, or with a few selected region-based image features, bounding boxes, \etc. To improve interaction between modalities, we take an early fusion approach, utilizing a local cross-attention module. It allows textual-visual interactions at token-patch level through multiple intermediate cross-attention layers. In training, we add a new auxiliary task and further replace the common contrastive loss used in previous \coir methods with a Recall@K surrogate loss \cite{patel2022recall} that matches the evaluation measure.

%------------- From an older version ------------

%Recently, Li \etal~\cite{BLIP} proposed {\it BLIP}, a new VLP framework that is transfer-flexible to various vision-language tasks, due to its ``early fusion'' ability. BLIP consists of four transformer \cite{vaswani2017attention} components that are jointly optimized: 1) image-encoder 2) text-encoder 3) image-grounded decoder and 4) image-grounded encoder. The first two are trained for ``late fusion'' image/text feature alignment, as done in CLIP. The third is a transformer decoder that was trained on caption generation. The third is an ``early fusion'' encoder that receives image/text features and feed them through cross-attention layers to interact with each other. Its final output is a binary prediction (of image/text alignment). All four components are jointly optimized while some of them share some layers, reducing the number of model's weights. In this work, we utilize BLIP's transfer-flexibility by initializing \our's components with pre-trained weights. 
%%%%%%%%%%%%
% ******************
% Dubey~\cite{decade_survey_cbir} surveys a decade of progress in deep learning methods for CBIR ({\it Content-Based Image Retrieval}), showing the take-over of CNN-based methods for feature representations. However, adding another modality, such as text, requires additional experts to handle each modality or their fusion. A common practice for text-to-image IR (T2I), for example, involves two separate encoders for image and text feature extraction \cite{BLIP,ALIGN,clip,NIPS2013_7cce53cf,DBLP:conf/mm/WuWSH19}, sometimes followed by a third fusion component for mixing features \cite{Align_before_use,li2020oscar,UNITER}.
% ======================
