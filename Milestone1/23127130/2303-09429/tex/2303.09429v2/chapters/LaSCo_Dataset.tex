\section{\ourDS Dataset}
\label{sec:lasco}
In this section we introduce \ourDS ({\it \ourDSfull}), a new \coir dataset consisting of open-domain natural images, that elevates the scale of existing datasets.
To construct \ourDS, we leverage the carefully labeled datasets that exist for the well-studied VQA task \cite{justAsk_VQA_2021}. 
%on its variations \cite{VQA,balanced_vqa_v2,vilbert,plotqa, levy2022classification, okvqa, DocVQA, X_VLM}. VQA requires answering an open language question about a given image. 
Specifically, we utilize the {\it VQA2.0} \cite{balanced_vqa_v2} dataset to create \ourDS with minimal human effort. %{\it VQA2.0} was labelled on COCO~\cite{coco} images
{\it VQA2.0} introduces two important features: 1) A balanced answer set for VQA; 2) Inclusion of ``complementary'' samples, with counter examples.
%used to ensure that the model is not lingual biased and visual input is attended during answer prediction (called 'Making the V in VQA Matter' \cite{balanced_vqa_v2}). 
A complementary image $I_c$ is one that is similar to an original image $I$ in VQA, but yields a different answer for the same question.

%The authors addressed a major textual bias in the previous VQA dataset \cite{VQA}, showing that models exploit language biases in order to answer questions. To mitigate this shortcoming, they collected ``complementary'' samples introducing {\it VQA2.0}, namely a balanced version of the VQA dataset with counter examples, as described below.
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\linewidth]{figures/lasco/VQA2_Modifications3.pdf}
   \caption{Generating transition texts from \textit{VQA2.0} samples. Given two paired triplets $(I, Q, A), (I_c, Q, A_c)$, where $I$ and $I_c$ are visually similar, but yield different answers $A$ and $A_c$ for the same question $Q$, a transition text from $I$ to $I_c$ is generated by $G:=${\it GPT-3} \cite{GPT_3}, based on $(Q, A_c)$.}
   \label{fig:our_vqa2}
\end{figure}

\subsection{Data Roaming}
\label{sec:Data Collection}
We generate \coir triplets from {\it VQA2.0} samples and their ``complementary'' counterparts, as demonstrated in \Cref{fig:our_vqa2}. 
For brevity, let us denote {\it VQA2.0} by set $\mathcal{D}$.
%We use each pair of original image $I$ and its counterpart $I_c$ as query and target images, respectively, and convert the question $Q$ and the answer $A_c$ (for the complementary image) into a transition-text using a language model $G$. 
%{\bf Image pairing:} 
Consider two complementary triplets $(I, Q, A)\in \mathcal{D}$ and $(I_c, Q, A_c)\in \mathcal{D}$. By construction, each $I_c$ image was manually selected from the 24 (visually) nearest neighbors of $I$ such that: 1) any premise assumed in $Q$ must hold in $I_c$; 2) it makes sense to ask $Q$ about $I_c$; and 3) the answer $A_c$ to $Q$ about $I_c$ differs from $A$. The actual answer $A_c$ was provided by a different annotator, in a second round. 
%Nearest neighbors were calculated by $L^2$ distance of penultimate VGG~\cite{vgg_net} features.
These properties of $\mathcal{D}$ enable building our new dataset by using the existing $(Q,A_c)$ pair to construct the transition text $S_c$ from $I$ to $I_c$.

{\bf Transition text:} 
The text conversion task is defined by $G:(Q,A_c) \mapsto S_c$. We employ a strong language model, \eg, $G:=${\it GPT-3} due to its few-shot capabilities~\cite{GPT_3}, to perform the conversion. To create an example set for the task, we recruit 20 annotators to manually rephrase $\sim\!300$ randomly sampled $(Q,A_c)$ pairs to valid transition texts $S_c$. We then provide $G$ a short description of the task, with three annotated examples of $(Q,A_c,S_c)$, and ask the model to perform the task on a new pair $(Q,A_c)$. We further exploit the symmetry in the transition, $I\rightarrow I_c$ and $I\leftarrow I_c$ to generate more triplets. Finally, we organize the triplets as $(I,S_{c},I_c)$, $(I_c,S,I)$, with $S_{c}$ and $S$ indicating the corresponding transition texts. For an extensive list of examples we refer the reader to our suppl. material.
% We construct a transition text from $I$ to $I_c$ by rephrasing $Q$ and $A_c$, since the images $I, I_c$ differ in $A, A_c$ w.r.t the question $Q$. Let us denote a function $G$ for phrasing a valid transition text, given the information in $Q$ and $A_c$, namely: $G(Q, A_c)$. For example, the pair $(Q,A_c)$ (``What color is the building? White'') is rephrased to $G(Q, A_c)=$ ``Change the color of the building to white'' (see \cref{fig:our_vqa2}). To automate this task with minimal manual labeling, we choose a strong language model $G:=${\it GPT-3} due to its few-shot capabilities \cite{GPT_3}. 
% %This language model is based on the idea of in-context learning, namely predicting the answer given only a natural language description of the task (without any gradient updates over the model).
% Following \cite{GPT_3}, we show the model few samples of our task, \ie converting a question and an answer to a transition-text. To this end, we recruit $~20$ annotators to manually rephrase $\sim300$ random $(Q,A_c)$ pairs to valid transition texts. To create variability, for each transition task we create new transition texts, with three random examples and $(Q,A_c)$ from our annotated dataset.

\subsection{Quality control}
We further conduct a data curation process and preliminary evaluation for the quality of our generated \coir triplets. We first remove triplets that share the same query/target image. For text, we apply automatic rule-based scripts to filter out potentially wrong conversions (\eg, too short, unexpected characters such as `\textbackslash{}n', etc.). In our manual examination of $1000$ random text conversions we judged $91.7\%$ of them to produce well-phrased and reasonable transition texts. Next, we conduct a short user study to compare the quality of our transition texts to fully human-annotated ones. We sample $\sim300$ random triplets $(Q_i, Q_t, T_i)$, of query-image, query-text, and target image (respectively). Triplets were shown to a total of 30 users, who were asked whether $Q_t$ ``adequately describes the transition/modification'', from $Q_i$ to $T_i$, or not. This experiment was conducted on three different CoIR datasets. The results show $82.02\%$ positive rate for \ourDS samples, compared to $81.15\%$ for \fashioniq, and $82.65\%$ for \cirr.

Finally, we performed a larger scale user study using the Amazon Mechanical Turk (AMT) platform. We randomly presented 1000 samples (triplets) from each dataset, and asked 3 different AMT workers to rate each sample using a 1–5 rating scale (worst–best, respectively). A mean opinion score (MOS) was computed for each sample as the average of the three ratings. Binarization of the ratings (considering 1,2 as ‘Bad’, otherwise as ‘Good’) yields a positive (Good) rate of 90.9\%, 93.8\% and 97.1\% for LaSCo, FashionIQ and CIRR, respectively. The overall (relative) gap between LaSCo and the other datasets is under 7\%, indicating that the generated texts are on-par with human annotations. For further information, please see our suppl.~material.

\input{tables/datasets}

\Cref{tab:datasets} compares statistics of \ourDS to previous \coir datasets. \ourDS contains over 389K queries, $\times$10 larger than previous datasets, with an image set containing 121.5K different images, compared to previous 19K--41K. The size of the test image corpus, determining the target search space, is almost 40K, compared to 2.3K in \cirr and 15.4K in \fashioniq. In terms of natural language, \ourDS is richer with 13.5K different language tokens, compared to 4.4K in \fashioniq and 6.8K in \cirr. Moreover, \ourDS and \emph{VQA2.0} are both derived from COCO's image set; thus, captions are available for each of \ourDS's images. Utilizing captions as an additional cue (see \Cref{sec:evaluation}) allows creating a rich dataset for training \coir methods to achieve high performance in both Text-to-Image and \coir tasks.


