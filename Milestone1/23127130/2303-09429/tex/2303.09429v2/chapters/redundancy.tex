\section{Modality Redundancy}
\label{sec:modality_redundancy}

In this section, we first propose a simple analysis of existing \coir datasets to examine the degree to which their queries require \emph{both} modalities for successful retrieval. Next, a similar analysis is proposed for assessing the bias of \coir methods towards modality redundancies.

An ideal composed query should require both modalities for retrieving the desired target. For example, a transition-text such ``Change the color to be more cream colored'' in the top row of \Cref{fig:fiq_cirr_ex}, will only succeed in finding the proper target in conjuction with the query image, since the type of object cannot be inferred from the text alone.
However, in practice, one of the modalities can become redundant, with the degree of redundancy depending on the information conveyed by the other modality.
On one extreme, the query-image might be completely redundant, reducing the task to Text-to-Image retrieval; on the other extreme, the query-text might be redundant, with the task becoming Image-to-Image retrieval.
To quantitatively assess the degree of redundancy in \coir datasets, we measure the Recall@K performance of naive Text-to-Image and Image-to-Image retrieval using the embeddings produced by an independent off-the-shelf CLIP model \cite{clip}.
%if one of the modalities is redundant (\eg, query-image), the query might be associated to another task (\eg Text-to-Image). However, such redundancies are not necessarily a binary characteristic, and depend on the relative information contributed by each modality. In order to examine these, we use an independent off-the-shelf CLIP \cite{clip} model for analyzing \coir queries. We evaluate its retrieval performance on each dataset, in two different tasks: Text-to-Image (T2I) and Image-to-Image, by encoding the proper modality for the shared embeddings space search.
% In composed image retrieval, the expectation is that the composed queries should require both modalities (image and text) in order to properly indicate the desired target.
%for a dataset (triplets) that require both image and text queries to reach the designated target.
% Datasets or samples where only one modality suffices for retrieving the target introduce a modality redundancy, and might drive the model to solve a different task of content-based-image retrieval (CBIR) or text-to-image (T2I).
%In a a general \coir dataset, one would expect both modalities to be necessary in order to retrieve the desired target, and that omitting one of them would result in a wrong retrieval.
%For example, a detailed description of the target contained in the query-text, may make the query-image redundant.
% We can categorize samples in a certain dataset as (1) Text-to-image, (2) Image-to-image, or (3) Composed, where either single modality or composition of the two is required for target retrieval. One would associate a higher redundancy level for a sample/dataset if it tends to belong to category (1) or (2) and lower redundancy level (equivalent to higher compositionality) if it tends to belong to category (3). In order to examine the level of modality redundancy in certain \coir datasets, we use an independent off-the-shelf CLIP \cite{clip} model and evaluate the retrieval performance in each dataset using Text-to-Image (T2I) and Image-to-Image by CLIP. Here, the search is conducted per-modality according to CLIP embeddings.
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.85\linewidth]{figures/clip_on_datasets.pdf}
   \caption{Modality redundancy level in several datasets (lower is better). Recall@K values for {\it Text-to-Image} and {\it Image-to-Image} retrieval using off-the-shelf CLIP \cite{clip}. Lower values indicate higher image and text compositionality. The results for  COCO  \cite{coco} are shown as a reference for a purely text-to-image (single modality) retrieval task, presenting an upper bound.
	}
   \label{fig:tti_clip}
\end{figure}
To create a continuous measure we compute the Recall@K for varying $K$ values. These measurements computed on several datasets are plotted in \Cref{fig:tti_clip}.
%\Cref{fig:tti_clip} indicates the level modality redundancy in several datasets.
A lower curve indicates that the corresponding dataset is more challenging for a uni-modal query. Note that the \ourDS and \fashioniq curves are much lower than \cirr, implying that more of the queries in \cirr are modality-redundant.
For reference, we also plot the performance of the CLIP-based Text-to-Image retriever using COCO captions as query text (a commonly used benchmark for Text-to-Image retrieval \cite{BLIP, Align_before_use, li2020oscar, clip}). While COCO may be viewed as an ``upper bound'' for this task, note that the \cirr curve is quite close to it.
%Note that COCO serves as an ``upper bound'' since its texts are the target-captions, which assumed to be sufficient for retrieval.

Next, we employ a similar analysis for studying the degree to which \coir methods (trained on a certain dataset) are affected by the presence of modality redundancies in the dataset.
Starting from the full \cirr validation set, denoted as $V$, we generate a sequence of progressively ``purified" subsets $V_n \subset V$, with each subset containing fewer modality redundancies. Specifically, subset $V_n$ is generated by removing from $V$ all of the queries for which the naive CLIP-based Text-to-Image retriever, retrieves the correct target image among it's top-$n$ results. In \Cref{fig:k-filter} we plot the average of Recall$@\{1,5,10,50\}$ as a function of $n$, measured by applying our baseline, \our on each dataset.

%We further take advantage of this approach for studying \ourDS influence on our model, by examining its \coir abilities, by the following experiment. We evaluate \our versions on different subsets of \cirr's validation set, denoted as $V$. In order to evaluate the \coir abilities (separated from T2I abilities), we ``purify'' $V$ by filtering out T2I queries, according to CLIP performance (T2I) on \cirr. Hence, all queries in $V$ which CLIP was successfully retrieved in top-$n$ were removed from $V$, defining $V_n\subseteq V$. Intuitively speaking, the higher $n$, the more difficult it gets to CLIP to retrieve the query's target, which based on transition-text only. We measure the performance on $V_n$ with an average of Recall$@\{1,5,10,50\}$ and present the results in \cref{fig:k-filter}.


% as the level of dependence on  the query-image. \rnote{Higher Recall@K values indicate that targets are likely to be retrieved given only the transition-text, without the need for the query-image.}
% \dnote{Fig. 4 is impossible to understand at this point, since the x-axis is not explained at all!}
% Lower result are therefore show higher bi-modality dependency requirement in the retrieval, at the specific dataset. Note that \ourDS and \fashioniq show a lower measures than \cirr, implying that the \cirr's distribution is tended to be a mix of Text-to-Image and composed samples. We further show results on COCO captions (a commonly used benchmark for Text-to-Image retrieval \cite{BLIP, Align_before_use, li2020oscar, clip}) as reference. The high modality redundancy values for COCO are expected as full textual description (captions), associated with the images, are sufficient for successful retrieval. 
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.85\linewidth]{figures/k-filtered.pdf}
       \caption{Average retrieval performance on subsets of \cirr determined by {\it image-redundancy} levels. Higher values of Average Recall, imply a desirable trend of higher level of required {\it compositionality} between text and image (lower modality redundancy). See \Cref{sec:modality_redundancy}.} 
   \label{fig:k-filter}
\end{figure}
Note that the performance of the CLIP-based retriever (blue line) vanishes at $V_{50}$, since by construction, $V_{50}$ contains only queries for which CLIP failed to retrieve the target within its top 50 results. A similar trend is observed with \our-(Text-Only), a variant of our model trained on transition-texts only, ignoring the image. \our-(Text-Only) exhibits performance degradation with increased $n$, as it relies solely on query-text. While \our that was trained on \cirr (orange line) shows better performance, it still suffers some degradation as $n$ grows, implying that some bias towards modality redundancies might still exist. However, when \our is trained or pre-trained on \ourDS dataset (pink and brown lines, respectively), it achieves the best performance, which is roughly constant regardless of $n$.
Thus, \ourDS appears to be effective at removing bias towards redundancies, and \our pre-trained on it is better suited for datasets with high compositionality.


%with approximately constant values across all $n$'s. This result shows the benefit from (pre-) training on \ourDS, that most likely require compositionality, and therefore are less biased towards transition-text. We believe that our suggested method can be used in future for evaluation of biases in both \coir models and datasets.


% \mnotec{For each $n\in\{0,...,100\}$,
% we construct $V_n\subseteq V$ according to CLIP Text-to-Image retrieval  performance (query-text to target-image). At each $n$ we create a new subset $V_n:=V\setminus C_n$, by removing the query samples where CLIP ranked the ground truth target image, in top-n candidates, denoted as $C_n$. Note that $\forall n\in \{0,..99\}~C_{n}\subseteq C_{n+1} \Rightarrow V_{n+1}\subseteq V_{n}$. Namely, $V_n$ includes all the queries that CLIP failed to retrieve their ground truth target within top-n candidates. Higher $n$, therefore indicate lower image redundancy, \ie the transition text is not sufficient and the target is more dependent on both of the query modalities.} 

%Denote $C_k\subseteq V$ as the queries on which CLIP rank the ground truth target image, in top-K candidates. We define $V_k:=V\setminus C_k$. 

%%%%%%%%
%filter out samples by excluding T2I samples while keeping \coir samples. We detect T2I samples by using off-the-shelf CLIP baseline. For each $k\in[100]$, we exclude samples that made it to the baseline's top-k rank. Namely, we consider $100$ different difficulty levels of retrieving samples based on text only, assuming T2I samples will get small values of $k$'s, while \coir samples will get high values since they are depend, by definition, on their query-image as well. \Cref{tab:tti_clip} demonstrates datasets difficulties for text-to-image (T2I) retrieval, by showing results of CLIP off-the-shelf baseline, for T2I task, without further finetuning. Note that COCO \cite{coco} is a commonly used benchmark for T2I task \cite{BLIP, Align_before_use, li2020oscar, clip}. \Cref{fig:k-filter} shows comparison between different models, on various k-filtered subsets of \cirr validation set.

%We observe that \cirr's domain includes but is not limited to samples of the \coir tasks. In fact, this dataset contains a mix of T2I and CIOR samples. \ie part of the samples could be solved by solely text queries (T2I) while the other require the query-image information (\coir). We assume such phenomenon may be caused by annotation mistakes, where annotators describe the target image while ignoring the query image. In order to mitigate this shortcoming, and to show its existence, we filter out samples by excluding T2I samples while keeping \coir samples. We detect T2I samples by using off-the-shelf CLIP baseline. For each $k\in[100]$, we exclude samples that made it to the baseline's top-k rank. Namely, we consider $100$ different difficulty levels of retrieving samples based on text only, assuming T2I samples will get small values of $k$'s, while \coir samples will get high values since they are depend, by definition, on their query-image as well. \Cref{tab:tti_clip} demonstrates datasets difficulties for text-to-image (T2I) retrieval, by showing results of CLIP off-the-shelf baseline, for T2I task, without further finetuning. Note that COCO \cite{coco} is a commonly used benchmark for T2I task \cite{BLIP, Align_before_use, li2020oscar, clip}. \Cref{fig:k-filter} shows comparison between different models, on various k-filtered subsets of \cirr validation set.


% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/fiq/fiq_test_ex_1.drawio.pdf}
%   \caption{Test examples from \fashioniq dataset. The query (image and text) is most left. \our top 5 ranking is presented. Ground truth answer is marked in green (rank 1).}
%   \label{fig:fiq_ex_1}
% \end{figure}

% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/cirr/cirr_val_ex_1.drawio.pdf}
%   \caption{Test examples from \cirr dataset. The query (image and text) is most left. \our top 4 ranking is presented. Ground truth answer is marked in green.}
%   \label{fig:cirr_ex_1}
% \end{figure}
