\section{Introduction}
\label{sec:intro}
% Description of the task
% Recent progress in the field of multi-modal learning \cite{clip, BLIP, vilbert} has been reflected in various downstream tasks, \eg, VQA \cite{VQA, balanced_vqa_v2, X_VLM, levy2022classification, prefil}, Visual Dialog \cite{visdial}, Image captioning \cite{li2020oscar,OFA}, Image Retrieval (in its variations) \cite{cclip, Align_before_use} and Composed Image Retrieval (\coir) \cite{tirg, Kim_Yu_Kim_Kim_2021_dcnet, VAL_IR, FashionVLP, cclip}.
Recent progress in the field of multi-modal learning \cite{clip, vilbert} has been reflected in various downstream tasks, \eg, VQA \cite{VQA, levy2022classification}, Visual Dialog \cite{visdial}, Image captioning \cite{li2020oscar}, Image Retrieval (in its variations) \cite{cclip, Align_before_use} and Composed Image Retrieval (\coir) \cite{tirg, VAL_IR, cclip}.
Image Retrieval (IR) is a longstanding task that aims to find a desired image in a large corpus, given a user query. While content-based image retrieval uses a single visual modality to convey the user intent ~\cite{barz2021content, dubey2021decade, fewshotIR_ICIP2020}, providing a bi-modal query can mitigate miss-interpretations. In \coir
the gist and attributes are more succinctly described visually, and further intent is specified via a lingual modality \cite{fashion200k, MITstates, tirg, DialogBasedIIR, fashioniq, cirr, Arithmetic_multimodal_IR}. Some examples of \coir queries and their results are shown in \Cref{fig:arc,fig:fiq_cirr_ex}.

\begin{figure*}[t]
	\centering
	%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.9\linewidth]{figures/Arc2.drawio.pdf}
	\caption{An overview of our \our baseline. The query-image is fed to a ViT encoder. The query-text is handled by our shift encoder \ie a BERT encoder with intermediate cross-attention layers, that receives the ViT output and fuses textual and visual information. The resulting $[\mathit{CLS}]$ token is then pooled and projected into a shared 256D space (circled $Q$). Finally, the $K$ nearest cosine-similarity neighbors of $Q$ are retrieved. For each training query, we also create a Reverse-Query by switching the roles of query and target images. A learnable special token $[\mathit{REV}]$ in our transformer, handles the {\it prediction of the query-image} (circled $RQ$) as the Reverse-Query task.}
	\label{fig:arc}
\end{figure*}

% Datasets
Despite the progress in foundation models and new \coir architectures, curating a dataset for CoIR remains a challenging chore, where the samples are triplets of query-image, accompanying transition-text, and the target image, serving as the ground-truth answer.
%The query-text serves as a {\it transition} text, {\it shifting} the query image toward the target.
%where one should arrange the dataset as triplets of query-image $Q_i$, query-text $Q_t$ (corresponding to $Q_i$) and the target-image $T_i$ to serve as the ground truth answer for the given ($Q_i$, $Q_t$) combination. Note that in ideal \coir, $Q_t$ serves as a {\it transition} text, {\it shifting} the query image toward the target $T_i$. 
%Relying on the input image is an important factor that distinguishes this task from Text-to-Image (T2I) retrieval.
There are several existing datasets for \coir that differ significantly from each other. \citet{tirg} propose a dataset of rendered images of simple 3D scenes. Other existing datasets suffer from a small amount of data, and some are domain-specific (\eg, shoes \cite{DialogBasedIIR}), while in others, the lingual modality is limited by transition-text used as a class label \cite{MITstates}, or generated based on pairs of image captions that differ by a single word~\cite{fashion200k}. Another dataset was labeled based on previous vision and language (V\&L) models~\cite{Arithmetic_multimodal_IR}. Recently, \citet{fashioniq} introduced \emph{FashionIQ}, another domain-specific dataset for \coir, which gained popularity (\eg \cite{FashionVLP, Lee_2021_CVPR_cosmo, cirr}
% \cite{fashioniq,FashionVLP,cirr, compo_image_text_query_ir, SAC, VAL_IR, Lee_2021_CVPR_cosmo, Kim_Yu_Kim_Kim_2021_dcnet, cclip, RTIC, MAAF, TRACE, JVSM}
) and contains human-annotated labels.

In addition to their small scale, shortcomings of these datasets include: 1) Not all acceptable target images for a given query are labeled as such, leading to incorrect count of false-negatives (\eg, \cref{fig:fiq_cirr_ex}); 2) Lack of visual complexity (due to restriction to a specific domain); and 3) Modality redundancy, \ie target images may often be retrieved using solely the query text, when descriptive enough to ignore the query image. We further refer to this issue as (lack of) {\it compositonality}: where the target should be determined by its query constituents combining the lingual and visual cues.

To break out from the previous domain-specific datasets to general and natural image domain, \citet{cirr} introduced the new \cirr (Composed Image Retrieval on Real-life images) dataset that contains open domain {\it natural images}, taken from NLVR2~\cite{nlvr2}. To the best of our knowledge, \cirr is the only existing dataset for \coir based on natural images with human annotated open-language texts. 
Despite attempts to reduce the false-negatives and relying on direct human labeling, \cirr still has two major shortcomings: 1) The image corpus is small, alleviating retrieval; 2) Modality redundancy still exists (see \cref{sec:modality_redundancy}), as well as false-negatives (according to our observations), reflecting the challenge in creating a ``clean" dataset.
%3) Lingual bias. We argue that these shortcomings are inherent characteristics of many existing datasets, although relying on direct human labeling. 

%\coir and T2I (Text-to-Image retrieval) samples, an outcome that is highly difficult to avoid with human labeling. Furthermore, we observe the existence of ``false-negatives'' in \cirr, despite the effort to prevent them. Nevertheless, to the best of our knowledge, \cirr is the only dataset for \coir based on natural images with human annotated open-language texts. We further suggest a simple new metric for analysis of \coir datasets, which estimates the fraction of samples with non-redundant modalities.
%\dnote{\sout{That sounds like we're improving existing COIR datasets, but we're creating a completely new one. How about: ... Can we say that we create a dataset of unprecedented size and diversity, or would that be too much?}}\mnote{Adopted. btw, i fixed it to "minimum effort", since we labeled about 300 examples for GPT3 few-shot}

In this work, we introduce a new large scale dataset for \coir, dubbed \ourDS (\ourDSfull dataset).
To construct it with minimal human effort, we employ a simple and effective methodology to rephrase labels from an existing large scale VQA dataset \cite{balanced_vqa_v2} into a form suited for \coir.
\ourDS contains an open and broad domain of natural images and rich text. Compared to \cirr, it has $\times\!10$ more queries, $\times\!2$ more unique tokens and $\times\!17$ more corpus images. We then propose a new approach for analyzing CoIR datasets and methods, which detects modality redundancy or necessity, in queries. Our analysis demonstrates that \ourDS shows a significantly smaller bias towards a single modality for retrieval. 
%\rnote{Need to add modality redundancy and CASE baseline.}
%\dnote{CASE is discussed below...}

SoTA approach \cite{cclip} employ CLIP \cite{clip} to separately encode the textual and the visual query, followed by feature vector concatenation and a learnable projection head. We experiment with an additional end-to-end learnable baseline, which leverages the layers of BLIP's \cite{BLIP} image-grounded text encoder and enables early interactions between textual tokens and individual areas (patches) in the image. This baseline, dubbed \our ({\it \ourfull}), builds upon bi-modal {\it Cross-Attention} to create an {\it Encoder} that {\it Shifts} the query-image towards the target in the embedding space (see \cref{fig:arc}).
\our is trained using a novel bi-directional objective, which we refer to as {\it Reverse-Query} (RQ), where the query-image is predicted from the target-image and the query-text.
%This objective is inspired by the {\it Reverse-Image} task in BLIP \cite{BLIP}, which predicts the query-text from the target-image and the query-image. We show that \our achieves SoTA performance on \fashioniq and \cirr, outperforming previous methods by a large margin. Moreover, \our performs better than previous methods on \cirr, using query-text only (see \cref{sec:ablations}), as a text-to-image (T2I) model.} 
%Our method significantly outperforms previous methods on two most common \coir benchmarks, \fashioniq and \cirr, as well as on our own new benchmark. We demonstrate generalization on both domains, fashion and natural images, unlike methods that tested only on single domains \cite{FashionVLP,SAC,VAL_IR,Kim_Yu_Kim_Kim_2021_dcnet, Arithmetic_multimodal_IR}. In contrast to other methods that adopted CLIP (\eg, \cite{cclip, Arithmetic_multimodal_IR}), we train our model end-to-end, gaining further performance boost when training our text/image encoder, adapting it to the specific dataset.
%Further pre-training our model on \ourDS boosts performance on \cirr dataset coming from the same domain, even at zero shot.
Being based on BLIP, \our uses a lower dimension latent vector of 256D, reducing retrieval complexity by a factor of $\times 2.5$ over previous SoTA. Furthermore, pre-training our baseline on \ourDS improves performance on \cirr dataset and even surpasses previous SoTA methods without training on \cirr (at zero-shot).

In summary, our key contributions in this paper are:
\begin{itemize}
\item {\it \ourDS:} A new large-scale, domain-free \coir dataset, a few orders of magnitude larger than existing datasets.
\item {\it Data Roaming:} A simple methodology for automatically generating \coir triplets from an existing VQA dataset. 
\item {{\it Modality Redundancy:} A method for analyzing redundancy between modalities in existing \coir datasets.}
\item {\it \our:} A new BLIP-based baseline, featuring early fusion and a novel bi-directional training objective, that achieves SoTA performance with a large gap on \fashioniq, \cirr and \ourDS benchmarks.
\end{itemize}
% \mnotec{Image Retrieval (IR) is a longstanding task that aims to find a desired image in a large corpus of images, given a user query.
% The query is the means by which users are able to convey what they are looking for. Arguably, the most common type of query consists of natural language text (\eg, Google Images, Bing Images, Google Photos, Yahoo Images, etc).
% %As the name implies, the desired photo should be found based on the user's text.
% Another variant of the IR task is CBIR (\emph{Content-based Image Retrieval}), where the query consists of a user-provided image whose content should be echoed by the retrieved target image (\eg, retrieve images of the Eiffel tower, given a single photo of it.)
% In this work, we are concerned with the 
% \emph{Composed Image Retrieval} (COIR) task, where the query is \emph{composed} from both an image and natural text. More formally, the task is to find a target image $T\in \mathcal{D}$ where $\mathcal{D}$ is a large corpus of images, and the query $Q:=(Q_i, Q_t)$ consists of an image $Q_i$ and a text $Q_t$. Examples of such queries are presented in Figures \ref{fig:arc}, \ref{fig:fiq_ex_1} and \ref{fig:cirr_ex_1}: the goal of the text $Q_t$ is to describe the semantic difference between the image $Q_i$ and the desired target $T$.

% Previous studies \cite{fashion200k, MITstates, tirg, DialogBasedIIR, Arithmetic_multimodal_IR} introduced new variations of IR that involve multi-modal queries, composed of image and text. However, some datasets are domain specific and borrowed from other tasks, by pairing images within classes or generating their textual descriptions based on token differences. In other cases, a toy dataset is automatically generated \cite{tirg} by a 3D engine or domain specific (\eg, shoes \cite{DialogBasedIIR}) with a fewer amount of data.
% Wu \etal \cite{fashioniq} introduced the {\it FashionIQ} dataset that contain multi-modal queries composed of fashion images and texts. It was annotated by pairing images with a relative English caption, describing the transition from one image to another. Liu \etal \cite{cirr}
% addressed two major shortcomings of previous COIR datasets \cite{fashion200k, fashioniq}: 1) Lack of visual complexity (due to restriction to the fashion domain) 2) Existence of false-negatives: corpus images that match a given query, despite not being labeled as the query's targets. To mitigate these issues, they introduced the new {\it CIRR} (Image Retrieval on Real-life images) dataset that contains open domain natural images, taken from the NLVR2 \cite{nlvr2} dataset. Its annotation process started by grouping images based on image similarity (using ResNet152 \cite{resnet} features), followed by image pairing within each group, forming a set of reference-target image pairs. Finally, human annotators were presented with image pairs and asked to write down sentence specifying the ``modifications'' for each reference-target pair.

% Despite the attempt to address major previous shortcomings, CIRR has two major shortcomings of its own: 1) Small size of image corpus 2) Image-independent queries. 
% The CIRR corpus consists of only $2,297$ images, compared to the FashionIQ corpus of size $15,415$. A smaller corpus makes it easier to distinguish or retrieve a desired image. Previous studies mitigated this issue by enriching the corpus with distractor images~\cite{revisitingOxfordParis}. However, adding new corpus images without annotations might aggravate the false-negatives shortcoming that CIRR attempts to solve in the first place.
% %Adding new corpus images without further annotations might result in images that fit some queries descriptions.
% The second shortcoming is the presence of modality-independent targets in CIRR: we observe that some target images, which are supposed to be retrieved by a multi-modal query (visual and text), are, in fact, independent of the query image. In other words, some target images may be retrieved using solely the query text. This issue occur with samples that were apparently annotated based only on the target image. We further demonstrate image independence of several COIR datasets by showing results of text-to-image CLIP \cite{clip} baseline on several COIR datasets (in \cref{tab:tti_clip}).

% \dnote{The intro ends abruptly and pre-maturely. Before ending it we should provide a brief overview of what we are doing, and our contributions/results.}

% Despite of their drawbacks, {\it FashionIQ} and {\it CIRR} are the only public datasets that was annotated for the COIR task, including query-texts consist of free natural language phrasing. In order to solve this multi-modal task, we propose {\it \ourfull (\our)}, a new method using cross-attention layers to fuse cross-modality information. \our achieves state-of-the-art performance on both {\it FashionIQ} and {\it CIRR} by a large margin. Moreover, our architecture performs better than previous methods on CIRR, using query-text only (see \cref{sec:ablations}), as a text-to-image (T2I) model. 
% We further present \ourDS ({\it \ourDSfull}), a new benchmark for scaling up existing COIR datasets. 
