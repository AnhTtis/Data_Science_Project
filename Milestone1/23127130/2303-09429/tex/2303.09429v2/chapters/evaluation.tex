\section{Evaluation}
\label{sec:evaluation}

%\dnote{\sout{We should be talking about evaluating our new dataset, not our model. The section needs a serious rewrite.}}
We evaluate our baseline on \fashioniq, \cirr, and {\it \ourDS} benchmarks, one domain-specific and the others more general and broad, based on natural images. First, we show the results from our newly suggested baseline, CASE. 
Next we examine the effect of using our new \ourDS dataset for training and pre-training (train/val.~split of 92\% \& 8\%). We also present results with pre-training with a mixture of COCO captions, that are very descriptive to better handle samples where the transition text is highly detailed, making the the query image often redundant (\ie text-to-Image retrieval). To this end, we conduct an experiment where we train \our on \ourDS, replacing 50\% of transition-texts $Q_t$, with captions, corresponding to the target image. Namely, we change the train distribution to combine both \coir and text-to-image samples, as discussed in \cref{sec:Data Collection}. We then explain the results thru the properties of different datasets in terms of modality redundancy.
\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=1\linewidth]{figures/retreivals_cirr_val_ex.drawio.pdf}
   \caption{\our top-4 retrievals (from left to right) for queries in \cirr (top two) and \fashioniq (bottom two). The query (image and text) is shown in the left column. The single ground truth target is framed in green. Arguably, additional images could be marked acceptable (referred as false-negatives).}
   \label{fig:fiq_cirr_ex}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}
\fashioniq \cite{fashioniq} contains crowdsourced descriptions of differences between images of fashion products. Images are collected from the web and divided to three categories of {\it Shirts}, {\it Dresses} and {\it Tops\&Tees}. The query and target images were automatically paired based on title similarities (crawled from the web). This dataset consists of 30K queries (see \Cref{tab:datasets}), annotated on 40.5K different fashion images. There are 4.4K different tokens in the transition-texts (according to {\it BERT} tokenizer). The validation corpus contains 15.4K different images (from which target should be retrieved). The bottom two rows of \Cref{fig:fiq_cirr_ex} show \fashioniq retrieval examples.

\cirr contains open domain natural images, taken from NLVR2 \cite{nlvr2}. It contains a total of 36.5K queries annotated on 19K different images, with 6.8K unique tokens in the transition texts. Examples may be seen in the top two rows of \cref{fig:fiq_cirr_ex}. Its validation corpus is relatively small, with a size of 2.3K. The authors further suggest two benchmarks, one \emph{general}, with the target search space as the entire validation corpus, and a \emph{subset}, where the search space is a subgroup of six images similar to the query image (based on pre-trained ResNet15 feature distance), demonstrating a fine-grained retrieval task. 
%------ Original version
% \fashioniq \cite{fashioniq} contains crowdsourced descriptions of differences between images of fashion products. Images are collected from the web and divided to three categories of {\it Shirts}, {\it Dresses} and {\it Tops\&Tees}. The query and target images were paired based on title similarities (crawled from the web). This dataset consists of 30K queries (\cf \Cref{tab:datasets}), annotated on 40.5K different fashion images. There are 4.4K different tokens in the transition texts (according to {\it BERT} tokenizer). The bottom two rows of \Cref{fig:fiq_cirr_ex} show \fashioniq retrieval examples.

% \cirr contains open domain natural images, taken from NLVR2\cite{nlvr2}. Images were divided to small subgroups of six similar images, based on pre-trained ResNet152\cite{resnet} feature distance. Annotators provided with such subgroup $A=\{I_1,...,I_6\}$ and $I_i,I_j \in A$, and were asked to write a ``transition text'' (originally called ``modification text'') from $I_i$ to $I_j$ that is valid solely to $I_j\in A$. The authors further suggest two benchmarks, one \textit{general}, where the target image is retrieved from the entire validation set, and a \textit{subset} benchmark, where the target is searched among its subgroup only, representing a fine-grained retrieval task. 
% Compared with existing datasets, \cirr places more emphasis on distinguishing between visually similar images, which provides a greater challenge, as well as a chance for studying fine-grained vision-and-language (V\&L) reasoning in \coir with natural images. The authors claim that their benchmark reduces the number of false-negative samples, being fully labeled \cite{cirr}. \cirr contains a total of 36.5K queries annotated on 19K different images, with 6.8K unique tokens in the transition texts.
% \dnote{the above is a very detailed description of \cirr. Do we really need to go over all of the above details? In fact, this whole subsection of evaluation seems just describes previous datasets and doesn't present any results.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation Details}
 We set an {\it AdamW} optimizer, initializing learning rate by $5\times 10^{-5}$ with a exponential decay rate of $0.93$ to $1\times 10^{-6}$.
 We train \our on \cirr with a batch size of 2048 for 6 epochs. For \fashioniq, we train with a batch size of 1024 for 20 epochs (further ablation on the batch size is available in suppl. material).
 On \ourDS we train with a batch size of 3840 for 10 epochs. We use the Recall@K surrogate loss~\cite{patel2022recall} as the differentiable version of the Recall@K metric. Training on four A100 nodes takes 0.5-6 minutes per epoch, depending on dataset size.
\subsection{Results}
\label{sec:results}
We start by showing the performance of our \our baseline on {\it FashionIQ}, in \Cref{tab:fiq_val}. The results are broken down to different clothing categories and Recall@K values. For demonstrating modality redundancies, we train two baselines only on query-image (Image-only) or query-text (Text-only).
%\rnote{\sout{I believe the results for Text-only are based on a different trained model.}}\mnote{It's ours}
Interestingly, \our achieves SoTA results, surpassing the previous top performing method (LF-CLIP \cite{cclip}) by a large margin (13.4\% and 11.6\% absolute points at Recall@10,50 respectively). %Comparing to previous SoTA, \our improves R@10 by $13.4\%$ and R@50 by $11.65\%$.
The poor results for \our (Image-only) baseline, show that visual information is not sufficient for {\it FashionIQ}, as often the transition text asks for a certain change in the image (see \Cref{sec:modality_redundancy}). However, the \our (Text-only) baseline results are close to the previous method of LF-CLIP, indicating the high level of redundancy in the image, as the single text modality is sufficient to reach the previous SoTA performance. 
%The LF-B baseline shows poor results, implying the \drep{efficiency}{effectiveness} of \dtext{using} cross-attention \sout{use}. \dtext{It also performs much worse than LF-C, which only differs by its encoders (CLIP instead of BLIP)}. \dnote{but why the BLIP version is so much worse?}
\input{tables/fashioniq}

\Cref{tab:cirr_test} shows results on \cirr. At the top we present the results from previous methods. The right columns $R_{\textit{subset}}@K$, correspond to the fine-grained retrieval task (introduced originally in \cite{cirr}). Here, we show five different variants of our model. As in {\it FashionIQ}, the poor results for \our (Image-Only) imply that the query-image alone is not sufficient for retrieval also on \cirr.
Interestingly, \our(Text-only) reference surpass previous methods in most metrics, further demonstrating the high level of modality redundancy in \cirr as shown in \cref{sec:modality_redundancy}. This baseline is also top performing in the {\it subset} benchmark.  We believe this is caused by the existing image similarity in the subset, making the query image redundant for the task (see visual examples in suppl.~material). 
%We show a visual example in \cref{fig:cirr_problematic_ex}, with the corresponding ground truth annotation. ***($R_{\textit{subset}}@K$), comparing to \our with rank \#5. However, \our ranks the ground truth image at rank \#16 among the entire corpus, comparing to \our(Only-Text) with rank \#20. In addition, results of \our respond better to the query than \our(Only-Text), preserving the same dog breed/color (see suppl. material).
\input{tables/CIRR}
Next, we observe the performance of \our, which consistently outperforms previous methods. 
%for all K values, on both \textit{general} and \textit{subset} benchmarks with a significant gap, \eg, by absolute 14.4\% at Recall@1 (33.59\% to 48\%). This is an evidence of a stronger image-text composition layers suggested in our baseline, containing the early fusion cross-attention module. Some qualitative examples are shown in \cref{fig:fiq_cirr_ex}, showing a high level of perception in our model. 
We further show visual and textual explainability maps in the supplementary material.

Now we examine the impact of our \ourDS dataset in two main aspects 1) Pr-training and 2) Zero-Shot inference. The results are shown under \our Pre-\ourDS, gaining $\sim$0.5-1\% improvement. We found that this relatively small improvement is due to the modality redundancy in the \cirr dataset. We justify this assumption by the analysis shown in \cref{fig:k-filter}, and by shifting \our towards the distribution of \cirr. Specifically, we train \our on a mix of \ourDS transition-texts with full target captions (taken from COCO captions), which we denote \ourDS.Ca, thus biasing the model towards highly descriptive texts. As shown in \cref{tab:cirr_test}, it further boosts performance on \cirr, a fact that we attribute to improvement of the Text-to-Image (T2I) search capability (see \Cref{sec:modality_redundancy}). 
The use of \ourDS.Ca significantly boosts performance also in \emph{zero-shot} on \cirr test set (\ie without even training on \cirr), surpassing previous methods in most metrics, indicating again the impact of modality redundancy on the results.

%{\bf \ourDS impact on image redundancy level:} In this analysis, we assess the extent of image-redundancy in \ourDS dataset, generated without human labor, in comparison to the manually created CIRR dataset. To this end, we plot the influence of \ourDS on non-redundant samples, according to the protocol discussed in \cref{sec:modality_redundancy}, and show it in \cref{fig:k-filter}. Note that the performance of the CLIP-based retriever (blue line) vanishes at $V_{50}$, since by construction, $V_{50}$ contains only queries for which CLIP failed to retrieve the target within its top 50 results. A similar trend is observed with \our-(Text-Only) (green line), showing performance degradation with increased $n$, as it also relies solely on query-text. While \our that was trained on \cirr (orange line) shows better performance, it still suffers some degradation as $n$ grows, implying that some bias towards modality redundancies might still exist. However, when \our is trained or pre-trained on \ourDS dataset (pink and brown lines, respectively), it achieves the best performance, which is roughly constant regardless of $n$. Thus, \ourDS appears to be effective at removing bias towards redundancies, and \our pre-trained on it is better suited for datasets with high compositionality.


Finally, in \Cref{tab:lasco_val}, we benchmark on \ourDS. To this end, we apply the two \our variants (Text-Only, Image-Only) that result in poor performance, implying the necessity of both modalities in this dataset. We further test LF-CLIP \cite{cclip} trained on \ourDS, and observe its significant drop in performance (compared to \cirr), implying that \ourDS dataset introduces a higher true \coir challenge. Finally, \our performs best also here, raising \eg, Recall@1 from 4.01\% (by prior LF-CLIP) to 7.08\%, and Recall@50 from 32.08\% to 50.25\%. 
%This experiment implies that the impact of early fusion is more significant than simply replacing CLIP with BLIP.
\input{tables/lasco}


