\section{\ourfull}
\label{sec:method}
Here we introduce a new strong baseline for \coir that leverages pre-trained BLIP components with early fusion, named
{\it \ourfull (\our)}.
%As depicted in \Cref{fig:arc}, the core idea is to use visual-lingual cross attention between the visual tokens of the query image and the query text in order to \emph{shift} the latent embedding of the query image towards the embedding of potential targets in the corpus.

\subsection{\our Architecture}
\label{sec:method_our}
The \our architecture, depicted in \Cref{fig:arc}, consists of two transformer components \cite{vaswani2017attention}. The first is a shift-encoder, based on an image-grounded text encoder, previously introduced in \cite{BLIP}. It is a BERT~\cite{devlin2019bert} encoder with additional intermediate cross-attention layers, to model vision-language interactions. The second component is a ViT \cite{DosovitskiyB0WZ21} encoder. ViT divides an input image into patches and encodes them as a sequence of \emph{image tokens}.
%, with an additional $[\mathit{CLS}]$ token to represent the global image feature. Following this concept, we refer to these encodings as image-tokens. 
The image tokens are then fed into cross-attention layers, allowing interaction between the lingual and visual branches. The output, a bi-modality conditioned sequence (text on image and image on text), is then pooled to a single vector and projected to a 256D latent space. \our allows {\it early fusion} between modalities, in contrast to previous {\it late fusion} methods \cite{cclip, tirg, Kim_Yu_Kim_Kim_2021_dcnet, ARTEMIS} or methods that take a middle way \cite{cirr, FashionVLP, VAL_IR, HosseinzadehW20}, as discussed in \Cref{sec:related_work}.

{\bf Utilizing Vision-Language Pre-training:} We initialize our model's weights using BLIP  \cite{BLIP} pre-trained weights, as follows:
Our shift-encoder's Self-Attention, Cross-Attention and Feed-Forward layers
are initialized with the corresponding layers of BLIP's image-grounded encoder. Our final projection layer is initialized with the final projection of BLIP's text-encoder. Finally, we initialize our ViT component with BLIP's image encoder. Our model is end-to-end trainable (see \Cref{fig:arc}).
%-------------------------
\subsection{Adding a Reverse Objective}
\label{sec:method_reverse_queries}
A common training approach for image retrieval tasks uses an objective of contrastive loss with the target image as positive \eg, \cite{FashionVLP, cclip, cirr, li2020oscar}.
% \cite{FashionVLP, cclip, cirr, li2020oscar, tirg, Align_before_use, fashioniq, Arithmetic_multimodal_IR, compo_image_text_query_ir, ALIGN}
Here, we propose an additional \emph{reverse} objective, where the goal is to retrieve the query image given the transition-text and the target image. One can view the reverse objective as flipping the shift vector of the original query (in the latent space) to point in the opposite direction, from the target image to the embedding of the query image.
Our reverse objective further suggests an additional task and can be viewed as a valid augmentation, effectively enlarging the dataset. We therefore train our model jointly with the reverse objective (see \Cref{fig:arc}). Namely, given a triplet $(Q_i, Q_t, T_i)$ of query-image, query-text and target-image (respectively), our model objective $M$ requires: $M(Q_i, Q_t)=T_i$ (standard \coir task), while simultaneously enforcing $M(T_i, [REV];Q_t)=Q_i$, where $[REV]$ is a special token provided to the model.
%One can interperate this objective is trying to predict which image was ``transitioned'' to this target image, by the query-text.
Although the reverse task is not one-to-one (multiple queries may be suitable), this objective has proven to be beneficial in practice.
% (see \cref{sec:ablations}).
% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1\linewidth]{figures/Train-REV.drawio.pdf}
%   \caption{An Input/Output diagram of the COIR and R-COIR tasks. The new query input to the R-COIR task composed of: 1) the original target-image 2) the original query-text with an added special token $[REV]$. Intuitively speaking: the image that should be found is the original image that was "shifted`` by the given text, to the current (image) result. \mnote{To supp. material?}}
%   \label{fig:train_rev}
% \end{figure}

\subsection{Retrieval Approach}
 We follow the most common approach for image retrieval: searching for matches in an embedding space shared by queries and targets (see \Cref{fig:arc}). First, corpus images (potential targets) are encoded by a ViT \cite{DosovitskiyB0WZ21} encoder, resulting in a single feature vector, representing each image. Then, a given query (composed of image and text) is projected by \our to  the shared embedding space. Finally, the target candidates are ranked by cosine-similarity distance w.r.t the shifted query embedding. By using a relatively small embedding space dimension of 256D, compared to 640D in the previous SoTA \cite{cclip}, the retrieval is sped up by $\times$2.5.
 
 % \subsection{Implementation Details}
 % We set an {\it AdamW} optimizer, initializing learning rate by $5\times 10^{-5}$ with a exponential decay rate of $0.93$ to $1\times 10^{-6}$.
 % We train \our on \cirr with a batch size of 2048 for 6 epochs. For \fashioniq, we train with a batch size of 1024 for 20 epochs (further ablation on the batch size is available in suppl. material).
 % On \ourDS we train with a batch size of 3840 for 10 epochs. We use the Recall@K surrogate loss~\cite{patel2022recall} as the differentiable version of the Recall@K metric. Training time is between 0.5 to 6 minutes per epoch (depending on the dataset size) on four {\it NVIDIA-A100} nodes.