% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{CJK}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions}%\thanks{Supported by organization x.}
%
\titlerunning{Eval. of ChatGPT as a QA System for Ans. Complex Qs.}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%\orcidID{0000-1111-2222-3333}
\author{Yiming Tan \inst{1} \and Dehai Min \inst{2} \and
Yu Li\inst{2} \and Wenbo Li\inst{3}
 \and Nan Hu\inst{2} \and Yongrui Chen\inst{2} 
 \and  Guilin Qi \inst{2}}
%
\authorrunning{Y. Tan et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{School of Cyber Science and Engineering, Southeast University, Nanjing, China \and
School of Computer Science and Engineering, Southeast University, Nanjing, China \and School of Computer Science and Technology, Anhui Unviersity, Hefei, China
\email{\{tt\underline{ }yymm,zhishanq,yuli\underline{ }11,nanhu,yrchen,gqi\}@seu.edu.cn},  \email{wenboli@stu.ahu.edu.cn} \\
 }
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
ChatGPT is a powerful large language model (LLM) that has made remarkable progress in natural language understanding. Nevertheless, the performance and limitations of the model still need to be extensively evaluated. As ChatGPT covers resources such as Wikipedia and supports natural language question answering, it has garnered attention as a potential replacement for traditional knowledge based question answering (KBQA) models. Complex question answering is a challenge task of KBQA, which comprehensively tests the ability of models in semantic parsing and reasoning. To assess the performance of ChatGPT as a question answering system (QAS) using its own knowledge, we present a framework that evaluates its ability to answer complex questions. Our approach involves categorizing the potential features of complex questions and describing each test question with multiple labels to identify combinatorial reasoning. Following the black-box testing specifications of CheckList proposed by \cite{ribeiro2020beyond}, we develop an evaluation method to measure the functionality and reliability of ChatGPT in reasoning for answering complex questions. We use the proposed framework to evaluate the performance of ChatGPT in question answering on 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual datasets, with a total of approximately 190,000 test cases. We compare the evaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common long-term problems in LLMs. The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.

\keywords{large language model \and Complex question answering \and Knowledge base \and ChatGPT \and Evaluation \and Black-box testing.}
\end{abstract}
 
\section{Introduction}
ChatGPT has demonstrated impressive natural language understanding abilities in human-machine interaction testing. As a result, researchers are interested in understanding the performance and limitations of large language models like ChatGPT on existing NLP tasks. 
The study conducted by \cite{petroni2019language} demonstrates that language model can be regarded as Knowledge Base (KB) to support downstream tasks. As a large language model, ChatGPT demonstrates powerful question answering abilities by leveraging its own knowledge. Thus, whether ChatGPT can replace traditional KBQA models has become a popular topic, given its extensive coverage of Wikipedia as training data and its impressive natural language understanding \cite{radford2019language}.


In various types of KBQA tasks, complex question answering (KB-based CQA) is a challenging task that requires question answering models to have the ability of compositional reasoning to answer questions that require multi-hop reasoning, attribute comparison, set operations, and other complex reasoning. 


We believe that evaluating ChatGPT's ability to answer complex questions using its own knowledge can help us answer the question of whether ChatGPT can replace traditional KBQA models or not. This evaluation assesses ChatGPT in two aspects: 1) analyzing its semantic parsing ability for complex questions, and 2) testing its reliability in answering questions through compositional reasoning processes that use its own knowledge.

After a careful review of ChatGPT related papers, we have seen a wide range of evaluation works including \cite{zhong2023can,kocon2023chatgpt,chen2023robust,zhuo2023exploring,huang2023chatgpt,omar2023chatgpt,wang2023can,wang2023cross,qin2023chatgpt,bang2023multitask}, which have summarized many interesting and valuable findings on the performance of LLM in question answering tasks. According to \cite{omar2023chatgpt}, who manually evaluated around 200 test cases, ChatGPT has lower stability compared to traditional KBQA models.  Additionally, \cite{bang2023multitask} found that ChatGPT is a "lazy reasoner" that suffers more with induction after analyzing 30 samples. Existing works often rely on a combination of small-scale sampling tests and manual evaluations to complete performance evaluations of ChatGPT due to API restrictions and the difficulty in evaluating generative answers through Exact Match (EM). As a result, coarse-grained and empirical discoveries are often obtained instead of quantifiable outcomes. Thus, a further validation is needed to ensure the generalizability of these conclusions.

In this paper, we conduct a thorough evaluation of ChatGPT on CQA using its own knowledge as a KB and compare its strengths and limitations with other similar LLMs and existing KBQA models. Our evaluation framework consists of two main steps: 
Firstly, inspired by the scenario-driven evaluation strategy of HELM \cite{liang2022holistic}, we design a feature-driven multi-label annotation method to label the answer-types, reasoning operations, and language involved in the test questions. 
These labels not only help us analyze ChatGPT's reasoning abilities one by one, but their combination can also help us discover many potential QA scenarios that ChatGPT is good at or not good at.
Then, following the CheckList \cite{ribeiro2020beyond} testing specification, the testing objectives are divided into three parts: Minimal Functionality Test (MFT), Invariance Test (INV) \cite{segura2016survey}, and Directional Expectation Test (DIR). The first one reflects the accuracy of the model's execution of various reasoning tasks, while the second and third one reflect the reliability of reasoning. To obtain more analyzable results in the INV and DIR, we adopted the Chain-of-Thought (CoT) \cite{weichain} approach to design prompt templates for building additional test cases.

To establish such an evaluation framework, two major challenges need to be addressed. 
The first is the unified annotation of multi-source datasets. Due to differences in release times, existing KB-based CQA datasets use different forms of answer, providing only question-answer pairs or additional inference paths or SPARQL, etc. To achieve unified annotation, we chose KBQA datasets with SPARQL as the resource for this work, using the SPARQL keywords to identify potential reasoning operations needed to answer questions. Using the syntactic structure of the question text and question words, we achieve automatic identification of answer types, and language tags are directly provided by multi-language datasets. 
Secondly, the task involves matching generative answers with answer phrases. ChatGPT typically generates an answer in the form of a text containing the relevant information, while the dataset provides specific answer phrases. The gap between the two makes it difficult to directly achieve accuracy based traditional Exact Match (EM). Therefore, we employed a phrase-based answer matching strategy. Utilizing the constituent tree \cite{he2021stem}, we process the output of ChatGPT into a set of noun phrases, and then obtain a multilingual set of aliases for the given answer(s) through Wikidata. Additionally, we also set a threshold for phrase vector similarity. When there was no exact match between the sets of phrases, but there are pairs of phrases with similarity higher than the threshold, we supplemented the final judgment of the samples with human evaluation.

Finally, we collect 6 English and 2 multilingual datasets as the test data for this evaluation experiment \footnote{Due to API limitations, our experimental results are based on the version of ChatGPT between February 7th, 2023 and March 5th, 2023. 
We sampled a subset of 500 questions from each dataset to measure performance differences between different versions of ChatGPT. After comparison, the March version of ChatGPT showed an absolute accuracy change in the range of 1-4\% compared to the February version.
}, with a scale of about 190,000 questions, including about 12,000 multilingual questions covering 13 languages. The LLMs used as comparisons include GPT-3 \cite{brown2020language}, GPT-3.5 \cite{ouyang2022training}, and FLAN-T5 \cite{chung2022scaling}. We also introduce the current state-of-the-art of relevant datasets as comparisons to supplement the evaluation of ChatGPT's unsupervised performance on relevant tasks.

Our key findings and insights are summarized as follows:

\begin{itemize}
    \item
        In monolingual question answering tests, ChatGPT performed better than comparable models for the most results, but its performance is not the best when answering questions with numerical or time-based answers. Additionally, when the questions involved multi-hop or star-shaped factual reasoning, its performance is weaker than that of GPT-3.5 v3. In multilingual tests, ChatGPT demonstrated stronger QA performance for answering low-resource language questions.

\end{itemize}

\begin{itemize}
    \item
        In CheckList testing, we found that ChatGPT has some limitations in answering complex questions, including: 1. Results from the MTF show that it is not proficient at answering questions that involve only one type of reasoning. 2. Results from the INV indicate that, compared to traditional KBQA models, the output of ChatGPT is less stable for similar/nearly identical inputs. 3. The DIR shows that ChatGPT does not always provide positive feedback for correct prompts. When facing modified test samples, the changes in its output are not always in line with our expectations.

\end{itemize}

\begin{itemize}
    \item
        The use of a general CoT (Chain-of-Thought) prompting to guide ChatGPT in progressively answering questions is beneficial, with particular efficacy in enhancing the resolution of questions that require counting for obtaining answers.
\end{itemize}

\section{Background and Related Work}
\subsection{large language models and Prompting}
Since the pioneering work of \cite{petroni2019language,jiang2020can,brown2020language}, which demonstrated that language models could perform various downstream NLP tasks with text instructions, there has been a significant amount of research on prompt learning and LLM. On the one hand, improved prompting can enable the information contained in LLM to be more accurately applied to the target task, and early representative works include \cite{reynolds2021prompt,qin2021learning}, which redefine downstream tasks through prompts. At this stage, language models also have clear distinctions in task adaptation, where GPT-3\cite{Rae2021scaling}, GPT-3.5, and FLAN-T5 mainly focus on generation tasks, while BERT \cite{kenton2019bert} is more used for natural language understanding. With the emergence of more powerful LLMs, their significantly enhanced natural language interaction capabilities make prompt design no longer need to be task-centric, but rather guide LLMs to generate intermediate reasoning steps through the establishment of a Chain-of-Thought.
On the other hand, the effectiveness of Prompt learning also depends on the natural language understanding ability of LLM. A lot of work has been done to improve LLM, including Gopher \cite{Rae2021scaling} and PaLM \cite{chowdhery2022palm}, which aim to extend LLM. Additionally, works such as Sanh, Mishra, and Ouyang have trained LLM through supervision and human feedback.
Clearly, ChatGPT has received the most attention, as it was built through reinforcement learning based on human feedback from the GPT-3.5 series of models, demonstrating impressive abilities. Specifically, it exhibits high-quality input responses, the ability to self-correct through dialogue, and the ability to reject inappropriate questions.



\subsection{Evaluation of large language model}
While LLM has demonstrated outstanding natural language understanding and generation capabilities, it is still necessary to further research its strengths, limitations, and potential risks in order to fully understand its advantages. 
Recently, many works aimed at evaluating LLMs have been proposed, which use existing NLP datasets to construct large-scale benchmarks, including BIG-Bench \cite{srivastava2022beyond}, AI LM Harness \cite{gao2021framework}, and SuperGLUE \cite{wang2019superglue} . Furthermore, HELM \cite{liang2022holistic} has been proposed to comprehensively evaluate the LLMs' performance in various task scenarios. Inspired by this idea, in this work, we establish a feature-driven evaluation system to comprehensively evaluate the LLMs' problem understanding and answer generation abilities when facing various complex problem features. Other works have also evaluated ChatGPT's specific abilities, such as mathematical ability \cite{frieder2023mathematical} and reasoning ability \cite{qin2023chatgpt}, through case analysis based on human input.


\subsection{Black-box Testing of NLP Model}
%CheckList
The high cost of training large language models (LLMs) makes white-box testing infeasible. Therefore, almost all evaluation work currently focuses on black-box testing of LLMs. Black-box testing for NLP models is not a new task, and there are many valuable works that can be used as a reference, such as the methods used by \cite{belinkov2019analysis,rychalska2019models} for evaluating robustness, the methods used by \cite{wu2019errudite} and Iyyer for adversarial changes, and the work on attention and interpretability proposed by \cite{wang2019superglue}. The most comprehensive approach currently available is the CheckList protocol proposed by \cite{ribeiro2020beyond}, which categorizes evaluation targets into three parts: Minimum Functionality Test (MFT), Invariance Test (INV), and Directional Expectation Test (DIR). MFT examines the model's basic functionality, INV examines whether the model can maintain functional correctness when non-answer-affecting information is added to the input, and DIR examines whether the model can output the expected result when the input is modified. In this work, we followed this evaluation plan and used CoT prompting to generate test cases for INV and DIR.


\begin{figure}[t]
\includegraphics[width=\textwidth]{Framework.png}
\caption{Overview of Evaluation Framework.} \label{fig1}
\end{figure}

\begin{figure}[t]
\includegraphics[width=\textwidth]{label_statistic.png}
\caption{The distribution of feature labels in the collect KB-based CQA datasets} \label{fig2}
\end{figure}

\section{Evaluation Framework of large language model}
As mentioned in Section 1, our evaluation framework consists of two stages. The first stage aims to describe the test questions by utilizing multi-labels to include question types, combinatorial reasoning types, and language features. The second stage evaluates LLM's QA functionality, robustness, and controllability based on the CheckList specification for each label. The process is illustrated in Figure \ref{fig1}. The following sections will provide a detailed explanation of the design of these stages.

% 
\subsection{Feature-driven multi-label question labeling}
Due to the fact that existing datasets often use different labels to identify answer types or reasoning types, etc., in order to conduct unified analysis in evaluations, we need to standardize the labeling of these feature types.
We design three categories of labels, including "Answer type", "Reasoning type" and "Language type" to describe the characteristics contained within a complex question. These features reflect the type of topic mentioned in the questions, the means of obtaining an answer, and the linguistic form of the question. Typically, these features correspond to sub-functions of a QA system. 
Each question generally contains only one "Answer Type" label. Based on the type definition of facts using named entity recognition (NER), the classification of question types based on English question words, and the induction of answer types from existing KBQA datasets \cite{dubey2019lc,longpre2021mkqa,cao2022kqa}, we set the answer types to the following eight types: Date/Time(DATE), Place (LOC), Person (PER), Reason (WHY), Yes/No (Boolean), Other Fact (MISC), Number (NUM), or Unable to Answer (UNA). Based on the SPARQL query provided by KBQA datasets, we also induced eight "types of reasoning" labels including Set Operation (SetOperation), Conditional Filtering (CondFilter), Counting (COUNT),  Extremum/Sorting (Comparative), Single-hop Reasoning (Single-hop), Multi-hop Reasoning (Multi-hop), and Reasoning on facts with a Star-shape (Star-shape) .
The "Language type" labels reflect the language used to write the question, Figure \ref{fig2} presents the label distribution of the data collected in this paper.


\begin{figure}[t]
\includegraphics[width=\textwidth]{matching.png}
\caption{The answer matching strategy via phrase extraction and alias collection.} \label{fig3}
\end{figure}


\subsection{Metric Methodology}
\subsubsection{Answer matching strategy}
There are generally two strategies for evaluating the output of KBQA (Knowledge-Based Question Answering) systems: SPARQL matching and answer matching. However, ChatGPT has difficulties in generating SPARQL queries with unified entity and relation IDs, making SPARQL matching difficult to automate. Therefore, in the QA evaluation part of our main experiment, we adopt an answer matching strategy. As a supplement, we set up test cases with SPARQL output in the DIR section to manually evaluate ChatGPT's ability to recognize the reasoning operations contained in the questions.

Unlike existing KBQA models, the output of ChatGPT in the QA scenario typically consists of a piece of text containing the answers, which makes it difficult to directly match the model's precision with the answers provided by the dataset. 
Due to the small scale of the sampled test set, existing evaluation works for ChatGPT generally rely on manual assessment to calculate performance. We need to establish a mostly automated metric approach for QA output of ChatGPT or other LLMs.
We adopted a naive approach of enhancing answer matching generalization by expanding the matching scope. Specifically, this approach includes two main steps as shown in Figure \ref{fig3}: 

1) By utilizing the subtree labels provided by the constituent tree, all noun phrases within the textual answer can be extracted. As illustrated in Figure \ref{fig3}, we obtain a list of noun phrases in ascending order of granularity (ranging from individual words to noun phrases, and even short sentences).

2) By utilizing Wikidata and WordNet, we have gathered additional answer list for the golden answer, including multilingual aliases and synonyms.

The exact matching between the noun phrase list and the answer list significantly improves the generalization of answer matching. For the samples that not achieve a match, we set a threshold based on the cosine similarity between phrase vectors to obtain potential matches. The parts that exceed this threshold are then manually judged for correctness. For QA with Answer Type of "DATE", "Boolean" and "NUM", we have established special judgment programs based on the characteristics of their golden answers.

As our matric method is essentially still an exact-match, we use Accuracy (Acc) as performance comparison metrics between models in the experimental part.


\begin{figure}[t]
\includegraphics[width=\textwidth]{INV_DIR.png}
\caption{Test cases design for INV and DIR.} \label{fig4}
\end{figure}


\subsubsection{Prompting-based CheckList strategy}
Following the idea of CheckList, we have also evaluated ChatGPT with three distinct  objectives: (1) To evaluate the ability of the LLMs to handle each feature in KB-based CQA scenarios through Minimal Functionality Test (MFT);
(2) To evaluate the robustness of LLM's ability to handle various featrues in KB-based CQA scenarios through Invariance Test (INV)
% (2) To evaluate the robustness of LLM's various functionalities through Invariance tests (INV); 
(3) To evaluate whether a LLM can produce outputs that meet human expectations for modified inputs through Directional Expectation Test (DIR), that is to say: the controllability of ChatGPT. The specific procedures of INV and DIR are presented as follows and the Figure \ref{fig4} presents the instances:

\paragraph{Minimum Functionality Test (MFT)} is a set of simple examples, along with their respective labels, that are designed to verify a particular behavior within a given capability. In this work, we use labels to select samples containing only a single type of reasoning and form them into MFT cases to examine ChatGPT's performance in executing basic functions such as "multi-hop reasoning", "counting", "sorting", etc. Table \ref{tab5} provides examples for the test cases.
% MFTs are particularly useful in detecting when models use shortcuts to handle complex inputs without fully mastering the corresponding capability. 
% In our work each label is considered as a corresponding function on the testing model, and the evaluation results are accomplished through label filtering.

\paragraph{Invariance Test (INV)} refers to applying slight perturbations to the model's inputs, while expecting the correctness of the model's output to remain unchanged. Various perturbation functions are required for different capabilities, such as altering location names for the Named Entity Recognition (NER) capability or introducing typos to evaluate the Robustness capability. In this paper, we have designed two methods to generate INV cases: the first is to randomly introduce spelling errors into sentences, and the second is to generate semantically equivalent paraphrases for the questions in the target examples. Subsequently, we evaluated the invariance of ChatGPT by examining the consistency of the resulting outputs when subjected to the three aforementioned inputs (the original test case, its version with added spelling errors, and its paraphrase).

%在这篇文章中，我们采用了两种方式生成INV测试用例，包括在句子中随机添加一些拼写错误，以及为目标测试用例生成一个复述。接着，我们通过观察三者作为输入所得结果的一致性来评估ChatGPT的不变性

\paragraph{Directional Expectation Test (DIR)} is a similar approach to INV, with the difference being that the label is expected to change in a certain way. In this study, we explore three modes for creating DIR cases: Firstly, we replaced reasoning-related phrases in questions, requiring the model to produce answers with a SPARQL query to observe whether the logical operations in ChatGPT's output corresponded to our modifications. Secondly, we add prompts that contained answer types to the inputs to examine whether ChatGPT could control the output answer types based on the prompts. Thirdly, taking inspiration from CoT, we use a general multi-round prompts to rewrite the test cases that allow ChatGPT to obtain answers through a "step-by-step" process to observe ChatGPT's sensitivity to CoT prompts on different types of questions.

\begin{table}\centering
\caption{The Statistical of collected KB-based CQA datasets, "Col. Size" represents the dataset we collect in our experiments.}\label{tab1}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.8mm}{
    \begin{tabular}{lllc}
    \hline
    Datasets   & Size & Col. Size & Lang    \\
    \hline
    KQApro     & 117,970 & 106,173 & EN  \\%& 16,960 & 1,209 (363 pred, 846 attr)  \\
    LC-quad2.0 & 26,975 & 26,975 & EN  \\
    WQSP       & 4737 & 4,700 & EN  \\
    CWQ        & 31,158 & 31,158 & EN   \\
    GrailQA    & 64,331 & 6,763 & EN   \\
    GraphQuestions    & 4,776 & 4,776 & EN    \\
    \hline
    QALD-9     & 6,045 & 6,045 & Mul   \\
    MKQA       & 260,000 & 6,144 & Mul   \\
    \hline
    Total Collected      & & 194,782 &   \\
    \hline
    \end{tabular}
    }
\end{table}


\section{Experiments}
\subsection{Datasets}
In this paper, we regard ChatGPT and other LLMs for comparison as unsupervised question answering models equipped with a knowledge base. The model takes natural language questions as input and produces textual paragraphs containing answers derived from its own knowledge coverage. 
Given that the training data for ChatGPT (and comparison LLMs) extensively covers Wikipedia, we adopt evaluating models using popular large-scale, multi-lingual open-domain complex question answering datasets that are related to Wikipedia.
% Given that the training data of the ChatGPT (LLM) covers Wikipedia extensively, we have aopted to evaluate models using popular large-scale, multi-lingual open-domain complex question answering datasets that is related to Wikipedia. 
Specifically, we collect 6 monolingual datasets and 2 multilingual datasets for this purpose, as follows:

\subsubsection{Monolingual datasets:}
\paragraph{WebQuestionSP} WebQuestionSP\cite{yih2016value} is a dataset constructed by Microsoft, which builds upon the WebQuestion \cite{berant2013semantic} dataset created by researchers at Stanford University using the Google Suggest API. It enhances the original dataset by annotating each answer with a corresponding SPARQL query statement and removing ambiguous, unclear, or unanswerable questions.

\paragraph{ComplexWebQuestion} The ComplexWebQuestions\cite{talmor2018web} dataset is established based on the WebQuestionsSP dataset. The construction methodology involves expanding the SPARQL statements involved in the WebQuestionsSP dataset's templates to form patterned complex questions, followed by manual rephrasing of the complex questions to generate natural language questions.

\paragraph{GraphQuestions} The GraphQuestions\cite{su2016generating} is a challenging dataset that involves complex logic and utilizes Freebase as its knowledge base. The dataset was constructed by first designing patterns of questions, which correspond to subgraphs in the knowledge base, and then having humans rewrite these patterns into natural language questions.

\paragraph{GrailQA} Generalizable Question Answering (GrailQA)\cite{gu2021beyond} is a large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.

\paragraph{KQApro} The KQApro \cite{cao2022kqa} dataset consists of approximately 120,000 natural language questions, and introduces a compositional and interpretable programming language, KoPL, to annotate the reasoning process of complex questions. KQApro can be used for both KBQA and semantic parsing tasks simultaneously.

\subsubsection{Multilingual datasets:}
\paragraph{QALD-9} QALD-9 \cite{ngomo20189th} is a standrad knowledge-based multilingual question answering dataset consisting of 580 questions, spanning approximately 13 languages. The key challenge of QALD-9 is to translate the users' information needs into a form such that they
can be evaluated using standard Semantic Web query processing and inferencing techniques.

\paragraph{MKQA} The Multilingual Knowledge Questions and Answers (MKQA) \cite{longpre2021mkqa} dataset is an open-domain question answering evaluation set comprising 10,000 QA pairs aligned across 26 typologically diverse languages (260,000 QA pairs in total).

Due to OpenAI's traffic limitations, we sampled larger datasets, such as MKQA (sampled by answer type) and GrailQA (test set only). The collection size of each dataset are summarized in Table \ref{tab1}.

\subsection{Comparative large language models}
\paragraph{GPT-3 davinci-1}  GPT-3 \cite{brown2020language} is a family of autoregressive Transformer language models trained on 570GB of Internet text (WebText), of which we evaluate GPT-3 davinci v1 (175B). The release date and model size for these models are based on \cite{brown2020language}.

\paragraph{GPT-3.5 davinci-2/davinci-3} GPT-3.5 davinci-2 (GPT-3.5 v2) is trained using both text and code, enabling it to develop strong reasoning capabilities and enhance its zero-shot learning ability. On the other hand, GPT-3.5 text-davinci-3 (GPT-3.5 v3)is a variant that employs instruction tuning with reinforcement learning from human feedback, similar to ChatGPT, but with potentially stronger contextual learning ability.

\paragraph{ChatGPT} ChatGPT is a variant of GPT-3.5, similar to the GPT-3.5 davinci-3 model (GPT-3.5 v3), which is fine-tuned through instruction tuning with reinforcement learning from human feedback (RLHF) based on human feedback. However, unlike GPT-3.5 davinci-003, ChatGPT appears to be less strongly influenced by contextual factors \cite{fu2022does}.


\paragraph{FLAN-T5} FLAN-T5 (Text-to-Text Transfer Transformer, 11B \cite{chung2022scaling})is an encoder-decoder Transformer language model that is trained on a filtered variant of CommonCrawl (C4) \cite{raffel2020exploring}. The release date and model size for this model are also based on \cite{raffel2020exploring}.


\begin{table}[t]\centering
\caption{Main result of Evaluation, We compare the QA Accuracy of ChatGPT with similar LLMs, including T5, GPT-3, and GPT-3.5 variants, and assessed their deviation from the current best fine-tuned (FT) and zero-shot (ZS) models.}\label{tab2}. 
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.4mm}{
    \begin{tabular}{l|l|l|cccc|c}
    \hline
    Datasets   & SOTA(FT) & SOTA(ZS) & FLAN-T5& GPT-3 & GPT-3.5v2 & GPT-3.5v3 & ChatGPT \\
    \hline
    \multicolumn{7}{c}{Monolingual} \\
    \hline
    KQApro     &  \textbf{93.85} \cite{perevalov2022knowledge} & 94.20 \cite{nie2022graphq}  & 37.27  & 38.28  & 38.01  & 40.35  & 47.93 \\
    LC-quad2.0 & \textbf{33.10} \cite{pramanik2021uniqorn} & - & 30.14  & 33.04  & 33.77  & 39.04  & 42.76 \\
    WQSP       & 73.10 \cite{hu2022logical} & - & 59.87  & 67.68  & 72.34  & 79.60  & \textbf{83.70} \\
    CWQ        & \textbf{72.20} \cite{hu2022logical} &  -  & 46.69  & 51.77  & 53.96  & 57.54  & 64.02  \\
    GrailQA    & \textbf{76.31} \footnote{https://dki-lab.github.io/GrailQA/} & 62.98 \cite{ye2022rng} & 29.02  & 27.58  & 30.50  & 35.43  & 46.77 \\
    GraphQuestions    & 41.30 &  & 32.27  & 38.32  & 40.85  & 47.95  & \textbf{53.10} \\
    \hline
    \multicolumn{7}{c}{Multilingual}\\
    \hline
    QALD-9   & \textbf{80.40} \cite{purkayastha2022deep} & - & 30.17  & 38.54  & 44.95  & 46.19  & 45.71 \\
    MKQA     & \textbf{46.00} \cite{longpre2021mkqa} & - & 20.17 & 26.97  & 30.14  & 39.05  &  44.30\\
    \hline
    \end{tabular}
    }
\end{table}

\begin{table}[h] \centering
\caption{Accuracy comparison based on Answer Types (AnsType) and Reasoning Types (RsgType) }\label{tab3}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|cccc|c}
    \hline
    MF &  FLAN-T5& GPT-3 & GPT-3.5v2 & GPT-3.5v3 & ChatGPT \\
    \hline
    \multicolumn{6}{c}{AnsType} \\
    \hline
    MISC    & 35.67  & 40.79  & 42.35  & 46.42  & \textbf{51.02} \\
    PER     & 30.84  & 37.53  & 41.36  & 45.10  & \textbf{48.65}\\
    LOC     & 52.91  & 56.92  & 58.93  & 62.71  & \textbf{63.55} \\
    ORG     & 41.62  & 50.01  & 50.58  & 54.62  & \textbf{61.18} \\
    DATE    & 24.81  & 37.07  & 36.15  & \textbf{42.54}  & 36.92 \\
    Boolean & 62.43  & 39.96  & 42.56  & 53.23 & \textbf{62.92} \\
    NUM     & 16.08  & 19.66  & 21.01  & 20.31  & \textbf{30.70} \\
    WHY     & 27.69  & 32.31  & 27.69  & \textbf{49.23}  & 40.00 \\
    UNA     &  - &  - &  - & -  & - \\
    \hline
    \multicolumn{6}{c}{RsgType} \\
    \hline
    SetOperation    & 38.86  & 40.31  & 43.03  & 47.87  & \textbf{70.00} \\
    Filtering   & 45.01  & 49.06 & 51.24  & 55.43  & \textbf{63.40} \\
    Counting    & 10.68  & 12.66  & 14.76 & 12.41  & \textbf{28.41} \\
    Comparison  & 44.88  & 45.63  & 46.85 & 47.49  & \textbf{74.74} \\
    Single-hop  & 41.00  & 38.72  & 42.54 & 49.22  & \textbf{54.00} \\
    Multi-hop   & 35.68  & 41.09  & 42.98 & \textbf{47.06}  & 44.88 \\
    Star-shape  & 37.23  & 42.28  & 43.96 & \textbf{48.17}  & 47.43 \\
    \hline
    \end{tabular}
    }
\end{table}

\subsection{Main Results}
The main experimental results are presented in Table 2. The results demonstrate that ChatGPT significantly outperforms other participating LLMs on 7 of the test datasets, and achieves state-of-the-art (SOTA) performance on the WQSP and GraphQuestions datasets, surpassing the current fine-tuning approach. However, ChatGPT's performance is still significantly inferior to the state-of-the-art (SOTA) of traditional models on other datasets, especially on more entity-rich datasets such as KQApro, LC-quad2.0, and GrailQA.
Subsequently, we summarize the experimental results from the perspectives of answer types and reasoning types, and observe some details in Table \ref{tab3} and Table \ref{tab4} shows the multilingual results: 

% 这里需要引入各数据集覆盖的实体类型范围作为参照更有说服力

\paragraph{From the perspective of answer types:} 
As shown in Table \ref{tab3}, based on the test results divided by answer types, it can be found that all LLMs participating in the test are not good at answering questions with numerical (NUM), causal (WHY), and temporal answers (DATE). In terms of performance comparison, ChatGPT outperforms the comparison models in most question types, but lags behind GPT-3.5 v3 in answering questions with answer types of "DATE" and "WHY". 


\paragraph{From the perspective of reasoning types:}
Initially, all LLMs involved in the testing stumbled upon the COUNT-type question, which is consistent with their performance on NUM answer type, as mentioned upon. The progress of ChatGPT in certain types of reasoning is nearly transformative, as evidenced by its significantly superior accuracy (Acc) in problems involving set operations and comparisons, with a margin of over 20\% when compared to other competitors. This finding corroborates Fu's viewpoint \cite{fu2022does}, as the introduction of code training significantly improved ChatGPT's logical reasoning ability. But, on the contrary, ChatGPT lags behind GPT-3.5 v3 in terms of chain-like multi-hop reasoning or star-shape facts reasoning in actuality. Based on the findings presented in Table 3, it can be inferred that the cause of this phenomenon lies in ChatGPT's inadequate ability to distinguish between the confused entities it has learned.


\paragraph{From the perspective of the linguistic form of questions:} 
The experimental results on the multilingual test set in Table \ref{tab4} show that ChatGPT outperforms the comparison models in all language tests, especially in low-resource languages (fa,hi\underline{ }IN and ru), where its performance advantage is more significant. However, the low score obtained in the Chinese test has puzzled us, and we cannot determine whether the cause of this situation is due to "insufficient Chinese resources" or "low resource quality."


\begin{table} \centering
\caption{Comparison of question answering accuracy on multilingual test sets.}\label{tab4}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|ccccccc|c}
    \hline
    Languages &  FLAN-T5 & GPT-3 & GPT-3.5v2 & GPT-3.5v3 & ChatGPT \\
    \hline
    en  & 30.29 & 57.53  & 56.99  & 64.16  & \textbf{66.49} \\
    nl  & 20.75 & 50.47 & 54.58 & 60.56 & \textbf{65.05}\\
    de  & 22.40 & 50.54 & 54.48 & 57.17 & \textbf{62.54 }\\
    es  & 21.68 & 48.22 & 55.70 & 58.50  & \textbf{61.87} \\
    fr  & 26.16 & 49.46 & 55.02 & 57.89 & \textbf{62.19} \\
    it  & 24.19 & 47.67 & 52.33 & 58.06  & \textbf{58.96} \\
    ro  & 22.28 & 44.38 & 50.94 & 54.12 & \textbf{59.55} \\
    pt\underline{ }BR & 15.38  & 38.46 & 38.46 & 42.31 & \textbf{50.00} \\
    pt  & 20.58 & 37.70 & 44.26 & 50.27 & \textbf{52.64} \\
    ru  & 7.29 & 20.58 & 29.69 & 21.68 & \textbf{32.24} \\
    hi\underline{ }IN  & 3.61 & 9.93 & 19.13 & 13.54  & \textbf{21.48} \\
    fa & 2.45 & 6.59 & 21.09 & 11.49 & \textbf{22.03} \\
    zh\underline{ }cn & 3.65 & 17.45 & 22.40 & 24.87 & \textbf{33.46} \\
    \hline
    \end{tabular}
    }
\end{table}

\subsection{MFT results}
We select test cases that only contained single inference labels or multi-labels of the same type (such as SetOperation+Comparison, SetOperation+Filtering), and aggregated their results to obtain the MFT, as shown in Table \ref{tab5}. The MFT test results indicate that when a question involves only one type of reasoning operation, ChatGPT's performance decreases and is even worse than the comparison results in Table 3. This suggests that ChatGPT performs better at answering questions with combined reasoning, and is not as good as single reasoning.


\begin{table}\centering
\caption{MFT results}\label{tab5}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|c|p{8cm}}
    \hline
    MF  & Acc  & Example of test case\\
    \hline
    SetOperation    & 60.22 & How many seats does the home stadium of FC Porto have? \\
    Filtering   &  51.39  & What is the elevation above sea level of the U.S. city with postal code 7?  \\
    Counting    &  24.16  & How many keyboard instruments are used by Jim Jonsin  \\
    Comparison  &  31.48  & Is the running time shorter for the Brothers, released in Greece, or The Fall, filmed in Indonesia? \\
    Single-hop  &  44.07  &  What are the four harry potter house names \\
    Multi-hop   &  48.27  & Who is the sister of Claudius whose public office is the consul of the Roman  \\
    Star-shape  &  50.75  &  What is the name of the city with a population different to 8100000 in 2015, and that is the capital of Liguria? \\
    \hline
    \end{tabular}
    }
\end{table}

\begin{table}\centering
\caption{INV results for AnsType}\label{tab6}
\setlength{\tabcolsep}{0.75mm}{
    \begin{tabular}{ccc|lllllllll}
    \hline
      Run 1 & Run 2 & Run 3 & ALL & MISC & PER & LOC & ORG & DATE & Boolean & NUM & WHY  \\
    \hline
      C &  C &  C  & 588 & 149 & 55 & 108 & 55 & 26 & 138 & 9 & 9  \\
      C &  C &  W  & 49  & 10  & 7  & 6   & 5  & 6  & 5   & 0 & 1  \\
      C &  W &  C  & 72  & 11  & 11 & 11  & 12 & 5  & 5   & 2 & 2  \\
      C &  W &  W  & 68  & 14  & 7  & 17  & 3  & 5  & 3   & 2 & 2  \\
      W &  C &  C  & 52  & 3   & 3  & 7   & 3  & 2  & 19  & 1 & 1  \\
      W &  C &  W  & 27  & 4   & 1  & 4   & 2  & 3  & 9   & 1 & 2  \\
      W &  W &  C  & 32  & 3   & 0  & 6   & 3  & 4  & 4   & 2 & 2  \\
      W &  W &  W  & 545 & 89  & 66 & 66  & 27 & 82 & 165 & 16 & 16  \\
    \hline
    \multicolumn{3}{c}{Stability Rate}  & 79.06
 & 83.22 & 80.67 & 77.33 & 74.55 & 81.20 & 72.27 & 82.56 & 75.76  \\
    \hline
    \end{tabular}
    }
\end{table}

% \begin{table}\centering
% \caption{MFT results}\label{tab2}
% % \resizebox{\linewidth}{!}{
% \setlength{\tabcolsep}{1.5mm}{
%     \begin{tabular}{c|cccc|c}
%     \hline
%     MF &  FLAN-T5& GPT-3 & GPT-3.5v2 & GPT-3.5v3 & ChatGPT \\
%     \hline
%     SetOperation    &   &   &   &   &  \\
%     Filtering   &   &  &   &   &  \\
%     Counting    &   &   &  &   &  \\
%     Comparison  &   &   &  &   &  \\
%     Single-hop  &   &   &  &   &  \\
%     Multi-hop   &   &   &  &   &  \\
%     Star-shape  &   &   &  &   &  \\
%     \hline
%     \end{tabular}
%     }
% \end{table}

\subsection{INV results}
Table \ref{tab6} and \ref{tab7} present the stability of ChatGPT across three runs on sampled test cases, with each answer type and each reasoning type evaluated. The results are reported using the following notation: C denotes that the question is answered correctly, while W denotes that the question is not answered correctly or fails to return any useful answer. The judgment process involves manual supervision, and model is deemed to be stable in its corresponding functional category only when the outputs of three rounds of testing are consistent.
Overall, the two tables present similar findings, indicating that ChatGPT exhibits a stability of approximately 79.0\% in complex question answering tasks. As a reference, Omar et. al \cite{omar2023chatgpt} pointed out that the stability of traditional KBQA models is 100\%, this difference will affect the applicable scenarios of ChatGPT as a QA model.


\begin{table}\centering
\caption{INV results for RsgType}\label{tab7}
\setlength{\tabcolsep}{1.1mm}{
    \begin{tabular}{ccc|lllllll}
    \hline
      Run 1 & Run 2 & Run 3 & ALL & SetOp & Filter & Count & Compar &  Multi-hop & StarShape  \\
     
    \hline
      C &  C &  C  & 1,447 & 329 & 354 & 48 & 317 & 193 & 206 \\
      C &  C &  W  & 111 & 17 & 20 & 4  & 16 & 27 & 27 \\
      C &  W &  C  & 181 & 42 & 44 & 5  & 39 & 26 & 26 \\
      C &  W &  W  & 132 & 8 &  15 & 3  & 9  & 28 & 49 \\
      W &  C &  C  & 104 & 13 & 19 & 17 & 5 & 25 & 25 \\
      W &  C &  W  & 52 &  4 & 8  & 9 & 1 & 15 &  15 \\
      W &  W &  C  & 67 & 7 & 14 & 4 & 7 & 17 & 18 \\
      W &  W &  W  & 1,054 & 54 & 149 & 128 & 49 & 331 &  343 \\
    \hline
    \multicolumn{3}{c}{Stability Rate} & 79.44 & 80.97 & 80.74 & 80.73 & 82.62 & 76.83 & 77.43  \\
    \hline
    \end{tabular}
    }
\end{table}

\subsection{DIR results}
As mentioned in Figure 4 above, we have designed three forms of the DIR to respectively examine ChatGPT's abilities in answer type recognition, reasoning, and response to the CoT prompting.
Firstly, with regard to the DIR focused on answer types, we employed a naive prompt to inform ChatGPT of the answer type of the current question, expecting it to utilize this information to restrict the candidate answer range to a specific type. As shown in Table \ref{tab8}, the results indicate that informing the answer type is particularly useful for questions with Boolean and NUM answers. However, overall, our prompt was not effective for most question types and even resulted in more errors in answer type identification.


\begin{table}\centering
\caption{DIR results for AnsType prompting} \label{tab8}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.2mm}{
    \begin{tabular}{c|c|c|c}
    \hline
    AnsType &  Without prompting & With prompting & +/- \\
    \hline
    MISC    & 73.73 & 80.51 & +6.78 \\
    PER     & 61.82 & 58.18 & -3.64 \\
    LOC     & 60.34 & 58.62 & -1.72 \\
    ORG     & 73.21 & 67.86 & -5.35 \\
    DATE    & 54.29 & 45.71 & -8.58 \\
    Boolean & 72.86 & 77.14 & +4.28 \\
    NUM     & 35.71 & 42.86 & +7.15 \\
    WHY     & 66.67 & 63.64 & -3.03 \\
    UNA     & - & - & - \\
    \hline
    \end{tabular}
    }
\end{table}

\begin{table}\centering
\caption{DIR results for RsgType} \label{tab9}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{c|c|c}
    \hline
    RsgType &  Failure rate & Size of Samples\\% & Failures example of test case & Expected \\
    \hline
    SetOperation   &  25.00  & 20\\
    Filtering      &  15.00  & 20\\
    Counting       &  30.00  & 20\\
    Comparison     &  35.00  & 20\\
    \hline
    \end{tabular}
    }
\end{table}

\begin{table}\centering
\caption{DIR results for CoT prompting} \label{tab10}
% \resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.2mm}{
    \begin{tabular}{c|c|c|c}
    \hline
     Types &  Without prompting & With prompting & +/- \\
    \hline
    \multicolumn{4}{c}{AnsType} \\
    \hline
    MISC    & 64.33 & 62.58 & -1.75 \\
    PER     & 53.33 & 48.67 & -4.66 \\
    LOC     & 63.11 & 64.00 & +0.89 \\
    ORG     & 68.18 & 64.55 & -3.63 \\
    DATE    & 31.57 & 30.07 & -1.50 \\
    Boolean & 75.63 & 78.99 & +3.36 \\
    NUM     & 23.64 & 54.26 & +30.62 \\
    WHY     & 39.39 & 45.45 & +6.06 \\
    UNA     & - & - & - \\
    \hline
    \multicolumn{4}{c}{RsgType} \\
    \hline
    SetOperation   & 83.51  & 91.33  & +7.82 \\
    Filtering   & 69.50  & 78.97  & +9.47 \\
    Counting    & 27.52  & 63.30  & +35.78 \\
    Comparison  & 86.00  & 86.45  & +0.45 \\
    Multi-hop   & 43.11  & 41.64  & -1.47 \\
    Star-shape  & 43.44  & 42.03  & -1.41 \\
    \hline
    \end{tabular}
    }
\end{table}

To test ChatGPT's reasoning functionality using DIR, we follow a series of steps. First, we input a pair of (question, SPARQL query) to ChatGPT. We then modify the phrases related to reasoning operations in the question and input the modified question to ChatGPT in the same dialogue turn. Next, we request ChatGPT to return the corresponding SPARQL. Finally, we compare the returned SPARQL with the expected SPARQL.
The matching process of SPARQL is carried out manually, making it difficult to scale up testing. To evaluate its performance, we conducted a testing experiment consisting of twenty sampled questions for each of the four reasoning types: set operation, conditional filtering, counting, and comparison. The evaluation metric was failure rate, which reflects the proportion of output errors in SPARQL.
The results presented in Table \ref{tab9} indicate that ChatGPT generally provides positive feedback on our modifications; however, utilizing this process as an intermediate step for complex question answering tasks can lead to significant error propagation.


CoT has been proven to be an effective method for utilizing LLM to perform downstream tasks. In this study, we established a naive CoT prompt to guide ChatGPT in collecting information and concepts related to complex questions before answering them. We believe that this process bears some resemblance to how humans gather information to answer questions. The results in Table 10 demonstrate that even the most naive CoT can have a positive impact on most types of questions. It is particularly noteworthy that CoT led to an Acc improvement of over 30\% for NUM-type questions, which also manifested in COUNT-type questions where ChatGPT performed poorly.

\section{Conclusion}
This paper presents a large-scale experimental analysis of ChatGPT's performance in answering complex questions using its own knowledge base, as compared to similar models and current state-of-the-art (SOTA) models. The analysis highlights the advantages, limitations, and deficiencies of ChatGPT. Using a checklist test, we provide a detailed analysis of ChatGPT's basic performance, stability, and controllability in dealing with various answer types and reasoning requirements. We believe that these findings will provide valuable insights and references for the development and downstream research of large-scale language models represented by ChatGPT.


% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{splncs-refs}
% \bibliography{mybibliography}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}
\end{document}
