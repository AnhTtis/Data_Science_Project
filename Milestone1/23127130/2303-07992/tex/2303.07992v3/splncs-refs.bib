@article{bang2023multitask,
  title={A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{lyu2023new,
  title={New trends in machine translation using large language models: Case examples with chatgpt},
  author={Lyu, Chenyang and Xu, Jitao and Wang, Longyue},
  journal={arXiv preprint arXiv:2305.01181},
  year={2023}
}

@article{bai2023benchmarking,
  title={Benchmarking Foundation Models with Language-Model-as-an-Examiner},
  author={Bai, Yushi and Ying, Jiahao and Cao, Yixin and Lv, Xin and He, Yuze and Wang, Xiaozhi and Yu, Jifan and Zeng, Kaisheng and Xiao, Yijia and Lyu, Haozhe and others},
  journal={arXiv preprint arXiv:2306.04181},
  year={2023}
}

@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{fu2022does,
  title={How does gpt obtain its ability? tracing emergent abilities of language models to their sources},
  author={Fu, Yao and Peng, Hao and Khot, Tushar},
  journal={Yao Fuâ€™s Notion},
  year={2022}
}
@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{zhong2023can,
  title={Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT},
  author={Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{pramanik2021uniqorn,
  title={UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text},
  author={Pramanik, Soumajit and Alabi, Jesujoba and Saha Roy, Rishiraj and Weikum, Gerhard},
  journal={arXiv e-prints},
  pages={arXiv--2108},
  year={2021}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{kocon2023chatgpt,
  title={ChatGPT: Jack of all trades, master of none},
  author={Koco{\'n}, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szyd{\l}o, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and others},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{chen2023robust,
  title={How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks},
  author={Chen, Xuanting and Ye, Junjie and Zu, Can and Xu, Nuo and Zheng, Rui and Peng, Minlong and Zhou, Jie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}

@article{zhuo2023exploring,
  title={Exploring AI Ethics of ChatGPT: A Diagnostic Analysis},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv e-prints},
  pages={arXiv--2301},
  year={2023}
}

@article{huang2023chatgpt,
  title={Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech},
  author={Huang, Fan and Kwak, Haewoon and An, Jisun},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{wang2023can,
  title={Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?},
  author={Wang, Shuai and Scells, Harrisen and Koopman, Bevan and Zuccon, Guido},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{longpre2021mkqa,
  title={MKQA: A linguistically diverse benchmark for multilingual open domain question answering},
  author={Longpre, Shayne and Lu, Yi and Daiber, Joachim},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1389--1406},
  year={2021},
  publisher={MIT Press}
}

@article{wang2023cross,
  title={Cross-Lingual Summarization via ChatGPT},
  author={Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Li, Zhixu and Qu, Jianfeng and Zhou, Jie},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{chang2023survey,
  title={A Survey on Evaluation of Large Language Models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={arXiv preprint arXiv:2307.03109},
  year={2023}
}

@article{qin2023chatgpt,
  title={Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{zhu2023promptbench,
  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{liang2022holistic,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv e-prints},
  pages={arXiv--2211},
  year={2022}
}

@article{jiang2020can,
  title={How Can We Know What Language Models Know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{Rae2021scaling,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv e-prints},
  pages={arXiv--2112},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{segura2016survey,
  title={A survey on metamorphic testing},
  author={Segura, Sergio and Fraser, Gordon and Sanchez, Ana B and Ruiz-Cort{\'e}s, Antonio},
  journal={IEEE Transactions on software engineering},
  volume={42},
  number={9},
  pages={805--824},
  year={2016},
  publisher={IEEE}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv e-prints},
  pages={arXiv--2204},
  year={2022}
}

@article{gao2021framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and McDonell, Kyle and Muennighoff, Niklas and others},
  journal={Version v0. 0.1. Sept},
  year={2021}
}

@article{belinkov2019analysis,
  title={Analysis methods in neural language processing: A survey},
  author={Belinkov, Yonatan and Glass, James},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={49--72},
  year={2019},
  publisher={MIT Press}
}

@article{omar2023chatgpt,
  title={ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots},
  author={Omar, Reham and Mangukiya, Omij and Kalnis, Panos and Mansour, Essam},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv e-prints},
  pages={arXiv--2203},
  year={2022}
}

@article{frieder2023mathematical,
  title={Mathematical Capabilities of ChatGPT},
  author={Frieder, Simon and Pinchetti, Luca and Griffiths, Ryan-Rhys and Salvatori, Tommaso and Lukasiewicz, Thomas and Petersen, Philipp Christian and Chevalier, Alexis and Berner, Julius},
  journal={arXiv e-prints},
  pages={arXiv--2301},
  year={2023}
}

@article{ngomo20189th,
  title={9th challenge on question answering over linked data (QALD-9)},
  author={Ngomo, Ngonga},
  journal={language},
  volume={7},
  number={1},
  pages={58--64},
  year={2018}
}

@article{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%------------------------------------------------------------------------------------

@inproceedings{ribeiro2020beyond,
  title={Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  booktitle={In Proc. ACL Conf.},
  pages={4902--4912},
  year={2020}
}

@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={In Proc. IJCAI Conf.},
  pages={2463--2473},
  year={2019}
}

@inproceedings{he2021stem,
  title={The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders},
  author={He, Han and Choi, Jinho D},
  booktitle={In Proc. EMNLP Conf.},
  pages={5555--5577},
  year={2021}
}

@inproceedings{weichain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny and others},
  booktitle={Advances in Neural Information Processing Systems}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={In Proc. NAACL-HLT Conf.},
  pages={4171--4186},
  year={2019}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={In Proc. CHI EA Conf. },
  pages={1--7},
  year={2021}
}

@inproceedings{qin2021learning,
  title={Learning How to Ask: Querying LMs with Mixtures of Soft Prompts},
  author={Qin, Guanghui and Eisner, Jason},
  booktitle={In Proc. NAACL-HLT Conf.},
  year={2021}
}

@inproceedings{wang2019superglue,
  title={SuperGLUE: a stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={In Proc. NeurIPS Conf.},
  pages={3266--3280},
  year={2019}
}

@inproceedings{rychalska2019models,
  title={Models in the wild: On corruption robustness of neural nlp systems},
  author={Rychalska, Barbara and Basaj, Dominika and Gosiewska, Alicja and Biecek, Przemys{\l}aw},
  booktitle={In Proc. ICONIP Conf.},
  pages={235--247},
  year={2019},
}

@inproceedings{wu2019errudite,
  title={Errudite: Scalable, reproducible, and testable error analysis},
  author={Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S},
  booktitle={In Proc. ACL Conf.},
  pages={747--763},
  year={2019}
}

@inproceedings{purkayastha2022deep,
  title={A Deep Neural Approach to KGQA via SPARQL Silhouette Generation},
  author={Purkayastha, Sukannya and Dana, Saswati and Garg, Dinesh and Khandelwal, Dinesh and Bhargav, GP Shrivatsa},
  booktitle={In Proc. IJCNN Conf.},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@inproceedings{nie2022graphq,
  title={GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation},
  author={Nie, Lunyiu and Cao, Shulin and Shi, Jiaxin and Sun, Jiuding and Tian, Qi and Hou, Lei and Li, Juanzi and Zhai, Jidong},
  booktitle={In Proc. EMNLP Conf.},
  pages={5848--5865},
  year={2022}
}

@inproceedings{he-choi-2021-stem,
    title = "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders",
    author = "He, Han and Choi, Jinho D.",
    booktitle = {In Proc. EMNLP Conf.},
    year = {2021},
    pages = "5555--5577",
    
}

@inproceedings{perevalov2022knowledge,
  title={Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis},
  author={Perevalov, Aleksandr and Yan, Xi and Kovriguina, Liubov and Jiang, Longquan and Both, Andreas and Usbeck, Ricardo},
  booktitle={In Proc. LREC Conf.},
  pages={2998--3007},
  year={2022}
}

@inproceedings{ye2022rng,
  title={RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering},
  author={Ye, Xi and Yavuz, Semih and Hashimoto, Kazuma and Zhou, Yingbo and Xiong, Caiming},
  booktitle={In Proc. ACL Conf.},
  pages={6032--6043},
  year={2022}
}

@inproceedings{hu2022logical,
  title={Logical form generation via multi-task learning for complex question answering over knowledge bases},
  author={Hu, Xixin and Wu, Xuan and Shu, Yiheng and Qu, Yuzhong},
  booktitle={In Proc. ICCL Conf.},
  pages={1687--1696},
  year={2022}
}

@inproceedings{dubey2019lc,
  title={LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia},
  author={Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
  booktitle={In Proc. ISWC Conf.},
  pages={69--78},
  year={2019}
}

@inproceedings{cao2022kqa,
  title={KQA pro: A dataset with explicit compositional programs for complex question answering over knowledge base},
  author={Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang},
  booktitle={In Proc. ACL Conf.},
  pages={6101--6119},
  year={2022},
}

@inproceedings{ner2003,
    title = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
    author = {Tjong Kim Sang, Erik F.  and
      De Meulder, Fien},
    booktitle = {In Proc. NAACL-HLT Conf.},
    year = {2003},
    pages = {142--147},
}


@inproceedings{yih2016value,
  title={The value of semantic parse labeling for knowledge base question answering},
  author={Yih, Wen-tau and Richardson, Matthew and Meek, Christopher and Chang, Ming-Wei and Suh, Jina},
  booktitle={In Proc. ACL Conf.},
  pages={201--206},
  year={2016}
}

@inproceedings{talmor2018web,
  title={The Web as a Knowledge-Base for Answering Complex Questions},
  author={Talmor, Alon and Berant, Jonathan},
  booktitle={In Proc. ACL Conf.},
  pages={641--651},
  year={2018}
}

@inproceedings{su2016generating,
  title={On generating characteristic-rich question sets for qa evaluation},
  author={Su, Yu and Sun, Huan and Sadler, Brian and Srivatsa, Mudhakar and G{\"u}r, Izzeddin and Yan, Zenghui and Yan, Xifeng},
  booktitle={In Proc. EMNLP Conf.},
  pages={562--572},
  year={2016}
}

@inproceedings{gu2021beyond,
  title={Beyond iid: three levels of generalization for question answering on knowledge bases},
  author={Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
  booktitle={In Proc. WWW Conf.},
  pages={3477--3488},
  year={2021}
}

@inproceedings{berant2013semantic,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={In Proc. EMNLP Conf.},
  pages={1533--1544},
  year={2013}
}

@inproceedings{gu2022arcaneqa,
  title={ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering},
  author={Gu, Yu and Su, Yu},
  booktitle={In Proc. COLING Conf.},
  pages={1718--1731},
  year={2022}
}

