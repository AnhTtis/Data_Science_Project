\documentclass[prx,twocolumn,floatfix,superscriptaddress,longbibliography]{revtex4-1}


\usepackage{mathtools}


\usepackage{amssymb,amsmath,amstext,amsthm,mathtools}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{bm}
\usepackage{appendix}
\usepackage[T1]{fontenc}
\usepackage{bbold}
\usepackage{bbm}
\usepackage{latexsym}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=magenta]{hyperref}
\usepackage{float}
\usepackage{verbatim}
\usepackage{lipsum}
\usepackage{braket}
\usepackage{amsthm}

% Defined commands
\newcommand{\pat}[1]{\textcolor{red}{#1}}
%\newcommand{\pat}[1]{\textcolor{red}{[P: #1]}}
%\newcommand{\eric}[1]{\textcolor{red}{[E: #1]}}
\newcommand{\abs}[2][]{#1| #2 #1|}
%\newcommand{\braket}[2]{\langle #1 \hspace{1pt} | \hspace{1pt} #2 \rangle}
%\newcommand{\braketq}[1]{\braket{#1}{#1}}
\newcommand{\ketbra}[2]{| \hspace{1pt} #1 \rangle \langle #2 \hspace{1pt} |}
\newcommand{\ketbras}[3]{| \hspace{1pt} #1 \rangle_{#3} \langle #2 \hspace{1pt} |}
\newcommand{\ketbraq}[1]{\ketbra{#1}{#1}}
\newcommand{\bramatket}[3]{\langle #1 \hspace{1pt} | #2 | \hspace{1pt} #3 \rangle}
\newcommand{\bramatketq}[2]{\bramatket{#1}{#2}{#1}}
\newcommand{\Norm}[2][]{#1| \! #1| #2 #1| \! #1|}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\nbox}[2][9]{\hspace{#1pt} \mbox{#2} \hspace{#1pt}}
\newcommand{\avg}[1]{\langle #1\rangle }
%\newcommand{\ket}[1]{|#1\rangle}               %ket
\newcommand{\colo}{\,\hbox{:}\,}              %colon in math with less space
%\newcommand{\bra}[1]{\langle #1|}              %bra
\newcommand{\dya}[1]{\ket{#1}\!\bra{#1}}
\newcommand{\dyad}[2]{\ket{#1}\!\bra{#2}}        %dyad
\newcommand{\ipa}[2]{\langle #1,#2\rangle}      %abstract inner product
\newcommand{\ip}[2]{\langle #1|#2\rangle}      %quantum inner product
\newcommand{\matl}[3]{\langle #1|#2|#3\rangle} %matrix element

%\newcommand{\ket}[1]{| #1 \rangle}
%\newcommand{\bra}[1]{\langle #1 |}
\def\hc{^{\dagger}}

%%% Circuits
% Power of one Qubit
\newcommand{\POOQ}{\ensuremath{\mathsf{POOQ}}\xspace}
% Hilbert-Schmidt Test
\newcommand{\HSTest}{\ensuremath{\mathsf{HS\mbox{-}Test}}\xspace}






\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}




\newcommand{\fid}{\text{fid}}
\newcommand{\sq}{\text{sq}}
\newcommand{\cor}{\text{cor}}
\newcommand{\secr}{\text{sec}}
\newcommand{\rob}{\text{rob}}
\newcommand{\rank}{\text{rank}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Sbb}{\mathbb{S}}
\newcommand{\Abb}{\mathbb{A}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Xbb}{\mathbb{X}}
\newcommand{\Ybb}{\mathbb{Y}}
\newcommand{\Zbb}{\mathbb{Z}}
\newcommand{\Abar}{\overline{A}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Zbar}{\overline{Z}}
\newcommand{\Khat}{\widehat{K}}
\newcommand{\AC}{\mathcal{A}}
\newcommand{\BC}{\mathcal{B}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\DC}{\mathcal{D}}
\newcommand{\EC}{\mathcal{E}}
\newcommand{\FC}{\mathcal{F}}
\newcommand{\GC}{\mathcal{G}}
\newcommand{\HC}{\mathcal{H}}
\newcommand{\IC}{\mathcal{I}}
\newcommand{\KC}{\mathcal{K}}
\newcommand{\LC}{\mathcal{L}}
\newcommand{\MC}{\mathcal{M}}
\newcommand{\NC}{\mathcal{N}}
\newcommand{\OC}{\mathcal{O}}
\newcommand{\PC}{\mathcal{P}}
\newcommand{\QC}{\mathcal{Q}}
\newcommand{\RC}{\mathcal{R}}
\newcommand{\SC}{\mathcal{S}}
\newcommand{\TC}{\mathcal{T}}
\newcommand{\UC}{\mathcal{U}}
\newcommand{\VC}{\mathcal{V}}
\newcommand{\WC}{\mathcal{W}}
\newcommand{\XC}{\mathcal{X}}
\newcommand{\YC}{\mathcal{Y}}
\newcommand{\ZC}{\mathcal{Z}}
\newcommand{\bX}{\mathbb{X}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\Thh}{\widehat{\Theta}}
\newcommand{\HS}{\text{HS}}
\newcommand{\tol}{\text{tol}}
\newcommand{\Tr}{{\rm Tr}}
\newcommand{\Var}{{\rm Var}}
\newcommand{\Det}{{\rm Det}}
\newcommand{\supp}{\text{supp}}
\newcommand{\pass}{\text{pass}}
\newcommand{\ave}[1]{\langle #1\rangle}               %average
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\newcommand{\mte}[2]{\langle#1|#2|#1\rangle }
\newcommand{\mted}[3]{\langle#1|#2|#3\rangle }
\newcommand{\eqprop}[2]{\stackrel{\tiny{#1}}{#2}}
\newcommand{\eqpropa}[2]{\stackrel{\scriptstyle{#1}}{#2}}
\newcommand{\leak}{\text{leak}^{\text{EC}}_{\text{obs}}}
\newcommand{\vectext}{\text{vec}}
\renewcommand{\Re}{\text{Re}}
\renewcommand{\Im}{\text{Im}}



\newcommand{\DCt}{\widetilde{\mathcal{D}}}
\newcommand{\VCt}{\widetilde{\mathcal{V}}}
\newcommand{\VBt}{\widetilde{\mathbb{V}}}
\newcommand{\Xt}{\widetilde{X}}
\newcommand{\Zt}{\widetilde{Z}}
\newcommand{\At}{\widetilde{A}}
\newcommand{\Bt}{\widetilde{B}}
\newcommand{\Ct}{\widetilde{C}}
\newcommand{\Ah}{\widehat{A}}
\newcommand{\Bh}{\widehat{B}}
\newcommand{\Ch}{\widehat{C}}
\newcommand{\Ab}{\overline{A}}
\newcommand{\Bb}{\overline{B}}
\newcommand{\Cb}{\overline{C}}

%\newcommand{\VB}{\textsf{V}}
\newcommand{\VB}{\mathbb{V}}

\newcommand{\Int}{\text{int}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%\def\vect#1{\vec{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}  % Bold vectors instead of arrow vectors
\newcommand{\ot}{\otimes}
\newcommand{\ad}{^\dagger}
\newcommand{\fip}[2]{\langle #1\, , \, #2\rangle} %Frobenius inner product
\newcommand{\trp}{^\textup{T}}
\newcommand*{\id}{\openone}
\newcommand*{\iso}{\cong}
\newcommand*{\Hmin}{H_{\min}}
\newcommand*{\Hmax}{H_{\max}}
\newcommand*{\Dmin}{D_{\min}}
\newcommand*{\Dmax}{D_{\max}}
\newcommand*{\guess}{\text{guess}}
\newcommand*{\g}{\text{guess}}

\newcommand{\rholh}{\hat{\rho}}
\newcommand{\rhoh}{\widehat{\rho}}
\newcommand{\rhob}{\overline{\rho}}
\newcommand{\rhot}{\tilde{\rho}}
\newcommand{\bs}{\textsf{BS}}
\newcommand{\qbs}{\textsf{QBS}}
\newcommand{\pbs}{\textsf{PBS}}
\newcommand{\phit}{\tilde{\phi}}
\newcommand{\psit}{\tilde{\psi}}

\newcommand{\poly}{\operatorname{poly}}

%Greek Letters
\newcommand{\f}{\text{fringe} }
\newcommand{\al}{\alpha }
\newcommand{\bt}{\beta }
\newcommand{\gm}{\gamma }
\newcommand{\Gm}{\Gamma }
\newcommand{\dl}{\delta }
\newcommand{\Dl}{\Delta}
\newcommand{\ep}{\epsilon}
\newcommand{\vep}{\varepsilon}
\newcommand{\zt}{\zeta }
\renewcommand{\th}{\theta } %Latex \th = thor n
\newcommand{\vth}{\vartheta }
\newcommand{\Th}{\Theta }
\newcommand{\io}{\iota }
\newcommand{\kp}{\kappa }
\newcommand{\lm}{\lambda }
\newcommand{\lmv}{\vec{\lambda} }
\newcommand{\gmv}{\vec{\gamma} }
\newcommand{\Gmv}{\vec{\Gamma} }
\newcommand{\Lm}{\Lambda }
\newcommand{\vpi}{\varpi}
\newcommand{\vrh}{\varrho}
\newcommand{\sg}{\sigma }
\newcommand{\vsg}{\varsigma}
\newcommand{\Sg}{\Sigma }
\newcommand{\up}{\upsilon }
\newcommand{\Up}{\Upsilon }
\newcommand{\vph}{\varphi }
\newcommand{\om}{\omega }
\newcommand{\Om}{\Omega }
\newcommand{\ddd}{d }
\newcommand{\En}{F}
\newcommand{\reg}{M}
\newcommand{\St}{\tilde{\mathbf{S} }}
\newcommand{\Sh}{\hat{\mathbf{S} }}

\newcommand{\Auto}{\text{Auto }}

\newcommand{\thv}{\vec{\theta}}
\newcommand{\gav}{\vec{\gamma}}
\newcommand{\alv}{\vec{\alpha}}


%Theorems, Lemmas, etc.
%\newtheoremstyle{example}{\topsep}{\topsep}%
{}%         Body font
{}%         Indent amount (empty = no indent, \parindent = para indent)
%{\parindent}%         Indent amount (empty = no indent, \parindent = para indent)
%{\itshape}% Thm head font
%{\bfseries}% Thm head font
%{:}%        Punctuation after thm head
%{   }%     Space after thm head (\newline = linebreak)
%{\thmname{#1}\thmnumber{ #2}}%\thmnote{ #3}}%         Thm head spec
%\theoremstyle{example}
\newtheorem{example}{Example}%[subsection]

%\theoremstyle{plain}
%\newtheorem*{thm3}{Lemma S1}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
%\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}
%\newtheorem{remark}[theorem]{Remark}

\newenvironment{specialproof}{\textit{Proof:}}{\hfill$\square$}
%\newenvironment{specialproof}{\paragraph{Proof:}}{\hfill$\square$}



\newcommand{\footnoteremember}[2]{\footnote{#2}\newcounter{#1}\setcounter{#1}{\value{footnote}}}
\newcommand{\footnoterecall}[1]{\footnotemark[\value{#1}]}

%\input{Qcircuit}

\begin{document}


%\title{Challenges and Opportunities in Quantum Deep Learning}
\title{Challenges and Opportunities in Quantum Machine Learning}


\author{M. Cerezo}
\affiliation{Information Sciences, Los Alamos National Laboratory, Los Alamos, NM 87545, USA}
\affiliation{Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA}
\affiliation{Quantum Science Center, Oak Ridge, TN 37931, USA}



\author{Guillaume Verdon}
\affiliation{X, Mountain View, CA, USA}
\affiliation{Institute for Quantum Computing, University of Waterloo, ON, Canada}
\affiliation{Department of Applied Mathematics, University of Waterloo, ON, Canada}



\author{Hsin-Yuan Huang}
\affiliation{Institute for Quantum Information and Matter, California Institute of Technology, USA}
\affiliation{Department of Computing and Mathematical Sciences, California Institute of Technology, USA}



\author{Lukasz Cincio}
\affiliation{Theoretical Division, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA}
\affiliation{Quantum Science Center, Oak Ridge, TN 37931, USA}


\author{Patrick J. Coles}
\affiliation{Normal Computing Corporation, New York, New York, USA}
\affiliation{Theoretical Division, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA}
\affiliation{Quantum Science Center, Oak Ridge, TN 37931, USA}



\begin{abstract}
At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.
%At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials and biochemistry. Nevertheless, challenges remain regarding the trainability of QML models such as deep quantum neural networks. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks. Finally, we discuss opportunities for quantum advantage with QML.
%At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) is an emerging field with the potential of  accelerating data analysis. This is especially true for quantum data (data coming from quantum-mechanical processes), with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the construction and trainability of QML models such as deep quantum neural networks. Here we review the current state-of-the-art methods for QML, discussing the key applications that stand to benefit. We highlight differences between quantum and classical machine learning, featuring issues that are unique to the quantum case, with a particular focus on quantum neural networks. Finally, we highlight possible opportunities for quantum advantage with QML.
\end{abstract}
\maketitle


\section{Introduction}\label{sc:intro}





%\textbf{quantum leads to generalized theories}

 The recognition that the world is quantum mechanical has allowed researchers to embed well-established, but classical, theories into the framework of quantum Hilbert spaces. Shannon's information theory, which is the basis of communication technology, has been generalized to quantum Shannon theory (or quantum information theory), opening up the possibility that quantum effects could make information transmission more efficient~\cite{nielsen2000quantum}. The field of biology has been extended to quantum biology to allow for a deeper understanding of biological processes like photosynthesis, smell, and enzyme catalysis~\cite{brookes2017quantum}. Turing's theory of universal computation has been extended to universal quantum computation~\cite{deutsch1985quantum}, potentially leading to exponentially faster simulations of physical systems. 

%\textbf{classical ML}

One of the most successful technologies of this century is machine learning (ML), which aims to classify, cluster, and recognize patterns for large data sets. Learning theory has been simultaneously developed alongside of ML technology in order to understand and improve upon its success. Concepts like support vector machines, neural networks, and generative adversarial networks have impacted science and technology in profound ways. ML is now ingrained into society to such a degree that any fundamental improvement to ML  leads to tremendous economic benefit.

%\textbf{Quantum ML}

Like other classical theories, ML and learning theory can in fact be embedded into the quantum mechanical formalism. Formally speaking, this embedding leads to the field known as Quantum Machine Learning (QML)~\cite{wiebe2014quantumdeep,schuld2015introduction,biamonte2017quantum}, which aims to understand the ultimate limits of data analysis allowed by the laws of physics. Practically speaking, the advent of quantum computers, with the hope of achieving a so-called quantum advantage (as defined below) for data analysis, is what has made QML so exciting. Quantum computing exploits entanglement, superposition, and interference to perform certain tasks with significant speedups over classical computing, sometimes even exponentially faster. Indeed while such speedup has already been observed for a contrived problem~\cite{arute2019quantum}, reaching it for data science is still uncertain even at the theoretical level, but this is one of the main goals for QML.




%\textbf{Different types of QML tasks}

In practice, QML is a broad term that encompasses all of the tasks shown in Fig.~\ref{fig:1}. For example, one can apply machine learning to quantum applications like discovering quantum algorithms~\cite{cincio2018learning} or optimizing quantum experiments~\cite{tranter2018multiparameter,kaubruegger2021quantum}, or one can use a quantum neural network to process either classical or quantum information~\cite{cong2019quantum}. Even classical tasks can be viewed as QML when they are quantum inspired~\cite{tang2019quantum}. We note that the focus of this article will be on quantum neural networks, quantum deep learning, and quantum kernels, even though the field of QML is quite broad and goes beyond these topics.


\begin{figure}[t]
\centering
\includegraphics[width=.8\columnwidth]{Fig1v2.pdf}
\caption{\textbf{Quantum Machine Learning (QML) tasks}. Quantum machine learning is usually considered for four main tasks. These include tasks where the data is either classical or quantum, and where the algorithm is either classical or quantum. Top left: tensor networks are quantum-inspired classical methods that can analyze classical data. Top right: unitary time-evolution data $U$ from a quantum system can be classically compiled into a quantum circuit. Bottom left: handwritten digits can be mapped to quantum states for classification on a quantum computer. Bottom right: molecular ground state data can be classified directly on a quantum computer. The figure shows ground state energy $E$ dependence on the distance $d$ between the atoms.}
\label{fig:1}
\end{figure}


% \label{fig:1}





%\textbf{Applications}


After the invention of the laser, it was called a solution in search of a problem. To some degree, the situation with QML is similar. The complete list of applications of QML is not fully known. Nevertheless, it is possible to speculate that all the areas shown in Fig.~\ref{fig:applications} will be impacted by QML. For example, QML will likely benefit chemistry, materials science, sensing and metrology, classical data analysis, quantum error correction, and quantum algorithm design. Some of these applications produce data that is inherently quantum mechanical, and hence it is natural to apply QML (rather than classical ML) to them.

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{New_Fig2.pdf}
\caption{\textbf{Key Applications for QML}. QML has been envisioned to bring a computational advantage in many applications. QML can enhance quantum simulation for chemistry (e.g., molecular ground states~\cite{peruzzo2014variational}, equilibrium states~\cite{verdon2019quantum}, and time evolution~\cite{cirstoiu2020variational}) and materials science (e.g., quantum phase recognition~\cite{cong2019quantum} and generative design with a target property in mind~\cite{sanchez2018inverse}). QML can enhance quantum computing by learning quantum error correction codes~\cite{cong2019quantum,johnson2017qvector} and syndrome decoders, performing quantum control, learning to mitigate errors, and compiling and optimizing quantum circuits. QML can enhance sensing and metrology~\cite{verdon2020qmp,meyer2020variational,beckey2020variational,broughton2020tensorflow,wang2017experimental} and extract hidden parameters from quantum systems. Finally, QML may speed up classical data analysis, including clustering and classification.}
\label{fig:applications}
\end{figure}



%\textbf{Classical vs quantum ML}

While there are similarities between classical and quantum ML, there are also some differences. Because QML employs quantum computers, noise from these computers can be a major issue. This includes hardware noise like decoherence as well as statistical noise (i.e., shot noise) that arises from measurements on quantum states. Both of these noise sources can complicate the QML training process. Moreover, non-linear operations (e.g., neural activation functions) that are natural in classical ML require more careful design of QML models due to the linearity of quantum transformations. 




%\textbf{Outlook for QML}

For the field of QML, the immediate goal for the near-future is demonstrating quantum advantage, i.e., outperforming classical methods, in a data science application. Achieving this goal will require keeping an open mind about which applications will benefit most from QML (e.g., it may be an application that is inherently quantum mechanical). Understanding how QML methods scale to large problem sizes will also be required, including analysis of trainability (gradient scaling) and prediction error. The availability of high quality quantum hardware~\cite{huang2021power,banchi2021generalization} will also be crucial. 


%\textbf{New perspective}

Finally, we note that QML provides a new way of thinking about established fields, like quantum information theory, quantum error correction, and quantum foundations. Viewing such applications from a data science perspective will likely lead to new breakthroughs. 




% \label{fig:applications}



\section{Framework}



\subsection{Data}

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{New_Fig6.pdf}
\caption{ \textbf{Classification with QML.} a) The classical data $x$, i.e., images of cats and images of dogs, is  encoded into a Hilbert space via some map $x\rightarrow\ket{\psi(x)}$. Ideally, data from different classes (here represented by dots and stars)  is mapped to different regions of the Hilbert space . b) Quantum data $\ket{\psi}$ can be directly analyzed on a quantum device. Here the dataset is composed of states representing metallic  or superconducting systems.  c) The dataset is used to train a QML model. Two common paradigms in QML are quantum neural networks and quantum kernels, both of which allow for classification of either classical or quantum data. In Kernel methods one fits a decision hyperplane that separates the classes.  d) Once the model is trained, it can be used to make predictions.  }
\label{fig:QMLsetting}
\end{figure}

As shown in Fig.~\ref{fig:QMLsetting},  QML can be used to learn from either classical or quantum data, and thus we begin by  contrasting these two types of data. Classical data is ultimately encoded in bits, each of which can be in a  $0$ or  $1$ state. This includes images, texts, graphs, medical records, stock prices, properties of molecules, outcomes from biological experiments, and collision traces from high energy physics experiments. Quantum data is encoded in quantum bits, called qubits, or higher-dimensional analogs. A qubit can be represented by the states $\ket{0}$, $\ket{1}$, or any normalized complex linear \textit{superposition} of these two. Here, the states contain information obtained from some physical process such as quantum sensing~\cite{degen2017quantum}, quantum metrology~\cite{giovannetti2011advances}, quantum networks~\cite{chiribella2009theoretical}, quantum control~\cite{dalessandro2010introduction}, or even quantum analog-digital transduction~\cite{verdon2020qadi}. Moreover, quantum data can also be the solution to problems obtained on a quantum computer, e.g., the preparation of various Hamiltonians' ground states. 

In principle, all classical data can be efficiently encoded in systems of qubits: a classical bitstring of length $n$ can be easily encoded onto $n$ qubits. However, the same cannot be said for the converse, since one cannot efficiently encode quantum data in bit systems, i.e., the state of a general $n$ qubit system requires $(2^n-1)$ complex numbers to be specified. Hence, systems of qubits (and more generally the quantum Hilbert space) constitute the ultimate data  representation medium, as they can encode not only classical information but also quantum information obtained from physical processes.  




In a QML setting, the term quantum data refers to data that is naturally already embedded in a Hilbert space $\HC$. When the data is quantum, it is already in the form of a set of quantum states $\{\ket{\psi_j}\}$ or a set of unitaries $\{U_j\}$ that could prepare these states on a quantum device (via the relation $\ket{\psi_j} = U_j\ket{\vec{0}}$). On the other hand, when the data $x$ is classical, it first needs to be encoded in a quantum system through some embedding mapping $x_j\rightarrow \ket{\psi(x_j)}$, with $\ket{\psi(x_j)}$ in $\HC$. In this case, the hope is that the QML model can solve the learning task by accessing the exponentially large dimension of the Hilbert space~\cite{rebentrost2014quantum,schuld2019quantum,lloyd2020quantum,schuld2021effect}. 



One of the most important and reasonable conjectures to make is that the availability of quantum data will significantly increase in the near future. The mere fact that people will use the quantum computers that are available will logically lead to more quantum problems being solved and quantum simulations being performed. These computations will produce quantum data sets, and hence it is reasonable to expect the rapid rise of quantum data.  Note that, in the near term, this quantum data will be stored on classical devices in the form of efficient descriptions of quantum circuits that prepare the datasets.

Finally, as our level of control over quantum technologies progress, coherent transduction of quantum information from the physical world to digital quantum computing platforms may be achieved \cite{verdon2020qadi}. This would quantum mechanically mimic the main information acquisition mechanism for classical data from the physical world, that being analog-digital conversion. Moreover, we can expect that the eventual advent of practical quantum error correction~\cite{roffe2019quantum} and quantum memories~\cite{shor1995scheme} will allow us to store quantum data on quantum computers themselves.






\subsection{Models}


% \label{fig:QMLsetting}




Analyzing and learning from data requires a parameterized model, and many different models have been proposed for QML applications. Classical models like neural networks and tensor networks (as shown in Fig.~\ref{fig:1}) are often useful for analyzing data coming from quantum experiments. However, due to their novelty, we will focus our discussion on quantum models using quantum algorithms, where one applies the learning methodology directly at the quantum level. 

Similar to classical ML, there exists several different QML paradigms: supervised learning (task-based)~\cite{havlivcek2019supervised,liu2021rigorous,schuld2021quantum}, unsupervised learning (data-based)~\cite{otterbach2017unsupervised,kerenidis2019q} and reinforced learning (reward-based)~\cite{saggio2021experimental,skolik2021quantum}. While each of these fields is exciting and thriving on its own, supervised learning has recently received considerable attention for its potential to achieve quantum advantage~\cite{huang2021quantumadvantage,havlivcek2019supervised}, resilience to noise~\cite{larose2020robust}, and good generalization properties~\cite{caro2021generalization,caro2022outofdistribution,caro2021encodingdependent}, which makes it a strong candidate for near-term applications. In what follows we discuss two popular QML models: quantum neural networks (QNNs) and quantum kernels, shown in Fig.~\ref{fig:QMLsetting}, with a particular emphasis on QNNs as these are the primary ingredient of several supervised, unsupervised, and reinforced learning schemes. 




\subsubsection{Quantum neural networks}




The most basic and key ingredient in QML models are Parameterized Quantum Circuits (PQCs). These involve a sequence of unitary gates acting on the quantum data states $\ket{\psi_j}$, some of which have free parameters $\thv$ that will be trained to solve the problem at hand~\cite{cerezo2020variationalreview}.   PQCs are conceptually analogous to neural networks, and indeed this analogy can be made precise, i.e., classical neural networks can be formally embedded into PQCs~\cite{wan2017quantum}. 





This has led researchers to refer to certain kinds of PQCs as Quantum Neural Networks (QNNs). In practice, the term QNN is used whenever a PQC is employed for a data science application, and hence we will use the term QNN in what follows. QNNs are employed in all three QML paradigms mentioned above. For instance, in a supervised classification task, the  goal of the QNN is to map the states in different classes to distinguishable regions of the Hilbert space~\cite{havlivcek2019supervised}. Moreover, in the unsupervised learning scenario of~\cite{otterbach2017unsupervised} a clustering task is mapped onto a MAXCUT problem and solved by training a QNN to maximize distance between classes. Finally, in the reinforced learning task of~\cite{skolik2021quantum}, a QNN can be used as the Q-function approximator, which can be used to determine the best action for a learning agent given its current state.

\begin{figure}%[t]
\centering
\includegraphics[width=1\columnwidth]{Fig_QNN_v3.pdf}
\caption{ \textbf{Examples of QNN architectures.} a) A classical feed-forward neural network has input, hidden, and output layers. This can be generalized to the quantum setting with a dissipative QNN, where some qubits are discarded and replaced by new qubits during the algorithm . Here we shown a quantum circuit representation for the dissipative QNN. In a circuit diagram each horizontal line represents a qubit, and the logical operations, or quantum gates, are represented by boxes connecting the qubit lines. Circuits are read from left to right. For instance, here the circuit is initialized in a product state $\ket{\psi_j} \otimes \ket{0}^{\otimes (N_h + N_o)}$, where $\ket{\psi_j}$ encodes the input data and $N_h$ ($N_o$) is the number of qubits in the hidden (output) layer. As one performs logical operations, the information forward propagates through the circuit. b) Another possible QNN strategy is to keep the qubits fixed, without discarding or replacing them. The circuit represents consecutive application of two-qubit gates $U_j$ and controlled-NOT (denoted by CNOT) gates. c) Quantum convolutional neural networks (QCNNs) measure and discard qubits during the algorithm. The QCNN circuit considered here is built with two-qubit quantum gates $U_j$ and is initialized in $\ket{\psi_j}$. }

\label{fig:QNNexamples}
\end{figure}

Figure~\ref{fig:QNNexamples} gives examples of three distinct QNN architectures where at each layer the number of qubits in the model is increased, preserved, or decreased. In Fig.~\ref{fig:QNNexamples}(a) we show a  dissipative QNN~\cite{beer2020training} which generalizes the classical feed-forward network. Here, each node corresponds to a qubit, while lines connecting qubits are unitary operations.  The term dissipative arises from the fact that qubits in a layer are discarded after the information forward-propagates to the (new) qubits in the next layer. Figure~\ref{fig:QNNexamples}(b) shows a standard  QNN where quantum data states are sent through a quantum circuit, at the end of which some or all of the qubits are measured. Here, no qubits are discarded or added as one goes deeper into the QNN. Finally, Fig.~\ref{fig:QNNexamples}(c) depicts a convolutional QNN~\cite{cong2019quantum}, where at each layer qubits are measured to reduce the dimension of the data while preserving its relevant features. Many other QNNs have been proposed~\cite{schuld2014quest,dallaire2018quantum,farhi2018classification,killoran2019continuous,bausch2020recurrent}, and constructing QNN architectures is currently an active area of research. 


To further accommodate for the limitation of near-term quantum computers, one can also employ a hybrid approach with models that have both classical and quantum neural networks~\cite{broughton2020tensorflow}. Here, QNNs act coherently on quantum states while deep classical neural networks  alleviate the need for higher-complexity quantum processing. Such hybridization  distributes the representational capacity and computational complexity across both quantum and classical computers. Moreover, since quantum states generally have a mixture of classical correlations and quantum correlations, hybrid quantum-classical models allow for the use of quantum computers as an additive resource to increase the ability for classical models to represent quantum-correlated distributions.   Applications of hybrid models include generating~\cite{verdon2019quantum} or learning and distilling information~\cite{broughton2020tensorflow} from multipartite-entangled distributions. 



\subsubsection{Quantum kernels}



As an alternative to QNNs, researchers have proposed quantum versions of kernel methods~\cite{havlivcek2019supervised,schuld2021quantum}.
A kernel method maps each input to a vector in a high-dimensional vector space, known as the reproducing kernel Hilbert space.
Then, a kernel method learns a linear function in the reproducing kernel Hilbert space.
The dimension of the reproducing kernel Hilbert space could be infinite, which enables the kernel method to be very powerful in terms of the expressiveness.
To learn a linear function in a potentially infinite-dimensional space, the kernel trick \cite{cortes1995support} is employed, which only requires efficient computation of the inner product between these high-dimensional vectors.
The inner product is also known as the kernel \cite{cortes1995support}.
Quantum kernel methods consider the computation of kernel functions using quantum computers. There are many possible implementations. For example, \cite{havlivcek2019supervised, schuld2021quantum} considered a reproducing kernel Hilbert space equal to the quantum state space, which is finite dimensional. Another approach~\cite{huang2021power} is to 
study an infinite-dimensional reproducing kernel Hilbert space that is equivalent to transforming classical vector using a quantum computer. It  then maps the transformed classical vectors to infinite-dimensional vectors.


\subsubsection{Inductive bias}

For both QNNs and quantum kernels, an important design criterion is their inductive bias. This bias refers to the fact that any model represents only a subset of functions and is naturally biased towards certain types of functions (i.e, functions relating the input features to the output prediction). One aspect of achieving quantum advantage with QML is to aim for QML models that have an inductive bias that is inefficient to simulate with a classical model. Indeed, it was recently shown~\cite{kubler2021inductive} that quantum kernels with this property can be constructed, albeit with some subtleties regarding their trainability. 

% \label{fig:QNNexamples}


Generally speaking, inductive bias encompasses any assumptions made in the design of the model or the optimization method which bias the search of the potential models to a subset in the set of all possible models. In the language of Bayesian probabilistic theory, we usually call these assumptions our prior. Having a certain parameterization of potential models, like QNNs, or choosing a particular embedding for quantum kernel methods \cite{havlivcek2019supervised,huang2021power,banchi2021generalization} is itself a restriction of the search space, and hence a prior. Adding a regularization term to the optimizer or modulating the learning rate to keep searches geometrically local also adds inherently a prior and focuses the search, and thus provides inductive bias. 



Ultimately, inductive biases from the design of the ML model, combined with a choice of training process, are what make or break an ML model. The main advantage of QML will then be to have the ability to sample from and learn models that are (at least partially) natively quantum mechanical. As such, they have inductive biases that classical models do not have. This discussion assumes that the dataset to be represented is quantum mechanical in nature, and is one of the reasons why researchers typically believe that QML has greater promise from quantum rather than classical data. 


\subsection{Training and Generalization}

The ultimate goal of ML (classical or quantum) is to train a model to solve a given task. Thus, understanding the training process of QML models is fundamental for their success. 





Consider the training process, whereby one aims to find the set of parameters $\thv$ that lead to the best performance. The latter can be accomplished, for instance, by minimizing a loss function $\LC(\thv)$ encoding the task at hand. Some methods for training QML models are leveraged from classical ML, like stochastic gradient descent. However, shot noise, hardware noise, and unique landscape features often make off-the-shelf classical optimization methods perform poorly for QML training~\footnote{This is due to the fact that extracting information from a quantum state requires computing the expectation values of some observable, which in practice need to be estimated via measurements on a noisy quantum computer. Hence, given a finite number of shots (measurement repetitions), these can only be resolved up to some additive errors. Moreover, such expectation values will be subject to corruption due to hardware noise.}. This realization led to development of quantum-aware optimizers, which account for the quantum idiosyncrasies of the QML training process. For example, shot-frugal optimizers~\cite{kubler2020adaptive, arrasmith2020operator, gu2021adaptive, sweke2020stochastic} can employ stochastic gradient descent while adapting the number of shots (or measurements) needed at each iteration, so as not to waste too many shots during the optimization. Quantum natural gradient~\cite{stokes2020quantum,koczor2019quantum} adjusts the step size according to the local geometry of the landscape (based on the quantum Fisher information metric). These and other quantum-aware optimizers often outperform standard classical optimization methods in QML training tasks.

For the case of supervised learning, one is not only interested in learning from a training data set but also in making accurate predictions on (generalize to) previously unseen data. This translates into achieving small training and prediction errors, with the second usually hinging on the first.  Thus, let us now consider prediction error, also known as generalization error, which has been studied only very recently for QML~\cite{sharma2020reformulation,abbas2020power,huang2021power,banchi2021generalization,caro2021encodingdependent,caro2021generalization}. Formally speaking, this error measures the extent to which a trained QML model performs well on unseen data. Prediction error depends on both the training error as well as the complexity of the trained model. If the training error is large, the prediction error is also typically large. If the training error is small but the complexity of the trained model is large, then the prediction error is likely still large. The prediction error is small only if training error is itself small and the complexity of the trained model is moderate (i.e., sufficiently smaller than training data size)~\cite{caro2021generalization,banchi2021generalization}.
The notion of complexity depends on the QML model. We have a good understanding of the complexity of quantum kernel methods~\cite{huang2021power,banchi2021generalization}, while more research is needed on QNN complexity.
Recent theoretical analysis of QNNs shows that their prediction performance is closely linked to the number of independent parameters in the QNN, with good generalization obtained when the amount of training data is roughly equal to the number of parameters~\cite{caro2021generalization}. This gives the exciting prospect of using only a small amount of training data to obtain good generalization. 

\section{Challenges in QML}

% \label{fig:5}


\begin{figure}%[t]
\centering
\includegraphics[width=1\columnwidth]{New_Fig5.pdf}
\caption{\textbf{Challenges for QML.} a) There are several ingredients and  priors needed to build a QML model:  a dataset (and an encoding scheme for classical data), the choice of parameterized model, loss function, and classical optimizer.  In this diagram, we show some of the challenges of the different components of the model. b-d) The success of the QML model hinges on an accurate and efficient training of the parameters. However, there are certain phenomena that can hinder the QML trainability. These include the abundance of low-quality local minima solutions shown in b), as well as the barren plateau phenomenon in c).  When a QML architecture exhibits a barren plateau, the landscape becomes exponentially flat (on average) as the number of qubits increases (seen as a transition from dashed to solid line). The presence of hardware noise has been shown to erase the features in the landscape as well as potentially shift the position of the minima. Here, the dashed (solid) line corresponds to the noiseless (noisy) landscape shown in d).}
\label{fig:5}
\end{figure}

Heuristic fields can face periods of stagnation (or ``winters'') due to unforeseen technical challenges. Indeed in classical ML, there was a gap between introducing a single perceptron~\cite{rosenblatt1957perceptron} and the multi-layer perceptron~\cite{haykin1994neural} (i.e., neural network), and there was also a gap between attempts to train multiple layers and the introduction of the backpropagation method~\cite{rumelhart1986learning}.    



Naturally we would like to avoid these stagnations or winters for QML. The obvious strategy is to try to determine all of the challenges as quickly possible, and focus research effort on addressing them. Fortunately, QML researchers have taken this strategy. Figure~\ref{fig:5} showcases some  of the different elements of QML models, as well as the challenges associated with them. In this section we detail various QML challenges and how one could potentially avoid them. 

\subsection{Embedding schemes and quantum datasets}

The access to high-quality, standardized datasets has played a key role in advancing classical ML. Hence, one could conjecture that such datasets will be crucial for QML as well. 

Currently, most QML architectures are benchmarked using classical datasets (such as MNIST, Dogs vs Cats, and Iris). While using classical datasets is natural due to their accessibility, it is still unclear how to best encode classical information onto quantum states. Several embedding schemes have been proposed~\cite{havlivcek2019supervised,lloyd2020quantum,hubregtsen2021training}, and there are some desirable properties they must possess. One such property is that the inner product between  output states of the embedding is classically hard to simulate (otherwise the quantum kernel would be classically simulable). In addition, the embedding should be practically useful, i.e., in a classification task, the states should be in  distinguishable regions of the Hilbert space. Unfortunately, embeddings that satisfy one of these properties do not necessarily satisfy the other~\cite{thanasilp2021subtleties}. Thus, developing  encoding schemes  is an active area of research, especially those that are equipped with an  inductive bias containing information about the dataset~\cite{kubler2021inductive}. 

Furthermore, some recent results suggest that achieving a quantum advantage with  classical data might not be straightforward~\cite{kubler2021inductive}. On the other hand, QML models with quantum data have a more promising route towards a quantum advantage~\cite{huang2021quantum,cotler2021revisiting, chen2021hierarchy, chen2021exponential}. Despite this fact, there is still a dearth of truly quantum datasets for QML, which just a few recently proposed~\cite{perrier2021qdataset,schatzki2021entangled}. Hence, the field needs standardized quantum datasets with easily preparable quantum states, as these can be used to benchmark QML models on true quantum data.





\subsection{Quantum landscapes}

Training the parameters of the QML model  corresponds in a wide array of cases to minimizing a loss function and navigating through a (usually non-convex) loss function landscape in search for its global minimum~\footnote{Technically speaking, the loss function defines a map from the model's parameter space to the real values. The loss function value can quantify, for instance, the model's error in solving a given so that our goal is to find the set of parameters that minimizes such error.}. Quantum landscape theory~\cite{arrasmith2021equivalence} aims to understand QML landscape properties and how to engineer them. Local minima and barren plateaus have received significant attention in quantum landscape theory.


\subsubsection{Local minima in quantum landscapes}

As schematically shown in Fig.~\ref{fig:5}(b), similar to classical ML,   the quantum loss landscape can have many local minima. Ultimately, this can lead to the overall non-convex optimization being NP-hard~\cite{bittel2021training}, which is again similar to the classical case. There have been some methods proposed to address local minima. For example, variable structure QNNs~\cite{bilkis2021semi,larose2019variational}, which grow and contract throughout the optimization, adaptively  change the model's prior and allow some local minima to be turned into saddle points. Moreover, evidence of the overparametrization phenomenon has been seen for QML~\cite{kiani2020learning,larocca2021theory}. Here, the optimization undergoes a computational phase transition, due to spurious local minima disappearing, whenever the number of parameters exceeds a critical value. 
 

\subsubsection{Overview of barren plateaus}

Local minima are not the only issue facing QML, as it has been shown that quantum landscapes can exhibit a  fascinating property known as a  \textit{barren plateau}~\cite{mcclean2018barren,cerezo2021cost,cerezo2020impact,arrasmith2020effect,holmes2021connecting,pesah2020absence,volkoff2021large,sharma2020trainability,holmes2020barren,marrero2020entanglement,uvarov2020barren,patti2020entanglement,abbas2020power,wang2020noise}. As depicted in Fig.~\ref{fig:5}(c), in a barren plateau the loss landscape becomes, on average, exponentially flat with the problem size. When this occurs, the valley containing the global mimimum also shrinks exponentially with problem size, leading to a so-called \textit{narrow gorge}~\cite{arrasmith2021equivalence}. As a consequence, one requires exponential resources (e.g., numbers of shots) to navigate through the landscape. The latter impacts the complexity of one's QML algorithm and can even destroy quantum speedup, since quantum algorithms typically aim to avoid the exponential complexity normally associated with classical algorithms.

\subsubsection{Barren plateaus from ignorance\\ or insufficient inductive bias}

The barren plateau phenomenon was  first studied in deep hardware-efficient QNNs~\cite{mcclean2018barren}, where they arise due to the high \textit{expressivity} of the model~\cite{holmes2021connecting}. By making no  assumptions about the underlying data, deep hardware-efficient architectures aims to solve a problem by being able to prepare a wide range of unitary evolutions. In other words, the prior over hypothesis space is relatively uninformed. Barren plateaus in this unsharp prior are caused by ignorance or the lack of sufficient inductive bias, and therefore a means to avoid them is to input knowledge into the construction of the QNN - making the design of QNNs with good inductive biases for the problem at hand a key solution.  

Fortunately various strategies have been developed to address these barren plateaus, such as clever initialization~\cite{verdon2019learning}, pre-training, and parameter correlation~\cite{volkoff2021large,pesah2020absence}. These are all examples of adding a sharper prior to one's search over the over-expressive parameterizations of hardware efficient QNNs. Below we further discuss how QNN architectures can be designed to further introduce inductive bias. 


\subsubsection{Barren plateaus from global observables}

Other mechanisms have been linked to barren plateaus. Simply defining a loss function based on a global observable (i.e., observables measuring all qubits) leads to barren plateaus even for shallow circuits with sharp priors~\cite{cerezo2021cost}, while local observables (those comparing quantum states at the single-qubit level) avoid this issue~\cite{cerezo2021cost, uvarov2020barren}. The latter is not due to bad inductive biases but rather to the fact that comparing objects in exponentially large Hilbert spaces requires an exponential precision, as their overlap is usually exponentially small. 

\subsubsection{Barren plateaus from entanglement}

While entanglement is one of the most important quantum resources for information processing tasks in quantum computers, it can also be detrimental for QML models. QNNs (or embedding schemes) that generate too much entanglement  also  lead to barren plateaus~\cite{sharma2020trainability,marrero2020entanglement,patti2020entanglement}.  Here, the issue arises when one entangles the visible qubits of the QNN (those that one measures at the QNN's output) with a large number of qubits in the hidden layers. Due to entanglement, the information of the state is stored in non-local correlations across all qubits, and hence the reduced state of the visible qubits concentrates around the maximally mixed state. This type of barren plateau can be solved by taming the entanglement generated across the QNN.    








\subsection{QNN architecture design}


One of the most active areas  is developing QNN architectures  that have sharp  priors. Since QNNs are a fundamental ingredient in supervised learning (deep learning, kernel methods), but also in unsupervised learning and reinforced learning, developing good QNN architectures is crucial for the field. 

For instance, it has been shown that QNNs with sharp priors can  avoid issues
such as barren plateaus altogether. One such example are Quantum Convolutional Neural Networks (QCNNs)~\cite{cong2019quantum}.  QCNNs possess an inductive bias from having a prior over the space of architectures that is much sharper than that of deep hardware-efficient architectures, as QCNNs are restricted to be hierarchically structured and translationally invariant. The significant reduction in the expressivity and parameter space dimension from this translational invariance assumption yields the greater trainability~\cite{pesah2020absence}.

The idea of embedding knowledge about the problem and dataset into our models (to achieve helpful inductive bias) will be key to improve the trainability of QML models. Recent proposals use Quantum Graph Neural Networks~\cite{verdon2019quantumgraph} for scenarios where quantum subsystems live on a graph, and potentially have further symmetries. For instance, the underlying graph-permutation symmetries of a quantum communication dataset were taken into account by a quantum graph convolutional network. Similarly, a quantum recurrent neural network has been used in scenarios where temporal recurrence of parameters occurs, e.g., as in the quantum dynamics of a stationary (time-dependent) quantum dynamical process.

To better understand how to go beyond the aforementioned inductive biases from temporal and/or translational invariance in grids and graphs, we can take inspiration from recent advances in the theory of classical deep learning. In classical ML, the study of the group theory  behind graph neural networks, namely the concepts of invariance and equivariance to various group actions on the input space, has led to a unifying theory of deep learning architectures based on group theory, called Geometric Deep Learning theory \cite{bronstein2021geometric}.

In order to have a prescription to create arbitrary architectures and inductive biases suitable for a given set of quantum physical data, a theory of quantum geometric deep learning could be key to design architectures with the right prior over the transformation space and inductive biases to ensure trainability and generalization. As the study of physics is often about the identification of inherent or emergent symmetries in particular systems, there is great potential for a future unifying theory of quantum geometric deep learning to provide consistent methods to create QML model architectures with inductive biases encoding knowledge of the basic symmetries and principles of the quantum physical system underlying given quantum datasets. This approach has been recently explored in~\cite{larocca2022group,skolik2022equivariant,meyer2022exploiting}. Moreover, the works of~\cite{larocca2021diagnosing,larocca2021theory} have also shown that the Lie algebra obtained from the generators of the QNN can be linked to properties of the QML landscape such as the presence of barren plateaus or the overparametrization phenomenon. 


\subsection{Effect of quantum noise}

The presence of hardware noise during quantum computations is one of the defining characteristics of Noisy Intermediate-Scale Quantum (NISQ) computing. Despite this fact, most QML research neglects noise in the analytical calculations and numerical simulations while still promising that the methods are near-term compatible. Accounting for the effects of hardware noise should be a crucial aspect of QML analysis if one wishes to pursue a quantum advantage with currently available hardware. 

Noise corrupts the information as it forward propagates in a quantum circuit, meaning that deeper circuits with longer run-times will be particularly affected. As such, noise affects all aspects of the model that make use of quantum  computers. This includes the dataset preparation scheme as well as  circuits used to compute quantum kernels. Moreover, when using QNNs, noise can hinder their trainability as it leads to noise-induced barren plateaus~\cite{wang2020noise,wang2021can}. Here, the relevant features of the landscape get exponentially suppressed by noise as the  depth of the circuit increases (see Fig.~\ref{fig:5}(d)). Ultimately, the effects of noise translate into a deformation of the inductive bias of the model from its original one, and an effective reduction of the dimension of the quantum feature space. Despite the critical impact of quantum noise, its effects are still largely unexplored, particularly on its impact on the classical simulability of the QML model~\cite{deshpande2021tight,hakkaku2021quantifying}.

Addressing noise-induced issues will likely require either: (1) reduction in hardware error rates, (2) partial quantum error correction~\cite{bultrini2022battle}, or (3) employing QNNs that are relatively shallow (i.e., whose depth grows sublinearly in the problem size)~\cite{wang2020noise}, such as QCNNs. Error mitigation techniques~\cite{temme2017error,czarnik2020error,endo2021hybrid} can also improve performance of QML models in the presence of noise, although they may not solve noise-induced trainability issues~\cite{wang2021can}. A different approach to dealing with noise is to engineer  QML models with noise-resilient properties~\cite{sharma2019noise,larose2020robust,cincio2020machine} (such as the position of the minima not changing due to noise).



\section{Outlook}



\subsection{Potential for Quantum advantage}



The first quantum advantages in QML will likely come from hidden parameter extraction from quantum data. This can be for quantum sensing or quantum state classification/regression. Fundamentally, we know from the theory of optimal measurement that non-local quantum measurements can extract hidden parameters using less samples. Using QML, one can form and search over a parameterization of hypotheses for such measurements. 

This is particularly useful when such optimal measurements are not known \textit{a priori}, for example, identifying the measurement that extracts an order parameter or identifies a particular phase of matter. As the information about this classical parameter is embedded in the structure of quantum correlations between subsystems, it is natural that a trained QML model with good inductive biases can exhibit an advantage over local measurements and classical representations.

Another area of application where classical parameter extraction may yield an advantage is in quantum machine perception ~\cite{verdon2020qmp,meyer2020variational,beckey2020variational,broughton2020tensorflow,wang2017experimental,huang2021quantum}, i.e. quantum sensing, metrology, and beyond. Here, leveraging the variational search over multipartite-entangled states for input to exposure to a quantum signal along with the optimization for optimal control and/or over post-processing schemes can find optimal measurements for the estimation of hidden parameters in the incoming signal. In particular, the variational approach may be able to find the optimal entanglement, exposure, and measurement scheme which filters signal from the noise \cite{layden2018spatial}, akin to variationally learning the quantum error correcting code which filters signal from noise, instead applied to quantum metrology.

Beyond classical parameter extraction embedded in quantum data, there may be an advantage for the discovery of quantum error correcting codes (QECCs) \cite{johnson2017qvector}. QECC's fundamentally encode data (typically) non-locally into a subsystem or subspace of the Hilbert space. As deep learning is fundamentally about the discovery of submanifolds of data space, identifying and decoding subspaces/subsystems from a Hilbert space which correspond to a quantum error correction subspace/subsystem is a natural place where differentiable quantum computing may yield an advantage. This is a barely explored area, mainly due to the difficulty of gaining insights with small-scale numerical simulations. Fundamentally, it is akin to a quantum data version of classical parameter embedding/extraction advantage. 


Finally, a quantum advantage for generative modelling may be achieved when one can generate ground states \cite{peruzzo2014variational}, equilibrium states \cite{mcardle2019variational,verdon2019quantum}, or quantum dynamics \cite{cirstoiu2020variational}, using generative models incorporating QNNs, in a way where the distribution cannot be sampled classically, and yields more accurate predictions or more extensively generalization compared to classical ML approaches. The nearest-term possibility for demonstrating such an advantage would likely be from variational optimization at the continuous time optimal control level on analogue quantum simulators. 



\subsection{What will quantum advantage look like?}



When the data originates from quantum-mechanical processes, such as from experiments in chemistry, material science, biology, and physics, it is more likely to see exponential quantum advantage in ML. The quantum advantage could be in \emph{sample complexity} or \emph{time complexity}. An exponential advantage in sample complexity always implies an exponential advantage in time complexity, but the reverse is not generally true. It was recently shown~\cite{huang2021information, aharonov2021quantum, huang2021quantum, chen2021hierarchy} that there is an exponential quantum advantage in sample complexity when we can use a quantum sensor, quantum memory, and quantum computer to retrieve, store, and process quantum information from experiments. Such a sample complexity advantage can be proven rigorously without the possibility of being \emph{dequantized}~\cite{tang2019quantum, chia2020sampling, cotler2021revisiting} in the future, i.e., it is impossible to find improved classical algorithms such that there is no exponential advantage.
This significant quantum advantage has recently been demonstrated on the Sycamore processor \cite{huang2021quantum} raising the hope for achieving quantum advantage using NISQ devices \cite{preskill2018quantum}.


The situation for advantage in \emph{time complexity} is more subtle.
Classical simulation of quantum process is intractable in many cases, hence one would expect exponential advantage in time complexity to be prevalent.
However, one should be cautious about the availability of data in ML tasks, which makes classical ML algorithms computationally more powerful \cite{huang2021power, huang2021provably}.
For instance, Ref.~\cite{huang2021provably} shows that \emph{in the worst case}, there is no exponential quantum advantage in predicting ground state properties in geometrically local gapped Hamiltonians.
Furthermore, the emergence of effective classical theory in quantum-mechanical processes could enable classical machines to provide accurate predictions.
For example, density functional theory~\cite{hohenberg1964inhomogeneous, kohn1999nobel} allows accurate prediction of molecular properties when we have an accurate approximation to the exchange-correlation functionals by conducting real-world experiments. 
It is still likely that an exponential advantage is possible in physical systems of practical interest, but there are no rigorous proofs yet.


When the data is of a purely classical origin, such as in applications for recommending products to customers \cite{tang2019quantum}, performing portfolio optimization \cite{alcazar2020classical, bouland2020prospects}, and processing human languages \cite{manning1999foundations} and everyday images \cite{russ2006image}, there is no known exponential advantage \cite{chia2020sampling}.
However, it is still reasonable to expect polynomial advantage. Furthermore, a quadratic advantage can be rigorously proven \cite{grover1996fast, bernstein1997quantum} for purely classical problems.
So we likely have a potential impact in the long-term when we have fault-tolerant quantum computers, albeit with the speedup significantly dampened by the overheads of quantum error correction \cite{babbush2021focus} for currently known fault-tolerant quantum computing schemes.


\subsection{Transition to the fault-tolerant era and beyond}

While QML has been proposed as a candidate to achieve a quantum advantage in the near-term using NISQ devices, one can still pose the question about its usability in the future. Here, researchers envision two different chronological eras post-NISQ. In the first, which we can refer to as ``partial error corrected'', quantum computers will have enough physical qubits (a couple of hundred of them), and sufficiently small error rates, to allow for a small number of fully error corrected logical qubits. Since one logical qubit is comprised of multiple physical qubits, in this era one will have the freedom to trade off and split the qubits in the device onto a subset of error corrected qubits, along with a subset of non-error corrected qubits. The next era, i.e., the ``fault-tolerant era'' will arise when the quantum hardware has a large number of error corrected qubits.

Indeed, one can easily envision QML being useful in both of these post-NISQ  eras. First, in the partial error corrected era, QML models will be able to execute  high-fidelity circuits  and thus have an improved performance. This will naturally enhance the trainability of the models by mitigating noise-induced barren plateaus, and also reduce noise-induced classification errors in QML models. Most importantly, QML will likely see its most widespread and critical use during the fault-tolerant era. Here, quantum algorithms such as those for quantum simulation~\cite{georgescu2014quantum,berry2015simulating} will be able to accurately prepare quantum data, and to faithfully store it in quantum memories~\cite{lvovsky2009optical}. Therefore QML will be the natural model to learn, infer, and make predictions from quantum data, as here the quantum computer will learn from the data itself directly.  



On the further-term horizon, we anticipate it will be possible to capture quantum data from nature directly via transduction from its natural analog form to one that is quantum digital (e.g., via quantum analog-digital interconversion \cite{verdon2020qadi}). This data will then be able to be shuttled around quantum networks for distributed and/or centralized processing with quantum machine learning models, using fault-tolerant quantum computation and error-corrected quantum communication. At this point, quantum machine learning will have reached a stage similar to where machine learning is today, where edge sensors capture data, the data is relayed to a central cloud, and machine learning models are trained on the aggregated data. As the modern advent of widespread classical machine learning arose at this point of abundant data, one could anticipate that ubiquitous access to quantum data in the fault-tolerant era could similarly propel quantum machine learning to even greater widespread use.




\section*{Acknowledgements}

MC acknowledges support from the Los Alamos National Laboratory (LANL) LDRD program under project number 20210116DR. MC was also supported by the Center for Nonlinear Studies at LANL. LC and PJC were supported by the U.S. Department of Energy (DOE), Office of Science, Office of Advanced Scientific Computing Research, under the Accelerated Research in Quantum Computing (ARQC) program. LC also acknowledges support from U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers, Quantum Science Center. PJC was also supported by the NNSAs Advanced Simulation and Computing Beyond Moores Law Program at LANL. GV would like to thank Faris Sbahi, Antonio J. Martinez, and Petar Velickovic for useful discussions. X, formerly known as Google[x], is
part of the Alphabet family of companies, which includes
Google, Verily, Waymo, and others (\url{www.x.company}).
HH is supported by a Google PhD Fellowship.


\section*{Author Contributions}
PJC drafted the manuscript structure. The manuscript was written and revised by MC, GV, HYH, LC and PJC.

%\section*{Data availability}
%No  data was generated for this article.

%\section*{Code availability}
%No code was generated for this article.

\section*{Competing Interests}
The authors declare no competing interests.

\newpage




% \bibliography{quantum.bib}

%merlin.mbs apsrev4-1.bst 2010-07-25 4.21a (PWD, AO, DPC) hacked
%Control: key (0)
%Control: author (0) dotless jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (0) allowed
%Control: page (1) range
%Control: year (0) verbatim
%Control: production of eprint (0) enabled
\begin{thebibliography}{132}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode
  `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{http://dx.doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {Nielsen}\ and\ \citenamefont
  {Chuang}(2000)}]{nielsen2000quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Michael~A.}\
  \bibnamefont {Nielsen}}\ and\ \bibinfo {author} {\bibfnamefont {Isaac~L.}\
  \bibnamefont {Chuang}},\ }\href@noop {} {\emph {\bibinfo {title} {Quantum
  Computation and Quantum Information}}}\ (\bibinfo  {publisher} {Cambridge
  University Press},\ \bibinfo {year} {2000})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Brookes}(2017)}]{brookes2017quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jennifer~C}\
  \bibnamefont {Brookes}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Quantum effects in biology: golden rule in enzymes, olfaction,
  photosynthesis and magnetodetection},}\ }\href {\doibase
  10.1098/rspa.2016.0822} {\bibfield  {journal} {\bibinfo  {journal}
  {Proceedings of the Royal Society A: Mathematical, Physical and Engineering
  Sciences}\ }\textbf {\bibinfo {volume} {473}},\ \bibinfo {pages} {20160822}
  (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Deutsch}(1985)}]{deutsch1985quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {David}\ \bibnamefont
  {Deutsch}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum theory,
  the church--turing principle and the universal quantum computer},}\ }\href
  {\doibase 10.1098/rspa.1985.0070} {\bibfield  {journal} {\bibinfo  {journal}
  {Proceedings of the Royal Society of London. A. Mathematical and Physical
  Sciences}\ }\textbf {\bibinfo {volume} {400}},\ \bibinfo {pages} {97--117}
  (\bibinfo {year} {1985})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wiebe}\ \emph {et~al.}(2014)\citenamefont {Wiebe},
  \citenamefont {Kapoor},\ and\ \citenamefont {Svore}}]{wiebe2014quantumdeep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont
  {Wiebe}}, \bibinfo {author} {\bibfnamefont {Ashish}\ \bibnamefont {Kapoor}},
  \ and\ \bibinfo {author} {\bibfnamefont {Krysta~M}\ \bibnamefont {Svore}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum deep learning},}\
  }\href {https://arxiv.org/abs/1412.3489} {\bibfield  {journal} {\bibinfo
  {journal} {arXiv preprint arXiv:1412.3489}\ } (\bibinfo {year}
  {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ \emph {et~al.}(2015)\citenamefont {Schuld},
  \citenamefont {Sinayskiy},\ and\ \citenamefont
  {Petruccione}}]{schuld2015introduction}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont
  {Schuld}}, \bibinfo {author} {\bibfnamefont {Ilya}\ \bibnamefont
  {Sinayskiy}}, \ and\ \bibinfo {author} {\bibfnamefont {Francesco}\
  \bibnamefont {Petruccione}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {An introduction to quantum machine learning},}\ }\href {\doibase
  10.1080/00107514.2014.964942} {\bibfield  {journal} {\bibinfo  {journal}
  {Contemporary Physics}\ }\textbf {\bibinfo {volume} {56}},\ \bibinfo {pages}
  {172--185} (\bibinfo {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Biamonte}\ \emph {et~al.}(2017)\citenamefont
  {Biamonte}, \citenamefont {Wittek}, \citenamefont {Pancotti}, \citenamefont
  {Rebentrost}, \citenamefont {Wiebe},\ and\ \citenamefont
  {Lloyd}}]{biamonte2017quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jacob}\ \bibnamefont
  {Biamonte}}, \bibinfo {author} {\bibfnamefont {Peter}\ \bibnamefont
  {Wittek}}, \bibinfo {author} {\bibfnamefont {Nicola}\ \bibnamefont
  {Pancotti}}, \bibinfo {author} {\bibfnamefont {Patrick}\ \bibnamefont
  {Rebentrost}}, \bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont
  {Wiebe}}, \ and\ \bibinfo {author} {\bibfnamefont {Seth}\ \bibnamefont
  {Lloyd}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum machine
  learning},}\ }\href {\doibase 10.1038/nature23474} {\bibfield  {journal}
  {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {549}},\ \bibinfo
  {pages} {195--202} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Arute}\ \emph {et~al.}(2019)\citenamefont {Arute},
  \citenamefont {Arya}, \citenamefont {Babbush}, \citenamefont {Bacon},
  \citenamefont {Bardin}, \citenamefont {Barends}, \citenamefont {Biswas},
  \citenamefont {Boixo}, \citenamefont {Brandao}, \citenamefont {Buell},
  \citenamefont {Burkett}, \citenamefont {Chen}, \citenamefont {Chen},
  \citenamefont {Chiaro}, \citenamefont {Collins}, \citenamefont {Courtney},
  \citenamefont {Dunsworth}, \citenamefont {Farhi}, \citenamefont {Foxen},
  \citenamefont {Fowler}, \citenamefont {Gidney}, \citenamefont {Giustina},
  \citenamefont {Graff}, \citenamefont {Guerin}, \citenamefont {Habegger},
  \citenamefont {Harrigan}, \citenamefont {Hartmann}, \citenamefont {Ho},
  \citenamefont {Hoffmann}, \citenamefont {Huang}, \citenamefont {Humble},
  \citenamefont {Isakov}, \citenamefont {Jeffrey}, \citenamefont {Jiang},
  \citenamefont {Kafri}, \citenamefont {Kechedzhi}, \citenamefont {Kelly},
  \citenamefont {Klimov}, \citenamefont {Knysh}, \citenamefont {Korotkov},
  \citenamefont {Kostritsa}, \citenamefont {Landhuis}, \citenamefont
  {Lindmark}, \citenamefont {Lucero}, \citenamefont {Lyakh}, \citenamefont
  {Mandr{\`a}}, \citenamefont {McClean}, \citenamefont {McEwen}, \citenamefont
  {Megrant}, \citenamefont {Mi}, \citenamefont {Michielsen}, \citenamefont
  {Mohseni}, \citenamefont {Mutus}, \citenamefont {Naaman}, \citenamefont
  {Neeley}, \citenamefont {Neill}, \citenamefont {Niu}, \citenamefont {Ostby},
  \citenamefont {Petukhov}, \citenamefont {Platt}, \citenamefont {Quintana},
  \citenamefont {Rieffel}, \citenamefont {Roushan}, \citenamefont {Rubin},
  \citenamefont {Sank}, \citenamefont {Satzinger}, \citenamefont {Smelyanskiy},
  \citenamefont {Sung}, \citenamefont {Trevithick}, \citenamefont
  {Vainsencher}, \citenamefont {Villalonga}, \citenamefont {White},
  \citenamefont {Yao}, \citenamefont {Yeh}, \citenamefont {Zalcman},
  \citenamefont {Neven},\ and\ \citenamefont {Martinis}}]{arute2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Frank}\ \bibnamefont
  {Arute}}, \bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont {Arya}},
  \bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont {Babbush}}, \bibinfo
  {author} {\bibfnamefont {Dave}\ \bibnamefont {Bacon}}, \bibinfo {author}
  {\bibfnamefont {Joseph~C.}\ \bibnamefont {Bardin}}, \bibinfo {author}
  {\bibfnamefont {Rami}\ \bibnamefont {Barends}}, \bibinfo {author}
  {\bibfnamefont {Rupak}\ \bibnamefont {Biswas}}, \bibinfo {author}
  {\bibfnamefont {Sergio}\ \bibnamefont {Boixo}}, \bibinfo {author}
  {\bibfnamefont {Fernando G. S.~L.}\ \bibnamefont {Brandao}}, \bibinfo
  {author} {\bibfnamefont {David~A.}\ \bibnamefont {Buell}}, \bibinfo {author}
  {\bibfnamefont {Brian}\ \bibnamefont {Burkett}}, \bibinfo {author}
  {\bibfnamefont {Yu}~\bibnamefont {Chen}}, \bibinfo {author} {\bibfnamefont
  {Zijun}\ \bibnamefont {Chen}}, \bibinfo {author} {\bibfnamefont {Ben}\
  \bibnamefont {Chiaro}}, \bibinfo {author} {\bibfnamefont {Roberto}\
  \bibnamefont {Collins}}, \bibinfo {author} {\bibfnamefont {William}\
  \bibnamefont {Courtney}}, \bibinfo {author} {\bibfnamefont {Andrew}\
  \bibnamefont {Dunsworth}}, \bibinfo {author} {\bibfnamefont {Edward}\
  \bibnamefont {Farhi}}, \bibinfo {author} {\bibfnamefont {Brooks}\
  \bibnamefont {Foxen}}, \bibinfo {author} {\bibfnamefont {Austin}\
  \bibnamefont {Fowler}}, \bibinfo {author} {\bibfnamefont {Craig}\
  \bibnamefont {Gidney}}, \bibinfo {author} {\bibfnamefont {Marissa}\
  \bibnamefont {Giustina}}, \bibinfo {author} {\bibfnamefont {Rob}\
  \bibnamefont {Graff}}, \bibinfo {author} {\bibfnamefont {Keith}\ \bibnamefont
  {Guerin}}, \bibinfo {author} {\bibfnamefont {Steve}\ \bibnamefont
  {Habegger}}, \bibinfo {author} {\bibfnamefont {Matthew~P.}\ \bibnamefont
  {Harrigan}}, \bibinfo {author} {\bibfnamefont {Michael~J.}\ \bibnamefont
  {Hartmann}}, \bibinfo {author} {\bibfnamefont {Alan}\ \bibnamefont {Ho}},
  \bibinfo {author} {\bibfnamefont {Markus}\ \bibnamefont {Hoffmann}}, \bibinfo
  {author} {\bibfnamefont {Trent}\ \bibnamefont {Huang}}, \bibinfo {author}
  {\bibfnamefont {Travis~S.}\ \bibnamefont {Humble}}, \bibinfo {author}
  {\bibfnamefont {Sergei~V.}\ \bibnamefont {Isakov}}, \bibinfo {author}
  {\bibfnamefont {Evan}\ \bibnamefont {Jeffrey}}, \bibinfo {author}
  {\bibfnamefont {Zhang}\ \bibnamefont {Jiang}}, \bibinfo {author}
  {\bibfnamefont {Dvir}\ \bibnamefont {Kafri}}, \bibinfo {author}
  {\bibfnamefont {Kostyantyn}\ \bibnamefont {Kechedzhi}}, \bibinfo {author}
  {\bibfnamefont {Julian}\ \bibnamefont {Kelly}}, \bibinfo {author}
  {\bibfnamefont {Paul~V.}\ \bibnamefont {Klimov}}, \bibinfo {author}
  {\bibfnamefont {Sergey}\ \bibnamefont {Knysh}}, \bibinfo {author}
  {\bibfnamefont {Alexander}\ \bibnamefont {Korotkov}}, \bibinfo {author}
  {\bibfnamefont {Fedor}\ \bibnamefont {Kostritsa}}, \bibinfo {author}
  {\bibfnamefont {David}\ \bibnamefont {Landhuis}}, \bibinfo {author}
  {\bibfnamefont {Mike}\ \bibnamefont {Lindmark}}, \bibinfo {author}
  {\bibfnamefont {Erik}\ \bibnamefont {Lucero}}, \bibinfo {author}
  {\bibfnamefont {Dmitry}\ \bibnamefont {Lyakh}}, \bibinfo {author}
  {\bibfnamefont {Salvatore}\ \bibnamefont {Mandr{\`a}}}, \bibinfo {author}
  {\bibfnamefont {Jarrod~R.}\ \bibnamefont {McClean}}, \bibinfo {author}
  {\bibfnamefont {Matthew}\ \bibnamefont {McEwen}}, \bibinfo {author}
  {\bibfnamefont {Anthony}\ \bibnamefont {Megrant}}, \bibinfo {author}
  {\bibfnamefont {Xiao}\ \bibnamefont {Mi}}, \bibinfo {author} {\bibfnamefont
  {Kristel}\ \bibnamefont {Michielsen}}, \bibinfo {author} {\bibfnamefont
  {Masoud}\ \bibnamefont {Mohseni}}, \bibinfo {author} {\bibfnamefont {Josh}\
  \bibnamefont {Mutus}}, \bibinfo {author} {\bibfnamefont {Ofer}\ \bibnamefont
  {Naaman}}, \bibinfo {author} {\bibfnamefont {Matthew}\ \bibnamefont
  {Neeley}}, \bibinfo {author} {\bibfnamefont {Charles}\ \bibnamefont {Neill}},
  \bibinfo {author} {\bibfnamefont {Murphy~Yuezhen}\ \bibnamefont {Niu}},
  \bibinfo {author} {\bibfnamefont {Eric}\ \bibnamefont {Ostby}}, \bibinfo
  {author} {\bibfnamefont {Andre}\ \bibnamefont {Petukhov}}, \bibinfo {author}
  {\bibfnamefont {John~C.}\ \bibnamefont {Platt}}, \bibinfo {author}
  {\bibfnamefont {Chris}\ \bibnamefont {Quintana}}, \bibinfo {author}
  {\bibfnamefont {Eleanor~G.}\ \bibnamefont {Rieffel}}, \bibinfo {author}
  {\bibfnamefont {Pedram}\ \bibnamefont {Roushan}}, \bibinfo {author}
  {\bibfnamefont {Nicholas~C.}\ \bibnamefont {Rubin}}, \bibinfo {author}
  {\bibfnamefont {Daniel}\ \bibnamefont {Sank}}, \bibinfo {author}
  {\bibfnamefont {Kevin~J.}\ \bibnamefont {Satzinger}}, \bibinfo {author}
  {\bibfnamefont {Vadim}\ \bibnamefont {Smelyanskiy}}, \bibinfo {author}
  {\bibfnamefont {Kevin~J.}\ \bibnamefont {Sung}}, \bibinfo {author}
  {\bibfnamefont {Matthew~D.}\ \bibnamefont {Trevithick}}, \bibinfo {author}
  {\bibfnamefont {Amit}\ \bibnamefont {Vainsencher}}, \bibinfo {author}
  {\bibfnamefont {Benjamin}\ \bibnamefont {Villalonga}}, \bibinfo {author}
  {\bibfnamefont {Theodore}\ \bibnamefont {White}}, \bibinfo {author}
  {\bibfnamefont {Z.~Jamie}\ \bibnamefont {Yao}}, \bibinfo {author}
  {\bibfnamefont {Ping}\ \bibnamefont {Yeh}}, \bibinfo {author} {\bibfnamefont
  {Adam}\ \bibnamefont {Zalcman}}, \bibinfo {author} {\bibfnamefont {Hartmut}\
  \bibnamefont {Neven}}, \ and\ \bibinfo {author} {\bibfnamefont {John~M.}\
  \bibnamefont {Martinis}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Quantum supremacy using a programmable superconducting processor},}\ }\href
  {\doibase 10.1038/s41586-019-1666-5} {\bibfield  {journal} {\bibinfo
  {journal} {Nature}\ }\textbf {\bibinfo {volume} {574}},\ \bibinfo {pages}
  {505--510} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cincio}\ \emph {et~al.}(2018)\citenamefont {Cincio},
  \citenamefont {Suba{\c{s}}{\i}}, \citenamefont {Sornborger},\ and\
  \citenamefont {Coles}}]{cincio2018learning}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont
  {Cincio}}, \bibinfo {author} {\bibfnamefont {Yi{\u{g}}it}\ \bibnamefont
  {Suba{\c{s}}{\i}}}, \bibinfo {author} {\bibfnamefont {Andrew~T}\ \bibnamefont
  {Sornborger}}, \ and\ \bibinfo {author} {\bibfnamefont {Patrick~J}\
  \bibnamefont {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Learning the quantum algorithm for state overlap},}\ }\href {\doibase
  10.1088/1367-2630/aae94a} {\bibfield  {journal} {\bibinfo  {journal} {New
  Journal of Physics}\ }\textbf {\bibinfo {volume} {20}},\ \bibinfo {pages}
  {113022} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Tranter}\ \emph {et~al.}(2018)\citenamefont
  {Tranter}, \citenamefont {Slatyer}, \citenamefont {Hush}, \citenamefont
  {Leung}, \citenamefont {Everett}, \citenamefont {Paul}, \citenamefont
  {Vernaz-Gris}, \citenamefont {Lam}, \citenamefont {Buchler},\ and\
  \citenamefont {Campbell}}]{tranter2018multiparameter}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Aaron~D}\ \bibnamefont
  {Tranter}}, \bibinfo {author} {\bibfnamefont {Harry~J}\ \bibnamefont
  {Slatyer}}, \bibinfo {author} {\bibfnamefont {Michael~R}\ \bibnamefont
  {Hush}}, \bibinfo {author} {\bibfnamefont {Anthony~C}\ \bibnamefont {Leung}},
  \bibinfo {author} {\bibfnamefont {Jesse~L}\ \bibnamefont {Everett}}, \bibinfo
  {author} {\bibfnamefont {Karun~V}\ \bibnamefont {Paul}}, \bibinfo {author}
  {\bibfnamefont {Pierre}\ \bibnamefont {Vernaz-Gris}}, \bibinfo {author}
  {\bibfnamefont {Ping~Koy}\ \bibnamefont {Lam}}, \bibinfo {author}
  {\bibfnamefont {Ben~C}\ \bibnamefont {Buchler}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Geoff~T}\ \bibnamefont {Campbell}},\ }\bibfield  {title}
  {\enquote {\bibinfo {title} {Multiparameter optimisation of a magneto-optical
  trap using deep learning},}\ }\href {\doibase 10.1038/s41467-018-06847-1}
  {\bibfield  {journal} {\bibinfo  {journal} {Nature communications}\ }\textbf
  {\bibinfo {volume} {9}},\ \bibinfo {pages} {1--8} (\bibinfo {year}
  {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kaubruegger}\ \emph {et~al.}(2021)\citenamefont
  {Kaubruegger}, \citenamefont {Vasilyev}, \citenamefont {Schulte},
  \citenamefont {Hammerer},\ and\ \citenamefont
  {Zoller}}]{kaubruegger2021quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Raphael}\ \bibnamefont
  {Kaubruegger}}, \bibinfo {author} {\bibfnamefont {Denis~V}\ \bibnamefont
  {Vasilyev}}, \bibinfo {author} {\bibfnamefont {Marius}\ \bibnamefont
  {Schulte}}, \bibinfo {author} {\bibfnamefont {Klemens}\ \bibnamefont
  {Hammerer}}, \ and\ \bibinfo {author} {\bibfnamefont {Peter}\ \bibnamefont
  {Zoller}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  variational optimization of ramsey interferometry and atomic clocks},}\
  }\href {\doibase 10.1103/PhysRevX.11.041045} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review X}\ }\textbf {\bibinfo {volume} {11}},\ \bibinfo
  {pages} {041045} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cong}\ \emph {et~al.}(2019)\citenamefont {Cong},
  \citenamefont {Choi},\ and\ \citenamefont {Lukin}}]{cong2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Iris}\ \bibnamefont
  {Cong}}, \bibinfo {author} {\bibfnamefont {Soonwon}\ \bibnamefont {Choi}}, \
  and\ \bibinfo {author} {\bibfnamefont {Mikhail~D}\ \bibnamefont {Lukin}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum convolutional neural
  networks},}\ }\href {\doibase 10.1038/s41567-019-0648-8} {\bibfield
  {journal} {\bibinfo  {journal} {Nature Physics}\ }\textbf {\bibinfo {volume}
  {15}},\ \bibinfo {pages} {1273--1278} (\bibinfo {year} {2019})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Tang}(2019)}]{tang2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Ewin}\ \bibnamefont
  {Tang}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {A quantum-inspired
  classical algorithm for recommendation systems},}\ }in\ \href {\doibase
  10.1145/3313276.3316310} {\emph {\bibinfo {booktitle} {Proceedings of the
  51st Annual ACM SIGACT Symposium on Theory of Computing}}}\ (\bibinfo {year}
  {2019})\ pp.\ \bibinfo {pages} {217--228}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Huang}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Huang}, \citenamefont {Broughton},
  \citenamefont {Mohseni}, \citenamefont {Babbush}, \citenamefont {Boixo},
  \citenamefont {Neven},\ and\ \citenamefont {McClean}}]{huang2021power}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {Michael}\
  \bibnamefont {Broughton}}, \bibinfo {author} {\bibfnamefont {Masoud}\
  \bibnamefont {Mohseni}}, \bibinfo {author} {\bibfnamefont {Ryan}\
  \bibnamefont {Babbush}}, \bibinfo {author} {\bibfnamefont {Sergio}\
  \bibnamefont {Boixo}}, \bibinfo {author} {\bibfnamefont {Hartmut}\
  \bibnamefont {Neven}}, \ and\ \bibinfo {author} {\bibfnamefont {Jarrod~R}\
  \bibnamefont {McClean}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Power of data in quantum machine learning},}\ }\href {\doibase
  10.1038/s41467-021-22539-9} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Communications}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages}
  {1--9} (\bibinfo {year} {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Banchi}\ \emph {et~al.}(2021)\citenamefont {Banchi},
  \citenamefont {Pereira},\ and\ \citenamefont
  {Pirandola}}]{banchi2021generalization}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Leonardo}\
  \bibnamefont {Banchi}}, \bibinfo {author} {\bibfnamefont {Jason}\
  \bibnamefont {Pereira}}, \ and\ \bibinfo {author} {\bibfnamefont {Stefano}\
  \bibnamefont {Pirandola}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Generalization in quantum machine learning: A quantum information
  standpoint},}\ }\href {\doibase 10.1103/PRXQuantum.2.040321} {\bibfield
  {journal} {\bibinfo  {journal} {PRX Quantum}\ }\textbf {\bibinfo {volume}
  {2}},\ \bibinfo {pages} {040321} (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Degen}\ \emph {et~al.}(2017)\citenamefont {Degen},
  \citenamefont {Reinhard},\ and\ \citenamefont
  {Cappellaro}}]{degen2017quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.~L.}\ \bibnamefont
  {Degen}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Reinhard}}, \
  and\ \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Cappellaro}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum sensing},}\ }\href
  {\doibase 10.1103/RevModPhys.89.035002} {\bibfield  {journal} {\bibinfo
  {journal} {Rev. Mod. Phys.}\ }\textbf {\bibinfo {volume} {89}},\ \bibinfo
  {pages} {035002} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Giovannetti}\ \emph {et~al.}(2011)\citenamefont
  {Giovannetti}, \citenamefont {Lloyd},\ and\ \citenamefont
  {Maccone}}]{giovannetti2011advances}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Vittorio}\
  \bibnamefont {Giovannetti}}, \bibinfo {author} {\bibfnamefont {Seth}\
  \bibnamefont {Lloyd}}, \ and\ \bibinfo {author} {\bibfnamefont {Lorenzo}\
  \bibnamefont {Maccone}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Advances in quantum metrology},}\ }\href
  {https://www.nature.com/articles/nphoton.2011.35} {\bibfield  {journal}
  {\bibinfo  {journal} {Nat. Photonics}\ }\textbf {\bibinfo {volume} {5}},\
  \bibinfo {pages} {222--229} (\bibinfo {year} {2011})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Chiribella}\ \emph {et~al.}(2009)\citenamefont
  {Chiribella}, \citenamefont {DAriano},\ and\ \citenamefont
  {Perinotti}}]{chiribella2009theoretical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Giulio}\ \bibnamefont
  {Chiribella}}, \bibinfo {author} {\bibfnamefont {Giacomo~Mauro}\ \bibnamefont
  {DAriano}}, \ and\ \bibinfo {author} {\bibfnamefont {Paolo}\ \bibnamefont
  {Perinotti}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Theoretical
  framework for quantum networks},}\ }\href {\doibase
  10.1103/PhysRevA.80.022339} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review A}\ }\textbf {\bibinfo {volume} {80}},\ \bibinfo {pages}
  {022339} (\bibinfo {year} {2009})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {D'Alessandro}(2007)}]{dalessandro2010introduction}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {D'Alessandro}},\ }\href {https://books.google.sm/books?id=HbMYmAEACAAJ}
  {\emph {\bibinfo {title} {Introduction to Quantum Control and Dynamics}}},\
  Chapman \& Hall/CRC Applied Mathematics \& Nonlinear Science\ (\bibinfo
  {publisher} {Taylor \& Francis},\ \bibinfo {year} {2007})\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Verdon-Akzam}(2020)}]{verdon2020qadi}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Guillaume}\
  \bibnamefont {Verdon-Akzam}},\ }\href@noop {} {\enquote {\bibinfo {title}
  {{Quantum Analog-digital Interconversion For Encoding And Decoding Quantum
  Signals}},}\ } (\bibinfo {year} {2020}),\ \bibinfo {note} {{ United States
  Patent Application No. 17063595.}}\BibitemShut {Stop}%
\bibitem [{\citenamefont {Rebentrost}\ \emph {et~al.}(2014)\citenamefont
  {Rebentrost}, \citenamefont {Mohseni},\ and\ \citenamefont
  {Lloyd}}]{rebentrost2014quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Patrick}\ \bibnamefont
  {Rebentrost}}, \bibinfo {author} {\bibfnamefont {Masoud}\ \bibnamefont
  {Mohseni}}, \ and\ \bibinfo {author} {\bibfnamefont {Seth}\ \bibnamefont
  {Lloyd}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum support
  vector machine for big data classification},}\ }\href {\doibase
  10.1103/PhysRevLett.113.130503} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical review letters}\ }\textbf {\bibinfo {volume} {113}},\ \bibinfo
  {pages} {130503} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ and\ \citenamefont
  {Killoran}(2019)}]{schuld2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont
  {Schuld}}\ and\ \bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont
  {Killoran}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  machine learning in feature hilbert spaces},}\ }\href {\doibase
  10.1103/PhysRevLett.122.040504} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical review letters}\ }\textbf {\bibinfo {volume} {122}},\ \bibinfo
  {pages} {040504} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lloyd}\ \emph {et~al.}(2020)\citenamefont {Lloyd},
  \citenamefont {Schuld}, \citenamefont {Ijaz}, \citenamefont {Izaac},\ and\
  \citenamefont {Killoran}}]{lloyd2020quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Seth}\ \bibnamefont
  {Lloyd}}, \bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont {Schuld}},
  \bibinfo {author} {\bibfnamefont {Aroosa}\ \bibnamefont {Ijaz}}, \bibinfo
  {author} {\bibfnamefont {Josh}\ \bibnamefont {Izaac}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Nathan}\ \bibnamefont {Killoran}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Quantum embeddings for machine
  learning},}\ }\href {https://arxiv.org/abs/2001.03622} {\bibfield  {journal}
  {\bibinfo  {journal} {arXiv preprint arXiv:2001.03622}\ } (\bibinfo {year}
  {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ \emph {et~al.}(2021)\citenamefont {Schuld},
  \citenamefont {Sweke},\ and\ \citenamefont {Meyer}}]{schuld2021effect}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont
  {Schuld}}, \bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont {Sweke}}, \
  and\ \bibinfo {author} {\bibfnamefont {Johannes~Jakob}\ \bibnamefont
  {Meyer}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Effect of data
  encoding on the expressive power of variational quantum-machine-learning
  models},}\ }\href {\doibase 10.1103/PhysRevA.103.032430} {\bibfield
  {journal} {\bibinfo  {journal} {Physical Review A}\ }\textbf {\bibinfo
  {volume} {103}},\ \bibinfo {pages} {032430} (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Roffe}(2019)}]{roffe2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Joschka}\ \bibnamefont
  {Roffe}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum error
  correction: an introductory guide},}\ }\href {\doibase
  10.1080/00107514.2019.1667078} {\bibfield  {journal} {\bibinfo  {journal}
  {Contemporary Physics}\ }\textbf {\bibinfo {volume} {60}},\ \bibinfo {pages}
  {226--245} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Shor}(1995)}]{shor1995scheme}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Peter~W}\ \bibnamefont
  {Shor}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Scheme for
  reducing decoherence in quantum computer memory},}\ }\href {\doibase
  https://doi.org/10.1103/PhysRevA.52.R2493} {\bibfield  {journal} {\bibinfo
  {journal} {Physical review A}\ }\textbf {\bibinfo {volume} {52}},\ \bibinfo
  {pages} {R2493} (\bibinfo {year} {1995})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Havl{\'\i}{\v{c}}ek}\ \emph
  {et~al.}(2019)\citenamefont {Havl{\'\i}{\v{c}}ek}, \citenamefont
  {C{\'o}rcoles}, \citenamefont {Temme}, \citenamefont {Harrow}, \citenamefont
  {Kandala}, \citenamefont {Chow},\ and\ \citenamefont
  {Gambetta}}]{havlivcek2019supervised}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Vojt{\v{e}}ch}\
  \bibnamefont {Havl{\'\i}{\v{c}}ek}}, \bibinfo {author} {\bibfnamefont
  {Antonio~D}\ \bibnamefont {C{\'o}rcoles}}, \bibinfo {author} {\bibfnamefont
  {Kristan}\ \bibnamefont {Temme}}, \bibinfo {author} {\bibfnamefont {Aram~W}\
  \bibnamefont {Harrow}}, \bibinfo {author} {\bibfnamefont {Abhinav}\
  \bibnamefont {Kandala}}, \bibinfo {author} {\bibfnamefont {Jerry~M}\
  \bibnamefont {Chow}}, \ and\ \bibinfo {author} {\bibfnamefont {Jay~M}\
  \bibnamefont {Gambetta}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Supervised learning with quantum-enhanced feature spaces},}\ }\href
  {\doibase 10.1038/s41586-019-0980-2} {\bibfield  {journal} {\bibinfo
  {journal} {Nature}\ }\textbf {\bibinfo {volume} {567}},\ \bibinfo {pages}
  {209--212} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liu}\ \emph {et~al.}(2021)\citenamefont {Liu},
  \citenamefont {Arunachalam},\ and\ \citenamefont {Temme}}]{liu2021rigorous}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Yunchao}\ \bibnamefont
  {Liu}}, \bibinfo {author} {\bibfnamefont {Srinivasan}\ \bibnamefont
  {Arunachalam}}, \ and\ \bibinfo {author} {\bibfnamefont {Kristan}\
  \bibnamefont {Temme}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {A
  rigorous and robust quantum speed-up in supervised machine learning},}\
  }\href {\doibase 10.1038/s41567-021-01287-z} {\bibfield  {journal} {\bibinfo
  {journal} {Nature Physics}\ ,\ \bibinfo {pages} {1--5}} (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}(2021)}]{schuld2021quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont
  {Schuld}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum machine
  learning models are kernel methods},}\ }\href
  {https://arxiv.org/abs/2101.11020} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2101.11020}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {{Otterbach}}\ \emph {et~al.}(2017)\citenamefont
  {{Otterbach}}, \citenamefont {{Manenti}}, \citenamefont {{Alidoust}},
  \citenamefont {{Bestwick}}, \citenamefont {{Block}}, \citenamefont {{Bloom}},
  \citenamefont {{Caldwell}}, \citenamefont {{Didier}}, \citenamefont
  {{Schuyler Fried}}, \citenamefont {{Hong}}, \citenamefont {{Karalekas}},
  \citenamefont {{Osborn}}, \citenamefont {{Papageorge}}, \citenamefont
  {{Peterson}}, \citenamefont {{Prawiroatmodjo}}, \citenamefont {{Rubin}},
  \citenamefont {{Ryan}}, \citenamefont {{Scarabelli}}, \citenamefont
  {{Scheer}}, \citenamefont {{Sete}}, \citenamefont {{Sivarajah}},
  \citenamefont {{Smith}}, \citenamefont {{Staley}}, \citenamefont {{Tezak}},
  \citenamefont {{Zeng}}, \citenamefont {{Hudson}}, \citenamefont {{Johnson}},
  \citenamefont {{Reagor}}, \citenamefont {{da Silva}},\ and\ \citenamefont
  {{Rigetti}}}]{otterbach2017unsupervised}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~S.}\ \bibnamefont
  {{Otterbach}}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont
  {{Manenti}}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {{Alidoust}}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {{Bestwick}}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {{Block}}},
  \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {{Bloom}}}, \bibinfo
  {author} {\bibfnamefont {S.}~\bibnamefont {{Caldwell}}}, \bibinfo {author}
  {\bibfnamefont {N.}~\bibnamefont {{Didier}}}, \bibinfo {author}
  {\bibfnamefont {E.}~\bibnamefont {{Schuyler Fried}}}, \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {{Hong}}}, \bibinfo {author} {\bibfnamefont
  {P.}~\bibnamefont {{Karalekas}}}, \bibinfo {author} {\bibfnamefont {C.~B.}\
  \bibnamefont {{Osborn}}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {{Papageorge}}}, \bibinfo {author} {\bibfnamefont {E.~C.}\ \bibnamefont
  {{Peterson}}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont
  {{Prawiroatmodjo}}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont
  {{Rubin}}}, \bibinfo {author} {\bibfnamefont {Colm~A.}\ \bibnamefont
  {{Ryan}}}, \bibinfo {author} {\bibfnamefont {D.}~\bibnamefont
  {{Scarabelli}}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {{Scheer}}}, \bibinfo {author} {\bibfnamefont {E.~A.}\ \bibnamefont
  {{Sete}}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {{Sivarajah}}},
  \bibinfo {author} {\bibfnamefont {Robert~S.}\ \bibnamefont {{Smith}}},
  \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {{Staley}}}, \bibinfo
  {author} {\bibfnamefont {N.}~\bibnamefont {{Tezak}}}, \bibinfo {author}
  {\bibfnamefont {W.~J.}\ \bibnamefont {{Zeng}}}, \bibinfo {author}
  {\bibfnamefont {A.}~\bibnamefont {{Hudson}}}, \bibinfo {author}
  {\bibfnamefont {Blake~R.}\ \bibnamefont {{Johnson}}}, \bibinfo {author}
  {\bibfnamefont {M.}~\bibnamefont {{Reagor}}}, \bibinfo {author}
  {\bibfnamefont {M.~P.}\ \bibnamefont {{da Silva}}}, \ and\ \bibinfo {author}
  {\bibfnamefont {C.}~\bibnamefont {{Rigetti}}},\ }\bibfield  {title} {\enquote
  {\bibinfo {title} {Unsupervised machine learning on a hybrid quantum
  computer},}\ }\href {https://arxiv.org/abs/1712.05771} {\bibfield  {journal}
  {\bibinfo  {journal} {arXiv preprint arXiv:1712.05771}\ } (\bibinfo {year}
  {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kerenidis}\ \emph {et~al.}(2019)\citenamefont
  {Kerenidis}, \citenamefont {Landman}, \citenamefont {Luongo},\ and\
  \citenamefont {Prakash}}]{kerenidis2019q}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Iordanis}\
  \bibnamefont {Kerenidis}}, \bibinfo {author} {\bibfnamefont {Jonas}\
  \bibnamefont {Landman}}, \bibinfo {author} {\bibfnamefont {Alessandro}\
  \bibnamefont {Luongo}}, \ and\ \bibinfo {author} {\bibfnamefont {Anupam}\
  \bibnamefont {Prakash}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {q-means: A quantum algorithm for unsupervised machine learning},}\ }\href
  {https://proceedings.neurips.cc/paper/2019/file/16026d60ff9b54410b3435b403afd226-Paper.pdf}
  {\bibfield  {journal} {\bibinfo  {journal} {Advances in Neural Information
  Processing Systems}\ }\textbf {\bibinfo {volume} {32}} (\bibinfo {year}
  {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Saggio}}\ \emph {et~al.}(2021)\citenamefont
  {{Saggio}}, \citenamefont {{Asenbeck}}, \citenamefont {{Hamann}},
  \citenamefont {{Str{\"o}mberg}}, \citenamefont {{Schiansky}}, \citenamefont
  {{Dunjko}}, \citenamefont {{Friis}}, \citenamefont {{Harris}}, \citenamefont
  {{Hochberg}}, \citenamefont {{Englund}}, \citenamefont {{W{\"o}lk}},
  \citenamefont {{Briegel}},\ and\ \citenamefont
  {{Walther}}}]{saggio2021experimental}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {{Saggio}}}, \bibinfo {author} {\bibfnamefont {B.~E.}\ \bibnamefont
  {{Asenbeck}}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont
  {{Hamann}}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont
  {{Str{\"o}mberg}}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {{Schiansky}}}, \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont
  {{Dunjko}}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {{Friis}}},
  \bibinfo {author} {\bibfnamefont {N.~C.}\ \bibnamefont {{Harris}}}, \bibinfo
  {author} {\bibfnamefont {M.}~\bibnamefont {{Hochberg}}}, \bibinfo {author}
  {\bibfnamefont {D.}~\bibnamefont {{Englund}}}, \bibinfo {author}
  {\bibfnamefont {S.}~\bibnamefont {{W{\"o}lk}}}, \bibinfo {author}
  {\bibfnamefont {H.~J.}\ \bibnamefont {{Briegel}}}, \ and\ \bibinfo {author}
  {\bibfnamefont {P.}~\bibnamefont {{Walther}}},\ }\bibfield  {title} {\enquote
  {\bibinfo {title} {Experimental quantum speed-up in reinforcement learning
  agents},}\ }\href {\doibase 10.1038/s41586-021-03242-7} {\bibfield  {journal}
  {\bibinfo  {journal} {Nature}\ }\textbf {\bibinfo {volume} {591}},\ \bibinfo
  {pages} {229--233} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Skolik}\ \emph {et~al.}(2021)\citenamefont {Skolik},
  \citenamefont {Jerbi},\ and\ \citenamefont {Dunjko}}]{skolik2021quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Andrea}\ \bibnamefont
  {Skolik}}, \bibinfo {author} {\bibfnamefont {Sofiene}\ \bibnamefont {Jerbi}},
  \ and\ \bibinfo {author} {\bibfnamefont {Vedran}\ \bibnamefont {Dunjko}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum agents in the gym: a
  variational quantum algorithm for deep q-learning},}\ }\href
  {https://arxiv.org/abs/2103.15084} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2103.15084}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Huang}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Huang}, \citenamefont {Broughton},
  \citenamefont {Cotler}, \citenamefont {Chen}, \citenamefont {Li},
  \citenamefont {Mohseni}, \citenamefont {Neven}, \citenamefont {Babbush},
  \citenamefont {Kueng}, \citenamefont {Preskill},\ and\ \citenamefont
  {McClean}}]{huang2021quantumadvantage}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {Michael}\
  \bibnamefont {Broughton}}, \bibinfo {author} {\bibfnamefont {Jordan}\
  \bibnamefont {Cotler}}, \bibinfo {author} {\bibfnamefont {Sitan}\
  \bibnamefont {Chen}}, \bibinfo {author} {\bibfnamefont {Jerry}\ \bibnamefont
  {Li}}, \bibinfo {author} {\bibfnamefont {Masoud}\ \bibnamefont {Mohseni}},
  \bibinfo {author} {\bibfnamefont {Hartmut}\ \bibnamefont {Neven}}, \bibinfo
  {author} {\bibfnamefont {Ryan}\ \bibnamefont {Babbush}}, \bibinfo {author}
  {\bibfnamefont {Richard}\ \bibnamefont {Kueng}}, \bibinfo {author}
  {\bibfnamefont {John}\ \bibnamefont {Preskill}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Jarrod~R.}\ \bibnamefont {McClean}},\ }\bibfield  {title}
  {\enquote {\bibinfo {title} {Quantum advantage in learning from
  experiments},}\ }\href {https://arxiv.org/abs/2112.00778} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2112.00778}\ } (\bibinfo
  {year} {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {LaRose}\ and\ \citenamefont
  {Coyle}(2020)}]{larose2020robust}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {LaRose}}\ and\ \bibinfo {author} {\bibfnamefont {Brian}\ \bibnamefont
  {Coyle}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Robust data
  encodings for quantum classifiers},}\ }\href {\doibase
  10.1103/PhysRevA.102.032420} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review A}\ }\textbf {\bibinfo {volume} {102}},\ \bibinfo {pages}
  {032420} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Caro}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Caro}, \citenamefont {Huang},
  \citenamefont {Cerezo}, \citenamefont {Sharma}, \citenamefont {Sornborger},
  \citenamefont {Cincio},\ and\ \citenamefont
  {Coles}}]{caro2021generalization}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Matthias~C.}\
  \bibnamefont {Caro}}, \bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Cerezo}}, \bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont {Sharma}},
  \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont {Sornborger}},
  \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\
  \bibinfo {author} {\bibfnamefont {Patrick~J.}\ \bibnamefont {Coles}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Generalization in quantum
  machine learning from few training data},}\ }\href
  {https://arxiv.org/abs/2111.05292} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2111.05292}\ } (\bibinfo {year}
  {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Caro}\ \emph {et~al.}()\citenamefont {Caro},
  \citenamefont {Huang}, \citenamefont {Ezzell}, \citenamefont {Gibbs},
  \citenamefont {Sornborger}, \citenamefont {Cincio}, \citenamefont {Coles},\
  and\ \citenamefont {Holmes}}]{caro2022outofdistribution}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Matthias~C.}\
  \bibnamefont {Caro}}, \bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {Nicholas}\
  \bibnamefont {Ezzell}}, \bibinfo {author} {\bibfnamefont {Joe}\ \bibnamefont
  {Gibbs}}, \bibinfo {author} {\bibfnamefont {Andrew~T.}\ \bibnamefont
  {Sornborger}}, \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont
  {Cincio}}, \bibinfo {author} {\bibfnamefont {Patrick~J.}\ \bibnamefont
  {Coles}}, \ and\ \bibinfo {author} {\bibfnamefont {Zoe}\ \bibnamefont
  {Holmes}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Out-of-distribution generalization for learning quantum dynamics},}\ }\href
  {https://arxiv.org/abs/2204.10268} {\bibinfo  {journal} {arXiv preprint
  arXiv:2204.10268}\ }\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Caro}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Caro}, \citenamefont {Gil-Fuster},
  \citenamefont {Meyer}, \citenamefont {Eisert},\ and\ \citenamefont
  {Sweke}}]{caro2021encodingdependent}%
  \BibitemOpen
\bibfield  {journal} {  }\bibfield  {author} {\bibinfo {author} {\bibfnamefont
  {Matthias~C.}\ \bibnamefont {Caro}}, \bibinfo {author} {\bibfnamefont
  {Elies}\ \bibnamefont {Gil-Fuster}}, \bibinfo {author} {\bibfnamefont
  {Johannes~Jakob}\ \bibnamefont {Meyer}}, \bibinfo {author} {\bibfnamefont
  {Jens}\ \bibnamefont {Eisert}}, \ and\ \bibinfo {author} {\bibfnamefont
  {Ryan}\ \bibnamefont {Sweke}},\ }\bibfield  {title} {\enquote {\bibinfo
  {title} {Encoding-dependent generalization bounds for parametrized quantum
  circuits},}\ }\href {\doibase 10.22331/q-2021-11-17-582} {\bibfield
  {journal} {\bibinfo  {journal} {{Quantum}}\ }\textbf {\bibinfo {volume}
  {5}},\ \bibinfo {pages} {582} (\bibinfo {year}
  {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Cerezo}, \citenamefont
  {Arrasmith}, \citenamefont {Babbush}, \citenamefont {Benjamin}, \citenamefont
  {Endo}, \citenamefont {Fujii}, \citenamefont {McClean}, \citenamefont
  {Mitarai}, \citenamefont {Yuan}, \citenamefont {Cincio},\ and\ \citenamefont
  {Coles}}]{cerezo2020variationalreview}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Cerezo}}, \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {Babbush}}, \bibinfo {author} {\bibfnamefont {Simon~C}\ \bibnamefont
  {Benjamin}}, \bibinfo {author} {\bibfnamefont {Suguru}\ \bibnamefont {Endo}},
  \bibinfo {author} {\bibfnamefont {Keisuke}\ \bibnamefont {Fujii}}, \bibinfo
  {author} {\bibfnamefont {Jarrod~R}\ \bibnamefont {McClean}}, \bibinfo
  {author} {\bibfnamefont {Kosuke}\ \bibnamefont {Mitarai}}, \bibinfo {author}
  {\bibfnamefont {Xiao}\ \bibnamefont {Yuan}}, \bibinfo {author} {\bibfnamefont
  {Lukasz}\ \bibnamefont {Cincio}}, \ and\ \bibinfo {author} {\bibfnamefont
  {Patrick~J.}\ \bibnamefont {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo
  {title} {Variational quantum algorithms},}\ }\href {\doibase
  10.1038/s42254-021-00348-9} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Reviews Physics}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages}
  {625644} (\bibinfo {year} {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wan}\ \emph {et~al.}(2017)\citenamefont {Wan},
  \citenamefont {Dahlsten}, \citenamefont {Kristj{\'a}nsson}, \citenamefont
  {Gardner},\ and\ \citenamefont {Kim}}]{wan2017quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Kwok~Ho}\ \bibnamefont
  {Wan}}, \bibinfo {author} {\bibfnamefont {Oscar}\ \bibnamefont {Dahlsten}},
  \bibinfo {author} {\bibfnamefont {Hl{\'e}r}\ \bibnamefont
  {Kristj{\'a}nsson}}, \bibinfo {author} {\bibfnamefont {Robert}\ \bibnamefont
  {Gardner}}, \ and\ \bibinfo {author} {\bibfnamefont {MS}~\bibnamefont
  {Kim}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  generalisation of feedforward neural networks},}\ }\href {\doibase
  10.1038/s41534-017-0032-4} {\bibfield  {journal} {\bibinfo  {journal} {npj
  Quantum information}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages}
  {1--8} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Beer}\ \emph {et~al.}(2020)\citenamefont {Beer},
  \citenamefont {Bondarenko}, \citenamefont {Farrelly}, \citenamefont
  {Osborne}, \citenamefont {Salzmann}, \citenamefont {Scheiermann},\ and\
  \citenamefont {Wolf}}]{beer2020training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Kerstin}\ \bibnamefont
  {Beer}}, \bibinfo {author} {\bibfnamefont {Dmytro}\ \bibnamefont
  {Bondarenko}}, \bibinfo {author} {\bibfnamefont {Terry}\ \bibnamefont
  {Farrelly}}, \bibinfo {author} {\bibfnamefont {Tobias~J.}\ \bibnamefont
  {Osborne}}, \bibinfo {author} {\bibfnamefont {Robert}\ \bibnamefont
  {Salzmann}}, \bibinfo {author} {\bibfnamefont {Daniel}\ \bibnamefont
  {Scheiermann}}, \ and\ \bibinfo {author} {\bibfnamefont {Ramona}\
  \bibnamefont {Wolf}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Training deep quantum neural networks},}\ }\href {\doibase
  10.1038/s41467-020-14454-2} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Communications}\ }\textbf {\bibinfo {volume} {11}},\ \bibinfo {pages}
  {808} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schuld}\ \emph {et~al.}(2014)\citenamefont {Schuld},
  \citenamefont {Sinayskiy},\ and\ \citenamefont
  {Petruccione}}]{schuld2014quest}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont
  {Schuld}}, \bibinfo {author} {\bibfnamefont {Ilya}\ \bibnamefont
  {Sinayskiy}}, \ and\ \bibinfo {author} {\bibfnamefont {Francesco}\
  \bibnamefont {Petruccione}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {The quest for a quantum neural network},}\ }\href {\doibase
  10.1007/s11128-014-0809-8} {\bibfield  {journal} {\bibinfo  {journal}
  {Quantum Information Processing}\ }\textbf {\bibinfo {volume} {13}},\
  \bibinfo {pages} {2567--2586} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dallaire-Demers}\ and\ \citenamefont
  {Killoran}(2018)}]{dallaire2018quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Pierre-Luc}\
  \bibnamefont {Dallaire-Demers}}\ and\ \bibinfo {author} {\bibfnamefont
  {Nathan}\ \bibnamefont {Killoran}},\ }\bibfield  {title} {\enquote {\bibinfo
  {title} {Quantum generative adversarial networks},}\ }\href {\doibase
  10.1103/PhysRevA.98.012324} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review A}\ }\textbf {\bibinfo {volume} {98}},\ \bibinfo {pages}
  {012324} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Farhi}\ and\ \citenamefont
  {Neven}(2018)}]{farhi2018classification}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Edward}\ \bibnamefont
  {Farhi}}\ and\ \bibinfo {author} {\bibfnamefont {Hartmut}\ \bibnamefont
  {Neven}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Classification
  with quantum neural networks on near term processors},}\ }\href
  {https://arxiv.org/abs/1802.06002} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:1802.06002}\ } (\bibinfo {year} {2018})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Killoran}\ \emph {et~al.}(2019)\citenamefont
  {Killoran}, \citenamefont {Bromley}, \citenamefont {Arrazola}, \citenamefont
  {Schuld}, \citenamefont {Quesada},\ and\ \citenamefont
  {Lloyd}}]{killoran2019continuous}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont
  {Killoran}}, \bibinfo {author} {\bibfnamefont {Thomas~R}\ \bibnamefont
  {Bromley}}, \bibinfo {author} {\bibfnamefont {Juan~Miguel}\ \bibnamefont
  {Arrazola}}, \bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont
  {Schuld}}, \bibinfo {author} {\bibfnamefont {Nicol{\'a}s}\ \bibnamefont
  {Quesada}}, \ and\ \bibinfo {author} {\bibfnamefont {Seth}\ \bibnamefont
  {Lloyd}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Continuous-variable quantum neural networks},}\ }\href {\doibase
  10.1103/PhysRevResearch.1.033063} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Research}\ }\textbf {\bibinfo {volume} {1}},\ \bibinfo
  {pages} {033063} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bausch}(2020)}]{bausch2020recurrent}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Johannes}\
  \bibnamefont {Bausch}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Recurrent quantum neural networks},}\ }\href
  {https://arxiv.org/abs/2006.14619} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2006.14619}\ } (\bibinfo {year} {2020})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {{Broughton}}\ \emph {et~al.}(2020)\citenamefont
  {{Broughton}}, \citenamefont {{Verdon}}, \citenamefont {{McCourt}},
  \citenamefont {{Martinez}}, \citenamefont {{Yoo}}, \citenamefont {{Isakov}},
  \citenamefont {{Massey}}, \citenamefont {{Halavati}}, \citenamefont {{Yuezhen
  Niu}}, \citenamefont {{Zlokapa}}, \citenamefont {{Peters}}, \citenamefont
  {{Lockwood}}, \citenamefont {{Skolik}}, \citenamefont {{Jerbi}},
  \citenamefont {{Dunjko}}, \citenamefont {{Leib}}, \citenamefont {{Streif}},
  \citenamefont {{Von Dollen}}, \citenamefont {{Chen}}, \citenamefont {{Cao}},
  \citenamefont {{Wiersema}}, \citenamefont {{Huang}}, \citenamefont
  {{McClean}}, \citenamefont {{Babbush}}, \citenamefont {{Boixo}},
  \citenamefont {{Bacon}}, \citenamefont {{Ho}}, \citenamefont {{Neven}},\ and\
  \citenamefont {{Mohseni}}}]{broughton2020tensorflow}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Michael}\ \bibnamefont
  {{Broughton}}}, \bibinfo {author} {\bibfnamefont {Guillaume}\ \bibnamefont
  {{Verdon}}}, \bibinfo {author} {\bibfnamefont {Trevor}\ \bibnamefont
  {{McCourt}}}, \bibinfo {author} {\bibfnamefont {Antonio~J.}\ \bibnamefont
  {{Martinez}}}, \bibinfo {author} {\bibfnamefont {Jae~Hyeon}\ \bibnamefont
  {{Yoo}}}, \bibinfo {author} {\bibfnamefont {Sergei~V.}\ \bibnamefont
  {{Isakov}}}, \bibinfo {author} {\bibfnamefont {Philip}\ \bibnamefont
  {{Massey}}}, \bibinfo {author} {\bibfnamefont {Ramin}\ \bibnamefont
  {{Halavati}}}, \bibinfo {author} {\bibfnamefont {Murphy}\ \bibnamefont
  {{Yuezhen Niu}}}, \bibinfo {author} {\bibfnamefont {Alexander}\ \bibnamefont
  {{Zlokapa}}}, \bibinfo {author} {\bibfnamefont {Evan}\ \bibnamefont
  {{Peters}}}, \bibinfo {author} {\bibfnamefont {Owen}\ \bibnamefont
  {{Lockwood}}}, \bibinfo {author} {\bibfnamefont {Andrea}\ \bibnamefont
  {{Skolik}}}, \bibinfo {author} {\bibfnamefont {Sofiene}\ \bibnamefont
  {{Jerbi}}}, \bibinfo {author} {\bibfnamefont {Vedran}\ \bibnamefont
  {{Dunjko}}}, \bibinfo {author} {\bibfnamefont {Martin}\ \bibnamefont
  {{Leib}}}, \bibinfo {author} {\bibfnamefont {Michael}\ \bibnamefont
  {{Streif}}}, \bibinfo {author} {\bibfnamefont {David}\ \bibnamefont {{Von
  Dollen}}}, \bibinfo {author} {\bibfnamefont {Hongxiang}\ \bibnamefont
  {{Chen}}}, \bibinfo {author} {\bibfnamefont {Shuxiang}\ \bibnamefont
  {{Cao}}}, \bibinfo {author} {\bibfnamefont {Roeland}\ \bibnamefont
  {{Wiersema}}}, \bibinfo {author} {\bibfnamefont {Hsin-Yuan}\ \bibnamefont
  {{Huang}}}, \bibinfo {author} {\bibfnamefont {Jarrod~R.}\ \bibnamefont
  {{McClean}}}, \bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {{Babbush}}}, \bibinfo {author} {\bibfnamefont {Sergio}\ \bibnamefont
  {{Boixo}}}, \bibinfo {author} {\bibfnamefont {Dave}\ \bibnamefont {{Bacon}}},
  \bibinfo {author} {\bibfnamefont {Alan~K.}\ \bibnamefont {{Ho}}}, \bibinfo
  {author} {\bibfnamefont {Hartmut}\ \bibnamefont {{Neven}}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Masoud}\ \bibnamefont {{Mohseni}}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Tensorflow quantum: A software framework
  for quantum machine learning},}\ }\href {https://arxiv.org/abs/2003.02989}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2003.02989}\
  } (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Verdon}\ \emph
  {et~al.}(2019{\natexlab{a}})\citenamefont {Verdon}, \citenamefont {Marks},
  \citenamefont {Nanda}, \citenamefont {Leichenauer},\ and\ \citenamefont
  {Hidary}}]{verdon2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Guillaume}\
  \bibnamefont {Verdon}}, \bibinfo {author} {\bibfnamefont {Jacob}\
  \bibnamefont {Marks}}, \bibinfo {author} {\bibfnamefont {Sasha}\ \bibnamefont
  {Nanda}}, \bibinfo {author} {\bibfnamefont {Stefan}\ \bibnamefont
  {Leichenauer}}, \ and\ \bibinfo {author} {\bibfnamefont {Jack}\ \bibnamefont
  {Hidary}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  {H}amiltonian-based models and the variational quantum thermalizer
  algorithm},}\ }\href {https://arxiv.org/abs/1910.02071} {\bibfield  {journal}
  {\bibinfo  {journal} {arXiv preprint arXiv:1910.02071}\ } (\bibinfo {year}
  {2019}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cortes}\ and\ \citenamefont
  {Vapnik}(1995)}]{cortes1995support}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Corinna}\ \bibnamefont
  {Cortes}}\ and\ \bibinfo {author} {\bibfnamefont {Vladimir}\ \bibnamefont
  {Vapnik}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Support-vector
  networks},}\ }\href {\doibase 10.1007/BF00994018} {\bibfield  {journal}
  {\bibinfo  {journal} {Mach. Learn.}\ }\textbf {\bibinfo {volume} {20}},\
  \bibinfo {pages} {273--297} (\bibinfo {year} {1995})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {K{\"u}bler}\ \emph {et~al.}(2021)\citenamefont
  {K{\"u}bler}, \citenamefont {Buchholz},\ and\ \citenamefont
  {Sch{\"o}lkopf}}]{kubler2021inductive}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jonas~M}\ \bibnamefont
  {K{\"u}bler}}, \bibinfo {author} {\bibfnamefont {Simon}\ \bibnamefont
  {Buchholz}}, \ and\ \bibinfo {author} {\bibfnamefont {Bernhard}\ \bibnamefont
  {Sch{\"o}lkopf}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {The
  inductive bias of quantum kernels},}\ }\href
  {https://arxiv.org/abs/2106.03747} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2106.03747}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{Note1()}]{Note1}%
  \BibitemOpen
  \bibinfo {note} {This is due to the fact that extracting information from a
  quantum state requires computing the expectation values of some observable,
  which in practice need to be estimated via measurements on a noisy quantum
  computer. Hence, given a finite number of shots (measurement repetitions),
  these can only be resolved up to some additive errors. Moreover, such
  expectation values will be subject to corruption due to hardware
  noise.}\BibitemShut {Stop}%
\bibitem [{\citenamefont {K{\"u}bler}\ \emph {et~al.}(2020)\citenamefont
  {K{\"u}bler}, \citenamefont {Arrasmith}, \citenamefont {Cincio},\ and\
  \citenamefont {Coles}}]{kubler2020adaptive}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jonas~M}\ \bibnamefont
  {K{\"u}bler}}, \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont
  {Cincio}}, \ and\ \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont
  {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {An adaptive
  optimizer for measurement-frugal variational algorithms},}\ }\href {\doibase
  10.22331/q-2020-05-11-263} {\bibfield  {journal} {\bibinfo  {journal}
  {Quantum}\ }\textbf {\bibinfo {volume} {4}},\ \bibinfo {pages} {263}
  (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Arrasmith}\ \emph {et~al.}(2020)\citenamefont
  {Arrasmith}, \citenamefont {Cincio}, \citenamefont {Somma},\ and\
  \citenamefont {Coles}}]{arrasmith2020operator}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont
  {Cincio}}, \bibinfo {author} {\bibfnamefont {Rolando~D}\ \bibnamefont
  {Somma}}, \ and\ \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont
  {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Operator sampling
  for shot-frugal optimization in variational algorithms},}\ }\href
  {https://arxiv.org/abs/2004.06252} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2004.06252}\ } (\bibinfo {year} {2020})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Gu}\ \emph {et~al.}(2021)\citenamefont {Gu},
  \citenamefont {Lowe}, \citenamefont {Dub}, \citenamefont {Coles},\ and\
  \citenamefont {Arrasmith}}]{gu2021adaptive}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Andi}\ \bibnamefont
  {Gu}}, \bibinfo {author} {\bibfnamefont {Angus}\ \bibnamefont {Lowe}},
  \bibinfo {author} {\bibfnamefont {Pavel~A}\ \bibnamefont {Dub}}, \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Andrew}\ \bibnamefont {Arrasmith}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Adaptive shot allocation for fast
  convergence in variational quantum algorithms},}\ }\href
  {https://arxiv.org/abs/2108.10434} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2108.10434}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Sweke}\ \emph {et~al.}(2020)\citenamefont {Sweke},
  \citenamefont {Wilde}, \citenamefont {Meyer}, \citenamefont {Schuld},
  \citenamefont {F{\"a}hrmann}, \citenamefont {Meynard-Piganeau},\ and\
  \citenamefont {Eisert}}]{sweke2020stochastic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {Sweke}}, \bibinfo {author} {\bibfnamefont {Frederik}\ \bibnamefont {Wilde}},
  \bibinfo {author} {\bibfnamefont {Johannes~Jakob}\ \bibnamefont {Meyer}},
  \bibinfo {author} {\bibfnamefont {Maria}\ \bibnamefont {Schuld}}, \bibinfo
  {author} {\bibfnamefont {Paul~K}\ \bibnamefont {F{\"a}hrmann}}, \bibinfo
  {author} {\bibfnamefont {Barth{\'e}l{\'e}my}\ \bibnamefont
  {Meynard-Piganeau}}, \ and\ \bibinfo {author} {\bibfnamefont {Jens}\
  \bibnamefont {Eisert}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Stochastic gradient descent for hybrid quantum-classical optimization},}\
  }\href {\doibase 10.22331/q-2020-08-31-314} {\bibfield  {journal} {\bibinfo
  {journal} {Quantum}\ }\textbf {\bibinfo {volume} {4}},\ \bibinfo {pages}
  {314} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Stokes}\ \emph {et~al.}(2020)\citenamefont {Stokes},
  \citenamefont {Izaac}, \citenamefont {Killoran},\ and\ \citenamefont
  {Carleo}}]{stokes2020quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {James}\ \bibnamefont
  {Stokes}}, \bibinfo {author} {\bibfnamefont {Josh}\ \bibnamefont {Izaac}},
  \bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont {Killoran}}, \ and\
  \bibinfo {author} {\bibfnamefont {Giuseppe}\ \bibnamefont {Carleo}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum natural gradient},}\
  }\href {\doibase 10.22331/q-2020-05-25-269} {\bibfield  {journal} {\bibinfo
  {journal} {Quantum}\ }\textbf {\bibinfo {volume} {4}},\ \bibinfo {pages}
  {269} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Koczor}\ and\ \citenamefont
  {Benjamin}(2019)}]{koczor2019quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B{\'a}lint}\
  \bibnamefont {Koczor}}\ and\ \bibinfo {author} {\bibfnamefont {Simon~C}\
  \bibnamefont {Benjamin}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Quantum natural gradient generalised to non-unitary circuits},}\ }\href
  {https://arxiv.org/abs/1912.08660} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:1912.08660}\ } (\bibinfo {year} {2019})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Sharma}\ \emph
  {et~al.}(2020{\natexlab{a}})\citenamefont {Sharma}, \citenamefont {Cerezo},
  \citenamefont {Holmes}, \citenamefont {Cincio}, \citenamefont {Sornborger},\
  and\ \citenamefont {Coles}}]{sharma2020reformulation}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont
  {Sharma}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},
  \bibinfo {author} {\bibfnamefont {Zo{\"e}}\ \bibnamefont {Holmes}}, \bibinfo
  {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \bibinfo {author}
  {\bibfnamefont {Andrew}\ \bibnamefont {Sornborger}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield  {title}
  {\enquote {\bibinfo {title} {Reformulation of the no-free-lunch theorem for
  entangled data sets},}\ }\href {https://arxiv.org/abs/2007.04900} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2007.04900}\ } (\bibinfo
  {year} {2020}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Abbas}\ \emph {et~al.}(2021)\citenamefont {Abbas},
  \citenamefont {Sutter}, \citenamefont {Zoufal}, \citenamefont {Lucchi},
  \citenamefont {Figalli},\ and\ \citenamefont {Woerner}}]{abbas2020power}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Amira}\ \bibnamefont
  {Abbas}}, \bibinfo {author} {\bibfnamefont {David}\ \bibnamefont {Sutter}},
  \bibinfo {author} {\bibfnamefont {Christa}\ \bibnamefont {Zoufal}}, \bibinfo
  {author} {\bibfnamefont {Aur{\'e}lien}\ \bibnamefont {Lucchi}}, \bibinfo
  {author} {\bibfnamefont {Alessio}\ \bibnamefont {Figalli}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Stefan}\ \bibnamefont {Woerner}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {The power of quantum neural networks},}\
  }\href {\doibase 10.1038/s43588-021-00084-1} {\bibfield  {journal} {\bibinfo
  {journal} {Nature Computational Science}\ }\textbf {\bibinfo {volume} {1}},\
  \bibinfo {pages} {403--409} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Rosenblatt}(1957)}]{rosenblatt1957perceptron}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Frank}\ \bibnamefont
  {Rosenblatt}},\ }\href
  {https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf} {\emph
  {\bibinfo {title} {The perceptron, a perceiving and recognizing automaton
  Project Para}}}\ (\bibinfo  {publisher} {Cornell Aeronautical Laboratory},\
  \bibinfo {year} {1957})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Haykin}(1994)}]{haykin1994neural}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Simon}\ \bibnamefont
  {Haykin}},\ }\href@noop {} {\emph {\bibinfo {title} {Neural networks: a
  comprehensive foundation}}}\ (\bibinfo  {publisher} {Prentice Hall PTR},\
  \bibinfo {year} {1994})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Rumelhart}\ \emph {et~al.}(1986)\citenamefont
  {Rumelhart}, \citenamefont {Hinton},\ and\ \citenamefont
  {Williams}}]{rumelhart1986learning}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {David~E}\ \bibnamefont
  {Rumelhart}}, \bibinfo {author} {\bibfnamefont {Geoffrey~E}\ \bibnamefont
  {Hinton}}, \ and\ \bibinfo {author} {\bibfnamefont {Ronald~J}\ \bibnamefont
  {Williams}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Learning
  representations by back-propagating errors},}\ }\href {\doibase
  10.1038/323533a0} {\bibfield  {journal} {\bibinfo  {journal} {nature}\
  }\textbf {\bibinfo {volume} {323}},\ \bibinfo {pages} {533--536} (\bibinfo
  {year} {1986})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hubregtsen}\ \emph {et~al.}(2021)\citenamefont
  {Hubregtsen}, \citenamefont {Wierichs}, \citenamefont {Gil-Fuster},
  \citenamefont {Derks}, \citenamefont {Faehrmann},\ and\ \citenamefont
  {Meyer}}]{hubregtsen2021training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Thomas}\ \bibnamefont
  {Hubregtsen}}, \bibinfo {author} {\bibfnamefont {David}\ \bibnamefont
  {Wierichs}}, \bibinfo {author} {\bibfnamefont {Elies}\ \bibnamefont
  {Gil-Fuster}}, \bibinfo {author} {\bibfnamefont {Peter-Jan~HS}\ \bibnamefont
  {Derks}}, \bibinfo {author} {\bibfnamefont {Paul~K}\ \bibnamefont
  {Faehrmann}}, \ and\ \bibinfo {author} {\bibfnamefont {Johannes~Jakob}\
  \bibnamefont {Meyer}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Training quantum embedding kernels on near-term quantum computers},}\ }\href
  {https://arxiv.org/abs/2105.02276} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2105.02276}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Thanasilp}\ \emph {et~al.}(2021)\citenamefont
  {Thanasilp}, \citenamefont {Wang}, \citenamefont {Nghiem}, \citenamefont
  {Coles},\ and\ \citenamefont {Cerezo}}]{thanasilp2021subtleties}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Supanut}\ \bibnamefont
  {Thanasilp}}, \bibinfo {author} {\bibfnamefont {Samson}\ \bibnamefont
  {Wang}}, \bibinfo {author} {\bibfnamefont {Nhat~A}\ \bibnamefont {Nghiem}},
  \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}}, \ and\
  \bibinfo {author} {\bibfnamefont {M}~\bibnamefont {Cerezo}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Subtleties in the trainability of
  quantum machine learning models},}\ }\href {https://arxiv.org/abs/2110.14753}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2110.14753}\
  } (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Huang}}\ \emph {et~al.}(2022)\citenamefont
  {{Huang}}, \citenamefont {{Broughton}}, \citenamefont {{Cotler}},
  \citenamefont {{Chen}}, \citenamefont {{Li}}, \citenamefont {{Mohseni}},
  \citenamefont {{Neven}}, \citenamefont {{Babbush}}, \citenamefont {{Kueng}},
  \citenamefont {{Preskill}},\ and\ \citenamefont
  {{McClean}}}]{huang2021quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {{Huang}}}, \bibinfo {author} {\bibfnamefont {Michael}\
  \bibnamefont {{Broughton}}}, \bibinfo {author} {\bibfnamefont {Jordan}\
  \bibnamefont {{Cotler}}}, \bibinfo {author} {\bibfnamefont {Sitan}\
  \bibnamefont {{Chen}}}, \bibinfo {author} {\bibfnamefont {Jerry}\
  \bibnamefont {{Li}}}, \bibinfo {author} {\bibfnamefont {Masoud}\ \bibnamefont
  {{Mohseni}}}, \bibinfo {author} {\bibfnamefont {Hartmut}\ \bibnamefont
  {{Neven}}}, \bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {{Babbush}}}, \bibinfo {author} {\bibfnamefont {Richard}\ \bibnamefont
  {{Kueng}}}, \bibinfo {author} {\bibfnamefont {John}\ \bibnamefont
  {{Preskill}}}, \ and\ \bibinfo {author} {\bibfnamefont {Jarrod~R.}\
  \bibnamefont {{McClean}}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Quantum advantage in learning from experiments},}\ }\href {\doibase
  10.1126/science.abn7293} {\bibfield  {journal} {\bibinfo  {journal}
  {Science}\ }\textbf {\bibinfo {volume} {376}},\ \bibinfo {pages} {1182--1186}
  (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cotler}\ \emph {et~al.}(2021)\citenamefont {Cotler},
  \citenamefont {Huang},\ and\ \citenamefont {McClean}}]{cotler2021revisiting}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jordan}\ \bibnamefont
  {Cotler}}, \bibinfo {author} {\bibfnamefont {Hsin-Yuan}\ \bibnamefont
  {Huang}}, \ and\ \bibinfo {author} {\bibfnamefont {Jarrod~R}\ \bibnamefont
  {McClean}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Revisiting
  dequantization and quantum advantage in learning tasks},}\ }\href
  {https://arxiv.org/abs/2112.00811} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2112.00811}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Chen}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Chen}, \citenamefont {Cotler},
  \citenamefont {Huang},\ and\ \citenamefont {Li}}]{chen2021hierarchy}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Sitan}\ \bibnamefont
  {Chen}}, \bibinfo {author} {\bibfnamefont {Jordan}\ \bibnamefont {Cotler}},
  \bibinfo {author} {\bibfnamefont {Hsin-Yuan}\ \bibnamefont {Huang}}, \ and\
  \bibinfo {author} {\bibfnamefont {Jerry}\ \bibnamefont {Li}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {A hierarchy for replica quantum
  advantage},}\ }\href {https://arxiv.org/abs/2111.05874} {\bibfield  {journal}
  {\bibinfo  {journal} {arXiv preprint arXiv:2111.05874}\ } (\bibinfo {year}
  {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Chen}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Chen}, \citenamefont {Cotler},
  \citenamefont {Huang},\ and\ \citenamefont {Li}}]{chen2021exponential}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Sitan}\ \bibnamefont
  {Chen}}, \bibinfo {author} {\bibfnamefont {Jordan}\ \bibnamefont {Cotler}},
  \bibinfo {author} {\bibfnamefont {Hsin-Yuan}\ \bibnamefont {Huang}}, \ and\
  \bibinfo {author} {\bibfnamefont {Jerry}\ \bibnamefont {Li}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Exponential separations between learning
  with and without quantum memory},}\ }\href {https://arxiv.org/abs/2111.05881}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2111.05881}\
  } (\bibinfo {year} {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Perrier}\ \emph {et~al.}(2021)\citenamefont
  {Perrier}, \citenamefont {Youssry},\ and\ \citenamefont
  {Ferrie}}]{perrier2021qdataset}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Elija}\ \bibnamefont
  {Perrier}}, \bibinfo {author} {\bibfnamefont {Akram}\ \bibnamefont
  {Youssry}}, \ and\ \bibinfo {author} {\bibfnamefont {Chris}\ \bibnamefont
  {Ferrie}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Qdataset:
  Quantum datasets for machine learning},}\ }\href
  {https://arxiv.org/abs/2108.06661} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2108.06661}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Schatzki}\ \emph {et~al.}(2021)\citenamefont
  {Schatzki}, \citenamefont {Arrasmith}, \citenamefont {Coles},\ and\
  \citenamefont {Cerezo}}]{schatzki2021entangled}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Louis}\ \bibnamefont
  {Schatzki}}, \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Patrick~J.}\ \bibnamefont
  {Coles}}, \ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Cerezo}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Entangled
  datasets for quantum machine learning},}\ }\href
  {https://arxiv.org/abs/2109.03400} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2109.03400}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{Note2()}]{Note2}%
  \BibitemOpen
  \bibinfo {note} {Technically speaking, the loss function defines a map from
  the model's parameter space to the real values. The loss function value can
  quantify, for instance, the model's error in solving a given so that our goal
  is to find the set of parameters that minimizes such error.}\BibitemShut
  {Stop}%
\bibitem [{\citenamefont {Arrasmith}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Arrasmith}, \citenamefont
  {Holmes}, \citenamefont {Cerezo},\ and\ \citenamefont
  {Coles}}]{arrasmith2021equivalence}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Zo{\"e}}\ \bibnamefont
  {Holmes}}, \bibinfo {author} {\bibfnamefont {M}~\bibnamefont {Cerezo}}, \
  and\ \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Equivalence of quantum
  barren plateaus to cost concentration and narrow gorges},}\ }\href
  {https://arxiv.org/abs/2104.05868} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2104.05868}\ } (\bibinfo {year}
  {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bittel}\ and\ \citenamefont
  {Kliesch}(2021)}]{bittel2021training}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Lennart}\ \bibnamefont
  {Bittel}}\ and\ \bibinfo {author} {\bibfnamefont {Martin}\ \bibnamefont
  {Kliesch}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Training
  variational quantum algorithms is np-hard},}\ }\href {\doibase
  10.1103/PhysRevLett.127.120502} {\bibfield  {journal} {\bibinfo  {journal}
  {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {127}},\ \bibinfo {pages}
  {120502} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bilkis}\ \emph {et~al.}(2021)\citenamefont {Bilkis},
  \citenamefont {Cerezo}, \citenamefont {Verdon}, \citenamefont {Coles},\ and\
  \citenamefont {Cincio}}]{bilkis2021semi}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M}~\bibnamefont
  {Bilkis}}, \bibinfo {author} {\bibfnamefont {M}~\bibnamefont {Cerezo}},
  \bibinfo {author} {\bibfnamefont {Guillaume}\ \bibnamefont {Verdon}},
  \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}}, \ and\
  \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {A semi-agnostic ansatz with
  variable structure for quantum machine learning},}\ }\href
  {https://arxiv.org/abs/2103.06712} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2103.06712}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {LaRose}\ \emph {et~al.}(2019)\citenamefont {LaRose},
  \citenamefont {Tikku}, \citenamefont {O'Neel-Judy}, \citenamefont {Cincio},\
  and\ \citenamefont {Coles}}]{larose2019variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {LaRose}}, \bibinfo {author} {\bibfnamefont {Arkin}\ \bibnamefont {Tikku}},
  \bibinfo {author} {\bibfnamefont {{\'E}tude}\ \bibnamefont {O'Neel-Judy}},
  \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\
  \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Variational quantum state
  diagonalization},}\ }\href {\doibase 10.1038/s41534-019-0167-6} {\bibfield
  {journal} {\bibinfo  {journal} {npj Quantum Information}\ }\textbf {\bibinfo
  {volume} {5}},\ \bibinfo {pages} {1--10} (\bibinfo {year}
  {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kiani}\ \emph {et~al.}(2020)\citenamefont {Kiani},
  \citenamefont {Lloyd},\ and\ \citenamefont {Maity}}]{kiani2020learning}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Bobak~Toussi}\
  \bibnamefont {Kiani}}, \bibinfo {author} {\bibfnamefont {Seth}\ \bibnamefont
  {Lloyd}}, \ and\ \bibinfo {author} {\bibfnamefont {Reevu}\ \bibnamefont
  {Maity}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Learning
  unitaries by gradient descent},}\ }\href {https://arxiv.org/abs/2001.11897}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2001.11897}\
  } (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Larocca}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Larocca}, \citenamefont {Ju},
  \citenamefont {Garca-Martn}, \citenamefont {Coles},\ and\ \citenamefont
  {Cerezo}}]{larocca2021theory}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Martin}\ \bibnamefont
  {Larocca}}, \bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont {Ju}},
  \bibinfo {author} {\bibfnamefont {Diego}\ \bibnamefont {Garca-Martn}},
  \bibinfo {author} {\bibfnamefont {Patrick~J.}\ \bibnamefont {Coles}}, \ and\
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Theory of overparametrization in quantum
  neural networks},}\ }\href {https://arxiv.org/abs/2109.11676} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2109.11676}\ } (\bibinfo
  {year} {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {McClean}\ \emph {et~al.}(2018)\citenamefont
  {McClean}, \citenamefont {Boixo}, \citenamefont {Smelyanskiy}, \citenamefont
  {Babbush},\ and\ \citenamefont {Neven}}]{mcclean2018barren}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jarrod~R}\
  \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {Sergio}\
  \bibnamefont {Boixo}}, \bibinfo {author} {\bibfnamefont {Vadim~N}\
  \bibnamefont {Smelyanskiy}}, \bibinfo {author} {\bibfnamefont {Ryan}\
  \bibnamefont {Babbush}}, \ and\ \bibinfo {author} {\bibfnamefont {Hartmut}\
  \bibnamefont {Neven}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Barren plateaus in quantum neural network training landscapes},}\ }\href
  {\doibase 10.1038/s41467-018-07090-4} {\bibfield  {journal} {\bibinfo
  {journal} {Nature Communications}\ }\textbf {\bibinfo {volume} {9}},\
  \bibinfo {pages} {1--6} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Cerezo}, \citenamefont {Sone},
  \citenamefont {Volkoff}, \citenamefont {Cincio},\ and\ \citenamefont
  {Coles}}]{cerezo2021cost}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M}~\bibnamefont
  {Cerezo}}, \bibinfo {author} {\bibfnamefont {Akira}\ \bibnamefont {Sone}},
  \bibinfo {author} {\bibfnamefont {Tyler}\ \bibnamefont {Volkoff}}, \bibinfo
  {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Cost function dependent barren plateaus
  in shallow parametrized quantum circuits},}\ }\href {\doibase
  10.1038/s41467-021-21728-w} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature Communications}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages}
  {1--12} (\bibinfo {year} {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cerezo}\ and\ \citenamefont
  {Coles}(2021)}]{cerezo2020impact}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Cerezo}}\ and\ \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont
  {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Higher order
  derivatives of quantum neural networks with barren plateaus},}\ }\href
  {\doibase 10.1088/2058-9565/abf51a} {\bibfield  {journal} {\bibinfo
  {journal} {Quantum Science and Technology}\ }\textbf {\bibinfo {volume}
  {6}},\ \bibinfo {pages} {035006} (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Arrasmith}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Arrasmith}, \citenamefont
  {Cerezo}, \citenamefont {Czarnik}, \citenamefont {Cincio},\ and\
  \citenamefont {Coles}}]{arrasmith2020effect}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},
  \bibinfo {author} {\bibfnamefont {Piotr}\ \bibnamefont {Czarnik}}, \bibinfo
  {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Effect of barren plateaus on
  gradient-free optimization},}\ }\href {\doibase 10.22331/q-2021-10-05-558}
  {\bibfield  {journal} {\bibinfo  {journal} {Quantum}\ }\textbf {\bibinfo
  {volume} {5}},\ \bibinfo {pages} {558} (\bibinfo {year}
  {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Holmes}\ \emph {et~al.}(2022)\citenamefont {Holmes},
  \citenamefont {Sharma}, \citenamefont {Cerezo},\ and\ \citenamefont
  {Coles}}]{holmes2021connecting}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Zo{\"e}}\ \bibnamefont
  {Holmes}}, \bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont {Sharma}},
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Connecting ansatz expressibility to
  gradient magnitudes and barren plateaus},}\ }\href {\doibase
  10.1103/PRXQuantum.3.010313} {\bibfield  {journal} {\bibinfo  {journal} {PRX
  Quantum}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {010313}
  (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Pesah}\ \emph {et~al.}(2021)\citenamefont {Pesah},
  \citenamefont {Cerezo}, \citenamefont {Wang}, \citenamefont {Volkoff},
  \citenamefont {Sornborger},\ and\ \citenamefont {Coles}}]{pesah2020absence}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Arthur}\ \bibnamefont
  {Pesah}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}},
  \bibinfo {author} {\bibfnamefont {Samson}\ \bibnamefont {Wang}}, \bibinfo
  {author} {\bibfnamefont {Tyler}\ \bibnamefont {Volkoff}}, \bibinfo {author}
  {\bibfnamefont {Andrew~T}\ \bibnamefont {Sornborger}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Absence of barren plateaus in quantum
  convolutional neural networks},}\ }\href {\doibase
  10.1103/PhysRevX.11.041011} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review X}\ }\textbf {\bibinfo {volume} {11}},\ \bibinfo {pages}
  {041011} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Volkoff}\ and\ \citenamefont
  {Coles}(2021)}]{volkoff2021large}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Tyler}\ \bibnamefont
  {Volkoff}}\ and\ \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont
  {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Large gradients
  via correlation in random parameterized quantum circuits},}\ }\href {\doibase
  10.1088/2058-9565/abd89} {\bibfield  {journal} {\bibinfo  {journal} {Quantum
  Science and Technology}\ }\textbf {\bibinfo {volume} {6}},\ \bibinfo {pages}
  {025008} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Sharma}\ \emph {et~al.}(2022)\citenamefont {Sharma},
  \citenamefont {Cerezo}, \citenamefont {Cincio},\ and\ \citenamefont
  {Coles}}]{sharma2020trainability}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont
  {Sharma}}, \bibinfo {author} {\bibfnamefont {Marco}\ \bibnamefont {Cerezo}},
  \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\
  \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Trainability of dissipative
  perceptron-based quantum neural networks},}\ }\href@noop {} {\bibfield
  {journal} {\bibinfo  {journal} {Physical Review Letters}\ }\textbf {\bibinfo
  {volume} {128}},\ \bibinfo {pages} {180505} (\bibinfo {year}
  {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Holmes}\ \emph {et~al.}(2021)\citenamefont {Holmes},
  \citenamefont {Arrasmith}, \citenamefont {Yan}, \citenamefont {Coles},
  \citenamefont {Albrecht},\ and\ \citenamefont
  {Sornborger}}]{holmes2020barren}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Zo{\"e}}\ \bibnamefont
  {Holmes}}, \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Bin}\ \bibnamefont {Yan}},
  \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}}, \bibinfo
  {author} {\bibfnamefont {Andreas}\ \bibnamefont {Albrecht}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Andrew~T}\ \bibnamefont {Sornborger}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Barren plateaus preclude learning
  scramblers},}\ }\href {\doibase 10.1103/PhysRevLett.126.190501} {\bibfield
  {journal} {\bibinfo  {journal} {Physical Review Letters}\ }\textbf {\bibinfo
  {volume} {126}},\ \bibinfo {pages} {190501} (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Marrero}\ \emph {et~al.}(2020)\citenamefont
  {Marrero}, \citenamefont {Kieferova},\ and\ \citenamefont
  {Wiebe}}]{marrero2020entanglement}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Carlos~Ortiz}\
  \bibnamefont {Marrero}}, \bibinfo {author} {\bibfnamefont {Maria}\
  \bibnamefont {Kieferova}}, \ and\ \bibinfo {author} {\bibfnamefont {Nathan}\
  \bibnamefont {Wiebe}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Entanglement induced barren plateaus},}\ }\href
  {https://arxiv.org/abs/2010.15968} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2010.15968}\ } (\bibinfo {year} {2020})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Uvarov}\ and\ \citenamefont
  {Biamonte}(2021)}]{uvarov2020barren}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {AV}~\bibnamefont
  {Uvarov}}\ and\ \bibinfo {author} {\bibfnamefont {Jacob~D}\ \bibnamefont
  {Biamonte}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {On barren
  plateaus and cost function locality in variational quantum algorithms},}\
  }\href {\doibase 10.1088/1751-8121/abfac7} {\bibfield  {journal} {\bibinfo
  {journal} {Journal of Physics A: Mathematical and Theoretical}\ }\textbf
  {\bibinfo {volume} {54}},\ \bibinfo {pages} {245301} (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Patti}\ \emph {et~al.}(2021)\citenamefont {Patti},
  \citenamefont {Najafi}, \citenamefont {Gao},\ and\ \citenamefont
  {Yelin}}]{patti2020entanglement}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Taylor~L}\
  \bibnamefont {Patti}}, \bibinfo {author} {\bibfnamefont {Khadijeh}\
  \bibnamefont {Najafi}}, \bibinfo {author} {\bibfnamefont {Xun}\ \bibnamefont
  {Gao}}, \ and\ \bibinfo {author} {\bibfnamefont {Susanne~F}\ \bibnamefont
  {Yelin}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Entanglement
  devised barren plateau mitigation},}\ }\href {\doibase
  10.1103/PhysRevResearch.3.033090} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Research}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo
  {pages} {033090} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wang}\ \emph
  {et~al.}(2021{\natexlab{a}})\citenamefont {Wang}, \citenamefont {Fontana},
  \citenamefont {Cerezo}, \citenamefont {Sharma}, \citenamefont {Sone},
  \citenamefont {Cincio},\ and\ \citenamefont {Coles}}]{wang2020noise}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Samson}\ \bibnamefont
  {Wang}}, \bibinfo {author} {\bibfnamefont {Enrico}\ \bibnamefont {Fontana}},
  \bibinfo {author} {\bibfnamefont {Marco}\ \bibnamefont {Cerezo}}, \bibinfo
  {author} {\bibfnamefont {Kunal}\ \bibnamefont {Sharma}}, \bibinfo {author}
  {\bibfnamefont {Akira}\ \bibnamefont {Sone}}, \bibinfo {author}
  {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield  {title}
  {\enquote {\bibinfo {title} {Noise-induced barren plateaus in variational
  quantum algorithms},}\ }\href {\doibase 10.1038/s41467-021-27045-6}
  {\bibfield  {journal} {\bibinfo  {journal} {Nature Communications}\ }\textbf
  {\bibinfo {volume} {12}},\ \bibinfo {pages} {1--11} (\bibinfo {year}
  {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Verdon}\ \emph
  {et~al.}(2019{\natexlab{b}})\citenamefont {Verdon}, \citenamefont
  {Broughton}, \citenamefont {McClean}, \citenamefont {Sung}, \citenamefont
  {Babbush}, \citenamefont {Jiang}, \citenamefont {Neven},\ and\ \citenamefont
  {Mohseni}}]{verdon2019learning}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Guillaume}\
  \bibnamefont {Verdon}}, \bibinfo {author} {\bibfnamefont {Michael}\
  \bibnamefont {Broughton}}, \bibinfo {author} {\bibfnamefont {Jarrod~R}\
  \bibnamefont {McClean}}, \bibinfo {author} {\bibfnamefont {Kevin~J}\
  \bibnamefont {Sung}}, \bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {Babbush}}, \bibinfo {author} {\bibfnamefont {Zhang}\ \bibnamefont {Jiang}},
  \bibinfo {author} {\bibfnamefont {Hartmut}\ \bibnamefont {Neven}}, \ and\
  \bibinfo {author} {\bibfnamefont {Masoud}\ \bibnamefont {Mohseni}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Learning to learn with
  quantum neural networks via classical neural networks},}\ }\href
  {https://arxiv.org/abs/1907.05415} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:1907.05415}\ } (\bibinfo {year}
  {2019}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Verdon}\ \emph
  {et~al.}(2019{\natexlab{c}})\citenamefont {Verdon}, \citenamefont {McCourt},
  \citenamefont {Luzhnica}, \citenamefont {Singh}, \citenamefont
  {Leichenauer},\ and\ \citenamefont {Hidary}}]{verdon2019quantumgraph}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Guillaume}\
  \bibnamefont {Verdon}}, \bibinfo {author} {\bibfnamefont {Trevor}\
  \bibnamefont {McCourt}}, \bibinfo {author} {\bibfnamefont {Enxhell}\
  \bibnamefont {Luzhnica}}, \bibinfo {author} {\bibfnamefont {Vikash}\
  \bibnamefont {Singh}}, \bibinfo {author} {\bibfnamefont {Stefan}\
  \bibnamefont {Leichenauer}}, \ and\ \bibinfo {author} {\bibfnamefont {Jack}\
  \bibnamefont {Hidary}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Quantum graph neural networks},}\ }\href {https://arxiv.org/abs/1909.12264}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:1909.12264}\
  } (\bibinfo {year} {2019}{\natexlab{c}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bronstein}\ \emph {et~al.}(2021)\citenamefont
  {Bronstein}, \citenamefont {Bruna}, \citenamefont {Cohen},\ and\
  \citenamefont {Veli{\v{c}}kovi{\'c}}}]{bronstein2021geometric}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Michael~M}\
  \bibnamefont {Bronstein}}, \bibinfo {author} {\bibfnamefont {Joan}\
  \bibnamefont {Bruna}}, \bibinfo {author} {\bibfnamefont {Taco}\ \bibnamefont
  {Cohen}}, \ and\ \bibinfo {author} {\bibfnamefont {Petar}\ \bibnamefont
  {Veli{\v{c}}kovi{\'c}}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},}\
  }\href {https://arxiv.org/abs/2104.13478} {\bibfield  {journal} {\bibinfo
  {journal} {arXiv preprint arXiv:2104.13478}\ } (\bibinfo {year}
  {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Larocca}\ \emph {et~al.}(2022)\citenamefont
  {Larocca}, \citenamefont {Sauvage}, \citenamefont {Sbahi}, \citenamefont
  {Verdon}, \citenamefont {Coles},\ and\ \citenamefont
  {Cerezo}}]{larocca2022group}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Martin}\ \bibnamefont
  {Larocca}}, \bibinfo {author} {\bibfnamefont {Frederic}\ \bibnamefont
  {Sauvage}}, \bibinfo {author} {\bibfnamefont {Faris~M}\ \bibnamefont
  {Sbahi}}, \bibinfo {author} {\bibfnamefont {Guillaume}\ \bibnamefont
  {Verdon}}, \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont
  {Coles}}, \ and\ \bibinfo {author} {\bibfnamefont {M}~\bibnamefont
  {Cerezo}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Group-invariant
  quantum machine learning},}\ }\href {https://arxiv.org/abs/2205.02261}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2205.02261}\
  } (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Skolik}\ \emph {et~al.}(2022)\citenamefont {Skolik},
  \citenamefont {Cattelan}, \citenamefont {Yarkoni}, \citenamefont {B{\"a}ck},\
  and\ \citenamefont {Dunjko}}]{skolik2022equivariant}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Andrea}\ \bibnamefont
  {Skolik}}, \bibinfo {author} {\bibfnamefont {Michele}\ \bibnamefont
  {Cattelan}}, \bibinfo {author} {\bibfnamefont {Sheir}\ \bibnamefont
  {Yarkoni}}, \bibinfo {author} {\bibfnamefont {Thomas}\ \bibnamefont
  {B{\"a}ck}}, \ and\ \bibinfo {author} {\bibfnamefont {Vedran}\ \bibnamefont
  {Dunjko}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Equivariant
  quantum circuits for learning on weighted graphs},}\ }\href
  {https://arxiv.org/abs/2205.06109} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2205.06109}\ } (\bibinfo {year} {2022})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Meyer}\ \emph {et~al.}(2022)\citenamefont {Meyer},
  \citenamefont {Mularski}, \citenamefont {Gil-Fuster}, \citenamefont {Mele},
  \citenamefont {Arzani}, \citenamefont {Wilms},\ and\ \citenamefont
  {Eisert}}]{meyer2022exploiting}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Johannes~Jakob}\
  \bibnamefont {Meyer}}, \bibinfo {author} {\bibfnamefont {Marian}\
  \bibnamefont {Mularski}}, \bibinfo {author} {\bibfnamefont {Elies}\
  \bibnamefont {Gil-Fuster}}, \bibinfo {author} {\bibfnamefont {Antonio~Anna}\
  \bibnamefont {Mele}}, \bibinfo {author} {\bibfnamefont {Francesco}\
  \bibnamefont {Arzani}}, \bibinfo {author} {\bibfnamefont {Alissa}\
  \bibnamefont {Wilms}}, \ and\ \bibinfo {author} {\bibfnamefont {Jens}\
  \bibnamefont {Eisert}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Exploiting symmetry in variational quantum machine learning},}\ }\href
  {https://arxiv.org/abs/2205.06217} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2205.06217}\ } (\bibinfo {year} {2022})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Larocca}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Larocca}, \citenamefont {Czarnik},
  \citenamefont {Sharma}, \citenamefont {Muraleedharan}, \citenamefont
  {Coles},\ and\ \citenamefont {Cerezo}}]{larocca2021diagnosing}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Martin}\ \bibnamefont
  {Larocca}}, \bibinfo {author} {\bibfnamefont {Piotr}\ \bibnamefont
  {Czarnik}}, \bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont {Sharma}},
  \bibinfo {author} {\bibfnamefont {Gopikrishnan}\ \bibnamefont
  {Muraleedharan}}, \bibinfo {author} {\bibfnamefont {Patrick~J.}\ \bibnamefont
  {Coles}}, \ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont
  {Cerezo}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Diagnosing
  barren plateaus with tools from quantum optimal control},}\ }\href
  {https://arxiv.org/abs/2105.14377} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2105.14377}\ } (\bibinfo {year}
  {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Wang}\ \emph
  {et~al.}(2021{\natexlab{b}})\citenamefont {Wang}, \citenamefont {Czarnik},
  \citenamefont {Arrasmith}, \citenamefont {Cerezo}, \citenamefont {Cincio},\
  and\ \citenamefont {Coles}}]{wang2021can}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Samson}\ \bibnamefont
  {Wang}}, \bibinfo {author} {\bibfnamefont {Piotr}\ \bibnamefont {Czarnik}},
  \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont {Arrasmith}},
  \bibinfo {author} {\bibfnamefont {M}~\bibnamefont {Cerezo}}, \bibinfo
  {author} {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Can error mitigation improve
  trainability of noisy variational quantum algorithms?}}\ }\href
  {https://arxiv.org/abs/2109.01051} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2109.01051}\ } (\bibinfo {year}
  {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Deshpande}\ \emph {et~al.}(2021)\citenamefont
  {Deshpande}, \citenamefont {Fefferman}, \citenamefont {Gorshkov},
  \citenamefont {Gullans}, \citenamefont {Niroula},\ and\ \citenamefont
  {Shtanko}}]{deshpande2021tight}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Abhinav}\ \bibnamefont
  {Deshpande}}, \bibinfo {author} {\bibfnamefont {Bill}\ \bibnamefont
  {Fefferman}}, \bibinfo {author} {\bibfnamefont {Alexey~V}\ \bibnamefont
  {Gorshkov}}, \bibinfo {author} {\bibfnamefont {Michael~J}\ \bibnamefont
  {Gullans}}, \bibinfo {author} {\bibfnamefont {Pradeep}\ \bibnamefont
  {Niroula}}, \ and\ \bibinfo {author} {\bibfnamefont {Oles}\ \bibnamefont
  {Shtanko}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Tight bounds on
  the convergence of noisy random circuits to uniform},}\ }\href
  {https://arxiv.org/abs/2112.00716} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2112.00716}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Hakkaku}\ \emph {et~al.}(2021)\citenamefont
  {Hakkaku}, \citenamefont {Tashima}, \citenamefont {Mitarai}, \citenamefont
  {Mizukami},\ and\ \citenamefont {Fujii}}]{hakkaku2021quantifying}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Shigeo}\ \bibnamefont
  {Hakkaku}}, \bibinfo {author} {\bibfnamefont {Yuichiro}\ \bibnamefont
  {Tashima}}, \bibinfo {author} {\bibfnamefont {Kosuke}\ \bibnamefont
  {Mitarai}}, \bibinfo {author} {\bibfnamefont {Wataru}\ \bibnamefont
  {Mizukami}}, \ and\ \bibinfo {author} {\bibfnamefont {Keisuke}\ \bibnamefont
  {Fujii}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantifying
  fermionic nonlinearity of quantum circuits},}\ }\href
  {https://arxiv.org/abs/2111.14599} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:2111.14599}\ } (\bibinfo {year} {2021})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Bultrini}\ \emph {et~al.}(2022)\citenamefont
  {Bultrini}, \citenamefont {Wang}, \citenamefont {Czarnik}, \citenamefont
  {Gordon}, \citenamefont {Cerezo}, \citenamefont {Coles},\ and\ \citenamefont
  {Cincio}}]{bultrini2022battle}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Daniel}\ \bibnamefont
  {Bultrini}}, \bibinfo {author} {\bibfnamefont {Samson}\ \bibnamefont {Wang}},
  \bibinfo {author} {\bibfnamefont {Piotr}\ \bibnamefont {Czarnik}}, \bibinfo
  {author} {\bibfnamefont {Max~Hunter}\ \bibnamefont {Gordon}}, \bibinfo
  {author} {\bibfnamefont {M}~\bibnamefont {Cerezo}}, \bibinfo {author}
  {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Lukasz}\ \bibnamefont {Cincio}},\ }\bibfield  {title}
  {\enquote {\bibinfo {title} {The battle of clean and dirty qubits in the era
  of partial error correction},}\ }\href {https://arxiv.org/abs/2205.13454}
  {\bibfield  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2205.13454}\
  } (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Temme}\ \emph {et~al.}(2017)\citenamefont {Temme},
  \citenamefont {Bravyi},\ and\ \citenamefont {Gambetta}}]{temme2017error}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Kristan}\ \bibnamefont
  {Temme}}, \bibinfo {author} {\bibfnamefont {Sergey}\ \bibnamefont {Bravyi}},
  \ and\ \bibinfo {author} {\bibfnamefont {Jay~M.}\ \bibnamefont {Gambetta}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Error mitigation for
  short-depth quantum circuits},}\ }\href {\doibase
  10.1103/PhysRevLett.119.180509} {\bibfield  {journal} {\bibinfo  {journal}
  {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {119}},\ \bibinfo {pages}
  {180509} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Czarnik}\ \emph {et~al.}(2021)\citenamefont
  {Czarnik}, \citenamefont {Arrasmith}, \citenamefont {Coles},\ and\
  \citenamefont {Cincio}}]{czarnik2020error}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Piotr}\ \bibnamefont
  {Czarnik}}, \bibinfo {author} {\bibfnamefont {Andrew}\ \bibnamefont
  {Arrasmith}}, \bibinfo {author} {\bibfnamefont {Patrick~J.}\ \bibnamefont
  {Coles}}, \ and\ \bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont
  {Cincio}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Error mitigation
  with {C}lifford quantum-circuit data},}\ }\href {\doibase
  10.22331/q-2021-11-26-592} {\bibfield  {journal} {\bibinfo  {journal}
  {{Quantum}}\ }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages} {592}
  (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Endo}\ \emph {et~al.}(2021)\citenamefont {Endo},
  \citenamefont {Cai}, \citenamefont {Benjamin},\ and\ \citenamefont
  {Yuan}}]{endo2021hybrid}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Suguru}\ \bibnamefont
  {Endo}}, \bibinfo {author} {\bibfnamefont {Zhenyu}\ \bibnamefont {Cai}},
  \bibinfo {author} {\bibfnamefont {Simon~C}\ \bibnamefont {Benjamin}}, \ and\
  \bibinfo {author} {\bibfnamefont {Xiao}\ \bibnamefont {Yuan}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Hybrid quantum-classical algorithms and
  quantum error mitigation},}\ }\href {\doibase 10.7566/JPSJ.90.032001}
  {\bibfield  {journal} {\bibinfo  {journal} {Journal of the Physical Society
  of Japan}\ }\textbf {\bibinfo {volume} {90}},\ \bibinfo {pages} {032001}
  (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Sharma}\ \emph
  {et~al.}(2020{\natexlab{b}})\citenamefont {Sharma}, \citenamefont {Khatri},
  \citenamefont {Cerezo},\ and\ \citenamefont {Coles}}]{sharma2019noise}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Kunal}\ \bibnamefont
  {Sharma}}, \bibinfo {author} {\bibfnamefont {Sumeet}\ \bibnamefont {Khatri}},
  \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Cerezo}}, \ and\ \bibinfo
  {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\ }\bibfield
  {title} {\enquote {\bibinfo {title} {Noise resilience of variational quantum
  compiling},}\ }\href {\doibase 10.1088/1367-2630/ab784c} {\bibfield
  {journal} {\bibinfo  {journal} {New Journal of Physics}\ }\textbf {\bibinfo
  {volume} {22}},\ \bibinfo {pages} {043006} (\bibinfo {year}
  {2020}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cincio}\ \emph {et~al.}(2021)\citenamefont {Cincio},
  \citenamefont {Rudinger}, \citenamefont {Sarovar},\ and\ \citenamefont
  {Coles}}]{cincio2020machine}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Lukasz}\ \bibnamefont
  {Cincio}}, \bibinfo {author} {\bibfnamefont {Kenneth}\ \bibnamefont
  {Rudinger}}, \bibinfo {author} {\bibfnamefont {Mohan}\ \bibnamefont
  {Sarovar}}, \ and\ \bibinfo {author} {\bibfnamefont {Patrick~J.}\
  \bibnamefont {Coles}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Machine learning of noise-resilient quantum circuits},}\ }\href {\doibase
  10.1103/PRXQuantum.2.010324} {\bibfield  {journal} {\bibinfo  {journal} {PRX
  Quantum}\ }\textbf {\bibinfo {volume} {2}},\ \bibinfo {pages} {010324}
  (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ho}\ \emph {et~al.}(2020)\citenamefont {Ho},
  \citenamefont {Verdon},\ and\ \citenamefont {Mohseni}}]{verdon2020qmp}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Alan}\ \bibnamefont
  {Ho}}, \bibinfo {author} {\bibfnamefont {Guillaume}\ \bibnamefont {Verdon}},
  \ and\ \bibinfo {author} {\bibfnamefont {Masoud}\ \bibnamefont {Mohseni}},\
  }\href@noop {} {\enquote {\bibinfo {title} {{Quantum Machine Perception}},}\
  } (\bibinfo {year} {2020}),\ \bibinfo {note} {{ United States Patent
  Application No. 17019564.}}\BibitemShut {Stop}%
\bibitem [{\citenamefont {Meyer}\ \emph {et~al.}(2021)\citenamefont {Meyer},
  \citenamefont {Borregaard},\ and\ \citenamefont
  {Eisert}}]{meyer2020variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Johannes~Jakob}\
  \bibnamefont {Meyer}}, \bibinfo {author} {\bibfnamefont {Johannes}\
  \bibnamefont {Borregaard}}, \ and\ \bibinfo {author} {\bibfnamefont {Jens}\
  \bibnamefont {Eisert}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {A
  variational toolbox for quantum multi-parameter estimation},}\ }\href
  {\doibase 10.1038/s41534-021-00425-y} {\bibfield  {journal} {\bibinfo
  {journal} {NPJ Quantum Information}\ }\textbf {\bibinfo {volume} {7}},\
  \bibinfo {pages} {1--5} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Beckey}\ \emph {et~al.}(2022)\citenamefont {Beckey},
  \citenamefont {Cerezo}, \citenamefont {Sone},\ and\ \citenamefont
  {Coles}}]{beckey2020variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jacob~L}\ \bibnamefont
  {Beckey}}, \bibinfo {author} {\bibfnamefont {M}~\bibnamefont {Cerezo}},
  \bibinfo {author} {\bibfnamefont {Akira}\ \bibnamefont {Sone}}, \ and\
  \bibinfo {author} {\bibfnamefont {Patrick~J}\ \bibnamefont {Coles}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Variational quantum
  algorithm for estimating the quantum {F}isher information},}\ }\href
  {\doibase 10.1103/PhysRevResearch.4.013083} {\bibfield  {journal} {\bibinfo
  {journal} {Physical Review Research}\ }\textbf {\bibinfo {volume} {4}},\
  \bibinfo {pages} {013083} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {{Wang}}\ \emph {et~al.}(2017)\citenamefont {{Wang}},
  \citenamefont {{Paesani}}, \citenamefont {{Santagati}}, \citenamefont
  {{Knauer}}, \citenamefont {{Gentile}}, \citenamefont {{Wiebe}}, \citenamefont
  {{Petruzzella}}, \citenamefont {{O'Brien}}, \citenamefont {{Rarity}},
  \citenamefont {{Laing}},\ and\ \citenamefont
  {{Thompson}}}]{wang2017experimental}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Jianwei}\ \bibnamefont
  {{Wang}}}, \bibinfo {author} {\bibfnamefont {Stefano}\ \bibnamefont
  {{Paesani}}}, \bibinfo {author} {\bibfnamefont {Raffaele}\ \bibnamefont
  {{Santagati}}}, \bibinfo {author} {\bibfnamefont {Sebastian}\ \bibnamefont
  {{Knauer}}}, \bibinfo {author} {\bibfnamefont {Antonio~A.}\ \bibnamefont
  {{Gentile}}}, \bibinfo {author} {\bibfnamefont {Nathan}\ \bibnamefont
  {{Wiebe}}}, \bibinfo {author} {\bibfnamefont {Maurangelo}\ \bibnamefont
  {{Petruzzella}}}, \bibinfo {author} {\bibfnamefont {Jeremy~L.}\ \bibnamefont
  {{O'Brien}}}, \bibinfo {author} {\bibfnamefont {John~G.}\ \bibnamefont
  {{Rarity}}}, \bibinfo {author} {\bibfnamefont {Anthony}\ \bibnamefont
  {{Laing}}}, \ and\ \bibinfo {author} {\bibfnamefont {Mark~G.}\ \bibnamefont
  {{Thompson}}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Experimental
  quantum {H}amiltonian learning},}\ }\href {\doibase 10.1038/nphys4074}
  {\bibfield  {journal} {\bibinfo  {journal} {Nature Physics}\ }\textbf
  {\bibinfo {volume} {13}},\ \bibinfo {pages} {551--555} (\bibinfo {year}
  {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Layden}\ and\ \citenamefont
  {Cappellaro}(2018)}]{layden2018spatial}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {David}\ \bibnamefont
  {Layden}}\ and\ \bibinfo {author} {\bibfnamefont {Paola}\ \bibnamefont
  {Cappellaro}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Spatial
  noise filtering through error correction for quantum sensing},}\ }\href
  {\doibase 10.1038/s41534-018-0082-2} {\bibfield  {journal} {\bibinfo
  {journal} {npj Quantum Information}\ }\textbf {\bibinfo {volume} {4}},\
  \bibinfo {pages} {1--6} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Johnson}\ \emph {et~al.}(2017)\citenamefont
  {Johnson}, \citenamefont {Romero}, \citenamefont {Olson}, \citenamefont
  {Cao},\ and\ \citenamefont {Aspuru-Guzik}}]{johnson2017qvector}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Peter~D}\ \bibnamefont
  {Johnson}}, \bibinfo {author} {\bibfnamefont {Jonathan}\ \bibnamefont
  {Romero}}, \bibinfo {author} {\bibfnamefont {Jonathan}\ \bibnamefont
  {Olson}}, \bibinfo {author} {\bibfnamefont {Yudong}\ \bibnamefont {Cao}}, \
  and\ \bibinfo {author} {\bibfnamefont {Al{\'a}n}\ \bibnamefont
  {Aspuru-Guzik}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Qvector:
  an algorithm for device-tailored quantum error correction},}\ }\href
  {https://arxiv.org/abs/1711.02249} {\bibfield  {journal} {\bibinfo  {journal}
  {arXiv preprint arXiv:1711.02249}\ } (\bibinfo {year} {2017})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Peruzzo}\ \emph {et~al.}(2014)\citenamefont
  {Peruzzo}, \citenamefont {McClean}, \citenamefont {Shadbolt}, \citenamefont
  {Yung}, \citenamefont {Zhou}, \citenamefont {Love}, \citenamefont
  {Aspuru-Guzik},\ and\ \citenamefont {Obrien}}]{peruzzo2014variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Alberto}\ \bibnamefont
  {Peruzzo}}, \bibinfo {author} {\bibfnamefont {Jarrod}\ \bibnamefont
  {McClean}}, \bibinfo {author} {\bibfnamefont {Peter}\ \bibnamefont
  {Shadbolt}}, \bibinfo {author} {\bibfnamefont {Man-Hong}\ \bibnamefont
  {Yung}}, \bibinfo {author} {\bibfnamefont {Xiao-Qi}\ \bibnamefont {Zhou}},
  \bibinfo {author} {\bibfnamefont {Peter~J}\ \bibnamefont {Love}}, \bibinfo
  {author} {\bibfnamefont {Al{\'a}n}\ \bibnamefont {Aspuru-Guzik}}, \ and\
  \bibinfo {author} {\bibfnamefont {Jeremy~L}\ \bibnamefont {Obrien}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {A variational eigenvalue
  solver on a photonic quantum processor},}\ }\href {\doibase
  doi.org/10.1038/ncomms5213} {\bibfield  {journal} {\bibinfo  {journal}
  {Nature communications}\ }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages}
  {1--7} (\bibinfo {year} {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {McArdle}\ \emph {et~al.}(2019)\citenamefont
  {McArdle}, \citenamefont {Jones}, \citenamefont {Endo}, \citenamefont {Li},
  \citenamefont {Benjamin},\ and\ \citenamefont
  {Yuan}}]{mcardle2019variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Sam}\ \bibnamefont
  {McArdle}}, \bibinfo {author} {\bibfnamefont {Tyson}\ \bibnamefont {Jones}},
  \bibinfo {author} {\bibfnamefont {Suguru}\ \bibnamefont {Endo}}, \bibinfo
  {author} {\bibfnamefont {Ying}\ \bibnamefont {Li}}, \bibinfo {author}
  {\bibfnamefont {Simon~C}\ \bibnamefont {Benjamin}}, \ and\ \bibinfo {author}
  {\bibfnamefont {Xiao}\ \bibnamefont {Yuan}},\ }\bibfield  {title} {\enquote
  {\bibinfo {title} {Variational ansatz-based quantum simulation of imaginary
  time evolution},}\ }\href {\doibase 10.1038/s41534-019-0187-2} {\bibfield
  {journal} {\bibinfo  {journal} {npj Quantum Information}\ }\textbf {\bibinfo
  {volume} {5}},\ \bibinfo {pages} {1--6} (\bibinfo {year} {2019})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Cirstoiu}\ \emph {et~al.}(2020)\citenamefont
  {Cirstoiu}, \citenamefont {Holmes}, \citenamefont {Iosue}, \citenamefont
  {Cincio}, \citenamefont {Coles},\ and\ \citenamefont
  {Sornborger}}]{cirstoiu2020variational}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Cristina}\
  \bibnamefont {Cirstoiu}}, \bibinfo {author} {\bibfnamefont {Zoe}\
  \bibnamefont {Holmes}}, \bibinfo {author} {\bibfnamefont {Joseph}\
  \bibnamefont {Iosue}}, \bibinfo {author} {\bibfnamefont {Lukasz}\
  \bibnamefont {Cincio}}, \bibinfo {author} {\bibfnamefont {Patrick~J}\
  \bibnamefont {Coles}}, \ and\ \bibinfo {author} {\bibfnamefont {Andrew}\
  \bibnamefont {Sornborger}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Variational fast forwarding for quantum simulation beyond the coherence
  time},}\ }\href {\doibase 10.1038/s41534-020-00302-0} {\bibfield  {journal}
  {\bibinfo  {journal} {npj Quantum Information}\ }\textbf {\bibinfo {volume}
  {6}},\ \bibinfo {pages} {1--10} (\bibinfo {year} {2020})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Huang}\ \emph
  {et~al.}(2021{\natexlab{c}})\citenamefont {Huang}, \citenamefont {Kueng},\
  and\ \citenamefont {Preskill}}]{huang2021information}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {Richard}\
  \bibnamefont {Kueng}}, \ and\ \bibinfo {author} {\bibfnamefont {John}\
  \bibnamefont {Preskill}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Information-theoretic bounds on quantum advantage in machine learning},}\
  }\href {\doibase 10.1103/PhysRevLett.126.190505} {\bibfield  {journal}
  {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {126}},\
  \bibinfo {pages} {190505} (\bibinfo {year} {2021}{\natexlab{c}})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Aharonov}\ \emph {et~al.}(2021)\citenamefont
  {Aharonov}, \citenamefont {Cotler},\ and\ \citenamefont
  {Qi}}]{aharonov2021quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Dorit}\ \bibnamefont
  {Aharonov}}, \bibinfo {author} {\bibfnamefont {Jordan}\ \bibnamefont
  {Cotler}}, \ and\ \bibinfo {author} {\bibfnamefont {Xiao-Liang}\ \bibnamefont
  {Qi}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum algorithmic
  measurement},}\ }\href {https://arxiv.org/abs/2101.04634} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2101.04634}\ } (\bibinfo
  {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Chia}\ \emph {et~al.}(2020)\citenamefont {Chia},
  \citenamefont {Gily{\'e}n}, \citenamefont {Li}, \citenamefont {Lin},
  \citenamefont {Tang},\ and\ \citenamefont {Wang}}]{chia2020sampling}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Nai-Hui}\ \bibnamefont
  {Chia}}, \bibinfo {author} {\bibfnamefont {Andr{\'a}s}\ \bibnamefont
  {Gily{\'e}n}}, \bibinfo {author} {\bibfnamefont {Tongyang}\ \bibnamefont
  {Li}}, \bibinfo {author} {\bibfnamefont {Han-Hsuan}\ \bibnamefont {Lin}},
  \bibinfo {author} {\bibfnamefont {Ewin}\ \bibnamefont {Tang}}, \ and\
  \bibinfo {author} {\bibfnamefont {Chunhao}\ \bibnamefont {Wang}},\ }\bibfield
   {title} {\enquote {\bibinfo {title} {Sampling-based sublinear low-rank
  matrix arithmetic framework for dequantizing quantum machine learning},}\
  }in\ \href {\doibase 10.1145/3357713.3384314} {\emph {\bibinfo {booktitle}
  {Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
  Computing}}}\ (\bibinfo {year} {2020})\ pp.\ \bibinfo {pages}
  {387--400}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Preskill}(2018)}]{preskill2018quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {John}\ \bibnamefont
  {Preskill}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  computing in the nisq era and beyond},}\ }\href {\doibase
  10.22331/q-2018-08-06-79} {\bibfield  {journal} {\bibinfo  {journal}
  {Quantum}\ }\textbf {\bibinfo {volume} {2}},\ \bibinfo {pages} {79} (\bibinfo
  {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Huang}\ \emph
  {et~al.}(2021{\natexlab{d}})\citenamefont {Huang}, \citenamefont {Kueng},
  \citenamefont {Torlai}, \citenamefont {Albert},\ and\ \citenamefont
  {Preskill}}]{huang2021provably}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Hsin-Yuan}\
  \bibnamefont {Huang}}, \bibinfo {author} {\bibfnamefont {Richard}\
  \bibnamefont {Kueng}}, \bibinfo {author} {\bibfnamefont {Giacomo}\
  \bibnamefont {Torlai}}, \bibinfo {author} {\bibfnamefont {Victor~V.}\
  \bibnamefont {Albert}}, \ and\ \bibinfo {author} {\bibfnamefont {John}\
  \bibnamefont {Preskill}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Provably efficient machine learning for quantum many-body problems},}\
  }\href {https://arxiv.org/abs/2106.12627} {\bibfield  {journal} {\bibinfo
  {journal} {arXiv preprint arXiv:2106.12627}\ } (\bibinfo {year}
  {2021}{\natexlab{d}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hohenberg}\ and\ \citenamefont
  {Kohn}(1964)}]{hohenberg1964inhomogeneous}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont
  {Hohenberg}}\ and\ \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont
  {Kohn}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Inhomogeneous
  electron gas},}\ }\href {\doibase 10.1103/PhysRev.136.B864} {\bibfield
  {journal} {\bibinfo  {journal} {Phys. Rev.}\ }\textbf {\bibinfo {volume}
  {136}},\ \bibinfo {pages} {B864--B871} (\bibinfo {year} {1964})}\BibitemShut
  {NoStop}%
\bibitem [{\citenamefont {Kohn}(1999)}]{kohn1999nobel}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont
  {Kohn}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Nobel lecture:
  Electronic structure of matter---wave functions and density functionals},}\
  }\href {\doibase 10.1103/RevModPhys.71.1253} {\bibfield  {journal} {\bibinfo
  {journal} {Rev. Mod. Phys.}\ }\textbf {\bibinfo {volume} {71}},\ \bibinfo
  {pages} {1253--1266} (\bibinfo {year} {1999})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Alcazar}\ \emph {et~al.}(2020)\citenamefont
  {Alcazar}, \citenamefont {Leyton-Ortega},\ and\ \citenamefont
  {Perdomo-Ortiz}}]{alcazar2020classical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Javier}\ \bibnamefont
  {Alcazar}}, \bibinfo {author} {\bibfnamefont {Vicente}\ \bibnamefont
  {Leyton-Ortega}}, \ and\ \bibinfo {author} {\bibfnamefont {Alejandro}\
  \bibnamefont {Perdomo-Ortiz}},\ }\bibfield  {title} {\enquote {\bibinfo
  {title} {Classical versus quantum models in machine learning: insights from a
  finance application},}\ }\href {\doibase 10.1088/2632-2153/ab9009} {\bibfield
   {journal} {\bibinfo  {journal} {Machine Learning: Science and Technology}\
  }\textbf {\bibinfo {volume} {1}},\ \bibinfo {pages} {035003} (\bibinfo {year}
  {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bouland}\ \emph {et~al.}(2020)\citenamefont
  {Bouland}, \citenamefont {van Dam}, \citenamefont {Joorati}, \citenamefont
  {Kerenidis},\ and\ \citenamefont {Prakash}}]{bouland2020prospects}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Adam}\ \bibnamefont
  {Bouland}}, \bibinfo {author} {\bibfnamefont {Wim}\ \bibnamefont {van Dam}},
  \bibinfo {author} {\bibfnamefont {Hamed}\ \bibnamefont {Joorati}}, \bibinfo
  {author} {\bibfnamefont {Iordanis}\ \bibnamefont {Kerenidis}}, \ and\
  \bibinfo {author} {\bibfnamefont {Anupam}\ \bibnamefont {Prakash}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Prospects and challenges of
  quantum finance},}\ }\href {https://arxiv.org/abs/2011.06492} {\bibfield
  {journal} {\bibinfo  {journal} {arXiv preprint arXiv:2011.06492}\ } (\bibinfo
  {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Manning}\ and\ \citenamefont
  {Schutze}(1999)}]{manning1999foundations}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Christopher}\
  \bibnamefont {Manning}}\ and\ \bibinfo {author} {\bibfnamefont {Hinrich}\
  \bibnamefont {Schutze}},\ }\href@noop {} {\emph {\bibinfo {title}
  {Foundations of statistical natural language processing}}}\ (\bibinfo
  {publisher} {MIT press},\ \bibinfo {year} {1999})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Russ}(2006)}]{russ2006image}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {John~C}\ \bibnamefont
  {Russ}},\ }\href@noop {} {\emph {\bibinfo {title} {The image processing
  handbook}}}\ (\bibinfo  {publisher} {CRC press},\ \bibinfo {year}
  {2006})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Grover}(1996)}]{grover1996fast}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Lov~K}\ \bibnamefont
  {Grover}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {A fast quantum
  mechanical algorithm for database search},}\ }in\ \href {\doibase
  10.1145/237814.237866} {\emph {\bibinfo {booktitle} {Proceedings of the
  twenty-eighth annual ACM symposium on Theory of computing}}}\ (\bibinfo
  {year} {1996})\ pp.\ \bibinfo {pages} {212--219}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bernstein}\ and\ \citenamefont
  {Vazirani}(1997)}]{bernstein1997quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Ethan}\ \bibnamefont
  {Bernstein}}\ and\ \bibinfo {author} {\bibfnamefont {Umesh}\ \bibnamefont
  {Vazirani}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  complexity theory},}\ }\href {\doibase
  https://doi.org/10.1137/S0097539796300921} {\bibfield  {journal} {\bibinfo
  {journal} {SIAM Journal on computing}\ }\textbf {\bibinfo {volume} {26}},\
  \bibinfo {pages} {1411--1473} (\bibinfo {year} {1997})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Babbush}\ \emph {et~al.}(2021)\citenamefont
  {Babbush}, \citenamefont {McClean}, \citenamefont {Newman}, \citenamefont
  {Gidney}, \citenamefont {Boixo},\ and\ \citenamefont
  {Neven}}]{babbush2021focus}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Ryan}\ \bibnamefont
  {Babbush}}, \bibinfo {author} {\bibfnamefont {Jarrod~R}\ \bibnamefont
  {McClean}}, \bibinfo {author} {\bibfnamefont {Michael}\ \bibnamefont
  {Newman}}, \bibinfo {author} {\bibfnamefont {Craig}\ \bibnamefont {Gidney}},
  \bibinfo {author} {\bibfnamefont {Sergio}\ \bibnamefont {Boixo}}, \ and\
  \bibinfo {author} {\bibfnamefont {Hartmut}\ \bibnamefont {Neven}},\
  }\bibfield  {title} {\enquote {\bibinfo {title} {Focus beyond quadratic
  speedups for error-corrected quantum advantage},}\ }\href {\doibase
  10.1103/PRXQuantum.2.010103} {\bibfield  {journal} {\bibinfo  {journal} {PRX
  Quantum}\ }\textbf {\bibinfo {volume} {2}},\ \bibinfo {pages} {010103}
  (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Georgescu}\ \emph {et~al.}(2014)\citenamefont
  {Georgescu}, \citenamefont {Ashhab},\ and\ \citenamefont
  {Nori}}]{georgescu2014quantum}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Iulia~M}\ \bibnamefont
  {Georgescu}}, \bibinfo {author} {\bibfnamefont {Sahel}\ \bibnamefont
  {Ashhab}}, \ and\ \bibinfo {author} {\bibfnamefont {Franco}\ \bibnamefont
  {Nori}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Quantum
  simulation},}\ }\href {\doibase 10.1103/RevModPhys.86.153} {\bibfield
  {journal} {\bibinfo  {journal} {Reviews of Modern Physics}\ }\textbf
  {\bibinfo {volume} {86}},\ \bibinfo {pages} {153} (\bibinfo {year}
  {2014})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Berry}\ \emph {et~al.}(2015)\citenamefont {Berry},
  \citenamefont {Childs}, \citenamefont {Cleve}, \citenamefont {Kothari},\ and\
  \citenamefont {Somma}}]{berry2015simulating}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Dominic~W}\
  \bibnamefont {Berry}}, \bibinfo {author} {\bibfnamefont {Andrew~M}\
  \bibnamefont {Childs}}, \bibinfo {author} {\bibfnamefont {Richard}\
  \bibnamefont {Cleve}}, \bibinfo {author} {\bibfnamefont {Robin}\ \bibnamefont
  {Kothari}}, \ and\ \bibinfo {author} {\bibfnamefont {Rolando~D}\ \bibnamefont
  {Somma}},\ }\bibfield  {title} {\enquote {\bibinfo {title} {Simulating
  hamiltonian dynamics with a truncated taylor series},}\ }\href {\doibase
  10.1103/PhysRevLett.114.090502} {\bibfield  {journal} {\bibinfo  {journal}
  {Physical Review Letters}\ }\textbf {\bibinfo {volume} {114}},\ \bibinfo
  {pages} {090502} (\bibinfo {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Lvovsky}\ \emph {et~al.}(2009)\citenamefont
  {Lvovsky}, \citenamefont {Sanders},\ and\ \citenamefont
  {Tittel}}]{lvovsky2009optical}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Alexander~I}\
  \bibnamefont {Lvovsky}}, \bibinfo {author} {\bibfnamefont {Barry~C}\
  \bibnamefont {Sanders}}, \ and\ \bibinfo {author} {\bibfnamefont {Wolfgang}\
  \bibnamefont {Tittel}},\ }\bibfield  {title} {\enquote {\bibinfo {title}
  {Optical quantum memory},}\ }\href {\doibase 10.1038/nphoton.2009.231}
  {\bibfield  {journal} {\bibinfo  {journal} {Nature photonics}\ }\textbf
  {\bibinfo {volume} {3}},\ \bibinfo {pages} {706--714} (\bibinfo {year}
  {2009})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Sanchez-Lengeling}\ and\ \citenamefont
  {Aspuru-Guzik}(2018)}]{sanchez2018inverse}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Benjamin}\
  \bibnamefont {Sanchez-Lengeling}}\ and\ \bibinfo {author} {\bibfnamefont
  {Al{\'a}n}\ \bibnamefont {Aspuru-Guzik}},\ }\bibfield  {title} {\enquote
  {\bibinfo {title} {Inverse molecular design using machine learning:
  Generative models for matter engineering},}\ }\href {\doibase
  10.1126/science.aat2663} {\bibfield  {journal} {\bibinfo  {journal}
  {Science}\ }\textbf {\bibinfo {volume} {361}},\ \bibinfo {pages} {360--365}
  (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\end{thebibliography}%








% \clearpage

% \begin{figure}%[t]
% \centering
% %\includegraphics[width=.35\columnwidth]{Fig1v2.pdf}
% \caption{\textbf{Quantum Machine Learning (QML) tasks}. Quantum machine learning is usually considered for four main tasks. These include tasks where the data is either classical or quantum, and where the algorithm is either classical or quantum. Top left: tensor networks are quantum-inspired classical methods that can analyze classical data. Top right: unitary time-evolution data $U$ from a quantum system can be classically compiled into a quantum circuit. Bottom left: handwritten digits can be mapped to quantum states for classification on a quantum computer. Bottom right: molecular ground state data can be classified directly on a quantum computer. The figure shows ground state energy $E$ dependence on the distance $d$ between the atoms.}
% \label{fig:1}
% \end{figure}

% \begin{figure}%[t]
% \centering
% %\includegraphics[width=0.5\columnwidth]{New_Fig2.pdf}
% \caption{\textbf{Key Applications for QML}. QML has been envisioned to bring a computational advantage in many applications. QML can enhance quantum simulation for chemistry (e.g., molecular ground states~\cite{peruzzo2014variational}, equilibrium states~\cite{verdon2019quantum}, and time evolution~\cite{cirstoiu2020variational}) and materials science (e.g., quantum phase recognition~\cite{cong2019quantum} and generative design with a target property in mind~\cite{sanchez2018inverse}). QML can enhance quantum computing by learning quantum error correction codes~\cite{cong2019quantum,johnson2017qvector} and syndrome decoders, performing quantum control, learning to mitigate errors, and compiling and optimizing quantum circuits. QML can enhance sensing and metrology~\cite{verdon2020qmp,meyer2020variational,beckey2020variational,broughton2020tensorflow,wang2017experimental} and extract hidden parameters from quantum systems. Finally, QML may speed up classical data analysis, including clustering and classification.}
% \label{fig:applications}
% \end{figure}

% \begin{figure}[t]
% \centering
% %\includegraphics[width=0.5\columnwidth]{New_Fig6.pdf}
% \caption{ \textbf{Classification with QML.} a) The classical data $x$, i.e., images of cats and images of dogs, is  encoded into a Hilbert space via some map $x\rightarrow\ket{\psi(x)}$. Ideally, data from different classes (here represented by dots and stars)  is mapped to different regions of the Hilbert space . b) Quantum data $\ket{\psi}$ can be directly analyzed on a quantum device. Here the dataset is composed of states representing metallic  or superconducting systems.  c) The dataset is used to train a QML model. Two common paradigms in QML are quantum neural networks and quantum kernels, both of which allow for classification of either classical or quantum data. In Kernel methods one fits a decision hyperplane that separates the classes.  d) Once the model is trained, it can be used to make predictions.  }
% \label{fig:QMLsetting}
% \end{figure}

% \begin{figure}%[t]
% \centering
% %\includegraphics[width=0.5\columnwidth]{Fig_QNN_v3.pdf}
% \caption{ \textbf{Examples of QNN architectures.} a) A classical feed-forward neural network has input, hidden, and output layers. This can be generalized to the quantum setting with a dissipative QNN, where some qubits are discarded and replaced by new qubits during the algorithm. Here we show a quantum circuit representation for the dissipative QNN. In a circuit diagram each horizontal line represents a qubit, and the logical operations, or quantum gates, are represented by boxes connecting the qubit lines. Circuits are read from left to right. For instance, here the circuit is initialized in a product state $\ket{\psi_j} \otimes \ket{0}^{\otimes (N_h + N_o)}$, where $\ket{\psi_j}$ encodes the input data and $N_h$ ($N_o$) is the number of ancilla qubits in the hidden (output) layer which are initialized to the fiduciary state $\ket{0}$. As one performs logical operations, the information forward propagates through the circuit into the ancillary qubits. b) Another possible QNN strategy is to keep the qubits fixed, without discarding or replacing them. The circuit represents consecutive application of two-qubit gates $U_j$ and controlled-NOT (denoted by CNOT) gates. c) Quantum convolutional neural networks (QCNNs) measure and discard qubits during the algorithm. The QCNN circuit considered here is built with two-qubit quantum gates $U_j$ and is initialized in $\ket{\psi_j}$.}

% \label{fig:QNNexamples}
% \end{figure}

% \begin{figure}%[t]
% \centering
% %\includegraphics[width=0.5\columnwidth]{New_Fig5.pdf}
% \caption{\textbf{Challenges for QML.} a) There are several ingredients and  priors needed to build a QML model:  a dataset (and an encoding scheme for classical data), the choice of parameterized model, loss function, and classical optimizer.  In this diagram, we show some of the challenges of the different components of the model. b-d) The success of the QML model hinges on an accurate and efficient training of the parameters. However, there are certain phenomena that can hinder the QML trainability. These include the abundance of low-quality local minima solutions shown in b), as well as the barren plateau phenomenon in c).  When a QML architecture exhibits a barren plateau, the landscape becomes exponentially flat (on average) as the number of qubits increases (seen as a transition from dashed to solid line). The presence of hardware noise has been shown to erase the features in the landscape as well as potentially shift the position of the minima. Here, the dashed (solid) line corresponds to the noiseless (noisy) landscape shown in d).}
% \label{fig:5}
% \end{figure}

\end{document}
