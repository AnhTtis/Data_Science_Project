\section{Event Simulation}
\label{sec:events}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.99\linewidth]{zenith_energy_combined.eps}
\caption{\textbf{\textit{Distributions of events in true energy and zenith.}}
\textit{Left:} The distribution of events used as a function of true neutrino energy.
As expected, the generated distribution is flat when binned logarithmically since the generation was sampled according to a $E_{\nu}^{-1}$ distribution.
Furthermore, the fraction of generated events which produce light, and the fraction of light-producing events which pass the trigger threshold increase with energy.
\textit{Right:} The same distribution as a function of true neutrino zenith angle.
Once again the generated distribution is flat in the cosine of this angle, which is proportional to the differential solid angle.
nearly flat, with slightly lower efficiency near the horizon.
}
\label{fig:event_dists}
\end{figure*}

Our benchmark case follows an ice-embedded IceCube-like geometry, where the OMs are spaced our approximately 125 meters horizontally and 17 meters vertically.
The events used in this work are $\mu^{-}$ from \numu{} charged-current interactions.
The initial neutrino sampling, charged lepton propagation, and photon propagation were simulated using the \texttt{Prometheus} package~\cite{santiago_giner_olavarrieta_2022_6804954}.
The incident neutrinos have energies between $10^2$~GeV and $10^6$~GeV sampled from a power-law with a spectral index of -1.
Since most of the events that trigger neutrino telescopes are downward-going cosmic-ray muons, we generated a down-going dataset.
Specifically, the initial momenta have zenith angles between $80^{\circ}$ and $180^{\circ}$.
It is worth noting that this definition of zenith angle is different from the convention which is typically used by neutrino telescopes, which take $0^{\circ}$ to be downgoing.
Internally, \texttt{LeptonInjector}~\cite{IceCube:2020tcq} samples the energy, direction, and interaction vertex.
\texttt{PROPOSAL}~\cite{koehne2013proposal} then propagates the outgoing muon, recording all energy losses that happen within 1~km of the instrumented volume.
\texttt{PPC}~\cite{PPCStandAlone} then generates the photon yield from the hadronic shower and each muon energy loss, and propagates these photons until they either reach an OM or are absorbed.
If a photon reaches an OM, the module ID, module position and time of arrival are recorded.
%Since Antarctic glacier ice properties are not readily available in digital format, we use an approximate ice model that considers the absorption and scattering length given in Ref.~\cite{}

We then add noise in the style of~\cite{Larson:2013xbf} to the resulting photon distributions.
This model accounts for nuclear decays in the OMs' glass pressure housings and thermal emission of an electron from a PMT's photocathode.
The latter process strongly depends on the ambient temperature near the OM and varies between PMTs.
Since this information is not publically available, we simplify the model and vary the thermal noise rate linearly from 40~Hz at the top of the detector to 20~Hz near the bottom, which approximately agrees with the findings from~\cite{Larson:2013xbf}.
We then take the nuclear decay rate to be 250~Hz and generate a number of photons drawn from a Poisson distribution with a mean of 8 for each decay.

Before moving on, it is important to note that the photons generated in the previous steps are only tracked to the surface of the OM.
In a full simulation of the detection process, one would need to simulate the electronics inside the OM, which could introduce timing uncertainties.
Furthermore, the digitized signal reported by the \textit{e.g.} the IceCube OMs must be unfolded to get the number of photons per unit time.
These steps require access to proprietary information that is not available externally.
Thus, we cannot include the effects from these detail detector performance in our simulation.

For example, the process by which IceCube unfolds the photon arrival times is described in~\cite{IceCube:2013dkx}.
They find that this process introduces a timing uncertainty typically on the order of 1~ns but that may grow up 10~ns under certain conditions.
While this may affect our results, we expect the impact to be small since by group the photon arrival times into ns-wide bins, we are introducing a timing uncertainty with a similar scale.

Once all photons have been added, we then implement a trigger criteria similar to the one described in~\cite{IceCube:2016zyt}.
This requires that a pair of neighboring or next-to-neighboring OMs see light in a 1~$\mu$s time window.
If 8 such pairs are achieved in a 5~$\mu$s time window, we consider the trigger to be satisfied.
As before, the exact details of the triggering process require access to proprietary information; however, the events which pass our trigger should be qualitatively similar to those which would trigger IceCube.
After this cut, we are left with 462892 events from 3 million simulated events, which we split between the training and test data sets of 412892 and 50000 sizes respectively.
One can see distributions of the events which pass this trigger as a function of true energy, zenith, and azimuth in~\cref{fig:event_dists}.

In addition to the trigger-level dataset, we also evaluate the network performance on a dataset with further quality cuts, so that we can understand performance on events which are more likely to make it into a final analysis.
In order to do this, we consider three quantities: $N_{\rm{OM}}$, $r_{\rm{COG}}$, and $R_{\rm{ell}}$.
The first two quantities---the number of distinct OMs that saw light and the distance between the charge-weighted center of gravity and the center of the detector---are fairly straightforward, but the last requires more explanation.
To compute $R_{\rm{ell}}$, we fit a two-parameter ellipsoid to all OMs which saw light, and then take the ratio of the long axis to the short axis.
A ratio close to one indicates a spherical events, whereas larger ratios indicate longer, track-like events.

% \begin{figure*}[bt]
% \centering
% \includegraphics[width=0.99\linewidth]{figures/zenith_energy_combined.pdf}
% \caption{\textbf{\textit{Distributions of events in true energy and zenith.}}
% \textit{Left:} The distribution of events used as a function of true neutrino energy.
% As expected, the generated distribution is flat when binned logarithmically since the generation was sampled according to a $E_{\nu}^{-1}$ distribution.
% Furthermore, the fraction of generated events which produce light, and the fraction of light-producing events which pass the trigger threshold increase with energy.
% \textit{Right:} The same distribution as a function of true neutrino zenith angle.
% Once again the generated distribution is flat in the cosine of this angle, which is proportional to the differential solid angle.
% nearly flat, with slightly lower efficiency near the horizon.
% }
% \label{fig:event_dists}
% \end{figure*}

We perform straight cuts on these variables, requiring events have $N_{\rm{OM}} > 11$, $r_{\rm{COG}} < 400$~m, and $2 < R_{\rm ell} < 8$.
The first cut removes low-charge events which are difficult to reconstruct, while the second removes ``corner clipper'' events caused by $\mu^{-}$ passing near the edge of the detector.
The final cut on $R_{\rm{ell}}$ helps ensure that the events have a long lever-arm for angular reconstruction.

These cuts reduce the split training and testing dataset sizes to 108585 and 13183 events, respectively.
The spatial sparsity of these improved quality events is about $\sim3\%$, as there are 154~OMs that were hit on average, out of the 5,160 total OMs in our example detector.
For the trigger level events, the spatial sparsity is about $\sim2\%$.
The time dimension adds another level of sparsity, as typical events can last tens of thousands of nanoseconds compared to the microsecond time window.