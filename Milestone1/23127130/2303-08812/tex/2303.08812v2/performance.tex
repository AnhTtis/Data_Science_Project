\section{Performance}
\label{sec:performance}

\subsection{Training and Architecture Details}

\begin{figure*}[t]
\centering
\includegraphics[trim=0 500 0 450, clip, width=0.99\textwidth]{architecture-cropped.eps}
\caption{\textbf{\textit{Network architecture overview.}} The network accepts as input a 4D point cloud of photon OM hits, as shown by the colored points in the figure. The color indicates the timing (red is earlier, blue is later). Residual connections, denoted with $\oplus$, are used in between convolutions. Downsampling (dashed red lines) is performed after a series of convolutions. The final layer of the network is a fully connected layer, which outputs the predicted quantity. In this work, we train the network to predict the logarithmic $\nu_\mu$ energy, and the three components of the normalized $\nu_\mu$ direction vector.}
\label{fig:arch}
\end{figure*}

We utilize a ResNet-based architecture, taking advantage of residual connections between layers to promote robust learning for deeper networks.
More details on the network architecture can be found in~\cref{fig:arch}.
A typical block of the network consists of a sparse submanifold convolution, followed by batch normalization and the parametric rectified linear unit (PReLU) activation function.
The selection of the activation function was determined after examining prevalent alternatives, such as the conventional ReLU or a smooth approximation, like the SELU.
Downsampling is performed using a stride 2 sparse submanifold convolution.
We use the \texttt{PyTorch} deep learning framework and the \texttt{MinkowskiEngine}~\cite{minkowski} library to implement the network.

This article focuses on the training of three distinct models. The objective of two of these models is the prediction of the three components of the neutrino directional pointing vector ($X_\nu$, $Y_\nu$, $Z_\nu$), with one model trained on the trigger-level dataset and the other trained on the quality dataset. The directional vector is learned rather than the zenith and azimuth angles because of complications with azimuthal periodicity and undesirable boundary condition behavior at large or small angles. Another model was trained to infer the primary neutrino energy $E_\nu$. The network is trained to predict the logarithmic energy, $\log_{10}(E_\nu)$, as they vary over a wide range of magnitudes. Each model was trained for 25 epochs using a batch size of 128 and the \texttt{Adam} optimizer.
The initial learning rate was set at $0.001$ and was reduced periodically during training. Dropout and weight decay are employed to prevent overfitting.

% For the purpose of this article, we train the network to infer the primary neutrino energy $E_\nu$, and the three components of its directional pointing vector, ($X_\nu$, $Y_\nu$, $Z_\nu$).
% The directional vector is learned rather than the zenith and azimuth angles because of complications with azimuthal periodicity and undesirable boundary condition behavior at large or small angles.
% The network is trained to predict the logarithmic energy, $\log_{10}(E_\nu)$, and the normalized directional vectors, as they can vary over a wide range of magnitudes. 

To train the energy reconstruction task, the \texttt{LogCosh} loss function is used, since it is more robust to outliers than the standard \texttt{MSE} loss.
The loss function is defined as follows,
%
\begin{equation}
    \mathcal{L}_E = \frac{1}{N} \sum^N_i{\log{(\cosh{(x_i - y_i)})}},
\end{equation}
%
where $N$ is the number of events in the batch, $x_i$ are the predictions, and $y_i$ are the labels.
For the angular reconstruction, an angular distance loss function is used, namely,
%
\begin{equation}
    \mathcal{L}_A = \frac{1}{N} \sum^N_i{\arccos{\left(\frac{\vec{X_i} \cdot \vec{Y_i}}{||X_i|| \: ||Y_i||}\right)}},
\end{equation}
%
where $\vec{X_i}$ and $\vec{Y_i}$ are the predicted and true directional vectors, respectively.
% Then, the total loss is given by
% %
% \begin{equation}
%     \mathcal{L}_{tot} = (1 - \alpha)\mathcal{L}_E + \alpha\mathcal{L}_A,
% \end{equation}
% where a weighting factor $\alpha$ is applied to each of the separate loss terms. An ensemble of networks was trained from scratch while $\alpha$ was varied. Performance on both energy and angular reconstruction was tested for each of these networks to determine the optimal value of $\alpha$. We found that setting $\alpha = 0.7$ results in superior angular reconstruction performance, without significantly affecting the energy reconstruction. 

% In addition to the SSCNN, we also train a separate CNN on the same datasets that allows for feature spreading. This network would produce the same results as a traditional CNN and serves as a comparison point. However, due to memory constraints, we modified the architecture of this model by slightly reducing its depth and complexity. Additionally, the traditional CNN model can only fit a batch size of 2 during training, which hinders its learning and makes it significantly slower to train. These compromises can allow the SSCNN to outperform the traditional CNN in certain tasks. 

\subsection{Run-time Performance}

\renewcommand{\arraystretch}{1.25}
\begin{table}[bht]
    \centering
    \begin{tabular}{|p{4.25cm}|p{3cm}|}
        \hline 
        \textbf{SSCNN Angular (GPU)} & \textbf{0.101 $\pm$ 0.003 ms} \\
        \hline
        \textbf{SSCNN Energy (GPU)} & \textbf{0.103 $\pm$ 0.008 ms} \\
        \hline
        \textbf{SSCNN Angular (CPU)} & \textbf{37.7 $\pm$ 53.4 ms} \\
        \hline
        \textbf{SSCNN Energy (CPU)} & \textbf{30.6 $\pm$ 48.9 ms} \\
        \hline
        % CNN (GPU) & 14.7 $\pm$ 7.81 ms \\
        % \hline
        Likelihood Angular (CPU) & 36 $\pm$ 152 ms \\
        \hline
        Likelihood Energy (CPU) & 6.58 $\pm$ 23 ms \\
        \hline
    \end{tabular}
    \caption{\textbf{\textit{Per-event average run-time performance.}} The forward pass run-times (mean $\pm$ STD) for SSCNN was evaluated on trigger-level events. A likelihood-based method for energy and angular reconstruction was included for reference~\cite{Mirco:2017}. It should be noted that likelihood-based methods usually require seeding or initial estimates, meaning the actual runtime is longer.}
    \label{tab:runtime}
\end{table}

We evaluate the run-time performance of SSCNN in terms of the forward pass duration on both CPU and GPU hardware.
The CPU benchmark is performed on a single core of an Intel Xeon Platinum 8358 CPU, while the GPU benchmark uses a 40~GB NVIDIA A100. As is generally the case for neural networks, running on GPU is preferred due to its superior parallel computation capabilities. Additionally, the use of sparse submanifold convolutions has greatly enhanced the GPU memory efficiency, enabling us to run larger batch sizes during inference. SSCNN can reconstruct direction at a rate of 9901 Hz on a 40~GB NVIDIA A100 GPU, while handling a batch size of 12288 events simultaneously.
This is fast enough to handle the expected $\sim$kHz current and planned large neutrino telescopes.
% This rate is over four times that of the current IceCube trigger rate.

The run-time on a single-core CPU is slower and largely dependent on the number of photons hits in the event due to the limited parallel computation capabilities. However, SSCNN run-time on a CPU core is comparable to that of the likelihood-based angular method and is more consistent, as indicated by the lower standard deviation on the run-time distribution. The run-time results on both GPU and CPU are summarized in Table~\ref{tab:runtime}.

% One of the most significant benefits of switching to sparse submanifold convolutions is their improved GPU memory efficiency.
% This enables larger batch sizes during inference, taking advantage of parallel computation.
% On a 40~GB NVIDIA A100~GPU, the SSCNN can process a batch size of 12,288 events at a time.
% We also evaluate the network runtime performance on a single core of an Intel Xeon Platinum 8358 CPU.
% The runtime on a single-core CPU is slower and largely dependent on the number of photons hits in the event due to the limited parallel computation capabilities.
% Because of this, running on GPU is preferred, as is generally the case for neural networks.


\subsection{Reconstruction Performance}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{angular_results.eps}
    \caption{\textbf{\textit{Angular reconstruction performance as a function of the true neutrino energy.}} The angular resolution results are binned by the true neutrino energy, with the median taken from each bin to form the lines shown.
    }
    \label{fig:angular_results}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{energy_results.eps}
    \caption{\textbf{\textit{Energy reconstruction performance at the trigger-level.}} The solid lines show the median of the predicted $\log_{10}(E_{\nu})$, while the shaded regions are the $5\%$ to $95\%$ confidence level bands. The events in the test dataset are separated into starting and through-going events. The solid black line serves as a reference for a perfect reconstruction.
    }
    \label{fig:energy_results}
\end{figure}

We first test SSCNN on reconstructing the direction of the primary neutrino.
We measure performance using the angular resolution metric, which is calculated by taking the angular difference between the predicted and true directional vectors.
Fig.~\ref{fig:angular_results} shows the angular resolutions as a function of the true neutrino energy.
Lower-energy events generally produce less photon hits, leading to a shorter lever-arm and, consequently, worse resolution.
As expected, the trigger-level events are harder to reconstruct due to the lower light yield and the presence of corner-clipper events.
On this event selection, SSCNN is able to reach under 4\textdegree{} median angular resolution on the highest-energy events.
% However, poorly-defined events, such as the corner-clippers previously mentioned, can easily pass the SMT-8 threshold.
% These events have little directional information to provide for the network due to their morphology.
Enforcing the previously described quality cuts improves the results of the SSCNN by roughly 2\textdegree{} across the entire energy range.
This performance is comparable to or better than current trigger-level reconstruction methods used in neutrino telescopes.
For example, the current trigger-level direction reconstruction at IceCube is done using the traditional \texttt{Linefit} algorithm~\cite{linefit}, which has a median angular resolution of approximately $10$\textdegree{} on raw data. 

We also test SSCNN on reconstructing the energy of the primary neutrino.
Fig.~\ref{fig:energy_results} summarizes the energy reconstruction results.
Events where the interaction point of the neutrino occurs outside the detector, known as through-going events, make up the majority of our dataset.
As a result, predicting the neutrino energy has an inherent, irreducible uncertainty produced by the unknown interaction vertex and the muon losses outside of the detector.
This missing-information problem leads to an intrinsic uncertainty in the logarithmic neutrino energy of approximately $0.4$ for a through-going event.
The behavior observed between 100 GeV and 1 TeV is due to the muon being in the minimally ionizing regime, up to around 700 GeV. 
Additionally, the network has a tendency to overpredict at the lowest energies, and underpredict at highest energies.
This can be attributed to the artificial energy bounds on the simulated training dataset. 
