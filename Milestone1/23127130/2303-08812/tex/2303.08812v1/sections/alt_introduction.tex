\section{Introduction}
\label{sec:intro}

Gigaton-scale neutrino telescopes have opened a new window to the Universe, allowing us to study the highest energy neutrinos.
While there are a variety of proposed designs, many follow the detection principle outlined by the DUMAND project~\cite{RevModPhys.64.259} and consist of an array of optical modules (OMs) deployed in liquid or solid water.
This detector paradigm shows great promise, and analyses by these experiments have already provided the first evidence of astrophysical neutrino sources~\cite{IceCube:2018cha,IceCube:2022der}.
Before they can be analyzed, however, high-energy neutrinos must be isolated from the immense cosmic-ray-muon-induced background.
While a high-energy neutrino may trigger a detector once every few minutes, cosmic-ray muons typically induce a trigger rate on the order of kHz.

\begin{figure}[bht!]
\centering
\includegraphics[width=0.47\textwidth]{figures/event_rates.png}
\caption{\textbf{\textit{Event rates of triggers in different neutrino telescopes~\cite{IceCube_triggers, baikaltdr, km3net_trigger} compared to the run-times of various reconstruction methods.}} Sparse submanifold CNNs and their performance are detailed in this article. The CNN and maximum likelihood method run-times are taken from~\cite{Mirco:2017}. Notably, sparse submanifold CNNs can process events well above standard trigger rates in both ice- and water-based experiments.}
\label{fig:event_rates}
\end{figure}

Since they are unable to traverse a substantial portion of the Earth without coming to rest, cosmic-ray muons have a distinct zenith dependence.
This allows them to be removed by cutting on the reconstructed direction of an event.
Thus, a reliable reconstruction that is capable of keeping up with the $\sim$kHz background rate is the first step in isolating neutrinos.
Moreover, a rapid reconstruction method could serve as part of an alert system that notifies researchers of events that are highly likely to be astrophysical neutrinos.
For example, the real-time follow up of such an IceCube event led to the observation of the first astrophysical neutrino source candidate, TXS 0506+056, by detecting a neutrino in coincidence with a gamma-ray flare~\cite{IceCube:2018cha}.
Along this line, similar efforts are underway in water-based detectors such as ANTARES, see~\cite{Albert:2022oul} for a recent review.

At the trigger-level, a simple but fast reconstruction is typically done by solving a least squares problem via matrix inversion, as is the case for \texttt{LineFit}~\cite{linefit} in IceCube or \texttt{QFit} in ANTARES~\cite{ANTARES:2011vtx}.
Machine learning has shown promise by delivering a comparable quality reconstruction with less runtime requirements~\cite{Mirco:2017,Garcia-Mendez:2021vts}; however, the fastest convolutional neural network (CNN) developed for high-energy neutrinos is not able to keep pace with a kHz-scale trigger-level rate.
In this article, we introduce a reconstruction method using a sparse submanifold CNN (SSCNN), which overcomes this runtime issue.
We will illustrate our method by focusing on solid-water detectors, but our results and conclusions readily generalize to water-based detectors.
In this context, our SSCNN achieves better angular resolutions than methods such as \texttt{Linefit} while requiring a comparable run-time, enabling improved trigger-level cuts and serving as a better seed for the likelihood-based reconstruction. 
Fig.~\ref{fig:event_rates} summarizes typical event rates found in neutrino telescopes and compares these to the execution rate of various reconstructions. At the same time, SSCNN is also able to reconstruct the neutrino energy, a task which has not been done at trigger-level. 

While the rest of this article will concentrate on the implementation of SSCNN in an ice-embedded IceCube-like detector, it should be noted that our method is also applicable to water-based neutrino telescopes, where we expect similar performance gains.
The rest of this article is organized as follows.
In Sec.~\ref{sec:architecture} we motivate and introduce sparse submanifold convolutions; in Sec.~\ref{sec:events} we describe the data sets used for training and testing; in Sec.~\ref{sec:performance} we evaluate the performance of the network.
Finally, in Sec.~\ref{sec:conclusion} we conclude with some parting words.
The code detailing our implementation of SSCNN has been made available at Ref.~\cite{GithubCode}.