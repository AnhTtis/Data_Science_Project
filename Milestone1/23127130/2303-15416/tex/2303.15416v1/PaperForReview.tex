% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
% \usepackage{hyperref}
\input{math_commands.tex}
% \usepackage{xcolor}
% \usepackage[pagebackref=false, breaklinks=true, letterpaper=true, colorlinks,
%             citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}
% \definecolor{citecolor}{HTML}{0071BC}
% \definecolor{linkcolor}{HTML}{ED1C24}
\usepackage[dvipsnames]{xcolor}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks, bookmarks=false]{hyperref}
% \hypersetup{
%     citecolor=ForestGreen,
%     % urlcolor=MidnightBlue,
% }
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=citecolor]{hyperref}
% \definecolor{citecolor}{HTML}{0071BC}
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{url}
\usepackage{graphicx}
% \graphicspath{{./vis/}}
% \graphicspath{{vis}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{animate}
% \usepackage{subfig}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{subcaption}
% \usepackage{movie15}
% \usepackage{floatrow}
% \usepackage{animate}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1253} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\newcommand{\methodname}{BA-Det}
\newcommand\red[1]{\textcolor{red}{#1}}
\newcommand\cyan[1]{\textcolor{cyan}{#1}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{3D Video Object Detection with Learnable Object-Centric Global Optimization}

\author{
        Jiawei He$^{1,2}$ \quad 
        Yuntao Chen$^{3}$ \quad 
        Naiyan Wang$^{4}$ \quad 
        Zhaoxiang Zhang$^{1,2,3}$ \\
        $^{1}$ CRIPAC, Institute of Automation, Chinese Academy of Sciences (CASIA)\\
        $^{2}$ School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS)\\
        $^{3}$ Centre for Artificial Intelligence and Robotics, HKISI\_CAS\quad
        $^{4}$ TuSimple\\
        {\tt\small
 \{hejiawei2019, zhaoxiang.zhang\}@ia.ac.cn \{chenyuntao08, winsty\}@gmail.com}
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle

\begin{abstract}
We explore long-term temporal visual correspondence-based optimization for 3D video object detection in this work.
% Visual correspondence refers to sparse or dense one-to-one mapping across pixels from multiple images, either spatially or temporally.
% This kind of information has been well-known to the SLAM community while less explored in 3D object detection.
Visual correspondence refers to one-to-one mappings for pixels across multiple images.
Correspondence-based optimization is the cornerstone for 3D scene reconstruction but is less studied in 3D video object detection, because moving objects violate multi-view geometry constraints and are treated as outliers during scene reconstruction.
We address this issue by treating objects as first-class citizens during correspondence-based optimization.
% We solve this problem by transforming all correspondence into a unified object-centric reference frame.
% Besides that, high performance object SfM also requires efficiently solving the large-scale bundle adjustment with accurate and long-term visual correspondence.
In this work, we propose \methodname{}, an end-to-end optimizable object detector with object-centric temporal correspondence learning and featuremetric object bundle adjustment.
Empirically, we verify the effectiveness and efficiency of \methodname{} for multiple baseline 3D detectors under various setups.
Our \methodname{} achieves SOTA performance on the large-scale Waymo Open Dataset (WOD) with only marginal computation cost. 
Our code is available at \url{https://github.com/jiaweihe1996/BA-Det}.

% \yuntao{historical review should be placed in the introduction section}Although image-based 3D object detection has made great progress in the past several years, video-based 3D object detection is far from well explored. 
% Pioneering work treats it as a simple post-process of the single-frame predictions without feature learning. 
% Recently, learning-based methods ignore the object-level geometric constraints, and cannot handle moving objects effectively. 
% Besides, long-term temporal information is also underutilized. We expect to explore a paradigm that can handle both static and moving objects and utilize long-term temporal information. 
% In this paper, we propose a video-based 3D object detection paradigm with learnable object-centric global optimization, called \emph{\methodname{}}. 
% The object-centric manner makes \methodname{} agnostic to object motion. 
% And global optimization leverages long-term temporal information.
% Specifically, we design a temporal correspondence learning module supervised by an object-centric bundle adjustment loss. 
% During inference, \methodname{} optimizes the object pose to boost the accuracy of single-frame predictions. 
% We achieve state-of-the-art performance on the large-scale Waymo Open Dataset (WOD). 
% Code will be released.
\end{abstract}
\section{Introduction}
\begin{figure*}[t]
\centering
      \includegraphics[width = 0.88\textwidth]{figures/fig1.pdf}
      \vspace{-15pt}
\end{figure*}
\begin{figure*}[t]
\centering

	\subfloat[Temporal Filtering]{\includegraphics[width = 0.22\textwidth]{figures/fig1a.pdf}\label{fig1a}}
	\hfill
	\subfloat[Temporal BEV]{\includegraphics[width = 0.22\textwidth]{figures/fig1b.pdf}\label{fig1b}}
	\hfill
	\subfloat[Stereo from Video]{\includegraphics[width = 0.22\textwidth]{figures/fig1c.pdf}\label{fig1c}} 
        \hfill
        \subfloat[\methodname{} (Ours)]{\includegraphics[width = 0.22\textwidth]{figures/fig1d.pdf}\label{fig1d}}
\caption{Illustration of how to leverage temporal information in different 3D video object detection paradigms.}
\label{fig1}
\end{figure*}
% \cyan{Structure of intro beginning is not proper, should be: 3D det --- image-based 3D det --- image-based with temporal --- temporal visual correspondence}
3D object detection is an important perception task, especially for indoor robots and autonomous-driving vehicles. 
Recently, image-only 3D object detection~\cite{zhang2021objects,li2022bevformer} has been proven practical and made great progress. 
In real-world applications, cameras capture video streams instead of unrelated frames, which suggests abundant temporal information is readily available for 3D object detection. 
% Especially, the Bird's-eye view (BEV) paradigm~\cite{li2022bevformer} is leading the new trend for image-based 3D object detection. 
In single-frame methods, despite simply relying on the prediction power of deep learning, 
finding correspondences play an important role in estimating per-pixel depth and the object pose in the camera frame. 
Popular correspondences include Perspective-n-Point (PnP) between pre-defined 3D keypoints ~\cite{zhang2021objects,li2022dcd} and their 2D projections in monocular 3D object detection, and Epipolar Geometry~\cite{chen2020dsgn,guo2021liga} in multi-view 3D object detection.
% \cyan{how about LoFTR?}
However, unlike the single-frame case, temporal visual correspondence has not been explored much in 3D video object detection.

% In real-world applications, cameras capture video streams instead of unrelated frames, which suggests abundant temporal information is readily available for 3D object detection. 
% However, unlike the single frame case, temporal visual correspondence has not been explored much in video 3D object detection.
As summarized in Fig.~\ref{fig1}, existing methods in 3D video object detection can be divided into three categories while each has its own limitations. 
Fig.~\ref{fig1a} shows methods with object tracking~\cite{brazil2020kinematic}, especially using a 3D Kalman Filter to smooth the trajectory of each detected object. 
This approach is detector-agnostic and thus widely adopted, but it is just an output-level smoothing process without any feature learning. As a result, the potential of video is under-exploited. 
Fig.~\ref{fig1b} illustrates the temporal BEV (Bird's-Eye View) approaches ~\cite{li2022bevformer,huang2022bevdet4d,liu2022petrv2} for 3D video object detection.
They introduce the multi-frame temporal cross-attention or concatenation for BEV features in an end-to-end fusion manner. 
As for utilizing temporal information, temporal BEV methods rely solely on feature fusion while ignoring explicit temporal correspondence.
% Except for ego-motion compensation, these methods make no other use of geometry information from videos.
Fig.~\ref{fig1c} depicts stereo-from-video methods~\cite{wang2022dfm,wang2022sts}.
These methods explicitly construct a pseudo-stereo view using ego-motion and then utilize the correspondence on the epipolar line of two frames for depth estimation.
However, the use of explicit correspondence in these methods is restricted to only two frames, thereby limiting its potential to utilize more temporal information.
% Although explicit correspondence is incorporated, their potential is limited to two frames, and they cannot use more temporal information.
Moreover, another inevitable defect of these methods is that moving objects break the epipolar constraints, which cannot be well handled, so monocular depth estimation has to be reused.
% So these methods generally fuse inaccurate stereo depth estimation with monocular depth estimation and leave the end-to-end detection network to adaptively choose from two sources of depth information. \cyan{(The last sentence is obscure, consider removing it.)}

% Although with epipolar constraint, it has an inevitable problem. 
% The moving object in two temporal frames does not satisfy the epipolar constraint. 
% So they can only add the single frame residual branch to make up for this problem.

% Besides, all of the above paradigms can only exploit the temporal information in a very small time window. \yuntao{I don't think is true for KF and BEV, both using temporal information over a infinite horizon. Only stereo is confined within 2 frames.}
% The long-range time dependency is ignored.

% Only a few existing methods can be divided to be three categories: 3D Kalman Filter~\cite{brazil2020kinematic} to smooth object trajectory, underutilizing the temporal constraints; feature-level attention in Bird's-eye view (BEV)~\cite{li2022bevformer,huang2022bevdet4d,liu2022petrv2}, without geometric constraints; regard two temporal views as stereo~\cite{wang2022dfm}, ignoring the moving objects.\par
% For video-based 3D object detection, the same instance in different frames can share the same feature space, which helps to detect the object in bad measurement. 
Considering the aforementioned shortcomings, we seek a new method that can \emph{handle both static and moving objects}, and \emph{utilize long-term temporal correspondences}. 
Firstly, in order to handle both static and moving objects, we draw experience from the object-centric global optimization with reprojection constraints in Simultaneous Localization and Mapping (SLAM)~\cite{yang2019cubeslam,li2018stereo}. 
Instead of directly estimating the depth for each pixel from temporal cues, we utilize them to construct useful temporal constraints to refine the object pose prediction from network prediction.
% \footnote{Check whether this claim is accurate.} 
Specifically, we construct a non-linear least-square optimization problem with the temporal correspondence constraint in an object-centric manner to optimize the pose of objects no matter whether they are moving or not. %Different from the traditional objective of SLAM which aims to optimize the ego camera pose in the global frames, we optimize the pose of each object. So a unified temporal optimization can be applied to all objects, regardless of whether they are moving or not. 
% \yuntao{Add some details to object-centric formulation} 
%So, a unified temporal optimization can be applied to both static and dynamic objects. 
% \cyan{(what is the relationship between last sentence and correspondence learning here?)}
Secondly, for long-term temporal correspondence learning, hand-crafted descriptors like SIFT~\cite{lowe2004sift} or ORB~\cite{rublee2011orb} are no longer suitable for our end-to-end object detector. Besides, the long-term temporal correspondence needs to be robust to viewpoint changes and severe occlusions, where these traditional sparse descriptors are incompetent. So, we expect to learn a dense temporal correspondence for all available frames.\par
% \yuntao{Add details for learnable visual correspondence.}
% \red{}
In this paper, as shown in Fig.~\ref{fig1d}, we propose a 3D video object detection paradigm with learnable long-term temporal visual correspondence, called \emph{\methodname{}}. 
Specifically, the detector has two stages. In the first stage, a CenterNet-style monocular 3D object detector is applied for single-frame object detection. 
After associating the same objects in the video, the second stage detector extracts RoI features for the objects in the tracklet and matches dense local features on the object among multi-frames, called the object-centric temporal correspondence learning (OTCL) module.
To make traditional object bundle adjustment (OBA) learnable, we formulate featuremetric OBA.
In the training time, with featuremetric OBA loss, the object detection and temporal feature correspondence are learned jointly.
During inference, we use the 3D object estimation from the first stage as the initial pose and associate the objects with 3D Kalman Filter. The object-centric bundle adjustment refines the pose and 3D box size of the object in each frame at the tracklet level, taking the initial object pose and temporal feature correspondence from OTCL as the input. 
% In this work, we show that our \methodname{} introduces a new approach for video 3D object detection. 
Experiment results on the large-scale Waymo Open Dataset (WOD) show that our \methodname{} could achieve state-of-the-art performance compared with other single-frame and multi-frame object detectors. 
We also conduct extensive ablation studies to demonstrate the effectiveness and efficiency of each component in our method.\par
% Especially, we outperform the 2022 WOD challenge winner BEVFormer~\cite{li2022bevformer} and MVFCOS3D++~\cite{wang2022mvfcos3d++}. \par
In summary, our work has the following contributions:
\begin{itemize}[leftmargin=*]
    \item We present a novel object-centric 3D video object detection approach \emph{\methodname{}} by learning object detection and temporal correspondence jointly. 
   \item We design the second-stage object-centric temporal correspondence learning module and the featuremetric object bundle adjustment loss.
   % learns an object-centric multi-frame feature correspondence learning module and a differentiable object-centric bundle adjustment loss.
    \item We achieve state-of-the-art performance on the large-scale WOD. The ablation study and comparisons show the effectiveness and efficiency of our \methodname{}.
\end{itemize}



\section{Related Work}
\subsection{3D Video Object Detection}
% In the 2D Video Object Detection task, due to the difficulties like motion blur, occlusion, and viewpoints changes, message passing between frames is a common module to aggregate temporal information. FGFA~\cite{zhu2017flow} is an end-to-end algorithm utilizing optical flow between frames to warp the corresponding features to the current frame and aggregate them by the attentional mechanism. SELSA~\cite{wu2019sequence} aggregates the object-level RoI features from other frames. MaskTrack R-CNN~\cite{yang2019video} first focuses on the instance mask instead of the bounding box to represent an object in the video.\par
For 3D video object detection, LiDAR-based methods~\cite{caesar2020nuscenes,yin2021center,fan2022embracing} usually align point clouds from consecutive frames by compensating ego-motion and simply accumulate them to alleviate the sparsity of point clouds. Object-level methods~\cite{qi2021offboard,you2022hindsight,chen2022mppnet,fan2023super}, handling the multi-frame point clouds of the tracked object, become a new trend. 
3D object detection from the monocular video has not received enough attention from researchers. Kinematic3D~\cite{brazil2020kinematic} is a pioneer work decomposing kinematic information into ego-motion and target object motion. However, they only apply 3D Kalman Filter~\cite{kalman1960new} based motion model for kinematic modeling and only consider the short-term temporal association (4 frames). Recently, BEVFormer~\cite{li2022bevformer} proposes an attentional transformer method to model the spatial and temporal relationship in the bird’s-eye-view (BEV). A concurrent work, DfM~\cite{wang2022dfm}, inspired by Multi-view Geometry, considers two frames as stereo and applies the cost volume in stereo to estimate depth. However, how to solve the moving objects is not well handled in this paradigm.

\subsection{Geometry in Videos}
Many researchers utilize 3D geometry in videos to reconstruct the scene and estimate the camera pose, which is a classic topic of computer vision. Structure from Motion (SfM)~\cite{schoenberger2016sfm} and Multi-view Stereo (MVS)~\cite{schoenberger2016mvs} are two paradigms to estimate the sparse and dense depth from multi-view images respectively. In robotics, 3D geometry theory is applied for Simultaneous Localization and Mapping (SLAM)~\cite{mur2015orb}. To globally optimize the 3D position of the feature points and the camera pose at each time, bundle adjustment algorithm~\cite{triggs1999bundle} is widely applied. However, most of them can only handle static regions in the scene. \par

In the deep learning era, with the development of object detection, object-level semantic SLAM~\cite{nicholson2018quadricslam,li2018stereo,yang2019cubeslam} is rising, aiming to reconstruct the objects instead of the whole scene. These methods can handle dynamic scenes and help the object localization in the video. Besides, feature correspondence learning~\cite{sarlin2020superglue,sun2021loftr} has received extensive attention in recent years. Deep learning has greatly changed the pipeline of feature matching. Differentiable bundle adjustment, like BANet~\cite{tang2018banet} and NRE~\cite{germain2021neural}, makes the whole 3D geometry system end-to-end learnable. Unlike these works, we focus on the representation of the 3D object and integrate feature correspondence learning into 3D object detection. Utilizing the learned temporal feature correspondence, the proposed \methodname{} optimizes the object pose of a tracklet in each frame.\par

\begin{figure*}[t]
          \centering
           \includegraphics[width=0.85\linewidth]{figures/pipeline1.pdf}
             \caption{A overview of the proposed \methodname{} framework. The left part of the framework is the first-stage object detector to predict the 3D object and its 2D bounding box. The second stage is called \emph{OTCL} module. In the OTCL module, we extract the RoI features $^k\mathbf{F}^t$ by RoIAlign, aggregate the RoI features and learn object-centric temporal correspondence using featuremetric object bundle adjustment loss.}
             \label{fig:pipeline}
\end{figure*} 
\section{Preliminary: Bundle Adjustment}
Bundle Adjustment~\cite{triggs1999bundle} is a widely used globally temporal optimization technology in 3D reconstruction, which means optimally adjusting bundles of light rays from a given 3D global position to the camera center among multi-frames. 
Specifically, we use $\mathbf{P}_i=[x_i,y_i,z_i]^\top$ to denote the $i$-th 3D point coordinates in the global reference frame. 
According to the perspective camera model, the image coordinates of the projected 3D point at time $t$ is
\begin{equation}
\Pi(\mathbf{T}^t_{cg},\mathbf{P}_i,\mathbf{K})=\frac{1}{z_i^t}\mathbf{K}(\mathbf{R}^t_{cg}\mathbf{P}_i+\mathbf{t}^t_{cg}),
\end{equation}
where $\Pi$ is the perspective projection transformation, $\mathbf{T}^t_{cg}=[\mathbf{R}^t_{cg}|\mathbf{t}^t_{cg}]$ is the camera extrinsic matrix at time $t$.
$\mathbf{R}^t_{cg}$ and $\mathbf{t}^t_{cg}$ are the rotation and the translation components of $\mathbf{T}^t_{cg}$, respectively. 
$\mathbf{K}$ is the camera intrinsic matrix, and
$z_i^t$ is the depth of the $i$-th 3D point in the camera frame at time $t$.

Bundle adjustment is a nonlinear least-square problem to minimize the reprojection error as:
\begin{equation}
\begin{split}
    &\{\bar{\mathbf{T}}^t_{cg}\}_{t=1}^T,\{\bar{\mathbf{P}}_{i}\}_{i=1}^m=\\
    &\argmin_{\{\mathbf{T}^t_{cg}\}_{t=1}^T,\{\mathbf{P}_i\}_{i=1}^m}\frac{1}{2}\sum_{i=1}^{m}\sum_{t=1}^{T}||\mathbf{p}_i^t-\Pi(\mathbf{T}^t_{cg},\mathbf{P}_i,\mathbf{K})||^2,
\end{split}
\end{equation}
where $\mathbf{p}_i^t$ is the observed image coordinates of 3D point $\mathbf{P}_i$ on frame $t$.
Bundle adjustment can be solved by Gauss-Newton or Levenberg–Marquardt algorithm effectively~\cite{Agarwal_Ceres_Solver_2022,kummerle2011g2o}.
% \footnote{Reference for the solver.}

\section{\methodname{}: Object-centric Global Optimizable Detector}
\label{sec:det}
% \begin{figure*}[t]
% \centering
% 	\subfloat{\includegraphics[width=0.7\linewidth]{figures/pipeline.pdf}}
% 	\hfill
% 	\subfloat{\includegraphics[width=0.3\textwidth]{figures/corrmap.pdf}}
%  \caption{Pipeline}
%  \label{fig:pipeline}
% \end{figure*} 
In this section, we introduce the framework of our \methodname{} (Fig.~\ref{fig:pipeline}), a learnable object-centric global optimization network. 
The pipeline consists of three parts: (1) First-stage single frame 3D object detection; (2) Second-stage object-centric temporal correspondence learning (OTCL) module; (3) Featuremetric object bundle adjustment loss for temporal feature correspondence learning.
% \subsection{Multi-Frame Object Global Optimization}
\subsection{Single-frame 3D Object Detection}
Given a video clip with consecutive frames $\mathcal{V}=\{I_1,I_2,\cdots,I_T\}$, 3D video object detection is to predict the class and the 3D bounding box of each object in each frame. 
Let $\mathcal{O}_k^t$ be the $k$-th object in frame $t$. 
For the 3D bounding box $\mathbf{B}_k^t$, we estimate the size of the bounding box $\mathbf{s}_t^k=[w,h,l]^\top$ and the object pose $^k\mathbf{T}^{t}_{co}$ in the camera frame, including translation $^k\mathbf{t}^{t}_{co}=[x_c,y_c,z_c]^\top$ and rotation $^k\mathbf{r}^{t}_{co}=[r_x,r_y,r_z]^\top$. 
In most 3D object detection datasets, with the flat ground assumption, only yaw rotation $r_y$ is considered.
\par
% \methodname{} consists of two stages. 
% We use a variant of CenterNet~\cite{zhou2019objects} as our first stage object detector for individual frames. 
We basically adopt MonoFlex~\cite{zhang2021objects} as our first-stage 3D object detector, which is a simple and widely-used baseline method.
Different from the standard MonoFlex, we make some modifications for simplicity and adaptation.
% We follow MonoFlex~\cite{zhang2021objects} in foreground label assignment by using the projected 3D object center on the image. 
% When the projected center is outside the image, we set the target to be the intersection between image edge and the line from 2D center to 3D projected center. We estimate the uncertainty of each bounding box as in MonoFlex. 
(1) Instead of ensemble the depth from keypoints and regression, we only used the regressed depth directly.
(2) The edge fusion module in MonoFlex is removed for simplicity and better performance.
The output of the first-stage object detector should be kept for the second stage. The predicted 2D bounding box $\mathbf{b}_k^t$ for each object is used for the object-centric feature extraction in the second stage. The 3D estimations should be the initial pose estimation and be associated between frames. We follow ImmortalTracker~\cite{wang2021immortal} to associate the 3D box prediction outputs with a 3D Kalman Filter frame by frame. For convenience and clarity, we use the same index $k$ to denote the objects belonging to the same tracklet in the video from now on.
% ImmortalTracker simply adopts 3D Kalman Filter with large max to estimate motion state, which is proven very effective.


\subsection{Object-Centric Temporal Correspondence Learning}
\label{sec:learn2dfeat}
% \begin{wrapfigure}{r}{0.4\textwidth}
% % \vspace{-20pt}
% \begin{center}
% \includegraphics[width=0.4\textwidth]{figures/corrmap.pdf}
% \end{center}
% \caption{Correspondence map.}
% \end{wrapfigure}
% The first-stage detector and the tracker offer us associated bounding box predictions in each frame.
Based on the predictions from the first-stage detector, we propose an object-centric temporal correspondence learning (\emph{OTCL}) module, which plays an indispensable role in the learnable optimization.
Specifically, the OTCL module is designed to learn the correspondence of the dense features for the same object among all available frames. 
Given a video $\{I_1,I_2,\cdots,I_T\}$ and image features $\{\mathbf{F}^1,\mathbf{F}^2,\cdots,\mathbf{F}^T\}$ from the backbone in the first stage, we extract the RoI features $^k\mathbf{F}^t\in \R^{H\times W\times C}$ of the object $\mathcal{O}^t_k$ by the RoIAlign operation~\cite{he2017mask},
\begin{equation}
    ^k\mathbf{F}^t=\mathtt{RoIAlign}(\mathbf{F}^t,\mathbf{b}_k^{t}).
\end{equation}
% where $\mathbf{F}_i^t\in \R^{W\times H\times C}$. \par

We apply $L$ layers of cross- and self-attention operations before calculating the correspondence map to aggregate and enhance the spatial and temporal information for RoI features. 
Note that the object tracklet is available with the aforementioned tracker, so the cross-attention is applied between the objects in different frames for the same tracklet.
% \yuntao{l-th group undefined, is it groups in MHSA?}
For each layer of attention operations between two adjacent frames $t$ and $t'$:
\begin{equation}
    \begin{cases}
    ^k\widetilde{\mathbf{F}}^{t}=\mathtt{Att_S}(Q,K,V)=\mathtt{Att_S}(^k\hat{\mathbf{F}}^{t},{}^k\hat{\mathbf{F}}^{t},{}^k\hat{\mathbf{F}}^{t}),\\
    ^k\widetilde{\mathbf{F}}^{t'}=\mathtt{Att_S}(Q,K,V)=\mathtt{Att_S}(^k\hat{\mathbf{F}}^{t'},{}^k\hat{\mathbf{F}}^{t'},{}^k\hat{\mathbf{F}}^{t'}),\\
    ^k\hat{\mathbf{F}}^{t'}=\mathtt{Att_T}(Q,K,V)=\mathtt{Att_T}(^k\widetilde{\mathbf{F}}^{t'},{}^k\widetilde{\mathbf{F}}^{t},{}^k\widetilde{\mathbf{F}}^{t}),
    % (^k\hat{\mathbf{F}}_i^{t(l)},^*\hat{\mathbf{F}}_i^{t(l)}),\\
    % ^k\hat{\mathbf{F}}_i^{t'(l)}=\mathtt{Att_S}(^k\hat{\mathbf{F}}_i^{t'(l)},^*\hat{\mathbf{F}}_i^{t'(l)}),\\
    % ^k\hat{\mathbf{F}}_i^{t'(l)}=\mathtt{Att_T}(^k\hat{\mathbf{F}}_i^{t'(l)},^*\hat{\mathbf{F}}_i^{t'(l)}),
    \end{cases}
\end{equation}
% \footnote{Will the temporal cross attention use the results from self-attention? If yes, change the notation on the left of first two equations, and the notations in third one correspondingly.}
where $^k\hat{\mathbf{F}}^{t} \in \mathbb{R}^{HW\times C}$ is the flattened RoI feature, $\mathtt{Att_S}$ is the spatial self-attention, $\mathtt{Att_T}$ is the temporal cross-attention.\par
% where $^k\hat{\mathbf{F}}_i^{t}$ is the $k$-th feature of RoI feature $\mathbf{F}_i^t$ in frame $t$. \yuntao{TODO: revise the symbols}
We then define the spatial correspondence map between two flattened RoI features after the attention operations. 
%We then define the correspondence map for each local feature of the object. \footnote{notation quite confused, subscript $i$ should appear in all the following equations.}
In frame pair $(t,t')$, we use $^k\mathbf{f}_i$ to denote $i$-th local feature in $^k\hat{\mathbf{F}}^{(L)}$ ($i\in \{1,2, \cdots, HW\}$).
% suppose that the 
% \footnote{should be $k \in [1 .. HW]$ th part of object $i$?}
% is at $(h_k,w_k)$ coordinate, 
The correspondence map $^k\mathbf{C}_{t}^{t'}\in \R^{HW\times HW}$ in two frames is defined as the inner product of two features in two frames:
\begin{equation}
    ^k\mathbf{C}_{t}^{t'}[i,i'] = {^k\mathbf{f}_i^{t}}*{^{k}\mathbf{f}_{i'}^{t'}}.
\end{equation}
To normalize the correspondence map, we perform softmax over all spatial locations $i'$,
\begin{equation}
    ^k\widetilde{\mathbf{C}}_{t}^{t'}[i,i'] = \mathtt{softmax}(^k\mathbf{C}_{t}^{t'}[i,i']).
\end{equation}
% the output of the correspondence network $\mathcal{H}_\theta({^k\mathbf{f}_i^t})$ is the $i$-th of the heatmap $^k\bar{\mathbf{C}}_{t}^{t'}$:
% \begin{equation}
%     ^k\hat{\mathbf{p}}_{i}^{t'} = \argmax_{i'}{}^k\bar{\mathbf{C}}_{t}^{t'}[i,i']
% \end{equation}




\subsection{Featuremetric Object Bundle Adjustment Loss}

In this subsection, we present that how to adapt and integrate the Object-centric Bundle Adjustment (OBA) into our learnable \methodname{} framework, based on the obtained correspondence map. Generally speaking, we formulate the featuremetric OBA loss to supervise the temporal feature correspondence learning. Note that here we only derive the tracklet-level OBA loss for the same object, and for the final supervision we will sum all the tracklet-level loss in the video.\par
First, we revisit the object-centric bundle adjustment, as shown in Fig.~\ref{fig:oba}. 
As proposed in Object SLAM~\cite{yang2019cubeslam,li2018stereo}, OBA assumes that the object can only have rigid motion relative to the camera. 
For the object $\mathcal{O}_k$, we denote the 3D points as $\mathcal{P}_{k}=\{^{k}\mathbf{P}_i\}_{i=1}^m$ in the object frame, 2D points as $\{^k\mathbf{p}_i^t\}_{i=1}^m$, 2D features at position $^k\mathbf{p}_i^t$ as $\{\mathbf{f}[^k\mathbf{p}_i^t]\}_{i=1}^m$, and the camera pose in the object reference frame as $\mathcal{T}_k=\{^k\mathbf{T}^{t}_{co}\}_{t=1}^T$, OBA can be casted as:
% \footnote{Should explain how the features are projected by intrinsic matrix.}
\begin{equation}
\begin{split}
    &\bar{\mathcal{T}}_k,\bar{\mathcal{P}}_k=\argmin_{\mathcal{T}_k,\mathcal{P}_k}\frac{1}{2}\sum_{i=1}^{m}\sum_{t=1}^{T}||^k\mathbf{p}_i^t-\Pi(^k\mathbf{T}^{t}_{co},{^k\mathbf{P}_i},\mathbf{K})||_2^2.\\
    % &\bar{\mathcal{T}}_k,\bar{\mathcal{P}}_k=\\
    % &\argmin_{\mathcal{T}_k,\mathcal{P}_k}\frac{1}{2}\sum_{i=1}^{m}\sum_{t=1}^{T}\sum_{t'=1}^{T}||{^k}\mathbf{p}_i^t-\Pi(^k\mathbf{T}^{t'}_{co},{^k\mathbf{P}_i},\mathbf{K})||_2^2,
\end{split}
\label{eq:objectba}
\end{equation}
To make the OBA layer end-to-end learnable, we formulate featuremetric~\cite{lindenberger2021pixel} OBA:
% \footnote{Should explain how the features are projected by intrinsic matrix.}
\begin{equation}
\begin{split}
    &\bar{\mathcal{T}}_k,\bar{\mathcal{P}}_k=\\
    &\argmin_{\mathcal{T}_k,\mathcal{P}_k}\frac{1}{2}\sum_{i=1}^{m}\sum_{t=1}^{T}\sum_{t'=1}^{T}||\mathbf{f}[^k\mathbf{p}_i^t]-\mathbf{f}[\Pi(^k\mathbf{T}^{t'}_{co},{^k\mathbf{P}_i},\mathbf{K})]||_2^2,
\end{split}
\label{eq:featobjectba}
\end{equation}
where $\mathbf{f}[\mathbf{p}]$ denotes the feature vector in pixel coordinates $\mathbf{p}$.
% To make the object-centric bundle adjustment layer end-to-end learnable, we a design differentiable object-centric BA loss from Eq.~\ref{eq:objectba}. 
% \red{Eq.~\ref{eq:featobjectba} requires the the ground-truth correspondence during training. With the ground-truth depth and object pose, we can solve the pixel coordinate (i.e., correspondence) of point $\mathbf{p^{t'}}$ in frame $t$. }
% During training, we use the ground-truth object pose in Eq.~\ref{eq:featobjectba} to learn the correspondence between 2D features. 
% And at inference, we estimate the object pose from the matched 2D features in different frames by minimizing the reprojection error.
% to maximize the Posterior $P(X|f)$\yuntao{where does max the posterior come from, why not just use "by minimizing the overall feature distances"}. 
% First, considering the featuremetric reprojection error of frame $t$
Representing the 3D point $^k\mathbf{P}_i$ in Eq.~\ref{eq:featobjectba} with 2D points in each frame, the featuremetric reprojection error of frame $t$ could be derived as
\begin{align}
    ^ke_{i}^{t}&=\sum_{t'=1}^T\mathbf{f}[{^k\mathbf{p}_i^{t}}]-\mathbf{f}[{^k\mathbf{p}_i^{t'}}]\\
    &=\sum_{t'=1}^T\mathbf{f}[{^k\mathbf{p}_i^{t}}]-\mathbf{f}[\Pi({^k{\mathbf{T}}^{t'}_{co}},\Pi^{-1}({^k{\mathbf{T}}^{t}_{co}},{}^k{\mathbf{p}}_i^{t},\mathbf{K}, {z}^{t}_i),\mathbf{K})],
\end{align}
% \footnote{Should explain how the inverse projection is performed.}
% where $^k\hat{\mathbf{p}}_i^{t'}=\mathcal{H}_\theta(\mathbf{f}[{^k\mathbf{p}_i^{t}}])$ is the predicted 2D coordinates in frame $t'$ corresponding to $\mathbf{f}[{^k\mathbf{p}_i^{t}}]$ from network $\mathcal{H}_\theta$.
% where $^k\hat{\mathbf{p}}_i^{t'}$ in frame $t'$ is the ground-truth correspondence of $[{^k\mathbf{p}_i^{t}}]$ in frame $t$.
where $\Pi^{-1}(\cdot)$ is the inverse projection function to lift the 2D point on the image to 3D in the object frame. ${z}_i^{t}$ is the ground-truth depth of $^k{\mathbf{p}}_i^{t}$ (from LiDAR point clouds only for training). In the training time, we learn the feature correspondence, given the ground-truth pose of the object $\mathcal{O}_k$, denoted as $^k{\mathbf{T}}^{{t}}_{co}$ and $^k{\mathbf{T}}^{{t'}}_{co}$ in frame $t$ and frame $t'$, respectively. 
Considering the featuremetric reprojection loss in all frames and all points,  the overall loss term for object $k$ can be formulated as
% \footnote{Quite confused about how to transform eqn9 to eqn10.}
\begin{align}
    \mathcal{L}_{\text{rep}}^k&=\sum_{i=1}^m\sum_{t=1}^T||^ke_{i}^{t}||^2_2=\sum_{i=1}^m\sum_{t=1}^T\sum_{t'=1}^T||^k\mathbf{f}_i^{t}-{^k\mathbf{f}_i^{t'}}||^2_2
    % &=\sum_{k=1}^K\sum_{t=1}^T||k\cdot \mathtt{Softmax}(\mathbf{C}_i^{t})-^k\mathbf{f}_i^{t(gt)}||^2_2.
    \label{reploss}
\end{align}
% Equivalently, the featuremetric reprojection loss can also 
% Finally, from Eq.~\ref{reploss}, because we obtain the normalized correspondence map $\bar{\mathbf{C}}$ in Sec.~\ref{sec:learn2dfeat}, we use the cosine distance to measure featuremetric reprojection error.
\begin{figure}[t]
\centering
	\subfloat[Object-centric Bundle Adjustment (OBA).]{\includegraphics[width = 0.4\textwidth]{figures/oba.pdf}\label{fig:oba}}
	\hfill
        \subfloat[The computation of the featuremetric OBA loss.]{\includegraphics[width = 0.4\textwidth]{figures/fobaloss.pdf}\label{fig:diffloss}}
\caption{Illustration of featuremetric object bundle adjustment.}
\vspace{-10pt}
\end{figure}

Finally, we replace the $L2$ norm in Eq.~\ref{reploss} with the cosine distance to measure the featuremetric reprojection error.
Thus we bring the normalized correspondence map $\widetilde{\mathbf{C}}$ in Sec.~\ref{sec:learn2dfeat} into the loss term.
With log-likelihood formulation, we formulate the featuremetric OBA loss to supervise the object-centric temporal correspondence learning:
% denote by the cosine distance of the normalized features:
\begin{align}
\mathcal{L}_{\text{OBA}}^k
&=-\sum_{i=1}^m\sum_{t=1}^T\sum_{t'=1}^T\log(^k\widetilde{\mathbf{C}}_{t}^{t'}[{^k\bar{\mathbf{p}}_i^{t}},{^k\bar{\mathbf{p}}_i^{t'}}]).
\end{align}
where $({^k\bar{\mathbf{p}}_i^{t}},{^k\bar{\mathbf{p}}_i^{t'}})$ are the ground-truth corresponding pair of the $i$-th local feature. The illustration of the loss computation is in Fig.~\ref{fig:diffloss}.



\subsection{Inference}
\label{sec:infer}
After introducing the training loss design, we present the inference process of \methodname{} as follows. \par
% We emphasize the difference from the training stage here.\par
% For more training and inference details and network architecture, please refer to the appendix.\par
\textbf{First-stage 3D object detection and association.} The first-stage detector makes the prediction of classification scores and 2D / 3D bounding boxes. The 3D bounding boxes are associated across the frames by ImmortalTracker~\cite{wang2021immortal}. The following process is on the tracklet level.\par
% \textbf{Object association in 3D space.} \red{Also performed in training time?} We follow ImmortalTracker~\cite{wang2021immortal}, applying the 3D Kalman Filter to associate the boxes predicted from our first stage object detector frame by frame. To make the tracklet robust to the measurement noise \red{(what is measurement noise)} and track the object as long as possible, we keep all tracklets as association candidates \red{(what is association candidates)} no matter how long ago the tracklet disappeared. Besides, we ignore the \red{strict condition of tracklet birth}, because the bad measurement always belongs to the short tracklet that we will not optimize. \par 
\textbf{Dense feature matching.} To optimize the object pose, we need to obtain the feature correspondence in each frame for the same object. As mentioned in Sec.~\ref{sec:learn2dfeat}, the OTCL module is trained to generate a dense correspondence map in all frames. During inference, we match  
 all $H\times W$ dense local features in RoI between adjacent two frames and between the first frame and last frame of the time window $[t,t+\tau]$. We use the RANSAC algorithm~\cite{fischler1981ransac} to filter the feature correspondence outliers. \par
 % To balance the number of valid features in each frame, we select the top $k$ feature correspondence for each frame based on the similarities.
 % Note that if the number of correspondences is imbalanced between frames, the optimization tends to give high weight excessively to the frames with more tracklets and make other frames deviate from the correct pose.\par
\textbf{Feature tracking.}
To form a long-term keypoint tracklet from the obtained correspondence, we leverage a graph-based algorithm.
First, the matched feature pairs are constructed into a graph $\mathcal{G}$. The features are on the vertices. If the features are matched, an edge is connected in the graph. Then we track the feature for the object in all available frames. We use the association method mainly following~\cite{dusmanu2020multi}. The graph partitioning method is applied to $\mathcal{G}$ to make each connected subgraph have at most one vertex per frame. The graph cut is based on the similarity of the matched features.
\par
% \footnote{Not clear enough. What is the criteria for tracking? Threshold for feature similarity or other metrics?}\par
% For more details about feature matching and feature tracking, please refer to Sec.~\ref{sec:feattrack} in the appendix.\par
\textbf{Object-centric bundle adjustment.} In the inference stage, given the initial pose estimation and the temporal feature correspondence, we solve the object-centric bundle adjustment by Levenberg–Marquardt algorithm, and the object pose in each frame and the 3D position of the keypoints can be globally optimized between frames.\par
\textbf{Post-processing.} We also apply some common post-processing in video object detection techniques like tracklet rescoring~\cite{kang2017tcnn} and bounding box temporal interpolation.
% \footnote{Give ref to the origin of these techniques.}
%The first technique is tracklet-level rescoring, which is to correct the score when the localization has been refined. We use the maximum score of the tracklet to replace all scores of the bounding boxes in each frame. We also add the bounding boxes when the box is not predicted in some frames by linear interpolation.\par
\input{tables/sota.tex}
\begin{figure*}[t]
\centering
        \resizebox{0.92\linewidth}{!}{
	\subfloat[Frame 8.]{\includegraphics[width = 0.2\linewidth]{figures/mainvis/1079008.png}}
	\hfill
	\subfloat[Frame 22.]{\includegraphics[width = 0.2\linewidth]{figures/mainvis/1079022.png}} 
        \hfill
        \subfloat[Frame 36.]{\includegraphics[width = 0.2\linewidth]{figures/mainvis/1079036.png}}
	\hfill
	\subfloat[Frame 50.]{\includegraphics[width = 0.2\linewidth]{figures/mainvis/1079050.png}} 
        \hfill
        \subfloat[Frame 57.]{\includegraphics[width = 0.2\linewidth]{figures/mainvis/1079057.png}}
        }
 %         \resizebox{\linewidth}{!}{
	% \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075000.png}}
	% \hfill
	% \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075016.png}}
	% \hfill
	% \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075032.png}} 
 %        \hfill
 %        \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075048.png}}
 %        \hfill
 %        \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075064.png}}
 %        }
\caption{Qualitative results from the BEV in different frames. We use \textcolor{blue}{blue} and \red{red} boxes to denote initial predictions and optimized predictions of the object we highlight. The \textcolor{green}{green} and black boxes denote the other box predictions and the ground truth boxes. The ego vehicle lies at the bottom of each figure.}
\label{fig:vismain}
\end{figure*}
\input{tables/multi_cam.tex}

% \subsection{Discussions}
% We make some discussions about some related methods related to BA-Det in this section. 
% \begin{itemize}
%     \item  \textbf{\methodname{} vs. multi-view geometry-based 3D object detectors~\cite{wang2022dfm,wang2022sts}.} Although they also try to utilize geometry in the video, they treat continuous frames as stereo to estimate the depth of each frame, ignoring the dynamic objects. So, they add the monocular depth estimator as the residual branch to compensate for the shortage. However, because we separate different objects and treat them as different tracklets, we can easily handle moving objects. With the long time range, objects can move very far from the first frame. We can utilize all available frames, but they only use frames within a very short time window.
%     \item \textbf{\methodname{} vs. differentiable bundle adjustment, like BANet~\cite{tang2018banet}.} Both of us train a network with bundle adjustment loss in an end-to-end style. However, we focus on the object level instead of the whole scene. The purpose of \methodname{} is to optimize the object pose in each frame, i.e., for temporal 3D object detection. So, we combine object detection and object-centric local feature learning into an integrated framework and treat temporal feature learning as the second stage of object detection.
% \end{itemize}

\section{Experiments}
\subsection{Datasets and metrics}
% We conduct our experiments on the large-scale autonomous driving dataset, Waymo Open Dataset (WOD)~\cite{sun2020scalability} v1.2. Five cameras are available in WOD, and following existing methods~\cite{reading2021categorical_caddn,li2022dcd}, we only train and evaluate using the images captured from the FRONT camera. Because we mainly focus on rigid objects, we report the results of the VEHICLE class, which is also the mainstream experiment setting. The evaluation metrics are 3D AP and APH (AP weighted by heading accuracy) under the IoU threshold of 0.7 and 0.5. WOD has two difficulty levels. LEVEL\_1 is easy, ignoring some ground truth bounding boxes that may be heavily occluded or far from the ego vehicle. LEVEL\_2 includes all ground truth.  \par

% Early work mainly reports the results on the v1.2 dataset, and we only compare our methods with the ones from WOD Challenge 2022 using the v1.3.1 dataset. Because we mainly focus on rigid objects, we report the results of the VEHICLE class.\par

\input{tables/range_condition.tex}
We conduct our experiments on the large autonomous driving dataset, Waymo Open Dataset (WOD)~\cite{sun2020scalability}. The WOD has different versions with different annotations and metrics. To keep the fairness of the comparisons, we report the results both on WOD v1.2 and WOD v1.3.1. The annotations on v1.2 are based on LiDAR and the official metrics are mAP IoU@0.7 and mAP IoU@0.5. Recently, v1.3.1 is released to support multi-camera 3D object detection, and the annotations are camera-synced boxes. On the v1.3.1 dataset, a series of new LET-IoU-based metrics~\cite{hung2022let} are introduced to slightly tolerate the localization error from the worse sensor, camera, than LiDAR. Early work mainly reports the results on the v1.2 dataset, and we only compare our methods with the ones from WOD Challenge 2022 using the v1.3.1 dataset. Because we mainly focus on rigid objects, we report the results of the VEHICLE class.\par
LET-3D-AP and LET-3D-APL are the new metrics, relying on the Longitudinal Error Tolerant IoU (LET-IoU).  LET-IoU is the 3D IoU calculated between the target ground truth box and the prediction box aligned with ground truth along the depth that has minimum depth error.
% where $P_{\text{aligned}}=(G\cdot u_P) \times u_P$ , and $u_P=P/|P|$ is the unit vector of $P$.
LET-3D-AP and LET-3D-APL are calculated from the average precision and the longitudinal affinity weighted average precision of the PR curve.
% \begin{equation}
%     \text{LET-3D-AP}=\int_0^1 p(r)dr,\ \text{LET-3D-APL}=\int_0^1 \overline{a_l} \cdot p(r)dr,
% \end{equation}
 % and the LET-3D-APL can be calculated with the longitudinal affinity weighted PR curve:
% where $p(r)$ is the precision value at recall $r$, $\overline{a_l}=1-\min{\left(|e_{\text{lon}}(P,G)|/T_{\text{l}}, 1.0\right)}$ is the longitudinal affinity. 
For more details, please refer to \cite{hung2022let}.

\input{tables/ablation.tex}
\input{tables/static.tex}


\subsection{Implementation Details}
\label{sec:impl}
The first stage network architecture of \methodname{} is the same as MonoFlex, with DLA-34~\cite{yu2018dla} backbone, the output feature map is with the stride of 8. In the second stage, the shape of the RoI feature is $60\times 80$. The spatial and temporal attention module is stacked with 4 layers. The implementation is based on the PyTorch framework. We train our model on 8 NVIDIA RTX 3090 GPUs for 14 epochs. Adam optimizer is applied with $\beta_1 =0.9$ and $\beta_2=0.999$. The initial learning rate is $5 \times 10^{-4}$ and weight decay is $10^{-5}$. The learning rate scheduler is one-cycle. We use the Levenberg-Marquardt algorithm, implemented by DeepLM~\cite{huang2021deeplm}, to solve object-centric bundle adjustment. The maximum iteration of the LM algorithm is 200. For the object that appears less than 10 frames or the average keypoint number is less than 5, we do not optimize it. 

\input{tables/ablation_orb_sift.tex}
% \begin{figure}[h]
% \centering
%         \resizebox{\linewidth}{!}{
% 	\subfloat[Frame 8.]{\includegraphics[width = 0.33\linewidth]{figures/mainvis/1079008.png}}
% 	\subfloat[Frame 18.]{\includegraphics[width = 0.33\linewidth]{figures/mainvis/1079018.png}} 
%         \subfloat[Frame 30.]{\includegraphics[width = 0.33\linewidth]{figures/mainvis/1079030.png}}}\\
% 	\resizebox{\linewidth}{!}{
% 	\subfloat[Frame 42.]{\includegraphics[width = 0.33\linewidth]{figures/mainvis/1079042.png}} 
%         \subfloat[Frame 54.]{\includegraphics[width = 0.33\linewidth]{figures/mainvis/1079054.png}}
%         \subfloat[Frame 66.]{\includegraphics[width = 0.33\linewidth]{figures/mainvis/1079066.png}}
%         }
%  %         \resizebox{\linewidth}{!}{
% 	% \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075000.png}}
% 	% \hfill
% 	% \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075016.png}}
% 	% \hfill
% 	% \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075032.png}} 
%  %        \hfill
%  %        \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075048.png}}
%  %        \hfill
%  %        \subfloat{\includegraphics[width = 0.2\textwidth]{figures/mainvis2/1075064.png}}
%  %        }
% \caption{Qualitative results from the BEV in different frames. We use \textcolor{blue}{blue} and \red{red} boxes to denote initial predictions and optimized predictions of the object we highlight. The \textcolor{green}{green} and black boxes denote the other boxes and the ground truth. The lower an object in the figure, the closer to the ego-vehicle.}
% \label{fig:vis}
% \end{figure}

\subsection{Comparisons with State-of-the-art Methods}
% We compare our \methodname{} with other state-of-the-art methods on WODv1.2 val set. As shown in~\ref{tab:sota}, using the FRONT camera, we outperform the SOTA method DCD~\cite{li2022dcd} for about 4AP and 4APH ($\sim$30\% improvement) under the 0.7 IoU threshold. Compared with the only temporal method BEVFormer~\cite{li2022bevformer}, we have double points of 3D AP$_{70}$ and 3D APH$_{70}$. Compared with our baseline MonoFlex~\cite{zhang2021objects}, we have a gain of 50\% evaluated with 3D AP$_{70}$, thanks to the object-centric global optimization in multi-frames.
% To validate the effectiveness, we also report the multi-camera results on the newly released WODv1.3.1. No published work reports the results on WODv1.3.1. So, we only compare with the open-source MV-FCOS3D++~\cite{wang2022mvfcos3d++}, the second-place winner of WOD 2022 challenge. We design the variant of \methodname{}, called \methodname{}$_\text{FCOS3D}$, to adapt to the multi-camera setting. \methodname{}$_\text{FCOS3D}$ is also a two-stage object detector. The first stage is the same as MV-FCOS3D++, but with the output of 2D bounding boxes. The second stage is OTFL module supervised with object-centric bundle adjustment loss. Although there are overlaps between 5 cameras, to simplify the framework, we ignore the object BA optimization between them. \methodname{}$_\text{FCOS3D}$ outperforms MV-FCOS3D++ under main metrics and traditional 3D IoU-based metrics. Because MV-FCOS3D++ also utilizes temporal information, the gain is not such huge as the one based on MonoFlex.
We compare our \methodname{} with other state-of-the-art methods under two different settings. WODv1.2 is for the front view camera and WODv1.3.1 has the official evaluator for all 5 cameras. As shown in Table~\ref{tab:sota}, using the FRONT camera, we outperform the SOTA method DCD~\cite{li2022dcd} for about 4AP and 4APH ($\sim$30\% improvement) under the 0.7 IoU threshold. Compared with the only temporal method BEVFormer~\cite{li2022bevformer}, we have double points of 3D AP$_{70}$ and 3D APH$_{70}$. To validate the effectiveness, we also report the multi-camera results on the newly released WODv1.3.1, as shown in Table~\ref{tab:multi_cam}. No published work reports the results on WODv1.3.1. So, we only compare with the open-source MV-FCOS3D++~\cite{wang2022mvfcos3d++}, the second-place winner of WOD 2022 challenge. We design the variant of \methodname{}, called \methodname{}$_\text{FCOS3D}$, to adapt to the multi-camera setting. \methodname{}$_\text{FCOS3D}$ is also a two-stage object detector. The first stage is the same as MV-FCOS3D++, but with the output of 2D bounding boxes. The second stage is OTCL module supervised with featuremetric object bundle adjustment loss. Although there are overlaps between 5 cameras, to simplify the framework, we ignore the object BA optimization across cameras and only conduct temporal optimization. \methodname{}$_\text{FCOS3D}$ outperforms MV-FCOS3D++ under main metrics and traditional 3D IoU-based metrics.
% Because MV-FCOS3D++ also utilizes temporal information, the gain is not such huge as the one based on MonoFlex.
\subsection{Qualitative Results}
In Fig.~\ref{fig:vismain}, we show the object-level qualitative results of the first-stage and second-stage predictions in different frames. For a tracklet, we can refine the bounding box predictions with the help of better measurements in other frames, even if there is a long time interval between them.
\subsection{Distance Conditioned Results}
We report the results with the different depth ranges in Table~\ref{tab:range}. The results indicate that the single frame methods, like DCD and MonoFlex, are seriously affected by object depth. When the object is farther away from the ego vehicle, the detection performance drops sharply. Compared with these methods, \methodname{}, has the gain almost from the object far away from the ego-vehicle. The 3D AP$_{70}$ and 3D APH$_{70}$ are 3$\times$ compared with the baseline when the object is located in $[50\text{m},\infty)$, 2$\times$ in $[30\text{m},50\text{m})$ and 1.2$\times$ in $[0\text{m},30\text{m})$. This is because we utilize the long-term temporal information for each object. In a tracklet, the predictions near the ego-vehicle can help to refine the object far away.
\subsection{Ablation study}

We ablate each component of \methodname{}. The results are shown in Table~\ref{tab:ablation}. The first stage detector is slightly better than the MonoFlex baseline mainly because we remove the edge fusion module, which is harmful to the truncated objects in WOD. 3D KF associates the objects and smooths the object's trajectory. This part of improvement can be regarded as similar to Kinematic3D~\cite{brazil2020kinematic}. The core of \methodname{} is the learnable global optimization module, which obtains the largest gain in all modules. The tracklet rescoring and temporal interpolation modules are also useful.
% \noindent{\textbf{The ablation study of each component.}}\par
\subsection{Further Discussions}

\noindent{\textbf{BA vs. Object BA.}} We conduct experiments to discuss whether the object-centric manner is important in temporal optimization. We modify our pipeline and optimize the whole scene in the \emph{global} frame instead of optimizing the object pose in the object frame, called Static BA in Table~\ref{tab:static}. Static BA ignores dynamic objects and treats them the same as static objects. The inability to handle dynamic objects causes decreases by about 2 AP compared with \methodname{}.\par
\noindent{\textbf{Temporal feature correspondence.}} As shown in Table~\ref{tab:orb}, we ablate the features used for object-centric bundle adjustment. Compared with traditional ORB feature~\cite{rublee2011orb}, widely used in SLAM, our feature learning module predicts denser and better correspondence. We find the average object tracklet length is 19.6 frames, and the average feature tracklet in our method is about 10 frames, which means we can keep a long feature dependency and better utilize long-range temporal information. However, the $\Bar{L}_t$ of the ORB feature is only 2.6 frames. The results show the short keypoint tracklet can not refine the long-term object pose well.\par
% \begin{figure}[]
%           \centering
%            \includegraphics[width=0.6\linewidth]{figures/num_keypoint.png}
%              \caption{\#keypoint vs. distance.}
%              \label{fig:num_keypoint}
% \end{figure}  

\noindent{\textbf{Inference latency of each step in \methodname{}.}} The inference latency of each step in \methodname{} is shown in Table~\ref{tab:latency}. The most time-consuming part is the first-stage object detector, more than 130ms per image, which is the same as the MonoFlex baseline. 
% For \methodname{}, the total inference latency is about 181.5ms. In other words, 
Our \methodname{} only takes an additional 50ms latency per image, compared with the single-frame detector MonoFlex. Besides, although the dense feature correspondence is calculated, thanks to the shared backbone with the first stage detector and parallel processing for the objects, the feature correspondence module is not very time-consuming.\par
% \vspace{-5pt}
\input{tables/inference_latency.tex}



% \begin{figure}[t]
%           \centering
%           \includemovie{1cm}{1cm}{figures/1_3.gif}
%            % \animategraphics{24}{./figures/vis/1075}{000}{074}
%              \caption{Vis}
%              \label{fig:vis}
% \end{figure}  
\section{Limitations and Future Work}
In the current version of this paper, we only focus on the objects, such as cars, trucks, and trailers. The performance of non-rigid objects such as pedestrians has not been investigated. However, with mesh-based and skeleton-based 3D human models, we believe that a unified keypoint temporal alignment module can be designed in the future. So, we will explore the extension of \methodname{} for non-rigid objects.
\section{Conclusion}
In this paper, we propose a 3D video object detection paradigm with long-term temporal visual correspondence, called BA-Det. \methodname{} is a two-stage object detector that can jointly learn object detection and temporal feature correspondence with proposed Featuremetric OBA loss. 
Object-centric bundle adjustment optimizes the first-stage object estimation globally in each frame. \methodname{} achieves state-of-the-art performance on WOD. \par

% Experiment results on the Waymo Open Dataset (WOD) show the effectiveness and efficiency of our method. 
\section*{Acknowledgements} 
This work was supported in part by the Major Project for New Generation of AI (No.2018AAA0100400), the National Natural Science Foundation of China (No. 61836014, No. U21B2042, No. 62072457, No. 62006231) and the InnoHK program. The authors thank Lue Fan and Yuqi Wang for their valuable suggestions.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\newpage
\section*{Appendix}
\renewcommand\thesection{\Alph{section}}
\renewcommand\thetable{\Alph{table}}
\renewcommand\thefigure{\Alph{figure}}
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\section{Network Architecture}
\label{sec:arc}
In this section, we explain the detailed network design of \methodname{}. The backbone is a standard `DLASeg' architecture, please refer to~\cite{yu2018dla} for more details. Then two detection heads are added in the first-stage detector. The cls head contains a 3$\times$3 Conv2d layer, and an FC to predict the classification. The reg head contains 8 independent `3$\times$3 Conv2d layer + FC' modules, regressing `2d dim', `3d offset', `corner offset', `corner uncertainty',`3d dim', `ori cls', `ori offset', `depth', `depth uncertainty' attributes. The `ori cls' and `ori offset' share the Conv2d layer. Note that the `inplace-abn' module in MonoFlex~\cite{zhang2021objects} is changed back to a BatchNorm2d layer and a ReLU activation function. The architecture of OTCL module includes RoIAlign, conv layer, attention layer, and correlation layer. We adopt torchvision's RoIAlign implementation\footnote{\url{https://github.com/pytorch/vision/blob/main/torchvision/ops/roi\_align.py}}. The output size of the RoI feature is 60$\times$80. The conv layer includes two `3$\times$3 Conv2d + BatchNorm2d + ReLU' modules. The attention layer contains 4 self-attention modules and 4 cross-attention modules, using standard MultiheadAttention implementation. The channel dimension is 64, and the number of attention heads is 4. The correlation layer contains the operation of the feature inner product and the softmax normalization.
\input{tables/config.tex}
\input{tables/config2.tex}
\section{Training and Inference Details}
\label{sec:detail}
\subsection{Training Settings}
In Table~\ref{tab:config} and Table~\ref{tab:config2}, we show the training details of \methodname{} and the variant \methodname{}$_{\text{FCOS3D}}$.\par

\begin{figure*}[h]
          \centering
            \vspace{-30pt}
          % \resizebox{\linewidth}{!}{
          \subfloat[segment-15948509588157321530\_7187\_290\_7207\_290]{\animategraphics[width=0.5\linewidth,autoplay=True,loop]{5}{./fig/vis/1075}{000}{074}}
	   \hfill
	   \subfloat[segment-16229547658178627464\_380\_000\_400\_000]{\animategraphics[width=0.5\linewidth,autoplay=True,loop]{5}{./fig/vis2/1079}{008}{066}} 
          % \includemovie{1cm}{1cm}{figures/1_3.gif}
           % }
             \caption{Qualitative results in the WOD \emph{val} set as videos. We use \textcolor{blue}{blue} and \red{red} boxes to denote initial predictions and optimized predictions of the object we highlight. The \textcolor{green}{green} and \textbf{black} boxes denote the other box predictions and the ground truth boxes. The ego vehicle lies at the bottom of each figure. Please view with Adobe Acrobat Reader to see the videos.}
             \label{fig:vis}
\end{figure*} 
\begin{figure*}[h]
          \centering
\animategraphics[width=0.9\linewidth,autoplay=True,loop]{1}{./vis/}{000}{065}
\caption{Video with detection and tracking results. Open with Adobe Acrobat Reader to play the video. Best viewed by zooming in.} 
% (Or download from \url{https://github.com/BA-Det/BA-Det}.)}
\vspace{-8pt}
\label{video}
\end{figure*}
\subsection{Inference Details}
In the main paper, we introduce the inference process in Sec.~\ref{sec:infer}. Here, we make some additional explanations of inference details.\par
\noindent{\bf{Dense feature matching.}} We match the dense RoI feature in two frames. The sliding window $\tau$ is 5 in the implementation. We adopt OPENCV's implementation of the RANSAC operation. To balance the number of valid features in each frame, we select the top $k$ feature correspondence for each frame based on the similarities. $k$ is set to 50 in the implementation.
 Note that if the number of correspondences is imbalanced between frames, the optimization tends to give high weight excessively to the frames with more tracklets and make other frames deviate from the correct pose.\par
\noindent{\bf{Object-centric bundle adjustment.}} We use DeepLM to solve the non-linear least-square optimization problem. Note that in the inference stage, we adopt the original OBA formulation with reprojection error, according to the correspondence prediction from the OTCL module. The initial 3D position of the keypoint is set to (0,0,0) in the object reference frame. In DeepLM, no robust loss (e.g., Huber loss) is used.\par
\noindent{\bf{Post-processing.}} For the tracklet rescoring process, we adopt the maximum predicted score of the tracklet to replace the first-stage predicted score in each frame. The bounding box interpolation is for the missing detection in the tracklet. This process is to interpolate the 3D location of the object center, 3D box size, and orientation in the global reference frame. Only the nearest two observations are considered in interpolation.\par
\section{Qualitative Results}
We show some additional qualitative results in the appendix. In Fig.~\ref{fig:vis}, the qualitative results of two sequences (segment-15948509588157321530\_7187\_290\_7207\_290 and segment-16229547658178627464\_380\_000\_400\_000) in the WOD~\cite{sun2020scalability} \emph{val} set are shown as videos. For better understanding, we show a more detailed video (Fig.~\ref{video}) with detection and tracking results in BEV. Please view with Adobe Acrobat to see the videos.
% \section{Limitations and Future Work}
% In this paper, we only focus on the objects with rigid motion, such as cars, trucks, and trailers. In the current version of this paper, we have not explored the performance in global optimization of objects without rigid motion such as pedestrians. However, with mesh-based and skeleton-based 3D human models, We believe that the keypoints can be aligned in the unified object reference frame. So, we will explore the solution for non-rigid objects in the future.
\section{Additional Experiments}
\input{tables/track}
\input{tables/ab3dtrack}
We evaluate the tracking results compared with the baseline of first-stage predictions + ImmortalTracker in Table~\ref{tab:track}.
Besides, to show how tracking quality affects detection, we use a weaker tracker, AB3DMOT~\cite{weng2020ab3dmot}. The results are shown in Table~\ref{tab:ab3d}. The results reveal that our BA-Det improves tracking performance and is robust to tracking quality.
\section{Reproducibility Statement}
We will release the training and inference codes to help reproducing our work. Limited by the license of WOD, the checkpoint of the model cannot be publicly available. However, we will provide the checkpoint by email. The implementation details, including the training and inference settings, network architecture, and the \methodname{}$_{\text{FCOS3D}}$ variant, are mentioned in Sec.~\ref{sec:det} and Sec.~\ref{sec:impl} in the main paper and Sec.~\ref{sec:arc} and \ref{sec:detail} in the appendix. WOD is a publicly available dataset.

\end{document}