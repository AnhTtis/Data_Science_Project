\section{Conclusions}

LLMs have shown their impressive ability in language generation and human-like reasoning. 
Their potential for integration and enhancement with other fields has attracted growing attention from different research areas.
In this work, we demonstrate the superiority of using an LLM to realize interactive multimodal perception. 
We propose \textbf{Matcha}, a multimodal interactive agent augmented with LLMs, and evaluate it on the task of uncovering object-latent properties. 
Experimental results suggest that our agent can perform interactive multimodal perception reasonably by taking advantage of the commonsense knowledge residing in the LLM, being generalizable due to its modularity and flexibility.

While strong LLMs perform well for tasks that require general knowledge, training and maintaining LLMs locally is currently costly, given the large computation and memory resources required by such models.
Future works will involve distilling the domain-specific knowledge from LLMs into more manageable local models, which can offer greater flexibility and control while maintaining high levels of performance for robotic applications. 
Furthermore, there is a necessity for additional investigation of prompt engineering and multimodal LLMs to augment the ability for complex dynamics in the real world.