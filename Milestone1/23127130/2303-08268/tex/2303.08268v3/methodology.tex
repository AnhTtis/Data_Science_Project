\section{Methodology}

\subsection{Architecture}
We propose \textbf{Matcha} (\textbf{m}ultimod\textbf{a}l environmen\textbf{t} \textbf{cha}tting)  \textbf{agent} which is able to interactively perceive (``chat" with) the environment through multimodal perception when the information from passive visual perception is insufficient for completing an instructed task.
The epistemic actions are executed autoregressively until the agent is confident enough about the information sufficiency in that situation. % for the task can be finally completed. 

Fig.~\ref{fig:overview-architecture2} provides an overview of the architecture of Matcha agent. It is a modular framework of three parts: an LLM backbone, multimodal perception modules, and a low-level command execution policy. They connect to each other with language as the intermediate representation for information exchange. 

To be specific, given a high-level instruction, especially the one that Matcha cannot directly perform with the command policy alone, the LLM backbone will reason the situations and select the most contextually admissible perceiving command to gather information.
After the execution of the policy module, the resulting environmental response is processed by a correspondingly evoked multimodal perception module into semantic descriptions, e.g.\ ``clinking sound" by an auditory module after the ``knock on" action. 
Finally, the executed command itself as well as the environmental state description are fed back to the LLM for future planning.

The LLM is used in a few-shot manner without any need for fine-tuning, being independent of other components.
Policy and perception modules can be separately designed and plugged into the framework whenever needed.
Intrinsically linked by natural language, this framework is flexible and can scale and adapt easily to possible robotic upgrades or diverse robotic scenarios.
\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \caption{Overview of Matcha. The framework contains an \textcolor{mypurple}{LLM}, \textcolor{myorange}{multimodal perception modules}, and a language-conditioned \textcolor{myyellow}{control policy}. 
    These components communicate with each other with natural language as the intermediate representation. Three types of language information are involved in composing the prompt: 
    \textcolor{myred}{\textit{I}} is a language \textcolor{myred}{instruction} from the user, \textcolor{mygreen}{\textit{C}} is a language \textcolor{mygreen}{command} produced by the LLM, and \textcolor{myblue}{\textit{F}} is semantic \textcolor{myblue}{feedback} from multimodal perceptions. Dotted lines indicate possibly evoking paths.}
    \label{fig:overview-architecture2}
\end{figure}
\subsection{Multimodal Perception and Execution Policy} 
We select a commonly feasible suit of modalities and a language-conditioned policy as an example implementation of our framework.
Other varieties for specific scenarios can also be easily integrated due to the flexibility of modularity of the framework. Detailed experimental implementations will be introduced in Sec. \ref{sec:experiments}.

\subsubsection{Vision} 
Usually, a monitoring camera is the cheapest option for a robot to passively perceive such rich information.
We employ pre-trained ViLD \cite{Gu2022OpenvocabularyObjectDetection}, an open-vocabulary visual detection model, as the vision perception module to detect objects with their categories and positions in the scene.
Then, the results will be delivered to a policy module for identification and execution.
Meanwhile, a prompt template ``The scene contains [\textit{OBJ1}, \textit{OBJ2}, ...]" is applied to construct a scene description, which enables the LLM to have an initial impression of the environment.
Typically, pre-trained vision models are not designed to discern attributes that extend beyond those easily extractable from topology or textures, such as material composition. 
The use of low-resolution images for expedited processing exacerbates the loss of information concerning such attributes.
In our experimental approach, we prioritize demonstrating the integration of diverse modalities instead of extensively fine-tuning ViLD to encompass all aspects.

\subsubsection{Impact Sound}
Impact sound commonly occurs from time to time, and can be useful for robotic multimodal learning \cite{Zhao22ImpactMakes}. 
Though it can be passively collected with a microphone attached to the robotic end-effector, without intentional intervention by the robot, a ``knock on" action in our case, a microphone may only be able to collect background noise.
This auditory perception module classifies the consequent impact sound into a description and then wraps it in a natural language form. 
Actually, a clip of audio may contain sufficient information for some of the usage, e.g.\ to distinguish metal from glass \cite{Dimiccoli22RecognizingObject}. However, it may not be the case for other scenarios, for example, to select the only targeted one among a set of similar ``dull" sounds that could indicate either plastic, wood or hard paper. Therefore, we showcase both of the designs, i.e.\ one with a specific material classification (e.g.\ ``glass") and another with solely low-level and non-distinct descriptions (e.g.\ ``tinkling").
The modular output is also wrapped with templates to a full sentence such as ``It sounds tinkling", to guarantee processing consistency with LLMs. 

\subsubsection{Weight}
Weight measurements are usually obtained via the torque exerted on the robotic arm subsequent to the execution of an ``weighing" action.
The weight information is directly translated into natural language like ``It is lightweight" or ``It weighs 30g". Note that with implicit clarification of the scenario and the type of objects that a robot is manipulating, LLMs can interpret numerical values into contextual meanings.

\subsubsection{Haptics}
Haptic perception is extremely important for humans to interact with their surroundings. It also provides a potential for robots when acquiring information related to physical properties, including hardness, texture, and so on. However, a high-resolution tactile sensor is costly and not worthwhile for many applications. Therefore, in our case, we only use highly abstract descriptions for the force-torque feedback subsequent to a ``touching" action on an object, e.g.\ ``It feels soft" or ``It feels hard and smooth".

\subsubsection{Execution Policy}
The execution policy is conditioned on the generated command by an LLM and the visual information provided by the vision perception module. 
Once an actionable command together with an identified target is suggested by the LLM, the policy module locates the targeted object and executes a certain action. Meanwhile, the environmental feedback will be concurrently collected for multimodal perception modules for further post-processing as demonstrated above.

\subsection{Prompt Engineering}
An issue of grounding LLMs on robotic scenarios is that some of the suggestions generated by LLMs are not executable for a specific robot \cite{Ahn22CanNot, Huang22LanguageModels}, which stems from the fact that LLMs are pre-trained with extremely large open-domain corpora, while the robot is constrained by its physical capability and application scenarios, e.g. a tabletop robot is not able to perform ``walk" action.

In this work, the LLM is applied for few-shot planning \cite{Mialon23AugmentedLanguage, Zeng23SocraticModels}, in which all the executable commands are defined together with several task examples as the initial ``chat" history. See Tab.~\ref{tab:prompt} for the leading prompt which enables the LLM to ground on the specific scenario and follow the contextual patterns for commanding the execution policy.
\input{init_prompt}

We found that only language models that are large enough can follow the patterns in the prompt strictly, i.e.\ only generate commands that have been defined in strictly case-sensitive letters and with the same amount of allowed parameters for each, while small ones can hardly obey this constraint and generate unexpected commands, which brings extra demands for tuning.
As the action planning is performed by LLMs constrained by a given prompt, 
the proposed framework demonstrates high flexibility and generalizability upon the possible incorporation of novel actions or perception modules into the system.