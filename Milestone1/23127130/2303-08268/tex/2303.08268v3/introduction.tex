\section{Introduction}

\textit{How do humans perceive the surroundings to uncover latent properties?}

Suppose you are presented with an uncommon object in a strange shape and of unknown material,
you may explore its properties in both passive and active ways, if possible, e.g.\ by observing the geometry, touching and even knocking on the surface in order to deduce its exact functionalities from the feedback.
Unnecessary explorations, which could be essential for other scenarios such as smelling, will not be performed in this context unless something counterintuitive happens.
We humans naturally perform these \textbf{multimodal observations and examinations} in daily life through \textbf{common sense and established knowledge}, and over time we adapt with the accumulation of experience \cite{Barnard06CrossModal}.

This is also now possible for a robot well-equipped with multiple sensors and Large Language Models (LLMs).
An environment may be filled with rich content, and the robot can be overwhelmed with diversified sensory stimuli.
An intelligent robot should 1) wisely choose stimuli to attend to, avoiding eagerly being bogged down into details, and 2) respond accordingly to the resulting sensations in the context of a specific task.
% \begin{figure}[thpb!]
\begin{figure}[t!] 
    \centering
    \includegraphics[width=\linewidth]{figures/newimage.pdf}
    \caption{Given instruction from a human, the robot recurrently ``chats" with the environment to obtain sufficient information for achieving the task. An LLM generates action commands to interactively perceive the environment and, in response, the environment provides multimodal feedback (MF) through multimodal perception modules. }
    \label{fig:exp}
\end{figure}

\subsection{Interactive Multimodal Perceptions}

Like humans, robots can perceive the environment in either a passive or an interactive way \cite{Kroemer21ReviewRobot}.
\textit{Passive perception} refers to ways such as visual or auditory monitoring, and it allows robots to quickly acquire information without intervening with the surroundings. However, the passive manner has its limits, among which the most outstanding one is its impotency when facing \textit{epistemic uncertainty} \cite{Celemin23KnowledgeandAmbiguityaware}, the uncertainty because of lacking knowledge.

Epistemic uncertainty inevitably arises from diverse sources, e.g.\ from the ambiguity in human instructions, from low-resolution sensing (e.g. reduced image size for convolution), or from insufficient modalities.
Many of them can only be reduced with \textit{interactive perception}, in which a robot actively interrogates the environment to increase accuracy and even uncover latent information.
For example, when being asked to deliver a \textit{steel} screw instead of one with a similar color \& shape but made of \textit{aluminum}, an assistant robot may need to locate possible candidates with \textit{passive} vision and further, \textit{interactively}, resort to a weighing or a magnetic module for confirmation.

Despite the promising advantages, interactive perception is less common than the passive manner because it entails increased complexity \cite{Li23InternallyRewarded}.
Efforts are needed to design a mediating system to handle various sensory data and to adapt to changes in the conditions of both the robot and the environment, such as a new robotic modular being available or the involvement of novel objects.

\subsection{Chatting with the Environment}
LLMs have been showing incredible potential in areas besides robotics \cite{Ahn22CanNot, Cui23NoRight, Lynch22InteractiveLanguage, Mialon23AugmentedLanguage}.
Human knowledge that resides in LLMs can help a robot abstract and select only suitable features, e.g. relevant to the region of interest or informative modalities, to simplify the learning process.
Moreover, in terms of generalizability, the knowledge of LLMs allows a behavioral agent to adapt efficiently to novel concepts and environmental structures.
For instance, when being asked to \textit{use one adjective for each to describe how a sponge and a brick feel}, ChatGPT\footnote{\url{https://openai.com/blog/chatgpt/}} will respond with ``soft'' and ``hard'' respectively. This is helpful for a robot with a haptics sensing module to distinguish between these two novel, never-seen objects.

LLMs are usually generative models that predict tokens to come, but with certain designs, e.g.\ conversational prompts, LLMs are capable of generating chat-fashion texts. This allows their integration with a robot to not only plan with respect to a robot's built-in ability \cite{Zeng23SocraticModels, Ahn22CanNot}  but also respond according to environmental feedback.

However, they cannot directly process application-specified raw multimodal data. We 
resort to modular perceptions for each modality that are separately trained before being plugged into the LLM backbone. Each module semantically translates the resulting multimodal sensations into natural language that can be understood by LLMs and processed in a unified manner.

Our contributions are threefold. Firstly, we establish a manipulation scenario with multimodal sensory data and language descriptions. 
Secondly, we propose \textbf{Matcha}\footnote{By the name of a type of East Asian green tea.
To fully appreciate matcha, one must engage multiple senses to perceive its appearance, aroma, taste, texture, and other sensory nuances.
} (\textbf{M}ultimod\textbf{a}l environmen\textbf{t} \textbf{cha}tting) \textbf{agent}, where an LLM is prompted to work in a chatting fashion, thus having continuous access to environmental feedback for contextual reasoning and planning.
Finally, we show that LLMs can be utilized to perform interactive multimodal perception and behavior explanation. Accordingly, an interactive robot can make reasonable and robust decisions by resorting to LLMs to examine objects and clarify their properties that are essential to completing the task (see Fig.~\ref{fig:exp}). 