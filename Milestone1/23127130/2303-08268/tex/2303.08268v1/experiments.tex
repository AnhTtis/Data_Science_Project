\section{EXPERIMENTS}
\label{sec:experiments}
% We consider diverse tasks requiring interactive perception in our experiments (see Table~\ref{table:tasks}). 
% Our target scenario is material-related object manipulation. 


% Experiments on these tasks demonstrate the versatility and scalability of the proposed architecture. 


% \begin{table}[ht!]
% \begin{tabularx}{\linewidth}{|X|l|c|c|c|}
% \hline
% \multirow{2}{*}{Instructions} & \multirow{2}{*}{Commonsense} & \multicolumn{3}{c|}{Modality} \\ \cline{3-5} 
% & & Sound & Haptic & Weight \\ \hline
% ``Pick up the red steel object. "       & Materials & High & Low & High \\ 
% ``pick up an object for cracking nuts. "   & Utilities  & Low & High & High \\ 
% ``Please give me some water. One of the bottles contains water. " & States & High  & Low & High \\ \hline
% \end{tabularx}
% \caption{\label{table:tasks} Example tasks. The importance of different modalities in achieving the task is distinct. }
% \end{table}

\subsection{Experimental Setup}
% - Introduce the task. Explain why interactive multimodal perception is needed. \\
We evaluate the proposed framework in an object-picking task: a robot is instructed to pick up an object that is referred to by a latent property -- \textit{material} -- which is, however, not visually distinguishable under our settings. 
% The properties in different modalities may provide compensating information when distinguishing between different materials.
Tasks are intentionally designed such that information from a single modality could be insufficient to determine object properties, while other perception sources can provide compensations to reduce or eliminate this ambiguity.
For example, glass and metal surfaces could exhibit similar hard and smooth properties upon contact, in which case differences in impact sound can aid in further differentiation.
% The description ``the object weighs a little bit heavy" cannot guarantee that the object is made of metal instead of glass.
% \mlnote{I think describing the weight by the specific value is not good, because they are meaningless without volume information. We are interested in utilizing commonsense knowledge in LLMs. These specific weight values are not related to commonsense knowledge to distinguish the materials of objects.}
% We also evaluate and report the performance of other LLMs in our experiments. 
TABLE~\ref{table:metarials-properties} lists variational multimodal descriptions of the materials. These properties are wrapped as natural language sentences before being fed back to the LLM.

Experiments are done in CoppeliaSim\footnote{https://www.coppeliarobotics.com/} simulations,
where several blocks in various colors, materials, weights and surface textures are randomly selected and placed on the table next to a brown container (See Fig.~\ref{fig:exp}).
% The robot is instructed by natural language to pick up an object with latent property, i.e. material and place it in the box. 
% Instructions are generated automatically using the template ``Pick up the [MATERIAL] block". 
The desktop robot is equipped with two \textit{Open-Manipulator-Pro} arms \footnote{https://emanual.robotis.com/docs/en/platform/openmanipulator\_p/overview/}, but only its right arm is activated to operate. It is capable of executing actions in [``knock on", ``touch", ``weigh", ``pick up"]. The first three actions correspond to the interactive perception of impact sound, haptics, and weight, respectively, and the last action finalizes the task by picking and placing an object into the box. 
Each instruction is guaranteed to be achievable with the capability of the robot. %, i.e., there is one object of the queried material on the table.

Due to the lack of support for physics-driven sound and deformable object simulation in Coppeliasim, we have implemented reasonable alternatives.
For the haptics of objects, we simplify haptic perception by assigning variational descriptions regarding its material, e.g.\ fibrous objects are usually perceived as ``soft" and a plastic object can be either ``soft" or ``hard". Note that advanced implementations can also be achieved using a neural network as is used in the sound perception module when haptics data for deformable objects is available. 
For the impact sound, we split the YCB-impact-sound dataset \cite{Dimiccoli22RecognizingObject} into training and testing sets and augment them with tricks such as shifting, random cropping and adding noise.
The training set is used to train our auditory classification neural networks,
while the audios in the testing part are randomly loaded as an alternative to run-time impact sound simulation for the materials mentioned,

Sound can be informative, though not perfect, for the discrimination of materials \cite{Dimiccoli22RecognizingObject}.
Besides showing the mediating ability of multiple modalities by the LLM, we further investigate its reasoning ability by employing indistinct descriptions instead of exact material labels.

\begin{itemize}
    \item  \textit{Distinct description}: the sound module describes sound feedback by the corresponding material name and its certainty from the classification model, e.g. ``It is probably glass" or ``It could be plastic with a 47\% chance, or ceramic with a 35\% chance". 
    The distinct description setting is more task-oriented. And it examines the robot's ability to mediate multiple sensory data for disambiguation.
    \item \textit{Indistinct description}: we listed some commonly used indistinct sound descriptions in human communications in TABLE~\ref{table:metarials-properties}, e.g.\ using ``dull" to describe the sound from a plastic block and ``tinkling" to describe the sound for both ceramic and glass objects.
    This setting is more task-agnostic and thus has the potential for generalization. 
    Moreover, it compels the LLM to infer ``professional" material terminology from ambiguous yet multimodal descriptions. 
\end{itemize}

The online OpenAI GPT-3 ``text-davinci-003" model\footnote{\label{footnote:gpt3-models} https://platform.openai.com/docs/models/gpt-3} as the LLM backend because it demonstrates robust and outstanding reasoning performance. We also evaluate with a weaker but far less expensive LLM ``text-ada-001" under the same setting as comparison.

\begin{table}[ht!]
\begin{tabularx}{\linewidth}{|l|X|X|X|}
\hline
\multicolumn{1}{|c}{\textbf{Materials}} & \multicolumn{1}{|c}{\textbf{Impact Sound}} & \multicolumn{1}{|c}{\textbf{Haptics}} & \multicolumn{1}{|c|}{\textbf{Weight}}  \\ \hline \hline
Metal & ``resonant and echoing", ``metallic", ``ringing" & ``hard and cold", ``rigid, cold, and smooth" & ``heavy", ``300g" \\\hline
Glass  &  ``tinkling", ``tinkling and brittle" & ``hard", ``hard and smooth", ``cold and smooth" & ``a little bit heavy", ``150g" \\ \hline
Ceramic & ``clinking and rattling", ``rattling", ``tinkling and brittle" & ``hard", ``tough" &  ``average weight", ``not too light nor not too heavy", ``100g"  \\ \hline
Plastic  & ``dull", ``muffled" & ``hard", ``soft" &  ``light", ``30g" \\ \hline
Fibre & ``muted", ``silent" & ``soft", ``flexible" & ``lightweight", ``underweight", ``10g" \\ \hline
\end{tabularx}
\caption{\label{table:metarials-properties}  Property descriptions of different materials. }
\end{table}

% - material success rate: 90.57\% success rate
% (higher than a theoretical success rate of 88.67\% ($p=0.93, 1/3*(p+2p^2)$))
% - 56.67\% description success rate.

\subsection{Results}
We test the proposed framework Matcha in 50 randomly generated scenarios for each setting and report the success rate. 

We report that the impact sound classification model pre-trained with the selected materials achieves an accuracy of 93.33\%.
When using distinct descriptions, suppose we are making hard-coded rules to utilize the sound module to identify the targeted material, the robot can randomly knock on an object among three, and classify the material until the one that is classified as the target. In theory, the success rate computes as  $\frac{1}{3} p + \frac{2}{3} p^2|_{p=93.33\%}=89.18\%$, where $p$ is the modular accuracy. Usually, other modalities, in this case, are not as distinct as sound, and it could be non-ideal for humans to craft such fusion rules for a possible slight improvement. Therefore, the theoretical success rate 
with only the sound module will be used as our baseline for analysis. Note that this is a reasonable rule that humans will follow, thus it can also be regarded as the upper bound for Matcha if it worked with only impact sound.

It is unsurprising that Matcha achieves a relatively higher success rate of 90.57\% compared to the ideal theory baseline, as it utilizes compensatory information from other modalities in addition to sound.
% Specifically, Matcha is able to distinguish glass and ceramic well when using the distinct description because the sound classification network directly predicts the material, preventing a significant loss of task-relevant information in the translation from sensory data to language.
% These two types of descriptions demonstrate a trade-off between the success rate on a specific task setup and the generalizability of the perception module. 
When using the indistinct description of impact sound, Matcha is still able to achieve a success rate of 56.67\%, which is larger than a chance success rate of 33.33\% achieved by randomly picking one from the three.
This result is remarkable as it performs zero-shot deduction with only indistinct adjectives available.
% where there is no obvious information to the task's requirement of the information except 
% The success comes from the reasoning ability of the LLM.
% which on the one hand guides the robot to interactively perceive the environment, and on the other hand, connects the natural language feedback of property descriptions to the queried material. 
By analyzing the failure cases, we found that the similar descriptions of glass and ceramic in terms of impact sound, haptics, and weight make it challenging to distinguish one material from the other.
This is not an issue with distinct descriptions, where the sound classification network directly predicts the material, preventing a significant loss of information in the translation from sensory data to language.
While the system displays remarkable reasoning ability, the results still highlight the significance of a well-designed modular description that is not overly abstract in facilitating interactive perceptions.

% This indicates that the design of the descriptions of the perception module influences the planning capability of the LLM significantly. 
% In our experimental setup, glass and ceramic also share similar haptics and weight perception descriptions, so it is challenging to distinguish between them based on the 
% Cases \mlnote{Case study is extremely important and interesting to see for this work because the impressive performance of LLMs can be observed from various actual cases. }

We observe that only strong LLMs (GPT-3 ``text-davinci-003" in our experiments) can perform the task following the primary prompt as we defined, while weak ones possibly generate commands that are not executable and show degraded performance (See TABLE \ref{table:rate}). 
For example, GPT-3 ``text-ada-001"\textsuperscript{\ref{footnote:gpt3-models}}, a smaller model in the GPT-3 model series, may generate commands like ``\textit{robot.knock\_on(metal block)}" or ``\textit{robot.weigh(yellow block, blue block)}", which are not feasible for the policy of the robot. 

\begin{table}[ht!]
\centering
\begin{tabularx}{0.8\linewidth}{lcc}
% \hline
\multicolumn{1}{c}{\textbf{LLM}}& \textbf{Type of Description} & \textbf{Success Rate} \\
\hline
\multirow{2}{*}{text-ada-001} & Indistinct & 19.05\% \\
 & Distinct & 28.57\% \\ 
 \hline
\multirow{2}{*}{text-davinci-003} & Indistinct & 56.67\% \\
 & Distinct & 90.57\% \\
\hline
\end{tabularx}
\caption{Effect of different LLMs on success rate. \label{table:rate}}
\end{table}

% \mlinline{This paragraph should be moved or elaborated. }
% Example cases of the interaction history are shown in Fig.~\ref{}. 

% \subsection{Ablation Study /Discussion}
% - Remove the choice of a modality. To demonstrate that 1. multimodal perception is useful to elevate ambiguity. Information from one modality compensates for the insufficient perception in other modalities. 2. Our model can choose the proper modalities. All the results can be shown in one table. 

\subsection{Case Studies}
We provide case studies to showcase the interactive perception ability of Matcha in multimodal environments.
Following the convention of this paper, texts containing information from humans, LLMs, and the environment are indicated in red, green, and blue, respectively.
% The impact sound is described using property description in Case 1 and certainty description in Case 2. The instructed tasks of both of them are successfully achieved. 

% \begin{figure*}[t]
%     \subcaptionbox{%
%       Case 1 \label{fig:case1}%
%     }[.33\textwidth]{%
%       \hfill\includegraphics[width=.33\textwidth]{RIG_with_LLMs/figures/demos/case3.pdf}\hfill\phantom{}%
%     }%
%     \hfill%
%     \subcaptionbox{case2\label{fig:case2}}[0.33\textwidth]{\includegraphics[width=0.33\textwidth]{RIG_with_LLMs/figures/demos/case2.pdf}}%
%     \hfill
%     \subcaptionbox{%
%       Case 3 \label{fig:case3}%
%     }[.33\textwidth]{%
%       \hfill\includegraphics[width=.33\textwidth]{RIG_with_LLMs/figures/demos/case3.pdf}\hfill\phantom{}%
%     }
%     \caption{Example cases. \label{fig:cases}}
% \end{figure*}
\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/demos/case1.pdf}
    \caption{A successful example in which the robot deduces ``fibre" material with indistinct descriptions of impact sound.}
    \label{fig:case1}
\end{figure}
\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/demos/case2.pdf}
    \caption{A successful example with the distinct description of impact sound. It shows that by leveraging multimodal perception, LLM rectifies the misclassification that may occur when relying solely on sound modules.}
    \label{fig:case2}
\end{figure}
\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/demos/case3.pdf}
    \caption{An example in which the agent fails to distinguish glass and ceramic in the setup of using indistinct descriptions of impact sound.}
    \label{fig:case3}
\end{figure}
Fig.~\ref{fig:case1} gives a completion that the agent can deduce the latent material property from multimodal yet indistinct descriptions.
During the planning process, the agent decidedly terminates exploration of the ``red block" and instead engages in interacting with the ``green block" exhaustively for a comprehensive examination.
Common sense and, moreover, established knowledge in the LLM enables Matcha to efficiently interact with the environment.
Matcha's proficient behaviors provide evidence of effective reasoning, as it aligns with the subsequent explanation provided by the LLM, namely, that fibre can often be considered ``flexible" rather than ``cold and smooth".
% The detailed exploration of the ``green block" gives the final decision a concrete and convincing reason, which is revealed by the explanation given by the LLM. 
The example depicted in Fig.~\ref{fig:case2} presents a fascinating observation: the impact sound of the ``orange block" suggests it is more likely to be plastic than metal, but Matcha accurately distinguishes it from plastics after engaging in the interactive perception of weight and haptics. This showcases the potential of multimodal perception to improve classification accuracy.

Fig.~\ref{fig:case3} provides a failure case with indistinct descriptions where the robot picks up a ceramic block when actually asked to pick up the one made of glass.
The underlying cause of this failure is the sensing similarity between glass and ceramic, which creates difficulty in resolving epistemic uncertainty.

\subsection{Generalisability Evaluation}
The Matcha framework exhibits a high degree of generalizability.
In addition to the primary experimental setup for material discrimination, we evaluate Matcha in various other interesting and challenging robotic scenarios.

\subsubsection{Property Combination}
In this scenario, we ask the robot to pick up blocks that satisfy multiple combinations of properties while keeping the prompt as is in the previous experiments.
The planning process reveals that Matcha can be seamlessly employed for changing tasks without any modifications or model fine-tuning.

\newtcolorbox{casebox}[1][]{
    title={\ifblank{#1}{}{ #1}}, % improved the case when #1 is blank
    % coltitle=mediumseagreen,
    colbacktitle=gray,
    colframe=gray,
    boxrule=1pt,
    % rightrule=3pt,
    % bottomrule=3pt
    fonttitle=\fontsize{9}{12}\selectfont, % \selectfont is required here
    standard jigsaw,
    opacityback=0,  
    frame hidden,
    interior hidden,
    boxsep=0pt,
    left=4pt,
    right=2pt,
    top=2pt,
    bottom=2pt,
    % code={\singlespacing},
    fontupper=\linespread{0.7}\selectfont,
    fontlower=\linespread{0.7}\selectfont,
}

% \begin{casebox}[Property Combination]
% {\footnotesize
% \textcolor{myred}{Human: ``pick up all of the hard and light objects" } \textcolor{mygreen}{in the scene that contains} \textcolor{myblue}{[yellow block, blue block, green block]} \\
% \textcolor{mygreen}{AI: \textit{robot.weigh(yellow block)}} \\
% \textcolor{myblue}{Feedback: It feels soft.}\\
% \textcolor{mygreen}{AI: \textit{robot.weigh(blue block)}}\\
% \textcolor{myblue}{Feedback: It feels hard.}\\
% \textcolor{mygreen}{AI: \textit{robot.weigh(green block)}}\\
% \textcolor{myblue}{Feedback: It weighs heavily.}\\
% \textcolor{mygreen}{AI: \textit{robot.touch(green block)}}\\
% \textcolor{myblue}{Feedback: It feels hard.}\\
% \textcolor{mygreen}{AI: \textit{robot.pick\_up(yellow block), robot.pick\_up(blue block)}}\\
% \textcolor{myred}{Human: Explain why.}\\
% \textcolor{mygreen}{AI: I picked up the yellow block and blue block because they both weigh light and feel hard, which is the criteria of the task.}\\
% \textcolor{mygreen}{\textit{done()}}
% }
% \end{casebox}

\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/task-property-comb.pdf}
    \caption{The planning process of Matcha on a task where the robot is asked to pick up multiple objects with a combination of multiple properties. }
    \label{fig:new-task-property-combine}
\end{figure}

\subsubsection{Utility Reasoning}
This scenario involves a robot being asked to pick up an object that possesses the utility for performing a specified task. 
To successfully complete the challenging task, the robot must utilize commonsense reasoning to establish a meaningful connection between the object's multimodal perceptions and the required utility, ultimately selecting an appropriate object.
Fig.~\ref{fig:new-task-utility} shows the planning process of Matcha when asked to pick up an object capable of cracking a nut.
We can see that its interactive perception process and the final decision of choosing the ``red block" are reasonable.

\subsection{Discussion}
Weak LLMs may not have sufficient capability for precise planning, and thus may require carefully engineered prompts or other grounding techniques. On the other hand, strong LLMs exhibit impressive abilities in zero-shot planning, even in new scenarios that necessitate contextual reasoning. 
These observations highlight the potential of leveraging knowledge within strong LLMs, as it enables the successful execution of tasks that were previously deemed infeasible.
% As long as the skills are guaranteed to be sufficient to complete certain requests, a robot with LLMs as a backbone will 
LLMs can derive significant advantages from utilizing common knowledge, being robust to various instructions regardless of their changes in synonym, linguistic structure or even semantic meanings out of the scope that the robot is initially designed within, e.g.\ in our example, the instruction permits variation from ``the metal block" to ``a block that may be suitable for cracking a nut".

Nonetheless, the reasoning trace may not always align with human expectations. There are cases that LLMs may prematurely draw conclusions due to their limited logical reasoning ability, particularly when faced with a task that requires reasoning from a long list of facts.

\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/task-utility.pdf}
    \caption{The planning process of Matcha on a task where the robot is required to select an object that is potentially suited for a specified utility.}
    \label{fig:new-task-utility}
\end{figure}

% Human: "pick up the object that a cat can play with" in a new scene contains [red block, green block on the left, yellow block, green block on the right]
% AI: > robot.touch(red block)
% Human: It feels rough.
% AI: > robot.touch(green block on the left)
% Human: It feels soft.
% AI: > robot.touch(yellow block)
% Human: It feels hard.
% AI: > robot.touch(green block on the right)
% Human: It feels squishy and malleable.
% AI: > robot.pick\_up(green block on the right)
% Human: Explain why.
% AI: I picked up the green block on the right because it felt squishy and malleable, which suggests that it is a soft material that a cat can safely play with.