\section{METHODOLOGY}

\subsection{Architecture}
% Our model (maybe give a name, e.g., \textbf{m}ultimod\textbf{a}l environmen\textbf{t} \textbf{ch}atting \textbf{a}gent (Matcha)) 
We propose \textbf{Matcha} (\textbf{m}ultimod\textbf{a}l environmen\textbf{t} \textbf{ch}atting \textbf{a}gent) which is able to interactively perceive (``chat" with) the environment through multimodal perception when the information from passive visual perception is insufficient for completing an instructed task.
% The agent actively evokes a perception module and perceives the environment in another chosen modality other than passive visual perception. 
The epistemic actions are executed autoregressively until the agent is confident enough about the information sufficiency in that situation. % for the task can be finally completed. 

Fig.~\ref{fig:overview-architecture2} provides an overview of the architecture of Matcha. It is a modular framework of three parts: an LLM backbone, multimodal perception modules and a low-level command execution policy. They connect to each other with language as the intermediate representation for information exchange. 

To be specific, given a high-level instruction, especially the one that Matcha cannot directly perform with the command policy alone, the LLM backbone will reason the situations and select the most contextually admissible perceiving command to gather information.
After the execution of the policy module, the resulting environmental response is processed by a correspondingly evoked multimodal perception module into semantic descriptions, e.g.\ ``clinking sound" by an auditory module after the ``knock on" action. 
Finally, the executed command itself as well as the environmental state description are fed back to the LLM for future planning.

The LLM is used in a zero-shot manner without any need for fine-tuning, being independent of other components.
Policy and perception modules can be separately designed and plugged into the framework whenever needed.
Intrinsically linked by natural language, this framework is flexible and can scale and adapt easily to possible robotic upgrades or diverse robotic scenarios.

% \begin{figure}[thpb]
%     \centering
%     \includegraphics[scale=0.6]{figures/overview.pdf}
%     \caption{Overview of llm + multimodal acquisation}
%     \label{fig:overview-architecture}
% \end{figure}

\begin{figure}[thpb]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \caption{Overview of Matcha. The framework contains an \textcolor{mypurple}{LLM}, \textcolor{myorange}{multimodal perception modules}, and a language-conditioned \textcolor{myyellow}{policy}. 
    These components communicate with each other with natural language as the intermediate representation. Three types of language information are involved in composing the prompt: 
    \textcolor{myred}{\textit{I}} is a language \textcolor{myred}{instruction} from the user, \textcolor{mygreen}{\textit{C}} is a language \textcolor{mygreen}{command} produced by the LLM, and \textcolor{myblue}{\textit{F}} is semantic \textcolor{myblue}{feedback} from multimodal perceptions. Dotted lines indicate possibly evoking paths.}
    \label{fig:overview-architecture2}
\end{figure}


\subsection{Multimodal Perception and Execution Policy} 

We select a commonly feasible suit of modalities and a language-conditioned policy as an example implementation of our framework.
Other varieties for specific scenarios  can also be easily integrated due to the flexibility of modularity of the framework. Detailed experimental implementations will be introduced in Sec. \ref{sec:experiments}.

% The output is wrapped in a natural language form.
\subsubsection{Vision} 
Usually, a monitoring camera is the cheapest option for a robot to passively perceive such rich information.
We employ pre-trained ViLD \cite{Gu2022OpenvocabularyObjectDetection}, an open-vocabulary visual detection model, as the vision perception module to detect objects with their categories and positions in the scene. Then, the results will be delivered to a policy module for identification and execution. Meanwhile, a prompt template ``The scene contains [\textit{OBJ1}, \textit{OBJ2}, ...]" is applied to construct a scene description, which enables the LLM to have an initial impression of the environment.

\subsubsection{Impact Sound}
Impact sound commonly occurs from time to time, and can be useful for robotic multimodal learning \cite{Zhao22ImpactMakes}. 
Though it can be passively collected with a microphone attached to the robotic end-effector, without intentional intervention by the robot, a ``knock on" action in our case, a microphone may only be able to collect background noise.
This auditory perception module classifies the consequent impact sound into a description and then wraps it in a natural language form. 
Actually, a clip of audio may contain sufficient information for some of the usage, e.g.\ to distinguish metal from glass \cite{Dimiccoli22RecognizingObject}. However, it may not be the case for other scenarios, for example, to select the only targeted one among a set of similar ``dull" sounds that could indicate either plastic, wood or hard paper. Therefore, we showcase both of the designs, i.e.\ one with a specific material classification (e.g.\ ``glass") and another with solely low-level and non-distinct descriptions (e.g.\ ``tinkling").
The modular output is also wrapped with templates to a full sentence such as ``It sounds tinkling", to guarantee processing consistency with LLMs. 

\subsubsection{Weight}
Weight measurements are usually obtained via the torque exerted on the robotic arm subsequent to the execution of an ``weighing" action.
The weight information is directly translated into natural language like ``It is lightweight" or ``It weighs 30g". Note that with implicit clarification of the scenario and the type of objects that a robot is manipulating, LLMs can interpret numerical values into contextual meanings.

\subsubsection{Haptics}
Haptic perception is extremely important for humans to interact with their surroundings. It also provides a potential for robots when acquiring information related to physical properties, including hardness, texture, and so on. However, a high-resolution tactile sensor is costly and not worthwhile for many applications. Therefore, in our case, we only use highly abstract descriptions for the force-torque feedback subsequent to a ``touching" action on an object, e.g.\ ``It feels soft" or ``It feels hard and smooth".

\subsubsection{Execution Policy}
The execution policy is conditioned on the generated command by an LLM and the visual information provided by the vision perception module. 
Once an actionable command together with an identified target is suggested by the LLM, the policy module locates the targeted object and executes a certain action. Meanwhile, the environmental feedback will be concurrently collected for multimodal perception modules for further post-processing as demonstrated above.

%  The command is in a form of ``Action(Object)", e.g., ``knock\_on(green block)". 
% Utilizing the detected object classes and corresponding positions by the visual perception module, the policy executes the instructed action on the designated object. 
% by means of inverse kinematics and hard-coded
% The policies for the ``knock on", ``grasping", and ``touching" actions are all hard-coded. 
% (What actions do we have? Do we have a "pick and place" action for completing the task?)

\subsection{Prompt Engineering}

An issue of grounding LLMs on robotic scenarios is that some of the suggestions generated by LLMs are not executable for a specific robot \cite{Ahn22CanNot, Huang22LanguageModels}, which stems from the fact that LLMs are pre-trained with extremely large open-domain corpora, while the robot is constrained by its physical capability and application scenarios, e.g. a tabletop robot is not able to perform ``walk" action. 

In this work, the LLM is applied for zero-shot planning \cite{Mialon23AugmentedLanguage, Zeng23SocraticModels} with prompt, in which all the executable commands are defined together with several task examples as the initial ``chat" history. This leading prompt enables the LLM to ground on the specific scenario and follow the contextual patterns for commanding the execution policy.
% and several leading instances are defined. 
% and prompt the model to choose one of them at each round of action planning. 
% Besides the definition of the executable actions, the initial prompt also contains ``chat" histories of some example tasks.
The initial prompt is around 500 words long, and here is a snippet:
\begin{tcolorbox}[
    title={Snippet of the Initial Prompt}, 
    fonttitle=\fontsize{9}{12}\selectfont, % \selectfont is required here
    colbacktitle=darkgray,
    colframe=darkgray,
    boxrule=1pt,
    standard jigsaw,
    % title=Snippet of the Initial Prompt,
    opacityback=0,  % this works only in combination with the key "standard jigsaw"
    % code={\singlespacing},
    frame hidden,
    interior hidden,
    boxsep=0pt,
    left=4pt,
    right=2pt,
    top=2pt,
    bottom=2pt,
    fontupper=\linespread{0.8}\selectfont,
    fontlower=\linespread{0.8}\selectfont,
]
{\small
\textcolor{gray}{AI has the following skills to help complete a task:\\
1. ``\textit{robot.knock\_on()}": to knock on any object and hear the sound to determine the material it consists of. Most of the materials can be determined by this skill.\\
2. ``\textit{robot.touch()}": to touch with haptics sensors. It is useful for some of the materials.\\
...} \\
\textcolor{gray}{Human:} \textcolor{gray}{``pick up the glass block" in the scene contains [yellow block, blue block, green block]}\\
\textcolor{gray}{AI:} \textcolor{gray}{\textit{robot.weigh(yellow block)}}\\
\textcolor{gray}{Feedback:} \textcolor{gray}{It weighs light.}\\
\textcolor{gray}{AI:} \textcolor{gray}{\textit{robot.weigh(blue block)}}\\
\textcolor{gray}{Feedback:} \textcolor{gray}{It weighs a little bit heavy.}\\
\textcolor{gray}{AI:} \textcolor{gray}{\textit{robot.knock\_on(blue block)}}\\
\textcolor{gray}{Feedback:} \textcolor{gray}{It sounds tinkling.}\\
% AI: \textit{robot.touch(gray block)}\\
% Feedback: It feels soft.\\
\textcolor{gray}{AI:} \textcolor{gray}{\textit{robot.pick\_up(blue block)}}\\
\textcolor{gray}{done() \\
...}
}
\end{tcolorbox}

We found that only language models that are large enough can follow the patterns in the prompt strictly, i.e.\ only generate commands that have been defined in strictly case-sensitive letters and with the same amount of allowed parameters for each, while small ones can hardly obey this constraint and generate unexpected commands, which brings extra demands for tuning.
% They are important for the LLM to achieve the task because they send task-specific information to the LLM. 
As the action planning is performed by LLMs constrained by a given prompt, 
the proposed framework demonstrates high flexibility and generalizability upon the possible incorporation of novel actions or perception modules into the system.
% As the planning of actions is made by an LLM in a zero-shot manner while adhering to the imposed constraints by the prompt, the proposed framework exhibits significant levels of generalizability upon the integration of fresh perception modules or actions into the system.

% \ml{Emphasize the setup that the choices of using all modalities are possible for the LLM. This is interesting and challenging. }
