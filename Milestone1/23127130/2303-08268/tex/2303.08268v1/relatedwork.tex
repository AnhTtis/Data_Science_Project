\section{RELATED WORK}

\textbf{Multimodal Learning and Robotic Information Gathering.}
Research in multimodality in robotics nowadays attracts growing attention \cite{Akkus23MultimodalDeep} because of its success in, for example, audio-visual learning \cite{Zhao22ImpactMakes, Wei22LearningAudiovisual, Zhu21DeepAudiovisual} and language-visual learning \cite{Shridhar22CLIPortWhat, Shridhar22PerceiveractorMultitask}. 
%There are also multimodal datasets \cite{Gao22ObjectFolderMultisensory} and object-centric models \cite{Devin18DeepObjectcentric} available for the study of manipulation tasks. \mlinline{What is the function of this sentence? }
It is beneficial and sometimes essential for a robot to learn from multimodality because one modality could carry some distinct information, e.g.\ tones in speech, that cannot be deduced from another. \cite{Lee22SoundguidedSemantic}.

Capable robots require managing one or several sensors to maximize the information needed for disambiguation \cite{Barnard06CrossModal} regarding a specific goal. This problem is known as \textit{active information acquisation} \cite{Atanasov15ActiveInformation, Wakulicz21ActiveInformation} or, particularly in robotics, \textit{robotic information gathering} \cite{Rankin21RoboticInformation}, where robots have to properly select perceiving actions to reduce ambiguity or uncertainty.
Besides handcrafted rules, some information advantage measures, e.g.\ entropy or information gain, are usually employed to maximize \cite{Atanasov15ActiveInformation}.
However, the combination of multimodal data is usually challenging. There are studies on fusing multimodal data according to their uncertainties, but this may face numerical instability and is difficult to transfer from one application to another \cite{Wang22UncertaintyawareMultimodal}.
Instead of directly fusing the multisensory data in a numerical space, we propose to use multimodal modules to translate them into natural language expressions that an LLM can easily digest.

\textbf{Large Language Models in Robotic Planning}.
Very recent works use LLMs to decompose high-level instructions into actionable low-level commands for zero-shot planning. They use LLMs as a planner to autoregressively select actions that are appropriate with respect to the instruction according to application-based prompts \cite{Zeng23SocraticModels}, the semantic similarity between mapped pairs \cite{Huang22LanguageModels}, or the contextual language score grounded on realistic robot affordances \cite{Ahn22CanNot}. Other works ground LLM knowledge in human-robot interaction \cite{Cui23NoRight} and other various fields where domain knowledge is distinct and modular frameworks can be composed via language as the intermediate representation \cite{Zeng23SocraticModels, Mialon23AugmentedLanguage}. 

However, these works design a robot to form a planning strategy with \textit{built-in knowledge}, rather than \textit{interact} with the surroundings and make decisions based on \textit{actively collected information} from the environment. There is no feedback loop for their LLMs to perceive the environmental cues, such that only ``blind" decisions were made in the robotic unrolling process. 
In contrast, our interactive architecture allows LLMs to access the environment state from multiple modalities for adaptive planning.

% \ml{- Socratic Models \cite{Zeng2022SocraticModelsComposing}. They use 
% Differently, we compose multimodal perception modules instead of pre-trained models for interactive perception. 
% (Their basic starting point is that they argue that commonsense knowledge in different foundation models trained in different domains is distinct and complementary. The diversity is symbiotic. Their motivation is to use diverse commonsense knowledge in various tasks. Their method is to use a modular framework in a zero-shot manner.) }
% \xf{done. integrated} 

% \ml{- Using LLMs for zero-shot planning . No interactive perception. No multimodal perception. Also focuses on high-level robotic tasks instructed by natural language. Improve the executability of the command generated by the LLM.  Map generated commands to admissible commands according to the semantic similarity. Generate commands autoregressive using mapped commands. Use weak supervision by prompting the model with a known example task similar to the current task. }
% \xf{done. integrated} 