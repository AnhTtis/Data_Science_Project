\section{INTRODUCTION}

\textit{How do humans perceive the surroundings to uncover latent properties?}

Suppose you are presented with an uncommon object in a strange shape and of unknown material,
you may explore its properties in both passive and active ways, if possible, e.g.\ by observing the geometry, touching and even knocking on the surface in order to deduce its exact functionalities from the feedback.
Unnecessary explorations, which could be essential for other scenarios such as smelling, will not be performed in this context unless something counterintuitive happens.
We humans naturally perform these \textbf{multimodal observations and examinations} in daily life through \textbf{common sense and established knowledge}, and over time we adapt it with the accumulation of experience \cite{Barnard06CrossModal}.

This is also now possible for a robot well-equipped with multiple sensors and LLMs.
An environment may be filled with rich content and the robot can be overwhelmed with diversified sensory stimuli.
An intelligent robot should wisely 1) choose stimuli to attend to, avoiding eagerly being bogged down into details, and 2) respond accordingly to the resulting sensations in the context of a specific task.
% \begin{figure}[thpb!]
\begin{figure}[t!] 
    \centering
    \includegraphics[width=\linewidth]{figures/newimage.pdf}
    \caption{Given instruction from a human, the robot recurrently ``chats" with the environment to obtain sufficient information for achieving the task. An LLM generates action commands to interactively perceive the environment; And in response, the environment provides multimodal feedback (MF) through multimodal perception modules. }
    \label{fig:exp}
\end{figure}

\subsection{Interactive Multimodal Perceptions}

Like humans, robots can perceive the environment in either a passive or an interactive way \cite{Kroemer21ReviewRobot}.
\textit{Passive perception} refers to ways such as visual or auditory monitoring, it allows robots to quickly acquire information without intervening with the surroundings. However, the passive manner has its limits, among which the most outstanding one is its impotency when facing \textit{epistemic uncertainty} \cite{Celemin23KnowledgeandAmbiguityaware, Murphy22ProbabilisticMachine}, the uncertainty because of lacking knowledge.
% : 1) data source is limited; 2) it is sometimes a waste of resources to keep every possibly useful sensor on.

Epistemic uncertainty inevitably arises from diverse sources, e.g.\ from the ambiguity in human instructions, from low-resolution sensing, or from insufficient modalities.
%, or even from the scope beyond passive observations.
% TODO \mlnote{remove those ``from" after ``e.g.". }
Many of them can only be reduced with \textit{interactive perception}, in which a robot actively interrogates the environment to increase estimate accuracy and even uncover latent information.
% Information in one modality, even though as rich as vision, can be insufficient for certain tasks.
For example, when being asked to deliver a \textit{steel} screw instead of the one with a similar shape but made of \textit{aluminium}, an assistant robot may need to locate possible candidates with \textit{passive} vision and further, \textit{interactively}, resort to a weighing or a magnetic module for confirmation.

Despite the promising advantages, interactive perception is less common than the passive manner because it entails increased complexity.
Efforts are needed to design a mediating system to handle various sensory data and to adapt to changes in the conditions of both the robot and the environment, such as a new robotic modular being available or the involvement of novel objects.

% llm
\subsection{Chatting with the Environment}
LLMs have been showing incredible potential in areas besides robotics \cite{Ahn22CanNot, Lynch22InteractiveLanguage, Cui23NoRight, Mialon23AugmentedLanguage}.
Human knowledge that resides in LLMs can help a robot abstract and select only suitable features, e.g. relevant to the region of interest or informative modalities, to simplify the learning process.
Moreover, in terms of generalizability, the knowledge of LLMs allows a behavioral agent to adapt efficiently to novel concepts and environmental structures.
For instance, when being asked to \textit{use one adjective for each to describe the touch feel of a sponge and a brick}, ChatGPT\footnote{https://openai.com/blog/chatgpt/} will respond with ``soft'' and ``hard'', respectively. This is helpful for a robot with a haptics sensing module to distinguish between these two novel, never-seen objects.

LLMs are usually generative models that predict tokens to come, but with certain designs, e.g.\ conversational prompts, LLMs are capable of generating chat-fashion texts. This allows their integration with a robot to not only plan with respect to a robot's built-in ability \cite{Zeng23SocraticModels, Ahn22CanNot}  but also respond according to environmental feedback.

% ML: introduce the challenge in using LLMs in these robotic tasks and introduce our method.
However, they cannot directly process application-specified raw multimodal data. We 
resort to modular perceptions for each modality that are separately trained before being plugged into the LLM backbone. Each module semantically translates the resulting multimodal sensations into natural language that can be understood by LLMs and processed in a unified manner.

Our contributions are threefold. Firstly, we establish a manipulation scenario with multimodal sensory data and language descriptions. 
Secondly, we propose \textbf{Matcha}\footnote{By the name of a type of East Asian green tea.
To fully appreciate matcha, one must engage multiple senses to perceive its appearance, aroma, taste, texture, and other sensory nuances.
} (\textbf{m}ultimod\textbf{a}l environmen\textbf{t} \textbf{ch}atting \textbf{a}gent), where an LLM is prompted to work in a chatting fashion, thus having continuous access to environmental feedback for contextual reasoning and planning.
Finally, we show that LLMs can be utilized to perform interactive multimodal perception and behavior explanation. Accordingly, an interactive robot can make reasonable and robust decisions by resorting to LLMs to examine objects and clarify their properties that are essential to completing the task (see Fig.~\ref{fig:exp}). The projectâ€™s website can be found at \href{https://xf-zhao.github.io/projects/Matcha}{xf-zhao.github.io/projects/Matcha}.

% Humans show consistent crossmodal correspondences when dealing with events in different sensory modalities.
% , for example, a high-pitched scratch sound, a smooth touch feeling and a transparent appearance are concurrently perceived for many objects made of glass.

% Robotic perceptions with different purposes, e.g.\ to explore multimodalities for a comprehensive understanding, can also be divided into passive and interactive manners

% ; 3) a robot may stuggle to fuse all of the data and extract patterns from that.


% ML: introduce the importance of commonsense reasoning and chain of thoughts in human and robotic tasks. 


% We use ViLD as the visual module, ... (networks for other modalities). 
% It works like the chain of thoughts in the brain. 