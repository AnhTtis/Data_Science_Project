\section{Generalization, Limitation and Future Work}
The Matcha framework exhibits a high degree of generalizability thanks to the commonsense knowledge inside LLMs.
Without LLMs, a control algorithm, e.g. one trained with reinforcement learning \cite{Li23InternallyRewarded, Singh20COGConnecting}, may require massive datasets/interactions to learn
the common sense \cite{Singh20COGConnecting} of collaborating different modalities, yet being less efficient and generalizable.

However, interpreting the real world with language can be limited to the complexity of the task and the environment dynamics.
For example, advanced reasoning techniques such as decomposing may be required to deal with a complicated task,
where the task is decomposed into several sub-tasks to tackle separately. 
This automatic operation highlights the flexibility of LLMs but also poses challenges to the static language expression of a complex world
--- The vision-to-language module should be called multiple times with flexible queries.
This brings the requirement of vision-enabled LLMs \cite{Zhu23MiniGPT4Enhancing, Brohan23RT2Visionlanguageaction}, 
built on which the reasoning can be malleable. But multimodal LLMs are yet less controllable and accurate in terms of describing the scene
compared with a templated module.

Despite current limitations, multimodal LLMs gain increasing attention due to their great potential and flexibility.
Future work will explore the multimodal models \cite{Tong22VideoMAEMasked, Brohan23RT2Visionlanguageaction} to leverage unified features.