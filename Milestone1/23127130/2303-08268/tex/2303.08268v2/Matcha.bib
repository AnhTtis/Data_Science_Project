@inproceedings{Brohan23RT2Visionlanguageaction,
  title = {{{RT-2}}: {{Vision-language-action}} Models Transfer Web Knowledge to Robotic Control},
  booktitle = {{{arXiv}} Preprint {{arXiv}}:2307.15818},
  author = {Brohan, Anthony and Brown, Noah and et. al},
  year = {2023},
  eprint = {2307.15818},
  archiveprefix = {arxiv},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Brohan23RT2Visionlanguageaction.pdf}
}

@misc{Kerzel23NICOLNeuroinspired,
  title = {{{NICOL}}: {{A Neuro-inspired Collaborative Semi-humanoid Robot}} That {{Bridges Social Interaction}} and {{Reliable Manipulation}}},
  shorttitle = {{{NICOL}}},
  author = {Kerzel, Matthias and Allgeuer, Philipp and Strahl, Erik and Frick, Nicolas and Habekost, Jan-Gerrit and Eppe, Manfred and Wermter, Stefan},
  year = {2023},
  month = jun,
  number = {arXiv:2305.08528},
  eprint = {2305.08528},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.08528},
  urldate = {2023-07-25},
  abstract = {Robotic platforms that can efficiently collaborate with humans in physical tasks constitute a major goal in robotics. However, many existing robotic platforms are either designed for social interaction or industrial object manipulation tasks. The design of collaborative robots seldom emphasizes both their social interaction and physical collaboration abilities. To bridge this gap, we present the novel semi-humanoid NICOL, the Neuro-Inspired COLlaborator. NICOL is a large, newly designed, scaled-up version of its well-evaluated predecessor, the Neuro-Inspired COmpanion (NICO). NICOL adopts NICO's head and facial expression display, and extends its manipulation abilities in terms of precision, object size and workspace size. To introduce and evaluate NICOL, we first develop and extend different neural and hybrid neuro-genetic visuomotor approaches initially developed for the NICO to the larger NICOL and its more complex kinematics. Furthermore, we present a novel neuro-genetic approach that improves the grasp-accuracy of the NICOL to over 99\%, outperforming the state-of-the-art IK solvers KDL, TRACK-IK and BIO-IK. Furthermore, we introduce the social interaction capabilities of NICOL, including the auditory and visual capabilities, but also the face and emotion generation capabilities. Overall, this article presents for the first time the humanoid robot NICOL and, thereby, with the neuro-genetic approaches, contributes to the integration of social robotics and neural visuomotor learning for humanoid robots.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Kerzel23NICOLNeuroinspired.pdf}
}

@misc{Ouyang22TrainingLanguage,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and et. al},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2023-07-30},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Ouyang22TrainingLanguage.pdf}
}

@misc{Patki19InferringCompact,
  title = {Inferring {{Compact Representations}} for {{Efficient Natural Language Understanding}} of {{Robot Instructions}}},
  author = {Patki, Siddharth and Daniele, Andrea F. and Walter, Matthew R. and Howard, Thomas M.},
  year = {2019},
  month = mar,
  number = {arXiv:1903.09243},
  eprint = {1903.09243},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-08-01},
  abstract = {The speed and accuracy with which robots are able to interpret natural language is fundamental to realizing effective human-robot interaction. A great deal of attention has been paid to developing models and approximate inference algorithms that improve the efficiency of language understanding. However, existing methods still attempt to reason over a representation of the environment that is flat and unnecessarily detailed, which limits scalability. An open problem is then to develop methods capable of producing the most compact environment model sufficient for accurate and efficient natural language understanding. We propose a model that leverages environment-related information encoded within instructions to identify the subset of observations and perceptual classifiers necessary to perceive a succinct, instruction-specific environment representation. The framework uses three probabilistic graphical models trained from a corpus of annotated instructions to infer salient scene semantics, perceptual classifiers, and grounded symbols. Experimental results on two robots operating in different environments demonstrate that by exploiting the content and the structure of the instructions, our method learns compact environment representations that significantly improve the efficiency of natural language symbol grounding.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Patki19InferringCompact.pdf}
}

@misc{Tong22VideoMAEMasked,
  title = {{{VideoMAE}}: {{Masked Autoencoders}} Are {{Data-Efficient Learners}} for {{Self-Supervised Video Pre-Training}}},
  shorttitle = {{{VideoMAE}}},
  author = {Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  year = {2022},
  month = oct,
  number = {arXiv:2203.12602},
  eprint = {2203.12602},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.12602},
  urldate = {2023-08-01},
  abstract = {Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90\% to 95\%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4\% on Kinetics-400, 75.4\% on Something-Something V2, 91.3\% on UCF101, and 62.6\% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Tong22VideoMAEMasked.pdf}
}

@misc{Zhu23MiniGPT4Enhancing,
  title = {{{MiniGPT-4}}: {{Enhancing Vision-Language Understanding}} with {{Advanced Large Language Models}}},
  shorttitle = {{{MiniGPT-4}}},
  author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  year = {2023},
  month = apr,
  number = {arXiv:2304.10592},
  eprint = {2304.10592},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.10592},
  urldate = {2023-08-01},
  abstract = {The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. We believe the primary reason for GPT-4's advanced multi-modal generation capabilities lies in the utilization of a more advanced large language model (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer. Our findings reveal that MiniGPT-4 possesses many capabilities similar to those exhibited by GPT-4 like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, providing solutions to problems shown in images, teaching users how to cook based on food photos, etc. In our experiment, we found that only performing the pretraining on raw image-text pairs could produce unnatural language outputs that lack coherency including repetition and fragmented sentences. To address this problem, we curate a high-quality, well-aligned dataset in the second stage to finetune our model using a conversational template. This step proved crucial for augmenting the model's generation reliability and overall usability. Notably, our model is highly computationally efficient, as we only train a projection layer utilizing approximately 5 million aligned image-text pairs. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Zhu23MiniGPT4Enhancing.pdf}
}

@inproceedings{Li23InternallyRewarded,
  title = {Internally {{Rewarded Reinforcement Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Li, Mengdi and Zhao, Xufeng and Lee, Jae Hee and Weber, Cornelius and Wermter, Stefan},
  year = {2023},
  month = jul,
  eprint = {2302.00270},
  primaryclass = {cs},
  urldate = {2023-07-28},
  abstract = {We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textbackslash textit\{Internally Rewarded Reinforcement Learning\} (IRRL) as the reward is not provided directly by the environment but \textbackslash textit\{internally\} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance compared with baselines in diverse tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Li23InternallyRewarded.pdf}
}

@inproceedings{Singh20COGConnecting,
  title = {{{COG}}: {{Connecting New Skills}} to {{Past Experience}} with {{Offline Reinforcement Learning}}},
  shorttitle = {{{COG}}},
  booktitle = {Conference on {{Robot Learning}} ({{CoRL}})},
  author = {Singh, Avi and Yu, Albert and Yang, Jonathan and Zhang, Jesse and Kumar, Aviral and Levine, Sergey},
  year = {2020},
  month = oct,
  eprint = {2010.14500},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2010.14500},
  urldate = {2023-08-01},
  abstract = {Reinforcement learning has been applied to a wide variety of robotics problems, but most of such applications involve collecting data from scratch for each new task. Since the amount of robot data we can collect for any single task is limited by time and cost considerations, the learned behavior is typically narrow: the policy can only execute the task in a handful of scenarios that it was trained on. What if there was a way to incorporate a large amount of prior data, either from previously solved tasks or from unsupervised or undirected environment interaction, to extend and generalize learned behaviors? While most prior work on extending robotic skills using pre-collected data focuses on building explicit hierarchies or skill decompositions, we show in this paper that we can reuse prior data to extend new skills simply through dynamic programming. We show that even when the prior data does not actually succeed at solving the new task, it can still be utilized for learning a better policy, by providing the agent with a broader understanding of the mechanics of its environment. We demonstrate the effectiveness of our approach by chaining together several behaviors seen in prior datasets for solving a new task, with our hardest experimental setting involving composing four robotic skills in a row: picking, placing, drawer opening, and grasping, where a +1/0 sparse reward is provided only on task completion. We train our policies in an end-to-end fashion, mapping high-dimensional image observations to low-level robot control commands, and present results in both simulated and real world domains. Additional materials and source code can be found on our project website: https://sites.google.com/view/cog-rl},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Singh20COGConnecting.pdf}
}
