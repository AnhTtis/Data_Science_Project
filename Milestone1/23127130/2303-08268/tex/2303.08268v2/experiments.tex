\section{Experiments}
\label{sec:experiments}
\subsection{Experimental Setup}
We evaluate the proposed framework in an object-picking task: a robot is instructed to pick up an object that is referred to by a latent property -- \textit{material} -- which is, however, not visually distinguishable under our settings. 
Tasks are intentionally designed such that information from a single modality could be insufficient to determine object properties, while other perception sources can provide compensations to reduce or eliminate this ambiguity.
For example, glass and metal surfaces could exhibit similar hard and smooth properties upon contact, in which case differences in impact sound can aid in further differentiation.
Tab.~\ref{table:metarials-properties} lists variational multimodal descriptions of the materials. These properties are wrapped as natural language sentences before being fed back to the LLM.

Experiments are done in CoppeliaSim\footnote{\url{https://www.coppeliarobotics.com/}} simulations with the NICOL robot \cite{Kerzel23NICOLNeuroinspired},
where several blocks in various colors, materials, weights, and surface textures are randomly selected and placed on the table next to a brown container (see Fig.~\ref{fig:exp}).
The ViLD \cite{Gu2022OpenvocabularyObjectDetection} model is meant to be easily generalized to describe complex scenes despite the simplicity of the object setting here.
After detection, the objects are represented universally by their name, which serves as a parameter for the action function to identify.
Objects with the same color will be distinguished as ``.. on the left/right'' given the simplicity of avoiding more than two duplicated colors for the same shape.
The desktop robot is equipped with two \textit{Open-Manipulator-Pro} arms \footnote{\url{https://emanual.robotis.com/docs/en/platform/openmanipulator\_p/overview/}}, but only its right arm is activated to operate.
It is capable of executing actions in [``knock on", ``touch", ``weigh", ``pick up"], with a parameter to indicate the targeted object.
The first three actions correspond to the interactive perception of impact sound, haptics, and weight respectively, and the last action finalizes the task by picking and transporting an object into the box.
Each instruction is guaranteed to be achievable with the capability of the robot. %, i.e., there is one object of the queried material on the table.

Due to the lack of support for physics-driven sound and deformable object simulation in Coppeliasim, we have implemented reasonable alternatives.
For the haptics of objects, we simplify haptic perception by assigning variational descriptions regarding its material, e.g.\ fibrous objects are usually perceived as ``soft" and a plastic object can be either ``soft" or ``hard". Note that advanced implementations can also be achieved using a neural network as is used in the sound perception module when haptics data for deformable objects is available. 
For the impact sound, we split the YCB-impact-sound dataset \cite{Dimiccoli22RecognizingObject} into training and testing sets and augment them with tricks such as shifting, random cropping, and adding noise.
The training set is used to train our auditory classification neural networks,
while the audios in the testing part are randomly loaded as an alternative to run-time impact sound simulation for the materials mentioned,

Sound can be informative, though not perfect, for determining materials \cite{Dimiccoli22RecognizingObject}.
Besides showing the mediating ability of multiple modalities by the LLM, we further investigate its reasoning ability by employing indistinct descriptions instead of exact material labels.
\begin{itemize}
    \item  \textit{Distinct description}: the sound module describes sound feedback by the corresponding material name and its certainty from the classification model, e.g. ``It is probably glass" or ``It could be plastic with a 47\% chance, or ceramic with a 35\% chance". 
    The distinct description setting is more task-oriented, and it examines the robot's ability to mediate multiple sensory data for disambiguation.
    \item \textit{Indistinct description}: we listed some commonly used indistinct sound descriptions in human communications in Tab.~\ref{table:metarials-properties}, e.g.\ using ``dull" to describe the sound from a plastic block and ``tinkling" to describe the sound for both ceramic and glass objects.
    This setting is more task-agnostic and thus has the potential for generalization. 
    Moreover, it compels the LLM to infer ``professional" material terminology from ambiguous yet multimodal descriptions. 
\end{itemize}
The online OpenAI \textit{text-davinci-003} API\footnote{\label{footnote:gpt3-models} \url{https://platform.openai.com/docs/models/gpt-3}} is applied as the LLM backend because it demonstrates robust instruction-following ability and outstanding reasoning performance.\footnote{The \textit{code-davinci-002} is not chosen because it is common sense instead of the ability of code completion that matters to the active perception. Upon the time that this experiment was carried out, the \textit{text-davinci-003} model is the state-of-the-art GPT-3.5 model available; 
while the later released ChatGPT or GPT-4 model showcases the impressive improved abilities of reasoning, future works will explore the potential of these models.}
We also evaluate with a weaker but far less expensive LLM \textit{text-ada-001}, a GPT-3 model which is usually fast and capable of simple tasks, under the same setting as comparison.
\begin{table}[ht!]
\begin{tabularx}{\linewidth}{|l|X|X|X|}
\hline
\multicolumn{1}{|c}{\textbf{Materials}} & \multicolumn{1}{|c}{\textbf{Impact Sound}} & \multicolumn{1}{|c}{\textbf{Haptics}} & \multicolumn{1}{|c|}{\textbf{Weight}}  \\ \hline \hline
Metal & ``resonant and echoing", ``metallic", ``ringing" & ``hard and cold", ``rigid, cold, and smooth" & ``heavy", ``300g" \\\hline
Glass  &  ``tinkling", ``tinkling and brittle" & ``hard", ``hard and smooth", ``cold and smooth" & ``a little bit heavy", ``150g" \\ \hline
Ceramic & ``clinking and rattling", ``rattling", ``tinkling and brittle" & ``hard", ``tough" &  ``average weight", ``not too light nor not too heavy", ``100g"  \\ \hline
Plastic  & ``dull", ``muffled" & ``hard", ``soft" &  ``light", ``30g" \\ \hline
Fibre & ``muted", ``silent" & ``soft", ``flexible" & ``lightweight", ``underweight", ``10g" \\ \hline
\end{tabularx}
\caption{\label{table:metarials-properties}  Property descriptions of different materials. }
\end{table}
\subsection{Results}
We test the proposed framework Matcha in 50 randomly generated scenarios for each setting and report the success rate. 

We report that the impact sound classification model pre-trained with the selected materials achieves an accuracy of 93.33\%.
When using distinct descriptions, suppose we are making hard-coded rules to utilize the sound module to identify the targeted material, the robot can randomly knock on an object among three, and classify the material until the one that is classified as the target. In theory, the success rate computes as  $\frac{1}{3} p + \frac{2}{3} p^2|_{p=93.33\%}=89.18\%$, where $p$ is the modular accuracy. Usually, other modalities, in this case, are not as distinct as sound, and it could be non-ideal for humans to craft such fusion rules for a possible slight improvement. Therefore, the theoretical success rate 
with only the sound module will be used as our baseline for analysis. Note that this is a reasonable rule that humans will follow, thus it can also be regarded as the upper bound for Matcha if it worked with only impact sound.

Unsurprisingly, Matcha achieves a relatively higher success rate of 90.57\% compared to the ideal theory baseline, as it utilizes compensatory information from other modalities in addition to sound.
When using the indistinct description of impact sound, Matcha is still able to achieve a success rate of 56.67\%, which is larger than a chance success rate of 33.33\% achieved by randomly picking one from the three.
This result is remarkable as it performs few-shot deduction with only indistinct adjectives available.
By analyzing the failure cases, we found that the similar descriptions of glass and ceramic in terms of impact sound, haptics, and weight make it challenging to distinguish one material from the other.
This is not an issue with distinct descriptions, where the sound classification network directly predicts the material, preventing a significant loss of information in the translation from sensory data to language.
While the system displays remarkable reasoning ability, the results still highlight the significance of a well-designed modular description that is not overly abstract in facilitating interactive perceptions.

We observe that only strong LLMs (GPT-3 \textit{text-davinci-003} in our experiments) can perform the task following the primary prompt as we defined, while weak ones possibly generate commands that are not executable and show degraded performance (see Tab.~\ref{table:rate}). 
For example, GPT-3 \textit{text-ada-001}\textsuperscript{\ref{footnote:gpt3-models}}, a smaller model in the GPT-3 model series, may generate commands like ``\textit{robot.knock\_on(metal block)}" or ``\textit{robot.weigh(yellow block, blue block)}", which is not feasible for the policy of the robot. 
\begin{table}[ht!]
\centering
\begin{tabularx}{0.8\linewidth}{lcc}
% \hline
\multicolumn{1}{c}{\textbf{LLM}}& \textbf{Type of Description} & \textbf{Success Rate} \\
\hline
\multirow{2}{*}{text-ada-001} & Indistinct & 19.05\% \\
 & Distinct & 28.57\% \\ 
 \hline
\multirow{2}{*}{text-davinci-003} & Indistinct & 56.67\% \\
 & Distinct & 90.57\% \\
\hline
\end{tabularx}
\caption{Effect of different LLMs on success rate. \label{table:rate}}
\end{table}
\subsection{Case Studies}
We provide case studies to showcase the interactive perception ability of Matcha in multimodal environments.
Following the convention of this paper, texts containing information from humans, LLMs, and the environment are indicated in red, green, and blue, respectively.
\begin{figure}[thpb]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/demos/case1.pdf}
    \caption{A successful example in which the robot deduces ``fiber" material with indistinct descriptions of impact sound.}
    \label{fig:case1}
\end{figure}
Fig.~\ref{fig:case1} gives a completion that the agent can deduce the latent material property from multimodal yet indistinct descriptions.
During the planning process, the agent decidedly terminates exploration of the ``red block" and instead engages in interacting with the ``green block" exhaustively for a comprehensive examination.
Common sense and, moreover, established knowledge in the LLM enables Matcha to efficiently interact with the environment.
Matcha's proficient behaviors provide evidence of effective reasoning, as it aligns with the subsequent explanation provided by the LLM, namely, that fiber can often be considered ``flexible" rather than ``cold and smooth".
The example depicted in Fig.~\ref{fig:case2} presents a fascinating observation: the impact sound of the ``orange block" suggests it is more likely to be plastic than metal, but Matcha accurately distinguishes it from plastics after engaging in the interactive perception of weight and haptics. This showcases the potential of multimodal perception to improve classification accuracy.
\begin{figure}[thpb]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/demos/case2.pdf}
    \caption{A successful example with a distinct description of impact sound. This example shows that by leveraging multimodal perception, LLM rectifies the misclassification that may occur when relying solely on sound modules.}
    \label{fig:case2}
\end{figure}
Fig.~\ref{fig:case3} provides a failure case with indistinct descriptions where the robot picks up a ceramic block when asked to pick up the one made of glass.
The underlying cause of this failure is the sensing similarity between glass and ceramic, which creates difficulty in resolving epistemic uncertainty.
\subsection{Discussion}
Weak LLMs, e.g. ones without fine-tuning on instruction alignment \cite{Ouyang22TrainingLanguage}, may not have sufficient capability for precise planning, and thus may require carefully engineered prompts or other grounding techniques.
On the other hand, strong LLMs exhibit impressive in-context learning abilities.
These observations highlight the potential of leveraging knowledge within strong LLMs, as it enables the successful execution of tasks that were previously deemed infeasible.
LLMs can derive significant advantages from utilizing common knowledge, being robust to various instructions regardless of their changes in synonym, linguistic structure or even semantic meanings out of the scope that the robot is initially designed within,
e.g.\ an instruction variation from ``the metal block" to ``a block that may be suitable for cracking a nut",
in which the robot has to establish a meaningful connection between the object's multimodal perceptions and the required utility.
\begin{figure}[thpb]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/demos/case3.pdf}
    \caption{An example in which the agent fails to distinguish glass and ceramic in the setup of using indistinct descriptions of impact sound.}
    \label{fig:case3}
\end{figure}

Nevertheless, the reasoning trace may not always align with human expectations.
There are cases that LLMs may prematurely draw conclusions due to their limited logical reasoning ability, particularly when faced with a task that requires reasoning from a long list of facts.