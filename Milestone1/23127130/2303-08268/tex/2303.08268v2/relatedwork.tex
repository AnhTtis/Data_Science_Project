\section{Related Work}

\textbf{Multimodal Learning and Robotic Information Gathering.}
Research in multimodality in robotics nowadays attracts growing attention \cite{Akkus23MultimodalDeep} because of its success in, for example, audio-visual learning \cite{Zhao22ImpactMakes, Wei22LearningAudiovisual, Zhu21DeepAudiovisual} and language-visual learning \cite{Shridhar22CLIPortWhat, Shridhar22PerceiveractorMultitask}. 
It is beneficial and sometimes essential for a robot to learn from multimodality because one modality could carry some distinct information, e.g.\ tones in speech, that cannot be deduced from another. \cite{Lee22SoundguidedSemantic}.

Capable robots require managing one or several sensors to maximize the information needed for disambiguation \cite{Barnard06CrossModal} regarding a specific goal. This problem is known as \textit{active information acquisition} \cite{Atanasov15ActiveInformation, Wakulicz21ActiveInformation} or, particularly in robotics, \textit{robotic information gathering} \cite{Rankin21RoboticInformation}, where robots have to properly select perceiving actions to reduce ambiguity or uncertainty.
Besides handcrafted rules, some information advantage measures, e.g.\ entropy or information gain, are usually employed to maximize \cite{Atanasov15ActiveInformation}.
However, the combination of multimodal data is usually challenging. There are studies on fusing multimodal data according to their uncertainties, but this may face numerical instability and is difficult to transfer from one application to another \cite{Wang22UncertaintyawareMultimodal}.
Instead of directly fusing the multisensory data in a numerical space, we propose to use multimodal modules to translate them into natural language expressions that an LLM can easily digest.

\textbf{Large Language Models in Robotic Planning}.
Very recent works use LLMs to decompose high-level instructions into actionable low-level commands for zero-shot planning. They use LLMs as a planner to autoregressively select actions that are appropriate with respect to the instruction according to application-based prompts \cite{Zeng23SocraticModels}, the semantic similarity between mapped pairs \cite{Huang22LanguageModels}, or the contextual language score grounded on realistic robot affordances \cite{Ahn22CanNot}.
Other approaches ground LLM knowledge in human interaction \cite{Cui23NoRight} or many other various fields where domain knowledge is distinct and modular frameworks can be composed via language as the intermediate representation \cite{Patki19InferringCompact, Mialon23AugmentedLanguage, Zeng23SocraticModels}.

However, these works design a robot to form a planning strategy with \textit{built-in knowledge}, rather than \textit{interact} with the surroundings and make decisions based on \textit{actively collected information} from the environment. There is no feedback loop for their LLMs to perceive the environmental cues, such that only ``blind" decisions are made in the robotic unrolling process. 
In contrast, our interactive architecture allows LLMs to access the environment state from multiple modalities for adaptive planning.