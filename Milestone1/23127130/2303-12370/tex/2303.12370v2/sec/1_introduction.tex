\section{Introduction}\label{sec:intro}
\input{fig/1_introduction/sequence_video}




A strong artificial intelligence (AI) system is expected to be able to learn knowledge from the open world in an embodied manner such that amounts of goal-oriented tasks are designed for reinforcement learning in the environment. In the area of video understanding, a great deal of pioneering work in video classification \cite{classification}, action localization \cite{localization}, and action segmentation \cite{segmentation} has been explored, laying the foundation for video understanding. Beyond these typical video understanding tasks, sequential videos (such as \cref{fig:sequence_video}) that usually describe how to perform a task in a certain sequence of procedures can be regarded as a goal-oriented task. Solving this task is extremely promising for guiding intelligence to learn a task like humans. It makes performing sequential video representations a potentially critical part of the road to strong AI.

Some efforts have been made for video representation learning for sequential videos. \eg, \cite{Timestamp,Xie2022Alignment} learns a video representation in an instructive video. However, these methods rely heavily on the annotations of temporal boundaries, \, i.e., the timestamps of sequential actions, which are usually difficult to be obtained due to the time-consuming human labeling in practice. A common but often overlooked scenario is that sequential videos usually occur accompanied with audio or text narrations, which show consistent steps with explanations. The rich text information describes the corresponding procedure in detail as shown in \cref{fig:sequence_video}, but they are usually not aligned with videos. Therefore, a question arises, \, i.e., whether it is possible to directly learn the video representation with unaligned text and video in a weakly supervised manner.

With the popularity of visual-language tasks, multi-modal learning has attracted growing attention and has been explored in a variety of areas, \, e.g., image classification \cite{kinetics, C3D}, object detection \cite{CAT-Det, detection}, and video understanding \cite{coca}. One of the most representative works is CLIP\cite{CLIP}. It has shown the potential of learning a powerful semantic representation from natural language supervision with a contrastive learning loss and the strong zero-shot generalization on the downstream tasks, such as text-video retrieval\cite{sun2022long, VLM}, action segmentation \cite{actbert}, multiple-choice videoQA \cite{bridgeformer, VLM} and action step localization\cite{LocalVTP}. VideoCLIP\cite{videoclip} presents a contrastive learning approach to pre-train a unified model with video-text pairs, and \cite{Timestamp} proposes a unified fully and timestamp-supervised framework for multi-model action segmentation. This provides us with an alternative for weakly supervised video representation learning. However, all these previous works are equipped with aligned texts and video frames \cite{Timestamp}, which is not existent in our weakly supervised setting. Thus, it is intractable to directly adapt the existing multi-modal video representation models to our task.  

To overcome the unalignment issue between text and video and learn a satisfactory video representation, we propose a weakly supervised video representation learning pipeline and introduce a multiple granularity contrastive loss to constrain the model, which takes full account of the pseudo temporal alignment between frames and sentences. To be specific, we first extract video and text features from a CLIP-based vision-language model, and a global contrastive loss is designed to constrain the complete video-paragraph alignment. It constrains that a video will be closer to the sequence of the texts describing it while far away from the rest of the texts, and vice versa. Secondly, we introduce a fine-grained contrastive learning loss, which encourages the frame sequences of representations to be more similar to the neighbor sentence representations than the remote sentences in the same paragraph. The intuition behind this constraint comes from a basic idea: \textit{if the $s_j$ is the corresponding sentence for frame $ h_i$, the corresponding sentence for frame $h_{i+1}$ is never before the $s_j$ in sequence}. Specifically, we take the probabilistic sample from the sentence-frame similarity metric. And we propose to apply the differentiable Gumbel-Softmax\cite{gumbel_softmax} tricks to generate predictions and propose three kinds of methods to generate the pseudo-labels that are based on the temporal relation of sentences in the temporal domain: 1) maximum-index sorting; 2) Viterbi algorithm \cite{viterbi}; 3) splitting. Finally, we calculate the Info-NCE contrastive loss based on the pseudo labels in order to guide the network to focus on the fine-grained action matching in sequential videos. %that regards pseudo labels as the positive samples. 

To evaluate the effectiveness of our weakly supervised video representation method, we conduct extensive experiments on two downstream tasks: video verification in procedures and text-to-video matching. The results of experiments show that our approach outperforms other baselines by a significant margin and also demonstrates the great generalization of our model.  

 We summarize our contributions in three folds:
\begin{itemize}
\item We propose a novel weakly supervised video representation learning pipeline with unaligned text for sequential videos, which can learn powerful and semantic video-text representations.
\item We design multiple granularity contrastive learning loss, including coarse-grained loss and fine-grained loss. Notably, we propose a novel method to implement the temporal alignment between frames and sentences. 
\item Our model also shows strong generalization ability to downstream tasks, such as video sequence verification for procedures in videos and text-to-video matching. 
\end{itemize}
