\section{Conclusions}\label{sec:conclusions}
In this paper, we propose a novel framework of weakly supervised video representation learning for sequential videos. Borrowing the multi-modal contrastive learning from CLIP, our method can learn video representation with unaligned text and video without relying on the accurate time-stamp level text-video annotation.
% A transformer that consists of image encoder and text encoder is designed to simultaneously learn frame-level features and the global video feature. 
We propose a multiple granularity loss where the video-paragraph contrastive loss constrains the matching between the whole video and the complete script, and a fine-grained frame-sentence contrastive loss constrains the matching between each action and its descriptions. We also propose to generate pseudo labels with temporal consistency in video and text. 
Experiments results show that our design is effective, and our method achieves state-of-the-art performance when transferred to downstream video sequence verification and text-to-video matching tasks.

\noindent \textbf{Acknowledgements}. 

% The work was supported by NSFC \#61932020, \#62172279, Science and Technology Commission of Shanghai Municipality (Grant No. 20ZR1436000), Program of Shanghai Academic Research Leader, and "Shuguang Program" supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission.

The work was supported by National Key R\&D Program of China (2018AAA0100704), NSFC \#61932020, \#62172279,  Science and Technology Commission of Shanghai Municipality (Grant No. 20ZR1436000), and â€œShuguang Program" supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission. 



