\appendix

% --- PDF will be split by an editor (e.g. macOS preview), so need to restart from page 1
% \setcounter{page}{9}

% --- repeat the title (AT: haven't found a more elegant way to do this...)
\twocolumn[
\centering
\Large
\textbf{Weakly Supervised Video Representation Learning \\with Unaligned Text for Sequential Videos} \\
\vspace{0.5em}Supplementary Material \\
\vspace{1.0em}
] %< twocolumn
\appendix

\section{Extra Experiment Studies}
In this section, we present additional ablation studies about our method, including the effects of batch size, the number of clips sampled per video, the approach of extracting paragraph-level language representation, and sequence align loss.

\subsection{Implementation Details}
% \label{sec:implementation}
We implement our method with PyTorch. The vision backbone we employ is the pre-trained CLIP vision encoder based on ViT-B\cite{vit}. And the model is initialized adopting Kaiming and Xavier uniform initialization for different layers\cite{kaiming,MAE}. In our module, the parameter of the vision backbone is unfrozen and finetuned when training. On the other hand, the language backbone is the pre-trained CLIP text encoder whose parameter is frozen totally. We split the raw video into 16 clips for a sequential video and randomly sample one raw frame from each clip in the training and uniformly sample frames in inference. The projection layer adopts a fully connected layer. The hidden layer dimension of transformer encoder\cite{vit} is 1024, and the depth is 2. The dimension of the video representations and paragraph representations is 512. The $\lambda_1$ in our model is equal to 1. 
The experiments are conducted on 4 NVIDIA 2080Ti GPUs with batch size 8. We adopt an AdamW optimizer\cite{adamw} with cosine annealing learning rate scheduler with a base learning rate of  $5\times 10^{-4}$, and weight decay 0.01. More implementation details should be seen in the supplementary materials.
And expect the experiment of sequence align loss to be conducted on the supervised sequence verification task, other experiments are conducted on the weakly supervised sequence verification task. We conduct all experiments on CSV dataset.

\subsection{Batch size}
To adapt to the change in batch size, we increase or decrease the learning rate exponentially. As \cref{tab:batchsize} shown, our method achieves the best performance when the batch size is equal to 16.
The larger the batch size, the more likely multiple videos of the same task will appear in the same mini-batch.
Due to the limitation of GPU memory, the largest batch size can be set as 8 if we unfreeze the vision backbone.
\begin{table}[htbp]
  \centering

    \begin{tabular}{c|c|c|c}
    \hline
    Method & Batch size & Frames & CSV \\
    \hline
    \multirow{5}[2]{*}{Ours} & 4     & 16    & 65.94 \\
          & 8     & 16    & 67.46 \\
          & 16    & 16    & 69.42 \\
          & 24    & 16    & 69.21 \\
          & 32    & 16    & 69.16 \\
    \hline
    \end{tabular}%
    \caption{Ablation studies of batch size on our proposed method}
    \label{tab:batchsize}%
\end{table}%
\subsection{Sampling}

While changing the frames of sampling per video, the training time is doubled with the increasing number of frames. In this ablation study, all models have been training no more than 100 epochs or 12 hours on two GPUs due to the limitation of computing resources. 

As \cref{tab:num_clip} shown, when frames are set to 16, our model achieves the best performance. It is worth noting that, with more training steps (about twice the training time), the performance of  $32$ frames will increase to 68.20. However, we choose $16$ as the default frame to balance batch size, number of frames, and training cost, we choose $16$ as the default frame. 

The significant reason for choosing sampling frames rather than video clips is the limitation of computational resources. Fine-tuning the full pre-trained backbone, such as VideoCLIP\cite{videoclip}, is expensive. Similarly, to balance the efficiency of the network and fairly compare our method with CAT\cite{SVIP}, we choose 16 frames as the same as CAT.

\begin{table}[htbp]
  \centering

    \begin{tabular}{c|c|c|c}
    \hline
    Method & Batch size & Frames & CSV \\
    \hline
    \multirow{4}[2]{*}{Ours} & 8     & 8     & 58.43  \\
          & 8     & 16    & 67.46  \\
          % & 8     & 24    & \color{red}{63.25}  \\
          & 8     & 32    & 65.64  \\
          & 8     & 48    & 64.40  \\
    \hline
    \end{tabular}%
      \caption{Ablation studies of the number of frames.}
  \label{tab:num_clip}%
\end{table}%

\subsection{Paragraph feature}
We design two ways to extract the feature of the paragraph. The one is concatenating all sentences into a paragraph description. Then we can obtain the paragraph-level representation by feeding the paragraph description into the language encoder. The other method is that feed individual procedure texts into the frozen language encoder to produce sentence representations and then obtain a paragraph-level representation by temporal mean pooling. The results shown in \cref{tab:pooling} illustrate that the method based on concatenation achieves better performance.

% We ablate mean pooling rather than concatenate all sentences to one paragraph to extract one language representation for one video. 
% The results shown in \cref{tab:pooling} present that pooling is not a good way to fusion sentence features compared with concatenating all sentences into one paragraph.  
\begin{table}[htbp]
  \centering
    \begin{tabular}{c|c|c}
    \hline
    Method & Paragraph feature & CSV \\
    \hline
    \multirow{2}[2]{*}{Ours} & pooling &67.07 \\
          & concat &67.46  \\
    \hline
    \end{tabular}%
      \caption{Ablation studies of the ways to extract paragraph features on our method.}
  \label{tab:pooling}%
\end{table}%concat

\subsection{Sequence alignment loss}
For a fair comparison, some adjustments have been made to the architecture of our model on the supervised sequence verification task. Specifically, following \cite{SVIP}, we apply the video sequence alignment mechanism to our model. Moreover, we also conduct experiments to investigate the effectiveness of using sequence alignment loss. We change the sequence align loss position to the last of the network. The results shown in \cref{tab:seq_loss} illustrate that sequence alignment loss $L_{\text{seq}}$ could restrict the model to learning a  better representation. 
% we ablate the sequence align loss $L_{\text{seq}}$ on supervised sequence verification task, which means we use classifier loss in training. \cref{tab:seq_loss} shows that using $L_{\text{seq}}$ has better performance.
\begin{table}[htbp]
  \centering

    \begin{tabular}{c|c|c}
    \hline
    Method & $L_{\text{seq}}$ & CSV \\
    \hline
    \multirow{2}[2]{*}{Ours} & \graycross &84.47  \\
          & \ding{51} &84.69  \\
    \hline
    \end{tabular}%
      \caption{Ablation studies of the sequence alignment loss on our method.}
  \label{tab:seq_loss}%
\end{table}%

\section{Gumbel-Softmax with Viterbi}
Due to the sum of the probabilities of each row cannot be greater than one and each probability value in a row should be the same, we simply set the value to $\frac{1}{N}$. 
As \cref{eq:viterbi_transition2} shown, we set each element value in the upper diagonal matrix to $\frac{1}{N}$ and others to zero to keep the path of probability will be a one-way path.
\begin{equation}
    A=
    \begin{bmatrix}
        \frac{1}{N} & \dots  & \frac{1}{N} \\
               & \ddots & \vdots \\
        0      &        & \frac{1}{N}\\
    \end{bmatrix}_{N \times N}
    \label{eq:viterbi_transition2}
\end{equation}
where $A$ represents the Transition matrix of Viterbi algorithm\cite{viterbi}.

\section{TSM module}
Following \cite{RepNet}, we add the Temporal Similarity Matrix (TSM) module with residual connection to our vision module. In this ablation study, we only use the task classification loss $L_\text{cls}$ instead of coarse-grained loss $L_\text{coarse}$ and fine-grained loss $L_\text{fine}$. As \cref{tab:tsm_type} shown, we verify different similarity distances of TSM and residual connection types. And the experiments indicate that the TSM module with residual connection will improve the model performance. 

However, as \cref{tab:add_tsm} shows, while we apply the TSM module to our method and train the model under weak supervision, the performance of the model degrades. 
It is reasonable that the model with the TSM module is not effective for language-video alignment tasks.
% We consider the reason is TSM has better performance works with $L_\text{cls}$ but not compatible with $L_\text{fine}$. 

\begin{table}[htbp]
  \centering

    \begin{tabular}{c|cc|c}
    \hline
    Method &  Dist & Residual & CSV \\
    \hline
    \multirow{5}[2]{*}{CLIP\cite{CLIP}+TE\cite{vit}+MLP} 
            & \graycross &\graycross &77.35  \\
    \cline{2-4}
            & L2    & add   & 77.42 \\
          & L2    & concat & 78.22 \\
          & Attn  & add   & 76.89 \\
          & Attn  & concat & 77.71 \\
    \hline
    \end{tabular}%
      \caption{Ablation studies of the different kinds of TSM module on the baseline.}
  \label{tab:tsm_type}%
\end{table}%

\begin{table}[htbp]
  \centering

    \begin{tabular}{c|c|c|c|c}
    \hline
    Method & $L_\text{fine}$ & $L_\text{coarse}$ & TSM   & CSV \\
    \hline
    \multirow{2}[4]{*}{Ours} & \ding{51} & \ding{51} & \graycross & 79.80 \\
\cline{2-5}          & \ding{51} & \ding{51} & \ding{51} & 76.00  \\
    \hline
    \end{tabular}%
      \caption{Ablation studies of the TSM on our proposed method.}
  \label{tab:add_tsm}%
\end{table}%
% \section{Qualitative results}
% \lorem{2}
\input{tables/classification}
\input{tables/limitation}
\section{Downstream tasks}
\subsection{Text-to-Video Matching}

We validate the performance of the video-language representations on text-to-video matching, which aims to find the correct video corresponding to a sequence of texts from a series of videos. Specifically, we train our model on the CSV dataset under weak supervision and test it on our proposed benchmark about text-to-video matching. We calculate the similarity between each video representation $V_i$ and paragraph representation $L$:
\begin{equation}
    d = dis(L,V_i)  \\
\end{equation}
where $dis(.,.)$ represents the normalized Euclidean distance. And $V_i$ represents $i_{th}(i \in [0,\dots,4])$ video representation.
At last, we select the text-video pair with the max similarity. 

\noindent \textbf{CSV-Matching}. 
To better evaluate the text-to-video matching, we rearrange the test set of CSV and propose a new scripted benchmark named CSV-Matching. 
It has 800 text-video pairs. Each text-video pair is composed of one sequence of text descriptions of procedures and five videos. All of the videos describe the same task but hold different procedures. There is only one correct video matching the text descriptions in each pair. CSV-test dataset contains 5 tasks and each task has 5 kinds of different procedures. We random select one kind of video from each procedure to compose one pairs. The benchmark and split script will be available.

\subsection{Video Classification}

To demonstrate our method's transfer ability, we evaluate models in the downstream video classification task. We re-divided the CSV dataset for video classification task. The train set contains 689 videos and test set contains 185 videos.
On the re-divided CSV test dataset (CSV-CLS), we evaluate representations of models with linear probing, which were pre-trained under weak supervision. As \cref{tab:classification} shown, our method achieves better performance in the video classification task. The benchmark and split script will be available.


\section{Limitations}

% \label{sec:limitation}
While our method performs well on the major part of the data, there still are some failure cases. In realistic sequential videos, sub-actions are often repeated. In that case, there are multiple sentences with high similarity to a frame. It could mislead the model to generate biased pseudo-labels, which will lead to the deterioration of performance. For example, the occurrence of a large number of repetitive actions repetitive action might hidden achieving further performance. 

The intuition of our fine-grained contrastive loss comes from a basic idea: \textit{if the $s_j$ is the corresponding sentence for frame $ h_i$, the corresponding sentence for frame $h_{i+1}$ is never before the $s_j$ in sequence}. Due to a large number of repetitive actions, it might be difficult to achieve further performance. However, this method is still promising. As \cref{tab:limitation} shown, we have re-divided the COIN-SV test dataset based on whether existing repetitive actions in videos or not, which are COIN-SV-Rep (675 video pairs) and COIN-SV-NoRep (325 video pairs). In the original COIN-SV test dataset, there are 1000 video pairs for sequential video verification, built by 328 videos containing repetitive actions and 123 videos that do not. The results show that although the occurrence of repetitive actions will cause the deterioration of performance, our method can still achieve better results than other baselines. Moreover, the results conducted by our method may reflect the bias from the dataset. 