\section{Related Works}\label{sec:related}

\noindent \textbf{Sequential Video}. The same task described in videos may consist of several sequential sub-actions in different orders for a sequential video. Sequential videos are generally accompanied by explanations such as audio or caption. Various kinds of studies related to sequential videos are now in the ascendant. For example, COIN\cite{coin}, Diving\cite{Diving}, CSV\cite{SVIP}, EPIC-KITCHENS\cite{epic-kitchens}, IKEA-ASM\cite{ikea} and Assembly101 \cite{assembly101} provide videos composed by multiple sequential actions and the corresponding step annotations.  \cite{assembly101} proposes a large-scale multi-view video dataset for understanding procedural activities, which is beneficial for the whole community.  \cite{SVIP} defines the pioneering sequence verification task and designs a method based on the alignment of video pairs. However, the method is seriously dependent on video pairs of the same tasks. \cite{procedureactivity} learns to recognize procedural activities in sequential videos with distant supervision\cite{zeng2015distant,mintz2009distant}. \cite{set_supervised} propose an action segmentation method using the set-supervised method for sequential videos.
\cite{kumar2022unsupervised} employs temporal optimal transport to generate pseudo labels to complete joint representation learning and online clustering for sequential video alignment.
D$^3$TW\cite{chang2019d3tw} aligns clips and transcripts with differentiable continuous relaxation.

\noindent \textbf{Vision-text Multi-modality Learning}. Vision-text multi-modality\cite{howto100m, sun2019learning, CLIP, videobert, actbert, LocalVTP, Xie2022Alignment, actionclip, wang2022long} has attracted increasing attention in computer vision communities over the recent year. One of the most representative works is CLIP\cite{CLIP}, which is able to learn a powerful visual representation from natural language supervision with contrastive learning loss. Due to the strong zero-shot generalization ability of the method, a large number of follow-up works have been proposed \cite{X-CLIP, videoclip, LocalVTP, Xie2022Alignment, actionclip, sun2022long,clip-event}. VideoCLIP\cite{videoclip} presents a contrastive approach to pre-train a unified model with video-text pairs. X-CLIP\cite{X-CLIP} effectively expands the pre-trained language-image model to video domains based on a cross-frame attention mechanism. However, these methods heavily rely on strong data augmentation and a large batch size. For downstream tasks, LocVTP\cite{LocalVTP} shows its transfer potentials on localization-based and retrieval-based tasks. CLIP4Clip\cite{clip4clip} uses the pretrained CLIP as our backbone to solve the video clip retrieval task from frame-level input. \cite{bridgeformer} bridges video-text retrieval with multiple-choice questions. LF-VILA\cite{sun2022long} applies a multi-modality temporal contrastive loss to implement long-form video-language pre-training, which heavily relies on the timestamp annotations of clip-sentence pairs.

\noindent \textbf{Video Representation Learning}. Learning good video representations has been heavily investigated in the literature. 3D convolution neural networks (3D-CNNs) are originally considered to learn deep video representations\cite{kinetics, tran2018closer, slowfast}. However, 3D-CNNs are limited to capturing long-term dependencies on the temporal domain with their insufficient receptive field. Due to the ability to capture long-term dependency of the self-attention mechanism\cite{transformer}, vision transformer models\cite{vit, swin, video-swin, MViT, frame-sequence, hu2022transrac, timesformer} show competitive performances against 3D-CNNs in video representation learning. Following the ViT\cite{vit}, many related works emerge. TimeSformer\cite{timesformer} designs different self-attention schemes in the temporal-spatial domain. Video Swin-Transformer \cite{video-swin} adopts the local attention in non-overlapping shifted windows to lead to a better speed-accuracy trade-off. Over recent years, weakly supervised or self-supervised learning\cite{frame-sequence, MPNet} is popular for learning better video representation. Following SimCLR\cite{SimCLR}, \cite{frame-sequence} introduces a self-supervised contrastive transformer framework to learn frame-wise action representations. \cite{cross-modal-rl} proposes a transformer-based cross-modal architecture for zero-shot action recognition. Previous works mainly focus on short-form simple video representation, whereas representation learning of sequential video is underexplored.
