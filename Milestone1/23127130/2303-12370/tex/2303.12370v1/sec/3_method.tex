\section{Method}\label{sec:method}
\input{fig/3_method/overview/framework}

In this section, we first present the overall architecture of the proposed framework in \cref{sec:overview}. Then we explain the vision representation module and language representation module in \cref{sec:vision-language}, followed by the designed multiple granularity contrastive learning module in \cref{sec:loss1} and \cref{sec:loss2}.

\subsection{Overview}
\label{sec:overview}
\cref{fig:overview} displays the overview of our framework. Our framework consists of three parts: a vision representation module, a language representation module, and a multiple granularity contrastive learning module. In the vision representation module, which shows in the right part of the figure, we sample frames from an untrimmed sequence video as input and extract visual features with the pre-trained vision encoder (unfrozen). After that, we concatenate the visual feature and pass them into the Transformer encoder. The Transformer encoder implements the cross-frame communication with self-attention and outputs the frame representations, following ViT\cite{vit}. Additionally, the results from Transformer encoder are then passed through the MLP module to integrate the frame representations and obtain the video representation. In the left language representation module, a collection of text descriptions of procedures and the description of the entire video pass into the pre-trained language encoder (frozen) separately, then we can obtain the sentence representations and a paragraph representation. More explanation about the aforementioned modules is in \cref{sec:vision-language}. Finally, we introduce multiple granularity contrastive loss to restrict learned representations in cross-model space.

\subsection{Vision-Language Modules}
\label{sec:vision-language}
As illustrated in \cref{fig:overview}, multi-level video representation and language representation are produced by the vision module and language module, respectively.

\noindent \textbf{Vision module}.
Following\cite{TSN}, given an untrimmed sequence video, we uniformly split the raw video into $N$ clips and randomly sample one frame per clip to form a sequence of $N$ frames, $X=\{x_1, x_2, \dots, x_N\}$. Then we feed the frame sequence $X$ into the pre-trained vision encoder $E_v$ to produce a sequence of feature maps $\{f_1, f_2, \dots, f_N\}$. This process can be denoted as $f_i = E_v(x_i), i\in [1, 2, \dots, N]$. After that, we prepend a learnable embedding $x_{cls}$ to the sequence of features, called $[class]$ token \cite{vit}. Then as \cref{eq:framerepresentations} shown, our method learns the frame representations by utilizing the transformer encoder (TE) to embed temporal and context information into frame representations $H =\{h_1, h_2, \dots, h_N\}$. 
% the transformer encoder, named TE, implements the cross-frame communication with self-attention to obtain the frame representations $H =\{h_1, h_2, \dots, h_N\}$.
 
\begin{equation}
    H = \text{TE}(\left[x_\text{cls}, f_1, f_2, \dots, f_N\right] + e^\text{pos})  \\
\label{eq:framerepresentations}
\end{equation}
where  $[. , .]$ concatenates the features of frames and $[class]$ token. And $e^\text{pos}$ represents the temporal position embedding of sequence.

At last, the MLP module, which consists of a full connection layer, takes all frame representations $H$ as input and outputs a video representation $v$ as follows:
\begin{equation}
    v  = \text{MLP}(H)  \\
\label{eq:videorepresentation}
\end{equation}


\noindent \textbf{Language module}. Specifically in our model, given a sequence of $K$ text descriptions of procedures $ T = \{t_1, t_2, \dots, t_K\}$, we first feed individual procedure texts into the frozen pre-trained language encoder $E_l$ to produce sentence representations $S=\{s_1, s_2, \dots, s_K\}$. The process can be denoted as $s_i = E_l(t_i), i \in [1,2,..K]$.

In the meantime, we combine the sequence of text descriptions of procedures $ T $ into a single text description of the entire video. Then, the pre-trained language encoder $E_l$ extract a paragraph-level language representation $l$ as follows:
\begin{equation}
    l =E_l(\left[t_1, t_2, \dots, t_K\right]) \\
\label{eq:paragraphrepresentation}
\end{equation}
where  $[. , .]$ represents simply the sequential concatenation of strings.
\vspace{-5pt}
\subsection{Coarse-grained Contrastive Loss}
\label{sec:loss1}


We first conduct contrastive learning at the video-paragraph level. Specifically, through the vision-language module that is explained in \cref{sec:vision-language}, we obtain a video representation $V$ and paragraph representation $L $, where $V, L \in \mathbb{R}^{1 \times D}$. Then use one batch of data, $V = \{v_1,v_2,\dots,v_N\}$, $L =\{l_1,l_2,\dots,l_N\}$, to calculate the loss.

After that, we formulate the global video-paragraph alignment into the standard contrastive framework\cite{CLIP} based on InfoNCE loss\cite{InfoNCE} as follows:
% \vspace{-0.5em}
% \begin{s\vspace{-1.5em}mall}
\begin{equation}
    L_{\text{InfoNCE}}(V,L)=-\frac{1}{N} \sum_{i=1}^{N} \log\frac{\exp{(\varphi(v_i,l_i)/\tau)}}{\sum_{j=1}^{N} \exp{(\varphi(v_j,l_j)/\tau})} \\
\label{eq:infonce}
\end{equation}
% \end{small}
\vspace{-10pt}
\begin{equation}
    \varphi(v_i,l_i) = \frac{v_i}{\left\lVert v_i \right\rVert} \cdot \frac{l_{i}^{T}}{\left\lVert l^T \right\rVert}
\label{eq:cos_sim}
\end{equation}
where $\tau$ is the temperature parameter optimized during training\cite{CLIP}. And $\varphi(.,.)$ represents the cosine similarity function, and $N$ is the number of video-text pairs. The $L_\text{InfoNCE}$ represents the InfoNCE loss.

Last, as shown \cref{eq:global}, we calculate symmetrically video-text and text-video loss by \cref{eq:infonce} to obtain the coarse-grained contrastive loss $L_\text{coarse}$:
\begin{equation}
    L_\text{coarse}= L_{\text{InfoNCE}}(V,L) + L_{\text{InfoNCE}}(L,V) \\
\label{eq:global}
\end{equation}

Showing in the upper left of \cref{fig:overview}, the coarse-grained global contrastive loss $L_\text{coarse}$ restricts the representation in the cross-model latent space with video-paragraph level supervision.



\subsection{Fine-grained Contrastive Loss} \label{sec:loss2}

\input{fig/3_method/gumbel/gumbel.tex}
Due to the lack of frame-level annotations, there is no annotation to locate the start frame and end frames per action. A frame can't know its correct corresponding sentence.
To overcome this problem, we propose an essential hypothesis based on the temporal relation between sentences and frames: if  $s_j$ is the corresponding sentence representation for the frame representation $ h_i$, the sentence representation for frame representation $h_{i+1}$ should be in the set of  $\{s_j,s_{j+1},s_{j+2},\dots, s_{K} \}$ and never in the set of  $\{s_1,s_{2},\dots, s_{j-1} \}$. The visualization of fine-grained contrastive loss can be seen in \cref{fig:gumbel}.

Specifically, in our model, we first obtain the sequence of frame-level sequences of representations $H$ and sequence of sentence-level representations $S$ through the vision-language module. As \cref{eq:fine2} shows, we symmetrically calculate the fine-grained contrastive learning loss, named $L_{fine}$, to achieve frame-sentence alignment. 
\begin{equation}
    \begin{aligned}
        L_{fine} = &\textit{CE}(\psi_{\text{preds}}(H,S), \phi_{\text{pseudo}}(H,S)) \\
                    &  +  \textit{CE}(\psi_{\text{preds}}(S,H), \phi_{\text{pseudo}}(S,H)) \\
    \end{aligned}
    \label{eq:fine2}
\end{equation}
where \textit{CE} is the Cross-Entropy loss. We use $\psi_{\text{preds}}$ to predict the most related sentence $s_j$ with one frame, where $s_j \in S$. The $\phi_{\text{pseudo}}$ could utilize the probability distribution of prediction and the similarity matrix of $H$ and $S$ to generate the pseudo labels as ground truth.
Then as \cref{eq:fine2} shown,  we calculate $L_\text{fine}$ by the \textit{CE} loss of the prediction and pseudo labels. 
And we separately introduce two methods of $\psi_{\text{preds}}$ and $\phi_{\text{pseudo}}$ in \cref{sec:sorting} and \cref{sec:viterbi}. The method of Gumbel-Softmax with splitting is shown in \cref{sec:ablation_pseudo}.

\subsubsection{Gumbel-Softmax with Sorting} \label{sec:sorting}

We first use \cref{eq:cos_sim} to calculate the similarity matrix between the frame representations $H$ and its sentence representations $S$. And we obtain the first prediction by \cref{eq:preds_gumbel}.
\vspace{-0.5em}
\begin{equation}
    \psi_{\text{preds}}(H,S) = \text{Gumbel-Softmax}(\varphi(H,S))
    \label{eq:preds_gumbel}
\end{equation}
% \vspace{-0.5em}
where Gumbel-Softmax is the straight-through Gumbel-Softmax function\cite{gumbel_softmax}. We utilize the Gumbel-Softmax to ensure the dispersed sampling from the original distribution can be calculated for the gradients in the backward pass.
Then, we get the maximum index through $\arg\max$ and sort the maximum-index list to an increasing order to generate pseudo labels. We regard them as the ground truth, which shows in \cref{fig:1b} in blue.
Finally, we finish the first kind of $\psi_{\text{preds}}$ and $\phi_{\text{pseudo}}$  by \cref{eq:preds_gumbel,eq:pseudo_gumbel}.
\begin{equation}
% \phi_{\text{pseudo}}(H,S) = \text{sort} \left[ \mathop{\arg\max}(\psi_{\text{preds}}(H,S)) \right] 
\phi_{\text{pseudo}}(H,S) = \text{sort} \left[ \mathop{\arg\max}_{i\in[1,K]}(\psi_{\text{preds}}(H,S)_{N\times K}) \right] 
\label{eq:pseudo_gumbel}
\end{equation}

\subsubsection{Gumbel-Softmax with Viterbi} \label{sec:viterbi}
% \input{tables/baselines}

Following the Viterbi algorithm\cite{viterbi}, it could generate the maximum a posteriori probability estimate, called the Viterbi path. The original Viterbi algorithm needs two important matrices: transition matrix and emission matrix. As shown in \cref{eq:viterbi_emission}, we use the similarity of the language and vision features as the emission with the shape $[N, K]$, where $N$ means the number of sampled frames and $K$ is the total number of its labels. Specifically in our method, as \cref{eq:viterbi_transition}shows, we use one upper triangular mask matrix as the transition matrix to limit the path of probability, which could make sure the way won't go back. Based on the Viterbi path (shown in \cref{fig:1c}), we obtain the pseudo-labels by \cref{eq:viterbi}. Different from our method using Viterbi algorithm to generate pseudo labels, \cite{review1_1,review1_3} apply Viterbi decoding prediction, and activities have constant action orders. More details about the Viterbi algorithm can be seen in supplementary materials. 
\vspace{-1em}
\begin{equation}
    \text{Transition matrix:} A=
    \begin{bmatrix}
        \frac{1}{n} & \dots  & \frac{1}{n} \\
               & \ddots & \vdots \\
        0      &        & \frac{1}{n}\\
    \end{bmatrix}_{N \times N}
    \label{eq:viterbi_transition}
\end{equation}
\vspace{-10pt}
\begin{equation}
    \text{Emission matrix: } B=\varphi(H,S)
    \label{eq:viterbi_emission}
\end{equation}
\vspace{-10pt}
\begin{equation}
    \phi_{\text{pseudo}}(H,S) = \text{Viterbi}(A, B) 
    \label{eq:viterbi}
\end{equation}
% \input{tables/baselines}

\subsubsection{Training Loss}

In conclusion, we train our module with the combination of the  proposed coarse-grained contrastive loss and fine-grained contrastive loss:
\vspace{-0.5em}
\begin{equation}
    L= L_{\text{coarse}} + \lambda_1 L_{\text{fine}} \\
\label{eq:lossVR}
\vspace{-0.5em}
\end{equation}
% \vspace{-0.5em}
where $\lambda_1$ represents the weight of fine-grained contrastive loss.

