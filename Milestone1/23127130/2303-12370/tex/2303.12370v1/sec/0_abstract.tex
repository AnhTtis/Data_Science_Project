\begin{abstract}
Sequential video understanding, as an emerging video understanding task, has driven lots of researchers' attention because of its goal-oriented nature. This paper studies weakly supervised sequential video understanding where the accurate time-stamp level text-video alignment is not provided. We solve this task by borrowing ideas from CLIP. Specifically, we use a transformer to aggregate frame-level features for video representation and use a pre-trained text encoder to encode the texts corresponding to each action and the whole video, respectively. To model the correspondence between text and video, we propose a multiple granularity loss, where the video-paragraph contrastive loss enforces matching between the whole video and the complete script, and a fine-grained frame-sentence contrastive loss enforces the matching between each action and its description. As the frame-sentence correspondence is not available, we propose to use the fact that video actions happen sequentially in the temporal domain to generate pseudo frame-sentence correspondence and supervise the network training with the pseudo labels. Extensive experiments on video sequence verification and text-to-video matching show that our method outperforms baselines by a large margin, which validates the effectiveness of our proposed approach. Code is available at \url{https://github.com/svip-lab/WeakSVR}.

% Code will be available \footnote{\url{https://github.com/svip-lab/Weakly-Supervised-Video-Representation-Learning-with-Unaligned-Text-for-Sequential-Videos}}.

%Sequential videos generally are composed by multiple consistent steps with explanations such as audio or narrations.
%Due to the large cost of manually annotating temporal boundaries, it is challenging to learn a discriminative video representation under weakly supervision. In recent years, vision-language multi-modality has attracted growing attention with the aim to learn transferable vision-language representations and hold strong zero-shot generalization on the downstream tasks. Previous works typically simply align video and paragraph using a vanilla contrastive loss but ignore the alignment between frames and sentences. In this paper, we propose a novel weakly supervised video representation learning framework with multi-grained vision-language contrastive learning loss: coarse-grained contrastive loss and fine-grained contrastive loss. We verify the effectiveness of them through comprehensive ablation studies. We conduct experiments on considered datasets and the results show that our method outperform other baselines. We also conduct extensive experiments to demonstrate the great zero-shot generalization on two downstream tasks: video sequence verification and text-to-video matching.


\end{abstract}