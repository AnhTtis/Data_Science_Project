
\section{Experiments}
\label{sec:experiments}
% \input{tables/baselines}

% \input{tables/sequence_verification}

In this section, we first introduce the implementation details, evaluation benchmarks and evaluation metrics in \cref{sec:datasets}. The experiments to verify the effectiveness of baselines for video-text representation learning are shown in \cref{sec:evaluation}. In addition, we also transfer our proposed framework to downstream sequence verification in \cref{sec:Verification} and text-to-video matching tasks in \cref{sec:matching}. 
% Moreover, we proposed a new benchmark for the text-to-video matching task.

\subsection{Experimental Details}
\noindent \textbf{Implementation Details}. 
\label{sec:implementation}
The vision backbone we employ is the pre-trained CLIP vision encoder based on ViT-B\cite{vit}. And the model is initialized adopting Kaiming and Xavier uniform initialization for different layers\cite{kaiming,MAE}. In our module, the parameter of the vision backbone is unfrozen and finetuned when training. On the other hand, the language backbone is the pre-trained CLIP text encoder whose parameter is frozen totally. More implementation details can be seen in supplementary materials. 

\noindent \textbf{Datasets}. \label{sec:datasets}
We conduct experiments on the datasets COIN-SV, Diving-SV and CSV. COIN-SV is rearranged from COIN and composed of 36 tasks that contain more than 20 comprehensive instructional videos in the training dataset. Diving-SV is rearranged from Diving and contains 48 kinds of diving competition videos. And CSV\cite{SVIP} includes 45 procedures for training and 25 procedures for testing. 
In these datasets, all kinds of videos in the test set are unseen in the training set.
% {\color{red}{In these datasets, all of the videos in the testing set are unseen in the training set.}}

\noindent \textbf{Testing phase}.
During inference, we apply the method that distinguishes positive pairs from negative pairs to evaluate the quality of learned video representations. Specifically in this paper, we calculate the normalized Euclidean distance between two video representations $v_1$ and $v_2$ in the same video pair:
\begin{equation}
    d = dis(v_1,v_2)  \\
\end{equation}
\vspace{-10pt}
\begin{equation}
    y=\begin{cases}
    1, d \leq \tau \\
    0, otherwise \\
    \end{cases}
\label{eq:logist}
\end{equation}
where $dis(.,.)$ means the $\ell2$-normalization Euclidean distance function. $\tau$ is a threshold to decide whether the sequences are consistent. $y = 1$ means the two sequences of videos are consistent, otherwise inconsistent.

\noindent \textbf{Evaluation Metrics}. 
We adopt the Area Under ROC Curve (\textbf{AUC}) as the measurement for all of our experiments, which is commonly used to evaluate the performance in the field of anomaly detection \cite{mist} and face verification\cite{arcface}. Higher AUC means better performance.

\subsection{Comparison of baselines}
\label{sec:evaluation}
Under weak supervision, the only annotations we know are the text descriptions of procedures, but the timestamps of actions and video task classification are unknown. The results of weakly supervised video sequence verification are shown in \cref{tab:baseline}. We compare our method with other baselines, including 1) MIL-NCE\cite{MIL-NCE}. 2) CAT, we change the SVIP\cite{SVIP} model architecture and add a text encoder to adapt to this task. 3) VideoSwin+MLP, we adopt the video swin transformer\cite{video-swin} as the vision encoder to extract frame features. 4) CLIP+Transformer Encoder+Pool. 5) CLIP+Transformer Encoder+MLP. To adapt to the task, we apply the CLIP text encoder as the text encoder of baselines except for MIL-NCE. Other methods but ours only calculate the coarse-grained contrastive loss.
\input{tables/baselines}
% The results shown in \cref{tab:baseline} illustrate that multiple granularity contrastive learning can learn discriminative video representations under weak supervision. 
The results in \cref{tab:baseline} demonstrate that multiple granularity contrastive learning is effective for learning discriminative video representations under weak supervision.

\subsection{Sequence Verification}
\label{sec:Verification}
\input{tables/sequence_verification}

Following the setting of sequence verification \cite{SVIP}, we know the classification of videos but yet do not know the timestamp of actions. 
% We conduct experiments and train models on our considered dataset.
% {\color{red}{We train models and conduct experiments on our dataset.}}
The testing results on sequence verification compared to other methods are shown in the \cref{tab:sequence_verification}. 
% In the setting of sequence verification for procedures in videos, we can use class information. 
We can use class information for sequence verification of procedures in videos.

For a fair comparison, some adjustments have been made to the architecture of our model in this task setting. 
Specifically, we add a classification layer on the top of the video representation and the classification loss to our model. Besides, we apply the adjusted video sequence alignment mechanism by ours and train with pair data that are the same as SVIP\cite{SVIP}.
This adjusted model is named "Ours". In addition, we also compare with some state-of-the-art methods\cite{SVIP,swin,TRN} of sequence verification and change some video-language pre-trained model \cite{howto100m} accordingly to adapt to this task. Weakly supervised means no classification information of tasks.
To clarify the improvements from technical differences, we replace the visual backbone of CAT\cite{SVIP} with CLIP-ViT\cite{CLIP} to form CLIP+TE+MLP. Then, we improve performance by adjusting network structure, e.g., the position of SEQ loss.   
Observed \cref{tab:sequence_verification}, our model outperforms them by a notable margin on all the considered datasets. The results also demonstrate that the fine-grained contrastive loss we proposed enforces the model to learn more discriminative representations.  The results of our weakly supervised model, which  surpasses other supervised methods, demonstrate our model's excellent performance.

\subsection{Text-to-Video Matching}
\label{sec:matching}
\noindent \textbf{Setting}.
We validate the performance of the video-language representations on text-to-video matching, which aims to find the correct video corresponding to a sequence of texts from a series of videos. Specifically, we train our model on the CSV dataset under weak supervision and test it on our proposed benchmark about text-to-video matching. 
% It is notable that this task verifies if the model has learned semantic and generalized video representation compared to the video sequence verification task.
This task evaluates the model's ability to learn semantic and generalized video representations.

\noindent \textbf{Benchmark}.
To better evaluate the text-to-video matching, we rearrange the test set of CSV\cite{SVIP} and propose a new scripted benchmark, named \textbf{CSV-Matching}. It has 800 text-video pairs. Each text-video pair is composed of one sequence of text descriptions of procedures and five videos. All of the videos describe the same task but hold different procedures. There is only one correct video matching the text descriptions in each pair. More details about the text-to-matching benchmark will show in the supplementary materials.


The text-to-video matching results in \cref{tab:matching} indicate  that our method has the best performance. And due to data of CSV-Matching being unseen when training, it shows that our method has a more powerful generalization ability. 
\input{tables/matching}