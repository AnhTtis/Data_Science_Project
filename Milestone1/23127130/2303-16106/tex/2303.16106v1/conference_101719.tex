\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{diagbox}
% updated with editorial comments 8/9/2021

\usepackage{orcidlink}

\begin{document}

\title{Common Subexpression-based Compression and Multiplication of Sparse Constant Matrices}

%\title{Multiplication of Sparse Constant Matrices Compressed by Utilizing Common Subexpressions}

\author{Emre Bilgili \orcidlink{0000-0003-2341-5180 } , Arda Yurdakul \orcidlink{0000-0001-7132-0042}, \textit{Computer Engineering Department, Bogazici University}}
% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

 
 
%  \documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% % The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
% \usepackage{algorithm2e}
% \usepackage{graphicx}
% \usepackage{textcomp}
% \usepackage{xcolor}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% \begin{document}

% \title{Multiplication of Sparse Constant Matrices Compressed by Utilizing Common Subexpressions 
% % {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
% % \thanks{Identify applicable funding agency here. If none, delete this.}
% }

% \author{\IEEEauthorblockN{Emre Bilgili}
% \IEEEauthorblockA{\textit{Computer Engineering Department} \\
% \textit{Bogazici University}\\
% Istanbul, Turkey \\
% emre.bilgili@boun.edu.tr}
% \and
% \IEEEauthorblockN{Arda Yurdakul}
% \IEEEauthorblockA{\textit{Computer Engineering Department} \\
% \textit{Bogazici University}\\
% Istanbul, Turkey \\
% yurdakul@boun.edu.tr}
% }

\maketitle
%Matrix multiplication is performed on compressed arrays. 
\begin{abstract}
In deep learning inference, model parameters are pruned and quantized to reduce the model size. Compression methods and common subexpression (CSE) elimination algorithms are applied on sparse constant matrices to deploy the models on low-cost embedded devices. However, the state-of-the-art CSE elimination methods do not scale well for handling large matrices. They reach hours for extracting CSEs in a $200 \times 200$ matrix while their matrix multiplication algorithms execute longer than the conventional matrix multiplication methods. Besides, there exist no compression methods for matrices utilizing CSEs. As a remedy to this problem, a random search-based algorithm is proposed in this paper to extract CSEs in the column pairs of a constant matrix. It produces an adder tree for a $1000 \times 1000$ matrix in a minute. To compress the adder tree, this paper presents a compression format by extending the Compressed Sparse Row (CSR) to include CSEs. While compression rates of more than $50\%$ can be achieved compared to the original CSR format, simulations for a single-core embedded system show that the matrix multiplication execution time can be reduced by $20\%$. %for a given $100 \times 100$ matrix multiplication compared to the state-of-the-art CSE methods. The storage size of the sparse matrices is also reduced by more than half in the experiments compared to the CSR format.

% Importance of the execution time, resource and energy costs of deep learning applications is increasing as the demand for their deployment on low-cost embedded devices is growing. These applications contain constant matrix multiplications, which has been studied for a long time. Current research aims to reduce the implementation cost of trained models by pruning and quantizing the weights. Pruning generates sparse matrices compressed into one-dimensional arrays without a loss. Quantization reduces the size of operations. Further utilization can be achieved with common subexpression elimination (CSE) as quantization increases the similarity in matrix weights. However, the state-of-the-art CSE methods do not scale well for the large matrices. They reach hours for extracting common subexpressions of a $200 \times 200$ binary matrix. As a remedy to this problem, a random search-based algorithm is proposed in this paper to extract common subexpressions in the column-pairs of a constant matrix. It produces an adder tree for a $1000 \times 1000$ matrix in a minute. To compress the adder tree, this paper presents a compression format by extending the Compressed Sparse Row (CSR) to include common subexpressions. Simulations for a single-core embedded system show that the latency is reduced by $80\%$ for a given $100 \times 100$ matrix multiplication compared to the state-of-the-art CSE methods. The storage size of the sparse matrices is also reduced by more than half in the experiments compared to the CSR format.
\end{abstract}

\begin{IEEEkeywords}
common subexpression elimination, compressed sparse row format, cpu simulation
\end{IEEEkeywords}

\section{Introduction}
% Most deep learning inferences perform a series of Constant Matrix Multiplication (CMM). Various improvements have been developed to speed up the process and reduce power consumption \cite{b1}. In one approach, the constant matrices are pruned at the cost of some accuracy loss. Then, the resulting sparse matrices are compressed to eliminate the processing with zero operands \cite{refMultilevelCSR}. Quantization also reduces the number of distinct elements \cite{b2}. The number of duplicate elements increases in this way. Common patterns largely exist in binary and ternary matrices \cite{refPhDThesis}. CSE algorithms are applied to remove the redundant operations. Besides, the sparse matrices are recorded into several one-dimensional arrays, and matrix multiplication is performed by processing the arrays \cite{b3}.
Tiny machine learning (TinyML) mainly targets on-device data analytics on extremely resource-constrained embedded platforms \cite{refTiny}. Most deep learning (DL) inferences perform a series of Constant Matrix Multiplication (CMM). Several methods have been proposed to speed up the process and reduce power consumption \cite{refDLHardware}. In one approach, the constant matrices are pruned at the cost of some accuracy loss. Then, the resulting sparse matrices are compressed to eliminate the processing with zero operands \cite{refMultilevelCSR}. They are recorded into several one-dimensional arrays, which are used in CMM \cite{refSparseMatrix}. Another approach utilizes quantization to reduce the number of unique entries and increase the number of duplicate elements. %Common patterns largely exist in binary and ternary matrices \cite{refPhDThesis}. 
Then, common subexpression (CSE) elimination algorithms are applied to remove the redundant operations \cite{refPowerOfTwo}.

Existing CSE extraction methods are not utilized to compress sparse matrices. %generate one-dimensional compression formats. 
Hence, though they greatly reduce the number of operations, a resulting representation is beneficial as long as it is implemented as an accelerator \cite{refCSEDSP}. As a CSE-reduced CMM still contains zeroes, its implementation is inefficient on a resource-constrained embedded platform. Based on this observation, this paper proposes a CSE extraction method and its CSE-reduced constant matrix compression format for efficient CMM implementations. The method relies on a heuristic-based search algorithm. The search space is limited to only size-of-two CSEs to reduce the extraction time. The lossless compression method is adapted from Compressed Sparse Row (CSR) format \cite{refSparseMatrixCSR}. %to reduce storage size and CMM latency. 
The proposed method compresses significantly more than CSR when the matrix weights are limited to a few numbers. CMM time also reduces compared to CMM with CSR. %Six one-dimensional arrays are produced by the proposed method. Processing them performs the matrix multiplication without data loss.

The paper is organized as follows. The next section summarizes the implementation styles of DL inferences. Section III describes the proposed method. The experiments are presented and discussed in Section IV. The final section concludes the work.

%The applied compression format is implemented on customizable hardware Field-Programmable Gate Arrays (FPGA) and low-end devices. In addition, the matrix is quantized to process on smaller execution units.

\section{Related Work}
% The studies to improve deep learning applications can be grouped into two categories. One group implies increasing the accuracy. The models are enlarged, and more complex functional models are introduced \cite{b4}. The models are upgraded to improve the accuracy, but the upgrades increase the costs of the applications at the same time.

% The studies in the second group aim to reduce the implementation costs of the trained models. The latency, power consumption and size of an inference model are minimized while the accuracy is kept above a certain threshold. The solutions for reducing the costs may be gathered under four topics. Firstly, the models are analyzed to be reshaped. The large layers are approximately compressed or decomposed into a set of small layers to remove the redundant operations and reduce the size \cite{b5}.

Most studies targeting low-cost platforms aim to reduce the latency, power consumption and size of the inference while keeping the accuracy above a certain threshold. The solutions for reducing the costs may be grouped into four. The first group studies reshaping. The large layers are approximately compressed or decomposed into smaller ones to remove the redundant operations and reduce the size \cite{b5}. Secondly, some matrix elements are set to zero during the training step or after the weights are produced \cite{b6}. Pruning reduces each matrix density by a  predefined ratio. Thirdly, the weights are quantized to reduce the storage \cite{b7}. If the hardware contains the processing unit for the target data type, the latency is also reduced. %Otherwise, the quantization operation still continues to be beneficial as the size of the weights is reduced.
The fourth group compresses the pruned matrices to ignore zero elements \cite{b8}. These methods store the non-zero elements into several one-dimensional arrays. The compression is lossless. Moreover, its two-dimensional form can be reconstructed during CMM by directly processing the one-dimensional arrays to produce the result vector. 

CSE extraction is a widely studied topic in circuit minimization. Both exact and approximate methods exist since it is an NP-complete problem \cite{refYurdakulJVLSI99}. Two efficient algorithms introduced in \cite{b9} and \cite{b10} are used to accelerate the deep learning inferences on hardware \cite{refUnrollingTNN, refMicroAI}. % $\leftarrow$ BU YÖNTEMLERİN KULLANILDIĞI DL UYGULAMALARINI REFERANS OLARAK KOYALIM. %They attempt to reduce the area and latency in the circuit implementation. 
Both methods use the same notation to express CSEs: a row-column pair is appended to the matrix for each CSE. As the number of CSEs increases, the matrix expands in both dimensions, resulting in a longer CSE generation time. Hence, the run-time of these methods does not scale well for large matrices. Besides, the size of the resulting matrix requires a longer processing time and bigger storage on a single-core embedded system.
\section{METHOD}
\label{chapter:method}

Let $\mathbf{T}$ be an $M \times N$ constant matrix where a column is accessed as $\mathbf{t}_j$. Let $\mathbf{v}$ represent an input vector, and $v_j$ is one of its entries. Then, a matrix-vector multiplication can be realized as 
%\begin{equation}\label{matvec}
%    \mathbf{Tv} = \mathbf{y} = \mathbf{t}_0v_0+\mathbf{t}_1v_1+...+\mathbf{t}_jv_j+...+\mathbf{t}_{M-1}v_{M-1}.
%\end{equation}
\begin{equation}\label{matvec}
    \mathbf{Tv} = \mathbf{y} = \sum_{j=0}^{M-1} \mathbf{t}_jv_j.
\end{equation}
Equation \ref{matvec} does matrix-vector multiplication in two steps. Firstly, each entry multiplies the related column. Then, the multiplied columns are added row-by-row. Hence, the number of multiplications is reduced when a column contains a constant more than once. When only non-zero elements contribute to CMM, the upper bound on the number of computations is determined by the number of non-zero elements, $E$. Sparsity increases as the non-zero ratio, $\alpha$, reduces:
\begin{equation}\label{NZR}
   \alpha = \frac{E}{M.N}.
\end{equation}

Computations can be reduced further if the non-zero elements are represented by only a few different numbers. For example, a matrix that contains only $-1$, $0$ and $1$ strips off the multiplications. A matrix of size $1000 \times 1000$ with $4-$bit fixed point entries requires at most 16000 multiplications instead of one million if the computation is carried out as shown in Equation (\ref{matvec}). Without loss of generality, it can be claimed that the number of unique values ($U$) in a matrix is essential in reducing the number of computations, regardless of the data format used in representing these values. %Thus, only letters are used in the representation of UV throughout the illustrative examples in this chapter.

%Given a constant matrix, NNZ and UV may be reduced through the quantization and pruning operations to reach better results regarding the number of additions, multiplications and compression size. The data loss may cause an error in the result vector. For this reason, the developer should decide the portions for the pruning and quantization by analyzing the benefits and accuracy loss. The proposed method does not provide a recommendation about the portions. It accepts any constant matrix with or without pruning and quantization.

The proposed method consists of common-subexpression extraction and compression steps. The number of multiplications and additions is reduced in the first step. Then, the constant matrix is losslessly compressed into several one-dimensional arrays in the second step. 
%Both steps are explained on an illustrative multiplication example given in Equation (\ref{yTv}) where $U$ = \{a, b, c\} represent the set of unique values.
%\begin{equation} 
%\setlength{\abovedisplayskip}{0pt}
%\setlength{\abovedisplayshortskip}{0pt}
%\setlength{\belowdisplayskip}{0pt}
%\setlength{\belowdisplayshortskip}{0pt}
%\mathbf{y} = \mathbf{Tv} = 
%\begin{bmatrix}
%y_0\\
%y_1\\
%y_2\\
%y_3\\
%y_4\\
%\end{bmatrix}
%=
%\begin{bmatrix}
%    a & c & b & c & a & a\\
%    b & a & c & b & b & c\\
%    b & 0 & c & 0 & b & c\\
%    a & c & 0 & c & 0 & a\\
%    c & c & b & b & a & a\\
%\end{bmatrix}
%.
%\begin{bmatrix}
%    v_0\\
%    v_1\\
%    v_2\\
%    v_3\\
%    v_4\\
%    v_5\\
%\end{bmatrix}
%\label{yTv}
%\end{equation}
\subsection{Common Subexpression (CSE) Elimination}

The proposed approach searches two-element common subexpressions. Let $\mathbf{t}_i$ and $\mathbf{t}_j$ be two selected columns from matrix $\mathbf{T}$. The elements at the $r$'th row of these columns are accessed as $t_{r,i}$ and $t_{r,j}$. Element-wise addition at these rows, $add_{r,ij}$ is defined as
\begin{equation}\label{addel}
   add_{r,ij}=t_{r,i}v_i+t_{r,j}v_j.
\end{equation}
If there exists another row $q$ such that $add_{r,ij}=add_{q,ij}$, then one of the additions can be eliminated since the result of the first addition can be used in the other. Assume that there are $z_{r,ij}$ occurrences of $add_{r,ij}$ in the $(\mathbf{t}_i,\mathbf{t}_j)$ pair. Then, the number of addition eliminations due to $add_{r,ij}$ can be computed as $z_{r,ij}-1$. Thus, a solution that maximizes $gain$ is sought:
\begin{equation}
\label{gain}
gain=\sum_{\forall(\mathbf{t}_i,\mathbf{t}_j)} \sum_k (z_{r,ij}-1).
\end{equation}

\begin{algorithm}
%\vspace*{-.2cm}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE {$\mathbf{T}$}
\ENSURE {$CSE, \mathbf{T}^u$}
\STATE $gain = 0$
\STATE $CSE = \emptyset $
\FOR{ every iteration $< It$}  \label{iterationstart}
	\STATE $Commons = \emptyset $ \label{initialstart}
	\STATE Form $T_a=\{\mathbf{t}_0,\mathbf{t}_1, \dots, \mathbf{t}_n\}$
	\STATE partition $T_a$ in to $N/2$ pairs: $\mathbf{t_{ij}}=(\mathbf{t}_i,\mathbf{t}_j)$
	\FOR {every $t_{ij}$ across all $r$ rows} 
		\STATE find $add_{r,ij}$ such that $z_{r,ij}>1$ is maximum for $t_{ij}$
     	\ENDFOR
	\STATE $gain = \sum_{\forall t_{ij}}\sum_r (z_{r,ij}-1)$
     	\STATE $Commons = \{add_{r,ij}$ that satisfy current $gain\}$ \label{initialend}
	\FOR {every attempt $< At$} 
     		\STATE dissolve any $t_{ij}$ and $t_{kl}$ to obtain $t_{ik}$ and $t_{jl}$ \label{attemptstart}
		\FOR {the new pairs across all $r$ rows} 
			\STATE find $add_{r,ij}$ such that $z_{r,ij}>1$ is maximized
     		\ENDFOR
		\IF {$gain \leq \sum_{\forall t_{ij}}\sum_r (z_{r,ij}-1))$}
     			\STATE update $gain$ and $Commons$ with  $t_{ik}$ and $t_{jl}$
		\ELSE \STATE {restore pairs as $t_{ij}$ and $t_{kl}$}
		\ENDIF \label{attemptend}
  	\ENDFOR 
	\STATE Update T by removing copies of $add_{r,ij} \in Commons$
  	\STATE Update $CSE$ by including $Commons$
\ENDFOR \label{iterationend}
\STATE $T^u=T$ 
\end{algorithmic}
\caption{Algorithm for CSE detection}
\label{AlgCSE}
\end{algorithm}

The search algorithm consists of a series of iterations (Algorithm \ref{AlgCSE}). Each iteration includes \textit{initial} and \textit{improvement} phases. In the initial phase, $N$ columns are randomly paired to form $\frac{N}{2}$ column pairs (lines \ref{initialstart}-\ref{initialend}). The improvement phase consists of a series of \textit{attempts}. In each attempt, two random columns from different pairs are temporarily exchanged for possible gain improvements (lines \ref{attemptstart}-\ref{attemptend}). At the end of an iteration, the common subexpressions are eliminated from the matrix. The next iteration runs on this updated matrix. The number of iterations ($It$) and the number of attempts ($At$) per iteration can be decided by the user though it can also end when gain does not decrease in successive iterations or no more addition pairs with $z_{r,ij}>1$ can be found. The algorithm returns a set of common subexpressions and the final update of the matrix, $T^u$. A sample iteration is shown in Fig. \ref{FigExampleAlg}. %: The columns are randomly paired as $(\mathbf{t}_0,\mathbf{t}_3)$, $(\mathbf{t}_1,\mathbf{t}_4)$ and $(\mathbf{t}_2,\mathbf{t}_5)$. Among them, $(\mathbf{t}_1,\mathbf{t}_4)$ contains $cv_1 + av_4$ three times. So, the gain for the expression is two. The common subexpression for the selected order is listed as follows:

\begin{figure}
\centering{\includegraphics[width=0.7\columnwidth]{FigMat.png}}
\centerline{(a)}
\raggedright
\begin{footnotesize}
\textbf{Start of an iteration}
\begin{itemize}
    \item $(\mathbf{t}_0,\mathbf{t}_3)$
    : $av_0 + cv_3$ occurs twice.
    \item $(\mathbf{t}_1,\mathbf{t}_4)$
    : $cv_1 + av_4$ occurs twice.
    \item $(\mathbf{t}_2,\mathbf{t}_5)$
    : $bv_2 + av_5$ and $cv_2 + cv_5$ occur twice.
\end{itemize}
$gain=4$. $Commons=\{(\mathbf{t}_0,\mathbf{t}_3), (\mathbf{t}_1,\mathbf{t}_4),(\mathbf{t}_2,\mathbf{t}_5)\}$.\\
\textbf{First attempt: }Pick $(\mathbf{t}_0,\mathbf{t}_3)$ and $(\mathbf{t}_2,\mathbf{t}_5)$. Exchange $\mathbf{t}_2$ and $\mathbf{t}_3$.
\begin{itemize}
    \item $(\mathbf{t}_0,\mathbf{t}_2)$
    : $bv_0 + cv_2$ occurs twice.
    \item $(\mathbf{t}_1,\mathbf{t}_4)$
    : $cv_1 + av_4$ occurs twice.
    \item $(\mathbf{t}_3,\mathbf{t}_5)$
    : $cv_3 + av_5$ occurs twice.
\end{itemize}
$gain=3$. Restore $(\mathbf{t}_0,\mathbf{t}_3)$ and $(\mathbf{t}_2,\mathbf{t}_5)$.\\
\textbf{Second attempt: }Pick $(\mathbf{t}_1,\mathbf{t}_4)$ and $(\mathbf{t}_2,\mathbf{t}_5)$. Exchange $\mathbf{t}_4$ and $\mathbf{t}_5$.
\begin{itemize}
    \item $(\mathbf{t}_0,\mathbf{t}_3)$
    : $av_0 + cv_3$ occurs twice.
    \item $(\mathbf{t}_1,\mathbf{t}_5)$
    : $cv_1 + av_5$ occurs three times.
    \item $(\mathbf{t}_2,\mathbf{t}_4)$
    : $bv_2 + av_4$ and $cv_2 + bv_4$ occur twice.
\end{itemize} 
$gain=5$. $Commons=\{(\mathbf{t}_0,\mathbf{t}_3), (\mathbf{t}_1,\mathbf{t}_5),(\mathbf{t}_2,\mathbf{t}_4)\}$.\\
\end{footnotesize}
\centerline{(b)}
\caption{Two-term CSE extraction: (a)Example matrix, (b)Sample iteration of Algorithm \ref{AlgCSE}.}
\label{FigExampleAlg}
\end{figure}


%Then the total gain becomes $(2 - 1) + (2 - 1) + (2 - 1)  + (2 - 1) = 4$. Once the gain is calculated, the initial phase of the iteration ends. In the improvement phase,  $(\mathbf{t}_2$ and $\mathbf{t}_3)$ are temporarily exchanged in the first attempt. The common subexpressions are listed as follows:
%The attempt is rejected as the new gain is $3$. In the second attempt, $(\mathbf{t}_4$ and $\mathbf{t}_5)$ are temporarily exchanged. The common subexpressions are listed as follows:
%This attempt is accepted as the new gain is $5$. The current iteration ends when the attempts end. The common subexpressions are removed from the matrix and put in a list. The next iteration is carried on the remaining matrix.  When all iterations end, the extracted common subexpressions and the remainder matrix of the last iteration are fed to the compression phase.

\subsection{CSE-based Matrix Compression}
%\subsection{Matrix Compression and Multiplication}
The CSE detection algorithm returns the set of two-term CSEs and the remainder matrix $\mathbf{T}^u$. We propose three pairs of one-dimensional arrays to store the original matrix $\mathbf{T}$. In each pair, the first array stores the values, and the second array stores the pointers to process the first array. The last element of the second array holds the length of the first array.
\subsubsection {Weights}
The first array stores nonzero entries in each column of the matrix as values in $U$ because each entry of the input vector is multiplied with the weights of the column only once. The second array $WP$ shows the start index of the weight set for the next column. In Fig. \ref{FigExampleAlg}(a), $a$ and $b$  appear twice in $\mathbf{t}_0$. Fig. \ref{FigExampleArr}(a) represents them as one $a$ and one $b$ in the yellow slots. They are multiplied by $v_0$ only once. $WP$ starts with 3, indicating the start of the unique weights for $\mathbf{t}_1$.

\subsubsection {CSE}
The elements of the CSE set obtained in Algorithm \ref{AlgCSE} are stored with their row positions in the first array. The second array $CP$ shows the start index of the next CSE. In Fig. \ref{FigExampleAlg}, $av_0+cv_3$ is selected as a CSE at the end of the iteration. It is composed of $a$ in the yellow slot and $c$ in the green slot of \textit{Weights} in Fig. \ref{FigExampleArr}(a). Their indices are 0 and 8 in the \textit{Weights} array, respectively. This information is stored as the first two entries in the \textit{CSE} array in Fig. \ref{FigExampleArr}(b). The next two slots are reserved for the rows where this CSE appears, because \textit{CP} begins with 4 as the start index of the next CSE. Fig. \ref{FigExampleAlg}(a) shows that $av_0+cv_3$ is at rows 0 and 3. Hence $CSE[2]=0$, $CSE[3]=3$ in Fig. \ref{FigExampleArr}(b). 

\subsubsection {Singles}
The elements of the remainder matrix $\mathbf{T}^u$ are stored one by one in the first array. Fig. \ref{FigExampleArr}(c) shows $av_1$ as the first entry. It is the third entry from the \textit{Weights} array. Its row 
in $\mathbf{T}^u$ is given in $SP$ array. $SP[0]$ shows the number of elements in row 0. $SP[r]-SP[r-1]$ shows the number of elements in row $r$. As $SP[0]=0$ and $SP[1]=2$ in Fig. \ref{FigExampleArr}(c), row 0 is empty and row[1] has two elements, $av_1$ and $bv_3$.

\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{FigExampleArr.png}
\caption{Array pairs for matrix compression: (a) weights, (b) CSE, (c) Singles}
\label{FigExampleArr}
\end{figure}

\subsection{Analysis}
In Compressed Sparse Row (CSR) representation, only nonzero elements with their column positions are stored. The row information is obtained in the same way that we use in the SP of the Singles array% (Fig. \ref{FigExampleArr}(c))
. Then, the storage cost of CSR is %given as 
\begin{equation}\label{eqCSR}
S_{CSR}=2. E+M.
\end{equation}
Using Equation \ref{NZR}, it can be deduced that CSR stores a matrix in a smaller memory when the following condition holds:
%Using Equation \ref{NZR}, the storage-efficiency of CSR representation is determined as
\begin{equation}
%\begin{aligned}
%storage_{CSR}=2 \times NNZ+M &< M \times N \\
\alpha < \frac{N-1}{2N}.
%\end{aligned}
\end{equation}
This shows CSR efficiently compresses large matrices if sparsity is more than 50\%. Our proposal offers more compression when the number of unique values is limited. The storage consumed by  \textit{Weights} and \textit{WP} is 
\begin{equation}
S_{Weights}\leq N.(U+1).
\end{equation}
The worst case occurs when all unique values appear in each column. The \textit{CSE} array pair requires two entries for each common subexpression in the CSE set of Algorithm \ref{AlgCSE}. The number of outputs that use the common subexpressions is $gain+ |CSE|$. The \textit{CP} array size is $|CSE|$. Then,
\begin{equation}
S_{CSE}=gain+ 4. |CSE|
\end{equation}
In the \textit{Singles} array pair, the first array has $E-2.(gain+|CSE|)$ elements. Including the number of rows, the storage cost is 
\begin{equation}
S_{Singles}=E-2.(gain+|CSE|)+ M.
\end{equation}
Then, the total cost of the proposed approach is
\begin{equation}\label{eqProposed}
S_{prop} \leq N.(U+1)+E+M+2.|CSE|-gain
\end{equation}
Equations \ref{eqCSR} and \ref{eqProposed} can be processed to show that the proposed approach compresses better than CSR when the following condition is satisfied:
\begin{equation}\label{eqCriterion}
\alpha > (U+1)/M+(2.|CSE|-gain)/(M.N)
\end{equation}
Using Equation \ref{eqCriterion}, Fig. \ref{FigCSRvsMNvsProp} is plotted to show the storage for a $1000\times1000$ matrix  with $|CSE|=0$ for different $\alpha$ and $U$ values. When $\alpha=0.1$ and $U=99$, the proposed approach compresses exactly the same as CSR. However, its compression is better than CSR as $U$ gets lower. Especially for binary or ternary matrices, compression is still achieved with the new scheme even though the matrix is not sparse. 
\begin{figure}
\centering
\includegraphics[width=1.0\columnwidth]{FigCSRvsMNvsProp.png}
\caption{Storage Comparison of CSR, Baseline and the Proposed Method.}
\label{FigCSRvsMNvsProp}
\end{figure}
Equation \ref{eqCriterion} also shows that when the average $gain$ per common subexpression is more than two, the number of unique values can be increased further.

% Three algorithms process the matrix to generate the arrays for compression. 
%\subsubsection {Intermediate Result Generator (IRG)}
%It performs columnwise scalar multiplications. The unique values in each column are stored in \textit{unique elements array} (UEA). The end of each column in UEA is indexed in the \textit{unique elements separator array} (UESA). The IRG processes these two arrays to build the \textit{multiplication result array} (MRA). Its pseudocode is given in Figure \ref{Function_Intermediate_Values_Unit}. For the illustrative example in Equation \ref{yTv}, the arrays are formed as follows:
%
%
%
%\begin{itemize}
%    \item UEA $=\{a,b,c,a,c,b,c,b,c,a,b,a,c\}$
%    \item UESA $=\{3,5,7,9,11,13\}$
% %   \item MRA: $|$ $av_0$ $bv_0$ $cv_0$ $av_1$ $cv_1$ $bv_2$ $cv_2$ $bv_3$ $cv_3$ $av_4$ $bv_4$ $av_5$ $cv_5$ $|$.
%        \item MRA $=\{av_0,bv_0,cv_0,av_1,cv_1,bv_2,cv_2,bv_3,cv_3,av_4,$ $bv_4,av_5,cv_5\}$.
%\end{itemize}
%
%MRA $=\{av_0,bv_0,cv_0,av_1,cv_1,bv_2,cv_2,bv_3,cv_3,av_4,$ $bv_4,av_5,cv_5\}$.
%
%\begin{algorithm}
%%\vspace*{-.2cm}
%\begin{algorithmic}[1]
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%    \REQUIRE {Vector[]}
%    \ENSURE{MRA[]}
%    \STATE $MRA[] = \{0\}; \quad end = 0$
%    \STATE $UEA[] = \{...\}; \quad UESA[] = \{...\}$
%    \COMMENT {Predefined}
%    % \STATE $UESA[] = \{...\}$
%    % \COMMENT {Predefined}
%    % \STATE $end = 0$
%    \FOR{$j = 0$ \TO $j < Column$}
%        \STATE $start = end; \quad end = UESA[j]$
%        % \STATE $end = UESA[j]$
%        \FOR{$k = start$ \TO $k < end$}
%        \STATE $MRA[k] = UEA[k] * vector[j]$
%        \ENDFOR
%    \ENDFOR
%    \RETURN $MRA[]$
%\end{algorithmic}
%%\vspace*{-.2cm}
%\caption{Pseudocode of the Intermediate Result Generator.}
%\label{Function_Intermediate_Values_Unit}
%%\vspace*{-.2cm}
%\end{algorithm}
%
%\subsubsection {Pair Copy Generator (PCG)}
%It performs additions in common subexpressions by using the values in MRA. The common subexpressions are saved into the \textit{copy pairs array} (CPA). The end index of each group in CPA is saved into the \textit{copy pairs separator array} (CPSA). A pair group contains two MRA indices and the row indices of the pairs in \textbf{T}. Two values from MRA is used to calculate the result of the subexpressions. Then, it is accumulated on the result vector elements. CPA and CPSA are filled according to \textbf{T} in Equation \ref{yTv} as follows:
%\begin{itemize}
%    \item CPA $=\{0,8,0,3,4,11,0,3,4,5,9,0,4,6,10,1,2,1,12,1,2\}$.
%    \item CPSA $=\{4,9,13,17,21\}$.
%\end{itemize}
%The order of the groups are not important. The pseudocode of the PCU is provided in Figure \ref{Function_Pairs_Copy_Unit}.
%
%\begin{algorithm}
%%\vspace*{-.2cm}
%\begin{algorithmic}[1]
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%    \REQUIRE {MRA[]}
%    \ENSURE{Result Vector[]}
%    \STATE $result\_vector[] = \{0\}; \quad end = 0$
%    \STATE $CPA[] = \{...\}; \quad CPSA[] = \{...\}$
%    \COMMENT {Predefined}
%    % \STATE $CPSA[] = \{...\}$
%    % \COMMENT {Predefined}
%    \STATE $pairs\_number = \{...\}$
%    \COMMENT {Predefined}
%    % \STATE $end = 0$
%    \FOR{$i = 0$ \TO $i < pairs\_number$}
%        \STATE $start = end; \quad end = CPSA[i]$
%        % \STATE $end = CPSA[i]$
%        \STATE $sum = MRA[CPA[start]] + MRA[CPA[start + 1]]$
%        \FOR{$k = start + 2$ \TO $k < end$}
%        \STATE $result\_vector[CPA[k]] += sum$
%        \ENDFOR
%    \ENDFOR
%    \RETURN $result\_vector[]$
%\end{algorithmic}
%%\vspace*{-.2cm}
%\caption{Pseudocode of the Pair Copy Unit.}
%\label{Function_Pairs_Copy_Unit}
%%\vspace*{-.2cm}
%\end{algorithm}
%
%PCU prepares the result vector containing only the results of the subexpressions and sends it to the Elements Copy Unit (ECU). The elements of the remaining matrix of the last iteration are accumulated on the result vector in the ECU. The elements are row-wise ordered, and their MRA indices are saved into the \textit{copy elements array} (CEA). The end index of each row is saved into the \textit{copy elements separator array} (CESA). CEA and CESA are filled according to $\textbf{T}^2_r$ in Equation \ref{yTv2} as follows:
%\begin{itemize}
%    \item CEA: $|$ $3$ $7$ $2$ $7$ $|$.
%    \item CESA: $|$ $0$ $2$ $2$ $2$ $4$ $|$.
%\end{itemize}
%% Note that some values occurs more than one CESA as $\textbf{T}^2_r$ contains empty rows. *
%The pseudocode of the ECU is provided in Figure \ref{Function_Elements_Copy_Unit}.
%
%\begin{figure}[h]
%\vspace*{-.2cm}
%\begin{algorithmic}[1]
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%    \REQUIRE {MRA[], Result Vector[]}
%    \ENSURE{Result Vector[]}
%    \STATE $CEA[] = \{...\}; \quad CESA[] = \{...\}$
%    \COMMENT {Predefined}
%    % \STATE $CESA[] = \{...\}$
%    % \COMMENT {Predefined}
%    \STATE $end = 0$
%    \FOR{$i = 0$ \TO $i < Row$}
%        \STATE $start = end; \quad end = CESA[i]$
%        % \STATE $end = CESA[i]$
%        \FOR{$k = start$ \TO $k < end$}
%        \STATE $result\_vector[i] += MRA[CEA[k]]$
%        \ENDFOR
%    \ENDFOR
%    \RETURN $result\_vector[]$
%\end{algorithmic}
%\vspace*{-.2cm}
%\caption{Pseudocode of the Element Copy Unit.}
%\label{Function_Elements_Copy_Unit}
%\vspace*{-.2cm}
%\end{figure}
%
%The output of ECU equals the result vector. The matrix multiplication is performed with six constant arrays and one temporary MRA. Array sizes depend on the matrix elements and CSE solution.

\section{Experiments}

Sample matrices are constructed to compare the baseline, CSR, \cite{b9}, \cite{b10} and the proposed method. Sparse matrices are obtained by removing the lowest absolute valued-entries until the desired $\alpha$ is attained \cite{refPruningQuantization}. Then, nonzero entries are linearly quantized with $U$ distinct values. The baseline consists of a nested for loop to multiply and accumulate nonzero entries. The matrix multiplication with each method is simulated on gem5 \cite{b13} for a single x86 ISA CPU running at 1 GHz, two levels of 256MB cache implemented as a single-port %RAM. 

Methods in \cite{b9} and \cite{b10} can process only 0-1 matrices. Hence, only the number of additions for matrices with $M=N=100$, $U = 2$ and $\alpha = \{0.25, 0.5, 0.75\}$  are listed in Table \ref{TabResults}. %The number of multiplications is not added because only the proposed method reduces them, and it can be $U \times M$ at most.
The matrix size is limited to 100 as \cite{b9} and \cite{b10} produce an adder tree in an hour. The proposed method consumes 70 seconds for the same matrix. As $\alpha$ decreases, the length of the CSEs decreases, and the proposed method produces better results. When the length of the CSEs is allowed to increase, \cite{b9} and \cite{b10} can yield fewer additions as their CSEs can include multiple terms. However, the CPU cycles for CMM using \cite{b9} and \cite{b10} are $2.0\times$--$4.7\times$ and $1.6\times$--$2.0\times$ of the proposed method as they lack compression. Execution time of CMM with CSR slightly exceeds the proposed method.

\vspace*{-.2cm}

\begin{table}[htbp]
\caption{The Number of Additions for $U = 2$ and $M=N=100$}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
% \hline
% \multicolumn{6}{|c|}{\textbf{Additions}} \\
\hline
$\alpha$ & CSR & baseline & \cite{b9} & \cite{b10} & CSE-based \\
\hline \hline
0.25 & 2500 & 2500 & 2093 & 2072 & \textbf{1923} \\
\hline
0.5 & 5000 & 5000 & 3353 & 3537 & \textbf{3326} \\
\hline
0.75 & 7500 & 7500 & \textbf{4229} & 4536 & 4492 \\
\hline
%\multicolumn{6}{|c|}{\textbf{CPU Cycles (Billion)}} \\
%\hline
%$\alpha$ & CSR & baseline & \cite{b9} & \cite{b10} & CSE-based \\
%\hline
%0.25 & 1.891 & \textbf{1.884} & 3.717 & 3.053 & 1.891 \\
%\hline
%0.5 & 1.925 & \textbf{1.884} & 6.049 & 3.588 & 1.911 \\
%\hline
%0.75 & 1.959 & \textbf{1.884} & 9.041 & 3.937 & 1.922 \\
%\hline
\end{tabular}
\label{TabResults}
\end{center}
\end{table}

\vspace*{-.2cm}

The performance of the proposed method, baseline and CSR are compared for $M=N=1000$ and $\alpha \leq  0.3$. Fig. \ref{FigLatency} shows that CSR  performs better with lower $\alpha$ while the proposed method is better with lower $U$ and higher $\alpha$.
% $\alpha$ & CSR & baseline & \cite{b9} & \cite{b10} & CSE-based \\
% \hline
% 0.25 & 1891776020 & 1884588862 & 3717304528 & 3053854808 & 1891814506 \\
% \hline
% 0.5 & 1925740653 & 1884538586 & 6049816730 & 3588777868 & 1911124570 \\
% \hline
% 0.75 & 1959451377 & 1884713750 & 9041786955 & 3937656395 & 1922898523 \\

% \begin{itemize}
%     \item "F": 2500 $(\alpha = 0.25)$, 5000 $(\alpha = 0.5)$, 7500 $(\alpha = 0.75)$
%     \item CSR: 2500 $(\alpha = 0.25)$, 5000 $(\alpha = 0.5)$, 7500 $(\alpha = 0.75)$
%     \item \cite{b9}: 2093 $(\alpha = 0.25)$, 3353 $(\alpha = 0.5)$, 4229 $(\alpha = 0.75)$
%     \item \cite{b10}: 2072 $(\alpha = 0.25)$, 3537 $(\alpha = 0.5)$, 4536 $(\alpha = 0.75)$
%     \item "P": 1923 $(\alpha = 0.25)$, 3326 $(\alpha = 0.5)$, 4492 $(\alpha = 0.75)$
% \end{itemize}

%The matrix size is limited to 100 because \cite{b9} and \cite{b10} produce an adder tree in an hour. The proposed method consumes 70 seconds for the same matrix. When $\alpha$ decrease, the length of the common subexpressions decreases and the proposed method produces better results. Vice versa, the state-of-the-art methods produce better results when the length of the common subexpressions increases because the proposed method searches only size-of-two expressions, and the state-of-the-art methods do not include a length limitation. The CPU cycles for \cite{b9} and \cite{b10} increase dramatically as their notation lacks a compression technique. The performance of the proposed method, baseline and CSR is not distinguishable well. So, they are compared in Figure \ref{FigLatency} for given $1000 \times 1000$ matrices where $\alpha = \{0.05, 0.1, 0.15, 0.2, 0.25, 0.3\}$. The proposed method intersects with the baseline curve between $0.15 \alpha$ and $0.3 \alpha$ for different unique values. CSR takes place between $U=2$ and $U=16$ curves. It shows better performance with lower $\alpha$ while the proposed method shows better performance with lower $U$ and higher $\alpha$.

\vspace*{-.2cm}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\columnwidth]{FigLatency.png}
\caption{CMM Computation time comparison of CSR, Baseline and the Proposed Method.}
\label{FigLatency}
\end{figure}

\vspace*{-.2cm}

As a DL use case, an autoencoder is trained with the MNIST dataset for 98.85\% accuracy. It contains eleven matrices from $64 \times 128$ to $2048 \times 4096$. Nine versions of this autoencoder are prepared with $U=\{2,4,8\}$ and $\alpha=\{0.25,0.50,0.75\}$. 
% In Table \ref{TabAutoencoder}, the compression percentage is presented as the total storage size of the method compared to the baseline $N \times M$. The results show that the proposed method compresses a quantized matrix better than CSR. The compression percentage increases when $U$ decreases.
In Table \ref{TabAutoencoder}, the ratio of the total storage size over the baseline $(N \times M)$ is presented. The results show that a matrix represented with a small unique value set is compressed better with the proposed method than it is done CSR. The storage size decreases when $U$ decreases in the proposed method. %CSR reduces the storage size up to 50\%.

\begin{table}[htbp]
\caption{Autoencoder Storage Size Relative to the Baseline}
\begin{center}
\begin{tabular}{|c||c|c|c|c|}
\hline
%\multicolumn{5}{|c|}{\textbf{The Number of Elements Over the Full Size}} \\
%\hline
$\alpha$ & CSR & CSE ($U = 2$) & CSE ($U = 4$) & CSE ($U = 8$) \\
\hline \hline
0.25 & 50.1\% & \textbf{22.5\%} & 23.4\% & 24.6\% \\
\hline
0.5 & 100.1\% & \textbf{38.2\%} & 39.4\% & 42.5\% \\
\hline
0.75 & 150.1\% & \textbf{47.5\%} & 48.8\% & 52.8\% \\
\hline
\end{tabular}
\label{TabAutoencoder}
\end{center}
\end{table}

\vspace*{-.2cm}

\section{Conclusion}

A fast CSE extraction algorithm with a dedicated compression format is proposed for large sparse matrix multiplication. It compresses more than CSR when weights are represented with a few numbers. Simulations show that it outperforms the state-of-the-art methods for this type of large matrices. % It can be used for the sparse matrices up to the 30\% density instead of the baseline. %The next study is upgrading the proposed method to utilize parallel processing.

\vspace*{-.2cm}

\begin{thebibliography}{00}
\bibitem{refTiny} MCUNet: J. Lin, W. Chen, Y. Lin, J. Cohn, C.Gan and S Han, “Tiny Deep Learning on IoT Devices,” \textit{ArXiv:2007.10319 [cs.CV]}, 2020.
\bibitem{refDLHardware} Y. LeCun, “1.1 Deep Learning Hardware: Past, Present, and Future,” \textit{IEEE International Solid-State Circuits Conference - (ISSCC)}, pp. 12--19, San Francisco, USA, 2019.
\bibitem{refMultilevelCSR} H. Kabir, J. D. Booth and P. Raghavan, “A Multilevel Compressed Sparse Row Format for Efficient Sparse Computations on Multicore Processors”, \textit{2014 21st International Conference on High Performance Computing (HiPC)}, Goa, India, 2014, pp. 1--10.
\bibitem{refSparseMatrix} N. Goharian., A. Jain and S. Qian, “Comparative Analysis of Sparse Matrix Algorithms for Information Retrieval,” \textit{Computer}, Vol. 2, pp. 0--4, 2003.
% \bibitem{refQuantization} S. Han, H. Mao and W. Dally, “Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,” \textit{ArXiv:1510.00149 [cs]}, 2016.
\bibitem{refPowerOfTwo} F. Xu, C. Chang and C. Jong, “Design of Low-Complexity FIR Filters Based on Signed-Powers-of-Two Coefficients With Reusable Common Subexpressions,” \textit{IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, Vol. 26, No. 10, pp. 1898--1907, 2007.
\bibitem{refCSEDSP} A. Hosangadi, F. Fallah and R. Kastner, “Common Subexpression Elimination Involving Multiple Variables Linear DSP Synthesis,” \textit{Proceedings. 15th IEEE International Conference on Application-Specific Systems, Architectures and Processors}, TX, USA, pp. 202--212, 2004.
\bibitem{refSparseMatrixCSR} J. Willcock and A. Lumsdaine, “Accelerating Sparse Matrix Computations via Data Compression,” \textit{Proceedings of the 20th Annual International Conference on Supercomputing}, pp. 307--316, 2006.
%\bibitem{refPhDThesis} E. Junttila, “Patterns in Permuted Binary Matrices,” PhD Thesis, Department of Computer Science, Univ. of Helsinki, Helsinki, Finland, 2011.
% \bibitem{b4} Alzubaidi, L., J. Zhang, A. Humaidi, A. Al-Shamma, J. Santamar ia, M. Fadhel, M. Al-Amidie and L. Farhan, “Review of Deep Learning: Concepts, CNN Architectures, Challenges, Applications, Future Directions”, Journal of Big Data, Vol. 8, p. 53, 2021.
\bibitem{b5} Y. Wang, W. Guo and X. Yue, “Tensor Decomposition to Compress Convolutional Layers in Deep Learning,” \textit{IISE Trans.}, pp. 1–-60, 2021.
\bibitem{b6} Z. Liu, M. Sun, T. Zhou, G. Huang and T. Darrell, “Rethinking the Value of Network Pruning,” \textit{ArXiv:1810.05270 [cs.LG]}, 2019.
\bibitem{b7} T. Liang, J. Glossner, L. Wang, S. Shi and X. Zhang, “Pruning and Quantization for Deep Neural Network Acceleration: A Survey,” \textit{Neurocomputing}, Vol. 461, No. C, pp. 370–403, 2021.
\bibitem{b8} D. Langr and P. Tvrdik, “Evaluation Criteria for Sparse Matrix Storage Formats,” \textit{IEEE Transactions on Parallel and Distributed Systems}, Vol. 27, No. 2, pp. 428–440, 2016.
\bibitem{refYurdakulJVLSI99} A. Yurdakul and G. Dundar, “Multiplierless Realization of Linear DSP Transforms by Using Common Two-Term Expressions,” \textit{Journal of VLSI Signal Processing}, Vol. 22, pp. 163--172, 1999.
\bibitem{b9} S. Hsiao, M. Chen and C. Tu, “Memory-free Low-cost Designs of Advanced Encryption Standard Using Common Subexpression Elimination for Subfunctions in Transformations,” \textit{IEEE Transactions on Circuits and Systems I: Regular Papers}, Vol. 53, No. 3, pp. 615–626, 2006.
\bibitem{b10} N. Wu, X. Zhang, Y. Ye and L. Lan, “Improving Common Subexpression Elimination Algorithm with A New Gate-Level Delay Computing Method,” \textit{Lecture Notes in Engineering and Computer Science}, Vol. 2, pp. 677–682, 2013.
% \bibitem{b11} X. Zhang, “A Low-power Parallel Architecture for Linear Feedback Shift Registers,” \textit{IEEE Transactions on Circuits and Systems II: Express Briefs}, Vol. 66, No. 3, pp. 412–416, 2018.
% \bibitem{b12} B. Pasuluri and V. K. Sonti, “FPGA Implementation of Enhanced Throughput Design of AES Architecture Using Nikhilam Sutra,” \textit{Tianjin Daxue Xuebao (Ziran Kexue yu Gongcheng Jishu Ban)/Journal of Tianjin University Science and Technology}, Vol. 55, 2022.
\bibitem{refUnrollingTNN} S. Tridgell et al., “Unrolling Ternary Neural Networks,” \textit{ACM Transactions on Reconfigurable Technology and Systems}, Vol. 12, Issue 4, No. 22, pp. 1--23, 2019.
% \bibitem{refUnrollingTNN} S. Tridgell, M. Kumm, M. Hardieck, D. Boland, D. Moss, P. Zipf and P. H. W. Leong, “Unrolling Ternary Neural Networks,” \textit{ACM Transactions on Reconfigurable Technology and Systems}, Vol. 12, Issue 4, No. 22, pp. 1--23, 2019.
\bibitem{refMicroAI} A. Mazumder et al., “A Survey on the Optimization of Neural Network Accelerators for Micro-AI On-Device Inference,” \textit{IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, Vol. 11, No. 4, pp. 532--547, 2021.
% \bibitem{refMicroAI} A. Mazumder, J. Meng, H. Rashid, U. Kallakuri, X. Zhang, and J. Seo and T. Mohsenin, “A Survey on the Optimization of Neural Network Accelerators for Micro-AI On-Device Inference,” \textit{IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, Vol. 11, No. 4, pp. 532--547, 2021.
% \bibitem{b13} N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell, M. Shoaib, N. Vaish, M. D. Hill and D. A. Wood, “The Gem5 Simulator,” \textit{SIGARCH Computer Architecture News}, Vol. 39, No. 2, pp. 1–7, 2011.
% \bibitem{b14}  L. Deng, "The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]", \textit{IEEE Signal Processing Magazine}, Vol. 29, No. 6, pp. 141-142, 2012.
\bibitem{refPruningQuantization} F. Tung and G. Mori, “Deep Neural Network Compression by In-Parallel Pruning-Quantization,” \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, Vol. 42, No. 3, pp. 568--579, 2020.
\bibitem{b13} N. Binkert et al., “The Gem5 Simulator,” \textit{SIGARCH Computer Architecture News}, Vol. 39, No. 2, pp. 1–7, 2011.
\end{thebibliography}

\clearpage

% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
