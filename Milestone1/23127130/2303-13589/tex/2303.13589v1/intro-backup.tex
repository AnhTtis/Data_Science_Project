% Given that labels are unavailable,
\pt{move to background.}
Such ``generalization predictors" rely upon other features that are well-correlated with performance to estimate, either directly or using light-weight regressors, the accuracy. Here, each featurization reflects a different belief in what aspects of the model, training distribution and target distributions are most relevant to generalization, and can be used to score individual samples, on the likelihood that they are correct. Broadly, these scores are based on distances \cite{Deng21_labelsnecessary}, confidences \cite{Guillory21_DoC}, manifold smoothness\cite{Ng22_LocalManifoldSmoothness} or model agreement \cite{Jiang22_Disagreement,Chen21_SelfTrainEnsemb}. find that the disagreement rate between two DNNs trained from different seeds is sufficient to predict the accuracy and demonstrate this is due to the well-calibrated nature of DNNs ensembles. Concurrently, \cite{Ng22_LocalManifoldSmoothness} find that classifier smoothness over the local data manifold is an effective feature that allows for black-box gap prediction.
% Such ``generalization predictors" rely upon other features that are well-correlated with performance to estimate, either directly or using light-weight regressors, the accuracy. Here, each featurization reflects a different belief in what aspects of the model, training distribution and target distributions are most relevant to generalization. For example, \cite{Deng21_labelsnecessary} use distributional distances, such a Fr√®chet distance or maximum mean discrepancy (MMD), to learn a linear regressor that relates generalization to the distance between the training distribution and target distributions. \reminder{The underlying intuition is that models generalize better to distributions that are close to the training set, and this intuition is formalized through the linear relationship found by the regressor.-redundant} Meanwhile, \cite{Guillory21_DoC} argue that using the difference of confidences, in lieu of distances or average confidence directly, is more effective, as using the relative confidence between distributions better reflects uncertainty and, consequently generalization.  Alternatively, \cite{Jiang22_Disagreement,Chen21_SelfTrainEnsemb} find that the disagreement rate between two DNNs trained from different seeds is sufficient to predict the accuracy and demonstrate this is due to the well-calibrated nature of DNNs ensembles. Concurrently, \cite{Ng22_LocalManifoldSmoothness} find that classifier smoothness over the local data manifold is an effective feature that allows for black-box gap prediction.

Given that predictors rely upon disparate features and that accurately predicting generalization is critical to safe model deployment, a better understanding of the types of distributions that cause a particular predictor to estimate generalization poorly is important for practitioners to trust its estimates. Indeed, the characteristics of a target distribution cannot be known \textit{apriori}, so recognizing and correcting for such troublesome distributions is critical to the adoption and design of better predictors. Therefore, in this paper, we ask \textit{when} do generalization gap predictors fail to make reliable estimates, and are datasets difficult to evaluate for a given predictor \textit{also difficult} for the other predictors? We strongly emphasize that the existence of any such ``adversarial" distribution must be rectified as these it is unknown if models will encounter these very distributions during deployment.  

% Generally, predictors are evaluated on their ability to estimate the accuracy on various natural and synthetic datasets, given some pretrained model. However, while some predictors directly estimate the accuracy \cite{Jiang22_Disagreement,Chen21_SelfTrainEnsemb}, other methods require additional datasets to train regressors \cite{Guillory21_DoC}. As we are interested in understanding the limitations of the inductive bias incurred by a specific featurization when evaluating on different target distributions, we first disentangle the effect of the prediction mechanism and featurization by uniformly evaluating methods using a simple, but effective, threshold-based scheme. Given this separation, we are then able to fairly evaluate the effectiveness of different featurizations.  %on different shifts, and identify a shared mechanism for which a universarily adversarial distribution can be created for harming predictor performance.
% \reminder{Redundant with next paragraph, allude to SB as the shared mechanism.}

 %For example, classifier smoothness is better unaligned for estimating generalization gaps under corruptions, as increased smoothness over data manifold, includes smoothness over corrupted data points. Moreover, we directly use the featurizations to adversarially construct distributions that can be used to evaluate predictors, e.g., further studying if predictors fail in the same way. 
%Our experiments suggest that the predictors must be selected to align with the expected nature of target distributions, lest they are evaluated on shifts for which they are systematically disadvantaged. 


\textbf{This Work.} In this work, we investigate when and how generalization gap predictors make poor estimates. We first show that predictors incur disparate behavior over 3 types of shifts: label noise, corruptions and under-sampling. Moreover, we directly use the featurizations to adversarially construct distributions that can be used to attack other predictors, further studying if predictors fail in the same way. %Our results suggests that predictors must be selected to align with the expected nature of target distributions, lest they are evaluated on shifts for which they are systematically disadvantaged.  
Our results suggests that the intuition of different featurizations are best suited for particular types of shifts, instead of being generally useful, contrary to the requirements of useful predictors.
Given this disparate behavior, we take a complementary perspective and also identify a \textit{shared mechanism} in the underlying training process that can be used to construct datasets which lead to poor estimates \textit{across} predictors. Namely, we find that well-known suspectibility of neural networks to simplicity bias \cite{Shah20_SimplicityBias}, e.g., the propensity of DNNs to rely upon simple linearly separable features, not only harms generalization but also enables the construction of distributions that induce poor estimates across predictors. %We propose a simple fix to ensembling based methods that allows us to circumvent such shortcoming, demonstrating the utility of identifying pitfalls of predictor evaluation.%Understanding such a mechanism allows us correct for predictor failure cases, irrespective of the underlying featurization. 
\begin{itemize}
    \item \textbf{Evaluation across Shifts.} Across three distribution shifts and four classes of predictors, we disentagle the effects of featurization and prediction mechanism to identify where predictors fall short.
    \item \textbf{Similar Failures?} We create ``adversarial" distributions using featurizations of one predictors to evaluate separate predictors, to determine when predictors fail in the same way. 
    \item \textbf{Shared Attacks using Simplicity Bias.} We exploit the tendency for DNNs to rely upon simple features to create datasets that lead to poor estimates across different classes of predictors.
 \end{itemize}
