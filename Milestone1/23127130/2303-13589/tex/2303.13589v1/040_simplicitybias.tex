In the preceding section, we found that training on compromised data may in fact decrease the GEP error. We hypothesize that this decrease can be partially attributed to the noisy datasets, mitigating \textit{simplicity bias}, i.e., the well known propensity of deep neural networks to rely upon simple, spurious features in lieu of more complex/expressive ones~\cite{brutzkus18_sgd,Gunasekar18,Shah20_SimplicityBias,geirhos18_texturebias}. Given that simple features are not expected to generalize on \textit{o.o.d} datasets, but DNNs remain susceptible to relying upon such simple features, we posit that GEPs will also see decreased performance on distributions where simple features are no longer indicative of the label. We test this hypothesis using a synthetic setting that controls the discriminability of simple features on target datasets, as discussed below.
\\
\noindent \textit{Experimental Setup.} We use a custom ``dominoes" dataset \cite{Shah20_SimplicityBias} of complex and simple features by pairing each class from CIFAR10 (complex feature) with the corresponding digit class in MNIST (simple feature) \cite{Trivedi22_Adaptation}. (See Fig. \ref{fig:cifar_mnist}). Three levels of correlation (95\%, 99\%, 100\%) between the target and simples features are considered during training.  When predicting generalization, we sample complex features from STL10, as well as create a variant that randomizes the spurious correlation between simple and complex features. We fine-tune a MoCo-V2 pretrained ResNet-50~\cite{He20_MoCo} for 20 epochs with lr=0.001 and average results over $3$ seeds.

\begin{figure}
    \centering
    \includegraphics[width = 0.75\columnwidth]{FIGS/cifar-mnist-3.png}
    \caption{\textbf{Simplicity Bias Dataset, (Fig. 2 \cite{Trivedi22_Adaptation}).} Dominoes comprised of complex (CIFAR10) and simple (MNIST) features are used to control the simplicity bias on target datasets.}
    \label{fig:cifar_mnist}
    \vspace{-0.3cm}
\end{figure}


As shown in Table. \ref{tab:simp_bias}, we see that the prediction error often substantially increases when evaluating on the randomized target dataset, e.g., where the simple feature is no longer predictive. While we would expect that the target accuracy decreases, the decreased GEP performance is particularly troubling as such methods are intended to detect these very failures. Moreover, we note that as the correlation between the simple and complex feature increases (Corr=0.95 vs. Corr=1.0), the gap between GEP's performance on the Corr. and Rand. variants of the target dataset increases. Indeed, the Corr. MAE decreases as the training dataset correlation increases (Corr=0.95 vs. 1.0), but the Rand. MAE increases. This result further highlights the harmful role of simplicity bias on GEP performance. 