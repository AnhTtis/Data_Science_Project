We begin by formally introducing the problem setting and scoring functions. 
Let $\mathcal{X}_u = \{\bar{\mathrm{x}}_i\}$ be an unlabeled, target dataset and $\mathcal{X}_{\ell} = \{(\mathrm{x}_i, \mathrm{y}_i)\}$ be a labeled training dataset where $y_i$ is one of $C$ classes. Further, let $F:x\rightarrow [0,1]^C$ be a model (e.g., DNN) trained on $\mathcal{X}_{\ell}$ that outputs softmax probabilities over $C$ classes. 
GEPs utilize scores computed from model features on the target data distribution, which are expected to be correlated with dataset performance. We focus on popular sample-level scoring functions,  $\mathrm{S}(\mathrm{x}; \mathrm{F}) \rightarrow \mathbb{R}$, and define them below.

\begin{itemize}[leftmargin=*]
% \itemsep0em 
    \item \textbf{Confidence} (\conf) \cite{Guillory21_DoC}. We can directly obtain a sample-level score from $F$ by using the maximum softmax probability (e.g., the predicted class's confidence): 
    \begin{equation}
    \mathrm{S}(\mathrm{x};\mathrm{F}) = \max \mathrm{F(x)}.
    \end{equation}
    
    \item \textbf{Local Manifold Smoothness} (\lms) \cite{Ng22_LocalManifoldSmoothness}. Let $q(\mathrm{x}^{\prime}|\mathrm{x})$ be a local probability distribution over augmented samples, $\mathrm{x}^{\prime}$, that can be generated from a given natural sample, $\mathrm{x}$. Then, the LMS score is defined as $\mathrm{S}(\mathrm{x}) = \mathbb{E}_{\mathrm{x}^{\prime} \sim q(\mathrm{x}^{\prime}|\mathrm{x})} \quad \mathbb{I}[\mathrm{F}(\mathrm{x}^{\prime}) =\mathrm{F}(\mathrm{x})]$, where $\mathbb{I}$ is the indicator function.
     Note, in practice, the expectation over $q$ is approximated by sampling set of $k$ augmented samples $\mathrm{X}^{\prime} \coloneqq \{\mathrm{x_j}^{\prime} \sim q(\mathrm{x}^{\prime}|\mathrm{x})\}_{j=1}^{k}$ and we define $q$ using RandAug \cite{Cubuk20_RandAug}.  
    \begin{equation}
    \mathrm{S}(\mathrm{x};\mathrm{F}) \approx \frac{1}{k}\sum_{j=1}^k \quad \mathbb{I}[\mathrm{F}(\mathrm{x_j}^{\prime}) =\mathrm{F}(\mathrm{x})].
    \end{equation}
    
    \item \textbf{Model Agreement} (\gde) \cite{Jiang19_GenGap,Jiang22_Disagreement, Chen21_SelfTrainEnsemb}. Let $F_{0\dots r}$ be a set of $r$ independently trained models (e.g., models are trained using $r$ different seeds). WLOG, let $F_0$ be the base model for which the accuracy is estimated. Then the score can be computed as: 
    \begin{equation}
    \mathrm{S}(\mathrm{x};\mathrm{F}) = \frac{1}{r-1} \sum_{i=1}^r \mathbb{I}[\mathrm{F_0}(\mathrm{x}) =\mathrm{F}_r(\mathrm{x})]
    \label{eq:ma}
    \end{equation} 
\end{itemize}

Given these sample-level scores, GEPs then estimate the performance of deep neural networks (DNNs) under a wide-variety of distribution shifts and are defined as follows.
\begin{definition}
For an unlabeled dataset $\mathcal{X}_u$, a generalization error predictor $\mathrm{G}(\mathcal{X}_u; \mathrm{S})$ returns the estimated error of the pretrained classifier $\mathrm{F}$, based on the scores from $\mathrm{S}(\mathcal{X}_u; F)$.
\vspace{-5pt}
\end{definition}

State-of-the-art approaches propose to curate multiple (labeled) calibration datasets or train multiple models (with different hyper-parameters) in order to construct a well-calibrated GEP~\cite{Deng21_labelsnecessary,Deng21_RotPred,Guillory21_DoC,Lakshminarayanan17_DeeEns}. However, it can be difficult to obtain such calibration datasets in practice and training multiple models is expensive. Moreover, using such strategies can obfuscate the effectiveness of a given scoring function. Therefore, we focus on a simple, popular \textit{thresholding-based} GEP. This GEP simply aggregates thresholded sample-level scores to obtain a dataset-level estimate: $\frac{1}{|X|}\sum_i \mathbb{I}(\mathrm{S}(\bar{\mathrm{x}}_i; \mathrm{F}) > \tau)$, where the threshold hyperparameter $\tau$ is identified by regressing $\mathrm{G}$ to recover the true accuracy on a pre-defined, validation dataset. Given this fixed GEP, we are ready to assess the performance of different scoring functions in a fair setting.  