\pt{Introduce source vs. target lingo. Explain what it means to predict a score (MAE).} We consider representative generalization gap predictors from three popular and effective families, namely disagreement-, confidence- and smoothness- based featurizations, which are capable of providing scores for individual samples. Purely distribution level predictors are excluded as they lack this granularity. 

\begin{itemize}
    \item \textbf{Disagreement.} Recently, it was demonstrated that the disagreement rate between two DNNs trained from different seeds is effective at predicting the generalization gap\cite{Jiang22_Disagreement,Chen21_SelfTrainEnsemb} and this is due to the well-calibrated nature of DNNs ensembles\cite{Bansal, Balaji}. Here, we compute the score for an individual sample as the fraction of models that disagree for a given point. \pt{Explain in equation?} 
    \item \textbf{Confidence.} \cite{Guillory21_DoC} recently found that the difference of the average confidences between target distribution and source distributions can be an effective predictor of the generalization gap as it accurately reflects uncertainty between distributions. However, as we require a sample-level score, we directly use the confidence of a given sample.  
    \item \textbf{Smoothness.} \cite{Ng22_LocalManifoldSmoothness} argue that samples whose predictions remain smooth under input-space perturbations are more likely to be predicted correctly. They propose computing the agreement between different augmentations of a given test sample to approximate the \textit{local manifold smoothness} (LMS).  We directly use LMS as per-sample scores.
\end{itemize}

We focus on methods that provide sample-level scores so that we can directly predict the generalization gap without relying upon many additional datasets or regressors. Indeed, distribution-level predictors often require additional datasets to calibrate regressors which predict the accuracy (gap) given some distribution-level score. Not only is it difficult to obtain such calibration datasets in practice, using a regressor obfuscates the utility of a particular scoring mechanisms, e.g., is the method of only effective because it was well-calibrated on the additional data? To this end, we opt for a simple thresholding mechanism that allows us to fairly evaluate the benefits of a given score. 

Specifically, we use a threshold that determines whether a sample with a given score should be considered as correct or incorrect. Then, predicted accuracy is simply the portion of samples with scores above the threshold. We use the validation source dataset to greedily compute the threshold that minimizes the mean absolute prediction error. 

% Meanwhile, \cite{Guillory21_DoC} argue that using the difference of confidences, in lieu of distances or average confidence directly, is more effective, as using the relative confidence between distributions better reflects uncertainty and, consequently generalization.  Alternatively, \cite{Jiang22_Disagreement,Chen21_SelfTrainEnsemb} find that the disagreement rate between two DNNs trained from different seeds is sufficient to predict the accuracy and demonstrate this is due to the well-calibrated nature of DNNs ensembles. Concurrently, \cite{Ng22_LocalManifoldSmoothness} find that classifier smoothness over the local data manifold is an effective feature that allows for black-box gap prediction.
% \begin{itemize}
%     \item We consider 3 types of popular generalization gap predictors. 
%     \item Disagreement 
%     \item Confidence
%     \item Smoothness
%     \item Provide the exact score definition here. 
%     \item Not using regressors. 
%     \item Why b/c it requires lot of datasets. Can be impractical.  
%     \item Also, lets us compare the value of the score separately from the regressor's effect.
%     \item Explain the thresholding on validation dataset.
% \end{itemize}