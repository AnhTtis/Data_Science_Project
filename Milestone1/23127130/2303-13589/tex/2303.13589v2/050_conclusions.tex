In this work, we rigorously studied the design of scoring functions in GEPs and found that their choice is critical to produce consistently reliable predictors across different distribution shifts and noise corruptions. In fact, when the GEP construction does not involve calibration datasets or training a large family of models, even state-of-the-art scoring functions such as \conf~and \lms~can struggle. In comparison, we found \gde~to be a more reliable alternative. 
Furthermore, using our new \gde~variant, we demonstrated that improving diversity of the ensemble leads to well-calibrated GEPs, while incurring only a small drop in target accuracies. Finally, in a controlled empirical setting, we showed how reliance on simple features can adversely affect the GEP performance. 