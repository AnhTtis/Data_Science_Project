As discussed in Sec. \ref{sec:intro}, it is critical to disentangle the scoring function from the GEP mechanism to understand the former's effectiveness. Using a fixed thresholding-based GEP, we evaluate the ability of different scoring functions to accurately predict generalization over various distribution shifts (Sec. \ref{sec:gep_dist}) and under the realistic setting of training on low fidelity data (Sec. \ref{sec:corrupted_training}). 

\noindent \textit{Experimental Setup.} For all experiments, CIFAR10 is the source distribution on which we train ResNet-18 for 200 epochs with lr=0.05. STL10, CIFAR10.1 and CIFAR-10-C~\cite{Hendrycks19_CIFAR10C} are the target distributions, for which we estimate the generalization performance. CIFAR10.1 and STL10 represent near and far distribution shifts respectively. CIFAR-10-C contains samples generated from $15$ different naturalistic corruptions, such as ``fog" or ``blur", applied at five severity levels. Increased severity corresponds to increased shift from the training data. In all experiments, we report the mean absolute error (MAE) between the true target accuracy and the predicted target accuracy. The threshold, $\tau$, is determined by optimizing the thresholding function to minimize prediction error on the CIFAR10 validation dataset. Note that all results are averaged over $10$ seeds. We compute \gde~using a 10-member ensemble and use $10$ augmentations (RandAug~\cite{Cubuk20_RandAug}) to compute \lms.

\input{031_clean_data}

\subsection{GEP Performance under Distribution Shifts}\label{sec:gep_dist}
Results are shown in Table \ref{table:clean_data}. We make the following observations. Across all target datasets, and corresponding levels of distribution shifts, \gde~is by far the most effective at predicting generalization. For example, on the challenging STL10 benchmark, \gde~achieves improvements of $10\%$ and $30\%$ over \conf~and \lms~scores respectively. Similarly, \gde~provides consistent gains at all corruption levels on CIFAR-10-C. Notably, while \lms~was originally proposed in the context of sample-level scoring, it trails behind
\begin{wrapfigure}{r}{0.5\columnwidth}
\vspace{-0.1in}
\begin{center}
\includegraphics[width=0.5\columnwidth]{FIGS/NumCls.pdf}
\end{center}
\vspace{-0.2in}
\caption{\textbf{Effect of ensemble size on \gde~performance.}
\vspace{-0.1in}
}
\label{fig:num_classifier}
\end{wrapfigure}\conf, which was shown to more effective as a distribution-level scoring function by \cite{Guillory21_DoC}. While we used RandAug as it was shown to be effective by \cite{Ng22_LocalManifoldSmoothness}, our results here indicate that smoothness to such perturbations can fail to capture properties relevant for generalization under the real-world corruptions and shifts. This suggests that the choice of augmentation strategy is critical to \lms's effectiveness, and it is non-trivial to identify such a strategy without access to \textit{o.o.d} data.

Given that \gde~requires multiple independently trained models, we evaluate the effect of ensemble size in generating reliable scores. As shown in Fig. \ref{fig:num_classifier}, we find that the performance of \gde~begins to saturate at 4 models, though further increase in the ensemble size reduces the variability. In Sec. \ref{sec:corrupted_training}, we will show that the diversity
of the ensemble also plays an important role on GEP performance, particularly under challenging training conditions.

\input{dirty_data_table}

\subsection{Impact of Training Data Fidelity on GEP}\label{sec:corrupted_training}
In the preceding section, we evaluated the effectiveness of different scoring functions on a large family of image-level corruptions and distribution shifts. While we found \gde~and \conf~to be particularly effective, here, we seek to further evaluate their efficacy in the realistic, but more challenging setting where \textit{training data} may be compromised. Specifically, we consider \textit{label noise, measurement errors and sampling discrepancies} as sources of low-fidelity training data. 
We focus on these particular sources as they not only represent situations that are likely to be encountered in practical scenarios, but they also compromise the training data at different granularities, namely sample-, dataset-, and distribution- level. 
Given that GEPs are used as failure indicators, it is critical to evaluate the scoring functions in such settings. Additionally, we evaluate a variant of \gde~designed to increase ensemble diversity and further improve its performance. %Below, we introduce the experimental set-up and present our observations.

\noindent \textit{Experimental Setup}. The following processes are used to create compromised data: (i) \textbf{Label Noise:} We randomly select 5\% of the training set and randomly flip their labels. (ii) \textbf{Measurement Noise:} We first apply a Gaussian blur ($\sigma_1 = 0.5$) and then add standard normal noise ($\sigma_2 = 0.07$) to all training images; (iii) \textbf{Under-Sampling:} 20\% of the samples are randomly dropped from the \textit{automobile}, and \textit{bird} classes. Given these compromised datasets, we follow the same experimental setup introduced in Sec. \ref{sec:method}. However, models are now trained for 250 epochs, instead of 200 epochs, to achieve acceptable convergence. In the following analysis, we specifically focus on the near (CIFAR10.1) and far (STL10) distribution shift settings respectively.

\noindent \textbf{\gde~with improved diversity}. Motivated by the saturating effect of ensemble size in Fig. \ref{fig:num_classifier}, we propose a simple variant to \gde~that is designed to improve the diversity of the ensemble by synthetically corrupting the data using low levels of label noise. Here, the intuition is that such label noise requires models to learn slightly different functions to effectively minimize the loss on the mis-labeled samples. Since these randomly-labeled points account for a small portion (2\%) of the overall dataset, they generally do not substantially affect the ensemble accuracy (see Table \ref{tab:data_fidelity}). We denote this variant, \gde$_{0.02}$, and discuss its behavior, in addition to other scoring functions below (see Table \ref{tab:data_fidelity}).

\noindent {\ul Obs. 1}: \gde~and \conf~remain considerably more effective than \lms~even with compromised data. This is to be expected as the considered data infidelities are not expected to change the type of smoothness that is indicative of generalization on STL10 and CIFAR10.1.
\\
\noindent {\ul Obs. 2}: While adding measurement noise (MN) and under-sampling (US) does minimally harm the true target accuracy, we see GEP performance of \lms~and \conf~improves on STL10--even better than their performance obtained by training on the clean dataset. We posit in Sec. \ref{sec:simp_bias} that this can be attributed to decreased reliance upon simple features that lead to over-confident, but ultimately misleading, scores.
\\
\noindent {\ul Obs. 3}: In contrast, on the near \textit{o.o.d} setting of CIFAR10.1, we not only see that the true target accuracy decreases, but also that GEP performance decreases across all methods relative to training on the clean dataset. Given the closeness of CIFAR10.1 to CIFAR10, it is likely that low fidelity data struggles to capture the features necessary for generalization or estimation on the target distribution. 
\\
\noindent {\ul Obs. 4}: Incurring only a small drop in the true target accuracy, our variant \gde$_{0.02}$ improves GEP performance on STL (over 50\% on CIFAR10, CIFAR10 (MN), CIFAR10 (US)), and maintains comparable GEP performance on the near \textit{o.o.d} setting. While target accuracy does decrease noticeably on CIFAR10 (LN), we note that this is an edge case, as the underlying dataset has already been perturbed by label noise. The efficacy of \gde$_{0.02}$ on CIFAR10.1 is surprising, as the perturbed distributions are certainly further from target than the clean source distribution, but this suggests diversity plays a critical role in obtaining effective scores. 
