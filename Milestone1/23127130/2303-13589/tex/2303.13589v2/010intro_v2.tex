Safe deployment of machine learning models requires suitable failure indicators so that models whose performance falls below an acceptable tolerance can be temporarily pulled from production.
While learning-theoretic complexity measures can be used to estimate model performance under \textit{i.i.d} assumptions~\cite{Jiang20_FantasticGen, Garg22_UnlabeldPred}, they are currently insufficient for estimating model generalization on \textit{out of distribution (o.o.d)} data.  
To this end, generalization error predictors (GEPs), which are designed to estimate performance on \textit{arbitrary} target datasets, have become popular~\cite{Ng22_LocalManifoldSmoothness,Guillory21_DoC,Chen21_Mandoline,Deng21_labelsnecessary,Djurisic22_ActivationShaping}.

\begin{figure}
    \centering
    \includegraphics[width = 0.85\columnwidth]{FIGS/overview_v2.png}
    \caption{\textbf{Generalization Error Prediction.} We focus on the problem of generalization error prediction with classifiers and study the design of scoring functions under various distribution shifts, corruptions, and data fidelity issues.}
    \label{fig:overview}
    \vspace{-0.3cm}
\end{figure}

In brief, GEPs \textit{aggregate} sample-level \textit{scores} to predict generalization error on unlabeled target datasets (see Fig. \ref{fig:overview}).
Popular scoring functions, such as manifold proximity \cite{Deng21_labelsnecessary}, confidence estimates \cite{Guillory21_DoC}, local manifold smoothness \cite{Ng22_LocalManifoldSmoothness}, and agreement between independently trained models \cite{Jiang22_Disagreement,Chen21_SelfTrainEnsemb,Nakkiran20_DistGen}, attempt to measure the likelihood that the predicted label of a sample is correct. 
GEPs then use different mechanisms (thresholding functions, regressors, calibration datasets) to create dataset-level error estimates from the provided scores. 
However, since these mechanisms can vary in complexity, the efficacy of a particular scoring functions can obfuscated. %by the choice of mechanism. 
For example, state-of-the-art GEPs often rely upon multiple, labeled calibrated datasets~\cite{Deng21_labelsnecessary,Deng21_RotPred,Guillory21_DoC,Lakshminarayanan17_DeeEns}, which can provide additional information that bolsters the performance of otherwise subpar scoring functions.
Therefore, in this paper, we use a simple, fixed GEP, and rigorously study the effectiveness of popular scoring functions (confidence, local manifold smoothness, model agreement) in several realistic settings and identify a potential cause of poor GEP performance.

\noindent \textbf{GEP performance under distribution shifts (Sec. \ref{sec:gep_dist}):} Using a large family of image-level corruptions and distribution shifts, we benchmark the three scoring functions and provide key insights on their efficacy in practice.

\noindent \textbf{Impact of training data fidelity on GEP performance (Sec. \ref{sec:corrupted_training}):} Scoring functions directly depend on data and model properties. Therefore, we consider common data fidelity issues (label noise, measurement errors and sampling discrepancies) to study what role data quality plays on the GEP performance. 

\noindent \textbf{Effect of simplicity bias on GEP performance (Sec. \ref{sec:simp_bias}):} Deep neural networks are susceptible to relying upon simple, spurious features \cite{Shah20_SimplicityBias} at the expensive of robust generalization. We study the impact of this behavior on the efficacy of different scoring functions \cite{Shah20_SimplicityBias,Trivedi22_Adaptation}.
