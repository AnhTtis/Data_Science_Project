\subsection{Motivation and overview}

The method of score distillation-based text-to-3D generation~\citep{poole2022dreamfusion}, despite its effectiveness, currently possesses a crucial problem: inconsistent and distorted geometry in generated scenes due to 2D diffusion models not having explicit awareness of the 3D space nor the camera viewpoint it is optimizing from~\citep{wang2023prolificdreamer}. Previous works~\citep{poole2022dreamfusion, wang2022score} attempt to circumvent this problem by adding the text prompts that roughly describe the camera viewpoint (\textit{e.g.}, ``\textit{side view}''). However, this ad-hoc approach is severely limited, as a wide range of different pose values is ambiguously represented by few text prompts, leaving the generation process still vulnerable to geometric inconsistencies and severe deformations. Therefore, to overcome this problem, the objective boils down to how we can inject more specific and explicit 3D awareness into pretrained 2D diffusion models.

Our method, \ours, achieves this objective by conditioning a 2D diffusion model to projected depth maps of coarsely generated 3D structures. Specifically, we first conduct semantic code sampling, in which we initially generate an image $\hat{x}$ from the given text prompt to specify the semantics of the 3D scene we wish to generate (Sec.~\ref{method:semcode}). From this image $\hat{x}$, we construct a coarse point cloud using an off-the-shelf point cloud generation model~\citep{nichol2022point, wu2023multiview}. For every rendering view, we leverage its projected depth maps as explicitly 3D consistent conditions for the 2D diffusion model. As the sparse depth map contains rich 3D information describing the scene from a given viewpoint, this approach effectively enables the diffusion model to generate the 3D scene in a 3D-aware manner (Sec.~\ref{method:sparsedepth}). We further leverage the semantic code through low-rank adaptation (LoRA)~\citep{hu2021lora} layers to ensure semantic consistency of the scene (Sec.~\ref{method:pivotal}). The overall architecture of \ours is described in Fig.~\ref{fig:network_overall}.

% \vspace{-13pt}
\subsection{Semantic code sampling}
% \vspace{-5pt}
\label{method:semcode}

We focus on that fact that when generating a 3D scene from a text, inherent ambiguity exists within the text prompt. For instance, the text prompt ``\textit{a cute cat}'' is ambiguous in regards to color, as it could refer to either a black or a white cat. Such lack of specificity leaves possibility for the 3D scene to be erroneously guided toward vastly different textures and semantics at different viewpoints, resulting in a semantically inconsistent 3D scene overall, as further demonstrated in Sec.~\ref{abl:semcode}.

We introduce a simple yet effective technique to reduce such text prompt ambiguity called semantic code sampling. To specify and secure the semantic identity of the scene we want to optimize towards, a 2D image $\hat{x}$ is generated from the text prompt $c$. Then, we optimize the text prompt embedding $e$ to better fit the generated image, similarly to the textual inversion~\citep{gal2022image}:
\begin{equation}
\vspace{-5pt}
\hat{e} = \underset{e}{\mathrm{argmin}} \ || {\epsilon}_{\theta}(\hat{x}_t,{e}) - {\epsilon} ||^2_2,
 \label{equation:embedding_optimize}
\vspace{-5pt}
\end{equation}
where $\hat{x}_t$ is a noised image of the generated image $\hat{x}$ with the noise $\epsilon$ and the noise level $t$. We refer to this pair of the generated image $\hat{x}$ and the optimized embedding $\hat{e}$ as semantic code ${s}$, \textit{i.e.}, $s := (\hat{x}, \hat{e})$, which would be input for our consistency injection module. 

\vspace{-5pt}


\subsection{Incorporating a coarse 3D prior}
\label{method:3dprior}

Our main objective is injecting 3D awareness into a pretrained diffusion model to enhance the 3D consistency of generated scenes, while fully leveraging the 2D diffusion model's expressive generative capabilities. We achieve this with a novel consistency injection module, which conditions diffusion models on sparse depth projections of a constructed point cloud. 
         
Specifically, we employ an off-the-shelf 3D model $\mathcal{D}(\cdot)$ to construct a 3D point cloud from the initial image $\hat{x}$ included in the semantic code ${s}$. $\mathcal{D}(\cdot)$ can be chosen from a wide variety of models: it could be a point cloud generative model such as Point-E~\citep{nichol2022point} or a single-image 3D reconstruction model such as MCC~\citep{wu2023multiview}. For every SDS optimization step, as an image of the 3D scene is rendered for the diffusion model at camera viewpoint $\pi$, the constructed point cloud is projected to the same viewpoint $\pi$ resulting in a point cloud depth map $p$:
\begin{align}
 {p} = \mathcal{P}(\mathcal{D}(\hat{x}),\pi),
 \label{equation:projection}
\end{align}
where $\mathcal{P}(\cdot,\pi)$ is a depth-projection function with a camera pose $\pi$. Adopting the architecture of ControlNet~\citep{zhang2023adding}, our \textit{sparse depth injector} $\mathcal{E}_\phi$ receives the sparse depth map $p$, and we add its output features to the intermediate features within pretrained diffusion U-net of $\epsilon_\theta \bigl(\hat{x}_t,\hat{e}\bigr)$ in a residual manner, which can be further formulated as $\epsilon_\theta \bigl(\hat{x}_t,\hat{e},\mathcal{E}_\phi(p) \bigr)$. 

The effectiveness of this approach can be understood from multiple aspects: most importantly, the sparse depth map $p$, explicitly modeled in 3D, provides the 2D diffusion model with 3D consistent outline of the scene it is expected to generate, effectively encouraging 3D geometric consistency in the generation process. This approach also adds much-needed \textit{controllability} to text-to-3D generation pipeline: because our method enables overall shape of the scene to be decided before the lengthy SDS optimization process, the user can pick and choose from a variety of initial point cloud shapes, allowing one to more easily generate specific 3D scenes tailored to their needs.

\vspace{-5pt}

\begin{wrapfigure}{r}{0.5\textwidth}\vspace{-15pt}
  \begin{center}
    \setlength\tabcolsep{0.8pt}
    {
    % \renewcommand{\arraystretch}{0.5}
    % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{cccccccc}
     \includegraphics[width=0.24\linewidth]{fig/depthcomp_a} &
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_b} &
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_c} &
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_d} \\
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_a2} &
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_b2} &
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_c2} &
      \includegraphics[width=0.24\linewidth]{fig/depthcomp_d2} \\
       (a) & (b) & (c) & (d) \\
    \end{tabular} }
    % }
  \end{center}
\vspace{-10pt}
\caption{\textbf{Qualitative results conditioned on the sparse depth map.} Given sparse depth maps (a), (b) are results of depth-conditional Stable Diffusion, (c) are results of ControlNet trained on MiDaS depths only, and (d) are \ours results. Given text prompts are ``\textit{a front view of an owl}'' and ``\textit{a majestic eagle}''.}
\label{fig:depthcomp} 
\vspace{-20pt}
\end{wrapfigure}

\paragraph{Training the sparse depth injector.}
\label{method:sparsedepth}
As the sparse point cloud obtained by the off-the-shelf 3D model inevitably contains errors and artifacts, its depth map also displays artifacts, shown in Fig.~\ref{fig:depthcomp}(a). Therefore, our module must be able to handle the inherent sparsity and the errors present in the projected depth map. 

To this end, we employ two training strategies for our sparse depth injector $\mathcal{E}_\phi(\cdot)$. First, we train our sparse depth injector using a paired dataset of real-life images and their point cloud depths~\citep{reizenstein2021common}. Our model is trained to generate the real-life images given its point cloud depth as condition, enabling it to infer dense structures from sparse geometry given in point cloud depths. To increase the robustness of our model against errors and artifacts, we impose augmentations by randomly subsampling and adding noisy points to the point clouds from which the depth maps originate.

Second, the injector $\mathcal{E}_\phi(\cdot)$ is also trained on dense depth maps of text-to-image pairs, predicted using MiDaS~\citep{ranftl2020towards}. This strengthens the modelâ€™s generalization capability, enabling it to infer dense structural information from categories not included in the 3D point cloud dataset for sparse depth training. In combination, given the depth map $p$ along with the corresponding image $y$ and caption $c$, the training objective of the depth injector $\mathcal{E}_\phi(\cdot)$ is as follows:
\begin{equation}
    \mathcal{L}_\mathrm{inject}(\phi)= \mathbb{E}_{y,c,p,t,\epsilon}\Bigl[ || \epsilon_\theta \bigl(y_t,c,\mathcal{E}_\phi(p) \bigr)-\epsilon ||^2_2 \Bigr].
\end{equation}
Note that only the depth injector $\mathcal{E}_\phi(\cdot)$ is trained while the diffusion model remains frozen, making the training process more efficient, akin to finetuning. 

These training strategies enable our model to receive sparse and noisy depth maps directly as input and successfully infer dense and robust structural information without needing any auxiliary network for depth completion. As shown in Fig.~\ref{fig:depthcomp}, our approach generates realistic results without being restricted to the domain of the point cloud dataset. Note the category of images (birds) used for the experiment is not included in the category of the point cloud dataset we use.

\begin{figure}[t]
\newcolumntype{M}[1]{>{\raggedright \arraybackslash}m{#1}}
\setlength{\tabcolsep}{0.8pt}
\renewcommand{\arraystretch}{0.4}
\centering
\small
\begin{tabular}{m{0.02\linewidth}M{0.157\linewidth}M{0.157\linewidth}@{\hskip 0.01\linewidth}M{0.157\linewidth}M{0.157\linewidth}@{\hskip 0.01\linewidth}M{0.157\linewidth}M{0.157\linewidth}}

\rotatebox[origin=cl]{90}{{ProlificDreamer}} &
     \includegraphics[width=\linewidth]{fig/prolific/pro_bed1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_bed2.png} &
      \includegraphics[width=\linewidth]{fig/PD/PD_33_90.png} &
      \includegraphics[width=\linewidth]{fig/PD/PD_33_135.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_lion2.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_lion1.png} \\
      
      
%      +ours &
\rotatebox[origin=cl]{90}{\textbf{+\ours}} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_bed1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_bed2.png} &
      \includegraphics[width=\linewidth]{fig/PD/Ours_33_90.png} &
      \includegraphics[width=\linewidth]{fig/PD/Ours_33_135.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_lion1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_lion2.png} \\ \\


       & \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a photo of \\ a comfortable bed"}}} & 
       \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a lantern with handle"}}} & 
       \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a zoomed out DSLR photo of a  \\ ceramic lion, white background"}}} \\ \\

%%%%%%%%% 2nd row %%%%%%%%%
\rotatebox[origin=cl]{90}{{ProlificDreamer}} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_motorcycle1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_motorcycle2.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_ironman1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_ironman2.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_tiger1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_tiger2.png} \\
      
      
%      +ours &
\rotatebox[origin=cl]{90}{\textbf{+\ours}} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_motorcycle1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_motorcycle2.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_ironman1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_ironman2.png} &      
      \includegraphics[width=\linewidth]{fig/prolific/ours_tiger1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_tiger2.png} \\ \\

       & \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a DSLR photo of \\ an origami motorcycle"}}} & 
       \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a DSLR photo of \\ an ironman figure"}}} & 
       \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a DSLR photo of an \\ a robot tiger"}}} \\ \\

%%%%%%%%% 3rd row %%%%%%%%%


\rotatebox[origin=cl]{90}{{ProlificDreamer}} &
     \includegraphics[width=\linewidth]{fig/prolific/pro_shuttle2.png} &
      \includegraphics[width=\linewidth]{fig/prolific/pro_shuttle1.png} &
      \includegraphics[width=\linewidth]{fig/PD/PD_31_45.png} &
      \includegraphics[width=\linewidth]{fig/PD/PD_31_135.png} &
      \includegraphics[width=\linewidth]{fig/PD/PD_20_45.png} &
      \includegraphics[width=\linewidth]{fig/PD/PD_20_135.png} \\
      
      
%      +ours &
\rotatebox[origin=cl]{90}{\textbf{+\ours}} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_shuttle1.png} &
      \includegraphics[width=\linewidth]{fig/prolific/ours_shuttle2.png} &
      \includegraphics[width=\linewidth]{fig/PD/Ours_31_45.png} &
      \includegraphics[width=\linewidth]{fig/PD/Ours_31_135.png} &
      \includegraphics[width=\linewidth]{fig/PD/Ours_20_45.png} &
      \includegraphics[width=\linewidth]{fig/PD/Ours_20_90.png} \\ \\

       & \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a DSLR photo of \\ a Space Shuttle"}}} & 
       \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a dragon-shaped teapot"}}} & 
       \multicolumn{2}{c}{\parbox{0.30\linewidth}{\centering \textit{``a full size figure of \\ princess wearing tiara"}}} \\ \\
       %& (a) & (b) & (c) & (d)\\      
    \end{tabular} 
    
    \vspace{-5pt}
    \caption{\textbf{Qualitative comparisons.} 
    We demonstrate the effectiveness of our approach on a ProlificDreamer~\citep{wang2023prolificdreamer} baseline. Incorporation of \ours framework drastically enhances 3D consistency and fidelity of generated scenes.
    }
    \label{fig:qual_one} 
    \vspace{-15pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\newcolumntype{M}[1]{>{\raggedright \arraybackslash}m{#1}}
\setlength{\tabcolsep}{0.8pt}
\renewcommand{\arraystretch}{0.4}
\centering
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{m{0.02\linewidth}M{0.157\linewidth}M{0.157\linewidth}M{0.157\linewidth} @{\hskip 0.01\linewidth}M{0.157\linewidth}M{0.157\linewidth}M{0.157\linewidth}}

\rotatebox[origin=cl]{90}{{Dreamfusion}} &
     \includegraphics[width=\linewidth]{fig/DF/DF_hippo_1.png} &
      \includegraphics[width=\linewidth]{fig/DF/DF_hippo_2.png} &
      \includegraphics[width=\linewidth]{fig/DF/DF_hippo_3.png} &
      \includegraphics[width=\linewidth]{fig/DF/DF_heels_1.png} &
      \includegraphics[width=\linewidth]{fig/DF/DF_heels_2.png} &
      \includegraphics[width=\linewidth]{fig/DF/DF_heels_3.png} \\
      
      
%      +ours &
\rotatebox[origin=cl]{90}{\textbf{+\ours}} &
      \includegraphics[width=\linewidth]{fig/DF/Ours_hippo_1.png} &
      \includegraphics[width=\linewidth]{fig/DF/Ours_hippo_2.png} &
      \includegraphics[width=\linewidth]{fig/DF/Ours_hippo_3.png} &
      \includegraphics[width=\linewidth]{fig/DF/Ours_heels_1.png} &
      \includegraphics[width=\linewidth]{fig/DF/Ours_heels_2.png} &
      \includegraphics[width=\linewidth]{fig/DF/Ours_heels_3.png} \\ \\

       & \multicolumn{3}{c}{\textit{``a photo of cute hippo"}} & \multicolumn{3}{c}{\textit{``black kill heels"}} \\ \\

\rotatebox[origin=cl]{90}{{SJC}} &
     \includegraphics[width=\linewidth]{fig/SJC/SJC_sheep_1.png} &
      \includegraphics[width=\linewidth]{fig/SJC/SJC_sheep_2.png} &
      \includegraphics[width=\linewidth]{fig/SJC/SJC_sheep_3.png} &
      \includegraphics[width=\linewidth]{fig/SJC/SJC_deer_1.png} &
      \includegraphics[width=\linewidth]{fig/SJC/SJC_deer_2.png} &
      \includegraphics[width=\linewidth]{fig/SJC/SJC_deer_3.png} \\
      
      
%      +ours &
\rotatebox[origin=cl]{90}{\textbf{+\ours}} &
      \includegraphics[width=\linewidth]{fig/SJC/Ours_sheep_1.png} &
      \includegraphics[width=\linewidth]{fig/SJC/Ours_sheep_2.png} &
      \includegraphics[width=\linewidth]{fig/SJC/Ours_sheep_3.png} &
      \includegraphics[width=\linewidth]{fig/SJC/Ours_deer_1.png} &
      \includegraphics[width=\linewidth]{fig/SJC/Ours_deer_2.png} &
      \includegraphics[width=\linewidth]{fig/SJC/Ours_deer_3.png}  \\ \\

    & \multicolumn{3}{c}{\textit{``a cute sheep with white fur"}} & \multicolumn{3}{c}{\parbox{0.48\linewidth}{\centering \textit{``a gentle deer with a spotted coat \\ with a peaceful expression"}}} \\ \\




       %& (a) & (b) & (c) & (d)\\ 
    \end{tabular} 
    
    \vspace{-5pt}
    \caption{\textbf{Adaptation to other baselines.} 
    We demonstrate the effectiveness our approach on Stable-DreamFusion~\citep{poole2022dreamfusion} and SJC~\citep{wang2022score} baselines. Incorporation of \ours drastically enhances 3D consistency and fidelity of generated scenes.
        }
    \label{fig:qual_two} 
    \vspace{-10pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%
\vspace{-5pt}
\subsection{Improving semantic consistency}
\vspace{-5pt}
\label{method:pivotal}
To result a scene semantically consistent at all viewpoints, the diffusion model should produce a score that maintains certain degree of semantic consistency regardless of given viewpoints. Although optimized embedding $\hat{e}$ partially achieves this, we further enhance this effect by adopting LoRA~\citep{hu2021lora} technique motivated by \citep{lora_diff}. LoRA layers with parameters $\psi$ consist of linear layers inserted into the residual path of the attention layers in the diffusion U-net. Training the LoRA layers in this manner instead of the entire diffusion model further helps it avoid overfitting to a specific viewpoint. In practice, given an image $\hat{x}$ generated from text prompt $c$, we fix the optimized embedding $\hat{e}$ and tune the LoRA layers $\psi$~\citep{roich2022pivotal}:
\begin{equation}
    \mathcal{L}_\mathrm{LoRA}(\psi) =\mathbb{E}_{\epsilon,t}\Bigl[ || \epsilon_{\theta,\psi}(\hat{x}_t,\hat{e},\mathcal{E}_\phi(p)) - \epsilon ||^2_2 \Bigr].
\end{equation}
\vspace{-10pt}