\begin{appendix}

% \addcontentsline{toc}{section}{Appendix} %
% \part{Appendix} 

{\huge Appendix}

% \pagebreak
In Sec.~\ref{supp:details}, we provide additional implementation details for our proposed method, \ours. In Sec.~\ref{supp:exps}, we present the results of additional experiments to validate our approach. In Sec.~\ref{supp:eval}, we provide details regarding the user study and the evaluation of our method. In Sec.~\ref{supp:limitation}, we discuss the limitations of our approach.


\section{Implementation details}
\label{supp:details}
\subsection{Training details} 
We use the Co3D~\citep{reizenstein2021common} dataset to train our sparse depth injector. The dataset is comprised of 50 categories and 5,625 annotated point cloud videos. From these point cloud-annotated videos, we randomly subsample three frames to create 16,875 pairs of RGB images and their projected depth map. We use 3k-5k points, augmented with 0-10\% noise points, sampled from the dense point clouds provided in the Co3D dataset. We employ the rasterizer from PyTorch3D~\citep{ravi2020pytorch3d} library to project the point clouds and synthesize sparse depth maps. To relieve domain constraints from the Co3D dataset, we also employ text-to-image pairs along with MiDaS~\citep{ranftl2020towards}-predicted dense depth maps. For ease of training, we start from the weights~\citep{zhang2023adding} trained on the text-to-image pairs with MiDaS depth and fine-tune the model using the sparse depth maps synthesized from the Co3D dataset for 2 additional epochs.
% \vspace{-10pt}

\subsection{Intensity controls}
All networks equipped with a pretrained diffusion model in our framework are designed to add external features to the intermediate features of the diffusion U-Net in a residual manner. Therefore, their intensities can be adjusted by multiplying scaling factors. This enables us to control the influence of the LoRA layers, used for semantic consistency, and the sparse depth injector, used for geometric consistency, on the diffusion model. In particular, adjusting the intensities of the LoRA layers is effective in controlling how much our 3D scene overfits to the initial image. We use scaling factors of 0.3 and 1.0 on the features passing through the LoRA layers and the sparse depth injector, respectively. 

\subsection{Semantic code sampling}
To perform semantic code sampling, an initial image is first generated based on the text prompt, followed by optimization of corresponding text prompt embedding. The specific method follows that of Textual Inversion~\citep{gal2022image}, wherein a new word is added to the embedding space and then optimized. Additionally, this initially generated image is used as input for the off-the-shelf 3D model in the auxiliary module. To facilitate the optimization and 3D shape inference processes, we add ``\textit{a front view of}'' and ``\textit{, white background}'' before and after the text prompt.

\subsection{Architectural choices}
% \paragraph{Diffusion models.}
There are manifold text-to-image diffusion models that have been trained at a large scale, including DALL-E2~\citep{ramesh2022hierarchical}, Imagen~\citep{saharia2022photorealistic}, and Stable Diffusion~\citep{rombach2022high}. However, as DALL-E2 and Imagen are not publicly available, we adopt Stable Diffusion, the most popular and widely used model.

% \paragraph{Sparse depth injector.}
To impose an additional condition on pretrained text-to-image diffusion models, various methods such as PITI~\citep{wang2022pretraining} and ControlNet~\citep{zhang2023adding} have been proposed. Recently, Depth-conditional Stable Diffusion~\citep{rombach2022high} has been introduced, which finetunes Stable Diffusion to receive MiDaS~\citep{ranftl2020towards} depth map as a condition for image translation. After conducting experiments with the above methods to find the optimal solution for our setting, we find that ControlNet achieves the best efficiency to train a sparse depth-conditioned diffusion model. We thus incorporate ControlNet architecture into our framework. Fig.~\ref{fig:detailed_architecture} illustrates our architectural choice incorporating ControlNet architecture into our \ours framework.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\linewidth]{fig/appendix/architecture.pdf}
\caption{\textbf{Architectural choices for \ours framework.} The sparse depth injector follows the architecture of ControlNet~\citep{zhang2023adding}, with the encoder side comprised of encoder blocks copied from the diffusion U-Net and the decoder side comprised of convolutional layers. The sparse depth map features are added to the intermediate features in the decoder blocks of the diffusion U-Net. LoRA layers are attached to every attention layer of the diffusion U-Net.}
\label{fig:detailed_architecture}
\vspace{-5pt}
\end{figure*}


\section{Additional experiments}
\label{supp:exps}


\subsection{Comparisons with novel view model-based concurrent works}
In Figure 14, we conduct a comparison of our model with different image-to-3D models~\citep{liu2023zero, qian2023magic123}, we first generate images from a single text prompt (through the Stable Diffusion~\citep{metzer2022latent} model) and give these images Zero-123~\citep{liu2023zero} and Magic-123~\cite{qian2023magic123} for 3D scene generation. This experiment setting can be seen as observing what happens if the image-to-3D component of our \ours methodology is replaced with other existing explicit image-to-3D models. Our \ours undergoes its original generation process with the identical text prompt, and the images given to each model are not curated. Also, to remove the aliasing-like effect in the renderings that occurred due to the feature rendering architecture of SJC, we have changed the backbone NeRF model of Zero-123 and \ours from feature-level SJC to pixel-level Instant-NGP backbone, which allows us to gain clearer results.

The experiment results show that our method’s generative capabilities enable our model to generate more detailed, high-fidelity results in comparison to image-to-3D models that attempt explicit novel view synthesis of conditioning images. We hypothesize the reason for this is that strict image-to-3D models struggle to predict novel views of an image when an image is outside the domain of training data, while our model is loosely conditioned and therefore gives the generative model more freedom to generate realistic details, fully realizing its generative capability. This is in agreement with what we have explained in our comparison Zero-123 in our main paper.


\subsection{Robustness}
In the main paper, we present experimental results of \ours with fixed random seeds for a fair comparison. Here, we conduct an additional experiment to verify the robustness of our approach. We compare our method with previous works~\citep{poole2022dreamfusion, wang2022score} with combinations of fixed text prompts and varying random seeds. Fig.~\ref{fig:qual_robust} demonstrates that our approach exhibits enhanced robustness compared to previous works in terms of stochasticity.





\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\linewidth]{fig/concurrent_comparision.pdf}
\caption{\textbf{Comparisons with novel view model-based concurrent works~\citep{liu2023zero, qian2023magic123}.}
The experiment results show that our method’s generative capabilities enable our model to generate more detailed, high-fidelity results in comparison to image-to-3D models that attempt explicit novel view synthesis of conditioning images.}

\label{fig:ablation_sss}
\vspace{-5pt}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.75\linewidth]{fig/Appendix_Robustness.pdf}
\caption{\textbf{Qualitative comparison for robustness.} Qualitative results with varying random seeds show our model's robustness against stochastic factors. We visualize the results generated with the seed from 0 to 4.}
\label{fig:qual_robust}
\vspace{-5pt}
\end{figure*}

\subsection{Sparsity variation}
 We provide additional analysis of our model's robustness against sparsity and ambiguity of point cloud. We drastically change the number of points in a point cloud and generate images from its depth map. As shown in Fig.~\ref{fig:qual_sparsity_variation}, despite varying sparsity, our model shows consistency in inferring dense structures and generates realistic images.

\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{fig/figB_new.pdf}
\caption{\textbf{Sparsity variation with a fixed random seed.}}
\label{fig:qual_sparsity_variation}
\vspace{-5pt}
\end{figure*}

% \vspace{-10pt}
\subsection{Additional qualitative results}
Fig.~\ref{fig:ablation_sss} displays additional qualitative results. Our qualitative results demonstrate that the outputs of our model have superior robustness and 3D-consistency compared to previous methods. 

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\linewidth]{fig/Appendix_more_qual.pdf}
\caption{\textbf{More qualitative results.} Additional qualitative results show \ours's superior performance in ensuring geometric and semantic consistency.}
% \label{fig:ablation_sparse_supple}
\vspace{-5pt}
\end{figure*}

% \vspace{-10pt}
\subsection{Ablation on sparse depth injector}
We conducted an ablation study regarding the effectiveness of conditional depth maps. Specifically, we remove the sparse depth injector from the diffusion model that receives an optimized embedding with LoRA layers. Fig.~\ref{fig:ablation_sparse} presents the result of our framework with and without the sparse depth injector, confirming its importance in achieving geometric consistency.

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\linewidth]{fig/Abl_sparse.pdf}
\caption{\textbf{Ablation on sparse depth injector.} Our results demonstrate that the absence of sparse depth injection induces the breakdown of geometry, resulting in blurry and inconsistent 3D shapes, proving the prominence of the sparse depth injector in our framework.  }
\label{fig:ablation_sparse}
\vspace{-1pt}
\end{figure*}





\section{Evaluation details}
\label{supp:eval}
\subsection{User study}
\label{spp:userstudy}
To qualitatively compare our \ours with previous methods, DreamFusion~\citep{poole2022dreamfusion} and SJC~\citep{wang2022score}, we conduct a user study. As the Imagen~\citep{saharia2022photorealistic} model used in DreamFusion is publicly unavailable, we utilize Stable-DreamFusion~\citep{stable-dreamfusion}, which uses Stable Diffusion~\citep{rombach2022high}. All methods employ Stable Diffusion v1.5 checkpoint. 

Our user study questionnaire shows rendered images of 3D scenes generated by \ours, SJC, and Stable-DreamFusion, using identical prompts. We ask the participants to select the result most fitting to each of the questions given below. We use 7 prompts and generate three 3D scenes per prompt using \ours, SJC, and Stable-DreamFusion. We give 5 rendered images for each 3D scene, with the camera poses identical across different models for a fair comparison. We do not disclose the models used to generate the results and randomize the order of methods for each question in order to keep the selection process fair. Our final user study is the statistics summarizing responses from a total of 102 participants. 

The questions used in the user study are as follows:
\vspace{-5pt}
\begin{itemize}
    \item These images are rendered by rotating the camera horizontally from 0 to 360 degrees. Which result has the most natural shape? (3D coherence)
    \vspace{-5pt}
    \item Which result seems to follow the user input text the best? (prompt adherence)
    \vspace{-5pt}
    \item Which result has the best overall quality? (overall quality)
    \vspace{-5pt}
\end{itemize}


\begin{figure*}[t!]
    \centering
    \small
    \setlength\tabcolsep{0.8pt}
    {
    % \renewcommand{\arraystretch}{0.5}
    % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{cccc}
      \includegraphics[width=0.18\linewidth]{fig/appendix/col1.png} &
      \includegraphics[width=0.18\linewidth]{fig/appendix/col2.png} &
      \includegraphics[width=0.18\linewidth]{fig/appendix/col3.png} &
      \includegraphics[width=0.18\linewidth]{fig/appendix/col4.png} \\
      \includegraphics[width=0.18\linewidth]{fig/appendix/col5.png} &
      \includegraphics[width=0.18\linewidth]{fig/appendix/col6.png} &
      \includegraphics[width=0.18\linewidth]{fig/appendix/col7.png} &
      \includegraphics[width=0.18\linewidth]{fig/appendix/col8.png} \\
       Rendered image & Reconstruction & Rendered image & Reconstruction\\
    \end{tabular} }
    % }
\vspace{5pt}
\caption{\textbf{Visualization of predicted camera pose from COLMAP~\citep{schonberger2016structure} optimization.} Left columns visualize COLMAP optimization results from the renderings of SJC-generated scenes~\citep{wang2022score}, while the right columns visualize the results from our \ours.}
    \label{fig:colmap} 
    \vspace{-5pt}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=0.6\linewidth, ]{fig/appendix/flamingo.pdf}
\vspace{1pt}
\caption{\textbf{Scenes evaluated by CLIP-based metric.} (a) An inconsistent 3D scene and (b) a consistent 3D scene. The inconsistent 3D scene achieves a higher score in the CLIP-based metric.}
\label{fig:clip}
\vspace{-5pt}
\end{figure}





% \vspace{-10pt}
\subsection{COLMAP-based metric}
\label{appendix:colmap}

We provide additional details of the COLMAP~\citep{schonberger2016structure}-based metric we propose. We sample 100 uniformly spaced cameras whose origin points are on a hemisphere of fixed radius at an identical elevation angle, which is 30 degrees in our setting. All cameras are directed towards the center of the hemisphere, and we render 100 images of the 3D scene from these camera viewpoints. Subsequently, COLMAP predicts the camera pose of each image for point cloud reconstruction. We measure the difference between the predicted camera poses of image pairs that were adjacent at the previous rendering stage. 

The variance between the different values is used as a metric for the 3D consistency evaluation. As initial differences between all adjacent camera pairs are identical at the rendering stage, a high variance in predicted difference values indicates the magnitude of inaccuracy in the COLMAP optimization process that predicts cameras. Such inaccuracies are caused by distortions and repeating artifacts in a 3D scene, whose ambiguous and repetitive nature confuses COLMAP optimization by offering multiple erroneous solutions in the camera pose prediction process. Therefore, it can be argued that this variance value corresponds to the prominence of 3D inconsistent features that make optimization for COLMAP difficult. 
% and we provide empirical results to further strengthen our point. 

Fig.~\ref{fig:colmap} visualizes the point cloud and the camera poses reconstructed through COLMAP optimization. The left columns show COLMAP optimization results of inconsistent 3D scenes generated by SJC~\citep{wang2022score}, and the right columns show COLMAP optimization results of consistent 3D scenes generated by \ours. It is noticeable that the cameras predicted from \ours's scene are more uniformly spaced and stable than those of SJC. In the left-right figure showing a horse generated by SJC, COLMAP displays evident difficulty in handling the ambiguous, repetitive features of the 3D-inconsistent horse, leading it to mistakenly predict that all cameras are distributed on only one side of the hemisphere. As this phenomenon is consistently observed throughout multiple scenes, we demonstrate that such distortions and 3D inconsistencies increase the difficulty of COLMAP optimization and harm the camera pose prediction quality. 

\subsection{Limitation of CLIP-based metric}
For 2D image generation, metrics such as FID~\citep{heusel2017gans} and Inception score~\citep{salimans2016improved} are used to quantify the generation quality by measuring the similarity between the distribution of ground truth images and generated images. However, the same approach cannot be applied to score distillation-based text-to-3D generation, due to the absence of 3D scenes corresponding to the text prompts. This presents a challenge for conducting a quantitative evaluation of zero-shot text-to-3D generative models, which previous works have bypassed by showcasing various qualitative results instead of quantitative evaluation~\citep{wang2022score} or conducting user study~\citep{lin2022magic3d}. 

Another approach used in previous works~\citep{jain2022zero,poole2022dreamfusion} is to use CLIP-based metrics, such as CLIP R-precision, which measures retrieval accuracy based on CLIP~\citep{radford2021learning} through projected 2D image and text input. Nevertheless, such CLIP-based metrics are ineffective in measuring the quality and consistency of generated 3D geometry. For example, when geometric failure such as a multiple-face problem occurs, producing frontal geometric features multiple times around the 3D scene, the CLIP-based metric measures all such viewpoints with frontal features as having high similarity with the text prompts. This causes such 3D scenes to have misleadingly high scores despite having incorrect geometry. Fig.~\ref{fig:clip} illustrates an example of this phenomenon, where each image rendered in a geometric inconsistent scene (first row) achieves a higher CLIP score than each image generated in a geometric consistent scene (second row).

\section{Limitation}       
\label{supp:limitation}
We have proposed a novel \ours framework that infuses 3D awareness into a pretrained 2D diffusion model while preserving its original generalization capability. While \ours stably optimizes NeRF for text-to-3D generation, demonstrating superior performance, there are still several limitations that need to be addressed. First, our approach inherits the difficulty in reflecting complex user prompts faithfully, as it relies on the ability of the pretrained diffusion model to follow text prompts. 
Additionally, our approach may face various societal biases inherent in the dataset~\citep{schuhmann2021laion}, similar to the text-to-image generation models~\citep{rombach2022high}.

\end{appendix}