\section{Related work}
\paragrapht{Diffusion models}
~\citep{ho2020denoising, song2021denoising} have gained much attention as generative models due to their stability, diversity, and scalability. Given these advantages, diffusion models have been applied in various fields, such as image translation~\citep{rombach2022high, seo2022midms} and conditional generation~\citep{kanizo2013palette,rombach2022high}. Especially, text-to-image generation has been highlighted with the introduction of various guidance techniques~\citep{ho2021classifier,dhariwal2021diffusion}. GLIDE~\citep{nichol2021glide} utilizes CLIP~\citep{radford2021learning} guidance to enable text-to-image generation, followed by large-scale text-to-image models such as DALL-E2~\citep{ramesh2022hierarchical} and Stable Diffusion~\citep{rombach2022high}. Such emergence has led to the utilization of pretrained text-to-image models for tasks such as endowing additional conditions~\citep{wang2022pretraining} or performing manipulations~\citep{gal2022image,kwon2022diffusion}.

\paragrapht{Text-to-3D generation} models generally employ pretrained vision-and-language models, such as CLIP~\citep{radford2021learning} to generate 3D shapes and scenes from text prompts. DreamFields~\citep{jain2022zero} incorporates CLIP with neural radiance fields (NeRF)~\citep{mildenhall2020nerf}, demonstrating the potential for zero-shot NeRF optimization using only CLIP as guidance. Recently, Dreamfusion~\citep{poole2022dreamfusion} and SJC~\citep{wang2022score} have demonstrated an impressive ability to generate NeRF with frozen diffusion models instead of CLIP. Concurrently, some approaches bring performance improvement through coarse-to-fine pipeline such as Magic3D~\citep{lin2022magic3d} and Fantasia3D~\citep{Chen_2023_ICCV}. ProlificDreamer~\citep{wang2023prolificdreamer} demonstrates extremely high-fidelity results through utilizing a variational version of SDS, named VSD. However, despite their impressive performance, the 3D inconsistency problem fundamentally remains, causing them to often generate distorted geometries. 

\paragrapht{Image-to-3D generation} models generate 3D scenes from a conditioning image, which is analogous to the task of generation-based novel view synthesis. There are 3D native diffusion models such Point-E~\citep{nichol2022point}, trained upon several million internal 3D models to generate point clouds, as well as SDS-based methods such as NeRDi~\citep{deng2023nerdi}. NeuralLift-360~\citep{Xu_2022_neuralLift} and Realfusion~\citep{melas2023realfusion} utilize reconstruction loss combined with monodepth estimation and textual inversion, respectively, to strengthen semantic alignment of 3D scene to the image. Among these, a concurrent work Zero123~\citep{liu2023zero} bears similarity to our work as it models 3D scenes in the 2D image domain by finetuning a 2D diffusion model to infer a novel view of the input image given relative camera pose. Despite its effectiveness, this method is not without shortcomings, such as geometric inconsistencies between viewpoints due to the difficulty of modeling 3D transformations solely in 2D domain~\citep{liu2023one2345}, as well as limited generative capability when inferring scene outside the domain of training data. Our work diverges from this approach in that we model the coarse 3D geometry explicitly. This explicit geometry enables a view-aligned depth map to be given as a condition for the diffusion model, allowing it to model 3D consistency more effectively. 
\vspace{-10pt}