
For the task of text-to-image generation, diffusion models such as Stable Diffusion~\citep{rombach2022high} receive a text prompt as an additional condition. Specifically, when a text prompt $c$ is given, a mapping model $\mathcal{T}(\cdot)$ maps the prompt $c$ into the embedding $e=\mathcal{T}(c)$. Then, the embedding $e$ is injected into the diffusion model with parameters $\theta$. Formally, we denote a score prediction network of the text-to-image diffusion model as $\epsilon_\theta(x_t, t, \mathcal{T}(c))$, where $x_t$ represents a noisy image with a noise level $t$ added to a clean image $x_0$. For the sake of brevity, we shall omit the variable $t$ and refer to the function $\epsilon_{\theta}(x_t, \mathcal{T}(c))$. 

Score distillation sampling (SDS)~\citep{poole2022dreamfusion} optimizes NeRF parameters toward following the direction of the score predicted by a frozen diffusion model towards higher-density regions. Specifically, let us denote ${\Theta}$ as parameters of NeRF, and $\mathcal{R}_\Theta(\pi)$ as a rendering function given a camera pose $\pi$. In SDS, random camera pose $\pi$ is sampled, and the diffusion model is utilized to infer the 2D score of the rendered image, \ie, $x=\mathcal{R}_\Theta(\pi)$. This score is used to optimize the NeRF parameters $\Theta$ by letting the rendered image move to the high-density regions, \ie, to be realistic. It can be understood as minimizing a standard noise prediction loss function~\citep{ho2020denoising} with respect to the NeRF parameters $\Theta$ instead of the diffusion model's parameters $\theta$ such that:
\begin{equation}
\Theta^* = \underset{\Theta}{\mathrm{argmin}}\,\mathbb{E}_{t,\epsilon,\pi}\Bigl[ w(t)\big\|\epsilon_\theta\bigl(x_t, \mathcal{T}(c) \big| x=\mathcal{R}_\Theta(\pi)\bigr) - \epsilon \big\|^2_2  \Bigr],
\label{equation:sds1}
\end{equation}
where $\epsilon$ is a Gaussian noise and $w(t)$ is a weighting function. The Jacobian term of diffusion U-net $\partial\epsilon_\theta\bigl(x_t,\mathcal{T}(c)\bigr)/\partial x_t$ from the gradient of Eq.~\ref{equation:sds1} can be omitted, formulating the new gradient  as:
\begin{equation}
\begin{split}
     \nabla_\Theta &\mathcal{L}_\mathrm{SDS}(x=\mathcal{R}_\Theta(\pi); \theta) 
    \triangleq \mathbb{E}_{t,\epsilon}\Bigl[\Tilde{w}\bigl(t\bigr)\bigl(\epsilon_\theta \bigl(x_t,\mathcal{T}(c) \bigr)-\epsilon\bigr)\frac{\partial x}{\partial \Theta}\Bigr],
\end{split}
\label{equation:sds2}
\end{equation}
where $\nabla_\Theta \mathcal{L}_\mathrm{SDS}$ is a gradient of $\mathcal{L}_\mathrm{diff}$ with the U-net Jacobian term omitted, and $\Tilde{w}(t)$ is a weighting function from the DDPM~\citep{ho2020denoising} diffusion process.    
%The derivation is discussed in detail in the supplementary materials.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1\textwidth,]{fig/Figure3_neww.pdf}
\end{center}
\vspace{-15pt}
\caption{\textbf{Overall architecture of \ours.} Our framework consists of semantic code sampling, followed by consistency injection module that gives 3D-aware condition to the diffusion model through a sparse depth injector. Semantic code, along with LoRA~\citep{hu2021lora} layers, helps maintain the semantic consistency of 3D generation.}
\label{fig:network_overall}
\vspace{-17pt}
\end{figure*}
