\documentclass[12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%   Note template by Fei Lu %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%   packages    %%%%%%%%%
\usepackage[small,compact]{titlesec}
\usepackage{amsmath,amssymb,amsfonts,mathabx,setspace}
\usepackage{graphicx,caption,epsfig,subfigure,epsfig,wrapfig}
% \usepackage{graphicx,epsfig,wrapfig,caption,epsfig,subcaption,sidecap}
\usepackage{url,color,verbatim,algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage[ruled,boxed]{algorithm} % {algorithm}% [ruled]{algorithm}  % algorithm;
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{enumitem}
\usepackage{booktabs}  % for \toprule
\usepackage[sort,compress]{cite}
\usepackage[scaled]{helvet}
\usepackage[T1]{fontenc}
% \usepackage[ansinew]{inputenc} %???? what is it for? 
\usepackage[bookmarks=true, bookmarksnumbered=true, colorlinks=true,   pdfstartview=FitV,
linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
% \usepackage{authblk,showlabels}
\usepackage[english]{babel}  % to include both US and UK English, and proper hyphenation
% \usepackage{times} % set to heavy times
\usepackage{multirow}
% \usepackage{natbib} % to reduce bib item spacing. Use it at last. 
%\usepackage{epstopdf}
%\epstopdfDeclareGraphicsRule{.tif}{png}{.png}{convert #1 \OutputFile}
%\AppendGraphicsExtensions{.tif}

\usepackage[sort,compress]{cite}
\usepackage[normalem]{ulem}%for \sout (strikeout)
\usepackage{mathtools}% for \mathsout
\newcommand{\mathsout}[1]% will draw line through middle of #1
{\bgroup\mathchoice
  {\sbox0{$\displaystyle{#1}$}%
    \usebox0\hspace{-\wd0}%
    \rule[0.5\ht0-0.5\dp0-.5pt]{\wd0}{1pt}}%
  {\sbox0{$\textstyle{#1}$}%
    \usebox0\hspace{-\wd0}%
    \rule[0.5\ht0-0.5\dp0-.5pt]{\wd0}{1pt}}%
  {\sbox0{$\scriptstyle{#1}$}%
    \usebox0\hspace{-\wd0}%
    \rule[0.5\ht0-0.5\dp0-.5pt]{\wd0}{1pt}}%
  {\sbox0{$\scriptscriptstyle{#1}$}%
    \usebox0\hspace{-\wd0}%
    \rule[0.5\ht0-0.5\dp0-.5pt]{\wd0}{1pt}}%
\egroup}
%%%%%%%%%%%----   page settings   ----- %%%%%%%%%
\marginparwidth 0pt
\oddsidemargin  -0.15in   % = 0.38 cm
\evensidemargin  -0.15in % = 0.38 cm
\marginparsep 0pt
\topmargin   -.55in   % = 1.40 cm
\textwidth   6.8in      % = 17.27 cm
\textheight  9.3in      % = 23.87 cm
%\footskip 3mm
%\setstretch{1.05}  \hoffset=-0.1in  \voffset= -0.8in
\parskip=0.0in


%%%%%%%%%%%----  new commands/defs   ----- %%%%%%%%%
%% for this project  
\def\by{\mathbf{y}} 
\def\bf{\mathbf{f}} 
\def\bW{\mathbf{W}} \def\bw{\mathbf{w}} 
\def\bL{\mathbf{L}}
\def\bA{\mathbf{A}}  \def\bC{\mathbf{C}}
\def\bB{\mathbf{B}}\def\bb{\mathbf{b}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}

%% general 
\def\clattice{c_{\mathrm{lat}}}           
\def\Rn{\mathbb{R}^n}
\def\R{\mathbb{R}}                            
\def\P{\mathbb{P}}
\def\bbeta{\bm \beta}      
\def\costF{\mathcal{E}}
\def\mH{\mathcal{H}}
                  
\newcommand{\Ebracket}[1]{\mathbb{E}\left[{#1}\right]}
\def\E{\mathbb{E}}
\newcommand{\rbracket}[1]{\left(#1\right)}
\newcommand{\sbracket}[1]{\left[#1\right]}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\normv}[1]{\left| #1\right|}
\newcommand{\innerp}[1]{\langle{#1}\rangle}
\newcommand{\dbinnerp}[1]{\langle\hspace{-1mm}\langle{#1}\rangle\hspace{-1mm}\rangle}
\newcommand{\vect}[1] {\pmb{#1}}
\newcommand{\mat}[1]{\pmb{#1}}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\mathspan}[1]{ \mathrm{span}\left\{ {#1} \right\} }

\newcommand{\Ito}{It\^o}

\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\operatorname{min}}\;}

%%%%%%%%%%%----  notation RKHS based regression   ----- %%%%%%%%%
\newcommand{\mbf}[1]{\boldsymbol{#1}}
\def\calE{\mathcal{E}}
\def\mathspan{\mathrm{span}}
\def\Gbar{ {\overline{G}} }
\def\Fbar{ {\overline{F}} }
\def\LGbar{ {\mathcal{L}_{\overline{G}}}  }
\def\LFbar{ {\mathcal{L}_{\overline{F}}}  }
\def\rhoT{\overline{\rho}}
\def\mH{\mathcal{H}}
\def\supp{\mathrm{supp}}

\def\calS{\mathcal{S}}
\def\calT{\mathcal{T}}
\def\spaceX{\mathbb{X}}
\def\spaceY{{L^2_\mu(\mathcal{T})}} 
%\mathbb{Y}}

%% Note commands
% \usepackage{soul,array}
%% margin notes
%\newcommand{\note}[1]{\marginpar{\renewcommand{\baselinestretch}{1.0}{\scriptsize{\parbox{0.5in}{\raggedright{\bf{\sffamily #1}}}}}}}
%\newcommand{\fnote}[1]{\note{Fei: {#1}}}

\newcommand{\FL}[1]{\textcolor{blue}{{#1}}}

%%%%% New commands by Yvonne 
\newcommand{\LGbarD}{\mathcal{L}_{\overline{G}^D}}
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
\newcommand{\beqas}{\begin{eqnarray*}}
\newcommand{\eeqas}{\end{eqnarray*}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqs}{\begin{equation*}}
\newcommand{\eeqs}{\end{equation*}}
\newcommand{\one}{{\rm{\mathbf{1}}}}
%%%%%%%%%%%---- new environments  
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
% \newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\numberwithin{equation}{section}
\numberwithin{theorem}{section}

% Colors
\usepackage{xcolor}
\definecolor{darkmagenta}{rgb}{0.55, 0.0, 0.55}
\colorlet{colorYO}{darkmagenta}
\newcommand{\YO}[1]{\textcolor{colorYO}{#1}}   % displays text in red color
\newcommand{\YOs}[1]{\textcolor{colorYO}{\sout{#1}}}   % displays -//- (crossed)
\newcommand{\YOr}[2]{\textcolor{colorYO}{\sout{#1}#2}}   % displays -//- (crossed)
\newcommand{\mYOs}[1]{\textcolor{colorYO}{\mathsout{#1}}}   % displays -//- (crossed)
\newcommand{\mYOr}[2]{\textcolor{colorYO}{\mathsout{#1}#2}}   % displays -//- (crossed)
% other new commands
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
%%%%
\begin{document}

%%% ============== Title  =======
\begin{center}
{\Large An adaptive RKHS regularization for Fredholm integral equations
} \\[0pt] 
\vspace{4mm}
Fei Lu and Miao-Jung Yvonne Ou
\footnote{FL: Department of Mathematics, Johns Hopkins University; feilu@math.jhu.edu  \\
MYO: Department of Mathematical Science, University of Delaware; mou@udel.edu }


\end{center}
 
% \author{}  \maketitle
\begin{abstract} 
Regularization is a long-standing challenge for ill-posed linear inverse problems, and a prototype is the Fredholm integral equation of the first kind.
We introduce a practical RKHS regularization algorithm adaptive to the discrete noisy measurement data and the underlying linear operator. This RKHS arises naturally in a variational approach, and its closure is the function space in which we can identify the true solution. Furthermore, we prove and numerically demonstrate that the RKHS-regularized estimator has a mean-square error converging linearly as the noise scale decreases. In contrast, the commonly-used $L^2$-regularized estimator has a flat mean-square error.  

\end{abstract}


 % \tableofcontents
 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  ====== section =======  %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
We consider the inverse problem of recovering the input function in the Fredholm integral equation from \emph{discrete noisy} output. Specifically, let $\calS\subset \R$ and $\calT\subset \R$ be two compact sets. We aim to recover the function $\phi:\calS\to \R$ in the Fredholm integral equation
\begin{align}\label{eq:FIE}
y(t) = \int_\calS K(t,s)\phi(s) \nu(ds)+ \sigma \dot W(t) =: L\phi(t) + \sigma \dot W(t), 
\end{align}
from a discrete noisy data $\by = \{y(t_i), t_0 < t_1 <\cdots <  t_m\}$ with $\calT= \{t_i\}_{i=0}^m$. Here $\nu$ is a finite measure on $\calS$, and it is suitable for both continuous and discrete models: it is the Lebesgue measure when $\calS$ is an interval and an atom measure when $\calS$ has finitely many elements. Similarly, we define $\mu$ to be an atomic measure with $\mu(t_i) =t_{i+1}-t_i$, which can be viewed as the discrete approximation of the Lebesgue measure. The operator $L: L^2_\nu(\calS)\to L^2_\mu(\calT)$ is the integral operator corresponding to $K$. 
 The integral kernel $K: \calT\times \calS \to \R$ is continuous and given. A typical example is 
\[  K(t,s) = s^{-2}e^{- s t}
\]
 with $\calS \subset [a,b]$ and $\calT\subset [0,T]$ for some $b>a>0$ and $T>0$, which arises from the magnetic resonance relaxometry (MRR) \cite{Bi2022span-of-regular}, see Section \ref{sec:num} for more details. 

The measurement noise  $\sigma \dot W(t)$ is the white noise; that is, the noise at $t_i$ has a standard Gaussian distribution $\mathcal{N}(0, \sigma^2 (t_{i+1}-t_i))$ for each $i$. Such a noise is integrable when the observation mesh refines, i.e., $\max_i(t_{i+1}-t_i)$ vanishes. 

Eq.\eqref{eq:FIE} is a prototype of ill-posed inverse problems, dating back from Hadamard \cite{hadamard1923lectures} and it remains a testbed for new regularization methods \cite{wahba1973convergence,hansen1998rank,Li2005modified}. It is ill-posed in $L^2_\nu(\calS)$ in the sense that the least squares solution with mini-norm is sensitive to small measurement noise in data \cite{nashed1974generalized}. In other words,  in the variational formulation of the inverse problem, the minimizer of a loss functional
\beqa
&&\mathcal{E}(\phi):=\norm{L\phi-\by}^2_{\spaceY} = \innerp{L\phi,L\phi}_{\spaceY}-2\innerp{L\phi,\by}_{\spaceY}+\norm{\by}^2_{\spaceY}
\label{least_sq_org}
\eeqa
with minimal norm is sensitive to the data $\by$. Such an ill-posedness roots in the fact that the operator $L: L^2_\nu(\calS)\to L^2_\mu(\calT)$ is compact with eigenvalues converging to zero.  


 Regularization, necessary to avoid amplifying the measurement noise, is a long-standing challenge for ill-posed linear inverse problems, and there are numerous regularization methods. Roughly speaking, in the variational formulation, a regularization method restricts the subset or subspace of search either by constraints on $\phi$ or the loss functional (e.g., minimizing $\calE(\phi)$ subject to $\|\phi\|_*<\delta$ or minimizing $\|\phi\|_*$ subject to $\calE(\phi)<\alpha$) or by adding a penalty to the loss functional; e.g., the well-known Tikhonov regulation:  
\beqa\label{eq:regu_loss}
\phi_\lambda = \argmin{\phi} \calE_\lambda(\phi) : =  \mathcal{E}(\phi)+\lambda \norm{\phi}_{*}^2. 
\eeqa
Here $\|\phi\|_*$ is a regularization norm and $\delta,\alpha, \lambda$ are the hyper-parameters. There is tremendous effort in selecting the norm and the hyper-parameter. Once the norm is chosen, the minimization can be solved by either direct or iterative methods. The direct methods solve the linear equations by canonical decompositions, and the selection of $\lambda$ has been thoroughly studied in \cite{wahba1977practical,hansen1998rank}. We will use the L-curve method in \cite{hansen_LcurveIts_a}. The iterative methods are flexible for high-dimensional problems, and we refer to \cite{gazzola2019ir,chen2022stochastic,zhang2022stochastic} for recent developments. 

We focus on selecting the regularization norm in \eqref{eq:regu_loss}. Inspired by the data-adaptive RKHS regularization for learning kernels in operators \cite{LLA22,LAY22,LangLu21,chada2022data}, we introduce a practical reproducing kernel Hilbert space (RKHS) norm that arises naturally in the variational approach. The closure of this RKHS is the function space in which we can identify the true solution. Specifically, we introduce another measure $\rho$ on $\calS$ that reflects the exploration of the data to the function $\phi$, and consider the variational inverse problem in $L^2_\rho(\calS)$.  The RKHS is then the image of the square root of the operator $\LGbar=L^*L: L^2_\rho(\calS)\to L^2_\rho(\calS)$ (see Eq.\eqref{LGbar_def}), and it has a reproducing kernel (see Eq.\eqref{Gbar_def})
 \begin{equation*}
  \Gbar(s,s'):=  \frac{1}{\frac{d\rho}{d\nu}(s)\frac{d\rho}{d\nu}(s')}\int_\calT K(t,s)K(t,s') \mu(dt), \quad \forall(s,s')\in \calS\times \calS. 
  \end{equation*}
The RKHS norm will lead to a penalty term $\|\phi\|_*^2= \innerp{\LGbar^{-1}\phi,\phi}_{L^2_\rho}$. This norm restricts the search to be inside the RKHS; hence the regularized solution is robust to measurement noise. 

We prove and numerically demonstrate that when the eigenvalue of $\LGbar$ converges exponentially to zero, the RKHS-regularized estimator has a mean-square error converging linearly as the noise scale decreases, whereas the commonly-used $L^2$-regularized estimator has a flat mean-square error. Also, numerical results show that the regularized estimator converges linearly as the observation mesh  $\max_i(t_{i+1}-t_i)$ decays. As far as we know, this is the first result on the convergence of the regularized estimator in the small noise limit, and the convergence rate agrees with the optimal rate in \cite{wahba1977practical,Li2005modified} when the kernel is smooth.  


There are various regularization norms, including the Euclidean norms (e.g., \cite{hansen1998rank,tihonov1963solution}), the total variation norm $\|\phi'\|_{L^1}$ in Rudin--Osher--Fatemi method in \cite{rudin1992nonlinear} or the $L^1$ norm $ \|\phi\|_{L^1}$ in LASSO (e.g., \cite{tibshirani1996_RegressionShrinkage}). However, these norms are based on pre-assumed properties of the solution and are generic without considering the specific inverse problem. In contrast, our RKHS norm is adaptive to the operator and the observation mesh. The norm close to our study is the norm $\|\phi\|_R^2$ of an RKHS with a user-specified reproducing kernel $R$ (e.g., \cite{wahba1977practical,bauer2007regularization,CZ07book}). However, the $L^2_\rho(\calS)$-closure of our RKHS is the function space of identifiability (FSOI). Also, from a statistics point of view, when $L^2_\rho(\calS)$ is infinite-dimensional, the kernel $R$ must be equivalent to our $\Gbar$ in the sense that its integral operator $\mathcal{L}_R$ must have the same image as $\LGbar$, because otherwise, these two Gaussian prior distributions will be singular by the Feldman--Hajek theorem (e.g., \cite[Theorem 2.25]{da2014stochastic}). Thus, our RKHS is a proper function space of regularization when there is no additional information about the solution. 


The rest of the paper is organized as follows. We introduce the adaptive RKHS in Section \ref{sec:FSOI}, with a characterization between the RKHS and the function space of identifiability. Section \ref{sec:conv} proves the convergence of the RKHS-regularized estimator, and Section \ref{sec:num} presents the algorithm and numerically demonstrates the robust convergence of the estimator when either the noise decays or the observation mesh refines. We conclude with a discussion on future developments of the adaptive RKHS regularization strategy in Section \ref{sec:conlusion}.   

%%%%%%%%%
\section{An adaptive RKHS for the inverse problem} \label{sec:FSOI} 


In this section, we introduce the RKHS based on the variational formulation of the inverse problem. 
A core element is an identifiability theory that specifies the function space in which the loss functional has a unique minimizer, so that the variational inverse problem is well-defined. Importantly, when the true solution is in this space, the minimizer of the loss function recovers the solution when there is no measurement noise in the data. Thus, we call this function space of identifiability (FSOI). Then, we introduce the RKHS, whose norm is used for regularization. This ensures that the search in minimizing the penalized functional takes place inside this FSOI.  



\subsection{The function space of identifiability} 
We first introduce an ambient function space $L^2_\rho(\calS)$, where the measure $\rho$ is defined as  
\beq
\label{exp_measure}
\frac{d\rho}{d\nu}(s):=\frac{1}{Z} \int_\calT | K(t,s)| \mu(dt), \, \forall s\in \calS, 
\eeq
where $Z= \int_\calS \int_\calT | K(t,s) |\mu(dt)\nu(ds)$ is the normalizing constant. This measure quantifies the exploration of the integral kernel $K$ to the unknown input function at the output set $\calT$, and hence is referred to as an \emph{exploration measure}.  

The major advantage of the space $L^2_\rho(\calS)$ over the original space $L^2_\nu(\calS)$ is that it is adaptive to the specific setting of the inverse problem. In particular, this weighted space takes into account the structure of the integral kernel and the data points in $\calT$. It can provide better scaling and reduce the ill-conditioning in computation (see \cite{LangLu21}). Thus, while the following identifiability theory can be carried out for both $L^2_\rho(\calS)$ and $L^2_\nu(\calS)$, we will focus only on $L^2_\rho(\calS)$. 
 

We define the function space of identifiability by the loss functional.
 \begin{definition} The function space of identifiability (FSOI) is the largest linear subspace of $L^2_\rho(\calS)$ where the loss functional $\mathcal{E}$ in \eqref{least_sq_org} has a unique minimizer. 
 \end{definition}
 
The FSOI is the space in which the Fr\'echet derivative of the quadratic loss functional has a unique zero. This motivates us to study the Fr\'{e}chet derivative of the loss function $\mathcal{E}$ in $L^2_\rho(\calS)$ and define an integral operator which is the operator of inversion in the variational approach. 

We start with writing the quadratic term $\innerp{L\phi,L\phi}_{\spaceY}$ in the loss function \eqref{least_sq_org} into a bilinear form
\beqa 
\label{double_innerp}
\innerp{L\phi,L\psi}_{\spaceY}= \int_\calS \int_\calS \phi(s)\psi(s')G(s,s')\nu(ds) \nu(ds'),
\eeqa
where the integral kernel $G:\calS\times \calS \to \R$ is defined as
 \beq \label{eq:G}
  G(s,s'):= \int_\calT K(t,s)K(t,s') \mu(dt).  \eeq
% \beq G(s,s'):=s^{-2}(s')^{-2} \int_0^T e^{-st}e^{-s' t} dt,\quad s,\, s' \in [a,b].  \eeq
Note that 
\begin{equation}\label{eq:bd_G}
G(s,s')\leq \|K\|_\infty Z \min\{\frac{d\rho}{d\nu}(s),\frac{d\rho}{d\nu}(s')\},\, \forall(s,s')\in \calS\times \calS.
\end{equation}
 Then, we can weigh it by the exploration measure and define 
\beq
\Gbar(s,s'):=% \one_{\calS}(s)  \one_{\calS}(s')
 \frac{G(s,s')}{\frac{d\rho}{d\nu}(s)\frac{d\rho}{d\nu}(s')}. 
\label{Gbar_def}
\eeq

The next lemma follows directly from the continuity of the kernel $K\in C(\calT\times \calS)$. 
% its proof can be found in \cite[Lemma 3.2]{LLA22} or \cite[ Section 4.2--4.3]{CZ07book}.
%%%%
\begin{lemma}
\label{mercer_kernel_prop} 
Assume that $K\in C(\calT\times \calS)$. Then, the following statements hold.
\begin{itemize}
\item[{\rm(a)}] The function $\Gbar$ in  \eqref{Gbar_def} is a Mercer kernel. With $\rho$ in \eqref{exp_measure}, the operator $\LGbar: L^2_\rho(\calS) \to L^2_\rho(\calS)$ defined by 
\beq
\label{LGbar_def}
\LGbar\phi(r):=\int_a^b \phi(s)\Gbar(r,s)\rho(ds)
\eeq
is compact, self-adjoint and positive, and for $\phi,\psi \in L^2_\rho(\calS)$ it satisfies 
\beq
\innerp{L\phi,L\psi}_{\spaceY} = \innerp{\LGbar \phi,\psi}_{L^2_\rho(\calS)}=\innerp{ \phi,\LGbar \psi}_{L^2_\rho(\calS)}.
\label{quadratic_2}
\eeq
\item[{\rm(b)}] Let $\{\lambda_i\}_{i\geq 1}$ denote the positive eigenvalues of $\LGbar$ arranged in a descending order and $\{\psi_i,\,\psi_j^0\}_{i,j}$ be the orthonormal eigenfunctions of $\LGbar$ with $\psi_i$ and $\psi_j^0$ corresponding to eigenvalues $\lambda_i$ and zero (if any), respectively. Then, $\{\lambda_i\}_{i\geq 1}$ is either finite or $\lambda_i\to 0$ as $i\to \infty$, and these eigenfunctions form a complete basis of $L^2_\rho(\calS)$. 
\item[{\rm(c)}] The operator $\LGbar$ is a trace-class operator in the sense that $\sum_{i}\lambda_i<\infty$, and    
\begin{equation}\label{eq:trace}
\sum_i \lambda_i = \int_\calS \Gbar(s,s)\rho(ds) \leq  \|K\|_\infty Z \nu(\calS), 
\end{equation}
where $\nu(\calS)$ denote the measure of $\calS$ and $Z$ is the normalizing constant in \eqref{exp_measure}. 
\end{itemize}
\end{lemma}

%  \iffalse  % ============== proof 
 \begin{proof} 
Since the kernel $K$ is continuous, so is $\rho$ and $G$ in \eqref{eq:G}, thus $\Gbar$ is also continuous.  Note that $\Gbar$ is symmetric, i.e., $\Gbar(s,s')=\Gbar(s', s)$ for any $s,s'\in \calS$. Also, it is positive semi-definite, i.e., 
 \beq \sum_{i=1}^n\sum_{j=1}^n c_i c_j \Gbar(s_i,s_j) =\int_\calT \left(\sum_{i=1}^n  c_i \one_{\calS}(s_i)  K(t,s_i)/\frac{d\rho}{d\nu}(s_i) \right)^2 \, \mu(dt) 
  % \int_0^T \left(\sum_{i=1}^m  \one_{\calS}(s_i) \frac{c_i s_i^{-2} e^{-s_i t} }{\rho(s_i)} \right)^2 \, dt \ge 0 
 \eeq
 for any $\{c_j\}_{j=1}^n\subset \R$, $\{s_j\}_{j=1}^k\subset \calS$ and $n\in \mathbb{N}$. 
 Also, we have 
 $\Gbar$ 
 is positive semidefinite and $\int_\calS \int_\calS \Gbar(s,s') ^2\rho(ds) \rho(ds') \leq \|K\|_\infty^2 Z^2 <\infty$ by \eqref{eq:bd_G}. Thus, $\LGbar$ is compact and positive self-adjoint (e.g., \cite[Proposition 4.6]{CZ07book}).

Part (b) follows directly from that $\LGbar$ is compact and positive self-adjoint (\cite[Theorem 4.7]{CZ07book}). 
 
 
To prove Part (c), recall that from the Mercer's Theorem, we have 
\[
\Gbar(s,s')=\sum_i \lambda_i \psi_i(s)\psi_i(s'),
\]
where the convergence is uniform on $\calS\times \calS$. Then, 
 \begin{align*}
 \sum_i\lambda_i = \int_\calS \Gbar(s,s)\rho(ds) = \int_\calS \frac{G(s,s)}{\frac{d\rho}{d\nu}(s)}ds \leq \|K\|_\infty Z \nu(\calS), 
 \end{align*}
 where the inequality follows from \eqref{eq:bd_G}. 
 \end{proof} 
%  \fi  %%%%% % ============== proof 

The next theorem characterizes the function space of identifiability through the Fr\'echet derivative of the loss functional, and highlights the role of $\LGbar$ as the operator of inversion in the variational approach. 
\begin{theorem}[Function space of identifiability]\label{thm:FSOI}
Assume that $K\in C(\calT\times \calS)$.  Then,%  we have,  
\begin{itemize}
\item[{\rm(a)}]  For each data $\by$ generated with $\phi_*$ by \eqref{eq:FIE}, there exists a unique $\phi^\by \in L_\rho^2(\calS)$ such that
\beq
\innerp{\phi^\by,\psi}_{L^2_\rho(\calS)}=\innerp{L\psi,\by}_{\spaceY},\quad \forall \psi \in L^2_\rho(\calS), \label{linear}
\eeq
and it has a decomposition 
\begin{equation}\label{eq:noise_dec}
\phi^\by = \LGbar \phi_*+ \phi^\sigma,
\end{equation} 
where $\phi^\sigma$ has a distribution $\mathcal{N}(0, \sigma^2\LGbar)$, i.e., $\phi^\sigma= \sum_i \sigma \xi_i \lambda_i^{1/2}\psi_i $ with $\{\xi_i\}$ being independent identically distributed standard Gaussian random variables and $\Ebracket{\|\phi^\sigma\|_{L^2_\rho(\calS)}^2}= \sigma^2\sum_i\lambda_i$. 
\item[{\rm(b)}] The Fr\'echet derivative of the loss functional in $L^2_\rho$ is $\nabla \calE(\phi) = 2(\LGbar \phi- \phi^\by)$. 
\item[{\rm(c)}] The FSOI of $\calE$ is $H:=\overline{ \mathspan\{\psi_i\}_{i:\lambda_i>0} }$ with closure in $L^2_\rho(\calS)$. 
\item[{\rm(d)}] Assume that the data is noisy. When $\sum_{i:\lambda_i>0} \lambda_i^{-1}<\infty$, the unique minimizer of $\calE$ in $H$ is $\widehat{\phi} = \LGbar^{-1}\phi^\by$; but when $\sum_{i:\lambda_i>0} \lambda_i^{-1}=\infty$, the solution $\LGbar^{-1}\phi^\by$ is ill-defined in $L^2_\rho(\calS)$  in the sense that $\Ebracket{\|\LGbar^{-1}\phi^\by\|_{L^2_\rho(\calS)}^2} = \infty$. 
\item[{\rm(e)}]  When the observation is noiseless, we have $\widehat{\phi} = \LGbar^{-1}\phi^\by= P_H\phi_*$, where $P_H$ is the projection operator of $H$. 
\end{itemize}
\end{theorem}

\begin{proof}
The existence and uniqueness of $\phi^\by$ in Part $(a)$ follow from the Riesz representation theorem. Denote the observation by $\by= L\phi_*+\sigma \Delta \mathbf{W}$ with $\Delta \mathbf{W} = \left(W(t_{i})- W(t_{i-1}): 1\leq i\leq m \right)$. Then, the decomposition of $\phi^\by$ follows from \eqref{linear}: 
\begin{align*}
\innerp{\phi^\by,\psi}_{L^2_\rho(\calS)} = \innerp{L\psi,L\phi_*}_\spaceY +   \innerp{L\psi,\sigma \Delta \mathbf{W}}_\spaceY 
& = \innerp{\LGbar\phi_{*},\psi}_{L^2_\rho(\calS)} + \innerp{\psi,\phi^\sigma}_{L^2_\rho(\calS)} ,
\end{align*}
where the first term in the last equation comes from the definitions of the operator $\LGbar$ in \eqref{LGbar_def}, and the second term comes from the Riesz representation. The distribution of $\phi^\sigma$ is $\mathcal{N}(0,\sigma^2\LGbar)$ because the random variable $ \innerp{\psi,\phi^\sigma}_{L^2_\rho(\calS)} =   \innerp{L\psi,\sigma \Delta \mathbf{W}}_\spaceY  $ is Gaussian with mean zero and variance $\sigma^2\innerp{\psi,\LGbar\psi}_{L^2_\rho(\calS)}$ for each $\psi \in L^2_\rho(\calS)$. Therefore, we can write $\phi^\sigma= \sum_i \sigma \xi_i \lambda_i^{1/2}\psi_i $ with $\{\xi_i\}$ being i.i.d.~standard Gaussian, and $\Ebracket{\|\phi^\sigma\|_{L^2_\rho(\calS)}^2}= \sigma^2\sum_i\lambda_i$, where the sum is finite by \eqref{eq:trace}. 


Part $(b)$ follows directly from the definition of the Fr\'echet derivative. In fact, by \eqref{quadratic_2} and \eqref{linear}, we can write the loss functional as 
\begin{equation}\label{eq:loss_L2}
\calE(\phi) =  \innerp{\LGbar \phi,\phi}_{L^2_\rho(\calS)} +  \innerp{\phi^\by,\phi}_{L^2_\rho(\calS)} + \|\by\|_\spaceY^2. 
\end{equation}
Then, the Fr\'echet derivative $\nabla \calE(\phi)$ in $L^2_\rho(\calS)$ is 
\[
\innerp{\nabla \calE(\phi), \psi}_{L^2_\rho(\calS)} = \lim_{h\to 0} \frac{\calE(\phi+h\psi) - \calE(\phi)}{h} =  \innerp{2 ( \LGbar \phi -\phi^\by), \psi}_{L^2_\rho(\calS)}, \, \forall \psi \in L^2_\rho(\calS). 
\]

For Part $(c)$, first, note that the quadratic loss functional has a unique minimizer in $H$ because its derivative has a unique zero. Meanwhile, note that $H$ is the orthogonal complement of the null space of $\LGbar$, and $\calE(\phi_*+\phi^0) = \calE(\phi_*)$ for any $\phi^0$ such that $\LGbar \phi^0=0$.  Thus, $H$ is the largest such linear subspace of $L^2_\rho(\calS)$, and we conclude that  $H$ is the FSOI.
 
For Part (d), note that 
\[  \LGbar^{-1}\phi^\sigma = \sum_{i:\lambda_i>0} \lambda_i^{-1/2} \sigma \xi_i \psi_i .
\]
Then,  when $\sum_{i:\lambda_i>0} \lambda_i^{-1}<\infty$ (which happens only when there are finitely many non-zero eigenvalues), we have  $ \LGbar^{-1}\phi^\sigma \in L^2_\rho(\calS)$ because $\Ebracket{ \| \LGbar^{-1}\phi^\sigma\|_{L^2_\rho(\calS)}^2} =\sigma^2  \sum_{i:\lambda_i>0} \lambda_i^{-1}<\infty$. Thus, $\phi^\sigma$ in $\LGbar(L^2_\rho)$, so is $\phi^\by$, and the estimator $\widehat{\phi} = \LGbar^{-1}\phi^\by$ is well-defined in $L^2_\rho$. By Part $(b)$, this estimator is the unique zero of the loss functional's Fr\'echet derivative in $H$. Hence it is the unique minimizer of $\calE(\phi)$ in $H$. 

On the other hand, when $\sum_{i} \lambda_i^{-1}=\infty$, we have $\Ebracket{\| \LGbar^{-1}\phi^\sigma \|_{L^2_\rho(\calS)}^2 }
 = \sum_{i} \lambda_i^{-1} \sigma^2 
=\infty$, and hence $\LGbar^{-1}\phi^\by$ is ill-defined.

For Part (e), when the data is noiseless, i.e., $\by=L\phi_*$, we have $\phi^\by= \LGbar\phi_*$ from Part $(a)$. Hence $\widehat{\phi} = \LGbar^{-1}\phi^\by = P_H \phi_*$. % That is, $\phi_*\in H$ is the unique minimizer of the loss functional $\calE$. 
\end{proof}



Theorem \ref{thm:FSOI} reveals the nature of the ill-posedness of this inverse problem, and provides insights on regularization: 
\begin{itemize}
\item The variational inverse problem is ill-defined beyond the FSOI $H$. Its ill-posedness in $H$ depends on the smallest eigenvalue of the operator $\LGbar$.    
\item When the data is noiseless, the minimizer of the loss function is the $H$-projection of the true input function. In other words, the inverse problem can only recover the $H$-projection of the true input function. 
\item When data is noisy, its minimizer $\LGbar^{-1}\phi^\by$ can be ill-defined in $L^2_\rho$ when $\sum_{i:\lambda_i>0} \lambda_i^{-1}=\infty$, but it is well-defined when $\sum_{i:\lambda_i>0} \lambda_i^{-1}<\infty$.
\end{itemize}
As a result, when regularizing the ill-posed problem, it is important to ensure the solution lies in the FSOI and either remove the non-integrable components related to small eigenvalues or reduce the bias caused by the noise.   
% When the measurement $y$ is contaminated by noise, the noisy $\phi^\by$  can be decomposed as
% \beq \phi^\by=\LGbar{\phi_*}+\phi^\sigma,\mbox{  with  } \phi_1^\delta\in \LGbar(L^2_\rho(\calS)),\, \phi_2^\delta\in (\LGbar(L^2_\rho(\calS)))^\perp. \label{noisy_phiy} \eeq



\subsection{An adaptive RKHS regularization}\label{sec:RKHS}

We introduce an adaptive RKHS Tikhonov regularization. This RKHS is defined based on the operator of inversion $\LGbar$; thus, it is adaptive to the original integral operator $L$ and the data settings. It ensures that the solution lies in the FSOI because its $L^2_\rho$ closure is the FSOI. It removes the non-integrable small eigenvalue component because of the smoothness of its functions. 

The next lemma characterizes this RKHS.  Its proof is straightforward (e.g., \cite[Section 4.4]{CZ07book} or \cite{LLA22}). 
\begin{lemma}[Characterization of the adaptive RKHS]\label{lemma:rkhs} 
Assume $K\in C(\calT\times\calS)$. The RKHS $H_G$  with $\Gbar$ as its reproducing kernel satisfies the following properties. 
\begin{enumerate}
\item[{\rm(a)}]  $H_G:=\LGbar^{\frac{1}{2}}(L^2_\rho(\calS))$ and its inner product satisfies 
\begin{equation*} % \label{eq:rkhs_innerp}
\innerp{\phi,\psi}_{H_G}:=\innerp{\LGbar^{-\frac{1}{2}}\phi,\LGbar^{-\frac{1}{2}}\psi}_{L^2_\rho(\calS)}. 
\end{equation*}
The operator $\LGbar$ is self-adjoint in $H_G$. Moreover, we have
$\innerp{\phi,\psi}_{L^2_\rho(\calS)}=\innerp{\LGbar \phi,\psi}_{H_G}$  for any $\phi\in L^2_\rho(\calS)$ and $\psi\in H_G$. 
\item [{\rm(b)}] 
  $\{\sqrt{\lambda_i} \psi_i\}_{i=1}^\infty$ is an orthonormal basis of $H_G$, where $\{\psi_i,\, \psi_j^0\}_{i,j}$ are the orthonormal eigenfunctions of $\LGbar$ in $L^2_\rho(\calS)$ corresponding to eigenvalues $\{\lambda_i, 0\}_{i,j}$. 
\item [{\rm(c)}] 
For any $\phi=\sum_{i=1}^\infty c_i \psi_i$, with $c_i\in \R$, we have 
\beq
\innerp{L\phi,L\phi}_\spaceY=\sum_{i=1}^\infty \lambda_i c_i^2,\quad \| \phi\|^2_{L^2_\rho}=\sum_{i=1}^\infty c_i^2, \quad \| \phi\|^2_{H_G}=\sum_{i=1}^\infty \lambda_i^{-1} c_i^2 \text{ if } \phi\in H_G. 
\label{norms}
\eeq  
Moreover, the $H_G$ norm is stronger than the $L^2_\rho$-norm: $\norm{\phi}^2_{H_G}\ge \lambda_1^{-1} \norm{\phi}^2_{L^2_\rho}$.
\item [{\rm(d)}]  $H=\overline{ H_G}$ with inclosure in $L^2_\rho(\calS)$, where $H=\overline{ \mathspan\{\psi_i\}_{i:\lambda_i>0} }$ is the FSOI. 
\end{enumerate}
\end{lemma}
 \begin{proof}
 The first part is from the standard characterization theorem of RKHS, e.g., \cite[Section 4.4]{CZ07book} or \cite{LLA22}. When $\phi\in L^2_\rho(\calS)$, we have $\LGbar\phi \in H_G$. Then, by the definition of the inner product and the symmetry of $\LGbar^{-1/2}$, we have $\innerp{\LGbar\phi,\psi}_{H_G} = \innerp{\LGbar^{1/2}\phi,\LGbar^{-1/2}\psi}_{L^2_\rho(\calS)} = \innerp{\phi,\psi}_{L^2_\rho(\calS)}$. 

Part \rm{(b)} follows directly from the characterization of the inner product in Part I. Part (c) follows directly from \eqref{quadratic_2}, the orthonormality of the eigenfunctions and the characterization of the inner product. Part \rm{(d)} is obvious because both function spaces have the same basis functions. 
 \end{proof}


Now we propose to use the RKHS norm to regularize the problem. Note that by using the RKHS norm, the loss functional is minimized over $H_G$, not $H\subset L^2_\rho(\calS)$, because elements in $H$ may have unbounded $H_G$ norm.  

\begin{proposition}[Regularized estimators] \label{prop_error} The $L^2_\rho$ and RHKS regularized estimators, i.e., the minimizers of the regularized loss functional by the $L^2_\rho$ and the RKHS norms,  are 
\beqa
\widehat{\phi}_\lambda^{L^2_\rho}= \argmin{\phi\in L^2_\rho(\calS)} \mathcal{E}(\phi)+\lambda \norm{\phi}_{L^2_\rho(\calS)}^2 = (\LGbar+\lambda I)^{-1}\phi^\by,\label{regs_L2}\\ 
\widehat{\phi}_\lambda^{H_G}= \argmin{\phi\in H_G} \mathcal{E}(\phi)+\lambda \norm{\phi}_{H_G}^2 = (\LGbar^2+\lambda I)^{-1}\LGbar \phi^\by. 
 \label{regs_HG}
\eeqa
Let the true function be $\phi_*= \sum_i c_i\psi_i + \sum_j d_j\psi_j^0$ and recall the eigen-decomposition of $\LGbar$ in Lemma {\rm\ref{mercer_kernel_prop}}.   Then, for any $\lambda>0$, the biases of these two regularized estimators satisfy the following estimates:  
 \beqa
  && \norm{\widehat{\phi}_\lambda^{L^2_\rho}-\phi_*}^2_{L^2_\rho} =   \sum_i (\lambda_i +\lambda)^{-2}(\sigma \lambda_i^{1/2}\xi_i - \lambda c_i)^2+ \sum_jd_j^2,\label{errL2_eig}\\ 
 && \norm{\widehat{\phi}_\lambda^{H_G}-\phi_*}^2_{L^2_\rho}  =   \sum_i (\lambda_i^2 +\lambda)^{-2}(\sigma \lambda_i^{3/2}\xi_i - \lambda c_i)^2+ \sum_jd_j^2, \label{errHG_eig}
  \eeqa
  where $\xi_i$ are i.i.d.~standard Gaussian random variables in Theorem {\rm \ref{thm:FSOI}}.  
% For any $\lambda>0$, the biases of these two regularized are controlled by
% \beqa
% && \norm{\widehat{\phi}_\lambda^{L^2_\rho}-\phi_*}_{L^2_\rho} \le \norm{(\LGbar+\lambda I)^{-1}\phi^\sigma}_{L^2_\rho}+\norm{(\LGbar+\lambda I)^{-1}\lambda\phi_*}_{L^2_\rho},\label{errL2}\\
% &&\norm{\widehat{\phi}_\lambda^{H_G}-\phi_*}_{L^2_\rho}\le \norm{(\LGbar^2+\lambda I)^{-1}\LGbar{\phi^\sigma}}_{L^2_\rho}+\norm{(\LGbar^2+\lambda)^{-1}\lambda\phi_*}_{L^2_\rho}, \label{errHG}
% \eeqa
 % where $\phi^\sigma$ is the noise-induced term in the decomposition of $\phi^\by$ in \eqref{eq:noise_dec}. 
\end{proposition}
\begin{proof}
The uniqueness of the minimizers and their explicit form follow from the Fr\'echet derivatives of the regularized loss functionals. In fact, by Theorem \ref{thm:FSOI}(b),  the Fr\'echet derivative of $\calE_\lambda  = \mathcal{E}(\phi)+\lambda \norm{\phi}_{L^2_\rho(\calS)}^2$ in $L^2_\rho(\calS)$ is  
\[
\nabla \calE_\lambda(\phi) = 2[ (\LGbar + \lambda I) \phi -\phi^\by ]. 
\]
It has a unique zero,  $\widehat{\phi}_\lambda^{L^2_\rho} = (\LGbar + \lambda I)^{-1}\phi^\by$, which is the unique minimizer of $\calE_\lambda$ in $L^2_\rho$, and thus we have \eqref{regs_L2}. Similarly, first note that by Lemma \ref{lemma:rkhs} {\rm(a)} and \eqref{eq:loss_L2},  for any $\phi\in H_G$, 
\[
\calE(\phi)  % =  \innerp{\LGbar \phi,\phi}_{L^2_\rho(\calS)} +  \innerp{\phi^\by,\phi}_{L^2_\rho(\calS)} + \|\by\|_\spaceY^2
=  \innerp{\LGbar^2\phi,\phi}_{H_G} +  \innerp{\LGbar\phi^\by,\phi}_{H_G} +  \|\by\|_\spaceY^2. 
\]
Then, the Fr\'echet derivative of $\widetilde \calE_\lambda  = \mathcal{E}(\phi)+\lambda \norm{\phi}_{H_G}^2$ in $H_G$ is  
\[
\nabla \widetilde \calE_\lambda(\phi) = 2[ (\LGbar^2 + \lambda I) \phi -\LGbar\phi^\by ]. 
\]
Its unique zero,  $\widehat{\phi}_\lambda^{H_G} = (\LGbar^2 + \lambda I)^{-1}\LGbar\phi^\by$, is the minimizer of $\widetilde \calE_\lambda$ in $H_G$ as in \eqref{regs_HG}. 

The eigenvalue characterizations of the biases in \eqref{errL2_eig} and \eqref{errHG_eig} follow from the decomposition of $\phi^\by = \LGbar \phi_*+\phi^\sigma$ with $\phi^\sigma= \sum_i \sigma \lambda_i^{1/2}\xi_i$ in \eqref{eq:noise_dec}. In fact, note that 
\begin{align*}
& \widehat{\phi}_\lambda^{L^2_\rho}=(\LGbar+\lambda I)^{-1}(\LGbar{\phi_*}+\lambda \phi_*-\lambda \phi_*+\phi^\sigma)=\phi_*+(\LGbar+\lambda I)^{-1}(-\lambda \phi_*+\phi^\sigma),\\
& \widehat{\phi}_\lambda^{H_G}=(\LGbar^2+\lambda I)^{-1}(\LGbar^2{\phi_*}+\LGbar\phi_1^\delta)=\phi_*+(\LGbar^2+\lambda I)^{-1}(-\lambda \phi_*+\LGbar\phi^\sigma).
\end{align*}
Then, we obtain \eqref{errL2_eig} and \eqref{errHG_eig}.  
\end{proof}

Proposition \ref{prop_error} demonstrates the complexity of choosing an optimal hyper-parameter $\lambda$. An optimal $\lambda$ aims to balance the error caused by the noise and the shift from the true solution caused by the regularization, and it depends on the spectrum of the operator, each realization of the noise, and the true solution. Thus, it is important to select an optimal $\lambda$ adaptive to these factors, and the L-curve method does so.  

Also, Proposition \ref{prop_error} shows that the true solution's components outside the FSOI remain a bias for both estimators, because there is no information about these components in the data. Thus, in analyzing the error of the estimators, we focus only on the error inside the FSOI. 
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%===============> New 
\section%{Convergence of the regularized estimator\YO{s} \YO{in the FSOI} as \YO{the} noise decays}
{Convergence of the regularized estimators as the noise decays}
\label{sec:conv}

We show that the RKHS regularized estimator converges in mean-squares at order $1$ as $\sigma\downarrow 0$ under the assumption that the spectrum of $\LGbar$ converges exponentially. In contrast, the $L^2_\rho$ regularized estimator has a flat error. The assumption and the convergence are observed in the numerical tests in Section \ref{sec:num}. 

Let $\phi_*= \sum_i c_i\psi_i\in H$.  
Recall that by Proposition \ref{prop_error}, the mean-square error of the regularized estimators in \eqref{regs_L2} and \eqref{regs_HG} are 
 \begin{equation}\label{eq:mse}
 \begin{aligned}
  e^{L^2_\rho}(\lambda) = &\E \norm{\widehat{\phi}_\lambda^{L^2_\rho}-\phi_*}^2_{L^2_\rho} =   \sum_i (\lambda_i +\lambda)^{-2}(\sigma^2 \lambda_i + \lambda^2 c_i^2),\\ 
e^{H_G}(\lambda) = & \E \norm{\widehat{\phi}_\lambda^{H_G}-\phi_*}^2_{L^2_\rho}  =   \sum_i (\lambda_i^2 +\lambda)^{-2}(\sigma^2 \lambda_i^{3}+ \lambda^2 c_i^2). 
 \end{aligned}
   \end{equation}

\begin{theorem}[Convergence of regularized estimators as noise decays]\label{thm:conv_sigma}
Assume that the spectrum of the operator $\LGbar$ in \eqref{LGbar_def} satisfies $\lambda_i = e^{-\theta i}$ for all $i\geq 1$ with $\theta>0$. Let $\phi_*= \sum_i c_i\psi_i$. Then, the mean-square error (MSE) of the RKHS-regularized estimator in \eqref{eq:mse} converges as $\sigma\to 0$, while the MSE of the $L^2_\rho$-regularized estimator does not. Specifically, 
\begin{itemize}
\item[(a)] Assume $c_i^2 =\lambda_i$. Then, the optimal hyper-parameters and the MSEs satisfy
\begin{equation}\label{eq:opt_lambda}
\begin{aligned}
\lambda_*&= \argmin{\lambda>0} e^{H_G}(\lambda) = \sigma^2,\quad  \text{ and } e^{H_G}(\lambda_*) 
% \E \norm{\widehat{\phi}_{\lambda*}^{H_G}-\phi_*}^2_{L^2_\rho} 
= C_1\sigma+ O(\sigma^2); \\ 
\widetilde \lambda_*&= \argmin{\lambda>0} e^{L^2_\rho}(\lambda) = a\sigma^2 + O(\sigma^3),\quad  \text{ and }  e^{L^2_\rho}(\widetilde \lambda_*) 
% \E \norm{\widehat{\phi}_{\lambda*}^{L^2_\rho}-\phi_*}^2_{L^2_\rho}  
= C_2 + O(\sigma^2), 
\end{aligned}
\end{equation}
where $a= 1+\sqrt{2}$,  $C_1 =\frac{\pi}{4\theta}$ and $C_2 = \frac{1}{a\theta}$. Here $O(\sigma^2)$ is the big-O notation.  
\item[(b)] Assume $\sum_i \lambda_i^{-1} c_i^2 <\infty$. Then,  
\begin{equation}\label{eq:opt_lambda}
\min_{\lambda>0}e^{H_G}(\lambda) \leq e^{H_G}(\sigma^2) \leq (1+ \sup_{i}\lambda_i^{-1}c_i^2) C_1\sigma+ O(\sigma^2). 
\end{equation}
\end{itemize}
\end{theorem}

To prove the theorem, we first introduce some notations and two technical lemmas. We rewrite the MSEs in \eqref{eq:mse} as 
\begin{equation}\label{eq:error_lambda}
e^{H_G}(\lambda) = \sigma^2 A(\lambda) + \lambda^2 B(\lambda); \quad e^{L^2_\rho}(\lambda) = \sigma^2 \widetilde A(\lambda) + \lambda^2 \widetilde B(\lambda). 
\end{equation}
where $A$ and $B$ are defined by
\begin{equation}\label{eq:AB}
\begin{aligned}
A(\lambda) &= \sum_i (\lambda_i^2+\lambda)^{-2} \lambda_i^{3}, \quad & B(\lambda) &= \sum_i (\lambda_i^2+\lambda)^{-2}  c_i^2, \\
\widetilde A(\lambda) &= \sum_i (\lambda_i+\lambda)^{-2} \lambda_i, \quad &\widetilde B(\lambda) & = \sum_i (\lambda_i+\lambda)^{-2} c_i^2. 
\end{aligned}
\end{equation}
Then, since a minimizer must be a critical point, we have that $\lambda_*$ must satisfy
\[
0=\frac{d}{d\lambda} e^{H_G}(\lambda)  =\sigma^2 A'(\lambda) + 2\lambda [ B(\lambda) +\frac{\lambda}{2}B'(\lambda)]
\]
and similarly for $\widetilde \lambda_*$. With $B_1(\lambda) : =B(\lambda) +\frac{\lambda}{2} B'(\lambda) $ and $\widetilde B_1(\lambda) : = \widetilde B(\lambda) +\frac{\lambda}{2} \widetilde B'(\lambda) $, we obtain the following lemma providing equations for the minimizers of the MSEs. 
\begin{lemma}\label{lemma:AB-lambda}
The minimizers of $e^{H_G}$ and $e^{L^2_\rho}$ in \eqref{eq:mse}, denoted by $\lambda_*$ and $\widetilde \lambda_*$, respectively, satisfy
\begin{equation}\label{eq:lambda_opt}
\lambda_* =- \sigma^2\frac{A'(\lambda_*)}{2B_1(\lambda_*)}, \quad \widetilde \lambda_* =- \sigma^2\frac{\widetilde A'(\widetilde \lambda_*)}{2\widetilde B_1(\widetilde \lambda_*)}, 
\end{equation}
where the functions $A'(\lambda)$ and $A'(\lambda)$ $B_1(\lambda)$, $\widetilde A(\lambda)$ and $\widetilde B_1(\lambda)$ are given by 
\begin{equation}\label{eq:A1B1}
\begin{aligned}
A'(\lambda) &= -2\sum_i (\lambda_i^2+\lambda)^{-3} \lambda_i^{3}, \quad &B_1(\lambda) =  \sum_i (\lambda_i^2+\lambda)^{-3} \lambda_i^2 c_i^2, \\
\widetilde A'(\lambda) &= -2\sum_i (\lambda_i+\lambda)^{-3} \lambda_i, \quad &\widetilde B_1(\lambda)=  \sum_i (\lambda_i+\lambda)^{-3}\lambda_i c_i^2. \\
\end{aligned}
\end{equation}
\end{lemma}

The next lemma estimates these series by Riemann sum when $\lambda$ is small. 
\begin{lemma}\label{lemma:seiresAB}
Assume that $\lambda_i = e^{-\theta i}$ for all $i\geq 1$ with $\theta>0$. Then, for small $\lambda>0$, we have 
\begin{equation}\label{eq:seriesAB}
\begin{aligned}
A(\lambda)=  \sum_i (\lambda_i^2+\lambda)^{-2} \lambda_i^{3} & = \frac{1}{2\theta \sqrt{\lambda}} [\arctan\frac{1}{\sqrt{\lambda}} - \frac{\sqrt{\lambda}}{1+\lambda}] + O(1), \\ 
  \sum_i (\lambda_i^2+\lambda)^{-2} \lambda_i & = \frac{1}{2\theta }\lambda^{-3/2} [\arctan\frac{1}{\sqrt{\lambda}} + \frac{\sqrt{\lambda}}{1+\lambda}] + O(1), \\ 
\widetilde A(\lambda)=   \sum_i (\lambda_i+\lambda)^{-2} \lambda_i & = \frac{1}{\theta \lambda(1+\lambda)}+ O(1), \\ 
\widetilde A'(\lambda)=  -2    \sum_i (\lambda_i+\lambda)^{-3} \lambda_i & = \frac{-(1+2\lambda)}{\theta \lambda^2(1+\lambda)^2}+ O(1), \\ 
          \sum_i (\lambda_i+\lambda)^{-3} \lambda_i^2 & = \frac{1}{2\theta \lambda (1+\lambda)^2}+ O(1). \\ 
\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
The proof is based on the Riemann sum approximation of integrals. Note that for $k\in \{1,3\}$, the function $f(x) = ( e^{-2\theta x} + \lambda)^{-2} e^{-\theta xk } $ satisfies $\int_0^\infty f(x) dx= \sum_{i=1} f(i)  + O(1)$. Thus, 
\begin{align*}
 & \sum_i (\lambda_i^2+\lambda)^{-2} \lambda_i^k  = \int_0^\infty  ( e^{-2\theta x} + \lambda)^{-2} e^{-\theta xk }  dx + O(1) =  \frac{1}{\theta} \int_0^1 \frac{t^{k-1} dt }{(t^2+\lambda)^2}  + O(1), 
\end{align*}
where we applied a change of variables $t= e^{-\theta x}$ to obtain the second equality. 
Then, the first two equations in \eqref{eq:seriesAB} follow directly from the facts that $\int_0^1 \frac{t^2 dt }{(t^2+\lambda)^2} = \frac{1}{2 \sqrt{\lambda}} [\arctan\frac{1}{\sqrt{\lambda}} - \frac{\sqrt{\lambda}}{1+\lambda}] $ and $\int_0^1 \frac{dt }{(t^2+\lambda)^2} = \frac{1}{2}\lambda^{-3/2} [\arctan\frac{1}{\sqrt{\lambda}} + \frac{\sqrt{\lambda}}{1+\lambda}] $. 

Similarly, we obtain the last three equations in \eqref{eq:seriesAB} by using the integrals $\int_0^1 \frac{1}{(t+\lambda)^2}dt $,  $\int_0^1 \frac{1}{(t+\lambda)^3}dt $ and $\int_0^1 \frac{t}{(t+\lambda)^3}dt $. 
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:conv_sigma}]
Part (a): Substituting $c_i^2= \lambda_i$ into the equation for $B_1(\lambda)$ in \eqref{eq:A1B1}, we have $B_1(\lambda) =  \sum_i (\lambda_i^2+\lambda)^{-3} \lambda_i^3 = \frac{-1}{2}A'(\lambda)$. Hence, Eq. \eqref{eq:lambda_opt} implies that $\lambda_*= \sigma^2$. Also, substituting  $c_i^2= \lambda_i$ into the equation for $B(\lambda)$ in \eqref{eq:AB}, we obtain
 \[
 B(\lambda) = \sum_i (\lambda_i^2+\lambda)^{-2}  \lambda_i =  \frac{1}{2\theta }\lambda^{-3/2} [\arctan\frac{1}{\sqrt{\lambda}} + \frac{\sqrt{\lambda}}{1+\lambda}] + O(1). 
 \]
 where the second equality follows from the second equation in \eqref{eq:seriesAB}. This equation, together with the first equation in  \eqref{eq:seriesAB} and \eqref{eq:error_lambda}, implies 
 \begin{align} \label{eq:errHG_min}
& \min_{\lambda>0} e^{H_G}(\lambda) = e^{H_G}(\sigma^2) = \left[ \sigma^2A(\lambda) + \lambda^2B(\lambda) \right] \mid_{\lambda = \sigma^2}  \notag \\
 = &  \left[ \frac{ \sigma^2}{2\theta \sqrt{\lambda}} (\arctan\frac{1}{\sqrt{\lambda}} - \frac{\sqrt{\lambda}}{1+\lambda}) + \frac{ 1}{2\theta }\sqrt{\lambda} (\arctan\frac{1}{\sqrt{\lambda}} - \frac{\sqrt{\lambda}}{1+\lambda}) \right] \big|_{\lambda= \sigma^2}+ O(\sigma^2) \notag \\
 = & \sigma^2 \frac{ 1}{2\theta\sqrt{\lambda}  }\arctan\frac{1}{\sqrt{\lambda}} \mid_{\lambda= \sigma^2}+ O(\sigma^2) = \frac{\pi}{4\theta} \sigma+ O(\sigma^2). 
 \end{align}
 
 Similarly, to find the minimizer of $e^{L^2_\rho}(\lambda) $, we substitute $c_i^2= \lambda_i$ into the equation for $\widetilde B_1(\lambda)$ in \eqref{eq:A1B1}, we have  $\widetilde B_1(\lambda) =  \sum_i (\lambda_i+\lambda)^{-3} \lambda_i^2 = \frac{1}{2\theta \lambda (1+\lambda)^2}+ O(1)$. Consequently, by the equation for $\widetilde A'(\lambda)$ in \eqref{eq:seriesAB}, the minimizer $\widetilde \lambda_*$ satisfies  
 \begin{align*}
 \lambda  = -\sigma^2 \frac{\widetilde A'(\lambda)}{2\widetilde B_1(\lambda)} =  \sigma^2 \frac{\frac{1+2\lambda}{\theta \lambda^2 (1+\lambda)^2}+ O(1) }{\frac{1}{2\theta \lambda (1+\lambda)^2}+ O(1)} \Rightarrow \lambda^2= \sigma^2+2\lambda \sigma^2+O(\lambda^2 \sigma^2 + \lambda^3).  
 \end{align*}
The positive solution to  $\lambda^2= \sigma^2+2\lambda \sigma^2$ is $(1+ \sqrt{2})\sigma^2$. Thus, the minimizer is $\widetilde \lambda_* = a \sigma^2 + o(\sigma^2)$ with $a= 1+ \sqrt{2}$. As a result, the fact that $\widetilde B(\lambda) = \sum_i (\lambda_i+\lambda)^{-2}\lambda_i = \widetilde A(\lambda)$ implies that  
 \begin{align*}
& \min_{\lambda>0} e^{L^2_\rho}(\lambda) = e^{L^2_\rho}(\lambda_*) =  \widetilde A(\lambda) ( \sigma^2 + \lambda^2 ) \big|_{\lambda = \lambda_*} \\
 = & \frac{1}{\theta \lambda (1+\lambda)} ( \sigma^2 + \lambda^2 ) + O(\sigma^2) \big|_{\lambda = \lambda_*} \\
 = & \frac{1}{\theta} \frac{\sigma^2(1+ a^2\sigma^2)  + o(\sigma^4)}{a\sigma^2(1+a\sigma^2) + o(\sigma^2)} + O(\sigma^2) = \frac{1}{a\theta} + O(\sigma^2), 
 \end{align*}
 where we used Eq.\eqref{eq:seriesAB} to obtain the third equality. 
 
The upper bound in Part (b) is straightforward. In fact, note that  $M_0:= \sup_{i} \lambda_i^{-1}c_i^2 <\infty$ since $\sum_{i} \lambda_i^{-1}c_i^2 <\infty$. Then, 
\begin{align*}
 B(\lambda) & = \sum_i (\lambda_i^2+\lambda)^{-2}  \lambda_i  \lambda_i^{-1}c_i^2 \leq M_0  \sum_i (\lambda_i^2+\lambda)^{-2}  \lambda_i \\
  & \leq  \frac{M_0}{2\theta }\lambda^{-3/2} [\arctan\frac{1}{\sqrt{\lambda}} + \frac{\sqrt{\lambda}}{1+\lambda}] + O(1), 
 \end{align*}
  where the last inequality follows from the second equation in \eqref{eq:seriesAB}.
 Thus, 
  \begin{align*}
\min_{\lambda>0} e^{H_G}(\lambda) \leq  e^{H_G}(\sigma^2) = \left[ \sigma^2A(\lambda) + \lambda^2B(\lambda) \right] \mid_{\lambda = \sigma^2}  \leq (1+ M_0) C_1\sigma+ O(\sigma^2)
 \end{align*}
 by the same argument as in  \eqref{eq:errHG_min}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  ====== section =======  %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm and numerical examples}\label{sec:num}

We demonstrate the RKHS regularization on the following Fredholm integral equation with the goal of estimating $\phi: [a,b]\to \R$:   
\begin{align}\label{eq:FIE_example}
y(t) = \int_{a}^b s^{-2}e^{- s t}\phi(s) ds + \sigma \dot W(t)  = :L\phi(t) + \sigma \dot W(t), t\in [c,d]
\end{align}
where from data $\by=(y(t_1),\ldots, y(t_m))\in \R^m$ where $ t_i =c+ i\Delta t$ for $0\leq i\leq m$ and $\Delta t=(d-c)/m$. This equation arises from the $T2$ problem in magnetic resonance relaxometry \cite{Bi2022span-of-regular}, where $\phi$ is the distribution of transverse nuclear relaxation times, and it characterizes the material composition of a sample. 

% Physically, $T_2$ represents the decay constant of the transverse magnetization with respect to time and serves as an indicator for identifying different materials. The range of possible $T_2$  values is finite. For example, it ranges from about 10 ms to 2000 ms in biomedical studies, with smaller $T_2$ values corresponding to more rigid tissues. The distribution function $\phi(T_2)$ of each pixel plays a crucial role in characterizing the material composition of a sample. MRR has been a crucial tool in the study of the progression of Alzheimer's Disease; see \cite{Bi2022span-of-regular} and the references therein.  The inverse problem for MRR is to solve the following 

\paragraph{Numerical settings.} We aim to recover $\bphi=  (\phi(s_1), \ldots,\phi(s_n)) \in \R^n$, where $\calS:= \{s_k\}_{k=1}^n$ being a given discrete set. This discrete vector provides a piecewise-constant approximation to the function $\phi$: $\phi(s)=\sum_{k=1}^n \phi(s_k) \mathbf{1}_{[s_{k-1},s_k]}(s)$ with $\delta(s_k)$, where $\mathbf{1}_A(s)$ is the indicator function of the set $A$ and we set $s_0=a$.  
Let $\delta_k:= s_k-s_{k-1}$ for $k\geq 1$, we discretize \eqref{eq:FIE_example} via Riemann sum approximation of the integral: 
\begin{equation}\label{eq:discrete}
\by = \bL \bphi + \bw, 
\end{equation}
where $\bw\in \R^m \sim \mathcal{N}(0,\sigma^2\Delta t I_m)$ and 
\begin{equation}
\bL = \begin{bmatrix}
   g(t_1,s_1)\delta_1 &g(t_1,s_2)\delta_2 & g(t_1,s_3)\delta_3 & \cdots & g(t_1,s_n)\delta_n \\
g(t_2,s_1)\delta_1 &g(t_2,s_2)\delta_2 & g(t_2,s_3)\delta_3 & \cdots & g(t_2,s_n)\delta_n \\
    \vdots & \vdots &  \vdots &  \ddots & \vdots \\
g(t_m,s_1)\delta_1 &g(t_m,s_2)\delta_2 & g(t_m,s_3)\delta_3 & \cdots & g(t_m,s_n)\delta_n \\
    \end{bmatrix}
    \text{ with }g(t,s) := s^{-2}e^{- s t}. 
\end{equation}

We set $(a,b,c,d)=(1,5,0,5)$. For simplicity, we set the mesh points to be $s_k= a+ k\delta $ with $ \delta=(b-a)/n$ (hence $\delta_k\equiv \delta$) and $n=100$. We set the standard deviation of the noise to be $\sigma= \|\bL \bphi  \|  \times nsr$, where the noise-to-signal ratio $nsr$ is set to be $nsr= 1$ unless otherwise specified. Also, we set $\Delta t = 0.01$ hence $m=500$ unless otherwise specified. 


\paragraph{The space $L^2_\rho(\calS)$.}
The exploration measure of \eqref{eq:discrete} can be computed analytically: 
\begin{equation}\label{eq:rho_example}
\rho(s)\, \propto \int_c^d s^{-2}e^{-st}dt = s^{-3}(e^{-cs}- e^{-ds}). 
\end{equation}
We then evaluate $\rho(s_k)$ for each $s_k$ in $\calS$. 
For general cases of \eqref{eq:FIE}, the analytical expression of $\rho$ is not available, but an empirical approximation can be used, i.e., $ \rho(s_k)=\frac{1}{z} \sum_{i=1}^m s_k^{-2}e^{-s_k t_i}\delta_k$ for  $k=1,\dots,n$, with $z$ being the normalizing constant.  

Our goal is to estimate $\bphi\in L^2(\calS, \rho)\subset  \R^n$, which is equipped with the norm $\|f\|_{L^2_\rho(\calS)}^2= \sum_{k=1}^n f_k^2\rho(s_k)$ (and we consider only the values on the support of $\rho$). The support of $\rho$ is where the operator explores the vector $\bphi$, for which we have information for an estimation. Thus, $L^2_\rho(\calS)$ is a proper space with a metric for estimating $\bphi\in \R^n$.

The discrete $\bphi$ uses the Cartesian basis $\{\be_i\}_{i=1}^n$ of  $\R^n$ such that $(\be_1,\be_2,\ldots, \be_n)= I_n$. In $L^2_\rho(\calS)$, these basis vectors have a (diagonal) basis matrix
 \begin{equation}\label{eq:basisMat}
 \bB = ( \innerp{\be_i,\be_j}_{L^2_\rho(\calS)} ) = \mathrm{diag}(\rho(s_k)).
 \end{equation}
and its inner product is 
\beqs
\innerp{\bphi,\boldsymbol{\psi}}_{\rho}:=\bphi^T \bB \boldsymbol{\psi},\, \, \bphi,\boldsymbol{\psi} \in \R^n.
\eeqs 


%%%%%%%%%%%%%%%%%%
\subsection{Ill-posedness and regulariztation}
We estimate the vector $\bphi$ in \eqref{eq:discrete} by least squares, which minimizes the loss function
\begin{align}\label{eq:lossFn_disc}
\widehat \bphi  = \argmin{\bphi\in \R^n}\costF(\bphi), \quad \quad  \costF(\bphi) &= \|\by- \bL \bphi\|^2 =  \sum_{i=1}^m |y_i- (\bL\bphi)_i |^2. 
\end{align}
With $\bA^{\dagger}$ denoting the pseudo-inverse of $\bA$, we compute a minimizer as
\begin{equation}\label{eq:lse_disc}
\widehat \bphi = \bA^{\dagger} \bb, \quad \text{ where }  \bA =  \bL^\top \bL, \, \bb =  \bL^\top \by .
\end{equation}

However, this inverse problem is ill-posed. The ill-posedness is rooted in the integral equation \eqref{eq:FIE_example}, as its solution involves the inversion of a compact operator with eigenvalues converging to zero \cite{nashed1974generalized}. Computationally, the ill-posedness is seen in the ill-conditionedness of the matrix $\bA$. Figure \ref{fig:eigVals} shows that most eigenvalues of $\bA$ are near zero. Similarly, the eigenvalues of the inversion operator $\LGbar:L^2_\rho(\calS) \to L^2_\rho(\calS) $ are mostly zero, and they are slightly larger than those of $\bA$ because the scaling effects of the basis matrix $\bB$. Here the eigenvalue of $\LGbar$ is solved by the generalized eigenvalue problem (see \cite[Theorem 4.1]{LLA22} or \cite[Proposition 5.6]{chada2022data})
\begin{equation}\label{eq:AbB1}
\bA V=  \bB\Lambda V, \quad s.t., V^\top \bB V = I_n, \quad \Lambda= \mathrm{Diag}(\lambda_1,\ldots,\lambda_n). 
\end{equation}
Note that those positive eigenvalues decay exponentially in $i$, providing an example of the assumption on $\lambda_i$ in Theorem \ref{thm:conv_sigma}. 



\begin{figure}[H]	\vspace{-2mm} 
    \centering 	
    {\includegraphics[width =0.45\textwidth]{eigen_val_vec.pdf}
    }
	\vspace{-2mm} 
\caption{Eigenvalues of $\bA$ and generalized eigenvalues of $(\bA,\bB)$. 
} 
\label{fig:eigVals}		\vspace{-5mm}
\end{figure}


As a result, regularization is necessary to reduce the effect of the observation noise. We consider Tikhonov regularization that adds a quadratic penalty term $\lambda \|\bphi\|_\bC^2 = \lambda \bphi^\top \bC \bphi$ to the loss function, and solves the minimizer by least squares 
\begin{equation}\label{eq:lse_disc_reg}
\widehat \bphi_\lambda = (\bA+\lambda \bC)^{-1} \bb = \argmin{\bphi\in\R^n} \costF(\bphi) + \lambda \|\bphi\|_\bC^2.
\end{equation} 
The hyper-parameter $\lambda$ controls the strength of the regularization, and its selection has been thoroughly studied in \cite{wahba1977practical,hansen1998rank,hansen_LcurveIts_a}. We will select it by the L-curve method in \cite{hansen_LcurveIts_a}. 
The L-curve is a log-log plot of the curve 
$ l(\lambda)=(y(\lambda),x(\lambda))$ with $y(\lambda) ^2=\| \bphi_\lambda\|_\bC^2 $ and $x(\lambda)^2 = \calE(\bphi_\lambda) $ with $\bphi_\lambda$ in \eqref{eq:lse_disc_reg}. The L-curve method maximizes the curvature to reach a balance between the minimization of the likelihood and the control of the regularization:  
\beqs %\begin{align}\label{eq:opt_lambda}
	\lambda_{*} 
	= \rm{argmax}_{\lambda_{\text{min}} \leq \lambda \leq \lambda_{\text{max}}}\kappa(l (\lambda)),   \quad \kappa(l (\lambda))= \frac{x'y'' - x' y''}{(x'\,^2 + y'\,^2)^{3/2}},
\eeqs %\end{align}
where $\lambda_{min}$ and $\lambda_{max}$ are the smallest and the largest eigenvalues of $\bA$.  


The main task left is to select the norm $\|\cdot\|_\bC$. We consider the norm of the RKHS $H_G$ (which we explain in details below), and we compare it with two commonly used norms: the Euclidean norm with $\bC=\mathbf{I}$ and the $L^2_\rho(\calS)$ norm with $\bC=\bB$ in \eqref{eq:basisMat}. We refer them as as the RKHS, $l^2$ and $L^2_\rho(\calS)$ regularization, respectively. They are summarized in Table \ref{tab:regularizers}. 
\begin{table}[htp] 
	\begin{center} 
		\caption{ Three regularizers using the norms of $l^2$, $L^2_\rho(\calS)$ and RKHS.   
		 } \label{tab:regularizers}
		\begin{tabular}{ l   l  l }		\toprule % \hline
                 Regularizer name            & $\bC$  & Regularized estimator  \\  \hline
	         l2   &   $\mathbf{I}$  & $ \bphi_\lambda^{l^2}= (\bA + \lambda \mathbf{I})^{-1}\bb$ \\ 
	         L2    &   $\bB$  &  $ \bphi_\lambda^{L^2}= (\bA + \lambda \bB)^{-1}\bb$  \\
	       RKHS     &  $\bC_{rkhs}$  &  $ \bphi_\lambda^{\small{H_G}}= (\bA + \lambda \bC_{rkhs})^{-1}\bb$  \\
	 			\bottomrule	  
		\end{tabular}  \vspace{-4mm}
	\end{center}
\end{table}



 
\paragraph{The RKHS regularization.} Let $\{(\lambda_i,\bpsi_i)\}_{i=1}^n$ be the eigenvalue and eigen-function of $\LGbar$ over $L^2_\rho(\calS)$, and recall that $\{\bpsi_i\}$ form an orthonormal basis of $L^2_\rho(\calS)$. Here they are solved by the generalized eigenvalue problem in \eqref{eq:AbB1}, thus, $\bpsi_i = \sum_j V_{ji}\be_j$ and $\boldsymbol{\Psi} = (\bpsi_1,\ldots,\bpsi_n) = \mathbf{I} V$. Then, $\bphi= \mathbf{I} \bphi = \boldsymbol{\Psi} V^{-1} \bphi$. 
Thus, the $H_G$ norm of $\bphi $ is 
\begin{align*}
 \|\bphi\|_{H_G}^2 = &
\innerp{\bphi,\LGbar^{-1}\bphi}_{L^2_\rho(\calS)}  =  \innerp{\boldsymbol{\Psi} V^{-1} \bphi,\LGbar^{-1}\boldsymbol{\Psi} V^{-1} \bphi}_{L^2_\rho(\calS)}   \\
=  & ( V^{-1}\bphi)^\top \innerp{\boldsymbol{\Psi} , \LGbar^{-1}\boldsymbol{\Psi}}_{L^2_\rho(\calS)} V^{-1} \bphi = \bphi^\top  ( V^{-1})^\top \Lambda^{\dagger}V^{-1} \bphi \\
= & \|\bphi\|_{\bC_{rkhs}}^2 \, \text{ with } \bC_{rkhs} :=( V^{-1})^\top \Lambda^{\dagger}V^{-1}.
 \end{align*}
In particular, if $\rho$ is a uniform measure and the basis matrix is $\bB=I_n$, we have $\bC_{rkhs}= \bA^{\dagger}$.

In computation, we improve numerical stability by avoiding the pseudo-inverse of a singular matrix. The procedure is as follows. Note that for $\bC_* := V \Lambda^{1/2} $, we have $\bC_*^\top  \bC_{rkhs} \bC_* = \begin{pmatrix}I_r & 0 \\ 0 & 0  \end{pmatrix} := \mathbf{I}_r$, where $I_r$ is the identity matrix with rank $r$, the number of positive eigenvalues in $\Lambda$.  %(We can replace $\bC_*$ by any matrix such that $\bC_*^\top  \bC_{rkhs} \bC_* = \mathbf{I}_r$, e.g. the Cholesky decomposition  $V\Lambda V^\top = \bC_* \bC_*^\top$. )
Then, the linear equation $(\bA + \lambda \bC_{rkhs}) c_\lambda =\bb$, the equation \eqref{eq:lse_disc_reg} with regularization matrix $\bC_{rkhs}$, is equivalent to 
\begin{equation}\label{eq:lsqminnorm}
(\bC_*\bA \bC_*+ \lambda \mathbf{I}_r) \widetilde \bphi_\lambda = \bC_*\bb
\end{equation}
 with $\widetilde \bphi_\lambda= \bC_*^{-1}\bphi_\lambda$. We compute $\widetilde \bphi_\lambda$ in the above equation by least squares with minimal norm, then we obtain $\bphi_\lambda= \bC_* \widetilde \bphi_\lambda$. These treatments avoid the inversions of ill-conditioned or singular matrices and lead to more robust estimators. 


The RKHS regularization is summarized in the following algorithm. 
\begin{algorithm}[H]
{\small
\begin{algorithmic}[1]
\Require{The regression triplet $(\bA, \bb, \bB)$ consisting of normal matrix $\bA$, vector $\bb$ in \eqref{eq:lse_disc} and basis matrix $\bB$ as in \eqref{eq:basisMat}.} 
\Ensure{An RKHS regularized estimator $\widehat \bphi_{\lambda_0}$ and its loss value $\calE(\widehat \bphi_{\lambda_0})$. }
	 \State Solve the generalized eigenvalue problem $\bA V = \bB V\Lambda$, $V^\top \bB V = I$.  % where $\Lambda$ is the diagonal matrix of eigenvalues and the matrix $V$ has columns being eigenvectors orthonormal.
	 \State Compute $\widetilde \bA= \bC_{*}^{\top}\bA \bC_{*}$ and $\widetilde \bb = \bC_{*} \bb $ with $ \bC_{*} = V \Lambda^{1/2}$. 	 % (V \Lambda V^\top)^{1/2}$. 
	\State Use the L-curve method to find an optimal estimator $\widehat \bphi_{\lambda_0}$: 
	\begin{itemize}
	\item Set the range for $\lambda$ to be the range of the eigenvalues in $\Lambda$. 
	\item For each $\lambda$,  solve $\widetilde \bphi_\lambda$ from $(\widetilde \bA +\lambda I) \widetilde \bphi_\lambda = \widetilde\bb$ by least squares with minimal norm and set $ \widehat \bphi_\lambda = \bC_* \widetilde \bphi_\lambda $. 
	\item  Select $\lambda_0$ maximizing the curvature of the $\lambda$-curve $(\log \calE( \widehat \bphi_\lambda), \log(\widehat \bphi_\lambda^\top\bC_{rkhs} \widehat \bphi_\lambda ))$. 
	\end{itemize}
	% , where the least squares estimator $\widehat \bphi_\lambda = (\bA+ \lambda\bC_{rkhs})^{-1}\bb$  minimizes the regularized loss function $\calE_\lambda (\bphi) =\calE(\bphi)   + \lambda \bphi^\top\bC_{rkhs} \bphi$. 
%	\[ \calE_\lambda (\bphi) =\calE(\bphi)   + \lambda \bphi^\top\bC_{rkhs} \bphi \, \, \text{ with } \, \calE(\bphi) =  \bphi^\top\bA \bphi  -2\bphi^\top \bb +  \bb^\top \bA^{-1} \bb,\] where the matrix inversion is a pseudo-inverse when it is singular. 
\end{algorithmic}
\caption{Inversion for the discrete model with an RKHS Regularization. 
 }\label{alg:dartr}
}
\end{algorithm}\vspace{-2mm}




\paragraph{Function space of identifiability (FSOI).} Recall that the FSOI $H$ % is the complement of the null space of $\LGbar$, denoted by $N(\LGbar)^\perp$. In other words, it 
is the linear subspace of $L^2_\rho(\calS)$ spanned by the eigenvectors of $\LGbar$ with nonzero eigenvalues. 
In computation, the FSOI is the eigenspace of $\LGbar$ spanned by the eigenvectors with eigenvalues greater than the numerical precision. Figure \ref{fig:eigVals} suggests that the FSOI is low-dimensional and contains only a few eigenvectors. 

 

Also, we access the performance of an estimator by its accuracy in estimating the projection of the true solution in the FSOI, 
since the data provides no information for estimating the components outside the FSOI. That is, we compute the error of an estimator $\bphi$ as 
\begin{equation}\label{eq:error}
Err = \|  P_H \widehat{\bphi} - P_H \bphi_* \|_{L^2_\rho(\calS)}^2, 
\end{equation} 
where $P_H$ is the projection to $H$. 


\subsection{Convergence as noise decays}
We test the convergence of the regularized estimators with decaying noise, in which we set the noise-to-signal ratio to be $nsr \in \{0.125,0.25,0.5,1,2\}$ and $\Delta t = 0.01$. Recall that $\sigma= \|\bL \bphi  \|  \times nsr$, so the noise-to-signal ratio is linear in $\sigma$. 


We consider two scenarios: (i) the true solution is the second eigenvector, thus it is \emph{inside the FSOI}; and (ii) the true solution is $\phi(x)= x^2$, which has significant components \emph{outside the FSOI}. We note that the later is relatively challenging to recover because the large values of the $\phi$ are explored little by the measure $\rho$ in \eqref{eq:rho_example}. 



Figure \ref{fig:estimator} shows the typical regularized estimators in the two scenarios, as well as their de-noised output $\widehat{y} = \bL \widehat \bphi$. Part (a) shows that when the true solution is inside the FSOI, the RKHS regularizer significantly outperforms the other two.  However, all the regularized estimators can recover the true signal (or de-noise the data) accurately, despite the fact that theses estimators may have a large bias. Part (b) shows that when the true solution has components outside the FSOI, the RKHS regularizer can be less accurate than the $L^2_\rho(\calS)$ regularizer, while both significantly outperform the $l^2$ regularizer. Yet again, all regularized estimators can de-noise the data accurately even when they have large errors.  
Thus, this inverse problem is severely ill-defined, and one must restrict the inverse to be in the FSOI. 

Figure \ref{fig:conv_nsr} demonstrates the convergence of the regularized estimators and their loss values as the noise-to-signal-ratio decreases, with $nsr \in \{0.125,0.25,0.5,1,2\}$. It presents the mean and standard deviations (SD) of the regularized estimators' $L^2_\rho(\calS)$ errors as well as the loss values in 100 noise realizations. The errors are computed for the estimators' projection inside the FSOI as in \eqref{eq:error}.  When the true solution is inside the FSOI, the RKHS regularized estimator has errors significantly smaller than those of the other two regularizers, while having the largest loss values. When the true solution is outside the FSOI, the RKHS regularized estimator has slightly smaller errors than the $L^2_\rho(\calS)$ regularizer, and both outperform the $l^2$ regularizer. Importantly, when the true solution is inside FOSI, the RKHS-regularized estimator shows a clear linear error decay in noise for $nsr\in \{0.25,0.5,1,2\}$, while the $L^2_\rho$-regularized estimator has an error remaining at a constant level. Both patterns of convergence agree with Theorem \ref{thm:conv_sigma}. 

% The linear decay of the RKHS-regularized estimator and the constant level of the $L^2_\rho$-regularized estimator 
% We note that the slope of decay is not uniform and it flattens as the noise level becomes small, due to the increasing difficulty in selecting the optimal hyper-parameter by the L-curve method.  

 

\begin{figure}[H]	\vspace{-2mm} 
    \centering 	
    {\includegraphics[width =0.49\textwidth]{insideFSOI_estimator.pdf}
    \includegraphics[width =0.49\textwidth]{outsideFSOI_estimator.pdf}
    }
       (a) True solution inside the FSOI \hspace{20mm}    (b) True solution Outside the FSOI\\
	\vspace{-2mm} 
\caption{Typical regularized estimators when $nsr=2$ and their recovery of the signal. % (a): the true solution is inside the FSOI. Right: the true solution is outside the FSOI. 
All estimators recover the true signal accurately. 
 The RKHS regularizer significantly outperforms the other two regularizers in (a), but it slightly underperforms the $L^2_\rho(\calS)$ regularizer in (b). 
} 
\label{fig:estimator}		\vspace{-5mm}
\end{figure}



\begin{figure}[H]	\vspace{-2mm} 
    \centering 	
    {\includegraphics[width =0.48\textwidth]{insideFSOI_L2rho_Loss2.pdf}}
    {\includegraphics[width =0.48\textwidth]{outsideFSOI_L2rho_Loss2.pdf}} \\
   (a) True solution inside the FSOI.  \hspace{20mm}  (b) True solution outside the FSOI.\\
	\vspace{2mm}  
\caption{Mean and standard deviations of the regularized estimators' $L^2_\rho(\calS)$ errors in the FSOI and loss function values in 100 noise realizations. 
 The RKHS regularizer significantly outperforms the other two regularizers in (a), but it slightly outperforms the $L^2_\rho(\calS)$ regularizer in (b). In both cases, the RKHS regularized estimator has errors decaying with the noise. 
 } \label{fig:conv_nsr}		\vspace{-5mm}
\end{figure}



\subsection{Convergence as observation mesh refines}
Next, we test the convergence of the regularized estimator as the observation mesh refines.  We set $\Delta t\in 0.005\times \{1,2,4,8,16\}$ and $nsr =1$. 

We continue to consider two scenarios as in the previous section: (i) the true solution is the second eigenvector; thus it is \emph{inside the FSOI}; and (ii) the true solution is $\phi(x)= x^2$, which has significant components \emph{outside the FSOI}. We compute the FSOI using a finer observation mesh $\Delta t = 0.0005$. 


Figure \ref{fig:conv_dt} demonstrates the convergence of the regularized estimators and their loss values as the observation mesh increases. It presents the mean and standard deviations (SD) of the regularized estimators' $L^2_\rho(\calS)$ errors and the loss values in 100 noise realizations. Similar to the case of noise decay in the previous section, the RKHS regularized estimator has errors significantly smaller than those of the other two regularizers when the true solution is inside the FSOI, but it slightly underperforms the $L^2$ regularizer when the true solution is outside the FSOI, and both outperforms the $l^2$ regularizer. Importantly, in both cases, the RKHS regularized estimator has errors consistently decaying with the time mesh $\Delta t$.  


\begin{figure}[H]	\vspace{-2mm} 
    \centering 	
    {\includegraphics[width =0.48\textwidth]{insideFSOI_ymesh_L2rho_Loss_discA2.pdf}}
    {\includegraphics[width =0.48\textwidth]{outsideFSOI_ymesh_L2rho_Loss_discA2.pdf}} \\
   (a) True solution inside the FSOI.  \hspace{20mm}  (b) True solution outside the FSOI.\\
	\vspace{2mm}  
\caption{Mean and standard deviations of the regularized estimators' $L^2_\rho(\calS)$ errors in the FSOI and loss function values  in 100 noise realizations. 
 The RKHS regularizer significantly outperforms the other two regularizers in (a), but it slightly outperforms the $L^2_\rho(\calS)$ regularizer in (b). In both cases, the RKHS regularized estimator has errors consistently decaying with the time mesh $\Delta t$. 
 } \label{fig:conv_dt}		\vspace{-5mm}
\end{figure}



%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%

\section{Conclusion and future work}\label{sec:conlusion}
We have introduced an adaptive RKHS-regularization for ill-posed linear inverse problems. The new element is an RKHS that arises in the variational formulation of the inverse problem. Its closure is the function space of identifiability determined by the linear operator and the observation mesh. The RKHS-regularization uses the L-curve method to select the optimal hyper-parameter; thus, it is adaptive to discrete noisy measurement data and the underlying linear operator.

We have proved that the RKHS-regularized estimator has a mean-square error converging linearly in the noise scale, whereas the commonly-used $L^2$-regularized estimator has a flat error. Furthermore, numerical tests confirm the robust convergence of the estimator when either the noise decays or the observation mesh refines. 

This study focuses on the direct regularization approach that solves linear inversion by matrix decomposition. A future direction is the iterative methods that minimize the penalized loss functional via iterations, which is suitable for solving high-dimension problems.   


 \paragraph{Acknowledgments} The work of FL is partially funded by the Johns Hopkins University Catalyst Award and AFOSR FA9550-21-1-0317 and FA9550-20-1-0288. FL would like to thank Quanjun Lang for helpful discussions on the L-curve methods. The work of MYO is partially supported by the European Union's Horizon 2020 Research and Innovation Programme under the Marie Sk\l odowska-Curie grant agreement 101008231 (EXPOWER).
 
 \bibliographystyle{plain}
\bibliography{ref_invLap,library,ref_regularization,ref_FeiLu2023_2}

\end{document}
