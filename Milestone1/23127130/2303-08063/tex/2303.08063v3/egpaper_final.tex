\documentclass[10pt,onecolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{subfigure}
\usepackage{parskip}
\usepackage{indentfirst}
\setlength{\parindent}{1em}
\maxdeadcycles=1000
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Interpretable ODE-style Generative Diffusion Model via Force Field Construction}

\author{Weiyang Jin\\
 Beijing Jiaotong University\\
bjtu.edu.cn\\
{\tt\small 21721009@bjtu.edu.cn}
% 对于作者都在同一机构的论文，
% 省略以下行，直到结束 ''}''。
% 可以使用“\and”添加其他作者和地址，
%就像第二作者一样。
% 要节省空间，请使用电子邮件地址或主页，而不是两者兼而有之
\and
Yongpei Zhu\\
Tsinghua University\\
tsinghua.edu.cn\\
{\tt\small zhuyp20@mails.tsinghua.edu.cn}
\and
Yuxi Peng\\
 Beijing Jiaotong University\\
bjtu.edu.cn\\
{\tt\small 20721017@bjtu.edu.cn}
}
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}

For a considerable time, researchers have focused on developing a method that establishes a deep connection between the generative diffusion model and mathematical physics. Despite previous efforts, progress has been limited to the pursuit of a single specialized method. In order to advance the interpretability of diffusion models and explore new research directions, it is essential to establish a unified ODE-style generative diffusion model. Such a model should draw inspiration from physical models and possess a clear geometric meaning. This paper aims to identify various physical models that are suitable for constructing ODE-style generative diffusion models accurately from a mathematical perspective. We then summarize these models into a unified method. Additionally, we perform a case study where we use the theoretical model identified by our method to develop a range of new diffusion model methods, and conduct experiments. Our experiments on CIFAR-10 demonstrate the effectiveness of our approach. We have constructed a computational framework that attains highly proficient results with regards to image generation speed, alongside an additional model that demonstrates exceptional performance in both Inception score and FID score. These results underscore the significance of our method in advancing the field of diffusion models.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Deep generative models have found successful applications in several fields such as computer vision \cite{brock2018large}, density estimation \cite{liu2021density}, natural language processing \cite{brown2020language}, and semi-supervised learning \cite{kingma2014semi}. They also provide a promising paradigm for unsupervised learning. However, despite their popularity, current deep generative models face several limitations. These limitations include training instability of GANs \cite{brock2018large,gulrajani2017improved,karras2020training}, sample quality issues of VAEs \cite{rezende2015variational}, normalized flow \cite{kobyzev2020normalizing,papamakarios2021normalizing}, and slow sampling speed of diffusion \cite{ho2020denoising,song2020denoising,yang2022diffusion,jayaram2021parallel} and score-based models \cite{song2020score}. Recently, Poisson flow generation model (PFGM) \cite{xu2022poisson} proposed an ODE-like generative-diffusion model inspired by Coulomb force, which has a clear geometric meaning. Although this approach is intuitive in physics, Coulomb force is not the only option for model building, and it seems possible to construct other forms of force using the same physical painting. Furthermore, while the model's physics-based foundation is strong, there is a lack of mathematical proof that it can learn the process of moving along the field line from an arbitrary point in the distance to a certain Coulomb force source data distribution. As such, it is imperative for researchers to investigate alternative modes of energy to foster model development, while concurrently providing a rigorous mathematical validation of the learning mechanism inherent to the ODE-based generative-diffusion model of PFGM.

To address the issue of determining the appropriate force field for constructing an ODE-style generative diffusion model, we propose a review of the fundamental task of the diffusion model. This task involves transforming samples from a simple distribution to samples from a target distribution by constructing a suitable transformation. Theoretically, constructing an ODE-style diffusion model \cite{song2020score} is equivalent to solving a relaxed and almost unconstrained indeterminate equation.

\begin{figure}[htbp]
\begin{center}
    \centering
   
        \includegraphics[width=3in]{p13.png}
        
    
\end{center}
    \caption{The process of generative model based on force field.}
    \label{fig.8}
    
\end{figure}
 Previous research has proposed specialized solutions for generative diffusion models, such as PFGM. This model utilizes Coulomb's law to construct a potential function that is inversely proportional to the $Nth$ power, forming the basis of the kinetic model. Using the field of potential functions, a flow model is defined where the probability distribution evolves based on the gradient flow $ \partial p_{t}(\mathbf{x}) / \partial t = -\nabla \cdot\left(p_{t}(\mathbf{x}) \mathbf{E}(\mathbf{x})\right) $. It is worth noting that gradient flow is a special case of the Fock-Planck equation~\cite{risken1984solutions}. The solution proposed by PFGM is an indeterminate equation with $ d+1 $ unknowns and one equation, indicating that there are potentially infinite solutions in theory. To derive a practical solution, additional assumptions are necessary.

 In this paper, we provide a mathematical proof that the physical model we constructed is capable of learning the target data distribution. Additionally, we present a unified method for identifying the appropriate force field for the generative diffusion model and summarize the construction process of this model in detail. A series of technical contributions we made are as follows. In practical applications, data distribution migration is achieved through solving a set of coupled forward and backward ordinary differential equations induced by a force field (refer to Figure \ref{fig.1}).

Starting from geometric intuition, we construct a specific vector field to ensure that the result satisfies the initial value distribution conditions, and then solve the differential equation to ensure the final value distribution conditions, and obtain a Green's function that satisfies both the initial value and the final value conditions. In particular, this method allows us to use arbitrary simple distributions as prior distributions, freeing us from previous reliance on Gaussian distributions to construct diffusion models. Finally, in experiments, we find two force field construction methods that perform well in terms of speed and image generation quality through this method.
\begin{figure}[htbp]
\begin{center}
    \centering
   
        \includegraphics[width=3in]{p7.png}
        
    
\end{center}
    \caption{The evolutions of a distribution by the field.}
    \label{fig.1}
    
\end{figure}
%-------------------------------------------------------------------------
\section{Related Works}


$\textbf{Normalizing Flow}$. Taking image generation as an example, the idea of Normalizing Flow \cite{kobyzev2020normalizing,papamakarios2021normalizing} is to find an invertible function $ f $, which can convert the distribution of $ X $ to a known simple distribution (such as Gaussian distribution) for an image set $ X $. Then generating graphics becomes very simple, as long as a point is sampled from a known simple distribution, and then through the inverse function of $ f $, the picture can be generated. The Continuous Normalizing Flows (CNFs) \cite{lipman2022flow} propose the concept of Flow Matching (FM), a simulation-free method for training CNFs based on regression vector fields with fixed conditional probability paths. Compared with Variational Autoencoder (VAE), the Normalizing Flow method \cite{dinh2016density} has achieved better performance in various image tasks.

 $\textbf{Score-based Generative Models}$. Score-based generative model \cite{song2020score} learns a kind of score functions, that is, gradients of log probability density functions, on large-scale noise-perturbed data distributions, and uses Langevin to sample samples that conform to the training set. In denoising diffusion probabilistic models (DDPM) \cite{ho2020denoising,song2020denoising,yang2022diffusion,jayaram2021parallel}, the diffusion process is divided into fixed T steps. In fact, the real data transformation process should have no deliberate steps, and they can be understood as a continuous transformation process in time, described by Stochastic Differential Equation (SDE) \cite{song2020score}. Therefore, the advantage of this method is that it can get the sampling effect of GAN level without confrontation learning. At the same time, it has uniquely identifiable representation learning \cite{ahuja2021properties} at the level of interpretability. 

$\textbf{Poisson Flow Generation Model}$. Poisson flow generation model \cite{xu2022poisson} is an ODE-style diffusion model inspired by "Coulomb's Law", which breaks through the dependence of many previous diffusion models on Gaussian assumptions, and is a new framework for constructing ODE-style diffusion models based on field theory. Therefore, our subsequent work will focus on mathematically proving how to finally learn the data distribution and the expansion of various field forms, and on the premise of finding out what kind of force field is suitable for building an ODE-style generative diffusion model, we can obtain a brand-new force field diffusion model that adapts to different requirements is developed.

\section{Background Knowledge}
$\textbf{Transport Equation}$. The transport equations can be expressed by flux integrals applicable to any finite area or by divergence operators applicable to a certain point. In this paper, Fourier's heat conduction equation will be used\begin{equation}\frac{\partial}{\partial t}F_t(\boldsymbol{x}_t) = D_t \nabla_{\boldsymbol{x}_t}^2 F_t(\boldsymbol{x}_t)\label{eq:heat}\end{equation}where $\boldsymbol{D}_t(\boldsymbol{x}_t)$ is a non-negative scalar function used to construct a loss function solution.

$\textbf{Mode Collapse}$. The Poisson flow generative model proposes a concept of mode collapse \cite{xu2022poisson}. This collapse comes from the force field offset generated by the isotropic solution, so it is necessary to add an additional dimension when constructing the force field model. In the experimental part of this paper, the mode collapse of the multi-sample straight line case will also be discussed.

$\textbf{Generative Modeling via Force Fields}$. Generative modeling can enable the transformation of a base distribution into a data distribution through trajectories defined by li force fields. Distribution-based samplers allow for adaptive sampling, accurate likelihood assessment, and modeling of continuous-time dynamics. For sampling, the learned reversible mappings can be directly integrated.

$\textbf{Dirac Delta Function}$. The modeling of particles and point charges relies on the concept of limits, whereby the volume is made infinitely large. However, this results in an infinite density of particles or charge density of point charges, which can be resolved by integrating the density in space to obtain a finite mass or charge. To describe such a density distribution, the Dirac delta function is employed. This scenario aligns with the initial value conditional constraints of the generator, which produces the data points in the model, and serves as a necessary demonstration of the solvability of the dynamical differential equation.

\section{Method}

\subsection{Construct the Force Field Equation}

$\textbf{Preparatory Work}$. Consider the system of first-order ordinary differential equations $ \frac{d\boldsymbol{x}_t}{dt}=\boldsymbol{F}_t(\boldsymbol{x}_t)\label{eq:ode} $ of $ \boldsymbol{x}_t\in\mathbb{R}^d, t\in(0,T) $. It describes a reversible transformation from $\boldsymbol{x}_0$ to $\boldsymbol{x}_T$. If $\boldsymbol{x}_0$ is a random variable, then $\boldsymbol{x}_t$ in the whole process is also a random variable. This distribution law can be described by the following equation
\begin{equation}\frac{\partial}{\partial t} d_t(\boldsymbol{x}_t) = - \nabla_{\boldsymbol{x}_t}\cdot\Big(\boldsymbol{F}_t(\boldsymbol{x}_t) d_t(\boldsymbol{x}_t)\Big)\label{eq:ode-f-eq-fp}\end{equation}The formulation of this equation is motivated by the transport equation in physics that characterizes the scenario where energy can only propagate through a continuous flux, bearing a striking resemblance to the diffusion model. The objective of the  model is to develop a transformation that converts samples originating from a basic distribution into samples that conform to a desired target distribution. Equation \eqref{eq:ode-f-eq-fp} enables one to identify the appropriate $ F_t(\boldsymbol{x}_t)$ for a given $d_t(\boldsymbol{x}_t)$ in principle.

$\textbf{Solving ODE Equations}$. First, by transposing Equation \eqref{eq:ode-f-eq-fp} to get $\left({\partial} / {\partial t}, \nabla_{\boldsymbol{x}_t}\right)\cdot (d_t( \boldsymbol{x}_t), \boldsymbol{F}_t(\boldsymbol{x}_t) d_t(\boldsymbol{x}_t))=0$, $\left({\partial} / {\partial t},\nabla_{\boldsymbol{x}_t}\right)$ can be regarded as a $d+1$ dimensional gradient, $\big(p_t( \boldsymbol{x}_t), \boldsymbol{F}_t(\boldsymbol{x}_t) d_t(\boldsymbol{x}_t)\big)$ can just form a $d+1$ vector , so Equation \eqref{eq:ode-f-eq-fp} can be written as a simple divergence equation $\nabla_{(t,\, \boldsymbol{x}_t)}\cdot\boldsymbol{v}(t, \boldsymbol{x}_t)=0$. In this way, it is not difficult for us to combine the general form of ODE to deduce the equation\begin{equation}\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{F}_t(\boldsymbol{x}_t) = \frac{\boldsymbol{v}_{i(i>1)}(t, \boldsymbol{x}_t)}{\boldsymbol{v}_1(t, \boldsymbol{x}_t)}\label{eq:div-eq-ode}\end{equation}Since our purpose is to derive the generated target sample distribution, there will be an initial value constraint such that $\boldsymbol{v}_1 = d_0(\boldsymbol{x}_0)$. This constraint unifies the probability and distribution of the initial distribution. Equation \eqref{eq:div-eq-ode} describes the trajectory of data points moving along the field lines. For such a conditional differential equation, by introducing a function $\boldsymbol{H}(x)$ to assist the solution\begin{equation}\left\{\begin{aligned} 
&\nabla_{(t,\, \boldsymbol{x}_t)}\cdot\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)=0\\ 
&\boldsymbol{H}_1(0, 0; \boldsymbol{x}_t, \boldsymbol{x}_0) = \delta(\boldsymbol{x}_t - \boldsymbol{x}_0)\\
&\boldsymbol{H}_1(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0) d\boldsymbol{x}_t = 1 
\end{aligned}\right.\label{eq:div-green}\end{equation}From this a feasible solution can be deduced: $\boldsymbol{v}(t, \boldsymbol{x}_t) =  \mathbb{E}_{(\boldsymbol{x}_0)}[\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)]$. Among them, $\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)$ is another way of expressing the conditional probability in the score-based model \cite{song2020score}. In particular, the defined $\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)$ still has the properties similar to the divergence function in mathematics, and it itself can be equivalent to the force field produced by the point source. Therefore, in the subsequent construction of the force field model, we use the properties of the $H(x)$ function to superimpose different force field trajectories to achieve different model effects, and find the model we need among them.

%-------------------------------------------------------------------------
\subsection{Solutions for Special Case}

Now, we proceed to solve specific results within the aforementioned framework. As previously mentioned, the aforementioned equations represent a type of indeterminate equations, which theoretically possess infinitely many solutions. However, in order to obtain a more clear solution, we need to introduce additional assumptions.

An isotropic assumption was put forward in the PFGM \cite{xu2022poisson}. This means that ${H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)$ points to the source point $(0,\,\boldsymbol{x}_0)$, and the modulus length only depends on $R = \sqrt{(t-0)^2 + \Vert \boldsymbol{x}_t - \boldsymbol{x}_0\Vert^2}$. Accordingly, it can be assumed that\begin{equation}\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0) = \varphi(R)(t, \boldsymbol{x}_t - \boldsymbol{x}_0)\label{eq5}\end{equation}Thus, a unique solution situation is constructed. Substituting the first equation in Equation \eqref{eq:div-green}, it can be deduced that $0 = \nabla_{(t,\, \boldsymbol{x}_t)}[\varphi(R)(t, \boldsymbol{x}_t - \boldsymbol{x}_0)]$. And then come to $\varphi(R)=A\times R^{-(d+1)}$. Finally, it is not difficult to have \begin{equation}\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0) = A\frac{(t, \boldsymbol{x}_t - \boldsymbol{x}_0)}{\left(t^2 + \Vert \boldsymbol{x}_t - \boldsymbol{x}_0\Vert^2\right)^{(d+1)/2}} \end{equation}Where $A$ represents a constant derived from the homogeneous solution of the differential equation for $\varphi(R)$. The existence of a solution notwithstanding, it is imperative to test whether the equation is solvable, which calls for a test based on the constraints imposed by Equation \eqref{eq5}. A constructive integral is employed for this purpose, given by

\begin{equation}\int\boldsymbol{H}_1 d\boldsymbol{x}_t  = A \int\frac{t}{\left(t^2 + \Vert \boldsymbol{x}_t - \boldsymbol{x}_0\Vert^2\right)^{(d+1)/2}}d\boldsymbol{x}_t\end{equation}

Here, $t$ and $\boldsymbol{x}_t$ can be eliminated by substitution. Let $\boldsymbol{x}=(\boldsymbol{x}_t-\boldsymbol{x}_0)/t$, then the points in the original equation take the following form\begin{equation}
\boldsymbol{H}_1(t,0;\boldsymbol{x}_t,\boldsymbol{x}_0)d\boldsymbol{x}_t = A\int \frac{1}{\left(1+\vert\boldsymbol{x}\vert^2\right)^{(d+1)/2}} d\boldsymbol{x}\label{EQ:PZ}
\end{equation}It is evident that the points result and the quantities of $\boldsymbol{x}_t$ and $t$ are independent. With the help of a constant conversion, the $H(x)$ function can be shown to be equal to one.



It can be observed that the particular solution bears a formal resemblance to the solution class proffered by the PFGM. Hence, we may infer that the Coulomb force's physical dynamic structure is a distinct instance of the equation, and as such, may be extended to more general cases.



\subsection{Establish Force Field Learning Mode}
\begin{figure}[htbp]
\begin{center}
    \centering
    \subfigure[One Sample Line]{
        \includegraphics[width=1.5in]{p1.pdf}
        \label{1.1}
    }
    \subfigure[Curve]{
	\includegraphics[width=1.5in]{p2.pdf}
        \label{1.2}
    }
    \subfigure[Multiple Sample Line]{
    	\includegraphics[width=1.5in]{p3.pdf}
        \label{1.3}
    }
\end{center}
    \caption{A variety of different trajectories from ${x}_T$ to ${x}_0$.}
    \label{fig.2}
    
\end{figure}
$\textbf{Convert a trainable target}$. First, construct a force field function that can be used 
 $\boldsymbol{L}_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = \boldsymbol{x}_T$. The aforementioned construct pertains to a cluster of trajectories, characterized by a fixed starting point $\boldsymbol{x}_0$ and an arbitrary end point $\boldsymbol{x}_T$. In this construct, the independent and dependent variables are represented by $t$ and $\boldsymbol{x}_t$, respectively. Notably, the starting point remains unchanged while the end point can be varied at will. Substituting ${x}_T$ from it to Equation \eqref{eq:div-green}, we can get $\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)=d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)
$. Upon derivation, the subsequent objectives can be acquired
\begin{equation}\mathbb{E}_{\boldsymbol{x}_0,\boldsymbol{x}_t \sim d_t(\boldsymbol{x}_t|\boldsymbol{x}_0)d 
_0(\boldsymbol{x}_0)}\left[\left\Vert \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) - \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2\right] 
\label{eq:score-match}\end{equation}
Equation \eqref{eq:score-match} describes the objective function of training in terms of score matching, where $\theta$ represents the training parameter. The goal is to learn a model $\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)$ that approximates $\boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$, which can be expressed in various ways, such as using $\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)$ in the probability model \cite{ho2020denoising,song2020denoising,yang2022diffusion}. In our case, we represent it using the inspired force field method as a set of force field trajectory modes. Finally, we use network design and sampling to improve the learning of this score matching function.

$\textbf{Introduction of target trajectories}$. We define three regression processes from ${x}_T$ to ${x}_0$ through the above method: linear type, distribution type, and curve type.
\begin{equation}\left\{\begin{aligned} 
&d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = \frac{d_T\left(\frac{\boldsymbol{x}_t - \boldsymbol{x}_0}{t} + \boldsymbol{x}_0\right)}{t^N}\\ 
&d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = \frac{d_T\left(\frac{\boldsymbol{x}_t - \boldsymbol{\mu}_t({x}_0)}{{{\sigma}_t}} \right)}{{{\sigma}_t}^N}\\
&d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = \frac{d_T\left(\frac{(t-1)(\boldsymbol{x}_t - \boldsymbol{x}_0)}{t^m} \right)}{t^{Nm}}
\end{aligned}\right.\label{eq10}\end{equation}

Taking the straight line type as an example, we can perform a simple derivation. For a linear relationship, the vector form of the line can be written $\boldsymbol{x}_t = (\boldsymbol{x}_T - \boldsymbol{x}_0)t + \boldsymbol{x}_0$. Then according to the differential equation established above, there will be $\nabla_{\boldsymbol{x}_t}\cdot \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)=\frac{d}{t}$. Finally, the above conclusion can be drawn.

Figure \ref{fig.2} employs a geometric approach to illustrate the movement of single-sample data points across various trajectories under different conditions when $T=1$. Further, Figure \ref{1.1} depicts the transformation process from a fixed sampling pattern to the original distribution in the case of a single-sample force field trajectory, where the sample points move linearly, resulting in high generation efficiency. In contrast, Figure \ref{1.2} illustrates the curved trajectory, which can adaptively fit the information hot spots of the sample under different force fields, while mitigating the influence of singularities and achieving better model robustness. Finally, Figure \ref{1.3} demonstrates a multi-sample straight line force field trajectory, which combines both straight line trajectories and curved trajectories to achieve good performance. As the sample size increases, the force field becomes more complex and the trajectory can be fitted to a more intricate situation. Therefore, we propose two approaches for identifying the most suitable training method: 1) utilizing a single straight line for multiple multi-sample training to fit various distributions, and 2) directly sampling multiple measurements far apart using a curved trajectory. These approaches will be discussed separately in Section \ref{E} of the experiment, and a better solution will be determined based on the specific conditions.
\begin{figure}[htbp]
\begin{center}
    \centering
    \subfigure[Gaussian Distribution]{
        \includegraphics[width=1.5in]{p8.3.png}
        \label{A.1}
    }
    \subfigure[One Sample Line]{
	\includegraphics[width=1.5in]{p8.1.png}
        \label{A.2}
    }
    \subfigure[Curve]{
    	\includegraphics[width=1.5in]{p8.2.png}
        \label{A.3}
    }
\end{center}
    \caption{Sampling methods for different situations.}
    \label{fig.A}
    
\end{figure}

$\textbf{How to sample}$. The Poisson flow generation model gives a more effective sampling mode \cite{xu2022poisson}.\begin{equation}\boldsymbol{x}_t = \boldsymbol{x}_0 + \Vert \boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert (1+\tau)^m \boldsymbol{u},\quad t = |\varepsilon_t| (1+\tau)^m\label{eq11}\end{equation}Among them $(\boldsymbol{\varepsilon}_{\boldsymbol{x}},\varepsilon_t)\sim\mathcal{N}(\boldsymbol{0}, \sigma^2\boldsymbol{I}_{(d+1)\times(d+1)}), m\sim U[0,M]$, $u$ is a unit vector uniformly distributed on the d-dimensional unit sphere, and $\tau$, $\sigma$ and $M$ are both constants. This design is much simpler to implement than the inverse cumulative function method, by changing the parameters, this type of sampling method has good generalizability, and the three trajectories we provide also use this type of method. According to Equation \eqref{EQ:PZ}, we can get the integration of the sampling mode and the $ H (x) $ function. For these formulas, it can be regarded as $P(H(\boldsymbol{x})) \propto {\left(1 + \Vert \boldsymbol{x}\Vert^2\right)^{-(d+1)/2}}$, that is, the probability function of $ H(x) $ is proportional to the latter. After substituting the Equation \eqref{eq11}, we will get \begin{equation}\begin{aligned} &\boldsymbol{x}_t = \boldsymbol{x}_0 + t\frac{\Vert \boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert}{|\varepsilon_t|} \boldsymbol{u} 
\\&p(\Vert\boldsymbol{x}\Vert) \propto \frac{\Vert \boldsymbol{x}\Vert^{d-1}}{\left(1 + \Vert \boldsymbol{x}\Vert^2\right)^{(d+1)/2}} \end{aligned} \label{eq12}\end{equation} The normal distribution is applicable for $\boldsymbol{\varepsilon}_{\boldsymbol{x}}$ and $\varepsilon_t$ in the above expression, which allows us to directly deduce that the probability function of $\frac{\Vert \boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert}{|\varepsilon_t|}$ is consistent with the Equation \eqref{eq12} presented below. Similarly, based on this structural property, one can deduce suitable sampling methods for single-sample straight line trajectory force fields and curved force fields, respectively\begin{equation}\begin{aligned}&\boldsymbol{x}_t = \boldsymbol{x}_0 + \Vert \boldsymbol{\varepsilon}_{\boldsymbol{x1}}\Vert \boldsymbol{u},\quad t = |\varepsilon_{t1}|  \\ &\boldsymbol{x}_t = \boldsymbol{x}_0 + \Vert \boldsymbol{\varepsilon}_{\boldsymbol{x2}}\Vert (1+\tau)^n \boldsymbol{u},\quad t = |\varepsilon_{t2}| (1+\tau)^n \end{aligned}\label{eq13}\end{equation}Both $(\boldsymbol{\varepsilon}_{\boldsymbol{x1}},\varepsilon_{t1})$ and $(\boldsymbol{\varepsilon}_{\boldsymbol{x2}},\varepsilon_{t2})$ are drawn from a uniform distribution $U[0,T]$, and the choice of $n$ is determined by the number of curves. Figure \ref{fig.A} visually illustrates the distinction between various sampling methods. It is apparent that the sampling approach is ideal for simple and original distributions. Our goal is to select the most appropriate sampling method to enhance the performance of the model, which is also the direction we have been considering in the preceding derivation. Additionally, as the original distribution pattern of the simple distribution of sampling varies with different force field model designs, it emphasizes the significance of deriving a unified sampling method.

\subsection{Conditional Control of Force Field Models}
Note that there are various paths for the transformation of any data point. Therefore, we need to theoretically prove a better solution model range, in order to better find the force field model under this conditional constraint. Among them, we assume two trajectories \begin{equation}\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{f}_t(\boldsymbol{x}_t),\quad \frac{ d\boldsymbol{y}_t}{dt} = \boldsymbol{g}_t(\boldsymbol{y}_t)\end{equation}
And in the conclusion of the stochastic differential equation \cite{song2020score}, there is an evolution equation \begin{equation}d\boldsymbol{x}_t = \left[\boldsymbol{f}_t(\boldsymbol{x}_t) - g_t^2 \nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t) \right] dt + g_t d\boldsymbol{w}\label{eq:reverse-sde}\end{equation}
which can be used to compare the evolutionary differences of different trajectories. First write the evolution equations of the two trajectories, and then perform a type of scaling to get the main difference as \begin{equation}-\frac{1}{2}g_t^2\mathbb{E}_{\boldsymbol{z }}\left[ (\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)-\nabla_{\boldsymbol {y}_t}\log q_t(\boldsymbol{y}_t))\right]\end{equation}
Using this indicator, we can perform a horizontal comparison of the similarities and differences between various types of trajectories during the evolutionary process. This enables us to better identify optimal solutions for a specific type of trajectory under given conditions, and significantly simplifies the search scope for obtaining the desired properties in experiments. It is noteworthy that the mathematical demonstration of the minimum value of this equation is unfeasible due to the existence of the inequality\begin{equation}\mathbb{E}_{\boldsymbol{z}}\left[ (\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)-\nabla_{\boldsymbol{y}_t}\log q_t(\boldsymbol{y}_t))\right]\geq 0\end{equation}is easy to give a counterexample. Consequently, the estimation process necessitates the derivation of an optimal solution predicated on the particular distribution of diverse trajectories.

\begin{table*}
	\begin{center}
		\begin{tabular}{l llll}
\hline
                                                       & Invertible? & Inception↑      & FID↓           & NFE↓        \\ \hline
StyleGAN2-ADA \cite{karras2020training}                                          & NO         & 10.10          & 2.42          & 1 \\
Glow \cite{kingma2018glow}                                                   & Yes        & 3.93           & 48.7          & 1 \\
DDIM,T=50 \cite{song2020denoising}                                              & Yes        & -              & 4.82          & 50         \\
DDIM,T=100 \cite{song2020denoising}                                              & Yes        & -              & 4.33          & 100        \\
PGFM \cite{xu2022poisson}                                                   & Yes        & 9.65           & 2.48          & 110        \\
Diffusion model with one-sample straight line          & Yes        & 10.12          & 18.70          & \textbf{24}         \\
Diffusion model with multi-sample linear superposition & Yes        & \textbf{12.11} & \textbf{2.33} & 800        \\
Diffusion model with curve                             & Yes        & 9.23           & 2.65          & 102        \\
Diffusion model with Gaussian distribution             & Yes        & 9.58           & 3.06          & 1000       \\ \hline\\
\end{tabular}
	\end{center}
 \caption{CIFAR-10 sample quality (FID, Inception) and number of function evaluation (NFE).}\label{tab1}
\end{table*}

\section{Experiment}\label{E}


$\textbf{Dataset}$. To identify force field patterns with high speed and high generation quality, we employed the CIFAR-10 dataset, which is a color image dataset that is closer to universal objects. CIFAR-10 was compiled by Hinton's students Alex Krizhevsky and Ilya Sutskever for pervasive object recognition and contains 10 categories of RGB color images. The images are sized at 32 × 32, and each category has 6000 images. The dataset contains 50000 training images and 10000 test images \cite{recht2018cifar,krizhevsky2009learning}. We evaluated the diversity and quality of the output images generated by the model using two indicators: Inception \cite{salimans2016improved} (higher is better) and FID scores, which allowed us to effectively assess the model's performance. Additionally, we assessed the model's speed by considering the model parameters and training step, to identify a suitable mode of force field construction.

$\textbf{Baselines}$. We conducted a comparison study of generative models, namely PFGM, GAN \cite{brock2018large,gulrajani2017improved}, and normalized flow \cite{papamakarios2021normalizing,kobyzev2020normalizing}, under various force field patterns. Our model selection protocol, in line with, involves selecting the checkpoint with the lowest FID score during training at every 50k iterations.

$\textbf{Optimization}$. In order to draw conclusions with higher confidence, we modified the training samples by randomly flipping and adding noise to construct a mini-batch of size 32. The Adam \cite{kingma2014adam} optimizer is used with an initial learning rate of $10^{-4}$ and the learning rate is multiplied by a factor of 0.8 every 3 epochs.
\begin{figure}[htbp]
\begin{center}
    \centering
   
        \includegraphics[width=3in]{p5.png}
        
    
\end{center}
    \caption{Evolution results of trajectories in several ways on CIFAR-10 datasets.}
    \label{fig.3}
    
\end{figure}

$\textbf{Results}$ For quantitative evaluation on CIFAR-10, we report the
Inception (higher is better) and FID scores (lower is better) in Table 1 second and third column. For speed evaluation on CIFAR-10, we report the number of function evaluation (NFE) \cite{xu2022poisson} in Table \ref{tab1} forth column.

$\textbf{Our main findings are:}$ (1) The results show that in terms of generation quality, diffusion model with multi-sample linear superposition achieves the best Inception scores and FID scores among the models, which respectively reached 12.11 and 2.33 within 800 steps, and the effect is significantly better than the same type of curve trajectory. (2) The diffusion model utilizing a one-sample straight line has exhibited a notable degree of proficiency within a limited number of iterations. Additionally, the model's reversibility and insignificant perturbation of the data point distribution engenders a degree of stability throughout the training process.



\subsection{Result Analysis}

\textbf{Curved Trajectory vs Multi-Sample Straight Trajectory}. Based on our experimental findings, it can be concluded that the multi-sample line model outperforms the curve fitting method in terms of generation quality. We now aim to provide an explanation for this observation. Following the derivation of Equation \eqref{eq5}, we have a correlation equation for the original distribution and the sampling result $\boldsymbol{x}_t = (\boldsymbol{x}_T - \boldsymbol{x}_0)t + \boldsymbol{x}_0$. One can employ the superposition of Taylor expansions to approximate the curve. This method can be applied to fit the sample distribution $\boldsymbol{x}_T = \frac{(t-1)(\boldsymbol{x}_t - \boldsymbol{x}_0)}{t^m}$ of the curve type. Thus, it can be written as $\boldsymbol{x}_t = \sum_{i=1}^{T}(\boldsymbol{x}_i - \boldsymbol{x}_{i-1}){t^i}$. Using the method of least squares, we can fit the approximate curve obtained by the superposition of Taylor expansions. Therefore, the quality of the final result obtained by the multi-sample straight line trajectory method is superior to that of the curve. Additionally, Figure 3 illustrates the image related to the trajectory movement.

\textbf{Some problems with Gaussian distribution}. In previous works, Gaussian distribution has been predominantly used as the basis for modeling \cite{song2020score,xu2022poisson}. However, our experiments have revealed certain limitations associated with this approach. Firstly, to obtain the equation about $\boldsymbol{x}_T$ through solving differential equations, it is necessary for the variance $\sigma_t$ of the Gaussian distribution to satisfy a monotonically increasing function. Secondly, our sampling design imposes strict requirements and is not very compatible with the model design for the Gaussian distribution \cite{xu2022poisson}.


\subsection{Further Mining Analysis} 
We conducted a special case analysis for multi-sample straight-line trajectories and discovered interesting properties during training. Similar to the mode collapse in the PFGM proposed in, we found that when the number of straight-line fittings reaches a certain level, the overall quality of the samples generated by the model decreases, and mode collapse occurs. As shown in Figure \ref{fig.4}, it can be seen that as the number of fitted curves increases, the quality of the generated image also increases, and the evaluation steps also increase. However, when the number of curves exceeds about 12, the model appears mode collapse phenomenon. To interpret this situation, we can leverage the Green's function method. Specifically, we assume that the trajectory between two points $\boldsymbol{x}_0$ and $\boldsymbol{x}_1$ is a straight line, which is only generated for a single point and can be considered as the Green's function solution. However, when the force field associated with a general distribution is superimposed on the Green's function, the resulting trajectory is no longer a straight line. This explains why the curve can be approximated by using a multi-sample straight line. When applying the Green's function method to solve differential equations with high degree, it is possible that the remaining term in the Taylor expansion has a degree that is comparable to the largest term, which can lead to an overall bias in the final function due to the error in the remaining term.
\begin{figure}[htbp]
\begin{center}
    \centering
   
        \includegraphics[width=3in]{p6.png}
        
    
\end{center}
    \caption{Trajectory overlay number and change of FID.}
    \label{fig.4}
    
\end{figure}

We have devised a comprehensive metric that takes into account the number of function evaluations and the quality of the samples generated using the FID and Inception metrics. To accomplish this, we first normalize and scale the three metrics, using the z-score method for normalization. The weights of the three metrics are determined based on the distribution of the column, with greater weight assigned to the metric that exhibits greater difference. This approach enables us to analyze the level of mode collapse for different levels of overlapping.
\begin{table}
\begin{center}
\begin{tabular}{ccccc}
\hline
\textbf{ON} & \textbf{Inception↑} & \textbf{FID↓} & \textbf{NFE↓} & \textbf{Index↑} \\ \hline
1           & 10.27               & 10.22         & 46            & 0.8965          \\
2           & 10.76               & 9.51          & 89            & 0.8995          \\
3           & 11.12               & 7.27          & 124           & 0.9118          \\
4           & 11.25               & 6.14          & 189           & \textbf{0.9143}          \\
5           & 11.54               & 5.12          & 322           & 0.9115          \\
6           & 12.01               & 4.44          & 632           & 0.8941          \\
7           & 11.79               & 3.75          & 689           & 0.8931          \\
8           & 12.07               & 3.42          & 744           & 0.8921          \\
9           & 12.11               & 2.91          & 782           & 0.8924          \\
10          & 12.11               & 2.33          & 800           & 0.8946          \\ \hline\\
\end{tabular}
\end{center}
\caption{Correlation metrics for different amounts of overlap. Among them, ON indicates the number of overlaps, and index is a comprehensive indicator of the three indicators after calculating the weight.}
\label{t2}
\end{table}
Table \ref{t2} presents the results of the comprehensive evaluation of the models in terms of image generation quality and speed. The evaluation considers three aspects of indicators and the final index is calculated by normalizing and combining them using the z-score method. The weights of the indicators are determined based on the properties of the distribution column, giving greater weight to indicators with larger differences. The table shows that the best overall performance is achieved when the number of overlaps is about 4 times. Beyond this point, the overall evaluation drops, possibly due to excessively large training steps. Therefore, when configuring the model, the number of overlaps should be considered from a comprehensive perspective and not set too high. However, to improve performance before mode collapse, the number of overlaps can be increased as much as possible. It is important to note that the assumptions made for the other hyperparameters in this experiment might differ from the settings of your model. Therefore, it is recommended to follow the actual situation as closely as possible.


\subsection{Experimental Conclusion}

Through the aforementioned experiments and analysis, we have systematically investigated the appropriate force field for building a generative model from an experimental standpoint. We have summarized some principles, such as higher complexity implying more assumptions, which can be difficult to test for compatibility with the target data. Furthermore, more complex trajectories can present higher analytical and experimental difficulties. Multi-point superposition generation can fit complex curves when the basic trajectory is a straight line. However, when the number of fittings is too high, the model may become highly unstable, leading to mode collapse. Therefore, it is crucial to identify the maximum number of generated masses before mode collapse or to aim for a model that can be trained in as few steps as possible, to achieve a higher generation quality.


\section{Conclusion}

This paper introduces a novel approach to generate force fields based on a theoretical mathematical proof and derivation. Specifically, we leverage the diffusion model to solve the ODE during training and propose three representative model paradigms. Through experimental comparisons with existing methods, we identify force field patterns that exhibit superior generation speed and quality. Additionally, we provide an intuitive analysis and derivation of the results obtained. Moreover, we analyze the essential principles underlying the construction of such force fields, thereby offering valuable insights for future research in this area.



\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{ahuja2021properties}
Kartik Ahuja, Jason Hartford, and Yoshua Bengio.
\newblock Properties from mechanisms: an equivariance perspective on
  identifiable representation learning.
\newblock {\em arXiv preprint arXiv:2110.15796}, 2021.

\bibitem{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock {\em arXiv preprint arXiv:1809.11096}, 2018.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{diederik2014auto}
P~Kingma Diederik, Max Welling, et~al.
\newblock Auto-encoding variational bayes.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, volume~1, 2014.

\bibitem{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{dormand1980family}
John~R Dormand and Peter~J Prince.
\newblock A family of embedded runge-kutta formulae.
\newblock {\em Journal of computational and applied mathematics}, 6(1):19--26,
  1980.

\bibitem{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of wasserstein gans.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{ho2019flow++}
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In {\em International Conference on Machine Learning}, pages
  2722--2730. PMLR, 2019.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{jayaram2021parallel}
Vivek Jayaram and John Thickstun.
\newblock Parallel and flexible sampling from autoregressive models via
  langevin dynamics.
\newblock In {\em International Conference on Machine Learning}, pages
  4807--4818. PMLR, 2021.

\bibitem{karras2020training}
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and
  Timo Aila.
\newblock Training generative adversarial networks with limited data.
\newblock {\em Advances in neural information processing systems},
  33:12104--12114, 2020.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{kingma2014semi}
Durk~P Kingma, Shakir Mohamed, Danilo Jimenez~Rezende, and Max Welling.
\newblock Semi-supervised learning with deep generative models.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{kobyzev2020normalizing}
Ivan Kobyzev, Simon~JD Prince, and Marcus~A Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  43(11):3964--3979, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em ~}, 2009.

\bibitem{lipman2022flow}
Yaron Lipman, Ricky~TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
\newblock Flow matching for generative modeling.
\newblock {\em arXiv preprint arXiv:2210.02747}, 2022.

\bibitem{liu2021density}
Qiao Liu, Jiaze Xu, Rui Jiang, and Wing~Hung Wong.
\newblock Density estimation using deep generative neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  118(15):e2101344118, 2021.

\bibitem{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International Conference on Machine Learning}, pages
  8162--8171. PMLR, 2021.

\bibitem{papamakarios2021normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em The Journal of Machine Learning Research}, 22(1):2617--2680,
  2021.

\bibitem{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock {\em arXiv preprint arXiv:1806.00451}, 2018.

\bibitem{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International conference on machine learning}, pages
  1530--1538. PMLR, 2015.

\bibitem{risken1984solutions}
Hannes Risken and Hannes Risken.
\newblock Solutions of the kramers equation.
\newblock {\em The Fokker-Planck Equation: Methods of Solution and
  Applications}, pages 229--275, 1984.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{unterthiner2017coulomb}
Thomas Unterthiner, Bernhard Nessler, Calvin Seward, G{\"u}nter Klambauer,
  Martin Heusel, Hubert Ramsauer, and Sepp Hochreiter.
\newblock Coulomb gans: Provably optimal nash equilibria via potential fields.
\newblock {\em arXiv preprint arXiv:1708.08819}, 2017.

\bibitem{vahdat2021score}
Arash Vahdat, Karsten Kreis, and Jan Kautz.
\newblock Score-based generative modeling in latent space. 2021.
\newblock {\em arXiv preprint arXiv:2106.05931}, 2021.

\bibitem{van1976stochastic}
Nicolaas~G Van~Kampen.
\newblock Stochastic differential equations.
\newblock {\em Physics reports}, 24(3):171--228, 1976.

\bibitem{xu2022poisson}
Yilun Xu, Ziming Liu, Max Tegmark, and Tommi Jaakkola.
\newblock Poisson flow generative models.
\newblock {\em arXiv preprint arXiv:2209.11178}, 2022.

\bibitem{yang2022diffusion}
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,
  Yingxia Shao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang.
\newblock Diffusion models: A comprehensive survey of methods and applications.
\newblock {\em arXiv preprint arXiv:2209.00796}, 2022.

\end{thebibliography}
\clearpage
\section*{APPENDIX}
\section{Proofs}
%-------------------------------------------------------------------------
\subsection{Derivation of Objective Function}

In Equation (6), we have such a conclusion: \begin{equation}\mathbb{E}_{\boldsymbol{x}_0,\boldsymbol{x}_t \sim d_t(\boldsymbol{x}_t|\boldsymbol{x}_0)d 
_0(\boldsymbol{x}_0)}\left[\left\Vert \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) - \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2\right] 
\nonumber\end{equation}

The source of the derivation of this equation is firstly the solution to the $\textbf{H(x)}$ introduced above. For the transfer equation, $\frac{\partial}{\partial t} d_t(\boldsymbol{x}_t) = - \nabla_{\boldsymbol{x}_t}\cdot(\boldsymbol{F}_t(\boldsymbol{x}_t) d_t(\boldsymbol{x}_t))$ can be rewritten as

\begin{equation}\frac{\partial}{\partial t} d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) + \nabla_{\boldsymbol{x}_t}d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) \cdot \boldsymbol{f}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)+d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) \nabla_{\boldsymbol{x}_t}\cdot \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = 0\nonumber\end{equation}

At this point, the problem is transformed into solving a class of partial differential equations. For a particular trajectory of the data, $\boldsymbol{F}_t$ can be transformed into a relative quantity of $\boldsymbol{x}_t$. Therefore, the overall expression can be simplified as 
\begin{equation}\frac{d}{dt}d_t(\boldsymbol{x}_t|\boldsymbol{x}_0)+ d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) \nabla_{\boldsymbol{x}_t}\cdot \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = 0\nonumber\end{equation}

For such a class of equations that have been transformed into ordinary differential equations, they can be directly brought into the formula to solve:
\begin{equation}d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = C \exp\left(\int_t^T \nabla_{\boldsymbol{x}_s}\cdot \boldsymbol{F}_s(\boldsymbol{x}_s|\boldsymbol{x}_0) ds\right)\nonumber\end{equation}

\begin{equation}d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = d_T(\boldsymbol{x}_T) \exp\left(\int_t^T \nabla_{\boldsymbol{x}_s}\cdot \boldsymbol{F}_s(\boldsymbol{x}_s|\boldsymbol{x}_0) ds\right)\nonumber\end{equation}
where $\boldsymbol{x}_T$ is the distribution sampled by the last sampling mode. Afterwards, we bring such a trajectory mode into it, and then we can simplify and get the related expression to find that: $\boldsymbol{H}(t, 0; \boldsymbol{x}_t, \boldsymbol{x}_0)=p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$. Guided by the $H$ function, we can find out by adding it to the constraints of the differential equation
\begin{equation}\begin{aligned}
\boldsymbol{v}_1(t, \boldsymbol{x}_t) =&\, \int d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d_0(\boldsymbol{x}_0) d\boldsymbol {x}_0 = d_t(\boldsymbol{x}_t)\\
\boldsymbol{v}_{t}(t, \boldsymbol{x}_t) =&\, \int \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d_0(\boldsymbol{x}_0) d\boldsymbol{x}_0
\end{aligned}\nonumber\end{equation}

Accordingly, using the definition of the expectation formula, we can derive the objective function we need\begin{equation}\begin{aligned} 
\boldsymbol{F}_t(\boldsymbol{x}_t)=&\,\frac{\boldsymbol{v}_{t}(t, \boldsymbol{x}_t)}{\boldsymbol{v}_1(t, \boldsymbol{x}_t)} \
=\frac{1}{d_t(\boldsymbol{x}_t)}\int \boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0) {d_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d_0(\boldsymbol{x}_0)} d\boldsymbol{x}_0 \\ 
=&\,\int d_t(\boldsymbol{x}_0|\boldsymbol{x}_t) \boldsymbol{f}_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d\boldsymbol{x}_0 \
=\mathbb{E}_{\boldsymbol{x}_0\sim d_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\boldsymbol{F}_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right] 
\end{aligned}\nonumber\end{equation}

\subsection{Details of the Sampling Method}
 The Poisson flow generation model gives a more effective sampling mode in Equation (11). In this section we will derivate this whole process. According to $(\boldsymbol{\varepsilon}_{\boldsymbol{x}},\varepsilon_t)\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I}_{(d+1)\times(d+1)})$, we can get\begin{equation}p(\Vert\boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert) \propto \Vert\boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert^{d-1} e^{-\Vert\boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert^2/2}, \quad p(|\varepsilon_t|) \propto e^{-|\varepsilon_t|^2/2}\nonumber\end{equation}
 let $r = \frac{\Vert \boldsymbol{\varepsilon}_{\boldsymbol{x}}\Vert}{|\varepsilon_t|}$, it can release 
 \begin{equation}\begin{aligned} 
p(r)dr =&\, \mathbb{E}_{|\varepsilon_t|\sim p(|\varepsilon_t|)}\big[p(\Vert\boldsymbol{\varepsilon}_{\boldsymbol{x}}{=r|\varepsilon_t|})d({r|\varepsilon_t|})\big] 
\propto  \mathbb{E}_{|\varepsilon_t|\sim p(|\varepsilon_t|)}\big[r^{d-1}|\varepsilon_t|^d e^{-r^2|\varepsilon_t|^2/2} dr\big]
\propto \\ \int_0^{\infty} r^{d-1}|\varepsilon_t|^d e^{-r^2|\varepsilon_t|^2/2} e^{-|\varepsilon_t|^2/2} d|\varepsilon_t| dr 
=&\, \int_0^{\infty} r^{d-1}|\varepsilon_t|^d e^{-(r^2+1)|\varepsilon_t|^2/2} d|\varepsilon_t| dr \\ 
=&\,  \frac{r^{d-1}}{(1+r^2)^{(d+1)/2}} \int_0^{\infty} k^d e^{-k^2/2} ds dr \quad\left(\text{let}~k = |\varepsilon_t|\sqrt{r^2+1}\right) 
\end{aligned}\nonumber\end{equation}
  Finally, the conclusion of Equation (12) is obvious.
\section{Results of Experiments}
\begin{figure}[htbp]
\begin{center}
    \centering
        \includegraphics[width=7in]{figureforsupport.png}
\end{center}
    \caption{Generated results in different situations.}
    \label{fig.A11}
    
\end{figure}
The outcomes of several techniques alluded to in the primary discourse are depicted in Figure 1, revealing that the manifestation of mode collapse evinced at 12 trajectories diverges from other instances.
\maxdeadcycles=1000
\begin{figure}[htbp]
\begin{center}
    \centering
        \includegraphics[width=4in]{sp2.png}
\end{center}
    \caption{Evolution of results on ImageNet-64 (using the best performing force field model).}
    \label{fig.B11}
\clearpage   
\end{figure}
\begin{figure}[htbp]
\begin{center}
    \centering
        \includegraphics[width=4in]{sp4.png}
\end{center}
    \caption{LSUN bedroom 256×256 samples using PFGM.}
    \label{fig.D11}
    
\end{figure}


\begin{figure}[htbp]
\begin{center}
    \centering
        \includegraphics[width=4in]{sp3.png}
\end{center}
    \caption{LSUN bedroom 256×256 samples using best performing force field model.}
    \label{fig.C11}
    
\end{figure}
\end{document}
