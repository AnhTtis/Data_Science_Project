\section{Conclusion}
In conclusion, we propose an efficient vision tuning method named \method{} that addresses the heavy computational costs. By drawing inspiration from deep implicit models with stable memory costs, \method{} only requires two equilibrium implicit layers in two ends of the pre-trained main backbone with parameters frozen. Additionally, pruning the parameters in these two layers according to the lottery hypothesis reduces training parameters. \method{} can obtain higher performance with a smaller parameter size than the state-of-the-art baseline VPT, especially under challenging scenes. Our experiments demonstrate that \method{} has a good generalization performance, making it an easy way to boost applications in the future. Overall, \method{} provides an economical solution for vision tasks and is promising for a wide range of datasets.