
\section{Methodology}

\subsection{Overall Architecture}
In vision prompt tuning, the goal is to adapt a pre-trained vision model to downstream tasks without modifying its weights and achieve comparable results with the commonly used fine-tuning method. 
Mathematically, given a pre-trained large-scale vision model $\mathcal{G}$ with parameters $\Theta$, it can be decomposed into two parts, the backbone $\mathcal{F}$ with frozen parameters $\Theta_f$ and the head layer $\mathcal{H}$ with trainable parameters $\Theta_h$. Thus, the input image $x_i \in \mathbb{R}^{H \times W \times 3}$ from the down-stream dataset $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$.
Our method, named \method{}, accomplishes the goal with an extremely lightweight prompt block $\mathcal{P}$ with a few trainable parameters $\Theta_p$, which utilizes the implicit equilibrium layer for activation. We only insert two prompt blocks in front of the input and head layers, respectively. Suppose that the output of the backbone is $z_i = \mathcal{F}(x_i;\Theta_f)$, two prompt-based blending representations can be written as:
\begin{equation}
\label{eq:p1}
    \tilde{x}_i = \alpha_1 x_i + \beta_{1} \mathcal{P}_{1}(x_i;\Theta_{p1}),
\end{equation}
\begin{equation}
\label{eq:p2}
\tilde{z}_i = \alpha_{2} \mathcal{F}(\tilde{x}_i;\Theta_f) + \beta_{2} MLP\left(\mathcal{P}_{2}(z_i;\Theta_{p2})\right).
\end{equation}
Here, $MLP$ represents the fully-connected layer for representation projection. $\alpha_i$ and $\beta_i$ represent balance coefficients determined based on their importance in the attention mechanism~\cite{mnih2014recurrent}. To generate these balance coefficients, we initialize two adjustable parameters, $g_{{\alpha}_i}$ and $g_{{\beta}_i}$, subject to the following constraints: ${\alpha}_i + {\beta}_i = 1$ and $0 < {\alpha}_i, {\beta}_i < 1$.
\begin{equation}
    \alpha_i = \frac{e^{g_{\alpha_i}}}{e^{g_{\alpha_i}}+e^{g_{\beta_i}}},  \beta_i = \frac{e^{g_{\beta_i}}}{e^{g_{\alpha_i}}+e^{g_{\beta_i}}}
\end{equation}
In this way, the task-specific knowledge of the downstream data is distilled and effectively incorporated into the trainable parameter set: $\Theta_{t} = Concat(\Theta_{p1} || \Theta_{p2} || \Theta_{h})$, activating the pre-trained model. 
The prompt-based blending representation promotes a balance between the original representations and the learned prompt, resulting in adaptive control. Our prompt blocks perform a generic architectural modification that enables the pre-trained vision model to be adapted to a downstream task with only a few additional parameters $\Theta_{t}$.

Intuitively, \method{} is illustrated in Figure \ref{fig:framework}. The whole process can be listed as follows: \textbf{i)} Blend the downstream input image with the vision prompt using an adaptive coefficient by feeding the downstream input image into the equilibrium layer. \textbf{ii)} Feed the combined image to the frozen pre-trained model to get the feature representations. \textbf{iii)} Input the downstream image into the equilibrium layer to produce vision prompts, and map the prompts to the representation size to create a combination. \textbf{iv)} Add a fully-connected layer to the top layers of the pre-trained model for the final prediction.


\subsection{Implicit Prompt Design}
The Deep Equilibrium (DEQ) Model, as described in \cite{bai2019deep}, employs a single layer that finds the fixed point of an iterative procedure. This layer, capable of expressing the entire deep network as an equilibrium computation, is just as powerful as multiple stacked explicit layers. Our proposed method, \method{}, leverages this capability by using a single DEQ layer to adapt a pre-trained vision model to downstream tasks through implicit layer training. The downstream inputs are equipped with a single implicit layer, implemented as a ResNet layer~\cite{he2016deep}, which is trained to learn task-specific prompts while the pre-trained model remains frozen.
Drawing inspiration from previous studies \cite{islam2021broad,lin2017refinenet,yosinski2014transferable}, we aim to improve the feature representation by utilizing high-frequency and low-frequency information as the vision prompt. To attain this goal while reducing the parameter storage burden, we propose using two-level prompt blocks, located prior to the input layer and the head layer. Our design of this lightweight architecture is supported by demonstrations of its convergence ability in Theoretical Analysis.

Unlike a conventional network where the output is the activation from the $L$ th layer, the output of an equilibrium layer is the equilibrium point itself. Therefore, the forward evaluation could be any procedure that solves for this equilibrium point. Conventional deep neural networks, if they converge to equilibrium, can be considered a form of fixed-point iterations for the forward process:
\begin{equation}
    z^{*} = \mathcal{P}(z^{*}, x; \Theta_p)
\end{equation}
Our goal will be to compute the vector-Jacobian product $\frac{\partial z^{*}(\cdot)}{\partial(\cdot)}^T y$ for some vector $y$, where $(\cdot)$ here is a stand-in for any quantity we want to differentiate the fixed point with respect to (i.e., the input $x$, or any parameters of the function $\mathcal{P}$, both of which of course will affect the final fixed point $z^*$). Since this vector-Jacobian product is the key aspect to integrating these DEQ layers within backpropagation, such a routine allows us to integrate the DEQ layer within standard automatic differentiation tools.

The derivation of the vector-Jacobian product largely mirrors that in previous sections, but we include the full derivation again here for completeness. Differentiating both sides of the fixed point solution, we have:
\begin{equation}
    \frac{\partial z^{*}(\cdot)}{\partial(\cdot)} = \left(I - \frac{\partial \mathcal{P}(z^{*},x)}{\partial(z^*)}\right)^{-1} \frac{\partial \mathcal{P}(z^{*},x)}{\partial(\cdot)}
\end{equation}

In order to calculate the vector-Jacobian product, we need the following information:
\begin{equation}
    \left(\frac{\partial z^{*}(\cdot)}{\partial(\cdot)}\right)^T y = \left(\frac{\partial \mathcal{P}(z^{*},x)}{\partial(\cdot)}\right)^T \left(I - \frac{\partial \mathcal{P}(z^{*},x)}{\partial(z^*)}\right)^{-T}y 
\end{equation}

The critical term of interest here is the solution in a linear system, and we can utilize the proxy $o = (I - \frac{\partial \mathcal{P}(z^{*},x)}{\partial(z^*)})^{-T}y$ to solve the fixed point equation and compute the final Jacobian vector product.





\subsection{Robust Training}

We utilize the robust training mechanism for trainable parameters $\Theta_t$ motivated by \cite{frankle2018lottery} to overcome the instability during prompt tuning. The Lottery Ticket Hypothesis in it posits that only a subset of a model's parameters is crucial for achieving good generalization, the rest are likely to overfit. To address this, we employ a criterion to separate crucial and non-crucial parameters and optimize them differently. Crucial parameters are updated with stochastic gradient descent, while non-crucial parameters are constrained to reduce their ability to overfit.

Recent pruning methods~\cite{frankle2018lottery,yeom2021pruning,xia2020robust} suggest that crucial parameters should have substantial magnitudes, as they play a key role in network propagation. Furthermore, early optimization shows that parameters with large gradients tend to contribute to generalized patterns~\cite{molchanov2019importance}, which are crucial for learning from clean samples. Therefore, the parameters' values and gradients should be considered when determining their importance. To capture this, we use the product of their values and gradients as the criterion for determining criticality. In mathematical terms, the criticality of parameter $\theta_t$ from $ \Theta_t = \{\theta_t^i \}_{i=1}^M$ is represented as:

\begin{equation}
	z(\theta_t) = \left|\frac{\partial \mathcal{L}}{\partial{\theta_t}}\cdot \theta_t \right|,
\end{equation}
where $\mathcal{L}$ represents the loss function.

The equation shows that when the gradient or value of a parameter is close to zero, its criticality is low, making it a non-crucial parameter that is prone to overfit. On the other hand, when the value of $z(\theta_t)$ is immense, $\theta_t$ is considered a crucial parameter for learning basic and generalized patterns. To control the number of crucial parameters, we introduce a threshold $\tau $, such that crucial parameters are selected and represented as:

\begin{equation}\label{eq:phi_c}
	\Theta_t^c = \{\theta_t | z(\theta_t) \geq \tau   \},
\end{equation}
\begin{equation}\label{eq:phi_n}
	\Theta_t^n = \{\theta_t | z(\theta_t) < \tau    \}.
\end{equation}


Unlike pruning methods, we do not eliminate the non-crucial parameters $\Theta_t^n$. Instead, we adopt a distinct optimization strategy. Here, the crucial parameters are updated in the usual way, while the non-crucial parameters are restricted to converge to zero for better generalization. Mathematically, the update rule for $\theta_t \in \Theta_t^c$ is represented as:

\begin{equation}\label{eq:13}
	\theta_t \leftarrow \theta_t - \eta \frac{\partial \mathcal{L}}{\partial{\theta_t} },
\end{equation}

The symbol $\eta$ denotes the learning rate in the above equation. For the non-crucial parameters, we shrink them utilizing strict regularization instead of minimizing the loss. In other words, the update rule for $\theta_t \in \Theta_t^n$ is:

\begin{equation}\label{eq:14}
	\theta_t \leftarrow \theta_t - \eta sign(\theta_t).
\end{equation}

\subsection{Optimization}

For our training process, we keep the pre-trained parameters $\Theta_f$ intact and only modify a limited set of parameters $\Theta_t$. This selective update of parameters makes our \method{} modular and efficient - it allows us to utilize an existing pre-trained vision model without having to modify or re-train it. Instead, we add a small number of additional parameters specific to each task,  which can be formulated as:
\begin{equation}
    \theta_t^* = \mathop{argmin}\limits_{\theta_t} \frac{1}{|\mathcal{D}|} \sum_{i=1}^N \ell \left( \mathcal{H}(\tilde{z}_i), y_i \right)
\end{equation}
With the pre-trained model frozen, we minimize the prediction error using cross-entropy (CE) loss. The ability of our \method{} to adapt a pre-trained vision model to a wide range of tasks while maintaining a high level of accuracy makes it a desirable solution for deployment in cloud services. The potential benefits of reduced computational and storage overhead, as well as the ability to offer real-time adaptation to new tasks, make our method an ideal choice for cloud service providers seeking to improve their offerings.


\subsection{Theoretical Analysis}
\label{sec:theory}

\noindent\textbf{Theoretical setup.} Our proposed \method{} aims to provide prompts to the vision input $x \in \mathbb{R}^d$ and the representation $z \in \mathbb{R}^h$ derived by the backbone. 
The whole network maps $x$ to the label $y \in \mathbb{R}$ with the loss $l(y,\hat{y})$ like cross-entropy, etc. 
We consider the network to be $f_{\theta}(x) = v^{\mathrm{T}}\sigma(Wx)$, where $v \in \mathbb{R}^k$, $W \in \mathbb{R}^{k \times d}$, and $\sigma$ is an element-wise activation function like ReLU. 
Assume that the random variables $x, y \sim P$ and the population loss can be denoted as 
$\mathcal{L}(\theta) = \mathbb{E}\left[l(f_{\theta}(x),y)\right] $.

Note that we have only two prompt blocks in different positions and architectures. We show that we can directly add the vision prompts to the first layer of the pre-trained model, while it cannot be implemented on the last layer. We should utilize another MLP block to ensure the optimal solution. Here we assume the $\sigma$ to be ReLU: $\sigma(x_i) = max(x_i,0)$. Given that the model is pre-trained with parameters $\hat{\theta} :(\hat{v}_{pre}, \hat{W}_{pre})$ which reach the optimal solution $\mathcal{L}(\hat{v},\hat{W}) = 0$. With the single-layer DEQ as the vision prompts network, we can derive the vision prompts with the suppose that: $x_{pro} = A x$ and $z_{pro} = B z$ for some invertible matrix $A, B$, where the corresponding label is unchanged: $y_{pro} = y$.



\begin{proposition}
   There exists the vision prompt $x_{pro} = A x$ for invertible $A$ and $y_{pro} = y$ that can minimize the population loss: $min_{W}\mathcal{L}(\hat{v}, W) = 0$. However, the vision prompt $z_{pro} = B z$ may not be sufficient: there exists such $B$ such that the population loss is non-zero for any choice of the parameter $v$: $min_{v}\mathcal{L}(v, \hat{W}) > 0$.
\end{proposition}
\begin{proof}
    Let $\hat{B}, \hat{v}$ can reach the optimal solutions so that $y = \hat{v}\sigma(\hat{W}x)$ for all $x,y$. Let $W = \hat{W} A^{-1}$, we have for all $x_{pro}$
    \begin{equation}
        \hat{v}\sigma(W x_{pro}) =  \hat{v}\sigma(\hat{W} A^{-1} A x) = \hat{v}\sigma(\hat{W} x) = y
    \end{equation}
    Therefore, the parameters $\hat{v},W$ achieves $\mathcal{L}(\hat{v}, W) = 0$.

    Following is a counterexample showing that last-layer prompts are impossible. Since $\sigma$ is the element-wise ReLU function, $\hat{W}z$ has only positive entries for all $z$. Let $B = -I$, which is an invertible diagonal matrix full of -1. Then for any $v$, we have $v \sigma(\hat{W}z_{pro}) = v\sigma(-\hat{W}x) = 0$, so the expected loss is positive. Therefore, $min_{v}\mathcal{L}(v, \hat{W}) > 0$.
\end{proof}


\begin{table*}[t]
\centering
\begin{tabular}{llcccccccc}
\toprule[1.5pt]
\multicolumn{1}{c}{Backbone} & \multicolumn{1}{l}{Method} & CIFAR10 & CIFAR100 & \multicolumn{1}{l}{ImageNet100} & Flower & Dogs & Cars & Clothing & Params \\ \midrule
\multirow{7}{*}{ResNet-50} & Retraining & 0.8281 & 0.5178 & 0.7088 & 0.8649 & 0.7942 & 0.6898 & 0.7649 & 23.529 \\
 & Head-tuning & 0.7627 & 0.4738 & 0.6167 & 0.8531 & 0.7713 & 0.6257 & 0.7324 & 0.277 \\
 & Fine-tuning & 0.8564 & 0.5271 & 0.7194 & 0.8942 & 0.8126 & 0.7548 & 0.7852 & 23.529 \\
 & Adapter & 0.8375 & 0.6021 & 0.7185 & 0.8702 & 0.8279 & 0.6816 & 0.7633 & 0.673 \\
 & Bias & 0.8319 & 0.5965 & 0.7003 & 0.8694 & \textbf{0.8316} & 0.7371 & 0.7803 & 0.494 \\
 & VPT & 0.8547 & \textbf{0.6289} & 0.7257 & 0.8876 & 0.8147 & 0.7459 & 0.7870 & 0.812 \\
 & \method{} & \textbf{0.8628} & 0.5484 & \textbf{0.7372} & \textbf{0.8976} & 0.8269 & \textbf{0.7642} & \textbf{0.7892} & 0.097 \\ \hline
\multirow{7}{*}{ResNet-101} & Retraining & 0.8357 & 0.5365 & 0.7275 & 0.8691 & 0.8015 & 0.7014 & 0.7892 & 43.713 \\
 & Head-tuning & 0.7689 & 0.4941 & 0.6287 & 0.8654 & 0.7786 & 0.6386 & 0.7641 & 0.461 \\
 & Fine-tuning & 0.8745 & 0.5487 & 0.7469 & 0.8989 & 0.8168 & 0.7715 & 0.8119 & 43.713 \\
 & Adapter & 0.8456 & 0.6233 & 0.7382 & 0.8745 & 0.8325 & 0.6895 & 0.7893 & 0.684 \\
 & Bias & 0.8517 & 0.6148 & 0.7249 & 0.8721 & \textbf{0.8421} & 0.7598 & 0.7894 & 0.517 \\
 & VPT & 0.8723 & \textbf{0.6319} & 0.7470 & 0.8898 & 0.8198 & 0.7581 & 0.8092 & 0.838 \\
 & \method{} & \textbf{0.8830} & 0.5898 & \textbf{0.7492} & \textbf{0.9024} & 0.8311 & \textbf{0.7762} & \textbf{0.8165} & 0.097 \\ \bottomrule[1.5pt]
\end{tabular}
\caption{The performance of \method{} and existing tuning baselines on six classification tasks using CNN-based pre-trained models. Params represents the maximum number of parameters that can be trained. The unit of measurement for Params is M.}
\label{tab:cnn_based}
\end{table*}


\subsection{Complexity Analysis}
We also compare our complexity with several baselines to demonstrate our superiority. For example, in the case of the ViT model, there are $N^2$ visual tokens as the input, each with a dimension of $d$. We first examine MLP-based methods, such as Adapter and Bias, which require $2d\tilde{d} \cdot L$ additional trainable parameters in $L$ layers for the projection from dimension $d$ to $\tilde{d}$. Next, for VPT, it requires $nd$ additional parameters in each layer due to the insertion of $n$ prompts, which needs $nLd$ trainable parameters in total. Our \method{} only add two prompt blocks with $m\tilde{d}$ parameters ($m \ll d$). 
In practice, our proposed \method{} has demonstrated significant advantages over Adapter and VPT, even with only slightly fewer parameters during the training stage in the below. 
This is due to the unique way in which \method{} utilizes the input data, which allows it to make more efficient use of the available parameters and achieve better results. Additionally, \method{} has a more flexible architecture that allows it to adapt to different types of input data, making it more versatile and applicable to a wide range of tasks. 

\begin{table*}[htbp]
\centering
\begin{tabular}{@{}llcccccccc@{}}
\toprule[1.5pt]
Backbone & \multicolumn{1}{l}{Method} & CIFAR10 & CIFAR100 & ImageNet100 & Flower & Dogs & Cars & Clothing & Params \\ \midrule
\multirow{7}{*}{ViT-B} & Retraining & 0.8761 & 0.5592 & 0.7277 & 0.8735 & 0.8145 & 0.7165 & 0.7959 & 85.721 \\
 & Head-tuning & 0.7914 & 0.5124 & 0.6431 & 0.8617 & 0.8019 & 0.6522 & 0.7671 & 0.187 \\
 & Fine-tuning & 0.9035 & 0.6499 & 0.7544 & 0.9003 & 0.8298 & 0.7934 & 0.8356 & 85.721 \\
 & Adapter & 0.8612 & 0.6319 & 0.7411 & 0.8777 & 0.8317 & 0.6934 & 0.8229 & 0.372 \\
 & Bias & 0.8898 & 0.6109 & 0.7326 & 0.8709 & 0.8348 & 0.7295 & 0.8210 & 0.215 \\
 & VPT & 0.9049 & \textbf{0.6689} & 0.7596 & 0.9013 & \textbf{0.8367} & 0.7682 & 0.8378 & 0.523 \\
 & \method{} & \textbf{0.9077} & 0.6541 & \textbf{0.7612} & \textbf{0.9054} & 0.8361 & \textbf{0.7991} & \textbf{0.8397} & 0.124 \\ \midrule
\multirow{7}{*}{Swin-B} & Retraining & 0.8896 & 0.5730 & 0.7316 & 0.8794 & 0.8357 & 0.7233 & 0.8112 & 86.954 \\
 & Head-tuning & 0.7991 & 0.5265 & 0.6558 & 0.8775 & 0.8150 & 0.6614 & 0.7739 & 0.295 \\
 & Fine-tuning & 0.9166 & 0.6631 & 0.7710 & 0.9056 & 0.8359 & 0.8016 & 0.8398 & 86.954 \\
 & Adapter & 0.8795 & 0.6511 & 0.7498 & 0.8812 & 0.8341 & 0.6952 & 0.8277 & 0.331 \\
 & Bias & 0.8971 & 0.6118 & 0.7401 & 0.8749 & 0.8442 & 0.7567 & 0.8261 & 0.287 \\
 & VPT & 0.9132 & \textbf{0.6816} & \textbf{0.7781} & 0.9026 & 0.8393 & 0.7982 & \textbf{0.8434} & 0.686 \\
 & \method{} & \textbf{0.9189} & 0.6705 & 0.7769 & \textbf{0.9061} & \textbf{0.8455} & \textbf{0.8027} & 0.8431 & 0.242 \\ \bottomrule[1.5pt]
\end{tabular}
\caption{The performance of \method{} and existing tuning baselines on six classification tasks using Transformer-based pre-trained models. Params represents the maximum number of parameters that can be trained. The unit of measurement for Params is M.}
\label{tab:transformer}
\end{table*}














 




