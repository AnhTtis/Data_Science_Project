\section{Introduction}

With the development of computer vision, models with more robust representations and larger sizes have been developed. Despite this, training these models with many parameters is becoming increasingly challenging.

One common approach to addressing this issue is pre-training on a large dataset, such as ImageNet~\cite{deng2009imagenet}, for general vision tasks and then fine-tuning the model on downstream tasks to improve performance. While this method has been widely used, several drawbacks should be considered. Firstly, fine-tuning requires a large amount of computational resources, especially for large models such as ViT-B~\cite{dosovitskiy2020image} (85.84M parameters) and Swin-B~\cite{liu2021swin} (86.87M parameters).  
Secondly, the model may become overfitted to the small target dataset and cannot be used for other tasks after fine-tuning. This phenomenon means separate sets of model parameters are needed for each task, leading to a high storage requirement.

In recent years, prompt-based learning has generated considerable interest in the natural language processing (NLP) community because of its fantastic performance on various downstream problems~\cite{liu2021pre,li2021prefix,lester2021power}. 
Prompt tuning aims to design a trainable lightweight block as a supplementary input, which can guide or direct the generation of powerful vision representations to achieve desirable performances, rather than fine-tuning pre-trained models to adapt to downstream tasks. 
% following this strat


\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{image/intro.png}
    \caption{Demonstration of the implicit vision prompt layer. The left part shows the traditional construction of the prompt block by stacking MLPs. The right is \method{} with the implicit equilibrium layer and robust training for the prompt block.  }
    \label{fig:intro}
\end{figure}



%% 现有的vision prompt的缺陷
To design a prompt framework that combines both a lightweight architecture and strong representation capability, we conducted a comprehensive study and analysis of the limitations of current vision prompt tuning methods.
\textbf{First}, existing approaches insert trainable networks as prompt blocks between each layer of the network \cite{jia2022visual}, assuming that the feature representations from different levels contribute to the network's generalization performance, especially for low- and mid-level representations. This, however, goes against the lightweight design philosophy of prompt tuning. The architecture design is also complex and heavily reliant on tuning skills, making applying to various vision backbone models with different architectures challenging.
\textbf{Second}, finding the right prompt is a challenging task that often takes a significant amount of time. Even small changes in the activation input can significantly impact performance. This can be attributed to the depth of big model architectures, making the trainable parameters of shallow network layers more challenging to train and converge.

%% 提出我们的方法
Based on the challenges above, we naturally raise a question, \textit{\textbf{can we design a single-layer network as the prompt block with favorable convergence to iterate continuously?}} We hope it can achieve the effect of multi-layer network training and thus significantly reduce the training parameters. 
Therefore, we propose the imp\underline{L}icit v\underline{I}sion pr\underline{O}mpt tu\underline{N}ing (\textbf{\method{}}), which is motivated by deep implicit models with stable low memory costs for various complex tasks. In particular, we merely insert two equilibrium implicit layers in two ends of the pre-trained backbone with parameters frozen. \method{} enables tuning various vision models, including convolutional neural networks (CNNs) and vision transformers.  

Specifically, \method{} constructs a lightweight prompt framework to generate task-specific discriminative prompts for each downstream input image. \method{} can generate a compact and robust downstream model that adapts tuning demands across a wide range of vision tasks while only training lightweight additional parameters per downstream task, which is implemented by blending the input and the representations with the learned prompts. 
Besides, since the hyper-parameters, like the learning rate, can severely affect the robustness of training the vision prompts, we prune the parameters in these two layers according to the lottery hypothesis. Only the critical parameters are kept in training to avoid over-fitting. 
More surprising is that \method{} essentially compresses the parameters of the existing vision prompt network, which allows it to be generalized to any subsequent vision prompt tuning method with trainable parameters.

Our proposed \method{} can be used to tune CNN-based and Transformer-based vision models, surpassing fine-tuning for various recognition tasks of image classification. It can perform well under a variety of practical scenarios, including generic objects, class imbalance, and few-shot learning. 
Learning vision-specific information while maintaining the pretrained knowledge, our \method{} delivers an average improvement of up to 1.3\% compared with VPT, but with much fewer trainable parameters. In summary, the main contributions of our work are three-fold:
\begin{itemize}
    \item We propose \method{}, a significantly lightweight yet effective tuning method that leverages few trainable implicit layers to adapt the pretrained model to downstream visual tasks.
    \item We construct a more robust optimization strategy by lottery hypothesis while proving the excellent convergence of our method through theoretical analysis and validation.
    \item Experimental results show that \method{} outperforms previous vision prompt tuning methods, evidencing its feasibility and usefulness for vision models, especially on few-shot and long-tail scenarios.
\end{itemize}