\section{Experiment}

\subsection{Experimental Setting}
 % To fully evaluate the performance of our \method{}, we implement detailed experiments on image classification from three categories of settings, including regular setting, long-tailed distribution setting, and out-of-distribution setting. 
 
\noindent\textbf{Dataset.} \textbf{CIFAR10}~\cite{krizhevsky2009learning} is a dataset of 60,000 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.
\textbf{CIFAR100}~\cite{krizhevsky2009learning} is a dataset of the same size as CIFAR10, but it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class.
\textbf{ImageNet100}~\cite{deng2009imagenet} is a subset of the ImageNet dataset containing 100 classes of natural images. Each class has between 500 and 1000 images for training and 50 to 100 for testing.
\textbf{Flower}~\cite{nilsback2008automated} is a dataset of images of flowers from 5 different species. It contains 4242 images, with 80-90 images per class. 
\textbf{Stanford Dogs}~\cite{khosla2011novel} is a dataset of images of 120 breeds of dogs, with a total of 20,580 images. 
\textbf{Stanford Cars}~\cite{gebru2017fine} is a dataset of cars, with a total of 16,185 images of 196 classes of cars. The dataset is organized by make, model, and year.
\textbf{Clothing}~\cite{tanaka2018joint} is a dataset containing images of various clothing types.



\noindent\textbf{Baselines.}
We compare \method{} to several commonly used protocols, including: 1) Retraining trains the entire vision model from scratch;
2) Head Fine-tuning fine-tunes the last layers of a pre-trained model while freezing the remaining layers and retraining the head classifier;
3) Fine-tuning adjusts the weights of a pre-trained model and retrains the head classifier;
4) Adapter~\cite{houlsby2019parameter} adds a new adapter structure to the transformer and updates only its parameters;
5) Bias~\cite{zaken2021bitfit} updates only the bias terms of the parameters;
6) VPT~\cite{jia2022visual} fine-tunes the model by incorporating prompts as input tokens.



\noindent\textbf{Implementation Details.}
We use the model pre-trained on ImageNet as the initialization for the following tuning for a fair comparison. Additionally, we extend our method to include CNN-based (ResNet-50, ResNet-101~\cite{he2016deep}) and Transformer-based (ViT~\cite{dosovitskiy2020image}, Swin Transformer~\cite{liu2022video}) backbones. The implementation of baselines for additional backbones involves leveraging the core idea presented in the original paper, and adapting it to suit the specific capabilities of each new backbone architecture. In experiments on the datasets above, we utilize the Adam optimizer with a momentum of 0.9, batch size of 64, and learning rate of 1e-5. The whole experiments are implemented on the NVIDIA V100 GPU with PyTorch.




\begin{table*}[htbp]
\centering
\scalebox{1}{
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{ll|cccc|cccc|c}
\toprule[1.5pt]
\multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{CIFAR10-LongTail} & \multicolumn{2}{c|}{CIFAR100-LongTail} & \multicolumn{4}{c|}{Fine-Grained Few-Shot} & \multirow{2}{*}{Params} \\  \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-10}
 &  & IR100 & IR50 & IR100 & IR50 & Pets & Food-101 & Cars & Flower &  \\ \hline
\multirow{4}{*}{ResNet-50} & Head-tuning & 0.7136 & 0.7358 & 0.3569 & 0.3891 & 0.6881 & 0.6013 & 0.3846 & 0.7218 & 0.277 \\
 & Fine-tuning & 0.7638 & 0.7962 & 0.4157 & 0.4468 & 0.7340 & 0.6531 & 0.4184 & 0.7735 & 23.529 \\
 & VPT & 0.7721 & 0.7956 & 0.4192 & 0.4510 & 0.7385 & 0.6522 & 0.4189 & 0.7790 & 0.812 \\
 & \method{} & \textbf{0.7831} & \textbf{0.8114} & \textbf{0.4328} & \textbf{0.4699} & \textbf{0.7412} & \textbf{0.6584} & \textbf{0.4203} & \textbf{0.7814} & 0.097 \\ \hline
\multirow{4}{*}{ResNet-101} & Head-tuning & 0.7268 & 0.7496 & 0.3751 & 0.4036 & 0.7027 & 0.6159 & 0.4008 & 0.7542 & 0.461 \\
 & Fine-tuning & 0.7754 & 0.8083 & 0.4309 & 0.4681 & 0.7593 & 0.6694 & 0.4325 & 0.8058 & 43.713 \\
 & VPT & 0.7881 & 0.8068 & 0.4356 & 0.4710 & 0.7603 & 0.6715 & 0.4351 & 0.8134 & 0.838 \\
 & \method{} & \textbf{0.8021} & \textbf{0.8294} & \textbf{0.4662} & \textbf{0.4982} & \textbf{0.7635} & \textbf{0.6783} & \textbf{0.4470} & \textbf{0.8175} & 0.097 \\ \hline
\multirow{4}{*}{ViT-B} & Head-tuning & 0.6849 & 0.7304 & 0.3871 & 0.4315 & 0.7245 & 0.6447 & 0.4139 & 0.7691 & 0.187 \\
 & Fine-tuning & 0.7692 & 0.7834 & 0.4778 & 0.5243 & 0.7719 & 0.6834 & 0.4498 & 0.8217 & 85.721 \\
 & VPT & 0.7631 & 0.7806 & 0.4785 & 0.5254 & 0.7731 & 0.6894 & 0.4524 & 0.8412 & 0.523 \\
 & \method{} & \textbf{0.7829} & \textbf{0.8008} & \textbf{0.5024} & \textbf{0.5517} & \textbf{0.7822} & \textbf{0.6911} & \textbf{0.4588} & \textbf{0.8507} & 0.124 \\ \hline
\multirow{4}{*}{Swin-B} & Head-tuning & 0.6947 & 0.7485 & 0.4118 & 0.4568 & 0.7324 & 0.6529 & 0.4309 & 0.7814 & 0.295 \\
 & Fine-tuning & 0.7852 & 0.7995 & 0.4826 & 0.5491 & 0.7786 & 0.6764 & 0.4625 & 0.8386 & 86.954 \\
 & VPT & 0.7725 & 0.8011 & 0.4910 & 0.5482 & 0.7797 & \textbf{0.6979} & 0.4692 & 0.8497 & 0.686 \\
 & \method{} & \textbf{0.8006} & \textbf{0.8231} & \textbf{0.5276} & \textbf{0.5708} & \textbf{0.7901} & 0.6976 & \textbf{0.4773} & \textbf{0.8526} & 0.242 \\ \bottomrule[1.5pt]
\end{tabular}}}
\caption{Results of extensive experiments on the long-tailed, and few-shot datasets to validate the ability against class imbalance and sample scarcity. IR represents the imbalance ratio and the unit of measurement for Params is M.}
\label{tab:longtail}
\end{table*}

\subsection{Performances}
\noindent\textbf{Image Classification.}
Quantitative results can be seen in Table \ref{tab:cnn_based} and Table \ref{tab:transformer}.
It can be observed that the proposed \method{} performs the best overall on all six tasks when using all four models as the backbones. The best performance for each task is highlighted in bold. For example, on CIFAR100, \method{} achieves an accuracy of 54.84 $\%$ when using ResNet-50, and 58.98 $\%$ when using ResNet-101, the highest among all the methods. For the transformer-based (i.e., ViT-B and Swin-B) methods, \method{} achieves accuracy improvement of 1.36 $\%$, 0.68 $\%$, 0.51 $\%$, 3.31 $\%$, 0.38 $\%$ and 0.19 $\%$ over other tuning methods, respectively on CIFAR10, CIFAR100, ImageNet100, Flower, Dogs, and Cars datasets. Regarding the maximum number of trainable parameters, the proposed method has the lowest value among all the methods, with only 0.097 M trainable parameters.
It is also worth noting that the performance of the other methods varies depending on the task and the backbone model used. For example, while VPT performs well on CIFAR100, it is not as effective on other tasks. On the other hand, the adapter method performs relatively well on the Flower and Dogs tasks, but not as well on the other tasks.
In summary, the proposed \method{} is the most effective tuning method across all tasks and backbone models, with the advantage of having the lowest number of trainable parameters.




\noindent\textbf{Long-tail Class Distribution.}
We perform experiments on benchmark datasets that have a long-tail class distribution, such as CIFAR10-LongTail and CIFAR100-LongTail. The results of the imbalance ratio 50 and 100 are shown in Table \ref{tab:longtail}.
Our proposed \method{} algorithm outperforms the best baseline, VPT, under all the settings. Specifically, we observe a gain of approximately 4\% in validation accuracy, while reducing the trainable parameters by 88\%. This trend holds across other settings as well.
Additionally, when compared with VPT on long-tailed CIFAR-10 with an imbalanced ratio of 100 under ViT-B, \method{} achieves superior validation accuracy using only 4.2x fewer trainable parameters. When evaluated under Swin-B, \method{} outperforms VPT on long-tailed CIFAR-10 with imbalanced ratios of 50 and 100, while also reducing the trainable parameters by 64.7\%.




\noindent\textbf{Few-shot Learning.}
We perform experiments on benchmark datasets, such as Pets~\cite{parkhi2012cats}, Food-10~\cite{bossard2014food}, Cars~\cite{gebru2017fine}, and Flower~\cite{nilsback2008automated} with eight examples per class,  which are widely used for evaluating few-shot learning algorithms.
Our experimental results, shown in Table \ref{tab:longtail}, demonstrate that \method{} achieves state-of-the-art results on average, while using the fewest trainable parameters. It is worth noting that although VPT outperforms our method on some datasets for the image classification task, we still achieve the best performance on all datasets in this scenario, which highlights our method's advantage. These observations confirm the capability and efficiency of our method in the low-data regime and further verify the effectiveness of the lightweight implicit vision prompt design.




\begin{table}[htbp]
\centering
\scalebox{1}{
\begin{tabular}{llccc}
\toprule[1.5pt]
 & Method & CIFAR10 & CIFAR100 & INet100 \\ \hline
\multirow{4}{*}{\rotatebox{90}{ResNet}} & \method{}-P1 & 0.8501 & 0.5349 & 0.7146 \\
 & \method{}-P2 & 0.8239 & 0.5136 & 0.6989 \\
 & \method{}-R & 0.8516 & 0.5356 & 0.7197 \\
 & \method{} & 0.8628 & 0.5484 & 0.7372 \\ \hline
\multirow{4}{*}{\rotatebox{90}{ViT-B}} & \method{}-P1 & 0.8876 & 0.6413 & 0.7492  \\
 & \method{}-P2 & 0.8527 & 0.6304 & 0.7186 \\
 & \method{}-R & 0.8896 & 0.6309 & 0.7437 \\
 & \method{} & 0.9077 & 0.6541 & 0.7612 \\ \bottomrule[1.5pt]
\end{tabular}}
\caption{Ablation study on important inner modules under two different backbones. INet100 represents ImageNet100.}
\label{tab:ablation}
\end{table}

\subsection{Ablation Study}
With the ImageNet pre-trained ResNet-50 and Vit-B, we perform extensive ablation studies to analyze the developed \method{} systematically. We introduce three model variants as follows:
(1) \textbf{\method{}-P1} removes the equilibrium layer before the pre-trained backbone to activate low-level features, i.e., $\mathcal{P}_{1}$ in the Eq. \ref{eq:p1}.
(2) \textbf{\method{}-P2} removes the equilibrium layer behind the pre-trained backbone to activate high-level features, i.e., $\mathcal{P}_{2}$ in the Eq. \ref{eq:p2}.
(3) \textbf{\method{}-R} removes the robust training mechanism with the standard optimization for all the parameters, i.e., optimization in Eq. \ref{eq:phi_c} and Eq. \ref{eq:phi_n}.
The results of these model variants are summarized in Table \ref{tab:ablation}. We have the following observations. First, our \method{} outperforms \method{}-$\mathcal{P}_{1}$ and \method{}-$\mathcal{P}_{2}$, which indicates that implicit vision prompt blocks work for vision semantic information activation. 
Second, \method{}-$\mathcal{P}_{2}$ obtains much worse than \method{}, showing that the high-level activation is vital for the vision prompt tuning.  
Third, the robust training mechanism allows better network optimization, as shown by \method{}-R's lower performance of 2\% compared to \method{}, validating the superiority of robust training for better optimization.




\subsection{Sensitivity Analysis}
In Figure \ref{fig:sensi}, we investigate the sensitivity of two hyper-parameters: $\tau$ in the robust training mechanism, which separates the crucial and non-crucial parameters in Eq. \ref{eq:phi_c} and Eq. \ref{eq:phi_n}. Moreover, we also investigate the number of the layer in DEQ by stacking the single layer attracted by its striking performances. 
We first vary $\tau$ in \{0.2, 0.4, 0.6, 0.8\} with other parameters fixed. 
As $\tau$ rises, the performance first increases and then decreases a little. The potential reason is that too large $\tau$ could filter the essential parameters for optimization.
We can observe that the performance of ours is not sensitive to $\tau$ in the range of $[0.4,0.6]$, and we can set it to any value in that interval. Further, we changed the number of layers from 1 to 4 with other parameters fixed. Obviously, our method can achieve a slight performance gain with the layer ranging from $1$ to $4$ but at the cost of several times the computation time. Therefore, $\tau$ and the layer number are set to $0.4$ and $1$ as default, respectively.   


\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{image/sensi.png}
    \caption{Sensitivity analysis on two hyper-parameters.}
    \label{fig:sensi}
\end{figure}



