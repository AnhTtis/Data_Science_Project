% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\input{macro.tex}
\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{spconf,amsmath,graphicx}
\usepackage{caption}
%\usepackage[tableposition=above]{caption}

\usepackage{booktabs}
\usepackage{mwe}
\usepackage{enumitem}
\usepackage{dingbat}
\usepackage{ragged2e} 
\usepackage{multirow}
\usepackage{booktabs,makecell, multirow, tabularx}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

%CMT: fancui930814

% Title.
% ------
\title{Relate Auditory Speech to EEG by Shallow-Deep \\ Attention-based Network}
\name{Fan Cui\textsuperscript{1}, Liyong Guo\textsuperscript{1}, Lang He\textsuperscript{2}, Jiyao Liu\textsuperscript{3}, ErCheng Pei\textsuperscript{2}, Yujun Wang\textsuperscript{1}, Dongmei Jiang\textsuperscript{4}}
%\address{Xiaomi Inc, Beijing, China \\   Northwestern Polytechnical University, Xi’an, China }

\address{\textsuperscript{1} Xiaomi Corp., Beijing, China \:\:  \textsuperscript{2} Xi'an University of Posts and Telecommunications, Xi’an, China  \\ \textsuperscript{3} Northwestern Polytechnical University, Xi’an, China \:\: \textsuperscript{4} Peng Cheng Laboratory, ShenZhen, China \\
\footnotesize{\texttt{\{cuifan, guoliyong, wangyujun\}@xiaomi.com, \:\:  \{langhe, ercheng.pei\}@xupt.edu.cn, jiangdm@nwpu.edu.cn}}}


% \texttt{cuifan@xiaomi.com}
%\footnotesize{\texttt{\{guoliyong, xiaoyuyang6, dpovey\}@xiaomi.com, pzelasko@meaning.team}}}
%\email{\{cuifan, guoliyong, yujunwang\}@xiaomi.com,  \{cuifan, guoliyong, yujunwang\}@mail.nwpu.edu.com }
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\vspace{-0.5cm}
\begin{document}
%\ninept
%
\maketitle
%

\begin{abstract}
%Electroencephalography (EEG) plays a vital role in %%neuroimaging techniques, which is used to 
%detecting how brain responses to different stimulus. In this paper, we propose a novel shallow-deep attention-based model (XXXX) to classify the correct auditory stimulus evoking the EEG signal. It adopts the attention-based correlation module (ACM), which makes it possible to discover the connection between auditory speech and EEG from global aspect. Furthermore, the shallow-deep similarity classification module (SDSM) decides the classification result via embeddings extracted from the shallow and deep layers. Moreover, training strategies and data augmentation are used to boost model robustness. We conduct experiments on the dataset provided by Auditory EEG challenge (ICASSP Signal Processing Grand Challenge 2023). The proposed method has a significant gain over the baseline on the match-mismatch track of the Auditory EEG challenge 2023.

Electroencephalography (EEG) plays a vital role in detecting how brain responses to different stimulus. In this paper, we propose a novel Shallow-Deep Attention-based Network (SDANet) to classify the correct auditory stimulus evoking the EEG signal. It adopts the Attention-based Correlation Module (ACM) to discover the connection between auditory speech and EEG from global aspect, and the Shallow-Deep Similarity Classification Module (SDSCM) to decide the classification result via the embeddings learned from the shallow and deep layers.
Moreover, various training strategies and data augmentation are used to boost the model robustness.
Experiments are conducted on the dataset provided by Auditory EEG challenge (ICASSP Signal Processing Grand Challenge 2023). Results show that the proposed model has a significant gain over the baseline on the match-mismatch track.
%of the Auditory EEG challenge 2023.




%The abstract and introduction must clearly mention that this work is done in the context of an  “ICASSP Signal Processing Grand Challenge“ ( +include (a) official challenge name, (b) the year of the challenge, and if applicable (c) the edition number if this is not the first edition of the challenge)

%Various neuroimaging techniques can be used to investigate how the brain processes sound. Electroencephalography (EEG) is popular because it is relatively easy to conduct and has a high temporal resolution. Besides fundamental neuroscience research, EEG-based measures of auditory processing in the brain are also helpful in detecting or diagnosing potential hearing loss. They enable differential diagnosis of populations that can otherwise not be tested, such as young children or people with mental disabilities. In addition, there is a growing field of research in which auditory attention is decoded from the brain, with potential applications in smart hearing aids. An increasingly popular method in these fields is to relate a person’s electroencephalogram (EEG) to a feature of the natural speech signal they were listening to. This is typically done using linear regression to predict the EEG signal from the stimulus or to decode the stimulus from the EEG. Given the very low signal-to-noise ratio of the EEG, this is a challenging problem, and several non-linear methods have been proposed to improve upon the linear regression methods. In the Auditory-EEG challenge, teams will compete to build the best model to relate speech to EEG. We provide a large auditory EEG dataset containing data from 85 subjects who listen on average to 108 minutes of single-speaker stimuli for a total of 157 hours of data. We define two tasks:
% We leverage 3-stage HuBERT training mechanism to get a better chunk-wised streaing AED model. 
\end{abstract}
%
\begin{keywords}
Electroencephalography, Attention
\end{keywords}
%
\vspace{-0.3cm}

\section{Introduction}

\label{sec:intro}
ICASSP 2023 Auditory EEG Challenge is designed to explore the relationship between auditory stimulus and evoked EEG signal. In this paper, we mainly focus on the first (match-mismatch) task. Traditional methods \cite{dmochowski2018extracting} adopt the linear model to fit the feature transform from the stimulus to the EEG signal. Recently, deep learning based methods have been proposed to improve the performance of relating speech with EEG signal\cite{monesi2020lstm, accou2021modeling}, where the feature transform modules are replaced by long context model, such as long short-term memory (LSTM), stacked dilated Convolutional blocks, etc.

In this study, we base our system design on the baseline\footnote{https://github.com/exporl/auditory-eeg-challenge-2023-code\label{baseline}}. Nevertheless, we combine attention structure in the proposed ACM module to extract correlation information from global view rather than employing stacked convolution layers to achieve a long context. Moreover, both shallow and deep layer embeddings are used to determine the similarity information in the SDSCM module. Additionally, we use random data augmentation and various training strategies to increase the robustness.

With the help of these efforts, the accuracy of our approach on the test dataset increases from 77\% to 80\%. The challenge's evaluation metrics for our method in the final blind testset are 78.94\%, 2\% higher than the baseline.


%\textbf{Large number of parameters}. Almost all the SOTA pre-training models have a large number of parameters. The advantage of such a large number of parameters is that they have a larger modeling capacity, leading to greater robustness.
 
\vspace{-0.3cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{eeg_model.jpg}
    \caption{Illustration of the proposed model structures.}
    \label{fig:workflow}
    \vspace{-0.5cm}
\end{figure}

\section{Methodology}
\vspace{-0.3cm}
\subsection{Main Structure}

As shown in Figure~\ref{fig:workflow}, the system contains three inputs, including two audio stimuli and a slice of 64-channel EEG signal. The match and mismatch signals are fed to the auditory encoder branch with shared model parameters. The layers of convolution with kernel size 3 and powers of 2 dilation rate create a tree-like structure to enlarge the receptive field. Moreover, BatchNorm and ReLU layers are added to the outputs of convolutional layers.
Except for the four convolutional blocks, ACM blocks are introduced in the EEG encoder branch. The audio segments and EEG signals are converted to high-dimension representations using these encoder branches. Lastly, the match and mismatch signals are identified by the SDSCM blocks.
\vspace{-0.3cm}
\subsection{Attention-based Correlation Module}
\vspace{-0.1cm}
 Attention mechanism has been instrumental to make remarkable performance gains in many deep learning tasks.
Rather than processing the auditory and EEG branches separately, we adopt the ACM, consisting of a residual attention layer and a feed forward layer, to learn the relationship between EEG and auditory signal. The attention module maps query $X_{n}$ against key $E_{n}$ associated with candidate keywords, then presents re-weighted $X_{n}^{'}$ to emphasize the most important information.  
\vspace{-0.5cm}
\subsection{Shallow-Deep Similarity Classification Module}

As shown in Figure~\ref{fig:workflow},  SDSCM consists of Shallow Similarity Classifications Module(SSCM) and Deep Similarity Classification Module(DSCM).
The relation between the EEG and auditory signal can be evaluated at different scales. The extracted embedding has more specific information at the shallow layer, while the deep layer can gather profound semantic information. In this study, we compute the shallow-deep similarity embeddings using the encoder outputs as follows:
%\begin{equation}
 \begin{eqnarray}
   E_{s}=Concat(\frac{X_1^{'}}{\left\|X_1^{'}\right\|_2}, \frac{E_1}{\left\|E_1\right\|_2}\cdot{\frac{Y_1^{'}}{\left\|Y_1^{'}\right\|_2}, \frac{E_1}{\left\|E_1\right\|_2})}  \\
   E_{d}=Concat(\frac{X_3^{'}}{\left\|X_3^{'}\right\|_2}, \frac{E_3}{\left\|E_3\right\|_2}\cdot{\frac{Y_3^{'}}{\left\|Y_3^{'}\right\|_2}, \frac{E_3}{\left\|E_3\right\|_2})}
\end{eqnarray}
where $X_1^{'},Y_1^{'},E_1,X_3^{'},Y_3^{'},E_3$ are the outputs of the convolutional blocks illustrated in Figure~\ref{fig:workflow}, $E_{s}$ and $E_{d}$ represent the similarity embeddings of the shallow and deep layers, respectively. In order to predict the binary classification probability, a Dense layer with one output is fed with the final classification embedding $E=Concat(E_{s}, E_{d})$.




\vspace{-0.3cm}


\section{EXPERIMENTS}
\label{sec:typestyle}
\vspace{-0.2cm}
\subsection{Data Processing} \label{data_proc}
In the experiments, we use EEG dataset\cite{K3VSND_2023} provided by the EEG challenge, and split it into train-val-test subsets\textsuperscript{\ref{baseline}}. The generating training samplers are shown in Figure~\ref{fig:traing_sampler}. The matching pairs are produced using a sliding window of $window\_size=3s$ and $ window\_shift=Rand(1.0s,2.0s)$. The mismatch pairs are produced by randomly choosing the windows with an intersection of less than 35\% in order to produce various training data. According to the baseline\textsuperscript{\ref{baseline}}, the EEG signal is downsampled to 64 Hz, and the speech envelope is extracted for the inputs of the network. Besides, SpecAug\cite{park2019specaugment} is applied to the feature by masking and warping features channels and time steps. 
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{training_sampler.jpg}
    \caption{Illustration of training samplers generation.}
    \label{fig:traing_sampler}
    % \vspace{-1cm}
    \vspace{-0.5cm}
\end{figure}
\vspace{-0.3cm}
\subsection{Training} \label{training}
\textbf{Training Configuration}: During training, Adam optimizer with learning rate 3e-4 is used, and when the validation loss stops dropping, we degrade the learning rate by a factor of 3. 64 samplers are created from 8 distinct subjects for each training batch. We also use weight decay (weight=0.0001) and dropout (drop\_rate=0.2) on each layer to prevent overfitting. The last ten models are utilized to do model averaging once the model has been trained for 100 epochs.

\vspace{-0.4cm}
\subsection{Results and Analysis} 
Table~\ref{tab:result1} displays accuracy comparison on test dataset. The proposed SDANet, as presented in Figure~\ref{fig:workflow}, contains four parts: Backbone, ACM, SSCM and DSCM. The backbone together with DSCM achieves 77.2\% accuracy. With our new data generation method, it achieves 2\% higher accuracy comparing official baseline approach\textsuperscript{\ref{baseline}}. After adding ACM, the accuracy further increases by 0.8\%. Finally, with both ACM and SDSCM modules, the accuracy is further improved to 80.2\%, which is 3.2\% above the baseline. On the final blind testset, our proposed model obtains a 78.94\% criteria determined in the official challenge description\textsuperscript{\ref{baseline}}. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[!ht]\footnotesize
    \vspace{-0.2cm}
\caption{Accuracy comparison results.}
    \vspace{-0.2cm}
\label{tab:result1}
\begin{tabular}{lllll|l}
\hline
\toprule
\multicolumn{5}{l|}{Methods}                                                                                                                                                                       & Acc    \\ \hline
\multicolumn{5}{l|}{Baseline}                                                                                                                                                                      & 77\%   \\ \hline
\multicolumn{1}{l|}{SDANet}                                                                          & \multicolumn{1}{l|}{Backbone} & \multicolumn{1}{l|}{DSCM} & \multicolumn{1}{l|}{ACM} & SSCM &        \\ \hline
\multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Baseline\\ Data Generation\end{tabular}}              & \multicolumn{1}{l|}{\checkmark}        & \multicolumn{1}{l|}{\checkmark}    & \multicolumn{1}{l|}{}    &      & 77.2\% \\ \hline
\multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Our \\ Data Generation\end{tabular}}} & \multicolumn{1}{l|}{\checkmark}        & \multicolumn{1}{l|}{\checkmark}    & \multicolumn{1}{l|}{}    &      & 79\%   \\ \cline{2-6} 
\multicolumn{1}{l|}{}                                                                                & \multicolumn{1}{l|}{\checkmark}        & \multicolumn{1}{l|}{\checkmark}    & \multicolumn{1}{l|}{\checkmark}   &      & 79.8\% \\ \cline{2-6} 
\multicolumn{1}{l|}{}                                                                                & \multicolumn{1}{l|}{\checkmark}        & \multicolumn{1}{l|}{\checkmark}    & \multicolumn{1}{l|}{\checkmark}   & \checkmark    & 80.2\% \\ \hline
\end{tabular}
\end{table}

    \vspace{-0.3cm}
\section{CONCLUSION}
The objective of the study is to learn the relationship between auditory stimulus and EEG. We propose a SDANet model with ACM and SDSCM to increase the crossing linkages rather than treating them separately. %In addition, various training strategies are integrated to enhance robustness.
Results show that our model improves the classification accuracy effectively.


%\newpage



%\section{REFERENCES}
%\label{sec:refs}
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\footnotesize
\bibliographystyle{IEEEbib}
\bibliography{mybib}

\end{document}
