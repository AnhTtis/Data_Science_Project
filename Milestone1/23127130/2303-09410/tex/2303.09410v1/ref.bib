@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})

% ADD
@String(DV = {Int. Conf. 3D Vis.})
@String(WACV = {IEEE Winter Conf. Appl. Comput. Vis.})

@inproceedings{sohn2015learning, 
  title={Learning structured output representation using deep conditional generative models},
  author={Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  booktitle=NIPS,
  year={2015}
} % cVAE

@inproceedings{wu2019unified, 
  title={Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations},
  author={Wu, Hao and Mao, Jiayuan and Zhang, Yufeng and Jiang, Yuning and Li, Lei and Sun, Weiwei and Ma, Wei-Ying},
  booktitle=CVPR,
  pages={6609--6618},
  year={2019}
} % Scene Graph Parser

@article{zhao2014indexing,
  title={Indexing 3{D} scenes using the interaction bisector surface},
  author={Zhao, Xi and Wang, He and Komura, Taku},
  journal=TOG,
  volume={33},
  number={3},
  pages={1--14},
  year={2014},
} % IBS

@article{kim2014shape2pose,
  title={Shape2pose: Human-centric shape analysis},
  author={Kim, Vladimir G and Chaudhuri, Siddhartha and Guibas, Leonidas and Funkhouser, Thomas},
  journal=TOG,
  volume={33},
  number={4},
  pages={1--12},
  year={2014},
}

@article{savva2016pigraphs,
  title={Pigraphs: learning interaction snapshots from observations},
  author={Savva, Manolis and Chang, Angel X and Hanrahan, Pat and Fisher, Matthew and Nie{\ss}ner, Matthias},
  journal=TOG,
  volume={35},
  number={4},
  pages={1--12},
  year={2016},
} % PiGraph

@inproceedings{chen2019holistic++,
  title={Holistic++ scene understanding: Single-view 3{D} holistic scene parsing and human pose estimation with human-object interaction and physical commonsense},
  author={Chen, Yixin and Huang, Siyuan and Yuan, Tao and Qi, Siyuan and Zhu, Yixin and Zhu, Song-Chun},
  booktitle=ICCV,
  pages={8648--8657},
  year={2019}
}

@article{monszpart2019imapper,
  title={iMapper: interaction-guided scene mapping from monocular videos},
  author={Monszpart, Aron and Guerrero, Paul and Ceylan, Duygu and Yumer, Ersin and Mitra, Niloy J},
  journal=TOG,
  volume={38},
  number={4},
  pages={1--15},
  year={2019},
}

@inproceedings{pavlakos2019expressive,
  title={Expressive body capture: 3{D} hands, face, and body from a single image},
  author={Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed AA and Tzionas, Dimitrios and Black, Michael J},
  booktitle=CVPR,
  pages={10975--10985},
  year={2019}
} % SMPL-X, VPoser

@inproceedings{hassan2019resolving,
  title={Resolving 3{D} human pose ambiguities with 3{D} scene constraints},
  author={Hassan, Mohamed and Choutas, Vasileios and Tzionas, Dimitrios and Black, Michael J},
  booktitle=ICCV,
  pages={2282--2292},
  year={2019}
} % PROX dataset

@inproceedings{zhang2020generating,
  title={Generating 3{D} people in scenes without people},
  author={Zhang, Yan and Hassan, Mohamed and Neumann, Heiko and Black, Michael J and Tang, Siyu},
  booktitle=CVPR,
  pages={6194--6204},
  year={2020}
} % PSI

@inproceedings{zhang2020place,
  title={PLACE: Proximity learning of articulation and contact in 3{D} environments},
  author={Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J and Tang, Siyu},
  booktitle=DV,
  pages={642--651},
  year={2020},
} % PLACE

@inproceedings{hassan2021populating,
  title={Populating 3{D} scenes by learning human-scene interaction},
  author={Hassan, Mohamed and Ghosh, Partha and Tesch, Joachim and Tzionas, Dimitrios and Black, Michael J},
  booktitle=CVPR,
  pages={14708--14718},
  year={2021}
} % POSA

@inproceedings{wang2021synthesizing,
  title={Synthesizing long-term 3{D} human motion and interaction in 3{D} scenes},
  author={Wang, Jiashun and Xu, Huazhe and Xu, Jingwei and Liu, Sifei and Wang, Xiaolong},
  booktitle=CVPR,
  pages={9401--9411},
  year={2021}
} % Long-term

@inproceedings{wang2022towards,
  title={Towards Diverse and Natural Scene-aware 3{D} Human Motion Synthesis},
  author={Wang, Jingbo and Rong, Yu and Liu, Jingyuan and Yan, Sijie and Lin, Dahua and Dai, Bo},
  booktitle=CVPR,
  pages={20460--20469},
  year={2022}
} % Towards

@article{zhao2022compositional,
  title={Compositional Human-Scene Interaction Synthesis with Semantic Control},
  author={Zhao, Kaifeng and Wang, Shaofei and Zhang, Yan and Beeler, Thabo and Tang, Siyu},
  journal={arXiv preprint arXiv:2207.12824},
  year={2022}
} % COINS

@article{huang2023diffusion,
  title={Diffusion-based Generation, Optimization, and Planning in 3{D} Scenes},
  author={Huang, Siyuan and Wang, Zan and Li, Puhao and Jia, Baoxiong and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2301.06015},
  year={2023}
}

@inproceedings{armeni20193d,
  title={3{D} scene graph: A structure for unified semantics, 3{D} space, and camera},
  author={Armeni, Iro and He, Zhi-Yang and Gwak, JunYoung and Zamir, Amir R and Fischer, Martin and Malik, Jitendra and Savarese, Silvio},
  booktitle=ICCV,
  pages={5664--5673},
  year={2019}
} % 3D Scene Graph 1

@article{rosinol20203d,
  title={3{D} dynamic scene graphs: Actionable spatial perception with places, objects, and humans},
  author={Rosinol, Antoni and Gupta, Arjun and Abate, Marcus and Shi, Jingnan and Carlone, Luca},
  journal={arXiv preprint arXiv:2002.06289},
  year={2020}
} % 3D Scene Graph 2

@inproceedings{wald2020learning,
  title={Learning 3{D} semantic scene graphs from 3{D} indoor reconstructions},
  author={Wald, Johanna and Dhamo, Helisa and Navab, Nassir and Tombari, Federico},
  booktitle=CVPR,
  pages={3961--3970},
  year={2020}
} % 3DSGG

@article{zhang2021knowledge,
  title={Knowledge-inspired 3{D} Scene Graph Prediction in Point Cloud},
  author={Zhang, Shoulong and Hao, Aimin and Qin, Hong and others},
  journal=NIPS,
  volume={34},
  pages={18620--18632},
  year={2021}
}

@inproceedings{wu2021scenegraphfusion,
  title={Scenegraphfusion: Incremental 3{D} scene graph prediction from rgb-d sequences},
  author={Wu, Shun-Cheng and Wald, Johanna and Tateno, Keisuke and Navab, Nassir and Tombari, Federico},
  booktitle=CVPR,
  pages={7515--7525},
  year={2021}
}

@inproceedings{zhang2021exploiting,
  title={Exploiting edge-oriented reasoning for 3{D} point-based scene graph analysis},
  author={Zhang, Chaoyi and Yu, Jianhui and Song, Yang and Cai, Weidong},
  booktitle=CVPR,
  pages={9705--9715},
  year={2021}
}

@inproceedings{zhou2019scenegraphnet,
  title={SceneGraphNet: Neural Message Passing for 3{D} Indoor Scene Augmentation},
  author={Zhou, Yang and While, Zachary and Kalogerakis, Evangelos},
  booktitle=ICCV,
  pages={7384--7392},
  year={2019},
}

@inproceedings{dhamo2021graph,
  title={Graph-to-3{D}: End-to-end generation and manipulation of 3{D} scenes using scene graphs},
  author={Dhamo, Helisa and Manhardt, Fabian and Navab, Nassir and Tombari, Federico},
  booktitle=ICCV,
  pages={16352--16361},
  year={2021}
}

@article{song2022actformer,
  title={ActFormer: A GAN Transformer Framework towards General Action-Conditioned 3{D} Human Motion Generation},
  author={Song, Ziyang and Wang, Dongliang and Jiang, Nan and Fang, Zhicheng and Ding, Chenjing and Gan, Weihao and Wu, Wei},
  journal={arXiv preprint arXiv:2203.07706},
  year={2022}
}

@article{youwang2022clip,
  title={CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes},
  author={Youwang, Kim and Ji-Yeon, Kim and Oh, Tae-Hyun},
  journal={arXiv preprint arXiv:2206.04382},
  year={2022}
}

@article{hong2022avatarclip,
  title={AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3{D} Avatars},
  author={Hong, Fangzhou and Zhang, Mingyuan and Pan, Liang and Cai, Zhongang and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2205.08535},
  year={2022}
}

@inproceedings{ghosh2021synthesis,
  title={Synthesis of compositional animations from textual descriptions},
  author={Ghosh, Anindita and Cheema, Noshaba and Oguz, Cennet and Theobalt, Christian and Slusallek, Philipp},
  booktitle=ICCV,
  pages={1396--1406},
  year={2021}
}

@article{petrovich2022temos,
  title={TEMOS: Generating diverse human motions from textual descriptions},
  author={Petrovich, Mathis and Black, Michael J and Varol, G{\"u}l},
  journal={arXiv preprint arXiv:2204.14109},
  year={2022}
}

@article{athanasiou2022teach,
  title={TEACH: Temporal Action Composition for 3{D} Humans},
  author={Athanasiou, Nikos and Petrovich, Mathis and Black, Michael J and Varol, G{\"u}l},
  journal={arXiv preprint arXiv:2209.04066},
  year={2022}
}

@inproceedings{guo2022generating,
  title={Generating Diverse and Natural 3{D} Human Motions From Text},
  author={Guo, Chuan and Zou, Shihao and Zuo, Xinxin and Wang, Sen and Ji, Wei and Li, Xingyu and Cheng, Li},
  booktitle=CVPR,
  pages={5152--5161},
  year={2022}
}

@inproceedings{yang2021sat,
  title={Sat: 2{D} semantics assisted training for 3{D} visual grounding},
  author={Yang, Zhengyuan and Zhang, Songyang and Wang, Liwei and Luo, Jiebo},
  booktitle=ICCV,
  pages={1856--1866},
  year={2021}
}

@inproceedings{feng2021free,
  title={Free-form description guided 3{D} visual graph network for object grounding in point cloud},
  author={Feng, Mingtao and Li, Zhen and Li, Qi and Zhang, Liang and Zhang, XiangDong and Zhu, Guangming and Zhang, Hui and Wang, Yaonan and Mian, Ajmal},
  booktitle=ICCV,
  pages={3722--3731},
  year={2021}
}

@inproceedings{roh2022languagerefer,
  title={Languagerefer: Spatial-language model for 3{D} visual grounding},
  author={Roh, Junha and Desingh, Karthik and Farhadi, Ali and Fox, Dieter},
  booktitle={Conf.  Rob. Learn.},
  pages={1046--1056},
  year={2022},
}

@inproceedings{luo20223d,
  title={3{D}-SPS: Single-Stage 3{D} Visual Grounding via Referred Point Progressive Selection},
  author={Luo, Junyu and Fu, Jiahui and Kong, Xianghao and Gao, Chen and Ren, Haibing and Shen, Hao and Xia, Huaxia and Liu, Si},
  booktitle=CVPR,
  pages={16454--16463},
  year={2022}
}

@inproceedings{zhou2019continuity,
  title={On the continuity of rotation representations in neural networks},
  author={Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  booktitle=CVPR,
  pages={5745--5753},
  year={2019}
}

@article{qi2017pointnet++,
  title={Pointnet++: Deep hierarchical feature learning on point sets in a metric space},
  author={Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  journal=NIPS,
  year={2017}
} %PointNet++

@inproceedings{li2019deepgcns,
  title={Deepgcns: Can gcns go as deep as cnns?},
  author={Li, Guohao and Muller, Matthias and Thabet, Ali and Ghanem, Bernard},
  booktitle=ICCV,
  pages={9267--9276},
  year={2019}
} % GCN

@article{chang2017matterport3d,
  title={Matterport3{D}: Learning from rgb-d data in indoor environments},
  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  journal={arXiv preprint arXiv:1709.06158},
  year={2017}
} % Matterport3D

@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3{D} reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle=CVPR,
  pages={5828--5839},
  year={2017}
} % ScanNet

@inproceedings{achlioptas2020referit3d,
  title={Referit3{D}: Neural listeners for fine-grained 3{D} object identification in real-world scenes},
  author={Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
  booktitle=ECCV,
  pages={422--440},
  year={2020},
} % Referit3D

@inproceedings{punnakkal2021babel,
  title={BABEL: Bodies, action and behavior with english labels},
  author={Punnakkal, Abhinanda R and Chandrasekaran, Arjun and Athanasiou, Nikos and Quiros-Ramirez, Alejandra and Black, Michael J},
  booktitle=CVPR,
  pages={722--731},
  year={2021}
} % BABEL

@inproceedings{yi2022human,
  title={Human-aware object placement for visual environment reconstruction},
  author={Yi, Hongwei and Huang, Chun-Hao P and Tzionas, Dimitrios and Kocabas, Muhammed and Hassan, Mohamed and Tang, Siyu and Thies, Justus and Black, Michael J},
  booktitle=CVPR,
  pages={3959--3970},
  year={2022}
}

@inproceedings{ahn2018text2action,
  title={Text2action: Generative adversarial synthesis from language to action},
  author={Ahn, Hyemin and Ha, Timothy and Choi, Yunho and Yoo, Hwiyeon and Oh, Songhwai},
  booktitle={{Int. Conf.  Rob. Autom.}},
  pages={5915--5920},
  year={2018},
}

@inproceedings{ahuja2019language2pose,
  title={Language2pose: Natural language grounded pose forecasting},
  author={Ahuja, Chaitanya and Morency, Louis-Philippe},
  booktitle=DV,
  pages={719--728},
  year={2019},
}

@inproceedings{chen2020scanrefer,
  title={Scanrefer: 3{D} object localization in rgb-d scans using natural language},
  author={Chen, Dave Zhenyu and Chang, Angel X and Nie{\ss}ner, Matthias},
  booktitle=ECCV,
  pages={202--221},
  year={2020},
}

@inproceedings{tan2018and,
  title={Where and who? automatic semantic-aware person composition},
  author={Tan, Fuwen and Bernier, Crispin and Cohen, Benjamin and Ordonez, Vicente and Barnes, Connelly},
  booktitle=WACV,
  pages={1519--1528},
  year={2018},
}

@inproceedings{li2019putting,
  title={Putting humans in a scene: Learning affordance in 3{D} indoor environments},
  author={Li, Xueting and Liu, Sifei and Kim, Kihwan and Wang, Xiaolong and Yang, Ming-Hsuan and Kautz, Jan},
  booktitle=CVPR,
  pages={12368--12376},
  year={2019}
}


%%% SUPPLEMENT
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{dutta2019via,
  title={The VIA annotation software for images, audio and video},
  author={Dutta, Abhishek and Zisserman, Andrew},
  booktitle=ACMMM,
  pages={2276--2279},
  year={2019}
} % VIA Annotation Tool
