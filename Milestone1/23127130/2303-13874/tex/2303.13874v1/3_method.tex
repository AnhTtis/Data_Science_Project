\section{Query-Dependent DETR}
% motivation
% 3.1    - cross-attention --> paragraph
% 3.2    - Negative pair --> paragraph
% 3.3    - global token --> paragraph
% - 3.2.1 Overall architecture
% - 3.3.3 Learning the query representation % 어떻게 활용
%     - decoder
%     - loss function
% \subsection{Background and Motivation}
Moment retrieval and highlight detection have the common objective to find preferred moments with the text query.
% Given video $V \in (l_v, d_v)$ and text features $T \in (l_t, d_t)$, the main objective is to localize the center coordinate $m_c$ and width $m_\sigma$ within the video and rank the highlight score~(saliency score) for each clip.
% Given video clips $\{v_1, v_2, ..., v_L\} \subset V$ and text features
% x_v --> integrated video token
%Given video $\{v_1, v_2, ..., v_L\}$ and text representations $\{ t_1, t_2, ..., t_{N} \}$ which are encoded by frozen video and text encoders.
Given a video of $L$ clips and a text query with $N$ words, we denote their representations as $\{v_1, v_2, ..., v_L\}$ and $\{ t_1, t_2, ..., t_{N} \}$ extracted by frozen video and text encoders, respectively.
%Given a video $\{v_1, v_2, ..., v_L\}$ consisting of $L$ clips and text query $\{ t_1, t_2, ..., t_{N} \}$ of $N$ words which are encoded by frozen video and text encoders.
With these representations, the main objective is to localize the center coordinate $m_c$ and width $m_\sigma$ within the video and rank the highlight score~(saliency score) $\{ s_1, s_2, ..., s_{L} \}$ for each clip.
%As an advanced architecture, transformer~\cite{vaswani2017attention, dosovitskiy2020image}, are dominant these days, intuitive approaches using transformer are making a moment-wise prediction as a set of clips~\cite{momentdetr} or generating the moment according to the clip-wise predictions~\cite{umt}.
A straightforward approach to utilize transfomer~\cite{vaswani2017attention, dosovitskiy2020image} for the MR is to make a moment-wise prediction as a set of clips~\cite{momentdetr}, or generating the moment according to the clip-wise predictions~\cite{umt}.
To exploit the multi-modal information, e.g., video and text query, they either simply concatenated the features across the modalities or inserted the texts to form the moment query to the transformer decoder. 
% \WJ{} % 더 좋게 적어보려고 현재 체크표시하고 건너뛰었음.
%However, since the configuration for MR/HD requires every video clip to be conditionally assessed with the text queries, we claim that careful design is needed to process the source and query differently. 
However, we claim that the relationship between the video and text query should be carefully considered rather than a simple concatenation since MR/HD requires every video clip to be conditionally assessed with the text queries.

Our overall architecture is described in Fig.~\ref{fig:overall architecture}, following the design of concrete baseline, Moment-DETR~\cite{momentdetr}.
%, which modified the detection transformer to deal with temporal video data.
Given a video and query representation extracted from fixed backbones, QD-DETR first transforms the video representation to be query-dependent using cross-attention layers. To further enhance the query-awareness of video representations, we incorporate irrelevant video-query pairs with a low saliency for the learning objective. Then, along with the transformer encoder-decoder architectures, the saliency token is defined that turns into an adaptive saliency predictor when attended by the specific video instance.
%Also, we employ dynamic anchors for the decoder query as the information across modalities are integrated at the encoder level.
% \WJ{Note that, QD-DETR is also capable of processing multi-modal sources with simple concatenation.}
% 글줄여야하면 이거 쓸모없는듯
%Extensive experiments verify the effectiveness of each component in Sec.~\ref{Sec.Experiments}.


% \subsubsection{Query-Dependent Video Representation}
% \label{Sec.Query-Dependent}
% In this work, we mainly focus on enhancing the query-dependency of the moment retrieval framework in both the architectural view and training scheme.
% In the following, we discuss three components to generate query-dependent video representation.

% first introduce the encoder design to consider the feature level query-dependency. 
% In detail, we incorporate the query information into the intermediate features representation of encoder by cross-attention, and build adaptive highlight score criteria adjusted by the input queries and videos.
% Then, we propose additional loss to consider the inter-relationship between the videos and queries, which have been neglected in previous methods.
% Then, we propose additional loss to consider the inter-relationship between the videos and queries by penalizing the highlight score of pairs with negative relations. 
% In following sections, we elaborate on the proposed method with the detailed explanation.




\subsection{Cross-Attentive Transformer Encoder}
\label{Sec.CrossAttentiveTransformerEncoder}
In this subsection, we use italic letters to represent {\it query}, {\it key}, and {\it value} of the cross-attention layers.
The key objective of the encoder for MR/HD is to produce clip-wise representations equipped with information regarding the degree of query-relevance since these features are directly used for retrieving the query-matched moments and predicting clip-wise saliency scores.
%Since these features are directly used for retrieving the query-matched moments and predicting clip-wise saliency scores, it is important to encode the contexts of the query into the video representations.
However, the encoding process of existing works may not ensure the query conditioning on every clip.
For example, Moment-DETR~\cite{momentdetr} naively concatenated the video with the query %to prepare multi-modal input
for input to the self-attention layers, which may result in an insignificant role of the query if the high similarities among the video clips overwhelm the contribution of the text query.
%outweighs . %the similarity to the text query.
On the other hand, UMT~\cite{umt} utilizes the text query only for the synthesis of a moment query in the transformer decoder so thus resulting video representations are not associated with the text query.
%In other words, video representations are not associated with the text query.
% On the other hand, UMT~\cite{umt} employs the encoder only processing the multi-modal video representations~(e.g. frames and audio), whereas the text queries are just used as synthesizing the moment query of the transformer decoder, instead of refining the video representations.
% ========================================

%For each video clip representation to be associated with the textual contexts,
To take the textual contexts into every video clip representation, we deploy cross-attention layers between the source and the query modalities at the very first layers of the encoder.
This ensures the consistent contribution of the query, thereby extracting query-dependent video representation.
In detail, whereas the {\it query} for cross-attention layers is prepared by projecting the video clips as $Q_v = [~p_q(v_1), ..., p_q(v_L)~]$, the {\it key} and {\it value} are computed with the query text features as $K_t = [~p_k(t_1), ..., p_k(t_N)~]$ and $V_t = [~p_v(t_1), ..., p_v(t_N)~]$.
$p_q(\cdot)$, $p_k(\cdot)$, and $p_v(\cdot)$ are projection layers for {\it query}, {\it key}, and {\it value}.
Then, the cross-attention layer operates as follows:
\begin{eqnarray}
    \label{eqn 1 cross attention}
    \text{Attention}(Q_v, K_t, V_t) = \text{softmax}(\frac{Q_vK_t^T}{\sqrt{d}})V_t,
\end{eqnarray}
where $d$ is the dimension of the projected {\it key}, {\it value}, and {\it query}.
Since the softmax scores are distributed only over the query elements, video clips are expressed with the weighted sum of the text queries in proportion to the similarity to texts.
Attention scores are then projected through MLP and integrated into the original video representations as the typical transformer layers. %do.
For the rest of the paper, we define the query-dependent video tokens, i.e., the output of cross-attention layers, as $X = \{ x_v^1, x_v^2, ..., x_v^L \}$.
% , as $\{x_v^1, x_v^2, ..., x_v^L\} \subset X$.

% \SE{} % overall framework 에서 설명하긴 했지만, 전체 흐름 및 notation 정리를 위해 중간 과정으 여기 추가하면 어떨까 합니다. 아래는 제가 적은 내용입니다.
% After the cross-attention encoder, we obtain a set of video tokens $\{x_v^1, x_v^2, ..., x_v^L\} \subset X$, which are query-dependent video representations integrated with the text features.





\begin{figure}[t]
      \centering
        \includegraphics[width=1.05\linewidth]{figs/fig3_negpair.pdf}
        \vspace{-0.8cm}
        \caption{
            Illustration of negative pair learning. Typical HL loss is defined only with a positive video-query pair, which is insufficient to learn various degrees of query-relevance. 
            %which lacks the training signal to learn the degree of query relevance since the paired video and query always share contextual information. 
            % \SE{} % training signal --> diversity of training signals? 페어의 다양성이 부족하다?
            On the other hand, our negative pair learning enforces the model to yield different scores for a video depending on the query and to be learned to suppress saliency scores for the negative query.
            %while saliency scores are learned to be suppressed when the negative query is given.
        % Illustration for negative pair learning for enhancing the query-video interrelationship.
        % % Typical HL loss 
        % % Given the positive query-video pairs, typical HL loss learns to rank the clips in a single video based on its saliency to the given text query.
        % % 기존 HL loss은 어떤 식으로 작동하고, "..." 가 부족함.
        % % 우리는 neg pair loss 를 통해 이를 보완하고, "..." 한 특성을 achieve 함.
        % % Differently, we consider the
        % % Typical HL loss learns to rank the clips in a single video based on their saliency to given query.
        % % Typical HL loss learns to distinguish the saliency of clips in a single video.
        % % We found this training scheme leads to the lack of capability to measure the query-relevance precisely.
        % % Differently, 
        % % 기존 + neg를 쓰고, 이를 통해 query-relevance를 반영하지 못하는 문제를 해결했다?
        % Typical HL loss learns to distinguish the saliency of clips in a only single video, and this lack of diversity in video-query pairs leads to improper query-relevance measurement.
        % Hence, we additionally propose negative pair learning, which suppresses the saliency score of negative pairs, to consider the general relationship between video-query pairs.
        % % 단순히 설명쪽으로 가는게 figure에 더 적절한가?
        }
    \label{fig:neg_pair}
\end{figure}



% \subsection{Video-Discriminative Query Information}
\subsection{Learning from Negative Relationship}
\label{Sec.VideoDiscriminativeQueryInformation}
% Then, in order to not only inject a lot of textual information into the video feature but also make it fully exploited, we leverage the negative video-query pairs generated by mixing the original pairs.
% Specifically, we suppress the saliency scores of the negative~(irrelevant) pairs.
% Our expectation is the increased contribution of the text query in prediction since the videos will be sometimes required to yield high saliency scores and sometimes low ones depending on whether the text query is positive or not.
While the cross-attention layers explicitly fuse the video and query features for intermediate video clip representations to engage the query information in an architectural way, we argue that given video-text pairs lack diversity to learn the general relationship.
%In our architectural design where cross-attention layers explicitly fused the video and query contexts, the query information is significantly engaged in the intermediate video clip representations.
%Besides, we argue that given video-text pairs lack diversity to learn the general relationship.
For instance, many consecutive clips in a single video often share similar appearances, and the similarity to a specific query will not be highly distinguishable, thereby, the text query may not much affect the prediction.


% Under these circumstances, we argue that given video-text pairs lack diversity to learn the general relationship between the modalities since the sequential frames in a single video often maintain similar appearances.
% Such limited information makes it hard to distinguish descriptive keywords~(e.g., action verbs, subjects, objects) from noise words~(e.g., 'a', 'the') in the query text.

% In detail, we argue that given video-text pairs lack diversity to learn the general relationship between the modalities since the sequential frames in a single video often maintain similar appearances.
% % That is, such limited information makes the model prevent from exploiting the powerful capability of text query descriptive the video context, by only providing the deficient visual features per text query.
% We argue the problem of the current training scheme only distinguishing the saliency of video clips in the given query-video pairs.
% In that case, the model focus on the fine-grained discrepancy between the video clips in a single video, neglecting the shared information along with the entire video sequence.
% % fine grained?
% However, in many cases, this shared information contains significant clues to understand the video context, e.g., places or objects that appeared in the video.
% % 이러한 경우, 비디오 전체 맥락에 맞는 text 들은 무시하고, fine grained 한 클립 간 구분만을 학습하게 된다.
% % We also show the saliency histogram with given negative and positive pairs in Fig.~\ref{???}.
% % Nevertheless, as in Fig.~~\ref{fig:motivation_ex}, we find that the naive usage of text query incurs the model's lack of capability to distinguish the query-relevant clips.
% % 1 안. 결과를 보여주고 (Cross-attention만 한 것) Sec 3,1로는 부족했다고 설명하기
% % 2 안. 단순히 구조적 변경으로는 부족할 수 있음을 얘기하기?
% % --> 2)로 가게될 시 "highly depends on the quality of the query feature" 유지?

% 원준 원본
% In our architectural design where cross-attention layers explicitly fused the video and query contexts, the model performance highly depends on the quality of the query features since the query information is significantly engaged in building video representation.
% However, in Fig.~~\ref{fig:motivation_ex}, we find that the naive usage of text query incurs the model's lack of capability to distinguish the query-relevant clips.
% For instance, not only similar saliency scores are yielded regardless of the accordance between video and text queries but also inconsistent scores to the degree of video-query relevance are observed.
% 
% Under these circumstances, we argue that given video-text pairs lack diversity to learn the general relationship between the modalities since the sequential frames in a single video often maintain similar appearances.
% Such limited information makes it hard to distinguish descriptive keywords~(e.g., action verbs, subjects, objects) from noise words~(e.g., 'a', 'the') in the query text.
% ==================================================

Thus, we consider the relationships between irrelevant pairs of videos %and queries from different pairs
inspired by many recognition practices~\cite{he2016deep, lin2017focal, li2020dividemix, moon2022difficulty} that learn discriminative features across different categories.
%where they learn discriminative features by considering the relationships to other categories.
%To implement such relationships, we define the given video-query pairs as positive pairs and mix the video-query pairs to construct negative pairs.
To implement such relationships, we define given training video-query pairs as positive pairs and mix the video and query from different pairs to construct negative pairs.
Fig.~\ref{fig:neg_pair} illustrates the ways to augment such negative pairs and utilize them with positive pairs in training.
%and utilize the positive and negative pairs.
While the video clips in positive pairs are trained to yield segmented saliency scores according to the query-relevance, irrelevant negative video-query pairs are enforced to have the lowest saliency scores.
Formally, the loss function for suppressing the saliency of negative pairs $x_v^{\text{neg}}$ are expressed as follows:
\begin{eqnarray}
    \label{eq.negative learning}
    L_{\text{neg}} = - \log(1 - S(x_v^{\text{neg}})),
\end{eqnarray}
where $S(\cdot)$ is the saliency score predictor.
This training scheme can also prevent the model from predicting the moments and highlights solely based on the inter-relationship among video clips without consideration of the query-relevance since the same video instance should be predicted differently depending on whether the positive or negative query is given.

%With a proposed training scheme, our expectation is that the model is enforced to consider the contexts of the query since the same video instance should be predicted differently depending on whether the positive or the negative query is given.
% With a proposed training scheme, our expectation is that positive pairs extract corresponding descriptive keywords from the query, whereas the negatively paired video clips will retain higher similarity to general noise words in the text query.




% 한 비디오내의 페어만 고려하면 general한 video-query 관계를 학습하기에 다양성이 부족하다.
% 특히, 비디오를 표현하는데 어떤 부분이 semantical 하고, e.g., action verb, objective, noun, 어떤것들이 의미 없는지, e.g., a, the, noise word, 구분하기 힘듬.
% 그래서 general한 semantic relation을 학습하기 위해 negative pair 를 고려한다.
% negative pair를 학습하면서, video마다 같은 query의 context가 saliency score가 높고 낮게 학습되면서
% video마다 query의 relevance를 잘학습하게 된다.
% 특히, we expect pos, neg relation을 학습하면서 positive pair에서는 video context랑 맞는 keyword가 highlight되고,
% negative pair에서는 항상 어느정도 관련이 있는 a, the 등이랑 관련이 높을것이다


% \begin{figure}
%     \centering
%     % Difference in nearest neighbor depending on the existence of saliency token
%     % \caption{Retrieved moments based on saliency token distance.}
%     \includegraphics[width=1.0\linewidth]{figs/figs_rebuttal/saliency_token_framework_squeezed.pdf}
%     % \includegraphics[width=0.94\linewidth]{figs/figs_rebuttal/saliency_token_framework.pdf}
%     % \includegraphics[width=1.05\linewidth]{figs/figs_rebuttal/saliency_token_eg_baseline.pdf}
%     \vspace{-1.3cm}
%     \caption{Detailed illustration of adaptive saliency predictor. 
%     % Randomly initialized and learnable saliency token is processed by transformer encoder within video tokens.
%     Saliency token is a learnable and randomly initialized vector.
%     By processing with transformer encoder, saliency token is re-organized with the input-dependent contexts which video tokens have. 
%     % After linear projection, saliency scores are computed by scaled-dot product between saliency and video tokens.
%     Then, we compute saliency scores by scaled-dot product between saliency and video tokens following linear projection.
%     \SE{} % 함 써본건데.. 이정도 들어간다하면 ack 까지 생각했을 때 본문 좀 줄여야하긴 하겟어요 
%     }
%     \label{fig:saliency token details}
%     % \vspace{-0.2cm}
% \end{figure}

% \subsection{Input-Dependent Saliency Predictor}
\subsection{Input-Adaptive Saliency Predictor}
\label{Sec.SaliencyToken}
% Naive implementation for saliency predictor $S(\cdot)$ would be stacking one or more fully-connected layers for the common use of various inputs~\cite{momentdetr, umt}.
Naive implementation for saliency predictor $S(\cdot)$ would be stacking one or more fully-connected layers. % for the common use of various inputs~\cite{momentdetr, umt}.
However, such a general head provides identical criteria for the saliency prediction of every video-query pair, neglecting the diverse nature of video and natural language query pairs. This violates our key idea to extract query-dependent video representation.

% r. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence

Thus, we define the saliency token $x_s$ to be utilized as an input-adaptive saliency predictor.
Briefly, the saliency token is a randomly initialized learnable vector that becomes an input-adaptive predictor when added to the sequence of encoded video tokens and projected through the transformer encoder. 
% Then, it measures the saliency scores based on the similarity measure.
% Briefly, the saliency token transforms into an input-adaptive predictor by the transformer encoder and measures the saliency based on the similarity measure.
To illustrate, as shown in Fig.~\ref{fig:overall architecture}, we first concatenate the saliency token with the query-dependent video tokens $X$.
We process these tokens to the transformer encoder which makes the saliency token to be re-organized with the input-dependent contexts.
Consequently, saliency and video tokens are projected by a corresponding single fully-connected layer with weights, $w_{s}$ and $w_{v}$, respectively, where their scaled-dot product becomes the saliency scores.
Formally, saliency score $S(x_v^i)$ is computed as follows:
\begin{eqnarray}
    \label{eq. saliency score prediction}
     S(x_v^i) = \frac{w_s^T x_s \cdot w_v^T x_v^i}{\sqrt{d}},
\end{eqnarray}
where $d$ is the channel dimension of projected tokens.
% \SE{} % 여기 x_s는 x_v^i에 대응되는 saliency token이 무조건 사용되니까 S() 안에 안넣었는데, 이게 문제가 될 수 있을까요?



% Basically, the saliency token $x_s$ is defined as a learnable vector. Then, we concatenate this token with a set of video tokens $X$, and give them as the input of the transformer encoder.
% After the transformer layers, we detach the saliency token from the other video tokens.
% These two types of tokens are linearly projected by the different learnable weights $w_{s}$ and $w_{v}$, for saliency and video token, respectively.



% \subsubsection{Overall Architecture}
% Our overall architecture is described in Fig.~\ref{fig:overall architecture}, following the design of concrete baseline, Moment-DeTR~\cite{momentdetr}, which modified the detection transformer to handle temporal video data.
% Given the video and query representation extracted from the fixed backbones, QD-DETR transforms the video representation to be query-dependent using cross-attention layers.
% Then, along with the transformer encoder-decoder architectures, the saliency token is defined and attended by the video instance and serves a role as an input-dependent saliency predictor.
% Also, our model can handle the additional modalities except for the video frames, e.g. audio, as UMT~\cite{umt} does. 
% In detail, these modalities can be added by simple concatenation to the video features.



% In this subsection, we illustrate how we utilize the query-dependent representation obtained from Sec.~\ref{Sec.Query-Dependent}.
\subsection{Decoder and Objectives}
\paragraph{Transformer Decoder.}
Recently, understanding the role of the query in the detection transformer is being spotlighted~\cite{conditionaldetr, dabdetr}. 
It is verified that designing the query with the positional information helps not only for acceleration of training but also for enhancing accuracy.
Yet, it is hard to directly employ these studies in tasks handling multi-modal data, e.g., MR/HD, since multi-modal data often have different definitions of position; the position can be understood as time in the video and word order in the text.
% \SE{} % 여기 이해하기 좀 여려움. modality 끼리 position (e.g. time, sentence length) 가 공유되지 않는다?




%On the contrary, our architectural design eliminates the need for the text query to be input to the decoder because the query information is already taken into the video representation.
On the contrary, our architectural design eliminates the need to feed the text query to the decoder since the query information is already taken into the video representations.
To this end, we modify the 2D dynamic anchor boxes~\cite{dabdetr} to represent 1D moments in the video.
Specifically, we utilize the center coordinate $m_c$ and the duration $m_\sigma$ of the moments to design the queries.
Similarly to the previous way in the image domain, we pool the features around the center coordinate and modulate the cross-attention map with the moment duration. Then, the coordinates and durations are layer-wisely revised.



\paragraph{Loss Functions.}
Training objectives for QD-DETR include loss functions for MR/HD, respectively.
% The training objectives compose of the parts for highlight detection~(estimating saliency score) and moment retrieval~(localizing the moment).
First, objective functions for MR, in which the key focus is to locate the desired moments, are adopted from the baseline~\cite{momentdetr}.
% For moment retrieval, we follow the objective functions of Moment-DETR.
Moment retrieval loss $L_{\text{mr}}$ measures the discrepancy between the GT moment and the predicted counterpart.
It consists of a $L1$ loss and a generalized IoU loss $L_{\text{gIoU}}(\cdot)$ from previous work~\cite{rezatofighi2019generalized} with minor modification to localize temporal moments.
Additionally, the cross-entropy loss is used to classify the predicted moments as  $\hat{y}$ either to foreground and background by $L_{\text{CE}} = - \sum_{y \in Y}^{} y \log ( \hat{y} )$ where $\{\text{fg}, \text{bg}\} \subset Y$.
Thus, $L_{\text{mr}}$ is defined as follows:
\begin{eqnarray}
    \label{eqn moment localization loss}
    &L_{\text{mr}} = \lambda_{L1} ||m - \hat{m}|| + \lambda_{\text{gIoU}} L_{\text{gIoU}}(m, \hat{m}) + \lambda_{\text{CE}} L_{\text{CE}},
    % &L_{\text{CE}} = - \sum_{y \in Y}^{} y \log ( \hat{y} )
\end{eqnarray}
where $m$ and $\hat{m}$ are ground-truth moment and its correspond prediction containing center coordinate $m_c$ and duration $m_\sigma$. 
Also, $\lambda_{*}$ are hyperparameters for balancing the losses.
% Also, $\lambda_{L1, \text{IoU}, \text{Ce}}$ are hyperparameters for balancing the losses.
% where $m$ and $\hat{m}$ are ground-truth moment and its correspond prediction, and $\lambda_{L1}$ and $\lambda_{\text{IoU}}$ are hyperparameters for balancing the two loss terms.




Loss functions for HD are to estimate the saliency score.
It comprises two components; margin ranking loss $L_{\text{margin}}$ and rank-aware contrastive loss $L_{\text{cont}}$. 
Following \cite{momentdetr}, the margin rank loss operates with two pairs of high-rank and low-rank clips.
To be specific, the high-rank clips are ensured to retain higher saliency scores than both the low-rank clips within the GT moment and the negative clips outside the GT moment.
In short, $L_{\text{margin}}$ is defined as: 
\begin{eqnarray}
    \label{eqn margin ranking loss}
    L_{\text{margin}} = \text{max}(0, \Delta + S(x_{}^{\text{low}}) - S(x_{}^{\text{high}})) 
\end{eqnarray}
where $\Delta$ is the margin, $S(\cdot)$ is the saliency score estimator, and $x^{\text{high}}$ and $x_{}^{\text{low}}$ are video tokens from two pairs of high and low-rank clips, respectively.
% \SE{} % x_pos x_neg
In addition to margin loss which only indirectly guides the saliency predictor, we employ rank-aware contrastive loss~\cite{hoffmann2022ranking} to learn the precisely segmented saliency levels with the contrastive loss.
Given the maximum rank value $R$, each clip in the mini-batch has a saliency score lower than $R$.
% Given the maximum rank value $R$, let us assume that we have a set of ranked samples $\{x_{r_1}, x_{r_2}, ..., x_{r_L}\} \subset X$ where $0 \le \{r_1, r_2, ..., r_L\} < R$.
Then, we iterate the batch for $R$ times, each time utilizing the samples with higher saliency scores than the iteration index~($r \in \{0, 1, ..., R-1\}$) to build the positive set $X^{\text{pos}}_r$.
Samples with a lower rank than the iteration index are included in the negative set $X^{\text{neg}}_r$.
Then, the rank-aware contrastive loss $L_{\text{cont}}$ is defined as: 
% \begin{eqnarray} % 한 줄 
%     \label{eqn contrastive loss}
%     L_{\text{cont}}=-\sum_{r=1}^{R}\text{log}\frac{
%     \sum\limits_{x\in X^\text{pos}}\text{exp}(S(x)/\tau)}
%     {
%     \sum\limits_{x\in X^\text{pos}}\text{exp}(S(x)/\tau)+
%     \sum\limits_{x\in X^\text{neg}}\text{exp}(S(x)/\tau)
%     }
% \end{eqnarray}
\begin{eqnarray} % 한 줄 
    \label{eqn contrastive loss}
    L_{\text{cont}}=-\sum_{r=1}^{R}\text{log}\frac{
    \sum_{x\in X_r^\text{pos}}\text{exp}(S(x)/\tau)}
    {
    \sum_{x\in (X_r^\text{pos} \cup X_r^\text{neg})}\text{exp}(S(x)/\tau)
    }
\end{eqnarray}
% \begin{eqnarray} % 두 줄
%     \label{eqn contrastive loss}
%     &L_{\text{contr}} = \sum_{r=1}^{R} \textit{l}_r, \\
%     &\textit{l}_r = - \text{log} \frac{
%     \sum_{i=1}^R \text{exp}(S(x_{i > r})/\tau)}
%     {
%     \sum_{i=1}^R \text{exp}(S(x_{i > r})/\tau) + 
%     \sum_{i=1}^R \text{exp}(S(x_{i \leq r})/\tau)
%     }
% \end{eqnarray}
where $\tau$ is a temperature scaling parameter.
Note that, $X^{\text{neg}}_r$ also include all clips in negative pairs $x_v^{\text{neg}}$ defined in Sec.~\ref{Sec.VideoDiscriminativeQueryInformation}.
Finally with margin loss and rank-aware contrastive loss, $L_{\text{hl}}$ and total loss function $L_{\text{total}}$ are defined as follows:
\begin{eqnarray}
    \label{eqn moment localization loss}
    &L_{\text{hl}} = \lambda_{\text{margin}} L_{\text{margin}} + \lambda_{\text{cont}} L_{\text{cont}}, \\
    &L_{\text{total}} = L_{\text{hl}} 
    + L_{\text{mr}} + \lambda_{\text{neg}} L_{\text{neg}}.
\end{eqnarray}

% Finally, aggregating the all aforementioned loss terms containing $L_{\text{neg}}$ can be expressed as:
% Finally, aggregating the aforementioned loss terms containing $L_{\text{neg}}$, the total loss function is defined as follows:
% \begin{eqnarray}
%     \label{eqn total loss}
%     % L_{\text{CE}} = - \sum_{}^{} y \log ( \hat{y} )
%         L_{\text{total}} = L_{\text{hl}} 
%     + L_{\text{mr}} + \lambda_{\text{neg}} L_{\text{neg}}
% \end{eqnarray}


% (margin, contra, neg = 1, span 10, giou = 1, class=4).

% $L_{enc} = L_{saliency} + L_{neg}$ 

% $L_{saliency}$ = rank contrastive + margin

% $L_{neg}$ = negative pair BCE

% $L_{moment}$ = L1 + IoU loss

% $L_{CE}$ = background / foreground classification
