\begin{abstract}
Recently, video moment retrieval and highlight detection~(MR/HD) are being spotlighted as the demand for video understanding is drastically increased.
The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query.
Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query.
For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency.
To tackle this issue, we introduce Query-Dependent DETR~(QD-DETR), a detection transformer tailored for MR/HD.
As we observe the insignificant role of a given query in transformer architectures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video representation.
Then, to enhance the model's capability of exploiting the query information, we manipulate the video-query pairs to produce irrelevant pairs. 
Such negative~(irrelevant) video-query pairs are trained to yield low saliency scores, which in turn, encourages the model to estimate precise accordance between query-video pairs.
Lastly, we present an input-adaptive saliency predictor which adaptively defines the criterion of saliency scores for the given video-query pairs.
Our extensive studies verify the importance of building the query-dependent representation for MR/HD.
Specifically, QD-DETR outperforms state-of-the-art methods on QVHighlights, TVSum, and Charades-STA datasets. 
Codes are available at \href{github.com/wjun0830/QD-DETR}{github.com/wjun0830/QD-DETR}.
\blfootnote{
$^*$ Equal contribution} 
\blfootnote{
$^\star$ Corresponding author
}
\end{abstract}



%  Sangeek's backup

% % One of these trials leads the task moment retrieval and highlight detection~(MR/HD), which localize the moment and estimate its accordance level to the given text query called salinecy.
% % For these tasks, Although the text query has a significant role to determine the moment and its saliency, the current design does not fully utilize the information of given query.
% % To handle these tasks, although the text query has a significant role to determine the moment and its saliency, the current design does not fully utilize the information of given query.
% Recently, video understanding arises as the demanded research topic in the computer vision community.
% One of the interesting topics is video moment retrieval and highlight detection~(MR/HD), which localizes the moment and estimates its accordance level to the given text query called saliency.
% However, despite recent advances, we found that the recent techniques do not fully utilize the information of a given query.
% \SU{} % 아래에 `relationship of text queries` 이 부분을 `relationship between text queries and qeuivalent videos` 라고 하는건 어떨까요? 비디오와 텍스트 쿼리의 관계를 무시한다가 말하고자 하는바 이지 않을까 해서요.
% % --> SE: relationshipe of text queirs 는 조금 이상한 것 같아 relationship towards text 로 바꿨습니다
% For example, the relationships between text queries and videos are sometimes neglected when predicting the moment and its saliency.
% To tackle this issue, we introduce Query-Dependent DETR~(QD-DETR), a tailored detection transformer for MR and HD.
% First, our encoding modules start with cross-attention layers to explicitly integrate the context of text queries into clip-wise video representation.
% % Then, we enhance the capability to spot query-relevant video clips through exploiting negative video-query pairs, so that saliency score reflects inter-relationship of those pairs.
% % Then, we exploit the negative video-query pairs to enhance the capability to spot query-relevant video clips, by considering inter-relationship of video and query.
% % Then, we consider the negative relationship between video and query, which previous methods disregard, enhancing the capability to spot query-relevant video clips.
% Then, we consider the negative relationship between video and query, which previous methods disregard, enhancing the capability of highlighting the contextually similar query and video clip pairs.
% % Lastly, we introduce the saliency token that provides a video-query pair-dependent saliency predictor to respect the diverse nature of videos and text queries.
% % Lastly, we introduce saliency token in transformer encoder as the a video-query dependent saliency predictor to adaptively estimate the matching score of video-query pairs.
% % Lastly, we introduce a saliency token as the video-query dependent saliency predictor to adaptively estimate the matching score by given video-query pairs.
% Lastly, we introduce a video-query dependent saliency predictor which adaptively defines the decision boundary of saliency scores by the given video-quer pairs.
% \SU{} % 34번째 줄의 마지막인 to respect the diverse nature of videos and text queries가 좀 뜬금없는 느낌이 있어요
% % --> SE: 문장 변경
% % Our extensive studies verify the importance of highlighting the role of text query in moment retrieval and highlight detection.
% Our extensive studies verify the importance of building the query-dependent representation in moment retrieval and highlight detection.
% \SU{} % 36: highlighting the role of text query or 'empowering the role of text query, highlight이라는말이 좀 자주 쓰이는 경향이 있는것 같아서영.
% % --> SE: 문장 수정
% Specifically, QD-DETR outperforms state-of-the-art approaches on QVHighlights, TVSum, and Charades-STA datasets. 

% \end{abstract}





% Therefore, we consider the inter video-query relationship by suppressing the matching score of negative video-text pair.

% Then, we consider the inter video-query relationship by suppressing the matching score of negative video-text pair.
% Then, we consider the inter video-query relationship to learn video-discriminative query information.
% Then, we enhance the discriminative capability by considering inter video-query relationship
% Then, we force the saliency estimation to be discriminative by penalizing the negative video-query pair for enhancing inter relationship.

% 한 비디오 내의 클립들은 비슷한 정보를 가지고 있는 경우가 많다. --> saliency score를 intra sample들만 가지고 학습하는 것은 매우 비효율 적이다. --> 따라서 우리는 inter video text query relationship을 고려한다.
% 또한 text query는 그 특성상 매우 다양하기 때문에
% 1. cross attention을 통해 explicitly integrate 한다.
% 2. inter video-query relationship --> why? text 당 moment sample 수를 늘리기 위해?
% Query discriminative feature
% 2. lack of capability to spot query-relevance
% Then, we enhance the capability to spot query-relevant video clips through exploiting negative video-query pair, so that saliency score reflects inter-relationship of those pairs.

% Query relevance
% 기존의 intra query-relevance
% Then, we extend the saliency estiamtion 


% Although the text query has a significant role to determine the moment and its highlight-ness, the current design restricts the contribution of query on the MR and HD.
% Although the text query determines the moment and its highlight-ness, the current design overlooks the role of the text query.
% Although the text query has significant role to determine the moment and its highlight-ness, the current design lessly exploit the query information on the
% Particularly, we argue that the text query is not sufficiently included in clip-wise video representation, and relationship between text and the video .
% 사용을 덜 한다. 텍스트 쿼리가 
% 

% Still, the emphasis on text queries also makes the model vulnerable to the quality of given queries. --> 실험적 검증등이 없음. vulnerable이 맞는 표현인가?







% ================================== 11/07 backup ===================================
% 원준 원본
% Recently, video suppliers and researchers are putting much effort into retrieving the desired moments in the video since the property of context-rich makes the video time-consuming.
% % \SE{} % 교수님 의견에 since 이하 문장 내용을 이해못하겠다 하신게 있는데, 만약 사람들이 이해하기 힘들어한다면 time-consuming 부분을 빼도 큰 상관 없겠다는 생각이 듭니다.
% % % For such tasks, the query plays an important role as the query directly represents the demand. 
% % \SE{} % MR task 자체에 대한 설명이 먼저 필요한거 같아요
% Despite the significant improvements thanks to the transformer architectures, the current design overlooks the role of the text query.
% Particularly, we argue that the text query is not sufficiently included in clip-wise video representation, and task-specific noise words are not taken into account.
% % By examining how the queries were taken into the video representation, in this paper, we are highly motivated to ensure that meaningful portions of natural language queries are conditioned on video input.
% To tackle these issues, we introduce Query-Dependent DETR~(QD-DETR), a tailored detection transformer for moment retrieval in both the architectural view and training scheme.
% First, our encoding layers start with cross-attention layers to explicitly integrate the context of text queries into clip-wise representation.
% Still, the emphasis on text queries also makes the model vulnerable to the quality of given queries.
% Therefore, we discourage the impact of the task-specific noise words in the text by suppressing the matching score to other video clips.
% % Still, as emphasizing the text queries also requires the quality of text representation, we form the negative video-query pairs to learn the video-relevant clues.
% Lastly, we introduce the saliency token that provides a video-query pair-dependent saliency predictor to respect the diversity of videos and queries.
% Our extensive studies verify the importance of highlighting the role of text query in moment retrieval and highlight detection.
% Specifically, QD-DETR outperforms state-of-the-art approaches on QVHighlights, TVSum, and Charades-STA datasets. 

% % \SE{} %제 입맛대로 좀 바꿔봤습니다. 앞 부분에 issue들은 좀 더 general 해야 보기가 편할거 같아서 text query가 잘 반영 안될때가 있다 정도만 썼고, noise text는 좀 이해가 힘든거 같아서 method 쪽 처럼 discriminative 해진다라고 써봤어요. 이 부분은 좀 더 깊게 생각해볼 여지가 있는거 같긴하네요.
% % Recently, video suppliers and researchers are putting much effort into retrieving the desired moments in the video.
% One of these trials leads the task moment retrieval and highlight detection~(MR/HD), which localize the moment and estimate its accordance level to the given text query called salinecy.
% % For these tasks, Although the text query has a significant role to determine the moment and its saliency, the current design does not fully utilize the information of given query.
% % To handle these tasks, although the text query has a significant role to determine the moment and its saliency, the current design does not fully utilize the information of given query.
% To handle these tasks, the text query has a significant role to determine the moment and its saliency.
% However, the current design does not fully utilize the information of given query.
% \SU{} % 아래에 `relationship of text queries` 이 부분을 `relationship between text queries and qeuivalent videos` 라고 하는건 어떨까요? 비디오와 텍스트 쿼리의 관계를 무시한다가 말하고자 하는바 이지 않을까 해서요.
% For example, relationship of text queries are sometimes neglected when predicting the moment and its saliency.
% To tackle this issue, we introduce Query-Dependent DETR~(QD-DETR), a tailored detection transformer for MR and HD.
% First, our encoding layers start with cross-attention layers to explicitly integrate the context of text queries into clip-wise representation.
% % Then, we encourage the saliency estimation to be discriminative by penalizing the negative video-query pairs to reflect inter-relationship of those pairs.
% % Then, we encourage the saliency estimation to be discriminative by penalizing the negative video-query pairs, so that saliency score reflects inter-relationship of those pairs.
% % Then, we encourage the saliency score to reflect inter-relationship between video and text query, by penalizing the negative video-query pairs.
% Then, we enhance the capability to spot query-relevant video clips through exploiting negative video-query pairs, so that saliency score reflects inter-relationship of those pairs.
% Lastly, we introduce the saliency token that provides a video-query pair-dependent saliency predictor to respect the diverse nature of videos and text queries.
% \SU{} % 34번째 줄의 마지막인 to respect the diverse nature of videos and text queries가 좀 뜬금없는 느낌이 있어요
% Our extensive studies verify the importance of highlighting the role of text query in moment retrieval and highlight detection.
% \SU{} % 36: highlighting the role of text query or 'empowering the role of text query, highlight이라는말이 좀 자주 쓰이는 경향이 있는것 같아서영.
% Specifically, QD-DETR outperforms state-of-the-art approaches on QVHighlights, TVSum, and Charades-STA datasets. 
