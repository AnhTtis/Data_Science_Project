% \begin{figure}[t]
%     % \begin{subfigure}{1\linewidth}
%     %   \centering
%     % %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn.pdf}  
%     % %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn_v2.pdf}  
%     %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn_v5.pdf}  
%     %   \vspace{-0.5cm}
%     %     \caption{Amount of attention added to each video clip from the source video and query text in the self-attention layers of Moment-DETR encoder.}
%     %     % \caption{Distribution of attention for source and query in Moment-DETR encoder}
%     %     % Visualization of video clip's self-attention score in Moment-DETR encoder.
%     %   \label{fig:fig1_text_attn_ex}
%     % \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \vspace{0.2cm}
%     % \begin{subfigure}{1\linewidth}
%       \centering
%     %   \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn.pdf}  
%       \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_v3.pdf}  
%       \vspace{-0.4cm}
%     %   \caption{Correspondence of saliency scores on the relevance between video clips and the text query.}
%     % \caption{Predicted saliency scores against the video relevant positive query and video irrelevant negative query}
%       \label{fig:fig1_neg_attn_ex}
%     % \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \caption{
%     % 원준 원본
%     % (a) Comparison between attention scores of source and query for each video clip~(We sum the attention scores from video and text). 
%     % We observe that the attention scores are dominated by other clips in the source video. 
%     % Text queries do not account for much attention regardless of the relevance to the video clips.
%     % \textbf{(a)} Inspection of the query dependency in Moment-DETR encoder.
%     % % We visualize the attention score of video tokens in the transformer encoder and observe that text query accounts for only a low portion of attention.
%     % % This tendency occurs regardless of the relevance between the text query and video clips. 
%     % We visualize the attention score of video tokens in the transformer encoder and observe 1) text query only accounts for a low portion of attention, and 2) relevance between video-query pair does not affect the attention scores ratio of text.
%     \textbf{(b)} Comparison of highlight-ness when relevant and non-relevant queries are input.
%     As observed in , existing work only uses queries to play an insignificant role, thereby may not be capable of detecting false queries and considering the video-query relevance even when the problem in (a) is resolved. 
%     % \SE{} % 이 부분이 "not capable of" 란 용어가 세다는 피드백이 있는 듯 합니다. 이러한 능력이 없다는 것은 굉장히 강한 어조인거 같기는 하고, 이러한 경우들이 종종 있다거나 좀 약화시킬 필요가 있어보이긴 하네요.
%     On the other hand, our QD-DETR yields a query-dependent representation that the relevance between the source video and query text is updated in the saliency scores.
%     There is a large gap between positive and negative saliency scores, and scores are consistent since the clips are all highly correlated to others.
%     }
%     \label{fig:motivation_ex}
%     % \captionsetup{belowskip=13pt}
%     % \setlength{\belowcaptionskip}{-10pt}
% \end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_1111.pdf}
    % \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_1109.pdf}
    % \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_stat.pdf}
    \vspace{-0.6cm}
    \caption{
        % \SE{} % 수정 필요
        Comparison of highlight-ness~(saliency score) when relevant and non-relevant queries are given.
        We found that the existing work only uses queries to play an insignificant role, thereby may not be capable of detecting negative queries and video-query relevance; saliency scores for clips in ground-truth~(GT) moments are low and equivalent for positive and negative queries.
        % This also results in mispredicted moments when ground-truth~(GT) moment is dominated by clips unrelated to GT since their prediction is highly focused on the video.
        % \SE{} % 여기 한번 더 보면 좋을 듯 합니다. GT moment에 unrelated한 clip이 많으면? label이 틀렷을 경우를 말씀하시는건지?
        % As observed in saliency graph, existing work only uses queries to play an insignificant role, thereby may not be capable of detecting false queries and considering the video-query relevance.
        On the other hand, query-dependent representations of QD-DETR result in corresponding saliency scores to the video-query relevance and precisely localized moments.
        % On the other hand, our QD-DETR yields a query-dependent representation that the
        % saliency scores are in accordance with the relevance between the video and query.
        % text is in accordance with the saliency scores.
        % There is a large gap between positive and negative saliency scores, and scores are consistent since the clips are all highly correlated to others.
}
    \label{fig:motivation_ex}
\end{figure}


\section{Introduction}
% 원준 원본
% Along with the advance of digital devices and platforms, video is now one of the most desired data type for consumers. However, although the large information capacity of videos may be beneficial in many aspects, e.g., informative and entertaining, on the contrary perspective, videos are time-consuming, and hard to search for desirable moments. 
% This has led many creators to use extra manpower to crop and edit the video to generate highlight clips to gain the consumer’s attention.
Along with the advance of digital devices and platforms, video is now one of the most desired data types for consumers~\cite{apostolidis2021video,wu2017deep}.
% SE: Video aware deep learning application & survey papers?
Although the large information capacity of videos might be beneficial in many aspects, e.g., informative and entertaining, inspecting the videos is time-consuming, so that it is hard to capture the desired moments~\cite{anne2017localizing,apostolidis2021video}. 
% This has led many creators to use extra manpower to crop and edit the video to generate highlight clips to gain the consumer’s attention.


% On the other side, 
Indeed, the need to retrieve user-requested or highlight moments within videos is greatly raised.
Numerous research efforts were put into the search for the requested moments in the video~\cite{anne2017localizing, gao2017tall, liu2015multi, escorcia2019temporal} and summarizing the video highlights~\cite{zhang2016video, mahasseni2017unsupervised, badamdorj2022contrastive, wei2022learning}.
% Numerous research efforts were put into the search for the requested moments in the video~\cite{anne2017localizing, gao2017tall, liu2015multi, escorcia2019temporal}, summarizing the video to generate highlights was another popular topic~\cite{zhang2016video, mahasseni2017unsupervised, badamdorj2022contrastive, wei2022learning}.
Recently, Moment-DETR~\cite{momentdetr} further spotlighted the topic by proposing a QVHighlights dataset that enables the model to perform both tasks, retrieving the moments with their highlight-ness, simultaneously.

% 원준 원본
% To detect the desired moments, previous works employed transformer encoder-decoder architectural designs to fuse the text query into the video representations. Moment-DETR~\cite{mDETR} modified detection transformer to process capture the moment as a set, and UMT~\cite{umt} implemented transformer decoder as to output clip-wise saliency. 
% Yet to their outstanding breakthroughs in the literature of moment retrieval with the seminal architectures, their limitation is that the role of the given text query is insignificant in representing the query-conditioned video representation; the attention mechanism of moment DETR is not explicitly conditioned on the text query, and the text query is conditioned on multi-modal clips where the differences between the clips are smoothed after encoding process in UMT.



% \begin{figure}[t]
% \centering
%     \begin{subfigure}[l]{0.37\linewidth}
%       \centering
%       \vspace{0.20cm}
%     %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn.pdf}  
%     %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn_v2.pdf}  
%       \includegraphics[width=1\linewidth]{figs/fig1_moti_violin_a.pdf}  
%       \vspace{-0.60cm}
%     %   \caption{text attention}
%         \caption{Importance of queries in video representation}
%       \label{fig:fig1_text_attn}
%     \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \vspace{0.2cm}
%     \begin{subfigure}[r]{0.61\linewidth}
%       \centering
%     %   \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn.pdf}  
%       \includegraphics[width=1\linewidth]{figs/fig1_moti_violin_b.pdf}  
%     %   \caption{neg attention}
%         % \caption{Relation between the highlight-ness and the relevance between videos and query texts.}
%         \caption{Highlight-ness~(saliency) histogram of positive and negative video-query pairs\SE{}}
%       \label{fig:fig1_neg_attn}
%     \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     % \vspace{-0.2cm}
%     \caption{Overall statistics for attention scores in Fig.~\ref{fig:motivation_ex} in QVHighlights dataset. 
%     (a) For the attention scores that measure how much the text query is generally involved in video representation, we use violin plots to show the probability density. We plot the score for each layer in the encoder.
%     % (b) Using the histogram, we compare how the baseline and QD-DETR yield different salient scores given the positive and negative video-text pairs.
%     (b) Saliency histogram shows the distributional gap between positive and negative video-text query pairs of baseline~(Moment-DETR) and proposed QD-DETR.\SE{}
%     }
%     \label{fig:motivation}
%     % \captionsetup{belowskip=13pt}
%     % \setlength{\belowcaptionskip}{-10pt}
% \end{figure}

% \begin{figure}[t]
% \centering

%     \begin{subfigure}[r]{1\linewidth}
%       \centering
%       \hspace{-0.2cm}
%     %   \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn.pdf}  
%       \includegraphics[width=1.1\linewidth]{figs/fig1_moti_violin_a_v2.pdf}  
%     %   \caption{neg attention}
%         % \caption{Relation between the highlight-ness and the relevance between videos and query texts.}
%         \vspace{-0.5cm}
%         % \caption{Saliency histogram of positive and negative video-query pairs}
%         \caption{We plot the histograms and its average value~(dotted line) to compare saliency scores when true and false text queries are given for each method. (left) Since the video representations do not include much textual information, both the true and false queries yield similar saliency scores. (Middle) Even when the video representation is enforced to be updated with the textual information, the issue is not much resolved. (Right) By extracting discriminative features in the text query, distributions are differentiated.
%         % \SE{} % R1@0.5 설명
%         Also, R1@0.5 indicates evaluation metric, Recall at 1 with IoU 0.5 threshold on QVhighlight \textit{val} set.
%         }
%       \label{fig:fig1_neg_attn}
%     \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \\
%     \begin{tabular}{cc}
%     \hspace{-0.2cm}
%         \begin{minipage}{.4\linewidth}
%             \begin{subfigure}[l]{1\linewidth}
%               \centering
%             %   \vspace{0.20cm}
%             %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn.pdf}  
%             %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn_v2.pdf}  
%               \includegraphics[width=1\linewidth]{figs/fig1_moti_violin_a.pdf}  
%               \vspace{-0.60cm}
%             %   \caption{text attention}
%                 \caption{Importance of queries in video representation}
%               \label{fig:fig1_text_attn}
%             \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%         \end{minipage}
        
%         \begin{minipage}{.6\linewidth}
%             \vspace{-0.2cm}
%             \caption{Overall statistics of Fig.~\ref{fig:motivation_ex} in QVHighlights dataset. 
%             (a) Saliency histogram shows the distributional gap between positive and negative video-text query pairs.
%             % (a) For the attention scores that measure how much the text query is generally involved in video representation, we use violin plots to show the probability density. We plot the score for each layer in the encoder.
%             % (b) Using the histogram, we compare how the baseline and QD-DETR yield different salient scores given the positive and negative video-text pairs.
%             % (b) Text ratio in self-attention layer to  of Moment-DETR
%             % (b) Ratio of text when representing video tokens in self-attention of Moment-DETR.
%             % (b) Magnitude of attention text query involved.
%             % (b) Attention score of video tokens
%             % (b) Magnitude of text query to refine the video tokens in self-attention layer of Moment-DETR.
%             (b) Probability density depicting the weight of the text query in attention score for video clips. Scores are from the self-attention layers in Moment-DETR encoder.
%             % (b) The text query ratio in attention score of video clips (Self-attention layer in Moment-DETR encoder). We use violin plots to show probability density.
%             % 텍스트 쿼리가, 비디오 피쳐에 얼만큼 attend 하는지
%             }
%         \end{minipage}
    
%     \end{tabular}
%     \vspace{-0.5cm}
%     \label{fig:moti}
%     % \captionsetup{belowskip=13pt}
%     % \setlength{\belowcaptionskip}{-10pt}
% \end{figure}


% \begin{figure}
%     \centering
%     % \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_1109.pdf}
%     \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_stat_v2.pdf}
%     \vspace{-0.8cm}
%     \caption{
%         Histogram of saliency when the positive and negative queries are given. We plot the histograms and its average value~(dotted line) to compare saliency scores when relevant~(positive) and irrelevant~(negative) text queries are given for each method. (Left) Since the video representations do not properly reflect textual information, both the positive and negative queries yield similar saliency scores. 
%         % (Middle) Even when the video representation is enforced to be updated with the textual information, the issue is not much resolved. 
%         (Right) By representing video clips in query-dependent manner, distributions are differentiated.
%     }
%     \vspace{-0.6cm}
%     \label{fig:motivation}
% \end{figure}


% One of the demanding task is moment retrieval task, which is detecting the desired moments from the given query, typically the text query.
When describing the moment, one of the most favored types of query is the natural language sentence~(text)\cite{anne2017localizing}. 
While early methods utilized convolution networks~\cite{zhang2020learning, gao2021fast, wang2020temporally}, recent approaches have shown that deploying the attention mechanism of transformer architecture is more effective to fuse the text query into the video representation.
% To handle these modalities, previous works simply employed the attention mechanism of transformer architecture to fuse the text query into the video representation.
For example, Moment-DETR~\cite{momentdetr} introduced the transformer architecture which processes both text and video tokens as input by modifying the detection transformer~(DETR), and UMT~\cite{umt} proposed transformer architectures to take multi-modal sources, e.g., video and audio. 
Also, they utilized the text queries in the transformer decoder.
Although they brought breakthroughs in the field of MR/HD with seminal architectures, they overlooked the role of the text query.
To validate our claim, we investigate the Moment-DETR~\cite{momentdetr} in terms of the impact of text query in MR/HD~(Fig.\ref{fig:motivation_ex}).
Given the video clips with a relevant positive query and an irrelevant negative query, we observe that the baseline often neglects the given text query when estimating the query-relevance scores, i.e., saliency scores, for each video clip.
% the output saliency score, i.e. query-relevance scores.
% Based on the observation, we traced the actual saliency prediction of the model against both the video-relevant query and the irrelevant dummy one where we find that the baseline often neglects the given text query when estimating the query-relevance scores of video clips.
% For example, in Fig.~\ref{fig:motivation_ex}, saliency scores are not affected even when the query is substituted with the dummy.
% % General statistics for Fig.~\ref{fig:motivation_ex} is shown in Fig.~\ref{fig:motivation}. 
% General statistics corresponding to Fig.~\ref{fig:motivation_ex} are also shown in Fig.~\ref{fig:motivation}.



% The limitation of the concrete baseline~\cite{momentdetr} is inspected in two different aspects; 1) Utilization of text-query in the encoding process and 2) the output saliency score, i.e. query-relevance scores.
% Firstly, we visualize the attention score when video clips are given as a query in self-attention. 
% We observe that the text queries have relatively small impacts compared to other video features, as shown in Fig.~\ref{fig:fig1_text_attn_ex}.
% That is, the text does not account for much in representing every video clip, although the goal of MR/HD is to detect query-relevant moments.
% Based on the observation, we traced the actual saliency prediction of the model against both the video-relevant query and the irrelevant dummy one where we find that the baseline often neglects the given text query when estimating the query-relevance scores of video clips.
% For example, in Fig.~\ref{fig:motivation_ex}, saliency scores are not affected even when the query is substituted with the dummy.
% % General statistics for Fig.~\ref{fig:motivation_ex} is shown in Fig.~\ref{fig:motivation}. 
% General statistics are also shown in Fig.~\ref{fig:motivation}.

% Consequently, in Fig.~\ref{fig:fig1_neg_attn_ex}~(b), we found that the baseline often neglects the given text query when estimating the query-relevance scores of video clips; 
% For example, 


% We validate the previous work sometimes neglects the given query when estimating the saliency of video clips.
% For example, there is an example that the saliency scores from positive and negative queries cannot be distinguishable, as shown in Fig.~\ref{fig:fig1_neg_attn_ex}.
% % 우리는 추가로 text attention을 추가도 해봤지만, 효과가 있긴 했으나, still 이슈가 있는 것을 확인하였다?
% % Still, we observe that assuring the high attendance of text queries does not resolve the overlap which motivates us to question the quality of the naive use of task-agnostic text representation~\cite{momentdetr, umt}.
% We found that introducing the text-attention for ensuring the high attendance of text queries relieve the overlap, but there still be a severe overlap.


% To validate their limitations, we inspect the impacts of text queries in the concrete baseline~\cite{momentdetr} with the two different aspects, 1) tendency of attention in self-attention layer and 2) saliency score, i.e. query-relevance scores. \SE{} % attention 이 갑자기 등장하는가?
% Firstly, we visualize the attention score when video clips are given as a query in self-attention. We observe the text queries have relatively low attention scores compared to the video features, as shown in Fig.~\ref{fig:fig1_text_attn_ex}.
% That is, the text does not account for much in representing every video clip, although the goal of MR/HD is to detect query-relevant moments.
% Based on this observation, we trace the actual saliency prediction of the model against both positive and negative text queries.
% We validate the previous work sometimes neglects the given query when estimating the saliency of video clips.
% For example, there is an example that the saliency scores from positive and negative queries cannot be distinguishable, as shown in Fig.~\ref{fig:fig1_neg_attn_ex}.
% % 우리는 추가로 text attention을 추가도 해봤지만, 효과가 있긴 했으나, still 이슈가 있는 것을 확인하였다?
% % Still, we observe that assuring the high attendance of text queries does not resolve the overlap which motivates us to question the quality of the naive use of task-agnostic text representation~\cite{momentdetr, umt}.
% We found that introducing the text-attention for ensuring the high attendance of text queries relieve the overlap, but there still be a severe overlap.



% Thus, we 
% query dependency를 높이기 위해 
% Cross-attention? text-attention? detailed explanation on text-attention should be needed?
% By handling these two issues, we find that more precise retrieval can be achieved.
% 
% 
%
% By projecting video-discriminative text features with high text attendance to source video, we f 
% We also find the need to improve the quality of query features since assuring high text attendance also results in...
% pairs are not finetuned to be discriminative that even the similarity within the pairs does not reflect the relevance between the query and the video clips.
% General statistics for Fig.~\ref{fig:motivation_ex} is shown in Fig.~\ref{fig:motivation}. 
% \SE{} % 이거 ??로 뜨는데, 위처럼 figure 그리면 label이 안되는걸까요
% \SE{}
% 형님 아래 사항 생각 좀 해보는게 좋을 거 같아요.
% fig 1. (a) 그림만 봤을 때 모든 clip에 대해 text attention이 일정이상 존재하긴 하니까, 뭔가 not assured to be conditioned가 와닿지 않는거 같아요.
% + 왜 text가 항상 attend 해야하나?
% not assured to be conditioned --> text shows relatively low affects compared to video 같이 실제 나타난 현상까지 같이 적으면 어떨까 싶어요.
% fig 1. (b) 덜 반영한다?

% \SU{}
% 일단 text가 attend 잘 되어야 한다는 것에 좀 궁금점이 생깁니다. 결국에는 text와 관련있는 frame들을 attend해서 higlight를 찾아야 하는게 아닐까요? 그리고, 현제 저희의 모델 구조상 text query가 Key와 Value로 거의 활용되고 있는데 그렇다면 결국에는 해당 모델은 text에 대한 attention이 전혀 없다고 봐도 무방하지 않을까요? 그런 면에서 text attention을 강조하는게 좀 걸리긴 합니다.

% Specifically, the text query is not assured to be explicitly conditioned on every clip of the video, and as the query texts are evenly treated, discriminative keywords may not be spotlighted.
% attention mechanism of Moment-DETR is not explicitly conditioned on the text query as shown in Fig~\ref{}(d), and in UMT, the text are only used for conditioning the queries while the video representation are refined itself by self-attention.

% \begin{figure}[t]
%     \begin{subfigure}{1\linewidth}
%       \centering
%     %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn.pdf}  
%     %   \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn_v2.pdf}  
%       \includegraphics[width=1\linewidth]{figs/fig_1_moti_textattn_v4.pdf}  
%       \vspace{-0.5cm}
%     %   \caption{text attention}
%         \caption{Distribution of attention scores in Moment-DETR encoder}
%       \label{fig:fig1_text_attn}
%     \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \vspace{0.2cm}
%     \begin{subfigure}{1\linewidth}
%       \centering
%     %   \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn.pdf}  
%       \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_v2.pdf}  
%       \vspace{-0.5cm}
%     %   \caption{neg attention}
%         \caption{Saliency score against positive and negative text queries}
%       \label{fig:fig1_neg_attn}
%     \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \vspace{0.2cm}
%     \begin{subfigure}{1\linewidth}
%       \centering
%     %   \includegraphics[width=1\linewidth]{figs/fig1_moti_violin.pdf}  
%       \includegraphics[width=1\linewidth]{figs/fig1_moti_violin_v2.pdf}  
%       \vspace{-0.5cm}
%       \caption{violin}
%       \label{fig:fig1_violin}
%     \end{subfigure}%\hfill% or  or \hspace{0.3\textwidth}
%     \vspace{-0.2cm}
%     \caption{(a) 1. portion of text attention vs. video attention 2. relation with text query and content (e.g. fg, bg) of clip seems not to affect the attention score
%     (b) 1. high variability even though entire clips are highly correlated with the given text query 2. positive and negative query makes overlaps on saliency score distribution
%     (3) actual distribution on validation dataset.}
%     \label{fig:motivation}
%     % \captionsetup{belowskip=13pt}
%     % \setlength{\belowcaptionskip}{-10pt}
% \end{figure}

To this end, we propose Query-Dependent DETR~(QD-DETR) that produces query-dependent video representation.
% Our key focus is to ensure each clip in predicted moments is explicitly conditioned by the query, particularly on the video-descriptive portion of the text query.
% Our key focus is to ensure that query-relevant clips are predicted by enforcing each clip to be explicitly conditioned by the query.
%Our key focus is to ensure that the model prediction for each clip is highly relevant to the query.
Our key focus is to ensure that the model's prediction for each clip is highly dependent on the query.
% by enforcing each clip to be explicitly conditioned by the query. :)
% hmm...
% \SE {} % "query-relevant clips are predicted" 이 문장이 좀 애매한거 같습니다. relevant 클립을 놓지지 않고 찾는 것을 보장한다? 이런 느낌인지 아니면 높은 saliency 를 주는게 목적이다? model prediction이 query-relevance를 반영하는 것을 보장한다?
% Our key focus is to ensure that the model prediction reflects query-relevance of clips by enforcing each clip to be explicitly conditioned by the query.
First, to fully utilize the contextual information in the query, we revise the transformer encoder to be equipped with cross-attention layers at the very first layers.
% 상익's thought :  single video - query간의 관계만 고려 - 같은 word가 더 많이 쓰이는 것을 보고 
% 교수님's thought : neg pair 를 쓰면 쿼리를 보지 않고서는 video clip간만 고려하는 것이 사라짐. 왜냐면 0으로 내보내야 하기 때문. --> SE: relative difference 만 고려하다가, 
By inserting a video as the query and a text as the key and value of the cross-attention layers, our encoder enforces the engagement of the text query in extracting video representation.
% 원준 교수님 코멘트 반영해서 다시
Then, in order to not only inject a lot of textual information into the video feature but also make it fully exploited, we leverage the negative video-query pairs generated by mixing the original pairs.
Specifically, the model is learned to suppress the saliency scores of such  negative~(irrelevant) pairs.
Our expectation is the increased contribution of the text query in prediction since the videos will be sometimes required to yield high saliency scores and sometimes low ones depending on whether the text query is relevant or not.
% \SE{}
% learns to?
% By suppressing the saliency scores of the irrelevant video-query pairs, the model learns to spotlight only the video-specific discriminative words in the query.
% % \SE{} % ====================== 상익 수정 ========================
% However, this architectural design still lacks the capability of identifying the video-descriptive keywords in the query.
% % However, this architectural design still lacks in identifying proper query relevance.
% This is because the current training scheme only focuses on the interactions of video and clips within a single video while neglecting information shared throughout the entire video.
% % We argue the problem of the current training scheme that only focuses on distinguishing the clips in a single video while neglecting information shared throughout the entire video.
% Therefore, we leverage the negative video-query relationships to enhance the capability of identifying the contextual similarity of query and video clips.
% 
% 원준 원본 
% However, this architectural design heavily relies on the quality of the text query.
% Therefore, we leverage the negative video-query relationships to enable the model to emphasize key corresponding query features.
% By suppressing the saliency scores of the irrelevant video-query pairs, the model learns to spotlight only the video-specific discriminative words in the query.
% =========================================================
Lastly, to apply the dynamic criterion to mark highlights for each instance, we deploy a saliency token to represent the entire video and utilize it as an input-adaptive saliency criterion. 
With all components combined, our QD-DETR produces query-dependent video representation by integrating source and query modalities.
This further allows the use of positional queries~\cite{dabdetr} in the transformer decoder.
% Furthermore, we can exploit the advanced DETR decoder architectures using the positional information, e.g., DAB-DETR, since our encoded tokens consist of identical position representations from a single modality.
% \SE{} % ====================== 상익 수정 ========================
% Furthermore, we can exploit the advanced DETR decoder architectures using the positional information, e.g., DAB-DETR, since our video clip tokens consist of identical position representations from a single modality.
% 원준 원본
% It also enables the use of advanced DETR decoder architectures, e.g., DAB-DETR, for the first time, as these works exploit the position information within a single modality.
% =========================================================
Overall, our superior performances over the existing approaches validate the significance of the role of text query for MR/HD.
% Our extensive experiments on QVHighlights, TVSum, and Charades-STA datasets validate the significance of considering the role and the quality of text query.

% All components combined with dynamic anchor moments for the query of decoder, our FOQUE fosters the query-dependent video representation, thereby making the 
% All components combined, our modified transformer encoding process fosters the query-dependent video representation thereby achieving the state-of-the-art results on various benchmarks of moment-retrieval and highlight detection.
	
% -	Video Platform & Streamer & Consumer의 증가. 
% Video는 다른 데이터 타입보다 정보가 많아 유용하지만, 이는 다른 말로 해석하면 video를 보는 것은 time-consuming 하고, 원하는 것을 찾아보기에는 힘들 수 있음.
% 따라서, 많은 매체에서는 사람들의 더 많은 이목을 끌기 위해 highlight 비디오라는 것을 편집하여 공유도 함.
% 하지만, highlight video를 만들기 위해 사람의 노력이 필요한 현 시점에서, This spotlights the need to retrieve the user-requested / Highlight moments in the video.

% -	이전에도 이러한 문제를 해결하기 위해 (asdfasdf) for moment retrieval, (asdfasdf) for highlight detection 등이 제안 되었지만, 이들은 비디오의 특정 영역을 찾는다는 공통된 목적을 가지고 있으면서도, 데이터 셋의 한계로 인해 따로 연구되었음. 이를 문제 삼으며, 최근에는 두 task를 동시에 학습할 수 있는 dataset이 소개 되었는데, 컴퓨터비전에서 최근 각광을 받고 있는 Transformer 모델 도입과 함께 큰 발전을 거듭하고 있음.

% -	구체적으로, 이 두가지 task를 수행하기 위해서는 transformer를 두가지 방법으로 이용할 수 있는데, moment-DETR 처럼 moment 를 clip의 set 단위로 예측할 수 있고, UMT 처럼 clip-wise prediction을 할 수 있음. 하지만, 이들은 query를 condition이 아닌 video와 동등한 레벨로 취급하거나 [mDETR], 매 클립이 self-attention으로 mixing 된 후에 condition을 걸어주어 clip간의 차이를 확실하지 이용하지 못하였고, 또한, 확실하게 condition으로 주지 못하였고, video와 query 사이의 관계를 한정적으로만 이용하였다.

% -	따라서, we explore three different ways to fully exploit query information. First, we design one-way cross-attention layer to condition every clip with the query features. Then, we utilized the negative video-text pairs to better model the relationships between the video and the text embeddings. Lastly, we define the saliency token to be the video-query dependent saliency estimator.


















% ===================== neg pair 부분 ===========================
% Nevertheless, the current training scheme, only considering the given video-query pair, still disturbs the model from identifying proper query-relevance prediction.
% In detail, the model focus on learning the fine-grained discrepancy between video clips, while neglecting the information they share, which contains significant clues to understand the context of video.
% Therefore, we leverage the negative video-query relationships to enhance the capability of identifying the contextual similarity of query and video clips.
% Therefore, we leverage the negative video-query relationships by suppressing those pairs, so that enhance the capability of identifying the contextual similarity of query and video clips.
% We hypothsize the diversity in query-video pairs are insufficient to learn the general relationship between text query and video.
% Therefore, we leverage the negative video-query relationships by suppressing the saliency scores of the irrelevant video-query pairs.
% However, this architectural design still lacks in identifying proper query relevance.
% We argue that the current training scheme only focuses on learning the fine-grained discrepancy between clips in a single video, while neglecting the information they share, which contains significant clues to understand the context of the video.
% Therefore, we leverage the negative video-query relationships to enhance the capability of identifying the contextual similarity of query and video clips.
% However, this architectural design still lacks in identifying proper query relevance.
% We argue the problem of the current training scheme that only focuses on learning the fine-grained discrepancy between clips in a single video.
% That is, the current design neglects the information shared throughout the video, although it contains significant clues to understand the context of the video.