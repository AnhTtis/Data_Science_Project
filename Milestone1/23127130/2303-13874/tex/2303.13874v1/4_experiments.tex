\section{Evaluation}
\label{Sec.Experiments}


\begingroup
\setlength{\tabcolsep}{7pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
\begin{table*}[]
	\centering
	{\small 
	\vspace{-0.1cm}  % 
 % 그 전체테이블에 똑같은 vspace넣는게 좋지않아? 0.5 말고 0,1씩 앞부버탇해보자
	\caption{Performance comparison on QVHighlights \textit{test} split. V and A in the Src column denote video and audio, respectively, representing the modalities of the source data. 
 % Methods with 'w/ PT' indicates the inclusion of the pre-training phase with captions from automatic speech recognition. 
 Our experiments are averaged over five runs and `$\pm$' denotes the standard deviation.}
	\label{table_QVHighlight}
        \begin{tabular}{l|c|ccccccc}
        \hlineB{2.5}
        \multicolumn{1}{c}{\multirow{3}{*}{Method}} & \multirow{3}{*}{Src} & \multicolumn{5}{c}{MR}                                                             & \multicolumn{2}{c}{HD}                        \\ \cline{3-9} 
        \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & \multicolumn{2}{c}{R1}          & \multicolumn{3}{c}{mAP}                          & \multicolumn{2}{c}{\textgreater{}= Very Good} \\ \cline{3-9} 
        \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} & @0.5           & @0.7           & @0.5           & @0.75          & Avg.           & mAP                   & HIT@1                 \\ \hlineB{2.5}
        BeautyThumb~\cite{song2016click}               & V                  & -              & -              & -              & -              & -              & 14.36                 & 20.88                 \\
        DVSE~\cite{liu2015multi}                   & V                  & -              & -              & -              & -              & -              & 18.75                 & 21.79                 \\
        MCN~\cite{anne2017localizing}                          & V               & 11.41          & 2.72           & 24.94          & 8.22           & 10.67          & -                     & -                     \\
        CAL~\cite{escorcia2019temporal}                    & V            & 25.49          & 11.54          & 23.40          & 7.65           & 9.89           & -                     & -                     \\
        XML~\cite{lei2020tvr}                                & V         & 41.83          & 30.35          & 44.63          & 31.73          & 32.14          & 34.49                 & 55.25                 \\
        XML+\cite{lei2020tvr}                      & V                & 46.69          & 33.46          & 47.89          & 34.67          & 34.90          & 35.38                 & 55.06                 \\ \hline
        Moment-DETR~\cite{momentdetr}  & V& 52.89$_{\pm{2.3}}$  & 33.02$_{\pm{1.7}}$ & 54.82$_{\pm{1.7}}$ & 29.40$_{\pm{1.7}}$ & 30.73$_{\pm{1.4}}$ & 35.69$_{\pm{0.5}}$ & 55.60$_{\pm{1.6}}$ \\
        % Moment-DETR w/ PT~\cite{momentdetr} & V                          & 59.78$_{\pm{0.3}}$          & 40.33$_{\pm{0.5}}$          & 60.51$_{\pm{0.2}}$          & 35.36$_{\pm{0.4}}$          & 36.14$_{\pm{0.3}}$          & 37.43$_{\pm{0.2}}$                 & 60.17$_{\pm{2.7}}$                 \\
        QD-DETR (\textbf{Ours})                  & V                  & \textbf{62.40}$_{\pm_{1.1}}$ & \textbf{44.98}$_{\pm_{0.8}}$ & \textbf{62.52}$_{\pm_{0.6}}$ & \textbf{39.88}$_{\pm_{0.7}}$ & \textbf{39.86}$_{\pm_{0.6}}$ & \textbf{38.94}$_{\pm_{0.4}}$        & \textbf{62.40}$_{\pm_{1.4}}$    \\ \hline
        UMT~\cite{umt}             & V+A                            & 56.23          & 41.18          & 53.38          & 37.01          & 36.12          & 38.18                 & 59.99                 \\
        % UMT w/ PT~\cite{umt}            & V+A                       & 60.83          & 43.26          & 57.33          & 39.12          & 38.08          & 39.12                 & 62.39                 \\ \hline
        QD-DETR (\textbf{Ours})  & V+A & \textbf{63.06}$_{\pm_{1.0}}$ & \textbf{45.10}$_{\pm_{0.7}}$ & \textbf{63.04}$_{\pm_{0.9}}$ & \textbf{40.10}$_{\pm_{1.0}}$ & \textbf{40.19$_{\pm_{0.6}}$} & \textbf{39.04$_{\pm_{0.3}}$}        & \textbf{62.87$_{\pm_{0.6}}$}    \\ \hlineB{2.5}
        % QD-DETR (\textbf{Ours}) + PT & V & \textbf{63.18$_{\pm_{1.0}}$} & \textbf{45.19$_{\pm_{0.7}}$} & \textbf{63.37$_{\pm_{0.7}}$} & \textbf{40.35$_{\pm_{0.8}}$} & \underline{39.96}$_{\pm_{0.4}}$ & 38.52$_{\pm_{0.1}}$        & \underline{61.91}$_{\pm_{0.5}}$    \\ \hlineB{2.5}
        \end{tabular}
        % \vspace{-0.1cm}  % 
        }
\end{table*}
\endgroup








\subsection{Experimental Settings}
\paragraph{Dataset and Evaluation Metrics.}
For the evaluation, we validate the effectiveness of query-dependent source representation on QVHighlights~\cite{momentdetr}, TVSum~\cite{song2015tvsum}, Charades-STA~\cite{gao2017tall}.
\textbf{QVHighlights} is the most recently publicized dataset for both moment retrieval and highlight detection. It is also the only dataset that has annotations for both tasks.
In detail, QVHighlights consists of over 10,000 videos annotated with human-written text queries.
It provides a fair benchmark as the evaluation for the test split can only be measured through submitting the prediction to the QVHighlights server\footnote{https://codalab.lisn.upsaclay.fr/competitions/6937}.
\textbf{Charades-STA} and \textbf{TVSum} are the dataset for moment retrieval and video summarization, respectively.
Each of them contains 9,848 videos regarding indoor activities and 50 videos of various genres, e.g., news, documentary, and vlog.
For all datasets, we follow the data splits from the existing works~\cite{momentdetr, umt}.

To measure the performances, we use the same evaluation metrics used in the baselines. 
Specifically, recall@1 with IoU thresholds 0.5 and 0.7, and mean average precision~(mAP) at different thresholds.
% 이래서 쓰인다.
Similarly, we use mAP and HIT@1 for evaluating the highlight detection.
HIT@1 is computed through the hit ratio of the highest-scored clip.


% \paragraph{Training Details}
% % On QVHighlights, we basically follow the settings from Moment-DETR~\cite{momentdetr}.
% % With the video features extracted from pretrained SlowFast and Clip encoder, audio features from Q... ,and text embeddings from Clip, we train QD-DETR for 200 epochs with the learning rate of 1e-4 and weight decay of 1e-4.
% % For the architectural designs, our encoder is composed of 2 cross-attention layers and 2 self-attention layers whereas there are only 2 decoding layers.
% % With the hidden dimension of 256 for the transformers, we use the batch size to 32 and set each $\lambda_{\text{margin}}$, $\lambda_{\text{cont}}$, $\lambda_{L1}$, $\lambda_{\text{gIoU}}$, $\lambda_{\text{CE}}$ and $\lambda_{neg}$ to (margin, contra, neg = 1, span 10, giou = 1, class = 4). 
% % Lastly, our experiments are mostly conducted with a single NVIDIA TITAN RTX GPU.
% % \textcolor{red}{Training details on Charade, TVSUM not done}
% % % 여기 Loss 다 제대로 쓴다음에 람다값 나열
% % 다른 데이터 셋에 관한 것도 나열

% % We basically follow the settings from Moment-DETR~\cite{momentdetr}.
% % For architectural desings, our encoder composes of 2 cross-attention layers and 2 self-attention layers each
% % For architectural designs, both of our transformer encoders compose of 2 attention layers  whereas there are only 2 decoding layers.
% % For architectural designs, we share identical configurations for the entire dataset we tested.
% For the unified configurations across all experiments, our encoder composes of 4 layers of transformer block~(2 cross-attention layers and 2 self-attention layers) whereas there are only 2 layers in the decoder~(For HD, we only use encoding layers).
% % In detail, our encoder composes of 4 layers of transformer block~(2 layers for the cross-attention transformer and the others for the transformer encoder) whereas there are only 2 layers in the decoder.
% We set the hidden dimension of transformers as 256, and use the Adam optimizer with a weight decay of 1e-4.
% Also, the basic setting for balancing parameters is $\lambda_{\text{margin}}=1$, $\lambda_{\text{cont}}=1$, $\lambda_{L1}=10$, $\lambda_{\text{gIoU}}=1$, $\lambda_{\text{CE}}=4$ and $\lambda_{\text{neg}}=1$, unless otherwise mentioned.

% % % V1
% For other training details on QVHighlight, we use video features extracted from both pretrained SlowFast~\cite{slowfast}~(SF) and CLIP encoder~\cite{CLIP}, and text embeddings from CLIP, following the Moment-DETR.
% We train QD-DETR for 200 epochs with a batch size of 32 and a learning rate of 1e-4.
% For the Charades-STA dataset, we utilize official VGG~\cite{VGG} features with GloVe~\cite{pennington2014glove} text embedding.
% To compare with additional baselines, we also test our model on pretrained C3D~\cite{C3D} and SlowFast for video features with CLIP text embedding.
% We train ours for 100 epochs with a batch size of 8 and a learning rate of 1e-4.
% Besides, we set $\lambda_{\text{margin}}$, $\lambda_{\text{cont}}$ and $\lambda_{\text{neg}}$ as 4 for this dataset.
% Lastly, for the TVSum dataset, we use I3D~\cite{I3D} features pretrained on Kinetics-400~\cite{kay2017kinetics} as a visual one, and CLIP for the text embedding.
% We train our model for 2000 epochs with a batch size of 4 and a learning rate of 1e-3.
% Additionally, we use PANN~\cite{kong2020panns} model trained on AudioSet~\cite{gemmeke2017audio} to extract audio features for experiments with audio modality.
\begin{table*}[]
	\centering
 % \vspace{-0.1cm}  % 
	{\small
	\caption{Highlight detection performance comparison on TVsum dataset. }
	\label{table_TVsum}
    \begin{tabular}{l|c|cccccccccc|c}
    \hlineB{2.5}
    \multicolumn{1}{c|}{Method} & Src & VT        & VU        & GA        & MS        & PK        & PR        & FM        & BK   & BT   & DS   & Avg. \\ \hlineB{2.5}
    sLSTM~\cite{zhang2016video}   &V                  & 41.1      & 46.2      & 46.3      & 47.7      & 44.8      & 46.1      & 45.2      & 40.6 & 47.1 & 45.5 & 45.1 \\
    SG~\cite{mahasseni2017unsupervised} &V                    & 42.3      & 47.2      & 47.5      & 48.9      & 45.6      & 47.3      & 46.4      & 41.7 & 48.3 & 46.6 & 46.2 \\
    LIM-S~\cite{xiong2019less}         &V              & 55.9      & 42.9      & 61.2      & 54.0      & 60.3      & 47.5      & 43.2      & 66.3 & 69.1 & 62.6 & 56.3 \\
    Trailer~\cite{wang2020learning}    &V                 & 61.3      & 54.6      & 65.7      & 60.8      & 59.1      & 70.1      & 58.2      & 64.7 & 65.6 & 68.1 & 62.8 \\
    SL-Module~\cite{xu2021cross}       &V            & 86.5      & 68.7      & 74.9      & \textbf{86.2}      & 79.0      & 63.2      & 58.9      & 72.6 & 78.9 & 64.0 & 73.3 \\ 
    QD-DETR (\textbf{Ours}) &V    & \textbf{88.2} & \textbf{87.4} & \textbf{85.6} & 85.0 & \textbf{85.8} & \textbf{86.9} & \textbf{76.4} & \textbf{91.3} & \textbf{89.2} & \textbf{73.7} & \textbf{85.0} \\ \hline
    MINI-Net~\cite{hong2020mini}       &V+A             & 80.6      & 68.3      & 78.2      & 81.8      & 78.1      & 65.8      & 57.8      & 75.0 & 80.2 & 65.5 & 73.2 \\
    TCG~\cite{ye2021temporal}         &V+A                & 85.0      & 71.4      & 81.9      & 78.6      & 80.2      & 75.5      & 71.6      & 77.3 & 78.6 & 68.1 & 76.8 \\
    Joint-VA~\cite{badamdorj2021joint}   &V+A                 & 83.7      & 57.3      & 78.5      & 86.1      & 80.1      & 69.2      & 70.0      & 73.0 & \textbf{97.4} & 67.5 & 76.3 \\
    UMT~\cite{umt}        &V+A          & 87.5      & 81.5      & 88.2      & 78.8      & 81.4      & 87.0      & 76.0      & 86.9 & 84.4 & \textbf{79.6} & 83.1 \\ \hline
    QD-DETR (\textbf{Ours}) &V+A & \textbf{87.6} & \textbf{91.7} & \textbf{90.2} & \textbf{88.3} & \textbf{84.1} & \textbf{88.3} & \textbf{78.7} & \textbf{91.2} & 87.8 & 77.7 & \textbf{86.6}  \\ \hlineB{2.5}
    \end{tabular}
    }
    \vspace{-0.1cm}  % 
\end{table*}



\begingroup
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
\begin{table}[]
    \centering
    {\footnotesize
    % \vspace{0.1cm}  % 
    \caption{Charades dataset. $\dagger$ denotes the method using the video and audio as the source. SF+C stands for Slowfast and CLIP features.}
    \label{table_charades}
    \begin{tabular}{llll|llll}
    \hlineB{2.5}
    Method & feat & R1@0.5 & R1@0.7 & Method      & feat & R1@0.5 & R1@0.7 \\ \hlineB{2.5}
    SAP    & VGG  & 27.42  & 13.36  & CTRL        & C3D  & 23.63  & 8.89   \\
    TripNet& VGG  & 36.61  & 14.50  & ACL         & C3D  & 30.48  & 12.20  \\
    SM-RL  & VGG  & 24.36  & 11.17  & RWM-RL      & C3D  & 36.70  & -      \\
    MAN    & VGG  & 41.24  & 20.54  & MAN         & C3D  & 46.53  & 22.72  \\
    2D-TAN & VGG  & 40.94  & 22.85  & DEBUG       & C3D  & 37.39  & 17.69  \\
    FVMR   & VGG  & 42.36  & 24.14  & VSLNet       & C3D  & 47.31  & 30.19  \\
    UMT$\dagger$  & VGG  & 48.31  & 29.25  & \textbf{Ours}        & C3D  & \textbf{50.67}  & \textbf{31.02}  \\ \cline{5-8} 
    % UMT$\ddagger$  & VGG  & 49.35  & 26.16  & \textbf{Ours}        & C3D  & \textbf{50.67}  & \textbf{31.02}  \\ \cline{5-8} 
    \textbf{Ours}   & VGG  & 52.77  & 31.13  & M-DETR & SF+C & 53.63  & 31.37  \\
    \textbf{Ours}$\dagger$ & VGG  & \textbf{55.51}  & \textbf{34.17}  & \textbf{Ours}        & SF+C & \textbf{57.31}  & \textbf{32.55}  \\ \hlineB{2.5}
    \end{tabular}
    }
\end{table}
\endgroup

\subsection{Experimental Results}
% Paragraph 1 : QVHighlight
% compare with both MR~\cite{}, HD~\cite{} methods.
% 최근 부상중인 transformer를 사용한 방법론들이 월등한것을 보이고 있음.
% 그 중에서도 Ours 가 모든 곳에서 가장 좋은 성능을 내고 있는데, 이는 query의 중요성을 보여줌.
% 오디오 추가했을때 성능 증가가 왜 많지 않은지에 대해서. Audio만했을때 실험한거 해야함.
We compare QD-DETR against baselines in MR and HD
% ~\cite{momentdetr, umt, lei2020tvr, song2016click, liu2015multi, anne2017localizing, escorcia2019temporal, lei2020tvr} 
% ~\cite{zhang2016video, mahasseni2017unsupervised, xiong2019less, wang2020learning, xu2021cross, hong2020mini, ye2021temporal, badamdorj2021joint} 
throughout Tab.~\ref{table_QVHighlight}, Tab.~\ref{table_charades}, and Tab.~\ref{table_TVsum}. 
Our experiments with multi-modal sources, i.e., video with audio, are implemented by simply concatenating the video and audio along the channel axis.
Throughout the tables, we use bolds to denote the best scores.

In Tab.~\ref{table_QVHighlight}, the task is to jointly learn and predict MR/HD.
As observed, our QD-DETR outperforms state-of-the-art~(SOTA) approaches with all evaluation metrics.
Among methods utilizing the video source, QD-DETR shows a dramatic increase with stricter metrics with high IOU; it outperforms previous SOTA by large margins up to 36\% in R1@0.7 and mAP@0.75.
On the other hand, QD-DETR with video and audio sources boosts 11.84\% on average of the metrics for MR compared to the SOTA method employing the multi-modal source data.
These results verify the importance of emphasizing the source~(video-only or video+audio) descriptive contexts in the text queries.



\begin{table*}[t]
	\centering
	{\small
	\vspace{0.1cm}  % 
	\caption{Ablation study on QVHighlights $\textit{val}$ split. CATE and DAM stands for cross-attentive transformer encoder and using dynamic anchor moments as the decoder query, respectively. All the quantities are averaged over 5 runs.}
	\label{table_ablation}
        \begin{tabular}{c|c|c|c|c|ccccccc}
        \hlineB{2.5}
        \multicolumn{1}{c|}{\multirow{3}{*}{}} & \multicolumn{1}{c|}{\multirow{3}{*}{CATE}} & \multicolumn{1}{c|}{\multirow{3}{*}{Neg. Pair}} & \multicolumn{1}{c|}{\multirow{3}{*}{Saliency Token}} & \multicolumn{1}{c|}{\multirow{3}{*}{DAM}} & \multicolumn{5}{c}{MR}                                                             & \multicolumn{2}{c}{HD}                        \\ \cline{6-12} 
        \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{2}{c}{R1}          & \multicolumn{3}{c}{mAP}                          & \multicolumn{2}{c}{\textgreater{}= Very Good} \\ \cline{6-12} 
        
        \multicolumn{1}{c|}{} &\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & @0.5           & @0.7           & @0.5           & @0.75          & Avg.           & mAP                   & HIT@1                 \\ \hline

        (a) &  &  &  &  & 52.89 & 33.02 & 54.82 & 29.40 & 30.73 & 35.69 & 55.60 \\ \hline
        (b) &\cmark &  &  &  & 56.16 & 38.71 & 56.48 & 33.42 & 34.07 & 37.14 & 58.34 \\ 
        (c) && \cmark &  &  & 58.69 & 39.83 & 58.39 & 34.84 & 35.40 & 39.02 & 62.81 \\
        (d) &&  & \cmark &  & 55.48 & 37.00 & 55.81 & 26.75 & 32.84 & 37.48 & 58.59 \\ 
        (e) &&  &  & \cmark & 53.19 & 35.91 & 55.58 & 32.55 & 33.33 & 35.68 & 55.56 \\ \hline
        (f) & \cmark &  &  & \cmark & 57.72 & 42.35 & 59.10 & 38.16 & 38.03 & 36.56 & 57.44 \\
        % (e) &&  &  & \cmark & 55.03 & 37.25 & 56.96 & 33.38 & 34.16 & 35.24 & 54.44 \\ \hline
        % (f) & \cmark &  &  & \cmark & 57.72 & 42.35 & 59.10 & 38.16 & 38.03 & 36.56 & 57.44 \\
        (g) & \cmark & \cmark & &  & 59.57 & 42.12 & 59.19 & 36.63 & 36.76 & 38.64 & 61.62 \\ 
        (h) && \cmark & \cmark &  & 60.00 & 40.97 & 59.21 & 35.41 & 35.89 & 39.06 & 62.88 \\ 
        (i) &\cmark & \cmark & \cmark &  & 60.32 & 42.39 & 59.47 & 36.79 & 36.93 & \textbf{39.21} & 62.76 \\ \hline
        (j) &\cmark  & \cmark & \cmark & \cmark & \textbf{62.68} & \textbf{46.66} & \textbf{62.23} & \textbf{41.82} & \textbf{41.22} & 39.13 & \textbf{63.03} \\ \hlineB{2.5}
        \end{tabular}
        }
\end{table*}



% Results in Tab.~\ref{table_charades} also compare the performances against \cite{semanticprop, wang2019language, zhang2019man, zhang2020learning, gao2021fast, umt} with VGG features, \cite{gao2017tall, liu2018attentive, he2019read, zhang2019man, lu2019debug, zhang2020span} with C3D features, and \cite{momentdetr} with slowfast and clip features.
Results in Tab.~\ref{table_charades} also compare MR performances against the models using VGG~\cite{semanticprop, wang2019language, zhang2019man, zhang2020learning, gao2021fast, umt},  C3D~\cite{gao2017tall, liu2018attentive, he2019read, zhang2019man, lu2019debug, zhang2020span}, and Slowfast~(SF) and CLIP features~\cite{momentdetr} on Charades dataset.
For a fair comparison, we enumerate each method with its backbone and compare within it.
For each feature from VGG, C3D, and SF+C, we follow the data preparation settings from UMT~\cite{umt}, VSLNet~\cite{zhang2020span}, and Moment-DETR~\cite{momentdetr}.
% As observed, our performances surpass the existing SOTA numbers using any features.
As reported, we validate that our model surpasses the existing SOTA methods in every type of feature.

\begin{table*}[]
	\centering
	{\small
	\vspace{0.1cm}  % 
	% \caption{Ablation study on cross-attention transformer encoder. 
 \caption{Ablation study on cross-attention transformer encoder. 
 % We compare with the deepened transformer encoder with self-attention layers. 
    We compare ours against the deepened transformer encoder with self-attention layers to validate that the performance gain does not come from additional parameters.
    % 성능의 향상이 parameter 증가와 관련 없이 이뤄졌다는걸 보여주기 위해.
	SATE and CATE each indicate the transformer encoder only with self-attention layers and our transformer encoder. The numbers in the parenthesis denote the number of layers. For the experiment with $\dagger$, we only use the query features as the condition in the encoder and only the video representations are processed by the decoder.}
	\label{table_ablation_T2V_4layer}
        \begin{tabular}{c|ccccccc}
        \hlineB{2.5}
        \multicolumn{1}{c|}{\multirow{3}{*}{T2V}} &  \multicolumn{5}{c}{MR} & \multicolumn{2}{c}{HD}                        \\ \cline{2-8} 
        \multicolumn{1}{c|}{} & \multicolumn{2}{c}{R1}          & \multicolumn{3}{c}{mAP}                          & \multicolumn{2}{c}{\textgreater{}= Very Good} \\ \cline{2-8} 
        \multicolumn{1}{c|}{}  & @0.5           & @0.7           & @0.5           & @0.75          & Avg.           & mAP                   & HIT@1                 \\ \hline
        Moment-DETR~(SATE 2)  &  52.89$_{\pm{2.3}}$  & 33.02$_{\pm{1.7}}$ & 54.82$_{\pm{1.7}}$ & 29.40$_{\pm{1.7}}$ & 30.73$_{\pm{1.4}}$ & 35.69$_{\pm{0.5}}$ & 55.60$_{\pm{1.6}}$ \\
        Moment-DETR~(SATE 4)  &  53.60$_{\pm{1.2}}$  & 35.81$_{\pm{0.9}}$ & 54.55$_{\pm{0.8}}$ & 30.64$_{\pm{0.7}}$ & 31.74$_{\pm{0.4}}$ & 35.96$_{\pm{0.2}}$ & 56.56$_{\pm{0.9}}$ \\
        Moment-DETR~(CATE 4)  &  55.10$_{\pm{0.7}}$  & 37.02$_{\pm{0.9}}$ & 56.21$_{\pm{0.3}}$ & 32.00$_{\pm{0.9}}$ & 33.19$_{\pm{0.6}}$ & 36.43$_{\pm{0.3}}$ & 56.98$_{\pm{0.6}}$ \\ 
        Moment-DETR~(CATE 4)$\dagger$  &  56.16$_{\pm{1.2}}$ & 38.71$_{\pm{1.1}}$ & 56.48$_{\pm{0.8}}$ & 33.42$_{\pm{0.7}}$ & 34.07$_{\pm{0.6}}$ & 37.14$_{\pm{0.4}}$ & 58.34$_{\pm{0.4}}$ \\  \hline
        QD-DETR~(SATE 4)$\dagger$ & 60.48$_{\pm_{0.7}}$ & 45.21$_{\pm_{1.0}}$ & 60.84$_{\pm_{0.5}}$ & 40.45$_{\pm_{0.7}}$ & 40.12$_{\pm_{0.6}}$ & 38.66$_{\pm_{0.2}}$ & 61.29$_{\pm_{1.0}}$    \\
        QD-DETR~(CATE 4)$\dagger$ &  \textbf{62.68$_{\pm_{1.1}}$} & \textbf{46.66$_{\pm_{0.6}}$} & \textbf{62.23$_{\pm_{1.0}}$} & \textbf{41.82$_{\pm_{0.9}}$} & \textbf{41.22$_{\pm_{0.4}}$} & \textbf{39.13$_{\pm_{0.3}}$}        & \textbf{63.03$_{\pm_{0.5}}$}    \\  \hlineB{2.5}
        
        \end{tabular}
        }
\end{table*}


% Note that we do not involve the pretraining phase with the captions from automatic speech recognition.
% This is because as the role of text query is significant in QD-DETR, pretrained weights with noisy captions do not differ from the randomly initialized weights.
% Still, we argue that it is not problematic since QD-DETR also outperforms baselines with PT by large margins even without the pretraining phase.

For video highlight detection in Tab.~\ref{table_TVsum}, we follow the protocols from the previous work~\cite{umt}. 
Specifically, we train the model for each category and average the mAP scores.
% For results, we observe that QD-DETR outperforms existing works on 5 categories when only given the video source and exceeds 8 out of 10 categories when with video and audio.
Out of 10 categories, QD-DETR outperforms baselines on 9 categories when the only video source is available, and 8 categories when both video and audio are available. 
%For results, we observe that QD-DETR outperforms existing methods on 9 categories when only given the video source and exceeds 8 out of 10 categories when both video and audio are given.
% As a result, we observe that QD-DETR outperforms 
Overall, compared to methods with video-only and multi-modal sources, QD-DETR establishes new SOTA performances by improving by 4.2\% in average compared to the previous SOTA model.


\begin{figure}
    \centering
    % \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_1109.pdf}
    \includegraphics[width=1\linewidth]{figs/fig_saliency_hist_stat.pdf}
    \vspace{-0.8cm}
    \caption{
        Ablation study in terms of saliency scores.
        We plot the histograms of the average value of saliency scores in each video when the positive and negative text queries are given. 
        For positive scores, we only account the scores within GT moments.
        The average value of each histogram are visualized by the dotted line.
        The decrease in the overlap between histograms and the increase in the gap between average values confirms the gradual improvements of significance of the query in extracting video representation.
        % 넹 머리 리셋하고 다시 보면 개선할 수 있을거라 믿슴다 저는 일단 퀄리티가 모든 부분에서 기준 사항?은 넘긴거 같아요
        % 이거 7-8 넘어갈때 latext에서 4.4 시작 딱 맞추려고 이러는거 같아요 vspace로 어떻게든 올려볼까요 Table-caption 사이 
        % 오키오키 고고간격 좀 줄여보겟슴다
        % 아니면 potential negative societal impacts 이거 날려도 되지 않을까 싶어서요 이게 무슨 사회에 악영향이 있는 아님다 형님 자세히 보시면 fig 3 바꼇어요 query 텍스트가
        % 피겨어케올리지..
        % 원본 형님 이거 figure 안올라오는거 칸 부족해서 그런거예요? 그런거같애 몰라 버그야~ 앞에 좀 줄이면 올라올까싶어서 보고있는데 fig 아직안바꿧다 너 ㅋㅋㅋㅋ 조아조아 개소리써논거지 ㅋㅋㅋㅋㅋㅋㅋㅋ 아 그러네 굿굿 일단은 맞추고 제출하고 fig3 내일 다시 캡션 봐바야겟다 나도 잘썻다고 생각이안되서
        % Histogram of saliency when the positive and negative queries are given. We plot the histograms and its average value~(dotted line) to compare saliency scores when relevant~(positive) and irrelevant~(negative) text queries are given for each method. (Left) Since the video representations do not properly reflect textual information, both the positive and negative queries yield similar saliency scores. 
        % % (Middle) Even when the video representation is enforced to be updated with the textual information, the issue is not much resolved. 
        % (Right) By representing video clips in query-dependent manner, distributions are differentiated.
    }
    \vspace{-0.6cm}
    \label{fig:saliency_stat}
\end{figure}
% \SE{} % saliency label이 없으므로, GT moment에 속하는 애들을 saliency=1 로 나머지를 0으로 학습 진행 했다 내용 추가?




\begin{figure*}
    \centering
    % \includegraphics[width=1.\linewidth]{figs/qualitative.pdf}
    \includegraphics[width=1.\linewidth]{figs/fig_qualitative_2.pdf}
    \vspace{-0.5cm}
    \caption{
    Visualization of results predicted by QD-DETR. 
    Predicted and ground-truth moments are bounded by the lines. 
    Blue, green, and red lines indicate the saliency scores for positive, semi-positive, and negative queries. The positive saliency scores are consistently higher than the others, while the scores for semi-positive are higher than the ones for the negative.
    % Visualization of results predicted by QD-DETR. Predicted and ground-truth moments are bounded by dotted lines. Blue, grey, and orange solid lines indicate the saliency scores for positive, semi-positive, and negative. The positive saliency scores are consistently higher than the others, even for the semi-positive query.
    }%We bound the moment with the appearances at the top, whereas the duration of the moment is expressed along the temporal axis at the bottom.  queries, respectively.}
    \label{fig:qualitative}
    \vspace{-0.3cm}
\end{figure*}


















% Paragraph TVsum

% Paragraph charade

\subsection{Ablation study}
% main ablation table.
% ablation 결과 나열 및 이유 나열.
% 1. b, c, d, e 는 각각 component에 대해서 효과 비교.
% - 모든 component가 성능이 오르는데, 특히 b, c, d는 represntation quality가 올라가니 좋음
% 예를들어, b 는 text condition, c는 discriminative, d는 diverse한 pair를 한공간에 mapping하지 않음 으로.
% 또한, d는 dynamic acnhor box는 decoder에만 영향을 주니 MR 성능이 오르지만, 오히려 HD는 조금 떨어지는 경향을 보임.
% cate + neg pair 해야할듯.
% (a) -> (e) & (b,f), (g,h) 비교했을떄 차이 비교. : decoder에 여러 modality 가 아닌 condition을 걸어서넣어주면 source modality의 position만 다루면 되니 clear해짐.


To investigate the effectiveness of each component in our work, we conduct an extensive ablation study in Tab.~\ref{table_ablation}.
Note that, CATE and DAM denote cross-attentive transformer encoder and dynamic anchor moments, respectively.
Rows (b) to (e) show the effectiveness of each component compared to the baseline~(a).
To explain, whereas (e) only boosts the MR performances since it only affects the transformer decoder, (b), (c), and (d) are especially beneficial for both MR/HD tasks since they are focused on query-dependent video representations %enhancing the representation quality
; (b) ensures the contributions of text query in the video representation, (c) fully exploits the contexts of the text query, and (d) provides input-adaptive saliency predictor instead of MLP. %to prevent the diverse input pairs being embedded into the same embedding space.
% \WJ{} % 고쳤으니 확인좀 --> 넵 --> conclusion도 어떻게 improve할지 아이디어좀. 보고.
Moreover, while our components are verified that they are all complementary to others, DAM's effectiveness is especially dependent on the usage of CATE~(compare between \{(a, e)\} and \{(b, f), (i, j)\}).
% For instance, in comparison to 8.5\% increase of (e) i
% \SE{} % 여기 조금 더 강조할 수 있을지, 숫자를 예시로 들어서 우리꺼랑 쓰면 상승효과가 있다던가? DAM이 혼자쓰면 실제로 무슨 메트릭 기준에서 좀 안좋다
% 형님 그냥 글 더 안써도 괜춘할거 같슴다 오키오키
We claim that this is because DAM exploits the position information of the input tokens to capture the corresponding moments.
However, without CATE, input tokens are a mixture of multi-modal tokens, thereby providing confusing position information.
% We claim that this is because of the position information used for DAM so that integrating the multi-modal representation before processing to the decoder is helpful when designing the query with positional information.

To provide in-depth examinations of each component, we inspect the difference between the positive and the negative saliency scores in Fig.~\ref{fig:saliency_stat}.
Since the role of text query is trivial in our baseline, each distribution significantly overlies on top of the other. 
Then, as we add CATE and negative pair learning, we observe a consistent decrease in overlapped areas and a larger gap between the average saliency scores of  positive and negative histograms.
Also, we believe that the widely-distributed histogram of saliency scores for the 'CATE+Neg.pair' is due to using an identical criterion for saliency prediction for diverse video-query representations.
By employing an input-adaptive saliency predictor, we notice that scores for positive queries are in almost optimal shape.
% By employing an input-adaptive saliency predictor, we notice that saliency scores for positive queries are in almost optimal shape.



In addition, some might ask whether CATE benefits the training because of additional encoder layers.
To answer this, we conduct another ablation study in Tab.~\ref{table_ablation_T2V_4layer}.
Briefly, since our transformer encoder utilizes 2 cross-attention layers and 2 self-attention layers, we conduct comparisons against the transformer encoder composed of 4 self-attention layers~(SATE).
First, we compare CATE and SATE on Moment-DETR; by comparing the results in $2^{\text{nd}}$ and $3^{\text{rd}}$ rows, we find that CATE is much more beneficial than SATE even with the same number of layers.
Furthermore, the last two rows show comparisons within QD-DETR architecture that has the same tendency.
These results clearly demonstrate that the improvements from CATE are mainly from emphasizing the role of the text query rather than additional layers.
%not from the additional layers but from emphasizing the role of the text query.
% ablation table for T2V
% T2V 가 2개ㅡ이 layer 더 써서 그런가?
% T2V 썼을때랑 encoding layer 4개 쓴거 비교.


% PT 에 대한 실험 고찰? 이건 rebuttal 을 위해 남겨둘까 음.

\subsection{Qualitative Results}
% 
In this subsection, we study how the query-dependent video representation sensitively reacts to the change in the contexts of the text query.
In Fig.~\ref{fig:qualitative}, the measured saliency scores according to the video-query relevance are visualized.
We found that the more the query is relevant to the video clips, the higher the saliency scores retained for the query.
For instance, whereas the negative query that is totally irrelevant to the video instance has the lowest scores, the scores for semi-positive reside between the positive and the negative ones.  
Also, we find that QD-DETR sometimes provides a more precise moment prediction than a given ground-truth moment, as can be seen with the temporal box bounded by the dotted lines.
We believe that the tendency of a bit higher saliency scores at non-relevant clips for a positive query is due to the information mixing in the self-attention layers.
