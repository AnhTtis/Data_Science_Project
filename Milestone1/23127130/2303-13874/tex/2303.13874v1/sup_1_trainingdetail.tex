\section{Training Details}
% \SE{} % tau 값 추가 (\tau = 0.5)
% C3D: https://github.com/IsaacChanghau/VSLNet
% VGG + GloVe text embedding, audio: https://github.com/TencentARC/UMT
% SF+c: https://github.com/jayleicn/moment_detr


% % On QVHighlights, we basically follow the settings from Moment-DETR~\cite{momentdetr}.
% % With the video features extracted from pretrained SlowFast and Clip encoder, audio features from Q... ,and text embeddings from Clip, we train QD-DETR for 200 epochs with the learning rate of 1e-4 and weight decay of 1e-4.
% % For the architectural designs, our encoder is composed of 2 cross-attention layers and 2 self-attention layers whereas there are only 2 decoding layers.
% % With the hidden dimension of 256 for the transformers, we use the batch size to 32 and set each $\lambda_{\text{margin}}$, $\lambda_{\text{cont}}$, $\lambda_{L1}$, $\lambda_{\text{gIoU}}$, $\lambda_{\text{CE}}$ and $\lambda_{neg}$ to (margin, contra, neg = 1, span 10, giou = 1, class = 4). 
% % Lastly, our experiments are mostly conducted with a single NVIDIA TITAN RTX GPU.
% % \textcolor{red}{Training details on Charade, TVSUM not done}
% % % 여기 Loss 다 제대로 쓴다음에 람다값 나열
% % 다른 데이터 셋에 관한 것도 나열

% % We basically follow the settings from Moment-DETR~\cite{momentdetr}.
% % For architectural desings, our encoder composes of 2 cross-attention layers and 2 self-attention layers each
% % For architectural designs, both of our transformer encoders compose of 2 attention layers  whereas there are only 2 decoding layers.
% % For architectural designs, we share identical configurations for the entire dataset we tested.
% For the unified configurations across all experiments, our encoder composes of 4 layers of transformer block~(2 cross-attention layers and 2 self-attention layers) whereas there are only 2 layers in the decoder~(For HD, we only use encoding layers).
% We set the hidden dimension of transformers as 256, and use the Adam optimizer with a weight decay of 1e-4.
% Also, the basic setting for balancing parameters is $\lambda_{\text{margin}}=1$, $\lambda_{\text{cont}}=1$, $\lambda_{L1}=10$, $\lambda_{\text{gIoU}}=1$, $\lambda_{\text{CE}}=4$ and $\lambda_{\text{neg}}=1$, unless otherwise mentioned.

% % % V1
% For other training details on QVHighlight, we use video features extracted from both pretrained SlowFast~\cite{slowfast}~(SF) and CLIP encoder~\cite{CLIP}, and text embeddings from CLIP, following the Moment-DETR.
% We train QD-DETR for 200 epochs with a batch size of 32 and a learning rate of 1e-4.
% For the Charades-STA dataset, we utilize official VGG~\cite{VGG} features with GloVe~\cite{pennington2014glove} text embedding.
% To compare with additional baselines, we also test our model on pretrained C3D~\cite{C3D} and SlowFast for video features with CLIP text embedding.
% We train ours for 100 epochs with a batch size of 8 and a learning rate of 1e-4.
% Besides, we set $\lambda_{\text{margin}}$, $\lambda_{\text{cont}}$ and $\lambda_{\text{neg}}$ as 4 for this dataset.
% Lastly, for the TVSum dataset, we use I3D~\cite{I3D} features pretrained on Kinetics-400~\cite{kay2017kinetics} as a visual one, and CLIP for the text embedding.
% We train our model for 2000 epochs with a batch size of 4 and a learning rate of 1e-3.
% Additionally, we use PANN~\cite{kong2020panns} model trained on AudioSet~\cite{gemmeke2017audio} to extract audio features for experiments with audio modality.

In this section, we elaborate on the implementation details and hyperparameters used for experiments in the main manuscript.
To unify configurations across all experiments, our encoder composes of 4 layers of transformer block~(2 cross-attention layers and 2 self-attention layers) whereas there are only 2 layers in the decoder~(For HD dataset, i.e., TVSum, we only use encoding layers).
We set the hidden dimension of transformers as 256, and use the Adam optimizer with a weight decay of 1e-4.
Besides, we set the temperature of a scaling parameter $\tau$ for contrastive loss as 0.5 for all experiments.
Loss balancing parameters are $\lambda_{\text{margin}}=1$, $\lambda_{\text{cont}}=1$, $\lambda_{L1}=10$, $\lambda_{\text{gIoU}}=1$, $\lambda_{\text{CE}}=4$ and $\lambda_{\text{neg}}=1$, unless otherwise mentioned.
Additionally, we use the PANN~\cite{kong2020panns} model trained on AudioSet~\cite{gemmeke2017audio} to extract audio features\footnotemark[1] for experiments with the audio modality.

Other configurations are described as follows:
% Other dataset dependent configurations are described as follows:

\noindent\textbf{QVHighlight.}
We use video features extracted from both pretrained SlowFast~\cite{slowfast}~(SF) and CLIP encoder~\cite{CLIP}, and text embeddings from CLIP, following the Moment-DETR.
We train QD-DETR for 200 epochs with a batch size of 32 and a learning rate of 1e-4.

\noindent\textbf{Charades-STA.}
We utilize official VGG~\cite{VGG} features with GloVe~\cite{pennington2014glove} text embedding.
To compare with additional baselines, we also test our model on pretrained C3D~\cite{C3D}, SlowFast and CLIP for video features with CLIP text embedding.
Specifically, we utilize pre-extracted features provided by other baselines repositories: UMT\footnote{https://github.com/TencentARC/UMT}, VSLNet\footnote{https://github.com/IsaacChanghau/VSLNet} and Moment-DETR\footnote{https://github.com/jayleicn/moment\_detr}.
We train ours for 100 epochs with a batch size of 8 and a learning rate of 1e-4.

\noindent\textbf{TVSum.}
I3D~\cite{I3D} features pretrained on Kinetics-400~\cite{kay2017kinetics} are utilized as a visual one, and CLIP features are used for the text embedding.
Following the most recent work~\cite{umt}, we train our model for 2000 epochs with a learning rate of 1e-3. The batch size is set to 4.