\section{Related Work}
% \subsection{Moment Retrieval and Video Highlight Detection}
\subsection{Moment Retrieval and Highlight Detection}
% % (2st version)
% Given the source video and the text query, moment retrieval is to localize the semantically query-relevant moments in the video~\cite{anne2017localizing}.
% To search for the query-relevant scenes, early work~\cite{liu2015multi} adopted deep visual-semantic embedding~\cite{fromedeep} to directly infer the relevance between the text query and video clips, and MCN~\cite{anne2017localizing} modeled the video context as a latent variable and attended the contexts with the specific video-query pair.
% % In earlier works, there were two main branches: focusing solely on the visual features~\cite{song2016click} and using the text query along with the video~\cite{liu2015multi}.
% % In earlier works for finding contextually similar moments, 
% % the research was conducted in two branches: focusing solely on the visual features~\cite{song2016click} and using the text query along with the video~\cite{liu2015multi}.
% % However, the latter stream has been popular in recent studies.
% Then, on the one hand, CAL~\cite{escorcia2019temporal} represented moments as a set of temporal regions and aligned each region to the specific query element with an efficient metric, the squared-Chamfer distance, to improve the applicability, and XML~\cite{lei2020tvr} decoupled the role of shallower and deeper layers to retrieve the coarse video and more fine-grained moments, respectively.
% More recently, 2D-TAN~\cite{zhang2020learning} proposed to promote the temporal relation and FVMR~\cite{gao2021fast} improved the cross-modal interaction module to speed up the inference.

% (1st version)
% Prior work to find the contextually similar moment in the video focus on only visual feature selecting attractive thumbnail with high relevance to video content and visual aesthetic quality\cite{song2016click}, but selected thumbnail only using visual information cannot represent video semantics. Accordingly, method to find the text query-dependent video moment is proposed utilizing visual-semantic embedding model to learn latent embedding space combining visual and textual information\cite{liu2015multi}.
% However, video with natural language query determine do not focus on temporal information. To address this issue, moment localization to integrate local and global video features considering spatio-temporal context is proposed \cite{anne2017localizing, escorcia2019temporal,lei2020tvr}.

% % (3st version)
% Moment retrieval is the task of localizing the moment relevant to the given query, typically the text description of the moment, from the video.
% The general approach for this task is considering the multi-modality of query-video pair or understanding the context of the temporal relation between video clips.
% For example, MCN~\cite{anne2017localizing} proposed network architecture which exploit both the temporally local and global information in videos.
% Similarly, 2D-TAN~\cite{zhang2020learning} models the relations between the moments to consider the temporal context of video.
% On the other hand, there are some trials exploiting the additional subtitle data to localzing the moment~\cite{lei2020tvr}, or enhancing the model in terms of inference speed for efficient MR~\cite{gao2021fast}.

% (4st version)
MR is the task of localizing the moment relevant to the given text description.
% Moment retrieval is the task of localizing the moment relevant to the given query, typically the text description of the moment, from the video.
Popular approaches are modeling the cross-modal interaction between text query-video pair~\cite{zhang2020span,yuan2019semantic,lu2019debug} or understanding the context of the temporal relation among video clips~\cite{anne2017localizing,zhang2020learning}.
% The general approach to resolve this task is modeling the cross-modal interaction between text query-video pair~\cite{zhang2020span,yuan2019semantic,lu2019debug} or understanding the context of the temporal relation between video clips~\cite{anne2017localizing,zhang2020learning}.
% zhang2020span 기반으로 cross-modal (multi-modal) interaction
% The general approach to resolve this task is considering the multi-modal characteristics of text query-video pair~\cite{zhang2020span,yuan2019semantic} or understanding the context of the temporal relation between video clips~\cite{anne2017localizing,zhang2020learning}.
% For example, MCN~\cite{anne2017localizing} proposed network architecture which exploit both the temporally local and global information in videos.
% Similarly, 2D-TAN~\cite{zhang2020learning} models the relations between the moments to consider the temporal context of video.
On the other hand, %other trials also exist.
TVT~\cite{lei2020tvr} exploited the additional data, i.e., subtitle, to capture the moment, and FVMR~\cite{gao2021fast} enhanced the model in terms of inference speed for efficient MR.
% On the other hand, there are some trials exploiting the additional subtitle data to localize the moment~\cite{lei2020tvr}, or enhancing the model in terms of inference speed for efficient MR~\cite{gao2021fast}.




% The most pioneering work~\cite{liu2015multi} focused on extracting the thumbnail of video matched with the given text query by learning to embed both visual and textual information into common space.
% To extend the finding of the snapshot into the moment, MCN~\cite{anne2017localizing} proposed the network architecture which integrates the local and global video representation to effectively localize the moment with natural language queries.
% On the other hand, XML~\cite{lei2020tvr} introduces the cross-modal moment localization model, which additionally utilizes the subtitle with the video as information for localization.
% % 2D-TAN temporal relation bewteen moments
%  More recently, 2D-TAN~\cite{zhang2020learning} is proposed to consider the temporal context by modeling the relations between the moments, and FVMR~\cite{gao2021fast} improved the cross-modal interaction module to speed up the inference.
% or enhancin the model in terms of inference speed.
% A broadly used approach for this task is considering the multi-modality of query-video pair and understanding the context of temporal relation between video clips.
% Cross modal을 고려하거나
% moment들의 temporal relation을 고려하거나
% 속도를 중점으로 고려해왔다.




% On the other hand, video highlight detection aims to spot the highlight segments for each video.
% 

% \SE{} % 지금 내용이 전체적으로 내용을 좀 줄여야해서 이 부분 적어주신 것 바탕으로 좀 축약하겠습니다. 혹시 내용이 틀린 부분 있으면 말씀 부탁드릴게요. :)
% \DC{} % 네 :) %\cite{anne2017localizing}, \cite{xu2021cross} 해당 논문의 인용은 제외시킨 이유가 있을까요? / 내용 확인했습니다. 궁금한 부분은 코멘트 달아놨어요.
% %\cite{anne2017localizing} --> 이거는 MR에 있어서 중복해서 적으면 좀 이상할까봐 제외했구 --> 아하 좋습니다!
% \cite{xu2021cross} --> 이거는 어디에 넣어야할지 잘 모르겠어서 일단 빼놨네요 이게 domain adaptation 느낌 맞죠? --> domain adaptation 네네 맞습니다 얘네가 unsupervised domain adaptation의 기존 5개의 알고리즘과 비교해봤더니 우위를 점했다 라고 주장해요. 음 이 구조로 보니까 조금 애매하긴 하네요 
% 
% vsLSTM
% Unsupervised video summarization with adversarial lstm networks 
% Less is more: Learning highlight detection from video duration
% CCANet - coattention (Learning Trailer Moments in Full-Length Movies with Co-Contrastive Attention)
% DL-VHD - cross domain 
% text query - Multi-task deep visual-semantic embedding for video thumbnail selection
% text query - MCN


Different from the MR, HD aims to measure the clip-wise importance level of the given video~\cite{sun2014ranking, yao2016highlight}.
Due to its popularity and applicability, HD can be divided into several branches.
From the perspective of annotation, we can categorize HD into supervised, weakly supervised, and unsupervised HD.
Supervised HD~\cite{gygli2016video2gif, xu2021cross, sun2014ranking} utilizes fine-grained highlight scores, which are very expensive to collect and annotate~\cite{xiong2019less}.
On the other hand, weakly supervised HD~\cite{cai2018weakly, panda2017weakly, xiong2019less} learns to detect segments as highlights %when only the video event label,
with video event labels, and finally, unsupervised HD~\cite{badamdorj2022contrastive,mahasseni2017unsupervised, khosla2013large, rochan2018video} does not require any annotations.
Also, while the task is often implemented only with the video, there are works to employ the extra data modalities.
Generally, multi-modality was taken into account by using the natural language query to find the desired thumbnail~\cite{liu2015multi} and using additional sources, i.e., audio, to predict highlights~\cite{ye2021temporal,badamdorj2021joint}.
% \WJ{} % 지금까지 video 적어주신것들 종합하여 renewal 해봤어요. 이상한 부분 코멘트 부탁드립니다. --> \cite{xiong2019less} 가 weakly supervised 로 볼 수 있는 걸까요?? 네 그렇더라구요.. 벤치마크가 --> 네 알겠습니다 --> 확인했습니다!

% sup
% weak sup cai2018weakly, panda2017weakly, xiong2019less
% unsup 
% multimodal
% - query 
% - source

% Different from the moment retrieval, highlight detection aims to measure the clip-wise importance level of the given video.
% The most common approach is training the model using the highlight-ness supervision that the dataset provides~\cite{zhang2016video,wang2020learning,wei2022learning}.
% Differently, the other mainstream is developing the model in an unsupervised way~\cite{badamdorj2022contrastive,xiong2019less,mahasseni2017unsupervised}.
% For example, some of those works utilize the duration information of video~\cite{xiong2019less} or exploit the adversarial training framework~\cite{mahasseni2017unsupervised}.
% % Similarly, recent advance~\cite{xu2021cross} exploits the unsupervised domain adaptation for transferring the knowledge of label-existing domain to label-free domain.
% Similarly, recent advance~\cite{xu2021cross} exploits the unsupervised domain adaptation algorithm for practical usage of HL on the label-absent domains.
% % 비슷하게, UDA를 활용하여 label-free domain에 대해
% Furthermore, there are studies to consider the multi-modal input such as using natural language query to find the desirable thumbnail~\cite{liu2015multi} or employing the additional data from different modalities like the audio to predict highlights~\cite{ye2021temporal,badamdorj2021joint}.


% (2nd version)
% \textcolor{red}{need to revise and confirm}
% Video highlight detection aims to score sections judged as highlights for each video segment. % vsLSTM 같은거보시면 이전에는 scoring하는게 아니라 1/0 binary하게 highlight clip이냐 아니냐로 나뉘었던 것 같습니다. -> frame-level 로 importance score 를 매긴다고 서술되어 있어서 1/0 binary 는 아닌 것 같습니다.  --> 혹시 이거 어떤 논문에서 이러는지 알려주실수있나요!? 이런 말이 citation이 필요해서 좀 더 찾아보고 해당 말을 사용하게되면 citation 추가하려고합니다!! --> % Our models use frames as its internal representation. The inputs are framelevel features x and the (target) outputs are either hard binary indicators or frame-level importance scores (i.e., softened indicators). 다시보니, binary 도 서술이 되어있긴 하네요. 'Video Summarization with Long Short-term Memory' 입니다.
% Because of the variable-range dependencies, vsLSTM~\cite{zhang2016video} addresses the large annotated data for training with supervised learning on SumMe, TVSum datasets. 
% %\cite{zhang2016video} : (supervised learning)
% But it costs a lot to expect all the videos to be annotated in a very large amount videos. 
% Therefore unsupervised methodologies such as focusing on selecting keyframes through LSTM~\cite{mahasseni2017unsupervised} and novel ranking framework~\cite{xiong2019less} have been proposed. 
% %\cite{mehasseni2017unsupervised} : (unsupervised) 비디오에서 키프레임들을 summarizer(Autoencoder LSTM) 와 discriminator (LSTM) 으로 스트럭쳐를 구성하여 adversarial framework 를 만듬.
% %\cite{xiong2019less} : (unsupervised) ranking framework
% CCANet utilizes co-attention and contrastive attention modules to augment the video features and to get semantic relation between moments~\cite{wang2020learning}. 
% %\cite{wang2020learning} : (weakly-supervised) novel ranking network utilizing the co-attention + contrastive attention module
% To handle the problem that inference is impossible because there is no supervisory signal in the target image, pair-based learning is conducted with SL-module which contains the highlights of video segments under a wide context by proposing a DL-VHD framework to solve this problem ~\cite{xu2021cross}. 
% %\cite{xu2021cross} : (weakly-supervised - knowledge distillation) 타겟 영상이 supervisory signal 이 없어 추론이 불가할 때 이를 해결하기 위하여 DL-VHD 프레임워크를 제안함으로써 넓은 맥락 아래에서 비디오 세그먼트들의 하이라이트를 소유하는 SL-module을 통해 pair-based learning 을 진행한다.
% Later, MCN and visual-semantic embedding model deal with a given text query for extracting highlights by understanding the context using the visual feature of the video ~\cite{anne2017localizing, liu2015multi}. 
% %\cite{anne2017localizing} : (text query-relevant based) 
% %\cite{liu2015multi} : (text query-relevant based)
% Recently, Bi-modal attention mechanisms that detect temporary-spatial relevant highlights using audio feature were proposed ~\cite{ye2021temporal,badamdorj2021joint}.
% %\cite{ye2021temporal} : bi-modal attention (with audio feature)
% %\cite{badamdorj2021joint} : bi-modal attention (with audio feature)


%supervised / unsupervised / weakly supervised 이렇게 나눠야할까? 의견 필요.
% 나누는게 좋아 보이는 것 같은데 어떤 흐름으로 쓰실건지 같이 정리해주시면 저희가 이해하고 다시 정리하는데 큰 도움이 될 것 같습니다.
% 영어로 해주실 필요는 없고 
% 예를들어, 아래에는 sup -> unsup, weak sup 차례대로 한번 간단히 작성해봤는데 
% 이런식으로 흐름도 적어주시면 좋겠습니다. 저희도 해당 method를 다 알지 못해서 작성해주신 부분만 가지고 글을 고치기가 매우 어렵네요 ㅠㅠ 
% 위에 각 문장별로 정리해봤습니다. 밑에도 시나리오별로 정리해봤는데, 체크 부탁드려요!


% video highlight detection은 어떤 분야다.. 이후에 (% On the other hand, video highlight detection aims to spot the highlight segments for each video.)
% 이를 해결하기 위해 (supervised 방법들) 
% 누구는 어땟고 누구는 어땟다.
% 하지만, 이들은 large annotation 을 필요로 하기에 unsupervised, weakly supervise 방법들도 고안되었다.
% unsupervised에서는 누구누구들이 이런 방식으로 문제를 풀려하였고,
% 반면에 wekaly supevised 는 누구누구들이 이런식으로 하려하였다.

%--------------------------------------------------
% On the other hand, video highlight detection aims to spot the highlight segments for each video.
% Because of the variable-range dependencies, vsLSTM~\cite{zhang2016video} addresses the large annotated data for training with supervised learning on SumMe, TVSum datasets.
% 하지만 이는 large annotation 을 필요로 하기 때문에 unsupervised, weakly supervised 방법들도 고안되었다.
% (unsupervised 방법에는 ~)unsupervised methodologies such as focusing on selecting keyframes through LSTM~\cite{mahasseni2017unsupervised} and novel ranking framework~\cite{xiong2019less} have been proposed. 
% (weakly supervised 방법에는 ~) CCANet utilizes co-attention and contrastive attention modules to augment the video features and to get semantic relation between moments~\cite{wang2020learning}. 
% To handle the problem that inference is impossible because there is no supervisory signal in the target image, pair-based learning is conducted with SL-module which contains the highlights of video segments under a wide context by proposing a DL-VHD framework to solve this problem ~\cite{xu2021cross}. 
% 이후 영상의 하이라이트를 선정할 때 시청자가 흥미가 있는 moment 를 주어진 text query 에 기반하여 특정 temporal 구간을 뽑는 방법론이 대두되었다.
% Later, MCN and visual-semantic embedding model deal with a given text query for extracting highlights by understanding the context using the visual feature of the video ~\cite{anne2017localizing, liu2015multi}. 
% 최근에는 visual feature 만 사용하는 것이 아닌, audio feature 를 사용하여 하이라이트 구간을 더 잘 찾을 수 있는 mechanism 이 나왔다.
% Recently, Bi-modal attention mechanisms that detect temporary-spatial relevant highlights using audio feature were proposed ~\cite{ye2021temporal,badamdorj2021joint}.
%-----------------------------------------------------
% On the other hand, video highlight detection aims to spot the highlight segments for each video. Because of the variable-range dependencies, vsLSTM~\cite{zhang2016video} addresses the large annotated data for training with supervised learning on SumMe, TVSum datasets. However, since this requires large annotation, unsupervised, weakly supervision methods have also been devised. Unsupervised methodologies such as focusing on selecting keyframes through LSTM~\cite{mahasseni2017unsupervised} and novel ranking framework~\cite{xiong2019less} have been proposed. For weakly supervision, CCANet utilizes co-attention and contrastive attention modules to augment the video features and to get semantic relation between moments~\cite{wang2020learning}. To handle the problem that inference is impossible because there is no supervisory signal in the target image, pair-based learning is conducted with SL-module which contains the highlights of video segments under a wide context by proposing a DL-VHD framework to solve this problem ~\cite{xu2021cross}. When selecting the highlight of the video, a methodology for selecting a specific temporary moments based on a given text query has emerged. MCN and visual-semantic embedding model deal with a given text query for extracting highlights by understanding the context using the visual feature of the video ~\cite{anne2017localizing, liu2015multi}. Recently, Bi-modal attention mechanisms that detect temporary-spatial relevant highlights using audio feature were proposed ~\cite{ye2021temporal,badamdorj2021joint}.


%48\cite{zhang2016video}
%22\cite{mahasseni2017unsupervised}
%40\cite{xiong2019less}
%38\cite{wang2020learning}
%41\cite{xu2021cross}
%1\cite{anne2017localizing}
%21\cite{liu2015multi}
%45\cite{ye2021temporal}
%3\cite{badamdorj2021joint}
\begin{figure*}
    \centering
    % \includegraphics[width=\linewidth]{figs/fig2_main_v3.pdf}
    \includegraphics[width=\linewidth]{figs/fig2_main_rebuttal.pdf}
    % \caption{Overview of QD-DETR architecture. Each clip in modalities, e.g., video and audio, are first processed to be attended solely by the text query. Then, query-attended representations are ~
    % Audio and
    \caption{
    Overview of the proposed QD-DETR architecture.
    Given a video and text query, we first extract video and text features from the frozen backbones.
    These video and text features are forwarded into the cross-attention transformer~(Sec.~\ref{Sec.CrossAttentiveTransformerEncoder}). This process ensures the consistent contribution of text queries to the video tokens and together with negative pair learning~(Sec.~\ref{Sec.VideoDiscriminativeQueryInformation}), builds query-dependent video representation.
    Then, accompanied by the saliency token~(Sec.~\ref{Sec.SaliencyToken}), video tokens are given to the transformer encoder.
    In this procedure, the saliency token is transformed into the adaptive saliency prediction criteria.
    The outputs of the encoder are then, processed to compute losses for both HD and MR.
    Specifically, the encoder's output tokens are directly projected to saliency scores and optimized for HD, and also provided to the transformer decoder with the learnable moment queries to estimate the query-described moments.
    Finally, losses for MR are computed by the discrepancy between predicted and their corresponding GT moments.
    % Overview of the proposed QD-DETR architecture.
    % Given video and text query, we first extract the video and text features from the frozen backbones.
    % These video and text features are forwarded into the cross-attention transformer~(Sec.~\ref{Sec.CrossAttentiveTransformerEncoder}). This process ensures the consistent contribution of text query to the video tokens, achieving the query-dependent video representation.
    % Then, accompanied by the saliency token~(Sec.~\ref{Sec.SaliencyToken}), video tokens are given to the transformer encoder.
    % In this procedure, the saliency token is transformed into the adaptive saliency prediction criteria.
    % After the transformer encoder, we compute the saliency score of each video clip and optimize the losses for HD.
    % Especially, they consist of saliency loss and negative pair loss~(Sec.~\ref{Sec.VideoDiscriminativeQueryInformation}) for learning to rank the saliency of given clips and enhancing the general inter-relationship between videos and queries.
    % On the other hand, video tokens are also given to the transformer decoder with the learnable moment queries to estimate the moment matched with the given query.
    % Losses for moment retrieval are computed by the discrepancy between the predicted moments and their corresponding ground-truth moment to optimize the entire architecture.
    }
    \label{fig:overall architecture}
\end{figure*}
% Very recently, these tasks were issued in that they share the common objective to discover the desired clips from the video.
% Then, in the absence of datasets, Moment-DeTR~\cite{momentdetr} introduced the QVHighlights dataset to study both at one take.
% They adopted the detection transformer architecture to find the demanded clips as a set and rank the scores for each clip inside the set.
% UMT~\cite{umt}, on the other hand, focused on handling multi-modal sources, e.g., video and audio, to yield clip-level predictions.
% We share a common idea with these recent works in adopting the transformer architecture, but the difference stems from the main focus of QD-DETR is to explore how to utilize the text query.

% Recently, along with a more accurate and challenging dataset, QVHighlights~\cite{momentdetr}, transformer architectures have shown their effectiveness for moment retrieval~\cite{momentdetr, umt}.

Although the MR/HD share a common objective to localize or discover the desired part of the given video, they have been studied separately.
To handle these tasks at once, Moment-DETR~\cite{momentdetr} proposed the QVHighlights dataset, which contains a human-written text query and its corresponding moment with clip-level saliency labels.
They also introduced the modified version of detection transformer~(DETR~\cite{detr}) to localize the query-relevant moments and their saliency scores.
% UMT~\cite{umt}, on the other hand, focused on handling multi-modal sources, e.g., video and audio, to yield clip-level predictions.
Following them, UMT~\cite{umt} focused on processing multi-modal data by utilizing both video and audio features.
Different from recent works deploying transformer architectures, here we concentrate on producing a query-dependent representation with the transformer.
% On the other hand, UMT~\cite{umt} focused on handling multi-modal data by utilizing the both video and audio features to solve MR and HD tasks.
% Differnt from aforementioned techniques, here we point out the lack of capability to detect the query-relecence of video and highlight the importance of query-dependent representation for retrieving the query-relevant moment and it accordance level.




\subsection{Detection Transformers}
DETR~\cite{detr}, an end-to-end object detector based on vision transformers, is one of the very recent works that %modify and bring 
utilize the transformer architectures for computer vision~\cite{dosovitskiy2020image, touvron2021training}.
% Although DETR suffered from slow convergence, it eliminated the need for anchor generation and non-maximum suppresion~(NMS).
Although DETR suffered from slow convergence, it simplifies the prediction process by eliminating the need for anchor generation and non-maximum suppression.
Since then, along with the advance in DETR~\cite{dai2021dynamic, zhu2020deformable, li2022dn}, DETR-like architectures have been popular in downstream tasks in both the image~\cite{cheng2022masked, cheng2021per, huang2022monodtr, zhang2022monodetr} and video domains~\cite{yang2022tubedetr, momentdetr}.
Some of these works focused on analyzing the role of the decoder query and discovered that using the positional information speeds up the training and also enhances the detection performance~\cite{conditionaldetr, dabdetr}.
% On the other hand, there are other trials striving to process multi-modal data using DETR-like architectures~\cite{kamath2021mdetr, momentdetr}. --> multi-modal data 를 처리하기 위해 발전
On the other hand, there are trials to extend the application of DETR on multi-modal data~\cite{kamath2021mdetr, momentdetr}, especially dealing with the query from different modalities, i.e., text, for detection~(or retrieval).
% On the other hand, there are trials to extend the application of DETR on multi-modal data~\cite{kamath2021mdetr, momentdetr}, especially dealing with the detection~(or retrieval) query of a different modality such as text.
They generally handle the multi-modal data % multi-modality of data 
by simply forwarding them together to the transformer.
% Some of these works focused on finding the role of the decoder query and discovered that using the positional information speeds up the training~\cite{conditionaldetr, dabdetr}, while others also strived to process multi-modal data using DETR-like architectures~\cite{akbari2021vatt, kamath2021mdetr, momentdetr}.
% \SE{} % multimodal 제외
% Our transformer architecture differs from previous multi-modal handling DETR variants in that our DETR focuses on conditioning query modality on the source modalities.
% multi modal 은 condition이 없음.
In this paper, we also focus on handling the multi-modal data based on DETR-like architecture.
However, different from the aforementioned techniques, we concentrate on the query-dependency of the prediction results.
% However, different from the aforementioned techniques, we concentrate on the query-dependant scheme to enhance the query-awareness in the prediction results.
%query-conditioning scheme to enhance the query-dependency in the predicted results.
% 우리는 retriving (detection) 하는 부분의
% Similar to previous techiques on dealing with multi-modal data, we 
% Different from the aforemention method, 
% 우리도 multimodality의 DETR을 연구하나, 그 중 different modality의 condition을 더 잘 반영하는 방법에 대한 연구를 진행한다.

% 그다음 발전 detr 들.... dino
% 그 중, 쿼리를 position정보 활용하고자 % conditional detr... % dab detr...도 있었고
% mdetr (multi-modal detr) 처럼 multi-kmodal 활용도 있어음.
% moment-detr
% 하지만, 기존 multi-modal 을 다루는건 treat equally, not condition이고
% moment detr은 그에 대해 특별한 조치가 없음.
% 우리는 이ㅡㄷㄹ과 다르게....쿼리 어쩌구...


