\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}

\usepackage{doi}
\usepackage{float} 
\usepackage{mathrsfs}
\usepackage{subfigure}

\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabto}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{makecell}
\usepackage{array}
\usepackage{diagbox}
\usepackage{amsmath,amssymb}
\usepackage{threeparttable}

\graphicspath{ {./images/} }


\title{A Confident Labelling Strategy Based on Deep Learning for Improving Early Detection of Knee OsteoArthritis}


\author{
 Zhe Wang \\
  IDP Laboratory, UMR CNRS 7013\\
  University of Orleans\\
  Orleans, France \\
  \texttt{zhe.wang@etu.univ-orleans.fr} \\
  %% examples of more authors
   \And
 Aladine Chetouani \\
  PRISME Laboratory, EA 4229\\
  University of Orleans\\
  Orleans, France\\
  \texttt{aladine.chetouani@univ-orleans.fr} \\
  \And
 Rachid Jennane \\
 IDP Laboratory, UMR CNRS 7013\\
  University of Orleans\\
  Orleans, France \\
  \texttt{rachid.jennane@univ-orleans.fr} \\
}

\begin{document}
\maketitle
\begin{abstract}
Knee OsteoArthritis (KOA) is a prevalent musculoskeletal disorder that causes decreased mobility in seniors. The diagnosis provided by physicians is subjective, however, as it relies on personal experience and the semi-quantitative Kellgren-Lawrence (KL) scoring system. KOA has been successfully diagnosed by Computer-Aided Diagnostic (CAD) systems that use deep learning techniques like Convolutional Neural Networks (CNN). In this paper, we propose a novel Siamese-based network, and we introduce a new hybrid loss strategy for the early detection of KOA. The model extends the classical Siamese network by integrating a collection of Global Average Pooling (GAP) layers for feature extraction at each level. Then, to improve the classification performance, a novel training strategy that partitions each training batch into low-, medium- and high-confidence subsets, and a specific hybrid loss function are used for each new label attributed to each sample. The final loss function is then derived by combining the latter loss functions with optimized weights. Our test results demonstrate that our proposed approach significantly improves the detection performance.
\end{abstract}

% keywords can be removed
\keywords{Siamese Network \and Confidence level \and Hybrid loss \and Knee osteoarthritis \and X-ray}


\section{Introduction}
Knee OsteoArthritis (KOA) known also as degenerative osteoarthropathy causes joint cartilage degeneration and damage, joint space narrowing, and subchondral bone reactive hyperplasia \cite{kneeoa}, with myriad compounding factors including old age, stress, trauma, etc. \cite{multi-factor}. Patients may experience excruciating pain and limited movement, impacting their quality of life and putting them at increased risk of various chronic diseases such as cardiovascular disease \cite{cardiovascular}. As predicted by the United Nations (UN), people over the age of 60 will make up more than 20$\%$ of the global population by the year 2050 \cite{2050}. To date, no cure has been developed yet \cite{nocure} and a definitive cause for KOA has yet to be found \cite{notclear}. Therefore, it is crucial to diagnose KOA as early as possible so that behavioral interventions, such as weight loss, can be made in a timely manner to delay the onset and worsening of KOA symptoms \cite{weightloss}.

The Kellgren-Lawrence (KL) grading system \cite{KL} was proposed in 1957 as a gold standard in the assessment of KOA when using plain radio-graphs. As presented in Table \ref{KL_grades}, KOA severity can be defined into five grades depending on the existence and degree of symptoms. However, it relies completely on the perception and judgement of each medical professional, with practitioners potentially differing in their assessments for the same knee X-ray image \cite{shamir}.

\begin{table}[htbp]
\centering
\caption{Description of the KL-grading system}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{lll}
\toprule
Grade &  Severity & Description\\
\midrule
KL-0 & none & definite absence of osteoarthritis\\
KL-1 & doubtful & possible osteophytic lipping \\
KL-2 & minimal & definite osteophytes and possible JSN\\
KL-3 & moderate & moderate multiple osteophytes, definite JSN, some\\
&& sclerosis and possible deformity of bone ends\\
KL-4 & severe & large osteophytes, definite JSN, severe sclerosis and\\
&& definite deformity of bone ends\\
\bottomrule
\end{tabular}
\label{KL_grades}
\end{table}

Recently, computer hardware advancements have enabled deep learning techniques to play a greater role in computer vision. Convolutional Neural Networks (CNNs) \cite{cnn}, specifically, have been able to effectively execute tasks such as detection \cite{CMIG_detection}, segmentation \cite{CMIG_segmentation} and classification \cite{CMIG_classification}. 

For KOA diagnosis, several learning models have been proposed. In \cite{antony2016}, Antony et al. detected knee joint centres using a linear Support Vector Machine (SVM) model \cite{SVM} and the Sobel horizontal image gradient. Then, they extracted a Region of Interest (ROI) using the corresponding recorded coordinates to feed the fine-tuned BVLC CaffeNet network \cite{caffe}. In \cite{antony1}, Antony et al. employed a Fully Convolutional Neural (FCN) network \cite{fcn} and a classical CNN for automatic knee joint detection and the classification of KL grades, respectively. In \cite{chen}, Chen et al. employed the YOLOv2 model \cite{yolov2} to detect knee joint automatically. In \cite{tiuplin}, Tiulpin et al. used two patches from the lateral and medial parts of the knee joint X-ray images, which were then used as inputs of the two sub-networks of a Siamese-CNN model. Although each models has its own characteristics and successes, the decision-making process is widely regarded as a black box. Therefore, in \cite{chen} and \cite{tiuplin}, the authors used the Grad-CAM \cite{gradcam} technique to visualize attention maps to enhance the interpretability of the models.

Existing approaches for the detection of KOA severity usually address the problem as an image classification task, assigning each KL grade to a distinct category \cite{tiuplin}\cite{tiulpin2}. However, quantifying KL grades as discrete integers only results from a desire to find a convenient way to represent the severity of KOA. As KOA develops continuously, KL grades are not essentially categorical but instead represent a continuous ordinal scale of increasing severity. In \cite{antony2016}, Antony et al. treated the problem as a regression task using the Mean Square Error (MSE) loss function. The performance was improved compared to the same model using the Cross-Entropy (CE) loss function. However, learning models with a single loss function do not fully solve the problem caused by the semi-quantitative nature of the KL grading system. Hence, several loss strategies have been proposed. In \cite{antony1}, Antony et al. first combined the CE and the MSE losses with optimized weights to balance the concepts of classification and regression. In \cite{chen}, Chen et al. modified the CE loss function using the adjustable ordinal matrix to assign a higher penalty to misclassification with a larger distance between the predicted and the real KL grades. In \cite{wang}, Wang et al. emphasized samples with high confidence and suppressed low confidence ones after extracting the label confidence information using a hybrid loss function to make the model learn relevant features better. Such approaches tend to introduce the concept of regression to the task of classification into a certain extent.

Moreover, it is noteworthy that continuity exists not only between different KL grades but also within the same KL grade. For different samples with the same KL grade label, medical practitioners may indicate different degrees of certainty. For example, two samples labelled with the same KL grade may not have been labelled with the same confidence level. In such cases, due to the robustness of the deep learning model, samples labelled with less certainty either receive an identical training as other regular-certainty samples, or they are treated as noises directly. Thus, the learning model exhibits a tendency to be overconfident \cite{overconfident}, and may fail to classify some samples in favor of those labelled with a high level of confidence. Therefore, a strategy for training each sample according to its confidence level should be proposed.

It is noteworthy that KOA is definitely present at KL-2 although of minimal severity. Patients at advanced stages of KOA (KL-3 and KL-4) usually have recourse to total knee replacement \cite{replacement}. Hence, early detection of KOA is clinically more worthwhile and is thus the objective in this study. However, since in the literature, the label data for KL-1 patients are often regarded as doubtful, they may contain high uncertainties that could lead to unstable model training and inaccurate prediction results. Therefore, in this study, we focused solely on KL-0 and KL-2 instead as early detection of KOA. To do this, we propose an approach combining the Siamese-based learning model and a novel hybrid loss strategy for the early detection of KOA (KL-0 vs KL-2). Our proposed learning model, called Siamese-GAP network, is inspired by the work of \cite{tiuplin} and \cite{wang}. So as not to consider only features from high levels and contrarily to the classical Siamese model, we also combine features from lower levels in the proposed approach. To do so, a collection of Global Average Pooling (GAP) layers are intergrated at various levels in order to extract key information at each level. This information is then concatenated in order to consider all the features and improve the KOA prediction performance. Then, a hybrid loss strategy is introduced to partition the samples in each training batch into high-, midium- and low-confidence subsets and three different loss functions are applied, seperately: Label Smoothing Cross-Entropy (LSCE) (see Eq. \ref{LSCE}), Kullback-Leibler Divergence (KLD) \cite{KLD} and the original CE loss. The final hybrid loss function is computed by combining the above loss functions along with optimized weights.

The main contributions of this paper are as follows:
\begin{itemize}
\item[$\bullet$] A novel Siamese-based learning model (Siamese-GAP) \cite{zhe} combining features from several levels (i.e., low to high) is proposed to set out features efficiently.
\item[$\bullet$] A smart partition strategy is used to divide each training batch into three subsets according to the confidence level of each sample.
\item[$\bullet$] An hybrid loss strategy is proposed to select the adequate loss function according to the high-, medium-, and low- confidence sets. 
\item[$\bullet$] Attention maps given by the Grad-CAM technique are provided to highlight regions that contribute to the network's decision.
\item[$\bullet$] The OsteoArthritis Initiative (OAI) \cite{OAI}, a freely-accessible database, was used for all experiments.
\end{itemize}

\section{Proposed approach}
\begin{figure}
\centering  %图片全局居中
\includegraphics[width=1\textwidth]{flowchart_final.png}
\caption{The flowchart of the proposed approach consists of two main parts: the Siamese-GAP model and the hybrid loss strategy. Black arrows represent the overall data-flow. The proposed Siamese-GAP network is shown in the left block. Red vertical arrows represent GAP units. Blue braces represent the concatenation operation. $\oplus$ symbolises the operation of bit-wise addition, and the purple block represents the fully connected and Softmax layers. The proposed hybrid loss strategy is shown in the right block. Light purple and light blue arrows are the data-flow of validation and training batches, respectively. $\lambda$ is used to rank the samples of each KL grade based on their confidence level. $\alpha$, $\beta$, and $\gamma$ are the hyper-parameters used to weight the three considered loss functions. The other parameters such as $c_0$, $c_2$ are defined in Section \ref{hyperparaters}}
\label{flowchart}
\end{figure}

The flowchart of this study combining the Siamese-GAP model and the hybrid loss strategy is presented in Fig. \ref{flowchart}. Firstly, in the distal region of a given knee X-ray, two ROIs are taken from the medial and lateral sides (see Fig. \ref{Fig-Patches}). Secondly, our proposed Siamese-GAP model is used to extract relevant features. Thirdly, the confidence level of each sample is computed through the probability distribution. Consequently, each training batch is divided based on the confidence level of each KL label. Finally, the hybrid loss strategy is applied during the training step to enhance the classification performance. The main symbols used in this paper are given in Table \ref{symbols}.

\begin{table}[htbp]
\centering
\caption{Important symbols used in this paper}
\setlength{\tabcolsep}{8mm}
\begin{tabular}{cl}
\toprule
Symbol &  Description\\
\midrule
$\mathcal{K}$ & Set of KL grades (KL-0 and KL-2)\\
$\mathcal{D}$ & Overall dataset\\
$\mathcal{T}$ & Training set\\
$\mathcal{V}$ & Validation set\\
$\hat{Y}$ & Predicted labels\\
$Y$ & Real labels\\
\bottomrule
\end{tabular}
\label{symbols}
\end{table}

\subsection{Proposed learning model}
\label{learning_model}
Before describing the proposed architecture, we briefly present the structure of the classical Siamese network, which was first proposed in \cite{siamese_original} to compare and verify whether two hand-signed signatures are identical. As shown in Fig. \ref{siamese}, the classical Siamese network is a coupling structure based on two artificial neural networks. It takes two samples as inputs and compares their similarity by calculating the distance between their feature vectors using either the Euclidean or the Cosine distance. The main advantage of this network is that all parameters of the two sub-networks are shared, which reduces the complexity of the model and maps different inputs to the same spatial dimension to make data distributions consistent.

\begin{figure}
\centering  %图片全局居中
\includegraphics[width=0.6\textwidth]{classical_siamese.png}
\caption{The structure of the classical Siamese network \cite{siamese_original}.}
\label{siamese}
\end{figure}

Initially intended as a means to determine the degree of similarity between two inputs, the Siamese-based network also has the advantage of being an efficient feature extractor. Tiuplin et al. \cite{tiuplin, tiulpin2} demonstrated the possibility of combining two sub-network outputs to derive a final feature vector, which then served as input into a fully connected layer for classification.

By integrating GAP modules at different levels, our proposed learning model, Siamese-GAP, was designed to identify pertinent features at all levels of the network. Our model consists of a pair of CNNs that share parameters and partition 13 layers into four blocks (i.e., 2, 3, 4, and 4) for each sub-network. Each of these four blocks is composed of a combination of Convolution with various-sized kernels (i.e., 32, 64, 128, and 256) layers, Batch Normalization (BN) layer and Rectified Linear Unit (ReLU) layer.

Considering the significance of texture in the medical imaging field \cite{yassine}, we therefore aimed to retain as much of the spatial information contained in the input data as possible. To accomplish this, we employed strides instead of any pooling layer to perform the down-sampling. The first convolutional layer's stride of each block was set to 2 from the second block onward, while it was set to 1 for the other blocks. Following each block, a GAP layer was then used to derive a feature vector. The resulting four feature vectors were then concatenated to obtain a single GAP-feature vector providing information from the low to high levels of one sub-network. It is noteworthy that GAP was considered rather than the classical Global Max Pooling (GMP) for the feature extraction (see Section \ref{results} for performance comparison). GAP is calculated as follows:

\begin{equation}
y_{k}=\frac{1}{\left | \mathbb{M}_k \right |}\sum_{(p,q)\in \mathbb{M}_k}^{}x_{kpq}
\end{equation}
where the single output value of the $k$-th feature map $\mathbb{M}$ produced by GAP is represented by $y_{k}$. $\left | \mathbb{M}_k \right |$ is the number of elements in the $k$-th feature map, and $x_{kpq}$ is the element at location $(p, q)$ of the $k$-th feature map.

The second sub-network is structurally similar and identically configured. The two GAP-feature vectors obtained from the two sub-networks are then combined by applying an element-wise addition operation to retrieve the final feature vector, which is then used as input of the fully connected layer of size 480. Lastly, the Softmax layer provides the probability distributions of KL-0 and KL-2 grades that give a prediction of the input data’s membership class. Compared with the element-wise addition, we also evaluated the concatenation of the two GAP-feature vectors directly. However, the increased number of parameters did not improve the performance.

\subsection{Partition of the dataset}
\label{Partition}
As explained previously, the aim of our proposed hybrid loss strategy is to define a confidence level for each sample with regard to its label. Intuitively, the basis of this approach should have been to divide the training batch into subsets with different confidence level according to confidence thresholds. However, as no prior information related to the confidence level of each sample is provided in the original database, we assume here that the high-confidence set, named $\mathcal{H}$, should account for more than half of the dataset and the low-confidence set, named $\mathcal{L}$, should be the smallest one. Otherwise, the dataset would be meaningless. The samples that are not included in either $\mathcal{H}$ or $\mathcal{L}$ are assigned to the medium-confidence set, named $\mathcal{M}$. The confidence level of each sample was computed by focusing on its conditional probability of being correctly predicted as follows:

\begin{equation}
c_s = P_{\hat{Y}|T} (\hat{Y} = k|T = k), \quad {\forall}k\in \mathcal{K}, {\forall}s \in \mathcal{D}
\end{equation}
where $\hat{Y}$ and $T$ are the predicted and the real labels, respectively, $k$ represents the KL grade of the sample $s$, $P_{\hat{Y}|T}$ is the conditional probability distribution, $\mathcal{K}$ is a set of KL grades (KL-0 and KL-2), and $\mathcal{D}$ is the overall dataset.

To determine the different confidence sets in each training batch, we first grouped the samples according to their KL grade and sorted them in descending order by their confidence level. Then, we defined a hyper-parameter ratio called $\lambda$ to split each training batch into $\mathcal{H}$, $\mathcal{M}$ and $\mathcal{L}$ sets (see Table \ref{abc}). The same ratio $\lambda$ was considered for both KL-0 and KL-2 classes. As a result, each training batch was divided into ($\mathcal{H}_0$, $\mathcal{M}_0$, $\mathcal{L}_0$) and ($\mathcal{H}_2$, $\mathcal{M}_2$, $\mathcal{L}_2$) for KL-0 and KL-2, respectively.

\subsection{Hybrid loss strategy}
\label{hyperparaters}
The proposed hybrid loss strategy is based on the use of three function losses: LSCE, KLD, and CE for $\mathcal{H}$, $\mathcal{M}$ and $\mathcal{L}$, respectively. 

Firstly, the LSCE loss function was used for the set $\mathcal{H}$ to avoid the learning model being overconfident. To this end, we used label smoothing \cite{LS} which is a regularization technique that consists in perturbing the target variable to make the learning model less certain of its prediction. 

Label smoothing replaces the one-hot encoded label vector $y_s^{hot}$ by a mixture of $y_s^{hot}$, named $y_{s}^{LS}$:

\begin{equation}
y_{s}^{LS} = y_{s}^{hot}(1-\varepsilon)+ \frac{\varepsilon}{2}, \quad y_{s}^{hot} \in \left\{0,1\right\}, \varepsilon \in (0,1)
\end{equation}

\begin{equation}
y_{s}^{hot}=\left\{
\begin{array}{lcr}
1       &      & {\hat Y      =      T}\\
0    &      & {\hat Y  \neq  T}\\
\end{array} \right.
\end{equation}
where $y_{s}^{hot}$ is the one-hot encoded ground-truth label of the sample $s$, and $\varepsilon$ is a hyper-parameter that determines the amount of smoothing.

Following this, the LSCE loss, $J_{LSCE}$ is computed as follows:

\begin{equation}
\begin{aligned}
J_{LSCE} &= \sum_{s\in \mathcal{H}_k}^{}-y_{s}^{LS}log(c_s)\\&=\sum_{s\in \mathcal{H}_k}^{}-(y_s^{hot}(1-\varepsilon) + \frac{\varepsilon}{2})log(c_s),{\forall}k \in \mathcal{K}, {\forall}s \in \mathcal{H}
\end{aligned}
\label{LSCE}
\end{equation}

Secondly, for the samples included in the set $\mathcal{M}$, the KLD loss \cite{KLD} was considered where the target probability distribution of each KL grade was derived from the corresponding average confidence obtained from the validation set (see Eq. \ref{p_target} and Eq. \ref{p_s}). Given that the dataset was randomly divided during the preprocessing stage, the confidence distribution of the validation set can be considered as representative of the overall data confidence distribution. 

The average confidence $c_k$ of each KL grade is computed as follows:

\begin{equation}
c_k = \frac{1}{\left | c_s \right |}\sum_{s\in \mathcal{V}_k}^{} c_s, \quad {\forall}k \in \mathcal{K}, {\forall}s \in \mathcal{V}
\end{equation}
where $\mathcal{V}$ is the validation set.\\

The KLD loss, $J_{KLD}$ is finally given by:

\begin{equation}
J_{KLD} = \sum_{s\in \mathcal{M}_k}^{}y_s^{hot}log\frac{p_{target}}{p_s}, \quad {\forall}k \in \mathcal{K}, {\forall}s \in \mathcal{M}
\end{equation}
where, 
\begin{equation}
\label{p_target}
p_{target}=\left\{
\begin{array}{lcr}
c_k       &      & {\hat Y      =      T}\\
1 - c_k     &      & {\hat Y  \neq  T}\\
\end{array} \right.
\end{equation}
and 
\begin{equation}
\label{p_s}
p_s=\left\{
\begin{array}{lcr}
c_s       &      & {\hat Y      =      T}\\
1 - c_s     &      & {\hat Y  \neq  T}\\
\end{array} \right.
\end{equation}

Thirdly, the original CE loss, $J_{CE}$ is used for the samples in the set $\mathcal{L}$ and computed as follows:

\begin{equation}
J_{CE} = \sum_{s\in \mathcal{L}_k}^{}-y_{s}^{hot}log(c_s), \quad {\forall}k \in \mathcal{K}, {\forall}s \in \mathcal{L}
\end{equation}

Finally, the proposed hybrid loss function is defined as follows:

\begin{equation}
J_{hybrid} = \alpha J_{LSCE} + \beta J_{KLD} + \gamma J_{CE}
\end{equation}
where the hyperparameters $\alpha$, $\beta$, and $\gamma$ were used to weight and better balance the considered loss functions.

\section{Experiments}
In this section, the experimental database, preprocessing process, and implementation details of our work are presented.

\subsection{Public knee database}
We used knee data from the Osteoarthritis Initiative (OAI) \cite{OAI}, which is a freely-accessible database. The OAI is a longitudinal study of 4796 individuals aged between 45 to 79 years of age followed over 96 months, with each participant having nine follow-up examinations.

\subsection{Data preprocessing}
\begin{figure}[htbp]
\centering
\subfigure[]{
\label{ROIs}
\begin{minipage}[t]{0.25\textwidth}
\centering
\includegraphics[width=1\textwidth]{dicom.png}
%\caption{a typical knee X-ray}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.241\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{knee.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=0.75\textwidth]{roi.png}
\end{minipage}
}
\caption{(a) Original OAI knee X-ray and obtained ROI in red box. (b) Extracted lateral and medial patches in blue and orange boxes. (c) Obtained patches as input of our proposed model.}
\label{Fig-Patches}
\end{figure}

As shown in Fig. \ref{Fig-Patches}, lateral and medial patches of the knee joint were the foci of this study to enable early KOA detection. More specifically, the same Regions Of Interest (ROIs) as in \cite{chen} were detected using YOLOv2 \cite{yolov2} and two lateral and medial patches were cropped using the completely automated segmentation approach as the work of \cite{tiuplin} to cover the entire distal area of the knee (including osteophytes and JSN). Firstly, two 128$\times$128 pixel square patches were taken from the lateral and medial one-third parts of the given ROI. Then, the medial patch was flipped horizontally. With this, these two patches served as inputs of our proposed Siamese-GAP network.

As a result of the preprocessing steps, 3,185 KL-0 and 2,126 KL-2 images were collected. According to each KL grade, the dataset was randomly divided into training, validation, and test sets with a ratio of 7:1:2, respectively.

%\begin{table}[htbp]
%\caption{Distribution of ROIs used for experiments}
%\label{data}
%\setlength{\tabcolsep}{10.8mm}
%\begin{tabular}{ccc}
%\toprule
%Dataset & KL-0 & KL-2\\
%\midrule
%All & 3185 & 2126\\
%Training & 2230 & 1488\\
%Validation & 318 & 213\\
%Test & 637 & 425\\  
%\bottomrule   
%\end{tabular}  
%\end{table}

\subsection{Experimental details}
The weights of the network were initialized by the Kaiming initialization procedure \cite{kaiming}. With a learning rate of 1e-03 and a mini-batch size of 32, the Adam optimizer \cite{adam} was used to train 500 epochs. Random brightness, rotation, and gamma correction were executed randomly in the data augmentation process. To prevent over-fitting, weight decay was parameterized at a coefficient of 3e-04 and dropout at 0.2. Moreover, to deal with the imbalanced dataset, the set of KL-2 was oversampled based on bootstrapping \cite{oversample}. The Nvidia TESLA A100 GPUs with RAM of 40GB and PyTorch v1.8.1 \cite{pytorch} were used to implement our work.

\section{Results and discussion}
In this section, experimental results are presented and discussed. The class of the KL-2  was considered as the positive class to calculate the F1-score. Moreover, to independently evaluate the performance of our proposed model, all accuracies in Table \ref{ablation} and Table \ref{comparision} was obtained using the original CE loss function.

\subsection{Results obtained using GAP/GMP layer(s)}
\label{results}
In our proposed learning model, except for the last regular GAP layer, three additional GAP layers were used in each sub-network. The place of each GAP/GMP layer governs the level of features to be considered. To show the contribution of each GAP/GMP layer, we performed an ablation study. As can be seen in Table \ref{ablation}, the performance achieved by exploiting GAP layers at various levels is almost consistently better than that of GMP, whatever the combination of the levels. This is mostly caused by the fact that GMP only maintains one element with the max value in each feature map; as a result, it discards much texture-related information. Therefore, GAP is better suited than GMP to extract and retain relevant features. This finding was also observed in \cite{gapvsgmp}. In addition, different GAP and GMP combinations were also evaluated, where GAP and GMP units were mixed. However, no performance improvement was observed.

\begin{table}[htbp]
\centering
\caption{Comparison of various combinations of GAP and GMP units}
\begin{threeparttable}
\setlength{\tabcolsep}{4.5mm}
\begin{tabular}{ccccccc} % 控制表格的格式
\toprule
\multicolumn{4}{c}{\bf Position$^1$}  & & \multicolumn{2}{c}{\bf Accuracy ($\%$)} \\
\cline{1-4}  % 这部分是画一条横线在2-6 排之间
\cline{6-7}  % 这部分是画一条横线在2-6 排之间
P1 & P2 & P3 & P4 && GAP & GMP\\
\midrule
 &   &  & $\times$  && 87.51 & 87.12\\ 
$\times$ &   &  & $\times$  && 88.01 & 87.94\\
     &   $\times$&  & $\times$  && 88.24 & 87.86\\ 
  &   & $\times$ & $\times$  &&  87.78 & 87.90\\ 
$\times$ & $\times$  & & $\times$ && 88.14 & 86.89\\
$\times$ & $\times$  & $\times$ & $\times$  &&  \bf 88.38 & 86.95\\
\bottomrule
\end{tabular}
\label{ablation}
\begin{tablenotes}
\footnotesize
\item[$1$]{P1, P2, P3 and P4 refer to positions of the GAP/GMP unit in the proposed Siamese-GAP network.}
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Comparison with common learning models}
In this section, our proposed Siamese-GAP network is compared to various common CNNs. To ensure a fair comparison, the same ROIs were used (see Fig. \ref{ROIs}). Results are shown in Table \ref{comparision} in terms of accuracy, F1-score and number of parameters required. As can be seen, the classical Siamese model and our proposed model outperform the other evaluated ones. This shows that the Siamese-based model can effectively extract meaningful and decisive features to improve the classification performance. Our proposed Siamese-GAP model continues to enhance feature extraction and thus improve the performance. Notably, compared to the other common model series (i.e., DenseNet, ResNet, and VGG), our model needs fewer parameters.

\begin{figure}
\centering
\subfigure[Original ROI]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=1\textwidth]{sample.png}
%\caption{a typical knee X-ray}
\end{minipage}
}
\subfigure[DenseNet-201]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=1\textwidth]{densenet_cam.jpg}
\end{minipage}
}
\subfigure[ResNet-18]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=1\textwidth]{resnet_cam.jpg}
\end{minipage}
}
\subfigure[VGG-11]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=1\textwidth]{vgg_cam1.jpg}
\end{minipage}
}
\subfigure[Classic Siamese]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=1\textwidth]{tiulpin_cam1.jpg}
\end{minipage}
}
\subfigure[Our model]{
\begin{minipage}[t]{0.15\textwidth}
\centering
\includegraphics[width=1\textwidth]{ourmodel.jpg}
\end{minipage}
}
\caption{Comparison of the attention maps generated by the final convolutional layer of each model.}
\label{Attention-Map}
\end{figure}

The Grad-Cam technique \cite{gradcam} was also applied to illustrate the regions that each model considered when making the decision. The attention maps thus obtained are presented in Fig. \ref{Attention-Map}. As can be noticed, all evaluated models show a certain sensitivity to regions that involve pertinent early KOA (KL-2) characteristics. However, they exhibit a reaction to background noise as well, which may potentially impact their classification performance negatively. Conversely, the proposed Siamese-based model focuses more on areas concerned by KOA. In addition, low computing resources are required. Compared to the classic Siamese, our proposed model once again improved the ability to locate the KOA area precisely.

\begin{table}[htbp]
\centering
\caption{Comparison of various commonly used models}
\begin{threeparttable}
\setlength{\tabcolsep}{3.5mm}
\begin{tabular}{lccc} % 控制表格的格式
\toprule
Models & Accuracy ($\%$)  & F1 ($\%$)  & Params ($M$)$^{1}$\\
\midrule
Densenet-121 & 80.84 & 77.03 & 6.95\\
Densenet-161 & 79.06 & 75.09 & 26.47\\
Densenet-169 & 81.81 & 78.23 & 12.48\\
Densenet-201 & 84.43 & 81.29 & 18.09\\
Resnet-18 & 83.59 & 80.30 & 11.17\\
%Siamese-Resnet-18 & 87.07 & 84.40\\
Resnet-34 & 80.14 & 76.35 & 21.28\\
%Siamese-Resnet-34 & 78.20 & 74.35\\
Resnet-50 & 81.78 & 78.29 & 23.51\\
%Siamese-Resnet-50 & 79.09 & 75.26\\
Resnet-101 & 77.60 & 72.98 & 42.50\\
%Siamese-Resnet-101 & 77.07 & 72.96\\
Resnet-152 & 76.47 & 72.13 & 58.14\\
%Siamese-Resnet-152 & 79.43 & 75.40\\
VGG-11 & 80.97 & 77.30 & 195.89\\
%Siamese-VGG-11 & 82.15 & 78.58\\
VGG-13 & 80.71 & 77.88 & 196.07\\
VGG-16 & 73.17 & 68.50 & 201.38\\
VGG-19 & 71.98 & 68.84 & 206.70\\
Classic Siamese$^{2}$ & 87.33 & 84.82 & \bf 2.71\\
Our model &  \bf 88.38 &  \bf 85.93 &  \bf 2.71\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[$1$] "Params" refers to the total number of parameters utilized by the model.
\item[$2$] Same structure as our model with no GAP modules
\end{tablenotes}
\end{threeparttable}
\label{comparision}
\end{table}

\subsection{Selection of the hyper-parameters}
\label{Selection}
To evaluate the impact of the hyper-parameters in the proposed hybrid loss strategy, different configurations were tested. We first determined the level of smoothing $\varepsilon$ $\in \left\{ 0.05, 0.1, 0.15, 0.2\right\}$. Then, we evaluated different partition ratios $\lambda$, namely 6:3:1, 7:2:1 and 8:1:1. Here, for greater convenience, we used only on whole integer values. Finally, we selected the weight hyper-parameters through a small grid search over combinations of $\alpha$, $\beta$, $\gamma$\ $\in [0.1, 1]$ ensuring that $\alpha + \beta + \gamma = 1$. For greater convenience, Table \ref{abc} only shows the performance obtained for each $\lambda$ and $\varepsilon$ with the most optimized weight parameters $\alpha$, $\beta$ and $\gamma$. As can be seen, the best performance was achieved with the partition ratio $\lambda$ of 7:2:1, $\varepsilon = 0.15$, $\alpha = 0.4$, $\beta = 0.5$, and $\gamma = 0.1$. Note that the weight $\gamma$ for the low-confidence set $\mathcal{L}$ is usually the smallest one, which prevents the model from learning much wrong information.

\begin{table}[htbp]
\centering
\caption{Effect of different hyper-parameters on performance}
\setlength{\tabcolsep}{3mm}
\begin{tabular}{ccccccc} % 控制表格的格式
\toprule
$\lambda$ & $\varepsilon$ & $\alpha$  & $\beta$  & $\gamma$ & Accuracy ($\%$) & F1 ($\%$)\\
\midrule
\multirow{4}{*}{6:3:1} & 0.05 & 0.4 & 0.5 & 0.1 & 88.64 & 86.15\\
& 0.1 & 0.4 & 0.4 & 0.2 &  88.59 & 86.08\\
& 0.15 & 0.5 & 0.4 & 0.1 &  88.75 & 86.28\\
& 0.2 & 0.3 & 0.5 & 0.2 & 88.82 & 86.39\\
\midrule
\multirow{4}{*}{\bf 7:2:1} & 0.05 & 0.3 & 0.5 & 0.2 & 88.96 & 86.59\\
& 0.1 & 0.4 & 0.3 & 0.3 &  88.51 & 86.02\\
& \bf 0.15 & \bf 0.4 & \bf 0.5 & \bf 0.1 &  \bf 89.14 & \bf 86.78\\
& 0.2 & 0.3 & 0.5 & 0.2 & 88.82 & 86.34\\
\midrule
\multirow{4}{*}{8:1:1} & 0.05 & 0.3 & 0.5 & 0.2 & 89.09 & 86.67\\
& 0.1 & 0.4 & 0.4 & 0.2 &  88.56 & 86.02\\
& 0.15 & 0.3 & 0.4 & 0.3 &  88.67 & 86.10\\
& 0.2 & 0.3 & 0.4 & 0.3 & 89.03 & 86.50\\
\bottomrule
\end{tabular}
\label{abc}
\end{table}

\subsection{Contribution of the hybrid loss}
As shown in Fig. \ref{confidence_disctibution}, the hybrid loss strategy effectively prevents the model from becoming overconfident, smooths the confidence distribution of each class, and strengthens inter-class continuity. Note that for each sample, the hybrid loss strategy does not increase its confidence level to an unsuitable degree, which could make the model fail to converge. 

\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.3\textwidth]{confidence_ce5.png}}
\subfigure[]{
\includegraphics[width=0.3\textwidth]{confidence_hybrid.png}}
\caption{Confidence distributions obtained using the proposed Siamese-GAP model with the CE loss (a) and the proposed hybrid loss (b). The confidence data were sorted in descending order.}
\label{confidence_disctibution}
\end{figure}

The t-SNE scatter plot in Fig. \ref{tnse} shows that compared with the CE loss function, the proposed hybrid loss strategy decreases the intra-class distance and improves the similarity of the feature distribution of the two classes.

\begin{figure}[htbp]
%\label{tnse}
\centering
\subfigure[]{
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{tnse2.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[t]{0.3\textwidth}
\centering
\includegraphics[width=1\textwidth]{tnse1.png}
\end{minipage}
}
\caption{t-SNE scatter plots obtained using the Siamese-GAP model along with the CE loss (a) and the proposed hybrid loss (b).}
\label{tnse}
\end{figure}

%\subsection{Effects of warm-up}
%To accelerate the convergence of the model, the warm-up epoch was used. As shown in Fig. \ref{warmup}, the loss value before applying the hybrid loss decreases as the number of warm-up epoch(s) increases and the performance of the warm-up depends mainly on the loss function used during the warm-up epoch(s). As can be seen, using the CE and LSCE loss ($\varepsilon = 0.05$) functions, the model converges respectively at about the 200th and 250th epochs, regardless of the number of warm-up epoch(s). On the contrary, without warm-up epoch the model converges at about the 400th epoch. 

%\begin{figure}[htbp]
%\label{warmup}
%\centering
%\subfigure[Warm-up using the CE loss]{
%\includegraphics[width=0.3\textwidth]{warmup_ce.png}}
%\subfigure[Warm-up using the LSCE loss]{
%\includegraphics[width=0.3\textwidth]{warmuplsce.png}}
%\caption{Effects of the number of warm-up epochs when using CE and LSCE loss functions.}
%\label{warmup}
%\end{figure}

\subsection{Analysis of the applicability}
For greater convenience, except for the Siamese-based model, we only selected the model from each DenseNet, ResNet, and VGG series that achieved the best performance in its group to evaluate the applicability of our proposed hybrid loss strategy. Table \ref{applicability} presents the performance comparison using the original CE loss and the proposed hybrid loss strategy with the same set of hyperparameter used in Section \ref{Selection}. As can be seen, the accuracy is improved for all the learning models evaluated. This shows the potential of the proposed hybrid loss strategy as this finding is still valid even if the set of hyperparameters is not appropriately selected for each learning model.

\begin{table}[htbp]
\centering
\caption{Applicability of the proposed hybrid loss}
\begin{threeparttable}
\setlength{\tabcolsep}{1.9mm}
\begin{tabular}{lccc} % 控制表格的格式
\toprule
Models & Accuracy ($\%$)$^1$  & Accuracy ($\%$)$^2$  & Difference ($\%$)\\
\midrule
Densenet-201 & 84.43 & 84.51 & 0.08 $\uparrow$\\
Resnet-18 & 83.59 & 84.16 & 0.57 $\uparrow$\\
VGG-11 & 80.97 & 82.56 & 1.59 $\uparrow$\\
Classical Siamese & 87.33 & 87.45 & 0.12 $\uparrow$\\
Our model & \bf 88.38 & \bf 89.14 & 0.76 $\uparrow$\\
\bottomrule
\end{tabular}
\label{generalization}
\begin{tablenotes}
\footnotesize
\item[$1$] Accuracy obtained using the CE loss function
\item[$2$] Accuracy obtained using the proposed hybrid loss strategy
\end{tablenotes}
\end{threeparttable}
\label{applicability}
\end{table}
  
\subsection{Discussion}
In this paper, we introduced a new approach combining a new learning model and a novel hybrid loss strategy for early KOA detection (KL-0 vs KL-2). Our proposed learning model is a Siamese-based network, where GAP layers were integrated to consider and merge different levels of features. The proposed hybrid loss strategy has three main steps. Firstly, in each training batch, the samples of each KL grade are sorted according to their confidence levels computed through the probability distribution. Then, the training batch is divided into high-, medium- and low-level confidence sets with a partition ratio $\lambda$. Finally, different loss functions (LSCE, KLD and CE) with optimized weights $\alpha$, $\beta$ and $\gamma$ are used to better adapt for sets with different confidence levels. 

Our experimental findings confirmed that the Siamese-based model has a higher ability of feature extraction compared to the common CNN models. Previous work has already proved the applicability of such models to detect KOA. In \cite{tiuplin}, Tiulpin et al. diagnosed the full KOA-grade (i.e., from KL-0 to KL-4) using the Siamese-based model, their method achieved the highest multi-class classifcation results. Inspired by their work, by integrating several GAP layers at various levels of the model, we were able to improve the feature extraction performance of the proposed model.

\subsubsection{Details of the proposed model}
As presented in Section \ref{learning_model}, instead of using max pooling layers for down-sampling, we used stride and more convolutional layers to preserve more spatial information, which increased the nunber of parameters to a certain extant compared to the work of Tiulpin et al. \cite{tiuplin}. We also evaluated the binary classification of KL-0 vs KL-2 on \cite{tiuplin} modifying the output units of their Siamese-based model from five to two. Using the same dataset, the accuracy reached 86.2$\%$ with 0.15$M$ parameters. As shown in Table \ref{comparision}, our proposed Siamese-GAP model achieved an accuracy of 88.38$\%$ with 2.71$M$ parameters. It is noteworthy that our model only needs a very small number of parameters compared to common evaluated CNN models (i.e., DenseNet, ResNet, and VGG). Hence, compared to \cite{tiuplin}, the greater number of parameters in our proposed model appears to be acceptable, given the significant improvement in performance.

\subsubsection{Comparison of the hybrid loss strategy}
In \cite{wang},  Wang et al. exchanged the features of high confidence samples using peer models to emphasize the high confidence set and suppress the low confidence one. According to their training flowchart, the features of high-confidence samples were simply trained twice. Therefore, to be more significant, they used the weights of the high- and low-confidence sets as the proportions of these two sets, and introduced a hyper-parameter $\lambda$. However, the contribution of their hybrid loss strategy to the performance improvement cannot be evaluated independently and precisely due to the two stacked models. In comparison, our proposed model and hybrid loss strategy are designed and evaluated independently. Assessing the impact of the different weight combinations on performance makes the contribution of each confidence set to the classification performance more intuitive, which greatly improves the interpretability and the understanding of the whole approach.

\subsubsection{Strengths and limitations}
This study presents several important strengths. From a clinical point of view, our proposed approach not only performs better than the compared models, but it is more clinically meaningful. We mainly focused on the detection of early KOA (i.e., KL-0 vs KL-2) to inform patients and make timely physical interventions to delay the onset and worsening of KOA symptoms. We also provided attention maps as complementary information for model decisions. We believe that providing such attention map in a CAD system can make the decision-making process more transparent, and the deep learning approach will therefore gain more trust and acceptance from medical practitioners in clinical practice. In comparison to the task as full classification using CE loss, the experimental results show that our proposed hybrid loss strategy, which adjusts and smooths the confidence level of the overall data, significantly improves the detection performance. Furthermore, we introduced the concept of regression into classification tasks that provides smoother probabilistic information indicating that KOA severity should be considered continuously. Subsequently, inexperienced practitioners will be greatly benefited and save their training time.

There were also several limitations in our study. 
%All experimental data we used were from OAI, and did not include commonly used dataset MOST. As a result, the model's capacity to extract pertinent KOA characteristics that can be applied to other datasets is not evaluated. 
We reduced the bit depth of the X-ray image from 24-bit to 8-bit, which may have resulted in the loss of the fine-grained information. Moreover, manual tuning of the hyper-parameters in the hybrid loss strategy consumes a large amount of machine resources and time. Therefore, dynamic and automatic adjustment of the weights could be of interest for the future work. It would also be more interesting to partition the samples more smartly and precisely by computing the ratio $\lambda$ automatically.

\section{Acknowledgements}
The authors would like to express their gratitude to the French National Research Agency (ANR) for supporting their work through the ANR-20-CE45-0013-01 project.

This manuscript was prepared using OAI data and does not necessarily reflect the opinions or views of the OAI investigators, the NIH, or the private funding partners. The authors would like to thank studies participants and clinical staff as well as the coordinating center at UCSF.

\bibliographystyle{unsrt}  
\bibliography{references}
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.
%%% Comment out this section when you \bibliography{references} is enabled.
\end{document}
