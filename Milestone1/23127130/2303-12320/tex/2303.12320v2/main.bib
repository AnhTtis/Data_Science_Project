% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz


@inproceedings{lin-etal-2019-kagnet,
    title = "{K}ag{N}et: Knowledge-Aware Graph Networks for Commonsense Reasoning",
    author = "Lin, Bill Yuchen  and
      Chen, Xinyue  and
      Chen, Jamin  and
      Ren, Xiang",
    booktitle = "Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    url = "https://aclanthology.org/D19-1282",
    doi = "10.18653/v1/D19-1282",
    pages = "2829--2839",
}


@article{https://doi.org/10.48550/arxiv.1909.11942,
  journal = {arXiv:1909.11942},
  
  url = {https://arxiv.org/abs/1909.11942},
  
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},

  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  
  year = {2019},
}

@ARTICLE{4700287,  
author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},  
journal={IEEE Transactions on Neural Networks},   
title={The Graph Neural Network Model},   
year={2009},  
volume={20},  
number={1},  
pages={61-80},  
doi={10.1109/TNN.2008.2005605}
}

@inproceedings{bollacker2008freebase,
  title={Freebase: a collaboratively created graph database for structuring human knowledge},
  author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle={ACM SIGMOD International Conference on Management of Data},
  pages={1247--1250},
  year={2008}
}
@inproceedings{vrandevcic2012wikidata,
  title={Wikidata: A new platform for collaborative data collection},
  author={Vrande{\v{c}}i{\'c}, Denny},
  booktitle={International Conference on World Wide Web (WWW)},
  pages={1063--1064},
  year={2012}
}
@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2017}
}
@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}
@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? A new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv:1809.02789},
  year={2018}
}
@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}
@inproceedings{
zhang2022greaselm,
title={Grease{LM}: Graph {REAS}oning Enhanced Language Models},
author={Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D Manning and Jure Leskovec},
booktitle={International Conference on Learning Representations (ICLR)},
year={2022},
url={https://openreview.net/forum?id=41e9o6cQPj}
}

@inproceedings{welling2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Welling, Max and Kipf, Thomas N},
  booktitle={J. International Conference on Learning Representations (ICLR 2017)},
  year={2016}
}
@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735â€“1780},
numpages = {46}
}
@inproceedings{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}
@inproceedings{schlichtkrull2018modeling,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Berg, Rianne van den and Titov, Ivan and Welling, Max},
  booktitle={European Semantic Web Conference},
  pages={593--607},
  year={2018},
}
@inproceedings{wang2019improving,
  title={Improving natural language inference using external knowledge in the science questions domain},
  author={Wang, Xiaoyan and Kapanipathi, Pavan and Musa, Ryan and Yu, Mo and Talamadupula, Kartik and Abdelaziz, Ibrahim and Chang, Maria and Fokoue, Achille and Makni, Bassem and Mattei, Nicholas and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  pages={7208--7215},
  year={2019}
}
@article{https://doi.org/10.48550/arxiv.2010.11784,
  journal = {arXiv:2010.11784},
  
  url = {https://arxiv.org/abs/2010.11784},
  
  author = {Liu, Fangyu and Shareghi, Ehsan and Meng, Zaiqiao and Basaldella, Marco and Collier, Nigel},
  
  title = {Self-Alignment Pretraining for Biomedical Entity Representations},
  
  year = {2020},
}

@article{https://doi.org/10.48550/arxiv.1703.05851,
  journal = {arXiv:1703.05851},
  
  url = {https://arxiv.org/abs/1703.05851},
  
  author = {Meng, Yuanliang and Rumshisky, Anna and Romanov, Alexey},

  title = {Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture},

  year = {2017},
}


@INPROCEEDINGS{7980526,  
author={Kahaduwa, Hasangi and Pathirana, Dilshan and Arachchi, Pathum Liyana and Dias, Vishma and Ranathunga, Surangika and Kohomban, Upali},  
booktitle={Moratuwa Engineering Research Conference (MERCon)},   
title={Question Answering system for the travel domain},
year={2017},  
volume={},  
number={},  
pages={449-454},  
doi={10.1109/MERCon.2017.7980526}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2019",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{10.1093/bioinformatics/btz682,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    title = "{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}",
    journal = {Bioinformatics},
    volume = {36},
    number = {4},
    pages = {1234-1240},
    year = {2019},
    month = {09},
    abstract = "{Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\\% F1 score improvement), biomedical relation extraction (2.80\\% F1 score improvement) and biomedical question answering (12.24\\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz682},
    url = {https://doi.org/10.1093/bioinformatics/btz682},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/32527770/btz682.pdf},
}


@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {{RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach}},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yasunaga-etal-2021-qa,
    title = "{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering",
    author = "Yasunaga, Michihiro  and
      Ren, Hongyu  and
      Bosselut, Antoine  and
      Liang, Percy  and
      Leskovec, Jure",
    booktitle = "North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.45",
    doi = "10.18653/v1/2021.naacl-main.45",
    pages = "535--546",
    abstract = "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.",
}
@inproceedings{feng-etal-2020-scalable,
    title = "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
    author = "Feng, Yanlin  and
      Chen, Xinyue  and
      Lin, Bill Yuchen  and
      Wang, Peifeng  and
      Yan, Jun  and
      Ren, Xiang",
    booktitle = "Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    url = "https://aclanthology.org/2020.emnlp-main.99",
    doi = "10.18653/v1/2020.emnlp-main.99",
    pages = "1295--1309",
    abstract = "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model{'}s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.",
}

@article{Honnibal_spaCy_Industrial-strength_Natural_2020,
author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
journal = {10.5281/zenodo.1212303},
title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
year = {2020}
}

@article{journals/corr/cs-CL-0205028,
  added-at = {2020-01-10T00:00:00.000+0100},
  author = {Loper, Edward and Bird, Steven},
  biburl = {https://www.bibsonomy.org/bibtex/2eac35636d7e2bb4a0264313ed0791372/dblp},
  ee = {https://arxiv.org/abs/cs/0205028},
  interhash = {1af05e5f1cea0feeea8da5f68707a841},
  intrahash = {eac35636d7e2bb4a0264313ed0791372},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2020-01-11T11:43:05.000+0100},
  title = {{NLTK: The Natural Language Toolkit}},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr0205.html#cs-CL-0205028},
  volume = {cs.CL/0205028},
  year = 2002
}

@inproceedings{conf/iclr/LanCGGSS20,
  added-at = {2021-05-21T12:19:07.000+0200},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  biburl = {https://www.bibsonomy.org/bibtex/289f1672459805fbdb093117ac4905028/lea-w},
  booktitle = {ICLR},
  ee = {https://openreview.net/forum?id=H1eA7AEtvS},
  interhash = {7bd1a54a73ac5dfa6984702365d9f2a9},
  intrahash = {89f1672459805fbdb093117ac4905028},
  keywords = {},
  publisher = {OpenReview.net},
  timestamp = {2021-05-21T12:19:07.000+0200},
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2020.html#LanCGGSS20},
  year = 2020
}


@inproceedings{wang-etal-2020-connecting,
    title = "Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering",
    author = "Wang, Peifeng  and
      Peng, Nanyun  and
      Ilievski, Filip  and
      Szekely, Pedro  and
      Ren, Xiang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.369",
    doi = "10.18653/v1/2020.findings-emnlp.369",
    pages = "4129--4140",
    abstract = "Commonsense question answering (QA) requires background knowledge which is not explicitly stated in a given context. Prior works use commonsense knowledge graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely on these KGs may not suffice, considering their limited coverage and the contextual dependence of their knowledge. In this paper, we augment a general commonsense QA framework with a knowledgeable path generator. By extrapolating over existing paths in a KG with a state-of-the-art language model, our generator learns to connect a pair of entities in text with a dynamic, and potentially novel, multi-hop relational path. Such paths can provide structured evidence for solving commonsense questions without fine-tuning the path generator. Experiments on two datasets show the superiority of our method over previous works which fully rely on knowledge from KGs (with up to 6{\%} improvement in accuracy), across various amounts of training data. Further evaluation suggests that the generated paths are typically interpretable, novel, and relevant to the task.",
}



@article{clark2019aristoroberta,
  journal = {arXiv:1909.01958},
  url = {https://arxiv.org/abs/1909.01958},
  author = {Clark, Peter and Etzioni, Oren and Khashabi, Daniel and Khot, Tushar and Mishra, Bhavana Dalvi and Richardson, Kyle and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind and Tandon, Niket and Bhakthavatsalam, Sumithra and Groeneveld, Dirk and Guerquin, Michal and Schmitz, Michael},
  title = {{From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project}},
  year = {2019},
}

@article{https://doi.org/10.48550/arxiv.1909.05311,
  journal = {arXiv:1909.05311},
  
  url = {https://arxiv.org/abs/1909.05311},
  
  author = {Lv, Shangwen and Guo, Daya and Xu, Jingjing and Tang, Duyu and Duan, Nan and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Hu, Songlin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering},
 
  year = {2019},
}

@article{https://doi.org/10.48550/arxiv.1910.14087,
  journal = {arXiv:1910.14087},
  
  url = {https://arxiv.org/abs/1910.14087},
  
  author = {Ma, Kaixin and Francis, Jonathan and Lu, Quanyang and Nyberg, Eric and Oltramari, Alessandro},

  title = {Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering},

  year = {2019},
}

@article{https://doi.org/10.48550/arxiv.1909.11764,
  journal = {arXiv:1909.11764},
  
  url = {https://arxiv.org/abs/1909.11764},
  
  author = {Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
  
  title = {{FreeLB: Enhanced Adversarial Training for Natural Language Understanding}},
 
  year = {2019},
}


@article{https://doi.org/10.48550/arxiv.2005.00700,
  journal = {arXiv:2005.00700},
  
  url = {https://arxiv.org/abs/2005.00700},
  
  author = {Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  title = {UnifiedQA: Crossing Format Boundaries With a Single QA System},
  year = {2020},

}
