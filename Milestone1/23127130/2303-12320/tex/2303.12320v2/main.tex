% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
%\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times, graphicx, footmisc, layouts}
\usepackage{latexsym,multirow}
% \usepackage{fontspec}
%Set the main languge next as main font.
% \newfontfamily\devanagarifont[Script=Devanagari]{Lohit Devanagari}
% \newfontfamily{\telugu}{Lohittelugu}[
% Path = ./Fonts/,
% Extension = .ttf,
% ]
% \newfontfamily{\tamil}[Script=Tamil]{Lohit Tamil}
% \newfontfamily{\gujarati}[Script=Gujarati]{Lohit Gujarati}
% \newfontfamily{\bengali}[Script=Bengali]{Lohit Bengali}
% \newfontfamily{\kannada}[Script=Kannada]{Lohit Kannada}
% % For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% \setmainlanguage{english}
% \setotherlanguages{hindi}
% \usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\data}{\textsc{XWikiRef}}
\newcommand{\task}{\textsc{XWikiGen}}

\newcommand{\spacing}{6pt}
\renewcommand\textfloatsep{\spacing}
\renewcommand\floatsep{\spacing}
\renewcommand\intextsep{\spacing}
\renewcommand\dbltextfloatsep{\spacing}
\renewcommand\dblfloatsep{\spacing}

%defining local variable
\def\langCount{8}
\def\lrLangCount{7}
\def\goldenCount{5402} %but which change once annotation is complete
\def\totalDataCount{0.45M}
\setlength{\tabcolsep}{4pt}

\usepackage{times}
\usepackage{latexsym}
\usepackage{svg}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{graphicx}
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{comment}

\let\aclpara\paragraph
\renewcommand{\paragraph}[1]{\vspace{1mm}\noindent\textbf{#1}}

% \newcommand{\bs}{\mathbf{s}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\brho}{\boldsymbol{\rho}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\mcG}{\mathcal{G}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\mcE}{\mathcal{E}}
\newcommand{\mcR}{\mathcal{R}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\roberta}{RoBERTa-large}
\newcommand{\aristo}{AristoRoBERTa-Large}
% MT: See the commands above to simplify writing bold face vectors \bb or calligraphy "sets" \mcF

\newcommand{\methodname}{GrapeQA}

\newcommand{\argmax}{\mathop{\mathrm{argmax}}}  


% properly add the dot after common acronyms
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

% \def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} \def\wolog{w.l.o.g\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\title{GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Tushar Abhishek, Daksh Rawat, Manish Gupta\thanks{The author also works as a researcher at Microsoft}~ and Vasudeva Varma \\
%         tushar.abhishek@research.iiit.ac.in, daksh.rawat@students.iiit.ac.in, \\
%         \{manish.gupta, vv\}@iiit.ac.in} 

% \author{Dhaval Taunk, Shivprasad Sagare, Anupam Patil, Shivansh Subramanian,\\ {\bf Manish Gupta and Vasudeva Varma} \\ Information Retrieval and Extraction Lab, IIIT Hyderabad, India \\ \{dhaval.taunk,shivprasad.sagare,shivansh.s\}@research.iiit.ac.in,anupampatil44@gmail.com\\ \{manish.gupta,vv\}@iiit.ac.in}

\author{Dhaval Taunk$^*$, Lakshya Khanna$^*$, Pavan Kandru$^*$,\\ {\bf Vasudeva Varma, Charu Sharma and Makarand Tapaswi} \\
IIIT Hyderabad, India\\ \{dhaval.taunk,lakshya.khanna,siri.venkata\}@research.iiit.ac.in\\ \{vv,charu.sharma,makarand.tapaswi\}@iiit.ac.in}

\begin{document}
\maketitle
\begin{abstract}
Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG).
A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks (GNNs).
This faces two major challenges:
(i) it is difficult to capture all the information from the QA in the WG, and
(ii) the WG contains some irrelevant nodes from the KG.
To address these, we propose \methodname{} with two simple improvements on the WG:
(i) Prominent Entities for Graph Augmentation
%\underline{P}rominent \underline{E}ntities for \underline{G}raph \underline{A}ugmentation (PEGA) 
identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and
(ii) Context-Aware Node Pruning
% \underline{C}ontext-\underline{A}ware \underline{N}ode \underline{P}runing (CANP)
removes nodes that are less relevant to the QA pair.
We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that \methodname{} shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improvements on OpenBookQA.
\end{abstract}

\section{Introduction}
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work.}
\def\thefootnote{\arabic{footnote}}

% \section{Introduction}
% \label{sec:intro}
% In the NLP domain, research has been ongoing for a long time in order to solve the question-answering challenge. Models have evolved from the rule based approach to intelligent RNN based models and eventually to transformer based SOTA pre-trained language models. With the advent of GNN, modelling text problems through graphs has become an active area of research. Language models are great at capturing the implicit patterns and contextual information within the data while KGs have been able to spatially capture the relation between the entities of the text. Significant amount of work has already been done in leveraging the power of both to enhance the QA systems. Previously, both LM and KGs' were trained independently and the information was fused from both to make a performance efficient model. Thereafter, several methods have been introduced to perform joint learning on both LM and KG. QA-GNN by \todo{add citation} et al has been one of the models that first introduced such idea. We have further extended the idea of QA GNN by augmenting it with more information from text which we term as GrapeQA.

%In the NLP domain, research has been ongoing for a long time to solve the question-answering challenge. Models have evolved from the rule based approach to neural network based models and eventually to transformer based SOTA pre-trained language models.

% Question-answering systems come under the umbrella of one of the most challenging problems in NLP space.
Answering questions is a challenging NLP problem as it involves understanding the question context and sifting through relevant information to identify the answer.
% is crucial to performance of question-answering system.
Question-answering models have evolved from rule-based~\cite{7980526} to RNN-based sequence models~\cite{https://doi.org/10.48550/arxiv.1703.05851} and now to Transformer-based Language Models (LM) such as \roberta~\cite{DBLP:journals/corr/abs-1907-11692}.
% or AlBERT~\cite{https://doi.org/10.48550/arxiv.1909.11942}.
% were proven to be state-of-the-art for solving such problems.
However, commonsense question-answering
% (see Fig.~\ref{fig:example})
adds a layer of complexity as the model needs to reason about questions relating diverse topics, making the task challenging for LMs that may not have seen something similar in the pre-training data.

% Graph Neural Network (GNNs)~\cite{welling2016semi}, \cite{4700287}, have gained prominence on a wide variety of modeling tasks.
While LMs capture the implicit patterns and contextual information within the data, KGs are able to capture explicit relations between the text entities.
KGs such as
Freebase~\cite{bollacker2008freebase},
Wikidata~\cite{vrandevcic2012wikidata}, or
ConceptNet~\cite{speer2017conceptnet} store knowledge in the form of graph triplets (topic-relationship-topic) and are well suited for Graph Neural Networks (GNNs), \eg ~\cite{welling2016semi}.
Thus, commonsense QA in particular has attracted interest in combining LMs and KGs with the reasoning ability of GNNs~\cite{lin-etal-2019-kagnet,yasunaga-etal-2021-qa}.


% \begin{figure}[t]
% \centering
% \vspace{-8mm}
% \includegraphics[width=8cm, height=9cm]{figures/eqfinal (4).pdf}
% \caption{Working Graph augmented with PEGA.}
% \vspace{-6mm}
% \label{fig:example}
% \end{figure}


Most works on LM + KG extract a sub-graph or Working Graph (WG) from the KG based on concepts mentioned in the QA pair~\cite{lin-etal-2019-kagnet,feng-etal-2020-scalable,yasunaga-etal-2021-qa} and focus on improving reasoning.
For example, \citet{lin-etal-2019-kagnet}~propose a graph network to score answers while~\citet{feng-etal-2020-scalable} focus on a multi-hop message passing framework that allows each node to attend to multi-hop neighbors in a single layer, combining interpretable path-based reasoning with scalable GNNs.
\citet{yasunaga-etal-2021-qa}~improve the extracted WG through a relevance scoring mechanism followed by joint reasoning and~\citet{zhang2022greaselm} fuse information from both the modalities (LM, KG) by mixing their tokens and nodes.

% In some existing works, the LM and KG are trained independently and the information is fused to make efficient models.
% \citet{lin-etal-2019-kagnet} create a ``schema graph'', a sub-graph from the KG
% introduce graph reasoning by creating a sub-graph by
% by extracting mentioned concepts from the QA pair.
% They propose a knowledge-aware graph network and score answers with graph representations. 
% then identify connections between the two mentioned concepts and build the schema graph.
% They performed the experiments by training a GCN + LSTM + HPA\footnote{HPA - Hierarchical path-based attention mechanism} model on the CommonSenseQA dataset. 
% \citet{feng-etal-2020-scalable} introduces a multi-hop graph reasoning module that unifies interpretable path-based reasoning methods and scalable GNNs.
% The multi-hop message passing framework allows each node to directly attend to multi-hop neighbors in a single layer.
% introduce a node type aware model in which they perform node type linear transformation on the input node features. Next,
% multi-hop message passing is used to make the GNN capable of directly modeling the paths and the parameterizing the attention score to perform classification. 
%
% After that, several methods were introduced to perform joint learning of both LM and KG. 
% Similar to~\cite{feng-etal-2020-scalable}, QA-GNN~\cite{yasunaga-etal-2021-qa} also extracts a ``working graph'' (WG), a sub-graph from the KG and performs joint reasoning with the question-answer (QA) context encoded using an LM.
% jointly learning the representations of QA context and entities from KG.
% A relevance scoring mechanism uses LMs to estimate the importance of KG nodes relative to the QA context a GNN performs reasoning over the WG.
% QA-GNN proposes a pre-trained language model (LM) + knowledge graph (KG) based approach that performs joint reasoning (using a GNN) over the QA context and a given knowledge graph. It extracts relevant information from the knowledge graph to solve the question-answering problem.
%
% After that, GreaseLM \cite{zhang2022greaselm} proposed cross-modal fusion component to fuse information from KG into Language representations and from language representations to KG. It encodes both the modalities separately and then fuses their learned representations.
% Recently, GreaseLM~\cite{zhang2022greaselm} introduces an LM + KG model where they use cross-modal fusion to fuse data from the KG into language representations and from language representations to the KG.
% It individually encodes both modalities before fusing their learned representations. 

%Recently, QA-GNN by \cite{yasunaga-etal-2021-qa} has been one of the models that first introduced such an idea. QA-GNN proposes a pre-trained language model (LM) + knowledge graph (KG) based approach that performs joint reasoning over the QA context and a given knowledge graph, as well as attempts to extract relevant information from the knowledge graph to solve the question-answering problem. The authors combine QA context and knowledge graph to perform joint reasoning and use graph neural networks to learn the representations simultaneously.

% Previous work on this task have also experimented joint training of the pre-trained language models and graphs neural networks along with introducing some different novel techniques. 

% We have further extended the concept of QA GNN by first augmenting it with more relevant information and then remove the less relevant information from text which we term as GrapeQA.

% Here 1 or 2 paragraphs can be added as related work if we want to remove the related work as a separate section.
% Short description of QAGNN


% Novelty introduced. Just a rough paragraph. Need to enhance it.

% In this paper, we propose GrapeQA which consists of graph augmentation and pruning with LM + KG based joint learning approach. We first augment the graph by adding the noun chunks as additional information. We also prune the working graph to remove the less relevant information. 
Our emphasis with GrapeQA lies in improving the working graph (WG) with two simple ideas.
(i)~We augment the WG with useful information from the question-answer pair reducing the burden on a single QA context node used in previous works.(discussed in \ref{22})
(ii)~Instead of keeping all nodes of the WG, or simply scoring relevance, we drop less relevant information (nodes) from the WG simplifying the graph reasoning process.
The improvements to the WG are combined with the reasoning process of QA-GNN~\cite{yasunaga-etal-2021-qa} and evaluated on three datasets, where we see especially large improvements on domain-specific OpenBookQA (discussed in \ref{23}).

% \item We evaluate our model on OpenBookQA, CommonSenseQA and MedQA-USMLE datasets and outperform the recent LM + KG approaches.
% \end{enumerate}



% We experiment with this approach and found slight improvement on CSQA and MedQA datasets while a significant improvement of results on OBQA dataset.

% The purpose of this technique is to introduce the new nodes to the graph that adds additional relevant information to the input graph.
% \glipsum{1-4}

% Building on top of 3_model_merge
\section{GrapeQA Methodology}

\begin{figure*}[t!]
% \vspace{2mm}
% \hspace{-1mm}
\centering %0.485
\includegraphics[width=0.95\textwidth]{figures/croppedmerged.pdf}
% \vspace{-8.5mm}
\vspace{-3mm}
\caption{Method overview showing the approach to score the question with each answer option.
GrapeQA improves QA-GNN~\cite{yasunaga-etal-2021-qa} by augmenting the Working Graph with additional nodes that capture information from the QA pair (step 4: PEGA) and then pruning the graph to remove the least relevant nodes (step 5: CANP).}
% \vspace{-4mm}
\label{fig:model_overview}
\end{figure*}
% \vspace{1mm}

% We briefly describe the QA-GNN approach as our work improves the WG used in the method.
% Thereafter, we describe proposed approaches for graph augmentation and pruning.
We briefly describe the QA-GNN approach before our graph augmentation and pruning strategies.
% Thereafter, we describe proposed approaches for graph augmentation and pruning.


\subsection{LM + KG: QA-GNN as a case study}
The objective of QA-GNN~\cite{yasunaga-etal-2021-qa} is to use both LM and KG for commonsense QA tasks.
Each multiple-choice QA consists of a question $q$ and $O$ answer options $\{a_o\}_{o=1}^O$ where only one is correct.
We create one Working Graph (WG) per answer option and reason over the graph to produce a score.
During training, cross-entropy loss is applied to scores of all answer options while we pick the highest scoring answer for inference.
% \MT{confirm, cross-entropy correct? yes}
% Softmax is used on the probability scores of each answer and the option with highest probability is selected.
% The WG is constructed by extracting nodes from KG.
% For GrapeQA, we are using pre-built concept net KG graph.

We discuss the WG creation process starting with the KG.
Let $\mcG = (\mcV, \mcE)$ be the KG with $\mcV$ nodes and a set of edges $\mcE \subseteq \mcV \times \mcR \times \mcV$ with $\mcR$ relation types. 
% $\mcG = (\mcV, \mcE)$ contains nodes $\mcV$, and $\mcR$ relation types of edges connecting them, $\mcE \subseteq \mcV \times \mcR \times \mcV$.
For a given question-answer pair $[q; a_o]$, all nodes in the KG may not be relevant.
Hence, \emph{Question / Answer entity nodes}, referred as $q_\text{KG}$ or $a_\text{KG}$, that have some text matching with the question $q$ or answer option $a_o$ are picked.
% Nodes with their text matching with question text and answer choice text are selected as Question nodes and Answer nodes, respectively. 
Indirect relations between Question and Answer entity nodes are captured through common neighbors (2-hop away) by including them as \emph{Extra nodes} $s_\text{KG}$.
% . These new nodes are also captured in the WG, $\mcG_w$ along with Question and Answer nodes and are termed as Extra Nodes.
The sub-graph $\mcG_\text{sub}$ is formed together with the edges in $\mcE$ that connect the chosen KG nodes.
In summary, the nodes of the sub-graph are $\{q_\text{KG}\} \cup \{a_\text{KG}\} \cup \{s_\text{KG}\}$.

% To create the WG for one answer option, we first add a \emph{QA context} node which represents the concatenated question and answer option to the subgraph.
% step in the construction of Working graph is to generate a Question and Answer Context (QA context).
% The QA context is the concatenation of question and answer option.
% For each question, there are $O$ (number of answer choices) QA contexts.
Next, a relevance scoring mechanism is used to prune irrelevant nodes that may appear in the sub-graph.
% Relevance scoring mechanism is used to prune them.
Scores are computed by encoding the \emph{QA context} (concatenated question and answer option text) and node label using an LM followed by a linear projection.
% A relevance score for each node in a subgraph is calculated by appending each node label with QA context. Then, \roberta-large model is used to compute the score for each node.
The relevance score influences the node representation in the sub-graph.
% Nodes with relevance score lower than a threshold are removed from the sub-graph.
Finally, to create the Working Graph $\mcG_w$, \emph{QA context} is added as a node to the sub-graph and connected with other nodes using a new edge type.
% All the connected edges to or from these nodes are also removed.
% Next, QA context node is added as an additional node in the sub-graph to make the final working graph. This context node is connected to all the other nodes with special relation type. 

Question, Answer, and Extra nodes in $\mcG_w$ are initialized by creating sentences based on triplets from the KG, feeding them to a pretrained LM, and average pooling over relevant tokens (see~\cite{feng-etal-2020-scalable} for details).
% \MT{can we describe this in 1-2 sentences? 
% Using templates knowledge triples in ConceptNet are turned into
% sentences and fed to pre-trained BERT model to obtain a sequence of token embeddings for each triple.
% For each entity in ConceptNet, we perform mean
% pooling over the tokens of the entityâ€™s occurrences
% across all the sentences to form a vector as
% its corresponding node representation.}
The QA context node is initialized as $\bz$, an encoding of the $[q; a_o]$ text using an LM.
To perform reasoning, a relation type aware Graph Network is adopted.
The output representations for all nodes are pooled and added to the LM's original encoding of the QA context.
Finally, an MLP is used to predict a score for the correctness of the answer option.
% The WG is given as an input to the network.
Fig.~\ref{fig:model_overview} illustrates QA-GNN along with our proposed modifications.
Additional details of QA-GNN are in App.~\ref{app:qagnn}.


\subsection{Graph Augmentation and Pruning}
\methodname{} proposes two improvements to the WG and corresponding adaptations to QA-GNN.
We overcome the limited capacity of the WG to exchange useful information between the QA context and the KG with \underline{P}rominent \underline{E}ntities for \underline{G}raph \underline{A}ugmentation (PEGA) that introduces additional nodes from the QA pair to the WG.
% These new nodes solve the information bottleneck caused by using a single node in the LM.
We also propose QA-\underline{C}ontext-\underline{A}ware \underline{N}ode \underline{P}runing (CANP), a pruning method that removes least relevant nodes.
% Please refer to Fig.~\ref{fig:contextualization} for an overview of this process.


\paragraph{Prominent Entities for Graph Augmentation (PEGA).}
\label{22}
Graph augmentation begins by extracting noun phrase chunks $c$ from the question and answer pair $[q; a_o]$.
We use Spacy's~\cite{Honnibal_spaCy_Industrial-strength_Natural_2020} \emph{noun} chunk extractor $f_\text{ext}$ to obtain
\begin{equation}
\mcV^{\prime}\ =\{c \mid c \in f_\text{ext}([q; a]) \} \, .
\end{equation}
The QA context is fed as input to the LM and representations of all the sub-word tokens are obtained.
Each extracted noun phrase is represented by averaging over the embeddings of its sub-word tokens.
As part of augmentation, these \emph{noun chunks nodes} ($\mcV^\prime$) are added as new nodes of type $n$ to the working graph $\mcG_{w}$.
\emph{Noun chunk nodes} also have two types of edges: $r_{no}$ between all the new ($\mcV^\prime$) and old $\mcG_w$ nodes, and $r_{nn}$ among the noun chunks themselves resulting in an augmented WG, $\mcG_w^\prime$:
\begin{eqnarray}
&\mcE^{\prime}\ = \{\mcV^\prime \times r_{nn} \times \mcV^\prime\} \cup \{\mcV^\prime \times r_{no} \times \mcV\} \, , \\
&\mcG_{w}^{\prime} = (\mcV \cup \mcV^{\prime},  \mcE \cup \mcE^{\prime}) \, .
\end{eqnarray}

% Detailed Analysis of $\mcG_w^\prime$ is given in Apeendix \ref{app:graphstats}
% Finally the augmented WG is
% \begin{equation}
% \mcG_{w}^{\prime} = (\mcV \cup \mcV^{\prime},  \mcE \cup \mcE^{\prime}) \, .
% \end{equation}


\paragraph{QA Context-Aware Node Pruning (CANP)}
\label{23}
aims to remove the less relevant nodes from the WG.
Our intuition is that some Extra nodes (i.e.~2-hop neighbors from the KG which do not match the QA text) may be less relevant to the QA as compared to the Question / Answer entity nodes.

To perform pruning, we first associate and cluster Extra nodes with \emph{Answer entity nodes}.
CANP is only applied when there are more than one Answer entity nodes.
Recall that the WG is created for one answer option (or one QA pair) and the number of Answer entity nodes (and clusters) depends on the number of nodes with text similar to the answer option in the KG.
% This means total number of clusters will be equal to the total number answer entities. Note that the number of answer entities is not equal to the number of answer choices. It can be more in number as well.
Similar to relevance scoring in QA-GNN, we calculate the relevance score for each Extra node $s_\text{KG}$ against each Answer entity $a_\text{KG}$ by encoding the concatenated text of the QA pair, the Answer entity, and the Extra node.
\begin{equation}
\psi_{sa}^\text{KG} = f_{\text{head}} \left( \text{LM} \left( [\text{text}(\bz); \text{text}(\ba_\text{KG}); \text{text}(\bs)] \right) \right) \, ,
\end{equation}
where $\text{text}(\cdot)$ corresponds to the node's label text:
$[q; a_o]$ pair for $\bz$,
Extra node's label $s_\text{KG}$ for $\bs$, and
the Answer entity label $a_\text{KG}$ for $\ba_\text{KG}$.
% This concatenated representation is provided as input to the \roberta-large model.
% Negative of Cross-Entropy loss is then used as a metric to calculate the relevance score for the extra node with respect to each answer entity. 
Thus, each Extra node $s_\text{KG}$ is assigned to the cluster $\mcV_x$ corresponding to the highest relevance score,
\begin{equation}
\mcV_{x} = \{s_\text{KG} \, | \, x = \arg\,\max_{a_\text{KG}} \, \psi_{sa}^\text{KG} \} \, .
\end{equation}
We compute the average relevance score for each cluster and identify the least relevant cluster $\mcV_r$ as
\begin{eqnarray}
\psi_x^\text{KG} &=& \sum_{s_\text{KG} \in \mcV_x} \psi_{sx}^\text{KG} / |\mcV_x| \, , \\
\mcV_{r} &=& \mcV_{x} \,\, \text{ s.t. } \, r = \arg\,\min_x \psi_x^\text{KG} \, .
\end{eqnarray}
% \MT{$\mcV_r$ should also include $x$ if answer entity node is being removed}
Finally, we remove the cluster with lowest average relevance score from the WG before continuing with graph-based reasoning.
The PEGA augmented WG can be pruned as
\begin{equation}
\mcG_{w}^{\prime\prime} = (\mcV \cup \mcV^{\prime} -\mcV_{r} , \mcE \cup \mcE^{\prime} - \{\mcV_{r} \times R \times \mcV\}).
\end{equation}


\section{Experiments}
\label{sec:experiments}

\begin{table}[b]
\caption{Average number of nodes of each type in WGs.}
\centering
% \small
\tabcolsep=0.12cm
% \vspace{-3mm}
\begin{tabular}{lccc}
\toprule
\textbf{Node Type} & \textbf{OBQA}  & \textbf{CSQA}  & \textbf{MedQA}  \\
\midrule
Question entity $q_\text{KG}$  & 6.52          & 7.36          & 6.1             \\
Answer entity $a_\text{KG}$    & 2.79          & 2.05          & 0.55            \\
Extra nodes $s_\text{KG}$    & 107.17        & 112.04        & 20.82           \\
Noun chunk nodes $\mcV^\prime$ & 3.88          & 4.13          & 33.46          \\
\bottomrule

\end{tabular}
% \vspace{-3mm}

% \vspace{-5mm}
\label{tab:nodecount}
\end{table}

% \begin{enumerate}
%     \item OBQA - clustering, entity, entity + clustering
%     \item CSQA - clustering, entity, entity + clustering
%     \item MedQA - entity
%     \item Description + Tables
% \end{enumerate}

% 1) Dataset \& baseline: We performed experiments on Dataset details
%     performance comparison with what - all previous ones
% 2) Implementation details
% 3) Comparison Methods - We compare our models with state-of-the-art
% baselines
% Tables:
% 1) Performance comparison on CommonsenseQA in-house split
% 2) Performance comparison on the CommonsenseQA official leaderboard.
% 3) Test accuracy on OpenBookQA.
% 4) Test accuracy on OpenBookQA leaderboard.

% 4) results 
%     Main Results
%     Ablation Studies

\begin{table*}[!t]
\caption{Comparison of Accuracy between LM+KG methods on the OpenBookQA, CommonsenseQA (left) and MedQA (right).}
\centering
\tabcolsep=0.12cm
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcc}
\toprule
& \bf OBQA & \bf CSQA \\
\textbf{Model} & \textbf{Test}  & \textbf{IHTest} \\
\midrule
RGCN~\small{\cite{schlichtkrull2018modeling}} & 62.45 & 68.4\\
GconAttn~\small{\cite{wang2019improving}} & 64.75 & 68.6\\
RN~\small{\cite{santoro2017simple}} & 65.20 & 69.1\\
MHGRN~\small{\cite{feng-etal-2020-scalable}} & 66.85 & 71.1\\
GreaseLM (AristoRoBERTa)~\small{\cite{zhang2022greaselm}} & \underline{84.8} & \underline{74.05}\\
\midrule
QA-GNN (\roberta)~\small{\cite{yasunaga-etal-2021-qa}} & 67.80 & 73.4 \\
\methodname{}: CANP (\textbf{Ours}) & 66.20 & \bf 74.94\\
\methodname{}: PEGA (\bf \textbf{Ours}) & 82.0 & 73.41 \\
\methodname{}: PEGA+CANP (\textbf{Ours}) & \bf 90.0 & 74.05 \\
\bottomrule
\end{tabular}
\quad
\begin{tabular}{lc}

\toprule
\multicolumn{2}{c}{\bf MedQA} \\
\textbf{Model} & \textbf{Test}\\
\midrule
BERT-base~\small{\cite{devlin-etal-2019-bert}} & 34.3 \\
BioBERT-base~\small{\cite{10.1093/bioinformatics/btz682}} & 34.1 \\
\roberta~\small{\cite{DBLP:journals/corr/abs-1907-11692}} & 35.0 \\
BioBERT-large~\small{\cite{10.1093/bioinformatics/btz682}} & 36.7 \\
SapBERT~\small{\cite{https://doi.org/10.48550/arxiv.2010.11784}} & 37.2 \\
GreaseLM~\small{\cite{zhang2022greaselm}} & \underline{38.5} \\
\midrule
QA-GNN~\small{\cite{yasunaga-etal-2021-qa}} & 38.0 \\ 
\methodname{}: (PEGA) (\bf \textbf{Ours}) & \bf 39.51 \\
& \\
\bottomrule

\end{tabular}
\end{adjustbox}



\label{tab:combined}
\end{table*}

% \begin{table*}[!t]
% \centering
% \tabcolsep=0.12cm
% \begin{adjustbox}{max width=\textwidth}

% \begin{tabular}{ccc}
% \toprule
%  & \multicolumn{2}{c}{ Accuracy } \\
% % \cline { 1 - 1 } \cline { 2 - 3 }
% \#layers & OBQA & CSQA \\
% \midrule
% 4 & 88.38 &  72.60\\ 
% 5 & \bf 90.00 &  \bf 74.05\\
% 6 & 88.96 & 71.88\\
% \bottomrule
% \end{tabular}
% \hspace{0.5cm}
% \begin{tabular}{ccc}
% \toprule
%  & \multicolumn{2}{c}{ Accuracy } \\
% % \cline { 1 - 1 } \cline { 2 - 3 }
% \#layers & OBQA & CSQA \\
% \midrule
% 4 & 83.20 & \bf 74.62\\ 
% 5 & \bf 82.00 &  73.41\\
% 6 & 81.40 & 73.17\\
% \bottomrule
% \end{tabular}
% \hspace{0.5cm}





% \begin{tabular}{cc}
% \toprule
% Noun chunk & Accuracy \\
% % \cline { 1 - 1 } \cline { 3 - 3 }
% extraction method & OBQA \\
% \midrule
% 20\% random words & 72.32\\ 
% NLTK~\small{\cite{journals/corr/cs-CL-0205028}} & 78.40 \\ 
% spaCy~\small{\cite{Honnibal_spaCy_Industrial-strength_Natural_2020}} & \bf 82.00 \\ 
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm}
% \caption{Ablation studies on CSQA IHtest set and OBQA test set using our model.
% \textbf{Left:} Impact of the number of GNN layers using the PEGA+CANP model.
% \textbf{Center:} Impact of the number of GNN layers using the PEGA only model. 
% \textbf{Right:} Impact of different noun chunk extraction methods on PEGA.}
% \vspace{-4mm}
% \label{tab:ablations}
% \end{table*}

\begin{table}[t]
\caption{PEGA Ablations: Impact of different noun chunk extraction methods on OBQA.}
\centering
\vspace{1mm}
\begin{tabular}{cc}
\toprule
\textbf{Noun chunk extraction method} & \textbf{Accuracy} \\
% \cline { 1 - 1 } \cline { 3 - 3 }
% extraction method & OBQA \\
\midrule
20\% random words & 72.32\\ 
NLTK~\small{\cite{journals/corr/cs-CL-0205028}} & 78.40 \\ 
spaCy~\small{\cite{Honnibal_spaCy_Industrial-strength_Natural_2020}} & \bf 82.00 \\ 
\bottomrule
\end{tabular}



\label{tab:noun_chunk_ablations}
\end{table}

% \subsection{Datasets}
We evaluate \methodname{} on three QA datasets: \\
% CommonsenseQA \cite{talmor2018commonsenseqa}, OpenBookQA \cite{mihaylov2018can}, and MedQA-USMLE \cite{jin2021disease}.
% \MT{I think we can really shorten these dataset descriptions if space is needed.. it's pretty standard mostly}
\textbf{1.~CommonsenseQA} (CSQA) is 5-way multiple-choice QA (MC-QA) dataset of 12,102 questions that requires commonsense reasoning to answer questions.
We use standard splits~\cite{lin-etal-2019-kagnet} and report results on the in-house test (IHtest).
%and the official test set.
\textbf{2.~OpenBookQA} (OBQA) is a 4-way MC-QA dataset of 5,957 questions based on elementary science knowledge; splits by~\citet{mihaylov2018can}.
\textbf{3.~MedQA-USMLE} is a 4-way MC-QA dataset based on biomedical and clinical knowledge and has 12,723 questions from United States Medical License Exams, with splits by~\citet{jin2021disease}.

Table~\ref{tab:nodecount} presents node counts in the WG for the above datasets while Table~\ref{tab:overlap} (App.~\ref{app:graphstats}) shows a small overlap between noun chunk and KG nodes.

% Table~\ref{tab:nodecount} shows that the average number of extra nodes is low as compared to other datasets which makes it unnecessary to remove them.
% Hence CANP is not applied on MedQA

\paragraph{Implementation \& training details.}
The LM adopted in our work is \roberta~\cite{DBLP:journals/corr/abs-1907-11692} for CSQA and OBQA, and SapBERT~\cite{https://doi.org/10.48550/arxiv.2010.11784} for MedQA.
%Each sample is used to generate multiple QA contexts by concatenating each answer choice. Distinct WGs are constructed for each QA context by selected QA entities grounded in KG and the relations and extra nodes in k-hop paths between them. LM representations of QA context and noun chunks in QA context are added as special nodes and connected to all other nodes.
ConceptNet~\cite{speer2017conceptnet} is our KG for generating the WG in CSQA and OBQA.
For MedQA, we use the graph constructed by QA-GNN~\cite{yasunaga-etal-2021-qa}.
% Distinct WGs are constructed for each QA context by concatenating each answer choice with the question separately. QA context and noun chunks representations fetched from LM were added in the working graph as special nodes. Thereafter, these special nodes are connected to rest of the nodes in WG.
%
Our model consists of an LM and a GNN with dim 200.
RADAM optimizer is used with a learning rate of $10^{-5}$ for the LM and $10^{-3}$ for the GNN.
OBQA \& MedQA are trained for 50 epochs with a batch size of 128 and CSQA for 20 epochs with a batch size of 64.
All models are a single run trained on 2 RTX 2080 Ti GPUs and take about 28 hours for OBQA and 16 hours for CSQA and MedQA.

\subsection{Comparisons with Baselines}
% To understand the advantage of reasoning with KG, we compare our results with state-of-the-art language models on CSQA.
We use accuracy as a metric and compare our results primarily against other works that also adopt LM + KG methods (see Table \ref{tab:combined}).
% : (1) Relation Network \cite{santoro2017simple}, (2) RGCN \cite{schlichtkrull2018modeling}, (3) GconAttn \cite{wang2019improving}, (4) KagNet \cite{lin-etal-2019-kagnet}, (5) MHGRN \cite{feng-etal-2020-scalable}, (6) QA-GNN \cite{yasunaga-etal-2021-qa}, (7) GreaseLM~\cite{zhang2022greaselm}. 
%(1), (2) \& (3) are based on \todo{}, 
\methodname{} builds on top of QA-GNN (for direct comparison) and improving the WG results in highest performance on OBQA \& MedQA and comparable performance on CSQA.
% as compared to previous existing work based on paradigm of LM + KG.
For a fair comparison, we use the same LM for all methods unless noted.

LM only methods tend to perform worse than the baseline QA-GNN.
\roberta~\cite{DBLP:journals/corr/abs-1907-11692} for CSQA provides 72.1\% while \roberta and AristoRoBERTa~\cite{clark2019aristoroberta} for OBQA show 64.80\% and 77.8\%, respectively.
For MedQA, the LM only model results are also shown in Table~\ref{tab:combined} (right); we see that LMs trained on medical data (\eg~SapBERT~\cite{https://doi.org/10.48550/arxiv.2010.11784}) outperform generic LMs on this domain-specific task.
\methodname{} outperforms all these approaches.

% To understand the advantage of reasoning with KG, we compare our results with state-of-the-art language models on CSQA.
% Table~\ref{tab:combined} captures \methodname{} results on OBQA, CSQA and MedQA datasets respectively and comparison with existing work.
% these baselines as for our model.
%\todo{GreaseLM{ is the existing top-performing model under this LM+KG paradigm. 
%The key difference between \methodname{} and these baseline methods is that \todo{they do not fuse the representations of both modalities across multiple interaction layers, allowing the representation of both modalities to affect the

% \CD{missing discussion on why big jump OBQA, small jumps CSQA, MedQA. Also why we think CANP helps in one place while PEGA in the other. Also why MedQA doesn't have CANP}

\paragraph{OBQA.}
CANP applied to the original QA-GNN WG is unable to improve performance (-1.6\%), probably because the WG is not rich.
However, PEGA provides a 14.2\% accuracy improvement over QA-GNN (82\% vs. 67.8\%).
Interestingly, CANP when used together with PEGA boosts the accuracy to 90\% (+22.2\%);
surpassing GreaseLM that uses an improved LM (AristoRoBERTa) and better integration of LM + KG by 5.2\%.
For the \emph{domain-specific} OBQA, PEGA adds relevant information while CANP effectively cleans up irrelevant nodes resulting in large improvements.
% as the original KG may not be well  direct pruning does not help.
% makes it difficult for model to learn.

% We hypothesize that since OBQA contains elementary level science based questions, extracting chunks from QA context adds the relevant information to the working graph and then CANP removes the less relevant nodes obtained from the ConceptNet graph and helps model learn in a better way

\paragraph{MedQA.}
PEGA achieves an improvement of 1.5\% over QA-GNN, and 1\% over GreaseLM, the previous SoTA.
A reason for the small improvement (compared to OBQA) could be that the WG for MedQA has fewer nodes (see Table~\ref{tab:nodecount}).
Additionally, the small number of Answer entity nodes in the WG also means that CANP is not applicable.
% CANP is not practically applicable to MedQA dataset owing to very less number of nodes in Knowledge Graph.
% The lower improvement can also be due to  knowledge graph used for MedQA dataset does not contain enough information to answer the question number.

\paragraph{CSQA.}
On \emph{generic commonsense} questions, the WG can have large amounts of irrelevant information that CANP can simplify.
We see an improvement of 1.5\% over QA-GNN when using CANP only.
% While experimenting with CSQA dataset, we observed slight improvement of (\textbf{1.5\%}) with CANP and PEGA+CANP techniques, while PEGA only gave similar performance.
However, unlike OBQA, PEGA shows comparable performance to QA-GNN as it may lead to stuffing the WG with common terms (noun chunks) that do not provide discriminatory information.
Nevertheless, CANP alone also improves over GreaseLM by 0.9\% (all in absolute points).

\subsection{Ablation experiments}
\label{sec:ablation}

% \begin{enumerate}
%     \item Layer wise experiments
%     \item entity only, entity + clustering with different layers
%     \item with aristo-roberta
%     \item pavan's experiments
% \end{enumerate}

% We now present some ablation studies.
% better understand the impact of different components on \methodname{}.
% Results are presented in Table~\ref{tab:ablations}.
% \ref{tab:nl}, \ref{tab:wp}, \ref{tab:wee}

\paragraph{Noun chunk extraction.}
While PEGA is an effective graph augmentation strategy, it relies on the noun chunk extraction method.
We evaluate automatic noun chunk extraction methods spaCy and NLTK (see App.~\ref{app:spacy_nltk} for details) against a simple baseline that randomly adds 20\% of the QA pair's words to the WG.
Table~\ref{tab:noun_chunk_ablations} shows that extracting meaningful chunks is important and may lead to large performance change (on OBQA).
Interestingly, even random chunks of the QA pair provides a 4.5\% boost over QA-GNN that only includes one node to encode the entire QA context.
% The results of these are shown in the below table.

\paragraph{Number of GNN layers}
is often an important hyperparameter.
We show results for both the PEGA+CANP (Table~\ref{tab:num_layers_pc}) and PEGA-only (Table~\ref{tab:num_layers_p}) models in Appendix~\ref{app:more_results}.
Generally, 5 layers seem to work well for all settings, while the CSQA PEGA-only model shows better results with 4 layers.

\section{Conclusion}
\label{sec:conclusion}

We presented GrapeQA, an effective approach to integrate information from QA (LM) and KG for commonsense QA.
We proposed two simple improvements to the working graph:
PEGA, a graph augmentation that improves information flow between the QA and the KG; and
CANP that prunes less relevant information.
Our approach led to new SoTA results on three datasets OBQA, CSQA, and MedQA, with a large 22\% increase on OBQA.
% We proposed two novel techniques:
% (1) Prominent Entities for Graph Augmentation (PEGA), which improves information flow between LM and KG, and
% (2) QA Context-Aware Node Pruning (CANP), which facilitates the removal of undesirable information from the working graph.
% GrapeQA demonstrated performance improvements over existing work, including a 22\% improvement on  domain-specific OBQA, while the ablations highlighted the approach's stability (\#GNN layers) and the need to augment the graph with appropriate information.
% the components that are crucial to the model performance.

\section{Ethical Impact}
\label{sec:ethical_impact}
% necessary: https://aclrollingreview.org/authorchecklist

In order to support commonsense thinking, this study suggests a general method for fusing language models and external knowledge graphs.
We rely on publicly available datasets and benchmarks and knowledge graphs for each experiment.
We could not anticipate any immediate social ramifications or ethical concerns as we neither amplify existing bias in the data nor do we inject any social or ethical bias into the model.

\bibliography{main}
\bibliographystyle{acl_natbib}


\appendix
\clearpage
\begin{center}
\textbf{\Large{Appendix}}
\end{center}
% \label{sec:appendix}

We first present additional results in Appendix~\ref{app:more_results} followed by a detailed explanation of QA-GNN in Appendix~\ref{app:qagnn}.
Appendix~\ref{app:graphstats} provides some statistics for the datasets and working graphs, while Appendix~\ref{app:spacy_nltk} presents details of noun chunk extraction methods used in PEGA.

\section{Additional Results}
\label{app:more_results}

\paragraph{Number of GNN layers ablations.}
Tables~\ref{tab:num_layers_pc} and \ref{tab:num_layers_p} show ablation studies by varying the number of GNN layers over PEGA+CANP and PEGA-only respectively.
5 layer GNNs seem to be a suitable for both methods, while CSQA with PEGA-only shows highest performance with 4 layers.

\paragraph{CANP is not necessary on MedQA.}
Table~\ref{tab:nodecount} of the main paper shows the average number of nodes of different types in a WG.
The number of extra concept nodes is much higher than the QA concept nodes except in the MedQA dataset.
This makes it necessary to prune these nodes to keep only the relevant ones.
In case of MedQA since the number of extra nodes in WG are already quite low, and the nodes from the KG are often meaningful (domain-specific) we do not perform CANP pruning.

\paragraph{Results on CSQA.}
Table~\ref{tab:csqa_off} shows the results of our model on the official test set for CommonsenseQA.
We compare our results with other existing approaches, both using powerful LMs (e.g.,~UnifiedQA) or LM+KG methods (QA-GNN, GreaseLM, etc.).
Unfortunately we were unable to evaluate our best performing model on the in-house test set (GrapeQA: CANP-only) due to limited number of submissions indicated for the evaluation.
Even on the in-house test set, we see no performance change between PEGA-only and QA-GNN (73.41\% vs. 73.4\%) while a $\pm$1\% variation exists due to random seeds.


\begin{table}[h]
\caption{Impact of the number of GNN layers using the PEGA+CANP model.}
\centering
\tabcolsep=0.10cm
\begin{tabular}{lcc}
\toprule
 & \multicolumn{2}{c}{ \textbf{Accuracy} } \\
\textbf{\#layers} & \textbf{OBQA} & \textbf{CSQA} \\
\midrule
4 & 88.38 &  72.60\\ 
5 & \bf 90.00 &  \bf 74.05\\
6 & 88.96 & 71.88\\
\bottomrule

\end{tabular}

\label{tab:num_layers_pc}
% \end{table}
% \quad
% \begin{table}[!t]
\vspace{5mm}
\caption{Impact of the number of GNN layers using the PEGA only model.}
\centering
\begin{tabular}{lcc}

\toprule
 & \multicolumn{2}{c}{ \textbf{Accuracy} } \\
\textbf{\#layers} & \textbf{OBQA} & \textbf{CSQA} \\
\midrule
4 & 83.20 & \bf 74.62\\ 
5 & \bf 82.00 &  73.41\\
6 & 81.40 & 73.17\\
\bottomrule
\end{tabular}

\label{tab:num_layers_p}
\end{table}


\begin{table}[t]
\small
\centering
\tabcolsep=0.02cm
\caption{Comparison on CommonSenseQA official test set using \roberta model. The best result is in \textbf{bold} and second best is \underline{underlined}.
Due to limited entries for evaluation, we were unable to evaluate our best method on CSQA: CANP-only.
*UnifiedQA has 11B parameters and is about 30x larger than QA-GNN and our model and is trained on much more data.}
\begin{tabular}{lc}

\toprule
\textbf{Model} & \textbf{Test Acc.}\\
\midrule
RoBERTa~\small{\cite{DBLP:journals/corr/abs-1907-11692}} & 72.1 \\
RoBERTa + FreeLB (ensemble)~\small{\cite{https://doi.org/10.48550/arxiv.1909.11764}}& 73.1 \\
RoBERTa + HyKAS~\small{\cite{https://doi.org/10.48550/arxiv.1910.14087}} & 73.2 \\
RoBERTa + KE (ensemble) & 73.3 \\
RoBERTa+KEDGN (ensemble) & 74.4 \\
XLNet+GraphReason~\small{\cite{https://doi.org/10.48550/arxiv.1909.05311}} & 75.3 \\
RoBERTa+MHGRN~\small{\cite{feng-etal-2020-scalable}} & 75.4 \\
Albert+PG~\small{\cite{wang-etal-2020-connecting}} & 75.6 \\ 
QA-GNN \small{\cite{mihaylov2018can}} & 76.1 \\
Albert (ensemble)~\small{\cite{conf/iclr/LanCGGSS20}} & 76.5 \\
UnifiedQA*~\small{\cite{https://doi.org/10.48550/arxiv.2005.00700}} & 79.1 \\
\midrule
\methodname{} (PEGA) (\bf \textbf{Ours}) & \bf 73.5 \\
\bottomrule
\end{tabular}

\label{tab:csqa_off}
\end{table}








\section{QA-GNN Method Details}
\label{app:qagnn}

We provide further details of the question-answering procedure adopted by QA-GNN~\cite{yasunaga-etal-2021-qa}.

\subsection{Relevance Scoring}
\label{sec_app:relevance_scoring}
Extracting a sub-graph by selecting few hop neighbors adds many irrelevant nodes to the sub-graph.
QA-GNN proposes a relevance scoring mechanism to add an ``importance score" to the initial embedding of concept nodes. This helps the GNN to focus on nodes with high relevance score while performing graph reasoning.
\begin{align}
\brho_{v} &= f_{\text{head}}\left(f_{\text{enc }}([\operatorname{text}(\bz) ; \operatorname{text}(\bv)])\right) \, , \\
\rho_{t} &=f_{\rho}\left(\brho_{v}\right) \, .
\end{align}
The text of each concept node $\text{text}(\bv)$ is concatenated with the QA-pair (referred to as $\text{text}(\bz)$).
The LM encoding and following an MLP head produces an embedding $\brho_v$ that is converted into a relevance score $\rho_{v}$ using an MLP $f_{\rho}$.
Nodes with a score lower than a threshold are discarded.

\subsection{Node and Relation Types}
QA-GNN constructs a working graph which is heterogeneous and multi-relational.
It uses a node type ($u$) aware and relation type ($r$) aware iterative message passing network to reason over it.
Different node types are represented using embeddings $\bu$. These include QA context, Question entity, Answer entity and Extra nodes. 
Whereas, edge embeddings $\be$ include relations in KG and two new relation types between the QA context node and KG entity nodes.

Node and relation types are embedded using MLPs $f_{u}$ and $f_{r}$ respectively,
\begin{align}
\bu_{t} &= f_{u} \left( u_{t} \right) \, , \\
\br_{s t} &= f_{r} \left( \be_{s t}, u_{s}, u_{t} \right) \, .
\end{align}
The message from the source to target node is constructed by concatenating the source node and type representations along with the relation embedding from the source to target and projecting it (to node embedding dimension) using the MLP $f_{m}$. 
\begin{equation}
\bm_{s t} = f_{m} \left( \bh_{s}^{(\ell)}, \bu_{s}, \br_{s t} \right) \, .
\end{equation}

\subsection{Message Passing}
Node representations are updated at each layer using the following attention mechanism
\begin{equation}
\bq_{s} = f_{q} \left( \bh_{s}^{(\ell)}, \bu_{s}, \brho_{s} \right) \, ,
\end{equation}
and
\begin{equation}
\bk_{t} = f_{k} \left( \bh_{t}^{(\ell)}, \bu_{t}, \brho_{t}, \br_{s t} \right) \, .
\end{equation}

The query $\bq_{s}$ and key $\bk_{t}$ vectors of the source and target nodes are computed using the node representation $\bh$, the node type embedding $\bu$ and relevance score embeddings $\brho$.
Finally, we score attention $\alpha_{st}$ as
\begin{equation}
\alpha_{s t}=\frac{\exp \left(\gamma_{s t}\right)}{\sum_{t^{\prime} \in \mathcal{N}_{s} \cup\{s\}} \exp \left(\gamma_{s t^{\prime}}\right)}, \,\, \gamma_{s t}=\frac{\boldsymbol{q}_{s}^{\top} \boldsymbol{k}_{t}}{\sqrt{D}} \, .
\end{equation}

The attention weights for messages from source to target $\alpha_{st}$ are calculated using $\bq$ and $\bk$ vectors.
Finally, we aggregate messages and update the node representation as
\begin{equation}
\label{eq:17}
\boldsymbol{h}_{t}^{(\ell+1)}=f_{n}\left(\sum_{s \in \mathcal{N}_{t} \cup\{t\}} \alpha_{s t} \boldsymbol{m}_{s t}\right)+\boldsymbol{h}_{t}^{(\ell)} \, .
\end{equation}





\begin{comment}
\section{CANP: QA Context-Aware Node Pruning}
\label{app:canp}
There are concept nodes which match with question, answer choice or don't match with them both. These extra concept nodes are one hop away from QA nodes in the KG. Although they are adding information about QA nodes they also add irrelevant noise too.
Table~\ref{tab:nodecount} shows the average number of nodes of different types in a WG. Number of extra concept nodes is very high compared to QA concept nodes except in MedQA dataset. This makes it necessary to prune these nodes to keep only the relevant ones. In case of MedQA since the number of extra nodes in WG are low and the nodes are domain specific we don't do any pruning.

% To prune the extra nodes we propose CANP which uses QA context to prune these extra nodes in each WG.  Each extra node in WG is assigned to one of the answer nodes in WG. To do this assignment for each extra node - answer node pair a relevance score is calculated as follows.
% Equation
% Each Extra node is assigned to that answer node which has the highest relevance with this extra node. Once all extra nodes in a WG are assigned this way, Each answer node's average relevance score is used to rank these sets of extra nodes. We remove the set with lowest relevance.
\end{comment}



% The idea of relevance scoring is introduced by QA-GNN authors where they score each KG node by concatenating it with QA context. The idea of CANP is built on top of the relevance scoring where we prune the working graph using this scoring mechanism. First, it clusters entities from answer into sets for each answer option. Then adds the nodes from ConceptNet graph in one of these cluster sets by calculating relevance score of that node with all the answer entities. That node is then assigned to that answer entity set for which the relevance score is highest for the corresponding answer entity. 






% \begin{table}[t]
% \centering
% \tabcolsep=0.10cm
% \begin{tabular}{lccc}
% \toprule
% \textbf{Node Type} & \textbf{OBQA}  & \textbf{CSQA}  & \textbf{MedQA}  \\
% \midrule
% Question entity $q_\text{KG}$  & 6.52          & 7.36          & 6.1             \\
% Answer entity $a_\text{KG}$    & 2.79          & 2.05          & 0.55            \\
% Extra nodes $s_\text{KG}$    & 107.17        & 112.04        & 20.82           \\
% Noun chunk nodes $\mcV^\prime$ & 3.88          & 4.13          & 33.46          \\
% \bottomrule
% \end{tabular}
% \caption{Average number of nodes of each type in Working Graphs}
% \label{tab:nodecount}
% \end{table}

\begin{table}[t]
\caption{Number of unique nodes across all WG of the dataset.
Even though more nodes are added from the KG on average (see Table~\ref{tab:nodecount}), they are not all unique across the dataset and result in a smaller count.}
\centering
\tabcolsep=0.12cm
\begin{tabular}{l ccc}
\toprule
    & \textbf{Noun chunk} & \textbf{Nodes}    & \textbf{Overlapping} \\
\textbf{Dataset} & \textbf{nodes}  & \textbf{from KG}  & \textbf{nodes} \\
\midrule
OBQA  & 14470    & 7506  & 1958    \\
CSQA  & 23881    & 12485 & 4023    \\
MedQA & 69370    & 2753  & 1268    \\
\bottomrule
\end{tabular}

\label{tab:overlap}
\end{table}


\begin{table}[t]
\caption{Average number of words in the question $q$ and answer option $a_o$ for the different datasets.}
\centering
\tabcolsep=0.12cm
\begin{tabular}{l cc}
\toprule
  & \textbf{Question} & \textbf{Answer} \\
\midrule
OBQA  & 13.5     & 2.8 \\
CSQA  & 13.8     & 1.5 \\
MedQA & 116.2    & 3.6 \\
\bottomrule
\end{tabular}

\label{tab:wordcount}
\end{table}

\section{Working Graph Statistics}
\label{app:graphstats}
Given a question and corresponding answer option, KG nodes with matching text entities are identified.
These matched nodes along with the Extra nodes that fall in 2-hop paths from them form the sub-graphs for each $[q; a]$ pair.
Working Graphs are constructed by joining these sub-graphs with QA context nodes initialized with the representation from LM.
In each Working Graph, the QA context node is connected to all the concept nodes in it which are extracted from the KG. 

\paragraph{Node counts.}
Table~\ref{tab:nodecount} in the main paper shows the number of nodes added to the WG on average.
We see that general KGs (ConceptNet) afford a large number of extra nodes (100+) while MedQA with a smaller KG only adds a few extra nodes ($\sim$20).
The large number of noun chunks added in the MedQA is explained by the fact that the questions in MedQA are very large as they include patient's description.
Table~\ref{tab:wordcount} presents the average number of words in the question and answer option.

\paragraph{Noun chunks are unique.}
Table~\ref{tab:overlap} shows the number of unique nodes present in each dataset.
It can be observed that the total number of \emph{unique} nodes selected from the KG is low as compared to the total number of unique noun chunk nodes extracted.
Even though Table~\ref{tab:nodecount} shows that a large number of nodes are added to the graph, they are not all unique.
Thus, even if the average number of noun chunk nodes for each WG are low, they are more diverse compared to nodes from KG.
A small overlap between noun chunk nodes and nodes from the KG indicates that this way of constructing the WG may provide better opportunity for graph reasoning to exchange information effectively between the QA (LM) and the KG.



\section{Noun chunk extraction methods}
\label{app:spacy_nltk}

\paragraph{SpaCy}
is a Python and Cython programming language-based open-source software library for sophisticated natural language processing%
\footnote{\url{https://spacy.io/}}.
The library is distributed under the MIT licence.

In our experiments, we used \texttt{en\_core\_web\_sm} package which provides functionalities like tok2vec, tagger, parser, attribute\_ruler, lemmatizer, ner. We have used the noun chunk parser technique for extracting the noun chunks.

\paragraph{NLTK}
In order to work with human language data, Python programs can be built using the NLTK framework.
NLTK offers simple access to more than 50 corpora and lexical resources, including WordNet, as well as a number of text processing libraries for categorization, tokenization, stemming, tagging, parsing, and semantic reasoning.

In our implementation, we first tokenize the input sentence using NLTK's \texttt{word\_tokenizer}.
Then to extract the noun chunks, the POS tagger of NLTK is used.
\end{document}