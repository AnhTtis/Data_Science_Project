\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{float}
\usepackage{multirow}
\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{10432} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Fine-Grained and High-Faithfulness Explanations for \\Convolutional Neural Networks}

\author{Changqing Qiu$^1$, Fusheng Jin$^1$, Yining Zhang$^2$\\
	\\
	$^1$Beijing Institute of Technology\\
	$^2$Peking University
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
   Recently, explaining CNNs has become a research hotspot. CAM (Class Activation Map)-based methods and LRP (Layer-wise Relevance Propagation) method are two common explanation methods. 
   However, due to the small spatial resolution of the last convolutional layer, the CAM-based methods can often only generate coarse-grained visual explanations that provide a coarse location of the target object. 
   LRP and its variants, on the other hand, can generate fine-grained explanations. But the faithfulness of the explanations is too low. 	
   In this paper, we propose FG-CAM (fine-grained CAM), which extends the CAM-based methods to generate fine-grained visual explanations with high faithfulness.
   FG-CAM uses the relationship between two adjacent layers of feature maps with resolution difference to gradually increase the explanation resolution, while finding the contributing pixels and filtering out the pixels that do not contribute at each step.
   Our method not only solves the shortcoming of CAM-based methods without changing their characteristics, but also generates fine-grained explanations that have higher faithfulness than LRP and its variants.
   We also present FG-CAM with denoising, which is a variant of FG-CAM and is able to generate less noisy explanations with almost no change in explanation faithfulness. 
   Experimental results show that the performance of FG-CAM is almost unaffected by the explanation resolution. FG-CAM outperforms existing CAM-based methods significantly in the both shallow and intermediate convolutional layers, and outperforms LRP and its variations significantly in the input layer.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
In recent years, CNNs have been widely used in many areas. As a result, it is increasingly important to explain why the network makes a specific decision. 
There has been extensive research on the interpretability of deep learning, especially in some important areas such as medicine~\cite{A25,A20,A22,A24,A23} and autonomous driving~\cite{A21,A26,A27}.
For visual explanation, a very large number of methods have been proposed, such as CAM-based methods~\cite{A3,A14,A36,A6,A35,A15,A34,A5,A37,A7,A2,A4,A28,A1}, LRP~\cite{A8} and its variants~\cite{A9,A10}, and other methods~\cite{A30,A16,A12,A33,A32}.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig1.pdf}
	
	\caption{Visualization results of FG-Grad-CAM, Grad-CAM, FG-Score-CAM, Score-CAM and Layer-CAM. Model is VGG 16 with batch normalization, Layer43 represents model.features[43]. Pixels with higher brightness are more important than others. Even in shallow layers, FG-CAM can still generate high-resolution explanation with high faithfulness.}
	\label{fig:1}
\end{figure}

CAM method~\cite{A1} generate visual explanation through a linearly weighted combination of  the feature map of the last convolutional layer.
Although CAM method can locate target object in input image, it has great restrictions on the model structure.
Therefore, as an extension of CAM, Grad-CAM~\cite{A2}, Score-CAM~\cite{A4} and other CAM-based methods have been proposed. 
However, before Layer-CAM~\cite{A6}, all CAM-based methods could only be applied to the final convolutional layer of the model, and if the target layer became shallow, the faithfulness (measure how accurately the explantion reveals the model decision) of the explanations would be significantly reduced, thus only coarse-grained visual explanations could be generated, which provides coarse location of the target object, limiting the performance on tasks that need pixel-accurate object locations~\cite{A6}.
Layer-CAM, on the other hand, although outperforming other CAM-based methods on shallow convolutional layers, suffers from shattered gradient problem~\cite{A13} as the network gets deep and the target layer becomes shallow, which can lead to problems such as making visual explanation noisy and reduced faithfulness (not as dramatically as other CAM-based methods, but still not negligible).
As shown in Figure~\ref{fig:1}, as the target layer becomes shallower, the accuracy of the explanations generated by Grad-CAM and Score-CAM will decrease significantly and even become unusable (unable to extract contributing pixels or filter out non-contributing pixels). 
The visual explanation of Layer-CAM has more and more noise, and the accuracy has decreased, while the FG-Grad-CAM and FG-Score-CAM still show good performance.

LRP and its variants, which are based on deep Taylor decomposition~\cite{A11}, backpropagate the relevance score from output layer to input layer through specific rules to directly obtain the contribution of each pixel in the input image. 
They can generate the explanations at the same resolution as the input image.
However, the explanations have low faithfulness.

In order to generate fine-grained visual explanation with high faithfulness, this paper proposes a novel method: FG-CAM. 
Different from other CAM-based methods which are directly applied to shallow layers to obtain high-resolution explanations, FG-CAM uses the relationship between two adjacent layers of feature maps with resolution difference to gradually increase the resolution of explanations produced on the last convolutional layer, while finding the contributing pixels and filtering out the pixels that do not contribute at each step.
FG-CAM not only solves the problem that CAM-based methods can only provide rough explanations without changing their characteristics, but also generates fine-grained explanations that have higher faithfulness than LRP and its variants. 
This paper also presents FG-CAM with denoising, which is able to generate less noisy explanations with almost no change in explanation faithfulness.

To the best of our knowledge, our method achieves the SOTA in shallow and intermediate convolutional layers as well as in the input layer (different target layers represent different explanation resolutions).
The contributions of this paper are as follows.

\begin{enumerate}
	\item This paper proposes FG-CAM, a novel visual explanation method, which can generate fine-grained visual explanations with high faithfulness in shallow and intermediate convolutional layers as well as in the input layer. FG-CAM solves the shortcoming of CAM-based methods without changing their characteristics, and achieves generality. Different from other CAM-based methods, the performance of FG-CAM is almost unaffected by the explanation resolution.
	
	\item This paper also presents FG-CAM with denoising, which is a variant of FG-CAM, and is able to generate less noisy explanations with almost no change in explanation faithfulness.
	
	\item We have verified qualitatively and quantitatively that FG-CAM performs significantly better than Layer-CAM (currently the best-performing CAM-based method for shallow convolutional layers), Grad-CAM, and Score-CAM in both shallow and intermediate convolutional layers. It is proved that whether FG-CAM is used or not makes a very large difference.
	
	\item Through experiments, we verify that FG-CAM can generate explanations with high faithfulness in the input layer, which is not possible for other CAM-based methods. And FG-CAM outperforms LRP, CLRP and SGLRP significantly.

\end{enumerate}

%-------------------------------------------------------------------------

\section{Related Work}

\subsection{CAM-based methods}

CAM-based methods generate visual explanation through a linearly weighted combination of the feature map of a certain convolutional layer (usually the last convolutional layer). Consider a convolutional layer $l$ in a CNN model $f$, $A_l^i$ represents the feature map of the $i$th channel output by the convolutional layer, then the visual explanation of the CAM-based methods for class $c$ can be defined as:

\begin{equation}
	L_{CAM}^c=ReLU(\sum_{i}w_c^iA_l^i)
	\label{eq:important}
\end{equation}
The ReLU function is used because CAM-based methods are usually only interested in positively contributing pixels. 

In original CAM~\cite{A1}, $w_c^i$ is the connection weight between classification layer and global average pooling layer which is applied in the final convolution layer. 
It can locate the target object in the image but has significant limitations on the model structure.

Grad-CAM~\cite{A2} uses the average gradient of the output w.r.t the feature map as the weight of the feature map, solving the model structure dependency problem of CAM method, and can be used on any CNN model. 
Grad-CAM is equivalent to CAM when the target convolutional layer is followed by one and only one fully connected layer. Other gradient-based CAM methods, such as Grad-CAM++~\cite{A3}, XGrad-CAM~\cite{A14}, will not be discussed in this paper.
We use Grad-CAM to verify that FG-CAM can be used on CAM-based methods based on gradient weight.

Different from gradient-based CAM methods, Score-CAM~\cite{A4} uses upsampled and normalized feature map as the mask of the image. 
The image perturbed by the mask is input into the model, and the weight of the feature map is obtained according to the output. 
Score-CAM gets rid of the dependence on gradients and shows very good performance. 
However, Score-CAM still can not be used at shallow convolutional layers.
We use Score-CAM to verify that FG-CAM can be used on the CAM-based methods based on the score weight.

Unlike other CAM-based methods where one feature map corresponds to one global weight, Layer-CAM~\cite{A6} uses pixel-level weights and uses the positive gradient of the output w.r.t the pixel of feature map as the weight (\ie $w_c^i=ReLU(\frac{\partial y^c}{\partial A_l^i}$) to address the problem that CAM-based methods can not be used at shallow convolutional layers.
Layer-CAM uses fine-grained weights and can generate high-resolution explanation at shallow convolutional layers, which is not possible for the majority of CAM-based methods. However, as the network gets deeper, the gradients become noisy and discontinuous~\cite{A15}. As shown in Figure~\ref{fig:1}, as the target layer becomes shallower, the visual explanation of Layer-CAM becomes more and more noisy.
In this paper, we compare the proposed FG-CAM with Layer-CAM which currently performs best in shallow convolutional layer to verify the superiority of FG-CAM.

\subsection{LRP and its variants}
LRP~\cite{A8} is another visual explanation method, which backpropagates the relevance score from the output layer to the input layer through specific rules to obtain the contribution of input image pixels to the model output.

The general propagation rule for LRP is the $z^+$ rule.
Consider a trained model, $i$ represents the $i$th neuron in layer $l$, $j$ represents the $j$th neuron in layer $l+1$, and $w_{ij}^ {(l)}$ represents the weight of connection between neuron $i$ and neuron $j$, and the relevance score of neuron $i$ and $j$ is $R_i^l$ and $R_j^{l+1}$ respectively. The $z^+$ rule can be defined as:
\begin{equation}
	R_{ij}^{l}=\frac{w_{ij}^{+(l)}x_i^l}{\sum_{i'}{w_{i'j}^{+(l)}x_{i'}^l}}R_j^{(l+1)}
	\label{eq:important}
\end{equation}
\begin{equation}
	R_i^l=\sum_{j}R_{ij}^l
	\label{eq:important}
\end{equation}
where $w_{ij}^{+(l)}=max(w_{i,j}^{(l)},0)$, $x_i^l$ is the output of the $i$th neuron in layer $l$. For the output layer $o$, LRP set $R^o_c$ to $y_c$ and $R^o_i (i\neq c)$ to 0, where $c$ is the target class and $y_c$ is the model output (before softmax) of class $c$. 
$z^+$ rule is often used when $x_i^l\geq 0$. For layers with potentially negative inputs, such as the input layer, the $z^\beta $ rule is often used, defined as:

\begin{equation}
	R_{ij}^l=\frac{w_{ij}^{(l)} x_i^l-w_{ij}^{+(l)} b_i^l-w_{ij}^{-(l)} h_i^l}{\sum_{i'}w_{i'j}^{(l)} x_{i'}^l-w_{i'j}^{+(l)} b_{i'}^l-w_{i'j}^{-(l)} h_{i'}^l} R_j^{l+1}
	\label{eq:important}
\end{equation}
where $w_{ij}^{-(l)}=min(w_{i,j}^{(l)},0)$, interval $[b_i^l, h_i^l]$ is the domain of the activation value $x_i^l$. 

After propagating relevance score to the input layer, LRP determines the contribution of each pixel in the input image based on its relevance score.
Therefore, LRP can generate a visual explanations with the same resolution as input image.
While LRP can generate fine-grained explanations, the results are not class-discriminative ~\cite{A9}. 
To address this problem, CLRP~\cite{A9} and SGLRP~\cite{A10} were proposed, which differ from LRP only in their definition of the relevance score for the output layer.
However, CLRP and SGLRP, which over-remove pixels that contribute to other classes, often result in important parts of the target class being removed (even removing the entire object).
It can significantly affect the correctness of the explanation and may lead people in the wrong direction. 
Therefore, the explanations generated by LRP, CLRP and SGLRP have low faithfulness.

\begin{figure*}
	\centering
	\includegraphics[width=0.8\linewidth]{fig3.pdf}
	\caption[width=0.8\linewidth]{FG-CAM pipeline. In the first step, The global weight of the feature map in the last convolution layer is obtained. In the second step, improve the resolution of the explanation components. Finally, generate fine-grained visual explanation.}
	\label{fig:3}
\end{figure*}

\section{Our Method}

\subsection{FG-CAM}
CAM-based methods usually assign a global weight to a feature map (\ie every pixel within the feature map has the same weight). 
However, In shallow layers, one feature map has not only the pixels of target class, but also the pixels of non-target class such as background. 
The global weight cannot filter out these irrelevant pixels and highlight the target class pixels.
Thus the generated explanation cannot focus attention on the target object (\eg Figure \ref{fig:1}, explanation of Score-CAM and Grad-CAM on layer13).
Therefore, CAM-based methods can only be applied in the last convolutional layer to generate explanations that although highlighting important regions about model decisions accurately, have very low resolution (\eg for VGG16 with batch normalization, the explanation only has $7\times7$ resolution, which far less than $224\times224$ of the input image). And we need to be able to determine the importance of each pixel in order to generate fine-grained explanation.

Layer-CAM proposes fine-grained weights so that it can be used directly at shallow layers to generate high-resolution explanations.
However, The explanations generated by Layer-CAM are very noisy, and almost all CAM-based methods fail to produce fine-grained weights.
Therefore, in order to achieve generality, we address the problem that CAM-based methods can only generate coarse-grained explanations from another perspective, which is to improve the explanation resolution, while finding the contributing pixels and filtering out the pixels that do not contribute (\ie make the explanation fine-grained), rather than trying to use CAM-based methods directly on the shallow layers.  

For the final explanation, it is difficult for us to fine-grained it because we cannot easily obtain the relationship between the correct fine-grained explanation and the current explanation. In other words, we cannot accurately determine the importance of each pixel in the fine-grained explanation. 
However, for feature maps, this is very easy to implement.
For two adjacent convolutional layers, the feature maps of previous layer usually have more details and higher resolution, and the feature maps of the later layer is calculated from the feature map of the previous layer.
Therefore, if we have the importance of each pixel in the feature map of later layer, then we can determine the importance of the pixels in the previous layer's feature map through the relationship between the two adjacent layer's feature maps.
through above method, we obtain the feature maps with higher resolution and more detail, and filter out non-contributing pixels.
By performing this layer by layer, we can obtain the importance of each pixel in the feature map with higher resolution of the target layer.  

Therefore, we can first use CAM-based methods to obtain the importance of each pixel in the last convolutional layer (without combining them to get the final explanation), which can be viewed as the explanation components.
Then we use the relationship between the two adjacent layer's feature maps to improve the resolution of the explanation components, in this paper, which is implemented using the relevance score assignment rules such as $z^+$ based on the computational relationship (consider the explanation component as the relevance score).
Finally, we sum the processed components to obtain fine-grained explanations. 
The pipeline of FG-CAM is shown in Figure~\ref{fig:3}. 

Consider a CNN $f$, the last convolutional layer of $f$ is $L$ (the shape of output is $C\times W \times H$), the target layer is $l$ (the shape of output is $c\times w\times h$).
The FG-CAM explanation, written $L_{FG-CAM}$ is defined as:
\begin{equation}
	w^{(C\times 1\times 1)}=CAM(A_L^{(C\times W\times H)})
	\label{eq:important}
\end{equation}
\begin{equation}
	I_L^{(C\times W\times H)}=w^{(C\times 1\times 1)}A_L^{(C\times W\times H)}
	\label{eq:important}
\end{equation}
\begin{equation}
	I_l^{(c\times w\times h)}=IR(I_L^{(C\times W\times H)})
	\label{eq:important}
\end{equation}
\begin{equation}
	L_{FG-CAM}=Relu(\sum_{i}^{c}I_l^i)
	\label{eq:important}
\end{equation}
where $w^{(C*1*1)}$ denotes global weights of feature map in last convolutional layers and can be obtained by any CAM-based methods (Grad-CAM, Score-CAM, \etc),
$I_L$ denotes the explanation component in layer $L$. 
$IR()$ denotes improving the resolution of the explanation components layer by layer while finding the contributing pixels and filtering out the pixels that do not contribute at each step.

Therefore, different from trying to apply CAM-based method directly to shallow layers, FG-CAM solves the problem that CAM-based methods can not generate fine-grained explanations while maintaining the good performance and characteristics of CAM-based methods, achieves generality. 
Even if new CAM-based methods are proposed in the future, FG-CAM can still be used to generate fine-grained explanations based on this method.
In addition, it can be found that FG-CAM can generate explanations with the same resolution as the input image, which is not possible with CAM-based methods.

\subsection{FG-CAM with denoising}
Due to the global weight, the explanations generated by the existing CAM-based methods in the last convolutional layer are likely to be noisy. 
These noises will spread as the explanation resolution increases, resulting in many background pixels that contribute (although their importance is very small compared to the target class pixels, they may also cause interference).
To solve the problem, in this paper, we propose FG-CAM with denoising, a variant of FG-CAM with one more step than FG-CAM. 
In FG-CAM with denoising, After getting $I_L^{(C\times W\times H)}$, we first reshape $I_L^{(C\times W\times H)}$ to $I_L^{(C\times K)}(where\quad K=W\times H)$, second perform singular value decomposition on $I_L^{(C\times K)}$, and then selects the top 10$\%$ singular values to reconstruct $I_L^{(C\times K)}$, finally reshape $I_L^{(C\times K)}$ to $I_L^{(C\times W\times H)}$. 
Then, we use the new $I_L^{(C\times W\times H)}$ as the input to $IR()$. 
FG-CAM with denoising can generate explanations that focus more attention on the target class and have less noise.



\section{Experiment}
In this section we compare FG-CAM with other methods through qualitative and quantitative experiments. FG-Score-CAM denotes FG-CAM that uses Score-CAM to obtain the global weights of the last convolutional layer, and so for FG-Grad-CAM.
We compare FG-Grad-CAM, FG-Score-CAM with Grad-CAM, Score-CAM, and Layer-CAM to verify that FG-CAM does not significantly reduce the faithfulness of explanation as the target layer becomes shallower,  as is the case with CAM-based methods, therefore solving the problem that CAM-based methods cannot generate fine-grained explanations, achieving SOTA.
We experimentally verify that FG-CAM has better performance than LRP, CLRP and SGLRP in the input layer. 

In the following experiments, we use pre-trained VGG16~\cite{A19} with batch normalization from Pytorch model zoo as a base model. ILSVRC2012 val~\cite{A29} is used as data set. For each image, we resize it to $(224\times 224 \times 3)$, convert it to range [0, 1], then normalize it using mean vector [0.485, 0.456, 0.406] and standard deviation vector [0.229, 0.224, 0.225], and no further pre-processing beyond that. Layer13, Layer23 and Layer33 represent the 2nd, 3rd and 4th maxpooling layer of the model, respectively.
For FG-CAM, LRP and its variants, we use the $z^\beta $ rule in input layer and the $z^+$ rule in other layers. 
The blurred image is realized by Gaussian blur, that is, $\widetilde{I}=guassian\_{blur2d}(I, ksize, sigma)$, where $I$ is the original image, $\widetilde{I}$ is the blurred image. In this paper, we set $ksize=51$, $sigma=50$.

\subsection{Comparison with CAM-based methods}
In this section, we verify that FG-CAM can generate fine-grained visual explanations with high faithfulness and has better performance in both shallow and intermediate convolutional layers.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig4.pdf}
	
	\caption{Qualitative comparisons of FG-CAM with other methods results on visual explanation. The results of FG-CAM are less noisy.}
	\label{fig:4}
\end{figure}

\normalsize {\textbf{Class Discriminative Visualization}}. 
We compare the visual explanation of each method in shallow and intermediate convolutional layers. 
As shown in Figure~\ref{fig:4}, as the target layer becomes shallower, the explanation accuracy of other methods is significantly reduced, or even become unusable (can not filter out pixels that do not contribute or can not extract pixels of the target class), while FG-CAM still performs very well.
At the same resolution, FG-CAM performs significantly better than other methods.
the higher the explanation resolution, the bigger the gap between other methods and FG-CAM.
Therefore, when generating high-resolution explanations, FG-CAM can still accurately extract the pixels with contribution and filter out non-contributing pixels. 
The explanations generated by FG-CAM with denoising have the least noise.

\normalsize {\textbf{Evaluation on Faithfulness}}. 
Faithfulness is an important characteristic to evaluate explanation methods, which measures whether the explanation results correctly highlight regions related to the model's decision.
For the Average Drop/Increase metric, we retain only the most important 50$\%$ pixels of the input image, and then evaluate the faithfulness by Average Drop (A.D.) and Average Increase (A.I.) as adopted in~\cite{A15,A7,A4}.
Average Drop is defined as:
\begin{equation}
	Average\ Drop=\sum_{i}^{N}\frac{\max(0,y_i^c-o_i^c)}{y_i^c}\times \frac{100}{N}
	\label{eq:important}
\end{equation}
Average Increase is defined as:
\begin{equation}
	Average\ Increase=\sum_{i}^{N}\frac{sign(y_i^c<o_i^c)}{N}
	\label{eq:important}
\end{equation}
where $N$ is the number of images, $y_i^c$ is the output softmax value for class $c$ on image $i$ , and $o_i^c$ is the output softmax value for class $c$ on the image after removing 50$\%$ of the pixels, $Sign()$ indicates a function that returns 1 if the input is true and 0 otherwise.
If the explanations correctly highlight important regions, Average Drop will have a low value and Average Increase will have a high value.
We randomly selected 2000 images from the ILSVRC2012 val for evaluation.
To remove 50$\%$ of the pixels, we replace the least important 50$\%$ with blurred pixels. The experimental result is reported in Table~\ref{tab:1}.
\begin{table}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lcccccc}
			\toprule
			& \multicolumn{2}{c}{Layer33} & \multicolumn{2}{c}{Layer23} & \multicolumn{2}{c}{Layer13} \\
			\midrule
			Method & A.D. & A.I. & A.D. & A.I. & A.D. & A.I. \\
			\hline
			Grad-CAM & 44.53 & 18.85 & 72.89 & 5.10 & 83.80 & 3.25 \\
			\hline
			FG-Grad-CAM & \textbf{18.69} & \textbf{34.90} & \textbf{19.55} & \textbf{33.45} & \textbf{19.65} & \textbf{32.35} \\
			\hline
			\makecell{FG-Grad-CAM\\(denoising)} & \textbf{18.00} & \textbf{33.85} & \textbf{17.75} & \textbf{33.55} & \textbf{17.90} & \textbf{34.00} \\
			\hline
			Score-CAM & 32.63 & 21.60 & 51.24 & 13.80 & 57.06 & 12.00 \\
			\hline
			FG-Score-CAM & \textbf{21.13} & \textbf{29.15} & \textbf{22.63} & \textbf{27.15} & \textbf{23.50} & \textbf{25.85} \\
			\hline
			\makecell{FG-Score-CAM\\(denoising)} & \textbf{19.19} & \textbf{31.70} & \textbf{18.99} & \textbf{32.00} & \textbf{19.24} & \textbf{30.50} \\
			\hline
			Layer-CAM & 24.25 & 26.80 & 27.32 & 24.50 & 30.76 & 24.40 \\
			\bottomrule
	\end{tabular}}
	\caption{The results of the faithfulness evaluation (lower is better for A.D., higher is better for A.I.).}
	\label{tab:1}
\end{table}

As shown in Table~\ref{tab:1}, FG-CAM performs better than other methods in both shallow and intermediate convolutional layers. For example, in Layer13, FG-Grad-CAM achieves 19.65 Average Drop and 32.35 Average Increase respectively, which outperforms other methods by large scale. Therefore, the fine-grained explanations generated by FG-CAM can highlight important regions in the image, and have higher faithfulness than explanations generated by other methods.
From the experimental results, it can be found that as the target layer becomes progressively shallower, the results of Score-CAM and Grad-CAM become significantly worse, while the performance of Layer-CAM declined obviously, and there is a growing gap between other methods and FG-CAM.
FG-CAM with denoising also performs better than FG-CAM in the evaluation.

To further verify the superiority of FG-CAM, we also conduct deletion and insertion tests  proposed in~\cite{A16} and adopted in~\cite{A5,A4}.
The deletion metric measures the decrease of the output softmax value for class $c$ when important pixels are gradually removed, where the importance of pixels is given by saliency map.
If the explanation results indicate the importance of pixels correctly, the output softmax value should drop sharply, and the area under the deletion curve (AUC) should be low.
Similarly, the insertion metric measures the increase of the ouput softmax value when important pixels are gradually added, with higher AUC indicate a better explanation.
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig5.pdf}
	
	\caption{Example of the deletion and insertion tests}
	\label{fig:5}
\end{figure}
\begin{table*}
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{lcccccccc}
			\toprule
			&&Grad-CAM&FG-Grad-CAM&\makecell{FG-Grad-CAM\\(denoising)}&Score-CAM&FG-Score-CAM&
			\makecell{FG-Score-CAM\\(denoising)}&Layer-CAM\\
			\midrule
			\multirow{3}{*}{Layer33} & Insertion &0.43920&0.60214&0.60247&0.51428&0.58202&0.59716&0.56566 \\
			& Deletion&0.15392&0.09640&0.11108&0.11988&0.10196&0.11198&0.09997 \\
			&Over-all&0.28528&\textbf{0.50574}&\textbf{0.49139}&0.39440&\textbf{0.48006}&\textbf{0.48518}&0.46569 \\
			\hline
			\multirow{3}{*}{Layer23} &Insertion&0.30033&0.59217&0.60248&0.39734&0.57210&0.59676&0.54096\\
			&Deletion&0.19113&0.09459&0.11378&0.14254&0.09985&0.11403&0.09333\\
			&Over-all&0.10920&\textbf{0.49758}&\textbf{0.48870}&0.25480&\textbf{0.47225}&\textbf{0.48273}&0.44763\\
			\hline
			\multirow{3}{*}{Layer13}
			&Insertion&0.23266&0.58630&0.59976&0.36485&0.56570&0.59341&0.51348\\
			&Deletion&0.16850&0.09256&0.11421&0.15037&0.09774&0.11435&0.08469\\
			&Over-all&0.06416&\textbf{0.49374}&\textbf{0.48555}&0.21448&\textbf{0.46796}&\textbf{0.47906}&0.42879\\
			\bottomrule
	\end{tabular}}
	\caption{Results of deletion and insertion tests. The Over-all score (higher is better) shows that FG-CAM outperforms other methods.}
	\label{tab:2}
\end{table*}

There are several methods of removing pixels~\cite{A17}, such as setting the pixel value to zero or using blurred pixels instead, each with its own advantages and disadvantages.
In this experiment, for the deletion metric, we replaced the original pixel with the blurred pixel, and for the insertion metric, we replaced the blurred pixel with the original pixel. We removed or added 448 pixels on each step. An example is shown in Figure~\ref{fig:5}.
We use the $Over-all$ score proposed in~\cite{A28} to comprehensively evaluate deletion and insertion results. The $Over-all$ can be calculated by $AUC(insertion)-AUC(deletion)$. 

The average result over 2000 images is reported in Table~\ref{tab:2}.
Similar to the results of the previous experiment, we find that as the target layer becomes progressively shallower, the Over-all score of Score-CAM and Grad-CAM decrease significantly, while FG-CAM is barely affected and achieves the highest $Over-all$ score.
Whether our method is used or not makes a huge difference, \eg in Layer13, the Over-all scores of FG-Grad-CAM and FG-Score are 0.42958 and 0.25348 higher than that of Grad-CAM and Score-CAM, respectively. 
FG-CAM with denoising increases in both metrics, does not have much impact on the final result.

\normalsize {\textbf{Localization Evaluation}}. 
We also measure the quality of explanation generated by each method through localization ability.
We did not use the pointing game proposed in~\cite{A18} for this measure because it is too simple, requiring only the point with max value in the saliency map to fall into the
target object bounding box, so it can not distinguish each method significantly.
Therefore, as in~\cite{A4}, we measure how many values in the saliency map fall into the target object bounding box, that is, measure the ability of visual explanation to locate target object using the Proportion metric, where $Proportion=\frac{\sum L_{(i,j)\in bbox}^c}{\sum L_{(i,j)\in bbox}^c+\sum L_{(i,j)\notin bbox}^c}$. 
As in~\cite{A4}, we randomly selecte 500 images with only one bounding box and no more than 50$\%$ of the pixels occupied by the target object bounding box for the experiment, and observe average Proportion value.

\begin{table*}
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{lcccccccc}
			\toprule
			&Grad-CAM&FG-Grad-CAM&\makecell{FG-Grad-CAM
				\\(denoising)}&Score-CAM&FG-Score-CAM&\makecell{FG-Score-CAM
			\\(denoising)}&Layer-CAM\\
			\midrule
			Layer33&0.32417&\textbf{0.51458}&\textbf{0.66736}&0.35646&\textbf{0.57161}&\textbf{0.68256}&0.49272\\
			\hline
			Layer23&0.31645&\textbf{0.50995}&\textbf{0.65744}&0.28588&\textbf{0.56192}&\textbf{0.67233}&0.49854\\
			\hline
			Layer13&0.30760&\textbf{0.50861}&\textbf{0.65472}&0.29219&\textbf{0.56001}&\textbf{0.66959}&0.48238\\
			\bottomrule
	\end{tabular}}
	\caption{Comparative evaluation on localization ability (higher is better).}
	\label{tab:3}
\end{table*}

The result is reported in Table~\ref{tab:3}. 
Even though the evaluation looks at how much attention falls within the target object bounding box (it cannot accurately reflect whether the explanation fits the edge of the object), FG-CAM performs better than other methods.
On average, over 50$\%$ of the values in the saliency map generated by FG-CAM fall into the target object bounding box, indicating that the explanation generated by FG-CAM is less noisy and more focused on the target object. 
It can be found from the result that FG-CAM with denoising performs significantly better than the original FG-CAM.

All experimental results show that there is a very significant difference whether our method is used or not. 
At the same resolution, FG-CAM outperforms other methods by a large margin.
This fully verifies that FG-CAM solves the problem that CAM-based methods cannot generate fine-grained explanations while achieving generality, and has better results than Layer-CAM, which is designed for shallow layers. 
Moreover, it can be concluded from the experimental results that FG-CAM's performance is almost unaffected by the explanation resolution.
We also found that FG-Grad-CAM performed better than FG-Score-CAM on faithfulness evaluation.
We believe that this is because the global weights obtained by Grad-CAM are more accurate and using interpolation masks this.


\subsection{Comparison with LRP and its variations}

\begin{table*}
	\centering
	\begin{tabular}{lccccccc}
		\toprule
		&LRP&CLRP&SGLRP&FG-Grad-CAM&\makecell{FG-Grad-CAM\\(denoising)}&FG-Score-CAM&\makecell{FG-Score-CAM\\(denoising)}\\
		\midrule
		A.D.&25.48&30.64&40.62&\textbf{16.81}&\textbf{16.13}&\textbf{20.28}&\textbf{16.84}\\
		\hline
		A.I.&25.0&24.15&21.75&\textbf{36.45}&\textbf{36.55}&\textbf{30.15}&\textbf{31.80}\\
		\bottomrule
	\end{tabular}
	\caption{The results of the faithfulness evaluation for LRP, CLRP, SGLRP and FG-CAM.}
	\label{tab:4}
\end{table*}

\begin{table*}
	\centering
	\begin{tabular}{lccccccc}
		\toprule
		&LRP&CLRP&SGLRP&FG-Grad-CAM&\makecell{FG-Grad-CAM\\(denoising)}&FG-Score-CAM&\makecell{FG-Score-CAM\\(denoising)}\\
		\midrule
		Insertion&0.55344&0.56740&0.51655&0.60757&0.61386&0.58608&0.60566\\
		\hline
		Deletion&0.09975&0.19168&0.22315&0.08668&0.11271&0.08980&0.11143\\
		\hline
		Over-all&0.45369&0.37572&0.29340&\textbf{0.52089}&\textbf{0.50115}&\textbf{0.49628}&\textbf{0.49423}\\
		\bottomrule
	\end{tabular}
	\caption{Results of insertion and deletion tests for LRP, CLRP, SGLRP and FG-CAM.}
	\label{tab:5}
\end{table*}

\begin{table*}[!htb]
	\centering
	\begin{tabular}{lccccccc}
		\toprule
		&LRP&CLRP&SGLRP&FG-Grad-CAM&\makecell{FG-Grad-CAM\\(denoising)}&FG-Score-CAM&\makecell{FG-Score-CAM\\(denoising)}\\
		\midrule
		Proportion&0.48385&0.59515&0.52136&0.49503&\textbf{0.65496}&0.5400&\textbf{0.66983}7\\
		\bottomrule
	\end{tabular}
	\caption{The results of evaluation on localization for LRP, CLRP, SGLRP and FG-CAM.}
	\label{tab:6}
\end{table*}

In this section, we compare LRP, SGLRP, CLRP with FG-CAM to verify that FG-CAM can generate explanation with high faithfulness and same resolution as the input image in the input layer, and outperforms LRP, SGLRP and CLRP significantly.
We performed the same experiment as in section 4.1, but in the visualization and faithfulness evaluations, the saliency map include pixels with negative contribution (do not calculate the ReLU result like CAM-based methods) to get more accurate evaluation results.
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{fig6.pdf}
	
	\caption{Visualization results of LRP, CLRP, SGLRP and FG-CAM. Red indicates positive contribution pixels (the darker the color, the greater the contribution) and blue indicates negative contributing pixels.}
	\label{fig:6}
\end{figure}

The visualization results are shown in Figure~\ref{fig:6}.
It can be found that the explanations generated by LRP are not class-discriminative (\eg on the first image, LRP even considers the pixels in the ’dog’ region to be more important).
The results of CLRP and SGLRP often remove the important regions of the target object, and even regard pixels of important regions as negatively contributing. 
SGLRP, in particular, may regard all pixels of the target object as negatively contributing, which may make people mistakenly think that CNN does not pay attention to the target object. 
FG-CAM correctly removed pixels of non-target classes without removing important parts of the target, and focused attention on the target class. 
The explanations generated by FG-CAM with denoising focus more attention on the target class and remove a lot of noise.


For the Average Drop/Increase metric, we retain only the most important $a\%$ pixels of the input image, where $a=\min(50,\frac{\sum_{i,j}Sign(I_{(i,j)}^c>0)}{M})$, $M$ is the number of pixels of saliency map. 
This is to ensure that only pixels with positive contribution are used. The result is reported in Table~\ref{tab:4}.
The A.D. and A.I. values of CLRP and SGLRP are very poor, due to the fact that CLRP and SGLRP often remove important regions of the target object. 
FG-CAM is significantly better than other methods in every metric, which indicates that FG-CAM still maintains high faithfulness of explanation in the input layer and can better highlight important pixels related to the model's decision. 

The results of the deletion and insertion tests are shown in Table~\ref{tab:5}.
In this tests, FG-CAM still outperforms LRP, CLRP and SGLRP significantly. 
For example, the Over-all score of FG-Grad-CAM is 0.0672 higher than LRP and 0.22749 higher than SGLRP, which is a huge lead. 
This indicates that the explanation with the same resolution as the input image generated by FG-CAM can fit the importance distribution of pixels well.

The localization evaluation results are shown in Table~\ref{tab:6}.
It can be found that CLRP and SGLRP have higher Proportion value than LRP, but lower faithfulness in explanation than LRP, which further suggests that they only focus on some regions of target object and often remove important regions of the target object.
The Proportion value of FG-CAM with denoising is significantly higher than that of LRP, CLRP and SGLRP. 
The Proportion value of FG-CAM is also better than that of LRP.
This indicates that FG-CAM maintains high attention to the target class without over-removing pixels.

The experiments in this section show that FG-CAM can not only generate explanations with the same resolution as the input image, but also outperform LRP and its variants by a large margin. 
Therefore, we believe that FG-CAM greatly mitigates the shortcoming of CAM-based methods.

\section{Conclusion}
In this paper, we propose FG-CAM, a novel visual explanation method.
Unlike existing CAM-based methods, FG-CAM uses the relationship between two adjacent layers of feature maps with resolution difference to gradually increase the explanation resolution, while finding the contributing pixels and filtering out the pixels that do not contribute at each step. 
FG-CAM solves the problem that CAM-based methods cannot generate fine-grained explanations from another perspective. 
It not only maintains the characteristics of CAM-based methods, but also produces better results. 
Moreover, different from other methods, the performance of FG-CAM is almost unaffected by the explanation resolution.
Therefore, FG-CAM can generate fine-grained visual explanation with high faithfulness in shallow and intermediate convolutional layers as well as in the input layer.
In the both shallow and intermediate convolutional layers, FG-CAM greatly outperforms CAM-based methods. 
While in the input layer, FG-CAM outperforms LRP and its variations significantly.
We also presents FG-CAM with denoising, which is able to generate less noisy explanations with almost no change in explanation faithfulness. Our method provides a new idea to solve the shortcoming of CAM methods, and achieves very good results. 
We believe that it will stimulate more research in this area.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}