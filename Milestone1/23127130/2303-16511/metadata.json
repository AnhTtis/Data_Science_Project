{
    "arxiv_id": "2303.16511",
    "paper_title": "Joint unsupervised and supervised learning for context-aware language identification",
    "authors": [
        "Jinseok Park",
        "Hyung Yong Kim",
        "Jihwan Park",
        "Byeong-Yeol Kim",
        "Shukjae Choi",
        "Yunkyu Lim"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "eess.AS"
    ],
    "abstract": "Language identification (LID) recognizes the language of a spoken utterance automatically. According to recent studies, LID models trained with an automatic speech recognition (ASR) task perform better than those trained with a LID task only. However, we need additional text labels to train the model to recognize speech, and acquiring the text labels is a cost high. In order to overcome this problem, we propose context-aware language identification using a combination of unsupervised and supervised learning without any text labels. The proposed method learns the context of speech through masked language modeling (MLM) loss and simultaneously trains to determine the language of the utterance with supervised learning loss. The proposed joint learning was found to reduce the error rate by 15.6% compared to the same structure model trained by supervised-only learning on a subset of the VoxLingua107 dataset consisting of sub-three-second utterances in 11 languages.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16511v1"
    ],
    "publication_venue": "Accepted by ICASSP 2023"
}