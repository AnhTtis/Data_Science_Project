% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage[normalem]{ulem}
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{multicol}


\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts}
\usepackage{bm}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{color}
\usepackage{booktabs} % For formal tables
\usepackage{dsfont}
\usepackage{subfigure}
\usepackage[normalem]{ulem}
\newcommand{\todo}[1]{\textcolor{red}{\hl{[#1]}}}



\begin{document}
%
\title{Large Language Models are Diverse Role-Players \\ for Summarization Evaluation}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Anonymous Author\inst{1}}%\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
%\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Anonymous Institute}% \and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}
%


\author{Ning Wu \and
Ming Gong\thanks{Corresponding author.} \and
Linjun Shou \and
Shining Liang \and
Daxin Jiang} 
%
\authorrunning{N. Wu et al.}
\institute{STCA Search \& Distribution Group, Microsoft, China\\
\email{wuning,migon,lisho,shiningliang,djiang@microsoft.com}}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%



\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. A document summary’s quality can be assessed by human annotators on various criteria, both objective ones like grammar and correctness, and subjective ones like informativeness, succinctness, and appeal.
Most of the automatic evaluation methods like BLUE/ROUGE may be not able to adequately capture the above dimensions. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a context-based prompting mechanism that is able to generate dynamic roleplayer profiles based on input context. Finally, we design a multi-roleplayer prompting technology based on batch prompting and integrate multiple outputs into the final evaluation results.   Experimental results on three real datasets for summarization show that our model is highly competitive and has a very high consistency with human annotators.

\keywords{Large Language Model  \and Summarization Evaluation \and Role Player.}
\end{abstract}
%
%
%
\section{Introduction} \label{sec:intro}
Text summarization has wide applications in various research and application fields.  Recently, some works found that there is a clear gap between existed metrics like BLEU~\cite{papineni2002bleu}, ROUGE, BertScore~\cite{zhang2019bertscore} and human annotations~\cite{goyal2022news,yuan2022selecting}.
Although typical overlap-based and model-based metrics can capture lexicon level or semantic level similarity between generated text and reference text, specific dimensions like coherence, grammar, and interestingness still can't be captured. As depicted in Figure 1, the summarization task reveals the inadequacy of traditional metrics such as BLUE/ROUGE: they are unable to reflect the true quality of the text after reaching a certain level. To achieve consistency between human evaluation and automatic metrics, we encounter two main challenges: 1) How to model objective criteria of evaluation such as coherence and grammar. 2) How to model subjective criteria of evaluation such as interestingness~\cite{gao2014modeling,hidi1986interestingness}, comprehensiveness, and usefulness from the standpoint of users. Natural language has various modes of expression for the same concept, so assessing its quality based on a few static criteria is hard.



\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{intro.pdf}
    \caption{Two summarizations of CNN News, they are generated by two models (GPT3~\cite{brown2020language}, T0~\cite{sanh2021multitask}), and have similar BLEU and ROUGE metrics, but the second summary is obviously worse than the first one on two more complicated dimensions. }
    \label{first}
\end{figure}

Motivated by the ability of LLMs to handle multi-domains, we investigate how to leverage LLMs for measurement in this paper. Since it is difficult to make LLMs provide a consistent and fair score for the generated text~\cite{wang2022self}, we propose a comparison-based evaluation method to quantify the quality of the generated text, namely DRPE, which stands for \uline{D}iverse \uline{R}ole-\uline{P}layer for Summarization Generation \uline{E}valuation. In particular, we devise a roleplayer-based prompting strategy in this system for objective and subjective dimension measurement. Our method comprises two parts: 1) Static roles construction and dynamic roles prompts generation. 2) A multi-roleplayer framework to conduct a comprehensive evaluation. 

%Secondly, some prompts  different dimensions are selected for different tasks
For a given generation task, its measurement can be broken down into several dimensions. Typical objective metrics such as coherence and grammar are relatively easy to be agreed upon by most people, so we manually created static roles for each objective dimension of the task. It is expressed as <\emph{Judger types}, \emph{Judger description}>. With a static role, we prompt LLM by asking it to impersonate a real judger based on the judger type and description and then vote for the better option. Furthermore, a comprehensive measurement is usually complex and dynamic. Depending on different cases in a summarization task, different aspects need to be taken into account. Therefore, we propose to dynamically generate some potential users based on the content and let LLMs conduct subjective measurements on behalf of these users. The dynamic roles can be expressed as <\emph{User types}, \emph{User description}>. Lastly, we design a multi-roleplayer framework to eliminate redundant roleplayers and integrate the vote results of multiple roleplayers. Moreover, the multi-roleplayer framework can also enhance the stability of the LLM-based measurement system with relatively low inference costs. Experimental results show that our method significantly surpasses zero-shot LLMs and existing metrics on three typical summarization datasets with human annotations.


\section{Related Works}
\subsection{Large Language Model}
Large language model has been found to be capable of few-shot learning~\cite{brown2020language}. Chain-of-Thought~\cite{wei2022chain} is proposed to empower model reasoning capability for complex tasks. ZeroShot-Cot~\cite{kojima2022large} still shows relatively strong reasoning ability without any examples. Least-to-Most~\cite{zhou2022least} LLM decomposes a complex question into several sub-questions and solves these sub-questions in sequence and finally gives a complete answer to the original question. Recent work~\cite{goyal2022news} discovered that both reference-based
and reference-free automatic metrics cannot reliably evaluate zero-shot summaries. In this paper, we mainly explore the capability of LLM to compare the generated text and reference text.


\subsection{Existed Metrics}

The most widely used metric in machine translation is BLEU~\cite{papineni2002bleu}, which includes several modifications to Exact-$P_n$. A smoothed variant, SENTBLEU is computed at the sentence level. METEOR~\cite{banerjee2005meteor} computes Exact-$P_1$ and Exact-$R_1$ while allowing backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases. ROUGE~\cite{lin2004rouge} is a commonly used metric for summarization evaluation. ROUGE-n computes Exact-$R_n$ (usually n = 1, 2), while ROUGE-L is a variant of Exact-$R_1$ with the numerator replaced by the length of the longest
common subsequence. BERTScore~\cite{zhang2019bertscore} computes a similarity score for each token in the candidate sentence with each token in the reference sentence. 
MoverScore~\cite{zhao2019moverscore} investigates the effectiveness of existing contextualized representations and Earth Mover’s Distance~\cite{rubner2000earth} for comparing system predictions and reference texts, leading to a new automated evaluation metric that achieves high correlation with human judgments of text quality.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{CrowdScore_Framework.pdf}
    \caption{The overall framework of DRPE. Objective roleplayers are curated manually based on task setting, and subjective roleplayers are dynamically generated by LLMs. After diversity clustering, similar roles are eliminated, and all roles are played by LLMs to compare two candidates in batch prompting. Finally, results from multiple roles are aggregated. }
    \label{first}
\end{figure*}

\section{Methodology}
As discussed in Section~\ref{sec:intro}, currently, it forms a gap with human evaluation that automatic metrics for text generation stop at surface similarity (lexicon level or semantic level) which leads to biased perception and evaluation of the text generation capability of LLMs. In this section, we elaborate our proposed measurement framework for text generation primarily includes diversified roleplayers generation and roleplayers-based evaluation.



\subsection{Diversified RolePlayers Generation}~\label{sec:role_gen}
To build a novel framework differing from existing calculation-based automatic metrics, we decompose this task into objective and subjective dimensions and propose an LLM-based measurement method. In our framework, the LLMs act as a judge with a distinctive role to evaluate the text quality in a corresponding dimension and generate its evaluation results. Thus, we need to generate diversified roleplayers for objective and subjective dimensions at first.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{static_role_v2.pdf}
    \caption{Three different static roles for summarization task. For different generation tasks, different aspects need to be taken into consideration. A good summary usually requires fluent writing, proper wording and capturing key points of raw article. }
    \label{fig-static}
\end{figure}

%, while other tasks like translations should be accurately aligned with source text and elegant

\subsubsection{Objective RolePlayers}
%Overlap-based metrics such as BLUE/ROUGE measure lexicon-level consistency with n-grams. Model-based metrics like BERTScore can capture subtle syntax or semantic changes with embeddings similarity, but they have two limitations in evaluating high-quality text from LLMs. We use BERTScore as an example. First, the embedding space of LLMs differs from that of BERT because they are pre-trained on different corpora and strategies. Second, the parameter scale of BERT is much smaller than that of LLMs, so it cannot represent LLMs’ rich semantics.
Overlap-based metrics such as BLUE/ROUGE measure lexicon-level consistency with n-grams. Model-based metrics like BERTScore can capture subtle syntax or semantic changes with embeddings similarity, but they have a limitation in evaluating high-quality text from LLMs. The parameter scale of BERT is much smaller than that of LLMs, so it cannot represent LLMs’ rich semantics.
Consequently in this paper, we take some advanced quality dimensions like fluency, consistency, grammar and coherence into consideration, which were rarely adopted before as they are difficult to be measured accurately. Since different tasks usually require different objective dimensions, and these dimensions are relatively easy to be agreed on by most people, hence we manually curated static objective dimensions for the summarization task and  make sure all these dimensions are fundamental and objective.  The static objective roles schema is presented below:<\emph{Judger type}, \emph{Judger description}>. where each \emph{Judger} works on one or multiple specific objective dimensions and \emph{Judger description} breaks down and exposit what specifically the Judger would focus on when evaluating. As shown in Figure~\ref{fig-static}, three different objective roles are designed for summarization tasks.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{dynamic_role.pdf}
    \caption{Coarse-grained and fine-grained prompting mechanism for comprehensive user profiles generation.  }
    \label{fig-dynamic}
\end{figure*}


\subsubsection{Subjective RolePlayers}
Text generation, unlike understanding tasks, does not have a perfect golden standard. Human-written material can only offer a high-quality example. Different readers may judge the text according to their own perspectives. For instance, consider the sports news about a renowned athlete in a game,
\begin{itemize}
    \item For the writing style, ordinary readers expect that it's concise and easy to understand, while journalists would pay attention to its structure and choices of words.
    \item For the content, causal fans like comprehensive data of the sports player and horizontal comparison with others, while die-hard fans are more eager for in-depth analysis of the sports player through data.
\end{itemize}

Therefore, we propose to collect subjective evaluations of model-generated text from diverse user perspectives, including whether they think the text is interesting, useful, etc. These dimensions are more abstract and difficult to quantify than objective dimensions which few studies have touched on and addressed to our knowledge. Specifically, we take each generated text as the context and prompt the LLM to generate its potential readers dynamically following the below schema: <\emph{User type}, \emph{User description}>. Here we design two user role generation prompts. As shown in Figure~\ref{fig-dynamic}, the former requires the LLM to consider the most common occupations with most people in the world which is coarse-grained and the latter aims to categorize people based on their familiarity with the text topics which is fine-grained. We merge objective judgers and subjective users generated by two kinds of prompting mechanisms as multi-role players for the next process. Considering that there may exist duplicate or similar users, we propose to conduct diversity clustering to improve measurement performance and reduce inference costs. First, each roleplayer type and its description are concatenated as the input of Sentence-BERT~\cite{reimers2019sentence} to obtain the representation. Next, we use the $k$-means algorithm to cluster roleplayers, and those closest to each cluster center are kept. Finally, the chosen role players will be leveraged for text evaluation.



\subsection{RolePlayer-based Evaluation}~\label{sec:txt_eval}
To mitigate the discrepancy between the human evaluation and automatic metrics, we propose to leverage the roleplayers as crowdsourcing voters to compare the summaries from multi-dimensions. Besides the static roleplayers that scrutinize the objective dimensions including grammar, fluency, coherence, etc, the dynamic roleplayers are generated according to the current article and simulate the psychology and behavior of the article readers (roles) to convey their subjective feelings. It's expected that our method could achieve higher consistency with human evaluation than existing automatic metrics focusing on surface similarity.

\subsubsection{Evaluation of RolePlayer}
Given the article $A$, we posit the reference summary generated by humans as $S$, and the candidate summary generated by models as $\hat{S}$, respectively. To evaluate, all the roleplayers perform pair-wise comparison as Figure~\ref{fig-batch}, since point-wise comparisons are inconsistent across different samples and list-wise comparisons are not stable. By our prompting, the LLMs play a specific role and output its analysis of two summaries, finally voting which summary is of better quality. In more detail, we parse and quantify the comparison result as $\hat{a}$:
\begin{equation}
    \hat{a} =
    \left\{ \begin{aligned}
        & 1    && {\text{If voting is candidate summary $\hat{S}$ }} \\
        & 0    &&  {\text{If voting is reference summary $S$ }} \\
    \end{aligned}
    \right.
\end{equation}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{case_study.pdf}
    \caption{Compare generated summary and reference summary by multiple roleplayers with batch prompting. }
    \label{fig-batch}
\end{figure*}

Assuming the DRPE score for generated text $\hat{S}$ is $\text{DRPE}(\hat{S}|A, S)$, it could be obtained by modeling the joint probability of reason and voting result as below:
\begin{align}\label{eq:DRPEScore}
    \text{DRPE}(\hat{S}|A, S) = \mathds{1}(\hat{a}=1)P(\hat{a}, \bm{r}|\bm{p}, A, S, \hat{S}, R)
\end{align}
where $\bm{r}$ is the comparison reason, $\bm{p}$ represents the prompt used here, $\hat{a}$ is voting result from LLMs, $A$ is raw article and $R$ is the role. To compute $P(\hat{a}, \bm{a})$, similar to~\cite{wang2022self}, we leverage the confidence when roleplayer generates its voting normalized by the output length~\cite{brown2020language},
\begin{align}
    & P(\hat{a}, \bm{r}|\bm{p}, A, S, \hat{S}, R) \nonumber \\
    & = \exp^{\frac{1}{K}\sum_{k=1}^K\log P(t_k|\bm{p}, A, S, \hat{S}, R, t_1, \dots, t_{k-1})}
\end{align}
where $\log P(t_k|\bm{p}, A, S, \hat{S}, R, t_1, \dots, t_{k-1})$ is the log probability of $k$-th token $t_k$ in $\bm{r}$ and $K$ is number of tokens.



% Then the voting results are aggregated as the corresponding voting probability distribution from the roleplayers, $\bm{w} = \{w_k\}_{k=1}^K$.
% Specifically, the voting probabilities for a summary pair ($\bm{s}_j$, $\bm{s}_k$) by the roleplayer $\bm{r}_i$ can be obtained as below:
% \begin{align}
%     P(\bm{w}_{j,k}|\bm{p}, A, \bm{s}_j, \bm{s}_k) = \sum_{i=1}^{N}P(\bm{w}_{j,k}|\bm{p}, A, \bm{s}_j, \bm{s}_k, \bm{r}_i)
% \end{align}
% where $\bm{w}_{j,k} = (w_j, w_k)$, $\bm{p}$ represents the prompt used here, and $N$ is the number of roleplayers.

\subsubsection{Batch Prompting}
To efficiently get comparison results for the summary pair ($S$, $\hat{S}$) from multiple roleplayers, we design a multi-roleplayer framework based on batch prompting to measure by both objective and subjective metrics in one-off inference. The different metrics are reflected by the objective and subjective roleplayers generated and clustered in Section~\ref{sec:role_gen}. 
As shown in Figure~\ref{fig-batch}, first, all the roleplayers are prompted to give votes for ($S$, $\hat{S}$) with respect to $A$ in their point of view, i.e., which summary could better describe the article, and tell us the reasons.
Then we aggregate the results to parse $\hat{a}=\{\hat{a}_j\}_{j=1}^N$ where $N$ is the number of roleplayers. According to Equation~\ref{eq:DRPEScore}, the multi-roleplayer DRPE score by batch prompting can be formulated as below:
\begin{align}
     & \text{DRPE}(\hat{S}|A, S) = P(\hat{a}|\bm{p}, A, S, \hat{S}) \nonumber \\
     & = \sum_{j=1}^N\mathds{1}(\hat{a}_j=1)P(\hat{a}_j, \bm{r}|\bm{p}, A, S, \hat{S}, R_j)
\end{align}
where $R_j$ denotes $j$-th roles. Compared with Self-Consistency CoT~\cite{wang2022self}, our framework decouples the answer (comparison result) and reasoning path (comparison reason), and brings higher inference efficiency as Self-Consistency CoT needs to generate each candidating answer separately while our method generates all voting results with once inference.

% As our evaluation method is not summary position sensitive, each summary will be compared once with others. Consequently, the LLM conducts inference totally $C_2^K$ times. 
% After going through all the summary pairs, the voting score for the summary $\bm{s}_j$ can be extracted and normalized as below:
% \begin{gather}
%     \bar{P}(w_j) = \sum_{k=1}^{j-1}P(\bm{w}_{k, j}) + \sum_{k=j+1}^{K}P(\bm{w}_{j, k}) \\
%     P(w_j) = \frac{\exp{\bar{P}(w_j)}}{\sum_{k=1}^K\exp{\bar{P}(w_k)}}
% \end{gather}
% Similarly, we could get the voting probability distribution $\bm{w}$ from our roleplayers and collate with human evaluation.


\section{Experiments}


\subsection{Experiments Setting}
\subsubsection{Datasets.} 


 \textbullet \textbf{CNN2022}~\cite{nallapati2016abstractive,hermann2015teaching}: contains reference summaries that are approximately 3-4 sentences long. Summaries in this dataset are highly extractive and lead-biased. We use human study data on 100 recent articles from CNN, collected between March 1, 2022, and June 31, 2022. Each article is annotated by at least three judgers, and they are asked to choose the best and worst summaries from three candidates. To reduce noises, we only keep the best summary with at least 2 out of 3 votes and worst summary with at least 2 out of 3 votes. Finally, we obtain 81 best and worst summaries as candidate summaries with a piece of corresponding news. Finally, we use GPT-3 of the text-DaVinci-003 to generate reference summarization, and finally use our method to compare the candidate summary.

  \textbullet \textbf{BBC2022}~\cite{narayan2018don}: contains 1 sentence summaries of BBC news articles. In this dataset, references summaries, and consequently generated summaries from fine-tuned models are highly abstractive. We use human study data on 100 recent articles from BBC, collected between March 1, 2022, and June 31, 2022. We also take a similar preprocessing strategy like CNN2022 on this dataset.


  \textbullet \textbf{SummEval}~\cite{fabbri2021summeval}: contains a large and diverse collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. For each news, we select two worst summaries and two best summaries according to their average scores on four dimensions~(coherence, consistency, fluency and relevance) labeled by experts. Finally, regarding one hundred news and corresponding reference summary in SummEval, we obtain 400 candidate summaries.


 

\subsubsection{Metrics.} 


To measure the consistency between various metrics and human annotations, we follow the WMT18~\cite{ma2018results} standard practice and use absolute Pearson correlation $|\rho|$ to evaluate metric quality.

%and compute significance with the Williams test for $|\rho|$.

%and calculate Kendall rank correlation $\tau$ 
\subsubsection{Baselines.} 
Automatic metrics proposed for summarization evaluation can be broadly divided into two categories: (1) overlap-based metrics, specifically ROUGE METEOR and BLEU, and (2) similarity-based metrics that compute the similarity between embeddings representations of generated and reference summaries. Specifically, we report BERTScore and MoverScore. For LLMScore, we carefully design prompts for LLM, and directly use it to predict a better passage. 

%We also design one LLM-based metric and called it LLMScore. 

%For MaunalDRPEScore, we manually design several roles for it as follows:

% \textbullet \textit{Students}: These are studying in high school and want to know more about the world.
 
% \textbullet \textit{Business Man}: These are working in a big company and are interested in any business opportunity.
 
% \textbullet \textit{Politician}: These are pursuing an election victory in a medium country.
 
% \textbullet \textit{General public}: These readers may be interested in the story and looking for updates on the investigation.
 
% \textbullet \textit{Critic}: These people will check fluent writing, clear sentences, and good wording in summary writing.
 
% \textbullet \textit{News Author}: These readers will check the summary for consistency with the original article.
 

\subsubsection{Implementation.}
We use the public GPT-3~\cite{brown2020language} of the text-DaVinci-003 version with 175B parameters from OpenAI for the LLMs implementation and use greedy decoding for inference with the temperature set to 0. We select this LLM because it has relatively good capability among public LLMs. Especially, we use three roles \textit{General Public}, \textit{Critic}, and \textit{News Author} which are described in Figure~\ref{fig-static}  as objective roleplayers in our DRPE, and we prompt the model to generate 4 dynamic roles for each case.



\subsection{Results}

%\begin{table}[!ht]
%  \centering
%  \caption{Pearson correlation between several automatic metrics and %human annotation. MDRPE denotes ManualDRPEScore and DRPE denotes% DRPEScore.}
%  \setlength{\tabcolsep}{3.0mm}{
%  \resizebox{0.95\columnwidth}{!}{%
%    \begin{tabular}{l|c|c|c|c}
%    \toprule
%    Metrics      & BertScore  & LLM  & MDRPE  & DRPE \\
%\midrule
%    CNN  & 0.55  & 0.58  & 0.75  & \textbf{0.90}  \\
%    XSum & 0.57  & 0.62  & 0.73  & \textbf{0.87}  \\
%    \bottomrule
%    \end{tabular}}%
%    }
%  \label{tab:abla}%
%\end{table}%



\begin{table}[!ht]
  \centering
  \caption{Pearson correlation between several automatic metrics and human annotation. We bold the highest numbers for each dataset, and use AVG to denote the average scores on the three datasets. Results of GPT-D3 and DRPE are averaged over five runs with slight changing in prompts.}
  \setlength{\tabcolsep}{3.0mm}{
  \resizebox{0.8\columnwidth}{!}{%
    \begin{tabular}{l|ccccc}
    \toprule
\hspace{0.5em} Type &    Method      & CNN2022  & SummEval  & BBC2022 & AVG \\
\midrule
\multirow{5}{*}{\hspace{0.2em} Overlap} &    ROUGE-1  & 0.466  & 0.431 & 0.469  & 0.461  \\
&    ROUGE-2  & 0.437  & 0.354  & 0.443  & 0.411  \\
&    ROUGE-L  & 0.422  & 0.322  & 0.436  & 0.393  \\
&    BLEU & 0.475  & 0.372  & 0.502  & 0.450  \\
&    METEOR & 0.514  & 0.473  & 0.561  & 0.516  \\
\midrule
\multirow{2}{*}{Similarity} &      BERTScore & 0.554  & 0.455  & 0.568  & 0.526  \\
&    MoverScore & 0.456  & 0.385  & 0.442  & 0.428 \\
\midrule
\multirow{2}{*}{\quad LLM} &      GPT-D3 & 0.713  & 0.503  & 0.692  & 0.636  \\
&    DRPE & \textbf{0.816}  & \textbf{0.683}  & \textbf{0.784}  & \textbf{0.761}  \\

    \bottomrule
    \end{tabular}}%
    }
  \label{tab:main}%
\end{table}%










\subsubsection{Comparison with Existed Metrics}
Tables~\ref{tab:main} shows Pearson correlation to human judgments. We observe that typical overlap-based metrics generally performed badly and relevance-based metrics also underperformed. The simplest LLM-based method has a consistently better performance than BERTScore. Two types of LLM-based methods, GPT-D3, and DRPEScore have a clear gap between the other two methods. Especially, DRPEScore consistently performs better than GPT-D3.

\begin{table}[!ht]
  \centering
  \caption{Pearson correlation between several automatic metrics and human annotation. AVG denotes the average scores on the three datasets.}
  \setlength{\tabcolsep}{3.0mm}{
  \resizebox{0.8\columnwidth}{!}{%
    \begin{tabular}{l|cccc}
    \toprule
   Method      & CNN2022  & SummEval  & BBC2022 & Avg \\
\midrule
   DRPE & 0.816  & 0.683 & 0.784  & 0.761  \\
   w/o Batch Inferring  & 0.822  & 0.672  & 0.766  & 0.753  \\
   w/o Clustering  & 0.782  & 0.665  & 0.751  & 0.733  \\
   w/o Dynamic Roles  & 0.742  & 0.669  & 0.703  & 0.705  \\
   w/o Static Roles  & 0.734  & 0.604  & 0.711  & 0.683  \\

    \bottomrule
    \end{tabular}}%
    }
  \label{tab:abla}%
\end{table}%


\subsubsection{Ablation Study}
We conducted an ablation study on DRPE to gain insights into the detailed method design. We prepare four variants of our method: 
(1) \uline{w/o Batch Inferring} denotes without the batch prompting, each role is inferred alone;
(2) \uline{w/o Clustering} denotes without clustering mechansim;
(3) \uline{w/o Dynamic Roles} denotes without dynamic roles generation;
(4) \uline{w/o Static Roles} denotes without the human designed static roles.
Table~\ref{tab:abla} presents all comparison results of the four variants. As we can see, the performance rank on three datasets can be given as: w/o Static Roles < w/o Dynamic Roles < w/o Clustering < w/o Batch Inferring < DRPE. These results indicate that all components are essential to improve performance. And we can also find that batch inferring is able to save lots of inference tokens without losing performance. 

%the margin between DPRE and w/o Batch Inferring is small, it means 

\begin{figure}[!ht]
\centering
\subfigure[Roles number k w.r.t correlation on CNN2022 dataset.]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=0.95\linewidth]{cnn_role_num_v3.pdf}
%\caption{fig1}
\end{minipage}\label{fig:case-study-a}
}%
\subfigure[Roles number k w.r.t correlation on BBC2022 dataset.]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=0.95\linewidth]{xsum_role_num_v3.pdf}
%\caption{fig2}
\end{minipage}\label{fig:case-study-b}
}%
%
%
\centering
\caption{Effect of role number on model performance. } \label{fig:retrieval_number}
\end{figure}



\subsubsection{Effects of Hyperparameters}


We test DRPEScore and \uline{w/o Static Roles} with different subjective role numbers $k$ in [0, 2, 4, 6] on two datasets. Figure~\ref{fig:retrieval_number} shows the experimental results. When $k$ increases from 0 to 6, the experimental indicators first increase and then slightly decrease on the CNN2022 dataset,  when $k$=4, the correlation to human evaluation achieves a peak. On the BBC2022 dataset, experimental results are similar, and more roles~(6) don't bring improvements compared to fewer roles~(4). %With the increase of role numbers k, we observe some irrelevant roles are generated and they are not so helpful to judgment. 

%Especially, when $k$ is 0, all subjective roles are disabled, and only 3 objective roles are kept for DRPE. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth]{final_case.pdf}
    \caption{Evaluation procedure for two summaries given a piece of news. We use the green font to represent the content generated by the model. With suitable prompting, the LLM is able to generate relevant roles and generate judgment for these roles.  }
    \label{fig-case}
\end{figure*}


\subsubsection{Qualitative Analysis}
We have previously demonstrated the effectiveness of our model on two summarization tasks. In this section, we conduct a qualitative analysis to explain why DRPEScore can achieve good performance. Figure~\ref{fig-case} shows an example of our model’s evaluation process. Given a news article about the legal dispute between Amber Heard and Johnny Depp, our model has to select a better summary from two candidates. First, we generate several subjective roles based on the news content, such as \emph{Amber Heard Fans User}, \emph{Johnny Depp Fans User}, \emph{Celebrity Gossip User}, and \emph{Legal System User}. These roles are representative of different perspectives and preferences that can be captured by LLMs. Second, we use LLMs to simulate each user and judger and compare the two summaries. We employ a batch prompting mechanism to accelerate the inference procedure. Notably, LLMs predict that the \emph{Johnny Depp Fans User}, who might have a negative attitude towards \emph{Amber Heard Fans User}, will favor summary 1.


% For each case, there are only two LLMs calls. 



\section{Conclusion}
We propose DRPE, a new comparison-based method for evaluating generated text against gold standard references. Our DRPE is designed to be simple and task-agnostic. Our experiments illustrate that DRPE could provide a human-like ability to conduct a comprehensive evaluation, especially
on challenging long text generation like summarization tasks.  In future work, we look forward to exploring the capabilities of LLMs as judgers on more text evaluation tasks and reducing the computation cost.



%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%

\bibliography{anthology,custom}

\bibliographystyle{splncs04}
\end{document}
