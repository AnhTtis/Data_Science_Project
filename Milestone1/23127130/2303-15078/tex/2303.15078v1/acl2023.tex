% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts, amsthm}
\usepackage{bm}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{color}
\usepackage{booktabs} % For formal tables
\usepackage{dsfont}
\usepackage{subfigure}
\usepackage[normalem]{ulem}
\newcommand{\todo}[1]{\textcolor{red}{\hl{[#1]}}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Large Language Models are Diverse Role-Players \\ for Summarization Evaluation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
  Ning Wu \quad Ming Gong \quad Linjun Shou \quad Shining Liang \quad Daxin Jiang\thanks{~~Corresponding author.} \\
  Microsoft STC Asia \\
\\
~~\{wuning, migon, lisho, shiningliang,dji \}@microsoft.com~~
}



\begin{document}
\maketitle
\begin{abstract}
%Large language models~(LLMs) have been widely used in many natural language processing~(NLP) tasks due to superior language comprehension ability and creativity. Some works start to explore the application of LLMs on traditional generation tasks like summarization and question generation. However, most of these works mentioned that there is a clear divergence between existed metrics and human evaluation on the content generated by LLM. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. Frist, we propose to model objective and subjective dimensions of generated text based on roleplayer prompting mechanism. Furthermore, we introduce a context-based prompting mechanism which is able to generate dynamic roleplayer profiles based on input  context. Finally, we design a multi-roleplayer prompting technology based on batch prompting to integrate multiple evaluation results into evaluation result.   Experimental results on two real datasets for summarization show that our model is highly competitive and have a very high consistency with human annotators.

% Daxin's comments
Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. For example, the quality of a document summary can be measured by human annotators from both objective aspects, such as grammatical and semantic correctness, as well as subjective dimensions, such as comprehensiveness, succinctness, and interestingness. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to capture the above dimensions well. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a context-based prompting mechanism that is able to generate dynamic roleplayer profiles based on input  context. Finally, we design a multi-roleplayer prompting technology based on batch prompting to integrate multiple evaluation results into evaluation results.   Experimental results on two real datasets for summarization show that our model is highly competitive and has a very high consistency with human annotators.


\end{abstract}
%With the rise of large language models~(LLMs), some works propose to apply LLMs to generation tasks to improve generation quality.
\section{Introduction}~\label{sec:intro}
%https://arxiv.org/pdf/2301.13848.pdf
Text summarization has wide applications in various research and application fields.  Recently, some works found that there is a clear gap between existed metrics like BLEU, ROUGE, BertScore~\cite{zhang2019bertscore} and human annotations~\cite{goyal2022news,yuan2022selecting}. Although typical overlap-based and model-based metrics can capture lexicon level or semantic level similarity between generated text and reference text, specific dimensions like coherence, grammar, interestingness and etc still can't be captured. As shown in Figure 1 for the summarization task, when the text quality reaches a certain level, typical metrics like BLUE/ROUGE can't represent real text quality. To bridge the gap between human evaluation and automatic metrics, there are mainly two challenges: 1) How to model objective evaluation dimensions like coherence, grammar and etc. 2) How to model subjective evaluation dimensions like interestingness~\cite{gao2014modeling, hidi1986interestingness}, comprehensiveness and usefulness from the view of users. Natural language has diverse expressions for one thing, so it's hard to measure its quality only based on several static dimensions. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{intro.pdf}
    \caption{Two summarizations of CNN News, they are generated by two models (GPT3~\cite{brown2020language}, T0~\cite{sanh2021multitask}), and have similar BLEU and ROUGE metrics, but the second summary is obviously worse than the first one on multiple dimensions. }
    \label{first}
\end{figure}

Inspired by the capability of LLMs on multi-domains, we will explore how to utilize LLMs to conduct measurement in this paper. Since it's hard to let LLMs give a consistent score across different samples\cite{wang2022self}, we propose a comparison-based evaluation method to quantify the quality of the generated text, namely DRPE, which stands \uline{D}iverse \uline{R}ole-\uline{P}layer for Text Generation \uline{E}valuation. Especially, we design a roleplayer-based prompting strategy in this system for objective and subjective dimension measurement. Our method consists of two parts: 1) Static role construction and dynamic role prompts generation. 2) A multi-roleplayer framework to conduct a comprehensive evaluation. 

%Secondly, some prompts  different dimensions are selected for different tasks
Given a generation task, its measurement can be decomposed into several dimensions. Typical objective metrics like coherence and grammar are relatively easy to be agreed on by most people, so we created static roles manually for each objective dimension of the task. It is formulated as <\emph{Judger types}, \emph{Judger description}>.  With a static role, we prompt LLM by asking it to pretend a real judger based on judger type and description and then vote for the better choice. However, a comprehensive measurement is usually complex and dynamic. With different cases in a text generation task, different dimensions need to be considered. Hence we propose to dynamically generate some potential users based on the content and let LLMs conduct subjective measurements for these users. The dynamic roles can be formulated as <\emph{User types}, \emph{User description}>.  Finally, we propose a multi-roleplayer framework to remove redundant roleplayers and integrate the vote results of multiple roleplayers. Besides, the multiple roleplayers framework can also improve the stability of the LLM-based measurement system. Experimental results demonstrate that our method significantly outperforms zero-shot LLMs on two typical summarization datasets.
%Intuitively, writing some labeling examples for each dimension, and using them for in-context learning is a straightforward way to reach the goal.  

%However, there are still some issues 

%LLMs in-context learning 

%However, there are still three potential issues when applying LLMs to measurement tasks: 1) 
%2) Unstablility: LLMs are easy to be affected by prompts and produce some exceptional bad results . 2) Evaluation: One meaning can always have many wonderful wording expressions, but existed overlap based metrics can only calculate the overlap between generated content and one ground truth.


%To conduct a comprehensive measurement for text generation, we first categorize the measurement metrics of text into two types: objective metrics and subjective metrics. Since different generation tasks usually require different measurement dimensions, we first design measurement dimensions based tasks
%directly prompt LLM for judgement 
%first design some Grammar, coherence, rhetoric dimensions  independence


%Hence, we propose a roleplayer-based prompting strategy to empower LLMs on content generation and evaluation, named \emph{RPG~(\uline{R}ole\uline{P}layer-based \uline{G}eneration)} and \emph{RPE~(\uline{R}ole\uline{P}layer-based \uline{E}valuation)}, respectively.




\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{CrowdScore_Framework.pdf}
    \caption{The overall framework of DRPE. Objective roleplayers are curated manually based on task setting, and subjective roleplayers are dynamically generated by LLMs. After diversity clustering, similar roles are eliminated, and all roles are played by LLMs to compare two candidates in batch prompting. Finally, results from multiple roles are aggregated. }
    \label{first}
\end{figure*}

\section{Related Works}
\subsection{Large Language Model}
Large language model has been found to be capable of few-shot learning~\cite{brown2020language}. Chain-of-Thought~\cite{wei2022chain} is proposed to empower model reasoning capability for complex tasks. ZeroShot-Cot~\cite{kojima2022large} still shows relatively strong reasoning ability without any examples. Least-to-Most~\cite{zhou2022least} LLM decomposes a complex question into several sub-questions and solves these sub-questions in sequence and finally give a complete answer to the original question. In this paper, we mainly explore the capability of LLM to compare the generated text and reference text.


\subsection{Existed Metrics}

The most widely used metric in machine translation is BLEU~\cite{papineni2002bleu}, which includes several modifications to Exact-$P_n$. A smoothed variant, SENTBLEU is computed at the sentence level. METEOR~\cite{banerjee2005meteor} computes Exact-$P_1$ and Exact-$R_1$ while allowing backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases. ROUGE~\cite{lin2004rouge} is a commonly used metric for summarization evaluation. ROUGE-n computes Exact-$R_n$ (usually n = 1, 2), while ROUGE-L is a variant of Exact-$R_1$ with the numerator replaced by the length of the longest
common subsequence. BERTScore~\cite{zhang2019bertscore} computes a similarity score for each token in the candidate sentence with each token in the reference sentence. 

\section{Methodology}
As discussed in Section~\ref{sec:intro}, currently, it forms a gap with human evaluation that automatic metrics for text generation stop at surface similarity (lexicon level or semantic level) which leads to biased perception and evaluation of the text generation capability of LLMs. In this section, we elaborate our proposed measurement framework for text generation primarily includes diversified roleplayers generation and roleplayers-based evaluation.

\subsection{Diversified RolePlayers Generation}~\label{sec:role_gen}
To build a novel framework differing from existing calculation-based automatic metrics, we decompose this task into objective and subjective dimensions and propose an LLM-based measurement method. In our framework, the LLMs act as a judge with a distinctive role to evaluate the text quality in a corresponding dimension and generate its evaluation results. Thus, we need to generate diversified roleplayers for objective and subjective dimensions at first.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{static_role.pdf}
    \caption{Multiple different static roles for summarization task. For different generation tasks, different aspects need to be taken into consideration. A good summary usually requires fluent writing, proper wording and capturing key points of raw article. }
    \label{fig-static}
\end{figure}

%, while other tasks like translations should be accurately aligned with source text and elegant

\paragraph{Objective RolePlayers}
In general, overlap-based metrics such as BLUE/ROUGE only focus on lexicon-level consistency by the n-gram mechanism. Even though model-based metrics like BERTScore could further handle subtle syntax or semantic changes by embeddings similarity, there are two reasons making us believe that model-based metrics are hard to give a realistic and reliable evaluation to high-quality text generated by LLMs. Here we take BERTScore for illustration. First, the embedding space of LLMs is much different from that of BERT as they are pre-trained using different corpora and strategies. Second, the parameter scale of BERT is much smaller than those of LLMs so it can't in turn represent LLMs' rich semantics.

Consequently in this paper, we take some advanced quality dimensions like fluency, consistency, grammar and coherence into consideration, which were rarely adopted before as they are difficult to be measured accurately. Since different tasks usually require different objective dimensions, and these dimensions are relatively easy to be agreed on by most people, hence we manually curated static objective dimensions for each task and  make sure all these dimensions are fundamental and objective. As shown in Figure~\ref{fig-static}, multiple different roles are designed for summarization tasks. The static objective roles schema is presented as below:
% and machine translation tasks
\centerline{<\emph{Judger type}, \emph{Judger description}>}

\noindent where each \emph{Judger} works on one or multiple specific objective dimensions and \emph{Judger description} breaks down and exposit what specifically the Judger would focus on when evaluating.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{dynamic_role.pdf}
    \caption{Coarse-grained and fine-grained prompting mechanism for comprehensive user profiles generation.  }
    \label{fig-dynamic}
\end{figure*}


\paragraph{Subjective RolePlayers}
As is well known, there is no ideal golden answer for text generation like understanding tasks. Human-written material should only serve as an excellent reference. Different readers would evaluate the text from their own standpoints. Take the sports news about a famous sports player in a match for example,
\begin{itemize}
    \item For the writing style, ordinary readers expect that it's concise and easy to understand, while journalists would pay attention to its structure and choices of words.
    \item For the content, causal fans like comprehensive data of the sports player and horizontal comparison with others, while die-hard fans are more eager for in-depth analysis of the sports player through data.
\end{itemize}

Therefore, we propose to collect subjective evaluations of model-generated text from diverse user perspectives, including whether they think the text is interesting, useful, etc. These dimensions are more abstract and difficult to quantify than objective dimensions which few studies have touched on and addressed to our knowledge. Specifically, we take each generated text as the context and prompt the LLM to generate its potential readers dynamically following the below schema:

\centerline{<\emph{User type}, \emph{User description}>}
\noindent Here we design two user role generation prompts. As shown in Figure~\ref{fig-dynamic}, the former requires the LLM to consider the most common occupations with most people in the world which is coarse-grained and the latter aims to categorize people based on their familiarity with the text topics which is fine-grained.

We merge objective judgers and subjective users as multi-role players for cooperation.
Considering that there may exist duplicate or similar users, we propose to conduct diversity clustering to improve measurement performance and reduce inference costs. First, each roleplayer type and its description are concatenated as the input of Sentence-BERT~\cite{reimers2019sentence} to obtain the representation. Next, we use DBSCAN~\cite{ester1996density} algorithm to cluster roleplayers and those closest to each cluster center are kept. Finally, the left role players will be leveraged for text evaluation.



\subsection{RolePlayer-based Evaluation}~\label{sec:txt_eval}
To mitigate the discrepancy between the human evaluation and automatic metrics, we propose to leverage the roleplayers as crowdsourcing voters to compare the summaries from multi-dimensions. Besides the static roleplayers that scrutinize the objective dimensions including grammar, fluency, coherence, etc, the dynamic roleplayers are generated according to the current article and simulate the psychology and behavior of the article readers (roles) to convey their subjective feelings. It's expected that our method could achieve higher consistency with human evaluation than existing automatic metrics focusing on surface similarity.

\paragraph{Evaluation of RolePlayer}
Given the article $A$, we posit the reference summary generated by humans as $S$, and the candidate summary generated by models as $\hat{S}$, respectively. To evaluate, all the roleplayers perform pair-wise comparison as Figure~\ref{fig-batch}, since point-wise comparisons are inconsistent across different samples and list-wise comparisons are not stable. By our prompting, the LLMs play a specific role and output its analysis of two summaries, finally voting which summary is of better quality. In more detail, we parse and quantify the comparison result as $a$:
\begin{equation}
    a =
    \left\{ \begin{aligned}
        & 0    && {\text{If voting is "Summary 1"}} \\
        & 1    && {\text{If voting is "Summary 2"}} \\
    \end{aligned}
    \right.
\end{equation}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{case_study.pdf}
    \caption{Compare generated summary and reference summary by multiple roleplayers with batch prompting. }
    \label{fig-batch}
\end{figure*}

Assuming the DRPEScore for generated text $\hat{S}$ is $P(a|\bm{p}, A, S, \hat{S}), a \in \{0,1\}$, it could be obtained by modeling the joint probability of reason and voting result as below:
\begin{align}\label{eq:DRPEScore}
    P(a|\bm{p}, A, S, \hat{S}) = \mathds{1}(a=\hat{a})P(\hat{a}, \bm{r}|\bm{p}, A, S, \hat{S}, R)
\end{align}
where $\bm{r}$ is the comparison reason, $\bm{p}$ represents the prompt used here, $a$ is voting result from LLMs, $A$ is raw article and $R$ is the roleplayer. To compute $P(\hat{a}, \bm{a})$, similar to~\citet{wang2022self}, we leverage the confidence when roleplayer generates its voting normalized by the output length~\cite{brown2020language},
\begin{align}
    & P(\hat{a}, \bm{r}|\bm{p}, A, S, \hat{S}, R) \nonumber \\
    & = \exp^{\frac{1}{K}\sum_{k=1}^K\log P(t_k|\bm{p}, A, S, \hat{S}, R, t_1, \dots, t_{k-1})}
\end{align}
where $\log P(t_k|\bm{p}, A, S, \hat{S}, R, t_1, \dots, t_{k-1})$ is the log probability of $k$-th token $t_k$ in $\bm{r}$ and $K$ is number of tokens.



% Then the voting results are aggregated as the corresponding voting probability distribution from the roleplayers, $\bm{w} = \{w_k\}_{k=1}^K$.
% Specifically, the voting probabilities for a summary pair ($\bm{s}_j$, $\bm{s}_k$) by the roleplayer $\bm{r}_i$ can be obtained as below:
% \begin{align}
%     P(\bm{w}_{j,k}|\bm{p}, A, \bm{s}_j, \bm{s}_k) = \sum_{i=1}^{N}P(\bm{w}_{j,k}|\bm{p}, A, \bm{s}_j, \bm{s}_k, \bm{r}_i)
% \end{align}
% where $\bm{w}_{j,k} = (w_j, w_k)$, $\bm{p}$ represents the prompt used here, and $N$ is the number of roleplayers.

\paragraph{Batch Prompting}
To efficiently get comparison results for the summary pair ($S$, $\hat{S}$) of multiple roleplayers, we design a multi-roleplayer framework based on batch prompting to measure by both objective and subjective metrics in one-off inference. The different metrics are reflected by the objective and subjective roleplayers generated and clustered in Section~\ref{sec:role_gen}. 
As shown in Figure~\ref{fig-batch}, first, all the roleplayers are prompted to give votes for ($S$, $\hat{S}$) with respect to $A$ in their point of view, i.e., which summary could better describe the article, and tell us the reasons.
Then we aggregate the results to parse $\bm{a}=\{a_j\}_{j=1}^N$ where $N$ is the number of roleplayers. According to Equation~\ref{eq:DRPEScore}, the multi-roleplayer DRPEScore by batch prompting can be formulated as below:
\begin{align}
     & P(a|\bm{p}, A, S, \hat{S}) \nonumber \\
     & = \sum_{j=1}^N\mathds{1}(a=\hat{a}_j)P(\hat{a}_j, \bm{r}|\bm{p}, A, S, \hat{S}, R_j)
\end{align}
Compared with Self-Consistency CoT~\cite{wang2022self}, our framework decouples the answer (comparison result) and reasoning path (comparison reason), and brings higher inference efficiency as Self-Consistency CoT needs to generate the candidate answers separately.

% As our evaluation method is not summary position sensitive, each summary will be compared once with others. Consequently, the LLM conducts inference totally $C_2^K$ times. 
% After going through all the summary pairs, the voting score for the summary $\bm{s}_j$ can be extracted and normalized as below:
% \begin{gather}
%     \bar{P}(w_j) = \sum_{k=1}^{j-1}P(\bm{w}_{k, j}) + \sum_{k=j+1}^{K}P(\bm{w}_{j, k}) \\
%     P(w_j) = \frac{\exp{\bar{P}(w_j)}}{\sum_{k=1}^K\exp{\bar{P}(w_k)}}
% \end{gather}
% Similarly, we could get the voting probability distribution $\bm{w}$ from our roleplayers and collate with human evaluation.


\section{Experiments}


\subsection{Experiments Setting}
\paragraph{Datasets} 

 \textbullet \textit{CNN/DM}~\cite{nallapati2016abstractive, hermann2015teaching}: contains reference summaries that are approximately 3-4 sentences long. Summaries in this dataset are highly extractive and lead-biased. We use human study data on 100 recent articles from CNN, collected between March 1, 2022, and June 31, 2022. Each article is annotated by at least three judgers, and they are asked to choose the best and worst summaries from three candidates. To reduce noises, we only keep the best summary with at least 2 out of 3 votes and worst summary with at least 2 out of 3 votes. Finally, we obtain 81 best and worst summary pairs with a piece of corresponding news.
 
  \textbullet \textit{XSum}~\cite{narayan2018don}: contains 1 sentence summaries of BBC news articles. In this dataset, references summaries, and consequently generated summaries from fine-tuned models are highly abstractive. We use human study data on 100 recent articles from BBC, collected between March 1, 2022, and June 31, 2022. We also take a similar preprocessing strategy on this dataset.


\paragraph{Metrics} 


To measure the consistency between various metrics and human annotations, we follow the WMT18~\cite{ma2018results} standard practice and use absolute Pearson correlation $|\rho|$ to evaluate metric quality, and compute significance with the Williams test for $|\rho|$ and bootstrap re-sampling for $\tau$ as suggested by Graham $\&$ Baldwin~\cite{graham2014testing}.

%and calculate Kendall rank correlation $\tau$ 
\paragraph{Baselines} 
Automatic metrics proposed for summarization evaluation can be broadly divided into two categories: (1) overlap-based metrics, specifically ROUGE METEOR and BLEU, and (2) similarity-based metrics that compute the similarity between embeddings representations of generated and reference summaries. Specifically, we report BERTScore and MoverScore. We also design two LLM-based metrics and called them LLMScore and ManualDRPEScore, respectively. For LLMScore, we carefully design prompts for LLM, and directly use it to predict a better passage. 
For MaunalDRPEScore, we manually design several roles for it as follows:

 \textbullet \textit{Students}: These are studying in high school and want to know more about the world.
 
 \textbullet \textit{Business Man}: These are working in a big company and are interested in any business opportunity.
 
 \textbullet \textit{Politician}: These are pursuing an election victory in a medium country.
 
 \textbullet \textit{General public}: These readers may be interested in the story and looking for updates on the investigation.
 
 \textbullet \textit{Critic}: These people will check fluent writing, clear sentences, and good wording in summary writing.
 
 \textbullet \textit{News Author}: These readers will check the summary for consistency with the original article.
 

\paragraph{Implementation}
We use the public GPT-3~\cite{brown2020language} of the text-DaVinci-003 version with 175B parameters from OpenAI for the LLMs implementation and use greedy decoding for inference with the temperature set to 0. We select this LLM because it has the strongest reasoning performance among public LLMs. Especially, we use three roles \textit{General Public}, \textit{Critic}, and \textit{News Author} as objective roleplayers in our DRPEScore, and we ask the model to generate 5 dynamic roles for each case.



\subsection{Results}

\begin{table}[!ht]
  \centering
  \caption{Pearson correlation between several automatic metrics and human annotation. MDRPE denotes ManualDRPEScore and DRPE denotes DRPEScore.}
  \setlength{\tabcolsep}{3.0mm}{
  \resizebox{0.95\columnwidth}{!}{%
    \begin{tabular}{l|c|c|c|c}
    \toprule
    Metrics      & BertScore  & LLM  & MDRPE  & DRPE \\
\midrule
    CNN  & 0.55  & 0.58  & 0.75  & \textbf{0.90}  \\
    XSum & 0.57  & 0.62  & 0.73  & \textbf{0.87}  \\
    \bottomrule
    \end{tabular}}%
    }
  \label{tab:abla}%
\end{table}%

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{final_case.pdf}
    \caption{Evaluation procedure for two summaries given a piece of news. We use the green font to represent the content generated by the model. With suitable prompting, the LLM is able to generate relevant roles and generate judgment for these roles.  }
    \label{fig-case}
\end{figure*}


\textbf{Comparison with Existed Metrics}
Tables~\ref{tab:abla} shows Pearson correlation to human judgments. We observe that typical relevance-based metrics BERTSscore consistently perform bad, and the simplest LLM-based method has a close performance to BERTScore. Two types of DRPEScore, manual DRPEScore, and DRPEScore have a clear gap between the other two methods. Especially, DRPEScore consistently performs better than the manual way.



\begin{figure}[!ht]
\centering
\subfigure[Roles number k w.r.t correlation on CNN dataset.]{
\begin{minipage}[t]{0.47\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{cnn_role_num_v2.pdf}
%\caption{fig1}
\end{minipage}\label{fig:case-study-a}
}%
\subfigure[Roles number k w.r.t correlation on XSum dataset.]{
\begin{minipage}[t]{0.47\linewidth}
\centering
\includegraphics[width=1.0\linewidth]{xsum_role_num_v2.pdf}
%\caption{fig2}
\end{minipage}\label{fig:case-study-b}
}%
%
%
\centering
\caption{Effect of role number on model performance. } \label{fig:retrieval_number}
\end{figure}




\textbf{Effects of Hyperparameters}


We test DRPEScore and ManualDRPEScore with different subjective role numbers $k$ in [0, 2, 4, 6] on two datasets, Figure~\ref{fig:retrieval_number} shows the experimental results. When $k$ increases from 0 to 6, the experimental indicators first increase and then slightly decrease on the CNN/DM dataset,  when $k$=4, the correlation to human evaluation achieves a peak. On the Xsum dataset, experimental results are similar, and more roles~(6) don't bring improvements compared to fewer roles~(4). With the increase of role numbers k, We observe some irrelevant roles are generated and they are not so helpful to judgment. Especially, when $k$ is 0, all subjective roles are disabled, and only 3 objective roles are kept. 




\textbf{Qualitative Analysis}
Previously, we have shown the effectiveness of our model on two summarizations tasks. In this part, we qualitatively analyze why DRPEScore is able to yield good performance.



As shown in Figure~\ref{fig-case}, given the latest news, it talks about the cases between Amber Heard and Johnny Depp, and our model needs to select a better one from two candidate summaries. Firstly, we generated several subjective roles like \emph{Amber Heard Fans User}, \emph{Johnny Depp Fans User}, \emph{Celebrity Gossip User}, \emph{Legal System User} based on news content. These users are very representative because of the strong inference ability of LLMs. Secondly, each user and judger are played by LLMs to compare the two summaries. We leverage a batch prompting mechanism to speed up the inference procedure. For each case, there are only two LLMs calls. Especially, LLMs believe that the \emph{Johnny Depp Fans User}, who might be negative to \emph{Amber Heard Fans User}, will prefer to choose summary 1. 



\section{Conclusion}
We propose DRPEScore, a new comparison-based metric for evaluating generated text against gold standard references. Our DRPEScore is designed to be simple and task-agnostic. Our experiments illustrate that DRPEScore could provide a human-like ability to conduct a comprehensive evaluation, especially
on challenging long text generation like summarization tasks.  In future work, we look forward to exploring the capabilities of LLMs as judgers on more text evaluation tasks and reducing the computation cost.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

% \appendix

% \label{sec:appendix}

% This is a section in the appendix.

\end{document}
