{
    "arxiv_id": "2303.15078",
    "paper_title": "Large Language Models are Diverse Role-Players for Summarization Evaluation",
    "authors": [
        "Ning Wu",
        "Ming Gong",
        "Linjun Shou",
        "Shining Liang",
        "Daxin Jiang"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CL"
    ],
    "abstract": "Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. For example, the quality of a document summary can be measured by human annotators from both objective aspects, such as grammatical and semantic correctness, as well as subjective dimensions, such as comprehensiveness, succinctness, and interestingness. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to capture the above dimensions well. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a context-based prompting mechanism that is able to generate dynamic roleplayer profiles based on input context. Finally, we design a multi-roleplayer prompting technology based on batch prompting to integrate multiple evaluation results into evaluation results. Experimental results on two real datasets for summarization show that our model is highly competitive and has a very high consistency with human annotators.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15078v1",
        "http://arxiv.org/pdf/2303.15078v2"
    ],
    "publication_venue": null
}