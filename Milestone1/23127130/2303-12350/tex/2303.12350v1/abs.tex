\documentclass[12pt]{article}

% DEFAULT PACKAGE SETUP

\usepackage{setspace,graphicx,epstopdf,amsmath,amsfonts,amssymb,amsthm}
\usepackage{marginnote,datetime,enumitem,subfigure,rotating,fancyvrb, booktabs}
\usepackage{float}
\usdate
\usepackage{lscape}
\usepackage{adjustbox}
%\usepackage[ruled]{algorithm2e}

% we have used too many packages for algorithm ?
 
\usepackage[ruled]{algorithm2e}  
\usepackage{pdflscape}
\usepackage{tikz}
\usepackage{mathpazo}
\usepackage{bm}
\usepackage{caption}
\usepackage{soul} 
\usepackage{color, xcolor}
% \usepackage{algorithm}  
\usepackage{amsmath}
\usepackage{algorithmicx}  
\usepackage{algpseudocode} 
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{longtable}
%\usepackage{versionPO}
%\usepackage[ left=1in,right=1in,top=1.5in,bottom=1.5in]{geometry}
%\usepackage[ left=1.25in,right=1.25in,top=1.25in,bottom=1.25in]{geometry}
\usepackage[ left=1in,right=1in,top=1.25in,bottom=1.25in]{geometry}
% These next lines allow including or excludingt versions of text
% using versionPO.sty
%\usepackage[notes,natbib,isbn=false,backend=biber]{biblatex-chicago}  
\usepackage[longnamesfirst]{natbib}
\usepackage{threeparttable}
\usepackage{authblk}
\algnewcommand\algorithmicparam{\textbf{Parameter:}}
\algnewcommand\Param{\item[\algorithmicparam]}


\newcommand{\E}{\mathbb{E}}
\newcommand{\se}{\sigma_\epsilon}
\newcommand{\sth}{\sigma_\theta}
\newcommand{\sx}{\sigma_x}
\newcommand{\lmd}{\lambda}
\newcommand{\ra}{\rightarrow}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\bt}{\beta}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newtheorem{corollary}{Corollary}[section]
\newtheorem{conjecture}{Conjecture}[section]
%\newtheorem{con-proposition}{``Proposition''}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
%\newtheorem{theorem_sec}{Theorem}[section]

% \newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}[section]

\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{question}{Question}%[section]

\newtheorem{assumption}{Assumption}[section]
%\newenvironment{proof}{{\noindent\it Proof}\quad}{\hfill $\square$\par}
\newtheorem*{theorem*}{Theorem}

\newtheorem*{prob}{Problem}
\numberwithin{equation}{section}


%\newtheorem{remark}{Remark}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{define}{Definition}[section]

\newtheorem{hyp}{Hypothesis}


\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=blue,
}

\usepackage{cleveref}



\begin{document}

\setlist{noitemsep}    
	
\title{Artificial Intelligence and Dual Contract}

\author{Wuming Fu}	
\author{Qian QI\footnote{Correspondence. No.5 Yiheyuan Road Haidian District, Beijing, P.R.China 100871. Email: {\href{mailto:qiqian@pku.edu.cn}{qiqian@pku.edu.cn}}.}}

\affil[1]{Peking University}


\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\singlespacing
	
\maketitle
	
\vspace{-.2in}

\begin{abstract}
    
With the dramatic progress of artificial intelligence algorithms in recent times, it is hoped that algorithms will soon supplant human decision-makers in various fields, such as contract design. We analyze the possible consequences by experimentally studying the behavior of algorithms powered by Artificial Intelligence (Multi-agent Q-learning) in a workhorse \emph{dual contract} model for dual-principal-agent problems. We find that the AI algorithms autonomously learn to design incentive-compatible contracts without external guidance or communication among themselves. We emphasize that the principal, powered by distinct AI algorithms, can play mixed-sum behavior such as collusion and competition. We find that the more intelligent principals tend to become cooperative, and the less intelligent principals are endogenizing myopia and tend to become competitive. Under the optimal contract, the lower contract incentive to the agent is sustained by collusive strategies between the principals. This finding is robust to principal heterogeneity, changes in the number of players involved in the contract, and various forms of uncertainty.
\end{abstract}

%This paper introduces the dual-contract design via Q-learning methods. In contrast to the standard principal-agent problem (a principal and an agent), we emphasize that the dual-contract problem can also be recognized as a dual-principal-agent problem (two principals and an agent). The method utilizes a combination of game theory and reinforcement learning (RL) to create a contract that is both fair and beneficial to multiple sides. In this problem, two principals (e.g., two departments of a headquarters) are jointly responsible for providing the resources and capital for an agent. The two principals may have different objectives and interests, which must be lanced to ensure that the project is completed efficiently and effectively. In this problem, both principals must design a contract that provides the right incentives for the agent to perform the task optimally, however, the conventional mathematical method is hard to illustrate the economic consequence of this dual-contracting problem. The main advantage of using multi-agent Q-learning for dual-contract design is that it allows for solving optimization problems that are both fair and beneficial to both sides. This is because the multi-agent Q-learning algorithm can take into account both sidesâ€™ preferences and optimize the contract parameters accordingly. Additionally, using a Q-learning algorithm allows the contract to be adjusted over time as conditions change, ensuring that the contract remains fair and beneficial to both sides in the long term. 

\medskip
	
\noindent \textbf{JEL classification}: D21, D43, D83, L12, L13
	
\medskip
\noindent \textbf{Keywords}: Artificial intelligence, dual contract, principal-agent problem, AI alignment.
	
\thispagestyle{empty}
	
\clearpage
	
\doublespacing
\setcounter{footnote}{0}
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{page}{1}
	
	
\newpage

\input{introduction}
\input{Qlearning}
\input{contracts}
\input{dualcontract}
\input{conclusion}
	
	

\clearpage
\bibliographystyle{jf}
\bibliography{ref}
\clearpage

\appendix
\input{appendix}

	
	
%\input{appendix}

\end{document}
