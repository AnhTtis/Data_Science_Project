\subsection{Baseline Parametrization and Initialization}
Initially, we focus on a baseline economic environment that consists of a single principal-agent contract problem with $T=2I$ and $c=2I$. For this specification, the profit $(T-I)e$ is equal to the agent's cost $\frac{1}{2}ce^2$ when agent's effort is equal to $1$. This means that when agent exerts effort $e=1$, the net profit will be equal to $0$.

As for the initial matrix $Q_0$, our baseline choice is to set the Q-values at $t=0$ at a random number between 0 and 1. This is done to ensure that the agent has no prior knowledge of the environment.

We also set the learning rate $\alpha$ to 0.1 and the discount factor $\gamma$ to 0.9. These values were chosen to ensure that the agent learns quickly and is able to take advantage of long-term rewards.

Finally, we set the exploration rate $\epsilon$ to 0.1. This value was chosen to ensure that the agent has enough exploration to learn about the environment, but not too much so that it does not get stuck in a suboptimal policy.

\subsection{Results}
We ran the simulation for 1000 episodes, with each episode consisting of 10 time steps. We tracked the agent's performance over the course of the simulation by measuring the average reward per episode.

The results of the simulation are shown in Figure \ref{fig:results}. As can be seen, the agent was able to learn an optimal policy and was able to maximize its reward over the course of the simulation.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{results.png}
    \caption{Average Reward per Episode over 1000 Episodes}
    \label{fig:results}
\end{figure}

Review #557A
Paper summary
In this paper the authors apply a standard q-learning algorithm to a simple optimal contracting problem. They try a few different variations of the problem, including allowing for two principals. This changes the problem setting to a multi-agent problem which makes the application of q-learning a bit more challenging. The authors report results from their experiments.

Strengths and Weaknesses
Strengths: The paper is generally well written and clearly laid out. The authors make an attempt to study a somewhat novel problem which they refer to as the "dual contracting problem." The authors contribute to the literature on using AI/ML for design of economic systems.

Weaknesses: The paper makes only a marginal contribution. The application of Q-learning to this quite restricted contracting problem is unlikely to be of general interest to a community like EC. Much of the paper is also effectively a discussion and review of standard techniques (including an in depth discussion of Q-learning and the Innes 1990 result) leaving a fairly small portion of the paper for novel results. In a community like EC, it is very likely that an interested reader would have the background to find these sections entirely redundant.

Comments for authors
In the setup of the dual contract problem, you state that you can go from randomized outcomes to expected values without loss of generality. While I agree that the math looks the same, I think it actually fundamentally changes the interpretation of the problem in such a way that I would feel uncomfortable with this reframing. The entire point of a moral hazard problem is that you cannot observe some action by the agent, and you must contract only indirectly on it. In the case you describe, we can just observe the payoffs and contract on those directly, implementing the first best.

Review #557B
Paper summary
Demonstrates solutions of principal-agent problems using Q-learning methods.

The examples given in the paper, where payoffs have simple analytic functional forms, are solvable by straightforward analytic methods and don't require AI. The idea is that this is a "proof of concept" for situations where the payoff function is not known and the optimal contract can be learned by experimentation. But this learning requires an environment which is unchanging for many periods. In Figure 1, it appears that the algorithm took over 10,000 periods to converge to the solution. In real life, you would need to learn a good solution after at most a few iterations of the problem.

Strengths and Weaknesses
Strength: Describes the model and methods clearly.

Weaknesses:

I don't see the utility of the AI method (Q-learning.) Here is the problem: (1) In the toy examples given, the solution can be found analytically, as in the Lagrangian method from 3.2. Of course, these examples were just a test case, demonstrating that Q-learning can replicate the analytic results. The idea is to apply these methods to a case where you don't know the payoffs until you experiment. But... (2) It's not possible to experiment for tens of thousands of periods in this kind of problem. In fact, it would be unusual to experiment with more than a few different contracts with a given agent, and often it's a one-shot interaction.

The writing has some grammatical issues and redundancies but generally clear.

Comments for authors
In Section 3, it would be easier to follow if "principal" and "agent" were used instead of "financier" and "entrepeneur," to match the rest of the paper.

Review #557C
Paper summary
This paper explores the use of reinforcement (Q-learning) algorithms for dynamic/repeated contracting problems. The learning algorithms are applied to a single-principal contracting model of Innes [1990], and an extension of that problem to two principals.

Both reviewers found that the paper lacked technical novelty and depth to be considered for EC. They found that the results offer little to no insight over results that were known and can be obtained analytically. They were also not fully convinced by the applicability (practicality) of the results.

We therefore concluded that the paper does not pass the bar for EC, and recommend the paper for (Round 1) rejection.

Strengths and Weaknesses
The paper is well written, and it's clear bout its approach and contribution
The paper lacks technical novelty/conceptual contribution to make the cut for EC
Comments for authors
The paper should discuss the long and rich literature on "automated mechanism design" that explores the use of computational and ML tools for the design of mechanisms

The authors should also discuss a growing body on "algorithmic contract theory", including work on multi-principal settings