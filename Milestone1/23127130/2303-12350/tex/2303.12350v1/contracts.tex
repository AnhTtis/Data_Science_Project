\section{Experiment Design}\label{contracts}

\indent We begin with a heuristic naive case by constructing Q-learning algorithms and allowing them to interact in a dynamic contract setting, hoping that the naive case will illustrate that AI algorithms can learn incentive-compatible contracts automatically without any external guidance. To this end, we take \cite{innes1990limited} as the reference model for the naive case to examine the effectiveness and convergence of the AI outcomes. The selection of the reference model remains for the following important reasons:

\begin{itemize}
    \item The model is tractable enough to obtain an analytical solution, providing a clear benchmark for evaluating the equilibrium outcomes of AI algorithms in the naive case.\footnote{Remarkably, the Q-learning algorithms can naturally extend the static model to a dynamic setting, thereby analyzing the dynamics of the extended model. Fortunately, this change in model setup does not affect the final equilibrium outcome in the naive case, indicating that the equilibrium outcome in the static model can also be used as a reference under dynamic settings. Therefore, the main contribution of these Q-learning algorithms here is to reveal different convergence paths under different initial conditions, which would be impossible to observe in a static model. The experiment results in the following sentences show that the AI algorithms are robust enough to learn the equilibrium outcomes under various initial conditions.}

    \item The setup of \cite{innes1990limited} is logically straight forward and easy to implement in the dynamic setting of the experiment environment for the AI algorithms. 

    \item The Model is simple and can be fully characterized by just a few parameters, the economic interpretation of which is clear.

    \item The setup is suitable for progressing to the dynamic dual-contract scenario, making the dual-principal-agent problem remain well defined and interpretable yet difficult to be analyzed using conventional methods.
    
\end{itemize}

\indent Note that learning a dynamic setting of such a static moral hazard model is not our goal in the current paper. The importance of selecting a parsimonious reference model is to prove the AI algorithms' learnability of incentive-compatible contracts in a relatively tractable way, although we can choose more complicated models and build up more intricate experiment environments for the AI algorithm. Here, we describe the reference model and the economic environment in which the algorithms operate, the exploration strategy they follow, and other aspects of the numerical simulations.





\subsection{Reference Model and Economic Environment}

Specifically, a risk-neutral principal provides investment funds to a risk-neutral agent, who then makes an unobservable ex-ante effort choice. Its key idea is that legal liability limits bind the investment contract.\footnote{
Recall the key idea of \cite{innes1990limited}, in the presence of limited liability when the downside of an investment is limited both for the agent and the principal, the closest one can get to a situation where the agent is a ``residual claimant'' is a (risky) debt contract. In other words, a debt contract provides the best incentives for effort provision by extracting as much as possible from the agent under low performance and giving her the total marginal return from effort provision in high-performance states where revenues are above the face value of the debt.} 

\begin{itemize}
\item Project requires initial investment $I$, which comes from principal.
\item agent exerts unobservable effort $e$ at cost $\frac{1}{2}ce^2$\
\item With probability $e$, project generates payoff $X^H$.
\item With probability $1-e$, generate payoff $X^L<X^H$.
\item Contract pays principal $D^L$ if payoff is $X^L$ and $D^H$ if payoff is $X^H$.
\item Agent retains the residual.
\end{itemize}
For a given contract $(D^L,D^H)$, the agent maximizes
\begin{equation}
\label{eqn:05}
e(X^H-D^H)+(1-e)(X^L-D^L)-\frac{1}{2}ce^2,
\end{equation}
The first-order condition for $e$ gives the incentive-compatible (IC) constraint:
\begin{equation}
\label{eqn:06}
(X^H-D^H)+(X^L-D^L)=ce,
\end{equation}
The individual rationality (IR) constraint is that the principal must also break even, so we need
\begin{equation}
\label{eqn:07}
eD^H+(1-e)D^L=I,
\end{equation}
 Lagrangian for optimal contract
 \begin{align}
\label{eqn:08}
\mathcal{L}& = e(X^H-D^H)+(1-e)(X^L-D^L)-\frac{1}{2}ce^2\nonumber\\&+\lambda_1(e-\frac{(X^H-D^H)-(X^L-D^L)}{c}+\lambda_2(1-eD^H-(1-e)D^L),
\end{align}
Derivative wrt $D^L$
\begin{equation}
\label{eqn:09}
\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^L}=-(1-e)-\frac{\lambda_1}{c}-\lambda_2(1-e),
\end{equation}
Derivative wrt $D^H$
\begin{equation}
\label{eqn:10}
\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^H}=-e+\frac{\lambda_1}{c}-e\lambda_2=-\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^L}-(1+\lambda_2),
\end{equation}
\paragraph{Claim}
Optimal to set $D^L=X^L$.
\paragraph{Proof by contradiction}
Suppose optimal $D^L<X^L$. Then it must be the case that $\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^L}=0$.
\begin{itemize}
\item If it were not, we would increase $D^L$.
\item But then we will have $\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^H}<0$, so we will want to set $D^H=0$.
\item But then we will induce negative effort.
\item Instead, set $D^L=X^L$ and $X^H>D^H>I$.
\end{itemize}

\subsection{Economic Environment}\label{single}
The reference model looks like a debt contract:
\begin{itemize}
\item At a low project payoff, the principal gets everything.
\item At high project payoff, the agent gets residual.
\item At high project payoff, the principal gets more than initially
provided, which is similar to interest payment.
\end{itemize}
The key intuition is to reward the agent for a high project payoff that induces effort.
\begin{itemize}
\item Leave the agent with the max possible cash in that state.
\item Compensate by giving the principal as much as possible in the low payoff state
\end{itemize}

\indent We adopt the setup of the reference model to construct our economic environment. Here, we drop the IR constraint (\Cref{eqn:07}), as we would like to demonstrate that AI algorithms can autonomously learn to be rational individuals.

\paragraph{Setup} Single principal-agent contract Problem
\begin{itemize}
\item Project requires initial investment $I$, which comes from the principal.
\item Principal chooses a ``tax rate'' $p{\in}[0,1]$.
\item Agent observes principal's ``tax rate'' then exerts effort $e{\in}[0,1]$ at cost $\frac{1}{2}ce^2$\
\item The project generates payoff $I+(T-I)e$, where $T$ is the highest possible payoff.
\item Contract pays principal $I+(T-I)ep$.
\item Agent retains the residual.
\end{itemize}

\subsection{Parametrization and Initialization}
Initially, we focus on a baseline economic environment that consists of a principal-agent problem with $T=2I$ and $c=2I$. For this specification, the profit $(T-I)e$ is equal to the agent's cost $\frac{1}{2}ce^2$ when the agent's effort is equal to $1$. This means that when the agent exerts effort $e=1$, the net profit will be equal to $0$.

As for the initial matrix $Q_0$, our baseline choice is to set the Q-values at $t=0$ to a random number between $0$ and $1$. The learning parameter $\alpha$ may be in the principal range from $0$ to $1$. It is well known, however, that high values of $\alpha$ may disrupt learning when experimentation is extensive as the algorithm would forget too rapidly what it has learned in the past. Learning must be persistent to be effective, requiring that $\alpha$ be relatively small. In machine learning literature, a value of $0.1$ is often used; accordingly, we choose $\alpha = 0.1$ in our baseline model.

As for the experimentation parameter $\epsilon$, the trade-off is as follows. First, the algorithms need to explore extensively, as the only way to learn is through multiple visits to every state-action cell (of which there are $101$ in our baseline experiment with $|S|=1,|A|=101$, and much more in more complex environments). Additionally, exploration is costly. One can abstract from the short-run cost by considering long-run outcomes. But exploration also entails another cost if there is more than one algorithm learning, in that if one algorithm experiments more extensively, this creates noise in the environment, making it harder for the other to learn. This externality means that, in principle, experimentation may be excessive, even discounting the short-term cost.

Our baseline model adopts the $\epsilon$-greedy model with a time-invariant exploration rate. Although a more complex, time-declining exploration rate can be designed, a fixed exploration rate is enough for our purpose.

\subsection{Results}
In this part of the simulation, we simulate a Q-learning algorithm (the principal) contracting with the agent. The Q-learning algorithm needs to choose a ``tax rate'' $p$. The agent will observe this $p$ and act in their best interest. The grid of allowable choices of $p$ includes 101 $p$ levels from $0$ to $1$. The outcome of this experiment is reported in \Cref{fig:one}. Expressly, we set $\epsilon=0.2$.

% Figure
% \begin{figure}
% \centerline{\includegraphics{singlePA.jpg}}
% \caption{The picture shows the Q-learning algorithm gradually finding the optimal p.}
% \label{fig:one}
% \end{figure}

\begin{figure}[htbp]
    \centering
    \subfigure[]{
        \includegraphics[width=2.5in]{singlePA.png}
    }
    \subfigure[]{
	\includegraphics[width=2.5in]{singlePA2.png}
    }
    \quad 
    \subfigure[]{
    \includegraphics[width=2.5in]{singlePA3.png}
    }
    \subfigure[]{
	\includegraphics[width=2.5in]{singlePA4.png}
    }
    \caption{These pictures show the Q-learning algorithm gradually finding the optimal tax rate $p$.}
    \label{fig:one}
\end{figure}

\paragraph{Result} Our algorithm converges to the optimal choice of $p$. One thing to note in \Cref{fig:one} is that there exists a static optimal $p$. After finding this optimal $p$, there is no need for the algorithm to explore further. This is intuitive, as the agent always acts in their best interest and has a fixed strategy for every $p$ the principal chooses. However, this may not be true if there are more than one algorithm learning and they are playing against each other, as seen in the following dual contract problem.