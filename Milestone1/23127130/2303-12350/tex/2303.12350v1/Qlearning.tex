\section{Q-learning}\label{Qlearning}

\indent We concentrate on Q-learning algorithms (see \cite{watkins1992q} and its application for economics \cite{calvano2020artificial}). Q-learning is a model-free reinforcement learning algorithm, which is also adopted for training artificial intelligence. It is an off-policy algorithm using a Q-value function to find the optimal action-selection policy. The Q-value function is a table that stores the expected utility of taking a given action in a given state. The algorithm updates the Q-value function based on the reward for taking action in a given state. The algorithm then uses the updated Q-value function to select the optimal action in the next state. The goal of the algorithm is to find the optimal policy that maximizes the expected reward. 


\indent This algorithm is based on reinforcement learning, a type of machine learning that focuses on taking suitable actions to maximize rewards. In this case, the rewards are associated with the choices made by the agent. The agent can then learn from its experiences and adjust its strategy accordingly. Q-learning assigns a value to each agent's action and updates these values based on the rewards received. This allows the agent to learn from experiences and adjust its strategy accordingly. Using Q-learning, the agent can learn to make better decisions and maximize rewards. There are several reasons why we chose Q-learning: first, it is an algorithm commonly adopted in practice; second, it provides a logical simulation of decision making and learning processes; third, its parameters have clear economic interpretations which allow for comprehensive comparative statics analysis while minimizing modeling choices; fourth, it has similar structure to advanced programs that have recently achieved remarkable successes such as ChatGPT (\cite{ouyang2022training}). In this section we will briefly introduce Q-learning.

% Head 2
\subsection{Single Agent Problems}

\indent Like all reinforcement-learning algorithms, Q-learning adjusts its behavior based on experience, taking actions that have been successful more often and those that have been unsuccessful less often. By using reinforcement learning, they can discover an optimal policy or policy close to optimal without prior knowledge of the specific problem.

\indent Q-learning is a reinforcement learning algorithm initially used to solve Markov Decision Processes (MDPs) with finite states and actions. In this model, the agent interacts with the environment by acting in a given state and receiving a reward. The goal of Q-learning is to learn the optimal policy, which is the sequence of actions that maximizes the expected cumulative reward. Q-learning does not require state-dependent actions, meaning that the same action can be taken in different states. In a stationary Markov decision process, in each period $t=0,1,2,...$ an agent observes a state variable $s_t{\in}S$ and then chooses an action $a_t{\in}A(s_t)$. For any $s_t$ and $a_t$, the agent obtains a reward $\pi_t$, and the system moves on to the next state $s_{t+1}$, according to a time-invariant (and possibly degenerate) probability distribution $F(\pi_t,s_{t+1}|s_t,a_t)$. Q-learning deals with the version of this model where $S$ and $A$ are finite, and $A$ is not state-dependent.

The decision-maker's problem is to maximize the expected present value of the reward stream:
% Numbered Equation
\begin{equation}
\label{eqn:01}
E[\sum_{t=0}^{\infty}{\delta^t}\pi_t],
\end{equation}
where $\delta\leq1$ represents the discount factor. This dynamic programming problem is usually tackled using Bellman's value function
% Numbered Equation
\begin{equation}
\label{eqn:02}
V(s)=\max\{E[\pi|s,a]+{\delta}E[V(s')|s,a]]\},
\end{equation}
where s' is a shorthand for $s_{t+1}$. Using Bellman's value function, we can instead consider the Q-function, which represents the discounted payoff of taking action $a$ in state $s$, for our purposes. It is implicitly defined as
\begin{equation}
\label{eqn:03}
Q(s,a)=E(\pi|s,a)+{\delta}E[\max_{a'{\in}A}Q(s',a')|s,a],
\end{equation}
where the first term on the right-hand side is the period payoff, and the second term is the continuation value. The Q-function is related to the value function by the simple identity $V(s){\equiv}\max _{a{\in}A}Q(s, a)$. Since $S$ and $A$ are finite, the Q-function can be represented as an $|S|{\times}|A|$ matrix.\\

% Head 3
\subsubsection*{learning}

Q-learning is used to determine the optimal action for a given state without knowing the underlying model. It does this by using trial and error to estimate the Q-matrix, which is a matrix that contains the expected rewards for each action taken in a given state. Once the Q-matrix is estimated, the agent can calculate the optimal action for any given state. Q-learning is essentially a method for estimating the Q-matrix without knowing the underlying model, i.e., the distribution function $F(\pi,s'|s, a)$.

Q-learning algorithms use an iterative process to approximate the Q-matrix. The algorithm starts from an arbitrary initial matrix $Q_0$, after choosing action $a_t$ in state $s_t$, the algorithm observes $\pi_t$ and $s_{t+1}$ and updates the corresponding cell of the matrix $Q_t(s,a)$ for $s=s_t, a=a_t$, according to the learning equation:
\begin{equation}
\label{eqn:04}
Q_{t+1}(s,a)=(1-\alpha)Q_t(s,a)+\alpha[\pi_t+{\delta}\max_{a{\in}A}Q_t(s',a)],
\end{equation}
according to equation (\ref{eqn:04}), for the cell visited, the new value $Q_{t+1}(s, a)$ is a convex combination of the previous value and the current reward plus the discounted value of the state that is reached next. For all other cells $s{\neq}s_t$ and $a{\neq}a_t$, the Q-value does not change: $Q_{t+1}(s,a)=Q_t(s,a)$. The learning rate is the weight $\alpha\in[0,1]$. Its purpose is to regulate the learning process and determine how much experience is taken into account when estimating current action values. \cite{watkins1992q} demonstrated that Q-learning could converge to the optimal policy in a Markov Decision Problem (MDP) for a single agent. However, there is no assurance that this will work for generalized multi-agent Q-learning. Difficulties arise from the lack of stationarity: each agent is faced with an unpredictable, ever-evolving environment, and the reward distribution is dependent on the actions of their opponents. When the Markov property is not satisfied, various experiments in the literature have found that independent Q-learning can still be effective in such settings. In addition, opponent-aware algorithms necessitate more data about each opponent's design and behavior, while the independent design approach maintains the reinforcement learning paradigm's model-free simplicity.

% Head 4
\subsubsection*{Experimentation}
All possible actions must be tested in all states to approximate the true matrix starting from an arbitrary $Q_0$. This requires exploring all possible combinations of states and actions to determine the best action for each state. The process of experimentation allows the algorithm to explore new possibilities and to learn from its mistakes, thus improving its performance over time. There is a cost associated with further exploration, necessitating a balance between utilizing the knowledge already gained and continuing to learn. Finding the optimal resolution to this trade-off can be difficult, but Q-learning algorithms do not attempt to optimize it. Instead, the mode and intensity of exploration are set externally. The $\epsilon$-greedy exploration policy is a simple approach for exploration in reinforcement learning. It involves selecting the optimal action with a fixed probability of $1-\epsilon$ and randomizing uniformly across all available actions with probability $\epsilon$. This approach allows for exploring different actions while exploiting the most rewarding activities. In this way, $1-\epsilon$ is the fraction of times the algorithm is in exploitation mode, while $\epsilon$ is the fraction of times it is in exploration mode. Even though more sophisticated exploration policies can be designed, our analysis will mainly focus on the $\epsilon$-greedy specification due to its simplicity.