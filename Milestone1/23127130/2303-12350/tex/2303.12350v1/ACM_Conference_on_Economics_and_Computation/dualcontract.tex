\section{Dual Contract Problem}\label{dualcontract}
A dual contract is an agreement between parties who have made two contracts for the same transaction. It can be an arrangement where an agent has two contracts with different principals. This is a situation in which an individual works for two employers simultaneously. This arrangement can be beneficial for both the agent and the principal, as it allows the agent to gain experience in different fields and the employers to benefit from the agent's skills and knowledge. However, it is important to ensure that the two principals are aware of the arrangement and that the agent is not overworked or taken advantage of. The dual contract is used to ensure that both parties are legally bound to the same terms and conditions. It is also used to protect the interests of both parties in the event of a dispute.

In this section, we use Q-learning to solve dual contract problems based on the two-principal Innes Model. We first define the action space and reward function. The principal's action space is defined as a discretized choice of ``tax rate'' $p$. The reward function is defined according to the rate of ``identity of interests'' $\beta$.

Next, we define the Q-learning algorithm. The algorithm starts by initializing the Q-table with random numbers. Then, for each state-action pair, the algorithm updates the Q-value using the Bellman equation. The algorithm then selects the action with the highest Q-value and performs the action. After performing the action, the algorithm updates the Q-table with the new reward. The algorithm then repeats the process until the goal is reached.

Finally, we test the Q-learning algorithm on the dual contract problem. We set the initial investment $I$ to $1$ and the initial highest possible payoff $T$ to $2$. We then run the Q-learning algorithm for $10000000$ iterations. The results show that the Q-learning algorithm is able to successfully solve the dual contract problem and reach the optimal solution.

\subsection{Multi-agent Q-learning}
Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It is a type of reinforcement learning that involves multiple agents interacting with each other in an environment. The agents learn from each other and the environment to maximize their reward. This type of learning can be used to solve complex problems that require cooperation between multiple agents.  It focuses on studying the behavior of multiple learning agents that coexist in a shared environment. Each agent is driven by its incentives, taking actions that benefit itself; in certain situations, these incentives may conflict with the interests of other agents, leading to intricate group behavior. MARL algorithms can be used to learn how to coordinate multiple agents to achieve a common goal, such as playing a game or navigating a complex environment.

In the dual contract problem, Multi-agent reinforcement learning is used to solve the problem. In this approach, each principal is a Q-learning algorithm trained to maximize its reward while considering the other principal's behavior. When multiple agents act in a shared environment, their interests might be aligned or misaligned. MARL is a powerful tool for studying how different alignments of interests between multiple agents in a shared environment can affect their behavior. MARL allows us to explore the various alignments of interests between agents, such as cooperative, competitive, or mixed, and how these alignments can influence the agents' behavior. By studying the effects of different alignments of interests, MARL can help us better understand how agents interact with each other in a shared environment. Specifically, we explore three settings:

\begin{itemize}
\item Pure competition region ($\beta=0)$: The principals' rewards are opposite to each other, and therefore they are playing against each other.
\item Pure collusion region ($\beta=0.5)$: The principals get the same rewards and therefore play with each other.
\item Mixed-sum region ($0<\beta<0.5)$: It covers all the games that combine elements of both collusion and competition.
\end{itemize}

\subsection{Setup}
Dual contract Problem
\begin{itemize}
\item Project $1$ requires initial investment $I_{1}$, which comes from the principal $1$.
\item Project $2$ requires initial investment $I_{2}$, which comes from the principal $2$.
\item Principal $1$ chooses a ``tax rate'' $p_{1}{\in}[0,1]$.
\item Principal $2$ chooses a ``tax rate'' $p_{2}{\in}[0,1]$ without observing $p_{1}$.
\item Agent observes $p_{1}$ and $p_{2}$ then exerts effort $e_{1}{\in}[0,1]$ in project $1$ and effort $e_{2}{\in}[0,1]$ in project $2$ at cost $\frac{1}{2}c(e_{1}+e_{2})^2$, where ($e_{1}+e_{2}\leq1$).
\item Project $1$ generates payoff $I_{1}+(T_{1}-I_{1})e_{1}$, where $T_{1}$ is the highest possible payoff.
\item Contract pays principal $1$: $I_{1}+(T_{1}-I_{1})e_{1}p_{1}$.
\item Project $2$ generates payoff $I_{2}+(T_{2}-I_{2})e_{2}$, where $T_{2}$ is the highest possible payoff.
\item Contract pays principal $2$: $I_{2}+(T_{2}-I_{2})e_{2}p_{2}$.
\item Agent retains the residual.
\end{itemize}

\subsection{Agent's strategy}
The agent observes the ``tax rate'' $p_{1}$ and $p_{2}$ two principals give and chooses the optimal effort in each project. Because the agent can select his action after observing $p_{1}$ and $p_{2}$, his strategy is unaffected by the learning process of the two principals. The agent's strategy is determined by the preferences of the two principals, and the agent's goal is to maximize his utility. Therefore, the agent's strategy will be based on the preferences of the two principals, and the agent will choose the action that will maximize his utility. The agent's strategy is not affected by the learning process of the two principals, as the agent's strategy is determined by the preferences of the two principals, not by the learning process.

% \begin{figure}[htbp]
% \centering
% \begin{minipage}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=6cm]{e1.png}
% \caption{Agent's effort in project 1}
% \end{minipage}
% \begin{minipage}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=6cm]{e2.png}
% \caption{Agent's effort in project 2}
% \end{minipage}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \subfigure[Agent's effort in project 1]{
%         \includegraphics[width=1.7in]{e1.png}
%     }
%     \subfigure[Agent's effort in project 2]{
% 	\includegraphics[width=1.7in]{e2.png}
%     }
%     \caption{These pictures show the agent's strategy.}
%     \label{fig:two}
% \end{figure}

% \begin{figure}
% \centerline{\includegraphics[width=1in]{agentProfit.png}}
% \caption{The picture shows the agent's maximum profit given $p1$ and $p2$}
% \label{fig:three}
% \end{figure}

\begin{figure}[htbp]
    \centering
    \subfigure[Agent's effort in project $1$]{
        \includegraphics[width=1.7in]{e1.png}
    }
    \subfigure[Agent's effort in project $2$]{
	\includegraphics[width=1.7in]{e2.png}
    }
    \subfigure[Agent's maximum profit given $p_{1}$ and $p_{2}$]{
        \includegraphics[width=1.7in]{agentProfit.png}
    }
    \caption{These pictures show the agent's strategy.}
    \label{fig:two}
\end{figure}

Panel (a) and Panel (b) of \Cref{fig:two} report the agent's effort $e_{1}$ in project $1$ and $e_{2}$ in project $2$ when two principals choose ``tax rate'' $p_{1}$ and $p_{2}$. One thing to notice in \Cref{fig:two} is that the agent only put the effort in project $1$ when $p_{1}<p_{2}$ and only put the effort in project $2$ when $p_{1}>p_{2}$. This is intuitive since the agent always acts in his best interest and the cost of effort in project $1$ and project $2$ are the same. Therefore, the agent will always choose the project with the lower ``tax rate'' and the higher expected return.

Panel (c) of \Cref{fig:two} reports the agent's maximum profit given principal $1$'s choice of $p_{1}$ and principal $2$'s choice of $p_{2}$. The agent can achieve this maximum profit if he acts in his best strategy. This strategy would involve the agent making decisions that maximize his expected profit. In this environment, this means putting effort into project $1$ and project $2$ according to the strategy demonstrated in Panel (a) and (b) of \Cref{fig:two}.

\subsection{Principal's reward function}
Two principals choose their own ``tax rate'' without knowing each other's choice. Initially, principal $1$'s reward is $(T_{1}-I_{1})e_{1}p_{1}$, and principal $2$'s reward is $(T_{2}-I_{2})e_{2}p_{2}$. But the competition between principal $1$ and principal $2$ will lead them to both choose a ``tax rate'' close to $0$. And if two principals fully collude with each other, it will be close to the single principal-agent problem. Therefore, we induce a parameter of the rate of ``identity of interests'' $\beta$. $\beta$ represents how much the two principal's incentives are aligned with each other. It is a measure of how closely the two principals are working together to achieve a common goal. The higher the $\beta$, the more aligned the two principals' interests are.

\begin{itemize}
\item Principal $1$'s reward is $(T_{1}-I_{1})(e_{1}p_{1}+e_{2}p_{2})\beta+(T_{1}-I_{1})e_{1}p_{1}(1-2\beta)$
\item Principal $2$'s reward is $(T_{2}-I_{2})(e_{1}p_{1}+e_{2}p_{2})\beta+(T_{2}-I_{2})e_{2}p_{2}(1-2\beta)$
\item $\beta\in[0,0.5]$
\end{itemize}

\subsection{Baseline Parametrization and Initialization}
Initially, we focus on a baseline economic environment that consists of a dual contract problem with parameters as follows.
\begin{itemize}
\item $I_{1}=I_{2}=1$
\item $T=I_{1}+I_{2}=2$
\item $c=I_{1}+I_{2}=2$
\end{itemize}

For this specification, the profit $(T-I)(e_{1}+e_{2})$ is equal to the agent's cost $\frac{1}{2}c(e_{1}+e_{2})^2$ when the agent's total effort is equal to $1$. This means that when the agent exerts total effort $e1+e2=1$, the net profit will be equal to $0$.

As for the initial matrix $Q_0$, our baseline choice is to set the Q-values at $t=0$ at a random number between $0$ and $1$.

The learning parameter $\alpha$ may be in the principal range from $0$ to $1$. we choose $\alpha=0.1$ in our baseline model, following common practice in the computer science literature.

As for the experimentation parameter $\epsilon$, since there is more than one algorithm learning, and if one algorithm experiments more extensively, this creates noise in the environment, which makes it harder for the other to learn. On the other hand, the algorithms need to explore extensively, as the only way to learn is through multiple visits to every state-action cell (of which there are 101 in our baseline experiment with $|S|=1,|A|=101$). 

In our baseline model, we use the $\epsilon$-greedy model with a time-declining exploration rate. Specifically, we set

\begin{equation}
\label{eqn:11}
\epsilon_t=e^{-kt},
\end{equation}

where $k>0$ is a parameter. This means that initially, the algorithms choose in a purely random fashion, but as time passes, they make the greedy choice more and more frequently. The greater k, the faster the exploration diminishes. Initially, we set $k=5\times10^{-6}$.

\subsection{Pure competition region}
When $\beta=0$, two principals seek to maximize the profit from their own project. Because the agent will only put effort into the project with a lower ``tax rate'', the competition between the two principals leads them to give a very low ``tax rate'', and most of the profit is pocketed by the agent. The outcome of this experiment is reported in \Cref{fig:four}.

\begin{figure}[htbp]
    \centering
    \subfigure[]{
        \includegraphics[width=2.5in]{beta0_1.png}
    }
    \subfigure[]{
	\includegraphics[width=2.5in]{beta0_2.png}
    }
    \quad
    \subfigure[]{
        \includegraphics[width=2.5in]{beta0_3.png}
    }
    \subfigure[]{
	\includegraphics[width=2.5in]{beta0_4.png}
    }
    \caption{These pictures show the Q-learning algorithms converge to a very low $p$ when $\beta=0$.}
    \label{fig:four}
\end{figure}

\paragraph{Result}
Both algorithms converge to the lowest possible positive $p$ ($0.01$ in our case).

In \Cref{fig:four}, both principal $1$ and principal $2$ converge to the lowest possible positive $p$. This is because if one principal chooses a ``tax rate'' $p$ higher than the lowest possible positive $p$, then the other principal will try to give a ``tax rate'' $p$ lower than his competitor. Then the agent will put all his effort into the project provided by the principal with the lower ``tax rate''. This will lead to the principal with the higher ``tax rate'' getting nothing. Because the two principals only consider their own interests, the competition between them forces them to give very low ``tax rates'', and both principals cannot get much from their contracts.

\subsection{Pure collusion region}
When $\beta=0.5$, two principals seek to maximize the profit from both projects. There is no difference between which project the agent puts effort into because two principals will divide the revenue from both contracts evenly. The agent will only put effort into the project with a lower ``tax rate'', and only the principal currently giving the lower ``tax rate'' $p$ can effectively learn during the learning process. Because the agent only does business with the principal with a lower ``tax rate'', the principal with a higher ``tax rate'' cannot gather enough information to effectively update his Q-table. Therefore, when $\beta=0$, the two algorithms seldom converge to the same ``tax rate'' $p$. The outcome of this experiment is reported in \Cref{fig:five}.

\begin{figure}[htbp]
    \centering
    \subfigure[]{
        \includegraphics[width=2.6in]{beta5_0.png}
    }
    \subfigure[]{
	\includegraphics[width=2.6in]{beta5_2.png}
    }
    \quad
    \subfigure[]{
        \includegraphics[width=2.6in]{beta5_3.png}
    }
    \subfigure[]{
	\includegraphics[width=2.6in]{beta5_4.png}
    }
    \caption{These pictures show the lower ``tax rate'' two Q-learning algorithms choose converges to the same $p$ ($p=0.5$) as in a single principal-agent problem when $\beta=0.5$.}
    \label{fig:five}
\end{figure}

\paragraph{Result}
The lower ``tax rate'' two Q-learning algorithms choose converges to the same $p$ ($p=0.5$) as in a single principal-agent problem.

In \Cref{fig:five}, either principal $1$ or principal $2$ converge to the same $p$ ($p=0.5$) as in a single principal-agent problem. This is because the two principals seek to maximize revenues from both contracts. After one principal converges to the optimal ``tax rate'' $p=0.5$, the other principal who gives a higher $p$ cannot get the information to effectively continue his learning process. So the other principal eventually stays at a ``tax rate'' higher than the optimal ``tax rate''. But we need only focus on the lower ``tax rate'' two principals choose, as that is what makes an impact on the final revenues the principals and the agent obtain.

One thing to notice in \Cref{fig:five} is that the lower ``tax rate'' the algorithms converge to is higher than the one when $\beta=0$. The total revenue two principals obtain is higher than the one when $\beta=0$. This is intuitive since the principals compete fiercely when $\beta=0$ and the competition between them cuts down their total benefit. When $\beta=0.5$, the interests of the two principals are aligned with each other, so they can reach a certain level of collusion and increase their total benefit.

\subsection{Mixed-sum region}
When $0<\beta<0.5$, the reward given to each principal is from both his own contract and the other principal's contract. The bigger the $\beta$, the more the reward is from the other principal's contract, and the less the reward is from his own contract. When $\beta$ is very low, most of the reward comes from the principal's own contract. When $\beta$ is high, the reward comes from both principals' contracts. The outcome is reported in \Cref{fig:six}.

\paragraph{Result}
The greater the $\beta$, the higher the ``tax rate'' $p$ both principals converge to.

\begin{figure}
\centerline{\includegraphics[width=5in]{betaResult.png}}
\caption{The picture shows the effective ``tax rate'' two algorithms choose converges to a higher value as $\beta$ grows.}
\label{fig:six}
\end{figure}

In \Cref{fig:six}, the ``tax rate'' $p$ both principals converge to increases as $\beta$ grows. This is because the greater the $\beta$, the more the two principals' interests are aligned with each other. And the more likely they are to cooperate and work together to raise their ``tax rate''. This can lead to a stronger and more successful relationship between the two parties. Aligning interests can also help to reduce conflict and increase the total profit gained by both principals. And the lower the $\beta$, the less two principals' interests are aligned with each other. This can lead to more competition between the two principals and reduce the total profit gained by both principals.

\section{Principal Heterogeneity}\label{asymmetry}
In the previous discussion, there is no difference between the two principals. The agent does business with two homogeneous principals. But the agent's cost of effort in the two principals can be different in the real economy. For example, one principal may be nearer to or have a closer relationship with the agent than the other principal. In this case, the agent may be more willing to do business with the more advantageous principal, given two principals' ``tax rates'' are equal. He may be more willing to put effort into the advantageous principal in order to maximize his expected payoff. On the other hand, the agent may be less willing to put effort into the disadvantaged principal if the cost of effort associated with it is higher or the risk associated with it is higher. In this section, we introduce asymmetry in the two principals.

Specifically, we set the agent's cost of effort
\begin{equation}
\label{eqn:11}
cost=\frac{1}{2}c(e_{1}+e_{2})^2(1-\kappa+2{\kappa}e_{2}/(e_{1}+e_{2})), e_{1}+e_{2}\neq0,
\end{equation}

where $e_{1}{\in}[0,1]$ is the effort the agent put into project $1$, $e_{2}{\in}[0,1]$ is the effort the agent put into project $2$, and $\kappa\in[0,1)$ is a parameter. The agent's cost is equal to $0$ when $e_{1}+e_{2}=0$. The larger the $\kappa$, the less cost per effort when the agent does business with principal $1$, and the more cost per effort when the agent does business with principal $2$. Therefore, the larger the $\kappa$, the more advantage principal $1$ has over principal $2$. Initially, we set $\kappa=0.2$ in our baseline model.

\begin{figure}[htbp]
    \centering
    \subfigure[Agent's effort in project 1]{
        \includegraphics[width=1.7in]{ae1.png}
    }
    \subfigure[Agent's effort in project 2]{
	\includegraphics[width=1.7in]{ae2.png}
    }
    \subfigure[Agent's maximum profit given $p_{1}$ and $p_{2}$]{
        \includegraphics[width=1.7in]{aProfit.png}
    }
    \caption{These pictures show the agent's strategy in an asymmetric environment.}
    \label{fig:seven}
\end{figure}

One thing to notice in \Cref{fig:seven} (a) and \Cref{fig:seven} (b) is that compared with the previous symmetric environment, in this asymmetric environment, principal $2$ needs to provide a much lower ``tax rate'' than his competitor principal $1$ in order to attract the agent to put the effort in his project. And there is an asymmetry in \Cref{fig:seven} (c). When the agent is doing business with principal $1$, the agent's profit is significantly higher. This is intuitive since the agent's cost per effort is lower when in project $1$ and higher in project $2$. The outcome of this experiment is reported in \Cref{fig:eight}.

\paragraph{Result}
\begin{itemize}
\item Principal $1$ chooses a ``tax rate'' much higher than $0$ when $\beta=0$.
\item The effective ``tax rate'' $p$ is higher than the one in the symmetric environment ($p=0.5$) when $\beta=0.5$.
\item The greater the $\beta$, the higher the effective ``tax rate'' $p$ (the ``tax rate'' of the project the agent puts effort into).
\end{itemize}

\begin{figure}
\centerline{\includegraphics[width=5in]{asymBeta.png}}
\caption{The picture shows the effective ``tax rate'' two algorithms choose when there is principal heterogeneity.}
\label{fig:eight}
\end{figure}

In \Cref{fig:eight}, the effective ``tax rate'' ($p_{1}$ in this case) when $\beta=0$ is no longer close to $0$. This is because when there is principal heterogeneity, principal $1$ has an advantage over principal $2$ due to the fact that the agent's cost per effort is lower when doing business with principal $1$ and higher when doing business with principal $2$. This advantage can, to a certain extent, protect principal $1$ from his competitor, principal $2$. Therefore, principal $1$ can offer a ``tax rate'' much higher than $0$, increasing his profit without worrying about the competition from principal $2$ when $\beta=0$.

When $\beta=0.5$, two principals seek to maximize the profit from both projects. Now there is no competition because, after all, the two principals divide the revenue from both contracts evenly. Because it is more efficient for the agent to do business with principal $1$, both two principals would like the agent to put effort into project $1$. Thus, the effective ``tax rate'' is literally the ``tax rate'' chosen by principal $1$. Because now the agent's cost per effort is lower, the agent will exert more effort given the same $p$ as in the previous discussion, and principal $1$ is able to raise his ``tax rate'' and gain more profit. In fact, the effective ``tax rate'' two Q-learning algorithms choose converges to the same $p$ as in a single principal-agent contract problem with the same parameters.

Similar to the case when there is no principal heterogeneity, the greater the $\beta$, the higher the effective ``tax rate'' $p$. When $\beta$ is low, two principals compete with each other, and the competition cuts down their total revenue, letting the agent take away most of the projects' proceeds. The bigger the $\beta$, the more the two principals' interests are aligned with each other, and their rising level of cooperation will help them raise the effective ``tax rate" and get a bigger share from the projects' proceeds.