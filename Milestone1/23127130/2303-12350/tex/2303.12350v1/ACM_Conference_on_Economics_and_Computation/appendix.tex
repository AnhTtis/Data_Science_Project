\section{Appendix}\label{appendix}
\subsection{Algorithms}

% Algorithm
\begin{algorithm}[htbp]
\SetAlgoNoLine
\KwIn{Experimentation  $\epsilon$, $t_{max}$, principal option dimension $D_p$ and learning rate $\alpha$}
\KwOut{Learned Q-table $Q$}
$t$ = 0; $Q$ = a $1*D_p$ array initiated with $Random(0,1)$\;
\Repeat{$t {\ge} t_{max}$}{
      
      \eIf{(Random(0,1) < $\epsilon$)}{
        $Act_{p1}$ = Randint(0, $D_p-1$)\;
      }
     {
        $Act_{p1}$ = Argmax($Q$)\;
     }
     $Act_{a}$ = agentAct($Act_{p1}$)\;
     $Profit_{p1}$ = calcProfit($Act_{p1}$, $Act_{a}$)\;
     $Q[Act_{p1}]$ = $(1-\alpha)Q[Act_{p1}]+{\alpha}Profit_{p1}$\;
     $t$ ++\;
      }
\caption{Learning Algorithm in Single Principal-agent Problem}
\label{alg:one}
\end{algorithm}

% Algorithm
\begin{algorithm}[htbp]
\SetAlgoNoLine
\KwIn{Exploration decay parameter $k$, $t_{max}$, principal option dimension $D_p$, learning rate $\alpha$ and $\beta$}
\KwOut{Learned Q-tables $Q_{p1}$ and $Q_{p2}$}
$t$ = 0; $Q_{p1}$ = a $1*D_p$ array initiated with $Random(0,1)$; $Q_{p2}$ = a $1*D_p$ array initiated with $Random(0,1)$\;
\Repeat{$t {\ge} t_{max}$}{
      
      \eIf{(Random(0,1) < $e^{-kt}$)}{
        $Act_{p1}$ = Randint(0, $D_p-1$)\;
      }
     {
        $Act_{p1}$ = Argmax($Q_{p1}$)\;
     }
     \eIf{(Random(0,1) < $e^{-kt}$)}{
        $Act_{p2}$ = Randint(0, $D_p-1$)\;
      }
     {
        $Act_{p2}$ = Argmax($Q_{p2}$)\;
     }
     $Act_{a}$ = agentAct($Act_{p1}$, $Act_{p2}$)\;
     $Profit_{p1}$, $Profit_{p2}$ = calcProfit($Act_{p1}$, $Act_{p2}$, $Act_{a}$)\;
     $Q_{p1}[Act_{p1}]$ = $(1-\alpha)Q[Act_{p1}]+{\alpha}({\beta}(Profit_{p1}+Profit_{p2})+(1-2\beta)Profit_{p1})$\;
     $Q_{p2}[Act_{p2}]$ = $(1-\alpha)Q[Act_{p2}]+{\alpha}({\beta}(Profit_{p1}+Profit_{p2})+(1-2\beta)Profit_{p2})$\;
     $t$ ++\;
      }
\caption{Learning Algorithm in Dual Contract Problem}
\label{alg:two}
\end{algorithm}

% Algorithm
\begin{algorithm}[htbp]
\SetAlgoNoLine
\KwIn{Principal option dimension $D_p$, agent option dimension $D_a$}
\KwOut{Agent choice table $T_a$}
$V_a$ = a $D_p*D_p*D_a$ array initiated with 0\;
$T_a$ = a $D_p*D_p$ array initiated with 0\;
i,j,k=0\;
\Repeat{$i {\ge} D_p$}{
\Repeat{$j {\ge} D_p$}{
\Repeat{$k {\ge} D_p$}{
$Act_{p1}$ = $i{\div}(D_p-1)$, $Act_{p2}$ = $j{\div}(D_p-1)$, $Act_{a}$ = Decode($k$);
$V_a[i][j][k]$=agentProfit($Act_{p1}$,$Act_{p2}$,$Act_{a}$)\;
$k$ ++\;
}
$j$ ++\;
}
$i$ ++\;
}
i,j=0\;
\Repeat{$i {\ge} D_p$}{
\Repeat{$j {\ge} D_p$}{
$T_a[i][j]$=Argmax($V_a[i][j]$)\;
$j$ ++\;
}
$i$ ++\;
}
\caption{Agent decision algorithm in Dual Contract Problem}
\label{alg:three}
\end{algorithm}