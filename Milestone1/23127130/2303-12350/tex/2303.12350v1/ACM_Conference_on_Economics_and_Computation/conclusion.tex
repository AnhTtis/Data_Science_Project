\section{Conclusion}\label{conclusion}
This paper presents a reinforcement learning approach to autonomously learn incentive compatible contract. We started from our baseline contract model, first showing that the Q-learning algorithm successfully tackle the single principal-agent problem. Then we used multi-agent Q-learning and showed that this method could find the optimal contract in various dual contract problem scenarios. We showed that the algorithms were able to learn an optimal policy and were able to maximize their reward throughout the simulation.

This approach has the potential to be applied to a variety of real-world scenarios, such as the design of incentive contracts or the optimization of resource allocation. Interestingly, many questions remain open. First, one may be worried about the robustness of our findings in allowing other artificial intelligence algorithms to make the contract. We expect that the new methodology we have identified will be present in many algorithms that operate with limited information. Related to that question of other algorithms is what would happen if we made the Q-learning algorithms more sophisticated, for example, by keeping as a state what the opponent's previous choice was.

Second, we looked at a straightforward environment with definite outcomes for definite actions. One interesting question is how introducing randomness will affect the algorithms finding the optimal contract. Multi-phase contracting and time-varying valuations are also of great interest since they would provide additional insights into using artificial intelligence algorithms that constantly experiment and try to adapt to the changing environment.

Although algorithm contracting may open the way to new forms of best incentive contract mechanism design, it is worth noting that contracting behavior among human decision-makers is more complex. Now we are only focusing on a contract model similar to the debt contract. Whether artificial intelligence algorithms can solve many other contract models remains an open question. It is still unclear how well algorithms can simulate real-life contracting behaviors.
