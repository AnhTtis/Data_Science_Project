\section{Introduction}\label{introduction}

\indent In the wake of recent advancements, a growing chorus of scholars and organizations has sounded the alarm regarding the potential for Artificial Intelligence (AI) algorithms to create an \emph{AI alignment problem}. This phenomenon arises when the specified reward function diverges from the actual values of relevant stakeholders, including designers, users, and those affected by the agent's behavior (see \cite{gabriel2020artificial} and \cite{eloundou2023gpts}). Notably, this issue bears a striking resemblance to the classic principal-agent problem (see \cite{hadfield2019incomplete}), where misaligned incentives can lead to suboptimal outcomes. We propose that the analytical framework of incomplete contracting, adapted to the context of AI algorithms, offers a fruitful approach to understanding the alignment of incentives among algorithms and mitigating the AI alignment problem.


\indent The advent of artificial intelligence (AI) has precipitated a plethora of concerns regarding the potential misalignment of AI algorithms. However, the veracity of this risk remains an open question, beset by both theoretical and empirical ambiguities. From an empirical perspective, the detection of misalignment from market outcomes is fraught with difficulty. The opacity of firms' financial and employment contracts, which are typically shrouded in secrecy, exacerbates this challenge. The lack of transparency in contractual arrangements hinders the ability to discern whether AI algorithms are, in fact, misaligned.\footnote{For instance, the agency problem inherent in executive compensation remains a contentious and complex issue, particularly in the digital era. A significant challenge in this realm is the endogeneity of compensation arrangements, which are often correlated with unobservable factors, thereby rendering the estimation of their causal effects on firm behavior and value extremely difficult (see \cite{frydman2010ceo}). Furthermore, the rapid growth of e-commerce, fintech, and platform economies has led to an proliferation of digital contracts, as exemplified by companies such as Amazon, Uber, and PayPal. However, the opacity of these contracts, driven in part by concerns over user privacy, poses significant obstacles to empirical analysis.} On the theoretical side, the interplay among reinforcement-learning algorithms gives rise to intricate dynamic stochastic multi-agent systems, whose complexity presently defies analytical tractability. The emergent properties of these systems, characterized by interacting adaptive agents, pose a significant challenge to theoretical analysis, rendering closed-form solutions elusive at present.


\indent To make some progress, this paper takes an experimental approach. The possibility arises from the recent evolution of AI algorithms from rule-based to multi-agent reinforcement learning (hereafter referred to as MARL)\footnote{See \cite{zhang2021multi} for more details about the MARL.} programs, which are able to learn from data and adapt to changing environments.By constructing AI-based agents, we enable them to engage in repeated interactions, thereby allowing us to examine the dynamics of contract negotiation and design. 

\indent A crucial challenge in this approach lies in selecting economically meaningful environments and algorithms that accurately reflect real-world contract design scenarios. To address this, we begin with a traditional principal-agent problem as a benchmark and subsequently extend our analysis to a three-sided contracting problem, where parties exhibit heterogeneous preferences over contract terms â€“ a scenario commonly referred to as the \emph{dual-contract} problem. Our MARL algorithms tackle this "dual-contract" problem, and our findings suggest that the emergence of algorithmic incentive compatibility is more than a theoretical possibility. Specifically, our results demonstrate that MARL algorithms can effectively learn incentive-compatible contracts, thereby providing new insights into the potential of AI in contract design.

 

\indent To clarify the basic contribution of this paper, we start by comparing the following concepts 

\begin{itemize}
\item \textbf{Classical Principal-agent Problem}, a paradigmatic issue in economics and contract theory, arises when one party (the principal) cedes decision-making authority to another (the agent). This fundamental asymmetry occurs when the principal provides the requisite resources and capital for a project, while the agent is tasked with its execution. The principal must therefore design incentives to ensure that the project is completed in an efficient and effective manner. This two-sided problem has been extensively examined in the literature, with applications in diverse fields, and remains a cornerstone of economic theory.

\item \textbf{Dual-Contracting Problem} is a three-player variant of the canonical principal-agent problem, which we term the Dual-Contracting Problem. This paradigmatic framework features two principals, each contributing resources and capital to a joint project. The dual principals may harbor identical or divergent objectives and interests, necessitating coordination or competition to ensure the project's efficient and effective completion. Despite its significance, this problem has received scant attention in the literature, particularly in dynamic settings, where the complexity of solving three (or more)-agent Markov games has posed a significant challenge to conventional methodologies. To the best of our knowledge, this study pioneers the application of innovative methodologies, including artificial intelligence algorithms, to tackle the well-defined dual-contracting problem, thereby contributing a novel approach to the existing literature.

\item \textbf{AI for Mechanism Design} has been explored in recent literature (e.g., \cite{calvano2020artificial}, \cite{banchio2023artificial}), where AI algorithms are employed to tackle mechanism design problems. Specifically, multi-agent reinforcement learning (MARL) programs have been proposed, which leverage data-driven learning and adapt to complex multi-agent interactions. By harnessing the capabilities of these algorithms, researchers can optimize the terms of a mechanism design problem and infer the behavior of artificial intelligence, ultimately aiming to maximize the expected utility of all parties involved.

\end{itemize}

\indent Along with investigating the AI alignment problem, we are interested in studying how to design contracts by AI algorithms for three alternative reasons. Firstly, the development of AI-driven contracts has significant implications for online contracting scenarios, particularly in the context of decentralized multi-sided platforms.

\indent Secondly, the proliferation of decentralized systems, such as blockchain and smart contracts, has led to the widespread adoption of incentive optimization tools. As Web 3.0 applications continue to gain traction, it is essential to examine the competitive dynamics that emerge when multiple agents employ similar algorithmic tools, each optimized to serve the interests of its respective owner.

\indent Thirdly, understanding the interplay between AI algorithms and contract design is crucial for the development of effective contracts that incentivize desired behavior in AI-driven applications. By elucidating the interactions between AI algorithms and contract design, we can create contracts that align with the objectives of AI systems, while also mitigating potential risks associated with these applications.

\indent In the context of dual-contracting problems, the intricate interplay between multiple principals and agents poses a significant challenge. Recent advances in artificial intelligence have led to the development of adaptive algorithms that can learn from data and navigate complex multi-agent interactions. Building upon a dynamic extension of the classic moral hazard model, we investigate the efficacy of these algorithms in facilitating incentive-compatible strategies. Our results demonstrate that, despite their relative simplicity, these contracting algorithms are capable of dynamically converging to Nash equilibrium outcomes. Furthermore, our baseline analysis reveals that the initial conditions of the environment cease to influence the equilibrium outcome, underscoring the robustness of our approach.
 
\indent This paper highlights a crucial distinction between the classical principal-agent paradigm and the dual-contracting framework. Unlike the traditional principal-agent problem, which is inherently a single-principal setup, the dual-contracting problem accommodates the complex interactions between multiple principals, thereby capturing the nuanced effects of collusion and competition on contract design. A closer examination of the dual-contracting problem reveals several key departures from the standard principal-agent framework, which can lead to divergent outcomes. Notably, the dual-contracting setup can give rise to multi-sided information asymmetry, a phenomenon that warrants further investigation. Specifically, we identify several key differences between the two frameworks that contribute to these disparate outcomes, including:

\begin{itemize}
    \item Misaligned contract incentives reduce principals' benefits.
    
    \item The principal responds strategically to changes in the behavior of agents and other principals.
    
    \item Advantageous principals, shielded from competition, reap enhanced market power and benefits.
    
\end{itemize}

\indent In an application of artificial intelligence to contract design, we observe that AI-based principals converge on incentive structures that exceed the single principal-agent equilibrium, yet fall short of the competitive benchmark. The emergence of these outcomes is facilitated by the sophisticated algorithms employed, which are characterized by advanced memory capabilities. Through iterative learning and adaptation, these algorithms develop strategies that mitigate myopic preferences and optimize long-term payoffs. Notably, these AI-based principals operate independently, without explicit instructions to collude or compete, and without prior knowledge of the environmental parameters. This phenomenon has significant implications for our understanding of decentralized decision-making and the design of optimal contracts in complex environments.

\indent In this study, we employ a symmetric duopoly framework featuring a principal-agent relationship, and subsequently conduct a comprehensive robustness analysis to account for heterogeneity among principals. Our findings suggest that a principal possessing an advantage over its competitor can derive protection from competition, with the protective effect intensifying as the level of competition increases. Notably, this protection effect yields a tax rate $p$ that is significantly higher than zero in the region of pure competition, thereby enhancing the profit of the advantaged principal without concern for competitive pressures from its rival. Furthermore, in the region of pure collusion, the two principals divide the revenue from both contracts equally, which creates an incentive for both principals to encourage the agent to exert effort on the project of the advantaged principal.

\indent We devised a series of experiments and simulations to disentangle the competing explanations for the observed phenomenon. Our results indicate that the primary driver of the disparity lies in the presence of multiple principals, whose interests exhibit varying degrees of alignment. This force operates distinctly in standard contract and dual-contract problems, respectively. Furthermore, our findings shed light on the mechanisms underlying the diminished overall welfare of a party afflicted by intra-group conflicts of interest, which arise from multi-sided information asymmetry. 


\indent This work provides proof of concept that AI algorithms can be used to autonomously learn incentive compatibility in contract design. The proposed multi-agent reinforcement learning (MARL) algorithm is a promising approach to the problem of contract design and negotiation, as it can autonomously learn incentive compatibility and reach a Nash equilibrium in a reasonable number of iterations.\footnote{Notably, our research highlights the efficacy of unsupervised learning algorithms in achieving convergence to stable outcomes within a remarkably brief time horizon. Specifically, our simulations, which entail hundreds of thousands of interactions, can be completed in a matter of hours. This feat is made possible by our innovative application of parallel computing techniques, implemented in C++, which enables high-performance computing. Moreover, the rapid advancement of artificial intelligence computing technologies, such as Graphics Processing Units (GPUs) and Neural Processing Units (NPUs), is poised to further accelerate the computational efficiency of contract design programs, potentially reducing processing times to mere minutes in the near future. This has significant implications for the development of efficient contract design mechanisms, with far-reaching consequences for the field of economics.} This research has far-reaching implications for the study of multi-sided contracting problems, with potential applications to three-sided and higher-dimensional settings. Moreover, the integration of alternative artificial intelligence (AI) methodologies, such as deep reinforcement learning, may yield further insights into the optimization of contractual agreements. Notably, the proposed multi-agent reinforcement learning (MARL) algorithm offers a promising avenue for maximizing expected utility for all parties involved, by optimizing the terms of a contract to achieve mutually beneficial outcomes.

The incorporation of artificial intelligence (AI) algorithms in contract design and negotiation can yield significant benefits. By leveraging machine learning capabilities, AI can identify potential risks inherent in a given contract and propose mitigating adjustments, thereby enhancing contractual robustness. Furthermore, AI-driven negotiation support systems can facilitate more efficient contract negotiations by generating terms that are likely to be mutually acceptable, thereby reducing the transaction costs associated with the negotiation process. Ultimately, the strategic deployment of AI algorithms can inform more effective contract design and negotiation strategies, leading to improved outcomes for all parties involved.

\subsection{Related Literature}

\indent This study advances the existing literature by introducing a novel Multi-Agent Reinforcement Learning (MARL) framework to tackle the dual-contract problem, and experimentally demonstrating its capacity to autonomously learn incentive-compatible mechanisms. Our proposed algorithm offers a promising solution for contract design, enabling organizations to make more informed decisions when designing and negotiating contracts in online environments.

The burgeoning literature on the application of artificial intelligence (AI) algorithms to mechanism design problems is still in its nascent stages. Nevertheless, a handful of pioneering studies have recently ventured into this uncharted territory, laying the groundwork for further exploration and innovation in this promising area of research. For example, \cite{banchio2022artificial} proposed an autonomous AI-based auction design using a reinforcement learning algorithm. \cite{hansen2021frontiers} show how misspecified implementation results in collusion by simulating a different algorithm from the bandit literature. In contrast to those works, the present paper is the first to explore the use of AI algorithms to solve the dual-contracting problem with incentive compatibility. We propose a MARL algorithm to solve the dual-contracting problem and analyze its performance regarding its ability to learn incentive compatibility. Our results suggest that AI algorithms can be used to autonomously learn incentive compatibility in dual-contract design.

\indent This paper contributes to an emerging literature that applies AI modeling in economics and finance. Recent literature in AI economics has been actively studying reinforcement learning that particularly utilizes the Q-learning method as the tool for experimental economics.  These include studies on learning and equilibrium selection in games (\cite{erev1998predicting}, \cite{waltman2008q}, \cite{klein2021autonomous}),  the role of AI in algorithmic pricing and potential collusion (\cite{10.1257/aer.102.5.2018}, \cite{calvano2020artificial}, \cite{klein2021autonomous}),  adaptive learning in economic settings (\cite{kasy2021adaptive}), and exploration of algorithmic biases and their impact (\cite{10.1257/pandp.20221059}). In contrast, our application of AI is motivated economically by the challenges observed in conventional dynamic contract theory and the pressing need for theoretically approximating humanity. We contribute conceptually by introducing a novel quantitative framework to solve the AI-based dual-contracting problem in a relatively transparent and interpretable modeling space.


\indent This paper hopes to usefully complement the rich theoretical literature on optimal contracting and principal-agent problems, such as \cite{innes1990limited}, \cite{schmidt1997managerial}, \cite{levin2003relational}, \cite{demarzo2006optimal}, \cite{demarzo2007optimal}, \cite{biais2007dynamic}, \cite{sannikov2008continuous} \cite{he2009optimal}, \cite{biais2010large}, \cite{garrett2012managerial}, \cite{demarzo2012dynamic}, \cite{edmans2012dynamic}, \cite{zhu2013optimal}, \cite{garrett2015dynamic}, and \cite{zhu2018myopic}, among many others. The optimal contract in these papers is typically highly complex, and they must engage several bounded assumptions or conditions to ensure the model's tractability. Note that most of these studies must suppose a specific scenario, such as one principal and one agent. In contrast, our paper considers a fairly general dual-contract setting with two principals and one agent, under a tractable AI setting, the model is able to deliver quantitative analysis in a dynamic multi-period setting and calibrate the model parameters using real data. 


\indent Our paper is organized as follows. In Section \Cref{Qlearning}, we provide a brief overview of Q-learning and multi-agent reinforcement learning. In \Cref{contracts}, adopt a two-agent Q-learning algorithm to analyze the single-principal-agent problem. \Cref{dualcontract} describes our proposed multi-agent Q-learning algorithm for the dual-contracting problem. In \Cref{robustness}, we present the results of the discussions and robustness checks. \Cref{conclusion} concludes. The omitted technical details are presented in \Cref{appendix}.


 