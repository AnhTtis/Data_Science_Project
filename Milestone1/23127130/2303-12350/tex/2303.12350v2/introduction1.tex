\section{Introduction}\label{introduction}
The rise of artificial intelligence (AI) has sparked concerns about the potential for misalignment between AI algorithms and human values. This "AI alignment problem" arises when algorithms designed to maximize a specific reward function produce outcomes that are undesirable or harmful, diverging from the intended goals of designers, users, and society as a whole. This issue bears a striking resemblance to the classic principal-agent problem in economics, where misaligned incentives between principals and agents can lead to suboptimal outcomes.
The challenge of assessing the AI alignment problem is compounded by both empirical and theoretical limitations. Empirically, the opaque nature of contracts and the endogeneity of compensation arrangements in the digital economy hinder our ability to observe and measure misalignment. Theoretically, the complex dynamics of multi-agent reinforcement learning (MARL) systems, characterized by interacting adaptive agents, pose significant challenges to analytical tractability.
This paper takes an experimental approach to address these challenges, leveraging the capabilities of MARL algorithms to investigate the dynamics of contract design and negotiation. We focus on the "dual-contract" problem, a three-sided contracting scenario with two principals and one agent, where principals may have heterogeneous preferences over contract terms. Our findings suggest that MARL algorithms can effectively learn incentive-compatible contracts, providing new insights into the potential of AI in contract design.
To clarify our contribution, we consider three key concepts:
\begin{itemize}
\item \textbf{Classical Principal-Agent Problem:} This canonical problem in economics arises when a principal delegates decision-making authority to an agent. The principal must design incentives to ensure the agent acts in their best interest.
\item \textbf{Dual-Contracting Problem:} This is a three-player variant of the principal-agent problem, where two principals contribute resources to a joint project. This framework captures the complexity of coordinating and competing interests between multiple principals.
\item \textbf{AI for Mechanism Design:} This emerging area explores the use of AI algorithms, specifically MARL, to tackle mechanism design problems. These algorithms learn from data and adapt to complex multi-agent interactions to optimize mechanism terms and infer AI behavior.
\end{itemize}
Beyond the AI alignment problem, our interest in AI-driven contract design is motivated by several factors. First, the development of AI-driven contracts has significant implications for online contracting scenarios, particularly in decentralized multi-sided platforms. Second, the proliferation of decentralized systems like blockchain and smart contracts has led to the widespread adoption of incentive optimization tools. Understanding the competitive dynamics between multiple agents using such tools is crucial. Third, understanding the interplay between AI algorithms and contract design is essential for developing effective contracts that incentivize desired behavior in AI-driven applications.
In the context of dual-contracting problems, the intricate interplay between multiple principals and agents poses a significant challenge. Recent advances in artificial intelligence have led to the development of adaptive algorithms that can learn from data and navigate complex multi-agent interactions. Building upon a dynamic extension of the classic moral hazard model, we investigate the efficacy of these algorithms in facilitating incentive-compatible strategies. Our results demonstrate that, despite their relative simplicity, these contracting algorithms are capable of dynamically converging to Nash equilibrium outcomes. Furthermore, our baseline analysis reveals that the initial conditions of the environment cease to influence the equilibrium outcome, underscoring the robustness of our approach.
This paper highlights a crucial distinction between the classical principal-agent paradigm and the dual-contracting framework. Unlike the traditional principal-agent problem, which is inherently a single-principal setup, the dual-contracting problem accommodates the complex interactions between multiple principals, thereby capturing the nuanced effects of collusion and competition on contract design. A closer examination of the dual-contracting problem reveals several key departures from the standard principal-agent framework, which can lead to divergent outcomes. Notably, the dual-contracting setup can give rise to multi-sided information asymmetry, a phenomenon that warrants further investigation. Specifically, we identify several key differences between the two frameworks that contribute to these disparate outcomes, including misaligned contract incentives, strategic responses by principals, and the emergence of advantageous principals with enhanced market power.
In an application of artificial intelligence to contract design, we observe that AI-based principals converge on incentive structures that exceed the single principal-agent equilibrium, yet fall short of the competitive benchmark. The emergence of these outcomes is facilitated by the sophisticated algorithms employed, which are characterized by advanced memory capabilities. Through iterative learning and adaptation, these algorithms develop strategies that mitigate myopic preferences and optimize long-term payoffs. Notably, these AI-based principals operate independently, without explicit instructions to collude or compete, and without prior knowledge of the environmental parameters. This phenomenon has significant implications for our understanding of decentralized decision-making and the design of optimal contracts in complex environments.
We devised a series of experiments and simulations to disentangle the competing explanations for the observed phenomenon. Our results indicate that the primary driver of the disparity lies in the presence of multiple principals, whose interests exhibit varying degrees of alignment. This force operates distinctly in standard contract and dual-contract problems, respectively. Furthermore, our findings shed light on the mechanisms underlying the diminished overall welfare of a party afflicted by intra-group conflicts of interest, which arise from multi-sided information asymmetry.
This work provides proof of concept that AI algorithms can be used to autonomously learn incentive compatibility in contract design. The proposed multi-agent reinforcement learning (MARL) algorithm is a promising approach to the problem of contract design and negotiation, as it can autonomously learn incentive compatibility and reach a Nash equilibrium in a reasonable number of iterations. This research has far-reaching implications for the study of multi-sided contracting problems, with potential applications to three-sided and higher-dimensional settings. Moreover, the integration of alternative artificial intelligence (AI) methodologies, such as deep reinforcement learning, may yield further insights into the optimization of contractual agreements. Notably, the proposed MARL algorithm offers a promising avenue for maximizing expected utility for all parties involved, by optimizing the terms of a contract to achieve mutually beneficial outcomes.
The incorporation of artificial intelligence (AI) algorithms in contract design and negotiation can yield significant benefits. By leveraging machine learning capabilities, AI can identify potential risks inherent in a given contract and propose mitigating adjustments, thereby enhancing contractual robustness. Furthermore, AI-driven negotiation support systems can facilitate more efficient contract negotiations by generating terms that are likely to be mutually acceptable, thereby reducing the transaction costs associated with the negotiation process. Ultimately, the strategic deployment of AI algorithms can inform more effective contract design and negotiation strategies, leading to improved outcomes for all parties involved.
\subsection{Related Literature}
This study advances the existing literature by introducing a novel Multi-Agent Reinforcement Learning (MARL) framework to tackle the dual-contract problem, and experimentally demonstrating its capacity to autonomously learn incentive-compatible mechanisms. Our proposed algorithm offers a promising solution for contract design, enabling organizations to make more informed decisions when designing and negotiating contracts in online environments.
The burgeoning literature on the application of artificial intelligence (AI) algorithms to mechanism design problems is still in its nascent stages. Nevertheless, a handful of pioneering studies have recently ventured into this uncharted territory, laying the groundwork for further exploration and innovation in this promising area of research. For example, \cite{banchio2022artificial} proposed an autonomous AI-based auction design using a reinforcement learning algorithm. \cite{hansen2021frontiers} show how misspecified implementation results in collusion by simulating a different algorithm from the bandit literature. In contrast to those works, the present paper is the first to explore the use of AI algorithms to solve the dual-contracting problem with incentive compatibility. We propose a MARL algorithm to solve the dual-contracting problem and analyze its performance regarding its ability to learn incentive compatibility. Our results suggest that AI algorithms can be used to autonomously learn incentive compatibility in dual-contract design.
This paper contributes to an emerging literature that applies AI modeling in economics and finance. Recent literature in AI economics has been actively studying reinforcement learning that particularly utilizes the Q-learning method as the tool for experimental economics. These include \cite{erev1998predicting}, \cite{10.1257/aer.102.5.2018}, \cite{calvano2020artificial}, \cite{https://doi.org/10.1111/1756-2171.12383}, \cite{kasy2021adaptive}, \cite{10.1257/pandp.20221059}, among many others. In contrast, our application of AI is motivated economically by the challenges observed in conventional dynamic contract theory and the pressing need for theoretically approximating humanity. We contribute conceptually by introducing a novel quantitative framework to solve the AI-based dual-contracting problem in a relatively transparent and interpretable modeling space.
This paper hopes to usefully complement the rich theoretical literature on optimal contracting and principal-agent problems, such as \cite{innes1990limited}, \cite{schmidt1997managerial}, \cite{levin2003relational}, \cite{demarzo2006optimal}, \cite{demarzo2007optimal}, \cite{biais2007dynamic}, \cite{sannikov2008continuous} \cite{he2009optimal}, \cite{biais2010large}, \cite{garrett2012managerial}, \cite{demarzo2012dynamic}, \cite{edmans2012dynamic}, \cite{zhu2013optimal}, \cite{garrett2015dynamic}, and \cite{zhu2018myopic}, among many others. The optimal contract in these papers is typically highly complex, and they must engage several bounded assumptions or conditions to ensure the model's tractability. Note that most of these studies must suppose a specific scenario, such as one principal and one agent. In contrast, our paper considers a fairly general dual-contract setting with two principals and one agent, under a tractable AI setting, the model is able to deliver quantitative analysis in a dynamic multi-period setting and calibrate the model parameters using real data.

This paper is organized as follows. In Section \ref{Qlearning}, we provide a brief overview of Q-learning and multi-agent reinforcement learning. In \ref{contracts}, we adopt a two-agent Q-learning algorithm to analyze the single-principal-agent problem. \ref{dualcontract} describes our proposed multi-agent Q-learning algorithm for the dual-contracting problem. In \ref{robustness}, we present the results of the discussions and robustness checks. \ref{conclusion} concludes. The omitted technical details are presented in \ref{appendix}.