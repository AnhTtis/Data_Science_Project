\section{Q-learning}\label{Qlearning}

\indent We focus on Q-learning algorithms \cite{watkins1992q} and \cite{calvano2020artificial}, a cornerstone of model-free reinforcement learning widely used in AI. These off-policy algorithms utilize a Q-value function—a matrix predicting the utility of actions in different states—to guide action selection. Through actions and rewards, the AI refines this function to maximize expected rewards over time, developing an optimal policy.\footnote{Q-learning, a reinforcement learning algorithm, aims to identify actions that yield the highest rewards. By learning from action outcomes, the decision-maker continuously improves its approach. Q-learning assigns values to actions, updating them based on new rewards to guide better decision-making. Our choice of Q-learning stems from its widespread real-world application, realistic simulation of decision-making, clear economic interpretation of parameters, and structural resemblance to advanced programs like ChatGPT \citep{ouyang2022training}. This section provides a concise overview, emphasizing its relevance and rationale for incorporation in our analysis.}


% Head 2
\subsection{Single Decision Maker Problems}

\indent Q-learning, a type of reinforcement learning, enables decision-makers to learn from experience and improve their choices. It seeks the optimal sequence of actions, known as a policy, to maximize rewards over time without prior knowledge of the problem. Initially designed for Markov Decision Processes (MDPs) with finite states and actions, Q-learning facilitates learning through interaction with the environment.

\indent In a stationary MDP, at each time step $t = 0, 1, 2,...$, a decision-maker observes state $s_t \in \mathcal{S}$ and chooses action $a_t \in \mathcal{A}$. Each state-action pair $(s_t, a_t)$ yields a reward $\pi_t$, and the system transitions to the next state $s_{t+1}$ according to a time-invariant probability distribution $F(\pi_t, s_{t+1} | s_t, a_t)$. Notably, Q-learning in this context assumes finite $\mathcal{S}$ and $\mathcal{A}$, with $\mathcal{A}$ being independent of the current state.


The decision-maker's problem is to maximize the expected present value of the reward stream:
% Numbered Equation
\begin{equation}
\label{eqn:01}
\mathbb{E} \left[ \sum_{t=0}^{\infty} \delta^t \pi_t \right],
\end{equation}
where $\delta \leq 1$ represents the discount factor. This dynamic programming problem is typically addressed using Bellman's value function:
% Numbered Equation
\begin{equation}
\label{eqn:02}
V(s_{t}) = \max_{a_{t} \in \mathcal{A}} \{ \mathbb{E}[\pi_{t} | s_{t}, a_{t}] + \delta \mathbb{E}[V(s_{t+1}) | s_{t}, a_{t}] \}.
\end{equation}
Building upon this, we introduce the Q-function, representing the discounted payoff of action $a$ in state $s$:
\begin{equation}
\label{eqn:03}
Q(s_{t}, a_{t}) = \mathbb{E}[\pi_{t} | s_{t}, a_{t}] + \delta \mathbb{E} \left[ \max_{a_{t+1} \in \mathcal{A}} Q(s_{t+1}, a_{t+1}) | s_{t}, a_{t} \right],
\end{equation}
where the first term represents the immediate reward, and the second term captures the discounted continuation value. The value function and Q-function are linked by $V(s) \equiv \max_{a \in \mathcal{A}} Q(s, a)$. With finite $\mathcal{S}$ and $\mathcal{A}$, the Q-function can be represented as an $|\mathcal{S}| \times |\mathcal{A}|$ matrix.

% Head 3
\subsection{Learning the Q-Matrix}

Q-learning aims to determine the optimal action for each state by estimating the Q-matrix, reflecting expected rewards for actions in different states. This process operates without prior knowledge of the underlying model, specifically $F(\pi_{t}, s_{t+1} | s_{t}, a_{t})$.

Q-learning algorithms employ an iterative approach to approximate the Q-matrix. Starting from an arbitrary initial matrix $Q_0$, the algorithm updates the corresponding cell $Q_t(s_{t}, a_{t})$ after observing reward $\pi_t$ and transition to state $s_{t+1}$ following action $a_t$ in state $s_t$:

\begin{equation}
\label{eqn:04}
Q_{t+1}(s_{t}, a_{t}) = (1 - \alpha) Q_t(s_{t}, a_{t}) + \alpha [\pi_t + \delta \max_{a_{t} \in \mathcal{A}} Q_t(s_{t+1}, a_{t})],
\end{equation}
where $\alpha \in [0, 1]$ is the learning rate, controlling the influence of new experience on the Q-value update. 

While \cite{watkins1992q} demonstrated the convergence of Q-learning to the optimal policy within an MDP for a single decision-maker, extending this guarantee to multi-agent scenarios is challenging due to non-stationarity. The interconnected reward structure and unpredictable actions of other agents introduce complexities. However, independent Q-learning, where agents learn without explicitly modeling opponents' strategies, has shown promise in such environments.\footnote{\cite{watkins1992q} revealed its potential to reach the optimal strategy within the confines of a Markov Decision Problem (MDP) for an individual decision maker. However, extending this certainty to multi-decision maker scenarios is problematic due to non-stationarity. decision makers must navigate a dynamic environment where the reward system is intertwined with the unpredictable actions of adversaries. Despite the absence of the Markov property, studies suggest that independent Q-learning can still yield positive outcomes in such complex environments. While algorithms that consider opponents’ strategies require detailed information about their tactics and behavior, an independent approach retains the uncomplicated, model-free essence of reinforcement learning.}


% Head 4
\subsection{Exploration Strategies}

\indent Effective learning necessitates exploring all possible state-action pairs to determine the most rewarding actions. The algorithm learns through trial and error, balancing the exploitation of existing knowledge with the exploration of new possibilities. While achieving the optimal balance is complex, Q-learning algorithms typically rely on predefined exploration parameters.

\indent The $\epsilon$-greedy policy is a common exploration strategy, selecting the best-known action with probability $1-\epsilon$ and choosing randomly among all actions with probability $\epsilon$. This approach balances exploiting known rewards with exploring potentially better alternatives.

\subsection{Beyond Single Decision Maker}

Although initially developed for single-agent MDPs, Q-learning has been extended to multi-agent systems. In these scenarios, agents learn simultaneously, facing the challenge of non-stationarity arising from the dynamic strategies of other agents. Despite these difficulties, independent Q-learning, where agents learn and adapt individually, often leads to effective outcomes in complex multi-agent environments.


 