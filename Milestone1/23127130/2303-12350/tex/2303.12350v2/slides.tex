\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\title{Artificial Intelligence and Dual Contract}
\author{Qian Qi}
\institute{Peking University\& HeptaAI}
\date{\today}
\begin{document}
\frame{\titlepage}
\section{Introduction}
\begin{frame}{Motivation}
\begin{itemize}
\item Rapid advancements in artificial intelligence (AI) raise concerns about potential misalignment between AI algorithms and stakeholder values.
\item This misalignment parallels the classic principal-agent problem in economics, highlighting the importance of incentive alignment in AI systems.
\item Studying AI-driven contract design is crucial for:
\begin{itemize}
\item Understanding the dynamics of online contracting in decentralized platforms.
\item Examining competitive dynamics in Web 3.0 applications using algorithmic tools.
\item Developing effective contracts that incentivize desired behavior in AI applications.
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{The AI Alignment Problem}
\begin{itemize}
\item AI algorithms, particularly those based on reinforcement learning, are designed to optimize specific reward functions.
\item Misalignment arises when these reward functions do not accurately reflect the broader values and goals of relevant stakeholders.
\item This discrepancy can lead to unintended consequences and potentially harmful outcomes.
\end{itemize}
\end{frame}
\begin{frame}{Challenges in Studying AI Alignment}
\begin{itemize}
\item \textbf{Empirical Challenges:}
\begin{itemize}
\item Difficulty in observing real-world AI systems and their impact due to data privacy and commercial sensitivity.
\item Opacity of contracts and agreements governing AI development and deployment.
\end{itemize}
\item \textbf{Theoretical Challenges:}
\begin{itemize}
\item Complexity of modeling strategic interactions between AI agents with learning capabilities.
\item Lack of analytical tools to predict the emergent behavior of AI systems in complex environments.
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{Our Approach}
\begin{itemize}
\item We employ an experimental approach using multi-agent reinforcement learning (MARL) to simulate contract design and negotiation in controlled environments.
\item MARL allows us to study the dynamics of AI agents learning and adapting their strategies in the presence of other agents.
\item We start with a classic single-principal-agent problem as a benchmark to establish a baseline understanding.
\item We then extend our analysis to a more complex dual-contract problem involving two principals and one agent to investigate the impact of multiple stakeholders.
\end{itemize}
\end{frame}
\section{Literature Review}
\begin{frame}{AI for Mechanism Design}
\begin{itemize}
\item Emerging field exploring the use of AI algorithms to design and optimize mechanisms for various applications, including:
\begin{itemize}
\item Auctions (e.g., \citeauthor{banchio2022artificial}, \citeyear{banchio2022artificial})
\item Contract negotiations
\item Resource allocation
\end{itemize}
\item Key techniques include:
\begin{itemize}
\item Reinforcement learning
\item Deep learning
\item Evolutionary algorithms
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning in Economics}
\begin{itemize}
\item Growing body of research applying reinforcement learning techniques to study economic phenomena, including:
\begin{itemize}
\item Learning in games (e.g., \citeauthor{erev1998predicting},
\item Market dynamics (e.g., \citeauthor{calvano2020artificial})
\item Equilibrium Selection (e.g., \citeauthor{kasy2021adaptive})
\end{itemize}
\item Q-learning, a model-free reinforcement learning algorithm, has been particularly popular in this domain.
\end{itemize}
\end{frame}

\begin{frame}{Dual-Contract Problem}
\begin{itemize}
\item Extends the classic principal-agent problem to a three-player setting with two principals and one agent.
\item Limited literature exploring the dual-contract problem, particularly in dynamic settings.
\item Our work contributes to this literature by:
\begin{itemize}
\item Developing a MARL algorithm to solve the dual-contract problem with incentive compatibility.
\item Investigating the impact of profit alignment and principal heterogeneity on contract outcomes.
\end{itemize}
\end{itemize}
\end{frame}
\section{Model}
\begin{frame}{Single Principal-Agent Model}
\begin{itemize}
\item Risk-neutral principal and agent.
\item Agent exerts hidden effort ($e_t$) that affects project outcomes.
\item Project revenue: $Revenue_t = I + (R-I)e_t$
\item Principal offers a contract with a tax rate ($p_t$) on agent's earnings.
\item Agent's payoff: $\Pi_t^{A} = (1 - p_t)(R - I)e_t - \frac{1}{2}ce_t^2$
\item Principal's payoff: $\Pi_t^{P} = I + (R-I)e_{t}p_t$
\item Q-learning algorithm used by both principal and agent to maximize their payoffs.
\end{itemize}
\end{frame}
\begin{frame}{Dual-Contract Model}
\begin{itemize}
\item Two principals ($P_1$, $P_2$) offer contracts to a single agent (A).
\item Agent allocates effort ($e_{1,t}$, $e_{2,t}$) between two projects, subject to $e_{1,t} + e_{2,t} \leq 1$.
\item Principals independently choose tax rates ($p_{1,t}$, $p_{2,t}$).
\item Agent's payoff:
\begin{align*}
\Pi^{A}{t} = &(1 - p{1,t})[I_1 + (R_1 - I_1)e_{1,t}] \
&+ (1 - p_{2,t})[I_2 + (R_2 - I_2)e_{2,t}] \
&- C(e_{1,t}, e_{2,t})
\end{align*}
\item Principal payoffs:
\begin{align*}
\Pi_t^{P_1} &= I_1 + (R_1 - I_1)e_{1,t}p_{1,t} \
\Pi_t^{P_2} &= I_2 + (R_2 - I_2)e_{2,t}p_{2,t}
\end{align*}
\item Profit alignment ($\gamma$) and principal heterogeneity ($\kappa$) parameters introduced to capture strategic interactions.
\end{itemize}
\end{frame}
\begin{frame}{Profit Alignment and Heterogeneity}
\begin{itemize}
\item \textbf{Profit Alignment ($\gamma$):}
\begin{itemize}
\item Represents the degree to which principals' profits are aligned.
\item $\gamma = 0$: Pure competition.
\item $\gamma = 0.5$: Pure collusion.
\end{itemize}
\item \textbf{Principal Heterogeneity ($\kappa$):}
\begin{itemize}
\item Reflects differences in the agent's cost of effort for each principal's project.
\item $\kappa = 0$: No heterogeneity.
\item $\kappa > 0$: Principal 1 has an advantage (lower effort cost for the agent).
\end{itemize}
\end{itemize}
\end{frame}
\section{Q-Learning Algorithm}
\begin{frame}{Q-Learning Algorithm}
\begin{itemize}
\item Each principal maintains a Q-table ($Q^{P_i}$) to estimate the expected profit for each state-action pair.
\item Agent also maintains a Q-table ($Q^A$) to estimate expected payoff.
\item Q-values are updated iteratively based on observed rewards and transitions:
\begin{align*}
Q_{t+1}(s_t, a_t) &= (1 - \alpha) Q_t(s_t, a_t) \
&+ \alpha [r_t + \delta \max_{a_{t+1}} Q_t(s_{t+1}, a_{t+1})]
\end{align*}
\item $\alpha$: Learning rate (how quickly agents update their beliefs)
\item $\delta$: Discount factor (importance of future rewards)
\end{itemize}
\end{frame}
\begin{frame}{Exploration-Exploitation Dilemma}
\begin{itemize}
\item Agents face a trade-off between:
\begin{itemize}
\item \textbf{Exploration:} Trying new actions to discover potentially better strategies.
\item \textbf{Exploitation:} Choosing actions that have yielded high rewards in the past.
\end{itemize}
\item $\epsilon$-greedy policy used to balance exploration and exploitation:
\begin{itemize}
\item With probability $\epsilon$, choose a random action (exploration).
\item With probability $1-\epsilon$, choose the action with the highest Q-value (exploitation).
\end{itemize}
\end{itemize}
\end{frame}
\section{Results}
\begin{frame}{Single Principal-Agent Results}
\begin{itemize}
\item Q-learning algorithm converges to a stable tax rate and effort level, representing an equilibrium outcome.
\item Learning rate ($\alpha$) and exploration rate ($\epsilon$) influence:
\begin{itemize}
\item Convergence speed: Higher values generally lead to faster convergence.
\item Profitability: Trade-off between exploration and exploitation affects final profits.
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{Single Principal-Agent Results (cont.)}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{results/single/subplots_combined.png}
\caption{Impact of Learning Rate and Exploration Rate on Q-learning Dynamics in a Dynamic Contract Setting.}
\label{fig:single-agent-results}
\end{figure}
\end{frame}
\begin{frame}{Dual-Contract Results: Profit Alignment}
\begin{itemize}
\item \textbf{Higher profit alignment ($\gamma$) leads to higher average profits for both principals.}
\item This suggests that even without explicit communication, principals learn to cooperate implicitly when their interests are aligned.
\item \textbf{Emergent cooperation:} Principals balance their self-interest with potential gains from coordinated action, resulting in higher tax rates and joint profit maximization.
\end{itemize}
\end{frame}
\begin{frame}{Dual-Contract Results: Profit Alignment (cont.)}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{results/dual/combined_heatmaps_group1_gamma_0_kappa_0_memory_1.png}
\caption{Impact of Profit Alignment on Principal Profits and Agent Effort ($\gamma = 0$)}
\end{figure}
\end{frame}
\begin{frame}{Dual-Contract Results: Profit Alignment (cont.)}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{results/dual/combined_heatmaps_group1_gamma_0.5_kappa_0_memory_1.png}
\caption{Impact of Profit Alignment on Principal Profits and Agent Effort ($\gamma = 0.5$)}
\end{figure}
\end{frame}
\begin{frame}{Dual-Contract Results: Principal Heterogeneity}
\begin{itemize}
\item \textbf{Principal heterogeneity ($\kappa > 0$) creates an advantage for the favored principal (Principal 1).}
\item This advantage stems from the agent's lower effort cost for Principal 1's project.
\item \textbf{Protection effect:} Principal 1 can sustain higher tax rates without losing the agent's effort, even under competitive pressure from Principal 2.
\end{itemize}
\end{frame}
\begin{frame}{Dual-Contract Results: Principal Heterogeneity (cont.)}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{results/dual/combined_heatmaps_group1_gamma_0.25_kappa_0_memory_1.png}
\caption{Impact of Principal Heterogeneity on Agent Effort Allocation ($\kappa = 0$)}
\end{figure}
\end{frame}
\begin{frame}{Dual-Contract Results: Principal Heterogeneity (cont.)}
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{results/dual/combined_heatmaps_group1_gamma_0.25_kappa_0.25_memory_1.png}
\caption{Impact of Principal Heterogeneity on Agent Effort Allocation ($\kappa = 0.25$)}
\end{figure}
\end{frame}
\section{Conclusion}
\begin{frame}{Conclusion}
\begin{itemize}
\item Our research demonstrates that MARL algorithms can effectively learn incentive-compatible contracts in both single- and dual-contract settings.
\item We find that profit alignment and principal heterogeneity significantly impact contract negotiation dynamics and outcomes.
\item Our findings highlight the potential of AI to automate and optimize contract design and negotiation processes.
\end{itemize}
\end{frame}
\begin{frame}{Future Work}
\begin{itemize}
\item Investigate the robustness of our findings with different MARL algorithms, learning parameters, and environmental settings.
\item Explore the impact of information asymmetry and incomplete contracts on AI-driven contract design.
\item Develop mechanisms to mitigate potential negative consequences of algorithmic collusion and ensure fairness in AI-mediated negotiations.
\item Apply our framework to real-world contract design problems in domains such as online platforms, supply chains, and employment contracts.
\end{itemize}
\end{frame}
\end{document}