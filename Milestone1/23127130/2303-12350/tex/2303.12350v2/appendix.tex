\section{Appendix}\label{appendix}

\subsection{\cite{innes1990limited}}
\begin{itemize}
\item Project requires initial investment $I$, which comes from principal.
\item agent exerts unobservable effort $e$ at cost $\frac{1}{2}ce^2$, where $c$ is an adjustment cost parameter.
\item With probability $e$, project generates payoff $X^H$.
\item With probability $1-e$, generate payoff $X^L<X^H$.
\item Contract pays principal $D^L$ if payoff is $X^L$ and $D^H$ if payoff is $X^H$.
\item Agent retains the residual.
\end{itemize}

For a given contract $(D^L,D^H)$, the agent maximizes
\begin{equation}
\label{eqn:05}
e(X^H-D^H)+(1-e)(X^L-D^L)-\frac{1}{2}ce^2,
\end{equation}
The first-order condition for $e$ gives the incentive-compatible (IC) constraint:
\begin{equation}
\label{eqn:06}
(X^H-D^H)+(X^L-D^L)=ce,
\end{equation}
The individual rationality (IR) constraint is that the principal must also break even, so we need
\begin{equation}
\label{eqn:07}
eD^H+(1-e)D^L=I,
\end{equation}
 Lagrangian for optimal contract
 \begin{align}
\label{eqn:08}
\mathcal{L}& = e(X^H-D^H)+(1-e)(X^L-D^L)-\frac{1}{2}ce^2\nonumber\\&+\lambda_1(e-\frac{(X^H-D^H)-(X^L-D^L)}{c}+\lambda_2(1-eD^H-(1-e)D^L),
\end{align}
Derivative wrt $D^L$
\begin{equation}
\label{eqn:09}
\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^L}=-(1-e)-\frac{\lambda_1}{c}-\lambda_2(1-e),
\end{equation}
Derivative wrt $D^H$
\begin{equation}
\label{eqn:10}
\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^H}=-e+\frac{\lambda_1}{c}-e\lambda_2=-\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^L}-(1+\lambda_2),
\end{equation}
\paragraph{Claim}
Optimal to set $D^L=X^L$.
\paragraph{Proof by contradiction}
Suppose optimal $D^L<X^L$. Then it must be the case that $\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^L}=0$.
\begin{itemize}
\item If it were not, we would increase $D^L$.
\item But then we will have $\frac{\mathrm{d}\mathcal{L}}{\mathrm{d}D^H}<0$, so we will want to set $D^H=0$.
\item But then we will induce negative effort.
\item Instead, set $D^L=X^L$ and $X^H>D^H>I$.
\end{itemize}


\section{Algorithms}


\begin{algorithm}[H]
\caption{Principal Algorithm (One Iteration)}
\begin{algorithmic}[1]
\Require $q\_table\_p1$, $q\_table\_p2$, $tax\_rate\_history\_p1$, $tax\_rate\_history\_p2$, $epsilon$, $alpha$, $memory\_length$, $gamma$, $kappa$
\State $state\_p1 \gets$ state\_to\_index($tax\_rate\_history\_p1$, $tax\_rate\_history\_p2$, $memory\_length$)
\State $state\_p2 \gets$ state\_to\_index($tax\_rate\_history\_p2$, $tax\_rate\_history\_p1$, $memory\_length$)
\State $action\_p1 \gets$ choose\_action($q\_table\_p1$, $state\_p1$, $epsilon$)
\State $tax\_rate\_p1 \gets TAX\_RATES[action\_p1]$
\State $action\_p2 \gets$ choose\_action($q\_table\_p2$, $state\_p2$, $epsilon$)
\State $tax\_rate\_p2 \gets TAX\_RATES[action\_p2]$
\State $effort\_p1$, $effort\_p2 \gets$ calculate\_effort($tax\_rate\_p1$, $tax\_rate\_p2$, $kappa$)
\State $profit\_p1$, $profit\_p2$, $profit\_a \gets$ calculate\_profit($tax\_rate\_p1$, $tax\_rate\_p2$, $effort\_p1$, $effort\_p2$)
\State $q\_table\_p1 \gets$ update\_q\_table($q\_table\_p1$, $state\_p1$, $action\_p1$, $profit\_p1$, $alpha$)
\State $q\_table\_p2 \gets$ update\_q\_table($q\_table\_p2$, $state\_p2$, $action\_p2$, $profit\_p2$, $alpha$)
\State Update $tax\_rate\_history\_p1$, $tax\_rate\_history\_p2$, and $epsilon$.
\State \Return $q\_table\_p1$, $q\_table\_p2$, $tax\_rate\_history\_p1$, $tax\_rate\_history\_p2$, $epsilon$, $profit\_p1$, $profit\_p2$, $profit\_a$, $effort\_p1$, $effort\_p2$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Agent Algorithm}
\begin{algorithmic}[1]
\Require $tax\_rate\_p1$, $tax\_rate\_p2$, $kappa$
\Function{calculate\_effort}{$tax\_rate\_p1$, $tax\_rate\_p2$, $kappa$}
    \State $e_1$, $e_2 \gets$  optimize.minimize(profit function, initial guess, bounds)
    \State \Return $e_1$, $e_2$
\EndFunction
\Function{calculate\_profit}{$tax\_rate\_p1$, $tax\_rate\_p2$, $effort\_p1$, $effort\_p2$}
    \State $profit\_p1 \gets I_1 + (R_1 - I_1) * effort\_p1 * tax\_rate\_p1$
    \State $profit\_p2 \gets I_2 + (R_2 - I_2) * effort\_p2 * tax\_rate\_p2$
    \State $profit\_a \gets (1 - tax\_rate\_p1) * (I_1 + (R_1 - I_1) * effort\_p1) + (1 - tax\_rate\_p2) * (I_2 + (R_2 - I_2) * effort\_p2) - 0.5 * C * (effort\_p1 + effort\_p2)^2$
    \State \Return $profit\_p1$, $profit\_p2$, $profit\_a$
\EndFunction
\end{algorithmic}
\end{algorithm}

