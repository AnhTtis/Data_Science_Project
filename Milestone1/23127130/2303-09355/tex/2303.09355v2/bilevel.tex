% interactnlmsample.tex
% v1.05 - August 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required
%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty
\makeatletter% @ becomes a letter
\def\NAT@def@citea{\def\@citea{\NAT@separator}}% Suppress spaces between citations using natbib.sty
\makeatother% @ becomes a symbol again

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\usepackage{verbatim}
%\usepackage{array}
%\usepackage{bm, fixmath, texdraw}
%\usepackage{epsfig}
%\usepackage{multirow}
%\usepackage{subfigure}
%\usepackage{algorithm}
%\usepackage{algorithmic}

% Packages added by me
%\usepackage{hyperref}
%\usepackage[backref=page]{hyperref} % This is better but they will probably complain.

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\begin{document}

\articletype{FULL PAPER}% Specify the article type or omit as appropriate

% A typical Full Paper for this journal should be no more than 6000 words; this limit does not include tables; figure captions; biographical note.

\title{Bidirectional Learning for Multi-step Prediction With Affordances}
\title{Learning bi-directional affordance prediction and planning with affordances via partial effect predictions}

\author{
\name{Utku Bozdogan\textsuperscript{a}\thanks{CONTACT Utku Bozdogan Email: utku-bozdogan@hotmail.com} and Emre Ugur\textsuperscript{b}}
\affil{\textsuperscript{a,b}Department of Computer Engineering, Bogazici University, Istanbul, Turkiye}
}

\maketitle

\begin{abstract} %200 word limit on abstract for full papers
%\\\resizebox{25pc}{!}{\includegraphics{abstract.eps}} A visual abstract is also required.
%This is a single, concise, pictorial and visual summary of the main findings of the article. It could either be the concluding figure from the article or better still a figure that is specially designed for the purpose, which captures the content of the article for readers at a single glance.
Affordances are action possibilities of an object, directly perceived by an actor based on their capabilities, learned from goal-free exploration through observing the effects of their actions on objects in an environment. The actor can then use the learned affordances to make plans to reach a goal since they now know which actions on a certain object are possible and which one results in the desired effect. This information can be utilized in robotics as well for goal-directed planning, either directly or with the aim of reducing the search space for possible solutions. In this work, the problem of making multi-step predictions for object manipulation is investigated in the continuous domain. A model which can be conditioned unidirectionally and bidirectionally between actions and effects, and a planner are used and the capacity to chain together a correct sequence of actions for an object to reach desired goal positions is achieved. The model is verified in experiments, generating and executing reasonable plans efficiently. Setting it apart from previous work, using continuous effect and actions enables the planner to also find solutions to configurations not seen in training where actions are not applied to their full extent, but only partially.
\end{abstract}

\begin{keywords}
Affordances, effect prediction, object motion trajectory prediction, multi-step planning
\end{keywords}


\section{Introduction}

The concept of affordances was introduced by J.J. Gibson to explain how inherent `values' and
`meanings' of things in the environment can be perceived, and that how this information
can be linked to the action possibilities offered to the organism by the environment \cite{gibson1986ecological}. Evidence has been reported in the fields of psychology and neurophysiology supporting the principle of affordances. Gibson put forward the groundwork, however since Gibson, a number of explanations have been proposed \cite{heft1989affordances, chemero2003outline} and there are also multiple interpretations of qualities of affordances such as how affordances should be represented, or whether affordances are properties of the environment, the agent, or a relation between the two. 
%A representation is shown in Figure \ref{fig:affordances}. 
Roboticists also have their own interpretations \cite{csahin2007afford}, sometimes adding to such as an action being afforded regardless of observing an expected effect, and sometimes conflicting with the orthodox definition such as using affordances with internal representations. There are developmental approaches among roboticists where the robot’s possible interactions with the environment are represented with an affordance model, where the robot learns from its interactions. In time, the robot is shown to learn certain affordances, and also there exist works where the learned affordances are used to learn higher-level, more complex affordances, which also agrees with the principle of affordances. 

Thinking from a human standpoint; mature humans are well capable of understanding how to act on and manipulate objects according to their goals, even if the objects or goals are novel to them and they are also good at predicting the outcomes of their actions (effects). Humans acquire this capacity through learning over time. Babies have certain reflexes (e.g. sucking, Palmar reflex) from birth which they use for gaining experience, but in time the reflexes disappear and intentional actions take their place, which are more complex, such as manipulating objects after reaching them, enabling further more complex discoveries \cite{piaget2008psychology}.

From a robotics standpoint, learning to predict the effects of a robot's actions beforehand would be a beneficial skill for robots, since this can prevent potential failures and dangerous situations to the robot and those around it, and enable planning for achieving certain goals.
%Challenges
Planning for multi-step tasks in the real world is difficult, and a generalized approach to solving this problem is even more so. Due to its difficulty, previous works compromise on certain aspects, such as predefining the actions, effects, and/or object categories, simplifying the task. Discretizing information in a continuous domain with the purpose of being able to make high-level planning with symbols or in an attempt to maintain a generalized approach to planning for single-step tasks, or making unidirectional predictions results in learning from incomplete or lossy information, leading to inaccuracies. 

In robotics, learned affordances have been used to choose objects for manipulation, to make discrete and continuous effect predictions given objects and actions, to make  plans to achieve goals. However, to the best of our knowledge, there is no single framework that can predict effects given objects and action, predict required actions to achieve desired goals, can predict the movement trajectory of the objects in response to parametric actions and can make plans composed of sequence of actions, including partial action executions, in order to achieve given goals. The contribution of our work is as follows:
\begin{itemize}
    \item Bi-direction prediction of affordances: Considering affordances as relations between objects, actions and effects, our neural network based architecture can make predictions in different directions. In other words, our system can predict effects given objects and actions and also predict the required actions to obtain desired effects given objects.
    \item Accurate continuous effect prediction: Our system can predict the motion trajectories of the objects expected to be generated by parametric robot actions. We showed that the high prediction performance compared to a strong baseline that also make affordance-based continuous effect prediction.
    \item Multi-step planning with partial actions: We exploited the outcome prediction capability of our system for partial actions in order to form multi-step plans that may include full or partial action executions. 
    
\end{itemize}

%The aim of this paper is to learn a deep model in the continuous domain, without constraints limiting generalization, which takes into account the affordances of objects, that can then be used to:
%\begin{enumerate}
%  \item predict the actions and effects from a given setting, provided only with minimal observation,
%  \item generate and execute multi-step plans efficiently.
%\end{enumerate}

%Our work approaches the problem by trying to remain coherent with the theory and puts emphasis on bidirectional prediction learning because this could be how infants learn to associate actions and effects \cite{elsner2001effect}. The contributions of this paper is as follows:

%In humans, visual processing occurs in two streams in parallel. Processing related to object recognition takes place in the ventral stream and it takes longer than the processing in the dorsal stream which is related to spatial information and certain object properties relevant to action generation \cite{goodale1992separate}. Many works in the field make use of some form of an image processing component to provide the robot the ability to make sense and make use of its surroundings. 

%\

%In this work, deep affordances are utilized for continuous object manipulation tasks, learned without object recognition but by using encoded depth images of the object, for depth information is shown to be relevant to affordance perception \cite{griffith2009toward}. The robot's own interaction experience is utilized with several objects and action types to learn from using relative representations for action and effect information. The aim is to use a conditional model which is shown to be superior in modeling temporal relationships to gain long-horizon predictive capabilities, pairing this with bidirectional prediction for a more grounded representation to create and execute multi-step plans efficiently and accurately.

\section{Related work}

% Deep Effect Trajectory Prediction in Robot Manipulation
% Bayesian Networks
% Discrete Effect Prediction
\paragraph*{Early affordances work on effect predictions}
In early works such as \cite{fitzpatrick2003learning}, objects were required to be recognized first, and object-specific affordances were learned from robot's interaction experience with those objects, therefore the learned affordances could not be generalized to different/novel objects.  \cite{ugur2007learning} studies learning of traversability affordance where the LIDAR input was directly processed without any object detection step. Therefore, `direct perception' aspect of affordance perception was realized in that work, however only a single pre-defined affordance category was addressed. Effect categories and affordances were discovered by the robot in \cite{ugur2011unsupervised,ugur2011going} via unsupervised clustering techniques. However, unsupervised clustering results depend on the particular clustering method and the feature space that might be composed of values with different metrics such as distance, frequency, angle, etc. In \cite{ugur2011goal}, this is taken one step further and hierarchical clustering is made over channels for better effect category discovery. Both \cite{ugur2011going} and \cite{ugur2011goal} also enable forward chaining for multi-step planning. \cite{Szedmak2014} propagated affordance predictions by exploiting similarities among object properties, action parameters and resulting effects using Maximum Margin Multi-Valued Regression (MMMVR), obtaining efficient affordance learning; however affordance categories were also pre-defined in that study. In \cite{ugur2014bootstrapping},  complex affordance learning was bootstrapped through using pre-learned basic-affordances as additional inputs of the complex affordance predictors or as cues in selecting the next objects to explore during learning.
In \cite{kroemer2012kernel} the objects were represented via point clouds, in a non-parametric manner, to provide direct perception and the grasping and a single pouring action were shown to generalize well on novel objects but result in single-step plan only. Bayesian Networks were used \cite{montesano2007modeling, montesano2008learning}, enabling bidirectional predictive capabilities using the robot's own interaction experience, but clustering is performed on object features and effects in a predefined manner. In this paper, we also study acquiring bi-directional prediction capabilities. Different from previous work, we do not find effect clusters or do not only aim predicting the final effect. Instead, our system aims to learn predicting the complete effect/action trajectory during action execution. 

\paragraph*{Learning affordances for planning}
\cite{ugur2015bottom} learned affordances for symbolic planning, by again clustering effect categories and using object categories as a collection of effect categories obtainable by actions available to the robot. This enables the representation of nonlinear relations in planning, however, their discrete representation while making planning easy, makes the estimations approximate, increasing long-horizon planning errors. This work was validated on a real-world setup with simple actions such as poke, grasp, and release, and also with a more complex action which is stack. The experience of the robot enabled it to gain experience from the simple actions and after experiencing the stack action enabled it to generate a valid plan for the stacking action. This framework was extended in \cite{ugur2015refining} by enabling the robot to progressively update the previously learned concepts and rules in order to better deal with novel situations that appear during multi-step action executions. Similar planning capabilities were obtained by the robot using deep encode-decoder neural networks with binary bottleneck layers \cite{ahmetoglujair} and with multi-head attention mechanism \cite{ahmetogluhum} by directly processing the pixel images instead of using hand-coded features.
In \cite{ames2018learning}, probabilistic planning with symbols using parameterized actions was applied to a real robot task, showing that continuous tasks can be performed with discrete planners using parameterized behaviours. Different from the previous work where only final outcomes were used for planning, our system can exploit intermediate effects expected to be observed for example in the middle of the action execution for planning.

\paragraph*{Learning visual grasp affordances}
Using RGB/RGBD images for predicting affordance classes or pixel-wise affordance labels for object manipulation has become popular in the recent years \cite{nguyen2016detecting, do2018affordancenet, mi2019object, hamalainen2019affordance, chu2019toward, thermos2021joint} and was shown to be a feasible approach for learning how to grasp different objects. Similarly, \cite{zhang2022inpaint2learn} are able to learn the affordances of objects such that they can place objects/humans in correct poses in a scene and also choose the correct object type to place in a given scene. \cite{ruiz2020geometric} uses point clouds to learn general geometric features from object interactions, enabling them to place objects in a scene correctly. In \cite{khazatsky2021can}, a generative model learned from interaction images was used to propose potential affordances. The aim was to learn a generalizable prior from interaction data and then utilize it to propose reasonable goals for unseen objects. These goals were then attempted to be executed by an offline RL policy, learned from interaction data, and tuned online efficiently to adapt to unseen objects. However, continous effect prediction and multi-step planning aspects were not  addressed in these studies.

%Integrating multi-modal information was shown to be a successful approach for learning tasks in general \cite{aytar2017see, arandjelovic2018objects} due to learning how to complement missing modalities in a given input by the information provided by the present modalities. The same approach was also shown to be effective for robotic tasks using sounds, language, or tactile information alongside joints \cite{mi2019object, noda2014multimodal, yamada2018paired, mori2020tactile, saito2020wiping, saito2021select}. \cite{allevato2020learning} was also interesting in that it uses bidirectional prediction and multimodal information together to ground affordances with human-provided language labels. 

\paragraph*{Affordances for efficient learning}
Affordances can also be used to reduce the search space in order to efficiently generate plans to solve long-horizon tasks. Recent approaches utilizing this idea extended the definition of affordances to represent not only single-step action-effect knowledge but as action feasibilities \cite{xu2020deep}, or intents which are similar to goals \cite{khetarpal2020can} in order to make affordances useful for multi-step plans. However, the feasibility concept of \cite{xu2020deep} accepts a grasp action achieving nothing as afforded, and would also accept a grasp action as afforded regardless of whether it was an appropriate grasp for an object. While \cite{khetarpal2020can} overcame this with intent representation, intents were specified a priori in their work and although a sub-goal discovery direction was proposed for learning, it was not explored.

\paragraph*{Learning continuous effect trajectories}
Similar to our work, \cite{seker2019deep} and \cite{tekden2020} learned to predict the full motion trajectory of an object, using the robot's own interaction experience, and top-down images of the objects. These studies are important for its inclusion of the temporal aspect of effects. The utility of different features was also investigated, such as hand-crafted shape features, CNN extracted features, or support point features extracted from a neural network. The authors used these features and the interaction experience to train a recurrent neural networks and were able to accurately predict trajectories resulting from a lever-up action in a real-world setting with multiple objects. A common shortcoming of the aforementioned methods which make predictions for action or effects is related to the use of recurrent methods for long-horizon tasks. The use of recurrent networks such as LSTMs \cite{hochreiter1997long} or GRUs \cite{cho2014learning} is shown to be effective for short-term  predictions. However, their recurrent structure causes any error in their prediction to accumulate over time, causing lower success rates in executing long-horizon tasks \cite{pekmezci2021learning}. In this paper, we compare the effect prediction capabilities of recurrent neural network based systems and our conditional neural process based system.





\section{Proposed Method}

\begin{comment}
\subsection{Background}
\subsubsection{Conditional Neural Processes}

Conditional Neural Processes (CNP) \cite{garnelo2018conditional} is an approach bringing together the inference potential of Gaussian Processes and the training of neural networks with gradient descent by learning a prior from data. This allows CNPs to remain scalable, and from a distribution conditioned with observations be able to make accurate predictions for desired targets. A neural network $h$ is used to encode varying numbers of sampled observations $O$ into fixed-sized representations $r_{i}$. The order of observations can vary, in accordance with stochastic processes, and they are aggregated via a commutative operation into a single vector $r$ which represents the prior knowledge available. This information is used for conditioning, and another neural network $g$ decodes $r$ and for targets $x_{j} \in T$ to generate predictions which are Gaussian distribution parameters. The formulation is as follows:

\begin{equation}
    r_{i} = h_{\theta}(x_{i},y_{i}), \quad \forall (x_{i}, y_{i}) \in O
    \label{eq:cnp1}
\end{equation}
\begin{equation}
    r = r_{1} \oplus r_{2} \oplus r_{3} \oplus ... \oplus r_{i}
    \label{eq:cnp2}
\end{equation}
\begin{equation}
    \phi_{j} = g_{\delta}(x_{j}, r), \quad \forall x_{j} \in T
    \label{eq:cnp3}
\end{equation}
\begin{equation}
    \phi_{j} = (\mu_{j}, \sigma^{2}_{j})
    \label{eq:cnp4}
\end{equation}
where $\oplus$ is a commutative operation, like summation or dot product. \\

The model after training is efficient, achieving an inference time complexity of $O(i+j)$ for predicting $j$ targets from $i$ observations. Please see \cite{garnelo2018conditional} for more details.

\subsubsection{Conditional Neural Movement Primitives}

Conditional Neural Movement Primitives (CNMP)\cite{seker2019conditional} is an extension of CNPs, designed to work with temporal relations ($t$) and external task parameters collectively named as ($\gamma$). An observation is denoted as $(t_{i}, \gamma)$ instead of $x_{i}$, and $y_{i}$ are replaced by $SM(t_{i})$ which correspond to sensorimotor data at time $t_{i}$, which can be multi-dimensional. Averaging operation is chosen for aggregating $r$, and concatenation is used to incorporate $(t_{i}, \gamma)$. A trajectory and a number of observation points are selected randomly from the training dataset, and target data is predicted using the observations during training. After training the model can be conditioned with any number of observations and predict single or multiple targets. On a real-world task, as shown by the authors in their experiments, the network can be conditioned to current observations, making it robust to changes in the environment.

The encoder and decoder networks are trained jointly, with the loss function:

\begin{equation}
    \mathcal{L}(\theta, \delta) = - log P(y_{j} | \mu_{j}, softmax(\sigma_{j}))
 \label{eq:cnmp_loss}
\end{equation}

\subsubsection{Deep Modality Blending Networks}

Deep Modality Blending Networks \cite{seker2022imitation} is a further augmentation over CNMPs, specifically tailored to incorporate multimodality better in the system. This is achieved by aside from encoding information coming from different modalities into a common latent space, using a weighted average of encoded modality representations, facilitating information sharing, and also providing a regularization effect on the representations learned, similar to dropout \cite{srivastava2014dropout}. Each modality is encoded separately by its own encoder, and the latent representations are subjected to a weighted averaging operation. This merged representation is then decoded by separate decoders, each corresponding to a different modality. After training, the authors show that the latent representations belonging to different modalities, initially encoded in different parts of the latent space, overlap; which enables the prediction of missing modalities.

\end{comment}


\begingroup
\begin{figure}[t]
    \centering
		\includegraphics[width=\columnwidth]{raw-figures/Figure 1.jpg}
		\caption{An overview of the proposed model. Given object image and action or effect information at any time-point, our system can generate the effect trajectory and action execution trajectory.}
		\label{fig:model_figure}
\end{figure}
\endgroup


% DMBN'den architecture olarak farkı yok aslında, external parameter olarak depth image ekledik, effect için kullanılan displacement from starting position ve action için kullanılan relative position to object farklı modality değil, ancak problem domaini farklı.

\subsection{General Architecture}


We propose a framework that can learn (i) predicting the effect trajectory given initial object image and action execution trajectory and (ii) finding the required action execution trajectory to achieve a desired effect on a given object. Our system is built on top of Conditional Neural Processes (CNPs) \cite{garnelo2018conditional} that bring together the inference potential of Gaussian Processes and the training of neural networks with gradient descent by learning a prior from data.  As we would like to predict both actions and effect from objects and (possibly missing) actions and effects, we propose a neural network structure that takes the object image as input together with action and/or effect values at different time-points. In other words, given initial object image, our network can be conditioned with action and/or effect values at different time-points in order to generate action and effect values for all time points during action execution. The general structure of the proposed system is shown in Fig.~\ref{fig:model_figure}. As shown the system is composed of two encoders that process action and effect information at different time-points, an image encoder that process the depth image of the object, averaging operations to acquire a common object-action-effect representation (r), which in turn can be used to predict action and effect values at other time-points using the corresponding two decoders.

In detail, inspired from Deep Modality Blending Networks \cite{seker2022imitation}, our system encodes information coming from different channels (object, action and effect) into a common latent space, using a weighted average of encoded modality representations, facilitating information sharing, and also providing a regularization effect on the representations learned, similar to dropout \cite{srivastava2014dropout}. Each channel is encoded separately by its own encoder, and the latent representations are subjected to a weighted averaging operation:

\begin{equation}
    r_{i} = \sum h_{a}(a_{t_{obs_{k}}}) * w_{1} +  h_{e}(e_{t_{obs{k}}}) * w_{2}
    \label{eq:affordx1}
\end{equation}

where $w_{1} + w_{2} = 1$, and

\begin{equation}
    r = r_{1} \oplus r_{2} \oplus r_{3} \oplus ... \oplus r_{i}
    \label{eq:cnp2}
\end{equation}

where the commutative operation used is averaging. Then, depth image features and target time-step are concatenated to form the merged representation $r_{mrg} = \{r,f(\gamma_{o}),t_{target}\}$.

This merged representation is then decoded by separate decoders, each corresponding to a different channel. This merged representation is decoded at the action decoder by
\begin{equation}
    g_{a}(r_{mrg}) = (\mu_{a_{t_{target}}}, \sigma_{a_{t_{target}}}),
\end{equation}
and the effect decoder by
\begin{equation}
    g_{e}(r_{mrg}) = (\mu_{e_{t_{target}}}, \sigma_{e_{t_{target}}}),
\end{equation}
to yield predictions for action and/or effect for the target time-step shown in Fig \ref{fig:model_figure}. 

The latent representation (r) can be viewed as the shared affordance representation for actions $a$ and effects $e$ for different objects $o$. Learned affordances can then be used to predict the effects of actions or the required action to generate target effects. This model can be used to create multi-step action plans to achieve goals beyond single action executions by chaining the predictions.

% Bu kısımda action ve effectin nasıl olduklarını açıklıyorum, experiments kısmında tekrar açıklamalı mıyım?

\subsection{Training}
In our implementation, the system is object-centric. An action is defined in terms of the distance of the robot's end-effector to the object. An effect is defined as the displacement of the object from its starting position throughout an action. A depth image of the object is included as an external parameter $\gamma$ for the action. $t_{i}$ is the $i$th time-step of an interaction trajectory from the dataset $D$.

\begin{equation}
    D_{d} = (\{a_{t},e_{t},o,t\}_{t=0}^{t=1})_{d}
\end{equation}
is an interaction trajectory where $1 \leq d \leq m$ and  $m$ is the number of trajectories in the data set $D$.  $0 \leq t \leq 1$ is a phase variable  in control of the passage of time where $t \in \mathbf{R}$.


At each training iteration, $k$ observations are sampled uniformly at random from a randomly selected interaction trajectory $D_{d}$ where $1 \leq k \leq obs_{max}$, $k \in \mathbf{N}$ is also sampled uniformly at random and $obs_{max}$ is a hyper-parameter denoting the maximum number of observations that the model is allowed to use during one iteration. These observations are then encoded and aggregated. A cropped object depth image is encoded separately on a CNN encoder network, and the resulting vector is concatenated at the end of this aggregated representation. Before a prediction can be made, a target time-step is also concatenated after the image features. Finally, this merged representation is decoded to yield predictions for action and/or effect for the target time-step shown in Fig \ref{fig:model_figure}. Gradient descent is used with the loss function (\ref{eq:cnmp_loss}) with Adam optimizer \cite{kingma2014adam}.

\begin{equation}
    \mathcal{L}(\theta, \delta) = - log P(y_{j} | \mu_{j}, softmax(\sigma_{j}))
 \label{eq:cnmp_loss}
\end{equation}

After training, the network is able to predict the entire interaction trajectory given a single observation at $t=0$. An A* planner \cite{Hart1968} is then used on top of the network to solve tasks requiring multiple actions and steps.

\subsection{Actions}


Our arm-hand robot is equipped with two actions, namely push and grasp. The parametric push action is specified by an angle $\theta \in [0,2\pi]$; a push distance $l = 0.05$ and a $radius = 0.2$ both in meters. The gripper starts the push execution from the red circumference of a circle of radius ($radius=20cm$) centered around the object (see Figure~\ref{fig:scene_push_params}), at an angle $\theta$ and pushes the object $l$ meters. 
%Push actions consist of the gripper following a trajectory starting on a circle with a radius of 20 cm. and at an angle $\theta$ parallel to the table, with the goal position 5 cm. behind the object's center of mass. 
Larger sized objects may be displaced more as a result of this setup. The model is expected to learn the rollability and pushability affordances from these interactions and based on the object shape be able to predict the trajectories of  rollable objects such as spheres or lying cylinders and non-rollable objects such as a cuboids or an upright cylinders.

Grasp actions, on the other hand, are realized by lowering the open gripper to grasp position, attempting to grasp an object by closing the gripper and lifting the gripper up. Our model is expected to learn the graspability affordance from these interactions. Based on object size and shape, it should also be able to predict the interaction trajectories.

\begingroup
\begin{figure}[t]
    \centering
		\includegraphics[width=0.95\columnwidth]{raw-figures/Figure 2.jpg}
		\caption{Scene showing the parameters of a push action around an object.}
		\label{fig:scene_push_params}
\end{figure}
\endgroup

\subsection{Planning with partial action executions}

The A* planner (with Euclidean distance to the goal heuristic) is used to generate a sequence of actions to move the object from its initial position to a goal position. Each branch in the search tree corresponds to either a push action or a grasp action. As grasp action is not parameterized, there is a single branch for grasp action. On other hand, as push action might be applied from different approach angles and for different push distances, the range of possible approach angles and push distances are discretized and used to create multiple branches from the same node in the search tree. 

The planner uses predictions of actions with different parameters to update the predicted location of the object. The search is completed if the difference between the predicted and goal object position is less than 2 cm. Our model is able to work with continuous inputs, however, the planner can only propose a finite amount of actions due to the mentioned discretization design choice. 

Interactions with single and multiple actions are generated, which also include partial actions. Partial actions are when an action is started to be applied, however it is not completed, i.e. it is executed partially. For example in a push action, the push may be cut short before the gripper even contacts the object, or the gripper reaches the center of mass of the object, meaning that the object has already started being pushed, but the push is not completed yet. Importantly, our model is trained only with full action interactions. Yet, our model can generate the effect at any desired time-point, and therefore it can predict the consequence of partial action executions. Planner can use effects of such partial action executions to generate plans with finer resolutions (compared to plans that can only include full action executions).

Note that an action can be applied only if the object is reachable by the robot. Our model is expected to learn the reachability affordance from its interaction experience and actions are only considered during planning if the object is reachable. 





% Argument for reachability reducing plan search space

\section{Experiments results}

\subsection{Environment and Actions}
A simulated scene was constructed in CoppeliaSim \cite{coppeliaSim} as shown in Figure~\ref{fig:scene_push_params}. A UR10 robot interacts with objects of different shapes and sizes by applying push and grasp actions on a tabletop setting. A Kinect sensor is placed above the table vertically such that the entire table is visible. The parts of the interaction where the object is potentially going to be displaced are recorded. The recorded information consists of action and effect data from once every 3 simulation steps (a single step is 50ms), which is chosen empirically, and a single depth image of the table with the object on top taken at the beginning of each interaction. The simulation dataset is split into training (\%80), validation (\%10) and test (\%10) sets.


\begingroup
\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=0.8\columnwidth]{raw-figures/Figure 3.jpg}
		\caption{Example lever-up action in the simulator, reused with permission from \cite{seker2019deep}.}
		\label{fig:lever_up}
	\end{center}
\end{figure}
\endgroup

\subsection{Effect prediction performance}

In this paper, we propose a novel system to predict effects given objects and actions. Furthermore, our system can generate complete motion trajectories of objects as effects rather than their final positions. In order to assess the performance of our system, we compare our system with a recent study that can also predict motion trajectories of objects using CNN and Long short Term Memory (LSTM) model \cite{seker2019deep}. We used the same dataset that \cite{seker2019deep} used where lever-up actions were applied to objects with different geometric shapes from different points. An example lever-up action is shown in Figure~\ref{fig:lever_up}. The objects had different number of edges (between 3 and 8), sizes and orientations. The objects were levered-up from different contact points. The authors translated and rotated the top-down 128x128 grayscale image according to the contact point and lever-up direction in order to simplify the prediction problem. The dataset was separated randomly into 80\% training, 10\% validation, and 10\% test as in \cite{seker2019deep}. 

%The original work used two different object representations: the top-down images or the so-called support points. We also trained and tested our model with these two alternative representations and compared the results with the original work.
%The network was trained with top-down images, along with random numbers of observations from the trajectory, and the center of mass pose of a target time is predicted. Our model was tested with either the top-down image or with the support points, was able to predict full effect trajectories.
%In the first experiment, support points are used, and added as external parameters to the CNMP network; and in the second experiment, the top-down images are used in the same manner. 

The experiments were performed with 5-fold cross-validation and early stopping with one million iterations. For comparison, we gathered n-step predictions, taking 15 previous steps as observations, and compared the results with the ones reported in the same manner in \cite{seker2019deep}. As shown in Figure~\ref{fig:lstm_n_step}, the error between the predicted and actual positions of the objects in \cite{seker2019deep} varies between $0.90-1.00$cm, whereas the error was in the range of $0.3-0.4$cm in our system (Table \ref{table:cnmp_n_step_errors}). This comparison shows our model yields significantly lower error rates compared to a recurrent method, since the output is predicted directly avoiding error accumulation for multi-step predictions.

\begingroup
\begin{figure}[t]
    \centering
		\includegraphics[width=0.8\columnwidth]{raw-figures/Figure 4.jpg}
		\caption{N-step error plots obtained in the LSTM approach, reused with permission \cite{seker2019deep}.}
		\label{fig:lstm_n_step}
\end{figure}
\endgroup


\begin{table}[thbp]
\begin{center}
\caption{N-step prediction errors obtained in our model.}
{\begin{tabular}{|c|c|} \hline

\textbf{1-step (cm)} &  $0.340 \pm 0.044$ \\\hline
\textbf{2-step (cm)} & $0.352 \pm 0.046$ \\\hline
\textbf{3-step (cm)} &  $0.364 \pm 0.049$ \\\hline
\textbf{4-step (cm)} &  $0.375 \pm 0.051$ \\\hline
\textbf{5-step (cm)} &  $0.387 \pm 0.054$ \\\hline
\end{tabular}}
\label{table:cnmp_n_step_errors}
\end{center}
\end{table}




\subsection{Single-action push and grasp effect prediction}

% TODO add grasp results
% TODO add former content

Different models were trained for push and grasp actions separately. The simulation data sets were always split between 80\% training, 10\% validation, and 10\% test data. For all the results reported in this work 10-fold cross-validation was applied unless otherwise specified. The models were trained for one million iterations, without batches due to variable length of inputs and early stopping was employed. The learning rate was set to $1\mathrm{e}{-4}$. All errors reported in meters denote the distances to a specified goal position. The predictions for a single action is fixed to take 25 time steps. 

For the push action, a data set made up of 500 trajectories was used. For each interaction, objects were chosen randomly and placed at the center of the table. An angle for the push $\theta \in [0,2\pi]$ was chosen randomly. The robot performed a complete push action and the resulting interaction data was recorded. The error in predicting the final position of the object was found to be around $0.02$m as shown in Figure~\ref{fig:push_action_single+partial+multi}. Similar to our analysis in the previous subsection, our system was shown to be effective in predicting object motion trajectories in push-like actions.

%In models where this push action is the single action, the object has a fixed starting location at the center, and the z-axis coordinates are redundant since no significant changes occur in z-axis coordinates for the objects during an interaction. The recorded part of the interaction has a short duration relatively, so the interaction ends before a rollable object falls off the table in this case. In models where only grasp action is learned, z-axis information is used as well.

For the grasp action, a data set made up of 100 trajectories was used. Objects of varying sizes were randomly chosen to be placed at the center of the table. The robot then performed the grasp action and the resulting interaction data was recorded. The results for the grasp action are provided in Table \ref{table:grasp_solo_single_errors}. In interpreting the performance and success of graspability prediction, if the change in height is larger than 0.1 meters, the grasp was assumed to be a success. If the change in height is less than 0.1 meters for both, it is a failed grasp that is predicted successfully. If the test data does not have a significant change in its z-axis coordinates but the predictions do, then this is a false positive and finally, if the test data has a significant change in z-axis coordinates but the predictions do not, then it is a false negative.  We have found that the robot had more difficulty grasping non-rollable objects of equal size, most likely due to the fact that a cube and an upright cylinder are both grasped by straight surfaces whereas large spheres and sideways cylinders are grasped by curved surfaces from points that are located above their center of mass, causing them to slip easier.
%The evaluation was performed by checking the difference between the z-axis coordinates of the first and last time steps of the test trajectory and the prediction outputs. 

As shown, our system could successfully predict graspability affordance. Howevr, it is important to note that the average grasp action error is significantly larger than the average push action error. The mean error was relatively high because the incorrect graspability predictions generated high positional errors that significantly increase the mean error value. This is potentially due to unsuccessful grasps. In the event of an unsuccessful grasp, the object may slowly slip from the robot's hands and land on the table close to its initial position. The object may topple or roll (sometimes off the table), leading to position changes that are uncertain beforehand and therefore cannot be accurately predicted. 

\begingroup
\begin{table}[thbp]
\tbl{Single grasp action prediction results on variable sized objects placed on a fixed location.}
{\begin{tabular}{|c|c|c|c|c|} \hline
\textbf{Error (m)}& \textbf{True}& \textbf{True}& \textbf{False}& \textbf{False} \\
 & \textbf{Positive (\%)} & \textbf{Negative (\%)} & \textbf{Positive (\%)} & \textbf{Negative (\%)} \\\hline
\textbf{$0.275 \pm 0.044$} & 75.17 & 90.48 & 9.52 & 24.82 \\\hline
\end{tabular}}
\label{table:grasp_solo_single_errors}
\end{table}
\endgroup

\begin{comment}
\subsection{Reachability affordance}

%Up to now, only single actions with objects placed on a fixed location were investigated. This is because when the location of the object is not fixed, there is no guarantee whether an action can be performed on it. The robot may not be able to reach the object at all, or the robot may start applying the action but stop before completion when it can't reach the target anymore. Our model does not take into account the position of the objects. Therefore, another component is required which takes into account the positions of the objects and the actions to be applied; which can decide whether an action is feasible or not. 

In this section we analyzed the performance of reachability affordance prediction. A data set of 1000 interactions was collected where object size and location on the table could vary. The reachability classifier was trained on this data using binary cross-entropy loss with a simple feed-forward neural network.

The inputs to the network are chosen to be the starting object position, the starting gripper position and the partialness parameter of the action. The output is binary, denoting whether the action is reachable or not. The results obtained for this classifier are presented in Table \ref{table:reachability_table}. When tested on the simulation environment, the classifier correctly identified and eliminated most of the unreachable configurations, enabling finding solutions faster.

In a separately gathered test data set, it was observed that for 34 out of 35 (97.14\%) instances of actions being attempted towards objects and failing due to the object being unreachable, the reachability classifier was correct in its prediction that the specific action was indeed unreachable. Figure \ref{fig:reachability_chart} shows one such example where the reachability classifier was tested with all proposals for a configuration. The efficiency gained from discarding unreachable proposals vary depending on the configuration. It was observed that it could be as high as predicting all the actions to be unreachable correctly, or it could be a smaller region as can be seen in the example provided.

\begingroup
\begin{table}[thbp]
\tbl{Results from the reachability classifier, outputting whether an action is reachable.}
{\begin{tabular}{|c|} \hline
\textbf{Classification Accuracy (\%)}\\\hline
$97.80 \pm 0.87$ \\\hline
\end{tabular}}
\label{table:reachability_table}
\end{table}
\endgroup

\begingroup
\begin{figure}[h]
    \centering
		\includegraphics[width=1\columnwidth]{raw-figures/Figure 5.jpg}
		\vskip\baselineskip % Leave a vertical skip below the figure
		\caption{Example reachability analysis of a configuration. The same action is tested for reachability at different partialness levels which can be observed at the inner and outer regions of the circle. Actions with angles in the blue regions are predicted to be reachable by the classifier, red regions are predicted to be unreachable. The action with angle at the black line is known to be unreachable.}
		\label{fig:reachability_chart}
\end{figure}
\endgroup
\end{comment}


\begingroup
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\columnwidth]{raw-figures/Figure 7.jpg}
		\caption{Results of applying the model's predicted actions in a scene. Images are ordered from left to right. The top row is from the first action execution, the bottom row is from the second action execution. The blue object denotes the target position, the red object is generated from the robot's effect predictions. The blue and red objects are not interactable by the robot. The green object is interactable and is acted upon by the robot, to show the ground truth results of the robot's predicted actions.}
		\label{fig:snapshotsshotsshots}
	\end{center}
\end{figure}
\endgroup


\subsection{Planning performance}

Next, our model is requested to generate plans to bring the object to goal positions that might be beyond the range of single pushes or closer than a full-push. Therefore, the planner is expected to generated sequence of actions that might include partial executions as well. A sample plan execution is shown in Figure~\ref{fig:push_action_single+partial+multi}, where the plan is composed of two actions. The goal is shown with a blue box, the actual object positions are shown with red color and the predictions are shown with green color. 

Given goal positions, we run our model in three modes:
\begin{itemize}
    \item predict one push action to reach a goal, which is maximum one-step ahead, 
    \item predict one (possibly partial) push action to reach a goal, which is maximum one-step ahead,
    \item predict sequence of (possibly partial) push actions to reach a goal, which is maximum three-steps ahead.
\end{itemize}

After plans are made, they are executed by the robot. The distance between the goal position and final actual position of the object is reported as error. The results are provided in Figure~\ref{fig:push_action_single+partial+multi}. As shown, the object can be brought to the goal position more accurately if the partial actions are considered during planning even in single action execution case. Even error obtained in multiple (potentially partial) action executions is smaller than single full action execution. This shows the effectiveness of our system in making accurate plans.


%The predictions made by the model appear to have low errors, but there is no guarantee that the predictions will hold in the environment where the data was gathered from. For this reason, the same simulation scene was used to verify that the model's action and effect predictions are in fact realistic and hold out in the simulation world. One such example is shown in Figure~\ref{fig:snapshotsshotsshots}. The model does not predict orientations, therefore the orientation between the predictions and the ground truth have no reason to match.

%The multi-step setting is 3 random partial push actions applied one after the other, but the target can be achieved in fewer steps since the robot will aim towards the target using the Euclidean distance heuristic. We observe this in our results, and also note that since the model has no waypoints/subgoals or a preferred mode of operation a goal that was generated by applying the actions $<left, left, up>$ can be achieved by our model in the same manner, by applying any permutation of these actions, or for example two diagonal push actions. 

\begingroup
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.8\columnwidth]{raw-figures/Figure 6.jpg}
		\caption{Results of applying push actions on objects on a fixed location with fixed size with different settings.}
		\label{fig:push_action_single+partial+multi}
	\end{center}
\end{figure}
\endgroup

%The planner is utilized by proposing actions and predicting the full action and effect trajectories for proposed actions in a classic forward-chaining manner. However, as explained the model is not unidirectional and although the purpose of implementing the model in a bidirectional manner is achieving better latent representations, it can also be used to propose effects and calculate the full effect and action trajectories in the same way.




\begin{comment}
\subsection{Effects of Object Type}

We investigated how the type of the object affects the push action prediction errors. The results are presented in Figure~\ref{fig:error_based_on_obj_type}. Sphere and sideways cylinder type objects have both higher errors and higher variability in their error, this is expected since they are rollable objects and have higher displacement from an applied push action. Due to the planner proposing only a limited amount of action angles, the applied action angle can be different from the correct action angle. This angular difference affects the final distance of the object from the target, since rolling objects have higher displacement the distance can be higher than a non-rolling object with the same angular difference. 

\begingroup
\begin{figure}[h]
    \centering
		\includegraphics[width=0.8\columnwidth]{raw-figures/Figure 8.jpg}
		\caption{Errors based on object type for a single push action on fixed sized objects placed on a fixed location.}
		\label{fig:error_based_on_obj_type}
\end{figure}
\endgroup

\subsection{Effects of Different Training Regimes}

An experiment was performed where the training regime of the network was altered. The training regime proposed in this work is where between action, effect, or both input channel options, one is selected randomly at each iteration. To compare, training was done on the same data used to report our results, but only using both input channels at each iteration. The results presented in Table \ref{table:training_regime} show that the difference is not significant at $p < .05$.

\begingroup
\begin{table}[h]
\tbl{Training regime results comparison for a single push action task on objects on a fixed location with fixed size.}
{\begin{tabular}{|c|c|} \hline
 & \textbf{Error (m)}\\\hline
\textbf{Random channel} & $0.020 \pm 0.002$ \\\hline
\textbf{Double channel} & $0.019 \pm 0.006$ \\\hline
\end{tabular}}
\label{table:training_regime}
\end{table}
\endgroup

\end{comment}

\section{Conclusion}

In this paper, we proposed a model for multi-step action and effect prediction. While previous work's utilization of bidirectional learning is limited, our model specifically creates its latent representations using this concept and is able to make multi-step predictions that are in accordance with ground truth manipulations. We emphasize using object-centric inputs to achieve generalizability and investigate simple affordances of several classes of objects of different sizes. By using a network for single interaction predictions which can be interpreted similar to a state transition function and pairing it with a planner with heuristics to propose goal-directed actions the model is shown to have low error in reaching target positions.

Our key observations from this work are as follows:
\begin{itemize}
\item Using object-centric input representations enables a level of generalization capability that could not be obtained utilizing the same amount of training data. This observation is obtained from our prelimninary experiments where object and gripper positions were used directly as inputs to the model and gave poor results comparatively.
\item Using bidirectional learning in the model enables higher predictive capabilities than using a single channel to predict both action and effect information. This observation is obtained from our ablation studies where a single action channel network predicting both action and effect information learned from the same datasets used to report our results failed to converge.
\item Using the proposed training regime does not appear to give a significant decrease in average error, however it may be utilized to reduce the variance of error.
\item Using partial actions are shown to be effective in reducing compounding multi-step prediction errors.
\end{itemize}

While the results of our experiments are promising, the model still requires verification in the real world. Our next step is gathering data and testing our implementation with a real robot.

The reachability classifier is shown to successfully learn the reachability affordance. We are currently investigating a way to incorporate this capability into the network itself, so that when an action is proposed that is unreachable, the network's prediction will be to stay stationary and not act. Such a capability will show that the network can reason about multiple affordances, and more affordances can then be attempted, such as interactions with other objects. Such spatial affordances however require the network to be provided with certain position information. If this succeeds, improvements to the planner may need to be considered.

Our work uses a conditional architecture to avoid the compounding error problems of recurrent architectures and models that are used in a similar way by feeding their current step output as next step input. This advantage of using conditional models is shown in this work against using an LSTM network, and against a Multimodal Variational Autoencoder (MVAE) in \cite{seker2022imitation}. However, transformer models recently gained popularity as being a good alternative to recurrent models. By using the attention mechanism they eliminate the need for recurrence, and attention can potentially be beneficial for our model as well. 


\section{Disclosure statement}

No potential conflict of interest was reported by the authors.


\section*{Acknowledgement(s)}

The numerical calculations reported in this paper were partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources). Authors would like thank to Alper Ahmetoglu for providing insightful comments for this paper. 


\section*{Funding}
This research was supported by TUBITAK (The Scientific and Technological Research Council of Turkey) ARDEB; 1001 program (project number: 120E274); TUBITAK BIDEB; 2210-A program; and by the BAGEP Award of the Science Academy. 


\bibliographystyle{tfnlm.bst}
\bibliography{references.bib}

\appendix

%Permission to reuse Figure \ref{fig:lever_up}, Figure \ref{fig:lstm_traj_errors} and Figure \ref{fig:lstm_n_step} from \cite{seker2019deep} is obtained from the publisher with the license number 5394110778533.


\end{document}
