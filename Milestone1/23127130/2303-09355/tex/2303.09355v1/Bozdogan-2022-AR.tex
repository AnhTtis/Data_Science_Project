% interactnlmsample.tex
% v1.05 - August 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required
%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty
\makeatletter% @ becomes a letter
\def\NAT@def@citea{\def\@citea{\NAT@separator}}% Suppress spaces between citations using natbib.sty
\makeatother% @ becomes a symbol again

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\usepackage{verbatim}
%\usepackage{array}
%\usepackage{bm, fixmath, texdraw}
%\usepackage{epsfig}
%\usepackage{multirow}
%\usepackage{subfigure}
%\usepackage{algorithm}
%\usepackage{algorithmic}

% Packages added by me
%\usepackage{hyperref}
%\usepackage[backref=page]{hyperref} % This is better but they will probably complain.

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\begin{document}

\articletype{FULL PAPER}% Specify the article type or omit as appropriate

\title{Multi-step planning with learned effects of (possibly partial) action executions}

\author{
\name{Utku Bozdogan\textsuperscript{a}\thanks{CONTACT Utku Bozdogan Email: utku-bozdogan@hotmail.com} and Emre Ugur\textsuperscript{b}}
\affil{\textsuperscript{a,b}Department of Computer Engineering, Bogazici University, Istanbul, Turkiye}
}

\maketitle

\begin{abstract} %200 word limit on abstract for full papers
\\\resizebox{20pc}{!}{\includegraphics{raw-figures/abstract.jpg}}\\
%[Visual abstract here] \\
In this paper, we propose an affordance model, which is built on Conditional Neural Processes, that can predict effect trajectories given objects, action or effect information at any time. Affordances are represented in a latent representation that combines object, action and effect channels. This model allows us to make predictions of intermediate effects expected to be obtained from partial action executions, and this capability is used to make multi-step plans that include partial actions in order to achieve goals. We first show that our model can make accurate continuous effect predictions. We compared our model with a recent LSTM-based effect predictor using an existing dataset that includes lever-up actions. Next, we showed that our model can generate accurate effect predictions for push and grasp actions. Finally, we showed that our system can generate successful multi-step plans in order to bring objects to desired positions. Importantly, the proposed system generated more accurate and effective plans with partial action executions compared to plans that only consider full action executions. Although continuous effect prediction and multi-step planning based on learning affordances have been studied in the literature, continuous affordance and effect predictions have not been utilized in making accurate and fine-grained plans.
%Affordances are action possibilities provided by the objects in the environment to embodied agents. In general agents learn affordances through goal-free exploration and interaction with the environment and by observing the effects they create on objects with the available actions. Affordances generally encode the relations between objects, actions and effects and can be represented by models that predict effects given objects and action. Such models can then be used to achieve goals either by directly selecting the corresponding actions or by finding the sequence of actions via multi-step planning. The learned affordances have been utilized to make sub-symbolic and symbolic plans in the literature or to predict continuous effects, e.g. to predict the motion trajectories of the objects in response to actions. However, such continuous affordance and effect predictions have not been utilized in making more accurate and fine-grained plans in the literature. In this paper, we propose an affordance model, which is built on Conditional Neural Processes, that can predict effect or action execution trajectories given objects, action or effect information at any time-point. Within this framework, affordances are represented in latent representation that combines object, action and effect channels. This model allows us to make predictions of intermediate effects expected to be obtained from partial action executions, and this capability can be used to make multi-step plans that include partial action executions in order to achieve given goals. We first show that that our model can make accurate continuous effect predictions by comparing it with a recent Long Short Term Memory based effect predictor model in an existing interaction data set that includes lever-up action. Next, we showed that our model can generate accurate effect predictions for push and grasp actions. Finally, we showed that our system can generate successful multi-step plans in order to bring objects to desired positions. Importantly, we verified that the proposed system generated more accurate and effective plans with partial action executions compared to plans that can only consider full action executions.
\end{abstract}

\begin{keywords}
Affordances, effect prediction, object motion trajectory prediction, multi-step planning
\end{keywords}


\section{Introduction}

From a robotics standpoint, learning to predict the effects of a robot's actions beforehand would be a beneficial skill for robots, since this can prevent potential failures and dangerous situations to the robot and those around it, and enable planning for achieving certain goals.
%Challenges
Planning for multi-step tasks in the real world is difficult, and a generalized approach to solving this problem is even more so. Due to its difficulty, previous works compromise on certain aspects, such as predefining effect categories or object categories, simplifying the task. Discretizing information in the continuous sensorimotor space with the purpose of high-level symbolic planning with symbols may result in inaccurate plans.

In robotics, learned affordances have been used to choose objects for manipulation, to make discrete and continuous effect predictions given objects and actions, to make  plans to achieve goals \cite{Jamone2016,Yamanobe2017,Zech2017,Taniguchi2018}. However, to the best of our knowledge, there is no single framework that can predict effects given objects and action, predict required actions to achieve desired goals, can predict the movement trajectory of the objects in response to parametric actions and can make plans composed of sequence of actions, including partial action executions, in order to achieve given goals. The contribution of our work is as follows:
\begin{itemize}
    \item A novel latent representation for affordances: Considering affordances as relations between objects, actions and effects, our neural network based architecture forms a representation that encodes object, continuous action and continuous effect information in a single latent layer. 
    \item Accurate continuous effect prediction: Our system can predict the motion trajectories of the objects expected to be generated by parametric robot actions. We showed that the high prediction performance compared to a strong baseline that also make affordance-based continuous effect prediction.
    \item Multi-step planning with partial actions: We exploited the outcome prediction capability of our system for partial actions in order to form multi-step plans that may include full or partial action executions. 
    
\end{itemize}


\section{Related work}

\paragraph*{Early affordances work on effect predictions}
In early works such as \cite{fitzpatrick2003learning}, objects were required to be recognized first, and object-specific affordances were learned from robot's interaction experience with those objects, therefore the learned affordances could not be generalized to different/novel objects.  \cite{ugur2007learning} studies learning of traversability affordance where the LIDAR input was directly processed without any object detection step. Therefore, `direct perception' aspect of affordance perception was realized in that work, however only a single pre-defined affordance category was addressed. Effect categories and affordances were discovered by the robot in \cite{ugur2011unsupervised,ugur2011going} via unsupervised clustering techniques. However, unsupervised clustering results depend on the particular clustering method and the feature space that might be composed of values with different metrics such as distance, frequency, angle, etc. In \cite{ugur2011goal}, this is taken one step further and hierarchical clustering is made over channels for better effect category discovery. Both \cite{ugur2011going} and \cite{ugur2011goal} also enable forward chaining for multi-step planning. \cite{Szedmak2014} propagated affordance predictions by exploiting similarities among object properties, action parameters and resulting effects using Maximum Margin Multi-Valued Regression (MMMVR), obtaining efficient affordance learning; however affordance categories were also pre-defined in that study. In \cite{ugur2014bootstrapping},  complex affordance learning was bootstrapped through using pre-learned basic-affordances as additional inputs of the complex affordance predictors or as cues in selecting the next objects to explore during learning.
In \cite{kroemer2012kernel} the objects were represented via point clouds, in a non-parametric manner, to provide direct perception and the grasping and a single pouring action were shown to generalize well on novel objects but result in single-step plan only. Bayesian Networks were used \cite{montesano2007modeling, montesano2008learning}, enabling bidirectional predictive capabilities using the robot's own interaction experience, but clustering is performed on object features and effects in a predefined manner. In this paper, we also study acquiring bi-directional prediction capabilities. Different from previous work, we do not find effect clusters or do not only aim predicting the final effect. Instead, our system aims to learn predicting the complete effect/action trajectory during action execution. 

\paragraph*{Learning affordances for planning}
\cite{ugur2015bottom} learned affordances for symbolic planning, by again clustering effect categories and using object categories as a collection of effect categories obtainable by actions available to the robot. This enables the representation of nonlinear relations in planning, however, their discrete representation while making planning easy, makes the estimations approximate, increasing long-horizon planning errors. This work was validated on a real-world setup with simple actions such as poke, grasp, and release, and also with a more complex action which is stack. The experience of the robot enabled it to gain experience from the simple actions and after experiencing the stack action enabled it to generate a valid plan for the stacking action. This framework was extended in \cite{ugur2015refining} by enabling the robot to progressively update the previously learned concepts and rules in order to better deal with novel situations that appear during multi-step action executions. Similar planning capabilities were obtained by the robot using deep encoder-decoder neural networks with binary bottleneck layers \cite{ahmetoglujair} and with multi-head attention mechanism \cite{ahmetogluhum} by directly processing the pixel images instead of using hand-coded features.
In \cite{ames2018learning}, probabilistic planning with symbols using parameterized actions was applied to a real robot task, showing that continuous tasks can be performed with discrete planners using parameterized behaviours. Different from the previous work where only final outcomes were used for planning, our system can exploit intermediate effects expected to be observed for example in the middle of the action execution for planning.

\paragraph*{Learning visual grasp affordances}
Using RGB/RGBD images for predicting affordance classes or pixel-wise affordance labels for object manipulation has become popular in the recent years \cite{nguyen2016detecting, do2018affordancenet, mi2019object, hamalainen2019affordance, chu2019toward, thermos2021joint} and was shown to be a feasible approach for learning how to grasp different objects. Similarly, \cite{zhang2022inpaint2learn} are able to learn the affordances of objects such that they can place objects/humans in correct poses in a scene and also choose the correct object type to place in a given scene. \cite{ruiz2020geometric} uses point clouds to learn general geometric features from object interactions, enabling them to place objects in a scene correctly. In \cite{khazatsky2021can}, a generative model learned from interaction images was used to propose potential affordances. The aim was to learn a generalizable prior from interaction data and then utilize it to propose reasonable goals for unseen objects. These goals were then attempted to be executed by an offline RL policy, learned from interaction data, and tuned online efficiently to adapt to unseen objects. However, continous effect prediction and multi-step planning aspects were not  addressed in these studies.

\paragraph*{Affordances for efficient learning}
Affordances can also be used to reduce the search space in order to efficiently generate plans to solve long-horizon tasks. Recent approaches utilizing this idea extended the definition of affordances to represent not only single-step action-effect knowledge but as action feasibilities \cite{xu2020deep}, or intents which are similar to goals \cite{khetarpal2020can} in order to make affordances useful for multi-step plans. However, the feasibility concept of \cite{xu2020deep} accepts a grasp action achieving nothing as afforded, and would also accept a grasp action as afforded regardless of whether it was an appropriate grasp for an object. While \cite{khetarpal2020can} overcame this with intent representation, intents were specified a priori in their work and although a sub-goal discovery direction was proposed for learning, it was not explored.

\paragraph*{Learning continuous effect trajectories}
Similar to our work, \cite{seker2019deep} and \cite{tekden2020} learned to predict the full motion trajectory of an object, using the robot's own interaction experience, and top-down images of the objects. These studies are important for their inclusion of the temporal aspect of effects. The utility of different features was also investigated, such as hand-crafted shape features, CNN extracted features, or support point features extracted from a neural network. The authors used these features and the interaction experience to train recurrent neural networks and were able to accurately predict trajectories resulting from a lever-up action in a real-world setting with multiple objects. A common shortcoming of the aforementioned methods which make predictions for action or effects is related to the use of recurrent methods for long-horizon tasks. The use of recurrent networks such as LSTMs \cite{hochreiter1997long} or GRUs \cite{cho2014learning} is shown to be effective for short-term  predictions. However, their recurrent structure causes any error in their prediction to accumulate over time, causing lower success rates in executing long-horizon tasks \cite{pekmezci2021learning}. In this paper, we compare the effect prediction capabilities of recurrent neural network based systems and our conditional neural process based system.




\section{Proposed Method}

\subsection{General Architecture}

We propose a framework that can learn (i) predicting the effect trajectory given initial object image and action execution trajectory and (ii) finding the required action execution trajectory to achieve a desired effect on a given object. Our system is built on top of Conditional Neural Processes (CNPs) \cite{garnelo2018conditional} that bring together the inference potential of Gaussian Processes and the training of neural networks with gradient descent by learning a prior from data.  As we would like to predict both actions and effect from objects and (possibly missing) actions and effects, we propose a neural network structure that takes the object image as input together with action and/or effect values at different time points. In other words, given initial object image, our network can be conditioned with action and/or effect values at different time points in order to generate action and effect values for all time points during action execution. The general structure of the proposed system is shown in Fig.~\ref{fig:Figure_1}. As shown the system is composed of two encoders that process action and effect information at different time points, an image encoder that processes the depth image of the object, averaging operations to acquire a common object-action-effect representation (r), which in turn can be used to predict action and effect values at other time points using the corresponding two decoders.

\begin{figure}[t]
    \centering
		\includegraphics[width=0.95\textwidth]{raw-figures/Figure_1.jpg}
		\caption{An overview of the proposed model. Given object image and action or effect information at any time-point, our system can generate the effect trajectory and action execution trajectory.}
		\label{fig:Figure_1}
\end{figure}
%[Figure 1 near here]

In detail, inspired from Deep Modality Blending Networks \cite{seker2022imitation}, our system encodes information coming from different channels (object, action and effect) into a common latent space, using a weighted average of encoded modality representations, facilitating information sharing, and also providing a regularization effect on the representations learned, similar to dropout \cite{srivastava2014dropout}. Each channel is encoded separately by its own encoder, and the latent representations are subjected to a weighted averaging operation:

\begin{equation}
    r_{i} = \sum h_{a}(a_{t_{obs_{k}}}) * w_{1} +  h_{e}(e_{t_{obs{k}}}) * w_{2}
    \label{eq:affordx1}
\end{equation}

where $w_{1} + w_{2} = 1$, and

\begin{equation}
    r = r_{1} \oplus r_{2} \oplus r_{3} \oplus ... \oplus r_{i}
    \label{eq:cnp2}
\end{equation}

where the commutative operation used is averaging. Then, depth image features and target time-step are concatenated to form the merged representation $r_{mrg} = \{r,f(\gamma_{o}),t_{target}\}$.

This merged representation is then decoded by separate decoders, each corresponding to a different channel. This merged representation is decoded at the action decoder by
\begin{equation}
    g_{a}(r_{mrg}) = (\mu_{a_{t_{target}}}, \sigma_{a_{t_{target}}}),
\end{equation}
and the effect decoder by
\begin{equation}
    g_{e}(r_{mrg}) = (\mu_{e_{t_{target}}}, \sigma_{e_{t_{target}}}),
\end{equation}
to yield predictions for action and/or effect for the target time step shown in Fig~\ref{fig:Figure_1}. 

The latent representation (r) can be viewed as the shared affordance representation for actions $a$ and effects $e$ for different objects $o$. Learned affordances can then be used to predict the effects of actions or the required action to generate target effects. This model can be used to create multi-step action plans to achieve goals beyond single action executions by chaining the predictions.



\subsection{Training}
In our implementation, the system is object-centric. An action is defined in terms of the distance of the robot's end-effector to the object. An effect is defined as the displacement of the object from its starting position throughout an action. A depth image of the object is included as an external parameter $\gamma$ for the action. $t_{i}$ is the $i$th time-step of an interaction trajectory from the dataset $D$.

\begin{equation}
    D_{d} = (\{a_{t},e_{t},o,t\}_{t=0}^{t=1})_{d}
\end{equation}
is an interaction trajectory where $1 \leq d \leq m$ and  $m$ is the number of trajectories in the data set $D$.  $0 \leq t \leq 1$ is a phase variable  in control of the passage of time where $t \in \mathbf{R}$.


At each training iteration, $k$ observations are sampled uniformly at random from a randomly selected interaction trajectory $D_{d}$ where $1 \leq k \leq obs_{max}$, $k \in \mathbf{N}$ is also sampled uniformly at random and $obs_{max}$ is a hyper-parameter denoting the maximum number of observations that the model is allowed to use during one iteration. These observations are then encoded and aggregated. A cropped object depth image is encoded separately on a CNN encoder network, and the resulting vector is concatenated at the end of this aggregated representation. Before a prediction can be made, a target time-step is also concatenated after the image features. Finally, this merged representation is decoded to yield predictions for action and/or effect for the target time step shown in Fig~\ref{fig:Figure_1}. Gradient descent is used with the loss function (\ref{eq:cnmp_loss}) with Adam optimizer \cite{kingma2014adam}.

\begin{equation}
    \mathcal{L}(\theta, \delta) = - log P(y_{j} | \mu_{j}, softmax(\sigma_{j}))
 \label{eq:cnmp_loss}
\end{equation}

After training, the network is able to predict the entire interaction trajectory given a single observation at $t=0$. An A* planner \cite{Hart1968} is then used on top of the network to solve tasks requiring multiple actions and steps.



\subsection{Actions}

Our arm-hand robot is equipped with two actions, namely push and grasp. The parametric push action is specified by an angle $\theta \in [0,2\pi]$; a push distance $l = 0.05$ and a $radius = 0.2$ both in meters. The gripper starts the push execution from the red circumference of a circle of radius ($radius=20cm$) centered around the object (see Figure~\ref{fig:Figure_2}), at an angle $\theta$ and pushes the object $l$ meters from its center of mass. 

Larger sized objects may be displaced more as a result of this setup. The model is expected to learn the rollability and pushability affordances from these interactions and based on the object shape be able to predict the trajectories of  rollable objects such as spheres or lying cylinders and non-rollable objects such as cuboids or upright cylinders.

Grasp actions, on the other hand, are realized by lowering the open gripper to grasp position, attempting to grasp an object by closing the gripper and lifting the gripper up. Our model is expected to learn the graspability affordance from these interactions. Based on object size and shape, it should also be able to predict the interaction trajectories.

\begin{figure}[t]
    \centering
		\includegraphics[width=0.45\textwidth]{raw-figures/Figure_2.jpg}
		\caption{Scene showing the parameters of a push action around an object.}
		\label{fig:Figure_2}
\end{figure}
%[Figure 2 near here]

\subsection{Planning with partial action executions}

The A* planner (with Euclidean distance to the goal heuristic) is used to generate a sequence of actions to move the object from its initial position to a goal position. Each branch in the search tree corresponds to either a push action or a grasp action. As grasp action is not parameterized, there is a single branch for grasp action. On the other hand, as a push action might be applied from different approach angles and for different push distances, the range of possible approach angles and push distances are discretized and used to create multiple branches from the same node in the search tree. The initial 20\%, 40\%, 60\%, 80\% and 100\% segments of push action was considered during search. Additionally, the push direction was discretized into 36 directions. Therefore, the branching factor was set to 180.

The planner uses predictions of actions with different parameters to update the predicted location of the object. The search is completed if the difference between the predicted and goal object position is less than 2 cm. Our model is able to work with continuous inputs, however, the planner can only propose a finite amount of actions due to the mentioned discretization design choice. 

Interactions with single and multiple actions are generated, which also include partial actions. Partial actions are when an action is started to be applied, however it is not completed, i.e. it is executed partially. For example in a push action, the push may be cut short before the gripper even contacts the object, or the gripper reaches the center of mass of the object, meaning that the object has already started being pushed, but the push is not completed yet. Importantly, our model is trained only with full action interactions. Yet, our model can generate the effect at any desired time point, and therefore it can predict the consequence of partial action executions. Planner can use effects of such partial action executions to generate plans with finer resolutions (compared to plans that can only include full action executions).

Note that an action can be applied only if the object is reachable by the robot. Our model is expected to learn the reachability affordance from its interaction experience and actions are only considered during planning if the object is reachable. 



\section{Experiments results}

\subsection{Effect prediction performance}

In this paper, we propose a novel system to predict effects given objects and actions. Furthermore, our system can generate complete motion trajectories of objects as effects rather than their final positions. In order to assess the performance of our system, we compare our system with a recent study that can also predict motion trajectories of objects using CNN and Long short Term Memory (LSTM) model \cite{seker2019deep}. We used the same dataset that \cite{seker2019deep} used where lever-up actions were applied to objects with different geometric shapes from different points. An example lever-up action is shown in Figure~\ref{fig:Figure_3}. The objects had different number of edges (between 3 and 8), sizes and orientations. The objects were levered-up from different contact points. The authors translated and rotated the top-down 128x128 grayscale image according to the contact point and lever-up direction in order to simplify the prediction problem. The dataset was separated randomly into 80\% training, 10\% validation, and 10\% test as in \cite{seker2019deep}. 

\begin{figure}[b]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{raw-figures/Figure_3.jpg}
		\caption{Example lever-up action in the simulator, reused with permission from \cite{seker2019deep}.}
		\label{fig:Figure_3}
	\end{center}
\end{figure}
%[Figure 3 near here]

The experiments were performed with 5-fold cross-validation and early stopping with one million iterations. For comparison, we gathered n-step predictions, taking 15 previous steps as observations, and compared the results with the ones reported in the same manner in \cite{seker2019deep}. As shown in Figure~\ref{fig:Figure_4}, the error between the predicted and actual positions of the objects in \cite{seker2019deep} varies between $0.90-1.00$cm, whereas the error was in the range of $0.3-0.4$cm in our system (Table \ref{table:Table_1}). This comparison shows our model yields significantly lower error rates compared to a recurrent method, since the output is predicted directly avoiding error accumulation for multi-step predictions.

%[Table 1 near here]
\begin{table}
\tbl{N-step prediction errors obtained in our model.}
{\begin{tabular}{|c|c|} \hline
\textbf{1-step (cm)} &  $0.340 \pm 0.044$ \\\hline
\textbf{2-step (cm)} & $0.352 \pm 0.046$ \\\hline
\textbf{3-step (cm)} &  $0.364 \pm 0.049$ \\\hline
\textbf{4-step (cm)} &  $0.375 \pm 0.051$ \\\hline
\textbf{5-step (cm)} &  $0.387 \pm 0.054$ \\\hline
\end{tabular}}
\label{table:Table_1}
\end{table}

\begin{figure}[t]
    \centering
		\includegraphics[width=0.45\textwidth]{raw-figures/Figure_4.jpg}
		\caption{N-step error plots obtained in the LSTM approach, reused with permission \cite{seker2019deep}.}
		\label{fig:Figure_4}
\end{figure}
%[Figure 4 near here]


\subsection{Training Environment}
A simulated scene was constructed in CoppeliaSim \cite{coppeliaSim} as shown in Figure~\ref{fig:Figure_2}. A UR10 robot interacts with objects of different shapes and sizes by applying push and grasp actions on a tabletop setting. A Kinect sensor is placed above the table vertically such that the entire table is visible. The parts of the interaction where the object is potentially going to be displaced are recorded. The recorded information consists of action and effect data from once every 3 simulation steps (a single step is 50ms), which is chosen empirically, and a single depth image of the table with the object on top taken at the beginning of each interaction. The simulation dataset is split into training (\%80), validation (\%10) and test (\%10) sets.


\subsection{Single-action push and grasp effect prediction}
Different models were trained for push and grasp actions separately. The simulation data sets were always split between 80\% training, 10\% validation, and 10\% test data. For all the results reported in this work 10-fold cross-validation was applied unless otherwise specified. The models were trained for one million iterations, without batches due to variable length of inputs and early stopping was employed. The learning rate was set to $1\mathrm{e}{-4}$. All errors reported in meters denote the distances to a specified goal position. The predictions for a single action is fixed to take 25 time steps. 

For the push action, a data set made up of 500 trajectories was used. For each interaction, objects were chosen randomly and placed at the center of the table. An angle for the push $\theta \in [0,2\pi]$ was chosen randomly. The robot performed a complete push action and the resulting interaction data was recorded. The error in predicting the final position of the object was found to be around $0.02$m as shown in Figure~\ref{fig:Figure_6}. Similar to our analysis in the previous subsection, our system was shown to be effective in predicting object motion trajectories in push-like actions.

For the grasp action, a data set made up of 100 trajectories was used. Objects of varying sizes were randomly chosen to be placed at the center of the table. The robot then performed the grasp action and the resulting interaction data was recorded. The results for the grasp action are provided in Table \ref{table:Table_2}. In interpreting the performance and success of graspability prediction, if the change in height is larger than 0.1 meters, the grasp was assumed to be a success. If the change in height is less than 0.1 meters, it is a failed grasp. If the test data does not have a significant change in its z-axis coordinates but the predictions do, then this is a false positive and finally, if the test data has a significant change in z-axis coordinates but the predictions do not, then it is a false negative.  We have found that the robot had more difficulty grasping non-rollable objects of equal size, most likely due to the fact that a cube and an upright cylinder are both grasped by straight surfaces whereas large spheres and sideways cylinders are grasped by curved surfaces from points that are located above their center of mass, causing them to slip easier.

%[Table 2 near here]
\begin{table}
\tbl{Single grasp action prediction results on variable sized objects placed on a fixed location.}
{\begin{tabular}{|c|c|c|c|c|} \hline
\textbf{Error (m)}& \textbf{True}& \textbf{True}& \textbf{False}& \textbf{False} \\
 & \textbf{Positive (\%)} & \textbf{Negative (\%)} & \textbf{Positive (\%)} & \textbf{Negative (\%)} \\\hline
\textbf{$0.275 \pm 0.044$} & 75.17 & 90.48 & 9.52 & 24.82 \\\hline
\end{tabular}}
\label{table:Table_2}
\end{table}

As shown, our system could successfully predict graspability affordance. However, it is important to note that the average grasp action error is significantly larger than the average push action error. The mean error was relatively high because the incorrect graspability predictions generated high positional errors that significantly increase the mean error value. This is potentially due to unsuccessful grasps. In the event of an unsuccessful grasp, the object may slowly slip from the robot's hands and land on the table close to its initial position. The object may topple or roll (sometimes off the table), leading to position changes that are uncertain beforehand and therefore cannot be accurately predicted. 


\subsection{Planning performance}

Next, our model is requested to generate plans to bring the object to goal positions that might be beyond the range of single pushes or closer than a full-push. Therefore, the planner is expected to generate sequences of actions that might include partial executions as well. A sample plan execution is shown in Figure~\ref{fig:Figure_5}, where the plan is composed of two actions. The goal is shown with a blue box, the actual object positions are shown with red color and the predictions are shown with green color.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\columnwidth]{raw-figures/Figure_5.jpg}
		\caption{Results of applying the model's predicted actions in a scene. Images are ordered from left to right. The top row is from the first action execution, the bottom row is from the second action execution. The blue object denotes the target position, the red object is generated from the robot's effect predictions. The blue and red objects are not interactable by the robot. The green object is interactable and is acted upon by the robot, to show the ground truth results of the robot's predicted actions.}
		\label{fig:Figure_5}
	\end{center}
\end{figure}
%[Figure 5 near here]

Given goal positions, we run our model in three modes:
\begin{itemize}
    \item predict one push action to reach a goal, which is maximum one-step ahead, 
    \item predict one (possibly partial) push action to reach a goal, which is maximum one-step ahead,
    \item predict sequence of (possibly partial) push actions to reach a goal, which is maximum three-steps ahead.
\end{itemize}

After plans are made, they are executed by the robot. The distance between the goal position and final actual position of the object is reported as error. The results are provided in Figure~\ref{fig:Figure_6}. As shown, the object can be brought to the goal position more accurately if the partial actions are considered during planning even in single action execution case. Even error obtained in multiple (potentially partial) action executions is smaller than single full action execution. This shows the effectiveness of our system in making accurate plans. For the results reported in Figure~\ref{fig:Figure_6}; the single full setting is tested with 50 goals, the single partial setting with 100 goals, and the multi partial setting with 80 goals.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{raw-figures/Figure_6.jpg}
		\caption{Results of applying push actions on objects on a fixed location with fixed size with different settings.}
		\label{fig:Figure_6}
	\end{center}
\end{figure}
%[Figure 6 near here]


\section{Conclusion}

In this paper, we realized a model for multi-step action and effect prediction. While previous work's utilization of bidirectional learning is limited, our model specifically creates its latent representations using this concept and is able to make multi-step predictions that are in accordance with ground truth manipulations. We emphasize using object-centric inputs to achieve generalizability and investigate simple affordances of several classes of objects of different sizes. By using a network for single interaction predictions which can be interpreted similar to a state transition function and pairing it with a planner with heuristics to propose goal-directed actions the model was shown to achieve low error in reaching target positions. While the results of our experiments are promising, the model still requires verification in the real world. Our next step is gathering data and testing our implementation with a real robot.

Our work uses a conditional architecture to avoid the compounding error problems of recurrent architectures and models that are used in a similar way by feeding their current step output as next step input. This advantage of using conditional models is shown in this work against using an LSTM network, and against a Multimodal Variational Autoencoder (MVAE) in \cite{seker2022imitation}. Recently, transformer models  gained popularity as being a good alternative to recurrent models. By using the attention mechanism they eliminate the need for recurrence, and attention can potentially be beneficial for our model as well. We plan to investigate the capabilities of such models in future.


\section{Disclosure statement}

No potential conflict of interest was reported by the authors.


\section*{Acknowledgement(s)}

The numerical calculations reported in this paper were partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources). Authors would like thank to Alper Ahmetoglu for providing insightful comments for this paper. 


\section*{Funding}
This research was supported by TUBITAK (The Scientific and Technological Research Council of Turkey) ARDEB; 1001 program (project number: 120E274); TUBITAK BIDEB; 2210-A program; and by the BAGEP Award of the Science Academy. 


\bibliographystyle{tfnlm.bst}
\bibliography{references.bib}

\appendix

%Permission to reuse Figure \ref{fig:Figure_3}, Figure \ref{fig:lstm_traj_errors} and Figure \ref{fig:Figure_4} from \cite{seker2019deep} is obtained from the publisher with the license number 5394110778533.

\end{document}
